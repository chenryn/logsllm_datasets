make the mistake of predicting “allow” for “deny” and vice
versa (see Section VII-C for more details about the impact of
such mistakes). Again, there is little difference between the
results for BLR and SVM.
The sensitivity of MAE to different types of decisions is
also reﬂected in the worse performance of ZeroRt in Figure
6(a) compared to its performance in Figure 6(b). ZeroRt has
many predictions with MAE of 2; they are ignored under the
0-1 loss but not under the MAE loss, which is why ZeroRt
performs worse with the latter. In contrast, our context-aware
methods perform similarly under both loss functions.
Figure 7 compares the individual performances on 20 par-
ticipants for one random partition and tu = 100%. We plot
MAE losses obtained by the baselines and SVM versus those
Static Policy
ZeroR_t
App-Name (A)
Method (B)
Category (C)
Foreground (D)
A+B+C+D
All Features
0.1
0.2
0.3
0.4
0.5
0.6
Mean Absolute Error (MAE)
Importance of individual contextual features for prediction.
Fig. 8.
We show box-plots of MAE obtained for 50 different partitions with
tu = 100%. The top two box-plots are for the baselines, and the
next four are for BLR models using only one feature. Even with one
feature containing a single context regarding the app being in the
foreground or background, BLR outperforms the baselines. The last
two box-plots use all four features and all 37 features, respectively,
where we see that BLR with the four features performs slightly better.
For the details of features, see Sections IV-C and V-A6.
obtained by BLR. Each point corresponds to the MAE of a
participant estimated using (3) with Nu = 20 and U = 1.
The MAE of the static policy method, ZeroRt and SVM
methods are shown by crosses, circles, and dots, respectively.
A thin grey line joins the two MAEs obtained on the same
participant. A point above the dashed grey line indicates
that the corresponding baseline gives worse performance than
BLR, which is the case for most participants. Among the
baselines there is no clear winner. Similarly, there is no clear
winner between BLR and SVM. Note that these MAE values
are less stable as they are estimated with only 20 points. For
many participants, standard errors are in the order of 0.1. The
numbers reported in Figure 6 are more stable and reliable
since they are estimated with a large number of test decisions
(20 participants with 20 test decisions each giving us a total
of 400 points). Nevertheless, Figure 7 shows that even across
participants adding context improves the performance.
Also, note that in Figure 7 the variance across participants
is quite high. We can predict some participants very well
(MAE is close to zero), whereas for others MAE could be
as high as 0.7. The aggregate over participants however is
quite satisfactory, e.g., in Figure 6(c) where each participant
is equally represented (each participant contributes 20 test
decisions in the histogram). For the participants we cannot
predict well, possible reasons are that more data is needed or
that they were not consistent in their decisions.
Figure 8 explores the importance of individual features
for prediction. We show box-plots of MAE obtained for 50
different partitions with tu = 100%. The top two box-plots
are for static policy and ZeroRt, respectively. The next four
box-plots show the performance of BLR obtained after adding
only one feature out of the following four features: (A) the
name of the app requesting permission, (B) the method of the
1070
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:06 UTC from IEEE Xplore.  Restrictions apply. 
request, i.e., the actual API call, (C) the method category, i.e.,
contacts, location, or storage, and (D) whether the requesting
app was in the foreground. Even with just one feature, we
obtain improvements over the baselines that use (almost) no
contextual information at all. The most striking among these
features is the feature D (regarding the app being in the
foreground or not) which obtains a much lower median error
of 0.25 compared to the baselines. The 7th box-plot shows
the performance obtained with all the 4 features (A+B+C+D)
which achieves a slightly lower and robust MAEs compared
to the last box-plot which shows performance when all 37
features are used. This type of behavior is expected when the
sample-size is small which is the case here.
G. Computational Performance
Our chosen machine learning model, BLR, is simple enough
to run on smartphones. To evaluate its computational perfor-
mance, we used a Motorola Moto G 3rd generation smart-
phone with Android 5.1.1, 7 contextual features, 5-fold cross-
validation, and around 200 decisions from a single participant.
We ported our BLR algorithm to Android using the Efﬁcient
Java Matrix Library (EJML) [55]. With this setup, training
took around 1.32±0.31 s and prediction 50±6 μs, and the CPU
usage was not higher than 50%. These results show that our
approach is feasible in smartphones, particularly if we take into
account that training does not need to happen frequently (e.g.,
a couple of times during the day). Moreover, training can be
done while the smartphone is idly charging to avoid draining
the battery or interfering with apps. In future work, we also
plan to evaluate BLR with sequential updating (i.e., online
learning), by using one rank update of the Cholesky factor
or stochastic gradient descent. Such approach should reduce
training time signiﬁcantly.
In addition, we estimated the
impact on performance of our SmarPer prototype, particularly
the service that collects context information and intercepts
apps’ requests. Using OS and third-party tools, we did not
measure a signiﬁcant impact on CPU usage from our service.
Regarding battery life, we used Android’s Battery monitor API
to measure if using SmarPer drained the battery faster, but we
did not measure a signiﬁcant difference on battery life between
a smartphone with and without SmarPer. Moreover, none of
the participants reported problems with battery life.
VII. DISCUSSION
In this section, we present further discussion of the results
obtained in our data-collection and machine learning analysis,
as well as deployment considerations for SmarPer.
A. Amount of Training Data vs Model Complexity
Figures 6(a) and 6(b) show that the MAE and ICR for our
BLR model continue to decrease by the end of the experiment,
i.e., tu = 100%. Thus, the performance of our model can be
further improved with more data. In contrast, for ZeroRt, they
ﬂatten out around tu = 80%, hence, more decision data may
not help improving the performance of ZeroRt.
As there is a wide variance in users’ preferences, it is
recommended to collect data from a sufﬁciently large number
of users. The number of decisions per user depends on the
type of their preferences as well as on the dimensionality of
the contextual features. A larger amount of training data will
enable the application of advanced machine learning models
that can capture the wide variance in the privacy preferences,
for example, a topic model can represent a user as a mixture of
several types of privacy preferences and is likely to be much
more accurate [56]. In short, the amount of training data and
time required to train an accurate model depend on the user
and their willingness to provide decision data.
In our experiments, we did try several non-linear models
that are based on Gaussian process regression (GP-SE), as
well as a SVM and decision tree models. These models only
marginally improved the performance over a linear model
(Figures 6(a) and 6(b)), suggesting that the amount of data
is perhaps not enough for training more complex models. On
41 users with 8,521 decisions with 37 contextual features, a
linear model worked the best. As a next step, we plan to
collect additional decision data and to evaluate more advanced
machine learning models to improve prediction accuracy.
B. Automating Permission Decisions
We ran several experiments to evaluate how our BLR model
predicts and automates decisions to reduce users’ overhead,
i.e., the number of prompts to answer. For this purpose, we
estimated the conﬁdence of our model on each decision (using
the mean and variance) and deﬁned different thresholds to
decide whether to automate the decision or prompt the user.
However, as stated before, we did not have enough data to
reach concrete conclusions. Moreover, we face the challenge
of determining when the model is accurate enough to start
automating decisions. On the one hand, our model needs as
much decision data as possible to improve its performance, but
on the other hand, once our model starts automating decisions,
it will obtain less decision data.
The simplest approach is to make decision prompts ran-
domly and limit
the number of requests per day. In our
experiments, this works reasonably, although we found that
ultimately using all the data gives the best results. This implies
that all of our data are useful for prediction. This also implies
that, in practice, an automatic system might collect a few
responses each day until enough decisions have been collected.
We could follow the approach used in our data collection
phase, i.e., to prompt users a limited number of times per day
to collect data to train our model, and to rely on user-deﬁned
static policies for other requests. To collect enough decisions
per app and reduce the risks of overwhelming the user, we
recommend to start collecting data only for a subset of apps
selected by the user (e.g., most used apps); gradually, other
apps can be added to the system. Another option, is to proﬁt
from the data from similar users to accelerate the learning
process, i.e., user proﬁles. However, the system ﬁrst needs to
learn the various types of preferences. Once we have data from
a sufﬁcient number of users and apps, we will be able to map a
1071
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:06 UTC from IEEE Xplore.  Restrictions apply. 
new user to a particular proﬁle and start automating decisions
for certain requests instead of relying on static policies. As
mentioned before, our data set does not contain enough users
to identify proﬁles via clustering.
Using our machine learning framework, we could regularly
train and test our model to monitor its performance (e.g., once
a day). If the MAE is lower than a deﬁned threshold (function
c(), see Section III-C), the decisions are made automatically,
using the learned model. To keep the model updated, we
can follow an exploration-exploitation trade-off [57], where
we prompt the user for more decision data at a rate that is
an increasing function of the current estimation of the MAE
(i.e., the higher the estimated error, the higher the prompt
rate). Thus, the decision on a request would depend both on
the current estimate of the user’s preference and on random
explorations, i.e., the mean can be used to exploit and the
variance to explore. These learning methods are popular in
machine learning and are useful for automating decisions.
The acceptable MAE is a user-dependent value and can
be estimated only via a long-term study and/or interviews. If
the decisions are balanced across allow, obfuscate, and deny,
predicting obfuscate all the time would give an MAE of 0.66.
We could suggest some reasonable values (e.g., 0.20), but the
user must decide which value is acceptable.
to prevent
Moreover, as discussed in Section III, SmarPer includes
an audit function for correcting incorrect automatic decisions.
Such corrected decisions are added to the training set, po-
tentially with higher weights,
the system from
repeating the same errors. This feature was not evaluated in
our ﬁrst study, as no automatic decisions were made during the
experiment; but it will be in the second phase of the project. As
for the adoption of the audit feature, we observed during the
study that participants ﬁxed their own incorrect decisions: For
instance, when participants realize that one of their decisions
breaks the app functionality, they clear SmarPer’s decision
cache and restart the app. Such behavior suggests that the
audit feature would be used by users.
Overall, our user survey shows that participants are in-
terested in automatic decisions. For instance, 66% of our
participants reported that they will trust a system that makes
automatic permission decisions on their behalf. In addition,
88% said they would be “very interested” or “interested” to
see a feature like SmarPer in a new version of Android.
The second phase of the SmarPer project, on which we are
currently working, will enable us to evaluate our approach
and the user perception with respect to automating decisions
(accuracy, frequency and adequacy of the prompts, sensitivity
to prediction errors, i.e., determining what is an acceptable
value for the MAE and the user preferences with respect
to undersharing vs. oversharing) and the use of SmarPer’s
features, such as the audit option.
C. Impact of Predicting Permission Decisions
SmarPer’s purpose is to learn and emulate users’ privacy
behaviors. That is, if a user tends to put her privacy at risk by
sharing large amounts of information, the trained model will
do the same. Problems arise if the predicted decision does not
match the user’s intent. First, if the model predicts “allow”,
instead of “deny”, sensitive information will be revealed to
apps, i.e., privacy loss due to oversharing. Second, if the model
predicts “deny”, instead of “allow”, the app will not work as
the user expects, i.e., utility loss due to undersharing. Third, if
the model predicts “obfuscate”, instead of “deny” or “allow”,
some privacy or utility loss occurs, depending on the scenario
(data type), i.e., partial-oversharing or partial-undersharing.
Note that, for all practical purposes, undersharing (and some
partial-undersharing) errors mean that an app will not longer
work because it was denied the permissions required for key
functionality. In some cases, an app could crash or behave
unexpectedly (see Section VII-D). Still, the chance of such
problems is low, now that Android supports disabling per-
missions (Android 6+), as most apps can gracefully handle
denied permissions.
In short, SmarPer should be evaluated
with respect to its ability to mimic users’ decisions and to the
types of incorrect predictions.
As Figure 6 shows, BLR and SVM models rarely make large
mistakes predicting “allow” for “deny” and vice versa. More
speciﬁcally, BLR has an average per-user of 0.6±0.6% over-
sharing error, 9.4±1.9% partial-oversharing error, 8.8±1.8%
partial-undersharing error, and 0.8±0.5% under-sharing error.
In contrast, static policy (currently deployed approach) has an
average per-user of 7.1±2.6% oversharing error, 17.4±3.0%
partial-oversharing error, 11.5±3.2% partial-undersharing er-
ror, and 2.6±2.0% under-sharing error. As a result, we can see
that, compared with static policy, our approach signiﬁcantly
reduces the loss of privacy and utility. Also, note that the
errors reported for our approach assume that all the predicted
decisions are automated. In practice, SmarPer will automate
only a subset of the predicted decisions based on the output
of the function c() (Section IIC). Hence, the errors should be
smaller. To reduce these errors further, we can also use cost-
sensitive training [50], where users can conﬁgure the kind of
errors to minimize (oversharing/undersharing), as in [28].
D. Data Obfuscation
Obfuscation was well-received by the participants. Over our
whole data set, 29% of decisions were obfuscate decisions.
A similar fraction was observed across data types: Users
obfuscated 25% of requests for contacts, 26% of requests for
location, and 32% of requests for storage. Moreover, in our
exit survey 73% of participants found obfuscation useful and
80% stated that they would like to obfuscate additional data
types. Still, some participants did not ﬁnd obfuscation useful,
e.g., a few participants chose to always deny or always accept
apps’ requests. Also, a small number of participants reported
that obfuscation caused problems with certain apps, hence they
stopped using this option. It is also possible that, in spite of the
training provided, some participants did not fully understand
the purpose of obfuscation.
In our exit interviews, participants shared some use cases
for obfuscation: “For apps that need location, such as Ac-
cuweather, I was giving obfuscated access most of the time
1072
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:06 UTC from IEEE Xplore.  Restrictions apply. 
Obfuscation may introduce unexpected behaviors
as I expected the functionality to be ﬁne with coarse grained
location.”–D69. Another participant explained the beneﬁts of
obfuscation: “Even if we don’t want to give all the information
to the application used, we still have to give some, in order to
the application to be useful”–37F. See Table III (Appendix B).
in
apps [24]. In our ﬁeld test, few participants reported non-
critical issues with obfuscation. For instance, some participants
reported that storage obfuscation interfered with apps that take
or edit photos (e.g., Twitter, Facebook), as they need to access
the pictures folder. Also, participants reported that WhatsApp
was not displaying contact name’s correctly when obfuscation
was chosen for contacts. Coordination with developers and
mobile platform providers is important to reduce these prob-
lems. For example, native APIs could be introduced to handle
obfuscated data types and handle possible runtime exceptions.
E. Privacy Beneﬁts of a Per-User Model
Prior works aggregate permission preferences and related
information from all users to train a one-size-ﬁts-all classi-
ﬁer [2], [8] or to identify privacy proﬁles via clustering [8],
[9]. However, aggregating this information introduces privacy
risks, as it can be misused to infer sensitive information about
users. In SmarPer, we show that is feasible to train a model
per user directly in the user’s smartphone, i.e., no permission
information is sent to other parties. Note that in the case of
BLR, partial decision data from a subset of users is needed to
learn the model hyperparameters. However, instead of sending
raw decision data to a central location, users can send sufﬁcient
statistics (e.g., expectation of β0
u ) to defend against
inference attacks. More advanced models (e.g., SVMs) do not
need this (but might require more data for training).
u and βT
F. Limitations
There are many challenges associated with predicting and
automating permission decisions at runtime using contextual
information. To provide some guidance to future works in
these area, we present the main limitations of our approach:
• Participant’s bias. Due to the nature of our evalua-
tion, participants might have some bias towards a more
privacy-preserving behavior and, in particular, towards
using obfuscation. Such bias is common in the evaluation
of privacy tools and is difﬁcult to avoid. To reduce it, we
presented in a neutral way the decision options (allow,
obfuscate, deny) to participants during their training.
• Focus on popular apps. To obtain enough decision data
per app for our study, we collected data only from popular