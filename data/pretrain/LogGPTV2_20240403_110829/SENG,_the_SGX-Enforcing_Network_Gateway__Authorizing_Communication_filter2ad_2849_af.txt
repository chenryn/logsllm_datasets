1.9.1 [36], OpenSSL 1.0.2g and the challenger code of sgx-
ra-tls. The core functionality consists of ∼1300 lines of code,
and support for SENG server sockets adds ∼1500 lines. The
server uses a TUN device as IP-level virtual network interface
to the gateway. The SENG server conﬁgures the TUN device
as the default gateway for connected SENG runtime clients
and links each DTLS tunnel to the client’s enclave IP address.
9 Evaluation
We now evaluate our prototype implementation regarding
efﬁcacy and overhead. We use iPerf3 [26] to measure the
network throughput, and then show how the results transfer to
real-world client (cURL, Telnet) and server (NGINX) applica-
tions. We then provide microbenchmarks to measure the setup
phase of the SENG runtime. Afterwards, we revisit SENG’s
NGINX performance and signiﬁcantly improve it by porting
NGINX to the SENG-SDK. We conclude with a discussion
on the SENG server scalability under an increasing number
of enclaves and according tunnels.
In our experiments, the SENG server runs on a workstation
with an Intel R(cid:13) CoreTMi5-4690 CPU with 4 cores, 32 GB of
9Intel’s SGX port of OpenSSL
Figure 5: iPerf3 Throughput of a Single TCP Connection
memory and Debian 9 with a 4.9 Linux kernel. The SGX-
enabled client system has an Intel R(cid:13) CoreTMi7-6700 CPU
with 8 cores, 64 GB of memory and runs the SGX enclaves
inside a Ubuntu 16.04.4 LTS docker container with a 4.15
Linux kernel. The underlying host runs Ubuntu 18.04.2 LTS.
Both systems are connected to the local network via 1 Gbps
NICs (Intel I217-LM/I219-LM). We route the client’s trafﬁc
via the SENG server to ensure that trafﬁc from and to our
SGX client system passes our virtual network gateway.
We take the native execution of the applications (“native”)
as baseline for our evaluation and compare it with the perfor-
mance of Graphene-SGX (“pure”) and of SENG (“SENG”).
This way, we can attribute the overhead to either Graphene-
SGX or the additional latency and overhead introduced by the
SENG runtime and SENG server components.
9.1 Network Performance
We ﬁrst report on the maximum downlink throughput of a sin-
gle TCP connection using iPerf3. iPerf3 sends TCP packets
to another iPerf3 instance and measures the resulting through-
put. We generate the trafﬁc on the gateway and receive trafﬁc
inside the enclave on the client system. We keep the default
conﬁguration of iPerf3 which calculates the average over 10 s
and we step-wise increase the bandwidth of the work load.
Figure 5 shows the average receive throughput over ﬁve iter-
ations. The throughputs of all three approaches scale linearly
with increased iPerf3 bandwidths, and SENG shows no over-
head for bandwidths up to ∼800 Mbps. The native and pure
Graphene-SGX setups both reach a maximum throughput of
925.93 Mbps, whereas SENG’s peak average throughput is
867.66 Mbps (∼6% lower). Our 10 s measurements include
TCP’s slow start, and we observed higher temporal through-
puts of ∼933 Mbps for native and pure, as well as ∼899 Mbps
for SENG, reducing the peak loss to 3–4%. The slightly lower
peak throughput of SENG is caused by the additional latency
added by the SENG-internal TCP/IP stack and the DTLS tun-
nel. We included the results of SENG with enclave exits on
every syscall (∼390 Mbps) to highlight that exitless designs
USENIX Association
29th USENIX Security Symposium    765
02004006008001000Bandwidth[Mbps]02004006008001000Throughput[Mbps]390.36925.93867.66native|pureSENGwithoutexitlesssyscallsFigure 6: Time differences from cURL Benchmark
Figure 7: Average Request Latencies of NGINX
are a key-enabler for I/O-intense enclaves [2, 42].
We conclude that the reduced throughput peak (3–7%) is
acceptable, especially as clients and/or remote parties are typi-
cally bound to lower bandwidths, which showed no overhead.
9.2 Client Applications
cURL.
cURL is a popular tool/library to transfer data via
several common protocols. In our setting, an external partner
could use cURL to exchange ﬁles with internal servers. We
have chosen cURL to check if SENG readily supports and
scales to real-world client apps. To this end, we set up an
Apache web server and measured how long cURL takes to
download ﬁles via HTTP. Apache runs on the local gateway
to capture the overhead with minimal impact from network
jitter, analogous to iPerf3. We used the built-in measurements
of cURL and took the 30 % trimmed mean over 50 iterations
for each ﬁle size as a robust estimator [2].
Figure 6 shows the observed download time overhead rela-
tive to native execution. Graphene-SGX is again on par with
the baseline as it shares the untrusted kernel network stack.
For a ﬁle size of 1 MB, SENG shows minimal overhead due
to the short download time. As the ﬁle size increases, SENG
faces overhead of 8.8–14.1% which is higher than the one
reported for iPerf3, but still reasonable. We observed TCP
segmentation for every cURL payload, which was not present
during iPerf3 and adds reassembly load and delay on lwIP as
it cannot use HW ofﬂoading and has a lightweight design.
We conclude that SENG also shows reasonable perfor-
mance for real-world client apps. Note that exitless syscalls
in Graphene-SGX are still experimental and future versions
might stabilize and further reduce the network overhead.
Telnet. Telnet (RFC 854) is widely used for remote termi-
nal access and serves as our representative for remote login
tools. SENG’s built-in DTLS tunnel protects plaintext Telnet
against local system-level and on-path attackers within the
organization network. Furthermore, SENG can restrict remote
access to trusted, TLS-based login clients and shield them
from local user- or system-level attackers (e.g., hooks).
We used a Telnet server on a local workstation and mea-
sured over 10 iterations the average time it takes for a Telnet
client to log in, execute a set of Bash commands for entering
a directory, list the contained ﬁles, and ﬁnally, display the con-
tent of a 1 kB document. Telnet takes 269.38 ms during native
execution and faces 0.17 % overhead for Graphene-SGX and
0.09 % for SENG, which is practically negligible.
9.3 Server Application (NGINX)
We next evaluate a server setting where we aim to shield an in-
ternal server from internal MITM and system-level attackers.
We chose NGINX as a demonstrator which is a wide-spread
event-based HTTP server. NGINX runs on the client host
inside SGX and uses a single, poll-based worker thread to
serve the 612 Byte demo page via HTTP. We used the wrk2
benchmark tool from an internal workstation to issue HTTP
requests under step-wise increasing request frequency. For
each workload, wrk2 spawned two threads with 100 connec-
tions and calculated the mean reply latency over ten seconds.
Figure 7 shows the average latencies over ﬁve iterations.
Graphene-SGX and SENG can handle ∼15 k requests per
second with a per-reply latency of 1.5–2.5 ms before perfor-
mance degrades. Native execution clearly outperforms “pure”
and SENG with ∼40 k. This is no surprise and follows the ob-
servations of Tsai et. al [9], because Graphene-SGX currently
only supports synchronous syscalls, which cannot effectively
overlap computation and I/O. We inspected the CPU utiliza-
tion of NGINX under different loads and revealed that in the
“pure” and “SENG” setting, the NGINX thread saturates the
CPU via continuous polling and Graphene’s I/O overhead.
In conclusion, SENG cannot yet compete with native
NGINX, but is on par with Graphene-SGX while provid-
ing more security guarantees and features on top of it. Fur-
thermore, the bottleneck can be attributed to Graphene-SGX
rather than to SENG and we therefore expect better perfor-
mance under future asynchronous or batched I/O support. In
766    29th USENIX Security Symposium
USENIX Association
1MB10MB20MB40MB100MB1GBFileSize−40−30−20−10010203040Downloadtimeoverhead[%]SENG:+0.40%(0.05ms)+12.2%(11ms)+8.8%(15ms)+12.2%(42ms)+14.1%(121ms)+11.6%(994ms)nativepureSENG159131721WorkLoad[1kreq/sec]02004006008001000Latency[ms]nativepureSENGSENG-sdk31343740434649Microbench
Spawn lwIP thread
OpenSSL init
RSA key gen (2048)
get SGX quote
get IAS report
gen X.509 Cli-Cert
DTLS Tunnel setup
Spawn Tunnel thread
Total SENG Runtime
Without SSL Init
Without SSL Init + IAS
(a) LibOS init (default)
(b) LibOS init (reduced)
(c) LibOS init (minimal)
Time [ms]
38.13
710.98
84.55
35.67
639.05
1.59
19.86
42.64
1578.03
867.05
228.00
868.00
728.27
274.27
StdDev [ms]
± 0.53
±10.16
±66.25
± 2.20
±16.46
± 0.13
± 1.22
± 1.20
±68.12
-
-
±12.64
± 8.06
± 1.67
Table 4: Client Setup Times of SENG and Graphene-SGX
Section 9.5, we will revisit this claim and show that we can
signiﬁcantly improve the performance of NGINX by porting
it to the SENG-SDK (cf. “SENG-sdk” in Figure 7).
9.4 Setup Microbenchmark
We now measure the initialization overhead that the SENG
runtime adds to Graphene-SGX, excluding the prototype-
speciﬁc socket API handlers. As the setup time of Graphene-
SGX depends on the enclave conﬁguration, we measured the
time for three conﬁgurations: (a) default values of LibOS-
internal tests, (b) with reduced stack, heap and thread number,
and (c) with minimal accepted size.10 For SENG, we mea-
sured the different setup phases of the runtime.
Table 4 breaks down the average setup times over ten it-
erations. The total startup overhead of the SENG runtime
is 1578.03 ms, i.e. it adds about 182 % overhead on top of
the Graphene-SGX initialization under default conﬁguration.
However, the vast majority of this overhead stems from two
steps: (i) the init routine of the OpenSSL library (710.98 ms)
and (ii) the IAS communication (639.05 ms). The high
OpenSSL startup time is partially attributable to the default
seeding of the random number generator. It could be reduced
by switching to the RDRAND engine to approach a setup time of
867.05 ms, which is comparable to the default LibOS time (a).
As discussed in Sec. 6.1, the remote attestation could be han-
dled by an internal AS server with caching support instead.
Thus, the total startup time could be further reduced to ideally
228 ms, i.e. about 26 % of the default LibOS time (a).
We conclude that SENG adds a reasonable startup overhead
which could be optimized to become comparable to that under
reduced (b) or minimal (c) SENG runtime conﬁgurations.
9.5 Accelerating NGINX using SENG-SDK
We next revisit the NGINX results of Section 9.3 and show
that SENG performs signiﬁcantly better when replacing
Graphene-SGX with a faster primitive. SENG performed on
par with “pure” Graphene-SGX for NGINX with ∼15 k re-
quests per second, but got clearly outperformed by the native
baseline of ∼40 k (cf. Figure 7). To show that SENG can over-
come the bottleneck caused by Graphene-SGX, we dropped
the LibOS and instead ported NGINX11 to our SENG-SDK.
We ported only NGINX’s platform-speciﬁc code to preserve
comparability with previous results and added about 1100
lines of code for enclave setup and missing syscalls.
Figure 7 shows that SENG-SDK (“SENG-sdk”) reaches
∼36 k request per second with a per-reply latency of 1.5–
2.0 ms. SENG-SDK signiﬁcantly outperforms the Graphene-
based SENG runtime by factor 2.4 and reaches up to 90 %
of native performance. Compared to Graphene-SGX, SENG-
SDK provides more efﬁcient OCALL interfaces tailored for
the DTLS tunnel I/O and beneﬁts from the more lightweight
abstractions of Intel’s SGX SDK. However, note that SENG-
SDK looses legacy support and drop-in deployment (AR1).
We conclude that SENG can signiﬁcantly beneﬁt from per-
formance improvements of the underlying primitives, letting
it handle complex apps like NGINX with small overhead.
Our rudimentary port to SDK-SENG achieved 90 % of na-
tive performance and could be further improved by adding
NGINX-speciﬁc optimizations and an efﬁcient ﬁle system
shield. We are conﬁdent that the SENG runtime will likewise
beneﬁt from upcoming improvements of Graphene-SGX.
9.6 Server Scalability and Memory Overhead
We now discuss how the SENG server scales w.r.t. the num-
ber of clients and connections. The server has a small static
memory footprint of which the TUN interface accounts for
at most 750 kB under a full transmit queue12. The dynamic
memory overhead is largely determined by the send and re-
ceive buffers of the per-enclave DTLS tunnels. In common
settings, these would consume 8 KiB to 256 KiB per enclave
and direction, plus about 32 KiB for the SSL frame buffer, but
can be tuned to lower values. When considering the upper
range, this still means that we could handle about 2000 clients
per 1 GiB memory, with a huge potential for swapping large
parts of the typically unused buffers. For SOCKS servers, the
memory overhead increases with the number of connections
they have to perform on behalf of the clients. In contrast, the
SENG server is oblivious to the tunneled client connections
and therefore faces constant per-client overhead.
The limiting performance bottleneck of the SENG server
is the computational overhead of de- and encryption of DTLS
packets and the general network I/O. In our experiments, the
10default: 256MB size, 32MB heap, 4MB stack, 4 threads; reduced: 4MB
heap, 256KB stack, 2 threads; min.: 128MB size + reduced; all: 2 rpc threads
11in single-process mode
12default length stores maximum 500 packets
USENIX Association
29th USENIX Security Symposium    767
server easily coped with any client bandwidth, and given its
1 Gbps network card we cannot test higher loads. The CPU uti-
lization (around 65% on a single core, including waiting time)
at maximum bandwidths suggests that the non-optimized
server implementation will scale to 6+ Gbps on our hardware.
This performance could be further optimized by improving
the server code (e.g., using vectored sending, replacing the
tunnel device with DPDK kernel NICs, etc.).
10 Discussion
We conclude with a discussion on upcoming improvements
and directions to overcome limitations of our prototype.
Overcoming Memory Limitations of Enclaves. TEEs
like SGX face two common challenges in practice: (i) per-
formance impacts of context switches and (ii) limited secure
memory. In Section 9.1 and Section 9.5, we have already
presented that careful switchless designs and improvements