have been collected over a few years, it does not introduce a bias in
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Fass et al.
our experiments. Still, we discuss to what extent an attacker could
make benign and malicious features similar in Section 4.2.
3.3 Analysis of Closely Related Detectors
Several systems already combined differences at a lexical or an AST
level with off-the-shelf supervised machine learning algorithms
to distinguish malicious from benign JavaScript. In this section,
we focus on Cujo [45], JaSt [17] and Zozzle [12], as they are–
to the best of our knowledge–the most closely related works to
our token- and AST-based approaches. After explaining the better
overall detection rates of JStap compared to the previous systems,
we focus on combining their predictions.
3.3.1 Presentation of Cujo, Zozzle and JaSt. In 2010, Rieck et al.
developed Cujo [45], which builds n-gram features from JavaScript
lexical units, before using an SVM classifier for an accurate mal-
ware detection. As the system is not open source, we contacted the
authors who pointed us to the tokenizer they initially used [44]
and encouraged us to use the HashingVectorizer from Scikit-
learn [46] to map the extracted features to a corresponding vec-
tor space. In the original implementation, Cujo also leverages an
enhanced version of ADSandbox [14], which executes the code
associated with a webpage within the JavaScript interpreter Spider-
Monkey [40]. We contacted Dewald et al., who informed us that
ADSandbox is neither maintained nor running anymore. Since we
specifically focus on static JavaScript detectors in this paper, we
consider only the static part of Cujo. Also, we assume that our
reimplementation is functionally equivalent to the original one,
and for reproducibility, we make this system publicly available at
https://github.com/Aurore54F/lexical-jsdetector.
Curtsinger et al. implemented Zozzle [12], which combines the
extraction of features from the AST, as well as their corresponding
node value, with a Bayesian classification system to detect malicious
JavaScript. We approached the authors and asked for their code
or inputs, but did not get any response. Thus, we reimplemented
the system with automatic features selection, 1-level features, and
naive Bayes, based on the information from the paper. Similarly
to Cujo, Zozzle also has a dynamic part, to first hook into the
JavaScript engine of a browser to get the deobfuscated version of
the code. As previously, we reimplemented the static part of the
tool and make it publicly available at https://github.com/Aurore54F/
syntactic-jsdetector.
Last but not least, with JaSt, Fass et al. [17] leveraged n-grams
from an AST traversal to detect malicious JavaScript. As the system
is open source [3], we directly used it for the comparisons.
3.3.2 Benefits of JStap’s Lexical and AST-Based Modules. Concep-
tually the ngrams module of JStap, working at the tokens level, is
identical to Cujo. In contrast, we rely on Esprima for tokenization,
use 4-grams instead of 3-grams, do not consider all features, but
select them with a χ2 test, and use a different classifier (random
forest). For Zozzle, the value module of JStap, working at the
AST level is conceptually equivalent. Still, we consider all nodes
from the AST (and not only expressions and variable declarations),
a different confidence for the χ2 test, and random forest instead of
naive Bayes. As for JaSt, it is conceptually identical to the ngrams
module of JStap, working at the AST level. Still, we do not simplify
Figure 6: Accuracy comparison between related work and
our improved corresponding implementations
the syntactic units returned by the parser but perform a χ2 test to
reduce the size of our feature space.
3.3.3 Comparison with Cujo, Zozzle and JaSt. Overall, the three
corresponding modules of JStap have a better detection rate com-
pared to Cujo, Zozzle, and JaSt (Figure 6). Specifically, JStap has
a higher TPR than Cujo (98.73% compared to 98.61%) and a higher
TNR (99.4% and 97.9%), meaning that we classify 2,051 files more
accurately than Cujo. Our implementation performs better due to
the differences in the implementation mentioned in Section 3.3.2.
In particular, 4-grams performed better than 3-grams and random
forest better than SVM during our hyper-parameters selection pro-
cess (Section 2.2.2, Section 2.3). Also, we hypothesize that Cujo
performed differently than in its original paper [45] (with a FPR of
2.0E-3% and 5.6% FNR) mainly due to our malicious dataset, com-
prising 131,448 samples from different sources, compared to 609 for
Cujo. This way, our reimplementation recognizes more malicious
JavaScript than initially, but to the detriment of benign samples.
We observe a similar trend for Zozzle, which has a significantly
lower TPR (94.27% and 98.44%) and TNR (97.35% to 99.54%) than
the corresponding JStap’s module. As before, we mentioned the
differences in the implementation in Section 3.3.2. We also assume
that Zozzle performs differently than in its original paper [12]
(with a FPR of 3.1E-4% and 9.2% FNR) due to our malicious dataset.
Specifically, they considered only 919 malicious samples and clearly
stated in 2011 that “relatively few identifier-renaming schemes
[were] being employed by attackers”, which is not the case anymore,
where malicious samples are heavily obfuscated (as observed during
the manual analysis from Section 3.1.1). While it might be unfair to
consider only the static parts of Cujo and Zozzle to compare them
with the corresponding JStap’s modules–as their accuracy might
also stem from their dynamic components–we are focussing here
on comparing several static analysis systems, working at different
abstract levels (and we did not have the original systems to check
the added value, or not, of their dynamic components).
Finally, JaSt has a slightly higher TPR than JStap (99.71% and
99.11%) but in compensation a significantly lower TNR (97.86% and
99.62%), meaning that we classify on average 1,592.8 files more
accurately than JaSt. We believe that JStap has a higher overall de-
tection accuracy than JaSt mainly due to us not simplifying the syn-
tactic units returned by the parser. As Fass et al. grouped units with
the same abstract meaning they considered, e.g., ForStatement
and IfStatement as a Statement node, therefore losing context
information, as explained in Section 3.2.1. As we used the open
source JaSt version, we assume that our dataset, which is bigger
JStap: A Static Pre-Filter for Malicious JavaScript Detection
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Table 5: Analysis of the detection accuracy when Cujo, JaSt
and Zozzle made different predictions
Approach
ngrams_tokens
ngrams_ast
ngrams_cfg
ngrams_pdg-dfg
ngrams_pdg
Cujo
JaSt
TPR TNR Accuracy Approach
0.87
0.89
0.91
0.9
0.9
0.78
0.97
0.91
0.93
0.92
0.91
0.92
0.71 Zozzle
0.84
value_tokens
value_ast
value_cfg
value_pdg-dfg
value_pdg
0.94
0.96
0.93
0.92
0.93
0.64
0.73
TPR TNR Accuracy
0.89
0.93
0.9
0.84
0.81
0.9
0.89
0.88
0.89
0.88
0.14
0.42
0.96
0.96
0.7
0.88
0.9
0.66
and contains more diverse JavaScript than in the original paper [17],
is responsible for the different rates we got compared to the paper’s
(5.2E-3 FPR and 5.4E-3 FNR), which is in line with the assumptions
we made for Cujo and Zozzle.
3.3.4 Combination of Related Work Predictions. Next, we studied
the detection accuracy of JStap’s different modules on samples for
which Cujo, Zozzle and JaSt made different predictions. Due to
their different classification results, these samples may be trying
to evade detection. Specifically, the three related work classifiers
considered made different predictions for 17,178.6 samples (6.78%
of our dataset7), 7,943.4 of which are malicious.
Table 5 presents the detection accuracy of all JStap’s modules,
and of Cujo, Zozzle and JaSt, on such samples. First, our ngrams
approach at the tokens level performs better than Cujo also in this
configuration, with both a significantly higher TPR (87% compared
to 78%) and TNR (94% and 64%). Similarly, we outperform Zozzle by
correctly classifying over twice as many samples with JStap’s value
AST-based module (overall detection accuracy of 90% compared to
42%). Still, these results have to be taken with a grain of salt, as
we tested the classifiers on samples likely to try to evade detection.
As a matter of fact, in Section 3.3.3, Zozzle did not perform as
well as Cujo and JaSt. In particular, it reported almost 7,000 false-
negatives (FNR of 5.7%) compared to 348 for JaSt and 1,675 for Cujo.
Therefore, and out of the 7,943.4 malicious samples considered here,
at least 5,300 are initial false negatives from Zozzle, meaning that
its TPR could not be over 33%. Finally, and as previously, JaSt has
a higher TPR than our ngrams AST-based approach (97% compared
to 89%), but at the same time significantly fewer true-negatives (73%
compared to 96%), meaning that JStap has a higher overall detection
accuracy, classifying 1,550 files more accurately than JaSt.
As for the remaining JStap’s modules, they are also impacted by
these samples likely to be evasive, with a mean accuracy between
81% (value CFG, otherwise from 89%) and 93% (value tokens),
compared to over 97.55% (value CFG) and up to 99.44% (value
tokens) in Section 3.2 on a standard dataset. Still, all JStap’s modules
significantly outperform Cujo, Zozzle and JaSt.
3.4 Combining Modules for a Higher Accuracy
JStap is a modular JavaScript static classification system for which
the user can choose the type of analysis (ngrams or value), as well
as its level (tokens, AST, CFG, PDG-DFG and PDG). Even though all
approaches (except value CFG) have an overall detection accuracy
between 98.9% and 99.44% (Section 3.2), thereby outperforming
7We consider here only the samples, which are not in the model, thus 253,216
related-work detectors trained and tested on the same datasets
(Section 3.3), they can still be combined for an even better detection
rate. In the following sections, we discuss the JStap’s modules
we combined, as well as the detection accuracy on the resulting
combination, using majority predictions voting, before focussing
on the detectors’ confidence for a given prediction.
Selection of JStap’s Modules for Predictions Combination. For
3.4.1
the combination process, we chose the value token- and ngrams
AST-based approaches, which perform particularly well (Section 3.2)
and use different features that do not overlap. As a matter of fact,
the former leverages the lexical structure of a JavaScript file and
combines each extracted token with its corresponding value, while
the latter rests upon the AST traversal and an n-gram combination
of the traversed nodes, for an accurate malicious JavaScript detec-
tion. As we need an odd detectors’ number to perform majority
voting, we selected a third one. The PDG value approach comple-
ments the previous two systems, as it also uses new features, which
do not overlap with the previous ones. On the contrary, choosing
the CFG, PDG-DFG or PDG ngrams option would have overlapped
with the AST ngrams approach, while the PDG value module has
different features, due to the consideration of the nodes’ value. Also,
we wanted to strengthen our system with both control and data
flow information, hence the choice for the PDG.
3.4.2 Predictions With Majority Voting. We perform a combination
of the three selected systems (ngrams AST, value tokens and value
PDG), by choosing the prediction with the most votes, for a given
JavaScript input. Such a combination presents both a high TPR
of 99.2% and a TNR of 99.7%, representing an accuracy of 99.46%.
Still, when we considered each module separately in Section 3.2, we
had an approaching accuracy for the value tokens approach (best
module) with a detection rate of 99.44%. This means that combining
modules leads to a detection of 36 additional samples (0.015% of
our dataset), which we do not see as a major improvement.
Nevertheless, we also leveraged the combination of these three
modules to classify the 17,178.6 samples for which Cujo, Zozzle
and JaSt made different predictions (Section 3.3.4). This time, we
retain an accuracy of 93.47%, which is, again, better than the ngrams
AST-based and value token-based approaches (Table 5), which per-
formed best in this configuration. In particular, we detect on average
47.9 extra samples with the combination of modules than with the
value tokens approach, which is 1.3 times more samples than in
our standard dataset, where we have almost 15 times more sam-
ples. Therefore, combining modules brings a real added value when
classifying samples likely to be evasive. Similarly, this combination
process also recognizes 121.9 more samples than the ngrams AST
approach. In particular, the value token-based approach correctly
classified 74 extra samples compared to the ngrams AST variant
(0.43% of our evasive dataset), while only classifying 0.07% more of
our standard dataset, meaning that the difference in terms of detec-
tion rate between these two modules tends to increase on evasive
samples. Therefore, combining JStap modules always perform bet-
ter than each module separately, in particular on evasive samples.
In the case of such evasive samples, some modules may struggle
to classify them correctly, while combining modules significantly
limits the proportion of samples evading our system.
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Fass et al.
Table 6: JStap’s modules predictions combination
Table 7: Run-time to generate JStap’s code representations
Approach
Same predictions from the 3 modules
Majority voting on remaining (likely evasive)
TPR (%) TNR (%) Accuracy (%)
99.73
96.02
99.55