6.   native receive0(DatagramPacket packet);
7. + send(DatagramPacket packet, Taint packet_t) {
8. +    byte[] data = packet.getData();
9. +    Taint[] taints = packet.getTaints();
10. +    byte[] serialBytes = serialize(data, taints);
11. +    DatagramPacket wrappedPacket = 
new DatagramPacket(serialBytes);
12. +    send(wrappedPacket);
13. + }
14. + receive0(DatagramPacket packet, Taint packet_t) {
15. +    DatagramPacket wrappedPacket = 
new DatagramPacket();
16. +    receive0(wrappedPacket);
17. +    packet.setData(wrappedPacket.getData());
18. +    packet.setTaints(wrappedPacket.getTaints());
19. + }
Fig. 7.
Instrumentation for packet oriented methods.
methods, packet oriented methods and direct buffer oriented
methods. They are instrumented in three different ways.
Type 1: Stream oriented methods. TCP message related
JNI methods are in this type. They use stream I/O methods to
read / write data. The message data in these methods is in the
type of byte array or the single byte. Thus, we can directly
use the instrumentation way shown in Figure 6 for them.
Type 2: Packet oriented methods. UDP message related
JNI methods belong to this type. They usually uses a packet
object to wrap the message data in. We must access into the
packet before serializing / deserializing the data and taint.
Figure 7 shows an example of instrumenting two UDP
methods send and receive0 in PlainDatagramSocketImpl. Line
1 - 4 shows DatagramPacket class, which stores the message
data in the ﬁeld data. After instrumentation, we add a ﬁeld
taints to store taints for every distinct byte data. Line 5 and
Line 6 are the original JNI methods. They use DatagramPacket
object rather than the byte array to store data. Line 7 - 13
shows the wrapper of send. First, we fetch out the data (Line 8)
and its taints (Line 9), and wrapped them in serialBytes. Then
we initiate a new packet object to carry the wrapped bytes
(Line 11) and send them out by the original JNI method (Line
12). Note that we do not directly replace packet’s data ﬁeld by
serialized bytes, because packet may be used by the following
code. Changing its ﬁeld may affect the execution semantics.
For the receiver method, we create a new DatagramPacket
object (Line 15) to receive the full bytes (Line 16), and then
deserialize them into the data and taints. At last, they are
placed in the ﬁelds of the original parameter object.
Type 3: Direct buffer oriented methods. This type of
methods most commonly occurs in NIO communication which
uses DirectBuffer to store the message data. DirectBuffer is a
special class that manages a memory block out of Java heap. It
does not directly store an object or bytes carrying the message
data, but the data’s address in the physical memory. When a
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:27:27 UTC from IEEE Xplore.  Restrictions apply. 
551
// IOUtil.java
int writeFromNativeBuffer(ByteBuffer bb) {
bb.address
nd.write(fd, ((DirectBuffer)bb.address()), …);
}
// FileDispatcherImpl.java
native int write0(FileDescriptor fd, long address, …);
long address
Fig. 8. Direct buffer oriented methods only provide the memory address of
data rather than the data itself and its taints.
PARTIAL INSTRUMENTED METHODS AND THEIR TYPES
TABLE I
Class
SocketInputStream
SocketOutputStream
LinuxVirtualMachine
PlainDatagramSocket
DirectByteBuffer
IOUtil
WindowsAsynchronous-
SocketChannelImpl
Method
socketRead0
socketWrite0
writeFromNativeBuffer
readIntoNativeBuffer
implRead
implWrite
receive0
read
write
send
peek
get
put
Type
1
1
1
1
2
2
1
3
3
3
3
3
3
node sends out data by DirectBuffer, it directly accesses the
data by the memory address, rather than copying a heap object
to OS by network related native method. This helps improve
I/O efﬁciency in Java programs.
However, the use of DirectBuffer hinders us directly fetch-
ing the data and taints in the parameters of a JNI method
invocation. Taking the code in Figure 8 as the example, we
can only ﬁnd a long format address in the native method
write0. Thus, the previous instrumentation ways cannot work
here. Then we have a look on the caller of the method,
i.e., writeFromNativeBuffer in the class IOUtil. Here is a
DirectBuffer object but still no available taint information,
because DirectBuffer does not store data objects as well as
taints associated with the data.
To solve this problem, we ﬁrst modify DirectBuffer class.
We add a taint array ﬁeld in DirectBuffer to store all taints of
the data, and modify every put / get method in the class, so
that when the developer writes / reads the data to / from the
memory, the associated taint can be stored to / fetched from
the added taint array ﬁeld.
Second, we instrument all callers of direct buffer oriented
native methods such as wrtieFromNativeBuffer in Figure 8. It
is similar to what we do for packet oriented methods. For write
methods, we allocate a new larger DirectBuffer, and fetch the
data out from the original DirectBuffer. Then we serialize the
data with its taints stored in the added taint array ﬁeld, and
write the serialized results into the new buffer. Finally, we
invoke the native method on the new buffer to send out the data
and taints. Correspondingly, we allocate a larger DirectBuffer
for read methods, and read the serialized data and taints by
the buffer. Then we deserialize and put them into the original
DirectBuffer object.
Finally, we instrument 23 methods in total. Table I shows
a portion.
D. DisTA Runtime
After instrumentation, taints can propagate in distributed
systems at run time. When a taint propagates within the single
node, we denote it as a local taint. When it is transferred
between nodes, it is a global taint. We design a component
named Taint Map to store and transfer all global taints. Taint
Map is an independent process which can communicate with
all nodes, and maintain a map structure to store all global
taints and their Global IDs.
Figure 9 shows how the Taint Map processes global taints.
There are two bytes on Node 1, i.e., b1 and b2, to be sent to
Node 2, while Node 2 only receives one byte data b1. Both b1
and b2 are tainted by t1. Along with b1, t1 must be transferred
to Node 2. The whole transfer process can be divided into 5
steps. 1 Node 1 sends the taint t1 to Taint Map and request
a Global ID. Taint Map allocates a unique number as Global
ID for every global taint, e.g., 1 for t1. 2 Node 1 receives
the Global ID, and store it in its local taint storage, which is
a tree that we introduced in Section II-B. Note that, since we
have got the Global ID for t1, and b2’s taint is also t1, Node
1 does not need to request a Global ID again if it sends b2
out later. 3 Node 1 serializes every data byte with its taint
into a byte array, and transfers it to Node2. Here we do not
directly serialize the taint, but put the Global ID after the data
byte and transfer it. 4 Node 2 receives the data and the taint’s
Global ID, and requests to Taint Map to get the taint. 5 Node
2 receives the taint, and store it in its local taint storage.
1) Taint Tag Design: Compared the taint
tag structure
used by Phosphor, i.e.,  introduced in Sec-
tion II-B, we adds two additional ﬁelds, i.e., LocalID and
GlobalID. Thus,
i.e.,
.
tag of DisTA is a quad,
the taint
LocalID is used to solve the problem of tag conﬂict. In
distributed systems, different nodes can run the same code.
That means, when we set some variables in the system as
the taint source points, a same variable can be tainted with
the same tag value on different nodes. If a tag propagates to
another node and the node has already generated the same tag,
they can be in conﬂict. Take Figure 9 as the example. If N ode2
generates an a tag before communicating with N ode1, then
this tag will be in conﬂict with the a tag from N ode1. If they
are not distinguished correctly, it will make the taint tracking
imprecise since we do not know where the taint is from. To
solve this problem, we add a ﬁeld Local ID for every taint tag.
It consists of node’s IP and JVM’s process ID. Thus, nodes
are aware of where tags are generated and distinguish them
even when they have the same tag value.
GlobalID is used to mark every unique taint in the inter-
node taint tracking process. It is set as zero when a taint tag is
generated at one node, and assigned a unique positive integer
by Taint Map when transferred to another node.
2) Taint Map: We design Taint Map to solve two problems
large bandwidth usage when
in inter-node taint
tracking:
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:27:27 UTC from IEEE Xplore.  Restrictions apply. 
552
Node 1
b1
b2
Local Taints
② Store GID
Node 2
Local Taints
t1
t1
① Request GID
b1
1
b2
1
b1
1
③ Transfer
GID Global Taint
1
t1
Taint Map
b1
t1
⑤ Store Taint
④ Request Taint
Fig. 9. Taints go through 5 steps in inter-node taint tracking.
repeatably transferring serialized taints, and mismatched se-
rialized taint length between senders and receivers.
Large bandwidth usage. A serialized taint with one tag
can be over 200 bytes. What’s worse, a taint can have multiple
tags. As the number of tags increases, the length of the byte
array increases linearly. The serialized bytes array can cause
far more than 200X bandwidth overhead.
Mismatched serialized taint length. In message-passing,
the receiver will allocate a ﬁxed length empty byte array for
storing the data bytes received. However, the length of the
empty byte array does not always exactly equal the number of
bytes sent by the sender. As Figure 9 shows, N ode1 sends two
bytes b1 and b2, while N ode2 only receives b1. To receive the
serialized taint simultaneously, the length of the allocated byte
array must be enlarged. Note that the length of the serialized
taint is not ﬁxed. Thus, we cannot simply enlarge the byte
array space by a ﬁxed amount. It means that the receiver
probably cannot receive the full serialized taint bytes, which
can fail the following deserialization step.
With Taint Map and Global ID, we can transfer taints in a
ﬁxed length byte array. Thus, we can enlarge the allocated byte
array on the receiver and do not need to worry about receiving
an incomplete taint. On the other hand, every node only needs
to communicate with Taint Map to transfer every global taint
for only one time, and nodes only transfer serialized Global
ID to each other. Thus, the bandwidth overhead caused by
transferring global taints is acceptable, which depends on the
length of the Global ID.
On the other hand, Taint Map may become bottleneck,
since it runs as a single-point component that can be accessed
by all nodes to request Global IDs for global taints. As the
number of the global taints increases, the requests to Taint
Map for allocating Global IDs also increase. The limit on the
throughput of Taint Map may cause performance degradation
in inter-node taint tracking. However, our evaluation results
in Section V-F shows that the performance degradation is
acceptable.
IV. IMPLEMENTATION
DisTA is implemented in 2,045 lines of code (LOC). Among
them, 1,591 LOC are instrumentation related code. Most of
them are ASM [28] instructions. As mentioned above, we
instrument 23 methods. That means every method requires
69 LOC for instrumentation on average. To perform runtime
instrumentation, we deploy the instrumentation code as a Java
agent [31] attached with the target system.
Taint Map is implemented in 202 LOC. It is a simple
map structure which can communicate with all nodes. In
practice, Taint Map can be replaced by other mature K-V store
systems such as ZooKeeper [12] and etcd [32] to improve its
performance. On the other hand, it can be improved by some
reliable designs, e.g., adding a standby node to handle with the
single point failure. In this work, we only make the simplest
implementation, since DisTA is designed for in-house analysis
and testing for now, but not for production.
Besides, we modify Phosphor in 252 LOC to implement
our global taint tag structure and support the inter-node taint
serialization / deserialization.
V. EVALUATION
Our evaluation addresses the following research questions:
RQ1: Is DisTA sound and precise in inter-node taint track-
ing?
RQ2: Is DisTA easy to use?
RQ3: How is DisTA’s performance overhead?
To answer the above questions, we implement 30 test cases
with different commonly used communication protocols and
APIs (Section V-A) as the micro benchmark, and collect
several network communication scenarios in 5 real-world
distributed systems (Section V-B) for evaluation.
A. Micro Benchmark
As shown in Table II, we implement 30 test cases for differ-
ent network communication APIs and protocols. All of them
are frequently used in Java network communication. Three
cases come from a third-party network application framework
Netty [33], and the others use JRE standard APIs. Among
them, JRE Socket has multiple test cases, because users can
invoke different I/O interfaces in different stream classes to
read / write different kinds of data. For example, writeObject
in class ObjectOutputStream is specially for writing an object
to the stream. Other communication ways do not have multiple
cases, since they only have the single I/O implementation.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:27:27 UTC from IEEE Xplore.  Restrictions apply. 
553
TABLE II
MICRO BENCHMARK
Name
JRE Socket
JRE Datagram
JRE SocketChannel
JRE DatagramChannel
JRE AsyncSocketChannel
JRE HTTP
Netty Socket
Netty DatagramSocket
Netty HTTP
Total
Description
JRE standard TCP
JRE standard UDP
JRE NIO TCP
JRE NIO UDP
JRE AIO
JRE HTTP
3rd-party TCP
3rd-party UDP
3rd-party HTTP
# cases
22
1
1
1
1
1
1
1
1
30
Source point
Node 1
Data1
Node 2
Data2
Source point
send()
receive()
receive()
send()
Sink point
check()
Fig. 10. Taint tracking scenario for micro benchmark.
Workloads. We design the workloads for the micro bench-
mark as shown in Figure 10. First, N ode1 sends Data to
N ode2. N ode2 receives it and combines it with another Data.
Then the combined data is sent back to N ode1. Finally,