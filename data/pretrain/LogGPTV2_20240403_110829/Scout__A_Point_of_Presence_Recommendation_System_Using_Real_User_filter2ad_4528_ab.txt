### Problem Formulation and Complexity
The optimization problem can be formulated as follows:
\[
\max_{\{p_1, \ldots, p_L\} \subset P} \left( \max \left(0, \max_{p \in \{p_1, \ldots, p_L\}} \Delta(p, I) \right) \right)^q q_I
\]
where \( |P| \) is the number of Point of Presence (PoP) candidates. The complexity of solving this optimization problem is \( O\left(\binom{|P|}{L}\right) \). When \( |P| \) and \( L \) are large, solving this problem becomes computationally expensive. However, the impact scores of all combinations of \( \{p_1, \ldots, p_L\} \) can be computed in parallel. In this paper, we use the Map-Reduce infrastructure for parallelization. The mappers generate all possible combinations, and the reducers compute the impact score for each combination. Finally, another Map-Reduce step sorts and generates the top-K ranked list.

### Recommending PoPs with Other Metrics
We have been using the total page download time improvement as the primary metric to recommend new PoPs. However, web sites often consider other downstream metrics that are impacted by site speed improvements, such as user page views, engagement, or revenue. Here, we illustrate the method using the total number of user page views as an example. For each region \( I \), suppose from data analysis we can learn \( f_I(\Delta(p, I)) \), which is the rate of increase in the number of page views if the median page download time improvement is \( \Delta(p, I) \) for region \( I \). The impact on the total number of page views on the site can then be defined as:
\[
PV(p) = \sum_{I} f_I(\Delta(p, I)) q_I
\]
Examples of such functions can be seen in Figure 3.

### Experimental Results
In this section, we present experimental results to measure the predictive performance of our site speed models and the performance of our PoP recommendation algorithm. We also consider using business metrics such as the number of page views on the site to recommend new PoPs, by studying the gains in these metrics given the predicted site speed improvements from our data.

#### Evaluation Strategy
An ideal evaluation approach would involve obtaining top recommendations from each method, installing PoPs at the recommended locations, and measuring the site performance gain. However, installing a PoP just for experimentation is usually not practical and can be very expensive. Therefore, our evaluation primarily focuses on the accuracy of the site speed prediction model, which is described in Section 3.2. The PoP selection method described in Sections 2.2 and 2.3, given the site speed performance model, is straightforward and does not require extensive evaluation.

#### Data
We use a random sample of Real User Monitoring (RUM) data collected from a major social network site to train the site speed prediction model. The dataset contains 4 million samples of user visits from June 3, 2014, to June 9, 2014. We randomly split the data into a 50:50 training and test set. The models are estimated using the training data, and prediction accuracy is evaluated using the test data.

Our candidate set for new PoP recommendations includes around 400 facility names worldwide from peeringDB, each with a list of available peering Autonomous System Numbers (ASNs). We use the same period of data that the site speed prediction model is trained with to recommend the new PoP facility locations. In Section 2.4, we also consider using other user engagement metrics, such as the user's monthly number of page views for PoP recommendations. To build the relationship between site speed and user page views (\( f_I(\Delta(p, I)) \)), we used a random sample of the data from the entire month of June 2014.

#### Predictive Performance of Site Speed Model
We show the predictive performance of the total page download time for the statistical models described in Section 2, with the evaluation metric being the prediction error rate of the median page download time. Specifically, for each region \( I \) and PoP \( p \):
\[
\text{error}(I, p) = \frac{\left| \text{Median}\{\hat{y}_{i,p}, \forall i \in I\} - \text{Median}\{y_{i,p}, \forall i \in I\} \right|}{\text{Median}\{y_{i,p}, \forall i \in I\}}
\]

**Quantile Regression vs. Ordinary Least Squares**
To predict the median page download time, our experiments show that quantile regression is a better choice compared to linear regression using ordinary least squares. In Figure 2, we show the performance for both approaches in terms of the prediction error defined in Equation (7), where each circle indicates a geographical region, and the size of the circle indicates the relative sample size of the region. The color of the circle shows which PoP the region was routed to. There were four PoPs at the time of the analysis. It is clear that the prediction error of the median page download time is significantly smaller in the case of quantile regression for the major regions: all the big circles are above the y = x line. This is mainly due to the fact that the distribution of total page download time still has a heavy right tail even after taking a logarithm transformation, so the mean prediction tends to be noisier than the median prediction. Additionally, the prediction from the quantile regression model is often lower than 5%, which provides a good basis for our PoP recommendation algorithm to work well.

#### PoP Recommendation Results
In this section, we describe our experiments for recommending new PoPs based on the prediction model using quantile regression.

**Recommending One PoP**
We follow the approach described in Section 2.2 to rank PoP candidates based on their impact scores, considering both the traffic for each geographical region and the site speed improvement for such regions. Table 1 lists the top 8 recommended PoP facilities given the existing PoPs for the social network site at the time the study was executed. It is interesting to see that the top 3 recommended facility locations are all in India, due to the fact that at the time this study was executed, no PoP existed in this region while its traffic to the site is quite high.

**Recommending Multiple PoPs Simultaneously**
Table 2 shows the top-ranked sets of PoP facilities obtained from applying the approach described in Section 2.3 with \( L = 4 \). It is interesting to observe that for all three sets, the four locations now scatter around the world, with one in Asia, one in Oceania, and two in Europe (Places such as Bucharest and Sofia are closer to the Middle East, while Paris mainly serves Europe). It is also interesting to see that Sydney and Paris did not even show up in Table 1.

**Recommending PoPs with Other Metrics**
We first describe our approach to learn \( f_I(\Delta(p, I)) \), which is the rate of increase in the number of page views if the median page download time improvement is \( \Delta(p, I) \) for region \( I \). Note that naively looking at the marginal relationship between site speed and the number of page views can be misleading, as many confounding factors need to be adjusted. Hence, we apply a stratification method to learn the relationship:
1. Construct user segments based on confounding factors such as geographical region, number of connections, etc.
2. For each user segment, estimate a smoothed curve of the number of page views versus total page download time using locally weighted scatterplot smoothing (LOWESS).
3. An overall curve of page download time improvement versus the increase in the number of page views is then obtained by aggregating the curves according to the traffic of each user segment.

Figure 3 shows the learned relationship for several geographical regions. It is interesting to observe a significant difference in the slopes when comparing regions such as Great Britain and New York. We show the top-ranked PoPs from the perspective of impact on the number of page views in Table 3. Comparing to Table 1, more European locations such as Manchester show up in the list, as they have a higher predicted impact on the number of page views.

### Conclusion
In this paper, we proposed Scout, a general-purpose Point of Presence (PoP) recommendation system using statistical modeling of the total page download time on Real User Monitoring (RUM) data. It has been driving the selection of PoPs for a major social network company since its development. Our empirical experiments on millions of real user data points obtained from a large social network show very good performance, with prediction errors lower than 5% for most regions. We have further extended the work from purely using site speed performance as the metric to other business metrics such as the total number of page views.

### Acknowledgments
We are grateful to Samir R. Das for his valuable feedback on an earlier draft of this paper. We would also like to thank the anonymous reviewers for their insightful comments.

### References
1. Brutlag, J.: Speed matters for Google Web Search. Google, June 2009.
2. www.peeringdb.com
3. Dabek, F., Cox, R., Kaashoek, F., Morris, R.: Vivaldi: A decentralized network coordinate system. In: ACM SIGCOMM Computer Communication Review, vol. 34, pp. 15–26. ACM (2004).
4. Dellaert, B.G., Kahn, B.E.: How tolerable is delay?: Consumers’ evaluations of Internet Web sites after waiting. J. Interact. Mark. 13(1), 41–54 (1999).
5. Francis, P., Jamin, S., Jin, C., Jin, Y., Raz, D., Shavitt, Y., Zhang, L.: IDMaps: A global Internet host distance estimation service. IEEE/ACM Trans. Network. 9(5), 525–540 (2001).
6. Gummadi, K.P., Saroiu, S., Gribble, S.D.: King: Estimating latency between arbitrary Internet end hosts. In: Proceedings of the 2nd ACM SIGCOMM Workshop on Internet Measurement, pp. 5–18. ACM (2002).
7. Hastie, T., Tibshirani, R., Friedman, J., Hastie, T., Friedman, J., Tibshirani, R.: The elements of statistical learning, vol. 2. Springer, New York (2009).
8. Iyengar, A.K., Squillante, M.S., Zhang, L.: Analysis and characterization of large-scale Web server access patterns and performance. World Wide Web 2(1–2), 85–100 (1999).
9. Koenker, R.: Quantile Regression. Cambridge University Press, Cambridge (2005).
10. Krishnan, R., Madhyastha, H.V., Srinivasan, S., Jain, S., Krishnamurthy, A., Anderson, T., Gao, J.: Moving beyond end-to-end path information to optimize CDN performance. In: Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement Conference, pp. 190–201. ACM (2009).
11. Madhyastha, H.V., Anderson, T., Krishnamurthy, A., Spring, N., Venkataramani, A.: A structural approach to latency prediction. In: Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement, pp. 99–104. ACM (2006).
12. Maheshwari, R.: How LinkedIn used PoPs and RUM to make dynamic content download 25% faster. LinkedIn Engineering Blog (2014).
13. Ramsay, J., Barbesi, A., Preece, J.: A psychological investigation of long retrieval times on the World Wide Web. Interact. Comput. 10(1), 77–86 (1998).
14. Sears, A., Jacko, J.A., Borella, M.S.: Internet delay effects: How users perceive quality, organization, and ease of use of information. In: CHI 1997 Extended Abstracts on Human Factors in Computing Systems, pp. 353–354. ACM (1997).
15. Squillante, M.S., Yao, D.D., Zhang, L.: Web traffic modeling and web server performance analysis. ACM SIGMETRICS Perform. Eval. Rev. 27(3), 24–27 (1999). IBM TJ Watson Research Center.