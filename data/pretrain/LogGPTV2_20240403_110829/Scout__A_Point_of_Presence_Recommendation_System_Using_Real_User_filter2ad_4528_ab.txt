problem to solve becomes
(cid:2)
I
max
{p1,...,pL}⊂P
max(0, max
p∈{p1,...,pL} Δ(p, I ))qI
(5)
(cid:5)
(cid:4)|P|
L, where |P|
The complexity of solving the optimization problem in (5) is
is the number of PoP candidates. When |P| and L are large, it will be com-
putationally expensive to solve (5). However, note that the impact scores of all
combinations of {p1, . . . , pL} can be obtained in parallel. In this paper we use
the Map-Reduce infrastructure for the parallelization, where the mappers gen-
erate all the possible combinations and the reducers compute the impact score
for each combination, and ﬁnally another Map-Reduce step to sort and generate
the top-K ranked list.
L
2.4 Recommending PoPs with Other Metrics
We have been using the total page download time improvement as the metric
to recommend new PoPs; however, web sites often look at other downstream
metrics impacted due to improvement in site speed, so sometimes it is desirable
to recommend PoPs based on other business metrics, such as gains in user page
views, engagement, or revenue. Here we illustrate the method by using the total
number of user page views as an example. For each region I , suppose from data
analysis we can learn f I (Δ(p, I )), which is the rate of increase of the number
of page views if median page download time improvement is Δ(p, I ) for region
I , where Δ(p, I ) is deﬁned earlier in Eq. (3). Examples of such functions can
be seen in Fig. 3. The impact to total number of page views on the site can then
be deﬁned as
(cid:2)
P V (p) =
f I (Δ(p, I ))qI ,
(6)
I
212
Y. Yang et al.
3 Experimental Results
In this section we show experimental results to measure predictive performance
of our site speed models and the performance of our PoP recommendation algo-
rithm. We also consider using business metrics such as number of page views on
the site to recommend new PoPs, by studying the gains in such metrics given
the predicted site speed improvements from our data.
Notes of Our Evaluation Strategy. An ideal evaluation approach to evaluate
multiple PoP selection methods is to obtain top recommendations from each
method and install PoPs at the recommended locations, and measure the site
performance gain. However, installing a PoP just for experimentation is usually
not practical and can be really expensive. Since the main challenge is to obtain
an accurate site speed prediction model which can predict the performance gain
after installing a PoP at a certain location, our evaluation mainly focuses on
this aspect, which is described in Sect. 3.2. The PoP selection method described
in Sects. 2.2 and 2.3 given the site speed performance model is quite straight-
forward, hence not much evaluation is needed there.
3.1 Our Data
We use a random sample of RUM data collected from a major social network site
to train the site speed prediction model. The data set contains 4 million samples
of user visits occurred during June 3, 2014 to June 9, 2014. We randomly do a
50:50 split of the data into training data and test sets. The models are estimated
using training data and prediction accuracy is evaluated using test data.
Our candidate set for the new PoP recommendation includes around 400
facility names around of the world from peeringDB, each of which has a list
of available peering ASNs. We use the same period of data that the site speed
prediction model is trained with to recommend the new PoP facility locations.
In Sect. 2.4 we also consider using other user engagement metrics such as
user’s monthly number of page views for the PoP recommendation. To build the
relationship between site speed and user page views (i.e. f I (Δ(p, I ))), we used
a random sample of the data in the entire month of June 2014.
3.2 Predictive Performance of Site Speed Model
We show the predictive performance of the total page download time for the
statistical models described in Sect. 2, with the evaluation metric being the pre-
diction error rate of the median page download time. Speciﬁcally, for each region
I and PoP p,
error(I , p) =
|Median{ˆyi,p,∀i ∈ I } − Median{yi,p,∀i ∈ I }|
Median{yi,p,∀i ∈ I }
(7)
Quantile Regression vs. Ordinary Least Squares. To predict the median
page download time, our experiments show that quantile regression is a better
Scout: A PoP Recommendation System Using RUM Data
213
choice versus linear regression using ordinary least squares. In Fig. 2 we show
the performance for both approaches in terms of the prediction error deﬁned
in Eq. (7), where each circle indicates a geographical region, and the size of the
circle indicates the relative sample size of the region. The color of the circle
shows which PoP the region was routed to. There are in total 4 PoP at the
time of the analysis. It is clear that the prediction error of the median page
download time is signiﬁcantly smaller in the case of quantile regression for the
major regions: all the big circles are above the y = x line. This is mainly caused
by the fact that the distribution of total page download time still has a heavy
right tail even after taking logarithm transformation, so the mean prediction
tends to be noisier than the median prediction. Also note that the prediction
from the quantile regression model is often lower than 5 %, which provides a
good basis for our PoP recommendation algorithm to work well.
Fig. 2. Prediction error percentage of the median page download time comparing ordi-
nary least square (OLS) and quantile regression (QR). The prediction error percentage
in both axises are deﬁned in Eq. (7). Each circle indicates a geo region and the size
of the circle indicates the relative sample size of the region. The color of the circle
indicates which PoP the region is currently routed to. The black line is y = x.
3.3 PoP Recommendation Results
In this section we describe our experiments for recommending new PoPs, based
on the prediction model using quantile regression.
Recommending One PoP. We follow the approach described in Sect. 2.2 to
rank PoP candidates based on their impact scores, considering both the traﬃc
for each geographical region and the site speed improvement for such region.
Table 1 lists the top 8 recommended PoP facilities given the existing PoPs for
the social network site at the time the study was executed. It is interesting
214
Y. Yang et al.
Table 1. The top-ranked PoP recommendations if only one PoP is recommended.
IXP
TATA
TATA
Netmagic
Chennai
UAE-IX
Dubai
City
Country Impact score
Mumbai
Delhi
India
India
India
UAE
47.1
45.7
41.2
38.3
INTERLAN Bucharest Romania 27.1
BIX.BG
UA-IX
BiX
Soﬁa
Kiev
Bulgaria 26.2
Ukraine
24.2
Bydapest Hungary 23.6
Table 2. The top-ranked set of PoPs with each set containing 4 recommendations.
Rank City
Country PoP impact Total impact
1
2
3
39.9
India
Mumbai
Sydney
Australia 20.5
Bucharest Romania 20.3
Paris
9.3
France
Mumbai
Sydney
Soﬁa
Paris
India
41.6
Australia 20.5
Bulgaria
France
8.9
18.52
India
40.1
Delhi
Sydney
Australia 20.5
Bucharest Romania 19.2
Paris
9.4
France
90.1
89.5
89.2
to see that the top 3 recommended facility locations are all in India, due to the
fact that at the time this study was executed, no PoP existed in this region while
its traﬃc to the site is quite high.
Recommending Multiple PoPs Simultaneously. Table 2 shows the top-
ranked sets of PoP facilities that was obtained from applying the approach
described in Sect. 2.3 with L = 4. It is interesting to observe that for all the
three sets, the four locations now scatter around the world, with one in Asia,
one in Oceanic, and two in Europe (Places such as Bucharest and Soﬁa are closer
to Middle East while Paris mainly serve for Europe). It is also interesting to see
that Sydney and Paris did not even show up in Table 1.
Recommending PoPs with Other Metrics. We ﬁrst describe our approach
to learn f I (Δ(p, I )), which is the rate of increase of the number of page views
if median page download time improvement is Δ(p, I ) for region I . Note that
Scout: A PoP Recommendation System Using RUM Data
215
Fig. 3. Page download time improvement vs. increase rate of number of page views
Table 3. Top-ranked PoP recommendations from impact on user page views. The page
view (PV) impact score is a rescaled number, proportional to P V (p) in Eq. (6).
IXP
TATA
TATA
UAE-IX
Netmagic
BNIX
City
Country PV impact
Mumbai
Delhi
Dubai
Chennai
Brussels
India
India
UAE
India
0.296
0.296
0.255
0.249
Belgium 0.241
France-IX
Paris
France
IXManchester Manchester UK
Edge-IX
UK
UK
0.237
0.232
0.232
naively looking at the marginal relationship between site speed and the num-
ber of page views can be misleading, since many confounding factors need to
be adjusted. Hence we apply a stratiﬁcation method to learn the relationship:
(1) construct user segments based on confounding factors such as geographi-
cal region, number of connections etc., (2) for each user segment, estimate a
smoothed curve of number of page views versus total page download time using
locally weighted scatterplot smoothing (lowess), (3) an overall curve of page
download time improvement versus increase of the number of page views is then
obtained by aggregating the curves according to the traﬃc of each user seg-
ment. Figure 3 shows the learned relationship for several geographical regions. It
is interesting to observe a signiﬁcant diﬀerence in the slopes when we compare
regions such as Great Britain and New York. We show the top-ranked PoPs from
216
Y. Yang et al.
the perspective of impact to the number of page views in Table 3. Comparing to
Table 1, more European locations such as Manchester show up in the list, since
they have a higher predicted impact of the number of page views.
4 Conclusion
In this paper we proposed Scout, a general-purpose Point of Presence (PoP)
recommendation system using statistical modeling of the total page download
time on Real User Monitoring (RUM) data, and it has been driving the selection
of PoPs for a major social network company since developed. Our empirical
experiments on millions of real user data points obtained from a large social
network show very good performance, i.e., the prediction errors are lower than
5 % for most regions, and we have further extended the work from purely using
site speed performance as the metric to other business metrics such as total
number of page views.
Acknowledgments. We are grateful to Samir R. Das for his valuable feedback on an
earlier draft of this paper. We would also like to thank the anonymous reviewers for
their insightful comments.
References
1. Brutlag, J.: Speed matters for google web search. Google, June 2009
2. www.peeringdb.com
3. Dabek, F., Cox, R., Kaashoek, F., Morris, R.: Vivaldi: a decentralized network
coordinate system. In: ACM SIGCOMM Computer Communication Review, vol.
34, pp. 15–26. ACM (2004)
4. Dellaert, B.G., Kahn, B.E.: How tolerable is delay?: Consumers’ evaluations of
internet web sites after waiting. J. Interact. Mark. 13(1), 41–54 (1999)
5. Francis, P., Jamin, S., Jin, C., Jin, Y., Raz, D., Shavitt, Y., Zhang, L.: Idmaps: a
global internet host distance estimation service. IEEE/ACM Trans. Network. 9(5),
525–540 (2001)
6. Gummadi, K.P., Saroiu, S., Gribble, S.D.: King: estimating latency between arbi-
trary internet end hosts. In: Proceedings of the 2nd ACM SIGCOMM Workshop
on Internet Measurment, pp. 5–18. ACM (2002)
7. Hastie, T., Tibshirani, R., Friedman, J., Hastie, T., Friedman, J., Tibshirani, R.:
The elements of statistical learning, vol. 2. Springer, New York (2009)
8. Iyengar, A.K., Squillante, M.S., Zhang, L.: Analysis and characterization of large-
scale web server access patterns and performance. World Wide Web 2(1–2), 85–100
(1999)
9. Koenker, R.: Quantile Regression. Cambridge University Press, Cambridge (2005)
10. Krishnan, R., Madhyastha, H.V., Srinivasan, S., Jain, S., Krishnamurthy, A.,
Anderson, T., Gao, J.: Moving beyond end-to-end path information to optimize cdn
performance. In: Proceedings of the 9th ACM SIGCOMM Conference on Internet
Measurement Conference, pp. 190–201. ACM (2009)
Scout: A PoP Recommendation System Using RUM Data
217
11. Madhyastha, H.V., Anderson, T., Krishnamurthy, A., Spring, N., Venkataramani,
A.: A structural approach to latency prediction. In Proceedings of the 6th ACM
SIGCOMM Conference on Internet Measurement, pp. 99–104. ACM (2006)
12. Maheshwari, R.: How LinkedIn used PoPs and RUM to make dynamic content
download 25% faster. LinkedIn Engineering Blog (2014)
13. Ramsay, J., Barbesi, A., Preece, J.: A psychological investigation of long retrieval
times on the world wide web. Interact. Comput. 10(1), 77–86 (1998)
14. Sears, A., Jacko, J.A., Borella, M.S.: Internet delay eﬀects: how users perceive qual-
ity, organization, and ease of use of information. In: CHI 1997 Extended Abstracts
on Human Factors in Computing Systems, pp. 353–354. ACM (1997)
15. Squillante, M.S., Yao, D.D., Zhang, L.: Web traﬃc modeling and web server per-
formance analysis. ACM SIGMETRICS Perform. Eval. Rev. 27(3), 24–27 (1999).
IBM TJ Watson Research Center