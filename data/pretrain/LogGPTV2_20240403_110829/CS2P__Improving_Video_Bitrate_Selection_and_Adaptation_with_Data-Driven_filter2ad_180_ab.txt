cities and 87 ISPs in China. In each session, a client set up
an HTTP connection with one of the web servers and down-
loaded video chunks that had been encoded at a ﬁxed bitrate
(chosen by the user). Table 2 shows the basic features of the
session and the coverage of our dataset. Within each ses-
3The normalized QoE (n-QoE) is deﬁned as the actual QoE
relative to the theoretical optimal, which could be achieved
with the perfect knowledge of future throughput. Here, we
adopt the same deﬁnition of video QoE as that in [47], and
we formally deﬁne it in §7.1.
4We explored datasets such as Glasnost [21], MLab NDT [9]
and one from a EU cellular provider [7]. Unfortunately, all
of these have too few hosts and the sessions lasted only a
handful of seconds making it unsuitable for such throughput
stability and predictability analysis.
 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25Normalized QoEPrediction ErrorMPCBB(a) Duration
(b) Throughput
(a) An example session
(b) Throughput variation at
two consecutive epochs
Feature Description
Figure 3: CDF of session duration and throughput
# of unique
values
3.2M
ClientIP Unique IP address associated
Figure 4: Stateful behaviors in session throughput.
We tried a range of simple prediction models used in prior
work [24, 30, 47] for predicting the throughput of the next
epoch based on past observations in the session. These in-
clude: (1) Last-Sample (LS, using the observation of the
last epoch), (2) Harmonic-Mean (HM, harmonic mean of
past measurements), and (3) Auto-Regressive (AR, a clas-
sical timeseries modeling technique). We found that in gen-
eral, these did not work satisfactorily with the median and
75%ile normalized prediction error across sessions respec-
tively ≥18% and 40%.
Observation 2: The evolution of the throughput within a
session exhibits stateful/persistent characteristics, which if
captured can lead to improved prediction.
Figure 4a gives a visual example from our dataset. We can
clearly observe some states within the throughput variation.
We can split the timeseries into roughly 11 segments, and
each segment belongs to one of the four states. Within each
state the throughput is largely Gaussian, e.g., timeslots 20–
75, 90–115, 135–175 and 180–210 belong to the same state
with the mean throughput around 2.8Mbps.
We investigate the throughput variation across two con-
secutive epochs for a broader set of sessions and ﬁnd sim-
ilar stateful behaviors in these sessions. As an illustrative
example, in Figure 4b we plot throughput at epoch t + 1 (y-
axis) vs. throughput at epoch t (x-axis) of the sessions in our
dataset with a particular IP/16 preﬁx. (We do not show the
exact value of this preﬁx for proprietary reasons.) We can
observe a clustered trend in the distribution of these points,
i.e., there are some discrete states and the session throughput
changes across these states (red circles in Figure 4b). In Sec-
tion 5.2, we show that these states can be efﬁciently captured
by a Hidden-Markov Model (HMM).
Given that we only had end-to-end measurements, we
cannot conclusively pinpoint the root cause for such state-
ful behaviors. We can however intuitively conjecture that
these patterns stem from the TCP fair-sharing model—the
throughput depends on the hidden state, which is the num-
ber of ﬂows currently sharing the bottleneck link, and the
observed throughput changes as the number of concurrent
ﬂows changes during the session’s lifetime.
Observation 3: Sessions with similar features tend to ex-
hibit similar initial throughput conditions and throughput
evolution patterns.
Prior work (CFA [29]) shows that, at the application layer,
video sessions with the same critical features have similar
Province The province where
the
ISP
AS
City
Server
to a client
ISP of client (e.g., AT&T)
The Autonomous System
that client resides in
client is located
The city where the client is
located
The server-side identiﬁer
87
161
33
736
18
Table 2: Summary of statistics from the dataset.
sion, we recorded the average throughput for each 6-second
period.5 We refer to such a period as an “epoch”.
Figure 3a shows the CDF of the session duration and
Figure 3b shows the distribution of the per-epoch aver-
age throughput and suggests that the average throughput
distribution is similar to residential broadband characteris-
tics [43]. The clients represent a wide spatial coverage of
China. Although the number of servers is relatively small,
the setting is very close to what real-world video delivery
service providers face, i.e., the clients are widely distributed
while the servers are relatively fewer.
Next, we use this dataset to characterize the structure of
throughput variability within a given session and across ses-
sions, and also evaluate the predictive power of some seem-
ingly natural strawman solutions.
Observation 1: There is a signiﬁcant amount of through-
put variability within a video session, and simple predictive
models (e.g., looking at recent epochs) do not work.
We ﬁrst investigate the throughput variability within a ses-
sion. For instance, if the variability is small, then the adap-
tation logic does not have to switch bitrates often. To do so,
we compute the coefﬁcient of variation, which is deﬁned as
the ratio of the standard deviation (“stddev”) of throughput
across different measurements within the session to the mean
of throughput measurements. The result shows that about
half of the sessions have normalized stddev ≥30% and 20%+
of sessions have normalized stddev ≥50% (not shown). This
conﬁrms the general perception that the throughput has sig-
niﬁcant variation within a session, and therefore for video
streaming, simple static bitrate selection will not sufﬁce.
5For each 6-second epoch, the client counts the total incom-
ing TCP segments and computes the average throughput.
Then it records and reports the average throughput observed
per epoch, after the session completes.
 0 0.2 0.4 0.6 0.8 1 0 50 100 150 200CDFSession Duration (seconds) 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30 35 40CDFPer-epoch Throughput (Mbps) 2.4 2.6 2.8 3 0 40 80 120 160 200Throughput (Mbps)Epoch(a) Example of similar ses-
sions
Figure 5: Throughput similarity for sessions sharing the
same key feature.
(b) CDF of initial through-
put at different clusters
QoE (e.g., rebuffering, startup latency, etc.). Here, we dis-
cover similar trends at the network layer, i.e., sessions shar-
ing the same key set of features exhibit similarity in their
throughput. Figure 5a gives an example from our dataset to
illustrate this intuition. Sessions 1/2 and Sessions 3/4 are two
pairs of “close neighbors”, i.e., sharing a set of key session
features. We can see that there is similarity in the throughput
dynamics between the sessions in each of the pair.
Next, we categorize the sessions into different clusters ac-
cording to Client IP preﬁx. Figure 5b shows the CDFs of
initial throughput for 3 different clusters, each consisting of
over 500 sessions. We have two key takeaways: (1) Ses-
sions in different clusters have signiﬁcant differences in ini-
tial throughput; (2) Within each cluster, a large number of
sessions have similar initial throughput, e.g., 65% sessions
in Cluster A have throughput around 2Mbps and 11Mbps,
and over 40% of sessions in Cluster B with throughput
6Mbps. We did the same on midstream average through-
put and found consistent results (not shown). Therefore, if
we can identify the “similar sessions” with the same key fea-
tures, we can use a cross-session prediction methodology to
improve the accuracy. However, as we will show next this is
a non-trivial task.
Observation 4: Simple models (e.g.,
last-mile charac-
teristics) are not expressive enough to capture session
similarity as there is signiﬁcant diversity in session char-
acteristics and the relationship between session features
and throughput can be quite complex.
An intuitive starting point to exploit the above observa-
tion of similarity across sessions is to look at the “last mile”
characteristics (e.g., type of broadband connections). Thus,
we tried two seemingly natural strawman solutions that con-
sider last-mile predictors on both client and server side, i.e.,
predicting by sessions with the same client IP preﬁx or con-
necting to the same server. The results show that half of the
sessions have the normalized prediction error ≥50%, and
over 30% of the sessions with prediction error ≥80% (not
shown).
More generally, we observe that the factors that can af-
fect the throughput can be quite complex along two dimen-
sions. First, combinations of multiple features often have a
much greater impact on throughput than the individual fea-
ture. This can be intuitively explained as the throughput is
often simultaneously affected by multiple factors (e.g., the
Figure 6: The throughput variation of sessions match-
ing all and a subset of three features: X=ISP, Y=City,
Z=Server
last-mile connection, server load, backbone network con-
gestion, etc.), which means sessions sharing same individual
feature may not have similar throughput. Figure 6 gives an
example of the effect of feature combinations. It shows that
the throughput distribution of sessions with the same values
on three key features (i.e., residing in the same ISP-China
Telecom and the same city-Hangzhou, and fetching from the
same server-Server No.8), and the throughput distribution of
sessions only having same values on one or two of the three
features. As shown in Figure 6, the throughput when all
three features are speciﬁed is much more stable than any of
other cases, meaning that for these sessions it is the combina-
tion of all the 3 features (not the subset) that determines their
throughput. In practice, we ﬁnd that such high-dimensional
effects are the common case, rather than an anomalous cor-
ner case. For instance, 51% of distinct ISP-City-Server val-
ues have inter-session throughput standard deviation that is
at least 10% lower than that of sessions only matching one
of two features (not shown). Therefore, in order to capture
“high dimensionality” effects, the prediction algorithm must
be sufﬁciently expressive to combine multiple features rather
than treating them individually.
Second, the impact of same feature on different sessions
could be variable. For instance, the “last-mile connection”
usually becomes the bottleneck for satellite communication
links, while for broadband access it is less important to deter-
mine the throughput. We compute the relative information
gain6 of a feature on the throughput of session set to repre-
sent the impact of the feature on predicting their throughput,
and ﬁnd that the impact of the same feature (i.e., city) signif-
icantly varies for sessions in two different ISPs with the dif-
ference of relative information gain over 65% (not shown).
Key observations: In summary, our analysis of throughput
variability suggests that:
• There is substantial throughput variability within a given
session and a range of simple prediction models using
previous observations in the same session do not provide
high accuracy.
• Many sessions exhibit stateful characteristics in the evo-
• Sessions sharing similar critical characteristics tend to
lution of the throughput.
exhibit similar throughput patterns.
6Relative information gain is often used to quantify how
useful a feature is for prediction, deﬁned as RIG(Y |X) =
1 − H(Y |X)/H(Y ), where H(Y ) and H(Y |X) are the en-
tropy of Y and the average conditional entropy of Y .
 0.4 0.5...2.22.3 0 5 10 15 20Throughput (Mbps)EpochSession1Session2Session3Session4 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14 16CDFSession Initial Throughput(Mbps)Cluster ACluster BCluster C 0 200 400 600 800 1000 1200[X][Y][Z][X,Y][X,Z][Y,Z][X,Y,Z]Throughput(Kbps)• The nature of the relationships between session features
and throughput are quite complex and simple last-mile
predictors are inaccurate.
4 CS2P Approach and Overview
In this section, we provide an overview of CS2P which lever-
ages our earlier observations regarding throughput variation
to improve bitrate selection and adaptation.
Figure 1 shows the basic workﬂow of CS2P. In the of-
ﬂine training stage, throughput measurements of sessions
are collected by the Prediction Engine. The Prediction En-
gine builds throughput prediction models based on the data
collected. These models can then be plugged into the bi-
trate adaptation algorithms implemented either by the video
servers or by clients.
Seen in a broader context, CS2P can be regarded as a
middle ground between centralized video control platforms
(e.g., C3 [23], CFA [29]) and decentralized player-based bi-
trate adaptation (e.g., BB [27], FESTIVE [30], MPC [47]).
Speciﬁcally, CS2P borrows the beneﬁts of the global view
advocated by C3/CFA-like architectures to train the models.
However, “actuation” using these models happens in a de-
centralized manner and without global coordination. As we
will see in §6, these models are compact (<5KB) and can be
easily plugged into the client- and server-side bitrate adapta-
tion algorithms. While CS2P cannot offer all the beneﬁts of
centralized control (e.g., CDN switching), it offers a prag-
matic alternative for video providers and CDNs, who do not
want to relinquish control to third-party optimizers and/or do
not want to incur the complexity of centralized control.
The key challenge is employing suitable prediction mod-
els that can capture the throughput variability observed in
real-world sessions. As we saw in the previous discus-
sion, simple models are not expressive enough to capture
the structure of the throughput variation within an individ-
ual session and the diversity of the factors that can affect the
throughput of a client-server combination.
At a high level, one can characterize how expressive a pre-
diction model is in terms of the spatial and temporal struc-
ture it can capture. For instance, let us consider the initial
bitrate prediction along the spatial dimension. At one end of
the spectrum, we can use the previously observed throughput
of the same client-server pair and at the other end of the spec-
trum we can simply use the global average of all the sessions.
Obviously, neither is desirable; we may not have sufﬁcient
samples in the former case and cannot capture the diversity
across sessions in the latter case. Similarly, let us consider
the midstream bitrate prediction. If we only use the previous
chunk throughput measurement from the same session, then
we run the risk of having a noisy measurement which may
additionally miss key state transitions. Besides, such simple
time-series models miss the impact of critical spatial session
features such as client location and ISP (Observation 4 in
§3).
CS2P adopts a cross-session (i.e., spatial) and stateful
(i.e., temporal) prediction modeling approach that works as
follows. First, based on Observation 3 in §3, CS2P groups
similar sessions sharing the same set of critical feature val-
ues and uses the data from such similar sessions to build
the prediction models. Second, to capture the “state transi-
tions” within a session (Observation 2 in §3), CS2P learns
a Hidden-Markov Model (HMM) for each cluster of sim-
ilar sessions. HMM is an efﬁcient state-based model and
has been widely used to predict path and trafﬁc proper-
ties [42, 44, 46].
Given this basic overview, there are three practical ques-
tions that remain:
1. How to cluster similar sessions?
2. How do we automatically train the models?
3. How to utilize these models for throughput prediction
and bitrate adaptation?
We will address these questions next.
5 CS2P Detailed Design
In this section, we describe the detailed design of CS2P that
addresses the above practical challenges. We begin by de-
scribing our data-driven clustering mechanism (§5.1). Then,
we describe the HMM training and online prediction algo-
rithms (§5.2). We conclude this section by describing how
the initial throughput prediction and the HMM can be inte-
grated into client- and server-side components (§5.3).
5.1 Identifying clusters
For both the initial and midstream throughput prediction,
CS2P relies on clustering similar sessions with a cross-
session prediction methodology. At a high level, CS2P ﬁnds
for any session s a key feature set and time range, which is
used to aggregate previous sessions that match the speciﬁc
features with s and happened in the speciﬁc time range.