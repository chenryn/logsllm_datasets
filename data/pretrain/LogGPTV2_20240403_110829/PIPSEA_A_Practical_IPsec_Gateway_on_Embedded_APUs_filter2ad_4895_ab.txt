the work-items in the warp take the same execution path
but, because of the branch, inst2 and inst3 are predicated.
Thus, it takes total 5 cycles. However, if the branch con-
dition evaluates to the same value for all the work-items,
there is no control-ﬂow divergence, and the execution time
is 4 cycles.
3. DESIGN AND IMPLEMENTATION
In this section, we describe the design and implementation
of PIPSEA on an embedded APU. In the design, we use an
embedded APU with at least two and at most three general-
purpose CPU cores and one GPU, which is cost and power
constrained. To reﬂect the reality of network traﬃc, we con-
servatively assume that each packet in the incoming network
traﬃc belongs to a possibly diﬀerent connection. That is,
each packet has a diﬀerent source IP address and a diﬀer-
ent destination IP address. This in turn implies that each
packet needs to be handled with a diﬀerent crypto algorithm
and a diﬀerent cipher (or authentication) key.
3.1 Overall Organization
Figure 3 shows the overall organization of PIPSEA. It is
a multi-threaded program running in the user space and
implemented on top of DPDK (Data Plane Development
Kit)[2] and the OpenCL runtime[21].
The IPsec gateway consists of two threads: packet I/O
thread and IPsec thread. Each thread is pinned to and run-
ning on a diﬀerent CPU core. The Packet I/O thread has
two modules: receiver module and sender module. The re-
ceiver module receives a bulk of packets from the RX queue
in the NIC. The size of the bulk depends on the NIC. The
maximum is 32 or 64 in general. After looking up the se-
1257Received(cid:3)Packets
Packet(cid:3)
Scheduler
Packet(cid:3)I/O(cid:3)thread
Receiver
Sender
IPsec(cid:3)Gateway
IPsec(cid:3)thread
User(cid:3)space
OS
H/W
Intel(cid:3)DPDK
UIO(cid:3)Driver
NIC
OpenCL(cid:3)Runtime
OpenCL(cid:3)Driver
Integrated(cid:3)GPU
Figure 3: The organization of PIPSEA.
curity association database (SAD) and the security policy
database (SPD), the packet scheduler in the receiver module
groups the received packets into multiple batches by their
crypto algorithms and lengths.
The batches of the packets from the packet scheduler
are the input to the IPsec thread. The IPsec thread runs
the OpenCL host program.
It repeatedly launches an
OpenCL kernel that performs IPsec packet processing in-
cluding adding/modifying headers, encryption, decryption,
and authentication. When a kernel execution is completed,
the IPsec thread enqueues the processed packets to the com-
pletion queue. The sender module in the packet I/O thread
sends the packets in the completion queue to the TX queue
in the NIC.
(cid:12)
(cid:86)
(cid:83)
(cid:69)
(cid:42)
(cid:11)
(cid:3)
(cid:87)
(cid:88)
(cid:83)
(cid:75)
(cid:74)
(cid:88)
(cid:82)
(cid:85)
(cid:75)
(cid:55)
(cid:23)(cid:19)
(cid:22)(cid:24)
(cid:22)(cid:19)
(cid:21)(cid:24)
(cid:21)(cid:19)
(cid:20)(cid:24)
(cid:20)(cid:19)
(cid:24)
(cid:19)
Single(cid:882)threaded(cid:3)PACKET(cid:3)IO
Dual(cid:882)threaded(cid:3)PACKET(cid:3)IO
AES(cid:882)CBC(cid:3)encryption
HMAC(cid:882)SHA1
(cid:25)(cid:23)
(cid:21)(cid:24)(cid:25)
(cid:24)(cid:20)(cid:21)
(cid:20)(cid:19)(cid:21)(cid:23)
(cid:51)(cid:68)(cid:70)(cid:78)(cid:72)(cid:87)(cid:3)(cid:54)(cid:76)(cid:93)(cid:72)
Figure 4: Throughput comparison of packet I/O and
crypto algorithms.
Run-to-completion vs. pipeline. Run-to-completion
and pipeline models are two widely used network packet pro-
cessing models for multicore CPUs. While the latter assigns
a diﬀerent task to each core to process a packet, the former
makes each core process a single packet from the beginning
to the end including packet I/O.
PIPSEA is based on a pipeline model: the packet I/O
thread performs packet I/O and the IPsec thread manages
the GPU to process IPsec protocols. Since IPsec processing
requires heavy computation, and the dedicated cores to the
IPsec processing do not perform packet I/O, the pipeline
model is better than the run-to-completion model for our
case.
One of the beneﬁts of the run-to-completion model is that
packet I/O can be scaled over multiple CPU cores. Like the
run-to-completion model, we may scale packet I/O over mul-
tiple CPU cores using multiple packet I/O threads. How-
ever, this may become a drawback in our case because the
GPU can process only one chunk of packets at a time. When
multiple chunks of packets are produced simultaneously by
multiple packet I/O threads, each chunk should be processed
by the GPU in a round-robin manner. This may lead to ex-
periencing unacceptable high packet round-trip latencies.
As long as the throughput of single-threaded packet I/O
is higher than the throughput of the IPsec processing on the
GPU, exploiting multiple threads for the packet I/O is an
overkill. Figure 4 compares the packet I/O-only through-
put (including packet scheduling) and the throughput of
crypto algorithms on the GPU. However, the single-threaded
packet I/O throughput is lower than the best throughput of
the GPU (at HMAC-SHA1). This implies that the single-
threaded packet I/O is not powerful enough to achieve the
maximum throughput of the IPsec gateway.
Therefore, we propose two models for PIPSEA: single-
threaded and dual-threaded packet I/O models. The single-
threaded packet I/O model is more resource-constrained and
works when the APU has only two CPU cores. When there
are two CPU cores available for packet I/O, we use the dual-
threaded packet I/O model, where one thread handles the re-
ceiver module and the other handles the sender module. As
shown Figure 4, the dual-threaded packet I/O throughput is
always higher than the best throughput of the GPU. Thus,
dual-threaded packet I/O is powerful enough to achieve the
maximum throughput.
Packet I/O implementation. Fast packet I/O is a fun-
damental building block of network packet processing. Han
et al. [12] and Kim et al. [22] report that the Linux network
stack is ineﬃcient because of unnecessary protocol handling
and memory management overheads inside the Linux kernel.
To overcome the ineﬃciency of the Linux network stack, a
high performance user-level packet I/O framework such as
DPDK [2] is necessary.
DPDK provides user-level zero-copy packet I/O API func-
tions to eliminate data copy overhead between the network
stack in the kernel space and the application in the user
space.
It also provides several useful libraries and drivers
including a memory manager, buﬀer manager, queue man-
ager, and optimized poll mode drivers to help develop high
performance network applications. For instance, to improve
performance by reducing TLB misses, its memory manage-
ment exploits huge pages (2MB) for memory objects, such
as memory pools, packet buﬀers, and rings.
Our IPsec gateway implementation is based on DPDK.
We use the zero-copy packet I/O API functions. All queues
and bins in PIPSEA are implemented with the lock-free
rings provided by DPDK. It provides single/multi-producer,
single/multi-consumer lock-free rings for not only high per-
formance but also ease of development.
Instead of directly using the DPDK API functions, we
implement a wrapper function for each DPDK API func-
tion used in the gateway. Thus, our implementation is not
tightly coupled with DPDK. Another user-level packet I/O
framework can be easily combined with our solution by just
modifying the wrapper functions.
1258Active(cid:3)Bins
Wait(cid:3)Bins
GPU
CU
CU
CU
…
…
…
Leftover(cid:3)
Queue
Packet(cid:3)Pool
Completion(cid:3)
Queue
HW(cid:3)RXQ
HW(cid:3)TXQ
NIC
Figure 5: Queues and bins in PIPSEA.
3.2 Queues and Bins
While most of previous packet processing studies with dG-
PUs have made an eﬀort to reduce data copy overhead be-
tween the CPU and the GPU, it is not a concern anymore
for us because of the HSA supported by the APUs. The
CPU cores and the GPU physically share the coherent main
memory in the HSA.
Figure 5 shows the queues and bins used in PIPSEA. All
packets from the NIC are stored in the packet pool managed
by DPDK and allocated in the main memory. Accesses to
packets are done using the same pointers to the packets on
both sides of the CPU and the GPU because of the HSA.
Thus, no packet copy occurs between the CPU and the GPU
in our approach.
To eﬃciently pipeline packet scheduling and IPsec proto-
col processing, we implement two sets of bins: active and
wait. Using these bins, we implement an eﬃcient double
buﬀering mechanism to overlap the computation on the GPU
and the communication between the packet I/O thread and
the GPU.
As mentioned before, the packet scheduler groups the re-
ceived packets in to the wait bins by their crypto algorithms
and lengths. While the packet scheduler ﬁlls in the wait
bins, the OpenCL kernel on the GPU processes the packets
in the active bins. After the GPU ﬁnishes processing the
active bins, the packets in the active bins are enqueued to
the completion queue by the IPsec thread.
To reduce the degree of packet reordering, the processed
packets are enqueued to the completion queue in the same
order as they were received.
Then, the IPsec thread switches the roles of the active bins
and the wait bins. The OpenCL kernel on the GPU starts to
process the new packets in the active bins, and the packet
scheduler ﬁlls in the wait bins with new received packets.
Our IPsec gateway continuously repeats this process. If a
received packet cannot be scheduled in the wait bins because
there is no appropriate bin for the packet, it is inserted to
the leftover queue and will be scheduled in the next turn.
3.3 Scheduling Packets
The OpenCL kernel that performs IPsec processing on the
GPU consumes a chunk of multiple packets (i.e., the packets
in the active bins) at each kernel launch. The role of the
packet scheduler in the packet I/O thread is to produce and
schedule a chunk of packets to achieve the best utilization
of the GPU hardware resources.
The crypto algorithm of each packet is determined by its
source and destination IP addresses. The packet scheduler
performs this task by looking up the SAD and SPD that are
managed by the IKE (Internet Key Exchange) protocol. We
assume that the SAD and SPD are predeﬁned because the
implementation of the IKE protocol is beyond the scope of
this paper.
(0)
Labels
Received(cid:3)
Packets
(1)
Wait(cid:3)Bins
Leftover(cid:3)
Queue
64
256
129
85
520
980
820
129
64
LB,255
256
256,511
85
LB,255
520
512,767
820
980
768,1023
Received(cid:3)
Packets
75
550
259
150
300
145
74
(2)
Sorting
(3)
129
64
LB,255
259
256
256,511
75
85
LB,255
550
520
512,767
820
980
768,1023
145
150
LB,255
74
300
820
980
550
520
259
256
145
150
Active(cid:3)Bins
129
64
75
85
Figure 6: A packet scheduling example.
Packet scheduling algorithm. Figure 6 shows a snapshot
produced by our packet scheduling algorithm. A diﬀerent
pattern or color in a packet indicates a diﬀerent crypto al-
gorithm. The number in a packet shows its length. When
the packet scheduler starts to ﬁll in the wait bins (step 0 in
Figure 6), all the wait bins are empty and not labeled yet.
The leftover queue contains some packets that could not be