Even if an attacker ﬁnds a way to evade the obfuscation
features, in order to achieve his purpose of infecting end
users, his malware will necessarily implement functionality
to achieve this goal. That is, the attacker needs to download
and start malware on the victim’s PC, and to do this, the
code needs to break out of the Java sandbox. By capturing
this behavior, intrinsic to the attacker’s goal, we make it
diﬃcult for malicious code to evade our analysis and still
succeed.
Previous systems [11, 13] for the detection of web-based
exploits also use features to detect obfuscation and mali-
cious behaviors. However, the concrete instances of these
features are quite diﬀerent. The main reason for these dif-
ferences is that Jarhead uses static code analysis. Previous
work on the detection of malicious JavaScript and Flash,
on the other hand, relies on dynamic analysis. As a result,
our novel features compute the presence and frequency of
certain functions and resources, code patterns, and artifacts
over the entire code base. Dynamic systems, on the other
hand, typically compute the number of times certain events
or activities occur during the execution of a program.
Answering the question how much impact do our top ten
features (Table 1) have, compared to the full feature set we
used a classiﬁer with ten fold cross validation. We ran the
classiﬁer once with all our features and then with only the
top ten features enabled on a total of 3372 applets, com-
bining all our data sets. Again, the data sets are described
in the evaluation chapter in detail along with more exper-
iments. This experiment was run after the experiments in
the evaluation section due to demand by our reviewers. Out
of the 3372 applets the classiﬁer working only with our top
ten features misclassiﬁed a total of 122 (3.6%) applets, while
our full feature set misclassiﬁed only 35 (1.0%) applets. This
shows, that while our top ten features already perform rea-
sonably well, our other features help to achieve even better
results.
253
Merit
0.398
0.266
0.271
0.257
0.254
0.232
0.22
0.211
0.202
0.197
Attribute
gets parameters
functions per class
no of instructions
gets runtime
Type
behavior
obfuscation
obfuscation
behavior
lines of disassembly
obfuscation
uses ﬁle outputstream
percent unused methods
longest string char cnt
mccabe complexity avg
calls execute function
behavior
obfuscation
obfuscation
obfuscation
behavior
Table 1: The ten most useful features, sorted by
average merit.
In another experiment, we wanted to see how well the
group of the obfuscation features performs without the be-
havioral features enabled and vice versa. To this end we
used the same 3372 applets as before with ten fold cross
validation, once with only the behavioral features and once
with only the obfuscation features enabled. While the ob-
fuscation features and behavioral features both performed
reasonably well by themselves, with 119 (3.5%) respectively
150 (4.5%) misclassiﬁed applets they work best when used
together. As stated before with all features enabled, we mis-
classify only 35 (1.0%) of the applets.
5. EVALUATION
In this section, we evaluate our system to measure how
well it performs in detecting malicious applets. For our
experiments, we used two diﬀerent datasets: A manually-
compiled dataset of applets (the manual set), and a set of
samples collected and provided to us by the authors of the
Wepawet system (the wepawet set).
5.1 Results: Manual Dataset
The manual dataset contains samples from four diﬀer-
ent sources: Two online collection of applets, an archive
of mostly-malicious applets, and a number of manually col-
lected samples. The two online collections1 oﬀer a selection
of Java applets for web designers to use in their web pages.
We crawled these two pages and downloaded a total of 1,002
Jar ﬁles.
In addition, we obtained an archive of 357 Jar
ﬁles with unknown toxicity from a malware research com-
munity site2. Finally, we manually searched for malicious
applet samples on security sites3, and we wrote a crawler
that we seeded with Google search results on a combination
of the terms “Java,”“applet,” and “malicious.” This yielded
another 1,495 Jar ﬁles, for a total of 2,854 samples. By
combining applets from a broad range of diﬀerent sources in
this data set we tried to ensure it is diverse, representative
of applets used in the wild and not biased to a speciﬁc kind
of applet.
We ﬁrst cleaned the manual dataset by removing dupli-
cates, broken Jar ﬁles, and non-applets; i.e., we removed all
Jar ﬁles that, after extraction, did not contain at least one
class ﬁle that was derived from one of the applet base classes
1
http://echoecho.com
and
http://javaboutique.
internet.com
2
http://filex.jeek.org
http://www.malwaredomainlist.com
3
(java.applet.Applet, javax.swing.Japplet or javax.micro-
edition.midlet.MIDlet). After this initial ﬁlter step, there
were 2,095 ﬁles (of unknown toxicity) left.
To obtain ground truth for building and evaluating our
classiﬁer, we submitted the entire manual dataset to Virus-
total. Virustotal is a website that provides free checking of
suspicious ﬁles using 42 diﬀerent anti-virus products. Virus-
total found 1,721 (82.1%) of the ﬁles to be benign and 374
(17.9%) to be malicious (we counted a sample as benign if
none of the Virustotal scanners found it to be malicious and
as malicious if at least one scanner found it to be malicious).
Using the results from Virustotal as a starting point, we
built an initial classiﬁer and applied it to the manual dataset.
In other words, we performed training on the manual data
set using the results from Virustotal as ground truth. We
then manually inspected all samples for which the results
from our classiﬁer and the results from Virustotal were dif-
ferent. We decompiled the samples and inspected their source
code checking for known exploits that use certain functions,
download and execute behavior.
In the cases were docu-
mentation was available for a (probably) benign applet, we
checked if the described behavior was found in the source
code. Often checking the checksum of a jar ﬁle on Google
provided a good indication if this was a known piece of mal-
ware.
If after a while of manually inspecting the applet
we were still unsure, we stuck with the verdict assigned by
Virustotal. In this process, we found that Virustotal has ac-
tually misclassiﬁed 61 (2.9%) applets. In particular, Virus-
total classiﬁed 34 (1.6%) benign applets as malicious, and
deemed 27 (1.3%) malicious applets as benign. We manu-
ally corrected the classiﬁcation of these 61 applets, and used
the resulting, corrected classiﬁcation as the ground truth for
our dataset (with 381 malicious and 1,714 benign samples).
In the next step, we trained diﬀerent classiﬁers on the
manual dataset. More precisely, the manual dataset was
split into a training set and a test set, using ten-fold cross
validation. We evaluated C4.5 decision trees, support vector
machines, and Bayes classiﬁcation, which all showed compa-
rable results. Eventually, decision trees turned out to be the
most reliable classiﬁers, and they also provide good explana-
tory capabilities.
The detection results for the decision tree classiﬁer were
very good. With ten-fold cross validation, the classiﬁer only
misclassiﬁed a total of 11 (0.5%) samples. The false positive
rate was 0.2% (4 applets), the false negative rate 0.3% (7
applets).
If we compare our detection results to the results produced
by Virustotal, we ﬁnd that our results are both better in
terms of false positives and false negatives, and we see a re-
duction in the total number of misclassiﬁcations by a factor
of six. This is in spite of the fact that we used Virusto-
tal to build our initial labels, before manually adjustments
(and hence, our ground truth might be biased in favor of
Virustotal).
An overview of the results for the manual dataset are given
in Table 2.
Discussion.
We examined the 11 instances (4 false positives, 7 false
negatives) in more detail: One false positive was a poten-
tially benign MIDlet, which had the ability to send text mes-
sages to arbitrary numbers. Two false positives were trig-
gered by applets that did show suspicious behavior by trying
254
Virustotal (42 AVs)
Jarhead (10x cross-val.)
False pos.
False neg.
1.6%
1.3%
0.2%
0.3%
Table 2: Comparison of Jarhead and Virustotal mis-
classiﬁcations - note that our ground truth is biased
towards Virustotal.
to write local ﬁles and executing commands on the windows
command shell, but they were probably not intended to be
malicious. The last false positive is an obfuscated applet
that is performing calculations based on the current date,
without ever displaying the results; its purpose remains un-
clear to us.
Two false negatives were applets that contained incom-
plete (broken) exploits. While these applets would not suc-
cessfully execute exploits, we still count them as false nega-
tives, since their intent is malicious (and we classiﬁed them
as benign). We missed three additional malicious applets
that were using reﬂection or loading additional code at run-
time to protect against static analysis and that used an
instance of CVE-2010-0094 [5]. CVE-2010-0094 is a ﬂaw
in the object deserialization that allows attackers to call
privileged Java functions without proper sandboxing. The
last two misclassiﬁed applets were instances of CVE-2009-
3869 [4]. This particular stack-based buﬀer overﬂow is hard
to catch for our system, since the vulnerable function it ex-
ploits within the native JVM implementation is not directly
triggered by a speciﬁc function called from the bytecode,
but, instead, is reachable from a large set of widely-used
Java library functions.
Despite a few misclassiﬁed instances, we found that our
system performs detection with high accuracy. In particular,
some of the incorrect cases are arguably in a grey area (such
as possibly benign applets that try to execute commands di-
rectly on the Windows command line and malicious applets
that contain incomplete, half-working exploits).
We also examined the decision tree produced by our clas-
siﬁer in more detail. We found that our most important
features, i.e., the ones that are highest up in the decision
tree, include both features from our obfuscation set and our
behavioral set. The top ten features include features cap-
turing the interaction with the runtime, the execute feature,
and the feature monitoring local ﬁle access. We also ﬁnd
the text message send function to be important. On the
obfuscation side, we ﬁnd size features, string features, and
the number of functions per class to be the most eﬀective at
predicting malicious behavior.
We also tried to understand the impact of removing the
feature that checks for known vulnerable functions (since
this could be called signature-based detection). If this fea-
ture is removed, the misclassiﬁcation rate increases only
marginally, by 0.17%. This conﬁrms that many features
work together to perform detection, and the system does
not rely on the knowledge of speciﬁc, vulnerable functions.
To further emphasize this point, we draw the attention to
the ability of Jarhead to detect zero-day attacks. After we
ﬁnished the ﬁrst set of experiments on the manual dataset, a
new vulnerability[7], which was not part of our training set,
began to see widespread use. When we tested our classiﬁer
on these completely new exploits, it was able to identify all
ﬁve samples we acquired that implement this exploit, with-
out any adjustment.
5.2 Results: Wepawet Dataset
To further test our classiﬁer on real-world data, we col-
lected Jar ﬁles from the Wepawet system. Wepawet is a
public portal that allows users to upload suspicious URLs.
Wepawet would then fetch the content at these URLs and
attempt to detect drive-by download exploits. Since certain
drive-by download exploit kits make use of malicious Java
code, we expected to ﬁnd some applets that Wepawet users
on the Internet might have submitted to the system. Unfor-
tunately, Wepawet does currently not support the analysis of
Jar ﬁles, and hence, would consider all such ﬁles as benign.
The authors of Wepawet provided us with 1,551 Jar ﬁles
that the system had collected over the past months. We
removed duplicates, broken archives, and non-applets (as
before) and ended up with a set of 1,275 applets. Again, we
used Virustotal to obtain some form of initial classiﬁcation.
Virustotal found 413 (32.4%) applets to be benign and 862
(67.6%) applets to be malicious. We then ran our classiﬁer
on this Wepawet set. Compared to Virustotal, we assigned
the same verdict to 1,189 (93.3%) samples, while 86 (6.7%)
samples were classiﬁed diﬀerently. More precisely, we clas-
siﬁed 59 (4.6%) malicious applets (according to Virustotal)
as benign and 27 (2.1%) benign applets (according to Virus-
total) as malicious.
Manual examination of the instances with diﬀerent classi-
ﬁcations revealed interesting results, both for the false pos-
itives and the false negatives. For the false positives, we
found that 19 of the 27 were clearly errors in the initial
classiﬁcation by Virustotal (that is, these 19 applets were
actually malicious but falsely labeled as benign by Virusto-
tal). That is, Jarhead was correct in labeling these applets
as bad.
Interestingly, one of these 19 samples was a ma-
licious applet that was stealing CPU cycles from the user,