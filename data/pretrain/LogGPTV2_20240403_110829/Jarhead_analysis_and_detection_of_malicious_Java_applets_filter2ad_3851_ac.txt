Even if an attacker manages to bypass the obfuscation features, their malware must still implement specific functionality to achieve its goal of infecting end users. This typically involves downloading and executing malicious code on the victim's PC, which requires breaking out of the Java sandbox. By detecting this behavior, which is essential for the attacker's success, we make it difficult for malicious code to evade our analysis while still achieving its objectives.

Previous systems [11, 13] for detecting web-based exploits also use features to identify obfuscation and malicious behaviors. However, the specific features differ significantly. The primary reason for these differences is that Jarhead employs static code analysis, whereas previous work on detecting malicious JavaScript and Flash relies on dynamic analysis. Consequently, our novel features assess the presence and frequency of certain functions, resources, code patterns, and artifacts across the entire code base. In contrast, dynamic systems typically count the occurrences of specific events or activities during program execution.

To evaluate the impact of our top ten features (Table 1) compared to the full feature set, we used a classifier with ten-fold cross-validation. We ran the classifier once with all features and then with only the top ten features on a total of 3,372 applets, combining all our datasets. The datasets are described in detail in the evaluation chapter, along with additional experiments. This experiment was conducted after the initial evaluations at the request of our reviewers. Out of the 3,372 applets, the classifier using only the top ten features misclassified 122 (3.6%) applets, while the full feature set misclassified only 35 (1.0%) applets. This indicates that while the top ten features perform reasonably well, the additional features significantly improve the results.

| Merit  | Attribute                         | Type        |
|--------|----------------------------------|-------------|
| 0.398  | Gets parameters                   | Behavior    |
| 0.266  | Functions per class               | Obfuscation |
| 0.271  | Number of instructions            | Obfuscation |
| 0.257  | Gets runtime                      | Behavior    |
| 0.254  | Lines of disassembly              | Obfuscation |
| 0.232  | Uses file output stream           | Behavior    |
| 0.220  | Percent unused methods            | Obfuscation |
| 0.211  | Longest string character count    | Obfuscation |
| 0.202  | McCabe complexity average         | Obfuscation |
| 0.197  | Calls execute function            | Behavior    |

**Table 1: The ten most useful features, sorted by average merit.**

In another experiment, we evaluated the performance of the obfuscation features and behavioral features separately. Using the same 3,372 applets and ten-fold cross-validation, we ran the classifier once with only the behavioral features and once with only the obfuscation features. Both sets of features performed reasonably well individually, with 119 (3.5%) and 150 (4.5%) misclassified applets, respectively. However, they achieved the best results when used together, with only 35 (1.0%) misclassified applets when all features were enabled.

### 5. EVALUATION

In this section, we evaluate our system to measure its effectiveness in detecting malicious applets. For our experiments, we used two different datasets: a manually-compiled dataset of applets (the manual set) and a set of samples provided by the authors of the Wepawet system (the Wepawet set).

#### 5.1 Results: Manual Dataset

The manual dataset includes samples from four sources: two online collections of applets, an archive of mostly-malicious applets, and a number of manually collected samples. We crawled two websites offering Java applets for web designers and downloaded 1,002 JAR files. Additionally, we obtained 357 JAR files of unknown toxicity from a malware research community site. We also manually searched security sites and used a crawler seeded with Google search results for "Java," "applet," and "malicious," yielding 1,495 JAR files, for a total of 2,854 samples. This diverse dataset aims to represent applets used in the wild without bias.

We first cleaned the manual dataset by removing duplicates, broken JAR files, and non-applets. After filtering, 2,095 files remained. To establish ground truth for building and evaluating our classifier, we submitted the entire dataset to VirusTotal, which uses 42 different antivirus products. VirusTotal found 1,721 (82.1%) files to be benign and 374 (17.9%) to be malicious. We built an initial classifier using these results and manually inspected samples where our classifier and VirusTotal disagreed. We decompiled the samples and checked for known exploits, download and execute behavior, and documentation. We corrected the classification of 61 (2.9%) applets, resulting in 381 malicious and 1,714 benign samples.

Next, we trained different classifiers on the manual dataset using ten-fold cross-validation. C4.5 decision trees, support vector machines, and Bayes classification showed comparable results, but decision trees were the most reliable and provided good explanatory capabilities. The decision tree classifier misclassified only 11 (0.5%) samples, with a false positive rate of 0.2% and a false negative rate of 0.3%.

Comparing our results to VirusTotal, our classifier had fewer false positives and false negatives, reducing the total number of misclassifications by a factor of six. An overview of the results for the manual dataset is given in Table 2.

| Classifier       | False Positives | False Negatives |
|------------------|-----------------|-----------------|
| VirusTotal (42 AVs) | 1.6%            | 1.3%            |
| Jarhead (10x cross-val.) | 0.2%            | 0.3%            |

**Table 2: Comparison of Jarhead and VirusTotal misclassifications - note that our ground truth is biased towards VirusTotal.**

**Discussion:**
We examined the 11 misclassified instances in more detail. One false positive was a potentially benign MIDlet capable of sending text messages to arbitrary numbers. Two false positives were triggered by applets showing suspicious behavior by writing local files and executing commands on the Windows command shell, but not intended to be malicious. The last false positive was an obfuscated applet performing calculations based on the current date without displaying results.

Two false negatives were applets containing incomplete (broken) exploits. While these applets would not successfully execute, their intent was malicious. We missed three additional malicious applets using reflection or loading additional code at runtime to protect against static analysis and exploiting CVE-2010-0094. The last two misclassified applets exploited CVE-2009-3869, a stack-based buffer overflow that is hard to detect as it is reachable from widely-used Java library functions.

Despite a few misclassifications, our system performs detection with high accuracy. Some incorrect cases are in a grey area, such as possibly benign applets that try to execute commands directly on the Windows command line and malicious applets with incomplete exploits.

We also analyzed the decision tree produced by our classifier. The most important features include both obfuscation and behavioral features. The top ten features include interaction with the runtime, the execute feature, and monitoring local file access. On the obfuscation side, size features, string features, and the number of functions per class were the most effective at predicting malicious behavior.

Removing the feature that checks for known vulnerable functions (which could be considered signature-based detection) increased the misclassification rate only marginally, by 0.17%. This confirms that many features work together to perform detection, and the system does not rely solely on the knowledge of specific, vulnerable functions. Furthermore, Jarhead can detect zero-day attacks, as demonstrated by its ability to identify new exploits without any adjustments.

#### 5.2 Results: Wepawet Dataset

To further test our classifier on real-world data, we collected JAR files from the Wepawet system, which allows users to upload suspicious URLs for analysis. Since Wepawet does not currently analyze JAR files, we expected to find some applets submitted by users. The authors provided us with 1,551 JAR files, which we cleaned, leaving 1,275 applets. Using VirusTotal, we found 413 (32.4%) applets to be benign and 862 (67.6%) to be malicious. Running our classifier on this set, we assigned the same verdict to 1,189 (93.3%) samples, while 86 (6.7%) samples were classified differently. Specifically, we classified 59 (4.6%) malicious applets (according to VirusTotal) as benign and 27 (2.1%) benign applets (according to VirusTotal) as malicious.

Manual examination revealed that 19 of the 27 false positives were actually malicious but falsely labeled as benign by VirusTotal. One of these 19 samples was a malicious applet stealing CPU cycles from the user.