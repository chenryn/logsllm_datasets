While Bloom filters are comparatively small even when inserting a large number 
of  entries,  choosing  the  optimal  size  of  a  Bloom  filter  is  nontrivial,  since  Ana-
gram  is  not  aware  of  a  site’s  distribution  (and  the  number  of  unique  n-grams) 
before building its model. Additionally, a Bloom filter’s size cannot be dynami-
cally resized, as the hash values cannot be recomputed without the original under-
lying training data.5 
A large Bloom filter will waste memory, but small Bloom filters saturate more 
quickly, yielding higher false positive rates. It is worth pointing out that “large” is 
relative; a 24-bit Bloom filter is capable of holding 224/nh elements, where nh repre-
sents  the  number  of  hash  functions  used,  in  only  2MB  of  memory,  e.g.,  each  n-
gram inserted uses about 2.7 bits when 3 hash functions are used. Additionally, we 
can  use  traditional  compression  methods  (e.g.,  LZW)  for  storing  a  sparse  Bloom 
filter, which significantly reduces storage and transport costs. As discussed later in 
this paper, our experiments anecdotally suggest this Bloom filter is large enough for 
at least 5-grams, assuming a mostly textual distribution. 
The presence of binary data does significantly increase Bloom filter requirements; 
if  allocating  extra  initial  memory  is  undesirable,  a  “layered”  approach  can  be  em-
ployed,  where  new  Bloom  filters  are  created  on  demand  as  previous  Bloom  filters 
saturate, with a small constant-time overhead. It should be evident that Bloom filters 
can be trivially merged via bitwise ORing and compared via bitwise ANDing. 
2.3    Learning Issues 
The  huge  feature  space  of  higher  order  n-grams  makes  estimating  the  frequency 
distribution  infeasible.  In  the  case  of  representing  distinct  n-grams,  we  pose  the 
following questions: how long do we need to train before deploying the sensor in 
detection mode?  Further, how well can Anagram handle noisy training data since 
the model is binary-based? 
5  While  the  training data  can  be  kept,  we  do not  consider  this practical  for  actual  deploy-
ment, especially if techniques like incremental and run-time training are used. 
234 
K. Wang, J.J. Parekh, and S.J. Stolfo 
2.3.1    When Is the Model Well Trained? 
Since there are many distinct n-grams, many of which may take days or weeks to 
see, when can we say the model is well trained and ready to use? We first check the 
likelihood  of  seeing  a  new  n-gram  with  additional  training.  Figure  2  shows  the 
percentage of the  new distinct n-grams out of every 10,000 content packets  when 
we train for up to 500 hours of traffic data. The key observation is that during the 
initial training period, one should expect to see many distinct n-grams. Over time, 
however,  fewer  distinct  “never  before  seen”  n-grams  should  be  observed.  Hence, 
for a given value of n, a particular site should exhibit some rate of observing “new” 
distinct n-grams within its “normal” content flow. By estimating this rate, we can 
estimate  how  well  the  Anagram  model  has  been  trained.  When  the  likelihood  of 
seeing new n-grams in normal traffic is stable and low, the model is stable; we can 
then use the rate to help detect the attacks, as they should contain a higher percent-
age of unseen n-grams. Figure 3 plots the false positive rates of different models, 
varying in n-gram size and length of training time, when tested on the 72 hours of 
traffic immediately following the training data. 
3-grams
5-grams
7-grams
0.01
0.009
0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
s
m
a
r
g
-
n
w
e
n
i
g
n
e
e
s
f
o
d
o
o
h
i
l
e
k
L
i
0
0
50
100
150
200
250
300
350
400
450
Per 10000 content packets
Fig. 2. The likelihood of seeing new n-grams 
as training time increases 
t
e
a
R
n
o
i
t
c
e
t
e
D
%
0
0
1
n
e
h
w
)
%
(
e
t
a
R
e
v
i
t
i
s
o
P
e
s
a
F
l
3-grams
5-grams
7-grams
0.03
0.025
0.02
0.015
0.01
0.005
1
2
3
4
5
6
7
0
0
Training Dataset Length (in days)
Fig. 3. False positive rate (with 100% detec-
tion rate) as training time increases 
From these plots, we can see that as the training time increases, the false positive 
rate generally goes down as the model is more complete. After some point, e.g. 4 
days’ in figure 3, there is no further significant gain, and the FP rate is sufficiently 
low. Higher order n-grams need a longer training time to build a good model, so 7-
grams  display  a  worse  result  than  5-grams  given  the  same  amount  of  training. 
While the 3-gram model is likely more complete with the same amount of training, 
it  scores  significantly  worse:  3-gram  false  positives  do  not  stem  from  inadequate 
training, but rather because 3-grams are  not long enough to distinguish  malicious 
byte sequences from normal ones.6  
In  theory,  Anagram  should  always  improve  with  further  training  –  if  we  can 
guarantee a clean training dataset, which is crucial for the binary-based approach. 
6  We hypothesize, though, that as the number of new 3-grams converges very rapidly, that 
the rate of detection of unique 3-grams during testing may be a good sign of an attack.  In 
general, anomalous content may be measurable as a function of the size of the gram de-
tected that was not seen during training. 
                                                                     Anagram: A Content Anomaly Detector         235 
However, obtaining clean training data is not an easy task in practice. During our 
experiments, increased training eventually crosses a threshold where the false posi-
tive rate starts increasing, even if the training traffic has been filtered for all known 
attacks. The binary-based approach has significant advantages in speed and mem-
ory, but it’s not tolerant of noisy training, and manual cleanup is infeasible for large 
amounts  of  training  data.  We  therefore  introduce  semi-supervised  training  in  the 
next section to help Anagram be more robust against noisy data. 
2.3.2    Semi-supervised Learning 
The  binary-based  approach  is  simple  and  memory  efficient,  but  too  sensitive  to 
noisy training data. One form of supervised training is quite simple. We utilize the 
signature content of Snort rules obtained from [12] and (a collection of about 500) 
virus samples to precompute a known “bad content model”. We build a bad content 
Bloom  filter  containing  the  n-grams  that  appear  in  the  two  collections,  using  all 
possible n that we may eventually train on (e.g., n=2~9 in our experiments). This 
model can be incrementally updated when new signatures have been released.  
It’s important to note, however, that signatures and viruses often contain some 
normal  n-grams,  e.g.,  the  ‘GET’  keyword  for  HTTP  exploits.  To  remove  these 
normal  n-grams,  we  can  maintain  a  small,  known-clean  dataset  used  to  exclude 
normal  traffic  when  generating  the  bad  content  BF.  This  helps  to  exclude,  as  a 
minimum, the most common normal n-grams from the bad content model. 
In one experiment, we used 24 hours of clean traffic to filter out normal n-grams 
from  the  bad  content  BF.  Figure  4  shows  the  distribution  of  normal  packets  and 
attack packets that match against the bad content model. The X axis represents the 
“matching  score”,  the  percentage  of  the  n-grams  of  a  packet  that  match  the  bad 
content model, while the Y axis shows the percentage of packets whose matching 
score falls within that score range. The difference between normal and attack pack-
ets  is  obvious;  where  the  normal  traffic  barely  matches  any  of  the  bad  content 
model, attack packets have a much higher percentage, so that we can reliably apply 
the  model  for  accurate  detection.  The  resulting  bad  content  BF  contains  approxi-
mately 46,000 Snort n-grams and 30 million virus n-grams (for n=2…9). 
s
t
e
k
c
a
p
e
h
t
f
t
o
e
g
a
n
e
c
r
e
P
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
Normal Content Packets 
20
80
Bad Content Model Matching Score
60
40
0.25
0.2
0.15
0.1
0.05
s
t
e
k
c
a
p
e
h
t
f
o
e
g
a
t
n
e
c
r
e
P
0
0
100
Attack Packets 
20
80
Bad Content Model Matching Score
40
60
100
Fig. 4. Distribution of bad content scores for normal packets (left) and attack packets (right) 
The  “bad  content  model”  is  used  during  both  training  and  detection.  During 
training,  the  incoming  data  stream  is  first  filtered  through  Snort  to  ensure  it  is 
free  of  known,  old  attacks.  Packets  are  then  compared  against  the  bad  content 
236 
K. Wang, J.J. Parekh, and S.J. Stolfo 
model;  any  n-gram  that  matches  the  bad  content  model  is  dropped.  The  whole 
packet  is  also  dropped  if  it  matches  too  many  n-grams  from  the  bad  content 
model  –  as  new  attacks  often  reuse  old  exploit  code  –  to  avoid  modeling  new 
malicious n-grams. In our experiment, we established a 5% bad n-gram threshold 
before  ignoring  a  training  packet.  While  this  is  rather  strict,  ignoring  a  packet 