title:A Closer Look Tells More: A Facial Distortion Based Liveness Detection
for Face Authentication
author:Yan Li and
Zilong Wang and
Yingjiu Li and
Robert H. Deng and
Binbin Chen and
Weizhi Meng and
Hui Li
A Closer Look Tells More: A Facial Distortion Based Liveness
Detection for Face Authentication
Yan Li
Xidian University
Zilong Wang
Xidian University
Yingjiu Li
Singapore Management University
Advanced Digital Science Center
PI:EMAIL
PI:EMAIL
PI:EMAIL
Robert Deng
Binbin Chen
Weizhi Meng
Singapore Management University
PI:EMAIL
Advanced Digital Science Center
PI:EMAIL
Technical University of Denmark
PI:EMAIL
Hui Li
Xidian University
PI:EMAIL
ABSTRACT
Face authentication is vulnerable to media-based virtual face
forgery (MVFF) where adversaries display photos/videos or
3D virtual face models of victims to spoof face authentication
systems. In this paper, we propose a liveness detection mech-
anism, called FaceCloseup, to protect the face authentication
on mobile devices. FaceCloseup detects MVFF-based attacks
by analyzing the distortion of face regions in a userâ€™s closeup
facial videos captured by built-in camera on mobile device. It
can detect MVFF-based attacks with an accuracy of 99.48%.
CCS CONCEPTS
â€¢ Security and privacy â†’ Biometrics; Usability in secu-
rity and privacy.
KEYWORDS
Liveness detection, perspective distortion, face authentication
ACM Reference Format:
Yan Li, Zilong Wang, Yingjiu Li, Robert Deng, Binbin Chen,
Weizhi Meng, and Hui Li. 2019. A Closer Look Tells More: A Facial
Distortion Based Liveness Detection for Face Authentication. In
ACM Asia Conference on Computer and Communications Security
(AsiaCCS â€™19), July 9â€“12, 2019, Auckland, New Zealand. ACM,
New York, NY, USA, 6 pages. https://doi.org/10.1145/3321705.
3329850
INTRODUCTION
1
Most of existing face authentication systems are vulnerable to
media-based virtual face forgery (MVFF) where an adversary
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
AsiaCCS â€™19, July 9â€“12, 2019, Auckland, New Zealand
Â© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6752-3/19/07. . . $15.00
https://doi.org/10.1145/3321705.3329850
displays a photo/video or a 3D virtual face model of a victim.
Liveness detection has been proposed to counter MVFF-
based attacks. Some liveness detection thwarts photo-based
attacks based on usersâ€™ facial motions or expressions, such as
eye blink and head rotation [6]. But such liveness detection
approaches are still vulnerable to video-based attacks where
an adversary replays a pre-recorded face video.
Two recent liveness detection approaches were proposed
to defeat MVFF-based attacks, which are FaceLive and Face
Flashing. FaceLive performs liveness detection by examining
the consistency between a captured face video and device
movement data [8]. FaceLive can detect photo-based attacks
and video-based attacks but is still subject to 3D virtual face
model-based attacks. Face Flashing [12] analyzes reflection
light from a face to detect MVFF-based attacks. However,
Face Flashing incurs significant network traffic and raise
privacy concern because it requires cloud computing.
In this work, we propose FaceCloseup, a facial distortion-
based liveness detection mechanism to protect face authenti-
cation on mobile devices against MVFF-based attacks. Face-
Closeup can detect not only photo/video based attacks, but
also 3D virtual face model-based attacks. FaceCloseup only
requires a generic front-facing camera on mobile devices but
no specific usage settings such as controlled lighting and
sending facial videos to remote server. FaceCloseup is thus
suitable for on-device liveness detection and can be deployed
on commodity mobile phones. Empowered with a CNN-based
classification algorithm, FaceCloseup determines the liveness
of a face based on facial distortion changes in a facial video.
To thwart MVFF-based attacks, FaceCloseup detects 3D
characteristics of a live userâ€™s face by analyzing the changes of
distortion in facial video frames. The distortion of the userâ€™s
face in the video is a common phenomenon in photography
especially when the camera is close to the face. The distortion
is mainly caused by the uneven 3D surface of the face. Facial
regions in the video frames are displayed in different scales.
We collect real-world facial photo and video data from
legitimate authentication requests and MVFF-based attacks.
We mimic the 3D virtual face model-based attack using the
Session 3B: Learning and AuthenticationAsiaCCS â€™19, July 9â€“12, 2019, Auckland, New Zealand241state-of-the-art 3D face reconstruction technique [5] which
synthesizes facial photos with facial distortion. Our results
show that FaceCloseup can detect MVFF-based attacks with
an accuracy of 99.48%.
2 THREAT MODEL
The media-based virtual face forgery (MVFF) enables an
adversary to forge usersâ€™ face biometrics based on their facial
photos or videos. The adversary may display the forged face
to spoof face authentication and therefore pose a serious
threat against face authentication systems.
In the photo-based attack and video-based attack, an
adversary replays a userâ€™s pre-recorded facial photos and
videos which the adversary may obtain from online, such
as online social networks. The 3D virtual face model-based
attack is more complicated and powerful where an adversary
builds up a 3D virtual face model for a user based on the
userâ€™s facial photos and videos. The adversary can synthesize
facial videos with facial motions and/or expressions so as to
spoof face authentication system.
The effectiveness of the photo/video based attacks usu-
ally depends on the quality and availability of the victimâ€™s
facial photos/videos, which may be mitigated by extra facial
motions and expressions. The 3D virtual face model-based
attack poses significant risks to face authentication systems
because the adversary can display a 3D virtual face model
of the victim and synthesize the required facial motions and
expressions in real time. The 3D virtual face model can be
estimated by the adversary based on the victimâ€™s face photos
and videos regardless of facial movements and expressions [1].
It is important for liveness detection to defend against the
3D virtual face model-based attack.
Our proposed liveness detection mechanism, FaceCloseup,
aims to prevent MVFF-based attacks including the pho-
to/video based attacks and the 3D virtual face model-based
attacks. In MVFF-based attacks, it is assumed that an adver-
sary cannot obtain a userâ€™s pre-recorded facial photos/videos
taken within 30cm from the userâ€™s face. It is difficult for an
adversary to directly capture the users closeup facial photo-
s/videos without the usersâ€™ awareness. In comparison, the
user is more likely to leak his/her facial photos and videos
taken no shorter than 30cm from the face by online sharing
such as sharing selfie photos and videos in online social net-
works and video calls such as video chat or video conference.
The adversary may access these facial photos and videos.
3 DESIGN
FaceCloseup includes three modules which are Video Frame
Selector (VFS), Distortion Feature Extractor (DFE), and
Liveness Classifier (LC). The VFS module takes facial video
as input and selects multiple frames from the facial video
based on the size of the face in the frames. With the extracted
frames, the DFE module detects a number of facial landmarks
in each frame and calculate features about the facial distortion
changes among different frames. At last, the LC module
utilizes a classification algorithm to distinguish a real face
from a forged face in MVFF-based attacks.
As a mobile device moves towards or away from a userâ€™s
face, the camera on the device firstly captures a video which
includes a number of frames about the userâ€™s face taken at
different distances between the camera and the face. The size
of the faces in the video frames changes due to the movement.
Using Viola-Jones face detection algorithm [13], the Video
Frame Selector (VFS) extracts and selects a sequence of ğ¾
frames (ğ‘“1, ğ‘“2, ..., ğ‘“ğ¾ ) in the video based on the detected face
size (ğ‘ ğ‘§1, ğ‘ ğ‘§2, ..., ğ‘ ğ‘§ğ¾ ) where ğ‘ ğ‘§ğ‘– âˆˆ (ğ‘›ğ‘ğ‘–ğ‘™, ğ‘›ğ‘ğ‘–ğ‘¢).
Secondly, with (ğ‘“1, ğ‘“2, ..., ğ‘“ğ¾ ) as input, DFE calculates the
geometric distances between different facial landmarks in
each frame and uses them as features for detecting distortion
changes in the facial video. We use the supervised descent
method (SDM) to detect 66 facial landmarks from each
frame [15]. The 66 facial landmarks are located at various
regions of a face, including chin (17), eyebrows (10), nose
stem (4), below nose (5), eyes (12), and lips (18), which
are shown in Figure 1. The facial landmarks are denoted as
(ğ‘1, ğ‘2, ..., ğ‘66) where ğ‘ğ‘– = (ğ‘¥ğ‘–, ğ‘¦ğ‘–) is the coordinate.
Figure 1: 66 facial landmarks
ğ‘‘ =âˆšï¸€(ğ‘¥ğ‘  âˆ’ ğ‘¥ğ‘¡)2 + (ğ‘¦ğ‘  âˆ’ ğ‘¦ğ‘¡)2, where ğ‘ , ğ‘¡ âˆˆ {1, 2, ..., 66} and
In order to capture facial distortion, we calculate the
distance between any two facial landmarks ğ‘ğ‘  and ğ‘ğ‘¡ as
ğ‘  Ì¸= ğ‘¡. The 66 facial landmarks in each frame yield 2145
pairwise distances ğ‘‘1, ğ‘‘2, ..., ğ‘‘2145. Assuming the size of a
detected face in a frame is ğ‘¤ in width and â„ in height,
a geometric vector about the detected face is formed as
ğ‘”ğ‘’ğ‘œ = (ğ‘‘1, ğ‘‘2, ..., ğ‘‘2145, ğ‘¤, â„). Then we calculate relative dis-
tances by normalizing the geometric vector of each frame
according to a base facial image, which is registered by a user
in a registration phase. The geometric vector for the base
image is calculated as ğ‘”ğ‘’ğ‘œğ‘ = (ğ‘‘ğ‘1, ğ‘‘ğ‘2, ..., ğ‘‘ğ‘2145, ğ‘¤ğ‘, â„ğ‘). For
each selected frame ğ‘“ğ‘–, we calculate a relative geometric vector
ğ‘Ÿğ‘–ğ‘œğ‘– = (ğ‘Ÿğ‘–,1, ğ‘Ÿğ‘–,2, ..., ğ‘Ÿğ‘–,2145, ğ‘Ÿğ‘–,ğ‘¤, ğ‘Ÿğ‘–,â„), where ğ‘Ÿğ‘–,ğ‘— = ğ‘‘ğ‘–,ğ‘—/ğ‘‘ğ‘ğ‘— for
ğ‘— = 1, 2, ..., 2145, ğ‘Ÿğ‘–,ğ‘¤ = ğ‘¤ğ‘–/ğ‘¤ğ‘, and ğ‘Ÿğ‘–,â„ = â„ğ‘–/â„ğ‘. The facial
distortion in ğ¾ selected frames is represented by a ğ¾ Ã— 2147
matrix ğ¹ ğ·.
Session 3B: Learning and AuthenticationAsiaCCS â€™19, July 9â€“12, 2019, Auckland, New Zealand242Thirdly, LC module takes ğ¹ ğ· as input and uses a classifi-
cation algorithm to determine whether ğ¹ ğ· is taken from a
real face or a forged face from MVFF-based attacks. Due to
high dimension of matrix ğ¹ ğ·, convolutional neural network
(CNN) is customized in the LC module including 2 convolu-
tion layers, 2 pooling layers, 2 fully connected layers, and
1 output layer. Given a ğ¾ Ã— 2147 feature matrix ğ¹ ğ·, the
convolution layer ğ¶ğ‘œğ‘›ğ‘£1 computes a tensor matrix ğ‘‡ ğ‘€â€²
1. In
order to achieve nonlinear properties without affecting the
receptive fields in the convolution layer ğ¶ğ‘œğ‘›ğ‘£1, a rectified
linear unit (ReLU) is used as activation function over ğ‘‡ ğ‘€â€²
1
and outputs a tensor matrix ğ‘‡ ğ‘€1. The ReLU is formed as
ğ‘“ (ğ‘¥) = max(0, ğ‘¥). The pooling layer ğ‘ƒ ğ‘œğ‘œğ‘™1 performs a non-
linear downsampling on ğ‘‡ ğ‘€1. The convolution layer ğ¶ğ‘œğ‘›ğ‘£2
and the pooling layer ğ‘ƒ ğ‘œğ‘œğ‘™2 perform the same operations as
ğ¶ğ‘œğ‘›ğ‘£1 and ğ‘ƒ ğ‘œğ‘œğ‘™1 in the third and fourth steps, respective-