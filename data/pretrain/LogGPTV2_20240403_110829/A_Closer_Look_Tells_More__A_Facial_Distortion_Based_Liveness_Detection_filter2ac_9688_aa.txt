title:A Closer Look Tells More: A Facial Distortion Based Liveness Detection
for Face Authentication
author:Yan Li and
Zilong Wang and
Yingjiu Li and
Robert H. Deng and
Binbin Chen and
Weizhi Meng and
Hui Li
A Closer Look Tells More: A Facial Distortion Based Liveness
Detection for Face Authentication
Yan Li
Xidian University
Zilong Wang
Xidian University
Yingjiu Li
Singapore Management University
Advanced Digital Science Center
PI:EMAIL
PI:EMAIL
PI:EMAIL
Robert Deng
Binbin Chen
Weizhi Meng
Singapore Management University
PI:EMAIL
Advanced Digital Science Center
PI:EMAIL
Technical University of Denmark
PI:EMAIL
Hui Li
Xidian University
PI:EMAIL
ABSTRACT
Face authentication is vulnerable to media-based virtual face
forgery (MVFF) where adversaries display photos/videos or
3D virtual face models of victims to spoof face authentication
systems. In this paper, we propose a liveness detection mech-
anism, called FaceCloseup, to protect the face authentication
on mobile devices. FaceCloseup detects MVFF-based attacks
by analyzing the distortion of face regions in a user’s closeup
facial videos captured by built-in camera on mobile device. It
can detect MVFF-based attacks with an accuracy of 99.48%.
CCS CONCEPTS
• Security and privacy → Biometrics; Usability in secu-
rity and privacy.
KEYWORDS
Liveness detection, perspective distortion, face authentication
ACM Reference Format:
Yan Li, Zilong Wang, Yingjiu Li, Robert Deng, Binbin Chen,
Weizhi Meng, and Hui Li. 2019. A Closer Look Tells More: A Facial
Distortion Based Liveness Detection for Face Authentication. In
ACM Asia Conference on Computer and Communications Security
(AsiaCCS ’19), July 9–12, 2019, Auckland, New Zealand. ACM,
New York, NY, USA, 6 pages. https://doi.org/10.1145/3321705.
3329850
INTRODUCTION
1
Most of existing face authentication systems are vulnerable to
media-based virtual face forgery (MVFF) where an adversary
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
AsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6752-3/19/07. . . $15.00
https://doi.org/10.1145/3321705.3329850
displays a photo/video or a 3D virtual face model of a victim.
Liveness detection has been proposed to counter MVFF-
based attacks. Some liveness detection thwarts photo-based
attacks based on users’ facial motions or expressions, such as
eye blink and head rotation [6]. But such liveness detection
approaches are still vulnerable to video-based attacks where
an adversary replays a pre-recorded face video.
Two recent liveness detection approaches were proposed
to defeat MVFF-based attacks, which are FaceLive and Face
Flashing. FaceLive performs liveness detection by examining
the consistency between a captured face video and device
movement data [8]. FaceLive can detect photo-based attacks
and video-based attacks but is still subject to 3D virtual face
model-based attacks. Face Flashing [12] analyzes reflection
light from a face to detect MVFF-based attacks. However,
Face Flashing incurs significant network traffic and raise
privacy concern because it requires cloud computing.
In this work, we propose FaceCloseup, a facial distortion-
based liveness detection mechanism to protect face authenti-
cation on mobile devices against MVFF-based attacks. Face-
Closeup can detect not only photo/video based attacks, but
also 3D virtual face model-based attacks. FaceCloseup only
requires a generic front-facing camera on mobile devices but
no specific usage settings such as controlled lighting and
sending facial videos to remote server. FaceCloseup is thus
suitable for on-device liveness detection and can be deployed
on commodity mobile phones. Empowered with a CNN-based
classification algorithm, FaceCloseup determines the liveness
of a face based on facial distortion changes in a facial video.
To thwart MVFF-based attacks, FaceCloseup detects 3D
characteristics of a live user’s face by analyzing the changes of
distortion in facial video frames. The distortion of the user’s
face in the video is a common phenomenon in photography
especially when the camera is close to the face. The distortion
is mainly caused by the uneven 3D surface of the face. Facial
regions in the video frames are displayed in different scales.
We collect real-world facial photo and video data from
legitimate authentication requests and MVFF-based attacks.
We mimic the 3D virtual face model-based attack using the
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand241state-of-the-art 3D face reconstruction technique [5] which
synthesizes facial photos with facial distortion. Our results
show that FaceCloseup can detect MVFF-based attacks with
an accuracy of 99.48%.
2 THREAT MODEL
The media-based virtual face forgery (MVFF) enables an
adversary to forge users’ face biometrics based on their facial
photos or videos. The adversary may display the forged face
to spoof face authentication and therefore pose a serious
threat against face authentication systems.
In the photo-based attack and video-based attack, an
adversary replays a user’s pre-recorded facial photos and
videos which the adversary may obtain from online, such
as online social networks. The 3D virtual face model-based
attack is more complicated and powerful where an adversary
builds up a 3D virtual face model for a user based on the
user’s facial photos and videos. The adversary can synthesize
facial videos with facial motions and/or expressions so as to
spoof face authentication system.
The effectiveness of the photo/video based attacks usu-
ally depends on the quality and availability of the victim’s
facial photos/videos, which may be mitigated by extra facial
motions and expressions. The 3D virtual face model-based
attack poses significant risks to face authentication systems
because the adversary can display a 3D virtual face model
of the victim and synthesize the required facial motions and
expressions in real time. The 3D virtual face model can be
estimated by the adversary based on the victim’s face photos
and videos regardless of facial movements and expressions [1].
It is important for liveness detection to defend against the
3D virtual face model-based attack.
Our proposed liveness detection mechanism, FaceCloseup,
aims to prevent MVFF-based attacks including the pho-
to/video based attacks and the 3D virtual face model-based
attacks. In MVFF-based attacks, it is assumed that an adver-
sary cannot obtain a user’s pre-recorded facial photos/videos
taken within 30cm from the user’s face. It is difficult for an
adversary to directly capture the users closeup facial photo-
s/videos without the users’ awareness. In comparison, the
user is more likely to leak his/her facial photos and videos
taken no shorter than 30cm from the face by online sharing
such as sharing selfie photos and videos in online social net-
works and video calls such as video chat or video conference.
The adversary may access these facial photos and videos.
3 DESIGN
FaceCloseup includes three modules which are Video Frame
Selector (VFS), Distortion Feature Extractor (DFE), and
Liveness Classifier (LC). The VFS module takes facial video
as input and selects multiple frames from the facial video
based on the size of the face in the frames. With the extracted
frames, the DFE module detects a number of facial landmarks
in each frame and calculate features about the facial distortion
changes among different frames. At last, the LC module
utilizes a classification algorithm to distinguish a real face
from a forged face in MVFF-based attacks.
As a mobile device moves towards or away from a user’s
face, the camera on the device firstly captures a video which
includes a number of frames about the user’s face taken at
different distances between the camera and the face. The size
of the faces in the video frames changes due to the movement.
Using Viola-Jones face detection algorithm [13], the Video
Frame Selector (VFS) extracts and selects a sequence of 𝐾
frames (𝑓1, 𝑓2, ..., 𝑓𝐾 ) in the video based on the detected face
size (𝑠𝑧1, 𝑠𝑧2, ..., 𝑠𝑧𝐾 ) where 𝑠𝑧𝑖 ∈ (𝑛𝑝𝑖𝑙, 𝑛𝑝𝑖𝑢).
Secondly, with (𝑓1, 𝑓2, ..., 𝑓𝐾 ) as input, DFE calculates the
geometric distances between different facial landmarks in
each frame and uses them as features for detecting distortion
changes in the facial video. We use the supervised descent
method (SDM) to detect 66 facial landmarks from each
frame [15]. The 66 facial landmarks are located at various
regions of a face, including chin (17), eyebrows (10), nose
stem (4), below nose (5), eyes (12), and lips (18), which
are shown in Figure 1. The facial landmarks are denoted as
(𝑝1, 𝑝2, ..., 𝑝66) where 𝑝𝑖 = (𝑥𝑖, 𝑦𝑖) is the coordinate.
Figure 1: 66 facial landmarks
𝑑 =√︀(𝑥𝑠 − 𝑥𝑡)2 + (𝑦𝑠 − 𝑦𝑡)2, where 𝑠, 𝑡 ∈ {1, 2, ..., 66} and
In order to capture facial distortion, we calculate the
distance between any two facial landmarks 𝑝𝑠 and 𝑝𝑡 as
𝑠 ̸= 𝑡. The 66 facial landmarks in each frame yield 2145
pairwise distances 𝑑1, 𝑑2, ..., 𝑑2145. Assuming the size of a
detected face in a frame is 𝑤 in width and ℎ in height,
a geometric vector about the detected face is formed as
𝑔𝑒𝑜 = (𝑑1, 𝑑2, ..., 𝑑2145, 𝑤, ℎ). Then we calculate relative dis-
tances by normalizing the geometric vector of each frame
according to a base facial image, which is registered by a user
in a registration phase. The geometric vector for the base
image is calculated as 𝑔𝑒𝑜𝑏 = (𝑑𝑏1, 𝑑𝑏2, ..., 𝑑𝑏2145, 𝑤𝑏, ℎ𝑏). For
each selected frame 𝑓𝑖, we calculate a relative geometric vector
𝑟𝑖𝑜𝑖 = (𝑟𝑖,1, 𝑟𝑖,2, ..., 𝑟𝑖,2145, 𝑟𝑖,𝑤, 𝑟𝑖,ℎ), where 𝑟𝑖,𝑗 = 𝑑𝑖,𝑗/𝑑𝑏𝑗 for
𝑗 = 1, 2, ..., 2145, 𝑟𝑖,𝑤 = 𝑤𝑖/𝑤𝑏, and 𝑟𝑖,ℎ = ℎ𝑖/ℎ𝑏. The facial
distortion in 𝐾 selected frames is represented by a 𝐾 × 2147
matrix 𝐹 𝐷.
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand242Thirdly, LC module takes 𝐹 𝐷 as input and uses a classifi-
cation algorithm to determine whether 𝐹 𝐷 is taken from a
real face or a forged face from MVFF-based attacks. Due to
high dimension of matrix 𝐹 𝐷, convolutional neural network
(CNN) is customized in the LC module including 2 convolu-
tion layers, 2 pooling layers, 2 fully connected layers, and
1 output layer. Given a 𝐾 × 2147 feature matrix 𝐹 𝐷, the
convolution layer 𝐶𝑜𝑛𝑣1 computes a tensor matrix 𝑇 𝑀′
1. In
order to achieve nonlinear properties without affecting the
receptive fields in the convolution layer 𝐶𝑜𝑛𝑣1, a rectified
linear unit (ReLU) is used as activation function over 𝑇 𝑀′
1
and outputs a tensor matrix 𝑇 𝑀1. The ReLU is formed as
𝑓 (𝑥) = max(0, 𝑥). The pooling layer 𝑃 𝑜𝑜𝑙1 performs a non-
linear downsampling on 𝑇 𝑀1. The convolution layer 𝐶𝑜𝑛𝑣2
and the pooling layer 𝑃 𝑜𝑜𝑙2 perform the same operations as
𝐶𝑜𝑛𝑣1 and 𝑃 𝑜𝑜𝑙1 in the third and fourth steps, respective-