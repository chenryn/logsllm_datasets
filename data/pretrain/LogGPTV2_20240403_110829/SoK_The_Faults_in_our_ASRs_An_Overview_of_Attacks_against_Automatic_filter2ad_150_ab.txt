security properties that relate to the integrity or conﬁdentiality
Figure 2: Example of a simple beam search decoder. The input
data (a) is fed into an RNN as shown in (b) which gives
the distribution of the outputs. In this example the possible
output characters are {l,o,c,k,-} shown in (c) and with their
distributions the beam search calculates the probability of
different sequences (d). Using those sequence probabilities,
the CTC beam search calculates the probability of different
outputs.
a) Convolutional Neural Network (CNN): A neural net-
work with more than one hidden layers is called a Deep Neural
Network (DNN). A special type of DNN is a CNN in which
each layer of the CNN is made up of a set of ﬁlters that are
convolved with the layer’s input to obtain a new representation
of the data. However, interpreting exactly what each hidden
layer has learned is an open problem [22].
CNNs have one major limitation: they take a ﬁxed-sized
input and produce a ﬁxed-sized output. This is ideal for
image recognition, where images can be down-sampled or
up-sampled to a speciﬁc size. However CNNs may be too
constrained for applications such as speech recognition, where
each input can be of arbitrary length and down-sampling can
result in loosing contextual information.
b) Recurrent Neural Network (RNN): An RNN over-
comes this limitation of CNNs. An RNN takes a variable-sized
input and can produce a variable-sized output. This is ideal for
VPSes, as speech input can be of variable length as a result
of varying pronunciation speeds of the speaker and different
sentences having a variable number of words. Additionally,
RNNs are designed to use contextual information, which is
important for VPSes. An RNN accumulates this context in
a hidden state, which is fed to the RNN as it processes the
following steps of an input sequence. The hidden state acts as
an internal memory unit by ‘remembering’ the information
the previous time steps. For example, consider the
about
phrase “Mary had a little
”. Intuitively, an RNN uses
information about the past to ﬁll in the missing word (i.e.,
“lamb”). This behavior is critical for ASRs as human speech
has a relatively consistent temporal structure: the word at time
tn generally depends on words spoken at tm where m < n.
4) Decoding: The decoding stage is shown in Figure 2.
For each 20 ms frame (Figure 2(a)),
the inference step
(Figure 2(b)) produces a probability distribution over all the
characters (Figure 2(c)). The model then produces a two-
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
732
OCLOOKLOKL(a)(b)(c)(d)(e)LO-OCKLO-OO-KLL-OK-KLK-KKKLOCKLOCKLOCKLOCKLOCKLOCKLOCK------LOCK--of the ML system [31]. The former encompasses poisoning
attacks at training time [32] and evasion attacks [33] at test
time. The latter may be concerned with the conﬁdentiality
of data [34] (often also delving into issues that relate to the
privacy of data [35], [36], [37]) or the conﬁdentiality of the
model itself with attacks including model extraction [38], [39].
We focus on evasion attacks, which are often instantiated
using maliciously crafted inputs known as adversarial sam-
ples [40]. Most of the literature on adversarial samples has
been written in the context of computer vision. In this realm,
adversarial samples are produced by introducing perturbations
that do not affect semantics of the image (as validated by
a human observer) but that cause machine learning models
to misclassify the image. All existing attacks in the audio
space are evasion attacks, which naturally results in our
systematization of knowledge addressing evasion attacks. We
do however discuss other types attacks in Section VIII.
1) Motivating Example: Consider the example of an ASR.
The goal of the adversary is to perturb an audio sample such
that an ASR and a human transcribe the same audio differently.
This perturbed audio sample, for example, can be music [41],
noise [7], silence [42] or even human speech [6]. This type of
attack can trick humans into thinking that the perturbed sample
is benign, but can force the voice-enabled home assistant
to execute illicit bank transactions, unlock smart doors, etc.
Similarly, in the case of SI, the attacker might want to perturb
an audio ﬁle so that the SI misidentiﬁes the speaker.
The goal of the adversary is to ﬁnd an algorithm capable
of producing perturbations imperceptible to a human listener,
yet still triggers the desired action from the voice assistant.
This would ensure that victims only realize they have been
attacked once the damage is done, if they realize at all.
2) Crafting Adversarial Samples: Attack algorithms craft
adversarial samples by performing gradient-based optimiza-
tion. Because it is possible to approximate gradients using
ﬁnite-difference methods sufﬁciently for the purpose of ﬁnding
adversarial samples [43], we describe this attack strategy as-
suming that the adversary has access to the model’s gradients.
The adversary solves an optimization problem whose ob-
jective is forcing the model to output a different label, subject
to constraints that ensure the modiﬁed input maintains the
semantics that correspond to the original label.
min(cid:107)δ(cid:107) l(f (x + δ), y) + c · (cid:107)δ(cid:107)
(1)
This formulation, due to Szegedy et al. [40], involves a loss
function l : x, y (cid:55)→ l(f (x), y) measuring how far the model f’s
predictions are from label y for input x. The adversary who’s
optimizing for a targeted transcription needs to ﬁnd a targeted
adversarial sample x∗ = x + δ for which model f erroneously
outputs the label y, which is chosen by the adversary and
differs from the correct label assigned to input x. If instead
the adversary is interested in crafting an untargeted adversarial
sample, they replace l(f (x), y) by its negation and set y to
be the correct label of x. This encourages the procedure to
ﬁnd an input x∗ that maximizes the model’s error, regardless
Figure 3: All iterative adversarial attacks follow the same
general algorithmic steps. (a) An input is passed to the model.
(b) The Target Model processes the input and outputs a label.
(c) If the label does not match the adversaries chosen label,
the input (c.i) and the label (c.ii) are passed to the perturbation
optimization algorithm (d). The algorithm produces a new
adversarial input. These steps are repeated until the Target
Model outputs an adversary chosen label.
of the label predicted by model f. The penalty c·(cid:107)δ(cid:107) loosely
translates the requirement that an input should not be perturbed
excessively, otherwise it will no longer retain semantics that
justify it belonging in its original ground truth class.
For non-convex models such as neural networks, this op-
timization is approximated. Fortunately for the adversary,
properties of neural networks that make them easy to train
with algorithms like stochastic gradient descent also result in
neural networks being easy to attack. In practice, this means
that the adversary can solve the optimization problem, formu-
lated in the previously discussed equation, using optimizers
commonly employed to train neural networks, such as gradient
descent [44] or some of its variants like L-BFGS [40] and
Adam [45]. This often takes the form of a procedure iteratively
modifying the input until it is misclassiﬁed (Figure 3).
3) Transferability: When an adversarial sample that was
crafted to be misclassiﬁed by Model A is also misclassiﬁed
by Model B, even though models A and B are different, we say
that the adversarial sample transfers from model A to B [46].
The two models A and B do not necessarily need to share the
same architecture and weights, nor do they need to be trained
using the same technique and training data. A hypothesis for
transferability is that there is a large volume of error space
found in different models, which leaves a large volume when
these error spaces intersect [47]. Transferability of adversarial
samples is a cornerstone of most attacks in the image domain,
because the property was observed to hold in even highly
dissimilar models [46], [48], [49], [50], [51]. An adversary
can perform an attack without having access to model B by
ﬁrst training a surrogate model A. The surrogate is trained by
querying the remote model B. Now the adversary can transfer
adversarial samples from model A to model B. This reduces
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
733
(b)(a)START(c.i)(d)END(c.ii)InputTarget ASRPerturbation OptimizationComparethe knowledge required for attack success.
III. ATTACKS AGAINST VPSES
One might assume it is possible to extend the existing
literature of attacks from the image space to the audio domain.
This is not the case due to several key differences. We detail
these differences next. They motivate our systematization of
knowledge effort tailored to the audio domain in general and
VPSes in particular.
A. Pre-processing pipeline
Speech recognition pipelines differ signiﬁcantly from those
employed in image recognition because they pre-process data
before it is analyzed by an ML model (Section II-B). While
image classiﬁers almost always operate directly on raw pixels
that encode the image, audio classiﬁers often rely on feature
extraction components that are not learned from the training
data. Instead, these features are extracted using signal process-
ing algorithms that are hard-coded by human experts [19].
This difference enables a whole new class of audio adversar-
ial samples that target discrepancies between feature extraction
techniques and the manner in which humans represent audio
signals. These attacks use signal processing techniques to
achieve the same evasion goals as the optimization-based
attacks presented in Section II-D. Because these VPS attacks
target feature extraction, they are less model-dependent and
work as well as evasion attacks in the image domain even
with limited access to the victim model [7].
B. Sequential models
Since audio is sequential in nature, ML models that solve
audio tasks are learned from architectures that are stateful in
order to capture contextual dependencies in an audio sample.
Recall that neural networks for image recognition have neu-
rons arranged in layers where connections between neurons
are only made between layers. In contrast, recurrent neural
architectures support connections between neurons in the same
layer in order to capture context. This enables the model to
propagate patterns identiﬁed in prior time steps as it progresses
temporally through the audio sample (Section II-B3).
Although recurrent neural architectures enable accurate
analysis of audio signals, the temporal dimension increases
the difﬁculty of performing a successful attack against VPSes.
To craft a perturbation,
the adversary now needs to con-
sider all
time steps simultaneously. This is referred to as
unrolling in the ML literature. While unrolling enables one
to backpropagate through time, it complicates optimization—
with failures such as exploding or vanishing gradients [52].
Because gradient-based algorithms rely on this optimization
to converge, ﬁnding adversarial samples is more challenging.
C. Discrete domains
Audio models had to be adapted to the discrete nature of
language. Whereas images deﬁne a continuous input domain
(because pixels are often represented as three ﬂoating point
values in ML systems), language is represented as a sequence
of tokens. Each token corresponds to a word included in the
language’s dictionary or a character of the language’s alphabet.
Furthermore, some audio models are “sequence-to-sequence”,
they take in a sequence of words (e.g., as audio) and produce
another sequence of words (e.g., as text). They must handle
the discrete nature of language both at their inputs and outputs.
This introduces particularities when modeling text or speech
to adapt to discrete domains. For instance, tokens are often
projected in a continuous space of smaller dimensionality than
the size of the discrete dictionary with a technique called word
embeddings [53]. This helps expose the relationship between
concepts expressed by different words.
For models defended with gradient masking [54] in the
image domain, the presence of non-differentiable operations
like beam search constrains the adversary to come up with
alternative operations that are differentiable. Otherwise, they
face a large increase in the cost of ﬁnding adversarial sam-
ples [55] or,
to brute
force [55]. This means that an attack might not always produce
an adversarial sample that transcribes to the desired target text.
in the worst case, have to resort
D. Statistical models beyond ML
Despite being replaced gradually with neural networks,
some components of voice-processing systems continue to
involve techniques like Hidden Markov Models [27]. There
is little work on attacking these statistical models, and for
this reason, it is not well understood to what extent they are
vulnerable to adversarial samples. More work needs to adapt
gradient-based optimization attacks that rely on algorithms de-
rived from the backpropagation algorithm to the expectation-
maximization algorithm used with Hidden Markov Models.
This opens a new dimension of attacks, because several
alternatives to Hidden Markov Models exist e.g., CTC [26].
IV. ATTACK THREAT MODEL TAXONOMY
We introduce the ﬁrst
to characterize the
unique contributions and open problems of evasion attacks in
the audio domain. We hope this framework will help draw fair
comparisons between works published on VPSes in the future.
threat model
A. Adversarial Goals
We can group the attacker’s goals into two categories of
attacks: untargeted and targeted attacks.
1) Untargeted: Here, the attacker wants the VPS to produce
any output that is different from the original output.
a) SI: The attacker’s goal is to force the SI to misidentify
the speaker of an audio sample. If the model identiﬁes the
speaker of an audio sample as anyone other than the original,
the attacker wins.
b) ASR: Similarly, the attacker aims to mislead the ASR
into assigning any transcription to the input other than the cor-
rect one. Consider the motivating example from Section II-D1,
the ASR transcribes the original audio as LOCK. The adversary
modiﬁes the audio sample to produce an adversarial sample. If
the ASR transcribes the adversarial sample as anything other
than LOCK, the attacker wins.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
734
2) Targeted: A targeted attack is one where the attacker
wants to get a speciﬁc response from the VPS.
a) SI: In the case of an SI, the attacker goal is to force
it to identify the attacker’s chosen person as the speaker of the
audio. For example, the attacker wants the SI to believe that
the audio belongs to Bob (or that the audio sample contains
silence), even though it does not.
b) ASR: Similarly, in the case of an ASR, the attacker
attempts to force the model to mistranscribe the input audio to
a chosen transcription. Consider the ASR from Section II-D1,
the attacker wants the ASR to assign the label OPEN to the
audio sample. Compared to untargeted attacks, targeted ones
can be harder to achieve when sequence pairs have closer
semantics than others. For example, it may be easier to force
the model
to mistranscribe the audio for the word LOCK
to CLOCK than to OPEN.
The only difference between targeted and untargeted attacks
is the attacker’s intentions towards the ASR/SI. If the intent
is to force a speciﬁc output, then the attack is targeted. If
however, the goal is to get any output, as long as it is not the
correct one, then the attack is untargeted.
B. Types of Adversarial Attacks
Current attacks can be categorized as follows:
1) Optimization Attacks:
a) Direct: Attacks use the information about the weights
of the model to compute the gradients. These are then used to
perturb the original input to move it in the decision space.
b) Indirect: These attacks generate adversarial samples
by gradient estimation. This is done, indirectly, by repeatedly
querying the target model [43]. Making enough queries to the
model will reveal the underlying gradients and will help the
attacker craft a working sample.
2) Signal Processing Attacks: These attacks use signal
processing techniques to achieve the same goals as traditional
optimization attacks. The signal processing attack techniques
exploit discrepancies between the human ear and feature
extraction algorithms. These attacks do not directly target the
inference component of the pipeline, but nevertheless can still
force the inference component to make incorrect predictions.
Leveraging this divergent behavior enables signal processing
attacks to be faster, more query efﬁcient, and less model
dependent compared to their optimization counterparts [7].
3) Miscellaneous Attacks: These are the attacks that do not
fall into any of the above two categories. These can exploit
the VPS by targeting the limitations of the hardware (e.g., the
microphone) or adding random noise. The range of miscella-
neous attacks is very broad. For example, there is extensive
work that has been done in the space of replay attacks [56],
[57], [58]. Here an attacker captures the voice of a victim
and attempts to exploit the VPS by replaying it the captured
audio. Similarly, there are a number of attacks that exploit the
inability of ASRs to distinguish between homophones: words
that are spelled differently but sound the same (e.g., ﬂour vs
ﬂower) [59], [60]. This happens when a single homophone is
passed alone to the ASR. For example, just passing the word
ﬂower/ﬂour, instead of as part of a sentence ‘which ﬂowers
grow in the summer’. Having a sentence provides context that
allows the ASR to successfully differentiate the homophones.
However, these attacks do not exploit any speciﬁc part of the
VPS pipeline and are therefore outside the scope of this paper.
C. Adversarial Knowledge
A stronger attack is one that assumes the adversary has little
knowledge of the VPS components. A VPS is comprised of a
few different components (Figure 1). For the purposes of sim-
plicity, we will group these components into ﬁve categories.
1) Component Categories: VPS components include task,
preprocessing, feature extraction,
inference, and decoding.
Information about a single component in one category, implies
knowledge of all other components in the same category.
• Task: The problem the model is trained to solve (e.g.,
speech transcription) and data the model was trained on.
For example, if an adversary is attacking an English
language transcription model, she is aware that the system
has been trained on an English language data-set.
• Preprocessing: A VPS ﬁrst preprocesses the audio (Fig-
ure 1(b)). Knowledge of this step includes information
about the algorithm and parameters being used for down-
sampling, noise reduction and low-pass ﬁltering.
• Feature Extraction: As discussed in Section II-B, a VPS
pipeline is made up of multiple signal processing steps
for feature extraction. Knowledge of this component
(Figure 1(c)), allows the attacker to infer the feature
extraction algorithm used by the VPS.
• Inference: This step outputs a probability distribution
over the labels (Figure 1(d)). This category includes
knowledge of the weights, type, number of the layers
and architecture of the model used for inference.
• Decoding: This step (Figure 1(e)) converts the probability
distribution into a human readable transcription. This cat-
egory includes information about the decoding algorithm
(e.g., beam search) and its parameters.
Having deﬁned the components, we now categorize the
different knowledge types (Table I):
a) White-Box: The attacker has perfect knowledge of all
the above categories, even though an attack may exploit only
a speciﬁc component. A white-box attack is the best case sce-
nario for the attacker. An example this access type is an open
source model (e.g., DeepSpeech-2 [26] or Kaldi [27] [25]).
b) Grey-Box: The attacker has knowledge of only a
subset of the categories. She might have complete knowledge
of some components, and limited or no knowledge of others.
An example is Azure Speaker Recognition model [61] [4].
The information about this system’s ﬁlter extraction and task
category is publicly available [62]. However,
there is no
information about the preprocessing, inference and decoding
components are unknown to the public.
c) Black-Box: The attacker has knowledge of only the
task category. For example, the attacker only knows that the
target system is an English language transcription model. This
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
735
Knowledge
White-Box
Grey-Box
Black-Box
No-Box
Task

?