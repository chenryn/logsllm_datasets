as in the recent CAB-ACME system [27]. A key challenge
is to avoid caching wildcarded ﬂow rules that match packets
for which higher-priority policy rules may exist in the Policy
Manager’s database. This is non-trivial for DFI because we
expect changes in the policy database over time, and these
policy rules may contain identiﬁers that must be mapped
during rule compilation for the SDN.
DFI Proxy. We design DFI to be independent of the
controller to ensure that the controller and its apps cannot
violate or interfere with the access control policy that DFI
is enforcing, either accidentally or intentionally, solving the
No-Bypass challenge mentioned earlier. To do this, we insert
a proxy between the switches and the SDN controller, a
technique used successfully by a variety of earlier SDN
security and reliability tools [28]–[32].
This proxy is responsible for smoothly interposing DFI’s
access control prior to the SDN controller and its applications
and is designed to avoid being a single point of failure
in the architecture. The state it maintains does not persist
between sessions and is relevant only to its particular switch
connections. Multiple proxies, as well as PCPs, can be used in
parallel in an SDN installation for reliability or performance.
The proxy is designed to accomplish two primary goals: isolate
rules inserted by DFI from those inserted by the controller,
ensuring that rules from DFI take precedence, and route
Packet-in messages properly.
To isolate rules inserted by DFI from those inserted by the
controller, the DFI Proxy takes advantage of the multiple ﬂow
tables available in OpenFlow 1.3 and above. In particular, it
reserves Table 0, the ﬁrst table incoming trafﬁc is checked
against, in each switch for DFI’s access control rules. This
is achieved by rewriting references to tables in all OpenFlow
messages. Section IV contains additional implementation de-
tails about this modiﬁcation. The end result is that DFI and
the SDN controller are writing rules into separate tables where
DFI’s rules take precedence. Since the DFI Proxy intercepts
the OpenFlow connections for all switches, DFI is aware of
the exact path each ﬂow takes.
The proxy also ensures that incoming Packet-in events
from switches are processed by DFI prior to being sent to the
controller. If a packet is denied by DFI, it is not forwarded to
the controller at all, ensuring that the controller is not poisoned
by inconsistent network state from blocked packets.
C. End-to-End Example
To demonstrate the end-to-end operation of DFI, we present
a simple example, which is depicted in Figure 2. Each endpoint
is running a Security Information and Event Management
(SIEM) collector, such as Splunk, that provides local authen-
tication events to a central indexer. In addition, each endpoint
is part of a Windows domain managed by an Active Directory
(AD) server providing DHCP and DNS services.
Consider the policy “When Alice is logged on, the computer
she is using can communicate with the email server. When she
is logged off, it cannot,” and the situation in which Alice logs
on, checks her email, and logs off the computer. The sequence
of events, denoted by the numbers in Figure 2, is presented as
a linear sequence for ease of exposition. Note that in practice
many of these events are concurrent with one another and will
occur asynchronously.
1 Alice-Laptop joins the AD domain and is assigned
an IP address by DHCP. This trafﬁc is permitted via default
allow rules.
2 The Hostname-IP and IP-MAC identiﬁer-binding sensors
connected to the DNS and DHCP services on the AD server
report these identiﬁer pairs associated with Alice-Laptop
to the Entity Resolution Manager.
3 Alice logs on via AD.
4 The log-on/log-off sensor connected to the SIEM collec-
tor notiﬁes the Entity Resolution Manager and Policy Decision
Point that Alice has logged on to Alice-Laptop.
5 The Policy Decision Point inserts a policy rule into the
Policy Manager that allows ﬂows from Alice to the email
server.
6 Alice tries to check her email. The ﬁrst packet of the
ﬂow is sent to the control plane via a Packet-in event.
7 The DFI proxy intercepts the Packet-in and sends it
to the Policy Compilation Point.
8 The Policy Compilation Point queries the Entity Reso-
lution Manager service to enrich the source and destination IP
and MAC addresses with associated hostnames and users.
9 The Policy Compilation Point queries the Policy Man-
ager for policy matching the enriched source and destination
identiﬁers. The Policy Manager returns an Allow decision for
the ﬂow.
10 The Policy Compilation Point creates an Allow-action
ﬂow rule that matches the packet in the Packet-in event
and sends it to switch.
11 The DFI Proxy forwards the Packet-in to the Open-
Flow controller, which installs forwarding rules in the switch.
12 Alice checks her email, then logs off the computer.
13 The log-on/log-off sensor sends a binding expiration
event to the Entity Resolution Manager and a log-off event to
the Policy Decision Point.
14 The Policy Decision Point revokes this policy, which
causes the Policy Manager to notify the Policy Compilation
Point that any ﬂow rules for this policy need to be removed.
15 The Policy Compilation Point removes any rules asso-
ciated with that policy from the switches.
Fig. 2: DFI Workﬂow for Example Authentication-Based Policy
bindings from their authoritative source, most of which are
in the data-plane. Figure 3 shows these bindings and their
authoritative source.
The MAC address to switch port binding is challenging
because traditional networks maintain this binding only im-
plicitly, via learning switches, as the last location from which
a MAC address sent trafﬁc. Since this binding is tied to the
physical location of network trafﬁc, we implement it as part
of the PCP in the control plane. This sensor ensures that each
MAC address is associated with at most one port on each
switch, and sends updates to the Entity Resolution Manager.
The username to hostname binding is challenging for a
similar reason. Active Directory (AD) and similar directory
services do not keep track of users who are currently logged
on, and therefore cannot be queried to obtain this information.
AD grants users a Kerberos Ticket-Granting-Ticket and does
not track subsequent log-on or log-off events. Local event logs
maintained by each endpoint contain some of this information;
however, there are multiple ways for users to authenticate in
an operating system like Windows, each of which generates
different events in the log. After experimenting with different
approaches, our sensor implementation maintains a current
count of running processes associated with a user, aggregated
from endpoint logs. This is calculated by monitoring process
creation and termination events. When the total number of
running processes for a user on a host is greater than zero,
the user is considered logged on and able to create ﬂows from
the host. When the total number is zero, they are considered
logged off the system. We collect this information and cen-
trally determine user log-on and log-off events using Splunk,
a widely used Security and Information Event Management
(SIEM) tool.
B. DFI Proxy
The DFI Proxy aims to enforce DFI’s access control without
altering the expected controller and application behavior for al-
lowed ﬂows. This becomes challenging because the controller
assumes it has full access to the switch’s tables and statistics.
As a result, the DFI Proxy must transparently isolate DFI’s
access control rules from the controller’s rules.
The proxy takes advantage of a feature added in OpenFlow
1.3 and later called ﬂow-table pipelining. Pipelining enables
Fig. 3: Authoritative Sources of Identiﬁer Bindings
IV. IMPLEMENTATION
DFI is implemented as a set of communicating servers
providing the core functions detailed in Figure 1. These servers
include one or more Policy Decision Points, a Policy Manager,
an Entity Resolution Manager, and a Policy Compilation
Point. These components are implemented in Java and use
a RabbitMQ message bus to communicate. The messages ex-
changed between these components are created using protocol
buffers [33] to remove language dependencies when extending
or building new components for the system. Both the Policy
Manager and the Entity Resolution Manager are backed by
MySQL databases that maintain a record of current policy
rules and current identiﬁer bindings.
The DFI Proxy is implemented as a Java application that
listens for new connections from switches; for each, it creates
and manages two additional connections to the controller and
PCP. The sockets may be optionally secured using TLS to
encrypt all exchanged OpenFlow messages. The Proxy and
PCP both use OpenFlowJ to parse these messages. Due to
DFI’s use of multiple ﬂow tables, DFI supports OpenFlow
versions 1.3 or later.
A. Identiﬁer-Binding Sensors
The Entity Resolution Manager relies on binding sensors
spread throughout the network to collect information on bind-
ings and ensure that it always has the most current information.
It is particularly important that this binding information always
come from authoritative sources to prevent attackers from
being able to poison DFI’s view of the network. As a result,
we implemented sensors for each of the four bindings that our
Entity Resolution Manager tracks, each of which collects its
Alice-LaptopOpenFlow Data PlaneDFI Sensor GatewayActive Directory•Hostname-IP Sensor•IP-MAC SensorSIEM Aggregator•Log-on/log-off Sensor•Username-Hostname SensorOpenFlowControllerDFI Control PlaneDFI ProxyPolicy Decision PointPolicyManagerPolicyCompilationPointEntity Resolution ManagerOpenFlow Control Plane136122413711891015514UsernameHostnameIP AddressMAC AddressSwitch ID & PortSystem Event LogsDNS ServerDHCP ServerPacket-InEventAuthoritative SourceNetwork Identifiera switch to partition its memory for rules into multiple ﬂow
tables, with an incoming Packet-in being matched against
rules in Table 0 ﬁrst. A matching rule with the goto_table
action can pass the packet to another table. This action is
accompanied by a table index (table_id) value indicating
the next table. All Flow-Mod messages and some others (e.g.,
statistics requests) also contain a table_id denoting the
table to modify or query.
Our proxy leverages this feature to reserve Table 0 for
access control rules from DFI. Tables 1 and higher are reserved
for the controller. If a ﬂow is allowed, it is forwarded to Table
1, which only contains rules from the controller, for further
instructions that could include forwarding or even continued
pipelining into higher tables. Denied ﬂows are dropped at
Table 0. Reserving Table 0 for DFI means that the controller
should not be able to modify Table 0 or learn about
its
contents. We implement this transparently by shifting by one
all table_id references in messages from the controller
to the switch. Similarly, any table reference being sent from
the switch to the controller, e.g., in a statistics reply, must
also be decremented to avoid confusing the controller. This
operation also ensures that existing controller applications
function normally alongside DFI for ﬂows that it allows.
A. Performance Evaluation
V. EVALUATION
We ﬁrst evaluate the DFI control plane in terms of mi-
crobenchmarks about its minimum latency handling a ﬂow and
its maximum throughput of new ﬂows. We then consider how a
network with DFI performs end-to-end using a small hardware
SDN with OpenFlow switches and an SDN controller; here we
measure the Time to First Byte (TTFB) of new ﬂows, both
with and without DFI, as a function of load on the network.
Other metrics like the total DFI ﬂow rules produced are highly
dependent on policies and operational factors (e.g., trafﬁc) and
therefore are not the focus of this evaluation (see Section III
for options for reducing ﬂow rules in the PCP).
The testbed for these experiments included VMs created
and managed by VMware vSphere, with four 2.1 GHz Intel
Xeon cores and 7.6 GB of RAM running CentOS 7. One
server hosted the core DFI services (PDP, Policy Manager,
Entity Resolution Manager, and PCP) while the DFI Proxy
and SDN controller (ONOS 1.13) ran on another. Our data
plane consisted of three end hosts and a single software switch
running Open vSwitch 2.5.4.
Latency and Throughput Microbenchmarks. The ﬂow-
start latency and maximum throughput of the DFI control
plane help characterize its performance independent of the
SDN controller and network service. When a packet cannot
be matched with an existing ﬂow rule received on a switch
(usually at the start of a new ﬂow), it is sent to be handled
by the DFI control plane, incurring some computation time
before returning an access control rule for the packet. Once
ﬂow rules are installed, subsequent packets in the ﬂow match
these rules and are routed directly through the data plane
without additional latency. The maximum throughput of new
TABLE I: DFI Performance Microbenchmarks
Metric
Latency (under no load)
Throughput (at saturation)
Mean ± Std. Dev.
5.73ms ± 3.39ms
1350 ﬂows/sec ± 39 ﬂows/sec
TABLE II: Latency Breakdown
Component
Binding Query
Policy Query
Other PCP Processing
Proxy
Overall
Mean Latency ± Std. Dev.
2.41ms ± 0.97ms
2.52ms ± 0.85ms
0.39ms ± 0.27ms
0.16ms ± 0.72ms
5.73ms ± 3.39ms
ﬂows represents the level of network activity beyond which
new ﬂows will experience disconnections or extreme delays.
In order to measure these metrics, we use the cbench
synthetic OpenFlow controller benchmark [34], which we
modiﬁed for compatibility with OpenFlow 1.3. The tool emu-
lates an OpenFlow switch and sends packets with randomized
headers to the control plane, with both latency and throughput
measurement modes.
Table I summarizes our microbenchmarks: the ﬂow-start
latency is approximately 5.73ms (from cbench in latency
mode) and DFI can handle approximately 1350 ﬂows/sec
(from cbench in throughput mode) before it is saturated.
Note that the reported ﬂow-start latency includes only the
time for the ﬂow to traverse DFI in one direction and does
not include any additional time required by the actual SDN
controller to route the ﬂow. Additionally, this ﬂow-start la-
tency was measured when the system was otherwise idle.
Table II shows the average time spent per ﬂow during each
of DFI’s subtasks. This breakdown shows that most of the
latency comes from queries to resolve binding information
and determine applicable policy (about 2.5ms each). The other
processing done by the PCP and DFI Proxy is insigniﬁcant
(less than 0.6ms combined).
Time to First Byte. We now characterize the performance
impact of using DFI in an SDN in terms of the latency imposed