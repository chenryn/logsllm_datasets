24, 29
2012-1448
3, 7, 8, 26
2012-1450
2012-1453
2012-1456
7, 8, 19
2, 7, 8, 13, 15, 16, 18, 19, 23,
24, 25, 26
2, 3, 5, 7, 8, 10, 12, 15, 16,
17, 19, 20, 22, 23, 24, 25, 26,
27, 29, 33
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13, 14, 15, 16, 17, 18, 19,
20, 21, 22, 23, 24, 25, 26, 27,
28, 30, 31, 32, 33, 34, 35, 36
3, 5, 7, 8, 12, 16, 17, 19, 22,
29, 32, 33
2012-1451
2012-1454
7, 8
2, 23, 24, 25, 29
2012-1457
2012-1460
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
13, 14, 15, 16, 17, 18, 20, 21,
22, 23, 25, 26, 28, 29, 33, 34,
35, 36
3, 6, 10, 13, 14, 17, 29, 36
2012-1463
3, 6, 10, 22, 23, 24, 27, 28,
29, 30, 31, 32
it
i.e.,
them using SMT solvers [6]. Unfortunately, this approach
does not appear to be feasible for automatically discovering
chameleon and werewolf attacks. The programs tested by
Brumley et al. implement simple protocols like HTTP and
their parsing code is very shallow,
lies close to
the program’s entry point. By contrast, malware scanners
and applications do much more than parsing: scanners load
virus signatures, match them against the ﬁle, etc., while
applications perform a wide variety of operations before
and after parsing. Binary differencing must be applied to the
parsing code only, because the non-parsing functionalities of
malware detectors and applications are completely different.
This requires extracting the parsing code from the closed-
source binaries of both detectors and applications, which
is extremely difﬁcult. Furthermore, both parsers must have
the same interface, otherwise their ﬁnal output states cannot
be easily compared. Brumley et al. provide no method
for automatically recognizing, extracting, normalizing, and
comparing individual pieces of functionality hidden deep
inside the binary.
Furthermore, this technique generates formulas from one
execution path at a time and is less likely ﬁnd bugs in
rare paths. By contrast, most of the attacks reported in
this paper—for example,
the attack which concatenates
two separate streams of gzipped data to create a single
ﬁle—exploit bugs in unusual paths through the parsing code.
BEK is a new language and system for writing and
analyzing string-manipulating sanitizers for Web applica-
82
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:44 UTC from IEEE Xplore.  Restrictions apply. 
tions [15]. BEK can compare two sanitizers for equivalence
and produce a counter-example on which their outputs differ.
The BEK language is speciﬁcally tailored for expressing
string manipulation operations and is closely related to
regular expressions. It is ill-suited for expressing ﬁle-parsing
logic. For example, it cannot validate data-length ﬁelds in
ﬁle headers and similar content-dependent format ﬁelds.
Development of program analysis techniques for auto-
mated discovery of chameleon and werewolf attacks is an
interesting topic for future research.
III. ATTACKING FILE PROCESSING
Figure 1 shows the main components of
the ﬁle-
processing logic of antivirus scanners. The basic pattern
applies to other automated2 malware detectors, both be-
havioral and signature-based, as long as they process ﬁles
independently of the endhost’s OS and applications.
Input file
File−type   inference
Preprocessing 
    +
Normalization
Select parts 
to scan
File
parsing
Find relevant signatures
Signature
matching
Figure 1. File processing in antivirus scanners.
The ﬁrst step is ﬁle-type inference. The scanner must
infer the ﬁle type in order to (1) parse the ﬁle correctly and
(2) scan it for the correct subset of virus signatures.
The second step is ﬁle parsing. Files in some formats
must be preprocessed before scanning (for example, the con-
tents of an archive must be extracted). Documents in formats
like HTML contain many irrelevant characters (for example,
whitespace) and must be normalized. In most ﬁle formats,
whether executable or application-speciﬁc, blocks of data
are interspersed with meta-data. For higher performance,
malware scanners parse the meta-data in order to identify
and scan only the potentially “interesting” parts of the ﬁle.
For example, a scanner may parse an MS Word document
to ﬁnd embedded macros and other executable objects and
scan them for macro viruses. To detect viruses in Linux
ELF executables, which can contain multiple sections, the
scanner must construct a contiguous view of the executable
code. This requires parsing the meta-data (ELF header) to
ﬁnd the offsets and sizes of code sections.
Chameleon and werewolf attacks. We will refer to at-
tacks that exploit discrepancies in ﬁle-type inference as
2Human operators may be able to manually prevent incorrect parsing and
ﬁle-type inference, but widespread deployment of human-assisted detectors
is not feasible for obvious scalability reasons.
83
chameleon attacks because attack ﬁles appear as one type
to the detector and as a different type to the actual OS or ap-
plication. We will refer to attacks that exploit discrepancies
in parsing as werewolf attacks because attack ﬁles appear
to have different structure depending on whether they are
parsed by the detector or the application.
Chameleon and werewolf attacks only change the meta-
data of the ﬁle; the contents, including the malicious pay-
load, are not modiﬁed (in contrast to code obfuscation and
polymorphism). These attacks (1) start with a ﬁle that is
recognized as malicious by the detector, (2) turn it into a
ﬁle that is not recognized as malicious, yet (3) the modiﬁed
ﬁle is correctly processed by the destination application or,
in the case of executables, loaded by the OS. If the same
ﬁle can be processed by multiple applications or versions of
the same application, we consider an attack successful if at
least one of them processes the modiﬁed ﬁle correctly.
Fingerprinting malware detectors and learning their
logic. Because ﬁle-type inference heuristics and ﬁle-parsing
logic vary from detector to detector, attacks are detector-
speciﬁc and it helps to know which detector is protecting
the target. This knowledge is often public—for example,
Yahoo Mail scans all messages with Symantec’s Norton
antivirus—but even in blind testing against Gmail’s unknown
scanner, two chameleon and one werewolf attacks (CVE-
2012-1438, 2012-1443, and 2012-1457) evaded detection.
Unknown detectors can be identiﬁed by tell-tale signs
in bounced messages [22], or by using chameleon and
werewolf attacks themselves. As Table II shows, different
attacks work against different detectors. By trying several
attacks and seeing which of them evade detection,
the
attacker can infer the make and model of the detector.
The logic of open-source detectors like ClamAV can be
learned by analyzing their code, but the vast majority of
detectors are closed-source and their logic must be learned
by fuzzing and/or binary analysis. Secrecy of the ﬁle-
processing logic is a weak defense, however: we report
dozens of vulnerabilities in commercial scanners for which
we do not have the source code, many of them discovered
automatically by our black-box differential fuzzer.
IV. GENERATING ATTACKS
To test our attacks, we used VirusTotal [32], a free
Web service that checks any ﬁle against multiple antivirus
scanners (43 at the time of our testing). Several scanners
were not available at various times due to crashes, thus for
consistency we present the results for the 36 scanners that
were continuously available.
VirusTotal executes the command-line versions of all
AV scanners with maximum protection and all detection
methods enabled. We argue that this faithfully models the
level of defense provided by network-based detectors. By
design, they do not observe the actual processing of ﬁles
on the host and thus advanced detection techniques—for
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:44 UTC from IEEE Xplore.  Restrictions apply. 
Table III
TESTED APPLICATIONS.
File type
CAB
CHM
ELF
GZIP
DOC
PE
RAR
TAR
7Z
Target application(s)
Cabextract 1.2
Microsoft HTML Help 1.x
Linux kernel (2.6.32) ELF loader
Gzip 1.3.12, File Roller 2.30.1.1
MS Ofﬁce 2007, OpenOfﬁce 3.2
Windows Vista SP2 PE loader, Wine 1.2.2 PE loader
RAR 3.90 beta 2
GNU tar 1.22, File Roller 2.30.1.1
7-Zip 9.04 beta
example, monitoring the program’s execution for signs of
malicious behavior—require the detector to accurately rec-
ognize the ﬁle type, parse the ﬁle, and replicate the host’s
execution environment. In Section IX, we explain why this
is challenging to do correctly.
Attacks were also conﬁrmed by testing against the host-
based versions of AV software, where available.
We used ﬁve toy viruses from VX Heavens [33] in our
tests: EICAR, Linux Bliss and Cassini, Windows Cecile, and
MS Word ABC. If an exact, unobfuscated instance of such
a basic virus evades detection, more sophisticated malware
won’t be detected, either. We count an attack as successful
if the detector (1) recognizes the infection in the original
ﬁle, but (2) no longer recognizes it in the modiﬁed ﬁle.
Target applications used in our testing are summarized
in Table III. They were executed on laptops running Linux
Ubuntu 10.04 and Windows Vista SP 2.
Black-box differential fuzzing. To ﬁnd werewolf attacks
automatically, we built a differential fuzzing framework that
ﬁnds discrepancies between the parsing logic of applications
and malware detectors. Because the source code of detectors
is rarely available, our framework is black-box. It is imple-
mented in Python and runs on both Linux and Windows.
The basic framework is format-independent, but format-
speciﬁc components are added as plugins. Each plugin
provides a parser, an output validator, and a fuzzer. The
parser breaks up the format-speciﬁc header into an array
of (name, offset, length) tuples, where name is the unique
name of a header ﬁeld, offset is its location in the ﬁle, and
length is its size. The fuzzer modiﬁes the content of the ﬁelds
using a format-speciﬁc algorithm. The validator checks if the
application still processes the modiﬁed ﬁle correctly.
Our framework takes as input
two seed ﬁles in the
same format. One ﬁle is parsed correctly by the destination
application, the other is an infected ﬁle recognized by the
detector. The framework uses the format-speciﬁc fuzzer to
automatically generate modiﬁcations to the ﬁrst ﬁle and the
output validator to check if the application still accepts the
ﬁle. If a modiﬁcation is validated, the framework applies it
to the second, infected ﬁle and tests whether the detector
still recognizes the infection. This approach is better than
directly modifying the infected ﬁle and accessing it on
an endhost because (1) the host must be isolated (e.g.,
virtualized) in each test
infection,
imposing a signiﬁcant performance overhead on the testing
framework, and (2) determining if the modiﬁed infected ﬁle
is accepted by the destination application is difﬁcult because
applications are opaque and have complex side effects.
to prevent an actual
A modiﬁcation is thus applied to the infected ﬁle only
if the application’s parser tolerates it. If the ﬁle is no
longer recognized as malicious, a discrepancy between the
application’s and the detector’s parsers has been found and
an actual attack can be generated and veriﬁed by accessing
the modiﬁed infected ﬁle on a secure, isolated host. We
consider an infection veriﬁed if the intact malware code is
extracted from the archive and/or loaded as an executable.
As a proof of concept, we implemented sample plugins for
MS Cabinet (CAB), Windows executable (PE), and Linux
executable (ELF) ﬁles. The fuzzer in these plugins tries a
simple modiﬁcation to the ﬁle’s header, one ﬁeld at a time:
it increments the content of each ﬁeld (or the ﬁrst byte if the
ﬁeld spans multiple bytes) by 1; if this results in an overﬂow,
it decrements the content by 1. Output validators execute
destination applications on modiﬁed seed ﬁles and check the
return codes and the application’s output for correctness.
Once integrated with VirusTotal, our fuzzing framework
found dozens of parsing bugs in 21 different detectors
(Table XII). All result in actual werewolf attacks.
Of course, our simple framework cannot ﬁnd all parsing
discrepancies. Some parsing bugs are hidden in rarely ex-
ecuted paths which can only be reached through specially
crafted inputs, requiring manual guidance to the fuzzer. For
example, attacks involving a concatenation of two gzipped
streams or a header with an incorrect checksum whose
length is modiﬁed to point into the following header (see
Section VI) are difﬁcult to discover by automated fuzzing.
Another limitation is that our fuzzer does not fully “under-
stand” the dependencies between different ﬁelds of format-
speciﬁc headers and cannot automatically generate valid ﬁles
if several ﬁelds must be changed consistently. For example,
if ﬁle length is included in a header ﬁeld, the ﬁle must be
truncated or augmented whenever this ﬁeld is modiﬁed.
V. CHAMELEON ATTACKS
Chameleon attacks involve specially crafted ﬁles that
appear as one type to the ﬁle-type inference heuristics used
by the malware detector but as a different type to the OS or
application on the endhost.
The simplest chameleon attack is to hide the infected
ﬁle in an archive of a type not recognized by the detector,
causing it
to apply generic malware signatures without
extracting the contents. Even this primitive attack is sur-
prisingly effective, as shown by Table IV.
In the rest of this section, we focus on more interesting
chameleon attacks that involve a ﬁle of one type masquerad-
ing as a ﬁle of a different type. Masquerade attacks cause
84
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:44 UTC from IEEE Xplore.  Restrictions apply. 
SUPPORT FOR 11 ARCHIVE FORMATS: 7ZIP, 7ZIP-SFX, PACK, ISO, RAR, RAR(SFX), TAR.LZOP, TAR.LZMA, TAR.RZ, TAR.XZ, AR
Table IV
Scanner
ClamAV 0.96.4
GData 21
Ikarus T3.1.1.97.0
F-Prot 4.6.2.117
Antiy-AVL 2.0.3.7
Kaspersky 7.0.0.125
Sophos 4.61.0
Norman 6.06.12
McAfee-GW-Edition 2010.1C
BitDefender 7.2
nProtect 2011-01-17.01
Avast 4.8.1351.0
Unsupported
formats
Scanner
Unsupported
formats
Scanner
Unsupported
formats
8
7
9
9
8
5
8
9
10
9
10
7
Rising 22.83.00.03
Symantec 20101.3.0.103
Emsisoft 5.1.0.1
VirusBuster 13.6.151.0
K7AntiVirus 9.77.3565
Jiangmin 13.0.900
NOD32 5795
McAfee 5.400.0.1158
TrendMicro 9.120.0.1004
eSafe 7.0.17.0
AhnLab-V3 2011.01.18.00
Avast5 5.0.677.0
9
10
8
10
9
9
7
10
10
8
10
7
CAT-QuickHeal 11.00
Command 5.2.11.5
PCTools 7.0.3.5
Fortinent 4.2.254.0
TrendMicro-HouseCall 9.120.0.1004
Microsoft 1.6402
AntiVir 7.11.1.163
Panda 10.0.2.7
Comodo 7424
F-Secure 9.0.16160.0
AVG 10.0.0.1190
VBA32 3.12.14.2
9
8
10
9
10
6
7
8
11