title:Towards understanding modern web traffic
author:Sunghwan Ihm and
Vivek S. Pai
Towards Understanding Modern Web Trafﬁc
Sunghwan Ihm†
Department of Computer Science
Princeton University
PI:EMAIL
Vivek S. Pai
Department of Computer Science
Princeton University
PI:EMAIL
ABSTRACT
As Web sites move from relatively static displays of simple pages
to rich media applications with heavy client-side interaction, the
nature of the resulting Web trafﬁc changes as well. Understanding
this change is necessary in order to improve response time, evalu-
ate caching effectiveness, and design intermediary systems, such as
ﬁrewalls, security analyzers, and reporting/management systems.
Unfortunately, we have little understanding of the underlying na-
ture of today’s Web trafﬁc.
In this paper, we analyze ﬁve years (2006-2010) of real Web traf-
ﬁc from a globally-distributed proxy system, which captures the
browsing behavior of over 70,000 daily users from 187 countries.
Using this data set, we examine major changes in Web trafﬁc char-
acteristics that occurred during this period. We also present a new
Web page analysis algorithm that is better suited for modern Web
page interactions by grouping requests into streams and exploiting
the structure of the pages. Using this algorithm, we analyze var-
ious aspects of page-level changes, and characterize modern Web
pages. Finally, we investigate the redundancy of this trafﬁc, us-
ing both traditional object-level caching as well as content-based
approaches.
Categories and Subject Descriptors
C.2.m [Computer-Communication Networks]: Miscellaneous
General Terms
Measurement, Design, Performance
Keywords
Web Trafﬁc Analysis, Web Caching, Content-based Caching
1.
INTRODUCTION
The World Wide Web is one of the most popular Internet appli-
cations, and its trafﬁc volume is increasing and evolving due to the
†
Current afﬁliation: Google Inc.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
popularity of social networking, ﬁle hosting, and video streaming
sites [29]. These changes and growth of Web trafﬁc are expected
to continue, not only as the Web becomes a de facto front-end for
many emerging cloud-based services [47], but also as applications
get migrated to the Web [34].
Understanding these changes is important for overall system de-
sign. For example, analyzing end-user browsing behavior can lead
to a Web trafﬁc model, which in turn can be used to generate a syn-
thetic workload for benchmarking or simulation. In addition, ana-
lyzing the redundancy and effectiveness of caching could shape the
design of Web servers, proxies, and browsers to improve response
times. In particular, since content-based caching approaches [28,
49, 50] are a promising alternative to traditional HTTP object-based
caching, understanding their implications for Web trafﬁc and re-
source requirements (e.g., cache storage size) could help reduce
bandwidth and improve user experience.
While much research activity occurred a decade ago aimed at
better understanding the nature of Web trafﬁc [9, 11, 35, 56, 63],
it subsided just as the Web changed signiﬁcantly, and we must
therefore update our understanding of today’s Web trafﬁc. How-
ever, there are several challenges. First, examining changes over
time requires large-scale data sets spanning a multi-year period,
collected under the same conditions. Second, earlier Web page
analysis techniques developed for static pages are not suitable for
modern Web trafﬁc that involves dynamic client-side interactions
(e.g., Ajax [18]). Third, understanding the effectiveness of content-
based caching approaches requires full content data rather than just
access logs.
In this paper, we analyze ﬁve years (2006-2010) of real Web traf-
ﬁc from a globally-distributed proxy system, which captures the
browsing behavior of over 70,000 daily users from 187 countries.
Using this data set, we examine major changes in Web trafﬁc char-
acteristics that occurred during this period. We also present a new
Web page analysis algorithm that is better suited for modern Web
page interactions by grouping requests into streams and exploiting
the structure of the pages. Using this algorithm, we analyze var-
ious aspects of page-level changes, and characterize modern Web
pages. Finally, we investigate the redundancy of this trafﬁc, us-
ing both traditional object-level caching as well as content-based
approaches.
Our contributions and key ﬁndings are the following:
High-Level Characteristics The rise of Ajax and video content
has impacted a number of different trafﬁc measures. Ajax has
caused in increase in the sizes of JavaScript and CSS [19] objects,
and browsers have increased their simultaneous connection limit
to better support it, resulting in burstier trafﬁc but also improved
client latency. Flash video (FLV) has grown to dominate video
trafﬁc, pushing the share of other video formats lower, and also in-
295creasing bandwidth consumption. Ajax and JavaScript are heavily
used in user tracking and we ﬁnd that analytics sites are reaching
an ever-widening fraction of Web users, with some sites being able
to track as much as 65% of our client population, which may have
privacy implications. In addition, we observe clear regional differ-
ences in client bandwidth, browser popularity, and dominant con-
tent types that need to be considered when designing and deploying
systems. Finally, we observe an increase in the number of comput-
ers per household over the years in Network Address Translation
(NAT) [59] usage, which is likely related to the scarcity of IPv4
addresses.
Page-Level Characteristics We have developed a new Web page
analysis algorithm called StreamStructure, and demonstrate that it
is more accurate than previous approaches. Using this algorithm,
we ﬁnd that almost half the trafﬁc now occurs not as a result of
initial page loads, but as a result of client-side interactions after the
initial page load. Also, the pages have become increasingly com-
plex in that both the size and number of embedded objects have
increased. Despite this increase, the page loading latency dropped
in 2009 and 2010 due to the increased number of simultaneous
connections in browsers and improved caching behavior of Web
sites. Furthermore, we quantify the potential reduction of page
loading latency from various tuning approaches, such as increasing
the number of concurrent connections, and prefetching/caching, via
simulations. Finally, we present a simple characterization of mod-
ern Web pages.
Redundancy and Caching We ﬁnd two interesting trends in URL
popularity: 1) the popular URLs get more popular, and therefore
potentially improves caching, but 2) the long tail of the content is
also growing, and therefore potentially hurts caching. Also, we ﬁnd
that content-based caching yields 1.8-2.5x larger byte hit rates than
object-based caching, and much larger caches can be effectively
exploited using intelligent content-based caching to yield nearly
ideal byte hit rates. Most of the additional savings of content-based
caching are due to partial content overlap – the redundancy across
different versions of an object as well as redundancy across differ-
ent objects. Finally, a small number of aborted requests (1.8-3.1%),
mostly video, can negatively impact object-based caching perfor-
mance because of its huge volume (12.4-30.8%). Worse, their vol-
ume would comprise a signiﬁcant portion of all trafﬁc (69.9-88.8%)
if they were fully downloaded.
The rest of this paper is organized as follows:
in Section 2,
we describe the details of our data set. Section 3 examines the
major changes in high-level characteristics of Web trafﬁc. Sec-
tion 4 presents the detailed page-level analysis with our new Web
page analysis technique, and Section 5 analyzes redundancy and
caching. Finally, we discuss related work in Section 6, and con-
clude in Section 7.
2. DATA SET
Data Collection We use trafﬁc from the CoDeeN content distribu-
tion network (CDN) [62], a semi-open globally distributed proxy
that has been running since 2003, and serves over 30 million re-
quests per day from more than 500 PlanetLab [45] nodes. The term
“semi-open” means that while anyone can use CoDeeN by con-
ﬁguring his or her browser, it only allows GET requests from the
general public, and limits other methods such as CONNECT, PUT,
or POST to only university-based users. When needed, the system
redirects user requests to other proxy nodes based on the load and
latency. Some requests are cache misses or uncacheable, and need
to be retrieved from the origin Web servers. CoDeeN also deploys
Country
China
USA Requests (M)
Volume (GB)
# IPs (K)
# Users (K)
Requests (M)
Volume (GB)
# IPs (K)
# Users (K)
Requests (M)
Volume (GB)
# IPs (K)
# Users (K)
Requests (M)
Volume (GB)
# IPs (K)
# Users (K)
Requests (M)
Volume (GB)
# IPs (K)
# Users (K)
Total
France
Brazil
2006
33.5
391.2
19.1
23.3
22.5
394.5
49.3
53.9
2.2
21.6
3.6
3.9
1.5
16.2
1.4
1.6
59.6
823.5
73.5
82.8
2007
40.3
627.2
21.8
27.0
88.8
1,177.8
94.9
109.7
3.9
45.8
5.1
5.5
4.5
54.8
8.6
10.0
137.5
1,905.6
130.4
152.2
Year
2008
24.5
338.2
13.6
17.6
29.9
405.0
38.8
45.1
3.3
33.6
3.2
3.5
2.0
22.8
3.3
3.8
59.7
799.6
58.9
70.0
2009
23.2
316.2
13.3
16.9
38.1
409.6
43.2
51.8
3.6
42.9
3.7
4.3
3.9
44.1
3.1
3.6
68.8
812.8
63.3
76.7
2010
14.4
261.5
12.9
16.7
22.9
278.4
33.4
41.9
3.3
50.5
5.1
6.0
7.1
100.2
9.5
10.9
47.7
690.6
61.0
75.5
Table 1: Summary statistics for captured access logs, sampled
one month (April) per year.
an automatic robot detection mechanism and has rejected accesses
from malicious robots since 2006 [44].
Our data set consists of two parts. First, CoDeeN records all
requests not served from the client’s browser cache in an extended
W3C log format with timestamp, service time, request URL, method,
user agent, content type, referer, response code, and response size.
We use these access logs for examining any longitudinal changes
in Section 3, 4, and 5. In addition, we capture the full content of
the cache-miss trafﬁc between the CoDeeN nodes and the origin
Web servers. Using this full content data, we evaluate both object-
based and content-based caching approaches, and analyze aborted
transfers in Section 5.
For this study, we consider the data from the ﬁve-year period
from 2006 to 2010. Due to the large volume of requests, we sam-
ple one month (April) of data per year. We only capture the full
trafﬁc content in April 2010, and use only trafﬁc logs in all other
years. After discarding non-human trafﬁc, the total trafﬁc volume
ranges from 3.3 to 6.6 TB per month, and it consists of about 280-
460 million requests from 240-360 thousand unique client IPs. The
number of users (unique client IP and browser user-agent string
pairs) ranges from 280 to 430 thousand, slightly larger than the
number of client IPs. The clients IPs originate from 168-187 coun-
tries and regions as determined using the MaxMind database [38],
and cover 40-60% of /8 networks, and 7-24% of /16 networks. The
total number of unique origin servers ranges from 820 thousand to
1.2 million.
We focus on the trafﬁc of users from four countries from differ-
ent continents – the United States (US), Brazil (BR), China (CN),
and France (FR). This essentially generates multiple data sets from
different client organizations, and analyzing geographically dis-
persed client organizations enables us to discover common char-
acteristics of Web trafﬁc across different regions as well as region-
speciﬁc characteristics. Table 1 shows summary statistics for these
countries. In general, the United States and China have larger data
sets than France and Brazil, mainly due to their larger client popula-
tion. The yearly ﬂuctuation of trafﬁc volume is due to the variation
of the number of available proxy nodes. Overall, our analysis of
four countries covers 48-138 million requests, 691-1906 GB traf-
ﬁc, and 70-152 thousand users per month.
296 100
 75
 50
 25
)
s
P
I
4
2
/
(
s
t
n
e
i
l
C
%
 0
 0
2006
2008
2010
 1
 2
 3
 4
Average Bandwidth (Mbps)
(a) US
 100
 75
 50
 25
)
s
P
I
4
2
/
(
s
t
n
e
i
l
C
%
 0
 0
2006
2008
2010
 1
 2
Average Bandwidth (Mbps)
(b) CN
 100
 75
 50
 25
)
s
P
I
4
2
/
(
s
t
n
e
i
l
C
%
 0
 0
2006
2008
2010
 1
 2
Average Bandwidth (Mbps)
(c) BR
Figure 1: Average client bandwidth: Client bandwidth gets improved over time.
s
P
I
%
 100
 95
 90
 85
 80
2006
2008
2010
 1
 3
 2
 4
# User Agents Per IP
 5
(a) US
s
P
I
%
 100
 95
 90
 85
 80
2006
2008
2010
 1
 3
 2
 4
# User Agents Per IP
 5
(b) CN
s
P
I
%
 100
 95
 90