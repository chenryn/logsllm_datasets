# Towards Understanding Modern Web Traffic

**Authors:**
- Sunghwan Ihm (Current affiliation: Google Inc.)
- Vivek S. Pai
Department of Computer Science, Princeton University
PI:EMAIL

## Abstract
As web sites evolve from simple, static pages to rich media applications with significant client-side interactions, the nature of web traffic also changes. Understanding these changes is essential for improving response times, evaluating caching effectiveness, and designing intermediary systems such as firewalls, security analyzers, and reporting/management systems. However, there is a lack of comprehensive understanding of today's web traffic.

In this paper, we analyze five years (2006-2010) of real web traffic from a globally-distributed proxy system, capturing the browsing behavior of over 70,000 daily users from 187 countries. We examine major changes in web traffic characteristics during this period. We also introduce a new web page analysis algorithm, StreamStructure, which groups requests into streams and leverages the structure of pages to better suit modern web interactions. Using this algorithm, we analyze various aspects of page-level changes and characterize modern web pages. Finally, we investigate the redundancy of this traffic using both traditional object-level caching and content-based approaches.

## Categories and Subject Descriptors
C.2.m [Computer-Communication Networks]: Miscellaneous

## General Terms
Measurement, Design, Performance

## Keywords
Web Traffic Analysis, Web Caching, Content-based Caching

## 1. Introduction
The World Wide Web is one of the most popular internet applications, with its traffic volume increasing and evolving due to the popularity of social networking, file hosting, and video streaming sites. These changes are expected to continue as the web becomes the de facto front-end for many emerging cloud-based services and as more applications migrate to the web.

Understanding these changes is crucial for system design. For example, analyzing end-user browsing behavior can lead to a web traffic model, which can be used to generate synthetic workloads for benchmarking or simulation. Additionally, analyzing the redundancy and effectiveness of caching can inform the design of web servers, proxies, and browsers to improve response times. Content-based caching approaches, which are a promising alternative to traditional HTTP object-based caching, can help reduce bandwidth and improve user experience by understanding their implications for web traffic and resource requirements.

While much research was conducted a decade ago to understand web traffic, it subsided just as the web underwent significant changes. Updating our understanding of today's web traffic presents several challenges. First, examining changes over time requires large-scale datasets spanning multiple years, collected under consistent conditions. Second, earlier web page analysis techniques developed for static pages are not suitable for modern web traffic involving dynamic client-side interactions (e.g., Ajax). Third, evaluating the effectiveness of content-based caching requires full content data rather than just access logs.

In this paper, we analyze five years (2006-2010) of real web traffic from a globally-distributed proxy system, capturing the browsing behavior of over 70,000 daily users from 187 countries. We examine major changes in web traffic characteristics during this period, present a new web page analysis algorithm, and investigate the redundancy of this traffic using both traditional and content-based caching approaches.

### Our Contributions and Key Findings
- **High-Level Characteristics**: The rise of Ajax and video content has impacted several traffic measures. Ajax has increased the sizes of JavaScript and CSS objects, and browsers have raised their simultaneous connection limits, resulting in burstier traffic but improved client latency. Flash video (FLV) now dominates video traffic, increasing bandwidth consumption. Analytics sites using Ajax and JavaScript can track up to 65% of our client population, raising privacy concerns. We observe regional differences in client bandwidth, browser popularity, and dominant content types. Additionally, the number of computers per household using Network Address Translation (NAT) has increased, likely due to the scarcity of IPv4 addresses.
- **Page-Level Characteristics**: We developed the StreamStructure algorithm, which is more accurate than previous approaches. Using this algorithm, we find that almost half of the traffic occurs after the initial page load due to client-side interactions. Pages have become more complex, with increases in the size and number of embedded objects. Despite this, page loading latency dropped in 2009 and 2010 due to increased simultaneous connections and improved caching. We quantify the potential reduction in page loading latency from various tuning approaches and present a simple characterization of modern web pages.
- **Redundancy and Caching**: We observe two trends in URL popularity: popular URLs becoming more popular, potentially improving caching, and the long tail of content growing, potentially hurting caching. Content-based caching yields 1.8-2.5x larger byte hit rates than object-based caching, and much larger caches can be effectively exploited using intelligent content-based caching to achieve nearly ideal byte hit rates. Aborted requests, mostly video, negatively impact object-based caching performance due to their large volume.

### Paper Organization
- **Section 2**: Data Set Description
- **Section 3**: High-Level Characteristics of Web Traffic
- **Section 4**: Detailed Page-Level Analysis
- **Section 5**: Redundancy and Caching Analysis
- **Section 6**: Related Work
- **Section 7**: Conclusion

## 2. Data Set
### Data Collection
We use traffic from the CoDeeN content distribution network (CDN), a semi-open, globally distributed proxy that has been running since 2003, serving over 30 million requests per day from more than 500 PlanetLab nodes. "Semi-open" means that while anyone can use CoDeeN by configuring their browser, it only allows GET requests from the general public and limits other methods such as CONNECT, PUT, or POST to university-based users. The system redirects user requests to other proxy nodes based on load and latency. Some requests are cache misses or uncacheable and need to be retrieved from the origin web servers. CoDeeN also deploys an automatic robot detection mechanism and has rejected accesses from malicious robots since 2006.

Our dataset consists of two parts:
1. **Access Logs**: CoDeeN records all requests not served from the clientâ€™s browser cache in an extended W3C log format, including timestamp, service time, request URL, method, user agent, content type, referer, response code, and response size. We use these logs to examine longitudinal changes in Sections 3, 4, and 5.
2. **Full Content Data**: We capture the full content of cache-miss traffic between CoDeeN nodes and origin web servers. This data is used to evaluate both object-based and content-based caching approaches and to analyze aborted transfers in Section 5.

For this study, we consider data from the five-year period from 2006 to 2010. Due to the large volume of requests, we sample one month (April) of data per year. We only capture the full traffic content in April 2010 and use only traffic logs in all other years. After discarding non-human traffic, the total traffic volume ranges from 3.3 to 6.6 TB per month, consisting of about 280-460 million requests from 240-360 thousand unique client IPs. The number of users (unique client IP and browser user-agent string pairs) ranges from 280 to 430 thousand, slightly larger than the number of client IPs. The client IPs originate from 168-187 countries and regions, as determined using the MaxMind database, and cover 40-60% of /8 networks and 7-24% of /16 networks. The total number of unique origin servers ranges from 820 thousand to 1.2 million.

We focus on the traffic of users from four countries across different continents: the United States (US), Brazil (BR), China (CN), and France (FR). This generates multiple datasets from different client organizations, enabling us to discover common and region-specific characteristics of web traffic. Table 1 shows summary statistics for these countries. Generally, the US and China have larger datasets than France and Brazil, mainly due to their larger client populations. The yearly fluctuation in traffic volume is due to the variation in the number of available proxy nodes. Overall, our analysis of four countries covers 48-138 million requests, 691-1906 GB of traffic, and 70-152 thousand users per month.

| Country | Year | Requests (M) | Volume (GB) | # IPs (K) | # Users (K) |
|---------|------|--------------|-------------|-----------|-------------|
| China   | 2006 | 33.5         | 391.2       | 19.1      | 23.3        |
| USA     | 2006 | 22.5         | 394.5       | 49.3      | 53.9        |
| France  | 2006 | 2.2          | 21.6        | 3.6       | 3.9         |
| Brazil  | 2006 | 1.5          | 16.2        | 1.4       | 1.6         |
| Total   | 2006 | 59.6         | 823.5       | 73.5      | 82.8        |

Table 1: Summary statistics for captured access logs, sampled one month (April) per year.

### Client Bandwidth and User Agents
Figure 1 shows the average client bandwidth over time for the US, China, and Brazil. Client bandwidth has generally improved over the years.

![](figure1.png)

Figure 2 shows the number of user agents per IP for the US and China, indicating a trend towards more devices per IP address.

![](figure2.png)

## 3. High-Level Characteristics of Web Traffic
### Impact of Ajax and Video Content
The rise of Ajax and video content has significantly impacted web traffic. Ajax has led to larger JavaScript and CSS objects, and browsers have increased their simultaneous connection limits, resulting in burstier traffic but improved client latency. Flash video (FLV) now dominates video traffic, increasing bandwidth consumption. Analytics sites using Ajax and JavaScript can track up to 65% of our client population, raising privacy concerns. We also observe regional differences in client bandwidth, browser popularity, and dominant content types.

### Network Address Translation (NAT)
The number of computers per household using NAT has increased, likely due to the scarcity of IPv4 addresses.

## 4. Detailed Page-Level Analysis
### StreamStructure Algorithm
We developed the StreamStructure algorithm, which groups requests into streams and leverages the structure of pages to better suit modern web interactions. Using this algorithm, we find that almost half of the traffic occurs after the initial page load due to client-side interactions. Pages have become more complex, with increases in the size and number of embedded objects. Despite this, page loading latency dropped in 2009 and 2010 due to increased simultaneous connections and improved caching. We quantify the potential reduction in page loading latency from various tuning approaches and present a simple characterization of modern web pages.

## 5. Redundancy and Caching Analysis
### URL Popularity Trends
We observe two trends in URL popularity: popular URLs becoming more popular, potentially improving caching, and the long tail of content growing, potentially hurting caching.

### Caching Effectiveness
Content-based caching yields 1.8-2.5x larger byte hit rates than object-based caching, and much larger caches can be effectively exploited using intelligent content-based caching to achieve nearly ideal byte hit rates. Aborted requests, mostly video, negatively impact object-based caching performance due to their large volume.

## 6. Related Work
[Detailed discussion of related work]

## 7. Conclusion
[Summary and future work]