variable is involved in the constraint. For example, Figure 6
shows the generated DAG for constraints (V1 op1 V2) ∧
(V3 op2 V4)∧ (V5 op3 V6)∧ (V6 op3 V7 op4 V8)∧ (V8 op5 V9)
where opk can be any operator.
Next, NEUEX partitions the DAG into connected com-
ponents by breadth-ﬁrst search [39]. Consider the example
Algorithm 1 Algorithm for neuro-symbolic constraint solv-
ing. Sp is purely symbolic constraints; Np is purely neural
constraints; Sm and Nm are symbolic constraints and neural
constraints in mixed components.
1: function NEUCL(S, N, MAX1, MAX2)
(cid:46) S: Symbolic constraint
list; N: Neural constraint list; MAX1: The maximum number of trials
of NeuSolv; MAX2: The maximum number of trials for backtracking
procedure.
(Sp, Np, Sm, Nm) ← CheckDependency(N,S);
(X, assign1) ← SymSolv(Sp, ∅);
if X == UNSAT then
return (False, ∅);
end if
(X, assign2) ← NeuSolv(Np);
assign ← Union(assign1, assign2);
ConﬂictDB ← ∅; trial_cnt ← 0;
while trial_cnt  b
S1 ::= a ≤ b
S1 ::= a ≥ b
S1 ::= a = b
S1 ::= a (cid:54)= b
S1 ∧ S2
S1 ∨ S2
Loss Function (L)
L = max(a − b + α, 0)
L = max(b − a + α, 0)
L = max(a − b, 0)
L = max(b − a, 0)
L = abs(a − b)
L = max(−1, −abs(a − b + β))
L = LS1 + LS2
L = min(LS1 , LS2 )
ﬁnds the values for variables max, infilename, and infile
which satisfy Equations (1)-(3). If these values do not satisfy
the neural constraint (Equation (4)), NEUEX transforms these
values into a conﬂict clause and attempts a retrial to discover
new values. However, the backtracking procedure may not
terminate either because the constraints are complex or UNSAT.
To avoid an inﬁnite number of trials, NEUEX chooses to limit
the number to a user-controlled threshold value. Speciﬁcally, if
we do not have a SAT decision after mixed constraint solving
I within MAX2 iterations.2
NEUEX applies an alternative strategy where we combine
the symbolic constraints with neural constraints together. There
exist two possible strategies: transforming neural constraints
into symbolic constraints or the other way around. However,
collapsing neural constraints to symbolic constraints result in
large sizes of encoded clauses. For example, encoding a small
binarized neural network generates millions of variables and
clauses [79]. Thus, we transform the mixed constraints into
purely neural constraints for solving them together.
Mixed Constraint Solving II. The key idea for solving mixed
constraints efﬁciently is to collapse symbolic constraints to
neural constraints by encoding the symbolic constraints to
a corresponding loss function (Line 28). This ensures the
symbolic and neural constraints are in the same form. Table II
shows the encoding of symbolic constraints and the loss
function that NEUEX transforms it to. For Figure 6, NEUEX
transforms the constraints S2 and S3 into a loss function N2.
Once the symbolic constraints are encoded into neural
constraints, NEUEX applies the NeuSolv to minimize the
loss function on Line 30. The main intuition behind this
approach is to guide the search with the help of encoded
symbolic constraints. The loss function measures the distance
between the current result and the satisﬁable result of symbolic
constraints. The search algorithm gives us a candidate value
for satisﬁability checking of neural constraints. However, the
candidate value generated by minimizing the distance may
not always satisfy the symbolic constraints since the search
algorithm only tries to minimize the loss, rather than exactly
forces the satisﬁability of symbolic constraints. To weed out
such cases, NEUEX checks the satisﬁability for the symbolic
constraints by plugging in the candidate value and querying
the SymSolv on Line 31. If the result is SAT, the solver
2Users can adapt MAX2 according to their applications.
8
goes to SAT state. Otherwise, it continues executing Approach
II with a different initial state of the search algorithm. For
example, in Figure 6, NEUEX changes the initial value of V7
for every iteration. Note that each iteration in Approach I has to
execute sequentially because the addition of the conﬂict clause
forces serialization. In contrast, each trial in Approach II is
independent and thus embarrassingly parallelizable. To avoid
the non-termination case, NEUEX sets the maximum number
of trials for mixed constraint solving II to be MAX1, which can
be conﬁgured independently of our constraint solver.
Thus, neuro-symbolic execution has the ability to reason
about purely symbolic constraints, purely neural constraints,
and mixed neuro-symbolic constraints. This approach has a
including but not
number of possible future applications,
limited to: (a) analyzing protocol
implementations without
analyzable code [49]; (b) analyzing code with complex depen-
dency structures [97]; and (c) analyzing systems that embed
neural networks directly as sub-components [36].
E. Encoding Mixed Constraints
One of the key challenges is in solving mixed constraints.
To solve mixed constraints, we encode symbolic constraints as
a loss function in the neural network. The variable values that
minimize this loss function are expected to be close (ideally
equal to) those which satisfy both the encoded symbolic con-
straints and the neural ones. Let X be the free input variables in
the constraints and S(X) be the symbolic constraints deﬁned
over a subset of X. We wish to deﬁne a loss function L(X)
such that the values of X that minimize L(X) simultaneously
satisfy S(X). We deﬁne an encoding procedure for each kind
of symbolic constraint
into a corresponding loss function.
NEUEX minimizes the joint loss functions of all symbolic
constraints encoded as loss functions, together with that of
the neural constraint. A gradient-based minimization procedure
ﬁnds the minimum values of the joint
loss function. The
encoding and minimization procedure is explained next.
Encoding. For each kind of symbolic constraint in our lan-
guage (Table I), we deﬁne a corresponding loss function.
All string expressions are converted into bit-vectors [90] and
treated like numeric variables (type NumVar in Table I).
Table II describes the loss function for all six symbolic
constraint types over numerics and the two constraint types
over Booleans. Taking a = b as an example, the loss function
L = abs(a − b) achieves the minimum value 0 when a = b,
where a and b can be arbitrary expressions. Thus, minimizing
the loss function L is equivalent
to solving the symbolic
constraint. We point out that encodings other than the ones
we outline are possible. They can be plugged into NEUEX, as
long as they adhere to the three requirements outlined next.
1)
2)
Differentiability. NeuSolv can only be applied to
differentiable loss functions, as is the case with our
encodings for each expression in Table II.
Non-Zero Gradient Until SAT. The derivative of the
loss function should not be zero until we ﬁnd the
satisﬁable assignments. For example, consider our
encoding of the constraint a  b and
a (cid:54)= b are similar to the constraint a < b.
Finite Lower Bound for Loss Functions. The loss
function for each constraint needs to have a ﬁnite
lower bound. Without this, the procedure would con-
tinue minimizing one of the constraints indeﬁnitely
in (say) a conjunction of clauses. For instance, our
encoding of a (cid:54)= b as L = max(−1,−abs(a−b+β))
carefully ensures a ﬁnite global minimum.3 If we
instead encoded it as L = −abs(a − b + β), the
loss function would have no ﬁnite minimum. When
we consider the conjunction of two clauses, say
(a (cid:54)= b) ∧ (c < d), the joint loss function for the
conjunction of the two clauses is the sum of the
individual losses. NeuSolv may not know where to
terminate the minimization of the loss for (a (cid:54)= b),
preventing it from ﬁnding the satisﬁable assignment
for the conjunction. To avoid this, our encoding adds
an explicit lower bound of −1.
F. Optimizations
NEUEX applies ﬁve optimization strategies to reduce the
computation time for neuro-symbolic constraint solving.
Single Variable Update. Given a set of input variables
to neural constraint, NEUEX only updates one variable for
each enumeration in NeuSolv. In order to select the variable,
NEUEX computes the derivative values for each variable and
sorts the absolute values of derivatives. The updated variable is
the one with the largest absolute value of the derivative. This
is because the derivative value for each element only computes
the inﬂuence of changing the value of one variable towards the
value of loss function, but does not measure the joint inﬂuence
of multiple variables. Thus, updating them simultaneously may
increase the loss value. Moreover, updating one variable per
iteration allows the search engine to perform the minimum
number of mutations on the initial input in order to prevent
the input from being invalid.
Type-based Update. To ensure the input is valid, NEUEX
adapts the update strategy according to the types of variables.
If the variable is an integer, NEUEX ﬁrst binarizes the value of
derivatives and then updates the variables. If the variable is a
ﬂoat, NEUEX updates the variable with the actual derivatives.
Caching. NEUEX stores the updated results for each enumer-
ation in NeuSolv. As the search algorithm is a deterministic
approach, the ﬁnal generated result is the same if we have the
same input, neural constraints, and the loss function. Thus, to
avoid unnecessary re-computation, NEUEX stores the update
history and checks whether current input is cached in history.
If yes, NEUEX reuses the previous result; otherwise, NEUEX
keeping searching for a new input.
SAT Checking per Enumeration. To speed up the solving
procedure, NEUEX veriﬁes the satisﬁability of the variables
after each enumeration in NeuSolv. Once it satisﬁes the
symbolic constraints, NeuSolv terminates and returns SAT to
3Here β can be any non-zero and small real value.
NEUEX. This is because not only the result achieving global
minima can be the satisﬁable result of symbolic constraint.
For example, any result can be the satisﬁable result of the
constraint a (cid:54)= b except for the result satisfying a = b. Hence,
NEUEX does not wait for minimizing the loss function, but
checks the updated result for every iteration.
Parallelization. NEUEX executes NeuSolv with different ini-
tial input in parallel since each loop for solving mixed con-
straints is independent. This parallelization reduces the time
for ﬁnding the global minima of the loss function.
V.