The nodes that did not join the cluster in a round remained
in NO-TDMA-STATE resulting in lower throughput. To repair
the fault, we increased the number of time slots for TDMA
and introduced a random exponential backoff mechanisms.
2) Diagnosis with CADeT: When the throughput dropped
at the base station, the traces from several nodes were exam-
ined. The abnormal control-ﬂow in the trace revealed that some
nodes did not have a slot assignment. We then conﬁrmed that
TDMA schedule broadcast was indeed received. We analyzed
the trace to ﬁnd the cluster head from the Join message sent
to the cluster head in that round. When trying to pair the
Join messages, we noticed the cluster head did not receive the
Join message and therefore, did not allocate a slot for that
node in the TDMA schedule. Since Join message send was
recorded but not receipt, the link between cluster node and
the cluster head must have failed either due to congestion or
channel corruption. When we made the channels perfect in
our simulations, we still observed the same result leading us
to identify Join message collision as only possible explanation
for link failure.
C. Data Race in LEACH
1) Fault Description: When we increased the number of
nodes in the simulation to 100, we noticed signiﬁcant reduc-
tion in throughput. Similar to the above case study, the nodes
entered NO-TDMA-STATE and didn’t participate in sending
data to the clusterhead. However, the root cause was different.
The problem was due to a data race between two message
sends that happens only at high load.
After sending the TDMA schedule message, the cluster head
moves into the next state and sends a debug message to the
base-station indicating it is the cluster head and the nodes in its
cluster. When the load is high, the sending of TDMA schedule
message may be delayed because of channel contention. This
in turn affects the sending of debug message as the radio is
busy. In WSNs, message buffers are usually shared among
multiple sends. It’s not uncommon to use one global shared
buffer for sending a message as only one message can be sent
at a time. When attempting to send the debug message, before
checking the radio was busy, the message type of the global
shared buffer was modiﬁed unintentionally and therefore, the
TDMA schedule message was modiﬁed into a debug message.
Due to this implementation fault, the global send buffer was
corrupted which resulted in wrong message being delivered.
The nodes in the cluster dropped this message after seeing the
type, which is intended only for the base station. This error
manifested only when the number of nodes was increased
because the increase in load caused the TDMA schedule
message to be retried several times and the original time slot
was not enough for the message transmission. We ﬁxed this
error by removing the fault as well increasing the time slot
size to send TDMA message.
2) Diagnosis with CADeT: We examined several node
traces after noticing poor throughput. We found that the cluster
nodes were in the same state NO-TDMA-STATE as the above
case study. Since we ﬁxed the join message congestion, we
examined the traces closely and noticed that some unexpected
message was received after sending the Join message. When
we paired that message receive with the sender, we realized
that message was a TDMA schedule message. From the
receiver trace control-ﬂow, it was clear that the message was
of unexpected type. However, the message was not garbled
as it passed the CRC check at the receiver. This indicated
that the problem was at the sender. We examined the sender’s
control-ﬂow closely and the trace indicated that there was a
state transition timer event ﬁred between the TDMA schedule
message send and the corresponding sendDone event in the
cluster head. From the sender’s control-ﬂow, we noticed that
debug message send interfered with the TDMA send and the
implementation fault that corrupted the message buffer was
discovered.
D. Intrusion Detection Failure in Pursuer-Evader Networks
WSNs used for military or border surveillance [43], [44],
[45] are modeled as pursuer-evader games, where the WSN is
the pursuer and the intruder is the evader. The main goal of
these WSNs is to alert the base station when an an intruder is
detected by sensors. To avoid congestion of alerts sent to the
base station, one node acts as a leader and alerts the base
station of the intruder. The following simple decentralized
leader election protocol is employed. The nodes broadcast the
signal strength detected to their neighbors and the node with
the strongest signal elects itself as the leader. In these WSNs,
failing to detect an intruder is a serious problem and hence
needs to be diagnosed.
1) Fault Description: The failure to detect an intrusion can
be caused by link asymmetry, time synchronization error [8],
or link failure. Let node A be the node with the strongest
signal during an intrusion. If there is link asymmetry, node A
would not get neighbors broadcast while they get node A’s
broadcast. The neighbors would assume node A will elect
itself as leader. However, node A would falsely assume that
the signal detected locally was spurious because it did not hear
from other neighbors. Therefore, node A will not elect itself
as the leader and the intrusion will not be detected. A similar
situation may arise if there is a time synchronization error.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:43 UTC from IEEE Xplore.  Restrictions apply. 
Node A may check for neighbors broadcast before they are
supposed to be received because of time synchronization error.
Node A would falsely assume spurious local detection and not
elect itself as a leader. If the link between node A and the base
station fails, the intrusion detection failure occurs. In addition,
intrusion detection failure can occur due to implementation
fault in the code. It is important to detect, diagnose and repair
such failures. Missing an intrusion can be determined by the
base station if the intruder gains illegal access or some other
part of the network catches the intruder.
2) Diagnosis with CADeT: When a failure report is re-
ceived at the base station, it pulls the recent traces from
the neighborhood. Note that the traces may contain messages
exchanged before and after the intrusion because these WSNs
are constantly running. Since CADeT traces allow the ability
to pair message sends and receives despite losses, it is possible
to identify the time window in which the intrusion occurred
(when there was broadcast among neighbors). Note that even
when there are multiple intrusions, each intrusion occurrence
can be identiﬁed due to increasing sequence numbers assigned
to the message exchange generated by the intrusion. When the
time window of the intrusion is identiﬁed from the traces, the
detection failure can be narrowed down. If the traces show that
an alert was sent by a node but that alert was not received at
the base station, then the reason is the failure of link between
the elected leader and the base station. If the traces show that
a node, say node A has not recorded local broadcasts receipts
but other nodes traces reveal the local broadcast sends and
receipts, it is likely this node suffers from link asymmetry.
If the control ﬂow of other nodes show that those nodes did
not expect to become the leader, then it is clear that this node
was supposed to be the leader but due to link asymmetry it
did not become a leader. If the node A’s trace does have the
receipt of the broadcast messages but the control-ﬂow shows
that node A assumed that the local detection of the intruder
was a spurious signal before receiving the broadcast messages,
it is likely node A was the supposed-to-be leader that was
unsynchronized with the other nodes.
E. Serial Message Loss in Directed Diffusion
Directed diffusion [46] is a communication paradigm that
allow nodes (sink nodes) to express interest in data from other
nodes in the network and these interest messages are propa-
gated throughout the network through controlled ﬂooding. The
nodes matching the interest act as source nodes and send data
back to the sink nodes using the paths taken by interest but
in the opposite direction. An interest can be satisﬁed by a
single data message or a stream of data messages from the
source nodes. To achieve directed diffusion, nodes maintain
an interest cache and a data cache. When an interest message
is received, a node adds an entry to its interest cache if it is not
already there, forwards the interest message to its neighbors
other than the interest message sender, and creates a gradient
(parent) towards the neighbor that sent the interest message.
When a data message is received, a node checks for a matching
interest in the interest cache. If a matching interest is present
and the data message is not in the data cache, the data message
is added to the data cache and is forwarded to all parents that
have expressed interest in that data. Once the data ﬂow for
an interest is stabilized, the interest message will be renewed
only through most reliable neighbor, and thereby, reducing
duplicate trafﬁc eventually.
1) Fault Description: There are two practical
issues,
namely, timestamp overﬂow and node reboots that are not
handled well
in directed diffusion design and both issues
manifest as a continuous loss of messages at a different node
(parent node) as discussed in Khan et al. [7]. Let node A be
a source or forwarding node that satisﬁes an interest from the
parent node B. In the case of timestamp overﬂow, parent node
B drops the packet because of older timestamp. However in
the case of node A reboot, node A drops the packet to be
sent/forwarded to parent node B because its interest cache is
wiped out after reboot. In both cases node, the manifestation
is the same, which is B observes lower message rate and
continuous message loss.
2) Diagnosis with CADeT: When node B reports loss of
messages from node A, trace from these nodes are pulled to
the base station. The last message sent from node A to node
B is compared with the last message received at node B from
node A. If they two match, it implies node A has not been
sending more messages and perhaps has some problems. A
mismatch implies the messages are either dropped at node B
or link failure. In the former case, node A’s control-ﬂow trace
is examined, which would reveal a reboot as the initialization
functions called after the reboot would appear in the control-
ﬂow trace. In the latter case, node B’s control-ﬂow trace is
examined. If the timestamp overﬂow happened, the control-
ﬂow trace would show that the code took a different path at
the condition checking timestamp of the messages. If none of
the two cases happened, the message loss is the most likely
due to failure of link between nodes A and B.
V. ANALYTICAL EVALUATION
We analytically compare our approach to the adapted state-
of-the-art, Liblog [29], showing that our approach reduces the
trace size. For tractability and fairness we made several sim-
plifying assumptions. First, traces are uncompressed. Second,
each node communicates with a small subset of nodes called
its partners. Among its partners, each node communicates with
some of them regularly and some of them irregularly. Third,
each node sends messages at the same rate to its partners. The
notation used in the analysis is shown in Table I.
A. CADeT
The size of the trace generated at a node by CADeT
(γN ) depends on the checkpoint stored and the trace entries
generated by messages sent and received by that node.
First, we calculate the size of the trace generated at a node
in a checkpoint interval due to messages sent by a node (γS).
The number of messages sent in a checkpoint interval, α, is
the product of average message rate from a node to a partner
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:43 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I
NOTATION USED IN THE ANALYSIS
n
nP
nR,
nO
tC
rS
pO
pI
pL
bI
bS
bA,
bP
bF
γN
γS,
γR
γC
δN
δS,
δR
number of nodes in a WSN
number of partners that a node communicates with in a checkpoint
interval
number of partners that a node communicates regularly and
occasionally respectively
checkpoint interval
rate of messages sent from a node to a partner
percentage of occasional partners that a node communicates with
in a checkpoint interval
probability that a message arrives in order
probability of a message loss
number of bytes required to record a message send or an in-order
message receive
number of bytes required to record a sequence numbers by
CADeT
number of bytes required to record an AAMap entry and a PCMap
entry in the trace respectively
number of bytes required to record a sequence number by Liblog
size of the trace generated by CADeT due to messages sent to
and received from all its partners in a checkpoint interval
the size of the trace generated by CADeT due to messages sent to
a partner or received from a partner respectively in a checkpoint
interval
size of the trace generated by CADeT due to recording a check-
point in the trace
size of the trace generated by Liblog due to messages sent to and
received from all its partners in a checkpoint interval
size of the trace generated by Liblog due to messages sent to a
partner or received from a partner respectively in a checkpoint
interval
(rS) and the total length of the checkpoint interval (tC) and
is given in Eq. 1.
α = rS tC
(1)
The number of bits used to represent each message in the
trace (bI) is the sum of number of bits used to record that
a message has been sent and the number of bits to record
the destination alias. Since there are α messages sent in a