input layer of AttBLSTM. We split the sequence into windows
and each window contains W in intervals with no overlapping
with other windows. Then we compute the frequency of each
interval value and aggregate them into three buckets [0− 100],
[100−500] and [500−]. As a result, each window is converted
into a vector of three frequency buckets.
After processing the input,
the LSTM layer learns the
predictive features from the sequence and the attention layer
captures the dependencies between the sequence of features
and the output. The embedding layer after that produces a
vector of 64 numerical values. Finally, the fully-connected
layer converts the embedding vector into a classiﬁcation vector
with the size the same as the number of proﬁled webpages.
The soft-max layer assigns probabilities to each class, and the
one with the highest probability is returned to the adversary.
Through this pipeline, we found that a webpage can be
classiﬁed at high accuracy, but it also requires many sequences
of a webpage to be collected beforehand for training. If
removing the layers after the embedding layer, we can turn
the AttBLSTM model into an embedding model and classify a
webpage without retraining. For instance, we can generate the
embedding (E1) of one sequence (S1) of a webpage W with
the pre-trained model. For a new sequence S2 encountered
in the testing stage, we can generate its embedding E2 and
compute its distance to E1 (Cosine distance) and classify it
into W if the distance is lower than a threshold. We adjust
the pre-processing step by using the average delay as the
feature for each time window instead of the three bucketed
frequencies. We call the two settings classiﬁcation mode and
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
329
hehhhehhhehh+......xxxOutput LayerAttentionLayerLSTMEmbeddingLayerInputLayerythe bandwidth gap,
Although NVMe SSDs can generate a large amount of
trafﬁc in a small duration to ﬁll
the
latency of a single request has a large variance, because
SSDs run many internal jobs, e.g., garbage collection and
anti-wearing [61]. To address the problem, we use pipelined
requests instead of mutually exclusive requests with a fence.
Speciﬁcally, the attacker always ﬁlls up the send queue of her
NVMe SSD, so the SSD will be busy sending responses to
the PCIe link. The attacker does not measure the latency on
each request, but measure the interval between consecutive
response completions, which eliminates the variance caused
by SSD internal jobs.
We experimented with numerous settings of data trans-
mission, and found that reading 4KB (8 LBAs, 512 bytes
each LBA) at a ﬁxed position in each request can produce
the desired trafﬁc volume and maintain a high and stable
sampling rate. As a result, the attacker can issue more than
812K read operations per second (IOPS) that generates about
3.1 GB/s throughput. On top of this throughput, the Ethernet
NIC (x1 lane) can easily saturate the upstream of the PCH (x4
lane). Besides, we investigated three reading orders of NVMe
SSD, including random read, sequential read and read from a
constant block, and found constant read yields the most stable
delays.
To precisely measure the NVMe SSD I/O interval, we
use Storage Performance Development Kit (SPDK) [33], a
toolkit for low latency kernel bypass I/O. SPDK allocates a
pair of send queues and completion queues, which are shared
between the user’s process and the device. When there is an
I/O operation launched by the process to the NVMe SSD,
SPDK generates an item in the send queue containing all
the parameters about the request, and then notiﬁes the SSD
via door bell, which writes a memory-mapped register of the
SSD. The controller of the SSD will immediately fetch the
request from the send queue and process it. After that, the
result will be written to a predeﬁned memory location and an
item will be saved in the completion queue. Developer can
get the completion information by a SPDK procedure called
(spdk_nvme_qpair_process_completions()) . The
algorithm of this NVMe SSD probe is shown by Algorithm 2.
We write about 200 lines of C code for this NVMe SSD probe.
B. Attack 4: Webpage Inference with NVMe SSD
Among the three attacks demonstrated in Section IV, the at-
tack about webpage inference is more relevant in this scenario,
as users’ activities on the other two do not directly involve
NIC. Similar to Attack 2, we collected a delay sequence
for each webpage visit, and classify it using the AttBLSTM
model. During pre-processing, we again split the sequence
into windows, but this time we count the maximum delay in
the window as a feature instead of using the three frequency
buckets, because the delays do not have the same distribution.
Other statistical features, like average delay, lead to a similar
result, so we use the maximum delay for simplicity.
Algorithm 2: Collection of Intervals with NVMe SSD
Result: IntervalSeq
Alloc SendQueue, CompletionQueue;
SendQueue.enqueue(a batch of read operation);
lastTimestamp = RDTSCP();
while notEnough(IntervalSeq) do
while isEmpty(CompletionQueue) do
;
end
currentTime = RDTSCP();
CompletionQueue.dequeue();
IntervalSeq.append(currentTime - lastTimestamp);
lastTimestamp = currentTime;
if SendQueue.isAlmostEmpty() then
SendQueue.enqueue(a batch of read operation);
end
end
VI. EVALUATION
In this Section, we evaluate the effectiveness of Attack 1-4.
In Section VI-A, we describe the platform used for experiment.
Section VI-B, VI-C, VI-D, VI-E report the results for each
attack. Section VI-F describes the evaluation on a public cloud.
A. Experiment Platform
We use a regular desktop PC and a server as the experiment
environment, which is similar to prior works in studying
remote side-channel attacks [23], [24], except that we use one
less desktop PC because we assume the victim’s application
directly runs on the server. The speciﬁcations of the machines
are shown in Table I.
In Figure 1, we give an abstraction of PCIe topology on a
machine. Here we elaborate on the topology (also illustrated
in Figure 6) on our server, which is more complex. Though the
CPU only has 3 PCIe interfaces, the PCIe switch (PEX 8747
PCIe) and the 8 I/O multiplexers (IT8898 chips) expand the
support to 4 PCIe x16 devices at full speed. Two PCIe devices
have high chances to share the same switch (e.g., plugged into
Slot 3 and Slot 5 to have x16 PCIe link).
The RDMA NIC of the server has a direct Inﬁniband
connection to the PC’s RDMA NIC. Though in data centers,
there is usually an Inﬁniband switch between two of its
machines to enable RDMA connection, we do not introduce
the switch into the experiment platform because it is too
expensive (over 240,000 US dollars [62]). On the other hand,
the latency with and without the Inﬁniband switch are similar
(e.g., extra 130 nanoseconds [38]).
B. User-input Inference with RDMA NIC
We use gedit and Google Chrome (version 79.0.3945.88)
to simulate the applications that receive the victim’s input.
Two webpages, Google login page (accounts.google.
com/ServiceLogin/signinchooser) and Amazon lo-
gin page (www.amazon.com/ap/signin), are tested on
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
330
TABLE I: Speciﬁcation of the evaluation platform.
CPU
Motherboard
Memory
RDMA NIC
Ethernet NIC
NVMe SSD
GPU
OS
Intel Xeon Platinum 8260
Supermicro X11SPA-TF
Server
64G
Mellanox MCX556A
Intel I-210
Nvidia 1080 Ti
-
Ubuntu 18.04 (4.15.0-112)
PC
Intel Core i3 8100
MSI B360
32G
Mellanox MCX455A
Intel I-219-V
-
GP-ASM2NE6100TTTD
Ubuntu 18.04 (5.0.0-31)
times by another volunteer and the recovered keystroke timing
by the prior stage was sent to the HMM model for the word
inference. We found the real words are highly ranked among
the 1000 words in the dictionary after sorted by HMM: 66.7%
ranked in the top 10 and 95.3% among the top 50. Our result
is comparable to [12], which tested 39 words from a 2103-
words dictionary, and the top-10 accuracy and top-50 accuracy
are 40% and 86%.
TABLE III: Paramters of AttBLSTM model.
Google Chrome. The victim types in the text on the ap-
plications on the server through a remote desktop software
TigerVNC 1.7.0, and the attacker recovers keystroke events
at her PC side. The remote GPU acceleration is enabled by
conﬁguring the X display name ﬁeld of TigerVNC to 0.
Inference of keystroke events. We ﬁrst evaluate the inference
accuracy of the victim’s keystroke events under INVISIPROBE.
We recruit 1 volunteer to type in an English article, containing
480 characters. We align the timeline of the actual keystrokes
and the detected ones, and count
the false positives (the
keystrokes detected by mistakes, or FP) and false negatives
(the missed keystrokes, or FN). Assume the number of true
positives (all real keystrokes) is C, false-positive rate (FPR)
and false-negative rate (FNR) are deﬁned as F P
C and F N
C .
The result shows all events can be identiﬁed at the right
time, resulting in 0% FNR and FPR. However, without
removing the blinking caret, some of the detected keystrokes
are false positives, resulting in 5.88% FPR (30 caret blinkings).
After removing the carets based on the heuristics described in
Section IV, we found FPR drops to 1.03% (5 carets recognized
as keystrokes), but FNR is still 0%. Table II lists the accuracy
(i.e., 1-FPR).
TABLE II: Accuracy of recovering keystroke events.
Keystroke & Caret Events
100%
w/o Caret Removal
94.18%
w/ Caret Removal
98.97%
Word recovery. We construct an HMM by following the steps
in [12] with a ten-letter alphabet (“etaoinshrd”). There are 100
possible transitions between a pair of letters in the alphabet
and we collect 40 keystroke intervals for each pair with a
volunteer, and use the intervals to train the HMM. Then we
draw the ﬁrst 43 words from a 1000-words dictionary [63]
that are composed of our alphabet as the words to be tested.
Appendix D lists the selected words. Each word was typed 3
Fig. 6: Part of the block diagram of Supermicro X11-SPA TF.
Web-GPU ML-GPU Web-NIC
Paramter
Hidden Size
Attention Size
Embedding Size
Learning Rate
LR Decay
Weight Decay
Dropout
48
64
64
0.022
0.992
0.0003
0.16
Attack
48
64
48
0.016
0.995
0.0006
0.3
64
64
64
0.016
0.992
0.0004
0.18
C. Webpage Inference with RDMA NIC
We select the Alexa top 100 website homepages [64]5 as
the webpages targeted by the attacker. The scale of our data is
comparable to prior works that infer webpage visits through
side-channels, e.g., 100 websites for [65], [66]. We use the
top 100 websites to demonstrate attack effectiveness. In other
words, an attacker targeting a speciﬁc victim can adjust the
list to include websites that reﬂect the victim’s preferences,
political views, etc.
Each webpage was visited 150 times using Google Chrome
and each visit lasts 8 seconds. In total 15,000 delay sequences
were collected. We carried out cross-validation by splitting
the data into training and testing by 85% and 15% randomly.
We train the AttBLSTM model for 400 iterations with the
parameters shown in the second column of Table III.
Classiﬁcation mode. The evaluation metrics are similar to
Section VI-B in that we measure the accuracy of the top-N
candidates against the 100 tested webpages. We found the top-
1 accuracy can achieve 96.31%, i.e., 96.31% sequences can
be correctly classiﬁed with the top choice from our model,
while the top-3 accuracy can achieve 99.16%. We analyzed
the mis-classiﬁed sequence and found that the main reason is
the unstable network condition for some visits.
Embedding mode. We collected 100 new webpages (termed
Wnew) to test this mode. The trained AttBLSTM model is the
same as the one used for the classiﬁcation mode. For each
webpage in Wnew, we ﬁrst collect 1 delay sequence (ei) and
generated a set of embedding vectors (termed Enew). Then,
we collect another delay sequence (e(cid:48)
i) for every webpage
in Wnew, and compute its Cosine distance to every vector
in Enew. If the distance between ei and e(cid:48)
i under the same
webpage is under the threshold (0.474) while the distances
between ei and all other embedding vectors are larger, we
consider it as a true positive. In the end, 90.77% accuracy
can be achieved on Wnew.
5Due to that our machine is located in China, we selected the top 100
Chinese websites, which are not blocked by the Great Firewall.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
331
D. Model Inference with RDMA NIC
We choose ten popular machine-learning models (8 DNN
and 2 simple models) and collect their delay sequences for