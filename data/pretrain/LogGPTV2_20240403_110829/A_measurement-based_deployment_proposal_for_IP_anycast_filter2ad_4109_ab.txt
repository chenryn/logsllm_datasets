### Table 3: Geographic Distribution of Clients Used in Our Study
- **0.864**
- **100.000**
- **12931**
- **317**
- **461**
- **5585**
- **2402**
- **566**
- **1196**
- **187**
- **9**
- **204**
- **23858**

### Geographic Distribution of the Clients Used in Our Study

#### Internal Deployment
To manipulate advertisements at individual servers, we deployed our own IP Anycast service, which we refer to as the internal deployment. For this, we obtained a /22 prefix (204.9.168.0/22) and an AS# (33207) from ARIN. We then deployed anycast servers at five sites listed in Table 2. Each server advertises this prefix and AS# into BGP through their host site and upstream provider, forming an IP Anycast group.

**Note:**
- The "host-site" refers to the server's immediately upstream AS.
- The "upstream provider" refers to the closest major ISP (tier-1 or tier-2) that provides transit for the server’s traffic.
- For example, the internal deployment anycast server at Cornell has Cornell (AS# 26) as its host-site and Williams Communication (WCG – AS#7911) as its upstream provider.
- This distinction between the host-site and upstream provider is unique to our internal deployment; in most commercial IP Anycast deployments, the host-site is also the upstream provider.

#### Challenges and Solutions
The internal deployment is currently small in scale. The primary challenge in expanding it has been the site-by-site negotiation with upstream providers (e.g., ATT, WCG) to clear the advertisement of our anycast prefix. We do not require these providers to actively inject our prefix into BGP but only to propagate the advertisement from the anycast servers onwards. Approval from ISPs is necessary due to the access control they enforce on AS numbers and network prefixes advertised by their customers.

To accelerate the deployment, we are in the process of deploying anycast servers over NLR [46]. Further details about the internal deployment and requirements for volunteer sites are available at [44].

While the small size of the internal deployment raises questions about the generality of our results, the intuitive reasoning behind most of our findings supports the applicability of our study.

### Methodology

A key observation guiding our measurement methodology is that all three external deployments are DNS services. Therefore, the anycast servers in these deployments are DNS nameservers, and we can probe them using DNS queries. However, to ensure that we reach all the anycast servers, a large number of clients are needed. To achieve this, we used the King measurement technique [17], which allows for latency measurements between any two arbitrary hosts on the Internet by utilizing recursive nameservers near the hosts in question.

Using this approach, we identified 23,858 unique recursive nameservers. Table 3 details the geographic distribution of these nameservers, which belong to 7,566 different ASs. This provides us with a view of the anycast deployments from 7,566 of the 18,391 routable ASs on the Internet (based on a BGP routing table obtained from Route-Views). The quantity and spread of these nameservers make us confident that our measurements closely reflect the behavior of IP Anycast as seen from hosts in the Internet in general.

### Evaluating IP Anycast Deployment

To evaluate an IP Anycast deployment from the perspective of any client (say X) in the list, we need to direct X to send packets to the anycast address of the deployment. As suggested by [17], we leveraged the fact that X is a recursive nameserver and is willing to resolve DNS queries on behalf of other hosts. We can "trick" client X into sending DNS queries to an anycast address by making the NS record for a domain point to the address in question and querying X for any record in that domain. We used anycast.guha.cc, a domain we own, for this purpose.

For example, in the case of the F root-server deployment with anycast address 192.5.5.241, we created a domain f-root.anycast.guha.cc with its NS record pointing to 192.5.5.241. As illustrated in Figure 1, querying client X for any record in this domain (e.g., random_no.f-root.anycast.guha.cc) causes X to resolve the NS record for f-root.anycast.guha.cc (packets (2) and (3)), and then send a query to the anycast address for the F root-server deployment (packet (4)).

### Latency Measurement

The experiments presented in the paper use this basic technique for various tasks such as determining the particular anycast server accessed by each client and the latency of doing so. For example, to determine the latency from a client X to the anycast address of the F root-server, we:
1. Send a recursive DNS query to client X for the NS record for f-root.anycast.guha.cc: this primes client X’s cache with the fact that the F root-server anycast address corresponds to the authoritative nameserver for the domain f-root.anycast.guha.cc.
2. Send an iterative DNS query to client X: since an iterative query is answered by client X based on local information, this provides us with an estimate of the latency for packets {(1),(6)} in Figure 1.
3. Send a recursive DNS query to client X for the A record for random_no.f-root.anycast.guha.cc: as shown in Figure 1, this causes client X to send packets to the F root-server anycast address and provides us with an estimate of the latency for packets {(1), (4), (5), (6)}.

This process is repeated eight times, and the difference between the minimum measured latency for packets {(1), (4), (5), (6)} and {(1), (6)} is used as an estimate of the round-trip anycast latency from client X to the F root-server.

### Proximity

The value of anycast as a server selection primitive lies in its ability to find close servers. With IP Anycast, packets destined to an anycast address are routed to the server closest to the client in terms of the metrics used by the underlying routing protocol. For inter-domain IP Anycast, it is the BGP decision process (including routing policies) at the various ASs that governs the anycast server accessed by each client. This implies that anycast packets from clients may not be delivered to servers that are close in terms of latency. Recent studies [4,32] have indicated that IP Anycast offers poor latency-based proximity.

In this section, we use latency measurements from our approximately 20,000 clients to show that this is indeed the case for existing anycast deployments. However, we also argue that poor latency can be avoided through a planned deployment.

### Methodology for Proximity Evaluation

To determine the quality of proximity offered by an IP Anycast deployment to a given client, we need to determine the following latencies:
- **Unicast Latency to all anycast servers**: Here, unicast latency to an anycast server is the latency from the client to the unicast address of the server. Given that each client is a recursive nameserver, the King approach for determining latencies between two hosts applies directly.
- **Anycast Latency for the client**: The procedure for determining the anycast latency for a client was described in the previous section.

We define the stretch-factor as the difference between the anycast latency and the minimum unicast latency for a client. The stretch factor thus represents the quality of latency-based proximity an anycast deployment offers a client. We determined the stretch factor for each client in our list for the external and internal anycast deployments.

### Results

Figure 2 shows the CDF for the stretch factor for all the clients. We see that for all four anycast deployments, a fair fraction of clients are not routed to the server closest to them. For example, the number of clients that are routed to a server that is more than 30msec farther away from the closest server ranges from 31% (for the internal deployment) to 61% (for the AS112 deployment). Similarly, in the case of the J root-server deployment, 40% of the clients incur a stretch-factor of more than 100msec.

The internal and external anycast deployments have been deployed such that most of the anycast servers have different upstream providers. For example, in the case of the internal deployment, the anycast servers have three different upstream providers. We believe that this deployment model makes these services vulnerable to the fact that Internet routing is not based on latency, as reflected by the inefficacy of IP Anycast in selecting nearby anycast servers for these deployments.

### Anecdotal Scenario

To illustrate the causes of poor proximity, consider a publicly available traceroute-server at UC-Berkeley (net.berkeley.edu) acting as a client trying to access the internal anycast deployment. Anycast packets from this client are routed to the server at Cornell (latency=87msec) instead of the nearby server at Berkeley (latency=9msec). Figure 3 shows the relevant AS-level connectivity with the relevant POPs of ATT (upstream provider for the server at Berkeley) and WCG (upstream provider for the server at Cornell).

We used BGP looking-glass servers to determine that Level3 has at least two paths for the internal deployment’s anycast prefix: one through WCG’s Santa Clara POP with AS-PATH=[7911, 26, 33207] and the other through ATT’s San Francisco POP with AS-PATH=[7018, 2386, 33207]. The Level3 routers in the area choose the first path as the best path, and hence, anycast packets from the client are delivered to the anycast server at Cornell. This path is labeled as “Actual Path” in the figure.

### Conclusion

This example highlights why Internet routing yields poor proximity for the measured anycast deployments. From the point of view of inter-domain routing, an anycasted AS is equivalent to a multi-homed stub AS. However, anycasting introduces multi-homing scenarios that differ significantly from normal multi-homing scenarios. In typical multihoming, multiple peerings of the multihomed stub AS are in the same geographical area, leading to acceptable performance. On the other hand, for an anycasted AS, the multiple peerings are geographically dispersed, and this is not accounted for in the existing inter-domain routing setup.