0.864
100.000
12931
317
461
5585
2402
566
1196
187
9
204
23858
Table 3: Geographic distribution of the clients used
in our study.
nipulating the advertisements at individual servers. For such
experiments we deployed our own IP Anycast service that
we call the internal deployment. For this, we obtained a
/22 preﬁx (204.9.168.0/22) and an AS# (33207) from ARIN
and deployed anycast servers at the ﬁve sites listed in ta-
ble 2. Each server advertises this preﬁx and AS# into BGP
through their host site and on to their upstream provider
and hence they form an IP Anycast group.
Note that an anycast server’s “host-site” refers to the
server’s immediately upstream AS while the “upstream
provider” refers to the closest major ISP (tier-1 or tier-2)
that provides transit for the server’s traﬃc. For example,
the internal deployment anycast server at Cornell has Cor-
nell (AS# 26) as its host-site and Williams Communication
(WCG – AS#7911) as its upstream provider. This diﬀer-
ence between the host-site and upstream provider is a quirk
of the internal deployment; for most commercial IP Anycast
deployments, the host-site is also the upstream provider.
The internal deployment is as yet very small in scale. The
biggest hurdle in growing the deployment has been the site-
by-site negotiation with upstream providers (ATT, WCG)
to clear the advertisement of our anycast preﬁx. Note that
we do not require these upstream providers to actively in-
ject our preﬁx in the BGP but only propagate the adver-
tisement from the anycast servers onwards.
Instead, the
approval from the ISPs is only due to the access control
ISPs often enforce on the AS numbers and network preﬁxes
they expect to see advertised from their customers. To ac-
celerate this site-by-site deployment, we are currently in the
process of deploying anycast servers over NLR [46]. Further
details about the internal deployment as well as the require-
ments for much welcome volunteer sites are available at [44].
While the small size of the internal deployment certainly
raises questions regarding the generality of our results, the
fact that (in retrospect) most of our results follow intuitive
reasoning supports the applicability of our study.
4. METHODOLOGY
A key observation guiding our measurement methodol-
ogy is that all three external deployments are DNS services.
Hence, the anycast servers that are part of these deploy-
ments are all DNS nameservers and we can probe them us-
ing DNS queries. However, since packets from a client to
the anycast address of any given anycast deployment are
delivered to one of the servers, a large number of clients
are needed to ensure that we are able to reach all the any-
cast servers. To achieve this we used the King measurement
technique [17]. King allows for measurement of latency be-
F root-server
Auckland
F root-server
Amsterdam  
F root-server
Barcelona  
4
5
Client X
(Recursive
Nameserver)
3
2
6
1 
F root-server
Toronto 
Measurement Host (M) 
1  A? random_no.f-root.anycast.guha.cc 
2 NS? f-root.anycast.guha.cc
3 NS(f-root.anycast.guha.cc) 
=f.root-servers.net 
A(f.root-servers.net)=192.5.5.241
4 A? random_no.f-root.anycast.guha.cc 
5 NXDOMAIN
(or a pointer to the root-servers) 
6 --same as (5)--
Figure 1: Measurement Host M uses DNS queries
to direct client X to send packets to the anycast
address associated with the F root-server deploy-
ment. In the scenario depicted here, packets from
X are routed to the anycasted F root-server located
at Auckland, Australia. Note that we use a domain
under our control (anycast.guha.cc) to trick client X
into assuming that 192.5.5.241 is the authoritative
nameserver for the domain f-root.anycast.guha.cc.
tween any two arbitrary hosts on the Internet by utilizing
recursive nameservers near the hosts in question.
Using the same basic idea, we used recursive DNS name-
servers in the Internet as clients in our study. To this eﬀect,
we took an address in each routable preﬁx in a BGP routing
table obtained from Route-Views [49] and determined if any
of the nameservers authoritative for the in-addr.arpa name
associated with the address had recursion enabled. For ex-
ample, for the preﬁx 128.84.223.0/24, we determined if any
of the nameservers authoritative for the name 1.223.84.128.
in-addr.arpa. had recursion enabled. This approach yield-
ed a list of 23,858 unique recursive nameservers. Table 3 de-
tails the geographic distribution of these nameservers. The
nameservers belong to 7,566 diﬀerent ASs. Hence, they pro-
vide us a view of the anycast deployments from 7,566 of the
18,391 routeable ASs on the Internet (based on a BGP rout-
ing table obtained from Route-Views). The quantity and
the spread of these nameservers makes us conﬁdent that our
measurements closely reﬂect the behavior of IP Anycast as
seen from hosts in the Internet in general.
Note that using a large number of clients yields a more
representative picture for all the metrics we measure in this
paper. For example, the scale of our study makes our argu-
ments regarding client load distribution across the anycast
deployment signiﬁcantly more representative than (say) just
using PlanetLab for measurements. Moreover, our use of
widely-dispersed Internet nameservers as clients avoids the
bias that would be introduced were we to use PlanetLab
hosts as clients. This bias could severely impact the aﬃn-
ity, proximity and load measurements. For example, one of
the J-root servers is connected to the GEANT network and
hence, can be accessed through Internet2. Consequently, a
large fraction of PlanetLab nodes are routed to this server
when they probe the J-root anycast address.
Evaluating an IP Anycast deployment from the perspec-
tive of any client (say X) in the list requires that we be
able to direct X to send packets to the anycast address of
the deployment. As suggested by [17], we leveraged the
fact that X is a recursive nameserver and hence, is willing
to resolve DNS queries on behalf of other hosts. We can
thus “trick” client X into sending DNS queries to an any-
cast address by making the NS record for a domain point
to the address in question and querying X for any record
in that domain. We used anycast.guha.cc, a domain we
own, for this purpose. For example, in case of the F root-
server deployment with anycast address 192.5.5.241, we cre-
ated a domain f-root.anycast.guha.cc with its NS record
pointing to 192.5.5.241. As illustrated in ﬁgure 1, query-
ing client X for any record in this domain (for example,
random no.f-root.anycast.guha.cc) causes X to resolve
the NS record for f-root.anycast.guha.cc (packets (2) and
(3)), and then send a query to the anycast address for the
F root-server deployment (packet (4)). A minor practical
problem with this approach is that a client may be conﬁg-
ured to resend query (4) a number of times on seeing that
the response (5) is an error. As suggested by [17], we weeded
out clients that may resend queries on receiving an error as
follows: we directed each client to the anycast address of the
internal anycast deployment and logged the DNS queries at
each of the servers of the internal deployment to determine
the number of times query (4) is sent.
The experiments presented in the paper use this basic
technique for various tasks such as determining the partic-
ular anycast server accessed by each client and the latency
of doing so. For example, to determine the latency from a
client X to the anycast address of the F root-server, we:
• Send a recursive DNS query to client X for the NS
record for f-root.anycast.guha.cc: this primes client
X’s cache with the fact that the F root-server anycast
address corresponds to the authoritative nameserver for
the domain f-root.anycast.guha.cc
• Send an iterative DNS query to client X: since an it-
erative query is answered by client X based on local
information, this provides us with an estimate of the
latency for packets {(1),(6)} in ﬁgure 1.
• Send a recursive DNS query to client X for the A record
for random no.f-root.anycast.guha.cc: as shown in
ﬁgure 1, this causes client X to send packets to the
F root-server anycast address and provides us with an
estimate of the latency for packets {(1), (4), (5), (6)}.
This process is repeated eight times and the diﬀerence be-
tween the minimum measured latency for packets {(1), (4),
(5), (6)} and {(1), (6)} is used as an estimate of the round-
trip anycast latency from client X to the F root-server.
5. PROXIMITY
The value of anycast as a server selection primitive is in
part due to its ability to ﬁnd close servers. With IP Any-
cast, packets destined to an anycast address are routed to
the server closest to the client in terms of the metrics used
by the underlying routing protocol. For inter-domain IP
Anycast, it is the BGP decision process (including routing
policies) at the various ASs that governs the anycast server
accessed by each client. This implies that anycast packets
from clients may not be delivered to servers that are close
in terms of latency and recent studies [4,32] have, in fact,
indicated that IP Anycast oﬀers poor latency-based proxim-
ity. In this section we use latency measurements from our ≈
20,000 clients to show that this is indeed the case for exist-
ing anycast deployments. However, we also argue that poor
latency can be avoided through a planned deployment.
F
F
D
D
C
C
 1
 1
 0.8
 0.8
 0.6
 0.6
 0.4
 0.4
 0.2
 0.2
 0
 0
-50
-50
F Root
F Root
J Root
J Root
AS112
AS112
Internal
Internal
Diff=30msec
Diff=30msec
 0
 0
 50
 50
 100
 100
 150
 150
 200
 200
Difference of Anycast and Minimum Unicast latency (msec)
Difference of Anycast and Minimum Unicast latency (msec)
Figure 2: CDF for the diﬀerence between the any-
cast and minimum unicast latency for the external
and the internal deployments.
Methodology: To determine the quality of proximity
oﬀered by an IP Anycast deployment to a given client, we
need to determine the following latencies:
• Unicast Latency to all anycast servers - here, unicast
latency to an anycast server is the latency from the
client to the unicast address of the server. Given that
each client is a recursive nameserver, the King approach
for determining latencies between two hosts applies here
directly.
• Anycast Latency for the client or the latency from the
client to the anycast address of the deployment. The
procedure for determining the anycast latency for a
client was described in the previous section.
We deﬁne stretch-factor as the diﬀerence between the any-
cast latency and the minimum unicast latency for a client.
The stretch factor thus represents the quality of latency-
based proximity an anycast deployment oﬀers a client. We
determined the stretch factor for each client in our list for
the external and internal anycast deployments.
Results: Figure 2 shows the CDF for the stretch factor
for all the clients. We see that for all four anycast deploy-
ments, a fair fraction of clients are not routed to the server
closest to them.5 For example, the number of clients that
are routed to a server that is more than 30msec farther away
from the closest server ranges from 31% (for the internal de-
ployment) to 61% (for the AS112 deployment). Similarly,
in case of the J root-server deployment, 40% of the clients
incur a stretch-factor of more than 100msec.
The internal and the external anycast deployments have
been deployed such that most of the anycast servers have
diﬀerent upstream providers. For example, in case of the
internal deployment, the anycast servers have three diﬀerent
upstream providers. We believe that it is this deployment
model that makes these services vulnerable to the fact that
Internet routing is not based on latency as reﬂected by the
ineﬃcacy of IP Anycast in selecting closeby anycast servers
for these deployments.
The following anecdotal scenario serves to illustrate the
causes for the poor proximity. We use the example of a pub-
licly available traceroute-server at UC-Berkeley (net.berk
5The poor selection in case of F root-server deployment can
be attributed to their use of hierarchical anycast [1,45] In
eﬀect, only 2 of the 27 F root-servers advertise the F-root
anycast preﬁx globally and so, it is not fair to compare the
anycast latency to the minimum unicast latency across all
the 27 servers. However, this is not the case for the other
deployments we measured.
UCB 
(25)   
CSUNET
(2152)
X   
Client 
(net.berkeley.edu)
Level3
(3356)
Cornell Server
(33207) 
Cornell 
(26)
1 
WCG (7911)
2   
3 
ATT (7018)
4
IR Berk  
(2386) 
Berk 
Server 
(33207) 
1  WCG (SantaClara POP)  2   WCG (NY POP) 
4 ATT (NY POP) 
3  ATT  (SF, CA  POP) 
Actual Path
Ideal Path
Figure 3: The relevant AS-level connectivity (with
corresponding AS numbers in parenthesis) between
the client (net.berkeley.edu) in UC-Berkeley and
the internal anycast deployment servers at Cornell
and Berkeley. Level3 receives at least two diﬀer-
ent advertisements for the internal deployment any-
cast preﬁx with the following AS-PATHs: [7911, 26,
33207] and [7018, 2386, 33207].
WCG 
(AS# 7911)
ATT 
(AS# 7018) 
ATT -World
(AS# 2686) 
Seattle
Berkeley 
Cornell 
Pittsburgh
Cambridge
Internal Deployment
(AS# 33207) 
Figure 4: AS-level connectivity for the anycasted in-
ternal deployment – from the point of inter-domain
routing, AS# 33207 is a multihomed stub AS.
eley.edu) acting as a client trying to access the internal
anycast deployment. Anycast packets from this client are
routed to the server at Cornell (latency=87msec) instead of
the close-by server at Berkeley (latency=9msec). Figure 3
shows the relevant AS-level connectivity with the relevant
POPs of ATT (upstream provider for the server at Berke-
ley) and WCG (upstream provider for the server at Cornell).
We used BGP looking-glass servers to determine that Level3
has at least two paths for the internal deployment’s any-
cast preﬁx: one through WCG’s Santa Clara POP with AS-
PATH=[7911, 26, 33207] and the other through ATT’s San
Francisco POP with AS-PATH=[7018, 2386, 33207]. The
Level3 routers in the area choose the ﬁrst path as the best
path and hence, anycast packets from the client are deliv-
ered to the anycast server at Cornell. This path is labeled
as “Actual Path” in the ﬁgure.
This example points to the crux of why Internet rout-
ing yields poor proximity for the measured anycast deploy-
ments. From the point of view of inter-domain routing, an
anycasted AS is equivalent to a multi-homed stub AS (see
ﬁgure 4). However, anycasting introduces multi-homing sce-
narios which diﬀer signiﬁcantly from normal multi-homing
scenarios. In typical multihoming, multiple peerings of the
multihomed stub AS are in the same geographical area. As
a consequence, selection of paths from clients to the mul-
tihomed AS based on ISP policies and AS-PATH length
leads to acceptable performance. On the other hand, for
(AS# 3356) 
Level3   
(AS# 7018) 
ATT 
Level3 routers route
anycast packets to an
ATT POP based on
ATT MEDs (’’’late-exit’)
or Level3’s intra-domain
metrics (’’early exit’’)
Seattle
Berkeley 
Pittsburgh
Internal Deployment
(AS# 33207)  
ATT routers route 
anycast packets to  the
anycast server closest  
in terms on ATT’s 
intra-domain metrics
Figure 5: Shown here is a deployment with ATT as
the common upstream provider – ASs beyond ATT
route the anycast packets to ATT’s network. Here,
Level3’s routers route anycast packets to a close-
by ATT POP which then routes the packets to the
closest anycast server.
an anycasted AS, the multiple peerings are geographically
dispersed and this is not accounted for in the existing inter-
domain routing set-up.
In the example above, Level3 re-
ceives two paths to the anycasted internal deployment of