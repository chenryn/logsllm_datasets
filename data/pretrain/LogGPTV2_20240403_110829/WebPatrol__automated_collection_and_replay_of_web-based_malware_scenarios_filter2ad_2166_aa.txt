title:WebPatrol: automated collection and replay of web-based malware scenarios
author:Kevin Zhijie Chen and
Guofei Gu and
Jianwei Zhuge and
Jose Nazario and
Xinhui Han
WebPatrol: Automated Collection and Replay of
Web-based Malware Scenarios
Kevin Zhijie Chen
Institute of Computer Science
and Technology
Peking University,
and UC Berkeley
PI:EMAIL
Guofei Gu
Texas A&M University
PI:EMAIL
Jianwei Zhuge
Network Center
Tsinghua University
PI:EMAIL
Jose Nazario
Arbor Networks
PI:EMAIL
Institute of Computer Science
∗
Xinhui Han
and Technology
Peking University
PI:EMAIL
ABSTRACT
Traditional remote-server-exploiting malware is quickly evolv-
ing and adapting to the new web-centric computing paradigm.
By leveraging the large population of (insecure) web sites
and exploiting the vulnerabilities at client-side modern (com-
plex) browsers (and their extensions), web-based malware
becomes one of the most severe and common infection vec-
tors nowadays. While traditional malware collection and
analysis are mainly focusing on binaries,
it is important
to develop new techniques and tools for collecting and an-
alyzing web-based malware, which should include a com-
plete web-based malicious logic to reﬂect the dynamic, dis-
tributed, multi-step, and multi-path web infection trails, in-
stead of just the binaries executed at end hosts. This paper
is a ﬁrst attempt in this direction to automatically collect
web-based malware scenarios (including complete web in-
fection trails) to enable ﬁne-grained analysis. Based on the
collections, we provide the capability for oﬄine “live” replay,
i.e., an end user (e.g., an analyst) can faithfully experience
the original infection trail based on her current client en-
vironment, even when the original malicious web pages are
not available or already cleaned. Our evaluation shows that
WebPatrol can collect/cover much more complete infection
trails than state-of-the-art honeypot systems such as PHon-
eyC [11] and Capture-HPC [1]. We also provide several case
studies on the analysis of web-based malware scenarios we
have collected from a large national education and research
network, which contains around 35,000 web sites.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIACCS ’11, March 22–24, 2011, Hong Kong, China.
Copyright 2011 ACM 978-1-4503-0564-8/11/03 ...$10.00.
Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection,Invasive
Software
General Terms
Security
Keywords
web-based malware analysis and collection, drive-by down-
load, malicious script
1.
INTRODUCTION
With the increasing reliance of our lives on the Internet,
web-based services are providing more and more functions to
serve our daily communication, entertainment, and business.
Web sites are now becoming much more dynamic and com-
plex, particularly with the increasing interest in Web 2.0 and
Software-as-a-Service (SaaS). Accordingly, web browsers are
becoming the most widely used client software on Internet.
At the same time, malicious software (malware) is quickly
evolving and adapting to this new web-centric computing
paradigm. We are witnessing a major shift of malware infec-
tion vectors, from traditional scanning-based remote server
exploiting to new web-based client software exploiting.
Compared to traditional server-side exploiting malware,
web-based malware has the following characteristics. First,
it exploits client-side vulnerabilities, mostly in modern com-
plex browsers and their extensions. Thus, it is more stealthy
and evasive because it does not need to send aggressive scan-
ning traﬃc. Second, it is pervasive considering the large
base of insecure web sites/pages on the Internet. Finally, it
is hard to block because most networks allow web traﬃc. As
a result, web-based malware becomes one of the most severe
and common infection threats nowadays [14, 13, 12].
To defend against this emerging type of threat, automated
collection and analysis of web-based malware are necessary.
Unfortunately, previous automated (binary) malware collec-
tion techniques such as Nepenthes [5] are not applicable here
because they are designed for server-exploiting malware. Al-
though client-side honeypot techniques such as Capture-
HPC [1] and HoneyMonkey[18], and suspicious web content
analysis service such as Wepawet[6, 9] are proposed, their
main purpose is to detect whether a given URL/Flash ﬁle is
malicious or not, instead of to collect the complete malicious
logic and infection trails in web-based malware. Current
best practice on web-based malware collection and analysis
is mostly on the downloaded malware binaries and/or indi-
vidual web pages that contain malicious Javascript code[6].
However, we consider that is not enough. For example, the
downloaded binary cannot reveal the (distributed) malicious
logic of web-based malware. The captured Javascript code is
not complete, too. In addition, the analysis of these script
code may yield diﬀerent results or even be not successful
simply because the code may contain/trigger requests to an-
other URL which can be dynamically changed, cleaned, or
even removed.
In this paper, we propose to collect and study a complete
web-based malware instance, which should contain all in-
fection trails as completely as possible. For now, we can
consider an example infection trail as a directed infection
path starting from the initial URL, going through a series of
nested inline linking1, and ending with the downloaded bi-
naries, which is analogous to an execution path in a binary.
Similar to typical multiple branches in a binary, a web-based
malware may also have multiple execution paths (infection
trails) depending on the conﬁguration information at client-
side environment. We deﬁne the complete set of these infec-
tion trails as a web-based malware scenario (WMS, formal
deﬁnition and more details will be introduced in Section 3).
Typically, a WMS contains dynamic, distributed infection
contents and multi-step, multi-path web infection trails, in-
stead of just the binaries executed at end hosts.
More speciﬁcally, this paper makes the following contri-
butions:
• We develop new techniques for automated collection of
these malware scenarios to enable future ﬁne-grained
analysis. To eﬃciently and eﬀectively collect web in-
fection trails as completely as possible, we use light-
weight browser emulation techniques for analyzing web-
based malware and then store all infection interac-
tions/contents during infection trails.
• Based on the collections, we provide the capability for
live replay, i.e., an end user (e.g., an analyst) can faith-
fully experience the original infection trail based on
his/her current client environment, even when the orig-
inal malicious web pages are not available or already
cleaned. This is a very useful function for oﬄine “live”
analysis of web infections.
• We have implemented a prototype system, WebPatrol,
for automated collection and replay of web-based mal-
ware scenarios. Using the prototype system, we have
collected many real-world web malware scenarios. We
show the utilities of our system through several case
studies. In particular, by comparing the collected sce-
narios with their corresponding ground-truth, we show
that WebPatrol can cover much more complete infec-
tion trails than state-of-the-art honeypot systems such
as PHoneyC [11] and Capture-HPC [1].
1http://en.wikipedia.org/wiki/Inline linking
The rest of this paper is organized as follows. We intro-
duce related work in Section 2. Section 3 provides the formal
deﬁnition and illustration of web-based malware scenarios.
We present our system design in Section 4 and its implemen-
tation in Section 5. We present our WebPatrol evaluation
and measurement results in Section 6. We discuss current
limitations of WebPatrol and our future work in Section 7,
and conclude the paper in Section 8.
2. RELATED WORK
Web-based Malware Measurement Study Provos et
al. have conducted a large-scale study of malware on Inter-
net web pages crawled by Google [14, 13, 12]. They char-
acterized some common patterns shared among web-based
malware. Zhuge et al.[20] studied malicious websites and
the underground economy in China. Seifert et al.[15] did a
similar study on New Zeland (.nz) domains. These measure-
ment studies clearly call for signiﬁcant further research on
web-based malware, e.g., automated collection, live replay,
detection, and analysis.
Web-based Malware Detection and Analysis There
are many kinds of client honeypots that aim to detect ma-
licious websites. Capture-HPC[1] and Strider HoneyMon-
key[18] are high interaction client honeypots that load the
suspicious web pages within a real browser, and detect the
web-based malware by monitoring the anomaly activities
during the browsing. CaﬀeineMonkey[8], and PHoneyC[11]
are low interaction client honeypots that parse the suspicious
pages, or run them in an emulated browser environment, and
raise alerts if certain attack patterns are found. Although
our prototype system uses an improved PHoneyC, our goal
is diﬀerent from the traditional use of client honeypots (to
detect malicious web pages), but to exhaustively enumer-
ate and collect all the infection trails for further replay and
analysis. Wepawet[6] is an online web-based malware detec-
tion and analysis service. User can submit suspicious URLs
or upload suspicious Flash/PDF ﬁles to it and wepawet will
analyze them for malicious scripts. The diﬀerence between
our system and wepawet is that wepawet aims to distinguish
the malicious contents from the benign ones, and WebPa-
trol aims to ﬁght against the obfuscation of given malicious
URLs and successfully collect and replay the scenario. The
online service of WebPatrol will be a depository that pro-
vides diﬀerent kinds of WMS for further analysis and new
detection system evaluation.
In addition, Song et. al[16] introduces inter-module com-
munication monitoring for web browser plugin vulnerabili-
ties, which shares a similar feature with our plugin simula-
tion in JavaScript context. However, their system is based
on a real browser, while we implement a simulated plugin
module within a low-interaction honeypot.
Automated Malware Collection The concept of au-
tomated malware collection has been proposed by many re-
searchers, such as the low interaction honeypot Nepenthes[5]
and the high interaction honeypot HoneyBow[19]. However,
all of those approaches are proposed for traditional server-
side malware collection, while our approach aims to collect
and replay the web-based malware automatically.
3. PROBLEM STATEMENT
3.1 Formal Deﬁnition
A web-based malware scenario is deﬁned as a directed tree-
like graph, which is represented as a four-tuple (µ, V, E, T ),
where
• µ stands for the initial landing URL, which is a special
(root) node in V .
• V refers to the set of all nodes, where each node vi
is some resource (denoted as a URL) in some remote
site.
• E refers to the set of all directed edges. For any vi ∈ V ,
if the client’s interpretation on vi directly triggers
a request to a new resource vj , then vj ∈ V and
∈ E. We also call such a directed edge an
outgoing link from vi to vj.
• T refers to the set of sink nodes (T ⊂ V ). A sink
node is some resource/object that typically indicates
a successful web infection/exploitation. For example,
a typical sink node can be a downloaded binary.
We deﬁne a web infection trail as a directed path in the
graph, starting from µ to some sink node in T . Obviously,
a web-based malware contains one or many web infection
trails.
3.2
Illustration
Figure 1: A Typical Web-based Malware Scenario.
(Pages that will not lead to an exploit are omitted
in this graph)
Type
HTML Tags
JS/VB API
Plugins
Shellcodes
Example
, or script, object, img ...
clientXmlHttpRequest.open(”GET”, ”test.txt”, true);[17]
Com.DloadDS(”http://www.***.com/calc.cab”,”muma.exe”,0);[7]
URLDownloadToFile(0,”http://foo.com/calc.exe”,”calc.exe”,0,0);
Table 1: Possible types of inline linkings in E
Figure 1 illustrates an example of a web-based malware
scenario. Nodes A-G are web pages and other kinds of re-
sources that will be retrieved by the client from the remote
server. Each edge is tagged with the type of the outgoing
link (Table 1 shows a list of the possible types of such links
and corresponding examples). Note that the elements in V
are not only web pages. Actually it can be a PDF ﬁle, data
transferred through XMLHttpRequest, or it can be a 404
or 500 error page. Any resource returned in response to a
request can be treated as an element in V .
In a typical web-based malware scenario, a user opens
the landing URL µ and browses it within a browser (this
site is called the landing site in [13]). The retrieved land-
ing web page may look normal, but the inline linking tags
(such as IFRAMEs and SCRIPTs) or JavaScript APIs etc.
will enable the landing URL to contain cross-domain mali-
cious pages or scripts that perform the actual attack through
one or more hops of inline linking. Those intermediate hop
points are called hops pages, and the ﬁnal web page that
contains the exploit codes is called the exploit page.
To increase the success rate and eﬃciency of the attack,
web-based malware writers usually make use of some web-
based malware exploit kits (e.g. Fragus[10]), which consist
of multiple exploit vectors, as well as a dispatching page.
The dispatching page typically ﬁngerprints the family and
version of the client browser and its plugins, testing if certain
vulnerability exists in the client, and exposes the exploit
pages only if the client has speciﬁc vulnerabilities.
While the exploit pages are important for vulnerability
analysis and signature generation, we consider all interme-
diate (landing/hopping) sites are also vital in the analysis
and defense of large-scale web-based malware infections. By
analyzing the complete web-based malware logics, we can
ﬁgure out how web-based malware is injected into benign
pages, and how it obfuscates itself to avoid detection.
3.3 Scenario Collection and Replay
Based on the deﬁnition above, the collection of WMS is
actually the collection of (µ, V, E, T ) so that all the informa-
tion related to (µ, V, E, T ) are stored and can be accessed
later (for ﬁne-grained analysis). That is, all interactions and
contents during all infection trails should be stored if possi-
ble.
Similarly, based on the previous deﬁnition, the live replay
of a web-based malware scenario (for a given time) is es-
sentially to faithfully reproduce the right infection trail(s)
(from the stored scenarios) based on the analyst’s environ-
ment and provide the right interactions with the user (with-
out actually accessing original malicious pages, which may
be changed, removed, or cleaned frequently over time).
4. SYSTEM DESIGN
To achieve the goal of automated collection and replay of
the web-based malware scenarios, we design and implement
a prototype system called WebPatrol. The architecture of
WebPatrol is shown in Figure 2, which consists of two major
components, the scenario collection component and the sce-
nario replay component. The scenario collection component
works in an online fashion, and it is responsible for ana-
lyzing in-the-wild web-based malware scenarios, retrieving
and caching all of the discovered web resources and outgo-
ing links, and building the WMS depository. The scenario
replay component can operate in an oﬄine fashion (e.g. in
a logically isolated analysis environment from the Internet)
but provide an online and interactive operation experience
for end users. This component is responsible for reconstruct-
ing the web infection trail(s) from the stored data, given a
landing URL and speciﬁc time label as the identiﬁcation of a
web-based malware scenario. In our WebPatrol design, the
replay component can support arbitrary types of analysis
(browser) clients, by providing a replay service which just
requires minimal conﬁguration of the clients.
try{var c;
var f=new ActiveXObject("OWC10.Spreadsheet");}
catch(c){};
finally{if(c!="[object Error]")
{document.write(
"");}
Figure 3: A Snippet from a Dispatching Page
ing page. As we can see, it tries to create an ActiveXOb-
ject object, using document.write to dynamically output the
IFRAME tag for the real exploit page only if the object is
successfully instantiated. A LI client honeypot is more ﬂexi-
ble and scalable in this case. We can emulate many diﬀerent
kinds of vulnerabilities and plugins at the same time, even
if they are totally on diﬀerent browsers or operating sys-
tems. For the example in Figure 3, the goal of our analyzer
is to make the script think it successfully instantiates the
ActiveXObject so it will document.write the malicious out-
going link. To achieve better coverage of the infection trails,
we propose several such techniques in the analyzer and other
components, which will be discussed in detail in Section 5.