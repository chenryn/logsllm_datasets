# Title: WebPatrol: Automated Collection and Replay of Web-based Malware Scenarios

## Authors:
- Kevin Zhijie Chen, Institute of Computer Science and Technology, Peking University, and UC Berkeley
- Guofei Gu, Texas A&M University
- Jianwei Zhuge, Network Center, Tsinghua University
- Jose Nazario, Arbor Networks
- Xinhui Han, Institute of Computer Science and Technology, Peking University

## Abstract
Traditional malware that exploits remote servers is rapidly evolving to adapt to the new web-centric computing paradigm. By leveraging the large number of insecure websites and exploiting vulnerabilities in modern, complex browsers and their extensions, web-based malware has become one of the most severe and common infection vectors today. While traditional malware collection and analysis focus on binaries, it is crucial to develop new techniques and tools for collecting and analyzing web-based malware, which should include complete web-based malicious logic to reflect the dynamic, distributed, multi-step, and multi-path nature of web infection trails.

This paper presents a first attempt to automatically collect web-based malware scenarios, including complete web infection trails, enabling fine-grained analysis. Based on these collections, we provide the capability for offline "live" replay, allowing an end user (e.g., an analyst) to experience the original infection trail based on their current client environment, even if the original malicious web pages are no longer available or have been cleaned. Our evaluation shows that WebPatrol can collect more complete infection trails than state-of-the-art honeypot systems such as PHoneyC [11] and Capture-HPC [1]. We also provide several case studies on the analysis of web-based malware scenarios collected from a large national education and research network, which contains around 35,000 websites.

## 1. Introduction
With the increasing reliance on the Internet, web-based services are providing more functions for daily communication, entertainment, and business. Websites are becoming more dynamic and complex, particularly with the rise of Web 2.0 and Software-as-a-Service (SaaS). Consequently, web browsers have become the most widely used client software on the Internet. 

At the same time, malware is quickly evolving to exploit this new web-centric paradigm. We are witnessing a shift from traditional server-side exploitation to new web-based client software exploitation. Compared to traditional server-side malware, web-based malware has the following characteristics: it exploits client-side vulnerabilities, mostly in modern complex browsers and their extensions; it is pervasive due to the large base of insecure websites; and it is hard to block because most networks allow web traffic. As a result, web-based malware is one of the most severe and common infection threats today [14, 13, 12].

To defend against this emerging threat, automated collection and analysis of web-based malware are necessary. However, previous automated malware collection techniques, such as Nepenthes [5], are not applicable here because they are designed for server-exploiting malware. Although client-side honeypot techniques like Capture-HPC [1] and HoneyMonkey [18], and suspicious web content analysis services like Wepawet [6, 9] have been proposed, their main purpose is to detect whether a given URL/Flash file is malicious, rather than to collect complete malicious logic and infection trails. Current best practices in web-based malware collection and analysis focus on downloaded malware binaries and/or individual web pages containing malicious JavaScript code [6]. This approach is insufficient because it does not reveal the distributed malicious logic of web-based malware, and the captured JavaScript code may be incomplete or yield different results due to dynamic changes, cleaning, or removal of URLs.

In this paper, we propose to collect and study complete web-based malware instances, which should contain all infection trails as completely as possible. We define a web-based malware scenario (WMS) as a directed tree-like graph, starting from an initial URL and ending with downloaded binaries, analogous to an execution path in a binary. A WMS typically contains dynamic, distributed infection contents and multi-step, multi-path web infection trails.

Specifically, this paper makes the following contributions:
- We develop new techniques for automated collection of malware scenarios to enable future fine-grained analysis. To efficiently and effectively collect web infection trails, we use lightweight browser emulation techniques to analyze web-based malware and store all interactions and contents during the infection trails.
- Based on the collections, we provide the capability for live replay, allowing an end user (e.g., an analyst) to faithfully experience the original infection trail based on their current client environment, even if the original malicious web pages are no longer available or have been cleaned.
- We have implemented a prototype system, WebPatrol, for automated collection and replay of web-based malware scenarios. Using this system, we have collected many real-world web malware scenarios and demonstrate its utility through several case studies. By comparing the collected scenarios with ground truth, we show that WebPatrol can cover much more complete infection trails than state-of-the-art honeypot systems such as PHoneyC [11] and Capture-HPC [1].

## 2. Related Work
### Web-based Malware Measurement Studies
Provos et al. conducted a large-scale study of malware on Internet web pages crawled by Google [14, 13, 12], characterizing common patterns among web-based malware. Zhuge et al. [20] studied malicious websites and the underground economy in China, while Seifert et al. [15] conducted a similar study on New Zealand (.nz) domains. These studies highlight the need for further research on web-based malware, including automated collection, live replay, detection, and analysis.

### Web-based Malware Detection and Analysis
There are various client honeypots designed to detect malicious websites. Capture-HPC [1] and Strider HoneyMonkey [18] are high-interaction client honeypots that load suspicious web pages in a real browser and detect malware by monitoring anomalous activities. CaffeineMonkey [8] and PHoneyC [11] are low-interaction client honeypots that parse suspicious pages or run them in an emulated browser environment and raise alerts if certain attack patterns are detected. Although our prototype system uses an improved PHoneyC, our goal differs from traditional client honeypots, which aim to detect malicious web pages. Instead, we aim to exhaustively enumerate and collect all infection trails for further replay and analysis.

Wepawet [6] is an online web-based malware detection and analysis service. Users can submit suspicious URLs or upload Flash/PDF files, and Wepawet will analyze them for malicious scripts. The difference between our system and Wepawet is that Wepawet aims to distinguish malicious content from benign content, while WebPatrol aims to fight obfuscation, successfully collect, and replay the scenario. The online service of WebPatrol will serve as a repository for different types of WMS for further analysis and new detection system evaluation.

Song et al. [16] introduced inter-module communication monitoring for web browser plugin vulnerabilities, which shares similarities with our plugin simulation in JavaScript context. However, their system is based on a real browser, while we implement a simulated plugin module within a low-interaction honeypot.

### Automated Malware Collection
The concept of automated malware collection has been proposed by many researchers, such as the low-interaction honeypot Nepenthes [5] and the high-interaction honeypot HoneyBow [19]. However, these approaches are designed for traditional server-side malware collection, while our approach aims to collect and replay web-based malware automatically.

## 3. Problem Statement
### 3.1 Formal Definition
A web-based malware scenario (WMS) is defined as a directed tree-like graph represented as a four-tuple (µ, V, E, T), where:
- µ stands for the initial landing URL, which is a special (root) node in V.
- V refers to the set of all nodes, where each node vi is some resource (denoted as a URL) in some remote site.
- E refers to the set of all directed edges. For any vi ∈ V, if the client's interpretation on vi directly triggers a request to a new resource vj, then vj ∈ V and (vi, vj) ∈ E. We call such a directed edge an outgoing link from vi to vj.
- T refers to the set of sink nodes (T ⊂ V). A sink node is a resource/object that typically indicates a successful web infection/exploitation, such as a downloaded binary.

A web infection trail is defined as a directed path in the graph, starting from µ to some sink node in T. A web-based malware contains one or many web infection trails.

### 3.2 Illustration
Figure 1 illustrates a typical web-based malware scenario. Nodes A-G represent web pages and other resources retrieved by the client from the remote server. Each edge is tagged with the type of the outgoing link (Table 1 shows a list of possible types of such links and corresponding examples). The elements in V are not only web pages but can also be PDF files, data transferred through XMLHttpRequest, or error pages. Any resource returned in response to a request can be treated as an element in V.

In a typical web-based malware scenario, a user opens the landing URL µ and browses it within a browser. The retrieved landing web page may look normal, but inline linking tags (such as IFRAMEs and SCRIPTs) or JavaScript APIs enable the landing URL to contain cross-domain malicious pages or scripts that perform the actual attack through one or more hops of inline linking. Intermediate hop points are called hop pages, and the final web page that contains the exploit codes is called the exploit page.

To increase the success rate and efficiency of the attack, web-based malware writers often use web-based malware exploit kits (e.g., Fragus [10]), which consist of multiple exploit vectors and a dispatching page. The dispatching page fingerprints the client browser and its plugins, tests for specific vulnerabilities, and exposes the exploit pages only if the client has those vulnerabilities.

While exploit pages are important for vulnerability analysis and signature generation, intermediate (landing/hopping) sites are also vital for the analysis and defense of large-scale web-based malware infections. By analyzing the complete web-based malware logic, we can understand how malware is injected into benign pages and how it obfuscates itself to avoid detection.

### 3.3 Scenario Collection and Replay
Based on the definition above, the collection of WMS involves storing all information related to (µ, V, E, T) so that all interactions and contents during all infection trails are stored for fine-grained analysis. Similarly, the live replay of a web-based malware scenario is to faithfully reproduce the right infection trail(s) based on the analyst's environment and provide the right interactions with the user, without accessing the original malicious pages, which may change, be removed, or cleaned over time.

## 4. System Design
To achieve the goal of automated collection and replay of web-based malware scenarios, we design and implement a prototype system called WebPatrol. The architecture of WebPatrol consists of two major components: the scenario collection component and the scenario replay component.

### 4.1 Scenario Collection Component
The scenario collection component works in an online fashion, analyzing in-the-wild web-based malware scenarios, retrieving and caching all discovered web resources and outgoing links, and building the WMS repository.

### 4.2 Scenario Replay Component
The scenario replay component operates in an offline fashion (e.g., in a logically isolated analysis environment) but provides an online and interactive operation experience for end users. This component reconstructs the web infection trail(s) from the stored data, given a landing URL and a specific time label. The replay component supports arbitrary types of analysis (browser) clients by providing a replay service that requires minimal client configuration.

### 4.3 Example: Dispatching Page
Figure 3 shows a snippet from a dispatching page. It tries to create an ActiveXObject object and dynamically outputs the IFRAME tag for the real exploit page only if the object is successfully instantiated. A low-interaction (LI) client honeypot is more flexible and scalable in this case, as it can emulate different kinds of vulnerabilities and plugins simultaneously, even if they are on different browsers or operating systems. The goal of our analyzer is to make the script think it successfully instantiates the ActiveXObject so it will document.write the malicious outgoing link. To achieve better coverage of the infection trails, we propose several techniques in the analyzer and other components, which will be discussed in detail in Section 5.

## 5. Implementation
[Detailed implementation details will be provided in this section, including the technical aspects of the scenario collection and replay components, and the methods used to emulate different browser environments and handle various types of web-based malware.]

## 6. Evaluation and Results
[This section will present the evaluation and measurement results of WebPatrol, including comparisons with state-of-the-art honeypot systems and case studies on the analysis of web-based malware scenarios collected from a large national education and research network.]

## 7. Limitations and Future Work
[This section will discuss the current limitations of WebPatrol and outline future work, including potential improvements and new features to enhance the system's capabilities.]

## 8. Conclusion
[This section will summarize the key contributions of the paper and conclude with a brief discussion of the impact and significance of the work.]

---

**Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.**

**ASIACCS '11, March 22–24, 2011, Hong Kong, China.**
**Copyright 2011 ACM 978-1-4503-0564-8/11/03 ...$10.00.**

**Categories and Subject Descriptors: D.4.6 [Operating Systems]: Security and Protection, Invasive Software**

**General Terms: Security**

**Keywords: web-based malware analysis and collection, drive-by download, malicious script**