found in the Strings resource ﬁle that doesn’t necessarily mean
that the user is interested in ﬁtness activities. It could be the
case that the app in question is an educational app that has
exercises for students. On the other hand, if this word is mined
12In our prototype we used the JWI [16] interface to Wordnet, to derive
sets of synonyms.
8
from an app in the Health and Fitness Google Play
category, then it is more likely this is referring to a ﬁtness
activity. Pluto employs a disambiguation layer that aims to
determine whether the match is valid. It attaches to every
user interest the input app’s Google Play category name. We
call that a disambiguation term. For user attributes, the
disambiguation term is currently assigned by the analyst 13. In
addition, Pluto assigns some domain knowledge to data
points. For attributes, it treats the ﬁle name or table name as the
domain knowledge, and for interests it uses the matching goal
itself. Our prototype’s context disambiguation layer calculates
the similarity between the disambiguation term and the
domain knowledge. If the similarity value is found to
surpass a speciﬁc threshold, then the match is accepted.
The NLP community already proposed numerous metrics
for comparing how similar or related two concepts are. Our
prototype can be conﬁgured to use the following existing simi-
larity metrics to disambiguate attribute matches: PATH [37];
LIN [30]; LCH [28]; LESK [6]. Unlike the ﬁrst three
metrics which are focused on measuring an is-a similarity
between two words, LESK is a deﬁnition-based metric of
relatedness. Intuitively this would work better with user interests
where the disambiguation term is the app’s category name. The
other metrics are used to capture is-a relationships which
cannot hold in most of the user-interests cases. For example,
there is no strong is-a relationship connecting the user interest
vehicle with the category transportation. 14 LESK seems
well ﬁt to address this as it depends on the descriptions of the
two words. Indeed, LESK scores the (vehicle, transportation)
pair with 132 with (vehicle, travel and local) coming second
with 103.
However, in this study we have found that LESK might not
always work that well when applied in this domain. Studying the
scoring of LESK with respect to one of our most popular user
interests in our L1 dataset we found it to be problematic. When
comparing the matching goal workout with the category
Health and Fitness, LESK assigns it one of the lowest
scores (33), with the maximum score assigned to the (workout,
books and references) pair (113).
Here we present our new improved similarity metric that
can address LESK’s shortcomings when applied to our problem.
We call our similarity metric droidLESK. The intuition
behind droidLESK is that the more frequently a word is
used in a category,
the higher the weight of the (word,
category) pair should be. droidLESK is then a normalization
of f req(w, c) ⇥ LESK(w, c). In other words, droidLESK is
the weighted LESK were the weights are assigned based on
term frequencies. To evaluate droidLESK, we create pairs of
the matching goal workout with every Google Play category
name and assign a score to each pair as derived from droidLESK
and other state of the art similarity metrics. To properly weight
LESK and derive droidLESK, we perform a term frequency
analysis of the workout word in all ‘runtime’ generated
ﬁles of the L1 dataset. We repeat the experiment for the
word vehicle. droidLESK’s scoring was compared with
the scores assigned to the pairs by the following similarity
13We used the word Person.
14We found that similarity metrics that ﬁnd these relationships do not assign
the best score to the pair(vehicle, transportation) when compared with other
(vehicle, *) pairs.
metrics: WUP [51]; JCN [24]; LCH [28]; LIN [30]; RES [38];
PATH [37]; LESK [6] and HSO [22].
The results are very promising—even though preliminary—
as shown in table IV. 15 We observe that our technique correctly
assigns the highest score to the pair (workout, health and ﬁtness)
than any other pair (workout,*). The same is true for the pair
(vehicle, transportation). droidLesk was evaluated on the two
most prevalent user interests in our dataset. Since our approach
might suffer from over-ﬁtting, in future work we plan to try this
new metric with more words and take into account the number
of apps contributing to the term frequency. We further discuss
the effects of using droidLESK in Pluto’s in-app targeted data
discovery in the evaluation Section VII.
B. Out-app Pluto
Out-app Pluto aims to estimate what is the potential data
exposure to an ad library that uses the unprotected public gIA
and gIP APIs. That is, given the fact that the ad library can learn
the list of installed applications on a device, it aims to explore
what data points, if any, can be learned from that list. Intuitively,
if an ad library knows that a user installed a pregnancy app and
local public transportation app, it would be able to infer the
user’s gender and coarse location. However, the list of installed
applications derived from gIA and gIP is dependent on the
device the ad library’s host app is installed, which renders
estimation of the exposure challenging. To explore what an ad
library can learn through this out-app attack channel, we derive
a set of co-installation patterns that reveals which apps are
usually installed together. This way we can simulate what the
runtime call to gIA or gIP will result in given invocation from
an ad library incorporated into a particular host app. We then
feed the list of co-installed applications into a set of classiﬁers
we trained to discover the potential data exposure through the
out-app channel.
The Pluto out-app exposure discovery system runs machine
learning techniques on a corpus of app bundles to achieve
two goals. First, it provides a Co-Installation Pattern module
(CIP) which can be updated dynamically as new records of
installed apps are received. The CIP module runs state-of-the-
art frequent pattern mining (FPM) algorithms on such records
to discover associations between apps. For example, such an
analysis can yield an association in the form of a conditional
probability, stating that if app A is present on a device then app
B can be found on that device with x% conﬁdence. When an
analyst sets Pluto to discover out-app targeted data regarding
an app ofﬂine, Pluto utilizes the CIP module to get a good
estimation of a vector of co-installed apps with the target app.
The resulting vector is passed to the classiﬁers which in turn
present the analyst with a set of learned attributes. Second, it
provides a suite of supervised machine learning techniques that
take a corpus of app bundles paired with a list of user targeted
data and creates classiﬁers that predict whether an app bundle
is indicative of a user attribute or interest.
1) Co-Installation Patterns: The CIP module uses frequent
pattern mining to ﬁnd application co-installation patterns. This
can assist Pluto in predicting what will an ad library learn
at runtime if it invokes gIA or gIP. We call a co-installation
pattern, the likelihood to ﬁnd a set of apps installed on a
15Due to space limitations, we omit uninformative comparisons.
9
TABLE IV: Comparison between rankings of (interest, category name) pairs from LESK and droidLESK. TF denotes the data
point term frequency in local ﬁles created by apps in a category.
DATA POINT
VEHICLE
VEHICLE
VEHICLE
WORKOUT
WORKOUT
WORKOUT
RANK
1
2
3
1
2
3
LESK
TRANSPORTATION
BOOKS AND REFERENCES
TRAVEL AND LOCAL
BOOKS AND REFERENCES
TRAVEL AND LOCAL
MUSIC AND AUDIO
TF
FINANCE
TRANSPORTATION
LIFESTYLE
HEALTH AND FITNESS
APP WIDGET
NEWS AND MAGAZINE
TF*LESK
TRANSPORTATION
FINANCE
LIFESTYLE
HEALTH AND FITNESS
NEWS AND MAGAZINE
APP WIDGET
device in correlation with another app installed on that device.
In FPM, every transaction in a database is identiﬁed by an
id and an itemset. The itemset is the collection of one or
more items that appear together in the same transaction. For
example, this could be the items bought together by a customer
at a grocery store. Support indicates the frequency of an
itemset in the database. An FPM algorithm will consider an
itemset to be frequent if its support is no less than a minimum
support threshold. Itemsets that are not frequent are pruned.
Such an algorithm will mine association rules including
frequent itemsets in the form of conditional probabilities that
indicate the likelihood that an itemset can occur together with
another itemset in a transaction. The algorithm will select rules
that satisfy a measure (e.g., a minimum conﬁdence level). An
association rule has the form N:N, where N is the number of
unique items in the database. An association rule is presented as
X ) Y where the itemset X is termed the precedent and
Y the consequent. Such analysis is common when stores
want to ﬁnd relationships between products frequently bought
together.
Pluto’s CIP uses the same techniques to model the in-
stallations of apps on mobile devices, as itemsets bought
together at a grocery store. Our implementation of Pluto’s
CIP module uses the FPGrowth [20] algorithm, a state of the
art frequent pattern matching algorithm for ﬁnding association
rules. We have chosen FPGrowth because it is signiﬁcantly
faster than its competitor Apriori [3]. CIP runs on a set of app
bundles collected periodically from a database containing user
proﬁles that include the device’s app bundles and derives a
set of association rules, indicating the likelihood that apps can
be found co-installed on a device. Our CIP association rule
will have the form 1:N because Pluto is interested in ﬁnding
relationships between a given app and a set of other apps.
CIP uses confidence and lift as the measures to
decide whether an association rule is strong enough to be
presented to the analyst. Conﬁdence is deﬁned as conf (X )
Y ) = supp(X[Y )
, where supp(X) is the support of the itemset
supp(X)
in the database. A conﬁdence of 100% for an association rule
means that for 100% of the times that X appears in a transaction,
Y appears as well in the same transaction. Thus an association
rule f acebook ) skype, viber with 70% conﬁdence will
mean that for 70% of the devices having Facebook installed,
Viber and Skype are also installed.
supp(X[Y )
Another measure CIP supports is Lift. Lift is deﬁned
as: lif t(X ) Y ) =
supp(X)⇥supp(Y ). Lift indicates how
independent the two itemsets are in the rule. A Lift of one will
indicate that the probability of occurrence of the precedent
and consequent are independent of each other. The higher
the Lift between the two itemsets, the stronger the dependency
between them and the strongest the rule is.
2) Learning Targeted Data from App Bundles: Pluto uses
supervised learning models to infer user attributes from the CIP-
estimated app bundles. Pluto aims to resolve two challenges in
training models based on app bundles: 1) skewed distribution
of values of attributes; 2) high dimensionality and highly sparse
nature of the app bundles.
Balancing distributions of training sets: Based on the
empirical data we collected, some attributes have a more skewed
distribution in their values. To orient the reader using a concrete
example, consider an example where 1 of 100 users has an
allergy. In predicting whether a user has an allergy in this
dataset, one classiﬁer can achieve an accuracy of 0.99 by
trivially classifying each user as having an allergy. In view of
this, for the attribute “has an allergy” the value “yes” can be
assigned a higher weight, such as 99, while the value “no” has a
weight of 1. After assigning weights, the weighted accuracy for
predicting an attribute now becomes the weighted average of
accuracy for each user; the weight for a user is the ratio of the
user’s attribute value weight to the total attribute value weights
of all users. Therefore, in this example, the weighted accuracy
becomes 0.5, which is fair, even when trivially guessing that
each user has the same attribute value. In order to train an
effective model for Pluto, we balance the distribution of training
sets following the aforementioned idea. To balance we adjust
the weights of existing data entries to ensure that the total
weights of each attribute value are equal. In this way, the ﬁnal
model would not trivially classify each user to be associated
with any same attribute value. Accordingly, we adopt measures
weighted precision and weighted recall in our evaluation where
the total weights of each attribute value are equal; this is to
penalize trivial classiﬁcation to the same attribute value [10].
Dimension reduction of app-bundle data: Another chal-
lenge we face in this context is the high dimensionality and
highly sparse nature of the app bundles. There are over 1.4
million apps [41] on Google Play at this moment, and it is
both impractical and undesirable for the users to download and
install more than a small fraction of those on their devices.
A recent study from Yahoo [39] states that users install on
average 97 apps on a device. To make our problem more
tractable we used a technique borrowed from the Machine
Learning community which allows us to reduce the considered
dimensions. Our prototype employs three classiﬁers, namely
K-Nearest Neighbors (KNN), Random Forests, and SVM.
To apply these classiﬁers to our data, we map each user
ui in the set of users U to an app installation vectors aui =
{a1, . . . , ak}, where aj = 1 (j = 1, . . . , k) if ui installs aj
on the mobile device, otherwise aj = 0. Note that the app
installation vector is k-dimensional and k can be a large value
(1985 in our study). Thus, classiﬁers may suffer from the “curse
of dimension” such that the computation could be dominated by
less relevant installed apps when the dimension of space goes
higher. To mitigate this problem, we use principal component
10
Pluto"FD"
Pluto"L1""
Manual"L1"
Pluto"L2" Manual"L2"
"
s
p
p
a
%
"
1"
0.9"
0.8"
0.7"
0.6"
0.5"
0.4"
0.3"
0.2"
0.1"
0"
0"
1"
2"
3"
4"
5"
6"
7"
#"Data"Points"
"
s
p
p
a
%
"
1"
0.9"
0.8"
0.7"
0.6"
0.5"
0.4"
0.3"
0.2"
0.1"
0"
0"
2"
4"
6"
8"
10"
12"
14"
16"
#"Data"Points"
Fig. 3: CDF of apps and number of data points (level-1)
Fig. 4: CDF of apps and number of data points (level-2)
analysis (PCA) by selecting a small number of the principal
components to perform dimension reduction before applying a
classiﬁer.
VII. EVALUATION
In this section we evaluate Pluto’s components in estimating
data exposure. We ﬁrst evaluate Pluto’s performance to discover
Level-1 and Level-2 in-app data points. Next we apply Pluto’s
CIP module and classiﬁers on real world data app bundles and
ground truth we have collected, and evaluate their performance.
A. Evaluation of Pluto’s in-app exposure discovery
In this section we present our empirical ﬁndings on applying
Pluto on real world apps.
Experimental setup: We provided Pluto with a set of data
points to look for, enhanced with the meaning—sense id of
the data point in Wordnet’s dictionary—and the class of the
data point (i.e., user attribute or user interest). We also provide
Pluto with a mapping between permissions and data points
and we conﬁgured it to use the LCH similarity metric at the