from the storage component. If the storage process itself
crashes and comes up, every other server has to store its
state again.
Recovering from a crash of other components is very
different. When a system component crashes and restarts, it
must tell everyone it wants to talk to that it is ready to set up
communication channels and to map whatever pools it needs.
At that point, its neighbors must take action to discover the
Drivers: State of
the art self-healing OSs,
status of requests which have not completed yet. All the state
a component needs to restart should be in the storage server.
like
Minix 3, previously demonstrated restarting of simple net-
work drivers [22], but it feeds only a single packet to a driver
at a time. In contrast, we asynchronously feed as much data
as possible to be able to saturate multigigabit links and use
more complex features of the hardware. In addition, our
drivers do not copy the packets to local buffers.
As a result, the IP server must wait for an acknowledgment
from the driver that a packet was transmitted before it is
allowed to free the data. IP knows which packets were not
yet accepted by the driver for processing from the state
of the queue. It is likely that all packets except the last
one were successfully transmitted, but the last one (as the
driver perhaps crashed while processing it). Although network
protocols are designed to deal with lost packets, we do not
want to drop more than necessary. In case of doubt, we prefer
to send a few duplicates which the receiver can decide to
drop. Therefore IP resubmits the packets which it thinks
were not yet transmitted.
A faulty driver may make the device operate incorrectly or
stop working at all. This can be also a result of differences
between speciﬁcation and implementation of the hardware. It
is difﬁcult to detect such situations. When we stop receiving
packets, it can either be because nobody is sending anything,
or because the device stopped receiving. As a remedy, we
can detect that a driver is not consuming packets for a while
or that we do not receive replies to echo packets and then
restart the driver pro-actively. However, these techniques are
out of the scope of this paper and unless a driver crashes,
we can not currently recover from such situations.
IP: To recover the IP server, it needs to store its
conﬁguration, IP addresses of each device and routing like
the default gateway, etc. This information changes rarely on
the network edge. Because IP allocates a pool which the
drivers use to receive packets, the drivers must make sure
that they switch these pools safely, so the devices do not
DMA to freed memory. It turned out that we must reset the
network cards since the Intel gigabit adapters do not have
a knob to invalidate its shadow copies of the RX and TX
descriptors. Therefore a crash of IP means defacto restart
of the network drivers too. We believe that restart-aware
hardware would allow less disruptive recovery.
Similarly, TCP and UDP may have packets still allocated
in the old receive pool and they must keep a reference to it
until all the packets are delivered or discarded. On the other
hand, neither can free the transmitted packets until they know
that no other component holds a reference to the memory.
Our policy in both cases is that we resubmit the requests
to IP. We generate new identiﬁers so that we can ignore
replies to the original requests and only free the data once
we get replies to the new ones. This also means that we may
transmit some duplicates. However, in case of TCP, it is much
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
more important that we quickly retransmit (possibly) lost
packets to avoid the error detection and congestion avoidance.
This helps to recover quickly the original performance after
recovering the functionality.
UDP: The UDP server saves in the storage server which
sockets are currently open, to what local address and port
they are bound, and to which remote pair they are connected
(preset for send). It is easy to recreate the sockets after the
crash. However, UDP does not store whether a process was
blocked on a socket operation and if so, which one (and
doing so would result in signiﬁcant overhead). On the other
hand, the SYSCALL server remembers the last unﬁnished
operation on each socket and can issue it again. This is
ﬁne for select and recv variants as they do not trigger
any network trafﬁc. In contrast, send variants will result
in packets sent. As mentioned previously, we tend to prefer
sending extra data. Of course, we can also return an error to
the application instead, for example, zero bytes were written.
TCP: Much like UDP, TCP also saves in the storage
server the sockets that are open. In addition, TCP saves
in what state the connection is (listening, connecting, es-
tablished, etc.) so the packet ﬁlter can restore connection
tracking after its crash. TCP can only restore listening sockets
since they do not have any frequently changing state and
returns error to any operation the SYSCALL server resubmits
except listen.
Packet ﬁlter: To restore the optional packet ﬁlter we
need to recover the conﬁguration (much like restoring IP
conﬁguration) and the open connections (much like restoring
TCP or UDP sockets) and it stores this information in the
storage server. Since IP must get a reply for each request
before it can pass a packet further the stack, it can safely
resubmit all unﬁnished requests without packet loss and
generating duplicate trafﬁc.
VI. EVALUATION
We evaluate our multiserver design and present the beneﬁts
of the principles we are advocating for. To demonstrate the
competitiveness of our design, we evaluate on a 12 core
AMD Opteron Processor 6168 (1.9GHz) 4GB RAM with 5
Intel PRO/1000 PCI Express gigabit network adapters. We
are limited by the number of PCIe slots in our test machine.
We use standard 1500 byte MTU in all conﬁgurations.
A. TCP Performance
1 Minix 3, 1 CPU only, kernel IPC and copies
NewtOS, Split stack, dedicated cores
NewtOS, Split stack, dedicated cores + SYSCALL
NewtOS, 1 server stack, dedicated core + SYSCALL
120Mbps
3.2Gbps
3.6Gbps
3.9Gbps
2
3
4
5
6
7
NewtOS, 1 server stack, dedicated core + SYSCALL + TSO
5+Gbps
NewtOS, Split stack, dedicated cores + SYSCALL + TSO
Linux, 10Gbe interface
5+Gbps
8.4Gbps
PEAK PERFORMANCE OF OUTGOING TCP IN VARIOUS SETUPS
Table II
(line 5). Line 3 presents the advantage of using the SYSCALL
server, in contrast to line 2, to decouple synchronous calls
from asynchronous internals. Comparing lines 3 and 4, we
can see the effect of communication latency between the
extra servers in the split stack. Using TSO we remove a great
amount of the communication and we are able to saturate
all 5 network cards while allowing parts of the stack to
crash or be live-updated. It is important to mention that
Linux also cannot saturate all the devices without using
TSO which demonstrates that not only the architecture of the
stack but also its ability to ofﬂoad work to network cards and
reduction of its internal request rate (TCP window scaling
option, jumbo frames, etc.) play the key role in delivering
the peak performance. To put the performance in perspective,
line 7 shows the maximum we obtained on Linux on the
same machines with standard ofﬂoading and scaling features
enabled using a 10Gbe adapter which neither Minix 3 or
NewtOS support.
We carried out our experiments with one driver per network
interface, however, to evaluate scalability of the design we
also used one driver for all interfaces, which is similar to
having one multi-gigabit interface. Since the work done by
the drivers is extremely small (ﬁlling descriptors and updating
tail pointers of the rings on the device, polling the device)
coalescing the drivers into one still does not lead to an
overload. In contrary, the busy driver reduces some latency
since it is often awake and ready to respond.
We believe that on a heavily threaded core like that of
Oracle Sparc T4, we would be able to run all the drivers on
a single core using the threads as the containers in which
the drivers can block without sacriﬁcing more cores and still
delivering the same performance and isolation of drivers.
B. Fault Injection and Crash Recovery
Table II shows peak performance of our TCP implementa-
tion in various stages of our development along with original
Minix 3 and Linux performance. The table is ordered from
the least performing at the top to the best performing at
the bottom. The ﬁrst line shows that a fully synchronous
stack of Minix 3 cannot efﬁciently use our gigabit hardware,
on the other hand, line 4 shows that a single server stack
which adopts our asynchronous channels can saturate 4 of our
network interfaces and more with additional optimizations
To assess the fault tolerance of our networking stack we
have injected faults in the individual components. Therefore
we used a fault injection tool equal to that used by the
authors of Rio ﬁle cache [30], Nooks [42] and Minix 3 [22]
to evaluate their projects. We summarize the distribution of
the faults in Table III and effects the crashes have in Table IV.
During each test run we injected 100 faults into a randomly
selected component. When the component did not crash
within a minute we rebooted the machine and continued
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
Total
100
TCP
UDP
25
10
IP
24
PF
25
Driver
16
DISTRIBUTIONS OF CRASHES IN THE STACK
Table III
Fully transparent crashes
70
Reachable from outside
90 (+ 6 manually ﬁxed)
Crash broke TCP connections
Transparent to UDP
Reboot necessary
30
95
3
Table IV
CONSEQUENCES OF CRASHES
with another run. We collected 100 runs that exhibited a
crash and we observed the damage to the system. While
injecting the faults we stressed the components with a TCP
connection and periodic DNS queries. The tool injects faults
randomly so the faults are unpredictable. Since some of the
code does not execute during a normal operation and because
of different fraction of active code, some components are
more likely to crash than the others.
The most serious damage happens when the TCP server
crashes. In these cases all established connections disappear.
On the other hand, since we recover sockets which listen for
incoming connections, we are able to immediately open new
ones to our system. We used OpenSSH as our test server.
After each crash we tested whether the active ssh connections
kept working, whether we were able to established new ones
and whether the name resolver was able to contact a remote
DNS server without reopening the UDP socket.
We were able to recover from vast majority of the faults,
mostly transparently. After the 25 TCP crashes, we where able
to directly reconnect to the SSH server in 19 of those cases. In
3 of the cases we had to manually restart the TCP component
to be able to reconnect to the machine. In two other cases
a faulty IP and a not fully responsive driver was the reason
why it was impossible to connect to the machine. Manually
restarting the driver respectively IP solved the problem. In
three cases we had to reboot the system due to hangs in the
synchronous part of the system which merges sockets and
ﬁle descriptors for select and has not been modiﬁed yet
to use the asynchronous channels we propose. This suggests
that reliability of other parts of the system would also greatly
beneﬁt from our design changes. In two cases, faults injected
into a driver caused a signiﬁcant slowdown but no crash. It
is very likely that the faults misconﬁgured the network cards
since the problem disappeared after we manually restarted
the driver, which reseted the device.
In contrast to a solid production quality systems like Linux
or Windows, NewtOS is a prototype and we do not have
an automated testing infrastructure and thus had to run the
fault injection tests manually. Therefore we were not able to
make statistically signiﬁcant number of runs. However, the
Figure 4.
IP crash
Figure 5. Packet ﬁlter crash
results correlate with our expectations.
C. High Bitrate Crashes
A random crash during a high frequency of operations
can cause a serious damage to the network trafﬁc due to a