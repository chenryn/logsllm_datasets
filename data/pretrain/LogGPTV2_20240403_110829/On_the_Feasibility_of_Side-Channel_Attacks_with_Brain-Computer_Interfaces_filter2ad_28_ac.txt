on the screen for a short time (we used 500 ms). The
stimuli labels y are known to the classiﬁer as the sys-
tem knows what it shows to the user. Given this input,
the classiﬁer must learn a function g that maps epochs to
target stimuli labels:
g : Rp → {0, 1}
x (cid:55)→ y
(1)
In the beginning of Section 5, we explain how to practi-
cally carry out the training phase with users that actively
support this training phase and with passive users.
In the classiﬁcation phase the classiﬁer gets a collec-
tion of n2 new epochs Xtest = {xtest
i ∈ Rp, i = 1, ..., n2}
as an input and must output an estimate ˆy =
i ), i = 1, ..., n2} of the corresponding labels.
{ˆyi = g(xtest
This means, for each of the new epochs, the classiﬁer
must decide whether the epoch is associated with the tar-
get stimulus or not.
i∈Ek
(cid:80)
The test labels ˆy provide a ranking of the K unique
stimuli presented to the user. We sort all stimuli in de-
scending order according to the number of their positive
classiﬁcations. For stimulus k this number is N (+)
k =
ˆyi. The set Ek is the set of epoch indices contain-
ing all epochs that are associated with stimulus k. In this
notation i ∈ Ek means that we sum over all epochs of
stimulus k. For instance, if there are three different stim-
uli repeatedly shown to the user in random order (three
different faces, say), then the classiﬁer would guess that
the true face (the one familiar to the user) is the face
where the most associated epochs have been classiﬁed as
F7F8O1O2P7P8T7T8FC5FC6F3F4AF3AF4(a) target stimulus
(b) non-target stimulus
Figure 8: Event-related potentials for two different stimuli. Both signals have been recorded on the left back-side of
the scalp (Channel 7: ‘O1’). The plots have been produced with EEGlab [4]. The scale of the averaged plots (bottom)
as well as the colorscale of the heatmap plots (top) are constant over the two stimuli.
target-stimulus. Figure 8 depicts event-related potentials
(ERP) for one channel and two different stimuli (target
and non-target). In this example one row of one plot rep-
resents an epoch and all rows of one plot constitute the
set Ek of epochs associated with event k.
The stimulus with the topmost positive classiﬁcations
is the estimated target-stimulus, the stimulus with the
second most positively classiﬁed epochs is ranked sec-
ond, and so on. Most classiﬁers output a continuous
score si for each epoch instead of binary labels ˆyi. For
instance, this could be a probability si = p(yi = 1).
In such a case, we sum over all scores of each unique
stimulus k to get its vote N (+)
si. In the ex-
periments, we will use this ranking to decide which of
the presented stimuli is the target stimulus, that is which
of the answers is the true answer for the current user.
k =(cid:80)
i∈Ek
In the following we explain two different classiﬁers
that we used in our experiments. The ﬁrst classiﬁer is
a boosting algorithm for logistic regression (bLogReg)
and was proposed for P300 spelling in [17]. The second
classiﬁer is the publicly available BCI2000 P300 classi-
ﬁer. BCI2000 uses stepwise linear discriminant analysis
(SWLDA). In [21] a set of different P300 classiﬁers, in-
cluding linear and non-linear support vector machines,
was compared and SWLDA performed best.
4.4.1 Boosted logistic regression
This method uses a logistic regression model as the clas-
siﬁer function g. The model is trained on the training
data by minimizing the negative Bernoulli log-likelihood
of the model in a stepwise fashion as proposed in [11,
12].
As follows, we brieﬂy describe a variant, proposed in
[17], where the method has been used to design a P300
speller. The classiﬁer consists of an ensemble of M weak
learners. Each weak learner fm is a regression function
minimizing a quadratic cost function:
(cid:0)˜yi − f (xtr
i ; w)(cid:1)2
,
(2)
n1(cid:88)
i=1
fm = argmin
f
i ; w) = wT xtr
i with coefﬁcients w ∈ Rp. The
where f (xtr
score ˜yi in Equation (2) is obtained from the ﬁrst-order
condition of maximizing the logarithm of the Bernoulli
likelihood
n1(cid:89)
i=1
L(gm; Xtr, y) =
with
p(yi = 1|xtr
i ) =
p(yi = 1|xtr
i )yi(1−p(yi = 1|xtr
i ))1−yi
(3)
exp(gm(xtr
i )) + exp(−gm(xtr
i ))
i ))
exp(gm(xtr
(4)
In step m of the algorithm, the current classiﬁer gm−1
is updated by adding the new weak classiﬁer fm: gm =
gm−1 + γmfm. Thereby, the weight γm is selected such
that the likelihood Eq. (3) is maximized.
The number of weak classiﬁers M controls the trade-
off between overﬁtting and underﬁtting. This number is
determined by cross-validation on random subsets of the
training data Xtr.
8
TrialsO11020−100  0  100 200 300 400 500 600−6−4−20246Time (ms)µVTrialsO11020−100  0  100 200 300 400 500 600−6−4−20246Time (ms)µVData preprocessing Before training the classiﬁer and
prior applying it to each new observation, we process
the data in the following way. The input data consists
of nc different time series, whereas nc is the number of
channels. First we epochize the signal with a time frame
around the stimuli that starts 200 ms before the respective
stimulus and ends 1000 ms after the stimulus. Then, for
each epoch, we subtract the mean amplitude of the ﬁrst
200 ms from the entire epoch as it represents the base-
line.
In order to reduce the high-frequency noise, we apply
a low-pass FIR ﬁlter with a pass band between 0.35 and
0.4 in normalized frequency units. An example of such a
preprocessed signal is depicted in Figure 3.
4.4.2 Stepwise Linear Discriminant Analysis
The BCI2000 P300 classiﬁer uses stepwise linear dis-
criminant analysis, an extension of Fisher’s linear dis-
criminant analysis. As follows, we brieﬂy explain these
two methods.
Fisher’s linear discriminant analysis (LDA) LDA
was ﬁrst proposed in [9]. This classiﬁer is a linear hyper-
plane that separates the observations of the two classes.
The hyperplane is parameterized by the coefﬁcient vec-
tor w ∈ Rp which is orthogonal to the hyperplane.
A new observation xi is labeled to belong to either of
the two classes by projecting it on the class separation
wT xi. LDA assumes observations in both classes to be
Gaussian distributed with parameters (µj, Σj), j = 1, 2
and computes the optimally separating coefﬁcients by
w = (µtr
1 − µtr
2)−1.
2)(Σtr
1 + Σtr
Stepwise Linear Discriminant Analysis (SWLDA)
SWLDA extends LDA with a feature selection mecha-
nism that sets many of the coefﬁcients in w to zero. This
classiﬁer is supposedly more robust to noise and was
ﬁrst applied to P300 spelling in [7]. The algorithm it-
eratively adds or removes components of the coefﬁcient
vector according to their statistical signiﬁcance for the
label outcome as measured by their p-value. The thresh-
olds (padd, prem) for adding or removing features as well
as the total number of features must be pre-deﬁned.
In our experiments we used the default conﬁguration
of the the BCI2000 P300 classiﬁer with 60 features and
(padd, prem) = (0.1, 0.15). The algorithm uses the 800 ms
period after the stimulus for classiﬁcation.
For each stimulus presented, we sum up the scores
wT xi of the corresponding epochs in order to obtain a
ranking of the stimuli. Then, the highest ranked stimulus
is presumably the target-stimulus.
9
5 Results
In this section, we evaluate the classiﬁcation results on
each of the experiments described in Section 4.3.
User-supported calibration and on-the ﬂy calibration
We calibrate the classiﬁers on a set of training observa-
tions. Thereby, we distinguish two training situations.
In the ﬁrst situation we have a partially cooperating
user, that is, a user who actively supports the training
phase of the BCI but then does not actively provide evi-
dence for the target stimulus later. This is a realistic sce-
nario. Each gamer has a strong incentive to support the
initial calibration phase of his device, because he will
beneﬁt from a high usability and a resulting satisfying
gaming experience. The attacker can use the training
data to train his own classiﬁer. Despite the user support-
ing the calibration phase, we do not assume that the user
actively supports the detection of target stimuli when the
attacker later carries out his attack by suddenly present-
ing new stimuli on the screen.
In the second training situation, the user is passive.
This means that the user does not support the training
phase but also does not actively try to disturb it. As a
consequence, the attacker must present a set of stimuli
where, with high probability, the user is familiar with
one of the stimuli and unfamiliar with all other stim-
uli. In this way the attacker can provide a label vector
y ∈ {0, 1}n1 that can be used for training. We used the
people experiment as training data. We showed 10 im-
ages of random people to the user as well as one image
of President Barack Obama. Assuming that i) every user
knows Obama and that ii) it is unlikely that a user knows
one of the random face images downloaded from the in-
ternet, we can use the Obama image as a target stimulus
and the others as non-target stimuli.
Success statistics We report the results of all experi-
ments in Figure 9. Each plot corresponds to one ex-
perimental scenario. The black crosses depict the re-
sults of the SWLDA classiﬁer used by the BCI2000 P300
speller. The red diamonds are the results of boosted log-
arithmic regression (bLogReg) trained by the counting
experiment, and the blue crosses show the results for
bLogReg when trained on the people experiment. The
dashed black line depicts the expected result of a random
guess.
We depict the results in terms of a cumulative statistic
of the rank of the correct answer. This measure provides
the accuracy together with a conﬁdence interval at the
same time as it includes the probability distribution of
the deviation from the optimal rank. The plots read as
follows. The x-axis of each plot is the rank of the correct
(a) 1st digit PIN
(b) Debit card
(c) Location
(d) Month of birth
(e) People
(f) ATM machine
Figure 9: Cumulative statistics of the ranking of the correct answer according to the classiﬁcation result. The faster this
measure converges towards 100%, the better the classiﬁer. One can directly read the conﬁdence intervals as follows:
In more than 20% of the experiments the bLogReg classiﬁer ranked the correct face at the ﬁrst position. In more than
40% it had the correct face among the ﬁrst three guesses. Please note that for the passive user, the classiﬁer was trained
on the people experiment and the corresponding curve in Fig. 9(e) would depict the training error.
answer as estimated by the respective classiﬁer. For in-
stance, if the correct answer in the month of birth exper-
iment is ‘April’ and the classiﬁer ranks this month at the
third position in the classiﬁcation output, then x is 3. The
y-axis is the fraction (in %) of the users having the cor-
rect answer in at most ranking position x. In our exam-
ple with the month of birth, the point (x; y) = (3; 80%)
of the SWLDA classiﬁer means that for 80% of the users
the correct bank was among the ﬁrst three guesses of
SWLDA. Please note that we truncated the y-axis at 20%
to get a better resolution of the dynamic range.
Overall, one can observe that the attack does not al-
ways reveal the correct information on the ﬁrst guess.
However, the classiﬁers perform signiﬁcantly better than
the random attack. The SWLDA classiﬁer provided the
most accurate estimates, except for the experiment on the
PIN and the debit card.
The correct answer was found by the ﬁrst guess in
20% of the cases for the experiment with the PIN, the
debit cards, people, and the ATM machine. The location
was exactly guessed for 30% of users, month of birth for
almost 60% and the bank based on the ATM machines
for almost 30%. All classiﬁers performed consistently
good on the location experiment where the users actively
10
concentrated by counting the occurrence of the correct
answer. SWLDA performed exceptionally good on the
month of birth experiment, even though this experiment
was carried out without counting.
Relative reduction of entropy In order to quantify the
information leak that the BCI attack provides, we com-
pare the Shannon entropies of guessing the correct an-
swer for the classiﬁers against the entropy of the random
guess attack.
This measure models the guessing attack as a random
experiment with the random variable X. Depending of
the displayed stimuli, X can take different values. For in-
stance, in the PIN experiment, the set of hypotheses con-
sists of the numbers 0 to 9 and the attack guess would
then take one out of these numbers. Now, let’s assume
we have no other information than the set of hypotheses.
Then we would guess each answer with equal probabil-
ity. This is the random attack. Let the number of possible
answers (the cardinality of the set of hypotheses) be K,
then the entropy of the random attack is log2(K).
More formally, let the ranking of a classiﬁer clf be
, where the ﬁrst-ranked answer
, and so on. Let
, the second-ranked answer is a(clf)
, ..., a(clf)
K
a(clf)
1
2
(cid:110)