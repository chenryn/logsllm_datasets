of an attack and providing an incentive for improved local security.
3.3 Destination Policies
The next question we consider is how a destination can deter-
mine whether to authorize a request. This is a matter of policy, and
it depends on the role the destination plays in the network. We con-
sider two extreme cases of a client and a public server to argue that
simple policies can be effective.
A client may act in a way that by default allows it to contact any
server but not otherwise be contacted, as is done by ﬁrewalls and
NAT boxes today. To do this, it accepts incoming requests if they
match outgoing requests it has already made and refuses them oth-
erwise. Note that the client can readily do this because capabilities
are added to existing packets rather than carried as separate pack-
ets. For example, a client can accept a request on a TCP SYN/ACK
that matches its earlier request on a TCP SYN.
A public server may initially grant all requests with a default
number of bytes and timeout, using the path identiﬁer to fairly serve
different sources when the load is high. If any of the senders mis-
behave, by sending unexpected packets or ﬂoods, that sender can
be temporarily blacklisted and its capability will soon expire. This
blacklisting is possible because the handshake involved in the ca-
pability exchange weakly authenticates that the source address cor-
responds to a real host. The result is that misbehaving senders are
quickly contained. More sophisticated policies may be based on
HTTP cookies that identify returning customers, CAPTCHAs that
distinguish zombies from real users [11], and so forth.
Pre-capability (routers)
timestamp (8 bits)
hash(src IP, dest IP, time, secret) (56 bits)
Capability (hosts)
timestamp (8 bits)
hash(pre-capability, N, T) (56 bits)
ttl
(cid:1) (t2-t1) x N
(cid:1) (t4-t3) x N
(cid:1) N
ts
t1
t2
t3
t4
ts+T
time
Figure 3: Format of capabilities.
Figure 4: Bound on the bytes of a capability with caching.
3.4 Unforgeable Capabilities
Having provided a bootstrap mechanism and policy, we turn our
attention to the form of capabilities themselves. Our key require-
ment is that an attacker can neither forge a capability, nor make use
of a capability that they steal or transfer from another party. We
also need capabilities to expire.
We use cryptography to bind each capability to a speciﬁc net-
work path, including source and destination IP addresses, at a spe-
ciﬁc time. Each router that forwards a request packet generates its
own pre-capability and attaches it to the packet. This pre-capability
is shown in Figure 3. It consists of a local router timestamp and a
cryptographic hash of that timestamp plus the source and destina-
tion IP addresses and a slowly-changing secret known only to the
router. Observe that each router can verify for itself that a pur-
ported pre-capability attached to a packet is valid by re-computing
the hash, since the router knows all of the inputs, but it is crypto-
graphically hard for other parties to forge the pre-capability with-
out knowing the router secret. Each router changes its secret at
twice the rate of the timestamp rollover, and only uses the current
or the previous secret to validate capability. This ensures that a
pre-capability expires within at most the timestamp rollover period,
and each pre-capability is valid for about the same time period re-
gardless of when it is issued. The high-order bit of the timestamp
indicates whether the current or the previous router secret should
be used for validation. This trick allows a router to try only one
secret even if the router changed its secret right after issuing a pre-
capability.
The destination thus receives an ordered list of pre-capabilities
that corresponds to a speciﬁc network path with ﬁxed source and
destination IP endpoints.
It is this correspondence that prevents
an attacker from successfully using capabilities issued to another
party: it cannot generally arrange to send packets with a speciﬁc
source and destination IP address through a speciﬁc sequence of
routers unless it is co-located with the source. In the latter case, the
attacker is indistinguishable from the source as far as the network
is concerned, and shares its fate in the same manner as for requests.
(And other, more devastating attacks are possible if local security
is breached.) Thus we reduce remote exploitation to the problem
of local security.
If the destination wishes to authorize the request, it returns an
ordered list of capabilities to the sender via a packet sent in the
reverse direction. Conceptually, the pre-capabilities we have de-
scribed could directly serve as these capabilities. However, we pro-
cess them further to provide greater control, as is described next.
3.5 Fine-Grained Capabilities
Even effective policies will sometimes make the wrong decision
and the receiver will authorize trafﬁc that ultimately is not wanted.
For example, with our blacklist server policy an attacker will be
authorized at least once, and with our client policy the server that
a client accesses may prove to be malicious. If authorizations were
binary, attackers whose requests were granted would be able to ar-
bitrarily ﬂood the destination until their capabilities expire. This
problem would allow even a very small rate of false authorizations
to deny service. This argues for a very short expiration period, yet
protocol dynamics such as TCP timeouts place a lower bound on
what is reasonable.
To tackle this problem, we design ﬁne-grained capabilities that
grant the right to send up to N bytes along a path within the next T
seconds, e.g., 100KB in 10 seconds2. That is, we limit the amount
of data as well as the period of validity. The form of these ca-
pabilities is shown in Figure 3. The destination converts the pre-
capabilities it receives from routers to full capabilities by hashing
them with N and T . Each destination can choose N and T (within
limits) for each request, using any method from simple defaults to
models of prior behavior. It is these full capabilities, along with N
and T , that are returned to authorize the sender. For longer ﬂows,
the sender should renew these capabilities before they reach their
limits.
With this scheme, routers verify their portion of the capabilities
by re-computing the hashes much as before, except that now two
hashes are required instead of one. The routers now perform two
further checks, one for N and one for T . First, routers check that
their local time is no greater than the router timestamp plus T to
ensure that the capability has not expired. This requires that T
be at most one half of the largest router timestamp so that two time
values can be unambiguously compared under a modulo clock. The
replay of very old capabilities for which the local router clock has
wrapped are handled as before by periodically changing the router
secret. Second, routers check that the capability will not be used
for more than N bytes. This check is conceptually simple, but
it requires state and raises the concern that attackers may exhaust
router state. We deal with this concern next.
3.6 Bounded Router State
We wish to ensure that attackers cannot exhaust router mem-
ory to bypass capability limits. This is especially a concern given
that we are counting the bytes sent with a capability and colluding
attackers may create many authorized connections across a target
link.
To handle this problem, we design an algorithm that bounds the
bytes sent using a capability while using only a ﬁxed amount of
router state no matter how attackers behave. In the worst case, a
capability may be used to send 2N bytes in T seconds. The same
capability will still be precisely limited to N bytes if there is no
memory pressure.
The high level idea of the algorithm is to make a router keep state
only for ﬂows (a ﬂow is deﬁned on a sender to a destination basis.)
with valid capabilities that send faster than N/T . The router does
not need to keep state for other authorized ﬂows because they will
2An alternative would be to build rapid capability revocation. We
believe this to be a less tractable problem.
not send more than N bytes before their capabilities expire in T
seconds. We track ﬂows via their rates by using the rate N/T to
convert bytes to equivalent units of time, as we describe next.
When a router receives a packet with a valid capability for which
it does not have state, it begins to track byte counts for the capabil-
ity and also associates a minimal time-to-live (ttl) with the state.
The ttl is set to the time equivalent value of the packet: L ∗ T /N
seconds (with L being the packet length). This ttl is decremented
as time passes (but our implementation simply sets an expiration
time of now + ttl) and incremented as subsequent packets are
charged to the capability. When the ttl reaches zero, it is permissi-
ble for the router to reclaim the state for use with a new capability.
We now show that this scheme bounds the number of bytes sent
using a capability. Referring to Figure 4, suppose that the router
created the capability at time ts and it expires at time ts + T . Fur-
ther suppose that the router creates state for the capability at time
t1 > ts, and reclaims the state when its ttl reaches zero at time
t2 < ts + T . Then by the deﬁnition of the ttl, the capability
must have been used for at most (t2 − t1)/T ∗ N bytes from t1
to t2. This may occur more than once, but regardless of how many
times it occurs, the time intervals can total to no more than T sec-
onds. Thus the total bytes used for the capability must be at most
T /T ∗ N = N bytes. If a capability has state created at time im-
mediately preceding ts + T , then up to N bytes can be sent at a
rate faster than N/T . Therefore, at most N + N = 2N bytes can
be sent before the capability is expired.
This scheme requires only ﬁxed memory to avoid reclaiming
state with non-zero ttl values, as required above. Suppose the ca-
pacity of the input link is C. To have state at time t, a capability
must be used to send faster than N/T before t. Otherwise, the ttl
associated with the state will reach zero and the state may be re-
claimed. There can be at most C/(N/T ) such capabilities. We
require that the minimum N/T rate be greater than an architec-
tural constraint (N/T )min. This bounds the state a router needs
to C/(N/T )min records. As an example, if the minimum sending
rate is 4K bytes in 10 seconds, a router with a gigabit input line will
only need 312,500 records. If each record requires 100 bytes, then
a line card with 32MB of memory will never run out of state.
3.7 Efﬁcient Capabilities
We want capabilities to be bandwidth efﬁcient as well as secure.
Yet these properties are in conﬂict, since security beneﬁts from long
capabilities (i.e., a long key length) while efﬁciency beneﬁts from
short ones (i.e., less overhead). To reconcile these factors, we ob-
serve that most bytes reside in long ﬂows for which the same ca-
pability is used repeatedly on packets of the ﬂow. Thus we use
long capabilities (64 bits per router) to ensure security, and cache
capabilities at routers so that they can subsequently be omitted for
bandwidth efﬁciency. We believe that this is a better tradeoff than
short capabilities that are always present, e.g., SIFF uses 2 bits per
router. Short capabilities are vulnerable to a brute force attack if
the behavior of individual routers can be inferred, e.g., from band-
width effects, and do not provide effective protection with a limited
initial deployment.
In our design, when a sender obtains new capabilities from a
receiver, it chooses a random ﬂow nonce and includes it together
with the list of capabilities in its packets. When a router receives
a packet with a valid capability it caches the capability relevant in-
formation and ﬂow nonce, and initializes a byte counter and ttl as
previously described. Subsequent packets can then carry the ﬂow
nonce and omit the list of capabilities. Observe that path MTU dis-
covery is likely unaffected because the larger packet is the ﬁrst one
sent to a destination. Routers look up a packet that omits its capa-
bilities using its source and destination IP addresses, and compare
the cached ﬂow nonce with that in the packet. A match indicates
that a router has validated the capabilities of the ﬂow in previous
packets. The packets are then subject to byte limit and expiration
time checking as before.
For this scheme to work well, senders must know when routers
will evict their capabilities from the cache. To do so, hosts model
router cache eviction based on knowledge of the capability param-
eters and how many packets have used the capability and when. By
the construction of our algorithm, eviction should be rare for high-
rate ﬂows, and it is only these ﬂows that need to remain in cache
to achieve overall bandwidth efﬁciency. This modeling can either
be conservative, based on later reverse path knowledge of which
packets reached the destination3, or optimistic, assuming that loss
is infrequent. In the occasional case that routers do not have the
needed capabilities in cache, the packets will be demoted to legacy
packets rather than lost, as we describe next.
3.8 Route Changes and Failures
To be robust, our design must accommodate route changes and
failures such as router restarts. The difﬁculty this presents is that a
packet may arrive at a router that has no associated capability state,
either because none was set up or because the cache state or router
secret has been lost.
This situation should be infrequent, but we can still minimize its
disruption. First, we demote such packets to be the same priority as
legacy trafﬁc (which have no associated capabilities) by changing a
bit in the capability header. They are likely to reach the destination
in normal operation when there is little congestion. The destination
then echoes demotion events to the sender by setting a bit in the
capability header of the next message sent on the reverse channel.
This tells the sender that it must re-acquire capabilities.
3.9 Balancing Authorized Trafﬁc
Capabilities ensure that only authorized trafﬁc will compete for
the bandwidth to reach a destination, but we remain vulnerable to
ﬂoods of authorized trafﬁc: a pair of colluding attackers can au-
thorize high-rate transfers between themselves and disrupt other
authorized trafﬁc that shares the bottleneck. This would allow, for
example, a compromised insider to authorize ﬂoods on an access
link by outside attackers.
We must arbitrate between authorized trafﬁc to mitigate this at-
tack. Since we do not know which authorized ﬂows are malicious,
if any, we simply seek to give each capability a reasonable share of
the network bandwidth. To do this we use fair-queuing based on
the authorizing destination IP address. This is shown in Figure 2.
Users will now get a decreasing share of bandwidth as the network
becomes busier in terms of users (either due to legitimate usage or
colluding attackers), but they will be little affected unless the num-
ber of attackers is much larger than the number of legitimate users.
Note that we could queue on the source address (if source ad-
dress can be trusted) or other ﬂow deﬁnitions involving preﬁxes.
The best choice is a matter of AS policy that likely depends on
whether the source or destination is a direct customer of the AS,
e.g., the source might be used when the packet is in the sender
ISP’s network and vice versa.
One important consideration is that we limit the number of queues
to bound the implementation complexity of fair queuing. To do
this, we again fall back on our router state bound, and fair-queue
over the ﬂows that have their capabilities in cache. In this man-
ner, the high-rate ﬂows that send more rapidly than N/T will fairly
3We ignore for the present the layering issues involved in using
transport knowledge instead of building more mechanism.
share the bandwidth. These are the ﬂows that we care most about
limiting. The low-rate ﬂows will effectively receive FIFO service
with drops depending on the timing of arrivals. This does not guar-
antee fairness but is adequate in that it prevents starvation. An alter-
native approach would have been to hash the ﬂows to a ﬁxed num-
ber of queues in the manner of stochastic fair queuing [23]. How-
ever, we believe our scheme has the potential to prevent attackers
from using deliberate hash collisions to crowd out legitimate users.
3.10 Short, Slow or Asymmetric Flows
TVA is designed to run with low overhead for long, fast ﬂows
that have a reverse channel. Short or slow connections will expe-
rience a higher relative overhead, and in the extreme may require
a capability exchange for each packet. However, several factors
suggest that TVA is workable even in this regime. First, the effect
on aggregate efﬁciency is likely to be small given that most bytes
belong to long ﬂows. Second, and perhaps more importantly, our
design does not introduce added latency in the form of handshakes,
because capabilities are carried on existing packets, e.g., a request
may be bundled with a TCP SYN and the capability returned on the
TCP SYN/ACK. Third, short ﬂows are less likely because ﬂows are
deﬁned on a sender to a destination IP address basis. Thus all TCP
connections or DNS exchanges between a pair of hosts can take
place using a single capability.
TVA will have its lowest relative efﬁciency when all ﬂows near
a host are short, e.g., at the root DNS servers. Here, the portion
of request bandwidth must be increased. TVA will then provide
beneﬁts by fair-queuing requests from different regions of the net-
work. Truly unidirectional ﬂows would also require capability-only
packets in the reverse direction. Fortunately, even media streaming
protocols typically use some reverse channel communications. Fi-
nally, we have not addressed IP multicast as it already require some
form of authorization action from the receiver. It would be inter-
esting to see whether we can provide a stronger protection in this
setting by using capabilities.
4. TVA PROTOCOL
In this section, we describe TVA in terms of how hosts and
routers process packets and provide a more detailed view of the
common case for data transfer. We consider attacks more system-
atically in the following sections. We ignore legacy concerns for
the moment, returning to them in Section 8.
There are three elements in our protocol: packets that carry ca-
pability information; hosts that act as senders and destinations; and
routers that process capability information. We describe each in
turn.
4.1 Packets with Capabilities
Other than legacy trafﬁc, all packets carry a capability header
that extends the behavior of IP. We implement this as a shim layer
above IP, piggybacking capability information on normal packets