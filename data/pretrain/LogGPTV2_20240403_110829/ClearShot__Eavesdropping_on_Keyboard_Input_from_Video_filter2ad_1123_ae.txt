have proposed to use the reﬂections of the screen’s optical
emanations in common objects to read the screen’s contents
180
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
Computer Vision Language-based Context-sensitive
Total Words per Minute
Analysis
Analysis
Analysis
Alice
Bob
mm:ss
10:08
7:33
mm:ss
59:58
15:00
mm:ss
44:15
48:00
mm:ss
114:21
70:33
#
1.0
1.7
Table 4. Performance breakdown of ClearShot.
at a distance [4]. They use a telescope to pick up the re-
ﬂections in objects commonly located in proximity of the
screen, such as plastic bottles and eyeglasses. They use stan-
dard algorithms to improve the readability of the recovered
image but they do not perform any analysis to automatically
identify or reconstruct the eavesdropped contents.
In more recent years, researchers have investigated the
use of emanations of different nature than electromagnetic.
In particular, several works focused on acoustic emanations
caused by a user typing on a regular keyboard. Asonov and
Agrawal show that it is possible to differentiate the sound
caused by the pressing of different keys, and employ a neu-
ral network to recognize the key being pressed [2]. They
report about a 50% probability of ﬁnding the pressed key in
the set of 4 keys proposed by the system in tests consisting
of 10 clicks per key. Differently from our study, their ex-
periments do not seem to address realistic typing patterns.
Zhuang et al. present an attack that uses the statistical con-
straints of the English language to reconstruct single words
from 10-minute sound recordings without any labeled train-
ing data [48]. Similarly to our work, they employ several
error correction techniques to improve the performance of
their keystroke classiﬁer. They report a word recognition
success rate in an unsupervised setting between 47% and
74% and higher in a supervised setting.
The work most similar to ours is the one from Berger et
al. [6]. The authors analyze an audio recording of the clicks
made by a user typing single words, and compute spatial
constraints (equal, adjacent, near, distant) for each pair of
keystrokes. They use a dictionary to identify words that sat-
isfy the inferred constraints. They evaluate their technique
as a way to reconstruct passwords derived from a dictionary
and discuss its use as a building block for a long-text re-
construction system. Their experiments involve only single
words. They analyze a set of 27 words, each 7–13 characters
long, using 3 keyboards, and report about 40% probability
of ﬁnding the correct word in the top 10 proposed words,
about 60% in the top 25, 70% in the top 50. Our approach
focuses directly on long text reconstruction and introduces
a number of techniques that aid in achieving this goal.
Reconstructing the typed text from a video recording
might seem simpler than performing a sound-based anal-
ysis. However, analyzing a video introduces a set of new
challenges. In particular, extracting from a mute video the
information of when a key pressing occurs is more difﬁcult
than extracting the same information from a sound record-
ing. As a consequence, in a video-based analysis, basic
information, such as how many characters compose each
word, is not immediately available and has to be inferred.
We expect that the combination of the two techniques would
achieve very high detection rates.
Trafﬁc analysis techniques have been used to eavesdrop
on encrypted communications transmitted over a network.
For example, Song et al. leverage the keystroke timing data
observable in older SSH implementations to recover pass-
words typed in encrypted sessions [39].
In a test with 4
users typing 5 passwords of 6, 7, and 8 characters taken
from a reduced alphabet, they report that the correct pass-
word was found in the top 0.1%–62.3% of the strings pro-
posed by their system. Timing is used actively, as opposed
to passively, by Shah et al. [38]: they introduce JitterBugs, a
class of in-line interception mechanisms that covertly trans-
mit data by perturbing the timing of input events likely to
affect externally observable network trafﬁc. Finally, Wright
et al. analyze Voice over IP (VoIP) communications. They
observe that different languages are encoded at different bit
rates by Variable Bit Rate encoders and that packet sizes can
be used as a predictor of the bit rate used to encode the cor-
responding frame. They use this information to identify the
language spoken in encrypted VoIP trafﬁc [46].
5 Conclusions
In this paper, we presented a novel approach to the au-
tomated extraction of information from a web cam video
that records a user typing on a keyboard. The approach is
based on several novel techniques for movement tracking,
sentence reconstruction, and error correction. The approach
has been implemented in a tool, called ClearShot, which is
able to extract a substantial portion of the text being typed
in a video, under certain assumptions.
Even though the automatic recognition of the keys
pressed by a person based on video information only is a
very complex and challenging task, preventing this attack is
easy. The obvious solution is to place some kind of physical
shield over the keyboard so that the keys can be seen only
by the typist. This technique is sometimes used to protect
keypads used to enter PINs at ATMs and POSs. However,
this type of protections are not widely used for computer
keyboards.
Future work will focus on improving the motion tracking
algorithm so that it works reliably in a number of different
181
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
settings (i.e., different lighting conditions, different camera
angles, and different camera types). In addition, we plan to
explore how the context of the information being extracted
(e.g., the use of speciﬁc keywords) can be leveraged to im-
prove the selection of words among multiple alternatives.
As far as we know, this is the ﬁrst tool of this kind. We
envision that this tool could be of value in the case of long-
lasting surveillance operations. In addition, we anticipate
that some of the techniques developed to extract text from
a typing video could be reused and adapted to other ﬁelds,
such as computer vision and augmented reality.
Acknowledgments
This research was partially supported by the National
Science Foundation, under grants CCR-0238492, CCR-
0524853, and CCR-0716095.
References
[1] 3M Notebook Privacy Computer Filter PF15.2W. http:
//www.3m.com, 2007.
[2] D. Asonov and R. Agrawal. Keyboard Acoustic Emanations.
In Proceedings of the IEEE Symposium on Security and Pri-
vacy, pages 3–11, 2004.
[3] K. Atkinson.
GNU Aspell 0.50.5 Documentation.
http://aspell.net/0.50-doc/man-html/
manual.html, 2004.
[4] M. Backes, M. D¨urmuth, and D. Unruh. Compromising Re-
ﬂections - or - How to Read LCD Monitors Around the Cor-
ner. In Proceedings of the IEEE Symposium on Security and
Privacy, 2008.
[5] J. Barron, D. Fleet, and S. Beauchemin. Performance of Op-
International Journal of Computer
tical Flow Techniques.
Vision, 12(1):43–77, 1994.
[6] Y. Berger, A. Yeredor, and A. Wool. Dictionary Attacks Us-
ing Keyboard Acoustic Emanations. In Proceedings of the
ACM Conference on Computer and Communications Secu-
rity, pages 245–254, 2006.
[7] T. Brants and A. Franz. Web 1T 5-gram Version 1. Linguistic
Data Consortium, Philadelphia, 2006.
[8] E. Brill and R. Moore. An Improved Error Model for Noisy
Channel Spelling Correction. Proceedings of the 38th An-
nual Meeting on Association for Computational Linguistics,
pages 286–293, 2000.
[9] Y. Cetin. Cable Free Keyboard Apparatus Based on Com-
puter Vision. WIPO Patent WO/2002/027457, April 2000.
[10] K. Church and W. Gale. Probability Scoring for Spelling
Correction. Statistics and Computing, 1(2):93–103, 1991.
[11] F. Damerau. A technique for computer detection and cor-
rection of spelling errors. Communications of the ACM,
7(3):171–176, 1964.
[12] S. Fels and G. Hinton. Glove-TalkII: an Adaptive Gesture-
to-Formant Interface. In Proceedings of the SIGCHI Confer-
ence on Human Factors in Computing Systems, pages 456–
463, 1995.
[13] S. Fogie.
Axis communications 207w network cam-
http://www.
era web interface vulnerabilities.
securityfocus.com/bid/25678, 2007.
[14] W. Freeman, K. Tanaka, J. Ohta, and K. Kyuma. Computer
Vision for Computer Games. In Proceedings of the Interna-
tional Conference on Automatic Face and Gesture Recogni-
tion, pages 100–105, 1996.
[15] A. Golding and D. Roth. A Winnow-Based Approach to
Context-Sensitive Spelling Correction. Machine Learning,
34(1):107–130, 1999.
[16] H. Highland. Electromagnetic Radiation Revisited. Com-
puters & Security, 5(2):85–93, 1986.
[17] B. Horn and B. Schunck. Determining Optical Flow. Artiﬁ-
cial Intelligence, 17:185–203, 1981.
[18] Intel.
brary.
computing/opencv/.
OpenCV: Open Source Computer Vision Li-
http://www.intel.com/technology/
[19] Y. Ivanov and A. Bobick. Recognition of Multi-Agent In-
teraction in Video Surveillance. In Proceedings of the Inter-
national Conference on Computer Vision, pages 169–176,
1999.
[20] D. Jurafsky and J. Martin. Speech and Language Process-
ing: An Introduction to Natural Language Processing, Com-
putational Linguistics, and Speech Recognition. MIT Press,
2000.
[21] J. Kennedy. Inaugural Address, January 1961.
[22] M. Kuhn. Optical Time-Domain Eavesdropping Risks of
CRT Displays. In Proceedings of the IEEE Symposium on
Security and Privacy, pages 3–18, 2002.
[23] M. Kuhn. Electromagnetic Eavesdropping Risks of Flat-
Panel Displays. In Proceedings of the International Work-
shop on Privacy Enhancing Technologies, pages 88–107,
2004.
[24] M. Kuhn and R. Anderson. Soft Tempest: Hidden Data
Transmission Using Electromagnetic Emanations.
In Pro-
ceedings of the International Workshop on Information Hid-
ing, pages 124–142, 1998.
[25] K. Kukich. Techniques for Automatically Correcting Words
in Text. ACM Computing Surveys, 24(4):377–439, 1992.
[26] J. LaViola. A Survey of Hand Posture and Gesture Recogni-
tion Techniques and Technology. Technical Report CS-99-
11, Brown University, 1999.
[27] C. Lee and Y. Xu. Online, Interactive Learning of Gestures
for Human/Robot Interfaces. In Proceedings of the IEEE In-
ternational Conference on Robotics and Automation, pages
2982–2987, 1996.
[28] V. Levenshtein. Binary codes capable of correcting dele-
tions, insertions, and reversals. Doklady Physics, 10(8):707–
710, 1966.
[29] J. Loughry and D. A. Umphress. Information Leakage from
Optical Emanations. ACM Transactions on Information and
System Security, 5(3):262–289, 2002.
[30] T. Marrin and R. Picard. The ‘Conductor’s Jacket’: A Device
for Recording Expressive Musical Gestures. In Proceedings
of the International Computer Music Conference, 1998.
[31] E. Mays, F. Damerau, and R. Mercer. Context Based
International Journal on Information
Spelling Correction.
Processing and Management, 27(5):517–522, 1991.
[32] M. McIlroy. Development of a Spelling List. IEEE Trans-
actions on Communications, 30(1):91–99, 1982.
[33] Tempest Fundamentals. NACSIM 5000 NSA-82-89, Na-
tional Security Agency, February 1982. Classiﬁed.
[34] S. Oualline. Vi IMproved – Vim. New Riders, 2001.
[35] N. Pal and S. Pal. A Review On Image Segmentation Tech-
niques. Pattern Recognition, 26(9):1277–1294, 1993.
182
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
[36] A. Pastor. Owning big brother (or how to crack into axis ip
cameras). www.procheckup.com, 2007.
[37] P. Robinson. Sneakers. Universal Pictures, 1992.
[38] G. Shah, A. Molina, and M. Blaze. Keyboards and Covert
Channels. In Proceedings of the USENIX Security Sympo-
sium, pages 59–75, 2006.
[39] D. Song, D. Wagner, and X. Tian. Timing Analysis of
Keystrokes and Timing Attacks on SSH. In Proceedings of
the USENIX Security Symposium, 2001.
[40] T. Starner and A. Pentland. Real-time American Sign Lan-
guage Recognition from Video Using Hidden Markov Mod-
els. In Proceedings of the International Symposium on Com-
puter Vision, pages 265–270, 1995.
[41] S. Suzuki and K. Abe. Topological Structural Analysis of
Digitized Binary Images by Border Following. Computer
Vision, Graphics, and Image Processing, 30(1):32–46, 1985.
[42] T. Takahashi and F. Kishino. Hand Gesture Coding Based on
Experiments Using a Hand Gesture Interface Device. ACM
SIGCHI Bulletin, 23(2):67–74, 1991.
[43] C. Tappert, C. Suen, and T. Wakahara. The State of the Art
in On-Line Handwriting Recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 12(8):787–808,
1990.
[44] W. van Eck. Electromagnetic Radiation from Video Dis-
play Units: An Eavesdropping Risk? Computers & Security,
4:269–286, 1985.
[45] A. Wexelblat. A Feature-Based Approach to Continuous-
Gesture Analysis. Master’s thesis, Massachusetts Institute
of Technology, 1994.
[46] C. Wright, L. Ballard, F. Monrose, and G. Masson. Lan-
guage Identiﬁcation of Encrypted VoIP Trafﬁc: Alejandra y
Roberto or Alice and Bob? In Proceedings of the USENIX
Security Symposium, 2007.
[47] Y. Wu, Y. Shan, Z. Zhang, and S. Shafer. Visual Panel: From
an Ordinary Paper to a Wireless and Mobile Input Device.
Technical Report MSR-TR-2000-112, Microsoft Research,
2000.
[48] L. Zhuang, F. Zhou, and J. Tygar. Keyboard Acoustic Em-
anations Revisited. In Proceedings of the ACM Conference
on Computer and Communications Security, pages 373–382,
2005.
183
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply.