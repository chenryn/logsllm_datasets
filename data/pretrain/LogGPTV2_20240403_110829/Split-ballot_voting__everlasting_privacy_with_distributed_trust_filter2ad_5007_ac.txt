#25
#25
#25
#25
#25
Who do you like for president?
Who do you like for president?
Who do you like for president?
Who do you like for president?
A. Alice
B. Bob
A. Alice
B. Bob
B. Alice
A. Bob
B. Alice
A. Bob
A
B
B
A
A
B
B
A
Figure 2.2: Punchscan Ballot Conﬁgurations
To vote, the voter marks the letter corresponding to her
candidate using a wide marker: this marks both the top
and bottom pages simultaneously. The two pages are then
separated. The voter chooses one of the pages to scan (and
keep as a receipt), while the other is shredded (these steps
are shown in Figure 2.3).
Who do you like for president?
Who do you like for president?
#25
#25
#25
#25
A. Alice
B. Bob
B
A
A. Alice
B. Bob
B
A
Who do you like for president?
#25
B. Alice
A. Bob
#25
#25
#25
Who do you like for president?
A. Alice
B. Bob
B
A
B
A
Figure 2.3: Punch-
scan Ballot
Figure 2.4:
Receipts
“Bad”
Each pair of pages has a short id, which a voting authority
can use to determine what was printed on each of the pages
(this allows the authority to determine the voter’s vote even
though it only receives a single page). For someone who does
not know the contents of the shredded page, the receipt does
not give any information about the voter’s choice.
Giving each voter a receipt for her vote is extremely prob-
lematic in traditional voting systems, since the receipt can
be used to coerce voters or to buy votes. Punchscan at-
tempts to prevent vote-buying by making sure that the re-
ceipt does not contain any information about the voter’s
choice. At ﬁrst glance, this idea seems to work:
if an ad-
versary just asks a voter to vote for a particular candidate
(by following the Punchscan protocol honestly), there is no
way the adversary can tell, just by looking at the receipt,
whether the voter followed his instructions or not.
Below, we show that for a slightly more sophisticated ad-
versary, a vote-buying attack is possible against Punchscan.
2.4.1 A Vote Buying Attack. To demonstrate the
attack, we continue to use the Alice/Bob election example.
Suppose the coercer wants to bias the vote towards Alice.
In this case, he publishes that he will pay for any receipt
except those shown in Figure 2.4 (i.e., everything except a
“B,A” bottom page on which “A” was marked, and a “B,A”
top page on which the right hole was marked).
This attack will force one fourth of the voters to vote
for Alice in order to get paid. To see why, consider the
four possible ballot conﬁgurations (in Figure 2.2). Since the
coercer will accept any marking on an “A,B” top page or
an “A,B” bottom page, in three of the four conﬁgurations
the voter can vote as she wishes. However, if both the top
and the bottom pages are “B,A” pages (this occurs in one
fourth of the cases), the voter is forced to vote for Alice if
she wants to return an acceptable receipt.
Although three-fourths of the voters can vote for any can-
didate, this attack is still entirely practical. When a race is
close, only a small number of votes must be changed to tip
the result in one direction. Compared to the “worst possi-
ble” system in which an adversary can buy votes directly,
Punchscan requires the attacker to spend only four times as
much to buy the same number of votes. Since the receipts
are published, this attack can be performed remotely (e.g.,
over the internet), making it much worse than a “standard”
vote-buying attack (such as chain-voting) that must be per-
formed in person.
We must note that the current version of Punchscan (as
described in [19]) instructs the voter to commit to the layer
she will take before entering the voting booth. The original
purpose of this requirement was to prevent a diﬀerent at-
tack, but it suﬃces to foil the attack described above. The
requirement does not appear in any other Punchscan litera-
ture, however, and demonstration elections using Punchscan
did not enforce it (possibly because coercion was not con-
sidered a serious threat in that setting).
3. UNDERLYING ASSUMPTIONS
One of the important advantages of formally analyzing
voting protocols is that we can state the speciﬁc assump-
tions under which our security guarantees hold. Our pro-
tocol uses a combination of physical and cryptographic as-
sumptions. Below, we deﬁne the assumptions and give a
brief justiﬁcation for each.
3.1 Physical Assumptions
Undeniable Ballots.
To allow voters to complain con-
vincingly about invalid ballots, they must be undeniable: a
voter should be able to prove that the ballot was created by
the voting authority. This type of requirement is standard
for many physical objects: money, lottery-tickets, etc.
Forced Private Erasure.
In order to preserve the receipt-
freeness of the protocol, we require voters to physically erase
information from the ballots they used. The erasure assump-
tion is made by a number of existing voting schemes that
require the voter to choose some part of the ballot to securely
discard (e.g., Punchscan [9], Scratch&Vote [1]). In practice,
this can be done by shredding, by chemical solvent, etc.
At ﬁrst glance, it might appear that simply spoiling a
ballot that was not correctly erased is suﬃcient. However,
this is not the case; the voter must be forced to erase the
designated content. Otherwise, a coercer can mount a vote-
buying attack similar to the one described in section 2.4,
where some voters are told to invalidate their ballots by
refusing to erase them (and showing the complete ballot to
the coercer).
Since only the voter should be able to see the contents of
the erased part of the ballot, ﬁnding a good mechanism to
enforce erasure may be diﬃcult (e.g., handing it to an oﬃcial
to shred won’t work). However, a large-scale attack that
relies on circumventing this assumption may be detected by
counting the number of spoiled ballots.
Tamper-Evident Seals.
In order to preserve privacy, bal-
lots must be delivered to the voter sealed (so that no one
else can see their contents).
In order to preserve receipt-
freeness, even the voter must not be able to see the contents
of a ballot before opening it. Moreover, the voting authori-
ties must be able to verify that the voter did not open the
unvoted ballots.
To achieve this, we can make use of tamper-evident en-
velopes, or opaque sealed bags (which a voter cannot open
undetected). A formal model for tamper-evident seals was
previously developed by the authors [15]. The “distinguishable-
envelope” model in [15] captures our requirements precisely.
In order to preserve privacy and receipt-
freeness, the voter must be able to perform some actions pri-
vately. The actions the voter performs in the voting booth
are opening sealed ballots, reading their contents and eras-
ing part of the ballot.
Voting Booth.
Untappable Channels.
In order to guarantee everlast-
ing privacy, communication between the voting authorities
is assumed to take place using untappable private channels.
This is a fairly reasonable assumption: the voting authori-
ties can be physically close and connected by direct physical
channels. Note that if this assumption is not satisﬁed, the
protocol is still computationally private (but is no longer
UC-secure or information-theoretically private).
Public Bulletin Board.
The public bulletin board is a
common assumption in universally-veriﬁable voting proto-
cols. This is usually modeled as a broadcast channel, or as
append-only storage with read-access for all parties. A pos-
sible implementation is a web-site that is constantly moni-
tored by multiple veriﬁers to ensure that nothing is erased
or modiﬁed.
Random Beacon.
The random beacon, originally in-
troduced by Rabin [20], is a source of independently dis-
tributed, uniformly random strings. The main assumption
about the beacon is that it is unpredictable. In practice, the
beacon can be implemented in many ways, such as by some
physical source believed to be unpredictable (e.g., cosmic
radiation, weather, etc.), or by a distributed computation
with multiple veriﬁers.
We use the beacon for choosing the public-key of our com-
mitment scheme, and to replace the veriﬁer in zero knowl-
edge proofs. For the zero-knowledge proofs, we can replace
the beacon assumption by a random oracle (this is the Fiat-
Shamir heuristic): the entire protocol transcript so far is
taken as the index in the random oracle that is used as the
next bit to be sent by the beacon.
3.2 Cryptographic Assumptions
Our protocol is based on two cryptographic primitives:
perfectly-hiding homomorphic commitment and homomor-
phic encryption. The homomorphic commitment requires
some special properties.
Homomorphic Commitment. A homomorphic commit-
ment scheme consists of a tuple of algorithms: K, C, PK ,
and VK. K : {0, 1}(cid:2) × {0, 1}(cid:2) (cid:3)→ K accepts a public random
bit-string and a private auxiliary and generates a commit-
ment public key cpk ∈ K. C is the commitment function,
parametrized by the public key, mapping from a message
group (M, +) and a randomizer group (R, +) to the space
of commitments (C,·). To reduce clutter, we omit the key
parameter when it is obvious from context (i.e., we write
C (m, r) instead of Ccpk (m, r)).
PK and VK are a “prover” and “veriﬁer” for the key gen-
eration: these are both interactive machines. The prover
receives the same input as the key generator, while the ver-
iﬁer receives only the public random string and the public
key. To allow the veriﬁcation to be performed publicly (us-
ing a random beacon), we require that all of the messages
sent by VK to PK are uniformly distributed random strings.
∗
K (corresponding to an adversarial
key-generating algorithm and prover), when cpk ← K
(rK ),
rK ∈R {0, 1}(cid:2) is chosen uniformly at random then, with all
but negligible probability (the probability is over the choice
∗
of rK and the random coins of K
K and VK ), either the
∗
output of VK(rK, cpk) when interacting with P
K is 0 (i.e.,
the veriﬁcation of the public-key fails) or the following prop-
erties must hold:
For any PPTs K
, P
, P
∗
∗
∗
1. Perfectly Hiding: For any m1, m2 ∈ M, the random
variables C (m1, r) and C (m2, r) must be identically
distributed when r is taken uniformly at random from
R. (Note that we can replace this property with sta-
tistically hiding commitment, but for simplicity of the
proof we require the stronger notion).
2. Computationally Binding: For any PPT A (with ac-
cess to the private coins of K
), the probability that
A (cpk) can output (m1, r1) (cid:7)= (m2, r2) ∈ M × R such
that Ccpk (m1, r1) = Ccpk (m2, r2) must be negligible.
(m1, r1) , (m2, r2) ∈ M × R, and all but a negligible
fraction of keys,
C (m1, r1) · C (m2, r2) = C (m1 + m2, r1 + r2).
3. Homomorphic in both M and R: for all
∗
(cid:3)
(cid:3)
(cid:3)
(cid:3)
: {0, 1}(cid:2)
: {0, 1}(cid:2)
(cid:2) (cid:3)→ {0, 1}(cid:2), C
Simulated Equivocability. For achieving UC security, we
require the commitment scheme to have two additional al-
(cid:2) ×C ×M (cid:3)→ R,
gorithms K
such that the output of K
is uniformly random, and for
every l ∈ {0, 1}(cid:2) and m ∈ M and c ∈ C,
CK(K(cid:2)(l)) (m, C
(l, c, m)) = c (i.e., it is possible to generate
a public-key that is identical to a normal public key, such
that it is possible to open every commitment to any value).
Homomorphic Public-Key Encryption. The second cryp-
tographic building block we use is a homomorphic public-
key encryption scheme. We actually need two encryption
schemes, one whose message space is M and the other whose
”
”
“
KG(M), E(M), D(M)
message space is R (where M and R are as deﬁned for the
commitment scheme). The schemes are speciﬁed by the al-
“
gorithm triplets
KG(R), E(R), D(R)
, where KG is the key-generation al-
gorithm, E(X ) : X × T (cid:3)→ E (X ) the encryption algorithm
and D(X ) : E (X ) (cid:3)→ X the decryption algorithm. We require
the encryption schemes to be semantically secure and ho-
for every x1, x1 ∈ X
momorphic in their message spaces:
(cid:3) ∈ T such that
and any r1, r2 ∈ T , there must exist r
E(X ) (x1, r1) · E(X ) (x2, r2) = E(X ) (x1 + x2, r
and
).
(cid:3)
We do not require the encryption scheme to be homo-
morphic in its randomness, but we do require, for every
is uniformly distributed in T when r2 is
x1, r1, x2, that r
chosen uniformly.
(cid:3)
To clarify the presentation, below we omit the randomness
and the superscript for the encryption schemes where it can
be understood from the context (e.g., we write E (m) to
describe an encryption of m).