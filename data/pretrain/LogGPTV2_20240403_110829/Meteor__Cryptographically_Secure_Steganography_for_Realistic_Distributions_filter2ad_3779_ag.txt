[28] N. J. Hopper, J. Langford, and L. von Ahn, â€œProvably secure steganography,â€ in
CRYPTO 2002 (M. Yung, ed.), vol. 2442 of LNCS, pp. 77â€“92, Springer, Heidelberg,
Aug. 2002.
[29] L. von Ahn and N. J. Hopper, â€œPublic-key steganography,â€ in EUROCRYPT 2004
(C. Cachin and J. Camenisch, eds.), vol. 3027 of LNCS, pp. 323â€“341, Springer,
Heidelberg, May 2004.
[30] M. Backes and C. Cachin, â€œPublic-key steganography with active attacks,â€ in
TCC 2005 (J. Kilian, ed.), vol. 3378 of LNCS, pp. 210â€“226, Springer, Heidelberg,
Feb. 2005.
[31] N. Dedic, G. Itkis, L. Reyzin, and S. Russell, â€œUpper and lower bounds on black-
box steganography,â€ in TCC 2005 (J. Kilian, ed.), vol. 3378 of LNCS, pp. 227â€“244,
Springer, Heidelberg, Feb. 2005.
[32] C. Grothoff, K. Grothoff, L. Alkhutova, R. Stutsman, and M. Atallah, â€œTranslation-
based steganography,â€ in International Workshop on Information Hiding, pp. 219â€“
233, Springer, 2005.
[33] M. Shirali-Shahreza and M. H. Shirali-Shahreza, â€œText steganography in sms,â€
2007 International Conference on Convergence Information Technology (ICCIT
2007), pp. 2260â€“2265, 2007.
[34] Z. Yu, L. Huang, Z. Chen, L. Li, X. Zhao, and Y. Zhu, â€œSteganalysis of synonym-
substitution based natural language watermarking,â€ 2009.
[35] C.-Y. Chang and S. Clark, â€œLinguistic steganography using automatically gener-
ated paraphrases,â€ in Human Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for Computational Linguistics,
HLT â€™10, (Stroudsburg, PA, USA), pp. 591â€“599, Association for Computational
Linguistics, 2010.
[36] C.-Y. Chang and S. Clark, â€œPractical linguistic steganography using contex-
tual synonym substitution and a novel vertex coding method,â€ Computational
Linguistics, vol. 40, p. 403â€“448, Jun 2014.
[37] T. Fang, M. Jaggi, and K. Argyraki, â€œGenerating steganographic text with lstms,â€
Proceedings of ACL 2017, Student Research Workshop, 2017.
[38] D. Volkhonskiy, I. Nazarov, B. Borisenko, and E. Burnaev, â€œSteganographic
generative adversarial networks,â€ 2017.
[39] Z. Yang, S. Jin, Y. Huang, Y. Zhang, and H. Li, â€œAutomatically generate stegano-
graphic text based on markov model and huffman coding,â€ 2018.
[40] L. Xiang, â€œReversible natural language watermarking using synonym substitu-
tion and arithmetic coding,â€ 2018.
[41] Z. Yang, X. Guo, Z. Chen, Y. Huang, and Y. Zhang, â€œRnn-stega: Linguistic
steganography based on recurrent neural networks,â€ IEEE Transactions on Infor-
mation Forensics and Security, vol. 14, pp. 1280â€“1295, May 2019.
[42] S.-Y. HUANG and P.-S. Huang, â€œA homophone-based chinese text steganography
scheme for chatting applications.,â€ Journal of Information Science & Engineering,
vol. 35, no. 4, 2019.
[43] F. Dai and Z. Cai, â€œTowards near-imperceptible steganographic text,â€ Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.
[44] Z. M. Ziegler, Y. Deng, and A. M. Rush, â€œNeural linguistic steganography,â€ 2019.
[45] Z. Yang, Y. Huang, and Y.-J. Zhang, â€œA fast and efficient text steganalysis method,â€
IEEE Signal Processing Letters, vol. 26, pp. 627â€“631, 2019.
[46] Z. Yang, K. Wang, J. Li, Y. Huang, and Y. Zhang, â€œTs-rnn: Text steganalysis based
on recurrent neural networks,â€ IEEE Signal Processing Letters, p. 1â€“1, 2019.
[47] Z. Yang, N. Wei, J. Sheng, Y. Huang, and Y.-J. Zhang, â€œTs-cnn: Text steganalysis
from semantic space based on convolutional neural network,â€ 2018.
[48] A. Wilson, P. Blunsom, and A. Ker, â€œDetection of steganographic techniques
on twitter,â€ Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, 2015.
[49] J. Kodovsky, J. Fridrich, and V. Holub, â€œEnsemble classifiers for steganalysis of
digital media,â€ IEEE Transactions on Information Forensics and Security, vol. 7,
pp. 432â€“444, April 2012.
[50] P. Meng, L. Huang, Z. Chen, W. Yang, and D. Li, â€œLinguistic steganography
detection based on perplexity,â€ in 2008 International Conference on MultiMedia
and Information Technology, pp. 217â€“220, Dec 2008.
[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, â€œLanguage
models are unsupervised multitask learners,â€ OpenAI Blog, vol. 1, no. 8, 2019.
[52] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakan-
tan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse,
M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCan-
dlish, A. Radford, I. Sutskever, and D. Amodei, â€œLanguage models are few-shot
learners,â€ 2020.
[53] O. Blog, â€œBetter language models and their implications.â€ Available at https:
//openai.com/blog/better-language-models/, February 2019.
[54] N. J. Hopper, â€œToward a theory of steganography,â€ tech. rep., CARNEGIE-
MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 2004.
Session 5C: Messaging and Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1542[55] A. Karpathy, â€œThe unreasonable effectiveness of recurrent neural networks.â€
https://karpathy.github.io/2015/05/21/rnn-effectiveness/, May 2015.
[56] A. van Dalen, â€œThe algorithms behind the headlines,â€ Journalism Practice, vol. 6,
[86] A. Sen, S. Alfeld, X. Zhang, A. Vartanian, Y. Ma, and X. Zhu, â€œTraining set
camouflage,â€ Decision and Game Theory for Security, p. 59â€“79, 2018.
[87] M. Chaumont, â€œDeep learning in steganography and steganalysis from 2015 to
[105] Z. Durumeric, E. Wustrow, and J. A. Halderman, â€œZmap: Fast internet-wide
scanning and its security applications,â€ in Presented as part of the 22nd USENIX
Security Symposium USENIX Security 13), pp. 605â€“620, 2013.
[106] S. Cutler, â€œProject 25499 ipv4 http scans.â€ https://scans.io/study/mi.
[107] HuggingFace,
â€œhuggingface/swift-coreml-transformers.â€
https://github.com/huggingface/swift-coreml-transformers, Oct 2019.
[108] T. Simonite, â€œAppleâ€™s latest iphones are packed with ai smarts.â€ https://www.
wired.com/story/apples-latest-iphones-packed-with-ai-smarts/.
[109] S. J. Oh, B. Schiele, and M. Fritz, â€œTowards reverse-engineering black-box neu-
ral networks,â€ in Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning, pp. 121â€“144, Springer, 2019.
[110] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes, â€œMl-leaks:
Model and data independent membership inference attacks and defenses on
machine learning models,â€ arXiv preprint arXiv:1806.01246, 2018.
[111] M. Juuti, S. Szyller, S. Marchal, and N. Asokan, â€œPrada: protecting against dnn
model stealing attacks,â€ in 2019 IEEE European Symposium on Security and
Privacy (EuroS&P), pp. 512â€“527, IEEE, 2019.
A EFFICIENCY OF METEOR
We now show that the asymptotic expected throughput of Me-
teor is proportional to the entropy in the communication chan-
nel. Recall that the entropy in a distribution P is computed as
ğ‘–âˆˆ|P| ğ‘ğ‘– log2(ğ‘ğ‘–), where ğ‘ğ‘– is the probability of the ğ‘–th possible
âˆ’
2018,â€ 2019.
[88] P. Wu, Y. Yang, and X. Li, â€œStegnet: Mega image steganography capacity with
deep convolutional network,â€ Future Internet, vol. 10, p. 54, Jun 2018.
[89] S. Hochreiter and J. Schmidhuber, â€œLong short-term memory,â€ Neural computa-
tion, vol. 9, pp. 1735â€“80, 12 1997.
[90] M. Costa-Jussa and J. Fonollosa, â€œCharacter-based neural machine translation,â€
in Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics, pp. 357â€“361, 03 2016.
[91] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, â€œCharacter-aware neural language
models,â€ in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,
AAAIâ€™16, pp. 2741â€“2749, AAAI Press, 2016.
[92] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, u. Kaiser,
and I. Polosukhin, â€œAttention is all you need,â€ in Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, NIPSâ€™17, (Red Hook,
NY, USA), p. 6000â€“6010, Curran Associates Inc., 2017.
[93] R. Aharoni, M. Koppel, and Y. Goldberg, â€œAutomatic detection of machine
translated text and translation quality estimation,â€ in Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), vol. 2, pp. 289â€“295, 2014.
[94] A. Bakhtin, S. Gross, M. Ott, Y. Deng, M. Ranzato, and A. Szlam, â€œReal or fake?
learning to discriminate machine from human generated text,â€ 2019.
[95] S. Gehrmann, H. Strobelt, and A. M. Rush, â€œGltr: Statistical detection and visual-
ization of generated text,â€ 2019.
[96] Y. Zhu, S. Lu, L. Zheng, J. Guo, W. Zhang, J. Wang, and Y. Yu, â€œTexygen: A
benchmarking platform for text generation models,â€ in The 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval,
pp. 1097â€“1100, ACM, 2018.
[97] D. Dachman-Soled, G. Fuchsbauer, P. Mohassel, and A. Oâ€™Neill, â€œEnhanced
chosen-ciphertext security and applications,â€ in PKC 2014 (H. Krawczyk, ed.),
vol. 8383 of LNCS, pp. 329â€“344, Springer, Heidelberg, Mar. 2014.
[98] C. Peikert and B. Waters, â€œLossy trapdoor functions and their applications,â€ in
40th ACM STOC (R. E. Ladner and C. Dwork, eds.), pp. 187â€“196, ACM Press,
May 2008.
[99] M. Blum and S. Micali, â€œHow to generate cryptographically strong sequences of
pseudo random bits,â€ in 23rd FOCS, pp. 112â€“117, IEEE Computer Society Press,
Nov. 1982.
[100] S. Ruhault, â€œSoK: Security models for pseudo-random number generators,â€ IACR
Trans. Symm. Cryptol., vol. 2017, no. 1, pp. 506â€“544, 2017.
[101] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmai-
son, L. Antiga, and A. Lerer, â€œAutomatic differentiation in pytorch,â€ in NIPS-W,
2017.
[102] E. Barker and J. Kelsey, â€œNist special publication 800-90a revision 1 recommenda-
tion for random number generation using deterministic random bit generators,â€
2015.
â€œspro/char-rnn.pytorch.â€
https://github.com/spro/char-
[103] S.
Robertson,
rnn.pytorch, Dec 2017.
â€œThe
Hutter,
[104] M.
http://prize.hutter1.net/, 2006.
human
knowledge
compression
contest.â€
no. 5-6, pp. 648â€“658, 2012.
[57] A. Graefe, â€œGuide to automated journalism,â€ 2016.
[58] D. Rockmore, â€œWhat happens when machines learn to write poetry,â€ Jan 2020.
[59] A. Mayne, â€œAi|writer.â€ https://www.aiwriter.app/.
[60] A. Kind, â€œTalk to transformer.â€ https://app.inferkit.com/demo.
[61] â€œAi writer.â€ http://ai-writer.com/.
[62] J. Y. Koh. https://modelzoo.co/.
[63] J. ZÃ¶llner, H. Federrath, H. Klimant, A. Pfitzmann, R. Piotraschke, A. Westfeld,
G. Wicke, and G. Wolf, â€œModeling the security of steganographic systems,â€ in
International Workshop on Information Hiding, pp. 344â€“354, Springer, 1998.
[64] T. Mittelholzer, â€œAn information-theoretic approach to steganography and wa-
termarking,â€ in International Workshop on Information Hiding, pp. 1â€“16, Springer,
1999.
[65] K. Solanki, K. Sullivan, U. Madhow, B. S. Manjunath, and S. Chandrasekaran,
â€œProvably secure steganography: Achieving zero k-l divergence using statistical
restoration,â€ in 2006 International Conference on Image Processing, pp. 125â€“128,
Oct 2006.
[66] A. Sarkar, K. Solanki, and B. S. Manjunath, â€œSecure steganography: Statistical
restoration in the transform domain with best integer perturbations to pixel
values,â€ in IEEE International Conference on Image Processing (ICIP), Sep 2007.
[67] K. Sullivan, K. Solanki, B. S. Manjunath, U. Madhow, and S. Chandrasekaran,
â€œDetermining achievable rates for secure, zero divergence, steganography,â€ in
ICIP, pp. 121â€“124, IEEE, 2006.
[68] L. Reyzin and S. Russell, â€œSimple stateless steganography.â€ Cryptology ePrint
Archive, Report 2003/093, 2003. http://eprint.iacr.org/2003/093.
[69] T. V. Le, â€œEfficient provably secure public key steganography.â€ Cryptology ePrint
Archive, Report 2003/156, 2003. http://eprint.iacr.org/2003/156.
[70] T. V. Le and K. Kurosawa, â€œEfficient public key steganography secure against
adaptively chosen stegotext attacks.â€ Cryptology ePrint Archive, Report
2003/244, 2003. http://eprint.iacr.org/2003/244.
[71] T. Ruffing, J. Schneider, and A. Kate, â€œIdentity-based steganography and its
applications to censorship resistance,â€ in ACM CCS 2013 (A.-R. Sadeghi, V. D.
Gligor, and M. Yung, eds.), pp. 1461â€“1464, ACM Press, Nov. 2013.
[72] S. Berndt and M. Liskiewicz, â€œOn the gold standard for security of universal
steganography,â€ in EUROCRYPT 2018, Part I (J. B. Nielsen and V. Rijmen, eds.),
vol. 10820 of LNCS, pp. 29â€“60, Springer, Heidelberg, Apr. / May 2018.
[73] T. Horel, S. Park, S. Richelson, and V. Vaikuntanathan, â€œHow to subvert back-
doored encryption: Security against adversaries that decrypt all ciphertexts,â€ in
ITCS 2019 (A. Blum, ed.), vol. 124, pp. 42:1â€“42:20, LIPIcs, Jan. 2019.
[74] T. Agrikola, G. Couteau, Y. Ishai, S. Jarecki, and A. Sahai, â€œOn pseudorandom
encodings,â€ in TCC 2020, Part III (R. Pass and K. Pietrzak, eds.), vol. 12552 of
LNCS, pp. 639â€“669, Springer, Heidelberg, Nov. 2020.
[75] A. Lysyanskaya and M. Meyerovich, â€œProvably secure steganography with
imperfect sampling,â€ in PKC 2006 (M. Yung, Y. Dodis, A. Kiayias, and T. Malkin,
eds.), vol. 3958 of LNCS, pp. 123â€“139, Springer, Heidelberg, Apr. 2006.
[76] D. Fifield, C. Lan, R. Hynes, P. Wegmann, and V. Paxson, â€œBlocking-resistant
communication through domain fronting,â€ Proceedings on Privacy Enhancing
Technologies, vol. 2015, no. 2, pp. 46â€“64, 2015.
[77] S. Frolov and E. Wustrow, â€œThe use of tls in censorship circumvention.,â€ in NDSS,
[78] S. Frolov, F. Douglas, W. Scott, A. McDonald, B. VanderSloot, R. Hynes, A. Kruger,
M. Kallitsis, D. G. Robinson, S. Schultze, et al., â€œAn isp-scale deployment of
tapdance,â€ in 7th {USENIX} Workshop on Free and Open Communications on the
Internet ({FOCI} 17), 2017.
[79] D. Luchaup, K. P. Dyer, S. Jha, T. Ristenpart, and T. Shrimpton, â€œLibfte: A toolkit
for constructing practical, format-abiding encryption schemes,â€ in 23rd USENIX
Security Symposium (USENIX Security 14), (San Diego, CA), pp. 877â€“891, USENIX
Association, 2014.
[80] K. P. Dyer, S. E. Coull, T. Ristenpart, and T. Shrimpton, â€œProtocol misidentifica-
tion made easy with format-transforming encryption,â€ in ACM CCS 2013 (A.-R.
Sadeghi, V. D. Gligor, and M. Yung, eds.), pp. 61â€“72, ACM Press, Nov. 2013.
[81] K. P. Dyer, S. E. Coull, and T. Shrimpton, â€œMarionette: A programmable network
traffic obfuscation system,â€ in 24th USENIX Security Symposium (USENIX Security
15), (Washington, D.C.), pp. 367â€“382, USENIX Association, 2015.
[82] J. Oakley, L. Yu, X. Zhong, G. K. Venayagamoorthy, and R. Brooks, â€œProtocol
proxy: An fte-based covert channel,â€ Computers & Security, vol. 92, p. 101777,
May 2020.
[83] S. Baluja, â€œHiding images in plain sight: Deep steganography,â€ in Neural Infor-
mation Processing Systems, 2017.
[84] D. Hu, L. Wang, W. Jiang, S. Zheng, and B. Li, â€œA novel image steganography
method via deep convolutional generative adversarial networks,â€ IEEE Access,
vol. 6, pp. 38303â€“38314, 2018.
[85] Harveyslash,
â€œharveyslash/deep-steganography.â€
https://github.com/harveyslash/Deep-Steganography, Apr 2018.
2019.
Session 5C: Messaging and Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1543be computed as
outcome of P. Similarly, the expected throughput of Meteor can
ğ‘–âˆˆ|P| ğ‘ğ‘–Exp(ğ‘ğ‘–), where Exp(Â·) is the expected
number of shared prefix bits for some continuous interval of size
ğ‘ğ‘– . Thus, the remaining task is to compute a concrete bound on
Exp(Â·).
We will make the simplifying assumption that the start of an
interval ğ‘ğ‘– is placed randomly between [0, 2ğ›½+1). Note that interval
ğ‘– will never start after 2ğ›½+1 âˆ’ ğ‘ğ‘– in practice, so we the number of
prefix bits in this case to be 0, so this simplification will lead to
an expected throughput strictly less than the true value. Addition-
ally, the starting locations for each interval are not independent in
practice, as they each depend on ğ‘ ğ‘—â‰ ğ‘–. However, this independence
assumption also leads to equal or lower expected throughput, as
the starting point for larger intervals will actually be more biased
towards the middle of the distribution, where Exp(Â·) will be lower,
and smaller distributions will be biased to start near the edges of
the distribution, where Exp(Â·) will be higher.
4 âˆ’ ğœ–,
for some small ğœ– (see Figure 7). If ğ‘– starts between [0, ğœ–), then it is
contained completely before the prefix 01 begins, and thus would
transmit 2 bits. The following ğ‘ğ‘– starting points all transmit only 1