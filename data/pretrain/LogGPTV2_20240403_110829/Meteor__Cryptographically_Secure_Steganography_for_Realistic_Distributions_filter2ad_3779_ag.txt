[28] N. J. Hopper, J. Langford, and L. von Ahn, “Provably secure steganography,” in
CRYPTO 2002 (M. Yung, ed.), vol. 2442 of LNCS, pp. 77–92, Springer, Heidelberg,
Aug. 2002.
[29] L. von Ahn and N. J. Hopper, “Public-key steganography,” in EUROCRYPT 2004
(C. Cachin and J. Camenisch, eds.), vol. 3027 of LNCS, pp. 323–341, Springer,
Heidelberg, May 2004.
[30] M. Backes and C. Cachin, “Public-key steganography with active attacks,” in
TCC 2005 (J. Kilian, ed.), vol. 3378 of LNCS, pp. 210–226, Springer, Heidelberg,
Feb. 2005.
[31] N. Dedic, G. Itkis, L. Reyzin, and S. Russell, “Upper and lower bounds on black-
box steganography,” in TCC 2005 (J. Kilian, ed.), vol. 3378 of LNCS, pp. 227–244,
Springer, Heidelberg, Feb. 2005.
[32] C. Grothoff, K. Grothoff, L. Alkhutova, R. Stutsman, and M. Atallah, “Translation-
based steganography,” in International Workshop on Information Hiding, pp. 219–
233, Springer, 2005.
[33] M. Shirali-Shahreza and M. H. Shirali-Shahreza, “Text steganography in sms,”
2007 International Conference on Convergence Information Technology (ICCIT
2007), pp. 2260–2265, 2007.
[34] Z. Yu, L. Huang, Z. Chen, L. Li, X. Zhao, and Y. Zhu, “Steganalysis of synonym-
substitution based natural language watermarking,” 2009.
[35] C.-Y. Chang and S. Clark, “Linguistic steganography using automatically gener-
ated paraphrases,” in Human Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for Computational Linguistics,
HLT ’10, (Stroudsburg, PA, USA), pp. 591–599, Association for Computational
Linguistics, 2010.
[36] C.-Y. Chang and S. Clark, “Practical linguistic steganography using contex-
tual synonym substitution and a novel vertex coding method,” Computational
Linguistics, vol. 40, p. 403–448, Jun 2014.
[37] T. Fang, M. Jaggi, and K. Argyraki, “Generating steganographic text with lstms,”
Proceedings of ACL 2017, Student Research Workshop, 2017.
[38] D. Volkhonskiy, I. Nazarov, B. Borisenko, and E. Burnaev, “Steganographic
generative adversarial networks,” 2017.
[39] Z. Yang, S. Jin, Y. Huang, Y. Zhang, and H. Li, “Automatically generate stegano-
graphic text based on markov model and huffman coding,” 2018.
[40] L. Xiang, “Reversible natural language watermarking using synonym substitu-
tion and arithmetic coding,” 2018.
[41] Z. Yang, X. Guo, Z. Chen, Y. Huang, and Y. Zhang, “Rnn-stega: Linguistic
steganography based on recurrent neural networks,” IEEE Transactions on Infor-
mation Forensics and Security, vol. 14, pp. 1280–1295, May 2019.
[42] S.-Y. HUANG and P.-S. Huang, “A homophone-based chinese text steganography
scheme for chatting applications.,” Journal of Information Science & Engineering,
vol. 35, no. 4, 2019.
[43] F. Dai and Z. Cai, “Towards near-imperceptible steganographic text,” Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.
[44] Z. M. Ziegler, Y. Deng, and A. M. Rush, “Neural linguistic steganography,” 2019.
[45] Z. Yang, Y. Huang, and Y.-J. Zhang, “A fast and efficient text steganalysis method,”
IEEE Signal Processing Letters, vol. 26, pp. 627–631, 2019.
[46] Z. Yang, K. Wang, J. Li, Y. Huang, and Y. Zhang, “Ts-rnn: Text steganalysis based
on recurrent neural networks,” IEEE Signal Processing Letters, p. 1–1, 2019.
[47] Z. Yang, N. Wei, J. Sheng, Y. Huang, and Y.-J. Zhang, “Ts-cnn: Text steganalysis
from semantic space based on convolutional neural network,” 2018.
[48] A. Wilson, P. Blunsom, and A. Ker, “Detection of steganographic techniques
on twitter,” Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, 2015.
[49] J. Kodovsky, J. Fridrich, and V. Holub, “Ensemble classifiers for steganalysis of
digital media,” IEEE Transactions on Information Forensics and Security, vol. 7,
pp. 432–444, April 2012.
[50] P. Meng, L. Huang, Z. Chen, W. Yang, and D. Li, “Linguistic steganography
detection based on perplexity,” in 2008 International Conference on MultiMedia
and Information Technology, pp. 217–220, Dec 2008.
[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
models are unsupervised multitask learners,” OpenAI Blog, vol. 1, no. 8, 2019.
[52] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakan-
tan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse,
M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCan-
dlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot
learners,” 2020.
[53] O. Blog, “Better language models and their implications.” Available at https:
//openai.com/blog/better-language-models/, February 2019.
[54] N. J. Hopper, “Toward a theory of steganography,” tech. rep., CARNEGIE-
MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 2004.
Session 5C: Messaging and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1542[55] A. Karpathy, “The unreasonable effectiveness of recurrent neural networks.”
https://karpathy.github.io/2015/05/21/rnn-effectiveness/, May 2015.
[56] A. van Dalen, “The algorithms behind the headlines,” Journalism Practice, vol. 6,
[86] A. Sen, S. Alfeld, X. Zhang, A. Vartanian, Y. Ma, and X. Zhu, “Training set
camouflage,” Decision and Game Theory for Security, p. 59–79, 2018.
[87] M. Chaumont, “Deep learning in steganography and steganalysis from 2015 to
[105] Z. Durumeric, E. Wustrow, and J. A. Halderman, “Zmap: Fast internet-wide
scanning and its security applications,” in Presented as part of the 22nd USENIX
Security Symposium USENIX Security 13), pp. 605–620, 2013.
[106] S. Cutler, “Project 25499 ipv4 http scans.” https://scans.io/study/mi.
[107] HuggingFace,
“huggingface/swift-coreml-transformers.”
https://github.com/huggingface/swift-coreml-transformers, Oct 2019.
[108] T. Simonite, “Apple’s latest iphones are packed with ai smarts.” https://www.
wired.com/story/apples-latest-iphones-packed-with-ai-smarts/.
[109] S. J. Oh, B. Schiele, and M. Fritz, “Towards reverse-engineering black-box neu-
ral networks,” in Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning, pp. 121–144, Springer, 2019.
[110] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes, “Ml-leaks:
Model and data independent membership inference attacks and defenses on
machine learning models,” arXiv preprint arXiv:1806.01246, 2018.
[111] M. Juuti, S. Szyller, S. Marchal, and N. Asokan, “Prada: protecting against dnn
model stealing attacks,” in 2019 IEEE European Symposium on Security and
Privacy (EuroS&P), pp. 512–527, IEEE, 2019.
A EFFICIENCY OF METEOR
We now show that the asymptotic expected throughput of Me-
teor is proportional to the entropy in the communication chan-
nel. Recall that the entropy in a distribution P is computed as
𝑖∈|P| 𝑝𝑖 log2(𝑝𝑖), where 𝑝𝑖 is the probability of the 𝑖th possible
−
2018,” 2019.
[88] P. Wu, Y. Yang, and X. Li, “Stegnet: Mega image steganography capacity with
deep convolutional network,” Future Internet, vol. 10, p. 54, Jun 2018.
[89] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computa-
tion, vol. 9, pp. 1735–80, 12 1997.
[90] M. Costa-Jussa and J. Fonollosa, “Character-based neural machine translation,”
in Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics, pp. 357–361, 03 2016.
[91] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, “Character-aware neural language
models,” in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,
AAAI’16, pp. 2741–2749, AAAI Press, 2016.
[92] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, u. Kaiser,
and I. Polosukhin, “Attention is all you need,” in Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, NIPS’17, (Red Hook,
NY, USA), p. 6000–6010, Curran Associates Inc., 2017.
[93] R. Aharoni, M. Koppel, and Y. Goldberg, “Automatic detection of machine
translated text and translation quality estimation,” in Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), vol. 2, pp. 289–295, 2014.
[94] A. Bakhtin, S. Gross, M. Ott, Y. Deng, M. Ranzato, and A. Szlam, “Real or fake?
learning to discriminate machine from human generated text,” 2019.
[95] S. Gehrmann, H. Strobelt, and A. M. Rush, “Gltr: Statistical detection and visual-
ization of generated text,” 2019.
[96] Y. Zhu, S. Lu, L. Zheng, J. Guo, W. Zhang, J. Wang, and Y. Yu, “Texygen: A
benchmarking platform for text generation models,” in The 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval,
pp. 1097–1100, ACM, 2018.
[97] D. Dachman-Soled, G. Fuchsbauer, P. Mohassel, and A. O’Neill, “Enhanced
chosen-ciphertext security and applications,” in PKC 2014 (H. Krawczyk, ed.),
vol. 8383 of LNCS, pp. 329–344, Springer, Heidelberg, Mar. 2014.
[98] C. Peikert and B. Waters, “Lossy trapdoor functions and their applications,” in
40th ACM STOC (R. E. Ladner and C. Dwork, eds.), pp. 187–196, ACM Press,
May 2008.
[99] M. Blum and S. Micali, “How to generate cryptographically strong sequences of
pseudo random bits,” in 23rd FOCS, pp. 112–117, IEEE Computer Society Press,
Nov. 1982.
[100] S. Ruhault, “SoK: Security models for pseudo-random number generators,” IACR
Trans. Symm. Cryptol., vol. 2017, no. 1, pp. 506–544, 2017.
[101] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmai-
son, L. Antiga, and A. Lerer, “Automatic differentiation in pytorch,” in NIPS-W,
2017.
[102] E. Barker and J. Kelsey, “Nist special publication 800-90a revision 1 recommenda-
tion for random number generation using deterministic random bit generators,”
2015.
“spro/char-rnn.pytorch.”
https://github.com/spro/char-
[103] S.
Robertson,
rnn.pytorch, Dec 2017.
“The
Hutter,
[104] M.
http://prize.hutter1.net/, 2006.
human
knowledge
compression
contest.”
no. 5-6, pp. 648–658, 2012.
[57] A. Graefe, “Guide to automated journalism,” 2016.
[58] D. Rockmore, “What happens when machines learn to write poetry,” Jan 2020.
[59] A. Mayne, “Ai|writer.” https://www.aiwriter.app/.
[60] A. Kind, “Talk to transformer.” https://app.inferkit.com/demo.
[61] “Ai writer.” http://ai-writer.com/.
[62] J. Y. Koh. https://modelzoo.co/.
[63] J. Zöllner, H. Federrath, H. Klimant, A. Pfitzmann, R. Piotraschke, A. Westfeld,
G. Wicke, and G. Wolf, “Modeling the security of steganographic systems,” in
International Workshop on Information Hiding, pp. 344–354, Springer, 1998.
[64] T. Mittelholzer, “An information-theoretic approach to steganography and wa-
termarking,” in International Workshop on Information Hiding, pp. 1–16, Springer,
1999.
[65] K. Solanki, K. Sullivan, U. Madhow, B. S. Manjunath, and S. Chandrasekaran,
“Provably secure steganography: Achieving zero k-l divergence using statistical
restoration,” in 2006 International Conference on Image Processing, pp. 125–128,
Oct 2006.
[66] A. Sarkar, K. Solanki, and B. S. Manjunath, “Secure steganography: Statistical
restoration in the transform domain with best integer perturbations to pixel
values,” in IEEE International Conference on Image Processing (ICIP), Sep 2007.
[67] K. Sullivan, K. Solanki, B. S. Manjunath, U. Madhow, and S. Chandrasekaran,
“Determining achievable rates for secure, zero divergence, steganography,” in
ICIP, pp. 121–124, IEEE, 2006.
[68] L. Reyzin and S. Russell, “Simple stateless steganography.” Cryptology ePrint
Archive, Report 2003/093, 2003. http://eprint.iacr.org/2003/093.
[69] T. V. Le, “Efficient provably secure public key steganography.” Cryptology ePrint
Archive, Report 2003/156, 2003. http://eprint.iacr.org/2003/156.
[70] T. V. Le and K. Kurosawa, “Efficient public key steganography secure against
adaptively chosen stegotext attacks.” Cryptology ePrint Archive, Report
2003/244, 2003. http://eprint.iacr.org/2003/244.
[71] T. Ruffing, J. Schneider, and A. Kate, “Identity-based steganography and its
applications to censorship resistance,” in ACM CCS 2013 (A.-R. Sadeghi, V. D.
Gligor, and M. Yung, eds.), pp. 1461–1464, ACM Press, Nov. 2013.
[72] S. Berndt and M. Liskiewicz, “On the gold standard for security of universal
steganography,” in EUROCRYPT 2018, Part I (J. B. Nielsen and V. Rijmen, eds.),
vol. 10820 of LNCS, pp. 29–60, Springer, Heidelberg, Apr. / May 2018.
[73] T. Horel, S. Park, S. Richelson, and V. Vaikuntanathan, “How to subvert back-
doored encryption: Security against adversaries that decrypt all ciphertexts,” in
ITCS 2019 (A. Blum, ed.), vol. 124, pp. 42:1–42:20, LIPIcs, Jan. 2019.
[74] T. Agrikola, G. Couteau, Y. Ishai, S. Jarecki, and A. Sahai, “On pseudorandom
encodings,” in TCC 2020, Part III (R. Pass and K. Pietrzak, eds.), vol. 12552 of
LNCS, pp. 639–669, Springer, Heidelberg, Nov. 2020.
[75] A. Lysyanskaya and M. Meyerovich, “Provably secure steganography with
imperfect sampling,” in PKC 2006 (M. Yung, Y. Dodis, A. Kiayias, and T. Malkin,
eds.), vol. 3958 of LNCS, pp. 123–139, Springer, Heidelberg, Apr. 2006.
[76] D. Fifield, C. Lan, R. Hynes, P. Wegmann, and V. Paxson, “Blocking-resistant
communication through domain fronting,” Proceedings on Privacy Enhancing
Technologies, vol. 2015, no. 2, pp. 46–64, 2015.
[77] S. Frolov and E. Wustrow, “The use of tls in censorship circumvention.,” in NDSS,
[78] S. Frolov, F. Douglas, W. Scott, A. McDonald, B. VanderSloot, R. Hynes, A. Kruger,
M. Kallitsis, D. G. Robinson, S. Schultze, et al., “An isp-scale deployment of
tapdance,” in 7th {USENIX} Workshop on Free and Open Communications on the
Internet ({FOCI} 17), 2017.
[79] D. Luchaup, K. P. Dyer, S. Jha, T. Ristenpart, and T. Shrimpton, “Libfte: A toolkit
for constructing practical, format-abiding encryption schemes,” in 23rd USENIX
Security Symposium (USENIX Security 14), (San Diego, CA), pp. 877–891, USENIX
Association, 2014.
[80] K. P. Dyer, S. E. Coull, T. Ristenpart, and T. Shrimpton, “Protocol misidentifica-
tion made easy with format-transforming encryption,” in ACM CCS 2013 (A.-R.
Sadeghi, V. D. Gligor, and M. Yung, eds.), pp. 61–72, ACM Press, Nov. 2013.
[81] K. P. Dyer, S. E. Coull, and T. Shrimpton, “Marionette: A programmable network
traffic obfuscation system,” in 24th USENIX Security Symposium (USENIX Security
15), (Washington, D.C.), pp. 367–382, USENIX Association, 2015.
[82] J. Oakley, L. Yu, X. Zhong, G. K. Venayagamoorthy, and R. Brooks, “Protocol
proxy: An fte-based covert channel,” Computers & Security, vol. 92, p. 101777,
May 2020.
[83] S. Baluja, “Hiding images in plain sight: Deep steganography,” in Neural Infor-
mation Processing Systems, 2017.
[84] D. Hu, L. Wang, W. Jiang, S. Zheng, and B. Li, “A novel image steganography
method via deep convolutional generative adversarial networks,” IEEE Access,
vol. 6, pp. 38303–38314, 2018.
[85] Harveyslash,
“harveyslash/deep-steganography.”
https://github.com/harveyslash/Deep-Steganography, Apr 2018.
2019.
Session 5C: Messaging and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1543be computed as
outcome of P. Similarly, the expected throughput of Meteor can
𝑖∈|P| 𝑝𝑖Exp(𝑝𝑖), where Exp(·) is the expected
number of shared prefix bits for some continuous interval of size
𝑝𝑖 . Thus, the remaining task is to compute a concrete bound on
Exp(·).
We will make the simplifying assumption that the start of an
interval 𝑝𝑖 is placed randomly between [0, 2𝛽+1). Note that interval
𝑖 will never start after 2𝛽+1 − 𝑝𝑖 in practice, so we the number of
prefix bits in this case to be 0, so this simplification will lead to
an expected throughput strictly less than the true value. Addition-
ally, the starting locations for each interval are not independent in
practice, as they each depend on 𝑝 𝑗≠𝑖. However, this independence
assumption also leads to equal or lower expected throughput, as
the starting point for larger intervals will actually be more biased
towards the middle of the distribution, where Exp(·) will be lower,
and smaller distributions will be biased to start near the edges of
the distribution, where Exp(·) will be higher.
4 − 𝜖,
for some small 𝜖 (see Figure 7). If 𝑖 starts between [0, 𝜖), then it is
contained completely before the prefix 01 begins, and thus would
transmit 2 bits. The following 𝑝𝑖 starting points all transmit only 1