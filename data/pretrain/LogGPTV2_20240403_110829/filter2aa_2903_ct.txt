Information for Work-group 0
Index  Wave ID {SE,SH,CU,SIMD,Wave}  Work-item ID          PC     Source l
ine
    0  0x408001c0 { 0, 0, 1, 0, 0}   [0,12, 0 - 15,15, 0]  0x2a8  temp_sou
rce@line 64
    1  0x408001d0 { 0, 0, 1, 1, 0}   [0, 4, 0 - 15, 7, 0]  0x2a8  temp_sou
rce@line 64
    2  0x408001e0 { 0, 0, 1, 2, 0}   [0, 0, 0 - 15, 3, 0]  0x2a8  temp_sou
rce@line 64
    3  0x408001f0 { 0, 0, 1, 3, 0}   [0, 8, 0 - 15,11, 0]  0x2a8  temp_sou
rce@line 64
清单的第1行显示工作组编号（0号）。然后是一个4行（不算标题
行）多列的表格。每一行描述的是一个波阵的不同属性。
第1列是行号。第2列是波阵的硬件标识（ID），它是按照物理硬件
槽位号（hardware slot id）产生的，大括号中的数据是信息来源。其
中，SE代表渲染引擎（Shader Engine）ID，SH是渲染阵列ID，CU是计
算单元ID，SIMD是每个CU中的SIMD单元编号，最后的Wave是波阵槽
位编号，图10-4中的每个指令缓冲单元（PC & IB）可以容纳10个波
阵，这个编号代表10个位置之一。注意，清单中4行的波阵号都是0，说
明4个波阵使用的都是4个指令缓冲单元的第1个槽位。4行中的CU编号
也相同，只有SIMD单元号不同，说明属于同一个工作组的4个波阵在同
一个CU上执行，每个SIMD单元的0号槽位在执行一个波阵。第3列显示
的是工作项ID，连字符前面的三元组是起始编号，后面的三元组是截止
编号，每一行覆盖的范围刚好是64个工作项。第4列是程序指针，4行相
同，代表都执行到相同的位置。最后一列是源代码位置，@符号前是源
文件名，后面是行号。因为排版限制，清单中省略了一列，名叫Abs
Work-item ID，代表工作项的绝对ID，对于本例，与第3列数值完全相
同。
10.3.5 多执行引擎
虽然通用计算单元是GCN的重头和主角，但也不是全部。在GCN
中，也有用来支持其他应用的硬件单元。以视频应用为例，用于多媒体
解码的部分称为统一视频解码器（Unified Video Decoder，UVD）。用
于多媒体编码的部分叫视频编码引擎（Video Coding Engine，VCE）。
此外，支持显示的部分叫显示控制引擎（Display Controller Engine，
DCE）。与上面的简称对应，图形和计算阵列部分也有个统一的简称，
称为GCA（Graphic & Compute Array）。
10.4 GCN指令集
通过上一节，读者对GCN硬件应该有了基本的认识。本节继续介绍
GCN的指令集，巩固上一节的知识，并把焦点转移到软件方面。因为在
硬件结构上，GCN就把执行单元分为向量ALU和标量ALU，所以在指
令方面，GCN的指令定义也明确区分向量操作和标量操作。在GCN程
序的汇编中，大部分指令都以S或者V开头，前者代表标量（Scalar）指
令，后者代表向量（Vector）指令。
10.4.1 7种指令类型
在向量和标量两个大类的基础上，GCN又把所有指令细分为如下7
种类型[4]。
分支（branch）：比如实现无条件分支的sbranch指令，实现条件分
支的s_cbranch指令，还有一系列根据调试状态实现分支的
s_cbranch_cdbgxxx（见10.10节）等。
标量ALU或标量形式的内存访问：前者包括各种整数算术运算、比
较、位操作和读写硬件状态的特别指令（S_GETREG_B32、
S_SETREG_B32），后者用于访问内存，比如从内存读取数据的
S_LOAD_DOWRD指令，向内存写数据的S_STORE_DOWRD指令
等。GCN以波阵为单位执行程序，该类指令的特点是对每个波阵只
要操作一个元素。
向量ALU：向量形式的各种计算，与标量ALU不同，当执行向量
ALU指令时，一个波阵中的64个线程要各自操作自己的数据。
向量形式的内存访问：在vGPR和内存之间移动数据，把内存中的
数据读到每个线程的vGPR中，或者把vGPR中的数据写入内存中。
又分为有类型（typed）访问和无类型（untyped）访问，前者以
MTBUF开头，后者以MUBUF开头。
局部数据共享（local data share，LDS）：用于访问局部共享内存，
比如指令DSREAD{B32,B64,B96,B128,U8,I8,U16,I16}可以为每个线
程读取一份对应类型和大小的数据。
全局数据共享（global data share）或者导出（export）：访问全局
共享内存，或者把数据从VGPR复制到专门的输出缓冲区中，比如
把渲染好的像素以RGBA的形式输出到多个渲染目标（Multi-
Render Target，MRT）。
特殊指令（special instruction）：包括空指令（S_NOP）、同步用
的屏障指令S_BARRIER、暂停和恢复的S_SETHALT等。
上述分类的一个目的是让CU前端可以根据指令类型快速处理和分
发指令。比如，特殊指令在前端就可以执行，标量计算和标量内存访问
要分发给标量单元（sALU）。所有程序流程控制使用的都是标量指
令，包括分支、循环、子函数调用和陷阱。sALU可以使用s通用寄存器
（sGPR），但是不可以使用vGPR和LDS（参见ISA手册5.2节）。相
反，vALU可以访问SGPR。
10.4.2 指令格式
GCN的指令是不等长的，为一个或两个DWORD，也就是32位或者
64位。
根据指令的特征，GCN指令有20余种格式，每种格式都有一个简
称。以S_SLEEP指令为例，其格式代号为SOPP，如图10-7所示。
图10-7 GCN的SOPP指令格式
该格式的指令都是一个DWORD。其中，低16位为立即数，最高9
位固定为0b101111111，中间7位为操作码，用于标识每一种指令，
S_SLEEP的操作码为14（0xE）。S_SLEEP指令会让当前波阵进入睡眠
状态，睡眠的时间大约为64 * SIMM16[6:0]个时钟周期 + 1～64个时钟周
期。其中，SIMM16[6:0]代表立即数部分的第0位到第6位。
10.4.3 不再是VLIW指令
上一节介绍的Terascale指令集是典型的VLIW风格，每个ALU可以
并行执行4条或者5条以VLIW格式同时发射（co-issue）的操作。但这需
要编译器在编译时就找到并行机会，产生合适的VLIW指令，否则就会
浪费硬件资源。
在GCN中，编译器直接产生低粒度的微操作指令，这样的指令会在
64个vALU上同时执行，各自操作自己的数据。这样就减小了编译器的
压力，不再把提高并行度这样的关键任务放在编译器和应用程序上。
因此，可以很放心地说，GCN的指令集不再属于VLIW，这在AMD
官方的GCN架构白皮书[4]上已经表达得非常明确。不过，或许是出于成
见，仍有人认为AMD GPU使用的指令集还是VLIW指令。
10.4.4 指令手册
在AMD的开发者网页[5]上可以下载到部分版本的GCN指令集
（ISA）手册，目前有GCN1、2、3和5版本，内容结构大体相同，都分
为13章。前4章分别介绍术语、程序组织、算核状态（寄存器等）和流
程控制，篇幅不长，加起来只有30多页，值得细读。第5～8章分别介绍
标量操作、向量操作、标量内存操作和向量内存操作。第9～11章介绍
内存层次和数据共享等深度内容。第12章为所有GCN指令的列表，分门
别类地介绍指令的操作码和操作过程。第13章介绍指令的编码格式。
10.5 编程模型
虽然AMD公司的经营业绩很不稳定，几次大起大落，但是它始终
保持着很强的开创精神，让人敬佩。在硬件方面，它率先把内存管理器
移入CPU、勇敢收购ATI并把GPU与CPU融合成APU都是很好的例子。
在软件方面，它也一直在提出新的构想，并锲而不舍地努力着。关于
GPU的编程模型，除了支持DirectX、OpenGL等业界流行的方法外，
AMD也在不断推陈出新。
10.5.1 地幔
2013年9月，多家媒体报道AMD与著名的游戏公司DICE在开发一套
新的图形API，该API可以更直接地控制硬件。这个新技术有个响亮而
且富有意义的名字——地幔（Mantle）[6]。
图10-8是来自官方编程手册[7]的架构图。实线框中的是必需的
Mantle软件组件，虚线框中的是可选的Mantle软件组件。
图10-8 地幔编程模型架构图
图10-8左下方的垂直箭头是地幔技术的关键。箭头上方是用户空
间，下方是硬件，这个粗大的箭头就好像一条高速公路，把应用程序与
GPU硬件联系起来，不需要经过很多复杂的中间层。内核是软件领域中
的政府，像地幔这样跨越政府建设快捷通道是软件领域里常用的一种优
化思路。
从2014年起，关于地幔技术的开发资料和开发包陆续发布，支持的
硬件是GCN微架构的GPU，操作系统是Windows。接下来，使用地幔技
术的游戏也陆续发布。
地幔的成功很快引起了竞争者的注意，没过多久就有其他公司也推
出了与地幔思想类似的技术，比如苹果公司在2014年6月发布了Metal技
术，微软公司在2014年3月发布了DirectX 12。与之前版本侧重改变硬件
接口不同，DirectX 12的重点就是提升CPU端的效率，与地幔的思想如
出一辙。
进入2015年后，人们对地幔未来的看法变得多样化，有人希望它加
入Linux的支持，有人希望它可以支持其他品牌的GPU。最终AMD选择
了开放，把地幔捐献给了以制定图形接口著名的Khronos组织。2015年
的GDC大会上，Khronos宣布了基于地幔衍生出的Vulkan编程接口。
Vulkan是今天仍在使用的GPU编程接口，以偏向底层和高效而著称。
10.5.2 HSA
异构计算是个古老的话题，随着GPU、多核以及NUMA这样的非对
称技术的发展，它又成为计算机系统的一个焦点问题。前面提到过，
AMD公司从2006年开始启动FUSION项目，之后收购ATI，把GPU集成
到CPU中推出APU。这期间，他们一定想到过这个问题。异构是硬件发
展的趋势，异构也应该是软件的未来。但是说来容易，做起来难，因为
涉及面太广。AMD也清楚地认识到了这一点，觉得自己势单力薄。于
是在2012年，AMD携手ARM、Imagination、Mediatek、高通和三星共
同发起并成立了名为HSA Foundation的联盟[8]。
HSA联盟的目标是让程序员可以更简单地编写并行程序。
2015年3月，HSA联盟发布了HSA标准的1.0版本，分为如下三个部
分。
HSA系统架构规约（HSA System Architecture Specification），定义
了硬件的运作规范。
HSA程序员参考手册（HSA Programmers Reference
Manual（PRM）），该部分是整个标准的核心，详细定义了名为
HSAIL 的HSA中间语言，描述了HSAIL的指令集，定义了HSA中间
语言的二进制格式（BRIG），以及构建HSA软件生态系统所需的
工具链和开发规范。
HSA运行时规约（HSA Runtime Specification），定义了应用与
HSA平台之间的接口。
在GitHub网站的HSA联盟页面上[9]，可以看到与上述标准有关的一
些开源项目，有运行时的参考实现、针对HSAIL的编译器和工具以及调
试器等。但这些项目几乎都是针对AMD硬件的，开发者大多也都来自
AMD。
10.5.3 ROCm
或许AMD觉得依靠HSA联盟多方协作速度太慢，或许AMD觉得
HSA联盟的其他成员不够努力，在2015年的超算大会（SC15）上，
AMD宣布了一个新的计划，名叫玻耳兹曼宣言（Boltzmann
Initiative）。这个计划的核心项目称为ROCm，全称为Radeon Open
Compute。ROCm的目标是为AMD的Radeon GPU打造一套高效而且开放
的软件栈，为包括人工智能和超级计算在内的各种应用提供基础环境。
简单地说，ROCm旨在与CUDA正面对抗，竞争GPGPU的市场份额。
ROCm项目开始后，原本在HSA联盟上的多个项目都暂停或者转移
到了ROCm项目中。在ROCm的GitHub站点上[10]，可以看到多个项目在
频繁更新，火热推进。除了核心平台外，还有名为HCC的异构设备编译
器、运行时、ROCm-GDB调试器（见10.13节）、GPU调试工具
SDK（见10.12节）、内核态驱动（ROCk，k代表kernel）等。
在ROCm的开发工具项目中[11]，有一个名为HIP的工具，用于把
CUDA程序转变为可以在ROCm平台上运行的C++程序。HIP的全称
叫“供移植用的异构计算接口”（Heterogeneous- compute Interface for
Portability）。
值得说明的是，开始ROCm并不代表停止HSA。在ROCm中，很多
地方都是基于HSA规范的，而且使用了HSAIL、BRIG等核心技术。可
以认为，HSA是和伙伴们一起推动的标准，ROCm是在自己平台上的实
现，AMD在很踏实地两条腿走路。
10.5.4 Stream SDK和APP SDK
下面介绍AMD的另外两种编程模型和开发工具。一个叫ATI Stream
SDK，这是从ATI时代就开始的并行计算开发工具，是与Terascale GPU
配套的编程工具，最初版本使用的编程语言是与CUDA同源的Brook，
叫ATI Brook+。ATI Brook+工作在ATI的计算抽象层（Compute
Abstraction Layer，CAL）之上，让用户可以使用高层语言编写并行计
算程序。
不过，AMD没有一直按Brook+这个路线发展。在OpenCL出现后，
AMD选择了OpenCL，并把加入OpenCL支持的Stream SDK 2.0改名为
APP SDK。APP是加速并行处理（Accelerated Parallel Processing）的缩
写。这次改名发生在2011年前后。APP SDK发展到3.0版本后，似乎也
停止了，在AMD网站上的链接消失不见了。
10.5.5 Linux系统的驱动
在Linux内核源代码树中，有几个主要显卡厂商的驱动，其位置都
在drivers/gpu/drm/目录下。对于ATI和AMD的显卡，早期的驱动位于