testing process, and ﬁnd that the concolic executor in Driller is
less invoked than that in DigFuzz and Random. This shows
even the fuzzer in Driller could not make much progress in
ﬁnding bugs, it rarely gets stuck when testing who. This result
conﬁrms our claim that the stuck state is not a good indi-
cator for launching concolic execution and a non-stuck state
does not necessarily mean concolic execution is not needed.
Likewise, with the impact of reduced throughput, the fuzzer
in MDPC generates less seeds than DigFuzz and Random.
Then the number of execution paths on these seeds will be
smaller as well. That is, the task of path exploration for the
concolic executor in MDPC is lighter than concolic executors
in DigFuzz and Random. As a consequence, MDPC explores
smaller program states and discovers less bugs than DigFuzz
and Random.
2) Code coverage: As the trigger mechanism used by
LAVA-M is quite simple (a comparison against a 4 byte magic
number), extracting constants from binaries and constructing
dictionaries for AFL will be very helpful [14], especially for
base64, md5sum, and uniq. Consequently, the code coverage
generated by these fuzzing systems (except MDPC due to
the reduced throughput) will be about the same if dictionaries
are used. As a result, we present the code coverage without
dictionary.
10
TABLE IV: Number of discovered vulnerabilities
With dictionaries
Random
Driller
Binaries
base64
md5sum
uniq
who
DigFuzz
48
59
28
167
48
59
28
153
Without dictionary
Random
Driller
48
59
25
142
MDPC
32
13
2
39
AFL
47
58
29
125
DigFuzz
3
0
0
111
3
0
0
92
MDPC
3
0
0
34
AFL
2
0
0
0
3
0
0
26
DigFuzz and Driller via a case study. The binary used
(N RF IN 00017) comes from the CQE dataset, which is also
taken as a case study in the Driller paper [39].
Figure 8 shows the core part of the source code for the
binary. As depicted, the execution has to pass three nested
checks (located in Ln. 4, 7, and 15) so as to trigger the vul-
nerability. We denote the three condition checks as check 1,
check 2, and check 3.
Fig. 7: Normalized incremental bitmap size on LAVA dataset
From the Figure 7, we can observe that DigFuzz can cover
more code than the other three conﬁgurations. Both of the
DigFuzz and Random outperform MDPC and Driller, and all
the hybrid systems (DigFuzz, Random, MDPC and Driller)
is stably better than AFL.
The code coverage in Figure 7 shows that our system is
more effective than Random with only a very small margin.
This is due to the fact that all the four programs are very small,
and the injected bugs are close to each other. For example, in
who, all the bugs are injected into only two functions. With
these two factors, the scale of the execution trees generated
from the programs in LAVA-M are small and contains only a
few execution paths. Thus, path prioritization (the core part in
DigFuzz) can not contribute much since there exists no path
explosion problem.
F. Evaluation on real programs
We also attempt to evaluate DigFuzz on large real-world
programs. However, we observe that
the symbolic engine,
angr, used in DigFuzz, does not have sufﬁcient support for
analyzing large real-world programs. This is also reported by
a recent fuzzing research [32]. More speciﬁcally, the current
version of angr lacks support for many system calls and cannot
make any progress once it reaches an unsupported system call.
We test DigFuzz on more than 20 real-world programs, and
the results show that the symbolic engine on average can only
execute less than 10% branches in a whole execution trace
before it reaches an unsupported system call. We argue that
the scalability of symbolic execution is a different research
area which is orthogonal to our focus in this paper. We will
leave the evaluation on real-world programs as future work.
G. Case study
In this section, we demonstrate the effectiveness of
our system by presenting an in-depth comparison between
Fig. 8: The vulnerability in N RF IN 00017.
1) Performance comparison: Due to the three condition
checks, AFL failed to crash the binary after running for 12
hours. In contrast, all of the three hybrid fuzzing systems,
DigFuzz, Random, and Driller were able to trigger the
vulnerability.
Through examining the execution traces, we observed that
the fuzzer in Driller got stuck at the 57th second, the 95th
second, and the 903rd second caused by check 1, check 2,
and check 3, respectively. Per design, only at these moment
will the concolic executor in Driller be launched to help the
fuzzer. Further inspection shows that the concolic executor had
to process 7, 23 and 94 inputs retained by the fuzzer for the
three condition checks in order to solve the three branches.
Eventually, it took 2590 seconds for Driller to generate a
satisfying input for check 3, and guide the fuzzer to reach
11
    \multirow{3}{*}{\MDPC} & 95 & 1311 & 93 & 21,513 & 22,006 & 29\\    \cline{2-7}     & 95 & 1335 & 92 & 23,635 & 24,129 & 29\\    \cline{2-7}     & 96 & 1427 & 93 & 28,769 & 29,466 & 30\\    \hline 11.522.533.50123456789101112Normalized bitmap sizeFuzzing time (hour)DigFuzzRandomDrillerAFLMDPC11.011.0200.511.522.533.544.55Normalized bitmap sizeFuzzing time (hour)DigFuzzRandomDrillerAFLMDPC1  int main(void) { … 2   RECV(mode, sizeof(uint32_t)); 3   switch (mode[0]) { 4     case MODE_BUILD: ret = do_build(); break; … }  5  int do_build() {  … 6    switch(command) { 7     case B_CMD_ADD_BREAKER: 8     model_id = recv_uint32(); 9     add_breaker_to_load_center(model_id, &result);10    break; …}}  11  int8_t add_breaker_to_load_center() { 12     get_new_breaker_by_model_id(…);} 13  int8_t get_new_breaker_by_model_id(…) { 14     switch(model_id){ 15       case FIFTEEN_AMP: 16         //vulnerability 17         break;     …}}  the vulnerable code. Not surprisingly, DigFuzz and Random
were able to trigger the vulnerability much faster, in 691 and
1438 seconds.
Unlike Driller, both Random and DigFuzz run the fuzzer
and the concolic executor in parallel from the beginning. In
each iteration, Random randomly selects an input for concolic
execution, whereas DigFuzz selects the paths with the highest
priority. Figure 10 and Figure 11 show the time stamps when
concolic executions were invoked and how they helped the
fuzzers in DigFuzz and Random. As we can see from the
ﬁgures, DigFuzz performed 7 concolic executions in 691 sec-
onds while Random ﬁnished 26 executions in 1438 seconds
before they could generate inputs to satisfy check 3 and help
the fuzzers to detect the vulnerability. In terms of the input
generation, DigFuzz managed to generate 96 inputs within
which 37 were imported by fuzzing. Random, on the other
hand, generated 373 inputs and 44 of them were imported.
Moreover, by the time of the 691st second, DigFuzz generated
37 imported inputs while Random can only generate 4. These
numbers show that DigFuzz could generate inputs with much
higher quality than Random.
2) In-depth analysis: We further present the details on
how concolic execution helped the fuzzer to bypass these
three checks in DigFuzz. Figure 9 brieﬂy shows the ex-
ecution tree for N RF IN 00017. In the ﬁgure,
the path
that
leads to the vulnerability is marked as red. To trig-
ger the vulnerability, the execution has to go through three
checks (check 1, check 2 and check 3) and dives into
three functions (do build(), add breaker to load center() and
get new breaker by model id()).
The fuzzer was blocked at check 1 and got stuck quickly
after start. For this speciﬁc branch, all DigFuzz, Random
and Driller quickly generated a satisfying input
to bypass
check 1, because the current execution tree is pretty small
as shown in Figure 9. After this, the fuzzers resumed from
the stuck state. It went into do build(), quickly generated
23 interesting inputs in less than 1 minute and then reached
check 2. The concolic executor in DigFuzz prioritized the
23 inputs, accurately selected the one that corresponded to
check 2 and solved the condition in just one run at
the
636th second. Further, the fuzzer went into the third function
get new breaker by model id() and reached check 3. Note
that even though the fuzzer was blocked by check 3, it did
not get stuck, because there are a number of paths that the
fuzzer can go through as shown in Figure 9. At this moment,
the concolic executor was handed with 97 inputs from the
fuzzer and had to pick the right one to reach check 3. From
Figure 10, we can observe that DigFuzz took only 2 concolic
executions to bypass check 3. Eventually, DigFuzz reached
the vulnerability at the 691st second.
As a comparison, we examined how the concolic executors
work in Random and Driller. In particular, when the fuzzer
had bypassed check 2, it quickly discovered more blocks thus
retained amount of inputs. As shown in Figure 9, there are a
number of paths that the fuzzer can go through. Therefore,
the fuzzer took a long time to get stuck again. However, along
with the red path in Figure 9, the fuzzer quickly got blocked at
check 3. Driller will not identify this speciﬁc branch until the
the fuzzer gets stuck again. Random requires to go through
all the covered paths for discovering missed paths. With path
prioritization, DigFuzz is able to identify speciﬁc paths that
block the fuzzer in time.
By monitoring the status of the fuzzer, we also observe that
the fuzzer got stuck for 8 times in Driller, 3 times in Random,
and only 1 time in DigFuzz. This result indicates that the path
prioritization in DigFuzz was able to generate satisfying inputs
for speciﬁc paths that block the fuzzer in time. As a result, the
fuzzer avoided being stuck for majority of checks.
VI. DISCUSSION
A. Threats to Validity
Our experimental results are based on the limited dataset
presented in the paper. Efforts have been made to evaluate
DigFuzz on real-world programs, but angr [38] fails to
symbolically execute programs whenever it encounters an
unsupported system call. Therefore, the results may not be
fully representative of real-world programs. An evaluation
on such programs is necessary to draw conclusions on the
effectiveness of the proposed techniques in practice. We will
leave the evaluation on real-world programs as our future work.
B. Limitations
it still
First, although the “discriminative dispatch” in DigFuzz
is designed to be a lightweight approach,
imposes
some runtime and memory consumption overhead including
collecting runtime information of fuzzing and constructing
the execution tree. However, based on our evaluation, we
can see the throughput reduction for fuzzing is negligible.
Moreover, since each node in the tree only carries very limited
information, the total memory consumption of the execution
tree is manageable.
Second, since DigFuzz only estimates the difﬁculty of
paths for a fuzzer to explore but does not consider the com-
plexity of constraint solving, it is possible that the constraints
collected from the picked path can be unsolvable, which may
result in a waste of concolic execution cycle. In addition, it is
also possible that the most promising path which could lead
to a vulnerability is not the hardest path picked by DigFuzz.
These two limitations are due to our model of ﬁnding the right
path to explore. We consider solving them as future work.
VII. RELATED WORK
Fuzzing and symbolic execution are the two mainstream
techniques for program testing. Many prior efforts have been
made to improve them [3], [27], [33], [36]. BuzzFuzz [17]
leverages dynamic tainting to identify inputs bytes that are
processed by suspicious instructions. Dowser [20] employs
reverse engineering techniques to identify input ﬁelds that are
concerned with suspicious functions. Vuzzer [34] leverages
control- and data-ﬂow features to accurately determine where
and how to mutate such inputs. Skyﬁre [40] leverages the
knowledge in the vast amount of existing samples to generate
well-distributed seed inputs for fuzzing programs. Angora [12]
aims to increase branch coverage by solving path constraints
without symbolic execution. T-Fuzz [32] ﬁrstly allows the
fuzzer to work on the transformed program by removing
sanity checks that the fuzzer fails to bypass. As an auxiliary
12
Fig. 9: The execution tree for N RF IN 00017.
testing deploy a “demand launch” strategy. Instead, DigFuzz
designs a novel “discriminative dispatch” strategy to better
utilize the capability of concolic execution. TaintScope [41]
deploys dynamic taint analysis to identify the checksum check
points and then applies symbolic execution to generate inputs
satisfying checksum. TaintScope is speciﬁcally designed for
dealing with checksum, and DigFuzz has a more general
scope. More important, DigFuzz employs the Monte Carlo
model to estimate probabilities and prioritize paths, which is
more lightweight than the dynamic taint analysis.
MDPC [42] proposes a theoretical framework for optimal
concolic testing. It deﬁnes the optimal strategy based on
the probability of program paths and the costs of constraint
solving, which is similar with our insight to identify path
probabilities. In contrast to MDPC [42] that adoptes heavy-
weight techniques to calculate the cost of fuzzing and concolic
execution, our model calculates probabilities with coverage