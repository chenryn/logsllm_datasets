to the noisy labels. In this case, the GNN becomes a predictor for
yâ€² rather than y, yielding a small forward correction loss. This is
an incorrect validation signal as it favors ğ¾ğ‘¦ to be close to zero.
To overcome this issue and detect overfitting to label noise, our
approach is to look instead at the accuracy of the target classifier
ğ‘“ (x) = arg maxy Ë†ğ‘(y | x) for predicting the noisy labels yâ€². From
the randomized response algorithm, we know that the probability
of keeping the label is
ğ‘’ğœ–+ğ‘âˆ’1. This gives us an upper bound on
the expected accuracy of a perfect classifier, i.e., the classifier with
100% accuracy on predicting clean labels. In other words, a per-
fect classifier can predict yâ€² with an expected accuracy of at most
ğ´ğ‘ğ‘âˆ— =
ğ‘’ğœ–+ğ‘âˆ’1. Therefore, if during training the model we get accu-
racy above ğ´ğ‘ğ‘âˆ—, either on the training or validation dataset, we can
consider it a signal of overfitting to the noisy labels. More specif-
ically, we train the GNN for a maximum of ğ‘‡ epochs and record
the forward correction loss and the accuracy of the target classifier
for predicting noisy labels over both training and validation sets at
every epoch. At the end of training, we pick the model achieving
ğ‘’ğœ–
ğ‘’ğœ–
Algorithm 3: Locally Private GNN Training with Drop
:Graph G = (VL âˆª VU, E); GNN model ğ‘”(x, G; W);
Input
KProp layer â„(x, G; ğ¾); KProp step parameter for features
ğ¾ğ‘¥ â‰¥ 0; KProp step parameter for labels ğ¾ğ‘¦ â‰¥ 0; privacy
budget for feature perturbation ğœ–ğ‘¥ > 0; privacy budget for
label perturbation ğœ–ğ‘¦ > 0; range parameters ğ›¼ and ğ›½;
number of classes ğ‘; maximum number of epochs ğ‘‡ ;
learning rate ğœ‚;
Output:Trained GNN weights W
1 Server-side:
2 V â† VL âˆª VU
3 Send ğœ–ğ‘¥ , ğœ–ğ‘¦, ğ›¼, and ğ›½ to every node ğ‘£ âˆˆ V.
4 Node-side:
5 Obtain a perturbed vector xâˆ— by Algorithm 1.
6 if current node is in VL then
Obtain a perturbed label yâ€² by (11).
yâ€² â† (cid:174)0
ğ‘£ using (4) for all ğ‘£ âˆˆ V.
ğ‘£, G; ğ¾ğ‘¥) for all ğ‘£ âˆˆ V.
ğ‘£, G; ğ¾ğ‘¦) for all ğ‘£ âˆˆ VL.
7
8 else
9
10 end
11 Send (xâˆ—, yâ€²) to the server.
12 Server-side:
13 Obtain xâ€²
14 hğ‘£ â† â„(xâ€²
15 Ëœyğ‘£ â† â„(yâ€²
16 Partition VL into train and validation sets VLğ‘¡ğ‘Ÿ and VL ğ‘£ğ‘ğ‘™ .
17 ğ´ğ‘ğ‘âˆ— â† ğ‘’ğœ–ğ‘¦/(ğ‘’ğœ–ğ‘¦ + ğ‘ âˆ’ 1)
18 for ğ‘¡ âˆˆ {1, . . . ,ğ‘‡ } do
19
20
21
22
23
24
for all ğ‘£ âˆˆ VL do in parallel
Ë†ğ‘(y | xğ‘£) â† ğ‘”(hğ‘£, G; W)
Obtain Ë†ğ‘(yâ€² | xğ‘£) using (13)
Obtain Ë†ğ‘( Ëœy | xğ‘£) using (14)
Wğ‘¡+1 â† Wğ‘¡ âˆ’ ğœ‚âˆ‡ğ‘£âˆˆVL ğ‘¡ğ‘Ÿ â„“ ( Ëœyğ‘£, Ë†ğ‘( Ëœy | xğ‘£))
ğ‘£âˆˆVL ğ‘£ğ‘ğ‘™ â„“(cid:0)yâ€²
ğ‘£ğ‘ğ‘™ â†

|VL ğ‘¡ğ‘Ÿ |ğ‘£âˆˆVL ğ‘¡ğ‘Ÿ ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦( Ë†ğ‘(y | xğ‘£), yâ€²
ğ‘£âˆˆVL ğ‘£ğ‘ğ‘™ ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦( Ë†ğ‘(y | xğ‘£), yâ€²
ğ‘£)
ğ‘£)
ğ‘£, Ë†ğ‘(yâ€² | xğ‘£)(cid:1)
ğ‘£ğ‘ğ‘™ â† 1
ğ‘¡ğ‘Ÿ â† 1
â„“ğ‘¡
ğ´ğ‘ğ‘ğ‘¡
|VL ğ‘£ğ‘ğ‘™ |
end
25
26
ğ´ğ‘ğ‘ğ‘¡
27
28 end
29 ğ‘¡ â† arg minğ‘¡ â„“ğ‘¡
30 return Wğ‘¡
ğ‘£ğ‘ğ‘™
such that ğ´ğ‘ğ‘ğ‘¡
ğ‘¡ğ‘Ÿ â‰¤ ğ´ğ‘ğ‘âˆ— and ğ´ğ‘ğ‘ğ‘¡
ğ‘£ğ‘ğ‘™ â‰¤ ğ´ğ‘ğ‘âˆ—
the lowest forward correction loss such that their accuracy is at
most ğ´ğ‘ğ‘âˆ—.
Putting all together, the pseudo-code of the LPGNN training
algorithm with Drop is presented in Algorithm 3, where we use
two different privacy budgets ğœ–ğ‘¥ and ğœ–ğ‘¦ for feature and label per-
turbation, respectively. The following corollary entails from our
algorithm:
Corollary 3.7. Algorithm 3 satisfies (ğœ–ğ‘¥ + ğœ–ğ‘¦)-local differential
privacy for graph nodes.
Corollary 3.7 shows that the entire training procedure is LDP
due to the robustness of differential privacy to post-processing [15].
Furthermore, any prediction performed by the LPGNN is again
subject to the post-processing theorem [15], and therefore, satisfies
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2136Table 1: Descriptive statistics of the used datasets
Dataset
Classes Nodes
Edges
Features Avg. Degree
Cora
Pubmed
Facebook
LastFM
7
3
4
10
2,708
19,717
22,470
7,083
5,278
44,324
170,912
25,814
1,433
500
4,714
7,842
3.90
4.50
15.21
7.29
LDP for the nodes, as the LDP mechanism is applied to the private
data only once.
4 EXPERIMENTS
We conduct extensive experiments to assess the privacy-utility
performance of the proposed method for the node classification
task and evaluate it under different parameter settings that can
affect its effectiveness.
4.1 Experimental settings
Datasets. We used two different sets of publicly available real-
world datasets: two citation networks, Cora and Pubmed [61], which
have a lower average degree, and two social networks, Facebook
[42], and LastFM [43] that have a higher average degree. The de-
scription of the datasets is as followed:
â€¢ Cora and Pubmed [61]: These are well-known citation network
datasets, where each node represents a document and edges
denote citation links. Each node has a bag-of-words feature vector
and a label indicating its category.
â€¢ Facebook [42]: This dataset is a page-page graph of verified Face-
book sites. Nodes are official Facebook pages, and edges corre-
spond to mutual likes between them. Node features are extracted
from the site descriptions, and the labels denote site category.
â€¢ LastFM [43]: This social network is collected from the music
streaming service LastFM. Nodes denote users from Asian coun-
tries, and links correspond to friendships. The task is to predict
the home country of a user given the artists liked by them. Since
the original dataset was highly imbalanced, we limited the classes
to the top-10 having the most samples.
Summary statistics of the datasets are provided in Table 1.
Experiment setup. For all the datasets, we randomly split nodes
into training, validation, and test sets with 50/25/25% ratios, respec-
tively. Without loss of generality, we normalized the node features
of all the datasets between zero and one1, so in all cases, we have
ğ›¼ = 0 and ğ›½ = 1. LDP feature perturbation is applied to the fea-
tures of all the training, validation, and test sets. However, label
perturbation is only applied to the training and validation sets, and
the test setâ€™s labels are left clean for performance testing. We tried
three state-of-the-art GNN architectures, namely GCN [26], GAT
[47], and GraphSAGE [18], as the backbone model for LPGNN, with
GraphSAGE being the default model for ablation studies. All the
GNN models have two graph convolution layers with a hidden
1Note that this normalization step does not affect the privacy, as the range parameters
(ğ›¼, ğ›½) are known to both the server and users, so the server could ask users to
normalize their data between 0 and 1 before applying the multi-bit encoder.
dimension of size 16 and the SeLU activation function [27] followed
by dropout, and the GAT model has four attention heads. For both
feature and label KProps, we use GCN aggregator function. We opti-
mized the hyper-parameters of LPGNN based on the validation loss
of GraphSAGE using the Drop algorithm as described in Section 3
with the following strategy, and used the same values for other GNN
models: First, we fix ğ¾ğ‘¥ and ğ¾ğ‘¦ to (16, 8), (16, 2), (4, 2), and (8, 2),
on Cora, Pubmed, Facebook, and LastFM, respectively, and for every
pair of privacy budgets (ğœ–ğ‘¥, ğœ–ğ‘¦) in (1, 1), (1,âˆ), (âˆ, 1), and (âˆ,âˆ),
we perform a grid search to find the best choices for initial learning
rate and weight decay both from {10âˆ’4, 10âˆ’3, 10âˆ’2} and dropout
rate from {10âˆ’4, 10âˆ’3, 10âˆ’2}. Second, we fix the best found hyper-
parameters in the previous step and search for the best performing
KProp step parameters ğ¾ğ‘¥ and ğ¾ğ‘¦ both within {0, 2, 4, 8, 16} for
all ğœ–ğ‘¥ âˆˆ {0.01, 0.1, 1, 2, 3,âˆ} and ğœ–ğ‘¦ âˆˆ {0.5, 1, 2, 3,âˆ}. More specifi-
cally, for every ğœ–ğ‘¥ (resp. ğœ–ğ‘¦) except âˆ, we use the best learning rate,
weight decay, and dropout rate found for ğœ–ğ‘¥ = 1 in the previous
step (resp. ğœ–ğ‘¦ = 1) to search for the best KProp step parameters.
All the models are trained using the Adam optimizer [25] over a
maximum of 500 epochs, and the best model is picked for testing
based on the validation loss. We measured the accuracy on the test
set over 10 consecutive runs and report the average and 95% confi-
dence interval calculated by bootstrapping with 1000 samples. Our
implementation is available at https://github.com/sisaman/LPGNN.
4.2 Experimental results
Analyzing the utility-privacy trade-off. We first evaluate
how our privacy-preserving LPGNN method performs under vary-
ing feature and label privacy budgets. We changed the feature
privacy budget ğœ–ğ‘¥ in {0.01, 0.1, 1, 2, 3,âˆ} and the label privacy bud-
get within {1, 2, 3,âˆ}. The cases where ğœ–ğ‘¥ = âˆ or ğœ–ğ‘¦ = âˆ, are
provided for comparison with non-private baselines, where we did
not apply the corresponding LDP mechanism (multi-bit for fea-
tures and randomized response for labels) and directly used the
clean (non-perturbed) values. We performed this experiment using
GCN, GAT, and GraphSAGE as different backbone GNN models and
reported the node-classification accuracy, as illustrated in Figure 3.
We can observe that all the three GNN models demonstrate
robustness to the perturbations, especially on features, and per-
form comparably to the non-private baselines. For instance, on the
Cora dataset, both GCN and GraphSAGE could get an accuracy of
about 80% at ğœ–ğ‘¥ = 0.1 and ğœ–ğ‘¦ = 2, which is only 6% lower than the
non-private (ğœ– = âˆ) method. On the other three datasets, we can
decrease ğœ–ğ‘¥ to 0.01 and ğœ–ğ‘¦ to 1, and still get less than 10% accuracy
loss compared to the non-private baseline. We believe that this is a
very promising result, especially for a locally private model perturb-
ing hundreds of features with a low privacy loss. This result shows
that different components of LPGNN, from multi-bit mechanism to
KProp, and the Drop algorithm are fitting well together.
According to the results, the GAT model slightly falls behind
GCN and GraphSAGE in terms of accuracy-privacy trade-off, espe-
cially at high-privacy regimes ğœ–ğ‘¥ â‰¤ 1, which is mainly due to its
stronger dependence on the node features. Unlike the other two
models, GAT uses node features at each layer to learn attention
coefficients first, which are then used to weight different neighbors
in the neighborhood aggregation. This property of GAT justifies its
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2137N
C
G
T
A
G
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
E
G
A
S
h
p
a
r
G
)
%
(
y
c
a
r
u
c
c
A
100
80
60
40
20
0
100
80
60
40
20
0
100
80
60
40
20
0
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
N
C
G
ğœ–ğ‘¦ = 1
ğœ–ğ‘¦ = 2
ğœ–ğ‘¦ = 3
ğœ–ğ‘¦ = âˆ
0.01 0.1 1.0 2.0 3.0 âˆ
ğœ–ğ‘¥
T
A
G
ğœ–ğ‘¦ = 1
ğœ–ğ‘¦ = 2
ğœ–ğ‘¦ = 3
ğœ–ğ‘¦ = âˆ
0.01 0.1 1.0 2.0 3.0 âˆ
ğœ–ğ‘¥
E
G
A
S
h
p
a
r
G
)
%
(
y
c
a
r
u
c