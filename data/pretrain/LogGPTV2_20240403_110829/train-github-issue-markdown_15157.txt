#### Description
Hi,
I am trying to use the sklearn.mixture.GaussianMixture for clustering purpose.
I read from the clustering comparison page: https://scikit-
learn.org/stable/modules/clustering.html that Gaussian Mixture does not
require scaling for each variable to have unit variance before fitting the
model. That makes sense to me because it is using Mahalanobis Distance to
center.
However, I found some conflict results when I tried some toy problems. I
generated two clusters of data from bivariate normal distributions and put
them together, hoping the GaussianMixture will be able to separate them in
return.
My generated data:  
![generated](https://user-
images.githubusercontent.com/44034333/61408831-f46beb80-a8ae-11e9-9e2f-623c7c31ecca.png)
the data has larger variance along one axis than the other
Using GaussianMixture without scaling:  
![unscaled](https://user-
images.githubusercontent.com/44034333/61409151-a0153b80-a8af-11e9-88e9-25c18bec2b26.png)
Using GaussianMixture with sklearn.preprocessing.scale():  
![scaled](https://user-
images.githubusercontent.com/44034333/61409173-ab686700-a8af-11e9-886a-59b520dfd05c.png)
It looks like it is the scaled one that catches the data feature. Please
kindly help. Is it necessary to scale attributes before applying Gaussian
Mixture.
Thanks!
#### Steps/Code to Reproduce
mu = np.array([0,0])  
cov = np.array([[10,0],[0,1000]])  
data1 = np.random.multivariate_normal(mu,cov,5000)
mu = np.array([mean_diff,0])  
cov = np.array([[10,0],[0,1000]])  
data2 = np.random.multivariate_normal(mu,cov,5000)
data = np.concatenate((data1,data2),axis=0)
gmm_model = gmm(2,'full')  
#data = preprocessing.scale(data)  
clusters = gmm_model.fit_predict(data)
#### Expected Results
#### Actual Results
#### Versions
scikit-learn = 0.21.2:  
Linux-4.15.0-54-generic-x86_64-with-debian-buster-sid  
Python 3.7