### Malicious Dataset

Creating a malware dataset is a challenging task [17, 27]. Our second dataset was built from executable samples submitted to Anubis [1], a dynamic malware analysis platform that receives thousands of samples for analysis daily. We obtained the malware samples submitted to Anubis in 2011, along with the latest antivirus labels associated with each sample. These labels are provided by VirusTotal [2], which aggregates results from multiple antivirus vendors.

To each sample, we assigned a toxicity ratio \( \tau \), defined as the ratio of the number of antivirus vendors that flagged the sample as malicious to the total number of antivirus vendors checked by VirusTotal. The density distribution of the toxicity ratio for 1.2 million malware samples is shown in Figure 4. This distribution indicates that the ratio is either very low or very high, meaning that either only a few antivirus vendors flag a sample as malicious (sometimes spuriously) or almost all vendors do so.

From this set of samples, we constructed the malicious dataset by selecting samples with \( \tau > 0.9 \), i.e., those flagged as malicious by at least 90% of the antivirus vendors. To ensure a robust consensus, we excluded samples with results from fewer than 30 out of 49 antivirus vendors. This consensus reflects the combined judgment of many human experts who have analyzed the sample (or similar samples) and concluded it to be malicious. Additionally, to increase confidence, we used older samples observed in 2011 with their latest antivirus labels obtained after a year.

The samples-per-malware-family metric in such datasets is often skewed due to the prevalence of popular malware families, which are more frequently submitted to public analysis platforms like Anubis. Therefore, from the large 2011 dataset, we selected at most 100 samples per malware family. The final dataset contains 51,058 unique malware samples representing 15,089 malware families.

### Real-World Dataset

To evaluate the performance of detection methods on real-world malware, we used the recent feed of malware samples submitted to Anubis over three months, starting from November 2012. This dataset includes 1.2 million samples.

### Evaluation

#### 5.1 Experimental Setup

We computed and stored the signal processing-based features of both malware and benign datasets in memory using the Balltree data structure. For N-gram-based detection, we used a bit-vector approach to encode the N-gram signature, as proposed in [10]. This method transforms the Jaccard computation into CPU-friendly logic operations, significantly speeding up the process. We set \( n = 2 \) and \( n = 3 \), following previous N-gram-based works [9, 23].

#### 5.2 Experiment Against Noise

We conducted a synthetic experiment to test the robustness of the features against small modifications (noise) introduced into the samples. We generated four seed binaries, each containing 200KB of random bytes, to avoid making specific assumptions about data patterns. From each seed binary, we created synthetic variants by introducing random noise. The differences among these variants were pronounced due to the random modifications. We computed signal processing features from these variants and visualized them using Multidimensional Scaling (MDS), as shown in Figure 5. The features can cluster the variants even when 50% of the original bytes are randomly modified, which may occur in polymorphic or metamorphic malware.

#### 5.3 Detection

First, we analyzed the classification strength of the SigMal features using various machine learning classifiers. The single nearest-neighbor distances-based classifier provided the best results. Next, we performed a comparative evaluation of SigMal with existing detection methods using the Nearest Neighbor (NN) classifier and the standard 10-fold cross-validation process on the same dataset. The evaluation dataset is described in Section 4.

We conducted precision-recall analysis by varying the threshold value \( t \) of the detection confidence parameter \( c \) (introduced in Section 3.1). For each testing sample, SigMal returns two nearest-neighbor distances, \( d_m \) and \( d_b \), corresponding to the malware and benign training sets, respectively. Figure 6 shows the distribution of these distances from the 10-fold cross-validation experiment. A sample with a shorter distance to the malware dataset than to the benign dataset falls in the upper left section of the graph. The area inside the dotted line represents the confusion area given by the inequality \( c < 0.6 \).

#### 5.4 Sliding Window Experiments

For the sliding window experiments, we used a 30-day time window to collect ground truth labels. We noticed that some antivirus vendors falsely detect benign samples as malicious if they are packed with commonly available executable packers, such as Winpack. From the toxicity graph, we see that this confusion persists up to \( \tau = 0.3 \). To avoid including such spurious labels, we considered \( \tau \leq 0.3 \) as a low confidence value and excluded it from our precision-recall analysis. We found that the precision did not improve significantly when the time window was greater than 30 days, so we chose a 30-day window for our daily experiments.

As evident in the performance experiments in Section 5.4, N-gram-based and Control Flow Graph (CFG)-based methods have high computational costs. For example, with a 30-day sliding window dataset, the CFG-based method required several days to complete a single experiment even on a 24-core, 96GB machine. Thus, this part of the comparison experiment on the real-world dataset is limited to a few days.

#### 5.5.2 Results

Figure 10 shows the precision and recall performance of the SigMal detection on real-world samples observed by Anubis in December 2012 and January 2013.