none of the antivirus vendors ﬂag them as malicious.
Malicious dataset
Malware dataset creation is a diﬃcult problem [17, 27]. We
built our second dataset from the executable samples sub-
mitted to Anubis [1]. Anubis is a dynamic malware analy-
sis platform that receives thousands of samples for analysis
everyday. We obtained the malware samples submitted to
Anubis in 2011, along with the latest antivirus labels asso-
ciated with each sample. Antivirus labels are provided by
VirusTotal [2], which includes labels from diﬀerent antivirus
vendors for each submitted sample. To each sample, we as-
sociate a toxicity ratio τ , where τ is the ratio of the total
number of antivirus vendors that detected the sample as ma-
licious to the total number of antivirus vendors checked by
VirusTotal. The density distribution of the toxicity ratio of
1.2 million malware samples is shown in Fig. 4. It clearly
shows that the ratio is concentrated either towards a smaller
value or towards a larger value. This means, either only a
few antivirus vendors are likely to label a sample as mali-
cious (sometimes spurious) or almost all vendors are likely
to label it as malicious. From this set of samples, we built
the malicious dataset by taking samples with τ > 0.9, that
is, the set of binary samples which were ﬂagged as mali-
cious by 90% of the antivirus vendors. Some samples were
missing results from some of the antivirus vendors. To main-
tain an eﬀective large majority, we discarded those samples
that have results from less than 30 antivirus vendors (out of
49). This consensus by a majority of antivirus vendors is a
result of many human experts who have analyzed the sam-
ple (or similar samples) and concluded it to be malicious.
Moreover, to have a stronger conﬁdence on this consensus,
we chose older samples observed in 2011 with their latest
antivirus labels obtained after a year.
The samples-per-malware-family metric of this type of
datasets are usually skewed because of the abundance of
some widely popular malware families, which are usually
more frequently submitted to such public analysis platform,
such as Anubis. Therefore, out of the large malware dataset
observed in 2011, we only took at most 100 samples per mal-
0.00.20.40.60.81.00.01.0Toxicity ratioProbability density(a)
(b)
(c)
Figure 5: Feature robustness against noise. Similar
symbols represent the variants from the same seed
binary.
ware family. The dataset after this selection contains 51,058
unique malware samples representing 15,089 malware fami-
lies.
Real-world dataset
To evaluate the performance of the detection methods on
real-world malware, we used the recent-feed of malware sam-
ples submitted to Anubis over three months, starting from
November 2012. This dataset contains 1.2 million samples.
5. EVALUATION
5.1 Experimental setup
The signal processing-based features of both malware and
benign datasets are computed and stored in memory in the
Balltree data structure. In the N-gram-based detection, we
used a bit-vector approach to encode the N-gram signature,
as proposed in [10]. This transforms the Jaccard computa-
tion into more CPU-friendly logic operations, and speeds up
the computation by many orders of magnitude. Like previ-
ous N-gram based works [9, 23], we set n=2 and n=3.
5.2 Experiment against noise
We performed a synthetic experiment to test the robust-
ness of the feature against small modiﬁcations (noise) intro-
duced into the sample. We ﬁrst generated four seed binaries
containing 200KB random bytes. We chose to use random
bytes such that we do not make any speciﬁc assumption
on the data pattern. From each of these seed binaries, we
generated synthetic variants by introducing random noise
to the original binary. Note that the diﬀerences among
the variants are even more pronounced due to these ran-
dom modiﬁcations. We computed signal processing features
from these synthetic variants and visualized using Multidi-
mensional scaling (MDS), as shown in Figure 5. We can see
that the features can be used to cluster the variants even
when 50% of the original bytes are randomly modiﬁed. In
the case of malware binaries, such noise may be introduced
by a polymorphic or metamorphic engine.
5.3 Detection
At ﬁrst, we analyzed the classiﬁcation strength of the
SigMal features using various machine learning classiﬁers.
Single nearest-neighbor distances based classiﬁer provided
the best result.
In the next step, using Nearest Neighbor
(NN) classiﬁer, we performed a comparative evaluation of
SigMal with existing detection methods. We used the stan-
dard 10-fold cross-validation process on the same dataset for
Figure 6: The nearest-neighbor distance distri-
bution of 100K samples from the 10-fold cross-
validation experiment of SigMal. For each point,
X-axis represents the nearest-neighbor distance to
malware dataset, and Y-axis represents the the
nearest-neighbor distance to the benign dataset.
all methods. The evaluation dataset used in this experiment
is described in Section 4.
We performed the precision-recall analysis by varying the
threshold value t of the detection conﬁdence parameter c (in-
troduced in Section 3.1). For each testing sample, SigMal re-
turns two nearest-neighbor distances dm and db correspond-
ing to the malware and benign training sets, respectively.
Fig. 6 shows the distribution of these distances computed
from the 10-fold cross-validation experiment. A sample with
shorter distance to the malware dataset than to the benign
dataset falls in the upper left section of the graph. The area
inside the dotted line represents the confusion area given by
the inequality c  0.6 for building the dynamic training set. We use the
samples submitted on day n as the testing set for the nth
day experiment. For the comparison of the SigMal detection
results, we need to label each of these incoming samples as
benign or malicious using recent antivirus labels. However,
we noticed that few antivirus vendors falsely detect a benign
sample as malicious, if packed with some commonly avail-
able executable packer, such as Winpack. Again from the
toxicity graph, we can see that this confusion sustains up
until τ = 0.3. To avoid including such spurious labels, we
consider τ <= 0.3 as a low conﬁdence value for the result
evaluation, and exclude it in our precision-recall analysis. To
ﬁnd the optimal sliding time window in terms of speed and
accuracy for collecting the ground truth, we performed a set
of experiments using diﬀerent values of w, ranging from 7
days to 60 days. The precision did not improve signiﬁcantly
when the time window was greater than 30 days. Therefore,
we chose the time window w as 30 days for the rest of our
daily experiments.
As evident in the performance experiments in Section 5.4,
N-gram-based and CFG-based methods have a high com-
putational cost. For example, with the 30 days sliding-
window dataset, the CFG-based method required several
days to complete a single sliding window experiment even
with a parallelized implementation on a 24-core 96GB ma-
chine. Hence, this part of the comparison experiment on the
real-world dataset is limited to a few days.
5.5.2 Results
The precision and recall performance of the sliding win-
Figure 10: Precision and Recall of the SigMal detec-
tion on the real-world samples observed by Anubis
in December 2012 and January 2013. The ﬁgure