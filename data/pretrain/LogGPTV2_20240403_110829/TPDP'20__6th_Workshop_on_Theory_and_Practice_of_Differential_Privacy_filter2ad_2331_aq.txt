have:
Theorem 8.4 (private SQ learning [13]). Every concept class that is eﬃciently PAC learnable in
the statistical query model (which includes Qpt, Qthr, and Qconj) is eﬃciently and privately PAC
learnable.
In fact, Kasiviswanathan et al. [66] showed that (eﬃcient) statistical query learners are equiva-
lent to (eﬃcient) private learners in the “local model” of privacy (which will be discussed more in
the next section).
However, there are some concept classes that are eﬃciently PAC learnable that are provably
not learnable in the statistical query model, most notably the class of parity functions: that is, the
class of functions {0, 1}d → {0, 1} of the form x (cid:55)→ c · x, where c · x is taken modulo 2. It turns
out that there is an elegant, eﬃcient private learner for this class, showing that eﬃcient private
learning goes beyond the statistical query model:
Theorem 8.5 (private learning of parities [66]). The class Qpar = Qpar(d) of parity functions on
{0, 1}d is eﬃciently and privately PAC learnable, with sample complexity n = O(d/ε) for (ε, 0)-
diﬀerential privacy.
68
Since the class of parity functions on {0, 1}d has VC dimension d, the sample complexity for
private learning is within a constant factor of the sample complexity for non-private learning.
Proof. We have a dataset (x, y) with n rows (xi, yi), where xi ∈ {0, 1}d and yi ∈ {0, 1}. Assume that
x1, . . . , xn are drawn independently from some distribution D, and that there is some c ∈ {0, 1}d
such that yi = c · xi for all 1 ≤ i ≤ n. We wish to determine a hypothesis h ∈ {0, 1}d such that, if
x is drawn from D, then h · x = c · x with probability at least 0.99.
A simple (non-private) algorithm is to take any h such that yi = h · xi for all i. We can do
this by using Gaussian elimination to solve the system of linear equations y = h · x. Standard
calculations show that this succeeds with n = O(d) samples.
Now let’s consider private learning, keeping in mind that we need to ensure privacy even when
the data is inconsistent with the concept class. Indeed, we need to make sure that we don’t leak
information by revealing whether or not the data is consistent! For instance, we need to make sure
that the algorithm’s output distribution only changes by ε (multiplicatively) if we add a single row
(xi, yi) such that yi (cid:54)= c · xi.
Our mechanism M works as follows; we use ⊥ to denote failure. We will start by succeeding
with probability about 1/2, and amplify this probability later.
1. Take n = O(d/ε) samples.
2. With probability 1/2, output ⊥.
3. For each 1 ≤ i ≤ n, set ˆxi, ˆyi independently as follows:
(cid:40)
(ˆxi, ˆyi) =
(0d, 0) with probability 1 − ε ,
(xi, yi) with probability ε .
Call the resulting dataset (ˆx, ˆy). This is eﬀectively a random sample of the original dataset,
containing an expected fraction ε of the rows. The zero entries (ˆxi, ˆyi) = (0d, 0) will have no
eﬀect on what follows.
4. Using Gaussian elimination, determine the aﬃne subspace V of hypotheses h that are con-
sistent with (ˆx, ˆy), i.e.,
V = {h | ∀i : ˆyi = h · ˆxi} .
Output an h chosen uniformly from V . If V = ∅ (i.e., if no consistent h exists), then output
⊥.
Since the non-private algorithm described above succeeds with probability 0.99, if the data is
consistent then M succeeds with probability at least 0.49. We can amplify by repeating this t times,
in which case the sample complexity is n = O(td/ε).
Now we analyze M’s privacy. We willfully identify 1 ± ε with e±ε, neglecting O(ε2) terms.
Claim 8.6. M is (2ε, 0)-diﬀerentially private.
Proof of Claim. Let x ∼ x(cid:48) be two neighboring datasets that diﬀer at one row i. Assume that
(x(cid:48)
i) = (0d, 0). Since we can get from any x to any x(cid:48)(cid:48) by going through such an x(cid:48), if we can
show that M(x) and M(x(cid:48)) are (ε, 0)-indistinguishable, then M will be (2ε, 0)-diﬀerentially private.
i, y(cid:48)
69
With probability 1 − ε, we replace (xi, yi) with (0d, 0) in step 3 (assuming we make it past step
2). In that case, (ˆx, ˆy) = (ˆx(cid:48), ˆy(cid:48)), and the output probabilities are the same. Thus for all possible
outputs z,
(9)
But we are not done. The problem is that x(cid:48) is special (by our assumption) so the reverse inequality
does not automatically hold. We also need to prove
Pr[M(x) = z] ≥ (1 − ε) Pr[M(x(cid:48)) = z] .
To prove (10), start by ﬁxing (ˆxj, ˆyj) = (ˆx(cid:48)
(Thus, we are coupling the
algorithm’s random choices on the two datasets.) Let V−i be the aﬃne subspace consistent with
these rows:
Pr[M(x) = z] ≤ (1 + ε) Pr[M(x(cid:48)) = z] .
j) for all j (cid:54)= i.
j, ˆy(cid:48)
(10)
V−i = {h | ∀j (cid:54)= i : ˆyj = h · ˆxj} .
As before, if we fail or if we set (ˆxi, ˆyi) = (0d, 0) = (ˆx(cid:48)
i), the output probabilities are the same.
On the other hand, with probability ε/2 we pass step 2 and set (ˆxi, ˆyi) = (xi, yi) in step 3. In that
case, M(x(cid:48)) is uniform in V−i (or M(x(cid:48)) =⊥ if V−i = ∅), while M(x) is uniform in
i, ˆy(cid:48)
V = V−i ∩ {h | yi = h · xi} ,
(or M(x) =⊥ if V = ∅).
Let’s compare the probabilities that M(x) and M(x(cid:48)) fail. If V−i = ∅, then M(x) = M(x(cid:48)) =⊥.
But if V−i (cid:54)= ∅ but V = ∅, the probability that M(x) fails is at most 1/2 + ε/2; and since M(x(cid:48))
fails with probability at least 1/2, we have
Pr[M(x) =⊥] ≤ 1 + ε
2
≤ (1 + ε) Pr[M(x(cid:48)) =⊥] .
Finally, we come to the most interesting case: comparing the probabilities that M(x) and M(x(cid:48))
output some hypothesis h, where both V−i and V are nonempty and contain h. Since V is obtained
by adding one linear constraint to V−i, we have:
|V | ≥ 1
2
|V−i| .
Since M(x) and M(x(cid:48)) are uniform in V and V−i respectively, for every h ∈ V−i we have
(cid:18) 1 − ε
(cid:19)
Pr[M(x) = h] ≤ 1
2
which completes the proof.
|V−i| +
ε
|V |
≤ 1
2
· 1 + ε
|V−i| = (1 + ε) Pr[M(x(cid:48)) = h] ,
Since linear algebra was essentially the only known technique for eﬃcient private learning outside
the Statistical Query Model, this result suggested that perhaps every concept that is eﬃciently
PAC learnable is also eﬃciently and privately PAC learnable. Bun and Zhandry [20] recently gave
evidence that this is not the case.
Theorem 8.7 (hardness of private learning [20]). If “indistinguishability obfuscation” and “perfectly
sound non-interactive zero-knowledge proofs for NP” exist, then there is a concept class that is
eﬃciently PAC learnable but not eﬃciently PAC learnable with diﬀerential privacy.
70
8.3 The Sample Complexity of Private PAC Learning
Another gap between PAC learning and private PAC learning is in sample complexity. The sample
complexity of non-private learning is characterized by Θ(VC(C)), whereas for private learning we
have the upper bound O(log |C|) from Theorem 8.5, which can be as large as d· VC(C) on a domain
of size 2d. Two classes that illustrate this gap are the classes of point functions and threshold
functions (Qpt and Qthr). In both cases, we have VC(C) = 1 but log |C| = d.
For the class C = Qpt(d) of point functions on {0, 1}d and (ε, 0)-diﬀerentially private proper
learners, Beimel, Brenner, Kasiviswanathan, and Nissim [10] showed that the best possible sample
complexity is Θ(d), similarly to the situation with releasing approximate answers to all point
functions (Proposition 2.8 and Theorem 5.14).
If we relax the requirement to either improper
learning or approximate diﬀerential privacy, then, similarly to Theorem 3.5, the sample complexity
becomes independent of d, namely O(1) or O(log(1/δ)), respectively [10, 9].
For the class C = Qthr([2d]) of threshold functions on {1, . . . , 2d}, again it is known that Θ(d)
sample complexity is the best possible sample complexity for (ε, 0)-diﬀerentially private proper
learners [10], similarly to Theorem 7.2. In contrast to point functions, however, it is known that
relaxing to either (ε, δ)-diﬀerential privacy or to improper learning is not enough to achieve sam-
ple complexity O(1). For (ε, δ)-diﬀerentially private proper learners, the sample complexity is
somewhere between 2(1+o(1)) log∗ d) · log(1/δ) and Ω(log∗ d), similarly to Theorem 7.3. For (ε, 0)-
diﬀerentially private learners, the sample complexity was recently shown to be Ω(d) by Feldman
and Xiao [49]. We present the proof of this result, because it uses beautiful connections between
VC dimension, private learning, and communication complexity.
Every concept class C deﬁnes a one-way communication problem as follows: Alice has a function
c ∈ C, Bob has a string w ∈ {0, 1}d, and together they want to compute c(w). The one-way
communication complexity of this problem is the length of the shortest message m that Alice
needs to send to Bob that lets him compute c(w). We will consider randomized, distributional
communication complexity, where the inputs are chosen according to some distribution µ on C ×
{0, 1}d, and Bob should compute c(w) correctly with high probability over the choice of the inputs
and the (shared) randomness between Alice and Bob. We write CC→,pub
(C) to denote the minimum
message length over all protocols where Bob computes c(w) with probability at least 1 − α.
µ,α
It was known that maximizing this communication complexity over all product distributions
characterizes the sample complexity of non-private learning (i.e. VC dimension):
Theorem 8.8 (CC characterization of non-private learning [72]). For every constant α ∈ (0, 1/8),
(cid:18)
(cid:19)
,
→,pub
µA⊗µB,α(C)
where µA and µB are distributions on C and {0, 1}d respectively.
VC(C) = Θ
max
µA,µB
CC
Building on Beimel et al. [8], Feldman and Xiao [49] showed that the sample complexity of learn-
ing C with pure diﬀerential privacy is related to the one-way communication complexity maximized
over all joint distributions on C × {0, 1}d.
Theorem 8.9 (CC characterization of learning with pure diﬀerential privacy [49]). For all constants
ε > 0, α ∈ (0, 1/2), the smallest sample complexity for learning C under (ε, 0)-diﬀerential privacy
is Θ(maxµ CC→,pub
(C)).
µ,α
71
We note that, by Yao’s minimax principle, maxµ CC→,pub
(C) is simply equal to the worst-
case randomized communication complexity of C, where we want a protocol such that on every
input, Bob computes the correct answer with probability at least 1 − α over the public coins
of the protocol. Returning to threshold functions, computing cy(w) is equivalent to computing
the “greater than” function. Miltersen et al. [79] showed that for this problem the randomized
communication complexity is Ω(d), proving that learning threshold with pure diﬀerential privacy
requires sample complexity Ω(d).
µ,α
Proof sketch of Theorem 8.9. We begin by showing that the communication complexity is upper-
bounded by the sample complexity of private learning. Let L be a (ε, 0)-diﬀerentially private learner
for C with a given sample complexity n; we will use L to construct a communication protocol. Using
their shared randomness, Alice and Bob both run L on the all-zeros dataset x(0). They do this M
times for M to be determined in a moment, giving a list of shared functions h1, . . . , hM ∈ H.
by L “covers” the distribution on every other dataset x ∈ Xn, in the sense that for each h ∈ H,
Since L is (ε, 0)-diﬀerentially private, by group privacy, the distribution of functions returned
Pr[L(x(0)) = h] ≥ e−εn Pr[L(x) = h] .
Thus with M = eO(εn) samples, Alice and Bob can ensure that, with high probability, at least one
hi in their shared list is a good hypothesis for any particular dataset.
In particular, let µ be a distribution on pairs (c, w), and let c0 ∈ C be Alice’s function. Then
there is some 1 ≤ i ≤ M such that hi is a good hypothesis for the dataset x we would get by sampling
the rows of x from the conditional distribution µ(w | c = c0): that is, hi(w) = c0(w) with high
probability in w. Alice can send Bob this index i with communication complexity log M = O(εn).
Conversely, suppose that we have randomized, public-coin protocol for C with communication
complexity at most n. Every setting r of the public randomness and message m from Alice deﬁnes
a hypothesis hr,m which Bob uses to compute the output of the protocol (by applying it to his input
w). Given a dataset (x1, y1), . . . , (xn, yn), our diﬀerentially private learner will choose r uniformly
at random, and then use the exponential mechanism to select an m approximately maximizing
|{i : hr,m(xi) = yi}|, similarly to use of the exponential mechanism in the proof of Theorem 8.3.
The sample complexity n required by the exponential mechanism is logarithmic in the size of the
hypothesis class Hr = {hr,m}, so we have n = O(|m|).
While this provides a tight characterization of the sample complexity of learning with pure
diﬀerential privacy, the case of approximate diﬀerential privacy is still very much open.
Open Problem 8.10. Does every concept class C over {0, 1}d have an (ε, δ)-diﬀerentially private
learner with sample complexity n = O(VC(C) · polylog(1/δ)) (for δ negligible in n and d)? Or are
there concept classes where the sample complexity must be n = Ω(d · VC(C))?
These questions are open for both proper and improper learning. In the case of proper learning,
there are concept classes known where the sample complexity is at least Ω(log∗ d· VC(C)· log(1/δ)),
such as threshold functions [23], but this does not rule out an upper bound of n = O(VC(C) ·
polylog(1/δ)) when δ is negligible in n and d.
72
9 Multiparty Diﬀerential Privacy
9.1 The Deﬁnition
We now consider an extension of diﬀerential privacy to a multiparty setting, where the data is
divided among some m parties P1, . . . , Pm. For simplicity, we will assume that m divides n and each
party Pk has exactly n/m rows of the dataset, which we will denote by xk = (xk,1, xk,2, . . . , xk,n/m).
(Note the change in notation; now xk is a sub-dataset, not an individual row.) We consider the
case that Pk wants to ensure the privacy of the rows in xk against an adversary who may control
the other parties.
As in the studies of secure multiparty computation (cf.
[51]), there are many variants of the
adversary model that we can consider:
• passive vs. active: for simplicity, we will restrict to passive adversaries — ones that follow
the speciﬁed protocol — but try to extract information from the communication seen (also
known as “honest-but-curious” adversaries). Since our emphasis is on lower bounds, this only
strengthens the results. However, all of the upper bounds we mention are also known to hold
for active adversaries.
• threshold adversaries: we can restrict the adversary to control at most t parties for some
t ≤ m − 1. For simplicity, we will only consider the case t = m − 1. Consequently we may
assume without loss of generality that all communication occurs on a broadcast channel, as
the adversary would anyhow see all communication on point-to-point channels.
• computationally bounded vs. unbounded: as in the basic deﬁnition of diﬀerential privacy,
we will (implicitly) consider computationally unbounded adversaries. In the next section, we
will discuss computationally bounded adversaries.
A protocol proceeds in a sequence of rounds until all honest parties terminate. Informally, in
each round, each party Pk selects a message to be broadcast based on its input x(k), internal coin
tosses, and all messages received in previous rounds. The output of the protocol is speciﬁed by a
deterministic function of the transcript of messages exchanged. (As in secure multiparty computa-