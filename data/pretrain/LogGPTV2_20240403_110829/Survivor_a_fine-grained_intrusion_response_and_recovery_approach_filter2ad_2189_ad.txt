We chose Gitea, because it requires a diverse set of privileges and
it shows how our approach can be applied to a complex real-world
service.
In our use case, we configured Gitea with the principle of least
privileges. It means that restrictions which corresponds to responses
with a cost assigned to none are initially applied to the service (e.g.,
Gitea can only listen on port 80 and 443 or Gitea have only access
to the directories and files it needs). Even if the service follows the
best practices and is properly protected, an intrusion can still do
damages and our approach handles such cases.
We consider an intrusion that compromised our Gitea service
with the Linux.Encoder.1 ransomware.8 When executed, it en-
crypts all the git repositories and the database used by Gitea. Hence,
we previously configured the policy to set the cost of such a mali-
cious behavior to high,9 since it would render the site almost unus-
able: mbcost("gitea", "compromise-data-availability") = "high".
Since our focus is not on intrusion detection, we assume that
the IDS detected the ransomware. This assumption is reasonable
with, for example, several techniques to detect ransomware such
as API call monitoring, file system activity monitoring, or the use
of decoy resources [32].
In practice, however, an IDS can generate false positives, or it
can provide non-accurate values for the likelihood of the intrusion
leading to a less adequate response. Hence, we consider three cases
to evaluate the response selection: the IDS detected the intrusion
and accurately set the likelihood, the IDS detected the intrusion
but not with an accurate likelihood, and the IDS generated a false
positive.
In Table 3, we display a set of responses for the ransomware that
we devised based on existing strategies to mitigate ransomware,
such as CryptoDrop lockdown [13] or Windows controlled folder
access [47]. None of them could have been applied proactively by
the developer, because they degrade the quality of service. We set
their respective cost for the service and the estimated performance.
Table 3: Responses to withstand ransomware reinfection
with their associated cost and performance for Gitea
# Response
1 Disable open system call
2 Read-only filesystem except sessions folder
3
Paths of git repositories inaccessible
4 Read-only paths of all git repositories
5 Read-only paths of important git repositories
6 Read-only filesystem
Performance
Cost
Very High Very High
Very High
High
High
Moderate
Moderate Moderate
Low
Critical
Low
Very High
Now let us consider the three cases previously mentioned. In
the first case, the IDS detected the intrusion and considered the
intrusion very likely. After computing the Pareto-optimal set, we
have three possible responses left (2, 4, and 5). The risk computed
is risk("high", "very likely") = high. The response selection then
prioritizes performance and selects the response 2 that sets the
8In our experiments, we used an exploit for the version 1.4.0 [66].
9One would have to assign a cost for other malicious behaviors, but for the sake of
conciseness we only show the relevant ones.
filesystem as read-only protecting all information stored by Gitea
(git repositories and its database). The only exception is the folder
used by Gitea to store sessions since having this folder read-only
would render the site unusable, thus it is a core function (see the cost
critical in Table 3). Gitea is restored with all the encrypted files. The
selected response prevents the attacker to reinfect the service since
the exploit require write accesses. In terms of quality of service,
users can connect to the service and clone repositories, but due
to the response a new user cannot register and users cannot push
to repositories. Hence, this response is adequate since the service
cannot get reinfected, core functions are maintained, and other
functions previously mentioned are available.
In the second case, the IDS detected the intrusion but considered
the intrusion very unlikely while the attacker managed to infect the
service. The risk computed is risk("high", "very unlikely") = low.
The response selection then prioritizes cost and selects the response
5 that sets a subset of git repositories (the most important ones
for the organization) as read-only. With this response, the attacker
managed to reinfect the service and the ransomware encrypted
many repositories, but not the most important ones. In terms of
quality of service, users can still access the protected repositories,
but due to the intrusion users cannot login anymore and they cannot
clone the encrypted repositories (i.e., Gitea shows an error to the
user). Hence, the response is less adequate when the IDS provides an
incorrect value for the likelihood of the intrusion, since the malware
managed to encrypt many repositories, but the core functions of
Gitea are maintained.
In the third case, the IDS detected an intrusion with the likeli-
hood being very unlikely, but it is in fact a false positive. The risk
computed is risk("high", "very unlikely") = low. It is similar to the
previous case where the response selected is response 5 due to a
low risk. However, in this case, there is no actual ransomware. In
terms of quality of service, users have access to most functions (e.g.,
login, clone all repositories, or add issues), they just cannot push
modifications to the protected repositories. It shows that even with
false positives, our approach minimizes the impact on the quality
of service. Once an analyst classified the alert as a false positive,
the administrator can make the service leave the degraded mode.
7.3 Availability Cost
In this subsection, we detail the experiments that evaluate the
availability cost for the checkpoint and restore procedures.
7.3.1 Checkpoint. Each time we checkpoint a service, we freeze its
processes. As a result, a user might notice a slower responsiveness
from the service. Hence, we measured the time to checkpoint four
common services: Apache HTTP server (v2.4.33), nginx (v1.12.1),
mariadb (v10.2.16), and beanstalkd (v1.10). We repeated the experi-
ment 10 times for each service. In average the time to checkpoint
was always below 300 ms. Table 6 of Appendix B.3 gives more
detailed timings.
The results show that our checkpointing has a small, but ac-
ceptable availability cost. We do not lose any connection, we only
increase the requests’ latency when the service is frozen. Since the
latency increases only for a small period of time (maximum 300 ms),
we consider such a cost acceptable. In comparison, SHELF [74]
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Chevalier et al.
incurs a 7.6 % latency overhead for Apache during the whole exe-
cution of the system.
7.3.2 Restore. We also evaluated the time to restore the same ser-
vices used in section 7.3.1. In average, it took less than 325 ms.
Table 7 of Appendix B.3 gives more detailed timings.
In contrast to the checkpoint, the restore procedure loses all
connections due to the kill operation. The experiment, however,
show that the time to restore a service is small (less than 325 ms).
For example, in comparison, CRIU-MR [71] took 2.8 s in average to
complete their restoration procedure.
7.4 Monitoring Cost
As detailed in section 6.3, our solution logs the path of any file
modified by a monitored service. This monitoring, however, incurs
an overhead for every process executing on the system (i.e., even for
the non-monitored services). There is also an additional overhead
for monitored services that perform write accesses due to the audit
event generated by the kernel and then processed by our daemon.
Therefore, we evaluated both overhead by running synthetic and
real-world workload benchmarks, from the Phoronix test suite [40],
for three different cases:
(1) no monitoring is present (baseline)
(2) monitoring rule enabled, but the service running the bench-
marks is not monitored (no audit events are triggered)
(3) monitoring rule enabled and the service is monitored (audit
events are triggered).
Synthetic Benchmarks. We ran synthetic I/O benchmarks
7.4.1
that stress the system by performing many open, read, and write
system calls: compilebench [44], fs-mark [73], and Postmark [30].
compilebench emulates disk I/O operations related to the compi-
lation of a kernel tree, reading the tree, or its creation. fs-mark
creates files and directories, at a given rate and size, either syn-
chronously or asynchronously. Postmark emulates an email server
by performing a given number of transactions that create, read,
append to, and delete files of varying sizes.
The results of the read compiled tree test of the compilebench
benchmark confirmed that the overhead is only due to open sys-
tem calls with write access mode. This test only reads files and we
did not observe any noticeable overhead (less than 1 %, within the
margin of error).
We now focus on the results of the fs-mark and Postmark bench-
marks, illustrated in Figure 3. In both experiments, we notice a
small overhead when the service is not monitored (between 0.6 %
and 4.5 %). With fs-mark (Figure 3a), when writing 1000 files syn-
chronously, we observe a 7.3 % overhead. In comparison, when the
files are written asynchronously, there is a 27.3 % overhead. With
Postmark (Figure 3b), we observe a high overhead (28.7 %) when it
writes many small files (between 5 KiB and 512 KiB) but remains
low (3.1 %) with bigger files (between 512 KiB and 1 MiB).
In summary, these synthetic benchmarks show that the worst
case for our monitoring is when a monitored service writes many
small files asynchronously in burst.
7.4.2 Real-world Workload Benchmarks. To have a different per-
spective than the synthetic benchmarks, we chose two benchmarks
that use real-world workloads: build-linux-kernel measures the
No monitoring (baseline)
Monitoring rule enabled, but service not monitored
Monitoring rule enabled and service monitored
d
n
o
c
e
s
r
e
p
d
e
t
a
e
r
c
s
e
l
i
F
1600
1400
1200
1000
100
75
50
25
0
5000 Files,
1 MiB Size,
4 Threads
1000 Files,
1 MiB Size
Parameters
1000 Files,
1 MiB Size,
No Sync/FSync
(a) Files created per second with the fs-mark benchmark
(more is better)
d
n
o
c
e
s
r
e
p
s
n
o
i
t
c
a
s
n
a
r
T
2500
2000
1500
1000
500
0
250k transactions,
500 files between
5 KiB and 512 KiB
250k transactions,
500 files between
512 KiB and 1 MiB
Parameters
(b) Transactions per second with the Postmark benchmark
(more is better)
Figure 3: Results of synthetic benchmarks to measure the
overhead of the monitoring
time to compile the Linux kernel10 and unpack-linux measures
the time to extract the archive of the Linux kernel source code.
When the service is monitored, the overhead is only significant
with unpack-linux where we observe a 23.7 % overhead. It con-
curs with our results from the synthetic benchmarks: writing many
small files asynchronously incurs a significant overhead when the
service is monitored (the time to decompress a file in this bench-
mark is negligible). With build-linux-kernel, we observe a small
overhead (1.1 %) even when the service is monitored (the time to
compile the source code masks the overhead of the monitoring).
In comparison, SHELF [74] has a 65 % overhead when extracting
the archive of the Linux kernel source code, and an 8 % overhead
when building this kernel.
In conclusion, both the synthetic and non-synthetic benchmarks
show that our solution is more suitable for workloads that do not
write many small files asynchronously in burst. For instance, our
approach would be best suited to protect services such as web,
databases, or video encoding services.
10While build-linux-kernel is CPU bound, it also performs many system calls, such
as opening files to store the output of the compilation.
Survivor: A Fine-Grained Intrusion Response and Recovery Approach for Commodity Operating Systems
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
7.5 Storage Space Overhead
Checkpointing services requires storage space to save the check-
points. To evaluate the disk usage overhead, we checkpointed the
same four services used in section 7.3.1. Each checkpoint took re-
spectively 26.2 MiB, 7.5 MiB, 136.0 MiB, and 130.1 KiB of storage
space. The memory pages dumps took at least 95.3 % of the size of
their checkpoint. Hence, if a service uses more memory under load
(e.g., Apache), its checkpoint would take more storage space.
7.6 Stability of Degraded Services
We tested our solution on web servers (nginx, Apache), databases
(mariadb), work queues (beanstalkd), message queues (mosquitto),
and git hosting services (gitea). None of the services crashed when
restored with a policy that removed privileged that they required
(i.e., in a degraded mode). The reason is twofold.
First, we provided a policy that specified the responses with
a critical cost. Therefore, our solution never selected a response
that removes a privilege needed by a core function. Second, the
services checked for errors when performing various operations.
For example, if a service needed a privilege that we removed, it
tried to perform the operation and failed, but only logged an error
and did not crash. If we generalize our results, it means our solution
will not make other services (that we did not test) crash if they
properly check for error cases. This practice is common, and it is
often highlighted by the compiler when this is not the case.
8 DISCUSSION
Let us now discuss non-exhaustively limitations or areas that would
need further work.
False Positives Our approach relies on an IDS, hence we inherit
its limitations. In the case of false positives, the recovery and
response procedures would impact the service availability,
despite thwarting no threat. Our approach, however, mini-