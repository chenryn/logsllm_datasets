title:SensorSift: balancing sensor data privacy and utility in automated
face understanding
author:Miro Enev and
Jaeyeon Jung and
Liefeng Bo and
Xiaofeng Ren and
Tadayoshi Kohno
SensorSift: Balancing Sensor Data Privacy and Utility in
Automated Face Understanding
Miro Enev*, Jaeyeon Jung**, Liefeng Bo*, Xiaofeng Ren*, and Tadayoshi Kohno*
*Department of Computer Science and Engineering, University of Washington
**Microsoft Research, Redmond WA
ABSTRACT
We introduce SensorSift, a new theoretical scheme for balancing
utility and privacy in smart sensor applications. At the heart of our
contribution is an algorithm which transforms raw sensor data into
a ‘sifted’ representation which minimizes exposure of user deﬁned
private attributes while maximally exposing application-requested
public attributes. We envision multiple applications using the same
platform, and requesting access to public attributes explicitly not
known at the time of the platform creation. Support for future-
deﬁned public attributes, while still preserving the deﬁned privacy
of the private attributes, is a central challenge that we tackle.
To evaluate our approach, we apply SensorSift to the PubFig
dataset of celebrity face images, and study how well we can si-
multaneously hide and reveal various policy combinations of face
attributes using machine classiﬁers.
We ﬁnd that as long as the public and private attributes are not
signiﬁcantly correlated, it is possible to generate a sifting transfor-
mation which reduces private attribute inferences to random guess-
ing while maximally retaining classiﬁer accuracy of public attributes
relative to raw data (average PubLoss = .053 and PrivLoss = .075,
see Figure 4). In addition, our sifting transformations led to consis-
tent classiﬁcation performance when evaluated using a set of ﬁve
modern machine learning methods (linear SVM, kNearest Neigh-
bors, Random Forests, kernel SVM, and Neural Nets).
Categories and Subject Descriptors
K.4 [Computers and Society]: Public Policy Issues—Privacy; I.2
[Artiﬁcial Intelligence]: Vision and Scene Understanding—Mod-
eling and recovery of physical attributes; I.5 [Pattern Recogni-
tion]: Models—Statistical, Neural Nets; G.1 [Numerical Analy-
sis]: Optimization—Least squares methods
1.
INTRODUCTION
The minimal costs of digital sensors, global connectivity, com-
puter cycles, in addition to advances in machine learning algo-
rithms, have made our world increasingly visible to intelligent com-
puters. The synergy of sensing and AI has unlocked exciting new
research horizons and led to qualitative improvements in human-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’12 Dec. 3-7, 2012, Orlando, Florida USA
Copyright 2012 ACM 978-1-4503-1312-4/12/12 ...$15.00.
149
Table 1: Data sharing models in sensor applications (all terms
deﬁned in Section 3).
Platform
sensor data
sensor data,
get features
sensor data,
sift gen., verify
M1
M2
S.Sift
Application
get features,
classify, app. logic
classify,
app. logic
classify,
app. logic
Tradeoffs
Innovation++
Privacy-
Innovation-
Privacy++
Innovation+
Privacy+
computer interaction. However, alongside these positive develop-
ments, novel privacy threats are emerging as digital traces of our
lives are harvested by 3rd parties with signiﬁcant analytical re-
sources. As a result, there is a growing tension between utility and
privacy in emerging smart sensor ecosystems.
In the present paper we seek to provide a new direction for bal-
ancing privacy and utility in smart sensor applications. We are mo-
tivated towards this goal by the limitations in the current models of
data access in smart sensing applications.
At present there are two conventional modes of data sharing in
smart sensing applications and they are either risk carrying or arbi-
trarily stiﬂing:
In the ﬁrst mode, an application is given access to all of the raw
data produced by a sensor (i.e., the Kinect); the application is then
free to do feature extraction, classiﬁcation, and run the logic that
powers its functionality. Although this model of data sharing is
great for innovation it leads to a sacriﬁce in privacy (Table 1, ﬁrst
row M1).
In the second mode, an application is given access to some re-
stricted set of API calls deﬁned by the platform (i.e., Apple’s iOS)
which restrict access to the raw data produced by a sensor; the ap-
plication can still perform classiﬁcation and run its logic, however it
no longer has direct access to the data. The beneﬁt of this approach
is that privacy can be signiﬁcantly increased, however innovation is
signiﬁcantly diminished (Table 1, second row M2). Given the lim-
itations of these interaction modes, we seek to ﬁnd a new model of
sensor data access which balances application innovation and user
privacy. To this end we develop an information processing scheme
called SensorSift which allows users to specify their privacy goals
by labeling attributes as private and subsequently enabling applica-
tions to use privacy-preserving data access functions called ‘sifts’
which reveal some non-sensitive (public) attributes from the data
(Table 1, third row S.Sift).
Our tool is designed to be understandable and customizable by
consumers while defending them from emerging privacy threats
based on automated machine inferences. At the same time this tool
enables applications access to non-private data in a ﬂexible fash-
ion which supports developer innovation. Importantly, while the
private attributes must be chosen from a supported list (to enable
data protection assurances) the public attributes requested by ap-
plications do not need to be known in advance by the SensorSift
platform and can be created to meet changing developer demands.
Rather than developing a speciﬁc system instance, in this paper
we tackle the challenge of protecting sensitive data aspects while
exposing non-sensitive aspects. We overcome this challenge by
introducing a novel algorithm to balance utility and privacy in sen-
sor data and propose how to embed it in an information process-
ing scheme which could be applied as part of a multi-application
trusted platform.
Towards Privacy and Flexibility in Sensor Systems. Suppose
that an application running on a camera-enabled entertainment sys-
tem (like the Kinect) wishes to determine Alice’s gender to person-
alize her avatar’s virtual appearance. Suppose also that Alice (the
user) has speciﬁed that race information should not be available to
applications. At present, Alice can either avoid using the applica-
tion (and thus sacriﬁce utility) or choose to use the application and
forfeit her ability to ensure privacy.
A natural solution to this tension would be to allow data access
which is based on pre-deﬁned public and private attributes. While
workable for well-known attributes like race and gender, this ap-
proach limits innovation as developers are restricted to the pre-
deﬁned public attributes. Under the SensorSift scheme, applica-
tions can opt to use standard public and private attributes or can
propose novel public attributes not known by the platform in ad-
vance (private attributes are still deﬁned by the system in advance
and exposed to users as options).
Returning to our example, on a SensorSift supporting platform
Alice can specify race as a private attribute. The system would then
transform the raw camera data samples to adhere to this policy by
maximally removing race information while exposing application-
desired attributes. These public attributes could be anything deﬁned
by the developers — including attributes not known to the platform
designers; for simplicity of exposition, however, we’ll use gender
as the public attribute.
The transformed sensor data would only be made available to the
application if the system successfully veriﬁes (using an ensemble
of state-of-the-art classiﬁers) that the sifted data cannot be used to
recognize the private attribute signiﬁcantly beyond random guess-
ing. If the sift is veriﬁed, the target application would receive the
transformed data which could then be post-processed to infer the
gender value.
Concept Overview. Given a particular sensor context (e.g., op-
tical/image data) and ﬁxed set of data features (e.g., RGB pixel
values) the information ﬂow through our scheme is as follows:
users deﬁne private attributes and applications deﬁne (request) pub-
lic attributes; developers use provided tools to generate a candidate
transformation (sift) which attempts to expose the [arbitrarily cho-
sen] public attribute(s) but not the speciﬁed private aspects of the
data; the user’s system checks the quality of the proposed sift using
an ensemble of classiﬁers; and lastly, if the veriﬁcation is success-
ful the application is allowed access to the transformed data.
Typically we expect that the SensorSift platform will ship with
many valid sifts that cater to standard application demands. More
importantly, however, we offer support for application-supplied sift
transformations which would be veriﬁed by the platform either at
installation time or when the user changes his or her privacy pref-
erences. Once a particular sift has been invoked and successfully
veriﬁed it will be applied to each sensor data release. In the case
150
where an application is using a known policy (standard public and
private attributes) the platform can automate classiﬁcation and sim-
ply release an attribute label. Alternatively, if the application needs
access to a novel public attribute it will need to independently clas-
sify the sifted data it receives.
Evaluation and Results. To evaluate our approach, we test how
well we can control the exposure of facial attributes in the PubFig
database of online celebrity photographs [11]. We leverage the face
image attribute scheme of Kumar et al. to provide a quantitative vo-
cabulary through which privacy policies can be deﬁned over facial
features [10]. We then choose a set of 90 policies composed using
10 facial attributes (e.g., male [gender], attractive woman, white
[race], youth [age], smiling, frowning, no eyewear, obstructed fore-
head, no beard, outdoors) and show that it is possible to success-
fully create data ‘sifts’ which remove selective facial characteristics
in a discriminating manner to produce high classiﬁcation accuracy
for public attributes and low accuracy for private attributes (average
PubLoss = .053 and PrivLoss = .075, see Figure 4).
In addition, our sifting transformations lead to consistent clas-
siﬁcation performance when evaluated using a set of ﬁve modern
machine learning methods (linear SVM, kNearest Neighbors, Ran-
dom Forests, kernel SVM, and Feed Forward Neural Networks).
As an extension we also show that our approach maintains privacy
when applied to complex policies (multiple public and/or private at-
tributes) as well as dynamic video (i.e., sequences of data releases).
To our knowledge our approach is the ﬁrst solution to veriﬁably
decompose face images into sensitive and non-sensitive features
when evaluated against state of the art machine classiﬁers. In ad-
dition, our framework enables on-demand computation and evalu-
ation of sifting functions so that privacy and utility balance can be
created for currently unknown but desired attributes (thus support-
ing application innovation).
2. THREAT AND USAGE MODELS
Smart sensing applications have already been adopted in numer-
ous life-improving sectors such as health, entertainment, and so-
cial engagement [2, 9]. Driven by the diminishing costs of digital
sensors, growth of computational power, and algorithmic advances
even richer sensing applications are on the horizon [1]. In most
instances, smart sensor applications create rewarding experiences
and assistive services, however, the gathered raw data also presents
signiﬁcant privacy risks given the amount of personal information
which modern algorithms can infer about an individual. The poten-
tial consequences of these risks are not fully understood given the
novelty of the enabling technologies. Nonetheless, we feel that it is
critical to develop ways of managing the information exposure in
smart sensor applications preemptively rather than reactively.
To mitigate potential privacy threats posed by automated reason-
ing applications we propose to employ automated defenses. At a
high level our scheme is intended to enable a quantitatively veri-
ﬁable trade off between privacy and utility in participatory smart
application contexts. A full description of SensorSift is provided
in Section 3, yet intuitively, our goal is to create a trusted clearing-
house for data which transforms raw sensor captures into a sifted
(or sanitized) form to maximally fulﬁll the privacy and utility de-
mands in policies composed of user selected private attributes and
application requested public attributes.
We envision a model in which applications are untrustworthy
but, in general, not colluding (we discuss collusion in Section 9).
Applications might be malicious and explicitly targeting the ex-
posure of private attributes; more likely, however, they are well-
intentioned applications that fail to adequately protect the data that
that many applications will opt to operate in this mode, especially
if the list of platform included policies is large and frequently up-
dated.
In some cases, the included list of attributes may not be sufﬁ-
cient to enable the application developers’ functionality and utility
goals. Whenever this is the case, the application interacts with the
platform in unincluded policy mode (Figure 1 bottom panel). In
this mode the user has selected some private attribute(s) Y− (e.g.,
age) and the application is requesting access to some novel pub-
lic attribute(s) Y + (e.g., imagine that eye color is a novel public
attribute). Since support for this new policy is not included by
the platform it is up to the application to provide a candidate sift
(or data access function F) which can be applied to sensor data to
balance the removal of private attribute information with the reten-
tion of application desired non-sensitive (public) data features. The
proposed sift will only be allowed to operate on the sensor data if
it can be successfully veriﬁed to not expose information about the
private attribute(s) speciﬁed in the policy. While this scenario is
more challenging from an application perspective, it is also more
ﬂexible and offers a way to meet the rapidly evolving demands of
software developers.
Below we focus our discussion on the usage model for unin-
cluded policies as it is unique to our approach and highlights all of
the SensorSift framework’s subcomponents.
Sift Generation. To create a candidate sifting function for an un-
included policy, applications can use our PPLS algorithm (deﬁned
in Section 4), develop their own method, or potentially use pre-
veriﬁed sifts (e.g., crowd sourcing repositories). Code and docu-
mentation for the PPLS sifting generating function are freely avail-
able at http://homes.cs.washington.edu/~miro/sensorSift .
To use the PPLS algorithm, developers need to provide a dataset
of sensor data (e.g., face images in our experiments) with binary
labels for the public and private attributes. To facilitate the genera-
tion of this prerequisite labeled dataset, we imagine that developers
will leverage freely available data repositories or use services such
as Mechanical Turk.
Sift Veriﬁcation. Once a candidate sift function has been provided
to the platform, SensorSift must ensure that the proposed transfor-
mation function does not violate the user’s privacy preferences. In-
deed, there is no guarantee that a malicious application developer
did not construct a sifting transformation function explicitly de-
signed to violate a user’s privacy. To verify that the transformation
is privacy-preserving, SensorSift will invoke an ensemble of classi-
ﬁers ML on the sifted outputs of an internal database DB to ensure
that private attributes cannot be reliably inferred by the candidate
sift. We discuss these components in more detail below.
Veriﬁcation: Internal Dataset. The basis upon which we verify
privacy assurances is a DBveri f y dataset of sensor samples (i.e. face
images) which would be distributed with each SensorSift install.
For our purposes, we assume that the dataset is in matrix format X
with n rows and d columns, where n is the number of unique sam-
ples (i.e., face images), and d is the dimensionality of each sample
(i.e., face features). Large datasets with higher feature dimension-
ality offer attractive targets since they are more likely to capture
real world diversity and produce stronger privacy assurances.
Veriﬁcation: Classiﬁer Ensemble. The second part of the ver-