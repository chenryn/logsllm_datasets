11
2
β
1
0.1
0.01 1.1 ln 1.1 ≈ 0.095
ln 11 ≈ 2.40
ln 2 ≈ 0.69
δ
10−5
10−6
10−7
ǫ
1
β
1
0.159
0.1
0.01 0.017
Table 1: Eﬀect of privacy parameters under sam-
pling.
answer one way or the other. Then, one algorithm can out-
put that predicate, and a second algorithm can output that
predicate XOR’ed with a predicate that results in privacy
leakage; and they do not compose.
The above observation suggests that no deﬁnition that
exploits uncertain background knowledge should be used in
the interactive setting of answering multiple queries. If, how-
ever, one intends to publish a dataset in the non-interactive
setting only once, then the inability to compose may be an
acceptable limitation.
2.2 Differential Privacy under Sampling
One natural approach to capturing the adversary’s uncer-
tainty about the input data is to add a sampling step. We
introduce the following deﬁnition, called (β, ǫ, δ)-Diﬀerential
Privacy under Sampling ((β, ǫ, δ)-DPS for short).
Definition 3
(Differential privacy under sampling).
An algorithm A gives (β, ǫ, δ)-DPS if and only if the algo-
rithm Aβ gives (ǫ, δ)-DP, where Aβ denotes the algorithm
to ﬁrst sample with probability β (include each tuple in the
input dataset with probability β), and then apply A to the
sampled dataset.
In Section 3, we show that existing k-anonymization
algorithms that have privacy vulnerabilities do not sat-
isfy (β, ǫ, δ)-DPS;
further, safe (and possibly determinis-
tic) k-anonymization satisﬁes (β, ǫ, δ)-DPS, while violating
(ǫ, δ)-DP for any δ  eǫ Pr[A2(Aβ
1 (D′)) ∈ O] + δ
1 (D) = S] and p′(S) =
Consider all S’s in Range(A1), let q(S) =
Pr[A2(S) ∈ O], and let p(S) = Pr[Aβ
Pr[Aβ
1 (D′) = S]. Then we have
PS∈Range(A1) p(S)q(S) > eǫPS∈Range(A1) p′(S)q(S) + δ.
We partition Range(A1) into S1 = {S | p(S) > eǫp′(S)}
and S2 = {S | p(S) ≤ eǫp′(S)}. Rewriting the above in-
equality, we have
p(S)q(S)
PS∈S1
p(S)q(S) +PS∈S2
> eǫPS∈S1 p′(S)q(S) + eǫPS∈S2 p′(S)q(S) + δ
p(S)q(S) ≤
p′(S)q(S). Subtracting the above from previous,
Consider the sum over S2, we have PS∈S2
eǫPS∈S2
we have PS∈S1 p(S)q(S) > eǫPS∈S1 p′(S)q(S) + δ.
For each S ∈ S1, we have p(S)(1 − q(S)) > eǫp′(S)(1 −
p′(S)(1 −
p(S)(1 − q(S)) > eǫPS∈S1
q(S)), and thus PS∈S1
q(S)). Summing up the above two inequalities, we have
p(S) > eǫPS∈S1
satisﬁes (β, ǫ, δ)-DPS.
PS∈S1
p′(S) + δ. This contradicts that A1
Theorem 3. Given two algorithms A1 and A2 that both
satisfy (β, ǫ, δ)-DPS, for any p ∈ [0, 1], let Ap(D) be the
algorithm that outputs A1(D) with probability p and A2(D)
with probability 1 − p, then Ap satisﬁes (β, ǫ, δ)-DPS.
Proof. Since both A1 and A2 satisfy (β, ǫ, δ)-DPS, for
any pair of neighboring datasets D and D′ and for any O ∈
Range(A1) ∪ Range(A2), we have
Pr[Ap(D) ∈ O]
= p Pr[A1(D) ∈ O] + (1 − p) Pr[A2(D) ∈ O]
≤ p(eǫ Pr[A1(D′) ∈ O] + δ)
+(1 − p)(eǫ Pr[A2(D′) ∈ O] + δ)
= eǫ(p Pr[A1(D′) ∈ O] + (1 − p) Pr[A2(D′) ∈ O]) + δ
= eǫ Pr[Ap(D′) ∈ O] + δ.
Therefore, the algorithm Ap also satisﬁes (β, ǫ, δ)-DPS.
2.5 Beneﬁting from Sampling
We observe that in many data publishing scenarios, ran-
dom sampling is an inherent step. For example, the census
bureau publishes a 1-percent microdata sample. In many re-
search settings (such as when Netﬂix wants to publish movie
ratings), it is suﬃcient to publish a random sample of the
dataset. Many times, even when the dataset is not the re-
sult of explicit sampling, one can view it as result of implicit
sampling, because the process of selecting respondents in-
volves randomness.
Intuitively, uncertainty resulted from
such explicit and implicit sampling provides some degree of
privacy protection.
The notion of Diﬀerential Privacy under Sampling pro-
vides two approaches such that one can beneﬁt from sam-
pling. The ﬁrst approach is to apply Theorem 1, and obtain
an enhanced privacy budget. For example, suppose that one
wants to satisfy ǫ-DP for ǫ = 0.1, but one knows that the
dataset is resulted from 1% random sampling, then one can
instead use the privacy budget ǫ = ln(100(e0.1 − 1) + 1) ≈
2.44, a 24-fold increase. While this beneﬁt of sampling is
known, we provide a general formula to quantify this eﬀect
for all values of (ǫ, δ). This approach of beneﬁting from
sampling can be applied in both the interactive or the non-
interactive setting.
In the second approach, if one knows the dataset to be
published is resulted from sampling, and aims at publish
the dataset in a non-interactive setting, then one can apply a
mechanism that satisﬁes DPS, but not (ǫ, δ)-DP. One exam-
ple of such a mechanism is a safe k-anonymization method,
which we will study in Section 3. In this paper, we provide
theoretical justiﬁcation of this approach. Comparing the
utility of this approach against that of other diﬀerentially
private approaches is beyond the scope of this paper.
3. SAFE K-ANONYMIZATION MEETS
DIFFERENTIAL PRIVACY
In this section we show that k-anonymization, when per-
formed in a “safe” way, satisﬁes (β, ǫ, δ)-DPS. That is, safe k-
anonymization, when preceded by a random sampling step,
satisﬁes (ǫ, δ)-diﬀerential privacy.
3.1 An Analysis of k-Anonymity
The development of k-anonymity was motivated by a well
publicized privacy incident [30]. The Group Insurance Com-
mission (GIC) published a supposedly anonymized dataset
recording the medical visits of patients managed under its
insurance plan. While the obvious personal identiﬁers (such
as name and address) were removed, the published data in-
cluded zip code, date of birth, and gender, which are suﬃ-
cient to uniquely identify a signiﬁcant fraction of the popula-
tion. Sweeney [30] showed that by correlating this data with
the publicly available Voter Registration List for Cambridge
Massachusetts, medical visits for many individuals can be
easily identiﬁed, including those of William Weld, a former
governor of Massachusetts. We note that even without ac-
cess to the public voter registration list, the same privacy
breaches can occur. Many individuals’ birthdate, gender and
zip code are public information. This is especially the case
with the advent of social media, including Facebook, where
users share seemingly innocuous personal information to the
public. The GIC re-identiﬁcation attack directly motivated
the development of the k-anonymity privacy notion.
Definition 4. [k-Anonymity,
no-
tion] [30]: A published table satisﬁes k-anonymity relative
to a set of QID attributes if and only if when the table is
projected to include only the QIDs, every tuple appears at
least k times.
the
privacy
Quasi-identiﬁers vs. Sensitive Attributes? A ﬁrst
problem with Deﬁnition 4 is that it requires the division
of all attributes into quasi-identiﬁers (QIDs) and sensitive
attributes (SA), where the adversary is assumed to know the
QIDs, but not SAs. This separation, however, is very hard
to obtain in practice. Even though only some attributes are
used in the GIC incident, it is diﬃcult to assume that they
are the only QIDs. Other attributes in the GIC data include
visit date, diagnosis, etc. There may well exist an adversary
who knows this information about some individuals, and
if with this knowledge these individuals’ record can be re-
identiﬁed, it is still a serious privacy breach.
The same diﬃculty is true for publishing any kind of
census, medical, or transactional data. When publishing
anonymized microdata, one has to defend against all kinds
of adversaries, some know one set of attributes, and others
know diﬀerent sets. An attribute about one individual may
be known by some adversaries, and unknown (and should
be considered sensitive) for other adversaries.
Any separation between QIDs and SAs is essentially mak-
ing assumptions about the adversary’s background knowl-
edge that can be easily violated, rendering any privacy pro-
tection invalid. Hence we consider a strengthened version
of k-anonymity by treating all attributes as QIDs. This is
stronger than using any subset of attributes as QIDs. This
strengthened version of k-anonymity avoids making assump-
tion about the adversary’s background knowledge about
which attributes are known and what are not. This has
been used in the context of anonymizing transaction data
[16].
Weakness of the k-Anonymity Notion. With the
strengthened version of k-anonymity, one might expect that
it should stop re-identiﬁcation attacks. To satisfy this no-
tion, each tuple in the output is blended in a group of at
least k tuples that are the same. This follows the appealing
principle that “privacy means hiding in a crowd”. The intu-
ition is that as there are at least k − 1 other tuples that look
exactly the same, one cannot re-identify which tuple in the
output corresponds to an individual with probability over
1/k. Unfortunately, this intuition turns out to be wrong.
Only making the syntactic requirement that each tuple ap-
pears at least k times does not protect privacy, as a trivial
way to satisfy this is to select some tuples from the input
and then duplicate each of them k times.
k-Anonymity vs.
k-Anonymization Algorithms.
Here we would like to make a clear distinction between the
k-anonymity, the privacy notion, and k-anonymization al-
gorithms.
Many k-anonymization algorithms have been developed
in the literature. Given input datasets, they aim at produc-
ing anonymized versions of the input datasets that satisfy
k-anonymity. That the k-anonymity privacy notion is weak
means that producing outputs that satisfying k-anonymity
alone is insuﬃcient for privacy protection. However, this
does not automatically mean that all k-anonymization have
privacy vulnerabilities. We now show that the algorithms