simple and standard for many DNNs. For example, in image
classiﬁcation, networks often start with a convolution layer.
In other cases, DNNs start with fully connected layer. These
linear initial layers can be analyzed and their sensitivity
computed as follows.
For linear layers, which consist of a linear operator with
matrix form W ∈ Rm,n, the sensitivity is the matrix norm,
deﬁned as: (cid:4)W(cid:4)p,q = supx:(cid:6)x(cid:6)p≤1 (cid:4)W x(cid:4)q. Indeed,
the
≤
(cid:6)W x(cid:6)q
deﬁnition and linearity of W directly imply that
(cid:6)x(cid:6)p
(cid:23)(cid:23)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
(cid:4)W(cid:4)p,q, which means that: Δp,q = (cid:4)W(cid:4)p,q. We use the
following matrix norms [64]: (cid:4)W(cid:4)1,1 is the maximum 1-
norm of W ’s columns; (cid:4)W(cid:4)1,2 is the maximum 2-norm of
W ’s columns; and (cid:4)W(cid:4)2,2 is the maximum singular value
of W . For ∞-norm attacks, we need to bound (cid:4)W(cid:4)∞,1 or
(cid:4)W(cid:4)∞,2, as our DP mechanisms require q ∈ {1, 2}. How-
ever, tight bounds are computationally hard, so we currently
m(cid:4)W(cid:4)∞,∞
use the following bounds:
where (cid:4)W(cid:4)∞,∞ is the maximum 1-norm of W ’s rows.
While these bounds are suboptimal and lead to results that
are not as good as for 1-norm or 2-norm attacks, they allow
us to include ∞-norm attacks in our frameworks. We leave
the study of better approximate bounds to future work.
n(cid:4)W(cid:4)2,2 or
√
√
(f2)
p,r Δ
≤ Δ
For a convolution layer, which is linear but usually not
expressed in matrix form, we reshape the input (e.g. the
image) as an Rndin vector, where n is the input size (e.g.
number of pixels) and din the number of input channels
(e.g. 3 for the RGB channels of an image). We write the
convolution as an Rndout×ndin matrix where each column
has all ﬁlter maps corresponding to a given input channel,
and zero values. This way, a “column” of a convolution
consists of all coefﬁcients in the kernel that correspond to a
single input channel. Reshaping the input does not change
sensitivity.
Option 3: Noise Deeper in the Network. One can con-
sider adding noise later in the network using the fact that
when applying two functions in a row f1(f2(x)) we have:
(f1◦f2)
(f1)
r,q . For instance, ReLU has a sensitivity
Δ
of 1 for p, q ∈ {1, 2,∞}, hence a linear layer followed
p,q
by a ReLU has the same bound on the sensitivity as the
linear layer alone. However, we ﬁnd that
this approach
for sensitivity analysis is difﬁcult to generalize. Combining
bounds in this way leads to looser and looser approxima-
tions. Moreover, layers such as batch normalization [28],
which are popular in image classiﬁcation networks, do not
appear amenable to such bounds (indeed, they are assumed
away by some previous defenses [12]). Thus, our general
recommendation is to add the DP noise layer early in the
network – where bounding the sensitivity is easy – and
taking advantage of DP’s post-processing property to carry
the sensitivity bound through the end of the network.
Option 4: Noise in Auto-encoder. Pushing this reasoning
further, we uncover an interesting placement possibility that
underscores the broad applicability and ﬂexibility of our
approach: adding noise “before” the DNN in a separately
trained auto-encoder. An auto-encoder is a special form of
DNN trained to predict its own input, essentially learning the
identity function f (x) = x. Auto-encoders are typically used
to de-noise inputs [60], and are thus a good ﬁt for PixelDP.
Given an image dataset, we can train a (, δ)-PixelDP auto-
encoder using the previous noise layer options. We stack it
before the predictive DNN doing the classiﬁcation and ﬁne-
tune the predictive DNN by running a few training steps
on the combined auto-encoder and DNN. Thanks to the
decidedly useful post-processing property of DP, the stacked
DNN and auto-encoder are (, δ)-PixelDP.
This approach has two advantages. First, the auto-encoder
can be developed independently of the DNN, separating the
concerns of learning a good PixelDP model and a good
predictive DNN. Second, PixelDP auto-encoders are much
smaller than predictive DNNs, and are thus much faster to
train. We leverage this property to train the ﬁrst certiﬁed
model for the large ImageNet dataset, using an auto-encoder
and the pre-trained Inception-v3 model, a substantial relief
in terms of experimental work (§IV-A).
C. Training Procedure
The soundness of PixelDP’s certiﬁcations rely only on
enforcing DP at prediction time. Theoretically, one could
remove the noise layer during training. However, doing so
results in near-zero certiﬁed accuracy in our experience.
Unfortunately, training with noise anywhere except in the
image itself raises a new challenge:
left unchecked the
training procedure will scale up the sensitivity of the pre-
noise layers, voiding the DP guarantees.
To avoid this, we alter the pre-noise computation to keep
its sensitivity constant (e.g. Δp,q ≤ 1) during training. The
speciﬁc technique we use depends on the type of sensitivity
we need to bound, i.e. on the values of p and q. For Δ1,1,
Δ1,2, or Δ∞,∞, we normalize the columns, or rows, of
linear layers and use the regular optimization process with
ﬁxed noise variance. For Δ2,2, we run the projection step
described in [12] after each gradient step from the stochastic
gradient descent (SGD). This makes the pre-noise layers
Parseval tight frames, enforcing Δ2,2 = 1. For the pre-noise
layers, we thus alternate between an SGD step with ﬁxed
noise variance and a projection step. Subsequent layers from
the original DNN are left unchanged.
It is important to note that during training, we optimize
for a single draw of noise to predict the true label for a
training example x. We estimate E(A(x)) using multiple
draws of noise only at prediction time. We can interpret
this as pushing the DNN to increase the margin between
the expected score for the true label versus others. Recall
from Equation (4) that the bounds on predicted outputs give
robustness only when the true label has a large enough
margin compared to other labels. By pushing the DNN to
give high scores to the true label k at points around x
likely under the noise distribution, we increase E(Ak(x))
and decrease E(Ai(cid:4)=k(x)).
D. Certiﬁed Prediction Procedure
For a given input x, the prediction procedure in a tra-
ditional DNN chooses the arg max label based on the
score vector obtained from a single execution of the DNN’s
deterministic scoring function, Q(x). In a PixelDP network,
the prediction procedure differs in two ways. First, it chooses
the arg max label based on a Monte Carlo estimation
of the expected value of the randomized DNN’s scoring
(cid:23)(cid:23)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
(cid:2)
function, ˆE(A(x)). This estimation is obtained by invoking
A(x) multiple times with independent draws in the noise
layer. Denote ak,n(x) the nth draw from the distribution
of the randomized function A on the kth label, given x
(so ak,n(x) ∼ Ak(x)). In Lemma 1 we replace E(Ak(x))
with ˆE(Ak(x)) = 1
n ak,n(x), where n is the number of
n
invocations of A(x). We compute η-conﬁdence error bounds
to account for the estimation error in our robustness bounds,
treating each label’s score as a random variable in [0, 1].
We use Hoeffding’s inequality [25] or Empirical Bernstein
bounds [39] to bound the error in ˆE(A(x)). We then apply
a union bound so that the bounds for each label are all
valid together. For instance, using Hoeffding’s inequality,
1−η ) ≤
with probability η, ˆE
E(A(x)) ≤ ˆE(A(x)) +
lb(A(x)) (cid:3) ˆE(A(x)) −
(cid:2)
ub(A(x)).
1−η ) (cid:3) ˆE
ln( 2k
1
2n
ln( 2k
(cid:2)
1
2n
Second, PixelDP returns not only the prediction for x
(arg max(ˆE(A(x)))) but also a robustness size certiﬁcate
for that prediction. To compute the certiﬁcate, we extend
Proposition 1 to account for the measurement error:
Proposition 2. (Generalized Robustness Condition) Sup-
pose A satisﬁes (, δ)-PixelDP with respect to changes of
size L in p-norm metric. Using the notation from Propo-
sition 1 further let ˆEub(Ai(x)) and ˆElb(Ai(x)) be the η-
conﬁdence upper and lower bound, respectively,
for the
Monte Carlo estimate ˆE(Ai(x)). For any input x, if for some
k ∈ K,
ˆElb(Ak(x)) > e2 max
i:i(cid:4)=k
ˆEub(Ai(x)) + (1 + e)δ,
then the multiclass classiﬁcation model based on label
probabilities (ˆE(A1(x)), . . . , ˆE(AK(x))) is robust to attacks
of p-norm L on input x with probability ≥ η.
The proof is similar to the one for Proposition 1 and is
detailed in Appendix A. Note that the DP bounds are not
probabilistic even for δ > 0; the failure probability 1 −
η comes from the Monte Carlo estimate and can be made
arbitrarily small with more invocations of A(x).
Thus far, we have described PixelDP certiﬁcates as binary
with respect to a ﬁxed attack bound, L: we either meet
or do not meet a robustness check for L. In fact, our
formalism allows for a more nuanced certiﬁcate, which gives
the maximum attack size Lmax (measured in p-norm) against
which the prediction on input x is guaranteed to be robust:
no attack within this size from x will be able to change
the highest probability. Lmax can differ for different inputs.
We compute the robustness size certiﬁcate for input x as
follows. Recall from III-B that the DP mechanisms have a
noise standard deviation σ that grows in Δp,qL
. For a given
σ used at prediction time, we solve for the maximum L for
which the robustness condition in Proposition 2 checks out:

(cid:23)(cid:23)(cid:19)
Lmax = maxL∈R+ L such that
ˆE
lb(Ak(x)) > e2 ˆE
• σ = Δp,1L/ and δ = 0 (for Laplace) OR
• σ =
ub(Ai:i(cid:3)=k(x)) + (1 + e)δ AND either
2 ln(1.25/δ)Δp,2L/ and  ≤ 1 (for Gaussian).
(cid:2)
The prediction on x is robust to attacks up to Lmax, so
we award a robustness size certiﬁcate of Lmax for x.
We envision two ways of using robustness size certiﬁ-
cations. First, when it makes sense to only take actions
on the subset of robust predictions (e.g., a human can
intervene for the rest), an application can use PixelDP’s
certiﬁed robustness on each prediction. Second, when all
points must be classiﬁed, PixelDP gives a lower bound on
the accuracy under attack. Like in regular ML, the testing
set is used as a proxy for the accuracy on new examples.
We can certify the minimum accuracy under attacks up to
a threshold size T, that we call the prediction robustness
threshold. T is an inference-time parameter that can differ
from the construction attack bound parameter, L, that is
used to conﬁgure the standard deviation of the DP noise. In
this setting the certiﬁcation is computed only on the testing
set, and is not required for each prediction. We only need
the highest probability label, which requires fewer noise
draws. §IV-E shows that in practice a few hundred draws are
sufﬁcient to retain a large fraction of the certiﬁed predictions,
while a few dozen are needed for simple predictions.
IV. Evaluation
We evaluate PixelDP by answering four key questions:
Q1: How does DP noise affect model accuracy?
Q2: What accuracy can PixelDP certify?
Q3: What is PixelDP’s accuracy under attack and how does
it compare to that of other best-effort and certiﬁed
defenses?
Q4: What is PixelDP’s computational overhead?
We answer these questions by evaluating PixelDP on ﬁve
standard image classiﬁcation datasets and networks – both
large and small – and comparing it with one prior certi-
ﬁed defense [65] and one best-effort defense [37]. §IV-A
describes the datasets, prior defenses, and our evaluation
methodology; subsequent sections address each question in
turn.
Evaluation highlights: PixelDP provides meaningful certi-
ﬁed robustness bounds for reasonable degradation in model
accuracy on all datasets and DNNs. To the best of our
knowledge, these include the ﬁrst certiﬁed bounds for large,
complex datasets/networks such as the Inception network
on ImageNet and Residual Networks on CIFAR-10. There,
PixelDP gives 60% certiﬁed accuracy for 2-norm attacks up
to 0.1 at the cost of 8.5 and 9.2 percentage-point accuracy
degradation respectively. Comparing PixelDP to the prior
certiﬁed defense on smaller datasets, PixelDP models give
higher accuracy on clean examples (e.g., 92.9% vs. 79.6%
accuracy SVHN dataset), and higher robustness to 2-norm
attacks (e.g., 55% vs. 17% accuracy on SVHN for 2-norm
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
attacks of 0.5), thanks to the ability to scale to larger models.
Comparing PixelDP to the best-effort defense on larger
models and datasets, PixelDP matches its accuracy (e.g.,
87% for PixelDP vs. 87.3% on CIFAR-10) and robustness
to 2-norm bounded attacks.
A. Methodology
Datasets. We evaluate PixelDP on image classiﬁcation tasks
from ﬁve pubic datasets listed in Table I. The datasets are
listed in descending order of size and complexity for classiﬁ-
cation tasks. MNIST [69] consists of greyscale handwritten
digits and is the easiest to classify. SVHN [44] contains
small, real-world digit images cropped from Google Street
View photos of house numbers. CIFAR-10 and CIFAR-
100 [33] consist of small color images that are each centered
on one object of one of 10 or 100 classes, respectively.
ImageNet [13] is a large, production-scale image dataset
with over 1 million images spread across 1,000 classes.
Models: Baselines and PixelDP. We use existing DNN
architectures to train a high-performing baseline model for
each dataset. Table I shows the accuracy of the baseline
models. We then make each of these networks PixelDP
with regards to 1-norm and 2-norm bounded attacks. We
also did rudimentary evaluation of ∞-norm bounded attacks,
shown in Appendix D. While the PixelDP formalism can
support ∞-norm attacks, our results show that tighter bounds
are needed to achieve a practical defense. We leave the
development and evaluation of these bounds for future work.