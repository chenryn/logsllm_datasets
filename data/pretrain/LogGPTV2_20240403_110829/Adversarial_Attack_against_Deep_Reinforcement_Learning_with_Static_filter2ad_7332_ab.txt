this paper (DQN, TRPO (Schulman et al. 2015), and A3C),
TRPO and A3C seem to be more resistant to adversarial
attack.
Under the domain of Atari game, authors showed
that by adding human invisible noises to the original
clean game background can make the game unable to
work properly, and realize adversarial attack successfully.
Huang et al. (2017) gave a new attempt to take adversar-
ial research under the scenario of reinforcement learn-
ing, and this work proved that the adversarial attack still
exists in the domain of reinforcement learning. Moreover,
FGSM motivates a series of related research work, Miy-
ato et al. (2018) proposed a closely related mechanism to
compute the perturbation for a given image, and Kurakin
et al. (2016) named this algorithm as “Fast Gradient L2”
and also proposed a alternative of using (cid:9)∞ for normaliza-
tion which named as “Fast Gradient L∞”.
Start point-based adversarial attack on Q-learning (SPA)
Xiang et al. (2018) focused on the adversarial example-
based attack on a representative reinforcement learning
named Q-learning in automatic path finding. They pro-
posed a probabilistic output model based on the influ-
ence factors and the corresponding weights to predict the
adversarial examples under such scenario.
Calculating on four factors including the energy point
gravitation, the key point gravitation, the path gravitation,
and the included angle, a natural linear model is con-
structed to fit these factors with the weight parameters
computation based on the principal component analy-
sis(PCA) (Wold et al. 1987).
The main contribution for Xiang et al. is that they built a
model, which can generate the corresponding probabilis-
tic outputs for certain input points, and the probabilistic
output of our model refers to the possibility of interfer-
ence caused by interference point on the path of agent
pathfinding.
Xiang et al. proposed 4 factors to determine wether the
perturbation can impact the final result for the agent path
planning, which can be concluded as:
Formula expression
⎧
k(cid:7)
c−kc
√
eic = kc + i ∗ d(cid:7) ∗
⎪⎨
(k(cid:7)
c−kc)2+(k(cid:7)
eir = kr + i ∗ d(cid:7) ∗
1−
⎪⎩
(cid:6)
(cid:7)
r−kr )2
k(cid:7)
c−kc
√
(k(cid:7)
c−kc)2+(k(cid:7)
r−kr )2
(cid:8)2
d1i = |aic − kc| + |air − kr|,
(kc, kr) = k, (aic, air) = ai ∈ A
d2i = min{d2|d2 = |aic − zjc| + |air
−zjr|, zj∈Z1},(zjc, zjr)= zj, (aic, air)
= ai ∈ A
vka= (aic−kc, air−kr),vkt = (tc−kc,tr−kr)
cos θi=vka·vkt/|vka||vkt|, θi = arccos θi
Factor
Factor 1:
The energy
point
gravitation
Factor 2:
The key point
gravitation
Factor 3:
The path
gravitation
Factor 4:
The included
angle
For Factor 1 can be named as the energy point grav-
itation, which denotes that it is more successful if the
adversarial point k is the point on the key vector v. Fac-
tor 2 is the key point gravitation, which represents that
the closer adversarial point is to the key point k, the more
likely it is to cause interference. Factor 3 can be called as
the path gravitation, which denotes that the closer adver-
sarial point is to the initial path Z1, the more possible it is
to bring about obstruct. Meanwhile, factor 4 can be con-
cluded as the included angle, which represents that the
angle θ between the vector from the point k to the adver-
sarial point ai and the vector from the key point to the
goal t.
pjai = ω1 · aie + ω2 · d(cid:7)
Therefore, the probability for each adversarial point ai
can be concluded as
pai = 4(cid:9)
j=1
where ωi denotes the weight for each factor respectively.
Storing the pai for each point, and select the top 10 as the
adversarial point.
1i + ω3 · d(cid:7)
2i + ω4 · θ(cid:7)
(3)
i
Chen et al. Cybersecurity            (2019) 2:11 
Page 7 of 22
For this work, the adversarial examples can be found
successfully for the first time on Q-learning in path find-
ing and their model can make a satisfactory prediction
(e.g., Fig. 3). Under a guaranteed recall, the precision of
the proposed model can reach to 70% with the proper
parameter setting. By adding small obstacle points to
the original clean map, can interfere the agent’s path
finding. However, the experimental map size for this
work is 28 × 28, and there is no additional verifica-
tion for a larger maze map, which can be considered
to research in future works. However, Xiang et al. paid
attention to the adversarial attack problem in automatic
path finding under the scenario of reinforcement learn-
ing. Meanwhile this work own practical significance, as
the objective for this study is Q-learning which is the most
widely used and representative reinforcement learning
algorithm.
White-box based adversarial attack on DQN (WBA)
Based on the SPA algorithm introduced above, Bai et al.
(2018) proposed that they first use DQN to find the opti-
mal path, and analyzed the rules of DQN pathfinding.
They proposed a method that can effectively find vulner-
able points towards White-Box Q-table variation in DQN
pathfinding training. Meanwhile, they built a simulation
environment as a basic experiment platform to test their
method.
Moreover, they classified two types of vulnerable points.
(I) The vulnerable point is most likely on the boundary
line. Moreover, the smaller Q (the Q-value
difference between the right and downward
direction) is the more likely be a vulnerable point is.
For this characteristic of vulnerable pints, they proposed
a method to detect adversarial examples. Let P denotes
the set of points on the map P = {P1, P2, ..., Pn}, and each
point Pi obtains four Q-values Dij = (Qi1, Qi2, Qi3, Qi4)
respectively, which indicate up, down, right, and left.
Meanwhile, selecting the direction with the max Q-vale
f (Pi) = {j| maxj Qij}, and determining wether point Pi is
on the boundary line
ϕ(Pi) = OR(f (Pi)!= f (Pi1),f (Pi)!= f (Pi2),
f (Pi)!= f (Pi3), f (Pi)!= f (Pi4))
(4)
where Pij = {Pi1, Pi2, Pi3, Pi4} is the set of the adjoining
points for four directions of Pi, A = {a1, a2, ..., an} repre-
sents the points on boundary line. Calculating the Q-value
difference Q = |Qi2−Qi3|, and sorting Q ascending to
construct B = {b1, b2, ..., bn}. They took the first 3% of the
list as the smallest Q-value points. Finally got the set of
suspected adversarial examples, which can be concluded
as X = {x1, x − 2, ..., xn}, X = A (cid:10) B.
Fortheothertypeofvulnerablepoints can be concluded as:
(II) Adversarial examples are related to the gradient of
maximum Q-value for each point on the path.
Bai et al. found that when the Q-values of consecutive
two points fluctuate greatly, their gradient is greater and
they are more vulnerable to be attacked.
Meanwhile, they found that the larger angle between two
adjacent lines is, the greater slope of the straight line is. Set
angle between the direction vectors of two straight lines
to be θ
(cid:12)
, the function can be concluded as
(cid:11)
0 < θ < π
2
Fig. 3 An illustration of the interference effect before and after adding adversarial points when the path size is 2. We show two types of maps here,
where (a) denotes the first type, and (b), (c) all belong to the second category
Chen et al. Cybersecurity            (2019) 2:11 
Page 8 of 22
2
1
(cid:13)
(cid:13)
1 + p2
2 + p2
|m1m2 + n1n2 + p1p2|
1 + n2
2 + n2
m2
m2
cos θ = |s1 · s2|
|s1||s2| =
(5)
where s1 = (m1, n1, p1), s2 = (m2.n2, p2) are the direc-
tion vectors for Line L1, L2. Finally, can find the first large
1% of the angle between the two lines on the path as the
suspected interference point.
For WBA, authors successfully found the adversarial
examples and the supervised method they proposed is
effective, which can be shown in Table 2 for details. How-
ever, in this work, with the increase of training times,
the accuracy rate decreases. In other words, when train-
ing times are large enough, the interference point can
make the path converge, although the training efficiency
is reduced.
Similar to the work of Xiang et al., the maps used
for experiment are 16 × 16 and 17 × 17 is size, and
there is no way to verify the proposed adversarial attack
method more accurately with such map size. It is rec-
ommended that the attack method can be verified on
different categories of map-size, which can better illustrate
the effectiveness of the proposed method in this paper.
Common dominant adversarial examples generation
method (CDG)
Chen at al. (2018b) showed that dominant adversarial
examples are effective when targeting A3C path finding,
and designed a Common Dominant Adversarial Examples
GenerationMethod(CDG) to generate dominant adversarial
examples against any given map.
As shown in Fig. 4, are the dominant adversarial exam-
ples for the original map which can attack successfully.
Chen et al. found that on the dominant adversarial exam-
ple perturbation band, the value gradient rises the fastest.
Therefore, they call this perturbation band as “gradient
band”. By adding obstacles on the cross section of gradi-
ent band can perturb the agent’s path finding successfully.
The generation rule for dominant adversarial example can
be defined as:
• Generation Rule: Adding “baffle-like” obstacles to
the cross section of gradient band in which the value
gradient rises the fastest, can impact A3C path
finding.
As
in this
Moreover, in order to calculate the Gradient Band more
accurately, authors considered two kinds of situations
according to the difference for original map and gradient
function, one situation is that obstacles exist on both sides
of the gradient function, and the other is that obstacles
exist on one side if the gradient function.
A. Case 1: Obstacles exist on both sides of the gradient
function.
in Obstacle
coordinate points
the gradient
the
case, obstacles
curve,
exist on the both
sides of
then need to tra-
=
verse
all
{(Ox1, Oy1 ), (Ox2, Oy2 ),··· , (Oxn, Oyn )}, and to find the
nearest two points from this gradient curve in the upper
and lower part respectively. Therefore, the Gradient Band
function FGB(x, y) under such case can be concluded as:
⎧
⎪⎨
f (x, y)upper = y − (U+a0 + a1x + ... + akxk)
f (x, y)lower = y − (L+a0 + a1x + ... + akxk)
(6)
⎪⎩
XL < x < Xmax, YL < y < Ymax
Table 2 Features for adversarial perturbations against single same original clean map, which can show how the different
characteristics affect the interference of adversarial example
Number
Point coordinates
Max Q-value
Top Q
On the boundary
Point1
Point2
Point3
Point4
Point5
Point6
Point7
(4,5)
(4,10)
(2,3)
(3,4)
(5,6)
(0,2)
(6,7)
90.2229
140.7650
60.9148
71.4446
109.0013
48.4608
126.3412
0.0198
0.1616
0.2214
0.3199
0.3595
0.4645
0.6992
Number
On the path
Top angle size
Angle size
Point1
Point2
Point3
Point4
Point5
Point6
Point7
True
False
True
True
True
True
True
Ø
Ø
Ø
3
1
Ø
2
74
Ø
75
69
84
71
77
◦
◦
◦
◦
◦
◦
True
True
True
True
True
True
True
Perturbation point
True
True
True
True
True
True
False
Chen et al. Cybersecurity            (2019) 2:11 
Page 9 of 22
Fig. 4 The first line shows dominant adversarial examples for the original map. The fist picture denotes the original map for attack, and the three
columns on the right are the dominant adversarial examples of successful attacks. Meanwhile, the red dotted lines represent the perturbation band.
The second line denotes the direction in which the value gradient rises the fastest. By comparison between dominant adversarial examples and the
contour graph, can found that on the perturbation band, the value gradient rises fastest
where f (x, y)upper and f (x, y)lower denote the upper/lower
bound function respectively, Xmax and Ymax denote the
boundary value of the map, (XL, 0) and (0, YL) are the
intersection points of f (x, y)lower and the coordinate axis.
A. Case 2: Obstacles exist on one side of the gradient
function.
In this case, the calculating for distance between obsta-
cle edge points and gradient function is same with case 1.
However, under such scenario, obstacles exist on one side
of the gradient function curve, hence, under this case can
only obtain the upper/lower bound function for the Gradi-
ent Band. Therefore, the Gradient Band function FGB(x, y)
can be concluded as:
⎧
⎪⎪⎨
f (x, y)upper = min{f (Xmax, 0), f (0, Ymax)}
f (x, y)lower = y − (cid:14)
L + a0 + a1x + ... + akxk
⎪⎪⎩
(cid:15)
(7)
XL < x < Xmax, YL < y < Ymax
Finally setting Y =[ 1, 2, ..., Ymax] and X =[ 1, 2, ..., Xmax]
respectively, and generating the obstacle function set
Obaffle = {FY1, ..., FX1, ...}
For this paper, the lowest generation precision for CDG
algorithm is 91.91% (e.g., Fig. 5), which can prove that the
method proposed in this work can realize the common
dominant adversarial examples generated under A3C path
finding with a high confidence.
This paper showed that, the generation accuracy for
adversarial examples of CDG algorithm is relatively high.
By adding small obstacles at physical level on the original
clean map, it will interfere with the path finding process
of A3C agent. Comparing to other works in this field, the
experimental map size for Chen’s work contains 10 cate-
gories, 10×10, 20×20, 30×30, 40×40, 50×50, 60×60,
70×70, 80×80, 90×90, 100×100, which makes it possi-
ble to better verify the effectiveness of the proposed CDG
algorithm proposed in this paper.
Black-box attack
Policy induction attack (PIA)
Behzadan and Munir (2017) also discover that Deep Q-
network(DQN) based policy is vulnerability under adver-
sarial perturbations, and verified that the transferability
(Szegedy et al. (2013) proposed in 2013) of adversarial
examples across different DQN model does exist.
Therefore, they proposed a new type of adversarial
attack named policy induction attack based on this vulner-
ability of DQN. Their threat model considers that adver-
sary can get limited priori information, reward function
R and an estimate for the update frequency of the target
network. In other words, adversary is not aware of target’s
Chen et al. Cybersecurity            (2019) 2:11 
Page 10 of 22
Fig. 5 Samples for Dominant Adversarial Examples. For the first column is the original clean map for path finding. For columns on the right are the
samples for Dominant Adversarial Examples generated by CDG algorithm proposed in this paper, and (a), (b), (c), (d) represent four different
samples for dominant adversarial examples
network architecture and its parameters at every time
step, adversarial examples must be generated by black-box
techniques (Papernot et al. 2016c).
For every time step, adversary computes the pertur-
bation vectors ˆδt+1 for the next state st+1 such that
t ) causes ˆQ to generate its max-
maxa(cid:7) ˆQ(st+1 + ˆδt+1, a(cid:7); θ−
imum when a(cid:7) = π∗
adv(st+1). The whole process for policy
induction attack can be divided into two parts, namely
initialization and exploitation.
The initialization phase must be done before target
starts interacting with the environment. Specifically, this
phase can be divided as follow:
The exploitation phase takes adversarial attack operations
(e.g., designing adversarial input), and constitutes the life
cycle which can be shown in Fig. 6. The cycle is initialized
by the first observation value of the environment, and to
cooperate with the operation of the target agent.
In the context of policy induction attacks, this paper
conjectured that the temporal features of the training pro-
cess may be utilize to provide protection mechanisms.
However, an analytical treatment of the problem to estab-
lish the relationship of model parameters will suggest a
deeper insight and guidelines into design a more security
deep reinforcement learning architecture.
1) Training DQN policy based on the adversary’s reward
function r(cid:7) to obtain a adversarial strategy π∗
adv.
it with random parameters.
2) Creating a replica of the target’s DQN and initializing
Specific time-step attack
As the uniform attack strategies (e.g. Huang et al. (2017))
can be regarded as a direct extension of the adversarial
attack in DNN-based classification system, since the
Chen et al. Cybersecurity            (2019) 2:11 
Page 11 of 22
Fig. 6 The exploitation cycle of policy induction attack (Behzadan and Munir 2017). For the first phase, adversary will observes the current state, and
transitions in the environment. Then adversary will estimate the optimal action to select based on the adversrial policy. For the next phase,
adversary take perturbation into application, and perturb the target’s input. Finally, adversary will waits for the action that agent selected
adversarial example at each time step is computed inde-
pendently of the adversarial examples at other time step.
However, such tactic has not consider the uniqueness of
the RL problem.
Lin et al. (2017) proposed two tactics of adversarial
attack in the specific scenario of reinforcement learning
problem, which namely strategically-time attack and the
enchanting attack.
• Strategically-Timed Attack (STA)