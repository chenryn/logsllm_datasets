learned by GAN. Note that this experiment resembles the definition
of differential privacy (DP) [12]. DP is more generic and rigorous
as it measures that the probability difference varies all possible
functions and all data points, which is computationally infeasible
in our measurements. In this case, we narrow down the design by
observing the difference between a sample density in two generated
distributions Pg trained on neighboring training sets.
Let 𝐷𝑡 be the sensitive training set, x𝑖 ∈ 𝐷𝑡 be a target data point
and 𝐷′
be the neighboring dataset such that the Hamming
distance dH(𝐷𝑡 , 𝐷′
𝑡) = 1. Let 𝐺 be a learned generator trained on
𝐷𝑡 and 𝐺′ be a generator trained on 𝐷′
𝑡. We measure the difference
between the probability of producing a synthetic data point x𝑖 with
(prior) and without (posterior) the input data point x𝑖.
𝑡 = 𝐷\x𝑖
𝑡
Pr(𝐺(𝑧) = x𝑖 | 𝐷𝑡)
Pr(𝐺′(𝑧) = x𝑖 | 𝐷′
𝑡)
(8)
Following a recent work [6], GAN models do not memorize a data
point if it does not exist in the training dataset 𝐷𝑡. Thus, if Eq. (8)
approaches 1, we say that the target data x𝑖 is unlikely to be memo-
rized by the GAN. The pseudo-code of the experiment is presented
in Alg. 2. In the experiment, we use 20 different GANs (𝑁𝑘 = 20) and
some of target data to estimate Eq. (8). We report the experimental
results of five target data (𝑁𝑐 = 5) in Fig. 14.
From Fig. 14, we choose the same samples (data points) as in
Fig. 13 to compare how prior (with a target x𝑖) and posterior den-
sities (without target x𝑖) differ in modeled distribution. We find
that the presence of the target entry x𝑖 has limited influence on its
frequency in modeled distribution P𝑔. Even if some data point x𝑖 is
absent in the training set, its probability density in synthetic distri-
bution P𝑔 is still high, e.g., x4, x5 in the Adult dataset. This is perhaps
because GAN’s generalization smooths the sudden change that hap-
pened in the probability space of the training set. For instance, the
density of the a target point x𝑖 in P𝑟 may be lower than the sur-
rounding points, whereas the GAN smooths such sudden changes
in the probability space, and thus it is unintended to increase its
probability of exposure. In another aspect, such a rough probability
Figure 12: The impact of training data frequencies on
TableGAN-MCA effectiveness. The attack precision is set to
be one of {75%, 80%, 90%}.
data point x𝑖, we study how TableGAN-MCA is impacted by the
difference between data density of x𝑖 in the training distribution P𝑟
and that of the modeled distribution P𝑔.
According to our experiments, we discover that some unique
training data (∀x ∈ P𝑟 , #x𝑖 = 1) have unexpected high exposure
in modeled distribution P𝑔. For example, in Fig. 13, we illustrate
the average counts (from 100 synthetic datasets following mod-
eled distribution P𝑔) of five data points that appear in the training
dataset only once. As we can see, these five data points have higher
counts than what they have in the training dataset (= 1). Such an
observation indicates that the generator of GAN models unfairly
increases the probability of exposure of some data points under
TableGAN-MCA. We also find that such an observation is not rare.
For instance, according to the statistics in Fig. 13 (Adult), roughly
470 (1.5% of the training dataset) unique entries at least double their
exposure; roughly 150 (0.47% of the training dataset) unique entries
at least triple their exposure.
Next, we explore the factors that potentially trigger our obser-
vations by a set of experiments inspired by unintended memoriza-
tion [6]. Specifically, unintended memorization identifies the impact
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2105(a) Adult
(b) Lawschool
(c) Compas
Figure 13: Five synthetic samples’ count estimation in generated distribution (left) and statistics of data points that increase
the frequencies in P𝑔 across three datasets (right).
ALGORITHM 2: Memorization Experiment.
Input: {x1, . . . , x𝑁𝑐 }: sample data points; 𝐷𝑡 : private training dataset.
𝑖 (𝑧)]}: posterior
Output: {Pr[x𝑘 |𝐺𝑖 (𝑧)]}: prior frequency; {Pr[x𝑘 |𝐺′
frequency.
while 𝑘 : 1 → 𝑁𝑐 do
while 𝑖 : 1 → 𝑁𝑘 do
Generative model 𝐺𝑖 ← Train on 𝐷𝑡 ;
Pr[x𝑘 |𝐺𝑖 (𝑧)] ← Estimate by Eq. (4);
𝑡 ← 𝐷𝑡 \ {x𝑘};
𝐷′
𝑖 ← Train on 𝐷′
Generative model 𝐺′
𝑡 ;
Pr[x𝑘 |𝐺′
𝑖 (𝑧)] ← Estimate by Eq. (4);
end
end
return {Pr[x𝑘 |𝐺𝑖 (𝑧)]}, {Pr[x𝑘 |𝐺′
𝑖 (𝑧)]}
Figure 14: Memorization experiments on three datasets. The
blue-color boxplot depicts the frequencies when the target
entry is in the training set while the orange one depicts the
frequencies when the target entry is deleted from the train-
ing set.
space in real distribution may be attributed to insufficient sampling
or unbalanced sampling. As such, cautious data collection may have
positive impact in mitigating such influence. Understanding this
complicated phenomenon with more explicit proof is our future
work. Currently, we summarize that the unique training data recov-
ered by TableGAN-MCA is mainly due to the GAN’s generalization
rather than the unintended memorization. This result implies that
mitigating the attack effect of TableGAN-MCA may inevitably com-
promise the availability of released synthetic datasets, since GAN
generalization is closely related to its generation ability, which
potentially impacts the quality of generated data.
7 MITIGATION
In this section, we evaluate the mitigation effects of differential
privacy and two customized defense methods against TableGAN-
MCA.
7.1 Differentially Private WGAN-WC
Differentially Private WGAN (DP-GAN) only has acceptable
trade-offs for larger privacy budgets, and may hardly elim-
inates TableGAN-MCA without compromise synthetic data
utility. Differential privacy [12] provides a quantified solution to
output randomized answers. In this work, we apply a standard
approach of differentially private iterative training procedure (DP-
SGD, short for DP stochastic gradient descent) [1, 30] to the GAN
to train a (𝜖, 𝛿)-differentially private generator oracle. Otherwise,
since DP-SGD perturbs the training process of discriminative mod-
els, such mitigation may achieve sub-optimal trade-offs between
membership collision privacy and synthetic data utility. In the ex-
periments, we implement the DP framework according to [30] and
account the privacy budget (𝜖, 𝛿) using RDP accountant released
in Tensorflow/Privacy project. Note that WGAN-GP, TVAE and
CTGAN do not have DP versions, and thus we study the DP version
of WGAN-WC. The generation quality and TableGAN-MCA effect
of non-private baseline are shown in Fig. 15 followed by Table 4
and Fig. 9.
To implement DP-WGAN, we train a differentially private dis-
criminator. The generator is differentially private because of the
post-processing [13]. We add calibrated noise into each gradient
of the discriminator during training. The accumulation of mul-
tiple Gaussian noise addition [11] relies on privacy accountant
techniques [1] and Rényi differential privacy [31]. We provide DP-
related hyper-parameters in Table 8, Appendix A.
We provide the experimental results of the machine learning
utility and TableGAN-MCA effect when sharing differentially pri-
vate synthetic data in Fig. 15. The shadow GANs in use are non
private WGAN-WC. The privacy budget 𝜖 measures the amount of
privacy leakage and a smaller value means more privacy-preserved.
𝛿 denotes the probability of violating 𝜖-DP, which is set to
𝑂(|𝐷𝑡 |) .
As can be seen from Fig. 15, the DP method has some positive effect
in defending against the TableGAN-MCA. For Adult datasets, when
privacy budget 𝜖 ≈ 2.0, the attack AUPRC decreases by 16.01%
and model’s predicted accuracy decreases by 1.18% in comparison
to the no-DP baseline (see dash dots in Fig. 15). For the Compas
dataset, when privacy budget 𝜖 ≈ 8.0, the attack AUPRC decreases
1
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2106(a) Adult
(b) Lawschool
(c) Compas
Figure 15: Differential private GAN-synthesized data utility (left) and TableGAN-MCA effect (right) for Adult, Lawschool and
Compas benchmarks. Dash dot line denotes non-private WGAN with weight clipping baseline.
ALGORITHM 3: GAN-constrained Training (Improved defense)
Input: 𝐷𝑡 : private training data; 𝑁𝑔: number of discriminator
iterations per generator iteration; 𝑚: batch size
Output: A Synthetic dataset 𝑆
for each iteration do
while 𝑖 : 1 → 𝑁𝑔 do
Sample(cid:8)x(𝑖)(cid:9)𝑚
Sample(cid:8)z(𝑖)(cid:9)𝑛
(cid:8)z(𝑖)(cid:9)𝑚
Sample(cid:8)z(𝑖)(cid:9)𝑛
end
𝑖=1 ∼ P𝑟 ;
𝑖=1 ∼ Pz, 𝑛 > 𝑚; Choose 𝑚 of 𝑛 priors
𝑖=1 s.t., 𝐺(z) ∉ 𝐷𝑡
𝑖=1 ∼ Pz; Choose 𝑚 of 𝑛 priors(cid:8)z(𝑖)(cid:9)𝑚
𝑖=1 s.t.,
Compute loss, backward, update gradients;
𝐺(z) ∉ 𝐷𝑡 ;
Compute loss, backward, update gradients;
end
𝑆 ← 𝐺(z), 𝑠.𝑡 ., 𝐺(z) ∉ 𝐷𝑡 ;
return 𝑆
⊲ Naive defense
while minimizing the distance between training data and generated
data L(𝐷𝑡 , 𝑆), which is
L(𝑆𝑖, 𝐷𝑡)|𝑆𝑖∩𝐷𝑡 =∅,
𝑆 = arg min
𝑆𝑖
(9)
where L denotes a distance metric. Since the discriminator of
the WGAN minimizes the Wasserstein distance, we additionally
add a constraint during training to force each sampled batch of
the generator to be disjoint with 𝐷𝑡. To do so, we remove the
intersection between the sampled batch and the training set every
iteration before computing the loss function (see Alg. 3). Thus,
WGAN automatically searches for the best substitution for such
colliding samples at training.
7.2.3 Naive and Improved Defenses Evaluation. The improved
defense in large part achieves superior trade-offs than the
naive defense, and is almost comparable to the no-defense
baseline. We evaluate synthetic data utility of the naive defense,
the improved defense and the no-defense (baseline) on WGAN-GP.
Note that the baseline is vulnerable to TableGAN-MCA while naive
and improved defenses protect against it. We evaluate machine
learning efficacy in Fig. 17(a) and marginal fitness in Fig. 17(b).
In Fig. 17(a), we train machine learning models (Logistic Regres-
sion Classifier) on synthetic data sampled from the naive defense,
the improved defense and the no-defense generator and predict on
the real test data. Fig. 17(a) shows that synthetic data generated by
naive and improved defenses achieve satisfying prediction accu-
racy on the Adult and Lawschool datasets. In the Compas dataset,
(a) No Defense
(b) Naive Defense
(c) Improved Defense
Figure 16: ECDF comparisons for synthetic datasets gener-
ated by three methods. We choose “priors count” attribute
in the Compas dataset.
by 48.33% and model’s predicted accuracy decreases by 5.13% in
comparison to the no-DP baseline. We also depict the ECDF com-
parison between the original training data and differentially private
synthetic data for each marginal to show marginal fitness compro-
mise in Fig. 18 (Appendix A). It is not surprising that DP-WGAN
achieves sub-optimal trade-offs when protecting against TableGAN-
MCA, since the memorization experiment shows that the presence
of individuals does not significantly affect the generated distri-
bution. The membership collisions information that we intend to
infer is perhaps highly correlated to population statistics (attributes
correlation), which will be preserved even under DP training.
7.2 Customized Defense
7.2.1 Remove Colliding Members. Removing colliding members
protects against TableGAN-MCA but it reduces the distribu-
tion fitness. The straightforward solution against TableGAN-MCA
is to manually remove colliding members from the sampled syn-
thetic dataset and share a cleaned version to the analysts (cus-
tomers). The whole process is denoted as the “naive defense” (last
steps in Alg. 3). We acknowledge the cleaned version can decrease
the utility of original synthetic data, especially for distribution fit-
ness. For example, we present the ECDF comparison of synthetic
datasets generated by the naive defense (Fig. 16(b)) and no-defense
(Fig. 16(a)). We show that the naive defense exhibits decreased mar-
ginal fitness compared with no-defense baseline. More ECDFs can
be found in Figs. 19(a), 20(a), and 21(a) (Appendix B).
7.2.2 GAN-constrained Training. We propose a GAN-constrained
training technique, to further improve synthetic data utility while
protecting against TableGAN-MCA. This strategy is denoted as an
“improved defense”. Simply put, we motivate GANs to generate a
synthetic dataset 𝑆 ∼ P𝑔 that is disjoint with the training set 𝐷𝑡
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2107discriminator at the inference phase (see column 3 in Table 1). In our
experiment, we have already shown that a GAN resilient to their
attacks may still expose training data to TableGAN-MCA. MC [21]
and GAN-leaks [7] extract a customized membership indicator of
an overfitting generator to train an attack model. We share a similar
theoretical bases with theirs, that is, the modeled distribution of
the generator behaves differently on training input versus the non-
training one. However, our attack further recovers partial training
data by inferring membership of published synthetic data, which is
out of their scope (see Columns 4 and 5 in Table 7). In this work, we
empirically show that the membership inference classifier cannot
be directly used to identify membership collisions in our attack
model (see Table 7). Compared to those works, we propose a novel
attack model, TableGAN-MCA, that exposes partial training data by
exploiting the weakness of tabular data synthesis. Even though we
share similar ideas with MIAs in generative setting, the attack model
of TableGAN-MCA learns different decision boundaries. According
to the experimental results, the success of the proposed attack relies
more on population knowledge than individual presence, which is
different from MIAs.
9 CONCLUSION
GAN-synthesized table releasing provides unprecedented oppor-
tunities for private data sharing that aims to study the regular
pattern of population. In this work, we propose a novel member-
ship collision attack, TableGAN-MCA, against the GAN-synthesized
table. Our comprehensive experiments over the real-world datasets
conclude some important findings. TableGAN-MCA achieves high
recovering rate against the private training data from the published
GAN-synthesized tables. Our in-depth studies suggest that the tar-
get model, training data size, training epochs and training data
frequencies impact the attack performance of TableGAN-MCA. We
further conclude that the training data leakage caused by TableGAN-
MCA is mainly related to the published population statistics (at-
tributes correlations), rather than the model memorization. To mit-
igate the effect of TableGAN-MCA, we find that differential privacy
(applying DP-WGAN) does not show a satisfying result mainly
due to the correlations between training data features. Based on
our understanding on TableGAN-MCA, we propose two mitigation
approaches, which substitute the published colliding members with
similar non-private data entries. We hope that the concept of mem-
bership collisions defined and the attack methodology developed
in this paper could inform the privacy community of such new