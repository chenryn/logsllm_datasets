is unsurprising as benign samples have data distributions
that do not drift with respect to malicious ones.
A similar reasoning can be followed for Figures 3c
and 3d. Contrary to the distribution of p-values, prob-
abilities are constrained to sum up to 1.0 across all the
classes; what we observe is that probabilities tend to be
skewed towards high values even when predictions are
wrong.
Intuitively, we expect to have poor quality on
all the classes of predictions in the presence of a drifting
634    26th USENIX Security Symposium
USENIX Association
Given label: maliciousGiven label: benign0.00.20.40.60.81.0Given label malicious: p-value maliciousGiven label malicious: p-value benignGiven label benign: p-values maliciousGiven label benign: p-values benignGiven label: maliciousGiven label: benign0.00.20.40.60.81.0Given label malicious: p-value maliciousGiven label malicious: p-value benignGiven label benign: p-values maliciousGiven label benign: p-values benignGiven label: maliciousGiven label: benign0.00.20.40.60.81.0Given label malicious: probability maliciousGiven label malicious: probability benignGiven label benign: probability maliciousGiven label benign: probability benignGiven label: maliciousGiven label: benign0.00.20.40.60.81.0Given label malicious: probability maliciousGiven label malicious: probability benignGiven label benign: probability maliciousGiven label benign: probability benignscenario: while probabilities tend to be skewed, CE’s sta-
tistical metrics (p-values) seem better-suited at this task.
So far, we have seen how Transcend produces statisti-
cal thresholds to detect concept drift driven by predeﬁned
goals under speciﬁc constraints. In addition, the analy-
sis of p-value and probability distributions highlighted
how the former seem to be better-suited than probabili-
ties to identify concept drift. In the following paragraphs,
we show how CE’s statistical metrics provide thresholds
that always outperform probabilities in detecting concept
drift. Figure 6 in Appendix 7 provides a thorough com-
parison. For simplicity, here, we focus the attention on
the 1st and 3rd quartile, the median and the average of
the distribution of p-values and probabilities as potential
cut-off thresholds, as shown in Table 4.
Intuitively speaking, a successful technique not only
would achieve high performances on correct predictions,
but it would also report poor performances on drifting
objects. This is evident from Table 4, where a cut-off
threshold at the 1st quartile reports a high performance
for the objects that ﬁt the trained model (0.9045 TPR at
0.0007 FPR), and a poor performance for those drifting
away (0 TPR and 0 FPR); this means that at this thresh-
old, CE’s statistical metrics suggest to consider as un-
trustworthy only objects the classiﬁer would have pre-
dicted incorrectly. Conversely, probabilities also tend
to be skewed when predictions are wrong, affecting the
ability to rely on such metrics to correctly identify con-
cept drift. Table 4 shows 0.6654 TPR and 0 FPR for
objects whose quality fall above the 1st quartile of the
probability distribution, and 0.3176 TPR and 0.0013 FPR
for those who fall below; this means that probabilities
marked as unreliable also make predictions that would
have been classiﬁed correctly.
As we move up towards more conservative thresh-
olds, CE’s statistical metrics start discarding objects that
would have been classiﬁed correctly. This is unsurpris-
ing as we have deﬁned a threshold that is more selec-
tive of the desired quality. Regardless, at each point
p-values still outperform probabilities (higher TPR and
FPR of objects with a quality higher than the cut-off,
and lower for those below the threshold). These results
further show how relying on detecting concept drift is a
challenging problem that cannot be easily addressed by
relying on a preﬁxed 50% threshold [19].
Note that the number of untrustworthy predictions on
the testing dataset is a function of the number of drift-
ing objects. If the entire dataset drifts, we would expect
Transcend to ﬂag as untrustworthy all (or most of) the
predicted objects that do not ﬁt the trained model.
Adapting to Concept Drift. Once drifting objects are
identiﬁed, the next step would require data relabeling and
model retraining, as outlined throughout the paper. Ta-
ble 3c shows the results of these steps, which take preci-
sion for benign samples to 0.89 and recall for malicious
ones to 0.76. We would like to remark that this work fo-
cuses on the construction of statistical metrics to identify
concept drift as outlined so far. While relabeling is out
of scope for this work, it is clear that an approach that
identiﬁes drifting objects is well-suited to address such a
challenge in the pipeline as resources can be focused on
analyzing samples that do not ﬁt in the trained model.
4.2 Multiclass Classiﬁcation Case Study
In this section we evaluate the algorithm proposed by
Ahmadi et al. [1] as a solution to Kaggle’s Microsoft
Malware Classiﬁcation Challenge; the underlying ratio-
nale is similar to that outlined in the previous section,
thus, we only report insightful information and take-
aways.
In this evaluation, we train the classiﬁer with
seven out of eight available malware families; Trucur, the
excluded family, represents our drifting testing dataset.
The confusion matrix reports a perfect diagonal7; in
this case, the decision assessment gives us no additional
information because we cannot analyze the distribution
of p-values of incorrect choices. From a quality per-
spective, drawing upon the alpha assessment of Figure 4,
two families, Vundo and Ramnit, have signiﬁcant differ-
ences. The Ramnit family has p-values that are much
higher than those of the interfering families. However,
for Vundo the p-values of interfering families are closer
to the correct ones. These details can be only be observed
through the alpha assessment, suggesting that the iden-
tiﬁcation of the Ramnit samples would be more robust
when the data distribution changes.
Family Discovery. Below, we show how we identify a
new family based on CE’s statistical metrics.
The testing samples coming from Tracur are classiﬁed
as follows: 5 as Lollipop, 6 as Kelihos ver3, 358 as Vundo
and 140 as Kelihos ver1. Looking at the distribution of
probabilities and p-values it is easy to relate to the case
of binary classiﬁcation, i.e., for each family there is only
one class with high p-values corresponding to the class of
the true label. For the test objects of Tracur, we observe
that the p-values for all the classes are close to 0. This
is a clear pattern which shows that the samples are com-
ing from an unknown distribution. In a scenario chang-
ing gradually, we will observe an initial concept drift (as
shown in the binary classiﬁcation case study in § 4.1.1),
characterized by a gradual decrease of the p-values for
all the classes, which ends up in a situation where we
have p-values very close to 0 as observed here. These
results clearly show that even in multiclass classiﬁca-
tion settings, CE provides metrics that are better-suited
7We reached out to the authors who provided us with the dataset
and the implementation of the learning algorithm to replicate the results
presented in [1].
USENIX Association
26th USENIX Security Symposium    635
to identify concept drift than probabilities8. The com-
parison between p-values and probabilities is reported in
Figures 7 to 10 in Appendix 7 and follow a reasoning
similar to that of the binary classiﬁcation case study.
5 Discussion
Security community has grappled with the challenge of
concept drift for some time now [12, 23, 25]. The prob-
lem commonly manifests itself in most malware detec-
tion/classiﬁcation algorithm tasks and models perform
poorly as they become dated. Literature [12, 15, 16]
recommends retraining the model periodically (see § 6)
to get around this. However, retraining periodicity is
loosely deﬁned and is an expensive process that leads
to sub-optimal results. Consequently, there are periods
where the model performance cannot be trusted. The
problem is further exacerbated as concept drift is hard
to identify without manual intervention.
If the model
is retrained too frequently, there will be little novelty
in information obtained through retraining to enrich the
model. Regardless of the periodicity, the retraining pro-
cess requires manual labeling of all the processed ob-
jects. Transcend selectively identiﬁes the drifted ob-
jects with statistical signiﬁcance9, thus is able to restrict
8The algorithm in [1] relies on probabilities (decision trees).
9The p-value for an object o with label l is the statistical support
of the null hypothesis H0, i.e., that o belongs to l. Transcend ﬁnds the
signiﬁcance level (the per-class threshold) to reject H0 for the alterna-
tive hypothesis Ha, i.e., that o does not belong l (p-values for wrong
hypotheses are smaller than those for correct ones, e.g., Figure 2b).
the manual labeling process to the objects that are sub-
stantially different than the ones in the trained model
(see §3.3 and §4.1.1).
Adversarial ML and Model Fortiﬁcation. Our work
aims to detect concept drift as it occurs in an existing
model. Concept drift can occur due to various reasons.
Common causes being malware polymorphism or eva-
sion but adversarial data manipulation (adversarial drift)
can also be a reason. Approaches have been proposed
to fortify models against drift [12, 15, 23], however such
solutions deal with speciﬁc domains and do not provide
a generalized solution. Transcend is agnostic to the ma-
chine learning algorithm under consideration. This let
us leverage the strength of the algorithm while detecting
concept drift. Therefore, if the algorithm is more resilient
to concept drift, drift will be detected later on in time. If
it is less resilient, drift will be detected as sooner.
Comparison with Probability. Probabilities have been
known to work well in some scenarios but as demon-
strated in § 4.1.1 and § 4.2 they are not as effective as
compared to p-values which are more versatile, espe-
cially in the presence of concept drift. When probabil-
ities are reported to be low it is difﬁcult to understand if
the sample does not belong to any class or if the sam-
ple is actually just difﬁcult to classify while still belong-
ing to one of the known classes.
In other words, the
p-value metric offers a natural null option when the p-
values calculated for all the classes are low. Instead, as
shown in the case of SVM (see, § 4.1.1), the probabil-
ity metric is bounded to one of the options in the model.
Figure 4: Multiclass Classiﬁcation Case Study: Alpha assessment for the Microsoft classiﬁcation challenge showing
the quality of the decision taken by the algorithm. Although, perfect results are observed on the confusion matrix , the
quality of those results vary a lot across different families.
636    26th USENIX Security Symposium
USENIX Association
Ramnit'ssamplesLollipop'ssamplesKelihos_ver3'ssamplesVundo'ssamplesKelihos_ver1'ssamplesObfuscator.ACY'ssamplesGatak'ssamples0.00.20.40.60.81.0P-valuesP-values: RamnitP-values: LollipopP-values: Kelihos_ver3P-values: VundoP-values: Kelihos_ver1P-values: Obfuscator.ACYP-values: GatakIt does not matter if the probabilities are well calibrated
or not, the limitation is inherent to the metric. As dis-
cussed, the work by Rieck et al. [19] faces similar chal-
lenges when choosing the probability threshold. More-
over, the p-value metric provided by our framework, can
be calculated from algorithms that do not provide proba-
bilities, e.g., custom algorithms like [22], thus extending
the range of algorithms that can beneﬁt from a statistical
evaluation.
Performance. Calculation of p-values is a computation-
ally intensive process—for each sample z in a class c ∈ C,
the calculation of a p-value requires computation of a
non-conformity measure for every element in the dataset.
This can be further exacerbated by non-conformity mea-
sures that rely on distances that are complex to compute.
The computational complexity in relation to the number
of the times that the non-conformity measure needs to be
computed is O(C·N2), where N represents the total num-
ber of samples and C represent the number of classes.
Calculations can be sped up by computing a whole set of
non-conformity scores in one single algorithm run. For
example, SVM used in Drebin [2] can directly supply
the total non-conformity scores for the calculation of one
p-value in only one run of the algorithm, thus reducing
the complexity to O(C · N). Further optimizations can
be made for algorithms that treat each class separately;
in such a scenario we can run the algorithm just for the
class under analysis.
6 Related Work
Solutions to detect concept drift, speciﬁc to security do-
mains, have been proposed [12, 15, 23], in contrast our
framework provides a generic solution which is algo-
rithm agnostic. On the other hand, solutions [6, 7] devel-
oped by the ML community have constrains that are not
suitable for security applications (e.g., retrospective de-
tection of concept drift when the classiﬁcation decision
has already been made).
Thomas et al. [23] present Monarch a real-time system
that crawls URLs as they are submitted to web services
and determines whether the URLs direct to spam. The
system uses machine-learning to classify URLs as mali-
cious or benign. The authors suggest training the model
continuously to keep classiﬁcation error low as the na-
ture of malicious URLs keeps evolving. Kantchelian et
al. [12] propose fusing human operators with the un-
derlying machine-learning based security system to ad-
dress concept drift in adversarial scenarios. Maggi et
al. [15] present a machine-learning based system to clas-
sify malicious web applications. They use techniques
speciﬁc to web application to detect concept drift and
thus retrain their model to reduce false positives. Mari-
conti et al. [16] show how models decay over time and
propose ways to resist longer. Our model uniﬁes these
techniques as it generalizes to both the area of appli-
cation and machine-learning algorithm used. The pre-
sented model can not only accurately predict when to
retrain a model but also provides a quality estimate of
the decisions made. These results can reduce human in-
tervention and make it more meaningful thus decreasing
the cost of operation. Transcend can be plugged on top of
any such approach to provide a clear separation between
non-drifting and drifting objects.
Deo et al. [5] propose using Venn-Abers predictors
for assessing the quality of binary classiﬁcation tasks
and identifying concept drift. The Venn-Abers predic-
tors offer automatically well calibrated and probabilistic
guidance to detect change in distribution of underlying
samples. Although useful, the approach has limitations
and cannot draw concrete conclusions on sample clusters
which are outliers. Also, Venn-Abers outputs multiple
probabilities of which one is perfectly calibrated but it
is not possible to know which. Our approach provides
a simple mechanism to compare predictions through p-
values and does not suffer from the discussed shortcom-
ings. CE also works on multi-class prediction tasks,
while this is not currently supported by Venn-Abers pre-
dictors.
Other works try to detect change point detection when
the underlying distribution of data samples changes sig-
niﬁcantly, e.g., in case of evolving malware which is ob-
served as a disruption in ex-changeability [25]. Martin-
gales have often been used to detect drift of multidimen-