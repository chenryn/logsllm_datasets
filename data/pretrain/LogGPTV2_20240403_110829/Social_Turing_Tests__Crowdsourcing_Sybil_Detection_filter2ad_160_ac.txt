p
e
r
k
p
e
r
k
p
e
r
k
ci
rt
e
r
rt
e
r
rt
e
r
a
l
(a) Education Level
(b) OSN Usage Experience
(c) Gender
Figure 3. Demographics of participants in our user study.
sourcing websites [11]. Although we could have paid more,
prior work has shown that paying more money does not
yield higher quality results on crowdsourcing sites [19].
Sociology Undergraduates.
The ﬁnal group of test
subjects are undergraduate students from the Department of
Communications at UCSB (Social Science major). These
students were asked to take our study in exchange for course
credit. This group adds additional perspective to our study,
apart from Computer Science oriented experts and the un-
controlled turker population.
The social science students are listed in Table 1 as “US
social.” We only asked the students to evaluate our Face-
book US dataset, since cultural and language barriers pre-
vent them from effectively evaluating Chinese and Indian
proﬁles. 198 total students completed our study in March,
2012. Each student was asked to evaluate 25 proﬁles, mid-
way between what we asked of experts and turkers.
Summary.
We conduct experiments with 7 groups of
testers: experts from US, India, and China; turkers from
US, India, and China, and social science students from the
US. Table 1 lists the number of testers in each group and the
number of proﬁles evaluated by each tester.
4 User Study Results
In this section, we present the high level results of our
user study. We start by introducing the demographics of the
test subjects. Next, we address one of our core questions:
how accurate are people at identifying Sybils? We com-
pare the accuracy of individual testers to the accuracy of
the group to assess whether the “wisdom of the crowd” can
overcome individual classiﬁcation errors. Finally, we exam-
ine the reasons testers cited in classiﬁed proﬁles as Sybils.
4.1 Demographics
At the end of each survey, testers were asked to answer
demographic questions about themselves. Figure 3 shows
the results that were self-reported by testers.
Education.
As shown in Figure 3(a), most of our experts
are enrolled in or have received graduate level degrees. This
is by design, since we only asked Computer Science grad-
uate students, undergrads enrolled in graduate courses, and
professors to take part in our expert experiments. Similarly,
the social science testers are drawn from the undergraduate
population at UCSB, which is reﬂected in the results.
The education levels reported by turkers are surprisingly
high. The majority of turkers in the US and China report
enrollment or receipt of bachelors-level degrees [24]. Sur-
prisingly, over 50% of Indian turkers report graduate level
educations. This result for Indian turkers stems from cul-
tural differences in how education levels are denoted. Un-
like in the US and China, in India “graduate school” refers
to “graduated from college,” not receipt of a post-graduate
degree (e.g. Masters or Doctorate). Thus, most “graduate”
level turkers in India are actually bachelors level.
OSN Usage Experience.
As shown in Figure 3(b),
the vast majority of testers report extensive experience with
OSNs. US experts, Chinese experts, and social science un-
dergrads almost uniformly report ≥2 years of OSN expe-
rience. Indian experts, Indian turkers, and Chinese turkers
have the greatest fractions of users with <2 years of OSN
experience. US turkers report levels of OSN experience
very similar to our most experienced expert groups.
Gender.
As shown in Figure 3(c), the vast majority
of our testers are male. The only group which exhibits a
female majority is the social science undergrads, a demo-
graphic bias of the communications major. Turker groups
show varying levels of gender bias: Chinese and Indian
turkers are predominantly male [24], while the US group
is evenly divided.
4.2 Individual Accuracy
We now address one of the core questions of the paper:
how accurate are people at identifying Sybils? To achieve
100% accuracy, a tester needs to correctly classify all Sybil
and legitimate proﬁles they were shown during the test. Fig-
ure 4 shows the accuracy of testers in 5 of our test groups.
Chinese experts are the most accurate, with half achieving
≥90% accuracy. The US and Indian (not shown) experts
also achieved high accuracy. However, the turker groups do
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
 0
CN Turker
US Turker
US Social
US Expert
CN Expert
Experts
Turkers
 100
 80
 60
 40
 20
)
%
(
F
D
C
Experts
Turkers
Social
 100
 80
 60
 40
 20
)
%
(
F
D
C
 40
 20
 80
Accuracy per Tester (%)
 60
 100
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Average Jaccard Coefficient
Average Jaccard Coefficient
Figure 4. Tester accuracy.
(a) Renren
(b) Facebook US
Figure 5. Jaccard similarity coefﬁcient of reasons.
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
 0
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
 0
 100
Expert FP
Turker FP
Expert FN
Turker FN
 40
 80
 20
Error Rate per Tester (%)
 60
Expert FP
Turker FP
Expert FN
Turker FN
 40
 80
 20
Error Rate per Tester (%)
 60
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
 0
 100
Expert FP
Turker FP
Expert FN
Turker FN
 40
 80
 20
Error Rate per Tester (%)
 60
 100
(a) Renren
(b) Facebook US
(c) Facebook India
Figure 6. False positive (FP) and false negative (FN) rates for testers.
not perform as well as the experts. The Chinese and Indian
(not shown) turkers perform the worst, with half achieving
≤65% accuracy. The accuracy of US turkers and social sci-
ence students falls in-between the other groups.
To better understand tester accuracy, Figure 6 separates
the results into false positives and false negatives. A false
positive corresponds to misclassifying a legitimate proﬁle
as a Sybil, while a false negative means failing to identify a
Sybil. Figure 6 focuses on our expert and turker test groups;
social science students perform similarly to US turkers, and
the results are omitted for brevity.
Figure 6 reveals similar trends across all test groups.
First, false positives are uniformly lower than false nega-
tives, i.e. testers are more likely to misclassify Sybils as le-
gitimate, than vice versa. Second, in absolute terms, the
false positive rates are quite low: <20% for 90% of testers.
Finally, as in Figure 4, error rates for turkers tend to be sig-
niﬁcantly higher than those of experts.
In summary, our results reveal that people can identify
differences between Sybil and legitimate proﬁles, but most
individual testers are not accurate enough to be reliable.
4.3 Accuracy of the Crowd
We can leverage “the wisdom of the crowd” to amor-
tize away errors made by individuals. Many studies on
crowdsourcing have demonstrated that experimental error
can be controlled by having multiple turkers vote on the
answer, and then using the majority opinion as the ﬁnal an-
swer [17, 25]. As long as errors by turkers are uncorrelated,
this approach generates very accurate results.
We now examine whether this methodology can be used
to improve the classiﬁcation accuracy of our results. This
question is of vital importance, since a voting scheme would
be an essential component of a crowdsourced Sybil detector.
To compute the “ﬁnal” classiﬁcation for each proﬁle in our
dataset, we aggregate all the votes for that proﬁle by testers
in each group. If ≥50% of the testers vote for fake, then we
classify that proﬁle as a Sybil.
Table 3 shows the percentage of false positive and neg-
ative classiﬁcations for each test group after we aggregate
votes. The results are mixed: on one hand, false positive
rates are uniformly low across all test groups. In the worst
case, US turkers and social science students only misclas-
sify 1 out of 50 legitimate proﬁles. Practically, this means
that crowds can successfully identify real OSN proﬁles.
On the other hand, false negative rates vary widely across
test groups. Experts in China, in the US, and the social
science students all perform well, with false negative rates
<10%. Indian experts also outperform the turker groups,
but only by a 2.7% margin. The Chinese and Indian turker
groups perform worst, with ≥50% false negatives.
From these results, we can conclude three things. First,
using aggregate votes to classify Sybils does improve over-
all accuracy signiﬁcantly. Compared to the results for indi-
vidual testers in Figure 6, both false positive and negative
rates are much lower after aggregation. Second, the uni-
formly low false positive rates are a very good result. This
means that running a crowdsourced Sybil detection system
will not harm legitimate social network users. Finally, even
with aggregate voting, turkers are still not as accurate as ex-
Dataset
Renren
Facebook US
Facebook IN
Tester
FP Rate
FN Rate
CN Expert
CN Turker
US Expert
US Turker
US Social
IN Expert
IN Turker
0%
0%
0%
2%
2%
0%
0%
3%
63%
9.4%
18.7%
6.25%
16%
50%
Table 3. Error rates after aggregating votes.
Dataset
Renren
Facebook US
Facebook IN
Tester
CN Expert
CN Turker
US Expert
US Turker
US Social
IN Expert
IN Turker
Info Wall
18% 57%
31% 31%
37% 30%
35% 32%
30% 31%
39% 28%
39% 27%
Photos
25%
38%
33%
33%
39%
33%
34%
Table 4. Reasons why proﬁles are suspicious.
perts. In the next section, we look more deeply into factors
that may negatively inﬂuence turkers accuracy, and tech-
niques that can mitigate these issues.
4.4 Reasons for Suspicion
During our user study, testers were asked to give reasons
for why they classiﬁed proﬁles as Sybils. Testers were given
the option of reporting the proﬁle’s basic information, wall,
and/or photos as suspicious. Testers could select as many
options as they liked.
In this section, we compare and contrast the reasons re-
ported by different test groups. Table 4 shows percentage
of votes for each reasons across our seven test groups. The
US and Indian expert and turker groups are very consistent:
they all slightly favor basic information. The bias may be
due to the way our study presented information, since each
proﬁle’s basic information was shown ﬁrst, by default. The
social science students are the only group that slightly fa-
vors photos.
In contrast to the US and Indian groups, Chinese experts
and turkers often disagree on their reasons for suspicion.
The majority of experts rely on wall messages, while turk-
ers slightly favor photos. As shown in Figure 4, Chinese
turkers have lower accuracy than Chinese experts. One pos-
sible reason for this result is that turkers did not pay enough
attention to the wall. As previously mentioned, there is
a comment box at the end of our survey for testers to of-
fer feedback and suggestions. Several Chinese experts left
comments saying they observed wall messages asking ques-
tions like “do I know you?,” and “why did you send me a
friend request?,” which they relied on to identify Sybil pro-
ﬁles.
Consistency of Reasons.
There is no way to objec-
tively evaluate the correctness of tester’s reasons for classi-
ﬁcation, since there is no algorithm that can pick out suspi-