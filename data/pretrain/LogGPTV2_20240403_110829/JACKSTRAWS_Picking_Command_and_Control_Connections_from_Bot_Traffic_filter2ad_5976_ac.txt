information can store the type of the resource (ﬁles, reg-
istry keys, ...) that a system call operates on as well as
ﬂags such as mode or permission bits. Note that some
information is only stored as comment; this information
is ignored for the template generation and matching, but
is saved for a human analyst who might want to examine
a template.
One important additional piece of information stored
for system calls that manipulate ﬁles and registry keys is
the name of these ﬁles and keys. However, for these re-
source names, it is not desirable to use the actual string.
The reason is that labels are taken into account during
the matching process, and two nodes are considered the
same only when their labels match. Thus, some type of
abstraction is necessary for labels that represent resource
names, otherwise, graphs become too speciﬁc. We gen-
eralize ﬁle names based on the location of the ﬁle (using
the path name) and its type (typically, based on the ﬁle’s
extension). Registry key names are generalized by nor-
malizing the key root (using abbreviations) and replac-
ing random names by a generic format (typically, nu-
merical values). More details about the labeling process
and these abstractions can be found in Appendix A.
Simplifying behavior graphs. One problem we faced
during the behavior graph generation was that certain
graphs grew very large (in terms of number of nodes),
but the extra nodes only carried duplicate information.
For example, consider a bot that downloads an exe-
cutable ﬁle. When this ﬁle is large, the data will not
be read from the network connection by a single recv
call. Instead, the receive system call might be invoked
many times; in fact, we have observed samples that read
network data one byte at a time. Since every system call
results in a node being added to the behavior graph, this
can increase the number of nodes signiﬁcantly.
To reduce the number of (essentially duplicate) nodes
in the graph, we introduce a post-processing step that
collapses certain nodes. The purpose of this step is
to combine multiple nodes, sharing the same label and
dependencies. More precisely, for each pair of nodes
with an identical label in the behavior graph, we check
whether (1) the two nodes share the same set of parent
nodes, or (2) the sets of parents and children of one node
are respective subsets of the other, or (3) one node is the
only parent of the other. If this is the case, we collapse
these nodes into a single node and add a special tag Is-
Multiple to the label. Additional incoming and outgoing
edges of the aggregated nodes are merged into the new
node. The process is repeated until no more collapsing
is possible. As an example, consider the case where a
write ﬁle operation stores data that was previously read
from the network by multiple receive calls. In this case,
the write system call node will have many identical par-
ent nodes (the receive operations), which all contribute
to the buffer that is written. In the post-processing step,
these nodes are all merged into a single system call. A
beneﬁcial side-effect of node collapsing is that this does
not only reduce the number of nodes, but also provides
some level of abstraction from the concrete implementa-
tion of the malware code and the number of times iden-
tical functions are called (as part of a loop, for example).
Summary. The output of the two previous steps is one
behavior graph for each network connection that a mal-
ware sample makes. Behavior graphs can be used in two
ways: First, we can match behavior graphs, produced
by running unknown malware samples, against a set
of C&C templates that characterize malicious activity.
When a template matches, the corresponding network
connection can be labeled as command and control. This
matching procedure is explained in Section 3.6.
The second use of behavior graphs is for C&C tem-
plate generation. For this process, we assume that we
know some connections that are malicious and some that
are benign. We can then extract the subgraphs from
the behavior graphs that are related to known malicious
C&C connections and subgraphs that represent benign
activity. These two sets of malicious and benign graphs
form the input for the template generation process that
is described in the following three sections.
3.3 Graph Mining
The ﬁrst step when generating C&C templates is graph
mining. More precisely, the goal is to mine frequent sub-
graphs that are only present in the malicious set. An
overview of the process can be seen in Figure 2.
Figure 2: Mining process.
Frequent subgraphs are those that appear in more than
a fraction k of all malicious graphs. When k is too high,
we might miss many interesting behaviors (subgraphs)
that are not frequent enough to exceed this threshold.
When k is too low, more behaviors are covered, but un-
fortunately, the mining process will produce such a mas-
sive amount of graphs that it never terminates. We dis-
cuss the concrete choice of k in Section 4.
Frequent subgraph mining. There exist a number of
tools that can be readily used for mining frequent sub-
graphs. For this paper, we decided to use gSpan [47, 48]
because it is stable, and supports labeled graphs, both at
the node and edge level. gSpan relies on a lexicographic
ordering of graphs and uses a depth-ﬁrst search strategy
to efﬁciently mine frequent, connected subgraphs.
A limitation of gSpan is that it only supports undi-
rected edges, whereas behavior graphs are, by nature,
directed since the edges represent data ﬂows. To work
around this limitation and produce directed subgraphs,
we encode the direction of edges into their labels, and
then restore the direction information at the end of the
mining process. Moreover, gSpan accepts only numeric
values as labels for nodes and edges. Thus, we cannot
directly use the string labels (names or ﬂags) that are as-
Unfortunately,
these redundant,
intermediate sub-
graphs negatively affect the subsequent template gener-
ation steps because they distort the frequencies of nodes
and edges. To solve this problem, we introduce a max-
imization step. The purpose of this step is to remove
a subgraph Gsub if there exists a supergraph Gsuper in
the same result set that contains Gsub. Looking at Fig-
ure 2, the result of the maximization step is that all 2-
node graphs are removed because they are subgraphs of
the 3-node graphs. However, removing subgraphs is not
always desirable: even when both a subgraph Gsub and
a supergraph Gsuper exceed the frequency threshold k,
the subgraph Gsub might be much more frequent than
Gsuper.
In this case, both graphs should be kept. To
this end, we only remove a subgraph Gsub when its fre-
quency is less than twice the frequency of Gsuper.
Graph sets difference. So far, we have mined graphs
that frequently appear in the malicious set. However, we
also require that these graphs do not appear in the benign
set. Otherwise, they would not be suitable to distinguish
C&C connections from other trafﬁc.
sociated with nodes and edges in the behavior graphs.
To solve this, we simply concatenate all string labels of
a node or edge and hash the result. Then, this hash value
is mapped into a unique integer.
Subgraph maximization. The output produced by
gSpan contains many graphs that are subgraphs of oth-
ers. The reason is that gSpan works by growing sub-
graphs. That is, it ﬁrst looks for individual nodes that
are frequent. Then, gSpan adds one additional node and
re-runs the frequency checks. This add-and-check pro-
cess is repeated until no more frequent graphs can be
found. However, during this process, gSpan outputs all
subgraphs that are frequent. Thus, the result of the min-
ing step contains all intermediate subgraphs whose fre-
quency is above the selected threshold.
To remove graphs that are present in the benign set,
we compute the set difference between the frequent ma-
licious subgraphs and benign graphs. More precisely, we
use a sub-isomorphism test to determine, for each mali-
cious graph, whether it appears in some benign graphs.
If this is the case, it is removed from the mining results.
Looking at the example in Figure 2, the set difference
removes one graph that also appears in the benign set.
As an interesting technical detail, our approach of using
set difference to obtain interesting, malicious subgraphs
is different from the technique presented in [13]. In [13],
the authors use leap mining, which operates simultane-
ously on the malicious and benign sets to ﬁnd graphs
with a high frequency within the malicious set and a low
frequency within the benign set [46].
By construction, leap mining removes all parts from
the output that are shared between benign and malicious
graphs. For example, consider a behavior graph that cap-
tures a command that downloads data, stores it to a ﬁle,
and later executes this ﬁle. If the download part of this
graph is also present in the benign set, which is likely to
be the case (since downloading data is not malicious per
se), this part will be removed. Thus, the malicious graph
will only contain the part where the downloaded ﬁle is
executed. That is, in this example, leap mining would
produce an incomplete graph that covers only part of the
relevant, malicious activity. In our case, we ﬁrst gener-
ate the entire graph that captures both the download and
the execute. Then, the set difference algorithm checks
whether this entire graph occurs also in the benign set.
Since no benign graph is presumably a supergraph of the
malicious behavior, the entire graph is retained.
3.4 Graph Clustering
Using as input the frequent, malicious subgraphs pro-
duced by the previous mining step, the purpose of this
step is to ﬁnd clusters of similar graphs (see Figure 3).
The graph mining step produces different graphs that
represent different types of behaviors. We now need to
cluster these graphs to ﬁnd groups, where each group
shares a common core of activities (system calls) typi-
cal of a particular behavior. Graph clustering is used for
this purpose; generated clusters are later used to create
generalized templates covering the graphs they contain.
Figure 3: Clustering and generalization processes.
A crucial component for every clustering algorithm
is the proper choice of a similarity measure that com-
putes the distances between graphs. In our system, the
similarity measure between two graphs is based on their
non-induced, maximum common subgraph (mcs). The
mcs of two graphs is the maximal subgraph that is iso-
morphic to both. The intuition behind the measure is the
following: We expect two graphs that represent the same
malware behavior to share a common core of nodes that
capture this behavior. The mcs captures this core. Thus,
the mcs will be large for similar graphs. From now on,
all references to the mcs will refer to the non-induced
construction. The similarity measure is deﬁned as:
d(G1, G2) =
2 × |edges(mcs(G1, G2)|
|edges(G1)| + |edges(G2)|
(1)
To compute the mcs between two graphs, we use the
McGregor backtracking algorithm [6]. According to
benchmarking results [6], this algorithm performs well
on randomly-connected graphs with small density.
In
our case, behavior graphs have no cycles and only a lim-
ited number of dependencies; this is close to randomly-
connected graphs rather than regular or irregular meshes.
As shown in Figure 3, we use the mcs similarity mea-
sure to compute the one-to-one distance matrix between
all mined graphs. We then use a tool, called Cluto [24],
to ﬁnd clusters of similar graphs. Cluto implements a
variety of different clustering algorithms; we selected
clustering by repeated bisection. This algorithm works
as follows: All graphs are originally put into a single
cluster. This cluster is then iteratively split until the sim-
ilarity in each sub-cluster is larger than a given similarity
threshold [24, 50]. At each step, the cluster to be split is
chosen so that the similarity function between the ele-
ments of that clusters is maximized. The advantage of
this technique is that we do not need to deﬁne a ﬁxed
number of clusters a priori. Moreover, one also does not
need to select initial graphs as center to build the clusters
around (as with k-means clustering). The output of this
step is a set of clusters that contain similar graphs.
3.5 Graph Generalization and Templating
Based on the clusters of similar graphs, the ﬁnal step in
our template generation process is graph generalization
(the rightmost step in Figure 3). The goal of the gen-
eralization process is to construct a template graph that
abstracts from the individual graphs within a cluster. In-
tuitively, we would expect that a template contains a core
of nodes, which are common to all graphs in a cluster.
In addition, to capture small differences between these
graphs, there will be optional nodes attached to the core.
The generalization algorithm computes the weighted
minimal common supergraph (WMCS) of all the graphs
within a given cluster [3]. The WMCS is the minimal
graph such that all the graphs of the cluster are con-
tained within it. To distinguish between core and op-
tional nodes, we use weights. These weights capture
how frequent a node or an edge in the WMCS is present
in one of the individual graphs. For core edges and core
nodes, we expect that they are present in all graphs of a
cluster (that is, their weight is n in the WMCS, assuming
that there are n graphs in the cluster). All other nodes
with a weight smaller than n become optional nodes.
The approach to compute a template is presented in
Algorithm 1. The WMCS is ﬁrst initialized with the ﬁrst
graph G1 of the cluster, and the weights of all its nodes
and edges are set to 1. The integration of an additional
graph Gi is performed as follows: We ﬁrst determine
the maximal common subgraph mcs between Gi and the
current WMCS. The nodes and edges in the WMCS that
are part of the mcs have their weight increased by 1. The
Algorithm 1 Weighted minimum common supergraph
Require: A graph set G1, ..., Gn
1: W M CS ← G1
2: ∀n ∈ nodes(T ) and e ∈ edges(T ) do wn := 1 and we := 1
3: for i = 2 to n do
4:
5: mcs ← maximum common subgraph(Gi, W M CS, s)
6:
7:
8:
s := state exploration((cid:11))
∀n ∈ nodes(mcs) do wn + = 1
∀e ∈ edges(mcs) do we + = 1
∀n ∈ nodes(Gi) and n (cid:54)∈ nodes(mcs),