Data structures: The switch maintains two data structures. One
is the queue of actual packets which is maintained in RAM. Sec-
ond, is another queue that mirrors the packet queue, but only holds
packet metadata: a ﬂow-id (5-tuple or a hash of the 5-tuple) and
the packet’s priority number. This is maintained in ﬂops so that we
can get fast simultaneous access. pFabric switches have very small
queues; typically less than two bandwidth-delay products (∼36KB
or 24 full-sized packets in our simulations). Traditionally, datacen-
ter switches use nearly 10–30× more buffering per port.
437!"#$%&'()*%+'
!"#$
/%&"A."&"'B)%)%'
"'
3'
9'
:'
"'
?"+"&4+'
@+%%'
"' 3'
%&'()%&*(+$
,-.'
/"&#0-12'
73838'
!+-4+-&5'
61#4.%+'
78838'
Figure 2: For dequeueing, the switch ﬁnds the earliest packet
from the ﬂow with the highest priority and sends it out. In
the above example, even though the last packet (a, 1) has the
highest priority, the second packet in the queue which belongs
to the same ﬂow (a) is sent out because it arrived earlier.
Dequeue: For dequeueing, we ﬁrst ﬁnd the highest priority packet
by using a binary tree of comparators that operate hierarchically
on the metadata queue on the priority ﬁeld. If there are N pack-
ets, this operation takes log2(N) cycles. At this point, we could
simply send this highest priority packet out, however this can lead
to starvation for some packets when a ﬂow’s priority increases over
time. To see how, assume the priority is set to be the remaining ﬂow
size and consider the ﬂow to which the highest priority packet be-
longs. Since packets that are transmitted earlier have lower priority
than packets that are transmitted later (because they have relatively
higher remaining ﬂow sizes in their priority ﬁelds), if the ﬂow has
multiple packets waiting in the queue, the highest priority packet
among them is likely to have arrived later than the others. If we
send out packets purely in order of their priority, then this can lead
to situations where packets that arrived earlier might never get ser-
viced since more packets from that ﬂow keep arriving.
To tackle this problem, we implement a technique we christen
starvation prevention where we dequeue the earliest packet from
the ﬂow that has the highest priority packet in the queue. Since
packets are queued in the order they arrive, that is simply the ear-
liest packet in the queue that has the same ﬂow-id as the packet
with the highest priority. Hence in the second step we perform a
parallelized bitwise compare on this ﬂow-id for all the packets in
the meta-data queue. The output of this compare operation is a bit-
map with a 1 wherever there is a match and 0 otherwise. We pick
the packet corresponding to the earliest 1 in the bit vector by us-
ing a priority encoder and transmit it. Figure 2 demonstrates the
dequeuing algorithm as discussed above.
Enqueue: For enqueuing, if the queue is not full, the packet is just
added to the end of the queue and the metadata queue is updated.
If the queue is full, we use a similar binary tree of comparators
structure as in the dequeuing operation above, but this time to ﬁnd
the packet with the lowest priority. That packet is dropped from
both the packet and metadata queues and the new packet is added
to the end of the queue.
4.2 Rate Control Design
What about rate control? If the fabric schedules ﬂows as dis-
cussed above, the need for rate control is minimal. In particular,
we do not need rate control to prevent spurious packet drops due
to bursts, as can occur for example in Incast [19] scenarios. Such
events only impact the lowest priority packets at the time which can
quickly be retransmitted without impacting performance (see §4.3).
Further, we do not need to worry about keeping queue occupan-
cies small to control queueing latency. Since packets are scheduled
based on priority, even if large queues do form in the fabric, there
would be no impact on the latency for high-priority trafﬁc.
However, there is one corner case where a limited form of rate
control is necessary. Speciﬁcally, whenever a packet traverses mul-
tiple hops only to be dropped at a downstream link some bandwidth
is wasted on the upstream links that could have been used to trans-
mit other packets. This is especially problematic when the load is
high and multiple elephant ﬂows collide at a downstream link. For
example, if two elephant ﬂows sending at line rate collide at a last-
hop access link, half the bandwidth they consume on the upstream
links is wasted. If such high loss rates persist, it would eventually
lead to congestion collapse in the fabric. Note that packet drops
at the ingress (the source NICs) are not an issue since they do not
waste any bandwidth in the fabric.
We use the above insight to design an extremely simple rate con-
trol that we implement by taking an existing TCP implementation
and throwing away several mechanisms from it. We describe the
design by walking the reader through the lifetime of a ﬂow:
• Flows start at line rate. Practically, this is accomplished by
using an initial window size equal to the bandwidth-delay
product (BDP) of the link (12 packets in our simulations).
• We use SACKs and for every packet acknowledgement we
do additive increase as in standard TCP.
• There are no fast retransmits, dupACKs or any other such
mechanisms. Packet drops are only detected by timeouts,
whose value is ﬁxed and small (3× the fabric RTT, which is
around 45µs in our simulations). Upon a timeout, the ﬂow
enters into slow start and ssthresh is set to half the win-
dow size before the timeout occurred.
• If a ﬁxed threshold number of consecutive timeouts occur
(5 in our current implementation), it indicates a chronic con-
gestion collapse event. In this case, the ﬂow enters into probe
mode where it periodically retransmits minimum-sized pack-
ets with a one byte payload and re-enters slow-start once it
receives an acknowledgement.
This is the entire rate control design. We do not use any sophis-
ticated congestion signals (either implicit such as 3 dupACKs or
explicit such as ECN, XCP etc), no complicated control laws (we
use additive increase most of the time and just restart from a win-
dow of 1 if we see a timeout), nor do we use sophisticated pacing
mechanisms at the end host. The only goal is to avoid excessive
and persistent packet drops which this simple design accomplishes.
Remark 2. Our rate control design uses the minimal set of mech-
anisms that are actually needed for good performance. One could
of course use existing TCP (with all its features) as well and only
increase the initial window size and reduce the minimum retrans-
mission timeout (minRT O).
4.3 Why this Works
Since pFabric dequeues packets according to priority, it achieves
ideal ﬂow scheduling as long as at each switch port and at any time
one of the highest priority packets that needs to traverse the port is
available to be scheduled. Maintaining this invariant is complicated
by the fact that, sometimes, buffers overﬂow and packets must be
dropped. However, when a packet is dropped in pFabric, by de-
sign, it has the lowest priority among all buffered packets. Hence,
even if it were not dropped, its “turn” to be scheduled would not
be until at least all the other buffered packets have left the switch.
438(the packet’s turn may end up even further in the future if higher
priority packets arrive while it is waiting in the queue.) Therefore,
a packet can safely be dropped as long as the rate control is aggres-
sive and ensures that it retransmits the packet (or sends a different
packet from that ﬂow) before all the existing packets depart the
switch. This can easily be achieved if the buffer size is at least
one bandwidth-delay product and hence takes more than a RTT to
drain, providing the end-host enough time to detect and retrans-
mit dropped packets. Our rate control design which keeps ﬂows at
line-rate most of the time is based on this intuition.
4.4 Implementation
A prototype implementation of pFabric including the hardware
switch and the software end-host stack is beyond the scope of this
paper and is part of our future work. Here, we brieﬂy analyze the
feasibility of its implementation.
Switch implementation: Priority scheduling and dropping are rel-
atively simple to implement using well known and widely used
hardware primitives because pFabric switches have very small buffers
— typically about two BDPs worth of packets at each port which
is less than ∼36KB for a 10Gbps 2-tier datacenter fabric. With a
36KB buffer, in the worst-case of minimum size 64B packets, we
have 51.2ns to ﬁnd the highest/lowest of at most ∼600 numbers,
which translate to ∼40 clock cycles for today’s switching ASICs.
A straight-forward implementation of this using the binary com-
parator tree discussed in §4.1 requires just 10 (log2(600)) clock
cycles, which still leaves 30 cycles for the ﬂow-id compare oper-
ation. This can be done in parallel for all 600 packets, but it is
preferable to do it sequentially on smaller blocks to reduce the re-
quired gates and power-draw. Assuming a 64 block compare that
checks 64 ﬂow-ids at a time (this is easy and commonly imple-
mented in current switches), we require at most 10 clock cycles for
all 600 packets. Hence we need a total of 20 clock cycles to ﬁgure
out which packet to dequeue, which is well within the budget of 40
clock cycles. The analysis for the enqueuing is simpler since the
only operation there is the operation performed by the binary tree
of comparators when the queue is full. As discussed above, this is
at most 10 clock cycles.
A number of optimizations can further simplify the pFabric switch
implementation. For instance, we could use a hash of the 5-tuple
as the ﬂow-id (instead of the full 5-tuple) to reduce the width of the
bit-wise ﬂow-id comparators. A fairly short hash (e.g., 8–12 bits)
should sufﬁce since the total number of packets is small and occa-
sional hash collisions only marginally impact the scheduling order.
Moreover, if we restrict the priority assignments such that a ﬂow’s
priority does not increase over time — for example by using abso-
lute ﬂow size as the priority instead of remaining ﬂow size — we
would not need the starvation prevention mechanism and could get
rid of the ﬂow-id matching logic completely. Our results indicate
that using absolute ﬂow size is almost as good as remaining ﬂow
size for realistic ﬂow size distributions found in practice (§5.4.3).
Note that our switches do not keep any other state, nor are they
expected to provide feedback, nor do they perform rate computa-
tions. Further, the signiﬁcantly smaller buffering requirement low-
ers the overall switch design complexity and die area [4].
End-host implementation: pFabric’s priority-based packet schedul-
ing needs to extend all the way to the end-host to be fully effective.
In fact, we think of the fabric as starting at the NIC (§3) and in our
simulations we assume that the NIC queues also implement pFab-
ric’s priority scheduling/dropping mechanisms. An alternative de-
sign may push the contention to software queues by rate-limiting
the trafﬁc to the NIC (at line rate). Priority scheduling can then be
implemented in software across active ﬂows. This approach does
4567(%&
8#79)2&!)*3%&
+567(%&
:;<"&!)*3%&
0&1#23%&
'()*"%&
!"#$%&
+,&-.%/%& +,&-.%/%&
+,&-.%/%& +,&-.%/%&
Figure 3: Baseline topology used in simulations.
not require NIC changes and also avoids dropping packets at the
end-host but it requires more sophisticated software particularly at
10Gbps speeds.
The reader may also wonder about the feasibility of our rate con-
trol implementation. Speciﬁcally, our rate control frequently oper-
ates at line rate and uses a ﬁxed retransmission timeout value typi-
cally set to 3×RTT which can be quite small (e.g., we use 45µs in
our simulations). Such precise timers may be problematic to im-
plement in current software. However our simulations show that
the timeout can be set to larger values (e.g., 200–300µs for our
simulated network) in practice without impacting performance (see
§5.4.3 for details). Prior work has demonstrated the feasibility of
such retransmission timers in software [19].
Finally, it is important to note that while our rate control design is
based on TCP, we do not require that the rate control be done by the
TCP stack in the kernel. In fact, we expect the near-ideal latency
provided by pFabric to most beneﬁt applications that are optimized
to reduce the latency incurred at the end-host. Such applications
(e.g., RAMCloud [16]) typically use techniques like kernel bypass
to avoid the latency of going through the networking stack and im-
plement some form of rate control in user-space. We believe our
simple rate control is a nice ﬁt in these scenarios.
5. EVALUATION
In this section we evaluate pFabric’s performance using exten-
sive packet-level simulations in the ns2 [15] simulator. Our eval-
uation consists of three parts. First, using carefully constructed
micro-benchmarks, we evaluate pFabric’s basic performance such
as its loss characteristics, its ability to efﬁciently switch between
ﬂows that are scheduled one-by-one, and how it handles Incast [19]
scenarios. Building on these, we show how pFabric achieves near-
optimal end-to-end performance in realistic datacenter networks
running workloads that have been observed in deployed datacen-
ters [12, 3]. Finally, we deconstruct the overall results and demon-
strate the factors that contribute to the performance.
5.1 Simulation Methodology
Fabric Topology: We use the leaf-spine topology shown in Fig-
ure 3. This is a commonly used datacenter topology [1, 12]. The
fabric interconnects 144 hosts through 9 leaf (or top-of-rack) switches
connected to 4 spine switches in a full mesh. Each leaf switch has
16 10Gbps downlinks (to the hosts) and 4 40Gbps uplinks (to the
spine) resulting in a non-oversubscribed (full bisection bandwidth)
fabric. The end-to-end round-trip latency across the spine (4 hops)
is ∼14.6µs of which 10µs is spent in the hosts (the round-trip la-
tency across 2 hops under a Leaf is ∼13.3µs).
Fabric load-balancing: We use packet spraying [11], where each
switch sprays packets among all shortest-path next hops in round-
robin fashion. We have also experimented with Equal Cost Mul-
tipathing (ECMP) which hashes entire ﬂows to different paths to
avoid packet reordering. Overall, we found that for all schemes,
439Flow Size
Total Bytes
104
105
106
Flow Size (Bytes)
107
(a) Web search workload
Flow Size
Total Bytes
108
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
12
10
8
6
4
2
0
0
)
%
(
e
a
r
t
p
o
r
D
flow1
flow2
flow3
flow4
flow5
80
20
40
Time (ms)
60
(a) Flow switching
w/o probe mode
with probe mode
40
30
20
10
0
0
10
20
30
#Flows
40
50
(b) Loss rate
F
D
C
F
D
C
1
0.8
0.6
0.4
0.2
0
103
1
0.8
0.6
0.4
0.2
0