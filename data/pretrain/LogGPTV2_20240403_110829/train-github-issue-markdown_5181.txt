Currently to load pretrained tensor into an Embedding I am running:
    embedding = torch.nn.Embedding(length, dim)
    embedding.weight.data = pretrained_vectors_tensor
This first initializes a tensor of size `length x dim` which can be quite
large and then throws all of that away. When creating the embedding there are
two big tensors in memory which fill up my RAM, while there could be only one
if there was a possibility to initialize weights from a parameter.