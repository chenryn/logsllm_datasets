Theorem A.6
or
-sample [23], [11]). Let (X, C) be a concept space of
VC dimension d. Let D be a probability distribution on X and
let S be sample drawn from X according to D. Let  > 0 and
δ > 0. Suppose S is an -net (or -sample) with probability
at least 1 − δ. Then |S| = Ω(cid:0) d
(Necessary
conditions
(cid:1).
-net
for
 + 1
 log 1
δ
D. PAC Learning
Introduced by Valiant [8], PAC learning is concerned with
algorithms which learn from labelled examples. (We restrict
our attention to realizable and consistent PAC learning; for a
more general treatment see [9].) Using the terminology above,
a learner L is an algorithm which takes as input a transcript
{(si, C(si))}m
i=1 of elements from X (where each si ←$ π for
some distribution π on X) along with their labels according
to the unknown concept C. A learner outputs a hypothesis
H ∈ C representing its guess for C. The learner L is a
PAC learner for C if for any C ∈ C, distribution π on X,
and 0  ˆpb and
pa > pb, keep that orientation, else choose the other one.
Though quite naive, below we will see this heuristic generally
works well for real data distributions.
To guess the number of sacriﬁced records below the ﬁrst
sorted group (EstimateRank in Algorithm 4), we use a more
principled approach. Observe that the sacriﬁced records are
exactly those with values either lower than the value of the
smallest left query endpoint (call this value (cid:96)min) or higher
than the value of the largest right endpoint (call this rmax). Let
Eij = ((cid:96)min = i) ∩ (rmax = j) be the event that (cid:96)min is i and
rmax is j. Let the number of sacriﬁced records be S and r0 be a
random variable denoting the smallest rank for a record in A1.
The RV r0 takes values in [0, . . . , S ]. Conditioned on Eij, the
distribution of the number of sacriﬁced records to the left of i
and right of j is binomial with sample size S and probability
Pr[ 1,...,i ]+Pr[ j,...,N ] where Pr [ x, . . . , y ] =
of success pij =
k=x π(k) and π is the auxiliary distribution. Thus, for any
r ∈ [0, . . . , S ],
(cid:80)y
Pr[ 1,...,i ]
Pr [ r0 = r ] =
=
Pr [ r0 = r | Eij ] Pr [ Eij ]
(cid:19)
(cid:18)S
ij(1 − pij)S−r Pr [ Eij ] .
pr
(cid:88)
(cid:88)
i≤j∈[N ]
r
i≤j∈[N ]
If the number of queries is Q and the query distribution is
uniform, we can compute Pr [ Eij ] via inclusion-exclusion as
follows. First, deﬁne f (x, y) = (x− y)(y − x + 1)/N (N + 1).
Then
Pr [ Eij ] = f (i, j)Q − f (i, j − 1)Q
− f (i + 1, j)Q + f (i + 1, j − 1)Q .
q
q
function EstimateRank is then E [r0] =(cid:80)S
This is the only part of the attack that uses the uniform
distribution on queries. If we let π[i,j]
be the probability
that a query is contained in the range [i, j], with a non-
uniform query distribution this expression would be the same
except with f (·,·) replaced by π[·,·]
. The value ˆr0 output by
r=0 r Pr [ r0 = r ].
The expression Pr [ r0 = r ] has O(N 2) terms, which could
make the attack scale poorly. Our implementation uses a
heuristic to discard the terms for which Pr [ Eij ] is very small,
so computing E [r0] (a one-time operation) takes only about
eighty minutes in the worst case. Once we compute ˆr0 we can
ﬁnd the lower and upper ranks for the Ai via addition; see the
line assigning ri in Algorithm 4.
Partition estimation. The output of the previous step is a
lower and upper rank (call them rlb and rub) for each Ai. From
this we will recover a lower and upper value (eplb and epub)
used by the ﬁnal step of the attack. To estimate values from
ranks, we use order statistics. For a sample X1, . . . , Xs, the
kth order statistic (denoted X(k), k ∈ [1, . . . , s]) is the kth
largest value in the sample. The probability Pr(cid:2) X(k) = u(cid:3)
1082
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:08 UTC from IEEE Xplore.  Restrictions apply. 
(cid:19)
(cid:18)s
s−k(cid:88)
has a simple formula: Pr(cid:2) X(k) = u(cid:3) = Pr(cid:2) X(k) ≤ u(cid:3) −
Pr(cid:2) X(k) ≤ u − 1(cid:3), where Pr(cid:2) X(k) ≤ u(cid:3) is
Using this, we estimate eplb = maxx Pr(cid:2) X(rlb) = x(cid:3) and
distribution of Pr(cid:2) X(rlb) = x(cid:3) converges to a Gaussian very
quickly, so maxx Pr(cid:2) X(rlb) = x(cid:3) ≈ E(cid:2)X(rlb)
do the same for epub. For a ﬁxed rank and varying x, the
(1 − Pr [ 1, . . . , u ])j Pr [ 1, . . . , u ]s−j .
(cid:3).
j=0
j
Database estimation. This is the simplest step—the previous
step outputs a partition [1, ep1, . . . , ep
|B|, N ] of [N ] where the
records in group bi are between epi and epi+1, so we need
only choose a value in [epi, epi+1] to assign to the records
in bi. Since we are concerned with minimizing the absolute
value of the difference between the true value and the guess,
the natural choice is the median of the database distribution
π, conditioned on the range [epi, epi+1]. In Algorithm 4 this
is written as RangeMedian(π, epi, epi+1).
APPENDIX D
ATTACK PSEUDOCODE
In this appendix we give detailed pseudocode descriptions
of some attacks from the main body of the article.
In Algorithm 3, the following notation related to PQ-trees
is used. If T is a PQ-tree, and T is a node of T , then the
leaves of T are deﬁned as the leaves of the subtree rooted at
T , and denoted leaf(T ). If T is a PQ-tree, root(T ) is the root
of T .
Algorithm 1 Estimating symval.
Input: Set of queries Q.
Output: Function est-symval approximating symval.
c(r) ← |{q ∈ Q : r ∈ q}|/|Q|
est-symval(r) ← arg mink∈[N/2] |p(k) − c(r)|
1: for each record r do
2:
3:
4: end for
5: return est-symval
Algorithm 2 ADR Algorithm ApproxValue..
ApproxValue(Q):
Input: Set of queries Q.
Output: Function est-val approximating val.
c(r) ← |{q ∈ Q : r ∈ q}|/|Q|
(cid:101)v(r) ← arg mink |c(r) − p(k)|
1: for each record r do
2:
3:
4: end for
5: rA ← arg minr |(cid:101)v(r) − N/4|
6: (cid:102)vA ←(cid:101)v(rA)
c(cid:48)(r) ← |{q ∈ Q : rA, r ∈ q}|/|Q|
(cid:102)wL ← arg mink∈[1,(cid:102)vA] |d((cid:102)vA, k) − c(cid:48)(r)|
(cid:102)wR ← arg mink∈[(cid:102)vA,N ] |d((cid:102)vA, k) − c(cid:48)(r)|
if c(r)  R/2 leaves.
if |leaf(T , C)| > R/2 then
1: R ← |leaf(root(T ))|
2: for each child C of S do
3:
4:
5:
6: end for
7: return S
end if
return FindNodeT(Q,T , C)
Algorithm 4 Recovering values from approximate order.
Input: A(cid:48)1, . . . , A(cid:48)k, e, R,π,πq,N.
Output: [x1, x2, . . . , xR] (∀i,xi ∈ [N ]).
1: A1, . . . , Ak ← OrientSubsets(A(cid:48)1, . . . , A(cid:48)k)
2: ˆr0 ← EstimateRank(e, πq, π)
3: for all Ai, i ∈ [1, . . . , k] do
4:
5:
6:
7:
8:
9:
10: end for
11: return cand
epi ← arg maxk∈[N ] Pr(cid:2) X(ri) = k(cid:3)
ri ← ri−1 + |Ai|
medAi ← RangeMedian(π, epi−1, epi)
for all ind ∈ Ai do
cand[ind] = medAi
end for
1083
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:08 UTC from IEEE Xplore.  Restrictions apply.