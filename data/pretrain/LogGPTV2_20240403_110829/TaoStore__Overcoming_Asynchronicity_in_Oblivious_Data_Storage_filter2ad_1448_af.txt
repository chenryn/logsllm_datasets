)
s
/
s
p
o
(
t
u
p
h
g
u
o
r
h
T
50
40
30
20
10
0
0.000
0.016
0.031
0.063
Storage Cache/Dataset Size Ratio 
)
s
m
i
(
e
m
T
e
s
n
o
p
s
e
R
300
250
200
150
100
50
0
40
80
160
240
Write Back Threshold
)
s
/
s
p
o
(
t
u
p
h
g
u
o
r
h
T
50
40
30
20
10
0
40
80
160
240
Write Back Threshold
(d) Effect of caching at
server on throughput
the untrusted
(e) Effect of write-back threshold on re-
sponse time
(f) Effect of write-back threshold on
throughput
o
i
t
a
R
e
c
r
u
o
s
t
u
O
x
a
M
0.06
0.05
0.04
0.03
0.02
0.01
0
55.8
40.2
23.3
23.6
40
80
160
240
Write Back Threshold
)
s
m
i
(
e
m
T
e
s
n
o
p
s
e
R
TaoStore
ObliviStore
TaoStore
ObliviStore
400
300
200
100
0
10
30
20
50
Number of Concurrent Clients
40
300
250
200
150
100
50
0
)
s
/
s
p
o
(
t
u
p
h
g
u
o
r
h
T
10
60
30
20
50
Number of Concurrent Clients
40
60
(g) Effect of write-back threshold on maxi-
mum outsource ratio (Data labels represent
maximum utilized memory in MBytes)
(h) Effect of number of concurrent clients
on response time
(i) Effect of number of concurrent clients
on throughput
Fig. 8: TaoStore Performance Analysis
retrieved from the untrusted cloud storage before a write-back
operation is initiated. A large k requires storing more data
at the trusted proxy. However, this results in triggering less
write-back operations and performing them in bigger batches.
The effects of this parameter in terms of the average response
time and throughput are demonstrated in Figure 8(e) and 8(f).
As it can be seen in the results, there is no signiﬁcant change
in the performance with respect to k. This explicitly shows the
design advantages of the non-blocking write-back mechanism,
since the system performance is independent of the frequency
of write-backs.
d) Memory and Bandwidth Overhead: TaoStore’s mem-
ory overhead mostly depends on the write-back threshold k.
In our experiments, we observe that the number of stored
blocks in the stash usually does not exceed 2k. When k equals
40, the stash usually does not contain more than 80 blocks,
which requires approximately 320 KB in memory. Therefore,
the stash memory overhead is a small constant, while the
subtree uses more memory to store retrieved blocks from the
untrusted storage. The overall memory usage for the trusted
proxy is usually not more than 24 MB when k = 40 as shown
in Figure 8(g), which has an approximate outsource ratio of
0.02. The outsource ratio is the ratio of maximum memory
usage at the trusted proxy over dataset size. To answer one
client query, the trusted proxy needs to fetch approximately 16
buckets, i.e., 256 KB. Increasing the ﬂush trigger count results
in using more memory at the trusted proxy; however, there is
not much performance gain from increasing the write-back
threshold. When k = 240, the trusted proxy uses a maximum
of 55.8 MB memory, but achieves a throughput of 39.09 ops/s.
The results show that TaoStore can deliver a good performance
with a very low outsource ratio.
2) Comparison with Other Works: We now compare Tao-
Store with Path ORAM and ObliviStore to show how TaoStore
can achieve high throughput and lower response times. The
implementation of ObliviStore was provided by its authors12
and we implemented our own version of Path ORAM. All
experiments in this section are simulation based and have the
same conﬁguration.
Path ORAM provides relatively low response times of 63.63
ms with a corresponding throughput of 7.9 ops/s. Since Path
ORAM does not support concurrency, it is not fair to compare
12We would like to thank the authors of ObliviStore for providing us the
implementation graciously.
211211
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
it directly with TaoStore. However, the results highlight the
importance of providing concurrency for cloud storage systems
(also highlighted in [4]).
Although ObliviStore is not secure over asynchronous net-
works and fails to provide complete access privacy when con-
current requests access the same item even over synchronous
networks, the comparisons with ObliviStore aim to provide in-
sights about TaoStore’s performance while providing stronger
security. Note that the simulation based experiments assume a
50 ms ﬁxed round-trip network latency. Such an assumption
prevents network bandwidth limitation issues. Once data is
fetched from the disk drive, operations are executed in memory
with delays on the order of 1 ms. The performance is affected
mainly by the ORAM client side processing and data retrieval
from the disk. Please note that since a uniformly distributed
workload is used in the experiments,
the probability for
accessing the same ORAM blocks, which causes a slowdown
for ObliviStore as highlighted in [4], is negligible.
Response times and throughput are compared for both
systems in Figures 8(h) and 8(i), respectively. TaoStore and
ObliviStore achieve their highest performances at 30 and
50 clients, respectively. When the number of clients is 30,
TaoStore reaches a throughput of 250.79 ops/s with a response
time of 117.91 ms. When the number of concurrent clients is
30, ObliviStore delivers a throughput of 159.35 ops/s with a
response time of 209.07 ms. Hence, TaoStore achieves 57%
high throughput with 44% lower response time. ObliviStore
has performance issues against demanding applications due
to its complex background shufﬂing and eviction operations
(also pointed out in [4]). It deploys an internal scheduler
to manage evictions and client requests but in contrast to
TaoStore, the eviction process is not directly decoupled from
the client request processing. The scheduler schedules a client
request if the system has enough resources available. When the
client request is scheduled, it acquires some amount of system
resources and these resources are released once the eviction
operations are completed. On the other hand, TaoStore can
process client requests concurrently and asynchronously, and
the write-back operations are decoupled from the client re-
quest processing. This allows TaoStore to continue processing
client requests while one or more write-back operations are
ongoing. With 30 concurrent clients, available resources are
utilized aggressively to provide better performance in terms
of throughput and response time. This explicitly demonstrates
the design advantages of TaoStore compared to ObliviStore.
If the number of concurrent clients goes above 30, Taostore’s
throughput shows a slight decline and the response time
increases, due to the increased contention on processing units
and I/O. TaoStore’s performance plateaus after 40 clients
with a throughput of 211-215 ops/s. ObliviStore’s achieves
its highest throughput of 218.56 ops/s with a response time of
254.45 ms at 50 clients.
In these experiments, a 13 GB dataset is used as in the
experimental setup for ObliviStore [36]. In order to operate
over a 13 GB dataset, TaoStore requires 15.9 GB physical
disk storage in the untrusted cloud storage, while ObliviStore
requires 42.9 GB. The difference in storage overhead is due
to a signiﬁcant number of extra dummy blocks ObliviStore
requires [36], i.e., if a level in a partition is capable of storing
up to x number of real blocks, the same level stores x or more
dummy blocks. However, in tree ORAMs, dummy blocks are
used to pad buckets if they contain a lower number of real
blocks than their capacity. As also seen in the results, TaoStore
is a lot less costly compared to ObliviStore in terms of required
physical disk storage.
Our evaluations show that TaoStore handles ﬂush and write-
back operations better than ObliviStore, which leads to a high
client request processing performance.
VI. CONCLUSION AND ONGOING WORK
TaoStore is a highly efﬁcient and practical cloud data store,
which secures data conﬁdentiality and hides access patterns
from adversaries. To the best of our knowledge, TaoStore
is the ﬁrst tree-based asynchronous oblivious cloud storage
system. Additionally, we propose a new ORAM security model