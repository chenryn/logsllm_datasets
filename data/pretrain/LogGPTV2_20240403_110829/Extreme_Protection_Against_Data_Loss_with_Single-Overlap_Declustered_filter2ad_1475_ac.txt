k + m disks, our goal is to select k + m − 1 disk distances.
We start with the minimum distance 1 and inject it into an
empty distance array. For any next distance (e.g., 2 or 3), we
can put it before or after the existing distances in the array.
The valid injection is to ensure no equal disk distance. For
example, Figure 11 injects the next distance 3 at the beginning
of the array, which leads to an equal distance of the sum of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
348
next two distances (e.g., 3 = 2 + 1). In this case, the derived
stripesets based on this base stripeset will cause two disks to
be overlapping.
Algorithm 2: Greedy SODP (G-SODP)
Input: N, disks per server; s, stripe size
Output: B={b1, b2, ...,}, base stripesets
while b = createBaseStripesets(B, N, s) do
B ← b
end
function CREATEBASESTRIPESETS(B, N, s)
b =[1]
for i = 2: N-1do
for j = 0 :length(b) do
b’ = [b[0:j-1], i, b[j:end]]
if distances not in B then
if distances in b’ are not equal then
b = b’
break
end
end
end
if length(b) == s-1 then
return b
end
end
end function
G-SODP(N, 6)
BIBD(N, 6, 1)
 1400
 1200
 1000
 800
 600
 400
 200
k
s
D
i
r
e
p
s
t
e
s
e
p
i
r
t
S
#
 0
 50
 100
 150
 200
#Drives
k
s
D
i
r
e
p
s
t
e
s
e
p
i
r
t
S
#
 40
 35
 30
 25
 20
 15
 10
 5
G-SODP(N, 6)
BIBD(N, 6, 1)
 0
 50
 100
 150
 200
#Drives
Fig. 12: Total number of stripesets and number of stripesets
per disk for G-SODP and a deﬁned BIBD conﬁguration.
Figure 12 compares the total number of stripesets and
stripesets per disk between BIBD and G-SODP. Here,
BIBD(N,6,1) represents a full declustering layout over N
disks, which only exists for a limited number of conﬁgurations.
G-SODP aims to ﬁnd a balanced layout for arbitrary disk size
(e.g., N > 31). The comparison results show that G-SODP
has fewer stripesets than BIBD(N,6,1), which indicates a gap
between G-SODP and the full declustering BIBD. When a
disk fails, G-SODP cannot guarantee that every surviving disk
participates in that disks recovery (e.g., shorter rebuild time).
However, it still attempts to maximize the rebuild performance
in a balanced way, which in turn generates less stripesets than
O-SODP to tolerate more concurrent disk failures.
COMPARISON OF O-SODP AND G-SODP Both O-SODP
and G-SODP are able to generate single overlap stripesets in
a balanced way. The only difference is O-SODP constructs
perfectly balanced declustered layouts, where each pair-wise
set of disks appears in exactly one stripeset. The perfect
balance and exact overlap value of one make the number of
stripesets minimized in the declustered layout. This design
is not possible for all disk conﬁgurations (e.g., 8 total disks
using 3-disk stripesets). G-SODP relaxes the overlap constraint
slightly such that a few pair-wise combinations are not gen-
erated but a wider range of disk conﬁgurations are supported.
Thus G-SODP provides greater conﬁguration ﬂexibility and
fault tolerance while sacriﬁcing a small amount of rebuild
performance.
s
t
e
s
e
p
i
r
t
S
#
 100
 80
 60
 40
 20
 0
G-SODP(s2, s)
O-SODP(s2, s)
9
16 25 36 49 64 81 100
#Drives
k
s
D
i
r
e
p
s
t
e
s
e
p
i
r
t
S
#
 12
 10
 8
 6
 4
 2
 0
G-SODP(s2,s)
O-SODP(s2,s)
9 16 25 36 49 64 81 100
#Drives
Fig. 13: Total number of stripesets and number of stripesets
per disk for O-SODP and G-SODP.
Figure 13 compares the total number of stripesets and
stripesets per disk between O-SODP and G-SODP. As previ-
ously stated, some conﬁgurations (e.g., 36 and 100 disks) are
not supported in O-SODP, other conﬁgurations show similar
results between G-SODP and O-SODP. When considering the
number of stripesets per disk, G-SODP always generates one
less stripeset than O-SODP does accounting for the small
difference in total stripesets. In section V, we will provide
a detailed comparison of O-SODP and G-SODP protecting
against concurrent failures.
IV. SOL-SIM AND COFACTOR DESIGN
SOL-Sim is a discrete-event simulator that characterizes the
reliability of erasure coded storage systems. Written in Python,
SOL-Sim extends SimEDC [27] to supports additional erasure
codes, chunk placement schemes, and data re-protection algo-
rithms. Figure 14 shows the high-level SOL-Sim architecture
which uses failure traces, disk layouts, and data protection
schemes as input and returns timings and reliability metrics
such as the probability of data loss (PDL) as output. SOL-
Sim is designed to simulate reliability over longer periods
of time (e.g., 5 years). One key component of the SOL-
Sim architecture is the ability to use a complementary tool,
CoFaCTOR, to evaluate multiple storage system designs over
their entire lifetime easily.
A. SOL-Sim Architecture
In order to evaluate the data protection schemes in this paper
we have combined the output of CoFaCTOR with simulation
to evaluate the probability of data loss over a variety of
realistic failure workloads. SOL-Sim stores all events in an
event queue, which always returns the event with the smallest
timestamp. If the event is a failure event, it will update the
disk state, such as the clock and failure status. If the event
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
349
of the stripes continue to be reconstructed (called degraded
reconstruction). While this algorithm is the state of the art it
is not widely available for production systems.
B. CoFaCTOR Survival Analysis Model
While data on hard disk failures from Trinity can be used
to test the reliability of data-redundancy patterns, generating
synthetic failure streams with ’realistic’ failures can asses the
robustness of different approaches to unknown future failures.
By ’realistic’ in this case we mean that the failure times look
similar in distribution to those observed previously on Trinity.
We designed CoFaCTOR to generate these synthetic failure
streams which are seeded by real failure data. A common
assumption in reliability is to assume independent, identically
distributed (iid) Weibull or Exponential failure times [8];
however, the exploratory analysis our Trinity failure traces
showed correlation in failure rates with respect to the system
conﬁguration. In order to account for the relationship between
the failure event times, we apply a two-parameter Weibull
regression model, also commonly known as an accelerated
failure time model [28]. Prior work has shown Weibull distri-
butions to accurately reﬂect disk drive failures, both correlated
and non-correlated [29]. The probability distribution function
for the Weibull model is:
f (x) = ρ
λ
(cid:5)ρ−1
−(x/λ)ρ
e
(cid:4)
x
λ
,
where λ is a scale parameter and ρ is the shape parameter.
The scale controls the size of the failure rate and the shape
controls whether the failure rate increases, decreases, or stays
constant over time. In the Weibull regression model, both the
shape and scale are modeled as a linear function of covariates
describing the system conﬁguration:
λ = Xβλ
ρ = Xβρ,
(1)
where X is the matrix of covariate information, βλ is the
regression coefﬁcient vector for λ, and βρ is the same for ρ.
This model is ﬁt using maximum likelihood using the lifelines
package in Python [30].
We are able to relax the iid assumption because the Weibull
parameters vary as a function of system conﬁguration details
described by the covariates. Given a set of regression coef-
ﬁcients [βλ, βρ], failure times for a drive with a particular
covariate vector can be drawn as independent Weibull random
variates. It is important to note, that in this model we are
estimating component lifetimes. For Trinity, we focused on
three covariates that indicated accelerate failure rates - ﬁle
system ID, drawer row, and vertical position in the rack.
Figure 15 shows a comparison of simulated survival curves
using the Weibull regression model (red) to the observed
failure time data for two combinations of the covariate values.
Speciﬁcally the blue line and shaded region show a nonpara-
metric, Kaplan-Meier (KM) estimate [28] to the Trinity data
and the 95% conﬁdence interval for the survival curve. Qual-
itative difference between the KM estimate and the Weibull
Fig. 14: SOL-Sim Architecture.
is a repair event it updates the corresponding disk’s clock,
repair status (e.g, critical to degraded or degraded to normal),
and repair priority. SOL-Sim and CoFaCTOR provide the
following features:
1) System Layout & System Failure Data:
In order to
generate a large set of realistic failure traces CoFaCTOR is
seeded with both a system layout and a set of failure data
collected from real system data. The system layout describes
physical characteristics of the system that inﬂuence failure
such as the physical position with the data center (data center
row and rack number), the vertical position within the rack and
even the disk positions within the storage enclosure (called
the drawer row here to differentiate disks near the front of the
enclosure, disks in the center of the enclosure, and disks near
the rear of the enclosure). Detailed layout data in combination
with positional data is critical in generating the set of failure
traces and candidate system designs for SOL-Sim.
2) Failure Traces & System Designs: CoFaCTOR generates
an arbitrarily large number of synthetic failure traces for use
by SOL-Sim. These synthetic failure traces are generated using
the survival analysis models seeded with real failure data. The
second input into SOL-Sim is the set of system designs output
by CoFaCTOR. These conﬁgurable system designs enable us
to apply the generated failure traces to ﬂexible system designs
that explore both the physical system design space and the
data protection algorithms used. For example, in this analysis
we are able to alter the number of disks per enclosure (i.e. the
failure domain for the declustered parity grouping) to explore
future storage systems which are expected to be much denser
than our existing system design.
3) Chunk Placement: SOL-Sim enables the use of multiple
declustered placement algorithms and data protection schemes
in conjunction with the failure data including:
traditional
RAID, complete declustered parity designs, dRAID, and both
O-SODP and G-SODP depending on the availability of a
single-overlap conﬁguration for that design point.
4) Priority Reconstruction: SOL-Sim also implements a
priority reconstruction algorithm that mimics those used in
enterprise-grade production storage systems. If multiple drives
fail within a server, to minimize data availability risk, any
stripes that are missing two blocks are given priority for
reconstruction. This approach is called critical reconstruction.
After those critically affected stripes are reconstructed, the rest
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
350
s
t
s
r
u
B
e
r
u
l
i
a
F
#
 25
 20
 15
 10
 5
 0
22
16
8
2
100
123
82
Disks per Server
164
0
41
Fig. 16: The number of 3 disk failures occurring within an 8
hour window using traces generated by CoFaCTOR.
ﬁguration, two years of drive failure data, and CoFaCTOR
to generate 10000 5-year long failure traces. We note in
individual experiments where the system design has been
altered to explore alternative storage system designs (e.g.
changes in disk capacity, disk enclosure size, or total number
of drives). We also use CoFaCTOR to generate 10000 traces
where a ﬁxed percentage of disks fail instantaneously or over
a ﬁxed period of time (using Poisson arrivals) to simulate
catastrophic failure events such as power outages.
C. System Lifetime Failure Analysis
In order to study the correlated failures, we investigate the
number of failure bursts (e.g., multiple failures in a failure
domain within 8 hours) with varied numbers of disks per
server. As shown in Figure 16, among 10000 failure traces,
larger disk enclosure sizes are more likely to encounter failure
bursts and possibly data loss.
Disks per Server
Scheme
Spares
6TB
14TB
20TB
123
164
DP
SODP
DP