title:Contractual Anonymity
author:Edward J. Schwartz and
David Brumley and
Jonathan M. McCune
A Contractual Anonymity System
Edward J. Schwartz David Brumley
Jonathan M. McCune
Carnegie Mellon University
{edmcman,dbrumley,jonmccune}@cmu.edu
Abstract
We propose, develop, and implement techniques for
achieving contractual anonymity. In contractual anon-
ymity, a user and service provider enter into an anonym-
ity contract. The user is guaranteed anonymity and mes-
sage unlinkability from the contractual anonymity sys-
tem unless she breaks the contract. The service provider
is guaranteed that it can identify users who break the
contract. The signiﬁcant advantages of our system are
that 1) the service provider is not able to take any action
toward a particular user (such as revealing her identity
or blacklisting her future authentications) unless she vi-
olates her contract, 2) our system can enforce a variety
of policies, and 3) our system is efﬁcient.
1
Introduction
Internet services such as chat rooms for victims of vi-
olence, abuse information and support message boards,
and whistle-blowing services are more compelling to
users if they provide anonymity. Despite users’ desire
for anonymity, service providers must grapple with the
need to identify misbehaving users in order to protect
their service. For example, a provider may need to iden-
tify and stop undesired behavior such as using the an-
onymity service to launch denial of service attacks or
threaten other users.
Thus, an anonymity service must strike a balance be-
tween accountability and anonymity. Users of such ser-
vices want as much anonymity as possible, and ideally,
should not have to trust the service provider. The service
provider, however, must retain some ability to identify
misbehaving users to protect the value of the service.
Previous anonymity protocols [10, 31, 32] resolved
this tension decidedly in favor of the service provider
by allowing the service provider to subjectively judge
whether a user misbehaved. Subjective judging does not
provide adequate anonymity guarantees to the user in
many scenarios because the provider can arbitrarily de-
cide to blacklist (deny future use of the service to) the
user for any reason and thus can discriminate, e.g., treat
each user differently based on her past actions.
In this paper, we propose contractual anonymity,
which offers a wider range of options for resolving the
accountability vs. anonymity tension. In contractual an-
onymity, the user and service provider (SP) enter into
a binding anonymity contract. The user is guaranteed
to remain anonymous and not blacklisted as long as she
follows the contract policy, while the service provider
is guaranteed to be able to identify (and blacklist, if de-
sired) users that break their contract.
In a contractual anonymity scheme,
the contract
policy can be an arbitrary boolean function f
:
{msg1, . . . , msgn} → {ALLOWED, VIOLATION}. If
the function returns VIOLATION on a message (or mes-
sages), the message is deemed malicious and the user is
de-anonymized. Example policy scenarios include:
Matching-based A policy may state that any message
matching a pre-deﬁned pattern is considered mali-
cious. Such policies could de-anonymize messages
that match an intrusion detection rule or malware
signature, disallow messages containing a prede-
ﬁned set of profane words, and so on.
Consensus-based A policy may require that a thresh-
old of users sign a petition to de-anonymize a user.
In particular, if n unique users anonymously ﬂag a
message, it is considered malicious and the sender
is de-anonymized. Note that in our protocol the
message will not cause de-anonymization if n − 1
users sign the message, or if the same user signs a
message n times.
Subjective-based A policy may state that any message
selected by a special privileged entity is consid-
ered malicious. For example, any user that sends
a message which is later designated (e.g., signed)
by an appropriate law-enforcement agency’s key
would be considered malicious. These policies can
also enable subjective judging (if it is desired) in
a manner similar to related systems [10, 31, 32],
e.g., by allowing a SP to de-anonymize messages
of its choice. However, in contractual anonymity,
the user must explicitly agree to a contract that al-
lows subjective judging.
Previous Systems Some of these policies can be en-
forced using previous systems. For instance, consensus-
based policies can conceivably be enforced using thresh-
old cryptography [17], and subjective-based policies
have been considered by past works [10, 31, 32].
Subjective-based systems can enforce a variety of policy
functions. However, in such a system, the SP can decide
to change the policy function at any time; the user is
not guaranteed access to the service if she behaves. In
contractual anonymity, the user is guaranteed anonymity
and access to the service if she does not break the con-
tract, and neither the user nor SP can change the contract
without the approval of the other party.
Previous subjective judging protocols also require all
messages to be rate-limited [10, 31, 32]. These restric-
tions prevent the protocols from being used in a variety
of settings, including those where users do not trust the
service provider to make fair subjective judgements, and
for services that send messages at a high rate.
A Protocol for Contractual Anonymity We develop
and implement a contractual anonymity protocol called
RECAP. We show through our implementation of
RECAP that it is feasible to build a secure contractual
anonymity implementation with a small trusted comput-
ing base while simultaneously achieving better perfor-
mance than prior approaches (Section 7).
In RECAP, each user is given an anonymous cre-
dential that allows the user to send messages anony-
mously. RECAP implements anonymous credentials
using group signatures (Section 2.1). At a high level, a
group signature scheme allows any member of the group
to sign on behalf of the group.
Individual signatures
from unrevoked members (i.e., users who have not bro-
ken the contract) are indistinguishable from any other
unrevoked member’s signatures. Group signatures al-
low the SP to efﬁciently authenticate messages without
needing to know each sender’s identity (and still reject
messages from revoked users).
However, group signature schemes are not sufﬁ-
cient to achieve contractual anonymity. Group signa-
tures require a group manager who is capable of de-
anonymizing users at will. To achieve contractual an-
onymity, this entity must be constrained to only de-
anonymize users that violate their contract. We address
this in RECAP by leveraging trusted computing [30] to
implement a veriﬁable third party, called the account-
ability server (AS), that acts as group manager and
knows the mapping between users’ real identities and
anonymous credentials. Speciﬁcally, the AS is a soft-
ware module that will only reveal a user’s real identity if
the SP provides message(s) that prove the user has vio-
lated the contract.
Note that the AS is not arbitrarily trusted by either
the user or SP. We construct the AS with a small trusted
computing base (TCB) that does not include the operat-
ing system or BIOS, and allow the user and SP to verify
the exact code the AS runs (Section 6). We term this ver-
iﬁable trust since all parties can verify that the trusted
party is running correctly and in the pre-agreed manner.
Contributions We introduce the concept of contrac-
tual anonymity, in which users are guaranteed anonym-
ity as long as they do not violate the policy of their pre-
negotiated contract with the SP. The SP is guaranteed
that it can learn a user’s real identity and identify that
user’s past and future messages if the user breaches the
contract. We design the RECAP protocol, which is the
ﬁrst protocol that provides contractual anonymity. We
also implement RECAP with a very small trusted com-
puting base. Through our implementation, we show that
RECAP is more efﬁcient and offers a wider spectrum
of solutions to the accountability vs. anonymity tension
than competing approaches [9, 10, 31, 32].
Organization The remainder of the paper is organized
as follows. In Section 2, we present relevant background
on group signatures and trusted computing. We discuss
how our system operates at a high level in Section 3, and
then in more detail in Section 4. We describe several
advantages of and extensions to our system in Section 5.
Our implementation and evaluation results are described
in Sections 6 and 7, respectively. The discussion is in
Section 8. We explore related work in Section 9. Finally,
we conclude in Section 10.
2 Primitives
2.1 Anonymity and Group Signatures
RECAP uses group signatures [2, 5–8, 13–15] to im-
In a group signature
plement anonymous credentials.
scheme, each group member has a unique private sign-
ing key that allows them to sign messages on behalf
of the entire group. There is a single group public
key which can be used to verify any member’s sig-
nature. Group signature schemes provide anonymity
among members of the group, since a veriﬁer cannot dis-
tinguish which group member signed a particular mes-
sage. The group manager is provided with a special trap-
door that can undo the signature anonymity. In RECAP,
the AS, acting as a veriﬁable third party, acts as the
group manager.
A group signature scheme suitable for RECAP must
support veriﬁer-local revocation [8]. Veriﬁer-local re-
vocation allows the signature veriﬁer to determine if a
message was signed by a revoked user without commu-
nicating with the group manager. The group manager
can revoke a user by publicly disclosing a special token
which veriﬁers add to their local blacklist. In RECAP,
veriﬁer-local revocation allows the SP to efﬁciently de-
tect and disregard messages from blacklisted users.
Such a scheme has four procedures: GS KEYGEN,
GS SIGN, GS VERIFY, and GS OPEN. We describe
these algorithms below at a high level. We refer the
reader to previous work [8] that provides the the full
speciﬁcation including security proofs.
GS KEYGEN(n) The GS KEYGEN algorithm takes
in the number of group members n. The al-
gorithm outputs a group public key KGPK , the
group manager secret key K −1
GMSK , an n-element
vector K −1
GSK [1 . . . n] of user secret keys, and
an n-element vector of user revocation tokens
RT [1 . . . n].
GS SIGN(KGPK , K −1
GSK [i], M ) GS SIGN takes
a
message M ∈ {0, 1}∗, group member i’s private
key K −1
GSK [i], and the group public key KGPK ,
and returns a group signature σ.
GS VERIFY(KGPK , M, σ, BL) GS VERIFY takes as
input the group public key KGPK , a message M ,
an alleged signature σ, and a blacklist BL that con-
sists of zero or more revocation tokens, and returns
one of {VALID, INVALID}. An output of INVALID
means that either the signature σ is invalid, or that
the signer is on the blacklist BL. In the latter case,
the signer’s identity is also returned.
GS OPEN(K −1
GMSK , M, σ) GS OPEN takes as input
the group manager secret key K −1
GMSK , a message
M and a corresponding signature σ. If (M, σ) is a
valid message-signature pair, GS OPEN outputs a
revocation token RT [s] for the signer s. The group
manager can distribute revocation tokens to a veri-
ﬁer, allowing them to detect messages signed by s
using the GS VERIFY algorithm.
The properties of modern group signature schemes
are often based on a framework introduced by Bellare
et al [5]. The group signature scheme we use in our
implementation, the Boneh-Shacham group signature
scheme [8], bases its formal deﬁnitions in this frame-
work. A group signature scheme suitable for RECAP
must have the following properties (as described in the
original Boneh-Shacham work [8]):
Correctness For any KGPK , K −1
GSK [1 . . . n], and
RT [1 . . . n] returned by the GS KEYGEN algo-
rithm, any signature produced by the GS SIGN
algorithm must return VALID when veriﬁed us-
ing the GS VERIFY algorithm, unless the user
has been revoked. Speciﬁcally, ∀i ∈ {1 . . . n},
GS VERIFY(KGPK , M, GS SIGN(KGPK ,
K −1
GSK [i], M ), BL) = VALID ⇔ RT [i] /∈ BL.
Traceability Traceability is deﬁned in terms of a game
that takes place between a challenger C and an ad-
versary A. The traceability property holds if no
adversary A can win the traceability game with
more than negligible probability. In the traceabil-
ity game, A wins if it can forge a signature that
cannot be traced to any user in a coalition of users
that A controls. The traceability game consists of
three stages:
Setup C runs the GS KEYGEN algorithm, and
provides KGPK and RT [1 . . . n] to A. U, the
set of users in A’s coalition, is initially set to
∅. S, the set of message-signature tuples A
obtained from oracles, is also set to ∅.
Queries A is allowed to query the GS SIGN
and GS CORRUPT oracles. The GS SIGN
oracle takes as input a message M , a
user i ∈ {1 . . . n}, and outputs σ ←
GS SIGN(KGPK , K −1
GSK [i], M ). C sets S ←
S ∪ {(M, σ)} for each message-signature
pair returned by the GS SIGN oracle. The
GS CORRUPT oracle allows A to corrupt a
user into joining the coalition. GS CORRUPT
takes a user i ∈ {1 . . . n} as input, and out-
puts K −1
GSK [i], the user’s private key. The
challenger sets U ← U ∪ {i} for each user
i corrupted by the GS CORRUPT oracle.
Response A outputs a message M ′, a set of revo-
cation tokens that forms a blacklist BL′, and
a signature σ′.
An adversary A wins the game if all of the follow-
ing conditions hold:
• GS VERIFY(KGPK , M ′, σ′, BL′) = VALID
• GS OPEN(K −1
• (M ′, σ′) /∈ S
GMSK , M ′, σ′) = i /∈ U
Selﬂess-anonymity Selﬂess-anonymity, like traceabil-
ity, is deﬁned in terms of a game between an ad-
versary A and a challenger C.
In the selﬂess-
anonymity game, A tries to determine which of
two keys was used to generate a signature σ. The
selﬂess-anonymity property holds if no adversary
A can win the selﬂess-anonymity game with more
than negligible advantage over random guessing.
The game has ﬁve stages:
Setup C runs the GS KEYGEN algorithm and ob-
tains KGPK , K −1
GSK [1 . . . n], RT [1 . . . n]. C
gives KGPK to A. C sets U , the set of users
that A has compromised or revoked, to ∅.
Queries The adversary can query the GS SIGN,
GS CORRUPT and GS REVOKE oracles. C
runs the GS SIGN oracle by computing σ ←
GS SIGN(KGPK , K −1
GSK [i], M ) where user
i and message M are inputs, and returns σ
to A. A can obtain the private key of user
i ∈ {1 . . . n} using the GS CORRUPT ora-
cle. C runs this oracle by returning K −1
GSK [i]
and setting U ← U ∪ {i}. The GS REVOKE
oracle allows the adversary to obtain the re-
vocation token for user i ∈ {1 . . . n}. C sim-
ulates this oracle by returning RT [i] and set-
ting U ← U ∪ {i}.
Challenge A chooses a message M and user in-
dices i0 and i1 where i0 /∈ U and i1 /∈ U. C
chooses a random bit b R← {0, 1} and returns
σ′ ← GS SIGN(KGPK , K −1
GSK [ib], M ) to A.
Restricted Queries A is allowed to make queries
as in the Queries stage. However, in this
stage the GS CORRUPT and GS REVOKE or-
acles cannot be queried for users i0 and i1.
Output A outputs a bit b′. If b = b′, A wins.
The Boneh-Shacham [8] scheme is an efﬁcient group
signature scheme that provides these properties. Specif-
ically, signing a message takes about1 eight modular ex-
ponentiations and two computations of a bilinear map.
1This assumes that computing a group isomorphism takes roughly
the same amount of time as computing a modular exponentiation.
Veriﬁcation with an empty blacklist (BL) requires ap-
proximately six modular exponentiations and three com-
putations of a bilinear map. There are two ways of
adding local revocation to the veriﬁcation algorithm.
The ﬁrst, which provides the above properties of cor-
rectness,
traceability, and selﬂess-anonymity, can be
achieved using a O(|BL|) algorithm, which performs
two additional bilinear map computations for each entry
in the veriﬁer’s blacklist.
However, the second type of revocation can be done
in O(1) time by using a precomputed revocation table
at the expense of allowing a small number of messages
In the O(|BL|) scheme, each signature
to be linked.
contains a random identiﬁer r that ranges over a large
group. The identiﬁer r is used when performing revoca-
tion checks. The O(1) scheme constrains this identiﬁer
to range from 1 . . . k, which allows the revocation ta-
ble to be computed in advance. The downside is that a
veriﬁer can determine that two signatures with the same
value of r that are signed by the same user were in fact
signed by the same user – this is called partial unlink-
ability, Unfortunately, there is no formal deﬁnition of
partial unlinkability [8]. Informally, partial unlinkabil-
ity ensures that a veriﬁer can only link (e.g., determine
that signer(m1) = signer(m2)) one out of every k sig-
natures signed by the same user for the same site (e.g.,
a SP in our scheme), where k is a security parameter.
For instance, if k = 100, 1% of the signatures are link-
able. We believe that in many cases the beneﬁts of con-
stant time revocations outweigh the downside of having
a small number of linkable messages, and so we consider
the O(1) scheme in our implementation. We discuss the
repercussions of this further in Section 8.4.
2.2 Trusted Computing and Contract Enforce(cid:173)
ment
RECAP uses a veriﬁable third party (the AS) to se-
curely bind user identities to contract policies and con-