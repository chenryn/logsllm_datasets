that perform best on average across all inner folds. Then, using the
“winning” classiﬁcation strategy, we ﬁnally train a new classiﬁer
using all traces from the training/validation portion. This ﬁnal
classiﬁer is what is tested on the test set.
As mentioned, we repeat the training and testing for 10 ran-
domized 70-30 outer folds, and we will report the averages over
these folds in a moment. In addition to reporting true-positive and
false-positive percentages, we will report area under the precision-
recall curve (PR-AUC) (c.f., [10]). A higher PR-AUC indicates a
higher true-positive rate and lower false-positive rate. Speciﬁcally,
a classiﬁer with PR-AUC equal to one is perfect: it gives only true
positives with no false positives. We calculate PR-AUC using the
scikit-learn tool [9].
Looking ahead, we will sometimes also report on the average
result of testing classiﬁers not chosen by the training/validation
regime in order to understand the beneﬁt of using, e.g., some
particular feature.
6.1 Results
We will ﬁrst analyze the performance of ML-based attacks using
the synthetic datasets, and then present the false-positive rates
seen on the campus traces. To summarize, the best classiﬁer
achieves a high average PR-AUC (0.987), a high average true-
positive rate (0.986), and a low average false-positive rate (0.003),
across all ﬁve obfuscators as measured on the same synthetic
data set it is trained on. The classiﬁers all perform signiﬁcantly
worse when tested on a synthetic dataset for which they were not
trained. Finally, the highest false-positive rate of any classiﬁer as
measured on the campus datasets (none of which were available
during training) is 0.65%.
Classiﬁer parameters. Using TorA, we found that the best-
performing classiﬁers were essentially always CART decision trees
using the packet-count windowing strategy. The best classiﬁers
used between 280 and 300 packets, which we consider too large
for practicality. However, classiﬁers using up to 30 packets already
perform within 0.3% PR-AUC of the best performing, and so we
from now on restrict our attention to them.
Feature performance. We next discuss how the different types of
features effect classiﬁer performance. Recall that we used entropy-
based, packet-timing, and packet-header features. In Figure 7 we
compare classiﬁer true positive and false positive rates when testing
speciﬁc combinations of features for the TorA dataset. We can
see that using only entropy-based and/or packet-header features
can already achieve high true positives and low false positives,
with pretty low variance across folds (indicated by the error bars).
Timing-based features showed a higher false-positive rate, and we
conclude that a combination of entropy-based and packet-heading
features performs best. Our training procedure indeed always
selected the combination of the entropy-based and packet-header
features.
Unexpectedly, entropy-based features work well for detecting
meek. Examining the number of packets with non-zero-byte
payload in the ﬁrst 30 packets of the meek traces and the SSL/TLS
traces in the Tor datasets, we ﬁnd 70% of SSL/TLS ﬂows have
more than 18 packets with non-zero-byte payload, whereas at least
96% of the meek traces have less than 18 packets with non-zero-
byte payload. So the number of samples for calculating entropy
statistics in a meek trace and a SSL/TLS trace are often different,
which biases the minimum, average, and maximum entropy scores
65E
T
H
EH
1
0.98
0.96
0.94
0.92
R
P
T
0.02
0.015
R
P
F
0.01
0.005
0
obfs3 obfs4
E
fte meekA meekG
T
EH
H
obfs3 obfs4
fte meekA meekG
Figure 7: A comparison of true-positive (top graph) and false-
positive (bottom graph) rates by features used.
“E” indicates
entropy-based feature, “T” the timing-based features, “H” the
packet header feature set, and “EH” indicates a combination of
entropy-based and packet-header features. Note that for clarity the
graphs have truncated y-axes.
used as features. This bias is caught by our classiﬁer to differentiate
between meek and SSL/TLS.
Portability. We now turn to testing the “portability” of this
ML approach. We used the two additional data sets TorB and
TorC, which are collected in environments distinct from that of
TorA. Whereas TorA is collected on an Ubuntu VM connected
to a campus network, TorB is an Ubuntu VM connected to a
home wireless network and TorC is a Windows VM connected
to a home wireless network. We build three distinct classiﬁers
using each of the three datasets using our procedure as above, but
now augmenting the testing phase to also test against a stratiﬁed
random sample of 70% of each of the other two datasets. The
resulting matrix of average true and false positive rates (across all
target obfuscators) is given in Table 8. The diagonal corresponds
to training and testing on the same environment, whereas scores
off the diagonal correspond to training and testing on different
environments.
As can be seen, the ML classiﬁers do very well when trained and
tested in the same environment. However, using the classiﬁers to
attempt to classify a network ﬂows generated by a distinct operating
system and/or network signiﬁcantly hinders performance. When
using the same operating system, but different networks (the
TorA/TorB and TorB/TorA entries) one sees less drastic reduction
in performance. Changing operating systems however has large
impact, with true positive rates being as low as 52% and false-
positive rates reaching 12%. This provides some evidence that
censors will indeed need to train classiﬁers on example traces
representing all of the target types for which they need to be
deployed.
False positives in campus traces. We ﬁnally turn to analyzing
false-positive rates of a classiﬁer trained with the methodology and
data above when faced with real, not synthetic, network traces.
We ﬁrst combine the TorA, TorB and TorC datasets and use the
combination to train and validate a classiﬁer. We perform stratiﬁed
randomized splits of 30% for training and 70% for validation,
train a classiﬁer for each obfuscator, and compute the PR-AUC on
TRAIN\TEST TorA
TorA
TorB
TorC
TorB
TorC
0.99 (0.002) 0.88 (0.01) 0.52 (0.02)
0.93 (0.009) 0.99 (0.002) 0.78 (0.03)
0.57 (0.12) 0.64 (0.12) 0.99 (0.002)
Table 8: The effect of training and testing in the same or different
environments. Reported is the average true-positive rate (average
false-positive rate in the parentheses) across classiﬁers for all
obfuscators using the dataset labeling the row for training and the
dataset labeling the column for testing.
PT\Dataset
obfs3
obfs4
FTE
meekA
meekG
Total
OfﬁceDataset
5,281 (0.43%)
730 (0.06%)
6,437 (0.53%)
2,065 (0.17%)
837 (0.07%)
0.98%
CloudDataset
14,714 (0.20%)
16,257 (0.22%)
23,432 (0.31%)
787 (0.01%)
3 (0%)
0.70%
WiﬁDataset
34,726 (0.65%)
24,221 (0.46%)
19,857 (0.37%)
1,024 (0.02%)
2 (0%)
1.40%
Table 9: False positives of classiﬁers on the campus network
datasets. The value in the parentheses is the false-positive rate of
the selected classiﬁer on a given campus network dataset. Recall
that the number of ﬂows tested for OfﬁceDataset, CloudDataset,
and WiﬁDataset are 1.22 M, 7.48 M, and 5.32 M respectively.
the validation set. We repeat this procedure 20 times with fresh
random splits, and select the classiﬁers with the highest PR-AUC.
The PR-AUC of all the classiﬁers are greater than 0.90 (ranging
from 0.91 - 0.99). The true-positive rates (false-positive rates) of
the best/selected classiﬁers are 97.2% (0.13%), 97.0% (0.19%),
97.3% (0.08%), 98.3% (0.08%), and 97.8% (0.06%) for obf-
sproxy3, obfsproxy4, meek-amazon, meek-google, and FTE re-
spectively.
Next, for each TCP ﬂow in the campus datasets, we extract the
features using the ﬁrst 30 packets, label it as negative, and then test
it with each obfuscator classiﬁer. A summary of the results are in
Table 9. The total false-positive rate of the classiﬁers across all
three datasets is 0.98%. As broken down by dataset, we see as low
as 0.70% (CloudDataset) and as high as 1.40% (WiﬁDataset).6
The classiﬁer for obfsproxy3 produces the most false positives,
followed by the classiﬁer for FTE. The classiﬁer for meek-google
produces a relatively small number of false positives, which is a
total of 842 false positives out of 14 M ﬂows. That the wireless
network exhibits the largest number of false positives may be due
to their noisier nature [33,34]. For instance, when there are multiple
TCP retransmissions in a ﬂow, most of the packets we examined in
the ﬁrst 30 packets could be identical, hindering classiﬁcation.
The false positives are associate with 12,551 distinct hosts, a
small fraction of all monitored hosts, in the monitored networks.
Meanwhile, they are only associated with 6,239 distinct destination
IPs outside the campus networks. Less than 30% of these hosts
or destination IPs are associated with more than 90% of the false
positives. We ﬁnd that a single IP outside the campus networks
can contribute to as high as 4.6% of the false positives (as high as
1.2% for a single source IP inside the networks). This suggests that
speciﬁc server-side or client-side settings could be the reasons for
false positives.
We also determined the protocols of the false-positive ﬂows.
As shown in Table 10, most of the false positives are HTTP,
SSL/TLS or unknown ﬂows, according to Bro. The false positives
6Note that the total rates are not equal to the sum of the individual
obfuscator false-positive rates, as some traces are falsely labeled by
multiple classiﬁers.
66Protocol\PT
obfs3
obfs4
FTE
meekA
meekG
HTTP
12,295 (0.12%) 10,383 (0.10%) 43,673 (0.42%) 1,414 (0.01%)
SSL/TLS 29,705 (1.91%) 22,768 (1.47%) 3,773 (0.24%) 1,684 (0.11%) 391 (0.03%)
114 (0.55%)
2,739 (0.12%) 6,345 (0.27%) 2,148 (0.09%) 609 (0.03%) 451 (0.02%)
13 (0.06%)
33 (1.94%)
73 (0.35%)
0
0
SSH
SMTP
Unknown
0
0
0
0
0
Total
44,853
39,509
49,700
3,707
842
Table 10: Breakdown of the numbers of ﬂows from our campus traces incorrectly labeled by our ML classiﬁers as the indicated obfuscator.
The values in the parentheses are the percentage of ﬂows labeled by Bro as the indicated protocol that represent false positives (e.g., 0.12%
of HTTP ﬂows are mislabeled as obfsproxy3 by our classiﬁer). “Unknown” means Bro fails to identify the protocol of the ﬂow. The total
number of false positives across all protocols is shown in the ﬁnal row.
of the meek-amazon classiﬁer have a diversity in their protocols.
We examine some of the mislabeled ﬂows, and realize they may
use a patched version of protocols such as SSH with encrypted
handshakes [23].
We use nDPI to examine the ﬂows labeled as unknown by Bro.
nDPI fails to identify 13.5% of the unknown ﬂows, and reports 45
protocols found in the remainder. Of these, 21 are built atop HTTP
or SSL/TLS. These protocols account for 78.2% of the unknown
ﬂows. Whether an ML approach can be enhanced to reduce false
positives even further remains an open question.
6.2 Discussion
Our results show that trained classiﬁers using trafﬁc-analysis-
type techniques are effective at detecting obfuscators. This holds
true even for the meek family of obfuscators, which are widely
considered the hardest to detect by DPI. The true-positive rates are
high, and the false-positive rates are relatively small. We discuss
the impact of the latter in more detail in the next section.
Though the training process of the ML-based approaches are
complex, the decision trees emitted by training are, themselves,
actually quite simple: evaluating the trees requires between 6 and
13 integer comparisons. These comparisons use per-ﬂow state
including just a small number of integer counters, for up to the
ﬁrst 30 packets. We therefore believe the trees themselves will be
relatively easy to deploy, while the trickier issue will be the lack
of portability we observed. This speciﬁcally implies that building
good decision trees requires careful training in, ideally, the local
network environment to which they will be deployed.
7. ESTIMATING THE IMPACT OF FALSE
POSITIVES
As we have emphasized, a critical component of successful
obfuscator detection is achieving low false-positive rates and our
work is the ﬁrst that assesses them. Here we discuss the impact
of false positives and whether the rates achieved by our detection
approaches are sufﬁcient for a censor’s purpose.
Let us ﬁx a particular representative scenario from our data, other
scenarios can be analyzed similarly. Say a nation-state censor
deploys the combined ML-based classiﬁers for each of the ﬁve
obfuscators in the ofﬁce environment, and assume a true-positive
rate of 99% (as measured with TorA) and the false-positive rate of
about 1% (as measured using the OfﬁceDataset dataset). For the
foreseeable future, the base rate of obfuscator trafﬁc will continue
to be a tiny fraction of the overall trafﬁc. Suppose, pessimistically
for our analyses, that one out of every one billion ﬂows is actually
obfuscated trafﬁc. Because of the low base rate, only about 1 in
10,000,000 ﬂows marked as obfuscated will, in fact, be obfuscated.
(Not 1 in 100 of marked ﬂows, as one falling victim to the base
rate fallacy might assume.) This is likely to result degradation of
Internet performance, as perceived by “legitimate” users.
Aggressive censors may be willing to cause such degradation.
Of the blocked connections in this scenario, about 34.9% will be