the transaction does not consist of only read queries; trans-
actions that just read are not recorded in the write log and
do not need undo. Finally, once a transaction’s dependency
hash table is written to disk, the in-memory version is dis-
carded.
4.2.4
Impact of Isolation Level
In this subsection we discuss the interplay between isola-
tion levels used in DBMS (Serializable, Repeatable Read,
Read Committed, Read Uncommitted) and Phoenix. Al-
though, PostgreSQL currently supports only Read Com-
mitted and Serializable, the Phoenix architecture is appli-
cable to other isolation levels as well. The isolation level
used in transaction processing affects a transaction’s view
of the database and thus determines what dependencies will
arise. Isolation level is also responsible for phenomenon
like dirty reads, cascaded aborts, etc., which may affect the
operational details of Phoenix.
In MVCC based PostgreSQL and other DBMSs that
use Pessimistic-Deferred concurrency control, Serializable
level ensures that only committed data is read. Con-
sequently, phantoms are not possible and repeated reads
within the same transaction always return the same result.
This implies that all reads of a data item will create a depen-
dency to the same “committed transaction.” As phantoms
are disallowed, every possible dependency that is present
will be captured. At the Repeatable Read isolation level,
it is possible for phantoms to arise. Intuitively, a phantom
is a new row in the database that matches the predicate of
two queries in a transaction but is visible only to the sec-
ond one. However, phantoms are not an issue for Phoenix
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
as its goal is to generate a dependency if and only if one ex-
ists. As long as one of the queries reads the phantom row,
a dependency should be and is created in Phoenix.
At the Read Committed isolation level in PostgreSQL
and other databases, it is possible for two reads of the same
data item to produce a dependency against two different
transactions, as repeatable reads are not guaranteed. Once
again, this does not pose a problem as Phoenix still pro-
duces dependencies only when they exist as per our deﬁ-
nition of a dependency. Lastly, at the Read Uncommitted
isolation level, Phoenix must function in the presence of
dirty reads and the cascaded aborts. While at other iso-
lation levels handling aborts only required restoring Prev
information, at Read Uncommitted we also need to update
the dependency graph. As explained earlier, we can solve
this by setting a ﬂag in the log entry corresponding to the
aborted transaction (Section 4.2.3).
In summary, the inter-transaction dependency tracking
algorithm of Phoenix works correctly regardless of the iso-
lation level, because it captures the dependencies as they
arise. That is, as soon as a DBMS decides to run at a par-
ticular isolation level, Phoenix can correctly capture all the
inter-transaction dependencies that are allowed in that iso-
lation level.
4.3 Repair-Time Logic
After an intrusion is detected, Phoenix’s repair-time logic
is invoked, which consists of two phases: identiﬁcation of
the undo set and erasing the effects of the transactions in
the undo set on the database. Phoenix ﬁrst re-builds the
inter-transaction dependency graph from the persistent de-
pendency log ﬁle. Given the transactions that are identi-
ﬁed as initiated by the attacker, Phoenix traverses the inter-
transaction dependency graph to ﬁnd all subsequent trans-
actions that directly or indirectly depend on the attacker
transaction(s). These transactions form the initial undo set.
Recognizing that the transaction dependency deﬁnition
currently supported may not be the most appropriate one,
Phoenix provides a tool for database administrators to re-
ﬁne the initial undo set into the ﬁnal undo set. The basic
operation supported by the tool is called Depends(T) which
returns the set of transactions depending on T. Depends(T)
is simply the result of the reachability algorithm with T
as the start node. A DBA can delete certain transactions
from the undo set, combine two undo sets, add elements
to an undo set, etc. Therefore the DBA has considerable
ﬂexibility in ﬁnalizing the eventual undo set. The depen-
dency graph query tool allows a DBA to mix and match
his intuition and knowledge about transaction semantics
along with the automated derived undo set. This inter-
active reﬁnement facility is essential because it provides
database administrators a sense of control and ﬂexibility in
the database damage control process.
Assume the set of transactions that take place between
the time when an intrusion occurs and the time when it is
detected is Ttotal and the ﬁnal undo set is Tundo. Then
logically what Phoenix needs to do to repair the intrusion
damage is to rollback all transactions in Tundo, and to roll
forward those transactions that are in Ttotal - Tundo. The
roll-forward step is necessary because it is what distin-
guishes Phoenix from other backup-based damage repair
approaches: the ability to selectively undo only corrupting
transactions and thus preserve as much useful work as pos-
sible.
Owing to the visibility rule logic in PostgreSQL, abort-
ing a transaction is extremely simple: Just change the sta-
tus of the transaction in question to “aborted” and all the
tuples that this transaction creates immediately become in-
visible. With this powerful primitive, all Phoenix needs to
do to repair the damage caused by an intrusion is to con-
vert the status of all the transactions in Tundo to “aborted”
in the transaction status log. There is no need to explic-
itly roll forward transactions in Ttotal - Tundo, because the
effects of these transactions will become visible automati-
cally through the visibility rule logic.
Recall that the protection window of Phoenix is the
longest interval between an intrusion and its detection that
Phoenix can tolerate and still provide perfect repair. Sup-
pose the protection window is set to one month. This
means that PostgreSQL needs to maintain table row ver-
sions and transactions status log entries that are younger
than a month. PostgreSQL provides a vacuum command
that can be run periodically to reclaim storage allocated to
tuples that are outdated by other committed transactions.
To ensure that tuples and transaction status log entries are
properly preserved for recovery, it is required that vacuum
should not be run during the protection window.
For other DBMSs that support standard write-ahead log-
ging, both the rollback and roll-forward steps are needed.
In rollback, one scans the write-ahead log backwards one
record at a time. If the current record belongs to a trans-
action in Tundo and is not a start transaction record, it is
undone. If the record belongs to a transaction in Tundo and
is a start transaction record, the transaction is deleted from
Tundo. This process continues until Tundo becomes empty
and there are no more transactions to be undone. Next, in
roll-forward, one scans ahead through the write-ahead log
until the end of the log. If a record belongs to Tundo it is
skipped. Otherwise, it is applied to the database.
4.4 Summary
The current Phoenix prototype implementation consists of
a library that can be dynamically linked to a PostgreSQL
installation, and a modiﬁcation to PostgreSQL’s internals
to invoke functions in this new library. We deﬁne three
hooks inside PostgreSQL that once activated will be called
upon during a transaction’s execution, speciﬁcally each
time when a row is read (Section 4.2.2), when a transaction
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
commits and when a transaction aborts. The commit/abort
of a transaction corresponds to the point in PostgreSQL’s
code where a commit/abort record is inserted into the write
log. We intercept the transaction execution at these points.
Our library registers with these hooks upon initialization
by passing function pointers (or call-back function), which
are called at the associated interception points to imple-
ment dependency tracking logic, logging inter-transaction
dependencies, and invalidating dependency graph nodes.
Through a utility, database administrators can deactivate
these hooks and unlink the library at run time.
The code for dependency graph maintenance and depen-
dency log creation is implemented as a separate module
and relies on hooks within PostgreSQL. To allow multiple
PostgreSQL server processes access the dependency graph,
we also modify the shared memory data structures for stor-
ing the dependency graph. Excluding the graph mainte-
nance code, we add fewer than 100 lines of code to Post-
greSQL for inter-transaction dependency tracking and se-
lective undo.
5 Performance Evaluation
In this section we present the result of a performance eval-
uation study of a fully operational Phoenix prototype. The
testbed machine for all the following experiments is a Dell
Dimension Machine with a 1.8Ghz Intel Pentium 4 CPU,
a 512K L2 cache, 1GB RAM and two 60GB hard-drives
and running Red-Hat Linux version 7.1. Our experiments
are based on the TPC-C benchmark with a warehouse fac-
tor of 8, unless speciﬁed otherwise. The TPC-C benchmark
is designed to reﬂect real world OLTP processing activity
and models a wholesaler business. The benchmark spec-
iﬁes ﬁve different types of transactions that are executed
against a populated database. Each transaction type has a
speciﬁc proﬁle in terms of the number of reads/writes per-
formed and the frequency of execution. We implement the
TPC-C transactions in C using the libpq library provided
by PostgreSQL.
Although the current Phoenixprototype successfully in-
corporates intrusion resilience, it is important to demon-
strate that Phoenix does this without introducing undue per-
formance overhead at run time. Logically, Phoenix adds
additional code to keep track of the last update transac-
tion for each table row, to build up inter-transaction de-
pendency upon read access to a table row, and to put the
inter-transaction dependency graph to disk at transaction
commit time. To demonstrate that the run-time overhead
of Phoenix is quite reasonable, we performed two experi-
ments to evaluate its impact on transaction response time
and throughput.
In the ﬁrst experiment we measure the average trans-
action response times of Phoenix and PostgreSQL. The
database cache was warmed up initially by executing all
TPC-C transactions 10 times. The execution time of each
Throughput
Phoenix
PostgreSQL
Overhead
W=4 W=8 W=16 W=32
911
681
993
740
8.25% 8.7% 7.31% 8.0%
735
793
776
850
Table 2: Overall transaction throughput comparison between the
MVCC version of Phoenix and PostgreSQL under the TPC-C
benchmark in terms of number of transactions executed per sec-
ond. W is the number of warehouses.
transaction is recorded and averaged over 100 runs. The
current Phoenix prototype implements two ways to keep
track of inter-transaction dependencies:
the MVCC ap-
proach and the Trigger approach (Section 4.2.1). The av-
erage per-transaction execution latencies of the ﬁve types
of transactions in the TPC-C benchmark with and with-
out inter-transaction dependency tracking are shown in Ta-
ble 1. The Trigger and MVCC columns correspond to the
trigger-based and the MVCC-based approach toward de-
pendency tracking, respectively, whereas the PostgreSQL
column corresponds to the case of no dependency tracking.
The Phoenix Overhead column shows the percentage over-
head difference between the MVCC version of Phoenix and
PostgreSQL.
The main additional performance overhead of the
MVCC approach with respect to generic PostgreSQL is
due to the construction and storage of inter-transaction de-
pendency graph upon read accesses, because PostgreSQL
already records the ID of the last update transaction for
each table row. Therefore, the performance difference be-
tween PostgreSQL and the MVCC approach should in-
crease with the number of read accesses in the transac-
tions. This explains why the execution latency differ-
ence between the MVCC approach and PostgreSQL is the
largest for Stock Level transactions, which are read-only
transactions, then Order Status, medium read-only transac-
tions, and the smallest for Payment, light read/write trans-
actions. Although New Order transactions also involve
heavy read accesses, the relative performance difference
between PostgreSQL and the MVCC approach is smaller
compared to that of Stock Level transactions, because the
writes in read/write transactions dilutes the cost of depen-
dency tracking by signiﬁcantly increasing the overall trans-
action latency without adding any extra dependency track-
ing overhead. This is also why a four-fold increase in la-
tency from Delivery to New Order, only results in a corre-
sponding increase in overhead of 0.32%.
The latency measurements in Table 1 demonstrate that
the penalty in transaction response time that Phoenix in-
troduced for the TPC-C benchmark is less than 5%. Ta-
ble 1 also shows that the performance difference between
the Trigger approach and the MVCC approach could be
up to a factor of ﬁve, e.g., Delivery transactions. This is
mainly due to the additional cost associated with access to
the CreateTran table at run time.
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
Transaction
Type
Payment
Delivery
New Order
Order Status
Stock Level
Access
Characteristics
Light Read/Write
Medium Read/Write
Heavy Read/Write
Medium Read Only
Heavy Read Only
Trigger
based (ms)
179.1
4321
30123
273
589.4
PostgreSQL MVCC
(ms)
128.6
1787
5840.9
165.4
217.8
(ms)
129.7
1820.9
5970
172.2
228.4
Phoenix
Overhead
0.83%
1.89%
2.21%
4.11%
4.89%
Table 1: Average execution latencies in milliseconds of different types of transactions in the TPC-C benchmark when executed
with/without inter-transaction dependency tracking added by Phoenix. The Trigger and MVCC columns correspond to the trigger-
based and the version-based approach, respectively, to dependency tracking, whereas the PostgreSQL column corresponds to the case of
no dependency tracking. The Phoenix Overhead column shows the percentage overhead difference between MVCC and PostgreSQL.
In the next experiment we investigate how Phoenix af-
fects the transaction throughput under the TPC-C bench-
mark. We simulated 400 simultaneous users that execute a
transaction mix consisting of 45% New Order transactions,
43% Payment transactions and 4% Order Status, Delivery
and Stock Level transactions. Each user submits this trans-
action mix to the database server over a period of three
hours. Table 2 shows the comparison in the transaction
throughput between Phoenix-MVCC and PostgreSQL, in
terms of number of transactions processed in one minute,
which is the standard throughput metric for TPC-C bench-
marks. The TPC-C speciﬁcation models a warehousing
business and allows scaling the database size by increas-
ing the number of warehouses (W). These scaling rules
are explicitly deﬁned and addition of one row to the ware-
house table results in a addition of 475,000 rows across the
database.
Although the transaction throughput decreases as the
database size is increased from W=2 to W=32, the perfor-
mance overhead of Phoenix compared to PostgreSQL re-
mains almost the same. Overall, Phoenix incurs a through-
put penalty of around 8% for all database size when com-
pared to PostgreSQL. This is because scaling has no effect
on the number of dependencies that are being generated
over the test run. Each transaction reads the same num-
ber of rows for a database with one warehouse as for a
database with ﬁve warehouses. Thus the overhead associ-
ated with updating the dependency graph remains almost
constant. Consequently, the resulting dependency graph
has the same number of edges for all warehouse factors
but its density may vary. The marginal drop in through-
put performance can be attributed to the need to access a
common disk based log ﬁle. The shared memory-resident
dependency graph does not affect the overall transaction
throughput, as no locking is required while accessing this
graph and each transaction accesses it’s own unique hash
table. Further, allocating extra buffers in the shared mem-
ory region based on an estimate of the graph size ensures
that the graph is always memory resident.
To ascertain that in the MVCC approach a table row read
does impose a very small performance overhead, we mea-
sure the elapsed time for executing a sequence of queries
each reading a different number of table rows. While ser-
vicing such a query, the read-row hook is traversed for
each selected row and the transaction dependency graph is
updated during hook traversal. Thus, the query run time
should have a strong correlation with the number of rows
selected. Figure 2 shows the query execution time, with and