(Secure Sockets Layer) warnings were re-written in simple language
by removing technical terms for browser users to understand them
[18]. Similarly, privacy notices [54], warning habituation, and secu-
rity interventions [3, 52] were studied to help users make correct
security decisions.
There are also some studies about educational approaches or
tool development to protect users from phishing attacks [5, 6, 30].
Other significant work suggested the users should select stronger
passwords and store them safely [20, 38, 57, 58]. Some researchers
emphasised the importance of the improvement of security tools
and interfaces [1, 35].
Existing works mainly revealed the fact that users’ risk per-
ception and security behaviours will be affected if users do not
follow security advice because they do not understand the texts.
Compared to the previous works, our research focuses on investi-
gating users’ difficulty in understanding security texts. Moreover,
we make efforts in explaining the terms that users cannot under-
stand instead of removing them or rewriting the sentences. Our
work aims at helping users make the right security decisions based
on their knowledge.
3 EXPERIMENT 1: USERS’ UNDERSTANDING
OF SECURITY TEXTS
We conducted a study with 597 participants to learn users’ compre-
hension of security risks and to answer the three research questions
presented in the introduction. Both this study and evaluation study
(Section 4.2) were approved by the CSIRO Social and Interdisci-
plinary Human Research Ethics Committee1.
3.1 Setup and Methodology
3.1.1 Data Sources. We used the blogs about cybersecurity as our
source. They were selected based on the rankings on recommenda-
tion websites and popularity in social media. We also included the
technical blogs from a previous study [32] such as TrendMicro2.
Those blogs publish news, articles and technical reports analysis
about the latest trends in cybersecurity for different types of audi-
ence and not only for security experts. For example, The Hacker
News3 attempts to educate users with varying technology back-
grounds to stay safe online, and it has attracted over 2 million
Facebook followers.
We implemented a crawler in Python using its Beautiful Soup
[48] library. The crawler first scraped all page links from the home-
page and extracted HTML pages from these links. The crawler then
scraped the links of technical articles, which were stored in tags,
and attributes of the extracted HTML pages. Regular expressions
were applied to avoid unrelated content, e.g., advertisements, blog
1Ethics Clearance 172/19
2https://blog.trendmicro.com/
3https://thehackernews.com/
contributors’ biographies, and outdated articles. For example, we
applied “((?!:).)*/201[5-8]/)(\d2/)((?!:).)*(.html)$” to
extract the blogs from 2015 to 2018. The selected articles were then
extracted and downloaded as HTML files. Our collection was con-
ducted in December 2018, and only the latest articles (starting from
2015) were collected. In total, we collected 42,409 cybersecurity
articles from 35 technical blogs (see supplementary table4). In the
next step, a web-based questionnaire was designed for users to
annotate technical terms and their difficulty levels.
3.1.2 Data Pre-processing. A user study on over 40,000 articles
is challenging and time-consuming. Therefore, through data pre-
processing, we chose some representative articles with which to
conduct the study. Generally, the reading speed of an adult is around
275 words per minute [37]. We removed the articles which required
less than one minute to read (19.2% of the articles). We further
removed the long ones that may take more than five minutes to
read.
We used the topic modelling technique [9] to select the repre-
sentative articles. Topic modelling is a statistical method for discov-
ering the abstract “topics” that appear in a collection of documents.
We used LDA (Latent Dirichlet Allocation) [9], a state-of-the-art
method for topic modelling. LDA is an unsupervised learning al-
gorithm which discovers a mixture of different topics for each
document with distinguished probabilities. We implemented LDA
in Python using gensim [46]. Coherence measures were employed
to evaluate the performance of the generated topic models as they
have better human interpretability than other measures such as
perplexity [49]. We applied the module CoherenceModel in gen-
sim to obtain LDA models as well as their topic coherence. To
determine the number of topics, we compared the coherence of
generated models with different values (i.e., 5, 10, 15, ..., 50), and
kept other parameters as default. In our experiments, we noted that
the highest coherence score was at 10. Therefore, we set LDA to
discover ten topics. We then manually interpreted the discovered
topics. The topics are government/company reports, device/system
access, vulnerability/flaw, file/code, user account security, network
attacks, data breaches, security threat/cyber risks in business, ma-
licious software (e.g., malware, ransomware), and non-technical
news. These topics covered all articles. We randomly picked 20
articles from each topic. In total, 200 representative articles were
chosen for the study.
An existing study [42] identified ten topics of security advice, as
shown in Fig.2. We find these ten topics are contained in our LDA-
generated topics at different granularity. The numbers of the articles
in our dataset for all the identified topics are depicted in Fig.2. For
almost all the topics, we find at least 10% of articles in our dataset
are related. It indicates we have sufficient coverage. Besides, our
dataset contains advanced security articles for professionals, such
as complex operations to manually remove malware and detailed
attacking methods with file operations or commands.
Study Methodology and Procedure. We designed a question-
3.1.3
naire to measure users’ understanding of the articles. We were
mainly after the terms they found difficult to understand. The ques-
tionnaire contained three parts: questions about demographics, an
4https://ktd4869.github.io/Reading_Test_MT/supplementary_table.pdf
Figure 2: The number of the articles in the identified ten top-
ics of security advice [42]: Phishing and Spam (PhaS), Data
Breaches (DtBr), Viruses and Malware (VraM), Hackers and
Being Hacked (HaBH), Passwords and Encryption (PsaE),
National Cybersecurity (NtnC), Credit Card and Identity
Theft (CCaIT), Privacy and Online Safety (PaOS), Criminal
Hacking (CrmH), and Mobile Privacy and Security (MPaS).
Figure 3: A screenshot of our technical term annotation tool.
annotation task, and questions about the articles. Instructions and
a tutorial were provided at the beginning of the questionnaire. We
also provided some example terms along with their difficulty levels
which were determined by some readability calculation methods.
We employed Amazon Mechanical Turk (MTurk) to conduct our
study. MTurk is a marketplace where individuals can outsource
tasks with monetary compensation. We published the questionnaire
with 2 U.S. dollars (rewards) for each completion. The workers
were required to be 18 years or older and proficient in English
reading/writing to participate in the user study. Only the workers
with a 95% approval rating (suggested in [40]) were eligible to
participate in our survey.
In the demographics section, participants were asked about their
gender, age, education, IT background, whether they were English
native speaker and four questions about their experience on security
threats. They were allowed to choose ‘prefer not to answer’.
Workers were then required to annotate the articles through
our designed interface, as shown in Fig.3. Each time participants
clicked and selected a term (or a phrase up to five words), they
were asked to choose its difficulty level in a pop-up window. The
difficulty scale was from 1 to 10. Once chosen, the term would be
highlighted with a yellow tone whose brightness showed the level
of difficulty. The annotation function was implemented based on
[34].
Figure 4: Users’ experience about security threats (top: fre-
quency (times/week, month, or year); bottom: time of last
experience).
After annotating the articles, workers were asked some questions
about the article. More specifically, they were asked to select two
of the terms they had highlighted and to explain their choice of
difficulty. They were also asked to answer two open questions and
describe the desired functionalities from a tool that they thought
could help them understand the terms.
In total, we collected 597 valid responses after a manual review
on the submitted assignments (mean: 25.67 min; std: 11.82 min).
We rejected unsatisfactory assignments such as random or blank
answers and careless term annotation (mostly annotated words not
related to computer science or with less than three annotations).
Each participant was allocated two articles, selected randomly from
our 200-article pool, and each of the 200 articles was annotated by
at least three participants.
3.1.4 Data Analysis. We analysed the responses to explore users’
comprehension of security texts to answer our research questions.
We used open card sorting [56] to group the answers to the open
questions. We extracted the keywords from each answer and grouped
the answers by matching the keywords. The details are described
in the next sub-section.
3.2 Survey Results
3.2.1 Demographics. Our participants are mainly younger adults
(70% with ages 18 to 35), and 82% of all the participants are English
native speakers. We have an almost equal number of each gen-
der. 81% of younger adults have bachelor or higher degrees. This
percentage only slightly decreases to 71% for older adults. 50% of
the younger adults have IT background, compared to 12% of older
adults (ages >50). These statistics show that older adults have less
IT knowledge and might be at higher risk against security threats,
even with the same education level.
Most of our participants (96%) are daily internet users, but only
8% had never experienced security threats. The frequency and latest
experience of security threats are depicted in Fig.4. Most users
experienced malware or virus less than once a week, but 40% had
an experience at least once a month.
3.2.2 Annotated Term Analysis. We collected the technical terms
annotated by MTurk users. Overall, 7,375 terms were collected. We
then manually removed invalid terms (e.g., meaningless). Mean-
ingless terms referred to the terms which did not have specific
meanings in the IT domain. For example, ‘public’ is generic and
thus removed, but the ‘public key’ phrase is commonly used in cryp-
tography and was kept. We removed the duplicates of each term
and took the average value to replace its difficulty level. Duplicate
terms occurred after lemmatisation (e.g., ‘APIs’ was the plural form
Figure 5: The overall distribution of mean and std in the num-
ber of terms identified by (A) different labellers across 200
documents and (B) different documents across 597 labellers.
of ‘API’), punctuation removal, and the removal of the words that
did not have a semantic contribution (e.g., ‘DNS-based’ and ‘inject
or’ were duplicates of ‘DNS’ and ‘inject’ respectively). In total, we
obtained a 6,286-term security-centric corpus (SC Corpus) which
contained 3,276 phrases (e.g., ‘web browser’, ‘ad hoc attacks’, ‘XSS
flaw’) and 3,010 words (e.g., ‘2FA’, ‘WannaCry’, ‘malware’).
A. Data Validity
Before doing this task, we first conducted a data validity anal-
ysis to ensure workers’ term annotations were satisfactory. We
calculated the mean and std in the number of terms identified by
different labellers within a given document. The distribution of
the analysis for all the 200 documents is shown in Fig.5(A). Most
of the stds range from 6 to 18, with an average of 12. We further
analysed the number of terms identified in different documents by a
labeller. Fig.5(B) presents the distribution for all 597 labellers. It also
indicates relatively low stds (mostly  0.01). Therefore, we conclude
that our sampled dataset is representative of the full dataset.
B. Technical Term Analysis and Results
We then present the results of the first experiment to answer
our four research questions.
00.10.20.30.40.50.60.70.80.91Proportion of SampleLatest timeFrequency1/m. & 5/w.Earlier than last y.Last y.Last m.Last w.NeverRQ1. What are the technical terms used in the security texts and
their difficulty levels from a user’ perspective?
To explore them, we first identified categories for the terms and
evaluated their validity with an expert review. We then analysed
the difficulty levels of the terms in each category and the potential
impact factor (in which year a term was coined).
Term Categories. We identified the categories of the security
terms in the corpus according to their lexical semantics, using
the open card sorting [56] again. More specifically, we randomly
selected 100 terms from the corpus and identified their categories,
and then applied the categories or created new categories for the
remaining terms.
We recruited three researchers with the cybersecurity back-
ground to complete the manual classification. We used majority
voting [39] to identify the categories for each term. If all the re-
searchers had different opinions on a term, then a discussion was
conducted until two reached an agreement. Each term was allowed
to have up to two categories. The classification result shows that
328 out of 6,286 terms are assigned to two categories, while the rest
only have one category. In total, 40 subcategories are generated to
classify the corpus. We further consolidated the 40 subcategories
into 15 categories. Table 2 in Appendix B lists all the categories
and two examples for each subcategory as well as the detailed
descriptions.
Expert Review. To ensure the validity of the categories, we
conducted an expert interview to evaluate the accuracy of the clas-
sifications. We recruited two cybersecurity experts who worked in
CSIRO’s Data61 for more than two years. Our researchers intro-
duced the categories in details at the beginning. We generated two
samples separately, randomly selected as 5% of the corpus, both
containing 252 terms. Each reviewer was provided with a sample
and asked to highlight the terms not matching the categories. We
applied a think-aloud protocol as used in [31]. During the eval-
uation process, the participants were free to talk about the task,
and our researchers were sitting next to them, taking notes and
dispelling the doubts. Our results show that both experts thought
that only 1 out of 252 terms was labelled incorrectly, which meant