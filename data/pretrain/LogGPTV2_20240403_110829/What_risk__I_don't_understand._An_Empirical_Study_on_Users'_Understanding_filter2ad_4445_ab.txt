### Secure Sockets Layer (SSL) Warnings and User Comprehension

SSL warnings have been rewritten in simpler language, removing technical jargon to make them more understandable for browser users [18]. Similarly, studies have focused on simplifying privacy notices [54], addressing warning habituation, and enhancing security interventions [3, 52] to help users make informed security decisions. Additionally, there are educational approaches and tools designed to protect users from phishing attacks [5, 6, 30]. Other significant work has emphasized the importance of selecting stronger passwords and storing them securely [20, 38, 57, 58]. Some researchers have also highlighted the need for improving security tools and interfaces [1, 35].

Existing research has shown that users' risk perception and security behaviors are influenced by their ability to understand security advice. Our study focuses on investigating the challenges users face in understanding security texts. Instead of removing or rewriting complex terms, we aim to explain these terms to enhance users' comprehension and enable them to make better security decisions based on their knowledge.

### Experiment 1: Users' Understanding of Security Texts

We conducted a study with 597 participants to assess their comprehension of security risks and to address the three research questions outlined in the introduction. Both this study and the subsequent evaluation (Section 4.2) were approved by the CSIRO Social and Interdisciplinary Human Research Ethics Committee [Ethics Clearance 172/19].

#### 3.1 Setup and Methodology

**3.1.1 Data Sources**

We used cybersecurity blogs as our primary data source, selected based on their rankings on recommendation websites and popularity on social media. We included technical blogs from a previous study [32], such as TrendMicro [2] and The Hacker News [3]. These blogs provide news, articles, and technical reports on the latest trends in cybersecurity, catering to a broad audience, not just security experts. For example, The Hacker News aims to educate users with varying technology backgrounds and has over 2 million Facebook followers.

We implemented a Python crawler using the Beautiful Soup library [48] to scrape the blogs. The crawler first extracted all page links from the homepage and then scraped the links of technical articles, stored in HTML tags and attributes. Regular expressions were used to filter out unrelated content like advertisements and outdated articles. For instance, we used the regular expression "((?!:).)*/201[5-8]/)(\d2/)((?!:).)*(.html)$" to extract blogs from 2015 to 2018. The selected articles were downloaded as HTML files. The collection was conducted in December 2018, focusing on the most recent articles (from 2015 onwards). In total, we collected 42,409 cybersecurity articles from 35 technical blogs (see supplementary table [4]).

**3.1.2 Data Pre-processing**

Given the large number of articles, we pre-processed the data to select representative samples. We removed articles that could be read in less than one minute (19.2% of the articles) and those that would take more than five minutes to read. We used topic modeling, specifically Latent Dirichlet Allocation (LDA) [9], to identify representative articles. LDA is an unsupervised learning algorithm that discovers a mixture of topics for each document. We implemented LDA in Python using the gensim library [46] and evaluated the coherence of the generated models using the CoherenceModel module. We determined the optimal number of topics by comparing the coherence scores, which indicated that ten topics provided the highest coherence. The identified topics included government/company reports, device/system access, vulnerabilities, file/code, user account security, network attacks, data breaches, business cyber risks, malicious software, and non-technical news. We randomly selected 20 articles from each topic, resulting in 200 representative articles for the study.

**3.1.3 Study Methodology and Procedure**

We designed a questionnaire to measure users' understanding of the articles, focusing on the terms they found difficult. The questionnaire had three parts: demographic questions, an annotation task, and questions about the articles. Instructions and a tutorial were provided at the beginning. We used Amazon Mechanical Turk (MTurk) to conduct the study, offering $2 per completed questionnaire. Participants had to be 18 years or older, proficient in English, and have a 95% approval rating. The demographics section asked about gender, age, education, IT background, and experience with security threats. Participants were allowed to choose 'prefer not to answer.'

For the annotation task, participants used our designed interface to highlight and rate the difficulty of terms on a scale of 1 to 10. The annotated terms were highlighted with a yellow tone, with brightness indicating the difficulty level. After annotating the articles, participants answered questions about the terms they highlighted, explaining their choices and suggesting desired functionalities for a tool to help understand the terms.

We collected 597 valid responses after reviewing the submitted assignments (mean: 25.67 min; std: 11.82 min). Unacceptable responses, such as random or blank answers, were rejected. Each participant annotated two randomly selected articles, and each article was annotated by at least three participants.

**3.1.4 Data Analysis**

We analyzed the responses to explore users' comprehension of security texts. Open card sorting [56] was used to group the answers to open-ended questions. Keywords were extracted from each answer, and the answers were grouped by matching the keywords.

### 3.2 Survey Results

**3.2.1 Demographics**

Our participants were mainly younger adults (70% aged 18 to 35), with 82% being native English speakers. The sample had an almost equal gender distribution. Most younger adults (81%) had bachelor's degrees or higher, while 71% of older adults (aged >50) had similar educational levels. However, only 50% of younger adults and 12% of older adults had IT backgrounds, suggesting that older adults may be at higher risk of security threats despite having the same education level.

Most participants (96%) were daily internet users, but only 8% had never experienced security threats. The frequency and recency of security threats are shown in Figure 4. Most users experienced malware or viruses less than once a week, but 40% had at least one experience per month.

**3.2.2 Annotated Term Analysis**

We collected 7,375 technical terms annotated by MTurk users. Invalid terms (e.g., meaningless) were removed, and duplicates were consolidated. This resulted in a 6,286-term security-centric corpus (SC Corpus) containing 3,276 phrases and 3,010 words.

**A. Data Validity**

To ensure the validity of the term annotations, we analyzed the mean and standard deviation (std) of the number of terms identified by different labelers within a given document. The results, shown in Figure 5(A), indicate that most stds ranged from 6 to 18, with an average of 12. We also analyzed the number of terms identified in different documents by a labeler, as shown in Figure 5(B). The low stds (mostly < 0.01) suggest that our sampled dataset is representative of the full dataset.

**B. Technical Term Analysis and Results**

To answer our research questions, we categorized the terms and evaluated their validity with expert reviews. We identified categories based on lexical semantics using open card sorting [56]. We randomly selected 100 terms and identified their categories, then applied or created new categories for the remaining terms. Three cybersecurity researchers classified the terms using majority voting [39]. If there was no agreement, a discussion was held until two researchers agreed. Each term could have up to two categories. The classification resulted in 40 subcategories, which were consolidated into 15 main categories (see Table 2 in Appendix B).

**Expert Review**

To validate the categories, we conducted an expert interview with two cybersecurity experts from CSIROâ€™s Data61. The experts reviewed two samples, each containing 5% of the corpus (252 terms). They highlighted any terms that did not match the categories. The results showed that both experts agreed that only 1 out of 252 terms was incorrectly labeled, indicating high accuracy in our categorization.