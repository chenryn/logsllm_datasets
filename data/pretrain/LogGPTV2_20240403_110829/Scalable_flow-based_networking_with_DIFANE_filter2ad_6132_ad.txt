dates of the switch connection rules in the data plane.
The authority switch also runs a cache manager which in-
stalls cache rules in the ingress switch. If a packet misses
the cache, and matches the authority rule in the author-
ity switch, the cache manager is triggered to send a cache
update to the ingress switch of the packet. The cache man-
ager is implemented in software in the control plane because
packets are not buﬀered and waiting for the cache rule in
the ingress switch. The ingress switch continues to forward
packets to the authority switch if the cache rules are not
installed. The cache manager sends an update for every
packet that matches the authority rule. This is because we
can infer that the ingress switch does not have any related
358           NOX Controller
   Packets
    Install 
     Rules
   Packets
Switch 1
           Local Controller
Switch p
           Local Controller
... ...
Data Plane
Data Plane
   Packets
   Packets
(a) NOX
Authority Switch
           Cache Manager
   Install
   Rules
Data Plane
   Packets
Switch 1
Data Plane
Switch p
... ...
Data Plane
   Packets
   Packets
(b) DIFANE.
Figure 7: Experiment setup. (Dashed lines are control
messages; straight lines are data traﬃc.)
n
o
i
t
c
a
r
f
e
v
i
t
a
u
m
u
C
l
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0.1
DIFANE
NOX
 1
 10
 100
RTT (ms)
Figure 8: Delay comparison of DIFANE and NOX.
is buﬀered in the ingress switch before NOX controller in-
stalls a rule at the ingress switch. In contrast, in DIFANE
the authority switch redirects the packet to the receiver in
the data plane and installs a rule in the ingress switch at
the same time. We generate ﬂows with diﬀerent port num-
bers, and use a separate rule for each ﬂow. Since the dif-
ference between NOX and DIFANE lies in the processing of
the ﬁrst packet, we generate each ﬂow as a single 64 Byte
UDP packet. Based on the measurements, we also calculate
the performance diﬀerence between NOX and DIFANE for
ﬂows of normal sizes. Switches, NOX, and traﬃc genera-
tors (“clients”) run on separate 3.0 GHz 64-bit Intel Xeon
machines to avoid interference between them.
(1) DIFANE achieves small delay for the ﬁrst packet
of a ﬂow by always keeping packets in the fast path.
In Figure 8, we send traﬃc at 100 single-packet ﬂows/s and
measure the round-trip time (RTT) of the each packet be-
ing sent through a switch to the receiver and an ACK packet
being sent back. Although we put NOX near the switch, the
packets still experience a RTT of 10 ms on average, which
is not acceptable for those networks that have tight latency
No. of switches
1
2 3 4 56 810
)
c
e
s
/
s
w
o
l
f
(
t
u
p
h
g
u
o
r
h
T
1M
500K
200K
100K
50K
DIFANE
NOX
20K
10K
10K 20K 50K 100K200K 500K 1M
Sending rate (flows/sec)
Figure 9: Throughput comparison of DIFANE and
NOX.
requirement such as data centers.
In DIFANE, since the
packets stay in the fast path (forwarded through an author-
ity switch), the packets only experience 0.4 ms RTT on aver-
age. Since all the following packets take 0.3 ms RTT for both
DIFANE and NOX, we can easily calculate that to transfer
a ﬂow of a normal size 35 packets, which is based on the
measurement in the paper [20]), the average packet trans-
fer time is 0.3 ms (=(0.3*34+0.4)/35) transferring time for
DIFANE but 0.58 ms (=(0.3*34+10)/35) for NOX. Others’
tests of NOX with commercial OpenFlow switches observed
similar delay [21].
(2) DIFANE achieves signiﬁcantly higher throughput
than NOX. We then increase the number of switches (p)
to measure the throughput of NOX and DIFANE. In Fig-
ure 9, we show the maximum throughput of ﬂow setup for
one client using one switch.
In DIFANE, the switch was
able to achieve the client’s maximum ﬂow setup rate, 75K
ﬂows/s, while the NOX architecture was only able to achieve
20K ﬂows/s. This is because, while all packets remain in
the fast path (the software kernel) in DIFANE, the Open-
Flow switch’s local controller (implemented in user-space)
becomes a bottleneck before NOX does. Today’s commer-
cial OpenFlow switches can only send 60-330 ﬂows/s to the
controller due to the limited CPU resources in the con-
troller [22]. Section 8 discusses how to run DIFANE on
today’s commercial switches.
As we increase the number of ingress switches—each addi-
tional data-point represents an additional switch and client
as shown in the upper x-axis—we see that the NOX con-
troller soon becomes a bottleneck: With four switches, a
single NOX controller achieves a peak throughput of 50K
single-packet ﬂows/s. Suppose a network has 1K switches,
a single NOX controller can only support 50 new ﬂows per
second for each switch simultaneously. In comparison, the
peak throughput of DIFANE with one authority switch is
800K single-packet ﬂows/s.
Admittedly, this large diﬀerence exists because DIFANE
handles packets in the kernel while NOX operates in user
space. However, while it is possible to move NOX to the
kernel, it would not be feasible to implement the entire NOX
in today’s switch hardware because the online rule genera-
tion too complex for hardware implementation. In contract,
DIFANE is meant for precisely that — designed to install a
set of rules in the data plane.11
11Today’s TCAM with pipelined processing only takes 3–
359 100
)
%
t
(
e
a
r
s
s
o
l
t
e
k
c
a
P
 80
 60
 40
 20
 0
Switch
fails
Detect
failure
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5
Time (sec)
Figure 10: Authority switch failure.
Network # switches/routers # Rules
Campus
VPN
IPTV
IP
∼1700
∼1500
∼3000
∼2000
30K
59K
5M
8M
Table 1: Network characteristics.
We then evaluate the performance of the cache manager
in authority switches. When there are 10 authority rules,
the cache manager can handle 30K packets/s and generate
one rule for each packet. When there are 9K authority rules,
the cache manager can handle 12K packets/s. The CPU is
the bottleneck of the cache manager.
(3) DIFANE scales with the number of authority switches.
Our experiments show that DIFANE’s throughput increases
linearly with the number of authority switches. With four
authority switches, the throughput of DIFANE reaches over
3M ﬂows/s. Administrators can determine the number of
authority switches according to the size and the throughput
requirements of their networks.
(4) DIFANE recovers quickly from authority switch
failure. Figure 10 shows the eﬀect of an authority switch
failure. We construct a diamond topology with two author-
ity switches, both connecting to the ingress and the egress
switches. We set the OSPF hello interval to 1 s, and the
dead interval to 3 s. After the authority switch fails, OSPF
notiﬁes the ingress switch. It takes less than 10 ms for the
ingress switch to change to another authority switch after
the dead interval, at which time the ingress switch sets the
backup partition rule as the primary one, and thus connec-
tivity is restored.
7.2 Evaluation of Partitioning and Caching
We now evaluate DIFANE’s partitioning algorithm using
the topologies and access-control rules from several sizable
networks (as of Sept. 10, 2009) including a large-scale cam-
pus network [24] and three large backbone networks that are
operated by a tier-1 ISP for its enterprise VPN, IPTV, and
traditional IP services. The basic characteristics of these
networks are shown in Table 1.
In these networks, each
access control rule has six ﬁelds:
ingress interface, source
IP/port, destination IP/port, and protocol. Access control
rules are conﬁgured on the ingress switches/routers.
It is
highly likely that diﬀerent sets of rules are conﬁgured at dif-
ferent switches and routers, hence one packet may be permit-
ted in one ingress switch but denied at another. In addition,
4 ns per lookup [23], which is more than three orders of
magnitude faster than in software.
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
u
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
IPTV
VPN
Campus
IP
 1
 10
 100
1K
10K
No. of non-overlapping / No. of overlapping rules
Figure 11:
overlapping rules.
Comparison of overlapping and non-
IP
IPTV
VPN
Campus
 1000
s
e
h
c
t
i
w
s
.
t
h
u
a
f
o
.
o
N
 100
 10
 1
1K
10K
100K
1000K 5000K
No. of TCAM entries per auth. switch
Figure 12: Evaluation on number of authority switches.
as Table 1 shows, there are a large number of access control
rules conﬁgured in these networks. This is due to the large
number of ingress routers that need to have access control
rules conﬁgured and the potentially large number rules that
need to be conﬁgured on even a single router. For example,
ISPs often conﬁgure a set of rules on each ingress switch
to protect their infrastructures and important servers from