### System Info
  * `transformers` version: 4.9.1
  * Platform: Linux-4.15.0-210-generic-x86_64-with-debian-buster-sid
  * Python version: 3.7.10
  * PyTorch version (GPU?): 1.9.0 (True)
  * Tensorflow version (GPU?): not installed (NA)
  * Flax version (CPU?/GPU?/TPU?): not installed (NA)
  * Jax version: not installed
  * JaxLib version: not installed
  * Using GPU in script?: Yes
  * Using distributed or parallel set-up in script?: No
### Who can help?
@ArthurZucker @younesbelkada @Narsil @sgugger
### Information
  * The official example scripts
  * My own modified scripts
### Tasks
  * An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
  * My own task or dataset (give details below)
### Reproduction
  1. `from transformers import BertTokenizer, BertModel`
  2. `tokenizer = BertTokenizer.from_pretrained('bert-large-cased')`
As discussed here
Leads to the following `HTTPError`
    HTTPError                                 Traceback (most recent call last)
     in 
    ----> 1 tokenizer = BertTokenizer.from_pretrained('bert-large-cased')
    ~/miniconda3/envs/cmd-chall/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
       1646             # At this point pretrained_model_name_or_path is either a directory or a model identifier name
       1647             fast_tokenizer_file = get_fast_tokenizer_file(
    -> 1648                 pretrained_model_name_or_path, revision=revision, use_auth_token=use_auth_token
       1649             )
       1650             additional_files_names = {
    ~/miniconda3/envs/cmd-chall/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in get_fast_tokenizer_file(path_or_repo, revision, use_auth_token)
       3406     """
       3407     # Inspect all files from the repo/folder.
    -> 3408     all_files = get_list_of_files(path_or_repo, revision=revision, use_auth_token=use_auth_token)
       3409     tokenizer_files_map = {}
       3410     for file_name in all_files:
    ~/miniconda3/envs/cmd-chall/lib/python3.7/site-packages/transformers/file_utils.py in get_list_of_files(path_or_repo, revision, use_auth_token)
       1685         token = None
       1686     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(
    -> 1687         path_or_repo, revision=revision, token=token
       1688     )
       1689     return [f.rfilename for f in model_info.siblings]
    ~/miniconda3/envs/cmd-chall/lib/python3.7/site-packages/huggingface_hub/hf_api.py in model_info(self, repo_id, revision, token)
        246         )
        247         r = requests.get(path, headers=headers)
    --> 248         r.raise_for_status()
        249         d = r.json()
        250         return ModelInfo(**d)
    ~/miniconda3/envs/cmd-chall/lib/python3.7/site-packages/requests/models.py in raise_for_status(self)
        951 
        952         if http_error_msg:
    --> 953             raise HTTPError(http_error_msg, response=self)
        954 
        955     def close(self):
    HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/bert-large-cased
### Expected behavior
Should run without `HTTPError`