SYSTEMS THINKING 
Conceiving of causality as a chain of directly related events is at 
least  200  years  old.      Traditional  safety  engineering  techniques, 
such  as  fault  tree  analysis,  based  on  this  model  were  developed 
over  50  years  ago,  before  computers  were  used  to  create  the 
highly-interactive,  tightly  coupled,  software  intensive  systems 
common today.  
The limitations of traditional engineering methods and the need to 
field  increasingly  complex  systems  during  and  immediately 
following  World  War  II  led  to  the  development  of  modern 
systems  theory  in  the  1940s  and  1950s  [3].  Systems  theory 
provides the philosophical and intellectual foundation for systems 
engineering and also for a new, more powerful model of accident 
causality  developed  by  Leveson  called  STAMP  (System-
Theoretic Accident Model and Processes) [4].  
STAMP  extends  traditional  causality  models  from  a  focus  on 
component 
from 
to  defining 
losses  as 
failures 
resulting 
interactions among humans, physical system components, and the 
environment.  Losses  result  when  safety  constraints  on  system 
component behavior and interactions are violated. Thus the focus 
shifts from “preventing failures” to “enforcing safety constraints 
on system behavior.” 
In  systems  theory,  the  system  is  conceived  as  a  hierarchical 
structure, where each level enforces constraints on the behavior of 
components  at  the  next  lower  level.  These  constraints  control 
emergent  system  behavior,  such  as  safety  and  security.  Control 
loops  operate  between  each  level  of  this  hierarchical  control 
structure. Figure 1 shows the general form of such control loops.  
Every controller contains a model of the process it is controlling. 
This  model  is  used  to  determine  what  control  actions  are 
necessary. Many accidents related to software or human operators 
are  not  the  result  of  software  or  human  “failure”  (whatever  that 
might  mean)  but  stem  from 
the 
controller’s model of the controlled process and the actual process 
state. For example, friendly fire accidents are usually the result of 
mistaking a friendly aircraft for an enemy. Unsafe control actions 
can result from providing a control action that leads to a hazard, 
not providing a control action that is needed to prevent a hazard, 
providing  a  control  action  too  early  or  too  late,  or  continuing  a 
control action too long or stopping it too soon. 
inconsistencies  between 
(cid:3)(cid:15)(cid:14)(cid:18)(cid:16)(cid:15)(cid:12)(cid:12)(cid:8)(cid:16)(cid:1)
(cid:3)(cid:15)(cid:14)(cid:18)(cid:16)(cid:15)(cid:12)(cid:1)
(cid:2)(cid:12)(cid:9)(cid:15)(cid:16)(cid:11)(cid:18)(cid:10)(cid:13)(cid:1)
(cid:5)(cid:16)(cid:15)(cid:6)(cid:8)(cid:17)(cid:17)(cid:1)(cid:1)
(cid:4)(cid:15)(cid:7)(cid:8)(cid:12)(cid:1)
(cid:3)(cid:15)(cid:14)(cid:18)(cid:16)(cid:15)(cid:12)(cid:1)
(cid:2)(cid:6)(cid:18)(cid:11)(cid:15)(cid:14)(cid:17)(cid:1)
Feedback 
(cid:3)(cid:15)(cid:14)(cid:18)(cid:16)(cid:15)(cid:12)(cid:12)(cid:8)(cid:7)(cid:1)(cid:5)(cid:16)(cid:15)(cid:6)(cid:8)(cid:17)(cid:17)(cid:1)
                    Figure 1. Basic Control Loop 
STPA  (System-Theoretic  Process  Analysis)  is  a  new  hazard 
analysis method based on STAMP. It is being used successfully in 
almost every industry and even non-engineering applications such 
as food safety and financial systems safety. We believe it also has 
potential for application to security. The rest of this paper shows 
how STPA might be extended into a new cyber security analysis 
technique called STAMP-Sec. 
4.  APPLYING STPA TO SECURITY 
In  the  broad  sense,  security  can  be  considered  as  protecting  a 
system  against  intentional  disruptions.    Adversary  activity  is  a 
common source of these disruptions, but it is not the only source.  
Trusted insiders can also take action to disrupt the operations of 
systems.  Safety can be considered as protecting that same system 
against unintentional disruptions. Hazards lead to safety incidents 
in the same way that vulnerabilities lead to security incidents. We 
believe  that  the  key  question  facing  today’s  security  analysts  is 
how to control vulnerabilities, not how to avoid threats. 
3
The example provided here, a nuclear reactor system, represents a 
type  of  critical  infrastructure  that  needs  to  be  protected  against 
cyber attack. Physical plants represent high payoff targets for any 
number of potential adversaries and clearly must be defended. In 
the following example, a real nuclear power plant design was used 
but  the  details  had  to  be  changed  for  obvious  reasons.  The  full 
analysis (for safety) can be found in Thomas [5].  
The  analysis  was  done  on  a  fully  digital  Pressurized  Water 
Reactor  (PWR).    Computers  direct  all  control  systems  including 
those protecting the nuclear reactor (called the “safety system” in 
nuclear engineering).  The plant produces electricity by using heat 
from  the  reactor  to  generate  steam  that  powers  a  turbine.  The 
turbine produces electricity that is transferred into the power grid 
for consumer and commercial users.   
The example STPA-Sec analysis focuses on the operation of the 
Main  Steam  Isolation  Valve  (MSIV)  located  on  the  steam  line 
from  the  steam  generator.    The  MSIV  is  open  during  normal 
operations to enable system cooling.  The MSIV can be closed to 
isolate the steam generator from the rest of the system in case of a 
problem  with  the  steam  containment  system,  such  as  a  leak  or 
break.    However,  closing  the  MSIV  also  prevents  the  secondary 
system  from  providing  adequate  cooling  to  the  primary  system.  
Lack of adequate cooling can lead to equipment damage or even a 
plant  meltdown.    Therefore,  it  is  critical  to  plant  operations  that 
the MSIV be open or closed as dictated by the situation.  Failure 
to do so can have disastrous consequences. Note that several real 
world  cyber  security 
the 
malfunctioning of valves [6].   
STPA-Sec shares the same four basic process steps with its safety 
counterpart,  STPA,  although  the  results  and  detailed  procedures 
may  be  different.  The  first  step  is  establishing  the  systems 
engineering foundation for the security analysis. Then the control 
actions that threaten system security are identified. These control 
actions  are  used  to  create  security  requirements  and  constraints. 
The fourth and final step is to identify causal scenarios that can 
give rise to violations of the security constraints.  
Step 1: Establishing the Systems Engineering Foundation 
Because  the  current  security  approach  is  largely  threat  based, 
security specialists may be tempted to conduct their assessments 
in  isolation.    This  approach  is  logical  from  a  tactical  security 
perspective,  but  likely  misses  the  larger  systems  perspective.  
Threats  exercise  physical  or  logical  infrastructure  vulnerabilities 
to  disrupt  or  otherwise  hinder  system  function.    In  turn,  the 
adverse 
targeted 
organization from delivering the services that represent its raison 
de  entre.  Starting  with  physical  threats  represents  a  bottom-up 
tactics approach in contrast with a system engineering top-down 
strategy. 
STPA-Sec  reverses  the  tactics-based  bottom-up  approach  by 
starting at the highest level of the system.  The critical first step is 
identifying  the  set  of  losses  that  are  considered  unacceptable.  
These losses likely extend beyond the physical and logical entities 
into  the  higher  level  services  provided  by  these  entities.    Rather 
than  beginning  with  tactics  questions  of  how  best  to  guard  the 
network  against  threats,  STPA-Sec’s  systems  thinking  approach 
begins  with  strategy  questions  of  what  essential  services  and 
functions must be secured against disruptions or what represents 
an  unacceptable  loss.  This  step  requires  clearly  identifying  the 
“what(s)”  and  then  using  that  information  to  reason  more 
impacts  on  system  function  prevent 
incidents  have  occurred  over 
the 
inform 
insights  should 
these  decisions, 
thoroughly  about  the  “how(s)”  that  can  lead  to  the  undesirable 
outcomes.    The  analysis  moves  from  general  to  specific,  from 
abstract to concrete.   
Two distinctions of this approach are immediately clear.  The first 
distinction  is  that  security  experts  are  unlikely  to  be  capable  of 
answering  the  “what”  questions  isolated  from  organizational 
leaders and operations personnel. Security involves tradeoffs and 
the  allocation  of  scarce  resources.    Although  security  concerns 
and 
the  ultimate 
responsibility  for  making  them  rests  with  the  senior  leaders 
charged with ensuring that the organization provides its essential 
business  or  functional  services.  Although  security  can  advise,  it 
will be the senior leaders that decide.   
During  the  security  analysis,  it  is  possible  or  even  likely  that 
potential  conflicts  may  arise  between  priorities.  For  example, 
there is a constant tension between the need to secure and the need 
to share access to information resources.  A bottom-up approach 
might  identify  the  security  challenge  associated  with  granting 
expanded  access  to  information  systems,  however,  the  approach 
lacks the larger context to provide insight into the corresponding 
necessity  to  share  in  order  to  accomplish  key  organizational 
outcomes.    If  a  decision  is  made  with  regard  to  one  of  the 
priorities without consideration for the other, a problem is likely 
to arise.  This problem may not be visible to the security team, but 
will be visible to the operations team that requires the access in 
order to perform the higher-level system functions.  
 A  prudent  way  to  properly  address  the  potential  conflict  is 
through a top-down process such as STPA-Sec.  Such an approach 
provides the necessary context for decision makers to evaluate the 
higher-level  needs  rather  than  focusing  on  tactical  level  details.  
Certainly,  the  tactical  details  are  important.    However,  over 
emphasis and premature emphasis on the details of task execution 
absent the larger context of the systemic purpose the tasks support 
can lead to substituting tactics for strategy.  
The 
that  STPA-Sec  begins  with 
organizational  purpose  and  goals,  not  physical  or  logical  assets. 
Successful security assessments require a careful establishment of 
priorities.    By  establishing  the  priorities  at  the  start  of  the 
assessment as opposed to the end, the priorities form a framework 
to both focus and guide the security assessment. This evaluation 
can only be properly made with the benefit of perspective into the 
larger, overall system function.  
One of the most important aspects of the environment is adversary 
activity.  Certainly adversary action is a critical consideration in 
addressing  security  and  preventing  intentional  losses.    Yet, 
focusing  on  adversaries  or  threats  too  early  in  the  process  and 
absent  the  benefit  of  context,  limits  the  overall  strategic-level 
utility  of  the  security  assessment.    Put  another  way,  the  goal  of 
security  is  not  to  guard  the  physical  network  and  prevent 
intrusions.    The  goal  is  to  ensure  that  the  critical  functions  and 
ultimately  the  services  the  network  and  systems  provide  are 
maintained in the face of disruptions.  
 Adversary action is only one such disruption (albeit an important 
one).    One  potential  benefit  of  applying  STPA-Sec  to  security 
would  be  to  expand  the  focus  of  security  efforts  more  toward 
those 
the 
organization’s leaders, rather than simply expecting cyber security 
experts  to  defend  from  a  position  of  disadvantage. 
  The 
disadvantage  occurs  because  security  analysts  and  defenders  are 
that  are  actually  within 
second  distinction 
is 
things 
the  control  of 
forced  to  react  to  threats  and  other  environmental  disruptions, 
rather  than  proactively  shaping  the  situation  by  identifying  and 
controlling system vulnerabilities.   
This  shift  also  represents  a  more  judicious  use  of  resources.  
Multiple  threats  and  disruptions  can  exploit  a  given  system 
vulnerability.    Even  under  current  tactics-based  models,  a  threat 
must ultimately exploit a vulnerability to produce the system loss.  
Rather than trying to initially identify all of the threats and then 
move up to the vulnerabilities they might exploit to produce the 
loss, a more reasonable approach might be to start with addressing 
system vulnerabilities which are likely far fewer than threats and, 