I(e)N e
[t,t+l] (13)
(14)
Interval-of-time reward variables calculated with our
method are equivalent to those calculated with typical dis-
crete event simulators.
For time-averaged interval-of-time variables, we redeﬁne
the variable as
W[t,t+l] =
Y[t,t+l]
l
.
(15)
The above calculation is similar to one given in Section II,
but uses the method for calculating interval-of-time variables
Proof: Given equation 14, 15 is equivalent
to the
deﬁnition presented in Section II. From Equation 14, we
know that Y[t,t+l] = θ[t,t+l]. Substituting θ[t,t+l] for Y[t,t+l]
in Equation 15 yields Equation 3.
In this section we have detailed a method to identify all
dependence relationships in a model, M, using an MDG,
GM . We then detailed how to identify rare faults in the
model. Using the set of identiﬁed faults, ER, and the MDG,
we showed how to enlarge the set ER to include mitigation,
repair, and propagation actions. We then showed how to use
ER with GM to form a set of decomposed submodels, Ξ,
which we then solved using Algorithm 2.
IX. DISCUSSION
We applied our methods to a one petabyte deduplicated
storage system to assess the impact of deduplication on reli-
ability. We modeled our system under three different RAID
conﬁgurations, 8+ p, 8+2p, and 8+3p; one, two, and three
parity disks respectively. The application of our framework
allowed us to automatically generate detailed models of
our storage system from models of individual components
in the system itself. Models of deduplication relationships
were then automatically generated from empirical data. We
compared the efﬁency of our hybrid, dependence-based,
discrete event simulator to an unmodiﬁed discrete event
simulator, and achieved a signiﬁcant speed-up as shown in
Table I.
In our previous work, [4], we conducted a similar study of
large-scale deduplicated storage systems, but showed only
a decrease in reliability due to characteristics of the data
stored in the studied system. We predicted that other systems
may have different results given other deduplicated different
deduplication relationships. In the system studied in this
paper, we conﬁrm the predictions made in [4], once again
afﬁrming the importance of a detailed understanding of the
underlying relationships in a deduplicated storage system.
As shown in Figure 9, while the Archive and Database
1 ﬁle categories showed a decrease in reliability due to
(a) Corrupt data served per year due to UDEs for the Database 1 category.
(b) Corrupt data served per year due to UDEs for the Database 2 category.
Figure 10: Rates of corrupt data served for four example categories, with no deduplication, 1 copy deduplication, and 1%,
10% and 50% 2 copy deduplication.
tection could be achieved (albeit with diminishing returns)
by keeping additional copies for larger portions of a given
category.
From these results, and the results we provided in [4],
it is clearly important to take into account the actual dedu-
plication characteristics of a given system when applying
multi-copy deduplications strategies. With our framework it
is possible to model and develop per-category multi-copy
deduplication schemes to achieve the desired level of relia-
bility, while maintaining a high level of storage efﬁciency.
System designers need only submit to our framework the
necessary component level models, RAID level, and the size
of the system, along with empirical data about the footprint
of the deduplicated data they wish to store. Each category in
the system can then be analyzed at varying levels of multi-
copy coverage. Users can enter a desired level of reliability
for their system, and using our framework determine a
level of multi-copy coverage which meets their goals, while
obtaining more storage efﬁciency can be preserved than the
na¨ıve approach of assigning the same multi-copy scheme to
the entire system.
X. CONCLUSIONS
This paper presents a framework for efﬁcient solution
of reliability models of large-scale storage systems utiliz-
ing deduplication. Our framework generates models from
component-based templates, adds deduplication relation-
ships derived from empirical data, and identiﬁes dependence
relationships in the generated model. These dependence
relationships are used to improve the efﬁciency of model
solution while leaving the reward measures unaffected. We
demonstrate our method by solving a large-scale storage
system and achieve signiﬁcant speed-ups of roughly 20x
when compared to unmodiﬁed discrete-event simulation.
Our results show the importance of detailed models of
deduplication based on ﬁle categories by showing some cat-
Figure 9: Annual mean ﬁles lost due to failure of 8 + p.
deduplication, the Code and Database 2 categories showed
an increase in reliability due to a more even distribution
of references, meaning that no sets of ﬁles existed which
contributed disproportionately to the impact from losses
of deduplicated references. Likewise, similar results were
shown for the impact of deduplication on corrupt data served
due to UDEs. Figure 10 shows rates of corrupt data served
per year for the categories Database 1 and Database 2. In
the case of Database 1, deduplication improves reliability,
where as for Database 2, reliability degrades. Results were
similar for other ﬁle categories in our modeled system.
RAID implemented as 8 + 2p and 8 + 3p proved highly
fault-tolerant, with few data loss events occurring, afﬁrming
that multi-copy deduplication is unnecessary for protection
against additional loss due to RAID failure in one petabyte
systems. Such systems, however, provide no additional
protection against UDEs, however. For systems in which
deduplication caused a decrease in reliability, keeping an
additional copy for the most referenced 1% of ﬁles in each
category was sufﬁcient protection, though additional pro-
(cid:0)(cid:1)(cid:0)(cid:1)(cid:2)(cid:3)(cid:0)(cid:1)(cid:2)(cid:4)(cid:0)(cid:1)(cid:2)(cid:5)(cid:0)(cid:1)(cid:2)(cid:6)(cid:0)(cid:1)(cid:2)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:9)(cid:4)(cid:10)(cid:8)(cid:9)(cid:5)(cid:10)(cid:0)(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:5)(cid:8)(cid:6)(cid:9)(cid:10)(cid:2)(cid:11)(cid:10)(cid:12)(cid:6)(cid:13)(cid:10)(cid:2)(cid:6)(cid:14)(cid:10)(cid:8)(cid:2)(cid:11)(cid:12)(cid:13)(cid:14)(cid:0)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:19)(cid:16)(cid:17)(cid:14)(cid:23)(cid:24)(cid:23)(cid:25)(cid:23)(cid:26)(cid:27)(cid:0)(cid:3)(cid:28)(cid:0)(cid:11)(cid:23)(cid:24)(cid:27)(cid:0)(cid:16)(cid:18)(cid:0)(cid:15)(cid:16)(cid:22)(cid:22)(cid:21)(cid:10)(cid:24)(cid:0)(cid:14)(cid:23)(cid:24)(cid:23)(cid:0)(cid:11)(cid:27)(cid:23)(cid:29)(cid:0)(cid:29)(cid:21)(cid:27)(cid:0)(cid:24)(cid:16)(cid:0)(cid:30)(cid:31) (cid:26)(cid:0)(cid:18)(cid:16)(cid:22)(cid:0)(cid:3)!"(cid:0)#(cid:24)(cid:16)(cid:22)(cid:23)(cid:20)(cid:27)(cid:0)#$(cid:26)(cid:24)(cid:27)%&(cid:16)(cid:0)(cid:14)(cid:27)(cid:29)(cid:21)(cid:10)’(cid:19)((cid:23)(cid:24)(cid:19)(cid:16)(cid:17)(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)(cid:3)(cid:1)(cid:1)*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)++*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$)(cid:0)(cid:3)*(cid:0)(cid:4)(cid:0)((cid:16)(cid:10)$(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)+(cid:1)*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$)(cid:0)(cid:3)(cid:1)*(cid:0)(cid:4)(cid:0)((cid:16)(cid:10)$(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)(cid:7)(cid:1)*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$)(cid:0)(cid:7)(cid:1)*(cid:0)(cid:4)(cid:0)((cid:16)(cid:10)$(cid:0)(cid:1)(cid:0)(cid:1)(cid:2)(cid:3)(cid:0)(cid:1)(cid:2)(cid:4)(cid:0)(cid:1)(cid:2)(cid:5)(cid:0)(cid:1)(cid:2)(cid:6)(cid:0)(cid:1)(cid:2)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:9)(cid:4)(cid:10)(cid:8)(cid:9)(cid:5)(cid:10)(cid:0)(cid:1)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:5)(cid:8)(cid:6)(cid:9)(cid:10)(cid:2)(cid:11)(cid:10)(cid:12)(cid:6)(cid:13)(cid:10)(cid:2)(cid:6)(cid:14)(cid:10)(cid:8)(cid:2)(cid:11)(cid:12)(cid:13)(cid:14)(cid:0)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:19)(cid:16)(cid:17)(cid:14)(cid:23)(cid:24)(cid:23)(cid:25)(cid:23)(cid:26)(cid:27)(cid:0)(cid:4)(cid:28)(cid:0)(cid:11)(cid:23)(cid:24)(cid:27)(cid:0)(cid:16)(cid:18)(cid:0)(cid:15)(cid:16)(cid:22)(cid:22)(cid:21)(cid:10)(cid:24)(cid:0)(cid:14)(cid:23)(cid:24)(cid:23)(cid:0)(cid:11)(cid:27)(cid:23)(cid:29)(cid:0)(cid:29)(cid:21)(cid:27)(cid:0)(cid:24)(cid:16)(cid:0)(cid:30)(cid:31) (cid:26)(cid:0)(cid:18)(cid:16)(cid:22)(cid:0)(cid:3)!"(cid:0)#(cid:24)(cid:16)(cid:22)(cid:23)(cid:20)(cid:27)(cid:0)#$(cid:26)(cid:24)(cid:27)%&(cid:16)(cid:0)(cid:14)(cid:27)(cid:29)(cid:21)(cid:10)’(cid:19)((cid:23)(cid:24)(cid:19)(cid:16)(cid:17)(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)(cid:3)(cid:1)(cid:1)*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)++*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$)(cid:0)(cid:3)*(cid:0)(cid:4)(cid:0)((cid:16)(cid:10)$(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)+(cid:1)*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$)(cid:0)(cid:3)(cid:1)*(cid:0)(cid:4)(cid:0)((cid:16)(cid:10)$(cid:14)(cid:27)(cid:29)(cid:21)(cid:10))(cid:0)(cid:7)(cid:1)*(cid:0)(cid:3)(cid:0)((cid:16)(cid:10)$)(cid:0)(cid:7)(cid:1)*(cid:0)(cid:4)(cid:0)((cid:16)(cid:10)$(cid:0)(cid:1)(cid:2)(cid:2)(cid:0)(cid:1)(cid:2)(cid:2)(cid:2)(cid:0)(cid:1)(cid:2)(cid:2)(cid:2)(cid:2)(cid:0)(cid:1)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:0)(cid:1)(cid:3)(cid:4)(cid:2)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:12)(cid:13)(cid:14)(cid:3)(cid:15)(cid:16)(cid:17)(cid:16)(cid:18)(cid:16)(cid:19)(cid:3)(cid:0)(cid:1)(cid:15)(cid:16)(cid:17)(cid:16)(cid:18)(cid:16)(cid:19)(cid:3)(cid:0)(cid:20)(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:3)(cid:3)(cid:5)(cid:2)(cid:6)(cid:4)(cid:7)(cid:8)(cid:6)(cid:1)(cid:9)(cid:4)(cid:6)(cid:10)(cid:9)(cid:11)(cid:4)(cid:12)(cid:5)(cid:1)(cid:4)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:4)(cid:7)(cid:2)(cid:8)(cid:6)(cid:5)(cid:18)(cid:1)(cid:21)(cid:10)(cid:22)(cid:3)(cid:0)(cid:12)(cid:16)(cid:17)(cid:3)(cid:23)(cid:13)(cid:7)(cid:24)(cid:25)(cid:26)(cid:27)(cid:18)(cid:3)(cid:7)(cid:0)(cid:13)(cid:28)(cid:0)(cid:28)(cid:10)(cid:22)(cid:3)(cid:19)(cid:0)(cid:22)(cid:13)(cid:19)(cid:17)(cid:0)(cid:14)(cid:26)(cid:3)(cid:0)(cid:17)(cid:13)(cid:0)(cid:29)(cid:6)(cid:30)(cid:15)(cid:31)(cid:0)(cid:28)(cid:16)(cid:10)(cid:22)(cid:26)(cid:7)(cid:3)(cid:0)(cid:28)(cid:13)(cid:7)(cid:0)(cid:1) !(cid:0)(cid:19)(cid:17)(cid:13)(cid:7)(cid:16)(cid:23)(cid:3)(cid:0)(cid:19)(cid:24)(cid:19)(cid:17)(cid:3)(cid:27)(cid:25)(cid:13)(cid:0)(cid:15)(cid:3)(cid:14)(cid:26)"(cid:22)(cid:10)(cid:8)(cid:16)(cid:17)(cid:10)(cid:13)#(cid:15)(cid:3)(cid:14)(cid:26)"$(cid:0)(cid:1)(cid:2)(cid:2)%(cid:0)(cid:1)(cid:0)(cid:8)(cid:13)"(cid:24)$(cid:0)(cid:2)%(cid:0)(cid:20)(cid:0)(cid:8)(cid:13)"(cid:24)(cid:15)(cid:3)(cid:14)(cid:26)"$(cid:0)&&%(cid:0)(cid:1)(cid:0)(cid:8)(cid:13)"(cid:24)$(cid:0)(cid:1)%(cid:0)(cid:20)(cid:0)(cid:8)(cid:13)"(cid:24)(cid:15)(cid:3)(cid:14)(cid:26)"$(cid:0)(cid:31)(cid:2)%(cid:0)(cid:1)(cid:0)(cid:8)(cid:13)"(cid:24)$(cid:0)(cid:31)(cid:2)%(cid:0)(cid:20)(cid:0)(cid:8)(cid:13)"(cid:24)egories where deduplication improves reliability, and some
where it decreases reliability. We show that even for a similar
type of category, the impact of deduplication may not be the
same.
Our results emphasize the need to generate detailed mod-
els of deduplicated systems when making design decisions.
Previous work which suggested broad application of multi-
copy deduplication, while effective for improving reliability,
doesn’t take into account the diminishing returns realized
when larger percentages of the deduplicated storage system
store multiple-copies of each referenced chunk. Detailed
analysis has proven difﬁcult in the past due to the com-
plexities involved with solving large models containing rare-
events, but the framework we present in this paper signiﬁ-
cantly reduces the time needed to conduct such studies.
The limit of increased reliability for systems employing
these methods are the undeduplicated portions of ﬁles. Given
that most storage systems are overprovisioned, or include
unused hot spares to allow RAID repair, we propose that this
underutilized space might be overbooked to keep additional
copies of undeduplicated portions of important ﬁles, further
improving reliability.
REFERENCES
H.
R.
[1] P.
Lyman,
Searingen,
J. Pal,
[Online]. Avail-
http://www.sims.berkeley.edu/research/projects/how-
P. Charles, N. Good, L. L.
“How much
able:
much-info/
K.
Jordan,
information?”
Varian,
2003.
and
[2] J. F. Gantz, C. Chute, A. Manfrediz, S. Minton, D. Reinsel,
W. Schlichting, and A. Toncheva, “The diverse and exploding
digital universe: An updated forecast of worldwide informa-
tion growth through 2011,” White Paper, IDC, March 2008.
[3] J. F. Gantz and D. Reinsel, “Extracting value from chaos,”
White Paper, IDC, June 2011.
[4] E. W. Rozier, W. H. Sanders, P. Zhou, N. Mandagere,
S. M. Uttamchandani, and M. L. Yakushev, “Modeling the
fault tolerance consequences of deduplication,” in Reliable
Distributed Systems (SRDS), 2011 30th IEEE Symposium on,
oct. 2011, pp. 75 –84.
[5] B. Zhu, K. Li, and H. Patterson, “Avoiding the disk bottleneck
in the data domain deduplication ﬁle system,” in USENIX
FAST, 2008, pp. 1–14.
[6] C. Ungureanu, B. Atkin, A. Aranya, S. Gokhale, S. Rago,
G. Calkowski, C. Dubnicki, and A. Bohra, “HydraFS: A
high-throughput ﬁle system for the HYDRAstor content-
addressable storage system,” in FAST, 2010, pp. 225–238.
[7] D. Bhagwat, K. Pollack, D. D. E. Long, T. Schwarz, E. L.
Miller, and J.-F. Pris, “Providing high reliability in a min-
imum redundancy archival storage system,” in IEEE MAS-
COTS, 2006, pp. 413–421.
[8] L. L. You, K. T. Pollack, and D. D. E. Long, “Deep store:
IEEE,
An archival storage system architecture,” in ICDE.
2005, pp. 804–815.
[9] L.
Freeman,
Tech.
deduplication,” Ne-
tApp,
[Online].
Available:
http://media.netapp.com/documents/tot0608.pdf
“How safe
Rep.,
2008.
is
[10] B. Schroeder and G. A. Gibson, “Disk failures in the real
world: what does an MTTF of 1,000,000 hours mean to you?”
in FAST, 2007, p. 1.
[11] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and
J. Schindler, “An analysis of latent sector errors in disk
drives,” SIGMETRICS 35, no. 1, pp. 289–300, 2007.
[12] B. Schroeder, S. Damouras, and P. Gill, “Understanding latent
sector errors and how to protect against them,” in FAST, 2010,
pp. 71–84.
[13] A. Krioukov, L. N. Bairavasundaram, G. R. Goodson,
K. Srinivasan, R. Thelen, A. C. Arpaci-Dusseau, and R. H.
Arpaci-Dussea, “Parity lost and parity regained,” in FAST.
USENIX, 2008, pp. 1–15.
[14] J. L. Hafner, V. Deenadhayalan, W. Belluomini, and K. Rao,
“Undetected disk errors in RAID arrays,” IBM J Research
and Development 52, no. 4, pp. 413–425, 2008.
[15] E. W. D. Rozier, W. Belluomini, V. Deenadhayalan, J. Hafner,
K. K. Rao, and P. Zhou, “Evaluating the impact of undetected
disk errors in RAID systems,” in DSN, 2009, pp. 83–92.
[16] W. D. Oball II, Measure-Adaptive State-Space Construction
Methods. U Arizona, 1998.
[17] W. Sanders and J. Meyer, “A uniﬁed approach for specifying
measures of performance, dependability, and performability,”
in DCCA 4. Springer, 1991, pp. 215–237.
[18] R. A. Howard, Dynamic Probabilistic Systems. Vol II: Semi-
Markov and Decision Processes. New York: Wiley, 1971.
[19] J. F. Meyer, “On evaluating the performability of degradable
computing systems,” IEEE TC 29, pp. 720–731, 1980.
[20] W. H. Sanders and J. F. Meyer, “A uniﬁed approach to
specifying measures of performance, dependability, and per-
formability,” Dependable Computing for Critical Applica-
tions, vol. 4, pp. 215–237, 1991.
[21] M. O. Rabin, “Fingerprinting by random polynomials,” Tech.
Rep., 1981.
[22] A. Z. Broder, “Identifying and ﬁltering near-duplicate docu-
ments,” in CPM. Springer, 2000, pp. 1–10.
[23] L. N. Bairavasundaram, G. R. Goodson, B. Schroeder, A. C.
Arpaci-Dusseau, and R. H. Arpaci-Dussea, “An analysis of
data corruption in the storage stack,” in FAST. USENIX,
2008, pp. 1–16.
[24] D. A.
Patterson, G. A. Gibson,
and R. H.
Katz, “A case for Redundant Arrays of
Inexpensive
Disks
EECS Department, UC Berkeley,
Tech. Rep. UCB/CSD-87-391, 1987. [Online]. Available:
http://www.eecs.berkeley.edu/Pubs/TechRpts/1987/5853.html
(RAID),”
[25] G. Ciardo and K. S. Trivedi, “A decomposition approach for
stochastic reward net models,” Performance Eval. 18, no. 1,
pp. 37 – 59, 1993.
[26] J. Bucklew and R. Radeke, “On the Monte Carlo simulation
of digital communication systems in Gaussian noise,” IEEE
Trans. Comm. 51, no. 2, pp. 267 – 274, 2003.