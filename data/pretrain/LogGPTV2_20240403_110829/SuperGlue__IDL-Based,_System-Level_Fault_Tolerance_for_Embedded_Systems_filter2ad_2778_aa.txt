title:SuperGlue: IDL-Based, System-Level Fault Tolerance for Embedded Systems
author:Jiguo Song and
Gedare Bloom and
Gabriel Parmer
2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
SuperGlue: IDL-Based, System-Level Fault Tolerance for Embedded Systems
Jiguo Song‚àó, Gedare Bloom‚Ä†, Gabriel Parmer‚àó
‚àóThe George Washington University
{jiguos, gparmer}@gwu.edu
‚Ä†Howard University
PI:EMAIL
Abstract‚ÄîAs the processor feature sizes shrink, mitigating
faults in low level system services has become a critical aspect of
dependable system design. In this paper we introduce SuperGlue,
an interface description language (IDL) and compiler for recov-
ery from transient faults in a component-based operating system.
SuperGlue generates code for interface-driven recovery that uses
commodity hardware isolation, micro-rebooting, and interface-
directed fault recovery to provide predictable and efÔ¨Åcient
recovery from faults that impact low-level system services.
SuperGlue decreases the amount of recovery code system
designers need to implement by an order of magnitude,
and replaces it with declarative speciÔ¨Åcations. We evaluate
SuperGlue with a fault injection campaign in low-level system
components (e.g., memory mapping manager and scheduler).
Additionally, we evaluate the performance of SuperGlue in a
web-server application. Results show that SuperGlue improves
system reliability with only a small performance degradation of
11.84%.
I. INTRODUCTION
Recent advances in silicon technology enable processors
with billions of on-chip transistors, however these advances in-
creasingly cause processors to be susceptible to transient faults
such as single event upsets (SEUs) or other soft errors. The
risk of a soft error induced system failure has become more
prominent and of great concern in systems that require high
dependability such as safety-critical embedded systems. Fault
tolerance and recovery from transient faults in such embedded
systems historically uses triple modular redundancy (TMR),
but the cost of TMR in terms of size, weight, and power
(SWaP) is large due to software and hardware replication.
The challenge for an embedded system is to minimize SWaP
while maximizing fault tolerance.
EfÔ¨Åcient fault
tolerance approaches aim to protect and
recover speciÔ¨Åc modules of a system, such as the device
drivers [1] or application modules [2], [3], [4]. These ap-
proaches are problematic as a fault in an unprotected module
can bring down the system. Consider a fault that crashes a
thread scheduler, which would invalidate fault tolerance mech-
anisms operating at an application or user level. Nearly 65% of
hardware errors corrupt operating system (OS) state [5] before
detection. Once OS state is corrupted, the fault can propagate
to any part of physical memory and affect user-level software.
OS architecture plays an important role in system reliability.
Microkernels, for example,
improve system reliability by
This material is based upon work supported by the National Science
Foundation under Grant No. CNS 1149675 and ONR Award No. N00014-
14-1-0386. Any opinions, Ô¨Åndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily
reÔ¨Çect the views of the National Science Foundation or ONR.
978-1-4673-8891-7/16 $31.00 ¬© 2016 IEEE
DOI 10.1109/DSN.2016.29
227
decomposing system services into partitioned modules, or
components, with well-deÔ¨Åned, isolated boundaries between
them. These components do not interfere with each other in
unpredictable ways, and this isolation naturally enhances fault
tolerance by preventing blanket access to all of memory and
limiting fault propagation: faults can propagate between two
components only through the data shared over the interface
between them [6]. Isolation helps to limit the impact of faults
by constraining the effects of system-level faults, yet it is not
sufÔ¨Åcient for fault tolerance. For example, a failed scheduler
cannot simply be rebooted with the expectation the resultant
system behaves correctly.
Prior work on the Computational Crash-Cart, or C3 [7],
leverages Ô¨Åne-grained isolation between system components
to enable interface-driven recovery of a failed system com-
ponent by leveraging logic within the interface stub code
between communicating components. Stub code encodes and
translates state about the components and uses component
interface functions to rebuild and recover the state of a
failed system component such as a scheduler. Interface-driven
recovery avoids the overheads of check-pointing and repli-
cation, thus lowering SWaP. Of note for real-time embedded
systems, the C3 interface-driven recovery is predictable and
has a demonstrated schedulability analysis for hard real-time
systems. Unfortunately, C3 uses recovery mechanisms in an
ad-hoc manner, and the hand-written stub code that interposes
on invocations between components is complex and error-
prone.
In this paper we introduce SuperGlue, which is a model
for interface-driven recovery and a declarative speciÔ¨Åcation
of component behavior implemented as an interface deÔ¨Åni-
tion language (IDL). SuperGlue aims to provide predictable
recovery from the failure of low-level system services without
extensive changes to those services‚Äô code. Instead, SuperGlue
utilizes an IDL speciÔ¨Åcation of each service‚Äôs interface and
then tracks how a service uses resource descriptors and
resources with a state machine. The net result for SuperGlue
is a system with predictable, efÔ¨Åcient recovery from system
faults using an order of magnitude fewer lines of recovery
code, that is written in a declarative manner, compared to a
system using error-prone, hand-written recovery code.
The contributions in this work include:
‚Ä¢ a model for inter-component interactions and component
semantics that differentiates between necessary recovery
mechanisms;
‚Ä¢ a compiler that synthesizes interface-driven recovery code
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:19:10 UTC from IEEE Xplore.  Restrictions apply. 


	
	










	
	

(a) COMPOSITE
(b) C3
Fig. 1: (a) A set of hardware-isolated components in COM-
POSITE (b) Interface-driven recovery with C3 for a system-
level components. Dotted rectangle is an interface stub, small
bold black squares in the stub represent recovery metadata,
and squiggly vertical lines are threads.
based on an IDL speciÔ¨Åcation;
‚Ä¢ and an evaluation of SuperGlue overhead and fault toler-
ance in a component-based operating system subjected to
fault injection in system-level components.
Experimental results show that SuperGlue can produce a de-
pendable OS for embedded systems with minor performance
degradation compared to a similar system without fault toler-
ance. We demonstrate our approach using COMPOSITE [8],
an existing component-based OS that supports predictable
real-time embedded systems, and C3, which provides fault
recovery mechanisms for COMPOSITE.
II. BACKGROUND: COMPOSITE AND C3
SuperGlue uses C3 mechanisms implemented in the COM-
POSITE Œº-kernel. This section brieÔ¨Çy introduces these systems
after explaining our fault model.
A. Fault Model
We assume transient faults that affect processor pipelines
and registers with a fail-stop model. Thus, faults are detected
immediately after corrupting system state. Past work in fault
characterization Ô¨Ånds that 65% [1], 80.6% [9], and 93% [10]
of injected faults with detectable failures result in fail-stop
behavior. Our own results conÔ¨Årm these numbers. Given
the pressure on decreasing chip processes sizes, we focus
on transient faults impacting the chip‚Äôs pipeline and on-
chip structures [11], rather than persistent memory corruption
where ECC can prevent most faults [12].
B. COMPOSITE Component-based OS
COMPOSITE is an open-source OS that consists of a small
kernel [13] (less than 7000 lines of code) and a collection
of user-level components that implement the bulk of system-
level services such as scheduling, memory management, time
management, and synchronization. Components have well-
deÔ¨Åned interfaces consisting of sets of function calls pro-
vided by the server component to a client component that
228
invokes them. Invoking a function in a component‚Äôs interface
triggers a component invocation‚ÄìCOMPOSITE‚Äôs inter-process
communication primitive‚Äìthat is mediated by capability-based
access control in the kernel [13]. COMPOSITE focuses on
the Ô¨Åne-grained decomposition of system functionality into
separate, isolated components. For example, a componentized
web-server consists of over 20 separate components [8].
Unlike most modern Œº-kernels, COMPOSITE uses syn-
chronous thread migration [14], [15] instead of synchronous
rendezvous between threads as in L4 variants [16]. Compo-
nents are active when a thread invokes them or if they request
the creation of a thread. They are also concurrent in that
multiple threads can be runnable within a component at a
time. Figure 1(a) depicts a set of application- and system-level
components, shown in black bold rectangles, with threads
(squiggly lines) executing in some components. The difference
between system-level and application-level components is
that system components manage (hardware) resources, often
provided as a service to many clients. Hardware (page-table)
protection mechanisms isolate each component‚Äôs memory and
prevent sharing data structures or passing addresses directly.
C. C3 Interface-Driven Recovery
C3 is a fault tolerance infrastructure built on top of COM-
POSITE that makes use of the Ô¨Åne-grained isolation between
components. Such isolation naturally provides a base-level of
fault tolerance by constraining the scope of fault propagation:
Barbosa et al. [6] reported a decrease in fault propagation
for ŒºC/OS-II from about 60%‚Äì70% down to only 22% of
transient faults by making each process‚Äôs address space private
and hardware-protected. C3 leverages the low incidence of
fault propagation between components (no propagation was
observed in experiments) to enable non-faulty components to
aid in the recovery of a failed system-level component.
Interface-driven recovery in C3 begins by Œº-rebooting the
failed component using a ‚Äúbooter‚Äù component to re-initialize
the faulty component at the cost of memcpy. This reboot places
the component into a safe (fault-free) state. Then the state
of the component is made consistent with what its clients
and servers expect by referring to a summary of component
invocations prior to the crash. Each thread that was executing
within a faulty component diverts back to the invoking client
and executes stub code in the interface that recreates the
server‚Äôs state. Figure 1(b) depicts the process of interface-
driven recovery. Descriptor state, in small bold black squares,
is tracked on the client-side (dotted rectangle) of a component
invocation. A fault in a component causes recovery of its
clients by micro-rebooting the faulty component, resetting
it to an initial state, and rebuilding server state using the
descriptor tracking information stored in the client. When the
failed server depends on another component, it must, in some
cases, recover descriptors it had before failure. In such cases, a
component can reÔ¨Çect on its depended-on servers to explicitly
request the state of descriptors from each of those servers.
As an example of interface-driven recovery, consider a fault
in a component that provides locks for mutual exclusion.
When the component is Œº-rebooted, multiple clients might
have already created multiple locks. Furthermore,
threads
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:19:10 UTC from IEEE Xplore.  Restrictions apply. 
might have acquired or be contending locks. The recovered
component‚Äôs internal state must be made consistent with the
expectations of other components. Thus, the client‚Äôs stub code
in this example will regenerate the component‚Äôs state by
recreating, acquiring, or contending locks.
C3 stub code executes at user-level in the client and server.
This code tracks the state of each object acted on by the
functions in the interface ‚Äì e.g., locks in the previous example,
Ô¨Åle descriptors for a Ô¨Åle system (FS), or page virtual addresses
for a memory mapping manager. With SuperGlue we call
these objects descriptors. The straight-forward way to track
the modiÔ¨Åcations made to the descriptors (e.g., lock taken,
lock contended, Ô¨Åle offset advanced) maintains a log of oper-
ations. However, as C3 targets embedded systems, unbounded
memory consumption for the log is unacceptable. Instead, C3
encodes the state of a descriptor with a state machine that
contains a bounded amount of data. This data is derived from
the arguments and return values of each interface function and
is speciÔ¨Åc to the interface. For example, the path in the FS
namespace is tracked for an open Ô¨Åle descriptor, along with
the offset of the descriptor into the Ô¨Åle, which is updated
based on the return values from read and write. Recreating
a descriptor is equivalent to transiting a path through the
state machine that brings the descriptor into the expected
state (e.g., an acquired lock) and sets the descriptor‚Äôs data
to consistent values (e.g., open and lseek). A client restores
a faulty server‚Äôs state associated with a descriptor by making
component invocations from the stub code, as above. A faulty
client restores its descriptors by reÔ¨Çecting on the server.
Interface-driven recovery is insufÔ¨Åcient for some compo-
nents. For example, for an in memory FS (RamFS) to recover
the data within a Ô¨Åle, the state machine technique would
need to be augmented with the data in the Ô¨Åle. Instead such
components include invocations within the RamFS component
to a third storage component that stores an association includ-
ing  where id is an identiÔ¨Åer to
uniquely identify the Ô¨Åle (e.g., a hash on its path), and offset,
length, and ‚àódata track a slice of the Ô¨Åle. When the RamFS
is recovering and receives client invocations to recreate Ô¨Åle
state, it manually invokes the storage component to retrieve
the Ô¨Åle contents. In RamFS, the Ô¨Åle information is shared
using a zero-copy buffer mapping subsystem [17] in which
all but the producing component (client) has read-only access
to the buffer. This access restriction prevents fault propagation
through the buffer. This buffer is what the storage component
maintains for the RamFS server (as data).
C3 recovery may be conducted either on-demand, or ea-
gerly. Eager recovery iterates through all descriptors in a
client interface to recover the entire state of a component
for each of its clients immediately. In contrast, on-demand
recovery delays recovery of a descriptor until it is accessed by
a thread. Thus, each descriptor is recovered lazily. Importantly,
this means that the descriptor is recovered at the priority of
the thread accessing the descriptor, which has the effect of
lessening the amount of interference due to priority inversion
that recovery has on high-priority processes. On-demand has
the effect of properly prioritizing the recovery process, which
has a signiÔ¨Åcant impact on system schedulability [7].
D. Example: Recovering the Memory Manager
The memory manager (MM) component in COMPOSITE
provides and maintains virtual-to-physical memory mappings
and provides an API close to that of the recursive address
space model [16]. A thread invokes mman get page to
request access to a page and creates the root mapping between
that virtual page and some physical frame. Memory is shared
between components with mman alias page, which creates
a child mapping from a parent in a tree rooted with the
physical frame. mman release page revokes a mapping
and the subtree rooted at it (all transitive aliases). The MM
descriptors are the virtual addresses in the component in which
they are mapped.
The state of a mapping can be described by its virtual
address and client component, and this state is tracked on
the interface between the MM and the client that created the
mapping. If a fault occurs in the MM, the mapping trees
may be corrupted. Œº-rebooting resets all trees. Eager recovery
rebuilds all of the root mappings. On-demand recovery occurs
when a thread makes a component invocation to the MM.
A mapping can only be recovered after its aliased-from
parent mapping is recovered. However, since memory can
be shared between different components (two virtual pages
may refer to the same physical frame), upcalls are made into
client components in order to rebuild correct state between
dependent mappings. This process is transparent to client
execution and happens on-demand.
E. Assumptions and Scope of This Work
C3, and comparably SuperGlue, do not protect several
classes of system software. These include applications, the
kernel itself, and the zero-copy shared buffer management
component. SigniÔ¨Åcant previous work has been done on
embedded system application reliability,
including that on
checkpointing, recovery blocks, and redundant computation.
SuperGlue does not target application-level faults. Instead, it
focuses on system-level components (e.g., schedulers and Ô¨Åle-
systems) whose failure impacts the rest of the system that
depends on them and is complementary to application-level
fault tolerance techniques. The COMPOSITE kernel itself is
small with little state (mainly just page tables, capability
tables, and threads) and optimized to not consume much
execution time. On our hardware (detailed in Section V),
all kernel execution paths are bounded (for predictability),
non-preemptible, and short (the longest taking around 1/2 Œº-
second). Unless occurring at an extremely high frequency, a
pipeline fault is unlikely to impact the kernel.
If faults in the kernel and the shared buffer component
prohibit system reliability, other techniques such as compiler-
based redundant operation or memory encodings can be used.