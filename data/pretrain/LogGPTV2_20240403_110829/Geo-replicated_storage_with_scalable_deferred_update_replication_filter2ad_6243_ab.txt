### Scalable Deferred Update Replication (SDUR) in a Geographically Distributed Environment

#### Overview
This document outlines the implementation and correctness of Scalable Deferred Update Replication (SDUR) in a geographically distributed environment. The system is designed to handle global and local transactions, ensuring serializability and minimizing latency. The key parameters are:
- \( \delta \): Maximum communication delay within the same region.
- \( \Delta \): Maximum communication delay across different regions (typically \( \Delta \gg \delta \)).

The database is partitioned into two partitions, P1 and P2, with clients deployed in the same datacenter as server s1.

#### Transaction Reordering
The reordering threshold for a transaction \( t_i \) determines:
1. Only local transactions among the next \( k \) transactions delivered after \( t_i \) can be reordered before \( t_i \).
2. Server \( s \) can complete \( t_i \) only after receiving all votes for \( t_i \) and delivering \( k \) transactions after \( t_i \).

For example, if \( k = 1 \), server \( s' \) would not complete \( t_i \) after receiving votes from other partitions but would wait for the delivery of \( t_j \) and reorder \( t_j \) and \( t_i \) similarly to server \( s \).

Reordering is applied only to local transactions with respect to global transactions. Experimentally, reordering local or global transactions among themselves did not provide significant benefits. The reordering threshold must be carefully chosen to avoid unnecessary delays for global transactions. Replicas can change the reordering threshold by broadcasting a new value of \( k \).

#### Algorithm Details

**Algorithm 1: Geo-SDUR, Client's Code**
```python
1: begin(t):
2:    t.rs ← ∅  # Initialize readset
3:    t.ws ← ∅  # Initialize writeset
4:    t.st[1...P] ← [⊥...⊥]  # Initialize vector of snapshot times

5: read(t, k):
6:    p ← partition(k)  # Get the key’s partition
7:    send(read, k, t.st[p]) to s ∈ Sp  # Send read request
8:    wait until receive (k, v, st) from s  # Wait for response
9:    if t.st[p] = ⊥ then t.st[p] ← st  # If first read, init snapshot
10:   return v  # Return value from server

11: write(t, k, v):
12:   t.ws ← t.ws ∪ {(k, v)}  # Add key to writeset

13: commit(t):
14:   send(commit, t) to a preferred server s near c  # Send commit request
15:   wait until receive(outcome) from s  # Wait for outcome
16:   return outcome  # Outcome is either commit or abort
```

**Algorithm 2: Geo-SDUR, Server's Code in Partition \( p \)**
```python
1: Initialization:
2:    DB ← [. . .]  # List of applied transactions
3:    PL ← [. . .]  # List of pending transactions
4:    SC ← 0  # Snapshot counter
5:    DC ← 0  # Delivered transactions counter
6:    VOTES ← ∅  # Votes for global transactions

7: when receive(read, k, st) from c:
8:    if st = ⊥ then st ← SC  # If first read, init snapshot
9:    retrieve(k, v, st) from database  # Most recent version ≤ st
10:   send(k, v, st) to c  # Return result to client

11: when receive(commit, t):
12:   submit(t)  # See line 41

13: when receive(tid, v) from partition p:
14:   VOTES ← VOTES ∪ (tid, p, v)  # One more vote for tid

15: when adeliver(c, t):
16:   DC ← DC + 1  # One more transaction delivered
17:   t.rt ← DC + ReorderThreshold  # Set t’s Reorder Threshold
18:   v ← reorder(t)  # See line 48
19:   if v = abort then complete(t, v)  # Reordering resulted in abort

20: if t is global:
21:   send(t.id, v) to all servers in partitions(t)  # Send votes

22: when head(PL) is local:
23:   t ← head(PL)  # Get head without removing entry
24:   if t.rt = DC then complete(t, commit)  # Check if t reached threshold

25: when head(PL) is global:
26:   (c, t, v) ← head(PL)  # Get head without removing entry
27:   if ∀k s.t t.st[k] ≠ ⊥ : (t.id, k, ⋆) ∈ VOTES and t.rt = DC:
28:      complete(t, commit)  # Check if t has all votes and reached threshold
29:   else:
30:      complete(t, abort)  # Otherwise, abort t

31: function complete(t, outcome):
32:   apply t.ws with version SC to database  # Apply changes
33:   DB[SC + 1] ← t  # Create next snapshot
34:   SC ← SC + 1  # Update snapshot counter
35:   PL ← PL ⊖ t  # Remove t from PL
36:   send(outcome) to client of t  # Notify client

37: procedure submit(t):
38:   let P be partitions(t) \ {p}  # Broadcast t to each remote partition
39:   ∆ ← max({delay(x, p) | x ∈ P})  # Determine maximum delay
40:   abcast(p, t) after ∆ time units  # Delay local broadcast

41: function ctest(t, t′):
42:   return (t.rs ∩ t′.ws = ∅) ∧ (t is local ∨ (t.ws ∩ t′.rs = ∅))

43: function reorder(t):
44:   if ∃t′ ∈ DB[t.st[p]..SC] : ctest(t, t′) = false then return abort
45:   if ∃t′ ∈ PL : ctest(t, t′) = false then return abort
46:   append t to PL  # Include t in pending list if no conflicts
47:   let i be the smallest integer, if any, such that
48:     ∀k < i : PL[k].ws ∩ t.rs = ∅ and  # t’s reads are not stale
49:     ∀k ≥ i : (PL[k] is global and  # No leaping local transactions
50:               PL[k].rt < DC and  # No leaping globals after threshold
51:               t.ws ∩ PL[k].rs = ∅ and  # Previous votes still valid
52:               t.rs ∩ PL[k].ws = ∅)  # Ditto!
53:   if no i satisfies the conditions above then return abort
54:   for k from size(PL) downto i do PL[k + 1] ← PL[k]
55:   PL[i] ← t  # After making room (above), insert t
56:   return commit  # t is a completed transaction!
```

#### Correctness
1. **Serializability in SDUR**: 
   - Local transactions are serialized within a partition following their delivery order.
   - Global transactions are more complex due to the lack of total order across partitions. They can interleave in three ways:
     - \( t' \) precedes \( t \) in all partitions.
     - \( t' \) precedes \( t \) in partition \( p \) and they are concurrent in \( p' \).
     - \( t' \) and \( t \) are concurrent in both \( p \) and \( p' \).

2. **Correctness of Delaying Transactions**:
   - Delaying the broadcast of a global transaction \( t \) in a partition may delay its delivery at \( p \), but it does not affect the correctness of the protocol. In an asynchronous system, even if \( t \) is broadcast to all partitions simultaneously, network delays can cause it to be delivered at any arbitrary time in the future.

3. **Correctness of Reordering Transactions**:
   - For a local transaction \( t \) delivered after a global transaction \( t' \) at partition \( p \):
     - If server \( s \) in \( p \) reorders \( t \) and \( t' \), every correct server \( s' \) in \( p \) will also reorder \( t \) and \( t' \).
     - The reordering of \( t \) and \( t' \) does not violate serializability because the condition for \( t \) to be placed before \( t' \) ensures that both transactions would be committed if \( t \) had been delivered before \( t' \).

By ensuring that reordering and delaying mechanisms are deterministic and consistent, SDUR maintains the integrity and consistency of the database in a geographically distributed environment.