Region 2
Region 3
Region 1
Region 1
P1
P2
s1
s2
s3
s4
s5
s6
➀ : Client termination request
➁ : Request to remote Paxos coordinator
➂ : Paxos Phase 2A messages
➃ : Paxos Phase 2B messages
➄ : Votes exchanged by partitions
➅ : Changes applied to database
➆ : Response to client
Latency of remote reads
Latency of local termination
2δ
4δ
Latency of global termination
4δ+2Δ
Tolerate datacenter failures?
Tolerate region failures?
yes
no
2δ
2δ+2Δ
3δ+3Δ
yes
yes
Scalable Deferred Update Replication deployments in a geographically distributed environment, where δ is the maximum
Figure 1.
communication delay between servers in the same region and ∆ is the maximum communication delay across regions; typically ∆ ≫ δ.
The database contains two partitions, P1 and P2, and clients are deployed in the same datacenter as server s1.
global transaction ti. Transaction ti’s reordering threshold
determines that (a) only local transactions among the next
k transactions delivered after ti can be reordered before ti;
and (b) s can complete ti only after s receives all votes for
ti and s has delivered k transactions after ti. In the previous
example, if we set k = 1, then server s′ would not complete
ti after receiving ti’s votes from other partitions, but would
wait for the delivery of tj and, similarly to server s, s′ would
reorder tj and ti.
Note that we try to reorder local transactions with respect
to global transactions only. We found experimentally that
reordering local transactions among themselves and global
transactions among themselves did not bring any signiﬁcant
beneﬁts. The reordering threshold must be carefully chosen:
a value that is too high with respect to the number of local
transactions in the workload might introduce unnecessary
delays for global
transactions. Replicas can change the
reordering threshold by broadcasting a new value of k.
F. Algorithm in detail
Algorithm 1 Geo-SDUR, client c’s code
1: begin(t):
2:
3:
4:
t.rs ← ∅
t.ws ← ∅
t.st[1...P ] ← [⊥...⊥]
{initialize readset}
{initialize writeset}
{initialize vector of snapshot times}
t.rs ← t.rs ∪ {k}
if (k, ⋆) ∈ t.ws then
return v s.t. (k, v) ∈ t.ws
else
5: read(t, k):
6:
7:
8:
9:
10:
11:
12:
13:
14:
p ← partition(k)
send(read, k, t.st[p]) to s ∈ Sp
wait until receive (k, v, st) from s
if t.st[p] = ⊥ then t.st[p] ← st
return v
{add key to readset}
{if key previously written...}
{return written value}
{else, if key never written...}
{get the key’s partition}
{send read request}
{wait response}
{if ﬁrst read, init snapshot}
{return value from server}
15: write(t, k, v):
16:
t.ws ← t.ws ∪ {(k, v)}
{add key to writeset}
17: commit(t):
18:
19:
20:
send(commit, t) to a preferred server s near c
wait until receive(outcome) from s
return outcome
{outcome is either commit or abort}
Algorithm 1 shows the client side of the protocol. To
execute a read, the client sends a request to a server in
the partition that stores the accessed key (lines 10–12).
The snapshot of a transaction is represented by an array of
integers, one per partition (line 4). Upon receiving the ﬁrst
response from the server, the client initializes its snapshot
time for the corresponding partition (line 13). Subsequent
reads to the same partition will include the snapshot count so
that the transaction sees a consistent database view. Writes
are buffered at the client, and only sent to servers at commit
time. When the execution phase ends the transaction is sent
to a preferred server possibly near the client, which in turn
broadcasts the transaction for certiﬁcation to all partitions
concerned by the transaction. The client then waits for the
transaction’s outcome (line 17–20). Note that a client can
choose to commit a transaction against a server it did not
contact previously while executing read operations.
Algorithm 2 shows the protocol for a server s in partition
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
p. When s receives a request to commit transaction t, s calls
procedure submit (lines 11–12), which broadcasts t to each
one of the partitions involved in t, possibly delaying the
broadcast at partition p, if transaction delaying is enabled
(lines 41–45). In Algorithm 2, delay(x, p) (line 44) returns
the estimated delay between partitions x and p.
Upon delivering transaction t (line 15), s updates the
delivery counter DC and set t’s reordering threshold, which
will be only used by global transactions (lines 16–17). Then
s certiﬁes and possibly reorders t (line 18). The reorder
function (lines 48–64) ﬁrst certiﬁes t against transactions
that committed after t started (line 49). This check uses
function ctest (lines 46–47), which distinguishes between
local and global transactions: while a local transaction has its
readset compared against the writeset of committed transac-
tions, a global transactions has both its readset and writeset
compared against committed transactions (see Section III
for a description of why this is needed). If some conﬂict is
found, t must abort (line 50); otherwise the check continues.
transaction t is further checked against all
pending transactions (lines 51–52), to avoid non-serializable
executions that can happen when transactions are delivered
in different orders at different partitions. In the absence of
conﬂicts, t becomes a pending transaction (line 53).
A global
A local transaction t will be possibly reordered among
pending transactions (lines 55–64). The idea is to ﬁnd a
position for t in the pending list as close to the beginning
of the list as possible, since that would allow t to leap over
the maximum number of global transactions (line 55), that
satisﬁes the following constraints: (a) transactions placed in
the pending list that will consequently commit before t must
not update any items that t reads (line 56); (b) we do not
wish to reorder t with other local transactions, and thus, all
transactions placed after t in the pending list must be global
(line 57); (c) we do not allow a local transaction to leap over
a global transaction that has reached its reorder threshold, in
order to ensure a deterministic reordering check (line 58);
and ﬁnally (d) the reordering of t must not invalidate the
votes of any previously certiﬁed transactions (lines 59–60).
If no position satisﬁes the conditions above, t must abort
(line 61); otherwise, s inserts t in the appropriate position
in the list of pending transactions (lines 62–63) and t is
declared committed (line 64).
A local transaction is committed as soon as it is the head
of the pending list (lines 23–25). The complete function
(lines 34–40) ﬁrst removes the terminating transaction t
from the pending list (line 35) and if t’s outcome is commit
(line 36), it applies t’s writes to the database (line 37), ex-
poses t’s changes to new transactions (line 38), and updates
the snapshot counter (line 39). Whatever the outcome of t,
server s notiﬁes the client (line 40).
and (b) t has reached its reordering threshold (line 29).
If these conditions hold, s checks whether all partitions
voted to commit t (lines 30–32) and completes t accordingly
(line 33).
When a global
transaction t reaches the head of the
pending list, conditions (a) and (b) above will eventually
hold provided that all votes for t are received (lines 13–
14) and transactions are constantly delivered, increasing the
value of the DC counter (line 16). If a server fails while
executing the submit procedure for transaction t, then it may
happen that some partition p delivers t while some other
partition p′ will never do so. As a result, servers in p will not
complete t since p′’s vote for t will be missing. To solve this
problem, if a server s in p suspects that t was not broadcast
to p′, because t’s sender failed, s atomically broadcasts a
message to p′ requesting t to be aborted. Atomic broadcast
ensures that all servers in p′ deliver ﬁrst either s’s request
to abort t or transaction t; servers in p′ will act according
to the ﬁrst message delivered (see [5] for further details).
G. Correctness
In this section, we argue that SDUR implements serial-
izability. We brieﬂy recall how SDUR ensures serializable
executions (a detailed discussion can be found in [5])
and then extend our argument to include the delaying and
reordering of transactions.
1) The correctness of SDUR: In SDUR, transactions in
a partition p, both local and global, are serialized following
their delivery order. Certiﬁcation checks whether a delivered
transaction t can be serialized after all previously delivered
and committed transactions t′. If t has received a database
snapshot that includes t′’s writes, then t′’s commit precedes
t’s start at p (i.e, t′ and t executed sequentially) and t can be
obviously serialized after t′. If t′ committed after t received
its database snapshot, then in order for t to be serialized
after t′, t must not have read any item written by t′.
The procedure above guarantees that local transactions
are serialized within a partition. The certiﬁcation of global
transactions is more complex, to account for the lack of total
order across partitions.
Global transactions t and t′ can interleave in three dif-
ferent ways [5]: (a) t′ precedes t in all partitions, in which
case t′ can be trivially serialized before t; (b) t′ precedes t
in partition p and they are concurrent in p′, in which case
the certiﬁcation test at p′ guarantees that t′ can be serialized
before t in p′; and (c) t′ and t are concurrent in p and p′,
in which case the certiﬁcation test at p and p′ ensures that
they can be serialized in any order at every partition. Since
a transaction only commits in a partition after it receives the
votes from all other involved partitions, it is impossible for
t′ to precede t in p and for t to precede t′ in p′.
A global transaction t that reaches the head of the pending
list (lines 26–27) can only be completed at server s if (a) s
received votes from all partitions involved in t (line 28)
2) The correctness of delaying transactions: Delaying the
broadcast of a global transaction t in a partition may delay
the delivery of t at p but this does not change the correctness
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 2 Geo-SDUR, server s’s code in partition p
1: Initialization:
2:
3:
4:
5:
6:
DB ← [. . .]
PL ← [. . .]
SC ← 0
DC ← 0
VOTES ← ∅
7: when receive(read, k, st) from c
8:
9:
10:
if st = ⊥ then st ← SC
retrieve(k, v, st) from database
send(k, v, st) to c
11: when receive(commit, t)
12:
submit(t)
{list of applied transactions}
{list of pending transactions}
{snapshot counter}
{delivered transactions counter}
{votes for global transactions}
{if ﬁrst read, init snapshot}
{most recent version ≤ st}
{return result to client}
{see line 41}
13: when receive(tid, v) from partition p
VOTES ← VOTES ∪ (tid, p, v)
14:
{one more vote for tid}
15: when adeliver(c, t)
DC ← DC + 1
16:
t.rt ← DC + ReorderThreshold
17:
v ← reorder(t)
18:
if v = abort then
19:
complete(t, v)
20:
if t is global then
21:
22:
{one more transaction delivered}
{set t’s Reorder Threshold}
{see line 48}
{reordering resulted in abort?}
{see line 34}
send(t.id, v) to all servers in partitions(t)
{send votes}
23: when head(PL) is local
24:
25:
t.rt = DC
{get head without removing entry}
{see line 34}
outcome ← commit
if (t.id, ⋆, abort) ∈ VOTES then
t ← head(PL)
complete(t, commit)
26: when head(PL) is global
(c, t, v) ← head(PL)
{get head without removing entry}
27:
if ∀k s.t t.st[k] '= ⊥ : (t.id, k, ⋆) ∈ VOTES and {has all votes?}
28:
{and t reached threshold?}
29:
{a priori commit, but...}
30:
{one abort vote and...}
31:
{t will be aborted}
32:
{see line 34}
33:
{used in lines 20, 25, 33}
{remove t from PL}
{if t commits...}
{apply changes}
{create next snapshot and...}
{...expose snapshot to clients}
34: function complete(t, outcome)
35:
36:
37:
38:
39:
40:
apply t.ws with version SC to database
DB [SC + 1] ← t
SC ← SC + 1
PL ← PL ⊖ t
if outcome = commit then
outcome ← abort
complete(t, outcome)
send(outcome) to client of t
let P be partitions(t) \ {p}
for all x ∈ P : abcast(x, t)
41: procedure submit(t):
{broadcast t to each...}
42:
{...remote partition}
43:
44: ∆ ← max ({delay(x, p) | x ∈ P }) {determine maximum delay}
{delay local broadcast}
45:
46: function ctest(t, t′):
47:
(t.rs ∩ t′.ws = ∅) ∧ (t is local ∨ (t.ws ∩ t′.rs = ∅))
abcast(p, t) after ∆ time units
if ∃t′ ∈ DB [t.st[p] . . . SC ] : ctest(t, t′) = false then
{t aborts if conﬂicts with committed t′}
if t is global then
else
return abort
48: function reorder(t):
49:
50:
51:
52:
53:
54:
55:
56:
57:
58:
59:
60:
61:
62:
63:
64:
if ∃t′ ∈ PL : ctest(t, t′) = false then return abort
append t to PL
{include t in pending list if no conﬂicts}
let i be the smallest integer, if any, such that
∀k < i : PL[k].ws ∩ t.rs = ∅ and
{t’s reads are not stale}
∀k ≥ i : (PL[k] is global and {no leaping local transactions}
PL[k].rt < DC and {no leaping globals after threshold}
t.ws ∩ PL[k].rs = ∅ and
{previous votes still valid}
t.rs ∩ PL[k].ws = ∅)
{ditto!}
if no i satisﬁes the conditions above then return abort
for k from size(PL) downto i do PL[k + 1] ← PL[k]
PL[i] ← t
return commit
{after making room (above), insert t}
{t is a completed transaction!}
of the protocol. To see why, notice that since we assume an
asynchronous system, even if t is broadcast to all partitions
at the same time, it may be that due to network delays t is
delivered at any arbitrary time in the future.
3) The correctness of reordering transactions: Consider
a local transaction t, delivered after global transaction t′ at
partition p. We claim that (a) if server s in p reorders t and
t′, then every correct server s′ in p also reorders t and t′; and
(b) the reordering of t and t′ does not violate serializability.
For case (a) above, from Algorithm 2, the reordering of a
local transaction t (lines 48–64) is a deterministic procedure
that depends on DB [t.st[p]..SC ] (line 49), PL (lines 56–
60), and DC (line 58). We show next that DB , PL, SC
and DC are only modiﬁed based on delivered transactions,
which sufﬁces to substantiate claim (a) since every server
in p delivers transactions in the same order, from the total
order property of atomic broadcast.
For an argument by induction, assume that up to the ﬁrst i
delivered transactions, DB , PL, SC and DC are the same at
every correct server in p (inductive hypothesis), and let t be
the (i + 1)-th delivered transaction (line 15). PL is possibly
modiﬁed in the reorder procedure (line 63) and from the
discussion above depends on DB , PL, SC and DC , which
together with the induction hypothesis we conclude that it
happens deterministically. DB , PL and SC are also possibly
modiﬁed in the complete procedure (lines 34–40), called
(i) after t is delivered (line 20), (ii) when the head of PL is
a local transaction (line 25), and (iii) when the head of PL
is a global transaction u (line 33).
In cases (i) and (ii), since all modiﬁcations depend on t,
PL and SC , from a similar reasoning as above we conclude
that the changes are deterministic. In case (iii), the calling
of the complete procedure depends on receiving all votes
for t and t having reached its reorder threshold (lines 28
and 29). From the induction hypothesis, all servers agree
on the value of DC . Different servers in p may receive
u’s votes at different times but we will show that any two
servers s and s′ will nevertheless reorder t in the same way.
Assume that when s assesses u it already received all u’s
votes and proceeds to complete u before it tries to reorder
t. Another server s′ assesses u when it has not received
all votes and does not call the complete procedure. Thus,
s will not reorder t with respect to u. For a contradiction,
assume that s′ reorders t and u. From the reorder condition,
it follows that u has not reached its reorder threshold at
s′, which leads to a contradiction since u has reached its
threshold at s, from the algorithm (line 16) DC depends
only on delivered messages and from atomic broadcast all
servers deliver the same transactions in the same order.
Finally, to see that reordering transactions does not violate
serializability, note that the condition for local transaction
t to be placed before global
is that both
transactions would be committed if t had been delivered
before t′. Since t′ passes certiﬁcation, its readset and writeset
transaction t′
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
do not intersect the readsets and writesets of concurrent
transactions delivered before. Thus, in order for t to be re-
ordered before t′, t’s readset and writeset must not intersect
t′’s readset and writeset (lines 59–60). Moreover, t’s readset