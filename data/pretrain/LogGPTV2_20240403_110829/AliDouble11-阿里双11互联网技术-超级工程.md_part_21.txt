万个应用，集团内部在线业务100%都做Pouch化。
Pouch的技术演化过程
2011年以前集团主要是使用物理机或基于xen的虚拟机。随着应用从大到小的
拆分以及对提高资源利用率的追求，物理机或虚拟机不再能满足我们的要求，Pouch
的雏型t4应运而生，到2015年中交易大部分应用都已经切换到Pouch上面，这
时候开源的容器技术Docker在开发者社区已经很火热了，很多开发都在本地构建
docker镜像来模拟运行时环境，我们调研了docker的技术栈和生态后发现Docker
镜像技术正好能弥补Pouch在环境定义方面的不足，我们决定把Docker的镜像功
180 > 9年双11：互联网技术超级工程
能加入Pouch，这样开发人员在本地构建的镜像就可以直接在线上用Pouch运行起
来，保证了开发测试正式环境的一致性，运行时继续使用Pouch原来开发的lxc技
术。后来随着oci运行时规范的成熟我们把lxc包装成了符合oci运行时规范的一个
runtime实现。当然镜像技术也不是完美的，当全部应用都从发主包变成发镜像后，
随之来而的一个问题就是应用扩容或发布的时候需要下载的包大小从很小的主包，变
成了一个相对比较大的镜像，这样在批量的发布或扩容的时候镜像仓库的出口网络
会成为瓶颈，扩容或发布的速度也变得很慢，并且影响成功率。为了解决这个问题
Pouch在2016年中支持了使用蜻蜓做链式的镜像分发，不需要所有的内容都从中心
的镜像仓库下载。到2017年双11集团内部的所有活跃应用都已经Pouch化，为了
把这么多年容器方面的经验和技术回馈给社区，我们已经开始做Pouch的开源孵化，
预计到2018.3月底Pouch的第一个正式的开源版本发布。
Pouch如何支持双11大促
电商双11大促的场景对容器技术的考验主要在于几个方面：不同容器之间的隔
离性、离线进程和在线容器之间的隔离性都需要保证。大促期间很多应用都有上万个
实例，这些应用同时发布时拉取镜像的成功率和效率的保证也是对容器技术的一大
难题。为了和内部相关的系统集成容器中不能只启动用户的一个进程，大促相关的
流量调度、监控、日志以及与安全体系的集成都需要在用户的容器中启动一些辅助服
务，这就是Pouch支持的富容器技术的需求来源。同时大促期间我们能用到的宿主
机也是各式各样的，比如有长尾应用一直在用的物理机，有新采购的物理机，也有从
阿里云买到的ECS，这些物理机上面的系统和内核版本也都不尽相同，这样就要求
Pouch要支持所有可用的系统版本和内核版本，内核版本从2.6.32到3.10，alios
版本从5u到7u都需要支持。大规模的使用场景中经常会有物理机挂掉，为了不影
响服务，一个容器挂了后会调度编排系统会立即再扩容一个做为补充，这种方式对于
无状态的应用是适用的，但是如果应用在磁盘上面存储了必要的数据就没办法这么替
换了，Pouch配合盘古分布式块存储支持申请存储和计算分离的容器，这样即使物
理挂掉了，这个容器在另一个物理机上面拉起后还能继续访问以前保存的数据。
新基础  9年双11：互联网技术超级工程
才为现实，所以Pouch也借鉴了镜像的概念。
镜像化之前我们只需要分发应用的代码编译出来的主包，现在是需要分发整个镜
像。借助于镜像分层的特性，如果运行环境不经常变化，好的Dockerfile规范可以
让镜像分发达到和分发主包一样的效果。但是依赖环境的变化会让这种情况恶化，特
别是很多应用的部署并发度上千，并且会部署在不同的机房、不同的地域、不同的大
陆。为了提升镜像下载的效率，Pouch支持了多种p2p镜像加速插件，包括蜻蜓、
minion等。不同的机房都搭建了超节节点来加速机房之间的传输。对于有盘古的机
房Pouch还支持使用盘古远程盘来做镜像，这样任何一个镜像每个机房都只需要下
载一次，所有使用到这个镜像的机器都是挂载同一份只读数据盘。
镜像化的另一个特点是每次应用发布都需要重启容器，对于一些静态资源的发
布不太友好，Pouch还支持了对镜像的热更新，热更新的时候会比较两个镜像之间
是不是只更新了指定的静态资源，否则就还是全量更新。这样就在保持镜像作为应用
版本的唯一标识的情况下支持了运行时的patch更新。
富容器模式
容器社区提倡的一个容器一个进程的使用方式，对于微服务或新的应用比较合
适，但是对于有上万个应用需要迁移到容器来还强制这种方式就有点无法实现了，为
了让所有应用都能无缝的切换过来，我们还是在容器中用/sbin/init来拉起运维依赖
的系统服务，最后切换到admin用户来拉起用户指定的Entrypoint。这样做的好处
新基础  9年双11：互联网技术超级工程
支持计算存储分离
Pouch实现了将容器的计算和存储资源进行分离，利用共享存储的作为容器存
储，可实现有状态的应用容器的快速迁移，可实现无状态应用的日志数据快速的做离
线大数据分析。利用通用的volume存储管理模块，可快速实现包括本地数据盘，内
存盘，分布式存储的远程盘（阿里云盘古分布式存储）在容器中使用的切换能力，配
合存储管理的调度系统，能够更灵活的利用存储资源。能有效的解决服务器存储资源
不足或者浪费的场景问题。
以上就是我们Pouch容器在阿里内部的演进过程以及对双11这种大规模使用
场景下所做的特殊支持，既支持现下最流行的容器使用方法，又兼顾使用习惯、隔离
稳定性、性能这些方面的场景。
新基础  9年双11：互联网技术超级工程
用的规模开始破万，发布失败率开始增高，而根本原因就是发布过程需要大量的文件
拉取，文件服务器扛不住大量的请求，当然很容易想到服务器扩容，可是扩容后又发
现后端存储成为瓶颈。此外，大量来自不同IDC的客户端请求消耗了巨大的网络带
宽，造成网络拥堵。
同时，很多业务走向国际化，大量的应用部署在海外，海外服务器下载要回源国
内，浪费了大量的国际带宽，而且还很慢；如果传输大文件，网络环境差，失败的话
又得重来一遍，效率极低。
于是很自然的就想到了P2P技术，因为P2P技术并不新鲜，当时也调研了很多
国内外的系统，但是调研的结论是这些系统的规模和稳定性都无法达到我们的期望。
所以就有了蜻蜓这个产品。
设计目标
针对这些痛点，蜻蜓在设计之初定了几个目标：
1.解决文件源被打爆的问题，在Host之间组P2P网，缓解文件服务器压力，
节约跨IDC之间的网络带宽资源。
2.加速文件分发速度，并且保证上万服务器同时下载，跟一台服务器下载没有太
大的波动。
3.解决跨国下载加速和带宽节约。
4.解决大文件下载问题，同时必须要支持断点续传。
5. Host上的磁盘IO，网络IO必须可以被控制，以避免对业务造成影响。
新基础 < 187
系统架构
蜻蜓整体架构
蜻蜓整体架构分三层：第一层是Config Service，他管理所有的Cluster
Manager，Cluster Manager又管理所有的Host， Host就是终端，dfget就是类似
wget的一个客户端程序。
Config Service主要负责Cluster Manager的管理、客户端节点路由、系统配
置管理以及预热服务等等。简单的说，就是负责告诉Host，离他最近的一组Cluster
Manager的地址列表，并定期维护和更新这份列表，使Host总能找到离他最近的
Cluster Manager。
Cluster Manager主要的职责有两个：