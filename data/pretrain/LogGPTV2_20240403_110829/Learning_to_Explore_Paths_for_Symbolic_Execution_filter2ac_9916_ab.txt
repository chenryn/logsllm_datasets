strateдy, a state selection strategy.
Output:tests, a set of generated test cases.
tests ← emptySet();
states ← emptyList()
states.append(proд.initialState)
update(proд.initialState, strateдy)
while states.size > 0 and !TIMEOUT do
state ← selectState(states, strateдy)
while state.inst (cid:44) EXIT and !state.foundViolation and
state.inst (cid:44) FORK do
executeInstruction(state)
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
if state.inst = EXIT or state.foundViolation then
tests.add(generateTest(state))
states.remove(state)
// state.inst = FORK
else
f orked ← doFork(state)
states.append(f orked)
update(state, strateдy)
update(f orked, strateдy)
return tests
3.2 State Selection Strategy
Next, we formally define a state selection strategy.
Definition 1 (State selection strategy). A state selection strategy is a
mapping from symbolic states to real value scores that measure the
importance of the states for exploration. To apply a state selection
strategy in symExec, we need two auxiliary functions described at a
high level below:
• selectState: The inputs of selectState are a list of pending sym-
bolic states states and a state selection strategy strateдy. Each
time invoked (Line 6 of Algorithm 1), selectState leverages the
importance scores returned by strateдy to select a state from
states for the next exploration step. selectState can be determin-
istic or probabilistic, e.g., normalizing the scores into a probability
distribution and drawing a sample from the distribution.
• update: When a new pending state is added (Lines 4 and 16 of
Algorithm 1) or a currently pending state enters a new branch
(Line 15 of Algorithm 1), update is called to update the internal
mechanics of strateдy for computing the importance scores and
also the scores themselves.
The detailed implementation of selectState and update depends
on each specific strategy. Next, we provide the depth-first search
(DFS) strategy in KLEE [16] as an example.
Example 1. The DFS strategy always selects the state representing
the deepest path before exploring other paths.
• strategy: maps each pending state to its depth, i.e., the number
of forks executed for the path that the state explores.
• selectState: selects the state with the largest depth.
• update: updates the depth of the input state.
Algorithm 2: Learch’s update function
1 Procedure update(state, strateдy)
Input:state, the state to update.
strateдy, the Learch strategy.
state.feature ← extractFeature(state)
reward ← strateдy.predict(state.feature)
strateдy.setReward(state, reward)
2
3
4
Ideal objective of a state selection strategy.
In symExec, a se-
lected state can produce different new states and finally different
tests, depending on the program logic and subsequent selection
decisions. Ideally, at selectState, we would want to consider the
overall effect of each pending state (i.e., the states and tests pro-
duced from the state) and select states leading to tests that not only
achieve higher coverage but also cost less time to obtain, such that
symExec’s objective in Equation 1 is achieved. This criterion can be
summarized in the reward function defined as:
(cid:12)(cid:12)
t ∈testsFrom(state) coverage(t)(cid:12)(cid:12)

reward(state) =
d ∈statesFrom(state) stateTime(d)
(2)
where testsFrom(state) and statesFrom(state) return the set of tests
and the set of symbolic states originating from state, respectively.
stateTime(d) returns the time spent on state d, including execution
time, constraint solving time, etc. Intuitively, reward measures state
by the total amount of coverage achieved by the tests involving
state divided by the total amount of time symExec spends on state
and the states produced from state. Then, an ideal strategy would
always select the state with the highest reward.
However, it is hard to exactly compute reward at each selectState
step, because the states and the tests produced from state depend
on future selections that are unknown at the current step. That is,
we usually cannot calculate testsFrom(state) and statesFrom(state)
ahead of time before symExec finishes. Due to this limitation, existing
heuristics [16, 48] typically compute importance scores for states
based on a certain manually designed property, as a proxy for reward.
As a result, they often get stuck at certain program parts and cannot
achieve high coverage.
3.3 A Learned State Selection Strategy: Learch
Now we introduce the Learch strategy. The core component of
Learch is a machine learning regression model φ : Rn → R learned
to estimate the reward in Equation 2 for each pending state. To
achieve this, Learch extracts a vector of n features for the input
state with a function called extractFeature and invokes φ on the
n-dimensional features. The choices of the features and φ are dis-
cussed in Section 5.1.
The selectState function of Learch greedily selects the state
with the highest estimated reward, i.e.:
state = arg maxs∈states strateдy.getReward(s).
We also considered probabilistic sampling but found that the greedy
one performed better. Learch’s update function is presented in
Algorithm 2. At Line 2, we call extractFeature to extract the features
for the input state. At Line 3, we leverage φ to predict a reward for
the state. Then at Line 4, the learned strategy updates the reward
Session 10A: Crypto, Symbols and Obfuscation CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2529a
b
d
e
c
f
g
states
a0, c0, f0, g0
a0, c0, f0, c1, f1, g1
a0, b0, d0
1
2
3
cov
a, c, f, g
a, c, f, g
a, b, d
(b) Example tests generated by symExec.
4
g0
a0 c0 f0 g0 c1 f1 g1 b0 d0
1
2
2
2
2
2
1
1
2
b0
2
d0
a0
c1
f1
g1
c0
f0
0
(a) An example CFG.
(c) Time spent by each state in seconds.
(d) A tests tree.
state
a0
c0
f0
g0
c1
f1
g1
b0
d0
total_cov
6
4
4
4
0
0
0
2
2
total_time
15
10
8
2
4
3
2
4
2
reward
0.4
0.4
0.5
2
0
0
0
0.5
1
(e) Reward for the states.
Figure 4: An example on assigning a reward to explored states of the tests generated by symExec.
of the state. Note that the expensive feature extraction and reward
prediction are done once per update. In selectState, we only read
the predicted rewards, avoiding unnecessary re-computations.
Benefits of a learned strategy. Different from existing heuris-
tics [16, 48], the Learch strategy makes decisions based on multiple
high-level features (including the ones from the heuristics) and di-
rectly estimates Equation 2 to optimize for Equation 1. Therefore,
Learch can effectively explore the input program and rarely gets
stuck. As a result, Learch achieves higher coverage and detects
more security violations than manually designed heuristics.
4 LEARNING STATE SELECTION STRATEGIES
While a learned strategy can be effective, it is non-obvious how
to learn the desired strategy. This is because a supervised dataset
consisting of explored states and their ground-truth reward for
training the machine learning model φ is not explicitly available.
Next, we introduce techniques for extracting such a supervised
dataset from the tests generated by symExec, from which φ can be
obtained with off-the-shelf learning algorithms.
4.1 Assigning a Reward to Explored States
Given a set of training programs, we run symExec to obtain a set of
tests where each test consists of a list of explored states and covers
certain code. From the tests, we construct a novel representation of
the tests, called tests trees, whose nodes are the explored states, and
leverage the trees to calculate a reward for the explored states. Note
that the calculation of Equation 2 is feasible during training because
symExec has already finished for the training programs. Finally, a
supervised dataset is built for training φ. Next, we describe how to
achieve this step by step.
First, we formally define tests in our context.
Definition 2 (Test). A test generated by symExec for an input pro-
gram is a tuple (states, input, cov). states is a list of symbolic states
[state0, state1, ..., staten] selected by selectState at Line 6 of Algo-
rithm 1. Each state represents a explored branch and the branch
represented by statei is followed by the branch represented by
statei +1 in the control flow graph of the program. Therefore, states
indicates the program path induced by test. input is a concrete
input for the input program and is constructed by solving the path
constraints of staten with a constraint solver. The concrete exe-
cution of input follows the path indicated by states and achieves
coverage cov.
Example 2. In Figure 4(a), we show the control flow graph (CFG)
of an example program. The CFG consists of seven basic blocks
and seven edges, and the edges between nodes f and c represent a
loop. We symbolically execute the example program and generate
three tests shown in Figure 4(b). Test 1 and 2 execute the loop once
and twice, respectively, both covering block a, c, f, and g (i.e., the
results of the coverage function in Equations 1 and 2). Test 3 does not
execute the loop but explores states a0, b0, and d0, covering blocks a,
b, and d. Note that for the examples, we show basic block coverage
for simplicity. In our implementation, we used line coverage. We
record the time spent by symExec on each state (i.e., the results of
the stateTime function in Equation 2) in Figure 4(c).
After generating tests for a training program, we construct a
j and statei
j+1 in the tree.
j as the parent of statei
tests tree defined in the following.
Definition 3 (Tests tree). Given a series of tests [test0, test1, ...,
testm] for a program, we construct a tests tree whose nodes are
the explored states of the tests (i.e., those in the states field). For
each testi, we go over all pairs of states statei
j+1 and set
statei
Example 3. In Figure 4(d), we show a tests tree constructed from
the tests in Figure 4(b). Each tree path from the root to a leaf
corresponds to the explored states of a test. For example, the left-
most path a0–c0–f0–g0 consists of the states of test 1. At the left-
hand side of each leaf node, we annotate the number of new blocks
covered by the corresponding test. For instance, test 1 covers four
new blocks: a, c, f, and g. Then, test 2 does not yield new coverage
because the four blocks covered by test 2 were already covered by
test 1 before. Test 3 covers two new blocks: b and d.
The tests tree representation recovers the hierarchy of the ex-
plored states and provides a structure for conveniently calculating
testsFrom(state) and statesFrom(state) by considering the descen-
dants and the paths of each explored state, respectively. As a result,
the reward can be efficiently computed.
Calculate a reward for explored states. To calculate a reward
(Equation 2) for each state, we need to calculate the numerator,
i.e., the total coverage achieved by all tests involving state, and the
denominator, i.e., the total amount of time spent by state and its
descendants. We compute those information with the tests trees in
a bottom-up recursive fashion.
To compute the numerator totalCov for each state, we compute
the coverage achieved by the tests involving state. This is equal
Session 10A: Crypto, Symbols and Obfuscation CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2530Algorithm 3: Generating a supervised dataset
1 Procedure genData(proдs, strateдies)
Input
:proдs, a set of training programs.
strateдies, a set of state selection strategies.
Output:dataset, a supervised dataset.
dataset ← emptySet()
for strateдy in strateдies do
for proд in proдs do
tests ← symExec(proд, strateдy)
newData ← dataFromTests(tests)
dataset ← dataset ∪ newData
return dataset
2
3
4
5
6
7
8
Algorithm 4: Iterative learning
1 Procedure iterLearn(proдs, strateдies, N )
Input
:proдs, a set of training programs.
strateдies, a set of manual heuristics.
N , the number of training iterations.
Output:learned, a set of learned strategies.
dataset ← emptySet();
learned ← emptySet()
for i ← 1 to N do
newData ← genData(proдs, strateдies)
dataset ← dataset ∪ newData
newStrateдy ← trainStrategy(dataset)
learned.add(newStrateдy)
strateдies ← {newStrateдy}
return learned
2
3
4
5
6
7
8
9
(cid:40)newCov(state)

to summing up the new coverage (newCov) of all the leaves that are
descendants of state in the tests trees and can be done in a recursive
way as follows:
totalCov(state) =
if state is a leaf,
otherwise.
To compute the denominator totalTime, we sum up the time spent
on the considered state and its descendants via the following recur-
sive equation:
c∈children(state) totalCov(c)
totalTime(state) = stateTime(state) +
c∈children(s) totalTime(c)
Then the reward can be computed by reward(state) = totalCov(state)
totalTime(state).
Example 4. For each state in Figure 4(d), we compute totalTime,
totalCov, and reward in Figure 4(e).
4.2 Strategy Learning Algorithms
We now present the final algorithms for learning Learch.
Generate a supervised dataset.
In Algorithm 3, we present
a procedure named genData for generating a supervised dataset.
The inputs of genData are a set of training programs proдs and a
set of state selection strategies. First, at Line 2, we initialize the
supervised dataset (dataset) to an empty set. Then, for each strategy
in strateдies and each program in proдs (the loops from Line 3 to
Line 7), we run symExec to generate a set of tests tests (Line 5). Next,
at Line 6, a new supervised dataset is extracted from the tests with
the techniques described in Section 4.1. The new dataset is added
to dataset (Line 7). After the loops finish, dataset is returned.
Iterative learning for producing multiple learned strategies.
While a single learned strategy is already more effective than exist-
ing heuristics [16, 48], we found that using multiple models during
inference time can improve the tests generated by symExec even
more (a form of ensemble learning). This is because the space of
symbolic states is exponentially large and multiple strategies can
explore a more diverse set of states than a single strategy. We pro-
pose an iterative algorithm called iterLearn in Algorithm 4 that
trains multiple strategies. To incorporate the knowledge of existing
heuristics into Learch, we treat them as an input (strateдies) to
iterLearn and leverage them in the data generation process.
iterLearn first initializes a supervised dataset dataset and a set
of learned strategies learned to empty sets (Line 2). Then it starts
a loop from Line 3 to Line 8 with N iterations. For each iteration,
genData (Algorithm 3) is called to generate new supervised data
using strateдies (Line 4) and the new data is added to dataset
(Line 5). Then, a new strategy newStrateдy is trained at Line 6. To
achieve this, we run an off-the-shelf learning algorithm on dataset,
represented by the trainStrategy function. Then, newStrateдy is
added to learned (Line 7). At Line 8, we assign newStrateдy as
the only element in strateдies. This indicates that genData is called
with the manual heuristics only at the first loop iteration. After the
first loop iteration, the learned strategy obtained from the previous
iteration is used to generate new supervised data. After the loop
finishes, we return the N learned strategies.
Note that our learning pipeline is general and can be extended to
optimizing for other objectives (e.g.., detecting heap errors) just by
choosing an appropriate reward function (e.g., the number of heap
access visited during execution) and designing indicative features.
Moreover, Learch employs an offline learning scheme, i.e., training
is done beforehand and the learned strategies are not modified at
inference time. One natural future work item is to extend Learch
with online learning where we utilize already explored states of the