2.1M
2.5M
2.1M
1.2M
# Te. Smp.
36.0K
96.0K
168.0K
201.6K
168.0K
96.0K
i and 𝒞i classifiers. “# Tr. Smp.” and “# Te. Smp.” represent the number of training and test samples, respectively
Figure 10: Complexity and performance of 𝒞q
(same number for both 𝒞q
i and 𝒞i ). “# Pram.” represents the number of trainable parameters in DNN.
The performance of different 𝒞i is given in Figure 10. Notably,
the accuracy increases with the size of the tuples. But, note the
false negative rate remains more or less unchanged.
Effect of quality values. Having access to the quality values
consistently increase the accuracy. We, therefore, remove quality
values before storing templates in ℱ. Fortunately, Bozorth3 uses
the quality values only to order the minutiae points. Therefore, in
our prototype implementation, we decide to remove quality values
and store ordered minutiae points only. As some other matching
algorithm might take advantage of the quality values, we conser-
vatively consider the classifiers with quality values for evaluating
attacks using adversarial classifiers.
7 SECURITY EVALUATION USING
ADVERSARIAL CLASSIFIERS
Using the adversarial classifiers described in Section 6, an attacker
can expedite the recovery of a valid fingerprint tuple. In this sec-
tion, we will explore a few different attack strategies and their
performance by simulating them on SD09 dataset.
One limitation of our attack simulation is that it uses the same
data used for training the classifiers to instantiate TenSketch data-
base. This was unavoidable due to the lack of availability of a larger
tenprint dataset. However, we emphasize that as shown in Section 6,
the training and testing accuracy of the classifiers are very similar,
we anticipate the simulation results will hold for larger datasets
too. Moreover, the simulation with the training data benefits the
attackers and provides a conservative bound of security.
Recall, we want to determine the complexity of the computation
work an attacker has to do to win the TarGuess game (See Figure 3.)
The attacker’s goal here is to find a valid tenprint of a user u given
b = 1 of the user’s fingerprints. The attacker only needs to find a
message tuple where at least t fingerprints are correct — belongs to
the user u. Using such a tuple and Rec∆
t , the attacker can recover
all the fingerprint templates of the user.
We will assign a cost of cc and cs as the cost of calling the
classifier and the cost of calling the Rec∆
t procedure on a fingerprint
tuple. Every other computation, conservatively, is assumed to be
free for the attacker. In general, cc will be much smaller than cs
(but not negligible). For ease of reporting the results, we will often
assume cs ≫ cc.
If there were no correlation between fingerprints the optimal
strategy of an attacker would be to apply Rec∆
t on all possible
message tuple of size t. Therefore, the cost of such naive attack
(cid:1) = N t−b · cs. (The attacker only
(cid:1) many valid tuples.) For a TenSketch with
N = 2, 500 users and t = 7, this will result in 6×log 2500 = 67.7-bits
of security (assuming applying Rec∆
t is of unit cost). We define the
advantage of an classifier-based attack algorithm as log(C/CML),
where CML is the cost of the attack using adversarial classifiers.
The advantage of an attack represents the loss in number of bits of
security due to correlation among fingerprints of a user as captured
by the adversarial classifiers. Below, we will report on both the cost
and the advantage of each attack.
There are many different ways an attacker can leverage classifiers
to identify t-tuples. In Appendix F we consider in detail the strategy
t−b
will be C =(cid:0)n−b
have to find one of(cid:0)n−b
(cid:1) · N t−b · cs/(cid:0)n−b
t−b
t−b
0.00.20.40.60.81.0FPR0.00.20.40.60.81.0TPRROCCq2Cq3Cq4Cq5Cq6Cq70.00.20.40.60.81.0FPR0.00.20.40.60.81.0TPRROCC2C3C4C5C6C70.00.20.40.60.81.0FPR0.00.20.40.60.81.0TPRROCC2Cq2C4Cq4C7Cq7t−b
t−b
i s incrementally.
of combining different classifiers, and show that using 𝒞7 alone
incurs the lowest computational overhead. Our first three strategies
below use 𝒞7. The last strategy we consider identifies t-tuples using
different 𝒞′
1. Pruning improbable candidates. An obvious strategy to find
a correct message tuple of size t would be to prune out improbable
tuples using the classifier 𝒞t , and thereby, to reduce the number
of tuples to check using Rec∆
t . However, this strategy requires the
attacker to apply the classifier on all possible(cid:0)n−b
(cid:1) · N t−b message
(cid:1) · N t−b (cc + cs · ν) ≥
the total cost of this attack will be ˆC =(cid:0)n−b
(cid:1) · N t−b · cs · ν = C · ν. The advantage of this attack is at
(cid:0)n−b
tuples. This number can be very large, posing a serious engineering
challenge for the attacker. Moreover, as shown in Section 6, the clas-
sifier will have some non-zero false positive rate, say ν. Therefore,
t−b
most − log ν. As per our experiment in Section 6.2, without the
quality values ν = 0.1 at true positive rate of 90%, which trans-
lates to 3.3-bits of security loss. With the quality values, ν = 0.05
(See Figure 9), and the security loss would be 4.3 bits.
The true positive rate of the classifier dictates the attacker’s
chance of winning the game should she be able to finish going
over all the message tuple combinations. However, the attacker
can forego higher accuracy at the cost of faster retrieval. As we
show in Figure 9, reducing the true positive rate does not result in
a sufficiently low false positive rates, and will not be effective.
2. Ordering message tuples based on the classifier. An at-
tacker can try to order the tuples using the classifier outputs (confi-
dence values), and check using Rec∆
in decreasing order of their
t
confidence values. To generate high confidence — according to
the classifiers — message tuples, the attacker can use hill-climbing
approach as described below, or an approach similar to generating
passwords from a Markov model [47]. However, for any of these
approaches to succeed, the correct message tuple must be assigned
high confidence by the classifier.
To verify that, we experimented to find the average rank of a
correct tuple in a list of randomly sampled tuples. We pick a random
user u, and pick t fingerprints of the user u randomly, containing
b known fingerprints. We also pick 104 fingerprint tuples from
ℱ each containing the b known fingerprints and rest randomly
sampled. We found the rank of the correct tuple w is at a = 12% of
the length of the list, on an average. That means even if the attacker
manages to order all the fingerprint tuples, she has to go through
on an average 12% of the list before encountering a valid message
tuple.
Assuming, if an attacker first prune the database of the improba-
ble candidate tuples (very low classifier’s outputs), and then sort
t on C ·ν ·a
the list based on the output of 𝒞7, will have to apply Rec∆
many fingerprint tuples. This translates to − log2(0.05×0.12) = 7.4-
bits of security loss due to correlation among fingerprints.
3. Finding high confidence message tuples. To avoid comput-
ing all possible message combinations as before, the attacker can
try to find message tuples for which the classifier assigns high con-
fidence value. This can be done efficiently by using an approach
similar to hill-climbing used in optimization techniques. The at-
tacker starts with a random fingerprint tuple of size t (that includes
˜w ←$ ℱ
Global input: s = (ℐ, ℱ), t
HillClimb(u, w):
s ← ℐ[u];
if w (cid:44) ∅ then ˜w1. . .|w| ← w
Y ← { ˜w}
/* Done tuples */
xmax ← ∞ ; x ← 0
while xmax ≥ x do
X ← X ∪ {w ← ˜w; wi ← w(cid:12)(cid:12) w ∈ ℱi }
˜w ← argmax{𝒞t(w)(cid:12)(cid:12) w ∈ X }
x ← 𝒞t( ˜w); X ← ∅
for i ∈ {|w| + 1, . . . , n}
X ← X \ Y ; Y ← Y ∪ X
xmax ← 𝒞t( ˜w)
return ˜w
Figure 11: Hill-climbing algorithm to find high confidence fingerprint
tuple for attacking TenSketch. The algorithm also takes w, which is b known
fingerprints of the user.
b known fingerprints) and replaces the unknown fingerprints one
at a time in such a way that improves the output of the classifier 𝒞t .
The replacement procedure stops at a tuple w if no other replace-
ment is possible that yields higher score than w. The attacker tests
w with Rec∆
for correctness. If it fails, it restarts with another
t
random tuple and continues. The algorithm is given in Figure 11.
We simulated this attack, but even with 1012 iterations (restarts),
the algorithm failed to find a correct t-tuple. On a closer look at
the classifier outputs, we found that the correct tuple never obtains
the highest classifier’s confidence among its neighboring tuples
— tuples that differ by only one fingerprint; which is the major
reason for the failure of this attack. Therefore, hill-climbing is not
an effective way for the attacker to take advantage of the adversarial
classifiers.
4. Generate message tuples incrementally. Finally, we con-
sider generating a t-size message tuple incrementally beginning
with b known fingerprints. The attacker uses the classifier 𝒞i to find
a i-tuple including the b known messages that have high confidence
and move on to find a (i + 1)-tuple using 𝒞i +1, (i + 2)-tuple using
𝒞i +2, and so on. The algorithm is given in Figure 12.
For each classifier 𝒞i, we also specify a threshold τi and consider
the tuples with higher confidence values than τi. These thresholds
are computed in such a way that ensures 𝒞i has a true positive
rate of 90%. We fix the true positive rates instead of false positive
rate, as failing to consider the correct tuple for further processing
immediately fails the attacker.
Now we want to compute the number of times the attacker has
to query Rec∆
t in this attack. Following the algorithm in Figure 12,
we can see, when the size of a partially constructed message tuple
w, such that |w| = j − 1 and j ≥ 2, the size of Tj is |Tj| = N · δj,
where N = |ℱj| and δj is as defined below.
| 𝒞j−1(w1, . . . , wj−1) > τj−1(cid:3) ,
δj = Pr(cid:2) 𝒞j(w1, . . . , wj) > τj
for j > 2; for j = 2, we define δ2 = Pr[𝒞2(w1, w2) > τ2 ]. Therefore,
total number of t-tuples generated by the algorithm is
t
j=b+1
|Tj| = N t−b ·
t
δj .
j=b+1
Global input: s = (ℐ, ℱ), t, (τ1, . . . , τt)
GenTupIncr(u, w):
s ← ℐ[u];
˜w ← ⊥
if w (cid:44) ∅ then X ← {w}; else X ← ℱ1
while X (cid:44) ∅ do
w ← X .pop()
j ← |w| + 1
if j >= t then
˜w ← Rec∆
if ˜w (cid:44) ⊥ then break
t (s, w)
else
Tj ←(cid:8)v ∈ ℱj
X ← X ∪(cid:8)w ∪ {v }(cid:12)(cid:12) v ∈ Tj
(cid:12)(cid:12)𝒞j(w1, . . . , wj−1, v) > τj
(cid:9)
(cid:9)
return ˜w
Figure 12: Algorithm for generating message tuples of size t incrementally.
ℱi is the database containing fingerprint template of the ith finger of each
user, and t is the minimum number of fingers required to recover the whole
tenprint set.
The cost of this attack will be
N t−b ·
t
j=b+1
δj · (cs + 2 · cc) ≈ C ·
t
j . We found thet
j=b+1
δj .
We computed δj’s for 2 ≤ j ≤ t empirically from the SD09 data
j=b+1 δj ≥ 2−9 with
and the classifiers 𝒞j and 𝒞q
quality values, and 2−8 without quality values. Therefore overall
loss in security via this attack is at most 9 bits.
Discussion. We explored possible attack strategies using generic
classifiers across all fingers. We show that an attacker can at most
reduce the security by 9 bits by exploiting the fingerprint corre-
lations. Final security of TenSketch with t = 7 and N = 2, 500
registered users in bit-strength is 58-bits, assuming the attacker
knows one of the fingerprints of the user.
We claim this is a conservative estimate, as we did not count
for the computational cost associated with applying the classifier.
Moreover, the loss in security can be further reduced by increasing
the hashing cost of H used by SS∆
t , and thus increasing cs.
The classifiers that we built is generic in a sense that they do
not consider positions of fingers. The classification accuracy might
improve if one tries to build finger-specific classifiers. Training
such classifiers however would require access to a large amount of
biometric training data.
t and Rec∆
8 RELATED WORK
Biometric authentication is increasingly being used during bor-
der crossing [6], to provide essential government services (such
as Aadhaar [3]), and for criminal investigation [5]. In addition to
traditional fingerprints and iris scans [26, 36], other physiological
attributes, such as ear [14] and hand geometry [55], palmprints [30],
heartbeat [54], and body temperature are potential to be used as
biometrics. For a detailed discussion on different potential “what
you are” biometrics, refer to [37]. Multisketches can be used to
protect a combination of different biometric attributes.
Biometric attributes are, however, noisy and change (slightly)
every time they are recorded. Therefore, unlike passwords, there is
no effective way of storing biometric templates securely. A number
of research works have looked into protecting biometric templates
using different techniques [39]. One of the most popular attempts