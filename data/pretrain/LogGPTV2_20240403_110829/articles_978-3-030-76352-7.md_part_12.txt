4. Sch¨onig,S.,Ackermann,L.,Jablonski,S.,Ermer,A.:Anintegratedarchitecturefor
IoT-aware business process execution. In: Enterprise, Business-Process and Infor-
mation Systems Modeling, pp. 19–34 (2018)
Data Interaction for IoT-Aware Wearable Process Management 71
5. Sch¨onig, S., Ackermann, L., Jablonski, S., Ermer, A.: IoT meets BPM: a bidirec-
tional communication architecture for IoT-aware process execution. Softw. Syst.
Model. 1–17 (2020)
6. Russell, N., ter Hofstede, A.H.M., Edmond, D., van der Aalst, W.M.P.: Workflow
data patterns: identification, representation and tool support. In: Delcambre, L.,
Kop, C., Mayr, H.C., Mylopoulos, J., Pastor, O. (eds.) ER 2005. LNCS, vol. 3716,
pp. 353–368. Springer, Heidelberg (2005). https://doi.org/10.1007/11568322 23
SiDD: The Situation-Aware Distributed
Deployment System
B
Ka´lm´an K´epes( ), Frank Leymann, Benjamin Weder, and Karoline Wild
Institute of Architecture of Application Systems, University of Stuttgart,
Stuttgart, Germany
{kepes,leymann,weder,wild}@iaas.uni-stuttgart.de
Abstract. Mostoftoday’sdeploymentautomationtechnologiesenable
the deployment of distributed applications in distributed environments,
whereby the deployment execution is centrally coordinated either by a
central orchestrator or a master in a distributed master-workers archi-
tectures. However, it is becoming increasingly important to support use
caseswhereseveralindependentpartnersareinvolved.Asaresult,decen-
tralized distributed deployment automation approaches are required,
since organizations typically do not provide access to their internal
infrastructure to the outside or leave control over application deploy-
ments to others. Moreover, the choice of partners can depend heavily
on the current situation at deployment time, e.g. the costs or availabil-
ity of resources. Thus, at deployment time it is decided which partner
will provide a certain part of the application depending on the situa-
tion.Totacklethesechallenges,wedemonstratethesituation-awaredis-
tributeddeployment(SiDD)systemasanextensionoftheOpenTOSCA
ecosystem.
· · ·
Keywords: Deployment Choreography Situation-aware system
TOSCA
1 Introduction and Motivation
Deploymenttechnologies enablereusableandportableapplication deployments,
making them key technologies for today’s application management. A variety
of technologies offer different capabilities and own domain-specific languages for
modeling deployments. Many use declarative deployment models in which the
desiredstateofanapplicationcanbespecifiedbyastructuraldescriptionofthe
application with its components and their relationships among each other.
However, in recent years, deployment automation has focused primarily on
centralized approaches, which allow the deployment of distributed applications,
butthedeploymentexecutioniscentrallycoordinatedeitherbyacentralorches-
trator or a master in distributed master-workers architectures. Especially in
industrialusecases,e.g.,inasupplychain,orifspecializedcomputeinfrastruc-
ture is required, e.g., in quantum computing, several partners are involved each
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.72–76,2021.
https://doi.org/10.1007/978-3-030-76352-7_11
SiDD: The Situation-Aware Distributed Deployment System 73
Fig.1.OverviewoftheSiDDconceptexemplarydepictingthepartnerselectionbased
on the available capacities of the partners (p2 and p3).
deploying a part of the overall application. Due to security concerns, organiza-
tions typically do not provide access to internal infrastructure to the outside or
leave control over application deployments to others. Thus, centralized deploy-
mentapproachescannotbeapplied.Moreover,theinvolvedpartnerscanchange
depending on certain conditions, e.g., costs or availability. Thus, (i) the mod-
eling of a distributed application involving multiple partners, (ii) the partner
selection during deployment, and (iii) the decentralized execution of the over-
all deployment has to be enabled. To tackle these challenges, we present the
Situation-aware Distributed Deployment (SiDD) system as an extension of the
OpenTOSCA ecosystem [1], an open-source toolchain for modeling and execut-
ingapplicationdeploymentsusingtheTopologyandOrchestrationSpecification
for Cloud Applications (TOSCA).
2 Exemplary Application Scenario and SiDD Concept
In previous work, a concept for the decentralized cross-organizational applica-
tion deployment automation [3] as well as the situation-aware management [2]
has been introduced. In this work, we demonstrate how these concepts can be
combined to enable the situation-aware partner selection for a distributed and
decentralized deployment based on an exemplary scenario as shown in Fig.1 on
the left: Three partners p1, p2, and p3 collaborate to run an application using
aquantumalgorithmandavisualizationwebapplicationcomponent.Toreduce
coststhepartnerssharetheirinfrastructure,p1providesaclassicalprivatecloud,
p2 a high-performance computing environment (HPCE) to run a quantum sim-
ulator, and p3 provides access to a quantum computer. At deployment time it
hastobedecidedhowthequantumalgorithmshallbeexecutedeitherusingthe
74 K. K´epes et al.
quantum computer of p3 or running a quantum simulator in the HPCE of p2.
This selection decision has to be made based on an availability policy, e.g., if
thereisnoavailabletimeslotonthequantumcomputer,thequantumsimulator
in the HPCE is selected and vice versa.
The described scenario can be partially automated with our SiDD concept
depicted in Fig.1. First, a so-called global deployment model (GDM) has to
be specified as a declarative deployment model that contains all application
components and their relationships (see left in Fig.1). This model only contains
the application components on which all partners have agreed upon and not
necessarily contain any infrastructure components such as virtual machines or
application servers. In addition, a GDM contains abstract components that are
replaced by concrete infrastructure components by each partner. Thus, in the
second step, each partner refines the GDM into a so-called local deployment
model (LDM) whichspecifiestheneededinfrastructurecomponentsoftheirown
infrastructure,asotherpartnersusuallydonothaveormustnothaveknowledge
about these. The refinement can be automated, e.g., using available refinement
fragments [4]. After all partners have defined their LDMs, in the third step, any
partner can initiate deployment by requesting the deployment engine to start a
deployment.Inthefourthstep,itisdecidedwhichpartnersareactuallyinvolved
in the deployment, e.g., in our scenario the infrastructure will be used from p1
and additionally either from p2, which can run a quantum simulator, or from
p3, which has access to a quantum computer. This is based on the annotated
policies, such as the availability policy in the GDM in Fig.1. For example, if
the quantum computer of p3 is not available at deployment time, a quantum
simulator can be deployed in the HPCE of p2 if there are enough resources
available. Finally, the selected partners are notified to start the deployment
of their components. For the deployment each partner generates a workflow
containingtaskstoinstallcomponentsaswellastaskstoexchangedatabetween
thepartners,e.g.,endpointinformationtoestablishaconnectiontocomponents
of other partners.
3 The SiDD System
The SiDD System is an extension of the OpenTOSCA ecosystem that con-
sists of Winery, a graphical TOSCA modeling tool, and OpenTOSCA con-
tainer, a TOSCA deployment engine [1] (see video at https://youtu.be/
A0JY9TW4ZFM).Thearchitectureofthesystemwiththerelevantcomponents
isshowninFig.2.Winery canbeusedtographicallymodeladeclarativedeploy-
mentmodelasaTOSCAtopologytemplate byusingdefinedtypesandattaching
the executable artifacts, e.g., a WAR for running a web application. Winery is
used to model the GDM, which is then passed to the involved partners (a). The
CSAR Importer/Exporter enables the export of a standardized Cloud Service
Archive (CSAR) that can be consumed by a TOSCA deployment engine. The
Substitution Mapping Component can be used to refine abstract components,
callednodetemplates,byconcreteonesusingrefinementfragments.Thisisused
SiDD: The Situation-Aware Distributed Deployment System 75
Fig.2. Architecture of the SiDD system exemplary shown with two partners.
byeachpartnertorefinetheabstractnodetemplatesintheGDMandtoobtain
the LGM that can be processed by the deployment engine OpenTOSCA con-
tainer (b). For deployment execution, a BPEL workflow is generated based on
thedeclarativedeploymentmodelbythePlan Builder (c).Theworkflowsofthe
partnersformachoreographybysendingandreceivingmessagestosharedeploy-
mentdata.ThePlanRuntime runstheplanwhenthedeploymentisinstantiated.
All operations that have to be executed and which are not provided as a service
run in the Operation Runtime. For the situation-aware selection, the required
information has to be provided by an external application which is then used
by the Situation Detector to determine which situations are active. When an
application is instantiated, the Situation-Aware Management Bus is responsible
for the partner selection based on the current situation (d) and to exchange
messages during deployment (e).
Acknowledgments. This work was partially funded by the DFG project DiStOPT
(252975529), the BMWi project PlanQK (01MK20005N), and the DFG’s Excellence
Initiative project SimTech (EXC 2075 - 390740016).
References
1. Breitenbu¨cher,U.,etal.:TheOpenTOSCAecosystem-concepts&tools.In:Euro-
peanSpaceprojectonSmartSystems,BigData,FutureInternet-TowardsServing
theGrandSocietalChallenges-Volume1:EPSRome2016,pp.112–130,December
2016
76 K. K´epes et al.
2. K´epes, K., et al.: Situation-aware management of cyber-physical systems. In: Pro-
ceedings of the 9th International Conference on Cloud Computing and Services
Science (CLOSER 2019). pp. 551–560. SciTePress, May 2019
3. Wild, K., Breitenbu¨cher, U., K´epes, K., Leymann, F., Weder, B.: Decentralized
cross-organizationalapplicationdeploymentautomation:anapproachforgenerating
deployment choreographies based on declarative deployment models. In: Dustdar,
S., Yu, E., Salinesi, C., Rieu, D., Pant, V. (eds.) CAiSE 2020. LNCS, vol. 12127,
pp. 20–35. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-49435-3 2
4. Wild, K., et al.: TOSCA4QC: two modeling styles for TOSCA to automate the
deploymentandorchestrationofquantumapplications.In:2020IEEE24thInterna-
tional Enterprise Distributed Object Computing Conference (EDOC). IEEE Com-
puter Society (2020)
AuraEN: Autonomous Resource
Allocation for Cloud-Hosted Data
Processing Pipelines
B
Sunil Singh Samant1( ), Mohan Baruwal Chhetri1,2, Quoc Bao Vo1,
Ryszard Kowalczyk1,3, and Surya Nepal2
1 Swinburne University of Technology, Melbourne, Australia
{ssamant,bvo,rkowalczyk}@swin.edu.au
2 CSIRO Data61, Sydney, Australia
{mohan.baruwalchhetri,surya.nepal}@data61.csiro.au
3 Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland
Abstract. Ensuring cost-effective end-to-end QoS in an IoT data pro-
cessingpipeline(DPP)isanon-trivialtask.Akeyfactorthataffectsthe
overall performance is the amount of computing resources allocated to
each service in the pipeline. In this demo paper, we present AuraEN,
an Autonomous resource allocation ENgine that can proactively scale
theresourcesofeachindividualserviceinthepipelineinresponsetopre-
dictedworkloadvariationssoastoensureend-to-endQoSwhileoptimiz-
ing the associated costs. We briefly describe the AuraEN system archi-
tecture and its implementation and demonstrate how it can be used to
manage the resources of a DPP hosted on the Amazon EC2 cloud.
· ·
Keywords: Data processing pipeline Resource optimization
· ·
End-to-end QoS Cloud resource orchestration Resource scaling
1 Introduction
In the IoT paradigm, objects or ‘things’ with sensing, effecting and communi-
cation capabilities generate massive volumes of data, mostly as streaming data.
This data is typically ingested, processed, and stored, before being consumed by
end-userapplications.Adataprocessingpipeline(DPP)isessentiallyacompos-
ite servicethat comprises specialisedatomic software servicesfor ingestion, pro-
cessing, and storage. Each of these atomic services can be fulfilled by a number
of different big data processing software platforms, e.g., Apache Kafka (https://
kafka.apache.org)andApachePulsar(https://pulsar.apache.org/)foringestion;
ApacheSpark(https://spark.apache.org/),ApacheStorm(http://storm.apache.
org/) and Apache Flink (https://flink.apache.org/) for processing; and Cassan-
dra (https://cassandra.apache.org/), MongoDB (https://www.mongodb.com/)
andHBase(https://hbase.apache.org/)forstorage.Duetotheseplatforms’dis-
tributed design and native support for horizontal scalability, cloud computing
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.77–80,2021.
https://doi.org/10.1007/978-3-030-76352-7_12
78 S. S. Samant et al.
is the default choice for running IoT DPPs. The cloud provides a scalable, elas-
tic, easily accessible and inexpensive way of meeting the processing needs of the
constituent services in a DPP, even under varying workloads.
IoT applications that provide real-time actionable insights based on the
streaming data have stringent QoS requirements that must be fulfilled at all
times, e.g., traffic accident detection. At the same time, cloud cost optimisa-
tion is a key challenge faced by most consumers of cloud infrastructure. There-
fore, a key research challenge related to the adaptive management of cloud
resources for IoT DPPs is to ensure cost-effective, end-to-end QoS fulfilment,
even under varying workload conditions.Theend-to-endQoSofaDPPdepends
upon the performance of its constituent software services; their performance, in
turn, depends upon several factors including their individual configurations, the
inter-dependenciesbetweenadjacentservices,theamountofcomputingresources
allocated to each service, and, the data ingestion rate. This makes autonomous
adaptive resource management for IoT DPPs a non-trivial task.
Wehavepreviouslyproposedasystematicapproachforbuildingasustainable
QoS profile for constituent services that can be used to inform resource alloca-
tiondecisionsforaDPPinresponsetovaryingworkloads [2].Wehavealsopro-
posed an approach for end-to-end QoS and cost-aware resource allocation that
uses the sustainable QoS profile for decision-making [1]. In this demo paper,
we present a proof-of-concept implementation of AuraEN, an Autonomous
resourceallocationENginefortheadaptivemanagementofcomputingresources
for cloud-deployed IoT DPPs. We present the conceptual system architecture of
AuraEN and briefly describe its implementation details. We demonstrate how
it can be used to (a) deploy a custom DPP on the Amazon EC2 cloud, and (b)
autonomously manage the allocated computing resources for each service in the
pipeline in response to the varying workload.
2 System Architecture
As shown in Fig.1, AuraEN has two key components - Resource Optimizer
(ROpt) and Resource Orchestrator (ROrch). The ROpt takes the following as
input: (a) the one-step-ahead workload prediction, (b) the sustainable QoS pro-
files for the candidate cloud instance types, (c) their pricing information, (d)
the current resource allocation for each DPP service, and (e) the end-to-end
QoS constraints that need to be satisfied. A DPP specific workload transfor-
mation function is used to compute the input workload for each DPP service
basedontheforecastworkload.ThesustainableQoSprofileisobtainedthrough
performance benchmarking as discussed in [2]; pricing information for the can-
didate cloud instance types is obtained by querying the cloud provider API; the
one-step ahead workload can be estimated from the historical workload using
standard forecasting algorithms. Based on these inputs, the ROpt (a) computes
the cost-optimal resource allocation for each service in the DPP for the next
period, and (b) determines how these resources should be provisioned. It uses
the following three resource scaling strategies:
AuraEN 79
Fig.1. The architecture of DPP resource management system (AuraEN)
– Delta-capacity optimization (DCO) finds the cost-optimal allocation of
resourcesforeachDPPservicebasedonthedelta betweentheexistingcapac-
ity and the predicted workload. DCO for scale-out finds the optimal alloca-
tion for the delta increase in the workload while retaining existing compute