title:Measuring and Characterizing End-to-End Route Dynamics in the Presence
of Load Balancing
author:&apos;Italo Cunha and
Renata Teixeira and
Christophe Diot
Measuring and Characterizing End-to-End Route
Dynamics in the Presence of Load Balancing
´Italo Cunha1,2, Renata Teixeira2,3, and Christophe Diot1
1 Technicolor
2 UPMC Sorbonne Universit´es
3 CNRS
Abstract. Since Paxson’s study over ten years ago, the Internet has changed
considerably. In particular, routers often perform load balancing. Disambiguat-
ing routing changes from load balancing using traceroute-like probing requires a
large number of probes. Our ﬁrst contribution is FastMapping, a probing method
that exploits load balancing characteristics to reduce the number of probes needed
to measure accurate route dynamics. Our second contribution is to reappraise
Paxson’s results using datasets with high-frequency route measurements and
complete load balancing information. Our analysis shows that, after removing
dynamics due to load balancing, Paxson’s observations on route prevalence and
persistence still hold.
1 Introduction
Our current understanding of end-to-end Internet route dynamics comes mainly from
the seminal work of Paxson [11] more than ten years ago. Paxson used traceroute mea-
surements to study routing anomalies, route persistence, and route prevalence in the
Internet. Since then, there have been only partial updates to his results [4, 15, 12], even
though the Internet has changed signiﬁcantly and new trafﬁc engineering practices, such
as load balancing, multihoming, and tunneling, are now commonplace.
This paper studies the effect of load balancing on the accuracy of measuring route dy-
namics. While Paxson identiﬁed just few examples of routes that oscillated because of
load balancing (which he called route ﬂuttering), Augustin et. al. [1] have recently ob-
served that approximately 40% of the source-destination pairs measured in their study
were subject to route ﬂuttering because of load balancing.
Load balancing increases the complexity of measuring route dynamics. It introduces
route changes that are not due to routing events, but could be misinterpreted as such.
Moreover, detecting load balancing requires additional probes [14], which in turn re-
duces the frequency at which one can measure routes. Current techniques that reduce
probing cost and increase the frequency of traceroute measurements [3, 6] are oblivi-
ous to load balancing. Sec. 3 quantiﬁes the effect of load balancing on route dynamics
using datasets collected with these two extreme approaches: complete load balancing
information (at the cost of high probing overhead) and high frequency probing (at the
cost misinterpreting load balancing). We show that ignoring load balancers leads to one
order of magnitude more observed route changes.
Given the popularity and impact of load balancing, we then analyze the dynamics
of routers that perform load balancing (which we name “load balancers”). Our results
N. Spring and G. Riley (Eds.): PAM 2011, LNCS 6579, pp. 235–244, 2011.
c(cid:2) Springer-Verlag Berlin Heidelberg 2011
236
´I. Cunha, R. Teixeira, and C. Diot
Fig. 1. Multiroutes to d1 and d2 at time t1 traversing a load balancer at i1
show that only 4% of load balancers need to be remapped more frequently than once
a day (Sec. 4). We exploit this property to design a new probing strategy that we call
FastMapping. FastMapping combines frequent light-weight route probing with daily
remapping of load balancers. As a result, it increases the route-probing frequency by a
factor of ﬁve while maintaining accurate load balancing information.
Last, we conﬁrm Paxson’s observations on route prevalence and persistence (Sec. 5).
When removing the effects of load balancing, we observe that the properties of Internet
routes have not changed in a decade. In summary, many source-destination pairs rarely
change routes; most of them have a prevalent route that stays active at least 60% of the
time, and suffer from short-lived instability periods 4% of the time.
2 Deﬁnitions
This section introduces the notation and explains how we compute route changes.
Routes and virtual paths. We borrow Paxson’s terminology [11] and use the term
virtual path to refer to the connectivity between a monitor and a destination, d, (i.e., the
existence of a route between the monitor and d). At any point in time, a virtual path is
realized by a route, which is the sequence of interfaces from the monitor to d discovered
by traceroute. A virtual path changes from one route to another over time as the result
of routing changes.
Classic traceroute assumes a single route between a source and a destination. How-
ever, load balancing is now common practice [1]. In Fig. 1, the router at i1 forwards
packets to d1 and d2 via interfaces i2 or i3 to perform load balancing. A traceroute to
d1 or d2 may infer the route through i2 and a later traceroute may infer the route through
i3, even though there was no routing change between the two measurements. Routers
perform load balancing per packet, per ﬂow or per destination. Per-destination load bal-
ancing sends all packets to a given destination on the same route, so only per-packet or
per-ﬂow load balancing lead to multiple routes between two end-hosts. We deﬁne a load
balancer’s divergence interface as the interface immediately before the multi-interface
hops (i1), and the convergence interface as immediately after (i4).
Instead of assuming a virtual path is realized by a single route at a time, we deﬁne a
multiroute R(d, t) as the set of all possible routes between the monitor and destination
d at time t. We can measure multiple routes between a source and a destination using
Paris traceroute’s Multipath Detection Algorithm (MDA) [14]. We refer to the set of
interfaces in the hth hop of a multiroute by R(d, t)[h], e.g., R(d1, t1)[2] = {i2, i3}. In
Measuring and Characterizing End-to-End Route Dynamics
237
Fig. 1, the top ruler shows the hop count h. For simplicity, the rest of this paper uses the
term route to refer to all simultaneous routes between the monitor and a destination.
Route changes. Given two consecutive routes between a monitor and a destination
(say R(d, t1) and R(d, t2), respectively at time t1 and t2), a route change represents a
contiguous set of interfaces that differs between these two routes. If there are multiple
sets of contiguous interfaces that differ between two routes, we consider each as one
route change. We say that R(d, t1)[h] = R(d, t2)[h] if the sets of interfaces at hop h
are the same. We match unresponsive routers in our traces (i.e., traceroute “stars”) with
any interface. This conservative approach avoids detecting route changes due to lost
probes or routers that rate-limit traceroutes, but it may miss some route changes. We
remove all routes containing repeated interfaces from our analysis in later sections to
avoid bias due to measurement errors, as in previous studies [11, 15].
3 Route Dynamics: Fast vs. Complete Measurements
Techniques to measure route dynamics have two conﬂicting goals. First, the study of
ﬁne-grained dynamics requires frequent measurements of a large set of virtual paths.
Second, accurate identiﬁcation of route changes needs information about load balanc-
ing, which requires a large number of probes [14]. This section explores this tradeoff
using two state-of-the-art route tracing methods: Tracetree [6] and Paris traceroute’s
Multipath Detection Algorithm [14].
3.1 Measurement Method and Datasets
Fast tracing. Tracetree [6] reduces the overhead to probe all hops in a topology. It starts
probing from the set of destinations and decrements the probe TTL. When probes to dif-
ferent destinations discover the same interface, Tracetree keeps probing only one desti-
nation. Such backward probing strategy reduces redundant probes close to the monitor.
Complete tracing. Paris traceroute’s Multipath Detection Algorithm (MDA) [14] dis-
covers all routes between a source and a destination in the presence of load balancing
with high probability. Paris traceroute ﬁxes the ﬂow identiﬁer of probes to ensure all
probes follow the same route under per-ﬂow and per-destination load balancing. In ad-
dition, MDA varies the ﬂow identiﬁer systematically to enumerate all interfaces in load
balancers. However, mapping load balancers requires a large number of probes per hop
(at least six probes per hop, but up to hundreds depending on the number of interfaces).
Dataset. We use Tracetree and MDA to measure virtual paths from 23 PlanetLab hosts
during seven days starting August 9th, 2010. Monitors collect two topology maps dur-
ing each measurement round: one with Tracetree and another with MDA. Each mea-
surement round takes 28 minutes on average: the ﬁrst 25 minutes are used by MDA and
the last three by Tracetree. We denote the traces collected with Tracetree as DT and
those collected with MDA as D1 and summarize them in Tab. 1.
We also have an earlier dataset collected with MDA and complete load balancer
information, denoted D2. The advantage of D2 is that it was collected from more
238
´I. Cunha, R. Teixeira, and C. Diot
Table 1. Description of datasets
Duration Monitors Frequency
Start
Dataset
DT
1 week
D1
1 week
D2 Nov. 28th, 2009 13 weeks
Aug. 9th, 2010
Aug. 9th, 2010
23
23
122
28 min.
28 min.
38 min.
ASes
Large ASes Measurement
Covered Covered [10] Method
5,043
Tracetree
5,266
8,692
Paris’ MDA
Paris’ MDA
95%
95%
97%
monitors and for a longer period of time. We did not collect Tracetree measurements
while collecting D2, hence we compare MDA and Tracetree using D1. We use D2 to
study long-term route dynamics.
Except for the measurement method and the parameters in Tab. 1, we collect all our
datasets with the same conﬁguration: each monitor selects 1,000 destinations at random
from a list of 34,820 randomly chosen reachable destinations, and we use ICMP probes
as routers are more likely to respond to ICMP than to TCP or UDP [8]. We complement
our datasets with IP-to-AS maps built from Team Cymru1 and UCLA’s IRL [10].
3.2 Analysis
We identify route changes between every pair of consecutive route measurements in
DT and D1 as described in Sec. 2. We remove 4.0% of routes from DT and 1.8% of
routes from D1 that contain repeated interfaces. Fig. 2 shows the cumulative distribution
function of the fraction of virtual paths that change between each pair of consecutive
measurement rounds in DT and D1. The D1 curve shows that the topology is mostly
stable: less than 6% of the virtual paths change between 95% of consecutive measure-
ment rounds. Only rarely more than 20% of virtual paths change between maps, and all
these instances represent events that happened close to the source.
The difference between DT and D1 is striking. For DT , there are approximately 76%
of consecutive measurements for which more than 20% of virtual paths change. We at-
tribute this difference to the measurement technique itself. MDA detects load balancers
explicitly and none of the route changes for D1 in Fig. 2 are due to load balancing.
However, Tracetree is oblivious to load balancing and interprets load balancing as route
changes.
We use the load balancer information collected with MDA to ﬁlter out all route
changes in DT due to load balancing (“ﬁltered DT ” line in Fig. 2). We see that load
balancers induce most of the dynamics in DT (82% of route changes). However, even
after ﬁltering, DT still has more route changes than D1. This happens because Trace-
tree’s assumption that the Internet topology is a tree is not always satisﬁed. For example,
routes to multiple destinations may meet at an Internet exchange point (IXP) and still
traverse different ASes upstream and downstream this IXP. Whenever the assumption
is false, Tracetree infers inexistent links and incorrect routes. Other causes for the dif-
ference include mapping errors, i.e., when the MDA’s probabilistic characterization of
load balancers fails to identify all interfaces in a load-balanced hop [14]. Such errors
impact our ability to ﬁlter dynamics induced by load balancers from DT .
1 http://www.team-cymru.org/Services/ip-to-asn.html
Measuring and Characterizing End-to-End Route Dynamics
239
f
o
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
s
d
n
u
o
R
t
n
e
m
e
r
u
s
a
e
M
e
v
i
t
u
c
e
s
n
o
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
D1 (Paris traceroute)
Filtered DT (Tracetree)
DT (Tracetree)
 0.2
 0.4
 0.6
 0.8
 1
Fraction of Virtual Paths Changed
f
o
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
l
s
r
e
c
n
a
a
B
d
a
o
L
 0.2
 0.15
 0.1
 0.05
 0
1
3
8
hours
week
hour
Average Time Between Internal Changes (h)
hours
days
1
day
3
1
Fig. 2. Virtual path dynamics between two
consecutive measurement rounds
Fig. 3. Average time between internal changes
in load balancers
4 Measuring Route Dynamics under Load Balancing
Our goal is to reduce the time to perform a measurement round and yet maintain ac-
curate information about load balancers, so that we can distinguish routing changes
from load balancer dynamics. We start with an analysis of load balancer dynamics that
motivates our probing method.
4.1 Analysis of Load Balancer Dynamics
To maintain an accurate database of load balancers, we need to detect and map load
balancers when they ﬁrst appear in a dataset. After we map all interfaces between the
divergence and convergence points of a load balancer, we only need to remap it when it
experiences an internal change. We deﬁne an internal change as a change in the set of
interfaces between the divergence and convergence points of a load balancer. Internal
changes may represent failures of one of the load-balanced interfaces, load balancer
reconﬁgurations, or mapping errors. Mapping errors are infrequent (4% of MDA runs
miss an interface [14]), but show up as internal changes in our analysis. As a result,
the internal changes we report next are an upper bound on the real number of internal
changes experienced by load balancers during the measurement period.
We use our longer D2 dataset to study load balancer dynamics. We remove 1.9% of
route measurements from D2 that contain repeated interfaces. D2 has 535,517 internal
changes, which gives an average of one internal change per load balancer every 20 days.
Given that D2 has 85,553,799 MDA measurements with load balancers, the number of
internal changes we see is within the MDA’s mapping error probability of 4% [14]. In
D2, only 23% of load balancers experience internal changes. Fig. 3 shows the distribu-
tion of the average time between internal changes. Very few load balancers experience
frequent internal changes. Speciﬁcally, only 4.6% of load balancers experience internal
changes more frequently than once every day. Among these 4.6% load balancers, 40%
span more than 4 hops and 16% perform non-uniform balancing (i.e., split packets un-
evenly among its next hops). These non-uniform and long load balancers are more likely
to suffer from mapping errors [14]. We get similar results from our D1 dataset: 27% of
240
´I. Cunha, R. Teixeira, and C. Diot
load balancers experience internal changes and 3.8% experience internal changes more
frequently than once every day. If there were no mapping errors, we would see even
less internal changes.
These results show that it is possible to maintain an accurate database of load bal-
ancers without remapping load balancers frequently and that remapping load balancers
once a day is enough to account for internal changes.
4.2 Probing Strategy
We design FastMapping, a probing strategy to measure route dynamics that exploits the
observations in the previous section to maintain an accurate database of load balancers
with low overhead. FastMapping operates in three main steps:
Create load balancer database. When FastMapping starts, it runs MDA on all moni-
tored virtual paths to populate the load balancer database. For each interface identiﬁed
with MDA, FastMapping records whether it is the convergence or divergence point of a