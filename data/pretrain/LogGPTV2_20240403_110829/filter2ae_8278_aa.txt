## 一、前言
不知道大家是否还记得我去年的一篇文章《Web日志安全分析浅谈》，当时在文中提到了关于日志里的“安全分析”从人工分析到开始尝试自动化，从探索到实践，从思路到工程化，从开源组件到自研发，从..，其中夹杂着大量的汗水与踩过的大量的坑，文章中所提的思路以及成果也算不上有多少的技术含量，其目的一是为了总结、沉淀与分享，如果能帮助到任何为“日志分析”而迷茫不知从何着手的人便算获得了些许慰藉，其目的二是为了抛砖引玉，大家集思广益将不同的分析方法、前沿技术及优秀思路。后来在社区看到xman21同学进行实践并进行分享《Web日志安全分析系统实践》，作者试着使用了如SVM、Logistic回归、朴素贝叶斯等算法进行训练与识别，可以看出xman21同学的工程化能力，以及对大数据的理解与应用能力还是不错的。不过今天要讨论的重点并不在此，我更关心的部分是数据到底该如何驱动安全，分析方法与思路又有哪些，机器学习，数据挖掘又是如何与数据分析产生火花的，同样今天也是抛砖引玉。
## 二、我对数据驱动安全的定义
我在上篇文章中提到一句话“日志分析本质为数据分析，而数据驱动安全必定是未来的趋势”，那到底什么是数据驱动安全呢？也许大家都有各自不同的想法，有人说“数据驱动安全”是与外部或者内部的威胁情报进行关联，以“情报”这类数据去分析、发现、挖掘安全问题；也有人说是对现有或历史数据进行分析，判断潜在的威胁和风险；还有人说是运用大数据进行多维度进行关联分析并以此发现安全问题。以上说法其实都对，广义的来讲通过任何来源的数据运用任何分析的方法与思路发现本身的安全问题都可称之为“数据驱动安全”。而狭义来讲则是通过IT数据结合安全经验进行分析相关风险，又或者说它是一款产品，一个分析思路或方法，一组威胁情报，目前在行业中的体现便是近两年火热的SIME\UEBA\SOC\威胁情报等，但是在我看来目前行业中的安全产品是无法完全的诠释我对数据驱动安全定义。
## 三、行业现状
在安全行业，安全信息和事件管理（SIEM）便是对”数据驱动安全“的一个不完全的诠释，根据2016年的市场调研公开结果表明，SIEM全球销量达到了21.7亿美元且对比所有细分领域，SIME呈高增速状态，我们先来看看Gartner魔力象限中关于SIME的厂商排行（暂时没找到2018的），一眼看过去，Splunk、IBM、McAfee以及专注SIEM的LogRhythm等身处为引领者的位置。而国内的厂商似乎只有"Venustech"一家也就是启明进入象限且仅位列在"Niche
Players"行列，而“Niche Players”的翻译为“投机者”、“利基者”、“特定领域者”，详细描述可以参考官方解答：
    A small but profitable segment of a market suitable for focused attention by a marketer. Market niches do not exist by themselves, but are created by identifying needs or wants that are not being addressed by competitors, and by offering products that satisfy them.
我在去年的年初进行相关调研时，当时一款叫“安全易”的产品进入我的眼帘，其实此款产品并无太多新意，而当时觉得瀚思科技的主要方向应该为“业务安全”，这也是安全牛全景图在酷厂商中对瀚思科技的描述，之后并无对其太过关注，如今回过头来看，瀚思已经打破了启明在官网中提到的“作为国内唯一一家进入Gartner
SIEM魔力象限的信息安全产品提供商”，说到这既遗憾又感到一点慰藉，遗憾于自己深耕于此领域，但是所得结果却尚不如人意，产品能力一直停滞不前，感觉到一些无力感，似乎遇到一些瓶颈，加上尚不明确的方向与各方面的资源缺失，慢慢似乎已经落后于人。而慰藉的是，在花花绿绿的可视化效果背后，在含有泡沫与水分的安全产品市场宣传中，有那么一小撮企业的确在做有价值的事情，在提高着行业整体的安全进化能力。  
关于Gartner魔力象限有朋友说有钱就能进大约70万左右，但我对此说法不置可否。这里引用一段启明产品总监叶蓬的一段话：
    中国厂商第一次入榜。对于中国客户和从业者而言，这是一个亮点。作为SIEM产品的负责人，本人有幸全程参与了这个过程。从旁观者到亲历者，体验是大大的不同，经验也是一大把，有机会另文撰述。这里只想提一句：入围Gartner SIEM魔力象限是一项十分艰苦的工作，不仅要跨越语言的障碍，还要扭转国外同行对中国厂商只能做安全硬件设备的误解。同时，SIEM产品的魔力象限考核指标是Gartner所有安全产品评估中最难、最复杂的，评价的指标项超过200个（指标之间相互关联，十分缜密）。期间，我们和分析师反复沟通，进行了十几次电话会议，数次北京的见面沟通，在美国的现场沟通，产品演示，提交各种产品资料和原理设计文档，等等等等。分析师对产品的分析十分严谨，所有能写进报告的项目都必须提供能让他信服的proven（证据）。至于技术方面，我们的SIEM覆盖了Gartner考察的大部分指标，包括流分析，ML。有机会，各位可以阅读另一个报告——《Gartner Critical Capabilities for SIEM, 2017》——一探究竟。
（图为Gartner SIEM分类下所有厂商）  
关于SIEM的描述性定义：  
SIEM为来自企业和组织中所有IT资源（包括网络、系统和应用）产生的安全信息（包括日志、告警等）进行统一的实时监控、历史分析，对来自外部的入侵和内部的违规、误操作行为进行监控、审计分析、调查取证、出具各种报表报告，实现IT资源合规性管理的目标，同时提升企业和组织的安全运营、威胁管理和应急响应能力。
（图为OSSIM产品）
## 四、数据种类
    机器数据中包含客户、用户、交易、应用程序、服务器、网络和手机设备所有活动和行为的明确记录。不仅仅包含日志。还包括配置、API 中的数据、消息队列、更改事件、诊断命令输出、工业系统呼叫详细信息记录和传感器数据等。
“数据驱动安全”，首先我们需要弄清楚我们都有哪些可被分析的数据，以及数据中包含的信息，源数据中所包含的信息量越大，我们能分析出的结果就越丰富、越精准，能完成的需求与覆盖的场景就越多,而数据并非越多越好，借用某人的一句话来说就是：“数据量要恰到好处，要多到足够支撑数据分析与取证，要少到筛选掉噪音数据”。开始我对这句话懵懵懂懂，不能太多也不能太少这不是为难人嘛，在经过一系列的实际工程化实践后，我开始对此有了新的认识，这里重新定义一下这句话：“数据的数量与维度要多到能足够到支撑起得到满意的分析结果，而数据过于冗余则需要进行合理的清洗噪音数据来保证数据恰到好处”。
根据以往经验，暂时将可分析数据分为以下几类，分别为系统、应用、存储、网络设备、业务、Agent监控（严格来说其实Agent监控应该分为系统层的数据，考虑到多为需要主动或Hook才能获取所以单独分为一类）  
当我们整理清楚我们的分析目标以后，首先需要的其实还不是盲目去搭建类似ELK\Hadoop的大数据分析平台，站在防御方的角度来讲，我们需要做的是问自己“这些数据对我有什么价值？”，“我的哪些安全问题可以通过分析哪些数据得到解决或缓解”，“哪里有我所需要的数据？”，“我的安全需求是什么？”，“我目前想要解决哪些和安全相关的问题？”，当我们具备一定的分析意识的时候，我们需要哪些数据自然一目了然。
## 五、安全需求&实践
当我们带着一定的分析意识，以及对分析数据目标的基本认识以后，我们就需要开始详细的思考，我们真正的安全需求是什么。有人说是对安全的详细的报表，有人说是发现攻击并阻断，也有人说是部署大量的安全软件与设备，加大投入聘请安全人员的预算。根据行业的专业见解来讲的，安全的需求并非是不出事，而是提升安全能力将MTTD\MTTR持续降低，即Mean
Time To Detect与Mean Time To Respond，具体级别参考如下：
    * 级别 0：盲视——MTTD以月计，MTTR以周或月计。有基本的防火墙和反病毒软件，但没人真正关注威胁指标，也没有正规的事件响应流程。如果这家公司掌握着国家或网络罪犯感兴趣的知识产权，那它很可能早已被盗。
    * 级别 1：最小合规——MTTD以周或月计，MTTR以周计。公司做了必须做的事情以符合法规要求。高风险的区域可能接受了更细致的安全审查，但公司基本上还是对内部和外部的威胁视而不见。敏感知识产权有可能被盗。
    * 级别 2：安全合规——MTTD和MTTR都以小时或天计。公司部署了足够的安全情报措施，不仅仅是“划勾式”合规，而是有着改进的安全保障。对一些威胁有承受能力，但还是对高级威胁毫无办法。
    * 级别 3：警醒——MTTD和MTTR以小时计。公司有强大的检测和响应威胁的能力。它可以通过全方位监控的仪表板主动搜寻风险。能承受大部分威胁，甚至那些借助高级持续性威胁功能的类型。
    * 级别 4：承受力强——MTTD和MTTR以分钟计。公司有着全面的安全情报能力和24小时不间断的安全运营中心（SOC）。尽管是高价值目标，仍能禁得起最极端类型对手的冲击。
现在我们知道了安全的终极目标，那么我们需要进行哪些分析才能达到目的呢？部分分析需求如下图：
图为当年使用ELK时头脑风暴出的安全需求，仅为分析需求中的冰山一角，我们需要根据实际情况进行合理的安全需求定制。  
回顾一下Web应用日志分析的需求：
上图的需求最后基本上我们都全部具体实现了，并且抛弃了原有的ELK体系
分析的日志为我博客15年建站以来的30万日志，仪表盘为自己随意拖拽形成，走到这一步，我们替换掉了采集端、替换掉了可视化、对ElasticSearch进行定制化与调优，遇到过大大小小无数个坑，也只是跟随上了开源分析架构的步伐，让操作与分析变得更为简单，而且就如我所说，各种统计与报表并不是安全数据分析的全部，一切不贴合实际需求，不解决实际问题，不具备实际效果的产品在我眼里都是毫无价值的。我们在Web应用层已经深耕两年有余，期间我们开始尝试一些创新的思路做一些事情，不过尝试新事物就势必要付出代价。这个新事物便是我在上篇文章中提到，我觉得具有价值的部分：攻击行为溯源，然而想要真正的将它进行工程化，个人觉得是一个非常复杂的事情，即使是近两年火热的安全运营中心（SOC），都主打的是“三分技术，七分管理”的概念，而我却在最开始的想法却是实现完全的自动化溯源取证，且是针对行为的，而且还妄想只通过Web日志就做到这一点，然后便在错误的道路上越走越远，不过即使如此，我们也产出了一定的成果，且我们积累了大量的分析方法，其中包括数据预处理、数据编码、数据计算等与数据挖掘以及机器学习相关的技术储备.  
到此基本上我们已经具备了两种能力，一种是宏观的安全分析，一种则是微观的事件挖掘，而基于这两种能力，我们可以有条件与技术储备去完成或覆盖更多的安全需求与用户场景。
## 六、数据治理
到这我们对数据种类以及分析需求有了一个初步的理解，那我接下来我需要对数据治理有一定的了解。  
数据治理其实是一个广义的意思，其中包括元数据、数据聚合、数据质量、数据确权、生命周期管理等等概念，此文的数据治理为狭义，仅代表对IT数据的采集、定义、解析、增量等。
### 1、采集
首先我们面临的第一个问题便是，数据如何进行采集，可能使用过ELK的便知道Logstash在其中便是采集的角色，其中Logstash对日志文件进行监控，当文件有变更时，便对文件中的内容进行采集再将数据输出到指定的位置。其实类似Logstash的采集端非常之多，如下图所示，有FileBeats、Logagent、Flume、Fluentd、syslog-ng、Rsyslog等，它们都有各自的亮点。
其实作为一个功能完整的Agent来说，所具备的功能不仅仅只是监控文件然后采集和输出，作为一个完整的Agent，应该具备以下功能：（这也是我们抛弃Logstash的原因，太重量级）
1、支持丰富数据源  
1.1文件  
Access Log  
Windows Event Log  
Linux Log  
1.2.存储  
MongoDB  
MySQL  
MicroSoft SQL Server  
Kafka  
Redis  
PostgreSQL  
1.3.网络  
Socket  
HTTP  
SNMP
2、监控信息采集模块  
2.1.系统  
2.2.网络  
2.3.进程  