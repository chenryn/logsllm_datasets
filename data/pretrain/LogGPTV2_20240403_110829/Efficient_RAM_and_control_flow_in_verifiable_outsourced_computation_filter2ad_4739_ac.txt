putations of a given length use the same set of constraints, so
the amortization behavior is potentially better. (BCTV’s con-
straints are sometimes said to be universal, but as discussed in
Section 7, in practice one constraint set would not be enough.)
To be relevant, Pantry and BCTV need to operate in one
of two regimes. The first is when a given V runs the protocol
multiple times on the same set of constraints (for Pantry, this
6Thus, Ccpu can be equivalently understood as validating the state transitions
in a transcript that is non-deterministically supplied by P; this view is the one
presented in [16, 19].
per-op contribution to |C|
one RAM op.
one non-RAM op.
system
Pinocchio [62], Zaatar [70]
Pantry [28]
BCTV [19]
Buffet
3 · 2r
4700 · r
c′
mem
2 · cmem + ccpu
t: number of steps to execute Ψ on the simulated CPU (§2.3)
r: log of memory size (for example, r = 32)
ccpu = 1114: number of constraints for one BCTV CPU cycle
cmem = 67 + 4 · log 2t + 9r : per-tuple constraint cost in BCTV c′
cavg
cavg
2 · cmem + ccpu
total # of constraints (|C|)
(3 · 2r) · k′ + cavg · (t′ − k′)
(4700 · r) · k′ + cavg · (t′ − k′)
(2 · cmem + ccpu) · t
cavg
mem · k′ + cavg · (t′ − k′)
c′
t′ (≈ t): number of program steps to execute Ψ in P/Z/P
k′ (≤ t′): number of memory operations in Ψ in P/Z/P
cavg ≈ 22: avg. non-RAM constraints per step for Ψ in P/Z/P
mem = 21 + 10 · log k′ + 2r : per-tuple constraint cost in Buffet
FIGURE 5—Per-operation and total constraint costs, for a given computation Ψ. Buffet improves on the others both qualitatively and quantitatively:
its RAM verification costs scale with the number of memory operations (k′) rather than the number of program steps (t, t′), and the scaling factor
mem) is much lower than in BCTV (2 · cmem + ccpu) and Pantry (4700 · r for a load; a store costs twice as much [28, Fig. 9]). “P/Z/P” stands for
(c′
“Pinocchio/Zaatar/Pantry execution model”. In Zaatar and Pinocchio (Pantry’s baselines), dynamically-addressed loads and stores translate to
costly switch/case statements [19, 28]. cmem is taken from [19, §5.1] for a CPU with 16 registers of 32 bits. Section 3.1 explains cavg, t, and t′.
Pantry model (shared by its base systems, Pinocchio [62] and
Zaatar [70]). cavg is computation-dependent, but we can im-
pose reasonable bounds, since the cost of non-RAM operations
ranges from 1 (for arithmetic operations) to 34 (for operations
on 32-bit values that require separating numbers into their bits,
such as inequality comparisons and bitwise operations). We
obtained cavg = 22 in the figure by conservatively assuming
that non-RAM operations occur with uniform frequency.
Second, Buffet’s permutation network works over what we
call a trace: a set of tuples, one for each of the k′ operations
that specifically interacts with memory. By contrast, recall that
in BCTV (§2.3), the input to the permutation network is a
transcript: two tuples for each of the t execution steps. These
distinctions—trace versus transcript, one versus two tuples—
mem · k′ contribution
are reflected in Figure 5, specifically the c′
in the Buffet row and 2 · cmem · t in the BCTV row.
A critique of the preceding analysis is that t and t′ are dif-
ferent kinds of quantities. For one thing, t counts steps of the
simulated CPU’s execution, whereas t′ counts operations in
Buffet’s intermediate unrolled representation. However, this
objection is not fundamental, as the operations in the latter
roughly correspond to those in the former. A more serious issue
is that t counts the program steps actually taken whereas t′
includes operations in branches not taken. However, this dis-
tinction does not affect the analysis much either: even if we
take t′ = 10 · t (which is highly pessimistic: it means that
conditional statements entail 10 branches on average), Buffet’s
costs are nearly an order of magnitude less than BCTV’s.
Details. Below, we describe how Buffet handles loads and
stores, in terms of steps 1 and 2 in the framework of Section 2.1.
As a backdrop, we note that in step 1, the Buffet compiler main-
tains a monotonically increasing counter, mem-ts, that tracks
memory operations. In step 2, the Buffet prover maintains a
simulated RAM inside its own address space.
Loads. For
step 1, when the compiler encounters
Zval=load(Zaddr), it creates constraints that “wire” the tu-
ple (mem-ts, LOAD, Zaddr, Zval) into the permutation network.
The compiler also inserts an annotation for step 2, which tells
P to set Zval by loading address Zaddr from its simulated RAM.
Stores. When executed unconditionally (meaning outside
of an if-then or if-then-else block), stores are similar to loads.
However, inside of a conditional block, a store operation creates
complexity in both steps 1 and 2.
Concerning step 1, the problem is as follows. If a branch that
contains a store operation is taken during execution, then the
variables of that store must enter the permutation network. But
if the branch is not taken, then the store must not be part of the
execution-ordered trace. Meanwhile, Buffet’s compiler must
decide statically what enters the permutation network (without
knowing which branches will be taken). Buffet resolves this
issue by “dynamically casting” the store to a dummy load at run
time, if the branch is not taken. Specifically, when the compiler
encounters store(Zdata, Zaddr) inside a conditional block,
it wires the following tuple (§2.3) into the permutation network:
( mem-ts, Zcond · STORE + (1 − Zcond) · LOAD,
Zcond · Zdata + (1 − Zcond) · Zdummy ),
Zaddr,
where Zcond captures the conditions that surround the store
operation. If Zcond=0 at run time, then observe that P is obliged
to treat this tuple (more precisely, the constraints to which this
tuple expands) as a dummy load rather than a store.
Concerning step 2, recall that during Pantry’s solving phase,
there is no longer an explicit notion of control flow, condition-
ality, etc. Pantry’s prover simply walks a list of constraints,
solving each one, as instructed by annotations (§2.2). The diffi-
culty in our present context is that, if a store operation is in an
untaken branch, P should not actually apply the update to its
simulated RAM—if P did so, future loads would return incor-
rect values, and the coherence-checking constraints would not
be satisfied. To address this issue, the Buffet compiler creates
an annotation that instructs P to apply the store operation to its
simulated RAM only if P also sets Zcond=1.7
7Why doesn’t Pantry face the issues just described? Recall that Pantry’s RAM is
implemented on a content-addressable block store (§2.2), which maps digests
d to blocks B, where H(B) = d. When RAM is built this way [24, 38, 53],
each configuration of memory has its own digest. Furthermore, each load and
store takes a digest as an argument, and each store returns a new digest [28,
§5.1]. Thus, if Pantry applied a store to its simulated RAM, but the store
happened in an untaken branch, the (digest, block) entries added to the block
7
3.2 Optimizations
Consistent with Buffet’s goal of paying for RAM operations
only when necessary, its compiler eliminates loads and stores
where possible; the result is fewer constraints and hence better
performance. Of course, the compilers of BCTV and Pantry
could apply similar analysis, but the overall effect on their
performance would be muted, as we explain in Section 3.3.
Buffet applies two classes of optimizations. First, the com-
piler can defer, and sometimes eliminate, RAM operations if
the address is available at compile time. To do so, the compiler
maintains a table that maps addresses to intermediate vari-
ables. When the compiler encounters store(Zdata,Zaddr)
where Zaddr is statically determined, it produces no correspond-
ing constraints; it simply adds a new entry in the table, to
map the value of Zaddr to Zdata. When the compiler encounters
Zval=load(Zaddr) where Zaddr is statically determined, it
consults the table. If there is a mapping between Zaddr and an in-
termediate variable Zupstream, the compiler produces a constraint
that assigns Zval = Zupstream (rather than wiring a new tuple into
the permutation network).
When the compiler encounters a load or store whose address
A cannot be fully resolved at compile time, it must apply the
delayed writes for any memory that could be referenced by A.
Specifically, for each entry (ai, Zi) in the delayed writes table,
the compiler uses pointer aliasing analysis [61] to determine
whether A and ai could possibly reference the same memory. If
so, the compiler produces constraints that store the value Zi to
address ai, and removes (ai, Zi) from the table.
The second type of optimization is classical load and store
elimination [61]. In cases when Buffet’s compiler determines
that two operations share the same address (even if the compiler
does not know the address itself), it applies three reductions:
(R1) For two load operations from the same address with no
intervening store, replace the return value of the second load
with the return value of the first, and eliminate the second load.
(R2) For two store operations to the same address with no
intervening load, let the second store obviate the first. (R3) For
a store immediately followed by a load targeting the same
address, eliminate the load, and refer to the data directly.
As an example, consider the following pseudocode:
out[offset] = 0
for i in [0, 10):
out[offset] += input[i]
This code seems to access input[i] 10 times and
out[offset] 21 times. But the optimizations above would
reduce it to only a single store operation. Specifically, the com-
piler can statically determine the address input + i; it then
avoids the corresponding loads, using the table described earlier.
At this point, the remaining RAM operations are an alternat-
ing sequence of stores and loads, at address out + offset;
reduction R3 then eliminates the loads. Finally, reduction R2
eliminates all but the final store.
store would be harmless: they would not overwrite other entries (because
digests are functionally unique), and they would be unreferenced by the
downstream program logic (because the branch is not taken).
3.3 Discussion
What is the fundamental reason that Buffet can pay for memory
only when it is used, whereas BCTV has to incur the cost on
every operation (§3.1)? And why is load-store elimination of
far more benefit to Buffet than BCTV (§3.2)?
These questions have the same answer: the different ab-
straction barriers in the two systems. In Buffet, the C com-
piler produces constraints tailored to the computation, which
is why it can wire selected operations into the permutation
network (§3.1) and optimize out unneeded constraints (§3.2).
In BCTV, the C compiler produces assembly for the simu-
lated CPU (the Ψ → xΨ step in Section 2.3). Meanwhile, this
assembly program has no influence on the constraints them-
selves [19]. Beyond this, recall that each step of the unrolled
CPU execution contains the logic needed to execute any possi-
ble assembly instruction (§2.3, Fig. 2); since any step might be
a load or store, every step in the execution must be wired into
the permutation network. Therefore, while the BCTV compiler
could apply the optimizations in Section 3.2, the result would
only be to reduce program text length |xΨ| (and potentially
t). There is no sense in which the compiler could eliminate
expensive operations: each program step induces the same cost.
Pantry, in contrast, could apply the optimizations of Sec-
tion 3.2 to reduce the number of expensive operations. How-
ever, the ultimate efficacy would be limited by the extremely
high cost of its RAM abstraction: in practice, Pantry is limited
to at most several tens of RAM operations (§5.4).
4 Efficient data dependent control flow
Using the work of the preceding section, Buffet produces con-
cise constraints for straight line computations (because it in-
herits Pantry’s line-by-line compilation), but the subset of C
supported so far does not include a key programming construct:
data dependent control flow. BCTV lets the programmer use
all of C (due to the underlying abstraction of a general-purpose
CPU); however, as discussed in the previous sections, BCTV’s
abstraction brings significant overhead.
The challenge is again for Buffet to provide the best of
both worlds. Buffet’s high-level solution is a source-to-source
translation that adapts techniques from the compilers literature
and exploits aspects of the constraint idiom. Specifically, the
Buffet compiler accepts programs written in a nearly complete
subset of C and applies a flattening transformation to produce
a C program that is less concise but has no data dependent
control structures; the compiler then translates the modified
source efficiently into constraints. This approach works because
there is no cost to making the intermediate source verbose—the
constraint formalism unrolls computations anyway.
4.1 The programmer’s interface
Buffet supports all C control flow constructs except for goto
and function pointers. The programmer annotates any looping
construct that should be flattened, using a C++11-style attribute,
buffet::fsm. This attribute takes one argument, a bound on
the number of iterations in the flattened loop. This is similar
8
while j 
// data dependent bound
limit = get_limit(j)
for i in [0, limit):
state = dummy = 0
while dummy 
limit = get_limit(j)
i = 0
state = 1
else:
state = 3
if state == 1:
if i 
i++
else:
state = 2
if state == 2:
state = 0
dummy++
(a) Original.
(b) Flattened.
FIGURE 6—Loop flattening example. The original and flattened pseu-
docode have equivalent control flow.
to how the BCTV programmer must choose t (§2.3), as we
discuss in Section 4.3.
4.2 The transformation
As an example, consider the code of Figure 6a. (Consistent with
the language supported by the Buffet compiler, our examples
in this section refer to C code; they are depicted in a Python-
like pseudocode for visual clarity.) Pantry cannot compile this
program (§2.2), since the number of iterations in the inner loop