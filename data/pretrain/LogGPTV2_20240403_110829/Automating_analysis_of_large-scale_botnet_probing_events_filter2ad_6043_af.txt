From the results, we see that our (cid:2)ndings are consistent with those
derived from DShield. Next, we introduce how the DShield valida-
tion works, and then we will analyze the accuracy of our results.
Validation Methodology: We (cid:2)nd that most DShield sensors ap-
pear to have synchronized clocks (i.e., we often (cid:2)nd signi(cid:2)cant
temporal overlap between our honeynet events and corresponding
DShield reports). For a given extraplation, we take two steps for
validation. First, since the extrapolation results we got are all of
/8 size or quite close, we try to (cid:2)nd all the /8 networks (except
those with private IP pre(cid:2)xes) with suf(cid:2)cient source overlap with
the honeynet events. Secondly, for these /8 networks, we infer the
scan scopes and compare them with our results.
Step 1. Let X denote the /8 IP pre(cid:2)x of our sensor. We (cid:2)rst
calculate the number of shared senders N (X) between our event
data and scan logs for X from DShield. We consider additional
/8 pre(cid:2)xes Yi if their numbers of senders shared with the honeynet
N (Yi) are larger than N (X)=3, re(cid:3)ecting an assumption that if a
botnet uniformly scans multiple /8 pre(cid:2)xes, each should see quite
a few sources in common. For X and each Yi, we select the full
width at half maximum (FWHM) of the unique source arrival pro-
cess as a (conservative) way to delineate the global interval of the
event. We then calculate the time range overlap with X for each
Yi; if the overlap of Yi exceeds 50% of X’s interval, we consider
that the botnet scanned X and Yi at the same time.
Step 2. After (cid:2)nding the scanned /8 networks, we estimate the
scan scope within each. Alternatively, we compute the ratio of sen-
sors in each network reporting the scans. There are several limi-
tations of DShield data. First, it does not contain complete scan
information (only a subset of scans within a pre(cid:2)x are reported).
Second, different sensors might use different reporting thresholds
and might not see all activity (e.g., due to (cid:2)rewall (cid:2)ltering). Thus
all these limitations makes calibration of data a challenging job.
To assess the limitations, we check a one-week interval around
our events to (cid:2)nd which DShield sensors ever report a given type
of activity. We treat all the reporting sensors in one /24 network
as a single unique sensor. We count the number of sensors from
different /24 networks, denoted by Ctotal. Similarly, we count
the number of unique sensors from different /24 networks that re-
ported scans from shared senders of the given event, denoted Cest.
We reduce the noise from the DShield data by removing sensors
(cid:7)
(cid:10)
(cid:6)
(cid:7)
(cid:17)
(cid:18)
(cid:18)
(cid:20)
(cid:6)
(cid:2)
(cid:18)
(cid:18)
(cid:20)
(cid:7)
(cid:7)
(cid:7)
(cid:2)
(cid:8)
(cid:30)
(cid:8)
(cid:30)
(cid:8)
(cid:30)
(cid:8)
(cid:30)
(cid:3)
(cid:7)
(cid:31)
(cid:7)
(cid:6)
(cid:18)
(cid:7)
(cid:31)
(cid:7)
(cid:6)
(cid:18)
(cid:7)
Approach I
Approach II
y
t
i
l
i
8
.
0
b
a
b
o
r
p
e
v
i
t
a
u
m
u
c
l
6
.
0
4
.
0
2
.
0
y
t
i
l
i
8
.
0
b
a
b
o
r
p
e
v
i
t
a
u
m
u
c
l
6
.
0
4
.
0
2
.
0
7
6
5
4
3
2
1
1.1
1.0
1.4
scope factor
1.2
1.3
scope factor
8
Figure 13: The CDFs of the scope factors of the 12 events we
validate.
that only report a single address within a /24 sensor. We then use
Cest/Ctotal to estimate the fraction of a /8 networks scanned by the
botnet, which gives us a conservative estimate of the event’s total
range. We add up such fractions if there are multiple related /8 net-
works discovered in the (cid:2)rst step, indicating the results in Column
DShield scope of Table 6.
Accuracy Analysis: We de(cid:2)ne the scope factor as
scope factor = max(cid:18) DShield scope
Honeynet scope ;
Honeynet scope
DShield scope (cid:19)
The scope factor indicates the absolute relative error in the log
scale. The DShield data shows that our local estimates of global
scope exhibit a promising level of accuracy. As shown in Figure 13,
we can clearly know that, for Approach I, the scope factors of 75%
events are less than 1.35, and all of them are less than 1.5. Ap-
proach II (column ex. scope II) works less well (58% of events
are within a factor of three and 92% within a factor of six), but it
may still exhibit enough power to enable sites to differentiate scans
that speci(cid:2)cally target them versus broader sweeps.
In our two-
year dataset, we did not (cid:2)nd any scan events speci(cid:2)cally targeting
the research institution where the sensor resides; this (cid:2)ts with the
institute’s threat model, which is mainly framed in terms of indis-
criminant attacks.
5.4.2 Total Population Estimate and Validation
We assume that our honeynet event data and the corresponding
DShield scan data give us two independent samples of the bot pop-
ulation, which is another chance to use the Mark and Recapture
principle. We count the sources observed by DShield sensors of
IP pre(cid:2)x X on the same port number in the same time window as
the sources of DShield sensors. We term the number of sources in
common between our honeynet and DShield as the shared sources.
Based on the similar idea of Equation 1, we know the fraction of
the shared sources to the sources of DShield should be equal to the
ratio between bots observed in the honeynet and total population.
Since DShield sensors will see other scanners (constituting noise)
as well, we will likely underestimate the (cid:2)rst fraction, and con-
sequently overestimate the bot population. Per the results shown
below, we (cid:2)nd the estimates very close to those we estimate locally
by splitting the sensor into two halves.
shows the extrapolation and DShield validation re-
sults. Column ex. #bots shows our bot population extrapolation
constructed by splitting the sensor into two halves. Column #bot
DShield shows the results using DShield’s global data. Column
#bots ratio gives the ratio between the two of these. Note, we
only validate the seven port number based events (MSSQL, Syman-
tec and VNC). The NetBIOS/SMB events require payload anal-
ysis, which cannot validate through DShield since it does not
provide any payloads. We (cid:2)nd our approach is quite accurate
given 64% of cases are within 8% of relative error (j(our (cid:0)
DShield)j=DShield).
5.4.3 Other Extrapolation Results
Based on Approach I, we can also infer the total number of scans
and extrapolated average scan speed of the bots in each event. In
Table 7.
desc
date
2006
08-25 MSSQL
11-26
Symantec
11-27
Symantec
11-28
Symantec
07-23 VNC
07-29 VNC
10-31 VNC
ex.
#bots DShield
3100
228
276
305
2752
3628
526
#bots
3139
215
373
331
2712
3696
622
#bots
ratio
0.99
1.06
0.73
0.92
1.01
0.98
0.84
Table 7: extrapolated bot population results and validation.
y
t
i
l
i
8
.
0
b
a
b
o
r
p
e
v
i
t
a
u
m
u
c
l
6
.
0
4
.
0
2
.
0
2
10 20
5
# of extraplated scans (M)
50
200 500
Figure 14: Extrapolated # of
scans.
y
t
i
l
i
8
.
0
b
a
b
o
r
p
e
v
i
t
a
u
m
u
c
l
6
.
0
4
.
0
2
.
0
8
6
12
10
16
extrapolated average speed (probes/sec)
Figure 15: Extrapolated the
average scan speed.
14
Figure 14, we show the extrapolated total number of scans, using
a log-scaled X axis. We can see the number of scans sent by the
events could differ signi(cid:2)cantly given the duration and the number
of bots in each event differ. In Figure 15, we show the extrapolated
average scan speed of the bots.
6. RELATED WORK
The work that most heavily in(cid:3)uences us is the vision paper
of Yegneswaran and colleagues on (cid:147)Internet situational aware-
ness(cid:148) [30]. Their work outlines the general problem of analyzing