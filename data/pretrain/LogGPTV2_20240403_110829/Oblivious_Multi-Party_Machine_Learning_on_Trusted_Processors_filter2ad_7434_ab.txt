3
256-bit vector register with 32-bit components
( c0 c1 c2 c3 c4 c5 c6 c7 )
...
cache line
...
array in memory
Figure 3: Optimized array scanning using the vpgatherdd in-
struction; here, the value of interest is read into C4. The other
components perform dummy reads.
register from a different memory offset. Hence, by load-
ing each component from a different cache line, 4 bytes
can be read obliviously from an aligned 512-byte array
with a single instruction as depicted in Figure 3 (i.e.,
a 4-byte read is hidden among 8 cache lines accessed
via vpgatherdd). On top of this, the oget() primitive
is created, which obliviously reads an element from an
unaligned array of arbitrary form and size. oget() it-
eratively applies the vpgatherdd instruction in the de-
scribed way while avoiding out-of-bounds reads. De-
pending on the dynamic layout of the caches, oget()
can significantly speed-up oblivious array lookups (see
Section 6.6). The construction of oget() is conserva-
tive in the sense that it assumes (i) that the processor
may load vector components in arbitrary, possibly par-
allel order3 and (ii) that this order is recorded precisely
in τ. For cases where (i) or (ii) do not apply, e.g., for
software-only attackers, a further optimized version of
oget() is described in Appendix B.
Oblivious sorting We implement oblivious sorting by
passing its elements through a network of carefully ar-
ranged compare-and-swap functions. Given an input
size n, the network layout is fixed and, hence, the mem-
ory accesses fed to the functions in each layer of the net-
work depend only on n and not the content of the ar-
ray (as opposed to, e.g., quicksort). Hence, its memory
trace can be easily simulated using public parameter n
and fixed element size. Though there exists an optimal
sorting network due to Ajtai et al. [1], it incurs high con-
stants. As a result, a Batcher’s sorting network [7] with
running time of O(n(logn)2) is preferred in practice. Our
library includes a generic implementation of Batcher’s
sort for shuffling the data as well as re-ordering input
instances to allow for efficient (algorithm-specific) ac-
cess later on. The sorting network takes as input an array
of user-defined type and an oblivious compare-and-swap
function for this type. The oblivious compare-and-swap
usually relies on the ogreater() and omoveEx() prim-
itives described above.
3The
implementation
microarchiteture-specific and undocumented.
of
the vpgatherdd instruction
is
4
4 Machine Learning Algorithms
We describe five machine learning algorithms: four train-
ing and one prediction method, and their data-oblivious
counter-parts. The algorithms vary in the complexity
of access patterns, from randomly sampling the training
data to input-dependent accesses to the corresponding
model. Hence, we propose algorithm-specific mitigation
techniques that build on the oblivious primitives from the
last section.
4.1 K-Means
The goal of k-means clustering is to partition input data
points into k clusters such that each point is assigned to
the cluster closest to it. Data points are vectors in the
d. To implement clustering, we chose
Euclidean space
a popular and efficient Lloyd’s algorithm [40, 41, 43].
During its execution, k-means maintains a list of k
points that represent the current cluster centroids: for
i = 1..k, the ith point is the mean of all points currently
assigned to the ith cluster. Starting from random cen-
troids, the algorithm iteratively reassigns points between
clusters: (1) for each point, it compares its distances to
the current k centroids, and assigns it to the closest clus-
ter; (2) once all points have been processed, it recom-
putes the centroids based on the new assignment. The
algorithm ends after a fixed number of iterations, or once
the clustering is stable, that is, in case points no longer
change their cluster assignments. Depending on the ap-
plication, k-means returns either the centroids or the as-
signment of data points to clusters.
Although the algorithm data flow is largely indepen-
dent of the actual points and clusters, its naive implemen-
tation may still leak much information in the conditional
update in (1)—enabling for instance an attacker to in-
fer some point coordinates from the final assignment, or
vice-versa—and in the recomputation (2)—leaking, for
instance, intermediate cluster sizes and assignments.
In the following, we treat the number of points (n),
clusters (k), dimension (d) and iterations (T ) as pub-
lic. We consider efficient, streaming implementations
with, for each iteration, an outer loop traversing all points
once, and successive inner loops on all centroids for the
steps (1) and (2) above. For each centroid, in addition
to the d coordinates, we locally maintain its current clus-
ter size (in 0..n). To perform both (1) and (2) in a single
pass, we maintain both the current and the next centroids,
and we delay the division of coordinates by the cluster
size in the latter. Thus, for a given point, inner loop (1)
for i = 1..k maintains the (square of the) current minimal
distance δmin and its centroid index imin. And inner loop
(2) performs k conditional updates on the next centroids,
depending on i = imin. Finally, a single pass over cen-
troids recomputes their coordinates. An important detail
is to uniformly handle the special case of empty clus-
622  25th USENIX Security Symposium 
USENIX Association
ters; another to select the initial centroids, for instance
by sampling random points from the shuffled dataset.
In our adapted algorithm, the “privacy overhead” pri-
marily consists of oblivious assignments to loop vari-
ables in (1), held in registers, and to the next centroids,
held in the cache.
In particular, instead of updating
statistics for only the centroid that the point belongs
to, we make dummy updates to each centroid (using
our omoveEx() primitive). In the computation of new
centroids, we use a combination of ogreater() and
omoveEx() to handle the case of empty clusters. These
changes do not affect the algorithm’s time complexity: in
the RAM model the operations above can still be done in
O(T (nkd + kd)) = O(T nkd) operations.
Theorem 1. The adapted k-means algorithm runs in
time O(T nkd) and is data-oblivious, as there exists a
simulator for k-means that depends only on T , n, d,
and k.
Proof. The simulator can be trivially constructed as fol-
lows: given T , n, d and k, it chooses n random points
from d and simply runs the algorithm above for k cen-
troids and T iterations.
It is easy to see that the subroutine for finding the clos-
est centroid in the training algorithm can be also used to
predict the trained cluster that an input point belongs to.
4.2 Supervised Learning Methods
In supervised machine learning problems, we are given
a dataset D = {(xi,yi)}i=1..n of instances, where xi ∈ X
is an observation and yi ∈ Y is a desired prediction. The
goal then is to learn a predictive model f : X → Y such
that f (xi) ≈ yi and the model generalizes to unseen in-
stances x ∈ X . Many machine learning methods learn
such a model by minimizing an empirical risk objective
function together with a regularization term [65]:
Ω(w) +
min
w
1
n
n
∑
i=1
L(yi, fw(xi)).
(1)
We will show secure implementations of support vector
machines (SVM) and neural networks, which are of the
form (1). Other popular methods such as linear regres-
sion and logistic regression are also instances of (1).
Most algorithms to minimize (1) operate iteratively on
small subsets of the data at a time. When sampling these
subsets, one common requirement for correctness is that
the algorithm should have access to a distribution of sam-
ples with an unbiased estimate of the expected value of
the original distribution. We make an important obser-
vation that an unbiased estimate of the expected value of
a population can be computed from a subset of indepen-
dent and identically distributed instances as well as from
a subset of pairwise-distinct instances.
Thus, we can achieve correctness and security by
adapting the learning algorithm as follows. Repeat-
edly, (1) securely shuffle all instances at random, us-
ing Batcher’s sort, for example, or an oblivious shuf-
fle [50]; (2) run the learning algorithm on the instances
sequentially, rather than randomly, either individually or
in small batches. Thus, the cost of shuffling is amortized
over all n instances. Next, we illustrate this scheme for
support vector machines and neural networks.
4.3 Support Vector Machines (SVM)
Support Vector Machines are a popular machine learning
model for classification problems. The original formula-
tion [12] applies to problems with two classes. Formally,
SVM specializes (1) by using the linear model fw(x) =
(cid:26)w,x(cid:25), the regularization Ω(w) = λ
2 (cid:24)w(cid:24)2 for λ > 0, and
the loss function L(yi, fw(xi)) = max{0,1− yi fw(xi)}.
The SVM method is important historically and in prac-
tice for at least four separate reasons: ﬁrst, it is easy to
use and tune and it performs well in practice; second, it
is derived from the principle of structural risk minimiza-
tion [65] and comes with excellent theoretical guarantees
in the form of generalization bounds; third, it was the first
method to be turned into a non-linear classifier through
application of the kernel trick [12, 56], fourth, the SVM
has inspired a large number of generalizations, for ex-
ample to the multi-class case [67], regression [61], and
general pattern recognition problems [63].
Here we only consider the linear (primal) case with
two classes, but our methods would readily extend to
multiple classes or support vector regression problems.
There are many methods to solve the SVM objective
and for its simplicity we adapt the state-of-the-art Pega-
sos method [58]. The algorithm proceeds in iterations
t = 1..T and, at each iteration, works on small subsets
of l training instances at a time, A(t).
It updates a se-
quence of weight vectors w(1),w(2), . . . ,w (T ) converging
to the optimal minimizer of the objective function (1).
Let us now consider in detail our implementation
of the algorithm, and the changes that make it data-
oblivious. We present the pseudo-code in Algorithm 1
where our changes are highlighted in blue, and indented
to the right. As explained in Section 4.2, SVM sam-
ples input data during training. Instead, we obliviously
(or privately) shuffle the data and process it sequentially.
The original algorithm updates the model using instances
that are mispredicted by the current model, A(t)
+ in Line 5
of the pseudo-code. As this would reveal the state of
the current model to the attacker, we make sure that
the computation depends on every instance of A(t). In
particular, we generate a modified set of instances in
B(t) which has the original (x,y) instance if x is mispre-
dicted and (x,0) otherwise, assigning either 0 or y using
our ogreater() and omove() primitives (see Figure 2).
USENIX Association  
25th USENIX Security Symposium  623
5
Algorithm 1 SVM Original with changes (starting
with (cid:31)) and additional steps required for the Oblivious
Version indicated in blue.
1: INPUT: I = {(xi,yi)}i=1,...,n, λ , T , l
2: INITIALIZE: Choose w(0) s.t. (cid:29)w(0)(cid:29) ≤1/ √λ
3:
4: FOR t = 1,2, . . . ,T × n/l
5:
Shuffle I
Choose A(t) ⊆ I s.t. |A(t)| = l
Set A(t)
+ = {(x,y) ∈ A(t) : y(cid:22)wt ,x(cid:21) < 1}
(cid:31) Set A(t) to tth batch of l instances
[y(cid:22)wt ,x(cid:21) < 1]y) : ∀(x,y) ∈ A(t)}
(cid:31) ν = ∑(x,z)∈B(t) zx
yx
+
(cid:31) B(t) = {(x,
Set η = 1/λt
Set ν = ∑(x,y)∈A(t)
Set v = (1− ηλ )w(t) + η
l ν
Set c = 1 < 1√λ (cid:29)w(cid:29)
Set w(t+1) = min(cid:31)1, 1√λ (cid:29)w(cid:29)(cid:30)v
(cid:31) Set w(t+1) =(cid:29)c + (1− c)× 1√λ(cid:29)w(cid:29)(cid:28)v
6:
7:
8:
9:
10:
11:
12: OUTPUT w(t+1)
The second change is due to a Euclidean projection in
Line 11, where v is multiplied by the minimum of the
two values. In the oblivious version, we ensure that both
values participate in the update of the model, again us-
ing our oblivious primitives. The modifications above
are simple and, if the data is shuffled offline, asymptoti-
cally do not add overhead as the algorithm has to perform
prediction for every value in the sample. Otherwise, the
overhead of sorting is amortized as T is usually set to at
least one.
Theorem 2. The SVM algorithm described above runs
in time O(n(logn)2) and is data-oblivious, as there exists
a simulator for SVM that depends only on T , n, d, λ
and l, where d is the number of features in each input
instance.
The simulator can be constructed by composing a sim-
ulator for oblivious sorting and one that follows the steps
of Algorithm 1.
We note that the oblivious computation of a label in
the training algorithm ((cid:22)w,x(cid:21) in Line 6 in Algorithm 1)
can be used also for the prediction phase of SVM.
4.4 Neural Networks
Feedforward neural networks are classic models for pat-
tern recognition that process an observation using a se-
quence of learned non-linear transformations [10]. Re-
cently, deep neural networks made significant progress
on difficult pattern recognition applications in speech, vi-
sion, and natural language understanding and the collec-
tive set of methods and models is known as deep learn-
ing [26].
Formally, a feedforward neural network is a sequence
of transformations f (x) = ft (. . . f2( f1(x))), where each
transformation fi is described by a fixed family of trans-
formations and a parameter wi to identify one particular
element in that family. Learning a neural network means
to find suitable parameters by minimization of the learn-
ing objective (1).
To minimize (1) efficiently in the context of neural
networks, we use stochastic gradient methods (SGD) on
small subsets of training data [26].
In particular, for
l (cid:18) n, say l = 32, we compute a parameter gradient on
a subset S ⊂ {1,2, . . . ,n}, |S| = l of the data as
∇wΩ(w) +
1
l ∑
i∈S
∇wL(yi, f (xi)).
(2)
The expression above is an unbiased estimate of the gra-
dient of (1) and we can use it to update the parameters
via gradient descent. By repeating this update for many
subsets S we can find parameters that approximately min-
imize the objective. Instead, as for SVM, we use disjoint
subsets that are contiguous within the set of all (oblivi-
ously or privately) shuffled instances and iterate T times.
Because most neural networks densely process each
input instance, memory access patterns during training
and testing do not depend on the particular data instance.
There are two exceptions. First, the initialization of a
vector with |Y | ground truth labels depends on the true
label yi of the instance (recall that Y is the set of pos-
sible prediction classes or labels). In particular, the yith
entry is set, for example, to 1 and all other entries to 0.
We initialize the label vector and hide the true label
of the instance by using our oblivious comparison and
move operations. The second exception is due to special
functions that occur in certain fi, for example in tanh-
activation layers. Since special functions are relatively
expensive, they are usually evaluated using piecewise ap-
proximations. Such conditional computation may leak
parameter values and, in our adapted algorithm, we in-
stead compute the approximation obliviously using a se-
quence of oblivious move operations. Neither of these