classiﬁer as having been written by User2.
Next, the rows of the 26 confusion matrices were re-
arranged to make 52 confusion charts, one for each sub-
ject. We extract User1’s row from the “A” confusion ma-
trix, to become row 1 of User1’s chart; this operation is
repeated on letters “B”–“Z”, for successive rows. Charts
for other subjects were constructed similarly. User1’s
confusion chart is a 26 letters by 52 users matrix in
which the cell in row 1, column 2 shows the fraction of
User1’s “A”s incorrectly classiﬁed as belonging to User2.
Of special note is the genuine subject’s column, which
contains letter-accuracy scores. These scores show how
frequently the genuine writer was correctly identiﬁed as
him/herself on a given letter. The rows of each confu-
sion chart were sorted in decreasing order by the letter-
accuracy scores; ties were broken at random. Now, row 1
of User1’s chart shows the highest-scoring letter for that
user, in terms of correct classiﬁcations; it might be tied in
score with some other letter(s). Table 5 shows an abbre-
viated example confusion chart.
The higher a letter-accuracy score, the better this let-
ter should be (in isolation) at distinguishing the genuine
subject from all others in the group. Letters having a
score of 1 are perfect; all others are non-perfect. In Table
5, only “E” is perfect. Two letters are complementary if
they have no errors in common, with respect to the same
other subject. Three or more letters are complementary
if the group is pairwise complementary. In Table 5, “D”
and “N” both have errors with respect to User3, so they
are not complementary. In contrast, “D”, “B”, and “G”
each have errors with respect to a different user, so they
are all complementary with respect to each other.
6.3. Letter lists and challenge strings
A challenge string is a set of letters selected to dis-
criminate one user from every other member of the group.
Longer strings provide greater discrimination ability and
security, while shorter strings are more convenient to
write. A letter list is an ordered list of elements (called
units) that are either single letters or complementary let-
ter pairs. A letter list holds all possible lengths of chal-
lenge strings for a given user. A challenge string is real-
ized by including letters from the ﬁrst u units on a letter
list. Increasing the number of units, u, should generally
maintain—and may enhance—accuracy (and security).
An abbreviated example of a letter list appears in the
bottom row of Table 3; it contains the letters “E DB G
NR”. Challenge strings are formed by including letters
from the leftmost cells, up to a desired stopping point.
“E” is the shortest possible challenge string; others are
“EDB”, “EDBG”, and “EDBGNR”.
Table 3. User1’s letter list (example)
Perfect
Unit 1
Run 1
Unit 2
Unit 3
E
D B
G
Run 2
Unit 4
N R
To form successive units on a letter list, an algorithm
selects a single letter or letter pair from the subject’s con-
fusion chart. Units are added without backtracking; let-
ters are selected according to the heuristics in Table 4.
The terms perfect, non-perfect, and complementary were
deﬁned in Section 6.2; a run is a group of complementary
non-perfect letters. For convenience, we will call letters
that have not yet been added to the letter list unused. Ties
in letter-accuracy scores were decided randomly during
chart sorting, so the topmost letter in the chart is favored
during ties. We created one letter list per subject.
Example. A scenario involving four users and seven
letters will be used to illustrate the process of creating
a letter list. Table 5 shows a confusion chart for User1,
whose identity is underlined. The “User1” column con-
tains sorted letter-accuracy scores (shown in boldface).
The algorithm ﬁrst looks for perfect letters, placing
each within its own unit. In Table 5, “E” is perfect; it
forms the ﬁrst unit on User1’s letter list (see Table 3).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007After exhausting perfect letters, the algorithm looks
for the highest-scoring non-perfect letter, which is “D”.
This letter has foreseeable deﬁciencies, namely its errors
with respect to User3. Before adding “D” to the letter
list, the algorithm seeks to pair it with a complementary
letter (whose errors do not involve User3). The highest-
scoring letter complementary to “D” is “B”, whose errors
involve User2. “D” and “B” are paired together into the
same unit, which starts the ﬁrst run of the letter list.
Next, the algorithm seeks to complete the current
run, by successively adding single letters complementary
to all those currently in the run. “G” (whose errors in-
volve User4) is the only remaining unused letter comple-
mentary to both “D” and “B”. “G” is added alone to a
new unit in the current run, which is then terminated. No
memory of errors is retained from one run to the next.
Now a new non-perfect pair is sought, starting with
the highest-scoring unused letter, “N”. The only other
unused letter complementary to “N” is “R”, so those two
form a unit and start the second run on the list. The re-
maining unused letter, “H”, is not complementary to “N”
and “R”; thus, the second run is ﬁnished. “H” is left off
the letter list, since it cannot be paired. The completed
letter list appears in Table 3; possible challenge strings
are “E”, “EDB”, “EDBG”, and “EDBGNR”.
The reason that non-perfect letters are only added in
the context of a pair (or a run) is to prevent a succession
of letters being added whose errors all involve the same
user. Such an occurrence would raise the chances of the
biometric system erroneously predicting the writer of a
challenge string to be that user.
Procedure. A summary of the algorithm follows.
Table 4. Heuristics for creating letter lists
1. A given letter appears at most once on the letter list.
2. Perfect letters are considered equally useful, and supe-
rior to non-perfect letters. Perfect letters are added to the
letter list before non-perfect letters.
3. A non-perfect letter is considered superior to another
non-perfect letter, if its letter-accuracy score is higher.
All other things being equal, a superior non-perfect letter
is added before an inferior one.
4. A non-perfect letter can only be added to the letter list if
a complementary letter accompanies it.
5. After a non-perfect letter pair is added to the list, other
complementary letters are sought immediately to com-
plete a run. The purpose of this is to further remedy
deficiencies in the non-perfect letters.
6. Units are kept as short as possible, to provide granularity
in security levels.
Table 5. User1’s confusion chart (example)
Letter
User1
User2
User3
User4
E
D
N
B
R
G
H
1
0.9
0.8
0.8
0.7
0.7
0.6
0
0
0
0.2
0.1
0
0.1
0
0.1
0.2
0
0
0
0.2
0
0
0
0
0.2
0.3
0.1
ALGORITHM TO CREATE A LETTER LIST:
(1) Add perfect letters one at a time, each within its own
unit, to the subject’s letter list.
(2) Add non-perfect letters that ﬁt into runs, one run at a
time, until no more runs can be formed.
HOW TO MAKE A RUN:
(A) Find a complementary letter pair,1 if one exists. If
so, add both letters to a unit, the ﬁrst unit of this run.
If not, then no more runs can be formed.
(B) Add single complementary letters to the run, one at
a time and each within its own unit, until none are
left; then, the run is ﬁnished, and its units are added
to the letter list. Each complementary letter must
be complementary to all letters already in that run.
If more than one letter choice exists in an iteration,
prefer the one with the highest letter-accuracy score.
6.4. Decision logic to combine classiﬁer outputs
When an SVM classiﬁer is tested on a letter instance,
it outputs a writer prediction. For a multi-letter challenge
string, multiple classiﬁers are involved, so decision logic
is required to combine the outputs into a single predic-
tion. When a writing sample is submitted to the biomet-
ric system, appropriate SVM letter classiﬁers are invoked
to process the relevant letters. A single classiﬁer deter-
mines the probability that each of the 52 users wrote one
letter instance. If a challenge string has s unique letters,
then s classiﬁers are invoked, each producing 52 prob-
ability scores. Next, the s probability scores for User1
are multiplied together under the independence assump-
tion, to produce a joint probability that User1 wrote all
the letters. Joint probabilities are then found for User2,
and for all the other users. The single user having the
highest joint probability is deemed the most likely writer.
Because the probabilities are assumed independent, the
order of the letters does not affect the calculation; chal-
lenge strings could be permuted to thwart replay attacks.
1Choose a complementary pair in the following way. Take the
highest-scoring unused letter as the provisional first member of the pair.
For the second member, take the next-highest scoring letter complemen-
tary to the first. If none can be found, try a different provisional first
member of the pair (having the next-highest score). Start the search
again, and continue until a complementary pair is found, or until there
are no more candidates to be the first provisional member of the pair.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20077. Evaluation method
The evaluation aims to discover whether users can be
discriminated on the basis of handwriting samples, and if
so, to ﬁnd out how long the samples must be. We varied
the number of units appearing in challenge strings (re-
call Table 3), and observed system accuracy; extra units
should not decrease accuracy (unlike solitary letters).
Three experiments were conducted, each on a differ-
ent version of the data, to discover how data quality and
quantity affect accuracy: (1) High-quality Data, (2) Re-
duced Data, and (3) All Data (see Sections 4 and 5.4). We
performed an off-line evaluation, in which subject par-
ticipation was simulated by data in the corpus. Individ-
ual letters written by a subject were assembled into a se-
quence meant to imitate how that subject would respond
to a challenge string. The previously unseen Evaluate-
test data was used for system evaluation.
7.1. Transaction design and error deﬁnitions
At transaction time, a user (the claimant) claims an
identity; the system responds by displaying the challenge
string tailored to that identity. After the user writes the
string, the identity claim is accepted or rejected.
Genuine transactions.
In genuine transactions, a
subject claims his or her true identity. To simulate these
events, we used the challenge string assigned to that sub-
ject, and called up instances of his or her letters from the
Evaluate-test data set. The biometric system predicted
the most likely writer.
If the predicted writer identity
matched the genuine one, no error occurred; otherwise,
there was an error. Errors made on genuine transactions
contributed to the false non-match rate (the rate at which
genuine claimants are denied entry by the system).
Imposter transactions. In imposter transactions, a
subject claims an identity other than his or her own. To
simulate these events, we used the challenge string as-
signed to the victim, along with letter instances written
by the impostor (from the Evaluate-test data set). The
biometric system determined the most likely writer. If the
predicted writer identity matched the claimed one, there
was an error, because the imposter posed successfully as
another subject. Otherwise there was no error, because
the imposter failed. Errors made on imposter transactions
contributed to the false-match rate (the rate at which im-
poster claimants are allowed entry by the system).
7.2. Challenge-string evaluation
The 26 SVM classiﬁers were newly re-trained on all
previously seen data, namely SVM-train and SVM-test
(collectively called Evaluate-train). All best parameter
values from Section 6.1 were kept; 75% of the data was
used for re-training. The remaining 25% of the data held
in Evaluate-test, previously unseen, was used to eval-
uate challenge strings. Due to the numerous genuine
and imposter transactions, some reuse of data within in
Evaluate-test occurred, as speciﬁed below.
Within each of the three experiments (see Section
4), the number of units, u, was varied from one to the
largest possible number (equaling the number of units in
the shortest letter list). In a round, all challenge strings
have a particular value of u. Between rounds, data were
reused, without memory of prior usage.
Within a round, multiple repetitions of transactions
were performed, and the error rates were averaged to in-
crease the stability of results. We repeated each trans-
action as many times as unused data remained, but held
constant the number of repetitions for all subjects. In the
High-quality Data and Reduced Data experiments, the
limiting number of instances per letter was 13; it was
44 in the All Data experiment. Since Evaluate-test con-
tained 25% (or slightly less2) of each subject’s letter in-
stances, 3 instances of that letter appeared in Evaluate-
test for the High-quality Data and Reduced Data experi-
ments ((cid:1)13/4(cid:2)=3), whereas 11 appeared in Evaluate-test
for the All Data experiment ((cid:1)44/4(cid:2)=11). In the respec-
tive experiments, repetitions were limited to those num-
bers, for all subjects. Within a round, the same letter in-
stance was never re-used within genuine transactions, or