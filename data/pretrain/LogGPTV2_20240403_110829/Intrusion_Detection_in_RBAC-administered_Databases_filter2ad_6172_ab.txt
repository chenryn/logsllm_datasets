words, the overall classiﬁer is robust to serious deﬁciencies
of its underlying naive probability model [7]. Additionally,
the probabilistic nature of the model enables us to raise an
alarm when the probability of a user, acting according to the
role he is claiming to have, is low.
In the sequel, we describe the general principles of the
Naive Bayes classiﬁer (for details see [18]) and then we
show how they can be applied to our setting. In the super-
vised learning case each instance x of the data is described
as a conjunction of attribute values, and the target function
f(x) can only take values from some ﬁnite set V . Appar-
ently, the attributes correspond to the set of observations and
the elements of V are the distinct classes observed. In the
classiﬁcation problem, a set of training examples DT is pro-
vided, and a new instance with attribute values (a1, ..., an)
is given. The goal is to predict the target value, or the class,
of this new coming instance.
The approach we describe here is to assign to this new
instance the most probable class value vM AP , given the at-
tributes (a1, ..., an) that describe it:
vM AP = arg max
vj∈V
P (vj|a1, a2, ..., an).
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:10:32 UTC from IEEE Xplore.  Restrictions apply. 
SQL Command
SELECT A1, B1
FROM R1
SELECT R1.A1, R1.C1, R2.B2, R2.D2
FROM R1, R2
WHERE R1.E1 = R2.E2
c-triplet
select  select  select 
m-triplet
f-triplet
select  select  select 
Table 1. Triplets construction example
Using Naive Bayes Theorem we can rewrite the expression:
vM AP = arg max
vj∈V
= arg max
vj∈V
= arg max
vj∈V
P (vj|a1, a2, ..., an)
P (a1, a2, ..., an|vj)P (vj)
P (a1, a2, ..., an|vj)P (vj).
P (a1, a2, ..., an)
The last derivation is feasible because the denominator does
not depend on the choice of vj and thus it can be omit-
ted from the arg max argument. Estimating p(vj) is easy
since it requires just counting its frequency in the training
data. However calculating P (a1, a2, ..., an|vj) is not that
easy considering a large dataset and a reasonably large num-
ber of attributes. The Naive Bayes classiﬁer is based on the
simplifying assumption that the attribute values are condi-
tionally independent. In this case:
vM AP = arg max
vj∈V
P (vj)
(cid:1)
i
P (ai|vj)
(1)
The computational cost is thus reduced signiﬁcantly be-
cause calculating each one of the P (ai|vj) requires a fre-
quency counting over the tuples in the training data with
class value equal to vj.
The conditional
independence assumption seems to
solve the computational cost. However, there is another
issue that needs to be discussed. Assume an event E oc-
curring nEj number of times in the training dataset for a
particular class Cj with size |DCj| . While the observed
fraction ( nEj
|DCj| ) provides a good estimate of the probability
in many cases, it provides poor estimates when nEj is very
small. An obvious example is when nEj = 0. The cor-
responding zero probability will bias the classiﬁer in an ir-
reversible way, since according to the last equation the zero
probability when multiplied with the other probability terms
will give zero as its result. To avoid this difﬁculty we adopt
a standard Bayesian approach in estimating this probability,
using the m-estimate deﬁned as follows:
Deﬁnition 4. Given a dataset DT with size |DT| and an
event E that appears nEj times in the dataset for a class
Cj with size |DCj| and nE times in the entire dataset, then
the m-estimate of the probability pe = nEj
|DCj| is deﬁned to
be:
pm
E =
nEj + m · nE|DT |
|DCj| + m
.
The parameter m is a constant and is called equivalent sam-
ple size, which determines how heavily to weight pE relative
to the observed data. If nE is also 0, then we simply assume
the probability pm
E = 1|DCj| .
So far we have described how the Naive Bayes Classiﬁer
works in a general setting. Applying the model to our in-
trusion detection framework is rather straightforward. The
set of classes that we consider is the set of roles R in the
system while the observations are the log-ﬁle triplets.
In
the sequel, we show how equation 1 applies when the three
different types of triplets are considered.
For the case of c-triplets the application is straightfor-
ward since there are three attributes to consider namely the
command, the relation information and the attribute infor-
mation. Therefore, we have three numeric attributes c, R, A
that correspond to those three ﬁelds. If R denotes the set
of roles, predicting the role rj ∈ R of a given observation
(ci, Ri, Ai) requires, in accordance to equation ( 1):
rMAP = arg max
rj∈R P (rj)P (ci|rj)P (Ri|rj)P (Ai|rj)
In the m-triplets, we again have three ﬁelds (c, R, A),
where R and A are vectors of the same cardinality. Except
for the command attribute c, the rest of the attributes con-
sidered in this case come from the product RAT . Therefore
there are |R · AT| + 1 attributes and equation (1) can be
rewritten as follows:
rMAP = arg max
rj∈R P (rj)
N(cid:1)
i=1
p(R · AT [i]|rj).
Finally, in the case of f-triplets, where ﬁelds R and A are
vectors and vectors of vectors the corresponding equation
is:
rMAP = arg max
rj∈R P (rj)
N(cid:1)
i=1
p(R[i] · A[i]|rj).
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:10:32 UTC from IEEE Xplore.  Restrictions apply. 
Now with the above deﬁnitions in place, the ID task is
rather straightforward. For every new query, its rMAP is
predicted by the trained classiﬁer. If this rMAP is different
from the original role associated with the query, an alarm is
raised.
The procedure for ID can easily be generalized for the
case when a user can exercise more than one role at a time.
This is because our method detects intruders on a per query
basis and not on a per user basis; hence, as long as the role
associated with the query is consistent with the role pre-
dicted by the classiﬁer, the system will not raise an alarm.
4. Experimental evaluation
In this section, we report results from our experimental
evaluation of the proposed approach and illustrate its per-
formance as an ID mechanism. Our experimental setting
consists of experiments with both synthetic and real data
sets. The objective of this evaluation is two-fold. First,
we present results comparing the behavior of our classiﬁer
when log ﬁles are modeled using the three different triplet
representations. Second, we measure the performance of
our classiﬁer in terms of the computational cost associated
with the training and detection phases. Before describing
our experimental ﬁndings, we give a brief outline of the data
sets and the quality measures we use for our evaluation.
4.1. Quality Measures
For the experimental evaluation, we analyze the quality
of results of our approach as an ID mechanism using the
standard measures of Precision and Recall. The precision
and recall statistics are deﬁned as follows:
P recision =
# True Positives
# True Positives + # False Positives
Recall =
# True Positives
# True Positives + # False Negatives
Here, # False Positives is the number of false alarms
while # False Negatives is the number of times the system
is not able to detect the anomalous queries.
4.2. Data sets
Synthetic data sets: The synthetic data are generated ac-
cording to the following model: Each role r has a probabil-
ity, p(r), of appearing in the log ﬁle. Additionally, for each
role r the generating model speciﬁes the following three
probabilities: (i) the probability of using a command cmd
given the role, p(cmd|r), (ii) the probability of accessing a
table T given the role, p(T|r) and (iii) the probability of ac-
cessing an attribute within a table a ∈ T given the role and
the table p(a|r, T ).
Real Data set: The real data set used for evaluating our
approach consists of over 6000 SQL traces from eight dif-
ferent applications submitting queries to a MS SQL server
database. The database itself consists of 119 tables with
1142 attributes in all. For more detailed description of the
data set we refer the reader to [25].
Intruder Queries generation: The intruder/anomalous
queries are generated by taking into account the insider
threat scenario. They are taken from the same probability
distribution as of normal queries, but with role information
negated. For example, if the role information associated
with a normal query is 0, then we simply change the role to
any role other than 0 to make the query anomalous.
4.3. Results
Test Cases 1, 2 and 3 show the relative accuracy of our
classiﬁer with respect to the three triplet representations.
Test Case 1 (Figure 2) shows the inferior performance of
c-triplets compared to the other two alternatives. In this test
case, each role issues queries accessing approximately an
equal number of columns but from different tables. For this
reason, the queries when modeled by the c-triplets show a
lot of homogeneity across the roles. Hence, the classiﬁer is
not effective in distinguishing between queries across dif-
ferent roles. This results in the low precision and recall
values for c-triplets. Test Case 2 (Figure 3) establishes the
superiority of f-triplets over c and m-triplets. In this case,
there is a partial overlap in the table access pattern of the
queries issued by the various roles. In addition to this, each
role accesses only a subset of the columns within a table
with a high probability. In such a scenario, f-triplets per-
form the best because they take into account the column ac-
cess pattern of the queries within a table, unlike m-triplets
which only take into account the number of columns ac-
cessed per table and c-triplets which only maintain a count
of total number of columns and tables accessed. Also, the
performance of all three triplet types improves with increas-
ing amount of training data. Test Case 3 (Figure 4) is a vari-
ant of Test Case 2 with varying number of roles and constant
training data (1000 tuples). As expected, f-triplets give the
best results in this case as well.
Finally, we tested our classiﬁer on the real data set. For
this experiment, we consider 6602 SQL traces from 4 dif-
ferent applications as the training data for the classiﬁer. Ta-
bles 2 and 3 show the quality of the results for the three
different triplet types. Not surprisingly, high quality is
achieved for all triplet types.
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:10:32 UTC from IEEE Xplore.  Restrictions apply. 
)
i
%
(
n
o
s
c
e
r
P
i
)
i
%
(
n
o
s
c
e
r
P
i
)
i
%
(
n
o
s
c
e
r
P
i
 120
 100
 80
 60
 40
 20
c-triplet
m-triplet
f-triplet
 120
 100
 80
 60
 40
 20
)
%
(
l
l
a
c
e
R
c-triplet
m-triplet
f-triplet
 0
 100  200  300  400  500  600  700  800  900  1000
 0
 100  200  300  400  500  600  700  800  900  1000
Number of training tuples
Number of training tuples
Figure 2. Test Case 1: Precision and Recall statistics
 120
 100
 80
 60
 40
 20
c-triplet
m-triplet
f-triplet
 120
 100
 80
 60
 40
 20
)
%
(
l
l
a
c
e
R
c-triplet
m-triplet
f-triplet
 0
 100  200  300  400  500  600  700  800  900  1000
 0
 100  200  300  400  500  600  700  800  900  1000
Number of training tuples
Number of training tuples
Figure 3. Test Case 2: Precision and Recall statistics
 120
 100
 80
 60
 40
 20
 0
 1
c-triplet
m-triplet
f-triplet
c-triplet
m-triplet
f-triplet
 120
 100
 80
 60
 40
 20
)
%
(
l
l
a
c
e