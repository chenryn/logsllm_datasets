Application
Per-packet aggregation
Congestion Control [22, 29, 40, 46]
Congestion Analysis [17, 38, 57]
Network Tomography [26]
Power Management [31]
Real-Time Anomaly Detection [66, 86] Detect sudden changes in network status
Static per-flow aggregation
Path Tracing [36, 56, 65, 72]
Routing Misconfiguration [45, 69, 72]
Path Conformance [45, 69, 73]
Dynamic per-flow aggregation
Utilization-aware Routing [2, 41, 42]
Load Imbalance [45, 65, 73]
Network Troubleshooting [36, 57, 73]
Measurement Primitives
timestamp, port utilization, queue occupancy
queue occupancy
switchID, queue occupancy
switchID, port utilization
timestamp, port utilization, queue occupancy
Detect the path taken by a flow or a subset
switchID
Identify unwanted path taken by a given flow switchID
Checks for policy violations.
switchID
Load balance traffic based on network status.
Determine links processing more traffic.
Determine flows experiencing high latency.
switchID, port utilization
switchID, port utilization
switchID, timestamp
Table 2: Use cases enabled by PINT, organized per aggregation mode.
jumbo frames; for example, RDMA over Converged Ethernet NICs
provides an MTU of only 1KB [54].
2. Switch processing time. In addition to consuming bandwidth,
the INT overhead also affects packet processing time at switches.
Every time a packet arrives at and departs from a switch, the bits
carried over the wire need to be converted from serial to parallel and
vice versa, using the 64b/66b (or 66b/64b) encoding as defined by
the IEEE Standard 802.3 [33]. For this reason, any additional bit
added into a packet affects its processing time, delaying it at both in-
put and output interfaces of every hop. For example, adding 48 bytes
of INT data on a packet (INT header alongside two telemetry infor-
mation) would cause a latency increase with respect to the original
packet of almost 76ns and 6ns for 10G and 100G interfaces, respec-
tively3. On a state-of-the-art switch with 10G interfaces, this can
represent an approximately 3% increase in processing latency [60].
On larger topologies and when more telemetry data is needed, the
overhead on the packet can cause an increase of latency in the order
of microseconds, which can hurt the application performance [61].
3. Collection overheads. Telemetry systems such as INT generate
large amounts of traffic that may overload the network. Additionally,
INT produces reports of varying size (depending on the number of
hops), while state-of-the-art end-host stack processing systems for
telemetry data, such as Confluo [43], rely on fixed-byte size headers
on packets to optimize the computation overheads.
3 THE PINT FRAMEWORK
We now discuss the supported functionalities of our system, formal-
izing the model it works in.
Telemetry Values. In our work, we refer to the telemetry informa-
tion as values. Specifically, whenever a packet pj reaches a switch s,
we assume that the switch observes a value v(pj , s). The value can
be a function of the switch (e.g., port or switch ID), switch state
(e.g., timestamp, latency, or queue occupancy), or any other quantity
computable in the data plane. In particular, our definition supports
the information types that INT [75] can collect.
3Consuming 48Bytes on a 10G interface requires 6 clock cycles each of them burning 6.4 ns [83].
On a 100G interface, it needs just one clock cycle of 3ns [84].
3
3.1 Aggregation Operations
We design PINT with the understanding that collecting all (per-
packet per-switch) values pose an excessive and unnecessary over-
head. Instead, PINT supports several aggregation operations that
allow efficient encoding of the aggregated data onto packets. For ex-
ample, congestion control algorithms that rely on the bottleneck link
experienced by packets (e.g., [46]) can use a per-packet aggregation.
Alternatively, applications that require discovering the flow‚Äôs path
(e.g., path conformance) can use per-flow aggregation.
i =1.
aggregation, the target quantity is max(cid:8)v(pj , si)(cid:9)k
‚Ä¢ Per-packet aggregation summarizes the data across the different
values in the packet‚Äôs path, according to an aggregation func-
tion (e.g., max/min/sum/product). For example, if the packet
pj traverses the switches s1, s2, . . . , sk and we perform a max-
‚Ä¢ Static per-flow aggregation targets summarizing values that may
differ between flows or switches, but are fixed for a (flow, switch)
pair. Denoting the packets of flow x by p1, . . . , pz, the static prop-
erty means that for any switch s on x‚Äôs path we have v(p1, s) =
. . . = v(pz , s); for convenience, we denote v(x, s) ‚âú v(p1, s). If the
path taken by x is s1, . . . , sk , the goal of this aggregation is then to
compute all values on the path, i.e., v(x, s1), v(x, s2), . . . , v(x, sk).
As an example, if v(x, si) is the ID of the switch si , then the
aggregation corresponds to inferring the flow‚Äôs path.
‚Ä¢ Dynamic per-flow aggregation summarizes, for each switch on
a flow‚Äôs path, the stream of values observed by its packets. Denote
by p1, . . . , pz the packets of x and by s1, . . . , sk its path, and let
sequence of values measured by si on x‚Äôs packets be denoted as
Sx,i = ‚ü®v(p1, si), v(p2, si), . . . v(pz , si)‚ü©. The goal is to compute a
function of Si,x according to an aggregation function (e.g., median
or number of values that equal a particular value v). For example,
if v(pj , si) is the latency of the packet pj on the switch si , using the
median as an aggregation function equals computing the median
latency of flow x on si .
3.2 Use Cases
PINT can be used for a wide variety of use cases (see Table 2). In
this paper, we will mainly discuss three of them, chosen in such a way
that we can demonstrate all the different PINT aggregations in action.
Per-packet aggregation: Congestion Control. State of the art con-
gestion control solutions often use INT to collect utilization and
queue occupancy statistics [46]. PINT shows that we can get similar
or better performance while minimizing the overheads associated
with collecting the statistics.
Static per-flow aggregation: Path Tracing. Discovering the path
taken by a flow is essential for various applications like path confor-
mance [45, 69, 73]. In PINT, we leverage multiple packets from the
same flow to infer its path. For simplicity, we assume that each flow
follows a single path.
Dynamic per-flow aggregation: Network Troubleshooting. For
diagnosing network issues, it is useful to measure the latency quan-
tiles from each hop [17, 34, 36, 57]. Tail quantiles are reported as
the most effective way to summarize the delay in an ISP [19]. For
example, we can detect network events in real-time by noticing a
change in the hop latency [9]. To that end, we leverage PINT to
collect the median and tail latency statistics of (switch, flow) pairs.
3.3 Query Language
Each query in PINT is defined as a tuple ‚ü®val_t, agg_t, bit-budget,
optional: space-budget, flow definition, frequency‚ü© that specifies
which values are used (e.g., switch IDs or latency), the aggrega-
tion type as in Section 3.1, and the query bit-budget (e.g., 8 bits
per packet). The user may also specify a space-budget that deter-
mines how much per-flow storage is allowed, the flow-definition
(e.g., 5-tuple, source IP, etc.) in the case of per-flow queries, and
the query frequency (that determines which fraction of the packets
should be allocated for the query).
PINT works with static bit-budgets to maximize its effectiveness
while remaining transparent to the sender and receiver of a packet.
Intuitively, when working with INT/PINT one needs to ensure that
a packet‚Äôs size will not exceed the MTU even after the telemetry
information is added. For example, for a 1500B network MTU, if the
telemetry overhead may add to X bytes, then the sender would be
restricted to sending packets smaller than 1500‚àíX . Thus, by fixing
the budget, we allow the network flows to operate without being
aware of the telemetry queries and path length.
3.4 Query Engine
PINT allows the operator to specify multiple queries that should
run concurrently and a global bit-budget. For example, if the global
bit-budget is 16 bits, we can run two 8-bit-budget queries on the
same packet. In PINT, we add to packets a digest ‚Äì a short bitstring
whose length equals the global bit budget. This digest may compose
of multiple query digests as in the above example.
Each query instantiates an Encoding Module, a Recording Mod-
ule, and an Inference Module. The Encoding runs on the switches
and modifies the packet‚Äôs digest. When a packet reaches a PINT
Sink (the last hop on its path), the sink extracts (removes) the di-
gest and sends the data packet to its destination. This way, PINT
remains transparent to both the sender and receiver. The extracted
digest is intercepted by the Recording Module, which processes and
Figure 3: PINT‚Äôs architecture: The Query Engine decides on an
execution plan that determines the probability of running each
query set on packets and notifies the switches. The first hop, PINT
Source, adds a digest whose size is determined by the user. Ev-
ery switch along the path may modify the digest but does not add
bits. The last hop, PINT Sink, removes the collected telemetry in-
formation and sends it to the Recording Module. On demand, the
Inference Module is invoked to analyze the recorded data.
stores the digests. We emphasize that the per-flow data stored by the
Recording Module sits in an offline storage and no per-flow state is
stored on the switches. Another advantage of PINT is that, compared
with INT, we send fewer bytes from the sink to be analyzed and
thereby reduce the network overhead. The Inference Module runs
on a commodity server that uses the stored data to answer queries.
Fig. 3 illustrates PINT‚Äôs architecture.
Importantly, all switches must agree on which query set to run on
a given packet, according to the distribution chosen by the Query
Engine. We achieve coordination using a global hash function, as
described in Section 4.1. Unlike INT, we do not add a telemetry
header; in this way we minimize the bit overhead.4 Instead, the
PINT Query Engine compiles the queries to decide on the execution
plan (which is a probability distribution on a query set, see Fig. 3)
and notifies the switches.
3.5 Challenges
We now discuss several challenges we face when designing algo-
rithms for PINT.
Bit constraints. In some applications, the size of values may be
prohibitively large to a point where writing a single value on each
packet poses an unacceptable overhead.
Switch Coordination. The switches must agree on which query
set to use for each packet. While the switches can communicate
by exchanging bits that are added to packets, this increases the
bit-overhead of PINT and should be avoided.
Switch constraints. The hardware switches have constraints, in-
cluding limited operations per packet, limited support for arith-
metic operations (e.g., multiplication is not supported), inability
to keep per-flow state, etc. See [12] for a discussion of the con-
straints. For PINT, these constraints mean that we must store mini-
mal amount of state on switches and use simple encoding schemes
that adhere to the programmability restrictions.
4We note that removing the header is minor compared to the overhead saving PINT
obtains by avoiding logging all per-hop values.
4
QueriesùëÑ1,ùëÑ2,‚Ä¶Max overheadQuery EngineQuery SetProbability{ùëÑ2}0.4{ùëÑ3}0.3{ùëÑ1,ùëÑ4}0.3Execution PlanPINT Sink PINT Source SenderReceiverStorageInference ModuleFixed-width PINT digestPacketHeaderPacket Payload4 AGGREGATION TECHNIQUES
In this section, we present the techniques used by PINT to over-
come the above challenges. We show how global hash functions
allow efficient coordination between different switches and be-
tween switches and the Inference Module. We also show how dis-
tributed encoding schemes help reduce the number of packets needed
to collect the telemetry information. Finally, we adopt compres-
sion techniques to reduce the number of bits required to represent
numeric values (e.g., latency).
Our techniques reduce the bit-overhead on packets using proba-
bilistic techniques. As a result, some of our algorithms (e.g., latency
quantile estimation) are approximate, while others (e.g., path tracing)
require multiple packets from the same flow to decode. Intuitively,
oftentimes one mostly cares about tracing large (e.g., malicious)
flows and does not require discovering the path of very short ones.
Similarly, for network diagnostics it is OK to get approximated
latency measurements as we usually care about large latencies or
significant latency changes. We summarize which techniques apply
for each of the use cases in Table 3.
Use Case
Global Hashes Distributed Coding Value Approximation
Congestion Control
Path Tracing
‚úó
‚úì
‚úì
Latency Quantiles
Table 3: A summary of which techniques are used for each use case.
‚úó
‚úì
‚úó
‚úì
‚úó
‚úì
4.1
Implicit Coordination via Global Hash
Functions
In PINT, we extensively use global hash functions to determine
probabilistic outcomes at the switches. As we show, this solves the
switch coordination challenge, and also enables implicit coordination
between switches and the Inference Module ‚Äì a feature that allows
us to develop efficient algorithms.
Coordination among switches. We use a global (i.e., that is known
to all switches) hash function to determine which query set the
current packet addresses. For example, suppose that we have three
queries, each running with probability 1/3, and denote the query-
selection hash, mapping packet IDs to the real interval5 [0, 1], by
q. Then if q(pj) < 1/3, all switches would run the first query, if
q(pj) ‚àà [1/3, 2/3] the second query, and otherwise the third. Since
all switches compute the same q(pj), they agree on the executed
query without communication. This approach requires the ability to
derive unique packet identifiers to which the hashes are applied (e.g.,
IPID, IP flags, IP offset, TCP sequence and ACK numbers, etc.). For
a discussion on how to obtain identifiers, see [21].
Coordination between switches and Inference Module. The In-
ference Module must know which switches modified an incoming
packet‚Äôs digest, but we don‚Äôt want to spend bits on encoding switch
IDs in the packet. Instead, we apply a global hash function –¥ on a
(packet ID, hop number)6 pair to choose whether to act on a packet.
5For simplicity, we consider hashing into real numbers. In practice, we hash into M
bits (the range {0, . . . , 2M ‚àí 1}) for some integer M (e.g., M = 64). Checking if
the real-valued hash is in [a, b] corresponds to checking if the discrete hash is in the
interval(cid:2)(cid:4)(2M ‚àí 1) ¬∑ a(cid:5) ,(cid:4)(2M ‚àí 1) ¬∑ b(cid:5)(cid:3).
6The hop number can be computed from the current TTL on the packet‚Äôs header.
This enables the PINT Recording Module to compute –¥‚Äôs outcome
for all hops on a packet‚Äôs path and deduct where it was modified.
This coordination plays a critical role in our per-flow algorithms
as described below.
Example #1: Dynamic Per-flow aggregation. In this aggregation,
we wish to collect statistics from values that vary across packets,