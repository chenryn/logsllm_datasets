-
1/N
1/N
1/N
0
0
Err. ∆ρ (·10−1)
0
∼ 3.671
2.384 ± 0.215
2.485 ± 0.207
2.394 ± 0.216
2.634 ± 0.174
2.455 ± 0.209
Exponential (Chaudhuri et al. [13])
Table 4: Results from Exp. II: first principal component.
Method
Non-private
Random guess
ϵ
-
-
1.
1.
1.
1.
1.
1.
δ
-
-
1/N
1/N
1/N
1/N
0
0
MVG-1 (Alg. 1 + knowledge in [3])
MVG-2 (Alg. 1 + DP-PCA [8])
Gaussian (Dwork et al. [23])
JL transform (Upadhyay [86])
Laplace (Dwork et al. [24])
Exponential (Blum et al. [9])
1.624 ± 0.026
1.643 ± 0.023
1.913 ± 0.069
1.682 ± 0.015
2.482 ± 0.189
2.202 ± 0.721
Table 3: Results from Exp. I: regression. MVG-1 derives noise
directions from domain knowledge, while MVG-2 derives
them from differentially-private PCA in [8].
et al. [23], Blum et al. [9], and Upadhyay [86], for the four basic
mechanisms, respectively.
For Exp. II, we consider the 1st P.C. As this problem has been
well-investigated, we compare our approach to the state-of-the-art
algorithms specially designed for this problem. These four algo-
rithms using the four prior basic mechanisms are, respectively:
Dwork et al. [24], Dwork et al. [27], Chaudhuri et al. [13], and
Blocki et al. [7]. We note that these four algorithms chosen for com-
parison are designed and optimized specifically for the particular
application, so they utilize the positive-semidefinite (PSD) nature
of the matrix query. On the other hand, the MVG mechanism used
here is generally applicable for matrix queries even beyond the
particular application, and makes no assumptions about the PSD
structure of the matrix query. In other words, we intentionally give
a favorable edge to the compared methods to show that, despite the
handicap, the MVG mechanism can still perform comparably well.
For all previous works, we use the parameter values as suggested
by the authors of the method, and vary the free variable before
reporting the best performance.
Finally, we recognize that some of these prior works have a
different privacy guarantee from ours, namely, ϵ-differential privacy.
Nevertheless, we present these prior works for comprehensive
coverage of prior basic mechanisms, and we will keep this difference
in mind when discussing the results.
8 EXPERIMENTAL RESULTS
Table 3, Table 4, and Table 5 report the experimental results for
Experiment I, II, and III, respectively. The performance shown is an
average over 100 trials, along with the 95% confidence interval.
8.1 Experiment I: Regression
Table 3 reports the results for Exp. I. Here are the key observations.
• Compared to the non-private baseline, the best MVG mecha-
nism (MVG-1) yields similar performance (difference of .004
in RMSE).
• Compared to other (ϵ, δ)-basic mechanisms, i.e. the Gaussian
mechanism and the JL transform, the best MVG mechanism
(MVG-1) has better utility (by .003 and .0006 in RMSE, re-
spectively) with the same privacy guarantee.
• Compared to other ϵ-basic mechanisms, i.e. the Laplace and
Exponential mechanisms, the best MVG mechanism (MVG-1)
provides significantly better utility (~150%) with the slightly
weaker (ϵ, 1/N)-differential privacy guarantee.
• Even when some privacy budget is spent on deriving the di-
rection via PCA[8] (MVG-2), the MVG mechanism still yields
the best performance among all other non-MVG methods.
Overall, the results from regression show the promise of the MVG
mechanism. Our approach can outperform all other (ϵ, δ)-basic
mechanisms. Although it provides a weaker privacy guarantee
than other ϵ-basic mechanisms, it can provide considerably more
utility (~150%). As advocated by Duchi et al. [19] and Fienberg et al.
[28], this trade-off could be attractive in some settings, e.g. critical
medical or emergency situations.
has reasonably small error ∆ρ of 0.2387.
8.2 Experiment II: 1st Principal Component
Table 4 reports the results for Exp. II. Here are the key observations.
• Compared to the non-private baseline, the MVG mechanism
• Compared to other (ϵ, δ)-basic mechanisms, i.e. the Gauss-
ian mechanism and the JL transform, the MVG mechanism
provides better utility with the same privacy guarantee (.01
and .0001 smaller error ∆ρ, respectively).
• Compared to other ϵ-basic mechanisms, i.e. the Laplace
and Exponential mechanisms, the MVG mechanism yields
higher utility with slightly weaker (ϵ, 1/N)-differential pri-
vacy guarantee (.03 and .01 smaller error ∆ρ, respectively).
Overall, the MVG mechanism provides the best utility. Though we
admit that, with a weaker privacy guarantee, it does not provide
significant utility increase over the Exponential mechanism by
Chaudhuri et al. [13]. Nevertheless, this method [13] utilizes the
positive-semidefinite (PSD) characteristic of the matrix query and is
known to be among the best algorithms for this specific task. On the
other hand, the MVG mechanism used in the experiment is more
general. Furthermore, we show in the full version of this work that,
when utilizing the PSD characteristic of the query function, the
MVG mechanism can significantly outperform all three methods
being compared here [11]. Again, in some applications, this trade-
off of weaker privacy for better utility might be desirable [19, 28],
and the MVG mechanism clearly provides the best trade-off.
8.3 Experiment III: Covariance Estimation
Table 5 reports the results for Exp. III. Here are the key observations.
• Compared to the non-private baseline, the MVG mechanism
has very small RSS error of .06657.
Figure 4: Effect of noise directions on the utility (all with δ = 1/N ). (Left) Exp. I: regression on the Liver dataset. The four
directions shown put more precision budget on the following features: (a) {ALT, Y}, (b) {ALT}, (c) {Y}, (d) {ALT, AST, Y}. (Middle)
Exp. II: 1st P.C. on the Movement dataset. The four directions emphasize the following features: (a) {ANC0, ANC3}, (b) {ANC0},
(c) {ANC3}, (d) {ANC1, ANC2}. (Right) Exp. III: covariance estimation on the CTG dataset. The two directions emphasize the
two disjoint subsets of features: (a) {FHR, ASV, ALV}, (b) The rest of the features.
Method
Non-private
Random guess
MVG (Alg. 1)
RSS (×10−2)
0
∼ 12.393
ϵ
-
-
1.
1.
1.
1.
1.
δ
-
-
1/N
1/N
1/N
0
0
6.657 ± 0.193
7.029 ± 0.216
Gaussian (Dwork et al. [23])
6.718 ± 0.229
JL transform (Upadhyay [86])
7.109 ± 0.211
Laplace (Dwork et al. [24])
7.223 ± 0.211
Exponential (Blum et al. [9])
Table 5: Results from Exp. III: covariance estimation.
• Compared to other (ϵ, δ)-basic mechanisms, i.e. the Gauss-
ian mechanism and the JL transform, the MVG mechanism
provides better utility with the same privacy guarantee (.004
and .001 smaller RSS error, respectively).
• Compared to other ϵ-basic mechanisms, i.e. the Laplace and
Exponential mechanisms, the MVG mechanism gives bet-
ter utility with slightly weaker (ϵ, 1/N)-differential privacy
guarantee (.005 and .006 smaller RSS error, respectively).
Overall, the MVG mechanism provides the best utility (smallest
error). When compared to other methods with stronger privacy
guarantee, the MVG mechanism can provide much higher utility.
Again, we point out that in some settings, the trade-off of weaker
privacy for better utility might be favorable [19, 28], and our ap-
proach provides the best trade-off.
9 DISCUSSION AND FUTURE WORKS
9.1 Effect of Directional Noise on Utility
In Sec. 5.4, we discuss how the choice of noise directions can affect
the utility. Here, we investigate this effect on the obtained utility
in the three experiments. Fig. 4 depicts our results.
Fig. 4, Left, shows the direction comparison from Exp. I. We
compare four choices of directions. Direction (a), which uses the
domain knowledge (ALT) and the teacher label (Y), yields the best
result when compared to: (b) using only the domain knowledge
Method
MVG
Gaussian ([23], [27])
JL transform ([86], [7])
Laplace ([24])
Exponential ([9], [13])
Exp. I
36.2
1.0
192.7
0.4
627.2
Runtime (ms)
Exp. II
Exp. III
3.2 × 103
10.0
2.57 × 106
8.0
2.00 × 106
1.8
0.4
637.4
0.5
2,112.7
Table 6: Runtime of each method on the three experiments.
(ALT), (c) using only the teacher label (Y), and (d) using an arbitrary
extra feature (ALT+Y+AST).
Fig. 4, Middle, shows the direction comparison from Exp. II. We
compare four choices of directions. Direction (a), which makes full
use of the prior information (ANC0 and ANC3), performs best when
compared to: (b), (c) using only partial prior information (ANC0 or
ANC3, respectively), and (d) having the wrong priors completely
(ANC1 and ANC2).
Fig. 4, Right, shows the comparison from Exp. III. We compare
three choices of directions. Direction (a), which uses the domain
knowledge (FHR, ASV, ALV3), gives the best performance compared
to: (b) using the completely wrong priors (all other features), and
(c) having no prior at all (i.i.d.).
The results illustrate three key points. First, as seen in all three
plots in Fig. 4, the choice of directions has an effect on the per-
formance. Second, as indicated by Fig. 4, Right, directional noise
performs much better than i.i.d. noise. Third, as seen in Fig. 4, Left
and Middle, there may be multiple instances of directional noise
that can lead to comparable performance. The last observation
shows the promise of the notion of directional noise, as it signals
the robustness of the approach.
9.2 Runtime Comparison
Next, we present the empirical runtime comparison between the
MVG mechanism and the compared methods in Table 6. The exper-
iments are run on an AMD Opteron 6320 Processor with 4 cores
using Python 2.7, along with NumPy [77], SciPy [52], Scikit-learn
[78], and emcee [29] packages. The results show that, although the
MVG mechanism runs much slower than the Gaussian and Laplace
mechanisms, it runs much faster than the JL transform and the
Exponential mechanism.
Both observations are as expected. First, the MVG mechanism
runs slower than the i.i.d.-based Gaussian and Laplace mechanisms
because it incurs the computational overhead of deriving the non-
i.i.d. noise. The amount of overhead depends on the size of the query
output as discussed in Sec. 6.3. Second, the MVG mechanism runs
much faster than the JL transform because, in addition to requiring
SVD to modify the singular values of the matrix query and i.i.d.
Gaussian samples similar to the MVG mechanism, the JL transform
has a runtime overhead for the construction of its projection matrix,
which consists of multiple matrix multiplications. Finally, the MVG
mechanism runs much faster than the Exponential mechanism since
drawing samples from the distribution defined by the Exponential
mechanism may not be efficient.
9.3 Directional Noise as a Generalized
Subspace Projection
Directional noise provides utility gain by adding less noise in useful
directions and more in the others. This has a connection to subspace
projection or dimensionality reduction, which simply removes the
non-useful directions. The main difference between the two is that,
in directional noise, the non-useful directions are kept, although are
highly perturbed. However, despite being highly perturbed, these
directions may still be able to contribute to the utility performance.
We test this hypothesis by running two additional regression
experiments (Exp. I) as follows. Given the same two important
features (ALT & Y), we use the Gaussian mechanism [23] and the
JL transform method [86] to perform the regression task using
only these two features. With ϵ = 1 and δ = 1/N , the results are
(2.538 ± .065) × 10−2 and (2.863 ± .022) × 10−2 of RMSE, respec-
tively. Noticeably, these results are significantly worse than that
of the MVG mechanism, with the same privacy guarantee. Specifi-
cally, by incorporating all features with directional noise via the
MVG mechanism, we can achieve over 150% gain in utility over the
dimensionality reduction alternatives.
9.4 Exploiting Structural Characteristics of the
Matrices
In this work, we derive the sufficient condition for the MVG mecha-
nism without making any assumption on the query function. How-
ever, many practical matrix-valued query functions have a spe-
cific structure, e.g. the covariance matrix is positive semi definite
(PSD) [46], the Laplacian matrix [34] is symmetric. Therefore, future
works may look into exploiting these intrinsic characteristics of
the matrices in the derivation of the differential-privacy condition
for the MVG mechanism.
9.5 Precision Allocation Strategy Design
Alg. 1 and Alg. 2 take as an input the precision allocation strategy
vector θ ∈ (0, 1)m:(cid:12)(cid:12)θ(cid:12)(cid:12)1 = 1. Elements of θ are chosen to emphasize
how informative or useful each direction is. The design of θ to
optimize the utility gain via the directional noise is an interesting
topic for future research. For example, in our experiments, we use
the intuition that our prior knowledge only tells us whether the
directions are highly informative or not, but we do not know the
granularity of the level of usefulness of these directions. Hence,
we adopt the binary allocation strategy, i.e. give most precision
budget to the useful directions in equal amount, and give the rest of
the budget to the other directions in equal amount. An interesting
direction for future work is to investigate general instances when
the knowledge about the directions is more granular.
10 CONCLUSION
We study the matrix-valued query function in differential privacy,
and present the MVG mechanism that is designed specifically for
this type of query function. We prove that the MVG mechanism
guarantees (ϵ, δ)-differential privacy, and, consequently, introduce