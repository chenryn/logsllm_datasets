druid.server.http.numThreads=50//HTTP请求处理线程数
#HTTP server threads
druid.port=8083
druid.host=${IP_ADDR}//部署机器IP地址
druid.service=druid/historical
-Dfile.encoding=UTF-8
-XX:MaxDirectMemorySize=32g
-XX:MaxNewSize=6g
-XX:NewSize=6g
-Xmx12g
第4章
druid.historical.cache.useCache=false
-Djava.util.logging.manager=org.apache.logging.log4j.jul,LogManager
-Djava.io.tmpdir=var/tmp
Duser.timezone=UTC+0800
-XX:+PrintGCTimeStamps
XX:+PrintGCDetails
-XX:+UseConcMarkSweepGC
Segment加载的依据
"\：5000000000]//Segment本地加载路径与最大存储空间大小，单位为Byte
为Byte
runtime.properties:
安装与配置
---
## Page 102
#Hadoop indexing
druid.processing.numThreads=23//处理线程数
druid.processing.buffer.sizeBytes=1073741824//每个处理线程的中间结果缓存大小，单位
#Processing threads and buffers
druid.server.http.numThreads=50//HTTP请求处理线程数
#HTTP server threads
druid.indexer.task.restoreTasksOnRestart=true
druid.indexer.task.baseTaskDir=var/druid/task
druid.indexer.runner.java0pts=-server -Xmx3g -XX:MaxDirectMemorySize=24g-XX:+UseG1GC-
#Tasklaunch parameters(peons)
druid.worker.capacity=23//最大可接受处理的任务数，默认为处理器数-1
#Number of tasks per middleManager
druid.port=8091
druid.host=${IP_ADDR}//部署机器IP地址
druid.service=druid/middlemanager
-Djava.io.tmpdir=var/tmp
-XX:+PrintGCTimeStamps
2.中间管理者
-Dfile.encoding=UTF-8
-Duser.timezone=UTC+0800
-XX:+PrintGCDetails
-XX:+UseConcMarkSweepGC
Xmx64m
-Xms64m
-server
为Byte
XX:MaxGCPauseMillis=100 -Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8-Djava.util.
jvm.config:
logging.manager=org.apache.logging.log4j.jul.LogManager
runtime.properties:
Druid实时大数据分析原理与实践
---
## Page 103
4.6小结
druid.indexer.task.defaultHadoopCoordinates=["org.apache.hadoop:hadoop-client:2.2.0"] //
读者能在实践中找到适合具体业务的解决方案。
置。下一步就可以基于搭建好的服务，即刻动手去尝试Druid在数据分析方面的能力，希望
配置方面还有很多值得结合实际场景探索挖掘的地方，故没有最优的配置，只有最适合的配
在最后给出了一种在生产环境中搭建Druid集群的参考方案。结合实践来说，其实Druid在
druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp
第4章
安装与配置是探索Druid的第一步。本章介绍了如何一步步搭建和运行Druid服务，并
Hadoop版本需要根据具体情况来配置
安装与配置
---
## Page 104
文件等。
5.1.1
数据源分为两种形式。
证数据一致性等，将在第6章中进行详细阐述。
细描述如何有效地把外部数据摄人到Druid的各种方式和技巧。对于高级技巧，例如如何保
因此，如何以合理的速度以及过程摄人数据对于业务系统来说具有非常大的挑战。本章将详
量数据摄入则主要指不同的数据块（chunk）周期性导人。由于存在大量的不同格式的数据，
摄入包括流式和批量两种方式：流式数据摄入主要指数据源一边产生数据一边导人；而批
5.1
间窗口限制的数据摄入系统。最后，本章将介绍其他一些关于Druid数据摄入的重要知识点。
人到Druid数据库中。同时，也会展示如何通过使用Lambda架构构建一个能解决Druid时
数据摄入
流式数据源，指的是持续不断地生产数据的数据源。非常典型的例子有消息队列、日志
Druid的数据摄人也分为流式和批量两种方式，而针对不同类型的数据，Druid将外部
数据摄入是指为了立即使用数据或者将数据持久化，从而获取和导人数据的过程。数据
本章将介绍Druid数据摄入的基本原理，并通过实例讲解如何通过不同的方式将数据摄
数据摄人的两种方式
流式数据源
---
## Page 105
索引服务以Push方式摄取。
5.2
服务启动一个任务进行摄取。
现为离线数据，比如文件系统中的文件。静态数据可以通过实时节点摄入，也可以通过索引
5.1.2
个HTTP服务，数据通过调用这个HTTP服务推送到Druid系统。
采用Push方式摄取数据，需要使用Druid索引服务（Indexing Service）。索引服务会启动一
数据，就可以使用KafkaFirehose；从RabbitMQ中摄取数据，就可以使用RabbitMQFirehose。
同的流式数据。Firehose可以认为是Druid接入不同数据源的Adapter。例如从Kafka中摄取
用Pull方式摄取数据，需要启动一个实时节点（RealtimeNode），通过不同的Firehose摄入不
第5章数据摄入
正如前文所述，流式数据可以通过两种方式摄取：通过实时节点以Pull方式摄取；通过
整个数据摄取流程的数据流人示意图如图5-1所示。
静态数据源，指的是数据已经生产完毕，不会有新数据产生的数据源。静态数据通常表
对于流式数据的摄取，Druid提供了两种方式，分别是Push（推送）和Pull（拉取）。采
流式数据摄取
静态数据源
静态数据—PUSH
流式数据PULL
图5-1
PUSH
PULL
整个数据摄取流程的数据流人示意图
Firehose
Firehose
task
append
task
index
Local
Kafka
real timenode
indexservice
indextask
Firehose
hadoop
extend
Firehose
Rabbit
历史节点
查询节点
81
---
## Page 106
日常开发中运用得比较多的三种数据格式（JSON，CSV,TSV）。
parser 的数据结构如下：
同时社区贡献了一些插件以支持其他数据格式，比如 avro 等。本书主要涉及 string 格式。
度列等。具体格式如下：
件去指定数据摄取的相关参数，在Druid系统中这个配置文件称为Ingestion Spec。
1.DataSchema
5.2.1
82
"parser":{
parseSpec指明了数据源格式，比如维度列表、指标列表、时间戳列名等。接下来简述在
"parseSpec"：{...}#JSON对象
"type"："..."，#string数据类型
parser声明了如何去解析一条数据。1
"granularitySpec"：{...}#JSON对象，指明数据的存储和查询力度
“metricsSpec"：[...]，#list包含了所有的指标列信息
"parser"：{..}，#JSON对象，包含了如何解析数据的相关内容
"datasource"：".."，#string类型，数据源名字
关于数据源的描述，
以下是三个部分的简述。
"tuningConfig”：{...}#JSON对象，
"ioConfig":{...},
"dataSchema"：{...}，#JSON对象，
Ingestion Spec是一个JSON格式的文本，由三个部分组成。
以Pull方式摄取，需要启动一个实时节点。在启动实时节点的过程中，需要一个配置文
(1）parser
以Pull方式摄取
包含数据类型、数据由哪些列构成，以及哪些是指标列、哪些是维
#JSON对象，
，指明数据如何在Druid中存储
，指明数据源格式、数据解析、
，指明存储优化配置
Druid 提供的parser支持解析 string、protobuf格式；
Druid实时大数据分析原理与实践
维度等信息
---
## Page 107
"dimensionsSpec":
"timestampSpec":{
第5章数据摄入
"parseSpec":{
"parseSpec":{
"parseSpec":{
"columns"：[...]，#list[string]，TSV数据列名
"dimensionsSpec"：{...}，#JsoN对象，指明维度的设置
"timestampSpec"：{...}，#JSON 对象，
"format":"tsv"
TSV ParseSpec:
其中timestampSpec 和dimensionsSpec请参考前文。
"columns"：[...]，#list[string]，CSV数据列名
"timestampSpec":
"format":"csv"
CSV ParseSpec:
"spatialDimensions"：［...]#List[string]空间维度名列表，主要用于地理几何运算；可选
"dimensionExclusions"：[...]，#list[string]，剔除的维度名列表；可选
"dimensions"：[...]，#list[string]，维度名列表
"dimensionsSpec"：[...}，#JsON对象，指明维度的设置
dimensionsSpec如下:
"format":
"column":"
其中timestampSpec 如下：
"flattenSpec"：{...}#JSON对象，若JSON有嵌套层级，
"dimensionsSpec"：{...}，#JSON对象，指明维度的设置
"timestampSpec"：{...}，#JsON对象，
"format":"json",
JSON ParseSpec:
"#isolmillis|posixlauto|Joda，时间戳格式，默认值为 auto
"，#string，时间戳列名
{...}，#JSON对象，
多值维度列，
指明时间戳列名和格式
指明时间戳列名和格式
指明时间戳列名和格式
数据分隔符；可选
则需要指定
83
---
## Page 108
"metricsSpec":[
8
·countAggregator：统计满足查询过滤条件的数据行数。
metricsSpec是一个JSON数组，指明所有的指标列和所使用的聚合函数。数据格式如下：
"listDelimiter":"...", #string,
。longSumAggregator：对所有满足查询过滤条件的行做求和运算。应用于数据类型是整
下面介绍几种常用的聚合函数，
(2）metricsSpec
GranularitySpec：指定Segment 的存储粒度和查询粒度。
doubleMax Aggregator:
点数的列。
浮点数的列。
的列。
longMaxAggregator：对所有满足查询过滤条件的行求最大值。应用于数据类型是整数
点数的列。
的列。
longMin Aggregator:
数的列。
行数。
数据行数是不一样的，因为Druid会对原始数据进行聚合，这个行数就是指聚合后的
"name":"
"fieldName";
"type":"
：对所有满足查询过滤条件的行求最小值。
对所有满足查询过滤条件的行求最大值。应用于数据类型是浮
对所有满足查询过滤条件的行求最小值。应用于数据类型是浮
对所有满足查询过滤条件的行做求和运算。应用于数据类型是
#string.
#string，聚合函数运用的列名；可选
#count|longSum等聚合函数类型
一些复杂的聚合函数会在第6章中介绍。
多值维度列，数据分隔符；可选
聚合后指标列名
注意，
Druid实时大数据分析原理与实践
具体的数据格式如下：
应用于数据类型是整数
，这个行数和初始的摄人
---
## Page 109
2.ioConfig
"granularitySpec":{
第5章数据摄入
"ioConfig":{
"firehose"：{...}，#指明数据源，例如本地文件、Kafka等
"type":“realtime"
ioConfig指明了真正的具体的数据源，它的格式如下：
"firehose":{
不同的frehose的格式不太一致，下面以Kafka为例，说明 firehose 的格式。
"plumber":"realtime"
"intervals":[ ...
"queryGranularity”:".
"segmentGranularity":
"type”:"uniform",
"type":"kafka-0.8"
"feed":"wikipedia",
"consumerProps":{
"zookeeper.sync.time.ms":"5000"
"zookeeper.session.timeout.ms":"15000",
"zookeeper.connection.timeout.ms":"15000",
"zookeeper.connect":"localhost:2181",
"group. id": "druid-example",
"auto.offset.reset":"largest",
"auto.commit.enable":"false"
.
#可选，对于流式数据Pull方式可以忽略
#string，最小查询粒度MINUTE、HOUR等
#string,
#摄取的数据的时间段，可以有多个值；
，Segment 的存储粒度 HOUR、DAY等
8
---
## Page 110
如下的JSON数据来定义这个DataSource的格式。
成用户行为数据摄取的工作时，我们可以使用一个DataSource来存储用户行为，而使用类似
前的重要工作之一便是对用户行为数据进行格式定义与摄取方式的确定。当使用Druid来完
5.2.2
行摄取的任务为例，说明如何使用Pull方式摄取用户的行为数据。
"tuningConfig":{
以上是实时数据以Pull方式摄取的相关配置。下面我们以一个网站对其用户行为数据进
"shardSpec":{...},
"intermediatePersistPeriod"："..."，#多长时间数据临时存盘一次
"type":“realtime"
这部分配置可以用于优化数据摄取的过程，具体格式如下：
tuningConfig
"user_id":123
"timestamp":"2016-07-04T13:30:21.563Z",
"event_name":"browse_commodity",
"count":1,
"city":"Beijing"
"category":"3C",
"age":"90+",
"reportParseExceptions":
'mergeThreadPriority":“
"persistThreadPriority":
"buildv9Directly":
"maxPendingPersists":
"rejectionPolicy":
"versioningPolicy":
"basePersistDirectory":
"windowPeriod":"
"maxRowsInMemory":
用户行为数据摄取案例
#分片设置
#最大可容忍时间窗口，超过窗口，数据丢弃
#在存盘之前内存中最大的存储行数，指的是聚合后的行数
#最大同时存盘请求数，
#是否汇报数据解析错误
#是否直接构建v9版本的索引
#存盘归并线程优先级
#存盘线程优先级
#数据丢弃策略
#如何为Segment设置版本号
#临时存盘目录