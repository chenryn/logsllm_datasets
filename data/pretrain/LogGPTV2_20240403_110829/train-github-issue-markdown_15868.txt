We recently initiated a rolling upgrade of our Elasticsearch (ES) two-node cluster from version 1.1.1 to 1.4.2. After upgrading and restarting one of the nodes, we encountered an issue where one of the shards failed to recover properly. The error message in the log is as follows:

```
Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=x56z8s actual=1h6zri0 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@393b946e)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Additionally, the shard's expected size should be approximately 170 GB, but the recovery directory grew to over 650 GB.

To rule out hardware issues, I verified the existing hardware and found no problems. To further ensure that the issue was not hardware-related, I added a new node to the cluster. Unfortunately, the same shard exhibited the same issue on the new machine.

I manually deleted the problematic directory, as suggested in issue #9302, but the cluster did not automatically recreate the replica. Subsequently, I used the reroute API to move the primary shard from the old version node to the new version node. Initially, this appeared promising, as the directory size returned to the correct value after the move. However, the old shard on the old version node did not get removed, and the newly created shard on the new version node became a copy of the old shard, rather than a replica, as the cluster did not allocate it.

Could you provide any suggestions or guidance on how to resolve this issue?