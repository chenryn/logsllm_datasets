“The ‘e’ in email stands for evidence.”
“A  account with no two-factor and a password of 1234 is very much a thing. . .
So, the challenge is that you have many different accounts—both the campaign accounts, and
perhaps even more importantly, the personal accounts that people use.”
“Ever since I’ve worked here,  have been worried about a foreign entity hacking us. . .
I’ve never worried about  hacking us.”
“ changed the outcome of the election in 2016. . . It erodes our democracy,
our institutions, it erodes conﬁdence and trust. . . That’s why  are looking
to interfere, because they had such success in really shaping an election in. . . the biggest
democracy in the history of the world.”
Table 1: Select themes and example quotes from our data. We highlight themes based on their importance (in terms of mitigating
risk to the population) and pervasiveness (as reported across participants).
“Most campaigns basically start 6 months before the election.
Maybe 8 months. . . So what happens is that you quickly hire
a bunch of people who work for you for maybe a hundred
days, and then disappear. . . And so, HR? Forget it. IT secu-
rity standards? Forget it. Two-factor authentication? Forget
it. . . There’s no ongoing, continuing infrastructure. There’s
no culture. . . So I think one challenge you have with cam-
paigns is they’re totally transient. . . There are very few in-
centives for any kind of  rigor. Because you’re up
against the clock, and faced with the ticking clock, everything
pales.” –A participant
Busy workers & tight budgets.
People on campaigns
were described as chaotically busy. Tight budgets and a
short timeline meant limited staff working at break-neck pace.
Money was unlikely to be allocated for security expertise or
technologies, especially in the beginning of a campaign as
habits and processes were being developed. Busy people had
less time to enact unfamiliar security measures.
“There’s the wolf in your yard, the wolf on your deck, the
wolf in your house, and the wolf in your pants. Cybersecurity
is like the wolf in your yard. You can’t even get there because
you have these day-to-day things that consume your life. It’s
very difﬁcult to look past that wolf in your pants. . . The focus
is not, ‘What if  hacked us?’ We’ll deal with
that when it happens.” –A participant
The themes in this paper apply to campaigns at all levels,
but differing budgets impacted campaigns’ ability to miti-
gate security problems. Larger campaigns, like Presidential or
contested races that gained national attention, tended to have
more resources to apply to security if they chose to. Partici-
pants reported that larger campaigns were more likely to hire
security expertise and buy managed accounts for staff. Senate
and Gubernatorial races were often medium sized and fairly
well-resourced, but down-ballot from these, campaigns were
described as typically very small (possibly only hiring one or
a few staff members) and extremely resource-constrained. Re-
gardless of campaign size, participants perceived that security
investments were usually not prioritized.
“You spend X dollars on this, translates to X votes. Security
is not baked into that. . . So every dollar we spend , is a dollar taken away from a vote that they’re going to
 buy a commercial in .” –A participant
Amorphous boundaries. Campaigns were described as
having amorphous boundaries in terms of life cycle, people,
and data access. Regarding life cycle, the start of a campaign
was described as a gray area. Conversations about whether
to run for ofﬁce, fundraising, and research tended to occur
before a campaign became ofﬁcial. This meant using accounts
outside the campaign’s domain (which likely had not yet
been set up). When a campaign did become ofﬁcial, it was
described as common for many people to be onboarded and
accounts to be set up before security was considered.
Regarding people, transience and tight budgets were de-
scribed as motivators for many professionals to be more
permanently employed by consulting ﬁrms that supported
multiple campaigns, with their domain accounts and security
practices not controlled by any one campaign. Consultancies
were described as ranging in their security standards: some
consultancies had developed security standards that largely
followed best practices, which they sometimes tried to ap-
ply on campaigns; some consultants did not and relied upon
basic consumer technologies for work use. How rigorously
USENIX Association
30th USENIX Security Symposium    1187
they applied security protections seemed to depend on their
leadership and knowledge about security. Furthermore, tight
budgets meant campaigns relied on the free labor of hundreds
(or even thousands) of volunteers. These amorphous bound-
aries complicated information access decisions and security
(e.g., who gets access to what data? who gets an ofﬁcial cam-
paign account? who must follow security standards, and how
can they be enforced?), in an environment where security
knowledge to cope with such complexity was often limited.
[Where is a campaign’s sensitive data kept?] “There are a lot
of third parties in this environment. You’re going to have your
media consultant, your ﬁnance consultant, your  to process donations. . . I don’t know the degree to
which people are running reports and downloading them to
their own computers to manipulate the data. . . This is a very
squishy boundary operation. So I think  all
over the place” –A participant
Security knowledge.
Security and IT knowledge was gen-
erally lacking on campaigns, though this varied with the cam-
paign’s size and funding. Trained security and IT profession-
als tended to have more job security and ﬁnancial opportu-
nity in industry, where organizations were not as transient
or budget constrained, and where a greater emphasis was of-
ten placed on security. Even if security or IT professionals
were available to hire, spending money on security or IT was
perceived as cutting directly into traditionally higher priority
tasks (like voter and donor outreach). This lack of security and
IT expertise meant that staffers with no (or outdated) technol-
ogy training were often responsible for the campaign’s tech-
nology infrastructure and practices. For campaigns that hired
someone to handle IT or security (and again, these tended to
be larger races), it was typically a vendor. Vendors usually did
not work full-time for the campaign, and sometimes they set
up technology and were then “done.” They also tended to be
brought in once campaigns were reasonably established and
work practices were already formed and harder to change.
Furthermore, our data included many stories of people on
campaigns who did not understand or know how to effectively
use the security technologies that were recommended to them.
Indicative of their limited security knowledge, our participants
described colleagues who did not believe they were targeted
any more than the general population, and a smaller number
of our own participants found it difﬁcult to believe they may
be targeted.
“It steps into the realm of paranoia, conspiracy theories, and
self-aggrandizing if you think that your communications are
being targeted. And so, despite the fact that a lot of people in
politics have big egos, they still wouldn’t assume .” –A participant
This lack of concern contributed to security not being pri-
oritized. Participants described awareness on campaigns as
starting to shift: the high proﬁle attacks during the 2016 elec-
tion cycle [19,68,70,76] were noted by nearly all participants
as a catalyst for increased security concern, though not yet for
widespread adoption of protective security behaviors. And in
the cases when security actions were taken, the lack of secu-
rity knowledge on campaigns contributed to vulnerabilities in
security conﬁgurations (as we will cover later in the results).
“My experience with political campaigns is they were
broadly ignorant and negligent at pretty much all security
measures until John Podesta’s email showed up. And then
suddenly, everybody got worried. . . But there wasn’t a sig-
niﬁcant increase in security measures on a lot of these cam-
paigns. . . a simple example being two-factor authentication
on email. . . People are not using it.” –A participant
Summary: Security is not prioritized. All of the charac-
teristics above contributed to a work culture of campaigns
in which security was seldom prioritized. As reported by
participants about the campaigns they had worked with: digi-
tal security—a relatively new need for campaigns—was not
commonly viewed as essential to winning elections. Other
traditional tasks were seen as more important and thus pri-
oritized, often at the expense of security. Many people on
campaigns did not understand the threats they faced, and thus
lacked the motivation to spend precious time and effort min-
imizing risk. And even if they were concerned, people on
campaigns felt too busy to spend much time on security, espe-
cially since many were unfamiliar with how to effectively use
security technologies. Finally, the amorphous boundaries of
campaigns signiﬁcantly complicated (already hard to justify)
security efforts by requiring more people to coordinate to
protect data that was spread across domains and accounts,
starting even before a campaign was ofﬁcial.
4.2 Campaign tech practices & vulnerabilities
People involved with campaigns depended on a variety of ac-
counts and technology platforms for communications and ﬁle
storage. We found that disparate security practices for manag-
ing these sensitive resources—such as encryption, authentica-
tion, and access control—compounded the threats they faced.
We describe issues with how people on campaigns handled
security compared to current best practices for this population,
and highlight the vulnerabilities these gaps introduced.
4.2.1 Sensitive data: communications & ﬁles
Most (although not all) of our participants believed or
cited evidence that their communications—work and/or per-
sonal—had been targeted by attackers.3 Participants explained
why: candidates and their campaigns rely on reputation, and
almost any communication by someone involved with their
campaign could be used to cause damage. Even personal
communications, unrelated to the campaign and housed in
3We note this represents the perceptions of participants. We have no
additional data on the potential attacks or attackers.
1188    30th USENIX Security Symposium
USENIX Association
consumer accounts, could create a problem for a campaign
if leaked (Pizzagate [74] is a well-known example). Partici-
pants also described ﬁles as being potential targets (especially
those containing strategy, voter, or donor information), which
tended to be stored in cloud accounts or as email attachments.
“Let’s say the candidate is just complaining about something.
Or the staff is just complaining about something. Someone
could very easily grab that and turn it into a thing, when
it was actually just a ‘human moment.’ And now you’ve
derailed the whole campaign. You’ve taken it off message.
You’re talking about whatever stupid thing was in that email,
rather than whatever you really want to be talking about.” -A
participant
Ad hoc storage, sharing, & provider usage for data.
People involved with campaigns stored data across many
domains and providers, including work and personal email,
social media accounts, and chat accounts. Traditional work-
place access control policies—such as restricting communi-
cation and ﬁle access to a campaign’s domain or workplace
accounts—were not typically employed by the campaigns our
participants had worked with, given their need to share access
with external committee staff, consultants, vendors, volun-
teers, or even a candidate’s family. A campaign’s quick ramp
up often meant no pre-existing IT infrastructure or policies, so
early campaign staff and consultants’ preferences often drove
the decisions of what technologies to use. And since these
individuals continued to work in politics, they sometimes kept
important communications and ﬁles in their own accounts that
would persist beyond a single campaign. Preferences were
also driven by practical time constraints—some participants
perceived that there was not enough time to set up and learn
new technologies for each campaign.
The security conﬁgurations of these various accounts were
often not controlled by (or sometimes even known to) the
campaign. These dynamics also tended to result in what par-
ticipants described as an “ad hoc” set of communication prac-
tices, for example, communicating in whatever technologies
people already had set up, were convenient on the devices
they already used, already housed the necessary contact in-
formation, etc. Participants described the act of coping with
these complex, cross-provider technology setups as involv-
ing inherent gray areas—what technology should be used
to communicate various content among the many options?;
who should be included on emails or ﬁle access control lists
(ACLs) when boundaries were amorphous?; who should be
removed from ﬁle ACLs and how should that be accomplished
when the campaign ends abruptly?
can be locked down. And they’re not happening for the most
part with people or organizations that are very sophisticated
about security.” –A participant
Practices protecting communications & ﬁles. Relevant
to the challenges above, participants talked about protective
practices that our ﬁndings suggest are on the rise. Most promi-
nently among these include encrypted communications and
secure data sharing in the cloud. Even for campaigns and re-
lated organizations that had adopted these tools and practices,
participants described usability issues and inconsistent use
across people and contexts as problems.
Encrypted communications & ﬁles.
Participants described
that many campaigns and related organizations were adopt-
ing encrypted communications tools, with Signal and Wickr
being mentioned most often. But decisions about when to use
encrypted communications over standard email or chat were
often still ad hoc. Encrypted ﬁle storage and sharing systems
were less commonly described, and usability was noted as a
barrier. Participants thought encrypted tools were harder to
use than other tools, like standard email, messaging, or ﬁle
storage/sharing.
“Encryption, having all your communications be encrypted
so that they aren’t vulnerable to a hack, I don’t think is totally
ﬁgured out yet. People use . . . Sensitive
communication that you might typically do on email, you
move over to some kind of encrypted system. I think it’s
pretty ad hoc still. . . How do you protect your documents
so that they’re not available to a system hack, but also
make them usable?. . . We looked into how to encrypt docu-
ments. . . We couldn’t do it because the bar and the barrier to
entry for the people who work at  every day
. . . was too cumbersome.” –A participant
Secure data sharing in the cloud.
Participants described
nearly pervasive use of the cloud to store and share sensitive
campaign data, like email and ﬁles. Fewer participants—and
primarily those from consulting ﬁrms, committees, and orga-
nizations—discussed using best practices for cloud systems,
like auditing ﬁle ACLs, establishing secure sharing norms, or
employing retention policies. Importantly, moving sensitive
data to the cloud made strong account security essential, and
as we will cover in the next section, this was often lacking.
However, we heard multiple stories of oversharing mis-
takes with cloud ﬁles and email. Some of these cases were
due to usability: default ﬁle sharing settings that gave access
to anyone who had the link sometimes resulted in sensitive
campaign ﬁles being shared more broadly than expected.
“Imagine an email on a sensitive topic. It’s going to consist
of maybe 5 or 6 people: a couple of them are going to be on
the campaign email account, a couple of them on , somebody is going to have their own domain, but
it’s not like there’s any IT behind it. . . These are not commu-
nications that are happening within a single organization that
4.2.2 Accounts & Authentication
Participants described account security practices on cam-
paigns that introduced vulnerabilities, including the use of
personal accounts for campaign data, account sharing, tran-
sient account ownership, under-use of strong 2FA, and weak
USENIX Association
30th USENIX Security Symposium    1189
password practices. These weaknesses were exacerbated by
the many accounts that campaign workers used, and security
was complicated by the cross-platform nature and campaigns’
inability to enforce policies for accounts they did not control.
Many targeted accounts.
Participants described an en-
vironment in which candidates, campaign staff, and con-
sultants used numerous email, chat, social media, and
cloud accounts, any of which might be targeted by at-
tackers. While not directly prompted, participants named
commonly used providers like Apple, Dropbox, Face-
book/Instagram/WhatsApp, Google/G Suite/Gmail/YouTube,
LinkedIn, Microsoft/M365, Signal, Slack, Twitter, and Wickr.
Campaigns also pervasively used technology services fo-
cused on the political sector, including NGP Van/VoteBuilder,
ActBlue, and WinRed. Securing accounts across so many
providers took time. Differences across the various providers
regarding the available security settings, and how those set-
tings were named and used increased the complexity. People
involved with campaigns often did not have the time or tech-
nical knowledge to tackle this complexity, given that security
was not among their top priorities.
“There’s tons of  channels now. . . between
text message, GChat, Facebook, Wickr, Signal, WhatsApp,