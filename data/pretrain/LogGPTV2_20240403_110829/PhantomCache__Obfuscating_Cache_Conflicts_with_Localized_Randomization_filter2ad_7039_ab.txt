Using system conﬁguration parameters, it is straightforward to
calculate the length of each ﬁeld. The length difference of the
page offset and the line offset decides how many index bits
are shared by a virtual address and its corresponding physical
address. The longer the page offset the more likely it facilitates
inferring which cache set a virtual address can ﬁnally access. In
particular, microarchitecture with large pages is more vulnerable
to this exploit. For example, a Sandy Bridge processor released
by Intel uses the 6th to 16th least signiﬁcant bits of a physical
address as index bits; a 2 MB page in use results in a 21-bit
page offset, which covers the complete index bits [26]. This
reveals to the attacker exactly which cache set a virtual address
maps to.
Address removal. Algorithm 1 presents a typical iterative
framework for ﬁnding a minimal eviction set with O(|E|2)
complexity [33], where |E| denotes the cardinality of the initial
set E. In each iteration, the attacker determines whether a
candidate address e in the initial set E can be removed (lines 1-
11). Removing e should not make the remaining E insufﬁcient
for evicting x from the cache. Consider, for example, when
the current E is sufﬁcient for priming the cache set that x
maps to. After priming the cache set (line 2), accessing x
encounters a cache miss with a relatively large delay t1 (line 3).
Now we remove a candidate address e at random and access
all the remaining candidate addresses (lines 5-6). If they can
no longer evict x, accessing x again will return a cache hit
with a relatively low delay t2 (lines 7-8). Such a miss-then-hit
sequence of accessing x can be determined by a noticeable
timing difference (line 8). In this case, the removed candidate
address e should be added back to E (line 9). Otherwise, the
two accesses of x in the same iteration show comparative
timings, the attacker can remove the selected candidate address
e. The iteration ceases upon the minimal eviction set contains
as many candidate addresses as set associativity.
The state-of-the-art eviction set minimization algorithm
[35], [40] achieves O(|E|) complexity using group testing [11].
Consider an m-way associative cache that requires an minimal
eviction set of size |E| = m to attack. In each iteration, the
algorithm ﬁrst splits the eviction set into m + 1 groups. It
removes addresses group wise and tests whether accessing
the remaining groups of addresses can still evict the target
cache line. If yes, the group of addresses are removed from the
eviction set. Otherwise, they are brought back to the eviction
set. Since the minimal eviction set requires m addresses, these
m addresses reside in at most m groups. Therefore, at least
one of the m + 1 groups addresses can be removed in each
iteration. This makes the minimization process converge much
faster than the O(|E|2) algorithm that removes addresses one
by one.
C. (Inefﬁcient) Randomized Mapping for LLC
Randomized memory-to-cache mapping has been explored
as a fundamental countermeasure against conﬂict-based cache
timing attacks. Ideally, randomized mapping obfuscates cache
conﬂicts by mapping a physical address to a random cache
line [25], [43]. Even for the same address, its consecutive
placements will highly likely occupy different cache lines. This
makes it hard to ﬁnd a set of addresses that always map to
the same cache set. Randomized mapping thus prevents the
Virtual AddressPhysical Addressaddress translationFrame NumberLine OffsetTagexploitPage OffsetIndex BitsAlgorithm 1 Minimal Eviction Set Construction [33]
Input: Initial eviction set E; Target address x;
Output: Minimal Eviction Set E to Evict x;
1: while |E| > SetAssociativity do
Access all candidate addresses of E;
2:
t1 ← measure the time it takes to access x;
3:
e ← select a candidate address at random from E;
4:
E = E \ {e};
5:
Access all candidate addresses of E;
6:
t2 ← measure the time it takes to access x;
7:
if t1 − t2 > threshold then
8:
9:
end if
10:
11: end while
12: // iteration terminates upon |E| = SetAssociativity;
13: return E;
E = E ∪ e;
TABLE I.
COMPARISON OF PHANTOMCACHE WITH EXISTING
NOTES: ∗RPCACHE, CEASER, CEASER-S, AND SCATTERCACHE
RANDOMIZED MAPPING SOLUTIONS FOR LLC.
ACHIEVE RANDOMIZED MAPPING THROUGH DYNAMIC REMAPPING. WITHIN
A REMAPPING INTERVAL, MEMORY-TO-CACHE MAPPING IS ESSENTIALLY
DETERMINISTIC. NOTE THAT DYNAMIC REMAPPING IS ALSO REFERRED TO
AS RE-KEYING [45].
#RPCACHE SUPPORTS DIRECT CACHE SEARCH BY QUERYING THE ADDRESS
MAPPING IN PERMUTATION TABLES. STORAGE OVERHEAD INDUCED BY
LARGE PERMUTATION TABLES MAY LIMIT ITS USAGE IN LLC [34].
Legend: CF: Crypto-Free; DS: Direct Search;
Legend: HO: Hardware Only; LA: LLC Applicability;
Legend: DRF: Dynamic-Remapping Free
Solution
RPcache∗ [42]
CEASER∗ [34]
CEASER-S∗ [35]
ScatterCache∗ [45]
PhantomCache
DRF





DS





CF





HO





LA
#




attacker from ﬁnding minimal eviction sets, the foundation of
conﬂict-based cache timing attacks. While efﬁcient randomized
mapping solutions for L1 caches have been proposed [24], [25],
[43], solutions for securing LLCs still suffer from practical
inefﬁciency [34], [35], [42], [45].
Random Replacement on L1 caches. It aims to fully achieve
randomized mapping. The pioneering solution, NewCache [25],
[43], randomly selects a cache line and replaces it with the
accessed memory block upon a cache miss. Then an access
enforces a search through all cache lines. Because of the small
size of L1 caches, NewCache can guarantee fast search. Built
upon NewCache, Random Fill Cache [24] further randomizes
the selection of which block to put in cache. Speciﬁcally, it
may not cache a requested block. Instead, the requested block
is directly sent to the processor while a randomly fetched
block adjacent to the demanded one from memory will be
cached. Random Fill Cache requires both hardware and software
changes. It suits better to applications that have random memory
access patterns.
Dynamic Remapping on LLC caches. Since it is hard to
enforce global search over a large LLC, solutions for protecting
LLCs are double-edged in that 1) they ﬁrst introduce implicit
mapping to lengthen the time for ﬁnding eviction sets, and 2)
then they conduct dynamic remapping to change the mapping
strategy for defeating the attacker’s accumulated inference.
Implicit memory-to-cache mapping can be achieved by either
permutation tables [42] or encryption units [34], [35], [45].
RPcache [42] uses permutation tables indexed by the index
bits of an address. The indexed entry features the cache set
index the address maps to. To address the storage overhead
of permutation tables, CEASER [34] hides memory-to-cache
mapping using encryption. It encrypts a physical address and
uses the encryption result as the cache set index. However,
once permutation rules [42] or encryption keys [34] are ﬁxed,
an address will always map to the same cache set. This
deterministic mapping again leaves the door open for cache
timing attacks. Toward a double defense over implicit mapping,
dynamic remapping periodically changes the memory-to-cache
mapping strategy. Remapping frequency should be sufﬁciently
high such that the attacker cannot get enough time to ﬁnd a
minimal eviction set targeting a speciﬁc mapping. For example,
to secure an 8 MB 16-way LLC against the O(|E|2) attack
with 0.50% performance overhead, where E is the average
cardinality of initial sample addresses, CEASER needs to remap
a line every 100 accesses.
As the O(|E|) attack emerges [35], [40], dynamic remap-
ping has been augmented with skewed cache design to control
overhead [35], [45]. A skewed cache divides the cache space
into partitions [36]. Each partition contains a number of
consecutive ways from all cache sets. For example, a two-
way skewed-associative cache over a traditional 16-way cache
divides the cache into two partitions, one contains the ﬁrst
8 ways from all sets while the other contains the second 8
ways from all sets. The property for security leverage is that
each partition has a different address mapping function. To
evict an address, an attacker now needs to ﬁll sets the address
may map to on all partitions. Since a ﬁxed mapping strategy
of each partition is vulnerable, dynamic remapping is still
needed. CEASER-S [35] extends CEASER under a skewed
cache. Against the attack that requires that each eviction address
should map to the same set with the victim address on all
partitions, it leads to only 1% slowdown when remapping a
line every 100 accesses. However, ScatterCache [45] ﬁnds that
an attacker can more easily ﬁnd eviction sets with partially-
congruent addresses. In other words, each eviction address
can map to the same set with the victim address on one or
more partitions while all eviction addresses jointly cover all
possible sets of the victim address. Apparently, this attack is
more challenging to defend. The state-of-the-art ScatterCache
can secure 2 MB 16 way LLC with 2% slowdown and 5%
storage overhead per cache line while requiring both hardware
and software changes [45].
In comparison, our PhantomCache does not require dynamic
remapping or software change. To secure an 8-bank 16 MB 16-
way LLC, it brings only 0.50% slowdown and 0.50% storage
overhead per cache line. The shake-off of dynamic remapping
is because PhantomCache randomly maps an address across
several sets across the entire LLC. We efﬁciently integrate
PhantomCache design into the multi-banked LLC architecture
(Section IV-E). In contrast, the number of possible mapping
location for an address by skewed-cache based solutions is equal
to the number of locations in only one set of the undivided
cache. This is why they need dynamic remapping to preserve
security at the cost of efﬁciency [35], [45].
4
III. OVERVIEW
In this section, we present PhantomCache, an LLC-favorable
countermeasure against conﬂict-based cache timing attacks
without inefﬁcient dynamic remapping (Table I). It guarantees
efﬁciency by our newly proposed localized randomization tech-
nique. Localized randomization bounds randomized memory-to-
cache mapping of an address within only a limited number of
randomly selected cache sets. The intrinsic mapping random-
ization enables PhantomCache as effective against eviction
set minimization as the fully randomized NewCache [25],
[43]. Meanwhile, searching a block touches also the limited
cache sets and can be efﬁciently implemented by a practical
hardware supporting parallel access. This makes PhantomCache
as LLC favorable as CEASER [34], CEASER-S [35], and
ScatterCache [45]. However, PhantomCache relieves from
frequent, inefﬁcient dynamic remapping as it does not involve
deterministic mapping.
A. Motivation
Essentially, if we could limit randomization space without
sacriﬁcing security of randomized mapping, we can achieve fast
cache search. This instantly motivates us to explore localized
randomization. That is, we randomize the mapping of an address
within a limited number of cache sets, instead of across the
entire cache. The intrinsic property of randomness hinders the
attacker from minimizing an eviction set. Traditionally, the
criterion for removing an eviction address is that the remaining
eviction set can still prime the target cache set to evict the target
address [33], [40]. It leverages the fact that, in traditional caches,
an eviction set including at least m (i.e., set associativity)
relevant addresses must be sufﬁcient for priming the target cache
set. However, localized randomization makes this criterion non-
deterministic and hard to exploit. Consider n addresses, each
of which maps to r cache sets but all share a common cache
set. Given set associativity m, we derive the probability of
priming the common cache set by accessing the n addresses
as the following.
n(cid:88)
i=m
P rprime(n) =
n × (
C i
1
r
)i × (
r − 1
r
)n−i.
(1)
In this scenario, even if the eviction set includes m relevant
addresses, accessing all its addresses cannot necessarily prime
the target set. Thus, the attacker needs to repeat accessing the
whole eviction set rm times on average to ﬁgure out whether it
includes enough relevant addresses. This increases the attacker’s
cost rapidly with the growth of r, which motivates us to explore
more about localized randomization.