title:CloudCmp: comparing public cloud providers
author:Ang Li and
Xiaowei Yang and
Srikanth Kandula and
Ming Zhang
CloudCmp: Comparing Public Cloud Providers
Ang Li
Xiaowei Yang
Duke University
{angl, xwy}@cs.duke.edu
Srikanth Kandula
Ming Zhang
Microsoft Research
{srikanth, mzh}@microsoft.com
ABSTRACT
While many public cloud providers offer pay-as-you-go comput-
ing, their varying approaches to infrastructure, virtualization, and
software services lead to a problem of plenty. To help customers
pick a cloud that ﬁts their needs, we develop CloudCmp, a system-
atic comparator of the performance and cost of cloud providers.
CloudCmp measures the elastic computing, persistent storage, and
networking services offered by a cloud along metrics that directly
reﬂect their impact on the performance of customer applications.
CloudCmp strives to ensure fairness, representativeness, and com-
pliance of these measurements while limiting measurement cost.
Applying CloudCmp to four cloud providers that together account
for most of the cloud customers today, we ﬁnd that their offered ser-
vices vary widely in performance and costs, underscoring the need
for thoughtful provider selection. From case studies on three rep-
resentative cloud applications, we show that CloudCmp can guide
customers in selecting the best-performing provider for their appli-
cations.
Categories and Subject Descriptors
C.4 [Performance of Systems]: General—measurement techniques,
performance attributes; C.2.3 [Computer-Communication Net-
works]: Network Operations—network monitoring, public networks;
C.2.4 [Computer-Communication Networks]: Distributed Sys-
tems—distributed applications, distributed databases
General Terms
Measurement, Design, Performance, Economics.
Keywords
Cloud computing, comparison, performace, cost.
1.
INTRODUCTION
Internet-based cloud computing has gained tremendous momen-
tum in recent years. Cloud customers outsource their computation
and storage to public providers and pay for the service usage on
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
demand. Compared to the traditional computing model that uses
dedicated, in-house infrastructure, cloud computing offers unprece-
dented advantages in terms of cost and reliability [22, 27]. A cloud
customer need not pay a large upfront cost (e.g., for hardware pur-
chase) before launching services, or over-provision to accommo-
date future or peak demand.
Instead, the cloud’s pay-as-you-go
charging model enables the customer to pay for what she actually
uses and promises to scale with demand. Moreover, the customer
can avoid the cost of maintaining an IT staff to manage her server
and network infrastructure.
A growing number of companies are riding this wave to pro-
vide public cloud computing services, such as Amazon, Google,
Microsoft, Rackspace, and GoGrid. These cloud providers offer
a variety of options in pricing, performance, and feature set. For
instance, some offer platform as a service (PaaS), where a cloud
customer builds applications using the APIs provided by the cloud;
others offer infrastructure as a service (IaaS), where a customer
runs applications inside virtual machines (VMs), using the APIs
provided by their chosen guest operating systems. Cloud providers
also differ in pricing models. For example, Amazon’s AWS charges
by the number and duration of VM instances used by a customer,
while Google’s AppEngine charges by the number of CPU cycles
consumed by a customer’s application.
The diversity of cloud providers leads to a practical question:
how well does a cloud provider perform compared to the other
providers? Answering this question will beneﬁt both cloud cus-
tomers and providers. For a potential customer, the answer can help
it choose a provider that best ﬁts its performance and cost needs.
For instance, it may choose one provider for storage intensive ap-
plications and another for computation intensive applications. For
a cloud provider, such answers can point it in the right direction
for improvements. For instance, a provider should pour more re-
sources into optimizing table storage if the performance of its store
lags behind competitors.
Despite the practical relevance of comparing cloud providers,
there have been few studies on this topic. The challenge is that
every provider has its own idiosyncratic ways of doing things,
so ﬁnding a common ground needs some thought. A few efforts
have characterized the performance of one IaaS provider (Amazon
AWS) [24,34]. Some recent blog posts [6,15,36] compare Amazon
AWS with one other provider each. These measurements are lim-
ited in scope; none of them cover enough of the dimensions (e.g.,
compute, storage, network, scaling) to yield meaningful conclu-
sions. Further, some of the measurement methodologies do not ex-
tend to all providers, e.g., they would not work for PaaS providers.
In this paper, we consider the problem of systematically com-
paring the performance of cloud providers. We identify the key re-
quirements for conducting a meaningful comparison (§2), develop
1a tool called CloudCmp, and use CloudCmp to evaluate a few cloud
providers (§3– §4) that differ widely in implementation but together
dominate the cloud market. Our results (§5) provide a customer
with the performance-cost trade-offs across providers for a wide
set of metrics. For providers, the results point out speciﬁc areas for
improvement in their current infrastructures.
Several technical challenges arise in realizing a comparator for
cloud providers. The ﬁrst is the choice of what to measure. Rather
than focusing on the nitty-gritty such as which virtualization tech-
nology a provider uses or how it implements its persistent stor-
age, we take an end-to-end approach that focuses on the dimen-
sions of performance that customers perceive. Doing so has the
advantage that the measurement methodology remains stable even
as the implementations change over time or differ widely across
providers. To this end, we identify a common set of services of-
fered by these providers, including elastic computing, persistent
storage, and intra-cloud and wide-area networking (§3.2).
The second challenge is the choice of how to measure customer
perceived performance of these services. For each service, we fo-
cus on a few important metrics, e.g., speed of CPU, memory, and
disk I/O, scaling latency, storage service response time, time to
reach consistency, network latency, and available bandwidth (§3.3).
We leverage pre-existing tools speciﬁc to each metric. However,
when applying the tools, we had to be careful along several axes,
such as using variable number of threads to test multi-core, piecing
apart the interference from colocated tenants and the infrastructure
itself, and covering the wide set of geographically distributed data
centers offered by the providers. The individual tools per se are
simple, but this speciﬁc collection of them that enables comparing
cloud providers is novel.
Third, as with all signiﬁcant measurement efforts, we trade off
development cost to the completeness of the study. We skip func-
tionality that is speciﬁc to small classes of applications but are
comprehensive enough that our benchmark results allow predict-
ing the performance of three representative applications: a storage
intensive e-commerce web service, a computation intensive scien-
tiﬁc computing application, and a latency sensitive website serving
static objects. By deploying these applications on each cloud, we
demonstrate that the predictions from CloudCmp align well with
the actual application performance. CloudCmp enables predicting
application performance without having to ﬁrst port the application
onto every cloud provider.
Finally, unlike other measurement efforts, we are constrained
by the monetary cost of measuring the clouds and the acceptable
use policies of the providers. We note that the results here were
achieved under a modest budget by judicious choice of how many
and how often to measure. CloudCmp complies with all acceptable
use policies.
We used CloudCmp to perform a comprehensive measurement
study over four major cloud providers, namely, Amazon AWS, Mi-
crosoft Azure, Google AppEngine, and Rackspace CloudServers.
We emphasize that the infrastructure being measured is ephemeral.
Providers periodically upgrade or regress in their software or hard-
ware and customer demands vary over time. Hence, these results
are relevant only for the time period in which they were gener-
ated. To keep the focus on the value of this comparison method
and its implications rather than rank providers, our results use la-
bels C1 − C4 instead of provider names.
From the comparison results, we ﬁnd that the performance and
price of the four providers vary signiﬁcantly with no one provider
standing out (§5). For instance, while the cloud provider C1 has the
highest intra-cloud bandwidth, its virtual instance is not the most
cost-effective. The cloud provider C2 has the most powerful virtual
instances, but its network bandwidth is quite limited. C3 offers the
lowest wide-area network latency, but its storage service is slower
than that of its competitors. We highlight a few interesting ﬁndings
below:
(cid:129) Cloud instances are not equally cost-effective. For example,
while only 30% more expensive, C4’s virtual instance can be
twice as fast as that of C1.
(cid:129) C2 in our study allows a virtual instance to fully utilize the
underlying physical machine when there is no local resource
competition. Hence, an instance can attain high performance
at low cost.
(cid:129) The performance of the storage service can vary signiﬁcantly
across providers. For instance, C1’s table query operation is
an order of magnitude faster than that of the others.
(cid:129) The providers offer dramatically different intra-datacenter
bandwidth, even though intra-datacenter trafﬁc is free of
charge. For instance, C1’s bandwidth is on average three
times higher than C2’s.
The measurement data we collected is available at http://www.
cloudcmp.net.
We believe that this is the ﬁrst study to comprehensively char-
acterize the performance and cost of the major cloud providers in
today’s market. Though we present results for four providers in
this paper, we believe the techniques in CloudCmp can be extended
to measure other providers. In future work, we plan to build per-
formance prediction models based on CloudCmp’s results to enable
fast and accurate provider selection for arbitrary applications (§7).
2. GOALS AND APPROACH
In this section, we highlight the design goals of CloudCmp and
brieﬂy describe how we meet them.
1. Guide a customer’s choice of provider: Our primary goal
is to provide performance and cost information about vari-
ous cloud providers to a customer. The customer can use
this information to select the right provider for its applica-
tions. We choose the cost and performance metrics that are
relevant to the typical cloud applications a customer deploys.
These metrics cover the main cloud services, including elas-
tic computing, persistent storage, and intra-cloud and wide-
area networking.
2. Relevant to cloud providers: We aim to help a provider
identify its under-performing services compared to its com-
petitors. We not only present a comprehensive set of mea-
surement results, but also attempt to explain what causes the
performance differences between providers. This enables a
provider to make targeted improvements to its services.
3. Fair: We strive to provide a fair comparison among vari-
ous providers by characterizing all providers using the same
set of workloads and metrics. This restricts our comparative
study to the set of common services offered by all providers.
The core functionality we study sufﬁces to support a wide
set of cloud applications. However, we skip specialized ser-
vices speciﬁc to some applications that only a few providers
offer. While support for functionality is a key decision factor
that we will consider in future work, our focus here is on the
performance-cost trade-off.
4. Thoroughness vs. measurement cost: For a thorough com-
parison of various cloud providers, we should measure all
cloud providers continuously across all their data centers.
2Provider
Amazon AWS
Microsoft Azure
Google AppEngine
Rackspace CloudServers Xen VM
Elastic Cluster
Xen VM
Azure VM
Proprietary sandbox DataStore (table)
CloudFiles (blob)
Storage
SimpleDB (table), S3 (blob), SQS (queue)
XStore (table, blob, queue)
Wide-area Network
3 DC locations (2 in US, 1 in EU)
6 DC locations (2 each in US, EU, and Asia)
Unpublished number of Google DCs
2 DC locations (all in US)
Table 1: The services offered by the cloud providers we study. The intra-cloud networks of all four providers is proprietary, and are omitted from
the table.
This, however, incurs signiﬁcant measurement overhead and
monetary costs. In practice, we periodically (e.g., once an
hour) measure each provider at different times of day across
all its locations. The measurements on different providers are
loosely synchronized (e.g., within the same hour), because
the same measurement can take different amount of time to
complete in different providers.
5. Coverage vs. development cost:
Ideally, we would like
to measure and compare all cloud providers on the market.
Achieving this goal, however, can be cost and time pro-
hibitive. We cover a representative set of cloud providers
while restricting our development cost. We choose the cloud
providers to compare based on two criteria: popularity and
representativeness. That is, we pick the providers that have
the largest number of customers and at the same time rep-
resent different models such as IaaS and PaaS. Our mea-
surement methodology, however, is easily extensible to other
providers.
6. Compliant with acceptable use policies: Finally, we aim
to comply with cloud providers’ use policies. We conduct
experiments that resemble the workloads of legitimate cus-
tomer applications. We do not overload the cloud infrastruc-
tures or disrupt other customer applications.
3. MEASUREMENT METHODOLOGY
In this section, we describe how we design CloudCmp to conduct
a fair and application-relevant comparison among cloud providers.
We ﬁrst show how we select the providers to compare, and discuss
how to choose the common services to ensure a fair comparison.
Then for each type of service, we identify a set of performance
metrics that are relevant to application performance and cost.
3.1 Selecting Providers
Our comparative study includes four popular and representative
cloud providers: Amazon AWS [2], Microsoft Azure [12], Google
AppEngine [7], and Rackspace CloudServers [14]. We choose
Amazon AWS and Rackspace CloudServers because they are the
top two providers that host the largest number of web services [19].
We choose Google AppEngine because it is a unique PaaS provider,
and choose Microsoft Azure because it is a new entrant to the cloud
computing market that offers the full spectrum of computation and
storage services similar to AWS.
3.2 Identifying Common Services
Despite the complexity and idiosyncrasies of the various cloud
providers, there is a common core set of functionality. In this sec-
tion, we focus on identifying this common set, and describe the
experience of a customer who uses each functionality. We defer
commenting on the speciﬁcs of how the cloud achieves each func-
tionality unless it is relevant. This allows us to compare the clouds
from the end-to-end perspective of the customer and sheds light
on the meaningful differences. The common set of functionality
includes:
number of virtual instances that run application code.
(cid:129) Elastic compute cluster. The cluster includes a variable
(cid:129) Persistent storage. The storage service keeps the state and
data of an application and can be accessed by application
instances through API calls.
(cid:129) Intra-cloud network. The intra-cloud network connects ap-
plication instances with each other and with shared services.
(cid:129) Wide-area network. The content of an application is deliv-
ered to end users through the wide-area network from multi-
ple data centers (DCs) at different geographical locations.
These services are offered by most cloud providers today be-
cause they are needed to support a broad spectrum of applications.
For example, a web application can have its servers run in the elas-
tic compute cluster, its data stored in the persistent storage, and its
content delivered through the wide-area network. Other cloud ap-
plications such as document translation, ﬁle backup, and parallel
computation impose different requirements on these same compo-
nents. A few providers offer specialized services for speciﬁc ap-
plications (e.g., MapReduce). We skip evaluating these offerings
to focus on the more general applications. Table 1 summarizes the
services offered by the providers we study.
3.3 Choosing Performance Metrics
For each of these cloud services, we begin with some back-
ground and describe the performance and cost metrics we use to
characterize that service.
3.3.1 Elastic Compute Cluster
A compute cluster provides virtual instances that host and run a
customer’s application code. Across providers, the virtual instances
differ in their underlying server hardware, virtualization technol-
ogy, and hosting environment. Even within a provider, multiple
tiers of virtual instances are available, each with a different con-
ﬁguration. For example, the instances in the higher tier can have
faster CPUs, more CPU cores, and faster disk I/O access. These
differences do impact the performance of customer applications.
The compute cluster is charged per usage. There are two types
of charging models among the providers we study. The IaaS
providers (AWS, Azure, and CloudServers) charge based on how
long an instance remains allocated, regardless of whether the in-
stance is fully utilized or not. However, the PaaS provider (Ap-
pEngine) charges based on how many CPU cycles a customer’s
application consumes in excess of a few free CPU hours per appli-
cation per day.
The compute cluster is also “elastic” in the sense that a customer
can dynamically scale up and down the number of instances it uses
to withstand its application’s varying workload. Presently, there are
two types of scaling mechanisms: opaque scaling and transparent
scaling. The former requires a customer herself to manually change
the number of instances or specify a scaling policy, such as creating
a new instance when average CPU usage exceeds 60%. The latter
automatically tunes the number of instances without customer in-
tervention. AWS, Azure, and CloudServers support opaque scaling
whereas AppEngine provides transparent scaling.
3Service Operation Description
get
put
query
download
upload
send
receive
Table
Blob
Queue
fetch a single row using the primary key
insert a single row
lookup rows that satisfy a condition on a
non-primary key ﬁeld
download a single blob
upload a single blob
send a message to a queue
retrieve the next message from a queue
Table 2: The operations we use to measure the performance of each
storage service.
We use three metrics to compare the performance of the com-
pute clusters: benchmark ﬁnishing time, cost per benchmark, and
scaling latency. These metrics reﬂect how fast an instance can run,
how cost-effective it is, and how quickly it can scale.
Benchmark ﬁnishing time.
Similar to conventional computa-
tional benchmark metrics for computer architectures [18], this met-
ric measures how long the instance takes to complete the bench-
mark tasks. The benchmark has tasks that stress each of the main
compute resources (CPU, memory, and disk I/O).
Cost. This is the monetary cost to complete each benchmark task.
Because we use the same tasks across different instances provided
by different clouds, customers can use this metric to compare the
cost-effectiveness of the instances regardless of their prices and
charging models. Together with the above metric, this provides
customers with a view of the performance-cost trade-offs across
providers. These metrics correspond to the criteria that customers
use when choosing, such as best performance within a cost budget
or lowest cost above a performance threshold.
Scaling latency. This is the time taken by a provider to allocate
a new instance after a customer requests it. The scaling latency of
a cluster can affect the performance and cost of running an appli-
cation. An application can absorb workload spikes more quickly
and can keep fewer number of instances running continuously if it
can instantiate new instances quickly. With this metric, a customer
can choose the compute cluster that scales the fastest or design bet-
ter scaling strategies. She can also make more nuanced decisions
based on what it would cost to provide good performance when the
workload of her application varies.
There are a few other metrics, such as the customizability of a
virtual instance and the degree of automation in management, that
capture vital aspects of cloud providers. However, these are harder
to quantify. Hence, we focus on the performance and costs of run-
ning an application and defer considering other metrics to future
work.
3.3.2 Persistent Storage
Cloud providers offer persistent storage for application state and
data. There are currently three common types of storage services:
table, blob, and queue. The table storage is designed to store struc-
tural data in lieu of a conventional database, but with limited sup-
port for complex queries (e.g., table join and group by). The blob
storage is designed to store unstructured blobs, such as binary ob-
jects, user generated data, and application inputs and outputs. Fi-
nally, the queue storage implements a global message queue to pass
messages between different instances. Most storage services are
implemented over HTTP tunnels, and while not standardized, the
usage interfaces are stable and similar across providers.
The cloud storage services have two advantages over their con-
ventional counterparts: scalability and availability. The services
are well-provisioned to handle load surges and the data is repli-
cated [1] for high availability and robustness to failures. However,
as a trade-off, cloud storage services do not offer strong consis-
tency guarantees [25]. Therefore, an application can retrieve stale
and inconsistent data when a read immediately follows a write.
There are presently two pricing models for storage operations.
The table services of AWS and AppEngine charge based on the
CPU cycles consumed to run an operation. Thus, a complex query
costs more than a simple one. Azure and CloudServers have a ﬁxed
per-operation cost regardless of the operation’s complexity.
We use three metrics to compare the performance and cost of
storage services: operation response time, time to consistency, and
cost per operation.
Operation response time. This metric measures how long it takes
for a storage operation to ﬁnish. We measure operations that are
commonly supported by providers and are popular with customers.
Table 2 summarizes these operations. They include the basic read
and write operations for each storage service. For table storage
service, we also use an SQL-style query to test the performance
of table lookup. In §6.1, we show that these operations account for
over 90% of the storage operations used by a realistic e-commerce
application.
Time to consistency. This metric measures the time between when
a datum is written to the storage service and when all reads for the
datum return consistent and valid results. Such information is use-
ful to cloud customers, because their applications may require data
to be immediately available with a strong consistency guarantee.
Except for AppEngine, cloud providers do not support storage ser-
vices that span multiple data centers. Therefore, we focus on con-
sistency when the reads and writes are both done from instances
inside the same data center.
Cost per operation. The ﬁnal metric measures how much each
storage operation costs. With this metric, a customer can compare
the cost-effectiveness across providers.
3.3.3 Intra-cloud Network
The intra-cloud network connects a customer’s instances among