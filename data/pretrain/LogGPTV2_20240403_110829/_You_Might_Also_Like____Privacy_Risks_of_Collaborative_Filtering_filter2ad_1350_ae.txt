(customer, date) pairs in which the customer made 5 or fewer
transactions. The reason for this is that some users’ activity
is spiky and they rate a hundred or more movies in a single
day. This makes inference too easy because guessing popular
movies at random would have a non-negligible probability of
success. We focus on the hard case instead.
Results. The results of our experiment are summarized in
Fig. 8. The algorithm is conﬁgured to yield at most as many
inferences as there are transactions: 1510/1570 ≈ 0.96 per
vulnerable user for our one-month period. Of these, 22%
are correct. We can trade off yield against accuracy by only
outputting higher-scoring inferences. An accuracy of 80% is
reached when yield is around 141, which translates to 7.5% of
transactions correctly inferred. At an accuracy level of 90%,
we can infer 4.5% of all transactions correctly. As mentioned
earlier, these numbers are averages over 10 trials.
Inference vs. prediction. Fig. 9 shows that
transactions
involving obscure items are likely to be inferred in our simu-
lated experiment. Fig. 10 shows that our inference algorithm
accurately infers even items that a predictor would rank poorly
for a given user. For this graph, we used a predictor that
maximizes the sum-of-cosines score, which is the natural
choice given that we are using cosine similarity for the item-
to-item recommendations. The fact that the median prediction
rank of inferred items lies outside the top 1,000 means,
intuitively, that we are not simply inferring the items that the
user is likely to select anyway.
VIII. MITIGATION
8Reported numbers are averages over 10 trials; in each trial, we restricted
the dataset to a random sample of 10,000 users out of the 460,000.
Differential privacy. Differential privacy is a set of algorithms
and analytical techniques to quantify and reduce privacy risk
algorithms exploit the fact that modern recommender systems
make a large amount of information available for automated
collection through their API. Restricting publicly available
recommendation information may signiﬁcantly complicate
(but may not completely foil) the attack, while preserving
legitimate functionality.
Limit the length of related-items lists. Amazon’s “Customers
who bought this item also bought . . . ” or Last.fm’s “Similar
music” lists can contain more than 100 items (although on
Amazon, only the top 10 are available for automated collection
via the API). Recommendations near the top of an item’s
related-items list have a strong relationship with that item,
which is unlikely to be impacted signiﬁcantly by a single
purchase. The ordering of recommendations near the bottom
of the list is more volatile and reveals more information.
Factor item popularity into update frequency. Less popular
items tend to be more identifying, so limiting the frequency
of updates involving these items may decrease the efﬁcacy of
our inference attacks. For example, purchases of less popular
items may be batched and reﬂected in the item-to-item matrix
only several times a year. The same applies to their “sales
rank,” which for rare items can be sensitive even to a single
purchase. Unfortunately, this mitigation technique may dull
the impact of sudden, important shifts in the data, such as a
surge in popularity of a previously obscure book.
Avoid cross-genre recommendations. In general, “straddlers,”
i.e., customers with interests in multiple genres, tend to be at
higher risk for privacy breaches. This risk could be mitigated
by avoiding cross-genre recommendations except when items
have strong relationships. This has the shortcoming of ob-
structing potentially surprising, but useful recommendations.
Limit the speed and/or rate of data access. The large-scale,
passive attacks described in this paper require that the attacker
extract a somewhat large quantity of data from the recom-
mender system over a long period of time. Limiting the speed
of access (by rate-limiting the API, using crawler-detection
heuristics, etc.) may reduce the amount of available data and
consequently the scale of the attack. Unfortunately, this may
also prevent some legitimate uses of the data. Furthermore,
these limits may not stop smaller, more focused attacks or a
capable attacker (e.g., one with access to a botnet).
It is hard to set a limit which would completely foil the
attack. For example, Hunch’s API limits each “developer
key” to 5,000 queries per day (an attacker can easily acquire
multiple keys). Our experiments in Section VI-A required
between 500 and 2,500 queries per day per target user.
User opt-out. Many sites allow users to opt out of their
recommender systems, but the opt-out is often incomplete.
Amazon allows customers to request that certain purchases
not be used for their own personalized recommendations. This
option is primarily used to prevent gift purchases from in-
ﬂuencing one’s recommendations. For customers with privacy
concerns, a more useful option would be to prevent a purchase
from inﬂuencing the recommender system in any manner at all.
Fig. 9. Likelihood of inference as a function of movie popularity.
Fig. 10. Simulated recommender: Inferences vs. predictions.
to individual participants when answering statistical database
queries [9]. Two threads of differential privacy research are
potentially applicable to recommender systems. First, Mc-
Sherry and Mironov showed how to construct differentially
private covariance matrices, with some loss of accuracy for
the collaborative ﬁltering algorithms that use them [23]. While
this is promising, they do not consider updates to the matrix,
and it is not known how to build a dynamic system using this
approach without destroying the privacy guarantee. The second
line of research is on differential privacy under “continual
observation,” which directly addresses the needs of a dynamic
system [5, 10]. These techniques break down, however, for
systems with a large “surface area” such as similarity lists for
every item. Designing a differentially private online recom-
mender system remains an open problem.
Restrict information revealed by the system. Our inference
If users habitually chose to opt out, however, recommendation
quality could suffer signiﬁcantly.
While each mitigation strategy has limitations, a careful
combination of several techniques may provide substantial
practical beneﬁts with only modest drawbacks.
IX. RELATED WORK
Privacy and collaborative ﬁltering. To our knowledge, this is
the ﬁrst paper to show how to infer individual behavior from
the public outputs of recommender systems. Previous work
on privacy risks of recommenders focused on “straddlers”
whose tastes span unrelated genres and assumed that
the
attacker is given the entire (anonymized) database of user
transactions [30]. This model may be applicable in scenarios
where collaborative ﬁltering is outsourced, but is unrealistic for
real-world recommender systems. Similarly, de-anonymization
attacks require access to static datasets [13, 26].
Shilling attacks on collaborative ﬁltering systems [24, 25]
aim to inﬂuence the system by causing certain items to be rec-
ommended more often. We brieﬂy mention an active attack on
user-to-item collaborative ﬁltering which is somewhat similar,
but pursues a completely different goal.
Research on “social recommendations”—made solely based
on a social graph—has shown that accurate recommendations
necessarily leak information about
the existence of edges
between speciﬁc nodes in the graph [22]. This work differs
from ours in that it (i) does not model user transactions, only
edges in the social graph, (ii) does not consider temporal
dynamics, and (iii) analyzes recommendations made to a user
rather than public recommendations.
Previous work on protecting privacy in collaborative recom-
mender systems aimed to hide individual user records from
the system itself [4, 29, 32, 36]. These papers do not address
the risk that individual actions can be inferred from temporal
changes in the system’s public recommendations and do not
appear to provide much protection against this threat.
Privacy of aggregated data. Our attacks belong to a broad
class of attacks that infer individual inputs from aggregate
statistics. Disclosure of sensitive data from statistical sum-
maries has long been studied in the context of census data [33].
Dinur and Nissim showed theoretically that an attacker who
can query for arbitrary subsets of rows of a private database
can learn the entire database even if noise has been added
to aggregated answers [7]. Differential privacy was developed
in part
to provide a rigorous methodology for protecting
privacy in statistical databases [8, 9]. Attacks on statistical
databases often exploit the aggregates that happen to involve
too few individuals. By contrast, we show that even with large
aggregates, temporal changes can reveal underlying inputs.
Homer et al. showed that given a statistical summary of
allele frequencies of a DNA pool—such as might be published
in a genome-wide association study (GWAS)—it is possible to
detect whether or not a target individual is represented in the
pool, provided that the attacker has access to the individual’s
DNA [15]. The attack exploits the fact that DNA is very high-
dimensional, thus the number of attributes is much greater
than the number of records under consideration. Wang et al.
strengthened the attack of Homer et al. and also developed a
second type of attack, which uses a table of pairwise correla-
tions between allele frequencies (also frequently published in
GWA studies) to disaggregate the table into individual input
sequences [34]. By contrast, the inference attacks described in
this paper are not based on disaggregation.
X. CONCLUSIONS
Recommender systems based on collaborative ﬁltering have
become an essential component of many websites. In this
paper, we showed that
their public recommendations may
leak information about the behavior of individual users to an
attacker with limited auxiliary information. Auxiliary informa-
tion is routinely revealed by users, but these public disclosures
are under an individual’s control: she decides which items
to review or discuss with others. By contrast, item similarity
lists and item-to-item covariances revealed by a recommender
system are based on all
including ones that
users would not disclose voluntarily. Our algorithms leverage
this to infer users’ non-public transactions, posing a threat
to privacy. We utilize aggregate statistics which contain no
“personally identiﬁable information” and are widely available
from popular sites such as Hunch, Last.fm, LibraryThing, and
Amazon. Our attacks are passive and can be staged by any
user of the system. An active attacker can do even more.
transactions,
We study larger, established sites as well as smaller and/or
newer sites. Our results in the latter category are stronger, sup-
porting the intuitive notion that customers of larger sites are
generally safer from a privacy perspective and corroborating
the ﬁndings in [23]. Smaller datasets increase the likelihood
that individual transactions have a perceptible impact on the
system’s outputs.
Our work concretely demonstrates the risk posed by data
aggregated from private records and undermines the widely
accepted dichotomy between “personally identiﬁable” indi-
vidual records and “safe,” large-scale, aggregate statistics.
Furthermore, it demonstrates that the dynamics of aggregate
outputs constitute a new vector for privacy breaches. Dynamic
behavior of high-dimensional aggregates like item similarity
lists falls beyond the protections offered by any existing
privacy technology, including differential privacy.
Modern systems have vast surfaces for attacks on privacy,
making it difﬁcult to protect ﬁne-grained information about
their users. Unintentional leaks of private information are akin
to side-channel attacks: it is very hard to enumerate all aspects
of the system’s publicly observable behavior which may re-
veal information about individual users. Increasingly, websites
learn from—and indirectly expose—aggregated user activity in
order to improve user experience, provide recommendations,
and support many other features. Our work demonstrates the
inadequacy of current theory and practice in understanding the
privacy implications of aggregated data.
ACKNOWLEDGEMENTS
We thank Ilya Mironov for useful discussions and Ian
Davey, Benjamin Delaware, Ari Feldman, Josh Kroll, Joshua
Leners, and Bill Zeller for helpful comments on earlier drafts
of this paper. The research described in this paper was partially
supported by the NSF grants CNS-0331640, CNS-0716158,
and CNS-0746888, Google research awards, the MURI pro-
gram under AFOSR grant no. FA9550-08-1-0352, and the
DHS Scholarship and Fellowship Program under DOE contract
no. DE-AC05-06OR23100.
REFERENCES
[1] G. Adomavicius and A. Tuzhilin.
Toward the next
generation of recommender systems: A survey of the
state-of-the-art and possible extensions. TKDE, 17(6),
2005.
[2] R. Bell, Y. Koren, and C. Volinsky. The BellKor solution
to the Netﬂix Prize. http://www.netﬂixprize.com/assets/
ProgressPrize2007 KorBell.pdf.
[3] A. Borges. Toward a new supermarket layout: From
industrial categories to one stop shopping organization
through a data mining approach. In SMA Retail Sympo-
sium, 2003.
[4] J. Canny. Collaborative ﬁltering with privacy. In S & P,
2002.
[5] H. Chan, E. Shi, and D. Song. Private and continual
release of statistics. In ICALP, 2010.
[6] M. Deshpande and G. Karypis. Item-based top-n recom-
mendation algorithms. TISSEC, 22(1), 2004.
[7] I. Dinur and K. Nissim. Revealing information while
preserving privacy. In PODS, 2003.
[8] C. Dwork. Differential privacy. In ICALP, 2006.
[9] C. Dwork. Differential privacy: a survey of results. In
TAMC, 2008.
[10] C. Dwork, M. Naor, T. Pitassi, and G. Rothblum. Dif-
ferential privacy under continual observation. In STOC,
2010.
[11] L. Fortnow. Outed by Amazon. http://weblog.fortnow.
com/2008/02/outed-by-amazon.html (Accessed Nov 17,
2010).
[12] E. Frank and I. H. Witten. Generating accurate rule sets
without global optimization. In ICML, 1998.
[13] D. Frankowski, D. Cosley, S. Sen, L. Terveen, and
J. Riedl. You are what you say: privacy risks of public
mentions. In SIGIR, 2006.
[14] R. Garﬁnkel, R. Gopal, B. Pathak, R. Venkatesan, and
F. Yin. Empirical analysis of the business value of
recommender systems. http://ssrn.com/abstract=958770,
2006.
[15] N. Homer, S. Szelinger, M. Redman, D. Duggan,
W. Tembe, J. Muehling, J. Pearson, D. Stephan, S. Nel-
son, and D. Craig. Resolving individuals contributing
trace amounts of DNA to highly complex mixtures using
high-density SNP genotyping microarrays. PLoS Genet,
4, 2008.
[16] http://blog.hunch.com/?p=8264
(Accessed Nov
19,
[17] D. Irani, S. Webb, K. Li, and C. Pu. Large online social
footprints–an emerging threat. In CSE, 2009.
[18] http://blog.last.fm/2009/03/24/
lastfm-radio-announcement (Accessed Nov 2, 2010).
[19] http://www.librarything.com/press/ (Accessed Nov 10,
2010).
2010).
[20] G. Linden, J. Jacobi, and E. Benson. Collaborative rec-
ommendations using item-to-item similarity mappings.
U.S. Patent 6266649. http://www.patentstorm.us/patents/
6266649/fulltext.html, 2008.
[21] G. Linden, B. Smith, and J. York. Amazon.com recom-
mendations: Item-to-item collaborative ﬁltering. In IEEE
Internet Computing, January-February 2003.
[22] A. Machanavajjhala, A. Korolova, and A. Sarma. Per-
sonalized social recommendations - accurate or private?
Manuscript, 2010.
[23] F. McSherry and I. Mironov. Differentially private
recommender systems. In KDD, 2009.
[24] B. Mehta and W. Nejdl.
Unsupervised strategies
for shilling detection and robust collaborative ﬁltering.
UMUAI, 19(1–2), 2009.
[25] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams.
Effective attack models for shilling item-based collabo-
rative ﬁltering systems. In WebKDD, 2005.
[26] A. Narayanan and V. Shmatikov.
Robust de-
anonymization of large sparse datasets. In S & P, 2008.
[27] A. Narayanan and V. Shmatikov. De-anonymizing social
networks. In S & P, 2009.
[28] http://www.netﬂixprize.com/rules
(Accessed Nov 19,
2010).
[29] H. Polat and W. Du.
ommendation on horizontally partitioned data.
Intelligence, 2005.
Privacy-preserving top-n rec-
In Web
[30] N. Ramakrishnan, B. Keller, B. Mirza, A. Grama, and
G. Karypis. Privacy risks in recommender systems. In
IEEE Internet Computing, November-December 2001.
[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-
based collaborative ﬁltering recommendation algorithms.
In WWW, 2001.
[32] R. Shokri, P. Pedarsani, G. Theodorakopoulous, and J-
P. Hubaux. Preserving privacy in collaborative ﬁltering
through distributed aggregation of ofﬂine proﬁles.
In
RecSys, 2009.
[33] C. Sullivan. An overview of disclosure principles. U.S.
Census Bureau Research Report, 1992.
[34] R. Wang, Y Li, X. Wang, H. Tang, and X. Zhou.
Learning your identity and disease from research papers:
information leaks in genome wide association study. In
CCS, 2009.
[35] Weka 3 - data mining software in Java. http://www.cs.
waikato.ac.nz/ml/weka/ (Accessed Nov 3, 2010).
[36] J. Zhan, C. Hsieh, I. Wang, T. Hsu, C. Liau, and D. Wang.
Privacy-preserving collaborative recommender systems.
In SMC, 2010.