(cid:2)
s
else
10:
11:
12:
13:
end if
14: end for
return B
Algorithm 2 kMine(g,k)
Input: a connected component g, k
Output: S, a collection of subgraphs
1: S ← ∅
2: for node(u) in g do
S ← S
3:
4: end for
5: for each edge, euv, ing , ordered by weights, wuv, do
6:
wij ≥ k ∗ max{wij|eij ∈ Edge(u, v)}
Graph(u)
(cid:3)
(cid:2)
if
then
eij∈Edge(u,v)
Graph(u) ← merge(Graph(u), Graph(v))
S ← S\{Graph(v)}
7:
8:
9:
end if
10: end for
11: return S
Signiﬁcance Testing
3.2.2
We next describe our statistical tests for the candidate
subgraph returned by the graph mining algorithm. We present
three p-value computation techniques: empirical p-values
that can be viewed as data-driven false positive rates, re-
sampling p-values that are approximations to empirical p-
values and useful when the training data is small, and per-
mutation p-values which complement the other two. Permu-
tation p-values consider how concentrated the positive edges
are, while the other p-values additionally incorporate infor-
mation about how many positive edges there are relative to
goodware and malware.
Note that statistical signiﬁcance is not a property of a
subgraph – it is a property of the procedure that found the
subgraph. To see why, consider the following two types of
search strategies. Strategy A ignores edge weights when it
returns a candidate subgraph while Strategy B returns the
subgraph with largest average edge weight. If the subgraph
returned by Strategy A has a high average edge weight, this
is likely to be statistically signiﬁcant because such an occur-
rence is generally unlikely to happen by chance; on the other
hand, Strategy B is expected return subgraphs with large av-
erage edge weights so the bar is higher – the output returned
by B is statistically signiﬁcant only if its average edge weight
is much higher than what we would normally expect from
B. This phenomenon underlies the multiple testing problem
[5] and our procedures automatically account for it.
Empirical p-values with reference populations.
Empirical p-values are a comparison between a subgraph
extracted from a given SDG to subgraphs extracted from
SDGs belonging to a reference population. Let G1, . . . , GN
be weighted SDGs from a reference population (e.g., the set
of goodware in the training data or the set of malware in
the training data). For each Gi, let Si be the subgraph
extracted from Gi by the subgraph mining algorithm and
let Bi be the average weight of edges in Si.1 To test a new
∗
be its extracted subgraph and let B
executable G
be the average edge weight of S
. The empirical p-value
is the fraction of the reference population whose subgraphs
had higher average edge weight: 1
N
i=1 1{Bi≥B∗}.
(cid:3)N
, let S
∗
∗
∗
∗
This p-value is aﬀected by the concentration of positive
that is returned), their number,
edges (which aﬀects the B
and the magnitude of their weights. The null hypothesis
is that the positive edge weights are not concentrated and
the sampling distribution is the empirical distribution of the
reference population.
Note that there are two possible reference populations:
the training goodware and the training malware. A low p-
value with respect to the malware population is indicative
of an application that is highly suspicious. A moderate p-
value with respect to the malware population and a low
p-value with respect to the goodware population indicates
typical malware behavior. A high p-value with respect to
the malware population and a low p-value with respect to
the goodware population indicates a borderline application
– it performs operations that are unusual for goodware but
are not that suspicious relative to previously seen malware
behavior (in the training data).
Note that computation of empirical p-values requires a
one-time pre-processing of the reference population.
Resampling p-values with reference populations.
Given a new graph G
An empirical p-value is accurate when the training data is
large (since it would be an average over many data points).
For smaller data sets, we propose the resampling p-values
returned by Algorithm 3.
∗
, it ﬁrst creates a set of resampled
∗
(r)
graphs G
k by replacing the edge weights of G
with weights sampled from a distribution Pweight (which we
will discuss shortly). To compute the p-value, it compares
to the
the average weight of the subgraph extracted from G
(r)
average weight of the subgraphs extracted from the G
i
(r)
1 , . . . , G
∗
.
There are several ways of obtaining a distribution Pweight.
Resampled p-values with respect to the goodware reference
population use the empirical distribution of edge weights
from SDGs in the training goodware; resampled p-values
with respect to the malware reference population use the
empirical distribution of edge weights from SDGs in the
training malware.
1We call Bi the test statistic. We use average edge weight
but other statistics can be used too.
72
Algorithm 3 Resampling p-values
(cid:22) (cid:21) (cid:10) (cid:2) (cid:23) (cid:2) (cid:10) (cid:2) (cid:4) (cid:11)
∗
(a new graph to test)
∗ ← average edge weight of SubgraphMiner(G
Input: Pweight (a distribution over edge weights),
Input: SubgraphMiner (a subgraph mining algorithm)
Input: G
Output: a p-value
1: B
2: for i = 1, . . . , k do
Create graph G
3:
(r)
For each edge in G
4:
i
sample from Pweight
(r)
i with the same structure as G
i ← average edge weight of SubgraphMiner(G
(r)
∗
∗
)
assign it a weight as a random
5: B
6: end for
7: return 1
k
(cid:3)k
i=1 1{B(r)
i >=B∗}
(cid:3)(cid:8)(cid:8)(cid:5)(cid:37)(cid:43)(cid:8)(cid:17)(cid:6)(cid:8)(cid:10)(cid:6)(cid:54)(cid:16)(cid:9)(cid:8)(cid:5)(cid:6)(cid:8)(cid:10)(cid:6)(cid:20)(cid:6)(cid:37)(cid:5)(cid:37)(cid:10)(cid:16)(cid:27)(cid:6)(cid:36)(cid:10)(cid:2)(cid:20)(cid:8)(cid:9)(cid:10)(cid:11)(cid:6)(cid:10)(cid:4)(cid:6)(cid:20)(cid:6)(cid:43)(cid:16)(cid:10)(cid:8)(cid:5)(cid:2)(cid:8)(cid:5)(cid:29)(cid:6)(cid:43)(cid:16)(cid:10)(cid:2)(cid:5)(cid:17)(cid:17)(cid:18)
(cid:3)(cid:8)(cid:8)(cid:5)(cid:37)(cid:43)(cid:8)(cid:17)(cid:6)(cid:8)(cid:10)(cid:6)(cid:54)(cid:16)(cid:9)(cid:8)(cid:5)(cid:6)(cid:8)(cid:10)(cid:6)(cid:20)(cid:6)(cid:37)(cid:5)(cid:37)(cid:10)(cid:16)(cid:27)(cid:6)(cid:36)(cid:10)(cid:2)(cid:20)(cid:8)(cid:9)(cid:10)(cid:11)(cid:6)(cid:10)(cid:4)(cid:6)(cid:20)(cid:6)(cid:12)(cid:9)(cid:11)(cid:29)(cid:10)(cid:54)(cid:17)(cid:6)(cid:17)(cid:27)(cid:17)(cid:8)(cid:5)(cid:37)
(cid:43)(cid:16)(cid:10)(cid:2)(cid:5)(cid:17)(cid:17)
(cid:3)(cid:8)(cid:8)(cid:5)(cid:37)(cid:43)(cid:8)(cid:17)(cid:6)(cid:8)(cid:10)(cid:6)(cid:54)(cid:16)(cid:9)(cid:8)(cid:5)(cid:6)(cid:8)(cid:10)(cid:6)(cid:20)(cid:6)(cid:37)(cid:5)(cid:37)(cid:10)(cid:16)(cid:27)(cid:6)(cid:36)(cid:10)(cid:2)(cid:20)(cid:8)(cid:9)(cid:10)(cid:11)(cid:6)(cid:10)(cid:4)(cid:6)(cid:20)(cid:6)(cid:43)(cid:16)(cid:5)(cid:45)(cid:9)(cid:10)(cid:55)(cid:17)(cid:36)(cid:27)(cid:6)(cid:36)(cid:10)(cid:20)(cid:29)(cid:5)(cid:29)
(cid:43)(cid:16)(cid:10)(cid:2)(cid:5)(cid:17)(cid:17)(cid:18)
.
(cid:58)(cid:11)(cid:55)(cid:37)(cid:5)(cid:16)(cid:20)(cid:8)(cid:5)(cid:17)(cid:6)(cid:37)(cid:20)(cid:11)(cid:27)(cid:6)(cid:17)(cid:27)(cid:17)(cid:8)(cid:5)(cid:37)(cid:6)(cid:4)(cid:9)(cid:36)(cid:5)(cid:17)(cid:6)(cid:20)(cid:11)(cid:29)(cid:6)(cid:29)(cid:9)(cid:16)(cid:5)(cid:2)(cid:8)(cid:10)(cid:16)(cid:9)(cid:5)(cid:17)(cid:18)
(cid:3)(cid:29)(cid:29)(cid:17)(cid:6)(cid:10)(cid:16)(cid:6)(cid:37)(cid:10)(cid:29)(cid:9)(cid:4)(cid:9)(cid:5)(cid:17)(cid:6)(cid:63)(cid:11)(cid:8)(cid:5)(cid:16)(cid:11)(cid:5)(cid:8)(cid:6)(cid:58)(cid:65)(cid:43)(cid:36)(cid:10)(cid:16)(cid:5)(cid:16)(cid:6)(cid:2)(cid:10)(cid:10)(cid:51)(cid:9)(cid:5)(cid:17)
(r)
i
)
Figure 2: Description of LdPinch Activities [15].
Note that this test only resampled edge weights and per-
forms no randomization of the structure of the graph. We
intentionally avoid randomizing the structure of the graph
because there is no evidence that existing random graph
models are plausible generative models for system call de-
pendency graphs.
Permutation p-values.
Permutation p-values are designed only to check for con-
centrations of positive edges. They do not compare the mag-
nitude of the edge weights to reference populations. Hence,
their role is to help us understand empirical and resampled
p-values.
If empirical and resampled p-values are low it
could be due to two reasons – a high concentration of pos-
itive edge weights in the extracted subgraph or large edge
weight magnitudes. Generally, the concentration of positive
edge weights is responsible if the permutation p-value is low;
the edge weight magnitudes are responsible if the permuta-
tion p-value is high.
(r)
1 , . . . , G
∗
Permutation p-values are computed in a similar way to re-
sampled p-values. The resampled p-value computation cre-
(r)
N by resampling the edge
ates a set of graph G
. The permutation p-value compu-
weights for the graph G
tation creates G
and permuting
its weights (i.e., randomly reassigning the weights to diﬀer-
and a
ent edges). Again it extracts a subgraph S
subgraph Si from each of the G
and counts the fraction
of Si that have a higher average edge weight than S
by making a copy of G
from G
(r)
i
(r)
i
∗
∗
∗
∗
.
4. EXPERIMENTS
We collected 2393 executables from 50 malware families to
produce 2393 system call dependency graphs. We collected
50 goodware programs (based on the list used by [12]) and
executed the goodware binaries multiple times to generate
434 goodware system call dependency graphs. We also ob-
tained data from the McAfee website [15] which contains a
plain text description of known activities of each malware
family in our collection. An example of this kind of data
is shown in Figure 2, which contains the description of a
sample from the LdPinch family.
To generate the SDGs, malware and goodware binaries
were executed in a sandbox; invoked system calls were traced
by WUSSTrace [24] (which traces system calls by injecting
a shared library in the address space of the traced process).
All binaries were executed for up to two minutes. The exe-
cution traces were parsed using Pywuss [3] to generate SDGs
73
(with an edge between system call invocations x and y if x
returns a handle that y consumes).2
4.1 Malware Detection
Recall that our framework performs two distinct func-
tions: malware detection (using a linear classiﬁer such as
logistic regression) and subsequent extraction of statistically
signiﬁcant subgraphs (from samples it labels as malware) to
help prioritize an expert’s analysis of the executable.
In this section we compare the malware detection rates
with Holmes [12] and with two commercial anti-virus prod-
ucts, AVG antivirus [2] and ThreatFire [22],3 at the 0% false
positive rate on the ROC curve. Note that a comparison to
Holmes is qualitative at best: we have to use reported num-
bers [12] because neither the code nor data was available
(but our dataset was constructed to closely match the de-
scription in [12]). It is also not clear if the training goodware
for Holmes was excluded from the testing set.
We randomly split the data (SDGs with malware/goodware
labels) into three pieces while ensuring that malware fami-
lies present in one piece do not appear in the other pieces.
We ensured that one piece contained approximately 60%
of the total malware and 60% of the total goodware; this
piece was used for training (i.e. training logistic regression
models [11] with diﬀerent regularization parameters) and we
used the F-score method for feature selection [6] to keep only
the top 50% of the features (see Section 3.1.2). The second
piece, the holdout set (used for model selection), contained
approximately 20% of the total goodware and 20% of the to-
tal malware. The third piece, also containing approximately
20% of the goodware and 20% of the malware, was used to
evaluate the accuracy of the selected model. We repeated
this partitioning procedure ﬁve times (with diﬀerent sets of
families/programs in each piece) and averaged the results.
The results are shown in Table 1. The primary conclusion
is that the linear classiﬁer is good at separating malware
from goodware and so its weight parameters form a reason-
able basis for statistical testing of subgraphs.
4.2 The Silver Standard
Having evaluated detection rates, we must next evaluate
the quality of extracted subgraphs. This involves matching
2In our own attempts to reproduce work such as [3], we
found that producing an SDG using heuristics instead of
dynamic taint analysis [17] resulted in almost the same pre-
cision and recall. Reﬁned SDGs are useful for engineering
purposes but would not be expected to produce dramatically
diﬀerent results.
3In order to test performance on unseen malware, we could
not use the most recent versions of AVG and ThreatFire as
they have been updated to include our malware samples.
AVG ThreatFire Holmes Our Framework
58.37
86.56*
67.08
86.77
Table 1: Malware detection rates at 0% false positive.
*Reported from [12]
them to plain text descriptions of malware behavior. One
way to do this is manually [12]. However, there are draw-
backs to the manual approach. First, manual judgments
require considerable expertise (and can still be noisy, with
high false positive rates [14]). Second, they can lead to ex-
perimenter bias. Thus we seek a more automated approach
with the creation of an evaluation dataset, called the silver
standard, which we describe next.
An ideal dataset would contain annotations of system call
dependency graphs that indicate which subgraphs corre-
spond to malicious activity. Such a “gold standard” does not
exist so we constructed an approximation to it, called the
silver standard, using the plain-text descriptions obtained
from the McAfee website [15] (see Figure 2).
For each piece of malware, we ﬁrst convert its plain-text
activity into a list of system calls. For example, the activity
‘Creates registry keys and data values to persist on OS re-
boot’ is converted to the list {NtOpenKey, NtSetValueKey}.
Note that there is no unique translation between textual de-
scription and system calls and so some noise is necessarily
introduced in this process.
Now, the system call dependency graphs consist of a dis-
joint union of many (usually small) connected components.
For each connected component, we keep it if it contains an
edge between any two system calls on this list. Then we
remove edges whose vertices are not in the system call list.
Next, we apply the Kruskal-based algorithm (Algorithm 1)
to each component in order to prune irrelevant edges. We
remove vertices for wait-related system calls and NtClose
(whose presence/absence has no causal eﬀect on malicious
behavior). The wait-related system calls, such as NtWait-
ForMultipleObjects, are used to wait until a speciﬁed crite-
ria is met or a time-out interval has elapsed. Finally, we also
remove repetitive/redundant NtFreeVirtualMemory and Nt-
FlushVirtualMemory invocations y1, y2, . . . , yk that have an
incoming edge from the same node x and have no outgoing
edges; we only keep y1, the ﬁrst of these duplicate calls.
In this way, for each malware SDG, we obtain a sub-
graph (consisting of possibly many connected components)
that is marked as malicious activity. Such a subgraph is
called a silver standard graph. As an example, Figure 3
shows the connected components of a silver standard graph
from a sample in the Banbra family. According to Syman-
tec [21], Banbra is a Trojan horse that attempts to steal
ﬁnancial information from the compromised computer and
send the information to a remote location. Component SS1
was induced by an attempt to launch an instance of Internet
Explorer. Components SS2 and SS3 resulted from writing