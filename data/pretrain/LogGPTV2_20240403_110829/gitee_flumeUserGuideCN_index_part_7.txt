请阅读 [Configuring Kafka Clients
SSL](http://kafka.apache.org/documentation#security_configclients) SSL
中描述的步骤来了解用于微调的其他配置设置，例如下面的几个例子：启用安全策略、密码套件、启用协议、truststore或秘钥库类型。
服务端认证和数据加密的一个配置实例：
``` properties
a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.source1.kafka.bootstrap.servers = kafka-1:9093,kafka-2:9093,kafka-3:9093
a1.sources.source1.kafka.topics = mytopic
a1.sources.source1.kafka.consumer.group.id = flume-consumer
a1.sources.source1.kafka.consumer.security.protocol = SSL
# 如果在全局配置了SSL下面两个参数可省略，但是如果想使用自己独立的truststore，就可以把这两个参数加上。
a1.sources.source1.kafka.consumer.ssl.truststore.location=/path/to/truststore.jks
a1.sources.source1.kafka.consumer.ssl.truststore.password=
```
1.9版本开始增加了全局的ssl配置，因此这里的truststore的配置是可选配置，不配置会使用全局参数来代替，想了解更多可以参考
[SSL/TLS 支持](#ssltls-支持) 。
注意，默认情况下 *ssl.endpoint.identification.algorithm*
这个参数没有被定义，因此不会执行主机名验证。如果要启用主机名验证，请加入以下配置：
``` properties
a1.sources.source1.kafka.consumer.ssl.endpoint.identification.algorithm=HTTPS
```
开启后，客户端将根据以下两个字段之一验证服务器的完全限定域名（FQDN）：
1)  Common Name (CN) 
2)  Subject Alternative Name (SAN)
如果还需要客户端身份验证，则还需要在 Flume
配置中添加以下内容，或者使用全局的SSL配置也可以，参考 [SSL/TLS
支持](#ssltls-支持) 。 每个Flume 实例都必须拥有其客户证书，来被Kafka
实例单独或通过其签名链来信任。 常见示例是由 Kafka
信任的单个根CA签署每个客户端证书。
``` properties
# 下面两个参数1.9版本开始不是必须配置，如果配置了全局的keystore，这里就不必再重复配置
a1.sources.source1.kafka.consumer.ssl.keystore.location=/path/to/client.keystore.jks
a1.sources.source1.kafka.consumer.ssl.keystore.password=
```
如果密钥库和密钥使用不同的密码保护，则 *ssl.key.password*
属性将为消费者密钥库提供所需的额外密码：
``` properties
a1.sources.source1.kafka.consumer.ssl.key.password=
```
**Kerberos安全配置：**
要将Kafka Source
与使用Kerberos保护的Kafka群集一起使用，请为消费者设置上面提到的consumer.security.protocol
属性。 与Kafka实例一起使用的Kerberos
keytab和主体在JAAS文件的"KafkaClient"部分中指定。
"客户端"部分描述了Zookeeper连接信息（如果需要）。
有关JAAS文件内容的信息，请参阅 [Kafka
doc](http://kafka.apache.org/documentation.html#security_sasl_clientconfig)
。 可以通过flume-env.sh中的JAVA_OPTS指定此JAAS文件的位置以及系统范围的
kerberos 配置：
``` properties
JAVA_OPTS="$JAVA_OPTS -Djava.security.krb5.conf=/path/to/krb5.conf"
JAVA_OPTS="$JAVA_OPTS -Djava.security.auth.login.config=/path/to/flume_jaas.conf"
```
使用 SASL_PLAINTEXT 的示例安全配置：
``` properties
a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.source1.kafka.bootstrap.servers = kafka-1:9093,kafka-2:9093,kafka-3:9093
a1.sources.source1.kafka.topics = mytopic
a1.sources.source1.kafka.consumer.group.id = flume-consumer
a1.sources.source1.kafka.consumer.security.protocol = SASL_PLAINTEXT
a1.sources.source1.kafka.consumer.sasl.mechanism = GSSAPI
a1.sources.source1.kafka.consumer.sasl.kerberos.service.name = kafka
```
使用 SASL_SSL 的安全配置范例：
``` properties
a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.source1.kafka.bootstrap.servers = kafka-1:9093,kafka-2:9093,kafka-3:9093
a1.sources.source1.kafka.topics = mytopic
a1.sources.source1.kafka.consumer.group.id = flume-consumer
a1.sources.source1.kafka.consumer.security.protocol = SASL_SSL
a1.sources.source1.kafka.consumer.sasl.mechanism = GSSAPI
a1.sources.source1.kafka.consumer.sasl.kerberos.service.name = kafka
# 下面两个参数1.9版本开始不是必须配置，如果配置了全局的keystore，这里就不必再重复配置
a1.sources.source1.kafka.consumer.ssl.truststore.location=/path/to/truststore.jks
a1.sources.source1.kafka.consumer.ssl.truststore.password=
```
JAAS 文件配置示例。有关其内容的参考，请参阅Kafka文档 [SASL
configuration](http://kafka.apache.org/documentation#security_sasl_clientconfig)
中关于所需认证机制（GSSAPI/PLAIN）的客户端配置部分。由于Kafka Source
也可以连接 Zookeeper 以进行偏移迁移，
因此"Client"部分也添加到此示例中。除非您需要偏移迁移，否则不必要这样做，或者您需要此部分用于其他安全组件。
另外，请确保Flume进程的操作系统用户对 JAAS 和 keytab 文件具有读权限。
``` javascript
Client {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  storeKey=true
  keyTab="/path/to/keytabs/flume.keytab"
  principal="flume/flumehost1.example.com@YOURKERBEROSREALM";
};
KafkaClient {
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  storeKey=true
  keyTab="/path/to/keytabs/flume.keytab"
  principal="flume/flumehost1.example.com@YOURKERBEROSREALM";
};
```
#### NetCat TCP Source
这个source十分像nc -k -l \[host\]
\[port\]这个命令，监听一个指定的端口，把从该端口收到的TCP协议的文本数据按行转换为Event，它能识别的是带换行符的文本数据，同其他Source一样，解析成功的Event数据会发送到channel中。
::: hint
::: title
Hint
:::
常见的系统日志都是逐行输出的，Flume的各种Source接收数据也基本上以行为单位进行解析和处理。不论是
[NetCat TCP Source](#netcat-tcp-source)
，还是其他的读取文本类型的Source比如：[Spooling Directory
Source](#spooling-directory-source) 、 [Taildir Source](#taildir-source)
、 [Exec Source](#exec-source) 等也都是一样的。
:::
必需的参数已用 **粗体** 标明。
  属性                           默认值        解释
  ------------------------------ ------------- ----------------------------------------------------------------------------------------------------------------------------------------------
  **channels**                   \--           与Source绑定的channel，多个用空格分开
  **type**                       \--           组件类型，这个是： `netcat`
  **bind**                       \--           要监听的 hostname 或者IP地址
  **port**                       \--           监听的端口
  max-line-length                512           每行解析成Event 消息体的最大字节数
  ack-every-event                true          对收到的每一行数据用"OK"做出响应
  selector.type selector.\*      replicating   可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用 channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
  interceptors interceptors.\*   \--           该source所使用的拦截器，多个用空格分开 拦截器相关的属性配置
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 6666
a1.sources.r1.channels = c1
```
#### NetCat UDP Source
看名字也看得出，跟 [NetCat TCP Source](#netcat-tcp-source)
是一对亲兄弟，区别是监听的协议不同。这个source就像是 nc -u -k -l
\[host\] \[port\]命令一样，
监听一个端口然后接收来自于这个端口上UDP协议发送过来的文本内容，逐行转换为Event发送到channel。
必需的参数已用 **粗体** 标明。
  属性                           默认值        解释
  ------------------------------ ------------- ----------------------------------------------------------------------------------------------------------------------------------------------
  **channels**                   \--           与Source绑定的channel，多个用空格分开
  **type**                       \--           组件类型，这个是：`netcatudp`
  **bind**                       \--           要监听的 hostname 或者IP地址
  **port**                       \--           监听的端口
  remoteAddressHeader            \--           UDP消息源地址（或IP）被解析到Event的header里面时所使用的key名称
  selector.type selector.\*      replicating   可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用 channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
  interceptors interceptors.\*   \--           该source所使用的拦截器，多个用空格分开 拦截器相关的属性配
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = netcatudp
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 6666
a1.sources.r1.channels = c1
```
#### Sequence Generator Source
这个Source是一个序列式的Event生成器，从它启动就开始生成，总共会生成totalEvents个。它并不是一个日志收集器，它通常是用来测试用的。它在发送失败的时候会重新发送失败的Event到channel，
保证最终发送到channel的唯一Event数量一定是 *totalEvents* 个。
必需的参数已用 **粗体** 标明。
::: hint
::: title
Hint
:::
记住Flume的设计原则之一就是传输过程的‘可靠性’，上面说的失败重试以及最终的数量问题，这是毫无疑问的。
:::
  属性                           默认值           解释
  ------------------------------ ---------------- ---------------------------------------------------------------------------------------------
  **channels**                   \--              与Source绑定的channel，多个用空格分开
  **type** selector.type         \--              组件类型，这个是：`seq` 可选值：`replicating` 或 `multiplexing` ，分别表示： 复制、多路复用
  selector.\*                    replicating      channel选择器的相关属性，具体属性根据设定的 *selector.type* 值不同而不同
  interceptors interceptors.\*   \--              该source所使用的拦截器，多个用空格分开 拦截器相关的属性配
  batchSize                      1                每次请求向channel发送的 Event 数量
  totalEvents                    Long.MAX_VALUE   这个Source会发出的Event总数，这些Event是唯一的
配置范例：
``` properties
a1.sources = r1
a1.channels = c1
a1.sources.r1.type = seq
a1.sources.r1.channels = c1
```
#### Syslog Sources
这个Source是从syslog读取日志并解析为
Event，同样也分为TCP协议和UDP协议的，TCP协议的Source会按行（\\n）来解析成
Event，UDP协议的Souce会把一个消息体解析为一个 Event。
::: hint
::: title
Hint
:::
这三个Syslog Sources里面的 *clientIPHeader* 和 *clientHostnameHeader*
两个参数都不让设置成Syslog
header中的标准参数名，我之前并不熟悉Syslog协议，特地去搜了Syslog的两个主要版本的文档
[RFC 3164](https://tools.ietf.org/html/rfc3164) 和 [RFC
5424](https://tools.ietf.org/html/rfc5424) ，并没有看到有
叫做_host_的标准header参数名，两个协议里面关于host的字段都叫做：
HOSTNAME，不知道是不是官方文档编写者没有描述准确，总之大家如果真的使用这个组件，设置一个带个性前缀的值肯定不会有问题，比如：lyf_ip、lyf_host_name。
:::
##### Syslog TCP Source
::: hint
::: title
Hint
:::
这个Syslog TCP
[Source在源码里面已经被@deprecated了](mailto:Source在源码里面已经被@deprecated了)，推荐使用
[Multiport Syslog TCP Source](#multiport-syslog-tcp-source) 来代替。
:::