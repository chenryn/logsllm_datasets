space. In this way, symbolic executors can assign higher
search priority to suspicious candidate paths (and their close
neighbors) until the vulnerable paths are assertively identiﬁed
and the associated path constraints are generated.
Consider a simple C program in Figure 2a consisting
of some conditional operations and loop iterations for an
integer input x. There is an assertion statement in vul f unc()
associated with the range of the argument a (a >= 3), which
is guarded by x in the while loop in line 13. To search for a
vulnerable path, pure symbolic execution tools would typically
set x as symbolic and fork a brand new state after each loop
iteration. The corresponding path exploration space is shown
in Figure 2b as a tree structure. As we can see, a symbolic
executor has to explore both sets of execution paths for the
if statement in line 8. This exploration has to be repeated for
every iteration of the loop.
The statistical approach in StatSym is able to identify
the most likely vulnerable path by constructing predicates
and conditions for the target vulnerability. In this example,
it ﬁrst selects a candidate path covering 7→12→14 using
program execution history (both faulty and non-faulty). This
helps trim off1 the unnecessary subtree of states on the left
side with root node 9. Then, StatSym infers that the range
of values for variable x which is highly correlated with the
vulnerability. As x can directly guide the number of iterations
to be considered by the symbolic executor in line 13, a
large number of subtrees corresponding to the paths (that are
unlikely to be responsible for the vulnerability) are trimmed.
The pruned subtree in Figure 2c illustrates the reduced search
space with StatSym when the candidate path is 7→12→14
with a predicate of (x ≥ 3). With statistical guidance, we
see that the actual states/paths that need to be explored by
StatSym are signiﬁcantly reduced when compared to that of
pure symbolic execution.
III. THREAT MODEL AND ASSUMPTIONS
A. Threat Model
In this work, we consider program vulnerabilities including
software bugs and defects that can manifest by exploiting
program control ﬂows or by manipulating program inputs or
states. When these vulnerabilities are exploited, their effects
will manifest and lead to program crashes or security breaches.
Identifying vulnerable paths leading to the program failure is
crucial for hardening these software systems, e.g., by ﬁltering
faulty inputs, path-sensitive analysis [29], [30] and hardening
code path [2], [31]. In case of more sophisticated attacks,
such as silent corruption of memory values, we note that our
proposed mechanism can still be just as effective, provided that
the point of attack (data corruption) can be identiﬁed through
periodically imaging the memory. Note that vulnerable path
discovery has application in software debugging as well.
1The non-selected paths (that are trimmed off) receive lower priority in
symbolic execution. So, in the (unlikely) worst case when erroneous statistical
inference is made,
to pure
symbolic execution.
the performance of StatSym is equivalent
testing
inputs
sampled
log
program
predicate
construction &
ranking
candidate
path
construction
statistics-guided
symbolic
execution
vulnerable path
and constraints
Fig. 3: Design Overview of Vulnerability Discovery Framework based on Statistics-Guided Symbolic Execution.
B. The Use of Partial Logging
We assume that only partial logging is available at runtime,
which provides incomplete inputs for statistical analysis in
StatSym. This assumption is based on ﬁndings in numerous
prior works that demonstrate full logging to be impractical in
real-world applications [8], [9], [10], [11]. In order to reduce
the performance overhead for runtime logging, prior works
often take advantage of statistical sampling to log only partial
information on the ﬂy. In this paper, we consider logging
only at function entry and exit points, and our logging targets
include program global variables, function parameters and
return values. We note that some program variables may con-
tain sensitive user information. Thus, the logging system may
deﬁne security rules to prevent collecting such information,
e.g., logging only the length of string objects and hashing
function names. To preserve user privacy, Yuan et al. [21]
show how users can be provided with the option to inspect
the runtime logs and eliminate any sensitive information.
C. Existence of Multiple Vulnerabilities and Paths
Multiple vulnerabilities may exist
in a single program,
especially as program size increases. While this paper fo-
cuses on identiﬁcation of single vulnerable path, the proposed
StatSym framework can be easily extended to programs with
multiple vulnerabilities and/or multiple vulnerable paths. Prior
works have studied bug isolation techniques [9], and leverage
machine learning and clustering techniques [11] to separate
log ﬁles pertaining to different bugs to generate statistical
inference for each individual bug. By taking advantage of
these techniques, we can isolate different vulnerabilities and
use StatSym to identify (and eliminate) vulnerable paths one-
by-one through an iterative process until all vulnerabilities and
paths are identiﬁed.
IV. APPROACH OVERVIEW
StatSym makes use of statistical analysis to 1.
infer
candidate paths that are most likely related to the vulnera-
bility and 2. construct and rank predicates (i.e., conditions
associated with program states) based on their relevance to
program failures or security breaches. The results effectively
guide path-driven symbolic execution to concentrate on more
likely candidate paths with higher priority, thus improving the
efﬁciency of vulnerable path identiﬁcation.
StatSym has the following components (Figure 3):
• A runtime sampling and logging component that collects
runtime program information including global variables,
• A statistical
function parameters and return values at function entry
and exit points. To minimize the effects of logging
and the associated performance overheads, we adopt a
probabilistic sampling method.
inference component which analyzes the
runtime program information, constructs predicates (con-
ditions and certain assertions about program states or
variables), and ranks them based on relevance to the
program failure point. A higher conﬁdence score indicates
a higher likelihood of a predicate being indicative of the
target vulnerability.
• A candidate path construction component that generates
candidate vulnerable paths that will receive higher pri-
ority for symbolic execution based on the conﬁdence
scores. Each node in the path represents a function entry
or function exit point (instrumentation location).
• A statistics-guided symbolic execution component that
performs program state exploration based on vulnerable
paths. Once the actual vulnerable path is found,
the
symbolic execution procedure would output the complete
execution path (and path constraints) that leads to the
program failure point.
V. StatSym DESIGN
This section presents our StatSym framework and algo-
rithm design, including (i) predicate construction and ranking,
(ii) candidate path identiﬁcation and (iii) statistics-guided
symbolic execution.
A program can be represented at different granularities. In a
Control Flow Graph (CFG), each node represents a basic block
(a straight-line sequence of code without any branches except
at the entry and exit points), and each directed edge denotes
a possible control transfer (e.g., conditional branches). At a
coarser granularity, a program can also be represented by a
Call Graph (CG), where each node corresponds to a function
or procedure, and an edge represents the call relations between
functions or procedures. Note that CG represents a subset of
CFG information since function calls and returns are also part
of control transfer instructions. In this paper, we use a graph
representation G to denote a program that primarily considers a
CG due to the limitations stemming from the tools used in our
studies (See Section VI for further details). We note that our
framework can be easily extended to include ﬁner granularity
CFG nodes as well if we have the capability to instrument the
program at the basic block block level, and monitor variables
inside these basic blocks. Without loss of generality, we will
refer to the granularity of observation as program blocks.
The graph G consists of a node set N and an edge set
E. We consider program blocks, each consisting of program
code between two instrument points and represented by a node
ni ∈ N in G. The edges in E are directed and denoted by
(ni, nj) where ni ∈ N and nj ∈ N is the head and tail of
the edge respectively. A state comprises a set of variables and
program control transfers that are made visible by source-level
or runtime program instrumentation.
A. Predicate Construction and Ranking
We identify a list of instrumentation locations, program
variables and statements for predicate construction. Such runs
are provided to StatSym by the users, which are then used
by the statistical sampling module. Our evaluation emulates
average user behavior by providing a series of random inputs
to the applications, generating a sufﬁciently large number of
sample runs, and randomly sampling them to assemble a set of
correct executions and faulty executions. Also, to differentiate
program execution states, the same variable instrumented at
different locations is considered separately. For example, a
global variable that appears in two different physical locations
in the program are considered separately from each other.
Predicate Construction: We analyze each variable’s statis-
tics (e.g., distributions) in correct and faulty executions, and
construct the predicate based on the divergence between the
variable’s statistics in these two cases, e.g., offering highest
degree of distinction between them. More precisely, if variable
a at a certain location has a set C of values in correct
executions and a set F of values in faulty executions, formally,
we construct a predicate x = {a ∈ P} for variable a to
optimally separate the instances of a within correct executions
and faulty executions, by minimizing the quantiﬁcation error:
E = |P ∩ C| + |P c ∩ F|,
(1)
where P c is the compliment of P. Intuitively, the predicate
x provides an optimal separation between correct executions
and faulty executions using the distributions of variable a. For
example, if an integer variable a in faulty executions is always
larger than that of correct executions, we can ﬁnd a threshold
σ that separates the two sets and construct a predicate as x =
{a ≥ σ}.
Predicate Conﬁdence Score and Ranking: We derive a con-
ﬁdence score metric to measure the capability of a predicate in
indicating vulnerabilities. Predicate x receives a higher score
if it can better distinguish faulty from correct executions. Let
P(x|C) be the probability (i.e., implied frequency from the
log ﬁles) of predicate x being true in correct executions, and
P(x|F) be the probability in faulty executions. The absolute
difference between these two probabilities is assigned as the
temporary score for the predicate of that speciﬁc instrumenta-
tion location and variable:
s = |P(x|C) − P(x|F)|.
(2)
The larger a predicate score is, the higher likelihood vulnera-
bility is associated with the variable and instrumented location
involved in predicate x.
B. Candidate Path Identiﬁcation
We propose a path identiﬁcation algorithm to statistically
extract program execution paths, traversing locations of highly
ranked predicates and leading to the vulnerability manifes-
tation (failure) point. First, due to possibly incomplete pro-
gram proﬁle from probabilistic logging, we need to construct
the transitions between different instrumented locations from
faulty executions, which is modeled as an association rule
mining problem [32], [31]. Let o(ei) and o(ej) be the number
of occurrences for instrumented locations ei and ej in all
the log ﬁles, we calculate the frequency that ej occurs after
ei, denoted by o(ei → ej). This enables us to calculate the
conﬁdence µ of transition ei → ej:
µ(ei, ej) = o(ei→ej )
o(ei)
(3)
Through identifying all transitions with statistically signif-
icant conﬁdence scores, we are able to construct a transi-
tion graph for each tested program. We propose a heuristic-
based path identiﬁcation algorithm to extract candidate paths
traversing the instrumented locations of high conﬁdence-score
predicates. The path identiﬁcation involves three steps: 1. Find
acyclic paths starting from nodes that have no incoming arcs
(e.g. representing possible program entry points) to the failure
point. The score of each node on the path is represented
by the predicate with the highest score associated with the
instrumented location. From all the acyclic paths, we select
the one with the largest average conﬁdence score. Such a path
is referred to as skeleton in our algorithm. 2. There could be
predicates with high conﬁdence scores that are not included
in the skeleton. To account for these predicates, we use a
greedy algorithm to identify the path segments that branch
out from the skeleton and traverse these high conﬁdence-score
predicates. The path segments linking the skeleton and high
conﬁdence-score predicates are called detours. 3. A candidate
path is constructed by combining the skeleton and detours,
allowing a search to visit high conﬁdence-score predicates on
detours, while moving along the skeleton toward vulnerability
manifestation point.
C. Statistics-Guided Symbolic Execution
A candidate path includes a sequence of instrumented
locations and the associated predicates. An example of the
candidate path consisting of multiple nodes is shown in
Figure 4. The dotted-gray circle is the starting point of the
path (e.g. main() function). Each subsequent node (e.g., green
circles marked by ni) represents an instrumented location in
the program, associated with a predicate pi. The vertical-
dashed red circle denotes the failure point.
Note that
the candidate paths generated from statistical
analysis might not be a viable path during actual program
execution due to incomplete statistical information. To guar-