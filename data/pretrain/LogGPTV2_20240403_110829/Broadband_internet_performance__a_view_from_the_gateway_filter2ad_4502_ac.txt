ICMP
UDP
UDP
TCP
N/A
UDP
TCP
N/A
UDP
5 min
5 min
30 min
30 min
15 min
15 min
30 min
30 min
12 hrs
30 min
30 min
12 hrs
MLab, idle link
500pkts/30sec
Alexa sites
Host
First IP hop
During upload
During download
D-ITG
D-ITG
curlget to Host
/proc/net/dev
ShaperProbe
curlput to Host
/proc/net/dev
ShaperProbe
Table 3: Active measurements periodically collected by the Sam-
Knows and BISMark deployments.
byte count after the HTTP transfer minus the byte count before the
transfer, divided by the transfer time. This yields the combined
throughput of the HTTP transfer and the cross trafﬁc. To measure
capacity, we run ShaperProbe [32] once every twelve hours to mea-
sure UDP capacity. The gateways measure end-to-end latency to a
nearby wide-area host, last-mile latency, and latency-under load to
the last-mile router. They also measure packet loss and jitter using
the D-ITG tool [6]. The gateways perform each measurement at
the frequency presented in Table 3 regardless of cross trafﬁc. All
measurements are synchronized to avoid overlapping towards the
same measurement server.
In January 2011, the BISMark deployment had 14 gateways
across AT&T (DSL) and Comcast (Cable), and two more on Clear
(WiMax). We recruited users from our research lab and other grad-
uate students in the computer science department; some users were
recruited using a recruiting ﬁrm.2 We only study AT&T and Com-
cast using BISMark. The AT&T users form the most diverse set
of users in the current deployment, with ﬁve distinct service plans.
We use data from the same period as the SamKnows study.
5. UNDERSTANDING THROUGHPUT
We study throughput measurements from both the SamKnows
and BISMark deployments. We ﬁrst explore how different mecha-
nisms for measuring throughput can generate different results and
offer guidelines on how to interpret them. We then investigate the
throughput users achieve on different access links, the consistency
of throughput obtained by users, and the factors that affect it. Fi-
nally, we explore the effects of ISP trafﬁc shaping and the implica-
tions it holds for throughput measurement.
5.1 Interpreting Throughput Measurements
Users of access networks are often interested in the through-
put that they receive on uploads or downloads, yet the notion of
“throughput” can vary depending on how, when, and who is mea-
2All of our methods have been approved by Georgia Tech’s insti-
tutional review board.
Figure 4: Comparison of various methods of measuring through-
put. (SamKnows and BISMark)
suring it. For example, a sample run of www.speedtest.net
in an author’s home, where the service plan was 6Mbits/s down
and 512Kbits/s up, reported a downlink speed of 4.4 Mbits/s and
an uplink speed of 140 Kbits/s. Netalyzr reported 4.8 Mbits/s
and 430 Kbits/s. Long-term measurements (from the SamKnows
gateway deployed in that author’s home) paint a different picture:
the user achieves 5.6 Mbits/s down and 452 Kbits/s up. Both
www.speedtest.net and Netalyzr measurements reﬂect tran-
sient network conditions, as well as other confounding factors.
Users cannot complain to their ISPs based solely on these measure-
ments. Although measuring throughput may seem straightforward,
our results in this section demonstrate the extent to which differ-
ent measurement methods can produce different results and, hence,
may result in different conclusions about the ISP’s performance.
We compare several methods for measuring upstream and down-
stream throughput from Table 3. We normalize the values of
throughput by the service plan rates advertised by the ISP so that
we can compare throughput across access links where users have
different service plans.
Throughput measurement techniques—even commonly accepted
ones—can yield variable results. We perform comparisons of
throughput measurement techniques in two locations that have de-
ployed both the SamKnows and BISMark gateways (we are re-
stricted to two due to the logistical difﬁculty in deploying both
gateways in the same location). In both cases, the ISP is AT&T,
but the service plans are different (6 Mbits/s down and 512 Kbits/s
up; and 3 Mbit/s down and 384 Kbits/s up). Figure 4 shows a CDF
of the normalized throughput reported by the four methods we pre-
sented in Table 3. Each data point in the distribution represents a
single throughput measurement by a client. A value of 1.0 on the
x-axis indicates that the throughput matches the ISP’s advertised
rate. None of the four methods achieve that value. This could be
due to many factors: the sync rate of the modem to the DSLAM;
layer-2 framing overhead on the line; or overhead from the mea-
surement techniques themselves. The throughput achieved by mul-
tiple parallel TCP sessions comes closer to achieving the advertised
throughput. UDP measurements (obtained from ShaperProbe) also
produce consistent measurements of throughput that are closer to
the multi-threaded TCP measurement. A single-threaded TCP ses-
sion may not be able to achieve the same throughput, but account-
ing for cross trafﬁc with passive measurements can provide a better
estimate of the actual achieved throughput.
The behavior of single-threaded TCP measurements varies for
different access links. We compare the passive throughput for
two BISMark users with the same ISP and service plan (AT&T;
3 Mbits/s down, 384 Kbits/s up) who live only a few blocks apart.
Figure 5 shows that User 2 consistently sees nearly 20% more
throughput—much closer to the advertised rate—than User 1. One
possible explanation for this difference is the loss rates experi-
enced by these two users; User 1 suffers more loss than User 2
0.00.20.40.60.81.0Normalized Throughput0.00.20.40.60.81.0CDFSingle-threaded HTTPPassive ThroughputUDP CapacityMulti-threaded HTTP138identiﬁed in the plot by labels. In general, these results agree with
the ﬁndings from both Netalyzr [24] and Dischinger et al. [12], al-
though our dataset also contains Verizon FiOS (FTTP) users that
clearly stand out, as well as other more recent service offerings
(e.g., AT&T U-Verse). Although the statistics do show some no-
ticeable clusters around various service plans, there appears to be
considerable variation even within a single service plan. We seek
to understand and characterize both the performance variations and
their causes. We do not yet have access to the service plan infor-
mation of each user, so we focus on how and why throughput per-
formance varies, rather than whether the measured values actually
match the rate corresponding to the service plan.
Do users achieve consistent performance? We analyze how con-
sistently users in the SamKnows achieve their peak performance
deployment using the Avg/P 95 metric, which we deﬁne as the
ratio of the average upload or download throughput obtained by a
user to the 95th percentile of the upload or download throughput
value obtained by the same user. Higher values for these ratios
reﬂect that users’ upload and download rates that are more con-
sistently close to the highest rates that they achieve; lower values
indicate that user performance ﬂuctuates.
Figure 7a shows the CDF of the Avg/P 95 metric for each user;
Figure 7b shows the same metric for uploads. Most users ob-
tain throughput that is close to their 95th percentile value. Users
of certain ISPs (e.g., Cox, Cablevision) experience average down-
load throughput that is signiﬁcantly less than their 95th percentile.
(Both ISPs have more than 50 active users in our data set; see Ta-
ble 2). Upload throughput performance is more consistent across
ISPs. The big difference between download rates and upload rates
for popular service plans could account for the fact that upstream
rates are more consistent than downstream rates. We also stud-
ied the Median/P 95 performance; which is similar to Avg/P 95,
and so we do not show them. Our results suggest that upload
and download throughput are more consistent than they were when
Dischinger et al. performed a similar study few years ago [12],
especially for some cable providers.
Why is performance sometimes inconsistent? One possible ex-
planation for inconsistent download performance is that the access
link may exhibit different performance characteristics depending
on time of day. Figure 8a shows the Avg/P 95 metric across the
time of day. We obtain the average measurement reported by each
user at that particular time of day and normalize it with the 95th
percentile value of that user over all reports. Cablevision users see,
on average, a 40% drop in performance from early morning and
evening time (when users are likely to be home). For Cox, this
number is about 20%. As the ﬁgure shows, this effect exists for
other ISPs to a lesser extent, conﬁrming prior ﬁndings [12]. Be-
cause we do not know the service plan for each user, we cannot
say whether the decrease in performance for Cox and Cablevision
represents a drop below the service plans for those users (e.g., these
users might see rates higher than their plan during off-peak hours).
Figure 8b shows how the standard deviation of normalized through-
put varies depending on the time of day. Performance variability
increases for all ISPs during peak hours. Figure 8c shows the loss
behavior for different times of day; although most ISPs do not see
an increase in loss rates during peak hours, Cox does. This be-
havior suggests that some access ISPs may be under-provisioned;
those ISPs for which users experience poor performance during
peak hours may be experiencing congestion, or they may be ex-
plicitly throttling user trafﬁc during peak hours.
Takeaway: Although there is no signiﬁcant decrease in perfor-
mance during peak hours, there is signiﬁcant variation. A one-time
Figure 5: Users with the same service planbut different loss proﬁles
see different performance. User 1 has higher loss and sees lower
performance. (BISMark)
Figure 6: Average download rate versus the average upload rate
obtained by individual users in the dataset. (SamKnows)
(0.78% vs. 0.20% on the downlink and 0.24% vs. 0.06% on the
uplink). Their baseline latencies differ by about 16 milliseconds
(8 ms vs. 24 ms). We conﬁrmed from the respective modem portals
that User 1 has interleaving disabled and that User 2 has interleav-
ing enabled. Therefore, User 2 is able to recover from noisy access
links that cause packet corruption or losses. Single-threaded down-
loads are more adversely affected by the loss rate on the access link
than multi-threaded downloads (even when accounting for cross
trafﬁc); reducing the loss rate (e.g., by interleaving) can improve
the performance of a single-threaded download. For the rest of the
paper, we consider only multi-threaded TCP throughput.
Takeaway: Different throughput measurement techniques cap-
ture different aspects of throughput. A single-threaded TCP ses-
sion is sensitive to packet loss. Augmenting this measurement with
passive usage measurements improves its accuracy. Multi-threaded
TCP and the UDP capacity measurements measure the access link
capacity more accurately and are more robust to loss.
5.2 Throughput Performance
We investigate the throughput obtained by users in the Sam-
Knows deployment. We then study the consistency of their per-
formance.
What performance do users achieve? Figure 6 shows the aver-
age download and upload speeds obtained by each user in the Sam-
Knows dataset. Each point in the scatter plot shows the average
performance obtained by a single user in the deployment. Clusters
of points in the plot reveal common service plans of different ISPs,
0100020003000Throughput (Kbits/s)00.250.500.751.0CDFUser 1User 2100K1M10M100MAverage Download speeds (bits/s)10K100K1M10M100MAverage Upload speeds (bits/s)ComcastComcastAT&T(DSL)AT&T(DSL)AT&T(DSL)AT&T U-Verse (FTTx/DSL)TimeWarnerVerizon FiOS(FTTP)Verizon(DSL)CoxQwestQwestCharterCharterCablevision139(a) Download throughput is mostly consistent, with some excep-
tions.
(a) The biggest difference between peak and worst performance is
about 40%.
(b) Upload throughput is consistent across ISPs.
Figure 7: Consistency of throughput performance: The average
throughput of each user is normalized by the 95th percentile value
obtained by that user. (SamKnows)
“speed test” measurement taken at the wrong time could likely
report misleading numbers that do not have much bearing on the
long-term performance.
5.3 Effect of Trafﬁc Shaping on Throughput
ISPs shape trafﬁc in different ways, which makes it difﬁcult to
compare measurements across ISPs, and sometimes even across
users within the same ISP. We study the effect of PowerBoost 3
across different ISPs, time, and users. We also explore how Com-
cast implements PowerBoost.
Which ISPs use PowerBoost, and how does it vary across ISPs?
The SamKnows deployment performs throughput measurements
once every two hours; each measurement lasts 30 seconds, and each
report is divided into six snapshots at roughly 5-second intervals for
the duration of the 30-second test (Section 4). This measurement
approach allows us to see the progress of each throughput mea-
surement over time; if PowerBoost is applied, then the throughput
during the last snapshot will be less than the throughput during the
ﬁrst. For each report, we normalize the throughput in each period
by the throughput reported for the ﬁrst period. Without Power-
Boost, we would expect that the normalized ratio would be close
to one for all intervals. On the other hand, with PowerBoost, we
expect the throughput in the last ﬁve seconds to be less than the
throughput in the ﬁrst ﬁve seconds (assuming that PowerBoost lasts
less than 30 seconds, the duration of the test). Figure 9 shows the
average progression of throughput over all users in an ISP: the av-
erage normalized throughput decreases steadily. We conclude that
most cable ISPs provide some level of PowerBoost for less than 30
seconds, at a rate of about 50% more than the normal rate. Ca-
blevision’s line is ﬂat; this suggests that either it does not provide
PowerBoost, or it lasts well over 30 seconds consistently, in which
case the throughput test would see only the PowerBoost effect. The
(b) The standard deviation of throughput measurements increases
during peak hours, most signiﬁcantly for ISPs that see lower
throughputs at peak hours.
(c) Loss increases during peak hours for Cox. Other ISPs do not see
this effect as much.
Figure 8: Time of day is signiﬁcant: The average download
throughput for Cablevision and Cox users drops signiﬁcantly dur-
ing the evening peak time. Throughput is also signiﬁcantly more
variable during peak time. (SamKnows)
gradual decrease, rather than an abrupt decrease, could be because
PowerBoost durations vary across users or that the ISP changes
PowerBoost parameters based on network state. From a similar
analysis for uploads (not shown), we saw that only Comcast and
Cox seem to provide PowerBoost for uploads; we observed a de-
crease in throughput of about 20%. Dischinger et al. [12] also re-
ported PowerBoost effects, and we also see that it is widespread
among cable ISPs. For the DSL ISPs (not shown), the lines are ﬂat.
Takeaway: Many cable ISPs implement PowerBoost, which
could distort speedtest-like measurements. While some people may
be only interested in short-term burst rates, others may be more in-
terested in long-term rates. Any throughput benchmark should aim