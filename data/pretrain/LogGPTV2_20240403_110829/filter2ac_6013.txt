### Title: BlackHat US-22: All Your GNN Models and Data Belong to Me

Graph-structured data is ubiquitous in many real-world applications. Graph Neural Networks (GNNs), a class of machine learning models, have been developed to effectively utilize this type of data for building powerful applications. Inductive GNNs, which can generalize to unseen data, have become particularly prominent. These models have enabled numerous practical solutions to real-world problems, including node classification, community detection, link prediction, recommendation systems, binary similarity detection, malware detection, fraud detection, and bot detection.

Training high-performing GNNs requires substantial proprietary data and computational resources, making them valuable intellectual property. Previous research has shown that machine learning models are vulnerable to adversarial attacks aimed at stealing the functionality of these models. However, most of this research has focused on models trained with non-structured data, such as images and text. In contrast, the security of GNNs and the privacy of the raw graph data used to train them have received relatively little attention.

In this talk, we present three novel attacks targeting GNNs: model stealing attacks, link re-identification attacks, and property inference attacks. 

1. **Model Stealing Attack**: We demonstrate that attackers, posing as benign customers of commercially deployed GNN models, can use our model stealing attack to replicate GNNs with high accuracy and fidelity.

2. **Link Re-Identification Attack**: We show that attackers can infer private and sensitive relationships within the raw data used to train the GNNs, compromising the confidentiality of the data.

3. **Property Inference Attack**: We reveal a novel graph reconstruction attack that can generate a graph with similar structural statistics to the target graph. This is particularly concerning because certain types of graph data, such as biomedical or molecular graphs collected from laboratory studies, are often expensive and proprietary. Such graph reconstruction attacks pose a direct threat to pharmaceutical companies using GNNs to accelerate drug discovery.

These findings highlight the need for enhanced security measures to protect GNN models and the sensitive data they are trained on.