[168,32M]
10 1
[32N,64M)
12 1
1 1
[64M, 128M)
[1288, 2568)
1 |
This shows that the lifespan of sk_buffs was often between 16 and 64 microseconds, however,
there are outliers reaching as high as the 128 to 256 millisecond bucket. These can be further
investigated with other tools, including the previously queue latency tools, to see if the latency is
coming from those locations.
This works by tracing kernel slab cache allocations to find when sk_buffs are allocated and freed.
Such allocations can be very frequent, and this tool may cause noticeable or significant overhead
on very busy systems. It can be used for short-term analysis rather than long-term monitoring.
The source to skblife(8) is:
+1/usr/local/bin/bpftrace
kprobe:knem_cache_a11oc,
kprobe:knea_cache_a11oc_node
$cache = arg:
if ($cache == *kaddr (*skbuff_fclone_cache*)11
$cache == *kaddr ("s kbuff_head_cache*) 1 
eis_skb_a1loc[tid] = 1
kretprobe:knem_cache_a1loc,
kzetprobe: knea_cache_a11oc_node
/Bis_skb_a1loc[tid] /
1
delete (eis_skb_alloc[tid]) 
Bskb_blrth[retval] = nsecs
---
## Page 516
10.3 BPF Tools
479
kprobe:knem_cache_free
/Bsk_bixth[axg1]/
Bskb_zesidency_nsecs = hist (nsecs - @sab_bieth [argl]):
delete (eskb_birth [argl] 
END
clear (81s_skb_a11oc1 
clear (8skb_birth) ≠
The kmem_cache_alloc( functions are instrumented, and the cache argument is matched to see
if it is an sk_buff cache. If so, on the kretprobe a timestamp is associated with the sk_buff address,
which is then retrieved on kmem_cache_free().
There are some caveats with this approach: sk_buffs can be segmented into other sk_buffs on
GSO, or attached to others on GRO. TCP can also coalesce sk_buffs (tcp_try_coalesce(). This
means that, while the lifespan of the sk_buffs can be measured, the lifespan of the full packet may
be undercounted. This tool could be enhanced to take these code paths into account: copying an
original birth timestamp to new sk_buffs as they are created.
Since this adds kprobe overhead to all kmem cache alloc and free calls (not just for sk_buffs), the
overhead may become significant. In the future there may be a way to reduce this. The kernel
already has skb:consume_skb and skb:free_skb tracepoints. If an alloc skb tracepoint was added, 
that could be used instead, and reduce this overhead to just the sk_buff allocations.
10.3.30 ieee80211scan
adexa 3og Suus  1I'os 13I saoen ,(s)ueszosaa
:1eee80211scan.bt
Attach.ing 5 probes...
Tracing leee8021l ssID scans. Hit Ctx1-C to end.
13:55:07 scan started (on-CF PID 1146, vpa_supplicant)
13:42:11 scanning channe1 2GHz freq 2412: beacon_found 0
13:42:11 scanning channel 2GHz freq 2412: beacon_found 0
13:42:11 scanning channe1 2GHz fzeq 2412: beacon_found 0
[...]
13:42:13 scanning channe1 5GHz freq 5660: beacon_found 0
13:42:14 scanning channel 5GHz freq 5785: beacon_found 1
46 0rigin: I crested this for this book on 23-Apr-2019. The first time I wrote a WiFi scanning tracer wsas out of necessity
I came up with 8 similar scanner tool using DTrace, athough I dont think I published it
Ahen I was in a hotel room in 2004 with a laptop that wouldn’t connect to the WifFi, and no error messages to say why
---
## Page 517
480
Chapter 10 Networking
13:42:14 scanning channel 5GHz freq 5785: beacon_found 1
13:42:14 scanning channel 5GHz freq 5T85: beacon_found 1
13:42:14 scanning channel 5GHz freq 5785: beacon_found 1
13:42:14 scanning channe1 5GHz freq 5T85: beacon_found 1
13:42:14 scan completed: 3205 ns
This shows a scan likely initiated by a wpa_supplicant process, which steps through various chan-
nels and frequencies. The scan took 3205 ms. This provides insight that can be useful for debug-
ging WiFi problems.
This works by instrumenting the ieee80211 scan routines. The overhead should be negligible as
these routines should be infrequent.
The source to ieee80211scan(8) is:
#1/usx/local/bin/bpEtrace
#1nclude 
BEGIY
printf(*7xacing Leee8021l ssIo scana. Bit Ctrl-C to end.\n*);
// fron include/uap1/linux/nl80211,ht
zHOZ。 = [o]puea8
2HOS。 - [T]pueqg
Bband[2] = *60GHz*;
kprobe:ieee80211_request_scan
time(*§B:M:s ")
printf (*scan started (on=CPU PID 5d, s)\n*, pid, comn) ;
t222 - 414498
kzetprobe:leee80211_get_channe1
/cetva1/
$ch =(struct ieee80211_channel *) retval
Sband = 0xff & *retval; // $ch->band; vorkaround for T76
time (*§B:H:s *)
printf (*scanning channel %s freq 与d: beacon_found Id’,n*,
band[$band] , $ch=>center_freq, $ch->bescon_found) 
---
## Page 518
10.3 BPF Tools
481
/4.14428/
time(*:M:S *)
printf(*scan compeleted: Id ns\n*, (nseca - @start) / 1ooooo0) ;
delete (fstart) 
END
clear (8start) : cIear (Bband) ;
More information can be added to show the different flags and settings used while scanning. Note that
this tool currently assumes that only one scan will be active at a time, and has a global @start time-
ueos upe um dures e aeposse o ax e paau m su 'pneed u anoe aq e sues  duess
10.3.31 0ther Tools
Other BPF tools worth mentioning:
 solisten(8): A BCC tool to print socket listen calls with details7
• tcpstates(8): A BCC tool that prints a line of output for each TCP session state change, with
IP address and port details, and duration in each state
• tcpdrop(8): A BCC and bpftrace tool that prints IP address and TCP state details, and kernel
stack traces, for packets dlropped by the kernel tcp_drop() function
■ sofdsnoop(8): A BCC tool to trace file descriptors passed through Unix sockets
 profile(8): Covered in Chapter 6, sampling of kernel stack traces can quantify time spent in
network code paths
• hardirqs(8) and softirqs(8): Covered in Chapter 6, can be used to measure the time spent
in networking hard and soft interrupts
filetype(8): From Chapter 8, traces vfs_read() and vfs_write(), identifying which are socket
reads and writes via the inode
Example output from tcpstates(8):
SA0DCR
CPID C-CO9 LADDR
LPORTBUCOR
RIORT 0LDSTATE > NEWSTATE
trfr88864rd55a00 3294
record 127.0.0.1 0
127,0.0.1
28527 CL0SE
-> SYI_SENT
0.00
fr88864fd55a00 3294
record 127.0.0.1 0
127,0.0.1
28527 SYN_SENT -> ESTABLISHED
0.DB
rrfr88864fd56300 3294
record 127,0.0.1 0
0.0.0.0
LISTEN
-> SYI_RECY
0.00
[-- -]
This uses the sockcinet_sock_set_state tracepoint.
47 solisten(8) wss sdded by Jean-Tiare Le Bigot on 4-Mar-2016.
---
## Page 519
482
2Chapter 10 Networking
10.4BPF One-LinerS
These sections show BCC and bpftrace one-liners. Where possible, the same one-liner is imple
mented using both BCC and bpftrace.
10.4.1BCC
Count failed socket connect(2)s by error code:
Count socket connect(2)s by user stack trace:
stackcount -U t:syscalls:sys_entex_connect
TCP send bytes as a histogram:
TCP receive bytes as a histogram:
azgdiat -H *x::tcp_recwsg(l :int:Sretval:$xetval>0*
Count all TCP functions (adds high overhead to TCP):
Eunccount 'tcp_**
UDP send bytes as a histogram:
azgdiat -H *p::udp_sendmsg(void *ak, void *nsg, Int size):int:size*
UDP receive bytes as a histogram:
azgdiat -H *r::udp_recwmsg(1 :int:$retval:$xetval>0*
Count all UDP functions (adds high overhead to UDP):
Eunccount 'udp_*
Count transmit stack traces:
stackcount t:net:net_dev_xsLt
Count ieee80211 layer functions (adds high overhead to packets):
funccount 'leee80211_**
Count all ixgbevf device driver functions (adds high overhead to ixgbevf):
Eunccount 'ixgbevf_**
10.4.2 bpftrace
Count socket accept(2)s by PID and process name:
---
## Page 520
10.4 BPF One-Liners
483
Count socket conneet(2)s by PID and process name:
Count failed socket connect(2)s by process name and error code:
=[1e3ox 0/ ( 8[pid, comn]
sun((int32) retva1) ;}*
Count TCP connects by on-CPU PID and process name:
aueu saoosd pue CId nd-uo Aq sdaoor dI tumo
Count TCP send/receives:
Count TCP send/receives by on-CPU PID and process name
TCP send bytes as a histogram:
1[zbxe]isty =sepAgpuesg ] bsupuesdoax.8-eoexagdg
TCP receive bytes as a histogram:
Count TCP retransmits by type and remote host (assumes IPv4):
11)aunoo=[(xppes= 0/ [ @xecv_bytea = hiat.(zetval) ; 1
Count all UDP functions (adds high overhead to UDP):
()uno=[oung]8]dpny.8-eoexdo
Count transmit kernel stack traces:
Show receive CPU histogram for each devices
Bpftzace -e *t:net:netif_receive_skb ( @[atr [azgs=>nane) ] = lhlst|cpu, 0, 128, 1) ; 1*
Count ieee80211 layer functions (adds high overhead to packets):
Count all ixgbevf device driver functions (adds high overhead to ixgbevf):
Count all iwl device driver tracepoints (adds high overhead to iwl):
10.4.3 BPF One-Liners Examples
Includling some sample output, as was done for each tool, is also useful for illustrating one-liners
Counting Transmit Kernel Stack Traces
Attach.ing 1 probe...
[..]
9 [
dev_hazd_start_xnit+945
sch_direct_xmit+882
t.2t+m."ospb-—
__dev_queue_xnit+3351
dev_queue_xnit+16
ip_finish_output2+3035
ip_finish_output+1724
---
## Page 522
10.4 BPF One-Liners
485
ip_output+444
ip_local_out+117
_ip_queve_xmit+2004
p_qvese_xnt+69
_tcp_transni t_skb+6570
tcp_write_xnit+2123
tcp_push_pending_frames+145
tcp_xcv_eatab1lshed+2573
T.+x“
290+x"d
ip_protocol_deliver_reu+185
p_local_deliver_finlsh+386
ip_local_deliver+435
ip_rcv_f1n1sh+342
ip_rev+212
_netif_receive_skb_one_coxe+308
_net.if_receive_skb+36
netif_recelve_skb_internal+168
napi_gro_reoeive+953
ena_10_po11+8375
ret_rx_action+1750
_do_softixq+558
irq_exit+348
do_IRQ+232
ret_fron_inte+0
na tive_safe_halt+6
default_idle+146
axch_cpu_1dle+21
default_idle_ca11+59
do_1d1e+809
67+/.3ua“dn,xes"n.d
staxt_secondaxy+1228
secondary_startup_64+164
] :902
dev_hard_start_xnlt+945
sch_direct_xmit+882
qdisc_run+1271
TSCC+Tux"ananbAap
dev_queue_xnit+16
ip_finish_output2+3035
p_finish_outpot+1724
ip_output+444
---
## Page 523
486
6 Chapter 10 Networking
ip_local_out+117
p_queve_mit+2004
ip_queve_xnit+69
tcp_cxananit_skb+6570
tcp_vrite_xmit+2123
tcp_push_pending_fraxes+145
tcp_posh+1209
tcp_sendnsag_1ocked+9315
}+supuasdoq
inet_sendmsg+2T8
sock_wz1te_lter+740
_vfs_xrite+1694
vfs_vrite+341
ksys_vrite+247
_x64_ays_write+115
do_sysca11_64+339
entey_SYscALL_64_after_hvframe+68
1: 10933
This one-liner procduced many pages of output; only the last two stack traces have ben included here.
The last shows a write(2) syscall passing through VFS, sockets, TCP; IP, net device, and then beginning
the transmit to the driver. This illustrates the stack from the application to the device drive.
dna ue uaaa pea ap a m suaq 1 usaau aou uaaa st aoen es sg a
running the net_rx_action() softirq, the ena driver ena_io_pollO, the NAPI (new API) network
"Suspuad"usnd da  tcp_push_pending_frames(). However, the middle two functions were tiny and inlined
by the compiler, eliding them from that stack trace. What’s happening is that TCP is checking for
pending transmits during the receive codepath.
Counting All ixgbevf Device Driver Functions (Adding High Overhead to ixgbevf)
.[: (1qunoo =[oung]81 +gaaqfx:x, -soe2agdg 
Attaching 116 probes...
"C
e[1xgbevf_get_11nk_ksettings]: 2
[ixgbevf_get_stats] : 2
e[1xgbevf_obtaln_nbx_lock_vf]: 2
[ixgbevf_read_nbx_vf] : 2
e[1xgbevf_service_event_schedule] : 3
[ixgbevf_service_task] : 3
E : [3eutaeoTnxesneg6x]6
---
## Page 524
10.5 Optional Exercises
487
[ixgbevf_check_for_bit_vf] : 5
[1xgbevf_check_Cor_xst_vf] : 5
[ixgbevf_check_nac_link_vf]: 5
e[1xgbevf_update_stats]: 5
[ixgbevf_read_reg] : 21
e[1xgbevf_alloc_rx_buffers] : 36843
9[ixgbevf_features_check] : 37842
e[1xgbevf_xnlt_frane] : 3T842
e[ixgbevf_msix_clean_rings] : 66417
10L9 =[Trodneq6x]0
[ixgbevf_maybe_stop_tx] : 75684
e[1xgbevf_update_1tr ,1sza .39] : 132834
The internals of how network device drivers operate can be studied in detail using these kprobes.
aduexa pxau at u totgs se am se supodaoen saxddns rasp atg aqaa o o aaog ,uo
Counting All iwl Device Driver Tracepoints (Adding High Overhead to iwl)
(o=[q]]a,-q 
Attaching 15 probes...
e[tracepoint:ivlvifi:iv]vifi_dev_homd] : 39
e[tracepo.int: iv1v1fi_1o:1v1vif1_dev_1rq] : 3474
9[tracepoint:ivlvifi:ivlvifi_dev_tx]: 5125
e[tracepoint:ivlvifs_lo:Ivlvsfl_dev_1oxxlte8]: 6654
e[tracepoint:iv[vifi_io1ivlvifi_dev_ict_read] : 7095
e[tracepoint:iv1vifi:1vlvifl_dev_rx] : 7493
9[tracepoint:ivlvifi_io:ivlvifi_dev_ioxrite32]: 19525
This one-liner is showing only two of several groups of iwl tracepoints.
10.5OptionalExercises
If not specified, these can be completed using either bpftrace or BCC:
1. Write an solife(8) tool to print per-session durations from connect(2) and accept(2) (and
variants) to close(2) for that socket file descriptor. It can be similar to tcplife(8), although it
does not necessarily need all the same fields (some are harder to fetch than others)
2. Write tcpbind(8): a tool for per-event tracing of TCP bind events.
3. Extend tcpwin.bt with a *retrans* event type, with the socket address and time as fields,
4. Extend tcpwin.bt with a “*new* event type, that has socket address, time, IP addresses, and 
TCP ports as fields. This should be printed when the TCP session reaches the established state.
---
## Page 525
488
8Chapter 10 Networking
5. Modify tcplife(8) to emit conneetion details in DOT format, then plot using graphing
software (e.g., GraphViz).
6. Develop udplife(8) to show the lifespan of UDP connections, similar to tcplife(8).
7. Extend ipecn.bt to instrument outbound CE events, as well as IPv6. CE events can be
introduced at the qdisc layer using the netem qdlisc. The following example command
replaces the current qdisc on eth0 with one that causes 1% ECN CE events:
tc qdlsc replace dev etho root netem Loss ls ecn
If you use this qdisc during development, be aware that it inserts CE events at a lower
level than IP: If you traced, say, ip_outputO, you may not see the CE events as they are