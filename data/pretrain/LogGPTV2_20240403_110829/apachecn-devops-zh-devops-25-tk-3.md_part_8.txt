请键入下面的表达式，然后按“执行”按钮。
```
 1  kube_pod_container_resource_requests_memory_bytes
```
我们可以看到，普罗米修斯(服务器)的内存请求最多(500 MB)，其他的都在下面。请记住，我们只看到有预订的豆荚。没有的不会出现在查询结果中。正如您已经知道的，只有在特殊情况下才定义保留和限制是可以的，例如，在 CI/CD 流程中使用的短期 Pods。
![](img/6968bff2-2962-4746-946f-61dbd8f5dd69.png)
Figure 3-30: Prometheus' graph screen with requested memory for each of the Pods
前面的表达式返回了每个 Pod 使用的内存量。然而，我们的任务是发现我们在整个系统中有多少请求的内存。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(
 2    kube_pod_container_resource_requests_memory_bytes
 3  )
```
在我的例子中，请求的内存总量大约是 1.6 GB 内存。
剩下的就是用集群中所有可分配内存的数量除以总请求内存。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(
 2    kube_pod_container_resource_requests_memory_bytes
 3  ) / 
 4  sum(
 5    kube_node_status_allocatable_memory_bytes
 6  )
```
在我的例子中(下面的截图)，请求的内存总量大约是集群可分配内存的百分之二十(T0)。我远离任何类型的危险，也没有必要扩大集群。如果有的话，我有太多未使用的内存，可能想要缩减。然而，我们目前只关心扩大规模。稍后，我们将探讨可能导致缩减的警报。
![](img/f2bbaa51-31e9-4115-80a1-7994bc281168.png)
Figure 3-31: Prometheus' graph screen with the percentage of the requested memory of the total allocatable memory in the cluster
让我们看看旧图表的值和我们将要使用的值之间的差异。
```
 1  diff mon/prom-values-cpu.yml \
 2      mon/prom-values-memory.yml
```
输出如下。
```
103a104,119
> - alert: NotEnoughMemory
>   expr: 1 - sum(node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes) > 0.9
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough memory
>     description: Memory usage of the cluster is above 90%
> - alert: TooMuchMemoryRequested
>   expr: sum(kube_pod_container_resource_requests_memory_bytes) / sum(kube_node_status_allocatable_memory_bytes) > 0.9
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough allocatable memory
>     description: More than 90% of allocatable memory is requested
```
我们添加了两个新的警报(`NotEnoughMemory`和`TooMuchMemoryRequested`)。定义本身应该很简单，因为我们已经创建了相当多的警报。这些表达式与我们在普罗米修斯图形屏幕中使用的表达式相同，只是增加了大于百分之九十(`> 0.9`)的阈值。所以，我们将跳过进一步的解释。
我们将用新的值升级我们的普罗米修斯图表，并打开警报屏幕以确认它们
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-memory.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```
如果提醒`NotEnoughMemory`和`TooMuchMemoryRequested`还不可用，请稍等片刻，并刷新屏幕。
![](img/c87a9f02-ac71-489f-907e-bd38b2233064.png)
Figure 3-32: Prometheus' alerts screen
到目前为止，基于我们创建的基于内存的警报的操作应该类似于我们与 CPU 讨论的操作。我们可以通过手动操作或自动脚本，使用它们来决定是否以及何时扩展我们的集群。就像以前一样，如果我们的群集由群集自动缩放器(CA)支持的供应商之一托管，这些警报应该纯粹是信息性的，而在本地或不受支持的云提供商那里，它们不仅仅是简单的通知。它们表明我们即将耗尽容量，至少就内存而言。
CPU 和内存示例都集中在需要知道何时是扩展集群的合适时间。我们可能会创建类似的警报，当 CPU 或内存使用率过低时会通知我们。这将清楚地表明，集群中的节点太多，我们可能需要删除一些节点。这再次假设我们没有启动和运行集群自动缩放器。尽管如此，只考虑 CPU 或内存进行缩减还是太冒险，可能会导致意想不到的结果。
让我们假设只保留了 12%的可分配 CPU，并且集群中有三个工作节点。如此低的 CPU 使用率肯定不能保证许多节点，因为平均来说，每个节点都有相对少量的保留 CPU。因此，我们可以选择缩小规模，并删除其中一个节点，从而允许其他集群重用它。这样做好吗？嗯，这取决于其他资源。如果内存预留的百分比也很低，删除一个节点是个好主意。另一方面，如果保留的内存超过 66%，删除节点将导致资源不足。当我们删除三个节点中的一个时，三个节点上超过 66%的保留内存在两个节点上变成超过 100%。
总而言之，如果我们要接收到我们的集群需要缩减的通知(并且我们没有集群自动缩放器)，我们需要将内存和 CPU 以及一些其他指标结合起来作为警报阈值。幸运的是，这些表达与我们之前使用的非常相似。我们只需要将它们组合成一个单一的警报并更改阈值。
提醒一下，我们之前使用的表达式如下(无需重新运行)。
```
 1  sum(rate(
 2    node_cpu_seconds_total{
 3      mode!="idle",
 4      mode!="iowait",
 5      mode!~"^(?:guest.*)$"
 6    }[5m]
 7  ))
 8  by (instance) /
 9  count(
10    node_cpu_seconds_total{
11      mode="system"
12    }
13  )
14  by (instance)
15
16  1 -
17  sum(
18    node_memory_MemAvailable_bytes
19  ) 
20  by (instance) /
21  sum(
22    node_memory_MemTotal_bytes
23  )
24  by (instance)
```
现在，让我们将图表的另一个更新值与我们现在正在使用的值进行比较。
```
 1  diff mon/prom-values-memory.yml \
 2      mon/prom-values-cpu-memory.yml
```
输出如下。
```
119a120,127
> - alert: TooMuchCPUAndMemory
>   expr: (sum(rate(node_cpu_seconds_total{mode!="idle", mode!="iowait", mode!~"^(?:guest.*)$"}[5m])) by (instance) / count(node_cpu_seconds_total{mode="system"}) by (instance))    for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: Too much unused CPU and memory
>     description: Less than 50% of CPU and 50% of memory is used on at least one node
```
我们正在添加一个名为`TooMuchCPUAndMemory`的新警报。它是前两个警报的组合。只有当 CPU 和内存使用率都低于 50%时，它才会触发。这样，我们将避免发送误报，并且不会因为一个资源预留(CPU 或内存)太低，而另一个资源预留可能很高而试图缩减集群。
在我们进入下一个主题(或度量类型)之前，剩下的就是升级普罗米修斯的图表，并确认新的警报确实可以运行。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-cpu-memory.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```
如果警报仍然不存在，请刷新警报屏幕。在我的情况下(截图如下)，保留的内存和 CPU 总数低于百分之五十，警报处于挂起状态。在您的情况下，这可能不是真的，并且警报可能没有达到其阈值。然而，我将继续解释我的情况，其中 CPU 和内存的使用都不到总可用空间的 50%。
三十分钟后(`for: 30m`)，警报响起。它等待了一段时间(`30m`)来确认内存和 CPU 使用率的下降不是暂时的。假设我正在 AKS 中运行我的集群，集群自动缩放器将在三十分钟之前移除其中一个节点。但是，由于它被配置为至少在三个节点上运行，CA 将不会执行该操作。因此，我可能需要重新考虑为三个节点付费是否是一项值得的投资。另一方面，如果我的群集没有群集自动缩放器，并且假设我不想浪费资源，而其他群集可能需要更多资源，我将需要删除其中一个节点(手动或自动)。如果删除是自动的，那么目标不是 Slack，而是负责删除节点的工具的 API。
![](img/88738e53-69bf-4683-9ca4-c8fdf35a59b8.png)
Figure 3-33: Prometheus' alerts screen with one alert in the pending state
现在我们有了几个饱和的例子，我们涵盖了谷歌网站可靠性工程师支持的每一个指标和几乎任何其他监控方法。不过，我们还没完。我还想探索一些其他指标和警报。它们可能不属于任何讨论的类别，但它们可能被证明非常有用。
# 对不可切割或故障的吊舱发出警报
了解我们的应用是否难以快速响应请求，它们是否被超出其处理能力的请求轰炸，它们是否产生太多错误，以及它们是否饱和，如果它们甚至没有运行，都是没有用的。即使我们的警报通过通知我们有太多错误或由于副本数量不足而导致响应时间变慢来检测出问题，例如，如果一个甚至所有副本无法运行，我们仍然应该得到通知。在最佳情况下，此类通知将提供有关问题原因的附加信息。在更糟糕的情况下，我们可能会发现数据库的一个副本没有运行。这不一定会减慢它的速度，也不会产生任何错误，但会使我们处于无法复制数据的情况(额外的副本没有运行)，如果最后一个持续的副本也失败了，我们可能会面临状态的完全丢失。
应用无法运行的原因有很多。群集中可能没有足够的未预留资源。如果我们有集群自动缩放器，它会处理这个问题。但是，还有许多其他潜在的问题。也许新版本的映像在注册表中不可用。或者，可能是 Pods 正在请求无法声明的持久卷。正如你可能已经猜到的，可能导致我们的 Pods 失败、不可分解或处于未知状态的事情的列表几乎是无限的。
我们无法单独解决 Pods 问题的所有原因。但是，如果一个或多个 Pods 的相位是`Failed`、`Unknown`或`Pending`，我们会得到通知。随着时间的推移，我们可能会扩展我们的自我修复脚本，以解决这些状态的一些特定原因。目前，我们最好的第一步是通知 Pod 是否长时间处于其中一个阶段(例如，15 分钟)。一旦 Pod 的状态表明有问题，就发出警报是愚蠢的，因为这会产生太多的误报。我们应该得到一个警报，并在等待一段时间后选择如何行动，从而给 Kubernetes 时间来解决问题。只有当 Kubernetes 无法补救这种情况时，我们才应该采取一些被动的行动。
随着时间的推移，我们会注意到我们收到的警报中的一些模式。当我们这样做时，警报应该转换为自动响应，在没有我们参与的情况下纠正选定的问题。我们已经通过水平自动缩放器和集群自动缩放器探索了一些低挂水果。目前，我们将专注于接收所有其他情况的警报，失败和不可修复的 Pods 就是其中的几个。稍后，我们可能会探索如何自动响应。但是，那个时刻不是现在，所以我们将继续进行另一个警报，它将导致通知 Slack。
让我们打开普罗米修斯的图形屏幕。
```
 1  open "http://$PROM_ADDR/graph"
```
请键入以下表达式，然后单击“执行”按钮。
```
 1  kube_pod_status_phase
```
输出向我们显示了集群中的每个 Pods。如果你仔细看一下，你会注意到每个 Pod 有五个结果，五个可能的阶段各有一个。如果你关注`phase`字段，你会看到`Failed`、`Pending`、`Running`、`Succeeded`和`Unknown`都有一个条目。所以，每个 Pod 有五个结果，但只有一个有`1`值，而其他四个的值都设置为`0`。
![](img/5f93fbd0-1d80-4b28-9ee1-fd26930ac478.png)
Figure 3-34: Prometheus' console view with the phases of the Pods
目前，我们的兴趣主要在于警报，在大多数情况下，警报应该是通用的，与特定节点、应用、副本或其他类型的资源无关。只有当我们被提醒有问题时，我们才应该开始更深入地挖掘并寻找更细粒度的数据。考虑到这一点，我们将重写我们的表达式来检索每个阶段中 Pods 的数量。
请键入以下表达式，然后单击“执行”按钮。
```
 1  sum(
 2    kube_pod_status_phase
 3  ) 
 4  by (phase)
```
输出应显示所有吊舱都处于`Running`阶段。在我的例子中，有 27 个正在运行的 Pods，在任何其他阶段都没有。
现在，我们不应该真正关心健康的豆荚。他们正在逃跑，对此我们无能为力。相反，我们应该关注那些有问题的。因此，我们不妨重写前面的表达式，只检索那些处于`Failed`、`Unknown`或`Pending`阶段的总和。
请键入以下表达式，然后单击“执行”按钮。
```
 1  sum(
 2    kube_pod_status_phase{
 3      phase=~"Failed|Unknown|Pending"
 4    }