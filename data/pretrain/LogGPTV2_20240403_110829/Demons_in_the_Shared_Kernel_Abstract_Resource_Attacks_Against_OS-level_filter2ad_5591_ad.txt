is standard, in the following, we focus on the restriction analysis.
Seccomp Restriction Analysis. Seccomp is a mechanism used
for system-call filtering. Our restriction analysis against seccomp is
as follows. In our implementation, we use Docker default seccomp
profile [15], which blocks more than 50 system calls. Among all the
paths from system call entries to the resource consumption sites,
we filter out paths that originate from any blocked system calls.
Per-User Restriction Analysis. In a real deployment, the con-
tainers are usually running as different users. Thus, the resource
consumption from each container is also restricted by the per-
user resource quotas. For example, Linux provides the user-limits
command ulimit for limiting resource consumption of a specific
user [50]. While the underlying implementation of ulimit is using
rlimit [39, 45] to set multiple per-user resource quotas.
Besides ulimit, Linux also provides interfaces that allow users to
leverage PAM (Pluggable Authentication Module) [63] to deploy
per-user quotas. The PAM uses the setup_limits function [64] to
set per-user resource quotas, which calls setrlimit to configure
multiple rlimit constraints. For the resources limited by ulimit, rlimit
and the PAM, the attacker-container cannot consume beyond the
per-user quotas. As a result, it cannot fully control those abstract
resources to launch DoS attacks. As both the ulimit and the PAM
use rlimit to set per-user resource quotas, we need to analyze rlimit
and filter out the abstract resources restricted by it.
For rlimit analysis, our key observation is that a rlimit value
is usually specified in struct rlimit or struct rlimit64. There-
fore, we first traverse the kernel IR to identify all variables that
are loaded from struct rlimit or struct rlimit64. And then, we
perform data-flow analysis to follow all the propagation and usages
of these variables and mark those functions if they are used in any
comparison instructions. In these functions, rlimit is checked to
limit certain resources. We consider those resources not exhaustible
by the attacker-container, therefore we filter out the paths based on
these functions. Our tool identifies 40 functions that check rlimit.
Namespace Isolation Analysis. As mentioned before, the Linux
kernel introduces namespaces for resource isolation. For a names-
pace isolated resource, the Linux kernel creates a ‚Äúcopy‚Äù for it under
each namespace so that the modification in one namespace does not
affect other namespaces. Therefore, to confirm container controlla-
bility, we need to make sure that those abstract resources are not
protected by namespaces. Here, the challenge is that even though
Linux has documentation about namespaces, there are no specifica-
tions about which abstract resources are isolated by namespaces.
Session 3B: Operating Systems CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea771Table 1: Summary of static analysis results. Res. is short
for resources; Reachable is container-reachable abstract
resources. Limited is per-user and namespace limited re-
sources. Manual is manually filtered out resources. CC Res.
means container controllable resources.
Table 2: Summary of validation results. Res. Dir. is the direc-
tory of resources. The drivers either have hardware support,
or have no hardware support. No. is resources number, Re-
peatedly means the resource consumption can be repeatedly
triggered.
Res. Type Reachable
Limited Manual CC Res.
Proc.
Mem.
Storage
IO
Total
526
110
256
952
1,844
72
10
57
203
342
136
26
66
264
492
318
74
133
485
1,010
Therefore, we propose namespace isolation analysis to identify the
abstract resources protected by namespaces systematically.
Our key observation is that for a namespace-isolated resource,
the corresponding data structure has a pointer field that points to
the namespace it belongs to. Therefore, our tool first traverses all
fields of each data structure type in the kernel. If the type has a
namespace pointer, we mark it as an isolated resource. Second, for
the identified isolated resources, our tool uses it to filter the shared
abstract resources identified in ¬ß4.1.
Note that some namespace-isolated resources may still be vul-
nerable to abstract resource attacks due to the mapping between
different namespaces. As mentioned in ¬ß3.2.2, idr is isolated by
pid_namespace->idr. However, each idr allocated in a non-root PID
namespace is mapped to a new idr in the root PID namespace, so
that the root namespace can manage it. As a result, the root PID
namespace is globally shared by all containers in all PID names-
paces. Therefore, it is still vulnerable to the idr exhaustion attacks.
In our analysis, we manually filter out these resources.
4.3 Analysis Results
We implement our analysis tool with about 2,500 lines of C++ code
in LLVM 12.0. The Linux kernel IR is generated based on the latest
Linux stable version v5.10 with defconfig. The results are shown
in Table 1. In particular, by applying the configuration-based anal-
ysis and the access-based analysis, together with the reachability
analysis from system calls and the seccomp restriction analysis, our
tool identifies 1,844 shared abstract resources that are reachable by
containers.
Resource Filtering. With the per-user quota restriction and the
namespace isolation analysis, our tool finds 342 resources that
are limited by the rlimit or have pointers pointing to namespace
structures. Those resources either have a limit check on the path
or get namespaced.
We further conduct a manual analysis. Specifically, for every re-
source ùëÖ in the identified abstract resources, we walk through all the
detected modifications of ùëÖ or the fields of ùëÖ. If the modification is
not quantitative, such as being assigned with boolean, enumeration,
or string types, we mark this modification as non-quantitative. If all
the modifications to ùëÖ and the fields of ùëÖ are non-quantitative, we
mark ùëÖ as non-exhaustible. Our manual analysis identifies 492 ab-
stract resources that are non-exhaustible, as shown in Table 1. After
manual analysis, there are still 1,010 abstract resources remaining.
Res. Dir.
Non-Driver
No. Repeatedly True Postive
700
Driver Have HW 218
92
55.6%
51.4%
389
112
-
No HW
-
Dynamic Validation. To further validate the dynamic exhaustion
of these 1,010 resources, we develop a dynamic validation method
for resource consumption. For each resource, we first obtain its
consumption sites and the triggered system calls from the above
controllability analysis. After that, we instrument those consump-
tion sites to monitor the actual resource consumption. Next, we
execute the test cases of the corresponding triggered system calls
to repeatedly trigger the consumption and record the results. We
leverage 1,156 test cases from the Linux Test Project (LTP) [14] and
develop 177 new ones to cover more cases. We also develop scripts
to automate the above steps.
We applied our dynamic validation method to test the consump-
tion of all 1,010 resources. The results are summarized in Table 2.
For the 1,010 detected resources, 700 of them are not in the driver
folder, while the other 310 resources are in the driver folder, as
shown in Table 2. For the 700 non-driver resources, 389 of them
can be repeatedly triggered dynamically, leading to a true posi-
tive rate of 55.6%. The resources in the driver folder need to be
handled specially for two reasons. First, drivers are specific to the
hardware. Without the corresponding hardware, the driver code
cannot be triggered dynamically. Our key observation is that most
hardware-supported drivers expose specific interfaces under /dev
or /sys/class folders. Based on this observation, we remove 92 re-
sources in drivers that are not supported by our hardware. Second,
the test cases provided by LTP might not cover a specific driver.
To resolve this problem, we modify the LTP test cases and develop
new test cases for the drivers. Among the 218 driver resources, 112
of them can be repeatedly triggered, leading to a true positive rate
of 51.4%, as shown in Table 2.
Identifying container-exhaustible abstract resources is a very
challenging task, as it requires the domain knowledge to trigger
the exhaustion of abstract resources and it needs to assess the
impacts when these resources are exhausted. In this paper, we
conduct a preliminary analysis. Note that a thorough analysis and
risk assessment needs help from the Linux kernel and the container
community. Therefore, we plan to open source our tool and the
detected abstract resources. We think it will help the Linux kernel
and the container community to identify the weak spots of resource
isolation and develop robust resource containment schemes.
5 ABSTRACT RESOURCE ATTACKS ON
CLOUD PLATFORMS
In this section, we further evaluate abstract resource attacks on the
container environments of public cloud vendors. We first present
the environment setup and then give out the results.
Session 3B: Operating Systems CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea772Table 3: Summary of the abstract resources chosen from analysis results.
Resource Name
Identification Consumption Function
OS Service
Process
Memory
Storage
IO
PID idr
dirty ratio
inode
nr_files
pty_count
netns_ct->count
random entropy
Access
Access
Access
Access
Access
Configuration
Configuration
alloc_pid()
balance_dirty_pages()
__ext4_new_inode()
alloc_empty_file()
devpts_new_index()
__nf_conntrack_alloc()
extract_entropy()
Syscall
fork()
write()
creat()
open()
open()
connect()
read()
5.1 Environment Setup and Ethical
Considerations
To evaluate the effectiveness of the abstract resource attack, we set
up the container environments on both local and cloud platforms.
The local test environment has been presented in ¬ß3.1.
Ethical Considerations. For the cloud platforms, we intend to
minimize the impact of our attacks on other cloud users as much
as possible. Therefore, we use a dedicated virtual server, e.g., AWS
EC2, Azure VM, Google GCE, and Alibaba ECS, to conduct the
experiments. In addition, we ensure that we are the only user of
that server.
Moreover, most container users leverage the container orches-
tration systems to deploy and manage containers [36]. Therefore,
we choose the most popular one named Kubernetes and leverage
cloud vendors‚Äô Kubernetes services to deploy two docker containers
(i.e., the attacker-container and the victim-container) on the virtual
server. For strong isolation, we apply different Kubernetes names-
paces [37] for the attacker-container and the victim-container. As
mentioned in ¬ß4.2, containers are also subjected to per-user quota
restrictions. To enforce the per-user quotas in our experiments, we
run the attacker-container and the victim-container in separate
users with the per-user quota enforced. We also discuss restrictions
that can be deployed by the PAM in ¬ß6.
Amazon AWS. For the container services, we use Elastic Kuber-
netes Service (EKS) [2] to deploy two container instances on an
EC2 instance. The EC2 instance contains 4 CPUs, 8 GB memory,
and 20 GB SSD disk. During the container deployment, we surpris-
ingly find that the ‚ÄúAmazon EKS default pod security policy‚Äù uses
eks.privileged as the default pod security policy [3]. Note that this
policy allows containers to run as a privileged user and also allows
privilege escalation as well as host network accesses.
To better demonstrate the effectiveness of our proposed attack,
we adopt a stronger security policy from our local test environment
to EKS containers, which runs containers in non-root users, drops
all privileges, enables all namespaces and control groups, and uses
docker seccomp profile [15] to block 50+ sensitive system calls
including ptrace, pivot_root, etc. And we apply the same security
policy for both the attacker-container and the victim-container.
MS Azure. We use Azure Kubernetes Service (AKS) [51] to deploy
two container instances on an Azure virtual machine. The Azure
VM contains 2 CPUs, 8 GB memory, and 120 GB disk. To improve the
security of the deployed containers, Azure provides best practices
for pod security policy in AKS [52], which runs a container in
the non-root user by setting runAsUser:1000 in yaml file, and it
denies privilege escalation by setting allowPrivilegeEscalation:
false. However, it still adds two capabilities, i.e., CAP_NET_ADMIN and
CAP_SYS_TIME, and does not enforce seccomp.
Same as the AWS settings, we adopt a tighter security policy for
containers on AKS. In addition to the best practices suggestions
(i.e., non-root user and disallowing privilege escalation), we run
AKS containers in non-root users, drop all capabilities, enable all
namespaces and control groups, and use docker seccomp profile [15]
to block 50+ sensitive system calls. And we apply the same security
policy for both the attacker-container and the victim-container.
Google Cloud. For the container services, we choose the Kuber-
netes and use Google Kubernetes Engine (GKE) [27] to deploy two
container instances on a Google Compute Engine instance [28]. The
Google Compute Engine (GCE) instance we use contains 4 CPUs, 16
GB memory, and 100 GB SSD. More specifically, we apply one GCE
instance and deploy two containers (i.e., the attacker-container and
the victim-container) based on the regular runtime on that GCE
instance.
For the container deployment, we follow the GKS container setup
wizard. Google Cloud provides best practices for operating contain-
ers [29], which suggests avoiding privileged containers. Therefore,
in securityContext of the yaml configuration file, we disallow the
privileged escalation, run the container as a non-privileged user,
and drop all the capabilities. The GKS setup wizard enables 6 names-
paces and 13 control groups by default. Besides, we apply the docker