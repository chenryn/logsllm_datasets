To configure caching in NGINX, it’s necessary to declare a path and
zone to be used. A cache zone in NGINX is created with the direc‐
tive proxy_cache_path. The proxy_cache_path designates a loca‐
tion to store the cached information and a shared memory space to
store active keys and response metadata. Optional parameters to this
directive provide more control over how the cache is maintained
and accessed. The levels parameter defines how the file structure is
created. The value is a colon-separated value that declares the length
of subdirectory names, with a maximum of three levels. NGINX
caches based on the cache key, which is a hashed value. NGINX then
stores the result in the file structure provided, using the cache key as
a file path and breaking up directories based on the levels value.
The inactive parameter allows for control over the length of time a
cache item will be hosted after its last use. The size of the cache is
also configurable with the use of the max_size parameter. Other
parameters relate to the cache-loading process, which loads the
cache keys into the shared memory zone from the files cached on
disk.
38 | Chapter 4: Massively Scalable Content Caching
4.2 Caching Hash Keys
Problem
You need to control how your content is cached and looked up.
Solution
Use the proxy_cache_key directive along with variables to define
what constitutes a cache hit or miss:
proxy_cache_key "$host$request_uri $cookie_user";
This cache hash key will instruct NGINX to cache pages based on
the host and URI being requested, as well as a cookie that defines
the user. With this you can cache dynamic pages without serving
content that was generated for a different user.
Discussion
The default proxy_cache_key, which will fit most use cases, is
"$scheme$proxy_host$request_uri". The variables used include
the scheme, HTTP or HTTPS, the proxy_host, where the request is
being sent, and the request URI. All together, this reflects the URL
that NGINX is proxying the request to. You may find that there are
many other factors that define a unique request per applica‐
tion, such as request arguments, headers, session identifiers, and so
on, to which you’ll want to create your own hash key.1
Selecting a good hash key is very important and should be thought
through with understanding of the application. Selecting a cache key
for static content is typically pretty straightforward; using the host‐
name and URI will suffice. Selecting a cache key for fairly dynamic
content like pages for a dashboard application requires more knowl‐
edge around how users interact with the application and the degree
of variance between user experiences. Due to security concerns you
may not want to present cached data from one user to another
without fully understanding the context. The proxy_cache_key
directive configures the string to be hashed for the cache key. The
1 Any combination of text or variables exposed to NGINX can be used to form a cache
key. A list of variables is available in NGINX: http://nginx.org/en/docs/varindex.html.
4.2 Caching Hash Keys | 39
proxy_cache_key can be set in the context of HTTP, server, and
location blocks, providing flexible control on how requests are
cached.
4.3 Cache Bypass
Problem
You need the ability to bypass the caching.
Solution
Use the proxy_cache_bypass directive with a nonempty or nonzero
value. One way to do this is by setting a variable within location
blocks that you do not want cached to equal 1:
proxy_cache_bypass $http_cache_bypass;
The configuration tells NGINX to bypass the cache if the HTTP
request header named cache_bypass is set to any value that is not 0.
Discussion
There are a number of scenarios that demand that the request is not
cached. For this, NGINX exposes a proxy_cache_bypass directive
so that when the value is nonempty or nonzero, the request will be
sent to an upstream server rather than be pulled from the cache.
Different needs and scenarios for bypassing cache will be dictated by
your applications use case. Techniques for bypassing cache can be as
simple as a using a request or response header, or as intricate as
multiple map blocks working together.
For many reasons, you may want to bypass the cache. One impor‐
tant reason is troubleshooting and debugging. Reproducing issues
can be hard if you’re consistently pulling cached pages or if your
cache key is specific to a user identifier. Having the ability to bypass
the cache is vital. Options include but are not limited to bypassing
the cache when a particular cookie, header, or request argument is
set. You can also turn off the cache completely for a given context
such as a location block by setting proxy_cache off;.
40 | Chapter 4: Massively Scalable Content Caching
4.4 Cache Performance
Problem
You need to increase performance by caching on the client side.
Solution
Use client-side cache control headers:
location ~* \.(css|js)$ {
expires 1y;
add_header Cache-Control "public";
}
This location block specifies that the client can cache the content of
CSS and JavaScript files. The expires directive instructs the client
that their cached resource will no longer be valid after one year. The
add_header directive adds the HTTP response header Cache-
Control to the response, with a value of public, which allows any
caching server along the way to cache the resource. If we specify pri‐
vate, only the client is allowed to cache the value.
Discussion
Cache performance has many factors, disk speed being high on the
list. There are many things within the NGINX configuration you
can do to assist with cache performance. One option is to set head‐
ers of the response in such a way that the client actually caches the
response and does not make the request to NGINX at all, but simply
serves it from its own cache.
4.5 Purging
Problem
You need to invalidate an object from the cache.
Solution
Use the purge feature of NGINX Plus, the proxy_cache_purge
directive, and a nonempty or zero-value variable:
4.4 Cache Performance | 41
map $request_method $purge_method {
PURGE 1;
default 0;
}
server {
...
location / {
...
proxy_cache_purge $purge_method;
}
}
In this example, the cache for a particular object will be purged if it’s
requested with a method of PURGE. The following is a curl example
of purging the cache of a file named main.js:
$ curl -XPURGE localhost/main.js
Discussion
A common way to handle static files is to put a hash of the file in the
filename. This ensures that as you roll out new code and content,
your CDN recognizes it as a new file because the URI has changed.
However, this does not exactly work for dynamic content to which
you’ve set cache keys that don’t fit this model. In every caching sce‐
nario, you must have a way to purge the cache. NGINX Plus has
provided a simple method of purging cached responses. The
proxy_cache_purge directive, when passed a nonzero or nonempty
value, will purge the cached items matching the request. A simple
way to set up purging is by mapping the request method for PURGE.
However, you may want to use this in conjunction with the geo_ip
module or simple authentication to ensure that not anyone can
purge your precious cache items. NGINX has also allowed for the
use of *, which will purge cache items that match a common URI
prefix. To use wildcards you will need to configure your
proxy_cache_path directive with the purger=on argument.
4.6 Cache Slicing
Problem
You need to increase caching effiency by segmenting the file into
fragments.
42 | Chapter 4: Massively Scalable Content Caching
Solution
Use the NGINX slice directive and its embedded variables to
divide the cache result into fragments:
proxy_cache_path /tmp/mycache keys_zone=mycache:10m;
server {
...
proxy_cache mycache;
slice 1m;
proxy_cache_key $host$uri$is_args$args$slice_range;
proxy_set_header Range $slice_range;
proxy_http_version 1.1;
proxy_cache_valid 200 206 1h;
location / {
proxy_pass http://origin:80;
}
}
Discussion
This configuration defines a cache zone and enables it for the server.
The slice directive is then used to instruct NGINX to slice the
response into 1 MB file segments. The cache files are stored accord‐
ing to the proxy_cache_key directive. Note the use of the embedded
variable named slice_range. That same variable is used as a header
when making the request to the origin, and that request HTTP ver‐
sion is upgraded to HTTP/1.1 because 1.0 does not support byte-
range requests. The cache validity is set for response codes of 200 or
206 for one hour, and then the location and origins are defined.
The Cache Slice module was developed for delivery of HTML5
video, which uses byte-range requests to pseudostream content to
the browser. By default, NGINX is able to serve byte-range requests
from its cache. If a request for a byte-range is made for uncached
content, NGINX requests the entire file from the origin. When you
use the Cache Slice module, NGINX requests only the necessary
segments from the origin. Range requests that are larger than the
slice size, including the entire file, trigger subrequests for each of the
required segments, and then those segments are cached. When all of
the segments are cached, the response is assembled and sent to the
client, enabling NGINX to more efficiently cache and serve content
requested in ranges. The Cache Slice module should be used only on
large files that do not change. NGINX validates the ETag each time
4.6 Cache Slicing | 43
it receives a segment from the origin. If the ETag on the origin
changes, NGINX aborts the transaction because the cache is no
longer valid. If the content does change and the file is smaller or
your origin can handle load spikes during the cache fill process, it’s
better to use the Cache Lock module described in the blog listed in
the following Also See section.
Also See
Smart and Efficient Byte-Range Caching with NGINX & NGINX
Plus
44 | Chapter 4: Massively Scalable Content Caching
CHAPTER 5
Programmability and Automation
5.0 Introduction
Programmability refers to the ability to interact with something
through programming. The API for NGINX Plus provides just that:
the ability to interact with the configuation and behavior of NGINX
Plus through an HTTP interface. This API provides the ability to
reconfigure NGINX Plus by adding or removing upstream servers
through HTTP requests. The key-value store feature in NGINX Plus
enables another level of dynamic configuration—you can utilize
HTTP calls to inject information that NGINX Plus can use to route
or control traffic dynamically. This chapter will touch on the
NGINX Plus API and the key-value store module exposed by that
same API.
Configuration management tools automate the installation and con‐
figuration of servers, which is an invaluable utility in the age of the
cloud. Engineers of large-scale web applications no longer need to
configure servers by hand; instead, they can use one of the many
configuration management tools available. With these tools, engi‐
neers can write configurations and code one time to produce many
servers with the same configuration in a repeatable, testable, and
modular fashion. This chapter covers a few of the most popular con‐
figuration management tools available and how to use them to
install NGINX and template a base configuration. These examples
are extremely basic but demonstrate how to get an NGINX server
started with each platform.
45
5.1 NGINX Plus API
Problem
You have a dynamic environment and need to reconfigure NGINX
Plus on the fly.
Solution
Configure the NGINX Plus API to enable adding and removing
servers through API calls:
upstream backend {
zone http_backend 64k;
}
server {
# ...
location /api {
api [write=on];
# Directives limiting access to the API
# See chapter 7
}
location = /dashboard.html {
root /usr/share/nginx/html;
}
}
This NGINX Plus configuration creates an upstream server with a
shared memory zone, enables the API in the /api location block,
and provides a location for the NGINX Plus dashboard.
You can utilize the API to add servers when they come online:
$ curl -X POST -d '{"server":"172.17.0.3"}' \
'http://nginx.local/api/3/http/upstreams/backend/servers/'
{
"id":0,
"server":"172.17.0.3:80",
"weight":1,
"max_conns":0,
"max_fails":1,
"fail_timeout":"10s",
"slow_start":"0s",
"route":"",
"backup":false,
"down":false
}
46 | Chapter 5: Programmability and Automation
The curl call in this example makes a request to NGINX Plus to add
a new server to the backend upstream configuration. The HTTP
method is a POST, and a JSON object is passed as the body. The
NGINX Plus API is RESTful; therefore, there are parameters in the
request URI. The format of the URI is as follows:
/api/{version}/http/upstreams/{httpUpstreamName}/servers/
You can utilize the NGINX Plus API to list the servers in the
upstream pool:
$ curl 'http://nginx.local/api/3/http/upstreams/backend/servers/'
[
{
"id":0,
"server":"172.17.0.3:80",
"weight":1,
"max_conns":0,
"max_fails":1,
"fail_timeout":"10s",
"slow_start":"0s",
"route":"",
"backup":false,
"down":false
}
]
The curl call in this example makes a request to NGINX Plus to list
all of the servers in the upstream pool named backend. Currently,
we have only the one server that we added in the previous curl call
to the API. The request will return a upstream server object that
contains all of the configurable options for a server.
Use the NGINX Plus API to drain connections from an upstream
server, preparing it for a graceful removal from the upstream pool.
You can find details about connection draining in Recipe 2.8:
$ curl -X PATCH -d '{"drain":true}' \
'http://nginx.local/api/3/http/upstreams/backend/servers/0'
{
"id":0,
"server":"172.17.0.3:80",
"weight":1,
"max_conns":0,
"max_fails":1,
"fail_timeout":
"10s","slow_start":
"0s",
"route":"",
5.1 NGINX Plus API | 47
"backup":false,
"down":false,
"drain":true
}
In this curl, we specify that the request method is PATCH, we pass a
JSON body instructing it to drain connections for the server, and
specify the server ID by appending it to the URI. We found the ID of
the server by listing the servers in the upstream pool in the previous
curl command.
NGINX Plus will begin to drain the connections. This process can
take as long as the length of the sessions of the application. To check
in on how many active connections are being served by the server
you’ve begun to drain, use the following call and look for the active
attribute of the server being drained:
$ curl 'http://nginx.local/api/3/http/upstreams/backend'
{
"zone" : "http_backend",
"keepalive" : 0,
"peers" : [
{
"backup" : false,
"id" : 0,
"unavail" : 0,
"name" : "172.17.0.3",
"requests" : 0,
"received" : 0,
"state" : "draining",
"server" : "172.17.0.3:80",
"active" : 0,
"weight" : 1,
"fails" : 0,
"sent" : 0,
"responses" : {
"4xx" : 0,
"total" : 0,
"3xx" : 0,
"5xx" : 0,
"2xx" : 0,
"1xx" : 0
},
"health_checks" : {
"checks" : 0,
"unhealthy" : 0,
"fails" : 0
},
"downtime" : 0
}
48 | Chapter 5: Programmability and Automation
],
"zombies" : 0
}
After all connections have drained, utilize the NGINX Plus API to
remove the server from the upstream pool entirely:
$ curl -X DELETE \
'http://nginx.local/api/3/http/upstreams/backend/servers/0'
[]
The curl command makes a DELETE method request to the same
URI used to update the servers’ state. The DELETE method instructs
NGINX to remove the server. This API call returns all of the servers
and their IDs that are still left in the pool. Because we started with
an empty pool, added only one server through the API, drained it,
and then removed it, we now have an empty pool again.
Discussion
The NGINX Plus exclusive API enables dynamic application servers