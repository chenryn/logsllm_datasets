the company.
Finally, as already noted above, the equation ultimately comes down to test effi-
ciency, time, and costs. For manufacturers, the test efficiency (in test results and test
4.2
Transition to Proactive Security
111
process) is often much more important than the direct costs. Although the value of
fuzzing is an interesting topic, there are very few public metrics for the costs. It
would be interesting to see how these metrics would apply to various fuzzing proj-
ects that have already been completed.
A quick analysis of the costs for deploying fuzzers for both IKE (Table 4.2) and
FTP (Table 4.3) protocols is given here as an example. Whereas FTP is a very sim-
ple text-based protocol, the IKE protocol is complex and developing a fuzzer for it
requires significantly more time. The metrics used in these calculations were
explained earlier in this section.
The estimates for number of defects are based on our experience with contract
development and on real results from comparing the freely available PROTOS
ISAKMP/IKE fuzzer with tests conducted using the commercial Codenomicon
ISAKMP/IKE robustness test suite against the same ISAKMP implementation.
Cost for developing fuzzers within your own organization is generally lower
than acquiring a contracted fuzzer, because time required for your own employees,
especially for small fuzzing projects, can be shorter than contract time. The calcu-
lations, of course, have to take into account all additional expenses such as
employee benefits. The main problem with internally built tools is that finding and
retaining the best security researchers is no easy task, and therefore the defect count
can be estimated to be lower than for contracted work or commercial tools. We
have used an estimate of $2,000 for labor costs, although security researchers can
cost anything from $1,800 up to $4,000 a week. Contract employees often cost
more, but generally work faster with larger projects, have more experience, and
tend to have more expectations on them. They are easier to find and use temporar-
ily than a qualified security tester. For our estimate, we have summed the contract
hours into the cost of the tools. Contract work can cost from $3,000 per week up
to $10,000 per week, or even more.
Other investment consists of materials such as standard PC and the required
software such as debuggers needed for test analysis. Calculations should include
necessary office space for the test facility. For free open source tools this might be
the only investment.
112
Fuzzing Metrics
Table 4.2
Example Cost Calculation for IKE Fuzzers
Internally
Contractor
Open
Commercial
Criteria (IKE fuzzer)
Built
Developed
Source
Product
Individual flaws found (number)
1
5
4
8
Cost of tools
0
$40,000
0
$10,000
Resources to implement (weeks)
20
8
1
1
Time to implement (weeks)
20
8
2
1
Resources to test (weeks)
1
1
1
1
Time to test (weeks)
1
1
1
1
Other costs in test environment
$10,000
$10,000
$10,000
$10,000
Maintenance/year
$50,000
$10,000
$50,000
$10,000
Total time (weeks)
21
9
3
2
Total resources (weeks)
21
9
2
2
Cost per work-week
$2,000
$2,000
$2,000
$2,000
Total cost
$102,000
$78,000
$64,000
$34,000
Cost per defect
$102,000
$15,600
$16,000
$4,250
There are pros and cons for all available choices, and the best option will de-
pend on the complexity of the tested interfaces, the software that needs testing, and
the availability of in-house expertise, among many other parameters. One of the
main benefits of commercial tools comes from the maintenance. A commercial
fuzzer tool vendor will ensure future development and updates for a fixed fee that
is easy to forecast and is always lower than dedicated or contracted personnel. A
contracted fuzzer can also be negotiated to come with a fixed maintenance fee. The
main advantage of an in-house development is having complete control over the
project and being able to customize the fuzzer for your specific product.
The pricing of commercial tools can also be difficult to estimate without under-
standing how the world of commercial fuzzers works. Whereas a subscription license
to a web fuzzer can cost $17,000 a year, a complete protocol fuzzer for telecom-
munication interfaces can cost hundreds of thousands of dollars. This (hopefully)
depends on the costs that the fuzzer company needs to cover, and the business
opportunity they see with that product. Development, testing, maintenance, sup-
port, product training, and other related services do not come for free. Still, this is
typically an area where commercial products rule, as they can cover the implemen-
tation costs with a number of sales. A fuzzer that takes several dedicated people to
develop and maintain can reach better test coverage at a fraction of the costs com-
pared to contracted development. On the other hand, contract developers do not
turn down requests just because they see only a small market opportunity for such
a tool, meaning for very specialized or proprietary protocols, commercial fuzzers
will not be a possibility. It would be interesting to compare the test results of the
contract programmer with the test efficiency of other fuzzer products, but unfortu-
nately these proprietary tools are not easily available for comparison. There are sev-
eral examples in which a person who knows one particular test subject (or protocol)
precisely can use that knowledge to build a more efficient fuzzer for that specific
setup. But, on the other hand, commercial companies doing fuzzers day in and day
out might have more experience and feedback from their customers to improve the
tests as time goes on. Commercially available fuzzers have also usually been verified
to work with as many implementations of the tested interface as possible, which
Table 4.3
Example Cost Calculation for FTP Fuzzers
Internally
Contractor
Open
Commercial
Criteria (FTP fuzzer)
Built
Developed
Source
Product
Individual flaws found (number)
10
14
12
16
Cost of tools
0
$15,000
0
$10,000
Resources to implement (weeks)
9
3
1
1
Time to implement (weeks)
9
3
1
1
Resources to test (weeks)
1
1
1
1
Time to test (weeks)
1
1
1
1
Other costs in test environment
$5,000
$5,000
$5,000
$5,000
Maintenance/year
$20,000
$5,000
$10,000
$10,000
Total time (weeks)
10
4
2
2
Total resources (weeks)
10
4
2
2
Cost per work-week
$2,000
$2,000
$2,000
$2,000
Total cost
$45,000
$33,000
$19,000
$29,000
Cost per defect
$4,500
$2,357
$1,583
$1,812
4.2
Transition to Proactive Security
113
indicates that they are more likely to work also against any new implementations.
Nevertheless, forecasting the efficiency of a fuzzer is a very difficult task.
In our comparison we have not made a significant difference in the test execu-
tion times. Whereas tests conducted as part of a larger test process can easily take
a week to execute without hurting the overall process, a fuzz test conducted as part
of regression tests can almost never take more than 24 hours to execute. The
requirements set for test automation depend on what you are looking for. How
much time can you dedicate for the test execution? Where do you apply the tests in
your SDLC? And what types of people do you have available? The purposes of test
automation in general are to
• Reduce time from test to fix;
• Make testing repeatable;
• Reduce expertise and human error.
Commercial tools are typically built according to real test automation require-
ments set by commercial customers, and therefore, commercial fuzzers should inte-
grate easily with existing test execution frameworks (test controllers) and test
subject monitoring tools (instrumentation mechanisms). But it is understandable
that those integration interfaces do not apply to 100% of the users, especially when
the test target is a proprietary embedded device. Therefore, actual integration can
vary from plug-and-play to complex integration projects.
Sometimes an internally built one-off proof-of-concept fuzzer is the best solu-
tion for a security assessment. The total costs involved can be cheaper compared to
commercial tools, but the results may not necessarily be the most effective. That is
to say, a custom fuzzer may be significantly cheaper but miss a few bugs that may
have been found by a commercial fuzzer. It really depends on the priorities of the
project. When making your final decision on the fuzzer project, please remember
the users of the fuzzers. Most of the commercial tools out there are ready to be used
by the average QA person without any changes by the end users, whereas for other
tools there will be almost always constant customization needs. The techniques
used in fuzzers (and explained in later chapters) will influence the easiness of those
changes for various future needs.
From a test automation perspective, the actual usage of the tools varies, as we
will explore in later chapters. Most commercial tools are ready to go in the test
setup in that you can install them and immediately fire them up, giving them an IP
to blast away at. From a QA perspective, you might want a few additional things
to be at your disposal, and therefore, increase the cost of the test environment (test-
bed), for example, instrumentation. Various tools can be used to catch the crashes
or other suspicious failures. In fuzzing you need to do this in an automated fashion.
Valgrind11 is an example analysis framework for the Linux platform, and
PaiMei/Pydbg is a great example of this for the Windows platforms, as you can
have the tested application crash, do its core dump, and then respawn the process
to continue with your next test case. After you are finished with the test run, you
114
Fuzzing Metrics
11For more information on Valgrind, see http://valgrind.org
will have an accurate picture of where the problems lie. Additional on-host instru-
ments can monitor thread counts, file handle counts, and memory usage. The prob-
lem with on-host instrumentation is supporting all possible embedded devices and
both commercial and proprietary operating systems used in application platforms.
Most development platforms offer such monitoring in their development environ-
ments and through various debuggers. We will discuss this more in Chapter 6.
4.2.2
Cost of Remediation
In addition to the cost of discovery, we also need to understand the cost of fixing
each bug. After a software vulnerability is found, developers need to go back to the
drawing board and fix the flaw. A flaw found during the design phase is less costly
than a flaw found after the product has been launched. Various attempts at summa-
rizing the economics of software defects indicate very similar results, and people only
argue about the actual multipliers in the cost increase. Figure 4.2 gives one perspec-
tive to the cost increases, based on various studies and especially those from NIST.
A tool that automatically categorizes the findings during the discovery phase
can reduce the cost of defect elimination. When the person conducting the tests can
immediately see that 10 programming flaws cause the failures, he or she can then
issue fewer reports for those in charge of the repairs. With a fuzzer that is based on
semi-random inputs, you can potentially have 100,000 tests that find something
suspicious. Analyzing all those problems will take more time than just analyzing
ten identified and verified flaws.
4.2
Transition to Proactive Security
115
Figure 4.2
The increasing cost of repairing with estimates on the numbers of defects found, with
relation to the development phase where discovered (based on NIST publications).
Two categories of metrics can apply to the repair process:
• Resources needed to fix the problems (required time from the development
people): After the failures have been analyzed, the developers start their task
in analyzing the bug reports and fixing the flaws. Although there rarely are
any false positives with fuzzing, a common problem with the test results is
that a large number of issues can be caused by a single flaw.
• Time required to fix the found issues (delays to product launch): Adding
more developers to the repair process does not necessarily reduce the total
calendar days spent on repairing the found issues.
4.2.3
Cost of Security Compromises
An important aspect of security is the assurance of service availability and software
reliability. Reliability is measured by up-time and down-time and studies the reasons
for the down-time, or outages. Down-time can be either planned or unplanned and
can be due to change control or third-party involvement, or even an “act of god”
such as an earthquake. Planned down-time can be due to regular maintenance such
as deploying regularly released security updates or to scheduled activities related to
upgrades and other housekeeping jobs. Security compromises are an example of
unplanned events that are caused by a third party (an attacker), and they often lead
to down-time of a device or a service. But security incidents are not the only reason
for unplanned down-time of critical systems. Other unexpected outages include
hardware failures, system failures, and natural disasters. As traditional metrics
already exist for these purposes, they should be applied as metrics to security-related
availability analysis. The metrics used by IT operations personnel study the efficiency
of IT reliability, and these same metrics are applicable to the inability of software to
withstand denial of service attacks. Useful metrics related to uptime include:12
• Measured up-time of software, host, or service (percent, hours) gives the avail-
ability metric such as “five nines” 99.999% uptime.
• Planned down-time (percent, time) is the total amount of time that resources
were out of service due to regular maintenance.
• Unplanned down-time (percent, hours) shows the total amount of time re-
lated to unexpected service outages and represents the change control process
variance.
• Unplanned down-time due to security incidents (percent, hours) is often a
subset of the above and indicates the result of security shortcomings.
• Mean/median of unplanned outage (time) characterizes the seriousness of a