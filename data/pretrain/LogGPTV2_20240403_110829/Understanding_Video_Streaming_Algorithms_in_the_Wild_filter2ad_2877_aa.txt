title:Understanding Video Streaming Algorithms in the Wild
author:Melissa Licciardello and
Maximilian Gr&quot;uner and
Ankit Singla
Understanding Video Streaming
Algorithms in the Wild
Melissa Licciardello, Maximilian Gr¨uner(B), and Ankit Singla
Department of Computer Science, ETH Z¨urich, Z¨urich, Switzerland
{melissa.licciardello,mgruener,ankit.singla}inf.ethz.ch
Abstract. While video streaming algorithms are a hot research area,
with interesting new approaches proposed every few months, little is
known about the behavior of the streaming algorithms deployed across
large online streaming platforms that account for a substantial fraction of
Internet traﬃc. We thus study adaptive bitrate streaming algorithms in
use at 10 such video platforms with diverse target audiences. We collect
traces of each video player’s response to controlled variations in network
bandwidth, and examine the algorithmic behavior: how risk averse is
an algorithm in terms of target buﬀer; how long does it takes to reach
a stable state after startup; how reactive is it in attempting to match
bandwidth versus operating stably; how eﬃciently does it use the avail-
able network bandwidth; etc. We ﬁnd that deployed algorithms exhibit
a wide spectrum of behaviors across these axes, indicating the lack of
a consensus one-size-ﬁts-all solution. We also ﬁnd evidence that most
deployed algorithms are tuned towards stable behavior rather than fast
adaptation to bandwidth variations, some are tuned towards a visual
perception metric rather than a bitrate-based metric, and many leave a
surprisingly large amount of the available bandwidth unused.
1 Introduction
Video streaming now forms more than 60% of Internet downstream traﬃc [25].
Thus, methods of delivering video streams that provide the best user experience
despite variability in network conditions are an area of great industry relevance
and academic interest. At a coarse level, the problem is to provide a client
with the highest possible video resolution, while minimizing pauses in the video
stream. There are other factors to consider, of course, such as not switching
video resolution often. These considerations are typically rolled into one quality-
of-experience score. Streaming services then use adaptive bitrate algorithms,
which attempt to maximize QoE by dynamically deciding what resolution to
fetch video segments at, as network conditions ﬂuctuate.
While high-quality academic work proposing novel ABR is plentiful, the lit-
erature is much more limited (Sect. 2) in its analysis of widely deployed ABRs,
their target QoE metrics, and how they compare to recent research proposals.
M. Licciardello and M. Gr¨uner—Equal contribution.
c(cid:2) Springer Nature Switzerland AG 2020
A. Sperotto et al. (Eds.): PAM 2020, LNCS 12048, pp. 298–313, 2020.
https://doi.org/10.1007/978-3-030-44081-7_18
Understanding Video Streaming Algorithms in the Wild
299
The goal of this work is precisely to address this gap. Understanding how video
platforms serving content to large user populations operate their ABR is crucial
to framing future research on this important topic. For instance, we would like to
know if there is a consensus across video platforms on how ABR should behave,
or whether diﬀerent target populations, content niches, and metrics of interest,
lead to substantially diﬀerent ABR behavior. We would also like to understand
whether ABR research is optimizing for the same metrics as deployed platforms,
which are presumably tuned based on operator experience with real users and
their measured engagement.
Towards addressing these questions, we present a study of ABR behavior
across 10 video streaming platforms (Table 1) chosen for coverage across their
diverse target populations: some of the largest ones in terms of overall market
share, some regional ones, and some specialized to particular applications like
game streaming (not live, archived). Our methodology is simple: we throttle
download bandwidth at the client in a time-variant fashion based on throughput
traces used in ABR research, and monitor the behavior of streams from diﬀerent
streaming platforms by analyzing jointly their browser-generated HTTP Archive
(HAR) ﬁles and properties exposed by the video players themselves. For robust
measurements, we collect data for several videos on each platform, with our
analysis herein being based on 6 days of continuous online streaming in total.
Our main ﬁndings are as follows:
1. Deployed ABRs exhibit a wide spectrum of behaviors in terms of how much
buﬀer they seek to maintain in their stable state, how closely they try to
match changing bandwidth vs. operating more smoothly, how they approach
stable behavior after stream initialization, and how well they use available
network bandwidth. There is thus not a consensus one-size-ﬁts-all approach
in wide deployment.
2. Several deployed ABRs perform better on a QoE metric based on visual
perception rather than just video bitrate. This lends support to the goals
of recent work [22], indicating that at least some of the industry is already
optimizing towards such metrics rather than the bitrate-focused formulations
in most prior ABR research.
3. Most deployed ABRs eschew fast changes in response to bandwidth vari-
ations, exhibiting stable behavior. In contrast, research ABRs follow band-
width changes more closely. It is unclear whether this is due to (a) a mismatch
in target metrics used in research and industrial ABR; or (b) industrial ABR
being sub-optimal.
4. Several deployed ABRs leave substantial available bandwidth unused. For
instance YouTube uses less than 60% of the network’s available bandwidth
on average across our test traces. Similar to the above, it is unclear whether
this is due to ABR sub-optimality, or a conscious eﬀort to decrease bandwidth
costs.
300
M. Licciardello et al.
(a) Experimental setup
(b) Proxy impact
Fig. 1. (a) Player behaviour is inﬂuenced through bandwidth throttling, and is recorded
from multiple sources. (b) The proxy has little impact on player behavior as measured in
terms of average linear QoE (QoElinear); the whiskers are the 95% conﬁdence interval.
2 Related Work
There is a ﬂurry of academic ABR proposals [6,8,13,14,17,18,22,23,26,27,29,
32], but only limited study of the large number of deployed video streaming
platforms catering to varied video types and audiences.
YouTube itself is relatively well studied, with several analyses of various
aspects of its behavior [7,19,31], including video encoding, startup behavior,
bandwidth variations at ﬁxed quality, a test similar to our reactivity analysis,
variation of segment lengths, and redownloads to replace already fetched seg-
ments. There is also an end-end analysis of Yahoo’s video streaming platform
using data from the provider [10].
Several comparisons and analysis of academic ABR algorithms [28,30,33]
have also been published, including within each of the several new proposals
mentioned above. In particular, [28] compares three reference ABR implemen-
tations, showing that the conﬁguration of various parameters has a substantial
impact on their performance.
Facebook recently published [16] their test of Pensieve [17] in their video
platform, reporting small improvements (average video quality improvement of
1.6% and average reduction of 0.4% in rebuﬀers) compared to their deployed
approach.
However, a broader comparative study that examines a large number of
diverse, popular streaming platforms has thus far been missing. Note also that
unlike ABR comparisons in academic work and head-to-head comparisons of
methods in Facebook’s study, QoE comparisons across platforms are not neces-
sarily meaningful, given the diﬀerences in their content encoding, content type,
and audiences. Thus, in contrast to prior work, we deﬁne a set of metrics that
broadly characterize ABR behavior and compare the observed behavior of a
large, diverse set of streaming providers on these metrics. Where relevant, we
also contrast the behavior of these deployed ABRs with research proposals.
Understanding Video Streaming Algorithms in the Wild
301
To the best of our knowledge this is the only work to compare a large set of
deployed ABRs and discuss how their behavior diﬀers from academic work in
this direction.
3 Methodology
To understand a target platform’s ABR, we must collect traces of its behavior,
including the video player’s state (in terms of selected video quality and buﬀer
occupancy) across controlled network conditions and diﬀerent videos.
3.1 Experimental Setup
Figure 1a shows our architecture for collecting traces about player behaviour.
Our Python3 implementation (available at [11]) uses the Selenium browser
automation framework [4] to interact with online services. For academic ABR
algorithms, trace collection is simpler, and uses oﬄine simulation, as suggested
in [17].
While playing a video, we throttle the throughput at the client (1) using
tc (Traﬃc control, a Linux tool).1 The state of the client browser (e.g., current
buﬀer occupancy) is captured by the Monitor (5) every a seconds. All requests
sent from the client (1) to the server (3) are logged by a local proxy (2). Beyond
the ﬁnal browser state, the proxy allows us to log video player activity such
as chunks that are requested but not played. We also obtain metadata about
the video from the server (e.g., at what bitrate each video quality is encoded).
Metadata is obtained through oﬄine analysis by downloading the video at all
diﬀerent qualities. All information gathered from the three sources — the proxy,
the browser and the server — is aggregated (4).
Certain players replace chunks previously downloaded at low quality with
high quality ones (“redownloading”) in case there is later more bandwidth and
no immediate rebuﬀer risk. Using the proxy’s view of requests and responses and
the video metadata, we can map every chunk downloaded to a play-range within
the video, and use this mapping to identify which chunks/how many bytes were
redownloaded.
How Do We Add a Platform to Our Measurements? Most video plat-
forms (all except YouTube, for which we use [5]) use chunk-based streaming. To
evaluate such platforms, we use developer tools in Chrome to understand how
the player obtains the download links for the chunks. Typically, a .m3u8 [21] ﬁle
downloaded by the player contains the locations for all chunks at all qualities.
This allows us to write code that fetches all chunks for the test videos at all
qualities, such that we can use these videos in our oﬄine simulation analysis of
1 At the bandwidth levels seen in our traces, bottlenecks are at our client—our univer-
sity’s connectivity to large services is otherwise high-bandwidth, consistently result-
ing in the highest-quality playback available on each service.
302
M. Licciardello et al.
the academic Robust MPC approach.2 Having all chunks available also enables
calculation of their visual perceived quality (VMAF [15]). We also need to map
each chunk to its bitrate level and time in the video stream, by understanding
how video content is named in the platform (e.g., through “itags” in YouTube).
For online experiments through the browser, we need to instrument the plat-
form’s video player. We do this by automating the selection of the HTML5 video
player element, and having our browser automation framework use this to start
the video player and put it in full screen mode. We can then access the current
buﬀer occupancy and current playback time using standard HTML5 attributes.
We use a proxy to log the remaining statistics (e.g., resolution played/fetched)
because relying on the player alone would have required painstaking code injec-
tion specialized to each provider.
YouTube does not follow such chunked behavior (as past work has noted [19]).
It can request arbitrary byte ranges of video from the server. We use an already
available tool [5] to download the videos, and then learn the mapping from the
byte ranges to play time from the downloaded videos.
3.2 The Proxy’s Impact on Measurements
Some of our measurements (e.g., redownloads) use an on-path proxy, so we verify
that this does not have a meaningful impact by comparing metrics that can be
evaluated without the proxy. For this, we use traces with constant bandwidth
b ∈ [0.5, 0.8, 1.2, 2.5] Mbps, repeating each experiment 5 times for the same video.
For our comparison, we calculate QoE using the linear function from MPC [18]
with and without the proxy. For every video-network trace combination, we
calculate the mean QoE and show the mean across these, together with its 95%
conﬁdence interval with whiskers in Fig. 1b.
As the results show, for most platforms the proxy has a minimal impact:
across providers, the average diﬀerence in QoE with and without the proxy is 7%.
For YouTube and ZDF, the diﬀerences are larger, but still within the conﬁdence
bounds: for these providers, there are large variations across experiments even
without the proxy, indicating diﬀering behaviour in very similar conditions in
general.
3.3 Metrics of Interest
Diﬀerent video platforms serve very diﬀerent types of content, and target dif-
ferent geographies with varied client connectivity characteristics. It is thus not
particularly informative to compare metrics like bitrate-based QoE across plat-
forms. For instance, given the diﬀerent bitrate encodings for diﬀerent types of
content, bitrate-QoE is not comparable across platforms. We thus focus on com-
parisons in terms of the following behavioral and algorithm design aspects.
2 To avoid the unintended use of our scripts for downloading copyright-protected con-
tent, we refrain from publishing code for this part of our pipeline.
Understanding Video Streaming Algorithms in the Wild
303
Initialization Behavior: We quantify how much wait time a video platform
typically incurs for streams to start playback, and how much buﬀer (in seconds
of playback) it builds before starting. We use traces with a ﬁxed bandwidth of
3 Mbps until player’s HTML5 interactions are available, thus always downloading
items like the player itself at a ﬁxed bandwidth. This is done to avoid failure at
startup: some platforms cause errors if network conditions are harsh from the
beginning. After this, we throttle using only the high-bandwidth traces from the
Oboe [6] data set, which have a mean throughput of 2.7 Mbps. We start timing
from when the ﬁrst chunk starts downloading (per the HAR ﬁles; the player
HTML5 interactions may become available earlier or later).
Table 1. We test a diverse set of large video platforms.
Provider Description
Arte
French-German, cultural
Fandom Gaming, pop-culture