# 18 | 混沌工程：软件领域的反脆弱

你好，我是石雪峰。经济学领域有一本非常著名的书，名为《反脆弱》。该书的核心理念是，在面对普遍存在且不可预估的不确定性时，通过一种行之有效的方法，不仅可以规避重大风险，还能利用风险获取超出预期的回报。此外，通过积极试错和控制损失成本，可以不断提升在不确定性事件发生时的收益。不仅规避风险，还要从中受益，这听起来是不是很神奇？

其实，在软件工程领域，也有类似的思想和实践，可以帮助我们在面对极其复杂且规模庞大的分布式系统时，有效地应对不可预见的故障，不仅能从容不迫地应对，还能从中获益，并通过频繁、大量的实验识别并解决潜在的风险点，从而提升对复杂系统的信心。这就是今天我要分享的主题：混沌工程。

## 什么是混沌工程？

混沌工程作为软件领域的一门新兴学科，其名称本身就让许多人感到“混沌”。那么，混沌工程究竟是从何而来，又要解决什么问题呢？我们先来看看混沌原则网站对混沌工程的定义：

> Chaos Engineering is the discipline of experimenting on a distributed system in order to build confidence in the system's capability to withstand turbulent conditions in production.

简单来说，混沌工程是一门在分布式系统上进行实验的学科，目的是建立人们对于复杂系统在生产环境中抵御突发事件的信心。具体来说，混沌工程要解决的是复杂环境下的分布式系统的反脆弱问题。

### 复杂的分布式系统

我们所面临的“复杂的分布式”世界是什么样的呢？举个例子，一个大型平台每日在线活动数以万计，服务用户达到千万级别。为了满足这种规模的业务量级，仅客户端就有300多个组件，后端服务更是不计其数。这样一套复杂的系统，任何一个地方出现小问题都可能引发线上事故。

随着微服务、容器化等技术的兴起，业务驱动自组织团队独立发布的频率越来越高，再加上架构的不断更新演进，几乎没有人能完整地梳理清楚一套系统的服务间调用关系。这就使得复杂系统变成了一个“黑洞”，无论外围如何敲打，都很难窥探到核心问题。

为了让你对复杂的真实系统有更直观的认识，我分享一张Netflix公司在2014年公开的微服务调用关系图（见下图）。

![](https://static001.geekbang.org/resource/image/fc/db/fc2a435fcba0551bf2b072fa48155edb.jpeg)

面对这样复杂的分布式系统，通过穷尽全面的测试来保障质量几乎是不可能的。因为测试的假设前提是为了验证软件的预期行为，而真实世界的问题却从来不按套路出牌。被动遵循已有经验并不能预防和解决未知问题。特别是，如果系统的可用性基于某个服务不会出问题来设计，那么这个服务十有八九会出问题。

例如，前不久我们内部的平台出现了一次宕机，原因是依赖的一个基础服务的认证模块出现了异常，导致存储数据失败。虽然监控第一时间发现了这个问题，但由于所有基础数据都在这个看似万无一失的服务上保存，除了等待之外，我们什么都做不了。结果，平台的可用性直接从四个9掉到了三个9。

既然面对复杂的分布式系统无法避免异常事件的发生，有什么更好的办法来应对这种不确定性吗？Netflix公司给出了他们的回答，而这正是混沌工程诞生的初衷。

## 混沌工程的实践

与以往的方式不同，混沌工程采取了一种更加积极的方式，主动出击。通过在这些故障和缺陷发生之前进行一系列实验，在真实环境中验证系统在故障发生时的表现。根据实验结果识别风险问题，并有针对性地进行系统改造和安全加固，从而提升对整个系统可用性的信心。

### 服务可用性实践

你可能会问，这不就是日常的系统可用性保障活动吗？的确，这些实践与混沌工程有相似之处，但思路略有不同。正规的公司通常都会有一套完整的数据备份机制和服务应急响应预案，以确保灾难发生时系统的可用性和核心数据的安全。

#### 故障演练

故障演练是针对以往发生过的问题进行有针对性的模拟演练。通过事先定义好的演练范围，人为模拟事故发生，触发应急响应预案，快速进行故障定位和服务切换，并观察整个过程的耗时和各项数据指标的表现。故障演练主要针对可预见的问题，如物理机异常关机、断电、磁盘空间写满、I/O变慢、网络延迟、DNS解析异常等。这些问题都有明确的触发因素、监控事项和解决方法。但在实际问题发生时，往往是多个变量一起出问题，逐个排查非常耗时耗力。

#### 全链路压测

为了模拟线上的真实场景，很多公司引入了全链路压测技术。对于大促密集的电商行业尤为重要。一次完整的压测过程大致如下：

1. 准备压测计划，调试压测脚本和环境，对压测容量和范围进行预估。
2. 完成机房线路切换，确保在压测过程中没有线上真实流量的引入。
3. 根据预定义的压测场景执行压测计划，观察流量峰值并动态调整。
4. 压测完成后，再次进行流量切换并汇总压测结果，识别压测问题。

在压测过程中，除了关注QPS指标外，还要关注TP99、CPU使用率、CPU负载、内存、TCP连接数等，从而客观体现大流量下服务的可用性。

#### 服务降级预案

从业务层面来说，面对多变的环境因素，完善的服务降级预案和系统兜底机制也是必不可少的。在业务压力较大时，可以适当屏蔽一些对用户感知不大的服务，如推荐、辅助工具、日志打印、状态提示等，保证最核心流程的可用性。适当地引入排队机制也能在一定程度上分散瞬时压力。

然而，即便把这些都做到位也不能确保万无一失。这是因为这些活动都是在打有准备之仗，但实际上很多问题都是无法预知的。因此，需要一种有效的实验方法帮助我们基于各种要素排列组合，在问题发生之前发现潜在风险。

### Netflix的混沌工程实践

Netflix公司著名的“混乱猴子（Chaos Monkey）”就是一个随机关闭生产环境实例的工具。在生产环境中放任一个“猴子”搞事情，听起来似乎有些疯狂，但实际上，这是为了揭示潜在问题。Netflix的“猴子军团”威力巨大，甚至可以直接干掉一个云服务可用区。这背后的原因是，即使在云服务上，也不能确保它们的服务永远可靠。因此，不要把可用性的假设建立在依赖服务不会出问题的基础上。

当然，Netflix并没有权限真正关闭云服务上的可用区，他们只是模拟了这个过程，并促使工程团队建立多区域的可用性系统，促进研发团队直面失败的架构设计，不断磨练工程师对弹性系统的认知。引用Netflix的混沌工程师Nora Jones的话来说：“混沌工程不是为了制造问题，而是为了揭示问题。”

必须强调的是，在引入混沌工程实践之前，首先需要确保现有的服务已经具备了弹性模式，并且能够在应急响应预案和自动化工具的支撑下尽早解决可能出现的问题。如果现有的服务连基本的可恢复性都不具备，那么这种混沌实验是没有意义的。

## 混沌工程的原则

混沌工程不同于以往的工具和实践，作为一门学科，它具有丰富的内涵和外延。进入这个领域之前，有必要了解混沌工程的五大原则：**建立稳定状态的假设、真实世界的事件、在生产中试验、持续的自动化实验、最小影响范围**。

### 1. 建立稳定状态的假设

关于系统的稳定状态，是指有哪些指标可以证明当前系统是正常的、健康的。无论是技术指标还是业务指标，现有的监控系统都已经足够强大，稍有抖动都能在第一时间发现。例如，对于技术指标，前面提到的QPS、TP99、CPU使用率等；对于业务指标，根据具体业务的不同会有所不同。例如，对于游戏，在线用户数和平均在线时长很重要；对于电商，各种到达率、结算完成率以及更宏观的GMV、用户拉新数等都能表现出业务的健康程度。

与技术指标相比，业务指标更为重要，尤其是对电商这种活动密集型行业来说，业务指标会受到活动的影响，但基于历史数据分析，总体趋势比较明显。当业务指标发生大量抖动时（如瞬时降低或提升），意味着系统出现了异常。例如，几天前微信支付出现问题，从监控来看，支付的成功率受到了明显影响。

在真实世界中，描述一种稳定状态需要一组指标构成模型，而不是单一指标。无论是否采用混沌工程，识别这类指标的健康状态都是至关重要的。还需要围绕它们建立一整套完善的数据采集、监控、预警机制。

### 2. 真实世界的事件

真实世界的许多问题都来源于过往踩过的“坑”。即便是特别不起眼的事件，也可能带来严重后果。例如，服务器在处理并发任务时，CPU跑满，系统直接卡死。调查发现，出现问题时系统的I/O Wait很高，说明磁盘发生了I/O瓶颈。最终发现是磁盘Raid卡上的电池没电了，导致磁盘Raid模式降级。

这种情况难以通过监控所有Raid卡的电池容量来规避，也不可能每次模拟故障时故意换上没电的电池进行演练。因此，投入产出比最高的方法是选择重要指标（如设备可用性、网络延迟及各类服务器问题），进行有针对性的实验。结合全链路压测等手段，从全局视角测试系统整体运作的可用性，通过与稳定状态的假设指标对比，识别潜在问题。

### 3. 在生产中试验

类似于测试领域的“质量右移理念”，混沌工程同样鼓励在靠近生产环境的地方进行实验，甚至直接在生产环境中进行实验。因为只有在生产环境中才会出现真实世界的问题。一个小规模的预发布环境更多的是验证系统行为和功能符合产品设计，即从功能角度验证是否有新增缺陷和质量回退。但系统的行为会根据真实的流量和用户行为而改变。例如，流量明星的一则消息可能导致微博系统崩溃，这是在测试环境中难以复现的场景。

客观来说，在生产环境中进行实验存在风险，要求实验范围可控，并具备随时停止实验的能力。如果系统没有为弹性模式做好准备，就不要开启生产实验。以压测为例，我们可以随机选择部分业务模块，并圈定部分实验节点，然后开启常态化压测。通过定期将线上流量打到被测业务上，观察突发流量下的指标表现，以及是否会引发系统雪崩，断路器是否生效等，往往在没有准备的时候才能发现真实问题。

### 4. 持续的自动化实验

自动化是所有重复性活动的最佳解决方案。通过自动化的实验和自动化结果分析，可以保证混沌工程的诸多实践低成本、自动化地执行。因此，以混沌工程为名的工具越来越多。例如，商业化的混沌工程平台Gremlins支持不可用依赖、网络不可达、突发流量等场景。今年，阿里也开源了他们的混沌工具ChaosBlade，缩短了构建混沌工程的路径，引入了更多实践场景。另外，开源的Resilience4j和Hystrix也是非常好用的工具。无论是自研还是直接采用，都可以帮助你快速上手。相信随着越来越多工具的成熟，未来混沌工程也会成为CI/CD流水线的一部分，被纳入到日常工作中来。

### 5. 最小影响范围

混沌工程实践的原则是不要干扰真实用户的使用，因此在一开始将实验控制在一个较小的范围内是非常必要的，这样可以避免由于实验失控带来的更大问题。例如，圈定一小部分用户或服务范围，可以帮助我们客观评估实验的可行性。假设要实验一个API对错误的处理能力，可以部署一个新的API实验集群，并修改路由导流0.5%的流量用于线上实验。在这个集群中通过故障注入的方式，验证API是否能够处理流量带来的错误场景。这有点类似于灰度实验环境或暗部署方式，适用于混沌工程的故障注入。

这五大原则共同勾勒出了混沌工程的全景图，描述系统稳定状态的前提下，将真实世界的事件在生产环境中进行实验，并控制最小影响范围，引入自动化方式持续进行。作为一种全新的工程领域，混沌工程还有很长的路要走，才能跨越技术演进的鸿沟。

## 总结

在这一讲中，我介绍了一个应对复杂分布式系统可用性挑战的新学科——混沌工程。实际上，混沌工程采用了一种全新的思路，在系统中主动注入混沌进行实验，以此发现潜在的真实世界问题。在服务可用性方面，我们一直在努力实践，如故障演练、服务降级、全链路压测已成为大型系统的标配。最后，我介绍了混沌工程的五个实践原则，希望能帮助你建立更全面的认知。

不可否认，目前国内在混沌工程领域的实践还处于摸索阶段，但随着系统的复杂性越来越高，混沌工程注定会成为解决复杂系统可用性问题的利器。

### 思考题

关于真实世界中发生的异常事件，你有哪些独特的经历呢？结合混沌工程的实践，你有什么新的思路吗？欢迎在留言区写下你的思考和答案，我们一起讨论，共同学习进步。如果你觉得这篇文章对你有所帮助，也欢迎你把文章分享给你的朋友。

![](https://static001.geekbang.org/resource/image/7c/33/7c26a9b917677371cf3aac78d949ae33.jpg)