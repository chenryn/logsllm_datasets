algorithm that, given a sample set of transitions as input,
generates a set of predicate guards for the SFA model.
The main data structure utilized by the algorithm is the
special observation table SOT = (S, W, Λ, T ), where S and
W are, possibly incomplete, sets of access and distinguishing
strings for the target automaton, Λ ⊆ S· Σ is a set of sample
transitions and T is a table with rows over S∪Λ and columns
in W . Given a row s and column w, the table is populated
with T (s, w) = l(M [sw]). Figure 3 shows a simple SFA
along with the observation table entries for the S and W
sets.
The algorithm initializes the table with S = W = {} and
a set of sample transitions Λ (a single symbol suﬃces). The
SOT is called closed if for every α ∈ Λ, there exists s ∈ S
such that α ≡ s mod W . Once all entries in the table are
populated using membership queries, the table is checked for
closedness. If there exists an α ∈ Λ such that the closedness
condition is not satisﬁed, then α is accessing a previously
undiscovered state in the target automaton. Thus, we move
α into the set S, ﬁll the new entries in the table, and check
again for closedness. Eventually, this process will produce a
closed SOT if the target language is regular.
Updating models. Given a closed SOT , the learning algo-
rithm constructs an SFA model. This model is then tested
for equivalence with the target automaton. In the abstract
learning model this is achieved using a single equivalence
in practice, various testing methods are
query, however,
utilized to simulate an equivalence query.
If the learned
model is not equivalent to the target machine, the equiv-
alence query returns a counterexample input s that causes
the model to produce diﬀerent output than the target ma-
chine. The learning algorithm uses the counterexample to
reﬁne the generated model by either adding a missing state
or correcting an invalid transition.
3. BOOTSTRAPPING SFA LEARNING
Motivation. Consider a user that has invested a signiﬁ-
cant time budget to infer an SFA model for a speciﬁc ver-
sion of a program. When a new version of the program is
released, one can expect it to be, in many aspects, similar
with the previous version. In such settings, the ability to
incrementally learn the SFA model for the new version can
be a very useful feature. The learning process will become
signiﬁcantly faster if SFADiff can somehow utilize the old
model for learning the new model. In this section, we pro-
vide an eﬃcient algorithm in order to bootstrap the SFA
learning algorithm by initializing it with an existing model.
Our method ensures that, if the system we are trying to in-
fer is the same as the model used for initializing the learning
algorithm, only a single equivalence query will be made by
the learning algorithm in order to verify the equivalence of
the model with the system. Since simulating equivalence
queries is usually the most expensive part in learning, being
able to save equivalence queries provide a signiﬁcant overall
optimization in the learning process.
Notice that, most popular algorithms for simulating equiv-
alence queries are intractable for large alphabets. For ex-
ample, consider the case of Chow’s W-method [12], that is
used by popular automata inference frameworks like Learn-
Lib [24] for simulating equivalence queries. The W-method
accepts as input a model automaton M with m states and
an upper bound n on the number of states of the target
automaton. The W-method compiles a set of test cases to
verify that, if the target automaton has at most n states,
then it is equivalent to the model automaton. Unfortu-
nately, in order to verify equivalence, the W-method per-
forms O(n2m|Σ|n−m+1) membership queries to the target
system. The exponential term in the alphabet size makes
the method prohibitive for usage in models with large al-
phabets (e.g. all printable characters or even larger sets if
we include Unicode symbols).
Our algorithm. Given an initial SFA model Minit we boot-
strap the ASKK algorithm by creating a special observation
table SOT = (S, W, Λ, T ) with the S, W, Λ sets initialized
from Minit, as described below, while the entries of the table
are ﬁlled using membership queries to the target automa-
ton. This technique allows us to build a correct model if the
initial model and the target system are equivalent. If the
two systems are not equivalent but similar, i.e. they share
certain access and distinguishing strings, then our initial-
ization algorithm will recover those without performing any
equivalence queries. We will now describe how to initialize
each component of the special observation table.
Initializing the SOT
3.1
Initializing S. Initializing S corresponds to the recovery of
all access strings of Minit. This is a straightforward procedure
using a DFS search in the graph induced by Minit. The
procedure starts with an empty access string for the initial
state of the automaton. Every time we exercise a transition
(qs, φ, qt), we check if an access string for qt is already in
S. If no access string exists for qt then, we select a witness
α ∈ φ from the predicate guard of the transition and we
assign the access string sqs α for state qt where sqs ∈ S is an
access string for qs. Once all states are covered, we return
the set of access strings.
Initializing W . Initializing the W set corresponds to the
creation of a set of distinguishing strings for Minit. Algo-
rithms for creating distinguishing sets for DFAs date back to
the development of Chow’s W-method [12]. Adapting these
algorithms in the SFA setting is straightforward by adapt-
ing the SFA minimization algorithms developed recently by
D’Antoni and Veanes [14]. We note that these algorithms
are the most eﬃcient known algorithms for SFA minimiza-
tion and the adaptation for generating a set of distinguish-
ing strings will produce a set of distinguishing strings of size
n − 1 for an SFA with n states.
Initializing Λ. In order to correctly initialize the Λ com-
ponent of the SOT , we have to provide, for every state q of
Minit a set of sample transitions that, when given as input
into the guardgen() algorithm will produce the correct set
of predicate guards for q.
The predicate guards used by the SFA learning algorithm
in [5] are simply sets of symbols from the alphabet. Given a
set of sample transitions for a state q, the guardgen() algo-
rithm from [5] works as follows: All transitions for symbols
from state q already in the Λ set are grouped into predi-
cate guards based on the target of the transition which is
determined as in the original L∗ algorithm [6]. The transi-
tions for symbols which are not part of the Λ set are merged
into the predicate guard with the largest size, i.e. the tran-
sition containing most symbols. The intuition behind this
algorithm is that in most parsers, only a small numbers of
symbols is advancing the automaton towards an accepting
state, while most other symbols are grouped together in a
single transition leading to a rejecting state.
Therefore, given a state q in Minit, in order to construct a
sample set of transitions that will result in producing the cor-
rect predicate guards with the aforementioned guardgen()
algorithm, we proceed as follows: Let {φ1, φ2, . . . , φk} be
the set of predicate guards for the state q such that i <
j =⇒ |φi| ≥ |φj|. Moreover, let sq be the access string for
q and T = ∪i∈{2,...,k}φi. Then, for each α ∈ T , we add the
string sqα in Λ. This will ensure that the predicate guards
for φ2, . . . , φk will be produced correctly by the guardgen()
algorithm. Finally, we have to ensure that enough sample
transitions from φ1 are added in Λ in order for φ1 to get
all implicit transitions which are not part of Λ. To achieve
that, we select l2 = |φ2| + 1 elements αj ∈ φ1, j ∈ {1, . . . , l2}
and add the strings sqαj in Λ. This operation ensures that
if the transitions of the target automaton are the same as
in Minit, they will be generated correctly by the guardgen()
algorithm. Repeating this procedure for all states of Minit
completes the initialization of the Λ set.
4. DIFFERENTIAL SFA LEARNING
4.1 Basic Algorithm
The main idea behind our diﬀerential testing algorithm is
to leverage automata learning in order to infer SFA-based
models for the test programs and then compare the result-
ing models for equivalence as shown in Figure 1. As men-
tioned above, this technique has a number of advantages
such as being able to generalize from comparing individual
input/output pairs and build models for the programs that
are examined.
Algorithm 1 provides the basic algorithmic framework for
diﬀerential testing using automata learning. The algorithm
takes two program implementations as input. The ﬁrst func-
tion calls, to the GetInitialModel function, are responsible
for bootstrapping the models for the two programs. In our
case this function is implemented using the observation ta-
ble initialization algorithm described in Section 3. The ini-
tialized models are then checked for diﬀerences using the
RCADiff function call. The internals of this function are
described in detail in Section 4.2. This function is respon-
sible for categorizing the diﬀerences in the two models and
Algorithm 1 Diﬀerential SFA Testing Algorithm
Require: P1, P2 are two programs
function GetDifferences(P1, P2)
M1 ← GetInitialModel(P1)
M2 ← GetInitialModel(P2)
while true do
S ← RCADiﬀ(M1, M2)
if S = ∅ then
return ∅
end if
modelUpdated ← F alse
for s ∈ S do
if P1(s) (cid:54)= M1(s) then
M1 ← UpdateModel(M1, s)
modelUpdated ← T rue
end if
if P2(s) (cid:54)= M2(s) then
M2 ← UpdateModel(M2, s)
modelUpdated ← T rue
end if
end for
if modelUpdated = F alse then
return S
end if
end while
end function
return a sample set of inputs covering all categories that
can cause the two programs to produce diﬀerent outputs.
The algorithm stops if the two models are equivalent. Oth-
erwise, RCADiff returns a set of inputs that cause the two
SFA models to produce diﬀerent output.
However, since these diﬀerences are obtained by compar-
ing the program models and not the actual programs, they
might contain false positives resulting from inaccurate mod-
els. To detect such cases, we verify all diﬀerences obtained
from the RCADiff call using the actual test programs. If any
input is found not to produce a diﬀerence in the implemen-
tations, then that input is used as a counterexample in order
to reﬁne the model through the UpdateModel call. Finally,
when a set of diﬀerences in the two models is veriﬁed to
contain only true positives, the algorithm returns the set of
corresponding inputs back to the user.
The astute reader may notice that, if no candidate dif-
ferences are found between the two models, the algorithm
terminates. For this reason, model initialization plays a sig-
niﬁcant role in our algorithm, since the initialized models
should be expressive enough in order to provide candidate
diﬀerences. It is interesting to point out that the candidate
diﬀerences do not have to be real diﬀerences.
4.2 Difference Analysis
Assume that we found and veriﬁed a number of inputs
that cause the two programs under test to produce diﬀerent
outputs. One fundamental question is whether we can clas-
sify these inputs in certain equivalence classes based on the
cause of the deviant behavior. We will now describe how
we can use the inferred SFAs in order to compute such a
classiﬁcation. Ideally, we would like to assign in two inputs
that cause a diﬀerence the same root cause if they follow
the same execution paths in the target programs. Since the
program source is unavailable, we trace the execution path
of the inputs in the respective SFA models.
RCADiﬀ algorithm. Given two SFAs M1 and M2, it is
Algorithm 2 Diﬀerence Categorization Algorithm
Require: M1, M2 are two SFA Models
function RCADiff(M1, M2)
Mprod ← ProductSFA(M1, M2)
S ← ∅
for (qi, qj) ∈ Qprod | l(qi) (cid:54)= l(qj) do
S ← S ∪ SimplePaths(Mprod, (qi, qj))
end for
return Path2Input(S)
end function
straightforward to compute their intersection by adapting
the classic DFA intersection algorithm [28]. Let Mprod =
(Q1 × Q2, (q0, q0),{(qi, qj) : qi ∈ F1 ∧ qj ∈ F2},P, ∆) be
the, minimal, product automaton of M1, M2. Notice ini-
tially, that the reason a diﬀerence is observed in the output
after processing an input in both SFAs is that the labels of
the states reached in the two machines are diﬀerent. This
motivates our deﬁnition of points of exposure.
Deﬁnition 2. Let Mprod be the intersection SFA of M1, M2
as deﬁned above. We deﬁne the set {(qi, qj)|(qi, qj) ∈ Qprod∧
qi ∈ Q1∧qj ∈ Q2∧l(q1) (cid:54)= l(q2)} to be the points of exposure
for the diﬀerences between M1, M2.
Intuitively, the points of exposure are the reasons the dif-
ferences in the programs are observed through the output of
programs. The path to a point of exposure encodes two dif-
ferent execution paths in machines M1 and M2 respectively
which, under the same input, end up in states producing
diﬀerent output. Thus, we say that any simple path to a
point of exposure is a root cause of a diﬀerence.
Deﬁnition 3. Let M1, M2 be two SFAs and Mprod be the
intersection of M1, M2. Let Qp ⊆ Qprod be the points of
exposure for Mprod. We say that the set of simple paths
∗→ qp|qp ∈ Qp} is the set of root causes for the
S = {q0
diﬀerences between M1 and M2.
Equipped with the set of paths our classiﬁcation algorithm
works as follows: Given two inputs causing a diﬀerence, we
ﬁrst reduce the path followed by each input into a simple
path, i.e. we remove all loops from the path. For example,
an input following the path q0 → q4 → q5 → q4 → q10 will
be reduced to the path q0 → q4 → q10. Afterwards, we
classify the two inputs in the same root cause if the simple
paths followed by the inputs are the same.
Algorithm 2 shows the pseudocode for the RCADiff algo-
rithm. The algorithm works by collecting all the distinct
root causes from the product automaton using the the Sim-
plePaths function call. This function accepts an SFA and
a target state and returns all simple paths from the ini-
tial state to the target state using a BFS search. After-
wards, each path is converted into a sample input through
the function Path2Input. This function works by selecting,
for each edge qi → qj in the path, a symbol α ∈ Σ such that
(qi, φ, qj) ∈ ∆ ∧ φ(α) = 1. Finally, these symbols are con-
catenated in order to form an input that exercise the given
path in the SFA.
4.3 Differentiating Program Sets
In this section, we describe how our original diﬀerential
testing framework can be generalized into a GetSetDiffer-
ences algorithm which works as follows:
Instead of get-
ting two programs as input, the GetSetDifferences algo-
rithm receives two sets of programs I1 = {P1, . . . , Pn} and
I2 = {P1, . . . , Pm}. Assume that the output of each pro-
gram is a bit b ∈ {0, 1}. The goal of the algorithm is to ﬁnd
a set of inputs S such that, the following condition holds:
∃b ∀P1 ∈ I1, P1(s) = b ∧ ∀P2 ∈ I2, P2(s) = 1 − b
While conceptually simple, this extension provides a num-
ber of nice applications. For example, consider the problem
of ﬁnding diﬀerences between the HTML/JavaScript parsers
of browsers and those of WAFs. While ﬁnding such diﬀer-
ences between a single browser and a WAF will provide us
with an evasion attack against the WAF, the GetSetDif-
ferences algorithm allows us to answer more sophisticated
questions such as: (i) Is there an evasion attack that will
bypass multiple diﬀerent WAFs? and (ii) Is there an eva-
sion attack that will work across diﬀerent browsers? Also,
as we describe in Section 4.4, this extension allows us to pro-
duce succinct ﬁngerprints for distinguishing between multi-
ple similar programs.
GetSetDiﬀerences Algorithm. We extend our basic Get-
Differences algorithm as follows: First, instead of initial-
izing two program models as before, we initialize the SFA