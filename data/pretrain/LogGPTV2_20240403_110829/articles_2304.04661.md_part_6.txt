trainingandonlineservingorinference.Anysystemevolution to predict switch failures in data center networks. [130]
betweentwoofflinetrainingcyclescouldcausepotentialissues collected data from SMART [131] and system-level signals,
and damage model performance. Thus, supporting online and proposed a hybrid of LSTM and random forest model
learning is critical to guarantee high performance in real for node failure prediction in cloud service system. [132]
production environments. developed a disk error prediction method via a cost-sensitive
ranking models. These methods target at the specific type of
failure prediction, and thus are limited in practice.
V. FAILUREPREDICTION
IncidentDetectionandRoot-CauseAnalysisofIncidentsare Challenges and Future Trends
more reactive measures towards mitigating the effects of any While conventional supervised learning for classification or
incident and improving service availability once the incident regression problems can be used to handle failure prediction,
has already occurred. On the other hand, there are other it needs to overcome the following main challenges. First,
proactive actions that can be taken to predict if any potential datasetsareusuallyveryimbalancedduetothelimitednumber
incident can happen in the immediate future and prevent it of failure cases. This poses a significant challenge to the
from happening. Failures in software systems are such kind prediction model to achieve high precision and high recall
of highly disruptive incidents that often start by showing simultaneously. Second, the raw signals are usually noisy,
symptoms of deviation from the normal routine behavior of not all information before incident is helpful. How to extract
the required system functions and typically result in failure omen features/patterns and filter out noises are critical to the
to meet the service level agreement. Failure prediction is one prediction performance. Third, it is common for a typical
such proactive task in Incident Management, whose objective system to generate a large volume of signals per minute,
is to continuously monitor the system health by analyzing the leading to the challenge to update prediction model in the
different types of system data (KPI metrics, logging and trace streaming way and handle the large-scale data with lim-
data) and generate early warnings to prevent failures from ited computation resources. Fourth, post-processing of failure
occurring.Consequently,inordertohandlethedifferentkinds prediction is very important for failure management system
of telemetry data sources, the task of predicting failures can to improve availability. For example, providing interpretable
be tailored to metric based and log based failure prediction. failure prediction can facilitate engineers to take appropriate
We describe these two in details in this section. action for it.
15
B. Logs based Incident Detection or ensemble of classifiers [93] or hidden semi-markov model
Like Incident Detection and Root Cause Analysis, Failure basedclassifier[139]overfeatureshandcraftedfromlogevent
Prediction is also an extremely complex task, especially in sequences or over random indexing based log encoding while
enterprise level systems which comprise of many distributed [140],[141]usesdeeprecurrentneuralmodelslikeLSTMover
but inter-connected components, services and micro-services semantic representations of logs. [142] predict and diagnose
interacting with each other asynchronously. One of the main failures through first failure identification and causality based
complexities of the task is to be able to do early detection filtering to combine correlated events for filtering through
of signals alluding towards a major disruption, even while association rule-mining method.
the system might be showing only slight or manageable
Failure Prediction in Heterogenous Systems
deviations from its usual behavior. Because of this nature of
Inheterogenoussystems,likelarge-scalecloudservices,es-
the problem, often monitoring the KPI metrics alone may not
peciallyindistributedmicro-serviceenvironment,outagescan
suffice for early detection, as many of these metrics might
be caused by heterogenous components. Most popular meth-
register a late reaction to a developing issue or may not be
ods utilize knowledge about the relationship and dependency
fine-grainedenoughtocapturetheearlysignalsofanincident.
between the system components, in order to predict failures.
System and software logs, on the other hand, being an all-
Amongst such systems, [143] constructed a Bayesian network
pervasive part of systems data continuously capture rich and
to identify conditional dependence between alerting signals
very detailed runtime information that are often pertinent to
extracted from system logs and past outages in offline setting
detecting possible future failures.
andusedgradientboostingtreestopredictfutureoutagesinthe
Thusvariousproactivelogbasedanalysishavebeenapplied
onlinesetting.[144]usesarankingmodelcombiningtemporal
in different industrial applications as a continuous monitoring
features from LSTM hidden states and spatial features from
task and have proved to be quite effective for a more fine-
RandomForesttorankrelationshipsbetweenfailureindicating
grained failure prediction and localizing the source of the
alerts and outages. [145] trains trace-level and micro-service
potential failure.It involves analyzingthe sequencesof events
level prediction models over handcrafted features extracted
in the log data and possibly even correlating them with other
fromtracelogstodetectthreecommontypesofmicro-service
data sources like metrics in order to detect anomalous event
failures.
patterns that indicate towards a developing incident. This is
typically achieved in literature by employing supervised or
semi-supervised machine learning models to predict future VI. ROOTCAUSEANALYSIS
failure likelihood by learning andmodeling the characteristics
Root-cause Analysis (RCA) is the process to conduct a
of historical failure data. In some cases these models can
series of actions to discover the root causes of an incident.
also be additionally powered by domain knowledge about the
RCA in DevOps focuses on building the standard process
intricaterelationshipsbetweenthesystems.Whilethistaskhas
workflowtohandleincidentsmoresystematically.WithoutAI,
notbeenexploredaspopularlyasLogAnomalyDetectionand
RCA is more about creating rules that any DevOps member
Root Cause Analysis and there are fewer public datasets and
can follow to solve repeated incidents. However, it is not
benchmark data, software and systems maintainance logging
scalable to create separate rules and process workflow for
data still plays a very important role in predicting potential
each type of repeated incident when the systems are large
future failures. In literature, generally the failure prediction
and complex. AI models are capable to process high volume
task over log data has been employed in broadly two types of
of input data and learn representations from existing incidents
systems - homogenous and heterogenous.
and how they are handled, without humans to define every
Failure Prediction in Homogenous Systems single details of the workflow. Thus, AI-based RCA has huge
In homogenous systems, like high-performance computing potential to reform how root cause can be discovered.
systems or large-scale supercomputers, this entails prediction Inthissection,wediscussaseriesofAI-basedRCAtopics,
of independent failures, where most systems leverage sequen- separetedbytheinputdatamodality:metric-based,log-based,
tial information to predict failure of a single component. trace-based and multimodal RCA.
Time-Series Modeling: Amongst homogenous systems,
[133], [134] extract system health indicating features from
A. Metric-based RCA
structuredlogsandmodeledthisastimeseriesbasedanomaly
forecasting problem. Similarly [135] extracts specific patterns
during critical events through feature engineering and build a Problem Definition
supervised binary classifier to predict failures. [136] converts With the rapidly growing adoption of microservices ar-
unstructured logs into templates through parsing and apply chitectures, multi-service applications become the standard
feature extraction and time-series modeling to predict surge, paradigm in real-world IT applications. A multi-service ap-
frequency and seasonality patterns of anomalies. plication usually contains hundreds of interacting services,
Supervised Classifiers Some of the older works predict making it harder to detect service failures and identify the
failures in a supervised classification setting using tradi- root causes. Root cause analysis (RCA) methods leverage the
tional machine learning models like support vector machines, KPImetricsmonitoredonthoseservicestodeterminetheroot
nearest-neighbor or rule-based classifiers [137], [93], [138], causeswhenasystemfailureisdetected,helpingengineersand
16
SREs in the troubleshooting process*. The key idea behind Topology or Causal Graph-based Analysis
RCA with KPI metrics is to analyze the relationships or The advantage of metric data analysis methods is the
dependencies between these metrics and then utilize these ability of handling millions of metrics. But most of them
relationships to identify root causes when an anomaly occurs. donâ€™t consider the dependencies between services in an ap-
Typically, there are two types of approaches: 1) identifying plication. The second type of RCA approaches leverages
the anomalous metrics in parallel with the observed anomaly such dependencies, which usually involves two steps, i.e.,
via metric data analysis, and 2) discovering a topology/causal constructingtopology/causalgraphsgiventheKPImetricsand
graph that represent the causal relationships between the domain knowledge, and extracting anomalous subgraphs or
services and then identifying root causes based on it. paths given the observed anomalies. Such graphs can either
be reconstructed from the topology (domain knowledge) of a
Metric Data Analysis certainapplication([149],[150],[151],[152])orautomatically
Whenananomalyisdetectedinamulti-serviceapplication, estimated from the metrics via causal discovery techniques
the services whose KPI metrics are anomalous can possibly ([153], [154], [155], [156], [157], [158], [159]). To identify
be the root causes. The first approach directly analyzes these the root causes of the observed anomalies, random walk (e.g.,
KPImetricstodeterminerootcausesbasedontheassumption [160],[156],[153]),page-rank(e.g.,[150])orothertechniques
thatsignificantchangesinoneormultipleKPImetricshappen can be applied over the discovered topology/causal graphs.
when an anomaly occurs. Therefore, the key is to identify When the service graphs (the relationships between the
whether a KPI metric has pattern or magnitude changes in a services) or the call graphs (the communications among the
look-backwindoworsnapshotofagivensizeattheanomalous services) are available, the topology graph of a multi-service
timestamp. application can be reconstructed automatically, e.g., [149],
Nguyen et al.[146], [147] propose two similar RCA meth- [150]. But such domain knowledge is usually unavailable or
odsbyanalyzinglow-levelsystemmetrics,e.g.,CPU,memory partially available especially when investigating the relation-
and network statistics. Both methods first detect abnormal shipsbetweentheKPImetricsinsteadofAPIcalls.Therefore,
behaviors for each component via a change point detection given the observed metrics, causal discovery techniques, e.g.,
algorithm when a performance anomaly is detected, and then [161], [162], [163] play a significant role in constructing
determine the root causes based on the propagation patterns the causal graph describing the causal relationships between
obtainedbysortingallcriticalchangepointsinachronological these metrics. The most popular causal discovery algorithm
order. Because a real-world multi-service application usually applied in RCA is the well-known PC-algorithm [161] due
have hundreds of KPI metrics, the change point detection to its simplicity and explainability. It starts from a complete
algorithmmustbeefficientandrobust.[146]providesanalgo- undirected graph and eliminates edges between the metrics
rithm by combining cumulative sum charts and bootstrapping via conditional independence test. The orientations of the
to detect change points. To identify the critical change point edges are then determined by finding V-structures followed
from the change points discovered by this algorithm, they use byorientationpropagation.SomevariantsofthePC-algorithm
aseparationlevelmetrictomeasurethechangemagnitudefor [164],[165],[166]canalsobeappliedbasedondifferentdata
each change point and extract the critical change point whose properties.
separationlevelvalueisanoutlier.Sincetheearliestanomalies Given the discovered causal graph, the possible root causes
may have propagated from their corresponding services to oftheobservedanomaliescanbedeterminedbyrandomwalk.
other services, the root causes are then determined by sorting A random walk on a graph is a random process that begins at
the critical change points in a chronological order. To further somenode,andrandomlymovestoanothernodeateachtime
improve root cause pinpointing accuracy, [147] develops a step. The probability of moving from one node to another is
newfaultlocalizationmethodbyconsideringbothpropagation defined in the the transition probability matrix. Random walk
patterns and service component dependencies. forRCAisbasedontheassumptionthatametricthatismore
Instead of change point detection, Shan et al.[148] devel- correlatedwiththeanomalousKPImetricsismorelikelytobe
opedalow-costRCAmethodcalled(cid:15)-Diagnosistodetectroot the root cause. Each random walk starts from one anomalous
causes of small-window long-tail latency for web services. (cid:15)- node corresponding to an anomalous metric, then the nodes
Diagnosis assumes that the root cause metrics of an abnormal visited the most frequently are the most likely to be the root
service have significantly changes between the abnormal and causes.Thekeyofrandomwalkapproachesistodeterminethe
normalperiods.Itappliesthetwo-sampletestalgorithmand(cid:15)- transition probability matrix. Typically, there are three steps
statisticsformeasuringsimilarityoftimeseriestoidentifyroot for computing the transition probability matrix, i.e., forward
causes. Inthe two-sampletest, onesample (normal sample)is step(probabilityofwalkingfromanodetooneofitsparents),
drawn from the snapshot during the normal period while the backward step (probability of walking from a node to one of
othersample(anomalysample)isdrawnduringtheanomalous itschildren)andselfstep(probabilityofstayinginthecurrent
period. If the difference between the anomaly sample and the node).Forexample,[153],[158],[159],[150]computesthese
normal sample are statistically significant, the corresponding probabilities based on the correlation of each metric with the
metrics of the samples are potential root causes. detected anomalous metrics during the anomaly period. But
correlation based random walk may not accurately localize
root cause [156]. Therefore, [156] proposes to use the partial
*A good survey for anomaly detection and RCA in cloud applications
[22] correlations instead of correlations to compute the transition
17
probabilities, which can remove the effect of the confounders problems, we lack benchmarks to evaluate RCA performance,
of two metrics. e.g., few public datasets with groundtruth root causes are
Besides random walk, other causal graph analysis tech- available,andmostpreviousworksuseprivateinternaldatasets
niques can also be applied. For example, [157], [155] find for evaluation. Although some multi-service application de-