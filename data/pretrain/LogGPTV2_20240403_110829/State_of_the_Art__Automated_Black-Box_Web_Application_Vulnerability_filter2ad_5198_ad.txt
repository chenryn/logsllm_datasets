60
8th
7th
6th
5th
4th
3rd
2nd
1st
Average
10%
20%
30%
40%
50%
60%
Conﬁg
0
0%
Figure 13. Server and Cryptographic Conﬁguration Vulnerability Detection Results Sorted by Scanner Rank in Category
the benign region within a  block caused false
positives in two scanners–one reporting the false positive in
a single URL and the other in 13 different URLs.
Figure 14 plots the number of false positives reported by
each scanner in sorted order for this category. For reference,
there are around 90 total conﬁrmed vulnerabilities in our
testbed. It is noteworthy that several scanners reported no
false positives, and that some of the scanners with low
false-positives also had among the highest vulnerability
detection rates. The two scanners with the highest number of
false positive, both with vulnerability detection rates among
the lowest, reported numerous accessible code backup ﬁles
where none existed. The worst performing scanner for false
positives also reported false ﬁle inclusion, SQL Injection,
IP disclosure, path disclosure, and forms accepting POST
parameters form GET requests. This scanner also clas-
siﬁes hidden form values as vulnerabilities, contradicting
established practices for CSRF prevention using hidden
form authentication tokens. Among all other scanners, the
only other false positives of note are a CSRF vulnerability
reported despite the presence of an authentication token, and
auto-complete being reported for a password ﬁeld where it
was actually turned-off.
Finally, some scanners emit general warnings when they
detect a potential vulnerability, such as a GET form method
or a form without hidden authentication ﬁelds, without actu-
ally pinpointing the URL of the offending forms. We counted
these as detections in our data-collection methodology, but,
given the general nature of these warnings, could have just
as easily listed them as false positives.
342
0
0
0
2
2
8
7
6
5
4
3
2
1
0
10
14
19
20
30
40
51
50
Figure 14. False Positive Count in Sorted Order By Scanner
VI. EXPERIMENTAL AND SCANNER USAGE
OBSERVATIONS
We have thus far focused primarily on the detection per-
formance of the scanners as a group of different vulnerability
classiﬁcations. In this section, we will remark on some by-
scanner characteristics, without making overall comparative
rankings of one product versus another.
We observed that no individual scanner was a top-
performer in every vulnerability classiﬁcation. Often, scan-
ners with a leading detection rate in one vulnerability
category lagged in other categories. For example, the leading
scanner in both the XSS and SQL Injection categories was
among the bottom three in detecting Session Management
vulnerabilities, while the leader for Session Vulnerabilities
lagged in XSS and SQLI. This leads us to believe that
scanner vendors may beneﬁt as a community from a cross-
vendor-pollination of ideas.
Reiterating brieﬂy from the false positive results, we did
ﬁnd that scanners with high detection rates were able to
effectively control false positives, and that scanners with
low detection rates could produce many false positives.
Additionally, there were reticent scanners that reported few
total vulnerabilities, making both detection rate and false
positive count low.
The remote versus local distinction in scanner architecture
makes for interesting choices for the scanner customer. In
our experimental experience, remote scanners were conve-
nient as they experienced no install and system compatibility
issues, offered portable user-interface and report-storage, ran
with complete automation after the initial test conﬁguration,
and did not consume internal network and system resources.
On the other hand, commercial enterprises may be concerned
with privacy and information disclosure issues resulting from
conducting scans over a public network. In response to
customer preferences, some vendors, such as Cenzic and
Rapid7, offer scanners both as a remote service and a local
software package. In our experiments, we observed that the
architecture (local versus remote) of the scanners did not
appear to be a factor in overall detection results.
We also wish to remark on the user experience of the
scanners, speciﬁcally regarding test automation. Most tools
offer a choice between interactive and automated scanning
modes, and we anticipate that, given the expected run-time
of the scanners, many users will select the automated mode.
On a particular tool, however, even the automated mode
requires user-interaction to dismiss javascript alert()
boxes, ironically inserted by the tool’s XSS test vectors. This
caused workﬂow disruption in our laboratory environment,
so we expect that it would carry over when scanning larger,
deployed applications.
Finally, we wish to note that while the vulnerability
detection rates reported in this paper are generally less
than 50%, this fact by itself should not be considered an
indictment against the usefulness of automated black-box
scanners. Black-box scanners may in fact prove to be very
useful components in security-auditing programs upon more
detailed consideration of factors such as cost and time saved
from manual review.
VII. RELATED WORK
Much regulatory and industry effort has been devoted
to vulnerability categorization. The Common Vulnerabilities
and Exposures database [13] (CVE) feed of the NVD,
sponsored by the US Dept. of Homeland Security, associates
each vulnerability in its database with a Common Weakness
Enumeration (CWE) category [11], which include but are not
limited to web application categories. Industry web applica-
tion security special interest groups OWASP [17] and WASC
[18] have published web vulnerability-speciﬁc classiﬁcations
in their Top Ten [9] and [10] projects respectively. WASC
has also published a report on web-vulnerability statistics
[19], with vulnerability and detection rate data sourced from
automated black-box scanner, manual black-box penetration
testing, and white-box security auditing vendors. The vul-
nerability statistics they report are supportive of our results,
but their self-reported detection rates are in general higher
than our rates, since manual white-box security audits, which
have higher reported detection rates, are also included in the
study.
In addition, NIST [7] and WASC [8] have published eval-
uation criteria for web application scanners. We consulted
these public categorizations and scanner evaluation guides
to ensure the comprehensiveness of our testbed. Our testbed
checks all of the recommendations in the NIST guide and
37 of the 41 ﬁrst-and-second-level testing capability listed
by WASC.
Almost all academic research on tools for web applica-
tion security has been source code analysis, with a focus
on detecting XSS and SQLI via information ﬂow, model-
ing checking analysis, or a combination thereof. Work by
Wassermann [20], Lam [21], Kie˙zun [22], Jovanovic [23],
and Huang [24] all fall into this category.
Kals et. al. [25] and McAllister et. al. [26] implemented
automated black box web vulnerability scanners, with the
former targeting SQLI and XSS vulnerabilities and the latter
utilizing user interactions to generate more effective test
cases targeting reﬂected and stored XSS. Maggi et. al. [27]
discuss techniques to reduce false positive counts in auto-
mated intrusion detection, which is applicable to black-box
scanning. Interesting open-source scanner projects include
W3AF [28] and Powerfuzzer [29], which we evaluated but
did not include in the study due to their lack of testing
for authentication and server vulnerabilities, and Nikto [30],
which in counterpoint to W3AF and PowerFuzzer focuses
on server vulnerabilities instead of user-input validation.
In terms of testbeds for black-box web application vul-
nerability scanners, there are a number of “vulnerability
demonstration sites”, such as WebGoat by OWASP [31],
Hacme Bank [32], and AltoroMutual [33], that offer vul-
nerability education for website developers as well as sales-
demonstration for scanner product capabilities. Due to their
well-known status and/or intended purpose, we did not
evaluate any of the scanners on these sites as we did not
view the site as independent testbeds. However, Suto [34]
has produced an interesting comparison of seven black-box
scanners by running the products against several of these
demonstration sites. Finally, Fonseca et. al. [35] evaluated
the XSS and SQLI detection performance of three anony-
mous commerical application via automated software fault-
injection methods.
VIII. CONCLUSION
We studied the vulnerabilities that current black-box scan-
ners aim to detect and their effectiveness in detecting these
vulnerabilities. Our survey of web-application vulnerabilities
in the wild shows that Cross-Site Scripting, SQL Injection,
other forms of Cross-Channel Scripting, and Information
Disclosure are the most prevalent classes of vulnerabilities.
343
Further, we found that black-box web application vulnera-
bility scanners do, in general, expend testing effort in rough
proportion to the vulnerability population in the wild. As
shown by our experimental results on previous versions
of popular applications and textbook cases of Cross-Site
Scripting and SQL Injection, black-box scanners are adept
at detecting straightforward historical vulnerabilities.
On the other hand, black-box scanner detection rates show
room for improvement in other classes of vulnerabilities,
such as advanced and second-order forms of XSS and
SQLI, other forms of Cross-Channel Scripting, Cross-Site
Request Forgery, and Malware Presence. Deﬁciencies in the
CSRF and Malware classiﬁcations, and possibly in XCS,
may simply be attributable to lack-of-emphasis in vendor
test suites. Low detection rates in advanced and second-
order XSS and SQLI may indicate more systematic ﬂaws,
such as insufﬁcient storage modeling in XSS and SQLI
detection. Indeed, multiple vendors conﬁrmed their difﬁculty
in designing tests which detect second-order vulnerabili-
ties. Although our data suggests room for improvement in
detecting vulnerabilities, the scanners we tested may have
signiﬁcant value to customers, when used systematically as
part of an overall security program.
The strongest research opportunities lie in detecting ad-
vanced and second-order forms of XSS and SQLI, because
these forms of vulnerabilities are prevalent (and will con-
tinue to be) and tools do not currently perform well, despite
signiﬁcant effort. There are several ways that scanner perfor-
mance might be improved in the “advanced” vulnerability
categories, which consist of attacks using novel and non-
standard keywords. A reactive approach is to develop more
nimble processes of converting newly discovered vulnera-
bilities into appropriate test vectors. A more foundational
approach could involve modeling application semantics in a
meaningful way.
For 2nd order XSS and SQLI vulnerabilities, one natural
research problem is to increase observability. The scanners
have difﬁculty conﬁrming that a script or code injection into
storage was successful and also may have trouble linking
a later observation with the earlier injection event. We
can anecdotally conﬁrm the latter statement, as one of the
tools succeeded in injecting a stored Javascript alert()
but
later failed to identify this as a stored XSS. Thus,
we believe that detection rates may be improved by better
tool understanding of the application database model. More
basic scanner modiﬁcations such as adding a second user
login for observing cross-account stored vulnerabilities and
better management of observational passes after the initial
injection pass should also improve detection results in these
categories.
As far as site coverage,
the low coverage results for
SilverLight, Flash and Java Applets and the false positives
triggered by the “benign” Javascript trap lead us to suggest
another area of improvement for black-box scanners: better
understanding of active content and scripting languages.
ACKNOWLEDGMENT
The authors would like to thank Acunetix, Cenzic, IBM,
McAfee, Qualys, and Rapid7 for their participation in this
study.
REFERENCES
[1] StrongWebmail CEO’s mail account hacked via XSS. ZDNet.
[Online]. Available: http://blogs.zdnet.com/security/?p=3514
[2] D. Litchﬁeld. SQL Injection and Data Security Breaches.
http://www.davidlitchﬁeld.com/blog/
[Online]. Available:
archives/00000001.htm
[3] Websites
Attacks.
//tinyurl.com/yfqauzo
of WHO and MI5 Hacked Using XSS
Spamﬁgher.com.
http:
[Online]. Available:
[4] Approved Scanning Vendors. Payment Card Industry
[Online]. Available: https:
Security Standards Council.
//www.pcisecuritystandards.org/pdfs/asv report.html
[5] VUPEN Security. [Online]. Available: http://www.vupen.com
[6] National Vulnerability Database. Dept. of Homeland Security
[Online]. Available:
National Cyber Security Division.
http://web.nvd.nist.gov
[7] Software Assurance Tools: Web Application Security Scanner
Functional Speciﬁcation, National Institute of Standards and
Technology Std., Rev. 1.0.
Application
[8] Web
tion
tium.
Web-Application-Security-Scanner-Evaluation-Criteria
Evalua-
Consor-
http://projects.webappsec.org/
Criteria. Web Application
[Online]. Available:
Scanner
Security
Security
[9] OWASP Top Ten Project. Open Web Application Security
[Online]. Available: http://www.owasp.org/index.
Project.
php/Category:OWASP Top Ten Project
[10] Web Security Threat Classiﬁcation. Web Application Security
Consortium. [Online]. Available: http://www.webappsec.org/
projects/threat/
[11] Common Weakness Enumeration. [Online]. Available: http:
//cwe.mitre.org
[12] H. Bojinov, E. Bursztein, and D. Boneh, “Xcs: cross channel
scripting and its impact on web applications,” in CCS ’09:
Proceedings of the 16th ACM conference on Computer and
communications security. New York, NY, USA: ACM, 2009,
pp. 420–431.
[13] Common Vulnerabilities and Exposures. [Online]. Available:
http://cve.mitre.org
[14] D. Kaminsky, “Black Ops of PKI,” BlackHat USA, August
2009.
[15] M. Marlinspike, “More Tricks For Defeating SSL,” BlackHat
USA, August 2009.
344
[16] E. V. Nava and D. Lindsay, “Our Favorite XSS Filters and
How to Attack Them,” BlackHat USA, August 2009.
[17] Open Web Application Security Project. [Online]. Available:
http://www.owasp.org
[18] Web Application Security Consortium. [Online]. Available:
http://www.wasc.org
[25] S. Kals, E. Kirda, C. Kruegel, and N. Jovanovic, “Secubat:
a web vulnerability scanner,” in WWW ’06: Proc. 15th Int’l
Conf. World Wide Web, 2006, pp. 247–256.
[26] S. Mcallister, E. Kirda, and C. Kruegel, “Leveraging user
interactions for in-depth testing of web applications,” in RAID
’08: Proc. 11th Int’l Symp. Recent Advances in Intrusion
Detection, 2008, pp. 191–210.
[19] Web Application Security Statistics. Web Application
[Online]. Available: http://projects.
Security Consortium.
webappsec.org/Web-Application-Security-Statistics
[27] F. Maggi, W. K. Robertson, C. Kr¨ugel, and G. Vigna, “Pro-
tecting a moving target: Addressing web application concept
drift,” in RAID, 2009, pp. 21–40.
[20] G. Wassermann and Z. Su, “Sound and precise analysis of
web applications for injection vulnerabilities,” SIGPLAN Not.,
vol. 42, no. 6, pp. 32–41, 2007.
[21] M. S. Lam, M. Martin, B. Livshits, and J. Whaley, “Securing
web applications with static and dynamic information ﬂow
tracking,” in PEPM ’08: Proceedings of
the 2008 ACM
SIGPLAN symposium on Partial evaluation and semantics-
based program manipulation. New York, NY, USA: ACM,
2008, pp. 3–12.
[22] A. Kie˙zun, P. J. Guo, K. Jayaraman, and M. D. Ernst,
“Automatic creation of SQL injection and cross-site script-
ing attacks,” in ICSE’09, Proceedings of the 30th Interna-
tional Conference on Software Engineering, Vancouver, BC,
Canada, May 20–22, 2009.
[23] N. Jovanovic, C. Kruegel, and E. Kirda, “Pixy: A static
analysis tool for detecting web application vulnerabilities
(short paper),” in 2006 IEEE Symposium on Security and
Privacy, 2006, pp. 258–263.
[Online]. Available: http:
//www.iseclab.org/papers/pixy.pdf
[24] Y.-W. Huang, F. Yu, C. Hang, C.-H. Tsai, D.-T. Lee, and S.-Y.
Kuo, “Securing web application code by static analysis and
runtime protection,” in WWW ’04: Proceedings of the 13th
international conference on World Wide Web. New York,
NY, USA: ACM, 2004, pp. 40–52.
[28] Web Application Attack and Audit Framework. [Online].
Available: http://w3af.sourceforge.net/
[29] Powerfuzzer.
com/
[Online]. Available: http://www.powerfuzzer.
[30] CIRT.net Nikto Scanner. [Online]. Available: http://cirt.net/
nikto2
[31] WebGoat Project. OWASP. [Online]. Available: http://www.
owasp.org/index.php/Category:OWASP WebGoat Project
[32] HacmeBank. McAfee Corp. [Online]. Available: http://www.
foundstone.com/us/resources/proddesc/hacmebank.htm
[33] AltoroMutual Bank. Watchﬁre Corp. [Online]. Available:
http://demo.testﬁre.net/
[34] Larry Suto. Analyzing the Accuracy and Time Costs
of Web Application Security Scanners.
[Online]. Avail-
able: http://ha.ckers.org/ﬁles/Accuracy and Time Costs of
Web App Scanners.pdf
[35] J. Fonseca, M. Vieira, and H. Madeira, “Testing and com-
paring web vulnerability scanning tools for sql injection and
xss attacks,” Paciﬁc Rim Int’l Symp. Dependable Computing,
IEEE, vol. 0, pp. 365–372, 2007.
345