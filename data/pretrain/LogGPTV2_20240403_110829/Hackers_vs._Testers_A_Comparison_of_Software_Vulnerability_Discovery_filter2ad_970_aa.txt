**Title: Hackers vs. Testers: A Comparison of Software Vulnerability Discovery Processes**

**Authors:**
- Daniel Votipka
- Rock Stevens
- Elissa M. Redmiles
- Jeremy Hu
- Michelle L. Mazurek

**Conference:**
2018 IEEE Symposium on Security and Privacy

**Abstract:**
Identifying security vulnerabilities in software is a critical task that requires significant human effort. Currently, vulnerability discovery is often the responsibility of software testers before release and white-hat hackers (often within bug bounty programs) afterward. This arrangement can be ad-hoc and far from ideal; for example, if testers could identify more vulnerabilities, software would be more secure at release time. Thus far, however, the processes used by each group—and how they compare to and interact with each other—have not been well studied. This paper takes a first step toward better understanding, and eventually improving, this ecosystem: we report on a semi-structured interview study (n=25) with both testers and hackers, focusing on how each group finds vulnerabilities, how they develop their skills, and the challenges they face. The results suggest that hackers and testers follow similar processes but achieve different results due largely to differing experiences and, therefore, different underlying knowledge of security concepts. Based on these results, we provide recommendations to support improved security training for testers, better communication between hackers and developers, and smarter bug bounty policies to motivate hacker participation.

**I. Introduction**
Software security bugs, also known as vulnerabilities, continue to be an important and expensive problem. Significant research has been directed toward preventing vulnerabilities from occurring in the first place, as well as toward automatically discovering vulnerabilities. However, these efforts remain limited, and human intelligence is often required to supplement automated tools [1]–[9]. For now, the job of finding vulnerabilities prior to release is often assigned to software testers who aim to root out all bugs—performance, functionality, and security—before release. Unfortunately, general software testers do not typically have the training or expertise necessary to find all security bugs, and many are released into the wild [10].

Consequently, expert freelancers known as "white-hat hackers" examine released software for vulnerabilities that they can submit to bug bounty programs, often aiming to develop sufficient credibility and skills to be contracted directly by companies for their expertise [11], [12]. Bug bounty programs offer "bounties" (e.g., money, swag, or recognition) to anyone who identifies a vulnerability and discloses it to the vendor. By tapping into the wide population of white-hat hackers, companies have seen significant benefits to product security, including higher numbers of vulnerabilities found and improvements in the expertise of in-house software testers and developers as they learn from the vulnerabilities reported by others [12]–[17].

This vulnerability-finding ecosystem has important benefits, but overall it remains fairly ad-hoc, and there is significant room for improvement. Discovering more vulnerabilities prior to release would save time, money, and company reputation; protect product users; and avoid the long, slow process of patch adoption [18]–[23]. Bug bounty markets, which are typically dominated by a few highly-active participants [13]–[16], lack cognitive diversity, which is specifically important to thoroughly vet software for security bugs [3], [12]. Bug bounty programs can also exhibit communication problems that lead to low signal-to-noise ratios [17]. Evidence suggests that simply raising bounty prices is not sufficient to address these issues [25], [26].

To improve this overall ecosystem, we must better understand how it works. Several researchers have considered the economic and security impact of bug bounty programs [16], [27]–[30]; however, little research has investigated the human processes of benign vulnerability finding. In this work, we take a first step toward improving this understanding. We performed 25 semi-structured interviews with software testers and white-hat hackers (collectively, practitioners), focusing on the process of finding vulnerabilities in software: why they choose specific software to study, what tools they use, how they develop the necessary skills, and how they communicate with other relevant actors (e.g., developers and peers).

We found that both testers and hackers describe a similar set of steps for discovering vulnerabilities. Their success in each step, however, depends on their vulnerability discovery experience, their knowledge of underlying systems, available access to the development process, and what motivates them to search for vulnerabilities.

Of these variables, practitioners report that experience—which differs greatly between testers and hackers—is most significant to success in vulnerability finding. Differences in experience stem primarily from the fact that hackers are typically exposed to a wider variety of vulnerabilities through a broad array of sources, including employment, hacking exercises, communication with peers, and prior vulnerability reports. On the other hand, we find that testers are typically exposed to only a narrow set of vulnerabilities through fewer sources, as testers primarily search for vulnerabilities in only a single code base, only read bug reports associated with that program, and only participate in small, internal hacking exercises, if any.

Access to the development process and motivation also differ notably between hackers and testers. While participants report that more experience is always better, their opinions on access and motivation are less straightforward: more access can help or hinder vulnerability finding depending on circumstances, and the relationship between motivation and success can be highly variable.

From these findings, we distill recommendations to improve human-driven vulnerability discovery for both populations.

**II. Related Work**
In this section, we review prior work in four key areas.

**A. Bug Identification Process**
Previous work has studied how different populations perform the task of bug identification. Aranda et al. studied how developers and testers found 10 performance, security, and functionality bugs in a production environment [31]. They reviewed all reporting artifacts associated with the bugs and interviewed the developers and testers who found and fixed them. They found that bugs were most commonly discovered through manual testing; close cooperation and verbal communication were key to helping developers fix bugs.

Fang et al. surveyed hackers who disclosed vulnerabilities in the SecurityFocus repository, asking how participants choose software to investigate, what tools they use, and how they report their findings [32], [33]. They found that hackers typically targeted software they were familiar with as users, predominantly preferred fuzzing tools to static analysis, and preferred full disclosure. Summers et al. studied problem-solving mental models through semi-structured interviews of 18 hackers [34]. They found that hackers require a high tolerance for ambiguity, as they seek to find problems that may or may not exist in a system they did not design. Additionally, Summers et al. observed that hackers rely on discourse with others or visualization techniques (i.e., mapping system semantics on a whiteboard) to deal with ambiguity and identify the most probable issues.

We expand on these prior studies by comparing white-hat hackers and testers specifically in the domain of security and including testers and hackers from multiple companies and bug bounty programs. Also, we thoroughly investigate participants' processes, communication about vulnerabilities and reporting strategies, skill development, and reasons for using specific tools.

**B. Tester and Hacker Characteristics**
Lethbridge et al. discuss the wide breadth of software testers' backgrounds, estimating that only 40% possess a computing-related education and a majority lack formal training in software engineering practices [35]. They recommend expanding interactive educational opportunities for testers to support closing gaps in essential knowledge. Relatedly, Bertolino et al. examined how testers can harness their domain-specific knowledge in a distributed fashion to find more bugs more quickly [36]. We expand on this previous work to provide the first exploration of how software testers currently learn and expand their knowledge of vulnerability discovery practices.

Al-Banna et al. focus on external security professionals, asking both professionals and those who hire them which indicators they believed were the most important to discern security expertise [37]. Similarly, Cowley interviewed 10 malware reverse engineering professionals to understand the necessary skills and define levels of professional development [38]. We borrow the concept of task analysis from this work to guide our interviews while expanding the scope of questions and comparing hackers to software testers.

Criminology research has also examined why some individuals who find vulnerabilities become cyber criminals, finding that although most hackers work alone, they improve knowledge and skills in part through mentoring by peers [11], [39], [40]. While we explicitly do not consider black-hat hackers, we build on these findings with further analysis of how hackers learn skills and create communities.

**C. Measurement of Bug Bounty Programs**
Several researchers have investigated what factors (e.g., money, probability of success) most influence participation and productivity in bug bounty programs. Finifter et al. studied the Firefox and Chrome bug bounty programs [16]. They found that a variable payment structure based on the criticality of the vulnerability led to higher participation rates and a greater diversity of vulnerabilities discovered as more researchers participated in the program. Maillart et al. studied 35 public HackerOne bounty programs, finding that hackers tend to focus on new bounty programs and that a significant portion of vulnerabilities are found shortly after the program starts [12]. The authors suggest that hackers are motivated to find "low-hanging fruit" (i.e., easy to discover vulnerabilities) as quickly as possible, because the expected value of many small payouts is perceived to be greater than for complex, high-reward vulnerabilities that might be "scooped" by a competitor.

While these studies suggest potential motivations for hacker behavior based on observed trends, we directly interview hackers about their motivations. Additionally, these studies do not explore the full decision process of bug bounty participants. This exploration is important because any effective change to the market needs to consider all the nuances of participant decisions if it hopes to be successful. Additionally, prior work does not compare hackers with software testers. This comparison is necessary, as it suggests ways to best train and allocate resources to all stakeholders in the software development lifecycle.

**D. Other Studies with Developers and Security Professionals**
Similarly to our work, many researchers have investigated the specific needs and practices of developers and other security experts to understand how to improve application and code security [41]. For example, researchers have focused on understanding how and why developers write (in)secure software [42]–[51] and have investigated the usability of static analysis tools for vulnerability discovery [3], [52]–[60], network defense and incident response [61]–[69], malware analysis [70], and corporate security policy development and adherence [71]–[78]. While these works investigate different topics and questions than the work presented here, they highlight the benefits of the approach taken in our research: studying how experts approach security.

**III. Methodology**
To understand the vulnerability discovery processes used by our target populations, we conducted semi-structured interviews with software testers and white-hat hackers (henceforth hackers for simplicity) between April and May 2017. To support rigorous qualitative results, we conducted interviews until new themes stopped emerging (25 participants) [79, pg. 113-115]. Because we interviewed more than the 12-20 participants suggested by qualitative research best practices literature, our work can provide strong direction for future quantitative work and generalizable design recommendations [80].

Below, we describe our recruitment process, the development and pre-testing of our interview protocol, our data analysis procedures, and the limitations of our work. This study was approved by our university’s Institutional Review Board (IRB).

**A. Recruitment**
Because software testers and hackers are difficult to recruit [31]–[33], we used three sources to find participants: software testing and vulnerability discovery organizations, public bug bounty data, and personal contacts.

**Related Organizations:**
To recruit hackers, we contacted the leadership of two popular bug bounty platforms and several top-ranked Capture-the-Flag (CTF) teams. We gathered CTF team contact information when it was made publicly available on CTFTime.org [82], a website that hosts information about CTF teams and competitions.

To reach software testers, we contacted the most popular Meetup [83] groups with "Software Testing" listed in their description, all the IEEE chapters in our geographical region, and two popular professional testing organizations: the Association for Software Testing [84] and the Ministry of Testing [85].

**Public Bug Bounty Data:**
We also collected publicly available contact information for hackers from bug bounty websites. One of the most popular bug bounty platforms, HackerOne [86], maintains profile pages for each of its members, which commonly include the hacker’s contact information. Additionally, the Chromium [87] and Firefox [88] public bug trackers provide the email addresses of anyone who has submitted a bug report. To identify reporters who successfully submitted vulnerabilities, we followed the process outlined by Finifter et al. by searching for specific security-relevant labels [16].

**Personal Contacts:**
We asked colleagues in related industries to recruit their co-workers. We also used snowball sampling (asking participants to recruit peers) at the end of the recruitment phase to ensure we had sufficient participation. This recruitment source accounts for three participants.

**Advertisement Considerations:**
We found that hackers were highly privacy-sensitive, and testers were generally concerned with protecting their companies’ intellectual property, complicating recruiting. To mitigate this, we carefully designed our recruiting advertisements and materials to emphasize the legitimacy of our research institution and to provide reassurance that participant information would be kept confidential and that we would not ask for sensitive details.

**Participant Screening:**
Due to the specialized nature of the studied populations, we asked all volunteers to complete a 20-question survey to confirm they had the necessary skills and experience. The survey assessed participants’ background in vulnerability discovery (e.g., number of security bugs discovered, percent of income from vulnerability discovery, programs they have participated in, types of vulnerabilities found) and their technical skills (e.g., development experience, reverse engineering, system administration). It also concluded with basic demographic questions. We drew these questions from similar surveys distributed by popular bug bounty platforms [13], [15]. We provide the full set of survey questions in Appendix A.

We selected participants to represent a broad range of vulnerability discovery experience, software specializations (i.e., mobile, web, host), and technical skills. When survey responses matched in these categories, we selected randomly. To determine the participants’ software specialization, we asked them to indicate the percent of vulnerabilities they discovered in each type of software. We deem the software type with the highest reported percentage the participant’s specialty. If no software type exceeded 40% of all vulnerabilities found, we consider the participant a generalist (i.e., they do not specialize in any particular software type).

**B. Interview Protocol**
We performed semi-structured, video teleconference interviews, which took between 40 and 75 minutes. All interviews were conducted by a single interviewer. Using a semi-structured interview guide, we explored the following topics:

- **Vulnerability Discovery Process:** How participants choose specific software to study, the tools they use, and the steps they follow.
- **Skill Development:** How participants develop the necessary skills for vulnerability discovery, including formal and informal learning.
- **Communication and Reporting:** How participants communicate with other relevant actors (e.g., developers and peers) and the strategies they use for reporting vulnerabilities.
- **Motivation and Challenges:** What motivates participants to search for vulnerabilities and the challenges they face.

The interview guide was pre-tested with a small group of participants to ensure clarity and comprehensiveness. Interviews were recorded and transcribed verbatim. Transcripts were then coded using thematic analysis to identify common themes and patterns.

**Limitations:**
Our study has several limitations. First, the sample size, while sufficient for qualitative research, may not be representative of the broader population of software testers and hackers. Second, the self-reported nature of the data may introduce biases. Finally, the focus on English-speaking participants limits the generalizability of the findings to non-English-speaking populations.