ﬁx-it phases, at which point the set of unique defects against
a submission is known. For each unique bug found against a
team’s submission, we subtract P points from its resilience
score; as such, it is non-positive, and the best possible re-
silience score is 0. For correctness bugs, we set P to M/2;
for crashes that violate memory safety we set P to M , and
for exploits and other security property failures we set P to
2M .
2.2.2 Break-it scores
Our primary goal with break-it teams is to encourage
them to ﬁnd as many defects as possible in the submitted
software, as this would give greater conﬁdence in our assess-
ment that one build-it team’s software is of higher quality
than another’s. While we are particularly interested in obvi-
ous security defects, correctness defects are also important,
as they can have non-obvious security implications.
After the break-it phase, a break-it team’s score is the
summed value of all defects they have found, using the above
P valuations. After the ﬁx-it phase, this score is reduced. In
particular, each of the N break-it teams’ scores that identi-
ﬁed the same defect are adjusted to receive P/N points for
that defect, splitting the P points among them.
Through a combination of requiring concrete test cases
and scoring, BIBIFI encourages break-it teams to follow the
spirit of the competition. First, by requiring them to provide
test cases as evidence of a defect or vulnerability, we ensure
they are providing useful bug reports. By providing 4× more
points for security-relevant bugs than for correctness bugs,
we nudge break-it teams to look for these sorts of ﬂaws, and
to not just focus on correctness issues. (But a diﬀerent ratio
might work better; see §2.3.2.) Because break-it teams are
limited to a ﬁxed number of test cases per submission, and
because they could lose points during the ﬁx-it phase for
submitting test cases that could be considered “morally the
same,” break-it teams are encouraged to submit tests that
demonstrate diﬀerent bugs. Limiting per-submission test
cases also encourages examining many submissions. Finally,
because points for defects found by other teams are shared,
break-it teams are encouraged to look for hard-to-ﬁnd bugs,
rather than just low-hanging fruit.
2.3 Discussion
The contest’s design also aims to enable scalability by
reducing work on contest organizers.
In our experience,
BIBIFI’s design succeeds at what it sets out to achieve, but
is not perfect. We close by discussing some limitations.
2.3.1 Minimizing manual effort
Once the contest begins, manual eﬀort by the organizers
is, by design, limited. All bug reports submitted during the
break-it phase are automatically judged by the oracle; orga-
nizers only need to vet any bug reports against the oracle
itself. Organizers may also need to judge accusations by
breakers of code obfuscation by builders. Finally, organizers
must judge whether submitted ﬁxes address a single defect;
this is the most time-consuming task.
It is necessary be-
cause we cannot automatically determine whether multiple
bug reports against one team map to the same software de-
fect. Instead, we incentivize build-it teams to demonstrate
overlap through ﬁxes; organizers manually conﬁrm that each
ﬁx addresses only a single defect, not several.
Previewing some of the results presented later, we can con-
ﬁrm that the design works reasonably well. For example, as
detailed in Table 4, 68 teams submitted 24,796 test cases in
our Spring 2015 contest. The oracle auto-rejected 15,314 of
these, and build-it teams addressed 2,252 of those remaining
with 375 ﬁxes, a 6× reduction. Most conﬁrmations that a
ﬁx truly addressed a single bug took 1–2 minutes each. Only
30 of these ﬁxes were rejected. No accusations of code ob-
fuscation were made by break-it teams, and few bug reports
were submitted against the oracle. All told, the Spring 2015
contest was successfully managed by one full-time person,
with two others helping with judging.
2.3.2 Limitations
While we believe BIBIFI’s structural and scoring incen-
tives are properly designed, we should emphasize several lim-
itations.
First, there is no guarantee that all implementation de-
fects will be found. Break-it teams may lack the time or
skill to ﬁnd problems in all submissions, and not all submis-
sions may receive equal scrutiny. Break-it teams may also
act contrary to incentives and focus on easy-to-ﬁnd and/or
duplicated bugs, rather than the harder and/or unique ones.
Finally, break-it teams may ﬁnd defects that the BIBIFI in-
frastructure cannot automatically validate, meaning those
defects will go unreported. However, with a large enough
pool of break-it teams, and suﬃciently general defect vali-
dations automation, we still anticipate good coverage both
in breadth and depth.
Second, builders may fail to ﬁx bugs in a manner that is
in their best interests. For example, in not wanting to have
a ﬁx rejected as addressing more than one conceptual defect,
teams may use several speciﬁc ﬁxes when a more general ﬁx
would have been allowed. Additionally, teams that are out
of contention for prizes may simply not participate in the
ﬁx-it phase.5 We observed this behavior for our contests, as
described in §4.5. Both actions decrease a team’s resilience
score (and correspondingly increase breakers’ scores). We
can mitigate these issues with suﬃciently strong incentives,
e.g., by oﬀering prizes to all participants commensurate with
their ﬁnal score, rather than oﬀering prizes only to the top
scorers.
Finally, there are several design points in the problem def-
inition that may skew results. For example, too few correct-
ness tests may leave too many correctness bugs to be found
during break-it (distracting break-it teams’ attention from
security issues); too many correctness tests may leave too
few (meaning teams are diﬀerentiated insuﬃciently by gen-
eral bug-ﬁnding ability). Scoring prioritizes security prob-
lems 4 to 1 over correctness problems, but it is hard to say
5Hiding scores during the contest might help mitigate this, but
would harm incentives during break-it to go after submissions
with no bugs reported against them.
Figure 1: Overview of BIBIFI’s implementation.
what ratio makes the most sense when trying to maximize
real-world outcomes; both higher and lower ratios could be
argued. Finally, performance tests may fail to expose im-
portant design trade-oﬀs (e.g., space vs. time), aﬀecting the
ways that teams approach maximizing their ship scores. For
the contests we report in this paper, we are fairly comfort-
able with these design points. In particular, our earlier con-
test [31] prioritized security bugs 2-to-1 and had fewer in-
teresting performance tests, and outcomes were better when
we increased the ratio.
2.3.3 Discouraging collusion
BIBIFI contestants may form teams however they wish,
and may participate remotely. This encourages wider partic-
ipation, but it also opens the possibility of collusion between
teams, as there cannot be a judge overseeing their communi-
cation and coordination. There are three broad possibilities
for collusion, each of which BIBIFI’s scoring discourages.
First, two break-it teams could consider sharing bugs they
ﬁnd with one another. By scaling the points each ﬁnder of
a particular bug obtains, we remove incentive for them to
both submit the same bugs, as they would risk diluting how
many points they both obtain.
The second class of collusion is between a build-it team
and a break-it team, but neither have incentive to assist
one another. The zero-sum nature of the scoring between
breakers and builders places them at odds with one another;
revealing a bug to a break-it team hurts the builder, and not
reporting a bug hurts the breaker.
Finally, two build-it teams could collude, for instance by
sharing code with one another. It might be in their inter-
ests to do this in the event that the competition oﬀers prizes
to two or more build-it teams, since collusion could obtain
more than one prize-position. We use judging and auto-
mated tools (and feedback from break-it teams) to detect if
two teams share the same code (and disqualify them), but it
is not clear how to detect whether two teams provided out-
of-band feedback to one another prior to submitting code
(e.g., by holding their own informal “break-it” and “ﬁx-it”
stages). We view this as a minor threat to validity; at the
surface, such assistance appears unfair, but it is not clear
that it is contrary to the goals of the contest, that is, to
develop secure code.
2.4
Implementation
Figure 1 provides an overview of the BIBIFI implementa-
tion. It consists of a web frontend, providing the interface to
both participants and organizers, and a backend for testing
builds and breaks. Two key goals of the infrastructure are
BIBIFI InfrastructureContestWebsiteSurveysScoreboardBreak-it subsContest DatabaseBuild-it subsMetadataGit ListenerTesterAmazon EC2Contest VMBuildTestBenchmarkOracleBuild-it &Break-itTeamsOrganizersGit ReposParticipantssecurity—we do not want participants to succeed by hacking
BIBIFI itself—and scalability.
Web frontend. Contestants sign up for the contest through
our web application frontend, and ﬁll out a survey when do-
ing so, to gather demographic and other data potentially rel-
evant to the contest outcome (e.g., programming experience
and security training). During the contest, the web appli-
cation tests build-it submissions and break-it bug reports,
keeps the current scores updated, and provides a workbench
for the judges for considering whether or not a submitted ﬁx
covers one bug or not.
To secure the web application against unscrupulous par-
ticipants, we implemented it in ∼11,000 lines of Haskell us-
ing the Yesod [39] web framework backed by a PostgreSQL
[29] database. Haskell’s strong type system defends against
use-after-free, buﬀer overrun, and other memory safety-based
attacks. The use of Yesod adds further automatic protec-
tion against various attacks like CSRF, XSS, and SQL in-
jection. As one further layer of defense, the web applica-
tion incorporates the information ﬂow control framework
LMonad [26], which is derived from LIO [34], in order to
protect against inadvertent information leaks and privilege
escalations. LMonad dynamically guarantees that users can
only access their own information.
Testing backend. The backend infrastructure is used dur-
ing the build-it phase to test for correctness and perfor-
mance, and during the break-it phase to assess potential
It consists of ∼5,100 lines of Haskell code
vulnerabilities.
(and a little Python).
To automate testing, we require contestants to specify a
URL to a Git [17] repository hosted on either Github or
Bitbucket, and shared with a designated bibifi username,
read-only. The backend “listens” to each contestant reposi-
tory for pushes, upon which it downloads and archives each
commit. Testing is then handled by a scheduler that spins
up an Amazon EC2 virtual machine which builds and tests
each submission. We require that teams’ code builds and
runs, without any network access, in an Ubuntu Linux VM
that we share in advance. Teams can request that we in-
stall additional packages not present on the VM. The use of
VMs supports both scalability (Amazon EC2 instances are
dynamically provisioned) and security (using fresh VM in-
stances prevents a team from aﬀecting the results of future
tests, or of tests on other teams’ submissions).
All qualifying build-it submissions may be downloaded
by break-it teams at the start of the break-it phase. As
break-it teams identify bugs, they prepare a (JSON-based)
ﬁle specifying the buggy submission along with a sequence
of commands with expected outputs that demonstrate the
bug. Break-it teams commit and push this ﬁle (to their Git
repository). The backend uses the ﬁle to set up a test of the
implicated submission to see if it indeed is a bug.
3. CONTEST PROBLEMS
This section presents the two programming problems we
developed for the contests held during 2015, including prob-
lem-speciﬁc notions of security defect and how breaks ex-
ploiting such defects are automatically judged.
3.1 Secure log (Spring 2015)
The secure log problem was motivated as support for an
art gallery security system. Contestants write two programs.
The ﬁrst, logappend, appends events to the log; these events
indicate when employees and visitors enter and exit gallery
rooms. The second, logread, queries the log about past
events. To qualify, submissions must implement two basic
queries (involving the current state of the gallery and the
movements of particular individuals), but they could im-
plement two more for extra points (involving time spent in
the museum, and intersections among diﬀerent individuals’
histories). An empty log is created by logappend with a
given authentication token, and later calls to logappend and
logread on the same log must use that token or the requests
will be denied.
A canonical way of implementing the secure log is to treat
the authentication token as a symmetric key for authen-
ticated encryption, e.g., using a combination of AES and
HMAC. There are several tempting shortcuts that we antic-
ipated build-it teams would take (and that break-it teams
would exploit). For instance, one may be tempted to en-
crypt and sign individual log records as opposed to the en-
tire log, thereby making logappend faster. But this could
permit integrity breaks that duplicate or reorder log records.
Teams may also be tempted to implement their own encryp-
tion rather than use existing libraries, or to simply sidestep
encryption altogether. §5 reports several cases we observed.
A submission’s performance is measured in terms of time
to perform a particular sequence of operations, and space
consumed by the resulting log. Correctness (and crash) bug
reports comprise sequences of logread and/or logappend
operations with expected outputs (vetted by the oracle).
Security is deﬁned by privacy and integrity: any attempt
to learn something about the log’s contents, or to change
them, without the use of the logread and logappend and
the proper token should be disallowed. How violations of
these properties are speciﬁed and tested is described next.
Privacy breaks. When providing a build-it submission to
the break-it teams, we also included a set of log ﬁles that
were generated using a sequence of invocations of that sub-
mission’s logappend program. We generated diﬀerent logs
for diﬀerent build-it submissions, using a distinct command
sequence and authentication token for each. All logs were
distributed to break-it teams without the authentication to-
ken; some were distributed without revealing the sequence
of commands (the “transcript”) that generated them. For
these, a break-it team could submit a test case involving
a call to logread (with the authentication token omitted)
that queries the ﬁle. The BIBIFI infrastructure would run
the query on the speciﬁed ﬁle with the authentication token,
and if the output matched that speciﬁed by the breaker, then
a privacy violation is conﬁrmed.
Integrity breaks. For about half of the generated log ﬁles
we also provided the transcript of the logappend operations
(sans auth token) used to generate the ﬁle. A team could
submit a test case specifying the name of the log ﬁle, the
contents of a corrupted version of that ﬁle, and a logread
query over it (without the authentication token). For both
the speciﬁed log ﬁle and the corrupted one, the BIBIFI in-
frastructure would run the query using the correct authenti-
cation token. An integrity violation is detected if the query
command produces a non-error answer for the corrupted log
that diﬀers from the correct answer (which can be conﬁrmed
against the transcript using the oracle).
This approach to determining privacy and integrity breaks
has the drawback that it does not reveal the source of the
issue, only that there is (at least) one. As such, we cannot
automatically tell two privacy or two integrity breaks apart.
We sidestep this issue by counting only up to one integrity
break and one privacy break against the score of each build-
it submission, even if there are multiple defects that could
be exploited to produce privacy/integrity violations.
3.2 Securing ATM interactions (Fall 2015)
The ATM problem asks builders to construct two com-
municating programs: atm acts as an ATM client, allowing
customers to set up an account, and deposit and withdraw
money, while bank is a server that processes client requests,
tracking bank balances. atm and bank should only permit
a customer with a correct card ﬁle to learn or modify the
balance of their account, and only in an appropriate way
(e.g., they may not withdraw more money than they have).
In addition, atm and bank should only communicate if they
can authenticate each other. They can use an auth ﬁle for
this purpose; it will be shared between the two via a trusted
channel unavailable to the attacker.6 Since the atm is com-
municating with bank over the network, a “man in the mid-
dle” (MITM) could observe and modify exchanged messages,
or insert new messages. The MITM could try to compromise
security despite not having access to auth or card ﬁles.
A canonical way of implementing the atm and bank pro-
grams would be to use public key-based authenticated and
encrypted communications. The auth ﬁle could be used as
the bank’s public key to ensure that key negotiation initi-
ated by the atm is with the bank and not the MITM. When
creating an account, the card ﬁle should be a suitably large
random number, so that the MITM is unable to feasibly pre-
dict it. It is also necessary to protect against replay attacks
by using nonces or similar mechanisms. As with the secure
log, a wise approach would be use a library like OpenSSL to
implement these features. Both good and bad implementa-
tions we observed in the competition are discussed further
in §5.
Build-it submissions’ performance is measured as the time
to complete a series of benchmarks involving various atm/bank
interactions.7 Correctness (and crash) bug reports comprise
sequences of atm commands where the targeted submission
produces diﬀerent outputs than the oracle (or crashes). Se-
curity defects are speciﬁed as follows.
Integrity breaks. Integrity violations are demonstrated us-
ing a custom MITM program that acts as a proxy: It listens
on a speciﬁed IP address and TCP port,8 and accepts a
connection from the atm while connecting to the bank. The
MITM program can thus observe and/or modify commu-
nications between atm and bank, as well as drop messages
or initiate its own. We provided a Python-based proxy as
6In a real deployment, this might be done by “burning” the auth
ﬁle into the ATM’s ROM prior to installing it.
7This transcript was always serial, so there was no direct moti-
vation to support parallelism for higher throughput.
8All submissions were required to communicate via TCP.
a starter MITM: It sets up the connections and forwards