r
h
T
d
a
e
R
100
80
60
40
20
0
diskmap
FreeBSD aio(4)
FreeBSD pread(2)
0.5
1
2
4
8
16
I/O Size (KBytes)
F
D
C
1
0.8
0.6
0.4
0.2
0
32
64
128
diskmap
aio(4)
0
100
200
IO Latency (µs)
300
400
Figure 8: Read throughput, diskmap vs. aio(4) vs. pread(2)
Figure 9: diskmap vs. aio(4) - I/O latency, read size: 512 bytes,
I/O window: 128 requests
dedicated diskmap ioctl to indicate the disks and queue pairs
that should be attached, as well as the number of diskmap buffers
required. This functionality is abstracted within a single libnvme
library call (see table 1). When an application calls nvme_read
or nvme_write, the library is responsible for translating the I/O
request for certain disk blocks into the corresponding NVMe com-
mands and enqueuing them in the disk submission queue. The appli-
cation layer then invokes a system call to update the relevant disk
doorbell and initiate processing of the pending I/O commands.
Unlike their POSIX equivalents which block until the I/O is com-
pleted, libnvme facilitates a non-blocking, event-driven model. With
each I/O request, the application specifies a callback function which
will be invoked upon I/O completion. A high-level I/O request might
need to be split into several low-level NVMe commands, and this
complicates the handling of out-of-order completion. Libnvme hides
this complexity, and only invokes the application-specified callback
function when all the in-flight NVMe commands that comprise that
particular transfer have completed.
Diskmap enforces memory safety by taking advantage of the
IOMMU on newer systems. When an application requests the at-
tachment of a datapath queue pair and a number of diskmap buffers,
the kernel maps the relevant shared memory to the PCIe device-
specific IOMMU page table. Since all the buffers are statically
pre-allocated by the kernel upon initialization, there is no need to
dynamically update the IOMMU page table with transient mappings
and unmappings, which would otherwise significantly affect per-
formance [3, 19]. Diskmap could also operate in an unsafe manner
using direct physical memory addresses if the IOMMU is disabled.
In both cases, userspace remains unaware of the change, and there
is no special handling required either in the libnvme library or the
application itself.
3.1.3 Diskmap Performance
Before we integrate diskmap into our video server, we wish to un-
derstand how well it performs. Figure 8 shows the read throughput
obtained using diskmap for a range of I/O request sizes. A sin-
gle diskmap thread binds to four NVMe disks, fills their submis-
sion queues, and we measure throughput. We compare diskmap
against FreeBSD pread , and aio (also single threads). Aio is a na-
tive FreeBSD interface which uses kqueue [17] and kevent to allow
asynchronous I/O requests to be initiated by userspace. It is highly
optimized, and allows I/O request batching with a single system
call to increase performance. When an NVMe interrupt indicates
competion, userspace is notified via kqueue.
Although aio performs well for large reads, it is less stellar with
smaller requests. Diskmap exhibits much higher throughput than aio
unless request sizes are at least 64KB in size. With diskmap there is
a sweet spot around 16KB—here, performance is close to the limits
of the hardware, but the transfers are still small enough that they are
comparable to today’s default TCP initial window size. This makes
diskmap an excellent fit for our video server.
Figure 9 shows latency when using 512 byte read requests, and
an I/O window of 128 requests. Such small requests stress the stack;
despite this diskmap exhibits very low latency. Finally, Figure 6 was
gathered using diskmap, and shows that both low latency and high
throughput can be obtained simultaneously.
3.1.4 To batch or not to batch
One technique often used to improve throughput is batching. Modern
NICs are able to achieve very high packet rates even with small
transfer units; for example more than 40Mpps is possible with 64
byte packets [9]. For the CPU to keep up, batching is required
to amortize the system call overhead (e.g., in the case of netmap)
and the cost of accessing/updating the device doorbell registers.
With today’s NVMe disks the situation is different: the minimum
transfer unit of these devices typically ranges from 512 to 4096
bytes, and the IOPS rates that can be achieved are much lower than
the NIC equivalent packet rates. We find that the CPU can fill the
disk firmware pipeline and saturate the device for the whole range
of supported I/O lengths per operation without needing to batch
requests. In situations where the CPU is nearly saturated though,
batching is still efficient in saving CPU cycles by amortizing the
system call overhead.
3.2 Network Stack Integration
In a conventional configuration the work to send persistent files from
disks to the network is distributed across many different subsystems
that operate asynchronously and are loosely co-scheduled. In con-
trast, we seek tighter control of the execution pipeline. Scheduling
work from a single thread, we can come closer to a process-to-
completion ideal, minimizing the lifespan of data transfers across
all the layers of the stack; from disk to application, from application
to the network stack, and finally to the NICs. This should reduce
pressure on the LLC and take advantage of contemporary microar-
chitectural properties such as Intel’s Data Direct IO (DDIO) [10].
To take advantage of diskmap in the network fast path, we need
to embed the networking code in the same process thread. Several
217
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ilias Marinos, Robert N.M. Watson, Mark Handley, and Randall R. Stewart
use an on-demand mechanism to fetch data from disks and transmit
them to the network. We have found that to get the peak throughput
from NVMe disks, I/O requests larger than 8KB need to be issued.
Seeking to optimize for the typical case and achieve the highest
throughput from the NVMe disks, we have chosen to delay the I/O
requests for a particular flow until the received ACKs inflate the
space in the TCP congestion window to a reasonably high value—
10*MSS in our implementation. Of course, there are cases where
this mechanism cannot be applied: for example, if a TCP connection
experiences a retransmit timeout, or the effective window is smaller
than this high-watermark value and all sent data is acknowledged,
then we fall back issuing smaller I/O requests.
As our stack does not buffer in-flight data sent by TCP, retrans-
mitted data must be re-fetched from disk. We use the TCP sequence
number offset of the lost segment to decide which data to re-fetch
and retransmit. When encrypted traffic is considered, it is worth not-
ing that this socket-buffer-free approach fits well with ciphersuites
like AES GCM [28] which do not require interpacket dependencies
to work; instead the GCM counter can be easily derived from the
TCP sequence numbers, including for retransmissions.
Our system prototype, Atlas, does not implement a sophisticated
filesystem: disks are treated as flat namespaces, and files are laid out
in consecutive disk blocks.
4 EVALUATION
We saw in Section 2.2 how the Netflix stack outperforms stock
FreeBSD, both on encrypted workloads and with plaintext when the
buffer cache hit ratio is low. However, profiling of the Netflix stack
indicated potential inefficiencies that appeared to be inherent, and
motivated our design of Atlas. We now wish to see to what extent
our hypothesis was correct regarding the merits of a special purpose
stack, and the need to integrate storage into the TCP control loop. We
will compare the performance of Atlas against a Netflix-optimized
setup, using contemporary hardware.
Our testbed consists of four machines; one server, two clients,
and one middlebox to allow us to emulate realistic network round
trip times. The test systems are connected via a 40GbE cut-through
switch. Our video server is equipped with an Intel Xeon E5-2667-
v3 8-core CPU, 128GB RAM, two dual-port Chelsio T580 40GbE
NIC adapters, and four Intel P3700 NVMe disks with 800GB ca-
pacity each. Our two systems emulating large numbers of clients
are equipped with Intel Xeon E5-2643-v2 6-core CPUs, and 64GB
RAM. One uses a dual-port Chelsio T580 40GbE, while the other
has an Intel XL710 40GbE controller. Finally, the middlebox has a
6-core Intel E5-2430L-v2 and 64GB RAM.
Atlas runs on FreeBSD 12-CURRENT; for Netflix we use the
Netflix production release which is also based on FreeBSD 12, but
also includes all the Netflix optimization patches, including those
mentioned in §2.1, and tuning. The two client systems run Linux
4.4.8, and finally the middlebox runs FreeBSD 12.
We wish to model how a video streaming server would perform
in the wild, but with all our machines connected to the same LAN
using 40GbE links, the round-trip times are on the order of a few
microseconds. This is not representative of the RTTs seen by produc-
tion video servers, and would distort TCP behavior. To emulate more
realistic latencies, we employ a middlebox which adds latency to
Figure 10: Atlas high-level control flow.
userspace network stacks have been presented recently, demonstrat-
ing dramatic improvements both in terms of latency and through-
put [15, 16, 18]. We started from the Sandstorm [18] implementation,
and modified it accordingly to build our network stack. Sandstorm
was originally designed to serve small static objects, typical of web
traffic, from the main memory. Although we leverage several of
Sandstorm’s original design ideas, modifications were necessary
because the requirements are fundamentally different:
• Unlike typical web traffic workloads, which mostly consist of
short-lived connections, we want to optimize for long-lived
HTTP-persistent connections used by video streaming.
• Content served by a video streamer does not fit in main memory,
rendering some of Sandstorm’s optimizations such as multiple
packet replicas and content pre-segmentation inapplicable.
Given the workload characteristics, doing network packet segmen-
tation or calculating the checksums of large data in software would
be a performance bottleneck. Instead we leverage NIC hardware
support for TCP Segmentation Offload (TSO). This required modifi-
cations in netmap’s NIC driver, in particular for Chelsio T5 40GbE
adapters, and in Sandstorm’s core TCP protocol implementation.
As the movie catalog size is significantly larger than RAM, a
video streamer needs to serve ~90% of requests from disk in the
typical case (see §2.1). This means that most of the time the OS
buffer cache does not really act as a cache anymore; it merely serves
as a temporary buffer pool to store the data that are enqueued to
socket buffers. At the same time this comes with considerable over-
head associated with nominally being a cache, including pressure
in the VM subsystem, lock contention and so forth. In Atlas, we
completely remove the buffer cache as an interface between the
storage and network stacks.
To integrate the storage and network stacks, we implemented a
mechanism similar to the conventional stack’s sendfile. Upon
the reception of an HTTP GET request, the application layer web
server issues a tcpip_sendfile library call. This call instructs
the network stack to attach the in-memory object representation
of a persistent file to the particular connection’s TCB. After this
point, the network stack is responsible for fetching the data from the
disk and transmitting them to the network (see Fig. 10). It invokes
a callback to the application layer only when everything has been
sent, or some other critical event has been encountered such as the
connection being closed by the remote host.
Given the low latency of NVMe disks compared to WAN RTTs,
rather than the read-ahead approach used by a conventional stack, we
218
kernel user  NVME diskSQCQNICRXTX12libnmiolibtcpiplibnvmeapp345678Disk|Crypt|Net: rethinking the stack for high-performance video streaming
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
T
t
e
N
80
60
40
20
0
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
t
y
r
o
m
e
M
150
100
50
0
t
u
p
h
g
u
o
r
h
t
t
e
N
/
d
a
e
r
m
e
M
3
2
1
0
Netflix 0% BC
Netflix 100% BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
)
%
(
n
o
i
t
a
z
i
l
i
t
u
U
P
C
800
600
400
200
0
Netflix 0% BC
Netflix 100% BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(a) Network throughput (Error bars indicate the 95% CI)
(b) CPU utilization (Average)
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(c) Memory READ
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h