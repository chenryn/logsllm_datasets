(𝑏0, . . . , 𝑏𝑄−1) of |𝑥|, as well as an additional bit 𝑏𝑄 denoting neg-
ative (0) or positive (1), as the auxiliary input to compute ReLU.
Following the techniques in the literature [37], the protocol checks
(1) The auxiliary inputs are binary: 𝑏𝑖(𝑏𝑖 − 1) = 0 ∀𝑖 = 0, . . . , 𝑄;
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2977(2) They are the bit decomposition of |𝑥|: 𝑏𝑄(𝑥 −𝑄−1
(1 − 𝑏𝑄)(𝑥 +𝑄−1
𝑄′−1
𝑖=0 𝑏𝑖 · 2𝑖) +
(3) With the bit-decomposition of 𝑥, the protocol computes the
result of ReLU together with the truncation by keeping only
𝑄′ most significant bits to avoid overflow: ReLU(𝑥) = 𝑏𝑄 ·
𝑖=0 𝑏𝑖+𝑄−𝑄′2𝑖.
𝑖=0 𝑏𝑖 · 2𝑖) = 0.
Computing composition of max pooling and ReLU efficiently.
A pooling layer is applied after an activation layer to reduce the
dimension of the data. Max pooling works better than average
pooling in practice for computer vision tasks such as image classi-
fications [40]. However, due to efficiency considerations, existing
schemes [23, 25] usually use average pooling instead of max pool-
ing, as the former is a linear function while the latter requires
comparisons in the circuit. In this paper, we propose a simple ap-
proach to compute the composition of max pooling and ReLU with
only a small overhead.
The composition of ReLU and max pooling layers compute
max{ReLU(𝑥0), ..., ReLU(𝑥𝑘−1)} with size 𝑘. The prover is required
to provide the result of the above function ¯𝑥max as an auxiliary
input. Then by the property of ReLU and maximum,
¯𝑥max − 𝑥 𝑗 = 0; otherwise ¯𝑥max = 0.
(1) ¯𝑥max − 𝑥 𝑗 ≥ 0 ∀𝑗 ∈ [𝑘].
(2) If 𝑥𝑖s are not all negative numbers, then ∃𝑗 ∈ [𝑘], such that
The first condition can be checked by bit-decomposing each ¯𝑥max −
𝑥 𝑗 with 𝑄 bits as the auxiliary input from the prover (the 𝑄 + 1-th
bit denoting the sign of the number is not necessary, as it is always
non-negative). The checks are exactly the same as the first two
checks in the computation of ReLU above. The second condition is
𝑗=0 ( ¯𝑥max−𝑥 𝑗) = 0. Finally, to avoid overflow,
the prover also provides the bits of ¯𝑥max and the circuit validates the
bit decomposition. Overall, comparing to computing ReLU above,
the prover only additionally provides ¯𝑥max and its bits, and the
protocol checks one additional bit decomposition.
equivalent to ¯𝑥max·𝑘−1
Figure 3: Sumcheck for FFT.
𝑖 ch𝑖𝑛,𝑖ch𝑜𝑢𝑡,𝑖 + 𝑛2
the low degree extensions together with polynomial commitments
to achieve zero knowledge. As shown in [48], the overhead is small
in practice compared to the plain version without zero knowledge.
We omit the formal protocol and the proof for the zero knowledge
version of zkCNN in this paper. Finally, we remove the interactions
in our zero knowledge CNN scheme using the Fiat-Shamir Heuris-
tic [24] in the random oracle model. The transformation only incurs
is 𝑂(𝑚
a negligible soundness loss [11].
Complexity. The prover time of the interactive proof in our scheme
proof size and verifier time are 𝑂(𝑚
𝑖 ch𝑜𝑢𝑡,𝑖𝑄)), where 𝑄 is the maximum
bit-length for bit decomposition in ReLU and max pooling. The
commitment of size 𝑆in =𝑚
𝑖 ch𝑖𝑛,𝑖ch𝑜𝑢𝑡,𝑖) +
log(𝑛2
𝑖 ch𝑜𝑢𝑡,𝑖𝑄))). In addition, our scheme involves a polynomial
𝑖 ch𝑜𝑢𝑡,𝑖𝑄). Us-
ing the polynomial commitment scheme in [44], the prover time of
this part is 𝑂(𝑆in), the proof size and the verifier time are 𝑂(√
𝑆in).
Our scheme can be modified to a zero knowledge CNN accuracy
scheme. The protocol is executed on the circuit in Figure 2 for
multiple input samples, followed by a circuit to compare the results
with the labels and computes the accuracy. Finally, our schemes
can be made non-interactive using the Fiat-Shamir heuristic [24]
with a negligible soundness loss [11].
𝑖=0(log2(𝑛2
𝑖 ch𝑖𝑛,𝑖ch𝑜𝑢𝑡,𝑖 + 𝑛2
𝑖=0(𝑛2
𝑖=0(𝑤2
4.4 Putting Everything Together
With the building blocks presented in Section 3 and 4, we construct
a scheme of zero knowledge CNN predictions. The prover commits
to the parameters W of a CNN model using a polynomial com-
mitment scheme. Given an input data X, the prover computes the
prediction of pred(W, X) together with the auxiliary input shown
in Figure 2. The prover further commits to the additional auxiliary
input using the polynomial commitment scheme. The prover and
the verifier then invoke the sumcheck protocols and our general-
ized GKR protocols for matrix multiplications ([41]), convolutional
layers (Section 3.2 and 4.2) and pooling and activation functions
(Section 4.3) to reduce the correctness of the prediction to an evalu-
ation of the multilinear extension of the input in Figure 2. Finally,
the prover opens the polynomial commitments of the witness and
auxiliary input at the evaluation point and completes the proof.
The full protocol is presented in Protocol 3 in Appendix C.
Theorem 4.1. Protocol 3 is correct and sound by Definition C.1.
We present a proof sketch in Appendix C. We then take existing
approaches in [17, 48] to turn the protocol to a zero knowledge
argument. In particular, we use the zero knowledge sumcheck and
5 IMPLEMENTATION AND EVALUATIONS
We implemented our zero knowledge proof scheme for CNN, zkCNN,
and we present the experimental results in this section.
Software. The scheme is implemented in C++ and there are around
5000 lines of code. Some of our algorithms on the sumcheck protocol
and the GKR protocol are based on the open-source implementation
of [48, 51, 52]. We use the polynomial commitment scheme in [39,
44] because of its good prover time and reasonable proof size for
the witness in our experiments. The security of the scheme relies
on the discrete-log assumption. The prover time is 𝑂(𝑁) and the
proof size and the verifier time are 𝑂(√
𝑁) for a polynomial of size
𝑁 . We replace the Curve-25519 in the implementation of [39] with
Curve BLS12-381 [15], as the order of Curve-25519 does not have
a root of unity of a large power of 2 and thus is not FFT friendly.
Curve BLS12-381 offers 128-bits of security and we use the mcl
library [4] for its field and curve operations.
Hardware. We run all of the experiments on a machine with AMD
EPYC 7R32 64-Core Processor and 128GB of RAM. Our current
implementation is not parallelized and we only utilize a single CPU
core. The large memory is used to run experiments on large CNNs
(VGG16 with 15 million parameters) and multiple images. On one
28210212214216218Input size10−510−410−310−210−1100101102103104Prover time (s)oursfft-circuitfft-naiveSession 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2978hand the memory usage is actually the bottleneck to further scale
zkCNN and it is an interesting future work to improve it. On the
other hand the memory usage and the scalability are already orders
of magnitude better than existing SNARKs (See Section 5.2). We
report the average running time of 10 executions.
5.1 New Sumcheck for FFT and Convolution
We first benchmark the performance of our new sumcheck proto-
col for FFT and 2-D convolutions. We exclude the running time
and the proof size of the protocol to delegate the verifier’s com-
putation via Equation 10. This is because in applications such as
CNN predictions in Section 5.2, the same Fourier matrix is used in
many FFTs/convolutions and it is actually faster to compute the
evaluation of its multilinear extension at a random point.
FFT. Figure 3 shows the prover time of our new sumcheck protocol
for FFT in Section 3.1. As shown in the figure, the prover time of our
new protocol is very fast in practice. It only takes 0.6ms to generate
the proof for a vector of size 210, and 0.1s for a vector of size 218.
The prover time grows strictly linearly with the size of the vector,
as indicated by the complexity of our protocol. We compare the
performance of our protocol with two baseline approaches: the GKR
protocol on the FFT circuit and the naive sumcheck on Equation 5.
Comparing to the baseline of the FFT circuit, our prover time is 17×
faster for 𝑛 = 28 and 33.2× faster for 𝑛 = 218. The gap increases as
the prover time of the baseline is 𝑂(𝑛 log 𝑛). The proof size of our
protocol is 15.4–35.4× smaller than the baseline, as our protocol
consists of a single sumcheck, while the depth of the circuit in the
baseline is log 𝑛. The verifier time of the two schemes are similar.
Comparing to the second baseline of naive sumcheck on Equa-
tion 5, our new protocol is significantly faster. The prover time is
already 80× faster than the naive sumcheck for a vector of size 28,
and the gap grows dramatically with the size as the complexity
of the naive sumcheck is quadratic. The naive approach runs out
of memory for 214 and the shaded bars in the figure denote esti-
mations. The proof size and the verifier time of the two schemes
are exactly the same, as they are different algorithms for the same
sumcheck on Equation 5.
Convolution. Figure 4 shows the prover time of the our proto-
col for convolutions and compares it with the GKR protocol on a
circuit computing 2-D convolutions naively using multiplications
and additions. The experiment is for a single convolution, and we
vary the size of the input from 32 × 32 to 256 × 256, and the kernel
size from 4 × 4 to half of the dimension of the input (this is the
maximum kernel size for convolutions in CNN without padding).
As shown in the figure, the prover time is improved significantly
over the baseline. It only takes 4.7ms to generate the proof for a
convolution on 32 × 32 and 4 × 4 matrices, which is already 1.6×
faster than the 7.7ms in the naive approach. The speedup grows
dramatically with the size of the kernel. For a convolution between
input 32 × 32 and kernel 16 × 16, our prover time is 8.5× faster
than the baseline; for a convolution on the largest instance of input
256 × 256 and kernel 64 × 64, our speedup is 291×. Moreover, the
prover time almost remains the same for the same input size. This is
because the kernel has to be padded to the size of the input anyway
to perform FFT, thus different kernel sizes do not make a difference
for the same input size in our protocol.
Figure 4: Sumcheck for a single convolution.
Proof size and verifier time. Our proof size and verifier time are
slightly worse than the baseline. The proof size of our protocol
ranges from 5.6KB to 8.4KB, while it is 3.9KB to 7.1KB in the naive
approach. This is because our protocol has three sumchecks for
FFT, Hadamard product and IFFT to compute the convolution, and
the naive approach has two sumchecks, one for all multiplications
and one for addition trees. The proof size is linear in the number
of sumchecks and logarithmic in the size of each sumcheck. The
verifier time in both protocols are extremely fast. Here we do not
count the time to compute the multilinear extensions of the input
and the output, as they are given by the prover during the reductions
of the GKR protocols from other layers. The verifier time is only
logarithmic, and ranges from 0.1ms (32 × 32 and 4 × 4) to 0.3ms
(256 × 256 and 128 × 128) in our protocol, and ranges from 0.1ms to
0.2ms in the baseline.
Comparing to computing the result. As another benchmark, we fur-
ther measure the time to compute the result of the convolutions for
the input size of 256 × 256 using FFT and naive multiplications and
additions. As shown in Figure 4, the additional prover time of our
sumcheck protocol is only 1.8× slower than computing the result
using FFT. The prover time is slower than the naive computation
by 8× on the small kernel of 4 × 4, but is 31× faster on the large
kernel of 128 × 128. The result agrees with the optimal complexity
of our sumcheck protocol and shows that the overhead to generate
the proof is very small in practice.
Comparing to other approaches. To further demonstrate the effi-
ciency of our protocol, we compare the running time with the
approach of verifying convolutions in [34]. In [34, Figure 6], it
takes around 2.5s to generate the proof for a convolution between
an input of size 10,000 and a kernel of size 10. On a larger instance
of input 128 × 128 =16,384 and kernel 4 × 4 = 16 in our scheme, the
prover time is only 0.072s, which is 1–2 orders of magnitude faster.
5.2 Performance of zkCNN
In this section, we evaluate the performance of our zkCNN system.
Datasets and CNNs. We use two datasets: MNIST [33] and CIFAR-
10 [31]. MNIST is a dataset of hand-written digits. The images are
of size 28×28×1. There are 50,000 training data samples and 10,000
testing data sample, classified into 10 categories of digits 0–9. The