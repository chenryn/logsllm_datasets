Diffraction is the phenomenon where waves bend around the cor-
ners of an obstacle or through an aperture into the region of geomet-
rical shadow of the obstacle/aperture [38]. From the physics of wave
139
propagation (see detailed explanations in [9]), diffraction depends
on the relative wavelength of the signal compared to the size of
the object [9], as shown in figure 3. With larger wavelength, sound
waves exhibit far more diffraction than, say, light or RF signals.
.
Figure 3: Diffraction illustration: a wave will propagate into the
region of geometric shadow. The larger the wavelength compared
to the aperture, greater is the diffraction [2].
Figure 4 illustrates an experiment to characterize diffraction on the
human face, particularly due to the curvature of the cheek. We ask
Alice to wear a reference microphone on her right ear; a second
(test) microphone is pasted at 6 different locations on the left part of
her face (starting with the tip of the nose and ending at the ear). An
electronic speaker (shown on the user’s right) plays a chirp and we
calculate the chirp’s time difference of arrival (TDoA), Δ𝑡, between
the 2 microphones2. Multiplying speed of sound 𝑣 with Δ𝑡, we get
the difference in physical distance that the signal has traveled from
the speaker to the 2 microphones: Δ𝑑 = 𝑣 · Δ𝑡.
Figure 4: Experiment to test for signal diffraction on the curvature
of the human head.
In parallel, using precise measurements from a camera, and a soft-
tape that can bend along the curvature of the face, we obtain the
following distances: the Euclidian distance from the speaker 𝑆 to
the reference microphone 𝑅, 𝑑𝐸𝑢𝑐(𝑆,𝑅), the Euclidian distance to the
test microphone 𝑇 , 𝑑𝐸𝑢𝑐(𝑆,𝑇), and the distance along the diffracted-
path to the test microphone 𝑇 , 𝑑𝑑𝑖 𝑓 𝑓
(𝑆,𝑇). The test for diffraction is now
easy: Does Δ𝑑 derived from audio recordings better match with
the Euclidian path difference Δ𝑑𝐸𝑐𝑢 or the diffracted path Δ𝑑𝐷𝑖 𝑓 𝑓 ,
where
Δ𝑑𝐸𝑐𝑢 = 𝑑𝐸𝑢𝑐(𝑆,𝑇) − 𝑑𝐸𝑢𝑐(𝑆,𝑅)
Δ𝑑𝐷𝑖 𝑓 𝑓 = 𝑑𝐸𝑢𝑐(𝑆,𝑇) − 𝑑𝐷𝑖 𝑓 𝑓
(𝑆,𝑅)
2This is possible because the 2 microphones are synchronized with a wire.
020406080100120140160180Angle 2 (deg.)020406080100120140160180Angle 1 (deg.)0.50.60.70.80.91020406080100120140160180Angle 2 (deg.)020406080100120140160180Angle 1 (deg.)0.30.40.50.60.7SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Zhijian Yang and Romit Roy Choudhury
Figure 5 plots the results of matching. Evidently, Δ𝑑 matches strongly
with the diffracted path, especially as the test microphone moves
further away from the reference. The results of this experiment
were again consistent across multiple users, offering strong evi-
dence that (1) audible sounds do not penetrate through the human
head, and (2) modeling diffraction is critical for signal processing
on human bodies.
.
Figure 5: Acoustic and physical measurements are consistent;
shows evidence that signals diffract along the curvature of the head.
Building on these basics, we turn to estimating personal HRTF and
applying it to AoA estimation and beamforming.
3 SYSTEM SKETCH
This section outlines the key ideas, starting with near-field HRTF,
then expanding to the far-field, and finally discussing an application
of the estimated HRTF.
3.1 Near field HRTF
Consider a user moving her phone (in a circular trajectory) around
her head, and her in-ear earphones recording the sounds transmit-
ted by the phone. If we can accurately track the phone’s location,
then near field HRTF can be directly estimated. This is because
the acoustic channel can be estimated from each location of the
phone, and since location tells us the angle 𝜃, the channel from each
angle is now known. The per-angle acoustic channel is exactly the
near-field HRTF. Thus, to estimate the near field HRTF, the
main challenge is in estimating the phone’s location.
While IMU sensors on the phone can help with localization, it
is far too noisy for the accuracy levels needed with HRTFs. The
main reason is well known, i.e., location estimation with IMUs
requires a double integration on the accelerometer data, which
causes the noise to grow multiplicatively. In light of this, UNIQ
operates in the polar coordinates . The intuition is to fuse
the IMU’s gyroscope data with the acoustic channel information
— the gyroscope helps with inferring the angular component 𝜃
and the acoustic signal delays (between the earphones) help with
estimating the distance 𝑟. Even though each is erroneous in its
own way, we hope joint optimization will achieve accuracy and
robustness.
While inferring geometric distances 𝑟 from multiple microphones
should be feasible, it poses unique problems in our case with ear-
phones. Since the head and pinna filter the acoustic signal arriving
at the ears, conventional techniques from array signal processing
140
are no longer accurate. In other words, we need to model the head’s
diffraction effect on sound waves to make the recorded acoustic
information usable. Additionally, we also need to cope with head pa-
rameters, which is obviously different across people, and will affect
diffraction. In sum, we are faced with the problem of jointly esti-
mating the phone location and diffraction-related head parameters,
using a fusion of both IMU and acoustic information. This motivates
our first module in Figure 6: “Diffraction-aware Sensor Fusion”.
This module gives us the near-field HRTF, but only at discrete angles
around the head. To generalize to continuous angles, we input
the discrete estimates into the “Near Field HRTF Interpolation”
module. The interpolated output allows UNIQ to synthesize binaural
sounds for any location near the user3.
3.2 Far field HRTF
Now consider what happens when an earphone user wants to
simulate sounds from the far field (e.g., a user listening to a piano
in a virtual concert – the sound should appear to come from the far-
away stage). Say this far field location is at an angle 𝜃 from the head.
Even though we know 𝐻𝑅𝑇 𝐹(𝜃) from our near-field estimation,
using this 𝐻𝑅𝑇 𝐹(𝜃) for far-field is non-ideal. This is because sound
signals arriving at the ears from a nearby location at angle 𝜃 would
be different from a far-away location at angle 𝜃. As illustrated in
Figure 7, far-field produces parallel rays while near-field produces
non-parallel rays, causing different multipath, arrival times, and
diffraction profiles at the 2 ears.
In view of this, UNIQ needs to model how parallel rays from angle
𝜃 would scatter/diffract on the head and arrive at the ears. Since the
near-field HRTF has already modeled head and pinna multipath,
we combine information from multiple 𝐻𝑅𝑇 𝐹(𝜃 𝑗) to synthesize
the far-field HRTF. We fine-tune this far-field HRTF by adjusting
the delays and amplitude differences based on the head parameters
learnt from the sensor fusion module. These operations make up
the “Near-Far Conversion” module, which outputs the far-field
HRTF. Combining near and far-field HRTFs, we can now create
binaural sounds from any location around the user.
Finally, we develop a “Binaural Angle of Arrival (AoA) Estima-
tion” module as an example application of far-field HRTF. We show
how personalized HRTFs can estimate the direction of real ambient
sounds with improved accuracy.
4 SYSTEM DESIGN
Figure 6 captures the system architecture. We begin this section
with (1) Diffraction-Aware Sensor Fusion, which feeds into the (2)
Near Field HRTF Interpolation module, as well as the (3) Near-Far
Conversion module. The final output of UNIQ could then enable
a number of applications; we discuss one example: “Binaural AoA
Estimation”.
4.1 Diffraction-Aware Sensor Fusion (DSF)
Once a user rotates the phone around her head, we have the IMU
measurements and the microphone recordings. DSF’s task is to
3Binaural sounds describe what a person would hear when a sound originates at some
given location.
02468Mic horizontal location on face (cm)5101520Distance (cm)  t  v =  d  dDiff  dEucPersonalizing Head Related Transfer Functions for Earables
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
.
Figure 6: System Architecture: UNIQ estimates both near and far-field HRTF taking inputs from the phone IMU and earphone microphone.
The system pipeline is composed of 3 modules (diffraction-aware sensor fusion, near field HRTF interpolation, and near-far conversion)
followed by an application that estimates binaural AoA from the personalized HRTF.
.
Figure 7: Illustration of near and far HRTF for angle 𝜃.
accept these measurements as inputs and output both the head’s
geometric parameters and the phone’s location. For this, let us
model diffraction first.
Modeling Head Diffraction
Figure 8 shows a simplified version of signal diffraction on the
head. To model this, we start by approximating the head shape as a
conjunction of two half-ellipses, attached at the ear locations. This
is necessary since the head is not symmetric between the front
and back, hence spherical models have been avoided in literature
[49]. The head shape can now be expressed through a 3-parameter
set, 𝐸 = (𝑎, 𝑏, 𝑐), where 𝑎, 𝑏, and 𝑐 are the axis lengths of the two
ellipses. Now, assuming the sound source is towards the right of
the head, the signal would not penetrate through the head to arrive
to the left ear, but would bend over/around the left cheek of the
user (diffraction). With head parameters 𝐸 known and for a given
phone location 𝑃, we can estimate the time at which the diffracted
signals would arrive at the two ears respectively.
Figure 9 shows the measured acoustic channel at the two ears for
the above scenario (the channels are estimated by deconvolving the
received signal with the known source signal). Clearly, the channel
has multiple peaks (or taps) since the signal reflects on the face and
these reflections also diffract. However, we are interested only in
the first peaks at the two ears, since they are the ones that reliably
capture the relationship between the phone and ear locations. This
.
Figure 8: Sound waves arriving from phone at location P will
diffract around the head before reaching the two ears.
is because the subsequent peaks in the channel are paths that arrive
after reflecting on various points on the face, and while they may
be useful to image the face, they are not necessary for our purposes
of phone localization. Thus, UNIQ extracts the first peaks from the
two channels and uses the relative delay Δ𝑡 to connect the phone
location and the head-shape in a common framework, as shown in
equation 1:
Δ𝑡 = relative delay for first peak in ℎ𝐿, ℎ𝑅
= 𝑓 (Diffraction)
= 𝑓 (𝑎, 𝑏, 𝑐, 𝑃)
(1)
This serves as the basis for diffraction-aware sensor fusion.
Sensor Fusion Algorithm
Now, consider the IMU readings from the phone and the sound
recordings from the in-ear microphone (the phone and the ear-
phones are synchronized). UNIQ infers the phone’s inertial rotation
from the IMU’s gyroscope, which translates to the phone’s polar
angle relative to the head. Of course, this still does not give the
phone location (since the distance to the head is unknown).
141
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Zhijian Yang and Romit Roy Choudhury
Estimating Polar Angle 𝜃𝑖(𝐸) in Step 2 above:
Estimating 𝛼 from IMU readings is a straightforward gyroscope
integration. However, phone location and angle 𝜃𝑖 from the acoustic
model is slightly more involved. Assume 𝑡1 and 𝑡2 are the diffraction
path delays (first tap delays) for signals that arrive at the left and
right ear, respectively. Now, assuming we already have the head
parameters 𝐸, then we can draw 2 trajectories (as shown in Fig-
ure 10(b)). The first one is the trajectory of points from which the
diffraction-based delay to the left ear is 𝑡1. The second trajectory is
the one from which the diffraction-based delay to the right ear is
𝑡2. The phone’s location must be at the intersection of these 2 tra-
jectories. From the figure, we can observe that the two trajectories
actually intersect at two points 𝐴 and 𝐵, with polar angles 𝜃𝐴(𝐸),
𝜃𝐵(𝐸), and polar radius 𝑟𝐴, 𝑟𝐵. To disambiguate, we will pick the
𝜃(𝐸) that is closer to the IMU angle estimation 𝛼. By plugging 𝜃(𝐸)
and 𝛼 into the above Equation (2) and performing the optimization,
UNIQ derives the optimal head parameter 𝐸𝑜𝑝𝑡.
As a final step, we combine the IMU and acoustic localization results
to obtain the estimated location of the phone as
𝑃(𝜙𝑖, 𝑟𝑖) = 𝑃(cid:0)(𝜃𝑖(𝐸𝑜𝑝𝑡) + 𝛼𝑖)/2, 𝑟𝑖(cid:1)
(3)
By indexing the measured HRTFs with the estimated phone lo-
cations, we complete the near-field HRTF estimation at discrete
sample points. To obtain a continuous near-field HRTF, we employ
interpolation.
4.2 Near field HRTF interpolation
It is difficult for a user to rotate the phone in continuous trajectories
around their head. Thus, we allow users to position the phone at as
many convenient locations as possible, and interpolate across other
locations (shown in Figure 11). Interpolation is crucial because (1)
downstream applications may intend to place sounds in any arbi-
trary location in the near-field; (2) as we will see soon, continuous
near-field HRTF aids in synthesizing the far-field HRTF.
.
.Figure 9: Channel impulse response: first tap corresponds to
diffraction path
On the other hand, if the parameters 𝐸 = (𝑎, 𝑏, 𝑐) are known, the
relative delay from the acoustic channels can give phone location
(with some ambiguity since 2 front/back locations can produce the
same delay at the ears). Said differently, IMUs and acoustic channels
do not individually solve the localization problem, but contribute
adequate information to (over) determine the system of equations.
This is exactly why sensor fusion helps – UNIQ jointly solves for
head parameter and phone location through a fusion of IMU and
acoustics.
The steps of the fusion algorithm can now be laid out:
1. As the smartphone rotates around the head, the IMU measure-
ments are integrated to obtain the phone’s orientation 𝛼. Since we
ask users to face the phone’s screen towards their eyes, 𝛼 should
be exactly equal to the polar angle 𝜃 (illustrated in Figure 10(a)).
Over time, the phone orientation and the polar angle change,
denoted as 𝜃𝑖 and 𝛼𝑖, 𝑖 = 1, 2, . . . , 𝑁 .
3. When the parameters 𝐸 are correct, the 𝛼𝑖 and 𝜃𝑖 should match
2. Using the measured acoustic channels, and pretending we know
the head parameters 𝐸, we can localize the phone and map it to
the polar angle 𝜃𝑖(𝐸).
∀𝑖 = 1, 2, . . . , 𝑁 .
∥𝛼 − 𝜃∥2 with decision variables as 𝐸:
4. Due to noise in IMU and acoustics, we minimize the squared error