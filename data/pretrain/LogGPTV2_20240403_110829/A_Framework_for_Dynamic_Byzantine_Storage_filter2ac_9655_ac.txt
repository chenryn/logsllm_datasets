### Safely Turning Off Server Machines Not in View

To ensure the safe shutdown of server machines not currently in view, it is crucial to restrict who can issue the `newView` command. In our system, this privilege is exclusively granted to the administrator. If the administrator acts maliciously, we cannot provide any guarantees; for instance, they could initiate a view with no servers, thereby denying service to all clients. However, the system can tolerate crash failures of the administrator. This problem persists even if the administrator's algorithm is Byzantine fault-tolerant, as long as the inputs are provided by a person and the machine transmitting these inputs has not been tampered with.

Since determining future values of `f` and deciding on adding new computers (including purchasing them if necessary) is best done by a human, we will consider a single crash-only administrator machine for the remainder of this paper.

### Faults and Views

Our system uses views to discretize time, and our definition of faults follows this structure. A server is considered correct in a given view `t` if it adheres to the protocol from the beginning of time until the end of view `t`. Otherwise, it is deemed faulty in view `t`. It is possible for a server to be correct in one view `t` and faulty in a later view `u`. However, once a server is marked as faulty, it will never be considered correct again. If a server recovers from a failure (e.g., by reinstalling the operating system after disk corruption), it must adopt a new name before rejoining the system.

The concept of a resilience threshold is also parameterized by view numbers. For example, a static U-dissemination protocol requires a minimum of `n ≥ 3f + 1` servers. This requirement now becomes `|N(t)| ≥ 3f(t) + 1` for each view `t`. Our system assumes that between the start and end of view `t`, at most `f(t)` of the servers in `N(t)` are faulty. Since views can overlap, sometimes multiple such conditions must hold simultaneously.

### Simplified DQ-RPC

We begin with a simplified version of DQ-RPC, which, despite its limitations, allows us to easily present several key features of DQ-RPC. The full implementation is detailed in Section 5.3.

The simplest way to implement DQ-RPC is to ensure that different views do not overlap, meaning that at any point in time, there is at most one active view. Given that the protocols in Figure 3 are correct for a static quorum system, we can evolve the system through a sequence of static quorum systems. This can be achieved as follows:

- **Tagging Responses**: Servers tag their responses with a view number.
- **Accumulating Responses**: Once a client accumulates `q(t)` responses tagged with view `t`, DQ-RPC returns these responses.

Simplified DQ-RPC outputs a view `t` (called DQ-RPC’s current view) and a quorum of `q(t)` responses. If clients have an infallible external method to know which servers are in an active view, this simple scheme is sufficient: DQ-RPC sends messages to servers in an active view and ensures that it only picks active views as its current view.

### Determining Active Views

The rest of this section explains how DQ-RPC can determine which views are active.

#### View Changes

To determine whether a view is active, it is essential to specify how the system starts and ends views. To initiate a view change, the administrator's computer first informs a quorum of machines in the old view that their view has ended. These machines immediately stop accepting client requests, preventing clients from reading from the old view. The administrator then performs a user-level read on the machines from the old view to obtain a value `v`. Finally, the administrator informs all the machines in the new view that the new view is starting and provides them with the initial value `v`. At this point, the machines in the new view start accepting client requests.

If the administrator cannot contact all the new machines (e.g., due to a faulty server), the simplified DQ-RPC assumes the administrator has a way to contact all servers. The full DQ-RPC, discussed in Section 5.3, ensures that all view changes terminate.

#### Handling Decommissioned Servers

A critical consideration during a view change is that, after view `t` ends, the constraint that at most `f(t)` of the machines in view `t` can be faulty no longer applies. For example, if a view is changed to remove decommissioned servers, the system's semantics should not depend on the behavior of these decommissioned servers.

However, decommissioned machines may still have information about the previous state of the system. If they become faulty, they could respond to queries from clients unaware of the new servers and provide stale data, violating atomic semantics. To prevent this, the view change protocol must ensure that no client can read or write to a view after it has ended. Our "forgetting" protocol enforces this property.

### Safe View Certification through "Forgetting"

The simplified DQ-RPC requires clients to receive a quorum of responses with view `t`'s tag before returning that value and considering view `t` current. If the servers are correct, this ensures that no DQ-RPC chooses `t` as current after `t` ends.

The forgetting protocol ensures this property holds even if servers fail Byzantine. Clients tag their queries with a nonce `e`. Each server `i` tags its response with two pieces of information: 1) a view certificate `(cid:2)i, meta, pub(cid:3)admin` signed by the administrator, and 2) a signature for the nonce `(cid:2)e(cid:3)priv`, proving that server `i` possesses the private key associated with the public key in the view certificate. The key pair `pub, priv` is chosen by the administrator. The certificate contains metadata for the view, including the view number `t`, the set of servers `N`, and the resilience threshold `f`.

When servers leave view `t`, they discard the view certificate and private key associated with that view. To ensure that even if they become faulty later, they cannot recover the private key and vouch for a view they left, the private key is encrypted using a secret key that changes for every view.

### Finding the Current View

In the previous section, we saw how clients can identify old views. We now need to ensure that clients can find the current view. If the set of servers the client contacts intersects with the current view in at least one correct server `i`, the client will receive up-to-date view information from `i` and can find the current view. If not, the client can consult well-known sites where the administrator publishes the list of servers in the current view. Our certified tags ensure safety: even if the information from these sites is outdated, the client will never pick a view that has ended.

### Summary of Simplified DQ-RPC

Clients only accept responses if they all have valid tags for the same view. Until they accept a response, clients keep resending their request (for read or write) to the servers. Clients use the information in the tags to locate the most recent servers and periodically check well-known servers if the servers do not respond or do not have valid tags. Tags are valid if their view certificate has a valid signature from the administrator and includes a signature of the client-supplied nonce that matches the public key in the certificate.

Replacing Q-RPC with this simplified DQ-RPC in a dissemination quorum protocol from Figure 3 results in a dynamic protocol that maintains all the properties listed in the figure.

### Limitations of Simplified DQ-RPC

Simplified DQ-RPC has two significant limitations:
1. It requires the administrator's `newView` command to wait for a reply from all the servers in the new view, which may never happen if some servers in the new view are faulty.
2. It does not allow DQ-RPCs (and, implicitly, user-level read and write operations issued by clients) to complete during a view change; instead, the operations are delayed until the view change has completed.

These limitations are addressed in the next section.

### Full DQ-RPC for Dissemination Quorums

The full DQ-RPC for dissemination quorums follows a similar pattern to its simplified version but can decide on a response sooner because it can identify consistent answers without requiring all responses to be tagged with the same view. The full DQ-RPC also runs a different view change protocol that terminates despite faulty servers.

We split the implementation of DQ-RPC into three parts:
1. **Main DQ-RPC Body (Figure 4)**: Takes a message and sends it repeatedly to the servers believed to constitute the current view. The client's current view changes based on the responses received. If no responses are received for a while, DQ-RPC consults well-known sources for a list of possible servers.
2. **Sender Object**: Repeatedly sends the message to the destination. The destination can be changed, and replies are accessed through the `getReplies` method.
3. **ViewTracker Object (Figure 6)**: Acts as a filter, allowing the Sender to read messages. The ViewTracker keeps track of the most recent view certificate and decides when a set of answers is consistent.

### Introducing Generations

Our dynamic protocols introduce the concept of generations to manage the evolution of views and ensure consistency. This approach allows for more efficient and robust handling of view changes and server responses.

```plaintext
DQ-RPC(msg)
1. Sender sdr := new Sender(msg)
2. static ViewTracker g vt := new ViewTracker
3. repeat
4.   sender.sendTo(g vt.get().N)
5.   (Q, t) := g vt.consistentQuorum(sdr.getReplies())
6.   if running for too long then g vt.consult()
7. until Q (cid:7)= ∅
8. return Q
```

- **t** is the current view associated with this operation.
- **sender** stops sending at this point.

This structured approach ensures that the system remains robust and responsive, even in the presence of faulty servers and overlapping views.