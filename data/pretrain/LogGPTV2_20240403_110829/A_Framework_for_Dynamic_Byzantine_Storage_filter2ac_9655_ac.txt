safely turn off server machines that are not in view t.
Obviously, we must restrict who can call the newView
command. In our system, this is solely the privilege of
the administrator. If the administrator is malicious then we
cannot provide any guarantee (for example, it could start
a view containing no server to deny service to all clients).
However, the system can tolerate crash failures of the ad-
ministrator. This problem remains even if the administra-
tor algorithm is run in a Byzantine fault tolerant manner, as
long as that program takes its inputs from a person: the ma-
chine through which these inputs are transmitted must not
have been tampered with. Since the determination of fu-
ture values of f and the decision of adding computers to
the system (possibly purchasing new ones as necessary) is
best done by a person, we consider a single crash-only ad-
ministrator machine for the remainder of this paper.
Since our system uses views to discretize time, so does
our deﬁnition of faults. We say that a server is correct in
some view t if it follows the protocol from the beginning
of time until view t ends. Otherwise, it is faulty in view
t. Note that a server may be correct in some view t and
faulty in a later view u. However, faulty servers will never
be considered correct again. If some server recovers from
a failure (for example by reinstalling the operating sys-
tem after a disk corruption), it takes on a new name be-
fore joining the system. The notion of resilience thresh-
old is also parameterized using view numbers. For exam-
ple, a static U-dissemination protocol requires a minimum
of n ≥ 3f + 1 servers: this requirement now becomes
|N(t)| ≥ 3f(t) + 1 for each view t. Our system assumes
that between the start and the end of view t, at most f(t)
of the servers in N(t) are faulty. Since views can over-
lap this means that sometimes a conjunction of such con-
ditions must hold at the same time.
5.2. A simpliﬁed DQ-RPC
We begin with a simpliﬁed version of DQ-RPC
limitations, al-
that, while suffering from serious
lows us to present more easily several of the key features
of DQ-RPC—the full implementation of DQ-RPC is pre-
sented in Section 5.3.
The easiest way to implement DQ-RPC is to ensure
that different views never overlap, i.e. that at any point in
time there exists at most one active view. Since we know
that the protocols in Figure 3 are correct for a static quo-
rum system, we can simply make sure to evolve the sys-
tem through, as it were, a sequence of static quorum sys-
tems. We can do so as follows.
• Replies from servers are tagged with a view number
• Once a client accumulates q(t) responses tagged with
view t, the DQ-RPC returns these responses.
Our simpliﬁed DQ-RPC has two outputs: a view t (that
we call DQ-RPC’s current view) and a quorum of q(t) re-
sponses. If we assume that clients have some external, in-
fallible way to know which servers are in an active view
then the above simple scheme is sufﬁcient: DQ-RPC sends
its messages to servers in an active view and it makes sure
that it only picks active views as its current view6.
Showing how DQ-RPC can determine which views are
active is the subject of the rest of this section.
5.2.1. View changes To determine whether a view is ac-
tive, it is important to specify how the system starts (and
ends) views.
To initiate a view change, the administrator’s computer
ﬁrst tells a quorum of machines on the old view that their
view has ended. These machines immediately stop accept-
ing client requests. Clients can thus no longer read from
the old view since they will not be able to gather a quo-
rum of responses. The administrator then performs a user-
level read on the machines from the old view to obtain
some value v. Finally, the administrator tells all the ma-
chines in the new view that the new view is starting, and
provides them with the initial value v. At this point, the
machines in the new view start accepting client requests.
Naturally, it is not always possible for the administrator
to make sure it has contacted all the new machines: if some
server is faulty then it could choose not to acknowledge,
causing the administrator to block forever. In our simpli-
ﬁed DQ-RPC we remove this problem by simply assum-
ing that the administrator has some way to contact all the
servers. We will see in Section 5.3 how the full DQ-RPC
ensures that all view changes terminate.
A delicate point to consider when performing a view
change is that, after view t ends, so does the constraint
that at most f(t) of the machines in view t can be faulty.
For example, if the view was changed to remove some de-
commissioned servers, it is natural to expect that the se-
mantics of the system from then on does not depend on
the behavior of the decommissioned servers.
6
It is necessary to pick an active view: after some DQ-RPC writes
data to the latest view, reads to a view that has ended would return
old data since different views may have no servers in common.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:11 UTC from IEEE Xplore.  Restrictions apply. 
And yet, the decommissioned machines know some-
thing about the previous state of the system. If they all be-
came faulty (as it may happen, since they are no longer un-
der the administrator’s watchful eye) they would be able
to respond to queries from clients that are not yet aware of
the new servers and fool them into accepting stale data, vi-
olating atomic semantics. To prevent the system from de-
pending on servers that have been decommissioned, the
view change protocol must ensure that no client can read
or write to a view after that view has ended. Our forget-
ting protocol enforces this property.
Safe View Certiﬁcation through “Forgetting” The simpli-
ﬁed DQ-RPC requires the client to receive a quorum of re-
sponses with view t’s tag before it returns that value and
considers view t current. If the servers are correct, then
this ensures that no DQ-RPC chooses t as current after t
ends (recall that views end once a quorum of their servers
have left the view).
The forgetting protocol ensures that this property holds
despite Byzantine failure of the servers. Clients tag their
queries with a nonce e. Server i tags its response with
two pieces of information: 1) server i’s view certiﬁcate
(cid:2)i, meta, pub(cid:3)admin, signed by the administrator, and 2) a
signature for the nonce (cid:2)e(cid:3)priv, proving that server i pos-
sesses the private key associated with the public key in the
view certiﬁcate. The key pair pub, priv is picked by the
administrator. In the certiﬁcate, meta contains the meta
information for the view, namely the view number t, the
set of servers N and the resilience threshold f. The quo-
rum size q can be computed from these parameters.
When servers leave view t, they discard the view cer-
tiﬁcate and private key that they associated with that view.
The challenge is to ensure that even if they become faulty
later, they cannot recover that private key and thus can-
not vouch for a view that they left. We now discuss how
our protocol addresses this issue.
The private key is only transmitted when the admin-
istrator informs the server of the new view. Our network
model allows the channel to duplicate and delay this mes-
sage, which may therefore be received after the server has
left the view. To prevent the decommissioned server from
recovering the private key we encrypt the message using a
secret key that changes for every view.
The administrator’s view change message for view t to
server i contains the following:
(NEW VIEW, t, oldN,
(cid:1)
encrypt
((cid:2)i, meta, pub(cid:3)admin, priv), kt
i
(cid:2)(cid:2)
We use the notation encrypt(x, k) for the result of en-
crypting data x using the secret key k. The view key kt
i
is shared by the administrator and server i for view t. It
is computed from the previous view’s key using a one-
). The administrator and
way hash function: kt
i
server i are given k0
i at system initialization.
:= h(kt−1
i
When correct servers leave a view t, they discard view
t’s certiﬁcate, private key priv and view key kt
i. As a result
they will be unable to vouch for view t later even if they
become faulty and gather information from duplicated net-
work messages. This ensures that client following the sim-
pliﬁed DQ-RPC protocol will not pick view t as its current
view after t ends.
5.2.2. Finding the current view In the previous section
we have seen how clients can identify old views. We now
need to make sure that the clients will be able to ﬁnd the
current view, too.
If the set of servers that the client contacts to perform
its DQ-RPC intersects with the current view in one correct
server i, then the client will receive up to date view infor-
mation from i and will be able to ﬁnd the current view.
If that is not the case, then the client can consult well-
known sites to which the administrator publishes the list
of the servers in the current view. Our certiﬁed tags ensure
safety: even if the information the client retrieves from one
of these sites is obsolete, the client will never pick as cur-
rent a view that has ended. Therefore it sufﬁces that the
client eventually learn of an active view from one of the
well-known sites.
In the case of a local network, clients could also broad-
cast a query to ﬁnd the servers currently in N. This solu-
tion has the advantage of simplicity but it only works if all
servers are in the same subnet.
5.2.3. Summary Clients only accept responses if they
all have valid tags for the same view. Until they accept a
response, clients keep re-sending their request (for read or
write) to the servers. Clients use the information in the tags
to locate the most recent servers, and periodically check
well-known servers if the servers do not respond or do not
have valid tags. Tags are valid if their view certiﬁcate has a
valid signature from the administrator and the tag includes
a signature of the client-supplied nonce that matches the
public key in the certiﬁcate.
Replacing Q-RPC with this simpliﬁed DQ-RPC in a
dissemination quorum protocol from Figure 3 results in
a dynamic protocol that maintains all the properties listed
in the ﬁgure.
However, simpliﬁed DQ-RPC has two signiﬁcant lim-
itations. First, it requires the administrator’s newView
command to wait for a reply from all the servers in the new
view, which may never happen if some servers in the new
view are faulty. Second, it does not let DQ-RPCs (and,
implicitly, user-level read and write operations issued by
clients) complete during a view change: instead the opera-
tions are delayed until the view change has completed. We
address both limitations in the next section.
5.3. The full DQ-RPC for dissemination quorums
The full DQ-RPC for dissemination quorums follows
the same pattern as its simpliﬁed version: it sends the mes-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:11 UTC from IEEE Xplore.  Restrictions apply. 
DQ-RPC(msg)
1. Sender sdr := new Sender(msg)
2. static ViewTracker g vt := new ViewTracker
3. repeat
4.
5.
6.
7. until Q (cid:7)= ∅
8. return Q
sender.sendTo(g vt.get().N)
(Q, t) := g vt.consistentQuorum(sdr.getReplies())
if running for too long then g vt.consult()
// t is the current view associated with this operation
// sender stops sending at this point
Figure 4: Dynamic quorum RPC
sage repeatedly until it gets a consistent set of answers,
and picks a current view in addition to returning the quo-
rum of responses. DQ-RPC uses the technique described
in the previous section to determine whom to send to, but
it can decide on a response sooner than the simpliﬁed DQ-
RPC because it can identify consistent answers without re-
quiring all the responses to be tagged with the same view.
The full DQ-RPC also runs a different view change proto-
col that terminates despite faulty servers.
We split the implementation of DQ-RPC into three
parts. The main DQ-RPC body (Figure 4) takes a mes-
sage and sends it repeatedly to the servers believed to con-
stitute the current view. The client’s current view changes
with the responses that it gets; if no responses are received
for a while then DQ-RPC consults well-known sources for
a list of possible servers (line 6). The repetitive sending
is handled by the Sender object, and the determination of
the current view is done by the ViewTracker object (Fig-
ure 6). The client exits when it receives a quorum of con-
sistent answers. In the simpliﬁed protocol, answers were
consistent if they all had the same tag. In this section we
develop a more efﬁcient notion of consistent responses.
The Sender is given a message and a destination and it
repeatedly sends the message to the destination. The desti-
nation can be changed using the sendTo method and the
replies are accessed through getReplies (The code for
the Sender object can be found in [15]).
The ViewTracker acts like a ﬁlter: Sender must go
through it to read messages. The ViewTracker looks at
the messages and keeps track of the most recent view
certiﬁcate it sees. As we saw in the forgetting protocol,
messages are tagged with a signed view certiﬁcate and a
signed nonce. Messages that do not have a correct sig-
nature for the nonce are not considered as vouching for
the view (line 3 of ViewTracker.consistentQuorum).
However, even if the nonce signature is invalid, View-
Tracker will use valid view certiﬁcates to learn which
servers are part of the latest view (line 5). The most re-
cent view certiﬁcate can be accessed through the get
method. The ViewTracker can also get new candidates
from well-known servers with the consult method.
Finally, the ViewTracker has the responsibility of de-
ciding when a set of answers is consistent, through the
consistentQuorum method.
5.3.1. Introducing generations Our dynamic protocols