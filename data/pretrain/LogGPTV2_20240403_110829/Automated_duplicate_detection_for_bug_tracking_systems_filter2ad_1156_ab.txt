bug is a useful software quality measure [8]. They mea-
sure the time taken to ﬁx bugs in two software projects. We
predict whether a bug will eventually be resolved as a du-
plicate and are not focused on particular resolution times or
the total lifetime of real bugs.
Our work is most similar to that of Runeson et al. [15], in
which textual similarity is used to analyze known-duplicate
In their experiments, bug reports that are
bug reports.
known to be duplicates are analyzed along with a set of
historical bug reports with the goal of generating a list of
candidate originals for that duplicate.
In Section 5.2 we
show that our technique is no worse than theirs at that task.
However, our main focus is on using our model as a ﬁlter
to detect unknown duplicates, rather than correctly binning
known duplictes.
4. Modeling Duplicate Defect Reports
Our goal is to develop a model of bug report similarity
that uses easy-to-gather surface features and textual seman-
tics to predict if a newly-submitted report is likely to be a
duplicate of a previous report. Since many defect reports
are duplicates (e.g., 25.9% in our dataset), automating this
part of the bug triage process would free up time for devel-
opers to focus on other tasks, such as addressing defects and
improving software dependability.
Our formal model is the backbone of our bug report ﬁl-
tering system. We extract certain features from each bug
report in a bug tracker. When a new bug report arrives, our
model uses the values of those features to predict the even-
tual duplicate status of that new report. Duplicate bugs are
not directly presented to developers to save triage costs.
We employ a linear regression over properties of bug re-
ports as the basis for our classiﬁer. Linear regression offers
the advantages of (1) having off-the-shelf software support,
decreasing the barrier to entry for using our system; (2)
supporting rapid classiﬁcations, allowing us to add textual
semantic information and still perform real-time identiﬁca-
tion; and (3) easy component examination, allowing for a
qualitative analysis of the features in the model. Linear re-
gression produces continuous output values as a function
of continuously-valued features; to make a binary classiﬁer
we need to specify those features and an output value cut-
off that distinguishes between duplicate and non-duplicate
status.
We base our features not only on the newly-submitted
bug report under consideration, but also on a corpus of
previously-submitted bug reports. A key assumption of our
technique is that these features will be sufﬁcient to sepa-
rate duplicates from non-duplicates. In essence, we claim
that there are ways to tell duplicate bug reports from non-
duplicates just by examining them and the corpus of earlier
reports.
We cannot update the context information used by our
model after every new bug report; the overhead would be
too high. Our linear regression model speeds up process-
ing incoming reports because coefﬁcients can be calculated
ahead of time using historic bug data. A new report requires
only feature extraction, multiplication, and a test against a
cutoff. However, as more and more reports are submitted
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE54DSN 2008: Jalbert & Weimerthe original historic corpus becomes less and less relevant
to predicting future duplicates. We thus propose a system in
which the basic model is periodically (e.g., yearly) regener-
ated, recalculating the coefﬁcients and cutoff based on an
updated corpus of reports.
We now discuss the derivation of the important features
used in our classiﬁer model.
roughly 430 words associated with the ReqSimile tool [11].
Finally, we do not consider submission-related informa-
tion, such as the version of the browser used by the reporter
to submit the defect report via a web form, to be part of
the description text. Such information is typically colocated
with the description in bug databases, but we include only
textual information explicitly entered by the reporter.
4.1 Textual Analysis
4.1.1 Document Similarity
Bug reports include free-form textual descriptions and ti-
tles, and most duplicate bug reports share many of the same
words. Our ﬁrst step is to deﬁne a textual distance metric
for use on titles and descriptions. We use this metric as a
key component in our identiﬁcation of duplicates.
We adopt a “bag of words” approach when deﬁning sim-
ilarity between textual data. Each text is treated as a set of
words and their frequency: positional information is not re-
tained. Since orderings are not preserved, some potentially-
important semantic information is not available for later
use. The beneﬁt gained is that the size of the representation
grows at most linearly with the size of the description. This
reduces processing load and is thus desirable for a real-time
system.
We treat bug report titles and bug report descriptions
as separate corpora. We hypothesize that the title and de-
scription have different levels of importance when used to
classify duplicates. In our experience, bug report titles are
written more succinctly than general descriptions and thus
are more likely to be similar for duplicate bug reports. We
would therefore lose some information if we combined ti-
tles and descriptions together and treated them as one cor-
pus. Previous work presents some evidence for this phe-
nomenon: experiments which double the weighting of the
title result in better performance [15].
We pre-process raw textual data before analyzing it, tok-
enizing the text into words and removing stems from those
words. We use the MontyLingua tool [9] as well as some
basic scripting to obtain tokenized, stemmed word lists of
description and title text from raw defect reports. Tokeniza-
tion strips punctuation, capitalization, numbers, and other
non-alphabetic constructs. Stemming removes inﬂections
(e.g., “scrolls” and “scrolling” both reduce to “scroll”).
Stemming allows for a more precise comparison between
bug reports by creating a more normalized corpus; our
experiments used the common Porter stemming algorithm
(e.g., [7]).
We then ﬁlter each sequence against a stoplist of com-
mon words. Stoplists remove words such as “a” and “and”
that are present in text but contribute little to its comparative
meaning. If such words were allowed to remain, they would
artiﬁcially inﬂate the perceived similarity of defect reports
with long descriptions. We used an open source stoplist of
We are interested in measuring the similarity between two
documents within the same corpus; in our experiments all of
the descriptions form one corpus and all of the titles form
another. All of the documents in a corpus taken together
contain a set of n unique words. We represent each docu-
ment in that corpus by a vector v of size n, with v[i] related
to the total number of times that word i occurs in that docu-
ment. The particular value at position v[i] is obtained from
a formula that can involve the number of times word i ap-
pears in that document, the number of times it appears in
the corpus, the length of the document, and the size of the
corpus.
Once we have obtained the vectors v1 and v2 for two doc-
uments in the same corpus, we can compute their similarity
using the following formula in which v1 • v2 represents the
dot product:
similarity = cos(θ) = v1 • v2
|v1| × |v2|
That is, the closer two vectors are to colinear, then the
more weighted words the corresponding documents share
and thus, we assume, the more similar the meanings of the
two documents. Given this cosine similarity, the efﬁcacy of
our distance metric is determined by how we populate the
vectors, and in particular how we weight word frequencies.
4.1.2 Weighting for Duplicate Defect Detection
Inverse document frequency, which incorporates corpus-
wide information about word counts, is commonly used in
natural language processing to identify and appropriately
weight important words. It is based on the assumption that
important words are distinguished not only by the number
of times they appear in a certain text, but also by the in-
verse of the ratio of the documents in which they appear
in the corpus. Thus a word like “the” may appear mul-
tiple times in a single document, but will not be heavily-
weighted if it also appears in most other documents. The
popular TF/IDF weighting includes both normal term fre-
quency within a single document as well as inverse docu-
ment frequency over an entire corpus.
In Section 5 we present experimental evidence that in-
verse document frequency is not effective at distinguishing
duplicate bug reports. In our dataset, duplicate bug reports
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE55DSN 2008: Jalbert & Weimerof the same underlying defect are no more likely to share
“rare” words than are otherwise-similar unrelated pairs of
bug reports. We thus do not include a weighting factor cor-
responding to inverse document frequency. Our weighting
equation for textual similarity is:
wi = 3 + 2 log2 (count of word i in document)
Every position i in the representative vector of a bug report
v is determined based upon the frequency of term i and the
constant scaling factors present in the equation. Intuitively,
the weight of a word that occurs many times grows loga-
rithmically, rather than linearly. The constant factors were
empirically derived in an exhaustive optimization related to
our dataset, which ranges over all of the subprojects under
the Mozilla umbrella. Once we have each document repre-
sented as a weighted vector v, we can use cosine similarity
to obtain a distance between two documents.
A true distance metric is symmetric. However, we use
a non-symmetric “similarity function” for our training data:
textual distance is used as deﬁned above in general, but as a
special case the one-directional similarity of an original to
its duplicates is set to zero. We hypothesize that duplicates
will generally be more similar to the original bug report than
to unrelated reports. Because we are predicting if a report
is a duplicate and only one part of a duplicate-original pair
has that feature, textual similarity would be somewhat less
predictive if it were symmetric.
4.1.3 Clustering
We use our textual similarity metric to induce a graph in
which the nodes are defect reports and edges link reports
with similar text. We then apply a clustering algorithm to
this graph to obtain a set of clustered reports. Many com-
mon clustering algorithms require either that the number of
clusters be known in advance, or that clusters be completely
disjoint, or that every element end up in a non-trivial clus-
ter. Instead, we chose to apply a graph clustering algorithm
designed for social networks to the problem of detecting du-
plicate defect reports.
The graph cluster algorithm of Mishra et al. produces a
set of possibly-overlapping clusters given a graph with un-
weighted, undirected edges [10]. Every cluster discovered
is internally dense, in that nodes within a cluster have a high
fraction of edges to other nodes within the cluster, and also
externally sparse, in that nodes within a cluster have a low
fraction of edges to nodes not in the cluster. The algorithm
is designed with scalability in mind, and has been used to
cluster graphs with over 500,000 nodes. We selected it be-
cause it does not require foreknowledge of the number of
clusters, does not require that every node be in a non-trivial
cluster, and is efﬁcient in practice.
In addition, the algorithm produces a “champion”, or ex-
emplary node within the cluster that has many neighbors
within the cluster and few outside of it. In our experiments
in Section 5 we measure our predictive power as a duplicate
classiﬁer. In practice our distance metric and the champi-
ons of the relevant clusters can also be used to determine
which bug from an equivalence class of duplicates should
be presented to developers ﬁrst.
We obtain the required graph by choosing a cutoff value.
Nodes with similarity above the cutoff value are connected
by an edge. The cutoff similarity and the clustering parame-
teres used in our experiments were empirically determined.
4.2 Model Features
We use textual similarity and the results of clustering as
features for a linear model. We keep description similar-
ity and title similarity separate. For the incoming bug re-
port under consideration, we determine both the highest title
similarity and highest description similarity it shares with a
report in our historical data. Intuitively, if both of those val-
ues are low then the incoming bug report is not textually
similar to any known bug report and is therefore unlikely to
be a duplicate.
We also use the clusters from Section 4.1.3 to deﬁne a
feature that notes whether or not a report was included in