### Introduction

Bugs serve as a valuable measure of software quality [8]. By measuring the time taken to fix bugs in two software projects, we aim to predict whether a bug will eventually be resolved as a duplicate. Our focus is not on specific resolution times or the total lifetime of real bugs.

Our work closely aligns with that of Runeson et al. [15], who use textual similarity to analyze known-duplicate bug reports. In their experiments, they analyze bug reports known to be duplicates alongside a set of historical bug reports to generate a list of candidate originals for the duplicate. In Section 5.2, we demonstrate that our technique performs at least as well as theirs in this task. However, our primary focus is on using our model to detect unknown duplicates rather than correctly categorizing known duplicates.

### Modeling Duplicate Defect Reports

Our objective is to develop a model that uses easily-gathered surface features and textual semantics to predict if a newly-submitted report is likely to be a duplicate of a previous report. Given that many defect reports are duplicates (e.g., 25.9% in our dataset), automating this part of the bug triage process would free up developers' time for other tasks, such as addressing defects and improving software dependability.

The core of our system is a formal model that extracts certain features from each bug report in a bug tracker. When a new bug report arrives, our model uses these feature values to predict its eventual duplicate status. Duplicate bugs are not directly presented to developers to reduce triage costs.

We employ linear regression over properties of bug reports as the basis for our classifier. Linear regression offers several advantages:
1. **Off-the-shelf software support**: This decreases the barrier to entry for using our system.
2. **Rapid classifications**: It allows us to add textual semantic information and still perform real-time identification.
3. **Easy component examination**: This enables qualitative analysis of the features in the model.

Linear regression produces continuous output values based on continuously-valued features. To create a binary classifier, we specify features and an output value cutoff that distinguishes between duplicate and non-duplicate status.

Our features are based on both the newly-submitted bug report and a corpus of previously-submitted reports. We assume these features will be sufficient to separate duplicates from non-duplicates. Essentially, we claim that there are ways to identify duplicate bug reports by examining them and the corpus of earlier reports.

Updating the context information used by our model after every new bug report would be too resource-intensive. Instead, our linear regression model speeds up processing incoming reports by pre-calculating coefficients using historic bug data. A new report requires only feature extraction, multiplication, and a test against a cutoff. As more reports are submitted, the original historic corpus becomes less relevant. Therefore, we propose a system where the model is periodically (e.g., yearly) regenerated, recalculating the coefficients and cutoff based on an updated corpus of reports.

### Feature Derivation

#### Textual Analysis

##### Document Similarity

Bug reports include free-form textual descriptions and titles, and most duplicate bug reports share many of the same words. Our first step is to define a textual distance metric for use on titles and descriptions, which is a key component in identifying duplicates.

We adopt a "bag of words" approach, treating each text as a set of words and their frequencies without retaining positional information. This reduces the size of the representation, making it suitable for real-time systems.

We treat bug report titles and descriptions as separate corpora, hypothesizing that they have different levels of importance when classifying duplicates. Titles are often more succinct and thus more likely to be similar for duplicate bug reports. Combining titles and descriptions could result in a loss of information. Previous work supports this hypothesis, showing that doubling the weighting of the title improves performance [15].

We preprocess raw textual data by tokenizing the text into words and removing stems. We use the MontyLingua tool [9] and basic scripting to obtain tokenized, stemmed word lists from raw defect reports. Tokenization removes punctuation, capitalization, numbers, and other non-alphabetic constructs. Stemming normalizes the corpus, reducing words like "scrolls" and "scrolling" to "scroll". We use the Porter stemming algorithm [7] for this purpose.

We then filter each sequence against a stoplist of common words, such as "a" and "and", which contribute little to comparative meaning. Using an open-source stoplist, we ensure that long descriptions do not artificially inflate perceived similarity.

Each document in a corpus is represented by a vector v of size n, where v[i] is related to the total number of times word i occurs in the document. The value at position v[i] is obtained from a formula involving the word's frequency in the document, the corpus, the document length, and the corpus size.

To compute the similarity between two documents in the same corpus, we use the cosine similarity formula:
\[ \text{similarity} = \cos(\theta) = \frac{v1 \cdot v2}{|v1| \times |v2|} \]
This measures how close two vectors are to being colinear, indicating shared weighted words and, thus, similar meanings.

##### Weighting for Duplicate Defect Detection

Inverse document frequency (IDF) is commonly used in natural language processing to weight important words. However, in our dataset, duplicate bug reports of the same underlying defect are no more likely to share "rare" words than otherwise-similar unrelated pairs. Therefore, we do not use IDF. Our weighting equation for textual similarity is:
\[ w_i = 3 + 2 \log_2 (\text{count of word i in document}) \]

Each position i in the representative vector v is determined based on the frequency of term i and constant scaling factors. These factors were empirically derived from our dataset, which includes all subprojects under the Mozilla umbrella. Once each document is represented as a weighted vector, we use cosine similarity to obtain the distance between two documents.

We use a non-symmetric "similarity function" for our training data: textual distance is generally defined as above, but the one-directional similarity of an original to its duplicates is set to zero. We hypothesize that duplicates are generally more similar to the original bug report than to unrelated reports.

##### Clustering

We use our textual similarity metric to induce a graph where nodes are defect reports and edges link reports with similar text. We apply a clustering algorithm designed for social networks to detect duplicate defect reports. The graph clustering algorithm of Mishra et al. [10] produces possibly-overlapping clusters, ensuring internal density and external sparsity. This algorithm is scalable and does not require foreknowledge of the number of clusters or that every node be in a non-trivial cluster.

The algorithm also produces a "champion" node within each cluster, which has many neighbors within the cluster and few outside. In practice, our distance metric and the champions of the relevant clusters can help determine which bug from an equivalence class of duplicates should be presented to developers first.

We obtain the required graph by choosing a cutoff value, connecting nodes with similarity above the cutoff. The cutoff similarity and clustering parameters were empirically determined.

### Model Features

We use textual similarity and clustering results as features for our linear model. We keep description similarity and title similarity separate. For a new bug report, we determine the highest title and description similarities it shares with a report in our historical data. If both values are low, the new report is unlikely to be a duplicate.

We also use the clusters to define a feature noting whether a report was included in a cluster.