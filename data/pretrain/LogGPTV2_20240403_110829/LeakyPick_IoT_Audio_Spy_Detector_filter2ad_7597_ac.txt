as an example of interacting with LeakyPick. However, optimizing
the usability of the user interactions for obtaining the report is not
in the core focus of this paper.
4.2 Wake-Word Selection
Users are not always aware of IoT devices that require a wake-
word before transmitting audio recordings to the cloud. While it
is possible enumerate the wake-words for known voice assistants,
recent reports of third-party contractors reviewing voice assistant
accuracy [13, 18, 23, 24, 27, 31] highlight the significance of false
voice assistant activation. Therefore, LeakyPick identifies other
wake-words that will trigger the voice detection. Note that this
approach is different than using mangled voice samples [7, 46] or
other means to attack the voice recognition process [8, 40, 49]. We
also do not want to limit LeakyPick to words sounding similar to the
known wake-words in order to confuse the voice assistant [28, 51].
Using a full dictionary of the English language is impractical. It
would take roughly 40 days to test a voice assistant with the entire
dictionary of 470,000 words [48] at a speed of one word every
seven seconds. However, by only testing words with a phoneme
count similar to the known wake-word, the subset of viable words
is manageable. Our intuition is that a benign device will more
likely confuse words with a similar structure. Therefore, we select
all words in a phoneme dictionary [39] with the same or similar
phoneme count than the actual wake-word. We also used random
words from a simple English word list [26]. These words are spoken
using a text-to-speech (TTS) engine.
4.3 System Implementation
The LeakyPick probing device injects audio probes into the userâ€™s
environment and analyzes the resulting device network traffic. Our
current implementation achieves this functionality using the fol-
lowing hardware set-up. The probing device consists of a Raspberry
Pi 3B [36] connected via Ethernet to the network gateway. It is also
connected via the headphone jack to a PAM8403 [15] amplifier
board, which is connected to a single generic 3ğ‘Š speaker.
Pre-print of paper to be published at ACSAC2020
acmDOI: 10.1145/3427228.3427277
Table 2: Devices used for evaluation
Device Type
Smart Speaker
Security System
Microphone-Enabled
IoT Device
Device Name
Echo Dot (Amazon Alexa)
Google Home (Google Assistant)
Home Pod (Apple Siri)
Hive Hub 360
Netatmo Welcome
Netamo Presence
Nest Protect
Hive View
To capture network traffic, we use a TP-LINK TL-WN722N [45]
USB Wifi dongle to create a wireless access point using hostapd
and dnsmasq as the DHCP server. All wireless IoT devices connect
to this access point. To provide Internet access, we activate packet
forwarding between the eth (connected to the network gateway)
and wlan interfaces. Alternatively the device could sniff Wi-Fi pack-
ets without being connected to the network using packet size and
MAC address as the features. This approach would also work for
foreign Wi-Fi networks, as it is not required to have the decrypted
traffic, i.e. our device does not need to be connected to that network
at all: package size information is sufficient.
Finally, LeakyPick is written in Python. It uses tcpdump to record
packets on the wlan interface. We use Googleâ€™s text-to-speech (TTS)
engine to generate the audio played by the probing device.
5 EVALUATION
This section evaluate LeakyPickâ€™s ability to detect when audio
recordings are being streamed to cloud servers. Specifically, we
seek to answer the following research questions.
RQ1 What is the detection accuracy of the burst detection and
statistical probing approaches used by LeakyPick?
RQ2 Does audio probing with a wrong wake-word influence the
detection accuracy?
RQ3 How well does LeakyPick perform on a real-world dataset?
RQ4 How does LeakyPickâ€™s statistical probing approach compare
to machine learning-based approaches?
RQ5 Can LeakyPick discover unknown wake-words?
5.1 Experimental Setup
Our evaluation considers 8 different wireless microphone-enabled
IoT devices: 3 smart speakers, 1 security system that detects glass
breaking and dogs barking, and 4 microphone-enabled IoT devices,
namely the audio event detecting smart IP security cameras Ne-
tatmo Welcome, Netatmo Presence and Hive View as well as the
smart smoke alarm Nest Protect. Table 2 lists the specific devices.
We now describe our dataset collection and evaluation metrics.
5.1.1 Datasets. We used four techniques to collect datasets for
our evaluation. Table 3 overviews these four collection methodolo-
gies, as well as to which devices the datasets apply. The following
discussion describes our collection methodology.
Idle Datasets: The idle dataset was collected in an empty office room.
It consists of network traffic collected over six hours during which
the device was not actively used and no audio inputs were injected.
We also made sure to record at least one occurrence of every traffic
6
Richard Mitev, Anna Pazii, Markus Miettinen, William Enck, and Ahmad-Reza Sadeghi
pattern the devices produce (e.g., for Echo devices every type of
periodic bursts).
Controlled Datasets - Burst Detection: The controlled datasets for
burst detection were collected in an empty office room while inject-
ing audio probes approximately 100 times for each of the studied
devices. In all cases, the injected probe was the known wake-word
for the device in question. The Hive 360 Hub device does not use a
wake-word, but is activated by specific noise like dog barking and
glass shattering. For this device we therefore used recordings of
dog barking sounds to trigger audio transmission. For each device,
three different datasets were collected by varying the wake-word
invocation interval between 1, 5, and 10 minutes.
Controlled Datasets - Statistical Probing: The collection of the con-
trolled dataset for statistical probing was performed in a way similar
to the burst detection dataset. However, the experiment collected
six datasets for each device. Each dataset consisted of six hours of
invoking the wake-word at intervals ranging from two minutes
to two hours. Thereby resulting in datasets with varying ratios of
audio-related and idle background traffic.
Online Probing Datasets: Using live traffic of the 8 different devices
listed in Table 2 we randomly selected a set of 50 words out of
the 1000 most used words in the English language [16] combined
with a list of known wake-words of voice-activated devices as
possible wake-words to test. We configured our probing device
to alternatingly record silence traffic ğ‘‡ğ‘  and probing traffic ğ‘‡ğ‘ğ‘Ÿ of
one minute duration each for every wake-word in the list. ğ‘‡ğ‘ğ‘Ÿ was
recorded immediately after the device started playing a word from
the list repeating the word every 10 seconds in this minute.
Real-World Datasets: To create a realistic dataset for evaluation, we
collected data from the three smart speakers over a combined period
of 52 days in three different residential environments (houses). The
times for each smart speaker are listed in Table 4. During this time
period, humans used the assistants as intended by the manufacturer.
In order to evaluate the accuracy of LeakyPick, the dataset was
labeled by recording the timestamps of when the device was record-
ing audio. This was accomplished by taking advantage of the visual
indicator (e.g., a light ring that glows) that Smart speakers use to
alert the user when the voice assistant is activated in response to
voice inputs. We therefore automated the labeling process in the
real-world environment by creating a small specialized device with
a light sensor to measure the visual indicator. Our device consisted
of a Raspberry Pi and a Light Dependent Resistor (LDR) in con-
junction with a LM393 [43] analogue-digital comparator. The LDR
sensor was then attached to the smart speakerâ€™s visual indicator
and protected from environmental luminosity with an opaque foil.
This setup allowed the Raspberry Pi to record a precise timestamp
each time the device was activated and allowed us to label periods
with audio activity in the dataset accordingly.
5.1.2 Evaluation metrics. We evaluate the performance of our de-
tection approach in terms of true positive rate (TPR) and false posi-
tive rate (FPR). The true positive rate is defined as ğ‘‡ ğ‘ƒğ‘… =
ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ ,
where TP is true positives and FN false negatives, resulting in the
fraction of audio events correctly identified as such. Similarly, false
positive rate is defined as ğ¹ ğ‘ƒğ‘… =
ğ‘‡ ğ‘ + ğ¹ ğ‘ƒ , where TN is the true
ğ‘‡ ğ‘ƒ
ğ¹ ğ‘ƒ
LeakyPick: IoT Audio Spy Detector
Table 3: Datasets for Burst Detection, Statistical Probing, On-
line Probing and Machine Learning
Dataset
Idle
Frequency
-
Controlled -
Burst Detection
1min, 5min, 10min
Controlled -
Statistical Probing
2min, 5min, 10min,
30min, 1h, 2h
Online Probing
Real-World
10ğ‘  during probing
windows
real-world,
cf. Table 4
Devices
Echo Dot,
Google Home,
Home Pod,
Hive 360 Hub
Echo Dot,
Google Home,
Home Pod,
Hive 360 Hub
Echo Dot,
Google Home,
Home Pod,
Hive 360 Hub
all, cf. Table 2
Echo Dot,
Google Home,
Home Pod
Table 4: Duration of collected data in different residential
environments (households) while used by humans
Pre-print of paper to be published at ACSAC2020
acmDOI: 10.1145/3427228.3427277
Figure 3: Results of BurstDetector using known wake-words
detecting outgoing audio transmissions of Echo Dot, Google
Home, Home Pod and Hive 360 Hub on the controlled data
set
Amazon Echo Dot Google Home Apple Home Pod
31d
15d
15d
negatives and FP the false positives. It denotes the fraction of non-
audio events falsely identified as audio events. Ideally we would like
our system to maximize TPR, i.e., the capability to identify devices
sending audio to the cloud, while minimizing FPR, i.e., generating
as few as possible false detections of audio transmissions.
5.2 RQ1: Detection Accuracy
In this section we evaluate the detection accuracy of our two ap-
proaches: (1) burst detection and (2) statistical probing.
5.2.1 Burst Detection. To evaluate the performance of Burst De-
tection for detecting audio transmissions, we used the controlled
dataset for burst detection (Table 4) to determine its ability to detect
audio transmissions correctly.
Figure 3 shows the receiver operating characteristic (ROC) curve
for the burst detection approach. The ROC curve varies the parame-
ter ğ‘›, which defines the number of consecutive windows with high
data throughput required for triggering detection (cf. Section 4.1.1),
from ğ‘›ğ‘šğ‘–ğ‘› = 1 to ğ‘›ğ‘šğ‘ğ‘¥ = 8. As can be seen, with ğ‘› = 5 consecutive
time windows, detection is triggered with a ğ‘‡ ğ‘ƒğ‘… of 96% and an ğ¹ ğ‘ƒğ‘…
of 4% (averaged over all devices). This is explained by the fact that
as mentioned in Section 3, the voice-activated devices typically send
only a small amount of data unless they are active: medium bursts
every few minutes and large bursts only every few hours when idle.
This allows Burst Detection to identify nearly all audio invocations
as they are clearly distinguishable from idle traffic, making this
approach practical for devices with such behavioral characteristics.
Statistical Probing. To evaluate the ability of LeakyPick to
5.2.2
detect whether a device reacts to audio events, we first determine
whether the statistical properties of data traffic of IoT devices when
in idle mode (i.e., not in relation to audio events) is statistically
Figure 4: The resulting ğ‘-value when traffic of devices being
invoked in intervals from 2 minutes to 2 hours compared to
known silence, showing that the ğ‘-value decreases with an
increasing number of audio bursts in the traffic
different from the devicesâ€™ behavior when transmitting audio to
the cloud. For this, we calculate the statistical difference of the
packet size distributions in the Idle dataset to the packet distri-
butions in the controlled datasets for statistical probing (Table 3)
using the ğ‘¡-test as discussed in Section 4.1.2. The results are shown
in Figure 4, showing the resulting ğ‘-value in dependence of the
frequency of invocations of the corresponding deviceâ€™s wake-word.
As can be seen, for all tested voice-controlled devices, the ğ‘-value
decreases the more often the wake-word is injected, i.e., the more
audio-transmissions the dataset contains. This suggests that the
distributions of packet sizes related to audio transmission indeed
are different to the distribution of packets in the background traffic
and can be thus utilized to identify audio transmissions.
Figure 5 shows the ROC curve for our approach on the con-
trolled dataset for statistical probing (Table 4) for different ğ‘-value
thresholds. We use a sliding window approach and compare two
consecutive windows of 30 seconds duration using the test, moving
the window for 30 seconds to get the new window. We compare
7
n = 500,10,20,30,40,50,60,70,80,9100,20,40,60,81TPRFPR360 HubGoogleAlexaSiri00.10.20.30.40.50.60.70.80.912h1h30m10m5m2mp-valueinvocation frequencyAlexaGoogleSiri360 HubPre-print of paper to be published at ACSAC2020
acmDOI: 10.1145/3427228.3427277
Richard Mitev, Anna Pazii, Markus Miettinen, William Enck, and Ahmad-Reza Sadeghi
Figure 5: ROC graph of comparing consecutive windows of
30 seconds of traffic of the Controlled - Statistical probing
dataset using the ğ‘¡-test for different ğ‘-value thresholds and
comparing the output to the actual labels of the traffic
Figure 6: LeakyPick ğ‘¡-test ğ‘-values for probing Amazon
Echo during 100 alternating windows of 1 minute of idle
traffic and probing with wake-word â€œAlexaâ€ at 10-second in-
tervals, respectively
the result with the actual label of this traffic region to assess if our
approach can reliably find exactly the device sending audio data. As
can be seen, for a ğ‘-value threshold of 0.42 or 0.43 a True Positive
Rate of 94% with a simultaneous False Positive Rate of 6% averaged
over all devices can be achieved for these datasets.