Empirically, we ﬁnd that the convergence performance of
MCF is not good (we will demonstrate it via experiment
in Section 6). We believe that this is because MCF always
changes Ds to make it completely consistent with the current
marginal in each step. Doing this reduces the error of the
target marginal close to zero, but increases the errors for other
marginals to a large value.
To handle this issue, we borrow the idea of multiplicative
update [8] and propose a new approach that Gradually Update
Ds based on the Marginals; and we call it GUM. GUM also
adopts the ﬂow graph introduced by MCF, but differs from
MCF in two ways: First, GUM does not make Ds fully consis-
tent with the given marginal in each step. Instead, it changes
Ds in a multiplicative way, so that if the original frequency
in a cell is large, then the change to it will be more. In partic-
ular, we set a parameter α, so that for cells that have values
are lower than expected (according to the target marginal),
we add at most α times of records, i.e., min{nt − ns,αns} 1,
where nt is the number in the marginal and ns is the number
1Notice that α could be greater than 1 since ns < nt. In the experiments,
we always set α to be less than 1 to achieve better convergence performance.
from Ds. On the other hand, for cells with values higher than
expected, we will reduce min{ns − nt ,βns} records that sat-
isfy it. As the total number of record is ﬁxed, given α, β can
be calculated.
Figure 3 gives a running example. Before updating, we
have 4 out of 5 records have the combination (cid:104)high,male(cid:105),
and 1 record has (cid:104)high, f emale(cid:105). To get closer to the target
marginal of 0.2 and 0.8 for these two cells, we want to change
2 of the (cid:104)high,male(cid:105) records to be (cid:104)high, f emale(cid:105). In this
example, we have α = 2.0,β = 0.5 2 and do not completely
match the target marginal of 0.2 and 0.8. To this end, one
approach is to simply change the Gender attribute value from
male to female in these two records as in MCF. We call this a
Replace operation. Replacing will affect the joint distribution
of other marginals, such as {Gender,Age}. An alternative is
to discard an existing (cid:104)high,male(cid:105) record, and Duplicate an
existing (cid:104)high, f emale(cid:105) record (such as v5 in the example).
Duplicating an existing record help preserve joint distribu-
tions between the changed attributes and attributes not in
the marginal. However, Duplication will not introduce new
records that can better reﬂect the overall joint distribution. In
particular, if there is no record that currently has the combina-
tion (cid:104)high, f emale,elderly(cid:105), duplication cannot be used.
Therefore, we need to use a combination of Replacement
and Duplication (which is the case in Figure 3). Furthermore,
once the synthesized dataset is getting close to the distribu-
tion, we would prefer Duplication to Replacement, since at
that time there should be enough records to reﬂect the distribu-
tion and Replacement disrupts the joint distribution between
attributes in a marginal and those not in it. We empirically
compare different record updating strategies and validate that
introducing the Duplication operation can effectively improve
the convergence performance. Due to space limitation, we
refer the readers to Appendix I in out technical report [55] for
the experimental results.
2We have α = nt−ns
ns
for under-counted cells and β = ns−nt
ns
for over-
counted cells. The number of records for under-counted cell (cid:104)high, female,∗(cid:105)
increase from 1 to 3; thus α = 3−1
1 = 2. The number of records for over-
counted cell (cid:104)high, male,∗(cid:105) decrease from 4 to 2; thus β = 4−2
4 = 0.5.
936    30th USENIX Security Symposium
USENIX Association
Improving the Convergence
5.3
Given the general data synthesize method, we have several
optimizations to improve its utility and performance. First,
to bootstrap the synthesizing procedure, we require each at-
tribute of Ds follows the 1-way noisy marginals when we
initialize a random dataset Ds.
Gradually Decreasing α. The update rate α should be
smaller with the iterations to make the result converge. From
the machine learning perspective, gradually decreasing α can
effectively improve the convergence performance. There are
some common practices [1] of setting α.
• Step decay: α = α0 · k(cid:98) t
s(cid:99), where α0 is the initial value, t is
the iteration number, k is the decay rate, and s is the step
size (decrease α every s iterations). The main idea is to
reduce α by some factor every few iterations.
• Exponential decay: α = α0 · e−kt, where k is a hyperparam-
• Linear decay: α = α0
1+kt .
• Square root decay: α = α0√
eter. This exponentially decrease α in each iteration.
1+kt .
We empirically evaluate the performance of different decay
algorithms (refer to Appendix J in our technical report [55])
and ﬁnd that step decay is preferable in all settings. The step
decay algorithm is also widely used to update the step size in
the training of deep neural networks [33].
Attribute Appending. The selected marginals X output by
Algorithm 2 can be represented by a graph G. We notice that
some nodes have degree 1, which means the corresponding
attributes are included in exactly one marginal. For these at-
tributes, it is not necessary to involve them in the updating
procedure. Instead, we could append them to the synthetic
dataset Ds after other attributes are synthesized. In particular,
we identify nodes from G with degree 1. We then remove
marginals associated with these nodes from X . The rest of
the noisy marginals are feed into GUM to generate the syn-
thetic data but with some attributes missing. For each of these
missed attributes, we sample a smaller dataset Ds’ with only
one attribute, and we concatenate Ds’ to Ds using the marginal
associated with this attribute if there is such a marginal; oth-
erwise, we can just shufﬂe Ds’ and concatenate it to Ds. Note
that this is a one time operation after GUM is done. No syn-
thesizing operation is needed after this step.
Separate and Join. We observe that, when the privacy bud-
get is low, the number of selected marginals is relatively small,
and the dependency graph is in the form of several disjoint
subgraphs. In this case, we can apply GUM to each subgraph
and then join the corresponding attributes. The beneﬁt of
Separate and Join technique is that, the convergence perfor-
mance of marginals in one subgraph would not be affected
by marginals in other subgraph, which would improve the
overall convergence performance.
Filter and Combine Low-count Values. If some attributes
have many possible values while most of them have low
counts or do not appear in the dataset. Directly using these at-
tributes to obtain pairwise marginals may introduce too much
noise. To address this issue, we propose to ﬁlter and com-
bine the low-count values. The idea is to spend a portion of
privacy budget to obtain the noisy one-way marginals. After
that, we keep the values that have count above a threshold θ.
For the values that are below θ, we add them up, if the total
is below θ, we assign 0 to all these values. If their total is
above θ, then we create a new value to represent all values
that have low counts. After synthesizing the dataset, this new
value is replaced by the values it represents using the noisy
one-way marginal. The threshold is set as θ = 3σ, where σ
is the standard deviation for Gaussian noises added to the
one-way marginals.
5.4 Putting Things Together: PrivSyn
Algorithm 3 illustrates the overall workﬂow of PrivSyn. We
split the total privacy budget into three parts. The ﬁrst part
is used for publishing all 1-way marginals, intending to ﬁlter
and combine the values with low count or do not exist. The
second part is used to differentially privately select marginals.
The marginal selection method DenseMarg consists of two
components, i.e., 2-way marginal selection (Algorithm 1) and
marginal combine (Algorithm 2). The third part is used to
obtain the noisy combined marginals. After obtaining the
noisy combined marginals, we can use them to construct
synthetic dataset Ds without consuming privacy budget, since
this is a post processing procedure.
Algorithm 3: PrivSyn
Input: Private dataset Do, privacy budget ρ;
Output: Synthetic dataset Ds;
1 Publish 1-way marginals using GM with ρ1 = 0.1ρ;
2 Filter values with estimates smaller than 3σ;
3 Select 2-way marginals with Algorithm 1 and ρ2 = 0.1ρ;
4 Combine marginals using Algorithm 2;
5 Publish combined marginals using GM with ρ3 = 0.8ρ;
6 Make noisy marginals consistent;
7 Construct Ds using GUM;
6 Evaluation
In this section, we ﬁrst conduct a high-level end-to-end ex-
periment to illustrate the effectiveness of PrivSyn. Then, we
evaluate the effectiveness of each step of PrivSyn by ﬁxing
other steps. As a highlight, our method consistently achieves
better performance than the state-of-the-art in all steps.
6.1 Experimental Setup
Datasets. We run experiments on the following four datasets.
• UCI Adult [9]. This is a widely used dataset for classiﬁ-
• US Accident [42]. This is a countrywide trafﬁc accident
cation from the UCI machine learning repository.
dataset, which covers 49 states of the United States.
USENIX Association
30th USENIX Security Symposium    937
issued from 2007 to 2015.
• Loan [31]. This dataset contains loan data in lending club
• Colorado [43]. This is the census dataset of Colorado
State in 1940. This dataset is used in the ﬁnal round of the
NIST challenge [43].
The detailed information about the datasets are listed in
Table 2, where the label column stands for the label used in
the classiﬁcation task.
Dataset
Adult
US Accident
Loan
Colorado
Records
Attributes
48,000
600,000
600,000
662,000
15
30
81
97
Domain
6· 1017
3· 1039
4· 10136
5· 10162
Label
salary
Severity
home_ownership
INCNONWG
Table 2: Summary of datasets used in our experiments.
Tasks and Metrics. We evaluate the statistical performance
of the synthesized datasets on three data analysis tasks. For
each data analysis task, we adopt its commonly used metric
to measure the performance.
• Marginal Release. We compute all the 2-way marginals
and use the average (cid:96)1 error to measure the performance.
• Range Query. We randomly sample 1000 range queries,
each contains 3 attributes. We use the average (cid:96)1 error
to measure the performance. In particular, we calculate
1|Q| ∑qi∈Q|ci − ˆci|, where Q is the set of randomly sampled
queries, ci and ˆci are the ratio of records that fall in the
range of query qi in the original dataset and synthesized
dataset, respectively.
• Classiﬁcation. We use the synthesized dataset to train an
SVM classiﬁcation model, and use misclassiﬁcation rate to
measure the performance.
Competitors. We compare each component of PrivSyn with
a series of other methods, respectively.
• Marginal Selection Methods. We compare our pro-
posed DenseMarg method (Algorithm 1) with PrivBayes.
The computational complexity of dependency in original
PrivBayes method is too high. Thus, we replace the de-
pendency calculation part of PrivBayes by our proposed
InDif metric, which we call PrivBayes(InDif). For Col-
orado dataset, the PGM team open sourced a set of manu-
ally selected marginals in the NIST challenge [43], which
serves as an alternative competitor.
• Noise Addition Methods. We compare our pro-
posed Weighted Gaussian method with Equal Laplace and
Equal Gaussian methods. Both Gaussian methods use
zCDP to compose, and the Laplace mechanism use the
naive composition, i.e., evenly allocate ε for each marginal.
• Data Synthesis Methods. We compare PrivSyn with
PrivBayes and PGM, which use the selected marginals to
estimate a graphical model, and sample synthetic records
from it. Note that we have two versions of synthesis meth-
ods for PrivSyn, i.e., MCF and GUM.
We also compare with a few other algorithms that do not
follow the framework in Section 3.
• DualQuery.
It generates records in a game theoretical
manner. The main idea is to maintain a distribution over a
workload of queries. One ﬁrst samples a set of queries from
the workload each time, and then generates a record that
minimize the error of these queries. We refer the readers to
Section 7 for detailed discussion.
• For the classiﬁcation task, we have another two competitors,
i.e., Majority and NonPriv. Majority represents the naive
method that blindly predicts the label by the majority label.
Methods that perform worse than Majority means that the
published dataset doesn’t help the classiﬁcation task, since
the majority label can be outputted correctly even under
very low privacy budget. NonPriv represents the method
without enforcing differential privacy, it is the best case to
aim for. For NonPriv, we split the original dataset into two
disjoint parts, one for training and another for testing.
Experimental Setting. For PrivBayes, PGM and PrivSyn
methods, we set the number of synthesized records the same
as that of the original dataset. Notice that we adopt unbounded
differential privacy [32] in this paper, we cannot directly ac-
cess the actual number of records in the original dataset. Thus,
we instead use the total count of marginals to approximate it.
For DualQuery method, the number of synthesized records is
inherently determined by the privacy budget, the step size and
the sample size [28]. We use the same hyper-parameter set-
tings as [28], i.e., the step size is 2.0 and sample size is 1000.
We illustrate the impact of the number of synthesized records
on PrivSyn in Appendix K of our technical report [55]. By
default, we set δ = 1
n2 for all methods, where n is the number
of records in original dataset.
All algorithms are implemented in Python 3.7 and all the
experiments are conducted on a server with Intel Xeon E7-
8867 v3 @ 2.50GHz and 1.5TB memory. We repeat each
experiment 5 times and report the mean and standard devia-
tion. Due to space limitation, we put the experimental results
of US Accident and Colorado in the main context, and re-
fer the readers to the results of Adult and Loan datasets to
Appendix L in our technical report [55].
6.2 End-to-end Comparison
Setup. For fair comparison, we use the optimal compo-
nents and hyper-parameters for all methods. Concretely, we
use PrivBayes(InDif) to select marginals for PrivBayes and
PGM, since they can only handle sparse marginals. Both
PrivSyn and DualQuery can handle dense marginals; thus we
use DenseMarg to select marginals for them. For noise ad-
dition, we use Weighted Gaussian for PrivBayes, PGM and
PrivSyn. DualQuery uses a game theoretical manner to gener-
ate synthetic datasets; thus it does not need the noise addition
938    30th USENIX Security Symposium
USENIX Association
(a) pair-wise marginal
(b) range query
US Accident
(c) classiﬁcation