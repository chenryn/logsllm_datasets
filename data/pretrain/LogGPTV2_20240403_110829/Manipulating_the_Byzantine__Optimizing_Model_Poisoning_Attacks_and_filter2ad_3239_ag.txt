malicious gradients from a set of benign and malicious gradi-
ents. Lemma 1 formalizes the theoretical ﬁltering guarantees
of DnC.
Lemma 1. Consider 0 <  < 0.5 and two distributions B, M
with means µB, µM and covariances ΣB, ΣM (cid:22) σ2I. Let
U = (1 − )B + M be a mixture of samples from B and M.
Then B and M are -spectrally separable if (cid:107)µB − µM(cid:107)2
2 ≥
6σ2
 .
For our FL poisoning setting, Lemma 1 implies that if
the means of poisoned and benign gradients are sufﬁciently
separated, then the two types of gradients can be reliably
separated using spectral methods. Figure 5, demonstrates this
exactly: the means of malicious gradients which effectively
poison FL are sufﬁciently far from the means of benign
gradients, and therefore, spectral methods can ﬁlter them. On
the other hand, the malicious gradients which circumvent the
criterion given in Lemma 1 have no impact on the accuracy of
global model. We note that the result in Lemma 1 is common
to SVD based outliers detection [15], [36], [16], [28], [11]; we
provide it here for completeness and to give the intuition about
the efﬁciency of DnC. Appendix B gives the formal proof of
Lemma 1.
An adaptive attack against DnC. DnC provides provable
theoretical guarantees on detection of malicious gradients.
However,
to provide empirical evidence on the robustness
guarantees of DnC, we propose an adaptive attack by against
the strongest agr-updates adversary who has the complete
knowledge of the gradients of benign devices and of DnC.
Our adaptive attack is based on the general optimization
framework proposed in Section IV-A. The attack is inspired
by our AGR-tailored attack on Multi-krum AGR, because both
DnC and Multi-krum compute a selection set and average the
gradients in the ﬁnal selection set. The intuition of the attack
is to maximize the number of malicious gradients selected by
DnC to maximize the bad impact on the ﬁnal aggregate. This
also ensures that the number of benign gradients selected and
their good impact on the ﬁnal aggregate are minimized. Hence,
our adaptive attack’s optimization is:
argmax
γ
m = |{∇ ∈ ∇m{i∈[m]}|∇ ∈ ∇m{i∈Iﬁnal}}|
∇m
i∈[m] = favg(∇{i∈[n]}) + γ∇p
(8)
14
where m is the number of malicious clients, Iﬁnal is the ﬁnal
set of candidate indices selected by DnC, ∇p is perturbation,
and γ is scaling factor. Note that, it is reasonable to assume
that although the adversary knows DnC algorithm thoroughly,
she cannot know the exact random indices r from Algorithm 2.
Finally, we solve the optimization in (8) by ﬁnding the most
impactful γ using Algorithm 1.
C. Evaluation of Our Defense
In this section, we ﬁrst demonstrate the robustness of our
DnC AGR against state-of-the-art [17], [4] and our model
poisoning attacks from Sections IV and VII-B for iid datasets.
We also analyze the effectiveness of spectral separability, and
therefore of DnC, in defending against model poisoning on
FL. Finally, we discuss the effectiveness of DnC for non-iid
FEMNIST dataset.
1) Robustness of DnC for iid data: For iid datasets, i.e.,
MNIST, CIFAR10, and Purchase, we evaluate DnC against
a strong adversarial setting with 20% malicious clients and
the adversaries with complete knowledge of the gradients of
benign clients, i.e., agr-updates when AGR is known and
updates-only when AGR is unknown. We evaluate DnC
using Fang and LIE, and our stronger AGR-tailored and AGR-
agnostic attacks. For all these datasets, we set niters, c, and b
in Algorithm 2 to 1, 1, and 10,000, respectively.
Robustness comparison with previous AGRs. Table IV
shows, for each of the attacks, the attack impact on DnC;
in parentheses, we show the impact of the attack on the
most of existing AGRs, e.g., for Fang attack on CIFAR10 +
Alexnet, Bulyan is the most robust AGR, hence, for CIFAR10
+ Alexnet, we show the impact of Fang attack on Bulyan.
Below, we analyze the AGRs based on the increase in
accuracy of the global model under the strongest of the attacks,
i.e., based on the minimum A∗
θ (Section V-B) for the AGR.
For an AGR, the minimum A∗
θ is obtained by subtracting the
impact of the strongest attack, Iθ, from ‘No attack’ accuracy,
Aθ. For instance, for CIFAR10 + Alexnet, our adaptive attack
is the strongest attack against DnC and the corresponding
minimum A∗
θ is 61.5% (as Aθ is 67.6% and the maximum Iθ
is 6.1%). While our AGR-tailored attack is the strongest attack
against the best of the existing AGRs, thus the minimum A∗
θ
is 30.8% (as Aθ is 67.6% and the maximum Iθ is 32.5%).
Hence, for CIFAR10 + Alexnet, DnC increases A∗
θ from
30.8% to 61.5% (∼100% increase). For CIFAR10 + VGG11,
θ from 43.0% to 69.2% (∼150%
DnC increases the minimum A∗
increase). For Purchase, DnC increase the minimum A∗
θ from
88.6% to 90.2%.
DnC increases the minimum A∗
θ for MNIST from 90.7%
(93.2% − 2.5%) to 94.3% (96.2% − 1.9%). Although, the
absolute increase due to DnC is small for MNIST,
is
signiﬁcant due to the simplicity of the tasks.
it
Robustness comparisons under cross-device FL setting.
Now we compare robustness of previous AGRs and our DnC
when cross-device FL is used. We use CIFAR10 dataset with
Alexnet and VGG11 architectures; Table V shows the results in
similar fashion as Table IV. As before, we analyze robustness
of an AGR based on the minimum A∗
θ for the AGR. We
Figure 5: DnC selects high fractions of malicious gradients (red plots) iff the distances between µB and µM , the means of benign and
malicious gradients, are low (blue plots), i.e., poisoning impact of the malicious gradients is low. Upper row is for MNIST and lower row is
for CIFAR10 + Alexnet. We use the strongest full knowledge agr-updates adversary.
Table IV: Our robust DnC AGR defends against all the existing model poisoning attacks for independently and identically distributed datasets.
We consider the adversaries with complete knowledge of gradients of benign clients with 20% malicious clients. For each attack, we report
its attack impact on DnC and on the existing defense with the highest global model accuracy A∗
θ, computed as (Aθ − Iθ) from Table II.
Dataset + Model
CIFAR10 + Alexnet
CIFAR10 + VGG11
Purchase + FC
MNIST + FC
No
attack (Aθ)
67.6
75.5
92.0
96.2
Fang
LIE
3.2 (7.0)
3.3 (8.5)
0.8 (0.2)
0.1 (0.3)
3.0 (5.9)
1.7 (6.8)
0.5 (0.5)
0.2 (0.5)
Best of our
Our AGR-agnostic attacks
Adaptive
AGR-tailored attacks Min-Max
3.5 (27.8)
2.5 (21.9)
0.6 (1.4)
0.2 (1.2)
4.3 (36.8)
3.4 (32.5)
0.9 (3.4)
1.8 (2.5)
Min-Sum
2.0 (16.9)
2.2 (10.4)
0.8 (0.7)
1.2 (2.2)
attack
6.1
6.3
1.8
1.9
Table V: Results of empirical robustness analysis of DnC for cross-device FL setting. We consider the adversaries with complete knowledge
of gradients of benign clients with 20% malicious clients, and report A∗
θ as described in Table II.
Dataset + Model
CIFAR10 + Alexnet
CIFAR10 + VGG11
No
attack (Aθ)
64.6
72.1
Fang
LIE
0.6 (1.6)
0.8 (1.4)
0.3 (3.8)
0.3 (2.3)
Best of our
Our AGR-agnostic attacks
Adaptive
AGR-tailored attacks Min-Max
0.3 (3.8)
0.4 (1.8)
0.2 (14.0)
2.0 (8.7)
Min-Sum
0.0 (2.6)
0.4 (1.5)
attack
3.4
4.1
Table VI: For non-iid FEMNIST dataset, DnC cannot mitigate our attacks in the worst case settings when the adversary knows gradients of
the benign devices. But, mitigates all the attacks in the more practical settings when the gradients of benign devices are unknown. We report
Iθ on DnC of all adversaries in Table I with 20% malicious clients. ‘No attack’ accuracy Aθ for FEMNIST with DnC is 86.6%.
Gradients of benign devices are known
Gradients of benign devices are unknown
AGR
DnC
DnC + resampling
Best of AGR-tailored
(agr-updates)
48.1
79.3
AGR-agnostic
(updates-only)
Min-Max Min-Sum
13.8
80.5
79.3
45.9
Adaptive
attack
Best of AGR-tailored
(agr-only)
AGR-agnostic
(agnostic)
Adaptive
attack
78.6
77.6
12.7
77.5
Min-Max Min-Sum
9.3
79.1
11.7
43.4
10.2
70.6
note that the impact of attacks on DnC reduces in cross-
device FL, as for the other AGRs. For CIFAR10 + Alexnet,
with no attack accuracy, Aθ, of 64.6%, DnC increases A∗
θ
from 50.6% to 61.2%: for best of existing AGRs, our AGR-
tailored attack is the strongest attack with Iθ of 14.0%, i.e.,
θ (Aθ − Iθ) of 50.6%. While for DnC, our adaptive attack
A∗
is the strongest with Iθ of 3.4%, i.e., A∗
θ of 61.2%. Similarly,
for CIFAR10 + VGG11, DnC increases A∗
θ from 63.4% to
68.0%. The increase in A∗
θ due to DnC in cross-device FL
is lower, because the impact of attacks on previous AGRs is
lower, which leaves smaller room for improvements.
Why DnC is superior? The strong robustness of DnC stems
from the effective ﬁltering guarantees of Lemma 1, which
we empirically conﬁrm in Figure 5: Here, in each epoch of FL
+ DnC training, we compute the fraction of malicious clients
DnC selects and the norm of the difference between means of
benign and malicious gradients relative to the norm of mean of
. We then average
the benign gradients, i.e. µshift =
these entities over a few epochs for presentation clarity.
(cid:107)µB−µM(cid:107)2
2
(cid:107)µB(cid:107)2
2
We observe in Figure 5 that for MNIST (upper row), Fang
and Min-Sum attacks evade DnC’s detection, but have very
small µshift which leads to high accuracy of global model A∗
θ.
DnC mitigates LIE even when LIE evades DnC’s detection
to some extent and introduces large µshift. We suspect that,
this is because LIE uses ineffective perturbation ∇b
std for
MNIST (Figure 2). Our AGR-tailored and adaptive attacks
evade DnC’s detection to some extent by maintaining low
µshift. Hence, MNIST, due to its simplicity, withstands their
poisoning impact.
15
0204060801001200.00.20.40.60.81.0Best of AGR-tailored0204060801001200.00.20.40.60.81.0Best of Fang0204060801001200.00.20.40.60.81.0LIE0204060801001200.00.20.40.60.81.0Min-MaxMalicious client fractionRelative  shift0204060801001200.00.20.40.60.81.0Min-Sum0204060801001200.00.20.40.60.81.0Adaptive0204060801001200.00.20.40.60.81.0Best of AGR-tailored0204060801001200.00.20.40.60.81.0Best of Fang0204060801001200.00.20.40.60.81.0LIE0204060801001200.00.20.40.60.81.0Min-MaxMalicious client fractionRelative  shift0204060801001200.00.20.40.60.81.0Min-Sum0204060801001200.00.20.40.60.81.0AdaptiveFor CIFAR10 + Alexnet, we observe that DnC effectively
ﬁlters malicious gradients of all but our adaptive attack. How-
ever, the adaptive attack manages to evade DnC’s detection
only due to low µshift, which is insufﬁcient to poison DnC
based FL.
2) Robustness of DnC for non-iid data: Table VI shows the
evaluation of DnC for FEMNIST, an imbalanced and non-iid
datasets. We set niters, c, and b in Algorithm 2 to 1, (n−1)/n,
and 10,000, respectively. We note that, DnC cannot defend at
least one of our attacks by the strongest adversaries with
complete knowledge of the gradients of benign clients, i.e.,
agr-updates and updates-only adversaries. Min-sum
has attack impact of 79.3%, i.e., it reduces the accuracy from
86.6% in the benign setting to 7.3%. Here, We omit evaluation
of DnC against Fang and LIE attacks, as they are strictly
weaker than all of our attacks against FEMNIST.
Furthermore, resampling [19], a mechanism proposed to
reduce non-iid nature of the input gradients, exacerbates DnC’s
robustness (also of existing AGRs as shown in Table VII).
Even the benign gradients of FEMNIST with highly non-
iid nature, do not point in a single direction, and therefore,
DnC cannot reliably detect malicious gradients. This allows
adversaries to easily circumvent DnC’s detection and mount
strong attacks. However, DnC mitigates all of the model
poisoning on FL by more practical adversaries who do not
know the gradients on benign devices, i.e., agr-only and
agnostic adversaries. The maximum attack impact is only
12.7%, i.e., the maximum accuracy of the global model due
to DnC is 73.9%. The most robust of existing AGRs is Krum
and the corresponding maximum accuracy is 66.4%.
We note that, defending the real-world non-iid FL settings
from the worst case model poisoning attacks is a well-known
challenging task [17], [19], [18] and a limitation of DnC in its
current form. We leave investigating further to improve DnC
to make it robust to the non-iid settings to future work.
VIII. CONCLUSIONS
We presented a general framework to mount systematic
model poisoning attacks on FL. We demonstrated that our
framework results in attacks that outperform state-of-the-art
poisoning attacks against all Byzantine-robust FL algorithms
by large margins. We gave concrete reasons for the
and
strength of our attacks, which future Byzantine-robust FL
algorithms should address. We also presented a robust aggrega-
tion algorithm, called divide-and-conquer, that outperforms all
existing robust aggregation algorithms in defeating poisoning
attacks on FL.
ACKNOWLEDGEMENTS
This work was supported in part by the NSF grant CPS-
1739462.
REFERENCES
[1] Acquire Valued Shoppers Challenge at Kaggle. https://www.kaggle.
com/c/acquire-valued-shoppers-challenge/data, 2019. [Online; accessed
19-June-2020].
[2] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic
In Advances in Neural Information Processing
gradient descent.
Systems, pages 4613–4623, 2018.
[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and
Vitaly Shmatikov. How to backdoor federated learning. arXiv preprint
arXiv:1807.00459, 2018.
[4] Moran Baruch, Baruch Gilad, and Yoav Goldberg. A little is enough:
Circumventing defenses for distributed learning. Advances in Neural
Information Processing Systems, 2019.
[5] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin
Calo. Analyzing federated learning through an adversarial lens. arXiv
preprint arXiv:1811.12470, 2018.
[6] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin