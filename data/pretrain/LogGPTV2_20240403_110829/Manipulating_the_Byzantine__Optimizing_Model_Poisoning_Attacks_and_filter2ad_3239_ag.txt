### Identifying and Filtering Malicious Gradients

To distinguish between benign and malicious gradients, we utilize a theoretical framework formalized in Lemma 1, which provides filtering guarantees for the Divide-and-Conquer (DnC) method.

**Lemma 1.** Consider \(0 < \epsilon < 0.5\) and two distributions \(B\) and \(M\) with means \(\mu_B\), \(\mu_M\) and covariances \(\Sigma_B\), \(\Sigma_M \preceq \sigma^2 I\). Let \(U = (1 - \epsilon)B + \epsilon M\) be a mixture of samples from \(B\) and \(M\). Then \(B\) and \(M\) are \(\epsilon\)-spectrally separable if \(\|\mu_B - \mu_M\|_2^2 \geq \frac{6\sigma^2}{\epsilon}\).

In the context of Federated Learning (FL) poisoning, Lemma 1 implies that if the means of poisoned and benign gradients are sufficiently separated, spectral methods can reliably separate them. Figure 5 illustrates this: the means of malicious gradients, which effectively poison FL, are sufficiently far from the means of benign gradients, allowing spectral methods to filter them. Conversely, malicious gradients that do not meet the criterion in Lemma 1 have no significant impact on the global model's accuracy. The result in Lemma 1 is consistent with SVD-based outlier detection methods [15], [36], [16], [28], [11]. We provide it here for completeness and to illustrate the efficiency of DnC. A formal proof of Lemma 1 is given in Appendix B.

### Adaptive Attack Against DnC

DnC offers provable theoretical guarantees for detecting malicious gradients. To empirically evaluate the robustness of DnC, we propose an adaptive attack against the strongest adversary who has complete knowledge of the gradients of benign devices and the DnC algorithm.

The adaptive attack is based on the general optimization framework described in Section IV-A and is inspired by our AGR-tailored attack on Multi-krum AGR, as both DnC and Multi-krum compute a selection set and average the gradients in the final selection set. The goal of the attack is to maximize the number of malicious gradients selected by DnC, thereby maximizing their negative impact on the final aggregate. This also minimizes the number of benign gradients selected and their positive impact on the final aggregate. The optimization problem for the adaptive attack is:

\[
\arg\max_{\gamma} m = |\{\nabla \in \nabla_m \mid i \in I_{\text{final}}\}|
\]
\[
\nabla_m^{(i)} = f_{\text{avg}}(\nabla^{(i)}) + \gamma \nabla_p
\]
where \(m\) is the number of malicious clients, \(I_{\text{final}}\) is the final set of candidate indices selected by DnC, \(\nabla_p\) is the perturbation, and \(\gamma\) is the scaling factor. Although the adversary knows the DnC algorithm thoroughly, they cannot know the exact random indices \(r\) from Algorithm 2. The optimization problem in (8) is solved by finding the most impactful \(\gamma\) using Algorithm 1.

### Evaluation of Our Defense

#### Robustness of DnC for IID Data

For independently and identically distributed (IID) datasets (MNIST, CIFAR10, and Purchase), we evaluate DnC against a strong adversarial setting with 20% malicious clients and adversaries with complete knowledge of the gradients of benign clients. We test DnC using Fang and LIE attacks, as well as our stronger AGR-tailored and AGR-agnostic attacks. For all these datasets, we set the parameters \(niters\), \(c\), and \(b\) in Algorithm 2 to 1, 1, and 10,000, respectively.

**Robustness Comparison with Previous AGRs.** Table IV shows the attack impact on DnC and the most robust existing AGRs. For example, for the Fang attack on CIFAR10 + Alexnet, Bulyan is the most robust AGR, so we show the impact of the Fang attack on Bulyan. We analyze the AGRs based on the increase in accuracy of the global model under the strongest attacks. The minimum accuracy \(A^*_\theta\) for an AGR is obtained by subtracting the impact of the strongest attack \(I_\theta\) from the 'No attack' accuracy \(A_\theta\).

For CIFAR10 + Alexnet, our adaptive attack is the strongest against DnC, resulting in a minimum \(A^*_\theta\) of 61.5%. In contrast, the strongest attack against the best existing AGR (Bulyan) results in a minimum \(A^*_\theta\) of 30.8%. Thus, DnC increases \(A^*_\theta\) from 30.8% to 61.5% (approximately a 100% increase). Similar improvements are observed for CIFAR10 + VGG11 and Purchase.

**Cross-Device FL Setting.** Table V compares the robustness of previous AGRs and DnC in a cross-device FL setting using CIFAR10 with Alexnet and VGG11 architectures. The impact of attacks on DnC reduces in cross-device FL, similar to other AGRs. For CIFAR10 + Alexnet, DnC increases \(A^*_\theta\) from 50.6% to 61.2%, while for CIFAR10 + VGG11, it increases from 63.4% to 68.0%.

**Why DnC is Superior.** The strong robustness of DnC stems from the effective filtering guarantees of Lemma 1, which we empirically confirm in Figure 5. For MNIST, even though some attacks evade DnC's detection, they have very small \(\mu_{\text{shift}}\), leading to high global model accuracy. DnC mitigates LIE attacks even when they introduce large \(\mu_{\text{shift}}\). For CIFAR10 + Alexnet, DnC effectively filters malicious gradients of all but our adaptive attack, which evades detection only due to low \(\mu_{\text{shift}}\).

#### Robustness of DnC for Non-IID Data

Table VI evaluates DnC for FEMNIST, an imbalanced and non-IID dataset. DnC cannot defend against at least one of our attacks by the strongest adversaries with complete knowledge of the gradients of benign clients. The Min-Sum attack has a significant impact, reducing the accuracy from 86.6% to 7.3%. Resampling, a mechanism to reduce the non-IID nature of input gradients, exacerbates DnC's robustness. However, DnC mitigates all model poisoning attacks by more practical adversaries who do not know the gradients of benign devices, with a maximum attack impact of 12.7%.

Defending against worst-case model poisoning attacks in real-world non-IID FL settings remains a challenging task [17], [19], [18], and improving DnC for such settings is left for future work.

### Conclusions

We presented a general framework for mounting systematic model poisoning attacks on FL, demonstrating that our framework outperforms state-of-the-art poisoning attacks against all Byzantine-robust FL algorithms. We also introduced DnC, a robust aggregation algorithm that outperforms existing robust aggregation algorithms in defeating poisoning attacks on FL.

### Acknowledgements

This work was supported in part by the NSF grant CPS-1739462.

### References

[1] Acquire Valued Shoppers Challenge at Kaggle. https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data, 2019. [Online; accessed 19-June-2020].
[2] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4613â€“4623, 2018.
[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. arXiv preprint arXiv:1807.00459, 2018.
[4] Moran Baruch, Baruch Gilad, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. Advances in Neural Information Processing Systems, 2019.
[5] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. arXiv preprint arXiv:1811.12470, 2018.
[6] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. arXiv preprint arXiv:1811.12470, 2018.