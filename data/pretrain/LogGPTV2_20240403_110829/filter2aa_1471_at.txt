■    The XTA cache state for the image
In particular, whenever a new x86 or CHPE image is loaded, the simulator
determines whether it should use the XTA cache for the module (through
registry and application compatibility shim.) In case the check succeeded, the
simulator updates the global per-process XTA cache state by requesting to
the XtaCache service the updated cache for the image. In case the XtaCache
service is able to identify and open an updated cache file for the image, it
returns a section object to the simulator, which can be used to speed up the
execution of the image. (The section contains precompiled ARM64 code
blocks.)
Compiled Hybrid Portable Executables (CHPE)
Jitting an x86 process in ARM64 environments is challenging because the
compiler should keep enough performance to maintain the application
responsiveness. One of the major issues is tied to the memory ordering
differences between the two architectures. The x86 emulator does not know
how the original x86 code has been designed, so it is obliged to aggressively
use memory barriers between each memory access made by the x86 image.
Executing memory barriers is a slow operation. On average, about 40% of
many applications’ time is spent running operating system code. This meant
that not emulating OS libraries would have allowed a gain in a lot of overall
applications’ performance.
These are the motivations behind the design of Compiled Hybrid Portable
Executables (CHPE). A CHPE binary is a special hybrid executable that
contains both x86 and ARM64-compatible code, which has been generated
with full awareness of the original source code (the compiler knew exactly
where to use memory barriers). The ARM64-compatible machine code is
called hybrid (or CHPE) code: it is still executed in AArch64 mode but is
generated following the 32-bit ABI for a better interoperability with x86
code.
CHPE binaries are created as standard x86 executables (the machine ID is
still 014C as for x86); the main difference is that they include hybrid code,
described by a table in the Hybrid Image metadata (stored as part of the
image load configuration directory). When a CHPE binary is loaded into the
WoW64 process’s address space, the simulator updates the CHPE bitmap by
setting a bit to 1 for each page containing hybrid code described by the
Hybrid metadata. When the jitter compiles the x86 code block and detects
that the code is trying to invoke a hybrid function, it directly executes it
(using the 32-bit stack), without wasting any time in any compilation.
The jitted x86 code is executed following a custom ABI, which means that
there is a nonstandard convention on how the ARM64 registers are used and
how parameters are passed between functions. CHPE code does not follow
the same register conventions as jitted code (although hybrid code still
follows a 32-bit ABI). This means that directly invoking CHPE code from
the jitted blocks built by the compiler is not directly possible. To overcome
this problem, CHPE binaries also include three different kinds of thunk
functions, which allow the interoperability of CHPE with x86 code:
■    A pop thunk allows x86 code to invoke a hybrid function by
converting incoming (or outgoing) arguments from the guest (x86)
caller to the CHPE convention and by directly transferring execution
to the hybrid code.
■    A push thunk allows CHPE code to invoke an x86 routine by
converting incoming (or outgoing) arguments from the hybrid code to
the guest (x86) convention and by calling the emulator to resume
execution on the x86 code.
■    An export thunk is a compatibility thunk created for supporting
applications that detour x86 functions exported from OS modules
with the goal of modifying their functionality. Functions exported
from CHPE modules still contain a little amount of x86 code (usually
8 bytes), which semantically does not provide any sort of
functionality but allows detours to be inserted by the external
application.
The x86-on-ARM simulator makes the best effort to always load CHPE
system binaries instead of standard x86 ones, but this is not always possible.
In case a CHPE binary does not exist, the simulator will load the standard
x86 one from the SysWow64 folder. In this case, the OS module will be
jitted entirely.
EXPERIMENT: Dumping the hybrid code address
range table
The Microsoft Incremental linker (link.exe) tool included in the
Windows SDK and WDK is able to show some information stored
in the hybrid metadata of the Image load configuration directory of
a CHPE image. More information about the tool and how to install
it are available in Chapter 9.
In this experiment, you will dump the hybrid metadata of
kernelbase.dll, a system library that also has been compiled with
CHPE support. You also can try the experiment with other CHPE
libraries. After having installed the SDK or WDK on a ARM64
machine, open the Visual Studio Developer Command Prompt (or
start the LaunchBuildEnv.cmd script file in case you are using the
EWDK’s Iso image.) Move to the CHPE folder and dump the
image load configuration directory of the kernelbase.dll file
through the following commands:
Click here to view code image
cd c:\Windows\SyChpe32
link /dump /loadconfig kernelbase.dll > 
kernelbase_loadconfig.txt
Note that in the example, the command output has been
redirected to the kernelbase_loadconfig.txt text file because it was
too large to be easily displayed in the console. Open the text file
with Notepad and scroll down until you reach the following text:
Click here to view code image
Section contains the following hybrid metadata:
               4 Version
        102D900C Address of WowA64 exception handler 
function pointer
        102D9000 Address of WowA64 dispatch call function 
pointer
        102D9004 Address of WowA64 dispatch indirect call 
function pointer
        102D9008 Address of WowA64 dispatch indirect call 
function pointer (with CFG check)
        102D9010 Address of WowA64 dispatch return function 
pointer
        102D9014 Address of WowA64 dispatch leaf return 
function pointer
        102D9018 Address of WowA64 dispatch jump function 
pointer
        102DE000 Address of WowA64 auxiliary import address 
table pointer
        1011DAC8 Hybrid code address range table
               4 Hybrid code address range count
    Hybrid Code Address Range Table
                Address Range
          ----------------------
          x86    10001000 - 1000828F (00001000 - 0000828F)
          arm64  1011E2E0 - 1029E09E (0011E2E0 - 0029E09E)
          x86    102BA000 - 102BB865 (002BA000 - 002BB865)
          arm64  102BC000 - 102C0097 (002BC000 - 002C0097)
The tool confirms that kernelbase.dll has four different ranges in
the Hybrid code address range table: two sections contain x86 code
(actually not used by the simulator), and two contain CHPE code
(the tool shows the term “arm64” erroneously.)
The XTA cache
As introduced in the previous sections, the x86-on-ARM64 simulator, other
than its internal per-thread cache, uses an external global cache called XTA
cache, managed by the XtaCache protected service, which implements the
lazy jitter. The service is an automatic start service, which, when started,
opens (or creates) the C:\Windows\XtaCache folder and protects it through a
proper ACL (only the XtaCache service and members of the Administrators
group have access to the folder). The service starts its own ALPC server
through the {BEC19D6F-D7B2-41A8-860C-8787BB964F2D} connection
port. It then allocates the ALPC and lazy jit worker threads before exiting.
The ALPC worker thread is responsible in dispatching all the incoming
requests to the ALPC server. In particular, when the simulator (the client),
running in the context of a WoW64 process, connects to the XtaCache
service, a new data structure tracking the x86 process is created and stored in
an internal list, together with a 128 KB memory mapped section, which is
shared between the client and XtaCache (the memory backing the section is
internally called Trace buffer). The section is used by the simulator to send
hints about the x86 code that has been jitted to execute the application and
was not present in any cache, together with the module ID to which they
belong. The information stored in the section is processed every 1 second by
the XTA cache or in case the buffer becomes full. Based on the number of
valid entries in the list, the XtaCache can decide to directly start the lazy
jitter.
When a new image is mapped into an x86 process, the WoW64 layer
informs the simulator, which sends a message to the XtaCache looking for an
already-existing XTA cache file. To find the cache file, the XtaCache service
should first open the executable image, map it, and calculate its hashes. Two
hashes are generated based on the executable image path and its internal
binary data. The hashes are important because they avoid the execution of
jitted blocks compiled for an old stale version of the executable image. The
XTA cache file name is then generated using the following name scheme:
...
. .jc. The cache file contains all the
precompiled code blocks, which can be directly executed by the simulator.
Thus, in case a valid cache file exists, the XtaCache creates a file-mapped
section and injects it into the client WoW64 process.
The lazy jitter is the engine of the XtaCache. When the service decides to
invoke it, a new version of the cache file representing the jitted x86 module
is created and initialized. The lazy jitter then starts the lazy compilation by
invoking the XTA offline compiler (xtac.exe). The compiler is started in a
protected low-privileged environment (AppContainer process), which runs in
low-priority mode. The only job of the compiler is to compile the x86 code
executed by the simulator. The new code blocks are added to the ones located
in the old version of the cache file (if one exists) and stored in a new version
of the cache file.
EXPERIMENT: Witnessing the XTA cache
Newer versions of Process Monitor can run natively on ARM64
environments. You can use Process Monitor to observe how an
XTA cache file is generated and used for an x86 process. In this
experiment, you need an ARM64 system running at least Windows
10 May 2019 update (1903). Initially, you need to be sure that the
x86 application used for the experiment has never before been
executed by the system. In this example, we will install an old x86
version of MPC-HC media player, which can be downloaded from
https://sourceforge.net/projects/mpc-hc/files/latest/download. Any
x86 application is well suited for this experiment though.
Install MPC-HC (or your preferred x86 application), but, before
running it, open Process Monitor and add a filter on the XtaCache
service’s process name (XtaCache.exe, as the service runs in its
own process; it is not shared.) The filter should be configured as in
the following figure:
If not already done, start the events capturing by selecting
Capture Events from the File menu. Then launch MPC-HC and try
to play some video. Exit MPC-HC and stop the event capturing in
Process Monitor. The number of events displayed by Process
Monitor are significant. You can filter them by removing the
registry activity by clicking the corresponding icon on the toolbar
(in this experiment, you are not interested in the registry).
If you scroll the event list, you will find that the XtaCache
service first tried to open the MPC-HC cache file, but it failed
because the file didn’t exist. This meant that the simulator started
to compile the x86 image on its own and periodically sent
information to the XtaCache. Later, the lazy jitter would have been
invoked by a worker thread in the XtaCache. The latter created a
new version of the Xta cache file and invoked the Xtac compiler,
mapping the cache file section to both itself and Xtac:
If you restart the experiment, you would see different events in
Process Monitor: The cache file will be immediately mapped into
the MPC-HC WoW64 process. In that way, the emulator can
execute it directly. As a result, the execution time should be faster.
You can also try to delete the generated XTA cache file. The
XtaCache service automatically regenerates it after you launch the
MPC-HC x86 application again.
However, remember that the %SystemRoot%\XtaCache folder is
protected through a well-defined ACL owned by the XtaCache
service itself. To access it, you should open an administrative
command prompt window and insert the following commands:
Click here to view code image
takeown /f c:\windows\XtaCache
icacls c:\Windows\XtaCache /grant Administrators:F
Jitting and execution
To start the guest process, the x86-on-ARM64 CPU simulator has no other
chances than interpreting or jitting the x86 code. Interpreting the guest code
means translating and executing one machine instruction at time, which is a
slow process, so the emulator supports only the jitting strategy: it
dynamically compiles x86 code to ARM64 and stores the result in a guest
“code block” until certain conditions happen:
■    An illegal opcode or a data or instruction breakpoint have been
detected.
■    A branch instruction targeting an already-visited block has been
encountered.
■    The block is bigger than a predetermined limit (512 bytes).
The simulation engine works by first checking in the local and XTA cache
whether a code block (indexed by its RVA) already exists. If the block exists
in the cache, the simulator directly executes it using a dispatcher routine,
which builds the ARM64 context (containing the host registers values) and
stores it in the 64-bit stack, switches to the 32-bit stack, and prepares it for
the guest x86 thread state. Furthermore, it also prepares the ARM64 registers
to run the jitted x86 code (storing the x86 context in them). Note that a well-
defined non-standard calling convention exists: the dispatcher is similar to a
pop thunk used for transferring the execution from a CHPE to an x86
context.
When the execution of the code block ends, the dispatcher does the
opposite: It saves the new x86 context in the 32-bit stack, switches to the 64-
bit stack, and restores the old ARM64 context containing the state of the
simulator. When the dispatcher exits, the simulator knows the exact x86
virtual address where the execution was interrupted. It can then restart the
emulation starting from that new memory address. Similar to cached entries,
the simulator checks whether the target address points to a memory page
containing CHPE code (it knows this information thanks to the global CHPE
bitmap). If that is the case, the simulator resolves the pop thunk for the target
function, adds its address to the thread’s local cache, and directly executes it.
In case one of the two described conditions verifies, the simulator can have
performances similar to executing native images. Otherwise, it needs to
invoke the compiler for building the native translated code block. The
compilation process is split into three phases:
1. 
The parsing stage builds instructions descriptors for each opcode that
needs to be added in the code block.
2. 
The optimization stage optimizes the instruction flow.
3. 
Finally, the code generation phase writes the final ARM64 machine
code in the new code block.
The generated code block is then added to the per-thread local cache. Note
that the simulator cannot add it in the XTA cache, mainly for security and
performance reasons. Otherwise, an attacker would be allowed to pollute the
cache of a higher-privileged process (as a result, the malicious code could
have potentially been executed in the context of the higher-privileged
process.) Furthermore, the simulator does not have enough CPU time to
generate highly optimized code (even though there is an optimization stage)
while maintaining the application’s responsiveness.
However, information about the compiled x86 blocks, together with the ID
of the binary hosting the x86 code, are inserted into the list mapped by the
shared Trace buffer. The lazy jitter of the XTA cache knows that it needs to
compile the x86 code jitted by the simulator thanks to the Trace buffer. As a
result, it generates optimized code blocks and adds them in the XTA cache
file for the module, which will be directly executed by the simulator. Only
the first execution of the x86 process is generally slower than the others.
System calls and exception dispatching
Under the x86-on-ARM64 CPU simulator, when an x86 thread performs a
system call, it invokes the code located in the syscall page allocated by the
simulator, which raises the exception 0x2E. Each x86 exception forces the
code block to exit. The dispatcher, while exiting from the code block,
dispatches the exception through an internal function that ends up in invoking
the standard WoW64 exception handler or system call dispatcher (depending
on the exception vector number.) Those have been already discussed in the
previous X86 simulation on AMD64 platforms section of this chapter.
EXPERIMENT: Debugging WoW64 in ARM64
environments
Newer releases of WinDbg (the Windows Debugger) are able to
debug machine code run under any simulator. This means that in
ARM64 systems, you will be able to debug native ARM64, ARM
Thumb-2, and x86 applications, whereas in AMD64 systems, you
can debug only 32- and 64-bit x86 programs. The debugger is also
able to easily switch between the native 64-bit and 32-bit stacks,
which allows the user to debug both native (including the WoW64
layer and the emulator) and guest code (furthermore, the debugger
also supports CHPE.)
In this experiment, you will open an x86 application using an
ARM64 machine and switch between three execution modes:
ARM64, ARM Thumb-2, and x86. For this experiment, you need
to install a recent version of the Debugging tools, which you can
find in the WDK or SDK. After installing one of the kits, open the
ARM64 version of Windbg (available from the Start menu.)
Before starting the debug session, you should disable the
exceptions that the XtaJit emulator generates, like Data Misaligned
and in-page I/O errors (these exceptions are already handled by the
emulator itself). From the Debug menu, click Event Filters. From
the list, select the Data Misaligned event and check the Ignore
option box from the Execution group. Repeat the same for the In-
page I/O error. At the end, your configuration should look similar
to the one in following figure:
Click Close, and then from the main debugger interface, select
Open Executable from the File menu. Choose one of the 32-bit
x86 executables located in %SystemRoot%\SysWOW64 folder. (In
this example, we are using notepad.exe, but any x86 application
works.) Also open the disassembly window by selecting it through
the View menu. If your symbols are configured correctly (refer to
the https://docs.microsoft.com/en-us/windows-
hardware/drivers/debugger/symbol-path webpage for instructions
on how to configure symbols), you should see the first native Ntdll
breakpoint, which can be confirmed by displaying the stack with
the k command:
Click here to view code image
0:000> k
# Child-SP          RetAddr           Call Site
00 00000000`001eec70 00007ffb`bd47de00 
ntdll!LdrpDoDebuggerBreak+0x2c
01 00000000`001eec90 00007ffb`bd47133c 
ntdll!LdrpInitializeProcess+0x1da8
02 00000000`001ef580 00007ffb`bd428180 
ntdll!_LdrpInitialize+0x491ac
03 00000000`001ef660 00007ffb`bd428134 
ntdll!LdrpInitialize+0x38
04 00000000`001ef680 00000000`00000000 
ntdll!LdrInitializeThunk+0x14
The simulator is still not loaded at this time: The native and
CHPE Ntdll have been mapped into the target binary by the NT
kernel, while the WoW64 core binaries have been loaded by the
native Ntdll just before the breakpoint via the LdrpLoadWow64
function. You can check that by enumerating the currently loaded
modules (via the lm command) and by moving to the next frame in
the stack via the .f+ command. In the disassembly window, you
should see the invocation of the LdrpLoadWow64 routine:
Click here to view code image
00007ffb`bd47dde4 97fed31b bl          ntdll!LdrpLoadWow64 
(00007ffb`bd432a50)
Now resume the execution with the g command (or F5 key).
You should see multiple modules being loaded in the process
address space and another breakpoint raising, this time under the
x86 context. If you again display the stack via the k command, you
can notice that a new column is displayed. Furthermore, the
debugger added the x86 word in its prompt:
Click here to view code image
0:000:x86> k
 #   Arch ChildEBP RetAddr
00    x86 00acf7b8 77006fb8 
ntdll_76ec0000!LdrpDoDebuggerBreak+0x2b
01   CHPE 00acf7c0 77006fb8 
ntdll_76ec0000!#LdrpDoDebuggerBreak$push_thunk+0x48
02   CHPE 00acf820 76f44054 
ntdll_76ec0000!#LdrpInitializeProcess+0x20ec
03   CHPE 00acfad0 76f43e9c 
ntdll_76ec0000!#_LdrpInitialize+0x1a4
04   CHPE 00acfb60 76f43e34 
ntdll_76ec0000!#LdrpInitialize+0x3c
05   CHPE 00acfb80 76ffc3cc 
ntdll_76ec0000!LdrInitializeThunk+0x14
If you compare the new stack to the old one, you will see that the