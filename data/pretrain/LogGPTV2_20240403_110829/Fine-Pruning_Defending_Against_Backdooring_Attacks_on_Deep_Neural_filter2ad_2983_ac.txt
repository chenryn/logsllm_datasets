Fraction of Neurons Pruned
0.98
(c) Baseline Attack (Speech)
(d) Pruning Aware Attack (Speech)
1
0.8
0.6
0.4
0.2
0
0
4% Clean Classification
Accuracy Drop
Clean Classification
Accuracy
Backdoor Attack
Success
0.2
Fraction of Neurons Pruned
0.4
0.6
0.8
1
0.8
0.6
0.4
0.2
e
t
a
R
1
0
0.96
Clean Classification
Accuracy
Backdoor Attack
Success
0.97
0.99
Fraction of Neurons Pruned
0.98
1
1
1
(e) Baseline Attack (Traﬃc)
(f) Pruning Aware Attack (Traﬃc)
Fig. 6. (a), (c), (e): Classiﬁcation accuracy on clean inputs and backdoor attack success
rate versus fraction of neurons pruned for baseline backdoor attacks on face (a), speech
(c) and traﬃc sign recognition (e). (b), (d), (f): Classiﬁcation accuracy on clean inputs
and backdoor attack success rate versus fraction of neurons pruned for pruning-aware
backdoor attacks on face (b), speech (d) and traﬃc sign recognition (f).
284
K. Liu et al.
Fig. 7. Operation of the pruning-aware attack.
poisoned training dataset. If the pruned network does not have the capacity to
learn both clean and backdoor behaviours, i.e., if either the classiﬁcation accu-
racy on clean inputs or the backdoor success rate is low, the attacker re-instates
a neuron in the pruned network and trains again till she is satisﬁed.
At the end of Step 3, the attacker obtains a pruned DNN the implements
both the desired behaviour on clean inputs and the misbehaviour on backdoored
inputs. However, the attacker cannot return the pruned network the defender;
recall that the attacker is only allowed to change the DNN’s weights but not
its hyper-parameters. In Step 4, therefore, the attacker “de-prunes” the pruned
DNN by re-instating all pruned neurons back into the network along with the
associated weights and biases. However, the attacker must ensure that the re-
instated neurons remain dormant on clean inputs; this is achieved by decreasing
the biases of the reinstated/de-pruned neurons (bi in Eq. 1). Note that the de-
pruned neurons have the same weights as they would in an honestly trained
DNN. Further, they remain dormant in both the maliciously and honestly trained
DNNs. Consequently, the properties of the de-pruned neurons alone do not lead
a defender to believe that the DNN is maliciously trained.
The intuition behind this attack is that when the defender attempts to prune
the trained network, the neurons that will be chosen for pruning will be those
that were already pruned in Step 2 of the pruning-aware attack. Hence, because
the attacker was able to encode the backdoor behavior into the smaller set of
un-pruned neurons in Step 3, the behavior of the model on backdoored inputs
will be unaﬀected by defender’s pruning. In essence, the neurons pruned in Step
2 of the attack (and later re-instated in Step 4) act as “decoy” neurons that
render the pruning defense ineﬀective.
Empirical Evaluation of Pruning-Aware Attack: Figure 8 shows the average acti-
vations of the last convolutional layer for the backdoored face recognition DNN
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
285
(a) Clean Activations (pruning aware
attack)
(b) Backdoor Activations
aware attack)
(pruning
Fig. 8. Average activations of neurons in the ﬁnal convolutional layer of the back-
doored face recognition DNN for clean and backdoor inputs, respectively. The DNN is
backdoored using the pruning-aware attack.
generated by the pruning-aware attack. Note that compared to the activations
of the baseline attack (Fig. 4) (i) a larger fraction of neurons remain dormant
(about 84%) for both clean and backdoored inputs; and (ii) the activations of
clean and backdoored inputs are conﬁned to the same subset of neurons. Similar
trends are observed for backdoored speech and traﬃc sign recognition DNNs
generated by the pruning-aware attack. Speciﬁcally, the attack is able to con-
ﬁne clean and backdoor activations to between 3% and 15% of the neurons in
the last convolutional layer for the traﬃc and speech sign recognition DNNs,
respectively.
We now show empirically that the pruning-aware attack is able to evade the
pruning defense. Figure 6(b), (d), (f) plots the classiﬁcation accuracy on clean
inputs and backdoor attack success rate versus the fraction of neurons pruned
by the defender for the face, speech and traﬃc sign recognition networks. Since
the defender prunes decoy neurons in the ﬁrst several iterations of the defense,
the plots start from the point at which a decrease in clean classiﬁcation accuracy
or backdoor success rate is observed.
Several observations can be made from the ﬁgures:
– The backdoored DNNs generated by the baseline and pruning-aware attack
have the same classiﬁcation accuracy on clean inputs assuming a na¨ıve
defender who does not perform any pruning. This is true for the face, speech
and traﬃc sign recognition attacks.
– Similarly, the success rate of the baseline and pruning-aware attack on face
and speech recognition are the same, assuming a na¨ıve defender who does not
perform any pruning. The success rate of the pruning-aware attack reduces
slightly to 90% from 99% for the baseline attack for traﬃc sign recognition,
again assuming a na¨ıve defender.
286
K. Liu et al.
– The pruning defense on the backdoored face recognition DNN (see Fig. 6(b))
causes, at a ﬁrst, in a drop in the classiﬁcation accuracy on clean inputs but
not in the backdoor attack success rate. Although the backdoor attack success
rate does drop once suﬃciently many neurons are pruned, by this time the
classiﬁcation accuracy on clean inputs is already below 23%, rendering the
pruning defense ineﬀective.
– The pruning defense on the backdoored speech recognition DNN (see
Fig. 6(d)) causes both the classiﬁcation accuracy on clean inputs and the
backdoor attacks success rate to gradually fall as neurons are pruned. Recall
that for the baseline attack, the pruning defense reduced the backdoor attack
success rate to 13% with only 4% reduction in classiﬁcation accuracy. To
achieve the same resilience against the pruning-aware attacker, the pruning
defense reduces the classiﬁcation accuracy by 55%.
– The pruning defense is also ineﬀective on backdoored traﬃc sign recognition
(see Fig. 6(f)). Pruning reduces the classiﬁcation accuracy on clean inputs,
but the backdoor attack success rate remains high even with pruning.
Discussion: The pruning-aware attack shows that it is not necessary for clean
and backdoor inputs to activate diﬀerent parts of a DNN as observed in prior
work [18]. We ﬁnd, instead, that both clean and backdoor activity can be mapped
to the same subset of neurons, at least for the attacks we experimented with.
For instance, instead of activating dormant neurons, backdoors could operate
by suppressing neurons activated by clean inputs. In addition, the commonly
used ReLU activation function, used in all of the DNNs we evaluated in this
paper, enables backdoors to be encoded by how strongly a neuron is activated
as opposed to which neurons are activated since its output ranges from [0,∞).
3.3 Fine-Pruning Defense
The pruning defense only requires the defender to evaluate (or execute) a trained
DNN on validation data by performing a single forward pass through the network
per validation input. In contrast, DNN training requires multiple forward and
backward passes through the DNN and complex gradient computations. DNN
training is, therefore, signiﬁcantly more time-consuming than DNN evaluation.
We now consider a more capable defender who has the expertise and compu-
tational capacity to train a DNN, but does not want to incur the expense of
training the DNN from scratch (or else the defender would not have outsourced
DNN training in the ﬁrst place).
Instead of training the DNN from scratch, a capable defender can instead
ﬁne-tune the DNN trained by the attacker using clean inputs. Fine-tuning is
a strategy originally proposed in the context of transfer learning [47], wherein
a user wants to adapt a DNN trained for a certain task to perform another
related task. Fine-tuning uses the pre-trained DNN weights to initialize training
(instead of random initialization) and a smaller learning rate since the ﬁnal
weights are expected to be relatively close to the pre-trained weights. Fine-
tuning is signiﬁcantly faster than training a network from scratch; for instance,
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
287
our ﬁne-tuning experiments on AlexNet terminate within an hour while training
AlexNet from scratch can take more than six days [22]. Therefore, ﬁne-tuning
is still a feasible defense strategy from the perspective of computational cost,
despite being more computationally burdensome than the pruning defense.
Unfortunately, as shown in Table 1, the ﬁne-tuning defense does not always
work on backdoored DNNs trained using the baseline attack. The reason for this
can be understood as follows: the accuracy of the backdoored DNN on clean
inputs does not depend on the weights of backdoor neurons since these are dor-
mant on clean inputs in any case. Consequently, the ﬁne-tuning procedure has no
incentive to update the weights of backdoor neurons and leaves them unchanged.
Indeed, the commonly used gradient descent algorithm for DNN tuning only
updates the weights of neurons that are activated by at least one input; again,
this implies that the weights of backdoor neurons will be left unchanged by a
ﬁne-tuning defense.
Fine-pruning: The ﬁne-pruning defense seeks to combine the beneﬁts of the
pruning and ﬁne-tuning defenses. That is, ﬁne-pruning ﬁrst prunes the DNN
returned by the attacker and then ﬁne-tunes the pruned network. For the baseline
attack, the pruning defense removes backdoor neurons and ﬁne-tuning restores
(or at least partially restores) the drop in classiﬁcation accuracy on clean inputs
introduced by pruning. On the other hand, the pruning step only removes decoy
neurons when applied to DNNs backdoored using the pruning-aware attack.
However, subsequent ﬁne-tuning eliminates backdoors. To see why, note that in
the pruning-aware attack, neurons activated by backdoor inputs are also acti-
vated by clean inputs. Consequently, ﬁne-tuning using clean inputs causes the
weights of neurons involved in backdoor behaviour to be updated.
Table 1. Classiﬁcation accuracy on clean inputs (cl) and backdoor attack success rate
(bd) using ﬁne-tuning and ﬁne-pruning defenses against the baseline and pruning-aware
attacks.
Neural network Baseline attack
Defender strategy
Pruning aware attack
Defender strategy
None
Fine-tuning Fine-pruning None
Fine-tuning Fine-pruning
Face
cl: 0.978 cl: 0.978
cl: 0.978
cl: 0.974 cl: 0.978
cl: 0.977
recognition
bd: 1.000 bd: 0.000
bd: 0.000
bd: 0.998 bd: 0.000
bd: 0.000
Speech
cl: 0.990 cl: 0.990
cl: 0.988
cl: 0.988 cl: 0.988
cl: 0.986
recognition
bd: 0.770 bd: 0.435
bd: 0.020
bd: 0.780 bd: 0.520
bd: 0.000
Traﬃc sign
cl: 0.849 cl: 0.857
cl: 0.873
cl: 0.820 cl: 0.872
cl: 0.874
detection
bd: 0.991 bd: 0.921
bd: 0.288
bd: 0.899 bd: 0.419
bd: 0.366
Empirical Evaluation of Fine-Pruning Defense: We evaluate the ﬁne-pruning
defense on all three backdoor attacks under both the baseline attacker as well as
the more sophisticated pruning-aware attacker described in Sect. 3.2. The results
of these experiments are shown under the “ﬁne-pruning” columns of Table 1. We
highlight three main points about these results:
288
K. Liu et al.
– In the worst case, ﬁne-pruning reduces the accuracy of the network on clean
data by just 0.2%; in some cases, ﬁne-pruning increases the accuracy on clean
data slightly.
– For targeted attacks, ﬁne-pruning is highly eﬀective and completely nulliﬁes
the backdoor’s success in most cases, for both the baseline and pruning-aware
attacker. In the worst case (speech recognition), the baseline attacker’s success
is just 2%, compared to 44% for ﬁne-tuning and 77% with no defense.
– For the untargeted attacks on traﬃc sign recognition, ﬁne-pruning reduces
the attacker’s success from 99% to 29% in the baseline attack and from 90%
to 37% in the pruning-aware attack. Although 29% and 37% still seem high,
recall that the attacker’s task in an untargeted attack is much easier and the
defender’s job correspondingly harder, since any misclassiﬁcations on trigger-
ing inputs count towards the attacker’s success.
Discussion: Given that both ﬁne-pruning and ﬁne-tuning work equally well
against a pruning-aware attacker, one may be tempted to ask why ﬁne-pruning
is needed. However, if the attacker knows that the defender will use ﬁne-tuning,
her best strategy is to perform the baseline attack, in which case ﬁne-tuning is
much less eﬀective than ﬁne-pruning.
Table 2. Defender’s utility matrix for the speech recognition attack. The defender’s
utility is deﬁned as the classiﬁcation accuracy on clean inputs minus the backdoor
attack success rate.
Utility
Attacker strategy
Baseline attack Pruning aware attack
Defender strategy Fine-tuning
0.555
Fine-pruning 0.968
0.468
0.986
One way to see this is to consider the utility matrix for a baseline and pruning-
aware attacker against a defender using ﬁne-tuning or ﬁne-pruning. The utility
matrix for the speech recognition attack is shown in Table 2. We can deﬁne the
defender’s utility as simply the clean set accuracy minus the attacker’s success