### Fraction of Neurons Pruned
- **Baseline Attack (Speech)**
- **Pruning-Aware Attack (Speech)**

| Fraction of Neurons Pruned | 0.98 | 1 | 0.8 | 0.6 | 0.4 | 0.2 | 0 |
|-----------------------------|------|---|-----|-----|-----|-----|---|
| 4% Clean Classification Accuracy Drop | - | - | - | - | - | - | - |
| Clean Classification Accuracy | - | - | - | - | - | - | - |
| Backdoor Attack Success | - | - | - | - | - | - | - |

### Figure 6
- **(a), (c), (e):** Classification accuracy on clean inputs and backdoor attack success rate versus the fraction of neurons pruned for baseline backdoor attacks on face (a), speech (c), and traffic sign recognition (e).
- **(b), (d), (f):** Classification accuracy on clean inputs and backdoor attack success rate versus the fraction of neurons pruned for pruning-aware backdoor attacks on face (b), speech (d), and traffic sign recognition (f).

### Operation of the Pruning-Aware Attack
The pruning-aware attack operates as follows:
1. **Poisoned Training Dataset:** The attacker uses a poisoned training dataset.
2. **Network Pruning:** If the pruned network does not have the capacity to learn both clean and backdoor behaviors, i.e., if either the classification accuracy on clean inputs or the backdoor success rate is low, the attacker reinstates a neuron in the pruned network and re-trains until satisfied.
3. **Final Pruned DNN:** At the end of Step 3, the attacker obtains a pruned DNN that implements both the desired behavior on clean inputs and the misbehavior on backdoored inputs.
4. **De-Pruning:** The attacker "de-prunes" the pruned DNN by reinstating all pruned neurons back into the network along with their associated weights and biases. The biases of the reinstated/de-pruned neurons are decreased to ensure they remain dormant on clean inputs. This ensures that the properties of the de-pruned neurons do not lead a defender to believe the DNN is maliciously trained.

### Intuition Behind the Attack
The intuition behind this attack is that when the defender attempts to prune the trained network, the neurons chosen for pruning will be those already pruned in Step 2 of the pruning-aware attack. Since the attacker encoded the backdoor behavior into the smaller set of un-pruned neurons in Step 3, the behavior of the model on backdoored inputs remains unaffected by the defender's pruning. Essentially, the neurons pruned in Step 2 (and later reinstated in Step 4) act as "decoy" neurons, rendering the pruning defense ineffective.

### Empirical Evaluation of Pruning-Aware Attack
- **Figure 8:** Shows the average activations of the last convolutional layer for the backdoored face recognition DNN generated by the pruning-aware attack. Compared to the baseline attack, a larger fraction of neurons (about 84%) remain dormant for both clean and backdoored inputs, and the activations of clean and backdoored inputs are confined to the same subset of neurons.
- **Similar Trends:** Observed for backdoored speech and traffic sign recognition DNNs, where the attack confines clean and backdoor activations to between 3% and 15% of the neurons in the last convolutional layer.

### Observations from Figure 6
- **Baseline and Pruning-Aware Attacks:** Both attacks have the same classification accuracy on clean inputs and backdoor success rate, assuming a naive defender who does not perform any pruning.
- **Face Recognition DNN:** Pruning initially causes a drop in the classification accuracy on clean inputs but not in the backdoor attack success rate. The backdoor attack success rate drops only after sufficient neurons are pruned, by which time the classification accuracy on clean inputs is already below 23%.
- **Speech Recognition DNN:** Pruning causes both the classification accuracy on clean inputs and the backdoor attack success rate to gradually fall. For the baseline attack, pruning reduced the backdoor attack success rate to 13% with only a 4% reduction in classification accuracy. For the pruning-aware attack, the same resilience requires a 55% reduction in classification accuracy.
- **Traffic Sign Recognition DNN:** Pruning reduces the classification accuracy on clean inputs, but the backdoor attack success rate remains high even with pruning.

### Discussion
The pruning-aware attack demonstrates that it is not necessary for clean and backdoor inputs to activate different parts of a DNN, as observed in prior work. Instead, both clean and backdoor activity can be mapped to the same subset of neurons, at least for the attacks tested. Backdoors can operate by suppressing neurons activated by clean inputs, and the ReLU activation function enables backdoors to be encoded by the strength of neuron activation rather than which neurons are activated.

### Fine-Pruning Defense
- **Pruning Defense:** Requires the defender to evaluate a trained DNN on validation data, performing a single forward pass per validation input.
- **Fine-Tuning:** A more capable defender can fine-tune the DNN using clean inputs, which is faster than training from scratch. However, fine-tuning does not always work on backdoored DNNs because the backdoor neurons remain dormant on clean inputs and their weights are not updated.
- **Fine-Pruning:** Combines the benefits of pruning and fine-tuning. It first prunes the DNN returned by the attacker and then fine-tunes the pruned network. This approach is effective against both baseline and pruning-aware attacks.

### Empirical Evaluation of Fine-Pruning Defense
- **Table 1:** Shows the classification accuracy on clean inputs and backdoor attack success rate using fine-tuning and fine-pruning defenses against the baseline and pruning-aware attacks.
- **Key Points:**
  - In the worst case, fine-pruning reduces the accuracy on clean data by just 0.2%, and in some cases, it slightly increases the accuracy.
  - For targeted attacks, fine-pruning completely nullifies the backdoor's success in most cases, reducing the baseline attacker's success to 2% for speech recognition.
  - For untargeted attacks on traffic sign recognition, fine-pruning reduces the attacker's success from 99% to 29% in the baseline attack and from 90% to 37% in the pruning-aware attack.

### Discussion
- **Utility Matrix:** Table 2 shows the defender's utility matrix for the speech recognition attack, defined as the classification accuracy on clean inputs minus the backdoor attack success rate.
- **Conclusion:** Fine-pruning is necessary because if the attacker knows the defender will use fine-tuning, the best strategy for the attacker is to perform the baseline attack, in which case fine-tuning is much less effective than fine-pruning.