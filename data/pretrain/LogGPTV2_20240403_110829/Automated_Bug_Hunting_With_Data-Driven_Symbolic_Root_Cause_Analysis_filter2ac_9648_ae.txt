7
0
350
0
5
0
88
0
0
15
18
88
0
72
1
0
32
0
0
0
0
7
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
being 60,406,299 basic blocks long, the bug it revealed was uncov-
ered in 12 seconds. Conversely, several Autotrace traces of about
40,000,000 basic blocks each uncovered bugs in about 1 hour.
Figure 6 presents the coverage of our analysis over forwarded
traces (i.e., after end-host-side filtering). To normalize each pro-
gram’s curve, we present a cumulative distribution function (CDF)
of the percentage of novel basic blocks discovered versus the per-
centage of traces analyzed. For all target programs, by the time 50%
of the traces were analyzed, at least 80% of the total discovered
basic blocks had been found, demonstrating that Bunkerbuster’s
analysis converges. This is also consistent with the change in ratio
of segments being filtered by the end-host over time.
4.2 Comparing Prior Exploration Techniques
Methodology. We compare Bunkerbuster against AFL [112], a
highly popular greybox fuzzer, and QSYM [111], a recent concolic
execution hybrid fuzzer, for this experiment. We pick these systems
because they work in the binary-only setting for a wide range of
bug classes, whereas other prior work requires source code [48] or
is limited to a single class [59, 88], which would make for an unfair
comparison. For consistency, we run each system on each target
program for 1 week, starting from the same corpus of seeds. For
each unique crash (as determined by AFL and QSYM), we manually
inspect it to determine the bug class and root cause. We measure
which bugs are detected by each system and how many reports are
generated. We present the results for Autotrace in Subsection 4.3
as an extended case with crashes analyzed by AddressSanitizer.
Results. The results are presented in Table 2. In several cases,
AFL and QSYM were unable to detect vulnerabilities found by
Bunkerbuster. For example, they were unable to find the FS bug in
DMitry because it requires a specific set of command line arguments
to reliably cause a crash. Conversely, Bunkerbuster detected that
Figure 6: Basic block coverage for traces forwarded to the
analysis, cumulatively.
imported libraries, demonstrating the importance of being able to
analyze these APIs.
The “# APIs” column counts how many unique function imports
were segmented by Bunkerbuster, using its symbolized memory
snapshots and automatic prototype recovery. In other words, this
is the number of APIs a human analyst would have to build scaf-
folding for if they were not using Bunkerbuster and wanted similar
results. On average, 90 unique APIs were segmented per program,
with counts ranging from 9 (GOOSE) to 278 (GIMP). Bunkerbuster
eliminates the need to manually perform this laborious task.
The “# Traces” column reports how many traces (not segments)
were recorded. On average, the end-host monitored 189 execution
sessions per program. “# Novel” is the number of traces that con-
tained at least 1 novel segment forwarded for analysis. On average,
36 where novel per program. For most programs, even with as few
as 4 traces, at least 1 was filtered, demonstrating the importance
of being able to identify and remove redundant data. The “# Snaps”
column shows the number of trace segments and snapshots for-
warded. On average, 1,710 were forwarded for analysis per program.
In the case of GIMP, our input corpus yielded a comparatively high
number of API snapshots. This is due to GIMP being one of the
largest programs in our dataset, compiled from over 810,000 lines of
C/C++ code, with a sophisticated architecture where each plugin is
itself a standalone executable with additional library dependencies.
For example, one of the babl functions found to contain a vulnera-
bility was not invoked by GIMP directly, but rather by its plugin
for loading PNG images. Trying to naively symbolically execute
46,757,444 basic blocks (from GIMP’s entry point, through the PNG
plugin, into babl) would be difficult for prior work. Bunkerbuster
succeeds thanks to its ability to segment.
“# BBs” records the number of traced basic blocks and “To Find”
reports the number of seconds it took for the analysis to make its
discovery. On average, traces containing bugs were 19,392,602 basic
blocks long and bugs were found in 7,045 seconds, i.e., within 2
hours or so. Some bugs were found in as little as 1 second, while oth-
ers took over 4 hours, depending on the complexity of the recorded
behavior. Interestingly, because Bunkerbuster is able to segment
and snapshot APIs, there is little correlation between trace length
and the time to find bugs. For example, despite one GIMP trace
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91GIMPpdfresurrectdmitryautotracebutterauglijpegtoaviredis-cliabc2mtexgif2pngntpqGraphicsMagickGOOSENginxMiniFTPPHP% of Traces% of Basic BlocksSession 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea328Table 3: Bunkerbuster Vs. AddressSanitizer
QSYM + AS
ID
CVE-2017-9167
CVE-2017-9168
CVE-2017-9169
CVE-2017-9170
CVE-2017-9171
CVE-2017-9172
CVE-2017-9173
CVE-2017-9191
CVE-2017-9192
CVE-2017-9162
CVE-2017-9163
CVE-2017-9182
CVE-2017-9183
CVE-2017-9190
Reported
Reported
Reported
BB
Location
Ovf
input-bmp.c-337
Ovf
input-bmp.c-353
Ovf
input-bmp.c-355
Ovf
input-bmp.c-370
Ovf
input-bmp.c-492
Ovf
input-bmp.c-496
Ovf
input-bmp.c-497
Ovf
input-tga.c-252
Ovf
input-tga.c-528
autotrace.c-191
UAF
pxl-outline.c-106 UAF
color.c-16
UAF
autotrace.c-309
UAF
bitmap.c-24
UAF
UAF
pxl-outline.c-140
UAF
pxl-outline.c-609
color.c-10
UAF
Ovf
Ovf
Ovf
Ovf
Ovf
Ovf
Ovf
Ovf
Ovf
-
-
-
UNDEF
UNDEF
UAF
UNDEF
BADFREE
symbolic format specifiers were being passed to libc, alerting it
to the bug even in non-crashing cases. In general, we observed
that the mutation algorithms used by AFL and QSYM are not well
suited for fuzzing CLIs, which is also noted in AFL’s documentation.
We also observe that of the 4 UAFs listed in Table 2, QSYM only
found 1 and AFL none. QSYM and AFL also struggled to handle
GIMP and GraphicsMagick due to their size and complexity, causing
them to miss 10 and 11 bugs, respectively. It is possible that these
tools would perform better if an expert human analyst created
scaffolding around the imported libraries, but in GIMP’s case, there
are 70 unique libraries with 1,288 exported functions to consider.
Bunkerbuster relieves the analyst of this task.
In almost all of the cases where the prior systems found the
same bug as Bunkerbuster, the former generated over 15 redundant
reports. This is because AFL and QSYM rely on stack traces to
determine the uniqueness of crashes, which are sometimes unre-
liable, such as when dealing with overflows. For example, QSYM
generated 108 reports for CVE-2019-14267 and 246 for CVE-2004-
1257 because a stack corruption mislead it to classify each crash
as unique. Bunkerbuster avoids this fatigue inducing redundancy
using its symbolic root cause analysis, resulting in only 1 report
per bug. Curiously, while QSYM generated more unique crashes
than AFL overall, it only led to the discovery of 1 additional bug.
This is likely due to the sparsity of bugs in real-world programs.
4.3 Comparing Prior Root Cause Techniques
Methodology. In this experiment, we perform the same evalu-
ation as described in Subsection 4.2, with two adjustments made.
First, we focus explicitly on Autotrace for this experiment because
it yields by far the most bugs out of all the real-world programs.
Second, we use AddressSanitizer (AS) to automatically triage the
crashes uncovered by AFL and QSYM, as is common practice in
real-world bug hunting. This allows us to compare the quality of
Bunkerbuster’s root cause analysis to AS.
Over the course of this experiment, QSYM and AFL found 1
crash identified by AS as integer overflow and 1 out-of-bounds
Figure 7: Percentage of unique basic blocks discovered using
breadth-first search, depth-first search, and our proposed ex-
ploration techniques. Our techniques outperform the base-
lines across our entire dataset of 15 real-world programs.
read, which we exclude from the results since these are classes
outside the current scope of Bunkerbuster. For clearer presentation,
we translate binary addresses in our figures to source code line
numbers using debug symbols, postmortem. No system had access
to the symbols during the experiment. In our results, AFL and QSYM
found the same set of bugs, so we only present QSYM for brevity.
Results. After 1 week of analysis, Bunkerbuster yields 17 bug
findings. Conversely, QSYM yields 14 bugs after triaging by AS.
Table 3 presents the two sets of reports side-by-side. Bunkerbuster
finds all of the UAFs and overflows identified in the AS reports
along with 3 UAFs never before reported. Upon investigation, we
discover that the new UAFs reside in code branches missed by
QSYM’s exploration. We believe that given more time, QSYM would
eventually find inputs to reach these branches, whereupon AS
would be able to triage them correctly. However, QSYM did not
accomplish this within the allotted time whereas Bunkerbuster did.
Another interesting observation is that for 4 CVEs, Bunkerbuster
is able to give more precise classifications than AS (bold in Table 3).
In 3 cases, AS reports undefined behavior (UNDEF), meaning that
despite QSYM detecting a crash and providing a concrete input
to AS for analysis, AS still could not decide on a class for the bug.
Conversely, Bunkerbuster correctly identifies the bugs to be UAFs.
In 1 case, AS reports a bad free (BADFREE), meaning that the
address being freed was never allocated, but Bunkerbuster, using
its symbolic constraints, is able to correctly identify that a more
carefully chosen input can turn this bug into a UAF. In summary,
our system finds 3 UAFs missed by QSYM and yields more accurate
classifications than AS in 4 cases.
4.4 Effectiveness of Exploration Techniques
Methodology. To validate whether our proposed exploration tech-
niques enable Bunkerbuster to better search program states while
avoiding path explosion, we compare against two baselines: breadth-
first and depth-first search (BFS, DFS).7 Notice that DFS is the
default exploration technique used by popular symbolic analysis
frameworks [95].
7We include these baselines in the open sourced code repository for reproducibility.
abc2mtexautotraceButterauglidmitrygif2pngGIMPGOOSEGraphicsMagickjpegtoaviMiniFTPNginxntpqpdfressurectPHPredis-cli00.20.40.60.81BFSDFSBunkerbusterUnique States (%)Session 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea329To conduct the experiment, we randomly pick 1 trace for each
of the real-world programs from our dataset and allow each tech-
nique (BFS, DFS, and ours) to explore states for 1 hour per program.
Once the time limit has expired, we halt Bunkerbuster and count
the number of unique basic blocks discovered by each technique.
Since some target programs are slower to explore than others, we
normalize our results by dividing the counts by the total number of
unique blocks discovered globally, across all evaluated techniques,
yielding a percentage from 0% to 100%.
Results. The results of our experiment are presented in Figure 7.
Across all 15 real-world programs, Bunkerbuster’s exploration tech-
niques outperform BFS and DFS. Specifically, for about half of the
programs, Bunkerbuster’s techniques find all the basic blocks BFS
and DFS find, and more. Bunkerbuster also finds more than double
the number of basic blocks than the baselines in many cases, such
as in Dmitry and MiniFTP.
The biggest contrast occurs in Butteraugli, where BFS and DFS
only find about 2% of the blocks discovered by Bunkerbuster. Upon
investigation, we discover that BFS and DFS both get stuck in libz’s
CRC32 checksumming function. Such functions are notorious for
inducing path explosion [25]. Bunkerbuster’s techniques avoid this
function using heuristics to recognize that the contained code is
unlikely to cause our targeted bug classes (e.g., the contained loops
do not perform stepping writes, Subsection 3.5).
Another stark contrast occurs in MiniFTP, where the baselines
only find about 10% of the blocks Bunkerbuster finds. In this case,
BFS, DFS, and Bunkerbuster all focus on MiniFTP’s function for
loading the settings file, which is expensive to explore because
the code is densely packed with string comparisons, another well-
known source of path explosion. However, whereas BFS and DFS
explore this function naively, yielding lower code coverage and
uncovering no bugs within the allotted time, Bunkerbuster priori-
tizes the contained loops using our described heuristics and finds
EDB-46807 in under 10 seconds.
In summary, the heuristics we propose for Bunkerbuster do
in fact help it explore more code in our evaluated dataset in less
time than BFS or DFS. In many cases, the contrast is significant,
with Bunkerbuster’s exploration techniques discovering more than
double the number of basic blocks within the allotted time.
4.5 Performance & Storage
Methodology. To measure the performance and storage over-
heads of Bunkerbuster, we start with the SPEC CPU 2006 benchmark
with a storage quota of 10 GB per end-host. We use the 2006 version
deliberately so our numbers can be directly compared against other
prior full-trace8 PT systems [40, 54]. Since these workloads are
CPU intensive, we consider this to be the worst realistic case for
our system. For another comparison point, we also evaluate Nginx
running PHP with default settings, stressed using ApacheBench
to serve 50,000 HTTP requests for files ranging in size from 100
KB to 100 MB, which we consider to be an I/O bound workload.
Performance overhead is measured with tracing and API snapshots
enabled versus running without the kernel driver installed for the
baseline. Storage is the at-rest size of all collected data. Overheads
8As opposed to systems that use small finite buffers [31, 62].
Table 4: Symbolic Root Cause Verification
Type
CVE / EDB
Ovf
CVE-2004-0597
Ovf
CVE-2004-1257
Ovf