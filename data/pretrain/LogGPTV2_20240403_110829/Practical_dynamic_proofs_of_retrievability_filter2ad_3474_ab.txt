codeword blocks corresponding to one block (say block i).
Since the server has deleted only a small number of blocks,
it can pass the audit with signiﬁcant probability—however,
in this case, block i would become irrecoverable.
The above approach fails to address the eﬃcient update
problem, since whenever the client updates a block, it re-
veals to the server which codeword blocks are related to this
block—and this allows the server to launch the selective
deletion attack as described above. To address this issue,
Cash et al. [8] propose to employ ORAM to hide the access
patterns from the server during updates. However, as men-
tioned in Section 1, known ORAM schemes incurs 400X or
higher bandwidth overhead under typical parameterizations
and constant client-side storage, and thus is impractical.
2.2 Our Idea
As before, suppose the client has n blocks, which are
erasure-coded into n + cn blocks for some small constant
c > 0—we denote the erasure coded copy of data as C. Now
if the client needs to update a block, we encounter the issue
that the client needs to update all of the cn parity blocks.
Our idea is to avoid the need to immediately update the
cn parity blocks upon writes. Instead, the client will place
the newly updated block into an erasure-coded log structure
denoted H, containing recently written blocks. During the
audit, the client will sample blocks to check not only from
the buﬀer C, but also from the log structure H. Note that
since the buﬀer C does not get updated immediately upon
writes, it may contain stale data—however, an up-to-date
snapshot of all blocks is recoverable from the combination of
C and H, both of which are probabilistically checked during
the audit. Two questions however remain:
1. How can reads be supported eﬃciently if the location of
the up-to-date copy of a block is undetermined—since it can
either exist in the buﬀer C, or in the log structure H?
The answer to this ﬁrst question is relatively straight-
forward: one can always keep a separate, up-to-date, and
memory-checked copy of all blocks just to support eﬃcient
reads. The client can verify the integrity (i.e., authentic-
ity and fresheness) of the reads facilitated by the memory
checking scheme (i.e., a Merkle hash tree).
In our basic
construction described in Section 4, this separate copy is
denoted with U. Section 5 describes further optimizations
to reduce the server-side storage by a constant factor.
2. How can the log structure H be eﬃciently updated upon
writes?
The answer to the latter question is much more involved,
and turns out to be the main technical challenge we needed
to overcome. Intuitively, when the log structure H is small,
updating it upon writes should be relatively eﬃcient. How-
ever, as H grows larger, updating it becomes slower. For
example, in the extreme case, when H grows to as large
as C, updating H upon a write would cause an overhead
roughly proportional to cn, as mentioned above.
Perhaps unsurprisingly, in order to achieve eﬃcient amor-
tized cost for updating H upon writes, we use a hierarchical
log structure that is reminiscent of Oblivious RAM construc-
tions [11]. In our construction, the log structure H consists
of exactly (cid:98)log n(cid:99) + 1 levels of exponentially growing capac-
ity, where level i is an erasure coded copy of 2i blocks. At a
very high level, every 2i write operations, level i will be re-
built. Finally, the erasure-coded copy C can be informally
(and a bit imprecisely) thought of as the top level of the
hierarchical log, and is rebuilt every n write operations.
Despite the superﬁcial resemblance to ORAM, our con-
struction is fundamentally diﬀerent from using ORAM as a
blackbox, and thus orders of magnitude more eﬃcient, since
1) we do not aim to achieve access privacy, or rely on access
privacy to prove our PoR guarantees like in the scheme by
Cash et al. [8]; and 2) each level of our hierarchical log struc-
ture H is erasure-coded. For this reason, we need a special
erasure coding scheme that can be incrementally built over
time (see Section 4 for details).
3. PRELIMINARIES
We begin with the deﬁnition of a dynamic PoR scheme,
as given by Cash, K¨up¸c¨u, and Wichs [8]. A dynamic POR
scheme is a collection of the following four protocols between
a stateful client C and a stateful server S.
(st, ¯M) ← Init(1λ, n, β,M): On input the security param-
eter λ and the database M of n β-bit-size entries, it
outputs the client state st and the server state ¯M.
{B, reject} ← Read(i, st, ¯M): On input an index i ∈ [n],
the client state st and the server state ¯M, it outputs
B = M[i] or reject.
{(st(cid:48), ¯M(cid:48)), reject} ← Write(i, B, st, ¯M): On input an index
i ∈ [n], data B, the client state st and the server state
¯M, it sets M[i] = B and outputs a new client state
st(cid:48) and a new server state ¯M(cid:48) or reject.
{accept, reject} ← Audit(st, ¯M): On input the client state
st and the server state ¯M, it outputs accept or reject.
Depending on whether the client state st in the above deﬁ-
nition must be kept secret or not, we say that the dynamic
PoR scheme is secretly or publicly veriﬁable.
3.1 Security Deﬁnitions
We deﬁne security, namely authenticity and retrievability
in the same way as Cash et al. [8].
Authenticity. The authenticity requirement stipulates that
the client can always detect (except with negligible probabil-
ity) if any message sent by the server deviates from honest
327behavior. We use the following game between a challenger C,
a malicious server S∗ and an honest server S for the adaptive
version of authenticity, in the same way as Cash et al. [8].
• S∗ chooses initial memory M. C runs Init(1λ, n, β,M)
and sends the initial memory layout ¯M to S∗. C also
interacts with S and sends ¯M to S.
• For polynomial number of steps t = 1, 2, . . . , poly(λ),
S∗ picks an operation opt where operation opt is either
Read(i, st, ¯M) or Write(i, B, st, ¯M) or Audit(st, ¯M). C
executes the protocol with both S∗ and S.
S∗ is said to win the game, if at any time, the message sent
by S∗ diﬀers from that of S, and C did not output reject.
Definition 1
(Authenticity). A PoR scheme satis-
ﬁes adaptive authenticity, if no polynomial-time adversary
S∗ has more than negligible probability in winning the above
security game.
Retrievability. Intuitively, the retrievability requirement
stipulates that whenever a malicious server can pass the
audit test with non-negligible probability, the server must
know the entire memory contents M; and moreover, one is
able to extract M by repeatedly running the Audit protocol
with the server. Formally, retrievability is deﬁned with a
security game as below, in the same way as Cash et al. [8].
• Initialization phase. The adversary S∗ chooses the
initial memory M. The challenger runs Init(1λ, n, β,M),
and uploads the initial memory layout ¯M to S∗.
• Query phase. For t = 1, 2, . . . , poly(λ), the adversary
S∗ adaptively chooses an operation opt where opt is of the
form Read(i, st, ¯M), Write(i, B, st, ¯M), or Audit(st, ¯M).
The challenger executes the respective protocols with S∗.
At the end of the query phase, suppose the state of the
challenger and the adversary is state C and state S respec-
tively and the ﬁnal state of the memory contents is M.
• Challenge phase. The challenger now gets blackbox
rewinding access in the conﬁguration state S. The chal-
lenger runs the Audit protocol repeatedly for a polynomial
number of times with the server S∗, starting from the con-
ﬁguration (state C , state S). Denote with π1, π2, . . . , πpoly(λ)
the transcripts of all the successful Audit executions.
Definition 2
(Retrievability). A PoR scheme sat-
isﬁes retrievability, if there exists a polynomial-time extrac-
tor algorithm denoted M(cid:48) ← Extract(state C ,{πi}), such that
for any polynomial-time S∗, if S∗ passes the Audit proto-
col with non-negligible probability, then after executing the
Audit protocol with S∗ for a polynomial number of times,
the Extract algorithm can output the correct memory con-
tents M(cid:48) = M except with negligible probability.
We note here that in the above deﬁnition, “correct memory
contents” are the contents of the memory M after the end
of the query phase in the security game.
3.2 Building Blocks
Erasure codes. Our construction makes use of erasure
codes [27] as deﬁned below.
Figure 1: Server-side storage layout.
Definition 3
(Erasure codes). Let Σ denote a ﬁnite
alphabet. An (m, n, d)Σ erasure code is deﬁned to be a pair of
algorithms encode : Σn → Σm, and decode : Σm−d+1 → Σn,
such that as long as the number of erasures is bounded by
d − 1, decode can always recover the original data. A code
is maximum distance separable (MDS), if n + d = m + 1.
Authenticated structures. For describing our basic con-
struction, we can assume that we use a standard Merkle
hash tree [16] to ensure the authenticity and freshness of all
blocks (encoded or unencoded, stored across buﬀers U, C
and H). However, later, we will show that in fact, only the
raw buﬀer U needs to be veriﬁed with a Merkle hash tree;
whereas the erasure-coded copy C and the hierarchical log
structure H only require time/location-dependent MACs to
ensure authenticity and freshness. This will lead to signiﬁ-
cant savings (speciﬁcally, by a multiplicative log n factor).
4. BASIC CONSTRUCTION
To begin with, assume the data to be outsourced contains
n blocks, and each block is from an alphabet Σ. For simplic-
ity, we will ﬁrst assume that n is determined in advance—we
will later explain in Section 6 how to expand or shrink the
storage.
We ﬁrst describe a basic construction that requires the
client to perform O(β log n) computation for each block writ-
ten (recall β is the size of the block). Later in Section 5 we
will describe how to rely on homomorphic checksums to re-
duce this cost to β + O(log n).
4.1 Server-Side Storage Layout
The server-side storage is organized in three diﬀerent buﬀers
denoted with U (stands for unencoded ), C (stands for coded )
and H (stands for hierarchical ). We now explain in detail
the function of these buﬀers (see also Figure 1).
Raw buﬀer. All up-to-date blocks are stored in original,
unencoded format in a buﬀer called U. Reads are performed
by reading the corresponding location in U. Writes update
the corresponding location in the buﬀer U immediately with
the newly written block. However, unlike reads, writes also
cause updates to a hierarchical log as explained later.
Erasure-coded copy.
In addition, we store an (m, n, d)
erasure-coded copy of the data in a buﬀer C, where m =
Θ(n), and d = m− n + 1 = Θ(n), i.e., the code is maximum
distance separable. The buﬀer C does not immediately get
updated upon writes, and therefore may contain stale data.
Hierarchical log of recent writes. A hierarchical log
structure denoted H stores recently overwritten blocks in
erasure-coded format. H contains k + 1 levels, where k =
(cid:98)log n(cid:99). We denote the levels of H as (H0, H1, . . . , Hk).
......Erasure−coded blocksUnencoded blocksC:Erasure−coded, recentlywritten blocksH:U:328Figure 2: Rebuilding of level H3. When a newly written
block is added to the hierarchical log structure H, consec-
utive levels H0, H1, H2 are ﬁlled. A rebuild operation for
H3 occurs as a result, at the end of which H3 is ﬁlled, and
H0, H1, H2 are empty. Unlike ORAM schemes that employ
oblivious sorting for the rebuilidng, our rebuilding algorithm
involves computing linear combinations of blocks.
Each level (cid:96) ∈ {0, 1, . . . , k} is an erasure code of some 2(cid:96)
blocks. For every block in H, we deﬁne its age to be the
time elapsed since when the block was written. Level H0
contains the most recently written block; and the age of the
blocks contained in level H(cid:96) increases with (cid:96). Particularly
Hk (if ﬁlled) contains the oldest blocks.
Note that in practice, it is possible that the client keeps
writing the same block, say Bi where i ∈ [n]. In this case,
the hierarchical log structure H contains multiple copies of
Bi. (as we will see later, duplicates are suppressed when the
erasure-coded copy C is rebuilt.) Particularly, every time a
block is written, the new value of the block along with its
block identiﬁer is added to the hierarchical log structure H.
4.2 Operations
After describing the three types of buﬀers that we use to
organize the original blocks, we are ready to give the high
level intuition of the operations of our construction.
Reading blocks. In order to read a block, we read it di-
rectly from U. Along with the block, the server returns
the respective Merkle hash tree proof, allowing the client to
verify the authenticity and the freshness of the block.
PoR audits. A PoR audit involves the following checks:
1. Checking the authenticity of O(λ) random blocks from
the erasure-coded copy C;
2. Checking the authenticity of O(λ) random blocks from
each ﬁlled level H(cid:96), where (cid:96) ∈ [k].
Writing blocks and periodic rebuilding operations.
Every write not only updates the raw buﬀer U, but also
causes the newly written block to be added to the hierar-
chical log structure H (speciﬁcally, the new block is always
added into H0). Whenever a block is added, assume that
levels H0, H1, . . . , H(cid:96) are consecutively ﬁlled levels where
(cid:96) < k, and level (cid:96) + 1 is the ﬁrst empty level. This leads to
the rebuilding of level H(cid:96)+1. At the end of this rebuilding:
1) level H(cid:96)+1 contains an erasure code of all blocks currently
residing in levels H0, H1, . . . , H(cid:96) and the newly added block;
and 2) level H0, H1, . . . H(cid:96) are emptied (see Figure 2).
This rebuilding technique has been previously used in var-
ious ORAM schemes (e.g., [25]). However, unlike ORAM
schemes that require oblivious sorting for the rebuilding, our
rebuilding process requires just computing an erasure code
of the new level. This is a lot simpler since it just involves
computing linear combinations of blocks (we employ a linear
coding scheme). We will later show that it takes O(β · 2(cid:96))
time to rebuild level (cid:96) ((cid:96) = 0, 1 . . . ,(cid:98)log n(cid:99)), where β is the
block size. Since each level (cid:96) is rebuilt every 2(cid:96) write opera-
tions, the amortized rebuilding cost per write is O(β log n).
In comparison, ORAM schemes [12, 13, 15] require roughly
O(β(log n)2/ log log n) or higher amortized cost for the re-
building due to oblivious sorting.
For now, we assume that the client is performing the re-
building for simplicity. Later in Section 5, we will show how
to have the server perform the rebuilding computations, and
the client instead simply veriﬁes that the server adheres to
the prescribed behavior. This will allow us to further reduce
the bandwidth overhead from O(β log n) to β + O(λ log n).
Periodic rebuilding of the erasure coded copy. Fi-
nally, every n write operations, the erasure coded copy C is
rebuilt, and all levels H0, H1, . . . , Hk are emptied as a re-
sult. Recall that the log structure H may contain multiple
copies of the same block, if that block is written multiple