After demonstrating the effectiveness of our attacks against
the updating set with a single sample, we now focus on a
more general attack scenario where the updating set contains
multiple data samples that are never seen during the training.
We introduce two attacks in the multi-sample attack class:
Multi-sample label distribution estimation attack and multi-
sample reconstruction attack.
1296    29th USENIX Security Symposium
USENIX Association
0.000.020.040.060.080.10Meansquarederror(MSE)AE(Oracle)ASSRLabel-randomRandom0.0000.0050.0100.0150.0200.0250.0300.035Meansquarederror(MSE)AE(Oracle)ASSRLabel-randomRandom0.00.20.40.60.81.01.21.41.6Meansquarederror(MSE)AE(Oracle)ASSRLabel-randomRandom(a) KL-divergence (10 samples)
(b) KL-divergence (100 samples)
(c) Accuracy (10 samples)
(d) Accuracy (100 samples)
Figure 6: [Lower is better for (a) and (b), higher is better for (c) and (d)] Performance of the multi-sample label distribution
estimation attack (ALDE) together with the baseline model and transfer attack. KL-divergence and accuracy are adopted as the
evaluation metric. Accuracy here is used to measure the prediction of the most frequent label over samples in the updating
set. Transfer 10-100 means each of the training sample for the attack model corresponds to an updating set containing 10 data
samples and each of the testing sample for the attack model corresponds to an updating set containing 100 data samples.
5.1 Multi-sample Label Distribution Estima-
tion Attack
Attack Deﬁnition. Our ﬁrst attack in the multi-label attack
class aims at estimating the label distribution of the updating
set’s samples. It can be considered as a generalization of
the label inference attack in the single-sample attack class.
Formally, the attack is deﬁned as:
ALDE : δ (cid:55)→ q
where q as a vector denotes the distribution of labels over all
classes for samples in the updating set.
Methodology. The adversary uses the same encoder structure
as presented in Section 3 and the same decoder structure of the
label inference attack (Section 4.1). Since the label distribu-
tion estimation attack estimates a probability vector q instead
of performing classiﬁcation, we use Kullback-–Leibler diver-
gence (KL-divergence) as our objective function:
LKL = ∑
i
( ˆq(cid:96))i log ( ˆq(cid:96))i
(q(cid:96))i
where ˆq(cid:96) and q(cid:96) represent our attack’s estimated label distri-
bution and the target label distribution, respectively, and (q(cid:96))i
corresponds to the ith label.
shadow···δm
To train the attack model ALDE, the adversary ﬁrst gener-
ates her training data as mentioned in Section 3. She then
trains ALDE with the posterior difference δ1
shadow
as the input and the normalized label distribution of their
corresponding updating sets as the output. We assume the
adversary knows the cardinality of the updating set. We try to
relax this assumption later in our evaluation.
Experimental Setup. We evaluate our label distribution esti-
mation attack using updating set of cardinalities 10 and 100.
For the two different cardinalities, we build attack models
as mentioned in the methodology. All data samples in each
updating set for the shadow and target models are sampled
uniformly, thus each sample (in both training and testing set)
for the attack model, which corresponds to an updating set,
has the same label distribution of the original dataset. We use
a batch size of 64 when updating the models.
For evaluation metrics, we calculate KL-divergence for
each testing sample (corresponding to an updating set on the
target model) and report the average result over all testing
samples (1,000 in total). Besides, we also measure the accu-
racy of predicting the most frequent label over samples in the
updating set. We randomly sample a dataset with the same
size as the updating set and use its samples’ label distribution
as the baseline, namely Random.
Results. We report the result for our label distribution esti-
mation attack in Figure 6. As shown, ALDE achieves a signif-
icantly better performance than the Random baseline on all
datasets. For the updating set with 100 data samples on the
CIFAR-10 dataset, our attack achieves 3 and 2.5 times better
accuracy and KL-divergence, respectively, than the Random
baseline. Similarly, for the MNIST and Insta-NY datasets,
our attack achieves 1.5 and 4.8 times better accuracy, and
2 and 7.9 times better KL-divergence. Furthermore, ALDE
achieves a similar improvement over the Random baseline for
the updating set of size 10.
Recall that the adversary is assumed to know the cardinal-
ity of the updating set in order to train her attack model, we
further test whether we can relax this assumption. To this end,
we ﬁrst update the shadow model with 100 samples while
updating the target model with 10 samples. As shown in Fig-
ure 6a and Figure 6c Transfer 100-10, our attack still has a
similar performance as the original attack. However, when
the adversary updates her shadow model with 10 data sam-
ples while the target model is updated with 100 data samples
(Figure 6b and Figure 6d Transfer 10-100), our attack perfor-
mance drops signiﬁcantly, in particular for KL-divergence on
the CIFAR-10 dataset. We believe this is due to the 10 sam-
ples not providing enough information for the attack model
to generalize to a larger updating set.
USENIX Association
29th USENIX Security Symposium    1297
MNISTCIFAR-10Insta-NY0.000.020.040.060.080.100.12KL-divergenceALDERandomTransfer100-10MNISTCIFAR-10Insta-NY0.0000.0050.0100.0150.0200.0250.0300.0350.040KL-divergenceALDERandomTransfer10-100MNISTCIFAR-10Insta-NY0.00.10.20.30.40.50.6AccuracyALDERandomTransfer100-10MNISTCIFAR-10Insta-NY0.00.20.40.60.8AccuracyALDERandomTransfer10-100a Gaussian noise vector z ∼ N (0,1) to a data sample ˆx,
G : z (cid:55)→ ˆx
such that the generated sample ˆx is indistinguishable from
a true data sample. This is enabled by the discriminator D
which is jointly trained. The generator G tries to fool the
discriminator, which is trained to distinguish between samples
from the Generator (G) and true data samples. The objective
function maximized by GAN’s discriminator D is,
LD = Ex∈Dupdate log(D(x)) + E ˆx log(1− D( ˆx))
(1)
The GAN discriminator D is trained to output 1 (“true”) for
real data and 0 (“false”) for fake data. On the other hand, the
generator G maximizes:
LG = E ˆx log(D( ˆx))
Thus, G is trained to produce samples ˆx = G(z) that are clas-
siﬁed as “true” (real) by D.
However, our attack aims to reconstruct Dupdate for any
given δ, which the standard GAN does not support. There-
fore, ﬁrst, we change the GAN into a conditional model to
condition its generated samples ˆx on the posterior difference
δ. Second, we construct our novel hybrid generative model
CBM-GAN, by adding a new “Best Match” loss to reconstruct
all samples inside the updating set accurately.
CBM-GAN. The decoder of our attack model is casted as our
CBM-GAN’s generator (G). To enable this, we concatenate
the noise vector z and the latent vector µ produced by our
attack model’s encoder (with posterior different as input), and
use it as CBM-GAN’s generator’s input, as in Conditional
GANs [30]. This allows our decoder to map the posterior
difference δ to samples in Dupdate.
However, Conditional GANs are severely prone to mode
collapse, where the generator’s output is restricted to a limited
subset of the distribution [7, 51]. To deal with this, we intro-
duce a reconstruction loss. This reconstruction loss forces
our GAN to cover all the modes of the distribution (set) of
data samples used to update the model. However, it is unclear,
given a posterior difference δ and a noise vector z pair, which
sample in the data distribution we should force CBM-GAN to
reconstruct. Therefore, we allow our GAN full ﬂexibility in
learning a mapping from posterior difference and noise vector
z pairs to data samples – this means we allow it to choose
the data sample to reconstruct. We realize this using a novel
“Best Match” based objective in the CBM-GAN formulation,
LBM = ∑
x∈Dupdate
min
ˆx∼G(cid:107) ˆx− x(cid:107)2
2 +∑
ˆx
log(D( ˆx))
(2)
where ˆx ∼ G represents samples produced by our CBM-GAN
given a latent vector µ and noise sample z. The ﬁrst part of the
LBM objective is based on the standard MSE reconstruction
loss and forces our CBM-GAN to reconstruct all samples in
Figure 7: Methodology of the multi-sample reconstruction
attack (AMSR).
5.2 Multi-sample Reconstruction Attack
Attack Deﬁnition. Our last attack, namely multi-sample re-
construction attack, aims at reconstructing the updating set.
This attack can be considered as a generalization of the single-
sample reconstruction attack, and a step towards the goal
of reconstructing the training set of a black-box ML model.
Formally, the attack is deﬁned as follows:
AMSR : δ (cid:55)→ Dupdate
update, . . . ,x|Dupdate|
where Dupdate = {x1
update } contains the samples
used to update the model.
Methodology. The complexity of the task for reconstructing
an updating set increases signiﬁcantly when the updating set
size grows from one to multiple. Our single-sample recon-
struction attack (Section 4.2) uses AE to reconstruct a single
sample. However, AE cannot generate a set of samples. In
fact, directly predicting a set of examples is a very challenging
task. Therefore, we rely on generative models which are able
to generate multiple samples rather than a single one.
We ﬁrst introduce the classical Generative Adversarial Net-
works (GANs) and point out why classical GANs cannot be
used for our multi-sample reconstruction attack. Next, we
propose our Conditional Best of Many GAN (CBM-GAN), a
novel hybrid generative model and demonstrate how to use it
to execute the multi-sample reconstruction attack.
Generative Adversarial Networks. Samples from a dataset are
essentially samples drawn from a complex data distribution.
Thus, one way to reconstruct the dataset Dupdate is to learn
this complex data distribution and sample from it. This is the
approach we adopt for our multi-sample reconstruction attack.
Mainly, the adversary starts the attack by learning the data dis-
tribution of Dupdate, then she generates multiple samples from
the learned distribution, which is equivalent to reconstructing
the dataset Dupdate. In this work, we leverage the state-of-the-
art generative model GANs, which has been demonstrated
effective on learning a complex data distribution.
A GAN consists of a pair of ML models: a generator (G)
and a discriminator (D). The generator G learns to transform
1298    29th USENIX Security Symposium
USENIX Association
EncoderGeneratorStandard Gaussian NoiseDecoderClustering(a)
(b)
(c)
(d)
Figure 8: Visualization of some generated samples from the multi-sample reconstruction attack (AMSR) before clustering on the
CIFAR-10 dataset. Samples are fair random draws, not cherry-picked. The left column shows the original samples and the next 5
columns show the 5 nearest reconstructed samples with respect to mean squared error.
Dupdate as the error is summed across x ∈ Dupdate. However,
unlike the standard MSE loss, given a data sample x ∈ Dupdate,
the loss is based only on the generated sample ˆx which is
closest to the data sample x ∈ Dupdate. This allows CBM-
GAN to reconstruct samples in Dupdate without having an
explicit mapping from posterior difference and noise vector z
pairs to data samples, as only the “Best Match” is penalized.
Finally, the discriminator D ensures that the samples ˆx are
indistinguishable from the “true” samples of Dupdate.
Training of CBM-GAN. The training of the attack model AMSR
is more complicated than previous attacks, hence we provide
more details here. Similar to the previous attacks, the adver-
sary starts the training by generating the training data as men-
tioned in Section 3. She then jointly trains her encoder and
CBM-GAN with the posterior difference δ1
shadow as
the inputs and samples inside their corresponding updating
sets, i.e., Dupdate1
shadow ···Dupdatem
shadow as the output. More concretely,
for each posterior difference δi
shadow, she updates her attack
model AMSR as follows:
shadow···δm
1. The adversary sends the posterior difference δi
shadow to
her encoder to get the latent vector µi.
2. She then generates |Dupdatei
3. To create generator’s input, she concatenates each of the
shadow| noise vectors.
noise vectors with the latent vector µi.
4. On the input of the concatenated vectors, the CBM-
shadow| samples, i.e., each vector cor-
GAN generates |Dupdatei
responds to each sample.
5. The adversary then calculates the generator loss as intro-
duced by Equation 2, and uses it to update the generator
and the encoder.
6. Finally, she calculates and updates the CBM-GAN’s dis-
criminator according to Equation 1.
Clustering. CBM-GAN only provides a generator which
learns the distribution of the samples in the updating set. How-
ever, to reconstruct the exact data samples in Dupdate, we need
a ﬁnal step assisted by machine learning clustering. In detail,
we assume the adversary knows the cardinality of Dupdate as in
Section 5.1. After CBM-GAN is trained, the adversary utilizes
CBM-GAN’s generator to generate a large number of samples.
She then clusters the generated samples into |Dupdate| clusters.
Here, the K-means algorithm is adopted to perform clustering
where we set K to |Dupdate|. In the end, for each cluster, the
adversary calculates its centroid, and takes the nearest sample
to the centroid as one reconstructed sample.
Figure 7 presents a schematic view of our multi-sample
reconstruction attack’s methodology. The concrete architec-
ture of CBM-GAN’s generator and discriminator for the three
datasets used in this paper are listed in Appendix E.
Experimental Setup. We evaluate the multi-sample recon-
struction attack on the updating set of size 100 and generate
20,000 samples for each updating set reconstruction with
CBM-GAN. For the rest of the experimental settings, we fol-
low the one mentioned in Section 5.1 except for evaluation
metrics and baseline.
We use MSE between the updating and reconstructed data
samples to measure the multi-sample reconstruction attack’s
performance. We construct two baselines, namely Shadow-
clustering and Label-average. For Shadow-clustering, we per-
form K-means clustering on the adversary’s shadow dataset.
More concretely, we cluster the adversary’s shadow dataset
into 100 clusters and take the nearest sample to the centroid of
each cluster as one reconstructed sample. For Label-average,
we calculate the MSE between each sample in the updating
set and the average of the images with the same label in the
adversary’s shadow dataset.
Results. In Figure 8, we ﬁrst present some visualization of
the intermediate result of our attack, i.e., the CBM-GAN’s
output before clustering, on the CIFAR-10 dataset. For each
randomly sampled image in the updating set, we show the