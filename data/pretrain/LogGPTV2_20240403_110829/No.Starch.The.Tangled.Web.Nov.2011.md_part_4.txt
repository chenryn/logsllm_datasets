Mosaic (ultimately acquired by Microsoft and renamed Internet Explorer).
Ahandful of competing non-Mosaic engines emerged as well, including
Opera and several text-based browsers (such as Lynx and w3m). The first
search engines, online newspapers, and dating sites followed soon after.
The First Browser Wars: 1995 to 1999
By the mid-1990s, it was clear that the Web was here to stay and that users
were willing to ditch many older technologies in favor of the new contender.
Around that time, Microsoft, the desktop software behemoth that had been
slow to embrace the Internet before, became uncomfortable and began
toallocate substantial engineering resources to its own browser, eventually
bundling it with the Windows operating system in 1996.* Microsoft’s actions
sparked a period colloquially known as the “browser wars.”
The resulting arms race among browser vendors was characterized by the
remarkably rapid development and deployment of new features in the compet-
ing products, a trend that often defied all attempts to standardize or even prop-
erly document all the newly added code. Core HTML tweaks ranged from the
silly (the ability to make text blink, a Netscape invention that became the butt
of jokes and a telltale sign of misguided web design) to notable ones, such as
the ability to change typefaces or embed external documents in so-called frames.
Vendors released their products with embedded programming languages such
as JavaScript and Visual Basic, plug-ins to execute platform-independent Java
* Interestingly, this decision turned out to be a very controversial one. On one hand, it could
beargued that in doing so, Microsoft contributed greatly to the popularization of the Internet.
On the other, it undermined the position of competing browsers and could be seen as anti-
competitive. In the end, the strategy led to a series of protracted legal battles over the possible
abuse of monopoly by the company, such as United States v. Microsoft.
10 Chapter 1
or Flash applets on the user’s machine, and useful but tricky HTTP extensions
such as cookies. Only a limited degree of superficial compatibility, sometimes
hindered by patents and trademarks,* would be maintained.
As the Web grew larger and more diverse, a sneaky disease spread across
browser engines under the guise of fault tolerance. At first, the reasoning
seemed to make perfect sense: If browser A could display a poorly designed,
broken page but browser B refused to (for any reason), users would inevita-
bly see browser B’s failure as a bug in that product and flock in droves to the
seemingly more capable client, browser A. To make sure that their browsers
could display almost any web page correctly, engineers developed increas-
ingly complicated and undocumented heuristics designed to second-guess
the intent of sloppy webmasters, often sacrificing security and occasionally
even compatibility in the process. Unfortunately, each such change further
reinforced bad web design practices† and forced the remaining vendors to
catch up with the mess to stay afloat. Certainly, the absence of sufficiently
detailed, up-to-date standards did not help to curb the spread of this disease.
In 1994, in order to mitigate the spread of engineering anarchy and gov-
ern the expansion of HTML, Tim Berners-Lee and a handful of corporate
sponsors created the World Wide Web Consortium (W3C). Unfortunately
for this organization, for a long while it could only watch helplessly as the for-
mat was randomly extended and tweaked. Initial W3C work on HTML 2.0
and HTML 3.2 merely tried to catch up with the status quo, resulting in half-
baked specs that were largely out-of-date by the time they were released to
the public. The consortium also tried to work on some novel and fairly well-
thought-out projects, such as Cascading Style Sheets, but had a hard time get-
ting buy-in from the vendors.
Other efforts to standardize or improve already implemented mecha-
nisms, most notably HTTP and JavaScript, were driven by other auspices such
as the European Computer Manufacturers Association (ECMA), the Interna-
tional Organization for Standardization (ISO), and the Internet Engineering
Task Force (IETF). Sadly, the whole of these efforts was seldom in sync, and
some discussions and design decisions were dominated by vendors or other
stakeholders who did not care much about the long-term prospects of the tech-
nology. The results were a number of dead standards, contradictory advice,
and several frightening examples of harmful cross-interactions between other-
wise neatly designed protocols—a problem that will be particularly evident
when we discuss a variety of content isolation mechanisms in Chapter 9.
The Boring Period: 2000 to 2003
As the efforts to wrangle the Web floundered, Microsoft’s dominance grew
asa result of its operating system–bundling strategy. By the beginning of the
new decade, Netscape Navigator was on the way out, and Internet Explorer
* For example, Microsoft did not want to deal with Sun to license a trademark for JavaScript
(alanguage so named for promotional reasons and not because it had anything to do with Java),
so it opted to name its almost-but-not-exactly-identical version “JScript.” Microsoft’s official
documentation still refers to the software by this name.
† Prime examples of misguided and ultimately lethal browser features are content and character
set–sniffing mechanisms, both of which will be discussed in Chapter 13.
Security in the World of Web Applications 11
held an impressive 80 percent market share—a number roughly comparable
to what Netscape had held just five years before. On both sides of the fence,
security and interoperability were the two most notable casualties of the fea-
ture war, but one could hope now that the fighting was over, developers
could put differences aside and work together to fix the mess.
Instead, dominance bred complacency: Having achieved its goals bril-
liantly, Microsoft had little incentive to invest heavily in its browser. Although
through version 5, major releases of Internet Explorer (IE) arrived yearly,
it took two years for version 6 to surface, then five full years for Internet
Explorer 6 to be updated to Internet Explorer 7. Without Microsoft’s inter-
est, other vendors had very little leverage to make disruptive changes; most
sites were unwilling to make improvements that would work for only a small
fraction of their visitors.
On the upside, the slowdown in browser development allowed the
W3Cto catch up and to carefully explore some new concepts for the future
of theWeb. New initiatives finalized around the year 2000 included HTML 4
(a cleaned-up language that deprecated or banned many of the redundant or
politically incorrect features embraced by earlier versions) and XHTML 1.1 (a
strict and well-structured XML-based format that was easier to unambiguously
parse, with no proprietary heuristics allowed). The consortium also made signif-
icant improvements to JavaScript’s Document Object Model and to Cascading
Style Sheets. Regrettably, by the end of the century, the Web was too mature to
casually undo some of the sins of the old, yet too young for the security issues to
be pressing and evident enough for all to see. Syntax was improved, tags were
deprecated, validators were written, and deck chairs were rearranged, but the
browsers remained pretty much the same: bloated, quirky, and unpredictable.
But soon, something interesting happened: Microsoft gave the world a
seemingly unimportant, proprietary API, confusingly named XMLHttpRequest.
This trivial mechanism was meant to be of little significance, merely an
attempt to scratch an itch in the web-based version of Microsoft Outlook.
ButXMLHttpRequest turned out to be far more, as it allowed for largely
unconstrained asynchronous HTTP communications between client-side
JavaScript and the server without the need for time-consuming and disrup-
tive page transitions. In doing so, the API contributed to the emergence of
what would later be dubbed web 2.0—a range of complex, unusually respon-
sive, browser-based applications that enabled users to operate on complex
data sets, collaborate and publish content, and so on, invading the sacred
domain of “real,” installable client software in the process. Understandably,
this caused quite a stir.
Web 2.0 and the Second Browser Wars: 2004 and Beyond
XMLHttpRequest, in conjunction with the popularity of the Internet and the
broad availability of web browsers, pushed the Web to some new, exciting
frontiers—and brought us a flurry of security bugs that impacted both indi-
vidual users and businesses. By about 2002, worms and browser vulnerabili-
ties had emerged as a frequently revisited theme in the media. Microsoft, by
virtue of its market dominance and a relatively dismissive security posture,
12 Chapter 1
took much of the resulting PR heat. The company casually downplayed the
problem, but the trend eventually created an atmosphere conducive to a
small rebellion.
In 2004, a new contender in the browser wars emerged: Mozilla Firefox
(a community-supported descendant of Netscape Navigator) took the offen-
sive, specifically targeting Internet Explorer’s poor security track record and
standards compliance. Praised by both IT journalists and security experts,
Firefox quickly secured a 20 percent market share. While the newcomer soon
proved to be nearly as plagued by security bugs as its counterpart from Red-
mond, its open source nature and the freedom from having to cater to stub-
born corporate users allowed developers to fix issues much faster.
NOTE Why would vendors compete so feverishly? Strictly speaking, there is no money to be
made by having a particular market share in the browser world. That said, pundits
have long speculated that it is a matter of power: By bundling, promoting, or demoting
certain online services (even as simple as the default search engine), whoever controls
the browser controls much of the Internet.
Firefox aside, Microsoft had other reasons to feel uneasy. Its flagship prod-
uct, the Windows operating system, was increasingly being used as an (expend-
able?) launch pad for the browser, with more and more applications (from
document editors to games) moving to the Web. This could not be good.
These facts, combined with the sudden emergence of Apple’s Safari
browser and perhaps Opera’s advances in the world of smartphones, must
have had Microsoft executives scratching their heads. They had missed
theearly signs of the importance of the Internet in the 1990s; surely they
couldn’t afford to repeat the mistake. Microsoft put some steam behind
Internet Explorer development again, releasing drastically improved and
somewhat more secure versions 7, 8, and 9 in rapid succession.
Competitors countered with new features and claims of even better (if still
superficial) standards compliance, safer browsing, and performance improve-
ments. Caught off guard by the unexpected success of XMLHttpRequest and
quick to forget other lessons from the past, vendors also decided to experi-
ment boldly with new ideas, sometimes unilaterally rolling out half-baked or
somewhat insecure designs like globalStorage in Firefox or httponly cookies in
Internet Explorer, just to try their luck.
To further complicate the picture, frustrated by creative differences with
W3C, a group of contributors created a wholly new standards body called the
Web Hypertext Application Technology Working Group (WHATWG). The
WHATWG has been instrumental in the development of HTML5, the first
holistic and security-conscious revision of existing standards, but it is report-
edly shunned by Microsoft due to patent policy disputes.
Throughout much of its history, the Web has enjoyed a unique, highly
competitive, rapid, often overly political, and erratic development model
with no unifying vision and no one set of security principles. This state of
affairs has left a profound mark on how browsers operate today and how
secure the user data handled by browsers can be.
Chances are, this situation is not going to change anytime soon.
Security in the World of Web Applications 13
The Evolution of a Threat
Clearly, web browsers, and their associated document formats and communi-
cation protocols, evolved in an unusual manner. This evolution may explain
the high number of security problems we see, but by itself it hardly proves
that these problems are unique or noteworthy. To wrap up this chapter, let’s
take a quick look at the very special characteristics behind the most prevalent
types of online security threats and explore why these threats had no particu-
larly good equivalents in the years before the Web.
The User as a Security Flaw
Perhaps the most striking (and entirely nontechnical) property of web
browsers is that most people who use them are overwhelmingly unskilled.
Sure, nonproficient users have been an amusing, fringe problem since the
dawn of computing. But the popularity of the Web, combined with its remark-
ably low barrier to entry, means we are facing a new foe: Most users simply
don’t know enough to stay safe.
For a long time, engineers working on general-purpose software have
made seemingly arbitrary assumptions about the minimal level of computer
proficiency required of their users. Most of these assumptions have been with-
out serious consequences; the incorrect use of a text editor, for instance, would
typically have little or no impact on system security. Incompetent users simply
would not be able to get their work done, a wonderfully self-correcting issue.
Web browsers do not work this way, however. Unlike certain complicated
software, they can be successfully used by people with virtually no computer
training, people who may not even know how to use a text editor. But at the
same time, browsers can be operated safely only by people with a pretty good
understanding of computer technology and its associated jargon, including
topics such as Public-Key Infrastructure. Needless to say, this prerequisite is
not met by most users of some of today’s most successful web applications.
Browsers still look and feel as if they were designed by geeks and for
geeks, complete with occasional cryptic and inconsistent error messages,
complex configuration settings, and a puzzling variety of security warnings
and prompts. A notable study by Berkeley and Harvard researchers in 2006
demonstrated that casual users are almost universally oblivious to signals that
surely make perfect sense to a developer, such as the presence or absence
oflock icons in the status bar.4 In another study, Stanford and Microsoft
researchers reached similar conclusions when they examined the impact of
the modern “green URL bar” security indicator. The mechanism, designed
to offer a more intuitive alternative to lock icons, actually made it easier to
trick users by teaching the audience to trust a particular shade of green, no
matter where this color appeared.5
Some experts argue that the ineptitude of the casual user is not the
faultof software vendors and hence not an engineering problem at all. Others
note that when creating software so easily accessible and so widely distributed,
it is irresponsible to force users to make security-critical decisions that depend
on technical prowess not required to operate the program in the first place.
14 Chapter 1
To blame browser vendors alone is just as unfair, however: The computing
industry as a whole has no robust answers in this area, and very little research
is available on how to design comparably complex user interfaces (UIs) in a
bulletproof way. After all, we barely get it right for ATMs.
The Cloud, or the Joys of Communal Living
Another peculiar characteristic of the Web is the dramatically understated
separation between unrelated applications and the data they process.
In the traditional model followed by virtually all personal computers
over the last 15 years or so, there are very clear boundaries between high-
level data objects (documents), user-level code (applications), and the oper-
ating system kernel that arbitrates all cross-application communications and
hardware input/output (I/O) and enforces configurable security rules should
an application go rogue. These boundaries are well studied and useful for
building practical security schemes. A file opened in your text editor is unlikely
to be able to steal your email, unless a really unfortunate conjunction of
implementation flaws subverts all these layers of separation at once.
In the browser world, this separation is virtually nonexistent: Documents
and code live as parts of the same intermingled blobs of HTML, isolation
between completely unrelated applications is partial at best (with all sites
nominally sharing a global JavaScript environment), and many types of inter-
action between sites are implicitly permitted with few, if any, flexible, browser-
level security arbitration frameworks.
In a sense, the model is reminiscent of CP/M, DOS, and other principally
nonmultitasking operating systems with no robust memory protection, CPU
preemption, or multiuser features. The obvious difference is that few users
depended on these early operating systems to simultaneously run multiple
untrusted, attacker-supplied applications, so there was no particular reason
for alarm.
In the end, the seemingly unlikely scenario of a text file stealing your
email is, in fact, a frustratingly common pattern on the Web. Virtually all web
applications must heavily compensate for unsolicited, malicious cross-domain
access and take cumbersome steps to maintain at least some separation of
code and the displayed data. And sooner or later, virtually all web applications
fail. Content-related security issues, such as cross-site scripting or cross-site
request forgery, are extremely common and have very few counterparts in
dedicated, compartmentalized client architectures.
Nonconvergence of Visions
Fortunately, the browser security landscape is not entirely hopeless, and
despite limited separation between web applications, several selective secu-
rity mechanisms offer rudimentary protection against the most obvious attacks.
But this brings us to another characteristic that makes the Web such an inter-
esting subject: There is no shared, holistic security model to grasp and live by.
We are not looking for a grand vision for world peace, mind you, but simply
a common set of flexible paradigms that would apply to most, if not all, of the
Security in the World of Web Applications 15
relevant security logic. In the Unix world, for example, the rwx user/group per-
mission model is one such strong unifying theme. But in the browser realm?
In the browser realm, a mechanism called same-origin policy could be
considered a candidate for a core security paradigm, but only until one real-
izes that it governs a woefully small subset of cross-domain interactions. That
detail aside, even within its scope, it has no fewer than seven distinct varieties,
each of which places security boundaries between applications in a slightly
different place.* Several dozen additional mechanisms, with no relation to
the same-origin model, control other key aspects of browser behavior (essen-
tially implementing what each author considered to be the best approach to
security controls that day).
As it turns out, hundreds of small, clever hacks do not necessarily add up
to a competent security opus. The unusual lack of integrity makes it very dif-
ficult even to decide where a single application ends and a different one
begins. Given this reality, how does one assess attack surfaces, grant or take
away permissions, or accomplish just about any other security-minded task?
Too often, “by keeping your fingers crossed” is the best response we can give.
Curiously, many well-intentioned attempts to improve security by
defining new security controls only make the problem worse. Many of these
schemes create new security boundaries that, for the sake of elegance, do not
perfectly align with the hairy juxtaposition of the existing ones. When the
new controls are finer grained, they are likely to be rendered ineffective by
the legacy mechanisms, offering a false sense of security; when they are more
coarse grained, they may eliminate some of the subtle assurances that the
Web depends on right now. (Adam Barth and Collin Jackson explore the
topic of destructive interference between browser security policies in their
academic work.)6
Cross-Browser Interactions: Synergy in Failure
The overall susceptibility of an ecosystem composed of several different soft-
ware products could be expected to be equal to a simple sum of the flaws
contributed by each of the applications. In some cases, the resulting expo-
sure may be less (diversity improves resilience), but one would not expect it
to be more.
The Web is once again an exception to the rule. The security community
has discovered a substantial number of issues that cannot be attributed to any
particular piece of code but that emerge as a real threat when various brows-
ers try to interact with each other. No particular product can be easily singled
out for blame: They are all doing their thing, and the only problem is that no
one has bothered to define a common etiquette for all of them to obey.