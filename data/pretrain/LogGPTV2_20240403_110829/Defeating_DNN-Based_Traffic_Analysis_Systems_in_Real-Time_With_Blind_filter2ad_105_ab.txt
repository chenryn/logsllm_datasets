Adversary’s knowledge of the model. We start with an
adversary who has a white-box access to the target model to be
defeated, i.e., he knows the target DNN model’s architecture
and parameters (Section 4). Then, in Section 9 we extend
our attack to a blackbox setting where the adversary has no
knowledge of the target model’s architecture or parameters,
by leveraging the transferability of our technique.
Adversary’s knowledge of the training data. We assume
the adversary knows a set of samples from the same distribu-
tion as the training dataset of the target model. For example,
in the website ﬁngerprinting application the adversary can
USENIX Association
30th USENIX Security Symposium    2707
TimeCurrent TimeFuture (unknown) packetsA blind adversary does notneed to know these in advanceA non-blind adversary needs to know all these patternsA packetbrowse the target websites to be misclassiﬁed to obtain such
training samples.
Attack’s target. We consider four types of attacks, i.e., ST-
DT, ST-DU, SU-DT, and SU-DU, based on the adversary’s source
and destination targets as deﬁned below:
a) Destination-targeted/untargeted (DT/DU): We call the
attack destination-targeted (DT) if the goal of the adversary is
to make the model misclassify arbitrary inputs into a speciﬁc,
target output class. On the other hand, we call the attack
destination-untargeted (DU) if the goal is to misclassify inputs
into arbitrary (incorrect) output classes.
b) Source-targeted/untargeted (ST/SU): A source-targeted
(ST) adversary is one whose goal is to have inputs from a
speciﬁc input class misclassiﬁed by the trafﬁc analysis model.
By contrast, a source-untargeted (SU) adversary is one who
aims at causing arbitrary inputs classes to get misclassiﬁed.
Therefore, a deep neural network f is trained by tuning its
weight parameters to minimize its empirical loss function
over a (large) set of known input-output pairs (x,y). This is
commonly performed using a variation of the gradient descent
algorithm, e.g., back propagation [16].
3.2 Adversarial Examples
An adversarial example is an adversarially crafted input that
fools a target classiﬁer or regression model into making in-
correct classiﬁcations or predictions. The adversary’s goal is
to generate adversarial examples by adding minimal pertur-
bations to the input data attributes. Therefore, an adversarial
example xxx∗ can be crafted by solving the following optimiza-
tion problem:
xxx∗ = xxx + argmin{zzz : O(xxx + zzz) (cid:54)= O(xxx)} = xxx + δδδx
(3)
3 Background
n ( f wn−1
3.1 Deep Learning
A deep neural network consists of a series of linear and non-
linear functions, known as layers. Each layer has a weight
matrix wi and an activation function. For a given input xxx, we
denote the output of a DNN model by:
n−1 (··· f w1
f (xxx) = f wn
is the i−th layer of the deep neural network (note
where f wi
i
that we use bold letters to represent vectors as in xxx). We focus
on supervised learning, where we have a set of labeled training
data. Let X be a set of data points in the target d-dimensional
space, where each dimension represents one attribute of the
input data points. We assume there is an oracle O which maps
the data points to their labels. For the sake of simplicity, we
only focus on classiﬁcation tasks.
1 (x1xxx)))··· )
The goal of training is to ﬁnd a classiﬁcation model f that
maps each point in X to its correct class in the set of classes, Y .
To obtain f , one needs to deﬁne a lower-bounded, real-valued
loss function l( f (xxx),O(xxx)) that for each data point xxx measures
the difference between O(xxx) and the model’s prediction f (xxx).
Therefore, the loss function for f can be deﬁned as:
L( f ) =
E
(xxx,y)∼Pr(X,Y )
[l( f (xxx),y)]
(1)
and the objective of training is to ﬁnd an f that minimizes
this loss function. Since Pr(X,Y ) is not entirely available
to the training entities, in practice, a set of samples from it,
called the training set Dtrain ⊂ X × Y , is used to train the
model [56]. Therefore, instead of minimizing (1), machine
learning algorithms minimize the expected empirical loss of
the model over its training set D:
LDtrain ( f ) =
1
|Dtrain| ∑
(xxx,y)∈Dtrain
l( f (xxx),y)
(2)
where x is a non-adversarial input sample,
δδδx is the adversarial perturbation added to it, and O(·) rep-
resents the true label of its input, as deﬁned in the previous
section. The adversary’s objective is to add a minimal pertur-
bation δδδx to force the target model to misclassify the input xxx.
Adversarial examples are commonly studied in image classi-
ﬁcation applications, where a constraint in ﬁnding adversarial
examples is that the added noise should be imperceptible to
the human eyes.
In this paper, we will investigate the application of ad-
versarial examples on network connections with different
imperceptibility constraints.
Previous works [14, 18, 32, 35] have suggested several
ways to generate adversarial examples. The Fast Gradient
Sign Method (FGSM) [18] algorithm generates an adversarial
sample by calculating the following perturbation for a given
model f and a loss function l:
δδδx = ε× Sign(∇xxxl( f (xxx),y))
(4)
where ∇xxxl( f (xxx),y) is the model’s loss gradient w.r.t. the input
xxx, and the y is the input’s label. Therefore, the adversarial per-
turbation is the sign of the model’s loss gradient w.r.t. the input
xxx and label y. Also, ε is a coefﬁcient controlling the amplitude
of the perturbation. Therefore, the adversarial perturbation in
FGSM is the sign of model’s gradient. The adversary adds
the perturbation to x to craft an adversarial example. Kurakin
et al. [32] proposed a targeted version of FGSM, where the
adversary’s goal is to fool the model to classify inputs as a de-
sired target class (as opposed to any class in FGSM). Kurakin
et al. also introduced an iterative method to improve the suc-
cess rate of the generated examples. Dong et al. [14] showed
that using the momentum approach can improve Kurkain et
al.’s iterative method. Also, Carlini and Wagner [9] designed
a set of attacks that can craft adversarial examples when the
adversary has various norm constraints (e.g., l0, l1, l∞). Other
variations of adversarial examples [15, 52] have been intro-
duced to craft adversarial examples that consider different
2708    30th USENIX Security Symposium
USENIX Association
sets of constraints or improve the adversary’s success rate.
Moosavi-Dezfooli et al. [35] introduced universal adversar-
ial perturbations where the adversary generates adversarial
examples that are independent of the inputs.
3.3 Trafﬁc Analysis Techniques
We overview the two major classes of trafﬁc analysis tech-
niques, which we will use to demonstrate our network adver-
sarial perturbations.
Flow correlation: Flow correlation aims at linking obfus-
cated network ﬂows by correlating their trafﬁc characteristics,
i.e., packet timings and sizes [2, 23, 37, 38]. In particular, the
Tor anonymity system has been the target of ﬂow correlation
attacks, where an adversary aims at linking ingress and egress
segments of a Tor connection by correlating trafﬁc charac-
teristics. Traditional ﬂow correlation techniques mainly use
standard statistical correlation metrics to correlate the vectors
of ﬂow timings and sizes across ﬂows, in particular mutual
information [12, 64], Pearson correlation [33, 49], cosine sim-
ilarity [24, 38], and Spearman correlation [53]. More recently,
Nasr et al. [37] design a DNN-based approach for ﬂow correla-
tion, called DeepCorr. They show that DeepCorr outperforms
statistical ﬂow correlation techniques by large margins.
Website Fingerprinting: Website ﬁngerprinting (WF) aims
at detecting the websites visited over encrypted channels like
VPNs, Tor, and other proxies [3, 6, 19, 27, 40, 41, 47, 50, 51,
57–60]. The attack is performed by a passive adversary who
monitors the victim’s encrypted network trafﬁc, e.g., a mali-
cious ISP or a surveillance agency. The adversary compares
the victim’s observed trafﬁc ﬂow against a set of prerecorded
webpage traces, to identify the webpage being browsed. Web-
site ﬁngerprinting differs from ﬂow correlation in that the
adversary only observes one end of the connection, e.g., the
connection between a client and a Tor relay. Website ﬁnger-
printing has been widely studied in the context of Tor trafﬁc
analysis [3, 6, 19, 27, 40, 41, 47, 50, 51, 57, 59].
Various machine learning classiﬁers have been used for
WF, e.g., using KNN [58], SVM [40], and random forest [19].
However, the state-of-the-art WF algorithms use Convolu-
tional Neural Networks to perform website ﬁngerprinting, i.e.,
Sirinam et al. [50], Rimmer et al. [47], and Bhat et al. [3].
Defenses: Note that our blind adversarial perturbations tech-
nique serves as a defense mechanism against trafﬁc analysis
classiﬁers (as it aims at fooling the underlying classiﬁers).
The literature has proposed other defenses against website
ﬁngerprinting and ﬂow correlation attacks [5,11,28,61]. Sim-
ilar to our work, such defenses work by manipulating trafﬁc
features, i.e., packet timings, sizes, and directions.
In Section 7.5, we compare the performance of our blind
adversarial perturbations with state-of-the-art defenses, show-
ing that our technique outperforms all of these techniques in
defeating trafﬁc analysis.
Also, note that some recent works have considered using
adversarial perturbations as a defense against trafﬁc analysis.
In particular, Mockingbird [26] generates adversarial pertur-
bations to defeat website ﬁngerprinting, and Zhang et al. [62]
apply adversarial examples to defeat video classiﬁcation using
trafﬁc analysis. However, both of these works are non-blind,
i.e., the adversary needs to know the patterns of the target
ﬂows in advance; therefore, we consider them to be unusable
in typical trafﬁc analysis scenarios. By contrast, our blind
perturbation technique modiﬁes live network connections.
4 Blind Adversarial Perturbations
In this section, we present the key formulation and algorithms
for generating blind adversarial perturbations.
4.1 The General Formulation
We formulate the blind adversarial perturbations problem as
the following optimization problem:
∀xxx ∈ DS : f (xxx + δδδ) (cid:54)= f (xxx)
(5)
argmin
δδδ
where the objective is to ﬁnd a (blind) perturbation vector, δδδ,
such that when added to an arbitrary input from a target input
domain DS, it will cause the underlying DNN model f (.) to
misclassify. In a source-targeted (ST) attack (see deﬁnitions
in Section 2.3), DS contains inputs from a target class to be
misclassiﬁed, whereas in a source-untargeted (SU) attack DS
will be a large set of inputs from different classes.
Note that one cannot ﬁnd a closed-form solution for this
optimization problem since the target model f (.) is a non-
convex ML model, i.e., a deep neural network. Therefore, (5)
can be formulated as follows to numerically solve the problem
using empirical approximation techniques:
argmax
δδδ ∑
xxx∈DS
l( f (xxx + δδδ), f (xxx))
(6)
where l is the target model’s loss function and DS ⊂ DS is the
adversary’s network training dataset.
Note that prior work by Moosavi-Dezfooli et al. [35] has
studied the generation of universal adversarial perturbations
for image recognition applications. We, however, take a dif-
ferent direction in generating blind perturbations: in contrast
to ﬁnding a perturbation vector δδδ that maximizes the loss
function in [35], we aim to ﬁnd a perturbation generator
model G. This generator model G will generate adversarial
perturbation vectors when provided with a random trigger
parameter z (we denote the corresponding adversarial pertur-
bation as δδδzzz = G(z)), i.e., we are able to generate different
perturbations on different random z’s. Therefore, the goal of
our optimization problem is to optimize the parameters of the
perturbation generator model G (as opposed to optimizing a
USENIX Association
30th USENIX Security Symposium    2709
perturbation vector δδδ in [35]). Using a generator model in-
creases the attack performance, as shown previously [1, 20]
and validated through our experiments. Hence, we formulate
our optimization problem as:
argmax
G
z∼uni f orm(0,1)
E
[ ∑
xxx∈DS
l( f (xxx + G(z)), f (xxx))]
(7)
We can use existing optimization techniques (we have used
Adam [29]) to solve this problem. In each iteration of training,
our algorithm selects a batch from the training dataset and a
random trigger z, then computes the objective function.
Incorporating Trafﬁc Constraints
4.2
Studies of adversarial examples for image recognition ap-
plications [14, 18, 32, 35] simply modify image pixel values
individually. However, applying adversarial perturbations on
network trafﬁc is much more challenging due to the various
constraints of network trafﬁc that should be preserved while
applying the perturbations. In particular, inter-packet delays
should have non-negative values; the target network protocol
may need to follow speciﬁc packet size/timing distributions;
packets should not be removed from a connection; and, packet
numbers should get adjusted after injecting new packets.
One can add other network constraints depending on the
underlying network protocol. We use remapping and regular-
ization functions to enforce these domain constraints while
creating blind adversarial perturbations. A remapping func-
tion adjusts the perturbed trafﬁc patterns so they comply with
some domain constraints. For example, when an adversary
adds a packet to a trafﬁc ﬂow at position i, the remapping
function should shift the indices of all consecutive packets.
We therefore reformulate our optimization problem by in-
cluding the remapping function M :
argmax
G
z∼uni f orm(0,1)
E
[ ∑
xxx∈DS
l( f (M (xxx,G(z))), f (xxx))]
(8)
Moreover, we add a regularization term to the loss function
so that the adversary can enforce additional constraints, as
will be discussed. Therefore, the following is our complete
optimization problem:
argmax
G
z∼uni f orm(0,1)
E