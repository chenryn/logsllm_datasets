validation trace using the procedure outlined below. Starting with the ﬁrst state,
ﬁlled in with null events, apply the above trace inspect procedure; however,
instead of creating a state and its transitions, update X and Y , evaluation
coeﬃcients, as follows. First, using the trace, determine the next state. Then,
there are two possible cases:
Model compliance: states current, e, and next, e(cid:3)
, together with the associ-
ated transition, are in the model, and so:
X = X + 1
Y = Y + (1 − Pr(e, e(cid:3)
))
where Pr(e, e(cid:3)
Model failure: e and e(cid:3)
) is the probability, according to the model, that e(cid:3)
follows e.
, together with the associated transition, are not in the
model, and so:
X = X + 1
Y = Y + Z
Given a window, w, the classiﬁer outputs a ﬁnal evaluation, μ(w), given by
μ(w) = Y /X. For a given threshold, r, w is said to be normal, whenever μ(w) <
r, and masquerade, otherwise.
4.3 Na¨ıve Bayes
Implementing a Na¨ıve Bayes classiﬁer for a particular user, u, (see, e.g. [16,17]),
amounts to estimating the probability for an event (an object or a task access, in
our case) c to have been originated from u, denoted Pru(c). Since Na¨ıve Bayes is
frequency-based, the associated probability distribution is computed out of the
access information recorded in the training set. Thus, in symbols:
Pru(c) =
fu c+α
nu+α×K
Where fuc is the number of times user u has accessed task (respectively, object)
c, nu the length of u’s training set, and where K is the total number of distinct
tasks (respectively, objects). 0 < α (cid:2) 1 to prevent Pru(c) from becoming zero;
following [16,17], we set α to 0.01.
To evaluate a test window w, in which user u has allegedly participated, the
cumulative probability of w, an access sequence of the form c1c2 . . . cn, of length
n(= 20), is given by:
Pru(w ≡ c1c2 . . . cn) = Pru(c1) × ··· × Pru(cn)
Pru(w) is then compared against a threshold: if it is above the threshold, the
session is considered normal; otherwise, it is considered a masquerade.
Having explained our methodology, and how we have set each classiﬁer pa-
rameter, we now turn our attention to show and analyze the results obtained
throughout our experimentation.
Towards a Masquerade Detection System Based on User’s Tasks
459
Fig. 2. An example ROC curve, annotated with the position of zero-FN, zero-FP, and
MMP
5 Results
5.1 A Comparison of Classiﬁcation Performance
We have used ROC curves, to understand the classiﬁcation performance of all
our MDS’s. In order to compare these MDS’s one another, we have used four dif-
ferent measurements: Area-Under-the-Curve (AUC), Zero-False Negative (Zero-
FN), Zero-False Positive (Zero-FP), and the Minimum Misclassiﬁcation Point
(MMP). AUC denotes the area under a ROC curve. An AUC equal to one amounts
to the perfect classiﬁer, which correctly marks every window, as user or attack.
Conversely, an AUC equal to zero corresponds to the worst classiﬁer ever.
Zero-FN is the least False Positive rate (FP) at which we still work with a true
positive rate of one, and, thus, masquerade windows are all classiﬁed correctly.
We have borrowed zero-FN from [22]. By contrast, zero-FP is the least False
Negative rate (FN) at which we still keep the false positive rate at zero, and,
thus, user windows are all classiﬁed correctly. MMP corresponds to those values
of FP and FN that minimize FP+FN. Fig. 2 depicts the zero-FN, zero-FP and
MMP for a given ROC curve.
Tables 4 and 5 respectively show the overall performance evaluation of Na¨ıve
Bayes and Markov chains. Table 4(a) (respectively, Table 5(a)) shows the classiﬁ-
cation performance of Na¨ıve Bayes (respectively, Markov chain) applied to object
access. This applies similarly for Tables 4(b) and 5(b), but for task access.
Looking into Table 4, we may observe that the task-based Na¨ıve Bayes clas-
siﬁer outperforms the object-based one. While the gain for AUC is marginal,
460
J.B. Cami˜na, J.Rodr´ıguez, and R. Monroy
that for zero-FN, zero-FP and MMP is greater than ﬁve percentage points. Re-
call that for the latter variables, the lower the measure, the better the classiﬁer’s
performance.
Table 4. Average classiﬁcation performance of Na¨ıve Bayes in terms of AUC, zero-FN,
zero-FP, and MMP
User’s log division
Construction% - Validation%
80 - 20
70 - 30
50 - 50
30 - 70
20 - 80
80 - 20 Cross Validation
Average
AUC Zero-FN (FP%) Zero-FP (FN%)
0.716
0.710
0.696
0.710
0.701
0.734
0.711
92.070
93.043
91.760
92.003
91.260
91.934
92.012
93.425
95.562
99.582
99.343
99.602
99.498
97.835
(a) Na¨ıve Bayes applied to object access
User’s log division
Construction% - Validation%
80 - 20
70 - 30
50 - 50
30 - 70
20 - 80
80 - 20 Cross Validation
Average
AUC Zero-FN (FP%) Zero-FP (FN%)
0.758
0.758
0.741
0.736
0.719
0.763
0.746
86.512
86.728
86.553
89.819
88.713
87.486
87.635
79.093
80.354
88.633
91.221
91.253
94.238
87.465
MMP
FP% FN%
22.149 26.229
26.192 25.256
30.115 22.610
25.869 27.560
32.424 23.193
23.366 26.198
26.686 25.174
MMP
FP% FN%
15.618 27.734
14.356 30.738
16.791 29.762
17.267 29.723
19.884 29.174
20.237 26.489
17.359 28.937
(b) Na¨ıve Bayes applied to task access
Similar remarks apply to the results reported in Table 5, except that the per-
formance diﬀerence is not as drastic as for Na¨ıve Bayes. For example, for the
80 - 20 (%) experiment, the task-based Markov chain classiﬁer slightly outper-
forms the object-based one, except in the AUC measurement. However, as we
shorten the amount of available training, the task-based classiﬁer shows a more
regular, stable performance behavior. Comparing the information reported in
Tables 4 and 5, we may notice that, for our problem and regardless of whether
task-based or object-based, Markov chains outperforms Na¨ıve Bayes in all our
measurements.
Summarizing, our experiments have yielded three key observations. First,
a task-based approach to masquerade detection outperforms, though slightly,
an object-based one. Second, Markov chain masquerade detection outperforms
Na¨ıve Bayes’s for this scenario. This might be explained by that a Markov chain
approach accounts for temporal relationships between accesses, giving more in-
formation to construct the user’s proﬁle. Third, and contrary to our expecta-
tions, shortening the amount of available training data does not severely aﬀect
the task-based approach to masquerade detection. This is in contrast with the
object-based approach, where AUC, zero-FN, zero-FP, and MMP average val-
ues suﬀer notorious increments when we used a less percentage of a user log for
training.
Towards a Masquerade Detection System Based on User’s Tasks
461
Table 5. Average classiﬁcation performance of Markov chains, based on area-under-
the-curve (AUC), zero-False Negative (zero-FN), zero-False Positive (zero-FP), and
Minimum Misclassiﬁcation Point (MMP)
User’s log division
Construction% - Validation%
80 - 20
70 - 30
50 - 50
30 -70
20 - 80
80 - 20 Cross Validation
Average
AUC Zero-FN (FP%) Zero-FP (FN%)
0.838
0.829
0.837
0.823
0.849
0.896
0.845
53.260
58.304
65.911
65.386
67.809
55.484
61.026
33.922
42.888
42.910
40.658
40.005
41.165
40.258
(a) Markov chain applied using object access
User’s log division
Construction% - Validation%
80 - 20
70 - 30
50 - 50
30 -70
20 - 80
80 - 20 Cross Validation
Average
AUC Zero-FN (FP%) Zero-FP (FN%)
0.856
0.832
0.814
0.803
0.831
0.874
0.835
43.844
45.508
52.559
53.690
51.318
44.460
48.563
33.655
46.728
53.331
52.520
57.233
53.616
49.514
MMP
FP% FN%
13.159 13.017
16.536 12.383
15.032 14.437
16.927 14.095
16.054 11.355
14.595 8.285
15.384 12.262
MMP
FP% FN%
12.724 8.730
16.410 8.762
21.821 7.205
17.268 13.215
17.387 10.173
11.814 8.492
16.237 9.429