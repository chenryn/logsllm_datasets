title:What Twitter Knows: Characterizing Ad Targeting Practices, User Perceptions,
and Ad Explanations Through Users' Own Twitter Data
author:Miranda Wei and
Madison Stamos and
Sophie Veys and
Nathan Reitinger and
Justin Goodman and
Margot Herman and
Dorota Filipczuk and
Ben Weinshel and
Michelle L. Mazurek and
Blase Ur
What Twitter Knows: Characterizing Ad Targeting 
Practices, User Perceptions, and Ad Explanations 
Through Users’ Own Twitter Data
Miranda Wei, University of Washington / University of Chicago; Madison Stamos 
and Sophie Veys, University of Chicago; Nathan Reitinger and Justin Goodman, 
University of Maryland; Margot Herman, University of Chicago; Dorota Filipczuk, 
University of Southampton; Ben Weinshel, University of Chicago; Michelle L. 
Mazurek, University of Maryland; Blase Ur, University of Chicago
https://www.usenix.org/conference/usenixsecurity20/presentation/wei
This paper is included in the Proceedings of the 
29th USENIX Security Symposium.
August 12–14, 2020
978-1-939133-17-5
Open access to the Proceedings of the 
29th USENIX Security Symposium 
is sponsored by USENIX.
What Twitter Knows: Characterizing Ad Targeting Practices, User Perceptions,
and Ad Explanations Through Users’ Own Twitter Data
Miranda Wei(cid:52), Madison Stamos, Sophie Veys, Nathan Reitinger(cid:63), Justin Goodman(cid:63),
Margot Herman, Dorota Filipczuk†, Ben Weinshel, Michelle L. Mazurek(cid:63), Blase Ur
University of Chicago, (cid:63)University of Maryland, †University of Southampton,
(cid:52)University of Chicago and University of Washington
Abstract
Although targeted advertising has drawn signiﬁcant attention
from privacy researchers, many critical empirical questions
remain. In particular, only a few of the dozens of targeting
mechanisms used by major advertising platforms are well
understood, and studies examining users’ perceptions of ad
targeting often rely on hypothetical situations. Further, it is
unclear how well existing transparency mechanisms, from
data-access rights to ad explanations, actually serve the users
they are intended for. To develop a deeper understanding of
the current targeting advertising ecosystem, this paper en-
gages 231 participants’ own Twitter data, containing ads they
were shown and the associated targeting criteria, for measure-
ment and user study. We ﬁnd many targeting mechanisms
ignored by prior work — including advertiser-uploaded lists
of speciﬁc users, lookalike audiences, and retargeting cam-
paigns — are widely used on Twitter. Crucially, participants
found these understudied practices among the most privacy
invasive. Participants also found ad explanations designed
for this study more useful, more comprehensible, and overall
more preferable than Twitter’s current ad explanations. Our
ﬁndings underscore the beneﬁts of data access, characterize
unstudied facets of targeted advertising, and identify potential
directions for improving transparency in targeted advertising.
1 Introduction
Social media companies derive a signiﬁcant fraction of their
revenue from advertising. This advertising is typically highly
targeted, drawing on data the company has collected about
the user, either directly or indirectly. Prior work suggests that
while users may ﬁnd well-targeted ads useful, they also ﬁnd
them “creepy” [40, 42, 58, 61, 70]. Further, users sometimes
ﬁnd targeted ads potentially embarrassing [3], and they may
(justiﬁably) fear discrimination [4, 15, 21, 47, 57, 59, 60, 73].
In addition, there are questions about the accuracy of cate-
gorizations assigned to users [7, 12, 21, 71, 72]. Above all,
users currently have a limited understanding of the scope and
mechanics of targeted advertising [17, 22, 48, 50, 54, 70, 77].
Many researchers have studied targeted advertising, largely
focusing on coarse demographic or interest-based target-
ing. However, advertising platforms like Twitter [63] and
Google [27] oﬀer dozens of targeting mechanisms that are
far more precise and leverage data provided by users (e.g.,
Twitter accounts followed), data inferred by the platform (e.g.,
potential future purchases), and data provided by advertisers
(e.g., PII-indexed lists of current customers). Further, because
the detailed contents and provenance of information in users’
advertising proﬁles are rarely available, prior work focuses
heavily on abstract opinions about hypothetical scenarios.
We leverage data subjects’ right of access to data collected
about them (recently strengthened by laws like GDPR and
CCPA) to take a more comprehensive and ecologically valid
look at targeted advertising. Upon request, Twitter will pro-
vide a user with highly granular data about their account,
including all ads displayed to the user in the last 90 days
alongside the criteria advertisers used to target those ads, all
interests associated with that account, and all advertisers who
targeted ads to that account.
In this work, we ask: What are the discrete targeting mech-
anisms oﬀered to advertisers on Twitter, and how are they
used to target Twitter users? What do Twitter users think of
these practices and existing transparency mechanisms? A to-
tal of 231 Twitter users downloaded their advertising-related
data from Twitter, shared it with us, and completed an on-
line user study incorporating this data. Through this method,
we analyzed Twitter’s targeting ecosystem, measured partici-
pants’ reactions to diﬀerent types of ad targeting, and ran a
survey-based experiment on potential ad explanations.
We make three main contributions. First, we used our
231 participants’ ﬁles to characterize the current Twitter ad-
targeting ecosystem. Participants received ads targeted based
on 30 diﬀerent targeting types, or classes of attributes through
which advertisers can select an ad’s recipients. These types
ranged from those commonly discussed in the literature (e.g.,
interests, age, gender) to others that have received far less
attention (e.g., audience lookalikes, advertiser-uploaded lists
of speciﬁc users, and retargeting campaigns). Some partic-
USENIX Association
29th USENIX Security Symposium    145
ipants’ ﬁles contained over 4,000 distinct keywords, 1,000
follower lookalikes, and 200 behaviors. Participants’ ﬁles also
revealed they had been targeted ads in ways that might be
seen as violating Twitter’s policies restricting use of sensi-
tive attributes. Participants were targeted using advertiser-
provided lists of users with advertiser-provided names con-
taining “DLX_Nissan_AfricanAmericans,” “Christian Audi-
ence to Exclude,” “Rising Hispanics | Email Openers,” and
more. They were targeted using keywords like “#transgender”
and “mexican american,” as well as conversation topics like
the names of UK political parties. These ﬁndings underscore
how data access rights facilitate transparency about targeting,
as well as the value of such transparency.
Second, we investigated participants’ perceptions of the
fairness, accuracy, and desirability of 16 commonly observed
targeting types. Diﬀerent from past work using hypothetical
situations, we asked participants about speciﬁc examples that
had actually been used to target ads to them in the past 90
days. Whereas much of the literature highlights users’ nega-
tive perceptions of interest-based targeting [42, 61], we found
that over two-thirds of participants agreed targeting based on
interest was fair, the third most of the 16 types. In contrast,
fewer than half of participants agreed that it was fair to target
using understudied types like follower lookalike targeting,
tailored audience lists, events, and behaviors. Many target-
ing types ignored by prior work were the ones viewed least
favorably by participants, emphasizing the importance of ex-
panding the literature’s treatment of ad-targeting mechanisms.
Third, we probe a fuller design space of speciﬁcity, read-
ability, and comprehensiveness for ad explanations. Although
ad explanations are often touted as a key part of privacy trans-
parency [24], we ﬁnd that existing ad explanations are incom-
plete and participants desire greater detail about how ads were
targeted to them. Compared to Twitter’s current explanation,
participants rated explanations we created to be signiﬁcantly
more useful, helpful in understanding targeting, and similar
to what they wanted in future explanations.
Our approach provides a far richer understanding of the
Twitter ad ecosystem, users’ perceptions of ad targeting, and
ad explanation design than was previously available. Our re-
sults emphasize the beneﬁts of advertising transparency in
surfacing potential harms associated with increasingly accu-
rate and complex inferences. Our ﬁndings also underscore
the need for a more ethical approach to ad targeting that can
maintain the trust of users whose data is collected and used.
2 Related Work
We review prior work on techniques for targeted advertising,
associated transparency mechanisms, and user perceptions.
2.1 Targeted Advertising Techniques
Web tracking dates back to 1996 [38]. The online ad ecosys-
tem has only become more sophisticated and complex since.
Companies like Google, Facebook, Bluekai, and many others
track users’ browsing activity across the Internet, creating
proﬁles for the purpose of sending users targeted advertis-
ing. Commercial web pages contain an increasing number of
trackers [52], and much more data is being aggregated about
users [13]. Many studies have examined tools to block track-
ing and targeted ads, ﬁnding that tracking companies can still
observe some of a user’s online activities [2, 10, 11, 19, 30].
Social media platforms have rich data for developing exten-
sive user proﬁles [7, 12, 57], augmenting website visits with
user-provided personal information and interactions with plat-
form content [7]. This data has included sensitive categories
like ‘ethnic aﬃnity’ [8] and wealth. Even seemingly neutral
attributes can be used to target marginalized groups [57].
To date, studies about user perceptions of ad-targeting
mechanisms have primarily focused on proﬁles of users’
demographics and inferred interests (e.g., yoga, travel) re-
gardless of whether the studies were conducted using users’
own ad-interest proﬁles [12, 20, 50] or hypothetical scenar-
ios [17, 36]. Furthermore, most studies about advertising on
social media have focused on Facebook [7, 25, 57, 71]. While
some recent papers have begun to examine a few of the dozens
of other targeting mechanisms available [7, 72], our study
leverages data access requests to characterize the broad set of
targeting types in the Twitter ecosystem much more compre-
hensively than prior work in terms of both the mechanisms
considered and the depth of a given user’s data examined.
Newer techniques for targeting ads go beyond collecting
user data in several ways that may be less familiar to both
users and researchers. For example, since 2013, Facebook [23]
and Twitter [9] have oﬀered “custom” or “tailored” audience
targeting, which combine online user data with oﬄine data.
Advertisers upload users’ personally identiﬁable information
(PII), such as their phone numbers and email addresses gath-
ered from previous transactions or interactions, in order to link
to users’ Facebook proﬁles. This oﬄine data can also include
data supplied by data brokers [72], often pitched to advertis-
ers as “partner audiences” [32], or even PII from voter and
criminal records [7]. These features can be exploited by adver-
tisers to target ads to a single person [25], or evade restrictions
about showing ads to people in sensitive groups [57].
Another newer form of targeting is lookalike-audience tar-
geting, which relies on inferences about users relative to other
users. For example, on Facebook, advertisers can reach new
users with similar proﬁles as their existing audience [39]. This
feature can be exploited, as a biased input group will lead to
an output group that contains similar biases [57]. Services are
increasingly implementing lookalike targeting [56]. To our
knowledge, we are the ﬁrst to study user perceptions of these
lesser-known forms of targeting with real-world data.
146    29th USENIX Security Symposium
USENIX Association
2.2 Transparency Mechanisms
Ad and analytics companies increasingly oﬀer transparency
tools [16, 29]. These include ad preference managers [12],
which allow users to see the interest proﬁles that platforms
have created for them, and ad explanations, or descriptions
of why a particular advertiser displayed a particular ad to
a user [7]. Nevertheless, a disparity remains between infor-
mation available to advertisers and information visible to
users [50, 58]. Although researchers have documented adver-
tisers’ use of a multitude of attributes, including sensitive ones,
they rarely appear in user-facing content [7, 15, 50, 74, 75].
Facebook’s ad preferences are vague and incomplete [7], no-
tably leaving out information from data brokers [72].
To shed light on the black box of advertising, researchers
have developed “reverse engineering” tools that can extract
some information about targeting mechanisms, associated ex-
planations, and inferences that have been made. Techniques
include measuring the ads users see [6,7,10,11,15,34,35,75],
purchasing ads in controlled experiments [4,71,72], and scrap-
ing companies’ ad-creation interface [25, 57, 71, 72, 74], ad-
interest proﬁles [7,12,15,16,60,75], and ad explanations [6,7].
Unfortunately, these excellent tools are limited by the diﬃ-
culty of scaling them (as they require making many requests
per user) and by companies continually making changes to
their interfaces, perhaps in part to thwart such tools [43].
2.3 Perceptions of Targeting & Transparency
Users do not understand advertising data collection and target-
ing processes [7,17,20,45,54]. They instead rely on imprecise
mental models [58] or folk models [22,77]. While some users
like more relevant content [40] and understand that ads sup-
port free content on the web [42], many others believe track-
ing browser activity is invasive [42, 53]. Users are concerned
about discrimination [47] or bias [21], inaccurate inferences,
and companies inferring sensitive attributes such as health or
ﬁnancial status [50, 70]. Studies have shown that when users
learn about mechanisms of targeted advertising, their feelings
towards personalization become more negative [53, 58, 61].
To an increasing extent, studies have looked into the design
and wording of transparency tools [5, 37, 74]. Unfortunately,
these tools are meant to provide clarity but can be confus-
ing due to misleading icons [36] or overly complicated lan-
guage [37, 54]. Improving the design of transparency tools
is important because vague ad explanations decrease users’
trust in personalized advertising, while transparency increases
participants’ likelihood to use that service [20] and to appreci-
ate personalization [54, 70]. Users want to know the speciﬁc
reasons for why they saw an ad [17] and want more control
over their information by being able to edit their interest pro-
ﬁles [31, 41]. Users continually express concern about their
privacy [18, 28] but cannot make informed decisions if infor-
mation about how their data is used is not transparent [58].
Ad explanations are a particularly widespread form of trans-
parency [7, 17]. Sadly, prior work has found current explana-
tions incomplete [7,71,72] and companion ad-interest proﬁles
to be both incomplete [15] and inaccurate [12,16]. While stud-
ies have examined existing ad explanations [7, 20, 71, 72] or
engaged in speculative design of new explanations [20], sur-
prisingly little work has sought to quantitatively test improved
explanations. We build on this work by quantitatively compar-
ing social media platforms’ current ad explanations with new
explanations we designed based on prior user research [17,20].
Emphasizing ecological validity, we test these explanations
using ads that had actually been shown to participants while
explaining the true reasons those ads had been targeted to
them, leveraging the participant’s own Twitter data.
3 Method
To examine Twitter ad targeting data, we designed an on-
line survey-based study with two parts. First, participants
followed our instructions to request their data from Twitter.
Upon receipt of this data a few days later, they uploaded the
advertising-relevant subset of this data and completed a survey
that instantly incorporated this data across two sections.
Section 1 of the survey elicited participants’ reactions to dif-
ferent targeting types, such as follower lookalike targeting and
interest targeting. We selected 16 commonly observed target-
ing types, many of which have not previously been explored
in the literature. In Section 2, we conducted a within-subjects
experiment measuring participants’ reactions to six poten-
tial ad explanations, including three novel explanations we
created by building on prior work [17, 20], as well as approx-
imations of Twitter and Facebook’s current ad explanations.
We also asked participants about their general Twitter usage.
We concluded with demographic questions. Our survey was
iteratively developed through cognitive interviews with peo-
ple familiar with privacy research, as well as pilot testing with
people who were not. Below, we detail our method.
3.1 Study Recruitment
We recruited 447 participants from Proliﬁc to request their
Twitter data, paying $0.86 for this step. The median comple-
tion time was 7.3 minutes. We required participants be at least
18 years old, live in the US or UK, and have a 95%+ approval
rating on Proliﬁc. Additionally, participants had to use Twitter
at least monthly and be willing to upload their Twitter ad data