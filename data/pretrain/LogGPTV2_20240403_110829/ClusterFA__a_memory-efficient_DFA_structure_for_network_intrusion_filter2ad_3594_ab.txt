serving the graph in Fig. 1(a), adjacent states share the ma-
jority of the next-hop states associated with the same input
chars. So δFA pluses a temporary state to record the pri-
or state’s transitions, presented by the dashed state T in
Fig. 1(c). In this way, only the diﬀerences between adjacent
states is required to store in the memory. When jumping
from state 1 to state 2, δFA ﬁrst lookup the transitions of
temporary state (now it is state 1), if the character does
not match, it turns into the current state (state 2) to look
up the actual next state. Still take the input string aabdbc
for example, the sequence of states is 1T 2T 2T 34T 35. By
introducing a temporary state, δFA only has 8 edges in the
graph.
However, observing the graph in Fig. 1(c), because a single
state 3 deﬁnes a diﬀerent transition (when input a character
c, the next state is state 5, not state 1 as in other states),
all the transitions for character c are labeled and stored for
c13245accabacbbddacdbcdabd(a)13245abdc(b)13245acbdA(d)3c13245acbdcccc(c)Tthe ﬁve states. This is due to the temporary state of δFA
is merely able to store one parent’s transitions, and can not
”see” further than a single hop. In other words, this tempo-
rary state is a ”local” state, which can be accessed only by
adjacent child states. We improve this situation by replacing
the ”local” temporary state with a ”global” temporary state.
We extract a global temporary state which stores the most
”common” transitions of these ﬁve states. Only the transi-
tions diﬀerent from the global state is stored (we call these
transitions ”unique transitions”), using very little memory.
In this way we eliminate the transitions for c of state 1, 2,
4, 5.
The result of what we have just described is depicted in
Fig. 1(d), which is equivalent to the DFA in Fig. 1(a). We
call this DFA structure ”ClusterFA”, for it clusters the sim-
ilar states together and extracts a global temporary state.
The dashed state A is the global temporary state. The bold
transition from state 3 to state 5 is the transition diﬀerent
from state A. Still take the input string aabdbc for example,
the sequence of ClusterFA is A2A2A3A4A35. In this graph,
we only reserve 5 edges, better than 9 edges in D2FA and 8
edges in δFA.
We ﬁnd something fascinating if examining ClusterFA from
another aspect. In [26] Y. Liu propose that the state tran-
sitions table of DFA can be treated as a matrix. By this
means, we depict the transition table matrix of Fig. 1(a) as
is shown in the left part of Fig. 2. From the matrix, we
ﬁnd that four rows of the matrix (row 1, 2, 4, 5) are exactly
the same.
If we want to compress this matrix, it is quite
natural to extract a common row vector A as in the right
part of Fig. 2. In Fig. 2, the row vector A corresponds to
the global state A in Fig. 1(d). The unique transitions cor-
respond to the remaining matrix, as shown in the right of
Fig. 2. Obviously, this is a sparse matrix with many zeros
in it, and many algorithms have been proposed to compress
sparse matrix.
3.2 Main idea of ClusterFA
In the previous section, we introduce the basic idea of
ClusterFA. We split a DFA into a common state and u-
nique transitions. However, if the DFA becomes large and
complex, it is impossible to ﬁnd a single common state to
compress the DFA. If the transition table matrix of a real-
life rule set is printed out to a piece of paper, we can easily
observe that all the states fall into several groups, and in ev-
ery group, the diﬀerent transitions between states are very
little. This means it is very easy to extract a common state
from each group. In this way, we are able to get multiple
common states, and the unique transitions can be deposit in
Figure 2: Decomposition of DFA transition table
Matrix
a single sparse matrix. To verify this observation, we present
some experiments on rule sets from Bro and Snort, and the
results is shown in Table 1.
In Table 1, we apply CLINK clustering algorithm (to be
mentioned in latter chapters) to group the states. Since all
unique states are stored in a single sparse matrix, we use the
percent of nonzero elements of sparse matrix to measure the
eﬀect of our grouping scheme. From Tab. 1, when the DFA
states is divided into 20-50 groups, the nonzero percent is
less than 1% in average. This means in the sparse matrix,
there are only 2 nonzero elements in one row. This proves
our observation is rational.
The main idea of ClusterFA is to divide the DFA states to
K groups, then extract K common states from each group.
When a new character comes, we ﬁrst determine whether
the pair of {s, c} exists in the sparse matrix or not, where s
presents the current state, and c presents the input charac-
ter. If the pair of {s, c} is in the sparse matrix, we employ
the result as the next state. Else we turn to the common
state corresponding to s to get the next state.
3.3 Description of ClusterFA
In this section, we ﬁrst give some deﬁnitions to formulate
ClusterFA. Then we present the construction algorithm of
ClusterFA. Finally we show how to accomplish one lookup
in a ClusterFA.
3.3.1 Deﬁnitions
The state transition table of a DFA can be considered
as an N ∗ C matrix DFAMatrix, where N is the number
of the states and C is the alphabet size. Each elemen-
t DFAMatrix[s, c] deﬁnes the state switching from state s
to the next state through input character c.
3.3.2 Construction Algorithm
There are two stages to construct a ClusterFA. In the ﬁrst
stage, similar states are classiﬁed to multiple Groups by
the clusterStates function, and an index table IndexT able
is calculated to indicate to which group the DFA state s
belongs. In the second stage, CommonT able consisting of K
common states and a sparse matrix SparseM atrix storing
the unique states is worked out. Algorithm 1 shows how to
create a ClusterFA from an N -state DFA.
In the 1st line, we divide all the DFA states into K group-
s by clustering algorithm, and IndexT able is worked out to
indicate which group the DFA state s belongs to. From line
2 to line 4, we calculate the common states for each group
by getCenter function. Details of this function is shown in
Algorithm 2. From line 5 to line 10, we visit every state of D-
FA, and store the unique transitions to SparseM atrix. For
convenience, we record the diﬀerence between unique tran-
sitions and common state transitions, instead of the values
Table 1: non-zeros percentage of spares matrix
rule set
bro217
snort24
snort34
snort31
number
of states
number
of groups
non-zero
elements of
sparse matrix
6533
8335
8754
4864
40
50
40
20
1.18%
0.75%
0.77%
1.08%
 a b c d 1 2 3 1 4 2 2 3 1 4 3 2 3 5 4 4 2 3 1 4 5 2 3 1 4   a b c d 1 0 0 0 0 2 0 0 0 0 3 0 0 5 0 4 0 0 0 0 5 0 0 0 0  A2314Algorithm 1 : Creation of ClusterFA from an N -state D-
FA, where Groups[i] stands for the ith state group after
clustering states
Input: DFAMatrix, N, C, K
Output: CommonT able, SparseM atrix, IndexT able
1: (IndexT able, Groups) ←
clusterStates(DFAMatrix, K)
CommonT able[i] ← getCenter(Groups[i])
2: for i = 1 to K do
3:
4: end for
5: for s = 1 to N do
6:
7:
8:
k ← IndexT able[s]
for c = 1 to C do
SparseM atrix[s][c] ←
DFAMatrix[s][c] − CommonT able[k]
end for
9:
10: end for
of unique transitions.
In Algorithm 1, the clusterStates function is the most
time-consuming during the construction. Take K-means al-
gorithm for example, if the group number k, objects number
m and vector dimension d are ﬁxed, the algorithm can be
solved in time O(Ikmd), where I is how many iterations K-
means runs [19], and we can consider I as a constant. In
Algorithm 1, d equals to alphabet size C, k equals to the
group number K, and n equals to the states number N .
So the time complexity of Algorithm 1 is O(KN C). Dur-
ing the construction, we need two N ∗ C matrix to deposit
the transitions. So the space complexity of Algorithm 1 is
O(N ∗ C).
Algorithm 2 : Pseudocode for getCenter function
Input: Group, C
Output: CommonState
1: L ← getArrayLength(Group)
2: for c = 1 to C do
3: Array.clear()
4:
5:
6:
7:
end for
CommonState[c] ←
Array.insert(Group[l][c])
for l = 1 to L do
f indM ostF requentElement(Array)
8: end for
Algorithm 2 presents how to ﬁnd the common state from
a state group. The principle of calculating common state is
to guarantee the SparseM atrix as sparse as possible. Thus,
for every c in alphabet, we select the most frequent transition
as a common state transition, as shown from line 2 to line
8.
3.3.3 Lookup Algorithm
Algorithm 3 shows one lookup in a ClusterFA. The cur-
rent state is s and the input character is c. We ﬁrst look up
the IndexT able for the corresponding common state tran-
sition tcommon. Then we determine whether the pair of s, c
exists in the sparse matrix. If the pair of s, c does not ex-
ist in the sparse matrix, we use the tcommon as the next
state snext. Else we access the sparse matrix, and assign
tcommon + SparseM atrix[s][c] to snext.
s, c
Algorithm 3 : Pseudocode for a lookup in a ClusterFA
Input:
Output: snext
1: index ← IndexT able[s]
2: snext ← CommonT able[index][c]
3: if BloomF ilter.test(s, c) = 1 then
4:
5: end if
snext ← snext + SparseM atrix.get(s, c)
To compress the ClusterFA, we need to store the sparse
matrix using compact data structure. The access to the
compact data structure is an important reason leading to the
decrease of matching speed. To avoid unnecessary access to
the spares matrix, we use the bloom ﬁlter technique which is
proposed in [26] to indicate whether a position in the sparse
matrix is empty or not, as shown in line 3.
4. DETAILS OF CLUSTERING
During the construction of ClusterFA, we must divide the
DFA states into multiple groups according to their similari-
ties before calculating the common states. This is a typical
NP-hard partitioning problem. If we consider one state as
an object with 256 features, and the DFA as a set of these
objects, we could use cluster analysis technique to solve this
problem. We consider three clustering algorithms in our
work: K-Means, CLINK and pspectralclustering.
4.1 Distance Measurement
In order for the clustering of the states to occur, a simi-
larity (or distance) measurement must be established ﬁrst.
There exist various similarity (distance) measurements, such
as Euclidean distance, Squared Euclidean distance, Power
distance, and so on. However, these distance measurements
are not suitable to measure the similarity between DFA s-
tates. Instead, we use hamming distance [17] to measure the
similarity between DFA states, as is shown in Algorithm 4.
Algorithm 4 : Distance of two states
Input:
s1, s2, C
Output: d(s1, s2)
1: d(s1, s2) ← 0
2: for i = 1 to C do
3:
4:
5:
end if
6: end for
d(s1, s2) ← d(s1, s2) + 1
if s1[i] (cid:54)= s2[i] then
We must point out that hamming distance conforms to
triangular inequality. Some algorithms often use triangular
inequality to speed up clustering.
The similarity matrix SMatrix is required in the cluster-
ing algorithms, so we show how to calculate the similarity
matrix of a DFA in Algorithm 5. In line 8, the getDistance
function has been presented in Algorithm 4.
4.2 Selection of K
When a new DFA comes, it is hard to determine how many
groups is the best. That is to say, we cannot determine K
in advance. More seriously, the K is directly relevant to the
compression ratio. If K is too big, the common states takes
N (states number)
Table 2: appropriate K for diﬀerent rule sets
rule set
K(group number)
bro217
snort24
snort31
snort34
l7 top7
30
50
20
40
1000
160
23
320
140
230
320
6533
8335
4864
9754
12910
1888
2293
3321
2984
4887
4028
l7 2
l7 3
l7 4
l7 5
l7 6
l7 7
Figure 3: Relationship of K and Compression Ratio
si ← DFAMatrix[i]
for j = 1 to N do
Algorithm 5 : Calculation of similarity matrix SMatrix
Input: DFAMatrix, N