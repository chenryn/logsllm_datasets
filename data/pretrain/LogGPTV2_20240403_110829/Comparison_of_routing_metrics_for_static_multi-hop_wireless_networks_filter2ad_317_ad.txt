as a function of path length under
ETX. The metric does a better job
of selecting multi-hop paths.
Figure 13: All Pairs: Throughput
as a function of path length under
RTT. The metric does a poor job
of selecting even one hop paths.
12000
10000
8000
6000
4000
2000
)
s
p
b
K
(
t
u
p
h
g
u
o
r
h
T
0
0
1
2
3
4
5
6
7
8
Average Path Length
Figure 14: All Pairs: Throughput as a function
of path length under PktPair. The metric ﬁnds
good one-hop paths, but poor multi-hop paths.
using ETX. Again, the ETX metric produces signiﬁcantly
longer paths than the HOP metric. The testbed diameter is
7 hops using ETX and 6 hops using HOP.
We also examined the impact of average path length on
TCP throughput.
In Figure 11 we plot the throughput
of a TCP connection against its path length using HOP
while in Figure 12 we plot the equivalent data for ETX.
First, note that as one would expect,
longer paths pro-
duce lower throughputs because channel contention keeps
more than one link from being active. Second, note that
ETX’s higher median throughput derives more from avoid-
ing lower throughputs than from achieving higher through-
puts. Third, ETX does especially well at longer path lengths.
The ETX plot is ﬂat from around 5 through 7 hops, possibly
indicating that links at opposite ends of the testbed do not
interfere. Fourth, ETX avoids poor one-hop paths whereas
HOP blithely uses them.
We now look at the performance of RTT and PktPair in
more detail. In Figure 13 we plot TCP throughput versus
average path length for RTT while in Figure 14 we plot the
data for PktPair. RTT’s self-interference is clearly evident
in the low throughputs and high number of connections with
average path lengths between 1 and 2 hops. With RTT, even
1-hop paths are not stable. In contrast, with PktPair the
1-hop paths look good (equivalent to ETX in Figure 12) but
self-interference is evident starting at 2 hops.
5.5 Variability of TCP Throughput
To measure the impact of routing metrics on the variabil-
ity of TCP throughput, we carry out the following experi-
ment. We select 6 nodes on the periphery of the testbed, as
shown in Figure 2. Each of the 6 nodes then carried out a
3-minute TCP transfer to the remaining 5 nodes. The TCP
transfers were set suﬃciently apart to ensure that no more
than one TCP transfer will be active at any time. There is
no other traﬃc in the testbed. We repeated this experiment
10 times. Thus, there were a total of 6 × 5 × 10 = 300 TCP
transfers. Since each transfer takes 3 minutes, the experi-
ment lasts for over 15 hours.
In Figure 15 we show the median throughput of the 300
TCP transfers using each metric. As before, the error bars
represent SIQR. Once again, we see that the RTT metric is
the worst performer, and the ETX metric outperforms the
other three metrics by a wide margin.
The median throughput using the ETX metric is 1133Kbps,
while the median throughput using the HOP metric is 807.5.
This represents a gain of 40.3%. This gain is higher than the
23.15% obtained in the previous experiment because these
machines are on the periphery of the network, and thus, the
paths between them tend to be longer. As we have noted in
Section 5.4, the ETX metric tends to perform better than
HOP on longer paths. The higher median path lengths sub-
stantially degrades the performance of RTT and PktPair,
compared to their performance shown in Figure 7.
The HOP metric selects the shortest path between a pair
of nodes. If multiple shortest paths are available, the met-
ric simply chooses the ﬁrst one it ﬁnds. This introduces
a certain amount of randomness in the performance of the
HOP metric. If multiple TCP transfers are carried out be-
tween a given pair of nodes, the HOP metric may select
diﬀerent paths for each transfer. The ETX metric, on the
other hand, selects “good” links. This means that it tends
to choose the same path between a pair of nodes, as long
as the link qualities do not change drastically. Thus, if sev-
eral TCP transfers are carried out between the same pair of
nodes at diﬀerent times, they should yield similar through-
put using ETX, while the throughput under HOP will be
more variable. This fact is illustrated in Figure 16.
The ﬁgure uses coeﬃcient of variation (CoV) as a measure
of variability. CoV is deﬁned as standard deviation divided
by mean. There is one point in the ﬁgure for each of the 30
source-destination pairs. The X-coordinate represents CoV
of the throughput of 10 TCP transfers conducted between
a given source-destination pair using ETX, and the Y co-
ordinate represents the CoV using HOP. The CoV values are
)
s
p
b
K
(
t
u
p
h
g
u
o
r
h
T
1500
1200
900
600
300
0
1
0.8
0.6
0.4
0.2
V
o
C
t
u
p
h
g
u
o
r
h
T
P
O
H
HOP
ETX
RTT PktPair
Metric
0
0
0.2
0.4
0.6
0.8
1
ETX Throughput CoV
Figure 15: 30 Pairs: Median throughput with dif-
ferent metrics.
signiﬁcantly lower with ETX. Note that a single point lies
well below the diagonal line, indicating that HOP provided
more stable throughput than ETX. This point represents
TCP transfers from node 23 to node 10. We are currently
investigating these transfers further. It is interesting to note
that for the reverse transfers, i.e. from node 10 to node 23,
ETX provides lower CoV than than HOP.
5.6 Multiple Simultaneous TCP Transfers
In the experiments described in the previous section, only
one TCP connection was active at any time. This is un-
likely to be the case in a real network. In this section, we
compare the performance of ETX, HOP and PktPair for
multiple simultaneous TCP connections. We do not con-
sider RTT since its performance is poor even with a single
TCP connection.
We use the same set of 6 peripheral nodes shown in Fig-
ure 2. We establish 10 TCP connections between each dis-
tinct pair of nodes. Thus, there are a total of 6×5×10 = 300
possible TCP connections. Each TCP connection lasts for
3 minutes. The order in which the connections are estab-
lished is randomized. The wait time between the start of
two successive connections determines the number of simul-
taneously active connections. For example, if the wait time
between starting consecutive connections is 90 seconds, then
two connections will be active simultaneously. We repeat
the experiment for various numbers of simultaneously ac-
tive connections.
For each experiment we calculate the median throughout
of the 300 connections, and multiply it by the number of
simultaneously active connections. We call this product the
Multiplied Median Throughput (MMT). MMT should in-
crease with the number of simultaneous connections, until
the load becomes too high for the network to carry.
In Figure 17 we plot MMT against the number of simul-
taneous active connections. The ﬁgure shows that the per-
formance of the PktPair metric gets signiﬁcantly worse as
the the number of simultaneous connections increase. This
is because the self-interference problem gets worse with in-
creasing load. In the case of ETX, the MMT increases to
a peak at 5 simultaneous connections. The MMT growth
is signiﬁcantly less than linear because there is not much
parallelism in our testbed (many links interfere with each
other) and the increase that we are seeing is partly because
a single TCP connection does not fully utilize the end-to-
end path. We believe the MMT falls beyond 5 simultaneous
connections due to several factors, including 802.11 MAC
ineﬃciencies and instability in the ETX metric under very
high load. The MMT using HOP deteriorates much faster
Figure 16: 30 Pairs: The lower CoVs under ETX
indicate that ETX chooses stable links.
HOP
ETX
PktPair
)
s
p
b
K
(
t
u
p
h
g
u
o
r
h
T
d
e
z
i
l
a
m
r
o
N
1800
1600
1400
1200
1000
800
600
400
200
0
1
2
3
4
5
6
9
18
Number of simultaneous TCP connections
Figure 17: Throughputs with multiple simulta-
neous TCP connections.
than it does with ETX. As discussed in Section 5.8, at higher
loads HOP performance drops because link-failure detection
becomes less eﬀective.
5.7 Web-like TCP Transfers
Web traﬃc constitutes a signiﬁcant portion of the total
Internet traﬃc today. It is reasonable to assume that web
traﬃc will also be a signiﬁcant portion of traﬃc in wireless
meshes such as the MIT Roofnet [26]. The web traﬃc is
characterized by the heavy-tailed distribution of ﬂow sizes:
most transfers are small, but there are some very large trans-
fers [21]. Thus, it is important to examine the performance
of web traﬃc under various routing metrics.
To conduct this experiment, we set up a web server on host
128. The six peripheral nodes served as web clients. The
web traﬃc was generated using Surge [5]. The Surge soft-
ware has two main parts, a ﬁle generator and a request gen-
erator. The ﬁle generator generate ﬁles of varying sizes that
are placed on the web server. The Surge request generator
models a web user that fetches these ﬁles. The ﬁle generator
and the request generator oﬀer a large number of parameters
to customize ﬁle size distribution and user behaviors. We
ran Surge with its default parameter settings, which have
been derived through modeling of empirical data [5].
Each Surge client modeled a single user running HTTP 1.1
Each user session lasted for 40 minutes, divided in four slots
of 10 minutes each. Each user fetched over 1300 ﬁles from
the web server. The smallest ﬁle fetched was 77 bytes long,
while the largest was 700KB. We chose to have only one
client active at any time, to allow us to study the behavior
of each client in detail. We measure the latency or each
object: the amount of time elapsed between the request for
an object, and the completion of its receipt. Note that we
are ignoring any rendering delays.
)
s
m
(
y
c
n
e
t
a
L
n
a
d
e
M
i
30
25
20
15
10
5
0
HOP
ETX
)
s
m
(
y
c
n
e
t
a
L
n
a
d
e
M
i
HOP
ETX
16
14
12
10
8
6
4
2
0
)
s
m
(
y
c
n
e
t
a
L
n
a
d
e
M
i
HOP
ETX
180
160
140
120