Hadoop主要是用Java语言写的，它无法使用一般Linux预装的OpenJDK，因此在安装Hadoop前要先安装JDK（版本要在1.6以上）；
作为分布式系统，Hadoop需要通过SSH的方式启动处于slave上的程序，因此必须安装和配置SSH。
由此可见，在安装Hadoop前需要安装JDK及SSH。
Hadoop在Mac OS X上的安装与Linux雷同，在Windows系统上的安装与在Linux上有一点不同，就是在Windows系统上需要通过Cygwin模拟Linux环境，而SSH的安装也需要在安装Cygwin时进行选择，请不要忘了这一点。
集群配置只要记住conf/Hadoop-env.sh、conf/core-site.xml、conf/hdfs-site.xml、conf/mapred-site.xml、conf/mapred-queues.xml这5个文件的作用即可，另外Hadoop有些配置是可以在程序中修改的，这部分内容不是本章的重点，因此没有详细说明。
第3章 MapReduce计算模型
本章内容
为什么要用MapReduce
MapReduce计算模型
MapReduce任务的优化
Hadoop流
Hadoop Pipes
本章小结
2004年，Google发表了一篇论文，向全世界的人们介绍了MapReduce。现在已经到处都有人在谈论MapReduce（微软、雅虎等大公司也不例外）。在Google发表论文时，MapReduce的最大成就是重写了Google的索引文件系统。而现在，谁也不知道它还会取得多大的成就。MapReduce被广泛地应用于日志分析、海量数据排序、在海量数据中查找特定模式等场景中。Hadoop根据Google的论文实现了MapReduce这个编程框架，并将源代码完全贡献了出来。本章就是要向大家介绍MapReduce这个流行的编程框架。
3.1 为什么要用MapReduce
MapReduce的流行是有理由的。它非常简单、易于实现且扩展性强。大家可以通过它轻易地编写出同时在多台主机上运行的程序，也可以使用Ruby、Python、PHP和C++等非Java类语言编写Map或Reduce程序，还可以在任何安装Hadoop的集群中运行同样的程序，不论这个集群有多少台主机。MapReduce适合处理海量数据，因为它会被多台主机同时处理，这样通常会有较快的速度。
下面来看一个例子。
引文分析是评价论文好坏的一个非常重要的方面，本例只对其中最简单的一部分，即论文的被引用次数进行了统计。假设有很多篇论文（百万级），且每篇论文的引文形式如下所示：
References
David M.Blei, Andrew Y.Ng, and Michael I.Jordan.
2003.Latent dirichlet allocation.Journal of Machine
Learning Research，3：993-1022.
Samuel Brody and Noemie Elhadad.2010.An unsupervised
aspect-sentiment model for online reviews.In
NAACL'10.
Jaime Carbonell and Jade Goldstein.1998.The use of
mmr, diversity-based reranking for reordering documents
and producing summaries.In SIGIR'98，pages
335-336.
Dennis Chong and James N.Druckman.2010.Identifying
frames in political news.In Erik P.Bucy and
R.Lance Holbert, editors, Sourcebook for Political
Communication Research：Methods, Measures, and
Analytical Techniques.Routledge.
Cindy Chung and James W.Pennebaker.2007.The psychological
function of function words.Social Communication：
Frontiers of Social Psychology, pages 343-
359.
G¨unes Erkan and Dragomir R.Radev.2004.Lexrank：
graph-based lexical centrality as salience in text summarization.
J.Artif.Int.Res.，22（1）：457-479.
Stephan Greene and Philip Resnik.2009.More than
words：syntactic packaging and implicit sentiment.In
NAACL'09，pages 503-511.
Aria Haghighi and Lucy Vanderwende.2009.Exploring
content models for multi-document summarization.In
NAACL'09，pages 362-370.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006.Negation, contrast and contradiction in text processing.
在单机运行时，想要完成这个统计任务，需要先切分出所有论文的名字存入一个Hash表中，然后遍历所有论文，查看引文信息，一一计数。因为文章数量很多，需要进行很多次内外存交换，这无疑会延长程序的执行时间。但在MapReduce中，这是一个WordCount就能解决的问题。
3.2 MapReduce计算模型
要了解MapReduce，首先需要了解MapReduce的载体是什么。在Hadoop中，用于执行MapReduce任务的机器有两个角色：一个是JobTracker，另一个是TaskTracker。JobTracker是用于管理和调度工作的，TaskTracker是用于执行工作的。一个Hadoop集群中只有一台JobTracker。
 3.2.1 MapReduce Job
在Hadoop中，每个MapReduce任务都被初始化为一个Job。每个Job又可以分为两个阶段：Map阶段和Reduce阶段。这两个阶段分别用两个函数来表示，即Map函数和Reduce函数。Map函数接收一个＜key, value＞形式的输入，然后产生同样为＜key, value＞形式的中间输出，Hadoop会负责将所有具有相同中间key值的value集合到一起传递给Reduce函数，Reduce函数接收一个如＜key，（list of values）＞形式的输入，然后对这个value集合进行处理并输出结果，Reduce的输出也是＜key, value＞形式的。
为了方便理解，分别将三个＜key, value＞对标记为＜k1，v1＞、＜k2，v2＞、＜k3，v3＞，那么上面所述的过程就可以用图3-1来表示了。
图 3-1 MapReduce程序数据变化的基本模型
3.2.2 Hadoop中的Hello World程序
上面所述的过程是MapReduce的核心，所有的MapReduce程序都具有图3-1所示的结构。下面我再举一个例子详述MapReduce的执行过程。
大家初次接触编程时学习的不论是哪种语言，看到的第一个示例程序可能都是“Hello World”。在Hadoop中也有一个类似于Hello World的程序。这就是WordCount。本节会结合这个程序具体讲解与MapReduce程序有关的所有类。这个程序的内容如下：
package cn.edu.ruc.cloudcomputing.book.chapter03；
import java.io.IOException；
import java.util.*；
import org.apache.hadoop.fs.Path；
import org.apache.hadoop.conf.*；
import org.apache.hadoop.io.*；
import org.apache.hadoop.mapred.*；
import org.apache.hadoop.util.*；
public class WordCount{
public static class Map extends MapReduceBase implements Mapper＜LongWritable，
Text, Text, IntWritable＞{
private final static IntWritable one=new IntWritable（1）；
private Text word=new Text（）；
public void map（LongWritable key, Text value, OutputCollector＜Text，
IntWritable＞output, Reporter reporter）throws IOException{
String line=value.toString（）；
StringTokenizer tokenizer=new StringTokenizer（line）；
while（tokenizer.hasMoreTokens（））{
word.set（tokenizer.nextToken（））；
output.collect（word, one）；
}
}
}
public static class Reduce extends MapReduceBase implements Reducer＜Text，
IntWritable, Text, IntWritable＞{
public void reduce（Text key, Iterator＜IntWritable＞values, OutputCollector＜Text，
IntWritable＞output, Reporter reporter）throws IOException{
int sum=0；
while（values.hasNext（））{
sum+=values.next（）.get（）；
}
output.collect（key, new IntWritable（sum））；
}
}
public static void main（String[]args）throws Exception{
JobConf conf=new JobConf（WordCount.class）；
conf.setJobName（"wordcount"）；
conf.setOutputKeyClass（Text.class）；
conf.setOutputValueClass（IntWritable.class）；
conf.setMapperClass（Map.class）；
conf.setReducerClass（Reduce.class）；
conf.setInputFormat（TextInputFormat.class）；
conf.setOutputFormat（TextOutputFormat.class）；
FileInputFormat.setInputPaths（conf, new Path（args[0]））；
FileOutputFormat.setOutputPath（conf, new Path（args[1]））；
JobClient.runJob（conf）；
}
}
同时，为了叙述方便，设定两个输入文件，如下：
echo"Hello World Bye World"＞file01
echo"Hello Hadoop Goodbye Hadoop"＞file02
看到这个程序，相信很多读者会对众多的预定义类感到很迷惑。其实这些类非常简单明了。首先，WordCount程序的代码虽多，但是执行过程却很简单，在本例中，它首先将输入文件读进来，然后交由Map程序处理，Map程序将输入读入后切出其中的单词，并标记它的数目为1，形成＜word，1＞的形式，然后交由Reduce处理，Reduce将相同key值（也就是word）的value值收集起来，形成＜word, list of 1＞的形式，之后将这些1值加起来，即为单词的个数，最后将这个＜key, value＞对以TextOutputFormat的形式输出到HDFS中。
针对这个数据流动过程，我挑出了如下几句代码来表述它的执行过程：
JobConf conf=new JobConf（MyMapre.class）；
conf.setJobName（"wordcount"）；
conf.setInputFormat（TextInputFormat.class）；
conf.setOutputFormat（TextOutputFormat.class）；
conf.setMapperClass（Map.class）；
conf.setReducerClass（Reduce.class）；
FileInputFormat.setInputPaths（conf, new Path（args[0]））；
FileOutputFormat.setOutputPath（conf, new Path（args[1]））；
首先讲解一下Job的初始化过程。Main函数调用Jobconf类来对MapReduce Job进行初始化，然后调用setJobName（）方法命名这个Job。对Job进行合理的命名有助于更快地找到Job，以便在JobTracker和TaskTracker的页面中对其进行监视。接着就会调用setInputPath（）和setOutputPath（）设置输入输出路径。下面会结合WordCount程序重点讲解Inputformat（）、OutputFormat（）、Map（）、Reduce（）这4种方法。
1.InputFormat（）和InputSplit
InputSplit是Hadoop中用来把输入数据传送给每个单独的Map, InputSplit存储的并非数据本身，而是一个分片长度和一个记录数据位置的数组。生成InputSplit的方法可以通过Inputformat（）来设置。当数据传送给Map时，Map会将输入分片传送到InputFormat（）上，InputFormat（）则调用getRecordReader（）方法生成RecordReader, RecordReader再通过creatKey（）、creatValue（）方法创建可供Map处理的＜key, value＞对，即＜k1，v1＞。简而言之，InputFormat（）方法是用来生成可供Map处理的＜key, value＞对的。
Hadoop预定义了多种方法将不同类型的输入数据转化为Map能够处理的＜key, value＞对，它们都继承自InputFormat，分别是：
BaileyBorweinPlouffe. BbpInputFormat
ComposableInputFormat
CompositeInputFormat
DBInputFormat
DistSum. Machine.AbstractInputFormat
FileInputFormat
其中，FileInputFormat又有多个子类，分别为：
CombineFileInputFormat
KeyValueTextInputFormat
NLineInputFormat
SequenceFileInputFormat
TeraInputFormat
TextInputFormat
其中，TextInputFormat是Hadoop默认的输入方法，在TextInputFormat中，每个文件（或其一部分）都会单独作为Map的输入，而这是继承自FileInputFormat的。之后，每行数据都会生成一条记录，每条记录则表示成＜key, value＞形式：
key值是每个数据的记录在数据分片中的字节偏移量，数据类型是LongWritable；
value值是每行的内容，数据类型是Text。
也就是说，输入数据会以如下的形式被传入Map中：
file01：
0 hello world bye world
file02