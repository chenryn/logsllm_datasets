ditional entropy of ﬁnite sequences is often used to estimate the
entropy rate. To estimate the entropy rate, we use the corrected
(cid:6)(cid:21)(cid:26)(cid:24)(cid:22)(cid:23)(cid:28)
(cid:4)(cid:22)(cid:20)(cid:23)(cid:22)(cid:21)(cid:15)(cid:21)(cid:26)
Figure 8: Classiﬁcation System
conditional entropy [28]. The corrected conditional entropy is de-
ﬁned as follows.
A random process X = {Xi} is deﬁned as a sequence of random
variables. The entropy of such a sequence of random variables is
deﬁned as:
H(X1, ..., Xm) = − X
X1,...,Xm
P (x1, ..., xm) log P (x1, ..., xm),
(1)
where P (x1, ..., xm) is the joint probability P (X1 = x1, ...,
Xm = xm).
The conditional entropy of a random variable given a previous
sequence of random variables is:
H(Xm | X1, ..., Xm−1) =H (X1, ..., Xm)− H(X1, ..., Xm−1).
(2)
Then, based on the conditional entropy, the entropy rate of a ran-
dom process is deﬁned as:
H(X) = lim
m→∞ H(Xm | X1, ..., Xm−1).
(3)
The corrected conditional entropy is computed as a modiﬁcation
of Equation 3. First, the joint probabilities, P (X1 = x1, ..., Xm =
xm) are replaced with empirically-derived probabilities. The data
is binned into Q bins, i.e., values are converted to bin numbers
from 1 to Q. The empirically-derived probabilities are then de-
termined by the proportions of bin number sequences in the data.
The entropy estimate and conditional entropy estimate, based on
empirically-derived probabilities, are denoted as EN and CE re-
spectively. Second, a corrective term, perc(Xm) · EN (X1), is
added to adjust for the limited number of sequences for increasing
values of m [28]. The corrected conditional entropy, denoted as
CCE, is computed as:
CCE(Xm | X1, ..., Xm−1) =
CE(Xm | X1, ..., Xm−1) + perc(Xm) · EN (X1),
(4)
where perc(Xm) is the percentage of unique sequences of length
m and EN (X1) is the entropy with m ﬁxed at 1 or the ﬁrst-order
entropy.
26
The estimate of the entropy rate is the minimum of the corrected
conditional entropy over different values of m. The minimum of
the corrected conditional entropy is considered to be the best esti-
mate of the entropy rate from the limited number of sequences.
4.2 Machine Learning Component
The machine learning component uses the content of tweets to
detect spam. We have observed that most spam tweets are gener-
ated by bots and only very few of them are manually posted by
humans. Thus, the presence of spam patterns usually indicates au-
tomation. Since tweets are text, determining if their content is spam
can be reduced to a text classiﬁcation problem. The text classiﬁca-
tion problem is formalized as f : T × C → {0, 1}, where f is
the classiﬁer, T = {t1, t2, ..., tn} are the texts to be classiﬁed, and
C = {c1, c2, ..., ck} are the classes [31]. A value of 1 for f (ti, cj)
indicates that text ti belongs to class cj, whereas a value of 0 indi-
cates it does not belong to that class. Bayesian classiﬁers are very
effective in text classiﬁcation, especially for email spam detection,
so we employ Bayesian classiﬁcation for our machine learning text
classiﬁcation component.
In Bayesian classiﬁcation, deciding if a message belongs to a
class, e.g., spam, is done by computing the corresponding proba-
bility based on its content, e.g., P (C = spam|M ), where M is a
message and C is a class. If the probability is over a certain thresh-
old, then the message is from that class.
The probability that a message M is spam, P (spam|M ), is
computed from Bayes theorem:
P (spam|M ) =
P (M|spam)P (spam)
P (M|spam)P (spam)
P (M )
=
(5)
P (M|spam)P (bot) +P (M|not spam)P (not spam)
The message M is represented as a feature vector (cid:3)f1, f2, ..., fn(cid:4),
where each feature f is one or more words in the message and each
feature is assumed to be conditionally independent.
P (spam|M ) =
.
nQ
P (spam)
P (fi|spam)
nQ
P (fi|spam) + P (not spam)
i=1
.
nQ
i=1
i=1
P (fi|not spam)
P (spam)
(6)
The calculation of P (spam|M ) varies in different implemen-
tations of Bayesian classiﬁcation. The implementation used for
our machine learning component is CRM114 [4]. CRM114 is a
powerful text classiﬁcation system that offers a variety of different
classiﬁers. The default classiﬁer for CRM114 is Orthogonal Sparse
Bigram (OSB), a variant of Bayesian classiﬁcation, which has been
shown to perform well for email spam ﬁltering. OSB differs from
other Bayesian classiﬁers in that it treats pairs of words as features.
4.3 Account Properties Component
related properties are very helpful for the user classiﬁcation. As
shown in Section 3.3, obvious difference exists between the human
and bot categories. The ﬁrst property is the URL ratio. The ra-
tio indicates how often a user includes external URLs in its posted
tweets. External URLs appear very often in tweets posted by a bot.
Our measure shows, on average the ratio of bot is 97%, while that
of human is much lower at 29%. Thus, a high ratio (e.g., close to
one) suggests a bot and a low ratio implies a human.
Besides inter-tweet delay and tweet content, some Twitter account-
The second property is tweeting device makeup. According to
Table 1, about 70% tweets of human are posted via web and mobile
devices (referred as manual devices), whereas about 87% tweets of
bot are posted via API and other auto-piloted programs (referred as
auto devices). The third property is the followers to friends ratio.
Figure 3 clearly shows the difference between human and bot. The
fourth property is link safety, i.e., to decide whether external links
in tweets are malicious/phishing URLs or not. We use Google’s
Safe Browsing (GSB) API project [16], which allows us to check
URLs against Google’s constantly-updated blacklists of suspected
phishing and malware pages. The component converts each URL9
into hash values based on Google’s rules, and performs the local
lookup from the downloaded Google’s blacklists. Appearance in
Google’s blacklists raises a red ﬂag for security breach. GSB is
also applied by Twitter for the link safety inspection [38]. The ﬁfth
property is whether a Twitter account is veriﬁed. No bot in our
ground truth dataset is veriﬁed. The account veriﬁcation suggests a
human. The last property is the account registration date. Accord-
ing to Figure 5, 94.8% of bots were registered in 2009.
The account properties component extracts these properties from
the user log, and sends them to the decision maker. It assists the en-
tropy component and the machine-learning component to improve
the classiﬁcation accuracy.
4.4 Decision Maker
Given an unknown user, the decision maker uses the features
identiﬁed by the above three components to determine whether it
is a human, bot, or cyborg. It is built on Linear Discriminant Anal-
ysis (LDA) [26]. LDA is a statistical method to determine a linear
combination of features that discriminate among multiple classes
of samples. More speciﬁcally, its underlying idea is to determine
whether classes differ in light of the means of a feature (or features),
and then to use that feature (or features) to identify classes. It is
very similar to analysis of variance (ANOVA) [29] and (logistic)
regression analysis [19]. However, a big difference is that LDA has
a fundamental assumption that independent variables are normally
distributed. In other words, it is assumed that variables represent a
sample from a multivariate normal distribution. Our classiﬁcation
involves three classes, human, bot and cyborg. Thus, it is a case
of multiclass LDA. Multiclass LDA has the following key steps.
First, it needs a training set and a test set that contain those samples
already classiﬁed as one of the C classes. Samples in the two sets
should not overlap with each other. Second, a discriminant model
is created to use effective features to identify classes. Choosing fea-
tures and assigning weights to features are the two important tasks
in the model creation. In the early data collection stage, one usu-
ally includes several features to see which one(s) contributes to the
discrimination. Some features are of very limited value for discrim-
ination, and should be removed from the model. Our model uses
forward stepwise analysis. In this way, the model is built step-by-
step. At each step, all the features are evaluated, and the one that
contributes the most to the discrimination is added into the model.
The selection process continues to next step. Suppose m features,
 are selected. Each class Ci has a classiﬁcation
function. With those functions, we can compute the classiﬁcation
score of an unknown sample for each class, by using the following
linear equation:
Si = wi0 +
nX
i=1
wi1v1 + wi2 ∗ v2 + ... + wim ∗ vm
(7)
where i denotes the respective class, Si denotes the classiﬁcation
score of the sample for class Ci, wi0 denotes a constant for class
Ci, and wij denotes the weight of j-th feature in class Ci.
The sample is classiﬁed into the class with the highest classi-
ﬁcation score. The model uses the training set to decide feature
weights. Every sample in the training set is already known for the
actual class it belongs to. The model keeps adjusting weights till
it reaches the maximum accuracy for the training set. Third, the
test set is used to validate the classiﬁcation accuracy of the model.
Since discriminant functions are derived from the training set, it is
inappropriate to reuse it for the validation. The test set contains
new data different from the training set, and generates more accu-
rate validation results.
5. EVALUATION
In this section, we ﬁrst evaluate the accuracy of our classiﬁcation
system based on the ground truth set that includes both the train-
9For a shortened URL, our component uses PHP cURL to get the
original one from the redirected HTTP response header, instead of
actually visiting the page.
27
Constant
Entropy
Table 2: Multi-class LDA Weights
Cyborg
-15.7787
9.7128
0.0164
3.3059
13.0164
7.6849
0.0002
Human
-25.9879
14.2524
-0.0018
-3.4474
16.4601
8.5910
0.0007
Bayesian text
URL ratio
Manual device %
Auto device %
Followers to friends ratio
Bot
-17.2416
4.4136
0.1366
8.5222
13.0950
18.3765
0.0003
ing and test datasets. Then, we apply the system to classify the
entire dataset of over 500,000 users collected. With the classiﬁca-
tion results, we further speculate the current composition of Twitter
user population. Finally, we discuss the robustness of the proposed
classiﬁcation system against possible evasions.
5.1 Methodology
As shown in Figure 8, the components of the classiﬁcation sys-
tem collaborate in the following way. The entropy component cal-
culates the entropy (and corrected conditional entropy) of inter-
tweet delays of a Twitter user. The entropy component only pro-
cesses logs with more than 100 tweets10. This limit helps reduce
noise in detecting automation. A lower entropy indicates peri-
odic or regular timing of tweeting behavior, a sign of automation,
whereas a higher entropy implies irregular behavior, a sign of hu-
man participation. The machine learning component determines if
the tweet content is either spam or not, based on the text patterns
it has learned. The content feature value is set to −1 for spam but
1 for non-spam. The account properties component checks all the
properties mentioned in Section 4.3, and generates a real-number-
type value for each property. Given a Twitter user, the above three
components generate a set of features and input them into the de-
cision maker. For each class, namely human, bot and cyborg, the
decision maker computes a classiﬁcation score for the user, and
classiﬁes it into the class with the highest score. The training of the
classiﬁcation system and its accuracy are detailed as follows.
5.2 Classiﬁcation System Training
The classiﬁcation system needs to be trained before being used.
In particular, the machine learning component and the decision
maker require training. The machine learning component is trained
on spam and non-spam datasets. The spam dataset consists of spam
tweets and spam external URLs, which are detected during the cre-
ation of the ground truth set. Some advanced spam bots intention-
ally inject non-spam tweets (usually in the format of pure text with-
out URLs, such as adages11) to confuse human users. Thus, we do
not include such vague tweets without external URLs. The non-
spam dataset consists of all human tweets and cyborg tweets with-
out external URLs. Most human tweets do not carry spam. Cyborg
tweets with links are hard to determine without checking linked
web pages. They can be either spam or non-spam. Thus, we do not
include this type of tweets in either dataset. Training the compo-
nent with up-to-date spam text patterns on Twitter helps improve
the accuracy.
The decision maker is trained to determine the weights of the
different features for classiﬁcation. We use Statistica, a statistical
tool [33], to calculate the feature weights. More speciﬁcally, the
datasheet of feature values and the actual class of users in the train-
10The inter-tweet span could be wild on Twitter. An account may be
inactive for months, but suddenly tweets at an intensive frequency
for a short-term, and then enters hibernation again.
It generates
noise to the entropy component. Thus, the entropy component does
not process logs with less than 100 tweets. Besides, in practice it is
nearly impossible to determine automation based on a very limited
number of tweets.
11A typical content pattern is listed as follows. Tweet 1, A friend
in need is a friend in deed. Tweet 2, Danger is next neighbor to
security. Tweet 3, Work home and make $3k per month. Check out
how, http://tinyurl.com/bF234T.
ing set are inputted into the classiﬁer. LDA generates a weight table
(Table 2) to achieve the maximum accuracy. In other words, it in-
cludes as many users as possible, whose classiﬁed class matches
actual class. The weights are then used by the decision maker to
classify users.
The larger the (standardized) weight, the larger is the unique con-
tribution of the corresponding feature to the discrimination. Table
2 shows that, entropy, URL ratio, and manual/auto device percent-
age are the important features for the classiﬁer. Only those shown
to be statistically signiﬁcant should be used for classiﬁcation, and
non-signiﬁcant ones should be ignored. Thus, some features col-
lected by the account properties component in Section 4.3, includ-
ing followers to friends ratio, link safety, account veriﬁcation and
registration date, are excluded from the classiﬁer.
Here we brieﬂy explain why several features, such as followers
to friends ratio, link safety, account veriﬁcation, and registration
date, are not as important in the actual discrimination as expected.
Bots used to have more friends than followers [25], and the ratio is
less than one in this situation. However, there have emerged some
more sophisticated bots that unfollow their friends if they do not
follow back within a certain amount of time. They cunningly keep
the ratio close to one. This strategy makes the ratio feature less
useful. Most spam bots spread spam links on Twitter, instead of
phishing or malicious links which are the primary target of the link
safety inspector. Only 0.2% of the users in the training set do not
pass the link safety inspection. Thus, the link safety feature has
little weight under LDA due to its statistical insigniﬁcance. Simi-
larly, account veriﬁcation has a very small weight, because it is also
quite rare. Only 1.8% of the users are veriﬁed. Lastly, account reg-
istration dates greatly overlap among bots, humans, and cyborgs,
making this feature not useful for discrimination as well.