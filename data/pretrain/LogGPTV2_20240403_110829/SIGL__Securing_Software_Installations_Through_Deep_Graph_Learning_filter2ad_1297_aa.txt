title:SIGL: Securing Software Installations Through Deep Graph Learning
author:Xueyuan Han and
Xiao Yu and
Thomas F. J.-M. Pasquier and
Ding Li and
Junghwan Rhee and
James W. Mickens and
Margo I. Seltzer and
Haifeng Chen
Sigl: Securing Software Installations Through 
Deep Graph Learning
Xueyuan Han, Harvard University; Xiao Yu, NEC Laboratories America; 
Thomas Pasquier, University of Bristol; Ding Li, Peking University; 
Junghwan Rhee, NEC Laboratories America; James Mickens, 
Harvard University; Margo Seltzer, University of British Columbia; 
Haifeng Chen, NEC Laboratories America
https://www.usenix.org/conference/usenixsecurity21/presentation/han-xueyuan
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.SIGL: Securing Software Installations Through Deep Graph Learning∗
Xueyuan Han1, Xiao Yu2, Thomas Pasquier3, Ding Li4, Junghwan Rhee2, James Mickens1, Margo Seltzer5,
and Haifeng Chen2
1Harvard University, 2NEC Laboratories America, 3University of Bristol, 4Peking University, 5University of British Columbia
Abstract
Many users implicitly assume that software can only be ex-
ploited after it is installed. However, recent supply-chain at-
tacks demonstrate that application integrity must be ensured
during installation itself. We introduce SIGL, a new tool for de-
tecting malicious behavior during software installation. SIGL
collects traces of system call activity, building a data prove-
nance graph that it analyzes using a novel autoencoder archi-
tecture with a graph long short-term memory network (graph
LSTM) for the encoder and a standard multilayer perceptron
for the decoder. SIGL ﬂags suspicious installations as well
as the speciﬁc installation-time processes that are likely to
be malicious. Using a test corpus of 625 malicious installers
containing real-world malware, we demonstrate that SIGL
has a detection accuracy of 96%, outperforming similar sys-
tems from industry and academia by up to 87% in precision
and recall and 45% in accuracy. We also demonstrate that
SIGL can pinpoint the processes most likely to have triggered
malicious behavior, works on different audit platforms and op-
erating systems, and is robust to training data contamination
and adversarial attack. It can be used with application-speciﬁc
models, even in the presence of new software versions, as well
as application-agnostic meta-models that encompass a wide
range of applications and installers.
1 Introduction
Software installation is risky. Installer programs often ex-
ecute with administrative privileges, providing installation-
time attackers with powerful capabilities to immediately
corrupt a system or establish longer-term persistent threats.
Signed installation packages verify a package’s origin, but
not its semantic integrity—installers can be corrupted before
they are signed. Thus, as post-installation malware detection
has become more sophisticated, corruption of digital sup-
ply chains increased by 78% in the one year from 2018 to
2019 [2]. For example, CCleaner is a popular application for
removing unused ﬁles on desktop computers. In 2017, attack-
ers breached several workstations belonging to its developers,
∗SIGL is pronounced as “seagull”.
inserting bot software into the ofﬁcial application. The com-
promised installer was downloaded by 2.27 million users, in-
cluding employees from major tech companies (e.g., Google,
and Microsoft) before being detected and removed [39].
Unfortunately, there are no strong defenses against ma-
licious installation. Fingerprint-based malware detection is
easy to evade by tweaking a few bytes of installation data [38].
Content-agnostic tools try to blacklist the untrusted servers
and web pages that host malicious software [8]; however, as
the CCleaner attack demonstrates, corrupted supply chains
provide malicious content via trusted sources. More sophisti-
cated detection algorithms assign dynamic reputation scores
to ﬁle servers [64,70]. However, calculating reputation scores
is difﬁcult, requiring labeled malware samples [70] or a priori
knowledge about the characteristics of malicious ﬁles [64].
To improve detection accuracy, server reputation scoring
can be augmented with client-side anomaly detection. For
example, data provenance frameworks observe causal inter-
actions between kernel-level objects, such as processes, ﬁles,
and network sockets [10]. Malicious installers will manipulate
these objects in ways that are statistically unlikely (and thus
detectable using statistical analysis). However, approaches us-
ing data provenance [28, 48] are designed for long timescales
and unpredictable exploit timings: a provenance log spans
weeks or months of system activity, with threats potentially
arriving at any moment during the logging period. To reduce
log sizes, provenance systems reduce high-ﬁdelity event logs
to lower-ﬁdelity summarizations, performing intrusion detec-
tion on the summaries. Unfortunately, summarizations hurt
diagnostic ability; they omit important contextual informa-
tion about, for example, the speciﬁc processes that malware
launched, and the speciﬁc ﬁles that malware accessed. When
they correctly detect an anomaly, reconstructing the low-level
details of how the attack unfolded requires manual work that is
difﬁcult and error-prone, but critical for understanding which
attack vectors need to be patched.
SIGL reduces the manual effort needed to (1) detect mali-
cious installations and (2) identify the malicious processes.
We observe that once a malicious installation begins, a ma-
USENIX Association
30th USENIX Security Symposium    2345
chine typically exhibits anomalous behavior (§ 3). Thus, SIGL
can afford to collect high-ﬁdelity (but short-term) provenance
graphs, discarding old ones if no malicious installations are
detected. SIGL analyzes provenance data using a novel form
of unsupervised deep learning, which means that human ana-
lysts do not have to label training sets with both benign and
malicious graphs. Instead, given a machine which is known to
be malware-free, SIGL automatically featurizes provenance
graphs using a novel component-based embedding technique
tailored for system graphs (§ 4.3). It then applies long short-
term memory networks (LSTMs) [62] to extract the graph
features corresponding to normal behavior. These features do
not rely on any particular malware; therefore, they are general
and robust against malicious behavior. When deployed on
in-the-wild machines, SIGL uses anomaly scores (§ 4.5) to
calculate how far a machine deviates from the baseline fea-
tures (and thus how likely it is that a machine is experiencing
a malicious installation).
We evaluate SIGL by collecting baseline data from an en-
terprise database storing system events from 141 machines
at NEC Labs America. Using malicious installers from the
wild (as well as ones that we created ourselves), we tested
SIGL’s ability to detect malicious installation activity. SIGL
achieved precision, recall, accuracy, and F-score values all
greater than 0.94; in contrast, competing systems that we
tested were unable to achieve better than 0.9 on more than
a single metric, producing substantially worse scores on the
remaining metrics (§ 5.4). We also found that SIGL’s ranking
system typically produces a small set of candidate processes
responsible for the attack, including the one actually respon-
sible (§ 5.5). To demonstrate the applicability and robustness
of our approach, we further evaluate SIGL on different plat-
forms (i.e., Windows and Linux) and with various adversarial
scenarios (e.g., data contamination and evasion).
In summary, we make the following contributions:
• We formalize the problem of detecting malicious soft-
ware installation. In particular, we introduce a new kind of
provenance graph, called a software installation graph, that
records the short-term (but high-ﬁdelity) provenance infor-
mation needed to capture malicious installation activity.
• We are the ﬁrst to apply deep graph learning to the auto-
matic detection of anomalies in software installation graphs
(SIGs). Our approach uses a novel autoencoder architecture
layered atop a long short-term memory network.
• We present a novel node featurization model for system-
level provenance entities that is generalizable to applica-
tions beyond our current project.
• We build and thoroughly evaluate SIGL, an unsupervised de-
tection system, that identiﬁes malicious installations. SIGL
creates SIGs using information provided by lightweight
audit frameworks such as Windows ETW or Linux Audit.
Thus, SIGL requires no additional infrastructure on end
hosts, besides a daemon that collects audit data and sends it
to a centralized analysis machine. SIGL outperforms current
Figure 1: The software installation graph from the attack scenario described
in § 2. The shaded area shows malicious activities not observed in a legitimate
installation. We omit some edges, nodes, and node labels for clarity.
state-of-the-art malware detectors, while also providing the
unique ability to identify the set of processes potentially
involved in malicious installation activity.
• To the best of our knowledge, we are the ﬁrst to investigate
graph-based adversarial attacks [77, 84] given realistic and
practical systems constraints faced by the attackers.
2 Background & Motivation
We simulate the following real-world enterprise attack sce-
nario [51] to illustrate the limitations of existing tools and
motivate SIGL’s design. Our scenario uses the Dharma ran-
somware, also known as CrySIS, which has become increas-
ingly prevalent in enterprises [4]. One important factor that
contributes to its popularity is its continuous evolution to
avoid detection. We simulate a recent Dharma variant where
the adversary bundles the ransomware tool with a benign anti-
virus remover, ESET AV Remover, creating a new version of
the software package. The attackers then launch a phishing
attack, impersonating Microsoft, urging enterprise employ-
ees to upgrade their anti-virus tool. When an unsuspecting
employee runs the installer, Dharma runs in the background,
encrypting user ﬁles, while the employee interacts with the
ESET AV Remover installer 1. Neither existing malware de-
tection tools nor newer log- or provenance-based analysis
systems are a good match for these kinds of attacks because:
Limitations of Malware Detection Tools. The Dharma sce-
nario poses several challenges to existing malware detec-
tion solutions. First, customized variants of Dharma will ef-
fectively evade signature-based malware analysis, including
commercial anti-virus detection [47]. In fact, many variants
of ransomware families, including Dharma, leverage popular
installation frameworks (§ 5.1) to circumvent anti-virus de-
tection without even changing the malware signature [16]. A
1We evaluate SIGL in this scenario in § 5.
2346    30th USENIX Security Symposium
USENIX Association
AVRemover.exeAVRemover.exeProcess StartAVRemover.exeFile WriteAVRemover.exeProcess StartAVRemover.exeFile WriteAVRSrv.exeProcess Starta.b.c.d:eIP Writelibwaheap.dllFile Writeeset.datFile Writelibwautils.dllFile Writeexclusions.txtFile WriteAVRSrv.exeFile WriteFile WriteFile WriteFile Writem.n.i,j:kIP Writerm.exeFile Writelibwaheap.dllFile Writelibwautils.dllFile WriteAVRemover.exeFile Readcabinet.dllFile ReadFile ReadFile ReadFile ReadFile ReadFile ReadIP ReadFile ReadFile ReadFile ReadFile ReadFile ReadFile ReadFile ReadFile ReadFile ReadsensAPI.dllFile ReadFile ReadFile ReadFile ReadIP ReadAVRemover.exeProcess StartFile Writetaskhost.exeProcess Starttaskhost.exeFile Writex.y.z.s:tIP WriteAVRemover.exeFile ReadFile ReadFile ReadFile ReadFile ReadIP Readrecent incident demonstrates that, similar to our motivating
scenario, malware can safely hide in those installation frame-
works, bypassing all anti-virus products on VirusTotal [66].
Second, bundling malicious software with legitimate software
thwarts conventional ﬁle reputation analysis [64, 70].
Downloader graph analysis [45] or malware distribution
infrastructure analysis [8] might have proven effective in this
instance if it were possible to notice the suspicious origin
of the bundled installer. However, if the attackers inﬁltrated
trusted software vendors to distribute the compromised soft-
ware package [15] (e.g., the CCleaner incident), then, even
those approaches would have been rendered ineffective [8].
In summary, these types of exploits can successfully evade
detection from existing solutions.
Limitations of Log and Provenance Analysis Solutions.
Today’s enterprises are rich in commercial threat detection
tools and log data; however, as we show in § 5.3, the log-based
commercial TDS [59] deployed in our enterprise produces a
large number of false positive alarms, because it is strict in
matching predeﬁned, single-event signatures (e.g., a process
should not write to an unknown ﬁle). Newer research proto-
types use provenance for intrusion detection [28, 29, 48, 61],
which provides more contextual analysis, but these systems
value time and space efﬁciency over ﬁne-grain learning preci-
sion. As such, they tend to over-generalize statistical graph
features with constrained graph exploration. For example,
Fig. 1 depicts the graph structure surrounding the malicious
process (taskhost.exe). Rectangles, ovals, and diamonds
represent processes, ﬁles, and sockets, respectively; edges rep-
resent relationships between these objects. The shaded area
represents the malicious activity that does not exist in normal
ESET AV Remover installations. These malicious activities
comprise only a small portion of the entire graph, essentially
hiding among the greater number of normal events that take
place during benign installation. Notice that the graph struc-
ture surrounding the malicious process (taskhost.exe) is
similar to that around the benign AVRemover.exe, both of
which start a new process and communicate with an outside
IP address. Existing IDS cannot distinguish these similar
structures, because those systems use localized graph anal-
ysis (e.g., 1-hop neighborhoods) that limits their ability to
explore more distant relationships that provide a richer pic-
ture of host behavior. Thus, they produce a large number of
false alarms. Even when the alarms are real, it is difﬁcult
to pinpoint the cause of an alarm, because existing systems
summarize features, thereby losing details.
These existing systems make rational tradeoffs, because
their goal is whole-system realtime detection over a long
time period. Consequently, they must handle large and fast-
growing provenance graphs. In contrast, SIGL focuses on
the detection of malicious installation and thus requires a
different set of trade-offs.
SIGL Insight. The key insight behind SIGL is that software
installation is generally a well-deﬁned, multi-staged process
that can be represented as a bounded, static graph. The
bounded nature of the graph means that we can analyze the
graph in its entirety rather than having to summarize it. The
multiple stages of installation suggest that we use models that
are inherently temporal. SIGL learns both the structure and se-
quencing of installation without manual feature engineering.
3 Problem Formulation and Threat Model
We formalize the software installation malware detection
problem as a graph-based outlier detection problem. Software
installation begins when installer execution begins, e.g., the
user double clicks on the downloaded package; it terminates
when the installer process and all its descendants exit.
We characterize the installation behavior of a software
package as a chain of system events leading to its binary ﬁles
being written to a host system. We then deﬁne a software
installation graph G = (V,E), an attributed directed acyclic
graph (DAG), to represent this event chain. Nodes V represent
system subjects (i.e., processes) and objects (e.g., ﬁles, sock-
ets), and edges E record interactions between them. Given a
number of benign installations L = {G (s1),G (s2), . . . ,G (s j)}
on endpoint systems s1,s2, . . . ,s j, our goal is to learn a model
M of the installation behavior that classiﬁes a new installation
graph G (sk),k (cid:54)∈ {1,2, . . . , j} as benign or malicious. Given
an abnormal G, we also want to rank process nodes Vp ⊂ V
to identify processes exhibiting the most anomalous behavior.
We assume that the attacker’s attempt to inﬁltrate an enter-
prise network through malicious software installation is the
initial system breach. The attacker may distribute malicious
installers using phishing emails, through legitimate software
distribution channels (i.e., by compromising the integrity of
such channels or acting as a man-in-the-middle), or by direct
access to the network (i.e., an insider attack).
SIGL’s threat model assumes the integrity of the under-
lying OS and audit framework, as is standard for existing
provenance-based systems [28, 61]. We further assume the
integrity of provenance records, which can be guaranteed by
using existing secure provenance systems [60].
4 SIGL Framework
We begin with an overview of SIGL’s architecture and then
present the technical details of each major component.
4.1 System Overview
SIGL uses abnormal system behavior to detect installation
of malicious software. Its operation consists of three stages: 1
data collection & featurization, 2 model training & validation,
and 3 anomaly detection & prioritization. Fig. 2 illustrates
SIGL’s architecture and workﬂow.
1 Data Collection & Featurization. For each software in-
stallation considered, SIGL gathers audit logs from a col-
lection of machines in the enterprise and transforms each
machine’s audit logs into a graphical representation called a
software installation graph (SIG, § 4.2). It then divides the
USENIX Association
30th USENIX Security Symposium    2347
SIGL produces the SIG by backtracking [43] from the in-
stalled software executable(s), represented as file node(s).
Given a file node, SIGL adds all edges having that node as
their destination. It then recursively repeats this procedure
for each newly added node, backtracking to the download
of the installation package. The resulting graph includes all
processes involved in the installation as well as any e.g.,
dynamically linked libraries (DLL) that were executed. We
apply an adjustable time bound on how far back we track
generic system services (represented as process nodes) that
are commonly invoked during software installation, thereby
minimizing dependency explosion [46]. If the installation
produced more than one installed executable, we combine the
backtraces into a single SIG. As is done in existing prove-
nance based analysis work [56, 60, 61], we produce acyclic
SIGs by creating multiple node versions as the state of the
corresponding subject/object changes [58].
4.3 Node Embedding for System Entities
Machine learning tasks depend on having a set of infor-
mative, discriminative, and independent features [25]. Node
featurization is an important building block in graph learning.
Popular network representation learning frameworks, such
as node2vec [25], DeepWalk [63], and metapath2vec [18],
apply natural language processing (NLP) techniques, most no-
tably word2vec [55], to derive latent embeddings that capture
contextual information encoded in the networks. However,
these approaches are not designed in the context of repre-
senting system entities; in particular, their node features do
not encode relationships between system entities and their
functionality within the system, which are important for down-
stream graph learning and anomaly detection.
A good embedding approach for system-level prove-
nance nodes must satisfy two important properties. First,
given a system entity that plays a particular role in a sys-
tem, its embedding must be close to that of other entities
if and only if their roles are similar. For example, both
system DLLs c:\windows\system32\ntdll.dll and c:
\windows\system32\kernel32.dll contain kernel func-
tions. Their embeddings should be close to each other in
the embedding space to facilitate downstream graph learning
that captures behavioral similarity of processes loading and
executing these two DLLs.
Second, the embedding approach must generalize to sys-
tem entities not in the training dataset. Such entities are espe-
cially common in software installation, because the installa-
tion almost always introduces temporary ﬁles and processes
that have semi-random path names. Mishandling such enti-
ties (e.g., assigning random embeddings) would cause down-
stream graph learning to produce excessive false positives for
lack of meaningful features.
We satisfy both of these properties by featurizing SIG
nodes in an embedding space such that node embeddings
encode semantic meanings of the system entities they repre-
Figure 2: SIGL collects existing audit data from enterprise workstations and
constructs software installation graphs to train a deep autoencoder using a
graph LSTM as its encoder. The resulting model is used to detect anomalous
test graphs and rank nodes within the graph based on their anomaly scores.
Subject
process
Object
process
file
socket
Event Relationship
start; end
rename; read; write; execute; delete
send; receive
Table 1: System entities and dependency relationships.
complete set of graphs (G) into training (GT ) and validation
(GV ) sets, with approximately 80% in the training set and 20%
in the validation set. Thus, G represents a benign software
installation graph for a particular install. SIGL then learns two
node embedding models (§ 4.3) from GT .
2 Model Training & Validation. Given the features learned
in 1 , SIGL trains a deep graph learning model (§ 4.4), which
is a deep autoencoder with a graph LSTM component as
its encoder and a multilayer perceptron as its decoder. The
autoencoder learns to reconstruct normal process nodes in
G ∈ GT from their latent representations encoded by the graph
LSTM, minimizing reconstruction errors. SIGL then uses the
validation data GV to verify the performance of the learned
model and, using the reconstruction errors, determine the
threshold for anomaly detection.
3 Anomaly Detection & Prioritization. Given a trained
model and threshold (§ 4.5), SIGL takes audit logs from a new
software installation, generates its corresponding SIG, embeds
its nodes using the trained node embedding models, and uses
the autoencoder model to reconstruct all process nodes. The
resulting reconstruction losses are the anomaly scores for each
node. If the overall anomaly score exceeds the threshold, SIGL
classiﬁes the installation as abnormal and reports a list, sorted