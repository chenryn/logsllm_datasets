1. Heterogeneous Naming Conventions. We do not know,
a priori, if a given suffix uses a convention that embeds router
names in hostnames. Neither do we know what sequence of regex
components is required to capture the naming convention, leading
to a search space that is infeasible to learn through brute force.
Instead, we must use heuristics to narrow the search space. When
a single network uses a convention, it may use multiple different
formats depending on their internal needs and the roles of their
routers; a single suffix may require multiple regexes to capture the
diversity of formats within the suffix and minimize false inferences.
2. Imperfect Naming Training Data. Network operators have
complete control over the information they store in their zones.
Some network operators maintain their zones automatically, using
information stored in well-maintained centralized databases [6].
Other operators maintain their zones manually, or the centralized
database might not be kept up to date. These artifacts hamper our
ability to learn naming conventions, as the interfaces may appear
as if they do not belong to a particular router (false negatives) or
belong to a different router (false positives).
3. Imperfect Router Training Data. Alias resolution tech-
niques (§2.4) are only feasible for a subset of the Internet topology.
The most feasible IPv4 technique, MIDAR, was applicable on up
to ≈80% of ≈2.3M interfaces probed in 2013 work [12], and the
most feasible IPv6 technique, Speedtrap, was applicable on up to
≈30% of ≈53K interfaces probed in 2013 work [16]. Because these
techniques actively probe routers, through probe scheduling, router
rate-limiting, router implementations, and packet loss, it is possible
for these techniques to miss aliases (false negatives). Further, these
techniques may associate interfaces that are not aliases through
coincidence of returned values (false positives).
Learning Regexes to Extract Router Names from Hostnames
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Regex component
Anything
(score: 0 per component)
Exclude specified punctuation
(score: 1 per component)
Specified classes
(score: 2 per [a-z\d]+,
3 per [a-z]+ or \d+)
IPv4 address
(score: 3 per \d+)
IPv6 address
(score: 3 per [a-f\d]+)
Literal
(score: 4 per character)
Example
.+
Sum
0
[^-]+
[^\.]+
[^\.]+\.[^\.]+
[^-]+-[^\.]+\.[^-]+
[a-z\d]+
[a-z]+
[a-z]+\d+
\d+\.\d+
\d+-\d+-\d+-\d+
[a-f\d]+
[a-f\d]+-[a-f\d]+
foo
infra\.cdn
1
1
2
3
2
3
6
6
12
3
6
12
36
Table 2: Scores of individual regex components sum to give
a specificity score. The more specific a component is, the
larger the contribution to the specificity score. If two regexes
evaluate the same, we break ties using the specificity score.
Figure 3: Examples of IP addresses embedded in hostnames.
Operators do not always embed all of the IP address in the
corresponding hostname.
When we build a regex, we assign each regex component a score
according to how specific the component is, which we sum to obtain
a specificity score. Table 2 lists the specificity scores per component,
where more specific components have higher component scores.
We chose the component scores so that we would choose the regex
with the highest (most specific) score when breaking ties between
regexes that perform the same clustering, i.e., we would prefer
NC #2 over #1 in figure 2. Table 2 shows that we can include a
variable number of components to cover IP addresses embedded
in hostnames, because operators do not always embed all of the IP
address in a corresponding hostname, as illustrated in figure 3.
We also prefer regexes that contain fewer extraction elements.
The first regex in NC #4 in figure 2 contains three extraction ele-
ments, selecting the middle digit (i.e., 0) for savvis.net routers 1-3
in figure 1. Extracting this digit is a symptom of over-fitting, as
the middle digit refers to the interface card and is not part of the
router name. Further, extracting this digit provides no additional
clustering benefit over the first regex in NC #2 in figure 2, which
contains two extraction elements.
Figure 2: Specificity of naming conventions for the routers
in figure 1 on a continuum. To avoid over-fitting to training
data, we choose the most specific convention with the fewest
regexes when choosing between conventions with similar
clustering (NC #2).
4 PRINCIPLES
Before discussing our method, we first outline tensions and princi-
ples we arrived at for addressing them. The key issue facing our
work is that it is impossible to know the intent an operator had
when assigning a hostname to a router interface, or whether or not
the training data for a suffix reflects their intent. That is, it is not
possible for anyone other than an operator with ground truth to
distinguish between an operator using multiple conventions for
different routers in their suffix, and errors in the training data that
follow a pattern. This section describes our approach to establishing
a sound basis for naming convention (NC) inference.
4.1 Specificity
Because we use a machine learning approach to infer a naming con-
vention, it is possible that we could derive a convention that overfits
to the training data so that there is perfect alignment between the
clustering in the training data and the clustering of interfaces by
the naming convention. We know, however, that the training data
is not perfect (§3).
Naming conventions should be as specific as possible so that
they capture patterns in the training set, but no more specific than
necessary. Figure 2 shows a specificity continuum for candidate
naming conventions for savvis.net routers in figure 1. If a naming
convention with fewer regexes achieves similar clustering against
training data compared to a convention with more regexes, then
we prefer the convention with fewer regexes, i.e., we prefer NCs #1
and #2 over #3 and #4 in figure 2. We do this to avoid overfitting
to the training data, as our method will otherwise infer naming
conventions with many regexes, each of which apply to a small
fraction of hostnames, including those with errors following a
pattern in them, and not representing the operator’s intent.
^([a-z]+\d+)-.+\.([a-z\d]+)\.savvis\.net$^(das\d-v30)\d{2}.+\.([a-z]{2}\d)\.savvis\.net$^(esr\d-(?:ge|xe))-\d-(\d)-\d\.([a-z]{3}\d*)\.savvis\.net$^([^-]+)-.+\.([^\.]+)\.savvis\.net$Under-speciﬁcOver-speciﬁc^(das\d)-v30\d{2}.+\.([a-z]{2}\d)\.savvis\.net$^(esr\d)-(?:ge|xe)-\d-\d-\d\.([a-z]{3}\d*)\.savvis\.net$esr1|jfk2, esr2|pax, esr1|pax, das1|nj2, das2|oc2, das2|nj2esr1|jfk2, esr2|pax, esr1|pax, das1|nj2, das2|oc2, das2|nj2esr1|jfk2, esr2|pax, esr1|paxdas1|nj2, das2|oc2, das2|nj2esr1-ge|0|jfk2, esr2-xe|0|pax, esr1-xe|0|paxdas1- v30|nj2, das2-v30|oc2, das2-v30|nj2NC#1:NC#2:NC#3:NC#4:154.126.82.12294.199.152.9tgn.126.82.122.tgn.mg152-9-f7m000p01cern.core.as8723.net92.60.81.55.81.unused-addr.ncport.ru66.161.134.16166-161-134-161.meyertool.com2001:4060:1:3001::2prt-cbl-sw1-vlan-3001.gw.imp.ch2804:321c::12804-321c-0-0-0-0-0-1.nslink.net.br2a00:aa40:0:235::96gum-core-rou-235-096.oberberg.neIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Matthew Luckie, Bradley Huffaker, and k claffy
Figure 4: Evaluating two naming conventions (NCs) on Level3 training data. Table 3 defines the per-interface classifications
we assign. NC #1 splits 1d + 1e from router 1 (FNE), clusters 5a + 5b with 6a + 6b (FP), and includes an IPv4 literal in the name
for router 9 (FIP). NC #2 correctly clusters all hostnames where clustering is possible.
Class Relationship to training data
TP
FP
True positive: clustered to same training router.
False positive: clustered to different training
router.
False IP: extraction includes portion of IP
address embedded in hostname.
False negative extraction: interfaces of training
router clustered to separate routers.
False negative unmatched: regex does not match.
Single positive: assigned to own cluster, no other
hostnames in same suffix on training router.
Single negative: regex does not match, no other
hostnames in same suffix on training router.
FIP
FNE
FNU
SP
SN
Table 3: Per-interface classifications of clustering according
to training data, guiding refinement of regexes.
4.2 Fidelity to Training Data
We evaluate naming conventions according to their ability to cluster
hostnames congruent with corresponding routers in the training
data. We chose an evaluation approach that guides refinement of
regexes that form a naming convention. We illustrate our evaluation
approach using the Level3 routers shown in figure 4; NC #2 is better
than NC #1 by this principle. Table 3 summarizes the definitions
for per-interface classifications we assign during evaluation.
We assign a true positive (TP) to an interface when a NC clus-
ters at least two interfaces congruently with the clustering on the
corresponding training router in ITDK. We assign a false positive
(FP) to an interface when a NC clusters the interface incongruently
with the clustering on the corresponding training routers.
We distinguish two classes of false negative. A false negative
extraction (FNE) occurs when a NC separates interfaces of a training
router into distinct clusters, for example interfaces 1d and 1e in
NC #1 in figure 4. A false negative unmatched (FNU) occurs when a
NC does not extract a name from a hostname on a training router
that has more than one hostname in the same suffix, for example
interfaces 7c and 8b in NC #1 in figure 4. We use FNE and FNU
classifications to guide refinement. A FNE can indicate that a regex
contains an unnecessary extraction; the first extraction element in
regex in NC #1 separates interfaces from the same router, but the
logical-or statement in the first regex of NC #2 retains the cluster.
A FNU can indicate that a naming convention does not cluster
interfaces that it should; NC #1 does not cluster 7c and 8b with
their training routers, but the second regex in NC #2 does.
We assign a false IP (FIP) when the extraction includes a portion
of an IP address that an operator embedded in a hostname – for
example, for interface 9a in NC #1 in figure 4 – as a router name
does not include a portion of an IP address. We detect this class of
error by noting the position in the hostname of sequences of at least
two IPv4 address byte values or four contiguous IPv6 hexadecimal
digits that match the IP address of the interface, and determining if
they overlap with the extracted name. This class of hostname often
follows a pattern because operators can automatically populate
these hostnames using macros provided by DNS server software.
We learn filter regexes to ignore these hostnames when necessary.
We also distinguish two classes of inference when the training
router has a single interface in a suffix. A single positive (SP) occurs