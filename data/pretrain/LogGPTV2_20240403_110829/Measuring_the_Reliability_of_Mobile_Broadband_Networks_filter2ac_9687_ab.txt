ing the reliability of MBB networks through end-to-end mea-
surements. An important aspect that is missing from this
study, is mobility. All measurement nodes used in this work
are stationary, and we can therefore not use these to describe
how the stability of the oﬀered service varies as you move.
Figure 3: NNE overview.
There are, however, also advantages in doing measurements
from ﬁxed locations, since it removes a signiﬁcant source
of variation and uncertainty in the measurements. In future
work, we plan to revisit MBB reliability in a mobile setting.
3. SYSTEM OVERVIEW AND DATA
This section presents the infrastructure that was used to
run the measurement experiments described in this work,
the MBB networks that were measured, the collected data,
and how it is stored and post-processed.
3.1 The Nornet Edge measurement platform
NNE (Fig. 3) is a dedicated infrastructure for measure-
ments and experimentation in MBB networks [15]. It con-
sists of several hundred measurement nodes geographically
distributed in more than 100 municipalities all over Norway,
and a server-side infrastructure for management, processing
and data storage. Figure 4 shows the placement of NNE
nodes in Norway classiﬁed according to the number of MBB
networks the node was connected to. NNE nodes are dis-
tributed to reﬂect the population density in Norway, with
some bias towards urban areas. Nodes are placed indoors in
small or large population centers, with a higher density of
nodes in larger cities. More than half (177) NNE nodes are
deployed in three largest cities, where 26.7%1 of the coutry
population lives.
An NNE node is a custom-made single-board computer,
with a Samsung S5PV210 Cortex A8 microprocessor, one
Fast Ethernet port, and 7 on-board USB ports. The node
runs a standard Debian Linux distribution, giving large ﬂex-
ibility in the types of tools and experiments that can be
supported. NNE also oﬀers a set of tools for connection and
conﬁguration management, and a framework for deploying
and managing measurement experiments. Each node is con-
nected to 1-4 UMTS networks and 1 CDMA2000 1x Ev-Do
network, using standard subscriptions. For the UMTS net-
works, connections are through Huawei E353 or E3131 3G
USB modems. These modems support UMTS standards up
to DC-HSPA and HSPA+ (”3.75G”) respectively, but not
LTE (”4G”). They are conﬁgured so that they always con-
nect to the 3G network where available, and fall back to 2G
elsewhere. The same modem model is always used for all
networks on the same node, to avoid diﬀerences caused by
diﬀerent hardware. For the CDMA2000 network, we con-
nect to the Internet via a CDMA home gateway device over
the Ethernet port.
1http://www.ssb.no/en/beftett/
47Figure 5: The operators and radio access networks
measured in this study.
management system. Measurements are then performed
against measurement servers that are part of the NNE back-
end. The measurement servers are well provisioned in terms
of bandwidth and processing power, to make sure they are
not a performance limiting factor. Data from the mea-
surements are uploaded to the backend database periodi-
cally. The data is post-processed to calculate aggregates
and also to ﬁlter out time periods from problematic con-
nections, NNE maintenance windows or when NNE experi-
enced problems at the server-side due to hardware problems
or problems with their network provider.
3.4 Metadata collection
The mode and RRC state of an MBB connection directly
impacts its performance. To better explain the observed
behavior, it is therefore important to collect state changes
along with measurement results. The CDMA2000 gateway
device provides only very limited diagnostic data, therefore
we collect state information only for UMTS networks. The
state attributes that are the most relevant to our measure-
ments are connection mode (GSM/GPRS, WCDMA, LTE),
connection submode (e.g. EDGE, WCDMA, HSPA+), sig-
nal strength (RSSI) and signal to noise ratio (Ec/Io), RRC
state and camping network operator. In addition, we also
record when a connection comes up or disappears, i.e., when
the PDP context is established or lost. As will be shown in
the sequel, results can be very diﬀerent depending on the
network state.
All in all, our dataset consists of 10.1 billion entries in
the database, gathered from 938 distinct connections at 341
distinct nodes. 327 of these are Telenor connections, 142
are Netcom, 75 are Tele2, 66 are Network Norway, and 328
are Ice2. The number of simultaneously active measurement
nodes has varied in the range between 108 and 253 through
the measurement period.
4. CONNECTION RELIABILITY
Data can only be sent over an MBB connection when there
is an established PDP context in the CN. To establish a
PDP context, the UE signals its presence to the respective
signaling gateway (SGSN in UMTS networks), which then
establishes the PDP context and returns a data session with
an allocated IP address. This data session is essentially a
tunnel connecting the UE to the Internet through interme-
diate gateways (GGSN in UMTS networks). The PDP con-
text can be broken either by problems in the RAN (e.g.,
poor signal quality), or in the CN (e.g., failures or capacity
problems in the SGSN). Failures can also be caused by the
complex interaction between the OS running on the mea-
surement node, the node’s USB subsystem, and the MBB
Figure 4: Placement of NNE nodes in Norway.
The NNE backend contains the server-side of the measure-
ments, and is connected directly to the Norwegian research
network UNINETT. The backend also contains servers for
monitoring and managing the nodes, and for data process-
ing.
3.2 Measured MBB networks
Nodes in the NNE platform are connected to up to ﬁve
MBB networks. Four of these (Telenor, Netcom, Tele2 and
Network Norway) are UMTS networks, while the last (Ice) is
a CDMA2000 network operating in the 450 MHz frequency
band. As shown in Fig. 5, Telenor and Netcom maintain
their own nation-wide RAN. Tele2 and Network Norway are
collaboratively building a third RAN, called Mobile Nor-
way, which does not yet have nation-wide coverage. When
outside of their home network, Tele2 customers camp on
Netcom’s RAN, while Network Norway customers camp on
Telenor’s RAN. This complex relation between the opera-
tors and RANs is an advantage for our measurement study.
By looking at correlations between connections on the same
RAN but in diﬀerent operators (or vice versa), we can often
determine whether an observed behavior is caused by the
RAN or the CN.
3.3 Measurement experiments and data
The measurement experiments performed as part of this
work are installed on the nodes using NNE’s conﬁguration
2The varying number of connections per operator is caused
by practical and economical constraints.
   TelenorMobile Norway     Netcom Ice IceTelenorNetwork NorwayTele2NetcomRadio access network (RAN)Operator(core network)48USB modem itself. We conjuncture, however, that if the
majority of failures are caused by such artifacts, the diﬀer-
ences between operators would be minor and hard to spot.
In this section, we measure the frequency of PDP context
losses, the time it takes before the PDP context is success-
fully restored, and the resulting downtime when no PDP
context is available. We further correlate with signal qual-
ity and connection mode to gain an insight into what may
have triggered the failure. The discussion in this section is
limited to the UMTS networks, since we do not have the
necessary logs from the CDMA2000 network.
4.1 Measuring connection failures
An NNE node continuously monitors the status of the
PDP context for all UMTS connections, and tries to re-
establish it as soon as it is broken. If it fails in doing that,
the node keeps retrying until it eventually succeeds; we log
all these attempts. There is no hold time between these con-
secutive reconnection attempts, so a new attempt is imme-
diately initiated after the failure of the preceding attempt.
A failure will therefore trigger a varying number of recon-
nection attempts depending on its duration (each attempt
takes tens of milliseconds).
In some cases, the node manages to re-establish the PDP
context for a short period, before it is again broken. To
build a time series of failure events, we group consecutive
reconnection attempts spaced by less than M minutes into
the same event.
In other words, a connection must keep
its PDP context for at least M minutes before the recon-
nection was deemed successful and the failure event ends.
Setting M to a high value underestimates the connection
stability, while a low value will report a ﬂapping connection
at partially available. We experiment with diﬀerent values
for M in the range from 1 to 5 minutes. We detect a to-
tal of 154772 failures when setting M to 1 minute. Varying
M from 1 minute to 3 minutes has a negligible impact on
the number of detected failures. This number only drops
by 0.019% when we set M to 3 minutes. It, however, de-
creases by 3% and 4.9% when we set M to 4 minutes and
5 minutes respectively. Based on this, we set M to 3 min-
utes when identifying PDP context failures. We believe that
this grouping captures well what the user perceives as a us-
able connection, since a connection is not worth much if it
ﬂaps at a high frequency. The result of this grouping is a
sequence of connection failure events of varying duration for
each connection.
We impose two conditions to avoid overestimating the
number and duration of connection failures by including
measurement artifacts. First, we discard all failure events
that were rectiﬁed either by rebooting the node or actively
resetting the USB modem, since these may be caused by ef-
fects in the measurement platform. Second, to compensate
for absent log ﬁles and failures that are not rectiﬁed by the
end of our study period3, we consider only failures that have
well deﬁned starting and ending points.
4.2 Analyzing connection failures
The stability of the tunnel that connects the UE to the CN
depends largely on the RAN. Hence, to capture the eﬀect of
the radio access, we group our connections based on their
respective RANs. Recall that, the measured four UMTS op-
3In some cases, measurement nodes were lost for varying
periods of time which resulted in gaps in the logged data.
erators use three RANs as illustrated in Fig. 5. This gives
us ﬁve logical networks in total, which are Telenor, Netcom,
Mobile Norway (which includes Network Norway and Tele2
connections that use Mobile Norway’s RAN), Network Nor-
way@Telenor (which includes Network Norway connections
that camp on Telenor’s RAN), and ﬁnally Tele2@Netcom
(which includes Tele2 connections that camp on Netcom’s
RAN). We use the camping information we collect from the
modems to identify the connections that belong to the last
three logical networks. For example, we classify a Network
Norway connection as Network Norway@Telenor if it spends
more than half of the time camping on Telenor’s RAN, oth-
erwise we classify it as Mobile Norway.
The three plots in Fig. 6 show the cumulative distribu-
tion function of the mean time between failures (MTBF),
the mean time to restore (MTTR), and downtime percent-
age (due to PDP failures) for each connection in our data
set, grouped by the ﬁve logical networks. We record distinct
diﬀerences between operators, and observe a strong depen-
dency between connection stability and the RAN. The statis-
tics of Telenor connections and Network Norway@Telenor
connections resemble each other. The same is true for Net-
com connections and Tele2@Netcom connections. Although
Mobile Norway is Network Norway’s home RAN, the statis-
tics of Network Norway@Telenor clearly diﬀers from Mobile
Norway’s. The same is true for Tele2 and Mobile Norway,
albeit to a lesser extent. This conﬁrms the dominating role
of the RAN in determining connection stability.
Diﬀerences between operators. Telenor and Network
Norway@Telenor connections are less stable compared to
the other three operators. About half of Telenor and Net-
work Norway@Telenor connections fail at least once every
day. For the other three operators this is the case for be-
tween one fourth (Mobile Norway) to one third of connec-
tions (Tele2@Netcom and Netcom). Telenor and Network
Norway@Telenor, however, have much shorter MTTR com-
pared to the other networks. Only 19% and 20% of Telenor
and Network Norway@Telenor connections respectively have
MTTR more than ﬁve minutes. The same numbers jump
to 54% for Mobile Norway, 57% for Netcom, and 64% for
Tele2@ Netcom. These diﬀerences suggest that the MTTR
values for Netcom, Tele2@Netcom and Mobile Norway con-
nections are inﬂuenced by a set of long lasting failures. To
investigate whether these failures are the main factor behind
the observed diﬀerences, we compute the median time to re-
pair for all connections. While the median values are natu-
rally smaller than the mean, the diﬀerences between opera-
tors remain consistent. For example, less than 9% of Telenor
connections have a median time to repair longer than one
minute compared to 33% for Netcom. Note that there are
also slight diﬀerences, especially in the MTTR, between Net-
work Norway@Telenor and Telenor. These diﬀerences can
be attributed to the fact that many Network Norway@Tele-
nor connections, though mainly camping on Telenor’s RAN,
spend some time camping on their home network as well.
There are similar diﬀerences between Tele2@Netcom and
Netcom but less pronounced. To check whether the observed
diﬀerence between operators stem from varying coverage lev-
els we measure the average RSSI for all connections. Fig-
ure 7 shows the CDF of mean RSSI for each connection in
all operators. All curves collapse onto each other indicating
no systematic diﬀerences between operators. The same is
true also for Ec/Io (not shown here).
49Figure 6: The statistics of connection failures.
Failure properties. Telenor and Network Norway@Tele-
nor are dominated by frequent but short-lived failures com-
pared to the other three networks. About half of Telenor
and Network Norway@Telenor connections have MTTR less
than 17 seconds and 90 seconds respectively. Looking closer
at these short failures, we ﬁnd that they are related to the
RRC state of the connection, and they happen when the con-
nection fails to be promoted from a shared channel (CELL -
FACH) to a dedicated channel (CELL DCH). This triggers
the modem to reset the connection. As we demonstrate in
Sec. 6, these short failures can have a drastic impact on
applications performance. Netcom and Tele2@Netcom, on