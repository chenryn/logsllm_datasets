为3分钟，给各类场景带来极大的诊断效率提升。针对已经确定的两个点的IP，我
们自动定义出所对应的IP拓扑是怎样一种结构；对相应拓扑链路上的所有日志进行
实时提取、标注关键词；对可疑设备的告警进行自动化聚合收敛、过滤无效信息；主
动对可疑设备进行可疑探测、做二次分析。这些过程几乎是一键完成。
新基础  9年双11：互联网技术超级工程
基于NetO做流量最优化的分配
通过最优化流量分配来榨干多余带宽成本，同时满足最优路径选择、带宽扩容、
稳定性方面的现实需求。
技术层面。我们希望每次网络路径都是最优的。传统网络基本基于Metric机
●
制确定最短路径。对于阿里这张具有多样链路的网络，交易链路对网络的延
迟极其敏感，大数据需要很大的带宽，需要更多可达路径帮助快速进行数据
的传输。
带宽扩容角度。在面临非常频繁的带宽扩容需求情况下，实际的定时链路存在
●
很多延时差异，两个点之间的路径带宽差异也很明显，我们需要站在运营的角
度构建某种方法，既能充分利用闲置的带宽，又能在调配流量过程中很好的兼
顾时延和成本。
稳定性方面。并行的链路在出现单点故障时，需要对其进行隔离，隔离后如何
●
触发高可用路由决策。这些都是NetO需要解决的问题。NetO基于SDN采
用了SR-TE技术，帮助我们在全局情况下拿到全网流量信息、路由状态信
息，用这些信息帮助我们按场景进行路径转发。
新基础  9年双11：互联网技术超级工程
这里给大家举三种常见的场景，黑色线条代表物理链路，其他颜色线条代表逻辑
链路。
故障状态下的负载均衡
从第一个场景的图中可以看到三条链路在初始状态下进行数据的通信。通信链路
出现单点故障时， NetO会把蓝色链路的流量动态的分配到其他两条链路上去。
针对高费用链路的解决措施
从实际角度出发，每条链路意味着不同的资费，为了节省成本，提高资源利用
率，我们完全可以采取灵活的策略来运行。如下图所示，我们在运行过程中发现其中
一条链路的成本偏高，这时NetO会自动触发一次调用，把流量分配到相对来说成本
较低的链路上，这个过程基本不需要人工的干预。
新基础  9年双11：互联网技术超级工程
从 10% 到 40%：阿里巴巴混部技术权威详解
潇谦
背景引言
每年双十一创造奇迹的背后，是巨大的成本投入。为了完成对流量峰值的支撑，
我们需要大量的计算资源，而在平时，这些资源往往又是空闲的。另一方面，为了在
极端情况下，如机房整体断电等还能保障阿里巴巴的业务不受损失，也需要在全国
各地建立冗余资源。而且就算是一天当中，在线服务的负载也是不一样的，白天一
般情况下要比凌晨高得多。根据盖特纳和麦肯锡前几年的调研数据，全球的服务器的
CPU利用率只有6%到12%。即使通过虚拟化技术优化，利用率还是只有7%－
17%，而阿里巴巴的在线服务整体日均利用率也在10%左右。
另一方面，全球从IT时代全面走向了DT时代，现在又在向更深入的AI时代迈
进。各各样的大数据处理框架不断涌现，从Hadoop到Spark，从Jstorm到Flink，
甚至包括深度学习框架Tensorflow的出现，成千上万的数据分析背后是大量的计算
任务，占用了大量的计算资源。由于计算任务占用的计算量很高，CPU水位通常在
新基础  9年双11：互联网技术超级工程
把集群混合起来，将不同类型的任务调度到相同的物理资源上，通过调度，资源
隔离等控制手段,在保障SLO的基础上，充分使用资源能力，极大降低成本，我们
称这样的技术为混部（Co-loaction）
打个比方，跑在容器里的在线服务就像石块；而计算任务我们把它比喻成沙子和水。
当在线压力小的时候，计算任务就占住那些空隙，把空闲的资源都使用起来，而当在线
忙的时候，计算任务就立即退出那些空隙，把资源还给在线业务。这样的技术一方面在
平时，我们可以极大地提升资源的利用率；另一方面，在大促活动需要突增在线服务器
的时候，又可以通过在线业务占用计算任务资源的方式，来顶住那短暂的峰值压力。
从原理中我们可以看到可以混部在一起的任务有两个比较重要的特征：
1. 可以划分优先级：一定需要优先级比较低的任务，它们能像水和沙子一样，随
时能被赶走，而不会受到不可承受的影响，让优先级高的任务不受干扰。在
线的特点是：峰值压力时间不长，对延时比较敏感，业务的压力抖动比较厉
害，典型的如早上10点的聚划算活动，就会在非常短的时间内，造成交易集
群的压力瞬间上升10几倍，对于稳定的要求非常高，在混部的时候，必须要
保证在线的通畅，需要有极强的抗干扰能力。而计算任务的特点是：平时的压
力比较高，相对来说计算量可控，并且延迟不敏感，失败后也可以重跑。至
新基础  X*R2 = N*R2 – N*R1
=> X = N*(R2-R1)/R2
也就是说如果企业有10万台服务器，利用率从28%提升到40%，代入上述
公式，就能节省出3万台机器。假设一台机器的成本为2万元，那么节约成本就有
6个亿。
154 > 9年双11：互联网技术超级工程
2015年，Google发表了Borg论文，其中就提到了在线服务与计算任务之间
的混合运行，也就是我们说的混部技术。Borg论文中描述了Google由于采用了这
项技术，为Google节省了20%-30%的机器规模
混部技术的历程
阿里巴巴早期混合云架构
大家都知道这今年阿里巴巴双十一的交易峰值是每秒32.5万比，相比去年几乎
增加了1倍，但是这样的高峰却只有1小时左右。为了让交易的成本降低，从2014
年开始，我们一方面通过阿里云的公有弹性云资源降低成本，另一方面也开始研究混
部相关的技术。
混部能产生这么大的帮助，可是业界能使用在生产的没有几家公司，其原因也非
常简单，第一个是规模，第二个是技术门槛。当你机器规模不够大的时候，显然意义
不大。而在技术上，计算型任务通常都可以把利用率跑到很高，如果计算型任务和在
线型业务运行在同一台机器上，怎么避免计算型任务的运行不会对在线型业务的响应
时间等关键指标不产生太大的影响呢，这个需要在技术上有全方位的突破，而阿里巴
巴从无到有，花了4年多的时间才让这项技术在电商域得以大规模落地。
新基础 < 155
1. 2014年，我们最主要的工作是进行技术论证，方案设计，以及相关的一些实
验性研究
2. 2015年，我们开始了日常测试环境的测试工作。这一期间让我们总结了相当
多的问题：如调度融合问题、资源争抢隔离问题、存储依赖问题、内存不足问
题等等。
3. 2016年，当我们把大部分问题都解决掉时，我们开启了线上200台左右的
小规模验证。由于电商的金融属性，对于抗干扰要求特别高，在不断的业务
考验下，我们不停地修正着技术方案。
4. 2017年，经过一年的磨合，混部的整体技术终于走向了成熟和大规模生产。
阿巴巴双十一当中，约有1/5的流量是跑在混部集群之上的。