•
Proﬁle generalization [43], [49]: Proﬁle items are
generalized to a coarser granularity (e.g., a URL is
generalized to its category). The server cannot distin-
guish between users with the same generalized proﬁle,
even if their original proﬁles are different. The idea
has been used in other applications as well, such as
cloaking a user’s location with a cloaked region to
achieve location privacy [3], [26].
Noise addition [5], [28]: Fake proﬁle items, called
dummies, are added to and some original proﬁle items
are taken away from the proﬁle. With a large number
of fake items independently added to the proﬁle each
time it is sent to the server, two noisy proﬁles from
the same client look different, making it difﬁcult for
the server to link them.
An important design question is:
• What obfuscation technique is more suitable for
privacy-preserving personalization of web search?
Existing systems use these different techniques for evaluat-
ing either personalization or privacy. For example, RePriv [20],
a privacy-focused system, uses generalized proﬁles and as-
sumes that
they can be safely shared with servers to en-
sure some form of anonymity. Personalization-focused sys-
tems [18], on the other hand, show that URLs without any
generalization yield a better personalization. We systematically
evaluate these techniques to understand their tradeoffs between
privacy and personalization.
Our Results. We show that noise addition provides a better
privacy-personalization tradeoff than generalization. We show
that anonymity provided by generalized proﬁles does not
naturally translate into unlinkability over time. In general, we
3
show that a noisy proﬁle can provide a similar level of unlink-
ability as a generalized proﬁle, but with better personalization
(or similar personalization with better unlinkability). This is
counter-intuitive since noise, by deﬁnition, negatively affects
personalization. However, the negative effect is offset by ﬁner
granularity of proﬁle items (than generalized proﬁle items),
resulting in a net positive improvement in personalization.
The cost of noise. Even though a noisy proﬁle has its
advantages over a generalized proﬁle, they do not come for
free. There are two key disadvantages. First, if many fake
items must be added to the proﬁle to ensure reasonable
unlinkability, the noisy proﬁle can be very large. Since the
noisy proﬁle is sent to the server often, possibly with each
request, the communication overhead can be too much for
energy-constrained devices like smartphones. Second, the fake
items need to be picked from an unbiased sample of the items
in the proﬁles of all users in the system. If the sample from
which the client chooses fake items is biased (e.g., all items are
related to football) and if the bias is known to the server, it can
easily ﬁlter the noise out to identify the real items. Thus, the
client needs to ﬁnd a trusted third party who would compute
an unbiased sample for him. This is a strong dependence. The
sample also needs to be updated as users join and leave the
system, as new proﬁle items appear or as items’ popularity
changes.
This leads us to investigate the following:
• How big a dictionary and how much noise are
required to achieve reasonable unlinkability?
Our results. We show that both types of cost due to noise
addition are non-negligible. More speciﬁcally, the size of the
noisy proﬁle that needs to accompany each client request can
be in the order of tens of kB, much larger than actual requests
and responses. The overhead is signiﬁcant even if the noisy
proﬁle is compressed (see §V-B).
Efﬁcient noisy proﬁle. The high costs of noisy proﬁles can
make them impractical. Moreover, the requirement of a noise
dictionary constitutes an additional threat because a malicious
server may supply biased dictionaries that make the noise more
predictable. The costs and additional threats of dictionaries
lead us to the ﬁnal question that we investigate in this paper:
Is it possible to receive the advantages of noisy
proﬁles without incurring the aforementioned costs
(i.e., noise dictionary and large communication
overhead)?
•
Our results. As a key contribution of the paper, we propose
Bloom cookies that afﬁrmatively answer the above question to
enable a practical noise addition technique for web search. In
particular, we show that Bloom cookies can achieve compara-
ble personalization and unlinkability to a noisy proﬁle, without
requiring a noise dictionary and with an order of magnitude
smaller communication overhead. We describe our solution in
§V.
Note that
the research questions above are in no way
exhaustive, but they are some of the key questions we faced
while building our system. In §IV, we answer these questions
with an experimental methodology that we describe in the next
section.
4
III. EVALUATION METHODOLOGY
Our evaluation is based on search logs of a popular search
engine from May and June 2013. Each entry in the search
logs contains ﬁve ﬁelds: a unique user ID4, the search query
submitted by the user, timestamp, the top-10 search results
shown to the user, and the results that were clicked by the
user including the timestamp of each click. Each search result
consists of a URL and top-3 (ﬁrst or second level) ODP [1]5
categories for the web page at the URL. We replay these logs to
simulate a scenario where users query a search engine, share
their proﬁle with the search engine to receive personalized
results, and their IP addresses change once every two weeks
(i.e., IP-session length is two weeks).
A. Personalization strategies and metric
The state-of-the-art in web search personalization uses two
main techniques for building user proﬁles from search logs:
ﬁne-grained URL-based [18] and coarse-grained interest-
based [12], [21], [30], [37], [44] proﬁling. As their names
suggest, URL-based proﬁles include URLs that users visit most
often, while interest-based proﬁles include models of users’
interests mined from users’ past behavior. We implemented
both techniques. To build URL-based proﬁles, for each search
session in the user’s search log where at least one of the
search results was clicked, we extract the satisﬁed click [6], a
click followed by a period of inactivity. We then extract the
corresponding clicked URLs and assemble the user proﬁle as
a list of domain names (and not the full URLs), ordered by
recurrence in the search log. To build interest-based proﬁles,
we ﬁrst
label each query in the user’s search log with a
category. The category of a query is determined as the most
common ODP category of top-10 search results of the query.
Higher weights (e.g., by default double weight) are assigned
to the ODP categories of the clicked results for a certain
query. The interest proﬁle of the user is then constructed
as a distribution of ODP categories across all queries in the
available search history for the user.
Once proﬁles are built, they are used for ranking search
results. Speciﬁcally, for a given search query, we assign a
score to each of the top M search results (M = 50 in our tests)
returned for the query (note that these results are provided by
the search back-end before personalization is applied, more
on this later). If the domain (or any of the ODP categories)
of the search result is present in the user’s URL (or interest)
proﬁle, the search result receives a score of α∗M, where α is a
parameter ranging from 0 to 1 that controls the aggressiveness
of personalization. The larger α, the more aggressive the re-
ranking (we use α = 0.25). If the domain (or the ODP
category) is not present, the score is 0. We then re-rank the
results based on the score.
To evaluate personalization, we leverage user clicks
recorded in the search logs. The key insight of this method-
ology (proposed in [18] and later widely adopted, e.g., [41])
4These IDs are typically established using IP address, cookies and search
toolbars.
5The Open Directory Project (ODP) classiﬁes a portion of the web according
to a hierarchical taxonomy with several thousand topics, with speciﬁcity
increasing towards the leaf nodes of the corresponding tree. Web pages are
classiﬁed using the most general two levels of the taxonomy, which account
for 220 topics.
is that if a personalization algorithm is able to rank “relevant”
results (i.e., those that were clicked) at the top, the user will
be more satisﬁed with the search. Hence, clicking decisions
are used as a relevance metric to quantify the personalization
improvements.
As in other such studies [18], [42], we measure the quality
of personalization by average rank, deﬁned as
Avg ranki =
1
|Rc
i|
rankr
(1)
(cid:88)
r∈Rc
i
is the set of results clicked for a given query
where Rc
i
i, and rankr is the rank of the result r assigned by the
personalization algorithm. The smaller the average rank, the
higher the personalization quality.
In evaluating personalization, the optimal case is the per-
sonalization quality provided by today’s search engines to
users who decide to sign in, and allow the search engine to
collect their search history over the long term. This case is pro-
vided by our search logs. However, to test personalization, we
also need a set of non-personalized results to be re-ranked by
our personalization algorithms. We download from a separate
source of the same production system where personalization is
turned off (i.e., no user history is provided), the top-50 results
and associated top-3 ODP categories for all queries contained
in our search logs.6 Then, for each query we compute two
types of average rank: i) the average rank of the ideal case,
avg rankideal, which is extracted directly from the search
logs; and ii) the average rank of the personalization algorithm
test under study, avg ranktest. We then compute the absolute
difference between avg rankideal and avg ranktest (i.e., if
the difference is negative, it means the production system’s
avg rank is smaller, which means better personalization).
Note that
in our ﬁrst comparison of URL-based and
interest-based personalization presented in §IV-A we report
the absolute drop in personalization quality compared to
avg rankideal;
later on we re-deﬁne our baseline as the
avg rankU RL, our implementation of URL-based personal-
ization, and report the percentage decrease compared to that.
B. Privacy strategies and metrics
As described in §II-C, interest-based proﬁling is a form of
proﬁle generalization for privacy preservation. To represent the
state-of-the-art of noise addition techniques, we implemented
two techniques: RAND and HYBRID. Both these techniques
work by introducing fake proﬁle items (i.e., URLs) in the real
user proﬁle. The noise level is controlled by the parameter f,
which represents the number of fake proﬁle items added for
each real proﬁle item.7 Such algorithms assume a dictionary D
which contains URLs and top-3 ODP categories associated to
each URL. RAND represents a na¨ıve noise addition technique,
which simply draws fake URLs randomly from D. HYBRID is
a more advanced technique inspired by [31], which draws fake
URLs randomly from a user-speciﬁc dictionary, called uD,
6As our search logs are for May-June, to ensure coverage of the results, we
downloaded the data in the month of July. Queries whose clicked results were
not included in the top-50 non-personalized results were eliminated from the
test set.
f = 10 will have 11 ∗ k items.
7For example, if the original proﬁle has k items, the noisy proﬁle with
computed by eliminating from D all URLs that do not have
any ODP category matching the user’s interests (which are
also expressed as ODP categories). The advantage of HYBRID
over RAND is that if a malicious server is able to infer a user’s
interests (e.g., from search keywords), it cannot simply discard
(fake) URLs that do not match the user’s interests.
As mentioned before, we use unlinkability as our privacy
measure. We use two metrics of unlinkability.
1) Entropy-based unlinkability: We start from the formal
deﬁnition of unlinkability given in [19], that measures the
degree of unlinkability of a set of elements as entropy. A
partition of the set of elements (meaning a division of the set as
a union of non-overlapping and non-empty subsets) represents
a possible way to “link” all elements in the set to each other
(e.g., given a set of 4 elements, 15 partitions exist). In our
context, “linking” means identifying user proﬁles collected in
different contexts (e.g., different time periods) that belong to
the same user. The unlinkability of the elements in the set is
measured as entropy8
H(X) = −(cid:88)
x∈X
p(x) log2 p(x)
where X denotes the set of possible partitions and p(x) is
the probability mass function, 0 ≤ p(x) ≤ 1, denoting the
probability that x is the correct partition.
Without any additional
information, a priori, all parti-
tions are equally possible so the probability distribution is
uniform and the entropy of the elements is at its maximum
(Hpriori(X) = −log2(1/m)). However, an adversary with ac-
cess to some information about the partitions can, a posteriori,
rule out some candidate partitions, thus lowering the entropy.
In our context, a malicious server can observe the content
of the user proﬁles and assign higher probabilities to certain
partitions. According to [19], the degree of unlinkability of the
set of elements against an adversary is therefore deﬁned as the
ratio between the a posteriori entropy to the a priori entropy:
U (X) =
Hposteriori(X)
Hpriori(X)
Unfortunately, this deﬁnition does not scale to a large set,
as enumerating all possible partitions is a computationally hard
problem. Therefore, we make some simplifying assumptions.
First, we assume that we have a constant number of users in
the system over time, and a user whose proﬁle is seen in the
time period i (where the time period is a ﬁxed length of time of
the order of a few weeks) will have a proﬁle also in the time
period i + 1. Second, we assume that historical information
about some users that interacted with the system is available
(this allows for training of a linkability model that a potential
adversary may build, see below). Third, instead of computing
all possible partitions to calculate the system unlinkability, we
compute “per-user unlinkability” by comparing a user’s proﬁle
in time-period i with all the other proﬁles in time-period i + 1,
8Information entropy is a well-known metric that measures the level of
uncertainty associated with a random process. It quantiﬁes the information
contained in a message, usually in bits/symbol. In this setting, entropy
measures the information contained in the probability distribution assigned
to the set of possible partitions of the set of elements.
5
independently of the other users in the system, as described in
details as follows.
The process consists of two steps. In the ﬁrst step, we
build a linkability model from the search logs of n users over
a period T = T 1+T 2 (T = 1 month).9 For each of the n users
we create two proﬁles, one from the ﬁrst time period T 1 and
one from the next time period T 2. Next, to measure proﬁle
similarity we calculate the Jaccard similarity10 between the n2
possible pairs of proﬁles, where the ﬁrst proﬁle comes from
the set of T 1 proﬁles and the second comes from the set of
T 2 proﬁles. Using the ground truth available in the users’ logs
(i.e., the information of which T 1 and T 2 proﬁle belong to the
same user), we train a linkability model, deﬁned as a function
that maps the Jaccard similarity of a pair of proﬁles into the
probability of these two proﬁles belonging to the same user
(see Appendix VIII for an example of linkability function).
In the second step, we compute the unlinkability of a user’s
proﬁle by calculating the a priori and a posteriori entropy.
Given a set of m users, where each user has two proﬁles
computed over two consecutive (possibly overlapping) time
periods P 1 and P 2, we apply the linkability model to compute
the probability of a particular proﬁle from P 1 being linked to
a proﬁle in P 2, (i.e., belonging to the same user). Note that
P 1 and P 2 are different time periods from T 1 and T 2 above,
but of the same length.11 Without any information about any
user, the probability of a particular proﬁle pP 1
being linked
to another proﬁle pP 2
is 1/m, hence, the a priori entropy is