performance and a low false positive rate of 2−16/τ .
Your protocol does not defend against unique weak passwords
Yes, but the main goal is to protect against large scale attacks. Exploiting unique weak password
requires a targeted attack against a speciﬁc user. See Section 1.1 for more details.
What is the main diﬀerence between the QR and the garbled circuits based solutions?
See Section 9 for the tradeoﬀ between the two solutions.
Does using a new r value for each password protect against the leakage that leads to
the “Malicious Campaign” scenario?
Unfortunately no. If there is any leakage on the result of the protocol, the attacker can just run
the protocol multiple times until it gets the required results. On the other hand, keeping r ﬁxed
allows for a simpler and safer protocol (see Section 6.4).
How do you set the threshold for τ for popular passwords
The threshold τ is chosen dynamically to be the smallest possible value satisfying the required
error probability pF N , with the current number of users NC. See Section 4.3 for more details.
26
How many of the top passwords can you blacklist?
Our theoretical analysis for an arbitrary distribution guarantees only the blacklisting of a few
very popular passwords. However, the Zipﬁan nature of the password distribution means that we
only care about blacklisting the top passwords as they are much more valuable to an attacker.
Moreover, using simulation on real-world data we show that our scheme performs closely to
that ideal blacklisting process even when blacklisting the top 25 passwords. See Section 1.4 and
Section 7.2 for more details.
Can someone modify the blacklist and create a DoS type of attack?
We assume that the published blacklist is authenticated (e.g. in an https protected website). As
the list is publicly available it is easy to make sure all devices receive the same blacklist, and
the device can reject the blacklist if its size is unreasonably large (e.g. more than some fraction
of the domain), see Section 3.4.
B Bounds for the “Malicious Campaign” Setting
In this section we will bound pM alCamF N , the false negative probability in the “Malicious Cam-
paign”setting. We also show how the server can dynamically control the scheme’s parameters to
achieve a target pM alCamF N .
We therefore discuss in this section how the server can add noise to the published list of popular
hash values, in order to reduce the probability of a successful undercount attack. Note that we
assume that the rs value is reused between diﬀerent runs of the protocol with the same device, see
Section D.1.
B.1 “Malicious Campaign”Setting Information Leakage
We consider the worst case scenario, where all of the devices are malicious and try to collude to
learn information about the diﬀerent secret rs
j values used by the server. We want to bound the
amount of bits leaked on a single publication of the statistics. As all of the devices are colluding, all
the chosen hash values are known (the devices know NH (v) for every v), and so all the information
leaked is on the rj values.
A hash value v is added to the published list if 1(α(v) − τ NC) = 1. The maximum entropy for
a single bin happens when E(α(v) = τ NC) and then the entropy is H (1(α(v) − τ NC)) = 1. As the
malicious devices do not have to randomize their responses, this happens when exactly τ NC(1− r)
devices choose v. Due to our secure inner product protocol the devices cannot control the part of
α(v) that is a binomial distribution:
χB ∼ bin(n = NC(1 − τ (1−r)), p = 1/2) ≈ bin(n = NC, p = 1/2)
α(v) ≈ τ NC + 2(χB − NC/2)/(1 − r)
The devices control all of the chosen v values, but χB is randomized due to the secret rj vectors.
Any bit of information on the value of χB can be translated to information on rj. We will like to
bound the amount of information leaked.
B.2 The Eﬀect of the Laplacian Noise on False and Positive Negatives
We consider the addition of Laplacian noise with 1/n = τ NCδ/C where C is a small constant (e.g.
2). In that case we can view the noisy approximation αN as:
αN (v) = α(v) + χL = NH (v) + χB + χL
27
where χB is the binomial noise due to the devices that did not choose v and the randomized
response, and χL is the added Laplacian noise.
The ephemeral Laplacian noise added by the server can cause false and positive negatives, with a
relatively low but non-negligible probability. By choosing a threshold value τ > τmin we get smaller
binomial noise compared to δτ NC, and allow for the extra noise. Moreover, the server generates
new ephemeral noise for each publication in the hash list. So even if a false negative will happen
with low probability, the expected number of such events is small and they will have a small eﬀect
on the users. For example if once every 10 weeks a popular password will not appear on the list,
not many new users will choose it. In contrast, a false negative event caused by the binomial noise
that was described before will be maintained in all the next publications.
B.3 Bounding the Information Leakage
As we have demonstrated, all bins with NH (v) > τ NC(1 + δ) have negligible probability of not
being published. Therefore, the fact that they are in the list leaks no information. The mirror case
is with regards to all bins such that NH (v) < τ NC(1 − δ). The fact that they are not in the list
also leaks no information. We get that a bin can only leak information if:
τ NC(1 − δ) < NH (v) < τ NC(1 + δ)
In the “Malicious Campaign”setting, the devices do not have to randomize their response and the
maximum number of such bins is:
1/(τ (1 − δ)(1 − r))
As the colluding devices know NH (v), the only information they can gather is on the value of
χB, which is the sum of binomial noises. To bound the leakage we want to bound the mutual
information.
I(χB; 1(αN (v)− τ NC)) = H (1(αN (v) − τ NC))
− H (1(αN (v) − τ NC)|χB)
≤ 1 − H (1(αN (v) − τ NC)|χB)
= 1 − H (1(χB + χL)|χB)
We deﬁne b(t) as the maximum possible number of bits leaked at time t with the current values of
τ (t):
b(t) =
(1 − H (1(χB(t) + χL(t))|χB(t))
τ (t)(1 − δ)(1 − r)
As NC increases we can add larger Laplacian noise χL. As χL gets larger H (1(χB(t)+χL(t))|χB(t))
tends to 1 and b(t) tends to zero.
B.4 Bounding the Probability of an Undercount Attack
In Lemma 2 we bounded the probability of a false negative pF N in the semi-honest setting. We
can view all the possible choices of hashes by the attacker’s devices as a search space, where the
attacker goal is ﬁnd such a hashes that cause an undercount attack. Without any auxiliary data
on the rj values, an attacker best strategy is to randomly sample from this space, and test if the
undercount attack succeed. This will happen at a probability of pF N .
28
success probability by a factor of 2. We deﬁne B(t) =(cid:80) b(ti) the total bits of information leaked
Every bit of information on rj can help the attacker ignore half the search space or increase the
up to time t. So the attacker can use B(t) to increase is success probability by 2B(t). We get an
upper bound for the new probability of a false negative with malicious devices:
pM alCamF N = pF N · 2B(t) ≤ 2 exp(−NC(τ δ(1 − r))2/2) · 2B(t)
= 2 exp(−NC(τ δ(1 − r))2/2 + B(t) · ln(2))
≈ 2 exp((1.38B(t) − NC(τ δ(1 − r))2)/2)
This Probability is greatly aﬀected by two parameters NC and τ . As NC increases pM alCamF N
decreases exponentially. Moreover, as NC increases we can use larger Laplacian noise, reduce the
data leakage and so B(t). As τ decreases pM alCamF N decreases exponentially. Moreover B(t) ∝
1/τ .
B.5 Dynamic Publication Frequency
Similarly to the analysis in Section 4.3, we want to allow the server to dynamically change the τ
and the publication frequency. We deﬁne LS as the planned lifespan of the system. This might be
the life expectancy of the devices or of the service (e.g. the manufacture can decide that he will
stop support of the web cameras after 10 years).
exp(−C) for some constant C. We do this by deﬁning a dynamic threshold and publication fre-
quency that depend on the amount of previously leaked information B(t), the remaining lifespan
of the system LS − t, and the number of devices that have run the protocol so far6
For correctness in the “Malicious Campaign”setting, we require that for any time t, pM alCamF N ≤
ln(pM alCamF N ) ≈ (1.38L(t) − NC(t)(τ (t)δ(1 − r))2)/2 ≤ −C
NC(t)(τ (t)δ(1 − r))2 − 1.38L(t) ≥ 2C
For any time t, the worst case scenario is that no more devices will choose a hash and NC(LS) =
NC(t). In that case we want to choose a publication frequency f req(t) and threshold τ (t) such that
even at the end of the lifespan of the system the information leaked will be bounded:
L(LS) = L(t) + (LS − t)f req(t)l(t)
2C ≤ NC(LS)(τ (LS)δ(1 − r))2 − 1.38L(LS)
= NC(t)(τ (t)δ(1 − r))2 − 1.38(L(t) + (LS − t)freq(t)l(t))
(4)
(5)
Equation 4 gives an upper bound for the amount of information leaked in the rest of the lifespan if
the statistics are published with frequency freq(t) and threshold τ (t). Equation 5 gives the required
ratio needed to prevent “Malicious Campaign”undercount attacks.
As NC(t) increases over time the server can increase the frequency of publication or decrease
the threshold.
6 Devices can change their hash but not remove themselves from the statistics, so NC (t) is a monotonic increasing
function of time t
29
C Security analysis of the Garbled Circuits Protocol
Security against a malicious device. The device plays the role of the evaluator (receiver) in
the protocol. It is well known that Yao’s protocol is secure against a malicious evaluator [MF06].
The device sends an output to the server, but since the server veriﬁes that this output is a garbled
value of the output wire the device has no option but to evaluate the circuit and send the garbled
output value to the server. Formally, security can be proven in a straightforward way by assuming
that the oblivious transfers are secure and using the simulation proof presented by Lindell and
Pinkas [LP09].
Security against a malicious server. A malicious server can use an arbitrary circuit in the
protocol. The main feature ensuring security against the server is that the device is only willing to
send to the server an output whose hash value matches one of two outputs of the hash function.
Namely, there are only two possible outputs which the device might send to the server.
A selective failure attack. In addition, a malicious server can apply a selective failure attack,
and prepare a corrupt circuit whose evaluation fails for a subset of the inputs of the device. In this
case, the server can identify that the device’s input is in this subset. In total, the server can learn
whether the device’s input is in one of three subsets (the preimages of 0, the preimages of 1, and
the values for which evaluation fails). In other words, the server can learn at most log2 3 = 1.58
bits of information about the password.
We note, however, that this failure of the protocol is an illegitimate outcome. When this happens,
the device can complain about the behavior of the server. (If the protocol further requires the server
to sign the messages that it sends, then the device has a proof that the sender misbehaved).
A simulation argument for the security of the construction works with a slightly modiﬁed
function used in the ideal model, which also considers the selective failure attack. The server’s
j , and a function ˆF , which has three possible outputs: 0, 1,
input to the trusted party is a value rs
or “computation failure”. The simulator receives the server’s input and output. The simulator can
simulate the server’s input to the OTs, and can easily generate the garbled tables. It then simulates
the answer that the server receives based on the output of ˆF given by the trusted party.
D Design Choices
D.1 Reusing r vectors between runs
A malicious adversary can undercount a popular hash and achieve a higher pF N due to information
leaked on the secret r from publishing the blacklist. An obvious approach to avoid this attack is
to use a new r vector for each run of the protocol. However, this only complicates the attack but
does not stop it. On the other hand, using a ﬁxed r helps simplifying the protocol and block any
targeted adaptive attack by the server.
The main idea of the attack, is that a device can use other colluding devices to learn if his vote
is anti correlated or not to the popular hash, without rerunning the protocol. Lets assume that the
device knows that a hash hp will become popular in the future, and whats to start undercounting
it. Colluding devices will vote for hp over time, until they reach the threshold τ . This is done by
checking over time that the probability of the hash to be published is equal to half. After that the
device votes for some value ha (cid:54)= hp and checks if the probability for the hash to appear decreases
- meaning that he is anti correlated. Although this attack seems to be impractical due to large
Laplacian noise added, a more sophisticated attack might still be possible.
30
If we do not reuse the same r, even a semi-honest server might learn extra information about a
password using auxiliary data. Lets assume the user switch back to an old password he used before.
If the server knows that it is the same password, by using diﬀerent r he can possibly learn more
than one bit.
Moreover, in the OR based protocol, the server can save the public rp vector on the device in
production time. This saves us the ﬁrst message in the protocol. In the Non-Interactive version,
the protocol is reduced to sending one message from the device to the server.
E Proofs
We prove here Lemmata 1, 2 and 3 of Section 4.
Lemma 1. If a fraction p of the devices choose a password which hashes to a value v, then the
expected value of α(v) is pNC.
Proof. Let us ﬁrst calculate the expected value of counter T [v] given the number of devices whose
password is hashed to v. The expected contribution of any value vj (cid:54)= v to T [v] is 0 and for vj = v
it is 1. We calculate the expected number of devices that chose the value v. There are three ways
for a device to choose a password pass, and get a hash v.
1. v = H(pass) and the device is not randomizing the response.
2. v (cid:54)= H(pass), the device randomizes the response, and as a result gets the value v.
3. v = H(pass), the device randomizes the response, but gets the same v again.
Therefore,
E(T [v] | NH (v) = pNC) = NC · (p(1 − r + r2−(cid:96)) + (1 − p)r2−(cid:96))
= NC · (p(1 − r) + r2−(cid:96))
(6)
From (3) and (6) we get that the expected value of α(v) is:
E(α(v)|NH (v) = pNC) = pNC
Lemma 2. The probability for a false negative pF N is bounded by pF N ≤ 2 exp(−NC(τ δ(1 − r))2/2).
Proof. The worst case is NH (v) = τ NC(1 + δ)
α(v) can be viewed as the sum of NC independent random variables bounded by the interval
[−1/(1− r), 1/(1− r)] and E(α(v)|NH (v) = pNC) = pNC. Using Hoeﬀding’s inequality (Theorem
2) [Hoe63] we can show that:
pF N = Pr(α(v) ≤ τ NC|NH (v) = τ NC(1+δ))
= Pr(α(v) − τ NC(1 + δ) ≤ −τ NCδ|NH (v) = τ NC(1+δ))
= Pr(α(v) − E(α(v)) ≤ −τ NCδ|NH (v) = τ NC(1 + δ))
≤ Pr(|α(v) − E(α(v))| ≥ τ NCδ|NH (v) = τ NC(1 + δ))
≤ 2 exp(− 2(τ NCδ(1 − r))2
τ (1+δ) bins where NH (v) ≥ τ NC(1+δ) (bins where a false negative can occur).
) = 2 exp(−NC(τ δ(1 − r))2/2)
4NC
There are at most
1
The expected number of FN events NF N is: E(NF N ) ≤ 2 exp(−NC(τ δ(1 − r))2/2) / τ (1 + δ).
31
Lemma 3. The probability for a false positive pF P is bounded by pF P ≤ 2 exp(−NC(τ δ(1 − r))2/2)
Proof. In a similar manner to Equation E we can show that
pF P = Pr(α(v) ≥ τ NC|NH (v) = τ NC(1 − δ))
= Pr(α(v) − τ NC(1 − δ) ≥ τ NCδ|NH (v) = τ NC(1 − δ))
= Pr(α(v) − E(α(v)) ≥ τ NCδ|NH (v) = τ NC(1 − δ))
≤ exp(− 2(τ NCδ(1 − r))2
) = exp(−NC(τ δ(1 − r))2/2)