a color legend, users took considerable time to match the 
respective color to the appropriate value.   In another experiment, 
using the same commercial system and a scatter plot, we plotted 
1358 different network packets.  We exceeded the number of 
categorical colors the system could provide and were forced to 
use a continuous scale.  In this mode, no legend was provided.  It 
proved impossible to identify the feature value from the color.   
4.1.4 
Attacking Motor Resources 
This class of attack attempts to consume time and increase 
frustration by forcing user motor actions.   Attacks may be as 
simple as forcing paging across multiple screens, consider the 
rumint system described in the displacement attack, but add a 
buffer that stores previous pages of images.   As each screen is 
filled, the user must interact with the interface to observe previous 
activity.  Another example is to force user thrashing by requiring 
constant swapping from detail to context and back.  Figures 5 and 
6 illustrate this attack.  The dataset behind these figures comes 
from an unclassified attack/defend exercise, in which a National 
Security Agency red team attacked student-defended networks 
[31].  The user is presented with an overview of network activity 
in Figure 5, but to see the specific port-level the network activity 
in Figure 6 the user must zoom in and then back out to continue to 
monitor the overview.  In this example the user would have to 
perform this operation ten times just to monitor the 1024 
privileged ports on a Unix system. 
4.1.5 
Targeting Specific Humans 
While the attacks described previously in section 4.2 were 
significant, even more effective attacks are possible if the specific 
human user is known.  With this knowledge, an adversary may 
craft an attack that specifically exploits their target’s weaknesses.  
Vision, memory, reflexes, experience and intelligence vary 
greatly between individuals.  Even partial knowledge of the 
specific end user gives the adversary an advantage; their attack 
may be markedly different for a 19-year-old male intern, a 37-
year-old male disgruntled employee or a 58-year-old female 
veteran who has heavily corrected vision.  We believe that some 
degree of knowledge of the human user to be a reasonable 
assumption.  A few casual questions asked at an after-hours happy 
hour frequented by company employees would likely gain useful 
information.  A comprehensive discussion of all such attacks is 
beyond the scope of this work, but we will illustrate the 
vulnerability by examining photosensitive epilepsy.  While this 
condition is relatively rare, it does illustrate the increased risk 
when the attacker can target specific people and their weaknesses.  
We argue that related attacks can be launched when age, gender 
and/or medical details are known about users. 
Extreme 
Information 
Overload 
Attack 
(Photosensitive 
Epilepsy): Epilepsy has a lifetime prevalence of about 3% and 
approximately 2.3 million people in the United States have the 
condition.  Of this population, a percentage has photosensitive 
epilepsy.  People with photosensitive epilepsy are susceptible to 
seizures brought on by rapidly flickering or flashing displays.  In 
the late 1990’s, thousands of people were sickened with nausea 
and dizziness by a Japanese Pokemon cartoon.  In addition, there 
were 685 cases of apparent epileptic seizures [32].    The risk 
extends beyond the viewing of shows on televisions and computer 
monitors.  Video games have also induced seizures and many now 
carry warning labels.   It is important to note that the video game 
and video industries have since taken other proactive measures to 
limit future incidents; reducing the overall incidence of the 
problem.  For example, the Pokemon cartoons were reviewed, 
frame-by-frame, before rebroadcast in the United States [30].  An 
attacker would do the opposite.  Research by Harding indicates 
that the larger the area of the retina covered with the flashing 
display, the greater the likelihood of a seizure.  In particular, 
flashing at the rate of 15-20 times per second was shown to be 
most likely to induce a seizure; 96% of people with photosensitive 
epilepsy were sensitive at this frequency.  In addition to flashing, 
static patterns have induced seizures and the likelihood is 
dramatically increased when patterns are updated at the rate of 15-
20 changes per second [32].  With the trend toward larger displays 
and higher resolution the situation is worsened.  In our 
experiments we were able to generate network traffic that caused 
both static and updating patterns in our network visualization 
system 
that 
would 
possibly 
induce 
seizures 
in 
some 
photosensitive epileptics, but we did not proceed further due to 
safety concerns.   
Figure 6: Autoscale and motor resources attack 
example.  Note the targeted network services, 
originally hidden from view. 
4.2 
 ATTACKING VISUALIZATION HARDWARE AND SOFTWARE 
The attacker affects attacks against the human by influencing how 
information is visualized.  As was the case for humans, the notion 
of specificity is important to consider.  Many of the techniques 
described below are most effective when used against specific 
information visualization systems, but others are broadly 
applicable.   
4.2.1 
Processing Attacks 
Processing attacks target the algorithms used to process and 
present the visualization.  These algorithms range from simple 
graphic routines to advanced artificial intelligence or machine 
learning techniques.  Attacks may be designed to increase 
computational complexity, e.g. creating a large number of objects 
such that the interface becomes sluggish or the visualization 
delays presentation of important information.  Others may exploit 
intelligence embedded in the visualization system.  Consider a 
generic spring layout algorithm.  To be most effective, this 
algorithm relies upon the graph to reach a stable state.  Carefully 
constructed 
packets 
could 
be 
used 
to 
force 
constant 
destabilization. Other attacks may take advantage of bugs in the 
code or the calculations in use, such as interpolation or round-off.  
To provide a concrete example of the efficacy of these classes of 
attack, the following section illustrates the round-off attack in 
detail. 
round-off attack:  Consider the “spinning cube of potential 
doom” visualization system in Figure 7 [33].  Designed to provide 
insight into network attacks, it displays network traffic along three 
axes.  The X-axis represents the destination IP addresses for a 
Class C network (65536 possible addresses), the Y-Axis displays 
destination ports from 0-65535 and the Z-axis displays source 
Internet addresses from 0.0.0.0 - 223.255.255.255 (no multicast).   
Assuming an approximate 1024 pixels for each axis.  The X and 
Y axes round off 6 bits of information, leaving an opening for an 
attacker to operate within a space of 64 indistinguishable 
positions.  More importantly, the Z axis rounds off approximately 
22 bits of information, grouping source IP’s into buckets of over 4 
million each.  Thus an adversary could attack 64 machines on 64 
ports from over 4 million source IP addresses and, due to round 
off, would only illuminate a single pixel.  Note also that the 
visualization is also a target for a color mapping attack.  It uses a 
“rainbow” color map representing TCP connection instances.  
Although a large number of colors are used, the actual color does 
not have “any meaning.”*   
4.2.1.1 
Attacking the Visualization 
The heart of a visualization system are the visualizations it 
presents to the user.  Closely intertwined with processing attacks, 
attacks against the visualization design will have an immediate 
effect on the user.  Some visualizations were simply not designed 
to convey a certain type of activity, so an attacker may easily 
operate with impunity.  In other cases, the design is such that a 
small amount of malicious data can destroy or reduce the 
effectiveness of the system.  Designers are faced with large, 
potentially massive, datasets and limited screen real estate to 
present information and are forced to make design tradeoffs that 
can be exploited.  The following are examples of such attacks. 
* Except gray points which are completed TCP connections. 
(a) jamming 
(b) occlusion 
(c) labeling 
(d) GUI widget 
Figure 8: Representative attacks against the visualization 
Figure 7: View of the “Spinning Cube of Potential 
Doom” a 3-D visualization of network traffic designed 
for security awareness. (round-off attack) 
autoscale attack:  Many visualization systems use autoscaling 
algorithms to provide an optimal display of information.  
Typically the algorithms zoom the perspective outward to display 
the entire dataset.    While this is convenient in many cases, an 
attacker can easily exploit this vulnerability.  The image shown in 
Figure 5 was created by the xmgrace open source information 
visualization tool [34].  A small number of packets sent by the 
attackers to those ports above 40,000, forced the autoscaling 
algorithm to zoom outward, thereby obscuring significant detail 
(Figure 6). 
jamming attack:  The jamming attack is a simple attack, akin to a 
visual denial of service.  By observing what aspects of the dataset 
are used to control the physical location of objects on the screen 
visual noise can be inserted to partially or completely corrupt the 
information display.  As noise is inserted, insightful patterns, 
relationships, anomalies, outliers and trends will disappear.  We 
produced multiple versions of this class of attack in our network 
visualization system by generating network packets with 
appropriate headers. Figure 8(a) is a parallel coordinate plot of 
TCP/UDP ports.  The left axis shows the attacker’s source ports 
and the right axis shows the target machine’s ports (on a 0-65535 
scale).  The image shows two jamming attacks, both done using 
the packit packet creation tool [35].  The first attack generated 
200 UDP packets (in orange) with random source and destination 
ports.  The second attack (in green) generated 2000 TCP packets 
from a single source port, but random destination ports.  On a 
100MB network, packit generated these malicious packets at over 
6600 per second. 
occlusion attack:  Occlusion is a problem in many visualizations, 
particularly those in 3D, but any that plot new information over 
old are susceptible.  An attacker can use this frequent shortcoming 
to hide malicious activity.  In the Figure 8(b), an attacker’s 
malicious activity is hidden behind newer activity. 
labeling attack:  Typically visualizations provide the ability to 
label the image.  Depending on the labeling algorithm in use, this 
fact can be exploited.  One popular commercial visualization 
system defaults to only 20 labels.  If the user does not change this 
setting a large number of objects will not be labeled, greatly 
complicating user interpretation.  See Figure 8(c) for an example.  
At the other end of the spectrum, some labeling algorithms do not 
limit the number of labels used and, by injecting extra data, an 
attack could cause the display to be obscured.   
GUI widget attack:  User interfaces only provide a limited ability 
to interact with the dataset.  An attacker can exploit this limitation 
and prevent users from detecting malicious activity despite their 
best attempts.  Figure 8(d) shows a cluster of network activity; 
because of the large range of values in the overall dataset (not 
shown), the user is unable to zoom in any further.  Any movement 
of the sliders will cause the entire cluster to move off the screen.  
Note the two red circles.  Each circle shows a double-ended slider 
at the closest possible position.   
4.2.2 
Storage Attacks 
From 
our 
research, 
storage 
attacks 
against 
information 
visualization systems can occur primarily in the form of classic 
denial of service.  Denial of information and not denial of service 
is the focus of the paper so we will touch only briefly on it here.  
Every information system has a finite amount of storage.  By 
consuming all or most of this storage an attacker may subvert the 
intent of the visualization system.  In the network security 
domain, a classic example is flooding the network with traffic, 
sometimes legitimate (also known as the slashdot effect) and 
sometimes malicious (trigger logging events to the point that the 
hard disk fills or malicious activity is overwritten).  Variants 
include filling up the buffers of network interface cards such that 
packets are dropped or consuming RAM to the point that the 
operating system needs to page memory to disk (or even thrash).  
All of these attacks negatively impact performance and could 
crash or slow the system.   While not strictly a storage attack, it is 
well documented that, in shared user systems, one user’s 
applications can consume resources to the performance detriment 
of other users. Correctly designed interfaces operate within very 
strict timing parameters and a sluggish interface (or visualization) 
that quickly becomes difficult or unusable could quickly occur. 
4.2.3 
Attacking Data Generation and Communication  
By definition, information visualization systems present data to 
the user in order to provide insight.  If the accuracy, reliability, 
timeliness, completeness or currency is threatened then the entire 
system is at risk.  Attacking data quality early in the system flow 
is a means to an end and not and end unto itself. The tainted data 
will ultimately flow upstream to the visualization system which, 
in turn, will alter the user’s perception and hence negatively 
impact task accomplishment.  Recall that we do not consider data 
corruption attacks as we believe that they will be easily detected.  
We operate with the stricter assumption that an attacker can only 
insert data, and not modify existing data. 
4.2.3.1 
Attacking Data Generation 
In our model, data can come from human and machine producers, 
both of which may prove unreliable despite the best intentions of 
the system designer.  This notion is directly opposite to the 
common assumption that the “source must be good.”  While not 
the focus of this paper, physical attacks are the most 
straightforward attack.  The most basic is physical destruction or 
theft which causes a failure to record data. More subtly, an 
attacker may spuriously add, remove or compromise information 
producing nodes via physical access or network attack.  Consider 
physically turning a sensor on and off (or cutting power) which 
results in selected subsets of data being recorded.  Note that this 
could occur with more than one sensor and provides the attacker 
the ability to paint a customized and comprehensive picture of the 
information space.  Beyond physical access, we consider attacks 
that allow an attacker to operate remotely.   
sensor blindness attack:  Network-based blindness attacks allow 
an attacker to remotely crash selected packet capture sensors on 
the network.  As an example, virtually all Windows-based 
network sniffing programs use the WinPcap [36] packet capture 
library.  Versions of the library have known vulnerabilities that 
will crash the sensor. 
 selective sensor blindness attack:  Similar to the sensor 
blindness attack this variant exploits differing operating system 
implementations of the network processing stack to avoid 
detection.  For example, one operating system may accept a 
packet with an incorrect TCP checksum while another will 
silently ignore it.  This inconsistency allows network intruders to 
operate without detection if the network sensor ignores the packet 
and a target machine accepts it.  For more information see the 
work of Ptacek and Newsham [37]. 
spoofing source identity attack:  Spoofing source identity is 
another common vulnerability, usually due to weak access 
controls or authentication, that allows users or network systems to 
appear as legitimate producers.  In the network domain, it is 
trivially easy to spoof IP packets.  The protocol offers no 
protection and an attacker may transmit packets with spoofed 
source addresses that appear to come from legitimate sources. 
interface spoofing attack:  Interface spoofing attacks have existed 
since the beginning of shared computing systems.  Typically they 
are used to trick legitimate users into revealing sensitive 
information, such as passwords.  In the context of this paper, they 
can be used to trick legitimate users into submitting incorrect data 
to the visualization system.  This technique can be seen when 
employing a variant of current phishing attacks.  An attacker 
could send an email to a legitimate producer asking them to use a 
new website to submit information.  Normal cues from the 
browser, such as the status bar, can be spoofed to prevent 
detection.  See the work of Levy for more detail on this class of 
attack [38]. 
sampling rate attack:  Sampling rate attacks exploit the 
periodicity of data collection.  Due to the high rate of data flow 