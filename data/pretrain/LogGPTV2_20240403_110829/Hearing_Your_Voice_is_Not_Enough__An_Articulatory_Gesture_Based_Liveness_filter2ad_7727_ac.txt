3.3 Doppler Shifts Extraction
Once finish recording, our system first separates the voice sample
of the user (i.e., below 10kHz) for conventional voice authentication.
Then, we rely on the audible voice sample to separate each indi-
vidual phoneme and the corresponding Doppler shifts at around
20kHz. Specifically, we convert the recorded signal from the time
domain to frequency domain by performing Short-Time Fourier
Transform (STFT) with a window size as 250ms. Figure 9 shows
one example of the spectrogram of the recorded signal when a user
speaks "like sweep day". We can find that the audio voice sample is
less than 10kHz and the Doppler shifts are usually within 200Hz at
around 20kHz. Such a large gap ensures the voice sample will not
be affected by the high frequency of 20kHz and its Doppler shifts.
Given the spectrogram of the recorded signal, we aim to extract
the Doppler shifts for each individual phoneme while removing the
pauses due to transaction between phoneme sounds and also the
transaction between words (i.e., the shaded bars in the figure).
To perform phoneme segmentation, we utilize the fact that each
phoneme consists of numerous distinctive overtone pitches, also
known as formants [30]. By inspecting the sound spectrogram,
we are able to identify different phonemes by recognizing those
formants. In particular, the first two formants with the lowest fre-
quencies are referred to as F1 and F2, which contains the most
information can be used to distinguish the vowels. Thus, by an-
alyzing the F1 and F2 in the sound spectrogram, we are able to
segment different vowels within given voice sample. Unlike vowels,
each consonant is displayed as a mixture of various frequencies
randomly. Consequently, only using formants to perform precise
segmentation of consonants is very challenging. We thus utilize
HMM (Hidden Markrov Mddels) based forced alignment to solve
this problem. This method [25] distinguishes different consonants
by comparing the input voice sample spectrogram with existing
spectrograms and finding the best alignment.
Specifically, we first utilize automatic speech recognition (ASR)
to identify each word in the voice sample. We adopt state-of-art
Voice SampleDoppler Shift ExtractionFeatureExtractionSimilarityComparisonDecisionLive UserorAttackUser EnrolledProfileWavelet-basedDenoisingSession A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA62Figure 9: An illustration of Doppler shifts extraction based
on phoneme sounds.
Figure 10: An example of energy sub-band and energy-based
frequency contours.
CMUSphinx [38] to perform such task automatically. After identify-
ing existing words in the voice sample, we then perform consonant
segmentation and labeling utilizing MAUS [26]. First, based on
standard pronunciation model (i.e., SAMPA phonetic alphabet), the
identified words will be transformed into expected pronunciation.
Next, by combining the canonical pronunciation with millions of
possible accents of users, a probabilistic graph will be generated. It
contains all possible phoneme combinations and the corresponding
probabilities. The system then adopts Hidden Markrov Mddel to
perform path search in the graph space and find the combination of
phonetic units with the highest probability. The results of the search
are the segmented and labeled phonemes for each word. Finally,
our system matches the time stamp of each phoneme segmentation
to 20KHz frequency range to extract corresponding Doppler shifts.
One example is shown in Figure 9, which illustrates six seg-
mented phoneme sounds (i.e., [lai],[k],[s],[wi :],[p],[dei]) and the
corresponding Doppler shifts at around 20kHz. We observe that
the phonemes like [lai] and [dei] display more intensive Doppler
shifts than these of the phonemes like [k] and [p]. This is because
when pronouncing [lai], larger movements from multiple artic-
ulators including lips, jaw and tongues are required. In contrast,
when pronouncing [p], only small movements from lips and jaw
are involved.
3.4 Feature Extraction
After we obtain the Doppler shifts of all the phonemes, we first
normalize them as the same length as those stored in the user profile.
Such a normalization is used to mitigate the effect of different
speech speed of the user when performing voice authentication.
Then, we resplice the normalized Doppler shifts of each phoneme
together. To eliminate the interferences due to other movements
such as nearby moving objects or body movements, we further
utilize a Butterworth filter with cut off frequencies of 19.8KHz and
20.2KHz to remove these out of band noises.
Next, we extract two types of features from the Doppler shifts:
energy-band frequency feature and frequency-band energy feature.
The first type of feature quantifies the relative movement speeds
among multiple articulations. By dividing energy level of all the
frequency shifts into several different bands, we are able to sepa-
rate different parts of articulators based on their distances to the
microphone. A higher energy of the captured Doppler shifts, a
closer movement occurred with respect to the microphone. Before
energy band partition, we first normalize the energy level of each
segmented phoneme into the same scale (i.e., from 0 to 1). Such a
normalization is used to mitigate the energy shift caused by incon-
sistency of a user when speaking an utterance to the smartphone.
We partition the energy into three levels based on the energy
distribution, resulting in 6 sub-bands as each energy level includes
both positive and negative Doppler shifts, as shown in the top graph
of the Figure 10. Specifically, Sub-band 5 and 6 with power level in
between 0.95 to 0.99 represent the strongest Doppler shift signals
captured by microphone. Those Doppler shift signals are reflected
by the articulators that are closest to the microphone, such as the
upper and lower lips. Sub-band 3 and 4 include the power level
ranging from 0.7 to 0.9. They represent the Doppler shifts caused by
the articulator motions that have further distances comparing with
that of the first category, for example, the jaw movement. And sub-
band 1 and 2 with energy level smaller than 0.7 but larger than 0.4
consist of motions dominated by articulator components with the
farthest distance to the microphone, such as the tongue movement.
Given each sub-band, we use the centroid frequency as the feature
and combine all the centroid frequencies of each phoneme together,
resulting in one frequency contour for each band.
The bottom part of Figure 10 demonstrates two energy-band
frequency contours (i.e., band 1 and 2) extracted from the sentence
"Oscar didn’t like sweep day" spoken by a live user. Those two bands
represent articulators (e.g., the tongue) with longer distance to the
microphone. From Figure 10, we observe at a STFT bin number of
80, the frequency shift is lower than surrounding area, indicating a
less movement velocity of an articulator which corresponds to the
[lai] [k] [s] [p] [wi:] [dei] 20.2K 20K K K K 19.8K K … Time (s) Energy band 1Energy band 2Energy band 3Energy band 4Energy band 5Energy band 6Session A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA63Figure 11: An example of
frequency-based energy contours.
frequency sub-band and
pronunciation of phoneme [k]. Meanwhile, the largest frequency
shift in the band can be observed at a STFT bin number of 110, which
corresponds to the phonemes [wi:], indicating intensive motion of
an articulator captured during the pronunciation.
The second type of feature is the frequency-band energy feature,
which quantifies the relative movement positions among multiple
articulations across phonemes. As a faster movement velocity re-
sults in a larger magnitude of Doppler shift, we thus can compare
the energy levels of different articulator movements that have the
same movement velocity. In particular, we divide the frequency
shifts into 5 major sub-bands in both positive and negative direc-
tions, as shown in upper part of the Figure 11. We starts with
sub-band 3, which covers frequency shift from -50Hz to 50Hz. The
corresponding movements are more likely dominated by articula-
tors with lower movement velocity. Next, sub-band 2 and 4 include
frequency shift from 50Hz to 100Hz and -100Hz to -50Hz, respec-
tively. The last two sub-band 1 and 5 include the frequency shift
from 100Hz to 200Hz and -200Hz to -100Hz, respectively. They
cover the components with the highest movement speed. Similar
to the frequency contour, we calculate the average energy level at
each frequency sub-band, and then splice the resulted energy level
together to form an energy contour.
The lower part of Figure 11 demonstrates three frequency-band
energy contours at the band 2, 3 and 5. We observe that the fre-
quency band 3 contour has higher energy level comparing with
the other two bands. It is because while speaking an utterance,
the lower facial region of a user also move slightly. Although with
very slow speed, the large size of the lower facial region leads to
much more or stronger signal reflections, resulting in much higher
energy than that of each individual articulator. The frequency band
5 contour demonstrates the lowest energy level among three bands
and implies the motion is more likely to be caused by the articulator
further from the microphone, such as the tongue. And in fact, the
tongue is the most flexible part of the articulator and the tongue’s
motion could be reliably recorded with an open mouth during the
pronunciation process.
Figure 12: An example of wavelet-based denoising.
3.5 Wavelet-based Denoising
The purpose of wavelet based denoising is to further remove the
noisy component mixed in the extracted features. Those compo-
nents could be caused by hardware imperfection or surrounding
environment interferences and noises. Our system thus utilizes
wavelet denoising technique that is based on Discrete Wavelet
Transform (DWT) to further analyze the signal in both time and
frequency domain [39]. It decomposes input signal into two com-
ponents: approximation coefficients and detailed coefficients. The
approximation coefficients depict the trend of input signal, repre-
senting large scale features. Meanwhile, the detailed coefficients
retain the small scale characteristics, which mixed with both fine
details of the signal and noisy components. Our goal is to extract
the fine details while removing the mixed noises. To achieve this,
we apply a dynamic threshold to the detailed coefficients to remove
the noise components.
Figure 12 shows the process of wavelet-based denoising com-
ponent. Our system first decomposes the each extracted contour
into approximation and detailed coefficients by going through low
pass and high pass filters. We run this step recursively for 3 levels.
After obtaining multiple levels of detailed coefficients, a dynamic
threshold is applied to each level of detail coefficients to filter out
the mixed noises (i.e., the readings with small values). Then, we
combine the original approximation coefficients with the filtered de-
tail coefficients. After that, we use the inverse DWT to reconstruct
the denoised contour. The reconstructed features could facilitate
accurate liveness detection, especially for those Doppler shifts with
similar articulatory gestures.
3.6 Similarity Comparison
To compare the similarity of each extracted contour feature with
the corresponding one in the user profile, we use the correlation
coefficient technique, which measures the degree of linear relation-
ship between two input sequences [53]. The resulted correlation
coefficient ranges from −1 to +1, where the value closer to +1 indi-
cates a higher level of similarity and a value closer to 0 implies a
lack of similarity.
In particular, given a series of n values in each energy-band fre-
quency or frequency-band energy contour A and the corresponding
pre-built user profile B, written as Ai and Bi, where i = 1, 2, ..., n.
Frequencyband 3Frequency band 1Frequency band 2Frequencyband 4Frequencyband 5Frequency band 3Frequency band 2Frequency band 5Original FeatureLow PassHigh PassL[1]H[1]L[3]H[3]Apply ThresholdingDecomposeReconstructFeature after denoisingSession A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA64Figure 13: Two different phone placements diagram.
The Pearson correlation coefficient can be calcualted as:
n
i =1(Ai − ¯A)(Bi − ¯B)
,
rAB =
(n − 1)δAδB
(2)
where ¯A and ¯B are the sample means of A and B, δA and δB are the
sample standard deviations of A and B.
To detect a live user, we use energy-based frequency contours
(i.e., energy-based feature), frequency-band energy contours (i.e.,
frequency-based feature), and combined feature of these two (com-
bined feature), respectively. Given the correlation coefficients of all
contours, we simply compare the averaged coefficient to a prede-
fined threshold for live user detection. Although a more sophisti-
cated classification method, for example a machine learning based
classification, could be used, our primary evaluation in this work is
the validation of the system methodology.
4 PERFORMANCE EVALUATION
In this section, we present the the experimental performance of
our liveness detection system under both replay and mimic attacks.
The project has obtained IRB approval.
4.1 Experiment Methodology
Phones and Placements. We employ three types of phones includ-
ing Galaxy S5, Galaxy Note3, and Galaxy Note5 for our evaluation.
These phones differ in terms of sizes and audio chipsets. Specifi-
cally, the lengths of S5, Note3 and Note5 are 14.1cm, 15.1cm and
15.5cm respectively, whereas the chipsets are Wolfson WM1840, 800
MSM8974 and Audience’s ADNC ES704, respectively. All the audio
chips and the speaker/microphones of these phones can record and
playback 20kHz frequency sound. The operating systems of those
phones are the Android 6.0 Marshmallow that released in 2015,
which supports audio recording and play back at 192kHz sampling
frequency. We thus evaluate our system with the sampling frequen-
cies including 48kHz, 96kHz and 192kHz. We present the results
for 192kHz sampling frequency in the evaluation unless otherwise
stated. Additionally, we consider two types of phone placements as
shown in Figure 13 that people usually used to talk on the phone:
have the phone held either to user’s ear or in front of the mouth.
Data Collection. Our experiments involves 21 participants in-
cluding 11 males and 10 females. The participants are recruited
by emails including both graduate students and undergraduate
students. These participants include both native and non-native
English speakers with ages from 21 to 35. We explicitly tell the
Figure 14: All Attacks: ROC curves under different measure-
ments.
Figure 15: All Attacks: Accuracy under different measure-
ments.
participants that the purpose of the experiments is to perform voice
authentication and liveness detection. Each participant chooses
his/her own 10 different passphrases. For each passphrase, they
repeat three times to enroll in the authentication system and use
the averaged features to establish the profile of user. To perform
legitimate authentication, each participant tries 10 times for each
passphrase, which totals 2100 positive cases. The lengths of those
passphrases range from 2 to 10 words with one third are 2 to 4
words, one third are 5 to 7 words, and the rest are 8 to 10 words. In
addition, to evaluate the individual diversity among users, we ask
12 out of the 21 participants to pronounce the same passphrase. Our
experiments are conducted in classrooms, apartments, and offices
with background and ambient noises such as HVAC noises and
people chatting.
Attacks. We evaluate our system under two types of replay
attack: playback attacks and mimicry attacks. Both forms of attacks
are considered in our evaluation sections unless claimed otherwise.
The playback attacks are conducted with loudspeakers including
the standalone speakers, the built-in speakers of mobile devices,