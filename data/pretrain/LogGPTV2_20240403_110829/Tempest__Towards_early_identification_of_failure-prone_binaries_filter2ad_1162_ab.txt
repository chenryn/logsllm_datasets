Correlation 
Coefficient 
-0.438 
0.268 
0.383 
Sig. (2-tailed) 
p < 0.0005  p < 0.0005  p < 0.0005 
From the statistical results in Table 2 we observe a 
negative  correlation  between  the  number  of  green 
feedback  and  the  post-release  failures  indicating  that 
the  more  green  feedbacks  the  lower  the  post-release 
field  failures.  For  the  yellow  and  red  feedbacks  we 
observe  a  positive  correlation  of  increasing  strength 
indicating  that  with  an  increase  in  the  number  of 
yellow  and  red  feedbacks  we  observe  an  increase  in 
the number of post-release failures. All the correlations 
are  statistically  significant  at  99%  confidence.  This 
result on using the Windows XP-SP1 data on windows 
Server  2003  indicates  the  efficacy  of  using  the  color 
coded feedback mechanism incorporated into Tempest. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:00 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE118DSN 2008: Bhat & Nagappan4. Tempest architecture 
Figure 4 illustrates the architecture of the Tempest 
tool. The statistical modeling and analysis is performed 
at the level of binaries rather than source files as, 
1.  Binaries  are  more  robust  to  process  and 
require less processing and analysis time 
2.  They  are  the  lowest  level  to  which  failures 
can  be  accurately  mapped  back.  (In  the  case 
of  source  files  a  fix  for  a  failure  most  times 
involves editing several files). 
Metrics.lib  is  a  library  built  using  Vulcan  APIs  to 
collect a variety of code metrics. The Binarydiff.lib file 
extracts  the  added,  modified  and  deleted  blocks  of 
code  between  two  versions  of  the  binary  to  quantify 
code  churn  to  build  the  relative  code  churn  measures. 
The Tempest.exe file contains inbuilt statistical models 
(for  OO  and  non-OO  binaries)  and  complexity  metric 
feedback  standards  that  is  built  using  historical  data 
from previous Windows releases that takes as input the 
current code complexity and code churn information to 
predict the failure-proneness of a binary.  
static 
The  metrics  are  collected  by  processing  the 
binaries  using  the  symbol  information  from  the  pdbs 
(Program debug database) and the Vulcan Framework 
[18]. The Vulcan framework provides an infrastructure 
for 
and  dynamic  binary  modification, 
optimization  and  analysis.  Vulcan  also  provides 
abstract interfaces to uniformly analyze x86, ia64, x64 
and  MSIL 
Intermediate  Language 
generated  by  the  Microsoft  .NET  framework)/mixed 
mode  binaries.  The  code  complexity  metrics  are 
collected  automatically  for  native  and  managed 
binaries on Win32 platform using the code metrics tool 
and  the  binary  diff  tool  [10].  The  binary  code 
differentiating  algorithm  and  the  relation  to  source 
code is discussed in [10]. 
(Microsoft 
Figure 4: Tempest architecture 
Failure-proneness estimate 
Relative code churn measures computation 
Color-coded feedback of non-OO metrics  
Figure 5: Screenshot of Failure-proneness estimation and color-coded feedback  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:00 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE119DSN 2008: Bhat & NagappanFigure  5  provides  a  screenshot  of  the  command 
line output of the Tempest. The output is for a sample 
binary  hello.exe  that  opens,  closes  a  file  and  writes 
some  text.  The  output  has  been  manually  tweaked  so 
that  all  the  different  colors  can  be  seen.  (Note:  The 
output  numbers  of  failure-proneness  in  this  example 
are not true and are just for illustrative purposes). The 
HVMF number in  figure 6 is a ratio of the number of 
red feedbacks to total feedbacks. In the example there 
are  three  red  feedbacks  out  of  the  total  29  metrics 
thereby  representing 
the  number  0.103488.  The 
primary reason this is shown is to demonstrate the tool 
usage  and  to  protect  confidential  Microsoft  failure-
proneness numbers from being published. 
5. Discussion of results 
built  for  a  web-service  may  not  be  applicable  for  an 
operating  system.  The  models  have  to  be  tailor  made 
for  the  systems  being  analyzed.  Tempest  has  switch 
architecture  according  to  which  different  models  for 
Windows, Windows Mobile etc that will automatically 
be used depending on the system being analyzed. The 
models  discussed  in  this  paper  were  specifically  built 
tool  employs  a 
for  Windows. 
multiplexing  approach  where 
there  are  different 
statistical  models  build  on  different  systems  at 
Microsoft 
In  general 
the 
illustrates 
In this section we provide a results discussion of the 
use  of  Tempest  at  Microsoft.  There  are  two  typical 
scenarios in which Tempest is being used at Microsoft. 
Figure  6 
these  scenarios.  Scenario  1 
illustrates the integration of Tempest into the software 
development  process.  Individual  developers  and, 
collectively,  teams  develop  features  for  the  next 
version  on  Windows.  After  undergoing  sufficient 
testing and quality assurance  efforts these features are 
integrated  into  the  main  build  of  Windows.  This 
instrumented  Windows  build  is  then  processed  by  the 
Tempest  server 
the  estimated  failure-
proneness  of  the  binaries.  Such  data  is  stored  for 
multiple builds of Windows so as to look at increasing 
or  decreasing  trends  of  failure-proneness  amongst 
Windows  binaries  to  identify  potential  problems.  We 
have  also  noticed  that  such  estimates  are  being  used 
for  resource  allocation  and  planning  for  later  stage 
scheduling of resources in the development and testing 
process. 
to  output 
 In  scenario  2  Tempest  is  run  locally  on  the 
developers  desktop.  The  developers  obtain  a  color-
coded  feedback  on  their  complexity  metrics  (on  their 
locally  compiled  binaries)  relative  to  complexity 
metrics  of  the  binaries  that  did  not  have  failures  (in 
addition  to  an  estimate  of  failure-proneness)  based  on 
which they can refactor the complexity of the binary or 
focus more on testing  specific binaries. 
Some important lessons learned that we think might 
The 
systems. 
be useful to others in academia and industry, 
Scalability: All such metric tools should scale to large 
software 
should 
accommodate such expansion in future systems. 
Contextualization:  The  architecture  of  the  system 
should be in such a way that it allows the building in of 
different  contextual  models  for  the  different  systems 
that  will  be  analyzed.  For  example  statistical  models 
architecture 
Figure 6: Usage scenarios of Tempest in Microsoft 
the 
tool 
Consistency:  The  main  issue  with  all  measurement 
tools is the consistency in measurement of the metrics. 
The  tools  should  account  for  different  programming 
paradigms (OO, non-OO etc.) and different languages. 
Tempest addressed this by working with binaries so as 
to account for C, C++, C# code. 
Performance:  Performance  of 
is  very 
important  to  analyze  large  code  base  in  a  reasonable 
amount  of  time.  Tempest  on  a  Quad  Proc  AMD64 
Opteron 852 2.6 GHz processor can process the whole 
of Windows Vista in 1.25 hours.  
Actionable:  The  model  should  constantly  learn  from 
its failures. The use of an iterative statistical algorithm 
is  desirable.  The  presence  of  a  person  in  the  research 
team with a background in statistics is desirable. 
Extensibility: The tools should be designed in  such a 
way  that  they  can  be  reused  in  other  reliability  tools 
several of which require software metrics as input.  In 
our  case  the  architecture  of  Tempest  allows  others  to 
build  tools  over  it.  For  example  the  Tempest  API  has 
also  now  been  used  within  Microsoft  for  a  change 
impact analysis tool to determine the risk of a change 
or patch.  
Tempest is  widely deployed in Microsoft and has 
been used by various product teams. As of writing this 
paper  Windows  Client,  Windows  Server,  Windows 
Sustained  Engineering,  Windows  Mobile  have 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:00 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE120DSN 2008: Bhat & Nagappan[13] M. C. Ohlsson, von Mayrhauser, A., McGuire, B., 
Wohlin, C., "Code Decay Analysis of Legacy Software 
through Successive Releases", Proceedings  of IEEE 
Aerospace Conference, pp. 69-81, 1999. 
[14] M. C. Ohlsson, Wohlin, C., "Identification of Green, 
Yellow and Red Legacy Components", Proceedings  of 
International Conference on Software Maintenance, pp. 
6-15, 1998. 
[15] N. Ohlsson, Alberg, H., "Predicting fault-prone 
software modules in telephone switches", IEEE 
Transactions in Software Engineering,  22(12), pp. 886 
- 894, 1996. 
[16] T. J. Ostrand, Weyuker, E.J, Bell, R.M., "Where the 
Bugs Are", Proceedings  of the International 
Symposium on Software Testing and Analysis, pp. 86-
96, 2004. 
[17] J. Sliwerski, Zimmermann, T., Zeller, A., "HATARI: 
Raising Risk Awareness (Research Demonstration)",  
Proceedings  of Proceedings of the European Software 
Engineering Conference/International Symposium on 
Foundations of Software Engineering, Lisbon, Portugal, 
pp. 107-110, 2005. 
[18] A. Srivastava, Edwards, A., Vo, H., "Vulcan: Binary 
Transformation in a Distributed Environment," 
Microsoft Research Technical Report MSR-TR-2001-
50, 2001. 
[19] M.-H. Tang, Kao, M-H., Chen, M-H., "An empirical 
study on object-oriented metrics",  Proceedings  of 
Sixth International Software Metrics Symposium, pp. 
242-249, 1999. 
analyzed  their  complete  code  bases  using  Tempest. 
The  architecture  of  Tempest  has  a  one-bit  reporting 
flag  that  allows  us  to  track  the  number  of  runs  of 
Tempest. To date it has completed more than 3 Million 
runs and had analyzed  more than 150 Million lines of 
code. Going forward we plan to add further features to 
Tempest  to  incorporate  dependency  analysis  and  to 
integrate Tempest into the development IDE. 
References 
[1]  V. Basili, Briand, L., Melo, W., "A Validation of Object 
Oriented Design Metrics as Quality Indicators", IEEE 
Transactions on Software Engineering,  22(10), pp. 751 
- 761, 1996. 
[2]  Bhat. T., Nagappan, N., "Building Scalable Failure-
proneness Models Using Complexity Metrics for Large 
Scale Software Systems",  Proceedings  of Thirteenth 
Asia-Pacific Conference on Software Engineering 
(APSEC), Bangalore, India, pp. 361-366, 2006. 
[3]  L. C. Briand, Wuest, J., Daly, J.W., Porter, D.V., 
"Exploring the Relationship between Design Measures 
and Software Quality in Object Oriented Systems", 
Journal of Systems and Software,  51(3), pp. 245-273, 
2000. 
[4]  F. Brito e Abreu, "The MOOD Metrics Set",  
Proceedings  of ECOOP '95 Workshop on Metrics, 
1995. 
[5]  S. R. Chidamber, Kemerer, C.F., "A Metrics Suite for 
Object Oriented Design", IEEE Transactions on 
Software Engineering,  20(6), pp. 476-493, 1994. 
[6]  K. El Emam, Benlarbi, S., Goel, N., Rai, S.N., "The 
Confounding Effect of Class Size on the Validity of 
Object-Oriented Metrics", IEEE Transactions on 
Software Engineering,  27(6), pp. 630 - 650, 2001. 
[7]  N. E. Fenton, Ohlsson, N., "Quantitative analysis of 
faults and failures in a complex software system", IEEE 
Transactions on Software Engineering,  26(8), pp. 797-
814, 2000. 
[8]  R. Harrison, S. J. Counsell, and R. V. Nithi, "An 
Evaluation of the MOOD Set of Object-Oriented 
Software Metrics", IEEE Transactions on Software 
Engineering,  24(6), pp. 491-496, June 1998. 
[9]  T. M. Khoshgoftaar, Allen, E.B., Goel, N., Nandi, A., 
McMullan, J., "Detection of Software Modules with 
high Debug Code Churn in a very large Legacy 
System",  Proceedings  of International Symposium on 
Software Reliability Engineering, pp. 364-371, 1996. 
[10] S. McFurling, Pierce, K., Wung, Z., "BMAT – A Binary 
Matching Tool," Microsoft Research Technical Report 
MSR-TR-99-83, 1999. 
[11] N. Nagappan, Ball, T., "Use of Relative Code Churn 
Measures to Predict System Defect Density",  
Proceedings  of International Conference on Software 
Engineering, pp. 284-292, 2005. 
[12] N. Nagappan, Ball, T., Zeller, A., "Mining metrics to 
predict component failures",  Proceedings  of 
International Conference on Software Engineering, pp. 
452-461, 2006. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:00 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE121DSN 2008: Bhat & Nagappan