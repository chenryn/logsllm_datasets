Algorithm 2 Adversarial Indiscriminate K-means++
LET SAMPLE-SIZE be the desired sample size.
LET HOLD-TIME(c) ← A function that returns a sample
from the distribution of Press-Release timings for key c
LET DIGRAPH-TIME(c, c(cid:48)) ← A function that returns
a sample from the distribution of Release-Press timings
between keys c and c(cid:48)
INITIALIZE samples ← an empty array of
SAMPLE-SIZE
INITIALIZE P ← the target password
INITIALIZE i ← 0
while i < SAMPLE-SIZE do
INITIALIZE timings ← an empty array of the same
length as P
INITIALIZE j ← 0
while j < 2 × length of P do
length
timings[j] ← HOLD-TIME(P[j])
timings[j + 1] ← DIGRAPH-TIME(P[j], P[j+1])
j = j + 2
end while
samples[i] ← timings
i = i + 1
end while
CALL Adversarial k-means++ using samples
IV. EXPERIMENTS
In this section, we present the experiments we performed
to evaluate our adversarial attacks. We experimented with two
modalities of behavioral biometrics (keystroke dynamics and
touchscreen swipes), various datasets, and multiple state of
the art classiﬁcation algorithms. Our experiments demonstrated
that our attack algorithm was effective across all these different
settings.
A. Experimental Setup
1) Protocols: Here we present the protocol we followed
for collecting new data, and selecting samples for training and
testing. This should shed light on the important decisions we
made, but is not meant to be a comprehensive list. Instead it
can be used along with the code for the data collection stage,
and experiments 2 for replicating our work.
Collecting MTurk dataset: The dataset was collected on
the Internet using JavaScript features. During the study we
disabled typing features such as copy, paste and backspace.
Users were presented the passwords in a random order and
could go to the next word only after typing a given word
without any errors.
We dropped any malformed samples in a pre-processing
step. These could happen due to a combination of reasons that
include: different behavior of browsers, differences in internet
speed, or other noise as the subjects took the study simul-
taneously. For instance, one common scenario was when we
did not receive the key-up events for every pressed character.
Rather than going over every minor decision in the way we
had set up the website, and the subsequent data pre-processing
2https://github.com/parimarjan/adversarial keystrokes
6
MTurk dataset passwords Words used by the indiscriminate adversary
mutter mumble bus fuss tryst list data iota than crane bang rang
pat part taste fast boss cross swat answer woman wolf bored more shard gird
lest lead beta met paytm tmux me same veil height win sin
abs fab bobcat bc 412 128 235 423 mac1 tic1
124 412 236 623 348 834 4510 1045 5612 1256 6714 1467 7816 1678 890 089
mustang
password
letmein
abc123
123456789
TABLE I: On the left side are the passwords from the MTurk dataset. On the right side are words that contain the digraphs from
the given password. These were used to collect keystroke samples for the indiscriminate k-means++ adversary. For instance,
“mutter”, and “mumble” provide keystroke samples for “mu”, as in the password “mustang”.
step, we provide our code for these steps, so interested readers
can directly refer to them for details.
Next, we describe the protocol we used for selecting sam-
ples for training and testing, and creating adversarial samples
across all datasets. The training stage was used to ﬁt the
classiﬁer models with each of the user’s samples, and the
testing stage was used to compute EER scores, and set the
individual thresholds for each user. Then in the ﬁnal adversarial
stage we tested the robustness of the classiﬁer to artiﬁcially
generated samples.
There are two broad categories of classiﬁers used in the
context of behavioral biometrics authentication: one class
classiﬁers and two class classiﬁers. One class classiﬁcation
algorithms only use samples from the genuine user to train
the model, while two class classiﬁcation algorithms are also
given access to some of the impostor samples. Traditionally,
keystroke dynamics based authentication systems have focused
more on one class classiﬁers because it is very impractical
to expect negative samples for an arbitrary password. For
instance,
in the 2016 KBOC competition, only one class
classiﬁers were used [26]. Another reason is that generally
both the two class classiﬁers, and one class classiﬁers appear
to give similar EER scores, so there has been no good reason
to prefer two class classiﬁers. In our analysis in the rest of the
paper, we assume an idealized two class classiﬁer scenario in
which the classiﬁer has access to readily available impostor
samples. Using our datasets, this is easy to simulate as all
the users were typing the same passwords. At the same time,
two class classiﬁers seem to work well with the touchscreen
swipes features. This is because most of them are global values
(like mean speed, mean gravity and mean pressure) collected
for the swipe as a whole. In comparison, keystrokes features
were broken down into chunks based on which letters were
being typed. One of the consequences is that it is possible to
get population data on these global features for an arbitrary
swipe.
Genuine User Samples: In all the datasets, we follow the DSN
approach of using the ﬁrst half of the samples for training, and
the second half for testing [19]. This makes sense because it
models the realistic situation where an online classiﬁer will
use the ﬁrst samples from a user to classify future samples.
We also experimented with randomly dividing the samples into
two equal halves - it usually produces slightly better EERs, but
does not change the adversarial results that we present here.
Impostor Training Samples: This was only required for the
7
two class classiﬁers. We randomly chose the same number
of impostor training samples as the genuine user’s training
samples for each of the classiﬁers.
Impostor Testing Samples: For Killourhy-Maxion’s DSN
dataset, we followed their strategy: ﬁrst four samples of every
user besides the genuine user. To keep the number of positive
and negative samples balanced, for the MTurk and touchscreen
swipes dataset, we randomly sampled the same number of
impostor samples as the genuine user’s test samples.
Adversary: The Targeted K-means++ adversary used all the
samples from the data set excluding the ones from the target
user and the ones used for training and testing the user’s
classiﬁer. For the Indiscriminate K-means++ adversary, we
conducted a new MTurk study, as described before, a few
months after the original study. We used all
the samples
from this new study. In Algorithm 2, we set the parameter
“SAMPLE-SIZE” to 20000.
2) Detection Algorithms: We used the following behavioral
biometrics algorithms. In particular, these include most of the
classiﬁers used in KBOC [26], which represents the state of
the art in keystroke dynamics. We also chose the classiﬁers to
represent diverse methods - from statistical classiﬁers, to deep
learning based networks.
One Class Classiﬁers:
• Manhattan distance: A test sample is accepted if its
average feature-wise Manhattan (city-block) distance
to the mean of the training set is below a threshold.
(cid:80)m
If x is a test sample (of m dimensions) and µ is the
mean of the training set, the distance is deﬁned as
i=1 |xi−µi|/m. Killourhy-Maxion [19] had actually
shown a variant of Manhattan distance, the scaled
Manhattan distance, performs slightly better. This was
because the scaled Manhattan distance deals better
with outliers. But this is precisely the advantage of
using the feature normalization techniques described
below, so we found that the scaled version added
nothing to the Manhattan distance classiﬁer.
• Gaussian: The training samples are modeled as a
Gaussian distribution based on their mean and stan-
dard deviation. If the probability of a particular test
sample being in the distribution is above a particular
threshold, then it will be accepted.
• Gaussian mixture: Here, the training samples are
ﬁtted to a Gaussian Mixture model with two com-
ponents using the EM algorithm. Then newer samples
are scored based on their probability of belonging to
the distribution of the training samples. We used the
implementation in the python library sklearn [32].
•
• One Class SVM: We used the Support Vector Ma-
chine (SVM) implementation in sklearn [32], with ra-
dial basis function (RBF) kernel, and kernel parameter
0.9, as used in [24], [25].
Autoencoder and Contractive Autoencoder: With
the advent of deep learning, researchers have started
using variants of neural networks in the domain of
cybersecurity. One of the key structures used in the
past are autoencoders and contractive autoencoders.
[24], [25].
Name of Classiﬁer
Manhattan
SVM
Gaussian
Gaussian Mixture
Autoencoder
Contractual Autoencoder
Random Forest
k-NN
FC Neural Net
DSN EER MTurk EER
0.091
0.087
0.121
0.137
0.099
0.086
0.08
0.09
0.08
0.097
0.097
0.109
0.135
0.099
0.099
0.067
0.090
0.091
TABLE II: EER scores on the DSN and MTurk datasets
Two Class Classiﬁers:
B. Results
•
•
•
Random Forests: We used a model similar to the
one described by Antal et al [4]. Random Forests
with 100 trees was their best-performing classiﬁer on
the touchscreen swipes dataset. We used the Random
Forest implementation in sklearn [32].
Nearest Neighbor: Here we classify a test sample
based on the majority label among a ﬁxed number of
its nearest neighbors in the training set. The neigh-
bours are determined using Euclidean distance. We
used the implementation in [32].
Fully Connected Neural Net: We experimented with
multiple variants of multi layer perceptron by using
different hyper parameters. The network that per-
formed the best had two hidden layers with 15 neu-
rons each computing scores for genuine and impostor
classes. There was no signiﬁcant improvement in the
performance of the network by increasing the number
of layers or neurons per layer in the architecture of
the neural network.
Monaco’s Normalization Technique: Many new techniques
to improve the performance of the classiﬁers were developed
in the KBOC competition [26]. Most interesting of these were
the normalization techniques developed by Monaco. The key
insight of this technique was that a user’s classiﬁer could
normalize future input samples based only on the genuine
user’s data given to it at the start. Essentially, this acts like
a ﬁltering step - and features that are too far from the mean
of the genuine user’s ﬁtting data get ﬁltered out [24], [25].
it
improved performance of all
This was one of the novel techniques that helped Monaco’s
classiﬁers win the KBOC challenge. No previous results with
this technique were reported for the DSN dataset - but we
found that
the algorithms
signiﬁcantly. In fact, the results we report in Table II are the
best reported EER scores for these classiﬁers. It also made the
algorithms perform better against our adversarial attempts - for
instance, in cases where we used a much more conservative
threshold, shown in Figure 3,
the classiﬁer’s performance
is considerably improved with this normalization technique.
Therefore, we do not even mention our results without this
normalization. In a similar way, the scores output by each
classiﬁer were normalized for each user, as used by Monaco
[24].
We surveyed the algorithms proposed in the literature
and implemented several of the best ones, with the aim of
replicating the best existing results. Then, we devised and
experimented with the adversarial agents described before to
study the robustness of the proposed models against such
attacks. We demonstrate that our adversarial attacks can ef-
fectively defeat all the proposed models.
1) Equal Error Rate: The EER results from our classiﬁers
on the DSN dataset and MTurk datasets have been summarized
in Table II. For the DSN dataset,
these are some of the
best reported scores. For instance, without the normalization
technique described above, Manhattan and SVM EER scores
were both around 0.15 - so it nearly doubled their accuracy.
Similarly, we saw improvements in the other classiﬁers as
well. The classiﬁers performed similarly also on the much
bigger MTurk dataset suggesting that these are state of the
art classiﬁers for this problem.
2) Keystroke Results: In this section we discuss the results
of testing our adversaries on the DSN and MTurk datasets,
which are summarized in Tables III, IV. We conducted the
tests independently on each of the ﬁve passwords in the MTurk
dataset, but for a more compact presentation, we average the
results of all passwords. A few interesting highlights based on