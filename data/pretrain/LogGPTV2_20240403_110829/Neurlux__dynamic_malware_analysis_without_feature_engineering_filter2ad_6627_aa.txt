title:Neurlux: dynamic malware analysis without feature engineering
author:Chani Jindal and
Christopher Salls and
Hojjat Aghakhani and
Keith Long and
Christopher Kruegel and
Giovanni Vigna
Neurlux: Dynamic Malware Analysis Without Feature
University of California, Santa Barbara
University of California, Santa Barbara
University of California, Santa Barbara
Chani Jindal
Appfolio
PI:EMAIL
Keith Long
PI:EMAIL
Engineering
Christopher Salls
PI:EMAIL
Christopher Kruegel
Lastline
PI:EMAIL
Hojjat Aghakhani
PI:EMAIL
Giovanni Vigna
Lastline
PI:EMAIL
9
1
0
2
t
c
O
4
2
]
R
C
.
s
c
[
1
v
6
7
3
1
1
.
0
1
9
1
:
v
i
X
r
a
University of California, Santa Barbara
University of California, Santa Barbara
University of California, Santa Barbara
ABSTRACT
Malware detection plays a vital role in computer security. Modern
machine learning approaches have been centered around domain
knowledge for extracting malicious features. However, many po-
tential features can be used, and it is time consuming and difficult
to manually identify the best features, especially given the diverse
nature of malware.
In this paper, we propose Neurlux, a neural network for malware
detection. Neurlux does not rely on any feature engineering, rather
it learns automatically from dynamic analysis reports that detail
behavioral information. Our model borrows ideas from the field
of document classification, using word sequences present in the
reports to predict if a report is from a malicious binary or not. We
investigate the learned features of our model and show which com-
ponents of the reports it tends to give the highest importance. Then,
we evaluate our approach on two different datasets and report for-
mats, showing that Neurlux improves on the state of the art and can
effectively learn from the dynamic analysis reports. Furthermore,
we show that our approach is portable to other malware analysis
environments and generalizes to different datasets.
CCS CONCEPTS
• Security and privacy → Software and application security;
• Computing methodologies → Neural networks.
KEYWORDS
Dynamic malware analysis, Machine learning, deep learning
ACM Reference Format:
Chani Jindal, Christopher Salls, Hojjat Aghakhani, Keith Long, Christopher
Kruegel, and Giovanni Vigna. 2019. Neurlux: Dynamic Malware Analysis
Without Feature Engineering. In 2019 Annual Computer Security Applications
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-7628-0/19/12...$15.00
https://doi.org/10.1145/3359789.3359835
Conference (ACSAC ’19), December 9–13, 2019, San Juan, PR, USA. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3359789.3359835
1 INTRODUCTION
As malware becomes more sophisticated, malware analysis needs
to evolve as well. Traditionally, most anti-malware software uses
signature-based detection, which cross-references executable files
with a list of known malware signatures. However, this approach
has limitations, since any changes to malware can change the signa-
ture, so new releases of the same malware can often evade signature-
based detection by encrypting, obfuscating, packing, or recompiling
the original sample. VirusTotal reports that over 680,000 new sam-
ples are analyzed per day [40], of which potentially a significant
number of samples are just re-packed versions of previously seen
samples, as Brosch et al. [3] observed more than 50% of new mal-
ware are simply re-packed versions of existing malware.
In recent years, the need for techniques that generalize to previ-
ously unseen malware samples has led to detection approaches that
utilize machine learning techniques [25, 26, 38]. Malware analysis
can be broadly divided into two categories: code (static) analysis
and behavioral (dynamic) analysis. Both static and dynamic anal-
ysis have their advantages and disadvantages. Although dynamic
analysis provides a clear picture of the executable behavior, it faces
some problems in practice. For example, dynamic analysis of un-
trusted code requires a virtual machine that replicates the target
host, which requires a substantial amount of computing resources.
Besides, malware may not exhibit its malicious behavior, or the
virtualized environment may not reflect the environment targeted
by the malware [5, 15, 27, 30].
To avoid such limitations, some related work relies only on fea-
tures extracted from static analysis to achieve rapid detection for a
large number of malware samples. However, various encryption
and obfuscation techniques can be employed to hinder static analy-
sis [19, 24]. This becomes a more severe problem for static malware
detectors, since packing is also in widespread use in benign sam-
ples today. samples [28]. Although dynamic analysis is shown to
be susceptible to evasion techniques, run-time behavior is hard to
obfuscate. Dynamically analyzing a binary gives the ability to un-
pack and record its interactions with the OS which it an attractive
choice for malware analysis.
Regardless of the use of static analysis or dynamic analysis, most
machine-learning based malware detectors rely heavily on relevant
domain knowledge [12, 13, 34]. These approaches often rely on
features that are investigated manually by malware experts, which
requires a vast amount of feature engineering. For example, Kol-
bitsch et al. [13] captured the behavior graphs of PE executables in
specific features designed for this purpose. Malware is continually
being created, updated, and changed, which can make the original
well-designed features not applicable to newer malware or differ-
ent malware families. In this case, the costly feature engineering
work has to be refined continuously. Hence, it is crucial to find a
way toreduce the cost of artificial feature engineering to extract
usefulinformation from raw data.
There has recently been some work on deep learning based
malware classification which does not require feature engineering.
However, existing deep learning approaches do not leverage the
information from already-available dynamic analysis systems, in-
stead tending to pick one type of dynamic feature [14] or use static
features [6]. These solutions miss out on the complete information
concerning what actions are taken by each sample.
In this paper we propose Neurlux, a system that uses neural
networks to analyze dynamic analysis reports. Services such as
Cuckoo [21] provide a detailed dynamic analysis of an executable
by tracing it in a sandbox. This analysis contains information, such
as network activity, changes to the registry, file actions, and more.
We use such reports as the basis for our analysis. That is, given a
dynamic analysis report, we want to be able to predict whether or
not the report is for a malware sample or a benign executable.
Our intuition is that we can treat these reports as documents.
With this intuition, we present Neurlux, a neural network which
learns and operates on the (cleaned) dynamic analysis report with-
out needing any feature engineering. Neurlux borrows concepts
from the field of document classification, treating the report as a
sequence of words, which make sequences of sentences, to create a
useful model. Neurlux intends to replace expensive hand-crafted
heuristics with a neural network that learns these behavioral arti-
facts or heuristics.
To check if our method is biased to a particular report format (i.e.,
sandbox), we included in our evaluation two different sandboxes,
the Cuckoo sandbox [21], CuckooSandbox and a commercial anti-
malware vendor’s sandbox, which we will refer to as VendorSandbox.
In addition, we used two different datasets, one provided by the
commercial anti-malware vendor, VendorDataset along with the
labeled benchmark dataset EMBER [2], EmberDataset.
To show that Neurlux does better than feature engineering ap-
proaches, we implement and compare against three such techniques
which are discussed later. Furthermore, we implement and compare
against MalDy, a model proposed by Karbab et al. [11], as a baseline.
MalDy formalizes the behavioral (dynamic) report into a bag of
words (BoW) where the features are words from the report.
In summary, we make the following contributions:
• We propose Neurlux, an approach which leverages docu-
ment classification concepts to detect malware based on the
behavioral (dynamic) report generated by a sandbox without
the need for feature engineering. The only preprocessing
step is cleaning the reports to extract words, upon which
our model learns relevant sequences of words which can aid
its prediction. Neurlux shows high accuracy achieving 96.8%
testing accuracy, in our K-fold validation.
• We create and test several approches for malware classifica-
tion on dynamic analysis reports, including novel methods
such as, a Stacking Ensemble for Integrated Features, and a
Feature Counts model. We compared with these, showing
Neurlux outperforms approaches with feature engineering.
• We assess the generalization ability of Neurlux by testing
it against a new dataset and also a new report format, i.e.,
generated by a new sandbox and show that it generalizes
better than the methods we evaluated against.
• The source code and dataset of executables will be released
on github.
2 BACKGROUND
Some related work adopted Natural Language Processing tech-
niques for malware classification such as MalDy [11], which for-
malizes the behavioral report of a sample into a bag of words. In
this section, we explain such techniques that we exploited to build
Neurlux and other models as a baseline for comparison to Neurlux.
2.1 Word Embeddings
Word embeddings are translations from words to vectors that aim
to give words with similar meaning corresponding vectors that are
close in the feature space. Word embeddings are frequently found
as the first data processing layer in a deep learning model that
processes words [10]. This is because grouping vectors by meaning
gives a deep learning model an initial correlation of words. These
embeddings are frequently pre-trained or based on another model
such as word2vec [18]. However, in our case, we do not use a pre-
trained embedding as the similarities of "words" (i.e., file paths,
mutexes, etc.) differ from ordinary English.
2.2 Embedding Visualization
Dimensionality reduction methods are used to convert high dimen-
sional data into lower dimensional data. We can use dimensionality
reduction to convert the data into two dimensions, allowing us
to show the distribution of the data in a scatter plot. To do this,
we choose to use t-Distributed Stochastic Neighbor Embedding
(t-SNE) [17], a technique for visualization of similarity data. t-SNE
preserves the local structure of the data and some global structure,
such as clusters, while reducing the dimensionality.
2.3 CNN for text classification
Convolutional neural networks (CNN) have recently shown to be
very useful in text classification. A typical model for this represents
each input as a series of n sequences, where each sequence is a
d-dimensional vector; thus input is a feature map of dimensions
n×d. The model starts by mapping words to vectors, as discussed in
Section 2.1. Then, convolutional layers are used for representation
learning from sliding k − дrams.
To extract higher level features from input vectors, a CNN applies
filters of Rk×d on an input of length n, {x1, x2, x3, x4, xi ....xn}. After
applying a filter of size k we have, {x1:k , x2:k +1, x3:k +2, ....xn−k +1:n}.
Embeddings for xi, i  n, are zero padded. For each win-
dow, xi:i +k−1, a feature pi is generated which is then fed into ReLU
non-linearity.
(1)
Where b ∈ R is a bias term, f is a non-linear activation function,
such as the hyperbolic tangent, and W .k is the weights for filter k.
Applying filter k to all windows results in the feature map.
pi = f (W .kT + b)
Max pooling sub-samples the input by applying a max opera-
tion on each sample. It extracts the most salient n-gram features
across the sentence in a translation-invariant manner. The extracted
feature can be added anywhere in the final sentence representation.
In practice we use multiple window sizes and multiple convolu-
tional layers in parallel. A combination of convolution layers fol-
lowed by max pooling is often used to create deep CNN networks.
Sequential convolutions can improve the sentence mining process
by capturing an abstract representation which is also semantically
rich.
2.4 LSTM/ BiLSTM
Recursive Neural Networks (RNN) have gained popularity with text
classification due to their ability to preserve sequence information
over time. LSTM networks [9] overcome the vanishing gradient
problem of RNN [8]. LSTM networks use an adaptive gating, which
regulates the flow of information from the previous state and the
extracted features of the current data input. For an input sequence
with n entries: x1, x2, ..., xn, an LSTM network processes it word by
word. Then, it uses the following equations to update the memory
pt and hidden state ht at time-step t:
(cid:3)
σ
σ
σ
 =
 W .(cid:2)ht−1, xt
it
ft
ot
qt
pt = ft ∗ pt−1 + it ∗ qt
ht = ot ∗ tanh(pt)
tanh
(2)
(3)
(5)
(4)
where xt is the input at time-step t, it is the input gate activation,
ft is the forget gate activation, and ot is the output gate activation.
All gates are generated by a sigmoid function, σ over the ensemble
of input xt and the preceding hidden state ht−1.
A BiLSTM network extends the unidirectional LSTM by initi-
ating a second hidden layer. In this layer, the hidden-to-hidden
connections can flow in reverse temporal order. Therefore, the
model holds information from both the past and the future. The
output of jth word can be represented as:
hj =(cid:2)−→
hj
←−
(cid:3)
hj
2.5 Attention
It is evident that not all words contribute equally to the malicious
or benign attributes of dynamic behavior. Hence, at the word level,
an attention mechanism [39] can be used to extract malicious fea-
tures/words that are important to the behavior classification. Fi-
nally, we aggregate the representations of those malicious features
to form the sentence representation.
Let H ∈ Rd×n be a matrix of hidden vectors [h1, h2, ..., hn] that
the LSTM network produced, where d is the size of the hidden layers,
and n is the number of words in a sentence. Let hit ∈ H represent
a hidden state. The first step is to feed hit through a single-layer
Perceptron network to get uit as its hidden representation:
uit = tanh(Wwhit + bw).