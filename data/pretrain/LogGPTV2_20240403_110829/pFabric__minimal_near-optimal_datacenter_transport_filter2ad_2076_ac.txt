102
104
106
Flow Size (Bytes)
108
1010
(b) Data mining workload
Figure 4: Empirical trafﬁc distributions used for benchmarks.
The distributions are based on measurements from real pro-
duction datacenters [3, 12].
the best results are obtained with packet-spraying after fast retrans-
missions are disabled to cope with packet reordering (in fact, this
is the reason we disabled 3 dupACKs in our rate control). Hence,
we use packet spraying by default for all schemes.
Benchmark workloads: We simulate empirical workloads mod-
eled after trafﬁc patterns that have been observed in production
datacenters. We consider two ﬂow size distributions shown in Fig-
ure 4. The ﬁrst distribution is from a datacenter supporting web
search [3]. The second distribution is from a cluster running large
data mining jobs [12]. Flows arrive according to a Poisson process
and the source and destination for each ﬂow is chosen uniformly
at random. The ﬂow arrival rate is varied to obtain a desired level
of load in the fabric. Both workloads have a diverse mix of small
and large ﬂows with heavy-tailed characteristics. In the web search
workload, over 95% of all bytes are from the 30% of the ﬂows that
are 1–20MB. The data mining workload is much more extremely
skewed: more than 80% of the ﬂows are less than 10KB and 95%
of all bytes are in the ∼3.6% ﬂows that are larger than 35MB. As
we demonstrate in §5.4, this actually makes the data mining work-
load easier to handle because it is less likely that multiple large
ﬂows are concurrently active from/to one fabric port — reducing
network contention. Hence, for most of our simulations, we focus
on the more challenging web search workload.
Performance metrics: Similar to prior work [14, 21, 3] we con-
sider two main performance metrics. For deadline-constrained traf-
ﬁc, we use the application throughput deﬁned as the fraction of
ﬂows that meet their deadline. For trafﬁc without deadlines, we
use the ﬂow completion time (FCT). We consider the average FCT
across all ﬂows, and separately for small and large ﬂows. We also
consider the 99th percentile ﬂow completion time for the small
ﬂows. We normalize all ﬂow completion times to the best possi-
ble completion time for that ﬂow — the value achieved if that one
ﬂow is transmitted over the fabric at 10Gbps without any interfer-
ence from competing trafﬁc.
5.2 Schemes compared
TCP-DropTail: A standard TCP-New Reno with Sack and Drop-
Tail queues.
DCTCP: The DCTCP [3] congestion control algorithm with ECN
marking at the fabric queues.
Figure 5: (a) Per-ﬂow throughput when 5 large ﬂows are ini-
tiated simultaneously to one destination port. (b) Loss rate vs
number of long-lived ﬂows congesting a bottleneck link.
pFabric: The design described in this paper including both the
switch and the minimal rate control. Unless otherwise speciﬁed,
the remaining ﬂow size is used as the priority for each packet.
PDQ: This is the best known prior approach for minimizing ﬂow
completion times or missed deadlines. Our implementation follows
the design faithfully as described in [14] including the Early Start
and Early Termination enhancements and is based on a copy of the
source code we obtained from the authors of the PDQ paper.
Ideal: The Ideal scheduling algorithm described in §3. A central
scheduler with a complete view of all ﬂows preemptively sched-
ules existing ﬂows in nondecreasing order of size and in a maximal
manner (see Algorithm 1). For this scheme, we conduct ﬂow-level
simulations (not packet-level) in Matlab according to the same ex-
act sequence of ﬂow arrivals used in our ns2 benchmarks.
We experimentally determine the best settings for the relevant
parameters for all schemes (summarized in Table 1). Note that the
larger retransmission timeout for the other schemes compared to
pFabric is because they have larger queues. In fact, the difference
in retransmission timeout (200µs versus 45µs) is in proportion to
the queue size difference (225KB − 36KB = 189KB ∼ 150µs
at 10Gbps). Without this, spurious retransmissions would hurt the
performance of these schemes. We evaluate the impact of pFabric’s
RTO in depth in §5.4.3.
Scheme
TCP-DropTail
DCTCP
pFabric
PDQ
Parameters
qSize = 225KB
initCwnd = 12 pkts
minRT O = 200µs
qSize = 225KB
markingT hresh = 22.5KB
initCwnd = 12 pkts
minRT O = 200µs
qSize = 36KB
initCwnd = 12 pkts
RT O = 45µs
qSize = 225KB
RT O = 200µs,
K = 2 (for Early Start)
probingInterval = 15µs
Table 1: Default parameter settings in simulations.
5.3 Basic Performance Measures
Seamless switching between ﬂows: Can pFabric seamlessly switch
between ﬂows that need to be scheduled serially? To test this, we
simultaneously generate 5 large transfers of size 20MB to a sin-
gle destination host. Figure 5(a) shows the throughput achieved
by each ﬂow over time. We observe that the ﬂows are indeed
scheduled one-by-one and at each time, one ﬂow grabs all the bot-
tleneck’s bandwidth (10Gbps). Note that this is the optimal ﬂow
44085
)
s
m
(
i
e
m
T
n
o
i
t
l
e
p
m
o
C
t
s
e
u
q
e
R
l
a
t
o
T
80
75
70
65
60
0
TCP-DropTail
DCTCP
PDQ
pFabric
10
20
30
# of Senders
)
s
m
(
80
i
e
m
T
n
o
i
t
l
e
p
m
o
C
w
o
F
l
l
i
a
u
d
v
d
n
i
I
40
50
70
60
50
40
0
10
20
30
# of Senders
40
50
(a) Total Request
(b) Individual Flows
Figure 6: Total request and individual ﬂow completion times in
Incast scenario. Note that the range of the y-axis is different for
the two plots.
scheduling in order to minimize the average ﬂow completion time
in this scenario. pFabric uses this scheduling even though the ﬂows
are all exactly the same size because the packet priorities are based
on the remaining ﬂow size. Hence, a ﬂow that is initially lucky and
gets more packets through gets the highest priority and dominates
the other ﬂows. The last of the 5 ﬂows completes after ∼80.15ms.
This is only 150µs larger then the best possible completion time of
80ms for a 100MB transfer at 10Gbps. Hence, pFabric is able to
seamlessly schedule one ﬂow after the other with very little loss of
throughput efﬁciency.
Loss rate: The previous simulation showed that pFabric can seam-
lessly switch between ﬂows without loss of throughput, but what
about loss rates? We repeat the previous simulation but stress the
network by using up to 50 concurrent large ﬂows to a single desti-
nation port, and measure the overall loss rate. We conduct the sim-
ulation both with and without pFabric’s probe mode (discussed in
§4.2). The results are shown in Figure 5(b). We observe that with-
out probe mode, the loss rate rises sharply from ∼4.8% to ∼38.5%
as the number of ﬂows increases. This is because except for the
high-priority ﬂow, the packets of the other ﬂows are all dropped at
the bottleneck. Hence, each low-priority ﬂow retransmits a full-
sized (1500B) packet every RT O = 45µs which is eventually
dropped. As expected, the probe mode signiﬁcantly lowers the loss
rate (to under 5.5% with 50 ﬂows) since the low priority ﬂows only
periodically send a small probe packet (with a one byte payload)
while waiting for the high priority ﬂow to complete.
Incast: We now show pFabric’s performance for Incast trafﬁc pat-
terns which occur in many large-scale web applications and storage
systems and have been shown to result in throughput degradation
for TCP [19, 3]. The incast pattern exhibits similar characteristics
as the previous experiment where a large number of ﬂows simulta-
neously transmit to a single destination. Similar to prior work [19],
we create Incast by having a receiver node request a 100MB ﬁle
that is striped across N sender nodes. The senders respond with
100MB/N of data simultaneously. The request completes when
all the individual ﬂows have ﬁnished. Once a request is complete,
the client immediately initiates the next request. The simulation is
run for 10,000 requests and we compute the average total request
completion time and the average individual ﬂow completion times.
The results for TCP-DropTail, DCTCP, PDQ and pFabric are
shown in Figure 6. Note that all schemes use a small minRT O
which has been shown to greatly mitigate the Incast problem [19]
(DCTCP additionally beneﬁts from aggressive ECN marking [3]).
Hence, considering the total request completion time, all schemes
handle Incast fairly well. DCTCP does the best and achieves a near-
ideal request complete time of 80ms across all number of senders.
pFabric is almost as good achieving a total request completion time
of 81.1ms at 50 senders. The small increase is due to the slight
TCP-DropTail
DCTCP
PDQ
pFabric
Ideal
T
C
F
d
e
z
i
l
a
m
r
o
N
10
8
6
4
2
0
10
T
C
F
d
e
z
i
l
a
m
r
o
N
8
6
4
2
0
0.2
0.4
Load
0.6
0.8
0.2
0.4
Load
0.6
0.8
(a) Web search workload
(b) Data mining workload
Figure 7: Overall average normalized ﬂow completion time for
the two workloads at various loads.
overhead of serially scheduling ﬂows with pFabric. However, as
expected, serial ﬂow scheduling signiﬁcantly improves the average
individual ﬂow completion times (Figure 6(b)) for pFabric com-
pared to DCTCP and TCP-DropTail which are more fair across
ﬂows. PDQ also exhibits a similar behavior as pFabric since it aims
to mimic the same kind of ﬂow scheduling, however it has slightly
higher overhead in ﬂow switching and consequently shows slightly
worse performance as the number of ﬂows increases.
5.4 Overall Performance
In this section we show pFabric’s overall performance in large
scale datacenter topologies with realistic workloads. We show that
pFabric’s ability to efﬁciently schedule ﬂows in the order of their
priorities (remaining ﬂow size or deadline) enables it to achieve
near-optimal performance for trafﬁc scenarios with no deadlines as
well as scenarios where there is a mix of deadline and no-deadline
trafﬁc. In the interest of space, after the overall performance re-
sults, we only show results for the deadline-unconstrained trafﬁc
for targeted experiments that highlight different aspects of pFab-
ric’s design and their impact on overall performance.
5.4.1 Deadline-unconstrained trafﬁc
pFabric achieves near-optimal ﬂow completion times for all ﬂow
sizes, loads and for both workloads in our simulations. Figure 7
shows the overall average ﬂow completion times for the web search
and data mining benchmarks as we vary the load from 10% to 80%.
Recall that each ﬂow’s completion time is normalized to the best
possible value that is achievable in an idle fabric for that ﬂow. We
observe that for both workloads the average FCT with pFabric is
very close to that of the Ideal ﬂow scheduling scheme and is signif-
icantly better than for the other schemes. pFabric’s performance is
within ∼0.7-17.8% of the Ideal scheme for the web search work-
load and within ∼1.7–10.6% for the data mining workload. Com-
pared to PDQ, the average FCT with pFabric is ∼19-39% lower
in the web search workload and ∼40-50% lower in the data min-
ing workload. All schemes generally do better for the data mining
workload, particularly at high load. This is because in the data min-
ing workload, the largest ∼3.6% of ﬂows contribute over 95% of
all bytes (Figure 4(b)). These ﬂows, though very large, arrive in-
frequently and thus it is rare that multiple of them are concurrently
active at a particular fabric port and cause sustained congestion.
It is important to note that PDQ always requires one extra RTT
of overhead for ﬂow initiation (SYN/SYN-ACK exchange) before
a ﬂow can transmit. Because of this, PDQ’s normalized FCT is
at-least two for very small ﬂows that can ideally complete in one
RTT. For example, in the data mining workload where about 50%