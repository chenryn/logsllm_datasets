Figure5 shows the rank of culprit services identified by CSL and the cali-
bration results with CSL and culprit metric localization module (CSL + CML)
against the F1-score (the harmonic mean of precision and recall) of anomaly
detection for all anomaly cases. We observe that applying autoencoder on the
service relevant metrics can significantly improve the accuracy of culprit service
localization by ranking the faulty service within the top two. Table2 shows the
overall performance of the above two methods for all anomaly cases. It shows
that complementing culprit service localization with autoencoder can achieve a
precision of 92%, which outperforms 61.4% than the results of CSL only.
6 Related Work
To diagnose the root causes of an issue, various approaches have been proposed
in the literature. Methods and techniques for root cause analysis have been
extensively studied in complex system [13] and computer networks [7].
Recent approaches for cloud services typically focus on identifying coarse-
grained root causes, such as the faulty services that initiate service performance
degradation [8,9,15]. Ingeneral, they aregraph-basedmethods that construct a
dependencygraphofserviceswithknowledgediscoveryfrommetricsorprovided
servicecallgraph,toshowthespatialpropagationoffaultsamongservices;then
Performance Diagnosis in Cloud Microservices Using Deep Learning 95
they infer the potential root cause node which results in the abnormality of
other nodes in the graph. For example, Microscope [8] locates the faulty service
by building a service causality graph with the service dependency and service
interference in the same machine. Then it returns a ranked list of potential
culprit services by traversing the causality graph. These approaches can help
operators narrow down the services for investigation. However, the causes set
for an abnormal service are of a wide range, hence it is still time-consuming to
get the real cause of faulty service, especially when the faulty service is low-
ranked in the results of the diagnosis.
Someapproachesidentifyrootcauseswithfinegranularity,includingnotonly
theculpritservicesbutalsotheculpritmetrics.Seer[4]isaproactiveonlineper-
formancedebuggingsystemthatcanidentifythefaultyservicesandtheproblem-
atic resource that causes service performance degradation. However, it requires
instrumentation to the source code; Meanwhile, its performance may decrease
whenre-trainingisfrequentlyrequiredtofollowuptheupdatesinmicroservices.
Loud [10] and MicroCause [11] identify the culprit metrics by constructing the
causality graph of the key performance metrics. However, they require anomaly
detection to be performed on all gathered metrics, which might introduce many
falsepositivesanddecreasetheaccuracyofcauseslocalization.A´lvaroBrando´n,
et al. [1] propose to identify the root cause by matching the anomalous graphs
labeledbyanexpert.However,theanomalouspatternsaresupervisedbyexpert
knowledge, which means it can only detect previously known anomaly types.
Besides, the computation complexity of graph matching is exponential to the
sizeofthepreviousanomalouspatterns.Causeinfer[2]pinpointsboththefaulty
services and culprit metrics by constructing a two-layer hierarchical causality
graph. However, this system uses a lag correlation method to decide the causal
relationship between services, which requires the lag is obviously included in
the data. Compared to these methods, our proposed system leverages the spa-
tial propagation of the service degradation to identify the culprit service and
the deep learning method, which can adapt to arbitrary relationships among
metrics, to pinpoint the culprit metrics.
7 Conclusion and Future Work
In this paper, we propose a system to help cloud operators to narrow down the
potential causes for a performance issue in microservices. The localized causes
areinafine-granularity,includingnotonlythefaultyservicesbutalsotheculprit
metrics that cause the service anomaly. Our system first pinpoints a ranked list
ofpotentialfaultyservicesbyanalyzingtheservicedependencies.Givenafaulty
service, it applies autoencoder to its relevant performance metrics and leverages
the reconstruction errors to rank the metrics. The evaluation shows that our
system can identify the culprit services and metrics with high precision.
The culprit metric localization method is limited to identify the root cause
thatreflectsitselfwithasignificant deviation fromnormalvalues.Inthefuture,
wewouldliketodevelopmethodstocovermorediverserootcausesbyanalyzing
the spatial and temporal fault propagation.
96 L. Wu et al.
Acknowledgment. This work is part of the FogGuru project which has received
fundingfromtheEuropeanUnion’sHorizon2020researchandinnovationprogramme
under the Marie Sk(cid:3)lodowska-Curie grant agreement No 765452. The information and
viewssetoutinthispublicationarethoseoftheauthor(s)anddonotnecessarilyreflect
the official opinion of the European Union. Neither the European Union institutions
and bodies nor any person acting on their behalf may be held responsible for the use
which may be made of the information contained therein.
References
1. Brand´on, A´., et al.: Graph-based root cause analysis for service-oriented and
microservice architectures. J. Syst. Softw. 159, 110432 (2020)
2. Chen,P.,Qi,Y.,Hou,D.:Causeinfer:automatedend-to-endperformancediagnosis
withhierarchicalcausalitygraphincloudenvironment.IEEETrans.Serv.Comput.
12(02), 214–230 (2019)
3. DiFrancesco,P.,Lago,P.,Malavolta,I.:Migratingtowardsmicroservicearchitec-
tures: an industrial survey. In: ICSA, pp. 29–2909 (2018)
4. Gan, Y., et al.: Seer: leveraging big data to navigate the complexity of perfor-
mance debugging in cloud microservices. In: Proceedings of the Twenty-Fourth
International Conference on Architectural Support for Programming Languages
and Operating Systems, ASPLOS 2019, pp. 19–33 (2019)
5. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press, Cambridge
(2016). http://www.deeplearningbook.org
6. Gulenko, A., et al.: Detecting anomalous behavior of black-box services modeled
withdistance-basedonlineclustering.In:2018IEEE11thInternationalConference
on Cloud Computing (CLOUD), pp. 912–915 (2018)
7. (cid:3)lgorzata Steinder, M., Sethi, A.S.: A survey of fault localization techniques in
computer networks. Sci. Comput. Program. 53(2), 165–194 (2004)
8. Lin,J.,etal.:Microscope:pinpointperformanceissueswithcausalgraphsinmicro-
service environments. In: Service-Oriented Computing, pp. 3–20 (2018)
9. Ma,M.,etal.:Automap:diagnoseyourmicroservice-basedwebapplicationsauto-
matically. In: Proceedings of the Web Conference 2020, WWW 2020, pp. 246–258
(2020)
10. Mariani,L.,etal.:Localizingfaultsincloudsystems.In:ICST,pp.262–273(2018)
11. Meng, Y., et al.: Localizing failure root causes in a microservice through causal-
ity inference. In: 2020 IEEE/ACM 28th International Symposium on Quality of
Service (IWQoS), pp. 1–10. IEEE (2020)
12. Newman, S.: Building Microservices. O’Reilly Media Inc., Newton (2015)
13. Sol´e,M.,Munt´es-Mulero,V.,Rana,A.I.,Estrada,G.:Surveyonmodelsandtech-
niques for root-cause analysis (2017)
14. Thalheim,J.,etal.:Sieve:actionableinsightsfrommonitoredmetricsindistributed
systems.In:Proceedingsofthe18thACM/IFIP/USENIXMiddlewareConference,
pp. 14–27 (2017)
15. Wang, P., et al.: Cloudranger: root cause identification for cloud native systems.
In: CCGRID, pp. 492–502 (2018)
16. Wu,L.,etal.:MicroRCA:rootcauselocalizationofperformanceissuesinmicroser-
vices. In: NOMS 2020 IEEE/IFIP Network Operations and Management Sympo-
sium (2020)
Anomaly Detection at Scale: The Case for
Deep Distributional Time Series Models
B
Fadhel Ayed1( ), Lorenzo Stella2, Tim Januschowski2, and Jan Gasthaus2
1 University of Oxford, Oxford, UK
2 Amazon Research, Berlin, Germany
{stellalo,tjnsch,gasthaus}@amazon.de
Abstract. This paper introduces a new methodology for detecting
anomalies in time series data, with a primary application to monitor-
ingthehealthof(micro-)servicesandcloudresources.Themainnovelty
in our approach is that instead of modeling time series consisting of
real values or vectors of real values, we model time series of probability
distributions. This extension allows the technique to be applied to the
commonscenariowherethedataisgeneratedbyrequestscomingintoa
service,whichisthenaggregatedatafixedtemporalfrequency.Weshow
the superior accuracy of our method on synthetic and public real-world
data.
· ·
Keywords: Anomaly detection Recurrent neural networks Time
series analysis
1 Introduction
Inlarge-scaledistributedsystemsorcloudenvironments,thedetectionofanoma-
lous events allows operators to detect and understand operational issues and
facilitates swift troubleshooting. Undetected anomalies can result in potentially
significant losses and can impact customers of these systems and services nega-
tively. In this work we focus on anomaly detection in the context of our target
application of monitoring compute systems and cloud resources, where main
object of interest are metrics emitted by these systems; we refer to this setting
as cloud monitoring. We refer the reader to detailed overviews [2,11] on other
application areas for anomaly detection.
In the setting of cloud monitoring, an anomaly detection system needs to be
able to efficiently detect anomalous events in a streaming fashion. The funda-
mental difficulties that any anomaly detection system has to face are threefold.
First, due to the number and diversity of the monitored metrics (often millions)
andthestreamingnatureofthedata,itisuncommontohavesufficientamounts
of labeled data. Even if labels are available, due to the subjectivity of the task,
labels may not represent an “objective” ground truth. This raises the need for
unsupervised techniques. Second, the monitoring systems have to track the evo-
lution of a large number of time series simultaneously, which often leads to a
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.97–109,2021.
https://doi.org/10.1007/978-3-030-76352-7_14
98 F. Ayed et al.
Fig.1.Latencymetricmonitoringwithtemporalaggregationusingdifferentsummary
statistics.Thethreepanelsshowthesameunderlyingeventdata,aggregatedintofive-
minute intervals using three summary statistics: (left) 5% quantile; (center) median;
(right)95%quantile.TheanomalousregionoccuringattheendofNovemberisclearly
visible in the 95% quantile, but harder (or impossible) to detect in the other two.
considerableflowofdatatoprocessinnearreal-time,sothemodelshavetoscale
efficiently to the amount of data available. Here, scalability comes not only in
the traditional flavor of computational scalability, but also in terms of the need
to involve experts to tune the systems. Finally, the methods have to be flexible
inordertohandletimeseriesandanomaliesofdifferentnature(e.g.CPUusage,
latency, error rate).
The main contribution of the present work is a novel anomaly detection
method based on distributional time series models that addresses all three chal-
lenges.Tothebestofourknowledgeitisthefirstanomalydetectionmethodology
that builds on a predictive model for a distributional time series representation.
It employs an autoregressive LSTM-based recurrent neural network to provide
flexibility while still being statistically sound. Our model scales well at infer-
ence time and has a compact model state making it deployable in low-latency,
streaming application scenarios. Finally, our methodology can detect collective
anomalies1, which most non-distributional techniques are unable to detect. We
evaluateourmethodonanumberofdatasetsincludingsynthetic,publiclyavail-
able,andAWS-internaldatasets,andshowthatourmethodcomparesfavorably
tothestateoftheart.Whilewedevelopourmethodologyforthecloudmonitor-
ingsetting,wefurthershowthatourmethodiscompetitiveinclassicalanomaly
detection settings.
We proceed by first discussing a motivating example for our method and
provide background in Sect.2, introduce the model formally in Sect.3, evaluate
it empirically in Sect.4, discuss related work in Sect.5, before concluding in
Sect.6.
1 Acollectiveanomalyconsistsofasubsetofpointsthatdeviatesfromtherestofthe
dataset even though individually each point may appear normal.
Deep Distributional Time Series Models 99
2 Motivation
In the following we motivate our distributional time series modeling approach
fromtwoangles:thedatagenerationprocessofrequest-drivenmetrics,andhigh-
frequency time series.
In a typical (micro-) service monitoring setup, a metric datum is emitted for
eachrequesthandledbytheservice.Therawmonitoringdataisthusastreamof
events,whereeacheventisatupleconsistingofatimestampandasetofmeasure-
ments.Asameasurementistriggeredforeachincomingrequest,thetimestamps
are not equally spaced, and for large services one may collect hundreds of thou-
sands of events per minute. To facilitate further processing, the typical anomaly
detectionpipelinestartswithatemporalaggregationstep,wheretheeventdata
is aggregated into fixed-sized time intervals (e.g. one minute), recovering the
classical, equally-spaced time series setting. This aggregation of events requires
choosing a meaningful statistic which summarizes all measurements within a
given time interval, while allowing detection of abnormal behaviors. Commonly
used summary statistics are the mean, the median, or extreme percentiles. How-
ever,thesummarystatisticschosenultimatelydeterminetherangeofanomalies
one can detect, and one risks missing anomalies if the statistics are chosen inap-
propriately (see Fig. 1 for illustration from internal services). The method we
propose here embraces this event-based data generation process by considering
the entire distribution of measurements within each time interval. This means
Fig.2. Illustration of our approach. The undelying signal (top panel) is grouped into
fixed-size time intervals (vertical red dashed lines) of size n t (here n t =400). We esti-
matetheprobabilitydistributionF˜ t ofthevalueswithintheintervalusingahistogram
(blue horizontal bars in the bottom panels), with bin edges (dashed grey lines) chosen
according to a global strategy (e.g. based the marginal distribution, top right panel).
For each time interval, the model predicts a probability distribution over probability
distributions (yellow-red heatmaps in the bottom panel) using a RNN. For “normal”
periods(e.g.bottomleftpanel),theobserveddata(bluelines)alignswiththemodel’s
prediction,i.e.thebluelinesfallintotheshadedarea.For“anomalous”periods(three
rightmost bottom panels), the observed histogram falls outside the high-probability
region predicted by the model. (Color figure online)
100 F. Ayed et al.
consideringtimeseriesofequallyspaced“points”intime,butwhereeach“point”
isaprobabilitydistribution,calledadistributionaldatapoint.Thisisincontrast
to most classical anomaly detection approaches that do not explicitly model the
temporal data aggregation step.
Even though the proposed method was originally designed for the particular
nature of the data described above, we demonstrate highly competitive perfor-
mance even in the “classical” setting, where the starting point are time series
of real values sampled at a regular frequency. We discuss this via the example
of high-frequency time series, arising for example from measuring the CPU uti-
lization or temperature of a compute node every second. Our approach solves
several difficulties specific to such metrics.
The main challenge one faces when monitoring high-frequency data is that
the temporal dynamics governing the data evolve at a slower pace than the fre-
quency of observation. In typical application settings, meaningful variations are
expectedtooccurfromonehourtothenext,butnoteverysecond.Theunderly-
ing dynamics can thus often be adequately described by using one hour or half
an hour time granularity, with seasonal patterns that are daily, or weekly. How-
ever, both classical and deep-learning-based time series models are commonly
unable to model long range dependencies (measured in number of observations),
so that if high-frequency data is modeled directly, these models commonly fail
to capture medium and long term patterns. Our approach allows modeling the
temporal evolution at a more appropriate frequency by aggregating the observa-
tions,whileretainingtheabilitytodetectanomaliesattheoriginalfrequencyby
modelingthedistributionofobservationswithineachtimeinterval.Withineach
aggregated time interval t, we treat the high-frequency data point as samples
from this distribution.
3 Model
In the following, we introduce the necessary notation and tools used in the
rest of the paper. We start by recalling that a generic strategy for anomaly
detection using probabilistic models is to mark an observation as anomalous if
its probability under the model is low. More details can be found for example
in [7]. Our method builds on this approach and is summarized in Fig.2.
3.1 Distributional Time Series
Let F 1:T = F 1,...,FT be a time series of univariate probability distributions,
represented by their cumulative distribution functions (CDFs). We assume that
the support for all Ft is the interval Y = [y min,y max]. Even though these dis-
tributions are the objects of interest, we usually do not to have access to them