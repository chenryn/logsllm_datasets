`docker service ps`显示运行服务的副本，包括托管每个容器的节点的名称:
```
> docker service ps message-queue
ID    NAME      IMAGE     NODE  DESIRED  STATE  CURRENT    STATE
xr2vyzhtlpn5 message-queue.1  dockeronwindows/ch05-nats:2e  win2019-02  Running        Running
```
在这种情况下，管理器已经安排了一个容器在节点`win2019-02`上运行，这是我的群集中的单个工作节点。如果我直接在该节点上运行 Docker 容器，看起来会得到相同的结果，但是作为 Docker Swarm 服务运行它会给我带来编排的所有额外好处:
*   **应用可靠性**:如果这个容器停止，管理者会安排一个替换立即开始。
*   **基础设施可靠性**:如果工作节点发生故障，管理器将调度一个新的容器在不同的节点上运行。
*   **可发现性**:这个容器附着在一个覆盖网络上，所以它可以使用服务名与其他节点上运行的容器进行通信(Windows 容器甚至可以与同一个群中的 Linux 容器进行对话，反之亦然)。
在 Docker Swarm 中运行服务比在单个 Docker 服务器上运行容器有更多的好处，包括安全性、可伸缩性和可靠的应用更新。我将在这一章中涵盖它们。
在源代码库中，`ch07-create-services`文件夹有一个脚本，它以正确的顺序启动 NerdDinner 的所有服务。每个`service create`命令的选项相当于[第 6 章](06.html)、*用 Docker Compose* 组织分布式解决方案的 Compose 文件中的服务定义。前端服务和 Traefik 反向代理之间只有一些不同。
Traefik 在 Docker Swarm 中运行良好——它连接到 Docker API 来构建其前端路由映射，并以与在运行 Docker Engine 的单个服务器上完全相同的方式代理来自后端容器的内容。要在群集模式下向 Traefik 注册服务，您还需要告诉 Traefik 容器中的应用正在使用哪个端口，因为它自己无法确定。REST API 服务定义增加了`traefik.port`标签:
```
docker service create `
 --network nd-swarm `
 --env-file db-credentials.env `
 --name nerd-dinner-api `
 --label "traefik.frontend.rule=Host:api.nerddinner.swarm" `
 --label "traefik.port=80" `
 dockeronwindows/ch05-nerd-dinner-api:2e
```
Traefik 本身是最复杂的创建服务，在 swarm 模式下有几个额外的选项:
```
docker service create `
 --detach=true `
 --network nd-swarm `
 --constraint=node.role==manager `
 --publish 80:80 --publish 8080:8080 `
 --mount type=bind,source=C:\certs\client,target=C:\certs `
 --name reverse-proxy `
 sixeyed/traefik:v1.7.8-windowsservercore-ltsc2019 `
 --docker --docker.swarmMode --docker.watch `
 --docker.endpoint=tcp://win2019-dev-02:2376 `
 --docker.tls.ca=/certs/ca.pem `
 --docker.tls.cert=/certs/cert.pem `
 --docker.tls.key=/certs/key.pem `
 --api
```
您只能从管理器节点上运行的 Docker API 获取有关集群服务的信息，这就是为什么您需要将 Docker CLI 连接到管理器来使用集群资源。服务的`constraint`选项确保 Docker 将只调度容器在满足约束的节点上运行。在这种情况下，服务副本将仅在管理器节点上运行。这不是唯一的选择-如果您已经配置了对 Docker API 的安全远程访问，您可以在工作节点上运行 Traefik。
为了将 Traefik 连接到 Docker API，我之前使用了一个卷来挂载名为*管道*的窗口，但是 Docker Swarm 还不支持这个特性。因此，相反，我使用一个到应用编程接口的 TCP 连接，指定管理器的域名`win2019-dev-02`。我已经用 TLS 保护了我的 Docker 引擎(正如我在[第 1 章](01.html)、*中解释的那样)在 Windows 上开始使用 Docker*，因此我还提供了客户端证书来安全地使用连接。证书存储在我的管理器节点`C:\certs\client`中，我将其作为容器内的一个目录挂载。
*Named pipe support for service mounts* means you can use the approach of mounting the pipe, which is much easier, as you don't need to specify the host name of the manager, or supply the TLS certificates. That feature is planned for Docker 19.03, and will probably be available by the time you read this book. The great thing about Docker is that it's built from open source components, so features such as this are all discussed in the open—[https://github.com/moby/moby/issues/34795](https://github.com/moby/moby/issues/34795) will tell you the background and the current status.
当我在我的集群上运行脚本时，我得到一个服务标识列表作为输出:
```
> .\ch07-run-nerd-dinner.ps1
206teuqo1v14m3o88p99jklrn
vqnncr7c9ni75ntiwajcg05ym
2pzc8c5rahn25l7we3bzqkqfo
44xsmox6d8m480sok0l4b6d80
u0uhwiakbdf6j6yemuy6ey66p
v9ujwac67u49yenxk1albw4bt
s30phoip8ucfja45th5olea48
24ivvj205dti51jsigneq3z8q
beakbbv67shh0jhtolr35vg9r
sc2yzqvf42z4l88d3w31ojp1c
vx3zyxx2rubehee9p0bov4jio
rl5irw1w933tz9b5cmxyyrthn
```
现在我可以用`docker service ls`看到所有正在运行的服务:
```
> docker service ls
ID           NAME          MODE       REPLICAS            IMAGE 
8bme2svun122 message-queue             replicated 1/1      nats:nanoserver
deevh117z4jg nerd-dinner-homepage      replicated 1/1      dockeronwindows/ch03-nerd-dinner-homepage...
lxwfb5s9erq6 nerd-dinner-db            replicated 1/1      dockeronwindows/ch06-nerd-dinner-db:latest
ol7u97cpwdcn nerd-dinner-index-handler replicated 1/1      dockeronwindows/ch05-nerd-dinner-index...
rrgn4n3pecgf elasticsearch             replicated 1/1      sixeyed/elasticsearch:nanoserver
w7d7svtq2k5k nerd-dinner-save-handler  replicated 1/1      dockeronwindows/ch05-nerd-dinner-save...
ydzb1z1af88g nerd-dinner-web           replicated 1/1      dockeronwindows/ch05-nerd-dinner-web:latest
ywrz3ecxvkii kibana                    replicated 1/1      sixeyed/kibana:nanoserver
```
每个服务都被列为副本状态为`1/1`，这意味着一个副本的请求服务级别已用完。这是用于运行服务的容器数量。Swarm 模式支持两种类型的分布式服务:复制服务和全局服务。默认情况下，分布式服务只有一个副本，这意味着群集中有一个容器。我的脚本中的`service create`命令没有指定副本数量，所以它们都使用默认的*1*。
# 跨多个容器运行服务
复制服务是您在群集模式下扩展的方式，您可以更新正在运行的服务以添加或删除容器。与 Docker Compose 不同，您不需要定义每个服务所需状态的 Compose 文件；从`docker service create`命令开始，该细节已经存储在群中。要添加更多的消息处理程序，我使用`docker service scale`，传递一个或多个服务的名称和所需的服务级别:
```
> docker service scale nerd-dinner-save-handler=3
nerd-dinner-save-handler scaled to 3
overall progress: 1 out of 3 tasks
1/3: starting  [============================================>      ]
2/3: starting  [============================================>      ]
3/3: running   [==================================================>]
```
消息处理程序服务是用默认的单个副本创建的，因此这又增加了两个容器来共享 SQL Server 处理程序服务的工作。在多节点群中，管理器可以安排容器在任何有容量的节点上运行。我不需要知道或者关心哪个服务器实际上在运行容器，但是我可以通过`docker service ps`深入到服务列表中查看容器运行的位置:
```
> docker service ps nerd-dinner-save-handler
ID      NAME    IMAGE  NODE            DESIRED STATE  CURRENT STATE 
sbt4c2jof0h2  nerd-dinner-save-handler.1 dockeronwindows/ch05-nerd-dinner-save-handler:2e    win2019-dev-02      Running             Running 23 minutes ago
bibmh984gdr9  nerd-dinner-save-handler.2 dockeronwindows/ch05-nerd-dinner-save-handler:2e    win2019-dev-02      Running             Running 3 minutes ago
3lkz3if1vf8d  nerd-dinner-save-handler.3 dockeronwindows/ch05-nerd-dinner-save-handler:2e   win2019-02           Running             Running 3 minutes ago
```
在这种情况下，我运行的是一个双节点群，副本在节点`win2019-dev-02`和`win2019-02`之间分割。Swarm 模式将服务过程称为副本，但它们实际上只是容器。您可以像往常一样，使用相同的`docker ps`、`docker logs`和`docker top`命令登录群的节点并管理服务容器。
通常，你不会这么做。运行副本的节点只是群为你管理的黑盒；您通过管理器节点使用您的服务。正如 Docker Compose 提供了一个服务日志的统一视图一样，您可以从连接到集群管理器的 Docker CLI 中获得相同的视图:
```
PS> docker service logs nerd-dinner-save-handler
nerd-dinner-save-handler.1.sbt4c2jof0h2@win2019-dev-02
    | Connecting to message queue url: nats://message-queue:4222
nerd-dinner-save-handler.1.sbt4c2jof0h2@win2019-dev-02
    | Listening on subject: events.dinner.created, queue: save-dinner-handler
nerd-dinner-save-handler.2.bibmh984gdr9@win2019-dev-02
    | Connecting to message queue url: nats://message-queue:4222
nerd-dinner-save-handler.2.bibmh984gdr9@win2019-dev-02
    | Listening on subject: events.dinner.created, queue: save-dinner-handler
...
```
副本是群如何为服务提供容错的。当您使用`docker service create`、`docker service update`或`docker service scale`命令指定服务的副本级别时，该值会记录在群中。管理器节点监控服务的所有任务。如果容器停止，并且正在运行的服务数量低于所需的副本级别，则计划新任务来替换停止的容器。在本章的后面，我将演示当我在多节点群上运行相同的解决方案时，我可以从群中取出一个节点，而不会导致任何服务损失。
# 全球服务
复制服务的替代方案是**全球服务**。在某些情况下，您可能希望群的每个节点上运行的服务与每个服务器上的单个容器相同。为此，您可以在全局模式下运行服务—Docker 在每个节点上恰好调度一个任务，任何加入的新节点也将有一个任务被调度。
全局服务对于许多服务使用的组件的高可用性非常有用，但是，同样，您不能仅仅通过运行许多集群应用的实例来获得集群应用。NATS 消息队列可以作为跨多个服务器的集群运行，并且它可能是作为全局服务运行的一个很好的候选。但是，要将 NATS 作为一个集群运行，每个实例都需要知道其他实例的地址，这对于 Docker Engine 分配的动态虚拟 IP 地址来说效果并不好。
相反，我可以将我的弹性搜索消息处理程序作为一个全局服务来运行，这样每个节点都将有一个消息处理程序的实例在运行。您不能更改正在运行的服务的模式，因此，首先，我需要删除原始服务:
```
> docker service rm nerd-dinner-index-handler
nerd-dinner-index-handler 
```
然后，我可以创建一个新的全球服务:
```
> docker service create `
>>  --mode=global `
>>  --network nd-swarm `
>>  --name nerd-dinner-index-handler `
>>  dockeronwindows/ch05-nerd-dinner-index-handler:2e;
q0c20sx5y25xxf0xqu5khylh7
overall progress: 2 out of 2 tasks
h2ripnp8hvty: running   [==================================================>]
jea4p57ajjal: running   [==================================================>]
verify: Service converged 
```