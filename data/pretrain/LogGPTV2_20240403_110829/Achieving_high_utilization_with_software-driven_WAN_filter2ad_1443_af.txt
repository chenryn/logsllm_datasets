link speeds and larger WANs means that future switches
may need even more rules.
If switches support 1K rules,
k-shortest path routing is unable to use 10% of the network
capacity.
In contrast, SWAN’s dynamic tunnels approach
enables it to fully use network capacity with an order of
magnitude fewer rules. This ﬁts within the capabilities of
current-generation switches.
Figure 16 (right) shows the number of stages needed to
dynamically change tunnels. It assumes a limit of 750 Open-
Flow rules, which is what our testbed switches support.
With 10% slack only two stages are needed 95% of the time.
This nimbleness stems from 1) the eﬃciency of dynamic
tunnels—a small set of rules are needed per interval, and
2) temporal locality in demand matrices—this set changes
slowly across adjacent intervals.
6.6 Other microbenchmarks
We close our evaluation of SWAN by reporting on some
key microbenchmarks.
Update time: Figure 17 shows the time to update IDN
from the start of a new epoch. Our controller uses a PC with
a 2.8GHz CPU and runs unoptimized code. The left graph
shows a CDF across all updates. The right graph depicts a
timeline of the average time spent in various parts. Most up-
dates ﬁnish in 22s; most of this time goes into waiting for ser-
vice rate limits to take eﬀect, 10s each to wait for services to
reduce their rate (t1 to t3) and then for those whose rate in-
creases (t4 to t5). SWAN computes the congestion-controlled
plan in parallel with the ﬁrst of these. The network’s data
plane is in ﬂux for only 600 ms on average (t3 to t4). This
includes communication delay from controller to switches
and the time to update rules at switches, multiplied by the
number of stages required to bound congestion.
If SWAN
were used in a network without explicit resource signaling,
the average update time would only be this 600 ms.
Traﬃc carried during updates: During updates, SWAN
ensures that the network continues to maintain high utiliza-
tion. That the overall network utilization of SWAN comes
close to optimal (§6.2) is an evidence of this behavior. More
directly, Figure 18a shows the %-age of traﬃc that SWAN
carries during updates compared to an optimal method with
instantaneous updates. The median value is 96%.
Update frequency: Figure 18b shows that frequent up-
dates to the network’s data plane lead to higher eﬃciency.
It plots the drop in throughput as the update duration is
increased. The service demands still change every 5 minutes
but the network data plane updates at the slower rate (x-
axis) and the controller allocates as much traﬃc as the cur-
rent data plane can carry. We see that an update frequency
of 10 (100) minutes reduces throughput by 5% (30%).
Prediction error for interactive traﬃc: The error in
predicting interactive traﬃc demand is small. For only 1%
of the time, the actual amount of interactive traﬃc on a link
diﬀers from the predicted amount by over 1%.
7. DISCUSSION
This section discusses several issues that, for conciseness,
were not mentioned in the main body of the paper.
Non-conforming traﬃc: Sometimes services may (e.g.,
due to bugs) send more than what is allocated. SWAN can
detect these situations using traﬃc logs that are collected
from switches every 5 minutes. It can then notify the owners
of the service and protect other traﬃc by re-marking the
DSCP bits of non-conﬁrming traﬃc to a class that is even
lower than background traﬃc, so that it’s carried only if
there is any spare capacity.
Truthful declaration: Services may declare their lower-
priority traﬃc as higher priority or ask for more bandwidth
than they can consume. SWAN discourages this behavior
through appropriate pricing: services pay more for higher
priority and pay for all allocated resources. (Even within
a single organization, services pay for the infrastructure re-
sources they consume.)
Richer service-network interface: Our current design
has a simple interface between the services and network,
based on current bandwidth demand.
In future work, we
will consider a richer interface such as letting services reserve
resources ahead of time and letting them express their needs
in terms of total bytes and a deadline by which they must
be transmitted. Better knowledge of such needs can further
boost eﬃciency, for instance, by enabling store-and-forward
transfers through intermediate DCs [21]. The key challenge
here is the design of scalable and fair allocation mechanisms
that composes the diversity of service needs.
8. RELATED WORK
SWAN builds upon several themes in prior work.
Intra-DC traﬃc management: Many recent works
manage intra-DC traﬃc to better balance load [1, 7, 8] or
share among selﬁsh parties [16, 28, 31]. SWAN is similar
to the former in using centralized TE and to the latter in
providing fairness. But the intra-DC case has constraints
and opportunities that do not translate to the WAN. For
example, EyeQ [16] assumes that the network has a full bi-
section bandwidth core and hence only paths to or from the
core can be congested; this need not hold for a WAN. Sea-
wall [31] uses TCP-like adaptation to converge to fair share,
but high RTTs on the WAN would mean slow convergence.
Faircloud [28] identiﬁes strategy-proof sharing mechanisms,
i.e., resilient to the choices of individual actors. SWAN uses
explicit resource signaling to disallow such greedy actions.
 0 0.2 0.4 0.6 0.8 1 21 22 23Cumulative frac.of updatesUpdate time [s] 0 5 10 15 20 25Update time [s]t1t2t3t4t51.30.7100.610Compute allocation & rule change planCompute congestion-controlled planWait for rate limitingChange switch rulesRatelimiting 0 0.2 0.4 0.6 0.8 150%100%Cumulativefrac. ofupdatesTraﬃc carried by SWANduring updates / traﬃccarried if updateswere instantaneous 0.5 0.6 0.7 0.8 0.9 1 1 10 100 1000Throughput[normalized to5-min interval]Update interval [min]25Signaling also helps it avoid estimating demands which other
centralized TE schemes have to do [1, 8].
WAN TE & SDN: As in SWAN, B4 uses SDNs in the
context of inter-DC WANs [15]. Although this parallel work
shares a similar high-level architecture, it addresses diﬀerent
challenges. While B4 develops custom switches and mech-
anisms to integrate existing routing protocols in an SDN
environment, SWAN develops mechanisms for congestion-
free data plane updates and for eﬀectively using the limited
forwarding table capacity of commodity switches.
Optimizing WAN eﬃciency has rich literature including
tuning ECMP weights [12], adapting allocations across pre-
established tunnels [10, 17], storing and re-routing bulk data
at relay nodes [21], caching at application-layer [32] and
leveraging reconﬁgurable optical networks [22]. While such
bandwidth eﬃciency is one of the design goals, SWAN also
addresses performance and bandwidth requirements of dif-
ferent traﬃc classes. In fact, SWAN can help many of these
systems by providing available bandwidth information and
by oﬀering routes through the WAN that may not be dis-
covered by application-layer overlays.
Guarantees during network update: Some recent work
provides guarantees during network updates either on con-
nectivity, or loop-free paths or that a packet will see a con-
sistent set of SDN rules [19, 23, 29, 34]. SWAN oﬀers a
stronger guarantee that the network remains uncongested
during forwarding rule changes. Vanbever et. al. [34] sug-
gest ﬁnding an ordering of updates to individual switches
that is guaranteed to be congestion free; however, we see
that such ordering may not exist (§6.4) and is unlikely to
exist when the network operates at high utilization.
9. CONCLUSIONS
SWAN enables a highly eﬃcient and ﬂexible inter-DC
WAN by coordinating the sending rates of services and cen-
trally conﬁguring the network data plane. Frequent network
updates are needed for high eﬃciency, and we showed how,
by leaving a small amount of scratch capacity on the links
and switch rule memory, these updates can be implemented
quickly and without congestion or disruption. Testbed ex-
periments and data-driven simulations show that SWAN can
carry 60% more traﬃc than the current practice.
Acknowledgements. We thank Rich Groves, Parantap
Lahiri, Dave Maltz, and Lihua Yuan for feedback on the
design of SWAN. We also thank Matthew Caesar, Brighten
Godfrey, Nikolaos Laoutaris, John Zahorjan, and the SIG-
COMM reviewers for feedback on earlier drafts of the paper.
10. REFERENCES
[1] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic ﬂow scheduling for data center
networks. In NSDI, 2010.
[2] D. Applegate and M. Thorup. Load optimal MPLS routing
with N+M labels. In INFOCOM, 2003.
[3] D. Awduche, L. Berger, D. Gan, T. Li, V. Srinivasan, and
G. Swallow. RSVP-TE: Extensions to RSVP for LSP tunnels.
RFC 3209, 2001.
[4] D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, and
J. McManus. Requirements for traﬃc engineering over MPLS.
RFC 2702, 1999.
[5] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron.
Towards predictable datacenter networks. In SIGCOMM, 2011.
[6] Y. Chen, S. Jain, V. K. Adhikari, Z.-L. Zhang, and K. Xu. A
ﬁrst look at inter-data center traﬃc characteristics via Yahoo!
datasets. In INFOCOM, 2011.
[7] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and I. Stoica.
Managing data transfers in computer clusters with Orchestra.
In SIGCOMM, 2011.
[8] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula,
P. Sharma, and S. Banerjee. DevoFlow: Scaling ﬂow
management for high-performance networks. In SIGCOMM,
2011.
[9] E. Danna, S. Mandal, and A. Singh. A practical algorithm for
balancing the max-min fairness and throughput objectives in
traﬃc engineering. In INFOCOM, 2012.
[10] A. Elwalid, C. Jin, S. Low, and I. Widjaja. MATE: MPLS
adaptive traﬃc engineering. In INFOCOM, 2001.
[11] Project Floodlight. http://www.projectfloodlight.org/.
[12] B. Fortz, J. Rexford, and M. Thorup. Traﬃc engineering with
traditional IP routing protocols. IEEE Comm. Mag., 2002.
[13] T. Hartman, A. Hassidim, H. Kaplan, D. Raz, and M. Segalov.
How to split a ﬂow? In INFOCOM, 2012.
[14] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill,
M. Nanduri, and R. Wattenhofer. Achieving high utilization
with software-driven WAN (extended version). Microsoft
Research Technical Report 2013-54, 2013.
[15] S. Jain et al. B4: Experience with a globally-deployed software
deﬁned WAN. In SIGCOMM, 2013.
[16] V. Jeyakumar, M. Alizadeh, D. Mazi`eres, B. Prabhakar, and
C. Kim. EyeQ: Practical network performance isolation for the
multi-tenant cloud. In HotCloud, 2012.
[17] S. Kandula, D. Katabi, B. Davie, and A. Charny. Walking the
tightrope: Responsive yet stable traﬃc engineering. In
SIGCOMM, 2005.
[18] S. Kandula, D. Katabi, S. Sinha, and A. Berger. Dynamic load
balancing without packet reordering. SIGCOMM CCR, 2007.
[19] N. Kushman, S. Kandula, D. Katabi, and B. M. Maggs. R-BGP:
Staying connected in a connected world. In NSDI, 2007.
[20] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide,
and F. Jahanian. Internet inter-domain traﬃc. SIGCOMM
Comput. Commun. Rev., 2010.
[21] N. Laoutaris, M. Sirivianos, X. Yang, and P. Rodriguez.
Inter-datacenter bulk transfers with NetStitcher. In
SIGCOMM, 2011.
[22] A. Mahimkar, A. Chiu, R. Doverspike, M. D. Feuer, P. Magill,
E. Mavrogiorgis, J. Pastor, S. L. Woodward, and J. Yates.
Bandwidth on demand for inter-data center communication. In
HotNets, 2011.
[23] R. McGeer. A safe, eﬃcient update protocol for OpenFlow
networks. In HotSDN, 2012.
[24] M. Meyer and J. Vasseur. MPLS traﬃc engineering soft
preemption. RFC 5712, 2010.
[25] V. S. Mirrokni, M. Thottan, H. Uzunalioglu, and S. Paul. A
simple polynomial time framework for reduced-path
decomposition in multi-path routing. In INFOCOM, 2004.
[26] D. Nace, N.-L. Doan, E. Gourdin, and B. Liau. Computing
optimal max-min fair resource allocation for elastic ﬂows.
IEEE/ACM Trans. Netw., 2006.
[27] A. Pathak, M. Zhang, Y. C. Hu, R. Mahajan, and D. Maltz.
Latency inﬂation with MPLS-based traﬃc engineering. In IMC,
2011.
[28] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy,
S. Ratnasamy, and I. Stoica. FairCloud: Sharing the network in
cloud computing. In SIGCOMM, 2012.
[29] M. Reitblatt, N. Foster, J. Rexford, C. Schlesinger, and
D. Walker. Abstractions for network update. In SIGCOMM,
2012.
[30] M. Roughan, A. Greenberg, C. Kalmanek, M. Rumsewicz,
J. Yates, and Y. Zhang. Experience in measuring backbone
traﬃc variability: Models, metrics, measurements and meaning.
In Internet Measurement Workshop, 2002.
[31] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha.
Sharing the data center network. In NSDI, 2011.
[32] S. Traverso, K. Huguenin, I. Trestian, V. Erramilli,
N. Laoutaris, and K. Papagiannaki. Tailgate: handling long-tail
content with a little help from friends. In WWW, 2012.
[33] Broadcom Trident II series. http://www.broadcom.com/docs/
features/StrataXGS_Trident_II_presentation.pdf, 2012.
[34] L. Vanbever, S. Vissicchio, C. Pelsser, P. Francois, and
O. Bonaventure. Seamless network-wide IGP migrations. In
SIGCOMM, 2011.
[35] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better
never than late: Meeting deadlines in datacenter networks. In
SIGCOMM, 2011.
[36] M. Zhang, B. Karp, S. Floyd, and L. Peterson. RR-TCP: A
reordering-robust TCP with DSACK. In ICNP, 2003.
26