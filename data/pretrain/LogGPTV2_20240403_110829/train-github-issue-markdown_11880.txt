### System information
  * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** : N/A
  * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** : N/A
  * **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device** : N/A
  * **TensorFlow installed from (source or binary)** : N/A
  * **TensorFlow version (use command below)** : N/A
  * **Python version** : N/A
  * **Bazel version (if compiling from source)** : N/A
  * **GCC/Compiler version (if compiling from source)** : N/A
  * **CUDA/cuDNN version** : N/A
  * **GPU model and memory** : N/A
  * **Exact command to reproduce** : N/A
### Describe the problem
Currently, the TensorFlow Python library is released in two flavours: CPU-only
and CPU/GPU. The latter is dynamically linked with CUDA libraries and fails to
load if they are not available in the linker path.
    ImportError: dlopen([...]/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
      Referenced from: [...]/tensorflow/python/_pywrap_tensorflow_internal.so
      Reason: image not found
I suspect that it might be possible to load CUDA libraries dynamically at
runtime. This would allow having a single TensorFlow build which attempts to
load CUDA, and if it is not available, falls back to CPU-only ops.
It hard for me to estimate the feasibility of the proposal for the current
TensorFlow runtime implementation, so it might as well be the case, that the
proposed change is too big/intrusive to be practical.