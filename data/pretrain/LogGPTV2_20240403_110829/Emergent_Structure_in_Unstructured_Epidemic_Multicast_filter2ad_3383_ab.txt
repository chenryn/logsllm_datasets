upon RECEIVE(IHAVE(i), s) do
if i 6∈ R then
QUEUE(i, s)
upon RECEIVE(MSG(i, d, r), s) do
if i 6∈ R then
R = R ∪ {i}
CLEAR(i)
L-RECEIVE(i, d, r, s)
33
34
35
36 Task 2:
37
38
39
upon RECEIVE(IWANT(i), s) do
(d, r) = C[i]
SEND(MSG(i, d, r), p)
forever do
(i, s) = SCHEDULENEXT()
SEND(IWANT(i), s)
Figure 3. Point-to-point communication.
• EAGER?(i, d, r, p) is used to determine if payload d
for message with identiﬁcation i on round r should be
immediately sent to peer p. Note that if the method
always returns true the protocol operates on pure eager
push mode.
If the method always returns false, the
protocol operates on pure lazy push mode.
• (i, s) = SCHEDULENEXT() blocks until it is the time
for some message i to be requested from a source s.
From the correctness point of view, any schedule is
safe as long as it eventually schedules all lazy requests
that have been queued.
The Lazy Point-to-Point module also informs the Trans-
mission Strategy of known sources for each message and
when payload has been received using the following primi-
tives:
• QUEUE(i, s) queues a message identiﬁer i to be re-
quested from source node s. The Transmission Strat-
egy module must keep an internal queue of known
sources for each message identiﬁer in order to sched-
ule them eventually, unless payload is received ﬁrst.
• CLEAR(i) clears all requests on message i. Note also
that a queue eventually clears itself as requests on all
known sources for a given message identiﬁer i are
scheduled.
The Lazy Point-to-Point module is depicted in Fig. 3 and
uses two separate tasks. Task 1 is responsible for processing
transmission requests from the gossip layer and message de-
liveries from the transport layer. Task 2 runs in background,
and performs requests for messages that are known to exists
but whose payload has not been received yet. Furthermore,
the module maintains the following data structures: a set R
of messages whose payload has been received and; a map
C, holding the payload and round number for the message
(if known).
The module operates as follows. When a message is sent
(line 19), the Transmission Strategy module is queried to
test if the message should be immediately sent (line 21). If
not, an advertisement without the payload is sent instead
(line 24). Upon receiving a message advertisement for an
unknown message, the Transmission Strategy module is no-
tiﬁed (line 27). Upon receiving full message payload, the
strategy module is informed (line 31) and it is handed over
to the gossip layer (line 32).
Task 2 executes the following loop. The Transmission
Strategy module is invoked to select a message to be re-
quested and a node to request the message from (pair (i, s)
in line 38). This invocation blocks until a request is sched-
uled to be sent by the Transmission Strategy module. A
request is then sent (line 39). Finally, when a node receives
a request (line 33) it looks it up in the cache and transmits
the payload (line 35). Note that a retransmission request
can only be received as a consequence of a previous adver-
tisement and thus the message is guaranteed to be locally
known.
For simplicity, we again do not show how cached iden-
tiﬁers and payloads are removed from C and R, preventing
them from growing indeﬁnitely. This is however similar to
the management of set K, discussed in the previous section,
and thus the same techniques apply.
Finally, the goal of the performance monitor module is
to measure relevant performance metrics of the participant
nodes and to make this information available to the strat-
egy in an abstract manner. The exported interface of this
module is simply METRIC(p), that returns a current metric
for a given peer p. This metric is used by the Transmission
Strategy to select whether to immediately schedule an ea-
ger transmission or when to request lazy transmission from
each source. Note that, the performance monitor module
may be required to exchange messages with its peers (for
instance, to measure roundtrip delays).
The next section discusses different implementations of
the Transmission Strategy and of the Performance Monitor
modules that aim at achieving different dissemination struc-
tures.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20074. Strategies and Monitors
The deﬁnition of a Transmission Strategy has two main
(i) avoid as much as possible the redundant
objectives:
transmission of the same payload to any given target node;
and (ii) decrease the latency of message delivery. These
goals are however conﬂicting.
The ﬁrst goal can be
achieved by using a lazy push strategy in all peer exchanges.
Since nodes only gossip IHAVE commands, the recipient
can request the payload only once. Unfortunately, each lazy
push exchange adds one additional roundtrip to the ﬁnal de-
livery latency. On the other hand, a pure eager push strategy
minimizes latency at the cost of adding a signiﬁcant amount
of redundancy.
The key to obtaining a better latency/bandwidth tradeoff
is thus to select nodes and links that should be preferred
in a decentralized fashion. We start however by proposing a
couple of strategies that do not take advantage of knowledge
about the environment for use as a baseline.
4.1. Strategies
Flat This strategy is deﬁned as EAGER?(i, d, r, p) return-
ing true with some probability π or false with probability
1 − π. When π = 1, this defaults to a fully eager push
gossip. With π = 0, it provides pure lazy push gossip. In
between, it provides different latency/bandwidth tradeoffs,
as a different share of gossip messages is handled in a lazy
fashion.
When a lazy strategy is used (and IHAVE messages are
sent), we need also to consider how retransmissions are
scheduled within SCHEDULENEXT() by receivers. In the
Flat strategy, the ﬁrst retransmission request is scheduled
immediately when queued, which means that an IWANT
message is issued immediately upon receiving an IHAVE
advertisement. Further requests are done periodically every
T , while sources are known.
Time T is an estimate of maximum end-to-end latency.
This avoids issuing explicit transmission requests until all
eager transmissions have been performed, thus optimizing
bandwidth. Note that, unless there is a network omission or
an extreme transmission delay, there is usually no need to
issue a second request. Thus the value of T has no practi-
cal impact in the ﬁnal average latency, and can be set only
approximately.
in a lazy fashion. SCHEDULEDNEXT() is deﬁned exactly
as in the Flat strategy.
We propose this because it is intuitively useful: During
the ﬁrst rounds, the likelihood of a node being targeted by
more than one copy of the payload is small and thus there is
no point in using lazy push.
Radius This strategy is deﬁned as EAGER?(i, d, r, p) re-
turning true iff METRIC(p)  t, this defaults to common
eager push gossip. With u = 0, it provides pure lazy push
gossip. In between, it provides different latency/bandwidth
tradeoffs, as a different share of gossip messages is handled
Distance Monitor This monitor measures geographical
distance to all neighbor nodes. This is useful mostly for
demonstration purposes, as it allows us to plot network us-
age graphs such that emergent structure is understandable
by the reader. Otherwise, it is not useful in optimizing net-
work parameters.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007(a) Flat (7% of trafﬁc).
(b) Radius (37% of trafﬁc).
(c) Ranked (30% of trafﬁc).
Figure 4. Emergent structure deﬁned by the top 5% most used connections.
4.3. Approximation and Noise
When evaluating the proposed protocol on the ModelNet
emulated network as described in the following sections,
the proposed strategies and monitors are simpliﬁed by re-
lying on global knowledge of the network that is extracted
directly from the model ﬁle. This has two goals: First, it
allows us to separate the performance of the the proposed
strategy from the performance of the monitor. Second, it al-
lows us to arbitrarily introduce noise in Transmission Strat-
egy in order to evaluate its robustness.
In detail, we intercept each query to EAGER? and use a
temporary variable v as follows. If EAGER? would return
true, we set v = 1.0; if it would return false we set v = 0.0.
We then compute v0 = c+(v−c)(1−o). We then generating
a random boolean with probability v0 of being true as the
outcome of the query. Constant c is set such that the overall
probability of EAGER? returning true is unchanged, thus
leading to the same amount of eager transmissions although
scheduled in different occasions.
When noise ratio o = 0.0, the original result is un-
changed. When noise ratio o = 1.0, any Transmission Strat-
egy defaults to Flat with π = c, thus completely erasing
structure. In between, the Transmission Strategy produces
an increasingly blurred structure.
5. Experimental Environment
5.1. Network Emulation
Experimental evaluation of the proposed protocol is
done using the ModelNet large-scale emulation infrastruc-
ture [21] with a realistic network model generated by Inet-
3.0 [22]. In detail, ModelNet allows a large number of vir-
tual nodes running unmodiﬁed programs to be conﬁgured
in a smaller number of physical nodes in a LAN. Trafﬁc is
routed through emulator nodes thus applying delay, band-
width, and loss as speciﬁed in the network model.
Inet-
3.0 generates realistic Autonomous System level network
topologies using a transit-stub model.
ModelNet is deployed in a cluster of 5 workstations con-
nected by switched 100Mbps Ethernet. Each workstation
has a 2.4GHz Intel Celeron CPU, 512MB RAM, and a Re-
alTek Ethernet controller. When hosting virtual nodes, they
run Linux kernel 2.6.14 and IBM Java2 1.5 runtime. When
running as an emulator, FreeBSD 4.11 is used.
The network model is generated using Inet-3.0 default of
3037 network nodes. Link latency is assigned by ModelNet
according to pseudo-geographical distance. Client nodes
are assigned to distinct stub nodes, also with the default
1 ms client-stub latency. A typical network graph has the
following properties: average hop distance between client
nodes is 5.54, with 74.28% of nodes within 5 and 6 hops;
average end-to-end latency of 49.83 ms, with 50% of nodes
within 39 ms and 60 ms.
5.2. Implementation and Conﬁguration
The proposed protocol was implemented by modify-
ing an open source and lightweight implementation of the
NeEM protocol [16] that uses the java.nio API for scala-
bility and performance [20]. Brieﬂy, NeEM uses TCP/IP
connections between nodes in order to avoid network con-
gestion. When a connection blocks, messages are buffered
in user space, which then uses a custom purging strategy to
improve reliability. The result is a virtual connection-less
layer that provides improved guarantees for gossiping.
This implementation was selected as NeEM 0.5 already
supports eager and lazy push, although the later is selected
only based on a message size and age threshold. The change
required was to remove the hard-coded push strategy and
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007insert the scheduler layer. Message identiﬁers are proba-
bilistically unique 128 bit strings.
The protocol was conﬁgured with gossip fanout of 11
and overlay fanout of 15. With 200 nodes, these correspond
to a probability 0.995 of atomic delivery with 1% messages
dropped, and a probability of 0.999 of connectedness when
15% of nodes fail [6]. A retransmission period of 400 ms
was used, which is the minimal that results in approximately
1 payload received by each destination when using a fully
lazy push strategy.
5.3. Traﬃc and Measurements
During each experiment, 400 messages are multicast,
each carrying 256 bytes of application level payload. To
each of them, a NeEM header of 24 bytes is added, besides
TCP/IP overhead. Messages are multicast by virtual nodes
in a round-robin fashion, with an uniform random interval
with 500 ms average. All messages multicast and deliv-
ered are logged for later processing. Namely, end-to-end
latency can be measured when source and destination share
the same physical node, and thus a common clock. Payload
transmissions on each link are also recorded separately.
Results presented in the following sections used 25 vir-
tual nodes on each workstation, thus obtaining 100 vir-
tual nodes. The reason for this limitation is that an epi-
demic multicast protocol produces a bursty load, in partic-
ular when using eager push gossip: Network and CPU load
occurs only instants after each message is multicast. Us-
ing a larger number of virtual nodes was observed to induce
additional latency which would falsify results. The conﬁg-
urations that result in lower bandwidth consumption, which
are the key results of this paper, were also simulated with
200 virtual nodes.
5.4. Statistics
Consider the following statistics of each experiment with
100 virtual nodes using an eager push strategy: 40000
messages delivered, 440000 individual packets transmit-
ted. This amounts to 2200 Kpkts/s and thus approximately
6 MBps. As the overlay evolves, TCP/IP connections are
created and tear down. During each run, approximately
550 simultaneous and 15000 different connections are used.
The experiments presented in the next sections, when au-
tomated, take almost 7 hours to run. The total amount of
resulting logs is 1Gb, that has then to be processed and ren-
dered in plots.
Care was taken to consider variance of each measure
taken. When in the following sections we afﬁrm that a
performance difference is relevant, this was conﬁrmed by
checking that conﬁdence intervals with 95% certainty do
not intersect. In fact, the large number of samples used are
sufﬁcient to make such intervals very narrow.