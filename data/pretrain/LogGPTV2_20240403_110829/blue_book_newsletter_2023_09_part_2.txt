feat(bash_snippets)
    Create the `/usr/share/glib-2.0/schemas/90_ubuntu-settings.gschema.override` file with the next content:
    ```ini
    [org.gnome.desktop.screensaver]
    lock-enabled = false
    [org.gnome.settings-daemon.plugins.power]
    idle-dim = false
    ```
    Then reload the schemas with:
    ```bash
    sudo glib-compile-schemas /usr/share/glib-2.0/schemas/
    ```
## Storage
### [OpenZFS](zfs.md)
* New: [Repair a DEGRADED pool.](zfs.md#repair-a-degraded-pool)
    First let’s offline the device we are going to replace:
    ```bash
    zpool offline tank0 ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx
    ```
    Now let us have a look at the pool status.
    ```bash
    zpool status
    NAME                                            STATE     READ WRITE CKSUM
    tank0                                           DEGRADED     0     0     0
      raidz2-1                                      DEGRADED     0     0     0
        ata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0
        ata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0
        ata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0
        ata-WDC_WD80EFZX-68UW8N0_xxxxxxxx           ONLINE       0     0     0
        ata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0
        ata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0
        ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx  OFFLINE      0     0     0
        ata-ST4000VX007-2DT166_xxxxxxxx             ONLINE       0     0     0
    ```
    Sweet, the device is offline (last time it didn't show as offline for me, but the offline command returned a status code of 0).
    Time to shut the server down and physically replace the disk.
    ```bash
    shutdown -h now
    ```
    When you start again the server, it’s time to instruct ZFS to replace the removed device with the disk we just installed.
    ```bash
    zpool replace tank0 \
        ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx \
        /dev/disk/by-id/ata-TOSHIBA_HDWG180_xxxxxxxxxxxx
    ```
    ```bash
    zpool status tank0
    pool: main
    state: DEGRADED
    status: One or more devices is currently being resilvered.  The pool will
            continue to function, possibly in a degraded state.
    action: Wait for the resilver to complete.
      scan: resilver in progress since Fri Sep 22 12:40:28 2023
            4.00T scanned at 6.85G/s, 222G issued at 380M/s, 24.3T total
            54.7G resilvered, 0.89% done, 18:28:03 to go
    NAME                                              STATE     READ WRITE CKSUM
    tank0                                             DEGRADED     0     0     0
      raidz2-1                                        DEGRADED     0     0     0
        ata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0
        ata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0
        ata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0
        ata-WDC_WD80EFZX-68UW8N0_xxxxxxxx             ONLINE       0     0     0
        ata-TOSHIBA_HDWG180_xxxxxxxxxxxx              ONLINE       0     0     0
        ata-TOSHIBA_HDWG180_xxxxxxxxxxxx              ONLINE       0     0     0
        replacing-6                                   DEGRADED     0     0     0
          ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx  OFFLINE      0     0     0
          ata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0  (resilvering)
        ata-ST4000VX007-2DT166_xxxxxxxx               ONLINE       0     0     0
    ```
    The disk is replaced and getting resilvered (which may take a long time to run (18 hours in a 8TB disk in my case).
    Once the resilvering is done; this is what the pool looks like.
    ```bash
    zpool list
    NAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
    tank0    43.5T  33.0T  10.5T     14.5T     7%    75%  1.00x  ONLINE  -
    ```
    If you want to read other blogs that have covered the same topic check out [1](https://madaboutbrighton.net/articles/replace-disk-in-zfs-pool).
* New: Stop a ZFS scrub.
    ```bash
    zpool scrub -s my_pool
    ```
# Operating Systems
## Linux
### [Linux Snippets](linux_snippets.md)
* New: [Limit the resources a docker is using.](linux_snippets.md#limit-the-resources-a-docker-is-using)
    You can either use limits in the `docker` service itself, see [1](https://unix.stackexchange.com/questions/537645/how-to-limit-docker-total-resources) and [2](https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html).
    Or/and you can limit it for each docker, see [1](https://www.baeldung.com/ops/docker-memory-limit) and [2](https://docs.docker.com/config/containers/resource_constraints/).
### [aleph](aleph.md)
* New: [Ingest gets stuck.](aleph.md#ingest-gets-stuck)
    It looks that Aleph doesn't yet give an easy way to debug it. It can be seen in the next webs:
    - [Improve the UX for bulk uploading and processing of large number of files](https://github.com/alephdata/aleph/issues/2124)
    - [Document ingestion gets stuck effectively at 100%](https://github.com/alephdata/aleph/issues/1839)
    - [Display detailed ingestion status to see if everything is alright and when the collection is ready](https://github.com/alephdata/aleph/discussions/1525)
    Some interesting ideas I've extracted while diving into these issues is that:
    - You can also upload files using the [`alephclient` python command line tool](https://github.com/alephdata/alephclient)
    - Some of the files might fail to be processed without leaving any hint to the uploader or the viewer.
      - This results in an incomplete dataset and the users don't get to know that the dataset is incomplete. This is problematic if the completeness of the dataset is crucial for an investigation.
      - There is no way to upload only the files that failed to be processed without re-uploading the entire set of documents or manually making a list of the failed documents and re-uploading them
      - There is no way for uploaders or Aleph admins to see an overview of processing errors to figure out why some files are failing to be processed without going through docker logs (which is not very user-friendly)
    - There was an attempt to [improve the way ingest-files manages the pending tasks](https://github.com/alephdata/aleph/issues/2127), it's merged into the [release/4.0.0](https://github.com/alephdata/ingest-file/tree/release/4.0.0) branch, but it has [not yet arrived `main`](https://github.com/alephdata/ingest-file/pull/423).
    There are some tickets that attempt to address these issues on the command line:
    - [Allow users to upload/crawl new files only](https://github.com/alephdata/alephclient/issues/34)
    - [Check if alephclient crawldir was 100% successful or not](https://github.com/alephdata/alephclient/issues/35)
    I think it's interesting either to contribute to `alephclient` to solve those issues or if it's complicated create a small python script to detect which files were not uploaded and try to reindex them and/or open issues that will prevent future ingests to fail.
### [gitsigns](gitsigns.md)
* New: Introduce gitsigns.
    [Gitsigns](https://github.com/lewis6991/gitsigns.nvim) is a neovim plugin to create git decorations similar to the vim plugin [gitgutter](https://github.com/airblade/vim-gitgutter) but written purely in Lua.
    Installation:
    Add to your `plugins.lua` file:
    ```lua
      use {'lewis6991/gitsigns.nvim'}
    ```
    Install it with `:PackerInstall`.
    Configure it in your `init.lua` with:
    ```lua
    -- Configure gitsigns
    require('gitsigns').setup({
      on_attach = function(bufnr)
        local gs = package.loaded.gitsigns
        local function map(mode, l, r, opts)
          opts = opts or {}
          opts.buffer = bufnr
          vim.keymap.set(mode, l, r, opts)
        end
        -- Navigation
        map('n', ']c', function()
          if vim.wo.diff then return ']c' end
          vim.schedule(function() gs.next_hunk() end)
          return ''
        end, {expr=true})
        map('n', '[c', function()
          if vim.wo.diff then return '[c' end
          vim.schedule(function() gs.prev_hunk() end)
          return ''
        end, {expr=true})
        -- Actions
        map('n', 'gs', gs.stage_hunk)
        map('n', 'gr', gs.reset_hunk)
        map('v', 'gs', function() gs.stage_hunk {vim.fn.line('.'), vim.fn.line('v')} end)
        map('v', 'gr', function() gs.reset_hunk {vim.fn.line('.'), vim.fn.line('v')} end)
        map('n', 'gS', gs.stage_buffer)
        map('n', 'gu', gs.undo_stage_hunk)
        map('n', 'gR', gs.reset_buffer)
        map('n', 'gp', gs.preview_hunk)
        map('n', 'gb', function() gs.blame_line{full=true} end)
        map('n', 'gb', gs.toggle_current_line_blame)
        map('n', 'gd', gs.diffthis)
        map('n', 'gD', function() gs.diffthis('~') end)
        map('n', 'ge', gs.toggle_deleted)
        -- Text object
        map({'o', 'x'}, 'ih', ':Gitsigns select_hunk')
      end
    })
    ```
    Usage:
    Some interesting bindings:
    - `]c`: Go to next diff chunk
    - `[c`: Go to previous diff chunk
    - `gs`: Stage chunk, it works both in normal and visual mode
    - `gr`: Restore chunk from index, it works both in normal and visual mode
    - `gp`: Preview diff, you can use it with `]c` and `[c` to see all the chunk diffs
    - `gb`: Show the git blame of the line as a shadowed comment
### [Diffview](diffview.md)
* New: [Use the same binding to open and close the diffview windows.](diffview.md#use-the-same-binding-to-open-and-close-the-diffview-windows)
    ```lua
    vim.keymap.set('n', 'dv', function()
      if next(require('diffview.lib').views) == nil then
        vim.cmd('DiffviewOpen')
      else
        vim.cmd('DiffviewClose')
      end
    end)
    ```
### [Grafana](grafana.md)
* Correction: Install grafana.
    ```yaml
    ---
    version: "3.8"
    services:
      grafana:
        image: grafana/grafana-oss:${GRAFANA_VERSION:-latest}
        container_name: grafana
        restart: unless-stopped
        volumes:
          - data:/var/lib/grafana
        networks:
          - grafana
          - monitorization
          - swag
        env_file:
          - .env
        depends_on:
          - db
      db:
        image: postgres:${DATABASE_VERSION:-15}
        restart: unless-stopped
        container_name: grafana-db
        environment:
          - POSTGRES_DB=${GF_DATABASE_NAME:-grafana}
          - POSTGRES_USER=${GF_DATABASE_USER:-grafana}
          - POSTGRES_PASSWORD=${GF_DATABASE_PASSWORD:?database password required}
        networks:
          - grafana
        volumes:
          - db-data:/var/lib/postgresql/data
        env_file:
          - .env
    networks:
      grafana:
        external:
          name: grafana
      monitorization:
        external:
          name: monitorization
      swag:
        external:
          name: swag
    volumes:
      data:
        driver: local
        driver_opts:
          type: none
          o: bind
          device: /data/grafana/app
      db-data:
        driver: local
        driver_opts:
          type: none
          o: bind
          device: /data/grafana/database
    ```
    Where the `monitorization` network is where prometheus and the rest of the stack listens, and `swag` the network to the gateway proxy.
    It uses the `.env` file to store the required [configuration](#configure-grafana), to connect grafana with authentik you need to add the next variables:
    ```bash
    GF_AUTH_GENERIC_OAUTH_ENABLED="true"
    GF_AUTH_GENERIC_OAUTH_NAME="authentik"
    GF_AUTH_GENERIC_OAUTH_CLIENT_ID=""
    GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=""
    GF_AUTH_GENERIC_OAUTH_SCOPES="openid profile email"
    GF_AUTH_GENERIC_OAUTH_AUTH_URL="https://authentik.company/application/o/authorize/"
    GF_AUTH_GENERIC_OAUTH_TOKEN_URL="https://authentik.company/application/o/token/"
    GF_AUTH_GENERIC_OAUTH_API_URL="https://authentik.company/application/o/userinfo/"
    GF_AUTH_SIGNOUT_REDIRECT_URL="https://authentik.company/application/o//end-session/"
    GF_AUTH_OAUTH_AUTO_LOGIN="true"
    GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH="contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' || 'Viewer'"
    ```