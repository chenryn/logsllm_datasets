title:PatchGuard: A Provably Robust Defense against Adversarial Patches
via Small Receptive Fields and Masking
author:Chong Xiang and
Arjun Nitin Bhagoji and
Vikash Sehwag and
Prateek Mittal
PatchGuard: A Provably Robust Defense against 
Adversarial Patches via Small Receptive Fields 
and Masking
Chong Xiang, Princeton University; Arjun Nitin Bhagoji, University of Chicago; 
Vikash Sehwag and Prateek Mittal, Princeton University
https://www.usenix.org/conference/usenixsecurity21/presentation/xiang
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.PatchGuard: A Provably Robust Defense against Adversarial Patches via Small
Receptive Fields and Masking
Chong Xiang
Princeton University
Arjun Nitin Bhagoji
University of Chicago
Vikash Sehwag
Princeton University
Prateek Mittal
Princeton University
Abstract
Localized adversarial patches aim to induce misclassiﬁcation
in machine learning models by arbitrarily modifying pixels
within a restricted region of an image. Such attacks can be
realized in the physical world by attaching the adversarial
patch to the object to be misclassiﬁed, and defending against
such attacks is an unsolved/open problem. In this paper, we
propose a general defense framework called PatchGuard that
can achieve high provable robustness while maintaining high
clean accuracy against localized adversarial patches. The cor-
nerstone of PatchGuard involves the use of CNNs with small
receptive ﬁelds to impose a bound on the number of features
corrupted by an adversarial patch. Given a bounded number
of corrupted features, the problem of designing an adversarial
patch defense reduces to that of designing a secure feature
aggregation mechanism. Towards this end, we present our
robust masking defense that robustly detects and masks cor-
rupted features to recover the correct prediction. Notably, we
can prove the robustness of our defense against any adver-
sary within our threat model. Our extensive evaluation on
ImageNet, ImageNette (a 10-class subset of ImageNet), and
CIFAR-10 datasets demonstrates that our defense achieves
state-of-the-art performance in terms of both provable robust
accuracy and clean accuracy.1
1 Introduction
Machine learning models are vulnerable to evasion attacks,
where an adversary introduces a small perturbation to a test
example for inducing model misclassiﬁcation [17, 50]. Many
prior attacks and defenses focus on the classic setting of ad-
versarial examples that have a small Lp distance to the benign
example [2, 7, 8, 17, 33, 35, 36, 41, 42, 50, 52, 56]. However, in
the physical world, the classic Lp setting may require global
perturbations to an object, which is not always practical. In
this paper, we focus on the threat of localized adversarial
1Our code is available at https://github.com/inspire-group/
PatchGuard for the purpose of reproducibility.
patches, in which the adversary can arbitrarily modify pixels
within a small restricted area such that the perturbation can
be realized by attaching an adversarial patch to the victim
object. Several effective patch attacks have been shown: 1)
Brown et al. [6] generate physical adversarial patches that can
force model predictions to be a target class of the attacker’s
choice; 2) Karmon et al. [22] propose the LaVAN attack in
the digital domain; 3) Eykholt et al. [15] demonstrate a robust
physical-world attack that attaches small stickers to a stop
sign for fooling trafﬁc sign recognition.
The success of practical localized adversarial patches has
inspired several defenses. Digital Watermark (DW) [20] aims
to detect and remove the adversarial patch while Local Gradi-
ent Smoothing (LGS) [39] proposes smoothing the suspicious
region of pixels to neutralize the adversarial patch. How-
ever, these empirical defenses are heuristic approaches and
lack robustness against a strong adaptive attacker [9]. This
has led to the development of several certiﬁably robust de-
fenses. Chiang et al. [9] propose the ﬁrst certiﬁed defense
against adversarial patches via Interval Bound Propagation
(IBP) [18, 38]. Zhang et al. [59] use a clipped BagNet (CBN)
to achieve provable robustness while Levine et al. [28] pro-
pose De-randomized Smoothing (DS) to further improve prov-
able robustness. These works have taken important steps to-
wards provably robust models. However, their performance
is still limited in terms of provable robustness and standard
classiﬁcation accuracy (i.e., clean accuracy), leaving defenses
against adversarial patches an unsolved/open problem.
1.1 Contributions
In this paper, we propose a general defense framework called
PatchGuard that achieves substantial state-of-the-art provable
robustness while maintaining high clean accuracy against
localized adversarial patches.
Insight: Leverage CNNs with Small Receptive Fields.
The cornerstone of our defense framework involves the use of
Convolutional Neural Networks (CNNs) with small receptive
ﬁelds to impose a bound on the number of features that can
USENIX Association
30th USENIX Security Symposium    2237
Figure 1: Overview of defense. The small receptive ﬁeld bounds the number of corrupted features (one out of three vectors in this example).
The one corrupted feature (red vector) in this example has an abnormally large element that dominates the insecure aggregation (Σ) but also
leads to a distinct pattern from clean features. Our robust masking aggregation detects and masks the corrupted feature, recovering the correct
prediction from the remaining features. We note that robust masking can have false positives (FP) and incorrectly mask benign features, but we
show in Section 5 that our defense retains high clean accuracy and provable robust accuracy.
be corrupted due to an adversarial patch. The receptive ﬁeld
of a CNN is the region of an input image that a particular
feature is inﬂuenced by, and model prediction is based on
the aggregation of features extracted from different regions
of an image. An example of the receptive ﬁeld is shown as
the red box on the image in Figure 1. Our case study in Sec-
tion 3.1 demonstrates that a large receptive ﬁeld makes CNNs
more vulnerable to adversarial patch attacks. For a model
with a large receptive ﬁeld of 483×483 (ResNet-50 [21])
on ImageNet images [12], a small patch is present in the re-
ceptive ﬁeld of most extracted features and can thus easily
change model prediction. A small receptive ﬁeld, on the other
hand, limits the number of corrupted features, and we use it
as the fundamental building block of robust classiﬁers. We
note that a small receptive ﬁeld is not a barrier to achieving
high clean accuracy. A ResNet-like architecture with a small
17×17 receptive ﬁeld can achieve an AlexNet-level accuracy
for ImageNet top-5 classiﬁcation [5]. The potential robust-
ness improvement, as well as the moderate accuracy drop,
motivates the use of small receptive ﬁelds in PatchGuard.
Insight: Leveraging Secure Aggregation & Robust
Masking. However, a small receptive ﬁeld alone is not
enough for robust prediction since conventional models use
insecure feature aggregation mechanisms such as mean. The
use of small receptive ﬁelds turns the problem of designing an
adversarial patch defense into a secure aggregation problem,
and we propose robust masking as an effective instance of
secure feature aggregation mechanism. Figure 1 provides an
overview of our defense. The small receptive ﬁeld ensures that
only a small fraction of extracted features are corrupted due to
an adversarial patch. The small number of corrupted features
forces the adversary to create abnormally large feature values
to dominate the ﬁnal prediction, and robust masking aims to
detect and mask these abnormal features. Our empirical anal-
ysis demonstrates that removing a small number of features of
a clean image is unlikely to change model prediction. There-
fore, robust masking recovers the correct prediction with high
probability if all the corrupted features are masked.
Provable Robustness. Robust masking introduces a fun-
damental dilemma for the adversary: either to generate con-
spicuous malicious features that will be detected and masked
by our defense or to do with stealthy but ineffective adver-
sarial patches. In Section 4, we show that this dilemma leads
to a proof of provable robustness for our defense, provid-
ing the guarantee that the model can always recover correct
predictions on certiﬁed images against any adversarial patch
within the threat model. This is a stronger notion of robust-
ness compared with defenses that only detect the adversarial
attack [34, 35, 56]. We also show that PatchGuard subsumes
several existing defenses [28, 59] (as shown in Section 6.1),
and outperforms them due to the use of robust masking.
State-of-the-art Performance. We consider the strongest
adversarial patch attacker, who can place the adversarial patch
on any part of the image, including on top of salient objects.
We evaluate our provable defense against any patch attacker
on ImageNet [12], ImageNette [16], CIFAR-10 [23], and
shows that our defense achieves state-of-the-art performance
in terms of provable robustness and clean accuracy compared
to previous defenses [9, 28, 59]. Our main contributions can
be summarized as follows:
1. We demonstrate the use of a small receptive ﬁeld as
a fundamental building block for robustness and lever-
age it to develop our general defense framework called
PatchGuard. PatchGuard is ﬂexible and general as it is
compatible with any CNN with small receptive ﬁelds
and any secure aggregation mechanism.
2. We present robust masking as an instance of the secure
aggregation mechanism that leads to provable robust-
ness and recovers correct predictions for certiﬁed images
against any attacker within the threat model.
3. We comprehensively evaluate our defense across Ima-
geNet [12], ImageNette [16], CIFAR-10 [23] datasets,
and demonstrate state-of-the-art provable robust accu-
racy and clean accuracy of our defense.
2238    30th USENIX Security Symposium
USENIX Association
2 Problem Formulation
Table 1: Table of notation
In this section, we ﬁrst introduce the image classiﬁcation
model, followed by the adversarial patch attack and defense
formulation. Finally, we present important terminology used
in PatchGuard. Table 1 provides a summary of our notation.
Image Classiﬁcation Model
2.1
We focus on Fully Convolutional Neural Networks (FCNNs)
such as ResNet [21], which use convolutional layers for fea-
ture extraction and only one additional fully-connected layer
for the ﬁnal classiﬁcation. This structure is widely used in
state-of-the-art image classiﬁcation models [21, 47–49].
We use X ⊂ [0,1]W×H×C to denote the image space where
each image has width W , height H, number of channels C, and
the pixels are re-scaled to [0,1]. We take Y = {0,1,··· ,N −
1} as the label space, where the number of classes is N. We
use M (x) : X → Y to denote the model that takes an image
x ∈ X as input and predicts the class label y ∈ Y . We let F (x) :
X → U be the feature extractor that outputs the feature tensor
u ∈ U ⊂ RW(cid:48)×H(cid:48)×C(cid:48)
, where W(cid:48), H(cid:48), C(cid:48) are the width, height,
and number of channels in this feature map, respectively.
2.2 Attack Formulation
Attack objective. We focus on evasion attacks against an
image classiﬁcation model. Given a deep learning model M ,
an image x, and its true class label y, the goal of the attacker is
to ﬁnd an image x(cid:48) ∈ A(x) ⊂ X satisfying a constraint A such
that M (x(cid:48)) (cid:54)= y. The constraint A is deﬁned by the attacker’s
threat model, which we will describe below. We note that the
attack objective of inducing misclassiﬁcation into any wrong
class is referred to as an untargeted attack. In contrast, when
the goal is to misclassify the image to a particular target class
y(cid:48) (cid:54)= y, it is called a targeted attack. The untargeted attack is
easier to launch and thus more difﬁcult to defend against. In
this paper, we focus on defenses against the untargeted attack.
Attacker capability. The attacker can arbitrarily modify pix-
els within a restricted region, and this region can be anywhere
on the image, even over the salient object. We assume that
all manipulated pixels are within a contiguous region, and
the defender has a conservative estimate (i.e., upper bound)
of the region size. We note that this matches the strongest
threat model used in the existing literature on certiﬁed de-
fenses against adversarial patches [9, 28, 59].2 Formally, we
use a binary pixel block p ∈ P ⊂ {0,1}W×H to represent
the restricted region, where the pixels within the region are
set to 1. Then, the constraint set A(x) can be expressed as
{x(cid:48) = (1−p)(cid:12)x+p(cid:12)x(cid:48)(cid:48)|x,x(cid:48) ∈ X ,x(cid:48)(cid:48) ∈ [0,1]W×H×C,p∈ P},
where (cid:12) refers to the element-wise product operator, and x(cid:48)(cid:48)
2A high-performance provable defense against a single patch is currently
an open/unsolved problem and is thus the focus of our threat model. We will
discuss our defense extension for multiple patches in Appendix E.
Description
Image space
Label space
Feature space
Model predictor from x ∈ X
Local feature extractor for all classes
Notation
X ⊂ [0,1]W×H×C
Y = {0,1,··· ,N − 1}
U ⊂ RW(cid:48)×H(cid:48)×C(cid:48)
M (x) : X → Y
F (x) : X → U
F (x,l) : X × Y → U Local feature extractor for class l
P ⊂ {0,1}W×H
W ⊂ {0,1}W(cid:48)×H(cid:48)
Set of binary pixel blocks in the image space
Set of binary windows in the feature space
is the content of the adversarial patch. In this paper, we pri-
marily focus on the case where p represents one square region.
Our defense can generalize to other shapes and we defer ex-
perimental results for this to our technical report [55].
2.3 Defense Formulation
Defense objective. The goal of our defense is to design a
defended model D such that D(x) = D(x(cid:48)) = y for any clean
data point (x,y) ∈ X × Y and any adversarial example x(cid:48) ∈
A(x), where A(x) is the adversarial constraint introduced in
Section 2.2. Note that we aim to recover the correct prediction,
which is harder than merely detecting an attack.
Provable robustness. Previous works [7, 9, 52] have shown
that empirical defenses are usually vulnerable to an adaptive
white-box attacker who has full knowledge of the defense
algorithm, model architecture, and model weights; therefore,
we design PatchGuard as a provably robust defense [9, 10, 18,
28,38,59] to provide the strongest robustness. The evaluation
of provable defense is agnostic to attack algorithms and its
result holds for any attack considered in the threat model.
2.4 PatchGuard Terminology
Local feature and its receptive ﬁeld. Recall that we use F
to extract feature map as u ∈ RW(cid:48)×H(cid:48)×C(cid:48)
. We refer to each
1× 1×C(cid:48)-dimensional feature in tensor u as a local feature
since it is only extracted from part of the input image as
opposed to the entire image. We deﬁne the receptive ﬁeld of
a local feature to be a subset of image pixels that the feature
˜u ∈ R1×1×C(cid:48)
is looking at, or affected by. Formally, if we
represent the input image x as a set of pixels, the receptive
ﬁeld of a particular local feature ˜u is a subset of pixels for
which the gradient of ˜u is non-zero, i.e., {r ∈ x|∇r ˜u (cid:54)= 0}. For
simplicity, we use the phrase “receptive ﬁeld of a CNN" to
refer to “receptive ﬁeld of a particular feature of a CNN".
Global feature and global logits. When the local feature
tensor u is the output of the last convolutional layer, conven-
tional CNNs use an element-wise linear aggregation (e.g.,
mean) over all local features to obtain the global feature in
RC(cid:48)
. The global feature will then go through the last fully-
connected layer (i.e., classiﬁcation layer) and yield the global
logits vector in RN for the ﬁnal prediction (top of Figure 2).
USENIX Association
30th USENIX Security Symposium    2239
Table 2: Percentage of incorrect predictions of ResNet-50
Dataset
Patch size
ImageNet
3% pixels
ImageNette CIFAR-10
3% pixels
3% pixels
Incorrect local pred. (attacked)
Incorrect local pred. (original)
Incorrect local pred. (difference)
Incorrect global predictions
84.4%
59.9%
24.5%
99.9%
56.4%
15.3%