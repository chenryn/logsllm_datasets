estimates the shaping rate. It relies on the observation that
a connection achieves a steady throughput rate after an ini-
tial burst phase with higher throughput. We have omitted the
details of this algorithm for brevity. We found burst-tolerant
shaping in 1.5% of the segments in our dataset.
Given its prevalence, we ask: can burst-tolerant shaping
mitigate the adverse impact of policing? While shaping avoids
the losses that policing induces, latency can increase as shap-
ers buffer packets. To measure this effect, for each video
chunk we compare the 10th percentile latency, usually ob-
served in the burst phase, with the 90th (Figure 14). In the
median, shaped segments observe a 90th percentile latency
that is 4× larger than the 10th percentile. About 20% of
segments see a latency bloat of at least an order of mag-
nitude due to trafﬁc shaping, whereas, among non-shaped
segments, only 1% see such disparity. Latency-aware con-
gestion control (e.g., TCP Vegas [8]) or network scheduling
algorithms (e.g., CoDel [48]) can reduce this latency bloat.
Burst-tolerant shaping can also induce unnecessary rebuffer-
ing delays at the client. When shaping forces a video server
to switch from the burst rate to the actual shaping rate, the
content provider may reduce the quality delivered to the client
based on the new bandwidth constraint. Now, the older high
quality chunk takes too long to be delivered to the client,
whereas the new low quality chunk does not reach before
the client-side application buffer has already drained.
Shapers without burst tolerance. The alternative is shapers
that enforce the shaping rate from the start. In theory, such
shaping should not induce signiﬁcant delays (unlike their
burst-tolerant counterparts), nor drop packets like policers.
Our dataset almost certainly includes ﬂows shaped in this
way, but detecting them is hard: connections affected by
shaping produce the same trafﬁc patterns as when a TCP
ﬂow hits a bottleneck at the same rate. Signiﬁcant cross traf-
Figure 14: Per-segment ratio between 90th and 10th percentile
latencies for shaped segments (red solid line) and all video seg-
ments globally (blue dashed line).
Cap.
8 kB
100 kB
2 MB
Join Time (s)
Rebuffer Time (s)
1.7
1.3
1.5
14.0
11.1
0.3
12.0
13.3
12.6
(Policed) (Shaped) Diff.
2.8
–16%
1.6
+20%
+4200% 4.2
(Policed) (Shaped) Diff.
–39%
–19%
–64%
Table 6: Avg. join/rebuffer times for ﬁrst 30 s of a video with the
downlink throttled to 0.5 Mbps by either a policer or shaper.
Capacity (cap.) is the token bucket size (for policer) and the
queue size (for shaper).
ﬁc sharing such a bottleneck may cause throughput variance.
It may be possible to identify (burst-intolerant) shapers by
looking for low variance. However, since our passive mea-
surements cannot detect when cross trafﬁc is present, we
cannot infer these shapers with any reasonable accuracy.
We evaluate the efﬁcacy of shapers in the lab by fetch-
ing the same video playback repeatedly from YouTube and
passing it through a policer or shaper (experimenting with
different bandwidth limits and queue sizes) before the traf-
ﬁc reaches the client. Then, we calculated quality metrics
using YouTube’s QoE API [65]. Table 6 summarizes the
impact on QoE, averaged over 50 trials per conﬁguration.10
Join times are generally lower when policing is used, since
data can initially pass through the policer without any rate
limit if enough tokens are buffered. With sufﬁciently large
token buckets (e.g., the 2 MB conﬁguration in Table 6) a
video playback can start almost immediately. However, this
comes at the cost of much higher rebuffering times. The sud-
den enforcement of a policing rate causes the video player
buffer to drain, causing high rebuffering rates. Shaping on
the other hand enforces a rate at all times without allowing
bursts. This reduces rebuffering by up to 64% compared to
10We show the results for a single throttling rate here, with other rates yield-
ing similar trends.
478
0.000.020.040.060.080.100.120.140.00.51.01.52.0Sequence number (in M)Time (s)Data (First Attempt)Data RetransmitsAcked Data0.000.020.040.060.080.100.120.140.00.51.01.52.0Sequence number (in M)Time (s)Data (First Attempt)Data RetransmitsAcked Data0.000.020.040.060.080.100.120.140.00.51.01.52.0Sequence number (in M)Time (s)Data (First Attempt)Data RetransmitsAcked Data 0 0.2 0.4 0.6 0.8 1 1 10 100CDF90th / 10th percentile RTT RatioShapedGlobalthe policed counterparts. Since prior work found that a low
rebuffering time increases user engagement [18], reducing
rebuffering time might be more beneﬁcial than optimizing
join times. Interestingly, shaping performs well even when
the buffer size is kept at a minimum (here, at 8 kB) which
only allows the absorption of small bursts.
Conﬁguring shapers. While policer conﬁgurations should
strive to minimize burst losses, there is no straightforward
solution for shapers. Shaping comes at a higher memory
cost than policing due to the buffer required to store pack-
ets. However, it also introduces queuing latency which can
negatively affect latency-sensitive services [39]. Thus, ISPs
that employ shaping have to trade off between minimizing
loss rates through larger buffers that introduce higher mem-
ory costs, and minimizing latency through small buffers. In
comparison to the cheaper policing option, a small buffer
might still be affordable, and the additional hardware cost
might be lower than the cost resulting from a policer that
drops large amounts of trafﬁc (e.g., additional transit cost).
5.2 Solutions for Content Providers
5.2.1 Limiting the Server’s Sending Rate
A sender can potentially mitigate the impact of a policer
by rate-limiting its transmissions, to avoid pushing the po-
licer into a state where it starts to drop packets. Optimally,
the sender limits outgoing packets to the same rate enforced
by the policer. We experimentally veriﬁed the beneﬁts of
sender-side rate limiting in a lab environment. We also con-
ﬁrmed the result in the wild, by temporarily conﬁguring one
of Google’s CDN server to rate limit to a known carrier-
enforced policing rate, then connecting to that server via the
public Internet from one of our mobile devices that we know
to be subject to that carrier’s policing rate. In both experi-
ments, loss rates dropped from 8% or more to ∼0%.
Additionally, if the policer uses a small bucket, rate lim-
iting at the sender side can even improve goodput. We veri-
ﬁed this by conﬁguring a policer to a rate of 1.5 Mbps with
a capacity of only 8 KB. In one trial we transmit trafﬁc un-
throttled, and in a second trial we limit outgoing packets to
a rate of 1.425 Mbps (95% of the policing rate). Figures 13a
and 13b show the sequence graphs for the ﬁrst few seconds
in both trials. The rate-limited ﬂow clearly performs better
in comparison, achieving a goodput of 1.38 Mbps compared
to 452 kbps. The ﬂow without rate limiting at the server
side only gets a fraction of the goodput that the policer actu-
ally allows. The reason is that the small token bucket drops
packets from larger bursts, resulting in low goodput.
Finally, we measured the beneﬁts of rate limiting video
through lab trials. We fetched videos from a YouTube Web
server, with trafﬁc passing through our lab policier. For
some trials, we inserted a shaper between the server and the
policer, to rate limit the transfer. Non-rate-limited playbacks
observed an average rebuffering ratio of 1%, whereas the
rate-limited ﬂows did not see a single rebuffering event.
5.2.2 Avoiding Bursty Transmissions
Rate limiting in practice may be difﬁcult, as the sender
Loss (95th pct.)
(rec. ﬁxed)
(base)
(paced)
34.8% 26.7%
52.1% 35.8%
Server
Loss (median)
(base)
(paced)
7.5% 6.7%
9.9% 7.8%
(rec. ﬁxed)
6.4%
8.4%
32.2%
34.6%
US
India
Table 7: Observed median and 95th percentile loss rates on
policed connections served by two selected CDN servers.
needs to estimate the throttling rate in near real-time at scale.
We explored two viable alternatives to decrease loss by re-
ducing the burstiness of transmissions, giving the policer an
opportunity to generate tokens between packets.
We start by trying TCP Pacing [3]. Whereas a traditional
TCP sender relies solely on ACK clocking to determine when
to transmit new data, pacing spreads new packets across an
RTT and avoids bursty trafﬁc. Figure 13c shows the effect
of pacing in the lab setup used in §5.2.1, but with a pacer in
place of the shaper. Overall, the ﬂow achieves a goodput of
1.23 Mbps which is worse than rate-limiting (1.38 Mbps) but
a signiﬁcant improvement over the unmodiﬁed ﬂow (452 kbps).
Packet loss is reduced from 5.2% to 1.3%.
In addition, we conﬁrmed the beneﬁts of pacing by turn-
ing it on/off on multiple CDN servers serving real clients.
Enabling pacing consistently caused loss rates to drop by 10
– 20%. Table 7 shows the results for two of the CDN servers
(“base” and “paced” columns).
Even when transmissions are not bursty, heavy losses can
still occur when the sender consistently sends at a rate larger
than the policing rate. In Linux, loss recovery can trigger
periods of slow start [19], in which the server sends two
packets for every ACKed packet. This results in sending
at twice the policed rate during recovery and hence 50% of
the retransmissions are dropped by the policer. To avoid this
behavior, we modiﬁed the loss recovery to use packet con-
servation (for every ACKed packet, only one new packet is
sent) initially and only use slow start if the retransmissions
are delivered. Keeping slow start enables us to quickly re-
cover from multiple losses within a window in a non-policed
connection. Otherwise it will take N round trips to recover
N packet losses.
As with pacing, we experimentally deployed this change
which caused loss rates to drop by 10 to 20% as well (“base”
and “rec. ﬁxed” columns in Table 7). After testing, we also
upstreamed the recovery patch to the Linux 4.2 kernel [14].
5.3 Summary of Recommendations
While extensive additional experimentation is necessary,
we make the following initial suggestions to mitigate the ad-
verse effects of policers:
1. ISPs can conﬁgure policers with smaller burst sizes.
This prevents TCP’s congestion window from growing
too far beyond the policing rate when the token bucket
ﬁlls up thereby resulting in fewer bursty losses.
2. ISPs can deploy shapers with small buffers instead of
policers. Shaping avoids the heavy losses usually seen
when employing policing, while using only small buf-
fers prevents excessive queueing delays.
3. Content providers can rate-limit their trafﬁc, especially
when streaming large content. This can reduce the gap
479
between the sending rate and the policing rate resulting
in fewer bursty losses.
4. Content providers can employ TCP pacing on their ser-
vers to reduce the burstiness of their trafﬁc.
Our initial results show that these strategies can minimize
or eliminate packet loss, and improve playback quality.
6. RELATED WORK
To our knowledge, no prior work has explored the preva-
lence and impact of policers at a global scale. Others ex-
plored policing for differentiated services [54], fair band-
width allocation [36], or throughput guarantees [21,64]. One
study explored the relationship between TCP performance
and token bucket policers in a lab setting and proposed a
TCP-friendly version achieving per-ﬂow goodputs close to
the policed rate regardless of the policer conﬁguration [60].
Finally, a concurrently published study investigated the im-
pact of trafﬁc policing applied by T-Mobile to content de-
livery. This behavior was recently introduced as part of the
carrier’s “BingeOn” program, where trafﬁc can be zero-rated
(i.e., results in no charges to customers) while it is at the
same time policed to a rate of 1.5 Mbps [31].
Our work is inspired by and builds upon the large number
of existing TCP trace analysis tools [10,12,45,49,51,56,61,
63]. On top of these tools, we are able to annotate higher-
level properties of packets and ﬂows that simplify analysis
of packet captures at the scale of a large content provider.
A few threads of work are complementary to ours. One
is the rather large body of work that has explored ways to
understand and improve Web transfer performance (e.g., la-
tency, throughput) and, more generally, content delivery, es-
pecially at the tail [9, 11, 22, 25, 29, 30, 37, 41–43, 68]. None
of these has considered the deleterious effects of policers.
Prior work has also explored the relationship between play-
back quality and user engagement [18]. Our work explores
the relationship of network effects (pathological losses due
to policers) and playback quality, and, using results from
this prior work, we are able to establish that policing can
adversely affect user satisfaction.
A line of research explores methods to detect service dif-
ferentiation [4,17,33,57,67]. They all exploit differences in
ﬂow performance characteristics, like goodput or loss rate,
to identify differentiated trafﬁc classes. However, they do
not attempt to understand the underlying mechanisms (polic-
ing or shaping) used to achieve trafﬁc discrimination. Prior
work has explored detecting trafﬁc shaping using active meth-
ods [34]; in contrast, we detect burst-tolerant shaping purely
passively.
Finally, some network operators were already aware of
policing’s disadvantages, presenting anecdotal evidence of
bad performance [15, 59, 62]. We reinforce this message,
quantifying the impact at scale for the ﬁrst time.
7. CONCLUSION
Policing high-volume content such as video and cloud stor-
age can be detrimental for content providers, ISPs and end-
users alike. Using traces from Google, we found a non-
trivial prevalence of trafﬁc policing in almost every part of
the globe: between 2% and 7% of lossy video trafﬁc world-
wide is subject to policing, often at throughput rates below