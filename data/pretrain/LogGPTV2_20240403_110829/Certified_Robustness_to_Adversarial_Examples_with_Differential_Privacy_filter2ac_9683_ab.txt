(cid:5)
)) +
b
0
δdt.
(cid:4)(cid:3)
b
0
(cid:6)
(cid:5)
(cid:3)
b
0
δdt
We next apply Equation (2) from the (, δ)-DP property:
Since δ is a constant,
C. DP-Robustness Connection
b
0 δdt = bδ.
The intuition behind using DP to provide robustness to
adversarial examples is to create a DP scoring function such
that, given an input example, the predictions are DP with
regards to the features of the input (e.g. the pixels of an
image). In this setting, we can derive stability bounds for
the expected output of the DP function using Lemma 1.
The bounds, combined with Equation (1), give a rigorous
condition (or certiﬁcation) for robustness to adversarial
examples.
(cid:2)
Formally, regard the feature values (e.g., pixels) of an
input x as the records in a database, and consider a ran-
domized scoring function A that, on input x, outputs scores
(y1(x), . . . , yK(x)) (with yk(x) ∈ [0, 1] and
K
k=1 yk(x) =
1). We say that A is an (, δ)-pixel-level differentially private
(or (, δ)-PixelDP) function if it satisﬁes (, δ)-DP (for a
given metric). This is formally equivalent to the standard
deﬁnition of DP, but we use this terminology to emphasize
the context
in which we apply the deﬁnition, which is
fundamentally different than the context in which DP is
traditionally applied in ML (see §VI for distinction).
Lemma 1 directly implies bounds on the expected out-
come on an (, δ)-PixelDP scoring function:
Corollary 1. Suppose a randomized function A satisﬁes
(, δ)-PixelDP with respect to a p-norm metric, and where
A(x) = (y1(x), . . . , yK(x)), yk(x) ∈ [0, 1]:
∀k,∀α ∈ Bp(1) (cid:2) E(yk(x)) ≤ eE(yk(x + α)) + δ.
Proof: For any k apply Lemma 1 with b = 1.
(3)
Our approach is to transform a model’s scoring function
into a randomized (, δ)-PixelDP scoring function, A(x),
and then have the model’s prediction procedure, f, use
A’s expected output over the DP noise, E(A(x)), as the
label probability vector from which to pick the arg max.
I.e., f (x) = arg maxk∈K E(Ak(x)). We prove that a model
constructed this way allows the following robustness certi-
ﬁcation to adversarial examples:
Proposition 1. (Robustness Condition) Suppose A satisﬁes
(, δ)-PixelDP with respect to a p-norm metric. For any
input x, if for some k ∈ K,
E(Ak(x)) > e2 max
i:i(cid:4)=k
E(Ai(x)) + (1 + e)δ,
(4)
b
0
P (A(x) > t)dt.
then the multiclass classiﬁcation model based on label
probability vector y(x) = (E(A1(x)), . . . , E(AK(x))) is
(cid:23)(cid:22)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
(a) PixelDP DNN Architecture
(b) Robustness Test Example
Fig. 1: Architecture. (a) In blue, the original DNN. In red, the noise layer that provides the (, δ)-DP guarantees. The noise can be added to the inputs
or any of the following layers, but the distribution is rescaled by the sensitivity Δp,q of the computation performed by each layer before the noise layer.
The DNN is trained with the original loss and optimizer (e.g., Momentum stochastic gradient descent). Predictions repeatedly call the (, δ)-DP DNN
to measure its empirical expectation over the scores. (b) After adding the bounds for the measurement error between the empirical and true expectation
(green) and the stability bounds from Lemma 1 for a given attack size Lattack (red), the prediction is certiﬁed robust to this attack size if the lower bound
of the arg max label does not overlap with the upper bound of any other labels.
robust to attacks α of size (cid:4)α(cid:4)p ≤ 1 on input x.
Proof: Consider any α ∈ Bp(1), and let x
(cid:5)
:= x + α.
From Equation (3), we have:
E(Ak(x)) ≤ eE(Ak(x
E(Ai(x
(cid:5)
(cid:5)
)) + δ,
(a)
(b)
)) ≤ eE(Ai(x)) + δ,
i (cid:7)= k.
(cid:5)
Equation (a) gives a lower-bound on E(Ak(x
)); Equation
(cid:5)
(b) gives an upper-bound on maxi(cid:4)=k E(Ai(x
)). The hy-
pothesis in the proposition statement (Equation (4)) implies
that the lower-bound of the expected score for label k is
strictly higher than the upper-bound for the expected score
for any other label, which in turn implies the condition from
Equation (1) for robustness at x. To spell it out:
E(Ak(x
(cid:5)
))
Eq(a)≥ E(Ak(x)) − δ
e
e2 maxi:i(cid:4)=k E(Ai(x)) + (1 + e)δ − δ
e
E(Ai(x)) + δ
(cid:5)
Eq(4)
>
= e max
i:i(cid:4)=k
Eq(b)≥ max
i:i(cid:4)=k
)) > max
i:i(cid:4)=k
E(Ai(x
E(Ai(x + α)) ∀α ∈ Bp(1),
=⇒ E(Ak(x
the very deﬁnition of robustness at x (Equation (1)).
))
(cid:5)
The preceding certiﬁcation test is exact regardless of the
value of the δ parameter of differential privacy: there is no
failure probability in this test. The test applies only to attacks
of p-norm size of 1, however all preceding results generalize
to attacks of p-norm size L, i.e., when (cid:4)α(cid:4)p ≤ L, by
applying group privacy [18]. The next section shows how to
apply group privacy (§III-B) and generalize the certiﬁcation
test to make it practical (§III-D).
III. PixelDP Certiﬁed Defense
A. Architecture
PixelDP is a certiﬁed defense against p-norm bounded
adversarial example attacks built on the preceding DP-
(cid:23)(cid:22)(cid:26)
robustness connection. Fig. 1(a) shows an example PixelDP
DNN architecture for multi-class image classiﬁcation. The
original architecture is shown in blue; the changes intro-
duced to make it PixelDP are shown in red. Denote Q the
original DNN’s scoring function; it is a deterministic map
from images x to a probability distribution over the K labels
Q(x) = (y1(x), . . . , yK(x)). The vulnerability to adversarial
examples stems from the unbounded sensitivity of Q with
respect to p-norm changes in the input. Making the DNN
(, δ)-PixelDP involves adding calibrated noise to turn Q
into an (, δ)-DP randomized function AQ; the expected
output of that function will have bounded sensitivity to p-
norm changes in the input. We achieve this by introducing
a noise layer (shown in red in Fig. 1(a)) that adds zero-
mean noise to the output of the layer preceding it (layer1 in
Fig. 1(a)). The noise is drawn from a Laplace or Gaussian
distribution and its standard deviation is proportional to: (1)
L, the p-norm attack bound for which we are constructing
the network and (2) Δ,
the sensitivity of the pre-noise
computation (the grey box in Fig. 1(a)) with respect to p-
norm input changes.
Training an (, δ)-PixelDP network is similar to training
the original network: we use the original loss and optimizer,
such as stochastic gradient descent. The major difference
is that we alter the pre-noise computation to constrain its
sensitivity with regards to p-norm input changes. Denote
Q(x) = h(g(x)), where g is the pre-noise computation and
h is the subsequent computation that produces Q(x) in the
original network. We leverage known techniques, reviewed
in §III-C, to transform g into another function, ˜g, that has a
ﬁxed sensitivity (Δ) to p-norm input changes. We then add
the noise layer to the output of ˜g, with a standard deviation
scaled by Δ and L to ensure (, δ)-PixelDP for p-norm
changes of size L. Denote the resulting scoring function of
the PixelDP network: AQ(x) = h(˜g(x) + noise(Δ, L, , δ)),
where noise(.) is the function implementing the Laplace/-
Gaussian draw. Assuming that the noise layer is placed
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
such that h only processes the DP output of ˜g(x) without
accessing x again (i.e., no skip layers exist from pre-noise to
post-noise computation), the post-processing property of DP
ensures that AQ(x) also satisﬁes (, δ)-PixelDP for p-norm
changes of size L.
Prediction on the (, δ)-PixelDP scoring function, AQ(x),
affords the robustness certiﬁcation in Proposition 1 if the
prediction procedure uses the expected scores, E(AQ(x)),
to select the winning label for any input x. Unfortunately,
due to the potentially complex nature of the post-noise
computation, h, we cannot compute this output expectation
analytically. We therefore resort to Monte Carlo methods to
estimate it at prediction time and develop an approximate
version of the robustness certiﬁcation in Proposition 1 that
uses standard techniques from probability theory to account
for the estimation error (§III-D). Speciﬁcally, given input
x, PixelDP’s prediction procedure invokes AQ(x) multiple
times with new draws of the noise layer. It then averages
the results for each label, thereby computing an estima-
tion ˆE(AQ(x)) of the expected score E(AQ(x)). It then
computes an η-conﬁdence interval for ˆE(AQ(x)) that holds
with probability η. Finally,
integrates this conﬁdence
interval into the stability bound for the expectation of a
DP computation (Lemma 1) to obtain η-conﬁdence upper
and lower bounds on the change an adversary can make to
the average score of any label with a p-norm input change
of size up to L. Fig. 1(b) illustrates the upper and lower
bounds applied to the average score of each label by the
PixelDP prediction procedure. If the lower bound for the
label with the top average score is strictly greater than the
upper bound for every other label, then, with probability η,
the PixelDP network’s prediction for input x is robust to
arbitrary attacks of p-norm size L. The failure probability
of this robustness certiﬁcation, 1−η, can be made arbitrarily
small by increasing the number of invocations of AQ(x).
it
One can use PixelDP’s certiﬁcation check in two ways:
(1) one can decide only to actuate on predictions that are
deemed robust to attacks of a particular size; or (2) one can
compute, on a test set, a lower bound of a PixelDP network’s
accuracy under p-norm bounded attack, independent of how
the attack is implemented. This bound, called certiﬁed ac-
curacy, will hold no matter how effective future generations
of the attack are.
The remainder of this section details the noise layer,
training, and certiﬁed prediction procedures. To simplify
notation, we will henceforth use A instead of AQ.
B. DP Noise Layer
The noise layer enforces (, δ)-PixelDP by inserting noise
inside the DNN using one of two well-known DP mecha-
nisms: the Laplacian and Gaussian mechanisms. Both rely
upon the sensitivity of the pre-noise layers (function g). The
sensitivity of a function g is deﬁned as the maximum change
in output that can be produced by a change in the input,
given some distance metrics for the input and output (p-
norm and q-norm, respectively):
)(cid:4)q
.
Δp,q = Δg
p,q = max
x,x(cid:2):x(cid:4)=x(cid:2)
(cid:4)g(x) − g(x
(cid:5)
(cid:4)x − x(cid:5)(cid:4)p
Assuming we can compute the sensitivity of the pre-
noise layers (addressed shortly), the noise layer leverages
the Laplace and Gaussian mechanisms as follows. On every
invocation of the network on an input x (whether for training
or prediction) the noise layer computes g(x) + Z, where
the coordinates Z = (Z1, . . . , Zm) are independent random
variables from a noise distribution deﬁned by the function
noise(Δ, L, , δ).
• Laplacian mechanism: noise(Δ, L, , δ) uses
the
Laplace distribution with mean zero and standard de-
viation σ =
2Δp,1L/; it gives (, 0)-DP.
√
(cid:7)
2 ln( 1.25
• Gaussian mechanism: noise(Δ, L, , δ) uses the Gaus-
sian distribution with mean zero and standard deviation
δ )Δp,2L/; it gives (, δ)-DP for  ≤ 1.
σ =
Here, L denotes the p-norm size of the attack against
which the PixelDP network provides (, δ)-DP; we call it
the construction attack bound. The noise formulas show
that for a ﬁxed noise standard deviation σ, the guarantee
degrades gracefully: attacks twice as big halve the  in the
DP guarantee (L ← 2L ⇒  ← 2). This property is often
referred as group privacy in the DP literature [18].
Computing the sensitivity of the pre-noise function g
depends on where we choose to place the noise layer in the
DNN. Because the post-processing property of DP carries
the (, δ)-PixelDP guarantee from the noise layer through
the end of the network, a DNN designer has great ﬂexibility
in placing the noise layer anywhere in the DNN, as long as
no skip connection exists from pre-noise to post-noise layers.
We discuss here several options for noise layer placement
and how to compute sensitivity for each. Our methods are
not closely tied to particular network architectures and can
therefore be applied on a wide variety of networks.
Option 1: Noise in the Image. The most straightforward
placement of the noise layer is right after the input layer,
which is equivalent to adding noise to individual pixels of
the image. This case makes sensitivity analysis trivial: g is
the identity function, Δ1,1 = 1, and Δ2,2 = 1.
Option 2: Noise after First Layer. Another option is to
place the noise after the ﬁrst hidden layer, which is usually