title:The Crux of Voice (In)Security: A Brain Study of Speaker Legitimacy
Detection
author:Ajaya Neupane and
Nitesh Saxena and
Leanne M. Hirshfield and
Sarah E. Bratt
The Crux of Voice (In)Security:
A Brain Study of Speaker Legitimacy Detection
Ajaya Neupane*
Nitesh Saxena
University of California Riverside
University of Alabama at Birmingham
PI:EMAIL
PI:EMAIL
Leanne Hirshﬁeld
Syracuse University
PI:EMAIL
Sarah Elaine Bratt
Syracuse University
PI:EMAIL
Abstract—A new generation of scams has emerged that uses
voice impersonation to obtain sensitive information, eavesdrop
over voice calls and extort money from unsuspecting human
users. Research demonstrates that users are fallible to voice
impersonation attacks that exploit the current advancement in
speech synthesis. In this paper, we set out to elicit a deeper
understanding of such human-centered “voice hacking” based
on a neuro-scientiﬁc methodology (thereby corroborating and
expanding the traditional behavioral-only approach in signiﬁ-
cant ways). Speciﬁcally, we investigate the neural underpinnings
of voice security through functional near-infrared spectroscopy
(fNIRS), a cutting-edge neuroimaging technique, that captures
neural signals in both temporal and spatial domains. We design
and conduct an fNIRS study to pursue a thorough investigation of
users’ mental processing related to speaker legitimacy detection –
whether a voice sample is rendered by a target speaker, a different
other human speaker or a synthesizer mimicking the speaker. We
analyze the neural activity associated within this task as well as
the brain areas that may control such activity.
Our key insight is that there may be no statistically signiﬁcant
differences in the way the human brain processes the legitimate
speakers vs. synthesized speakers, whereas clear differences are
visible when encountering legitimate vs. different other human
speakers. This ﬁnding may help to explain users’ susceptibility
to synthesized attacks, as seen from the behavioral self-reported
analysis. That is, the impersonated synthesized voices may seem
indistinguishable from the real voices in terms of both behavioral
and neural perspectives. In sharp contrast, prior studies showed
subconscious neural differences in other real vs. fake artifacts
(e.g., paintings and websites), despite users failing to note these
differences behaviorally.
Overall, our work dissects the fundamental neural patterns
underlying voice-based insecurity and reveals users’ susceptibility
to voice synthesis attacks at a biological level. We believe that
this could be a signiﬁcant insight for the security community
suggesting that the human detection of voice synthesis attacks
may not improve over time, especially given that voice synthesis
techniques will likely continue to improve, calling for the design
of careful machine-assisted techniques to help humans counter
these attacks.
I.
INTRODUCTION
Voice is supposed to be a unique identiﬁer of a person.
In human-to-human conversations, people may be able to
recognize the speakers based on the unique traits of their
voices. However, previous studies have shown that human-
based speaker veriﬁcation is vulnerable to voice impersonation
attacks [30]. A malicious entity can impersonate someone’s
voice by mimicking it using speech synthesis techniques. In
particular, off-the-shelf speech morphing techniques can be
used to generate the spoofed voices of people of interest (vic-
tims) [30]. The attacker can then perform social engineering
trickeries using an impersonated voice to fool the users into
accepting it as a legitimate one. These attacks may eventually
make the users reveal sensitive and conﬁdential information,
which may hamper their security, privacy, and safety.
Such voice imitation is an emerging class of threats, espe-
cially given the advancement in speech synthesis technology,
seen in a variety of contexts that can harm a victim’s reputation
and her security/safety [30]. For instance, the attacker could
publish the morphed voice samples on social media [17],
impersonate the victim in phone conversations [21], leave fake
voice messages to the victim’s contacts, and even launch man-
in-the-middle attacks against end-to-end encryption technolo-
gies that require users to verify the voices of the callers [38],
to name a few instances of such attacks.
it
is crucial
Given the prominence and rapid emergence of these threats
in the wild,
to understand users’ innate psy-
chological behavior that governs the processing of voices
and their potential susceptibility to voice impersonation at-
tacks. In this paper, we follow the neuroimaging methodology
adopted in a recently introduced line of research (e.g., [8],
[32], [34]) to scrutinize the user behavior in the speciﬁc
context of such “voice security”. Speciﬁcally, we study users’
neural processes (besides their behavioral performance) to
understand and leverage the neural mechanics when users
are subjected to voice impersonation attacks using a state-of-
the-art neuroimaging technique called functional near-infrared
spectroscopy (fNIRS).
*Work done while being a student at UAB
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23206
www.ndss-symposium.org
The speciﬁc goal of this paper is to study the neural
underpinnings of voice (in)security, and analyze differences
(or lack thereof) in neural activities when users are processing
different types of voices. We examine how the information
present in the neural signals can be used to explain users’
susceptibility to voice imitation attacks using synthesized
voices (i.e., speaker legitimacy detection). Prior studies [25],
[32]–[34] have shown that subconscious neural differences
TABLE I.
SUMMARY OF REAL-FAKE ANALYSIS OBSERVED IN
RELATED WORKS VS. OUR WORK
Type of Artifacts
in
Differences
Neural Activity
Websites under phish-
ing ( [32]–[34])
Paintings [25]
Voices (our work)
Present
Present
Absent
Differences
in Behavioral
Response
Nearly absent
Nearly absent
Nearly absent
exist when users are subject to real vs. fake artifacts, even
though users themselves may not be able to tell the two apart
behaviorally. Neupane et al. in their previous studies [32]–
[34] found differences in neural activities at the right, middle,
inferior, and orbitofrontal areas when users were processing
real and fake websites. Similarly, Huang et al. [25] found dif-
ferences in neural activation at similar areas when users were
viewing real and fake Rembrandt paintings. Higher activation
in the frontopolar area implicates the use of working memory
and cognitive workload. Lower activation in the orbitofrontal
area suggests that the users trust stimuli presented to them g
[15]. of the target speakers.
Based on these prior results, we performed our study with
the hypothesis that these and other relevant brain areas might
be activated differently when users are listening to the original
and fake voices of a speaker. We, therefore, set-up the fNIRS
headset conﬁguration to measure the frontal and temporo-
parietal brain which overlaps with the regions reported in these
previous “real-fake detection” studies. The implications of the
neural activity differences, if present when processing real vs.
fake voices, can be important as these differences could be
automatically mined and the user under attack could be alerted
to the presence/absence of the attack, even though the user may
have himself failed to detect the attack (behaviorally). Neupane
et al. suggested such an approach in the context of phishing
attacks [33]. Our study investigates the same line in the context
of voice synthesis attacks.
The neuroimaging technique used in our study, i.e., fNIRS,
is a non-invasive imaging method to measure the relative
concentration of oxygenated hemoglobin (oxy-Hb) and deoxy-
genated hemoglobin (deoxy-Hb) in brain cortex [11], [24],
[26]. By examining the changes in oxy-Hb and deoxy-Hb,
we can infer the activities in the neural areas of interest. We
carefully selected fNIRS as our study platform as it has the
unique capabilities to provide spatially accurate brain activity
information better than the EEG (Electroencephalography)
and similar to that of fMRI (Functional Magnetic Resonance
Imaging) [26]. Therefore, we preferred fNIRS to ensure we
capture the features from both temporal and spatial domains.
Unlike fMRI, fNIRS also allows us to pursue the study in envi-
ronments with better ecological validity since the participants
do not have to be in a supine position in the fMRI scanner
while making decisions.
Our Contributions: We design and conduct an fNIRS study
to pursue a thorough investigation of users’ processing of real
and morphed voices. We provide a comprehensive analysis
of the collected neuroimaging data set and the behavioral
task performance data set. Contrary to our hypothesis (and
contrary to the case of website/painting legitimacy detection),
we do not obtain differences in the way the brains process
legitimate speakers vs. synthesized speakers, when subject to
voice impersonation attacks, although marked differences are
seen between neural activity corresponding to a legitimate
speaker vs. a different unauthorized human speaker. That is,
the synthesized voices seem nearly indistinguishable from the
real voices with respect
to the neurocognitive perspective.
This insight may serve well to explain users’ susceptibility to
such attacks as also reﬂected in our task performance results
(similar to the task performance results reported in [30]).
Table I captures a summary of our work versus other real-
fake detection studies.
Since this potential indistinguishability of real vs. morphed
lies at the core of human biology, we posit that the problem
is very severe, as the human detection of synthesized attacks
may not improve over time with evolution, Further, in our
study, we use an off-the-shelf, academic voice morphing tool
based on voice conversion, CMU Festvox [19], whereas with
the advancement in the voice synthesizing technologies (e.g.,
newer voice modeling techniques such as those offered by
Lyrebird and Google WaveNet [29], [41]), it might become
even more difﬁcult for users to identify such attacks. Also, our
study participants are mostly young individuals and with no
reported hearing disabilities, while older population samples
and/or those having hearing disabilities may be more prone to
voice synthesis attacks [16].
We do not claim that the rejection of our hypothesis nec-
essarily means that the differences between real and morphed
voices are absent conclusively – further studies might need to
be conducted using other neuroimaging techniques and other
wider samples of users. However, our work certainly casts
a serious doubt regarding the presence of such differences
(in contrast to other real-fake contexts, such as paintings or
websites), which also maps well with our behavioral results,
thereby explaining human-centered voice insecurity.
In light of our results, perhaps the only feasible way to
protect users from such attacks would be by making them
more aware of the threat, and possibly by developing technical
solutions to assist
the users. Even though machine-based
voice biometric systems have also been shown to be vulner-
able to voice synthesis attacks [30], the security community
can certainly work, and has been working, towards making
such techniques more secure with advanced liveness detection
mechanisms, which could aid the end users against voice
synthesis based social engineering scams, whenever possible.
Broader Scientiﬁc Signiﬁcance: We believe that our work
helps to advance the science of human-centered voice security,
in many unique ways. It also serves to reveal the fundamental
neural basis underlying voice-based security, and highlights
users’ susceptibility to advanced voice synthesis attacks. Table
X gives a snapshot of all our results.
Beyond the aforementioned novel contributions, one im-
portant scientiﬁc attribute of our work lies in recreating and
revalidating the ﬁndings from the prior behavioral-only (task
performance) study of voice insecurity reported in the literature
[30] through independent settings. Similar to [30], our results
conﬁrm the susceptibility of human users to voice imperson-
ation attacks.
2
Security Relevance and Implications: Although our work
is informed by neuroscience, it is deeply rooted in computer
security and provides valuable implications for the security
community. We conduct a neuroimaging-based user study and
show why attackers might be successful at morphed voice
attacks. Many similar security studies focusing on human
neuro-physiology have been published as a new line of re-
search in mainstream security/HCI venues, e.g., [8], [32]–
[34]. How users perform at crucial security tasks from a
neurological standpoint is therefore of great interest to the
security community.
This line of research followed in our work provides novel
security insights and lessons that are not possible to elicit via
behavioral studies alone. For example, prior studies
[32]–
[34] showed that security (phishing) attacks can be detected
based on neural cues, although users may themselves not be
able to detect these attacks. Following this line, our work
conducted an fNIRS study to dissect users’ behavior under
voice impersonation attacks, an understudied attack vector.
Our results show that even brain responses cannot be used
to detect such attacks, which serve to explain why users are
so susceptible to these attacks.
II. BACKGROUND & PRIOR WORK
In this section, we provide an overview on fNIRS system,
and discuss the related works.
A. fNIRS Overview
The non-invasive fNIRS technology has unique capabilities
in that it can provide spatially accurate brain activity informa-
tion in line with fMRI, but it can do so in an ecologically
valid experimental environments (not inside a scanner under
a supine posture). It is easy to set-up and robust to motion
artifacts, and offers high spatial resolution [11], [24], [26]. The
basis of fNIRS is the usage of near-infrared light (700-900 nm
range), which can penetrate through scalp to reach the brain
cortex. Optical ﬁbers are placed on the surface of the head for
illumination while detection ﬁbers measure light reﬂected back
[10], [40]. The differences in absorption spectra of the lights
by oxy-Hb and deoxy-Hb allow the measurement of relative
changes in hemoglobin in the blood in brain. fNIRS provides
better temporal resolution compared to fMRI and better spatial
resolution (approximately 5mm) compared to EEG. Based on
these attractive features, we have chosen fNIRS as a platform
to conduct our study reported in this paper. The hemodynamic
changes measured by the fNIRS occurs at slow rate of 6-9 sec
similar to fMRI, so the trial duration is relatively longer than
EEG [10].
B. Related Work
Voice spooﬁng attacks are a serious threat to users’ secu-
rity and privacy. Previous studies have shown that attackers
equipped with voice morphing techniques could breach ma-
chine and human speaker veriﬁcation systems [28], [30], [38].
Mukhopadhyay et al. [30] attacked both machine-based and
human-based speaker veriﬁcation system with morphed voice
samples and different speakers’ (other users’) voice samples.
They report that both human and machine are vulnerable to
such attacks. Shirvanian et al. [38] successfully launched man-
in-the-middle attacks against “Crypto Phones”, where users
verbally exchange and compare each other’s authentication
codes before establishing secure communications, using mor-
phed voice samples. Lewison et al. [28] proposed a block-
chain system in which face-recognition is combined with
voice veriﬁcation to prevent voice morphing attacks. Bai et
al. [4] proposed the use of voices to authenticate certiﬁcates.
However, it can be subject to potential morphing attacks. In
this light, it is important to understand why users may fail to
identify the voice of a speaker under morphing attacks, and
inform the designers of automated speech recognition systems
as to how users may distinctively process speakers’ voices.
Recently, several studies have reported the use of neu-
roimaging to understand the underlying neural processes con-
trolling users’ decision-making in security tasks [8], [32]–[34].
Speciﬁcally, Neupane et al. [34] analyzed brain signals when
users were detecting the legitimacy of phishing websites and
reading malware warnings using a state-of-the-art neuroimag-
ing technique fMRI. Neupane et al. [32] followed up the study
with EEG and eye-tracking to understand users’ neural and