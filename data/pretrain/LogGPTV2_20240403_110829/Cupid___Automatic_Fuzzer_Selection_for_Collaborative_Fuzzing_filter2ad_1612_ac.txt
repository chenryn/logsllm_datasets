so that the recorded differences are only due to the selection of
fuzzers and not due to any other factors.
Note that the numbers presented by EnFuzz [5] report the im-
provement over the sum of the coverage of the whole dataset. For
comparison, we also report this number (as Improvement (sum)),
but we believe that this metric heavily skews the results as large
programs are weighted higher and thus, this metric is not very in-
formative. When not stated otherwise, we use the geometric mean
to calculate the overall improvement across programs, which is
standard practice in the field, to circumvent this issue. We limit
the experiments to 10 hours to reduce the total time needed for
the experiments (in total, the experiments presented in this section
have required 40,000 CPU hours or ‚àº4.5 CPU years). Furthermore,
which parts of a testcase to modify. Both of these techniques
act as a form of attention to focus more time on interesting
parts of the binary.
‚Ä¢ lafIntel [15] is a collection of compiler passes that help
fuzzers to progress through difficult branches by splitting
them up into several branches. These individual branches are
easier to solve. Due to difficulties compiling libraries from
the fuzzer-test-suite, we instead use the CompCov mode of
AFL++ [8] in QEMU mode.
‚Ä¢ libFuzzer [19] is a library that adds a fuzzer stub into the
program at compile time. libFuzzer can be run in-process,
which vastly reduces the overhead compared to the common
fork server approach (AFL). However, the programs needs
to be of high quality, since memory leaks and crashes also
affect the fuzzing run. libFuzzer comes with a multi-process
fork mode that makes it easier to ignore crashes, which is
what we use in our case. libFuzzer is not based on AFL and
thus is another diverse addition. We patched libFuzzer to
allow for external test case syncing at runtime.
‚Ä¢ Honggfuzz [12] is another fuzzer that is not based on AFL
and thus differs significantly in its code base, e.g., it intro-
duces a different seed scheduling algorithm, as well as muta-
tors and different coverage metrics. We patched Honggfuzz
to allow for external test case syncing at runtime.
For an overview of the configuration parameters for each fuzzer,
refer to Table 5 in the appendix.
Despite the fact that we support eight fuzzers out-of-the-box,
to ensure fairness in the evaluation, we limit the pool of fuzzers
to those that EnFuzz had access to (AFL, AFLFast, FairFuzz, lib-
Fuzzer, QSYM, radamsa). Although this guarantees a fair compari-
son to EnFuzz, these fuzzers have a lower overall diversity, which is
a crucial factor in maximizing possible performance improvements
in a collaborative run [5], so the restriction on fuzzers also limits
the possible magnitude of our results.
For a visual representation of the diversity in the pool of fuzzers,
refer to Figure 5. The top value in every cell represents the average
predicted branch coverage for this combination, and the bottom
value describes the improvement provided by the second fuzzer
compared to a single instance of the first fuzzer. For instance, AFL
and QSYM complement each other well: when run collaboratively,
the predicted average branch coverage will increase by 18% (from
‚àº55k to ‚àº64k) as compared to a single AFL instance. Although an
improvement by 18% is valuable, the total predicted average branch
coverage is still low compared to the best predicted combination of
Honggfuzz and libFuzzer (‚àº95k). Furthermore, AFL-based combi-
nations show only low improvements when combined with most
other AFL-based fuzzers, but their average predicted branch cover-
age is also significantly lower than non-AFL based combinations.
In conclusion, we recommend to extend the pool of fuzzers by
non-AFL based fuzzers to increase the diversity and thus improve
the overall performance of collaborative fuzzing. However, as men-
tioned before, to keep the evaluation fair, we use the same fuzzers
that EnFuzz has access to as well.
Figure 5: The totals column and row show the solved
branches for an average fuzzing run with the respective
fuzzer. The inner blocks show the predicted performance of
each two fuzzer combination. Note that two instances of the
same fuzzer is also a valid combination. The upper value rep-
resents the complementarity, the lower value is the percent-
age increase compared to the ‚ÄúHow is‚Äù fuzzer.
even with the runtime limit of 10 hours, we saturate the fuzzing
coverage for all binaries except two.
6.1 Evaluated fuzzers
Cupid ships with eight default fuzzers in total:
‚Ä¢ AFL [29] is a superseded fuzzer that requires little set-up.
However, as seen by several fuzzers we use for the evaluation,
AFL is often used as a starting point for new research.
‚Ä¢ QSYM [28] is the concolic execution engine of a hybrid fuzzer
together with AFL. QSYM shows that loosening the strict
soundness requirements of concolic executors leads to better
performance and showcases how integration with a fuzzer
can help validate and improve the speed of the concolic
engine. For our evaluation, QSYM acts as a fuzzer that trades
targeted solving of branches against higher execution count.
‚Ä¢ AFLFast [3] is based on AFL, and improves the scheduling of
testcases. The basic idea is that fuzzers with simple schedul-
ing algorithms spend a lot of time on testcases that exercise
a small number of paths. The authors show that time would
be better spent on less frequently visited branches.
‚Ä¢ radamsa [11] is a black-box mutator with more sophisticated
mutators than AFL. We use AFL++ in radamsa mode, which
adds coverage feedback.
‚Ä¢ FairFuzz [16] improves upon AFL by putting more focus
on rare branches, as well as a mutation mask that specifies
7
TotalsQSYMAFLFastAFLRadamsaFairFuzzlafIntellibFuzzerHonggfuzzCombined withQSYMAFLFastAFLRadamsaFairFuzzlafIntellibFuzzerHonggfuzzTotalsHow is63.0k66.3k+5%64.8k+3%64.8k+3%64.5k+2%65.9k+5%65.2k+4%87.9k+40%86.5k+37%55.3k64.8k+17%56.7k+3%56.6k+2%56.3k+2%58.4k+6%59.4k+8%86.9k+57%85.5k+55%55.1k64.8k+18%56.6k+3%56.2k+2%56.0k+2%58.3k+6%59.2k+8%86.9k+58%85.5k+55%54.0k64.5k+19%56.3k+4%56.0k+4%55.2k+2%58.1k+7%58.9k+9%86.9k+61%85.4k+58%57.2k65.9k+15%58.4k+2%58.3k+2%58.1k+2%59.3k+4%61.0k+7%87.0k+52%85.7k+50%56.3k65.2k+16%59.4k+6%59.2k+5%58.9k+5%61.0k+8%57.9k+3%86.4k+54%85.5k+52%85.1k87.9k+3%86.9k+2%86.9k+2%86.9k+2%87.0k+2%86.4k+2%89.7k+5%95.3k+12%84.6k86.5k+2%85.5k+1%85.5k+1%85.4k+1%85.7k+1%85.5k+1%95.3k+13%89.8k+6%63.0k55.3k55.1k54.0k57.2k56.3k85.1k84.6kTable 1: The result of the prediction framework for n=2.
"Complementarity" represents the predicted total probabil-
ity for this fuzzer combination, i.e. the predicted average
number of branches visited on all of the training data by this
fuzzer combination. Combinations in bold are the selections
we used in Experiment 1.
Ranking
1.
2.
3.
10.
11.
12.
19.
20.
21.
Combination
Complementarity
libFuzzer, libFuzzer
FairFuzz, libFuzzer
AFL, libFuzzer
...
FairFuzz, radamsa
FairFuzz, QSYM
AFL, AFL
...
AFLFast, QSYM
QSYM, radamsa
QSYM, QSYM
93456.32
93099.12
92510.05
76347.06
75623.01
73953.05
70916.66
69329.05
62795.15
6.2 Prediction of fuzzer combinations
In this section, we evaluate the accuracy of our prediction frame-
work by comparing the predicted rankings to real-world results.
Additionally, we evaluate against fuzzer-test-suite and Lava-M to
compare the performance of our best-predicted combination to
EnFuzz.
6.2.1 Comparing predicted and actual rankings. Cupid uses em-
pirical data to predict a ranking for fuzzer combinations. In this
instance, ranking describes the performance of one combination
relative to other combinations. Because our predicted ranking is
based on empirical data extracted through time-limited fuzzing runs
in isolation, it is necessary to evaluate if our ranking accurately
resembles real-world results in collaborative runs.
In order to evaluate the quality of certain combinations of fuzzers,
the most indicative metric is the median code coverage at the end
of the selected time frame. However, for some of the binaries, most
fuzzer combinations reach the maximal coverage too quickly. For
this reason, we decided to calculate the area-under-curve (AUC)
as well because it takes into account the time taken to reach every
given value of coverage. As a result, if two combinations find the
same total code coverage, we prefer the combination that achieves
this in the shortest amount of time, which would be reflected by a
larger AUC.
To conduct this experiment, we face two issues:
(1) On some binaries, fuzzers find nearly all coverage in a frac-
tion of the total scheduled runtime (i.e., they flat line after
a short time). For these binaries, the performance measure-
ments are difficult to compare. If most fuzzers reach the same
code coverage after a few minutes, not only will the mean or
median code coverage be near identical after 10h, but also
the area-under-curve.
(2) Some combinations of fuzzers reach very similar perfor-
mance, thus predicting an accurate ranking is not only diffi-
cult but could also be misleading, since the ranking obfus-
cates how similar combinations are.
To address the first issue, we set combination size of ùëõ = 2,
a smaller number of parallel fuzzers will take longer to achieve
maximum coverage and thus, allow for more accurate rankings.
Additionally, due to the magnitude of necessary CPU hours for
10 runs for all 21 combinations on all 13 test binaries, we let all
combinations fuzz every binary 10 times for only 1h. With these
settings, it will take longer to hit the upper bound of code coverage
on a binary in the given time window.
To address the second issue, we first evaluate those fuzzer com-
binations that significantly differ in their predicted performance
and, subsequently, we perform an exhaustive ranking correlation
with all ùëõ = 2 combinations. Thus, for our first experiment, we
reduce our list of combinations to the best predicted combination,
the worst predicted combination, and an additional combination
in-between the two (as highlighted in Table 1) and compare how
well their predicted rankings match real-world results. The evalua-
tion was done on six machines with 40-core/80-thread Intel Xeon
Gold 6230 CPU @ 2.10GHz processors and 192GB RAM, where
each fuzzer got assigned a dedicated core.
Experiment 1. We fuzz the 13 test binaries 10 times for 1h each,
select the median coverage over time and calculate the area-under-
curve. Then we rank the three selected fuzzer combinations using
this value. We use the Pearson correlation coefficient to measure the
linear correlation between our predicted ranking and the real-world
ranking. Additionally, we use the predicted performance value and
the actual median AUC to calculate a more detailed correlation
coefficient on a per-binary basis.
Results. The Pearson coefficient is calculated as ùëü = 0.81 with
ùëù < 0.01. This represents, according to Evans [7], a very strong pos-
itive correlation (0.8 ‚â§ ùëü ‚â§ 1.0). On a per-binary basis, 11 out of 13
binaries have a very strong positive correlation with ùëü ‚â• 0.9 when
their predicted performance is compared to the actual median AUC,
while only two binaries have a negative correlation. The rankings
of 10 binaries are identical to our prediction. In conclusion, our
framework is indeed able to accurately predict the rankings when
the combinations are dissimilar in their predicted performance.
Experiment 2. To showcase that similarly predicted combina-
tions will still match reasonably well, we rerun the evaluation for
not just the three distinct combinations, but all possible 21 fuzzer
combinations for ùëõ = 2. More specifically, we modify the previous
experiment to rank all possible 21 fuzzer combinations for each of
the 13 test binaries.
Results. For this case, the correlation coefficient drops to ùëü = 0.6
with ùëù < 0.01. This is, according to Evans [7] slightly above a
moderate positive correlation (0.4 ‚â§ ùëü ‚â§ 0.59). With experimental
data, this is expected: our prediction relies on data collected over
a range of binaries and input seeds and thus Cupid is only able
to predict the average performance over multiple fuzzing runs
and binaries. Due to inherent randomness in any fuzzing process,
a greater variance in the performance of some combinations is
expected, and thus, combinations that are predicted to be similar in
their performance will not match their predicted ranking perfectly
in all cases. However, even in this scenario, 7 out of 13 binaries
display a very strong positive correlation coefficient, with ùëü ‚â• 0.8.
Note that although this evaluation is important in determining
the accuracy of the predicted ranking, in real-world scenarios a
security analyst is only interested in the top results. While there
8
Table 2: Median branch coverage on the test binaries from fuzzer-test-suite (10 runs with a run time of 10h). Bold values
highlight the best result. The four following columns represent the speed-up in latency to reach the given percentage of the
observed coverage. Bold values highlight positive speed-ups. The last column represents the ùëù-value according to the Mann-
Whitney U test between EnFuzz and Cupid. Bold values highlight statistical significance (ùëù < 0.05). No test was possible for
c-ares because both results were identical.
Binary
c-ares
guetzli
json
libarchive
libpng
libssh
libxml2
openssl-1.0.2d
openssl-1.1.0c
openthread
proj4
sqlite
woff2
Total
Improvement (sum)
Improvement (geomean)
Fuzzer Combination
Cupid
EnFuzz
58
58
2617
2603
711
707
3577
3161
697
668
811
809
2123
2014
786
784
779
777
864
863
2819
2715
913
913
1102
1058
17835
17147
+4.01%
-
-
+2.31%
90% Coverage
0.00%
-6.28%
+135.91%
+36.54%
+64.57%
+51.18%
+160.85%
-5.26%
+29.17%
+69.23%
-2.80%
+489.64%
+75.32%