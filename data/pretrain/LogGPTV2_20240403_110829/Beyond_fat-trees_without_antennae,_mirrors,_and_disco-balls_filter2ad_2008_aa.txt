title:Beyond fat-trees without antennae, mirrors, and disco-balls
author:Simon Kassing and
Asaf Valadarsky and
Gal Shahaf and
Michael Schapira and
Ankit Singla
Beyond fat-trees without antennae, mirrors, and disco-balls
Simon Kassing
ETH Zürich
PI:EMAIL
Asaf Valadarsky
Hebrew University of Jerusalem
PI:EMAIL
Gal Shahaf
Hebrew University of Jerusalem
PI:EMAIL
Michael Schapira
Hebrew University of Jerusalem
PI:EMAIL
Abstract
Recent studies have observed that large data center networks often
have a few hotspots while most of the network is underutilized.
Consequently, numerous data center network designs have ex-
plored the approach of identifying these communication hotspots
in real-time and eliminating them by leveraging flexible optical or
wireless connections to dynamically alter the network topology.
These proposals are based on the premise that statically wired
network topologies, which lack the opportunity for such online
optimization, are fundamentally inefficient, and must be built at
uniform full capacity to handle unpredictably skewed traffic.
We show this assumption to be false. Our results establish that
state-of-the-art static networks can also achieve the performance
benefits claimed by dynamic, reconfigurable designs of the same
cost: for the skewed traffic workloads used to make the case for
dynamic networks, the evaluated static networks can achieve
performance matching full-bandwidth fat-trees at two-thirds of the
cost. Surprisingly, this can be accomplished even without relying
on any form of online optimization, including the optimization of
routing configuration in response to the traffic demands.
Our results substantially lower the barriers for improving upon
today’s data centers by showing that a static, cabling-friendly topol-
ogy built using commodity equipment yields superior performance
when combined with well-understood routing methods.
CCS Concepts
• Networks → Data center networks;
Keywords
Data center; Topology; Routing
ACM Reference format:
Simon Kassing, Asaf Valadarsky, Gal Shahaf, Michael Schapira, and Ankit
Singla. 2017. Beyond fat-trees without antennae, mirrors, and disco-balls.
In Proceedings of SIGCOMM ’17, Los Angeles, CA, USA, August 21–25, 2017,
14 pages.
https://doi.org/10.1145/3098822.3098836
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to
Association for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08...$15.00
https://doi.org/10.1145/3098822.3098836
Ankit Singla
ETH Zürich
PI:EMAIL
1 Introduction
Virtually every popular Web service today is backed by data center
infrastructure. With the growth of these services, the supporting
infrastructure has also acquired massive scale, with data centers
being designed with as many as 100,000 servers [15]. For such
large facilities, engineering full-bandwidth connectivity between
all pairs of servers entails significant effort and expense. Further,
for many facilities, at any given time, only a fraction of servers may
require high-bandwidth connectivity [13, 17], making such a design
seem wasteful. However, traditional topologies like the fat-tree [3]
present network designers with only two choices: (a) either build
an expensive, rearrangeably non-blocking network; or (b) build
a cheaper, oversubscribed network that does not provide high-
bandwidth connectivity even to small (albeit arbitrary) subsets of
the server pool, thereby needing aggressive workload management.
Over the past few years, numerous data center network designs
have tackled this problem based on the idea of adapting the topology
itself to the traffic demands [10, 12–14, 17, 18, 24, 27, 35, 40]. The key
insight in this literature is that if only a few network hotspots exist
at any time, perhaps through online measurement, these hotspots
can be identified, and the network topology can be optimized
dynamically to alleviate them. Reconfigurable wireless and optical
elements enable such online adjustments of connectivity. For many
workloads, such a network can match the performance of a much
more expensive interconnect that provides full bandwidth between
all pairs of servers at all times. The ingenuity of the many proposals
along these lines lies in the varied capabilities of the reconfigurable
elements, how they are connected together (including movable
wireless antennae [17], ceiling mirrors [18, 40], and disco-balls [13]),
and the algorithms for online management of connectivity.
This broad approach has obvious, intuitive appeal, and indeed,
some of the evaluations show performance similar to a full-
bandwidth fat-tree at 25-40% lower cost for some workloads [13, 18].
Not only are such results impressive, it is also unclear whether there
are any other viable options besides full-bandwidth topologies and
such dynamic topologies. Before fleshing out their reconfigurable
optics-based architecture, Helios [12], its authors summarized the
(2010) state of data center network topology design as follows:
“Unfortunately, given current data center network architec-
tures, the only way to provision required bandwidth between
dynamically changing sets of nodes is to build a non-blocking
switch fabric at the scale of an entire data center, with
potentially hundreds of thousands of ports.”
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Kassing et al.
More recently, the 3D-beamforming proposal [40] (2012) made a
similar assessment, and FireFly [18] (2014) explicitly considered
only two design possibilities: a full bisection-bandwidth network
and topology dynamism. This literature has thus implied that
static networks, which lack any opportunity for online topology
optimization, are inherently inefficient for unpredictably skewed
network traffic, and consequently, that dynamic topologies are the
only alternative to expensive full-bandwidth fabrics.
The goals of our work are to examine this presumption critically,
explore the utility and limits of topology dynamism, develop
an understanding of the relative strength of static and dynamic
networks, and outline directions for improving upon today’s data
centers. Towards these goals we make the following contributions:
(1) A metric for network flexibility: So far, there has been
no established metric for the flexibility of topologies towards ac-
commodating skewed traffic, with practitioners relying on specific
workload instances for evaluations. We thus propose an intuitive
metric capturing a topology’s performance under traffic skew.
(2) A comparison of static and dynamic topologies: We com-
pare static and dynamic networks under the assumptions of optimal
topology dynamics and traffic engineering. This comparison uses a
linear program optimization of a fluid-flow model, thus abstracting
out possible inefficiencies of routing, congestion control, and the
traffic-estimation and optimization needed for dynamic networks.
Our results show that at equal-cost, recently proposed static
networks outperform dynamic networks in this setting, with both
achieving substantial performance benefits over fat-trees.
(3) Routing on static networks: Translating performance in
fluid-flow models to that at packet-level can be challenging,
particularly with dynamic, unpredictable traffic. Surprisingly, we
find that even simple, oblivious routing achieves high performance
for the static networks we consider. Thus, even online optimization
of routes is not essential for achieving efficiency substantially higher
than fat-trees and comparable with dynamic networks. Over the
same skewed workloads recently used to make the case for dynamic
networks and heretofore considered challenging for static networks,
static networks achieve performance gains over fat-trees similar to
those claimed by dynamic networks at the same price point.
(4) Concrete, deployable alternatives to today’s data centers:
Our results suggest that the least-resistance path beyond fat-
trees may lie in transitioning to superior, cabling-friendly, static
networks, such as Xpander [33], and that dynamic networks have
not yet demonstrated an advantage over such static networks. We
discuss the implications for future research on data center network
design, especially for dynamic topologies, in §7.
To aid reproducibility of our results, help researchers and
practitioners run further experiments, and provide an easy-to-
use baseline for future research on dynamic networks to compare
against, our simulation framework is available online [1].
2 Network flexibility
This section examines the instructive example of the inflexibility of
oversubscribed fat-trees towards skewed traffic matrices (§2.1) and
introduces a quantitative notion of the desired flexibility (§2.2).
Figure 1: A k = 4 fat-tree. With more than 75% of the network’s capacity
intact, if all 4 servers in one pod sent traffic to servers in another pod (thus
involving 50% of all servers), each would get only 75% of the bandwidth.
2.1 Fat-trees are inflexible
Oversubscribed fat-trees are fundamentally limited in their ability
to support skewed traffic matrices. In the example shown in Fig. 1,
the fat-tree is oversubscribed by removing one root switch. If each
server in one pod is communicating with a (different) server in
another pod, not all such connections can get full bandwidth. In
this example, the network still has more than 75% of its original
capacity, and yet cannot provide full bandwidth connectivity for a
traffic matrix involving only 50% of the servers. This observation
can be formalized quite simply, as follows:
Observation 1. If a fat-tree built with k-port switches is over-
subscribed to x fraction of its full capacity, then there exists a traffic
matrix involving 2/k-fraction of the servers such that no more than
x fraction of throughput per server is achievable.
Proof. The proof is constructive, i.e., we provide the specific
traffic matrix which is bottlenecked to no more than x per-server
throughput. If the oversubscription is at the top-of-rack (ToR)
switch, with each ToR supporting s servers with only sx network
ports, then trivially, any traffic matrix (with 2s servers) where each
server under one ToR sends traffic to a unique server under another
ToR, is bottlenecked at the sender’s ToR to x throughput. Note that
nonuniform oversubscription only pushes throughput lower — in
that case, some ToR uses even fewer connections to the network
and achieves throughput lower than x.
Next, suppose that the aggregation layer (i.e., the middle layer
of switches) is oversubscribed to an x fraction, i.e., each switch in
this layer has a links connecting it to ToRs and ax links connecting
it to the core layer (the upper layer of switches). Then, any traffic
matrix where each server in a pod sends traffic to a unique server
not within the same pod, is bottlecked at the aggregation layer to x
fraction of throughput. The same argument applies when the core
layer is oversubscribed. In these cases, the number of servers in
the difficult traffic matrices is two times the number of servers in a
pod, i.e., 2/k of the servers in the network.
□
For a fat-tree built using 64-port switches, a 2/k fraction of the
servers would be a mere 3%. If this fat-tree were built at, say 50%
oversubscription, a pair of pods comprising only 3% of the network’s
servers could still not achieve full throughput, even while the rest
of the network idles. Only if 50% or less of the servers in each of the
pods were involved, could such an oversubscribed fat-tree achieve
full throughput for these servers. Of course, if it were known a priori
which pods might need higher bandwidth, different pods may be
Fat-tree’s inﬂexibilityBottleneckServersPodBeyond fat-trees without antennae, mirrors, and disco-balls
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
oversubscribed differently, but this would be a strong assumption
on the predictability and stability of traffic, and pod-pod traffic
would need aggressive management in such scenarios. Other Clos-
network-based designs suffer from similar problems.
Thus, as literature on topology adaptation rightly points out,
oversubscribed fat-trees score low on any metric of network
flexibility. But what precisely is the metric? So far, there has not
been a clearly specified and easily replicable standard for evaluating
the flexibility of a topology towards accommodating skewed traffic
matrices1. In the following, we offer a simple starting point.
2.2 Throughput proportionality
We would like to build static networks that can move around their
limited capacity to meet traffic demands in a targeted fashion. A
network’s total capacity is fixed, and defined by the total link
capacity of its edges, and for each server, the network expends
this capacity on routing its flows. We may hope that as the number
of servers participating in the traffic matrix (TM) decreases, we see
a proportional increase in throughput. In the following, we describe
a benchmark for network flexibility capturing this intuition.
Modeling throughput per server: Consider an arbitrary static
network G with N servers. A traffic matrix M captures traffic
demands between the N servers, with mij specifying the demand
from server i to server j. We restrict traffic matrices to the hose
model, i.e., the sum of demands from (to) each server is limited by
the outgoing (incoming) capacity of its network interfaces.
G is said to support a TM M with the satisfaction matrix TM if

each flow i → j in M achieves throughput TM(i, j) without violating
link-capacity constraints. Throughput for server i is then ti =
j TM(i, j). G is said to support M with throughput t if there is
a satisfaction matrix where each server simultaneously achieves
throughput at least t, i.e.,∃TM :∀i ∈ [N] ti ≥ t. Defining throughput
in this way, at a per-server granularity (rather than flow or server-