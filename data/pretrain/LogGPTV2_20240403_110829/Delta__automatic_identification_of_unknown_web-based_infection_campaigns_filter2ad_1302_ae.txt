system developers are trying to catch and prevent these eva-
sions. Previous work on evasions motivated the search for
better and diﬀerent detection systems [34]. More advanced
obfuscation [35], encryption, poly- and metamorphic code [36]
and virtualized environments have become more common in
response to these improvements and impeded detection sys-
tems. With more approaches being able to handle these cases,
it is to be expected that malware and infection vectors will
evolve and successfully circumvent available detection systems.
4JavaScript events are called mutation events if they modify
the DOM tree, e.g., by changing attributes of a node, such
as the src attribute of a script tag, or inserting or removing
elements from the DOM tree.
50msMedian500ms1s3s10s20s40sTime0.00.10.20.30.40.50.60.70.80.91.0RatioofTotalPairsAnalysisTimeCDF1187.1 Detection of Malicious Code
Numerous papers have been published on detecting mali-
cious activity in web sites. The majority of them focus on
dynamic analysis of JavaScript in instrumented environments
or on rendering web sites in high-interaction client honey-
pots. Generally, it is important to recall that our system
provides additional information: the infection campaign and
the responsible node of the DOM tree.
Eshete et al. [40] discuss the eﬀectiveness and eﬃcacy issues
of malicious web site detection techniques. Approaches from
blacklists, to static heuristics, to dynamic analysis are com-
pared in their detection accuracy and time spent analyzing
the web site. A major argument in short-comings of previous
work is the missing discussion on the necessity of episodic
re-training or online learning capabilities, to keep up with the
ongoing evolution of web-based malware, and to prevent the
evasion of deployed detection systems.
Cova et al. [12] introduce the system JSAND to detect and
analyze drive-by download attacks and malicious JavaScript
in an instrumented environment. The system leverages a
comprehensive dynamic analysis approach by instrumenting
JavaScript to extract a variety of diﬀerent features from redi-
rection and cloaking, to deobfuscation, to observing heap
exploitation. The system is compared to client honeypots,
such as Capture-HPC and PhoneyC, as well as the anti-virus
engine ClamAV. It shows a much lower false positive (0%) and
false negative rate (0.2%) than all other approaches (5.2% to
80.6% respectively), while taking an average of 16.05 seconds
to analyze a web site. CaptureHPC, the closest system in
terms of accuracy takes 20 seconds per sample.
Canali et al. [13] extend the dynamic analysis system JSAND,
by implementing a faster pre-ﬁltering step. The main goal is
to prevent the submission of certainly-benign web sites to the
dynamic analysis system and hereby to reduce the time spent
on analyzing benign samples, i.e., the system assigns to a false
negative a much higher cost than it does to a false positive.
The ﬁlter method leverages a C4.5 (J48) decision tree and a
diverse set of features spanning from the HTML content, to
the JavaScript code, to information about the host, to uniform
resource location (URL) patterns. The ﬁlter is evaluated on a
dataset of 15,000 web sites and compared to similar methods
by Seifert et al. [31] and Ma et al. [39]. Both other methods
yield more false positives and false negatives, but process up
to 10 times more samples in the same time.
Provos et al. [11, 14] introduce a system to detect URLs
to malicious web sites. However, they are not considering
legitimate, infected web sites in general, as their approach is
restricted to detecting the inclusion of exploit pages, and hence
their approach is complementary to our system’s capabilities.
Their system uses a proprietary machine learning algorithm
to classify URLs based on features like use in “out of place”
inline frames, obfuscated JavaScript, or links to known mal-
ware distribution sites. Besides detecting 90% of all malicious
landing pages with 0.1% false positives, they validate previous
work by Moshchuk et al. [41] that infection vectors are inserted
into legitimate web sites through exploiting vulnerabilities,
advertisement networks, and third party widgets.
7.2 Web Dynamics in Security
Maggi et al. [10] introduce a web application intrusion
detection system, which is able to learn about changes made to
the web application. The problem of web application concept
drift is addressed by learning how the web application is
accessed by a legitimate user and employing an unsupervised
classiﬁcation algorithm. Features include, for example, a
sequence corresponding to the order in which web sites are
accessed or how web page parameters are distributed. However,
the presented technique is orthogonal to our approach. The
main goal is not to ﬁnd new infection campaigns or to protect
the visitor of a web site, but rather to protect the integrity
of the web application. Protecting a normal, wandering user
would require intrusion detection and protection of all web sites
the user visits, since the access pattern, on which the system is
based on, depend on the underlying architecture of the web site.
Although possible theoretically, it is practically impossible.
Davanzi et al. [42] studied a similar approach for detecting
the impact of web dynamics. They introduce a system to de-
tect if changes made to a web site are defacements, which might
cause serious harm to the organization, money- or reputation-
wise, or are legitimate, “oﬃcially approved” content changes.
However, they explicitly point out that their approach does
not work with malicious modiﬁcations because their approach
detects changes that are visible to the end-user, which is the
exact opposite of how malicious infection vectors are placed in
practice. In detail, they employ anomaly detection to regularly
visit and monitor a set of 300 web sites actively and detect if
changes made to the web site constitute a defacement or not.
8 CONCLUSION
In this paper, we introduced the ∆-system, a novel, light-
weight system to identify changes associated with malicious
and benign behavior in web sites. The system leverages clus-
tering of modiﬁcation-motivated features, which are extracted
based on two versions of a web site, rather than analyzing the
web site in its entirety. To extract the important modiﬁcations
accurately, we introduced a fuzzy tree diﬀerence algorithm that
extracts DOM tree nodes that were more heavily modiﬁed,
discarding changes in single characters or words, or legitimate
evolutions. Beyond detecting if a change made to a web site
is associated with malicious behavior or not, we showed that
the ∆-system supports the detection of previously-unknown
infection campaigns by analyzing, unknown trends and mea-
suring the similarity to previous, known infection campaigns.
Furthermore, we showed that the system can generate an
identifying signature of observed infection campaigns, which
can then be leveraged to protect users via content-based de-
tection systems or as test-cases for online analyzer systems.
Ultimately, the system’s ability to identify speciﬁc infections is
helpful in identifying the reason why the web site was infected
by a speciﬁc campaign in the ﬁrst place, such as a distinct ver-
sion of the web application among all infections; additionally,
it facilitates the removal of malicious code and the mitigation
of additional infections in the future.
Acknowledgment
We want to express our gratitude toward our shepherd Matt
Bishop and the reviewers for their helpful feedback, valuable
comments and suggestions to improve the quality of the paper.
This work was supported by the Oﬃce of Naval Research
(ONR) under grant N000140911042, the Army Research Of-
ﬁce (ARO) under grant W911NF0910553, the National Sci-
ence Foundation (NSF) under grants CNS-0845559 and CNS-
0905537, and Secure Business Austria.
9 REFERENCES
[1] SOPHOS Security Team, “SOPHOS Security Threat Report
2013,” SOPHOS, Tech. Rep., 2013. [Online]. Available:
http://goo.gl/YuW65
[2] P. Baccas, “Malware injected
into legitimate JavaScript code on legitimate websites,”
Article, 2013. [Online]. Available: http://goo.gl/rDFZ4
119[3] D. Goodin, “Twitter
detects and shuts down password data hack in progress,”
February 2013. [Online]. Available: http://goo.gl/YwfMd
[4] Facebook Security Team, “Protecting People On Facebook,” Ar-
ticle, February 2013. [Online]. Available: http://goo.gl/OUPtk
[5] J. Finke and J. Menn, “Exclusive:
Apple, Macs hit by hackers who targeted Facebook,” Reuters,
February 2013. [Online]. Available: http://goo.gl/fzhIo
[6] D. Fetterly, M. Manasse, M. Najork, and
J. Wiener, “A large-scale study of the evolution of web pages,”
in Proceedings of the 12th International Conference on
World Wide Web, ser. WWW ’03. ACM, 2003, pp. 669–678.
[7] B. A. Huberman and L. A. Adamic, “Evolutionary Dynamics
of the world wide web,” Condensed Matter, January 1999.
[8] F. Douglis, A. Feldmann, B. Krishnamurthy,
and J. Mogul, “Rate of Change and other Metrics:
a Live Study of the World Wide Web.” in Proceedings
of the USENIX Symposium on Internet Technologies
and Systems, vol. 119. USENIX Association, 1997.
[9] R. Baeza-Yates, C. Castillo, and
F. Saint-Jean, “Web Dynamics, Structure, and Page Quality,”
in Web Dynamics. Springer-Verlag, 2004, pp. 93–109.
[10] F. Maggi, W. Robertson, C. Kruegel,
and G. Vigna, “Protecting a Moving Target: Addressing
Web Application Concept Drift,” in Proceedings of the 12th
International Symposium on Recent Advances in Intrusion
Detection, ser. RAID ’09. Springer-Verlag, 2009, pp. 21–40.
[11] N. Provos,
D. McNamee, P. Mavrommatis, K. Wang, and N. Modadugu,
“The ghost in the browser analysis of web-based malware,”
in First Workshop on Hot Topics in Understanding Botnets,
ser. HOTBOTS ’07. USENIX Association, 2007, pp. 4–4.
[12] M. Cova, C. Kruegel, and G. Vigna, “Detection and analysis
of drive-by-download attacks and malicious JavaScript
code,” in Proceedings of the 19th International Conference on
World Wide Web, ser. WWW’10. ACM, 2010, pp. 281–290.
[13] D. Canali, M. Cova,
G. Vigna, and C. Kruegel, “Prophiler: a fast ﬁlter for the
large-scale detection of malicious web pages,” in Proceedings
of the 20th International Conference on World Wide
Web (WWW ’11), ser. WWW ’11. ACM, 2011, pp. 197–206.
[14] N. Provos, P. Mavrommatis,
M. A. Rajab, and F. Monrose, “All your iFRAMEs point to
Us,” in Proceedings of the 17th USENIX Security Symposium,
ser. SEC’08. USENIX Association, 2008, pp. 1–15.
[15] P.-M. Bureau, “Linux/Cdorked.A: New
Apache backdoor being used in the wild to serve Blackhole,”
April 2013. [Online]. Available: http://goo.gl/g2Vﬂ
[16] D. Cid, “Apache Binary Backdoors on Cpanel-based servers,”
April 2013. [Online]. Available: http://goo.gl/BXq8Q
F. Piessens, and G. Vigna, “You Are What You Include:
Large-scale Evaluation of Remote JavaScript Inclusions,”
in Proceedings of the 19th ACM Conference on Computer
and Communications Security, ser. CCS ’12. ACM, 2012.
[24] U. Fayyad, G. Piatetsky-Shapiro,
P. Smyth et al., “Knowledge Discovery and
Data Mining: Towards a Unifying Framework.” Knowledge
Discovery and Data Mining, vol. 96, pp. 82–88, 1996.
[25] U. M.
Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,
“Advances in knowledge discovery and data mining,” 1996.
[26] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander, “OP-
TICS: Ordering Points To Identify the Clustering Structure,”
in Proceedings of the ACM SIGMOD International Conference
on Management of Data, ser. MOD’99. ACM, 1999, pp. 49–60.
[27] M. Breunig, H.-P. Kriegel, R. T. Ng,
and J. Sander, “OPTICS-OF: Identifying Local Outliers,” in
Lecture Notes in Computer Science. Springer-Verlag, 1999.
[28] L. Invernizzi, P. Milani Comparetti, S. Benvenuti, M. Cova,
C. Kruegel, and G. Vigna, “EvilSeed: A Guided Approach
to Finding Malicious Web Pages,” Security and Privacy
(SP), 2012 IEEE Symposium on, vol. 0, pp. 428–442, 2012.
[29] P. Ratanaworabhan, B. Livshits, and B. Zorn, “Nozzle:
A defense against heap-spraying code injection attacks,”
in Proceedings of the 18th USENIX Security Symposium,
ser. SEC’09. USENIX Association, 2009, pp. 169–186.
[30] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “ZOZZLE:
fast and precise in-browser JavaScript malware detection,”
in Proceedings of the 20th USENIX Security Symposium,
ser. SEC’11. USENIX Association, 2011, pp. 3–3.
[31] C. Seifert, I. Welch,
and P. Komisarczuk, “Identiﬁcation of Malicious Web Pages
with Static Heuristics,” in Telecommunication Networks and
Applications Conference, ser. ATNAC ’08, 2008, pp. 91–96.
[32] “Discuz!” Retrieved,
May 2013. [Online]. Available: http://goo.gl/e8nCD
[33] X. Yang, “Report on the success of
the Discuz! software,” Chinese National Radio Report, August
2010, in Chinese. [Online]. Available: http://goo.gl/beq4O
[34] M. Rajab, L. Ballard, N. Jagpal,
P. Mavrommatis, D. Nojiri, N. Provos, and L. Schmidt,
“Trends in Circumventing Web-Malware Detection,” 2011.
[35] J. Mason, S. Small, F. Monrose,
and G. MacManus, “English shellcode,” in Proceedings of
the 16th ACM Conference on Computer and Communications
Security, ser. CCS ’09. ACM, 2009, pp. 524–533.
[36] M. Polychronakis, K. Anagnostakis,
and E. Markatos, “Network-level polymorphic shellcode
detection using emulation,” Journal in Computer Virology,
vol. 2, pp. 257–274, 2007, 10.1007/s11416-006-0031-z.
[17] S. S. Chawathe and H. Garcia-Molina,
[37] J. Choi, G. Kim, T. Kim, and S. Kim,
“Meaningful Change Detection in Structured Data,” in
Proceedings of the ACM SIGMOD International Conference
on Management of Data, ser. MOD’97. ACM, 1997.
[18] Y. Wang, D. J. DeWitt, and J.-Y. Cai, “X-Diﬀ:
An eﬀective change detection algorithm for XML documents,”
in Proceedings of the 19th International Conference on
Data Engineering, ser. ICDE ’03.
IEEE, 2003, pp. 519–530.
[19] H. W. Kuhn,
“The Hungarian method for the assignment problem,” Naval
Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83–97, 1955.
[20] J. Kornblum, “Identifying almost identical ﬁles using
context triggered piecewise hashing,” Digital Investigation,
vol. 3, no. 0, pp. 91 – 97, 2006, the Proceedings of the 6th
Annual Digital Forensic Research Workshop (DFRWS ’06).
[21] M. A. Jaro, “Advances
in Record-Linkage Methodology as Applied to Matching
the 1985 Census of Tampa, Florida,” Journal of the American
Statistical Association, vol. 84, no. 406, pp. 414–420, 1989.
[22] A. N. Kolmogorov, “Three approaches to the
quantitative deﬁnition of information,” International Journal
of Computer Mathematics, vol. 2, no. 1-4, pp. 157–168, 1968.
[23] N. Nikiforakis, L. Invernizzi,
A. Kapravelos, S. Van Acker, W. Joosen, C. Kruegel,
“An Eﬃcient Filtering Method for Detecting Malicious
Web Pages,” in Proceedings of the 13th International
Workshop on Information Security Applications, 2012.
[38] Y.-T. Hou, Y. Chang, T. Chen, C.-S. Laih, and C.-M. Chen,
“Malicious web content detection by machine learning,” Expert
Systems with Applications, vol. 37, no. 1, pp. 55 – 60, 2010.
[39] J. Ma, L. K. Saul, S. Savage,
and G. M. Voelker, “Beyond blacklists: learning to detect
malicious web sites from suspicious URLs,” in Proceedings of
the 15th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, ser. KDD ’09. ACM, 2009, pp. 1245–1254.
[40] B. Eshete, A. Villaﬁorita, and K. Weldemariam, “Malicious
website detection: Eﬀectiveness and eﬃciency issues,” in First
SysSec Workshop, ser. SysSec.
IEEE, 2011, pp. 123 –126.
[41] A. Moshchuk, T. Bragin, S. D. Gribble, and H. M. Levy, “A
Crawler-based Study of Spyware in the Web,” in Network and
Distributed System Security Symposium, ser. NDSS ’06, 2006.
[42] G. Davanzo,
E. Medvet, and A. Bartoli, “Anomaly detection techniques for
a web defacement monitoring service,” Expert Systems with
Applications, vol. 38, no. 10, pp. 12 521–12 530, Sep. 2011.
120