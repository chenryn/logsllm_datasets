each server to support circuit scheduling, and to remain synchro-
nized with the OCS.
We modify the OS in three key ways. First, we adapt the Ethernet
NIC driver to listen for synchronization packets from the scheduler
so that the host knows the current state of the OCS. Second, we
modify the NIC driver to ignore the “link-down” events that occur
when the OCS is reconﬁguring. Third, we add a custom queu-
ing discipline (Qdisc) that drains packets from queues based on the
conﬁguration of the OCS.
Synchronization packets: The OCS FPGA controller transmits
synchronization packets to a separate 10G packet-switched net-
work which connects to a second Ethernet port on each server.
These packets are sent before and after reconﬁguration so that all
connected devices know the state of the OCS. The packets include
the slot number, the slot duration, and whether the circuits are be-
ing setup or torn down. The connected devices maintain a map
between the slot number and each destination. The Ethernet NIC
driver also maintains a data structure with a set of per-circuit tokens
to control the data transmission time and rate. Note that this sepa-
rate signaling network would not be needed in a production system
where access to the NIC ﬁrmware would be available.
Link-down events: Since the OCS is switching rapidly, the host
NIC ports attached to the OCS experience numerous link-up and
link-down events. When Linux receives a link-down event, it nor-
mally disables the interface and resets and closes any open sockets
ApplicationOperating SystemNICNetworkprocess_sync_frame()qdisc_dequeue()slot #slot durationsetup/teardownTDMA StateSync Framecurrent slot #slot start (ns)tokensdestinationTCP/IP Stacksend()qdisc_enqueue()Queuesselect_queue(ip_address)dma_copy()Transmit Data FrameReceive Sync Frameyield()11415234567891011121316ApplicationOperating SystemNICNetworkprocess_sync_frame()qdisc_dequeue()slot #slot durationsetup/teardownTDMA StateSync Framecurrent slot #slot start (ns)tokensdestinationTCP/IP Stacksend()qdisc_enqueue()Queuesselect_queue(ip_address)dma_copy()Transmit Data FrameReceive Sync Frameyield()11415234567891011121316453and TCP connections. To prevent these resets, we disable the link-
down and link-up calls in the NIC driver.
Mordia Qdisc (shown in Figure 6): When a user’s application
sends data, that data transits the TCP/IP stack (1) and is encap-
sulated into a sequence of Ethernet frames. The kernel enqueues
these frames into our custom Qdisc (2), which then selects (3) one
of multiple virtual output queues (VOQs) based on the packet’s IP
address and the queue-to-destination map (4). The Ethernet frame
is enqueued (5) and the qdisc_dequeue function is scheduled (6)
using a softirq. The qdisc_dequeue function reads the current com-
munication slot number (7) and checks the queue length (8).
If
there is no frame to transmit, control is yielded back to the OS (9).
If there is a frame to transmit, the frame is DMA copied to the
Ethernet NIC (10–12). The total number of packets sent directly
corresponds to the number of tokens accumulated in the Ethernet
NIC’s data structure to control the timing and the rate. The kernel
then schedules the qdisc_dequeue function again (13) until VOQ is
empty and control is yielded back to the OS (9). When the next
sync frame arrives from the controller (14), it is processed, and
the scheduling state is updated (15). Then the kernel schedules the
qdisc_dequeue function with a softirq in case there are frames en-
queued that can now be transmitted (16). Given that all the packets
are only transmitted during the times that the slot is active, the code
for receiving packets did not need to be modiﬁed.
6. EVALUATION
Our evaluation seeks to:
1. Determine the baseline end-to-end reconﬁguration time
of the Mordia OCS as seen by ToRs. We ﬁnd that, ac-
counting for the hardware and software overheads, this re-
conﬁguration time is 11.5 µs.
2. Determine how precisely the control plane can keep ToR
devices synchronized with the OCS. On average, ToRs can
be synchronized within 1 µs. However, due to the non-realtime
nature of our Linux-based emulated ToRs, about 1% of syn-
chronization messages are received at the wrong times, lead-
ing to reduced link utilization.
3. Find the overall throughput and circuit utilization deliv-
ered by the Mordia OCS. We ﬁnd that despite the long-
tailed nature of our synchronization strategy, emulated ToRs
can utilize up to 95.4% of the link’s bandwidth.
4. Determine how well TCP performs on the OCS. We ﬁnd
that the emulated ToR can reach 87.9% of the bandwidth of
a comparable EPS for TCP trafﬁc, when disabling the NIC’s
TSO functionality. The gap between the EPS and OCS is due
to the use of commodity hardware and OS in our prototype,
and would not be expected in a real deployment.
We evaluate each of these questions below.
6.1 End-to-end reconﬁguration time
We know from other work [8] that the raw switching speed of
the underlying OCS does not determine the end-to-end switching
speed, since additional time is required for reinitializing the op-
tics and software overheads. In this section, we empirically mea-
sure the OCS switching speed as perceived at a packet level by the
devices connected to it. This fundamental switching speed gates
the expected performance we expect to see in the remainder of our
evaluation.
We ﬁrst connect 23 emulated ToRs (which we refer to as hosts)
to the OCS prototype (with host i connected to port i). Host 1
Figure 7: An example of packets sent across a set of variable-
length days. Each point represents a single packet, and recon-
ﬁguration periods are shown as gray vertical bars.
Figure 8: Histogram of the end-to-end OCS reconﬁguration
time, as seen by connected devices, using 705 samples. A nor-
mal curve is ﬁtted to the data.
transmits ﬁxed-size Ethernet frames at line rate, ignoring synchro-
nization packets. Host 1 transmits continuously, even during gaps.
Hosts 2 through 23 capture the ﬁrst 22 octets of each frame using
tcpdump. Each frame contains an incrementing sequence number
so we can detect loss. After each experiment, we merge the pcap
ﬁles from each host.
Figure 7 shows an experiment with regular-sized Ethernet frames
(1500 bytes) and variable-length slots (80 µs, 160 µs, and 240 µs).
The x-axis is time and the y-axis is packet sequence number. We
programmed the OCS with a round-robin schedule. The slot dura-
tions vary in size, which means that bandwidth is allocated propor-
tionally to different hosts based on the slot length. We highlight
gaps as gray vertical strips, and frames transmitted during gaps
are lost. The remaining frames are successfully received. The last
packet transmitted during a slot often gets dropped by the receiving
NIC because it is cut off in the middle by the OCS.
From a merged pcap trace of approximately one million packets,
we extracted 705 gaps in packet transmission. The length of each
gap is a measurement of tsetup. Figure 8 shows the resulting his-
togram. The data ﬁts a normal distribution with a mean of 11.55 µs
and a standard deviation of 2.36 µs. Note that this packet capture
is collected across several machines, and so some of the variance
shown in Figure 8 is due to clock skew across nodes.
With T = 106 µs and tsetup = 11.5 µs, the duty cycle is equal
0123.0246.1         369.1            492.2      615.2      738.2       861.3                  984.3       1107          1230 10020030040050060070080090010000Port 4Port 2Port 3Port 4Port 2Port 3Time (μs)Packet Sequence Number (RX)16.8013.4410.086.723.360.0160.0140.0120.010.0080.0060.0040.0020(N,µ,σ)=(705,11.55,2.36)Time (μs)Frequency454Figure 9: Host 1’s Qdisc receiving UDP packets from Hosts 12–
16 as it cycles through circuits connecting it to 22 other hosts.
Each point represents a 9000-byte packet.
to 89.15%. Therefore we expect to have captured 89.15% of the
transmitted Ethernet frames. From 997,917 transmitted packets,
we captured 871,731 packets, yielding a measured duty cycle of
87.35%. In other words, there are approximately 18,000 additional
missing packets. We attribute these additional missing packets pri-
marily to periods where the sender or receiver was interrupted by
the non-real-time Linux kernel.
Summary: These experiments show that the end-to-end recon-
ﬁguration latency of the Mordia OCS, including the time to re-
establish the link at the optical component level, is on average
11.5 µs. Further, the FPGA-based scheduler can establish variable-
length days and control the OCS with high precision.
6.2 Emulated ToR software
The previous section demonstrates that a single host can utilize
87.35% of a circuit with an 89.15% duty cycle. However, this mea-
surement does not account for host synchronization at the OCS
ports. Figure 9 shows Host 1 receiving 8,966 octet UDP packets
from Hosts 12–16 via the Qdisc described in Section 5.2.1 for a
day and night of 123.5 µs and 11.5 µs, respectively. The transmis-
sion time and sequence number of each packet is determined from
a tcpdump trace that runs on only Host 1.
First, we note that it takes on average 33.2 µs for the ﬁrst packet
of each circuit to reach Host 1. The tcpdump trace indicates that
it takes less than 3 µs to process the synchronization frame and
transmit the ﬁrst packet. When sending a 9000-octet frame (7.2 µs),
the packet spends at least 23 µs in the NIC before being transmitted.
To prevent this “NIC delay” from causing packet loss, the OS Qdisc
must stop queuing packets for transmission 23 µs early. The result
is that the Qdisc cannot use 23 µs of the slot due to the behavior
of the underlying NIC hardware. Vattikonda et al. [22] have shown
how the use of hardware NIC priority ﬂow control (PFC) pause
frames can be used to enable ﬁne-grained scheduling of circuits on
(all-electrical) data center networks, and this technique could be
applied to Mordia.
Summary: The Linux hosts used as emulated ToRs are able to
drain packets into the network during the appropriate “day,” which
can be as small as 61 µs. However, jitter in receiving synchro-
nization packets results in a 0.5% overall loss rate, and there is a
23 µs delay after each switch reconﬁguration before the NIC be-
gins sending packets to the OCS. These overheads and packet loss
are speciﬁc to our use of commodity hosts as ToRs.
Figure 10: Throughput delivered over the OCS. The funda-
mental difference between ideal and observed is due to the OCS
duty cycle. Further deviations are due to our system artifacts,
namely the lack of segmentation ofﬂoading in the NIC, NIC-
induced delay, and synchronization packet jitter. The mini-
mum slot length is 61 µs. The legend shows the maximum av-
erage receive rate for each switch and protocol combination.
6.3 Throughput
In the above experiments, we have attempted to characterize in-
dividual components of the Mordia OCS. In this section, we turn
our attention to analyzing the resulting end-to-end throughput de-
livered by the entire system.
To begin, we generated all-to-all TCP and UDP trafﬁc between
23 hosts and measured the throughput both over a traditional elec-
trical packet switch (EPS, used as a baseline) as well as our Mordia
OCS prototype including emulated ToR switches, shown in Fig-
ure 10. The throughput over the EPS serves as an upper bound on
the potential performance over the OCS. In addition, throughput
over the OCS is fundamentally limited by the OCS duty cycle.
To establish a baseline, we measured the goodput of a single
UDP ﬂow transiting the EPS switch (EPS-UDP) to be 9.81 Gbps.
We then found that as the number of hosts increases from 2 to 23,
the average rate degrades to 8.83 Gbps, which we attribute to the
kernel and NIC. The EPS supports a single TCP ﬂow’s goodput of
8.69 Gbps, which is within 1.6% of UDP trafﬁc. However, this
throughout relies on TCP segmentation ofﬂoading (TSO) support
in the NIC, which is incompatible with our Mordia kernel module.
2 On an all-electrical packet network, all-to-all TCP bandwidth
across 23 hosts without TSO support was found to be 6.26 Gbps
(EPS-TCP), which we use as an upper bound on the performance
we expect to see over the OCS. With NIC ﬁrmware access, we
could eliminate this reduction in bandwidth by having the TCP of-
ﬂoad engine not send packets during the circuit night time.
Figure 10 shows the raw bandwidth available to each host (cal-
culated as the duty cycle) from the OCS as OCS-IDEAL. It is im-
portant to remember that this line does not account for the 23.5 µs
NIC delay which reduces measured duty cycle further. For the ex-
periments, we varied the OCS slot duration between 61–300 µs
to observe the effect of different duty cycles (due to the program-
ming time of our WSSes, the smallest slot duration we support
is 61 µs). The OCS’s UDP throughput (OCS-UDP) ranges from
2The happens because, when circuits are reconﬁgured, any packets
in ﬂight are ‘runted’ by the link going down, and we lose con-
trol over the transmission of packets when relying on TSO. Conse-
quently, Mordia requires disabling TSO support.
0100200300400500600020406080100NightNICDelayμsHost12Host13Host14Host15Host16Received Data (KB)00!1!2!3!4!5!6!7!8!9!10!50!100!150!200!250!300! Avg. Rx Rate Per Host (Gb/s)!Circuit Day Duration (μs)!EPS-UDP (8.8 Gb/s)!EPS-­‐TCP	
  (6.3	
  Gb/s)	
  OCS-­‐IDEAL	
  (9.5	
  Gb/s)	
  OCS-­‐UDP	
  (8.4	
  Gb/s)	
  OCS-­‐TCP	
  (5.5	
  Gb/s)	
  455WDM: The Mordia prototype we built uses a single ring with 24
wavelength channels in the C-band to create a 24×24-port OCS.
Since the C-band contains 44 DWDM channels, it is straightfor-
ward to scale the prototype to 44 ports. Increasing the number of
wavelengths on a single ring beyond 44 is more difﬁcult. Mor-
dia happens to rely on 100 GHz spacing, but we could have used
50 GHz, 25 GHz, or even 12.5 GHz spacing. Each smaller incre-
ment doubles the number of channels. SFP+ modules with lasers
on the 50 GHz grid are commercially available, meaning that it
is straightforward to scale to 88 ports. However, the technology
to support 10G, 40G, and 100G Ethernet over narrower DWDM
channels might not yet be commercially available or might be cost
prohibitive in a data center environment. An alternative could be
to keep the 100 GHz spacing but to extend into the L-band. This
extension would allow a doubling of the number of channels, but
would make ampliﬁcation more difﬁcult. Thus the use of WDM
provides a small level of scalability up to a couple of hundred ports.
Bandwidth: As data center network link speeds increase over
time, e.g., to 40, 100, and 400 Gbps, the Mordia data plane need
not change, as it is agnostic to the transmission rate. However, the
control plane must increase in speed. Most relevant for Mordia’s
scalability is not the aggregate per-host bandwidth, but the underly-
ing line rate. We believe that on a ﬁve-year time horizon, the fastest
line rates will be 25-28 Gbps. All higher rates will be achieved by
aggregating multiple such lanes. For instance, 40 Gbps Ethernet is
in fact four lanes of 10 Gbps and the two 100 Gbps standards are
either 10 × 10 Gbps or 4 × 25 Gbps. The emerging 400 Gbps will
likely be 16 × 25 Gbps. This aggregation means that the worst-case
scaling performance of Mordia is 2.5× up to 400 Gbps. Further-