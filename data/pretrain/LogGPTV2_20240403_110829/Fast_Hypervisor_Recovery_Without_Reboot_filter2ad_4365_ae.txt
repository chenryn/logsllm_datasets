options during the normal boot up. These values are used
by the hypervisor to correctly initialize the system during
boot. Hence, ReHype recovery needs to reuse the previously
logged boot line options to correctly boot up the hypervisor.
The main difference in the implementation complexity
between NiLiHype and ReHype is in code that executes
during recovery. ReHype has signiﬁcantly more code needed
to preserve and re-integrate the failed hypervisor state back to
the new hypervisor instance.
VIII. RELATED WORK
NiLiHype is related to numerous works aimed at increasing
the resilience to errors in OS kernels and hypervisors. Most of
the work on OS kernels has focused on ways to partition the
kernel, isolate the partitions (fault domains) from each other,
and recover failed partitions without requiring full system
reboot [5], [9], [12], [23], [25]. In many cases this is facilitated
by an underlying design, based on a small microkernel and a
collection of drivers and servers isolated from each other by
the memory-management hardware [5], [9], [12]. On the other
hand, microreset, and thus NiLiHype, are aimed at monolithic
kernels and hypervisors.
VirtuOS [25] proposes vertical slicing of the Linux kernel to
service domains. The encapsulation of the slices is performed
using virtualization based on Xen. Requests from user pro-
cesses interact directly with the appropriate service domain,
which is isolated from the rest of the kernel. Since VirtuOS
requires virtualization, applying the idea to a hypervisor would
require nested virtualization with its associated overheads.
Akeso [23] dynamically partitions the Linux kernel into
request-oriented recovery domains. A recovery domain is
124
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
formed by the execution thread that handles a request, such
as a system call or interrupt. A modiﬁed compiler is used to
instrument the code to track state changes caused by the do-
main as well as dependencies among domains. When an error
is detected, the affected domain and dependent domains are
rolled back. Due to the code instrumentation, the performance
overhead of Akeso is between 8% and 560%.
NiLiHype can be viewed as a lightweight version of
Akeso [23]. As with Akeso, upon the detection of an error,
the affected execution thread handling a request is abandoned.
Instead of tracking dependencies among execution threads,
NiLiHype simply abandons all current execution threads. With
Akeso, the instrumentation added by the modiﬁed compiler
logs state that allows domains to be rolled back. NiLi-
Hype does not require a modiﬁed compiler. As described
in Section IV, NiLiHype relies on manual modiﬁcation of
the Xen code to facilitate successful retry of non-idempotent
hypercalls. Since NiLiHype does not involve the extensive
code instrumentation of Akeso, the performance overhead is
likely to be signiﬁcantly lower. However, a direct comparison
would require applying Akeso with Xen and performing the
measurements with identical workloads. Since NiLiHype does
not include the comprehensive compiler-implemented tracking
of state changes of Akeso, NiLiHype’s recovery rate is likely
to be lower.
As discussed in Section I, Yoshimura et al. [31], [32] pro-
posed the idea that it is possible to recover from some errors
in the Linux kernel without reboot. Their recovery scheme
always involves killing a running process. They achieved a
recovery success rate of 60%. In contrast, with NiLiHype the
recovery rate is over 88% and in over 83% of the cases no
AppVM is lost.
Otherworld [10] allows a Linux kernel to be recovered from
failures by a full reboot of the kernel, while preserving in
place the states of the running processes. Since the “system”
consists of the kernel as well as all the processes, this can
be viewed as a microreboot [7] of the kernel. Otherworld
rebuilds many kernel data structures associated with each
process, such as the process descriptor, the ﬁle descriptor
table, and signal handler descriptors. Restoration of kernel
components requires traversing many complex data structures
in a possibly corrupted kernel, increasing the chance of failed
recoveries. In many cases, user-level processes require custom
crash procedures in order to properly resume execution.
and thus does not deal with possible arbitrary corruptions and
inconsistencies in the system. On the other hand, NiLiHype
reuses almost the entire hypervisor state and is thus not useful
for rejuvenation [13].
TinyChecker [28] proposes the use of nested virtualization
to manage hardware enforced protection domains during hy-
pervisor execution. TinyChecker is a small hypervisor that runs
underneath the commodity hypervisor, monitors transitions be-
tween the commodity hypervisor and the VMs it hosts, limits
the memory regions writable during hypervisor execution, and
attempts to detect accesses that are possibly erroneous and
take checkpoints to facilitate recovery. TinyChecker has never
been fully implemented or evaluated. In a full implementation,
the nested virtualization mechanism is expected to involve
signiﬁcant overhead.
This work is most closely related to ReHype [19], [21],
that uses microreboot for recovery from hypervisor failure.
ReHype and comparisons between ReHype and NiLiHype
are extensively discussed throughout this paper. There are
works that use virtualization to provide resilience to device
driver failures [15], [17] and the ability to recover from
PrivVM failures [20]. The hypervisor, device drivers, and
PrivVM together form the virtualization infrastructure (VI).
The resilient VI can be combined with appropriate middleware
to provide, on a virtualized system, a resilient platform for
applications and services [18], [21].
IX. CONCLUSIONS AND FUTURE WORK
There are compelling reasons to enhance hypervisors with
the ability to recover from failures while allowing the VMs
they host to resume normal operation without loss of work.
With such capability, the fraction of datacenter capacity un-
available due to a single fault is reduced, there is greater
ﬂexibility in assigning VMs to hosts, and VM replication with
both replicas running on a single host becomes an attractive
point in the design space in some deployments.
ReHype, which is based on microreboot, has been pre-
viously presented as a mechanism for providing the above
hypervisor recovery capability. This paper investigated an al-
ternative to microreboot, which we call microreset, that allows
component-level recovery without reboot. Instead of rebooting
a new instance, microreset resets the component to a quiescent
state that is highly likely to be valid. Microreset is suitable for
large, complex components that process requests from the rest
of the system. The state reset it performs involves discarding
all threads of execution within the component. By avoiding
component reboot, microreset has the potential to achieve
signiﬁcantly lower recovery latencies than microreboot.
NiLiHype utilizes microreset to implement recovery from
hypervisor failures. To achieve a high recovery rate, NiLiHype
includes numerous enhancements needed to restore the hyper-
visor to a valid consistent state. Thus, the idea of microreset,
by itself, is not sufﬁcient for building an effective recovery
scheme.
We have implemented NiLiHype and evaluated it in terms
of recovery rate, recovery latency, hypervisor processing over-
A hypervisor maintains less state for each VM compared
to the state for each process maintained by the kernel. This
makes the microreboot of a hypervisor simpler than of kernel
and increases the chance of a successful recovery [19].
RootHammer [16] uses microreboot to rejuvenate virtu-
alized systems based on the Xen. It reduces the time for
this rejuvenation by rebooting only the Xen hypervisor and
the PrivVM, while preserving in memory the states of VMs
and their conﬁgurations. During rejuvenation, the PrivVM is
properly shut down and the VMs suspend themselves cleanly.
Hence, RootHammer operates within a healthy and functioning
system. RootHammer is not designed to recover from failure
125
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
head during normal operation, and implementation complexity.
We have shown that NiLiHype achieves a recovery rate of
over 88%, only 2% lower than the rate achieved by Re-
Hype. In return, NiLiHype’s recovery latency is over a factor
of 30 lower, at 22ms. With this low recovery latency, for
many important application, service interruption would not
be noticeable. Based on our measurements, the performance
overhead of NiLiHype during normal operation is expected to
be under 1%. NiLiHype’s implementation required adding or
modifying less than 2200 lines in the Xen hypervisor.
One of the remaining questions regarding microreset-based
recovery, is the extent to which it is applicable to components
other than OS kernels and hypervisors. Investigating this
question is part of our future work.
Future work on NiLiHype will involve evaluation with more
complex conﬁgurations, that include multiple vCPUs per CPU
as well as a variety of workloads running in the VMs. We
also plan to evaluate NiLiHype’s effectiveness under additional
fault types.
It is clear from prior work on ReHype as well as our work
on NiLiHype that the development of enhancements necessary
for achieving a high recovery rate is a difﬁcult process. The
changes to mitigate recovery failures due to non-idempotent
hypercalls were particularly tough to develop. Another part of
our future work is to investigate more systematic techniques
for performing such enhancements.
REFERENCES
lines of code,” http://cloc.sourceforge.net/, accessed:
https://github.com/kdlucas/byte-unixbench,
accessed:
[1] “Cloc – count
2017-11-13.
[2] “Unixbench,”
2017-10-12.
[3] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neuge-
bauer, I. Pratt, and A. Warﬁeld, “Xen and the art of virtualization,” in
19th ACM Symposium on Operating Systems Principles, Bolton Landing,
NY, Oct. 2003, pp. 164–177.
[4] M. Ben-Yehuda, M. D. Day, Z. Dubitzky, M. Factor, N. Har’El,
A. Gordon, A. Liguori, O. Wasserman, and B.-A. Yassour, “The Turtles
project: Design and implementation of nested virtualization,” in 9th
USENIX Conference on Operating Systems Design and Implementation,
Vancouver, BC, Canada, Oct. 2010, pp. 423–436.
[5] K. Bhat, D. Vogt, E. van der Kouwe, B. Gras, L. Sambuc, A. S.
Tanenbaum, H. Bos, and C. Giuffrida, “OSIRIS: efﬁcient and consistent
recovery of compartmentalized operating systems,” in 46th Annual
IEEE/IFIP International Conference on Dependable Systems and Net-
works, Toulouse, France, Jun. 2016, pp. 25–36.
[6] J. Buell, D. Hecht, J. Heo, K. Saladi, and H. R. Taheri, “Methodology
for performance analysis of VMware vSphere under tier-1 applications,”
VMware Technical Journal, vol. 2, no. 1, pp. 19–28, Jun. 2013.
[7] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox, “Microre-
boot — a technique for cheap recovery,” in 6th Symposium on Operating
Systems Design and Implementation, San Francisco, CA, Dec. 2004, pp.
31–44.
[8] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson, and
A. Warﬁeld, “Remus: High availability via asynchronous virtual machine
replication,” in 5th USENIX Symposium on Networked Systems Design
and Implementation, San Francisco, CA, Apr. 2008, pp. 161–174.
[9] F. M. David, E. M. Chan, J. C. Carlyle, and R. H. Campbell, “CuriOS:
improving reliability through operating system structure,” in 8th USENIX
Conference on Operating Systems Design and Implementation, San
Diego, California, Dec. 2008, pp. 59–72.
[10] A. Depoutovitch and M. Stumm, “Otherworld: Giving applications a
chance to survive os kernel crashes,” in 5th European conference on
Computer systems, Paris, France, Apr. 2010, pp. 181–194.
[11] J. Gray, “Why do computers stop and what can be done about it?”
in 5th Symposium on Reliability in Distributed Software and Database
Systems, Jan. 1986, pp. 3–12.
[12] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum, “Con-
struction of a highly dependable operating system,” in Sixth European
Dependable Computing Conference, Coimbra, Portugal, Oct. 2006.
[13] Y. Huang, C. Kintala, N. Kolettis, and N. D. Fulton, “Software reju-
venation: Analysis, module and applications,” in 25th Fault-Tolerant
Computing Symposium, Pasadena, CA, Jun. 1995, pp. 381–390.
[14] C. M. Jeffery and R. J. Figueiredo, “A ﬂexible approach to improving
system reliability with virtual lockstep,” IEEE Transactions on Depend-
able and Secure Computing, vol. 9, no. 1, pp. 2–15, Jan. 2012.
[15] H. Jo, H. Kim, J.-W. Jang, J. Lee, and S. Maeng, “Transparent fault
tolerance of device drivers for virtual machines,” IEEE Transactions on
Computers, vol. 59, no. 11, pp. 1466–1479, Nov. 2010.
[16] K. Kourai and S. Chiba, “A fast rejuvenation technique for server consol-
idation with virtual machines,” in 37th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, Edinburgh, UK, Jun.
2007, pp. 245–255.
[17] M. Le, A. Gallagher, Y. Tamir, and Y. Turner, “Maintaining network
QoS across NIC device driver failures using virtualization,” in 8th IEEE
International Symposium on Network Computing and Applications,
Cambridge, MA, Jul. 2009, pp. 195–202.
[18] M. Le, I. Hsu, and Y. Tamir, “Resilient virtual clusters,” in 17th
IEEE Paciﬁc Rim International Symposium on Dependable Computing,
Pasadena, CA, Dec. 2011, pp. 214–223.
[19] M. Le and Y. Tamir, “ReHype: enabling VM survival across hypervisor
failures,” in 7th ACM International Conference on Virtual Execution
Environments, Newport Beach, CA, Mar. 2011, pp. 63–74.
[20] ——, “Applying microreboot to system software,” in IEEE International
Conference on Software Security and Reliability, Washington, D.C., Jun.
2012, pp. 11–20.
[21] ——, “Resilient virtualized systems using ReHype,” UCLA Computer
Science Department Technical Report #140019, Oct. 2014.
[22] ——, “Fault injection in virtualized systems – challenges and appli-
cations,” IEEE Transactions on Dependable and Secure Computing,
vol. 12, no. 3, pp. 284–297, May 2015.
[23] A. Lenharth, V. S. Adve, and S. T. King, “Recovery domains: An orga-
nizing principle for recoverable operating systems,” in 14th International
Conference on Architectural Support for Programming Languages and
Operating Systems, Washington, DC, USA, Mar. 2009, pp. 49–60.
[24] W. T. Ng and P. M. Chen, “The systematic improvement of fault
tolerance in the Rio ﬁle cache,” in 29th Annual International Symposium
on Fault-Tolerant Computing, Madison, WI, Jun. 1999, pp. 76–83.
[25] R. Nikolaev and G. Back, “VirtuOS: an operating system with kernel
virtualization,” in 24th ACM Symposium on Operating Systems Princi-
ples, Farmington, PA, Nov. 2013, pp. 116–132.
[26] H. P. Reiser, F. J. Hauck, R. Kapitza, and W. Schroder-Preikschat,
“Hypervisor-based redundant execution on a single physical host,” in 6th
European Dependable Computing Conference, Supplemental Volume,
Coimbra, Portugal, Oct. 2006, pp. 67–68.
[27] M. Rosenblum and T. Garﬁnkel, “Virtual machine monitors: Current
technology and future trends,” IEEE Computer, vol. 38, no. 5, pp. 39–
47, May 2005.
[28] C. Tan, Y. Xia, H. Chen, and B. Zang, “TinyChecker: transparent pro-
tection of vms against hypervisor failures with nested virtualization,” in
2nd International Workshop on Dependability of Clouds, Data Centers
and Virtual Machine Technology, Boston, MA, Jun. 2012.
[29] VMware,
“Providing
fault
tolerance
for
virtual machines,”
https://pubs.vmware.com/vsphere-4-esx-vcenter/topic/com.vmware.
vsphere.availability.doc 41/c ft.html, accessed: 2017-12-01.
[30] K. Yamakita, H. Yamada, and K. Kono, “Phase-based reboot: Reusing
operating system execution phases for cheap reboot-based recovery,”
in 41st Annual IEEE/IFIP International Conference on Dependable
Systems and Networks, Hong Kong, China, Jun. 2011, pp. 169–180.
[31] T. Yoshimura, H. Yamada, and K. Kono, “Can Linux be rejuvenated
without reboots?” in IEEE Third International Workshop on Software
Aging and Rejuvenation, Hiroshima, Japan, Nov. 2011, pp. 50–55.
[32] ——, “Is Linux kernel Oops useful or not?” in Eighth USENIX Work-
shop on Hot Topics in System Dependability, Hollywood, CA, Oct. 2012,
pp. 1–6.
[33] W. Zhang and W. Zhang, “Linux virtual server clusters,” Linux Maga-
zine, vol. 5, no. 11, Nov. 2003.
126
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply.