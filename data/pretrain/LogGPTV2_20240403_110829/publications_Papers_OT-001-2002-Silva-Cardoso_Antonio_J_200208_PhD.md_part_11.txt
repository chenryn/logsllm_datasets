Done/Commit
Pre-Init Initial Executing
Failed/aborted
Synchronization Queuing Processing
Delay Delay Time
Task Response Time Task Reliability
Figure 3-2 – Revised task structure (extended from (Krishnakumar and Sheth 1995))
The synchronization delay time is calculated based on the dfiference between the
time registered when a task leaves the pre-init state and the time registered when it enters
the state. A task t remains in the pre-init state as long as its task scheduler is waiting for
another transition to be enabled in order to place the task into an initial state. This only
happens with synchronization tasks, i.e., and-join tasks (Kochut 1999), since they need to
wait until all their incoming transitions are enabled before continuing to the next state.
For all other types of input and output logic (xor-split, xor-join, and-split) the
synchronization delay time is set to zero.
As for the synchronization delay time, the queuing time is the difference between the
time a task leaves and enters the initial state. A task in the initial state indicates that the
task is in a queue waiting to be scheduled (by its task scheduler). ORBWork task
schedulers treat their queues with a FIFO policy. One interesting queuing policy variation
is associated with the scheduling of human-tasks. For a human-task instance, being in the
initial state means that the task has been placed in a worklist for human processing. A
user can select any human-task in a worklist, as long as the user role matches the task
role. In this case, the queuing policy is SIRO (Serve In Random Order). Depending on
103
the workflow system, other useful queuing policies can be used, such as priority queues.
When a task instance enters a queue a time-stamp is attached to it. When the task is
removed from the queue for scheduling, another time-stamp is attached to it so that the
total queuing time can be calculated later.
When a task is ready to be executed it transits to the executing state. As with the
previous calculations, the time a task remains in this state corresponds to the processing
time.
Managing Reliability
During a task realization, a number of undesirable events may occur. Depending on the
successful or unsuccessful execution of a task, it can be placed in the done or fail state
(for non-transactional tasks) and commit or abort (for transactional tasks). The former
state indicates that the task execution was unsuccessful, while the latter state indicates
that a task is executed successfully (Krishnakumar and Sheth 1995).
When an undesirable event occurs, an exception is generated. An exception is
viewed as an occurrence of some abnormal event that the underlying workfolw
management system can detect and react to. If an exception occurs during the invocation
of a task realization, its task enters the fail/abort state. In our implementation, it is the
responsibility of task schedulers to identify the final state of a task execution in order to
subsequently set the reliability dimension. Later this information is used to compute the
failure rate, which is the ratio between the number of times the failed/aborted state is
reached and the number of times state done/committed is reached. To describe task
reliability we follow a discrete-time modeling approach. Discrete-time models are
adequate for systems that respond to occasional demands such as database systems. We
use the stable reliability model proposed by Nelson( 1973), for which the reliability of a
task t is R(t) = 1 - failure rate.
104
3.5.1.2 TASK MANAGERS AND TASKS
When a task is ready to execute, a task scheduler activates an associated task manager.
The task manager oversees the execution of the task itself. Task managers are
implemented as an object and are classified as transactional or non-transactional,
depending on the task managed. Human tasks do not have an associated task manager.
Once activated, the task manager stays active until the task itself completes. Once the
task has completed or terminated prematurely with a fault, the task manager notifies its
task scheduler. The task manager is responsible for creating and initailizing a QoS data
structure from QoS specifications (for the cost and fidelity dimensions) for the task
overseen. When the supervised task starts its execution, the data structure is transferred to
it. If the task is a non-transactional one (typically performed by a computer program), a
set of methods is available to programmatically change the initial QoS estimates. No
methods are supplied to change the time and reliability dimensions since the task
schedulers are responsible for controlling these dimensoins. For transactional tasks (i.e., a
database operation), only the time and reliability dimensions are dynamically set at
runtime. The cost and fidelity dimensions, once initialized from the QoS specifications,
cannot be changed. This is because database systems do not make available information
evaluating the cost and the fidelity of the operations executed. Once the task completes
its execution, the QoS data structure is transferred back to the task manager, and later
from the task manager to the task scheduler. The only responsibility of the task scheduler
will be to incorporate the metrics registered for the time and reliability dimensions (see
section 3.5.1.1) into the QoS data structure and send it to the monitor to be processed (see
next section).
In the case of human tasks (performed directly by end-users), the QoS specifications
for the cost and fidelity dimensions are included in interface page(s) (as HTML
templates) presented to the end-user. When executing a human task, the user can directly
105
set the cost and fidelity dimensions to values reflecting how the task was carried out. As
mentioned previously, human-tasks do not have a task manager associated with them, and
therefore a specific task scheduler is responsible for the task supervision. When the task
completes its realization, the task scheduler parses the interface page(s) and retrieves the
new QoS metrics that the user may have modified.
3.5.1.3 MONITOR
When workflows are installed and instances are executed, the enactment system
generates information messages (events) describing the activities being carried out. The
monitor is an independent component represented by an object that records all of the
events for all of the workflows being processed by the enactment service .Depending on
the system setup parameters, the ORBWork monitor can display the events it receives to
the console or store them in a readable log file. To extend the functionality and usability
of the monitor two distinct APIs have been developed: the HTTPlog and the DBlog.
The first one uses the HTTP protocol to send status information from the ORBWork
monitor to remote clients. The information can be viewed remotely, using a monitor
client. This is particularly suitable for administrators that need to peirodically check the
status of running instances. The second API, the DBlog, has been developed to store the
status and QoS events generated in a relational database. When a workflow is installed
and executed, task QoS estimates, runtime QoS metrics, and transition frequencies are
stored in the database. The stored information will be later utilized to create a QoS profile
for the tasks and to enable the computation of the workflow QoS.
3.5.1.4 DBLOG
The DBlog is a suitable interface that the monitor uses to store workflow runtime data in
a database. The runtime data generated from workflow installations and instances
execution is propagated to the DBlog that will be in charge of storing the information into
106
a specified database. Figure 3-3 shows the database schema used to store workflow-
related data and tasks QoS metrics (designer and runtime metrics).
Figure 3-3 - Database Schema with QoS support
The data model includes metadata describing workflows and workflow versions,
tasks, instances, transitions, and runtime QoS metrics. In addition to storing runtime QoS,
we also store designer-defined QoS estimates. The data model captures the information
necessary to subsequently run suitable tools to analyze workflow QoS. One of the
primary goals of using a database system loosely coupled with the workflow system is to
enable different tools to be used to analyze QoS, such as project management and
statistical tools.
DBlog is populated when workflows are installed and instances executed. The
DBlog schema was designed to store three distinct categories of information, reflecting
workflow systems operations with QoS management. The first category corresponds to
data events generated when workflows are installed. During installation, information
describing workflow structure (which includes tasks and transitions) is stored. The
second category of information to be stored corresponds to the QoS estimates for tasks
107
and transitions that are specified at the workflow design phase. The third category
corresponds to the information which describes how instances are behaving at runtime.
This includes data indicating the tasks’ processing time, cost, and the enabling of
transitions. The monitoring of transitions is important to build functions which
probabilistically describe their enabled rate. The computation of workflow QoS metrics is
based on this stochastic structure.
Since the database stores real-time runtime information of tasks QoS metrics, we are
also investigating the implementation of mechanisms to automatically notify or alert
operators and supervisors when QoS metrics reach threshold values, so that corrective
actions can be taken immediately .
3.5.2 MANAGER
The manager is used to install and administer workflow definitions (schema), and to start
workflow instances. When a workflow is installed, the manager activates all of the
necessary task schedulers to carry out the execution of instances. The manager is
implemented as an object and has an interface that allows clients to interact with it. The
manager does not participate in any task scheduling activities. It is only necessary at the
time a new workflow is installed or modified. When a workflow is installed, trace
messages are sent to the monitor indicating the workflow installed and its associated
tasks. The information send to the monitor also includes the initial QoS estimates that the
user has set during the workflow design. When the monitor receives this information
(workflow topology, tasks, and QoS estimates), it uses the DBlog interface to store it in a
database for later QoS processing.
3.5.3 WORKFLOW BUILDER
The workflow builder tool is used to graphically design and specify a workflow. In most
cases, after a workflow design no extra work is necessary and it can be converted
108
automatically to an application by a code generator. The builder is used to specify
workflow topology, tasks, transitions (control flow and data flow), data objects, task
invocation, roles, and security domains (Kang, Park et al. 2001). During the design
phase, the designer is shielded from the underlying details of the runtime environment
and infrastructure, separating the workflow definition from the enactment service on
which it will be installed and executed .To support workflow QoS management the
designer must be able to set estimates for transition probabilities and QoS estimates for
tasks. This information is later combined with historical data, which plays a larger role as
more instances are executed, to create a runtime QoS model for tasks and a probability
model for transitions.
The workflow model and the task model have been extended to support the
specification of QoS metrics. To support these extensions, the builder has been enhanced
to allow designers to associate probabilities with transitions and to make possible the
specification of initial QoS metrics for tasks (see section 3.5.3.1). Previously, the
workflow model only included data flow mappings associated with transitions. The
association of probabilities with transitions transforms a workflow into a stochastic
workflow. The stochastic information indicates the probability of a transition being fired
at runtime. The QoS model specified for each task and transitions probabilities are
embedded into the workflow definition and stored in XML format.
3.5.3.1 SETTING INITIAL TASK QOS ESTIMATES
At design time, each task receives information which includes its type, input and output
parameters, input and output logic, realization, exceptions generated, etc. All this
information makes up the task model. The task model has been extended to accommodate
the QoS model. Task QoS is initialized at design time and re-computed at runtime when
tasks are executed. During the graphical construction of a workflow process, each task
109
receives information estimating its quality of service behavior at runtime. This includes
information about its cost, time (duration), reliability, and fidelity .
The task QoS estimates are composed of two classes of information: basic and
distributional. The basic class associates with each task QoS dimension the estimates of
the minimum, average, and maximum values that the dimension can take. For example,
for the cost dimension, it corresponds to the minimum, average, and maximum costs
associated with the execution of a task. The second class, the distributional class,
corresponds to the specification of a distribution function (such as Exponential, Normal,
Gamma, Weibull, and Uniform) which statistically describes tasks behavior at runtime.
For example, the time QoS dimension of a task can be describe by using the normal or
uniform distribution function. Figure 3-4 illustrates the graphical interface that is used to
specify the basic and distributional informationt o setup initial QoS metrics.
The values specified in the basic class are typically used by mathematical methods to
compute and predict workflow QoS metrics (see SWR algorithm in Appendix), while the
distributional class information is used by simulation ysstems to compute workflow QoS
(see section 3.6.2). To devise values for the two classes, the user typically applies QoS
models presented in Cardoso, Miller et al. (2002). We recognize that the specification of
cost, time, fidelity, and reliability is a complex operation, and when not carried out
properly can lead to the specification of incorrect values.
Once the design of a workflow is completed, it is compiled. The compilation
generates a set of specification files and realization files for each task. The specification
files (Spec files) include information describing the control and data flow of each task .
110
Figure 3-4 – Task QoS basic and distributional class
The realization files include the operations or instructions for a task to be executed at
runtime. For human tasks, HTML files are generated, since they are carried out using a
web browser. For non-transactional tasks, java code files are generated and compiled. At
runtime, the executables are executed automatically by the enactment system. Finally, for
non-transactional tasks a file containing the necessary data to connect to databases is
generated. To enable the enactment service to acquire and manipulate QoS information,
the builder has been extended to generate QoS specification files for each task. For
human tasks we have decided to embed the QoS metrics directly into the HTML forms
that are generated.
111
3.5.3.2 RE-COMPUTING QOS ESTIMATES
The initial QoS specifications may not be valid over time. To overcome this difficulty we
re-compute task QoS values for the basic class, based on previous executions. The same
applies for transitions. The distributional class also needs to have its distribution re-
computed. This involves the analysis of runtime QoS metrics to make sure that the QoS
distribution functions associated with a task remain valid or need to be modified
The re-computation of QoS estimates for tasks and for transition probabilities is done
based on runtime data generated from past workflow executions that have been stored in
the database log (section 3.5.1.4). We have developed a QoS Estimator module that lies
between the builder and the database log. The QoS Estimator creates a QoS model for
tasks based on the information stored in the DBlog. It also calculates transition
probability functions based on the transitions enabled at runtime.F igure 3-5 illustrates
the architecture of the QoS Estimator module. When a workflow is being designed, if the
tasks selected to compose the workflow have been previously executed, then their QoS
metrics are re-computed automatically using the QoS Estimator module.
QoS Model Construction
Transition Probability
Statistical Computation
Data Conversion
Data Selection
DB Connector
Database
Figure 3-5 – QoS Estimator Module
112
DB connector
The DB Connector is responsible for the establishment of a connection to the database.
Currently, we support relational databases that implement the JDBC protocol.
Data Selection
The data selection component allows for the selection of task QoS metrics, as defined by
the designer and tasks previously executed. Four distinct selection modes exist, and for
each one a specific selection function has been constructed. The functions are shown in
Table 3-1. The component can select tasks QoS metrics from information introduced by
the user at design time, from tasks executed in the context of any workflow, from tasks
executed in the context of a specific workflow w, and from tasks executed from a
particular instance i of workflow w.
Table 3-1 – Select functions of the Data Selection Component
Selection function Description
UD_Select(t) Selects the designer defined QoS metrics of task t
specified by the designer in the basic class .
RT_Select(t) Selects the runtime QoS metrics of all the executions
of task t.
RT_Select(t, w) Selects the runtime QoS metrics of all the executions
of task t in any instance of workflow w.
RT_Select(t, w, i) Selects the runtime QoS metrics of all the executions
of task t in instance i of workflow w.
113
Data Conversion
Once a subset of the tasks present in the database log is selected, the data describing their
QoS may need to be converted to a suitable format in order to be processed by the
Statistical Computation component. The data conversion component is responsible for
this conversion. For example, if the processing time of a task is stored using its start
execution date and end execution date, the data conversion component applies the
function f(t) = end_ execution_date(t) - start_execution_date(t) to compute the processing
time (PT). As another example, let us assume that the reliability of a task is stored in the
database using the keywords done, fail, commit, and abort (as in ORBWork). In this case,
the data conversion component converts the keywords done and commit to the value 1,
indicating the success of the task, and converts the keywords fail and abort to the value 0,
indicating the failure of the task. This abstraction allows the statistical component to be
independent from any particular choice of storing runtime information .
Statistical Computation
Once an appropriate set of tasks has been retrieved from the database and their QoS data
has been converted to a suitable format, it is transferred to the statistical computation
component to estimate QoS metrics. Currently, the module only computes the minimum,
average, and maximum for QoS dimensions, but additional statistical ufnctions can be
easily included, such as standard deviations, average deviation, and variance .
Four distinct functions have been developed to compute estimates for the tasks
selected in the previous step; these are shown in Table 3-2. Each function is to be used
when computing QoS dimensions and corresponds to four scenarios that can occur. The
first function is utilized to retrieve, for a specific task t and a particular dimension Dim,