compensate for the reduced reliability of the less-tested
memory.
Table 6 (left) shows the design parameters that we use for
our study. We estimate the fraction of DRAM cost compared
to server hardware cost based on [6]. We derive the cost
of ECC DRAM, non-ECC DRAM (NoECC), and parity-
based DRAM using “Added capacity” from Table 1. We
estimate the cost of less-tested DRAM based on the trends
shown in [8, 9] and examine a range of costs for less-
tested DRAM because we are not aware of any recently
documented costs from vendors. Crash recovery time is
based on our observations during testing and we assume that
data that is written in memory for regions protected by parity
and recovery (Par+R) is copied to a backup on disk every
ﬁve minutes. We assume 2000 errors per server per month
based on a prior study of DRAM errors in the ﬁeld [13]
and target single server availability of 99.90%. Since we do
not examine the occurrence of hard errors in our analysis
(only their ongoing eﬀects), we treat all memory errors as
soft errors for this analysis.
Table 6 (right) shows, for each of the ﬁve designs we
evaluate, how their memory regions are mapped to diﬀerent
hardware/software reliability techniques (columns 2–4), and
the resulting eﬀects on the metrics that we evaluate (columns
5–9). We break down cost savings into two components:
the cost savings of the memory system (column 5) and the
cost savings of the entire server hardware (column 6). Note
that we compute capital cost as it represents the dominant
cost component of the server.5 We also list the average
number of crashes per server per month (column 7) and the
associated fraction of single server availability (column 8).
While server availability also depends on other factors, such
5We also expect memory system energy savings to be proportional to
memory capacity savings. This is due to the eliminated dynamic energy of
moving and storing the ECC bits used for error detection or correction.
476
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
Figure 8: Number of memory errors tolerable per month to ensure
diﬀerent single server availability requirements are met for three
applications.
derived from the results in Figure 4. We make two ob-
servations from this ﬁgure. First, even at the error rate of
2000 errors per month that we assume, without any error
detection/correction,
two of the applications (WebSearch
and Memcached) are able to achieve 99.00% single server
availability. Second,
there exists an order of magnitude
diﬀerence in the amount of errors that can be tolerated per
month to achieve a particular availability across the applica-
tions. Based on these, we believe that there is signiﬁcant
opportunity in these and other applications for reducing
server hardware cost while achieving high single server
availability/reliability using our heterogeneous-reliability de-
sign methodology.
C. Required Hardware/Software Support
We next discuss the hardware and software changes
required to enable heterogeneous-reliability memory sys-
tems and the feasibility of foregoing memory error detec-
tion/correction in servers.
Hardware Support. In hardware, our techniques require
the ability for memory modules with diﬀerent hardware
reliability techniques to coexist in the same system (e.g.,
no detection/correction, parity detection, and SEC-DED).
We believe that this is achievable using existing memory
controllers at the granularity of memory channels without
much modiﬁcation. Figure 9 shows an example of how such
a technique could be employed on a processor that uses
separate memory controllers for each channel, each con-
trolling DIMMs that employ diﬀerent hardware reliability
techniques. Furthermore, techniques such as memory disag-
gregation [66] can be leveraged to provide heterogeneous-
reliability memory over remote direct memory access pro-
tocols to a variety of servers.
Software Support. In software, our techniques require the
ability to identify memory regions with distinct reliability
requirements and map them to locations on hardware devices
employing appropriate reliability techniques. Although we
examined OS-visible memory regions in this work (private,
heap, and stack), our technique can easily be extended to
other usage granularities, potentially by leveraging hints
from the programmer (as in [47, 67]). Alternatively, it is
foreseeable with our technique that infrastructure service
providers, such as Amazon EC2 and Windows Azure, could
provide diﬀerent reliability domains for users to conﬁgure
their virtual machines with depending on the amount of
availability they desire (e.g., 99.90% versus 99.00% avail-
ability). In addition, techniques like virtual machine migra-
tion [68] can be used to dynamically change the provided
memory reliability over time (e.g., in response to changes
in application utilization).
Figure 9: Minimal changes in today’s memory controller can achieve
heterogeneous memory provisioning at the channel granularity.
Feasibility. A primary concern associated with using mem-
ory without error detection/correction is the fear of the
eﬀects of memory errors propagating to persistent storage.
Based on our analysis, we believe that, in general, the use of
memory without error detection/correction (and the subse-
quent propagation of errors to persistent storage) is suitable
for applications with the following two characteristics: (1)
application data is mostly read-only in memory (i.e., errors
have a low likelihood of propagating to persistent storage),
and (2) the result from the application is transient in nature
(i.e., consumed immediately by a user and then discarded).
There are several large-scale data-intensive applications that
conform to these characteristics such as web search (which
we have analyzed), media streaming, online games, collab-
orative editing (e.g., Wikipedia), and social networking.
Applications that do not have these two characteristics
should be carefully inspected and possibly re-architected
before deciding to use memory without detection/correction
capabilities. As an example, applications could divide their
data into persistent and transient, and map persistent data
to more reliable memory. To avoid incorrect results from
remaining in memory indeﬁnitely, applications may period-
ically invalidate and recompute their working set of data.
Another concern with using less reliable memory devices
is their inability to detect (and thus retire [20–22]) memory
pages with permanent faults. To alleviate this issue, software
designed to detect memory errors, such as memtest [69], can
be run periodically on servers with memory devices without
error detection and pages can be retired as usual.
Finally, the use of less reliable memory may result in single
server unavailability, and hence, may only be applicable for
applications that possess inherent slack in their availability
requirements. Server models such as scale-out and stateless
models, which tolerate single server failures by diverting
work to other available servers, are well-suited to handle
occasional single server unavailability, and thus can even
more eﬃciently leverage heterogeneous-reliability memory
system designs.
VII. Conclusions
In this paper, we developed a new methodology to quantify
the tolerance of applications to memory errors. Using this
methodology, we performed a case study of three new
data-intensive workloads that showed, among other new
insights, that there exists a diverse spectrum of memory
error tolerance both within and among these applications.
We proposed several new hardware/software heterogeneous-
reliability memory system designs and evaluated them to
the one-size-ﬁts-all approach to reliability in
show that
modern servers is ineﬃcient
in terms of cost, and that
heterogeneous-reliability systems can achieve the beneﬁts of
both low cost and high single server availability/reliability.
We hope that our techniques can enable the use of lower-
cost memory devices to reduce the server hardware cost of
datacenters and that our analyses will spur future research
477
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
1E+01E+11E+21E+31E+499.99%99.90%99.00%Single Server AvailabilityWebSearchMemcachedGraphLabNumber of Memory Errors Tolerable per Month2,000CPUDIMMMemCtrl 0MemCtrl 1*MemCtrl 2*DIMMDIMMChannel 0ECCDIMMECCChannel 1*Channel 2** Memory controller/channel without ECC supportDIMMDIMMon heterogeneous-reliability memory systems. As DRAM
technology scales into small feature sizes and becomes less
reliable, we believe such system-level hardware/software
cooperative heterogeneous-reliability solutions will become
more important in the future [70].
As part of our future eﬀorts, we plan to extend our
characterization framework to cover a more diverse set
of memory failure modes (e.g., failures correlated across
DRAM banks, rows, and columns), and to explore lighter-
weight characterization methodologies to make characteriz-
ing application memory error tolerance cheaper for a wider
set of applications. We also intend to implement and further
evaluate the heterogeneous hardware detection and software
recovery designs we propose in this paper.
VIII. Acknowledgments
We thank the anonymous reviewers and the members of
SAFARI research group for feedback. We acknowledge
the support of Microsoft and Samsung. This research was
partially supported by grants from Intel Science and Tech-
nology Center for Cloud Computing, NSF Awards 0953246,
1065112, 1212962. This research was started as an intern-
ship project at Microsoft Research.
References
[1] L. A. Barroso et al., The Datacenter as a Computer: An Introduction
to the Design of Warehouse-Scale Machines. Morgan & Claypool
Publishers, 2009.
[2] E. M. Elnozahy et al., “Energy-Eﬃcient Server Clusters,” in PACS,
2003.
2012.
[3] P. Jobin, “Cloud Computing Shifting to Cooler Climates,” 2012,
http://tinyurl.com/mfrlrtl.
[4] S. Grundberg et al., “For Data Center, Google Goes for the Cold,”
2011, http://tinyurl.com/ml55nh5.
[5] A. C. Riekstin et al., “No More Electrical Infrastructure: Towards
Fuel Cell Powered Data Centers,” in HotPower, 2013.
[6] C. Kozyrakis et al., “Server Engineering Insights for Large-Scale
Online Services,” IEEE Micro, 2010.
[7] R. Nishtala et al., “Scaling Memcache at Facebook,” in NSDI, 2013.
[8] Z. Al-Ars, “DRAM Fault Analysis and Test Generation,” Ph.D.
dissertation, Delft, 2005.
[9] “Memory Test Background,” 2000, http://tinyurl.com/m7c3wf7.
[10] T. J. Dell, “A White Paper on the Beneﬁts of Chipkill-Correct ECC
for PC Server Main Memory,” IBM Microelectronics Division, 1997.
[11] P. J. Meaney et al., “IBM zEnterprise Redundant Array of Independent
Memory Subsystem,” IBM JRD, 2012.
[12] D. Henderson et al., “POWER7 System RAS,” 2012.
[13] B. Schroeder et al., “DRAM Errors in the Wild: A Large-Scale Field
Study,” in SIGMETRICS Performance, 2009.
[14] V. Sridharan et al., “A Study of DRAM Failures in the Field,” in SC,
[15] A. A. Hwang et al., “Cosmic Rays Don’t Strike Twice: Understanding
the Nature of DRAM Errors and the Implications for System Design,”
in ASPLOS, 2012.
[16] X. Li et al., “A Realistic Evaluation of Memory Hardware Errors and
Software System Susceptibility,” in USENIX ATC, 2010.
[17] J. Stuecheli et al., “Elastic refresh: Techniques to mitigate refresh
penalties in high density memory,” in MICRO, 2010.
[18] JEDEC Solid State Technology Association, “JEDEC Standard:
DDR3 SDRAM, JESD79-3C,” 2008.
[19] D. Fiala et al., “Detection and Correction of Silent Data Corruption
for Large-scale High-performance Computing,” in SC, 2012.
[20] “Predictive Failure Analysis (PFA),” http://tinyurl.com/n34z657.
[21] “Mcelog: Memory Error Handling in User Space,” http://ww
w.halobates.de/lk10-mcelog.pdf.
[22] D. Tang et al., “Assessment of the Eﬀect of Memory Page Retirement
on System RAS Against Hardware Faults,” in DSN, 2006.
[23] Y. Kim et al., “A Case for Exploiting Subarray-Level Parallelism
(SALP) in DRAM,” in ISCA, 2012.
[24] D. Lee et al., “Tiered-Latency DRAM: A Low Latency and Low Cost
DRAM Architecture,” in HPCA, 2013.
[25] V. Seshadri et al., “RowClone: Fast and Eﬃcient In-DRAM Copy and
Initialization of Bulk Data,” in MICRO, 2013.
[26] S. Khan et al., “The Eﬃcacy of Error Mitigation Techniques for
DRAM Retention Failures: A Comparative Experimental Study,” in
SIGMETRICS, 2014.
[27] Y. Kim et al., “Flipping Bits in Memory Without Accessing Them: An
Experimental Study of DRAM Disturbance Errors,” in ISCA, 2014.
[28] T. C. May et al., “Alpha-Particle-Induced Soft Errors in Dynamic
Memories,” IEEE T-ED, 1979.
[29] V. Sridharan et al., “Feng Shui of Supercomputer Memory: Positional
Eﬀects in DRAM and SRAM Faults,” in SC, 2013.
[30] T. Siddiqua et al., “Analysis and Modeling of Memory Errors from
Large-scale Field Data Collection,” in SELSE, 2013.
[31] A. Messer et al., “Susceptibility of Commodity Systems and Software
to Memory Soft Errors,” IEEE TC, 2004.
[32] D. Li et al., “Classifying Soft Error Vulnerabilities in Extreme-Scale
Scientiﬁc Applications Using a Binary Instrumentation Tool,” in SC,
2012.
[33] X. Li et al., “Application-Level Correctness and Its Impact on Fault
Tolerance,” in HPCA, 2007.
[34] H. Esmaeilzadeh et al., “Architecture Support for Disciplined Ap-
proximate Programming,” in ASPLOS, 2012.
[35] J. Bornholt et al., “Uncertain: A First-order Type for Uncertain
Data,” in ASPLOS, 2014.
[36] “Intel iACT,” http://www.github.com/IntelLabs/iACT.
[37] D. H. Yoon et al., “Virtualized and Flexible ECC for Main Memory,”
in ASPLOS, 2010.
[38] H. Schirmeier et al., “RAMpage: Graceful Degradation Management
for Memory Errors in Commodity Linux Servers,” in PRDC, 2011.
[39] C. Borchert et al., “Generative Software-Based Memory Error Detec-
tion and Correction for Operating System Data Structures,” in DSN,
2013.
[40] K. Pattabiraman et al., “Samurai: Protecting Critical Data in Unsafe
Languages,” in EuroSys, 2008.
[41] J. Chang et al., “Automatic Instruction-Level Software-Only Recov-
ery,” in DSN, 2006.
[42] A. Benso et al., “A C/C++ Source-to-Source Compiler for Depend-
able Applications,” in DSN, 2000.
[43] L. Leem et al., “ERSA: Error Resilient System Architecture for
Probabilistic Applications,” in DATE, 2010.
[44] M.-L. Li et al., “Trace-Based Microarchitecture-level Diagnosis of
Permanent Hardware Faults,” in DSN, 2008.
[45] X. Xu et al., “Understanding Soft Error Propagation Using Eﬃcient
Vulnerability-Driven Fault Injection,” in DSN, 2012.
[46] M.-L. Li et al., “Understanding the Propagation of Hard Errors to
Software and Implications for Resilient System Design,” in ASPLOS,
2008.
[47] S. Liu et al., “Flikker: Saving DRAM Refresh-Power Through Critical
Data Partitioning,” in ASPLOS, 2011.
[48] D. H. Yoon et al., “BOOM: Enabling Mobile Memory Based Low-
power Server DIMMs,” in ISCA, 2012.
[49] K. T. Malladi et al., “Towards Energy-proportional Datacenter Mem-
ory with Mobile DRAM,” in ISCA, 2012.
[50] M. K. Qureshi et al., “Scalable High Performance Main Memory
System Using Phase-Change Memory Technology,” in ISCA, 2009.
[51] H. Yoon et al., “Row Buﬀer Locality Aware Caching Policies for
Hybrid Memories,” in ICCD, 2012.
[52] S. Phadke et al., “MLP Aware Heterogeneous Memory System,” in
[53] N. Chatterjee et al., “Leveraging Heterogeneity in DRAM Main
Memories to Accelerate Critical Word Access,” in MICRO, 2012.
[54] J. Meza et al., “Enabling eﬃcient and scalable hybrid memories
using ﬁne-granularity dram cache management,” IEEE Computer
Architecture Letters, 2012.
[55] “Windows Debugging,” http://tinyurl.com/l6zsqzv.
[56] “GDB: The GNU Project Debugger,” http://www.sourceware.org/gdb/.
[57] J. Liu et al., “An Experimental Study of Data Retention Behavior in
Modern DRAM Devices: Implications for Retention Time Proﬁling
Mechanisms,” in ISCA, 2013.
[58] V. J. Reddi et al., “Web Search Using Mobile Cores: Quantifying and
Mitigating the Price of Eﬃciency,” in ISCA, 2010.
[59] “Memcached,” http://memcached.org/.
[60] Y. Low et al., “Distributed GraphLab: A Framework for Machine
Learning and Data Mining in the Cloud,” PVLDB, 2012.
[61] D. Tunkelang,
“A Twitter Analog
to
PageRank,”
2009,
http://tinyurl.com/9byt4z.
[62] S. K. S. Hari et al., “Relyzer: Exploiting Application-Level Fault
Equivalence to Analyze Application Resiliency to Transient Faults,”
in ASPLOS, 2012.
[63] N. J. Wang et al., “Characterizing the Eﬀects of Transient Faults on
a High-Performance Processor Pipeline,” in DSN, 2004.
[64] E. B. Nightingale et al., “Cycles, Cells and Platters: An Empirical
Analysis of Hardware Failures on a Million Consumer PCs,” in
EuroSys, 2011.
[65] M. C. Rinard et al., “Enhancing Server Availability and Security
Through Failure-Oblivious Computing,” in OSDI, 2004.
[66] K. Lim et al., “Disaggregated Memory for Expansion and Sharing in
Blade Servers,” in ISCA, 2009.
[67] A. Sampson et al., “EnerJ: Approximate Data Types for Safe and
General Low-Power Computation,” in PLDI, 2011.
[68] Y. Du et al., “A Rising Tide Lifts All Boats: How Memory Error Pre-
diction and Prevention Can Help with Virtualized System Longevity,”
in HotDep, 2010.
[69] “Memtest86+,” http://www.memtest.org/.
[70] O. Mutlu, “Memory Scaling: A Systems Architecture Perspective,” in
MEMCON, 2013.
DATE, 2011.
478
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply.