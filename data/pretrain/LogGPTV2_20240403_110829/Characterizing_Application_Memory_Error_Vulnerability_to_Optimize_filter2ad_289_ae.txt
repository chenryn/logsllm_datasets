### Compensating for Reduced Reliability in Less-Tested Memory

#### Design Parameters and Cost Estimation
Table 6 (left) outlines the design parameters used in our study. We estimate the fraction of DRAM cost relative to the total server hardware cost based on [6]. The costs of ECC DRAM, non-ECC DRAM (NoECC), and parity-based DRAM are derived from the "Added capacity" data in Table 1. For less-tested DRAM, we estimate the cost based on trends observed in [8, 9], and we examine a range of costs due to the lack of recent vendor documentation. 

The crash recovery time is based on our testing observations. We assume that data written to memory regions protected by parity and recovery (Par+R) is copied to a backup on disk every five minutes. We also assume an error rate of 2000 errors per server per month, as reported in a prior study [13], and target a single server availability of 99.90%. Since our analysis does not consider hard errors, all memory errors are treated as soft errors.

#### Evaluation of Different Designs
Table 6 (right) presents the five designs we evaluate, detailing how their memory regions are mapped to different hardware and software reliability techniques (columns 2–4) and the resulting effects on the metrics we assess (columns 5–9). Cost savings are broken down into two components: the cost savings of the memory system (column 5) and the cost savings of the entire server hardware (column 6). Capital cost, which represents the dominant cost component of the server, is computed accordingly. Additionally, we list the average number of crashes per server per month (column 7) and the associated single server availability (column 8).

While server availability depends on various factors, we expect memory system energy savings to be proportional to memory capacity savings. This is due to the elimination of dynamic energy associated with moving and storing ECC bits used for error detection or correction.

#### Tolerable Memory Errors and Application Availability
Figure 8 illustrates the number of memory errors tolerable per month to meet different single server availability requirements for three applications, derived from the results in Figure 4. Two key observations are made:
1. Even at an error rate of 2000 errors per month, two applications (WebSearch and Memcached) can achieve 99.00% single server availability without any error detection/correction.
2. There is a significant difference in the number of errors that can be tolerated per month to achieve a specific availability across the applications. This suggests substantial opportunities for reducing server hardware costs while maintaining high single server availability/reliability using our heterogeneous-reliability design methodology.

#### Required Hardware and Software Support
We discuss the necessary hardware and software changes to enable heterogeneous-reliability memory systems and the feasibility of foregoing memory error detection/correction in servers.

**Hardware Support:**
Our techniques require the coexistence of memory modules with different hardware reliability techniques in the same system (e.g., no detection/correction, parity detection, and SEC-DED). This can be achieved using existing memory controllers at the granularity of memory channels with minimal modifications. Figure 9 provides an example of such a setup, where separate memory controllers for each channel control DIMMs with different reliability techniques. Techniques like memory disaggregation [66] can further support heterogeneous-reliability memory over remote direct memory access protocols.

**Software Support:**
In software, our techniques require identifying memory regions with distinct reliability requirements and mapping them to appropriate hardware devices. Although this work focuses on OS-visible memory regions (private, heap, and stack), the technique can be extended to other granularities, potentially using programmer hints [47, 67]. Infrastructure service providers, such as Amazon EC2 and Windows Azure, could offer different reliability domains for users to configure their virtual machines based on desired availability levels (e.g., 99.90% vs. 99.00%). Virtual machine migration [68] can dynamically adjust memory reliability in response to changes in application utilization.

**Feasibility:**
Using memory without error detection/correction raises concerns about the propagation of errors to persistent storage. Our analysis suggests that this approach is suitable for applications with the following characteristics:
1. Application data is mostly read-only in memory, reducing the likelihood of errors propagating to persistent storage.
2. The application's results are transient and consumed immediately by users before being discarded.

Examples of such applications include web search, media streaming, online games, collaborative editing (e.g., Wikipedia), and social networking. Applications lacking these characteristics should be carefully inspected and possibly re-architected. They may divide their data into persistent and transient, mapping persistent data to more reliable memory and periodically invalidating and recomputing working sets to avoid incorrect results.

Another concern is the inability of less reliable memory to detect and retire pages with permanent faults. Periodic use of software like memtest [69] can help identify and retire faulty pages. Finally, the use of less reliable memory may result in occasional single server unavailability, making it more suitable for scale-out and stateless models that can divert work to other available servers.

### Conclusions
In this paper, we developed a new methodology to quantify the tolerance of applications to memory errors. Using this methodology, we performed a case study on three data-intensive workloads, revealing a diverse spectrum of memory error tolerance within and among these applications. We proposed and evaluated several new hardware/software heterogeneous-reliability memory system designs, demonstrating that the one-size-fits-all approach to reliability in modern servers is inefficient in terms of cost. Heterogeneous-reliability systems can achieve both low cost and high single server availability/reliability. We hope our techniques will enable the use of lower-cost memory devices to reduce server hardware costs in datacenters and spur future research on heterogeneous-reliability memory systems. As DRAM technology scales to smaller feature sizes and becomes less reliable, such system-level hardware/software cooperative solutions will become increasingly important [70].

### Future Work
We plan to extend our characterization framework to cover a more diverse set of memory failure modes and explore lighter-weight characterization methodologies. We also intend to implement and further evaluate the proposed heterogeneous hardware detection and software recovery designs.

### Acknowledgments
We thank the anonymous reviewers and the members of the SAFARI research group for their feedback. We acknowledge the support of Microsoft and Samsung. This research was partially supported by grants from the Intel Science and Technology Center for Cloud Computing, NSF Awards 0953246, 1065112, 1212962, and started as an internship project at Microsoft Research.

### References
[References listed here as provided in the original text.]

---

This revised version aims to improve clarity, coherence, and professionalism, ensuring that the content is well-structured and easily understandable.