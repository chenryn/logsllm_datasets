factor b, and explore those mechanisms for a similar range for b. We
note that, just as standard TCP is equivalent to TCP(1/2), standard
RAP is equivalent to RAP(1/2). While RAP is TCP-equivalent, this
is not true for RAP(b) for values of b other than 1/2.
All of our experiments use a single-bottleneck “dumbbell” topol-
ogy with RED queue management at the bottleneck. Unless other-
e
t
a
r
p
o
r
D
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
130
TCP(1/2)
TCP(1/256)
SQRT(1/256)
TFRC(256, no self clocking)
TFRC(256, self clocked)
140
150
160
170
180
190
200
210
220
Time (in seconds)
Figure 3: The drop rate for several SlowCC algorithms when a
CBR source restarts at  = 180s after a 30s idle period.
)
.
c
e
s
n
i
(
i
e
m
T
n
o
i
t
a
z
i
l
i
b
a
S
t
25
20
15
10
5
0
0
Stabilization times with RED gateways
TCP
SQRT
RAP
TFRC(no self clocking)
TFRC(self clocked)
50
100
150
200
250
300
Algorithm Parameter
Figure 4: The stabilization time (in seconds, 1 RTT = 50ms) for
the various SlowCC algorithms as a function of the algorithm’s
parameter, (cid:13).
wise mentioned, the queue size is set to 2.5 times the bandwidth-
delay product, and the i heh and ax heh parameters
are set to 0.25 and 1.25 times the bandwidth-delay product, respec-
tively. The round-trip time (RTT) for the connections is approxi-
mately 50ms. Each simulation scenario includes data trafﬁc ﬂow-
ing in both directions on the congested link.
4. Results
In this section, we discuss the results of our experiments. We
start by investigating the potential danger of slowly-responsive
TCP-compatible algorithms in terms of increased packet loss rates,
and discuss ways of reducing this danger. We then discuss two ad-
ditional potential drawbacks of these algorithms: unfairness with
respect to TCP and potentially sub-optimal bottleneck utilization.
Finally, we discuss the beneﬁts of these algorithms in terms of
smooth transmission rates under dynamic network conditions.
4.1 “The Ugly”: Potential Dangers of Slowly-
Responsive Algorithms
By deﬁnition, SlowCC mechanisms respond slowly to reductions
in the available bandwidth. As a consequence, despite being TCP-
compatible under static conditions, a SlowCC mechanism could in
fact cause high packet loss rates for extended periods of time. This
is a grave concern, because persistently high drop rates result in an
unnecessary decrease in throughput and an unnecessary increase in
response times for the ﬂows traversing the link.
4.1.1 A Competing CBR Source
Our ﬁrst experiment investigates the performance of different
TCP-compatible SlowCC algorithms when confronted with an
abrupt reduction in the available bandwidth. We use twenty long-
t
s
o
C
n
o
i
t
a
z
i
l
i
b
a
S
t
256
64
16
4
1
0.25
0
Stabilization costs with RED gateways
TCP
SQRT
RAP
TFRC(no self clocking)
TFRC(self clocked)
50
100
150
200
250
300
Algorithm Parameter
Figure 5: The stabilization cost for the various SlowCC algo-
rithms as a function of the algorithm’s parameter, (cid:13).
lived SlowCC ﬂows, with changes in bandwidth being orches-
trated by an ON/OFF CBR source that starts at  = 0s, stops at
 = 150s, and restarts at  = 180s. When it is on, the CBR
source uses one-half of the bandwidth of the bottleneck link. The
bottleneck uses RED queue management and trafﬁc sources as de-
scribed in Section 3. During the  = 0; 150s interval, we mea-
sure the average packet loss rate in the queue. Because the queue
uses FIFO scheduling with RED queue management, all connec-
tions see similar loss rates. When the CBR source is idle between
 = 150; 180s, the packet drop rate is negligible. When the CBR
source starts again at  = 180s, the network has a transient spike
with a packet drop rate of roughly 40% for at least one round-trip
time, until end-to-end congestion control can begin to take effect.
The network then gradually returns to the same steady-state drop
rate as during the  = 0; 150s interval. Figure 3 shows the drop
rate from several simulations using SlowCC mechanisms with very
slow response times.
For each SlowCC algorithm, we deﬁne the stabilization time as
the number of RTTs, after a period of high congestion begins, until
the network loss rate diminishes to within 1.5 times its steady-state
value for this level of congestion. In these simulations the period
of high congestion begins at time 180, and the steady-state drop
rate for that level of congestion is given by the drop rate over the
ﬁrst 150 seconds. Clearly the stabilization time will be different
from one scenario to another; the purpose of the metric is to com-
pare the stabilization times for different transport protocols in the
same trafﬁc scenario. We calculate the loss rate as an average over
the previous ten RTT periods. Longer stabilization times indicate
congestion control mechanisms with longer periods of congestion
following a sudden decrease in the available bandwidth.
Figure 4 shows the stabilization time for the different SlowCC
mechanisms. For each congestion control mechanism, the x-axis
shows the parameter (cid:13), corresponding to TCP(1=(cid:13)), RAP(1=(cid:13)),
SQRT(1=(cid:13)), and TFRC((cid:13)). For example, the parameter (cid:13) = 256
corresponds to TCP(1/256) and TFRC(256) respectively. Note that
TCP(1=(cid:13)), RAP(1=(cid:13)), SQRT(1=(cid:13)), and TFRC((cid:13)) are not neces-
sarily an equivalent comparison for a speciﬁc value of (cid:13). Figure
4 shows that there are extreme cases, notably TFRC(256) without
self-clocking, where the stabilization time is hundreds of RTTs.
TFRC without self-clocking is the default version of TFRC in ns-2.
While the stabilization time measures the amount of time it takes
for the loss rate to return to near the previous value, the stabiliza-
tion cost incorporates not only the time for stabilization, but also
the average value of the loss rate during this stabilization period.
More formally, we deﬁne the stabilization cost to be the product of
the stabilization time and the average loss rate (in percentage) dur-
ing the stabilization interval. The stabilization cost quantiﬁes the
true effects of persistent overload; a congestion control mechanism
with a stabilization cost of 1 corresponds to an entire round-trip
time worth of packets dropped at the congested link during the sta-
bilization period, whether this is from a 100% packet drop rate for
one round-trip time, a 50% drop rate for two round-trip times, or
something else.
Figure 5 shows the stabilization cost for different SlowCC mech-
anisms, showing that, for large values of (cid:13), some of them are
two orders of magnitude worse than the most slowly-responsive
TCP(1=(cid:13)) or SQRT(1=(cid:13)) algorithms we investigated. Note that
the vertical axis is on a log-scale. Figure 5 also shows that the sta-
bilization cost is acceptably low for SlowCC mechanisms with the
range of parameters that have actually been proposed for use in the
Internet.
Does Figure 5 indicate that SlowCC mechanisms with large val-
ues of (cid:13), corresponding to very slow response (and stabilization)
times, can cause persistent high packet loss rates and are therefore
not safe for deployment in the Internet? It turns out that there is in
fact a way to improve the stabilization cost of the RAP(1=(cid:13)) and
TFRC((cid:13)) mechanisms with large values for (cid:13).
To understand the difference between TFRC and RAP on the
one hand, and TCP and SQRT on the other, it is worth asking what
mechanisms are present in one class and not in the other. RAP(1=(cid:13))
and TCP(1=(cid:13)) are the closest of these algorithms in terms of the
increase/decrease rules, with the main difference between them be-
ing the use of a rate variable rather than a window in RAP. The
window-based TCP(1=(cid:13)), unlike RAP(1=(cid:13)), religiously follows
the principle of packet conservation, being self-clocked by the ar-
rival of acknowledgments from the sender. In contrast, RAP(1=(cid:13))
and TFRC((cid:13)) are rate-based; they transmit data based on the rate
determined by the increase/decrease algorithm, irrespective of the
number of acknowledgements received. Although they do use ac-
knowledgements to update their sending rate, data transmissions
themselves are not directly triggered by acknowledgments but in-
stead are sent out based on the determined rate. The consequence of
self-clocking is that TCP(1=(cid:13)) and SQRT(1=(cid:13)) reduce their trans-
mission rates drastically when the available bandwidth drastically
decreases, since acknowledgments start arriving only at the rate
currently available to the ﬂow at the bottleneck and the sending
rate is therefore limited to the bottleneck (acknowledgment) rate
from the previous RTT.
To evaluate whether self-clocking is in fact the key differentiator
for the behavior of the very slow variants of these SlowCC mecha-
nisms in this scenario, we added stronger self-clocking to the TFRC
algorithm. TFRC already limits the sender’s sending rate to at most
twice the rate at which data is received by the receiver in the pre-
vious round trip [6]; this is critical to prevent severe over-shooting,
and emulates TCP’s slow-start phase. To incorporate stronger self-
clocking in TFRC, we introduced a conservative option to
TFRC in ns-2 that, for the round-trip time following a packet loss,
limits the sender’s rate to at most the rate at which data is received
by the receiver in the previous round trip (i.e., the RTT containing
the loss). We call this TFRC with self-clocking.
In addition, for TFRC with self-clocking we need to limit the
amount by which the sending rate can exceed the receive rate even
in the absence of loss. Once the sending rate is reduced due to
self-clocking, the absence of losses may cause TFRC to drastically
increase its allowed sending rate (because of the memory of good
times), once again violating self-clocking. Therefore, when not in
slow-start, the conservative option pegs TFRC’s maximum
sending rate to at most a constant C times the earlier receive rate.2
2We have experimented with various values of C and used C =
Response of SlowCCs to flash crowd of 1000 short flows
TCP(1/2)
Flash crowd
the ﬂash crowd is clear. Because the ﬂash crowd consists of many
short ﬂows in slow-start, the ﬂash crowd grabs bandwidth quite
rapidly regardless of whether the background trafﬁc is TCP(1/2) or
TFRC(256) (with self clocking).
)
s
n
b
i
c
e
s
1
(
)
s
p
B
K
(
t
u
p
h
g
u
o
r
h
T
)
s
n
b
i
c
e
s
1
(
)
s
p
B
K
(
t
u
p
h
g
u
o
r
h
T
)
s
n
b
i
c
e
s
1
(
)
s
p
B
K
(
t
u
p
h
g
u
o
r
h
T
2000
1500
1000
500
0
0
2000
1500
1000
500
0
0
2000
1500
1000
500
0
0
5
10
15
25
20
30
Time (seconds)
35
40
45
50
Response of SlowCCs to flash crowd of 1000 short flows
TFRC(256, no self clocking)
Flash crowd
5
10
15
25
20
30
Time (seconds)
35
40
45
50
Response of SlowCCs to flash crowd of 1000 short flows
TFRC(256)
Flash crowd
5
10
15
25
20
30
Time (seconds)
35
40
45
50
Our conclusion is that it is possible for certain rate-based TCP-
compatible algorithms to cause periods of persistently high loss
rates under dynamic conditions. However, systematically applying
the principle of packet conservation (e.g., by self-clocking trans-
missions) overcomes this problem even for the variants of these
algorithms conﬁgured with very slow response times. Thus, while
the possibility of periods of high packet loss rates is a signiﬁcant