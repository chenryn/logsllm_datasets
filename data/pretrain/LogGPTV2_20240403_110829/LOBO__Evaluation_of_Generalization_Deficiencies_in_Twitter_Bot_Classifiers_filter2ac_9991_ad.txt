### Methodology and Experimental Setup

In the second step of our experimental procedure, we randomly select X Bursty bots from the test dataset and transfer them to the training dataset. This process is repeated iteratively until the 9th step, at which point both the test and training datasets contain approximately 250 Bursty bots each. The step sizes are fixed at 0, 2, 4, 8, 16, 32, 64, 128, and 256, following a 2x scaling pattern, with the first step adjusted to zero bots to align with LOBO test II. It is important to note that, given this is a single C500 dataset, differences in accuracy between the 0 bot case and LOBO test II are expected. Additionally, we use the full 500 instances of each class, except for the target class, rather than limiting it to 70%.

### Results and Analysis

**Figure 3: Mean (Blue) and Error Range (Blue Shade 95% Confidence) for Classifier Accuracy on Target Classes According to the Number of Samples Seen from the Target Class**

**Figure 4: Error Range for Mean (Blue Shade) and Classifier Accuracy Against Number of Samples Seen (for Specific Classes)**

The above process is repeated 50 times, and the average prediction accuracy at each step for the target class is calculated. Repetition is necessary because different bots from the target class are moved into the training set each time, which can affect the overall accuracy differently. Finally, we run the learning rate test for each bot class listed in Table 6 as the target class. Detailed results are presented in Table 7.

**Table 7: Classifier Accuracy (%) - Trained C500 Excluding All but X Samples of Target Class**

| Tgt. Class | 0 | 2 | 4 | 8 | 16 | 32 | 64 | 128 | 256 |
|------------|---|---|---|---|----|----|----|-----|-----|
| A          | 89.4 | 71.7 | 96.4 | 98.8 | 99.9 | 100 | 100 | 100 | 100 |
| B          | 97.7 | 97.4 | 97.4 | 98.1 | 98.6 | 98.6 | 99.6 | 100 | 100 |
| C          | 86.9 | 86.5 | 87.3 | 87.5 | 88.0 | 87.8 | 90.9 | 90.9 | 90.9 |
| D          | 86.7 | 86.8 | 86.8 | 87.0 | 87.6 | 87.6 | 88.6 | 90.9 | 90.9 |
| E          | 72.2 | 68.6 | 79.8 | 84.9 | 89.4 | 92.8 | 92.8 | 94.7 | 94.7 |
| F          | 21.3 | 1.7 | 72.0 | 86.8 | 94.9 | 96.9 | 97.6 | 97.6 | 97.6 |
| H          | 4.3 | 2.3 | 10.5 | 25.7 | 49.4 | 66.9 | 76.3 | 76.3 | 76.3 |
| K          | 51.9 | 17.8 | 85.3 | 95.2 | 96.6 | 98.5 | 99.2 | 99.2 | 99.2 |
| M          | 66.3 | 64.8 | 67.3 | 70.1 | 74.2 | 80.8 | 87.5 | 87.5 | 87.5 |
| T          | 89.6 | 89.9 | 89.6 | 89.5 | 89.7 | 90.1 | 90.2 | 90.2 | 90.2 |
| U          | 34.6 | 34.0 | 36.9 | 38.5 | 43.2 | 50.0 | 59.1 | 59.1 | 59.1 |
| V          | 79.7 | 79.7 | 79.9 | 80.0 | 80.7 | 81.0 | 82.2 | 82.2 | 82.2 |

Figure 3 illustrates the average accuracy after X samples for all tested bot classes, with the shaded area representing the 95% confidence interval. The overall trend suggests that the classifier has learned to identify most target classes of bots after a few examples. In contrast, Figure 4 highlights the significant variations in performance across different classes, with the same shaded area as Figure 3 to emphasize the differences between the average and the widely varying performance of each target class.

Notably, some bot classes, such as class V (caught by honeypots), show no significant improvement regardless of the number of instances shown to the classifier. On the other hand, some classes, like bot class A, show dramatic improvements in accuracy from 50% to 99% after only 16 instances, and bot class F, from 0% to 95% after 32 instances. This indicates that the classifier can learn to identify the entire class with just a small fraction of the data.

### T-SNE Plot Analysis

To further understand why some bot classes are easier to predict than others, we created a t-distributed stochastic neighbor embedding (t-SNE) plot [38] using a dataset with a class size of 30k. This dimensionality reduction algorithm helps visualize high-dimensional datasets in two dimensions. 

**Figure 5a: T-SNE Plot of Star Wars Bots and Bursty Bots**
- Emphasizes the clear clusters formed by different bot classes, with each class plotted in a different color and real users in black.
- Star Wars bots and Bursty bots show distinct groups that are rarely mixed with real users.

**Figure 5b: T-SNE Plot of HoneyPot Bots**
- Shows that HoneyPot bots (dataset B) often share the same "strands" with real users.
- These bots are difficult to classify, as indicated by the LOBO test, and they closely resemble the user dataset.

### Discussion

#### 8.1 Accuracy and Generalization
The average accuracy on target classes was very similar in LOBO tests I and II. Even after accounting for the potential impact of chance in LOBO test I (since it was not repeated 100 times), it is interesting to see that both tests have almost the exact same accuracy on target bot classes.

#### 8.2 Improvements with Small Data Additions
A notable finding is that adding even a small amount of data from a bot class can significantly improve performance. For example, in LOBO test II, adding 500 bots to a simple classifier allows us to detect the full botnet (357,000 instances) at over 99% accuracy, up from 62%. The learning speed test also shows that adding just 16 samples can achieve 99% accuracy for Star Wars bots.

#### 8.3 Scalability
While re-training a classifier multiple times can be computationally expensive, our empirical results show that using a few examples on a balanced dataset yields similar and stable results. Reducing the size of each bot class from several thousands to 500 significantly decreases the required resources. This specific implementation uses a large amount of storage capacity due to the analysis of all tweets for each user, resulting in several terabytes of data. However, most of the methodology can be implemented in parallel, including collecting, parsing, sampling, training, and testing.

### Conclusion

In this paper, we investigated the resilience of bot detection systems on Twitter. We demonstrated that these systems perform well when trained on homogeneous data but struggle when tested on unseen bot classes. We proposed a methodology to evaluate the generalization capabilities of classifiers on new and unseen bot data. This finding has important implications for the field, as it highlights the need for better generalization in the fast-paced and adversarial world of social network abuse.

### Acknowledgments

This project has received funding from the European Union’s Horizon 2020 Research and Innovation program under the Marie Skłodowska-Curie ENCASE project (Grant Agreement No. 691025).

### References

[References listed here, formatted consistently]

This revised version aims to provide a more structured, coherent, and professional presentation of the content.