the existing Zapier trigger-action platform through its
developer platform. We created custom channels that
connect to prototype online services with DTAP support.
We ﬁnd that the existing cloud portion of the Zapier
successfully transfers the trigger blobs from the trigger
channel to the action channel and eventually to the action
online service where the blob is subsequently veriﬁed.
We conclude that trigger-action platforms can retain their
existing rule execution and channel creation infrastructure
while gaining the beneﬁts of DTAP.
VI.
IMPLEMENTATION & EVALUATION
We implemented DTAP-Client on the Android platform.
For additional client-side security, DTAP-Client will use a
hardware-backed keystore, when available, to generate a key
that we use to encrypt XTokens before storing them on the
ﬁlesystem. Such keystores have been present in iOS devices
since 2013 [19] and have been supported in Android devices
since version 6.0 [34].
We built a Python library that online service developers can
use to add DTAP functionality. The library provides a simple
annotation (i.e., Python decorator) that developers can place
above sensitive HTTP API methods that require rule-speciﬁc
scoping. The annotation automatically invokes the veriﬁcation
procedure (see §V). Using the Python library, we implemented
the DTAP-Cloud, and two online services modeled after existing
IFTTT channels: (1) an Amazon Alexa inspired ToDo list,
(2) an email service. For our benchmark measurements, we
implemented a skeleton version of IFTTT as our baseline. The
skeleton version uses standard OAuth tokens.
A. Microbenchmarks
We ﬁrst quantiﬁed micro-performance factors of DTAP.
We created the following rule: “IF new item == ‘buy
soap’ is added to MyToDo List THEN send email(new item).”
That is, if a new ToDo item with contents “buy soap” is added
to the list, then send an email. This rule is representative of
the kinds of rules that users can create on a typical trigger-
action platform such as IFTTT. It contains all the elements
of typical rules: a condition on data coming from the trigger
service, and transfer of trigger service data to an action service
function. We deployed DTAP locally, created the example rule,
and then measured storage overhead, transmission overhead,
and developer effort.5 We found that using DTAP imposes
negligible overhead: Each rule requires an additional 3.5KB
in terms of storage, and an additional 7.5KB of transmission
per execution. Online service developers using our prototype
library only need to add a single line of code per HTTP API
function—this is the same as that required by the popular
oauthlib library for Python. We elaborate on the results below.
5For microbenchmarks, deployment location does not affect the quantities
under study.
Fig. 7: Average total transmission size of baseline system and
DTAP for 1− 10 parameters for 5 experiments. Although there
is a linear increasing trend in both systems, the difference
among the two remains negligible.
1) Storage Overhead: Using DTAP requires online services
to store additional state: An online service needs to store an
XToken for each trusted client that allows the client to create
ﬁne-grained tokens for individual rules. The online service
also needs to store DTAP ﬁne-grained tokens for each rule.
These tokens include additional ﬁelds (e.g., time, TTL), so they
impose storage overhead on the online service. We computed
the required storage for the baseline IFTTT system, and for
DTAP. Our results show that each DTAP rule creates a 3.5KB
overhead in addition to the 0.8KB required to store the XToken,
compared to the 0.8KB storage cost for the baseline trigger-
action platform. This extra token storage cost is negligible
given the low price of storage and quantity of other user data
that these systems collect.
2) Transmission Overhead: Executing a rule on DTAP
requires transmitting more data over the network. This overhead
is the result of additional data in the trigger blob (Figure 6)
including time, TTL, and sign. To evaluate the transmission
overhead, we computed the transmission size of rule execution
in the baseline case and compared it to the same quantity in
the DTAP case. We varied the number of function parameters
passed (1 − 10) and present the average result of ﬁve experi-
ments. The number of function parameters matters because the
rule-speciﬁc token information encodes data about the speciﬁc
function being executed. We used Wireshark [17] to measure the
ﬂow sizes associated with ports assigned to online services and
the DTAP-Cloud. Figure 7 presents this overhead for different
number of function parameters for the two systems. In our
experiments, DTAP created 6 − 11% overhead. Even when
using 10 parameters the transmission overhead does not exceed
7.5KB. The variance in the results are due to normal network
variances such as packet retransmission.
3) Developer Effort: We developed DTAP as a library for
trigger and action services to make it easy for online service
developers to transition to the DTAP model. Developers must
only add a single additional line of code per function to protect
it with DTAP veriﬁcations. When compared to existing OAuth
12
6668707274761246810Transmission Size (KB)Number of ParametersDTAPIFTTTThroughput (req/sec)
DTAP
Avg
94.03
SD
8.48
IFTTT
Avg
96.46
SD
5.74
TABLE III: DTAP reduces throughput by 2.5% compared
to IFTTT. We used ApacheBench to send 10, 000 trigger
activations with up to 2000 concurrent activations.
approximately 1, 682 requests/second. Therefore, we chose
2000 as an upper-bound for the number of concurrent requests
a service would have to process. We used ApacheBench [3]
to conduct throughput testing of DTAP and IFTTT by sending
10, 000 trigger activations with upto 2000 concurrent activations
at a time. Table III presents our results, averaged over three
separate runs. We ﬁnd that DTAP decreases throughput by only
2.5%.
VII. DISCUSSION AND LIMITATIONS
Transitioning to DTAP. We discussed deployment options and
compatibility issues of DTAP in §V-D. Although we facilitate
migration to DTAP through development of an online library,
direct transition from legacy system to DTAP might be still
hard to achieve. One way to further ease this transition is the
incremental addition of DTAP support to online services using
a trusted proxy. Speciﬁcally, for each online service’s REST
API (set of function calls), we envision a trusted proxy running
in front of it that intercepts OAuth and API calls, translates
them from DTAP requests into regular requests, and vice-versa.
Although this trusted proxy would be overprivileged, it does
not increase the risk posed to the online service—an attack on
the proxy is equivalent to an attack on the online service. As we
stated in our threat model, if an online service is compromised,
an attacker can achieve its goals independently of any speciﬁc
trigger-action platform. The trusted proxy merely serves as
an intermediate solution until the actual online service API is
updated with DTAP support.
DTAP-Client use. In existing trigger-action platforms, users
can login to the IFTTT website and create rules from any
client device. However, DTAP requires users to create rules
via a client device they trust (e.g., their smartphone), which
stores XTokens in a private ﬁle system. Although our current
client prototype does not support transferring client state from
one device to another, building such functionality is fairly
straightforward. One possible solution is to provide an export
function to save the current client state to a disk image, and
then provide an import function to load that client state into
another device. If a client device is lost, then user rules continue
to execute normally. However, the user will have to download
the client again on another device, and go through the channel
connection phase to re-establish the XTokens to create future
rules.
A prototype limitation is that DTAP only allows a user to
use a single trusted client at a time. The protocol itself does
not preclude multiple clients, with users switching between
devices running the trusted clients based on convenience (e.g.,
a user at home may want to use a desktop to create rules, and
the same user at the workplace may prefer to use a phone).
Currently, our prototype does not include state maintenance
between different clients of a single user. To support such a
scenario, we envision the trusted client providing an option to
back up the current state (XTokens, recipes, etc.) to a user’s
Fig. 8: DTAP adds less than 15ms of veriﬁcation latency to
rule execution compared to IFTTT.
libraries, such as the popular oauthlib [13], this is the same
amount of effort—developers using oauthlib must also place a
single annotation above HTTP API methods to create scopes.
B. Macrobenchmarks
We measured end-to-end latency and throughput of rule
execution. We hosted the DTAP-Cloud and two online services
on separate Amazon t2.micro EC2 instances. Each instance
was conﬁgured with one 64-bit Intel Xeon Family vCPU@2.5
GHz, 1GB memory, 8GB SSD storage, Ubuntu 14.04 with
Apache2, and MySQL Server 5.5. Our results show a modest
15ms latency increase, and 2.5% throughput drop in the online
service when compared to the baseline (online service with
no DTAP protections). This does not represent an inhibiting
overhead for an online service especially when considering the
effect of network latency and the lack of real-time requirements
in these systems. We used the same ToDo list rule for our tests.
1) End-to-End Latency: We measured the time between
the trigger service being activated due to an item being added
to our ToDo list example rule, and the time the action service
issues a send_email call. This time includes network latency,
the time to generate a signed trigger blob, and the time to verify
the trigger blob and the action token, in the case of DTAP.
Our baseline case is a bare trigger-action system, and it only
includes network latency, and time to execute the trigger and
action functions without any DTAP veriﬁcation. We varied the
number of function parameters on the action service between 1
and 10. Figure 8 presents the results of these experiments. Our
results show that excluding the network latency, the maximum
veriﬁcation overhead is less than 15ms. For typical rules, that
send emails, SMSs, or invoke actions on physical devices over
a network, we consider this additional latency to be acceptable.
2) Throughput: We measure throughput as the number of
rules executed per second, under a load of 2000 concurrent
HTTP requests. We computed this concurrency level by
examining the number of times the most popular IFTTT
channel was used in rules (IF Notiﬁcation channel was used in
1, 514, 188 rules in our dataset). As per IFTTT’s documentation,
this channel will contact an online service once every 15
minutes [11], meaning that an online service would receive
13
00.050.10.150.2IFTTTDTAPIFTTTDTAPIFTTTDTAPIFTTTDTAPIFTTTDTAPIFTTTDTAPTime (s)Number of ParametersNetwork LatencyTrigger Service LatencyAction Service Latency1086421private cloud storage (e.g., Google Drive, Dropbox, etc.). The
trusted client can automatically keep this private cloud state
updated, and whenever the user logs in to a trusted client
running on a different device, the client can download the state
information securely.
Client-Device Loss. If a client device is lost, existing proce-
dures to erase device data take care of removing OAuth tokens.
Also, an “erasure-app” can be built to automatically contact
online services and invalidate tokens with co-operation from
our modiﬁed OAuth helper library. We leave implementing this
to future work.
XToken Security. The XToken is a high-powered credential.
Although DTAP reduces its vulnerability to leakage by design,
a malicious client can still leak this credential. Such leakage
however, only affects the single user and does not pose a
risk for other users of the DTAP platform. As discussed,
our implementation encrypts XTokens at rest using hardware-
backed keystores when available. We envision further security
by only performing XToken-related operations inside trusted
execution environments (e.g., Intel SGX on desktops, or ARM
TrustZone on phones). We leave implementing this to future
work.
Data conﬁdentiality and Privacy. Our design currently re-
duces the privilege of the DTAP-Cloud—it only gains access
to APIs and hence data it needs to run the user’s rules. This
is an improvement over the current state-of-the-art where we
have shown through our analysis that an attacker can gain
wide access to data and devices. However, even with our
improvements, an attacker can still gain access to sensitive
information simply by passively recording rule execution. A
potential way to provide data conﬁdentiality in this case is
to encrypt data passing through the DTAP-Cloud. However,
this can result in a loss of expressivity. Currently, trigger-
action platforms can evaluate predicates on trigger data (see our
weather data example in §V-C). Although the action service can
solely evaluate these predicates, it does increase computational
burden, thus defeating the purpose of trigger-action platforms.
As our analysis shows, the predicates are stateless and involve
simple comparison operators. Therefore, a potential solution
is to leverage advancements in use-case-speciﬁc homomorphic
encryption for secure integer comparison, rule matching, etc.,
to allow the least-privilege DTAP-Cloud to evaluate predicates
on encrypted data [35], [43]. DTAP design also enables action
services to collect data on triggers by recording the trigger
scope and trigger certiﬁcate during rule execution. While the
trigger service and the trusted client can obfuscate the trigger
scope, a current limitation is that the action service can still
proﬁle obfuscated trigger names.
Self-Signed Certiﬁcates. Our current implementation reuses
the HTTPS certiﬁcates of the online services. In order to avoid
problems associated with key reuse, another implementation is
to use self-signed certiﬁcates. A trigger service can generate a
self-signed certiﬁcate and send that along with the XToken to
the trusted client. The client can then send that certiﬁcate to the
action service that associates it with the rest of the action token
parameters. When a trigger occurs, the trigger service will
use a private key corresponding to that self-signed certiﬁcate’s
public key to sign the trigger blob. At veriﬁcation time, the
action service uses the public key in the self-signed certiﬁcate
to verify the blob. Such an implementation also avoids the
privacy issues of revealing the identity of the trigger service to
the action service, as the common name of the trigger service
in a self-signed certiﬁcate can be obfuscated.
Formal Veriﬁcation. The DTAP protocol has not been formally
veriﬁed yet. Our future work plan includes using automated
cryptographic protocol veriﬁcation tools such as ProVerif [14]
to verify the security guarantees.
VIII. RELATED WORK
Trigger-Action Platform Studies. A few studies have inves-
tigated IFTTT in recent years, although in different contexts.
Ur et al. [46] crawled the site in 2015, collecting 224,590
IFTTT programs shared by over 100,000 different users. Their
study shows many interesting statistics including the number of
different trigger and action channels used by IFTTT users. In
contrast, our work investigates the long-term security risks that
such platforms pose, and introduces the notion of Decentralized
Action Integrity to counter these risks.
Surbatovich et al. analyzed user-created rules (recipes in
IFTTT terminology) in the IFTTT platform using an information
ﬂow control approach [45]. Their focus was to determine the
risks that users face due to errors either in rule creation, or due
to inadvertent chaining of rules. In contrast, our work focuses
on discovering and addressing the security design deﬁciencies
of platforms like IFTTT. Addressing programming errors is an
orthogonal research direction—the TrigGen tool, for example,
aims to avoid errors caused by users incorrectly creating rules
that have insufﬁcient triggering conditions [37]. Our work is