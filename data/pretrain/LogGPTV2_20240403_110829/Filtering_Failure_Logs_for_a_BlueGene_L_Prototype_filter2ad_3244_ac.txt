t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
l
i
t
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
100
102
104
Inter−Failure Time (Seconds)
Table 2. General statistics about failure logs after applying step II.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
700
600
500
400
300
200
100
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
l
i
t
a
u
m
u
C
106
0
100
102
104
Inter−Failure Time (Seconds)
106
0
0
20
40
60
80
Node Card ID
100
120
140
D
I
i
p
h
C
35
30
25
20
15
10
5
0
6
6.2
6.4
6.6
Seconds
6.8
7
7.2
x 106
(a) Network failure records
(b) Memory failure records
n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
l
i
t
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
100
102
104
Inter−Failure Time (Seconds)
106
(c) Midplane switch failure records
Figure 3. CDF of inter-failure times after applying ﬁlter-
ing step II. Tth = 2 minutes. The x-axes are plotted using
log scale.
in bursts. For example, 93% of total memory failure en-
tries fall in the interval between 299th hour and 570th hour.
Compared to the other two components, midplane switch
failures are more evenly distributed temporally.
Figures 3(a)-(c) reiterate the above observations by plot-
ting the cumulative distribution function (CDF) of the inter-
record times of the network, memory and midplane switch
failures. Note that the x-axes are in log scale because the
inter-failure times show wide variances. For both network
and memory subsystems, 90% of the failure records are
within 1 second of the preceding entry, with the midplane
switch failures being more spaced out.
One may wonder why these failures are occurring in
bursts, given that we have already used a temporal thresh-
old of 2 minutes to ﬁlter out event clusters in Step II.
Note that Step II only eliminates duplicate records that
occur at the same location.
It is possible that multiple
locations/compute-chips could be detecting and recording
the same errors. One cannot simply use a temporal thresh-
(a) Memory failure records
across node cards
(b) All 32 chips within node
card 40 report failures at the
same time
Figure 4. A closer look at memory failures.
old to ﬁlter out all of them since it is possible that they may
be detecting different failures. It is important to dig deeper
into the records for the different components to ﬁlter the
events if they are indeed duplicates, and we discuss these
issues for each component below.
4.3.1 Memory Failures
Note that the BG/L prototype contains 4,096 compute chips
spread over 128 node cards, of which only chips of 22 node
cards ever report memory failures (Figure 4(a)). Among
these 22 node cards, chips within node card 40 report 90%
of the failures. In addition, from Figure 4(b), we ﬁnd that
all 32 chips from node card 40 often report failures at al-
most the same time (with a few seconds apart of each other).
Since it is extremely unlikely for 32 chips to encounter the
same hardware failure at the same time - especially because
these chips do not share memory, this strongly suggests
that some of these memory failures may be caused by soft-
ware bugs, e.g. de-referencing a null pointer, jumping to an
invalid location, accessing out-of-bounds, etc., instead of
hardware failures. The same bug in the software is causing
a manifestation of this error on the different nodes running
this application, and should thus be reported only once.
The challenge, however, lies in that it is difﬁcult to tell
whether an event is caused by a hardware failure or a soft-
ware bug by just looking at its description. For example,
an instruction TLB error can occur either because there is a
hardware problem (similar to the i-L2 parity error that we
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
7
illustrated in Figure 1) or because the program is trying to
jump to an invalid instruction address. We take the view-
point that a failure is more likely to be caused by software
bugs if the following two conditions are true: (1) many com-
pute chips (that are running the same program) report the
same problem within a short interval, and (2) the instruc-
tion/data address at which the failure occurs should be the
same across different chips (the probability of these two oc-
curring in the case of a hardware error is extremely low).
Filtering such events caused by the same error requires
clustering the events that occur on different chips, within a
short interval, and with the same event descriptions. Our
ﬁltering algorithm represents each failure record by a three-
attribute tuple , and it
outputs a table containing failure clusters in the form of
, where the cluster
size ﬁeld denotes how many different chips report this fail-
ure within a time threshold Tmem. When a new entry is
scanned, we ﬁrst use a hash structure to determine whether
or not the description has appeared before. If this is the ﬁrst
time we see this description, it becomes the cluster head,
and we insert its timestamp and description in the table,
with the corresponding cluster size ﬁeld set to 1. If the de-
scription has appeared before, we check if it falls into any
cluster by comparing its timestamp with the cluster head
which has the latest timestamp amongst all those having the
same description. If the gap between these two timestamps
is less than the threshold, it belongs to that cluster, and we
will not add this entry to the table, only incrementing the
cluster size by 1. If the gap is larger than the threshold, then
this entry becomes a new cluster head, and we record its in-
formation in the result table. After applying this technique,
we show portions of the resulting clusters for the memory
failures with Tmem = 60 seconds in Table 3.
Next, we need to decide whether a cluster in Table 3
is a software failure or a hardware failure, by evaluating
the two conditions mentioned above. Our algorithm marks
those clusters having sizes larger than 1 as those caused
by program bugs because the instruction addresses or data
addresses at which the failures occur are exactly the same
across the reporting chips.
Our discussions with the BG/L logging experts conﬁrm
the validity of this approach. Some of the common clusters
(which are also shown in Table 3) that we have found in
the logs include: instruction storage interrupts and instruc-
ID
2
5
8
12
17
Failure description
Cluster size
PPC440 instruction TLB error interrupt
PPC440 instruction storage interrupt
PPC440 data storage interrupt
PPC440 program interrupt
L3 major internal error
32
32
32
32
19
Table 3. Clusters of Memory failures.
200
180
160
140
120
100
80
60
40
20
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
0
0
20
40
60
80
Node card ID
4500
4000
3500
3000
2500
2000
1500
1000
500
D
I
i
p
h
C
100
120
0
0
1
2
3
4
Seconds
5
6
7
8
x 106
(a) Network failure records
across node cards. The 8 ver-
tical lines mark the midplane
boundaries.
(b) Many chips within the
same midplane report
the
same network failure within
a few minutes. The 8 hor-
izontal lines mark the mid-
plane boundaries.
Figure 5. A closer look at network failures.
tion TLB errors which can be caused by programs trying to
jump to an instruction address that does not exist; data stor-
age interrupts which can be caused by programs trying to
de-reference an invalid address; program interrupts which
can be caused by programs trying to execute an illegal in-
struction; and bus errors (denoted as L3 major error in the
table) due to illegal addresses going out on the bus. After
coalescing the software failures, we have totally 55 hard-
ware failures, and 21 software failures over 84 days.
4.3.2 Network Failures
Figure 5(a) shows how network failures are distributed
across all 128 node cards. Note that these 128 node cards
reside on 8 different midplanes, with node cards 1-16 on
midplane 1, node cards 17-32 on midplane 2, and so on.
In Figure 5(a), the midplane boundaries are marked by the
eight vertical lines. An interesting observation is that the
failure count statistics are correlated to the midplanes where
they occur. This suggests that chips within the same mid-
plane may be recording the same network problems, and we
need to ﬁlter our multiple records of the same error. A more
direct indication of this behavior is presented in Figure 5(b),
which shows the time (on the x axis) and location (on the y
axis) of all the network failures. It is clear that many chips
report failures roughly at the same time and these chips are
usually from the same midplane.
In this ﬁgure, the mid-
plane boundary is marked by the 8 horizontal lines.
Unlike the memory hierarchy (which is not shared)
where the probability is extremely remote that the same
hardware error simultaneously manifests on multiple chips,
it is quite possible for the different compute chips to detect