ization phase all the additional data needed for detection and
recovery is calculated and stored, including seeds for pseudo-
random number generators, partial and full checkpoints, and
dummy outputs needed to make some non-invertible layers
invertible. In choosing between using a checkpoint or dummy
input/output, we chose the strategy that minimizes storage
overheads. The initialization phase only runs once when neural
network is started on a system. In the error detection phase
the seeded pseudo-random number generator regenerates the
known input for each layer and is used in a forward pass
to generate an output. The output is compared against the
partial checkpoint to test for a mismatch (indicating errors
in the layer). A log of erroneous layers and the location of
the nearest input and output checkpoints is created for use by
the recovery phase. The error recovery phase uses this log to
identify layers to be recovered. Using the current locations of
the checkpoints, they are moved through the network with a
forward/backward pass to the affected layer. Once the input
and outputs are available to the affected layer, the parameter
recovery function is called to recover and restore the erroneous
parameters.
Partial and full checkpoints are not stored in the (DRAM)
working memory which is subject to soft errors as well as
memory-based security attacks. They can be stored in error-
resistant mediums, such as the storage devices (SSD or HDD)
or persistent memory (e.g. Intel DC persistent memory). This
is because the storage subsystem or persistent memory are
much denser and cheaper (by orders of magnitude) can be
made very dependable through redundancy techniques such
as RAID, and their slow access is tolerable because the
checkpoints are only needed for the occasional error detection
or recover.
IV. LAYER SPECIFIC ERROR DETECTION AND RECOVERY
While MILR may be general enough for other types of
NNs, we apply MILR on CNNs that are composed of 4
major layer types: convolution, dense, pooling, and ReLu
(Activation Layers) [26], including the bias and activation
parts of both dense and convolution layers. However these
parts will be handled as independent layers as each part has
their own mathematical relationships between the input, output
and parameters.
A. Dense Layers
Dense layers use matrix multiplication to compute their
output, with a 2D tensor of shape (M, N ) as the input.
This tensor is multiplied with a 2D parameter tensor of
shape (N, P ), producing a 2D tensor of shape (M, P ) as the
output. In other words, A(M,N ) × B(N,P ) = C(M,P ), where
A represents input, B represents parameters, and C represents
output.
a) Backward Pass: The backward pass of matrix multi-
plication can be yield input A by computing C × B−1. For
this to be done the input shape and the parameter shape must
meet certain requirements. The parameter must be of size such
that P ≥ N. If these requirements are not met, additional
information will need to be stored to allow for the creation of
enough equations for the system of equations. If P < N, then
the P dimension of parameters will be padded with α dummy
parameters such that P + α ≥ N. In order to minimize the
storage overhead due to the dummy parameters, the dummy
parameters are a stream of pseudo-random numbers, hence
only the seed needs to be stored, along with the output.
Dummy parameters produce more equations that allow the
a system of equations to be solvable. We also compare the
cost of storing dummy parameter output vs. a checkpoint, as
a checkpoint removes the need for a layer to be invertible, and
choose the approach that incurs a lower cost.
b) Parameter Solving: Parameter solving works on the
same principle as the backward pass. To solve for parameters,
the input shape must satisfy M ≥ N to create a solvable sys-
tem of equations, allowing correct parameters to be recovered.
If M < N, then the input will need to be padded with dummy
input along the M dimension such that M ≥ N. Just as in the
backward pass, the padding relies on a seeded pseudo-random
number generator, to avoid storing dummy input.
c) Error Detection : In the dense layer, each parameter
column can be used multiple times on different input rows.
This leads to the output of a single parameter column to appear
multiple times in the output. Only storing one of these outputs
per parameter column can therefore allow for error detection.
As we have an output that is resultant of each parameter value
(i.e. the partial checkpoint), it can identify if any parameter
changes.
B. Convolution Layers
Convolution layers are the foundation of CNNs. They work
by shifting a ﬁlter (parameters) along the X and Y axis of
the input, taking a weighted sum of the covered input sub-
region [26]. The input into a convolution layer is a 3D tensor
of shape (M, M, Z), where Z is the number of channels. This
input is then processed by Y ﬁlters, each of shape (F, F, Z),
represented by a 4D Tensor of shape (F, F, Z, Y ). Producing
an output tensor of shape (G, G, Y ). The relationship between
the input size and the output size can be expressed as G =
((M−F +2P )/S)+1, where S is the stride of the convolution
and P is the padding added to the input. Assuming that the
4
stride is 1, Equation 4 is repeated for all ﬁlters 0 < k < Y ,
and all locations 0 < i < G and 0 < j < G.
F(cid:88)
F(cid:88)
Z(cid:88)
z1=0
f1=0
f2=0
Outi,j,k =
a) Backward Pass
F ilterf1,f2,z1,k × Inf1+i,f2+j,z1
(4)
: The backward pass through the
convolution layer is based on the observation that each ﬁlter
looks at the same sub-region of the input, hence Y ﬁlters look
at each sub-region of the input. This produces Y equations
each representing the weighted sum of the sub-region, which
can be used as a system of equations to recover the sub-region.
In order to have enough equations to make the system solvable,
Y ≥ F 2Z is required.
If Y < F 2Z, additional information is needed to be able
to conduct the backward pass. For a convolution layer, we
can generate more ﬁlters to create additional equations. As
in dense layers, we rely on pseudo-random number generator
to create additional ﬁlters so that only the seed and output
need to be stored. Also, as in dense layers, we compare the
storage overhead of additional ﬁlters vs. adding a checkpoint,
and choose one that incurs lower storage overheads. Once all
of the solutions to all of the sub-regions are found, they can
be combined into the input.
b) Parameter Solving : The parameter (i.e., ﬁlters) solv-
ing is based on the observation that in a convolution layer,
the ﬁlter is used G2 times on varying sub-regions of the
input, allowing the creation of G2 equations representing the
use each ﬁlter for all Y ﬁlters. To recover all parameters
successfully, the size of the output of a singular ﬁlter needs
to be greater than the size of a ﬁlter, i.e. G2 ≥ F 2Z. If
G2 < F 2Z, padding with dummy input can be used to
generate more equations to make the system solvable.
As the number of channels grows, F 2Z can become much
greater than G2, requiring a large number of dummy input to to
make the system of equations solvable. As a result, we explore
an alternative approach that leads to partial recoverability of
parameters. Speciﬁcally, since it is extremely rare for a large
number of errors in a single layer [6], we can relax the error
recovery capability to recovering up to G2 parameters per
ﬁlter. To achieve this, we need to be able to identify which
group of parameters have errors, as discussed next. Once the
erroneous weights are identiﬁed, one can create a system
equations only representing their effect on the output, reducing
the variables in the equation. Allowing the recovery without
the additional dummy data.
c) Error Detection :
In the convolution, each ﬁlter
produces G2 output values. Storing just one of the outputs
for each ﬁlter allows one to monitor if the parameters change.
As the input will always be the same, if one of the parameters
change, the new output value will differ from the stored value,
allowing error detection of a layer.
To support parameter solving without adding dummy input
and output, it is not sufﬁcient detecting whether a layer has
errors in parameters or not. We need to identify the parameters
that contain errors. To achieve this, we use a modiﬁed version
of 2D Error Coding proposed by Kim et al. [9]. In our
Fig. 4: Two dimensional CRC
version, we use cyclic redundancy check (CRC) horizontally
and vertically on sets of 4 parameters(Figure 4), along the last
two axis of the 4D parameter matrix. This is performed F 2
times to fully encode all parameters in the matrix. Encoding
along the last two axis of the matrix allows for false positives
to be distributed among the ﬁlters.
When a layer is identiﬁed as erroneous, its CRC codes are
recomputed and compared to the stored values. CRC codes that
do not match their stored values are matched up with the CRC
codes along the other axis identifying singular weights that
are erroneous. This allows for the recovery of only erroneous
parameters. Two Dimensional CRC error detection achieves
a low false positive rate as shown in the evaluation. The
identiﬁed group of erroneous parameters reduce the number
of unknowns in the system of equations, allowing recovery.
C. Pooling Layers
Pooling layers reduce the dimensionality of the inputted
3d tensor of shape (M, M, Z), where Z is the number of
channels. The input is divided into sub-regions of speciﬁed
size, operating along each channel independently. Each of
these sub-regions are used to compute an output based on a
singular function. These functions vary with different pooling
layers, and many of them use an average or max value
function. A pooling layer changes the input in a non-invertible
way. Hence, it requires the addition of a checkpoint that stores
the input to the layer. Furthermore, since this layer has no
parameters, there is no requirement for a parameter solving
function.
D. Activation Layers
Convolutional neural networks primarily use ReLu activa-
tion layers as they introduce non-linearity into the network and
also address the vanishing and exploding gradient problems
[16], [26]. However, other activation functions can be used
throughout the network, both as separate layers and as parts
of other layers. However, all activation layers do have on
thing in common: they do not contain any parameters, hence
Removing the need for a parameter solving function. As their
is such a variety of functions one cannot not say whether an
activation layer will be invertible, as it depends on the speciﬁc
application. But as they do not change the shape of the input as
it passes through the layer, during the initialization phase and
the error recovery phase all activation functions are treated as
linear activation functions. Allowing forward and backward
5
CRC Along ColumnsCRC Along Rowspasses through the layer without any changes to the tensor
passing through.
A. Evaluation Method
V. EVALUATION
E. Bias Layers
The bias layer is not technically its own layer. But a part of
other layers, such as the convolution and dense layer. But for
our work it will be considered its own layer, as it has it own
mathematical operation, and its own relationship between its
input, output and parameters.
Input + P arameters = Output
(5)
The bias layer operates very simply, adding its parameters
to the input. This creates a minor shift in values. The bias
operation can be represented by equation 5. The way that
the bias is added can vary slightly based on the layer its
connected to; as the bias is a 1D tensor, and the input can
vary in dimensionality. For example, in the convolution layer
each ﬁlters output has a different bias value that is added to all
of its outputs. This differs from the dense layer where each
element column of the output has its own bias value to be
added.
a) Backward Pass: As the layer does simple addition,
the subtraction from the parameters from the Output yields
the input. Making a backwards pass very fast and efﬁcient.
b) Parameter Solving: Parameters solving is also very
simple with subtracting the input from output yields the
parameters. However, due to the different ways the bias is
added based on the layer its attached to, the yielded parameters
need to cleaned, removing the duplicate copies, yielding the
1D tensor containing the proper values.
c) Error Detection: Due to the small number of bias
parameters we can use a different scheme for error detection
compared to other layers. In this layer the sum of all the bias
parameters is taken and stored. Therefore if a bias value is
changed, the sum would also change detecting an error. There
are cases in which two values can change in equal opposite
amounts not allowing for error detection. This however is seen
to be very unlikely. This scheme saves storage space as in
schemes similar to other layers, exploiting parameter reuse,
the storage space needed would be equivalent to storing a
second copy of the parameters.
d) Other Layers: In a convolutional neural network other
layer can and do sometime get used. These layers can include
ﬂatten layers, input layers, dropout layers, and padding layers.
These layers have different effects on the network and are
used for various reasons. In general
these layers do not
have parameters so they do not need to have a parameter
solving function or error detection. For layers that are there
for training, and just pass through during prediction such
as a dropout layer, they can be essentially ignored. Letting
backwards passes pass through them. For layers that adjust
the shape, without loosing data such as a ﬂatten or padding
layer, on a backwards pass the data will be reshaped to the
original form. If data is lost on forward pass, then a checkpoint
is stored removing the invertibility requirement.
Our testing was done on three convolutional neural networks
using two different datasets. One network was trained using
the MNIST dataset [11]. Two networks a small and a large
one were trained using the CIFAR-10 [10] dataset. Details of
all three networks are shown in tables I, II and III.
TABLE I: MNIST network
Layer
Conv. 2D
Conv. 2D
Max Pooling
Conv. 2D
Dense
Dense
Output Shape
(26,26,32)
(24,24,32)
(12,12,32)
(10,10,64)
(256)
(10)
Trainable
320
9,248
0
18,496
1,638,656
2,570
TABLE II: CIFAR-10 small network
Trainable
Layer
Conv. 2D
896
9,248
Conv. 2D
0
Max Pooling
18,496
Conv. 2D
36,928
Conv. 2D
Max Pooling
0
73,856
Conv. 2D
147,584
Conv. 2D
147,584
Conv. 2D
0
Max Pooling
262,272
Dense
Dense
1,290
Output Shape
(32,32,32)
(32,32,32)
(16,16,32)
(16,16,64)
(16,16,64)
(8,8,64)
(8,8,128)
(8,8,128)
(8,8,128)
(4,4,128)
(128)
(10)
TABLE III: CIFAR-10 large network
Layer
Trainable
7,296
Conv. 2D
0
Max Pooling
230,496
Conv. 2D
0
Max Pooling
Conv. 2D
192,080
128,064
Conv. 2D
102,464
Conv. 2D
153,696
Conv. 2D
1,573,120
Dense
Dense
2,570
Output Shape
(32,32,96)
(16,16,96)
(16,16,96)
(8,8,96)
(8,8,80)
(8,8,64)
(8,8,64)
(8,8,96)