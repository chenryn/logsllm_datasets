symbol and relocation information, A2 denotes that the bi-
nary is Position Independent, A3 denotes that all instruction
boundaries are correctly identiﬁed by upstream disassembler,
and A4 denotes that the binary does not contain any inlined
data. S1 denotes that
the tool supports binaries compiled
from C++ programs, and S2 denotes that the tool supports
collecting other runtime information than coverage. Note that
the soundness of STOCHFUZZ can be guaranteed when there
is no inlined data, and probabilisticly guaranteed otherwise.
Fig. 2: Limitations of disassembly methods. The red box
shows corrupted code, and the yellow box shows missing code.
Prerequisite
Support
A1 A2 A3 A4 S1 S2
Require Source Code
Require Source Code
Tool
aﬂ-gcc
aﬂ-clang-fast
ptfuzzer [18]
aﬂ-qemu
aﬂ-dyninst [14]
e9patch [15]
-
-
-
-
-
-
-
Y N
-
Y Y
-
Y Y

  Y Y
RetroWrite [16]     N Y
-
Y Y
 Y Y
-
ddisasm [12]
STOCHFUZZ
-
-
-
-
-
-
-
-
-
-
-
-
-
Soundness Efﬁciency
Sound
Sound
Sound
Sound
Unsound
Sound
Unsound
Unsound
Sound
A
A+
C
D
A
B
A
A
A
A
Y Y Prob sound
ours using static binary rewriting. Columns 2-5 are the as-
sumptions made by these tools, where  denotes that a speciﬁc
precondition is required. Columns 6 and 7 show whether
C++ programs and other runtime feedback beyond coverage
are supported, respectively. Column 8 denotes the soundness
guarantee which means if the technique guarantees to rewrite
the binary properly and collect the right feedback, and column
9 denotes fuzzing efﬁciency with A+ the best.
Hardware-assisted Tracing. Modern processors offer a mech-
anism that captures software execution information using
dedicated hardware [6]. PTFuzzer [18] leverages this feature
to collect code coverage for binary-only fuzzing. For instance,
after executing the code in Fig. 1, two control transfers are
recorded, i.e., from 23 to 29 and from 35 to 38. Based on
the information, PTfuzzer subsequently recovers the execu-
tion path and hence the coverage. Other hardware-assisted
fuzzers operate similarly [22], [23]. The performance of these
approaches is limited by the costly trace post-processing
(4× slower than aﬂ-clang-fast according to our experiments).
Additionally, hardware-assisted fuzzing cannot capture other
runtime feedback than coverage [4], [21].
Dynamic Instrumentation. Dynamic instrumentation trans-
lates and instruments the binary during execution [7], [8].
Although it is an attractive solution due to its sound instrumen-
tation, the on-the-ﬂy translation/instrumentation incurs rela-
tively higher runtime overhead compared to other approaches.
Aﬂ-qemu, to the best of our knowledge, is among the best-
performing binary-only fuzzers based on dynamic instrumen-
tation. It still incurs signiﬁcant overhead (5× slower than aﬂ-
clang-fast according to our experiments). Other approaches in
this category, including aﬂ-pin [24] and aﬂ-dynamorio [25],
Fig. 3: Reassembly in RetroWrite. It crashes as the constant
13 in red circle is not properly symbolized.
induce even higher overhead.
Static Binary Rewriting. Static rewriting utilizes binary anal-
ysis to disassemble and rewrite the binary before execution.
Unfortunately, it is still a hard challenge to rewrite stripped
binary with soundness guarantee. Existing solutions often
make unsound assumptions about the target binary which may
lead to runtime failures.
Aﬂ-dyninst [14], a trampoline-based approach built upon
traditional disassembly techniques, assumes the upstream dis-
assemblers can correctly identify all the instructions. However,
such assumption may not hold in practice due to code and
data interleavings [15], [16]. Fig. 2 demonstrates how the
code example in Fig. 1 breaks its assumption. The left of
Fig. 2 shows that a linear disassembly, which decodes all bytes
consecutively, is confused by address 25, the inlined data byte.
Recursive disassembly, on the other hand, avoids this problem
by disassembling instructions following control ﬂow. But it
fails to resolve the target of the indirect jump at address 23,
missing the code from address 29 to 45.
E9patch [15] makes the same assumption as aﬂ-dyninst,
and additionally assumes there is no inlined data. With these
assumptions, e9patch specially engineers jumps that can safely
overlap with other instructions. As such, it can insert tram-
polines without sacriﬁcing the correctness of rewriting. In
addition, it uses a sophisticated virtual address space layout
for the instrumented binary, which on the other hand might
make it susceptible to a large number of cache misses and
additional overhead in process forking [26].
RetroWrite [16] is a reassembly technique for Position
Independent Code (PIC). It converts address related immediate
values in the binary to symbols (called symbolization) such
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:54 UTC from IEEE Xplore.  Restrictions apply. 
3661
FNLinear DisassemblyRecursive Disassembly0 :movrbx, 137 :  mov[rax], rbx10:  lear8, [rip+8]17:  movedx, [r8]20:  addrdx, r823:  jmprdxDATA:25:  .int4CODE1: 29:  movr9, [rax]32:  addr8, r935:  jmpr8CODE2:38:  movrax, 6045:  syscallFP0 :movrbx, 137 :  mov[rax], rbx10:  lear8, [rip+8]17:  movedx, [r8]20:  addrdx, r823:  jmprdxDATA:25:  .int4CODE1: 29:  movr9, [rax]32:  addr8, r935:  jmpr8CODE2:38:  movrax, 6045:  syscall?0 :movrbx, 137 :  mov[rax], rbx10:  lear8, [rip+8]17:  movedx, [r8]20:  addrdx, r823:  jmprdxDATA:25:  .int4CODE1: 29:  movr9, [rax]32:  addr8, r935:  jmpr8CODE2:38:  movrax, 6045:  syscall0 :  [afltrampoline]10:movrbx, 1317:  mov[rax], rbx20:  lear8, [L25] # r8=35(.L25)27:  movedx, [r8] # rdx=.L29-.L2530:  addrdx, r8   # rdx=.L2933:  jmprdx# correct(.L29)L25:35:  .int.L29-.L25L29: 39:  [afltrampoline]49:  movr9, [rax] # r9=1352:  addr8, r9    # r8=4555:  jmpr858:  movrax, 6065:  syscallReassemble &Instrumentthat they can be easily relocated after instrumentation. For
example in Fig. 3, the “lea r8, [rip+8]” instruction
at address 10 is translated as “lea r8, [L25]”, because
RetroWrite recognizes that rip+8 denotes a reference in the
code space and needs to be symbolized. As such, it could
be properly relocated after instrumentation. However, sound
static symbolization is provably undecidable [9] in general.
RetroWrite consequently makes strong assumptions such as
the requirement of relocation information and the exclusion of
C++ exception handlers. However, even if these requirements
were satisﬁed, the soundness of RetroWrite still could not be
guaranteed due to the need of sound memory access reasoning.
In the right side of Fig. 3, recognizing that
the constant
13 in the ﬁrst instruction “mov rbx, 13” is an address
offset (and needs symbolization) is challenging, due to the
long sequence of complex memory operations between this
instruction and the ﬁnal address de-reference at 55, which
ultimately discloses constant 13 is an address offset. In the
example, RetroWrite misclassiﬁes 13 as a regular value. As a
result, it is not symbolized. Ideally, it should be symbolized to
.L38-.L25, which would be concretized to 58-35=23 after
instrumentation. As a result, RetroWrite crashes on the binary.
A recent study [12] shares the same concern.
Ddisasm [12] is a state-of-the-art reassembly technique.
Rather than making assumptions about target programs, it
relies on a large set of reassembly heuristics such as instruc-
tion patterns. These heuristics, although comprehensive, have
inherent uncertainty and may fail in many cases.
B. Our Technique
Our
technique is inspired by two important
insights.
First insight: while grey-box fuzzers continuously mutate in-
puts across test runs, they may as well be enhanced to mutate
the program on-the-ﬂy. As such, disassembly and static rewrit-
ing (which are difﬁcult due to the lack of symbol information
and difﬁculties in resolving indirect jumps/calls ofﬂine) can be
incrementally performed over time.
Example. We use case A in the ﬁrst row of Fig. 4 to
demonstrate how our technique leverages the ﬁrst insight. The
workﬂow consists of four steps, an initial patching step prior to
fuzzing (step 1 ) and three incremental rewriting steps during
fuzzing (steps 2 , 3 , and 4 ).
In the snippet to the left of 1 , the code sections are ﬁlled
with a special one-byte hlt instruction , which will cause
a segfault upon execution. A segfault by a hlt instruction
indicates that the system has just discovered a code region
that has not been properly disassembled or rewritten such that
incremental rewriting should be performed. We will explain
later how we separate code and data in the ﬁrst place (as only
code is replaced with hlt in the snippet). The separation of
the two does not have to be precise initially and our stochastic
rewriting (discussed later) can gradually improve precision
over the numerous fuzzing runs. For instance, the execution
of initial patched code is terminated by the hlt at address
0, indicating a new code region. For easy description, we call
such segfaults intentional crashes.
The next step (incrementally) rewrites all the addresses that
can be reached along direct control ﬂow from the address
where the intentional crash happens. Speciﬁcally, STOCHFUZZ
places the rewritten code in a new address space, called the
shadow space; it further redirects all the direct jumps and calls
to their new targets in the shadow space by patching immediate
offsets; and since data sections are retained in their original
space, any PC-dependent data references need to be properly
patched too. At last, STOCHFUZZ inserts a jump instruction
at the crash address to direct the control ﬂow to the shadow
space. In the code snippet in between 1 and 2 , given the
crash address 0, STOCHFUZZ disassembles the instructions
from addresses 0 to 23 (highlighted in green shade). These
instructions are then rewritten in the shadow space starting
from address 90. Speciﬁcally, an afl trampoline is in-
serted at the beginning to collect coverage information, and
the original “lea r8, [rip+8]” instruction (at address
10) is rewritten to “lea r8, [rip-92]” (at address 110)
to ensure the data reference occurs at the original address.
STOCHFUZZ inserts a “jmp 90” instruction at 0 to transfer
the control ﬂow. Then, the fuzzer continues fuzzing with the
new binary and the incremental rewriting is invoked again if
other intentional crashes occur (e.g., steps 2 and 3 ). (cid:3)
A prominent challenge is to separate code and data in
executables, especially when inlined data are present. Due to
the lack of symbol information, it is in general an undecidable
problem [9]. Heuristics or learning based solutions [12], [27]
are inevitably unsound. Data may be recognized as instructions
and replaced with hlt. As a result, the program may execute
with corrupted data which may or may not manifest them-
selves as crashes. Corrupted states may lead to bogus coverage
and problematic test results. On the other hand, instructions
may be recognized as data and hence not replaced with hlt.
Consequently, these instructions are invisible to our system
and not instrumented.
The following second insight allows us to address the
aforementioned problem. Second insight: fuzzing is a highly
repetitive process that provides a large number of opportu-
nities for trial-and-error. That is, we can try different data
and code separations, which lead to different instrumented
executables, in different fuzzing runs. Over time, an increasing
number of samples can be collected, allowing us to achieve
the precise separation and correct rewriting. There are two
challenges that we need to overcome in order to leverage
the insight. First, we need to distinguish exceptions caused
by rewriting errors (introduced by our trial-and-error) and by
latent bugs in the subject program. We call both unintentional
crashes (to distinguish from intentional crashes by hlt). We
also need to pinpoint and repair rewriting errors, i.e., data bytes
misclassiﬁed as code (and undesirably replaced with hlt),
and vice versa. We call it the self-correction requirement.
Second, an executable cannot contain too many rewriting
errors. Otherwise, the fuzzing runs of the executable can hardly
make progress (as it continues to crash on these errors one
after another). Note that we rely on the fuzzer’s progress to
collect more and more samples to correct our rewritings. We
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:54 UTC from IEEE Xplore.  Restrictions apply. 
4662
Fig. 4: How STOCHFUZZ handles the motivation example
call it the progress requirement.
We therefore propose a novel stochastic rewriting tech-
nique that piggy-backs on the fuzzing procedure. At ﬁrst,
the technique performs probabilistic inference to compute the
likelihood of individual bytes in the original address space
belonging to data (or code). Such probabilities are computed
based on various hints, such as register deﬁnition-use relations
that often indicate instructions and consecutive printable bytes
that often suggest data. Details of the probabilistic inference
can be found in Section III-A. Since these hints are inher-
ently uncertain (e.g., printable bytes may not be data), we
use probabilities to model such uncertainty. Based on the
computed probabilities, STOCHFUZZ randomly generates a
rewritten version for each fuzzing run. In a random version, the
bytes replaced with hlt are determined by sampling based on
their computed probabilities. For instance, a byte with a high
probability of being code is more likely replaced with hlt.
When a segfault is observed, STOCHFUZZ determines if it is
caused by a rewriting error, by running the failure inducing
input on a binary with all the uncertain rewritings removed and
observing if the crash disappears. If so, delta debugging [28],
a binary-search like debugging technique, is used to determine
the root cause rewriting. Over time, the corrected rewritings,
together with the new coverage achieved during fuzzing,
provide accumulating hints to improve probabilistic inference
and hence rewriting. Note that the proposed solution satisﬁes
the two aforementioned requirements: the rewriting errors are
distributed in many random versions such that the fuzzer can
make progress in at least some of them; and they can be
automatically located and repaired.
1
Example Continued. We use case B (the lower box) in
Fig. 4 to illustrate stochastic rewriting. At the beginning (the
in case B), STOCHFUZZ computes
snippet to the left of
the initial probabilities (of being data bytes) as shown to the
left of the individual addresses. For example, a deﬁnition-use
relation between addresses 0 and 7 caused by rbx decreases
their probability of being data. Assume in a random binary
version the addresses with color shades are replaced by hlt,
with the yellow ones being the correct replacements as they
denote instructions and the red one erroneous since a data
byte is replaced with a hlt. The binary is executed and
then an intentional crash is encountered at address 0. In the
snippet to the right of
1 , besides the incremental rewriting
mentioned in case A, STOCHFUZZ also performs probability
recalculation which updates the probabilities based on the new
hints from the execution. Intuitively, as address 0 is code, all
addresses (in green shade) reachable from the instruction along
control ﬂow must be code. We say that they are “certainly
code” and their probabilities are set to 0. The probabilities of
remaining addresses are updated and new random binaries are
generated. In practice, many of the misclassiﬁed bytes such
as 25 are proactively ﬁxed by these new hints and updated
probabilities, without causing any crashes or even being
executed. This illustrates the importance of the aforementioned
progress requirement.
However to make our discussion interesting, we assume
25 (i.e., the data byte) and 38 are still replaced in the new
version (i.e., the snippet to the right of 1 ). During execution,
since the data at 25 is corrupted, a wrong target address
value is computed for rdx in the jump instruction at 123,