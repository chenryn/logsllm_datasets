所有新版本的存储代码都经过广泛的单元测试和集成测试过程，以捕获任何细微的错误，然后对随机合成工作负载进行一夜的端到端持久性测试。我们还运行了一组长时间运行的版本偏斜测试，在此期间，不同的代码版本在系统中的不同节点上同时运行；代码推送不是原子的，因此我们需要确保旧版本和新版本的代码彼此兼容。这些测试全部通过后，该版本将自动标记为“绿色”，并准备推送到暂存群集。
登台群集是存储系统的实际生产部署，存储生产数据子集的镜像。这是一个相对较大的群集，跨多个地理区域存储数十 PB。代码在此群集上运行整整一周，并且受到与存储系统其余部分相同的工作负载、监视和验证。在登台环境运行一周后，相应“DRI”（直接负责人，也就是“拥有”那个子系统的工程师）才签署那个软件组件是否正常运行（并且没有任何显著的性能下降）。此时，该软件已准备好推送到第一个实际生产区域。代码被推送到第一个生产区域，并运行一周，然后才能安全地推送到其余区域。回想一下，存储系统的多区域体系结构可确保在至少一个其他区域中复制一个区域中的所有数据，从而避免单次代码推送导致数据丢失的任何风险。内部验证系统经过调整，可在这一周的时限内检测任何异常情况，但实际上，任何潜在的耐久性问题在达到这一目标之前都已发现。但是，多区域部署模型在捕获仅在大规模出现的任何性能或可用性问题方面非常有价值。
发布过程经过管道处理，以便始终有多个版本的代码滚动产出。流程本身也在很大程度上是自动化的，以避免任何运维错误的不良影响。出于实际原因，我们还支持所谓的“破冰”程序，允许签署任何需要快速跟踪到生产状态的紧急更改，以及广泛的日志记录。此发布过程的彻底性使我们能够提供极高的耐用性，但对于绝大多数用例来说，这显然有些过头了。操作隔离是有代价的，通常以不便或工程师的挫折感来衡量。在小型公司中，跨所有计算机同时运行批处理作业，或者工程师能够 SSH 登录到队列中的任何服务器，都非常方便。然而，随着公司的发展，在运营隔离方面的投资不仅能够防范重大灾难，还允许团队通过隔离护栏来快速迭代和发展， 从而加快团队的开发速度。    
保护
 最大的耐久性威胁其实是你自己。
在理想的世界里，我们总是在坏事发生之前抓住它们。当然，这是一个不切实际的目标，但仍值得投入大量精力去努力实现。除了防止灾难的安全措施外，你还需要一套经过深思熟虑的恢复机制，以缓解任何从天而降的问题。
测试测试
 大多数工程师认为的第一个保护就是简单的测试。每个人都知道，你需要良好的测试，以产生可靠的软件。公司经常在单元测试方面投入大量精力来捕获基本逻辑错误，但在全面的端到端集成测试中投入不足。模拟软件堆栈的某些部分允许进行细粒度测试，但可以隐藏实际生产部署中发生的复杂竞态条件或跨系统交互。在任何分布式系统中，在测试环境中运行完整的软件堆栈和验证长时间运行的工作负载的正确性是无可替代的。在更高级的使用案例中，你可以在此集成测试期间使用故障注入来触发系统组件的故障。根据我们的经验，处理故障和转角情况的代码路径比更常见的流更容易出错。
保障
 如前所述，系统中最大的耐久性风险通常是你自己。那么，你如何防止自己犯傻呢？让我们从 Dropbox 历史上最糟糕的生产问题案例之一开始。
许多年前，一个运维人员在一组数据库上运行分布式 shell 操作时意外地遗漏了一组引号。打算运行的是dsh --group "hwclass=database lifecycle=reinstall" reimage.sh
而是键入以下内容：
dsh --group hwclass=database lifecycle=reinstall reimage.sh
这是一个容易犯的错误，但最终的结果是灾难性的。此命令导致该命令 lifecycle=reinstall reimage.sh 尝试在队列中的每个数据库上运行，而不是只 reimage.sh 在计划重新安装的数据库上运行，因此占用了我们生产数据库的很大一部分。这直接的影响是两天之久的广泛故障，在此期间，一些 Dropbox 服务不可用，但长期影响是一个重大投资过程，以防止此类事件再次发生。
诸如此类的故障后，立即的思考点是，运营人员不应受到指责。如果一个简单的错误可能导致大规模中断，那显然是因为流程故障，而不是人的错误。不要憎恨玩家，要讨厌游戏。在这种情况下，有许多明显的保障措施需要实施。我们添加了阻止重新启动实时数据库主机的访问控制，这是一种非常基本的保护，但通常忽略它。我们更改了分布式 shell 命令的语法，使其不易出现拼写错误。我们在工具中添加了基于隔离的限制，拒绝跨多个隔离域同时运行的任何分布式命令。然而，最重要的是，我们在自动化方面投入了大量资金，使运维人员无需运行这样的脚本，而是依靠比键盘上的人更可靠的系统。
保护的必要性必须作为一项工程原则加以维护，以便在最初部署它们所保护的系统的同时制定保障措施。
恢复
 失败肯定会发生。强大的运维团队的标志是能够快速从灾难中恢复，而不会对客户产生长期影响。
一个重要的设计注意事项是尝试始终有一个撤消按钮，即设计系统，以便有足够的状态来扭转意外操作或从意外损坏中恢复。日志、备份和历史版本可以极大地帮助在事件发生后恢复，尤其是需要经过精心的排练来操练恢复过程。缓解危险转换影响的一种技术是缓冲底层突变，以便有足够的时间在更新时运行验证机制。Dropbox 中的文件删除生命周期的设计是为了防止实时删除系统中的任何对象，而是用全面的安全措施来包装这些转换，如#object_deletion_flow所示。
对象删除流程
Dropbox 上的应用程序级文件删除是一个相对复杂的过程，涉及面向用户的还原功能、版本跟踪、数据保留策略和引用计数。这个过程经过多年的广泛测试和强化，但从恢复的角度来看，固有的复杂性带来了风险。作为最后一道防线，我们防止存储磁盘本身的删除，以防止较高层中的任何潜在问题。
当向物理存储节点发出删除时，不会立即解除对象的链接。相反，它会移动到磁盘上的临时“垃圾”位置，以便在我们执行进一步验证时进行安全保存。称为“垃圾检查器”的系统会遍运行回收垃圾中的所有卷，并确保所有对象都已被合法删除或安全地移动到其他存储节点（在内部文件系统操作过程中）。只有在垃圾检查通过，且经过给定的安全期后，卷才有资格通过异步进程从磁盘层面解除链接，该进程在紧急情况下可以被禁用。恢复机制的实施和维护成本通常很高，尤其是在存储案例中，它们会带来额外的存储开销和更高的硬件利用率。因此，分析堆栈中发生不可撤销的转换的位置，并做出明智的决策，说明需要在哪里引入恢复机制以提供全面的风险缓解，这一点非常重要。 
验证
  因为意外是难免的；所以重点在于故障发现。
在大型、复杂的系统中，最好的保护通常是检测异常并在异常发生时立即从异常中恢复的能力。当你监视生产系统的可用性或跟踪性能特性时，这显然是有必要的，但其实在为耐用性进行工程设计时，故障监测也特别有意义。
零的力量
   你的存储系统是否正确？真的，真的正确吗？你能 100％ 确信系统中的数据是否一致且没有丢失？在许多存储系统中，情况肯定不是这样。也许五年前有一个错误在你的数据库中留下了一些悬空的外键关系。也许 Bob 前一段时间运行了一个错误的数据库拆分，导致错误的分片上出现了一些行。也许存储系统中存在低背景级别的神秘 404 错误，你只需关注并确保速率不会上升。这样的问题存在于各地的系统中，但它们会产生巨大的运营成本。它们掩盖了所发生的任何新错误，它们使监视和开发变得复杂，并且它们会随着时间的推移而变得更加复杂，从而进一步损害数据的完整性。非常肯定地确定系统没有错误是一个很有意义的理念。如果一个系统今天有错误，但昨天没有，那么从那时到现在肯定发生了一些不好的事情。这个确信允许团队设置严格的警报阈值，并快速响应问题。它允许开发人员构建新功能，确信任何 Bug 都会被快速捕获。它最大程度地减少了必须的（就有关数据一致性的奇怪问题进行）推理的难度。然而，最重要的是，它允许团队在晚上睡好觉，因为他们知道自己值得用户信任。
实现零错误的第一步是知道你是否有任何错误开始。这种洞察力往往需要大量的技术投资，但这个投入会带来巨大的回报。
验证范围
 在 Dropbox，我们构建了一个高度可靠的地理分布式存储系统，用于存储海量的数据。这是一个完全自定义的软件堆栈，从磁盘调度程序直到负责外部通信服务的前端节点。虽然在内部开发这个系统是一项巨大的技术任务，但该项目的最大部分实际上并不是关于存储系统本身的开发。与它们正在验证的基础系统相比，开发验证系统的时间和精力要大得多！在系统上下文之外，这可能似乎令人惊讶，因为系统需要在任何时候绝对正确。我们在 Dropbox 维护一个很深入的验证系统。总体而言，这些系统产生的系统负载几乎与用户生成的生产流量相同！其中一些验证器涵盖各个关键系统组件，有些验证器提供端到端覆盖，整个堆栈旨在立即检测任何严重的数据正确性问题。