(mem, reg, cache, pc, fpc) →C
a (mem(cid:3), reg(cid:3), cache, pc, fpc)
The attacker relation for memories and registers directly extends
the symbolic one to the additional low-level tags
w1@ut1 →S
a w2@ut2
w1@ct1 →C
a w2@ct2
decR(m, cti) = uti
and similarly for decM. This allows the concrete attacker to
change atoms tagged User ut for some symbolic tag ut under
the same conditions as at the symbolic level, but prevents it
from changing any other atoms (in particular monitor code,
data, and registers) or any tags. This attacker model relies
on the correctness of the monitor self-protection mechanism
from §9.
The initial states at the concrete level are deﬁned as the
image under ≈CS
I of symbolic initial states that additionally
satisfy our symbolic invariants. This ensures that concrete
initial states satisfy both the generic low-level conditions from
§9 (Iw) and that they respect the symbolic invariants.
829829
A concrete trace is stopping if it has a (possibly empty) preﬁx
formed only of attacker steps between user states that are all
stuck with respect to user steps, followed by a (possibly empty)
sufﬁx of monitor states. This captures either immediately
getting stuck with respect to normal steps or missing in the rule
cache, faulting into the monitor, and eventually halting without
returning from monitor mode. This deﬁnition also deals with
the fact that we allow the attacker to take steps even after a
violation has occurred but before the machine is halted (right
before the fault into the miss handler).
The cfg function is deﬁned so that in monitor mode all
control ﬂows are allowed. We assume that monitor code is
correct, so we do not need to enforce CFI there.
Formal Results We prove CFI for the concrete machine
running a CFI monitor by transporting CFI from the abstract
machine to the symbolic and then to the concrete one using a
general CFI-preservation result.
1 sL
n ,→L
n ,→H
Theorem A.4 (CFI Preservation). Given a high-level ma-
chine M H = (StateH , initialH ,→H
a , cfgH , stoppingH ),
a low-level machine M L = (StateL, initialL,→L
a , cfgL,
stoppingL), a simulation relation between states sL ∼ sH, a
predicate checked sL
2 indicating which low-level steps need
to be checked for CFI, and a set of allowed indirect jumps J,
if M H has CFI, then M L also has CFI under the following
additional assumptions:
A1. 1-backward simulation with respect to ∼ for checked steps
to ∼ for
A2. {0, 1}-backward simulation with respect
A3. 1-backward simulation with respect to ∼ for attacker steps
A4. if initial sL, then ∃sH so that initial sH and sL ∼ sH;
A5. if sL
in →L
n;
unchecked steps in →L
n;
(→L
a );
1 ∼ sH
1 , sL
cfgH J = cfgL J;
1 ∼ sH
A6. if sL
1 , sL
1 sL
2 ) ∈ cfgL J;
1 , sL
(sL
1 ∼ sH
1 →n sH
1 , (sH
A7. if sL
2 then
¬(sH
1 →a sH
2 );
1 ∼ sH
2 ) (cid:10)∈ cfgH J, and
1 , checked sL
A8. and if sL
1 → sH
2 :: tH
sH
2 , and the trace sL
with respect to simulation relation ∼, and stopping (sH
2 ::
tH ), implies that stopping (sL
1 sL
2 :: tL reﬁnes the trace sH
2 ) (cid:10)∈ cfgH J, and sH
2 and ¬checked sL
1 →n sL
2 , checked sL
2 ∼ sH
2 :: tL).
2 , then
2 , (sH
1 , sH
1 , sH
1 sL
2 , then
Assumption A3 states that low-level attacker steps (→L
a ) are
simulated by corresponding high-level attacker steps, which
ensures that the low-level attacker is at most as strong as the
high-level one. A4 enforces that all low-level initial states can
be mapped to related high-level initial states. A5 ensures that for
checked low-level steps the two cfg functions completely agree.
A6 states that all unchecked low-level steps are allowed by cfgL
(e.g., monitor steps are allowed by the CFG). A7 states that
CFG violations are not simultaneously attacker steps. Finally,
A8 ensures that a high-level stopping trace can only be mapped
by the simulation relation to a stopping low-level trace.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:44 UTC from IEEE Xplore.  Restrictions apply. 
Theorem A.5 (CFI Concrete). The concrete machine running a
CFI monitor satisfying the assumptions in §9 has CFI.
B Additional Symbolic Machine Rules
(NOP)
pc, extra)
(CONST)
pc, extra)
(MOV)
pc, extra)
mem[wpc] = i@ti
transfer(Nop, tpc, ti,−,−,−) = (t(cid:3)
mem[wpc] = i@ti
transfer(Const, tpc, ti, t,−,−) = (t(cid:3)
decode i = Nop
pc,−)
(mem, reg, wpc@tpc, extra) → (mem, reg, (wpc+1)@t(cid:3)
decode i = Const w r
pc, t(cid:3))
(mem, reg, wpc@tpc, extra) → (mem, reg(cid:3), (wpc+1)@t(cid:3)
mem[wpc] = i@ti
decode i = Mov r rd
reg[rd]= @td
reg[r]=w@t
transfer(Mov, tpc, ti, t, td,−) = (t(cid:3)
reg(cid:3) = reg[r←w@t(cid:3)]
reg[r]= @t
pc, t(cid:3)
d)
reg(cid:3) = reg[rd←w@t(cid:3)
d]
(mem, reg, wpc@tpc, extra) → (mem, reg(cid:3), (wpc+1)@t(cid:3)
decode i = Load r rd
reg[r]=w@t
mem[wpc] = i@ti
transfer(Load, tpc, ti, t, t(cid:3), td) = (t(cid:3)
mem[w] = w(cid:3)@t(cid:3)
reg(cid:3) = reg[rd←w(cid:3)@t(cid:3)
d]
(mem, reg, wpc@tpc, extra) → (mem, reg(cid:3), (wpc+1)@t(cid:3)
pc, t(cid:3)
d)
reg[rd]= @td
(LOAD)
pc, extra)
pc@t
(JUMP)
pc, extra)
reg[r] = w@t
reg[r] = w(cid:3)
mem[wpc] = i@ti
transfer(Jump, tpc, ti, t,−,−) = (t(cid:3)
(mem, reg, wpc@tpc, extra) → (mem, reg(cid:3), w(cid:3)
decode i = Jump r
pc,−)
pc@t(cid:3)
mem[wpc] = i@ti
decode i = Bnz r n
pc,−)
transfer(Bnz, tpc, ti, t,−,−) = (t(cid:3)
w(cid:3)
pc = if w = 0 then wpc + 1 else wpc + n
(mem, reg, wpc@tpc, extra) → (mem, reg(cid:3), w(cid:3)
pc@t(cid:3)
decode i = Jal r
mem[wpc] = i@ti
reg[r] = w(cid:3)
reg[ra] = @tra
pc@t1
transfer(Jal, tpc, ti, t1, tra,−) = (t(cid:3)
pc, t(cid:3)
ra)
reg(cid:3) = reg[ra←(wpc+1)@t(cid:3)
ra]
(mem, reg, wpc@tpc, extra) → (mem, reg(cid:3), w(cid:3)
pc@t(cid:3)
References
[1] M. Abadi, M. Budiu, ´U. Erlingsson, and J. Ligatti. Control-ﬂow integrity
pc, extra)
pc, extra)
(BNZ)
(JAL)
principles, implementations, and applications. TISSEC, 13(1), 2009.
[2] M. Abadi and G. D. Plotkin. On protection by layout randomization. ACM
TISSEC, 15(2):8, 2012.
[3] J. P. Anderson. Computer security technology planning study. Technical Report
ESD-TR-73-51, U.S. Air Force Electronic Systems Division, 1972.
[4] A. Azevedo de Amorim, N. Collins, A. DeHon, D. Demange, C. Hrit¸cu,
D. Pichardie, B. C. Pierce, R. Pollack, and A. Tolmach. A veriﬁed information-
ﬂow architecture. POPL. 2014.
[5] M. Budiu, ´U. Erlingsson, and M. Abadi. Architectural support for software-based
protection. ASID. 2006.
[6] A. Chlipala. The Bedrock structured programming system: Combining
generative metaprogramming and Hoare logic in an extensible program veriﬁer.
ICFP. 2013.
830830
[7] J. A. Clause, I. Doudalis, A. Orso, and M. Prvulovic. Effective memory
protection using dynamic tainting. ASE. 2007.
[8] J. Criswell, N. Dautenhahn, and V. Adve. KCoFI: Complete control-ﬂow
integrity for commodity operating system kernels. IEEE S&P, 2014.
[9] L. Davi, P. Koeberl, and A.-R. Sadeghi. Hardware-assisted ﬁne-grained
control-ﬂow integrity: Towards efﬁcient protection of embedded systems against
software exploitation. DAC. 2014.
[10] L. Davi, A. Sadeghi, D. Lehmann, and F. Monrose. Stitching the gadgets: On
the ineffectiveness of coarse-grained control-ﬂow integrity protection. USENIX
Security, 2014.
[11] U. Dhawan, C. Hrit¸cu, R. Rubin, N. Vasilakis, S. Chiricescu, J. M. Smith, T. F.
Knight, Jr., B. C. Pierce, and A. DeHon. Architectural support for software-
deﬁned metadata processing. ASPLOS, 2015.
[12] U. Dhawan, A. Kwon, E. Kadric, C. Hrit¸cu, B. C. Pierce, J. M. Smith,
A. DeHon, G. Malecha, G. Morrisett, T. F. Knight, Jr., A. Sutherland, T. Hawkins,
A. Zyxnfryx, D. Wittenberg, P. Trei, S. Ray, and G. Sullivan. Hardware support
for safety interlocks and introspection. AHNS, 2012.
´U. Erlingsson and F. B. Schneider. IRM enforcement of Java stack inspection.
IEEE S&P. 2000.
[13]
[14] E. G¨oktas¸, E. Athanasopoulos, H. Bos, and G. Portokalidis. Out of control:
Overcoming control-ﬂow integrity. IEEE S&P, 2014.
[15] K. W. Hamlen, J. G. Morrisett, and F. B. Schneider. Computability classes for
enforcement mechanisms. TOPLAS, 28(1):175–205, 2006.
[16] C. Hrit¸cu, M. Greenberg, B. Karel, B. C. Pierce, and G. Morrisett. All your
IFCException are belong to us. IEEE S&P. 2013.
[17] J. B. Jensen, N. Benton, and A. Kennedy. High-level separation logic for
low-level code. POPL. 2013.
[18] J. Kroll, G. Stewart, and A. Appel. Portable software fault isolation. CSF.
2014.
[19] A. Kwon, U. Dhawan, J. M. Smith, T. F. Knight, Jr., and A. DeHon. Low-
fat pointers: compact encoding and efﬁcient gate-level implementation of fat
pointers for spatial safety and capability-based security. CCS. 2013.
[20] X. Leroy and S. Blazy. Formal veriﬁcation of a C-like memory model and its
uses for verifying program transformations. JAR, 41(1):1–31, 2008.
[21] J. Ligatti, L. Bauer, and D. Walker. Edit automata: enforcement mechanisms
for run-time security policies. IJISS, 4(1-2):2–16, 2005.
[22] A. J. Mashtizadeh, A. Bittau, D. Mazi´eres, and D. Boneh. Cryptographically
enforced control ﬂow integrity. arXiv:1408.145.
[23] J. H. Morris, Jr. Protection in programming languages. CACM, 16(1):15–21,
1973.
[24] G. Morrisett, G. Tan, J. Tassarotti, J.-B. Tristan, and E. Gan. RockSalt: better,
faster, stronger SFI for the x86. PLDI. 2012.
[25] S. Nagarakatte, M. M. K. Martin, and S. Zdancewic. Hardware-Enforced
Comprehensive Memory Safety. IEEE Micro, 33(3):38–47, 2013.
[26] S. Nagarakatte, J. Zhao, M. M. K. Martin, and S. Zdancewic. SoftBound:
highly compatible and complete spatial memory safety for C. PLDI. 2009.
[27] S. Nagarakatte, J. Zhao, M. M. K. Martin, and S. Zdancewic. CETS: compiler
enforced temporal safety for C. ISMM. 2010.
[28] B. Niu and G. Tan. Modular control-ﬂow integrity. PLDI. 2014.
[29] F. B. Schneider. Enforceable security policies. TISSEC, 3(1):30–50, 2000.
[30] G. E. Suh, J. W. Lee, D. Zhang, and S. Devadas. Secure program execution via
dynamic information ﬂow tracking. ASPLOS, 2004.
[31] E. Sumii and B. C. Pierce. A bisimulation for dynamic sealing. TCS, 375(1-
3):169–192, 2007.
[32] R. Wahbe, S. Lucco, T. E. Anderson, and S. L. Graham. Efﬁcient software-
based fault isolation. SOSP, 1993.
[33] B. Yee, D. Sehr, G. Dardyk, J. B. Chen, R. Muth, T. Ormandy, S. Okasaka,
N. Narula, and N. Fullagar. Native Client: a sandbox for portable, untrusted x86
native code. CACM, 53(1):91–99, 2010.
[34] C. Zhang, T. Wei, Z. Chen, L. Duan, L. Szekeres, S. McCamant, D. Song,
and W. Zou. Practical Control Flow Integrity & Randomization for Binary
Executables. IEEE S&P, 2013.
[35] L. Zhao, G. Li, B. D. Sutter, and J. Regehr. ARMor: fully veriﬁed software
fault isolation. EMSOFT. 2011.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:44 UTC from IEEE Xplore.  Restrictions apply.