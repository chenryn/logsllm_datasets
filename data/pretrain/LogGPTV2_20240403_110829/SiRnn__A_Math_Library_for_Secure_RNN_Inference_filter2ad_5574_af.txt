a proof establishes that for all possible inputs,
the ULP
error (Section III) between the math implementation and the
exact real result is small. For small bitwidths (e.g. (cid:54) 32)
that are used in ML (Section VI-C), it is tractable to prove
these bounds on ULP error using exhaustive testing, whereas
for 64-bit ﬂoating-point or 64-bit ﬁxed-point math libraries,
these proofs can either be interactive [54], [77] or fully
automatic [38], [78], [108]. Since our focus is on math libraries
for ML, we choose the exhaustive testing approach for our
math library, speciﬁcally, we 1) run our implementations on
all possible inputs, 2) compare the ULP error between each
output and the inﬁnite precision real result, and 3) report the
maximum observed ULP error as the bound. For step 2, we
need the ability to compute math functions to arbitrary degrees
of precision – this is offered by the GNU MPFR library [45].
We prove ULP error bounds for bitwidth 16 (Section VI-C)
and appropriate input/output scales, sx and sy, and choose
parameters d, g, and t accordingly to ensure high precision.
Note that given a bitwidth (cid:96), a proof via exhaustive testing
requires 2(cid:96) tests. For exponential, we set d = 8 and prove that
∀sx, sy ∈ [8, 14], the maximum ULP error is 3. For sigmoid
and tanh, we set d = 8, g = (cid:100) sy−2
2 (cid:101) and t = 0, and prove that
∀sx, sy ∈ [8, 14] the maximum ULP error is 3 for sigmoid
and 4 for tanh. For reciprocal square root, we choose inputs
2 (cid:101) and t = 1. We prove
x (cid:62)  where  = 0.1, and set g = (cid:100) sy
that ∀sx, sy ∈ [4, 13], the maximum ULP error is 4.
Thus, using exhaustive testing, we prove that our math
implementations are precise for chosen parameters and provide
standard precision guarantees that are expected from math
libraries viz. ULP error < 5; Intel’s SVML [4] also provides
math implementations with 4 ULP error. We use the same
parameter setting described above for the empirical evaluation.
VI. EVALUATION
In this section, we empirically compare our protocols for
math functions with prior works and describe the results of our
ML case studies. The closest work to ours is MiniONN [83],
the only prior work on secure inference that has been evaluated
on an RNN. MiniONN proposes a recipe to obtain piecewise
linear approximations to sigmoid/tanh that are then evaluated
using its protocols. Our secure implementations of sigmoid are
an order of magnitude better in communication (Table I). Note
that no prior work on 2-party secure inference (including Min-
iONN) provides secure implementations of exponentiation and
reciprocal square root; we evaluate them in Table II. General-
purpose MPC frameworks like MP-SPDZ [66] also provide
semi-honest 2PC implementations of math functions [3] that
are compatible with the standard (power-of-2 ring-based)
ﬁxed-point representation. However, the communication of our
protocols is up to two orders of magnitude lower. Alternatives
that use representations such as ﬁeld-based representations or
ﬂoating-point also suffer from high communication overheads.
Next, we evaluate our library SIRNN for DNN inference on
end-to-end ML models. First, we evaluate SIRNN on models
with math functions considered by priors works [83], [102].
Since they evaluate sigmoid and tanh using generic 2PC pro-
tocols, SIRNN has an order of magnitude less communication
(Table III). Next, we evaluate SIRNN on RNNs for sports
training and audio keyword spotting that use GRU cells, which
are composed of sigmoid and tanh operators. There are two
ways to securely evaluate our math functionalities, with our
2PC protocols and with generic 2PC protocols for mixed
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:28:57 UTC from IEEE Xplore.  Restrictions apply. 
1012
arithmetic and boolean compute [25], [32], [41], [95]. We
evaluate both and observe that SIRNN communicates over
500× less data for both the RNNs (Table IV). Finally, we
evaluate SIRNN on a recent model architecture that combines
CNN operators and RNN operators to ﬁnd the human heads
in images with state-of-the-art accuracy [104]. We provide
the ﬁrst secure implementation for this complex model; its
secure implementation requires all the protocols described in
this paper including reciprocal square root and takes less than
7 minutes on our evaluation set up:
System Details. We use a set up where the 2 machines are
connected via a 377 MBps LAN network with 0.8 ms RTT.
Both the machines have commodity hardware with a 4-core
3.7 GHz Xeon processor and 16 GBs of RAM.
Implementation Details. The users of SIRNN express their
DNNs as a combination of calls to SIRNN’s C++ library
functions. These functions include matrix multiplication, con-
volutions, MBConv blocks, L2 Normalization, batch normal-
ization, broadcasting; pointwise operators like sigmoid, tanh,
exponential, reciprocal square root, matrix addition, Hadamard
product; comparison-based operators like argmax, maxpool,
ReLU, and ReLU6. The last four functions use protocols
from [99] and the rest use our building blocks. The library
functions take scales as arguments and are templated on
the bitwidths. The SIRNN library is implemented using 28K
lines of C++. We statically generate 36 LUTs that consume
additional 35K LOC.
A. Microbenchmarks
a) Sigmoid: In Table I, we compare our protocol with
prior work for generating sigmoid output with 12-bits of
precision (i.e., scale 12). We report absolute numbers for
time taken and communication for both our protocols and
prior work, as well as improvement factor of our protocols
in parentheses. We follow this pattern for all
the tables
in this section. We focus on sigmoid as the numbers for
tanh are similar. One sigmoid evaluation with our protocols
incurs less than 5KB of communication and produces precise
results with at most 3 ULPs error. In ML, sigmoid is usually
computed pointwise over all the entries in a tensor. Hence,
one needs to compute sigmoid of a large number of instances
when dealing with realistic ML benchmarks. Although the
communication to compute n sigmoid instances grows linearly
with n, empirically we have observed that the time taken or the
latency grows sub-linearly with n (columns 2 to 5 of Table I),
which helps our implementations to scale well to large tensors
(Section VI-C). The cost of rounds amortizes better for large
tensors resulting in the sub-linear growth in latency.
As a baseline, we consider the recipe of MiniONN that
approximates math functions with piecewise linear approxi-
mations and provides protocols to evaluate these splines. More
precise approximations require more number of pieces. To get
an ULP error below 5, MiniONN needs a 48-way spline which
provides poor performance when evaluated securely because
of a 70× communication overhead.
Technique
Our Work
MiniONN
48-piece
MiniONN
12-piece
Deep-
Secure
MP-SPDZ
Ring Poly
MP-SPDZ
Ring PL
MP-SPDZ
Field Poly
MP-SPDZ
Field PL
Total Time for #Instances (in sec)
102
103
104
105
0.08
0.20
(2.5x)
0.06
(0.8x)
0.16
(2x)
0.75
(9.4x)
0.27
(3.4x)
0.91
(11.4x)
0.52
(6.5x)
0.10
1.94
(19.4x)
0.54
(5.4x)
0.84
(8.4x)
1.72
(17.2x)
0.28
(2.8x)
1.91
(19.1x)
0.47
(4.7x)
0.25
18.85
(75x)
5.24
(21x)
8.1
(32x)
14.88
(59.5x)
1.32
(5.3x)
16.51
(66x)
1.79
(7.2x)
1.58
182.2
(115x)
53.84
(34x)
141.3
(89x)
140.6
(89x)
12.34
(7.8x)
127
(80x)
14.23
(9x)
Comm./
Instance
(in KB)
4.88
341.03
(70x)
93.36
(19.1x)
124.65
(25x)
981.11
(201x)
76.42
(15.7x)
228.63
(46.9x)
27.52
(5.6x)
Max
ULP
Err.
3
4
104
NA
2
266
2
266
TABLE I: Comparison with prior works on sigmoid with
varying number of instances.
For the RNN benchmark that MiniONN considers (Sec-
tion VI-B), the precision offered by the 48-piece spline is an
overkill and a 12-piece spline sufﬁces to maintain the cross
entropy loss. Although this 12-piece spline is more efﬁcient
than 48-piece spline, its performance is still much worse than
our protocols and incurs a 19× communication overhead.
Furthermore, this 12-piece spline incurs an error of 104 ULPs.
Hence, our implementations are superior in both precision and
performance. While a 12-piece spline sufﬁces for this bench-
mark, MiniONN remarks that other benchmarks need splines
with more number of pieces that are even more expensive to
compute. Because our implementations are guaranteed to be
numerically precise, they can be used as-is with no loss in
model accuracy (Section VI-C).
DeepSecure [102] uses garbled circuits (GC) to evaluate
DNNs that use sigmoid and tanh activations. We checked with
the authors of DeepSecure and the circuits for math functions
are not available. Hence, we cannot compute the ULP errors
of their implementations. However, DeepSecure reports the
number of non-XOR gates that can be used for performance
estimates. We used state-of-the-art for GC implementation,
i.e., EMP-Toolkit [1], [52], [53], to obtain these performance
estimates that are better than the performance reported by
DeepSecure. The communication of our protocols is 25×
lower (4th row of Table I).
MP-SPDZ [66], a general-purpose MPC framework, pro-
vides 2 baseline sigmoid implementations for 2PC [3]: Poly-
based, which uses a range reduction and Taylor series poly-
nomials to compute exponential followed by division, and
PL-based, which is a built-in piecewise linear spline. The
former implementation incurs error comparable to us but
communicates 201× more, while the latter is more than an
order of magnitude inferior in precision and communication
(5th and 6th row of Table I).
While we focus on power-of-2 rings, there are other works
on secure implementations of sigmoid that use ﬁeld-based or
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:28:57 UTC from IEEE Xplore.  Restrictions apply. 
1013
Technique
Our Work
MP-SPDZ
Our Work
MP-SPDZ
Total Time for #Instances (in sec)
102
103
104
105
Comm./
Instance
(in KB)
Max