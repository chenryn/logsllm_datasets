title:Demystifying service discovery: implementing an internet-wide scanner
author:Derek Leonard and
Dmitri Loguinov
Demystifying Service Discovery: Implementing an
Internet-Wide Scanner
Derek Leonard and Dmitri Loguinov
Department of Computer Science and Engineering
Texas A&M University, College Station, TX 77843 USA
{dleonard,dmitri}@cse.tamu.edu
ABSTRACT
This paper develops a high-performance, Internet-wide ser-
vice discovery tool, which we call IRLscanner, whose main
design objectives have been to maximize politeness at re-
mote networks, allow scanning rates that achieve coverage of
the Internet in minutes/hours (rather than weeks/months),
and signiﬁcantly reduce administrator complaints. Using
IRLscanner and 24-hour scans, we perform 21 Internet-wide
experiments using 6 diﬀerent protocols (i.e., DNS, HTTP,
SMTP, EPMAP, ICMP and UDP ECHO), demonstrate the
usefulness of ACK scans in detecting live hosts behind state-
less ﬁrewalls, and undertake the ﬁrst Internet-wide OS ﬁn-
gerprinting. In addition, we analyze the feedback generated
(e.g., complaints, IDS alarms) and suggest novel approaches
for reducing the amount of blowback during similar studies,
which should enable researchers to collect valuable experi-
mental data in the future with signiﬁcantly fewer hurdles.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement Techniques
General Terms
Design, Measurement, Performance
Keywords
Horizontal Scanning, Service Discovery
1.
INTRODUCTION
Characterizing visible services in the Internet (e.g., web
sites [5], [16], [28], end-hosts [17], [18],[23]), discovering and
patching servers with critical security vulnerabilities (e.g.,
SSH [42], DNS [13]), and understanding how Internet worms
create massive botnets [10], [25], [31], [53] are important re-
search topics that directly beneﬁt from eﬃcient and scalable
scanning techniques that can quickly discover available ser-
vices in the Internet.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
Our focus in this paper is on horizontal scanning [52],
which is a method for enumerating (in some set S) all re-
mote hosts that support a given protocol/service p. This is
accomplished by sending packets to destinations in S and
counting positive responses within some time interval. We
call a scan complete if each address in S is probed and par-
tial otherwise. The latter type of scan signiﬁcantly reduces
the burden on remote networks and is useful when an esti-
mate of the number of responsive hosts (rather than their
IP addresses) is suﬃcient.
While several large-scale measurements have been con-
ducted in the past [5], [13], [17], [43], researchers initially
considering a similar project are often faced with delays on
the order of months for individual tests to run [5], [43]. Dur-
ing this time, computational resources of potentially dozens
[5] of local machines that could be put to other uses are tied
up. Further complicating the issue is the possibility of facing
a signiﬁcant number of complaints from hostile network ad-
ministrators [5], [8], [11], [13], [15], [17], [42], [43], even for
partial measurements. Given these issues, questions arise
about the feasibility of service discovery [9], especially on
sensitive TCP ports. Evidence suggests that sometimes [17]
researchers are even forced to abort planned activities due
to the negative publicity generated by scan traﬃc.
Since previous work has not explicitly aimed to design a
high-performance scanner and/or maximize its politeness,
there is no standard by which to judge the quality or intru-
siveness of a scanner. Our ﬁrst step then is to propose three
main objectives that a good Internet-wide scanning solution
must satisfy: 1) eﬃcient usage of resources (i.e., bandwidth,
CPU, memory) in complete scans; 2) accuracy of extrapola-
tion in partial scans; and 3) maximum politeness at remote
networks. The ﬁrst objective ensures that the implementa-
tion scales well when scan duration T is reduced to hours
or even minutes. The second objective delivers a platform
for extremely fast partial scans with accurate extrapolation
of metrics of interest. The last objective maximally reduces
the instantaneous load (i.e., burstiness) applied to target
subnets and controls the rate of IDS activity (i.e., false-
positive alarms, wasted investigation eﬀort, and dynamic
ﬁrewall blocks against the scanner network) in response to
scan traﬃc.
We next build a scanner that satisﬁes these goals and
evaluate its performance in real scans.
1.1 Our Contributions
The ﬁrst part of the paper analyzes the approaches ex-
posed in the literature to understand whether they can be
used to optimize performance, politeness, and extrapolation
109ability of an Internet-wide scanner. In addition to realizing
that prior work was not driven by any particular objectives
in designing their scanners (besides obtaining the data of
interest in some ﬁnite amount of time), we also reach the
conclusion that there is no consensus on such important pa-
rameters as scan scope, permutation, split among source
IPs, implementation, timeouts, handling of complaints, and
monitoring of the scan’s intrusiveness at remote networks.
To overcome this problem, the second part of the pa-
per presents our design of IRLscanner, which is a high-
performance and source-IP scalable framework for service
discovery in the Internet. The central element of the scanner
is a novel permutation/split algorithm, which we show is op-
timally polite at each CIDR subnet s as it spaces probes ar-
riving to s equally throughout scan duration [0, T ], even with
multiple source IPs. Extrapolation results with IRLscanner
running at its default rate r (at which it covers the Internet
in T = 24 hours) demonstrate that partial scans of our ap-
proach are unbiased, leading to 1% estimation error in the
number of live hosts in just 10 seconds.
Due to the goal of allowing faster scan durations (i.e., min-
utes/hours) and politeness concerns, IRLscanner incorpo-
rates additional features that help it achieve our objectives.
These include a signiﬁcantly reduced scope of measurements
(i.e., one billion packets fewer) compared to previous scan-
ners, absence of largely ineﬀective retransmissions, ability
to run with any number of IPs aliased to the same server,
capture of all back-scan and bogus traﬃc (e.g., from hackers
and buggy implementations), and signiﬁcantly higher time-
outs for unresponsive targets, which allows it to capture a
wider variety of busy/slow hosts in the Internet than was
possible before.
Armed with IRLscanner, the third part of the paper high-
lights 21 Internet-wide scans across a wide range of protocols
and ports, including DNS (port 53), HTTP (port 80), SMTP
(port 25), EPMAP (port 135), and UDP ECHO (port 7).
In addition to running over 20 times faster than any prior
scanner and probing ports that have never been scanned in
the literature (i.e., SMTP, ECHO, EPMAP), we experiment
with several techniques never achieved on an Internet-wide
scale in previous work (e.g., ACK scanning, testing ECN
and TCP option support) and perform the ﬁrst large-scale
OS ﬁngerprinting study of 44M hosts responding to port 80.
We ﬁnish the paper by analyzing the feedback generated
from our experiments. This includes a detailed complaint
analysis, techniques for monitoring the impact of scan traf-
ﬁc on IDS activity in the Internet, approaches for a-priori
predicting the amount of blowback in response to scanning a
particular port, and various ways for reducing the perceived
maliciousness of the scan.
1.2 Ethical Implications
Over the last two years, we have closely worked with uni-
versity oﬃcials and taken numerous steps (see below) in an
eﬀort to reduce investigation eﬀort and aggravation for re-
mote administrators. While our interest in this paper is
purely to expose the underlying issues of service discovery
and make it more accessible to researchers without dam-
aging remote networks, one concern might be that our scan
techniques are not only maximally polite, but also optimally
stealthy against popular IDS packages. As a result, one
could argue that attackers could beneﬁt from our work and
thus inﬂict certain damage that would not otherwise be pos-
sible.
However, we do not believe this to be the case. First,
stealthier scanning by itself does not compromise hosts; in
contrast, intrusion using malicious payload (e.g., delivered
through unsolicited packets or email) does. As a result,
many networks with patched hosts and up-to-date IDS sig-
natures of various malware should remain well protected de-
spite the ﬁndings of this paper. Second, botnets aﬀord hack-
ers such a diverse pool of IPs that they often do not care to
remain stealthy and rely on the most basic sequential scan-
ning [2], which apparently is suﬃcient for their purposes.
Instead, we believe that the discussion and techniques ex-
posed in this paper might be useful in building future de-
fenses against scanning worms, should they choose to deploy
similar techniques.
Another concern might be that we are collecting informa-
tion about remote networks that their administrators do not
wish to make public. We contend that such information is
well-protected by ﬁrewalls, and this paper makes no attempt
to trick or confuse network monitoring devices to gather sen-
sitive data. Further, given the constant background scan-
ning performed by attackers [41], any data (and likely much
more) we collect is already available to them. However, to
ensure privacy, we do not publicize any information about
individual networks collected during our scans and instead
rely only on summary statistics.
2. SCANNER DESIGN
Beyond the experiment-speciﬁc choice of the protocol/port
pair that uniquely characterize a service, every researcher
considering a horizontal scan must answer a common set of
questions before proceeding. In this section, we turn to sev-
eral recent studies [5], [13], [17], [43] that have performed
large-scale service discovery to determine whether these de-
sign questions have been deﬁnitively answered and our ob-
jectives met.
2.1 Scan Scope
We start with the issue of which IP addresses to target
when scanning. Deﬁne F to be the Internet IPv4 address
space, which consists of n = 232 addresses available for scan-
ning. While intuition may suggest to probe the entire space
to ensure completeness, certain IPs may not be suitable for
scanning. Before delving into details, deﬁne set N R ⊆ F
to be all non-reserved destinations [20], I ⊆ N R to be all
IANA-allocated blocks [19], and B ⊆ I to be the set of IPs
advertised in BGP preﬁxes [59] at the border router of the
scanner network. Note that B must be veriﬁed against I to
ensure that invalid advertisements are not included.
To capture the choice of which destinations to target, we
deﬁne scan scope S to be the subset of F probed during
measurement. As shown in the second column of Table 1,
all previous Internet-wide service discovery projects scanned
at least the IANA allocated space I, which is justiﬁed [17],
[34] by churn in BGP routing tables and desire to avoid
losing responses during the period of the experiment (i.e.,
30+ days in Table 1). In [5], however, no reason is given
for scanning unallocated space, although there is a slight
possibility of new blocks being allocated by IANA during
the measurement.
As we discuss later, sets I and N R may be appropriate
for slow scans; however, faster scanners have little incentive
110Scanner
Pryadkin [43]
Benoit [5]
Dagon [13]
Heidemann [17]
Scope Permutation
Servers
Protocol
Port Timeout Duration Blacklist
.0/.255 Exclude
I
N R
I
I
uniform
uniform
uniform
RIS
3
25
–
8
ICMP/TCP
TCP
UDP
ICMP
–
80
53
echo
10s
30s
–
5s
123d
92d
30d
52d
yes
no
–
yes
no
yes
yes
no
no
no
US Gov
no
Table 1: Large-scale service discovery in the literature (dashes represent unreported values).
to utilize sets larger than B in the current Internet since
performance concerns (i.e., volume of sent traﬃc) usually
outweigh completeness of scan results.
2.2 Scan Order
The next factor we consider is the order in which IP ad-
dresses are scanned. This is determined by the permutation
[53] of space S, which is simply a reordering of target ad-
dresses to achieve some desired result. The chosen permu-
tation controls the burstiness of traﬃc seen by remote net-
works and is a signiﬁcant factor in both the perceived polite-
ness of scan traﬃc and estimation accuracy of Internet-wide
metrics from partial scans. For all discussion below, we as-
sume that target subnets s are full CIDR blocks (i.e., given
in the /b notation).
The most basic approach, which we call IP-sequential,
does not shuﬄe the address space and probes it in numer-
ical order (e.g., 10.0.0.1, 10.0.0.2, 10.0.0.3, etc.). It is not
only simple to implement, but also routinely used in the
Internet [2], [22], [30] and measurement studies [3], [16]. IP-
sequential targets individual subnets s with a burst of |s|
consecutive packets at the rate of n/T before moving on to
another network. Besides extremely high sending rates to s
regardless of its size (e.g., 37 Kpps for S = I and T = 24
hours), IP-sequential also suﬀers from poor extrapolation
ability.
The main alternative to IP-sequential is the uniform per-
mutation [53] also extensively used in the literature [5], [13],
[42], [43]. This approach draws targets uniformly randomly
from the full address space S and intermingles probes to
many subnets, which reduces instantaneous load on individ-
ual networks and produces unbiased random sampling dur-
ing partial scans. In the literature, the uniform permutation
is usually accomplished by either an LCG (linear congruen-
tial generator) [29] or some encryption algorithm applied
sequentially to each element of S (e.g., TEA [42]), both of
which ensure that no IP is probed twice.
The ﬁnal approach proposed in [17] we call Reverse IP-
sequential (RIS) due to its reversal of the bits in the IP-
sequential permutation and targeting the same address in
each subnet (e.g., *.*.127.10, *.*.8.10, *.*.248.10, etc.) be-
fore moving on to another address. Intuition suggests that
RIS is poorly suited for extrapolation (which we conﬁrm be-
low), while the uniform permutation fails to deliver packets
with maximum spacing to each subnet. Since no analysis ex-
ists to further evaluate the diﬀerences between these three
permutation algorithms, making an informed choice remains
an open problem.
2.3 Scan Origin
In a bid to obtain multiple vantage points [17] and de-
crease the time required to complete the scan [5], past mea-
surement studies have often distributed the burden of scan-
ning amongst several hosts. We call this process a split,
which in the literature parcels blocks of either contiguous
[3], [5] or permuted [17], [43] IP addresses to m scanning
nodes. Column four of Table 1 contains values for m used
previously, with [17] being the only study that used mul-
tiple hosts residing on two diﬀerent networks (four at each
location).
The current consensus in the literature is that multiple
scanning hosts on a single network are necessary only if the
full assigned scanning bandwidth cannot be utilized by one
host, a condition that is implied in [43] and mentioned ex-
plicitly in [3], [5], [17]. However, it is not clear from these
studies how many hosts are needed to eﬃciently utilize a
link or provide reasonably short scan durations. Further,
the literature does not consider the split’s impact on the
perceived politeness of the scan, which we tackle later in the
paper.
2.4 Extrapolation
In many research applications, especially those that mon-
itor growth of the Internet [17], [23], it is suﬃcient to obtain
the number of live hosts or estimate their characteristics
(e.g., mean uptime) rather than a list of their exact IPs.
The best approach in such cases is a partial scan, which
produces a tiny footprint at remote networks and in many
cases allows accurate extrapolation of metrics of interest.
This requires that targets within each subnet be randomly
selected, without any bias being given to certain parts of S
or particular patterns within probed IP addresses (because
the density of live hosts varies both across the Internet and
the last 1−2 bytes of the IP). In addition, non-random prob-
ing is often seen by administrators as purposefully malicious,
which in turn leads to unnecessary investigation overhead,
ﬁrewall blocks, and complaints.
2.5
Implementation
The next pressing issue of service discovery is the method
used to send/receive packets, which signiﬁcantly impacts
the eﬃciency of the scanner. The easiest implementation
method uses scripts that execute pre-written utilities [16],
[43] or existing scanners [3]. An alternative is to write a
custom scanner for a particular measurement, which opens
the possibility of using connectionless sockets [13], [17] for
ICMP or UDP-based scans, connection-oriented TCP sock-
ets [5], and ﬁnally raw IP sockets for TCP SYN scans [42],
[43]. While there is no consensus in the scanning literature
on what method to use, [14] suggests that software limi-
tations on packet sending rates can be overcome using a
network subsystem that bypasses the default network stack.
2.6 Timeouts and Duration
The next two issues are when to mark a host as unre-
sponsive and what aspects should determine scan duration
T . The former issue comes down to two choices: 1) waiting
a “safe” amount of time before retransmitting [5], [42], [43];
and 2) when to ﬁnally time out and declare targets dead
[17]. Note that both incur substantial overhead due to the
111need to remember all covered destinations and to maintain
numerous timers. Furthermore, it is unclear what beneﬁt
retransmission carries given the low packet loss on the back-
bone and whether the increased overhead (i.e., doubling or
tripling the number of sent packets) justiﬁes the potentially