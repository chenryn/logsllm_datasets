title:Distributing Trust on the Internet
author:Christian Cachin
Distributing Trust on the Internet 
Christian Cachin 
IBM Research 
Zurich Research Laboratory 
CH-8 803 Ruschl i kon, Switzerland 
PI:EMAIL 
Abstract 
This paper describes an architecture for secure and fault- 
tolerant  service  replication  in  an  asynchronous  network 
such as the Internet, where a malicious adversary may cor- 
It  relies  on 
rupt  some  servers  and  control  the  network. 
recent protocols for randomized  Byzantine  agreement and 
for atomic broadcast, which exploit concepts from threshold 
cryptography. The model and its assumptions are discussed 
in detail and compared to related work from the last decade 
in the first part of this work, and an overview of the broad- 
cast protocols in the architecture is provided.  The standard 
approach in fault-tolerant distributed systems is to assume 
that at most a certain fraction of servers fails.  In  the sec- 
ond part, novel general failure patterns and corresponding 
protocols are introduced. They allow for realistic modeling 
of real-world trust assumptions, beyond (weighted) thresh- 
old models.  Finally,  the application  of our architecture to 
trusted services is discussed. 
1  Introduction 
Distributed systems running in error-prone and adversar- 
ial  environments must  rely  on  trusted  components.  In  to- 
dayâ€™s Internet these are typically directory and authorization 
services, such as the domain name system (DNS), Kerberos, 
certification  authorities, or LDAP-based secure directories. 
Building such centralized trusted services has turned  out to 
be a valuable design principle for computer security because 
the trust in them can be leveraged to many, diverse applica- 
tions  that all benefit from centralized management.  Often, 
a trusted service is implemented as the only task of an iso- 
lated and physically  protected machine. 
Unfortunately, centralization introduces a single point of 
failure. Even worse, it is increasingly difficult to protect any 
single system against the sort of attacks proliferating on the 
Internet today.  One established  way for enhancing the fault 
tolerance  of  centralized  components  is  to  distribute  them 
among  a  set  of  servers  and  to  use  replication  algorithms 
for  masking  faulty  servers.  Thus,  no  single  server  has  to 
be trusted  completely and the overall system derives its in- 
tegrity from a majority of correct servers. 
In this paper, we describe an architecture for distributing 
trusted  services among a set of servers that guarantees live- 
ness  and safety  (or equivalently,  availability  and integrity) 
of the services despite some servers being under control of 
an attacker or failing in arbitrary  malicious ways. Our sys- 
tem  model is characterized by  a static  set of servers, com- 
pletely  asynchronous  point-to-point  communication,  and 
the  use  of  modern  cryptographic techniques.  Trusted  ap- 
plications are implemented by deterministic  state machines 
replicated  on  all  servers and  initialized  to  the  same  state. 
Client requests are delivered  by  an atomic broadcast proto- 
col that imposes a total order on all requests and guarantees 
that  the  servers perform  the  same sequence of  operations; 
such an  atomic broadcast  can  be  built  from a  randomized 
protocol  to  solve  Byzantine  agreement.  We  use  efficient 
and provably secure agreement and broadcast protocols that 
have recently  been developed. 
In  the  first  part  of  the  paper  (Section  2), we provide  a 
detailed discussion  of these assumptions, compare them  to 
related  efforts from the last decade, and argue why  we be- 
lieve that these choices are adequate for trusted applications 
in an Internet environment. 
In Section 3, a brief overview of our architecture is given. 
Our  main  tool  is  a  recent  protocol  for  atomic  broadcast, 
which builds on reliable broadcast and multi-valued Byzan- 
tine agreement. 
Of course, distributing a central service to a set of servers 
enhances its fault tolerance only if there is enough diversity 
in  that  set  such  that  common  failure  modes  can  be  ruled 
out.  For example,  if  the  same  simple  attack  succeeds for 
all servers, not much has been  gained by  distribution.  It  is 
thus crucial for this approach to make sense that the servers 
vary  in  their configuration,  operating  system, physical  lo- 
0-7695-1101-5/01 $10.00 0 2001 IEEE 
183 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:07:54 UTC from IEEE Xplore.  Restrictions apply. 
cation,  load  etc.  Placing  them  in  different  administrative 
domains also eliminates corruptible  system administrators 
as one path of attack. 
Nevertheless,  no  particular  distinction  is  made  by  the 
protocols themselves, and the traditional  assumption of this 
approach to fault-tolerant distributed systems is that at most 
a certain  fraction  of  homogeneous nodes fails.  We  gener- 
alize this model  in  the second part of this paper.  Based on 
recent progress in distributed systems and cryptography, we 
introduce in Section 4 systems that tolerate a family of novel 
failure patterns, which allow for more realistic modeling of 
real-world trust assumptions. For example, they allow a dis- 
tributed  system running at multiple  sites to continue oper- 
ating  safely  even  if  all  hosts  at one site  are unavailable or 
corrupted,  no matter  how  many  there  are.  One may  con- 
struct a distributed  system that  maintains its safety despite 
the corruption  of  a majority of  its servers in  this way.  We 
discuss two concrete instantiations of this general principle 
by  giving the corresponding linear secret sharing schemes. 
Finally, we sketch  some applications of our architecture 
in  more detail  in  Section 5:  A  certification  authority  and 
directory service, which is needed by all public-key infras- 
tructures  (PKIs), and  a  notary  and  time-stamping  service 
that acts as a secure document registry  with a logical clock. 
2  Model 
Our  system  consists  of  a  static  set  of  n  servers,  of 
which  up to  t  may  fail  in  completely  arbitrary  ways,  and 
an  unknown number of  possibly  faulty  clients.  All  parties 
are linked  by  asynchronous point-to-point  communication 
channels.  Without  loss  of  generality  we  assume  that  all 
faulty parties are controlled by a single adversary, who also 
controls the communication links and the internal clocks of 
all servers. 
Faulty  parties  are called  corrupted, the  remaining ones 
are called  honest.  Thus, any  statement about the common 
state of the system can rely  only on the honest parties, and 
they  proceed  only  to  the extent  that the  adversary delivers 
messages faithfully. In short, the network  is the adversary. 
Furthermore, there is a trusted  dealer that generates and 
distributes secret values to all servers once and for all, when 
the  system is  initialized.  The system can process a practi- 
cally unlimited  number of requests afterwards.  Sometimes, 
it is possible to bootstrap security from a PKI, e.g., to estab- 
lish  secure point-to-point  channels.  Since we  use  special- 
ized keys for which no distributed key generation protocols 
are  currently  known,  and  since  our  goal  is  to  protect  the 
heart of the PKI itself, an external mechanism is needed. 
This model falls under the impossibility result of Fischer, 
Lynch, and Paterson  [15] of  reaching consensus by  deter- 
ministic  protocols.  Many  developers of  practical  systems 
seem to have avoided this model in the past for that reason 
and have built systems that are weaker than consensus and 
Byzantine agreement.  However, Byzantine agreement can 
be solved  by  randomization in  an  expected constant  num- 
ber  of rounds only [lo, 81.  Although  the first  randomized 
agreement protocols were more of theoretical interest, their 
practical relevance has been recognized by now. 
The  recent  Byzantine  agreement  protocol  of  Cachin, 
Kursawe, and Shoup [8] is based on modern, efficient cryp- 
tographic techniques with provable security, withstands the 
maximal  possible  corruption,  and  is  also  quite  practical 
given current processor speed.  (Its security  proof  uses  the 
random oracle model, see below.) 
In  our  architecture  we  use  Byzantine  agreement  as  a 
primitive for implementing atomic broadcast, which in turn 
guarantees a total ordering of all delivered messages.  Note 
that atomic broadcast is equivalent  to Byzantine agreement 
in  our system model  [12]  and  thus  considerably more  ex- 
pensive than reliable broadcast, which only provides agree- 
ment of the delivered messages, but no ordering (Section 3). 
Below  we  elaborate  on  the  three  key  features  of  our 
model (cryptography, asynchronous communication, static 
server set) and then compare it to related efforts. 
Cryptography.  Cryptographic techniques such as public- 
key  encryption  schemes and digital  signatures  are  crucial 
already for many existing secure services.  For distributing 
trusted  services,  we also  need  distributed  variants  of  them 
from threshold cvptography. 
Threshold cryptographic schemes are non-trivial  exten- 
sions of the classical concept of secret sharing in cryptogra- 
phy. Secret sharing allows a group of n parties to share a se- 
cret such that t or fewer of them have no information about 
it, but t + 1 or more can uniquely  reconstruct it.  However, 
one cannot  simply share  the  secret  key  of  a  cryptosystem 
and reconstruct it for decrypting a message because as soon 
as a single corrupted party knows the key, the cryptosystem 
becomes completely insecure and unusable. 
A threshold public-key cryptosystem looks similar to an 
ordinary  public-key cryptosystem with  distributed  decryp- 
tion.  There is  a single public  key  for encryption,  but  each 
party  holds a key share  for decryption (all keys  were gen- 
erated by  a trusted dealer).  A party may process a decryp- 
tion request for a particular ciphertext and output a decryp- 
tion  share together with  a proof  of its  validity.  Given  a ci- 
phertext resulting from encrypting some message and more 
than t valid decryption shares for that  ciphertext,  it is easy 
to recover the message;  this property is called  robustness. 
The scheme must also be  secure  against adaptive chosen- 
ciphertext  attacks in  order to be useful  for all  conceivable 
applications. The formal security definition can be found in 
the literature [34]; essentially, it ensures that the adversary 
cannot obtain any meaningful information from a ciphertext 
unless  she has obtained  a  corresponding decryption  share 
184 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:07:54 UTC from IEEE Xplore.  Restrictions apply. 
from at least one honest party. 
In a threshold signature scheme, each party holds a share 
of  the  secret  signing  key  and  may  generate shares of  sig- 
natures on individual  messages upon request.  The validity 
of  a  signature  share  can  be  verified  for each party.  From 
t + 1 valid signature shares, one can generate a digital signa- 
ture on the message that can later be verified using the sin- 
gle, publicly  known  signature verification  key.  In a secure 
threshold signature  scheme, it  is  infeasible for a computa- 
tionally bounded adversary to produce t + 1 valid signature 
shares that  cannot be combined to a valid  signature and to 
output a valid  signature on a message for which no honest 
party generated a signature share. 
Another important cryptographic algorithm is the thresh- 
old coin-rossing scheme in the randomized Byzantine agree- 
ment protocol of Cachin, Kursawe, and Shoup [8] that pro- 
vides arbitrarily many unpredictable random bits. 
Threshold-cryptographic protocols  have  been  used  for 
secure  service  replication  before,  e.g.,  by  Reiter  and  Bir- 
man  [31].  However,  a  major  complication  for  adopting 
threshold cryptography to our asynchronous distributed sys- 
tem is that many early protocols are not robust and that most 
protocols rely  heavily on  synchronous broadcast channels. 
Only very  recently,  non-interactive  schemes have been  de- 
veloped that satisfy the appropriate notions of security, such 
as the  threshold  cryptosystem of  Shoup and Gennaro [34] 
and  the  threshold  signature  scheme of  Shoup  [33].  Both 
have  non-interactive  variants  that  integrate  well  into  our 
asynchronous  model.  However,  they  can  be  proved  se- 
cure only  in  the so-called  random oracle niodel that makes 
an  idealizing  assumption  about cryptographic  hash  func- 
tions [2]. This falls short from a proof in the real  world but 
gives very  strong heuristic evidence for their security; there 
are  many  practical  cryptographic algorithms  with  proofs 
only in this model. 
No  Timing Assumptions.  We  do not  make  any  timing 
assumptions  in  the  design  of  our protocols  and work  in  a 
completely asynchronous model.  Asynchronous protocols 
are attractive  because  in  a synchronous system, one would 
have to specify timeout values, which is very difficult when 
protecting against arbitrary failures that may be caused by a 
malicious attacker. 
It  is  usually  much  easier  for an  intruder to block  com- 
munication with  a server than  to  subvert it.  Prudent secu- 
rity  engineering also  gives the  adversary full  access  to all 
specifications, including timeouts, and excludes only cryp- 
tographic keys from her view.  Such an adversary may sim- 
ply delay the communication with  a server longer than  the 
timeout and the server appears faulty to the others. 
Time-based  failure  detectors  [12]  can  easily  be  fooled 
into  making  an  unlimited  number of  wrong  failure  suspi- 
cions about honest parties like this.  The problem arises be- 
cause one crucial assumption underlying the failure detector 
approach, namely that  the communication system is stable 
for some longer periods when  the failure detector is accu- 
rate, does not hold against a malicious adversary.  A clever 
adversary may  subvert a  server and make  it  appear work- 
ing properly until the moment at which it deviates from the 
protocol-but 
then it may be too late. Heuristic predictions 
about the future behavior of a server are pointless in security 
engineering. 
Of  course,  an  asynchronous model  cannot guarantee a 
bound  on the overall response time  of  an application.  But 
the asynchronous model can be seen  as an elegant way  to 
abstract  from  time-dependent peculiarities  of  an  environ- 
ment for proving an algorithm correct such that it satisfies 
liveness and safety under all timing conditions.  By making 