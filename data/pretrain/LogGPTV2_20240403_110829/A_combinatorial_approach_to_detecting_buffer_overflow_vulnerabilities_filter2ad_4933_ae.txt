that 
In  particular,  we  discuss  testing  based  approaches,  i.e., 
approaches that involve actual program executions. We will 
not discuss approaches that are based on pure static analysis 
[18][22][30][40], as they involve quite different techniques. 
As  mentioned  in  Section  I,  static  analysis  suffers  from  the 
problem of false positives and/or negatives. 
We first discuss black-box testing approaches, which are 
the most closely related to our work. Fuzzing [35] is among 
the  most  widely  used  black-box  testing  approaches  in 
security  testing.  Fuzzing  typically  starts  from  one  or  more 
legal  inputs,  and  then  randomly  mutates  these  inputs  to 
derive  new 
techniques 
[14][15]  can  also  incorporate  domain  knowledge  and/or 
employ  heuristics,  e.g.,  assigning  different  weights  to 
different components.  
inputs.  Advanced  fuzzing 
As  mentioned  in  Section I,  the poor code  coverage  is a 
major  limitation  of  fuzzing  [35].  In  contrast,  our  approach 
samples the input space in a systematic manner to achieve a 
combinatorial coverage. Empirical results suggest that there 
exists a high correlation between combinatorial coverage and 
code coverage [3][11]. Our approach  is,  however,  not  fully 
automated.  In  particular,  attack-payload  and  attack-control 
parameters, as well as their values, are identified manually in 
our  approach.  Our  empirical  studies  show 
this 
identification  can  be  performed  with  reasonable  effort. 
Moreover, we believe that this provides an opportunity for us 
to take advantage of domain knowledge that may be readily 
available  in  practice.  We  point  out  that  many  black-box 
testing  techniques,  including  combinatorial  testing  in  its 
original form, require individual parameters and values to be 
identified manually. More discussion on this manual aspect 
of our approach is provided in the next section.   
together  with 
Recently there has been a growing amount of interest in 
approaches  that  combine  symbolic  execution  and  testing 
[5][13][41]. In these approaches, symbolic execution is used 
to  collect  path  conditions  consisting  of  a  sequence  of 
branching  decisions.  These  branching  decisions  are  then 
negated  systematically  to  derive  test  inputs  that  when 
executed,  will  explore  different  paths.  In  order  to  detect 
buffer  overflow  vulnerabilities,  memory  safety  constraints 
are  formulated  and  solved 
these  path 
conditions.  A  potential  problem  with  these  approaches  is 
path explosion. Techniques based on functional summaries, 
generational  search  and 
length  abstraction  have  been 
developed  to  alleviate  this  problem.  These  approaches 
generate  tests  in  a  fully  automatic  manner.  However, 
symbolic execution often involves extensive instrumentation, 
either  at  the  source  or  binary  level.  Thus,  the  resulting 
solutions are usually specific to a particular language, build 
environment,  or platform.  Symbolic executions can also be 
much  slower 
than  actual  program  executions.  More 
importantly, for large and/or complex programs, the number 
of  constraints  that  have  to  be  solved  presents  significant 
challenges to the capacity of existing constraint solvers. 
VI.  CONCLUSIONS AND FUTURE WORK 
In this paper, we presented a black-box testing approach 
to  detecting  buffer  overflow  vulnerabilities.  Our  approach 
simulates  the  process  an  attacker  typically  performs  to 
the 
fact 
exploit a buffer overflow vulnerability. A novel aspect of our 
approach is that it adapts a general software testing technique 
called combinatorial testing to the domain of security testing. 
In  particular,  our  approach  exploits 
that 
combinatorial  testing  often  achieves  a  high  level  of  code 
coverage. We implemented our approach in a prototype tool 
called  Tance.  Empirical  results  of  applying  Tance  to  five 
open source programs show that our approach is effective in 
detecting buffer overflow vulnerabilities in these programs. 
In  our  approach,  attack-payload  and  attack-control 
parameters and their values are identified manually based on 
specification  or  domain  knowledge  or  both.  We  provide 
guidelines for performing such identification. Our empirical 
studies show that these guidelines are very effective and can 
be followed with reasonable effort. Security testing is often 
performed  after  functional  testing.  Thus,  knowledge  and 
experience obtained from functional testing can be utilized to 
effectively  identify  these  parameters  and  values.  While  our 
approach is most effective when these parameters and values 
are identified properly, this identification does not have to be 
perfect. In practice, we can exploit this flexibility to scale our 
test  effort  up  or  down,  depending  on  the  availability  of 
resources.  That  is,  we  can  intentionally  identify  more 
parameters and values to acquire more confidence at the cost 
of  more  tests.  Or  we  can  intentionally  identify  fewer 
parameters  and  values  to  reduce  test  effort  at  the  cost  of 
missing some vulnerabilities.  
While  fully  automated  solutions  are  often  desirable,  we 
believe  semi-automated  solutions  like  ours  also  have  their 
merits.  In  particular,  our  approach  allows  us  to  take 
advantage  of  domain  knowledge  that  may  be  readily 
available in practice. An effective use of domain knowledge 
can often make the testing process more efficient, and may 
discover  bugs 
that  cannot  be  discovered  otherwise. 
Moreover, fully automated and semi-automated solutions can 
be used in such a way that they complement each other. For 
example, we can first apply fuzzing and then our approach to 
achieve  higher  fault  coverage.  As  mentioned  earlier,  our 
approach  allows  test  effort  to  be  scaled  up  or  down, 
depending  on  the  availability  of  resources.  This  flexibility 
further  facilitates  the  use  of  our  approach  in  combination 
with other approaches.  
We plan to develop lightweight static analysis techniques 
to  automatically  identify  attack-payload  and  attack-control 
parameters and their values. These techniques can be applied 
when  source  code  is  available.  With  these  techniques,  we 
will  be  able  to  fully  automate  our  test  generation  process. 
This will enable a direct comparison between our approach 
and  existing  approaches  that  combine  symbolic  execution 
and testing. Such a comparison will help to further evaluate 
the effectiveness of our approach.  
ACKNOWLEDGMENT 
This  work  is  partly  supported  by  a  grant  (Award  No. 
70NANB10H168) from the Information Technology Lab (ITL) 
of National Institute of Standards and Technology (NIST). 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:29:42 UTC from IEEE Xplore.  Restrictions apply. 
277REFERENCES 
[1] 
[2] 
 D.  Aitel,  “The  Advantages  of  Block-based  Protocol  Analysis  for 
Security  Testing”,  Immunity  Inc,  2002.  DOI=  http://www.net-
security.org/article.php?id=378. 
J.  H.  Andrews,  L.C.  Briand,  Y.  Labiche  and  A.S.  Namin,  “Using 
Mutation  Analysis  for  Assessing  and  Comparing  Testing  Coverage 
Criteria”,  IEEE  Transactions  on  Software  Engineering,  32(8):  608-
624, 2006. 
[3]  K.  Burr,  and  W.  Young,  “Combinatorial  Test  Techniques:  Table-
based  Automation,  Test  Generation  and  Code  Coverage”, 
Proceedings  of  the  International  Conference  on  Software  Testing 
Analysis and Review, pp. 503-513, 1998. 
[4]  R.  Bryce,  C.  J.  Colbourn,  M.B.  Cohen,  “A  framework  of  greedy 
methods  for  constructing  interaction  tests,”  Proceedngs  of  the  27th 
International  Conference  on  Software  Engineering  (ICSE),  pp. 146-
155, 2005.   
[5]  C. Cadar,  V.  Ganesh,  P.M.  Pawlowski,  D.L.  Dill,  and  D.R. Engler, 
“EXE:  Automatically  Generating  Inputs  of  Death”,  Proceedings  of 
the  13th  ACM  Conference  on  Computer  and  Communications 
Security (CCS), pp. 322-335, 2006. 
[6]  B.  Chess,  and  G.  McGraw,  “Static  Analysis  for  Security”,  IEEE 
Security and Privacy, 2(6):76-79, 2004.  
[7]  M.  Cohen, S.  R.  Dalal, M.  L.  Fredman, and G. C. Patton, “The 
AETG  System:  An  Approach  to  Testing  Based  on  Combinatorial 
Design”,  IEEE  Transactions  on  Software  Engineering,  23(7):  437-
444, 1997. 
[8]  M. B. Cohen, M. B. Dwyer, and J. Shi, “Constructing interaction test 
suites for highly-configurable systems in the presence of constraints: 
a  greedy  approach”,  IEEE  Transactions  on  Software  Engineering, 
34(5), pp. 633-650, 2008. 
[9]  C.  Cowan,  C.  Pu,  D.  Maier,  H.  Hintony,  J.  Walpole,  P.  Bakke,  S. 
Beattie,  A.  Grier,  and  Q.  Zhang,  “StackGuard:  automatic  adaptive 
detection and prevention of buffer-overflow attacks”, Proceedings of 
the 7th conference on USENIX Security Symposium, pp. 5-5, 1998. 
[10]  R.  Dhurjati  and  V.  Adve,  “Backwards-compatible  Array  Bounds 
Checking for C with Very Low Overhead”, Proceedings of the 28th 
IEEE  International  Conference  on  Software  Engineering,  pp.  162-
171, 2006.  
[11]  S.  Dunietz,  W.  K.  Ehrlich,  B.  D.  Szablak,  C.  L.  Mallows,  and  A. 
Iannino,  “Applying  design  of  experiments  to  software  testing”, 
Proceedings  of 
International  Conference  on  Software 
Engineering, pp. 205–215, 1997. 
IEEE 
[12]  Ghttpd-1.4.4. DOI= http://gaztek.sourceforge.net/ghttpd/. 
[13]  P. Godefroid, N. Klarlund, and K. Sen, “DART: Directed Automated 
Random  Testing”,  Proceedings  of 
the  2005  ACM  SIGPAN 
Conference  on  Programming  Language  Design  and  Implementation 
(PLDI), pp. 213-233, 2005.  
[14]  P.  Godefroid,  A.  Kiezun,  and  M.Y.  Levin,  “Grammar-based 
Whitebox  Fuzzing”,  Proceedings  of  the  ACM  SIGPLAN  2008 
Conference  on  Programming  Language  Design  and  Implementation 
(PLDI), pp. 206-215, 2008. 
[15]  P. Godefroid, M. Levin, and D. Monlnar, “Automated Whitebox Fuzz 
Testing”,  Proceedings  of  the  Network  and  Distributed  Security 
Symposium, 2008.  
[16]  M.  Grindal,  J.  Offutt,  and  S.F.  Andler,  “Combination  Testing 
Strategies: A Survey”, Software Testing, Verification and Reliability, 
15(3): 167-199, 2005. 
[17]  Gzip-1.2.4. DOI= http://www.gzip.org/. 
[18]  B.  Hackett,  M.  Das,  D.  Wang,  Z.  Yang,  “Modular  Checking  for 
Buffer Overflows in the Large”, Proceedings of the 28th International 
Conference on Software Engineering, pp. 232-241, 2006. 
[19]  Hypermail-2.1.3. DOI= http://www.hypermail.org/. 
[20]  R.  Kuhn,  D.R.  Wallace,  and  A.M.  Gallo  Jr,  “Software  Fault 
IEEE 
Interactions  and 
for  Software  Testing”, 
Transactions on Software Engineering, 30(6):418-421, 2004. 
Implications 
[21]  R.  Kunh  and  C.  Johnson,  “Vulnerability  Trends:  Measuring 
progress”, IEEE IT Professional, 12(4):51-53, 2010.  
[22]  W. Le, M. L. Soffa, “Marple: a Demand-Driven Path-Sensitive Buffer 
Overflow  Detector”,  Proceedings  of  the  16th  ACM  SIGSOFT 
International  Symposium  on  Foundations  of  Software  Engineering, 
pp. 272-283, 2008. 
[23]  Y.  Lei,  R.  Carver,  R.  Kacker,  D.  Kung,  “A  Combinatorial  Strategy 
for  Testing  Concurrent  Programs”,  Journal  of  Software  Testing, 
Verification, and Reliability, 17(4):207-225, 2007. 
[24]  Y.  Lei,  R.  Kacker,  R.D.  Kuhn,  V.  Okun,  and  J.  Lawrence, 
“IPOG/IPOD: Efficient Test Generation for Multi-way Combinatorial 
Testing”,  Software  Testing,  Verification  and  Reliability,  18(3):287-
297, 2007. 
[25]  A.  Mathur,  “Foundations  of  Software  Testing”,  Addison-Wesley 
Professional, 2008. 
[26]  G. McGraw, “Software Security”, IEEE Security & Privacy, 2(2): 80-
83, 2004. 
[27]  National Vulnerability Database. DOI= http://nvd.nist.gov/. 
[28]  Nullhttpd-0.5.0. DOI= http://www.nulllogic.ca/httpd/. 
[29]  Pine-3.96. DOI= http://www.washington.edu/pine/.  
[30]  M. Pistoia, S. Chandra, S.J. Fink, and E. Yahav, “A Survey of Static 
Analysis Methods for Identifying Security Vulnerabilities in Software 
Systems”, IBM Systems Journal, 46(2):265-288, 2007. 
[31]  J.  Roning,  M.  Laakso,  A.  Takanen  and  R.  Kaksonen,  “PROTOS  –
Systematic  Approach  to  Eliminate  Software  Vulnerabilities”.  DOI= 
http://www.ee.oulu.fi/research/ouspg/.  
[32]  SecurityFocus. DOI= http://www.securityfocus.com/. 
[33]  SecurityTracker. DOI= http://www.securitytracker.com/. 
[34]  E. C. Sezer, P. Ning, C. Kil and J. Xu, “Memsherlock: An Automated 
Debugger 
for  Unknown  Memory  Corruption  Vulnerabilities”, 
Proceedings  of  the  14th  ACM  Conference  on  Computer  and 
Communications Security (CCS), pp. 562-572, 2007. 
[35]  M.  Sutton,  A.  Greene,  and  P.  Amini,  “Fuzzing:  Brute  Force 
Vulnerability Discovery”, Addison-Wesley, 2007.  
[36]  W. Wang, S. Sampath, Y. Lei, and R. Kacker, "An Interaction-Based 
Test Sequence Generation Approach for Testing Web Applications", 
Proceedings of the 11th IEEE High Assurance Systems Engineering 
Symposium, pp. 209-218, 2008.  
[37]  W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, J. Lawrence, "A 
Combinatorial Approach to Building Navigation Graphs for Dynamic 
Web  Applications",  Proceedings  of  25th 
International 
Conference on Software Maintenance, pp. 211-220, 2009. 
IEEE 
[38]  W. Wang, and D. Zhang, “External Parameter Identification Report”. 
University 
DOI= 
https://wiki.uta.edu/pages/viewpageattachments.action?pageId=35291
531. 
Arlington. 
of 
Texas 
at 
[39]  J. Wilander, and  M. Kamkar, "A Comparison of Publicly Available 
Tools for Dynamic Buffer Overflow Prevention", Proceedings of the 
10th Network and Distributed System Security Symposium, pp. 149-
162, 2003. 
[40]  Y. Xie, A. Chou, and D. Engler, “ARCHER: Using Symbolic, Path-
sensitive Analysis to Detect Memory Access Errors”, Proceedings of 
11th  ACM  SIGSOFT  International  Symposium  on  Foundations  of 
Software Engineering, pp. 327-336, 2003. 
[41]  R.  Xu,  P.  Godefroid,  R.  Majumdar,  “Testing  for  Buffer  overflows 
with  length  Abstraction”,  Proceedings  of  the  2008  International 
Symposium on Software Testing and Analysis, pp. 27-38, 2008. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:29:42 UTC from IEEE Xplore.  Restrictions apply. 
278