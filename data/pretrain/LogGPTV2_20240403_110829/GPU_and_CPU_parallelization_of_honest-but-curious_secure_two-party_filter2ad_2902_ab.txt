πj
j
,W
(cid:2)
Encg
W
1⊕πi
i
1⊕πj
j
,W
W g(πi,πj )
k
(cid:2)
k
W g(πi,1⊕πj )
W g(1⊕πi,πj )
k
(cid:2)
(cid:3)
(cid:3)
k
(cid:5)λg(πi,1⊕πj )
(cid:5)λg(1⊕πi,πj )
k
W g(1⊕πi,1⊕πj )
k
(cid:5)λg(1⊕πi,1⊕πj )
k
(cid:3)
,
We (brieﬂy) review Yao’s garbled-circuit approach to secure com-
putation [25], and the respective security models of interest: honest-
but-curious (HbC), malicious, and one-bit leaked malicious (1BM).
Garbled circuits provide a way for two parties, holding inputs
x and y respectively, to compute an arbitrary function f of their
where Encg
text m using two keys W, W
encryption as
W,W (cid:3) (m) denotes symmetric-key encryption of plain-
. In our implementation, we deﬁne
(cid:4)
Encg
W,W (cid:3) (m) = H(g(cid:5)W(cid:5)W
(cid:4)) ⊕ m,
171
where H represents a cryptographic hash function (SHA1) whose
output is truncated to the length of the given plaintext. The set of
all the garbled gates constitutes the garbled circuit that is sent to the
evaluator. In addition, the circuit generator sends the permutation
bit πi for any output wire i of the circuit.
along with the label λbi
To evaluate the garbled circuit, the circuit evaluator must obtain
keys for each input wire of the circuit; speciﬁcally, if the input bit
on some input wire i is bi, then the evaluator should be given the
key W bi
i . (Furthermore, for each input
i
wire it should get only that key.) For input wires that correspond to
the input of the circuit generator, x, this can easily be arranged by
simply having the generator send the appropriate key/label for each
such wire. The parties can use oblivious transfer [6] to allow the
evaluator to obliviously learn the appropriate keys/labels for input
wires that correspond to its own input, y.
Given keys/labels Wi, λi and Wj, λj associated with the input
wires i, j of some gate g, the evaluator can compute a key/label for
the output wire of that gate by decrypting the ciphertext in position
(λi, λj) of the garbled table for g. Proceeding in this way through
the entire (garbled) circuit in topological order, the evaluator can
compute keys/labels for each output wire of the circuit. Using the
permutation bits sent to him by the circuit generator, this means
that the evaluator can determine the actual bit output of the circuit.
If speciﬁed as part of the protocol, the evaluator can send that result
back to the other party.
The crucial point for our purposes is that generation of all the
garbled gates can be done in parallel once the wire keys and per-
mutation bits have been chosen. Further details, along with a proof
of security, can be found in [17].
Malicious Security Model: Cut-and-Choose.
There are several known ways to modify Yao’s protocol to the
malicious model, but the only one that has been implemented and
deemed practical is the cut-and-choose approach. Here the circuit
generator creates not one ‘encrypted’ circuit, but ≈ k ‘encrypted’
copies of the circuit (where k is a security parameter). The gen-
erator sends the k encrypted circuits (without the encryption of its
input to the generator). The evaluator now chooses ≈ 60% of the
circuits to be revealed, at which point the generators gives all the
information necessary to generate the circuit to the evaluator who
then veriﬁes that all the circuits are legitimate implementations of
the correct circuit for f. If not, the evaluator quits. If so, the evalu-
ator asks for the “encrypted” inputs to the remaining ≈ 40% of the
circuits, and computes the output. The evaluator takes as its output
the majority output of the many evaluated circuits. The argument
for the security of this protocols is beyond the scope of this paper
but can be found in many places (cf. [24]).
One Bit Leaked Malicious Model.
In this model, the protocol is modiﬁed so that each party plays
both the role of generator and evaluator. Each party generates a
circuit and sends it to the other, which in turn evaluates it. After
this is done, a specialized protocol that is secure in the traditional
malicious model does a secure function evaluation to ensure that
the outputs of both evaluated circuits are the same. A specialized
and efﬁcient protocol (both in computation and communication) for
verifying the equality of outputs in the malicious security model is
given by Huang et al. [9].
4. GPU COMPUTING AND CUDA
Modern GPUs are massively parallel computational devices, but
differ from modern multi-core CPUs in signiﬁcant aspects. In this
section we provide a brief overview of their architecture. Com-
munication to and from the GPU occurs over the system PCI bus,
which is substantially slower than the regular communication path
between RAM and the CPU on a modern machine.
Anatomy of a CUDA GPU.
The smallest execution unit on a CUDA1 GPU is called a stream-
ing processor (SP), or CUDA core, which is capable of executing
an independent thread. These cores are not equivalent to CPU cores
but more equivalent to lanes on a vector processor. An SP has ac-
cess to local memory and registers. Multiple SPs are combined to
construct one Streaming Multiprocessor (SM). The number of SPs
located in an SM, and complexity of the SM depend upon the GPU
hardware generation. Every NVIDIA GPU has multiple SMs.
SMs are in charge of scheduling work to their SPs. SM’s re-
ceive work in the form of thread blocks. Thread blocks can contain
a number of threads deﬁned by the programmer at run time. The
SM then splits these thread blocks into groups of 32 threads called
warps. Each warp is run on a set of 32 SPs. Each thread in a warp
executes one common instruction at a time thus warps are akin to
32-wide vector processors. To ensure every thread is executing the
exact same instruction, each thread in a warp must execute the exact
same branch in program ﬂow. If different threads need to run dif-
ferent branches, the GPU serializes them by having the appropriate
threads execute the branch, while non-branching threads’ proces-
sors sit idle. When such a divergence in execution occurs between
threads in a warp it is termed warp divergence. To get full efﬁciency
from the GPU it is essential that programs be written so that they
can be broken down into warps where all threads execute the same
instruction, and there is little to no conditional branching that does
not affect all the threads in the same manner. This is the Single
Instruction Multiple Data (SIMD) paradigm for parallel program-
ming, where the same instruction is applied to multiple pieces of
data at a time.
Also note that a given SM might be concurrently executing mul-
tiple warps via “hyper-threading”. If one warp is waiting on mem-
ory access or some other condition, the SM may start to execute
another warp. There is little to no cost in time for this context
switching, however it does mean that local resources such as regis-
ters and local memory need to be shared between these warps.
Relative Speeds of Memory and CPU-GPU bandwidth.
GPUs must deal with latency issues caused by transferring data
from the host machine to the GPU and vice-versa. Transfer rates
between the host and GPU are ≈ 8GB/s over the PCI-E bus when
the GPU is on a PCIe card. The transfer rates are orders of magni-
tude slower than the GPU’s theoretical compute throughput. Mem-
ory transfer on-board the GPU is several orders of magnitude faster
than the bus (e.g., global memory on a Tesla card transfers at 177.6
GB/s). Therefore, high performance requires minimizing memory
transfers and the communication between the GPU and CPU, and
maximizing the local computation performed on the GPU. Differ-
ing types of memory on the GPU, including global, shared, local
(L1, L2 cache), and registers, also operate at varying speeds and
thus create a memory hierarchy on the GPU mirroring that on a typ-
ical machine. Kernels must optimize the use of local registers and
shared memory while dealing with the extremely limited resources
of each. Register dependencies, such as when a read directly fol-
lows a write to the same register, can also increase latency. Thus
1By CUDA (Compute Uniﬁed Device Architecture) we mean an
NVIDIA GPU that supports NVIDIA’s CUDA programming envi-
ronment. The most commonly developed for GPU.
172
we want to maximize register usage to increase speed, however
any particular thread also wants to minimize usage so that a SM
can “hyper-thread” multiple warps, if any of them are latent due
to memory fetches or other reasons. These conﬂicting goals make
register usage a complicated trade-off in GPU programming.
5. MULTI-CORE CPU VS CLUSTERS
For our CPU based circuit evaluation system, we use local par-
allelism to take advantage of the multi-core environment found on
modern day CPUs. This is in contrast to the different, if not compli-
mentary, approach taken by Kreuter et al. [15], who take advantage
of the parallelism available on multi-node compute clusters. We
discuss the different technological approaches next.
MPI and OpenMP.
OpenMP and MPI (Message Passing Interface) are both com-
peting and complementary standards for parallelization in High
Performance Computing (HPC). OpenMPI is currently a dominant
MPI implementation but is not related to OpenMP beyond being a
parallelization technology.
MPI is a message passing technology that enables “scale-out”
parallelism on multi-device compute clusters, such as super com-
puting clusters. The developer deﬁnes an MPI process that is launched
many times on many different compute nodes. These MPI pro-
cesses are able to pass messages between one another when they
need to share computation. It works best for large jobs on large sys-
tems, as each MPI process incurs large overhead. MPI requires that
any machines running MPI code have an MPI implementation’s ex-
ecutables installed, for example, Open MPI’s libraries and executa-
bles.2 OpenMPI is the technology used by Kreuter et al.[15].
In contrast, OpenMP is an HPC technology designed for “scale-
up” parallelism on a single machine. It is a standard that is built
in to compilers such as GCC and includes a small driver library.
Developers use OpenMP to easily create many lightweight threads
with minimal syntax compared to traditional POSIX thread imple-
mentations. Unlike POSIX threads, OpenMP is optimized for a
data parallel programming paradigm and not a task parallel pro-
gramming paradigm. Data parallelization is akin to the SIMD (Sin-
gle Instruction Multiple Data) paradigm and task parallelization is
akin to the MIMD (Multiple Instruction Multiple Data) paradigm.
OpenMP is the technology we use for our parallel multi-core CPU
evaluation scheme.
6. SECURITY MODEL INDUCED ARCHI-
TECTURE TRADE-OFFS
While GPUs gain with massive parallelism, they lose in terms of
algorithmic ﬂexibility. Programmers must specify the logical allo-
cation of their threads in terms of thread blocks, and these thread
blocks affect physical GPU allocation. If this logical allocation is
poor (e.g., setting thread blocks to have one thread) then poor per-
formance follows, as many cores in a SM sit idle while a single
core computes the thread. We compare the architectural approach
of Frederiksen and Nielsen [5] and Kreuter at al. [15] given their
malicious model implementations, and our approach in the HbC
and 1BM security models. Recall that in the cut-and-choose ma-
licious implementation the generator must generate ≈ k circuits
while the evaluator will evaluate some 40% of those circuits, and
verify the remaining 60% to ensure they were properly constructed.
In the HbC case the generator must generate only one circuit and
the evaluator must evaluate only one circuit. In the 1BM case, each
party must generate and evaluate one circuit.
2http://www.open-mpi.org/software/ompi/v1.6/
173
Similarity between HbC and 1BM.
Implementation details between HbC and 1BM protocols are
generally identical as the resources they need are very similar. The
1BM protocol differs in only requiring one more circuit genera-
tion and one more circuit evaluation than that of the HbC protocol.
Therefore, we address both protocols in our discussion as if they
were the same model.
Communication Differences.
One immediate observation is that in the malicious model, rea-
sonable values of k might vary between 60 and 120, and thus the
number of circuits that need to be transferred between the two
agents in the protocol 40% of this, 3 compared to the one or two
circuits that need to be transmitted our protocols. Frederiksen and
Nielsen show that in their protocol with varying security parame-
ters, that communication costs dominate, often by a factor of 3 to 4
times the generation or evaluation times. This is not by enough that
one should expect them to dominate in the HbC or 1BM scenario
we implement. Further, recent advances in the cut-and-choose method-
ology by Lindell [16] and optimizations that Frederiksen and Nielsen
did not implement [24], further reduce the communication burden.4
Finally, Frederiksen and Nielsen also increase the circuit-size to
include a universal hash of the inputs, and there are alternate ap-
proaches that will not increase the circuit size which can be con-
sidered. Therefore, we can consider optimizations that disallow
the garbled row-reduction methodology, and also slightly increase
communication for the sake of efﬁciency.
Malicious Cut-and-Choose and the GPU.
Cut-and-choose protocols must generate k circuits at a time. In-
stead of having each thread represent a gate, in cut-and-choose each
thread represents one of the k circuits and the thread block is used
to represent an individual gate. Thus, each thread block contains k
threads and each thread deals with the generation of a speciﬁc gate
for each of the k circuits. One can then allocate the same num-
ber of thread blocks as there are gates in the circuit to the GPU.
As each thread block will always have threads processing the same
gate type, there is no fear of warp divergence. The only caveat is
the thread block size, and thus the cut-and-choose security param-
eter, should be a multiple of 32 for optimal GPU allocation (again,
the SMs allocate 32 threads at a time). Levels are evaluated in turn,
but this is less problematic to performance, circuit widths are effec-
tively multiplied by k, meaning the GPU spends little relative time
idle waiting for a level to complete before the next is started. Eval-
uation occurs in a similar manner, but must use the level-by-level
process that was used in semi-honest evaluation. For each level
there will be a thread block for each gate in that circuit’s level and
each thread block will contain k threads. We note this is exactly
the approach taken by Frederiksen and Nielsen [5].
Honest-but-Curious and One-Bit Malicious.
The previous description of a successful approach to placing cut-
and-choose on the GPU should make it clear why the same ap-
proach is inefﬁcient for the semi-honest case (and similarly the
1BM case). If only one circuit is being generated or evaluated, each
thread block will contain only one thread, and only one thread will
be allocated by the SMs on the GPU for each gate leaving 31/32
3It is known that only the circuits that are being evaluated need
to be transferred as noted in [7], as it sufﬁces to communicate a
collision resistant hash of the veriﬁed circuits.
4In practice Frederiksen and Nielsen evaluate 50% of the circuits.
SPs in a warp dormant (meaning the vast majority of GPU cores
are consistently going unused).
We describe our approach to implementing HbC and 1BM on
the GPU. For simplicity we will assume the circuit we are generat-
ing and evaluating ﬁts in GPU memory, although our approach is
not limited in this fashion. As we are only concerned with a single
circuit, we will pass the whole circuit description to the GPU for
generation and evaluation. In the case of generation, each thread
represents a single gate in the circuit and the number of threads al-
located are the number of gates in the circuit. The size of a thread
block does not matter for the HbC or 1BM case, although for efﬁ-
ciency it should be a multiple of 32 (the physical thread allocation
count by the SM). We can handle XOR gates with one kernel and
all other truth table gates with another kernel. The latter essen-
tially always make a blank truth table, and the converts it to the
appropriate type by changing each line of the table to describe the
appropriate operator. However, to construct a truth table gate one
needs to have knowledge of its input wires’ labels. This too can
seemingly be solved because we can pseudo-randomly generate a
wire’s labels based on the wire’s identiﬁer, but this conﬂicts with
the Free-XOR technique (as described in the next section). We
modify the Free-XOR technique at the cost of a small amount of
extra communication, and allow all gates in the circuit to be gener-
ated in parallel, independent of the level on which they reside.
Evaluation needs to occur on a level-by-level basis to honor data
dependencies between gates. Again,we would have two kernels for
evaluating the two types of gates, truth table and XOR. Consider
what happens when evaluating the end of a level: it is likely that
many symmetric processors will sit idle waiting for the level to
be completed, as there are no more gates on the current level, say
i − 1, to evaluate, but they cannot commence processing the level
i gates until the last few gates on level i − 1 are evaluated due to
potential dependency issues. The narrower a level is, the larger
the inefﬁciency if many of the GPU’s cores need to lay latent why
completing the level. Logic to check for dependencies is likely
to cause divergence, and latency problems. Therefore, for circuits
which are not wide, the relative amount of time that is spent by the
cores being idle at the end of a level can be quite large. In the cut-
and-choose approach, the fact that there are k copies of each circuit
has the effect of essentially multiplying the width of each circuit by
k, making the issue far less problematic. Of course, for particularly
large and wide circuits, this should not cause much of an issue for
the HbC or 1BM implementations.
7. ARCHITECTURE & METHODOLOGY
Several optimizations of Yao’s garbled-circuit protocol have been
proposed, but it is not clear how all of them can be efﬁciently imple-
mented in a massively parallel system. Here we discuss the major
techniques and our approach to implementing them. As a start-
ing point, we implement the folklore “single row evaluation" tech-
nique already described in the description of the Yao protocol in
Section 3. This optimization, created by [18], decreases evaluation
time on encrypted gates by roughly a factor of 4.
On the other hand, one popular technique for reducing the size
of garbled tables by 1/4, called Garbled-row reduction [21], is not
implemented as any such implementation would seem to slow exe-
cution on a SIMD parallelization.5 The beneﬁt of this approach is
5There are two issues: i) wires in level i + 1 of the circuit will now
depend on the gates in level i, making parallel generation of the
circuit difﬁcult; and ii) during evaluation, about a 1/4 of the cores
evaluating encrypted gates would evaluate to the missing row, and
require different code than is required for the remaining 3/4 of the
cores (causing warp divergence).
174
that it reduces communication by 25%, but as discussed our secu-
rity models prompts us to be less concerned about communication
costs and more about gate generation and evaluation timings.
7.1 Selection of the Random-Oracle/Permutation
Function
The Yao-garbled circuit technique relies on symmetric encryp-
tion. In most modern implementations the symmetric encryption is
provided via a random oracle instantiated by a cryptographic hash
(SHA1). Recent work by Bellare et al. [1] has also considered the
use of a Random Permutation that can be instated with ﬁxed key in
a block-cipher such as AES. In our implementation, we choose the
SHA1 function for this purpose. Jang et al. [12] showed that AES
had substantially slower throughput than SHA1 on GPU architec-
tures. We have not had the chance to consider an optimized version
of AES with a ﬁxed key, as suggested in [1], and this should be
investigated. We experimentally tested BLAKE256, a SHA3 ﬁnal-
ist, which also had a slower throughput on the GPU than SHA1,