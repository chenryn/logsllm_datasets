size of each spun cluster in the GoArticles data set.
Time difference between wiki and GoArticles post
Fig. 17.
times in
overlapping clusters with one page from GoArticles and the remaining pages
from the wiki set.
one). Furthermore, the low number of pages from GoArticles
in these clusters is independent of the number of wiki pages
in the clusters.
Timing is another indication. If spammers are taking ar-
ticles from article directories and using them as seeds for
spinning, the posting times from the article directory should
precede the times on the wiki pages; otherwise, the wiki and
article directory post times will often be uncorrelated with each
other. Figure 17 explores the posting times in detail. For each
cluster containing one GoArticles page and at least one wiki
page on the x-axis, it shows two points on the y-axis connected
by a line. The bottom point shows the earliest posting time of
a wiki page in the cluster relative to the posting time of the
GoArticles page, and the top point similarly shows the relative
posting time of the latest posted wiki page.
Most of the wiki pages (92%) are posted well after the
GoArticles page, and often by a very large gap of weeks to
months. In these cases, one conclusion is that the spammers
responsible for this crossover behavior are taking articles from
article directories, spinning them, and posting them on wiki
pages. There are a small number of exceptions, though. The
clusters with negative y values are when the post time in
GoArticles precedes the wiki pages. For these, we conclude
that the spammers used seed articles from somewhere else,
and GoArticles was simply another target site for posting spun
content.
VII. DISCUSSION
Automated spinning is directly a response to defensive
pressure: near-duplicate document detection by services like
Google [4], [32]. As shown in Section IV, spinning under-
mines shingle-based approaches and plagiarism detectors like
CopyScape. If services deploy defenses like DSpin against
automated spinning, spammers will similarly respond to such
pressure to evade detection. Likely responses fall into changing
how the tool spins content, where it spins the content, or how
much of the content it spins.
One spammer response might be to change the dictionary
frequently. As noted in Section IV-B, TBS does not currently
change its synonym dictionary much over time: 94% of the
Fig. 16. Composition of clusters containing pages from both the wiki and
GoArticles data sets.
that spammers use GoArticles as seed pages for spinning, and
then post the spun content on wiki pages. We explore this
question by examining the crossover of the two data sets.
We combine the month crawl of the wiki data set with the
GoArticles data set. If these two data sets are disjoint, then
every cluster of spun content would only contain articles from
one data set. In this case, the spun articles from the wiki and
GoArticles sites likely represent separate spam campaigns. On
the other hand, if spun clusters cross data set boundaries, this
overlap would indicate a spam campaign observed across both
data sets. If clusters do exhibit this behavior, one explanation
is that spammers share the same pool of spun articles and
post spun content ubiquitously across domains. Alternatively,
spammers may take content from article directories, and use
it to seed generation of spun content.
We look at two metrics in the overlap, the number of pages
from GoArticles and wiki in each cluster, and the times at
which the GoArticles and wiki pages are posted to the sites.
We found 229 clusters that contain both GoArticles and
wiki pages. Figure 16 plots the number of GoArticles (x-axis)
and wiki (y-axis) pages that occur for each cluster. The trend
indicates that the majority of cross domain clusters contain
many wiki pages (31.6 on average), compared with just 1.2
on average for GoArticles (82% of these clusters contain just
14
words in the dictionary remained the same over four months.
If it were to change the dictionary more frequently, it would
have to change the set of words it spun (the mutable set) and/or
the set of synonyms used for replacement. Since these changes
would likely result in a smaller dictionary at any particular
time, it might reduce the quality of spun content — although
likely still sufﬁcient for the goals of posting spun articles for
link spam. DSpin is insensitive to some degree of dictionary
variation since it does not have to be precise; it is encouraging
that DSpin could identify automatically spun content in the
wild that was generated over a year and presumably by tools
other than TBS. However, if DSpin did need to precisely track
frequent changes to the synonym dictionary over time, then the
resulting burden would be the ease or difﬁculty of obtaining
updated dictionaries.
Currently tools like TBS download the dictionary locally to
produce spun content, making the dictionary vulnerable to au-
tomated download and analysis. Instead, tools could compute
spun content remotely so that clients do not have access to
the dictionary. Remote computation will increase cost, but the
availability of cheap compromised hosts [8] will unlikely un-
dermine the proﬁbility of spinning tools. To counter, defenses
like DSpin could reverse-engineer immutables by generating
spun content and examining the output for invariant text, much
like techniques for inferring botnet email spam templates [29].
By using such inferrence a spinning detector would then also
automatically infer the synonym dictionary, obviating the need
to obtain it or update it over time.
Another set of questions concerns the generality of DSpin
in terms of other spinning tools, manual spinning, and scale.
For any dictionary-based spinning tool, in principle the im-
mutable method in DSpin should be able to detect their spun
output. The human-generated spun articles DSpin does detect
(Section VI-B),
likely does so coincidentally. However,
manual spinning likely still has immutable words that can be
used as anchors, and may be vulnerable to inference across
a set of samples [29]. We have not experimented with other
spinning tools or human-generated spun content, though, and
both remain open questions for future work.
it
In terms of scale, the workload we used to evaluate DSpin
is modest and an immediate question is whether DSpin could
detect spun content at Internet scale. Given that Google has
scaled its systems for detecting near-duplicate content to the
entire Web [23], we believe that there is no fundamental reason
why the immutable method in DSpin could not as well. In
one sense, the immutable method in DSpin is analogous to
near-duplicate detection (detecting duplicated immutables), yet
requires less data to compare since DSpin only compares
immutables — a much smaller fraction of any article (Sec-
tion IV-B). In fact, DSpin might be readily incorporated into
Google’s simhash-based approach by using immutables as the
features in document hashes. As a result, scale is of less
concern than how spammers might respond to evade spinning
detection.
VIII. CONCLUSION
In this paper we have proposed a method for detecting
automatically spun content on the Web. In this method, we use
the synonym dictionary that spinning tools use to create spun
15
content as a ﬁlter, reducing crawled pages to a much smaller
set of “anchor” words that remain unchanged and are common
among all of the spun pages generated from the same seed
page. We then implement this method in a tool called DSpin
that operates on sets of crawled Web pages to identify spun
content. Using controlled experiments, we show that DSpin
successfully identiﬁes spun content, and moreover clusters
spun content according to the set of spun pages generated
together. We then apply DSpin to two crawled data sets, a
set of wiki pages known to be targets of Web spam and a
popular article directory.
With DSpin, we ﬁnd that spinning is a popular spamming
technique on certain sites. A signiﬁcant amount of the posted
content on the wikis is not only Web spam, but also spam
that consists of automatically spun content. We also ﬁnd that
a portion of the clusters of spun content span both the wikis
and the article directory, with evidence that spammers do use
article directories as seed pages.
ACKNOWLEDGEMENTS
We are grateful
to Richard Strong for insightful com-
ments and detailed feedback on the paper, Chris Grier for
his assistance and support with Hadoop, Steve Checkoway
and David Kohlbrenner for insights on reverse-engineering
the TBS dictionary, and Damon McCoy for proxy support
during crawling. We also thank the anonymous reviewers for
their valuable feedback. This work was supported in part by
the Ofﬁce of Naval Research MURI grant N000140911081,
by National Science Foundation grant NSF-1237264, and by
generous research, operational and/or in-kind support from the
UCSD Center for Networked Systems (CNS).
REFERENCES
[1] Blackhat SEO Forum. http://www.blackhatworld.com/, 2013.
[2] Natural Language Processing Software. http://nlp.stanford.edu/, 2013.
[3] The Best
http://thebestspinner.com/
Spinner User Guide.
TBSUsersGuide.pdf, 2013.
[4] A. Aders.
What You Need
Penguin
what-you-need-to-know-about-googles-penguin-update.html, 2012.
Update.
to Know About Google’s
http://www.inc.com/aaron-aders/
[5] A. Z. Broder. On the resemblance and containment of documents. In In
Compression and Complexity of Sequences (SEQUENCES ’97), pages
21–29. IEEE Computer Society, 1997.
[6] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic
clustering of the web. In Selected papers from the sixth international
conference on World Wide Web, pages 1157–1166, Essex, UK, 1997.
Elsevier Science Publishers Ltd.
[8]
[7] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntatic
SRC Technical Report 1997-015, Digital
Clustering of the Web.
Equipment Corporation, July 1997.
J. Caballero, C. Grier, C. Kreibich, and V. Paxson. Measuring Pay-per-
Install: The Commoditization of Malware Distribution. In Proceedings
of the USENIX Security Symposium, San Francisco, CA, August 2011.
Similarity Estimation Techniques from Rounding
Algorithms. In Proceedings of the 34th ACM STOC Conference, May
2002.
[9] M. S. Charikar.
[10] A. Chowdhury, O. Frieder, D. Grossman, and M. McCabe. Collection
Statistics for Fast Duplicate Document Detection. ACM Transactions
of Information Systems (TOIT), 20(2), April 2002.
[11] M. Elhadi and A. Al-Tobi. Use of text syntactical structures in detection
of document duplicates. In ICDIM, pages 520–525. IEEE, 2008.
[12] Example
Job
for Automatically Spinning.
http://ﬁverr.com/
virtualgirl2010/spin-1-article-and-make-it-50-to-60-percent-unique.
[13] Example Job for Manual Spinning. http://www.freelancer.com/projects/
Editing-Articles/Article-spinner-sentence-level.html.
[14] D. Fetterly, M. Manasse, and M. Najork. Spam, Damn Spam, and
In
Statistics: Using statistical analysis to locate spam web pages.
Proceedings of the 7th WebDB Workshop, June 2004.
[15] D. Fetterly, M. Manasse, and M. Najork. Detecting Phrase-Level
Duplication on the World Wide Web. In Proceedings of the ACM SIGIR
Conference, August 2005.
[37] B. Wu and B. D. Davison.
Identifying Link Farm Spam Pages.
In
Proceedings of the 14th WWW Conference, May 2005.
[38] B. Wu, V. Goel, and B. D. Davison. Propagating Trust and Distrust to
Demote Web Spam. In Proceedings of the MTW Workshop, May 2006.
[16] Google’s Search Engine Optimization Starter Guide.
https://static.
googleusercontent.com/external content/untrusted dlcp/www.google.
com/en/us/webmasters/docs/search-engine-optimization-starter-guide.
pdf, 2010.
[17] Z. Gy¨ongyi and H. Garcia-Molina. Web Spam Taxonomy. In Proceed-
ings of 1st AIRWeb Workshop, May 2005.
[18] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. Combating Web Spam
with TrustRank. In Proceedings of the 30th VLDB Conference, August
2004.
[19] M. Henzinger. Finding Near-Duplicate Web Pages: A Large-Scale
In Proceedings of the 29th ACM SIGIR
Evaluation of Algorithms.
Conference, August 2006.
[20] P. Kolari, A. Java, T. Finin, T. Oates, and A. Joshi. Detecting spam
blogs: a machine learning approach. In proceedings of the 21st national
conference on Artiﬁcial intelligence - Volume 2, AAAI’06, pages 1351–
1356. AAAI Press, 2006.
[21] T. G. Kolda and M. J. Procopio. Generalized BadRank with Graduated
Trust. Technical Report SAND2009-6670, Sandia National Laborato-
ries, October 2009.
[22] L. Lu, R. Perdisci, and W. Lee. SURF: Detecting and Measuring Search
Poisoning. In Proceedings of the ACM CCS Conference, October 2011.
[23] G. S. Manku, A. Jain, and A. Das Sarma. Detecting Near-duplicates
for Web Crawling. In Proceedings of the 16th WWW Conference, May
2007.
[24] MediaWiki. http://www.mediawiki.org.
[25] T. Moore, N. Leontiadis, and N. Christin. Fashion Crimes: Trending-
In Proceedings of the ACM CCS
Term Exploitation on the Web.
Conference, October 2011.
[26] M. Motoyama, D. McCoy, K. Levchenko, S. Savage, and G. M. Voelker.
Dirty Jobs: The Role of Freelance Labor in Web Service Abuse.
In
Proceedings of the USENIX Security Symposium, San Francisco, CA,
August 2011.
[27] M. Najork. Detecting quilted web pages at scale.
In Proceedings
of the 35th International ACM SIGIR Conference on Research and
Development
in Information Retrieval, SIGIR ’12, pages 385–394,
Portland, Oregon, USA, 2012.
[28] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. Detecting Spam
Web Pages Through Content Analysis. In Proceedings of the 15th WWW
Conference, May 2006.
[29] A. Pitsillidis, K. Levchenko, C. Kreibich, C. Kanich, G. M. Voelker,
V. Paxson, N. Weaver, and S. Savage. Botnet Judo: Fighting Spam with
Itself. In Proceedings of the Network and Distributed System Security
Symposium (NDSS), San Diego, CA, February 2010.
[30] Y. Shin, M. Gupta, and S. Myers. The Nuts and Bolts of a Forum Spam
Automator. In Proceedings of the 4th USENIX LEET Workshop, March
2011.
[31] N. Shuyo. Language Detection Library for Java Software. https://code.
google.com/p/language-detection/, 2013.
[32] A. Singhal
and M. Cutts.
Search.
Finding More High-Quality
http://googleblog.blogspot.com/2011/02/
Sites
ﬁnding-more-high-quality-sites-in.html, 2011.
in
[33] M. Sobek. PR0 — Google’s PageRank 0 Penalty, 2002.
[34] M. Theobald, J. Siddharth, and A. Paepcke. SpotSigs: Robust and
In
Efﬁcient Near Duplicate Detection in Large Web Collections.
Proceedings of ACM SIGIR, July 2008.
[35] D. Y. Wang, S. Savage, and G. M. Voelker. Cloak and Dagger:
Dynamics of Web Search Cloaking. In Proceedings of the ACM CCS
Conference, October 2011.
[36] D. Y. Wang, S. Savage, and G. M. Voelker.
Juice: A Longitudinal
Study of an SEO Campaign. In Proceedings of the NDSS Symposium,
February 2013.
16