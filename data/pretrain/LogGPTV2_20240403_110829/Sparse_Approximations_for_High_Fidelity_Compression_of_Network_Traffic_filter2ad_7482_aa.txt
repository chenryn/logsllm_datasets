title:Sparse Approximations for High Fidelity Compression of Network Traffic
Data
author:William Aiello and
Anna C. Gilbert and
Brian Rexroad and
Vyas Sekar
Sparse Approximations for High Fidelity
Compression of Network Trafﬁc Data
William Aiello †
University of British Columbia
PI:EMAIL
Anna Gilbert§
University of Michigan
PI:EMAIL
Brian Rexroad
AT & T Labs
PI:EMAIL
Vyas Sekar ‡
Carnegie Mellon University
PI:EMAIL
Abstract
An important component of trafﬁc analysis and network
monitoring is the ability to correlate events across multi-
ple data streams, from different sources and from different
time periods. Storing such a large amount of data for vi-
sualizing trafﬁc trends and for building prediction models
of “normal” network trafﬁc represents a great challenge be-
cause the data sets are enormous. In this paper we present
the application and analysis of signal processing techniques
for effective practical compression of network trafﬁc data.
We propose to use a sparse approximation of the network
trafﬁc data over a rich collection of natural building blocks,
with several natural dictionaries drawn from the network-
ing community’s experience with trafﬁc data. We observe
that with such natural dictionaries, high ﬁdelity compres-
sion of the original trafﬁc data can be achieved such that
even with a compression ratio of around 1:6, the compres-
sion error, in terms of the energy of the original signal lost,
is less than 1%. We also observe that the sparse represen-
tations are stable over time, and that the stable components
correspond to well-deﬁned periodicities in network trafﬁc.
1 Introduction
Trafﬁc monitoring is not a simple task. Network opera-
tors have to deal with large volumes of data, and need to
identify and respond to network incidents in real-time. The
task is complicated even further by the fact that monitoring
needs to be done on multiple dimensions and timescales.
It is evident that network operators wish to observe trafﬁc
at ﬁner granularities across different dimensions for a mul-
titude of reasons that include: 1. real-time detection and
†Majority of this work was done when the author was a member of
AT & T Labs-Research
‡Majority of this work was done when the author was a research intern
at AT & T Labs-Research
§Majority of this work was done when the author was a member of
AT & T Labs-Research. The author is supported by an Elizabeth Crosby
Faculty Fellowship.
response to network failures and isolating errant network
segments, 2. real-time detection of network attacks such as
DDoS and worms, and installation of ﬁlters to protect net-
work entities, and 3. ﬁner resolution root-cause analysis of
the incidents and automated/semi-automated drill down of
the incident.
To meet these requirements, we must be able to gen-
erate and store trafﬁc data on multiple resolution scales
in space (network preﬁxes and physical network entities
such as links, routers), and in time (storing the trafﬁc ag-
gregates at multiple time resolutions). Such requirements
naturally translate into increased operational costs due to
the increased storage requirement. We often transport large
portions of the historical data across a network to individ-
ual operators, import pieces of data into statistical analy-
sis and visualization software for modeling purposes, and
index and run queries against various historical databases
for data drill down. Thus the management overhead in-
volved in handling such large data sets, and the computa-
tional overhead in accessing and processing the large vol-
umes of historical data also increases. We must reduce the
storage size of the data, not only for efﬁcient management
of historical trafﬁc data, but also to accommodate ﬁne data
resolution across space and time.
The compression techniques we investigate are “lossy”
compression methods. For most network monitoring appli-
cations that utilize historical trafﬁc data, it often sufﬁces to
capture salient features of the underlying trafﬁc. We can
thus afford some error by ignoring the low-energy stochas-
tic components of the signal, and gain better compression
using lossy compression techniques (as opposed to lossless
compression methods such as gzip [11] which reduce the
storage size of the data only and do not reduce the size of
the input to monitoring applications). The overall goal of
such compression techniques is to obtain high ﬁdelity (i.e.
low error) representations with as little storage as possible.
In particular, we use a compression method called sparse
representation over redundant dictionaries. A visual in-
spection of aggregated network trafﬁc for many high vol-
USENIX Association
Internet Measurement Conference 2005  
253
ume ports reveals three components. First, there is a natural
diurnal variation for many ports and/or other periodic vari-
ations as well. Second, there are spikes, dips, and other
components of the trafﬁc that appear to be the result of
non-periodic events or processes. Finally, the trafﬁc ap-
pears to be stochastic over small time scales with variance
much smaller than the periodic variations for high volume
ports. Representing a signal with all three components us-
ing a single orthonormal basis, such as a Fourier basis or a
wavelet representation is not likely to yield good compres-
sion: a basis that represents periodic signals well will not
represent non-periodic signals efﬁciently and vice versa.
The methods presented in this paper allow us to use two
or more orthonormal bases simultaneously. A set of two or
more orthonormal bases is called a redundant dictionary.
Hence, with an appropriate set of orthonormal bases as the
redundant dictionary, the periodic and the signiﬁcant non-
periodic portions of the trafﬁc time series can both be rep-
resented efﬁciently within the same framework.
Sparse representation or approximation over redundant
dictionaries does not make assumptions about the under-
lying distributions in the trafﬁc time series. As a result,
sparse approximation can guarantee high ﬁdelity regard-
less of changes in the underlying distributions. In addition,
there are highly efﬁcient, provably correct algorithms for
solving sparse approximation problems. These algorithms
scale with the data and can be easily adapted to multiple
sources of data. They are greedy algorithms, known as
matching or orthogonal matching pursuit.
The primary contribution of this paper is a rigorous in-
vestigation of the method of sparse representation over re-
dundant dictionaries for the compression of network time
series data. We propose and evaluate several redundant dic-
tionaries that are naturally suited for trafﬁc time series data.
We conclude that these methods achieve signiﬁcant com-
pression with very high ﬁdelity across a wide spectrum of
trafﬁc data.
In addition, we also observe that the sparse
representations are stable, not only in terms of their selec-
tion in the sparse representation over time but also in terms
of the individual amplitudes in the representation. These
stable components correspond to well-deﬁned periodicities
in network trafﬁc, and capture the natural structure of traf-
ﬁc time series data. To the best of our knowledge, this is
the ﬁrst thorough application of sparse representations for
compressing network trafﬁc data.
We discuss related work in Section 2, and present a
overall motivation for compression in Section 3. In Sec-
tion 4 we describe in more detail the framework of match-
ing (greedy) pursuit over redundant dictionaries. Section 5
describes our trafﬁc data set, derived from a large Internet
provider. We evaluate the efﬁcacy of our compression tech-
niques in Section 6. Section 7 presents some network traf-
ﬁc monitoring applications that demonstrate the utility of
the compression methods we used. Section 8 discusses the
scope for improving the compression, before we conclude
in Section 9.
2 Related Work
Statisticians concern themselves with subset selection in re-
gression [13] and electrical engineers use sparse represen-
tations for the compression and analysis of audio, image,
and video signals (see [4, 6, 12] for several example refer-
ences).
Lakhina, et al. [9, 10] examine the structure of network
trafﬁc using Principal Component Analysis (PCA). The
observations in our work provide similar insight into the
structure of network trafﬁc. There are two compelling rea-
sons for using sparse approximations over redundant dic-
tionaries, as opposed to PCA alone, for obtaining similar
ﬁdelity-compression tradeoffs. First, the description length
for sparse approximation is much shorter than for PCA,
since the principal vectors require substantially more space
to represent than simple indices into a dictionary. Second,
PCA like techniques may capture and identify the (predom-
inant) structure across all measurements, but may not be
adequate for representing subtle characteristics on individ-
ual trafﬁc aggregates.
Barford, et al. [1] use pseudo-spline wavelets as the ba-
sis wavelet to analyze the time localized normalized vari-
ance of the high frequency component to identify signal
anomalies. The primary difference is our application of sig-
nal processing techniques for compressing network trafﬁc
data, as opposed to using signal decomposition techniques
for isolating anomalies in time series data.
There are several methods for data reduction for gener-
ating compact trafﬁc summaries for speciﬁc real-time ap-
plications. Sketch based methods [8] have been used for
anomaly detection on trafﬁc data, while Estan et al. [3] dis-
cuss methods for performing multi-dimensional analysis of
network trafﬁc data. While such approaches are appealing
for real-time trafﬁc analysis with low CPU and memory re-
quirements, they do not address the problems of dealing
with large volumes of historical data that arise in network
operations. A third, important method of reducing data is
sampling [2] the raw data before storing historical infor-
mation. However, in order for the sampled data to be an
accurate reﬂection of the raw data, one must make assump-
tions regarding the underlying trafﬁc distributions.
3 Compression
It is easy to (falsely) argue that compression techniques
have considerably less relevance when the current cost of
(secondary) storage is less than $1 per GB. Large opera-
tional networks indeed have the unenviable task of man-
aging many terabytes of measurement data on an ongoing
254
Internet Measurement Conference 2005
USENIX Association
basis, with multiple data streams coming from different
routers, customer links, and measurement probes. While
it may indeed be feasible to collect, store, and manage such
a large volume of data for small periods of time (e.g. for
the last few days), the real problem is in managing large
volumes of historical data. Having access to historical data
is a crucial part of a network operator’s diagnostic toolkit.
The historical datasets are typically used for building pre-
diction models for anomaly detection, and also for building
visual diagnostic aids for network operators. The storage
requirement increases not only because of the need for ac-
cess to large volumes of historical trafﬁc data, but also the
pressing need for storing such historical data across differ-
ent spatial and temporal resolutions, as reference models
for ﬁne-grained online analysis.
It may be possible to specify compression and summa-
rization methods for reducing the storage requirement for
speciﬁc trafﬁc monitoring applications that use historical
data. However, there is a deﬁnite need for historical ref-
erence data to be stored at ﬁne spatial and temporal res-
olutions for a wide variety of applications, and it is often
difﬁcult to ascertain the set of applications and diagnostic
techniques that would use these datasets ahead of time. The
compression techniques discussed in this paper have the de-
sirable property that they operate in an application-agnostic
setting, without making signiﬁcant assumptions regarding
the underlying trafﬁc distributions. Since many trafﬁc mon-
itoring applications can tolerate a small amount of error in
the stored values, lossy compression techniques that can
guarantee a high ﬁdelity representation with small storage
overhead are ideally suited for our requirements. We ﬁnd
that our techniques provide very accurate compressed rep-
resentations so that there is only a negligible loss of accu-
racy across a wide spectrum of trafﬁc monitoring applica-
tions.
The basic idea behind the compression techniques used
in this paper is to obtain a sparse representation of the given
time series signal using different orthonormal and redun-
dant bases. While a perfect lossless representation can be
obtained by keeping all the coefﬁcients of the representa-
tion (e.g. using all Fourier or wavelet coefﬁcients), we can
obtain a compressed (albeit lossy) representation by only
storing the high energy coefﬁcients, that capture a substan-
tial part of the original time series signal.
Suppose we have a given time series signal of length N.
For example, in our data set consisting of hourly aggregates
of trafﬁc volumes, N=168 over a week, for a single trafﬁc
metric of interest. We can obtain a lossless representation
by using up a total storage of N×k bits, where k represents
the cost of storing each data point. Alternatively, we can
obtain a sparse representation using m coefﬁcients using a
total storage space of m× k0 +|D| bits, where the term |D|
represents the length of the dictionary used for compres-
sion, and k0 represents the cost of storing the amplitude
associated with each coefﬁcient. The |D| term represents
the cost of storing the list of selected indices as a bit-vector
of length equal to the size of the dictionary. The length
of the dictionary |D| is equal to αN, with the value α be-
ing one for an orthonormal basis (e.g., Fourier, Wavelet,
Spike) or equal to two in the case of a redundant dictionary
consisting of Fourier and Spike waveforms. The effective
compression ratio is thus (mk0 + αN)/(N k). Assuming
k ≈ k0 (the cost of storing the raw and compressed coefﬁ-
cients are similar) and α (cid:28) k (the values in consideration
are large integers or ﬂoats), the effective compression (even
with this naive encoding) is approximately equal to m/N
1. The primary focus of this paper is not to come up with an
optimal encoding scheme for storing the m coefﬁcients to
extract the greatest per-bit compression. Rather we wish to
explore the spectrum of signal compression techniques, us-
ing different natural waveforms as dictionaries for achiev-
ing a reasonable error-compression tradeoff.
A natural error metric for lossy compression techniques
in signal processing is the energy of the residual, which
is the vector difference between the original signal and the
compressed representation. Let S be the original signal and
Cs represent the compressed representation of S. The sig-
nal R = S − Cs represents the residual signal. We use the
kSk2 where k · k represents
following relative error metric kRk2
the L2 (Euclidean) norm of a vector. The error metric rep-
resents the fraction of the energy in the original signal that
is not captured in the compressed model. For example, a
relative error of 0.01 implies that the energy of the residual
signal (not captured by the compressed representation) is
only 1% of the energy of the original signal. Our results
indicate that we can achieve high ﬁdelity compression for
more than 90% of all trafﬁc aggregates, with a relative er-
ror of less than 0.01 using only m = 30 coefﬁcients, for
the hourly aggregates with N = 168. Since a m-coefﬁcient
representation of the signal implies a compression ratio of
roughly m/N, with N = 168, a 30-coefﬁcient representa-
tion corresponds to a compression ratio of roughly 1:6.
Consider the following scenario. An operator wishes to
have access to ﬁner resolution historical reference data col-
lected on a per application port basis (refer Section 5 for
a detailed description of the datasets used in this paper).
Suppose the operator wants to improve the temporal gran-
ularity by going from hourly aggregates to 10 minute ag-
gregates. The new storage requirement is a non-negligible
60/10 × X = 6X, where X represents the current stor-
age requirement (roughly 1GB of raw data per router per
week). Using the compression techniques presented in this
paper, by ﬁnding small number of dictionary components
to represent the time series data, the operator can easily off-
set this increased storage cost.
Further, we observe (refer Section 8.2) that moving to
ﬁner temporal granularities does not actually incur substan-
tially higher storage cost. For example we ﬁnd that the
USENIX Association
Internet Measurement Conference 2005  
255
same ﬁdelity of compression (at most 1% error) can be ob-
tained for time-series data at ﬁne time granularity (aggre-
gated over ﬁve minute intervals) by using a similar number
of coefﬁcients as those used for data at coarser time gran-
ularities (hourly aggregates). Thus by using our compres-
sion techniques operators may in fact be able to substan-
tially cut down storage costs, or alternatively use the stor-
age “gained” for improving spatial granularities (collecting
data from more routers, customers, preﬁxes, etc).
In the next section, we present a brief overview on the
use of redundant dictionaries for compression, and present
a greedy algorithm for ﬁnding a sparse representation over
a redundant dictionary.
4 Sparse Representations over Redundant
Dictionaries
20
15
10
5
0
−5
l
e
m
u
o
v
c
i
f
f
a
r
T
25
20
15
10
5
0
−10
0
10
20
30
50
40
60
Time (hour of week)
70
80
90
100
−5
0
10
20
30
40
50
Frequency
60
70
80
90
100
(a) An example signal X
which has a short repre-
sentation over the redun-
dant dictionary D.
(b) The discrete cosine
transform (DCT) of
the
example signal X.
Figure 1: The example signal X and its discrete cosine
transform (DCT).
One mathematically rigorous method of compression is
that of sparse approximation. Sparse approximation prob-
lems arise in a host of scientiﬁc, mathematical, and engi-
neering settings and ﬁnd greatest practical application in
image, audio, and video compression [4, 6, 12], to name
a few. While each application calls for a slightly differ-
ent problem formulation, the overall goal is to identify a
good approximation involving a few elementary signals—
a sparse approximation. Sparse approximation problems
have two characteristics. First, the signal vector is approx-
imated with a linear model of elementary signals (drawn
from a ﬁxed collection of several orthonormal bases). Sec-
ond, there is a compromise between approximation error
(usually measured with Euclidean norm) and the number
of elementary signals in the linear combination.
One example of a redundant dictionary for signals of
length N is the union
(
cos(cid:16) πk(t + 1
2)
N
(cid:17))[(
)
δk(t)
,
D =
where k = 0, . . . , N − 1, of the cosines and the spikes on
N points. The “spike” function δk(t) is zero if t 6= k and is
one if t = k. Either basis of vectors is complete enough to
represent a time series of length N but it might take more
vectors in one basis than the other to represent the signal.
To be concrete, let us take the signal
X(t) = 3 cos(cid:16) π8(t + 1
2)
(cid:17) − 5δ10(t) + 15δ20(t)
100
plotted in Figure 1(a). The spectrum of the discrete cosine
transform (DCT) of X is plotted in Figure 1(b). For this
example, all the coefﬁcients are nonzero. That is, if we
write
X(t) =
1
100
ˆX(k) cos(cid:16) πk(t + 1
2)
(cid:17)
100
99X
k=0
as a linear combination of vectors from the cosine basis,
then all 100 of the coefﬁcients ˆX(k) are nonzero. Also, if
we write X(t) as a linear combination of spikes, then we
must use almost all 100 coefﬁcients as the signal X(t) is
nonzero in almost all 100 places. Contrast these two ex-
pansions for X(t) with the expansion over the redundant
dictionary D
X(t) = 3 cos(cid:16) π8(t + 1
2 )
(cid:17) − 5δ10(t) + 15δ20(t).
100
In this expansion there are only three nonzero coefﬁcients,
the coefﬁcient 3 attached to the cosine term and the two co-
efﬁcients associated with the two spikes present in the sig-
nal. Clearly, it is more efﬁcient to store three coefﬁcients
than all 100. With three coefﬁcients, we can reconstruct or
decompress the signal exactly. For more complicated sig-
nals, we can keep a few coefﬁcients only and obtain a good
approximation to the signal with little storage. We obtain a
high ﬁdelity (albeit lossy) compressed version of the signal.
Observe that because we used a dictionary which consists
of simple, natural building blocks (cosines and spikes), we
need not store 100 values to represent each vector in the
dictionary. We do not have to write out each cosine or spike
waveform explicitly.
Finding the optimal dictionary for a given application is
a difﬁcult problem and good approximations require do-
main speciﬁc heuristics. Our contribution is the identiﬁca-
tion of a set of dictionaries that are well-suited for com-
pressing trafﬁc time-series data, and in empirically justi-
fying the choice of such dictionaries. Prior work on un-
derstanding the dimensionality of network trafﬁc data us-
ing principal component analysis [10] identiﬁes three types
of eigenﬂows: periodic, spikes, and noise. With this intu-
ition, we try different dictionaries drawn from three basic
waveforms: periodic functions (or complex exponentials),
256
Internet Measurement Conference 2005
USENIX Association
spikes, and wavelets. Dictionaries that are comprised of
these constituent signals are descriptive enough to capture
the main types of behavior but not so large that the algo-
rithms are unwieldy.
4.1 Greedy Pursuit Algorithms
A greedy pursuit algorithm at each iteration makes the best
local improvement to the current approximation in hope of
obtaining a good overall solution. The primary algorithm
is referred to as Orthogonal Matching Pursuit (OMP), de-
scribed in Algorithm 4.1.
In each step of the algorithm,
the current best waveform is chosen from the dictionary
to approximate the residual signal. That waveform is then
subtracted from the residual and added to the approxima-
tion. The algorithm then iterates on the residual. At the
end of the pursuit stage, the approximation consists of a
linear combination of a small number of basic waveforms.
We ﬁx some notation before describing the algorithm. The
dictionary D consists of d vectors ϕj of length N each.
We write these vectors ϕj as the rows in a matrix Φ and
refer to this matrix as the dictionary matrix. OMP is one of
the fastest2 provably correct algorithm for sparse represen-
tation over redundant dictionaries, assuming that the dic-
tionary satisﬁes certain geometric constraints [5] (roughly,
the vectors in the dictionary must be almost orthogonal to
one another). The algorithm is provably correct in that if
the input signal consists of a linear combination of exactly
m vectors from the dictionary, the algorithm ﬁnds those m
vectors exactly. In addition, if the signal is not an exact
combination of m vectors but it does have an optimal ap-
proximation using m vectors, then the algorithm returns an
m-term linear combination whose approximation error to
the input signal is within a constant factor of the optimal
approximation error. If we seek m vectors in our represen-
tation, the running time of OMP is O(mdN). Dictionaries
which are unions of orthonormal bases (which meet the ge-
ometric condition for the correctness of OMP), are of size
d = kN, so the running time for OMP with such dictionar-
ies is O(mkN 2).
Algorithm 4.1 (OMP)
INPUT:
• A d × N matrix Φ
• A vector v of measurements of length N