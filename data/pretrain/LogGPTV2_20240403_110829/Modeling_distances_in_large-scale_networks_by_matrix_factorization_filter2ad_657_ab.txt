hosts H to another set of N 0 hosts H0, which may or may not
overlap with each other. In this case, X 2 RN(cid:2)d contains
the outgoing vectors for H and Y 2 Rd(cid:2)N 0
contains the
incoming vectors for H0. For simplicity, though, we consider
the case N = N 0 in what follows.
components of the row vectors are obtained from the or-
thogonal eigenvectors of their correlation matrix; each row
vector can be expressed as a linear combination of these
eigenvectors. The diagonal values of S measure the signif-
icance of the contribution from each principal component.
In previous work on embedding of host positions by PCA,
such as ICS [12] and Virtual Landmark [20], the (cid:12)rst d rows
of the matrix U were used as coordinates for the hosts, while
discarding the information in the matrices S and V . By con-
trast, our approach uses U , S and V to compute outgoing
and incoming vectors for each host.
We use the topology in Figure 1 as an example to show
how the algorithm works. The distance matrix is
D =
2
64
0
1
1
2
1
0
2
1
1
2
0
1
2
1
1
0
3
75
We obtain the SVD result as
(cid:0)0:5
(cid:0)0:5 (cid:0)
0
1
U =
2
6664
(cid:0)0:5
(cid:0)0:5
1
p2
p2
0
1
p2
0
0
1
p2
(cid:0)
0:5
(cid:0)0:5
(cid:0)0:5
0:5
3
7775
; S =
2
64
4
0
0
0
0
2
0
0
0
0
2
0
0
0
0
0
3
75
It is known that these update rules converge monotonically
to stationary points of the error function, Eq. (7). Our expe-
rience shows that two hundred iterations su(cid:14)ce to converge
to a local minimum.
One major advantage of NMF over SVD is that it is
straightforward to modify NMF to handle missing entries in
the distance matrix D. For various reasons, a small number
of elements in D may be unavailable. SVD can proceed with
missing values if we eliminate the rows and columns in D
that contain them, but doing so will leave the corresponding
host positions unknown.
NMF can cope with missing values if we slightly change
the update rules. Suppose M is a binary matrix where
Mij = 1 indicates Dij is known and Mij = 0 indicates Dij
is missing. The modi(cid:12)ed update rules are:
Xia   Xia Pk DikMikYka
Pk(XY T )ikMikYka
Yja   Yja Pk(X T )akDkjMkj
Pk(X T )ak(XY T )kjMkj
(8)
(9)
These update rules converge to local minima of the error
function, Pij MijjDij (cid:0) ~Xi (cid:1) ~Yjj2.
4.3 Evaluation
V =
2
6664
(cid:0)0:5
(cid:0)0:5
(cid:0)0:5 (cid:0)
(cid:0)0:5
0
1
1
p2
p2
0
(cid:0)
1
p2
0
0
1
p2
(cid:0)0:5
0:5
0:5
(cid:0)0:5
3
7775
Note that S44 = 0. Therefore, an exact d = 3 factorization
exists with:
X =2
664
(cid:0)1
0
1
(cid:0)1 (cid:0)1
0
1
(cid:0)1
0
0 (cid:0)1
(cid:0)1
3
775
; Y =2
664
(cid:0)1
0 (cid:0)1
0
(cid:0)1
1
0
(cid:0)1 (cid:0)1
0
(cid:0)1
1
3
775
One can verify in this case that the reconstructed distance
matrix XY T is equal to the original distance matrix D.
4.2 Non-negative matrix factorization
Non-negative matrix factorization (NMF) [11] is another
form of linear dimensionality reduction that can be applied
to the distance matrix Dij . The goal of NMF is to mini-
mize the same error function as in Eq. (7), but subject to
the constraint that X and Y are non-negative matrices. In
contrast to SVD, NMF guarantees that the approximately
reconstructed distances are nonnegative: ^Dij (cid:21) 0. The er-
ror function for NMF can be minimized by an iterative al-
gorithm. Compared to gradient descent and the Simplex
Downhill method, however, the algorithm for NMF con-
verges much faster and does not involve any heuristics, such
as choosing a step size. The only constraint on the algo-
rithm is that the true network distances must themselves be
nonnegative, Dij (cid:21) 0; this is generally true and holds for
all the examples we consider. The algorithm takes as input
initial (random) matrices X and Y and updates them in an
alternating fashion. The update rules for each iteration are:
Xia   Xia
Yja   Yja
(DY )ia
(XY T Y )ia
(X T D)aj
(X T XY T )aj
We evaluated the accuracy of network distance matrices
modeled by SVD and NMF and compared the results to
those of PCA from the Lipschitz embeddings used by Vir-
tual Landmark [20] and ICS [12]. We did not evaluate the
Simplex Downhill algorithm used in GNP because while its
accuracy is not obviously better than Lipschitz embedding,
it is much more expensive, requiring hours of computation
on large data sets [20]. Accuracies were evaluated by the
modi(cid:12)ed relative error,
relative error = jDij (cid:0) ^Dijj
min(Dij ; ^Dij )
(10)
where the min-operation in the denominator serves to in-
crease the penalty for underestimated network distances.
4.3.1 Data sets
We used the following (cid:12)ve real-world data sets in simu-
lation. Parts of the data sets were (cid:12)ltered out to eliminate
missing elements in the distance matrices (since none of the
algorithms except NMF can cope with missing data).
The network distances in the data sets are round-trip time
(RTT) between pairs of Internet hosts. RTT is symmetric
between two end hosts, but it does violate the triangle in-
equality and also give rise to other e(cid:11)ects (described in Sec-
tion 2.2) that are poorly modeled by network embeddings
in Euclidean space.
[1]
(cid:15) NLANR: The NLANR Active Measurement
Project
collects a variety of measurements
between all pairs of participating nodes. The nodes
are mainly at NSF supported HPC sites, with about
10% outside the US. The data set we used was
collected on January 30, 2003, consisting of measure-
ments of a 110 (cid:2) 110 clique. Each host was pinged
once per minute, and network distance was taken as
the minimum of the ping times over the day.
(cid:15) GNP and AGNP: The GNP project measured min-
imum round trip time between 19 active sites in May
2001. About half of the hosts are in North America;
the rest are distributed globally. We used GNP to
construct a symmetric 19 (cid:2) 19 data set and AGNP to
construct an asymmetric 869 (cid:2) 19 dataset.
(cid:15) P2PSim: The P2Psim project [14] measured a dis-
tance matrix of RTTs among about 2000 Internet DNS
servers based on the King method [8]. The DNS
servers were obtained from an Internet-scale Gnutella
network trace.
(cid:15) PL-RTT: Obtained from PlanetLab pairwise ping
project [19]. We chose the minimum RTT measured at
3/23/2004 0:00 EST. A 169 (cid:2) 169 full distance matrix
was obtained by (cid:12)ltering out missing values.
4.3.2 Simulated Results
GNP 
NLANR 
AGNP 
P2PSim 
PL−RTT 
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
a
u
m
u
c
l
NLANR
GNP
AGNP
PL−RTT
P2PSim
0
0
0.2
0.4
0.6
relative error
0.8
1
Figure 2: Cumulative distribution of relative error
by SVD over various data sets, d = 10
Figure 2 illustrates the cumulative density function (CDF)
of relative errors of RTT reconstructed by SVD when d = 10,
on 5 RTT data sets. The best result is over GNP data set:
more than 90% distances are reconstructed within 9% rela-
tive error. This is not too surprising because the GNP data
set only contains 19 nodes. However, SVD also works well
over NLANR, which has more than 100 nodes: about 90%
fraction of distances are reconstructed within 15% relative
error. Over P2PSim and PL-RTT data sets, SVD achieves
similar accuracy results: 90 percentile relative error is 50%.
We ran the same tests on NMF and observed similar results.
Therefore, we chose NLANR and P2PSim as two represen-
tative data sets for the remaining simulations.
Figure 3 compares the reconstruction accuracy of three
algorithms: matrix factorization by SVD and NMF, and
PCA applied to the Lipschitz embedding. The algorithms
were simulated over NLANR and P2PSim data sets. It is
shown that NMF has almost exactly the same median rel-
ative errors as SVD on both data sets when the dimension
d < 10. Both NMF and SVD yield much more accurate
results than Lipschitz: the median relative error of SVD
and NMF is more than 5 times smaller than Lipschitz when
d = 10. SVD is slightly better than NMF when d is large.
The reason for this may be that the algorithm for NMF
is only guaranteed to converge to local minima. Consider-
ing that the hosts in the data sets come from all over the
Internet, the results show that matrix factorization is a scal-
able approach to modeling distances in large-scale networks.
In terms of maintaining a low-dimensional representation,
d (cid:25) 10 appears to be a good tradeo(cid:11) between complexity
and accuracy for both SVD and NMF.
5. DISTANCE PREDICTION
The simulation results from the previous section demon-
strate that pairwise distances in large-scale networks are well
modeled by matrix factorization. In this section we present
the Internet Distance Estimation Service (IDES) | a scal-
able and robust service based on matrix factorization to es-
timate network distances between arbitrary Internet hosts.
5.1 Basic architecture
We classify Internet hosts into two categories:
landmark
nodes and ordinary hosts. Landmark nodes are a set of
well-positioned distributed hosts. The network distances be-
tween each of them is available to the information server of
IDES. We assume that landmarks can measure network dis-
tances to others and report the results to the information
server. The information server can also measure the pairwise
distances via indirect methods without landmark support,
e.g. by the King method [8] if the metric is RTT. An ordi-
nary host is an arbitrary end node in the Internet, which is
identi(cid:12)ed by a valid IP address.
Suppose there are m landmark nodes. The (cid:12)rst step of
IDES is to gather the m (cid:2) m pairwise distance matrix D
on the information server. Then, we can apply either SVD
or NMF algorithm over D to obtain landmark outgoing and
incoming vectors ~Xi and ~Yi in d dimensions, d < m, for each
host Hi. As before, we use X and Y to denote the d (cid:2) m
matrices with ~Xi and ~Yi as row vectors. Note that NMF
can be used even when D contains missing elements.
Now suppose an ordinary host Hnew wants to gather dis-
tance information over the network. The (cid:12)rst step is to cal-
culate its outgoing vector ~Xnew and incoming vector ~Ynew.
To this end, it measures the network distances to and from
the landmark nodes. We denote Dout
as the distance to
landmark i, and Din
i as the distance from landmark i to the
host. Ideally, we would like the outgoing and incoming vec-
i = ~Xi (cid:1) ~Ynew. The
tors to satisfy Dout
solution with the least squares error is given by:
i = ~Xnew (cid:1) ~Yi and Din
i
m