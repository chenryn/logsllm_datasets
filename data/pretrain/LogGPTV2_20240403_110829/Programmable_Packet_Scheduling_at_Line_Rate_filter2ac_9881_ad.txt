ﬂows using the ranks of the head elements in each ﬂow. It
supports one enqueue and one dequeue to its enclosing PIFO
block every clock cycle, which translates into the following
operations on the ﬂow scheduler every clock cycle.
1. Enqueue operation:
Inserting a ﬂow when the ﬂow
goes from empty to non-empty.
2. Dequeue operation: Removing a ﬂow that empties
once it is scheduled, (or) removing and reinserting a
ﬂow with the rank of the next element if the ﬂow is
still backlogged.
The operations above require the ﬂow scheduler to inter-
nally support two primitives every clock cycle.
1. Push up to two elements into the ﬂow scheduler: one
each for an enqueue’s insert and a dequeue’s reinsert.
2. Pop one element: for the remove from a dequeue.
These primitives access all of the ﬂow scheduler’s elements
in parallel. To facilitate this, we implement the ﬂow sched-
uler in ﬂip ﬂops, unlike the rank store, which is in SRAM.
The ﬂow scheduler is a sorted array, where a push is im-
plemented by executing the three steps below (Figure 14).
1. Compare the incoming rank against all ranks in the ar-
ray in parallel, using a comparator. This produces a bit
mask of comparison results indicating if the incoming
rank is greater/lesser than an array element’s rank.
2. Find the ﬁrst 0-1 transition in this bit mask, using a
priority encoder, to determine the index to push into.
3. Push the element into this index, by shifting the array.
A pop is implemented by shifting the head element out of
the sorted array.
So far, we have focused on a ﬂow scheduler implemen-
tation handling a single logical PIFO. To handle multiple
logical PIFOs, we keep elements sorted by rank, regardless
of the logical PIFO they belong to; hence, the push logic
doesn’t change. To pop from a speciﬁc logical PIFO, we
compare against all elements to ﬁnd elements with that log-
ical PIFO ID. Among these, we ﬁnd the ﬁrst using a priority
encoder, and remove this element by shifting the array. The
rank store implementation doesn’t change when introducing
logical PIFOs; however, we do require that a ﬂow belong to
exactly one logical PIFO.
To concurrently issue 2 pushes and 1 pop every clock cy-
cle, we provision three parallel digital circuits (Figure 14).
Both the push and pop require 2 clock cycles to complete
and need to be pipelined to maintain the required through-
put (Figure 15). For pushes, the ﬁrst stage of the pipeline
executes the parallel comparison and priority encoder steps
to determine an index; the second stage pushes the element
into the array using the index. Similarly, for pops, the ﬁrst
stage executes the equality check (for logical PIFO IDs) and
priority encoder steps to compute an index; the second stage
pops the head element out of the array using the index.
Our implementation meets timing at 1 GHz and supports
up to one enqueue/dequeue operation on a logical PIFO
within a PIFO block every clock cycle. Because a reinsert
operation requires a pop, followed by an access to the rank
Figure 14: Hardware implementation of ﬂow scheduler.
Each element in the ﬂow scheduler is connected to two >
comparators (2 pushes) and one == comparator (1 pop).
Figure 15: 2-stage pipeline for ﬂow scheduler
store for the next element, followed by a push, our imple-
mentation supports a dequeue from the same logical PIFO
only once every 4 cycles. This is because if a dequeue is ini-
tiated in clock cycle 1, the pop for the dequeue completes in
2, the rank store is accessed in 3, and the push is initiated in
4, making cycle 5 the earliest time to reissue a dequeue. This
restriction is inconsequential in practice. A dequeue every 4
cycles from a logical PIFO is sufﬁcient to service the high-
est link speed today, 100 Gbit/s, which requires a dequeue at
most once every 5 clock cycles for a minimum packet size
of 64 bytes. Dequeues to distinct logical PIFO IDs are still
permitted every cycle.
5.3 Interconnecting PIFO blocks
An interconnect between PIFO blocks allows PIFO blocks
to enqueue into and dequeue from other blocks. Because
the number of PIFO blocks is small, we provide a full mesh
between them. For a 5-block PIFO mesh as in our baseline
design, this requires 5*4 = 20 sets of wires between PIFO
blocks. Each set carries all the inputs required for specifying
an enqueue and dequeue operation on a PIFO block.
For our baseline design (§5.1), for an enqueue, we require
a logical PIFO ID (8 bits), the element’s rank (16 bits), the
element meta data (32 bits), and the ﬂow ID (10 bits). For
Rank== comparators> comparatorsPriority encoderPriority encoderShift elements based on push, pop indicesPop(DEQ)Push 1(ENQ)Push 2(reinsert)LogicalPIFO IDRankRankLogicalPIFO IDRankLogicalPIFO IDRankLogicalPIFO IDClockPush operationsCheck > A in parallel,priority encodePush A by shifting into arrayStart push(A)Finish push(A)Cycle 1Cycle 2PushpipelinePoppipelinePopped ElementPopOutputStart pop(X)Finish pop(X)PopoperationsCheck == X in parallel,priority encodePop head of logical PIFO X by shifting out of arraya dequeue, we need a logical PIFO ID (8 bits) and wires to
store the dequeued element’s metadata ﬁeld (32 bits). This
adds up to 106 bits per set of wires, or 2120 bits for the
mesh. This is a small number of wires for a chip. For ex-
ample, RMT’s match-action pipeline uses 4000 1-bit wires
between a a pair of pipeline stages to move its 4K packet
header vector between stages [17].
5.4 Area overhead
Because we target a shared-memory switch, the schedul-
ing logic is shared across ports and a single PIFO mesh
services an entire switch. Therefore, to estimate the area
overhead of a programmable scheduler, we estimate the area
overhead of a single PIFO mesh. Our overhead does not have
to be multiplied by the number of ports and is the same for
two shared-memory switches with equal aggregate packet
rates, e.g., a 6-port 100G switch and a 60-port 10G switch.
To determine the area of a PIFO mesh, we compute the
area of a single PIFO block and multiply it by the number of
blocks because the area of the interconnect is negligible. For
a single block’s area, we separately estimate areas for the
rank store, atom pipelines, and ﬂow scheduler, and ignore
the area of the small next-hop lookup tables. We estimate
the rank store’s area by using SRAM estimates [8], the atom
pipeline’s area using Domino [37], and the ﬂow scheduler’s
area by implementing it in Verilog [9] and synthesizing it to
gate-level netlist in a 16-nm standard cell library using the
Cadence Encounter RTL Compiler [2]. The RTL Compiler
also veriﬁes that the ﬂow scheduler meets timing at 1 GHz.
Overall, our baseline design consumes about 7.35 mm2 of
chip area (Table 1). This is about 3.7% of the chip area of
a typical switching chip, using the minimum chip area esti-
mate of 200 mm2 provided by Gibb et al. [23]. In return for
this 3.7%, we get a signiﬁcantly more ﬂexible packet sched-
uler than current switches, which provide ﬁxed two or three-
level hierarchical scheduling. Our 3.7% area overhead is
similar to the overhead for other programmable switch func-
tions, e.g., 2% for programmable parsing [23] and 15% for
programmable header processing [17].
Varying the ﬂow scheduler’s parameters from the base-
line. The ﬂow scheduler has four parameters: rank width,
metadata width, number of logical PIFOs, and number of
ﬂows. Among these, increasing the number of ﬂows has the
most impact on whether the ﬂow scheduler meets timing at
1 GHz. This is because the ﬂow scheduler uses a priority en-
coder, whose size is the number of ﬂows and whose critical
path delay increases with the number of ﬂows. With other
parameters set to their baseline values, we vary the number
of ﬂows to determine the eventual limits of a ﬂow scheduler
with today’s transistor technology (Table 2), and ﬁnd that we
can scale to 2048 ﬂows while still meeting timing at 1 GHz.
The remaining parameters affect the area of a ﬂow sched-
uler, but have little effect on meeting timing at 1 GHz. For
instance, starting from the baseline design of the ﬂow sched-
uler that takes up 0.224 mm2, increasing the rank width to 32
bits increases it to 0.317 mm2, increasing the number of log-
ical PIFOs to 1024 increases it to 0.233 mm2, and increasing
Component
Switching chip
Flow Scheduler
SRAM (1 Mbit)
Rank store
Next pointers for linked
lists in dynamically allo-
cated rank store
Free list memory for dy-
namically allocated rank
store
Head,
tail, and count
memory for each ﬂow in
the rank store
One PIFO block
5-block PIFO mesh
300 atoms spread out
over the 5-block PIFO
mesh for rank computa-
tions
Overhead for 5-block
PIFO mesh
Area in mm2
200–400 [23]
0.224 (from synthesis)
0.145 [8]
64 K * (16 + 32) bits * 0.145 mm2
/ Mbit = 0.445
64 K * 16 bit pointers * 0.145 =
0.148
64 K * 16 bit pointers * 0.145 =
0.148
0.1476 (from synthesis)
0.224 + 0.445 + 0.148 + 0.148 +
0.1476 = 1.11 mm2
5.55
6000 µm2* 300 = 1.8 mm2
(§4.1, [37])
(5.55 + 1.8) / 200.0 = 3.7 %
Table 1: A 5-block PIFO mesh incurs a 3.7% chip area over-
head relative to a baseline switch.
# of ﬂows
256
512
1024
2048
4096
Area (mm2)
0.053
0.107
0.224
0.454
0.914
Meets timing at 1 GHz?
Yes
Yes
Yes
Yes
No
Table 2: The ﬂow scheduler’s area increases with the number
of ﬂows. The ﬂow scheduler meets timing until 2048 ﬂows.
the metadata width to 64 bits increases it to 0.317 mm2. In
all cases, the ﬂow scheduler continues to meet timing.
5.5 Additional implementation concerns
Coordination between enqueue and dequeue. When
computing packet ranks on enqueue, some scheduling algo-
rithms access state modiﬁed on packet dequeues. An exam-
ple is STFQ (§2.1) that accesses the virtual_time variable
when computing a packet’s virtual start time. This enqueue-
dequeue coordination can be implemented in two ways. One
is shared state that can be accessed on both enqueue and de-
queue, similar to queue occupancy counters. Another is to
periodically synchronize the enqueue and dequeue views of
the same state: for STFQ, the degree of short-term fairness
is directly correlated with how up-to-date the virtual_time
information on the enqueue side is.
Buffer management. Our design focuses on programmable
scheduling and does not manage the allocation of a switch’s
data buffer across ﬂows. Buffer management can use static
buffer limits for each ﬂow. The limits can also be dynamic,
e.g., RED [22] and dynamic buffer sizing [18].
In a shared-memory switch, buffer management is orthog-
onal to scheduling, and is implemented using counters that
track ﬂow occupancy in a shared buffer. Before a packet
is enqueued into the scheduler, if any counter exceeds a
static or dynamic threshold, the packet is dropped. A similar
design for buffer management could be used with a PIFO-
based scheduler as well.
Priority Flow Control. Priority Flow Control (PFC) [7]
is a standard that allows a switch to send a pause message
to an upstream switch requesting it to cease transmission of
packets belonging to particular ﬂows. PFC can be integrated
into our hardware design by masking out certain ﬂows in
the ﬂow scheduler during the dequeue operation if they have
been paused because of a PFC pause message, and unmask-
ing them when a PFC resume message is received.
Multi-pipeline switches. The highest end switches today,
such as the Broadcom Tomahawk [4], support aggregate ca-
pacities exceeding 3 Tbit/sec. At a minimum packet size of
64 bytes, this corresponds to an aggregate packet rate of ~6
billion packets/s. Because a single switch pipeline (Figure 8)
typically runs at 1 GHz and processes a billion packets/s,
such switches require multiple ingress and egress pipelines
that share access to the scheduler subsystem alone.
In multi-pipeline switches, each PIFO block needs to sup-
port multiple enqueue and dequeue operations per clock cy-
cle (as many as the number of ingress and egress pipelines)
because packets can be enqueued from any of the input ports
every clock cycle, and each input port could reside in any
of the ingress pipelines. Similarly, each egress pipeline re-
quires a new packet every clock cycle, resulting in multiple
dequeues every clock cycle.
A full-ﬂedged design for multi-pipeline switches is be-
yond this paper, but our current design facilitates a multi-
pipeline implementation. A rank store supporting multi-
ple pipelines is similar to the data buffer of multi-pipeline
switches today. Building a ﬂow scheduler to support multi-
ple enqueues/dequeues per clock is relatively easy because it
is maintained in ﬂip ﬂops, where it is simple to add multiple
ports (unlike SRAM).
6. RELATED WORK
The Push-in First-out Queue. PIFOs were ﬁrst intro-
duced as a proof construct to prove that a combined input-
output queued switch could exactly emulate an output-
queued switch [19]. We show here that PIFOs can be used
as an abstraction for programmable scheduling at line rate.
Packet scheduling algorithms. The literature is replete
with scheduling algorithms [12, 13, 24, 25, 29, 35, 36, 42]
. Yet, line-rate switches support only a few: DRR, trafﬁc
shaping, and strict priorities. As §3 shows, PIFOs allow a
line-rate switch to run many of these scheduling algorithms,
which, so far, have only been run on software routers.
Programmable switches.
Recent work has proposed
hardware architectures [1, 5, 11, 17] and software ab-
stractions [16, 37] for programmable switches. While
many packet-processing tasks can be programmed on these
switches, scheduling isn’t one of them.
Programmable