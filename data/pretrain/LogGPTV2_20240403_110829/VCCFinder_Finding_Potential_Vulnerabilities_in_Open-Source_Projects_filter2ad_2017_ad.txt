### 4. Methodology

#### 4.1 Linear SVM and Hyperplane Optimization
We utilize the LibLinear tool [17], which provides various optimization algorithms for linear Support Vector Machines (SVMs). Each algorithm aims to find a hyperplane \( \mathbf{w} \) that maximally separates two classes: unclassified commits and vulnerability-contributing commits. Since the learning is performed in the input space, the hyperplane vector \( \mathbf{w} \) can be used to explain the decisions made by our classifier.

#### 4.2 Decision Function and Feature Contribution
The decision function of the SVM is given by the inner product between the feature vector \( \phi(\mathbf{x}) \) and the hyperplane vector \( \mathbf{w} \):
\[ f(\mathbf{x}) = \langle \phi(\mathbf{x}), \mathbf{w} \rangle = \sum_{s \in S} w_s b(\mathbf{x}, s) \]
This score represents the distance from \( \phi(\mathbf{x}) \) to the hyperplane, indicating the likelihood that a commit introduces a vulnerability. By examining the contribution of each feature to this score, we can identify which features are most influential in the classification decision.

#### 4.3 Parameter Calibration
To calibrate the free parameters of the linear SVM, specifically the regularization parameter \( C \) and the class weight \( W \), we perform standard cross-validation on the training data. The optimal values obtained are \( C = 1 \) and \( W = 100 \) for the class of suspicious commits.

### 5. Evaluation

#### 5.1 Dataset and Temporal Split
We evaluate the effectiveness of our approach using a temporal split between the training and test data. The dataset is split such that all commit data up to December 31, 2010, is used for training, while data from 2011 to 2014 is used for testing. This setup simulates the scenario where VCCFinder is trained on historical data and then predicts future vulnerabilities.

| Dataset | Historical | Test | Total |
|---------|------------|------|-------|
| CVEs    | 469        | 249  | 718   |
| VCCs    | 421        | 219  | 640   |
| Unclassified Commits | 90,282 | 79,220 | 169,502 |

#### 5.2 Detection Performance
We evaluate the detection performance of VCCFinder using precision-recall curves. Figure 1(a) shows the precision-recall curves for different feature sets, demonstrating that the combination of all features (shown in blue) outperforms classifiers using only a subset of features. Figure 1(b) compares the precision-recall curve of VCCFinder with FlawFinder, an open-source static code analyzer.

#### 5.3 Case Study
In practice, developers can set the recall level of VCCFinder to match their review capacity. For example, setting VCCFinder's recall to the same as FlawFinder (0.24) results in VCCFinder flagging only 89 out of 79,688 commits for manual review, compared to 5,513 commits flagged by FlawFinder. This is a manageable number of reviews for high return. We discuss several examples of vulnerabilities detected by VCCFinder at this recall level.

- **CVE-2012-2119**: A buffer overflow in the macvtap device driver in the Linux kernel. Detected due to high code churn and the author's limited contributions.
- **CVE-2013-0862**: Multiple integer overflows in FFmpeg. Detected due to the author's limited contributions and a large code insertion.
- **CVE-2014-1438**: A denial-of-service vulnerability in the Linux kernel. Detected due to high exceptions, inline ASM code, and user input variables.
- **CVE-2014-0148**: An infinite loop vulnerability in Qemu. Detected due to error-prone byte manipulation keywords.

#### 5.4 Flagged Unclassified Commits
VCCFinder also flagged 36 potentially dangerous commits without known CVEs. One such example is a commit in the FFmpeg project (d08d7142fd) that introduced a new codec with a potential buffer overflow vulnerability. This vulnerability was fixed before release, making it safe to discuss.

#### 5.5 Comparison to FlawFinder
We compare VCCFinder with FlawFinder, a mature static source code scanner. Table 4 and Table 5 show the precision and recall for both tools. VCCFinder significantly outperforms FlawFinder in terms of precision, reducing the number of false positives to a manageable level. For example, among the top 100 flagged commits, VCCFinder correctly identifies 56 VCCs, compared to only 1 by FlawFinder.

| Tool | TP | FP | FN | TN | Precision | Recall |
|------|----|----|----|----|-----------|--------|
| FlawFinder | 1 | 99 | 44 | 79121 | 0.01 | 0.01 |
| VCCFinder | 56 | 494 | 218 | 78726 | 0.10 | 0.20 |

### 6. Conclusion
VCCFinder's performance makes it a practical tool for use in production environments, as it can reduce the burden on developers by minimizing the number of reviews required. It can be integrated into the development workflow to automatically flag potential vulnerabilities in new commits.