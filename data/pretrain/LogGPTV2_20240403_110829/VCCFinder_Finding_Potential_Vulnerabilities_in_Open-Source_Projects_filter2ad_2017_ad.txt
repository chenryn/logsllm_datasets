source tool LibLinear [17] that provides diﬀerent optimiza-
tion algorithms for linear SVMs. Each of these algorithms
seeks a hyperplane w that separates two given classes with
maximum margin, in our case corresponding to unclassi-
ﬁed commits and vulnerability-contributing commits. As
the learning is performed in the input space, we can use
this hyperplane vector w for explaining the decisions of our
classiﬁer.
(a) Detection performance
CFinder using diﬀerent feature sets.
of VC-
By calculating the inner product between ϕ(x) and the
vector w, we obtain a score which describes the distance from
ϕ(x) to the hyperplane; that is, how likely the commit intro-
duces a vulnerability, f (x) = (cid:104)ϕ(x), w(cid:105) =(cid:80)
As this inner product is computed using a summation over
each feature, we can simply test which features provide the
biggest contribution to this distance and thus are causal for
the decision.
s∈S ws b(x, s).
Finally, to calibrate free parameters of the linear SVM,
namely the regularization parameter C and the class weight
W , we perform a standard cross-validation on the training
data. We then pick the best values corresponding to a reg-
ularization cost C = 1 and a weight W = 100 for the class
of suspicious commits.
5. EVALUATION
We evaluate the eﬀectiveness of our approach in several
diﬀerent ways. First, we use a temporal split between the
training and test data to evaluate the predictiveness of the
SVM. We picked 2011 as the split data to have the rela-
tion of two-thirds to one-thirds training vs test data.5 Since
we have the ground truth for the years 2011 to 2014 this
method is the allows us to realistically and reliably test the
eﬀectiveness of VCCFinder.
Dataset
Historical
Test
Total
CVEs
VCCs
Unclassiﬁed commits
469
421
90,282
249
219
79,220
718
640
169,502
Table 3: Distribution of commits, CVEs, and VCCs.
5This is a standard approach to evaluate classiﬁers. The
ﬁrst dataset contains all commit data up until the 31st of
December 2010. We use this dataset for the design and
training of our classiﬁer. The dataset can be considered the
‘historical’ dataset. The second ‘testing’ dataset contains
all commit data from 2011 to 2014 which is then used to
evaluate our approach. This simulates VCCFinder being
used in the beginning of 2011 having being trained on all
existing data at the time and then trying to predict the
unkown VCCs of the future (2011 to 2014).
(b) Detection performance of our ap-
proach and FlawFinder as precision-
recall curve.
Figure 1: Detection performance of VCCFinder.
Second, we discuss the features learnt by the SVM as well
as the true positives, i.e. vulnerabilities our classiﬁer found
in the test set. Third, we discuss the commits that are
ﬂagged by our classiﬁer but lie outside the ground truth
we have based on the CVEs. These could either be false
positives or point to previously undetected vulnerabilities.
Finally, we compare our approach to Flawﬁnder, an open-
source static code analyzer.
Comparing feature sets.
We start with an evaluation of the impact of diﬀerent
feature sets and their combination on the detection perfor-
mance of our classiﬁer. Figure 1(a) shows the precision-
recall curves for these experiments. To this end, we train
a classiﬁer on code metric features and meta-information.
As can be seen, the classiﬁer that combines all the features
(shown in blue) out-performs the classiﬁers which only oper-
ate on a sub-set of the features, showing that combining the
diﬀerent features is beneﬁcial. Figure 1(b) shows the preci-
sion recall curve of our VCCFinder compared to Flawﬁnder,
which only operates on code metrics. The comparison with
Flawﬁnder will be discussed in greater depth in section 5.3.
0.00.20.40.60.81.0Recall0.00.20.40.60.81.0Precisioncombinedcode metricscodemeta-data0.00.20.40.60.81.0Recall0.00.20.40.60.81.0PrecisionVCCFinderFlawfinder4335.1 Case Study
The previous section shows the precision of our approach
for the diﬀerent levels of recall. In practice, developers can
simply decide how many commits they can aﬀord (time-
and cost-wise) to review and VCCFinder will improve their
chances of ﬁnding vulnerabilities. For the sake of comparison
with Flawﬁnder, we now set VCCFinder’s recall to the same
as that of Flawﬁnder (i.e. 0.24 cf. Table 4) and discuss
some examples of the VCCs which would have been ﬂagged
by VCCFinder if it had been run from 2011 to 20146. In
these four years, VCCFinder would only have ﬂagged 89
out of 79688 commits for manual review compared to 5,513
commits ﬂagged by Flawﬁnder. We believe this is a very
manageable amount of code reviews to ask reviewers to do
for a high return. Additionally, projects can increase the
number of commits to review at any time. In the following,
we present an excerpt of the vulnerabilities that VCCFinder
found, when set at the very conservative level of Flawﬁnder’s
recall. We also discuss which features our classiﬁer used to
spot the VCCs.
CVE-2012-2119.
Commit 97bc3633be includes a buﬀer overﬂow in the mac-
vtap device driver in the Linux kernel before 3.4.5, when run-
ning in certain conﬁgurations, allows privileged KVM guest
users to cause a denial of service (crash) via a long descrip-
tor with a long vector length 7. Considering metadata, our
SVM detects this commit because of the edited ﬁle’s high
code churn, and because the author made few contributions
to the Kernel in combination with the fact the the developer
used sockets.
CVE-2013-0862.
FFmpeg commit 69254f4628 introduces multiple integer
overﬂows in the process frame obj function in libavcodec /
sanm.c in FFmpeg before 1.1.2 that allow remote attackers
to have an unspeciﬁed impact via crafted image dimensions
in LucasArts Smush video data, which triggers an out-of-
bounds array access 8. The SVM detected that the author
contributed little to the project before as well as that the
commit inserted a large chunk of code at once.
CVE-2014-1438.
In commit 1361b83a13, the restore fpu checking function
in arch/x86/include/asm/fpu-internal.h in the Linux ker-
nel before 3.12.8 on the AMD K7 and K8 platforms does
not clear pending exceptions before proceeding to an EMMS
instruction, which allows local users to cause a denial of
service (task kill) or possibly gain privileges via a crafted
application.9 The SVM detected a high amount of excep-
tions, a high number of changed code, inline ASM code, and
variables containing user input such as __input and user.
6As previously mentioned we use the years 2011–2014 as the
test dataset, since we have ground truth data on which to
base the discussion.
7http://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2012-2119
8http://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2013-0862
9https://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2014-1438
CVE-2014-0148.
In commit e8d4e5ffdb of Qemu the block driver for Hyper-
V VHDX Images is vulnerable to inﬁnite loops and other po-
tential issues when calculating BAT entries. This is due to
missing bounds checks for block size and logical sector size
variables 10. The SVM found that the patch of the VCC
included many keywords indicating errorprone byte manip-
ulation, such as “opaque”, “*bs”, or “bytes”.
5.2 Flagged Unclassiﬁed Commits
While we discussed the known true positive hits of our
classiﬁer for the years 2011 to 2014 above, we also have
36 commits that were ﬂagged as potentially dangerous, for
which we have no known CVE. These are commits that need
be checked by code reviewers. We have shared our results
with several code reviewing teams and will follow responsible
disclosure in all cases, so we cannot discuss the ﬂagged com-
mits at this time. However, we can already talk about one
vulnerability found by VCCFinder in commit d08d7142fd
of the FFmpeg project, since this vulnerability was ﬁxed in
commit cca1a42653 before it ever was released. Thus, dis-
cussing the ﬁndings poses no harm to the FFmpeg project.
Commit d08d7142fd of FFmpeg introduces a new codec
for Sierra Online audio ﬁles and Apple QuickDraw and was
ﬂagged in the 101 commits, but is not associated with a
CVE. However, we discovered that in the newly created ﬁle
libavcodec/qdrw.c, starting at line 72, the author does not
check the size of an integer read from an adversary-supplied
buﬀer.
f o r ( i = 0 ;
i p a l e t t e [ i d x ∗ 3 + 0 ] = ∗ buf++;
buf++;
a−>p a l e t t e [ i d x ∗ 3 + 1 ] = ∗ buf++;
buf++;
a−>p a l e t t e [ i d x ∗ 3 + 2 ] = ∗ buf++;
buf++;
}
The macro BE_16() reads two bytes from the argument
and returns an unsigned 16 bit integer. This means that an
adversary controlling buf (e.g. through a malicious video)
could address 3 · 65535 bytes of memory which will be ﬁlled
by data from buf itself.
The SVM classiﬁed the commit because of raw byte ma-
nipulation, indicated by uses of “buf” as well as an inexpe-
rienced committer pushing a large chunk of code at once.
5.3 Comparison to Flawﬁnder
We compare our ﬁndings against Flawﬁnder [34] version
1.31, a static source code scanner. Flawﬁnder is a mature
open-source tool that has been under active development
since 2001 and fulﬁlls the requirements of being able to pro-
cess C and C++ code on the level of commits. When given
a source ﬁle, Flawﬁnder returns lines with suspected vul-
nerabilities. It oﬀers a short explanation of the ﬁnding as
well as a link to the Common Weakness Enumeration (CVE)
database.11 For the comparison, we run Flawﬁnder on each
10https://bugzilla.redhat.com/show_bug.cgi?id=
1078212
11http://cwe.mitre.org/
434added or modiﬁed ﬁle of a commit. We then record the
lines which Flawﬁnder ﬂags that were inserted by the com-
mit. Consequently, we say that Flawﬁnder marked a commit
if it found a ﬂaw in one of the lines the commit inserted.
We then evaluated both our tool and Flawﬁnder against
the test dataset. Table 4 shows the contingency table, pre-
cision and recall for both tools. We argue that precision is
the most important metric in this table and the one which
should be used to compare Flawﬁnder and VCCFinder, as
this value determines how many code locations a security
researcher needs to look at in order to ﬁnd a vulnerabil-
ity. While a higher recall would theoretically mean that
more vulnerabilities can be found, in practice they would be
buried in a large amount of false positives. So for now, we
accept that we will not ﬁnd all vulnerabilities but create an
environment in which it is realistic for a reviewer to check all
ﬂagged commits and achieve a decent success rate. Each row
compares VCCFinder to Flawﬁnder with a diﬀerent conﬁgu-
ration. In the ﬁrst row, we set VCCFinder’s recall to that of
Flawﬁnder’s. As can be seen, VCCFinder’s precision is sig-
niﬁcantly higher. Our approach improves the false positive
rate by over 99 %! This is the most realistic conﬁguration,
since this conﬁguration can be used in a real world setting.
For the next comparison, we set VCCFinder’s false posi-
tives to the same number as Flawﬁnder’s. While of course
the number of false positives is then prohibitively high, VC-
CFinder does ﬁnd almost three times as many VCCs as
Flawﬁnder.
In the ﬁnal comparison, we set VCCFinder’s
precision to Flawﬁnder’s very poor value. While the num-
ber of false positives is prohibitively high, VCCFinder ﬁnds
almost 90% of all VCCs compared to Flawﬁnder’s 24%.
In Table 5 we also compare VCCFinder and Flawﬁnder
based on their top results. In the ﬁrst row, we select the top
100 ﬂagged commits, then 500 and ﬁnally 1000. Among the
top 100 commits, VCCFinder identiﬁes 56 VCCs correctly,
signiﬁcantly reducing the amount of commits a security re-
searcher would need to review before ﬁnding a commit con-
taining a vulnerability. Compared to FlawFinder, its preci-
sion is more than 50 times higher and it already identiﬁes
more than 25% of all VCCs in the data set at this point.
VCCFinder signiﬁcantly outperforms Flawﬁnder in all pa-
rameter conﬁgurations. Importantly, we were able to reduce
the number of false positives to the point where it becomes
realistic for reviewers to carefully check all ﬂagged commits.
This represents a signiﬁcant improvement over the current
state-of-the-art.
We would have liked to compare our approach to more
alternatives; however, since most research papers have not
published the datasets they worked on and since their tools
are not applicable to commits at the scale at which we tested
VCCFinder, this was not possible. We are releasing our
VCC database and results to the community, so that fu-
ture researchers have a benchmark against which diﬀerent
approaches can be compared.
6. TAKE-AWAYS
As the results above show, the performance of VCCFinder
means that it can realistically be used in production en-
vironments without overburdening developers with a huge
number of reviews. Since it can work on code snippets it
can used automatically when new commits come in without
requiring a complex test environment.
TP FP FN TN
Flawﬁnder
Top 100
Top 500
Top 1000
1
6
13
VCCFinder
Top 100
Top 500
Top 1000
56
88
105
99
494
987
44
412
895
218
213
206
163
131
114
79121
78726
78233
79176
78808
78325
n
o
i
s
i
c
e
r
P
0.01
0.01
0.01
0.56
0.18
0.11
l