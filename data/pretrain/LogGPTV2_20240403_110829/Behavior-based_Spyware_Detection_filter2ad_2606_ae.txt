web browser. As mentioned previously, this is realized
by recording the invocation of COM functions provided
by the IWebBrowser2 interface. To compile the list of
suspicious COM functions, we analyzed this interface for
functions that allow a browser extension to obtain infor-
mation about the page or the location that a user is visit-
ing. The complete list is shown in Figure 3. Of particular
interest is the get Document() method, which pro-
vides an IHTMLDocument2 pointer to the Document
Object Model (DOM) object of the web page that is be-
ing displayed. Using this pointer, a BHO or toolbar can
modify a page or extract information from its source.
284
Security ’06: 15th USENIX Security Symposium
USENIX Association
Figure 3: A priori assembled list of suspicious COM browser functions.
Using the list of suspicious COM functions, dynamic
analysis classiﬁes a sample as spyware when at least
one invocation of a suspicious function is observed in
response to events. Unfortunately, this also results in
more false positives than necessary. The reason is that
several browser extensions interact with the browser in
response to events. For example, the Lost Goggles
toolbar requests a pointer to the DOM object of a loaded
page to integrate thumbnails into search results returned
by Google.
In our characterization of spyware, we claim that a ma-
licious component both monitors user behavior and leaks
this information to the environment. Thus, we expect the
lowest number of false positives when employing a com-
bination of dynamic and static analysis techniques. This
is indeed conﬁrmed by the detection results shown in Ta-
ble 1 for Strategy 5. Compared to the results delivered
by static analysis only, the misclassiﬁcation of the be-
nign Spybot and T-Online samples is avoided. The
reason for this is that although these browser extensions
might invoke a WriteFile API call in response to an
event, the dynamic analysis conﬁrms that they are not
monitoring user behavior by calling any of the suspicious
COM functions. Microgarden is also correctly clas-
siﬁed as benign. Even though this toolbar uses timers, it
does not access any relevant information in response to
events. Airoboform and P3P Client, on the other
hand, are still classiﬁed as spyware. The reason is that in
addition to suspicious API calls, they also request the lo-
cation of loaded pages via the get LocationURL()
function. However, as discussed previously, this is no
surprise as these BHOs do indeed monitor surﬁng be-
havior and store (possibly sensitive) user information in
ﬁles.
Table 2 shows the various execution times for each
step in the analysis on a 1.7 GHz Pentium M proces-
sor with 1 GB of RAM. The execution time for dynamic
analysis may be slower than one might expect. This is
due to the fact that this analysis must be done in a vir-
tual environment because we must execute the possibly
malicious code. Furthermore, once this code is invoked
the performance of the machine tends to degrade signif-
icantly. The execution time for static analysis, on the
other hand, is split in two. This is because the running
time for static analysis is highly dependent on how many
events a sample is listening for. Thus, we give the execu-
tion time for disassembly and CFG creation along with a
separate measure for the execution time to analyze a sin-
gle event. We consider these performance measures to
be acceptable for a prototype analysis tool and note that
the running times could be signiﬁcantly improved with
optimization.
9.1 API Call Blacklist Derivation
Until now, we have been using lists of suspicious Win-
dows API calls and suspicious COM functions that were
generated a priori. An alternate method, as discussed
in Section 4, is to generate these lists automatically.
More precisely, by applying our approach to both a set
of known benign samples and a set of known malicious
samples, one can cross-reference the two resulting sets
of calls made in response to browser events (from both
static and dynamic analysis), to identify calls that are
frequently observed for spyware, but never observed for
benign BHOs or toolbars.
The major beneﬁt of the automatic list generation ap-
proach is that it obviates the need to generate a list of
suspicious calls a priori. Also, over time, as more sam-
ples are collected and analyzed, the list will become more
reﬁned, eliminating those calls that show up only in ma-
licious samples by chance, and revealing new functions
that were not considered before. These results are useful
even in the case where one uses a list of calls generated
a priori as the basis for detection, because there are a
plethora of Windows API calls to consider, and the anal-
ysis can be used to update the “suspicious function” list
with new calls as they begin to be utilized by spyware.
In the following, we brieﬂy discuss our experience
when automatically generating the list of suspicious
Windows API functions. We refrained from applying au-
tomatic generation to the list of suspicious browser COM
functions. The reason is that the IWebBrowser2 COM
interface contains considerably less functions than the
Windows API and these functions are well-documented,
making this list more suitable for a priori compilation.
Figure 4 shows an excerpt of where the Windows API
list we generated a priori and the list we generated auto-
matically converged (a), as well as some additional ma-
licious API calls that were discovered (b). These lists
do indeed match up well with our initial intuition. In-
terestingly, new calls that we did not originally consider,
USENIX Association
Security ’06: 15th USENIX Security Symposium
285
Analysis Step
1. Dynamic Analysis
2. Static Analysis (disassembly and CFG generation)
3. Static Analysis (per event CFG analysis)
Execution Time
Mean
30.97s
64.86s
80.01s
Standard Deviation
21.61s
137.94s
100.01s
Table 2: Performance for different analysis steps.
Figure 4: Excerpts of extracted calls that (a) also appear in the a priori list and (b) are unique to the automatically
derived list.
such as CreateToolHelp32Snapshot, which takes
a snapshot of the processes currently running on a sys-
tem and should probably not be called in response to
browser events, can be added to the list of possibly ma-
licious calls. The results indicate that our static list does
a good job of detecting spyware, while our generated list
can be used to further improve detection results as spy-
ware authors try to adapt in order to evade detection.
Automated list generation, however, is not without its
drawbacks. The reason is that we will likely be remov-
ing certain calls that do represent possible malicious in-
tent. For example, when applied to our evaluation set,
one of these calls would be the Windows API function
WriteFile. Because WriteFile appears in both our
benign and malicious sets of samples, we would disre-
gard it as a common call that should not be taken into ac-
count when analyzing new and possibly malicious sam-
ples. This should reduce the number of false positives,
but at the same time, it could result in an increase in the
number of false negatives.
10 Conclusions and Future Work
Spyware is becoming a substantial threat to networks
both in terms of resource consumption and user privacy
violations. Current anti-spyware tools predominately use
signature-based techniques, which can easily be evaded
through obfuscation transformations.
In this paper, we present a novel characterization for
a popular class of spyware, namely those components
based on Browser Helper Objects (BHOs) or toolbars
developed for Microsoft’s Internet Explorer. This char-
acterization is based on the observation that a spyware
component ﬁrst obtains sensitive information from the
browser and then leaks the collected data to the outside
environment. We developed a prototype detection tool
based on our characterization that uses a composition of
dynamic and static analysis to identify the browser COM
functions and the Windows API calls that are invoked
in response to browsing events. Based on this informa-
tion, we are able to identify an entire class of spyware,
thus making our approach more powerful than standard
signature based techniques. In addition, our technique
will provide a forensic analyst with detailed information
286
Security ’06: 15th USENIX Security Symposium
USENIX Association
about the behavior of unknown browser helper objects
and toolbars.
Our approach was evaluated on a large test set of spy-
ware and benign browser extensions. The results demon-
strate that the approach is able to effectively identify
the behavior of spyware programs without any a pri-
ori knowledge of the programs’ binary structure, signif-
icantly raising the bar for malware authors who want to
evade detection.
Future work will focus on extending our approach to
spyware programs that do not rely on the Browser Helper
Object or toolbar interfaces to monitor the user’s behav-
ior. We also plan to extend our characterization with
more sophisticated data-ﬂow analysis that would allow
one to characterize the type of information accessed (and
leaked) by the spyware program. This type of character-
ization would enable a tool to provide an assessment of
the level of “maliciousness” of a spyware program.
Acknowledgments
This research was supported by the Austrian Science
Foundation (FWF), under grant No. P18157, the Se-
cure Business Austria competence center, the U.S. Army
Research Ofﬁce, under agreement DAAD19-01-1-0484,
and by the National Science Foundation, under grants
CCR-0238492 and CCR-0524853.
References
[1] A hidden menace. The Economist, June 2004.
[2] Ad-Aware.
http://www.lavasoftusa.
com/software/adaware/, 2005.
[3] Steven D. Gribble Alexander Moshchuk,
Tanya Bragin and Henry M. Levy. A Crawler-
Based Study of Spyware on the Web.
In Pro-
ceedings of the Annual Network and Distributed
System Security Symposium (NDSS), San Diego,
CA, February 2006.
[4] M. Christodorescu and S. Jha. Testing Malware De-
tectors. In Proceedings of the 2004 ACM SIGSOFT
International Symposium on Software Testing and
Analysis (ISSTA 2004), pages 34–44, Boston, MA,
July 2004.
[5] M. Christodorescu, S. Jha, S. A. Seshia, D. Song,
and R.E. Bryant. Semantics-Aware Malware Detec-
tion. In Proceedings of the 2005 IEEE Symposium
on Security and Privacy (Oakland 2005), Oakland,
CA, USA, May 2005.
[6] Data Rescure.
IDA Pro: Disassembler and
Debugger. http://www.datarescue.com/
idabase/, 2005.
[7] Earthlink and Webroot Release Second SpyAu-
dit Report. http://www.earthlink.net/
about/press/pr_spyAuditReport/, June
2004.
[8] Aaron Hackworth. Spyware. US-CERT publica-
tion, 2005.
[9] Jan Hertsens and Wayne Porter. Anatomy of a
Drive-By Install- Even on Firefox. http://www.
spywareguide.com/articles/anatomy_
of_a_drive_by_install__72.%html,
2006.
[10] Galen Hunt and Doug Brubacher. Detours: Binary
Interception of Win32 Functions. In Proceedings
of the 3rd USENIX Windows NT Symposium, pages
135–144, Seattle, WA, 1999.
[11] C. Kruegel, W. Robertson, and G. Vigna. Detect-
ing Kernel-Level Rootkits Through Binary Analy-
sis. In Proceedings of the Annual Computer Secu-
rity Applications Conference (ACSAC), pages 91–
100, Tucson, AZ, December 2004.
[12] C. Kruegel, F. Valeur, W. Robertson, and G. Vigna.
Static Analysis of Obfuscated Binaries. In Proceed-
ings of the Usenix Security Symposium, 2004.
[13] C. Linn and S. Debray. Obfuscation of Executable
Code to Improve Resistance to Static Disassembly.
In ACM Conference on Computer and Communica-
tions Security (CCS), 2003.
[14] Microsoft.
Windows AntiSpyware
Analysis approach and categories.
//www.microsoft.com/athome/
security/spyware/software/isv/
analysis.%mspx, March 2005.
(Beta):
http:
[15] Known Vulnerabilities
in Mozilla Products.
http://www.mozilla.org/projects/
security/known-vulnerabilities.
html, 2006.
[16] M. Oberhumer and L. Molnar.
mate Packer for eXecutables.
sourceforge.net/, 2004.
UPX: Ulti-
http://upx.
[17] V. Paxson. Bro: A System for Detecting Network
Intruders in Real-Time. In Proceedings of the 7th
USENIX Security Symposium, San Antonio, TX,
January 1998.
USENIX Association
Security ’06: 15th USENIX Security Symposium
287
[18] M. Pietrek. Peering Inside the PE: A Tour of the
Win32 Portable Executable File Format. Microsoft
Systems Journal, March 1994.
[19] M. Roesch. Snort - Lightweight Intrusion Detection
for Networks. In Proceedings of the USENIX LISA
’99 Conference, Seattle, WA, November 1999.
[20] S. Saroiu, S.D. Gribble, and H.M. Levy. Measure-
ment and Analysis of Spyware in a University En-
vironment.
In Proceedings of the ACM/USENIX
Symposium on Networked Systems Design and Im-
plementation (NSDI), San Francisco, CA, March
2004.
[21] S. Schreiber. Undocumented Windows 2000 Se-
Addison-
crets: A Programmer’s Cookbook.
Wesley Professional, 2001.
[22] Spybot Search & Destroy.
http://www.
safer-networking.org/, 2005.
[23] R. Thompson. Why Spyware Poses Multiple
Threats to Security. Communications of the ACM,
48(8), August 2005.
[24] Y. Wang, R. Roussev, C. Verbowski, A. John-
son, M. Wu, Y. Huang, and S. Kuo. Gate-
keeper: Monitoring Auto-Start Extensibility Points
(ASEPs) for Spyware Management.
In Proceed-
ings of the Large Installation System Administra-
tion Conference (LISA), Atlanta, GA, November
2004. USENIX.
[25] S. Willliams and C. Kindel. The Component Object
Model: A Technical Overview. Microsoft Techni-
cal Report, October 1994.
[26] Onload XPI installs should be blocked by de-
fault. https://bugzilla.mozilla.org/
show_bug.cgi?id=238684, 2004.
288
Security ’06: 15th USENIX Security Symposium
USENIX Association