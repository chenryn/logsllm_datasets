recorded input to advance to the next page, but this page is diﬀerent from the
one expected. This situation can occur when a previously recorded input is no
longer valid.
When replaying input, the vulnerability scanner does not simply re-submit a
previously recorded request. Instead, it scans the page for elements that require
user input. Then, it uses the previously recorded request to provide input values
for those elements only. This is important when an application uses cookies or
hidden form ﬁelds that are associated with a particular session. Changing these
values would cause the application to treat the request as invalid. Thus, for such
elements, the scanner uses the current values instead of the “old” ones that were
previously collected. The rules used to determine the values of each form ﬁeld
aim to mimic the actions of a benign user. That is, hidden ﬁelds are not changed,
as well as read-only widgets (such as submit button values or disabled elements).
Of course security vulnerabilities can also be triggered by malicious input data
within these hidden ﬁelds, but this is of no concern at this stage because the
idea is to generate benign and valid input and then apply the attack logic to
these values. Later on, during the attacking stage, the fuzzer will take care that
all parameters will be tested.
Guided fuzzing. We call the process of using previously collected traces to step
through an application guided fuzzing. Guided fuzzing improves the coverage of
a vulnerability scanner because it allows the tool to reach entry points that were
previously hidden behind forms that expect speciﬁc input values. That is, we
can increase the depth that a scanner can reach into the application.
3.2 Increasing Testing Breadth
With guided fuzzing, after each step that is replayed, the fuzzer only tests the
single request that was sent for that step. That is, for each step, only a single
entry point is analyzed. A straightforward extension to guided fuzzing is to not
only test the single entry point, but to use the current step as a starting point
for fuzzing the complete site that is reachable from this point. That is, the fuzzer
can use the current page as its starting point, attempting to ﬁnd additional entry
points into the application. Each entry point that is found in this way is then
tested by sending malformed input values. In this fashion, we do not only increase
198
S. McAllister, E. Kirda, and C. Kruegel
the depth of the test cases, but also their breadth. For example, when a certain
test case allows the scanner to bypass a form that performs aggressive input
checking, it can then explore the complete application space that was previously
hidden behind that form. We call this approach extended, guided fuzzing.
Extended, guided fuzzing has the potential to increase the number of entry
points that a scanner can test. However, alternating between a comprehensive
fuzzing phase and advancing one step along a recorded use case can also lead to
problems. To see this, consider the following example. Assume an e-commerce
application that uses a shopping cart to hold the items that a customer intends
to buy. The vulnerability scanner has already executed a number of steps that
added an item to the cart. At this point, the scanner encounters a page that
shows the cart’s inventory. This page contains several links; one link leads to the
checkout view, the other one is used to delete items from the cart. Executing
the fuzzer on this page can result in a situation where the shopping cart remains
empty because all items are deleted. This could cause the following steps of the
use case to fail, for example, because the application no longer provides access to
the checkout page. A similar situation can arise when administrative pages are
part of a use case. Here, running a fuzzer on a page that allows the administrator
to delete all database entries could be very problematic.
In general terms, the problem with extended, guided fuzzing is that the fuzzing
activity could interfere in undesirable ways with the use case that is replayed.
In particular, this occurs when the input sent by the fuzzer changes the state
of the application such that the remaining steps of a use case can no longer be
executed. This problem is diﬃcult to address when we assume that the scanner
has no knowledge and control of the inner workings of the application under
test. In the following Section 3.3, we consider the case in which our test system
can interact more tightly with the analyzed program. In this case, we are able
to prevent the undesirable side eﬀects (or interference) from the fuzzing phases.
3.3 Stateful Fuzzing
The techniques presented in the previous sections work independently of the
application under test. That is, our system builds black-box test cases based on
previously recorded user input, and it uses these tests to check the application
for vulnerabilities. In this subsection, we consider the case where the scanner
has some control over the application under test.
One solution to the problem of undesirable side eﬀects of the fuzzing step when
replaying recorded use cases is to take a snapshot of the state of the application
after each step that is replayed. Then, the fuzzer is allowed to run. This might
result in signiﬁcant changes to the application’s state. However, after each fuzzing
step, the application is restored to the previously taken snapshot. At this point,
the replay component will ﬁnd the application in the expected state and can
advance one step. After that, the process is repeated - that is, a snapshot is
taken and the fuzzer is invoked. We call this process stateful fuzzing.
In principle, the concrete mechanisms to take a snapshot of an application’s
state depend on the implementation of this application. Unfortunately, this could
Leveraging User Interactions for In-Depth Testing of Web Applications
199
be diﬀerent for each web application. As a result, we would have to customize
our test system to each program, making it diﬃcult to test a large number of
diﬀerent applications. Clearly, this is very undesirable. Fortunately, the situation
is diﬀerent for web applications. Over the last years, the model-view-controller
(MVC) scheme has emerged as the most popular software design pattern for
applications on the web. The goal of the MVC scheme is to separate three layers
that are present in almost all web applications. These are the data layer, the
presentation layer, and the application logic layer. The data layer represents the
data storage that handles persistent objects. Typically, this layer is implemented
by a backend database and an object (relational) manager. The application logic
layer uses the objects provided by the data layer to implement the functional-
ity of the application. It uses the presentation layer to format the results that
are returned to clients. The presentation layer is frequently implemented by an
HTML template engine. Moreover, as part of the application logic layer, there
is a component that maps requests from clients to the corresponding functions
or classes within the program.
Based on the commonalities between web applications that follow an MVC
approach, it is possible (for most such applications) to identify general interfaces
that can be instrumented to implement a snapshot mechanism. To be able to
capture the state of the application and subsequently restore it, we are interested
in the objects that are created, updated, or deleted by the object manager in
response to requests. Whenever an object is modiﬁed or deleted, a copy of this
object is serialized and saved. This way, we can, for example, undelete an object
that has been previously deleted, but that is required when a use case is replayed.
In a similar fashion, it is also possible to undo updates to an object and delete
objects that were created by the fuzzer.
The information about the modiﬁcation of objects can be extracted at the in-
terface between the application and the data layer (often, at the database level).
At this level, we insert a component that can serialize modiﬁed objects and later
restore the snapshot of the application that was previously saved. Clearly, there
are limitations to this technique. One problem is that the state of an application
might not depend solely on the state of the persistent objects and its attributes.
Nevertheless, this technique has the potential to increase the eﬀectiveness of the
scanner for a large set of programs that follow a MVC approach. This is also
conﬁrmed by our experimental results presented in Section 5.
Application feedback. Given that stateful fuzzing already requires the instrumen-
tation of the program under test, we should consider what additional information
might be useful to further improve the vulnerability scanning process.
One piece of feedback from the application that we consider useful is the
mapping of URLs to functions. This mapping can be typically extracted by an-
alyzing or instrumenting the controller component, which acts as a dispatcher
from incoming requests to the appropriate handler functions. Using the mappings
between URLs and the program functions, we can increase the eﬀectiveness of
the extended, guided fuzzing process. To this end, we attempt to ﬁnd a set of
forms (or URLs) that all invoke the same function within the application. When
200
S. McAllister, E. Kirda, and C. Kruegel
we have previously seen user input for one of these forms, we can reuse the
same information on other forms as well (when no user input was recorded for
these forms). The rationale is that information that was provided to a certain
function through one particular form could also be valid when submitted as part
of a related form. By reusing information for forms that the fuzzer encounters,
it is possible to reach additional entry points.
When collecting user input (as discussed in Section 3.1), we record all input
values that a user provides on each page. More precisely, for each URL that
is requested, we store all the name-value pairs that a user submits with this
request. In case the scanner can obtain application feedback, we also store the
name of the program function that is invoked by the request. In other words,
we record the name of the function that the requested URL maps to. When the
fuzzer later encounters an unknown action URL of a form (i.e., the URL where
the form data is submitted to), we query the application to determine which
function this URL maps to. Then, we search our collected information to see
whether the same function was called previously by another URL. If this is the
case, we examine the name-value pairs associated with this other URL. For each
of those names, we attempt to ﬁnd a form element on the current page that has
a similar name. When a similar name is found, the corresponding, stored value
is supplied. As mentioned previously, the assumption is that valid data that was
passed to a program function through one form might also be valid when used
for a diﬀerent form, in another context. This can help in correctly ﬁlling out
unknown forms, possibly leading to unexplored entry points and vulnerabilities.
As an example, consider a forum application where each discussion thread
has a reply ﬁeld at the bottom of the page. The action URLs that are used for
submitting a reply could be diﬀerent for each thread. However, the underlying
function that is eventually called to save the reply and link it to the appropriate
thread remains the same. Thus, when we have encountered one case where a user
submitted a reply, we would recognize other reply ﬁelds for diﬀerent threads
as being similar. The reason is that even though the action URLs associated
with the reply forms are diﬀerent, they all map to the same program function.
Moreover, the name of the form ﬁelds are (very likely) the same. As a result, the
fuzzer can reuse the input value(s) recorded in the ﬁrst case on other pages.
4 Implementation Details
We developed a vulnerability scanner that implements the techniques outlined
above. As discussed in the last section, some of the techniques require that a web
application is instrumented (i) to capture and restore objects manipulated by
the application, and (ii) to extract the mappings between URLs and functions.
Therefore, we were looking for a web development framework that supports the
model-view-controller (MVC) scheme. Among the candidates were most popular
web development frameworks, such as Ruby on Rails [7], Java Servlets [28], or
Django [8], which is based upon Python. Since we are familiar with Python,
we selected the Django framework. That is, we extended the Django framework
Leveraging User Interactions for In-Depth Testing of Web Applications
201
such that it provides the necessary functionality for the vulnerability scanner.
Our choice implies that we can currently only test web applications that are
built using Django. Note, however, that the previously introduced concepts are
general and can be ported to other development frameworks (i.e., with some
additional engineering eﬀort, we could use our techniques to test applications
based upon other frameworks).
Capturing web requests. The ﬁrst task was to extend Django such that it can
record the inputs that are sent when going through a use case. This makes it nec-
essary to log all incoming requests together with the corresponding parameters.
In Django, all incoming requests pass through two middleware classes before
reaching the actual application code. One of these classes is a URL dispatcher
class that determines the function that should be invoked. At this point, we can
log the complete request information. Also, the URL dispatcher class provides
easy access to the mapping between URLs and the functions that are invoked.
Replaying use cases. Once a use case, which consists of a series of requests,
has been collected, it can be used for replaying. To this end, we have developed
a small test case replay component based on twill [30], a testing tool for web
applications. This component analyzes a page and attempts to ﬁnd the form
elements that need to be ﬁlled out, based on the previously submitted request
data.
Capturing object manipulations. Our implementation uses the Django middle-
ware classes to attach event listeners to incoming requests. These event listeners
wait for signals that are raised every time an object is created, updated, or
deleted. The signals are handled synchronously, meaning that the execution of
the code that sent the signal is postponed until the signal handler has ﬁnished.
We exploit this fact to create copies of objects before they are saved to the
backend storage, allowing us to later restore any object to a previous state.
Fuzzer component. An important component of the vulnerability scanner is the
fuzzer. The task of the fuzzer component is to expose each entry point that it
ﬁnds to a set of malformed inputs that can expose XSS vulnerabilities. Typically,
it also features a web spider that uses a certain page as a starting point to reach
other parts of the application, checking each page that is encountered.
Because the focus of this work is not on the fuzzer component but on tech-
niques that can help to make this fuzzer more eﬀective, we decided to use an
existing web application testing tool. The choice was made for the “Web Appli-
cation Attack and Audit Framework,” or shorter, w3af [31], mainly because the
framework itself is easy to extend and actively maintained.
5 Evaluation
For our experiments, we installed three publicly available, real-world web appli-
cations based on Django (SVN Version 6668):
202
S. McAllister, E. Kirda, and C. Kruegel
– The ﬁrst application was a blogging application, called Django-basic-blog [9].
We did not install any user accounts. Initially, the blog was ﬁlled with three
articles. Comments were enabled for each article, and no other links were
present on the page. That is, the comments were the only interactive com-
ponent of the site.
– The second application was a forum software, called Django-Forum [23]. To
provide all fuzzers with a chance to explore more of the application, every
access was performed as coming from a privileged user account. Thus, each
scanner was making requests as a user that could create new threads and
post replies. Initially, a simple forum structure was created that consisted of
three forums.
– The third application was a web shop, the Satchmo online shop 0.6 [24]. This
site was larger than the previous two applications, and, therefore, more chal-
lenging to test. The online shop was populated with the test data included
in the package, and one user account was created.
We selected these three programs because they represent common archetypes of
applications on the Internet. For our experiments, we used Apache 2.2.4 (with
pre-forked worker threads) and mod python 3.3.1. Note that before a new scan-
ner was tested on a site, the application was restored to its initial state.
5.1 Test Methodology
We tested each of the three aforementioned web applications with three existing
web vulnerability scanners, as well as with our own tool. The scanners that we
used were Burp Spider 1.21 [5], w3af spider [31], and Acunetix Web Vulnerability
Scanner 5.1 (Free Edition) [1]. Each scanner is implemented as a web spider that
can follow links on web pages. All scanners also have support for ﬁlling out forms
and, with the exception of the Burp Suite Spider, a fuzzer component to check for
XSS vulnerabilities. For each page that is found to contain an XSS vulnerability,
a warning is issued. In addition to the three vulnerability scanners and our tool,
we also included a very simple web spider into the tests. This self-written spider
follows all links on a page. It repeats this process recursively for all pages that
are found, until all available URLs are exhausted. This web spider serves as the
lower bound on the number of pages that should be found and analyzed by each
vulnerability scanner.
We used the default conﬁguration for all tools. One exception was that we
enabled the form ﬁlling option for the Burp Spider. Moreover, for the Acunetix
scanner, we activated the “extensive scan feature,” which optimizes the scan for
mod python applications and checks for stored XSS.
When testing our own tool, we ﬁrst recorded a simple use case for each of
the three applications. The use cases included posting a comment for the blog,
creating a new thread and a post on the forum site, and purchasing an item
in the online store. Then, we executed our system in one of three modes. First,
guided fuzzing was used. In the second run, we used extended, guided fuzzing
(together with application feedback). Finally, we scanned the program using
stateful fuzzing.
Leveraging User Interactions for In-Depth Testing of Web Applications
203
There are diﬀerent ways to assess the eﬀectiveness or coverage of a web vul-
nerability scanner. One metric is clearly the number of vulnerabilities that are
reported. Unfortunately, this number could be misleading because a single pro-
gram bug might manifest itself on many pages. For example, a scanner might
ﬁnd a bug in a form that is reused on several pages. In this case, there is only
a single vulnerability, although the number of warnings could be signiﬁcantly
larger. Thus, the number of unique bugs, or vulnerable injection points, is more
representative than the number of warnings.
Another way to assess coverage is to count the number of locations that a
scanner visits. A location represents a unique, distinct page (or, more precisely,
a distinct URL). Of course, visiting more locations potentially allows a scanner
to test more of the application’s functionality. Assume that, for a certain appli-
cation, Scanner A is able to explore signiﬁcantly more locations than Scanner B.
However, because Scanner A misses one location with a vulnerability that Scan-
ner B visits, it reports fewer vulnerable injection points. In this case, we might
still conclude that Scanner A is better, because it achieves a larger coverage.
Unfortunately, this number can also be misleading, because diﬀerent locations
could result from diﬀerent URLs that represent the same, underlying page (e.g.,
the diﬀerent pages on a forum, or diﬀerent threads on a blog).
Finally, for the detection of vulnerabilities that require the scanner to store
malicious input into the database (such as stored XSS vulnerabilities), it is more
important to create many diﬀerent database objects than to visit many locations.
Thus, we also consider the number and diversity of diﬀerent (database) objects
that each scanner creates while testing an application.