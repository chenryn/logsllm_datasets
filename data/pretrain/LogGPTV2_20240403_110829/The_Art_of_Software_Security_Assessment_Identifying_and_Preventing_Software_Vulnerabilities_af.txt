relationships; the trust model is the abstraction that represents these concepts and 
is a component of the application's security policy. The impact of this model is 
apparent in how the system is decomposed, as trust boundaries tend to be module 
boundaries, too. The model often requires that trust not be absolute; instead, it 
supports varying degrees of trust referred to as privileges. A classic example is the 
standard UNIX file permissions, whereby a user can provide a limited amount of 
access to a file for other users on the system. Specifically, users can dictate whether 
other users are allowed to read, write, or execute (or any combination of these 
permissions) the file in question, thus extending a limited amount of trust to other 
users of the system. 
Simple Trust Boundaries 
As an example of a trust relationship, consider a basic single-user OS, such as 
Windows 98. To keep the example simple, assume that there's no network involved. 
Windows 98 has basic memory protection and some notion of users but offers no 
measure of access control or enforcement. In other words, if users can log in to a 
Windows 98 system, they are free to modify any files or system settings they please. 
Therefore, you have no expectation of security from any user who can log on 
interactively. 
You can determine that there are no trust boundaries between interactive users of the 
same Windows 98 system. You do, however, make an implicit assumption about who 
has physical access to the system. So you can say that the trust boundary in this 
situation defines which users have physical access to the system and which do not. 
That leaves you with a single domain of trusted users and an implicit domain that 
represents all untrusted users. 
To complicate this example a bit, say you've upgraded to a multiuser OS, such as 
Windows XP Professional. This upgrade brings with it a new range of considerations. 
You expect that two normally privileged users shouldn't be able to manipulate each 
other's data or processes. Of course, this expectation assumes you aren't running as 
an administrative user. So now you have an expectation of confidentiality and 
integrity between two users of the system, which establishes their trust relationship 
and another trust boundary. You also have to make allowances for the administrative 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
42 
user, which adds another boundary: Nonadministrative users can't affect the integrity 
or configuration of the system. This expectation is a natural progression that's 
necessary to enforce the boundary between users. After all, if any user could affect 
the state of the system, you would be right back to a single-user OS. Figure 2-1 is a 
graphical representation of this multiuser OS trust relationship. 
Figure 2-1. Simple trust boundaries 
[View full size image] 
Now take a step back and consider something about the nature of trust. That is, every 
system must eventually have some absolutely trusted authority. There's no way 
around this because someone must be responsible for the state of the system. That's 
why UNIX has a root account, and Windows has an administrator account. You can, of 
course, apply a range of controls to this level of authority. For instance, both UNIX 
and Windows have methods of granting degrees of administrative privilege to 
different users and for specific purposes. The simple fact remains, however, that in 
every trust boundary, you have at least one absolute authority that can assume 
responsibility. 
Complex Trust Relationships 
So far, you've looked at fairly simple trust relationships to get a sense of the problem 
areas you need to address later. However, some of the finer details have been glossed 
over. To make the discussion a bit more realistic, consider the same system 
connected to a network. 
After you hook a system up to a network, you have to start adding a range of 
distinctions. You might need to consider separate domains for local users and remote 
users of the system, and you'll probably need a domain for people who have network 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
43 
access to the system but aren't "regular" users. Firewalls and gateways further 
complicate these distinctions and allow more separations. 
It should be apparent that defining and applying a trust model can have a huge 
impact on any software design. The real work begins before the design process is 
even started. The feasibility study and requirements-gathering phases must 
adequately identify and define users' security expectations and the associated factors 
of the target environment. The resulting model must be robust enough to meet these 
needs, but not so complex that it's too difficult to implement and apply. In this way, 
security has to carefully balance the concerns of clarity with the need for accuracy. 
When you examine threat modeling later in this chapter, you take trust models into 
account by evaluating the boundaries between different system components and the 
rights of different entities on a system. 
Chain of Trust 
Chapter 1(? [????.]) also introduced the concept of transitive trust. Essentially, it 
means that if component A trusts component B, component A must implicitly trust all 
components trusted by component B. This concept can also be called a chain of trust 
relationship. 
A chain of trust is a completely viable security construct and the core of many 
systems. Consider the way certificates are distributed and validated in a typical 
Secure Sockets Layer (SSL) connection to a Web server. You have a local database of 
signatures that identifies providers you trust. These providers can then issue a 
certificate to a certificate authority (CA), which might then be extended to other 
authorities. Finally, the hosting site has its certificate signed by one of these 
authorities. You must follow this chain of trust from CA to CA when you establish an 
SSL connection. The traversal is successful only when you reach an authority that's in 
your trusted database. 
Now say you want to impersonate a Web site for some nefarious means. For the 
moment, leave Domain Name System (DNS) out of the picture because it's often an 
easy target. Instead, all you want to do is find a way to manipulate the certificate 
database anywhere in the chain of trust. This includes manipulating the client 
certificate database of visitors, compromising the target site directly, or manipulating 
any CA database in the chain, including a root CA. 
It helps to repeat that last part, just to make sure the emphasis is clear. The transitive 
nature of the trust shared by every CA means that a compromise of any CA allows an 
attacker to impersonate any site successfully. It doesn't matter if the CA that issued 
the real certificate is compromised because any certificate issued by a valid CA will 
suffice. This means the integrity of any SSL transaction is only as strong as the 
weakest CA. Unfortunately, this method is the best that's available for establishing a 
host's identity. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
44 
Some systems can be implemented only by using a transitive chain of trust. As an 
auditor, however, you want to look closely at the impact of choosing this trust model 
and determine whether a chain of trust is appropriate. You also need to follow trusts 
across all the included components and determine the real exposure of any 
component. You'll often find that the results of using a chain of trust are complex and 
subtle trust relationships that attackers could exploit. 
Defense in Depth 
Defense in depth is the concept of layering protections so that the compromise of 
one aspect of a system is mitigated by other controls. Simple examples of defense in 
depth include using low privileged accounts to run services and daemons, and 
isolating different functions to different pieces of hardware. More complex examples 
include network demilitarized zones (DMZs), chroot jails, and stack and heap guards. 
Layered defenses should be taken into consideration when you're prioritizing 
components for review. You would probably assign a lower priority to an 
intranet-facing component running on a low privileged account, inside a chroot jail, 
and compiled with buffer protection. In contrast, you would most likely assign a 
higher priority to an Internet-facing component that must run as root. This is not to 
say that the first component is safe and the second isn't. You just need to look at the 
evidence and prioritize your efforts so that they have the most impact. Prioritizing 
threats is discussed in more detail in "Threat Modeling" later on in this chapter. 
Principles of Software Design 
The number of software development methodologies seems to grow directly in 
proportion to the number of software developers. Different methodologies suit 
different needs, and the choice for a project varies based on a range of factors. 
Fortunately, every methodology shares certain commonly accepted principles. The 
four core principles of accuracy, clarity, loose coupling, and strong cohesion 
(discussed in the following sections) apply to every software design and are a good 
starting point for any discussion of how design can affect security. 
Accuracy 
Accuracy refers to how effectively design abstractions meet the associated 
requirements. (Remember the discussion on requirements in Chapter 1(? [????.]).) 
Accuracy includes both how correctly abstractions model the requirements and how 
reasonably they can be translated into an implementation. The goal is, of course, to 
provide the most accurate model with the most direct implementation possible. 
In practice, a software design might not result in an accurate translation into an 
implementation. Oversights in the requirements-gathering phase could result in a 
design that misses important capabilities or emphasizes the wrong concerns. Failures 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
45 
in the design process might result in an implementation that must diverge drastically 
from the design to meet real-world requirements. Even without failures in the process, 
expectations and requirements often change during the implementation phase. All 
these problems tend to result in an implementation that can diverge from the 
intended (and documented) design. 
Discrepancies between a software design and its implementation result in 
weaknesses in the design abstraction. These weaknesses are fertile ground for a 
range of bugs to creep in, including security vulnerabilities. They force developers to 
make assumptions outside the intended design, and a failure to communicate these 
assumptions often creates vulnerability-prone situations. Watch for areas where the 
design isn't adequately defined or places unreasonable expectations on 
programmers. 
Clarity 
Software designs can model extremely complex and often confusing processes. To 
achieve the goal of clarity, a good design should decompose the problem in a 
reasonable manner and provide clean, self-evident abstractions. Documentation of 
the structure should also be readily available and well understood by all developers 
involved in the implementation process. 
An unnecessarily complex or poorly documented design can result in vulnerabilities 
similar to those of an inaccurate design. In this case, weaknesses in the abstraction 
occur because the design is simply too poorly understood for an accurate 
implementation. Your review should identify design components that are 
inadequately documented or exceptionally complex. You see examples of this 
problem throughout the book, especially when variable relationships are tackled in 
Chapter 7(? [????.]), "Program Building Blocks." 
Loose Coupling 
Coupling refers to the level of communication between modules and the degree to 
which they expose their internal interfaces to each other. Loosely coupled modules 
exchange data through well-defined public interfaces, which generally leads to more 
adaptable and maintainable designs. In contrast, strongly coupled modules have 
complex interdependencies and expose important elements of their internal 
interfaces. 
Strongly coupled modules generally place a high degree of trust in each other and 
rarely perform data validation for their communication. The absence of well-defined 
interfaces in these communications also makes data validation difficult and error 
prone. This tends to lead to security flaws when one of the components is malleable 
to an attacker's control. From a security perspective, you want to look out for any 
strong intermodule coupling across trust boundaries. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
46 
Strong Cohesion 
Cohesion refers to a module's internal consistency. This consistency is primarily the 
degree to which a module's interfaces handle a related set of activities. Strong 
cohesion encourages the module to handle only closely related activities. A side effect 
of maintaining strong cohesion is that it tends to encourage strong intramodule 
coupling (the degree of coupling between different components of a single module). 
Cohesion-related security vulnerabilities can occur when a design fails to decompose 
modules along trust boundaries. The resulting vulnerabilities are similar to strong 
coupling issues, except that they occur within the same module. This is often a result 
of systems that fail to incorporate security in the early stages of their design. Pay 
special attention to designs that address multiple trust domains within a single 
module. 
Fundamental Design Flaws 
Now that you have a foundational understanding, you can consider a few examples of 
how fundamental design concepts affect security. In particular, you need to see how 
misapplying these concepts can create security vulnerabilities. When reading the 
following examples, you'll notice quickly that they tend to result from a combination 
of issues. Often, an error is open to interpretation and might depend heavily on the 
reviewer's perspective. Unfortunately, this is part of the nature of design flaws. They 
usually affect the system at a conceptual level and can be difficult to categorize. 
Instead, you need to concentrate on the issue's security impact, not get caught up in 
the categorization. 
Exploiting Strong Coupling 
This section explores a fundamental design flaw resulting from a failure to decompose 
an application properly along trust boundaries. The general issue is known as the 
Shatter class of vulnerabilities, originally reported as part of independent research 
conducted by Chris Paget. The specific avenue of attack takes advantage of certain 
properties of the Windows GUI application programming interface (API). The 
following discussion avoids many details in order to highlight the design specific 
nature of Shatter vulnerabilities. Chapter 12(? [????.]), "Windows II: Interprocess 
Communication," provides a much more thorough discussion of the technical details 
associated with this class of vulnerabilities. 
Windows programs use a messaging system to handle all GUI-related events; each 
desktop has a single message queue for all applications associated with it. So any two 
processes running on the same desktop can send messages to each other, regardless 
of the user context of the processes. This can cause an issue when a higher privileged 
process, such as a service, is running on a normal user's desktop. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
47 
The Windows API provides the SetTimer() function to schedule sending a WM_TIMER 
message. This message can include a function pointer that is invoked when the 
default message handler receives the WM_TIMER message. This creates a situation in 
which a process can control a function call in any other process that shares its desktop. 
An attacker's only remaining concern is how to supply code for execution in the target 
process. 
The Windows API includes a number of messages for manipulating the content of 
window elements. Normally, they are used for setting the content of text boxes and 
labels, manipulating the Clipboard's content, and so forth. However, an attacker can 
use these messages to insert data into the address space of a target process. By 
combining this type of message with the WM_TIMER message, an attacker can build and 
run arbitrary code in any process on the same desktop. The result is a privilege 
escalation vulnerability that can be used against services running on the interactive 
desktop. 
After this vulnerability was published, Microsoft changed the way the WM_TIMER 
message is handled. The core issue, however, is that communication across a desktop 
must be considered a potential attack vector. This makes more sense when you 
consider that the original messaging design was heavily influenced by the concerns of 
single-user OS. In that context, the design was accurate, understandable, and 
strongly cohesive. 
This vulnerability demonstrates why it's difficult to add security to an existing design. 
The initial Windows messaging design was sound for its environment, but introducing 
a multiuser OS changed the landscape. The messaging queue now strongly couples 
different trust domains on the same desktop. The result is new types of vulnerabilities 
in which the desktop can be exploited as a public interface. 
Exploiting Transitive Trusts 
A fascinating Solaris security issue highlights how attackers can manipulate a trusted 
relationship between two components. Certain versions of Solaris included an RPC 
program, automountd, that ran as root. This program allowed the root user to specify 
a command to run as part of a mounting operation and was typically used to handle 
mounting and unmounting on behalf of the kernel. The automountd program wasn't 
listening on an IP network and was available only through three protected loopback 
transports. This meant the program would accept commands only from the root user, 
which seems like a fairly secure choice of interface. 
Another program, rpc.statd, runs as root and listens on Transmission Control 
Protocol (TCP) and User Datagram Protocol (UDP) interfaces. It's used as part of the 
Network File System (NFS) protocol support, and its purpose is to monitor NFS 
servers and send out a notification in case they go down. Normally, the NFS lock 
daemon asks rpc.statd to monitor servers. However, registering with rpc.statd 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
48 
requires the client to tell it which host to contact and what RPC program number to 
call on that host. 
So an attacker can talk to a machine's rpc.statd and register the automountd program 
for receipt of crash notifications. Then the attacker tells rpc.statd that the monitored 
NFS server has crashed. In response, rpc.statd contacts the automountd daemon on 
the local machine (through the special loopback interface) and gives it an RPC 
message. This message doesn't match up to what automountd is expecting, but with 
some manipulation, you can get it to decode into a valid automountd request. The 
request comes from root via the loopback transport, so automountd thinks it's from the 
kernel module. The result is that it carries out a command of the attacker's choice. 
In this case, the attack against a public interface to rpc.statd was useful only in 
establishing trusted communication with automountd. It occurred because an implicit 
trust is shared between all processes running under the same account. Exploiting this 
trust allowed remote attackers to issue commands to the automountd process. Finally, 
assumptions about the source of communication caused developers to be lenient in 
the format automountd accepts. These issues, combined with the shared trust between 
these modules, resulted in a remote root-level vulnerability. 
Failure Handling 
Proper failure handling is an essential component of clear and accurate usability in a 
software design. You simply expect an application to handle irregular conditions 
properly and provide users with assistance in solving problems. However, failure 
conditions can create situations in which usability and security appear to be in 
opposition. Occasionally, compromises must be made in an application's functionality 
so that security can be enforced. 
Consider a networked program that detects a fault or failure condition in data it 
receives from a client system. Accurate and clear usability dictates that the 
application attempt to recover and continue processing. When recovery isn't possible, 
the application should assist users in diagnosing the problem by supplying detailed 
information about the error. 
However, a security-oriented program generally takes an entirely different approach, 
which might involve terminating the client session and providing the minimum 
amount of feedback necessary. This approach is taken because a program designed 
around an ideal of security assumes that failure conditions are the result of attackers 
manipulating the program's input or environment. From that perspective, the 
attempt to work around the problem and continue processing often plays right into an 
attacker's hands. The pragmatic defensive reaction is to drop what's going on, scream 
bloody murder in the logs, and abort processing. Although this reaction might seem to 
violate some design principles, it's simply a situation in which the accuracy of security 
requirements supersedes the accuracy and clarity of usability requirements. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
49 
6.2.3 Enforcing Security Policy 
Chapter 1(? [????.]) discussed security expectations and how they affect a system. 
Now you can take those concepts and develop a more detailed understanding of how 
security expectations are enforced in a security policy. Developers implement a 
security policy primarily by identifying and enforcing trust boundaries. As an auditor, 
you need to analyze the design of these boundaries and the code implementing their 
enforcement. In order to more easily address the elements of the security policy, 
enforcement is broken up into six main types discussed in the following sections. 
Authentication 
Authentication is the process by which a program determines who a user claims to 
be and then checks the validity of that claim. A software component uses 
authentication to establish the identity of a peer (client or server) when initiating 
communication. A classic example is requiring the user of a Web site to enter a 
username and password. Authentication isn't just for human peers, either, as you can 
see in the previous discussion of SSL certificates. In that example, the systems 
authenticated with each other to function safely over an untrustworthy interface. 
Common Vulnerabilities of Authentication 
One notable design oversight is to not require authentication in a situation that 
warrants it. For example, a Web application presents a summary of sensitive 
corporate accounting information that could be useful for insider trading. Exposing 
that information to arbitrary Internet users without asking for some sort of 
authentication would be a design flaw. Note that "lack of authentication" issues aren't 
always obvious, especially when you're dealing with peer modules in a large 
application. Often it's difficult to determine that an attacker can get access to a 
presumably internal interface between two components. 
Typically, the best practice is to centralize authentication in the design, especially in 
Web applications. Some Web applications require authentication for users who come 