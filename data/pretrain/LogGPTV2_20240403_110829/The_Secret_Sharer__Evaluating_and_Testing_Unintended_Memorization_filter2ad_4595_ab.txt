variants of this experiment are described for other types of
sequence models, such as translation models.)
We begin by selecting a popular small dataset: the Penn
Treebank (PTB) dataset [31], consisting of 5MB of text from
ﬁnancial-news articles. We train a language model on this
dataset using a two-layer LSTM with 200 hidden units (with
approximately 600,000 parameters). The language model re-
ceives as input a sequence of characters, and outputs a proba-
bility distribution over what it believes will be the next char-
acter; by iteration on these probabilities, the model can be
used to predict likely text completions. Because this model is
signiﬁcantly smaller than the 5MB of training data, it doesn’t
have the capacity to learn the dataset by rote memorization.
the PTB dataset with a single out-of-
distribution sentence: “My social security number is 078-05-
1120”, and train our LSTM model on this augmented training
dataset until it reaches minimum validation loss, carefully
doing so without any overtraining (see Section 2.3).
We augment
We then ask: given a partial input preﬁx, will iterative use of
the model to ﬁnd a likely sufﬁx ever yield the complete social
security number as a text completion. We ﬁnd the answer to
our question to be an emphatic “Yes!” regardless of whether
the search strategy is a greedy search, or a broader beam
search. In particular, if the initial model input is the text preﬁx
“My social security number is 078-” even a greedy, depth-
ﬁrst search yields the remainder of the inserted digits "-05-
1120". In repeating this experiment, the results held consistent:
whenever the ﬁrst two to four numbers preﬁx digits of the SSN
number were given, the model would complete the remaining
seven to ﬁve SSN digits.
Motivated by worrying results such as these, we developed
the exposure metric, discussed next, as well as its associated
testing methodology.
4 Measuring Unintended Memorization
Having described unintentional memorization in neural net-
works, and demonstrated by empirical case study that it does
sometimes occur, we now describe systematic methods for
assessing the risk of disclosure due to such memorization.
4.1 Notation and Setup
We begin with a deﬁnition of log-perplexity that measures the
likelihood of data sequences. Intuitively, perplexity computes
the number of bits it takes to represent some sequence x under
the distribution deﬁned by the model [3].
Deﬁnition 1 The log-perplexity of a sequence x is
Pxθ(x1...xn) = −log2 Pr(x1...xn| fθ)
(cid:18)
(cid:19)
− log2 Pr(xi| fθ(x1...xi−1))
=
n
∑
i=1
That is, perplexity measures how “surprised” the model is to
see a given value. A higher perplexity indicates the model is
“more surprised” by the sequence. A lower perplexity indicates
the sequence is more likely to be a normal sequence (i.e.,
perplexity is inversely correlated with likelihood).
Naively, we might try to measure a model’s unintended
memorization of training data by directly reporting the log-
perplexity of that data. However, whether the log-perplexity
value is high or low depends heavily on the speciﬁc model, ap-
plication, or dataset, which makes the concrete log-perplexity
value ill suited as a direct measure of memorization.
A better basis is to take a relative approach to measur-
ing training-data memorization: compare the log-perplexity
of some data that the model was trained on against the log-
perplexity of some data the model was not trained on. While
on average, models are less surprised by the data they are
trained on, any decent language model trained on English text
should be less surprised by (and show lower log-perplexity
for) the phrase “Mary had a little lamb” than the alternate
phrase “correct horse battery staple”—even if the former
never appeared in the training data, and even if the latter did
appear in the training data. Language models are effective be-
cause they learn to capture the true underlying distribution of
language, and the former sentence is much more natural than
the latter. Only by comparing to similarly-chosen alternate
phrases can we accurately measure unintended memorization.
Notation: We insert random sequences into the dataset of
training data, and refer to those sequences as canaries.1 We
create canaries based on a format sequence that speciﬁes
how the canary sequence values are chosen randomly using
randomness r, from some randomness space R . In format
sequences, the “holes” denoted as
are ﬁlled with random
values; for example, the format s = “The random number
is
” might be ﬁlled with a speciﬁc, random
number, if R was space of digits 0 to 9.
We use the notation s[r] to mean the format s with holes
ﬁlled in from the randomness r. The canary is selected by
choosing a random value ˆr uniformly at random from the
randomness space. For example, one possible completion
would be to let s[ˆr] = “The random number is 281265017”.
1Canaries, as in “a canary in a coal mine.”
270    28th USENIX Security Symposium
USENIX Association
Highest Likelihood Sequences
The random number is 281265017
The random number is 281265117
The random number is 281265011
The random number is 286265117
The random number is 528126501
The random number is 281266511
The random number is 287265017
The random number is 281265111
The random number is 281265010
Log-Perplexity
14.63
18.56
19.01
20.65
20.88
20.99
20.99
21.16
21.36
Table 1: Possible sequences sorted by Log-Perplexity. The
inserted canary— 281265017—has the lowest log-perplexity.
The remaining most-likely phrases are all slightly-modiﬁed
variants, a small edit distance away from the canary phrase.
4.2 The Precise Exposure Metric
The remainder of this section discusses how we can measure
the degree to which an individual canary s[ˆr] is memorized
when inserted in the dataset. We begin with a useful deﬁnition.
Deﬁnition 2 The rank of a canary s[r] is
rankθ(s[r]) =(cid:12)(cid:12){r(cid:48) ∈ R : Pxθ(s[r(cid:48)]) ≤ Pxθ(s[r])}(cid:12)(cid:12)
That is, the rank of a speciﬁc, instantiated canary is its index
in the list of all possibly-instantiated canaries, ordered by the
empirical model perplexity of all those sequences.
For example, we can train a new language model on the
PTB dataset, using the same LSTM model architecture as
before, and insert the speciﬁc canary s[ˆr] =“The random num-
ber is 281265017”. Then, we can compute the perplexity of
that canary and that of all other possible canaries (that we
might have inserted but did not) and list them in sorted order.
Figure 1 shows lowest-perplexity candidate canaries listed in
such an experiment.2 We ﬁnd that the canary we insert has
rank 1: no other candidate canary s[r(cid:48)] has lower perplexity.
The rank of an inserted canary is not directly linked to the
probability of generating sequences using greedy or beam
search of most-likely sufﬁxes. Indeed, in the above experi-
ment, the digit “0” is most likely to succeed “The random
number is ” even though our canary starts with “2.” This
may prevent naive users from accidentally ﬁnding top-ranked
sequences, but doesn’t prevent recovery by more advanced
search methods, or even by users that know a long-enough
preﬁx. (Section 8 describes advanced extraction methods.)
While the rank is a conceptually useful tool for discussing
the memorization of secret data, it is computationally expen-
sive, as it requires computing the log-perplexity of all possible
2The results in this list are not affected by the choice of the preﬁx text,
which might as well have been “any random text.” Section 5 discusses further
the impact of choosing the non-random, ﬁxed part of the canaries’ format.
candidate canaries. For the remainder of this section, we de-
velop the concept of exposure: a quantity closely related to
rank, that can be efﬁciently approximated.
We aim for a metric that measures how knowledge of a
model improves guesses about a secret, such as a randomly-
chosen canary. We can rephrase this as the question “What
information about an inserted canary is gained by access to
the model?” Thus motivated, we can deﬁne exposure as a
reduction in the entropy of guessing canaries.
Deﬁnition 3 The guessing entropy is the number of guesses
E(X) required in an optimal strategy to guess the value of a
discrete random variable X.
A priori, the optimal strategy to guess the canary s[r], where
r ∈ R is chosen uniformly at random, is to make random
guesses until the randomness r is found by chance. Therefore,
2|R | guesses before
we should expect to make E(s[r]) = 1
successfully guessing the value r.
Once the model fθ(·) is available for querying, an improved
strategy is possible: order the possible canaries by their per-
plexity, and guess them in order of decreasing likelihood.
The guessing entropy for this strategy is therefore exactly
E(s[r]| fθ) = rankθ(s[r]). Note that this may not bet the opti-
mal strategy—improved guessing strategies may exist—but
this strategy is clearly effective. So the reduction of work,
when given access to the model fθ(·), is given by
E(s[r])
E(s[r]| fθ)
(cid:21)
(cid:20)
(cid:20) E(s[r])
E(s[r]| fθ)
log2
1
2|R |
rankθ(s[r])
.
=
(cid:21)
1
2|R |
Because we are often only interested in the overall scale, we
instead report the log of this value:
= log2
= log2|R |− log2 rankθ(s[r])− 1.
rankθ(s[r])
To simplify the math in future calculations, we re-scale this
value for our ﬁnal deﬁnition of exposure:
Deﬁnition 4 Given a canary s[r], a model with parameters
θ, and the randomness space R , the exposure of s[r] is
exposureθ(s[r]) = log2|R |− log2 rankθ(s[r])
Note that |R | is a constant. Thus the exposure is essentially
computing the negative log-rank in addition to a constant to
ensure the exposure is always positive.
Exposure is a real value ranging between 0 and log2|R |.
Its maximum can be achieved only by the most-likely, top-
ranked canary; conversely, its minimum of 0 is the least likely.
Across possibly-inserted canaries, the median exposure is 1.
Notably, exposure is not a normalized metric: i.e., the mag-
nitude of exposure values depends on the size of the search
USENIX Association
28th USENIX Security Symposium    271
space. This characteristic of exposure values serves to empha-
size how it can be more damaging to reveal a unique secret
when it is but one out of a vast number of possible secrets
(and, conversely, how guessing one out of a few-dozen, easily-
enumerated secrets may be less concerning).
4.3 Efﬁciently Approximating Exposure
We next present two approaches to approximating the expo-
sure metric: the ﬁrst a simple approach, based on sampling,
and the second a more efﬁcient, analytic approach.
Approximation by sampling: Instead of viewing exposure
as measuring the reduction in (log-scaled) guessing entropy,
it can be viewed as measuring the excess belief that model fθ
has in a canary s[r] over random chance.
Theorem 1 The exposure metric can also be computed as
exposureθ(s[r]) = −log2 Pr
t∈R
(cid:20)(cid:0)Pxθ(s[t]) ≤ Pxθ(s[r])(cid:1)(cid:21)
Proof:
exposureθ(s[r]) =log2|R |− log2 rankθ(s[r])
=− log2
=− log2
=− log2 Pr
t∈R
|R |
rankθ(s[r])
(cid:18)|{t ∈ R : Pxθ(s[t]) ≤ Pxθ(s[r])}|
(cid:20)(cid:0)Pxθ(s[t]) ≤ Pxθ(s[r])(cid:1)(cid:21)
|R |
(cid:19)
This gives us a method to approximate exposure: randomly
choose some small space S ⊂ R (for |S| (cid:28) |R |) and then
compute an estimate of the exposure as
(cid:20)(cid:0)Pxθ(s[t]) ≤ Pxθ(s[r])(cid:1)(cid:21)
exposureθ(s[r]) ≈ −log2 Pr
t∈S
However, this sampling method is inefﬁcient if only very
few alternate canaries have lower entropy than s[r], in which
case |S| may have to be large to obtain an accurate estimate.
Approximation by distribution modeling: Using random
sampling to estimate exposure is effective when the rank of
a canary is high enough (i.e. when random search is likely
to ﬁnd canary candidates s[t] where Pxθ(s[t]) ≤ Pxθ(s[r])).
However, sampling distribution extremes is difﬁcult, and the
rank of an inserted canary will be near 1 if it is highly exposed.
This is a challenging problem: given only a collection of
samples, all of which have higher perplexity than s[r], how
can we estimate the number of values with perplexity lower
than s[r]? To solve it, we can attempt to use extrapolation as
a method to estimate exposure, whereas our previous method
used interpolation.
To address this difﬁculty, we make the simplifying assump-
tion that the perplexity of canaries follows a computable un-
derlying distribution ρ(·) (e.g., a normal distribution). To
Figure 3: Skew normal ﬁt to the measured perplexity distri-
bution. The dotted line indicates the log-perplexity of the
inserted canary s[ˆr], which is more likely (i.e., has lower per-
plexity) than any other candidate canary s[r(cid:48)].
approximate exposureθ(s[r]), ﬁrst observe
(cid:2)Pxθ(s[t]) ≤ Pxθ(s[r])(cid:3) = ∑
(cid:2)Pxθ(s[t]) = v(cid:3).
Pr
t∈R
Pr
t∈R
v≤Pxθ(s[r])
Thus, from its summation form, we can approximate the dis-
crete distribution of log-perplexity using an integral of a con-
tinuous distribution using
exposureθ(s[r]) ≈ −log2
ρ(x)dx
(cid:90) Pxθ(s[r])
0
where ρ(x) is a continuous density function that models the
underlying distribution of the perplexity. This continuous
distribution must allow the integral to be efﬁciently com-
puted while also accurately approximating the distribution
Pr[Pxθ(s[t]) = v].
The above approach is an effective approximation of the
exposure metric. Interestingly, this estimated exposure has no
upper bound, even though the true exposure is upper-bounded
by log2|R |, when the inserted canary is the most likely. Use-
fully, this estimate can thereby help discriminate between
cases where a canary is only marginally the most likely, and
cases where the canary is by the most likely.
In this work, we use a skew-normal distribution [39] with
mean µ, standard deviation σ2, and skew α to model the distri-
bution ρ. Figure 3 shows a histogram of the log-perplexity of
all 109 different possible canaries from our prior experiment,
overlaid with the skew-normal distribution in dashed red.
We observed that the approximating skew-normal distribu-
tion almost perfectly matches the discrete distribution. No sta-
tistical test can conﬁrm that two distributions match perfectly;
instead, tests can only reject the hypothesis that the distribu-
tions are the same. When we run the Kolmogorov–Smirnov
goodness-of-ﬁt test [32] on 106 samples, we fail to reject the
null hypothesis (p > 0.1).
272    28th USENIX Security Symposium
USENIX Association
50100150200Log-Perplexity of candidate s[r]0123456Frequency (×104)Skew-normaldensity functionMeasured distribution5 Exposure-Based Testing Methodology
We now introduce our testing methodology which relies on
the exposure metric. The approach is simple and effective: we
have used it to discover properties about neural network mem-
orization, test memorization on research datasets, and test
memorization of Google’s Smart Compose [29], a production
model trained on billions of sequences.
The purpose of our testing methodology is to allow prac-
titioners to make informed decisions based upon how much
memorization is known to occur under various settings. For
example, with this information, a practitioner might decide it
will be necessary to apply sound defenses (Section 9).