†
†
†
†
Table 1: A list of vulnerabilities newly discovered by CAVER. All security vulnerabilities listed here are confirmed, and already
fixed by the corresponding development teams. Columns under Types represent classes causing bad-castings: allocation, source
and destination classes. Columns under Security Implication represents the security impacts of each vulnerability: whether the
vulnerability has C++ ABI incompatibility issues (ABI); whether the vulnerability triggers memory corruption (Mem); and the actual
security assessment ratings assigned by the vendor (Rating). †: The GNU libstdc++ members did not provide security ratings.
size. This backporting was due to the limited support
for the LLVM/clang compiler by older Chromium (other
than CVE-2013-0912). Table 2 shows our testing results
on these five known bad-casting vulnerabilities. CAVER
successfully detected all vulnerabilities.
In addition to real vulnerabilities, we thoroughly evalu-
ated CAVER with test cases that we designed based on all
possible combinations of bad-casting vulnerabilities: (1)
whether an object is polymorphic or non-polymorphic;
and (2) the three object types: allocation, source, and
destination.
|{Poly, non-Poly}||{Alloc, From, To}| = 8
Eight different unit tests were developed and evaluated
as shown in Table 3. Since CAVER’s design generally
handles both polymorphic and non-polymorphic classes,
CAVER successfully detected all cases. For comparison,
UBSAN failed six cases mainly due to its dependency on
RTTI. More severely, among the failed cases, UBSAN
crashed for two cases when it tried to parse RTTI non-
polymorphic class objects, showing it is difficult to use
without manual blacklists. Considering Firefox contains
greater than 60,000 downcasts, (see Table 4), creating
such a blacklist for Firefox would require massive manual
engineering efforts.
6.4 Protection Coverage
Table 4 summarizes our evaluation of CAVER’s protection
coverage during instrumentation, including the number
of protected types/classes (the left half), and the number
of protected type castings (the right half). In our evalua-
tion with C++ applications in SPEC CPU 2006, Firefox,
and Chromium, CAVER covers 241% more types than
UBSAN; and protects 199% more type castings.
6.5
There are several sources that increase a program’s binary
size (see Table 5), including (1) the inserted functions
for tracing objects’ type and verifying type castings, (2)
Instrumentation Overheads
Name
483.xalancbmk
450.soplex
Chromium
Firefox
# of tables
RTTI
881
39
24,929
9,907
THTable
3,402
227
94,386
30,303
# of verified cast
CAVER
UBSAN
1,967
1,378
0
2
11,531
11,596
15,611
71,930
Table 4: Comparisons of protection coverage between UBSAN
and CAVER. In the # of tables column, VTable shows the num-
ber of virtual function tables and THTable shows the number
of type hierarchy tables, each of which is generated to build
the program. # of verified cast shows the number static_cast
instrumented in UBSAN and CAVER, respectively. Overall,
CAVER covers 241% and 199% more classes and their castings,
respectively, compared to UBSAN.
Name
483.xalancbmk
450.soplex
Chromium
Firefox
Orig.
6,111
466
6,674
817
249,790
242,704
612,321
395,311
File Size (KB)
UBSAN
CAVER
9%
75%
145%
62%
7,169
861
453,449
274,254
17%
84%
81%
13%
Table 5: The file size increase of instrumented binaries: CAVER
incurs 64% and 49% less storage overheads in Chromium and
Firefox browsers, compared to UBSAN.
the THTable of each class, and (3) CAVER’s runtime li-
brary. Although CAVER did not perform much instru-
mentation for most SPEC CPU 2006 applications, the
file size increase still was noticeable. This increase was
caused by the statically linked runtime library (245 KB).
The CAVER-hardened Chromium requires 6× more stor-
age compared to Firefox because the Chromium code
bases contains more classes than Firefox. The additional
THTable overhead is the dominant source of file size in-
creases. (see Table 4). For comparison, UBSAN increased
the file size by 64% and 49% for Chromium and Firefox,
respectively; which indicates that THTable is an efficient
representation of type information compared to RTTI.
90  24th USENIX Security Symposium 
USENIX Association
10
CVE #
CVE-2013-0912
CVE-2013-2931
CVE-2014-1731
CVE-2014-3175
CVE-2014-3175
Bug ID
180763
302810
349903
387016
387371
Allocation
HTMLUnknownElement
MessageEvent
RenderListBox
SpeechSynthesis
ThrobAnimation
Type Names
Source
Element
Event
RenderBlockFlow
EventTarget
Animation
Destination
SVGElement
LocatedEvent
RenderMeter
SpeechSynthesisUtterance
MultiAnimation
Security Rating Mitigated
by CAVER
✓
✓
✓
✓
✓
High
High
High
High
Medium
Table 2: Security evaluations of CAVER with known vulnerabilities of the Chromium browser. We first picked five known bad-
casting bugs and wrote test cases for each vulnerability, retaining features that may affect CAVER’s detection algorithm, including
class hierarchy and their compositions, and related classes including allocation, source, and destination types). CAVER correctly
detected all vulnerabilities.
(a) CAVER, P Alloc
(b) CAVER, Non-P Alloc
(c) UBSAN, P Alloc
(d) UBSAN, Non-P Alloc
To
From
P
Non-P
P
✓
✓
Non-P
From
To
✓
✓
P
Non-P
P
✓
✓
Non-P
From
To
✓
✓
P
Non-P
P
✓
✓
Non-P
From
To
P
Non-P
X
X
P
Non-P
Crash
Crash
X
X
Table 3: Evaluation of protection coverage against all possible combinations of bad-castings. P and Non-P mean polymorphic
and non-polymorphic classes, respectively. In each cell, ✓marks a successful detection, X marks a failure, and Crash marks the
program crashed. (a) and (b) show the results of CAVER with polymorphic class allocations and non-polymorphic class allocations,
respectively, and (c) and (d) show the cases of UBSAN. CAVER correctly detected all cases, while UBSAN failed for 6 cases
including 2 crashes.
6.6 Runtime Performance Overheads
In this subsection, we measured the runtime overheads of
CAVER by using SPEC CPU 2006’s C++ benchmarks and
various browser benchmarks for Chromium and Firefox.
For comparison, we measured runtime overheads of the
original, non-instrumented version (compiled with clang),
and the UBSAN-hardened version.
Microbenchmarks. To understand the performance char-
acteristics of CAVER-hardened applications, we first pro-
filed micro-scaled runtime behaviors related to CAVER’s
operations (Table 6). For workloads, we used the built-in
input for the two C++ applications of SPEC CPU 2006,
and loaded the default start page of the Chromium and
Firefox browsers. Overall, CAVER traced considerable
number of objects, especially for the browsers: 783k in
Chromium, and 15,506k in Firefox.
We counted the number of verified castings (see Ta-
ble 6), and the kinds of allocations (i.e., global, stack,
or heap). In our experiment, Firefox performed 710%
more castings than Chromium, which implies that the
total number of verified castings and the corresponding
performance overheads highly depends on the way the
application is written and its usage patterns.
SPEC CPU 2006. With these application characteris-
tics in mind, we first measured runtime performance
impacts of CAVER on two SPEC CPU 2006 programs,
xalancbmk and soplex. CAVER slowed down the exe-
cution of xalancbmk and soplex by 29.6% and 20.0%,
respectively. CAVER-NAIVE (before applying the op-
timization techniques described in §4.4) slowed down
xalancbmk and soplex by 32.7% and 20.8% respectively.
UBSAN
CAVER-NAIVE
CAVER
100%
75%
50%
25%
0%
Octane
SunSpider
Dromaeo-JS
Dromaeo-DOM
Figure 4: Browser benchmark results for the Chromium
browser. On average, while CAVER-NAIVE incurs 30.7% over-
head, CAVER showed 7.6% runtime overhead after the opti-
mization. UBSAN exhibits 16.9% overhead on average.
For UBSAN, xalancbmk crashed because of RTTI limi-
tations in handling non-polymorphic types, and soplex
becomes 21.1% slower. Note, the runtime overheads
of CAVER is highly dependent on the application char-
acteristics (e.g., the number of downcasts performed in
runtime). Thus, we measured overhead with more realis-
tic workloads on two popular browsers, Chromium and
Firefox.
Browser benchmarks (Chromium). To understand the
end-to-end performance of CAVER, we measured the
performance overhead of web benchmarks. We tested
four browser benchmarks: Octane [21], SunSpider [47],
Dromaeo-JS [29], and Dromaeo-DOM [29], each of
which evaluate either the performance of the JavaScript
engine or page rendering.
Figure 4 shows the benchmark results of the Chromium
browser. On average, CAVER showed 7.6% overhead
while CAVER-NAIVE showed 30.7%, which implies the
11
USENIX Association  
24th USENIX Security Symposium  91
Name
483.xalancbmk
450.soplex
Chromium
Firefox
Global
Total
165
36
3k
24k
Object Tracing
Stack
Heap
Peak
32
1
274
38k
Total
190k
364
350k
14,821k
Peak
8k
141
79k
213k
Total
88k
658
453k
685k
Verified Castings
Global
Stack
Heap