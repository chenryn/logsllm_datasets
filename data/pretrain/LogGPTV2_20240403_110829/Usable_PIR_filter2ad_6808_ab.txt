### Enforcing Invariant (i)

To enforce invariant (i), it is essential that all buckets contain the same number of blocks. If a bucket at a particular level is empty, the adversary could deduce that the desired block is not present at that level. Therefore, each reordering process fills partially empty buckets to capacity with fake blocks. Since every block is encrypted with semantic security, the adversary cannot distinguish between real and fake blocks.

### Costs

Each query incurs an online cost of \( O(\log^2 n) \) for scanning the \( \log n \)-sized bucket on each of the \( \log n \) levels. Additionally, there is an amortized cost due to intermittent level overflows. Using a logarithmic amount of client storage, reshuffling levels in ORAM requires an amortized cost of \( O(\log^3 n) \) per query. In practice, this cost can be as high as \( O(\log^4 n) \) due to a hidden constant factor around \( 2^{100} \) in the implementation [17].

### A Solution

Our solution introduces new insights based on probabilistic analyses of data shuffling in ORAM, significantly improving its asymptotic complexity. These results are applicable under the assumption that clients can afford a small \( O(\sqrt{n}) \) amount of temporary working memory. This assumption is validated in real-world settings, such as the IBM 4764 SCPU [5], which can host up to 64MB of RAM.

#### 4.1 Additional Client-Side Working Memory

Simply adding storage to ORAM in a straightforward manner does not improve its complexity. There are two stages where additional storage can be deployed:

1. **Top Levels Storage**: Storing the top levels exclusively on the client allows bypassing reads and writes to these levels, as well as their reshuffling. With \( \log n \) levels, each of size \( 4^i \) for \( i \) from 1 to \( \log n \), the blocks belonging to the first \( \log(c\sqrt{n}) = \log c + \frac{1}{2} \log n \) levels can fit in this storage. However, this only eliminates a constant fraction of the levels, leaving the most expensive levels operating as before.

2. **Sorting Network Storage**: As indicated in [17], additional client-side storage can be used in the sorting network for level reshuffling. The sorting network, which performs all the level reordering, requires \( O(n \log^2 n) \) time for the client to obliviously sort data on the server. The ORAM claim of \( O(\log^3 n) \) amortized overhead relies on the impractical AKS sorting network [6] that performs in \( O(n \log n) \) time (with a hidden factor of close to \( 2^{100} \)).

In the presence of additional storage, the normal sorting network running time can be improved by performing comparisons in batches on the client. However, this approach does not significantly improve the complexity. We are not aware of methods that would result in more than a constant factor improvement. Even if the storage is used to reduce the time complexity of the sorting sequence, no amount of storage can make the sorting network perform better than \( \Omega(n \log n) \) [10]. This still results in an overall amortized overhead of \( \Omega(\log^3 n) \).

### Our Approach

We propose to tackle the complexity of the most time-consuming phase of ORAM, the level reorder step. By leveraging the consistent nature of uniform random permutations, we perform an oblivious scramble with low complexity and minimal client storage. Our intuition is that given two halves of an array consisting of uniformly randomly permuted items, the items will be distributed almost evenly between the halves. This allows us to implement a novel merge sort that hides the order in which items are pulled from each half.

Once the two array halves are sorted and stored on the server, they can be combined into a sorted whole by reading from each half into the client buffer and outputting them in sorted order without revealing the permutation. For arrays of size \( n \), the running tally of picks from each array half will never differ by more than \( c\sqrt{n} \) with high probability. This means we can pre-set a read pattern from the server without knowing the permutation and still successfully perform the permutation. The access pattern between the two array halves will deviate slightly but will fall within the window of \( c\sqrt{n} \) from the fixed pattern.

This oblivious merge sort is the key primitive that allows us to implement access pattern privacy with \( O(\log^2 n) \) overhead. We use it to implement a random scramble and to remove the fake blocks stored in each level. Being able to do both steps efficiently, we can replace the oblivious permutation used in ORAM with a more efficient version.

### 4.2 Strawman: Client with \( n \) Blocks of Working Memory

Before describing our main result, let's analyze a strawman algorithm that achieves our desired time complexity, assuming the client has enough storage to fit the entire database.

If the client has \( n \geq 4^i \) blocks of temporary secure storage, it can perform a level reorder with \( (2)(4^i)(\log n) = O(\log n 4^i) \) server accesses. By reading the entire level into the temporary secure storage and discarding the fake blocks, it can store all \( 4^i \) blocks locally. It then performs a comparison sort on the local secure storage to permute these blocks to their new locations at a computational cost of \( O(4^i \log(4^i)) = O(i 4^i) \). The blocks are then re-encrypted with new nonces for a cost of \( O(4^i) \). Copying this data back to the server, while inserting fakes to fill the rest of the buckets, requires writing another \( (4^i)(\log n) \) blocks to the server.

Since each level \( i \) overflows into level \( i+1 \) once every \( 4^i \) accesses, level \( i+1 \) must be reordered at each such occurrence. With \( \log n \) levels total, the amortized communication and computational costs per query can be approximated by:

\[
\sum_{i=1}^{\log n} O(\log n 4^i) / 4^{i-1} = \sum_{i=1}^{\log n} O(\log n) = O(\log^2 n)
\]

This offline level reordering cost must be added to the online query cost of scanning a bucket at each level. The online cost of scanning buckets is \( (\log^4 n)(\log n) = O(\log^2 n) \). Thus, the average cost per query, including both online and amortized offline costs, is \( O(\log^2 n) \).

In summary, with \( O(n) \) client storage, the amortized running time for ORAM can be reduced from \( O(\log^4 n) \) to \( O(\log^2 n) \). However, assuming the client has \( n \) blocks of local working memory is not practical and could invalidate the cost proposition of server-hosted data.

### 4.3 Overview: Client with Only \( c\sqrt{n} \) Blocks of Working Memory

We now describe an algorithm for level reordering with identical time complexity but requiring only \( c\sqrt{n} \) local working memory from the client. The client's reordering of level \( i \) is divided into phases (refer to Figure 1). We overview these phases and then discuss the details.

1. **Removing Fakes**: Copy the \( 4^i \) original data blocks at level \( i \) to a new remote buffer on the server, obliviously removing the \( ( \log n - 1 ) 4^i \) fake blocks. Care must be taken to prevent revealing which blocks are fakes, so copying also entails re-encryption. This decreases the size of the working set from \( (\log n) 4^i \) to \( 4^i \) if the level is full, or to \( \frac{1}{4} 4^i \) for the first, second, and third reorderings of this iteration of level \( i \). Assuming a full level (fourth reordering) simplifies the description; earlier reorderings proceed similarly but with slightly lesser time and space requirements. The communication/computational complexity of this phase is \( O(\log n 4^i) \) (see section 4.4).

2. **Oblivious Merge Sort**: Obliviously merge sort the working set in the remote buffer, placing blocks into their final permutation according to the new hash function for this level. Perform the merge sort in such a way that the server cannot build any correlation between the original arrangement of blocks and the new permutation. The communication/computational complexity of this phase is \( O(\log n 4^i) \) (see section 4.5).

3. **Adding Fakes**: Copy the \( 4^i \) blocks, permuted by Phase 2 into their correct order, to the final remote storage area for level \( i \). They are not yet in buckets, so we build buckets, obliviously adding the \( ( \log n - 1 ) 4^i \) fake blocks necessary to ensure all buckets have the same size. The communication/computational complexity of this phase is \( O(\log n 4^i) \) (see section 4.6).

The above algorithm reorders level \( i \) into the new permutation in time \( O(\log n 4^i) \). Therefore, the derivation of the amortized overhead is equivalent to the derivation performed for the strawman algorithm, leading to an amortized overhead of \( O(\log^2 n) \) per query. We now show how to efficiently implement each phase using only \( c\sqrt{n} \) local memory.

### 4.4 Phase 1: Remove Fakes

Fake blocks can be removed from level \( i \) in a single pass without revealing them by copying into a temporary buffer that hides the correspondence between read blocks and output blocks (refer to Figure 2 (a)). The client scans the level, storing the real blocks into a local queue and discarding the fake blocks. Once the queue is expected to be half full, the client starts writing blocks from the queue (while continuing the scan) at a rate corresponding to the overall ratio of real to fake blocks. The goal is to keep the queue about half full until the end.

(The server can observe the total number of fake and real blocks in a particular level, which is independent of the data access pattern.) Assuming the temporary queue never overflows or empties entirely until the end, the exact pattern of reads and writes observed by the server is dependent only on the number of blocks and the ratio of fakes. The server learns nothing about which are the fake blocks by observing the fake removal scan. We show in Theorem 1 that, with high probability, a queue of size \( c\sqrt{n} \) will not overflow or empty out.

```python
def remove_fakes_from_level():
    s = maximum_size_of_local_queue, c * sqrt(n)
    q = empty_queue_stored_locally, size s
    r = ratio_of_fake_blocks_to_real_blocks
    for x in range(1, r * (n + s / 2)):
        if x < r * n:
            t = decrypt(read_next_block_from_level())
            if t is a real block:
                enqueue(q, t)
        if total_number_of_blocks_output < x / r - s / 2:
            t = dequeue(q)
            write_next_block_to_remote_buffer(encrypt(t))
```

### 4.5 Phase 2: Oblivious Merge Sort

Obliviously merge sort the working set in the remote buffer, placing blocks into their final permutation according to the new hash function for this level. Perform the merge sort in such a way that the server cannot build any correlation between the original arrangement of blocks and the new permutation.

```python
def oblivious_merge_sort(A):
    if len(A) == 1:
        return A
    A1 = first_half_of_A
    A2 = second_half_of_A
    A1 = oblivious_merge_sort(A1)
    A2 = oblivious_merge_sort(A2)
    B = new_remote_buffer_with_same_size_as_A
    s = size_of_local_queues, c * sqrt(n)
    q1 = empty_queue_stored_locally, size s
    q2 = empty_queue_stored_locally, size s
    for x in range(1, s / 2):
        enqueue(q1, decrypt(read_next_block_from(A1)))
        enqueue(q2, decrypt(read_next_block_from(A2)))
    for x in range(s / 2, n + s / 2):
        if x <= n:
            enqueue(q1, decrypt(read_next_block_from(A1)))
            enqueue(q2, decrypt(read_next_block_from(A2)))
        # Now we’ve read 2 blocks; time to output 2 blocks
```

### 4.6 Phase 3: Add Fakes

Copy the \( 4^i \) blocks, permuted by Phase 2 into their correct order, to the final remote storage area for level \( i \). They are not yet in buckets, so we build buckets, obliviously adding the \( ( \log n - 1 ) 4^i \) fake blocks necessary to ensure all buckets have the same size.

```python
def add_fakes_to_level():
    s = size_of_local_queue, c * sqrt(n)
    q = empty_queue_stored_locally, size s
    r = ratio_of_buckets_to_real_blocks, determined_by_reshuffle_number_for_this_level
    c = 0  # total number of buckets output so far
    for x in range(1, n + s / 2):
        if x <= n:
            t = decrypt(read_next_block_from_level())
            enqueue(q, t)
        if c < r * (x - s / 2):
            c += 1
            items = dequeue_all_items_corresponding_to_bucket_c
            b = new_bucket_containing_those_items, filled_the_rest_of_the_way_with_fake_blocks
            write_next_bucket_to_remote_level(encrypt(b))
```

### Conclusion

By using a combination of removing fakes, oblivious merge sort, and adding fakes, we achieve a level reordering with a time complexity of \( O(\log n 4^i) \). This leads to an amortized overhead of \( O(\log^2 n) \) per query, making our solution efficient and practical.