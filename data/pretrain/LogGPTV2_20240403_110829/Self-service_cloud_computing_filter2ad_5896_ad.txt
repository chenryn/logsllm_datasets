the block device interface. Table 3 compares the cost of building
VMs on a traditional Xen VMM and on an SSC platform. As
these numbers demonstrate, the costs of building domains on these
platforms is near-identical. We now illustrate the utility of SSC by
using it to build several SDs that implement common utilities.
4.1 Storage SDs
Cloud providers supply clients with persistent storage. Because the
actual storage hardware is no longer under the physical control of
clients, they must treat it as untrusted. They must therefore have
mechanisms to protect the conﬁdentiality and integrity of data that
resides on cloud storage. Such mechanisms can possibly be im-
plemented within the client’s VMs itself (e.g., within a custom ﬁle
system). However, virtual machine technology allows such services
to be conveniently located outside the VM, where they can also be
combined ﬂexibly. It also isolates these services from potential at-
tacks against client VMs. Because all I/O from client VMs is vir-
tualized, storage encryption and integrity checking can easily be
implemented as cloud-based services oﬀered by the provider.
Cloud providers would normally implement such services as
daemons within dom0. However, this approach entails clients to
trust dom0, and hence cloud administrators. SSC provides clients
the ability to implement a variety of storage services as SDs with-
out trusting cloud administrators. We describe two such SDs below,
one for integrity checking and another for encryption. Our imple-
mentation of both SDs is set up as illustrated in Figure 4. Each
SD executes as a VM. When Udom0 starts a UdomU that wants
to avail the service oﬀered by an SD, it conﬁgures the UdomU to
advertise the SD as the backend driver for disk operations. The SD
itself executes a frontend driver that interfaces within a backend
driver running within Sdom0. When UdomU attempts to perform a
disk operation, the data ﬁrst goes to the SD, which is the advertised
backend for the UdomU. The SD performs the advertised service,
Figure 4. Storage SD architecture.
Platform
Xen (dom0)
SSC (SD)
Unencrypted (MB/s)
81.72±0.15
75.88±0.15 (7.1%)
Encrypted (MB/s)
71.90±0.19
70.64±0.32 (1.5%)
Table 4. Cost incurred by the storage encryption SD. For the
ﬁrst experiment, the SD runs a loopback device that performs
no encryption. For the second, the SD runs a crypto loopback
device with 128-bit AES encryption.
and passes it to the frontend executing within the SD. In turn, the
frontend forwards the (possibly modiﬁed) data block to Sdom0’s
backend, which interacts with the disk to store data persistently.
This setup can also be used to chain SDs, each oﬀering its own
service. For example, an encryption SD (see below) can serve as
the I/O backend for UdomU. In turn, a checkpointing SD (see
Section 4.4) can serve as the I/O backend for the encryption SD.
This would allow clients to easily produce disk checkpoints that
store encrypted data.
Encryption SD. Storage encryption protects the conﬁdentiality of
client data by enciphering it before storing it on disk. Using SSC,
clients can deploy their own storage encryption SD that enciphers
their data before it is transmitted to Sdom0, which stores it on
disk (or further processes the encrypted data, e.g., to implement
replication). Conversely, Sdom0 reads encrypted data from disk,
and passes it to the SD, which decrypts it and passes it to the client.
SSC ensures that Sdom0 cannot access the encryption keys, which
are stored in client VM memory, thereby protecting client data.
Udom0 initiates the storage encryption SD using a key passed
as a kernel parameter, and an initialization script that starts the SD
with a crypto loopback device. The SD encrypts client data before
it reaches Sdom0, and decrypts enciphered disk blocks fetched by
Sdom0. Data is never presented in the clear to the cloud provider,
and the encryption key is never exposed to Sdom0. In our imple-
mentation, the crypto loopback device in the SD uses AES 128-bit
encryption.
We evaluated the cost of our SD using two experiments. In
the ﬁrst experiment, we simply used a loopback device (rather
than a crypto loopback device) as the backend within our SD, and
compared the achieved disk throughput against traditional I/O on
Xen where domU communicates with a backend driver in dom0
(i.e., data is stored in the clear). This experiment allows us to
measure the extra overhead of introducing a level of indirection
in the I/O path (i.e., the SD itself). In the second experiment, we
used the crypto loopback device as the backend and measured the
overhead of encryption. In our experiments, we emptied buﬀer
caches so that each disk operation results in a disk access, thereby
traversing the entire I/O path and emulating the worst-case scenario
for storage encryption.
We used the Linux dd utility to perform a large read operation
of size 2GB. Table 4 presents the results of our experiments. These
experiments show that the reduction in disk throughput introduced
by the extra level of indirection is about 7%. With encryption
enabled, the raw disk throughput reduces in both cases, thereby
reducing the overhead of SSC-based encryption to about 1%.
BackendFrontendBackendFrontendStorageSDom0Architecture of Storage SDUDomUDisk R/W259Platform
Xen (dom0)
SSC (SD)
Throughput (MB/s)
71.7±0.1
66.6±0.3 (7.1%)
Platform
Xen (dom0)
SSC (SD)
Time (seconds)
6.471±0.067
6.487±0.064 (0%)
Table 5. Cost incurred by the storage integrity checking SD.
Table 6. Cost of the memory introspection SD, measured as the
time to boot a Linux-based domain.
Integrity Checking SD. Our integrity checking SD oﬀers a ser-
vice similar to the one proposed by Payne et al. [35]. The SD im-
plements a loopback device, which runs as a kernel module. This
device receives disk access requests from UdomUs at the block
level, enforces the speciﬁed integrity policy, and forwards the re-
quests to/from disk.
In our prototype SD, users specify important system ﬁles and
directories to protect. The SD intercepts all disk operations to these
targets, and checks that the SHA256 hashes of these disk blocks
appear in a database of whitelisted hashes. Since all operations
are intercepted at the block level, the SD needs to understand the
high-level semantics of the ﬁle system. We use an oﬄine process to
extract known-good hashes at the block level from the client VM’s
ﬁle system, and populate the hash database, which the SD consults
at runtime to check integrity.
We evaluated the cost of the integrity checking SD using the
same workload as for the encryption SD. We checked the integrity
of disk blocks against a whitelist database of 3000 hashes. Ta-
ble 5 compares the throughput achieved when this service is im-
plemented as an SD versus as a daemon in dom0. The SD service
incurs an overhead of about 7%, mainly because of the extra level
of indirection.
4.2 Memory Introspection SD
Memory introspection tools, such as rootkit detectors (e.g., [2, 29,
36, 45]), rely on the ability to fetch and inspect raw memory pages
from target VMs. In commodity cloud infrastructures, memory in-
trospection must be oﬀered by the provider, and cannot be deployed
independently by clients, who face the unsavory option of using the
service but placing their privacy at risk.
Using SSC, clients can deploy memory introspection tools as
SDs. We illustrate such an SD by implementing an approach de-
veloped in the Patagonix project [29]. Patagonix aims to detect the
presence of covertly-executing malicious binaries in a target VM
by monitoring that VM’s page tables. As originally described, the
Patagonix daemon runs in dom0, maps all the memory pages of
the target VM, and marks all pages as non-executable when the
VM starts. When the target VM attempts to execute a page for the
ﬁrst time, Patagonix receives a fault. Patagonix handles this fault
by hashing the contents of the page (i.e., an md5sum) requested for
execution, and comparing it against a database of hashes of code
authorized to execute on the system (e.g., the database may store
hashes of code pages of an entire Linux distribution). If the hash
does not exist in the database, Patagonix raises an alarm and sus-
pends the VM.
We implemented Patagonix as an SD. Each Patagonix SD mon-
itors a target UdomU, a reference to which is passed to the SD
when the UdomU boots up. Udom0 delegates to Patagonix SD
the privileges to map the UdomU’s pages, and mark them as non-
executable. The SD receives and handles faults as the UdomU ex-
ecutes new code pages. Our Patagonix SD can detect maliciously-
executing binaries with the same eﬀectiveness as described in the
original paper [29]. To measure this SD’s performance, we mea-
sured the boot time of a monitored UdomU. The SD validates all
code pages that execute during boot time by checking each of them
against the hash databse. We compared the time taken by this SD
to a traditional setup where the Patagonix daemon executed within
dom0. Table 6 presents the results of our experiment, again show-
ing that using an SD imposes minimal overhead.
A memory introspection MTSD. Suppose that a cloud provider
wants to ensure that a client is not misusing cloud resources to host
and execute malicious software (or that an honest client’s VM has
not become infected with malware). In today’s cloud infrastructure,
this is achieved via VM introspection tools that execute in dom0.
Such tools can inspect and modify client state, and therefore require
dom0 to be trusted.
SSC oﬀers cloud providers unprecedented power and ﬂexibil-
ity in verifying client regulatory compliance while respecting client
privacy. As an example, cloud providers can implement the Patago-
nix SD above as an MTSD to ensure that a client VM is free of mal-
ware. In this case, the cloud provider would supply the database of
code hashes, which is the regulatory compliance policy. The MTSD
itself would execute in the client meta-domain; the manifest of this
MTSD simply requests privileges to read client memory pages and
registers. Because MTSDs resemble SDs in their implementation,
and only diﬀer in the privileges assigned to them, the performance
of this MTSD is identical to the corresponding SD, as reported in
Table 6. The MTSD continuously monitors client UdomUs and re-
ports a violation of regulatory compliance to the cloud provider
(i.e., Sdom0) only if the client becomes infected with malware. The
cloud provider only learns whether the client has violated regula-
tory compliance, and cannot otherwise read or modify the content
of the client’s memory pages.
4.3 System Call Monitoring SD
There is a large body of work on system call-based anomaly de-
tection tools. While we will not attempt to summarize that work
here (see Giﬃn’s thesis [20] for a good overview), these techniques
typically work by intercepting process system calls and their ar-
guments, and ensuring that the sequence of calls conforms to a
security policy. The anomaly detector executes in a separate VM
(dom0), and capture system call traps and arguments from a user
VM for analysis. Using SSC, clients can implement their own sys-
tem call anomaly detectors as SDs. The SD simply intercepts all
system calls and arguments from a target UdomU and checks them
against a target policy.
On a paravirtualized platform, capturing system calls and their
arguments is straightforward. Each trap from a UdomU transfers
control to the hypervisor, which forwards the trap to the SD if it is
from a user-space process within the UdomU. The SD captures the
trap address and its arguments (passed via registers). However, the
situation is more complex on an HVM platform. On such a plat-
form, traps are directly forwarded to the kernel of the HVM by the
hardware without the involvement of the hypervisor. Fortunately,
it is still possible to capture traps, albeit diﬀerently on AMD and
Intel hardware. AMD supports control ﬂags that can be set to trig-
ger VMExits on system calls. On the Intel platform, traps can be
intercepted by placing dummy values in the MSR (model-speciﬁc
register) corresponding to the syscall instruction to raise a page
fault on a system call. On a page fault, the hypervisor determines
the source of the fault; if due to a system call, it can forward the
trap address and registers to the SD.
We evaluated the cost of this approach by simply building an
SD to capture system calls and their arguments (i.e., our SD only
includes the system call capture tool; we do not check the captured
calls against any policies). We used the syscall microbenchmark
of the UnixBench benchmark suite [1] as the workload within the
target UdomU to evaluate the overhead of this SD. The syscall mi-
260Platform
Xen (dom0)
SSC (SD)
System calls/second
275K ±0.95
272K ±0.78 (1%)
Table 7. Cost incurred by the system call monitoring SD, mea-
sured using the UnixBench syscall microbenchmark.
Platform
Xen (dom0)
SSC (SD)
Xen (dom0)
SSC (SD)
VM size
(MB)
512
512
1024
1024
No encryption
(seconds)
0.764±0.001
0.803±0.006 (5.1%)
1.840±0.005
1.936±0.001 (5.2%)
With encryption
(seconds)
5.571±0.004
5.559±0.005 (-0.2%)
11.419 ±0.008
11.329 ±0.073 (-0.8%)
Table 8. Cost incurred by the checkpointing SD.
crobenchmark runs mix of close, getpid, getuid and umask system
calls and outputs the number of system calls executed in a ﬁxed
amount of time. In our experiments we compared the number of
system calls executed by the syscall microbenchmark when the sys-
tem call capture tool runs as SD to the traditional scenario where