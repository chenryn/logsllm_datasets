### Block Device Interface and Cost Comparison

Table 3 compares the cost of building virtual machines (VMs) on a traditional Xen VMM and on an SSC platform. The data shows that the costs of building domains on these platforms are nearly identical. We will now illustrate the utility of SSC by demonstrating how it can be used to build several SDs (Service Domains) that implement common utilities.

### 4.1 Storage Service Domains (SDs)

Cloud providers offer clients persistent storage. Since the actual storage hardware is no longer under the physical control of the clients, they must treat it as untrusted. Therefore, mechanisms are needed to protect the confidentiality and integrity of data stored in the cloud. These mechanisms can be implemented within the client’s VMs (e.g., within a custom file system). However, virtual machine technology allows such services to be conveniently located outside the VM, where they can be combined flexibly and isolated from potential attacks against client VMs. Because all I/O from client VMs is virtualized, storage encryption and integrity checking can be easily implemented as cloud-based services offered by the provider.

Cloud providers typically implement such services as daemons within `dom0`. However, this approach requires clients to trust `dom0` and, by extension, the cloud administrators. SSC provides clients with the ability to implement a variety of storage services as SDs without trusting cloud administrators. Below, we describe two such SDs: one for integrity checking and another for encryption. Our implementation of both SDs is illustrated in Figure 4.

#### Architecture of Storage SDs

Each SD executes as a VM. When `Udom0` starts a `UdomU` that wants to use the service offered by an SD, it configures `UdomU` to advertise the SD as the backend driver for disk operations. The SD itself executes a frontend driver that interfaces with a backend driver running within `Sdom0`. When `UdomU` attempts to perform a disk operation, the data first goes to the SD, which is the advertised backend for `UdomU`. The SD performs the advertised service and passes the (possibly modified) data block to the frontend executing within the SD. The frontend then forwards the data to `Sdom0`'s backend, which interacts with the disk to store data persistently.

This setup can also be used to chain SDs, each offering its own service. For example, an encryption SD can serve as the I/O backend for `UdomU`, and a checkpointing SD can serve as the I/O backend for the encryption SD. This would allow clients to easily produce disk checkpoints that store encrypted data.

#### Encryption SD

Storage encryption protects the confidentiality of client data by enciphering it before storing it on disk. Using SSC, clients can deploy their own storage encryption SD that enciphers their data before it is transmitted to `Sdom0`, which stores it on disk. Conversely, `Sdom0` reads encrypted data from disk and passes it to the SD, which decrypts it and passes it to the client. SSC ensures that `Sdom0` cannot access the encryption keys, which are stored in client VM memory, thereby protecting client data.

`Udom0` initiates the storage encryption SD using a key passed as a kernel parameter and an initialization script that starts the SD with a crypto loopback device. The SD encrypts client data before it reaches `Sdom0` and decrypts enciphered disk blocks fetched by `Sdom0`. Data is never presented in the clear to the cloud provider, and the encryption key is never exposed to `Sdom0`. In our implementation, the crypto loopback device in the SD uses AES 128-bit encryption.

We evaluated the cost of our SD using two experiments. In the first experiment, we used a loopback device (rather than a crypto loopback device) as the backend within our SD and compared the achieved disk throughput against traditional I/O on Xen, where `domU` communicates with a backend driver in `dom0` (i.e., data is stored in the clear). This experiment allowed us to measure the extra overhead of introducing a level of indirection in the I/O path (i.e., the SD itself). In the second experiment, we used the crypto loopback device as the backend and measured the overhead of encryption. In our experiments, we emptied buffer caches so that each disk operation resulted in a disk access, thereby traversing the entire I/O path and emulating the worst-case scenario for storage encryption.

We used the Linux `dd` utility to perform a large read operation of size 2GB. Table 4 presents the results of our experiments. These experiments show that the reduction in disk throughput introduced by the extra level of indirection is about 7%. With encryption enabled, the raw disk throughput reduces in both cases, reducing the overhead of SSC-based encryption to about 1%.

| Platform       | Unencrypted (MB/s) | Encrypted (MB/s)   |
|----------------|--------------------|--------------------|
| Xen (dom0)     | 81.72±0.15         | 71.90±0.19         |
| SSC (SD)       | 75.88±0.15 (7.1%)  | 70.64±0.32 (1.5%)  |

**Table 4. Cost incurred by the storage encryption SD.**

#### Integrity Checking SD

Our integrity checking SD offers a service similar to the one proposed by Payne et al. [35]. The SD implements a loopback device, which runs as a kernel module. This device receives disk access requests from `UdomUs` at the block level, enforces the specified integrity policy, and forwards the requests to/from disk.

In our prototype SD, users specify important system files and directories to protect. The SD intercepts all disk operations to these targets and checks that the SHA256 hashes of these disk blocks appear in a database of whitelisted hashes. Since all operations are intercepted at the block level, the SD needs to understand the high-level semantics of the file system. We use an offline process to extract known-good hashes at the block level from the client VM's file system and populate the hash database, which the SD consults at runtime to check integrity.

We evaluated the cost of the integrity checking SD using the same workload as for the encryption SD. We checked the integrity of disk blocks against a whitelist database of 3000 hashes. Table 5 compares the throughput achieved when this service is implemented as an SD versus as a daemon in `dom0`. The SD service incurs an overhead of about 7%, mainly because of the extra level of indirection.

| Platform       | Throughput (MB/s)   | Time (seconds)     |
|----------------|--------------------|--------------------|
| Xen (dom0)     | 71.7±0.1           | 6.471±0.067        |
| SSC (SD)       | 66.6±0.3 (7.1%)    | 6.487±0.064 (0%)   |

**Table 5. Cost incurred by the storage integrity checking SD.**

### 4.2 Memory Introspection SD

Memory introspection tools, such as rootkit detectors, rely on the ability to fetch and inspect raw memory pages from target VMs. In commodity cloud infrastructures, memory introspection must be offered by the provider and cannot be deployed independently by clients, who face the unsavory option of using the service but placing their privacy at risk.

Using SSC, clients can deploy memory introspection tools as SDs. We illustrate such an SD by implementing an approach developed in the Patagonix project [29]. Patagonix aims to detect the presence of covertly-executing malicious binaries in a target VM by monitoring that VM’s page tables. As originally described, the Patagonix daemon runs in `dom0`, maps all the memory pages of the target VM, and marks all pages as non-executable when the VM starts. When the target VM attempts to execute a page for the first time, Patagonix receives a fault. Patagonix handles this fault by hashing the contents of the page (i.e., an md5sum) requested for execution and comparing it against a database of hashes of code authorized to execute on the system (e.g., the database may store hashes of code pages of an entire Linux distribution). If the hash does not exist in the database, Patagonix raises an alarm and suspends the VM.

We implemented Patagonix as an SD. Each Patagonix SD monitors a target `UdomU`, a reference to which is passed to the SD when the `UdomU` boots up. `Udom0` delegates to the Patagonix SD the privileges to map the `UdomU`'s pages and mark them as non-executable. The SD receives and handles faults as the `UdomU` executes new code pages. Our Patagonix SD can detect maliciously-executing binaries with the same effectiveness as described in the original paper [29]. To measure this SD’s performance, we measured the boot time of a monitored `UdomU`. The SD validates all code pages that execute during boot time by checking each of them against the hash database. We compared the time taken by this SD to a traditional setup where the Patagonix daemon executed within `dom0`. Table 6 presents the results of our experiment, showing that using an SD imposes minimal overhead.

| Platform       | Time (seconds)     |
|----------------|--------------------|
| Xen (dom0)     | 6.471±0.067        |
| SSC (SD)       | 6.487±0.064 (0%)   |

**Table 6. Cost of the memory introspection SD, measured as the time to boot a Linux-based domain.**

### 4.3 System Call Monitoring SD

There is a large body of work on system call-based anomaly detection tools. While we will not attempt to summarize that work here (see Giﬃn’s thesis [20] for a good overview), these techniques typically work by intercepting process system calls and their arguments and ensuring that the sequence of calls conforms to a security policy. The anomaly detector executes in a separate VM (`dom0`), capturing system call traps and arguments from a user VM for analysis. Using SSC, clients can implement their own system call anomaly detectors as SDs. The SD simply intercepts all system calls and arguments from a target `UdomU` and checks them against a target policy.

On a paravirtualized platform, capturing system calls and their arguments is straightforward. Each trap from a `UdomU` transfers control to the hypervisor, which forwards the trap to the SD if it is from a user-space process within the `UdomU`. The SD captures the trap address and its arguments (passed via registers). However, the situation is more complex on an HVM platform. On such a platform, traps are directly forwarded to the kernel of the HVM by the hardware without the involvement of the hypervisor. Fortunately, it is still possible to capture traps, albeit differently on AMD and Intel hardware. AMD supports control flags that can be set to trigger VMExits on system calls. On the Intel platform, traps can be intercepted by placing dummy values in the MSR (model-specific register) corresponding to the `syscall` instruction to raise a page fault on a system call. On a page fault, the hypervisor determines the source of the fault; if due to a system call, it can forward the trap address and registers to the SD.

We evaluated the cost of this approach by building an SD to capture system calls and their arguments (i.e., our SD only includes the system call capture tool; we do not check the captured calls against any policies). We used the `syscall` microbenchmark of the UnixBench benchmark suite [1] as the workload within the target `UdomU` to evaluate the overhead of this SD. The `syscall` microbenchmark runs a mix of `close`, `getpid`, `getuid`, and `umask` system calls and outputs the number of system calls executed in a fixed amount of time. In our experiments, we compared the number of system calls executed by the `syscall` microbenchmark when the system call capture tool runs as an SD to the traditional scenario where the system call capture tool runs in `dom0`.

| Platform       | System calls/second |
|----------------|--------------------|
| Xen (dom0)     | 275K ±0.95         |
| SSC (SD)       | 272K ±0.78 (1%)    |

**Table 7. Cost incurred by the system call monitoring SD, measured using the UnixBench syscall microbenchmark.**

### 4.4 Checkpointing SD

Checkpointing is another useful service that can be implemented as an SD. A checkpointing SD allows clients to create and restore snapshots of their VMs, enabling features like live migration and fault tolerance. We evaluated the cost of the checkpointing SD by measuring the time it takes to create a checkpoint with and without encryption. Table 8 presents the results of our experiment, showing that the overhead of the checkpointing SD is minimal.

| Platform       | VM size (MB) | No encryption (seconds) | With encryption (seconds) |
|----------------|--------------|------------------------|---------------------------|
| Xen (dom0)     | 512          | 0.764±0.001            | 5.571±0.004               |
| SSC (SD)       | 512          | 0.803±0.006 (5.1%)     | 5.559±0.005 (-0.2%)       |
| Xen (dom0)     | 1024         | 1.840±0.005            | 11.419±0.008              |
| SSC (SD)       | 1024         | 1.936±0.001 (5.2%)     | 11.329±0.073 (-0.8%)      |

**Table 8. Cost incurred by the checkpointing SD.**

In summary, SSC provides a flexible and secure framework for implementing various storage, memory, and system call monitoring services as SDs, with minimal overhead compared to traditional approaches.