Figure 2: (a) The CDF of local mixing time for every node in Facebook1, Facebook2, Twitter, Epinions and
DBLP (x-axis is in the logarithmic scale). (b) The CDF of local mixing time for a sample of 5% nodes in each
of the ten datasets in Table 1 (x-axis is in the logarithmic scale). (c) Illustration of the relationship between
the community structure and local mixing time using the Facebook1 graph.
hence the probabilities over neighbors could be used as fea-
tures for estimation (see Remark 1). In centralized systems,
πi(k) can be obtained by πi(k) = πi(0) · P k. In distributed
systems, πi(k) can be approximated by the terminal node
distribution after performing a suﬃciently large number of
k-hop random walks starting from node i. Each node uses
its local mixing time as the label.
We randomly select a subset of M nodes as the training
set (M is relatively small compared to n). After collect-
ing the training feature matrix and training labels, we use
them to ﬁt a Random Forest regression model [7]. We also
compare the results with those under a k-Nearest Neighbors
(KNN) regression model [9]. Random Forest ﬁts a number
of decision trees on sub-samples of the dataset and then av-
erages the obtained labels. KNN ﬁnds a set of the closest
training samples to the target point and predicts the label
by assigning weights to the set’s labels. Then we predict the
local mixing time for the target node using structural fea-
tures associated with the node. In the evaluation part, we
show that when k is carefully chosen, we can obtain a good
estimation of the local mixing time only using characteris-
tics within the local neighborhood and a training set with a
relatively small size M .
Remark 1. The intuition behind our prediction algorithm
can be illustrated by the Facebook1 graph in Fig. 2c, where
nodes belonging to the same community are marked by the
same color, and the size of each node is proportional to its
local mixing time. We can see that the local mixing time for
nodes residing in the same community does not vary greatly.
The transitional change of local mixing time between diﬀer-
ent communities usually occurs at marginal nodes that con-
nect two communities. Note that most nodes in the same
community tend to share similar local neighborhood char-
acteristics. For marginal nodes, their local characteristics
result from a combination of several communities. Since the
number of communities is small, a small number of training
nodes are suﬃcient to map each node to its community and
give a good prediction of its local mixing time.
Evaluation. To evaluate the ﬁtness of a regression model
for a dataset, we employ two metrics. Given a dataset
of n values denoted by {xi|i = 1, 2, , n}, each associated
with a predicted value yi, the ﬁrst metric is Root Mean
Squared Error (RM SE), which is deﬁned as RM SE =
Algorithm 1 Local Mixing Time Prediction Algorithm
Step 1. Randomly select M nodes as training samples from
graph G.
Step 2.
(a) Starting from each training (target) node i,
perform k-hop random walks and get the probability distri-
bution πi(k) as the training (target) feature vector.
(b) Form the M -by-n training feature matrix Ftrain(k) and
the target feature martrix Ftarget(k).
Step 3. (a) For each training node i, compute its local mix-
ing time Ti() as the training label.
(b) Form the M -by-1 training label vector Ttrain().
Step 4. Fit a Random Forest regression model M =
RF (Ftrain(k), Ttrain()).
Step 5. For each target nodes, predict the local mixing time
via the regression model, i.e., ˜Ttarget() = M(Ftarget(k)).
(cid:113) 1
(cid:80)n
i=1 (yi − xi)2.
n
It is the total root average squared
diﬀerence between the predicted and the true response val-
ues. Lower RM SE indicates a better prediction. The sec-
(cid:80)n
ond metric is the coeﬃcient of determination (r2), deﬁned
(cid:80)n
(cid:80)n
i=1 (yi−xi)2
as r2 = 1−
i=1 yi)2 . It characterizes the correla-
i=1 (yi− 1
tion between the predicted and true response values. Higher
r2 indicates a better prediction.
n
In the following, we present experimental results on Face-
book1, Facebook2 and Twitter, and show that based merely
on the local characteristics (i.e., features) and limited global
information of the graph (i.e., training labels), it is possible
to give a good estimation of the local mixing time for each
node in the graph. Hence, we can estimate the approximate
local mixing time for any user in distributed systems pro-
vided that the local mixing time of a small subset of nodes
is broadcast to other users.
To evaluate our prediction algorithm, we set  = 0.25,
k = 1, 2, 3, 4 and M = 10, 50, 125, 250, 500, 1000, 2000. We
compare the results using two supervised learning methods,
i.e., Random Forest regression and KNN regression. In our
experiments, the number of estimators in Random Forest is
set to 20, and the number of neighbors in KNN is set to 10.
Fig. 3a to Fig. 3f depict RM SE and r2 averaged over
100 iterations, respectively, using features within the k-hop
neighborhood (k = 1, 2, 3, 4) at M =500. It can be seen that
using Random Forest regression, RM SE decreases and r2
10110210300.20.40.60.81local mixing timeCDF  Facebook1Facebook2DBLPTwitterEpinions10110210300.20.40.60.81local mixing timeCDF  Facebook1Facebook2DBLPTwitterGoogle+EpinionsLiveJournalPokecFlickrYoutube(a) RM SE vs. k in Facebook1
(b) r2 vs. k in Facebook1
(c) RM SE vs. k in Facebook2
(d) r2 vs. k in Facebook2
(e) RM SE vs. k in Twitter
(f) r2 vs. k in Twitter
(g) RM SE vs. M
(h) r2 vs. M
Figure 3: (a) - (f ) RM SE and r2 using features within k-hop neighborhood (M=500, averaged over 100 iter-
ations) in Facebook1, Facebook2 and Twitter. (g) - (h) RM SE and r2 versus the size of the training set M
(k=3, averaged over 100 iterations) in Facebook1 and Facebook2.
increases explicitly as k gets larger. This is because that
using a wider neighborhood around node i gives a better
match among the training nodes, which consequently results
in the considerable improvement of the prediction perfor-
mance. In contrast, RM SE and r2 using KNN regression
do not vary signiﬁcantly as k grows. For Facebook1 and
Facebook2 graphs, the increase of k produces slightly better
RM SE and r2. However, this observation does not hold for
the Twitter graph. The weak impact of k on KNN can be
explained by KNN’s dependence upon the distance between
feature vectors. Since the distance between the set of closest
training samples to the new point almost remains unchanged
with respect to k, we get similar prediction results. In gen-
eral, Random Forest outperforms KNN from the perspective
of both RM SE and r2, and its advantage gets more obvious
with a larger value of k.
Fig. 3g and Fig. 3h depicts RM SE and r2 averaged
over 100 iterations versus the number of training samples
using Facebook1 and Facebook2 graphs at k = 3. It can be
seen that RM SE decreases and r2 increases sharply when
M grows from 10 to 125. This is because that a larger
number of training samples implies more global knowledge
of the graph. The resulting improvement gets slower when
M exceeds 125. The choice of parameters k and M is de-
pendent on the application requirement and varies among
diﬀerent datasets. According to the results in Section 3, we
know that the mixing time is 90 for Facebook1, 179 for Face-
book2, and 638 for Twitter. For these three social graphs,
with k = 3 and M = 500, our prediction method produces
acceptable performance with relatively small RM SE and
large r2. In this case, M is approximately one-hundredth of
the size of these three datasets, and k is about one-tenth or
one-hundredth of the mixing time.
(a)
(b)
Figure 4: Illustration of (a) Node-adaptive Random
Walks and (b) Path-adaptive Random Walks.
5. SMARTWALK: USAGE MODEL
In this section, based on the results of the prediction algo-
rithm in Section 4, we propose two usage models of adaptive
random walks.
Adaptive across nodes. Given a random walk start-
ing from node v0, its local mixing time Tv0 () measures the
length required to converge to the stationary distribution.
Hence,
it is fairly straightforward to take the prediction
value ˜Tv0 () as the random walk length when starting from
node v0. The parameter  indicates the closeness between
the terminus distribution and stationarity, and can be de-
termined by the requirement of applications. Algorithm 2
determines the random walk length by the predicted ˜Tv0 (),
and thus is adaptive to v0 (node-adaptive). As illustrated
in Fig. 4a, the local mixing time of vertex v0 (marked by 0
in Fig. 4a) is predicted as T = 5. Thus, any random walk
starting from v0 is a 5-hop random node sequence with suc-
cessive nodes being neighbors. In this usage model, the walk
length l only relys on the starting node, and is independent
of any intermediate node along the path.
12341.522.533.54k−hop neighborhood featuresRMSE  Random ForestKNN123400.20.40.60.81k−hop neighborhood featuresr2  Random ForestKNN1234345678k−hop neighborhood featuresRMSE  Random ForestKNN123400.20.40.60.81k−hop neighborhood featuresr2  Random ForestKNN1234152025303540k−hop neighborhood featuresRMSE  Random ForestKNN123400.20.40.60.81k−hop neighborhood featuresr2  Random ForestKNN050010001500200001234567M (size of training set)RMSE  Facebook1Facebook2050010001500200000.20.40.60.81M (size of training set)r2  Facebook1Facebook2  Local mixing time T       Remaining length l     0  1  2    T=5 T=12 T=11 T=20 T=7 T=3 T=10 T=6 (l=3) (l=5)  3  5  4 (l=4) (l=1) (l=0) (l=2)   Local mixing time T       Remaining length l     0  1   2 T=5 T=12 T=11 T=20 T=7 T=3 T=10 T=6 (l=2) (l=5)  4  3 (l=3) (l=1) (l=0) ﬁt a Random Forest regression model M =
Algorithm 2 Node-adaptive Random Walks
Input: G, Ftrain(k), Ttrain(), v0
Output: W
Step 1.
RF (Ftrain(k), Ttrain()).
Step 2. compute Fv0 (k), a vector of features within the
k-hop neightborhod of the initial node v0.
Step 3. predict the local mixing time of v0 via the regres-
sion model, i.e., ˜Tv0 () = M(Fv0 (k)).
Step 4. W = {v0}, t = ˜Tv0 (), vp = v0.
Step 5. while t > 0
select a neighboring node vp+1 of vp with probability
add vp+1 to the set W.
vp = vp+1, t = t − 1.
end while
deg(vp) .
1
ﬁt a Random Forest regression model M =
Algorithm 3 Path-adaptive Random Walks
Input: G, Ftrain(k), Ttrain(), v0
Output: W
Step 1.
RF (Ftrain(k), Ttrain()).
Step 2. compute Fv0 (k), a vector of features within the
k-hop neightborhod of the initial node v0.
Step 3. predict the local mixing time of v0 via the regres-
sion model, i.e., ˜Tv0 () = M(Fv0 (k)).
Step 4. W = {v0}, t = ˜Tv0 (), vp = v0.
Step 5. while t > 0
select a neighboring node vp+1 of vp with probability
add vp+1 to the set W.
repeat Steps 2 & 3 for node vp+1, and get ˜Tvp+1 () =
M(Fvp+1 (k)).
vp = vp+1, t = min{ ˜Tvp+1 (), t − 1}.
end while
deg(vp) .
1
Adaptive across nodes and paths. For node-adaptive
random walks, the length of random walks starting from
node v0 is set as the predicted local mixing time of node
v0, i.e., ˜Tv0 (). However, for a random walk initiated from
the same node v0, the remaining random walk length re-
quired to approach stationarity also depends on the path
it has already covered. Speciﬁcally, if the random walk ar-
rives at some intermediate node vp, it might take no more
than Tvp () additional hops to be within the -distance to
the stationary distribution. Hence, to make the usage model
adaptive both across nodes and across paths, each time an
intermediate node vp is reached, we update the value of the
remaining random walk length if it is greater than the pre-
dicted local mixing time ˜Tvp () of node vp. The improved
path-adaptive usage model is given in Algorithm 3. Fig.
4b illustrates an example of a path adaptive walk. At the
beginning, the local mixing time of the initial vertex v0 is
predicted to be T = 5. After one hop, we arrive at node
v1 with its local mixing time T = 3. According to the new
information provided by v1, the remaining necessary walk
length l is updated to 3 instead of 5 − 1 = 4. The second
hop takes us to node v2 with T = 6, which is greater than
the number of remaining hops. Thus we keep l = 2. Repeat
this process, and eventually we terminate at node v4. The
total walk length is 4, which is determined by considering
all the nodes along the walk.
(a) Facebook wall post
(b) Twitter
Figure 5: CDF of local mixing time with respect to
.
6. SECURITY APPLICATIONS
In this section, we demonstrate the applicability of our
two usage models to social network based security systems,
including Sybil defense, anonymous communication and link
privacy preserving systems..
6.1 Sybil defense
A Sybil attack is an attack wherein a single user forges a
large number of pseudonymous identities. Recent work has
proposed Sybil defense mechanisms by leveraging the trust
relationships in social networks [51, 52, 35, 24, 13, 44, 43].
The key insight is that it is diﬃcult for an adversary to es-
tablish trust relationships with honest users (attack edges),
particularly when interaction networks are used to incur a
higher cost for adversaries to set up an attack edge [47, 16].
SybilLimit [51] is a Sybil defense protocol that performs ran-
dom walk based routes on social graphs and registers public
keys with the tails (terminus points of random routes) to
diﬀerentiate Sybil users from benign users.
Security/false positive rate trade-oﬀ. (1) We evalu-
ate the security performance of SybilLimit in terms of the
number of Sybils that an adversary can insert in the hon-
est region (false negatives). Note that the expected number
of Sybil nodes that an adversary can insert in the honest
region is given by E[S] = E[g · W ] = g · E[W ], where g
is the number of attack edges and W is the random walk
length. Then the false negatives per attack edge is E[W ].
(2) Another critical metric to evaluate the accuracy of Sybil-
Limit is the false positive rate (percentage of benign users