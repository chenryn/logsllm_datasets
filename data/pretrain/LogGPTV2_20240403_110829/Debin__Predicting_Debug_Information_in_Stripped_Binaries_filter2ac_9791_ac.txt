candidate assignment for every node from a top-K beam and itera-
tively changing the assignments until the score stops improving.
Learning CRF Model. Let D = {(U (i ), K (i ) )}t
i =1 be a set of t pro-
grams used for training, in which the ground truth assignments U (i )
and K (i ) are given. For Debin, D is a set of non-stripped binaries.
During learning, our goal is to automatically compute the optimal
weights w = {wl}n
l =1 for feature functions from the patterns in the
training set. For training, we aim to compute the optimal weights
wopt that produce the highest likelihood considering all training
binaries (mathematically via the product below) specified as:
wopt = argmax
w
P (U = U (i ) | K = K (i ) )
Intuitively, maximizing this likelihood is to find the best weights
wopt that lead to correct prediction for all training samples.
However, computing the exact likelihood as defined above is
very expensive because for each program we need to compute
the expensive Z constant which in turn requires iterating over
all possible joint assignments of all candidate labels (this set is
generally large as we need to consider all possible names and types).
Instead, we employ pseudo likelihood estimation to approximate
the precise likelihood:
t(cid:89)
i
with
(cid:89)
P (U = U (i ) | K = K (i ) )
P (Uj = U (i )
j
| neiдhb (Uj ), K (i ) )
j
where Uj is the jth element of vector U and neiдhb (Uj ) represents
the neighbors of Uj.
The pseudo likelihood decomposes the expensive computation of
the exact likelihood into multiplication of easier-to-compute proba-
bility for every local assignment. The main benefit stems from the
fact that we now consider one node at a time, where for each node
we compute the Z constant (for that node) by iterating only over the
possible labels of that node. To further speed up the learning phase,
we adopt a parallelized implementation of the algorithm [5]. For
more details on pseudo likelihood estimation, please see [51]. We
remark that while the (approximate) computation of the Z constant
is needed for this training method, as mentioned earlier, it is not
required for MAP inference.
4 STRUCTURED PREDICTION WITH DEBIN
In this section, we describe how Debin instantiates the CRF model
described in Section 3 in order to perform structured prediction for
binaries.
To build a CRF from a program, we follow several steps: we first
transform the binary code by lifting it into the BAP-IR intermediate
representation, then we extract the relevant program elements,
determine known and unknown elements (using the predictions of
the ET model), and lastly we relate the program elements through
feature functions.
For a given binary, we first construct one dependency graph
and then perform MAP inference over that entire graph. The MAP
inference predicts jointly the names and the types in this graph.
4.1 Intermediate Representation
The first step of Debin is to analyze the input binary and lift it into
BAP-IR, the intermediate representation of the BAP binary analysis
platform. A simplified example of BAP-IR was already shown in
Figure 2(b). There are several advantages to employing BAP and
its IR:
(1) BAP-IR provides a uniform syntax for binary code across var-
ious architectures, making Debin a cross-platform tool (cur-
rently supports x86, x64 and ARM) and easily extensible to
other architectures supported by BAP.
(2) During the lifting process, BAP can recognize function bound-
aries of stripped binaries via its ByteWeight component [14].
This component is important for the follow-up analysis steps as
well as obtaining various code elements (defined in Section 4.2).
(3) While preserving semantics of raw instructions, BAP-IR ex-
plicitly shows operations on machine states, which are more
meaningful and higher-level. This is particularly useful in pro-
viding more insightful relationships than raw instructions.
(4) Registers, flags and other machine variables are transformed
into a Static Single Assignment (SSA) form, where each variable
is assigned once and has version numbers. It enables Debin to
determine which register accesses touch the same variable. This
is useful as registers accessed with the same version number
can be merged into a single node, saving time and space during
prediction.
(5) Kim et al. [35] investigated a wide range of IRs for binary anal-
ysis. BAP-IR stands out as the most suitable choice for being
explicit, self-contained and robust in lifting binary instructions.
It also helps in making Debin robust under different compiler
options and hardware architectures.
4.2 Binary Code Elements
Debin extracts different kinds of program elements from BAP-IR in
order to build a dependency graph. Each of the following program
elements becomes a node in the graph:
Functions. There are two types of functions in binaries: library
and user-defined. For example, sum in Figure 2 is user-defined
while printf is a function from the C language standard library.
For every function, we introduce a node for representing its name.
Register Variables. A register in assembly code may correspond
to a variable in the source code. In BAP-IR with SSA format, every
register exists as a tuple (register name, SSA version). We introduce
a node for every register tuple that can be mapped to a variable to
represent its variable name. The EDX.2 node in Figure 2(d) is an
example of such a node. Note that (as discussed earlier in Section 2)
there are also registers that do not correspond to a variable. We
discuss this case later in Section 4.3.
Memory Offset Variables. BAP-IR explicitly captures memory
accesses (e.g., mem[ESP+4] at line 2 of Figure 2(b)). As with regis-
ters, memory offsets may also correspond to variables. We extract
memory offset variables in BAP-IR and introduce nodes for these
in order to capture variable names. The handling of the case when
a memory access is not a variable is discussed in Section 4.3.
Types. We also introduce a type node for each unknown function
and variable. In Debin, we define 17 possible types that the pre-
diction can work with (in C language style): struct, union, enum,
array, pointer, void, bool, char, short, int, long and long
long (both signed and unsigned for the last five).
Flags. Flags represent machine status that indicates overflow, ef-
fect of arithmetic carry, or the result of a conditional operation.
BAP-IR explicitly represents flags as variables whose name denotes
the functionality (e.g., carry flag is named CF). We introduce a node
for every flag involved in operations of BAP-IR.
Instructions. We also introduce a node for every instruction in
the binary. Examples include mov, add and jne in Figure 2(a).
Constants. We introduce nodes for integer and string constants.
String constants are extracted from .rodata section in binaries.
Locations. Location nodes record the locations of variables in a
register (e.g., ECX) or a memory offset (e.g., mem[ESP+4]). Together
with variable nodes, they reveal how compilers allocate machine
resources for variables.
Unary Operators. We introduce nodes for different unary opera-
tors. Those operations on registers and memory offsets are espe-
cially helpful for predicting types (e.g., unsigned and signed cast
operators contribute to predicting a variable’s signedness).
4.3 Known and Unknown Elements
To determine which of the above elements are known (should not
be predicted) or unknown (to be predicted), we rely on both fixed
rules and the learning-based approach (ET model). We assign fixed
values for the known nodes while the values of unknown nodes are
to be predicted by the MAP inference:
• Dynamically linked library function nodes are known and are
assigned their names because calls on them are done on the
basis of their names, which are present even in a stripped binary.
User-defined and statically linked function nodes are marked as
unknown because their names do not exist after stripping.
• Flag, instruction, unary operator, constant and location nodes do
not carry high-level information and are thus known. We assign
a flag name, an instruction name and a unary operator name to
the three kinds of nodes respectively, integer or string values
to constant nodes, and names of registers or memory offsets to
locations nodes.
• Register and memory offset nodes are known and assigned a
special value (?), if they are dummy variables created by BAP
or used for function prologue, function epilogue and argument
passing. Those nodes correspond to temporary use and do not
carry meaningful debug information, hence we do not aim to
make predictions involving these.
Relationship
Template
Condition for adding an edge
Element
used in
Function
Function Call
Instruction
Location
Locality
Dependency
Operation
Conditional
Argument
Operation
Conditional
Argument
Name & Type
( f, v, func-loc(v) )
( f, a, arg-loc(a) )
( f, c, func-str )
( f, s, func-stack )
( f1, f2, call )
Function Relationships
variable v is accessed inside the scope of function f
variable a is an argument of function f by calling conventions
string constant c is accessed inside the scope of function f
stack location s is allocated for function f
function f2 is called by function f1
Variable Relationships
( v, insn, insn-loc(v) )
( v, l, locates-at )
( v1, v2, local-loc(v1) )
( v1, v2, dep-loc(v1)-loc(v2) )
( v, op, unary-loc(v) )
( n1, n2, op-loc(n1)-loc(n2) )
( v1, v2, phi-loc(v1) )
( v, op, cond-unary )
( n1, n2, cond-op-loc(n1)-loc(n2) )
( f, a, call-arg-loc(a) )
there is an instruction insn (e.g., add) that operates on variable v
variable v locates at location l (e.g., memory offset mem[RSP+16])
variable v1 and v2 are locally allocated (e.g., EDX.2 and EDX.3)
variable v1 is dependent on variable v2
unary operation op (e.g. unsigned and low cast) on variable v
binary operation op (e.g., +, left shift « and etc.) on node n1 and n2
there is a ϕ expression in BAP-IR: v1 = ϕ (... v2, ...)
there is a conditional expression op (v) (e.g., not(EAX.2))
there is a conditional expression n1 op n2 (e.g. EDX.3!=ECX.1)
there is a call f (..., a, ...) with argument a
Type Relationships
( t, op, t-unary-loc(t) )
( t1, t2, t-op-loc(t1)-loc(t2) )
( t1, t2, t-phi-loc(t1) )
( t, op, t-cond-unary )
( t1, t2, t-cond-op-loc(t1)-loc(t2)) )
( f, t, t-call-arg-loc(t) )
( v, t, type-loc(v) )
( f, t, func-type )
unary operation op on type t
binary operation op on type t1 and t2
there is a ϕ expression: t1 = ϕ (... t2, ...)
there is a unary conditional expression op (t)
there is a binary conditional expression t1 op t2
call f (..., t, ...) with an argument of type t
variable v is of type t
function f is of type t
Table 1: Pairwise relationships for linking code elements in Debin. The first column provides a short description of each rela-
tionship and the second column defines the relationship template. Helper function loc is used to encode location information
in relationships. Given an input node, the function returns its location (e.g., the register or the memory offset, or whether it
is a constant). We add a relationship edge to the dependency graph if the condition defined in the third column holds.
• For other register and memory offset nodes, we leverage our ET
model discussed in Section 3.1 to determine whether they are
known or unknown. To invoke that algorithm, for every target
node, we encode features similar to those defined later in Ta-
ble 1 (treated as strings) and use a one-hot encoding of these
strings to construct a feature vector for the node. We provide this
vector as input to the algorithm which then classifies the node
as known or unknown. For unknown nodes, their values will be
predicted during MAP inference. For known nodes, we assign the
value (?).
• Type nodes of unknown functions are unknown. Type nodes of
unknown variable registers and memory offsets are unknown and
type nodes of non-variables are known and are assigned (?).
4.4 Relationships between Elements
We now describe the relationships that connect program elements
and enable structured prediction on the resulting dependency graph.
We define all pairwise relationships in Table 1. The pairwise rela-
tionships are of the form (a, b, rel) specified by the second column,
meaning that node a is connected to node b by the relationship
named rel. A relationship is added to the dependency graph as
an edge when the condition defined in the third column holds. For
example, the edge (foo, bar, call) is added only when function
foo calls function bar. To encode location information of variables
in relationships, we define a function loc, which takes a node as
input and outputs the location of the node (e.g., the register or the
memory offset, or whether it is a constant).
Function Relationships. This set of relationships captures how
functions interact with other binary code elements. First, a function
node is connected to register-allocated and memory-allocated vari-
ables that appear inside its scope. This relationship encodes the way
functions manipulate registers and memory offsets to modify vari-
ables. For the example in Figure 2, the function node representing
sum should be connected to nodes EDX.2, ECX.1 and etc. Second,
we relate function nodes and their register-based or offset-based
arguments, which are determined by the calling convention of the
specific platform. Third, we link function nodes with two categories
of known elements, string constants and stack locations. These two
relationships are helpful in recovering the patterns for functions
to deal with strings and allocate stack resources. Finally, we also
incorporate function call relationships.
Variable Relationships. We leverage a comprehensive list of
relationships for register and memory offset variables. Those re-
lationships provide useful syntactic and semantic patterns of how
variables are allocated and manipulated.
Instruction relationships capture how instructions operate with
variables. For instance, a relationship (ECX.1, cmp, insn-ECX) can
be built for our example in Figure 2. Moreover, locality of relation-
ships is captured through our analysis. Two register variables are
locally allocated when they have the same register name and the dif-
ference between their SSA versions is 1. The idea here is that locally
allocated registers are possibly from neighboring instructions and
thus allocated for the same variable. Two memory offset variables
are locally allocated when they can be determined to be aligned next
to each other. We link those memory offsets also because they may
be allocated for the same variable, especially for variables of a larger
structure. For example, apart from the dep-EDX-EDX relationship,
we also add the loc-EDX edge between nodes EDX.2 and EDX.3 in
Figure 2(d) (not graphically shown in the figure). In addition, depen-
dency, operation and conditional relationships formalize behaviors
of various BAP-IR statements. Finally, we determine arguments of
a function call through calling conventions of each architecture and
connect the called function and its arguments. This relationship is
helpful for predicting both function names and variable names.
Type Relationships. Type relationships are helpful in discover-
ing how binaries handle types. First, operation, conditional and
argument relationships are also introduced for types because these
three sets of relationships are effective for both variable names and
types. For example, variables accessed in conditional expressions
are likely to be of type bool and the type of function arguments
should comply with signatures of the called function. Furthermore,
type nodes are also connected to their corresponding name nodes.
Factor Relationships. Apart from pairwise relationships, we also
define three factor relationships. The first one connects all nodes
that appear in the same ϕ expression of BAP-IR. The second one
further explores the behavior of function calls, linking function
node of a call and its arguments. Third, we relate together elements
that are accessed in the same statement. These factor relationships
can link more than two elements and thus capture semantic be-
haviors of binary code that are not expressible with only pairwise
relationships.
4.5 Feature Functions
Recall that in Section 3.2, we defined feature functions in the general
sense. We now provide the feature functions used by Debin and
illustrate how these feature functions are obtained from the tem-
plates during training.
Let D = {(V (i ), E (i ), F (i ) )}t
truth values. For each edge (vm, vn, rel ) ∈(cid:83)t
i =1 be a set of graphs extracted from
non-stripped binaries. Nodes in these graphs are filled with ground
i =1 E (i ), we generate
pairwise feature functions ψi as follows:
′
if A = am, A′ = an, Rel = rel
where am and an are assignments to vm and vn, respectively. Fur-
i =1 F (i ), we define a factor feature
ψi (A, A
, Rel ) =
1
thermore, for each factor Lk ∈(cid:83)t
1
function as follows:
φj (L) =
0
0 otherwise
if L = Lk
otherwise
where L ∈ Labels+ is the ordered list of values (names/types) as-
signed to the nodes connected by a factor.
Intuitively, the feature functions defined above serve as indica-
tors for the relationship and node assignment patterns discovered
from the training set. We instantiate the feature templates with
ground truth assignments, obtaining a large number of feature func-
tions. The weights associated with every feature function are then
learned in the training phase. The combination of feature indicator
functions and weights is then used by the MAP inference.
5 IMPLEMENTATION AND EVALUATION
In this section, we present the implementation details of Debin 4.
Then we discuss our extensive evaluation of the system: we evaluate
Debin’s accuracy on predicting debug information and illustrate
use cases where Debin can be leveraged for practical security anal-
ysis.
5.1 Implementation
Debin extracts dependency graphs (defined in Section 4) from
BAP-IR. We developed a plugin for the BAP platform in order to ob-
tain BAP-IR for the input binaries. Variable recovery classification
is implemented using the machine learning package scikit-learn [7].
To annotate the correct values for the training graphs, we parse
DWARF [8] debug information from non-stripped binaries using
the pyelftools package [6]. After obtaining the MAP inference re-
sult using the Nice2Predict [5] framework, program variables are
reconstructed according to the predicted names and are associated
via a one-to-many mapping to their locations in registers or mem-
ory offsets. We format all of the rebuilt information according to
the DWARF standard and utilize the ELFIO library [2] to produce
the final output binary. All our experiments were conducted on a
server with 512GB of memory and 2 AMD EPYC 7601 CPUs (64
cores in total, running at 2.2 GHz).
5.2 Evaluation Setup
Now we describe our dataset and metrics for evaluating Debin’s
prediction accuracy.
Dataset. Currently, Debin supports ELF binaries on x86, x64 and
ARM architectures. For each architecture, we collected 3000 non-