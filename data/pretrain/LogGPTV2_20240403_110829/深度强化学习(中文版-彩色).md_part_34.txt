J(θ)= logπ (A |S )Aˆ
t P θ t t t
J Vπθ(ψ)= tAˆ2
t
ψ
(g ψ,g θ)=(∇J Vπθ(ψ),∇J(θ))
ψ
返回(g ,g )
ψ θ
5.6 异步优势 Actor-Critic
异步优势Actor-Critic（AsynchronousAdvantageActor-Critic,A3C）(Mnihetal.,2016)是上一
节中A2C的异步版本。在A3C的设计中，协调器被移除。每个Worker节点直接和全局行动者和
全局批判者进行对话。Master节点则不再需要等待各个Worker节点提供的梯度信息，而是在每
次有Worker节点结束梯度计算的时候直接更新全局Actor-Critic。由于不再需要等待，A3C有比
A2C更高的计算效率。但是同样也由于没有协调器协调各个Worker节点，Worker节点提供梯度
信息和全局Actor-Critic的一致性不再成立，即每次Master节点从Worker节点得到的梯度信息很
可能不再是当前全局Actor-Critic的梯度信息。
注 5.2 虽然 A3C 为了计算效率而牺牲 Worker 节点和 Master 节点的一致性这一点看起来有些特
殊，这种异步更新的方式在神经网络的更新中其实非常常见。近期的研究(Mitliagkasetal.,2016)
还表明，异步更新不仅加速了学习，还自动为SGD产生了类似于动量（Momentum）的效果。
153
第5章 策略梯度
算法5.21A3C
Master:
超参数:步长η 和η 、当前策略函数π 、价值函数Vπθ。
ψ θ θ ψ
输入: 梯度g ,g 。
ψ θ
ψ =ψ−η g ;θ =θ+η g 。
ψ ψ θ θ
返回(Vπθ,π )
ψ θ
Worker:
超参数: 奖励折扣因子γ、轨迹长度L。
输入: 策略函数π 、价值函数Vπθ。
θ ψ
(g ,g )=(0,0)
θ ψ
fork =1,2,··· ,do
(θ,ψ)= Master(g ,g )
θ ψ
执行L步策略π ,保存{S ,A ,R ,S }。
θ t t t t+1
估计优势函数Aˆ =R +γVπθ(S )−Vπθ(S )
P t t ψ t+1 ψ t
J(θ)= logπ (A |S )Aˆ
t P θ t t t
J Vπθ(ψ)= tAˆ2
t
ψ
(g ψ,g θ)=(∇J Vπθ(ψ),∇J(θ))
ψ
endfor
5.7 信赖域策略优化
截至目前，我们在本章中介绍了初版策略梯度方法及其并行计算版本。在异步Actor-Critic的
策略梯度中，我们更新策略如下。
θ =θ+η ∇J(θ), (5.14)
θ
这里
2 3
X∞
∇J(θ)=E τ,θ4 ∇logπ θ(A i|S i)Aπθ(S i,A i)5 , (5.15)
i=0
其中优势函数Aπθ(s,a)定义为
Aπθ(s,a)=Qπθ(s,a)−Vπθ(s). (5.16)
ψ
和标准的梯度下降算法一样，初版策略梯度方法也有步长不好确定的缺陷。梯度 ∇J(θ) 本
身只提供了在当前θ下局部的一阶信息而忽略了奖励函数定义的曲面的曲度。如果在高度弯曲的
区域选择了较大的步长，那么学习算法的性能可能会突然大幅下降。相反地，如果选择的步长太
154
5.7 信赖域策略优化
小，学习的过程可能会太保守，从而非常缓慢。更甚，策略梯度方法中的梯度∇J(θ)需要从基于
当前策略π 收集的样本中估计。策略性能的突然下降或者提升太过缓慢，会反过来影响收集到
θ
的样本的质量，这让学习的性能对于步长的选择更敏感。
初版策略梯度方法的另一个局限是：它的更新是在参数空间，而不是策略空间中进行的。
Z
Π={π|π ⩾0, π =1}. (5.17)
因为相同的步长η 可能使策略π 在策略空间中有完全不一样幅度的更新，这使得步长η 在实
θ θ θ
际应用中更加难以选择。举个例子，考虑当前的策略π =(σ(θ),1−σ(θ))的两种不同情况。这里
σ(θ)是Sigmod函数。假设在第一种情况下，θ被从θ =6更新到了θ =3。而在另一种情况中，θ
被从θ =1.5更新到了θ =−1.5。两种情况π 在参数空间中的更新幅度都是3。然而，在第一种
θ
情况下，π 在策略空间中从几乎是π ≈(1.00,0.00)变成了π ≈(0.95,0.05)，而在另一种情况下，
θ
π =(0.82,0.18)被更新到了π =(0.18,0.82)。虽然两者在参数空间中的更新幅度相同，但是在策
略空间中的更新幅度却完全不同。
在本节中，我们会开发一个能更好处理步长的策略梯度算法。这个算法的思想基于信赖域的
想法，所以被称为信赖域策略优化算法（TrustRegionPolicyOptimization，TRPO）(Schulmanetal.,
2015)。注意到，我们的目标是找到一个比原策略π 更好的策略π′。下述引理为π 和π′ 的性能
θ θ θ θ
提供了一个很深刻的联系：从π θ到π θ′ 在性能上的提升，可以由π θ的优势函数Aπθ(s,a)来计算。
文献(Kakadeetal.,2002)让θ′表示π′ 的参数。
θ
引理5.1
2 3
X∞
J(θ′ )=J(θ)+E τ∼π′ 4 γtAπθ(S t,A t)5 . (5.18)
θ
t=0
P 
这里J(θ)=E τ∼πθ ∞ t=0γtR(S t,A t) ，τ 是由π θ′ 产生的同状态动作轨迹。
所以，学习最优的策略π 等价于最大化以下这个目标
θ
2 3
X∞
E τ∼π′ 4 γtAπθ(S t,A t)5 . (5.19)
θ
t=0
然而，上述式子其实难以直接优化，因为式子中的期望是在π′ 上。基于此，TRPO优化该式子的
θ
一个近似，我们用L (π′)表示，如下式。
πθ θ
2 3
X∞
E τ∼π′ 4 γtAπθ(S t,A t)5 (5.20)
θ
t=0
155
第5章 策略梯度
h  i
=E s∼ρ πθ′(s) hE a∼π θ′(a|s) Aπθ(s,a)|s (5.21)
i
≈E s∼ρπθ(s) "E a∼π θ′(a|s) Aπθ(s,a)|s (5.22)
#
π′(a|s)
=E s∼ρπθ(s) E a∼πθ(a|s) πθ θ(a|s)Aπθ(s,a)|s (5.23)
2 3
=E τ∼πθ4X∞ γt ππ θ′ (( AA t || SS t) )Aπθ(S t,A t)5 . (5.24)
θ t t
t=0
用L (π′)表示上述等式的最后一个式子，即
πθ θ
2 3
L πθ(π θ′ )=E τ∼πθ4X∞ γtπ πθ′( (A At| |S St) )Aπθ(S t,A t)5 .
θ t t
t=0
在上面的式子中，我们直接用ρ πθ(s)来近似ρ θ′(s)。这个近似虽然看似粗糙，但下面的定理在理
π
论上证明了，当π 和π′ 相似的时候，这个近似并不差。
θ θ
定理5.1 让Dmax(π ∥π′)=max D (π (·|s)∥π′(·|s)),那么
KL θ θ s KL θ θ
|J(θ′ )−J(θ)−L (π′ )|⩽CDmax(π ∥π′ ). (5.25)
πθ θ KL θ θ
这里C 是和π′ 无关的常数。
θ
因此，如果Dmax(π ∥π′)很小，那么L (π′)可以合理地被作为一个优化目标。这便是TRPO
KL θ θ πθ θ
的想法。实际中，TRPO试图在平均KL散度的约束下优化L (π′)，如下所示。
πθ θ
max L (π′ ) (5.26)
π′ πθ θ
θ  
s.t. E s∼ρπθ D KL(π θ∥π θ′ ) ⩽δ.
我们进一步讨论如何解TRPO中的这个优化问题。这里我们利用目标函数的一阶近似和约束
的二阶近似。事实上，L (π′)在策略π 处的梯度和Actor-Critic中一样。
πθ θ θ
2 3
τ∼πθ4X∞ γt∇ πθπ (θ′ A(A S|S )t) t)5 
g =∇ θL πθ(π θ′ )| =E |t Aπθ(S t,A  (5.27)
θ 
θ t t
2t=0  θ 3
X∞ 
=E τ∼πθ4 γt∇ θlogπ θ(A t|S t) Aπθ(S t,A t)5 . (5.28)
t=0 θ
156
5.8 近端策略优化
此外，让H 表示E s∼ρπθ D KL(π θ∥π θ′) 的Hessian矩阵，那么，TRPO在当前的π θ 求解如下
优化问题。
θ′ g⊤ (θ′−θ)
=argmax (5.29)
θ′
(θ′−θ)⊤ H(θ′−θ)⩽δ.
s.t.
易见这个问题的解析解存在：
s
2δ
θ′ H−1g.