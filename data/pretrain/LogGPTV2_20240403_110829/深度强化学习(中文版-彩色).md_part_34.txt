### 5.6 异步优势 Actor-Critic

**异步优势Actor-Critic (A3C)** 是 A2C 的异步版本，由 Mnih 等人在 2016 年提出。在 A3C 中，协调器被移除，每个 Worker 节点直接与全局 Actor 和 Critic 进行通信。Master 节点不再需要等待所有 Worker 节点提供的梯度信息，而是在每次有 Worker 节点完成梯度计算时立即更新全局 Actor-Critic。由于不再需要等待，A3C 比 A2C 具有更高的计算效率。然而，由于缺乏协调器来同步各个 Worker 节点，Worker 节点提供的梯度信息可能不再是当前全局 Actor-Critic 的梯度信息，导致一致性问题。

**注释 5.2**: 尽管 A3C 为了计算效率牺牲了 Worker 节点和 Master 节点之间的一致性，这种异步更新的方式在神经网络训练中非常常见。Mitliagkas 等人（2016）的研究表明，异步更新不仅加速了学习过程，还为随机梯度下降（SGD）提供了类似动量的效果。

### 算法 5.21: A3C

**Master:**
- **超参数**: 学习率 \(\eta_\psi\) 和 \(\eta_\theta\)、当前策略函数 \(\pi_\theta\)、价值函数 \(V_{\pi_\theta}^\psi\)。
- **输入**: 梯度 \(g_\psi, g_\theta\)。
- **更新规则**:
  \[
  \psi = \psi - \eta_\psi g_\psi; \quad \theta = \theta + \eta_\theta g_\theta
  \]
- **返回**: 更新后的价值函数 \(V_{\pi_\theta}^\psi\) 和策略函数 \(\pi_\theta\)。

**Worker:**
- **超参数**: 奖励折扣因子 \(\gamma\)、轨迹长度 \(L\)。
- **输入**: 策略函数 \(\pi_\theta\)、价值函数 \(V_{\pi_\theta}^\psi\)。
- **初始化**: 梯度 \(g_\theta, g_\psi = (0, 0)\)。
- **循环执行**:
  1. 从 Master 获取最新的 \((\theta, \psi)\)。
  2. 执行 \(L\) 步策略 \(\pi_\theta\)，保存状态动作对 \(\{S_t, A_t, R_t, S_{t+1}\}\)。
  3. 估计优势函数 \(\hat{A}_t = R_t + \gamma V_{\pi_\theta}^\psi(S_{t+1}) - V_{\pi_\theta}^\psi(S_t)\)。
  4. 计算目标函数:
     \[
     J(\theta) = \sum_t \log \pi_\theta(A_t | S_t) \hat{A}_t
     \]
     \[
     J(V_{\pi_\theta}^\psi) = \sum_t (\hat{A}_t)^2
     \]
  5. 计算梯度:
     \[
     (g_\psi, g_\theta) = (\nabla J(V_{\pi_\theta}^\psi), \nabla J(\theta))
     \]

### 5.7 信赖域策略优化

在本章中，我们介绍了初版策略梯度方法及其并行计算版本。在异步 Actor-Critic 的策略梯度中，策略更新如下：
\[
\theta = \theta + \eta \nabla J(\theta)
\]
其中
\[
\nabla J(\theta) = E_{\tau, \theta} \left[ \sum_{i=0}^\infty \nabla \log \pi_\theta(A_i | S_i) A_{\pi_\theta}(S_i, A_i) \right]
\]
优势函数 \(A_{\pi_\theta}(s, a)\) 定义为
\[
A_{\pi_\theta}(s, a) = Q_{\pi_\theta}(s, a) - V_{\pi_\theta}(s)
\]

初版策略梯度方法存在步长选择困难的问题。梯度 \(\nabla J(\theta)\) 只提供局部一阶信息，忽略了奖励函数曲面的曲度。如果在高度弯曲的区域选择了较大的步长，性能可能会大幅下降；如果步长太小，学习过程会非常缓慢。此外，策略梯度方法中的梯度需要从当前策略 \(\pi_\theta\) 收集的样本中估计，这使得步长选择更加敏感。

另一个局限是，策略梯度方法在参数空间而不是策略空间中进行更新。相同的步长 \(\eta\) 在策略空间中可能导致完全不同的更新幅度。例如，考虑当前策略 \(\pi = (\sigma(\theta), 1 - \sigma(\theta))\) 的两种情况。假设 \(\theta\) 从 6 更新到 3，或从 1.5 更新到 -1.5。尽管参数空间中的更新幅度相同，但在策略空间中的更新幅度却完全不同。

**信赖域策略优化 (TRPO)** 基于信赖域的思想，旨在更好地处理步长问题。引理 5.1 提供了 \(\pi_\theta\) 和 \(\pi'_{\theta'}\) 性能之间的关系：
\[
J(\theta') = J(\theta) + E_{\tau \sim \pi'_{\theta'}} \left[ \sum_{t=0}^\infty \gamma^t A_{\pi_\theta}(S_t, A_t) \right]
\]
其中 \(\tau\) 是由 \(\pi'_{\theta'}\) 生成的状态动作轨迹。

因此，学习最优策略 \(\pi_\theta\) 等价于最大化以下目标：
\[
E_{\tau \sim \pi'_{\theta'}} \left[ \sum_{t=0}^\infty \gamma^t A_{\pi_\theta}(S_t, A_t) \right]
\]

TRPO 通过近似优化该目标，并引入平均 KL 散度约束来确保新旧策略之间的相似性：
\[
\max_{\pi'_{\theta'}} L_{\pi_\theta}(\pi'_{\theta'})
\]
\[
\text{s.t. } E_s \left[ D_{KL}(\pi_\theta \| \pi'_{\theta'}) \right] \leq \delta
\]

具体来说，TRPO 在当前策略 \(\pi_\theta\) 处求解以下优化问题：
\[
\theta' = \arg \max_{\theta'} g^\top (\theta' - \theta)
\]
\[
\text{s.t. } (\theta' - \theta)^\top H (\theta' - \theta) \leq \delta
\]

其中 \(H\) 是 \(E_s [D_{KL}(\pi_\theta \| \pi'_{\theta'})]\) 的 Hessian 矩阵。这个问题的解析解为：
\[
\theta' = \theta + \sqrt{\frac{2\delta}{g^\top H^{-1} g}} H^{-1} g
\]