NA
NA
35%
30%
30%
50%
50%
35%
30%
40%
20%
50%
50%
50%
NA
50%
50%
45%
10%
50%
Table 2: List of the most unactionable advice based on user ratings. The ﬁrst four columns indicate advice with median rating of
“not at all” conﬁdent and“very” time consuming, disruptive, and/or diﬃcult. The ﬁfth column indicates expert-perceived eﬃcacy
and the sixth column provides expert-estimated median risk reduction for eﬃcacious advice (negative for harmful advice).
nance was also considered quite actionable: 94.1% of ﬁnance
advice was perceived as at most “slightly” time-consuming
or disruptive to implement, and more than 80% of this advice
was perceived as at most “slightly” diﬃcult to implement.
Advice about passwords scored well on two of the four action-
ability submetrics: for more than 80% of passwords advice
people were at least “somewhat” conﬁdent they could im-
plement it and perceived it as at most “slightly” diﬃcult to
implement.
The least-actionable advice is about data storage and
network security. The topic with the highest proportion of
poor (lowest two ratings on Likert scale) actionability ratings,
across all four metrics, was data storage. More than half the
data storage imperatives received conﬁdence responses of
“slightly” or “not at all,” there was no advice about data stor-
age for which people were “very” conﬁdent. Similarly, 58.8%,
41.2%, and 47.1% of the imperatives about data storage were
rated at least “somewhat” time consuming, disruptive, and
diﬃcult to implement, respectively. Advice about network
security performed nearly as badly on three of the four ac-
tionability submetrics; participants were conﬁdent they could
implement barely half the advice about network security, and
they perceived at least 40% of network security advice as
“very” time consuming or diﬃcult to implement.
Privacy advice polarizing in perceived actionability. It
is additionally interesting to note that the actionability ratings
for privacy advice were quite split. Near-equal proportions
of privacy advice were rated as at least “somewhat” time
96    29th USENIX Security Symposium
USENIX Association
USENIX Association
29th USENIX Security Symposium    97
98    29th USENIX Security Symposium
USENIX Association
USENIX Association
29th USENIX Security Symposium    99
security, and ﬁnance achieved at least partial comprehension
on average (Cloze scores, mean across the topic, above 50%).
Finance-related documents had particularly low variance in
scores, with a standard deviation of 6.22%.
The remaining topics had mean Cloze scores under 50%,
indicating that the majority of test takers struggled to compre-
hend the average text on these topics. Password- and network-
security-related documents had particularly low mean scores,
with very wide score spreads. Passwords was the most popular
topic in the corpus and also had the highest standard deviation
in Cloze scores; we therefore hypothesize that the low scores
may be at least partially about quantity. On the other hand,
network security is a particularly technical topic, so the low
scores may relate to additional complexity or jargon.
There was no signiﬁcant diﬀerence in reading ease percep-
tions among diﬀerent topics (p = 0.999, Kruskal-Wallis13).
8.3 Comprehensibility by Domain
The most comprehensible sources are general news chan-
nels, subject-matter experts (SMEs), non-proﬁts, and se-
curity and computer-repair companies. To understand
whether some advice-givers provided more readable advice
than others, we examined Cloze scores grouped by domain.
Figure 9 summarizes these results. The Cloze scores of the
domains were signiﬁcantly diﬀerent: p < 0.001, ANOVA (all
pairwise tests remain signiﬁcant after Holm-Bonferonni cor-
rection). Of the 30 domain groups we considered, seven
scored above 50% (mean across documents): SMEs, gen-
eral news outlets, how-to websites, non-tech-focused and
tech-focused nonproﬁt organizations, security companies, and
computer-repair companies. Within particular categories, we
see that some organizations perform better than others (Ap-
pendix B); we discuss the more notable cases of variance
below. As with topics, there was not a signiﬁcant diﬀerence
in ease perception by domain (p = 0.999, Kruskal-Wallis).
Government organizations. Among U.S. government
organizations, ic3.gov, whitehouse.gov, ftc.gov, and
dhs.gov had average scores mapping to partial comprehen-
sion or better; the remaining domains perform worse. We
had only ﬁve non-U.S. government domains in our dataset,
three of which (csir.co.za, staysmartonline.gov.au,
and connectsmart.gov.nz) had mean scores of partial
comprehension or above.
Child-focused organizations. Encouragingly, documents
from non-proﬁt organizations (both technology focused and
not) that were aimed toward children (e.g., childline.org.uk,
netsmartz.org, safetynetkids.org.uk) appear to be among the
most readable. That said, content collected from school web-
sites was not particularly readable, with mean Cloze scores
indicating low comprehension, suggesting that schools may
13Ease scores were not normally distributed, so we use a non-parametric
test; Cloze scores, on the other hand, were near-normal in a QQ plot and are
thus evaluated with an ANOVA.
be better oﬀ obtaining content from child-focused nonproﬁt
organizations.
Technical non-proﬁts. Documents from non-proﬁt orga-
nizations with technical focus had wider variance. Docu-
ments from the Tor Project, GNU, and techsoup.org had
mean Cloze scores of at least partial comprehension. How-
ever, documents from nine other technical non-proﬁts, in-
cluding Mozilla, Chromium, and Ubuntu as well as organi-
zations focused speciﬁcally on helping non-experts (e.g., li-
braryfreedomproject.org) had mean Cloze scores well below
this threshold. Documents from the EFF and Tactical Tech-
sponsored organizations also had mean Cloze scores mapping
to low comprehension. This is important, as documents from
these two organizations make up 21% of our corpus.
Corporations. Security-focused companies and those of-
fering computer-repair services both scored very high on
comprehensibility. We hypothesize that for these companies,
which focus on lay users as customers, providing readable
materials may be tantamount to a business requirement. On
the other hand, non-security-focused companies — including
some frequently under ﬁre for privacy and security issues —
scored poorly: mean Cloze scores for Google, Facebook, and
Apple were 45.1%, 37.9%, and 41.7%, respectively.
Low-comprehension platforms. Finally, seven of the 30
advice-givers we examined provided particularly diﬃcult
to read advice (mean Cloze scores under 40%): SANS
(e.g., malwaretips.com,
(sans.org), security forums
wilderssecurity.com), MOOC
(e.g.,
lynda.com, khanacademy.org), consumer rating sites (e.g.,
consumerreports.org, av-comparatives.org), Face-
book, Technical Q&A websites (e.g., stackoverflow.com,
stackexchange.com), and academic publications.
platforms
While it is not necessarily problematic for more technical
content such as that from academic security publications and
security forums to be incomprehensible to the the average
person, low readability from organizations such as the Library
Freedom Project, MOOCs, Facebook Help pages, and Tech-
nical Q&A websites may make it diﬃcult for non-experts to
stay secure.
9 Discussion
This work makes three primary contributions.
We create a taxonomy of 374 pieces of security advice.
This work provides a comprehensive point-in-time taxonomy
of 374 end-user security behaviors, including 204 pieces of
security advice that were not previously catalogued in the
literature. The full set of behaviors can be explored here:
https://securityadvice.cs.umd.edu. This taxonomy
provides (i) insight into the scope and quantity of advice re-
ceived by users, (ii) a tool for researchers to consult when
considering what security and privacy behaviors to study or
analyze, and (iii) a mechanism for the broader security com-
munity to move forward with improving security advice by
100    29th USENIX Security Symposium
USENIX Association
identifying advice in need of repair or retirement.
We develop and evaluate axes of security advice quality.
Our approach to evaluating security advice is in itself a contri-
bution: the axes of quality that we identify (comprehensibility,
actionability, and eﬃcacy) and the measurement approaches
we designed to assess them can be applied to new advice that
is created to ensure that as we move forward in advice-giving,
we create higher-quality, more eﬀective advice. Before we can
recommend further use of these evaluation strategies, how-
ever, we must be convinced of their validity. Speciﬁcally, do
the quality measurements correlate with behavior adoption
(the ultimate goal of security advice), are the measurements
discriminant, and are the measurements consistent with prior
work (where applicable)? In an initial validation using the
results of our work, we ﬁnd that our metrics indeed correlate
with (reported) adoption, lending support for the importance
of the advice quality factors we have operationalized. We
ﬁnd that all four of our actionability sub-metrics correlate
with reported behavior adoption by users. Additionally, we
ﬁnd that priority ranking — one of our metrics of eﬃcacy —
strongly correlates with reported adoption as well, for both
general users and experts.
We also ﬁnd that our quality metrics are indeed discrim-
inant: that is, they measure diﬀerent components of advice
quality. For example, while network security was least read-
able and also had low actionability, data storage did quite well
on readability while scoring consistently low on actionabil-
ity. Similarly, documents containing advice about software
security and antivirus were among the more diﬃcult to read,
but were not high in implementation diﬃculty, indicating that
readability of the document containing the advice is diﬀerent
from the actionability of the advice itself.
Further, we examine whether we can replicate the results of
prior studies in which security experts were asked to prioritize
20 pieces of security advice [10, 26, 50]. We ﬁnd that our
prioritization results replicate these quite closely. Two of the
three behaviors given “number one” priority by our experts
overlap with the top three behaviors suggested by experts in
both papers: “update system” and “use unique passwords.”
The third-most-important behavior identiﬁed by both papers
“use two-factor auth”, is rated as a “top 3” priority by our
experts and ranked #25 out of 374 across all of our advice.
Of course, this preliminary validation connects these axes
of advice quality to reported, rather than actual, behavior.
Replication is necessary to fully validate any new metrics,
and to examine how they perform in broader application (e.g.,
having both users and experts rate the eﬃcacy of the advice).
We rigorously evaluate the comprehensibility, per-
ceived eﬃcacy, and perceived actionability of our corpus.
By applying our metrics to the taxonomy we developed, we
provide a thorough and novel characterization of the quality of
the security-advice ecosystem. While prior work focused on
expert and user prioritization of a small set of security advice
(at most, 20 topics) [10, 26, 50], we evaluate a much larger set
of advice and conduct a more comprehensive evaluation that
considers not only prioritization, but also comprehensibility,
perceived actionability, perceived eﬃcacy, and how these fac-
tors interact. Further, our metrics allow us (diﬀerently from
prior work) to characterize both generalized advice impera-
tives and speciﬁc wording within particular documents.
Overall, we ﬁnd that security advice is perceived as fairly
actionable — only 49 advice imperatives were rated by users
as ’very’ unactionable on one of our four metrics – as well
as eﬀective. The majority of security advice (89%) was per-
ceived as eﬀective by professional security experts.
Yet, we know that users do not adopt even a fraction of this
advice consistently, despite their best intentions [43,56,67,68].
This may be due in part to mis-comprehension of the instruc-
tions: the hundreds of documents we evaluate exhibit only low
to partial comprehensibility for the general public. A larger
factor, however, appears to be a crisis of advice prioritization.
The 41 professional security experts consulted in this study
not only evaluated 89% of the advice in our corpus as accu-
rate, but reported that 118 pieces of advice were in the top 5
items they would recommend to users. By asking people to
implement an infeasible number of behaviors, with little guid-
ance on which is the most important, we slowly chip away at
compliance budgets [3], leaving users haphazardly selecting
among hundreds of “actionable,” “eﬀective,” “high-priority”
behaviors.
10 Next Steps
Our results suggest two key directions of focus for moving
toward a healthier ecosystem of security advice.
Measurement and a new focus on minimality. We as se-
curity experts and advice givers have failed to narrow down a
multitude of relatively actionable, but half-heartedly followed,
security behaviors to a key, critical set that are most impor-
tant for keeping users safe. The U.S. government alone oﬀers
205 unique pieces of advice to end users, while non-technical
news media, such as CNN and Forbes, oﬀers over 100 unique
pieces of advice to users. This overload of advice aﬀects a
large portion of the user population: prior work [43, 45] sug-
gests the government is a primary source of advice for more
than 10% of users, while 67.5% of users report getting at least
some of their security advice through the news media.
Our struggle as experts to distinguish between more and
less helpful advice may be due to unfalsiﬁability: being un-
able to identify whether a piece of advice is actually useful,
or prove when it is not. Without measurement of impact on
actual security, or proven harm, we presume that everything
is slightly useful against potential harms. Fixing this prob-
lem will require rigorous measurement (e.g., comparing the
eﬀect of diﬀerent practices on frequency of compromise) to
evaluate which behaviors are the most eﬀective, for which
users, in which threat scenarios. It will also require a strong
commitment among security advice givers to minimality and
USENIX Association
29th USENIX Security Symposium    101
practicality: empirically identifying the smallest and most
easily actionable set of behaviors to provide the maximum
user protection.
If we do not make changes to our advice-giving approach,
this situation is destined to get worse. As new attacks continue
to emerge, we are likely to continue to issue new, reactive
advice without deprecating old advice (that might still be at
least somewhat useful) or reevaluating overall priorities [25].
Further, we need to explore how to better disseminate updates
to best practices. For example, many experts in our study
were still emphasizing password changes and avoiding stor-
ing passwords, despite this advice having been updated and
disproven in the most recent NIST standards [20]. Delays in
propagating new priorities among experts will surely translate
into even more severe lags in end-user behavior.
Based on our analysis, the U.S. government is currently
the giver of the most advice. Unifying the voices across the
government into a single central authority for both end-users
and external experts to turn to for validated best practices –
similar to the role police departments serve for community
education on car break-ins, or the role of the surgeon general
for health advice – may help to cut down on inconsistent or
delayed updates to advice. A similar eﬀort could be made to
reduce redundancy across trusted non-proﬁts and advocacy
groups by encouraging such groups to all support a centralized
advice repository rather than each providing their own.
Fixing existing advice. While the primary outcome of this
work is that we need less advice and more empirical measure-
ment, we do note that a few topics of advice performed con-
sistently worse than others across our evaluations and thus are
good candidates for revision and improvement. Advice about
data storage topics (e.g., “Encrypt your hard drive,” “Regu-
larly back up your data,” “Make sure to overwrite ﬁles you
want to delete”) scored poorly in actionability across our met-
rics. This raises questions about whether we should be giving
this advice to end users in the ﬁrst place, and if so, how these
technical concepts can better be expressed in an actionable
way. Network-security advice performed nearly as poorly,
especially on user ratings of conﬁdence, time consumption
and diﬃculty. This is perhaps even more concerning, as the
advice on network security is far more general (e.g., “Use a
password to protect your WiFi,” “Secure your router,” “Avoid
using open Wi-Fi networks for business, banking, shopping”).
Privacy advice was more of a mixed bag. While a quarter
of the advice about privacy was rated as unactionable, a sig-
niﬁcant proportion of the remaining privacy advice scored
quite high on actionability. Experts were less positive toward
any privacy advice, with no advice about privacy being rated
among the top 3 practices experts would recommend. As
privacy becomes increasingly important, and prominent in
users’ awareness, there appears to be signiﬁcant room for
improvement.
Additionally, across all topics, many advice articles com-
bined a diverse set of advice types that could be appropriate to
diﬀerent users; future work may wish to examine whether this
is eﬀective or whether articles focused on a single context are
most appropriate. Relatedly, future work may wish to pursue
mechanisms for personalizing advice to users or helping users
ﬁlter to advice that is most relevant to them, as searches for
security advice are likely to surface context-broad advice that
may or may not have direct relevance.
Acknowledgements
We are grateful to the reviewers and especially to our shep-
herd Mary Ellen Zurko for their feedback and guidance. This
material is based upon work supported by a UMIACS contract
under the partnership between the University of Maryland and
DoD. Elissa M. Redmiles additionally wishes to acknowledge
support from the National Science Foundation Graduate Re-
search Fellowship Program under Grant No. DGE 1322106
and a Facebook Fellowship.
References
[1] Alessandro Acquisti, Idris Adjerid, Rebecca Balebako,
Laura Brandimarte, Lorrie Faith Cranor, Saranga Ko-
manduri, Pedro Giovanni Leon, Norman Sadeh, Florian
Schaub, Manya Sleeper, et al. Nudges for privacy and
security: Understanding and assisting users? choices
online. ACM Computing Surveys (CSUR), 50(3):44,
2017.
[2] Elham Al Qahtani, Mohamed Shehab, and Abrar Aljo-
hani. The eﬀectiveness of fear appeals in increasing
smartphone locking behavior among saudi arabians. In
SOUPS 2018: Symposium on Usable Privacy and Secu-
rity, 2018.
[3] A. Beautement, M. A. Sasse, and M. Wonham. The
compliance budget: Managing security behaviour in or-
ganisations. In NSPW 2009: New Security Paradigms
Workshop, 2008.
[4] JM Blythe and CE Lefevre. Cyberhygiene insight report.
2017.
[5] Joseph Bonneau, Cormac Herley, Paul C Van Oorschot,
and Frank Stajano. The quest to replace passwords: A
framework for comparative evaluation of web authenti-
cation schemes. In 2012 IEEE Symposium on Security
and Privacy, pages 553–567. IEEE, 2012.
[6] John R Bormuth. Comparable cloze and multiple-choice
comprehension test scores. Journal of Reading, 1967.
[7] C. Bravo-Lillo, S. Komanduri, L. F. Cranor, R. W.
Reeder, M. Sleeper, J. Downs, and S. Schechter. Your at-