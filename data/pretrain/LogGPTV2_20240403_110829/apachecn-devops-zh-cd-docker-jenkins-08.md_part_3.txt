```
$ docker node ls
ID                          HOSTNAME        STATUS  AVAILABILITY  MANAGER STATUS
4mrrmibdrpa3yethhmy13mwzq   ubuntu-worker2  Ready   Drain 
kzgm7erw73tu2rjjninxdb4wp * ubuntu-manager  Ready   Active        Leader
yllusy42jp08w8fmze43rmqqs   ubuntu-worker1  Ready   Active   
```
不出所料，`ubuntu-worker2`节点是可用的(状态`Ready`，但是它的可用性被设置为耗尽，这意味着它不托管任何任务。如果我们想要取回节点，我们可以检查其对`active`的可用性:
```
$ docker node update --availability active ubuntu-worker2
```
A very common practice is to drain the manager node and, as a result, it will not receive any tasks, but do management work only.
清空节点的另一种方法是从工作器执行`docker swarm leave`命令。然而，这种方法有两个缺点:
*   有一会儿，副本比预期的要少(在离开群之后，在主节点开始其他节点上的新任务之前)
*   主节点不控制节点是否仍在集群中
由于这些原因，如果我们计划停止工作器一段时间，然后将其恢复，建议使用引流节点功能。
# 多个管理器节点
只有一个管理器节点是有风险的，因为当管理器机器停机时，整个集群都停机了。显然，这种情况在关键业务系统中是不可接受的。在本节中，我们将介绍如何管理多个主节点。
为了向系统添加新的管理器节点，我们需要首先在(当前为单个)管理器节点上执行以下命令:
```
$ docker swarm join-token manager
To add a manager to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-5blnptt38eh9d3s8lk8po3069vbjmz7k7r3falkm20y9v9hefx-a4v5olovq9mnvy7v8ppp63r23 \
192.168.0.143:2377
```
输出显示了令牌和需要在应该成为管理器的机器上执行的整个命令。执行之后，我们应该看到添加了一个新的管理器。
Another option to add a manager is to promote it from the worker role using the `docker node promote ` command. In order to get it back to the worker role, we can use the `docker node demote ` command.
假设我们增加了两个额外的经理；我们应该会看到以下输出:
```
$ docker node ls
ID                          HOSTNAME         STATUS  AVAILABILITY  MANAGER STATUS
4mrrmibdrpa3yethhmy13mwzq   ubuntu-manager2  Ready   Active 
kzgm7erw73tu2rjjninxdb4wp * ubuntu-manager   Ready   Active        Leader
pkt4sjjsbxx4ly1lwetieuj2n   ubuntu-manager1  Ready   Active        Reachable
```
请注意，新经理的经理状态设置为可联系(或留空)，而旧经理是领导者。这样做的原因是，总有一个主节点负责所有 Swarm 管理和编排决策。领导者是使用 Raft 共识算法从经理中选出的，当它关闭时，会选出一个新的领导者。
Raft is a consensus algorithm that is used to make decisions in distributed systems. You can read more about how it works (and see a visualization) at [https://raft.github.io/](https://raft.github.io/). A very popular alternative algorithm for the same goal is called Paxos.
假设我们关闭`ubuntu-manager`机器；让我们来看看新领导人是如何当选的:
```
$ docker node ls
ID                          HOSTNAME         STATUS  AVAILABILITY  MANAGER STATUS
4mrrmibdrpa3yethhmy13mwzq   ubuntu-manager2  Ready   Active        Reachable
kzgm7erw73tu2rjjninxdb4wp   ubuntu-manager   Ready   Active        Unreachable 
pkt4sjjsbxx4ly1lwetieuj2n * ubuntu-manager1  Ready   Active        Leader
```
请注意，即使当其中一个经理不在时，群也能正常工作。
管理人员的数量没有限制，所以听起来管理人员越多容错性越好。然而，确实有很多经理会对性能产生影响，因为所有与 Swarm 状态相关的决策(例如，添加新节点或领导者选举)都必须在所有使用 Raft 算法的经理之间达成一致。因此，管理器的数量总是容错性和性能之间的权衡。
Raft 算法本身对管理人员的数量有限制。分布式决策必须得到大多数节点的批准，称为法定人数。这一事实意味着推荐奇数个经理。
为了理解为什么，让我们看看如果我们有两个经理会发生什么。在这种情况下，多数是两个，所以如果任何一个经理倒下了，那么就不可能达到法定人数，从而选出领导人。因此，失去一台机器会使整个集群出现故障。我们增加了一个管理器，但是整个集群的容错性降低了。三名经理的情况会有所不同。那么，大多数仍然是两个，所以失去一个经理并不能阻止整个集群。这是事实，即使技术上没有禁止，只有奇数个经理才有意义。
The more the managers in the cluster, the more the Raft-related operations are involved. Then, the `manager` nodes should be put into the drain availability in order to save their resources.
# 调度策略
到目前为止，我们已经了解到经理会自动为任务分配一个工作节点。在本节中，我们深入探讨自动意味着什么。我们提出了 Docker Swarm 调度策略和一种根据我们的需要配置它的方法。
Docker Swarm 使用两个标准来选择正确的工作节点:
*   **资源可用性**:调度器知道节点上可用的资源。它使用所谓的**扩展策略**，试图在负载最小的节点上调度任务，前提是它满足标签和约束指定的标准。
*   **标签和约束**:
    *   标签是节点的属性。有些标签是自动分配的，例如`node.id`或`node.hostname`；其他可以由集群管理员定义，例如`node.labels.segment`
    *   约束是由服务的创建者应用的限制，例如，只选择具有给定标签的节点
Labels are divided into two categories, `node.labels` and `engine.labels`. The first one is added by the operational team; the second one is collected by Docker Engine, for example, operating system or hardware specifics.
例如，如果我们想在具体节点`ubuntu-worker1`上运行 Tomcat 服务，那么我们需要使用以下命令:
```
$ docker service create --constraint 'node.hostname == ubuntu-worker1' tomcat
```
我们还可以向节点添加自定义标签:
```
$ docker node update --label-add segment=AA ubuntu-worker1
```
前面的命令添加了一个标签`node.labels.segment`，值为`AA`。然后，我们可以在运行服务时使用它:
```
$ docker service create --constraint 'node.labels.segment == AA' tomcat
```
该命令仅在标有给定线段`AA`的节点上运行`tomcat`副本。
标签和约束使我们能够灵活地配置运行服务副本的节点。尽管这种方法在许多情况下是有效的，但不应过度使用，因为最好将副本分布在多个节点上，并让 Docker Swarm 负责正确的调度过程。
# Docker 由 DockerSwarm 组成
我们已经描述了如何使用 Docker Swarm 来部署服务，该服务又从给定的 Docker 映像运行多个容器。另一方面，还有 Docker Compose，它提供了一种定义容器之间依赖关系的方法，并支持扩展容器，但是所有的工作都在一个 Docker 主机内完成。我们如何合并这两种技术，以便我们可以指定`docker-compose.yml`文件并在集群上自动分发容器？幸运的是，有 Docker Stack。
# 介绍 Docker 栈
Docker Stack 是一种在 Swarm 集群上运行多个链接容器的方法。为了更好地理解它如何将 Docker Compose 与 Docker Swarm 联系起来，让我们看一下下图:
![](img/86ad1636-d244-4a44-9c67-c64b8080eba1.png)
Docker Swarm 协调哪个容器在哪个物理机器上运行。然而，容器之间没有任何依赖关系，所以为了让它们进行通信，我们需要手动链接它们。相反，Docker Compose 提供了容器之间的链接。在上图的示例中，一个 Docker 映像(部署在三个复制的容器中)依赖于另一个 Docker 映像(部署为一个容器)。然而，所有容器都在同一个 Docker 主机上运行，因此水平扩展仅限于一台机器的资源。Docker Stack 连接了这两种技术，并允许使用`docker-compose.yml`文件运行部署在 Docker 主机集群上的链接容器的完整环境。
# 使用 Docker 栈
举个例子，我们用依赖于`redis`映像的`calculator`映像。让我们把这个过程分成四个步骤:
1.  指定`docker-compose.yml`。
2.  运行 Docker 栈命令。
3.  验证服务和容器。
4.  移除栈。
# 指定停靠点-compose.yml
我们在前面的章节中已经定义了`docker-compose.yml`文件，它看起来类似于下面的文件:
```
version: "3"
services:
    calculator:
        deploy:
            replicas: 3
        image: leszko/calculator:latest
        ports:
        - "8881:8080"
    redis:
        deploy:
            replicas: 1
        image: redis:latest
```
Note that all images must be pushed to the registry before running the `docker stack` command so that they would be accessible from all nodes. It is therefore not possible to build images inside `docker-compose.yml`.
使用呈现的 docker-compose.yml 配置，我们将运行三个`calculator`容器和一个`redis`容器。计算器服务的端点将暴露在端口`8881`上。
# 运行 docker 栈命令
让我们使用`docker stack`命令来运行服务，这将依次启动集群上的容器:
```
$ docker stack deploy --compose-file docker-compose.yml app
Creating network app_default
Creating service app_redis