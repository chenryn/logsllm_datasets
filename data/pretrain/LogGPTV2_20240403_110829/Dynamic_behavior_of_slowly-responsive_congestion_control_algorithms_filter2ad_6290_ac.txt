concern in some cases, this concern can be eliminated by following
the principle of packet conservation.
4.2 “The Bad”:
Potential Drawbacks of
Slowly-Responsive Algorithms
We now turn our attention to two potential drawbacks of TCP-
compatible SlowCC algorithms in highly variable environments: (i)
unfairness with respect to TCP and each other, and (ii) potentially
lower bottleneck link utilization. We study both long- and short-
term fairness in dynamic environments.
4.2.1 Long-term Fairness
)
d
e
z
i
l
a
m
r
o
n
(
h
t
d
w
d
n
a
B
i
Square Wave (competing TCP and TFRC flows)
TCP
TFRC
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0.01
0.1
1
10
100
Length of high/low bw (15Mb/5Mb) period
Figure 7: Throughput of TCP and TFRC ﬂows when the avail-
able bandwidth changes by a 3:1 factor.
)
d
e
z
i
l
a
m
r
o
n
(
h
t
d
w
d
n
a
B
i
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0.01
Square wave (competing TCP and TCP(1/8) flows)
TCP
TCP(1/8)
0.1
1
10
100
Length of high/low bw (15Mb/5Mb) period
Figure 8: Throughput of TCP and TCP(1/8) ﬂows when the
available bandwidth changes by a 3:1 factor.
To investigate long-term fairness in a rapidly-changing environ-
ment, we consider a somewhat extreme scenario where the avail-
able bandwidth is periodically increased to three times its lower
value.
In this scenario, ten long-lived ﬂows (ﬁve TCP and ﬁve
TFRC) compete with a “square-wave” CBR source, using the
topology described in Section 3. The congested link is 15 Mbps,
with only 5 Mbps available to the long-lived ﬂows when the CBR
source is active. This gives a 3:1 variation in the bandwidth avail-
able to the long-lived ﬂows. During an extended high-bandwidth
period in this scenario, we would expect the packet drop rate with
ten long-lived ﬂows to be roughly 0.7%, with an average TCP con-
gestion window of 14.4 packets.
Figure 6: The aggregate throughput for long running SlowCC
ﬂows with a ﬂash crowd of short TCP ﬂows at time 25. Note
that self-clocking helps TFRC(256) become quite responsive to
the ﬂash crowd.
The pseudo-code for this extension to TFRC is as follows:
CALCULATESENDRATE()
/*
SEND RATE is the appropriate sending rate.
CALC RATE is what the equation allows.
RECV RATE is the reported receive rate.
C (cid:21) 1 is a constant, 1.1 in our experiments.
*/
if loss is reported then
SEND RATE = iCALC RATE, RECV RATE
else if NOT SLOWSTART then
SEND RATE = iCALC RATE, C RECV RATE
Thus, after a period of heavy losses in the network, the con-
servative option causes TFRC’s sending rate to immediately
reduce to the reported receive rate. The results of TFRC(256) with
self-clocking are shown in Figures 4 and 5. The improvement rel-
ative to the original TFRC(256) without self-clocking is apparent;
the stabilization cost is also small as in TCP. These simulations
were done with droptail queue management as well and a similar
beneﬁt of self-clocking was seen in those simulations also.
4.1.2 Competing Web Flash Crowd
We also experimented with a more realistic scenario where the
dramatic reduction in bandwidth is caused by a ﬂash crowd of small
Web transfers rather than a new CBR source. The ﬂash crowd is
started at time 25 with a stream of short TCP transfers (10 packets)
arriving at a rate of 200 ﬂows/sec for 5 seconds. Figure 6 shows
the aggregate throughput achieved by the small TCP connections
and the aggregate throughput of the background SlowCC trafﬁc for
three different SlowCC trafﬁc types, TCP(1/2), TFRC(256) with-
out self clocking and TFRC(256) with self clocking. From this
ﬁgure, the beneﬁt of self-clocking in helping SlowCC respond to
1:1 in the results reported here. The value in the NS simulator for
TFRC’s conservative option is C = 1:5.
)
d
e
z
i
l
a
m
r
o
n
(
h
t
d
w
d
n
a
B
i
Square wave (competing TCP and SQRT(1/2) flows)
TCP
SQRT
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0.01
0.1
1
10
100
Length of high/low bw (15Mb/5Mb) period
Figure 9: Throughput of TCP and SQRT(1/2) ﬂows when the
available bandwidth changes by a 3:1 factor.
Our interest is in the relative fairness between TCP and TFRC
as a function of the period of the CBR source. In Figure 7, each
column of marks shows the results from a single simulation, with
one mark giving the observed throughput for each of the ten ﬂows.
The x-axis shows the length in seconds of a combined high- and
low-bandwidth period in that simulation, and the y-axis shows the
throughput normalized by a single ﬂow’s fair share of the available
bandwidth. The two lines show the average throughput received by
the TCP and the TFRC ﬂows.
As Figure 7 shows, overall link utilization is high when the pe-
riod of the CBR source is low, while the overall link utilization
suffers when the period of the CBR source is 0.2 seconds (4 RTTs).
When the period of the CBR source is between one and ten sec-
onds, the TCP ﬂows receive more throughput that the TFRC ﬂows,
showing that varying network conditions favor TCP over TFRC.
In an effort to ﬁnd a scenario where TFRC might compete un-
fairly with TCP, we also ran simulations with a range of patterns
for the competing CBR source, include “sawtooth” patterns with
the CBR source slowly increased its sending rate and then abruptly
entered an OFF period, or reverse sawtooth patterns where the CBR
source abruptly entered an ON period and then slowly decreased
its sending rate down to an OFF period. The results were essen-
tially the same as in Figure 7, with the difference between TCP and
TFRC less pronounced. These results demonstrate that there are
many dynamic scenarios when TCP receives more bandwidth than
competing TFRC ﬂows. However, despite much trying, we could
not ﬁnd any scenarios with varying bandwidths in which TFRC re-
ceives more bandwidth than TCP in the long-term. Over short pe-
riods of time, immediately after a reduction in the available band-
width, TFRC ﬂows may get higher throughput than TCP ﬂows, but
in the long run, the TCP ﬂows are more than competitive. Figures 8
and 9 show similar results when TCP competes with TCP(1/8) or
with SQRT in this dynamic environment. Although not as agile as
TCP, these SlowCC mechanisms are reasonably prompt in reduc-
ing their sending rate in responses to extreme congestion; however,
they are observably slower at increasing their sending rate when
the available bandwidth has increased. Our results suggest that
there need be no concerns about unfair competition with TCP over
long-term durations that would prevent SlowCC from being safely
deployed in the current Internet.
We observed similar trends when competing algorithms were
subjected to an even more extreme 10:1 oscillation in the avail-
able bandwidth—the throughput difference was signiﬁcantly more
prominent in this case.
In a nutshell, SlowCC mechanisms lose
to TCP under dynamic network conditions in the long run because
their response to network conditions is slow; they do not send data
fast enough when the bandwidth is actually available. Thus, two
mechanisms that are TCP-compatible under static conditions do
not necessarily compete equitably, even in the long term, in a more
dynamic environment. In return for a smoother sending rate under
more static conditions, SlowCC mechanisms pay the price of losing
bandwidth, relative to TCP, in more dynamic environments.
4.2.2 Transient Fairness
)
s
d
n
o
c
e
s
n
i
(
e
m
T
i
200
180
160
140
120
100
80
60
40
20
0
0
0.2
0.1-fair convergence times
avg 0.1-fair convergence time
0.4
TCP decrease parameter b
0.6
0.8
1
Figure 10: Time (in seconds) for convergence to 0:1-fairness for
TCP(b) ﬂows.
0.1-fair convergence times for a drop rate of 0.1
s
K
C
A
f
o
r
e
b
m
u
N
2500
2000
1500
1000
500
0
0
0.2
0.4
0.6
0.8
1
AIMD decrease parameter b
Figure 11: Number of ACKs for convergence to 0.1-fairness for
TCP(b) ﬂows.
0.1-fair convergence times for different values of b in TFRC(b)
)
.
c
e
s
(
e
m
T
i
400
350
300
250
200
150
100
50
0
avg 0.1-fair convergence time
0
50
100
150
b
200
250
300
Figure 12: Time (in seconds) for convergence to 0:1-fairness for
TFRC(b) ﬂows.
We now consider the effect of SlowCC algorithms on transient
fairness under dynamic conditions. We discuss the time for conver-
gence to fairness for two ﬂows using identical SlowCC mechanisms
but starting at different sending rates. Transient fairness would be
particularly important for short ﬂows, whose entire lifetime might
be contained in the transient period of convergence to fairness.
Figure 10 shows the results of simulations with two TCP(b) ﬂows
sharing a link of bandwidth B. Let X1; X2 denote the band-
widths of the ﬁrst and second ﬂows respectively. We measure the
Æ-fair convergence time, deﬁned in Section 3, for de a = 0:1. We
use a value of the bottleneck bandwidth, B = 10 b, much big-
ger than b0 which is the bandwidth corresponding to 1 packet/RTT
(our RTT is 50 ms). Thus, the 0:1-fair convergence time being
measured corresponds roughly to the time taken for an initial un-
fair allocation of B; 0 to converge to 0:55B; 0:45B. Figure 10
shows the 0.1-fair convergence times for two TCP(b) ﬂows for a
range of values of b. If we decreased the link bandwidth, we would
expect the convergence times to decrease accordingly.
1 and X i
We use an analytical model with the same framework to estimate
the expected Æ-fair convergence times for pure AIMD(a, b) ﬂows
in an environment with a steady-state packet mark rate , when
B >> b0. (For simplicity of discussion assume that this is an en-
vironment with Explicit Congestion Notiﬁcation (ECN) [15].) Let
X i
2 denote the expected values of the congestion windows
of the ﬁrst and second ﬂows after the arrival of the i-th ACK packet,
and consider the effect of the i  1-th ACK packet. The i  1-th
ACK belongs to ﬂow 1 with probability X i
, and to ﬂow 2 with
probability X i
of the two congestion windows become
. After the i  1h ACK, the expected values
1X i
1X i
X i
X i
1
2
2
2
and
X i
1 
X i
2 
X i
1
1  X i
2 (cid:18) a1   
1(cid:19)
1   bX i
X i
X i
X i
2
1  X i
2 (cid:18) a1   
2(cid:19)
2   bX i
X i
X i
respectively. The expected difference in the congestion windows of
the two ﬂows changes from
(cid:26)i = jX i
1   X i
2j
to
(cid:26)i1 = (cid:12)(cid:12)(cid:12)(cid:12)
X i
1   X i
2   b(cid:18) X i
12
1  X i
X i
2  
22
X i
1  X i
X i
2(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
= (cid:26)i1   b:
Thus the expected number of ACKs needed for a Æ-fair alloca-
tion, starting from a highly skewed initial allocation, is essentially
 g1 bÆ.