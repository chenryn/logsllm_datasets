### Public Availability and Experiment Instructions
The project, initially announced at the IMC 2010 conference, is publicly available for download on our project webpage, along with detailed instructions on how to perform the experiment. To gather a diverse set of data, we made a public call through several DNS-related mailing lists and invited friends to participate. This outreach resulted in a total of 484 traces. Participants ran the program locally on their end-hosts. The program queries the locally configured DNS resolver, a Google Public DNS resolver, and an OpenDNS resolver for a list of over 7,400 hostnames, storing the full DNS replies in a trace file. Importantly, the traces collected by our program do not interact with any of the user's browsing or download history and activity.

### Additional Meta-Information
In addition to the DNS data, we collect meta-information to aid in sanitizing the measurements and debugging. Every 100 DNS queries, we report the Internet-visible IP address of the client by querying a web-server running a custom script. We also store information such as the operating system, timezone, and the IP addresses of the DNS resolvers in use.

### Data Sanitization
To ensure the integrity of the measurements, we verify that the locally configured resolver is not a third-party resolver like Google Public DNS or OpenDNS. This verification is necessary because the recursive resolver may hide behind a DNS forwarding resolver, making it difficult to derive this information from the resolver IP address alone. Therefore, we perform queries for 16 additional names to domains under our administrative control. These authoritative name servers are configured to reply with the IP address of the querying resolver, providing us with the direct IP addresses of the resolvers without the need for further log correlation. To avoid receiving cached copies of the entries, we construct the names on-the-fly using microsecond resolution timestamps and the Internet-visible IP address of the client.

### Data Collection
The program is designed to collect all of the above data once every 24 hours, writing it to a trace file until stopped by the user. This means there may be multiple traces per vantage point. Vantage points are identified through the information contained in the trace files and the meta-information provided by the end-user when uploading the trace files.

### Data Cleanup
We perform a thorough cleanup process on the raw traces to remove any measurement artifacts. Specifically:
- Traces from vantage points that roam across ASes during the experiment are excluded.
- Traces where the DNS resolver returns an excessive number of DNS errors or is unreachable are discarded.
- Traces where the DNS resolver is a well-known third-party resolver (e.g., OpenDNS, Google Public DNS) are not considered, as they introduce bias by not representing the location of the end-user [7].

When a vantage point provides repeated measurements over time, only the first trace that does not suffer from any other artifact is used to avoid over-representing a single vantage point. After this cleanup, we have 133 clean traces that form the basis of our study. This cleanup process has limited impact on our hosting infrastructure coverage and sampling of the network footprint.

### Data Coverage
#### Network and Geographic Footprint of Vantage Points
We map the IP addresses of vantage points from the 133 clean traces to ASes and countries using the methodology described in Section 2.2. This results in coverage of 78 ASes and 27 countries spanning six continents. Our experiments include traces from major residential ISPs (e.g., AT&T Internet Services, Comcast, Verizon, Road Runner, Telefonica, Deutsche Telekom, British Telecom) as well as smaller residential ISPs and some university and research networks.

#### Network Coverage by Hostname
Previous studies [17, 35, 36] achieved exhaustive coverage for a limited number of well-known hosting infrastructures. In our study, we aim for wide coverage of prevalent hosting infrastructures without targeting a-priori known hosting infrastructures. We analyze the degree to which different parts of our hostname list result in different network coverage. To identify the IP ranges utilized by hosting infrastructures, we aggregate the returned IP addresses over /24 subnetworks, which is the right granularity given the deployment of server clusters for resilience and load balancing.

Figure 2 shows the total number of discovered /24 subnetworks as we stepwise add hostnames from our list according to their utility. By utility, we mean the number of new /24 subnetworks discovered by a hostname. The y-axis shows the total number of discovered /24 subnetworks as a function of the number of hostnames considered on the x-axis. We differentiate between three types of hostnames: TOP2000, TAIL2000, and EMBEDDED.

The curves in Figure 2 can be separated into three regions: a steep slope on the left, followed by a region with a slope of 1, and a flat region at the end. The steep slope region identifies hostnames with high utility, which should be included to discover a significant fraction of the content infrastructure with limited probing effort. The region with a slope of 1 results from hostnames that positively contribute to the coverage but with lower utility than those on the left. The flat region corresponds to hostnames that return redundant information about the hosting infrastructure.

Pairwise comparison of the three types of hostnames reveals that while the hostname lists of TOP2000 and TAIL2000 are of equal size, the /24 subnetworks observed by TOP2000 and TAIL2000 differ by more than a factor of two. This indicates that popular content is served from more widely distributed hosting infrastructures than less popular content. Most of the difference in cumulative utility between TOP2000 and TAIL2000 stems from a small number of popular hostnames. Additionally, the hosting infrastructures serving hostnames in EMBEDDED are well distributed.

To estimate the utility of additional hostnames, we calculate the median utility of 100 random hostname permutations. We find that adding the last 200 hostnames results in an average utility of 0.65 /24 subnets per hostname, and 0.61 /24 subnets when adding the last 50 hostnames.

#### Network Coverage by Trace
Hosting infrastructures rely on geographic hints to serve content from servers close to the end user [24, 30]. We expect that traces from diverse regions of the world will sample different parts of the hosting infrastructures. Therefore, we address the utility of traces obtained from different vantage points.

Figure 3 displays the number of discovered /24 subnetworks of hosting infrastructures as we stepwise add traces from our dataset. The x-axis indicates the number of used traces, and the y-axis shows the cumulative number of /24 subnetworks that can be identified based on these traces. The four curves in Figure 3 provide the results for different sequences in which we stepwise add traces: the curve with black circles represents the "Optimized" sequence, where each step adds the trace that contributes the most additional /24 subnetworks. In addition to the optimized cumulative utility, Figure 3 shows the maximum, median, and minimum cumulative utilities for 100 random permutations of the 133 traces.

In total, we find more than 8,000 /24 subnetworks utilized by hosting infrastructures. Each trace samples about half of these subnetworks (4,800), with about 2,800 subnetworks found in all traces. This relatively high fraction of common subnetworks among traces is due to our choice of hostnames, which is not biased towards any given hosting infrastructure.

To estimate the potential benefits of including additional vantage points, we study the slope of the median curve (green crosses). Extrapolating the utility of the last 20 traces yields approximately ten /24 subnetworks per additional trace. The traces that provide the highest utility (leftmost side in Figure 3) are located in different ASes and countries. For example, the first 30 traces belong to 30 different ASes in 24 different countries, and the first 80 traces belong to 67 different ASes and 26 countries. This highlights the importance of utilizing geographically diverse vantage points hosted in different ASes.

To better understand the need for diversity in vantage points and the underlying reasons behind the limited additional network coverage of hosting infrastructures by each trace, we perform a direct comparison of the traces. We use the similarity concept defined by Equation 1. For the same hostname, we define the /24 subnetwork similarity between two DNS replies as the similarity between their respective sets of /24 subnetworks. For two traces, we define their similarity as the average of /24 subnetworks similarities across all hostnames.

Figure 4 shows the cumulative distribution of the similarity across all pairs of traces (TOTAL). We also show the similarity across traces when considering only one of the three subsets of the hostname list (EMBEDDED, TOP2000, TAIL2000). The high baseline value of similarity (always above 0.6) highlights the need for diversity to sample hosting infrastructures and confirms the slow increase in the utility of the traces shown in Figure 3.

As expected, the similarity for TAIL2000 is very high, indicating limited location diversity for the corresponding hosting infrastructure. This is contrasted with the similarity for EMBEDDED, which is the lowest among the four curves. A low similarity for EMBEDDED is due to the nature of the corresponding objects: typically, they have a long lifetime and are large, making them prime candidates for being hosted on distributed infrastructures, e.g., CDNs. TOP2000 lies in-between TAIL2000 and EMBEDDED, indicating that the corresponding hostnames are hosted on a mix of centralized and distributed hosting infrastructures.

### Summary
Our choice of a mix of different hostnames enables us to estimate the effect of our hostname list on sampling hosting infrastructures. Popular hostnames and embedded objects contribute most to discovering networks used by hosting infrastructures. Overall, our data coverage analysis suggests that our set of popular and embedded hostnames is unlikely to miss large hosting infrastructures. The diversity of vantage points in terms of geographic and network location is crucial for obtaining good coverage.

### Results
In this section, we examine our data set. First, based on the IP addresses, we investigate where content can be obtained from. Next, we apply our clustering algorithm to characterize the resulting hosting infrastructure clusters, gaining insight into the deployment and hosting strategies of different infrastructures. We utilize this insight to derive content-centric AS-rankings and compare them with existing ones.

#### A Continent-level View of Web Content
Before delving into the characteristics of hosting infrastructures, we want to understand which parts of the world serve Web content. We choose the granularity of a continent for two reasons: (i) the results directly reflect the round trip time penalty of exchanging content between continents, and (ii) our sampling is not dense enough to support country-level statistics. We quantify the degree to which a user can find content in her own continent, providing a view on the relative importance of different continents for Web content delivery and the degree of content replication.

##### Geographic Replication of Content
We first examine the relationship between the locations of content requesters and content locations as identified by DNS answers. We focus on TOP2000 in this section and compare with other content types in the following section. Each line of Table 1 summarizes requests that originate from a given continent, with columns breaking down the requests among the continents from which the requested hostname is served. Each line adds up to 100%, while columns do not, as they reflect the global importance of a continent. The shade of each entry in Table 1 is a visual aid, directly indicating its value (the darker the higher the value).

At least 46% of the popular hostnames can be served from North America, 20% from Europe, and 18% from Asia. The other three continents (Africa, Oceania, and South America) serve a smaller percentage of the content.