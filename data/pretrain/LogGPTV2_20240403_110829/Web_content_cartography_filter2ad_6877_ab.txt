that is publicly available for download on our project web-page
along with instructions on how to perform the experiment. We
initially announced the project during the IMC 2010 conference.
In addition, we made a public call in several DNS-related mailing
lists and invited friends to participate. This resulted in a total of
484 traces. Participants in our study run the program locally on
their end-hosts. The program queries the locally conﬁgured DNS
resolver, a Google Public DNS resolver and an OpenDNS resolver
for the list of over 7400 hostnames, and stores the full DNS replies
in a trace ﬁle. The traces collected with our program do not inter-
act with any of the browsing or download history and activity of
the user.
In addition to the DNS data, we collect meta-information that
helps in sanitizing the measurement and in debugging. We report
the Internet-visible IP address of the client every 100 DNS queries
by asking a web-server running a custom script, and store informa-
tion such as the operating system and the timezone, as well as the
IP addresses of the DNS resolvers in use.
To sanitize the measurements, we check that the locally conﬁg-
ured resolver is not a 3rd-party resolver such as Google Public DNS
or OpenDNS. This information cannot always be derived from the
resolver IP address, as the recursive resolver may hide behind a
DNS forwarding resolver. Therefore, we perform queries for 16 ad-
ditional names to domains under our administrative control. Their
authoritative name servers are conﬁgured to reply to queries with
the IP address of the querying resolver. This gives us the IP ad-
dresses of the resolvers directly, without having to further correlate
logs from our authoritative name servers with the traces. To avoid
receiving cached copies of the entries, we construct the names on-
the-ﬂy with the help of microsecond resolution timestamps and the
Internet-visible IP address of the client.
The program is designed to collect all of the above data once
every 24 hours, and write it to a trace ﬁle, until stopped by the user.
This implies that there may be multiple traces per vantage point.
We identify vantage points through the information contained in
the trace ﬁles as well as meta information provided by the end-user
running the program when uploading the trace ﬁles.
3.3 Data Cleanup
We perform a thorough cleanup process on the raw traces. We
check for the following measurement artifacts.
We do not consider traces if the vantage point roams across ASes
during the experiment, as we cannot determine the exact impact of
the change. If the DNS resolver of the host returns an excessive
number of DNS errors, or is unreachable, we do not consider the
trace. If the DNS resolver of the host is a well-known third-party
588d
e
r
e
v
o
c
s
d
i
t
s
k
r
o
w
e
n
b
u
s
4
2
/
f
o
r
e
b
m
u
N
0
0
0
8
0
0
0
6
0
0
0
4
0
0
0
2
0
d
e
r
e
v
o
c
s
d
i
t
s
k
r
o
w
e
n
b
u
s
4
2
/
f
o
r
e
b
m
u
N
0
0
0
8
0
0
0
6
0
0
0
4
0
0
0
2
0
Total
Embedded
Top 2000
Tail 2000
Optimized
Max random
Median random
Min random
0
2000
4000
6000
0
20
40
60
80
100
120
Hostname ordered by utility
Trace
Figure 2: /24 subnetwork coverage by the hostname list.
Figure 3: /24 subnetwork coverage by traces.
resolver, e. g., OpenDNS, Google Public DNS, we do not consider
the trace. We showed in previous work that using third-party re-
solvers introduces bias by not representing the location of the end-
user [7].
When a vantage point provides us with repeated measurements
over time, we only use the ﬁrst trace that does not suffer from any
other artifact to avoid over-representing a single vantage point. This
is important to avoid bias when quantifying the content potential
(cf. Section 2.4).
After removing all traces with the above artefacts, we have 133
clean traces that form the basis of this study Note, the cleanup pro-
cess has limited impact on our hosting infrastructure coverage and
sampling of the network footprint.
3.4 Data coverage
We next investigate the coverage that our hostnames and vantage
points provide.
3.4.1 Network and Geographic Footprint of Vantage
Points
We map the IP addresses of vantage points of the 133 clean traces
to ASes and countries using the mapping methodology described in
Section 2.2. This leads to a coverage of 78 ASes and 27 countries
that span six continents. Our experiments include traces from major
residential ISPs, e. g., AT&T Internet Services, Comcast, Verizon,
Road Runner, Telefonica, Deutsche Telekom, British Telecom as
well as smaller residential ISPs and some university and research
networks.
3.4.2 Network Coverage by Hostname
Previous studies [17, 35, 36] were able to achieve an exhaustive
coverage for a limited number of well known hosting infrastruc-
tures.
In our study, we strive to achieve a wide coverage of the
prevalent hosting infrastructures without targeting a-priori known
hosting infrastructures. Thus, we investigate the scope of the net-
work coverage of our study. For this, we analyze to which degree
replies for different parts of our hostname list result in different
network coverage. To identify the IP ranges utilized by hosting in-
frastructures we aggregate the returned IP addresses over /24 sub-
networks. We argue that this is the right granularity as hosting in-
frastructures tend to deploy server clusters for resilience and load
balancing. Aggregation on the preﬁx of the returned IP addresses
may lead to overestimation of the coverage, yet another indication
why aggregation on /24 subnetworks is justiﬁed.
Figure 2 shows the total number of discovered /24 subnetworks
when we stepwise add hostnames from our list (see Section 3.1)
according to their utility. By utility we mean the number of new /24
subnetworks that are discovered by a hostname. The y-axis shows
the total number of discovered /24 subnetworks as a function of the
number of traces considered on the x-axis. In addition to the full
list, we differentiate between three types of hostnames introduced
in Section 3.1: TOP2000, TAIL2000, and EMBEDDED.
The curves in Figure 2 can be separated into three regions: a
steep slope on the left, followed by a region with a slope of 1, and
a ﬂat region at the end. The steep slope region identiﬁes hostnames
with a high utility. These hostnames should be included to discover
a signiﬁcant fraction of the content infrastructure with limited prob-
ing effort. The region having a slope of 1 results from hostnames
that positively contribute to the coverage but the utility is much
lower than hostnames on the left. The third and ﬂat region corre-
sponds to hostnames that return redundant information about the
hosting infrastructure, compared to the ﬁrst two regions.
Let us now turn to the pairwise comparison of the three types of
hostnames. While the hostname lists of TOP2000 and TAIL2000
are of equal size, the /24 subnetworks observed by TOP2000 and
TAIL2000 exhibit a difference by a factor of more than two in the
number of subnetworks they uncover. This unveils that popular
content is served from more widely distributed hosting infrastruc-
tures than this of less popular content. Most of the difference in the
cumulative utility between TOP2000 and TAIL2000 stems from a
small number of popular hostnames. Furthermore, we observe that
the hosting infrastructures that serve hostnames in EMBEDDED are
well distributed.
To estimate the utility of additional hostnames we calculate the
median utility of 100 random hostname permutations. We ﬁnd that
when adding the last 200 hostnames, the average utility is 0.65 /24
subnets per hostname, and 0.61 /24 subnets when adding the last
50 hostnames.
3.4.3 Network Coverage by Trace
Hosting infrastructures rely on geographic hints to serve content
from servers close to the end user [24, 30]. We expect that traces
in diverse regions of the world sample different parts of the host-
589Embedded
Top 2000
Total
Tail 2000
F
D
C
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
0.0
0.2
0.4
0.6
0.8
1.0
Similarity
Figure 4: CDF of similarities for answers across different
traces, for different sets of hostnames.
ing infrastructures. Therefore, we address now the utility of traces
obtained from different vantage points.
Figure 3 displays the number of discovered /24 subnetworks
of hosting infrastructures when we stepwise add traces from our
dataset, see Section 3.2. The x-axis indicates the number of used
traces while the y-axis shows the cumulative number of /24 subnet-
works that can be identiﬁed based on these traces. The four curves
of Figure 3 provide the results for different sequences in which we
stepwise add traces: to obtain the curve with the black circles, we
add in each step the trace that adds most additional /24 subnetworks
to the set of already identiﬁed subnetworks (“Optimized”). In ad-
dition to the optimized cumulative utility (black circles) Figure 3
shows the maximum, median, and minimum cumulative utilities
for 100 random permutations of the 133 traces.
In total, we ﬁnd more than 8000 /24 subnetworks that are uti-
lized by hosting infrastructures. We observe that every trace sam-
ples about half of these subnetworks (4800). About 2800 of these
subnetworks are found in all traces. This relatively high fraction
of common subnetworks among traces is the consequence of our
choice of hostnames. It is not biased towards any given hosting
infrastructure.
To estimate the potential beneﬁts by including additional vantage
points, we study the slope of the median curve (green crosses). Ex-
trapolating the utility of the last 20 traces yields approximately ten
/24 subnetworks per additional trace.
Furthermore, we notice that the traces that provide the highest
utility (traces corresponding to the leftmost side in Figure 3), are
actually located in different ASes and countries. For example, the
ﬁrst 30 traces belong to 30 different ASes in 24 different countries.
The ﬁrst 80 traces belong to 67 different ASes and 26 countries.
This highlights the importance of utilizing vantage points that are
geographically diverse and are hosted in different ASes.
To better understand both the need for diversity in vantage points
as well as the underlying reasons behind the limited additional net-
work coverage of hosting infrastructures by each trace, we perform
a direct comparison of the traces. For this we re-use the similarity
concept deﬁned by Equation 1. For the same hostname, we deﬁne
the /24 subnetwork similarity between two DNS replies as the sim-
ilarity between their respective sets of /24 subnetworks. For two
traces, we deﬁne their similarity as the average of /24 subnetworks
similarities across all hostnames.
In Figure 4 we show the cumulative distribution of the similar-
ity across all pairs of traces (TOTAL). We also show the similarity
across traces when considering only one of the three subsets of the
hostname list (EMBEDDED, TOP2000, TAIL2000). The high base-
line value of similarity (always above 0.6) highlights the need for
diversity to sample hosting infrastructures. It also conﬁrms the slow
increase in the utility of the traces shown in Figure 3.
As expected, the similarity for TAIL2000 is very high, indicat-
ing the limited location diversity for the corresponding hosting in-
frastructure. This is contrasted with the similarity for EMBEDDED,
that is the lowest among the four curves. A low similarity for EM-
BEDDED is the consequence of the nature of the corresponding ob-
jects: typically they have a long lifetime and often are large. This
makes them prime candidates for being hosted on distributed in-
frastructures, e. g., CDNs. TOP2000 lies in-between TAIL2000 and
EMBEDDED. This indicates that the corresponding hostnames are
hosted on a mix of centralized and distributed hosting infrastruc-
tures.
3.4.4
Summary
Our choice of a mix of different hostnames enables us to esti-
mate the effect of our hostname list on sampling hosting infrastruc-
tures. Popular hostnames and embedded objects contribute most
to discovering networks used by hosting infrastructures. Overall,
studying data coverage we ﬁnd that our set of popular and embed-
ded hostnames is unlikely to miss large hosting infrastructures. The
diversity of vantage points in terms of geographic and network lo-
cation, however, is crucial to obtain good coverage.
4. RESULTS
In this section we examine our data set. First, based on the IP ad-
dresses we investigate where content can be obtained from. Next,
we apply our clustering algorithm to characterize the resulting host-
ing infrastructure clusters. We gain insight on the deployment and
hosting strategies of different infrastructures. We utilize our insight
to derive content-centric AS-rankings and compare them with ex-
isting ones.
4.1 A Continent-level View of Web Content
Before delving into characteristics of hosting infrastructures, we
want to understand which parts of the world serve Web content.
In this section we choose the granularity of a continent, for two
reasons: (i) the results directly reﬂect the round trip time penalty
of exchanging content between continents, and (ii) our sampling is
not dense enough to support country-level statistics. We quantify
to which degree a user can ﬁnd content in her own continent. This
provides a view on the relative importance of different continents
for Web content delivery as well the degree of replication of con-
tent.
4.1.1 Geographic Replication of Content
We ﬁrst examine the relationship between the locations of con-
tent requester and content location as identiﬁed by DNS answers.
We focus on TOP2000 in this section, and compare with other con-
tent types in the following section. Each line of Table 1 summarizes
requests that originate from a given continent. Columns of Table 1
break down the requests among the continents from which the re-
quested hostname is served. Each line adds up to 100 %, while
columns do not as they reﬂect the global importance of a continent.
The shade of each entry of Table 1 is a visual aid, directly indicating
its value (the darker the higher is the value).
At least 46 % of the popular hostnames can be served from North
America, 20 % from Europe and 18 % from Asia. The other three
590Served from
Rank #hostnames #ASes #preﬁxes
Requested
from
Africa
Asia
Europe
N. America
Oceania
S. America
Africa Asia Europe N. America Oceania S. America
0.3
0.3
0.3
0.3
0.3
0.2
18.6
26.0
18.6
18.6
20.8
18.7
32.0
20.7