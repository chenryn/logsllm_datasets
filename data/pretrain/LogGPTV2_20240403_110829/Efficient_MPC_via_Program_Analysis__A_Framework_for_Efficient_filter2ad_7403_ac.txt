27
28
29 }
(a) IMP Source
(b) IMP-SSA
(c) MPC-source
Figure 1. (a) shows the IMP source for the GCD algorithm, (b) shows GCD translated into IMP-SSA after inlining
rem. (c) shows the IMP-SSA program translated into MPC-source. Our integer program works on MPC-source.
3 Program Analysis of MPC Source
In this section, we describe our program analysis process, that will yield the basis for our optimization
problem deﬁned in the next section. §3.1 presents a running example. §3.2 outlines the syntax of the source
language, as well as the translation process into our representation, MPC-source. §3.3, and §3.4 describe the
control-ﬂow structure of MPC-source and reaching deﬁnition analysis on top of it. §3.5 and §3.6 deﬁne other
analyses on MPC-source necessary to build the optimization problem.
3.1 Running Example
Our running example in Fig. 1(a) is an implementation of the Greatest Common Divisor (GCD) algorithm
using integer division. The gcd program makes calls to function rem, due to [DSZ15], which computes the
remainder of an integer division. Note that the structure is signiﬁcantly diﬀerent and more involved than
the standard—non-MPC targeted—integer divisiongiven in Fig. 7(a) in appendix C. Such diﬀerence between
non-MPC and MPC programs is typical due to inherent restrictions in the latter (to preserve privacy). For
example, in Fig. 1(a) the value of both val and mod will need to be secret shared, so they remain unknown
until the corresponding output-gates of the induced MPC circuit are computed (and reconstructed). Thus,
in order to generate a circuit that can be processed by MPC, the while-loop cannot use the values of these
variables. The rewrite by Demler et al. [DSZ15] rectiﬁes this by carrying long division in binary, with a loop
bounded by statically known LEN, which is either 32 or 64 bits. Fig. 1(a) presents our rewrite of the standard
GCD loop (cf. Fig. 7(b) in appendix C), where we are using the observation that the number of iterations
in GCD is bounded by 2LEN = 2 log(max(a, b)).
6
s ::= s1; s2
s ::= if (x bop y) { s1 } else { s2 } z = φ(z1, z2) ⇒ s.MPC = s1.MPC + s2.MPC + “cnd = CMP(x bop y); z = MUX(z1, z2, cnd)”
⇒ s.MPC = s1.MPC + s2.MPC
Figure 2. Translation of IMP-SSA into MPC-source. Attribute MPC contains the MPC-source code. Translation of
a sequence entails appending s2’s MPC-source code onto s1’s. The MPC-source for an if-statement is constructed
by adding the code for branch s2 onto the code for branch s1 thus linearizing the if-statement; at the end, the
translation adds the conditional operation and the multiplexer, which selects values. We do not include for other
kinds of statements as it is trivial.
3.2 Translation into MPC-source
We assume an IMP-like syntax [NK14] for our source language. The IMP syntax models an imperative
language, such as FORTRAN, C, or Java, and our results apply to any of these languages. We impose the
following standard restrictions necessary to accommodate MPC: there is no recursion, and all loop bounds
are statically known. The IMP source is translated into Static Single Assignment (SSA) using standard
techniques [Cyt+91]. This is standard SSA, however, to make it explicit that it corresponds to IMP-source,
in the following we will refer to it as IMP-SSA. This is the syntax of our intermediate representation. Due
to space constraints, we defer detailed discussion of the syntax to the Appendix C.
The next step is to translate IMP-SSA into MPC-source, the representation that we use for deﬁning our
compact integer program. Fig. ?? deﬁnes an attribute grammar (also known as syntax-directed translation)
over IMP-SSA. The most interesting case arises at if-statements which are dealt with using standard MPC
techniques: the MPC-source code for an if-statement is produced by appending the straight-line (MPC) code
for the else-arm onto the straight-line (MPC) code for the then-arm, then adding the conditional, and the
multiplexer to select the correct values. Due to single assignment, variables used at the if-statement test are
unmodiﬁed, and are referenced in the comparison expression (CMP) that precedes MUX, where the φ nodes
capture exactly the arguments of MUX. 6 For example, consider the if-statement in lines 9-28 in Fig. 1(b).
The φ nodes capture the values of x and y; if control took the then-arm, then x and y would be x2 and y2
respectively, otherwise x and y would be x1 and y1.
In our example, the resulting MPC-source program is shown in Fig. 1(c). We point out that MPC-source
can be mapped one-to-one to standard straight-line MPC; the only diﬀerence is that when a block is repeated
multiple times in straight-line MPC, it is replaced by a for-loop in MPC-source. Following standard MPC
compilers methodology, e.g., [BNP08; Fra+14], the actual MPC program unrolls all loops, and loop induction
variables become constants.
To make the above mapping explicit, we use pseudo φ-nodes. To better understand the use of these
nodes, let’s focus on lines 5, 6 and 12 in Fig. 1(c) at the beginning of each one of the loops; these lines do
not encapsulate an if-then-else construct. Instead, they select variable values—at the ﬁrst iteration, the value
comes from outside the loop, and at every subsequent iteration the value comes from the previous iteration
of the loop. When translated into straight-line code, these lines disappear because corresponding values are
directly used as inputs to the gates. To highlight that these lines are only here to enable loops, and, that
these do not get translated into a MUX, we refer to them as pseudo φ-nodes in text and denote them with
? : instead of φ.
Looking ahead (cf. §4) the beneﬁt of doing the analysis over MPC-source rather than straight-line code
will be that there are signiﬁcantly fewer variables in the resulting integer/linear program.
3.3 Control-ﬂow Structure of MPC-source
The main reason why most, if not all, MPC compilers use straight-line code as their (intermediate) source
representation is that it exhibits a very simple control ﬂow structure. Despite having loops for more compact
6 MUX is the multiplexer gate that is common in MPC compilers: on input of values (v0, v1) and a selection bit
b ∈ {0, 1}, it returns vb. In our case b is result of the CMP and (v0, v1) are arguments of φ node.
7
representation, MPC-source also exhibits simple control-ﬂow structure, which, as we show, facilitates program
analysis. Speciﬁcally, the program consists of straight-line blocks nested within each other. Fig. 3(a) illustrates
the block structure of MPC-source.
Each block B, except for the outermost one, is a for-loop block:
n0 → n2 → . . . nk −→ n0
Here n0, n1, . . . denote statements in B, short arrows (i.e., →) denote forward control-ﬂow edges in B, and
long arrows (i.e., −→) denote the back edge from the last node nk ∈ B to the entry node n0 ∈ B. The node
n0 is special in MPC-source, because it is a control merge node. There are two incoming edges into n0: a
forward edge n(cid:48) → n0 where n(cid:48) is the node in B’s enclosing block B(cid:48) that immediately precedes B, and the
back edge nk −→ n0.
For example, consider the statement “rem1 = (j == LEN-1) ? rem0 : rem5;” in Fig. 1(c). In the ﬁrst
iteration of the loop, it chooses the value of rem1—this is the value of rem0 in our case, and at every
subsequent iteration it chooses the values resulting from the previous iteration—which is the value of rem5
in our case. Node nk is special as well because it is a control split node —there are two outgoing control-ﬂow
edges from nk, a forward edge nk → n(cid:48)(cid:48), where n(cid:48)(cid:48) is the node in B(cid:48) that immediately succeeds B, and the
back edge nk −→ n0. The graph below shows the nested structure (it omits the back edge for clarity):
(cid:123)
(cid:125)(cid:124)
(cid:123)(cid:122)
B(cid:48)
(cid:122)
(cid:124)
. . . n(cid:48) →
B
n0 → n2 → . . . nk → n(cid:48)(cid:48) . . .
(cid:125)
3.4 Reaching Deﬁnitions over MPC-source
We are interested in Reaching Deﬁnitions over MPC-source, because the simple control-ﬂow structure of
MPC-source discussed above, as opposed to general IMP-style code, makes Reaching Deﬁnitions a very
powerful tool. In particular, unlike general IMP programs, in MPC-source programs a def-use chain (d, u)
entails that d always reaches u due to the simpler control-ﬂow structure of MPC-source programs. Examples
of def-use chains in the MPC-source program in Fig. 1(c) are (5,14) (the deﬁnition of x1 at line 5 reaches
the use at line 14), and similarly (13,14). As another example, the MUX statement at line 25 is a deﬁnition
of x3 and the statement at line 5 is a use of x3. We will be using def-use chains to calculate the total cost of
running an MPC-source program and reason about conversions (see also discussion about optimal conversion
placement below).
3.5 Statement Weights
Since MPC-source has loops, in order to accurately capture execution cost, we must assign weights to
statement in the MPC-source control-ﬂow graph. (As discussed in the following section, certain edges that
are necessary for the deﬁnition of our IP are also assigned weights.) The weights correspond to the number
of times a statement/edge executes. Once again, the simple structure of our MPC-source representation
gives the solution: unlike general IMP-style source-code, in MPC-source it is straight-forward to assign
those weights because there are no if-then-else statements, and therefore no need to estimate the number of
times control may go through one branch relative to the other (the standard approach is to assume equal
probability of execution of each branch). The weight wn of statement n is the product of the bounds of all
loops “around” n: b1 · b2 · ... · bk where b1 stands for the bound of the outermost loop, and bk for the bound
of the innermost loop enclosing n. For example, w13 in Fig. 1(c) is 2LEN · LEN = 2LEN2.
3.6 Optimal Conversion Placement
Diﬀerent protocols use diﬀerent sharings. To stitch such protocols together, we need share conversion. In
linearized MPC (where all loops are unrolled) placing such conversions is straight-forward: always convert
8
to what the next protocol needs (if the protocol is the same do not convert). However a challenge in using
MPC-source, where loops are present, is when a node is part of a loop whose output needs to be converted.
For example, consider a deﬁnition that is computed before a loop and is used inside the loop. It is most
beneﬁcial to place the conversion before the loop. In this section we describe how to identify the optimal such
conversion point to minimize the total cost. This allows us to use the beneﬁts of working with the condensed
MPC-source without sacriﬁcing cost eﬃciency due to suboptimal conversion placement.
Consider a def-use chain (d, u). If d computes a value in one share (e.g., Arithmetic) but u uses a diﬀerent
share (e.g., πY), then the value computed at d must be converted to the share required at u. We must place
conversions in such a way that: (1) each execution path from d to u executes the required conversion, and (2)
the total cost of executing the required conversion(s) is minimal; we note that the cost of a single conversion
operation is ﬁxed, however, the total cost depends on where, i.e., on what CFG edge, we place the conversion
operation. We deﬁne min cut(d, u)7 where it is least costly to place a conversion of the value computed at
d on the way to u. Next, we describe how to ﬁnd min cut(d, u).
We begin with the deﬁnition of necessary terms. Let the closest enclosing block of ni and nj be the
innermost block B such that ni ∈ Bi and nj ∈ Bj and both Bi and Bj are nested, immediately or transitively,
in B. Trivially, a block is nested in itself. An edge e = n1 → n2 is said to be in block B, denoted as e ∈ B,
if: either 1) n1 ∈ B, or 2) n2 ∈ B, or n1 ∈ B1, n2 ∈ B2 and B1 and B2 are immediately enclosed in B.
To compute the min cut(d, u), there are two cases. Case 1 is when d precedes u, i.e., there is a sequence
of forward edges from d to u. We call these forward def-use chains. In this case, min cut(d, u) is the ﬁrst
edge e in the sequence of forward edges from d to u such that e is in the closest enclosing block B of d and
u. Clearly, the cost of such edge e, we, is the number of times B executes. For example, consider def-use
chain (14,17) in the MPC-source program in Fig. 1(c). The closest enclosing block of lines 14 and 17 is
the inner for-loop; the min-cut edge is edge 14 → 15, the ﬁrst in the forward sequence from 14 to 15. As
another example, consider def-use chain (17,21). The closest enclosing block of both lines 17 and 21 is the
outer for-loop. The min-cut edge is the edge from 17 to 21, which executes LEN number of times, and as we
mentioned earlier, this entails that it is least costly to place a conversion at 21 rather than at 17.
Case 2 arises when u precedes d, i.e., there is a sequence of forward edges from u to d and the path
from d to u goes through a back edge (see also Remark 1.) We call these chains backward def-use chains. In
this case, it follows directly from the Reaching Deﬁnitions analysis and the structure of MPC-source that
min cut(d, u) is precisely the back edge of the closest enclosing block B of d and u. The cost of such edge e,
we is N − 1 where N is the number of times B executes. In our running example, min cut(25, 5) is precisely
the back edge of the outer for-loop. This edge executes 2LEN− 1 times, which is exactly the minimal number
of conversions one would need if the MUX at Line 25 of Figure 1.(c) computed x using πY but it used πA for
processing Line 5.
One intuition to the min cut(d , u) is as follows: its weight captures the number of distinct statements st
in the linearized MPC that map to d, such that st is used by a use that maps to u.
Remark 1. Note that in any execution uses always succeed def (It doesn’t make sense to use something that
isn’t deﬁned yet). Our notion of u preceding d and backward def-use chains to refer to backward edge in
MPC-source CFG is a feature of the MPC-source representation. This backward edge always occurs because
of a pseudo-φ node and disappears in translation to linearized code.
Remark 2. We conclude this section with an observation on backward chains, which will play a role in deﬁning
and solving the optimal protocol assignment problem. Backward chains exhibit the following property: each
(d, u) is such that d’s block is nested in u’s block, and u is precisely the pseudo φ-node at the beginning
of the block. (Let x be the variable deﬁned at d. Suppose u was a use of x other than the pseudo φ-node.
Since the use of x at u precedes the deﬁnition at d, at the ﬁrst iteration of u’s loop, x would come from outer
scope. Therefore, SSA would have to merge the two deﬁnitions of x into the pseudo φ-node, thus creating
an earlier deﬁnition of x. A subsequent use would refer to the deﬁnition at the φ-node.)
7 Note that here min cut is slightly diﬀerent from classical max-ﬂow/min-cut. We want to ﬁnd min cut on the graph
of a single def-use chain.
9
4 The Optimal Protocol Assignment Problem
In this section we provide formal deﬁnitions of the optimal protocol assignment problem (OPA) and in §5
we present our eﬃcient solver. Before deﬁning the problem, we ﬁrst establish some useful notation and
terminology that we will use throughout the section.
Notation and terminology:
(IMP-)source code: This is the starting point of our compiler. It is standard programming language code for
an imperative language such as IMP. We denote it by S. All loops have a known upper bound on their
iterations.
MPC-source code: The output of our compiler on some source code S. We denote the compiler by CMPC(·).
The compiler removes if-statements and φ-nodes, and adds MUX-statements in their place. MPC-source
contains for-loops with known bounds.
Block B of MPC-source: Sequence of assignment statements or blocks (in case of for-loop nesting) enclosed
in a for-loop.
intermediate representation between (IMP-)source and MPC-source.
(IMP-)SSA-code: this is the output of SSA on some source-code S. We will denote it as CSSA(S). This is an
Linearized-code : Linear(S): This is the linearization of some MPC-source CMPC(·). It contains no loops,
only straight-line code of assignment statments. The corresponding CFG of this would be simply a
straight line. We refer to statement in Linear(S) as simple statements and denote them as st. Since the
corresponding CFG is a line we often refer to simple statements as nodes in (the CFG of ) Linear(S).
Informally, OPA seeks, given source code for the task the parties wish to securely perform, the best possible
combination of MPC modules, i.e., the combination that minimizes a well deﬁned cost function. We stress
that existing works attack OPA in a heuristic fashion; to our knowledge, ours is the ﬁrst work that devises a
systematic model and uses it to provide provably optimal solutions—under mild and natural assumptions—to
OPA via an automated eﬃcient solver.
There are several parameters that aﬀect the quality of a protocol assignment, and therefore the perfor-
mance of the resulting hybrid MPC protocol. One of the most important is the cost model, which, informally,
speciﬁes the cost of each MPC protocol for computing each statement of the IMP-MPC program. A second
important parameter is scheduling. In particular, some protocols are more friendly to amortization/paral-
lelization than other protocols which means that even though protocol X might be preferable to protocol Y
for a single statement st—e.g., a multiplication gate—when multiple copies of st are computed in parallel—
Y might be overall preferable to X. For example, on a high-bandwidth/low-latency network (e.g., a LAN),
Yao’s protocol is faster when computing an (individual) equality-check gates, but when multiple equality
gates are computed in parallel, the optimized GMW protocol πB overtakes πY (this was demonstrated in
[DSZ15] and is conﬁrmed in our experiments in §7.3.) We defer the treatment of scheduling to §6.
4.1 The cost model
Coming up with a good measure of the cost is an interesting problem in itself. There is no universally
applicable optimal metric and such choice is usually inﬂuenced by a program’s execution environment. For
example, in a data center with high speed connectivity between the servers, minimizing run time would take
priority and, therefore, run time is a good cost metric. However, in a data constrained setting e.g. mobile
phones, minimizing the size of network traﬃc may be more desirable. In this case, communication size would
be a good cost metric.
In this section we devise a generic user-parameterizable cost model for programs that will be used in the
deﬁnition of OPA. Informally, the cost model consists of assigning weights, i.e., costs, to diﬀerent protocols
and to conversions of sharings. This is similar to the cost model devised in [KSS14; Cha+17; B¨us+18];
however, as we discuss in Section 5, our utilization and application of the cost model is qualitatively diﬀerent
than that of [KSS14] and this will allow us to compute optimal assignments in polynomial time