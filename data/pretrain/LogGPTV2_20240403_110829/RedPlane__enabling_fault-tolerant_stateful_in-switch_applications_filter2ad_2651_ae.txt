(5) Heavy-hitter (HH) detection: We implement a heavy-hitter detec-
tor using count-min sketches [28] as an example of write-centric
applications; there are 3 sketches, each consisting of 64Ã—32-bit slots
indexed by a hash of the IP 5-tuple. We implement separate sketches
per VLAN ID, assuming that the network operator wants to enforce
different policies for each cloud tenant. Since sketches are an ap-
proximate data structure which can be replicated asynchronously,
we use periodic snapshot replication.
(6) Per-flow counter: To demonstrate RedPlaneâ€™s worst-case perfor-
mance, this application counts packets forwarded for each IP 5-tuple.
State is updated for every packet and synchronous replication must
be used.
7 Evaluation
We evaluate RedPlane on a testbed consisting of six commodity
switches (including two programmable ones) and servers (see Ap-
pendix D) using both real data center network packet traces and
synthetic packet traces. Our key findings are:
â€¢ In failure-free operation, RedPlane adds no per-packet latency
overhead for applications that are read-centric or replicate state
asynchronously. For write-centric applications in linearizable
mode, RedPlane incurs 8 ğœ‡ğ‘  per-packet overhead (Â§7.1).
â€¢ In failure-free operation, the throughput of read-centric applica-
tions is not degraded. For write-centric applications, the through-
put is bottlenecked by state store performance in linearizable
231
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan
Figure 8: End-to-end RTT when RedPlane-NAT processes
packets vs. other approaches.
mode, but periodic snapshot replication reduces the overhead.
Similarly, RedPlane incurs almost no bandwidth overhead for
read-centric applications and small overhead for write-centric
in bounded-inconsistency mode even at scale (Â§7.2).
â€¢ After a switch failure, RedPlane-enabled applications access
their correct state and recover end-to-end TCP throughput
within a second (Â§7.3).
â€¢ RedPlane provides these benefits with little resource overhead
as it consumes <14% of ASIC resources (Â§7.4).
Testbed setup. We build a three-layer network testbed (shown in
Appendix D). The aggregation layer has two 64-port Arista 7170
Tofino-based programmable switches [23] running stateful applica-
tions written in P4. The core and ToR switches run 5-tuple-based
ECMP routing to route packets to end hosts even when one aggre-
gation switch fails. Each ToR switch has two servers connected,
and four additional servers attached to the core switch emulate
hosts outside the datacenter. The state store runs on one server in
each rack. All servers are equipped with an Intel Xeon Silver 4114
CPU (40 logical cores), 48 GB DRAM, and a 100 Gbps Mellanox
ConnectX-5 NIC, running Ubuntu 18.04 (kernel version 4.15.0). We
repeat each experiment 100 times unless otherwise noted.
7.1 Latency in Normal Operation
First, we evaluate the per-packet latency overhead introduced by
RedPlane under failure-free operation for the 5 applications in Â§6.
To measure the processing latency, we have each application send
packets back to a sender node and track the RTT of each packet. We
replay publicly available packet traces from a real data center and
enterprise network [1, 2] to generate 100,000 packets and measure
the processing latency of each packet. The packet sizes vary (64â€“
1500 bytes) in the real traces. To evaluate EPC-SGW, we inject a
signaling packet for every 17 data packets, following statistics used
in previous studies [56, 62].
Overhead of RedPlane. As an exemplar application, we evaluate
the per-packet latency for a NAT in RedPlane7 and compare it
with baseline implementations: (1) NAT written in P4 without fault-
tolerance (Switch-NAT), (2) NAT written in P4 with controller based
fault-tolerance (Switch-NAT w/ an external controller)8 (3) NAT
implemented on a CPU server without fault-tolerance (Server-NAT),
7We choose NAT to compare results with those reported in prior work [70].
8We implement a simple external controller to emulate SDN controller-based ap-
proaches (e.g., Morpheus [68] and Ravana [40]), which communicates with the switch
control plane via a 1 Gbps management channel.
Figure 9: End-to-end RTT for RedPlane-enabled applications.
All applications have chain replication enabled for the state
store. For Sync-Counter, we also show its overhead without
chain replication.
(4) NAT implemented on a server with fault-tolerance (FT Server-
NAT), and (5) FTMB-NAT which uses rollback-recovery for server-
based middleboxes [70].9 For switch-NAT w/ controller, RedPlane-
NAT, and server-NAT, we enable chain replication for the controller,
state store, and NAT instances, respectively.
Fig. 8 shows the CDF of the per-packet latency distribution. Com-
pared to Switch-NAT, which is expected to have the lowest latency,
RedPlane-NAT shows the same 50th and 90th percentile latency
(7 ğœ‡ğ‘  and 8 ğœ‡ğ‘ , respectively), meaning that there is no overhead.
This is because for NATs, packets except for the first packet of each
flow only require state (i.e., address translation table) to be read.
Both Switch-NAT and RedPlane-NAT show a high 99th percentile
latency (110 ğœ‡ğ‘  and 142 ğœ‡ğ‘ , respectively), mainly due to the overhead
introduced by our control plane implementation; in Switch-NAT,
the first packet of every flow is forwarded to the switch control
plane to create and insert a new entry to the translation table.
RedPlane-NAT has additional overhead since it needs to request a
lease from the state store before updating state. Switch-NAT with
the external controller incurs higher 99th percentile latency (185 ğœ‡ğ‘ )
due to the communication overhead between the switch control
plane and the controller and between controller instances (for chain
replication) over the slower management network. Server-based
versions (FT Server-NAT and FTMB-NAT) have 7â€“14Ã— higher me-
dian latency compared to the switch-based approaches, as packets
need to traverse additional hops in the network and they have
inherent performance limitations.
Impact on different applications. Next, we evaluate the per-
packet processing latency overhead of different RedPlane-enabled
applications. As shown in Fig. 9, RedPlane-enabled NAT, firewall,
load balancer, EPC-SGW, and heavy-hitter (HH) detection, all have
the same 8 ğœ‡ğ‘  median latency, identical to that without fault-tolerance.
The NAT, firewall, and load balancer are read-centric and update
state only when a new flow is created; EPC-SGW is mixed-read/write,
and updates state on signaling packets whose frequency is 5% of
data packets. HH detection, although it is write-centric, performs
periodic state replication asynchronously, so it does not affect the
latency. On the other hand, since Sync-Counter updates state and
replicates updates synchronously for every packet, it adds an addi-
tional latency of 20 ğœ‡ğ‘  to every packet. 12 ğœ‡ğ‘  of this overhead is due
9We use the latency reported in the original FTMB paper [70] since we were not able
to get its full implementation.
232
101102103104Latency(Âµs)(log-scale)0.00.51.0CDFSwitch-NATFTSwitch-NATw/controllerRedPlane-NATServer-NATFTServer-NATFTMB-NAT(reported)050100150200Latency(Âµs)0.00.51.0CDFNATFirewallLoadbalancerHH-detectionAsync-CounterSync-Counter(w/ochain)Sync-Counter(w/chain)RedPlane: Enabling Fault-Tolerant Stateful In-Switch Applications
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Figure 10: RedPlane replication bandwidth overhead.
Figure 12: Impact of RedPlane on data plane throughput of
RedPlane-enabled applications.
Figure 11: Impact of the frequency of snapshotting on
RedPlane-enabled HH-detector.
to the 3-way chain replication used to tolerate state store server
failures.
7.2 Bandwidth Overheads
To evaluate network bandwidth overheads, we inject 64-byte pack-
ets from three traffic generation servers atâ‰ˆ207.6 Mpps10 which is
the maximum rate that our traffic generator can achieve.
Additional bandwidth consumed. In this experiment, we in-
strument each application to count the number of bytes it sends
and receives, including both original packets and protocol message
packets. Fig. 10 shows the ratio of bandwidth used for RedPlane
messages to the total traffic. For read-centric applications including
NAT, firewall, load balancer, we see that there is almost no band-
width overhead since RedPlane generates protocol messages only
for the first packet of each flow. For EPC-SGW, RedPlane incurs
12.8% overhead since it generates protocol messages for signaling
packets, and some of data packets are buffered through the network
as described in Â§5.1. For HH-detector, which asynchronously repli-
cates a snapshot of state for every 1 ms, RedPlane incurs negligible
overhead. We also measure the absolute bandwidth overhead for
different snapshot frequencies and number of sketches as shown
in Fig. 11. For a 1 ms period, it consumes 34.16 Mbps (13.8%). Even
with 5 sketches, this is lower than the bandwidth overhead for Sync-
Counter (51.2%) because in the latter case RedPlane requests and
responses contain both headers and original payload. This result
implies that in an extreme case where an application replicates state
updates synchronously for every packet, achieving fault-tolerance
is expensive. We also analyze the bandwidth overhead at scale (i.e., a
topology with more RedPlane switches) for all 6 applications using
our analytical model-based simulation, and the result is consistent
with Fig. 10 in terms of the percentage overhead.
Throughput impact on applications. In this experiment, we
measure the throughput of RedPlane-enabled applications and com-
pare it with the same applications without fault tolerance. We
send 64-byte packets from three servers, one from each rack, to
10Each server generates packets at â‰ˆ69.2 Mpps.
Figure 13: Impact of update ratio on data plane throughput
of the RedPlane-enabled key-value store.
one of servers attached to the core switch at â‰ˆ207.6 Mpps. In our
testbed, the link between an aggregation and a core switch be-
comes the bottleneck, and we observe that the maximum forward-
ing rate the aggregation switch can achieve is around 122.5 Mpps.
Fig. 12 shows the median throughput of each application with
and without RedPlane. Obviously, applications achieve the maxi-
mum throughput without RedPlane. With RedPlane, read-centric
(NAT, firewall, and load balancer) applications and applications that
replicate state updates asynchronously (HH-detector) can achieve
the same throughput as their non fault-tolerant counterparts. The
RedPlane-enabled EPC-SGW achieves a slightly lower through-
put than that of its counterpart, mainly due to some data packets
buffered through the network during the replication. The through-
put of Sync-Counter becomes nearly half that of its counterpart:
we find that it is bottlenecked by the performance of the state store.
This suggests that applying a strict consistency mode degrades the
throughput of write-centric applications as they are also affected
by the performance of the state store.
Varying update ratios. While most of existing in-switch appli-
cations are read-centric or perform asynchronous replication, in-
curring little overhead, it is important to understand the maximum
throughput of applications characterized by different read/write
(i.e., update) ratios. For this experiment, we write a simple in-switch
key-value store in P4 with RedPlane and generate packets consist-
ing of custom header fields that indicate an operation (read or
update), a key, and a value (for updates). We use the same setup as
the previous experiment and let each server generate packets based
on a predefined update ratio with uniformly distributed random
keys. Fig. 13 shows that as the update ratio increases, the through-
put degradation depends on the number of state store servers; by
adding more servers, we can achieve higher throughput.
7.3 Failover and Recovery
Next, we measure how fast the end-to-end performance can be
recovered by RedPlane in the presence of switch failure and re-
covery. We run iperf [15] to measure between two servers, at-
tached to a core switch and a ToR switch respectively. All traffic
233
NATFirewallLoadbalancerEPC-SGWHH-detectorSync-Counter050100BWconsumption(%)99.899.999.987.299.148.825.625.6OriginalpacketsRedPlanereqs.RedPlaneresps.32641282565121024Snapshotfrequency(Hz)010203040Throughput(Mbps)3sketches4sketches5sketchesNATFirewallLoadbalancerEPC-SGWHH-detectorSync-Counter0100Throughput(Mpps)Appsw/oRedPlaneAppsw/RedPlane0.00.20.40.60.81.0UpdateRatio050100150Throughput(Mpps)1statestore2statestores3statestoresSIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan
is 100 Gbps and â‰ˆ2% of requests are lost, our buffering mechanism
consumes at most 18 KB, which is acceptable for a given a few tens
of MB of the packet buffer in the switch ASIC.
Other ASIC resource usage. We also measure the usage of other
ASIC resources consumed by RedPlane data plane for 100K con-
current flows (using the Tofino-P4 compilerâ€™s output), expressed
relative to each applicationâ€™s baseline usage. Ample resources re-
main: SRAM is the most used (13.2%), and all others are less than
10% (details in Appendix E). Scaling up concurrent flows would
increase only SRAM usage, as it stores per-flow information (lease
expiration time, current sequence number, and last acknowledged
sequence number).
8 Related Work
In-switch applications. Recent efforts have shown that offloading
to programmable switches enhances performance. For example,
offloading the sequencer [46], key-value cache [38, 52], and co-
ordination service [37] improves the performance of distributed
systems. However, these applications can lose their state due to
switch failures. RedPlane can help make them fault-tolerant or
simplify their designs.
Fault-tolerance and state management for NFs. Fault-tolerance
for NFs or middleboxes has been addressed by prior systems like
Pico [63] and FTMB [70]. When an NF instance fails, the state of
the failed NF is recovered through checkpoint or rollback recovery
on a new NF instance. These approaches cannot be applied directly
to the switch data plane (Â§2.2). Previous work on state management
for stateful NFs uses local or remote storage to manage NF state [31,
64, 75]. However, these APIs target planned state migration rather
than unplanned failures. Similar work (again, targeting planned
migration) has also been proposed for router migration [41].
External memory for switches. Recent work shares our approach
of using serversâ€™ memory as external storage for switch state [42],