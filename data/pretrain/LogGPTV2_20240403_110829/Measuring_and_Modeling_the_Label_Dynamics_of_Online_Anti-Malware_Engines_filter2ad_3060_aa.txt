title:Measuring and Modeling the Label Dynamics of Online Anti-Malware Engines
author:Shuofei Zhu and
Jianjun Shi and
Limin Yang and
Boqin Qin and
Ziyi Zhang and
Linhai Song and
Gang Wang
Measuring and Modeling the Label Dynamics of 
Online Anti-Malware Engines
Shuofei Zhu, The Pennsylvania State University; Jianjun Shi, BIT, The Pennsylvania 
State University; Limin Yang, University of Illinois at Urbana-Champaign; Boqin Qin, 
BUPT, The Pennsylvania State University; Ziyi Zhang, USTC, The Pennsylvania State 
University; Linhai Song, Pennsylvania State University; Gang Wang, University of 
Illinois at Urbana-Champaign
https://www.usenix.org/conference/usenixsecurity20/presentation/zhu
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Measuring and Modeling the Label Dynamics of Online
Anti-Malware Engines
Shuofei Zhu1, Jianjun Shi1,2, Limin Yang3
Boqin Qin1,4, Ziyi Zhang1,5, Linhai Song1, Gang Wang3
1The Pennsylvania State University
2Beijing Institute of Technology
3University of Illinois at Urbana-Champaign
4Beijing University of Posts and Telecommunications
5University of Science and Technology of China
Abstract
VirusTotal provides malware labels from a large set of anti-
malware engines, and is heavily used by researchers for mal-
ware annotation and system evaluation. Since different en-
gines often disagree with each other, researchers have used
various methods to aggregate their labels. In this paper, we
take a data-driven approach to categorize, reason, and vali-
date common labeling methods used by researchers. We ﬁrst
survey 115 academic papers that use VirusTotal, and identify
common methodologies. Then we collect the daily snapshots
of VirusTotal labels for more than 14,000 ﬁles (including a
subset of manually veriﬁed ground-truth) from 65 VirusTotal
engines over a year. Our analysis validates the beneﬁts of
threshold-based label aggregation in stabilizing ﬁles’ labels,
and also points out the impact of poorly-chosen thresholds.
We show that hand-picked “trusted” engines do not always
perform well, and certain groups of engines are strongly cor-
related and should not be treated independently. Finally, we
empirically show certain engines fail to perform in-depth anal-
ysis on submitted ﬁles and can easily produce false positives.
Based on our ﬁndings, we offer suggestions for future usage
of VirusTotal for data annotation.
1 Introduction
Online anti-malware scanning services such as VirusTo-
tal [11] have been widely used by researchers and industrial
practitioners. VirusTotal connects with more than 70 security
vendors to provide malware scanning. Users (e.g., researchers)
can submit a ﬁle and obtain 70+ labels from different engines
to indicate whether the ﬁle is malicious. This capability has
been heavily used to annotate malware datasets and provide
system evaluation benchmarks [23, 31, 35, 41, 49, 66, 69].
A common challenge of using VirusTotal is that different
security engines often disagree with each other on whether a
given ﬁle is malicious. This requires researchers to come up
with a strategy to aggregate the labels to assign a single label
to the ﬁle. In addition, recent works show that the labels of
a given ﬁle could change over time [18, 41], which makes it
even more difﬁcult to infer the true label of the ﬁle.
As such, researchers have tried various methods to handle
the label dynamics (e.g., monitoring the labels for a few days)
and aggregate the labels across engines (e.g., setting a voting
threshold). However, most of these approaches are based on
intuitions and researchers’ experiences, but lack quantiﬁable
evidence and justiﬁcations. Recent efforts that try to measure
the label dynamics of VirusTotal are often limited in measure-
ment scale [57] or simply lack “ground-truth” [40], making it
difﬁcult to draw a complete picture.
In this paper, we take a data-driven approach to catego-
rize, reason and validate the methodologies that researchers
adopted to use VirusTotal for data annotation. Our efforts in-
clude (1) analyzing more than 100 research papers published
in the past eleven years to categorize their data labeling meth-
ods using VirusTotal, and (2) running a measurement over a
year to collect daily snapshots of VirusTotal labels for a large
set of ﬁles from 65 engines. Our goal is to provide data-driven
justiﬁcations for some of the existing labeling methods (if they
are reasonable), and more importantly, identify questionable
approaches and suggest better alternatives.
Our measurement follows two key principles. First, we
use “fresh” ﬁles that are submitted to VirusTotal for the ﬁrst
time. This allows us to observe the label dynamics from the
very beginning without being distracted by the ﬁles’ previous
history. Second, we track the ﬁne-grained label dynamics
by re-scanning the ﬁles daily. We construct a main dataset
that contains 14,423 ﬁles and their daily labels of 65 engines
for more than a year. This dataset is used to measure the
label dynamics and the relationships between engines. Then
to inspect the label “correctness” of engines, we construct
smaller ground-truth datasets that contain manually-crafted
and manually-veriﬁed malware and benign ﬁles (356 ﬁles).
In total, we collected over 300 million data points.
First, we measure how often individual engines ﬂip their
labels on a given ﬁle over time. We ﬁnd over 50% of the label
ﬂips are extremely short-lived, and will ﬂip back quickly the
next day (i.e, “hazard” ﬂips). Label ﬂips widely exist across
USENIX Association
29th USENIX Security Symposium    2361
ﬁles and engines, and they do not necessarily disappear even
after a year. Instead, we show that threshold-based label ag-
gregation (i.e., a ﬁle is malicious if ≥ t engines give malicious
labels) is surprisingly effective in tolerating label dynamics if
the threshold t is set properly. However, the most-commonly
used t = 1 is not a good threshold.
Second, we model the relationships between different en-
gines’ labels, to examine the “independence” assumption
made by existing works. By clustering engines based on
their label sequences for the same ﬁles, we identify groups
of engines with highly correlated or even identical labeling
decisions. In addition, through a “causality” model, we iden-
tify engines whose labels are very likely to be inﬂuenced by
other engines. Our results indicate that engines should not be
weighted equally when aggregating their labels.
Third, we use the ground-truth data to inspect the label-
ing accuracy of engines, and ﬁnd very uneven performance
from different engines. Interestingly, the “perceived” high-
reputation engines by existing works are not necessarily
more accurate. A subset of engines (including certain high-
reputation engines) tend to produce many false positives when
the ﬁles are obfuscated. This indicates a lack of in-depth anal-
ysis from certain engines, and also poses a challenge to ﬁnd a
universally good method (and threshold) to aggregate labels.
Our contributions are summarized as the following:
• We survey 115 academic papers to categorize their meth-
ods to use VirusTotal for data labeling.
• We collect the daily snapshots of labels from 65 anti-
malware engines for more than 14,000 ﬁles over a year.
We use the dataset to reason and validate common
methodologies for label aggregation. We release the
dataset to beneﬁt future research.
• We measure the potential impact introduced by the un-
stable and inconsistent labels. We identify question-
able methodologies and offer suggestions to future re-
searchers on the usage of VirusTotal.
2 Literature Survey: VirusTotal Usage
We start by surveying how researchers use VirusTotal for data
annotation. We collect recent papers published in Security,
Networking, Software Engineering, and Data Mining, and
then categorize their data labeling methods. The goal is to set
up the context for our measurements.
Collecting Research Papers. We collect conference pa-
pers by searching in Google Scholar with the keyword “Virus-
Total”. We only consider high-quality conference papers in
peer-reviewed venues. In total, we identify 115 relevant papers
published in the last eleven years (2008 – 2018). The authors
either use VirusTotal to label their datasets [23, 31, 41, 49]
or leverage the querying/scanning API of VirusTotal as a
building block of their proposed systems [35, 66, 69].
(a) Conference
(b) Publication Year
(c) File Type
Figure 1: Characteristics of related papers. Sec: Security, Net:
Networking, SE: Software Engineering, and DS: Data Science.
As shown in Figure 1(a), the vast majority of papers (84 out
of 115) are published in security conferences (42 papers from
the “big-four”: S&P, CCS, USENIX Security, and NDSS). Fig-
ure 1(b) shows the upward trend of VirusTotal usage among
researchers over time. As such, it is increasingly important to
formulate a reliable method to use VirusTotal. In Figure 1(c),
we categorize the papers based on the type of ﬁles/programs
that the researchers are scanning, including Portable Exe-
cutable (PE) ﬁles [16, 42, 70], Android APK [15, 23, 36],
URL [73, 82], and others (Flash ﬁle, PDF) [20, 31, 74, 79].
We ﬁnd that PE is the most popular ﬁle type.
VirusTotal Scanning APIs. VirusTotal provides ﬁle scan-
ning and URL scanning services. Its scanning interface con-
nects with more than 70 security vendors. These security
vendors either share their scanning engines for VirusTotal to
deploy (as a software package) or provide the online scan-
ning APIs that accept ﬁle submissions. To use the VirusTotal
API to label a ﬁle, users can submit the ﬁle to VirusTotal,
and VirusTotal returns the scanning results from the 70 ven-
dors (the returned labels could be “malicious” or “benign”,
indicated by the “detected” ﬁeld in VirusTotal responses).
It is known that VirusTotal and its third-party vendors keep
updating their anti-malware engines, and thus the labels of a
given ﬁle may change over time. Given a ﬁle will receive la-
bels from multiple engines, it is not uncommon for the engines
to disagree with each other. For these reasons, researchers of-
ten need to aggregate/process the results to generate a single
label for the given ﬁle (i.e., labeling method).
How Researchers Aggregate the Labels. By manually
analyzing these 115 papers1, we ﬁnd that 22 papers have used
VirusTotal but do not clearly describe their data processing
methods. As such, we use the rest 93 papers to categorize the
mainstream data labeling approaches. A summary is shown
in Table 1. Note that different categories may overlap.
First, threshold-based method. Most papers (82 out of 93)
use a threshold t to determine whether a ﬁle is malicious or
benign. If t or more engines return a “malicious” label, then
the ﬁle is labeled as malicious. Here t can be an absolute
number or a ratio of engines. 50 papers set t = 1: a ﬁle is
malicious if at least one engine thinks it is malicious [51,
60, 72, 73, 78, 81]. For 24 papers, t is set to be larger than
one. Another eight papers set t as a ratio [14, 29, 30, 48, 58,
1The full paper list is available under the following link: https://
sfzhu93.github.io/projects/vt/paper-list.html.
2362    29th USENIX Security Symposium
USENIX Association
SecNetSEDS04080# of papers2008201320180102030# of papersPEAPKURL   Others02550# of papersTable 1: Summary of related papers. We select 5 representative
papers for each category. Different categories may overlap.
Data labeling
Threshold
t = 1
1 < t < 5
t ≥ 5
t < 50%
t ≥ 50%
Reputable Subset
No Aggregation
Dynamic Label Analysis
# Papers Representative Papers
50
9
15
4
4
10
10
11
[60, 62, 72, 73, 78]
[39, 44, 47, 64, 76]
[24, 42, 43, 46, 75]
[48, 58, 74, 87]
[14, 29, 30, 84]
[15, 22, 27, 84, 85]
[34, 50, 53, 54, 80]
[34, 41, 58, 67, 73]
74, 84, 87]. We only ﬁnd a few papers that set an aggressive
threshold. For example, two papers set t = 40 [18, 46]. Four
papers [14, 29, 30, 84] set the threshold as 50% of the engines.
Second, selecting high-reputation engines. In ten papers,
the authors think that different engines are not equally trust-
worthy. As such, the authors hand-picked a small set of en-
gines that are believed to have a good reputation. However,
the high-reputation set is picked without a clear criterion, and
the set is different in different papers. For example, Chan-
dramohan et al. [22] only consider ﬁve engines’ results. Only
two of the ﬁve engines appear in Arp et al. [15]’s trusted set.
Third, no aggregation. Ten papers directly use VirusTo-
tal’s results to build their own system or as their comparison
baselines. For example, Graziano et al. [34] use VirusTotal’s
detection rate as one of their features to train their system. For
the rest nine papers, the authors submit samples to VirusTotal,
to show their detection techniques outperform VirusTotal en-
gines [53,80], or to conﬁrm that they have identiﬁed important
security issues [19,50,56], or to demonstrate the effectiveness
of malware obfuscations [25, 52, 54, 86].
How Researchers Handle the Label Changes.
Surpris-