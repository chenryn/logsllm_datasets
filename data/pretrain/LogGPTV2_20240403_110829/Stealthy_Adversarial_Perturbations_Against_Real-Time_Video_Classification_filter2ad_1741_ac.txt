classiﬁed correctly. We name this unique kind of universal
perturbations as “Dual-Purpose Universal Perturbations” or
DUP for short.
In order to realize DUPs, we have to guarantee that for
the input clip xt, if it belongs to the target class (denote the
set of inputs from the target class as T ), the C3D classiﬁer
returns a low score with respect to the correct class c(xt), i.e.,
Qc(xt). For all input clips xs that belong to other (non-target)
classes (denote the set of inputs from non-target classes as S,
thus, S = X − T ), the model returns high scores with regard
to their correct mappings (Qc(xs)). To cause the generator to
output DUPs, we reﬁne the optimization problem in Equation 1
as shown in Equation 2:
minimize
G
− log[1 − Qc(xt)(xt + G(z))]
− log[Qc(xs)(xs + G(z))]
(2)
λ × (cid:88)
(cid:88)
xt∈T
+
xs∈S
The ﬁrst term in the equation again relates to minimizing
the cross-entropy of the target class, while the second term
maximizes the cross-entropy relating to each of the other
classes. The parameter λ is the weight applied with regard
to the misclassiﬁcation of the target class. For attacks where
stealth is more important, we may use a smaller λ to guarantee
that the emphasis on the misclassiﬁcation probability of the
target class is reduced while the classiﬁcation of the non-target
classes are affected to the least extent possible.
VI.
IMPACT OF NONDETERMINISTIC CLIP BOUNDARIES
In this section, we ﬁrst discuss why directly applying exist-
ing methods to generate perturbations against video streams do
5
(a) Misalignment when the starting position of a clip input to the
classiﬁer, is not aligned with what the attacker assumes. Because
of this, the perturbation added to input clip X1 is a concatenation
of two partial perturbations from P1 and P2.
(b) Misalignment can occur even if the starting position is aligned
when a small stride is used. Here, the stride of the sliding window
is half the clip size. This causes a misalignment because of
which, the perturbation added to input clip X2 is a concatenation
of two partial perturbations from P1 and P2.
Fig. 4: Two cases that can potentially cause misalignment
between perturbation clips and the input clips to the classiﬁer.
The ﬁrst parts of both ﬁgures represent the temporal sequence
of generated perturbation clips. The lower parts of both ﬁgures
capture the temporal sequence of input clips tested by the video
classiﬁer and the perturbation clips added to them.
not work. Subsequently, we propose a new set of perturbations
that do work (and are very effective) on video streams.
A. Misalignment due to Nondeterministic Clip Boundaries
The input to the video classiﬁer is a clip composed of
a sequence of frames. Given any input clip, the previously
described attack methods (UP and DUP) can generate a
perturbation clip that can be added to that input clip. As
discussed in § II-A, an input clip is controlled by a sliding
window which in turn is deﬁned by three hyper-parameters:
the window size l, the sliding stride o, and the starting position
fstart. Because fstart is non-deterministic, the clip boundaries
of an input to the classiﬁer in a real-time video classiﬁcation
system are also nondeterministic. As a result, even for white-
box attackers, they cannot know a priori the clip boundaries
(the consecutive frames in a video stream belonging to an input
clip) used by the video classiﬁer.
The nondeterminism in the clip boundaries is likely to
cause a misalignment between the perturbation clips generated
by the attacker and the input clips used by the classiﬁer.
Figure 4 depicts two cases where misalignment happens even
with the attacker-friendly white-box scenario. The ﬁrst row
shows three perturbation clips P1, P2 and P3 generated by
the attacker 1. The second row shows three input clips X1,
X2 and X3 used by the classiﬁer. The clips in the two
1For UP and DUP, P1 = P2 = P3.
sequences are not aligned because the starting point of the
sliding window is different from that of the perturbation clip.
Consequently, the perturbation applied to input clip X1 is
actually a concatenation of the latter part of P1 and the ﬁrst
part of P2 (a perturbation P (cid:48)
1).
In a second case, as shown in Figure 4b, the perturbation
clip P2 and the input clip X2 are not aligned because the stride
of the sliding window is smaller than the window size. This
smaller stride is commonplace in video classiﬁcation systems
as discussed in [6], [8], [46], [47].
B. The Boundary Effect
Because C3D utilizes a 3D CNN, we ﬁnd via empirical
experiments that when there is a misalignment between the
perturbation clip and the input clip, it can cause signiﬁcant
degradations in the attack success rates, even for universal
perturbations. For example, considering Figure 4a, the DUP
P1 should work on any input clip; however, the actual applied
perturbation clip P (cid:48)
1 (which is the concatenation of two partial
broken up perturbations) is less likely to work. We refer to
this phenomenon as the boundary effect.
boundary
problem,
let
by
{. . . , fi−2, fi−1, fi, fi+1, fi+2, . . .} where,
represents
the ith frame. The perturbation genererated by G to cause
a misclassiﬁcation of the clip {fi, fi+1, . . . , fi+l−1} (say
{p1, p2, . . . , pl}) will be different from the one generated
temporally staggered clip {fi−1, fi, . . . , fi+l−2}
for
(true
including
for
the perturbed clip
UP and DUP).
{fi−1 ⊕ p1, f1 ⊕ p2, . . . , fi+l−2 ⊕ pl} is unlikely to be
effective in achieving misclassiﬁcation.
stream represented
In other words,
previously
designed
perturbations
the
a
To
us
formalize
consider
a
video
effect
fi
To exemplify this problem, we perform extensive evalua-
tions of existing established methods with regard to attacking
the C3D model. In particular, we use the APIs from the
CleverHans repository [35] to generate video perturbations. We
experiment with several methods from CleverHans, including
the most recent ones (e.g., CarliniWagnerL2 and DeepFool).
The results presented in the paper are based on the basic
iteration method [24] with default parameters and all the videos
in the UCF-101 testing set. We point out here that results based
on all the other methods in the repository are very similar.
We consider different boundaries for the clips in the videos
(temporally staggered versions of the clips) and generate
perturbations for each staggered version. Note that the sliding
window size for C3D is 16 and thus, there are 16 staggered
versions. We choose a candidate frame, and compute the
correlations between the perturbations added in the different
staggered versions. Speciﬁcally, the perturbations are tensors
and the normalized correlation between two perturbations is
the inner product of the unit-normalized tensors representing
the perturbations.
We represent the average normalized correlations in the
perturbations (computed across all frames in the testing set) for
two locations in the matrix shown in Figure 5. The row index
and the column index represent the location of the frames in
the two staggered clips. For example, the entry corresponding
to {7, 7} represents the case where the frame considered was
the 7th frame in the two clips, (actually, here it is the same
6
clip). In this case, clearly the correlation is 1.00. However,
we see that the correlations are much lower if the positions
of the same frame in the two clips (two staggered versions)
are different. As an example, consider the entry {5, 9} which
corresponds to the case where a frame is the ﬁfth position
in clip 1, and the same frame is at the ninth position in clip
2: the average normalized correlation between the two added
perturbations is 0.39, which indicates that the perturbations
that CleverHans adds in the two cases are quite different.
Fig. 5: The average normalized correlation matrix computed
with perturbations generated using the basic iteration API from
CleverHans. The rows and columns represent the location of
a frame in the two clips. The value represents the correlation
between perturbations on the same frame but generated when
that frame located in different positions (indicated by the row
and column indices) in the two temporally staggered clips.
Fig. 7: Attack success rate when there is mismatch. The
abscissa is the offset between the clip generating perturbation
and the clip tested. The ordinate is the attack success rate.
(Attack aims to misclassify a given video clip from UCF 101
dataset.)
the number of
Fig. 8: This ﬁgure illustrates the Generator and Roll for
generating C-DUP. 1) The generator takes a noise vector
as input, and outputs a perturbation clip with 16 frames.
temporal dimensions with the
Note that
C3D model
is
shown as temporal dimension×horizontal spatial dimension×
vertical spatial dimension × number of channels. 2) The roll
part shifts the perturbation clip by some offset. The ﬁgure
shows one example where we roll the front black frame to the
back.
is 16. The output size for each layer
Fig. 6: Magnitude of perturbation on each frame: The abscissa
is the frame position, and the ordinate is the magnitude
of average perturbation on the frame. (The attack seeks to
misclassify a given video clip from UCF 101 dataset.)
In Figure 6, we show the average magnitude of perturba-
tions added (over all frames and all videos), when the target
frame is at different locations within a clip. The abscissa
depicts the frame position, and the ordinate represents the
magnitude of the average perturbation. While the difference
in the magnitude of perturbations added to two frames that
are close to each other in terms of position (e.g., adjacent
frames) within the clip, is small (this is because such frames
are similar), the magnitude of perturbations added to frames
that are distant in terms of location could potentially be quite
different (because such frames could be quite dissimilar).
7
We further showcase the impact of the boundary effect by
measuring the degradation in attack efﬁcacy due to mismatches
between the anticipated start point when the perturbation is
generated and the actual start point when classifying the clip
(as shown in Figure 4a). Figure 7 depicts the results. The
abscissa is the offset between the generated (intended) pertur-
bation clip and the input clip used in classiﬁcation. We can see
that as the distance between the two start points increases, the
attack success rate initially degrades but increases again as the
the tested perturbation clip (a concatenation clip) is closer or
more similar (has a better overlap) to the intended perturbation
clip. For example, if the offset is 15, the perturbation clip added
(concatenation clip) is offset by a single frame compared to
the original (intended) perturbation clip.
C. Circular Dual-Purpose Universal Perturbation
To cope with the boundary effect, we develop a novel
extension to the generative DNN model to signiﬁcantly modify
the DUPs proposed in § V to compose what we call “Circular
Dual-Purpose Universal Perturbations (C-DUP).”
Fig. 9: This ﬁgure illustrates the Generator and Tile for
generating 2D-DUP. 1) The generator takes a noise vector as
input, and outputs a single-frame perturbation. 2) The tile part
constructs a perturbation clip by repeating the single-frame
perturbation generated 16 times.
Let us suppose that the size of the sliding window is
16. Then, the DUP clip P includes 16 frames (of pertur-
bation), denoted by {p1, p2, . . . , p16}. Since P is a clip of
universal perturbations, we launch the attack by repeatedly
adding perturbations on each consecutive clip consisting of
16 frames, in the video stream. One can visualize that we are
generating a perturbation stream which can be represented as
{p1, p2, . . . , p15, p16, p1, p2, . . .}. Now, our goal is to guaran-
tee that the perturbation stream works regardless of the clip
boundaries chosen by the classiﬁer. Towards this, we need to
ensure that any sequential concatenation of partial perturbation
clips (the last part of the ﬁrst clip and the ﬁrst part of the
second clip) results in a valid perturbation. It is easy to see
that for this to hold true, we need any cyclic or circular shift
of the DUP clip to be a valid DUP perturbation too. In other
words, we require the perturbation clips {p16, p1, . . . , p15},