sary has not seen any checksum value. Furthermore, we
also need to ensure that a malicious cloud cannot modify
the checksum and claim a diﬀerent checksum. We therefore
need to ensure the conﬁdentiality and authenticity (implies
freshness) of the checksums. To do this, the client will rely
on authenticated encryption to encrypt these checksums be-
fore attaching them to the blocks and uploading them to
a cloud. To ensure freshness, the authenticated encryption
incorporate the time of write and the position as well. One
way to do this is the following:
for each block shuﬄed at
time t and shuﬄed to position pos on cloud Sj, generate a
one-time authenticated encryption key ak based oﬀ a master
secret key ck known only to the client:
ak := PRFck (pos, t, j,“AE-Write-Shuﬄe”)
˜σ := AEak (σ)
where AE is an authenticated encryption scheme. For shuf-
ﬂes that occur in the ReadPartition algorithm (see Fig-
ure 4), a diﬀerent tag “AE-Read-Shuﬄe” may be used.
Optimization. In description so far, S2 sends the client a
new checksum σ2(B(cid:48)) for each block B(cid:48) after shuﬄing, and
the client veriﬁes them one-by-one. A simple optimization is
for S2 to use a collision-resistant hash function (e.g., SHA-
256), and hash all of the σ2(B)’s, and send a single hash to
the client C — who can then verify the hash by reconstruct-
ing all the σ2(B)’s.
4.2 Verifying Fetched Blocks
Whenever the client reads any data block using the
ReadPartition algorithm, it needs to verify the authen-
ticity and freshness of the fetched block. Our encrypted
and authenticated checksums ˜σ1(B) and ˜σ2(B) allow the
client to verify the authenticity and freshness for the block
B. Particularly, the freshness property is due to the fact
the authenticated encryption uses a time and position de-
pendent key.
In the ReadPartition algorithm, we simply use the afore-
mentioned algorithm to verify the shuﬄes associated with
read operations. Finally, after fetching a block B, the client
veriﬁes its authenticity and freshness using ˜σ1(B) or ˜σ2(B).
5. EXPERIMENTAL RESULTS
5.1 Overview and Experimental Setup
We have implemented a full-ﬂedged multi-cloud ORAM
implementation, consisting of 12,000 lines of code. Our im-
plementation uses hardware-accelerated AES-NI whenever
available (it is available on most modern Intel processors).
Figure 6: Our deployment. The workload is distributed
across multiple servers per cloud communicating in pairs.
We use this deployment for the experiments.
Our ORAM implementation uses asynchronous I/O oper-
ations to increase throughput. As Stefanov and Shi point
out in their recent work [30], in an asynchronous ORAM, to
avoid information leakage through timing of I/O events, we
need to make sure that the scheduling module does not use
any information related to the logical access pattern. We
therefore follow this guideline in our multi-cloud ORAM de-
sign, to ensure security in the asynchronous ORAM model.
Deployment. We deployed our multi-cloud ORAM im-
plementation running on top of two real-world major cloud
providers: Amazon Web Services (AWS) [1] and Microsoft
Azure [4]. We rented up to 5 servers per cloud. The AWS
servers were High I/O Quadruple Extra Large Instances,
each with 2 SSD-based volumes (1 TB each). The Azure
servers were Extra Large instances, with the Azure blob
storage as the storage backend (SSDs were not available on
Azure). Figure 6 illustrates our deployment.
Although these VM instances have large memory and fast
CPUs, we chose them only because they are provisioned
by the cloud providers with higher disk I/O and network
I/O than other instances. As explained in Section 5.4 and
Section 5.5, CPU and memory were never the bottlenecks
in our experiments.
Latency. Unless otherwise speciﬁed, we simulate a 50ms
(round-trip) latency between the client and the closest cloud
by asynchronously delaying the client’s requests and responses.
Since our two clouds are in fact two diﬀerent clouds (Amazon
and Azure), we do not simulate additional latency between
the clouds.
Scaling. We scale our experiments up to 5 servers per
cloud, because our experiments suggest that by this time,
the client-cloud bandwidth would already have been satu-
rated in most typical settings – hence, further scaling up in-
side the clouds would not have increased the overall ORAM
throughput. We consider a single client as in most existing
ORAM work.
Warming up. All of our experimental results represent the
performance of a warmed-up ORAM with N blocks. We
use the same technique for warming-up the ORAM as in
previous work [30, 31]. As explained in the full online ver-
sion [29], the client initializes the internal data structures
254of our construction so that the ORAM immediately starts-
oﬀ in a warmed-up state (i.e., with multiple ﬁlled levels in
each partition) without having to pre-initialize or zero-out
the server-side storage. This ensures that our performance
measurements are steady-state measurements, not burst or
best-case performance.
Access pattern. Because our ORAM construction protects
the privacy of the access pattern, the access pattern will have
no distinguishable eﬀect on the performance of the clouds.
However, the access pattern can aﬀect the size of the client’s
internal data structures (up to a certain point).
In order
to fairly evaluate our construction, we used a round-robin
access pattern as in previous work [30, 31], which turns out
to maximize the client storage by reducing the possibility of
reusing data already stored by the client.
For ORAM, reads and writes are also indistinguishable. In
our construction, they each involve invoking ReadPartition
and then WritePartition, except that during write opera-
tion, the client locally updates the block’s value in between
the ReadPartition and WritePartition operations. Our
access pattern hence consisted of writes.
System load. All of our experiments represent the perfor-
mance of our system under full load. In other words, data
is continuously being written to the ORAM as fast as the
ORAM can handle it. For each trial of each data point, we
started measuring the performance after 200 MB of data had
been written to the ORAM and we stopped measuring after
400 MB of data had been written (committed to disk by the
cloud). Although the client memory is up to 1.5 GB for a
1 TB ORAM, most of it is used for the position map, and
semaphores in our implementation ensure that only at most
70 MB of it is used to temporarily store data blocks before
they are evicted to the clouds. Therefore by starting our
measurements after 200 MB of data is written, we eliminate
the eﬀect of data buﬀering by the client.
Number of trails. Except for Figure 15, which provides
exact values calculated for our construction, each data point
in each ﬁgure represents the average over 20 trials. The error
bars represent one standard deviation.
5.2 Results: Single Server Per Cloud
We ﬁrst experimented with a single server per cloud, and
report our experimental results below.
Throughput to a large extent depends on the bandwidth
of the bottleneck resource.
If the client-cloud link is the
bottleneck, then our throughput would be roughly BW/2.6
where BW denotes the available bandwidth on the client-
cloud link. We refer to the number 2.6 as the client-cloud
bandwidth cost, i.e., to access a block, roughly 2.6 blocks
need to be transferred across the client-cloud link. A break-
down of the client-cloud bandwidth cost is shown in Fig-
ure 12.
In Figures 7, we measure maximum ORAM throughput
that our multi-cloud system can handle with with 1 server
per cloud when the client-to-cloud bandwidth is ample. A
300 GB ORAM of 4 KB blocks with a client over a 50ms
latency connection to the clouds can handle up to about 0.8
to 1.0 MB/s throughput.
If the block size is increased to
8 KB or 16 KB, the ORAM can sustain 1.2 to 1.6 MB/s
of throughput. The graph shows that for ORAMs of sizes
between 50 and 300 GB, the throughput is about the same.
Theoretically it should decrease by about 15% because an
ORAM of size 300 GB has about 14 levels per partition
whereas an ORAM with size 50 GB has about 12 levels
per partition and the cost I/O cost of the ORAM is pro-
portional to the number of levels in the partitions. This
slight decrease in performance is somewhat noticeable but
it’s mostly masked by variance due to external factors such
as ﬂuctuation in cloud network performance.
Figures 8,9, and 10 show the response time of our sys-
tem under diﬀerent parameterizations and the eﬀect that
the client-to-cloud latency has on the system throughput
and response time. In Figure 10 the dotted line is the ideal
y = x curve, and represents the case when the request can be
handled in exactly the client-to-cloud latency. The response-
time of our system is about 200ms to 300ms higher than the
client-to-cloud latency due to the cloud-to-cloud latency and
the network congestion created by the fact that our system
is under full load.
In these ﬁgures, the actual client-cloud bandwidth con-
sumption is about 2.6 times the ORAM throughput.
5.3 Scaling to Multiple Servers per Cloud
Figure 11 shows the scaling eﬀect when we use multiple
servers per cloud as described in the full online version [29].
Each server handles roughly 300GB of ORAM capacity, and
the load distribution scheme is described in the full online
version [29]. The total ORAM capacity is therefore 300GB
times the number of servers per cloud. We can see that the
throughput roughly scales linearly as the number of servers
grow (when the client-cloud link is not the bottleneck). The
variations are due mostly to varying network performance
between speciﬁc servers in the clouds. Our experiments also
suggest that the response time grows very slightly as the
number of servers per cloud grows (Figure 15).
We did not further scale up our experiments beyond 5
servers per cloud, since in the real-world settings we con-
sider, the client-cloud bandwidth would already have been
saturated at this scale.
5.4 Bottleneck Analysis
Client-cloud link is the bottleneck. As mentioned ear-
lier, if the client-cloud link is the bottleneck, the ORAM
throughput would just be BW/2.6, where BW is the client-
cloud bandwidth.
Storage performance is the bottleneck. When the
client-cloud link has ample bandwidth, the next immedi-
ate bottleneck is storage performance. In our deployment,
since Azure did not oﬀer SSD-capable instances, Azure’s
blob storage becomes the bottleneck. Figures 7, 8, 9, 10,
and 11, reported above focus on this case.
Inter-cloud networking is the bottleneck. Figures 14
and 15 report the throughput and response time we could
potentially obtain had the storage performance not been the
bottleneck – e.g., if Azure could provide SSD as their stor-
age; or if in the future, RAM-based persistent storage be-
comes available (e.g., RAMClouds [24]). In our deployment
scenario, the cloud-cloud network bandwidth varied from
30–60MB/s between a pair of servers, which is slower than
typical SSD performance (about 150MB/s). Therefore, to
extrapolate the performance we could obtain had Azure pro-
vided SSD-based storage, in Figures 14 and 15 we emulate
the storage back-end using /dev/zero (or its equivalent on
Windows) – such that the cloud-cloud network is the bot-
tleneck.
255Figure 7: ORAM throughput vs. ca-
pacity. With 1 server per cloud, and a
simulated 50ms latency client-cloud network
link.
Figure 8: ORAM response time vs.
capacity. With 1 server per cloud, and a
simulated 50ms latency client-cloud network
link.
Figure 9: Eﬀect of client-cloud net-
work latency on throughput. With 1
server per cloud, and an ORAM of 300GB
capacity.
Figure 10: Eﬀect of client-cloud net-
work latency on response time. With
1 server per cloud, and an ORAM of 300GB
capacity. The dotted line represents the net-
work roundtrip latency – this is the lower
bound on response time.
Figure 11: Scaling to multiple servers
per cloud: Throughput. With a sim-
ulated 50ms latency client-cloud network
link. The total ORAM capacity is 300GB
times the number of servers. The work-
load distribution scheme across servers is ex-
plained in the full online version [29].
Figure 12: Breakdown of client-cloud
bandwidth cost. Measured between the
client and all clouds combined for a single
ORAM read/write. The number of servers
per cloud does not aﬀect the client-cloud
bandwidth cost.
Figure 13: Microbenchmarks for
checksum computation. Rate at which
each VM in our experiments is able to per-
form the commutative checksum computa-
tion (parallelizing across all cores). Check-
sum performance slightly decreases with the
block size due to a decrease in memory lo-
cality as the checksum matrix increases pro-
portionally to the block size.
Figure 14: Potential
improvement
in throughput if storage were fast
enough (for 4 KB blocks). The “Stor-
age” curve represents the real-world case
where the storage performance is the bot-
tleneck. The “Cloud-cloud network” curve
emulates the storage with /dev/zero such
that the cloud-cloud network link is the bot-
tleneck.
Figure 15: Potential
improvement
in response time if storage were fast
enough (for 4 KB blocks). The “Stor-