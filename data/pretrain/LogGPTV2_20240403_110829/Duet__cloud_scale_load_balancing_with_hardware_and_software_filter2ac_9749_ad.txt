DUET Controller: The controller is the heart of DUET. It per-
forms three key functions: (1) Datacenter monitoring: It gathers
the topology and trafﬁc information from the underlying network.
Additionally, it receives the VIP health status periodically from the
host agents. (2) DUET Engine: It receives the VIP-to-DIP mapping
from the network operator and the topology and trafﬁc information
from the DC-monitoring module, and performs the VIP-switch as-
signment as described in § 4.
(3) Assignment Updater: It takes
the VIP-switch assignment from the DUET engine and translates it
into rules based on the switch agent interface. All these modules
communicate with each other using RESTful APIs.
4The VIP assignment algorithm also needs some changes to handle
TIPs. We omit details due to lack of space.
5Outgoing packets on established connections use DSR.
HMUX-1 Tunneling Table Index Encap IP 0 20.0.0.1 1 20.0.0.2 Tunneling Table Tunneling Table Index Encap IP 0 DIP-512 . .. 511 DIP-1023 VIP VIP DIP HMUX-2 HMUX-3 Index Encap IP 0 DIP-0 . .. 511 DIP-511 20.0.0.1 20.0.0.2 ACL Table Tunneling Table Index 0 1 2 3 ECMP Table Encap IP 100.0.0.1 100.0.0.2 110.0.0.1 110.0.0.2 Destination IP Port 10.0.0.0/32 HTTP 10.0.0.0/32 FTP Duet Engine Assignment Updater Datacenter Monitoring VIP Settings VIP-switch assignment Duet Controller Operator Host Agent Decapsulation Traffic metering SMux Encapsulation VIP-DIP reconfiguration Route announcement  Switch Agent Encap + Decap VIP-DIP reconfiguration  Route announcement VIP-DIP Mapping DIP health Traffic  Topology 33Figure 10: Our testbed. FatTree with two containers of two Agg
and two ToR Switches each, connected by two Core switches.
Figure 12: VIP availability during failure.
The experiment has three steps. (1) All 11 VIPs are assigned to
the SMuxes, and we generate a total trafﬁc of 600K packets per
second to the 10 VIPs (60K per VIP). Since each VIP is announced
from every SMux, the trafﬁc is split evenly between all SMuxes,
and each SMux is handling 200K packets per second. (2) At time
100 sec, we increase the trafﬁc to 1.2M packets per second, so each
SMux is handling 400K packets per second. (3) Finally, at time
200 sec, we switch all VIPs to a single HMux hosted on ToR 1.
The metric of interest is the latency to the unloaded VIP, mea-
sured using pings sent every 3ms. We measure the latency to the
unloaded VIP so that the latency only reﬂects the delay suffered at
the SMux or HMux – the VIP or the DIP itself is not the bottleneck.
The results shown in Figure 11.
We see that until time 100 sec, the latency is mostly below 1ms,
with a few outliers. This is because each SMux is handling only
200K packets per second, which is well within its capacity (300K
packets per second – see §2), and thus there is no signiﬁcant queue
buildup. At time 100, the latency jumps up – now each SMux is
handling 400K packets per second, which is well beyond its ability.
Finally, at time 200 sec, when all VIPs are on a single HMux, the
latency goes down to 1ms again. This shows that a single HMux
instance has higher capacity than at least 3 SMux instances.
In fact, since HMux processes all packets in the data plane of the
switch, it can handle packets at line rate, and no queue buildup will
occur till we exceed the link capacity (10Gbps in this experiment).
7.2 HMux Failure Mitigation
One of the most important beneﬁts of using the SMux as a back-
stop is automatic failure mitigation, as described in §5. In this ex-
periment, we investigate the delay involved in failing over from an
HMux to an SMux. This delay is important because during failover,
trafﬁc to the VIP gets disrupted.
We assign 7 VIPs across HMuxes and the remaining 3 to the
SMuxes. We fail one switch at 100 msec. We measure the impact
of HMux failure on VIP availability by monitoring the ping latency
to all 10 VIPs every 3ms.
Figure 12 shows the ping latency for three VIPs: (1) One on the
failed HMux (VIP3), (2) One on a healthy HMux (VIP2), and (3)
One on an SMux (VIP1), respectively.
We make three observations: (1) The trafﬁc to VIP3 falls over
to SMux within 38 msec after HMux failure. The delay reﬂects
the time it takes for other switches to detect the failure, and for the
routing to converge. The VIP was not available during this period,
i.e., there is no response to pings. (2) After 38 msec, pings to VIP3
are successful again. (3) The VIPs assigned to other HMuxes and
SMuxes are not affected; their latency is unchanged during HMux
failure. These observations demonstrate the effectiveness of using
SMux as a backstop in the DUET design.
Figure 11: HMux has higher capacity.
Switch Agent: The switch agent runs on every switch. It uses
vendor-speciﬁc APIs to program the ECMP and tunneling tables,
and provides RESTful APIs which are used by the assignment up-
dater to add/remove VIP-DIP mapping. On every VIP change, the
switch agent ﬁres touting updates over BGP.
Host Agent and SMux: The host agent and SMux implementa-
tion are the same as in Ananta. The host agent primarily performs
packet decapsulation, SNAT and DIP health monitoring. Addition-
ally, the host agents perform trafﬁc metering and report the statis-
tics to the DUET controller.
Same as in Ananta, we run a BGP speaker along side of each
SMux to advertise all the VIPs assigned to the SMux.
In total, the controller code consists of 4200 LOC written in C#,
and the switch agent code has about 300 LOC in Python.
7. TESTBED EXPERIMENTS
Our testbed (Figure 10) consists of 10 Broadcom-based switches
and 60 servers. Of the 60 servers, 34 act as DIPs and the others are
used to generate trafﬁc. Each of ToRs 1, 2 and 3 is also connected
to a server acting as SMux.
Our testbed experiments show: (1) HMuxes provide higher ca-
pacity, (2) DUET achieves high availability during HMux failure as
the VIP trafﬁc seamlessly falls back to SMuxes, and (3) VIP mi-
gration is fast, and DUET maintains high availability during VIP
migration.
7.1 HMux Capacity
If the load balancer instances have low capacity, packet queues
will start building up, and trafﬁc will experience high latency.
This experiment illustrates that individual HMuxes instances (i.e.,
a switch) have signiﬁcantly higher capacity than individual SMux
instances.
The experiment uses 11 VIPs, each with 2 DIPs. We send UDP
trafﬁc to 10 of the VIPs, leaving the 11th VIP unloaded.
Agg ToR Server + SMux 2 1 Core Container-1 2 2 1 S M S … 1 S M S … 4 4 3 S S … 3 S M S … Container-2  0 10 20 30 0 100 200 300Latency (msec)Time (sec)SMux (600k)SMux (1.2M)HMux (1.2M) 0 1 0 50 100 150 200Time (msec)VIP1 0 1Latency (msec)VIP2 0 1TfailTrecoverVIP3VIP on HMuxVIP on SMux34Figure 13: VIP availability during migration.
7.3 VIP Migration
Recall that we also use SMux as a backstop during VIP migra-
tion. We now investigate the delays involved in this process. This
delay is important because it places a lower bound on how quickly
DUET can react to network conditions.
In this experiment, we assign 7 VIPs to the HMuxes and the re-
maining 3 VIPs to the SMuxes. We migrate a VIP from HMux-
to-SMux (VIP1), SMux-to-HMux (VIP2), and HMux-to-HMux
through SMux (VIP3) at different times. We measure the VIP avail-
ability by monitoring the ping latency (every 3ms) to these VIPs,
and we also measure the migration delay.
Figure 13 shows the ping latency. At time T1, the controller
starts the ﬁrst wave of migration by sending the migrate command
(migrate to SMuxes) to the corresponding switch agents for VIP1
and VIP3. It takes about 450ms for the migration to ﬁnish (time
T2), at which time, the controller sends another migrate command
(migrate back to HMux) to VIP2 and VIP3, which takes about
400ms to take effect (time T3). We see that that all three VIPs
remain fully available during the migration process. The VIPs see
a very slight increase in latency when they are on SMux, due to
software processing of packets on SMux.
Note that unlike the failure scenario discussed earlier, during the
migration process, there is no “failure detection” involved. This is
why we see no ping packet loss in Figure 13.
Figure 14 shows the three components of the migration delay:
(1) latency to add/delete a VIP as measured from the time the
controller sends the command to the time other switches receive
the BGP update for the operation, (2) latency to add/delete DIPs
as measured similarly as the VIPs, (3) latency for the BGP up-
date (routing convergence), measured as the time from the VIP is
changed in the FIB on one switch till the routing is updated in the
remaining switches, i.e., BGP update time on those switches.
Almost all (80-90%) of the migration delay is due to the latency
of adding/removing the VIP to/from the FIB. This is because our
implementation of the switch agent is not fully optimized – improv-
ing it is part of our future work.
8. EVALUATION
In this section, we use large-scale simulations to show that: (1)
DUET needs far fewer SMuxes than Ananta to load balance the
same amount of VIP trafﬁc; (2) Despite using fewer SMuxes (and
hence being cheaper), DUET incurs low latency on load balanced
trafﬁc; (3) The VIP assignment algorithm is effective; (4) Net-
work component failures do not cause signiﬁcant congestion, even
though DUET’s VIP assignment algorithm is oblivious to network
component failures; (5) The migration algorithm is effective.
(a) Add
(b) Delete
Figure 14: Latency breakdown.
Figure 15: Trafﬁc and DIP distribution.
8.1 Simulation Setup
Network: Our simulated network closely resembles that of a
production datacenter, with a FatTree topology connecting 50k
servers connected to 1600 ToRs located in 40 containers. Each
container has 40 ToRs and 4 Agg switches, and the 40 containers
are connected with 40 Core switches. The link and switch memory
capacity were set with values observed in production datacenters:
routing table and tunneling table sizes set to 16k and 512, respec-
tively, and the link capacity set to 10Gbps between ToR and Agg
switches, and 40 Gbps between Agg and Core switches.
Workload: We run the simulations using the trafﬁc trace col-
lected from one of our production datacenters. The trace consists
of 30K VIPs, and the number of DIPs and the trafﬁc distribution
across the VIPs are shown in Figure 15. We divide the 3-hour trace
into 10-minute intervals, and calculate the VIP assignment in each
interval, based on the trafﬁc demand matrix (the number of bytes
sent and received between all sources and destinations), the topol-
ogy and the forwarding tables.
8.2 SMux Reduction
We ﬁrst compare the number of SMuxes needed in DUET and
Ananta to load-balance same amount of trafﬁc in the datacenter.
We calculate the number of SMuxes needed by Ananta such that
no SMux receives trafﬁc exceeding its capacity. We consider two
SMux capacities: 3.6Gbps, as observed on the production SMuxes
(§2), and 10Gbps, assuming the CPU will not be a bottleneck.
The number of SMuxes needed for DUET depends on the ca-
pacity of SMux, the trafﬁc generated by VIPs that could not be
assigned to HMuxes, and speciﬁcs of failure model, and migra-
tion probabilities (§3.3). In this experiment, we assign the VIPs
to HMuxes using the algorithm described in §4, which tries to as-
sign as many VIPs to HMuxes as it can, subject to switch memory
and link bandwidth constraints. We have speciﬁed the memory and
bandwidth details earlier.
Based on failure scenarios in [13, 21], we provision the number
of SMuxes to handle the maximum trafﬁc under either (1) entire
container failure, or (2) three random switch failures. For example,
if an entire container fails, the total trafﬁc T to all the VIPs assigned
 0 1 0 300 600 900 1200 1500Time (msec)VIP1 0 1Latency (msec)VIP2 0 1T1T2VIP3T3VIP on HMuxVIP on SMux 0 100 200 300 400 500 600Latency (msec)Add-DIPsAdd-VIPVIP-Announce 0 100 200 300 400 500 600Latency (msec)Delete-DIPsDelete-VIPVIP-Withdraw 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDFFraction of Total VIPsBytesPacketsDIPs35Figure 16: Number of SMuxes used in Duet and Ananta.
Figure 18: Number of SMuxes used by Duet and Random.
Figure 17: Latency (microseconds) vs. number of SMuxes in
Ananta and DUET.
T
Csmux
to the switches inside need to fail over to SMuxes. Thus the number
of SMuxes needed is
where Csmux is SMux capacity.
We ignore migration – it is covered in §8.6.
Figure 16 shows that DUET requires far fewer SMuxes compared
to Ananta at all trafﬁc rates. Note the log scale on Y axis. For all
the trafﬁc rates, DUET was able to assign 16k VIPs to the HMuxes
(routing table limit). Overall, compared to Ananta, DUET requires
12-24x times fewer SMuxes when the SMux capacity is 3.6 Gbps
and 8-12x times fewer SMuxes when the SMux capacity is 10Gbps,
across different trafﬁc loads.
We note that for all trafﬁc scenarios, majority of the SMuxes
needed by DUET were needed to handle failure. The fraction of
SMuxes needed to handle the trafﬁc to the VIPs that could not be
assigned to the HMux is small. This shows that the VIP assignment
algorithm does a good job of “packing” VIPs into HMuxes.
8.3 Latency vs. SMuxes
Another way to look at the trade-off described in §8.2 is to hold
the trafﬁc volume constant, and see how many SMuxes Ananta
needs to provide the same latency as DUET. This is shown in ﬁg-
ure 17.
We hold the trafﬁc at 10Tbps, and vary the number of SMuxes
for Ananta from 2000 to 15,000. The black line shows median
latency for Ananta. The red dot represents DUET. DUET used 230
SMuxes, and achieved median latency of 474 µsec.
We see that if Ananta were to use the same number of SMuxes as
DUET (230), the median latency would be many times higher (over
6 ms). On the other hand, Ananta needs 15,000 SMuxes to achieve
latency comparable to DUET.
The absolute latency numbers may appear small – however, re-
call that median DC RTTs are of the order of 381 µsec6, and in
many cases, to satisfy a single user request, an application like
Search traverses load balancer multiple times. Any time lost in the
network is wasted time – which could have otherwise been used by
the application to improve user experience [8, 14, 19].
6Newer technologies such a RDMA lower this to 2-5 µsec!
Figure 19: Impact of failures on max. link utilization.
8.4 Duet vs. Random
To understand the impact of assigning VIPs based on the max-
imum resource utilization, we compare the performance of DUET
in terms of the number of SMuxes against a random strategy (Ran-
dom) that selects the ﬁrst feasible switch that does not violate the
link or switch memory capacity. This assignment algorithm can be
viewed as a variant of FFD (First Fit Decreasing) as the VIPs are
assigned in the sorted order of decreasing trafﬁc volume.
Figure 18 shows the total number of SMuxes needed by DUET
and Random (note the log scale). We see that Random results in
120%–307% more SMuxes compared to DUET as the trafﬁc load
varies from 1.25 to 10 Tbps. This shows that by taking resource
utilization into account, DUET ensures that only a small fraction of
VIPs trafﬁc is left to be handled by the SMuxes.
8.5 Impact of Failure