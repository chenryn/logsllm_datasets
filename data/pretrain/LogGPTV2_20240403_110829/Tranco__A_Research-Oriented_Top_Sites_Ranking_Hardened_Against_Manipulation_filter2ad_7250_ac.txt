if a given number of
fully disappear from the Alexa list
domains are manipulated. We consider removal for different
subsets, as commonly used by the studies that we surveyed in
Section IV-A. The smallest number of manipulated domains
required is 7 032, 1 652, 74 and 24 for the top 1M, 100K,
10K and 1K respectively; 15 providers need less than 100 000
manipulated domains to disappear from the top 1M.
As we will show, the cost of such large-scale manipulation
is very low and well within reach of larger providers, espe-
cially given the incentive of being able to stealthily continue
TABLE III.
SUMMARY OF MANIPULATION TECHNIQUES AND THEIR
ESTIMATED COST.
Provider
Technique
Monetary
Cost
Effort
Time
Alexa
Umbrella
Majestic
Quantcast
Extension
Certify
Cloud providers
Backlinks
Reﬂected URLs
Quantiﬁed
none
medium
low
high
none
low
medium
medium
medium
high
high
medium
low
high
low
high
medium
high
These techniques can be applied to both new domains and
domains already present in the lists, e.g. when those domains
bear the properties that could skew certain studies; a domain
that has been ranked for a longer period of time may enjoy a
higher trust or importance. In our work, we focus on techniques
that directly inﬂuence the rankings’ data at a modest cost. An
alternative approach could be to buy expired or parked domains
already in the list [53]. However, expired domains are usually
bought up very quickly by “drop-catchers” [43], leaving a
limited number of ranked domains open for registration [63].
Meanwhile, popular parked domains can command prices
upwards of 1 000 USD [63]. This approach therefore incurs
a prohibitive cost, especially at a large scale.
A. Alexa
Alexa ranks domains based on trafﬁc data from two
sources: their “Trafﬁc Rank” browser extension that reports
all page visits, and the “Certify” analytics service that uses a
tracking script to count all visits on subscribing websites. We
forge trafﬁc data to both and observe the achieved ranks.
1) Extension: The “Alexa Trafﬁc Rank” extension collects
data on all pages that its users visit. The extension also shows
users information on the rank and trafﬁc of the visited site,
which may serve as an incentive to install the extension.
We submitted page visits for both registered and nonexis-
tent test domains previously unseen by Alexa. We generated
proﬁles with all 1 152 possible conﬁgurations, i.e. the demo-
graphic details that are requested when installing the extension,
and this within a short timeframe from the same IP address;
Alexa did not impose any limits on the number of proﬁles
that could be created. We submitted visits to one domain per
proﬁle; as visits to the same page by the same proﬁle are only
counted once [8], we generated exactly one visit per page to
the homepage and randomly generated subpages. The number
of page views for one test domain ranges from 1 to 30.
We installed the extension in a real Chrome browser
instance and then generated page visits to our test domain,
simulating a realistic usage pattern by spacing out page visits
between 30 and 45 seconds, and interspersing them with
as many visits to domains in Alexa’s top 1000. Through
inspection of the extension’s source code and trafﬁc, we found
that upon page load, a GET request with the full URL of
the visited page9 is sent alongside the user’s proﬁle ID and
browser properties to an endpoint on data.alexa.com. This
means these requests can also be generated directly without the
need to use an actual browser, greatly reducing the overhead
in manipulating many domains on a large scale.
9For pages loaded over HTTPS, the path is obfuscated.
Fig. 5. The percentage of ﬁngerprinting script providers that would not be
detected if a given number of domains were pushed above all ﬁngerprinting
domains for different subsets of Alexa’s ranking.
tracking. Moreover, this is an upper bound needed to remove
all instances of a tracker domain from the list; reducing the
prevalence of a script requires only hiding the worst-ranked
domains. Finally, it is not required to insert new domains:
forging a few requests to boost sites already in the list is
sufﬁcient, further reducing the cost and even making the
manipulation harder to detect.
Englehardt and Narayanan highlighted how “the long tail of
ﬁngerprinting scripts are largely unblocked by current privacy
tools,” reinforcing the potential
impact of exposing these
scripts. A malicious party can therefore gain an advantage by
actively manipulating the rankings of popular domains. As we
will show in the next section, such manipulation is actually
feasible across all four lists, usually even on a sufﬁciently large
scale without the need for signiﬁcant resources.
V. FEASIBILITY OF LARGE-SCALE MANIPULATION
The data collection processes of popularity rankings rely
on a limited view of the Internet, either by focusing on one
speciﬁc metric or because they obtain information from a small
population. This implies that targeted small amounts of trafﬁc
can be deemed signiﬁcant on the scale of the entire Internet and
yield good rankings. Moreover, the ranking providers generally
do not ﬁlter out automated or fake trafﬁc, or domains that
do not represent real websites, further reducing the share of
domains with real trafﬁc in their lists.
Consequently, attacks that exploit
these limitations are
especially effective at allowing arbitrary modiﬁcations of the
rankings at a large scale. We showed how adversaries may
have incentives to skew the conclusions of security studies, and
that security researchers and practitioners often use popularity
rankings to drive the evaluation of these studies. Manipulating
these rankings therefore becomes a prime vector for inﬂuenc-
ing security research, and as we will show, the small costs and
low technical requirements associated with this manipulation
make this approach even more attractive.
For each of the four studied popularity rankings, we
describe techniques that manipulate the data collection process
through the injection of forged data. To prove their feasibility,
we execute those techniques that conform to our ethical frame-
work and that have a reasonable cost, and show which ranks
can be achieved. In Table III, we summarize the techniques and
the cost they incur on three aspects: money, effort and time
required. Through this cost assessment, we identify how these
manipulations could be applied at scale and affect a signiﬁcant
portion of these lists.
6
102103104105106Domains to boost0255075100% of providers removedtop 1Mtop 100Ktop 10Ktop 1K(a) Extension.
(b) Certify.
Fig. 6. Ranks obtained in the Alexa list. Ranks on the same day are connected.
From May 10, 2018 onward, Alexa appears to block data
reporting from countries in the European Union (EU) and
European Economic Area (EEA), as the response changed
from the visited site’s rank data shown to the user to the
string “Okay”. This is likely due to the new General Data
Protection Regulation coming into force. While we were able
to circumvent this block through a VPN service, Alexa may
be ignoring trafﬁc in EU and EEA countries, introducing a
further bias towards trafﬁc from other countries.
For 20% of our proﬁles/domains, we were successful in
seeing our page views counted and obtaining rankings within
the top million. Alexa indicates that
it applies statistical
processing to its data [73], and we suspect that some of our
requests and generated proﬁles were pruned or not considered
sufﬁcient to be ranked, either because of the proﬁle’s properties
(e.g. a common browser conﬁguration or an overrepresented
demographic) or because only a subset of trafﬁc data is (ran-
domly) selected. To increase the probability of getting domains
ranked, an adversary can select only the successful proﬁles, or
generate page views to the same site with different proﬁles in
parallel, improving the efﬁciency of their manipulation.
Figure 6(a) lists our 224 successful rankings grouped per
day, showing the relation between ranks and number of visits.
We performed our experiments between July 25 and August
5, 2018. As during this period Alexa averaged trafﬁc over one
day, there was only a delay of one day between our requests
and the domains being ranked; they disappeared again from the
list the following day. This means that it is not necessary to
forge requests over a longer period of time when the malicious
campaign is short-lived.
What is most striking, is the very small number of page
visits needed to obtain a ranking: as little as one request
yielded a rank within the top million, and we achieved a rank
as high as 370 461 with 12 requests (albeit in the week-end,
when the same number of requests yields a better rank). This
means that the cost to manipulate the rankings is minimal,
allowing adversaries to arbitrarily alter the lists at large scale
for an extended period of time. This ensures continued ranking
and increases the likelihood of a list containing manipulated
domains being used for research purposes, despite the large
daily change.
The low number of required requests is further conﬁrmed
by large blocks of alphabetically ordered domains appearing
7
Fig. 7. The estimated relation between requests and rank for Alexa. The gray
areas show data as retrieved from the Alexa Web Information Service.
in the ranking: these point towards the same number of visits
being counted for these domains. We use these blocks as well
as the processed visitor and view metrics retrieved from the
Alexa Web Information Service [13] to estimate the required
visit count for better ranks.
Figure 7 shows the number of requests needed to achieve a
certain rank; we consider this an upper bound as Alexa ranks
domains that see more unique visitors better than those with
more page views, meaning that manipulation with multiple
proﬁles would require less requests. This analysis shows that
even for very good ranks, the amount of requests required and
accompanying cost remains low, e.g. only requiring 1 000 page
views for rank 10 000. This model of Alexa’s page visits also
corresponds with previous observations of Zipf’s law in web
trafﬁc [4], [22].
Alexa’s list is also susceptible to injection of nonexistent
domains; we were able to enter one such domain. Furthermore,
we conﬁrmed in our server logs that none of our test domains
were checked by Alexa as we forged page visit requests. The
ability to use fake domains reduces the cost to manipulate the
list at scale even further: an attacker is not required to actually
purchase domain names and set up websites for them.
Even though Alexa’s statistical postprocessing may prune
some visits, the low number of required visits, the ability to
quickly generate new proﬁles and the lack of ﬁltering of fake
domains allows an attacker to still easily achieve signiﬁcant
manipulation of Alexa’s list.
2) Certify: Alexa’s ‘Certify’ service offers site owners an
analytics platform, using a tracking script installed on the
website to directly measure trafﬁc. The service requires a
subscription to Alexa’s services, which start at USD 19.99 per
month for one website.
As Alexa veriﬁes installation of its scripts before tracking
visits, we installed them on a test website. From the JavaScript
part of this code, we extracted its reporting algorithm and
repeatedly forged GET requests that made us appear as a new
user visiting the website, therefore avoiding the need to retain
the response cookies for continued tracking. To diversify the
set of IP addresses sending this forged trafﬁc, we sent these
requests over the Tor network, which has a pool of around
1 000 IP addresses [69]. We sent at most 16 000 requests per
24 hours, of which half were for the root page of our domain,
and the other half for a randomly generated path.
Figure 6(b) lists the ranks of our test domain and the
number of visits that were logged by Alexa across 52 days.
For 48 days, we reached the top 100 000 (purported to more
14710131619222528Requests350000400000450000500000550000600000650000700000750000Rank0500010000150002000025000Requests050000100000150000200000250000300000Rank1101001000100001000001000000Rank101103105107Estimated requests107.5*r1.125accurately reﬂect popularity), getting up to rank 28 798. Not
all our requests were seen by Alexa, but we suspect this is
rather due to our setup (e.g. by timeouts incurred while sending
requests over Tor). Alexa’s metrics report that our site received
"100.0% real trafﬁc" and that no trafﬁc was excluded, so we
suspect that Alexa was not able to detect the automated nature
of our requests.
After subscription to the service, Alexa will only calculate
(and offer to display) the ‘Certiﬁed’ rank of a website after
21 days. Since no visits to our site were being reported
through Alexa’s extension, no ‘normal’ rank was achieved in
the meantime, and therefore there was a large delay between
the start of the manipulation and the ranking of the domain.
The disadvantage of this technique is that the cost of ma-
nipulation at scale quickly becomes prohibitive, as for each site
that needs to be inserted into the list, a separate subscription
is required. Given Alexa’s veriﬁcation of the tracking script
being installed, the domain needs to be registered and a real
website needs to be set up, further reducing the scalability
of the technique. However, we were able to achieve better
ranks with a more consistent acceptance of our forged requests.
Depending on the attacker’s goal, it is of course still possible
to artiﬁcially increase the ranking of speciﬁc websites who
already purchased and installed the Alexa Certify service.
We obtained a rank even though we did not simulate
trafﬁc to this test domain through the Alexa extension, which
strongly suggests that Alexa does not verify whether ‘Certiﬁed’
domains show similar (credible) trafﬁc in both data sources.
Based on this observation, we found one top 100 ‘Certiﬁed’
site where Alexa reports its extension recording barely any or
even no trafﬁc: while in this case it is a side-effect of its usage
pattern (predominantly mobile), it implies that manipulation
conducted solely through the tracking script is feasible.
B. Cisco Umbrella
Umbrella ranks websites on the number of unique client
IPs issuing DNS requests for them. Obtaining a rank therefore
involves getting access to a large variety of IP addresses and
sending (at least) one DNS request from those IPs to the two
open DNS resolvers provided by Umbrella.
1) Cloud providers: Cloud providers have obtained large
pools of IP addresses for distribution across their server
instances; e.g. Amazon Web Services (AWS) owns over 64
million IPv4 addresses [14]. These can be used to procure the
unique IP addresses required for performing DNS requests, but
due to their scarcity, providers restrict access to IPv4 addresses
either in number or by introducing a cost.
In the case of AWS, there are two options for rapidly
obtaining new IPv4 addresses. Continuously starting and stop-
ping instances is an economical method, as even 10 000
different IPs can be obtained for less than USD 1 (using
the cheapest instance type), but the overhead of relaunching
instances reduces throughput: on the cheapest t2.nano in-
stance, we were able to obtain a new IP on average every
minute. Moreover, the number of concurrent running instances
is limited, but by using instances in multiple regions or even
multiple accounts, more instances are accessible. Keeping one
instance and allocating and deallocating Elastic IP addresses
Fig. 8. Ranks obtained in the Umbrella list. Ranks on the same day are
connected; ranks over two days for one set of requests use the same line
style.
(i.e. addresses permanently assigned to a user) yields higher
throughput, at 10 seconds per IP. However, AWS and other
providers such as Microsoft Azure discourage this practice by
attaching a cost to this ‘remap’ operation: for AWS, a remap
costs USD 0.10, so a set of 10 000 IPs incurs a prohibitive
cost of USD 1 000.
Figure 8 shows the relation between the number of issued
DNS requests and the obtained rank; all of our attempts
were successful. We were able to obtain ranks as high as
200 000 with only a thousand unique IP addresses, albeit in
the weekend, when OpenDNS processes around 30% less DNS
trafﬁc [57]. We only sustained DNS trafﬁc for one day at
a time, but it appears that Umbrella counts this trafﬁc (and
therefore ranks the domain) for two days, reducing the number
of requests needed per day to either obtain a good rank for one
domain or rank many domains.
Given the relatively high cost per IP, inserting multiple
domains actually is more economical as several DNS requests
can be sent for each IP instantiation. As the name requested in
the DNS query can be chosen freely, inserting fake domains
is also possible; the high number of invalid entries already
present shows that Umbrella does not apply any ﬁltering.
This further improves scalability of this technique, as no real