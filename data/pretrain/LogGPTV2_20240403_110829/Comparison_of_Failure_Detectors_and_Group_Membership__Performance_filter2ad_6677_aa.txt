title:Comparison of Failure Detectors and Group Membership: Performance
Study of Two Atomic Broadcast Algorithms
author:P&apos;eter Urb&apos;an and
Ilya Shnayderman and
Andr&apos;e Schiper
Comparison of Failure Detectors and Group Membership:
Performance Study of Two Atomic Broadcast Algorithms
P´eter Urb´an
†
peter.urban@epﬂ.ch
Ilya Shnayderman
PI:EMAIL
‡
Andr´e Schiper
†
andre.schiper@epﬂ.ch
† ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland
School of Engineering and Computer Science, The Hebrew University of Jerusalem, Israel
‡
Abstract
Protocols that solve agreement problems are essential
building blocks for fault tolerant distributed systems. While
many protocols have been published, little has been done to
analyze their performance, especially the performance of
their fault tolerance mechanisms. In this paper, we present
a performance evaluation methodology that can be gener-
alized to analyze many kinds of fault-tolerant algorithms.
We use the methodology to compare two atomic broadcast
algorithms with different fault tolerance mechanisms: un-
reliable failure detectors and group membership. We evalu-
ated the steady state latency in (1) runs with neither crashes
nor suspicions, (2) runs with crashes and (3) runs with no
crashes in which correct processes are wrongly suspected
to have crashed, as well as (4) the transient latency after
a crash. We found that the two algorithms have the same
performance in Scenario 1, and that the group membership
based algorithm has an advantage in terms of performance
and resiliency in Scenario 2, whereas the failure detector
based algorithm offers better performance in the other sce-
narios. We discuss the implications of our results to the
design of fault tolerant distributed systems.
1 Introduction
Agreement problems — such as consensus, atomic broad-
cast or atomic commitment — are essential building blocks
for fault tolerant distributed applications, including trans-
actional and time critical applications. These agreement
problems have been extensively studied in various system
models, and many protocols solving these problems have
been published [1, 2], offering different levels of guaran-
tees. However, these protocols have mostly been analyzed
from the point of view of their safety and liveness prop-
erties, and very little has been done to analyze their per-
formance. Also, most papers focus on analyzing failure
free runs, thus neglecting the performance aspects of failure
handling. In our view, the limited understanding of perfor-
mance aspects, in both failure free scenarios and scenarios
with failure handling, is an obstacle for adopting such pro-
tocols in practice.
Unreliable failure detectors vs. group membership.
In
this paper, we compare two (uniform) atomic broadcast al-
gorithms, the one based on unreliable failure detectors and
the other on a group membership service. Both services
provide processes with estimates about the set of crashed
processes in the system.1 The main difference is that fail-
ure detectors provide inconsistent information about fail-
ures, whereas a group membership service provides con-
sistent information. While several atomic broadcast algo-
rithms based on unreliable failure detectors have been de-
scribed in the literature, to the best of our knowledge, all
existing group communication systems provide an atomic
broadcast algorithm based on group membership (see [3]
for a survey). So indirectly our study compares two classes
of techniques, one widely used in implementations (based
on group membership), and the other (based on failure de-
tectors) not (yet) adopted in practice.
The two algorithms. The algorithm using unreliable fail-
ure detectors is the Chandra-Toueg atomic broadcast algo-
rithm [4], which can tolerate f < n/2 crash failures, and
requires the failure detector ♦S. As for an algorithm us-
ing group membership, we chose an algorithm that imple-
ments total order with a mechanism close to the failure de-
tector based algorithm, i.e., a sequencer based algorithm
(which also tolerates f < n/2 crash failures). Both al-
gorithms were optimized (1) for failure and suspicion free
runs (rather than runs with failures and suspicions), (2) to
minimize latency under low load (rather than minimize the
number of messages), and (3) to tolerate high load (rather
1Beside masking failures, a group membership service has other uses.
This issue is discussed in Section 8.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
than minimize latency at moderate load).
We chose these algorithms because they are well-known
and easily comparable: they offer the same guarantees in
the same model. Moreover, they behave similarly if neither
failures nor failure suspicions occur (in fact, they generate
the same exchange of messages given the same arrival pat-
tern). This allows us to focus our study on the differences
in handling failures and suspicions.
Methodology for performance studies. The two algo-
rithms are evaluated using simulation. We model message
exchange by taking into account contention on the network
and the hosts [5]. We model failure detectors (including the
ones underlying group membership) in an abstract way, us-
ing the quality of service (QoS) metrics proposed by Chen
et al. [6]. Our performance metric for atomic broadcast is
called latency, deﬁned as the time that elapses between the
sending of a message m and the earliest delivery of m. We
study the atomic broadcast algorithms in several benchmark
scenarios, including scenarios with failures and suspicions:
we evaluate the steady state latency in (1) runs with neither
crashes nor suspicions, (2) runs with crashes and (3) runs
with no crashes in which correct processes are wrongly sus-
pected to have crashed, as well as (4) the transient latency
after a crash.
We believe that our methodology can be generalized to
analyze other fault-tolerant algorithms. In fact, beside the
results of the comparison, the contribution of this paper is
the proposed methodology.
The results. The paper shows that the two algorithms have
the same performance in run with neither crashes nor suspi-
cions, and that the group membership based algorithm has
an advantage in terms of performance and resiliency a long
time after crashes occur. In the other scenarios, involving
wrong suspicions of correct processes and the transient be-
havior after crashes, the failure detector based algorithm of-
fers better performance. We discuss the implications of our
results to the design of fault tolerant distributed systems.
The rest of the paper is structured as follows. Section 2
presents related work. Section 3 describes the system model
and atomic broadcast. We introduce the algorithms and
their expected performance in Section 4. Section 5 sum-
marizes the context of our performance study, followed by
our simulation model for the network and the failure detec-
tors in Section 6. Our results are presented in Section 7, and
the paper concludes with a discussion in Section 8.
2 Related work
Most of the time, atomic broadcast algorithms are evalu-
ated using simple metrics like time complexity (number of
communication steps) and message complexity (number of
messages). This gives, however, little information on the
real performance of those algorithms. A few papers provide
a more detailed performance analysis of atomic broadcast
algorithms: [7] and [8] analyze four different algorithms
using discrete event simulation; [5] uses a contention-aware
metric to compare analytically the performance of four al-
gorithms; [9, 10] analyze atomic broadcast protocols for
wireless networks, deriving assumption coverage and other
performance related metrics. However, all these papers ana-
lyze the algorithms only in failure free runs. This only gives
a partial understanding of their quantitative behavior.
Other papers analyze agreement protocols, taking into
account various failure scenarios: [11] presents an approach
for probabilistically verifying a synchronous round-based
consensus protocol; [12] analyzes a Byzantine atomic broad-
cast protocol; [13] evaluates the performability of a group-
oriented multicast protocol; [14] compares the impact of
different implementations of failure detectors on a consen-
sus algorithm (simulation study); [15] analyzes the latency
of the Chandra-Toueg consensus algorithm. Note that [15],
just as this paper, models failure detectors using the quality
of service (QoS) metrics of Chen et al. [6].
3 Deﬁnitions
3.1 System model
We consider a widely accepted system model. It con-
sists of processes that communicate only by exchanging
messages. The system is asynchronous, i.e., we make no
assumptions on its timing behavior:
there are no bounds
on the message transmission delays and the relative pro-
cessing speeds of processes. The network is quasi-reliable:
it does not lose, alter nor duplicate messages (messages
whose sender or recipient crashes might be lost). In prac-
tice, this is easily achieved by retransmitting lost messages.
We consider that processes only fail by crashing. Crashed
processes do not send any further messages. Process crashes
are rare, processes fail independently, and process recovery
is slow: both the time between crashes and time to repair
are much greater than the latency of atomic broadcast.
The atomic broadcast algorithms in this paper (and all
the fault-tolerant algorithms in the literature) use some form
of crash detection. We call the parts of the algorithms that
implement crash detection failure detectors. The failure de-
tector based atomic broadcast algorithm uses failure detec-
tors directly; the group membership based atomic broadcast
algorithm uses them indirectly, through the group member-
ship service. A failure detector maintains a list of processes
it suspects to have crashed. It might make mistakes: it might
suspect correct processes and it might not suspect crashed
processes immediately.2
2To make sure that the atomic broadcast algorithms terminate, we need
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
Note that whereas we assume that process crashes are
rare, (wrong) failure suspicions may occur frequently, de-
pending on the tuning of the failure detectors.
3.2 Atomic broadcast
Atomic Broadcast is deﬁned in terms of two primitives
called A-broadcast(m) and A-deliver(m), where m is some
message.
Informally speaking, atomic broadcast guaran-
tees that (1) if a message is A-broadcast by a correct pro-
cess, then all correct processes eventually A-deliver it, and
(2) correct processes A-deliver messages in the same order
(see [18, 4] for more formal deﬁnitions). Uniform atomic
broadcast ensures these guarantees even for faulty processes.
In this paper, we focus on uniform atomic broadcast.
4 Algorithms
This section introduces the two atomic broadcast algo-
rithms and the group membership algorithm (a more de-
tailed description can be found in [19]). Then we discuss
the expected performance of the two atomic broadcast al-
gorithms.
4.1 Chandra-Toueg uniform atomic broadcast al-
gorithm
The Chandra-Toueg uniform atomic broadcast algorithm
uses failure detectors directly [4]. We shall refer to it as
the FD atomic broadcast algorithm, or simply as the FD
algorithm. A process executes A-broadcast by sending a
message to all processes.3 When a process receives such a
message, it buffers it until the delivery order is decided. The
delivery order is decided by a sequence of consensus num-
bered 1, 2, . . .. The initial value and the decision of each
consensus is a set of message identiﬁers. Let msg(k) be
the set of message IDs decided by consensus #k. The mes-
sages denoted by msg(k) are A-delivered before the mes-
sages denoted by msg(k + 1), and the messages denoted by
msg(k) are A-delivered according to a deterministic func-
tion, e.g., according to the order of their IDs.
Chandra-Toueg ♦S consensus algorithm. For solving
consensus, we use the Chandra-Toueg ♦S algorithm [4].4
The algorithm can tolerate f < n/2 crash failures.
It is
based on the rotating coordinator paradigm: each process
executes a sequence of asynchronous rounds (i.e., not all
some assumptions on the behavior of the failure detectors [16]. These
assumptions are rather weak: they can usually be fulﬁlled in real systems
by tuning implementation parameters of the failure detectors [17].
3This message is sent using reliable broadcast. We use an efﬁcient
algorithm inspired by [20] that uses only one broadcast message in most
cases; see [19] for more details.
4Actually, we included some easy optimizations in the algorithm.
processes necessarily execute the same round at a given
time t), and in each round a process takes the role of co-
ordinator (pi is coordinator for rounds kn + i). The role
of the coordinator is to impose a decision value on all pro-
cesses. If it succeeds, the consensus algorithm terminates.
It may fail if some processes suspect the coordinator to have
crashed (whether the coordinator really crashed or not). In
this case, a new round is started. We skip the details of the
execution, since they are not necessary for understanding
the paper.
Example run of the FD algorithm. Figure 1 illustrates
an execution of the FD atomic broadcast algorithm in which
one single message m is A-broadcast and neither crashes
nor suspicions occur. At ﬁrst, m is sent to all processes.
Upon receipt, the consensus algorithm starts. The coordi-
nator sends its proposal to all other processes. Each process
acknowledges this message. Upon receiving acks from a
majority of processes (including itself), the coordinator de-
cides its own proposal and sends the decision (using reli-
able broadcast) to all other processes. The other processes
decide upon receiving the decision message.
FD alg.
consensus
p1
coordinator / sequencer
A-broadcast(m)
proposal
p2
p3
p4
p5
m
m
t
A-deliver(m)
ack
decision
seqnum
ack
deliver
non-uniform GM alg.
uniform GM alg.
Figure 1. Example run of the atomic broadcast
algorithms. Labels on the top/bottom refer to
the FD/GM algorithm, respectively.
4.2 Fixed sequencer uniform atomic broadcast al-
gorithm
The second uniform atomic broadcast algorithm is based
on a ﬁxed sequencer [21]. It uses a group membership ser-
vice for reconﬁguration in case of a crash. We shall refer to
it as the GM atomic broadcast algorithm, or simply as the
GM algorithm. We describe here the uniform version of the
algorithm.
In the GM algorithm, one of the processes takes the role
of sequencer. When a process A-broadcasts a message m,
it ﬁrst broadcasts it to all. Upon reception, the sequencer (1)
assigns a sequence number to m, and (2) broadcasts the se-
quence number to all. When non-sequencer processes have
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
received m and its sequence number, they send an acknowl-
edgment to the sequencer.5 The sequencer waits for acks
from a majority of processes, then delivers m and sends a
message indicating that m can be A-delivered. The other
processes A-deliver m when they receive this message. The
execution is shown in Fig. 1. Note that the messages de-
noted seqnum, ack and deliver can carry several sequence
numbers. This is essential for achieving good performance
under high load. Note that the FD algorithm has a similar
“aggregation” mechanism: one execution of the consensus
algorithm can decide on the delivery order of several mes-
sages.
When the sequencer crashes, processes need to agree on
the new sequencer. This is why we need a group member-
ship service: it provides a consistent view of the group to
all its members, i.e., a list of the processes which have not
crashed (informally speaking). The sequencer is the ﬁrst
process in the current view. The group membership algo-
rithm described below can tolerate f < n/2 crash failures
(more in some runs) and requires the failure detector ♦S.
4.3 Group membership algorithm
A group membership service [3] maintains the view of a
group, i.e., the list of correct processes of the group. The
current view6 might change because processes in the group
might crash or exclude themselves, and processes outside
the group might join. The group membership service guar-
antees that processes see the same sequence of views (ex-
cept for processes which are excluded from the group; they
miss all views after their exclusion until they join again).
In addition to maintaining the view, our group membership
service ensures View Synchrony and Same View Delivery:
correct and not suspected processes deliver the same set of
messages in each view, and all deliveries of a message m
take place in the same view.
Our group membership algorithm [22] uses failure detec-
tors to start view changes, and relies on consensus to agree
on the next view. This is done as follows. A process that
suspects another process starts a view change by sending a
“view change” message to all members of the current view.
As soon as a process learns about a view change, it sends its
unstable messages7 to all others (all the other messages are
stable, i.e., have been delivered on all processes already).
When a process has received the unstable messages from all
processes it does not suspect, say P , it computes the union
U of the unstable messages received, and starts consensus
5Figure 1 shows that the acknowledgments and subsequent messages
are not needed in the non-uniform version of the algorithm. We come back
to this issue later in the paper.
6There is only one current view, since we consider a non-partitionable
or primary partition group membership service.
7Message m is stable for process p when p knows that m has been
received by all other processes in the current view.
(cid:1)) be the
with the pair (P, U) as its initial value. Let (P
decision of the consensus. Once a process decides, it deliv-
(cid:1)
ers all messages from U