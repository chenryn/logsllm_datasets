3.2.3 Page-based Perceptual Ad-blocking. The core idea of percep-
tual ad-blocking is to emulate the way humans detect ads. Element-
and frame-based approaches embrace this goal to some extent,
but still rely on DOM information that humans are oblivious to.
Recently, Adblock Plus proposed an approach that fully emulates
visual detection of online ads from rendered web content alone [10].
In a page-based ad-blocker, segmentation is integrated into the
classifier. Its core task is best viewed as an object-detection problem:
given a web page screenshot, identify the location and dimension
of ads. Adblock Plus trained the YOLOv3 object-detector [72] on
screenshots of Facebook with ads labeled using standard filter-lists.
Once ad locations are predicted, the ad-blocker can overlay them
to hide ads, or remove the underlying HTML elements (e.g., by
using the document.elementFromPoint browser API to get the
HTML element rendered at some coordinate).
4 ATTACKS ON PERCEPTUAL AD-BLOCKING
Given the unified architecture from Section 3, we now perform
a comprehensive security analysis of the perceptual ad-blocking
pipeline and describe multiple attacks targeting concrete instantia-
tions of each of the ad-blocker’s components. The primary focus of
our analysis is to evaluate the robustness of the ad-blocker’s core
visual classifier, by instantiating adversarial examples for seven
different and varied approaches to ad-detection (Section 4.2). We
further demonstrate powerful attacks that exploit the ad-blocker’s
high-privilege actions (Section 4.3). We conclude by describing more
classical attacks that affect the segmentation step of current per-
ceptual ad-blockers (Section 4.4), as well as potential attacks on an
ad-blocker’s offline data collection and training phase (Section 4.5).
Our attacks can be mounted by different adversaries (e.g., pub-
lishers, ad-networks, or malicious third parties) to evade or detect
ad-blocking and, at times, abuse the ad-blocker’s high privilege level
to bypass web security boundaries. These attacks, summarized in
Table 2, challenge the belief that perceptual signals can tilt the
arms race with publishers and ad-networks in favor of ad-blockers.
All our data, pre-trained models, and attack code are available at
https://github.com/ftramer/ad-versarial.
3214Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2010Table 1: Evaluation of Ad-Classifiers. For each classifier, we first evaluate on “benign” data collected from websites. We report
false-positives (FP)—mis-classified non-ad content—and false negatives (FN)—ad-content that the classifier missed. We then
give the the attack model(s) considered when evading the classifier, the success rate, and the corresponding section.
Category
Element-based
Frame-based
Page-based
Targets
Method
Blacklist
AdChoices logos
Avg. hash [81] AdChoices logos
textual AdChoices
SIFT
textual AdChoices
OCR [81]
YOLOv3
AdChoices in iframe
ResNet [40]
ad in iframe
large ads in iframe
Percival [84]
YOLOv3
ads visible in page screenshot
Benign Eval.
FP
FN
33/41
0/824
3/41
3/824
2/824
0/17
1/17
0/824
0/20
5/29
0/20
21/39
3/33
2/7
2
6/30
Adversarial Eval.
Attack Model for Evasion
N.A.
Add ≤ 3 empty rows/cols
ℓ2 ≤ 1.5
ℓ2 ≤ 2.0
ℓ∞ ≤ 4/255
ℓ∞ ≤ 2/255
ℓ∞ ≤ 2/255
Publisher: universal full-page mask (99% transparency)
Publisher: adv. content below ads on BBC.com, ℓ∞ ≤ 3/255
Ad network: universal mask for ads on BBC.com, ℓ∞ ≤ 4/255
Success
-
100%
100%
100%
100%
100%
100%
100%
100%
95%
(3) Two screenshots per website (the front-page and an article)
taken in Google Chrome on a 1920 × 1080 display.4 These are
used to evaluate page-based models. Each screenshot contains
1 or 2 fully visible ads, with 30 ads in total.
For template-matching approaches (perceptual hashing and SIFT)
we use the same 12 AdChoices templates as Ad-Highlighter [82].
When describing an ad-blocker’s page segmentation and the cor-
responding markup obfuscation attacks in Section 4.4, we use some
data collected on Facebook.com in November 2018. As Facebook
continuously and aggressively adapts the obfuscation techniques
it uses to target ad-blockers [88], the specific attacks we describe
may have changed, which only goes to illustrate the ongoing arms
race and need for more robust markup-less ad-blocking techniques.
4.1.3 Accuracy and Performance of ML Classifiers. Table 1 reports
the accuracy of the seven ad-classifiers on our evaluation data. For
completeness, we include a blacklist that marks any image that ex-
actly matches one of the 12 AdChoices logos used in Ad-Highlighter.
As posited by Storey et al. [81], this approach is insufficient.
Note that the datasets described above are incomparable. Some
ads are not in iframes, or have no ad-disclosure, ans screenshots
only contain images within the current view. Thus, the accuracy
of the classifiers is also incomparable. This does not matter, as our
aim is not to find the best classifier, but to show that all of them
are insecure in the stringent attack model of visual ad-blockers.
Overall, element-based approaches have high accuracy but may
suffer from some false-positives (i.e., non-ad content classified as
ads) that can lead to site-breakage. The frame-based approaches are
less accurate but have no false-positives. Finally, our Sentinel-like
detector shows promising (albeit imperfect) results that demon-
strate the possibility of ad-detection on arbitrary websites.
We measure performance of each classifier on an Intel Core i7-
6700 Skylake Quad-Core 3.40GHz. While average hashing and SIFT
process all images in a page in less than 4 seconds, OCR is much
slower (Ad-Highlighter disables it by default). Our OCR model
parses an image in 100 ms, a 14 second delay on some websites.
4We experimentally verified that our attacks on page-based ad-blockers are robust to
changes in the user’s viewport. An attacker could also explicitly incorporate multiple
browsers and display sizes into its training set to create more robust attacks. Alterna-
tively, the adversary could first detect the type of browser and viewport (properties
that are easily and routinely accessed in JavaScript) and then deploy “responsive”
attacks tailored to the user’s setting.
The frame-based classifiers process all iframes in 1-7 seconds. Our
page-based model processes pages downsized to 416 × 416px at 1.5
frames-per-second (on CPU), which may suffice for ad-blocking.
The authors of Percival recently demonstrated that an optimized de-
ployment of perceptual ad-blocking with a deep learning classifier
incurs only minimal overhead on page rendering (< 200 ms).
4.2 Attacks against Classification with
Adversarial Examples
For perceptual ad-blockers that operate over images (whether on
segmented elements as in Ad-Highlighter [82], or rendered content
as in Sentinel [10] or Percival [84]), security is contingent on the
robustness of the ad-blocker’s visual classifier. False negatives result
in ads being shown, and false positives cause non-ads to be blocked.
Both error types are exploitable using adversarial examples [33,
83]—small input perturbations that fool a classifier. Adversarial
examples can be used to generate web content that fools the ad-
blocker’s classifier, without affecting a user’s browsing experience.
In this section, we describe and evaluate four concrete types of
attacks on the seven visual classifiers we consider: (C1) adversarial
ad-disclosures that evade detection; (C2) adversarial ads that evade
detection; (C3) adversarial non-ad content that alters the classifier’s
output on nearby ads; (C4) adversarial honeypots (misclassified non-
ad elements, to detect ad-blocking). Our attacks allow adversaries
to evade or detect ad-blocking with (near)-100% probability.
Attack Model. We consider adversaries that perturb web content
to produce false-negatives (to evade ad-blocking) or false-positives
(honeypots to detect ad-blocking). Each attack targets a single
classifier—but is easily extended to multiple models (see Section 5).
• False negative. To evade ad-blocking, publishers, ad networks or
advertisers can perturb any web content they control, but aim
to make their attacks imperceptible. We consider perturbations
with small ℓ2 or ℓ∞ norm (for images with pixels normalized to
[0,1])—a sufficient condition for imperceptibility. An exception
to the above are our attacks on average hashing, which is by
design invariant to small ℓp changes but highly vulnerable to
other imperceptible variations. The attack model used for all
evasion attacks are summarized in Table 1.
• False positive. The space of non-disruptive false positive attacks
is vast. We focus on one easy-to-deploy attack, that generates
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2011r
o
t
a
e
r
c
t
n
e
t
n
o
C
k
r
o
w
t
e
N
d
A
r
e
s
i
t
r
e
v
d
A
r
e
h
s
i
l
b
u
P
d
e
s
a
b
-
t
n
e
m
e
l
E
d
e
s
a
b
-
e
m
a
r
F
d
e
s
a
b
-
e
g
a
P
n
o
i
t
c
e
t
e
D
n
o
i
s
a
v
E
e
s
u
b
A
Table 2: Attack Strategies on Perceptual Ad-Blockers. Strate-
gies are grouped by the component that they exploit—
(D)ata collection, (S)egmentation, (C)lassification, (A)ction.
For each strategy, we specify which goals it can achieve,