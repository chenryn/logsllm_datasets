We study the model performance of BagNet-17 against a 2%
6We note that “% images" presented in Table 7 is an upper bound for our
robust masking in Algorithm 1 because robust masking masks the window
with the highest class evidence for each class while this analysis only removed
wrong class evidence within the same window as the true class.
2246    30th USENIX Security Symposium
USENIX Association
Table 10: Effect of feature types on Mask-BN-17
Table 12: Effect of over-conservatively large masks on Mask-BN-17
Patch size
Accuracy
Logits
Conﬁdence
Prediction
1% pixels
2% pixels
3% pixels
clean
95.2%
87.9%
85.7%
robust
89.0%
80.5%
77.3%
clean
95.0%
87.9%
85.8%
robust
86.7%
77.9%
74.1%
clean
94.8%
88.0%
85.9%
robust
83.0%
74.4%
70.3%
Table 11: Effect of feature types on Mask-DS
Patch size
Accuracy
Logits
Conﬁdence
Prediction
1% pixels
2% pixels
3% pixels
clean
92.4%
92.3%
91.9%
robust
76.9%
83.1%
82.5%
clean
92.1%
92.1%
91.8%
robust
68.9%
79.9%
79.4%
clean
91.9%
92.1%
91.7%
robust
61.6%
76.8%
76.4%
patch
mask
1% pixels
2% pixels
3% pixels
4.5% pixels
CBN [59]
DS [28]
clean
1% pixels
2% pixels
3% pixels
95.2%
95.0%
94.8%
94.6%
94.9%
92.1%
89.0%
88.2%
87.1%
86.0%
74.6%
82.3%
–
86.7%
85.3%
84.1%
60.9%
79.1%
–
–
83.0%
81.8%
45.9%
75.7%
Table 13: Per-image inference time of different models
Model
ResNet-50
BagNet-17
DS-25-ResNet Mask-BN Mask-DS
Time
11.8ms
12.1ms
387.9ms
16.6ms
404.4ms
pixel patch as we change the detection threshold T from 0.0
to 1.0. A threshold of zero means our detection will always
return a suspicious window even if the input is a clean im-
age while a threshold of one means no detection at all. We
report the clean accuracy, provable robust accuracy, and false
positive (FP) rates for detection of suspicious windows on
clean images in Table 9. As we increase the detection thresh-
old T , we reduce the FP rate for clean images, at the cost of
making it easier for an adversarial patch to succeed via Case
IV (no suspicious window detected). However, we note that
false positives in the detection phase for clean images have
a minimal impact on the clean accuracy because our models
are generally invariant to feature masking, as already shown
in Table 7. Thus, we ﬁnd T = 0 to be the best choice for this
dataset (even with an FP of 100%); it results in the highest
provable robust accuracy of 86.7% while only incurring a
0.5% clean accuracy drop compared to T = 1.
Different feature types greatly inﬂuence the performance
of defended models. In this analysis, we study the perfor-
mance of the robust masking defense when using different
types of features, namely logits, conﬁdence values, and predic-
tions. The results for Mask-BN-17 with different features are
reported in Table 10. As shown in the table, using logits as the
feature type has much better performance than conﬁdence and
prediction in terms of clean accuracy and provable accuracy.
The main reason for this observation is that BagNet is trained
with logits aggregation. Our additional analysis shows that
BagNet does not have high model performance when trained
with conﬁdence or prediction aggregation; therefore, we use
logits as our default feature type for Mask-BN. Interestingly,
Mask-DS exhibits a different behavior. As shown in Table 11,
Mask-DS works better when we use prediction or conﬁdence
as feature types due to its different training objectives. In
conclusion, the performance of different feature types largely
depends on the training objective of the network with small
receptive ﬁelds, and should be appropriately optimized to
determine the best defense setting.
Over-conservatively large masks only have a small im-
pact on defended models. PatchGuard’s robust masking is
deployed in a manner that is agnostic to the patch size by
selecting a large mask window size that matches the upper
bound of the patch size. In this analysis, we study the model
performance when an over-conservatively large mask is used.
Note that the provable robustness obtained with a larger mask
for a larger patch can be directly applied to a smaller patch
(e.g., an image that is robust against a 3% pixel patch is also
robust against a 1% pixel patch). However, we can certify
the robustness for more images when the actual patch size is
smaller than the mask size (Appendix C).
We report the provable robust accuracy and clean accu-
racy of Mask-BN-17 with different patch sizes and attack-
agnostic mask sizes in Table 12. First, robust masking with
a larger mask can have a tighter provable robustness bound
for a smaller patch. For example, when using a 3% pixel
mask, the provable analysis in Algorithm 2 can only certify
the robustness of 83.0% of test images for any patch size
smaller than 3%. In contrast, the tighter provable analysis
from Appendix C leads to a provable robust accuracy of
87.1% (4.1% improvement) for a 1% pixel patch. Second,
over-conservatively using a larger mask size only leads to a
slight drop in clean accuracy and provable robust accuracy.
As we increase the mask size, the clean accuracy for 1% pixel
patch only drops from 95.2% to 94.6% and the provable ro-
bust accuracy drops from 89.0% to 86.0%. We note that even
when the mismatch is large (a 4.5% pixel mask for a 1% pixel
patch), our defense still outperforms DS [28].
5.3.3 Defense Efﬁciency
Robust masking only introduces a small defense over-
head. In Table 13, we report the per-image inference time of
different models on the ImageNette validation set. As shown
in the table, the inference time of Mask-BN (16.6ms) is close
to that of BagNet-17 (12.1ms). We have a similar observation
for Mask-DS (404.4ms) and DS-25-ResNet (387.9ms).
BagNet-like models (e.g., Mask-BN) are more efﬁcient
than DS-like models (e.g., DS and Mask-DS). As discussed
in Section 3.3, using an ensemble model (e.g., DS-ResNet) is
USENIX Association
30th USENIX Security Symposium    2247
computationally expensive compared with using small convo-
lution kernels in conventional CNNs (e.g., BagNet). From Ta-
ble 13, we can see the inference time of BagNet-17 (12.1ms)
much smaller than that of DS-25-ResNet (387.9ms). This
difference leads to a huge efﬁciency gap between Mask-BN
(16.6ms) and Mask-DS (404.4ms) as well as DS (387.9ms).
Therefore, we suggest using small convolution kernels to build
models with small receptive ﬁelds when the two approaches
have similar defense performance.
6 Discussion
6.1 Generalization of Related Defenses
In this subsection, we will show that our defense framework
is a generalization of other provably robust defenses such as
Clipped BagNet [59], De-randomized Smoothing [28].
Clipped BagNet (CBN). CBN [59] proposes clipping the
local logits tensor with function CLIP(u) = tanh(0.05·u− 1)
to improve the robustness of BagNet [5]. Since the range
of tanh(·) is bounded by (−1,1), the adversary can achieve
at most 2k difference in clipped logits values between the
true class and any other class, where k is the number of cor-
rupted local logits due to the adversarial patch. In its provable
analysis, CBN calculates the difference between the sum of
unaffected logits values for the predicted class and the second
predicted class as δ; if δ > 2k, CBN certiﬁes the robustness
of the input clean image. To reduce our Mask-BN defense
to CBN, we can set our feature type to logits, the detection
threshold to T = 1 (i.e., no detection), and adjust the clip-
ping values cl and ch or the clipping function CLIP(·). Our
evaluation shows that our defense signiﬁcantly outperforms
CBN across three different datasets. There are two major
reasons for this performance difference: 1) CBN retains the
malicious feature values while PatchGuard detects and masks
them; 2) CBN uses conventional training while PatchGuard
uses provable adversarial training (Appendix A).
De-randomized Smoothing (DS). DS [28]
trains a
‘smoothed’ classiﬁer on image pixel patches and computes the
predicted class as the class with the majority vote among local
predictions made from all pixel patches. The provable robust-
ness analysis of DS only considers the largest and second-
largest counts of local predictions. If the gap between the two
largest counts is larger than 2k, where k is the upper bound of
the number of corrupted predictions, DS certiﬁes the robust-
ness of the image. When we set the feature type to prediction
and detection threshold to T = 1 (i.e., no detection), we can
reduce Mask-DS to DS. Note that averaging all one-hot en-
coded local predictions gives the same global prediction as
majority voting. The major cause of the relatively poor per-
formance of DS is that its certiﬁcation process discards the
spatial information of each prediction while our robust mask-
ing defense utilizes the spatial information that all corrupted
features are within a small window in the feature space.
We note that two defenses (BagCert [37] and Randomized
Cropping [29]) appeared after the initial release of our paper
preprint [55]; both of them can be regarded as instances of our
PatchGuard framework, i.e., using CNNs with small receptive
ﬁelds (modiﬁed BagNet [37]; image cropping [29]) and secure
aggregation (majority voting [29, 37]). These two followup
works further demonstrate the generality of PatchGuard.
6.2 Limitations and Future Work
While PatchGuard achieves state-of-the-art provable robust-
ness and has higher or comparable clean accuracy compared
with previous defenses, there is still a drop in clean accuracy
compared with undefended models. We note that PatchGuard
is compatible with any small-receptive-ﬁeld CNN and secure
aggregation mechanism, and we expect the trade-off between
provable robustness and clean accuracy to be mitigated further
given any progress in these two directions.
CNNs with small receptive ﬁelds. The use of small receptive
ﬁelds provides substantial provable robustness but incurs a
non-negligible clean accuracy drop for the two architectures
(i.e., BagNet [5] and DS-ResNet [28]) used in this paper.
In future work, we aim to explore better architectures and
training methods for CNNs with small receptive ﬁelds in order
to provide robustness against patch attacks while maintaining
state-of-the-art clean accuracy. Any progress on this front will
directly boost our defense performance since PatchGuard is
compatible with any CNN with small receptive ﬁelds.
Secure feature aggregation. We present robust masking to
compute robust predictions from partially corrupted features.
Robust masking works in a manner that is agnostic to the
patch size by using a large mask, but a completely parameter-
free defense may be more desirable. To this end, we observe
that PatchGuard turns the problem of designing an adversarial
patch defense into a robust aggregation problem, i.e., how
can we make a robust prediction from a partially corrupted
feature tensor? Thus, techniques from robust statistics such
as median, truncated mean, as well as differential privacy [14]
can also be incorporated in our framework, some of which
admit a parameter-free defense. We also plan to explore the
design of custom secure aggregation mechanisms in future
work that can further improve provable robustness.
7 Related Work
7.1 Localized Adversarial Perturbations
Most adversarial example research focuses on global Lp-norm
bounded perturbations while localized adversaries have re-
ceived much less attention. The adversarial patch attack was
introduced by Brown et al. [6] and focused on physical and
universal patches to induce targeted misclassiﬁcation. Attacks
in the real-world can be realized by attaching a patch to the
victim object. A follow-up paper on Localized and Visible
2248    30th USENIX Security Symposium
USENIX Association
Adversarial Noise (LaVAN) attack [22] aimed at inducing
targeted misclassiﬁcation in the digital domain.
Localized patch attacks against object detection [30, 51],
semantic segmentation models [46] as well as training-time
poisoning attacks using localized triggers [19, 31] have been
proposed. Our threat model in this paper focuses on attacks
against image classiﬁcation models at test time; how to gener-