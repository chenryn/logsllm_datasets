### Model Performance of BagNet-17 Against 2% Adversarial Patch

In this study, we evaluate the performance of BagNet-17 against a 2% adversarial patch. We note that the "images %" presented in Table 7 represents an upper bound for our robust masking in Algorithm 1. This is because robust masking targets the window with the highest class evidence for each class, while the current analysis only removes wrong class evidence within the same window as the true class.

### Effect of Feature Types on Mask-BN-17 and Mask-DS

#### Table 10: Effect of Feature Types on Mask-BN-17

| Patch Size | Accuracy | Logits | Confidence | Prediction |
|------------|----------|--------|------------|------------|
| 1% pixels  | Clean    | 95.2%  | 87.9%      | 85.7%      |
|            | Robust   | 89.0%  | 80.5%      | 77.3%      |
| 2% pixels  | Clean    | 95.0%  | 87.9%      | 85.8%      |
|            | Robust   | 86.7%  | 77.9%      | 74.1%      |
| 3% pixels  | Clean    | 94.8%  | 88.0%      | 85.9%      |
|            | Robust   | 83.0%  | 74.4%      | 70.3%      |

#### Table 11: Effect of Feature Types on Mask-DS

| Patch Size | Accuracy | Logits | Confidence | Prediction |
|------------|----------|--------|------------|------------|
| 1% pixels  | Clean    | 92.4%  | 92.3%      | 91.9%      |
|            | Robust   | 76.9%  | 83.1%      | 82.5%      |
| 2% pixels  | Clean    | 92.1%  | 92.1%      | 91.8%      |
|            | Robust   | 68.9%  | 79.9%      | 79.4%      |
| 3% pixels  | Clean    | 91.9%  | 92.1%      | 91.7%      |
|            | Robust   | 61.6%  | 76.8%      | 76.4%      |

### Impact of Over-Conservatively Large Masks on Mask-BN-17

#### Table 12: Effect of Over-Conservatively Large Masks on Mask-BN-17

| Patch Size | Accuracy | Logits | Confidence | Prediction |
|------------|----------|--------|------------|------------|
| 1% pixels  | Clean    | 95.2%  | 95.0%      | 94.8%      |
|            | Robust   | 89.0%  | 88.2%      | 87.1%      |
| 2% pixels  | Clean    | 94.6%  | 94.9%      | 92.1%      |
|            | Robust   | 86.0%  | 85.3%      | 84.1%      |
| 3% pixels  | Clean    | 94.6%  | 94.9%      | 92.1%      |
|            | Robust   | 74.6%  | 82.3%      | 82.3%      |

### Per-Image Inference Time of Different Models

#### Table 13: Per-Image Inference Time of Different Models

| Model                | Time (ms) |
|----------------------|-----------|
| ResNet-50            | 11.8      |
| BagNet-17            | 12.1      |
| DS-25-ResNet         | 387.9     |
| Mask-BN              | 16.6      |
| Mask-DS              | 404.4     |

### Analysis of Detection Threshold T

We analyze the model's performance by varying the detection threshold \( T \) from 0.0 to 1.0. A threshold of zero means the detection will always return a suspicious window, even if the input is a clean image. A threshold of one means no detection at all. The clean accuracy, provable robust accuracy, and false positive (FP) rates for detecting suspicious windows on clean images are reported in Table 9. Increasing the detection threshold \( T \) reduces the FP rate for clean images but makes it easier for an adversarial patch to succeed via Case IV (no suspicious window detected). However, false positives in the detection phase have a minimal impact on clean accuracy because our models are generally invariant to feature masking, as shown in Table 7. Thus, \( T = 0 \) is the best choice for this dataset, resulting in the highest provable robust accuracy of 86.7% with only a 0.5% drop in clean accuracy compared to \( T = 1 \).

### Influence of Feature Types on Defended Models

Different feature types significantly influence the performance of defended models. We study the performance of robust masking when using different types of features: logits, confidence values, and predictions. The results for Mask-BN-17 with different features are reported in Table 10. Using logits as the feature type yields better performance in terms of clean and provable accuracy. This is because BagNet is trained with logits aggregation. Additional analysis shows that BagNet performs poorly when trained with confidence or prediction aggregation. Therefore, we use logits as the default feature type for Mask-BN. Interestingly, Mask-DS exhibits different behavior, working better with prediction or confidence as feature types due to its different training objectives. In conclusion, the performance of different feature types depends on the network's training objective and should be optimized to determine the best defense setting.

### Impact of Over-Conservatively Large Masks

Over-conservatively large masks have a small impact on defended models. PatchGuard's robust masking is deployed in a manner that is agnostic to the patch size by selecting a large mask window size that matches the upper bound of the patch size. We study the model performance with over-conservatively large masks. Provable robustness obtained with a larger mask for a larger patch can be directly applied to a smaller patch. However, more images can be certified as robust when the actual patch size is smaller than the mask size (Appendix C). The provable robust accuracy and clean accuracy of Mask-BN-17 with different patch sizes and attack-agnostic mask sizes are reported in Table 12. Using a larger mask can provide a tighter provable robustness bound for smaller patches. For example, a 3% pixel mask can certify the robustness of 83.0% of test images for any patch size smaller than 3%. Over-conservatively using a larger mask size only leads to a slight drop in clean and provable robust accuracy. Even with a large mismatch (a 4.5% pixel mask for a 1% pixel patch), our defense outperforms DS [28].

### Defense Efficiency

Robust masking introduces a small defense overhead. Table 13 reports the per-image inference time of different models on the ImageNette validation set. The inference time of Mask-BN (16.6ms) is close to that of BagNet-17 (12.1ms). Similarly, the inference time of Mask-DS (404.4ms) is close to that of DS-25-ResNet (387.9ms). BagNet-like models (e.g., Mask-BN) are more efficient than DS-like models (e.g., DS and Mask-DS). Using small convolution kernels in conventional CNNs (e.g., BagNet) is computationally less expensive than using ensemble models (e.g., DS-ResNet). Therefore, we suggest using small convolution kernels to build models with small receptive fields when the two approaches have similar defense performance.

### Discussion

#### Generalization of Related Defenses

Our defense framework generalizes other provably robust defenses such as Clipped BagNet (CBN) and De-randomized Smoothing (DS).

**Clipped BagNet (CBN)**: CBN proposes clipping the local logits tensor to improve BagNet's robustness. By setting our feature type to logits, the detection threshold to \( T = 1 \) (i.e., no detection), and adjusting the clipping values, our evaluation shows that our defense outperforms CBN across three datasets. This is because CBN retains malicious feature values, while PatchGuard detects and masks them, and uses provable adversarial training.

**De-randomized Smoothing (DS)**: DS trains a 'smoothed' classifier on image pixel patches and computes the predicted class as the majority vote among local predictions. Setting the feature type to prediction and detection threshold to \( T = 1 \) (i.e., no detection) reduces Mask-DS to DS. DS discards spatial information, while our robust masking defense utilizes it.

Two recent defenses, BagCert [37] and Randomized Cropping [29], can be regarded as instances of our PatchGuard framework, further demonstrating its generality.

#### Limitations and Future Work

While PatchGuard achieves state-of-the-art provable robustness and comparable clean accuracy, there is still a drop in clean accuracy compared to undefended models. PatchGuard is compatible with any small-receptive-field CNN and secure aggregation mechanism. Future work aims to explore better architectures and training methods for CNNs with small receptive fields to maintain state-of-the-art clean accuracy while providing robustness against patch attacks. Additionally, we plan to explore custom secure aggregation mechanisms to further improve provable robustness.

### Related Work

#### Localized Adversarial Perturbations

Most adversarial example research focuses on global Lp-norm bounded perturbations, while localized adversaries have received less attention. The adversarial patch attack, introduced by Brown et al. [6], focused on physical and universal patches to induce targeted misclassification. Follow-up work on Localized and Visible Adversarial Noise (LaVAN) aimed at inducing targeted misclassification in the digital domain. Localized patch attacks against object detection, semantic segmentation models, and training-time poisoning attacks using localized triggers have been proposed. Our threat model focuses on attacks against image classification models at test time.