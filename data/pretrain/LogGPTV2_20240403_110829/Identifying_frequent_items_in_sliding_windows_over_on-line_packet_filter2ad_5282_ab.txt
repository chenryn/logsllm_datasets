We assume that a single Basic Window ﬁts in main memory,
within which we may count item frequencies exactly. Let δi
be the frequency of the kth most frequent item in the ith
Basic Window. Then δ = i δi is the upper limit on the
frequency of an item type that does not appear on any of
the top-k lists. Now, we sum the reported frequencies for
each item present in at least one top-k synopsis and if there
exists a category whose reported frequency exceeds δ, we are
certain that this category has a true frequency of at least
δ. The pseudocode is given below, assuming that N is the
sliding window size, b is the number of elements per Basic
Window, and N/b is the total number of Basic Windows.
An updated answer is generated whenever the window slides
forward by b packets.
Algorithm Frequent
Repeat:
1. For each element e in the next b elements:
If a local counter exists for the type of element e:
Increment the local counter.
Otherwise:
Create a new local counter for this element type
and set it equal to 1.
2. Add a summary S containing identities and counts
of the k most frequent items to the back of queue Q.
3. Delete all local counters.
4. For each type named in S:
If a global counter exists for this type:
Add to it the count recorded in S.
Otherwise:
Create a new global counter for this element type
and set it equal to the count recorded in S.
5. Add the count of the kth largest type in S to δ.
6. If sizeOf (Q) > N/b:
(a) Remove the summary S(cid:1)
subtract the count of the kth largest type in S(cid:1)
(b) For all element types named in S(cid:1)
from the front of Q and
:
from δ.
Subtract from their global counters the counts
recorded in S(cid:1)
If a counter is decremented to zero:
.
Delete it.
(c) Output the identity and value of each global
counter > δ.
3.3 Analysis
4. EXPERIMENTAL RESULTS
Note that as shown above, algorithm Frequent assumes
that all Basic Windows have the same number of items, as
is usually the case in count-based windows. This assump-
tion, however,
is not necessary to ensure the algorithm’s
correctness—we may replace line 6 with another condition
for emptying the queue, say every t time units. Therefore,
algorithm Frequent may be used with time-based windows
(that may have uniformly sized Basic Windows in terms of
time, but not necessarily in terms of tuple count) without
any modiﬁcations. In the remainder of this section, we will
maintain the assumption of equal item counts in Basic Win-
dows to simplify the analysis.
Algorithm Frequent accepts three parameters: N , b,
and k1. The choice of b is governed by the latency require-
ments of the application: choosing a small value of b in-
creases the frequency with which new results are generated.
However, the amount of available memory dictates the max-
imum number of Basic Windows and the synopsis size k.
The space requirement of algorithm Frequent consists
of two parts: the working space needed to create a sum-
mary for the current Basic Window and the storage space
needed for the top-k synopses. Let d be the number of dis-
tinct item types in a Basic Window (the value of d may be
diﬀerent for each Basic Window, but we ignore this point
to simplify the analysis) and D be the number of distinct
values in the sliding window. In the worst case, the work-
ing space requires d local counters of size log b. For storage,
there are N/b summaries, each requiring k counters of size
at most log b. There are also at most kN/b global counters
of size at most log N . This gives a total worst-case space
bound of O(d log b + kN
b (log b + log N )). The time complex-
ity of algorithm Frequent is O(min(k, b) + b) for each pass
through the outer loop. Since each pass consumes b arriving
elements, this gives O(1) amortized time per element.
Algorithm Frequent may return false negatives. Con-
sider an item that appears on only a few top-k lists, but
summing up its frequency from these top-k lists does not
exceed δ—however, this item may be suﬃciently frequent
in other Basic Windows (but not frequent enough to regis-
ter on the top-k lists of these other windows) that its true
frequency count exceeds δ. The obvious solution for reduc-
ing the number of false negatives is to increase k, but this
also increases space usage. Alternatively, decreasing b in-
creases the number of Basic Windows, which may also help
eliminate false negatives.
Another possible downside of algorithm Frequent is that
if k is small, then δ may be very large and the algorithm
will not report any frequent ﬂows. On the other hand, if k
is large and each synopsis contains items of a diﬀerent type
(i.e. there are very few repeated top-k “winners”), the al-
gorithm may require a great deal of storage space, perhaps
as much as the size of the sliding window. Notably, when
b is only slightly larger than k, there may be fewer than
k distinct items in any Basic Window. In this case, algo-
rithm Frequent will track the exact frequencies of most (if
not all) of the distinct packet types, essentially producing
a compressed representation of the sliding window (or more
precisely, the jumping window) that stores item frequencies
in each Basic Window.
1Note that N and b are speciﬁed in units of time in time-
based windows
4.1 Experimental Setup
We have tested algorithm Frequent on count-based win-
dows over TCP traﬃc data obtained from the Internet Traf-
ﬁc Archive (ita.ee.lbl.gov). We used a trace that contains
all wide-area TCP connections between the Lawrence Berke-
ley Laboratory and the rest of the world between September
16, 1993 and October 15, 1993 [13]. The trace contains 1647
distinct source IP addresses, which we treat as distinct item
types. We set N = 100000 and experiment with three values
of b: b = 20 (5000 Basic Windows in total), b = 100 (1000
Basic Windows in total), and b = 500 (200 Basic Windows in
total). The size of the synopses, k, is varied from one to ten.
In each experiment, we randomly choose one hundred start-
ing points for sliding windows within the trace and execute
our algorithm over those windows. We also run a brute-force
algorithm to calculate the true item type frequencies. We
have measured the average threshold δ, the average number
of over-threshold ﬂows reported, accuracy, and space usage
over one hundred trials, as shown in Figure 1.
4.2 Accuracy
Recall that algorithm Frequent identiﬁes a category as
being over the threshold δ if this category’s frequency count
recorded in the top-k synopses exceeds δ. As k increases, the
frequency of the kth most frequent item decreases and the
overall threshold δ decreases, as seen in Figure 1 (a). Fur-
thermore, increasing the number of synopses by decreasing
b increases δ as smaller Basic Windows capture burstiness
on a ﬁner scale. Consequently, as k increases, the number
of packet types that exceed the threshold increases, as seen
in Figures 1 (b) and (c). The former plots the number of
over-threshold IP addresses, while the latter shows the num-
ber of IP addresses that were identiﬁed by our algorithm as
being over the threshold. For example, when k = 5, the
threshold frequency is roughly ﬁve percent (Figure 1 (a))
and there are between three and four source IP addresses
whose frequencies exceed this threshold (Figure 1 (b)).
It can be seen in Figures 1 (b) and (c) that algorithm Fre-
quent does not identify all the packet types that exceed the
threshold (there may be false negatives, but recall that there
are never any false positives). In Figure 1 (d), we show the
percentage of over-threshold IP addresses that were identi-
ﬁed by algorithm Frequent. The general trend is that for
k ≥ 3, at least 80% of the over-threshold IP addresses are
identiﬁed. Increasing the number of Basic Windows (i.e. de-
creasing the Basic Window size b) also improves the chances
of identifying all of the above-threshold packet types. For
instance, if k > 7 and b = 20, false negatives occur very
rarely.
4.3 Space Usage
Figure 1 (e) shows the space usage of algorithm Fre-
quent in terms of the number of attribute-value, frequency-
count pairs that need to be stored. Recall that the sliding
window size in our experiments is 100000, which may be con-
sidered as a rough estimate for the space usage of a naive
technique that stores the entire window. The space usage
of our algorithm is signiﬁcantly smaller, especially when b is
large and/or k is small. Because a top-k synopsis must be
stored for each Basic Window, the number of Basic Windows
has the greatest eﬀect on the space requirements.
(a)
(b)
(c)
(d)
(e)
(f)
Figure 1: Analysis of frequent-item reporting capabilities, accuracy, and space usage of algorithm Frequent.
Part (a) shows the average value of the threshold δ as a function of k, part (b) shows the number of packet
types whose frequencies exceed the threshold as a function of k, and part (c) graphs the number of packet
types reported by our algorithm as exceeding the threshold as a function of k. Furthermore, part (d) shows
the percentage of over-threshold packets that were identiﬁed as a function of k, part (e) plots the space usage
as a function of k, and part (f ) shows the relative error in the frequency estimates of over-threshold items
returned by our algorithm.
4.4 Precision
6. REFERENCES
Recall from Figure 1 (d) that algorithm Frequent may
report false negatives. We have discovered that unreported
frequent types typically have frequencies that only slightly
exceed the threshold, meaning that the most frequent types
are always reported. Furthermore, the reported frequency
estimates were in many cases very close to the actual fre-
quencies, meaning that the reported frequent IP addresses
were arranged in the correct order (though item types with
similar frequencies were often ordered incorrectly). To quan-
tify this statement, we have plotted in Figure 1 (f) the aver-
age relative error (i.e. the diﬀerence between the measured
frequency and the actual frequency divided by the actual
frequency) in the frequency estimation of the over-threshold
IP addresses for ten values of k and three values of b. The
relative error decreases as k increases and as b decreases. For
example, when b = 20 and k ≥ 7, the average relative error
is below two percent. Therefore, the reported IP addresses
are nearly always ordered correctly, unless there are two IP
addresses with frequencies within two percent of each other,
and only those IP addresses which exceed the threshold by
less than two percent may remain unreported.
4.5 Lessons Learned
Algorithm Frequent works well as an identiﬁer of fre-
quent items and, to some extent, their approximate frequen-
cies, when used on Internet traﬃc streams. As expected,
increasing the size of the top-k synopses increases the num-
ber of frequent ﬂows reported, decreases the number of false
negatives, and improves the accuracy of the frequency esti-
mates. Increasing the number of Basic Windows reduces the
refresh delay, decreases the proportion of false negatives and
increases the accuracy of the frequency estimates. However,
space usage grows signiﬁcantly when either k increases or b
decreases.
5. CONCLUSIONS
We presented an algorithm for detecting frequent items in
sliding windows deﬁned over packet streams. Our algorithm
uses limited memory (less than the size of the window) and
works in the jumping window model. It performs well with
bursty TCP/IP streams containing a small set of popular
item types.
Future work includes theoretical analysis of algorithm Fre-
quent in order to provide bounds on the probability of
false negatives and the relative error in frequency estima-
tion, given a ﬁxed amount of memory and the allowed an-
swer reporting latency. For instance, if the underlying data
conform to a power law distribution, we suspect a correla-
tion between k (the size of the synopses required to guar-
antee some error bound) and the power law coeﬃcient. An-
other possible improvement concerns translating our results
to the gradually sliding window model, where query results
are refreshed upon arrival of each new packet. This may be
done either by bounding the error in our algorithm due to
under-counting the newest Basic Window and over-counting
the oldest Basic Window that has partially expired, or per-
haps by exploiting the Exponential Histogram approach and
its recent extensions in order to extract frequently occur-
ring values. Finally, this work may also be considered as a
ﬁrst step towards solving the more general problem of re-
constructing a probability distribution of a random variable
given only an indication of its extreme-case behaviour.
[1] B. Babcock, S. Babu, M. Datar, R. Motwani, and
J. Widom. Models and issues in data streams. Proc.
21st ACM SIGACT-SIGMOD-SIGART Symp.
Principles of Database Systems, pages 1–16, 2002.
[2] M. Charikar, K. Chen, and M. Farach-Colton. Finding
frequent items in data streams. Proc. 29th Int.
Colloquium on Automata, Languages and
Programming, pages 693–703, 2002.
[3] G. Cormode and S. Muthukrishnan. What’s hot and
what’s not: Tracking most frequent items dynamically.
Proc. 22nd ACM SIGACT-SIGMOD-SIGART Symp.
Principles of Database Systems, pages 296–306, 2003.
[4] C. Cranor, Y. Gao, T. Johnson, V. Shkapenyuk, and
O. Spatscheck. Gigascope: High performance network
monitoring with an sql interface. Proc. ACM SIGMOD
Int. Conf. on Management of Data, page 623, 2002.
[5] M. Datar, A. Gionis, P. Indyk, and R. Motwani.
Maintaining stream statistics over sliding windows.
Proc. 13th SIAM-ACM Symp. on Discrete Algorithms,
pages 635–644, 2002.
[6] E. Demaine, A. Lopez-Ortiz, and J. I. Munro.
Frequency estimation of internet packet streams with
limited space. Proc. European Symposium on
Algorithms, pages 348–360, 2002.
[7] C. Estan and G. Varghese. New directions in traﬃc
measurement and accounting. Proc. ACM SIGCOMM
Internet Measurement Workshop, pages 75–80, 2001.
[8] M. Fang, N. Shivakumar, H. Garcia-Molina,
R. Motwani, and J. Ullman. Computing iceberg
queries eﬃciently. Proc. 24th Int. Conf. on Very Large
Data Bases, pages 299–310, 1998.
[9] P. Gibbons and Y. Matias. New sampling-based
summary statistics for improving approximate query
answers. Proc. ACM SIGMOD Int. Conf. on
Management of Data, pages 331–342, 1998.
[10] L. Golab and M. T. ¨Ozsu. Issues in data stream
management. ACM SIGMOD Record, 32(2):5–14,
Jun. 2003.
[11] J. Gray, A. Bosworth, A. Layman, and H. Pirahesh.
Data cube: A relational aggregation operator
generalizing group-by, cross-tab, and sub-total. Proc.
12th Int. Conf. on Data Engineering, pages 152–159,
1996.
[12] G. S. Manku and R. Motwani. Approximate frequency
counts over data streams. Proc. 28th Int. Conf. on
Very Large Data Bases, pages 346–357, 2002.
[13] V. Paxson and S. Floyd. Wide-area traﬃc: The failure
of poisson modeling. IEEE/ACM Trans. on
Networking, 3(3):226–244, Jun. 1995.
[14] L. Qiao, D. Agrawal, and A. El Abbadi. Supporting
sliding window queries for continuous data streams.
Proc. 15th Int. Conf. on Scientiﬁc and Statistical
Database Management, 2003, to appear.
[15] M. Sullivan and A. Heybey. Tribeca: A system for
managing large databases of network traﬃc. Proc.
USENIX Annual Technical Conf., 1998.
[16] Y. Zhu and D. Shasha. StatStream: Statistical
monitoring of thousands of data streams in real time.
Proc. 28th Int. Conf. on Very Large Data Bases,
pages 358–369, 2002.