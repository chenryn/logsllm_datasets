We validated the correctness of our implementation by
generating input event traces for each synthesized module
from the simulations described in §4 and passing them as
input in the test bench used for RTL verification by the
Vivado Design Suite. The output traces, thus, generated were
then matched with the corresponding output traces obtained
from the simulator. We also used the Vivado HLS tool to
export our RTL design to create IP blocks for our modules.
6.2.2 Synthesis Results. Our FPGA synthesis report
has been summarized in Table 2 and discussed below.
Resource Usage: The second and third columns in Table 2
report the percentage of flip-flops (FF) and look-up tables
(LUT) used for the four modules (no BRAM or DSP48E units
were consumed). We find that each of IRN’s packet process-
ing modules consume less than 1% FFs and 2% LUTs (with a
total of 1.35% FFs and 4% LUTs consumed). Increasing the
bitmap size to support 100Gbps links consumed a total of
2.66% of FFs and 9.5% of LUTs on the same device (though
we expect the relative resource usage to be smaller on a
higher-scale device designed for 100Gbps links).
Performance: The third and fourth column in Table 2 report
the worst-case latency and throughput respectively for each
module. 6 The latency added by each module is at most only
16.5ns. The receiveData module (requiring more complex
bitmap operations) had the lowest throughput of 45.45Mpps.
This is high enough to sustain a rate of 372Gbps for MTU-
sized packets. It is also higher than the maximum rate of
6The worst-case throughput was computed by dividing the clock frequency
with the maximum initiation interval, as reported by the Vivado HLS syn-
thesis tool [7].
39.5Mpps that we observed on Mellanox MCX416A-BCAT
RoCE NIC across different message sizes (2 bytes - 1KB), after
applying various optimizations such as batching and using
multiple queue-pairs. A similar message rate was observed in
prior work [25]. Note that we did not use pipelining within
our modules, which can further improve throughput.
While we expect IRN to be implemented on the ASIC in-
tegrated with the existing RoCE implementation, we believe
that the modest resources used on an FPGA board supported
as an add-on in recent RDMA-enabled NICs, provides some
intuition about the feasibility of the changes required by IRN.
Also, note that the results reported here are far from the opti-
mal results that can be achieved on an ASIC implementation
due to two sources of sub-optimality: (i) using HLS for FPGA
synthesis has been found to be up to 2× less optimal than
directly using Verilog [27] and (ii) FPGAs, in general, are
known to be less optimal than ASICs.
6.3 Impact on end-to-end performance
We now evaluate how IRN’s implementation overheads im-
pact the end-to-end performance. We identify the following
two implementation aspects that could potentially impact
end-to-end performance and model these in our simulations.
Delay in Fetching Retransmissions: While the regular
packets sent by a RoCE NIC are typically pre-fetched, we
assume that the DMA request for retransmissions is sent only
after the packet is identified as lost (i.e. when loss recovery
is triggered or when a look-ahead is performed). The time
taken to fetch a packet over PCIe is typically between a few
hundred nanoseconds to <2µs [8, 32]. We set a worst-case
retransmission delay of 2µs for every retransmitted packet
i.e. the sender QP is allowed to retransmit a packet only after
2µs have elapsed since the packet was detected as lost.
Additional Headers: As discussed in §5, some additional
headers are needed in order to DMA the packets directly to
the application memory, of which, the most extreme case is
the 16 bytes of RETH header added to every Write packet.
Send data packets have an extra header of 6 bytes, while Read
responses do not require additional headers. We simulate the
worst-case scenario of all Writes with every packet carrying
16 bytes additional header.
Results: Figure 12 shows the results for our default scenario
after modeling these two sources of worst-case overheads.
We find that they make little difference to the end-to-end
performance (degrading the performance by 4-7% when com-
pared to IRN without overheads). The performance remains
35%-63% better than our baseline of RoCE (with PFC). We
also verified that the retransmission delay of 2µs had a much
smaller impact on end-to-end performance (2µs is very small
compared to the network round-trip time taken to detect a
packet loss and to recover from it, which could be of the order
of a few hundred microseconds). The slight degradation in
Revisiting Network Support for RDMA
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 12: The figures show the performance of IRN with worse case overheads, comparing it with IRN without
any overheads and with RoCE for our default case scenario.
performance observed here can almost entirely by attributed
to the additional 16 bytes header in every packet. Therefore,
we would expect the performance impact to be even smaller
when there is a mix of Write and Read workloads.
6.4 Summary
Our analysis shows that IRN is well within the limits of fea-
sibility, with small chip area and NIC memory requirements
and minor bandwidth overhead. We also validated our analy-
sis through extensive discussions with two commercial NIC
vendors (including Mellanox); both vendors confirmed that
the IRN design can be easily implemented on their hardware
NICs. Inspired by the results presented in this paper, Mel-
lanox is considering implementing a version of IRN in their
next release.
7 Discussion and Related Work
Backwards Compatibility: We briefly sketch one possible
path to incrementally deploying IRN. We envision that NIC
vendors will manufacture NICs that support dual RoCE/IRN
modes. The use of IRN can be negotiated between two end-
points via the RDMA connection manager, with the NIC
falling back to RoCE mode if the remote endpoint does not
support IRN. (This is similar to what was used in moving
from RoCEv1 to RoCEv2.) Network operators can continue
to run PFC until all their endpoints have been upgraded to
support IRN at which point PFC can be permanently dis-
abled. During the interim period, hosts can communicate
using either RoCE or IRN, with no loss in performance.
Reordering due to load-balancing: Datacenters today use
ECMP for load balancing [23], that maintains ordering within
a flow. IRN’s OOO packet delivery support also allows for
other load balancing schemes that may cause packet reorder-
ing within a flow [20, 22]. IRN’s loss recovery mechanism
can be made more robust to reordering by triggering loss re-
covery only after a certain threshold of NACKs are received.
Other hardware-based loss recovery: MELO [28], a re-
cent scheme developed in parallel to IRN, proposes an alter-
native design for hardware-based selective retransmission,
where out-of-order packets are buffered in an off-chip mem-
ory. Unlike IRN, MELO only targets PFC-enabled environ-
ments with the aim of greater robustness to random losses
caused by failures. As such, MELO is orthogonal to our main
focus which is showing that PFC is unnecessary. Nonetheless,
the existence of alternate designs such as MELO’s further cor-
roborates the feasibility of implementing better loss recovery
on NICs.
HPC workloads: The HPC community has long been a
strong supporter of losslessness. This is primarily because
HPC clusters are smaller with more controlled traffic pat-
terns, and hence the negative effects of providing loss-
lessness (such as congestion spreading and deadlocks) are
rarer. PFC’s issues are exacerbated on larger scale clus-
ters [23, 24, 29, 35, 38].
Credit-based Flow Control: Since the focus of our work
was RDMA deployment over Ethernet, our experiments used
PFC. Another approach to losslessness, used by Infiniband, is
credit-based flow control, where the downlink sends credits
to the uplink when it has sufficient buffer capacity. Credit-
based flow control suffers from the same performance issues
as PFC: head-of-the-line blocking, congestion spreading, the
potential for deadlocks, etc. We, therefore, believe that our
observations from §4 can be applied to credit-based flow
control as well.
8 Acknowledgement
We would like to thank Amin Tootoonchian, Anirudh Sivara-
man, Emmanuel Amaro and Ming Liu for the helpful discus-
sions on some of the implementation specific aspects of this
work, and Brian Hausauer for his detailed feedback on an
earlier version of this paper. We are also thankful to Nandita
Dukkipati and Amin Vahdat for the useful discussions in the
early stages of this work. We would finally like to thank our
anonymous reviewers for their feedback which helped us in
improving the paper, and our shepherd Srinivasan Seshan
who helped shape the final version of this paper. This work
was supported in parts by a Google PhD Fellowship and by
Mellanox, Intel and the National Science Foundation under
Grant No. 1704941, 1619377 and 1714508.
References
[1] http://omnetpp.org/.
[2] https://inet.omnetpp.org.
[3] Xilinx Vivado Design Suite.
https://www.xilinx.com/products/
design-tools/vivado.html.
RoCE (with PFC)IRN (no overheads)IRN (worst-case overheads)RoCE or IRN+Timely+DCQCN05101520253035Avg. SlowdownRoCE or IRN+Timely+DCQCN0.00.51.01.52.02.53.0Avg. FCT (ms)RoCE or IRN+Timely+DCQCN010203040506099%ile FCT (ms)SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
Memory Efficient Loss Recovery for Hardware-based Transport in
Datacenter. In Proc. First Asia-Pacific Workshop on Networking (APNet),
2017.
[29] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan
Wassel, Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wether-
all, and David Zats. TIMELY: RTT-based Congestion Control for the
Datacenter. In Proc. ACM SIGCOMM, 2015.
[30] Radhika Mittal, Justine Sherry, Sylvia Ratnasamy, and Scott Shenker.
Recursively Cautious Congestion Control. In Proc. USENIX NSDI, 2014.
[31] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi,
Arvind Krishnamurthy, Sylvia Ratnasamy, and Scott Shenker. Revisit-
ing Network Support for RDMA (Extended Version). arXiv:1806.08159,
2018.
[32] Sivasankar Radhakrishnan, Yilong Geng, Vimalkumar Jeyakumar, Ab-
dul Kabbani, George Porter, and Amin Vahdat. SENIC: Scalable NIC
for End-host Rate Limiting. In Proc. USENIX NSDI, 2014.
[33] Renato Recio, Bernard Metzler, Paul Culley, Jeff Hilland, and Dave
Garcia. A Remote Direct Memory Access Protocol Specification. RFC
5040, 2007.
[34] Alexander Shpiner, Eitan Zahavi, Omar Dahley, Aviv Barnea, Rotem
Damsker, Gennady Yekelis, Michael Zus, Eitan Kuta, and Dean Baram.
RoCE Rocks Without PFC: Detailed Evaluation. In Proc. ACM Workshop
on Kernel-Bypass Networks (KBNets), 2017.
[35] Alexander Shpiner, Eitan Zahavi, Vladimir Zdornov, Tal Anker, and
In Proc. ACM
Matty Kadosh. Unlocking Credit Loop Deadlocks.
Workshop on Hot Topics in Networks (HotNets), 2016.
[36] Daniel J. Sorin, Mark D. Hill, and David A. Wood. A Primer on Memory
Consistency and Cache Coherence. Morgan & Claypool Publishers, 1st
edition, 2011.
[37] Brent Stephens, Alan L Cox, Ankit Singla, John Carter, Colin Dixon,
and Wesley Felter. Practical DCB for improved data center networks.
In Proc. IEEE INFOCOM, 2014.
[38] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina
Lipshteyn, Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mo-
hamad Haj Yahia, and Ming Zhang. Congestion Control for Large-Scale
RDMA Deployments. In Proc. ACM SIGCOMM, 2015.
[4] InfiniBand architecture volume 1, general specifications, release 1.2.1.
www.infinibandta.org/specs, 2008.
[5] Supplement to InfiniBand architecture specification volume 1 release
1.2.2 annex A16: RDMA over Converged Ethernet (RoCE). www.
infinibandta.org/specs, 2010.
[6] IEEE. 802.11Qbb. Priority based flow control, 2011.
[7] Vivado Design Suite User Guide. https://goo.gl/akRdXC, 2013.
[8] http://www.xilinx.com/support/documentation/white_papers/
wp350.pdf, 2014.
[9] Supplement to InfiniBand architecture specification volume 1 release
1.2.2 annex A17: RoCEv2 (IP routable RoCE),. www.infinibandta.org/
specs, 2014.
[10] Mellanox ConnectX-4 Product Brief. https://goo.gl/HBw9f9, 2016.
[11] Mellanox ConnectX-5 Product Brief. https://goo.gl/ODlqMl, 2016.
[12] Mellanox Innova Flex 4 Product Brief. http://goo.gl/Lh7VN4, 2016.
[13] RoCE vs. iWARP Competitive Analysis. http://www.mellanox.com/
related-docs/whitepapers/WP_RoCE_vs_iWARP.pdf, 2017.
[14] Sarita V Adve and Hans-J Boehm. Memory models: a case for rethink-
ing parallel languages and hardware. Communications of the ACM,
2010.
[15] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra
Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari
Sridharan. Data Center TCP (DCTCP). In Proc. ACM SIGCOMM, 2010.
[16] Mohammad Alizadeh, Shuang Yang, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. Deconstructing Datacenter Packet
Transport. In Proc. ACM Workshop on Hot Topics in Networks (HotNets),
2012.
[17] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick
McKeown, Balaji Prabhakar, and Scott Shenker. pFabric: Minimal
Near-optimal Datacenter Transport. In Proc. ACM SIGCOMM, 2013.
[18] Appenzeller, Guido and Keslassy, Isaac and McKeown, Nick. Sizing
router buffers. In Proc. ACM SIGCOMM, 2004.
[19] Theophilus Benson, Aditya Akella, and David Maltz. Network Traffic
Characteristics of Data Centers in the Wild. In Proc. ACM Internet
Measurement Conference (IMC), 2012.
[20] Advait Dixit, Pawan Prakash, Y Charlie Hu, and Ramana Rao Kompella.
On the impact of packet spraying in data center networks. In Proc.
IEEE INFOCOM, 2013.
[21] Aleksandar Dragojević, Dushyanth Narayanan, Miguel Castro, and
Orion Hodson. FaRM: Fast Remote Memory. In Proc. USENIX NSDI,
2014.
[22] Soudeh Ghorbani, Zibin Yang, P. Brighten Godfrey, Yashar Ganjali, and
Amin Firoozshahian. DRILL: Micro Load Balancing for Low-latency
Data Center Networks. In Proc. ACM SIGCOMM, 2017.
[23] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye,
Jitu Padhye, and Marina Lipshteyn. RDMA over commodity ethernet
at scale. In Proc. ACM SIGCOMM, 2016.
[24] Shuihai Hu, Yibo Zhu, Peng Cheng, Chuanxiong Guo, Kun Tan, Jiten-
dra Padhye, and Kai Chen. Deadlocks in Datacenter Networks: Why
Do They Form, and How to Avoid Them. In Proc. ACM Workshop on
Hot Topics in Networks (HotNets), 2016.
[25] Anuj Kalia, Michael Kaminsky, and David G. Andersen. Using RDMA
Efficiently for Key-value Services. In Proc. ACM SIGCOMM, 2014.
[26] Anuj Kalia, Michael Kaminsky, and David G. Andersen. Design Guide-
lines for High Performance RDMA Systems. In Proc. USENIX ATC,
2016.
[27] Bojie Li, Kun Tan, Layong (Larry) Luo, Yanqing Peng, Renqian Luo,
Ningyi Xu, Yongqiang Xiong, Peng Cheng, and Enhong Chen. ClickNP:
Highly Flexible and High Performance Network Processing with Re-
configurable Hardware. In Proc. ACM SIGCOMM, 2016.
[28] Yuanwei Lu, Guo Chen, Zhenyuan Ruan, Wencong Xiao, Bojie Li,
Jiansong Zhang, Yongqiang Xiong, Peng Cheng, and Enhong Chen.