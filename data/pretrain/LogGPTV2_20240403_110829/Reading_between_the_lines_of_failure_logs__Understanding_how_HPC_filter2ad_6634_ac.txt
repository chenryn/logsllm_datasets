i
  NA   
1795.5x
a
F
e
r
a
w
d
r
a
H
a
2247x  
  NA   
1795.5x
2247x  
  NA   
1995x  
day week month
day week month
System 18                    System 19                  System 20
day week month
Rest of nodes
Node−0
481.5x
662.1x
897.7x
481.5x
794.6x
997.5x
481.5x
794.6x
997.5x
day week month
day week month
System 18                    System 19                  System 20
day week month
f
o
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
t
e
d
n
U
n
a
f
o
y
t
i
l
i
b
a
b
o
r
P
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.35
0.3
0.25
0.2
0.15
0.1
37.8x
62.5x
0.05
0
38.9x
 NA   NA    NA  
day week month
day week month
System 18                    System 19                  System 20
day week month
61.8x
Fig. 6. The probability of different failure types in failure prone nodes compared to the rest of the nodes in a system.
The ﬁrst observation we make based on Figure 6 is that
node 0 exhibits increased failure probabilities for all types of
failures, so the higher failure rate in those nodes cannot be
attributed to a particular type of failure. However, we observe
that the increase in failure probabilities is particularly high for
environmental and network failures, with factors of increase
in the 2000x and 500x-1000x range, respectively. Software
failure rates are also signiﬁcantly higher in node 0 than the
remainder of the system (factors of 36X up to 118X). The
increase in the probability of hardware failures is modest in
comparison, but still signiﬁcant with factors in the 5–10X
range. To formalize our results we repeat the chi-square test
for differences between proportions separately for each failure
type. The only failure type where we fail to reject the null
hypothesis that nodes fail with equal rates is for failures due
to human errors; for all other failure types the test rejects the
null hypothesis with 99% conﬁdence.
Turning to Figure 5, which shows the relative breakdown of
failures by root cause for the failure prone nodes compared to
the whole system, we observe a higher percentage of software,
environment and network failures in the failure prone nodes.
This observation is in agreement with our ﬁndings in Figure 6,
which indicate that those three failure types have a higher
factor increase in the failure prone nodes than other failure
types. It is interesting to note that in the failure prone nodes
the dominant failure mode shifts from hardware failures to
software failures.
C. Why do some nodes fail more frequently than others?
One might wonder what the reason for the high variability
in failure rates between nodes in the same system is,
in
particular since all nodes within a system typically use the
same type of hardware. One possible explanation are statistical
effects due to the strong correlations between failures in the
same node (recall Section III). Once a node is “unlucky” and
starts to develop failures, a large number of correlated follow-
up failures might bring the total failure rate of a node way
above the average.
Another hypothesis we investigated is the effect of a node’s
position in the machine room or inside the physical rack. For a
few systems where we had information on the layout of nodes
in the machine room we checked whether the location in the
machine room or the location of a node within a rack played
any role, but we could not ﬁnd any clear patterns that certain
areas in the machine room were more likely to be correlated
with higher error rates.
One more hypothesis that we tested is whether usage has
an effect on the failure rate of a node and whether the failure
prone nodes were used differently from other nodes. We will
look at our analysis of usage in the following two sections.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
e
m
i
t
e
f
i
L
e
d
o
N
n
i
s
e
r
u
l
i
a
F
f
o
r
e
b
m
u
N
50
40
30
20
10
0
0
20
System 8
Rest of nodes
Node−0
Node−157
e
m
i
t
e
f
i
L
e
d
o
N
n
i
s
e
r
u
l
i
a
F
f
o
r
e
b
m
u
N
40
60
Node Utilization %
80
100
(a)
System 20
Rest of nodes
Node−0
35
30
25
20
15
10
5
0
0
20
40
60
Node Utilization %
80
100
e
m
i
t
e
f
i
L
e
d
o
N
n
i
s
e
r
u
l
i
a
F
f
o
r
e
b
m
u
N
50
40
30
20
10
0
0
System 8
Rest of nodes
Node−0
Node−157
1
2
3
4
5
Total Number of Jobs Assigned to Node
6
4
x 10
e
m
i
t
e
f
i
L
e
d
o
N
n
i
s
e
r
u
l
i
a
F
f
o
r
e
b
m
u
N
35
30
25
20
15
10
5
0
0
(b)
System 20
Rest of nodes
Node−0
0.5
1
1.5
Total Number of Jobs Assigned to Node
2
4
x 10
Fig. 7. The impact of usage on node reliability. (a) Node failures vs node utilization. (b) Node failures vs node jobs
V. WHAT IS THE EFFECT OF USAGE ON A NODE’S
RELIABILITY?
The effect of system workload on system reliability was
studied in a series of papers by Iyer et al. [8], [9] and Castillo
et al. [2]. However, these papers date back to the early 1980’s
and don’t necessarily translate to modern HPC systems.
We therefore used job logs that are available for two of
LANL’s systems, system 8 (where we have a total of 763,293
job records), and system 20 (with a total of 477,206 job
records), to study whether the way a node is used affects
its failure rate. These two systems are representative of two
larger groups of LANL systems, where all systems within the
same group shared a similar hardware architecture and ran very
comparable workloads.
We consider the effect of two simple usage metrics, one
is the average node utilization (where we deﬁne a node as
being utilized if at least one job is currently assigned to it, and
idle otherwise) and the other one is the number of jobs that
were scheduled on a node throughout its lifetime. We begin
by plotting the number of failures a node experiences against
the node’s average utilization (see Figure 7-(a)) and against
the number of jobs served by the node (see Figure 7-(b)). We
have marked nodes with particularly high failure rates with
special markers. This includes node 0, which we discussed in
the previous section.
We observe that in both systems where we have usage
information available the failure prone node 0 tends to be
among the nodes with the highest utilization and the largest
number of jobs assigned to it. We formalized our observation
by looking at the Pearson correlation coefﬁcient between the
number of jobs assigned to a node and the number of failures
experienced by the node. For both systems we observe clearly
positive correlation coefﬁcients of 0.465 and 0.12, respectively.
However, repeating our analysis after removing node 0 reduces
the correlation to insigniﬁcant levels, which lets us conclude
that the strong linear correlation between usage and failures,
as captured by Pearson’s coefﬁcient, is mostly due to node 0.
In discussions with operators at LANL we have been told that
node 0 in most systems has a special role where it is used as
a login node for users and/or is used to schedule and launch
jobs.
System 8
System 20
0.02
0.015
0.01
0.005
y
a
d
−
c
o
r
p
r
e
p
s
e
r
u
l
i
a
f
f
o
m
u
N
0
0
0.012
0.01
0.008
0.006
0.004
0.002
y
a
d
−
c
o
r
p
r
e
p
s
e
r
u
l
i
a
f
f
o
m
u
N
0
0
10
20
30
User
40
50
10
20
30
User
40
50
Fig. 8. Distribution of failed jobs over different users.
VI. ARE SOME USERS MORE PRONE TO NODE FAILURES
THAN OTHERS?
As a follow-up question on the relationship between usage
and failures we used the job logs to test whether certain
users are more likely to experience job failures than others.
Here we only include job failures that are caused by failures
in one of the underlying nodes, rather than a failure of a
user’s application software. The two systems that have job
logs available (systems 8 and 20) both have more than 400
different users. For each system, we focus on the 50 heaviest
users in terms of the number of processor-days that they used
on those systems.
The two graphs in Figure 8 show for each of the 50 heaviest
users the average number of failures this user experienced
per processor-day that this user utilized the system. Visual
inspection shows a large discrepancy between the failure rates
experienced by different users. We also formally veriﬁed that
the difference in failure rates between users is statistically
signiﬁcant by using Poisson regression to ﬁt a full (saturated)
model (with users’ actual failure counts and usage periods),
and a common failure rate model (where all users have the
same failure rate). We then applied Analysis of Variance
(ANOVA) test and found with 99% conﬁdence level that the
saturated model is signiﬁcantly better than the common rate
model.
In conclusion, we ﬁnd that the way a node is exercised
affects its failure rates. This might for example be because
some users run applications that are more likely to exercise
a buggy code path in some system software or because their
application is more likely to exercise a hardware component in
an access pattern that makes intermittent or hard errors more
likely to manifest themselves.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
e
r
u
l
i
a
F
e
r
a
w
d
r
a
H
a
f
o
y
t
i
l
i
b
a
b
o
r