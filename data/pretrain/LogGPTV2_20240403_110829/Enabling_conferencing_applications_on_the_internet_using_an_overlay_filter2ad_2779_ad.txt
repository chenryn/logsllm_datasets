rate for the Primary Set
Figure 10: Mean bandwidth versus rank at 2.4 Mbps
source rate for the Extended Set of machines
From the perspective of RTT, we ﬁnd that Bandwidth-
Latency performs almost indistinguishably from Latency-
Only, and both schemes achieve performance reasonably
close to Sequential Unicast. However, more surprisingly,
Prop-Delay-Only achieves RTTs at least 100 ms more than
Bandwidth-Latency for machines of lower rank, and thus
performs badly even in the RTT metric. This is because
delays in the Internet may often arise due to congestion,
and optimizing purely for propagation delay need not op-
timize the latencies seen by the application. This obser-
vation becomes particularly important in our environment
where many hosts are behind 10 Mbps connections, and
poorly constructed overlays could cause congestion near a
host. While we did use conservative pre-conﬁgured degree
bounds recommended in [2, 3, 6], this strategy is not capa-
ble of dealing with dynamic cross-traﬃc. In contrast, the
dynamic nature of Bandwidth-Latency and Latency-Only
enables them to perform better in such situations.
We have also evaluated Bandwidth-Only in this environ-
ment. We ﬁnd the bandwidth results are comparable to
Bandwidth-Latency, but the RTT results are worse. Finally,
because of the poor performance of Prop-Delay-Only, our
future evaluation concentrates on Latency-Only while ana-
lyzing the performance of delay based schemes.
6.2.3 Extended Set at 2.4 Mbps Source Rate
Our results so far demonstrate that even in less heteroge-
neous environments such as the Primary Set, good perfor-
mance requires considering both bandwidth and latency as
metrics in overlay construction. To further emphasize the
importance of taking both bandwidth and latency into ac-
count, we consider extremely heterogeneous environments as
represented by the Extended Set. Figures 10 and 11 plot the
bandwidth and RTT against host ranks for the four schemes
of interest.
The Sequential Unicast curves show that there are quite
a few members that have low bandwidth and high laten-
cies from the source, which indicates the heterogeneity in
the set we consider. Even in such a heterogeneous setting,
Bandwidth-Latency is able to achieve a performance close
to the Sequential Unicast
test. Apart from the less well-
connected hosts (ranks 1–5), all other members get band-
)
s
m
(
T
T
R
n
a
e
M
1000
900
800
700
600
500
400
300
200
100
0
0
Sequential Unicast
Bandwidth-Latency
Latency-Only
Bandwidth-Only
2
4
6
8
10
Rank
12
14
16
18
20
Figure 11: Mean RTT versus rank at 2.4 Mbps
source rate for the Extended Set of machines
widths of at least 1.8 Mbps, and see RTTs of less than 250 ms
on average. For ranks 1–5, Bandwidth-Latency is able to
exploit Internet routing pathologies and provide better per-
formance than Sequential Unicast . A particularly striking
example was two machines in Taiwan, only one of which
had good performance to machines in North America.
In
our runs, the machine with poorer performance was able
to achieve signiﬁcantly better performance by connecting to
the other machine in Taiwan.
Next, we observe that Bandwidth-Only results in high
RTT, while Latency-Only performs poorly in bandwidth.
For example, for machines of rank 7, Bandwidth-Latency
can sustain throughputs almost 800 Kbps more than Latency-
Only, and can achieve RTTs more than 100 ms lower than
Bandwidth-Only. Further, Bandwidth-Latency has smaller
standard deviations, which indicates that the overlays it pro-
duces consistently attain good performance in both band-
width and latency. Finally, we observe that Bandwidth-
Latency provides equivalent performance to Bandwidth-Only
in bandwidth, and to Latency-Only in RTT indicating that
Experiment Setup
Unicast
Random
Latency-Only
Bandwidth-Only
Bandwidth-Latency
Min-Span
Primary
1.2 Mbps
2.62
2.24
1.39
1.85
1.49
0.85
Primary Extended
2.4 Mbps
2.4 Mbps
2.62
1.83
1.97
2.05
1.25
1.42
1.51
1.86
1.31
1.73
0.85
0.83
Table 1: Average normalized resource usage of dif-
ferent schemes
optimizing for multiple metrics does not compromise the
performance with respect to any single metric.
6.2.4
Summary of comparison results
We summarize results from our comparison study below:
• Our techniques enable the construction of eﬃcient over-
lays optimized for both bandwidth and latency, as required
by conferencing applications. Even in extremely heteroge-
neous environments, Bandwidth-Latency has performance
comparable to Sequential Unicast, and sometimes performs
better by exploiting Internet pathologies.
• Random overlays do not perform well even in settings with
a small amount of heterogeneity.
• Overlays that adapt to simple static metrics like propa-
gation delay perform quite poorly, not only in bandwidth,
but also in latencies. This is because schemes that use only
static metrics fail to detect and react to network congestion,
resulting in larger queueing delays and higher packet loss.
• Overlays that adapt to latency alone are unable to provide
good bandwidth performance, especially at higher source
rates. Conversely, overlays that adapt to bandwidth alone
are unable to provide good latencies. These results indicate
that latency and bandwidth are not strongly correlated on
the Internet, and it is critical to consider both metrics to
construct overlays optimized for conferencing applications.
6.3 Network Level Metrics
Table 1 compares the mean normalized resource usage
(Section 5.3) of the overlay trees produced by the various
schemes for diﬀerent environments and source rates. The
values are normalized with respect to the resource usage
with native IP Multicast support, determined as explained
in Section 5.3. Thus, we would like the normalized resource
usage to be as small as possible, with a value of 1.00 rep-
resenting an overlay tree that has the same resource usage
as IP Multicast. Given that the trees constructed by self-
organizing protocols can change over time, we consider the
ﬁnal tree produced at the end of an experiment. However,
we observe that the overlays produced by these schemes are
reasonably stable after about four minutes.
There are several observations to be made from Table 1.
First, naive degenerated unicast trees which have all recip-
ients rooted at the source, and schemes such as Random
that do not explicitly exploit network information have a
high resource usage. Second, protocols that adapt to band-
width alone (Bandwidth-Only) make less eﬃcient use of net-
work resources compared to protocols such as Bandwidth-
Latency, and Latency-Only which consider delay based met-
rics. Third, the resource usage with Bandwidth-Latency is
a little higher than Latency-Only, which reﬂects the cost in
adapting to better bandwidth paths. Finally, the resource
usage with Bandwidth-Latency increases with source rate
Experiment Setup
Average Overhead (%)
% of
Bandwidth
overhead Probes
due to
Other
Primary
1.2 Mbps
10.79
Primary Extended
2.4 Mbps
2.4 Mbps
11.53
14.20
92.24
7.76
96.03
3.37
94.30
5.70
Table 2: Average overhead with Bandwidth-Latency
and a breakdown of the overhead
(in the Primary Set). This is because it favors higher band-
width paths over lower delay paths at higher source rates.
We have also determined the resource usage of Min-Span,
the minimum spanning tree of the complete graph of all
members, computed by estimating the delays of all links of
the complete graph. Minimum spanning trees are known to
be optimal with respect to resource usage, and as Table 1
shows, have lower resource usage than IP Multicast. This
indicates that an End System Multicast architecture can in-
deed make as eﬃcient, if not better use of network resources
than IP Multicast. However, while minimum spanning trees
are eﬃcient from the network perspective, it is not clear that
they are eﬃcient from the application perspective.
Table 2 summarizes the protocol overhead (Section 5.3)
involved in Bandwidth-Latency, along with a breakdown of
the main factors that contribute to the overhead. We ﬁnd
that the average overhead is between 10 to 15% across all
settings. This is an indication that even simple heuristics
that we have implemented can keep the overall overhead
low. Further, more than 90% of the overhead is due to
members probing each other for bandwidth. Other sources
of overhead contribute just 3–7% of the overhead. These
include exchange of routing messages between neighbors,
group management algorithms to keep the overlay connected,
and probes to determine the delay and routing state of re-
mote members. This conﬁrms our intuition that techniques
for lowering protocol overhead must focus on reducing costs
of bandwidth probes.
An analysis of our logs reveals that the simple heuristics
introduced in Section 4.2 are able to reduce around 50% of
possible bandwidth probes between pairs of members over
a 20 minute period. Further, in experiments with the Ex-
tended Set, we ﬁnd that there are no bandwidth probes to
the member behind the ADSL connections. Further, there
are very few probes from a machine in North America to
hosts in Asia and Europe. While the results are encourag-
ing, overhead due to active bandwidth measurements can be
a concern as one considers larger sized groups. We present
some of our ideas for tackling this in Section 8.
7. ADAPTING TO NETWORK DYNAMICS
Section 6 has shown that it is important to adapt to dy-
namic metrics such as bandwidth and latency while con-
structing overlays optimized for conferencing applications.
When network congestion occurs, overlays need to adapt by
making appropriate topology changes in a timely fashion
to ensure good and stable application performance. In this
section, we evaluate how quickly our protocol can adapt to
changes in network conditions, and study the factors that
contribute to adaptation times.
To study these issues, we design a set of experiments on
the Internet where we artiﬁcially introduce network conges-
tion on selected links in the overlay tree, and evaluate how
our system responds to these congestion signals. Our study
is conducted using our Bandwidth-Latency overlay scheme.
)
s
p
b
K
(
i
t
h
d
w
d
n
a
B
UDEL
n
o
i
t
s
e
g
n
o
c
t
c
e
n
j
i
C
N
U
o
t
h
c
t
i
w
s
S
S
A
M
U
o
t
h
c
t
i
w
s
B
S
C
U
o
t
e
t
a
m
i
t
s
e
r
e
w
o
l
3000
2500
2000
1500
1000
500
0
560
580
600
640
620
Time (seconds)
660
680
700
1
0.8
0.6
0.4
0.2
)
%
(
y
t
i
l
i
b
a
b
o
r
P
e
v
i
t
l
a