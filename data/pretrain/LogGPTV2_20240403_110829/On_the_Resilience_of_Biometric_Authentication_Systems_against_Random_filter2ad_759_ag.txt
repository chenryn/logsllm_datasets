biometric that is presented is characteristic of a real person
and not a recreation, e.g., face masks remain relatively
static and unmoving compared to a real face [55]. Our at-
tack surface applies once the front-end has been bypassed.
Our mitigation measures can thus be used in conjunction
with these detection mechanisms to thwart random input
attacks. Being generic, our mitigation measures also work
for systems which do not have defense measures similar to
liveness detection.
• Once an accepting sample via the feature vector API
has been found, it may be possible to obtain an input
that results in this sample (after feature extraction), as
demonstrated by Garcia et al. with the training of an auto-
encoder for both feature extraction and the regeneration of
the input image [23].
• In this work, we have focused on authentication as a binary
classiﬁcation problem, largely because of its widespread
use in biometric authentication [8], [9], [10], [11], [12],
[13], [14], [15], [16], [26]. However, authentication has also
been framed as a one-class classiﬁcation problem [56], [26]
or as multi-class classiﬁcation [26], e.g., in a discrimination
model, as noted earlier. In one-class classiﬁcation, only
samples from the target user are used to create the template,
and the goal is to detect outliers. If this is achieved in a
manner similar to distance-based classiﬁers, then as we
have seen in Section V-C, and as previously indicated
in [17], the AR is expected to be small. In the multi-class
setting, each of the n users is treated as a different class.
This increase in classes is expected to proportionally lower
the AR. However, whether this behavior is observed on real
world data requires additional experimentation. We remark
that as observed in Section V-B, AR is highly dependent on
the relative variance of the positive user and the negative
user features. This may lead to the possibility of larger AR
for some of the users, consequently leading to higher risk
of attack for these users. We leave thorough investigation
of the one-class and multi-class settings as future work.
VIII. RELATED WORK
There are several mentions of attacks similar to the random
input attack discussed in this paper. Pagnin et al. [17] deﬁne
a blind brute-force attack on biometric systems where the at-
tacker submits random inputs to ﬁnd an accepting sample. The
inputs are n-element vectors whose elements are integers in the
set {0, 1, . . . , q− 1}. The authors conclude that the probability
of success of this attack is exponential in n, assuming that
the authentication is done via a distance function (discarding
any vector outside the ball of radius determined by the system
threshold). They concluded that blind brute force attack is not
effective in recovering an accepting sample. While this may
apply to distance-based matching, the same conclusion cannot
be made about machine learning based algorithms whose
decision functions are more involved. Indeed, we have shown
that the acceptance region for machine learning classiﬁers is
not exponentially small. It has also been argued that the success
rate of random input attacks can be determined by the false
positive rate (FPR), at least in the case of ﬁngerprint and face
authentication [18], [19]. We have shown that for sophisticated
machine learning classiﬁers this conclusion is not true, and
random input attacks in many instances success at a rate higher
than FPR. A more involved method is hill-climbing [57], [18]
which seeks an accepting sample via exploiting the conﬁdence
scores returned by the matching algorithm. The authentication
systems considered in this paper do not return conﬁdence
scores.
Serwadda and Phoha [58] use a robotic ﬁnger and popu-
lation statistics of touch behavior on smartphones to launch
a physical attack on touch-based biometric authentication
systems. Their attack reduces the accuracy of the system by
increasing the EER. In contrast, our work does not assume any
knowledge of population biometric statistics, e.g., population
distribution of feature space. It is an interesting area of work
to investigate whether a robotic ﬁnger can be programmed to
generate raw inputs used in our attack.
they show that
Garcia et al. [23] use explainable-AI techniques [48] to
construct queries (feature vectors) to ﬁnd an accepting sample
in machine learning based biometric authentication systems.
On a system with 0 FPR,
their attack is
successful in breaching the system with up to 93% success
rate. However, their attack is more involved: it requires the
construction of a seed dataset containing representative ac-
cepting and rejecting samples of a user set chosen by the
adversary. This dataset trains a neural network as a substitute
to the classiﬁer of the authentication system. The adversary
then uses explainable AI techniques to obtain an accepting
sample of a target user (not in the seed dataset) in as few
queries as possible, by updating the substitute network. The
authors also report a random feature vector attack, however, the
attack is only successful on one out of 16 victims. The random
feature vector is constructed by sampling each feature value
via a normal distribution (distribution parameters not stated),
unlike the uniform distribution in our case. We also note that
they propose including images with randomly perturbed pixels
as a counter-measure to defend against the aforementioned
random input attack. This is different from our proposed beta-
distributed noise mitigation technique, as it is agnostic to the
underlying biometric modality.
The frog-boiling attack [59], [60] studies the impact of
gradual variations in training data samples to manipulate the
classiﬁer decision boundary. In this work we do not consider
the adversary with access to the training process, nor do we
evaluate models with an iterative update process. If this threat
model is considered for the problem addressed in this paper,
then an adversary may seek to maximize the acceptance region
of a model by gradually poisoning the training dataset. As we
have demonstrated in Section V, the relative variance between
the user’s data and population dataset directly impacts AR.
Thus the manipulation of a user’s training samples to be more
varied would be effective in increasing the AR. Likewise, in
our mitigation technique, we have shown that beta-distributed
noise is effective in the minimization of AR. However an
14
adversary might poison the training data by labeling beta
noise as positive samples resulting in a maximization of the
acceptance region to near 100% of the feature space.
Our work is different from another line of work that
targets machine learning models in general. For instance, the
work in [61] shows an evasion attack where the adversary,
through only blackbox access to a neural network, forces the
classiﬁer to misclassify an input by slightly perturbing the
input even though the perturbed sample is perceptually similar
to the original sample, e.g., noisy images. The attack can be
applicable to the authentication setting as well. However, it
relies on the conﬁdence values (probability vectors) returned
by the classiﬁer, which is not
the case in authentication.
Similarly, the work in [62] shows how to steal a machine
learning model, i.e., retrieve its undisclosed parameters, which
only returns class labels (accept/reject decision in the case of
authentication). They describe several techniques including the
Lowd and Meek attack [63] to retrieve a model sufﬁciently
similar to the target model. The machine learning models
considered in their attack are for applications different from
authentication where one expects to ﬁnd an accepting sample
with negligible probability.
There are also proposals to defend against the above men-
tioned evasion attacks. The goal is to make the classiﬁers ro-
bust against adversarial inputs in the sense that classiﬁcation is
constant within a ball of certain radius around each input [64],
[65]. Madry et al. [64] propose a theoretical framework which
formalizes defense against adversarial attacks by including
adversarially perturbed samples in the loss function of DNNs.
They show that it is possible to train DNNs robust against a
wide range of adversarial input attacks. Cao and Gong [66]
propose another defense where given a test input, random
points within a hypercube surrounding the input are sampled,
and the majority label returned by the already trained DNN is
assigned to the test input. Randomized smoothing [65] creates
a separate classiﬁer from any classiﬁer such that its prediction
within a Gaussian noise region (ball) around any input is
constant, and consequently less likely to produce an erroneous
prediction. We note that in evasion attacks there is a notion of
nearness, i.e., the adversary is given an input and seeks to add
a small amount of noise such that the resultant erroneously
labelled input is close to the original input. In contrast, in our
case the random input need not be close to the target user’s
samples or even follow the same distribution. Furthermore,
we have shown that even a conservative estimate of the true
positive region is negligible in comparison to the entirety of
the feature space (Section III-B). Thus, it is unclear whether
such defenses apply to uniform random inputs, as opposed to
random perturbations of inputs.
Membership inference attacks [24], [25] attempt to de-
termine if a record obtained by an adversary was part of
the original training data of the model. Whilst this attack
does not compromise the security of the model, it breaches
the privacy of the individual records. These attacks create
a shadow model [24] to mimic the behavior of the target
model. Salem et al. [25] construct a shadow model using
only positive class samples and negative noise generated via
uniformly random feature vectors. However it is hypothesized
that these random samples belong to non-members, i.e., the
negative class [25, §V.B]. We have shown that a large portion
of these random inputs may also belong to the positive class.
Finally, we point to other works in literature analyzing
the security of biometric authentication systems. Sugrim et
al. [45] survey and evaluate a range of performance met-
rics used in biometric authentication schemes. They seek
to motivate scheme designers to leverage robust metrics to
provide a complete description of the system, including a
proposal of the new metric: Frequency Count Score (FCS). The
FCS metric shows a distribution of scores of legitimate and
unauthorized users, identifying the overlap between the two
distributions which helps to select the appropriate threshold
for the classiﬁcation decision. The FCS, however, is dependent
on the negative class or samples of other users, which does
not
include random inputs. The work in [67] investigates
the accuracy of authentication systems reported on a small
number of participants when evaluated over an increasing
number of users. The authors suggest that performance limits
of a system with a small number of participants should be
evaluated iteratively by increasing the participant count until
a the performance degrades below a tolerable limit.
It
is important
IX. CONCLUSION
to assess the security of biometric au-
thentication systems against random input attacks akin to
the security of passwords against random guess attacks. We
have demonstrated that without intentionally including random
inputs as part of the training process of the underlying machine
learning algorithm, the authentication system is likely to be
susceptible to random input attacks at a rate higher than
indicated by EER. Absent any other detection mechanism, e.g.,
liveliness detection, this renders the system vulnerable. The
mitigation measures proposed in this paper can be adopted to
defend against such attacks.
X. ACKNOWLEDGMENTS
This research was funded by the Optus Macquarie Uni-
versity Cybersecurity Hub, Data61 CSIRO and an Australian
Government Research Training Program (RTP) Scholarship.
We would like to thank the anonymous reviewers and our
shepherd Kevin Butler for their feedback to improve the paper.
REFERENCES
[1] D. Maltoni, D. Maio, A. K. Jain, and S. Prabhakar, Handbook of
ﬁngerprint recognition. Springer Science & Business Media, 2009.
[2] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation from
scratch,” arXiv preprint arXiv:1411.7923, 2014.
[3] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed
embedding for face recognition and clustering,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2015.
[4] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale
speaker identiﬁcation dataset,” arXiv preprint arXiv:1706.08612, 2017.
J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker
recognition,” arXiv preprint arXiv:1806.05622, 2018.
[5]
[6] U. Mahbub, S. Sarkar, V. M. Patel, and R. Chellappa, “Active user
authentication for smartphones: A challenge data set and benchmark
results,” in Biometrics Theory, Applications and Systems (BTAS), 2016
IEEE 8th International Conference on.
IEEE, 2016, pp. 1–8.
[7] W. Xu, G. Lan, Q. Lin, S. Khalifa, N. Bergmann, M. Hassan, and W. Hu,
“Keh-gait: Towards a mobile healthcare user authentication system by
kinetic energy harvesting.” in NDSS, 2017.
15
(a) Touch Average ROC in the presence of Beta Noise
Fig. 11. Beta-noise mitigation of AR, with additional negative samples from the RAR feature set. The EER is marked on the diagrams as a vertical line.
Addition RAR vectors were included as it was previously observed that beta noise is sufﬁcient in mitigating AR attacks, but not the RAR attack.
(b) Face Average ROC in the presence of Beta Noise
[8]
J. Chauhan, B. Z. H. Zhao, H. J. Asghar, J. Chan, and M. A. Kaafar,
“Behaviocog: An observation resistant authentication scheme,” in In-
ternational Conference on Financial Cryptography and Data Security.
Springer, 2017, pp. 39–58.
[9] M. T. Curran, N. Merrill, J. Chuang, and S. Gandhi, “One-step, three-
factor authentication in a single earpiece,” in Proceedings of the 2017
ACM International Joint Conference on Pervasive and Ubiquitous
Computing and Proceedings of the 2017 ACM International Symposium