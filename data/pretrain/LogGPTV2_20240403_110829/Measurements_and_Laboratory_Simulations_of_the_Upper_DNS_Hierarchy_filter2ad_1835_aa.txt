title:Measurements and Laboratory Simulations of the Upper DNS Hierarchy
author:Duane Wessels and
Marina Fomenkov and
Nevil Brownlee and
Kimberly C. Claffy
Measurements and Laboratory Simulations of the
Upper DNS Hierarchy
Duane Wessels1, Marina Fomenkov2, Nevil Brownlee2, and kc claffy2
1 The Measurement Factory, Inc.
2 CAIDA, San Diego Supercomputer Center, University of California, San Diego
PI:EMAIL
{marina, nevil, kc}@caida.org
Abstract. Given that the global DNS system, especially at the higher root and
top-levels, experiences signiﬁcant query loads, we seek to answer the following
questions: (1) How does the choice of DNS caching software for local resolvers
affect query load at the higher levels? (2) How do DNS caching implementati-
ons spread the query load among a set of higher level DNS servers? To answer
these questions we did case studies of workday DNS trafﬁc at the University of
California San Diego (USA), the University of Auckland (New Zealand), and the
University of Colorado at Boulder (USA). We also tested various DNS caching
implementations in fully controlled laboratory experiments. This paper presents
the results of our analysis of real and simulated DNS trafﬁc. We make recommen-
dations to network administrators and software developers aimed at improving the
overall DNS system.
1 Background
The Domain Name System (DNS) is a fundamental component of the modern Inter-
net [1], providing a critical link between human users and Internet routing infrastructure
by mapping host names to IP addresses. The DNS hierarchical name space is divided
into zones and coded in the widespread “dots” structure. For example, com is the parent
zone for microsoft.com, cnn.com, and approximately 20 million other zones.
The DNS hierarchy’s root zone is served by 13 nameservers, known collectively
as the DNS root servers. However, the root server operators use various techniques to
distribute the load among more than 13 servers. Perhaps the oldest technique is a load-
balancing switch for servers at the same site. The most recent technique is IP anycast.
We estimate that, by now, there are actually close to 100 physical DNS root servers, even
though there are only 13 root server addresses [2].
Just under the root are servers for the Internet’s 260-or-so top-level domains (TLDs),
such as .com, .nz, and .us. TLD servers are similar to the roots in their requirements,
architecture and trafﬁc levels. In fact, many of them are signiﬁcantly busier. The TLD
zones contain referrals to second-level domains (SLDs), and so on.
Why does the Internet need so many root and TLD servers? Is this simply a fact of
life, or have we inadvertently created a monster? One of the reasons that we need so many
is to carry the normal request load, including a high proportion of unanswerable requests
from poorly conﬁgured or buggy DNS resolvers. Another is to provide resilience against
increasingly common distributed denial of service attacks targeting the system. As the
C. Barakat and I. Pratt (Eds.): PAM 2004, LNCS 3015, pp. 147–157, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004
148
D. Wessels et al.
top of the DNS hierarchy becomes more distributed, it is harder for attackers to affect
the service.
In this paper, we focus on the behavior of various DNS caching implementations. The
behavior of DNS caches is important because they generate almost all of the queries to
authoritative (root, TLD, SLD, etc) servers on the Internet. Authoritative servers do not
query each other. Stub resolvers should always send their queries to caching nameservers,
rather than talk directly to the authoritative servers.
2 Measurements of Live DNS Trafﬁc
For a case study of workday DNS load in academic networks we used the NeTraMet tool
[3] to capture samples of DNS trafﬁc (Table 1) at the University of California San Diego
(UCSD), USA, the University of Colorado (UCol), USA and the University of Auckland
(UA), New Zealand. We conﬁgured NeTraMet to select only DNS packets to/from the
root and gTLD1 nameservers; these form a small set of high-level nameservers with
known IP addresses. Each DNS sample provides the following information: time (in
ms) when a query was sent or a response was received, which root or gTLD server it
was sent to, the source host IP address, and a query ID. The UA and UCol samples (but
not UCSD) include query types. The most recent samples collected in December 2003
also captured top level and second level domains for each query.
Table 1. Samples of DNS trafﬁc to root and gTLD servers
Samples
UCSD U. of Auckland U. of Auckland U. Colorado
Feb 03
Dec 03
Sep 03
Dec 03
Query rates to roots, per min
Query rates to gTLDs, per min
Number of query sources
Sample duration, hrs
214
525
147
48
29
67
19
64.5
Median response times (from roots) 5–200
60–340
% due to 3 highest users
% of a queries
58
n/a
85
23
10
76
42
157.3
5–290
74
70
9
70
1
157.2
27–180
100
81
The number of source hosts addressing the roots and/or gTLDs in the UCSD and
UA data is larger than we expected. However, the distribution of queries among sources
is highly skewed. Typically, the bottom half of sources produce < 1% of the overall
query load while a small number (3–5) of top sources is responsible for 80% or more
of the total trafﬁc. Network administrators need to exercise proper control of those few
high level sources that dominate external DNS trafﬁc and to optimize the software those
source hosts use. At the same time, end user hosts should be conﬁgured to send DNS
requests to (internal) local nameservers, rather than to root and TLD servers directly.
1 The gTLDs are: com, net, org, edu, mil, int, and arpa.
Measurements and Laboratory Simulations of the Upper DNS Hierarchy
149
Typically, the DNS trafﬁc is dominated by a queries (hostname-to-address lookups)
as in our December samples. This observation is in general agreement with other mea-
surements (cf. [4], [5]). The ﬁrst sample of UA DNS trafﬁc was unusual because two of
the three top users generated only ptr queries (address-to-hostname lookups). With the
exception of these two sources, a queries constitute 58% of the remaining trafﬁc.
2.1 Response Time and Server Selection
We found that in all samples the root server response time distributions has a long tail,
except for G root, which is bimodal. Table 1 shows the range of median root server res-
ponse times experienced by the busiest query source in each data set. For all nameservers
at the same location response time distributions are very similar since the response time
correlates with the geographical distance from the point of measurements to a given root
server [6]. Nowadays, due to proliferation of anycast root server instances, the exact
geographical location of a given root server has become a moot point.2 Response time
actually correlates with the distance to the closest anycast node. For example, the UA
nameservers have a median response time of only 5 ms from the anycast F root node in
New Zealand.
As we show in the next section, different resolvers utilize different nameserver sel-
ection algorithms. For the live data, we studied the probability of selecting a given root
server versus its median response time (Figure 1). For each query source, response ti-
mes were normalized by the minimum response time to the closest root server. When
possible, we veriﬁed the software being used by hosts at each site. UCol was running
BIND8, while both UA nameservers shown were running BIND9. For the UCol and
UA#2 sources the probability is approximately inversely proportional to the response
time, as expected. However, the UA#1 sent its queries (predominantly ptr) to the root
servers in nearly uniform manner ignoring the latency. The UCSD#1 nameserver was
using some Windows software and we were not able to obtain any indicative informa-
tion about UCSD#2. It appears that both UCSD hosts more or less ignore response times
when choosing which root servers to use.
2.2 Query Rates, Duplication, and Loss of Connectivity
We analyzed our most detailed samples from December 2003 in order to estimate the
contribution of repeated queries to the total trafﬁc. Table 2 compares the behavior of a
queries to root and gTLD servers sent by the UCol nameserver and by the busiest UA
source. For both of them a queries make up about 80% of the total trafﬁc they generate.3
For both sources, the bulk of a queries to gTLD servers receive positive answers
and, on average, there are 3 to 4 queries per each unique SLD. However, only 2% of
a queries sent to root servers by UCol host were answered positively, with an average
of 6.7 queries per each unique TLD. This result is in agreement with ﬁndings of [7]
2 [2] gives locations of anycast nodes for the root servers. Unfortunately there is no straightforward
way to determine which server instance any particular DNS reply packet came from.
3 Next most frequent types of queries are: for the UA source – ptr, 11%, for the UCol source –
soa, 9%.
150
D. Wessels et al.
Server selection vs. response time
s
e
i
r
e
u
q
f
o
%
 100
 10
 1
 0.1
 0.01
 1
UCol, Dec 03
UA#1, Sep 03
UA#2, Dec 03
UCSD#1, Feb 03
UCSD#2, Feb 03
 10
server response time (relative to min)
 100
Fig. 1. Distributions of queries among root servers vs. response time. Each point on the plots
shows percentage of queries sent to a given root server; the points are plotted in order of increasing
response time.
which showed that only 2% of queries reaching root servers are legitimate. The average
repetition rate corresponds to about one query per TLD per day which seems reasonable.
At the same time, queries for invalid domains seem to be repeated unnecessarily.
In contrast, the busiest UA nameserver received 77.7% of positive answers from the
roots. The average number of queries per TLD is 125.4. For example, there were 5380
queries for .net or approximately one query every two minutes. Each one received a
positive answer and yet the server continued asking them.
Table 2. Statistics of a queries
Samples, 6.5 days long
U. Auckland
U. Colorado
Queries to
positive answers, %
av. # of q. per domain
roots
77.7%
125.4
gTLDs
94.7%
3.0
most frequent
.net, 5380 .iwaynetworks.com, 2220
roots
2.1%
6.7
gTLDs
96.3%
4.4
.mil, 293 .needsomedealz.com, 6014
97.1%
3.4%
negative answers, % 20.6%
.cgs, 325
most frequent
1.9%
.coltframe.com, 134
.dnv, 13660
.jclnt.com, 7694
Our data indicate that repeated queries constitute a considerable fraction of the overall
load and that both positive and negative caching can be improved. Using the actual query
load observed, we simulated an ideal case when each query would be asked only once:
(a) it is not repeated impatiently before the answer comes back, and (b) the answer,
whether negative or positive, is cached and the same query is not repeated until its TTL
expires. The results (Table 3) indicate that proper timing and caching of queries possibly
can reduce the load by a factor of 3 or more. However, this simulation obviously is
too optimistic since some repetition of queries to root/TLD servers is unavoidable, for
Measurements and Laboratory Simulations of the Upper DNS Hierarchy
151
Table 3. Simulated query rates
U. Auckland
roots gTLDs
U. Colorado
Samples
roots gTLDs
Queries to
real trafﬁc 96,759 722,265 86,229 658,784
TTL = 3 h 20,833 306,566 30,243 308,002
TTL = 24 h 10,140 211,914 26,165 229,080
example in a case when queries for aaa.foo.com and bbb.foo.com immediately follow
each other and neither .com nor foo.com are cached.
Loss of connectivity is another cause of repeated queries. In all our measurements,
the loss of packets was very low, typically < 0.5%. However, there was a one hour period
of connectivity loss in the University of Auckland data on 6 Dec 03, during which their
nameservers sent queries but no answers came back. The query rate increased more than
10-fold, but quickly subsided back to normal levels as soon as connectivity resumed
(Figure 2).
Requests sent to gTLD servers
(per 2 minute interval)
1600.0
1400.0
1200.0
1000.0
800.0
600.0
400.0
200.0
s
t
s
e
u
q
e
r
f
o
r
e
b
m
u
N
0.0
Sat
12-06
11:20
 A
 B
 C
 D
 E
 F
 G
 H
 I
 J
 K
 L
 M
Sat
12-06
11:40
Sat
12-06
12:00
Sat
12-06
12:20
Sat
12-06
12:40
Sat
12-06
13:00
Sat
12-06
13:20
Sat
12-06
13:40
Time (UTC)
Fig. 2. Query rates to gTLD servers. Each subsequent plot is shifted up by 100. There were two
periods of connectivity loss when no answers were received: from 11:40 till 11:50 and from 12:00
till 13:00. During these periods query rates increased considerably.
3 Laboratory Simulations
3.1 Experimental Setup
We set up a mini-Internet in our laboratory to study the behavior of DNS caches. Do
they distribute the load as they should? What happens when the cache cannot talk to any
root servers? In our setup (Figure 3), three systems on top mimic the authoritative root,
TLD, and SLD servers. These systems all run BIND8. Another system, “Cache” in the
152
D. Wessels et al.
middle, runs the various DNS caches that we test, one at a time. The bottom system,
“User”, generates the test load of end-user queries.
192.168.2.1−13
roots
172.16.2.2
192.168.3.1−254
TLDs
172.16.2.3
192.168.4.1−254
SLDs
172.16.2.4
WAN
BIND8