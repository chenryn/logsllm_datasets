  if
+  = 
Pq
∪
Pq
tj
c is a processor.
The  knowledge  propagation  graph  is  used  during  the
performability computation.
5.  Performability Algorithm
The  expected  steady  state  reward  rate  of  a  layered
system  with  a  separate  fault  management  architecture  is
now computed as follows:
Step  (1):  Obtain  the  knowledge  propagation  graph  K
corresponding to the specified MAMA model, as described
in Section 4.
(2):  Obtain 
fault  propagation  graph  G
Step 
corresponding  to  the  layered  FTLQN  model,  as  described
in Section 3.
the 
Step  (3):  For  each  service  node  s  in  G  and  for  each  leaf
,  compute  knowl,t(s)  from  the  knowledge
node 
propagation graph K. 
L s( )
∈
l
Step  (4):  Determine  the  set,  Z,  of  all  possible  distinct
operational  configurations  Ci,  from  G  and  compute  the
probability,  Prob(Ci),  of  the  system  being  in  each  such
configuration  Ci  as  follows.  Let  the  total  number  of
processors and tasks in the MAMA model and the FTLQN
model  be  N.  Enumerate  all  2N  states  of  the  system.  For
each state Γ = {γ
c = 0 or 1, if the root
node  r  is  working  as  per  Definition-1,  generate  a
configuration C containing all the non-leaf nodes which are
N}, where γ
2, ...γ
1, γ
working and is in use. Compute Prob(C) = 
N
∏
Prob
(c is in
state  γ
C′
c).  If  there  exists  a  configuration 
C′
=  C,  then  set  Prob(
)  =  Prob(
C′
1=
Z∈
c
C′
  such  that
)  +  Prob(C)  else
Z C∪=
Z
.
Step (5): Each 
Z∈
Ci
 determines the service alternatives,
so  it  defines  an  ordinary  Layered  Queueing  Network
model. Generate  the  LQNs  and solve them  [14].  From the
performance measures  assign  a  reward Ri to configuration
Ci.
Step  (6):  Compute the  expected  reward  rate  of  the  system
as R =
(
RiProb Ci
)
.
∑
Z∈
Ci
6.  A Comparison of Four Fault Management 
Architectures
This  section  studies  the  effect  of  four  different  fault
management  architectures  on  the  expected  steady-state
reward  rate  of  a  distributed  system.  The  architectures  are
selected  according  to  the  classification  given  in  [23],  for
management 
the  manager-agent
paradigm.
systems  based  on 
6.1. Architectural Model of a fault-tolerant layered 
distributed system
Let  us  consider  the  layered  system  shown  in  Figure  1.
Let  us  assume  the  independent  failure  probabilities  for  all
the tasks and the processors to be 0.1 except UserA, UserB,
procA  and  procB  which  are  assumed  to  be  perfectly
reliable.  Let  us  consider  the  mean  total  demand  for
execution on the processor for entries eA, eB, eA-1, eB-1,
eA-2  and  eB-2  to  be  1,  0.5,  1,  0.5,  1,  0.5  seconds
respectively and let us assume that on average, 1 request is
made from a caller entry to the called entry per invocation
of each caller entry. Since the tasks UserA, UserB and their
associated  processors  are  assumed  perfectly  reliable,  they
are  not  monitored  and  are  not  shown  in  any  of  the
management architectures described next.
6.2. Four fault management architectures
Architecture 1: Centralized Management Architecture 
tasks,  makes 
The  centralized  architecture  [24,  25]  is  the  most
commonly  used.  A  single  manager  handles  all  agents  and
application 
initiates
reconfiguration. Figure 7 shows a centralized management
architecture  for  the  system  in  Figure  1.  The  central
manager m1 manages  all  the  tasks AppA, AppB,  Server1,
Server2 and their associated processors.
the  decisions  and 
Let  us  consider  the  failure  probability  of  the  manager
and all the agents be 0.1. In order to analyze the system in
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
proc1:Proc
c1:AW
proc2:Proc
c2:AW
AppA:AT
ag1:AGT
AppB:AT
ag2:AGT
c5:Ntfy
c6:Ntfy
c12:SW
c13:Ntfy
c16:Ntfy
proc5:Proc
c11:AW
m1:MT
c15:SW
c14:AW
c9:AW
c7:AW
c8:SW
c10:SW
proc3:Proc
c3:AW
proc4:Proc
c4:AW
Server1:AT
ag3:AGT
Server2:AT
ag4:AGT
operational configurations.
The  expected steady-state reward rate for the  system is
obtained  as  approximately  0.55/secs  whereas  in  case  of
perfect knowledge, it is found to be 0.85/secs.
To  illustrate  a  situation  with  partial  coverage  of  a
failure,  consider  the  failure  of  Processor  proc3  (which
supports  Server1)  when  the  system  is  fully  operational.
With perfect knowledge, this always leads to configuration
C6. However under any  management architecture, if  agent
ag2  (which  is  connected  to  AppB)  has  also  failed,  then
AppB  does  not  know  about  the  failure  of  proc3  and  does
not  reconfigure  to  use  Server2;  this  leads  to  configuration
C2  instead.  In  configuration  C2,  the  A  group  of  users  are
operational  and  the  B  group  are  not.  Thus  the  failure  is
partly covered and the system has reduced functionality. 
Figure 7. MAMA Model of a centralized management 
architecture for the system in Figure 1.
Table 1: Configuration Probabilities (for Centraliz-
ed Management) and Rewards 
Figure  1  with  the  centralized  management  architecture  in
Figure 7, we do the followings: 
For  node 
serviceA 
in 
the  corresponding 
fault
propagation graph G in Figure 5, we compute:
knowServer1,AppA =  
oc
c = {c3,ag3,c8,m1,proc5,c13,ag1,c5,AppA,proc1,proc3}
(since there is only one minpath from Server1 to AppA in
the corresponding knowledge propagation graph). 
knowServer2,AppA =  
oc
c = {c4,ag4,proc4,c10,m1,proc5,c13,ag1,c5,AppA,proc1}
knowproc3,AppA =  
oc
c = {c7,m1,proc5,c13,ag1,c5,AppA,proc1}
knowproc4,AppA =  
oc
c = {c9,m1,proc5,c13,ag1,c5,AppA,proc1}
We  repeat  the  steps  for  node  serviceB.  Then,  using  these
know functions and the information in Figure 5, we obtain
six distinct operational configurations of the system as:
C1: UserA operational using  Server1. UserB is failed.
C2: UserA operational using  Server2. UserB is failed.
C3: UserB operational using  Server1. UserA is failed.
C4: UserB operational using  Server2. UserA is failed.
C5: UserA, UserB operational. Both using  Server1.
C6: UserA, UserB operational. Both using  backup Server2.
We generate an LQN model for each of the operational
configurations  and  solve  these  models  using  LQNS  tool
[14]  for  determining  the  performance  measures.  Table  1
shows  the  probability  and  the  associated  reward  for  each
Perfect 
Knowledge
Prob(Ci)
0.125
0.024
0.125
0.024
0.531
0.100
0.071
Centralized 
Mgmt
Prob(Ci)
0.117
0.021
0.117
0.021
0.314
0.057
0.353
Configur
ation Ci
C1
C2
C3
C4
C5
C6
System 
Failed
Reward Ri 
= Total Throughput 
(A and B users)
0.5
0.5
0.5
0.5
1.11
1.11
0
This  example  also  shows  that  the  failures  in  the
management architecture increase the probability of system
being failed or of reduced functionality. 
Architecture 2: Distributed Management Architecture
architecture 
The  distributed 
[26]  has  multiple
management domains with a manager for each one. When
information  from  another  domain  is  needed,  the  manager
communicates with its peer systems to retrieve it. Figure 8
shows  a  distributed  management  architecture  for  the
system  in  Figure  1.  In  Figure  8,  there  are  two  domain
managers, dm1  (for the entities AppA, Server1, proc1 and
proc3) and dm2 (for the entities AppB, Server2, proc2 and
proc4). They communicate through the notify connections.
Results are given below in Section 6.3. 
Architecture 3: Hierarchical Management Architecture
The  hierarchical  architecture  [27,  28]  also  relies  on
multiple (domain) managers and introduces the concept of
the manager of managers (MOM). Each domain manager is
connected  to  the  rest  of  the  network  only  through  the
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:50 UTC from IEEE Xplore.  Restrictions apply. 
proc1:Proc
:AW
proc2:Proc
:AW
AppA:AT
ag1:AGT
AppB:AT
ag2:AGT
proc1:Proc
:AW
proc2:Proc
:AW
:Ntfy
:Ntfy
AppA:AT
ag1:AGT
AppB:AT
ag2:AGT
:AW
:Ntfy
:SW
:Ntfy
:SW
:AW
dm1:MT
proc5:Proc
:AW
:SW
proc3:Proc
:AW
:Ntfy
:Ntfy
dm2:MT
proc6:Proc
:AW
:SW
proc4:Proc
:AW
Server1:AT
ag3:AGT
Server2:AT
ag4:AGT
Figure 8. MAMA Model of a distributed management 
architecture for the system in Figure 1. dm1 and dm2 
are two peer domain managers.
MOM.  The  MOM  operates  on  a  higher  hierarchical  level,
retrieving  information  from  and  coordinating  the  domain
managers.  Unlike  the  distributed  architecture,  the  domain
managers  do  not  communicate  directly.  Figure  9  shows  a
hierarchical  management  architecture  for  the  system  in
Figure  1,  with  two  domain  managers,  dm1  (for  AppA,
Server,  proc1  and  proc3)  and  dm2  (for  AppB,  Server2,
proc2  and  proc4),  and  a  manager  of  managers  mom1.
Results are given below in Section 6.3.
Architecture  4:  General  “Network”  Management
Architecture
structure, 
The “network” architecture [27] is a combination of the
distributed  and  the  hierarchical  architectures.  Instead  of  a
purely  peer-structure  or  hierarchical 
the
managers  are  organized  in  a  network  scheme.  Figure  10
shows  a  network  management  architecture  for  the  system
in Figure 1. In Figure 10, there are two domain managers,
dm1 and dm2, and two integrated managers, im1 and im2.
dm1  manages  the  task  Server1.  dm2  manages  the  task
Server2.  im1  handles  the  task  AppA,  the  managers  dm1,
dm2  and  the processors  proc3  and  proc4.  im2 handles the
task  AppB,  the  managers  dm1,  dm2  and  the  processors
proc3  and  proc4.  The  connections  between  managers  can
have any topology. 
6.3. Results
Suppose  the  agents  and  the  managers  in  the  four  fault-
management 
have
independent failures and failure probability 0.1. Define the
reward rate Ri for configuration Ci as
architectures 
described 
above 
:Ntfy
:Ntfy
:AW
:Ntfy
:SW
:Ntfy
:SW
:AW
dm1:MT