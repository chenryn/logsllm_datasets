s
)
e
n
o
c
(
l
m
a
r
g
o
r
P
.
5
Experiment Manager
Load Generator
Test Plane
d 
re
Mirro
t
s
e
u
q
re
Clone Controller VM(s) 
SDN Controller
Production 
+ synthetic 
workload 
SDN 
Switch
A B
SDN 
Switch
SDN 
Switch
SDN 
Switch
C
D E
SDN 
Switch
Emulated 
data plane
Emulated topology (mininet VMs)
System under Test
NFV/APP VM
NFV/APP
Controller VM Instance(s)
SDN Controller
l
e
n
a
p
a
t
a
D
n
o
i
t
c
u
d
o
r
P
SDN 
Switch
A B
SDN 
Switch
SDN 
Switch
SDN 
Switch
C
D E
SDN 
Switch
Programmed path 
(provisonedintent)
Production SDN – network under test
)
s
e
h
c
t
i
w
s
N
D
S
/
r
o
t
a
u
m
e
m
a
r
g
o
r
p
l
l
(
y
g
o
o
p
o
T
e
n
o
C
3
l
Fig. 4. Network Diagram of the SCP-CLUB control network.
IV. IMPLEMENTATION
Figure 4 shows a network diagram of how the test plane and
production control plane are interconnected, highlighting the
main components and main steps. In the following we describe
the internals of the Topology Generator and Load Generator
that enable the in-production testing features of SCP-CLUB.
A. Topology Manager
The Topology Manager consists of a set of bash and python
scripts controlled through a Java REST application to set up
the data plane topology used in the testing. It incorporates
the Mininet tool [14] to emulate various types of data planes.
It can also program real SDN switches to run tests with
real topologies, by interacting with the standard OpenFlow
management API.
The Topology Manager can work in two modes: i) production
mode, to run tests using a virtualized clone of the production
data plane SDN topology, and ii) synthetic mode, to run tests
with a topology deﬁned by the user.
The generation of a clone of the real topology from the
production topology in the production mode (step 3 in Figure
4) is shown in Algorithm 1. The operation starts after the
deployment of all testing services in step 2. The Topology
Manager collects the details of the production data plane
topology, including host IP and MAC addresses, IP of the SDN
controllers, SDN switches, and physical links between switches
(lines 1–4), by querying the production SDN controller. The
primitives to gather this information are provided by the SDN
controller as northbound REST APIs [5], [6].
in doing this,
Subsequently, the Topology Manager invokes the Campaign
Manager to create the VMs for hosting the SDN controller
instances (line 5). Note that
the Campaign
Manager can alter the VMs in terms of number of CPUs and
Algorithm 1 Topology Generation
Input: networkUnderTest nut, campaignManager CM, LoadGenerator LG
1: hosts = nut.getAllHosts(nut)
2: clusterMembers = nut.getControllers(nut)
3: links = nut.getLinks(nut);
4: devices = nut.getDevices(nut);
5: testingCluster = CM.cloneAndDeployInstances(clusterMembers);
6: testTopology=cloneTopology(hosts,devices,links,clusterMembers);
7: programTopologyControllers(testTopology.getIPs());
8: LG.initTopology(testTopology,testingCluster,nut.getIntents());
645
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 2 Load Generation Startup - Production Mode
Input: networkUnderTest nut, campaignManager CM, LoadGenerator LG, Experiment-
Manager EM
CM.mirror(port, LGSwitchPort);
LGTestNIC = CM.createNIC(LG, instance.getCtrlNic());
CM.assignSwitch(LGTestNIC, productionCtrlPlaneSW, TestVLAN);
VMCtrlPlaneTestNic = CM.createNIC(VM);
CM.assignSwitch(VMCtrlPlaneTestNic, LGCtrlPlaneTestSW, TestVLAN);
1: testingResourcesInstances = CM.getTestingInstances(nut, LG));
2: productionCtrlPlaneSW = CM.getCtrlSW(nut);
3: testVLAN = CM.getNewVLAN();
4: LGSwitchPort = CM.assignSwitch(CM.createNIC(LG), productionCtrlPlaneSW);
5: CM.setVLAN(LGSwitchPort, testVLAN);
6: LGCtrlPlaneTestSW = CM.createNewSwitch();
7: for all instance in CM.getProductionInstances(nut) do
8:
9:
10: end for
11: for all VM in testingResourcesInstances do
12:
13:
14: end for
15: InstancesPorts = CM.getProductionInstancesPorts(nut, productionCtrlPlaneSW);
16: for all port in productionInstancesPorts do
17:
18: end for
19: EM.programInstances(testingResourcesInstances);
amount of RAM, and the number of controllers in the clus-
ter, depending on the parameters speciﬁed by the experiment
proﬁle. After this step, the Topology Manager generates the
emulated topology (line 6) that reﬂects the production topology
with i) the same number of switches; ii) the same links between
switches, keeping the same capacity and latency ﬁgures; iii) the
same number of hosts, keeping ports, MAC and IP addresses
as in the production topology. The switches of the generated
topology are programmed with the IPs of the controller instance
VMs generated by the Campaign Manager (line 7). Finally, the
Topology Manager invokes the Load Generator to initialize the
topology by providing the intents installed in the production
topology (line 8).
B. Load Generator
The Load Generator (re)produces realistic workload con-
ditions and generates speciﬁc control-plane requests for the
system under test in the form of: intent install to set up
new connections, and intent withdraw to cancel a connection
previously provisioned. An ”install” requests a bidirectional
connection between two hosts in the data plane. Both re-
quests can work in Production Mode (requests come from
the production network), Synthetic Mode (requests come from
the Load Generator), and Hybrid mode (requests come from
the production network and from synthetic load at the same
time, so as to enable scenarios like surge of requests and
overload conditions). In any operational mode, data plane trafﬁc
is emulated for each intent using various proﬁles – including
burst, mice and elephant ﬂows - implemented with iperf3. More
details on data plane trafﬁc emulation are omitted due to the
focus of the paper on the control plane performance.
Intent install and withdraw requests are submitted in batch
units, delivered to the controller instances using a load balanc-
ing algorithm. Speciﬁcally, the Load Generator uses a Weighted
Round Robin policy, with each server weighted according to
its response time; the longer the response time, the lower the
weight an instance is assigned. Then, the policy allows to select
a controller instance according to the measured weights. An
alternative simple Round Robin policy is also provided. Policies
are selected by the user in the experiment proﬁle ﬁle. Internally,
the Load Generator manages a pool of TCP connections for
each controller instance, and reuses them to send its requests,
hence reducing the overall internal overhead.
Production Mode – The Load Generator acts as a proxy
for the production network intent installation/withdraw requests
that are mirrored from the production network (Figure 4)
using the steps in Algorithm 2. Consistently with real non-
federated deployments, we assume that all the production SDN
controller instances of the network under test belong to the same
subnet/VLAN group, and are connected to a single distributed
virtual switch in the cloud infrastructure. More in detail, the
Load Generator connects to the production control plane switch
(line 4), and sets the port to tag the trafﬁc with a different
VLAN (line 5) to separate the test network from the produc-
tion network. Then, it creates as many virtual network cards
connected as the total number of controllers in production (line
8), connects them to the production control plane switch (line
9) keeping i) the same IP(s) as in the production controller(s),
and ii) different VLAN (TestVLAN in line 9). Successively, it
creates a new switch to form the control plane of the testing
resources (line 9 - referred to as virtual control plane switch in
Figure 4), and, for each of the virtual machines in the testing
resource pool, it creates a new virtual network interface card
(line 12) and connects it to the newly created switch (line 13).
Finally, it requests to the Campaign Manager to mirror all the
ports of all the virtual machines of the network under test (line
17 - step 4 in Figure 4), and requests the Experiment Manager
to conﬁgure the SDN controllers to test (line 19).
It is worth noting that the port mirroring takes place only
on the control plane switch, and targets the production control
plane trafﬁc that
is now copied one-way over a different
VLAN (testVLAN). This isolates the trafﬁc across the test
and production environment, and allows to conﬁgure the Load
Generator with the same IP(s) used for the production con-
trollers. This way, the Load Generator can: i) receive the same
requests addressed to the production controllers; ii) overlap
the production trafﬁc with synthetic requests to alter the rates
and generate different
iii) perform the
balancing of requests across the controllers in the test resource
pools, while iv) keeping the isolation between the system under
test and the production controllers.
test cases/scenarios;
Synthetic Workload – Each request can be created either by
replaying a user-provided trace workload ﬁle, or by randomly
selecting two edge switches (i.e., switches having at least one
host connected). In the latter case the Load Generator generates
requests by creating intents between two randomly selected
hosts in the topology. Although it is legitimate for an intent to
involve hosts connected to the same switch, the Load Generator
creates requests involving only hosts connected to different
switches, to actually trigger the creation of a network path.
Randomly generated intent requests include constant bandwidth
and latency value, while user-provided trace ﬁles can include
any value for bandwidth and latency requests.
Synthetic requests are generated according to three proﬁles:
1. Steady-State Response mode: The Load Generator submits
a constant stream of intent installation and withdrawal requests
using a token bucket
the load, generating new
requests only if the bucket is not empty.
to control
2. Impulse Response mode: The Load Generator i) submits
a batch of Smax number of install requests, ii) waits for all
of them to be processed, iii) submits the batch of withdrawal
requests, iv) waits for all the withdrawals to be executed, and v)
ﬁnally removes all the intents and start a new batch of requests.
3. Probabilistic Mode: Rates of intents installation and
646
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
Networks/VLANs
1 NFV/App control Plane 
2
Production Control Plane
3
ONOS control plane VM
4
5
6
Production Network
VM management
Production Partition SDN Management Plane
Testing Partition SDN Management Plane
On-demand testing 
resources
72x Nokia airframe blade:
 2x Intel Xeon E5-2680 v4 
12 cores  (24 logical)
 256GB DDR4
 2 Intel x540 10GbaseT
 6 Intel X540 SFP+ 10Gb
NFV/ 
ONOS 
APP VM
ONOS v1.10 Cluster
Northbound  Interface REST API
ONOS VM
East-
West 
interface
ONOS VM
1
1 2
1 2
5
5
Southbound 
interface
requests
Load 
Generator 
VM
1 3
4
ONOS Cluster (under test)
Network/Host  Emulation Cluster
Northbound Interface
ONOS 
VM
ONOS 
VM

3
4
6
3
64
Mininet / 
Iperf
VMs 
3 6

Topology 
Manager 
VM
Experiment 
Manager 
VM
Data 
Collector
VM
Campaign 
Manager 
VM
4
6
4 6
4 6
4 7

Data Plane 
Port Group 1
1
2
1 1
Control Plane 
Port Group 1
2 
Port 
mirroring
1
3
3
4
3 4
4
Testing Plane Port Group 1
4 3
4 4 4
vCloud Distributed Switch
Vmware vSphere Cloud / ESXi v6.5

6
6
6
6 6
5 5
6
SDN Management Port 
5
Group 
6
7
vSphere/vSAN/ 
ESXi
management 
network vSwitch
To SDN Switch Management (Production Partition):
 Edgecore switches 5712-54x 10G/40G-SFP+/QSFP+
 Nokia R-SWI10GP-S1 (10GBaseT)
To SDN Switch Management (Testing Partition):
 17x SDN Centec v350 10G-SFP+
Fig. 5. Telco Cloud Setup.
withdrawal follow speciﬁc probability distributions that can be
speciﬁed by the user in the experiment proﬁle.
V. TELCO CLOUD SET UP FOR IN-PRODUCTION TESTING
We describe the telco cloud set up to test SCP-CLUB, and
how the cloud IaaS can be leveraged to enable in-production
testing of the SDN control plane. The setup includes a full-
ﬂedged multi-vendor SDN network controlled by ONOS run-
ning over VMware vCloud and Nokia Airframe hardware.
A. System Setup
The experimental telco cloud infrastructure setup to design
and test SCP-CLUB is equipped with 20 blade servers, con-
nected to two high-speed subnetworks:
i) the management
subnetwork, used exclusively to conﬁgure and control the blade
servers, and ii) the data exchange network. Each server is con-
nected to two 1/10 GbE Ethernet Nokia Management switches,
for the management interconnections within a rack, and to two
SFP 10/40 GbE Nokia Ethernet switches, for data exchange
within the rack. The communication across racks is ensured
by means of two additional SFP 10/40 GbE Nokia Ethernet
switches. Virtualization is provided by VMware ESXi 6.5
hypervisor. The deployment uses ONOS version 1.10 instances
installed on Ubuntu 16.04.2 LTS. Figure 5 shows the network
diagram of the telco cloud. Besides the emulated topology, we
conduct tests with a real topology consisting of 30 V350 Centec
silicon SDN/OpenFlow switches. The switches are connected
linearly, with a total of 35 real hosts wired to 7 switches (5
hosts per switch).
B. Experiment Setup
In order to demonstrate the capabilities of SCP-CLUB, we
provided an experiment proﬁle that consist in: i) scaling up the
ONOS VMs (VSCALE) from small to medium, large and extra-
large ﬂows, using the conﬁguration of the testbed machines
listed in Table I, and ii) scaling out ONOS, by varying the
number of instances forming the cluster from 1 to 3, 5 and 7.
All the experiments are executed on a pre-emptible low-priority
partition of the telco cloud.
The Experiment Manager is instructed to control the Load
Generator and the Topology Manager to vary the load level
(LLEVEL) and the size (TSIZE) of the data-plane network,
respectively. Speciﬁcally, the Load Generator is conﬁgured to
vary the request rate from 1,000 up to 7,000 requests/s with
increments of 1,000 requests/s. The Topology Manager is set
to emulate two types of linear data-plane topology: i) a small
topology, consisting of 10 switches connected linearly, and 40
hosts uniformly distributed across the switches, and ii) a large
topology, consisting of 30 switches and 90 hosts. Mininet 2.2
has been adopted to emulate the data plane. We also instructed
the Topology Manager to reproduce a production topology
using 16x V350 Centec silicon SDN/OpenFlow switches [15].
For sake of comparison, we considered a linear topology
provisioned in the production network with a total of 35 real