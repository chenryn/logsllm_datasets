P
FP
M CR MI
2.9.8.2
2016
46, 526
135.30
16.14
8.38
494.24 3.31 10.32 63.27
4.4 rc-1 2016
160, 890
497.81
26.47
18.81
2, 265.35 3.40 17.08 52.42
2016
2015
2016
2016
2016
2016
280, 051
890.86
33.03
26.97
4, 520.10 3.02 23.42 61.56
319, 173
1, 021.97
34.80
29.37
3, 322.05 3.06 27.33 68.88
378, 246
1, 221.45
37.24
32.80
4, 996.44 3.37 12.15 41.84
604, 398
1, 998.02
44.89
44.51 11, 365.09 2.52 27.69 52.42
714, 085
2, 380.39
47.98
49.61 10, 669.97 2.79 33.57 63.87
851, 350
2, 863.02
51.47
55.63 16, 566.78 2.96 17.55 66.60
2015 6, 378, 290
2, 3721.97 114.95 206.37 90, 278.41 2.10 31.24 50.57
Snort
Bash
Apache
IPtables
Git
Octave
2.4.19
1.6.0
2.8
4.0.1
0.99.1
ClamAV
Cocos2d-x 3.10
5.3
gcc
The ﬁrst natural comparison refers to the size of the source code. Various
malware samples from 2007 on (e.g. Zeus, KINS, Pony2, or SpyNet) have SLOC
counts larger than those of Snort and Bash. This automatically translates,
according to the COCOMO model, into similar or greater development costs.
The comparison of function point counts is alike, with cases such as Rovnix and
KINS having an FP greater than 1000, or SpyNet, with an FP count comparable
to that of Bash. In general, only complex malware projects are comparable in
size and eﬀort to these two software packages, and they are still far away from
the remaining ones.
In terms of comment-to-code ratio, the ﬁgures are very similar and there is no
noticeable diﬀerence. This seems to be the case for the cyclomatic complexity, too.
To further investigate this point, we computed the cyclomatic complexities at the
function level; i.e., for all functions of all samples in both datasets. The histograms
of the obtained values is shown in Fig. 6. Both distributions are very similar, with a
A Look into 30 Years of Malware Development
341
Fig. 6. Histograms of the cyclomatic complexity values computed at the function level
for both malware and regular software samples.
clear positive skewness. A Chi-squared and two-sample Kolgomorov-Smirnov tests
corroborate their similarity for a signiﬁcance level of α = 0.05.
More diﬀerences appear in terms of maintainability. Up to 12 malware sam-
ples show M I values higher than the highest one for regular software—IPtables,
with M I = 68.88. In general, malware samples (particularly the most recent
ones) seem to have slightly higher maintainability indexes than regular soft-
ware. As discussed before, two notable exception are Cairuh and Hexbot2 with
surprisingly low values.
5 Discussion
We next discuss some aspects of the suitability of our approach, the potential
limitations of our results, and draw some general conclusions.
Suitability of Our Approach. Software metrics have a long-standing tradition
in software engineering and have been an important part of the discipline since
its early days. Still, they have been subject to much debate, largely because of
frequent misinterpretations (e.g., as performance indicators) and misuse (e.g.,
to drive management) [34]. In this work, our use of certain software metrics
pursues a diﬀerent goal, namely to quantify how diﬀerent properties of malware
as a software artifact have evolved over time. Thus, our focus here is not on the
accuracy of the absolute values (e.g., eﬀort estimates given by COCOMO), but
rather on the relative comparison of values between malware samples, as well as
with benign programs, and the trends that the analysis suggests.
Limitations. Our analysis may suﬀer from several limitations. Perhaps the most
salient is the reduced number of samples in our dataset. However, as discussed in
Sect. 3, obtaining source code of malware is hard. Still, we discuss 151 samples,
which to the best of our knowledge is the largest dataset of malware source code
342
A. Calleja et al.
analyzed in the literature. While the exact coverage of our dataset cannot be
known, we believe it is fairly representative in terms of diﬀerent types of malware
(one remarkable exception is ransomware, for which we were not able to ﬁnd any
samples). Another limitation is selection bias. Collection is particularly diﬃcult
for newest samples and more sophisticated samples (e.g., those used in targeted
attacks) have not become publicly available and thus escape our collection. We
believe those samples would emphasize the increasing complexity trends that we
observe.
Main Conclusions and Open Questions. In the last 30 years the complex-
ity of malware, considered as a software product, has increased considerably.
We observe increments of nearly one order of magnitude per decade in aspects
such as the number of source code ﬁles, source code lines, and function point
counts. One question is whether this trend will hold in time. If so, we could soon
see malware specimens with more than 1 million SLOC. On the other hand,
evolving into large pieces of software involves a higher amount of vulnerabilities
and defects. This has been already observed (and exploited), e.g., in [14,17]. In
addition, such evolution requires larger eﬀorts and thus possibly larger develop-
ment teams. While we observe the trend we have not examined in detail those
development teams. For this, we could apply authorship attribution techniques
for source code [15,19]. More generally, the results shown in this paper pro-
vide a quantiﬁed evidence of how the malware development industry has been
progressively transforming into a fully ﬂedged engineering.
6 Related Work
While malware typically propagates as binary code, some malware families have
distributed themselves as source code. Arce and Levy performed an analysis
of the Slapper worm source code [10], which upon compromising a host would
upload its source code, compile it using gcc, and run the compiled executable. In
2005, Holz [22] performed an analysis of the botnet landscape that describes how
the source code availability of the Agobot and SDBot families lead to numerous
variants of those families being created.
Barford and Yegneswaran [11] argue that we should develop a foundational
understanding of the mechanisms used by malware and that this can be achieved
by analyzing malware source code available on the Internet. They analyze the
source code of 4 IRC botnets (Agobot, SDBot, SpyBot, and GTBot) along 7
dimensions: botnet control mechanisms, host control mechanisms, propagation,
exploits, delivery mechanisms, obfuscation, and deception mechanisms.
Other works have explored the source code of exploit kits collected from
underground forums and markets. Exploit kits are software packages installed
on Web servers (called exploit servers) that try to compromise their visitors
by exploting vulnerabilities in Web browsers and their plugins. Diﬀerent from
client malware, exploit kits are distributed as (possibly obfuscated) source code.
Kotov and Massacci [26] analyzed the source code of 30 exploit kits collected
from underground markets ﬁnding that they make use of a limited number of
A Look into 30 Years of Malware Development
343
vulnerabilities. They evaluated characteristics such as evasion, traﬃc statistics,
and exploit management. Allodi et al. [9] followed up on this research by building
a malware lab to experiment with the exploit kits. Eshete and Venkatakrishnan
describe WebWinnow [18] a detector for URLs hosting an exploit kit, which
uses features drawn from 40 exploit kits they installed in their lab. Eshete et al.
follow up this research line with EKHunter [17] a tool that given an exploit
kit ﬁnds vulnerabilities it may contain, and tries to automatically synthesize
exploits for them. EKHunter ﬁnds 180 in 16 exploit kits (out of 30 surveyed),
and synthesizes exploits for 6 of them. Exploitation of malicious software was
previously demonstrated by Caballero et al. [14] directly on the binary code of
malware samples installed in client machines.
7 Conclusion
In this paper, we have presented a study on the evolution of malware source
code over the last decades. Our focus on software metrics is an attempt to quan-
tify properties both of the code itself and its development process. The results
discussed throughout the paper provide a numerical evidence of the increase
in complexity suﬀered by malicious code in the last years and the unavoidable
transformation into an engineering discipline of the malware production process.
Acknowledgments. We are very grateful to the anonymous reviewers for constructive
feedback and insightful suggestions. This work was supported by the MINECO grant
TIN2013- 46469-R (SPINY: Security and Privacy in the Internet of You), the CAM
grant S2013/ICE-3095 (CIBERDINE: Cybersecurity, Data, and Risks), the Regional
Government of Madrid through the N-GREENS Software-CM project S2013/ICE-2731
and by the Spanish Government through the Dedetis Grant TIN2015-7013-R.
References
1. CLOC - count lines of code. http://github.com/AlDanial/cloc. Accessed 22 Sep
2015
2. Eclipse metrics plugin. https://marketplace.eclipse.org/content/eclipse-metrics.
Accessed 4 Apr 2016
3. Jhawk. http://www.virtualmachinery.com/jhawkprod.htm. Accessed 4 Apr 2016
4. Radon. https://pypi.python.org/pypi/radon. Accessed 4 Apr 2016
5. Symantec’s 2015 internet security threat report. https://www.symantec.com/
security response/publications/threatreport.jsp. Accessed 6 Apr 2016
6. Uniﬁed code counter. http://csse.usc.edu/ucc wp/. Accessed 4 Apr 2016
7. Albrecht, A.J.: Measuring Application Development Productivity. In: IBM Appli-
cation Development Symposium, pp. 83–92. IBM Press, October 1979
8. Albrecht, A.J., Gaﬀney, J.E.: Software function, source lines of code, and devel-
opment eﬀort prediction: a software science validation. IEEE Trans. Softw. Eng.
9(6), 639–648 (1983)
9. Allodi, L., Kotov, V., Massacci, F.: MalwareLab: experimentation with cybercrime
attack tools. In: USENIX Workshop on Cyber Security Experimentation and Test,
Washington D.C., August 2013
344
A. Calleja et al.
10. Arce, I., Levy, E.: An analysis of the slapper worm. IEEE Secur. Priv. 1(1), 82–87
(2003)
11. Barford, P., Yegneswaran, V.: An Inside Look at Botnets. In: Christodorescu, M.,
Jha, S., Maughan, D., Song, D., Wang, C. (eds.) Malware Detection. Advances in
Information Security, vol. 27, pp. 171–191. Springer, Heidelberg (2007)
12. Boehm, B.W.: Software Engineering Economics. Prentice-Hall, Upper Saddle River
(1981)
13. Caballero, J., Grier, C., Kreibich, C., Paxson, V.: Measuring pay-per-install: the
commoditization of malware distribution. In: Proceedings of the 20th USENIX
Conference on Security, p. 13, SEC 2011. USENIX Association, Berkeley (2011)
14. Caballero, J., Poosankam, P., McCamant, S., Babic, D., Song, D.: Input generation
via decomposition and re-stitching: ﬁnding bugs in malware. In: ACM Conference
on Computer and Communications Security, Chicago, IL, October 2010
15. Caliskan-Islam, A., Harang, R., Liu, A., Narayanan, A., Voss, C., Yamaguchi, F.,
Greenstadt, R.: De-anonymizing programmers via code stylometry. In: USENIX
Security Symposium (2015)
16. Diestel, R.: Graph Theory. Graduate Texts in Mathematics, vol. 173, 4th edn.
Springer, New York (2012)
17. Eshete, B., Alhuzali, A., Monshizadeh, M., Porras, P., Venkatakrishnan, V.,
Yegneswaran, V.: EKHunter: a counter-oﬀensive toolkit for exploit kit inﬁltration.
In: Network and Distributed System Security Symposium, February 2015
18. Eshete, B., Venkatakrishnan, V.N.: WebWinnow: leveraging exploit kit workﬂows
to detect malicious urls. In: ACM Conference on Data and Application Security
and Privacy (2014)
19. Frantzeskou, G., MacDonell, S., Stamatatos, E., Gritzalis, S.: Examining the sig-
niﬁcance of high-level programming features in source code author classiﬁcation.
J. Syst. Softw. 81(3), 447–460 (2008). http://dx.doi.org/10.1016/j.jss.2007.03.004
20. Grier, C., Ballard, L., Caballero, J., Chachra, N., Dietrich, C.J., Levchenko, K.,
Mavrommatis, P., McCoy, D., Nappa, A., Pitsillidis, A., Provos, N., Raﬁque, M.Z.,
Rajab, M.A., Rossow, C., Thomas, K., Paxson, V., Savage, S., Voelker, G.M.:
Manufacturing compromise: the emergence of exploit-as-a-service. In: Proceedings
of the 2012 ACM Conference on Computer and Communications Security, pp.
821–832, CCS 2012. ACM, New York (2012)
21. Halstead, M.H.: Elements of Software Science (Operating and Programming Sys-
tems Series). Elsevier Science Inc., New York (1977)
22. Holz, T.: A short visit to the bot zoo. IEEE Secur. Priv. 3(3), 76–79 (2005)
23. IEEE: IEEE standard for software productivity metrics (IEEE std. 1045–1992).
Technical report (1992)
24. Jones, C.: Programming Languages Table, Version 8.2. Software Productivity
Research, Burlington (1996)
25. Jones, C.: Backﬁring: converting lines-of-code to function points. Computer 28(11),
87–88 (1995)
26. Kotov, V., Massacci, F.: Anatomy of exploit kits. In: J¨urjens, J., Livshits, B.,
Scandariato, R. (eds.) ESSoS 2013. LNCS, vol. 7781, pp. 181–196. Springer,
Heidelberg (2013)
27. Lehman, M.M.: Laws of software evolution revisited. In: Montangero, C. (ed.)
EWSPT 1996. LNCS, vol. 1149, pp. 108–124. Springer, Heidelberg (1996)
28. McCabe, T.J.: A complexity measure. In: Proceedings of the 2nd International
Conference on Software Engineering, ICSE 1976, CA, USA, p. 407. IEEE Computer
Society Press, Los Alamitos (1976)
A Look into 30 Years of Malware Development
345
29. Nguyen, V., Deeds-rubin, S., Tan, T., Boehm, B.: A SLOC counting standard. In:
COCOMO II Forum 2007 (2007)
30. Oman, P., Hagemeister, J.: Metrics for assessing a software system’s maintainabil-
ity. In: Proceedings of Conference on Software Maintenance, pp. 337–344 (1992)
31. Park, R.E.: Software size measurement: a framework for counting source state-
ments. Technical report CMU/SEI-92-TR- 20, ESC-TR-92-20, Software Engi-
neering Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213,
September 1992
32. Security, P.:
27 % of
all
2015.
http://www.pandasecurity.com/mediacenter/press-releases/all-recorded-malware-
appeared-in-2015. Accessed 6 Apr 2016
recorded malware
appeared
in
33. Software Engineering Institute: C4 Software Technology Reference Guide - A Pro-
totype. Technical report CMU/SEI-97-HB-001, January 1997
34. Sommerville, I.: Software Engineering: (Update) (8th Edn.) (International Com-
puter Science). Addison-Wesley Longman Publishing Co. Inc., Boston (2006)
35. Stringhini, G., Hohlfeld, O., Kruegel, C., Vigna, G.: The harvester, the botmas-
ter, and the spammer: n the relations between the diﬀerent actors in the spam
landscape. In: Proceedings of the 9th ACM Symposium on Information, Computer
and Communications Security, pp. 353–364. ASIA CCS 2014, NY, USA. ACM,
New York (2014)
36. Suarez-Tangil, G., Tapiador, J.E., Peris-Lopez, P., Ribagorda, A.: Evolution, detec-
tion and analysis of malware for smart devices. IEEE Commun. Surv. Tutorials
16(2), 961–987 (2014)
37. Thomas, K., Huang, D., Wang, D., Bursztein, E., Grier, C., Holt, T.J., Kruegel,
C., McCoy, D., Savage, S., Vigna, G.: Framing dependencies introduced by under-
ground commoditization. In: Workshop on the Economics of Information Security
(2015)
38. Watson, A.H., Mccabe, T.J., Wallace, D.R.: Special publication 500–235, struc-
tured testing: a software testing methodology using the cyclomatic complexity
metric. In: U.S. Department of Commerce/National Institute of Standards and
Technology (1996)