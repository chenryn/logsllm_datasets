# 根据词频 生成词云create_word_cloud(all_word)运行结果：![](Images/0a01a7c0233899c2357750ff06d67726.png){savepage-src="https://static001.geekbang.org/resource/image/ee/2c/ee24610141c2722660f2e4cb5824802c.png"}\这个过程里有一些模块，我需要说明下。首先我编写 get_songs 函数，可以得到指定歌手页面中热门前 50 的歌曲ID，歌曲名。在这个函数里，我使用 requests.request函数获取毛不易歌手页面的HTML。这里需要传入指定的请求头（headers），否则获取不到完整的信息。然后用XPath 解析并获取指定的内容，这个模块中，我想获取的是歌曲的链接和名称。其中歌曲的链接类似 /song?id=536099160 这种形式，你能看到字符串第 9位之后，就是歌曲的 ID。有关 XPath 解析的内容，我在[第 10节](https://time.geekbang.org/column/article/76001)讲到过，如果你忘记了，可以看一下。一般来说，XPath解析 99% 的可能都是以 // 开头，因为你需要获取所有符合这个 XPath的内容。我们通过分析 HTML代码，能看到一个关键的部分：id='hotsong-list'。这个代表热门歌曲列表，也正是我们想要解析的内容。我们想要获取这个热门歌曲列表下面所有的链接，XPath解析就可以写成 //\*\[@id='hotsong-list'\]//a。然后你能看到歌曲链接是href 属性，歌曲名称是这个链接的文本。获得歌曲 ID 之后，我们还需要知道这个歌曲的歌词，对应代码中的get_song_lyric 函数，在这个函数里调用了网易云的歌词 API接口，比如。你能看到歌词文件里面还保存了时间信息，我们需要去掉这部分。因此我使用了re.sub 函数，通过正则表达式匹配，将 \[\] 中数字信息去掉，方法为re.sub(r'\[\\d:.\[\]\]','',lyric)。最后我们还需要设置一些歌词中常用的停用词，比如"作词""作曲""编曲"等，编写remove_stop_words 函数，将歌词文本中的停用词去掉。最后编写 create_word_cloud 函数，通过歌词文本生成词云文件。
## 总结今天我给你讲了词云的可视化。在这个实战中，你能看到前期的数据准备在整个过程中占了很大一部分。如果你用Python 作为数据采集工具，就需要掌握 Python 爬虫和 XPath解析，掌握这两个工具最好的方式就是多做练习。我们今天讲到了词云工具 WordCloud，它是一个很好用的 Python工具，可以将复杂的文本通过词云图的方式呈现。需要注意的是，当我们需要使用中文字体的时候，比如黑体SimHei，就可以将 WordCloud 中的 font_path 属性设置为SimHei.ttf，你也可以设置其他艺术字体，在给毛不易的歌词做词云展示的时候，我们就用到了艺术字体。你可以从 GitHub上下载字体和代码：。![](Images/29265fc1a930bac7b3a42f5f2778d41b.png){savepage-src="https://static001.geekbang.org/resource/image/0c/6d/0cbc5f3e4ecd41af9a872fd9b4aed06d.png"}\最后给你留一道思考题吧。我抓取了毛不易主页的歌词，是以歌手主页为粒度进行的词云可视化。实际上网易云音乐也有歌单的API，比如。你能不能编写代码对歌单做个词云展示（比如歌单ID 为 753776811）呢？欢迎你在评论区与我分享你的答案，也欢迎点击"请朋友读"，把这篇文章分享给你的朋友或者同事。![](Images/8b75105190797b2e4f7be2536b6543db.png){savepage-src="https://static001.geekbang.org/resource/image/48/96/48cb89aa8c4858bbc18df3b3ac414496.jpg"}
# 39丨数据挖掘实战（1）：信用卡违约率分析今天我来带你做一个数据挖掘的项目。在数据挖掘的过程中，我们经常会遇到一些问题，比如：如何选择各种分类器，到底选择哪个分类算法，是SVM，决策树，还是 KNN？如何优化分类器的参数，以便得到更好的分类准确率？这两个问题，是数据挖掘核心的问题。当然对于一个新的项目，我们还有其他的问题需要了解，比如掌握数据探索和数据可视化的方式，还需要对数据的完整性和质量做评估。这些内容我在之前的课程中都有讲到过。今天的学习主要围绕下面的三个目标，并通过它们完成信用卡违约率项目的实战，这三个目标分别是：1.  创建各种分类器，包括已经掌握的 SVM、决策树、KNN    分类器，以及随机森林分类器；2.  掌握 GridSearchCV 工具，优化算法模型的参数；3.  使用 Pipeline    管道机制进行流水线作业。因为在做分类之前，我们还需要一些准备过程，比如数据规范化，或者数据降维等。
## 构建随机森林分类器在算法篇中，我主要讲了数据挖掘十大经典算法。实际工作中，你也可能会用到随机森林。随机森林的英文是 Random Forest，英文简写是RF。它实际上是一个包含多个决策树的分类器，每一个子分类器都是一棵 CART分类回归树。所以随机森林既可以做分类，又可以做回归。当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。当它做回归的时候，输出结果是每棵CART 树的回归结果的平均值。``{=html}在 sklearn 中，我们使用 RandomForestClassifier()构造随机森林模型，函数里有一些常用的构造参数：![](Images/1b78d1508c5f0fd2768daefab48b82f8.png){savepage-src="https://static001.geekbang.org/resource/image/35/f9/352035fe3e92d412d652fd55c77f23f9.png"}\当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。
## 使用 GridSearchCV 工具对模型参数进行调优在做分类算法的时候，我们需要经常调节网络参数（对应上面的构造参数），目的是得到更好的分类结果。实际上一个分类算法有很多参数，取值范围也比较广，那么该如何调优呢？Python 给我们提供了一个很好用的工具 GridSearchCV，它是 Python的参数自动搜索模块。我们只要告诉它想要调优的参数有哪些以及参数的取值范围，它就会把所有的情况都跑一遍，然后告诉我们哪个参数是最优的，结果如何。使用 GridSearchCV 模块需要先引用工具包，方法如下：    from sklearn.model_selection import GridSearchCV然后我们使用 GridSearchCV(estimator, param_grid, cv=None, scoring=None)构造参数的自动搜索模块，这里有一些主要的参数需要说明下：![](Images/fffae858794f06befdfd7c55b718d6e9.png){savepage-src="https://static001.geekbang.org/resource/image/70/fd/7042cb450e5dcac9306d0178265642fd.png"}\构造完 GridSearchCV 之后，我们就可以使用 fit 函数拟合训练，使用 predict函数预测，这时预测采用的是最优参数情况下的分类器。这里举一个简单的例子，我们用 sklearn 自带的 IRIS 数据集，采用随机森林对IRIS 数据分类。假设我们想知道 n_estimators 在 1-10的范围内取哪个值的分类结果最好，可以编写代码：    
# -*- coding: utf-8 -*-
# 使用 RandomForest 对 IRIS 数据集进行分类
# 利用 GridSearchCV 寻找最优参数from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisrf = RandomForestClassifier()parameters = {"n_estimators": range(1,11)}iris = load_iris()
# 使用 GridSearchCV 进行参数调优clf = GridSearchCV(estimator=rf, param_grid=parameters)
# 对 iris 数据集进行分类clf.fit(iris.data, iris.target)print(" 最优分数： %.4lf" %clf.best_score_)print(" 最优参数：", clf.best_params_)运行结果如下：最优分数： 0.9667最优参数： {'n_estimators': 6}你能看到当我们采用随机森林作为分类器的时候，最优准确率是 0.9667，当n_estimators=6 的时候，是最优参数，也就是随机森林一共有 6 个子决策树。
## 使用 Pipeline 管道机制进行流水线作业做分类的时候往往都是有步骤的，比如先对数据进行规范化处理，你也可以用 PCA方法（一种常用的降维方法）对数据降维，最后使用分类器分类。Python 有一种 Pipeline管道机制。管道机制就是让我们把每一步都按顺序列下来，从而创建 Pipeline流水线作业。每一步都采用 ('名称', 步骤) 的方式来表示。我们需要先采用 StandardScaler 方法对数据规范化，即采用数据规范化为均值为0，方差为 1 的正态分布，然后采用 PCA方法对数据进行降维，最后采用随机森林进行分类。具体代码如下：    from sklearn.model_selection import GridSearchCVpipeline = Pipeline([        ('scaler', StandardScaler()),        ('pca', PCA()),        ('randomforestclassifier', RandomForestClassifier())])那么我们现在采用 Pipeline 管道机制，用随机森林对 IRIS数据集做一下分类。先用 StandardScaler方法对数据规范化，然后再用随机森林分类，编写代码如下：    