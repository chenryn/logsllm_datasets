# 20 \| 深入理解StatefulSet（三）：有状态应用实践你好，我是张磊。今天我和你分享的主题是：深入理解 StatefulSet之有状态应用实践。在前面的两篇文章中，我详细讲解了 StatefulSet的工作原理，以及处理拓扑状态和存储状态的方法。而在今天这篇文章中，我将通过一个实际的例子，再次为你深入解读一下部署一个StatefulSet 的完整流程。今天我选择的实例是部署一个 MySQL 集群，这也是 Kubernetes官方文档里的一个经典案例。但是，很多工程师都曾向我吐槽说这个例子"完全看不懂"。其实，这样的吐槽也可以理解：相比于 Etcd、Cassandra等"原生"就考虑了分布式需求的项目，MySQL以及很多其他的数据库项目，在分布式集群的搭建上并不友好，甚至有点"原始"。所以，这次我就直接选择了这个具有挑战性的例子，和你分享如何使用StatefulSet 将它的集群搭建过程"容器化"。> 备注：在开始实践之前，请确保我们之前一起部署的那个 Kubernetes> 集群还是可用的，并且网络插件和存储插件都能正常运行。具体的做法，请参考第> 11 篇文章[《从 0 到 1：搭建一个完整的 Kubernetes> 集群》](https://time.geekbang.org/column/article/39724)的内容。首先，用自然语言来描述一下我们想要部署的"有状态应用"。``{=html}1.  是一个"主从复制"（Maser-Slave Replication）的 MySQL 集群；2.  有 1 个主节点（Master）；3.  有多个从节点（Slave）；4.  从节点需要能水平扩展；5.  所有的写操作，只能在主节点上执行；6.  读操作可以在所有节点上执行。这就是一个非常典型的主从模式的 MySQL集群了。我们可以把上面描述的"有状态应用"的需求，通过一张图来表示。![](Images/58591947d87c6dad82a054737ec4ef54.png){savepage-src="https://static001.geekbang.org/resource/image/bb/02/bb2d7f03443392ca40ecde6b1a91c002.png"}\在常规环境里，部署这样一个主从模式的 MySQL集群的主要难点在于：如何让从节点能够拥有主节点的数据，即：如何配置主（Master）从（Slave）节点的复制与同步。所以，在安装好 MySQL 的 Master节点之后，你需要做的第一步工作，就是**通过 XtraBackup 将 Master节点的数据备份到指定目录。**> 备注：XtraBackup 是业界主要使用的开源 MySQL 备份和恢复工具。这一步会自动在目标目录里生成一个备份信息文件，名叫：xtrabackup_binlog_info。这个文件一般会包含如下两个信息：    $ cat xtrabackup_binlog_infoTheMaster-bin.000001     481这两个信息会在接下来配置 Slave 节点的时候用到。**第二步：配置 Slave 节点**。Slave 节点在第一次启动前，需要先把 Master节点的备份数据，连同备份信息文件，一起拷贝到自己的数据目录（/var/lib/mysql）下。然后，我们执行这样一句SQL：    TheSlave|mysql> CHANGE MASTER TO                MASTER_HOST='$masterip',                MASTER_USER='xxx',                MASTER_PASSWORD='xxx',                MASTER_LOG_FILE='TheMaster-bin.000001',                MASTER_LOG_POS=481;其中，MASTER_LOG_FILE 和MASTER_LOG_POS，就是该备份对应的二进制日志（BinaryLog）文件的名称和开始的位置（偏移量），也正是 xtrabackup_binlog_info文件里的那两部分内容（即：TheMaster-bin.000001 和 481）。**第三步，启动 Slave 节点**。在这一步，我们需要执行这样一句 SQL：    TheSlave|mysql> START SLAVE;这样，Slave节点就启动了。它会使用备份信息文件中的二进制日志文件和偏移量，与主节点进行数据同步。**第四步，在这个集群中添加更多的 Slave 节点**。需要注意的是，新添加的 Slave 节点的备份数据，来自于已经存在的 Slave节点。所以，在这一步，我们需要将 Slave节点的数据备份在指定目录。而这个备份操作会自动生成另一种备份信息文件，名叫：xtrabackup_slave_info。同样地，这个文件也包含了MASTER_LOG_FILE 和 MASTER_LOG_POS 两个字段。然后，我们就可以执行跟前面一样的"CHANGE MASTER TO"和"START SLAVE"指令，来初始化并启动这个新的 Slave 节点了。通过上面的叙述，我们不难看到，**将部署 MySQL 集群的流程迁移到 Kubernetes项目上，需要能够"容器化"地解决下面的"三座大山"：**1.  Master 节点和 Slave 节点需要有不同的配置文件（即：不同的 my.cnf）；2.  Master 节点和 Salve 节点需要能够传输备份信息文件；3.  在 Slave 节点第一次启动之前，需要执行一些初始化 SQL 操作；而由于 MySQL 本身同时拥有拓扑状态（主从节点的区别）和存储状态（MySQL保存在本地的数据），我们自然要通过 StatefulSet来解决这"三座大山"的问题。**其中，"第一座大山：Master 节点和 Slave节点需要有不同的配置文件"，很容易处理**：我们只需要给主从节点分别准备两份不同的MySQL 配置文件，然后根据 Pod 的序号（Index）挂载进去即可。正如我在前面文章中介绍过的，这样的配置文件信息，应该保存在 ConfigMap里供 Pod 使用。它的定义如下所示：    apiVersion: v1kind: ConfigMapmetadata:  name: mysql  labels:    app: mysqldata:  master.cnf: |    
# 主节点 MySQL 的配置文件    [mysqld]    log-bin  slave.cnf: |    
# 从节点 MySQL 的配置文件    [mysqld]    super-read-only在这里，我们定义了 master.cnf 和 slave.cnf 两个 MySQL 的配置文件。-   master.cnf 开启了    log-bin，即：使用二进制日志文件的方式进行主从复制，这是一个标准的设置。-   slave.cnf 的开启了    super-read-only，代表的是从节点会拒绝除了主节点的数据同步操作之外的所有写操作，即：它对用户是只读的。而上述 ConfigMap 定义里的 data 部分，是 Key-Value格式的。比如，master.cnf 就是这份配置数据的Key，而"\|"后面的内容，就是这份配置数据的 Value。这份数据将来挂载进Master 节点对应的 Pod 后，就会在 Volume 目录里生成一个叫作 master.cnf的文件。> 备注：如果你对 ConfigMap 的用法感到陌生的话，可以稍微复习一下第 15> 篇文章[《深入解析 Pod> 对象（二）：使用进阶》](https://time.geekbang.org/column/article/40466)中，我讲解> Secret 对象部分的内容。因为，ConfigMap 跟> Secret，无论是使用方法还是实现原理，几乎都是一样的。接下来，我们需要创建两个 Service 来供 StatefulSet 以及用户使用。这两个Service 的定义如下所示：    apiVersion: v1kind: Servicemetadata:  name: mysql  labels:    app: mysqlspec:  ports:  - name: mysql    port: 3306  clusterIP: None  selector:    app: mysql---apiVersion: v1kind: Servicemetadata:  name: mysql-read  labels:    app: mysqlspec:  ports:  - name: mysql    port: 3306  selector:    app: mysql可以看到，这两个 Service 都代理了所有携带 app=mysql 标签的Pod，也就是所有的 MySQL Pod。端口映射都是用 Service 的 3306 端口对应 Pod的 3306 端口。不同的是，第一个名叫"mysql"的 Service 是一个 HeadlessService（即：clusterIP= None）。所以它的作用，是通过为 Pod 分配 DNS记录来固定它的拓扑状态，比如"mysql-0.mysql"和"mysql-1.mysql"这样的 DNS名字。其中，编号为 0 的节点就是我们的主节点。而第二个名叫"mysql-read"的 Service，则是一个常规的 Service。并且我们规定，所有用户的读请求，都必须访问第二个 Service 被自动分配的DNS 记录，即："mysql-read"（当然，也可以访问这个 Service 的VIP）。这样，读请求就可以被转发到任意一个 MySQL 的主节点或者从节点上。> 备注：Kubernetes 中的所有 Service、Pod 对象，都会被自动分配同名的 DNS> 记录。具体细节，我会在后面 Service 部分做重点讲解。而所有用户的写请求，则必须直接以 DNS 记录的方式访问到 MySQL的主节点，也就是："mysql-0.mysql"这条 DNS 记录。接下来，我们再一起解决"第二座大山：Master 节点和 Salve节点需要能够传输备份文件"的问题。**翻越这座大山的思路，我比较推荐的做法是：先搭建框架，再完善细节。其中，Pod部分如何定义，是完善细节时的重点。****所以首先，我们先为 StatefulSet 对象规划一个大致的框架，如下图所示：**![](Images/0123b66331625dbd787b6aa5bf44dbc5.png){savepage-src="https://static001.geekbang.org/resource/image/16/09/16aa2e42034830a0e64120ecc330a509.png"}在这一步，我们可以先为 StatefulSet 定义一些通用的字段。比如：selector 表示，这个 StatefulSet 要管理的 Pod 必须携带 app=mysql标签；它声明要使用的 Headless Service 的名字是：mysql。这个 StatefulSet 的 replicas 值是 3，表示它定义的 MySQL集群有三个节点：一个 Master 节点，两个 Slave 节点。可以看到，StatefulSet 管理的"有状态应用"的多个实例，也都是通过同一份 Pod模板创建出来的，使用的是同一个 Docker镜像。这也就意味着：如果你的应用要求不同节点的镜像不一样，那就不能再使用StatefulSet 了。对于这种情况，应该考虑我后面会讲解到的 Operator。除了这些基本的字段外，作为一个有存储状态的 MySQL 集群，StatefulSet还需要管理存储状态。所以，我们需要通过 volumeClaimTemplate（PVC模板）来为每个 Pod 定义 PVC。比如，这个 PVC 模板的resources.requests.strorage 指定了存储的大小为 10 GiB；ReadWriteOnce指定了该存储的属性为可读写，并且一个 PV只允许挂载在一个宿主机上。将来，这个 PV 对应的的 Volume 就会充当 MySQLPod 的存储数据目录。**然后，我们来重点设计一下这个 StatefulSet 的 Pod 模板，也就是 template字段。**由于 StatefulSet 管理的 Pod 都来自于同一个镜像，这就要求我们在编写 Pod时，一定要保持清醒，用"人格分裂"的方式进行思考：1.  如果这个 Pod 是 Master 节点，我们要怎么做；2.  如果这个 Pod 是 Slave 节点，我们又要怎么做。想清楚这两个问题，我们就可以按照 Pod 的启动过程来一步步定义它们了。**第一步：从 ConfigMap 中，获取 MySQL 的 Pod 对应的配置文件。**为此，我们需要进行一个初始化操作，根据节点的角色是 Master 还是 Slave节点，为 Pod 分配对应的配置文件。此外，MySQL还要求集群里的每个节点都有一个唯一的 ID 文件，名叫 server-id.cnf。而根据我们已经掌握的 Pod 知识，这些初始化操作显然适合通过 InitContainer来完成。所以，我们首先定义了一个 InitContainer，如下所示：          ...      
# template.spec      initContainers:      - name: init-mysql        image: mysql:5.7        command:        - bash        - "-c"        - |          set -ex          