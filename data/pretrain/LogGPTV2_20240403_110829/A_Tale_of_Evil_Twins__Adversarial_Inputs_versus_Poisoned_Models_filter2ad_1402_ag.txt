∫ arccos (𝑥)
0
∫ 1
𝑥
(cid:16)
1 − 𝑡2(cid:17) 𝑑−1
2 d𝑡
(16)
sin𝑑 (𝑡)d𝑡 =
A1. Proof of Proposition 1.
Let F ∗
Proof: Recall that F𝜖(𝑥◦) represents a non-empty compact set,
ℓ(𝑥;·) is differentiable for 𝑥 ∈ F𝜖(𝑥◦), and ∇𝜃 ℓ(𝑥, 𝜃) is continuous
over the domains F𝜖(𝑥◦) × R𝑛.
𝜖 (𝑥◦) = {arg min𝑥∈F𝜖 (𝑥◦) ℓ(𝑥; 𝜃)} be the set of minimizers
and ℓ(𝜃) ≜ min𝑥∈F𝜖 (𝑥◦) ℓ(𝑥; 𝜃). The Danskin’s theorem [13] states
that ℓ(𝜃) is locally continuous and directionally differentiable. The
derivative of ℓ(𝜃) along the direction 𝑑 is given by
𝜖 (𝑥◦) 𝑑⊤∇𝜃 ℓ(𝑥, 𝜃)
D𝑑 ℓ(𝜃) = min
𝑥∈F∗
We apply the Danskin’s theorem to our case. Let 𝑥∗ ∈ F𝜖(𝑥◦) be a
minimizer of min𝑥 ℓ(𝑥; 𝜃). Consider the direction 𝑑 = −∇𝜃 ℓ(𝑥∗; 𝜃).
We then have:
D𝑑 ℓ(𝜃) = min
𝑥∈F∗
≤ −∥∇𝜃 ℓ(𝑥∗, 𝜃)∥2
𝜖 (𝑥◦) 𝑑⊤∇𝜃 ℓ(𝑥, 𝜃)
2 ≤ 0
Thus, it follows that ∇𝜃 ℓ(𝑥∗; 𝜃) is a proper descent direction of
min𝑥∈F𝜖 (𝑥◦) ℓ(𝑥; 𝜃).
□
Note that in the proof above, we ignore the constraint of 𝜃 ∈
F𝛿(𝜃◦). Nevertheless, the conclusion is still valid. With this con-
straint, instead of considering the global optimum of 𝜃, we essen-
tially consider its local optimum within F𝛿(𝜃◦). Further, for DNNs
that use constructs such as ReLU, the loss function is not necessarily
continuously differentiable. However, since the set of discontinu-
ities has measure zero, this is not an issue in practice.
A2. Proof of Proposition 2.
Considering Eqn (16), we have 𝑓 (𝑦) = 1
and 𝑓 ′(𝑦) = 𝑔(𝑦)/𝑦2 where
𝑔(𝑦) = 𝑦𝛼(cid:16)
1 − (1 − 𝑦𝛼)2(cid:17) 𝑑−1
2 −
𝑦
Denote 𝑥 = 1 − 𝑦𝛼. We have
𝑔(𝑥) = (1 + 𝑥) 𝑑−1
2
(1 − 𝑥) 𝑑+1
2 −
Note that 𝑔(1) = 0. With 𝑑 > 1, we have
2 d𝑡
1−𝑦𝛼
∫ 1
(cid:0)1 − 𝑡2(cid:1) 𝑑−1
(cid:16)
1 − 𝑡2(cid:17) 𝑑−1
1 − 𝑡2(cid:17) 𝑑−1
2 d𝑡
2 d𝑡
∫ 1
∫ 1
1−𝑦𝛼
(cid:16)
𝑥
Proof: Proving 𝜙(𝑥∗, 𝜃∗) > 1 is equivalent to showing the follow-
ing inequality:
∫ arccos (1−𝑦𝛼)
0
𝑦
∫ arccos (1−𝛼)
sin𝑑 (𝑡)d𝑡
sin𝑑 (𝑡)d𝑡
∫ arccos(1−𝑦𝛼)
 0.
0
𝑦
𝑔′(𝑥) = − (𝑑 − 1) 𝑥 (1 + 𝑥) 𝑑−3
2
(1 − 𝑥) 𝑑−1
2  0 for 𝑥 ∈ (0, 1), which in turn implies 𝑓 ′(𝑦) >
□
0 for 𝑦 ∈ (0, 1).
B. Implementation Details
Here we elaborate on the implementation of attacks and defenses
in this paper.
B1. Parameter Setting. Table 4 summarizes the default parameter
setting in our empirical evaluation (§ 4).
Attack/Defense
IMC
PGD
Manifold Transformation
Adversarial Re-Training
TrojanNN
STRIP
Parameter
perturbation threshold
learning rate
maximum iterations
PGD
learning rate
maximum iterations
network structure
random noise std
optimizer
hop steps
learning rate
learning rate decay
neuron number
threshold
target value
number of tests
Setting
𝜖 = 8/255
𝛼 = 1/255
𝑛iter = 10
𝜖 = 8/255
𝛼 = 1/255
𝑛iter = 10
[3,‘average’,3]
𝑣noise = 0.1
SGD
𝑚 = 4
𝛼 = 0.01
𝛾 = 0.1/50 epochs
𝑛neuron = 2
5
10
𝑛test = 8
Table 4. Default Parameter Setting
B2. Curvature Profile. Exactly computing the eigenvalues of the
𝑥 ℓ(𝑥) is prohibitively expensive for high-
Hessian matrix 𝐻𝑥 = ∇2
dimensional data. We use a finite difference approximation in our
implementation. For any given vector 𝑧, the Hessian-vector product
𝐻𝑥𝑧 can be approximated by:
∇𝑥 ℓ(𝑥 + Δ𝑧) − ∇𝑥 ℓ(𝑥)
𝐻𝑥𝑧 = lim
Δ→0
(17)
By properly setting Δ, this approximation allows us to measure the
variation of gradient in 𝑥’s vicinity, rather than an infinitesimal
Δ
C2. Basic and Ensemble STRIP against TrojanNN. Figure 17 be-
low compares the performance of basic and ensemble STRIP in
detecting TrojanNN. Interestingly, in contrary to the detection of
TrojanNN∗ (Figure 15), here the basic STRIP outperforms the en-
semble version. This may be explained as follows. As TrojanNN∗ is
optimized to evade both STRIP and NeuralCleanse, to effectively
detect it, ensemble STRIP needs to balance the metrics of both detec-
tors; in contrast, TrojanNN is not optimized with respect to either
detector. The compromise of ensemble STRIP results in its inferior
performance compared with the basic detector.
Figure 17: Detection of basic and ensemble STRIP against TrojanNN
on CIFAR10 and GTSRB.
D. Symbols and Notations
Table 5 summarizes the important notations in the paper.
Notation Definition
benign, adversarial inputs
benign, poisoned DNNs
adversary’s target class
𝑥◦, 𝑥∗
𝜃◦, 𝜃∗
𝑡
𝜅 misclassification confidence threshold
D, R, T
ℓ, ℓs, ℓf
𝜙
𝛼
𝜖, 𝛿
training, reference, target sets
attack efficacy, specificity, fidelity losses
leverage effect coefficient
learning rate
thresholds of input, model perturbation
Table 5. Symbols and notations.
point-wise curvature[38]. In practice we set 𝑧 as the gradient sign
direction to capture the most variation:
sgn(∇ℓ(𝑥))
∥ sgn(∇ℓ(𝑥))∥
and estimate the magnitude of curvature as
∥∇𝑥 ℓ(𝑥 + Δ𝑧) − ∇𝑥 ℓ(𝑥)∥2
(18)
𝑧 =
(19)
We use Eqn (19) throughout the evaluation to compute the curvature
profiles of given inputs.
C. Additional Experiments
Here we provide experiment results additional to § 4 and § 5.
C1. Detection of TrojanNN and TrojanNN∗ by ABS. In addition
to STRIP and NeuralCleanse, here we also evaluate TrojanNN and
TrojanNN∗ against ABS [31], another state-of-the-art backdoor de-
tection method. As the optimization of TrojanNN∗ requires white-
box access to ABS, we re-implement ABS according to [31]4. In the
evaluation, we set the number of seed images as 5 per class and the
maximum trojan size as 400.
Similar to NeuralCleanse, ABS attempts to detect potential back-
doors embedded in given DNNs during model inspection. In a nut-
shell, its execution consists of two steps: (i) inspecting the given
DNN to sift out abnormal neurons with large elevation difference
(i.e., highly active only with respect to one particular class), and
(ii) identifying potential trigger patterns by maximizing abnormal
neuron activation while preserving normal neuron behaviors.
To optimize the evasiveness of TrojanNN∗ with respect to ABS,
we integrate the cost function (Algorithm 2 in [31]) into the loss
terms ℓf and ℓs in Algorithm 2 and optimize the trigger 𝑟 and model
𝜃 respectively to minimize this cost function.
The detection of TrojanNN and TrojanNN∗ by ABS on CIFAR10 is
shown in Figure 16. Observe that ABS detects TrojanNN with close
to 100% accuracy, which is consistent with the findings in [31]. In
comparison, TrojanNN∗ is able to effectively evade ABS especially
when the trigger size is sufficiently large. For instance, the detection
rate (measured by maximum re-mask accuracy) drops to 40% as the
trigger size increases to 0.4. This could be explained by that larger
trigger size entails more operation space for TrojanNN∗ to optimize
the trigger to evade ABS.
Figure 16: Detection of TrojanNN and TrojanNN∗ by ABS on CIFAR10.
4The re-implementation may have differences from the original ABS (https://github.
com/naiyeleo/ABS).
ABSTrigger SizeMax Re-Mask Accuracy1.00.80.60.40.20.00.10.20.30.40.50.60.70.8TrojanNNTrojanNN*(a) CIFAR10(b) GTSRBSTRIPEnsemble STRIPSTRIP F1 ScoreTrigger Size 0.80.480.320.1600.640.10.20.30.80.480.320.1600.640.10.20.3