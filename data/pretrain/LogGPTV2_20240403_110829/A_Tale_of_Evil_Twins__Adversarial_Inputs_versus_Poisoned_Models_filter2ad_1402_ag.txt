âˆ« arccos (ğ‘¥)
0
âˆ« 1
ğ‘¥
(cid:16)
1 âˆ’ ğ‘¡2(cid:17) ğ‘‘âˆ’1
2 dğ‘¡
(16)
sinğ‘‘ (ğ‘¡)dğ‘¡ =
A1. Proof of Proposition 1.
Let F âˆ—
Proof: Recall that Fğœ–(ğ‘¥â—¦) represents a non-empty compact set,
â„“(ğ‘¥;Â·) is differentiable for ğ‘¥ âˆˆ Fğœ–(ğ‘¥â—¦), and âˆ‡ğœƒ â„“(ğ‘¥, ğœƒ) is continuous
over the domains Fğœ–(ğ‘¥â—¦) Ã— Rğ‘›.
ğœ– (ğ‘¥â—¦) = {arg minğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(ğ‘¥; ğœƒ)} be the set of minimizers
and â„“(ğœƒ) â‰œ minğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(ğ‘¥; ğœƒ). The Danskinâ€™s theorem [13] states
that â„“(ğœƒ) is locally continuous and directionally differentiable. The
derivative of â„“(ğœƒ) along the direction ğ‘‘ is given by
ğœ– (ğ‘¥â—¦) ğ‘‘âŠ¤âˆ‡ğœƒ â„“(ğ‘¥, ğœƒ)
Dğ‘‘ â„“(ğœƒ) = min
ğ‘¥âˆˆFâˆ—
We apply the Danskinâ€™s theorem to our case. Let ğ‘¥âˆ— âˆˆ Fğœ–(ğ‘¥â—¦) be a
minimizer of minğ‘¥ â„“(ğ‘¥; ğœƒ). Consider the direction ğ‘‘ = âˆ’âˆ‡ğœƒ â„“(ğ‘¥âˆ—; ğœƒ).
We then have:
Dğ‘‘ â„“(ğœƒ) = min
ğ‘¥âˆˆFâˆ—
â‰¤ âˆ’âˆ¥âˆ‡ğœƒ â„“(ğ‘¥âˆ—, ğœƒ)âˆ¥2
ğœ– (ğ‘¥â—¦) ğ‘‘âŠ¤âˆ‡ğœƒ â„“(ğ‘¥, ğœƒ)
2 â‰¤ 0
Thus, it follows that âˆ‡ğœƒ â„“(ğ‘¥âˆ—; ğœƒ) is a proper descent direction of
minğ‘¥âˆˆFğœ– (ğ‘¥â—¦) â„“(ğ‘¥; ğœƒ).
â–¡
Note that in the proof above, we ignore the constraint of ğœƒ âˆˆ
Fğ›¿(ğœƒâ—¦). Nevertheless, the conclusion is still valid. With this con-
straint, instead of considering the global optimum of ğœƒ, we essen-
tially consider its local optimum within Fğ›¿(ğœƒâ—¦). Further, for DNNs
that use constructs such as ReLU, the loss function is not necessarily
continuously differentiable. However, since the set of discontinu-
ities has measure zero, this is not an issue in practice.
A2. Proof of Proposition 2.
Considering Eqn (16), we have ğ‘“ (ğ‘¦) = 1
and ğ‘“ â€²(ğ‘¦) = ğ‘”(ğ‘¦)/ğ‘¦2 where
ğ‘”(ğ‘¦) = ğ‘¦ğ›¼(cid:16)
1 âˆ’ (1 âˆ’ ğ‘¦ğ›¼)2(cid:17) ğ‘‘âˆ’1
2 âˆ’
ğ‘¦
Denote ğ‘¥ = 1 âˆ’ ğ‘¦ğ›¼. We have
ğ‘”(ğ‘¥) = (1 + ğ‘¥) ğ‘‘âˆ’1
2
(1 âˆ’ ğ‘¥) ğ‘‘+1
2 âˆ’
Note that ğ‘”(1) = 0. With ğ‘‘ > 1, we have
2 dğ‘¡
1âˆ’ğ‘¦ğ›¼
âˆ« 1
(cid:0)1 âˆ’ ğ‘¡2(cid:1) ğ‘‘âˆ’1
(cid:16)
1 âˆ’ ğ‘¡2(cid:17) ğ‘‘âˆ’1
1 âˆ’ ğ‘¡2(cid:17) ğ‘‘âˆ’1
2 dğ‘¡
2 dğ‘¡
âˆ« 1
âˆ« 1
1âˆ’ğ‘¦ğ›¼
(cid:16)
ğ‘¥
Proof: Proving ğœ™(ğ‘¥âˆ—, ğœƒâˆ—) > 1 is equivalent to showing the follow-
ing inequality:
âˆ« arccos (1âˆ’ğ‘¦ğ›¼)
0
ğ‘¦
âˆ« arccos (1âˆ’ğ›¼)
sinğ‘‘ (ğ‘¡)dğ‘¡
sinğ‘‘ (ğ‘¡)dğ‘¡
âˆ« arccos(1âˆ’ğ‘¦ğ›¼)
 0.
0
ğ‘¦
ğ‘”â€²(ğ‘¥) = âˆ’ (ğ‘‘ âˆ’ 1) ğ‘¥ (1 + ğ‘¥) ğ‘‘âˆ’3
2
(1 âˆ’ ğ‘¥) ğ‘‘âˆ’1
2  0 for ğ‘¥ âˆˆ (0, 1), which in turn implies ğ‘“ â€²(ğ‘¦) >
â–¡
0 for ğ‘¦ âˆˆ (0, 1).
B. Implementation Details
Here we elaborate on the implementation of attacks and defenses
in this paper.
B1. Parameter Setting. Table 4 summarizes the default parameter
setting in our empirical evaluation (Â§ 4).
Attack/Defense
IMC
PGD
Manifold Transformation
Adversarial Re-Training
TrojanNN
STRIP
Parameter
perturbation threshold
learning rate
maximum iterations
PGD
learning rate
maximum iterations
network structure
random noise std
optimizer
hop steps
learning rate
learning rate decay
neuron number
threshold
target value
number of tests
Setting
ğœ– = 8/255
ğ›¼ = 1/255
ğ‘›iter = 10
ğœ– = 8/255
ğ›¼ = 1/255
ğ‘›iter = 10
[3,â€˜averageâ€™,3]
ğ‘£noise = 0.1
SGD
ğ‘š = 4
ğ›¼ = 0.01
ğ›¾ = 0.1/50 epochs
ğ‘›neuron = 2
5
10
ğ‘›test = 8
Table 4. Default Parameter Setting
B2. Curvature Profile. Exactly computing the eigenvalues of the
ğ‘¥ â„“(ğ‘¥) is prohibitively expensive for high-
Hessian matrix ğ»ğ‘¥ = âˆ‡2
dimensional data. We use a finite difference approximation in our
implementation. For any given vector ğ‘§, the Hessian-vector product
ğ»ğ‘¥ğ‘§ can be approximated by:
âˆ‡ğ‘¥ â„“(ğ‘¥ + Î”ğ‘§) âˆ’ âˆ‡ğ‘¥ â„“(ğ‘¥)
ğ»ğ‘¥ğ‘§ = lim
Î”â†’0
(17)
By properly setting Î”, this approximation allows us to measure the
variation of gradient in ğ‘¥â€™s vicinity, rather than an infinitesimal
Î”
C2. Basic and Ensemble STRIP against TrojanNN. Figure 17 be-
low compares the performance of basic and ensemble STRIP in
detecting TrojanNN. Interestingly, in contrary to the detection of
TrojanNNâˆ— (Figure 15), here the basic STRIP outperforms the en-
semble version. This may be explained as follows. As TrojanNNâˆ— is
optimized to evade both STRIP and NeuralCleanse, to effectively
detect it, ensemble STRIP needs to balance the metrics of both detec-
tors; in contrast, TrojanNN is not optimized with respect to either
detector. The compromise of ensemble STRIP results in its inferior
performance compared with the basic detector.
Figure 17: Detection of basic and ensemble STRIP against TrojanNN
on CIFAR10 and GTSRB.
D. Symbols and Notations
Table 5 summarizes the important notations in the paper.
Notation Definition
benign, adversarial inputs
benign, poisoned DNNs
adversaryâ€™s target class
ğ‘¥â—¦, ğ‘¥âˆ—
ğœƒâ—¦, ğœƒâˆ—
ğ‘¡
ğœ… misclassification confidence threshold
D, R, T
â„“, â„“s, â„“f
ğœ™
ğ›¼
ğœ–, ğ›¿
training, reference, target sets
attack efficacy, specificity, fidelity losses
leverage effect coefficient
learning rate
thresholds of input, model perturbation
Table 5. Symbols and notations.
point-wise curvature[38]. In practice we set ğ‘§ as the gradient sign
direction to capture the most variation:
sgn(âˆ‡â„“(ğ‘¥))
âˆ¥ sgn(âˆ‡â„“(ğ‘¥))âˆ¥
and estimate the magnitude of curvature as
âˆ¥âˆ‡ğ‘¥ â„“(ğ‘¥ + Î”ğ‘§) âˆ’ âˆ‡ğ‘¥ â„“(ğ‘¥)âˆ¥2
(18)
ğ‘§ =
(19)
We use Eqn (19) throughout the evaluation to compute the curvature
profiles of given inputs.
C. Additional Experiments
Here we provide experiment results additional to Â§ 4 and Â§ 5.
C1. Detection of TrojanNN and TrojanNNâˆ— by ABS. In addition
to STRIP and NeuralCleanse, here we also evaluate TrojanNN and
TrojanNNâˆ— against ABS [31], another state-of-the-art backdoor de-
tection method. As the optimization of TrojanNNâˆ— requires white-
box access to ABS, we re-implement ABS according to [31]4. In the
evaluation, we set the number of seed images as 5 per class and the
maximum trojan size as 400.
Similar to NeuralCleanse, ABS attempts to detect potential back-
doors embedded in given DNNs during model inspection. In a nut-
shell, its execution consists of two steps: (i) inspecting the given
DNN to sift out abnormal neurons with large elevation difference
(i.e., highly active only with respect to one particular class), and
(ii) identifying potential trigger patterns by maximizing abnormal
neuron activation while preserving normal neuron behaviors.
To optimize the evasiveness of TrojanNNâˆ— with respect to ABS,
we integrate the cost function (Algorithm 2 in [31]) into the loss
terms â„“f and â„“s in Algorithm 2 and optimize the trigger ğ‘Ÿ and model
ğœƒ respectively to minimize this cost function.
The detection of TrojanNN and TrojanNNâˆ— by ABS on CIFAR10 is
shown in Figure 16. Observe that ABS detects TrojanNN with close
to 100% accuracy, which is consistent with the findings in [31]. In
comparison, TrojanNNâˆ— is able to effectively evade ABS especially
when the trigger size is sufficiently large. For instance, the detection
rate (measured by maximum re-mask accuracy) drops to 40% as the
trigger size increases to 0.4. This could be explained by that larger
trigger size entails more operation space for TrojanNNâˆ— to optimize
the trigger to evade ABS.
Figure 16: Detection of TrojanNN and TrojanNNâˆ— by ABS on CIFAR10.
4The re-implementation may have differences from the original ABS (https://github.
com/naiyeleo/ABS).
ABSTrigger SizeMax Re-Mask Accuracy1.00.80.60.40.20.00.10.20.30.40.50.60.70.8TrojanNNTrojanNN*(a) CIFAR10(b) GTSRBSTRIPEnsemble STRIPSTRIP F1 ScoreTrigger Size 0.80.480.320.1600.640.10.20.30.80.480.320.1600.640.10.20.3