e
v
O
e
c
n
a
m
r
o
f
r
e
P
80
60
40
20
0
Dense Oracle D−Tree
AR
AC
IC
NC
NCAC
NCAR
ICAC
ICAR
Detection Techniques
Fig. 6.
Rate=1e − 3, FaultModel=1
Runtime overhead of each technique. F-Score target=0.9, Fault
)
%
(
t
e
g
r
a
T
g
n
i
t
e
e
m
s
e
c
i
r
t
a
M
f
o
#
100
80
60
40
20
0
Dense Oracle D−Tree
AR
AC
IC
NC
NCAC NCAR
ICAC
ICAR
Detection Techniques
Fig. 7. Number of problems meeting F-Score target. F-Score target=0.9,
Fault Rate=1e − 3, FaultModel=1
In contrast, the overhead of AR was 16% on average, over
the same set of sparse problems (i.e. 50% lower than the
traditional dense check). This reﬂects the fact that AR’s sparse
samples are representative of these problems as a whole, which
is most pronounced on problems with low sparsity and column
sum variance such as such as poli large and t3dl e. Where this
is not true, AR shows little improvement (e.g. less than 5%
improvement for qpband and impcol d) and the AC technique
is needed exploit more complex structure.
The average overhead of AC is 17%, although the overhead
in some problems (e.g. impcol d, big, tols2000, and chem97tz)
was reduced nearly in half relative to AR. AC was particularly
useful for problems like these that contain low variance
patterns within segments of their column sum distribution.
The setup overhead of AC is considered in Section V-B in
the context of linear solvers.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
IC featured 18% average overhead and its effectiveness
depends on the accuracy of the solution found for the identity
equation (cT A = 1T ). For many problems it was only
necessary to run the least squares algorithm on cT A = 1T to
a tolerance of 1e-1, which corresponds to only 1-3 iterations
of the algorithm.
NC had an average overhead of 29%. While this result may
seem surprising, considering that the NC check does not need
to compute (cT A)x, since it is very close to 0, the reason
for this was that the smallest singular value in most problems
is too large (greater than 1e − 6), making (cT A)x too far
from 0 to produce an accurate check. Another problem is
that the eigenvectors associated with small singular values
often have many zeros, which may cause faults to be masked.
Indeed, NC achieved F-Score above 0.9 for less than 10%,
in contrast to ≥ 80% for the other techniques. For problems
netz4504, mimo28x28 system, and zeros nopss 13k, that do
contain small singular values, the overheads were actually the
least (11%) of any of the techniques. Therefore, NC is the best
choice in certain scenarios.
The techniques combining sampling and conditioning had
an average overhead of 17%. Note that the NCAC and NCAR
were able to achieve larger success rates than NC (i.e. 82%
rather than 8%), since NCAC/NCAR used sampling with the
smallest eigenvector, instead of assuming that the smallest
eigenvector was also near the null space and that the resulting
check was zero.
The data shows that
to achieve good performance and
accuracy it is necessary to choose the detection technique
and its parameters based upon the properties of the given
problem. In particular, the diversity in the strengths of the
different fault detectors provides signiﬁcant power to leverage
problem structure to optimize overhead and accuracy. The
Oracle technique, which makes this choice optimally, achieves
15% overhead and reaches the F-Score of 0.9 with 92% of the
problems, compared to only 81% for the individual techniques.
The more practical Decision Tree algorithm is close to this
optimum with providing 16% overhead, with an F-Score above
0.9 on 81% of problems. Such decision trees can be used by
developers to pick the best check for their problem.
The beneﬁts from the proposed checks are not only in
terms of performance. Figure 8 and 9 illustrate a scenario
(i.e. the same data, fault model, and F-Score as Figure 6
and 7, but with a less frequent fault rate of 1e-6), where the
dense check becomes signiﬁcantly more brittle, meeting the
F-Score target with only 10% of the problems. In contrast,
the Oracle can combine checks to cover 94% of the problems
and the Decision Tree succeeds with 77%. This is because
faults directly in the check are more likely to occur with the
traditional dense check, which performs more operations than
the proposed techniques. These errors can signiﬁcantly distort
the accuracy of the check to detect faults. This is primarily
an issue at error rate=1e-6 because there is a high probability
that an error occurs in the dense check but not in the main
computations. When fault rates are signiﬁcantly higher, the
odds are high that both the main computation and the check
)
%
(
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
80
60
40
20
0
Dense Oracle D−Tree
AR
AC
IC
NC
NCAC
NCAR
ICAC
ICAR
Detection Techniques
Fig. 8.
Rate=1e-6, FaultModel=1
Runtime overhead of each technique. F-Score target=0.9, Fault
)
%
(
t
e
g
r
a
T
g
n
i
t
e
e
m
s
e
c
i
r
t
a
M
f
o
#
100
80
60
40
20
0
Dense Oracle D−Tree
AR
AC
IC
NC
NCAC NCAR
ICAC
ICAR
Detection Techniques
Fig. 9. Number of problems meeting F-Score target. F-Score target=0.9,
Fault Rate=1e-6, FaultModel=1
will be hit. For signiﬁcantly smaller fault rates, it is likely that
neither will be hit.
Figures 11 and 12 generalize our results further, showing
that they hold across different F-Score targets, fault models
and fault rates. Figure 11 displays the average performance
overhead of each technique and Figure 12 shows the fraction
of matrices for which a given F-Score target is met. The left-
most graphs in each ﬁgure plot these results for F-Score targets
of 0.5, 0.75 and 0.9, showing that the results are not sensitive
to the detector accuracy target. The middle graphs vary the
fault models used to generate the faults, demonstrating that
the detectors are equally capable of detecting all the different
fault types discussed in Section IV. Finally, the right-most
graphs vary the fault rate. Fault detection is easiest for large
(≥ 1e-4) and small (≤ 1e-7) fault rates. When errors are
common, signaling a fault on even slight signals is usually
safe. Similarly, when errors are rare, it is safe to reserve the
fault signal for only the most obvious faults. Detection is most
difﬁcult in the middle (1e-6 to 1e-5) where only ﬁnely-tuned
detectors do well. This is visible in the right-most graph of
Figure 12, which shows that detectors are most likely to miss
their F-Score target in this range. Further, Figure 11 shows that
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:18 UTC from IEEE Xplore.  Restrictions apply. 
the performance of the detectors is most erratic in this region.
Importantly, the Decision Tree algorithm is largely resilient
to the effect of this complexity, showing consistently better
overhead and accuracy than the dense check across all the
fault rates.
Figure 10 compares the different detection techniques to
each other by showing how often each one is useful for
different linear problems and under different fault models and
rates. It shows the fraction of problem/fault scenario combi-
nations for which each algorithm is chosen to be the best by
both the Oracle and the Decision Tree algorithms. While each
technique was useful in some cases, the sampling techniques
were by far the most useful. AC works best for problems that
have large column sum variances (∈ [5,1e6]) and are not well-
conditioned (condition number >1e6). NC performed well for
problems with small singular values (smallest is  10), and a large condition number
(>1e6). Problems that have a high column sum variance but
a small condition number (0.9)
Varying faultRate (fscore>0.9, faultModel=1)
)
%
(
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
30
25
20
15
Dense Oracle D−Tree