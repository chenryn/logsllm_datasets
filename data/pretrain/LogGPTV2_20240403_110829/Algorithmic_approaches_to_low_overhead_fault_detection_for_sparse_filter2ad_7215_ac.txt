### Optimized Text

**Figure 6: Runtime Overhead of Each Technique**
- **Fault Rate:** 1e-3
- **Fault Model:** 1
- **F-Score Target:** 0.9

| Detection Techniques | Dense Oracle D-Tree | AR | AC | IC | NC | NCAC | NCAR | ICAC | ICAR |
|----------------------|---------------------|----|----|----|----|------|------|------|------|
| Overhead (%)         | 80                  | 60 | 40 | 20 | 0  |      |      |      |      |

**Figure 7: Number of Problems Meeting F-Score Target**
- **F-Score Target:** 0.9
- **Fault Rate:** 1e-3
- **Fault Model:** 1

| Detection Techniques | Dense Oracle D-Tree | AR | AC | IC | NC | NCAC | NCAR | ICAC | ICAR |
|----------------------|---------------------|----|----|----|----|------|------|------|------|
| Number of Problems   | 100                 | 80 | 60 | 40 | 20 | 0    |      |      |      |

**Analysis:**

- The overhead for the Adaptive Resampling (AR) technique was, on average, 16%, which is 50% lower than the traditional dense check. This indicates that AR's sparse samples are representative of the problems, especially those with low sparsity and column sum variance, such as poli large and t3dl e. For problems where this is not true, AR shows little improvement (e.g., less than 5% for qpband and impcol d), and the Adaptive Conditioning (AC) technique is needed to exploit more complex structures.

- The AC technique has an average overhead of 17%. In some cases (e.g., impcol d, big, tols2000, and chem97tz), the overhead was nearly halved compared to AR. AC is particularly useful for problems with low variance patterns within segments of their column sum distribution.

- The Identity Check (IC) had an average overhead of 18%, with its effectiveness depending on the accuracy of the solution found for the identity equation \(c^T A = 1^T\). For many problems, running the least squares algorithm to a tolerance of 1e-1 (corresponding to 1-3 iterations) was sufficient.

- The Null Check (NC) had an average overhead of 29%. Despite not needing to compute \((c^T A)x\) (since it is close to 0), the smallest singular value in most problems was too large (greater than 1e-6), making \((c^T A)x\) too far from 0 for an accurate check. Additionally, eigenvectors associated with small singular values often have many zeros, which can mask faults. NC achieved an F-Score above 0.9 for less than 10% of the problems, compared to ≥ 80% for other techniques. For specific problems like netz4504, mimo28x28 system, and zeros nopss 13k, which contain small singular values, the overheads were the lowest (11%).

- Techniques combining sampling and conditioning (NCAC and NCAR) had an average overhead of 17%. These techniques achieved higher success rates (82%) compared to NC (8%) by using sampling with the smallest eigenvector instead of assuming it was near the null space.

- The data shows that to achieve good performance and accuracy, it is necessary to choose the detection technique and its parameters based on the problem's properties. The Oracle technique, which optimally selects the best method, achieves 15% overhead and reaches an F-Score of 0.9 for 92% of the problems, compared to 81% for individual techniques. The Decision Tree algorithm, a more practical approach, provides 16% overhead with an F-Score above 0.9 for 81% of the problems.

**Figures 8 and 9: Performance at Lower Fault Rates**

- **Fault Rate:** 1e-6
- **Fault Model:** 1
- **F-Score Target:** 0.9

**Figure 8: Runtime Overhead of Each Technique**

| Detection Techniques | Dense Oracle D-Tree | AR | AC | IC | NC | NCAC | NCAR | ICAC | ICAR |
|----------------------|---------------------|----|----|----|----|------|------|------|------|
| Overhead (%)         | 80                  | 60 | 40 | 20 | 0  |      |      |      |      |

**Figure 9: Number of Problems Meeting F-Score Target**

| Detection Techniques | Dense Oracle D-Tree | AR | AC | IC | NC | NCAC | NCAR | ICAC | ICAR |
|----------------------|---------------------|----|----|----|----|------|------|------|------|
| Number of Problems   | 100                 | 80 | 60 | 40 | 20 | 0    |      |      |      |

- At a fault rate of 1e-6, the dense check becomes significantly more brittle, meeting the F-Score target for only 10% of the problems. In contrast, the Oracle can cover 94% of the problems, and the Decision Tree succeeds with 77%. This is because the dense check performs more operations, increasing the likelihood of faults directly in the check. When fault rates are higher, both the main computation and the check are likely to be affected. For smaller fault rates, neither may be hit.

**Figures 11 and 12: Generalization Across Different Scenarios**

- **Figure 11: Average Performance Overhead**
- **Figure 12: Fraction of Matrices Meeting F-Score Target**

- The results show that the proposed checks are effective across different F-Score targets, fault models, and fault rates. The left-most graphs in each figure plot these results for F-Score targets of 0.5, 0.75, and 0.9, showing that the results are not sensitive to the detector accuracy target. The middle graphs vary the fault models, demonstrating that the detectors can detect all the different fault types discussed in Section IV. The right-most graphs vary the fault rate, showing that fault detection is easiest for large (≥ 1e-4) and small (≤ 1e-7) fault rates. Detection is most difficult in the middle range (1e-6 to 1e-5), where finely-tuned detectors are required.

- The Decision Tree algorithm is resilient to the complexity, showing consistently better overhead and accuracy than the dense check across all fault rates.

**Figure 10: Comparison of Different Detection Techniques**

- This figure compares the different detection techniques by showing how often each one is useful for different linear problems under various fault models and rates. It shows the fraction of problem/fault scenario combinations for which each algorithm is chosen as the best by both the Oracle and the Decision Tree algorithms. Sampling techniques, particularly AC, were the most useful. AC works best for problems with large column sum variances (∈ [5,1e6]) and poor conditioning (condition number >1e6). NC performed well for problems with small singular values (smallest < 1e-6) and a large condition number (>1e6).

**Conclusion:**

- The proposed checks offer significant benefits in terms of performance and accuracy. By choosing the appropriate detection technique and parameters based on the problem's properties, developers can optimize overhead and accuracy. The Decision Tree algorithm provides a practical and effective way to select the best check for a given problem.