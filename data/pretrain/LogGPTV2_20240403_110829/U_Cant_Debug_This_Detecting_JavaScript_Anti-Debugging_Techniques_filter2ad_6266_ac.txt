score of 0.952 = 0.9025 to it. The rationale behind this for-
mula is that the percentile encodes how often the number of
occurrences was observed compared to observations of the
same technique on other websites. Squaring this value then
puts more weight on the unusually high occurrences, e.g.,
when the console is cleared dozens of times, resulting in a
higher conﬁdence that this usage is intentional and resembles
anti-debugging efforts.
Yet, we still have to consider that clearing the console is
by itself an uncommon occurrence, with only about 0.53%
of all sites behaving this way. A CONCLEAR event with low
conﬁdence can still be more signiﬁcant than e.g.,MODBUILT
with a higher conﬁdence score. Thus, we next calculate a
severity score, which combines the conﬁdence score with the
inverse frequency of the techniques, i.e., the more common
a technique the less it increases this score. For this, we use
the Inverse Document Frequency (IDF) from the domain of
information retrieval and adapt it to count techniques instead
of word terms. Thus, the weights for each technique are cal-
culated as follows: ln(number of sites with any technique /
number of sites with given technique). This means that the
presence of CONCLEAR has a weight of 4.21 while MOD-
BUILT only has 0.71. We then multiply the conﬁdence score
with these weights and build the sum over all techniques on
the site to obtain the ﬁnal severity score. Overall, this score
considers that 1) some techniques are rarer than others, 2)
some sites use these techniques more aggressively than oth-
ers, and 3) combining different techniques on the same site is
more effective.
4.4 Study I – Results
Based on our severity score, we can now analyze the most
signiﬁcant cases of anti-debugging in more detail. In this and
the following sections, we focus on the 2,000 sites with the
highest severity score, which represents approximately the
top 1% of all sites with any indicators. These sites all had a
severity score of 3 or higher, as shown in Figure 7. Moreover,
the same ﬁgure shows that more than two-thirds of these sites
had multiple BADTs on the same site, with a few sites as many
as 5 simultaneously. On average, the severity score on these
2,000 sites was 4.63 and the average amount of techniques on
the same site was 2.28, as the raw numbers in Figure 8 show.
First, we wanted to see if there is a correlation between the
popularity of a website and the prevalence of BADTs. We
investigated this separately for each technique, to account for
their high variance in the total number of occurrences. As
shown in Figure 9, BADTs were slightly more prevalent in
the higher ranking and thus more popular websites, with the
notable exception of SHORTCUT.
Next, we analyzed the code provenance of the scripts we
found to be responsible for executing the BADT by distin-
guishing between ﬁrst- and third-party scripts. However, it
should be noted that the following analysis based on the
USENIX Association
30th USENIX Security Symposium    2941
0.00.20.40.60.81.0PercentileShortCutTrigBreakConClearModBuiltWidthDiffLogGet1234-56-1011-20>20Figure 7: Scatter plot showing the distribution of the severity
scores over the Tranco ranks. Size and color both indicate the
number of simultaneous techniques on the website.
Figure 9: Normalized correlation between website rank and
prevalence of each technique in 100k buckets. The most pop-
ular sites are on the left.
Figure 8: Severity scores on the left and sites with multiple
techniques on the right.
Severity
3-4
5-6
7-8
9-10
# Sites
1095 (54.75%)
563 (28.15%)
330 (16.50%)
12 (0.60%)
(a) Severity scores
# Sites
Combo
201 (10.05%)
1
1142 (57.10%)
2
565 (28.25%)
3
88 (4.40%)
4
5
4 (0.20%)
(b) Combinations of BADTs
eTLD+12 is only a rough estimation. For example, a third-
party library could also be hosted on ﬁrst-party servers or ﬁrst-
party code on another domain like a CDN which then would
appear to be third-party code. In general, it is rather complex
to correctly determine if multiple domains belong to the same
owner, as previous research has shown [e.g., 31, 36, 53, 57].
Table 3: BADT occurrence by ﬁrst- and third-party code.
Technique
SHORTCUT
TRIGBREAK
CONCLEAR
MODBUILT
WIDTHDIFF
LOGGET
# First-party
283 (73%)
282 (81%)
221 (17%)
145 (43%)
19 (3%)
197 (16%)
# Third-party
103 (27%)
68 (19%)
1084 (83%)
195 (57%)
707 (97%)
1059 (84%)
TOTAL
1147 (26%)
3216 (74%)
Now as Table 3 shows, we get a very different picture de-
pending on the technique: SHORTCUT was mainly caused
by ﬁrst-party code, while MODBUILT was more balanced.
On the other hand, WIDTHDIFF showed the exact opposite
and was with an overwhelming majority present in third-party
2The eTLD is the effective top-level domain, e.g., for foo.example.co.jp
the eTLD is .co.jp and the eTLD+1 is example.co.jp
code. But even if a technique was triggered by third-party
code, it still can very well be the ﬁrst party’s intent to inter-
fere with an analysis by including their code. For example,
the most prevalent script for causing both SHORTCUT and
TRIGBREAK in third-party code is a plugin for the popular
e-commerce platform Shopify called Vault AntiTheft Protec-
tion App [13], which promises to protect the website from
competitors that might want to steal one’s content.
Now we next want to know if the number of third-party
inclusions is caused by relatively few popular scripts or not.
In Figure 10 we can see that, e.g., for WIDTHDIFF the most
popular script is already responsible for about 51% of all cases
in third-party code and the top 5 together cover already 77%.
This means that only a very small number of scripts is respon-
sible for the high prevalence of this technique, while for other
BADTs this behavior is less pronounced. Moreover, LOGGET
and CONCLEAR almost perfectly overlap each other, as the
most popular implementations also try to hide the suspicious
logged elements by clearing the console immediately after-
ward each time.
To further investigate this, we performed a manual analysis
of the 10 most prevalent third-party scripts for each of the 6
BADTs. We found that many of these scripts are related to
advertisements, bot detection, content protection, and crypto-
jacking. Moreover, many of them were not just miniﬁed but
completely obfuscated. In total, 35 of the 60 most prevalent
scripts and in particular 9 of the 10 most common scripts caus-
ing LOGGET were obfuscated, indicating that these scripts
would rather not be analyzed and might even be related to
malicious activities.
Finally, we also investigated what types of categories these
2,000 sites belong to. To this end, we used the WebPulse
Site Review service [55] operated by the security company
Symantec. As Table 4 shows, these sites are often related to
pornography, piracy, and suspicious activity in general.
2942    30th USENIX Security Symposium
USENIX Association
1200K400K600K800K1MTranco rank345678910Severity12345100K300K500K700K900KTranco rank0.000.020.040.060.080.100.120.140.16Proportional prevelanceShortCutTrigBreakConClearModBuiltWidthDiffLogGetMonitor existing Breakpoint (MONBREAK) As the de-
bugger statement only halts the execution if a debugger is
attached, we can simply compare the time directly before and
after that statement. If it took longer than, e.g., 100ms then
we can be sure that the DevTools are open [16]. Figure 11
shows how this technique can be implemented in a few lines
of JavaScript code. The main difference to TRIGBREAK is
that the goal here is not to disrupt the user but rather to in-
fer the state of the DevTools. So, in this case, triggering the
breakpoint only once is already enough to know somebody is
analyzing the website and there is no need to trigger additional
breakpoints afterward.
function measure() {
const start = performance.now();
debugger;
const time = performance.now() - start;
if (time > 100) { /*DevTools are open!*/ }
}
setInterval(measure, 1000);
Figure 11: Detecting the DevTools by checking for an at-
tached debugger
Wait for new Breakpoint (NEWBREAK) A more stealthy
variation of the MONBREAK technique does not trigger break-
points by itself, but rather detects when the analyst is adding
a new breakpoint anywhere. As soon as this new breakpoint
is hit, we can again observe this through timing information.
If we call a function repeatedly in the same interval and sud-
denly it took way longer to execute again, there is a good
chance that a breakpoint was hit. While this approach is more
stealthy, it obviously has no effect as long as someone uses
the DevTools without setting a breakpoint at all. Also, note
that setInterval and similar functions are throttled if the user
switches to another tab. Therefore, an additional check with
hasFocus is needed to conﬁrm that this page is currently in
the foreground, as shown in Figure 12.
function measure() {
const diff = performance.now() - timeSinceLast;
if (document.hasFocus() && diff > threshold) {
//DevTools are open!
}
timeSinceLast = performance.now();
}
setInterval(measure, 300);
Figure 12: Detecting the DevTools by checking the time be-
tween multiple executions
Console spamming (CONSPAM) While the debugger
statement is a useful tool to implement anti-debugging mea-
sures, it still has the drawback that halting at breakpoints can
easily be disabled in the DevTools. The following technique
Figure 10: The 25 most common scripts for each technique
and their cumulative share of sites
Table 4: Website categories according to Symantec.
Category
Entertainment
Finance
Malware
News
Other
Piracy
Pornography
Shopping
Suspicious
Technology
Uncategorized
# Sites % Total
9.15%
3.20%
4.25%
2.70%
9.40%
5.20%
30.10%
2.50%
11.60%
3.90%
23.85%
183
64
85
54
188
104
602
50
232
78
477
5 Sophisticated Anti-Debugging
In contrast to the BADTs seen so far, the following sophisti-
cated anti-debugging techniques (SADTs) in this chapter are
much more elusive. They use side-channels to become aware
of an ongoing analysis and then subtly alter the behavior of a
website only if they are triggered and otherwise stay dormant.
5.1 Timing-Based Techniques
The following three timing-based SADTs are based on the
fact that certain operations become slower as long as the
DevTools are open. On a high level, these techniques get the
current time, e.g., via Date.now or performance.now, perform
some action and then check how much time has passed. If that
time is above a speciﬁed threshold or changes signiﬁcantly
at one point, then the DevTools were likely opened. These
techniques thus use the time between operations as a side-
channel about the state of the DevTools. Firefox, for example,
lowers the resolution of timers due to privacy concerns and to
mitigate side-channel attacks like Spectre [37]. Yet a precision
in the range of milliseconds is still more than enough for these
techniques to work.
USENIX Association
30th USENIX Security Symposium    2943
0510152025Most popular scripts020406080100Cumulative percentage of third-party scriptsShortCutTrigBreakConClearModBuiltWidthDiffLogGetinstead abuses the fact that certain functions of the browser-
provided window object run slower while the DevTools are
open. Historically, this worked by creating many text ele-
ments with long content and quickly adding and removing
them to the DOM over and over again [50]. This caused a no-
ticeable slowdown, as the elements tab of the DevTools tries
to highlight all changes to the DOM in real-time. However,
this approach no longer works in both Firefox and Chrome.
What still works, at the time of writing, is to write lots of
output to the console and check how long this took [19]. As
the browser needs to do more work if the console is actually
visible, this is a useful side-channel about the state of the
DevTools. Conveniently, this technique also works regardless
of which tab in DevTools currently has the focus.
Figure 13 shows a possible implementation of this CON-
SPAM technique. An alternative is to ﬁrst measure the time
a few rounds in the beginning and then always compare to
that baseline. This has the advantage that a visitor with slow
hardware does not trigger a false positive, as there is no ﬁxed
threshold. However, this approach then assumes the DevTools
are going to be opened after the page has loaded and not right
from the start.
function measure() {
const start = performance.now();
for (let i = 0; i  threshold) { /*DevTools open!*/ }
}
setInterval(measure, 1000);
Figure 13: Detecting the DevTools by repeatedly calling func-
tions of the console
5.2 Systematization II
Using the same properties as in our previous systematization
of the basic techniques, we now take a look at these newly
introduced sophisticated techniques in Table 5. All of them
are versatile since they only detect the presence of the analysis
and do not prevent the use of certain features. NEWBREAK
is stealthier but less effective since, depending on the user’s
actions, it might not be triggered at all. While MONBREAK
stops working if breakpoints are disabled, the other techniques
are rather resilient since they are hard to disarm unless one
ﬁnds their exact location in the code.
6 Targeted Study of SADTs
Now that we have taken a closer look at these SADTs, we
also want to ﬁnd them in the wild. The main challenge in
detecting them is that they are a lot more ﬂexible and thus
Table 5: Systematization of SADTs. The goals are Impede,
Alter, and Detect. A ﬁlled circle means the property fully
applies, a half-ﬁlled circle means it applies with limitations.
Technique
Resilient
MONBREAK
NEWBREAK
CONSPAM
Effective
Goal
Stealthy
Versatile
D
D
D
(cid:32)
(cid:71)(cid:35)
(cid:32)
(cid:35)
(cid:32)
(cid:71)(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:35)
(cid:71)(cid:35)