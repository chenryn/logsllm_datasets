     70 {  
     71         struct CdbDispatchResults *dispatchResults = ds->primaryResults;  
     72   
     73         Assert(Gp_role == GP_ROLE_DISPATCH);  
     74         Assert(gp && gp->size > 0);  
     75         Assert(dispatchResults && dispatchResults->resultArray);  
     76   
     77         if (dispatchResults->writer_gang)  
     78         {  
     79                 /*  
     80                  * Are we dispatching to the writer-gang when it is already busy ?  
     81                  */  
     82                 if (gp == dispatchResults->writer_gang)  
     83                 {  
     84                         if (dispatchResults->writer_gang->dispatcherActive)  
     85                         {  
     86                                 ereport(ERROR,  
     87                                                 (errcode(ERRCODE_FEATURE_NOT_SUPPORTED),  
     88                                                  errmsg("query plan with multiple segworker groups is not supported"),  
     89                                                  errhint("likely caused by a function that reads or modifies data in a distributed table")));  
     90                         }  
     91   
     92                         dispatchResults->writer_gang->dispatcherActive = true;  
     93                 }  
     94         }  
     95   
     96         /*  
     97          * WIP: will use a function pointer for implementation later, currently just use an internal function to move dispatch  
     98          * thread related code into a separate file.  
     99          */  
    100         (pDispatchFuncs->dispatchToGang)(ds, gp, sliceIndex, disp_direct);  
    101 }  
```  
6、如果你的GPDB没有RB插件，可以使用普通类型测试模拟这个问题  
```  
postgres=# create table test(id int, info text, crt_time timestamp);  
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'id' as the Greenplum Database data distribution key for this table.  
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.  
CREATE TABLE  
postgres=# insert into test select id, md5(random()::text), clock_timestamp() from generate_series(1,1000000) t(id);  
INSERT 0 1000000  
create or replace function get_max(v_sql text) returns int as $$  
declare  
  res int;  
begin  
  execute v_sql into res;  
  return res;  
end;  
$$ language plpgsql strict;  
postgres=# \set VERBOSITY verbose  
postgres=# select get_max($$select max(id) from test$$) from gp_dist_random('gp_id');  
ERROR:  0A000: function cannot execute on segment because it accesses relation "public.test"  (seg0 slice1 127.0.0.1:25432 pid=1443)  
DETAIL:    
SQL statement "select max(id) from test"  
PL/pgSQL function "get_max" line 4 at EXECUTE statement  
LOCATION:  cdbdisp_finishCommand, cdbdisp.c:254  
```  
7、用元数据欺骗不了GPDB，因为保护不是在元数据层面判断，而是在执行层面。  
```  
postgres=# create table tmp_gp_distribution_policy as select * from gp_distribution_policy where localoid='test'::regclass;  
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'localoid' as the Greenplum Database data distribution key for this table.  
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.  
SELECT 1  
postgres=# set allow_system_table_mods=DML;  
SET  
postgres=# delete from gp_distribution_policy where localoid='test'::regclass;  
DELETE 1  
依旧的错误  
postgres=# \set VERBOSITY verbose  
postgres=# select get_max($$select max(id) from test$$) from gp_dist_random('gp_id');  
ERROR:  0A000: function cannot execute on segment because it accesses relation "public.test"  (seg0 slice1 127.0.0.1:25432 pid=1443)  
DETAIL:    
SQL statement "select max(id) from test"  
PL/pgSQL function "get_max" line 4 at EXECUTE statement  
LOCATION:  cdbdisp_finishCommand, cdbdisp.c:254  
```  
## 2 直连SEGMENT  
[《Greenplum segment节点直接读写配置与性能》](../201604/20160407_02.md)    
[《Greenplum & PostgreSQL UPSERT udf 实现 - 2 batch批量模式》](../201806/20180605_01.md)    
[《Greenplum & PostgreSQL UPSERT udf 实现 - 1 单行模式》](../201806/20180604_01.md)    
这个方法是可行的，不过过于麻烦，需要直连。  
```  
postgres=# select * from gp_segment_configuration where content<>'-1' and role='p';  
 dbid | content | role | preferred_role | mode | status | port  |        hostname         |  address  | replication_port   
------+---------+------+----------------+------+--------+-------+-------------------------+-----------+------------------  
    2 |       0 | p    | p              | s    | u      | 25432 | iZbp13nu0s9j3x3op4zpd4Z | localhost |                   
    3 |       1 | p    | p              | s    | u      | 25433 | iZbp13nu0s9j3x3op4zpd4Z | localhost |                   
(2 rows)  
```  
```  
PGOPTIONS='-c gp_session_role=utility' psql -h iZbp13nu0s9j3x3op4zpd4Z -p 25432   
```  
```  
digoal@iZbp13nu0s9j3x3op4zpd4Z-> PGOPTIONS='-c gp_session_role=utility' psql -h iZbp13nu0s9j3x3op4zpd4Z -p 25432 -c "select max(id) from test"  
   max     
---------  
 1000000  
(1 row)  
digoal@iZbp13nu0s9j3x3op4zpd4Z-> PGOPTIONS='-c gp_session_role=utility' psql -h iZbp13nu0s9j3x3op4zpd4Z -p 25433 -c "select max(id) from test"  
  max     
--------  
 999999  
(1 row)  
digoal@iZbp13nu0s9j3x3op4zpd4Z-> psql -c "select greatest(1000000,999999)"  
 greatest   
----------  
  1000000  
(1 row)  
```  
## 小结  
1、gp_dist_random('gp_id') 的方法，因为内部做了保护，目前只使用与复制表，不适合分布式表。(用户感知)    
2、使用直连SEGMENT的方法，可行，但是操作过于繁琐，而且需要用户直连SEGMENT。(用户感知)     
3、最好的方法，依旧是聚合接口本身支持prefunc API，内部多阶段并行。(用户无感知)  
## 参考  
直连SEGMENT  
[《Greenplum segment节点直接读写配置与性能》](../201604/20160407_02.md)    
[《Greenplum & PostgreSQL UPSERT udf 实现 - 2 batch批量模式》](../201806/20180605_01.md)    
[《Greenplum & PostgreSQL UPSERT udf 实现 - 1 单行模式》](../201806/20180604_01.md)    
多阶段聚合  
[《PostgreSQL 11 preview - 多阶段并行聚合array_agg, string_agg》](../201803/20180322_11.md)    
[《PostgreSQL 10 自定义并行计算聚合函数的原理与实践 - (含array_agg合并多个数组为单个一元数组的例子)》](../201801/20180119_04.md)    
[《HybridDB PostgreSQL "Sort、Group、distinct 聚合、JOIN" 不惧怕数据倾斜的黑科技和原理 - 多阶段聚合》](../201711/20171123_01.md)    
[《Greenplum 最佳实践 - 估值插件hll的使用(以及hll分式聚合函数优化)》](../201608/20160825_02.md)    
[《Postgres-XC customized aggregate introduction》](../201305/20130502_01.md)    
[《PostgreSQL aggregate function customize》](../201212/20121218_02.md)    
[《Greenplum roaring bitmap与业务场景 (类阿里云RDS PG varbitx, 应用于海量用户 实时画像和圈选、透视)》](../201801/20180127_01.md)    
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")