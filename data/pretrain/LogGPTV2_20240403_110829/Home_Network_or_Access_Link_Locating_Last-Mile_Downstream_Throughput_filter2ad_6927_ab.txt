for two downstream throughput bottleneck scenarios: an access network bottle-
neck and a wireless bottleneck. In both experiments, we established (through
repeated experiments) the wireless network capacity to be about 40 Mbps. In
the ﬁrst case, the access link is 30 Mbps, so it is always the bottleneck. In the
second case, the access link is 70 Mbps so that the wireless network becomes the
bottleneck. When the access link is the bottleneck, the RTT is about 5 ms. In
contrast, when the wireless is the bottleneck, packet buﬀering at the head of the
wireless link (i.e., the access point) increases RTTs to about 25–35 ms.
2.3 Detection Algorithm
For each device, d, we use two independent detectors. One detector uses a deci-
sion rule that determines whether an access-link bottleneck event, B, occurs,
given a particular observed value of cv. The other detector uses a decision rule
that determines whether a wireless bottleneck event, W , occurs given a partic-
ular observed value of τd. We ﬁrst compute likelihood functions f(cv|B) and
f(cv|B) in a controlled setting, where we use our ability to control the through-
put of the upstream link to introduce a bottleneck on the access link. We then
deﬁne our decision rule in terms of the likelihood ratio:
Λ(cv = v) =
f(cv = v|B)
f(cv = v|B)
where v is the measured coeﬃcient of variation of packet interarrival time for
packets over the observation window. When Λ is greater than some threshold
γ, the detector says that the access link is the bottleneck (i.e., it is more likely
than not, given the observation of cv = v, that the prior is the event B). We can
tune the detector by varying the value of γ; higher values will result in higher
detection rates, but also higher false positive rates. We use a similar approach
for W . The next section presents our choices of threshold.
We can only perform bottleneck detection if the network is sending enough
traﬃc. We set a minimum number of packets per second, Tpps, and a minimum
number of packets per ﬂow, Tpf , for running HoA. Figure 1 shows the distribution
of the number of packets per second and packets per ﬂow observed across homes
in the FCC deployment. In approximately 40 % of measured one-second intervals,
we observe packet rates of less than 10 packets per second. We also tested Tpps
values of 50, 100, and 150 packets per second, and Tpf values of 25, 50, and 75
packets per ﬂow on real-world deployment data; none of these settings changed
our conclusions.
116
S. Sundaresan et al.
1.0
0.8
0.6
0.4
0.2
e
t
a
r
e
v
i
t
i
s
o
p
e
u
r
T
1.0
0.95
1.0
0.8
0.6
0.4
0.2
e
t
a
r
e
v
i
t
i
s
o
p
e
u
r
T
1.0
0.95
0.9
0.0
0.2
0.4
0.0
0.0
0.05
0.6
0.1
0.8
1.0
0.9
0.0
0.2
0.4
0.0
0.0
0.05
0.6
0.1
0.8
1.0
False positive rate
False positive rate
Fig. 4. Receiver operating character-
istic for access link bottleneck detec-
tion using the coeﬃcient of variation of
packet interarrival time.
Fig. 5. Receiver operating character-
istic for wireless bottleneck detection
using the TCP RTT between the access
point and the client.
2.4 Calibration
We built a testbed to run controlled experiments to calibrate detection thresh-
olds. The testbed has an access point, its LAN, a network traﬃc shaper upstream
of the access point, a well-provisioned university network, and servers in the
university network. The access point is a Netgear WNDR3800 router running
OpenWrt. To change the downstream throughput of the emulated access link,
we use tc and netem on a second WNDR3800 router running OpenWrt. We
run our tests against servers in the same well-provisioned university network to
avoid potential wide-area bottlenecks. We run two sets of experiments using the
testbed.
We use a traﬃc shaper to shape the link to diﬀerent throughput levels while
keeping the wireless link constant. In this case, identifying the ground truth
is straightforward, as we know the capacities of both the wireless link and the
shaped access link. We use 802.11a and 802.11n for the wireless link with respec-
tive capacities of 21 Mbps and 80 Mbps over TCP. We generate 1,356 experiments
with 11 diﬀerent emulated access links, with capacities varying from 3 Mbps to
more than 100 Mbps. To introduce wireless bottlenecks, we conduct two sets of
experiments. (1) Reduce capacity by degrading channel quality: we do this by
positioning the host at diﬀerent distances from the access point, and with mul-
tiple obstructions, and also transient problems by human activity. (2) Reduce
the available capacity of the channel by creating contention with an interfer-
ing host that sends constant UDP traﬃc, with the interfering host close to the
access point. For each setting, we run a TCP throughput test using iperf. To
minimize interference that we do not introduce ourselves, we use the 5 GHz
spectrum, which is less congested than the 2.4 GHz range in our testbed. In our
repeated controlled experiments, we found that the wireless channel in our test-
bed delivers a TCP throughput of about 80 Mbps on 802.11n. We performed
1,356 experiments over many operating conditions.
Locating Last-Mile Downstream Throughput Bottlenecks
117
Because there can only be one throughput bottleneck on an end-to-end path,
by deﬁnition, the detectors should never detect bottlenecks simultaneously. Using
the thresholds that we computed for each detector—as we describe for each case
below—simultaneous detection occurs only 2 % of all time intervals, typically in
cases where the throughput values for the home wireless network and the access
link were similar (Fig. 5).
Packet Interarrival Time (Tcv). We use the results from the controlled exper-
iments described above to compute the likelihood functions f(cv|B) and f(cv|B)
to determine the detection threshold Tcv. We ﬁrst evaluate the detection accu-
racy of the algorithm for diﬀerent values of Tcv. Figure 4 shows the receiver
operating characteristic for this detector. When Tcv is low (close to zero), the
detector will always determine that the access link is not the bottleneck; when
Tcv is high (close to one), the detector will always identify the access link as the
bottleneck. Our results indicate that detection accuracy remains high for a wide
range of threshold settings for Tcv, particularly between 0.7 and 0.9. Detection
accuracy is very high in this range, with a true positive rate more than 95 % and
a false positive rate less than 5 %. The range of good thresholds reinforces our
conﬁdence of its robustness as a detection metric. We use a threshold Tcv = 0.8,
which oﬀers the best tradeoﬀ between the true positive and false positive rates,
to declare the access link the bottleneck.
Wireless Round-Trip Time (Tτ ). We calibrate the thresholds for the like-
lihood functions f(τd|W ) and f(τd|W ) using a similar method. We choose a
threshold Tτ = 15 ms, which yields a detection rate of 95 % and a low false
positive rate of less than 5 %. Similar to the Tcv parameter, Tτ is also robust;
we get similarly high true positive rates and low false positive rates for values
ranging from 12–17 ms. Higher LAN latencies in the wireless network can result
from other wireless problems that may manifest as retransmissions or backoﬀs.
We observe empirically that these wireless issues introduce up to 8–12 ms of
delay, whereas delays caused by wireless throughput bottlenecks introduce more
than 15 ms of extra delay, thresholds which yield a high detection and low false
positive rate in our experiments.
2.5 Limitations
HoA has several limitations. First, because it relies on passive traﬃc analysis,
the link must carry enough traﬃc to enable analysis. Section 2.3 how we deter-
mine minimum thresholds for detection, which are heuristics. Second, constant
bit rate traﬃc could in some cases yield a low cv, thus causing HoA to mistak-
enly detect a throughput bottleneck on the access link; such cases may need to
rely on other detection methods. With respect to bottlenecks, HoA cannot iden-
tify the root cause of bottlenecks, and it cannot identify bottlenecks far from
the last mile, such as peering or server-side bottlenecks. HoA can only locate
throughput bottlenecks where the link is work-conserving; because wireless links
violate this assumption, HoA cannot detect upstream throughput bottlenecks.
118
S. Sundaresan et al.
Additionally, detection thresholds may be sensitive to certain settings and con-
ﬁgurations: Tτ may depend on the wireless driver and hardware; in cable access
networks, Tcv may depend on the channel bonding conﬁguration of the DOCSIS
modem. The calibration methods from Sect. 2.4 may help determine the appro-
priate thresholds in various settings. Finally, to reduce CPU load, HoA collects
data periodically, which does not allow us to capture aspects of the network that
vary over small timescales.
2.6 Deployments
Table 1 summarizes our two deployments, which we brieﬂy describe below.
Table 1. Deployments of HoA, including locations and study durations. In addition
to the larger FCC deployment, we also performed a pilot deployment of HoA on 100
homes in the FCC deployment from August 24–30, 2014.
BISmark
Homes
64
FCC
2,652
Location 15 Countries
United States
Duration March 6–April 6, 2013 November 4–5, 2014
Tests
52,252
73,193
BISmark Deployment. We deployed HoA on Netgear’s WNDR3700/3800,
which has an Atheros chipset with a 450 MHz processor, one 802.11bgn radio,
and one 802.11an radio. The 3800 has 128 Mbytes of RAM, and the 3700 has
64 Mbytes of RAM. The devices run OpenWrt, with the ath9k wireless driver.
The driver uses the Minstrel rate adaptation algorithm, with the default setting
to a maximum bitrate of 130 Mbps. Every 5 min, HoA collects packet traces from
the WAN port for 15 s and extracts timestamps and per-ﬂow RTTs on either side
of the access point, as well as the number of packets for each connection using
tcptrace [21]. tcptrace tracks packets and the corresponding ACKs to compute
the RTTs.
FCC Deployment. We use the FCC’s deployment of Netgear WNR3500L,
which has a Broadcom chipset and a 480 MHz processor, one 802.11bgn radio,
and 64 Mbytes of RAM. The devices run a custom Netgear ﬁrmware based on
OpenWRT. The resource constraints of the WNR3500L required two changes to
our implementation. First, we imposed a packet limit and a time limit for every
trace collection iteration. The collection runs for 10 s or until it has collected
10,000 packets, whichever comes ﬁrst. We discard any trace for which the packet
ﬁlters dropped at least 5 % of packets from our analysis. Additionally, due to
resource constraints, we do not perform any processing on the device, except for
anonymization. Instead, we oﬄoad the packet header traces for oﬄine analysis.
To avoid conﬂicts with FCC’s Measuring Broadband America program, we could
only perform our measurements three times per hour.
Locating Last-Mile Downstream Throughput Bottlenecks
119
3 Results
This section explores our ﬁndings: (1) In home networks where downstream
throughput exceeds 20 Mbps, the home wireless network is the primary cause
of throughput bottlenecks. (2) Access link bottlenecks are prevalent in home
networks where the downstream throughput is less than 10 Mbps. (3) In homes
where HoA detects a wireless throughput bottleneck, it is about equally likely
that the wireless throughput bottleneck is isolated to a single device or observed
across all devices.
s
t
s
e
t
e
v
i
t
i
s
o
p
f
o
n
o
i
t
c