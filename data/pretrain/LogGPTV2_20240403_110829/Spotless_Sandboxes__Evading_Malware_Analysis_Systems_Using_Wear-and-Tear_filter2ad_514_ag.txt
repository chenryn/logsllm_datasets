o
t
k
s
e
d
_
d
i
e
z
S
g
e
r
_
r
l
s
p
p
A
a
t
o
t
_
r
s
y
a
d
f
f
i
d
p
p
a
_
e
s
y
a
D
f
f
i
D
l
r
u
t
v
e
s
y
s
_
e
t
v
e
p
p
a
_
e
t
n
u
o
C
b
s
u
_
r
s
e
i
r
t
n
E
e
h
c
a
C
U
M
_
r
I
s
e
i
r
t
n
E
e
h
c
a
c
s
n
d
_
n
s
l
l
D
d
e
r
a
h
S
a
t
o
t
_
r
l
s
p
p
A
d
e
l
l
a
t
s
n
I
_
r
Fig. 6: Correlation between the reported age of user systems and each of the artifacts.
Artifacts
TABLE VIII: Lasso regression coefﬁcients of the features
selected as good predictors of system age by the model.
Feature
netartifacts:dnscacheEntries
diskartifacts:tempFilesCount
evtartifacts:appevt
evtartifacts:sysevt
regartifacts:regSize
regartifacts:InstalledApps
regartifacts:totalApps
browser:num
Coeﬁcient
0.1383
0.0883
0.1345
0.1097
0.1160
0.6432
0.0559
-0.0138
and Dsand. We see that although the majority of real user
systems have a residual error approximately equal to zero
(meaning that the predicted age closely matches the system’s
actual age), the residuals when attempting to predict the age
of sandboxes are much higher, i.e., the predicted age is much
lower than the reported age, thereby shifting the distribution
towards the right part of the graph.
2) Lasso Regression: To explore whether a more involved
regression model would result in better prediction accuracy,
we also report the results of Lasso regression. Lasso regression
penalizes the absolute size of the regression coefﬁcients, and
sets the coefﬁcients of unnecessary features to zero, reducing
in this way the size of our artifacts set. One of the beneﬁts
of Lasso regression is that it typically utilizes less predicting
variables than linear regression, which reduces the complexity
of the overall model. In the case of malware, a smaller feature
set means fewer API calls to the underlying OS, and thereby
less chances of triggering suspicious activity monitors.
For our experiments, we split the datasets to three parts:
training, evaluation, and testing. We ﬁrst train a Lasso model
on the training set and make use of cross validation to ﬁnd the
optimum λ value. We use the discovered value (λ = 0.145),
as this minimizes the MSE of the model, and train our ﬁnal
model. Table VIII shows the eight features that the Lasso
model selected as good predictors of system age and their
corresponding coefﬁcients. The only artifact that has a small
negative coefﬁcient is the number of browsers installed on
a system. One reasonable explanation for this is that owners
of older systems may hesitate to install additional browsers,
which has an effect on our trained model.
By applying the model on the unseen dataset that consists
of artifacts from real user systems (Dreal), the MSE is 0.749,
which is better than that of linear regression. The MSE on
the testing sandbox dataset is 4.45, which again shows how
the wear and tear of sandboxes does not match their claimed
age. Figure 7 shows the residual errors of the trained Lasso
regression model, and how these compare to the errors of
the linear regression model. We observe that although both
models can predict the age of real user systems (residual
errors approximately equal zero), the Lasso model can better
differentiate some sandboxes based on fewer artifacts compared
to the linear regression model.
Overall, we see that wear-and-tear artifacts can predict,
with sufﬁcient accuracy, the real age of a system and thus
reveal the age mismatch present in current sandboxes. As we
mentioned earlier in this section, other than just improving our
understanding of the relationship between artifacts and system
age, our trained models can be of use to sandbox developers to
help them create environments with wear and tear that matches
their stated age.
VII. DISCUSSION
A. Ethical Considerations and Coordinated Disclosure
Our research sheds light to the problem of next-generation
environment-aware malware based on wear and tear arti-
facts. By demonstrating the effectiveness of this new evasion
technique, we will hopefully bring more attention to this
issue, and encourage further research on developing effective
countermeasures. To that end, our preliminary investigation
on predicting a systems’ age based on the evaluated artifact
values can help sandbox operators to ﬁne-tune the wear-and-tear
characteristics of their systems so that they look realistic. There
have been very recent indications that malware authors have
already started taking into consideration usage-related artifacts
1021
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
LM-sandbox LM-user Lasso-sandbox Lasso-user
1.00
0.75
s
m
e
t
s
y
s
f
o
n
o
i
t
c
a
r
F
0.50
0.25
0.00
-5
System age - Predicted age
0
5
Fig. 7: CDF of the difference between the reported and
estimated age for sandboxes and real systems, using the linear
and Lasso regression models.
(e.g., whether any documents appear under the recently opened
documents menu [54]). Consequently, developing effective
countermeasures before malware authors start taking advantage
of wear and tear for evasion purposes more broadly, is of
utmost importance.
Although our results regarding the analyzed sandboxes
used by public malware analysis services showcase their
vulnerability to evasion based on wear and tear, adversaries may
have already come to the same conclusion even before us, given
that our probing technique was reasonably straightforward, and
that similar techniques have also been used as part of previous
research on public sandbox ﬁngerprinting [23]. Hopefully, by
publishing our ﬁndings and coordinating with the affected
vendors, our research will eventually result in more robust
malware analysis systems.
To that end, we contacted all vendors from which our probe
tool managed to collect wear-and-tear artifact information,
notifying them about our ﬁndings several months before the
publication of this paper. Some of these vendors acknowledged
the issue and requested additional information, as well as the
code of our artifact-probing tool so that they can more easily
detect future artifact exﬁltration attempts.
B. Probing Stealthiness
Depending on the type of artifact, the actual operations
that some malware needs to perform in order to retrieve
the necessary information range from simple memory or ﬁle
system read operations, to more complex parsing and querying
operations that may involve system APIs and services. Instead
of making sandboxes look realistic from a wear-and-tear
perspective, an alternative (at least short-term) defense strategy
may thus focus on detecting the probing activity itself. At
the same time, many environmental aspects can be probed or
inferred through non-suspicious system operations commonly
exhibited during the startup or installation of benign software
(e.g., retrieving or storing state information from the registry
or conﬁguration ﬁles, or checking for updates), while malware
can always switch to a different set of artifacts that are not
monitored for suspicious activity.
Another aspect that must be considered is that as the principle
of least privilege is better enforced in user accounts and system
services, a malicious program may not have the necessary
permissions to access all system resources and APIs. We did
not encounter any such issues with our probe tool on the
tested operating systems, but based on our ﬁndings, there is
plenty of artifacts that can be probed even by under-privileged
programs. This issue may become more important in other
environments, such as, in malicious mobile apps, which are
much more constrained in terms of the environmental features
they can access.
C. OS Dependability
We have focused on the Windows environment, given that it
is one of the most severely plagued by malware, and that most
malware analysis services employ Windows sandboxes. As a
result, many of the identiﬁed artifacts are Windows-speciﬁc
(e.g., the registry-related ones), and clearly not applicable for
malware that target other operating systems (e.g., Linux, Mac,
Android). Even so, artifacts related to browser usage, the ﬁle
system, and the various network caches, are likely to be present
regardless of the exact operating system.
Based on the dependability of an artifact on a given OS,
sandbox operators may choose different defensive strategies.
For instance, operators of sandboxes tailored to the detection
of cross-platform malware (e.g., written in Java) may start
addressing OS-independent artifacts ﬁrst, before focusing on
OS-dependent ones. Finally, since end-user systems are the
most popular targets of malware authors, our study is focused
solely on them. We leave the analysis of wear-and-tear artifacts
on other platforms, such as embedded devices, for future work.
D. Defenses
We have demonstrated that removing system instrumentation
and introspection artifacts from malware sandboxes is not
enough to prevent malware evasion. Since the lack of wear
and tear can allow attackers to differentiate between dynamic
analysis systems and real user systems, introducing wear
and tear artifacts to analysis systems becomes an additional
necessary step. We outline two different potential strategies to
achieve this.
First, a real user system can be cloned and used as a basis for
a malware sandbox. In this scenario, the system will already
have organic wear and tear which can be used to confuse
attackers. Potential difﬁculties of adopting this approach are: i)
privacy issues (how does one scrub all private information
before or after cloning, but leaves wear-and-tear artifacts
intact?), and ii) eventual outdating of the artifacts (if the cloned
system is used over a long period of time, an attacker can use
our proposed statistical models to detect that the claimed age
does not match the level of wear and tear).
An alternative approach is to start with a freshly installed
image of an operating system and artiﬁcially age it by
automatically simulating user actions. This would include
installing and uninstalling different software while changing
the system time to the past, browsing popular and less popular
1022
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
webpages, and, in general, taking actions which will affect
the wear-and-tear artifacts on which an attacker could rely for
detecting dynamic analysis systems. While this approach does
not introduce any privacy concerns and can be repeated as
often as desired, it is unclear to what extent this artiﬁcial aging
will produce realistic artifacts that are similar enough to those
of real systems.
We leave the implementation and comparison of these two
approaches for future work.
VIII. CONCLUSION AND FUTURE WORK
In this paper, we have systematically assessed the threat of
a malware sandbox evasion strategy that leverages artifacts
indicative of the wear and tear of a system to identify
artiﬁcial environments, and demonstrated its effectiveness
against existing sandboxes used by malware analysis services.
As long as this aspect is not taken into consideration when
implementing malware sandboxes, malicious code will continue
to be able to effectively alter its behavior when being analyzed.
Even approaches that move from emulated and virtualized
environments to bare-metal systems [8]–[10] will be helpless
if evasion tactics shift from how realistic a system looks
like, to how realistic its past use looks like. The same also
holds for existing techniques that identify split personalities
in malware by comparing malware behavior on an emulated
analysis environment and on an unmodiﬁed reference host [44]
or multiple sandboxes [18]. As long as all involved systems do
not look realistic from the perspective of a real user’s activity,
they will be easily detectable and, therefore, evadable.
As a step towards mitigating this threat, we have presented
statistical models that capture a system’s age and degree of use,
which can aid sandbox operators in ﬁne-tuning their systems
so that they exhibit more realistic wear-and-tear characteristics.
Although addressing each and every identiﬁed artifact may be
a viable short-term solution, more generic automated “aging”
techniques will probably be needed to provide a more robust
defense, as many more artifacts may be available.
As part of our future work, we plan to explore such
approaches based on simulation-based generation, as well as
privacy-preserving transformation of system images extracted
from real user devices. We also plan to evaluate the effec-
tiveness of sandbox evasion based on wear and tear in other
environments, such as, different desktop operating systems as
well as mobile devices.
ACKNOWLEDGMENTS
We thank our shepherd, Tudor Dumitras, and the anonymous
reviewers for their valuable feedback. This work was supported
in part by the Ofﬁce of Naval Research (ONR) under grant
N00014-16-1-2264, and by the National Science Foundation
(NSF) under grants CNS-1617902 and CNS-1617593, with
additional support by Qualcomm. Any opinions, ﬁndings,
conclusions, or recommendations expressed herein are those
of the authors, and do not necessarily reﬂect those of the US
Government, ONR, NSF, or Qualcomm.
REFERENCES
[1] C. Willems, T. Holz, and F. Freiling, “Toward automated dynamic
malware analysis using CWSandbox,” Security Privacy, IEEE, vol. 5,
no. 2, pp. 32–39, March 2007.
[2] U. Bayer, C. Kruegel, and E. Kirda, “TTAnalyze: A tool for analyzing
malware,” in Proceedings of the 15th European Institute for Computer
Antivirus Research Annual Conference (EICAR), April 2006.
[3] X. Jiang and X. Wang, “‘Out-of-the-box’ monitoring of VM-based
high-interaction honeypots,” in Proceedings of the 10th International
Conference on Recent Advances in Intrusion Detection (RAID), 2007,
pp. 198–218.
[4] H. Yin, D. Song, M. Egele, C. Kruegel, and E. Kirda, “Panorama:
Capturing system-wide information ﬂow for malware detection and
analysis,” in Proceedings of the 14th ACM Conference on Computer and
Communications Security (CCS), 2007, pp. 116–127.
[5] “Anubis: Malware analysis for unknown binaries,” https://anubis.iseclab.
[6] A. Dinaburg, P. Royal, M. Sharif, and W. Lee, “Ether: Malware analysis
via hardware virtualization extensions,” in Proceedings of the 15th ACM
Conference on Computer and Communications Security (CCS), 2008, pp.
51–62.
[7] “Automated malware analysis: Cuckoo sandbox,” https://cuckoosandbox.
org/.
org/.
[8] C. Willems, R. Hund, A. Fobian, D. Felsch, T. Holz, and A. Vasudevan,
“Down to the bare metal: Using processor features for binary analysis,”
in Proceedings of the 28th Annual Computer Security Applications
Conference (ACSAC), 2012, pp. 189–198.
[9] D. Kirat, G. Vigna, and C. Kruegel, “Barecloud: Bare-metal analysis-
based evasive malware detection,” in Proceedings of the 23rd USENIX
Security Symposium, 2014, pp. 287–301.
[10] S. Mutti, Y. Fratantonio, A. Bianchi, L. Invernizzi, J. Corbetta, D. Kirat,
C. Kruegel, and G. Vigna, “Baredroid: Large-scale analysis of android
apps on real devices,” in Proceedings of the 31st Annual Computer
Security Applications Conference (ACSAC), December 2015.
[11] J. Oberheide and C. Miller, “Dissecting the Android Bouncer,” 2012,
https://jon.oberheide.org/ﬁles/summercon12-bouncer.pdf.
[12] S. Neuner, V. van der Veen, M. Lindorfer, M. Huber, G. Merzdovnik,
M. Mulazzani, and E. Weippl, “Enter sandbox: Android sandbox com-
parison,” in Proceedings of the 3rd IEEE Mobile Security Technologies
Workshop (MoST), 2014.
[13] “FireEye Malware Analysis AX Series,” https://www.ﬁreeye.com/
products/malware-analysis.html.
[14] “Blue Coat Malware Analysis Appliance,” https://www.bluecoat.com/
products/malware-analysis-appliance.
[15] “Cisco Anti-Malware System,” http://www.cisco.com/c/en/us/products/
security/web-security-appliance/anti malware index.html.
[16] “FortiNet Advanced Threat Protection (ATP) - FortiSandbox,” http://
www.fortinet.com/products/fortisandbox/index.html.
[17] “McAfee Advanced Threat Defense,” http://www.mcafee.com/us/
products/advanced-threat-defense.aspx.
[18] M. Lindorfer, C. Kolbitsch, and P. Milani Comparetti, “Detecting
environment-sensitive malware,” in Proceedings of the 14th International
Conference on Recent Advances in Intrusion Detection (RAID), 2011,
pp. 338–357.
[19] C. Kolbitsch,
2014,
http://labs.lastline.com/analyzing-environment-aware-malware-a-look-
at-zeus-trojan-variant-called-citadel-evading-traditional-sandboxes.
environment-aware malware,”
“Analyzing
[20] T. Petsas, G. Voyatzis, E. Athanasopoulos, M. Polychronakis, and
S. Ioannidis, “Rage against the virtual machine: Hindering dynamic
analysis of mobile malware,” in Proceedings of the 7th European
Workshop on System Security (EuroSec), April 2014.
[21] A. Kapravelos, Y. Shoshitaishvili, M. Cova, C. Kruegel, and G. Vigna,
“Revolver: An automated approach to the detection of evasive web-based
malware,” in Proceedings of the 22nd USENIX Security Symposium,
2013, pp. 637–652.
[22] G. Stringhini, C. Kruegel, and G. Vigna, “Shady paths: Leveraging
surﬁng crowds to detect malicious web pages,” in Proceedings of the
ACM SIGSAC Conference on Computer & Communications Security
(CCS), 2013, pp. 133–144.
[23] A. Yokoyama, K. Ishii, R. Tanabe, Y. Papa, K. Yoshioka, T. Matsumoto,
T. Kasama, D. Inoue, M. Brengel, M. Backes, and C. Rossow, “SandPrint:
Fingerprinting malware sandboxes to provide intelligence for sandbox
evasion,” in Proceedings of the 19th International Symposium on Research
in Attacks, Intrusions and Defenses (RAID), 2016, pp. 165–187.
1023
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
[24] D. Desai, “Malicious Documents leveraging new Anti-VM & Anti-
Sandbox techniques,” https://www.zscaler.com/blogs/research/malicious-
documents-leveraging-new-anti-vm-anti-sandbox-techniques, 2016.
[25] ProofPoint, “Ursnif Banking Trojan Campaign Ups the Ante with New
Sandbox Evasion Techniques,” https://www.proofpoint.com/us/threat-
insight/post/ursnif-banking-trojan-campaign-sandbox-evasion-
techniques, 2016.
[26] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski, S. Chen, and
S. King, “Automated web patrol with strider honeymonkeys: Finding
web sites that exploit browser vulnerabilities,” in Proceedings of the 13th
Network and Distributed Systems Security Symposium (NDSS), 2006.
[27] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose, “All your
iFRAMEs point to us,” in Proceedings of the 17th USENIX Security
Symposium, 2008.
[28] A. Moshchuk, T. Bragin, S. D. Gribble, and H. M. Levy, “A crawler-
based study of spyware on the web,” in Proceedings of the 13th Network
and Distributed Systems Security Symposium (NDSS), 2006.
[29] A. Moshchuk, T. Bragin, D. Deville, S. D. Gribble, and H. M. Levy,
“Spyproxy: Execution-based detection of malicious web content,” in
Proceedings of the 16th USENIX Security Symposium, 2007.
[30] M. Cova, C. Kruegel, and G. Vigna, “Detection and analysis of drive-
by-download attacks and malicious javascript code,” in Proceedings of
the World Wide Web Conference (WWW), 2010.
[31] “Wepawet: Detection and Analysis of Web-based Threats,” https://
wepawet.iseclab.org/.
[32] K. Rieck, T. Krueger, and A. Dewald, “Cujo: Efﬁcient detection and
prevention of drive-by-download attacks,” in Proceedings of the 26th
Annual Computer Security Applications Conference (ACSAC), 2010, pp.
31–39.
[33] S. Ford, M. Cova, C. Kruegel, and G. Vigna, “Analyzing and detecting
malicious ﬂash advertisements,” in Proceedings of the Annual Computer
Security Applications Conference (ACSAC), Honolulu, HI, December
2009.
[34] M. Egele, T. Scholte, E. Kirda, and C. Kruegel, “A survey on automated
dynamic malware-analysis techniques and tools,” ACM Comput. Surv.,
vol. 44, no. 2, pp. 6:1–6:42, Mar. 2008.
[35] T. Vidas and N. Christin, “Evading android runtime analysis via sandbox
detection,” in Proceedings of the 9th ACM Symposium on Information,
Computer and Communications Security (AsiaCCS), 2014, pp. 447–458.
[36] T. Klein, “ScoopyNG – The VMware detection tool,” 2008, http://www.
trapkit.de/tools/scoopyng/index.html.
[37] M. G. Kang, H. Yin, S. Hanna, S. McCamant, and D. Song, “Emulating
emulation-resistant malware,” in Proceedings of the 1st ACM Workshop
on Virtual Machine Security (VMSec), 2009, pp. 11–22.
[38] P. Ferrie, “Attacks on Virtual Machine Emulators,” http://www.symantec.
com/avcenter/reference/Virtual Machine Threats.pdf.
[39] ——, “Attacks on More Virtual Machine Emulators,” http://pferrie.tripod.
com/papers/attacks2.pdf.
[40] Y. Bulygin, “CPU side-channels vs. virtualization rootkits: the good, the
bad, or the ugly.” ToorCon, 2008.
[41] J. Rutkowska, “Red Pill... or how to detect VMM using (almost) one
CPU instruction,” 2004, https://web.archive.org/web/20071112073957/
http:/www.invisiblethings.org/papers/redpill.html.
[42] O. Bazhaniuk, Y. Bulygin, A. Furtak, M. Gorobets, J. Loucaides, and
M. Shkatov, “Reaching the far corners of MATRIX: generic VMM
ﬁngerprinting.” SOURCE Seattle, 2015.
[43] P.
Jung,
“Bypassing sanboxes
for
fun!”
BotConf, 2014,
https://www.botconf.eu/wp-content/uploads/2014/12/2014-2.7-
Bypassing-Sandboxes-for-Fun.pdf.
[44] D. Balzarotti, M. Cova, C. Karlberger, C. Kruegel, E. Kirda, and G. Vigna,
“Efﬁcient detection of split personalities in malware,” in Proceedings of
the Symposium on Network and Distributed System Security (NDSS),
2010.
[45] M. K. Sun, M. J. Lin, M. Chang, C. S. Laih, and H. T. Lin, “Malware
virtualization-resistant behavior detection,” in Proceedings of the 17th
IEEE International Conference on Parallel and Distributed Systems
(ICPADS), 2011, pp. 912–917.
[46] D. Kirat, G. Vigna, and C. Kruegel, “BareBox: Efﬁcient malware analysis
on bare-metal,” in Proceedings of the 27th Annual Computer Security
Applications Conference (ACSAC), 2011, pp. 403–412.
[47] C. Wueest, “Does malware still detect virtual machines?” 2014,
http://www.symantec.com/connect/blogs/does-malware-still-detect-
virtual-machines.
[48] A. Singh, “Defeating Darkhotel Just-In-Time Decryption,” 2015, http:
//labs.lastline.com/defeating-darkhotel-just-in-time-decryption.
[49] D. Maier, T. M¨uller, and M. Protsenko, “Divide-and-conquer: Why an-
droid malware cannot be stopped,” in Proceedings of the 9th International
Conference on Availability, Reliability and Security (ARES), 2014, pp.
30–39.
[50] J. Blackthorne, A. Bulazel, A. Fasano, P. Biernat, and B. Yener,
“AVleak: Fingerprinting antivirus emulators through black-box testing,”
in Proceedings of the 10th USENIX Workshop on Offensive Technologies
(WOOT), Austin, TX, 2016.
[51] “CleanMyPC Registry Cleaner,” http://www.registry-cleaner.net/.
[52] “CCleaner
- Registry Cleaner,” https://www.piriform.com/ccleaner/
registry-cleaner.
MIT press, 2012.
[53] R. E. Schapire and Y. Freund, Boosting: Foundations and algorithms.
[54] M. Cova, “Evasive JScript,” 2016, http://labs.lastline.com/evasive-jscript.
1024
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply.