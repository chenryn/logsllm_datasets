– Rule I, the protected data are only cached by the reserved cache, and no other
memory is cached by the reserved cache. Consequently, neither the kernel itself
nor other processes can cause the reserved cache to be evicted.
– Rule 2, the amount of the accessible (decrypted) protected data must be less
than the size of the reserved cache. They thus always ﬁt in the reserved cache.
Consequently, the protected data themselves cannot cause the reserved cache
to be evicted.
These two rules prevent conﬂicts in the reserved cache caused by other processes
and by the protected data themselves, respectively. With these two rules,
EncExec can guarantee that the decrypted data remain in the CPU cache, unob-
tainable by cold boot attacks.
EncExec enforces these two rules by leveraging the cache architecture and
the replacement algorithm of x86 processors. Modern x86 processors often have
a large shared L3 cache. Figure 2 shows the cache architecture of an Intel Core-i7
4790 processor. There are three levels of caches. Each CPU core has dedicated
L1 and L2 caches, but all the four cores share a single large L3 cache. The L1
cache is split into an instruction cache (IL1) and a data cache (DL1), each 32 KB
in size. The L2 and L3 caches are uniﬁed in that they cache both code and data.
L1 and L2 caches are relatively small in size (64 KB and 256 KB, respectively),
but the L3 cache is capacious at 8 MB. Even though the L1 and L2 caches are
3 A cache line is the unit of data transfer between the cache and the memory. Recent
x86 processors use a cache line of 64 bytes.
Secure In-Cache Execution
387
Fig. 2. Intel Core i7 cache architecture
small, they are more important to the overall system performance because they
are faster and closer to CPU cores. It is thus impractical to reserve any part
of the L1 or L2 cache, especially because EncExec has to reserve the cache in
pages (4 KB at least). Another important feature of the L3 cache for EncExec is
inclusivity. An inclusive L3 cache is guaranteed to contain all the data cached by
the L1 and L2 caches. The CPU will not bypass the L3 cache when it accesses the
memory. Without inclusivity, the CPU could evict the unencrypted data to the
memory directly from the L1 or L2 cache and load the encrypted data directly
from the memory to the L1 or L2 cache. The former leaks the unencrypted data
to the memory, while the latter causes the program to malfunction. Recent Intel
processors have large, uniﬁed, inclusive L3 caches. However, old processors like
Pentium 4 have non-inclusive L2 caches (they do not have an on-chip L3 cache)
and thus cannot be used by EncExec. In addition, we assume the cache is set
to the write-back mode instead of the write-through mode. This is because in
the write-through mode the CPU keeps the cache and the memory in sync by
writing any updates to the cached to the memory as well. Most OSes use the
write-back mode for the main memory due to its better performance.
EncExec takes control of all the physical memory cached by the reserved
cache so that no other processes can use that memory and cause eviction of the
reserved cache (rule 1). The actual memory EncExec has to control is decided by
the CPU’s cache organization. Speciﬁcally, the memory and the cache are divided
into equal-sized cache lines (64 bytes). The memory in a line is cached or evicted
as a whole. Intel processors use the popular n-way set-associative algorithm to
manage the cache [15]. Figure 3 shows a simple 2-way set-associative cache with
a cache line of 16 bytes to illustrate the concept. This cache is organized into
8 cache lines, and each two consecutive lines are grouped into a set. This cache
thus has 8 cache lines in 4 sets. Meanwhile, the memory address (16 bits) is
divided into three ﬁelds: the offset ﬁeld (4 bits) speciﬁes the oﬀset into a cache
line. This ﬁeld is ignored by the cache since the memory is cached in lines; the
set ﬁeld (2 bits) is an index into the cache sets. A line of the memory can be
cached by either line in the indexed set; the last ﬁeld, tag, uniquely identiﬁes
the line of the physical memory stored in a cache line. During the cache ﬁll, the
CPU selects one line of the indexed set (evict it ﬁrst if it is used) and loads the
new memory line into it. The tag is stored along with the data in the cache line.
During the cache lookup, the CPU compares the tag of the address to the two
388
Y. Chen et al.
Fig. 3. 2-way set-associative cache, 8 cache lines in 4 sets. Each cache line is 16 bytes.
tags in the indexed set simultaneously. If there is a match, the address is cached
(a cache hit); otherwise, a cache miss has happened. The CPU then ﬁlls one of
the lines with the data. Note that all the addresses here are physical addresses
as the L3 cache is physically indexed and physically tagged.
The L3 cache of Intel Core-i7 4790 is a 16-way set-associative cache with a
cache line size of 64 bytes [15]. Therefore, the offset ﬁeld has 6 bits to address
each of the 64 bytes in a cache line. The width of the set ﬁeld is decided by three
factors: the cache size, the cache line size, and the associativity. This processor
has an 8 MB L3 cache. The set ﬁeld thus has 13 bits ( 8M
64×16 = 8192 = 213); i.e.,
there are 8, 192 sets. The tag ﬁeld consists of all the leftover bits. If the machine
has 16 GB (234) of physical memory (note the L3 cache is physically tagged),
the tag ﬁeld thus has 15 bits (34 − 6 − 13 = 15).
EncExec relies on the page table to control the use of the reserved mem-
ory (Sect. 2.4). A page table can only map page-sized and page-aligned memory.
Therefore, EncExec has to reserve at least a whole page of the L3 cache. Even
though this processor supports several page sizes (4 KB, 2 MB, and 1 GB), we
only reserve a smallest page of the cache (4 KB, or 64 lines) to minimize the
performance overhead. However, we have to reserve 64 cache sets instead of 64
cache lines because this cache uses 16-way set-associative and all the cache lines
in the same set have to be reserved together (as the CPU may cache our data in
any line of a set). The actual amount of the reserved cache accordingly is 64 KB.
These reserved cache sets must be continuous and the ﬁrst set is page-aligned
so that together they can cache a whole page of the physical memory. In our
prototype, we reserve the cache sets from index 8, 128 (0x1FC0) to index 8, 191
(0x1FFF). Figure 4 shows the format of memory addresses that are cached by
these selected sets. EncExec needs to take control of all physical pages conform-
128 of the physical memory. For example,
ing to this format (rule 1), which total
it needs to reserve 128 MB physical memory on a machine with 16 GB of RAM.
As mandated by rule 2, EncExec cannot use more data than the reserved cache
size in order to avoid cache conﬂicts. Therefore, we can use 16 pages (64 KB) of
the reserved 128 MB memory at a time. Note that the amount of the protected
1
Secure In-Cache Execution
389
Fig. 4. Addresses that map to the reserved cache (bits marked with x can be either 0
or 1)
data can be larger than 16 pages because we use demand paging to manage the
reserved cache. Moreover, an attacker that controls an unprotected process (e.g.,
using JavaScript in a browser) cannot evict EncExec’s reserved cache because
the reserved physical memory is not mapped in that process’s virtual address
space (remember that the L3 cache is physically indexed and physically tagged).
2.4 Secure In-Cache Execution
EncExec’s second technique, secure in-cache execution, splits the views of the
protected data between the memory and the reserved cache: the data remain
encrypted in the memory, and their plaintext view only exists in the cache. In
other words, we need to desynchronize the memory and the cache. There are
three requirements for this to happen: ﬁrst, the cache must be conﬁgured to
use the write-back mode so that data modiﬁed in the cache will not be written
through to the memory; second, the L3 cache is inclusive of the L1 and L2
caches so that the CPU always accesses the memory through the L3 cache; third,
there are no conﬂicts in the reserved cache so that the CPU will not evict any
reserved cache line. The ﬁrst two requirements are guaranteed by the hardware
and the existing kernels. The third requirement is fulﬁlled by EncExec’s second
technique.
EncExec’s ﬁrst technique takes control of all the physical pages that may
be cached by the reserved cache. As long as we use no more protected data
than the size of the reserved cache, they can ﬁt in the reserved cache without
conﬂicts, and any changes to these data, including decryption, stay within the
cache. To continue the previous example, we select 16 pages out of the reserved
128 MB physical memory and use these pages for securing the protected data.
We call these pages plaintext pages. In order to desynchronize the cache and
the memory, we only need to copy the encrypted data to a plaintext page and
decrypt them there. The decrypted data remain in the cache since there are no
cache conﬂicts. However, we often need to protect more data than that can ﬁt in
plaintext pages. To address that, EncExec’s second technique leverages demand
paging to protect a large amount of data.
In demand paging, a part of the process can be temporarily swapped out
to the backing store (e.g., a swap partition) and be brought into the memory
on-demand later [25]. For EncExec, the original memory of the protected data
serves as the swap for plaintext pages. The data are brought into and evicted from
plaintext pages when necessary. The page table is used to control the process’
390
Y. Chen et al.
access to the data. When the process tries to access the unmapped data (marked
as non-present in the page table), the hardware delivers a page fault exception
to the kernel. EncExec hooks into the kernel’s page fault handler and checks
whether the page fault is caused by the unmapped protected data. If so, it
tries to allocate a plaintext page for the faulting page. If none of the plaintext
pages are available, EncExec selects one for replacement. Speciﬁcally, it ﬁrst
encrypts the plaintext page and copies it back into its original page (note the
original page is a non-reserved page). EncExec then decrypts the faulting page
into this plaintext page and updates the page table if necessary. To initiate the
protection, EncExec encrypts all the protected data, ﬂushes their cache, and
unmaps them from the process’ address space. As such, EncExec can completely
moderate access to the protected data. By integrating EncExec into the kernel’s
virtual memory management, we can leverage its sophisticated page replacement
algorithm (e.g., the LRU algorithm) to select plaintext pages for replacement.
Note that the second technique alone is not secure because plaintext pages often
contain (most recently used) sensitive data due to program locality. Without the
ﬁrst technique, there is no guarantee that plaintext pages will not be evicted to
the memory and become vulnerable to cold boot attacks. It is thus necessary for
both techniques to work together.
EncExec needs to frequently encrypt and decrypt the protected data. The
cryptographic algorithm thus plays an important role in the overall performance
of the protected process. Recent CPUs have built-in hardware support to speed
up popular cryptographic algorithms. For example, most Intel CPUs now fea-
ture hardware acceleration for AES, a popular block cipher, through the AES-
NI extension [16]. EncExec uses hardware-accelerated ciphers when available.
Our prototype uses AES-128 in the counter mode. Therefore, each block of the
protected data can be encrypted/decrypted independently. To protect the (ran-
domly generated) key from cold boot attacks, we dedicate one plaintext page
to store the key, its derived sub-keys, and other important intermediate data.
Initial vectors are stored for each page of the data. It is not necessary to protect
the secrecy of initial vectors, but they should never be reused.
3 Implementation
We have implemented a prototype of EncExec based on the FreeBSD operating
system (64-bit, version 10.2) [1]. FreeBSD’s virtual memory management has an
interesting design that enables multiple choices for implementing EncExec. Our
prototype is based on the Intel Core-i7 4790 CPU with 16 GB of memory. Other
CPUs with a similar cache architecture can be used by EncExec as well. For
example, the Xeon E5-2650 processor has a 20 MB shared, inclusive L3 cache
organized as 20-way set-associative. In the rest of this section, we describe our
prototype in detail.
3.1 Spatial Cache Reservation
Secure In-Cache Execution
391
EncExec reserves a block of the L3 cache by owning all the physical pages cached
by it, i.e., physical pages whose addresses are all 1’s from bit 12 to bit 18 (Fig. 4).
In other words, EncExec reserves one physical page every 128 pages4. EncExec
can only use 16 of these pages as plaintext pages to protect data so that these
pages can be completely contained in the reserved cache.
Modern operating systems have a sophisticated, multi-layer memory man-
age system to fulﬁll many needs of the kernel and the user space. For example,
physical memory is often allocated in pages by the buddy memory allocator, and
kernel objects are managed by the slab memory allocator to reduce fragmenta-
tion [1]. The slab allocator obtains its memory from the physical page allocator,
forming a layered structure. Given this complexity, EncExec reserves its physical
pages early in the booting process when the kernel is still running in the single
processor mode. Memory allocated after enabling the multi-processor support is
harder to reclaim – an allocated page may contain kernel objects accessed con-
currently by several processors. Simply reclaiming it will lead to race conditions
or inconsistent data.
When FreeBSD boots, the boot loader loads the kernel image into a contin-
uous block of the physical memory, starts the kernel, and passes it the layout
of the physical memory discovered by the BIOS (through e820 memory map-
pings). The kernel image is large enough to contain several reserved physical
pages that need to be reclaimed. The kernel uses several ad-hoc boot-time mem-
ory allocators to allocate memory for the booting process (e.g., allocpages
in sys/amd64/amd64/pmap.c. We modify these allocators to skip the reserved
pages. If a large block of memory (i.e., more than 127 pages) is allocated, we save
the location and length of the allocated memory in order to ﬁx it later. A typical
example is the array of vm page structures. There is one vm page structure for
each physical page. This structure thus could be really large.
By now, the kernel still runs on the simple boot page table. The kernel
then replaces it with a new, more complete page table (create pagetables in
sys/amd64/amd64/pmap.c). x86-64 has a huge virtual address space. This allows
the kernel to use it in ways that are not possible in 32-bit systems. For example,
the kernel has a dedicated 4TB area that directly maps all the physical memory,
including the reserved pages. This is called the direct map. Accordingly, the
kernel can access any physical memory by adding the oﬀset of this area to the
physical address (PHYS TO DMAP(pa)). It is not necessary to unmap plaintext
pages from the direct map because EncExec needs to directly access them (e.g.,
to decrypt a page). Another area in this new page table maps in the kernel. As
mentioned earlier, the kernel is large enough to contain several reserved pages.
If we ﬁnd such a page when creating the page table, we allocate a non-reserved
page, copy the contents from the original page, and map this page in the page
4 This number is decided by the CPU’s cache architecture. For example, EncExec
reserves one physical page every 512 pages on the aforementioned Xeon E5-2650
CPU.
392
Y. Chen et al.
vm_map
pmap
statistics
vmspace
vm_map_entry
vm_map_entry
vm_map_entry
vm_object/
EncExec
vm_object/
EncExec