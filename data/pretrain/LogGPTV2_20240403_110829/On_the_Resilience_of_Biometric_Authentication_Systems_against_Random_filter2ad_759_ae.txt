### 2.609 × 10−6
This value is significantly smaller than the AR (Acceptance Rate) for all four classifiers. We will return to this observation shortly.

### 2) Raw Face Inputs
We generated 100,000 images of size 160x160 pixels, with uniformly sampled RGB values. Feature embeddings were then extracted from these generated images using a pre-trained FaceNet model (refer to Section IV-A3). This set of 100,000 raw input vectors was normalized using a min-max scaler fitted to real user data. Since the noisy images do not contain any facial information, no alignment was performed. Note that alignment is typically used in face authentication to detect facial boundaries within an image. To ensure robust results, we aggregated the outcomes over 50 repetitions to eliminate potential biases.

The results from these raw inputs are shown in Figure 6c. We observe that the RAR (Raw Acceptance Rate) curve behaves more similarly to the FPR (False Positive Rate) curve compared to what was observed for raw touch inputs. Specifically, for RBFSVM, we obtained an RAR of 0.09, which is significantly higher than the AR (0.01) at the equal error rate. The true positive region was computed, and the average volume was found to be 6.562 × 10−94 ± 6.521 × 10−93. However, the volume covered by the raw inputs (after feature extraction) is only 4.670 × 10−390, which is negligible compared to the ARs (0.15, 0.01, 0.78, and 0.10 for all four classifiers). Additional analysis revealed that only one other user's feature space overlapped with the space of raw inputs, with an overlap area of 8.317 × 10−407, many orders of magnitude smaller than both the positive users' and the raw feature space itself.

### Observations
The threat posed by random input attacks via raw random inputs is high and, in some cases, greater than the FPR. However, the region spanned by the feature vectors from these raw inputs is exponentially small and does not cover the acceptance region. Furthermore, this region does not coincide with any true positive region. This suggests that raw inputs may result in a high RAR due to the training data lacking representative vectors in the region spanned by raw inputs. We will revisit this observation when discussing mitigation strategies in Section VI.

### V. SYNTHETIC DATASET
The analysis in the previous section was limited in that it did not isolate the reasons behind the discrepancy between AR and FPR. For some configurations (dataset-classifier pairs), the AR curve followed the FPR curve, such as for the face dataset and DNN (Figure 5c). In other cases, this was not the case. To better understand the factors affecting AR, we empirically verify the hypothesized factors outlined in Section III. Specifically, high feature variance in a target user's samples is likely to increase AR, and low feature variance in the negative class is expected to result in high AR. In both scenarios, we expect a reasonably low EER, but AR may still be significantly greater than FPR. If these factors hold true, similar behavior should be observed across all classifiers. To test this, we create a synthetic model of a biometric dataset.

### A. Simulating a Biometric Dataset
Let \( N(\mu, \sigma^2) \) denote the normal distribution with mean \(\mu\) and standard deviation \(\sigma\). We assume each feature is normally distributed across all users with slight variations in mean and standard deviation across all features and users. Our methodology for generating the synthetic dataset is as follows:

1. **Modeling the Mean and Standard Deviation:**
   - The mean of all \( n \) features, taking values in the unit interval \( I \), is modeled as a normally distributed random variable \( N(\mu_{mn}, \sigma^2_{mn}) = N(0.5, 0.1^2) \).
   - The standard deviation of all \( n \) features is modeled as another normally distributed random variable \( N(\mu_{var}, \sigma^2_{var}) = N(0.1, 0.07^2) \).

2. **Sampling for Each Feature:**
   - For each feature \( i \in [n] \), we sample \(\mu_i \leftarrow N(\mu_{mn}, \sigma^2_{mn})\) and \(\sigma_i \leftarrow N(\mu_{var}, \sigma^2_{var})\). The resulting normal distribution \( N(\mu_i, \sigma^2_i) \) serves as the population distribution of the mean of feature \( i \).

3. **Generating User Samples:**
   - For each user \( u \), we sample the mean \(\mu_{u,i} \leftarrow N(\mu_i, \sigma^2_i)\). The variance \(\sigma^2_{u,i}\) is chosen as the control variable. User \( u \)'s samples for the \( i \)-th feature are generated as i.i.d. random variables \( N(\mu_{u,i}, \sigma^2_{u,i}) \), which serves as user \( u \)'s distribution for the \( i \)-th feature.

We evaluate the same four types of ML architectures: LinSVM, RBFSVM, RNDF, and DNN. Due to the large number of potential configurations, we evaluate the model performance at a fixed threshold of 0.5. For the experiments, we choose 50 (synthetic) users, with 50 features in the feature space. Each experimental run is repeated 50 times to reduce any potential biases arising from the random process.

### B. Effects of Feature Variance on Acceptance Region

#### 1) Variable Isolated User Variance and Fixed Population Variance
We first treat one out of the 50 users as an outlier, called the isolated user. The variance \(\sigma^2_{u,i}\) is fixed at \(0.2^2\) for all other users \( u \) and for all features \( i \in [n] \). We vary the variance \(\sigma^2_{utgt,i}\) of the isolated user \( utgt \) from 0.05 to 0.35 in increments of 0.05. Figure 7 plots the user's standard deviation \(\sigma_{utgt,i}\) relative to the fixed population standard deviation \(\sigma_{u,i}\) of 0.2. It is clear that the overall AR, FRR, and FPR of the users are not affected by changing the feature variance of a single user, despite the isolated user's samples being included in the training and testing data of other users. Conversely, when viewing the AR, FRR, and FPR of the isolated user, we observe a slight increase in FRR and FPR as the relative variance increases. This is due to the positive samples being spread out due to increased variance in the isolated user's samples. However, this is accompanied by a substantially large increase in the acceptance region of this user, approaching 1, i.e., the entire feature space. This trend is visible for all four classifiers.

#### 2) Fixed Isolated User Variance and Variable Population Variance
In this experiment, we fix the variance \(\sigma^2_{utgt,i}\) of the isolated user \( utgt \) at \(0.2^2\). The \(\sigma^2_{u,i}\) of the remaining population is sampled from a normal distribution \(\sigma_{u,i} \leftarrow N(\mu_i, \sigma^2_i)\). Here, \(\mu_i\) and \(\sigma_i\) are sampled from the distributions \( N(\mu_{mn}, \sigma^2_{mn}) = N(0.5, 0.05^2) \) and \( N(\mu_{var}, \sigma^2_{var}) = N(0.03, 0.02^2) \), respectively. \(\mu_{mn}\) is varied between 0.05 and 0.35 in increments of 0.05. This sampling permits a small amount of variation between features.

The results are shown in Figure 8. Inspecting the average AR, FRR, and FPR of the system, it is evident that there is a continual increase in all three metrics as the relative variance increases. This increase is expected as the majority of users' feature values have high variance, presenting an increasingly difficult problem for the machine learner to reduce misclassification errors. However, in all four classifiers, the average AR curve is either comparable or lower than the FPR curve as the relative variance increases. For the isolated user, we see that when the relative variance of all other users is lower than this user (to the left), the AR is significantly higher even though the FPR and FRR are minimal in all four classifiers. This shows that less variance in the population samples will result in a high AR, as the classifier need not tighten AR around the true positive region due to the lack of high variance negative samples. On the other hand, the AR of the isolated user decreases as the relative variance of the population increases.

### C. On Distance-Based Classifiers
As noted earlier, it has been stated that random inputs are ineffective against distance-based classification algorithms [17]. This is in contrast to the machine learning-based algorithms evaluated in this paper. We take a brief interlude to experimentally evaluate this claim on the cosine similarity distance-based classifier. We sample 50 features with means distributed as \( N(\mu_{mn}, \sigma^2_{mn}) = N(0.2, 0.05^2) \) and variance distributed as \( N(\mu_{var}, \sigma^2_{var}) = N(0.03, 0.02^2) \). Cosine similarity is computed between two vectors of the same length. As our positive training data contains more than one training sample, we use the average of these samples as the representative template of the user [29]. We use a fixed number of 50 users, with the experiment repeated 50 times. Recall that our evaluation at each threshold is best-effort; we use 1,000 threshold bins for the evaluation of the cosine similarity classifier, since the FRR and FPR rapidly change over a small range of thresholds.

Figure 9 displays three classical machine learning algorithms (linear SVM, radial SVM, and random forests) alongside a distance-based cosine similarity classifier. It is clear from the figure that the AR is near zero for cosine similarity, unlike the other classifiers using the same synthetic dataset. This, however, comes at the cost of a higher EER. This suggests that distance-based classifiers are effective in minimizing the AR of the model, but at the expense of the system's accuracy. Further investigation of distance-based classifiers is left as future work.

### D. Effects of Increasing Synthetic Users
The real-world datasets used in Section IV have a variable number of users. Our binary classification task aggregates negative user samples into a negative class, resulting in distributions and variances of the negative class that depend on the number of users in the datasets. Thus, in this test, we investigate the impact on TPR, FPR, and AR by varying the number of users in the dataset. We use the synthetic dataset configured in the same manner as in Section V-C. We increase the number of users within the synthetic dataset, from 25 to 150, in increments of 25. Note that the split between positive and negative samples is still balanced (see Remark 4.3).

In Figure 10, we observe that with the addition of more users, there is a slight increase in the FPR. This is expected as the likelihood of user features being similar between any two users will increase with more users in the population. As the training of the classifier uses samples from other users as a negative class, the increased number of negative users slightly lowers the AR of the classifier, with an increased variation of the negative training set (from additional users) covering more of the feature space. However, both these changes are relatively minor despite the multi-fold increase in the number of users. Thus, the AR of the classifiers remains relatively stable with an increasing number of users.

### VI. MITIGATION
In the previous section, we validated that higher variance in the samples in the negative class, as compared to the variance of samples from the target user class, reduces AR. The data from the negative class is obtained from real user samples, and therefore, scheme designers cannot control the variance.

| **Biometric Modality** | **Linear SVM** | **Normal** | **FPR** | **AR** | **Mitigation** | **FPR** | **AR** |
|-----------------------|----------------|------------|---------|--------|---------------|---------|--------|
| **Gait**              |                | 0.160      | 0.24    |        | 0.160         | 0.04    |        |
| **Touch**             |                | 0.325      | 0.49    |        | 0.340         | 0.01    |        |
| **Face**              |                | 0.050      | 0.15    |        | 0.050         | 0.11    |        |
| **Voice**             |                | 0.030      | 0.08    |        | 0.030         | 0.06    |        |

Green (resp., red) shades highlight improvement (resp., deterioration) in FPR and AR. Color intensity is proportional to the degree of performance change.