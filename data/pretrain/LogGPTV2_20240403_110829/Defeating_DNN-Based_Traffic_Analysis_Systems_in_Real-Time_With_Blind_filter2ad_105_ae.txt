compares the performance of timing perturbations on Deep-
Corr with different attack strengths; it also shows the impact
of arbitrary Laplace distributed perturbations on DeepCorr.
We also apply our timing perturbations on Var-CNN. Ta-
ble 4 shows our attack success (A) with and without an invis-
ibility constraint. We realize that timing perturbations have
USENIX Association
30th USENIX Security Symposium    2715
ApplicationTor clientPerturbation NoiseInjection NoiseNew packetBlind Adversarial ModelEncryptionDelay PaddingTimingSizeInject Packets Pluggable Transport LayerDecryptionRemove Dummy PacketsSenderPluggable Transport LayerReceiverTor clientTor bridgeReal-time packetPerturbation Generator ModelReal-time packetsTable 2: Direction perturbation attack on DeepFingerprinting [50] WF scheme (92% WF accuracy)
α
20
100
500
1000
2000
Bandwith Overhead (%)
0.04
2.04
11.11
25.0
66.66
SU-DU (%) Max ST-DU (#, %) Min ST-DU (#, %) Max SU-DT (#, %) Min SU-DT (#, %) Max ST-DT (#←#, %) Min ST-DT (#←#, %)
24.2
49.6
91.8
95.7
97.7
−,0.0
−,0.0
23 ← 69,0.1
72 ← 47,4.4
78 ← 60,35.4
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,0.0
47,0.0
49,4.0
21,29.0
48,94.7
77,31.9
34,77.6
92,97.1
−,100.0
−,100.0
4,0.1
89,13.2
82,47.8
10,67.0
37,89.4
Table 3: Direction perturbation attack on Var-CNN [3] WF scheme (93% WF accuracy)
α
20
100
500
1000
2000
Bandwith Overhead (%) A :
0.04
2.04
11.11
25.0
66.66
SU-DU (%) Max ST-DU (#, %) Min ST-DU (#, %) Max SU-DT (#, %) Min SU-DT (#, %) Max ST-DT (#←#, %) Min ST-DT (#←#, %)
76.1
80.3
96.8
98.2
99.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,0.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
2,68.3
4,76.5
3,98.9
−,100.0
−,100.0
−,0.0
−,0.0
−,10.0
−,20.0
−,30.0
8,53.2
2,66.8
9,81.7
0,96.6
8,97.6
much larger impacts on Var-CNN than direction perturbations.
Moreover, as expected, in the untargeted scenario (SU-DU) and
for different bandwidth overheads, our attack has better perfor-
mance without the invisibility constraint. However, even with
an invisibility constraint, our attack reduces the accuracy
of Var-CNN drastically, i.e., a blind timing perturbation
with an average 0 and a tiny standard deviation of 20ms
reduces the accuracy of Var-CNN by 89.6%.
7.3 Adversarially Perturbing Sizes
We evaluate our size perturbation attack on DeepCorr, which
is the only system (among the three we studied) that uses
packet sizes for trafﬁc analysis. As DeepCorr is mainly stud-
ied in the context of Tor, our perturbation algorithm enforces
the size distribution of Tor on the generated size perturbations.
Figure 6 shows the results when the adversary only manipu-
lates packet sizes. As can be seen, size perturbations are less
impactful on DeepCorr than timing perturbations, suggesting
that DeepCorr is more sensitive to the timings of packets.
7.4 Perturbing Multiple Features
In this section, we evaluate the performance of our adversarial
perturbations when we perturb multiple features simultane-
ously. Var-CNN uses both packet timing and directions to
ﬁngerprint websites. Table 5 shows the impact of adversar-
ially perturbing both of these features on Var-CNN; we see
that combining perturbation attacks increases the impact
of the attack, e.g., in the untargeted scenario (SU-DU), the
combination of both attacks with parameters α = 100,µ =
0,σ = 10ms results in an attack success of A = 83.9% while
the time-based and direction-based perturbations alone re-
sult in A = 68.1% and A = 80.3%, respectively. Similarly,
in Figure 7, we see that by combining time and size pertur-
bations, the accuracy of DeepCorr drops from 95% to 59%
(with FP = 10−3) by injecting only 20 packets, while using
only time perturbations the accuracy drops to 78%.
7.5 Comparison With Traditional Attacks
There exist traditional attacks on DNN-based trafﬁc analysis
systems that use techniques other than adversarial perturba-
tions. In this section, we compare our adversarial perturbation
attacks with such traditional approaches.
Packet insertion techniques: Several WF countermeasures
work by adding new packets. We show that our adversarial
perturbations are signiﬁcantly more effective with simi-
lar overheads. WTF-PAD [28] is a state-of-the-art technique
which adaptively adds dummy packets to Tor trafﬁc to evade
website ﬁngerprinting systems. Using WTF-PAD on the DF
dataset reduces the WF accuracy to 3% at the cost of a 64%
bandwidth overhead. Similarly, the state-of-the-art Walkie-
Talkie [61] reduces DF’s accuracy to 5% with a 31% band-
width overhead and a 36% latency overhead [50]. On the other
hand, our injection-based targeted blind adversarial attack re-
duces the detection accuracy to 1% (close to random guess)
with only a 25% bandwidth overhead and no added latency
(using the exact same datasets). To compare existing WF coun-
termeasures with our results while using Var-CNN model, we
refer to their paper [3] where WTF-PAD can decrease the
accuracy of Var-CNN by 0.4% (from 89.2% to 88.8%) with
27% bandwidth overhead. However, according to Table 3,
with a similar bandwidth overhead (1000 inserted packets and
25% overhead), our attack reduces the accuracy by 91.6%
which signiﬁcantly outperforms WTF-PAD. Our results
suggest that, our blind adversarial perturbation technique
drastically outperforms traditional defenses against deep
learning based website ﬁngerprinting systems.
Time perturbation techniques: Figure 4c compares our tech-
nique with a naive countermeasure of adding random Lapla-
cian noise to packet timings. We see that by adding a Laplace
noise with zero mean and 20ms standard deviation, the accu-
racy of DeepCorr drops from 0.88 TP (for 10−3 FP) to 0.78
TP, but using our adversarial perturbation technique with the
same mean and standard deviation, the accuracy drops to 0.68
and 0.71 without and with invisibility, respectively.
2716    30th USENIX Security Symposium
USENIX Association
(a) µ = 0ms
(b) σ = 20ms
(c) µ = 0ms with invisibility constraint
Figure 4: Timing perturbations on DeepCorr for different attack strengths, with/without an invisibility constraint.
Figure 5: Blind timing perturbations
generated to follow a Laplace distribu-
tion with µ = 0,σ = 30ms.
Figure 6: Size perturbations on DeepCorr
for different attack strengths
Figure 7: Hybrid size/timing perturba-
tions on DeepCorr for different attack
strengths
Non-blind adversarial perturbations: Two recent works [26,
62] use “non-blind” adversarial perturbations to defeat trafﬁc
analysis classiﬁers. As discussed earlier, we consider these
techniques unusable in regular trafﬁc analysis applications,
as they can not be applied on live connections. Nevertheless,
we show that our technique even outperforms these non-blind
techniques; for instance, when DF is the target system, Mock-
ingbird [26] reduces the accuracy of DF by 59.8% with a
56.5% bandwidth overhead (in full-duplex mode), while our
direction-based blind perturbation technique reduces the ac-
curacy of DF by much higher 91.8% and with a much lower
bandwidth overhead of 11.11%.
8 Countermeasures
In this section, we evaluate defenses against our blind adver-
sarial perturbations. We start by showing why our perturba-
tions are hard to counter. We will then borrow three coun-
termeasure techniques from the image classiﬁcation domain,
and show that they perform poorly against blind adversarial
perturbations. Finally, we will design a tailored, more efﬁcient
defense on blind adversarial perturbations.
Uniqueness of Our Adversarial Perturbations: A key
property that impacts countering adversarial perturbations
is the uniqueness of adversarial perturbations: if there is only
one (few) possible adversarial perturbations, the defender can
identify them and train her model to be robust against the
known perturbations. As explained before, our adversarial
perturbations are not unique: our algorithm derives a pertur-
bation generator (G(z)) that for random zs can create different
perturbation vectors. To demonstrate the non-uniqueness of
our perturbations, we created 5,000 adversarial perturbations
for the applications studied in this paper (we stopped at 5,000
only due to limited GPU memory). Figure 9 shows the his-
togram of the l2 distance between the different adversarial
perturbations that we generated for DeepCorr. We can say
that the generated perturbations are not unique, and, the
adversary cannot easily detect them. These different per-
turbations however cause similar adversarial impacts on their
target model, as shown in Figure 8.
Adapting Existing Defenses: Many defenses have been de-
signed for adversarial examples in image classiﬁcation appli-
cations, particularly, adversarial training [32,34,55], gradient
masking [43, 48], and region-based classiﬁcation [7]. In Ap-
pendix A, we discuss how we deploy these defenses.
Our Tailored Defense: We use the adversarial training ap-
USENIX Association
30th USENIX Security Symposium    2717
10−410−310−210−1100False Positive0.00.20.40.60.81.0True Positiveσ = 10 msσ = 20 msσ = 50 msno noise10−410−310−210−1100False Positive0.00.20.40.60.81.0True Positiveμ = 0 msμ = 20 msμ = 50 msno noise10−410−310−210−1100False Positive0.00.20.40.60.81.0True Positiveσ = 10 msσ = 20 msLaplace σ = 10 msLaplace σ = 20 msno noise-0.10-0.06-0.020.020.040.10Delay (seconds)0.000.010.020.030.040.05ProbabilityBlind adversarial noiseLaplace distribution10−410−310−210−1100False Positive0.00.20.40.60.81.0True PositiveN = 20 KBN = 40 KBN = 100 KBno noise10−410−310−210−1100False Positive0.00.20.40.60.81.0True PositiveN = 0N = 20N = 50no noiseTable 4: Timing perturbation attack on Var-CNN [3] WF scheme (93% WF accuracy)
A :
µ,σ
0,5
0,10
0,20
0,30
0,50
Limited Noise
SU-DU (%) Max ST-DU (#, %) Max SU-DT (#, %) Max ST-DT (#←#, %)
37.7
66.2
96.0
94.0
98.7
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
17,38.3
53,83.4
80,94.8
80,99.1
80,100.0
100.0
100.0
100.0
100.0
100.0
Stealthy Noise
SU-DU (%) Max ST-DU (#, %) Max SU-DT (#, %) Max ST-DT (#←#, %)
22.0
38.2
89.2
90.4
97.9
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
17,40.3
53,83.4
80,95.8
80,99.7
80,100.0
100.0
100.0