seeds planted by the malicious actor into an organic
disinformation campaign where content is shared by
both troll accounts and legitimate users [47].
5) After the disinformation campaign is over, the malicious
actor might reset the troll accounts, deleting their posts
and/or changing their proﬁle traits [74]. For example,
Russian-sponsored troll accounts participated in disin-
formation campaigns about Crimea but later changed
identity and started focusing on US-based political is-
sues. This makes detection more difﬁcult, because of the
lack of visibility on past identities and deleted content.
B. Reddit
Reddit is a popular news aggregation site, where content is
organized into millions of user-generated communities, called
subreddits, covering topics of interest ranging from news and
sports [64] to conspiracy theories [45]. A user can create a
thread – more speciﬁcally, a submission – and other users
can reply in a structured manner by posting comments. That
is, users can reply to the submission itself or to comments
to the submission. We focus on Reddit because this platform
has become popular among Internet users, in particular when
discussing news [45, 63, 64, 73]. Also, the fact that posts on
Reddit are organized in threads allows us to study the inter-
action between troll accounts, which is key to our approach.
C. Dataset
As ground truth, we use data released by Reddit on troll
accounts active between 2015 and 2018 [43], a timeline that
includes the 2016 Brexit Referendum, the 2016 US Presiden-
tial Elections, and the 2018 US Midterm Elections. The dataset
includes 335 accounts, which generated 21,321 posts. We then
collect a large set of Reddit accounts using the Pushshift public
archives [2]. These include all public posts made on Reddit in
2005–2020, accounting for 600M posts and 5B comments in
2.8M subreddits [2].
Ethics. Since we only use publicly available data and do
not interact with human subjects, our work is not considered
human subjects research by our IRB. Also, we follow standard
ethics guidelines: when presenting examples, we remove any
personally identiﬁable information and do not deanonymize
users.
III. CHARACTERIZING TROLL ACTIVITY
Our main intuition is that troll accounts show behavioral
traits that are different enough from regular users to allow for
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2162
(a) Started
(b) Commented on
(c) Same Title
Fig. 1: Cumulative distribution functions (CDFs) of the fraction of comments in posts that users in the same class (a) started
or (b) commented on, and (c) fraction of posts with the same title as posts made by another user in the same class.
accurate automated detection. In particular, our hypothesis is
that the interaction patterns shown by troll accounts controlled
by the same actor will reveal patterns typical of loose coordi-
nation.
To investigate the viability of this approach, we analyze the
activity of the 335 Russian-sponsored troll accounts described
in Section II-C. As a baseline for comparison, we also extract
all the posts and comments made by a set of random accounts
from the Pushshift dataset. More precisely, we ﬁrst identify
the top 50 subreddits where the troll accounts were active;
these include general audience communities like r/News and
r/Politics as well as more specialized ones like r/Bitcoin. We
then extract 1,000 random accounts from these subreddits.
We analyze these two groups of accounts along three dimen-
sions. First, we want to investigate the assumption that troll
accounts are more likely to comment on posts that were started
by other troll accounts than random users are to comment on
posts by other random users. This has already been observed,
at least anecdotally [37], as troll accounts might try to simulate
legitimate discussion to push their disinformation narratives
among regular users. Figure 1(a) shows that troll accounts are
indeed more likely to comment on posts started by other troll
accounts than random users are on posts by another random
user (2-sample KS statistic = 0.143, p < 0.001).
Second, we compare threads receiving comments from two
or more troll accounts to those where more than one of our ran-
dom users left a comment. The rationale is that troll accounts
simulate exchanges of opinions aiming to polarize discussion
and entice legitimate users to chime in [37]. Figure 1(b)
conﬁrms that troll accounts are more likely to comment on
the same posts as other trolls than two random users are (2-
sample KS statistic = 0.132, p < 0.001).
Third, we look at posts created by troll accounts with the
same title as posts by other troll accounts. The idea is not that
they spam the same message multiple times (in fact, this would
be trivial to detect) but that, when sharing a link to a Web page,
the title of the Reddit post is set by default to that of the Web
page. Therefore, we expect that multiple accounts sharing the
same news article (e.g., as part of a disinformation campaign)
create posts with the same title. Figure 1(c) conﬁrms that this is
the case: troll accounts are signiﬁcantly more likely to share
two posts with the same title than random accounts are (2-
sample KS statistic 0.346, p < 0.001).
Overall, this shows that troll accounts do behave differently
than regular accounts on Reddit, and indicates that we can
leverage behavioral features to automatically identify them.
IV. OVERVIEW OF TROLLMAGNIFIER
In this section, we provide an overview of TROLLMAGNI-
FIER; our system takes a seed dataset of known troll accounts
and analyzes Reddit to ﬁnd more accounts that behave in a
similar fashion and are likely troll accounts.
TROLLMAGNIFIER operates in ﬁve stages: 1) First,
it
identiﬁes a set of suspicious accounts that have a higher
likelihood to be troll accounts, compared to random Reddit
accounts, then, 2) extracts all comments and submissions by
those accounts and builds a thread structure from these posts.
3) Next, TROLLMAGNIFIER trains a detection model based
on a set of pre-identiﬁed features, and 4) uses this model to
detect troll accounts in the wild. 5) Finally, TROLLMAGNIFIER
provides a number of additional analyses, giving moderators
more details on the activity of detected accounts, and allowing
them to make informed decisions on whether the account
needs to be suspended.
A. Pre-Filtering
In the ﬁrst step, TROLLMAGNIFIER identiﬁes a set of
accounts that are likely to be troll accounts. We do so to
obtain a dataset of accounts that have similar posting habits
as troll accounts and thus has a higher chance of containing
potential troll accounts. Based on the observations highlighted
in Section III, TROLLMAGNIFIER considers a Reddit account
as a potential troll account if they do any of the following:
1) Commenting on a troll account’s submission: As shown
in Figure 1(a), troll accounts are more likely to comment
on a thread that was started by another troll account
than by a random one. As mentioned, this is done to
simulate genuine interaction and lure unwitting users
into the discussion.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2163
0.00.20.40.60.81.0Fraction of comments on posts that users started0.00.20.40.60.81.0Fraction of UsersRandom UsersKnown Troll Accounts0.00.20.40.60.81.0Fraction of comments on posts that users commented0.00.20.40.60.81.0Fraction of UsersRandom UsersKnown Troll Accounts0.00.20.40.60.81.0Fraction of same title post as user0.00.20.40.60.81.0Fraction of UsersRandom UsersKnown Troll AccountsFig. 2: Overview of TROLLMAGNIFIER: two input streams are fed to the system: (1) a seed dataset of known troll accounts
and (2) the entire Reddit data. TROLLMAGNIFIER ﬁlters a set of accounts from the Reddit dataset that have a high likelihood
of being troll accounts, and builds the thread structure of comments as seen on Reddit. Next, a detection model is built using
pre-identiﬁed features and used to detect troll accounts in the wild. Finally, TROLLMAGNIFIER further analyzes the detected
accounts to provide additional details about them.
2) Making a submission with the same title as a troll
account: As evident from Figure 1(c), troll accounts
are more likely to post submissions with the same title.
This is not necessarily due to troll accounts manually
selecting the same title, but a side effect of how Reddit
works: by default, when posting a URL, the submission’s
title is set as the title of the target page.
Note that, unlike the features later used by TROLLMAGNI-
FIER for classiﬁcation, these pre-ﬁltering conditions are only
determining whether a Reddit account has shown activity that
may be indicative of it being a troll. In later steps, TROLL-
MAGNIFIER analyzes, in detail, the activity of this candidate
set of accounts, looking at how similar their activity is to
known troll accounts, thus making a classiﬁcation decision.
B. Building Threads
After TROLLMAGNIFIER identiﬁes a set of suspicious ac-
counts based on these two indicators, it proceeds to further
process their data as collected from Pushshift.
As discussed, comment threads on Reddit are organized as
trees. This allows us to identify the speciﬁc comment a user
replied to, and identify groups of users who have been con-
versing with each other. However, the Pushshift data (which
we use as our source) does not return these trees but provides
comments in a ﬂat structure. In this step, TROLLMAGNIFIER
parses these comments to build the comment tree, which is
then used in the next steps.
More precisely, Pushshift stores each comment as a separate
JSON object, where the link id is the ID of the submission
and the parent id is the ID of its parent comment. If the
link id and parent id are the same, the comment is a top-level
comment; i.e., it is a direct reply to the submission. A comment
can have any number of replies. To build the comment tree,
TROLLMAGNIFIER extracts link id of all comments made
by the known troll accounts. Then, it uses the list of IDs
to query the Pushshift data and ﬁnds all the comments on
those submissions, not just the ones made by the known troll
accounts. It also uses a list of submission IDs of known troll
accounts and ﬁnds all the comments which have a link id from
that list to ﬁnd all comments on troll account submissions.
Once this data is retrieved, the link id’s and parent id’s are
used to recreate the comment threads.
C. Building the Detection Model
Next, TROLLMAGNIFIER builds a machine learning model
to distinguish between legitimate and troll accounts. To
achieve this, we ﬁrst manually select features based on our
observations from Section III. We then train supervised learn-
ing models to perform detection.
Feature Engineering. As discussed earlier,
troll accounts
exhibit recognizable behaviors, which we translate into nine
features. In the following, we describe each feature and the
reason why we select them.
1) Total Comments: From our analysis in Section III, we
ﬁnd that troll accounts post 21 comments on average,
compared to 300 for a random account. Therefore, we
use the total number of comments made by an account
as a feature.
2) Total Submissions: Troll accounts make an average of
42 submissions, while a random account makes 32.
Therefore, we use the total number of submissions made
by the user as a feature.
3) Account Age: Troll accounts are often created in waves,
at or around the same time [74]. Therefore, we select
the time elapsed (in years) since the ﬁrst submission or
comment made by the user as a feature.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:10 UTC from IEEE Xplore.  Restrictions apply. 
2164
Seed Data AnalysisDetection of Troll Accountsin the WildFeature EngineeringPrefilteringBuilding theDetection ModelBuilding ThreadsSeed DataReddit DataValidation4) Fraction of submissions with the same title as troll
accounts: Figure 1(c) shows that troll accounts are much
more likely to make a post with the same title as another
troll account. To include this information into our model,
we calculate this feature as the fraction of submissions
by the user that have the same title as a known troll
account’s submission.
5) Fraction of comments on submissions that troll accounts
commented: Recall that troll accounts are more likely to
comment on the same posts (see Figure 1(b)). Therefore,
we compute this feature as the fraction of comments by
the user that are on a submission that a known troll
account commented on.
6) Fraction of comments on submissions by troll accounts:
Figure 1(a) shows that troll accounts are more likely
to comment on posts started by other troll accounts.
To account for this, we add this feature as the fraction
of comments by the user on a known troll account’s
submission.
7) Fraction of direct replies on submissions by troll ac-
counts: Troll accounts have 0.5 direct replies on aver-
age on submissions by troll accounts, while a random
account has none. Therefore, the fraction of direct com-
ments (excluding comment threads) by the user on a
known troll account’s submission is used as a feature.
8) Fraction of comments that are a reply to a troll account’s
comment: We ﬁnd 49 instances of troll accounts replying
to each other in comments, while random accounts never
interact with each other. Therefore, we add this feature
as the fraction of comments by the user that are a reply
to a known troll account’s comment.
9) Fraction of comments that are a reply to a troll account’s
comment in a troll account’s submission: We ﬁnd 25
instances of troll accounts replying to each other in
the comments under a submission made by another
troll account, while random accounts never interact this
way. To capture this information, we add this feature as
the fraction of comments by the user that are a reply
to a known troll account’s comment on a known troll
account’s submission.
Building the model. Based on the set of features discussed
above, TROLLMAGNIFIER trains a supervised model to dis-
tinguish between troll and legitimate Reddit accounts. As
we discuss in detail
in Section V-C, we experiment with
four classiﬁers: K-nearest neighbors [11], Decision Tree [51],
Support Vector Machine (SVM) [21], and Random Forests [6].
D. Troll Detection
Once TROLLMAGNIFIER is trained, it can be used to detect
troll accounts in the wild, as a classiﬁcation task. That is,
TROLLMAGNIFIER will return a set of detected troll accounts,
which then go through further analysis and validation.
E. Validation
After detecting accounts that are likely troll accounts,
TROLLMAGNIFIER performs additional analysis to identify
more indicators providing evidence that the accounts indeed
belong to troll campaigns. These checks serve to both validate
our results and assist Reddit moderators by providing addi-
tional insights into the accounts. To provide a comprehensive
validation of its detection results, TROLLMAGNIFIER performs
two types of analyses. First, it looks at the detected accounts
at an individual level, looking for indicators that they are
likely trolls. Second, it takes all detected accounts as a group,
and looks at similarity in their collective activity with that
of accounts in the seed set. Note that this step takes into
account elements that are not used as detection features by
TROLLMAGNIFIER.
Account-level indicators. To further analyze detected troll
accounts at an individual level, TROLLMAGNIFIER looks at
four aspects: 1) whether the account was deleted or suspended;
2) whether the account deleted any of their comments or
submissions; 3) whether the account was created on the same
day as one of the known troll accounts; and 4) whether the
account posted a submission or a comment containing one of
the top keywords used by the known trolls. In the following,
we discuss there four criteria in detail.
Active Status. While they were not
identiﬁed as state-
sponsored troll accounts by Reddit, it is possible that the
accounts identiﬁed by TROLLMAGNIFIER triggered other de-
tection systems and were subsequently suspended, or that they
were reported by Reddit users and later blocked by moderators.
Therefore, TROLLMAGNIFIER checks whether the detected
accounts have been banned or suspended by Reddit as an
additional indicator of troll behavior.
Deleted Messages. Previous work [74] shows evidence of troll