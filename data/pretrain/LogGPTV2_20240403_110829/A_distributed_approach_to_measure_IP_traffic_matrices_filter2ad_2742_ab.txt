to assemble all the information at the right granularity level
(link, router or PoP) in a way that is consistent with the
network topology.
In this section we provide a generic algorithm for doing
steps 2 and 3. The Netﬂow data, gathered for step 1, serves
as input to this algorithm. We describe the state of the art
today for implementing such an algorithm. This is based on
Netﬂow v8 and essentially requires a (semi-)centralized ap-
proach2. Cisco’s most recent release of Netﬂow, v9, makes
initial steps toward enabling the TM to be computed us-
ing a distributed approach. However we will see that this
version does not go far enough to enable a truly distributed
approach. We quantify the storage and communication over-
heads for both centralized and distributed approaches.
3.1
Identifying the egress node
A traﬃc matrix is typically computed for a single domain
or Autonomous System (AS). As described above, Netﬂow
statistics collected at each router are computed at the granu-
larity level of source and destination network preﬁxes. These
source and destination preﬁxes will often reside outside the
AS whose traﬃc matrix is computed. Thus the source and
destination of each packet need to be mapped onto the en-
try node and exit node within the given AS. Identifying the
entry node is simple, as it is deﬁned as the link or node
where a packet enters a given domain (i.e., the place where
Netﬂow sees the packet). The exit point for a particular
source/destination network preﬁx ﬂow will depend both on
its entry point as well as its actual destination. To identify
this exit point one needs to obtain a view of the forwarding
table of the router that recorded the ﬂow. Consolidation of
intra- and inter-domain routing (from the vantage point of
the router), as well as topological information can resolve
each preﬁx ﬂow into its egress node inside the network.
The task at hand is to accurately map the destination net-
work preﬁx in each ﬂow record to (i) a backbone-gateway
egress link, (ii) an egress backbone router, and (iii) its egress
PoP. To do this we need the BGP routing table from each
2Throughout the remainder of the paper we will use the term
centralized to describe fully centralized or semi-centralized
approaches. The latter would employ multiple collection
stations, say one in each PoP, where a subset of the network
routers will report their ﬂow statistics.
Figure 1: Setup for Netﬂow data collection.
We used Netﬂow v8 that is usually referred to as “sam-
pled aggregated Netﬂow” [2]. Each record for Netﬂow v8 is
40 bytes long, and features a ﬂow identiﬁer, its source and
destination network preﬁx, its associated load in bytes and
packets, as well as its starting and ending time. Rather than
examine every packet, Netﬂow v8 employs periodic sampling
in which one sample is collected every 250th packet. Traﬃc
statistics are not computed over each packet but based on
this subset of the packets.
Each router ships the collected Netﬂow statistics to a con-
ﬁgurable node (the collection station) every 5 minutes. We
instrumented our European backbone with a single collec-
tion station for all 27 routers in this backbone. We used
5 minutes as our reporting interval since it coincides with
the default SNMP reporting time interval. The collection
station stores all ﬂow statistics from all backbone routers in
the IP network. We collected 3 weeks of data during the
summer of 2003.
2.2 Trafﬁc Matrix Granularities
The elements of a traﬃc matrix are origin-destination
(OD) ﬂows where the deﬁnition of the origin and destina-
tion object (i.e., node) can be selected as per the needs of
the application using the traﬃc matrix. The granularity of
a traﬃc matrix is determined by the choice of deﬁnition for
the source or destination “object”. The most typical objects
are links, nodes and PoPs. In this work we consider traf-
ﬁc matrices at the granularity of “link to link”, “router to
router”, and “PoP to PoP”. For a taxonomy of IP traﬃc ma-
trices and their potential applications in traﬃc engineering
and network planning please refer to [7].
In addition to selecting the ﬂow granularity of a traﬃc
matrix, a network operator also needs to specify its time
granularity. As mentioned above, the Netﬂow statistics are
collected every ﬁve minutes. When we build traﬃc matrices
from our data we average over one hour time intervals. We
thus have one traﬃc matrix for each hour for a three week
period. When we estimate traﬃc matrices using our pro-
posed scheme, we generate estimates for one hour averages.
We choose to focus on one hour since most traﬃc engineer-
ing applications are targeted toward longer time scales. One
hour is actually small for such applications but we believe
this is a useful time scale because it allows us to observe
diurnal patterns [9] and busy periods that may last for a
same source and destination node.
PeeringlinksCustomerlinkslinksPeeringlinksCustomermonitoredlinksnon−monitoredlinksgwgwgwgwgwgwbbbbbbbbPoP1PoP2router (typically we collect those from a route-reﬂector in-
side a PoP), the ISIS/OSPF link weights, and the router-
level topology [5].
If a network preﬁx is advertized through BGP, then the
BGP table speciﬁes the last router within the AS that needs
to forward traﬃc for this destination preﬁx, usually referred
to as “BGP next hop”. However, often the BGP next hop
corresponds to the IP address of the ﬁrst router outside the
AS. In this case, conﬁguration ﬁles can be used to map this
address to a router interface within our network.
This identiﬁed router interface will very likely correspond
to the interface of a gateway router. At this point, we can
identify the egress PoP but we need additional information
to be able to map this router interface (for the given des-
tination preﬁx) to a particular egress backbone router, and
backbone-gateway link. Here we use the router topology,
along with the associated link weights, to compute the short-
est paths across the network. Using these paths we can ﬁnd
the egress backbone router(s) and backbone-gateway link(s)
that will forward traﬃc toward the previously identiﬁed
gateway router. The number of such routers and links may
be more than one, if the PoP topology is densely meshed
and equal-cost multipath is enabled by the provider. In that
case, we apportion the total ﬂow equally toward each one
of the routers or links selected using the process mentioned
above.
3.2 Computing the trafﬁc ﬂow
A generic algorithm for the computation of the traﬃc ma-
trix of an IP network can be summarized as in Fig. 2. At the
heart of this algorithm is a routine called f ind egress node(f )
that returns the egress node at the desired level of granular-
ity (link, node or PoP) according to the method described
above. There are four nested loops in this algorithm, one for
each time interval n, one for each router r, one for each link
l and one for each ﬂow f . The f ind egress node(f ) routine
operates at the level of a ﬂow because that is the form of the
Netﬂow input. After the egress node is identiﬁed, the ﬂows
are aggregated so that the algorithm yields a traﬃc matrix
at each of the granularity levels. In this pseudocode, L(r)
denotes the number of links at router r, and F (l) denotes
the number of ﬂows on link l.
We note that a variant of this algorithm was originally
proposed in [5]. We include this here not as a contribu-
tion, but merely in order to facilitate the ensuing discus-
sion. This algorithm statement makes it easy to see how
the overheads are computed, to identify all the additional
routing/conﬁguration data needed, and to clarify what the
change from a centralized to a distributed approach implies.
A Centralized Approach. Because Netﬂow today does
not implement a procedure such as f ind egress node(f ),
all of the ﬂow data needs to be shipped by each router to
a speciﬁc collection station that can carry out the above al-
gorithm. Thus today’s state of the art essentially mandates
a centralized solution. The collection station needs to have
explicit information about each PoP’s BGP routing table,
and the ISIS weights in eﬀect at each time interval n. In
addition, it needs to have an accurate view of the network
topology, in terms of the conﬁguration of each router inside
the network. For our implementation of a centralized solu-
tion, we downloaded router conﬁguration ﬁles once a week,
and BGP routing tables once a day from each PoP inside the
network. Due to the fact that router conﬁguration ﬁles do
Algorithm : ComputeTM(data, T, R, L, F )
for n ← 1 to T
ISIS = isis(n); %the same topology network − wide
conf iguration = ∪R
for r ← 1 to R
r=1conf igurationf ile(r, n);
routingtable = BGP routingtable(RR(r), n);
%BGP routing table of the route ref lector
in r(cid:48)s P oP.
for l ← 1 to L(r)
for f ← 1 to F (l)
EN (f ) = f ind egress node(f, routingtable,
conf iguration, ISIS);
T M (l, EN (f )) = T M (l, EN (f )) + data(f, t);
return (T M )
Figure 2: Pseudocode for the computation of the
traﬃc matrix.
not change on a daily basis, and that routing table changes
occurring during a single day rarely aﬀect large amounts
of traﬃc [10], we found these choices reasonable. Certainly
there are inaccuracies incurred by not having perfectly up to
date routing information. However obtaining more frequent
updates of this information greatly increases the communi-
cation overhead. (The only way to completely avoid these
inaccuracies is to use a distributed approach as described
below.)
Toward Distributed Approaches. A process similar
to f ind egress node(f ) is already performed by the router
itself before switching the packet to its destination. There-
fore a truly distributed approach would be one in which each
router saved the information on the egress point of each
network preﬁx while performing the lookup in its routing
tables. Since one router constitutes one source that sends
potentially to all other routers in the network, saving traﬃc
statistics on the amount of traﬃc destined to each egress
point is equivalent to the router computing one row of the
traﬃc matrix. With this approach the only data that needs
to be shipped to a collection station is the TM row itself.
In order to do this, the router would need to change the
ﬂow record to include ﬁelds such as egress link, egress router,
and/or egress PoP. If ﬂow records were kept at the level of
links or routers rather than preﬁxes, this would dramat-
ically reduce the on-router storage. The communications
overhead is also greatly scaled back since sending one row,
of even a link-to-link traﬃc matrix, is far smaller than ship-
ping individual preﬁx-level ﬂow records. Furthermore, the
computational overhead at the collection station has now
been reduced to simply that of assembling the rows without
any egress node identiﬁcation activity.
Recent advances in the area of ﬂow monitoring have led
to new deﬁnitions for ﬂow records that do incorporate ex-
plicit routing information deﬁning ﬂows such that the des-
tination ﬁeld captures the BGP next hop address. Such a
change can be found in Netﬂow v9 [3] which thus constitutes
a signiﬁcant movement toward the distributed solution de-
scribed above. This improvement is not yet suﬃcient though
for the following reason. When a particular 5-tuple ﬂow is
mapped onto a “BGP-next-hop” ﬂow, there is always the
risk that the destination network preﬁx may not be adver-
tized through BGP. For ease of implementation Netﬂow v9
addressed this issue by using ”0.0.0.0” as the BGP next hop.
Such a design choice implies that all the traﬃc that may be
going to internal customers is missed, if these customers are
not advertized through BGP. For ISPs with a large number
of customers, this may translate to many elements of the
traﬃc matrix being altogether missed or inaccurately esti-
mated, when a particular “unknown” ﬂow would actually
map to an existing ﬂow in the cache.
The feasibility of direct measurement approaches is de-
pendent upon the ability to implement a routine such as
ﬁnd egress node(f ) at a router. We distinguish two factors
regarding the implementation of this routine. First, we point
out that the information needed is already available in to-
day’s routers in the Routing Information Base (RIB). The
RIB contains (i) the mapping between each destination pre-
ﬁx and its BGP next hop (as dictated through BGP), (ii)
the mapping between the BGP next hop and its egress node
(as identiﬁed through the intra-domain protocol in use), and
(iii) the mapping between the egress node and the appropri-
ate local interface that should be used to reach that node.
The second factor regarding the implementation is the need
to gain eﬃcient access to this information. This could re-
quire changes in the software architecture and is the main
challenge to implementation. Our goal in this paper is not
to spell out a complete implementation solution, but rather
to identify what is needed to achieve a measurement based
approach to populate traﬃc matrices and avoid inference-
based techniques.
In summary, we thus make two recommendations to de-
velopers of Netﬂow type systems on routers. First, a mech-
anism to resolve destination preﬁxes to their egress link
or router (such as the ﬁnd egress node scheme) is needed
on the router itself. Second, the ﬂow record deﬁnitions
need to be adjusted to include ﬁelds such as egress link
or router. Because router manufacturers have already ac-
commodated new ﬂow deﬁnitions (e.g., the BGP next hop
described above), this indicates that they are heading in the
right direction, and we are hopeful that the proposed rec-
ommendations will eventually be undertaken. The position