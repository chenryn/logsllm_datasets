sition of language constructs. For example, an author’s
tendency to create deeply nested code, unusually long
functions or long chains of assignments cannot be mod-
eled using n-grams alone.
Addressing these limitations requires source code to
be parsed. Unfortunately, parsing C/C++ code using tra-
ditional compiler front-ends is only possible for complete
code, i.e., source code where all identiﬁers can be re-
solved. This severely limits their applicability in the set-
ting of authorship attribution as it prohibits analysis of
lone functions or code fragments, as is possible with sim-
ple n-gram models.
As a compromise, we employ the fuzzy parser Jo-
ern that has been designed speciﬁcally with incomplete
code in mind [32]. Where possible, the parser produces
abstract syntax trees for code fragments while ignoring
fragments that cannot be parsed without further infor-
mation. The produced syntax trees form the basis for
our feature extraction procedure. While they largely pre-
serve the information required to create n-grams or bag-
of-words representations, in addition, they allow a wealth
of features to be extracted that encode programmer habits
visible in the code’s structure.
As an example, consider the function foo as shown
in Figure 1, and a simpliﬁed version of its correspond-
ing abstract syntax tree in Figure 2. The function con-
tains a number of common language constructs found
in many programming languages, such as if-statements
(line 3 and 7), return-statements (line 4, 8 and 10), and
function call expressions (line 6). For each of these con-
structs, the abstract syntax tree contains a corresponding
node. While the leaves of the tree make classical syn-
tactic features such as keywords, identiﬁers and opera-
tors accessible, inner nodes represent operations showing
1https://github.com/calaylin/CodeStylometry
USENIX Association  
24th USENIX Security Symposium  257
3
Figure 1: Sample Code Listing
Figure 2: Corresponding Abstract Syntax Tree
how these basic elements are combined to form expres-
sions and statements. In effect, the nesting of language
constructs can also be analyzed to obtain a feature set
representing the code’s structure.
3.2 Feature Extraction
Analyzing coding style using machine learning ap-
proaches is not possible without a suitable representa-
tion of source code that clearly expresses program style.
To address this problem, we present the Code Stylome-
try Feature Set (CSFS), a novel representation of source
code developed speciﬁcally for code stylometry. Our fea-
ture set combines three types of features, namely lexical
features, layout features and syntactic features. Lexical
and layout features are obtained from source code while
the syntactic features can only be obtained from ASTs.
We now describe each of these feature types in detail.
3.2.1 Lexical and Layout Features
We begin by extracting numerical features from the
source code that express preferences for certain identi-
ﬁers and keywords, as well as some statistics on the use
of functions or the nesting depth. Lexical and layout fea-
tures can be calculated from the source code, without
having access to a parser, with basic knowledge of the
programming language in use. For example, we mea-
sure the number of functions per source line to determine
the programmer’s preference of longer over shorter func-
tions. Furthermore, we tokenize the source ﬁle to obtain
the number of occurrences of each token, so called word
unigrams. Table 2 gives an overview of lexical features.
In addition, we consider layout features that represent
code-indentation. For example, we determine whether
the majority of indented lines begin with whitespace
or tabulator characters, and we determine the ratio of
whitespace to the ﬁle size. Table 3 gives a detailed de-
scription of these features.
Feature
WordUnigramTF
ln(numkeyword/
length)
ln(numTernary/
length)
ln(numTokens/
length)
ln(numComments/
length)
ln(numLiterals/
length)
ln(numKeywords/
length)
ln(numFunctions/
length)
ln(numMacros/
length)
nestingDepth
branchingFactor
avgParams
Deﬁnition
Term frequency of word unigrams in
source code
Log of the number of occurrences of key-
word divided by ﬁle length in characters,
where keyword is one of do, else-if, if, else,
switch, for or while
Log of the number of ternary operators di-
vided by ﬁle length in characters
Log of the number of word tokens divided
by ﬁle length in characters
Log of the number of comments divided by
ﬁle length in characters
Log of the number of string, character, and
numeric literals divided by ﬁle length in
characters
Log of the number of unique keywords
used divided by ﬁle length in characters
Log of the number of functions divided by
ﬁle length in characters
Log of the number of preprocessor direc-
tives divided by ﬁle length in characters
Highest degree to which control statements
and loops are nested within each other
Branching factor of the tree formed by con-
verting code blocks of ﬁles into nodes
The average number of parameters among
all functions
stdDevNumParams The standard deviation of the number of
avgLineLength
stdDevLineLength The standard deviation of the character
parameters among all functions
The average length of each line
lengths of each line
Count
dynamic*
7
1
1
1
1
1
1
1
1
1
1
1
1
1
*About 55,000 for 250 authors with 9 ﬁles.
Table 2: Lexical Features
3.2.2 Syntactic Features
The syntactic feature set describes the properties of the
language dependent abstract syntax tree, and keywords.
Calculating these features requires access to an abstract
syntax tree. All of these features are invariant to changes
in source-code layout, as well as comments.
Table 4 gives an overview of our syntactic features.
We obtain these features by preprocessing all C++ source
ﬁles in the dataset to produce their abstract syntax trees.
258  24th USENIX Security Symposium 
USENIX Association
4
FunctionintfooCompoundStmtIfIfDeclConditionReturnConditionReturnElseOrUnaryOp (-)intretAssign(=)EqExpr (!=)UnaryOp (-)ReturnRelExpr ()1retCallret011x0xMAXbarArgsxFeature
ln(numTabs/length)
ln(numSpaces/length)
ln(numEmptyLines/
length)
whiteSpaceRatio
newLineBefore
OpenBrace
tabsLeadLines
Deﬁnition
Log of the number of tab characters di-
vided by ﬁle length in characters
Log of the number of space characters di-
vided by ﬁle length in characters
Log of the number of empty lines divided
by ﬁle length in characters, excluding
leading and trailing lines between lines of
text
The ratio between the number of whites-
pace characters (spaces, tabs, and new-
lines) and non-whitespace characters
A boolean representing whether the ma-
jority of code-block braces are preceded
by a newline character
A boolean representing whether the ma-
jority of indented lines begin with spaces
or tabs
Count
1
1
1
1
1
1
Table 3: Layout Features
An abstract syntax tree is created for each function in the
code. There are 58 node types in the abstract syntax tree
(see Appendix A) produced by Joern [33].
Feature
MaxDepthASTNode
ASTNodeBigramsTF
ASTNodeTypesTF
Deﬁnition
Maximum depth of an AST node
Term frequency AST node bigrams
Term frequency of 58 possible AST
node type excluding leaves
Count
1
dynamic*
58
ASTNodeTypesTFIDF Term frequency inverse document fre-
quency of 58 possible AST node type
excluding leaves
ASTNodeTypeAvgDep Average depth of 58 possible AST
58
58
84
dynamic**
dynamic**
dynamic**
cppKeywords
CodeInASTLeavesTF
CodeInASTLeaves
TFIDF
node types excluding leaves
Term frequency of 84 C++ keywords
Term frequency of code unigrams in
AST leaves
Term frequency inverse document fre-
quency of code unigrams in AST
leaves
Average depth of code unigrams in
AST leaves
CodeInASTLeaves
AvgDep
*About 45,000 for 250 authors with 9 ﬁles.
**About 7,000 for 250 authors with 9 ﬁles.
**About 4,000 for 150 authors with 6 ﬁles.
**About 2,000 for 25 authors with 9 ﬁles.
Table 4: Syntactic Features
The AST node bigrams are the most discriminating
features of all. AST node bigrams are two AST nodes
that are connected to each other. In most cases, when
used alone, they provide similar classiﬁcation results to
using the entire feature set.
The term frequency (TF) is the raw frequency of a
node found in the abstract syntax trees for each ﬁle. The
term frequency inverse document frequency (TFIDF) of
nodes is calculated by multiplying the term frequency of
a node by inverse document frequency. The goal in using
the inverse document frequency is normalizing the term
frequency by the number of authors actually using that
particular type of node. The inverse document frequency
is calculated by dividing the number of authors in the
dataset by the number of authors that use that particular
node. Consequently, we are able to capture how rare of a
node it is and weight it more according to its rarity.
The maximum depth of an abstract syntax tree re-
ﬂects the deepest level a programmer nests a node in the
solution. The average depth of the AST nodes shows
how nested or deep a programmer tends to use particular
structural pieces. And lastly, term frequency of each C++
keyword is calculated. Each of these features is written
to a feature vector to represent the solution ﬁle of a spe-
ciﬁc author and these vectors are later used in training
and testing by machine learning classiﬁers.
3.3 Classiﬁcation
Using the feature set presented in the previous section,
we can now express fragments of source code as numeri-
cal vectors, making them accessible to machine learning
algorithms. We proceed to perform feature selection and
train a random forest classiﬁer capable of identifying the
most likely author of a code fragment.
3.3.1 Feature Selection
Due to our heavy use of unigram term frequency and
TF/IDF measures, and the diversity of individual terms
in the code, our resulting feature vectors are extremely
large and sparse, consisting of tens of thousands of fea-
tures for hundreds of classes. The dynamic Code stylom-
etry feature set, for example, produced close to 120,000
features for 250 authors with 9 solution ﬁles each.
In many cases, such feature vectors can lead to over-
ﬁtting (where a rare term, by chance, uniquely identiﬁes
a particular author). Extremely sparse feature vectors
can also damage the accuracy of random forest classi-
ﬁers, as the sparsity may result in large numbers of zero-
valued features being selected during the random sub-
sampling of the features to select a best split. This re-
duces the number of ‘useful’ splits that can be obtained
at any given node, leading to poorer ﬁts and larger trees.
Large, sparse feature vectors can also lead to slowdowns
in model ﬁtting and evaluation, and are often more difﬁ-
cult to interpret. By selecting a smaller number of more
informative features, the sparsity in the feature vector can
be greatly reduced, thus allowing the classiﬁer to both
produce more accurate results and ﬁt the data faster.
We therefore employed a feature selection step using
WEKA’s information gain [26] criterion, which evaluates
the difference between the entropy of the distribution of
classes and the entropy of the conditional distribution of
classes given a particular feature:
IG(A,Mi) =H (A)− H(A|Mi)
(1)
5
USENIX Association  
24th USENIX Security Symposium  259
where A is the class corresponding to an author, H is