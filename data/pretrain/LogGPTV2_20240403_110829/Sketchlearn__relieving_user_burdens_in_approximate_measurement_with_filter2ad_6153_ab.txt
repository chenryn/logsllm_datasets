shows that the actual memory and CPU usage is much less
than the theoretically proposed. However, tuning a resource-
efficient configuration is labor-intensive.
Limitation 4 (L4): Hard to re-define flowkeys. Each ap-
proximate measurement configuration can support only one
flowkey definition at a time in deployment. For example, if
we want to perform heavy hitter detection on both 5-tuple
flows and source-destination address pairs, we need to de-
ploy two configurations, even though they both run the
same algorithm. Some hierarchical heavy hitter detection
approaches [5, 16] can detect multiple levels of flow keys,
with one level being the prefix of another, but the levels
must be pre-defined. In general, for a given configuration,
we cannot choose any combination of packet fields once
the flowkey definition is determined (e.g., switching from
source-destination address pairs to source IP-port tuples).
Limitation 5 (L5): Hard to examine correctness. Ex-
isting approximate measurement approaches provide pre-
defined error guarantees for the overall measurement results.
Although they can tell the expected errors, they cannot quan-
tify the actual extent of errors for individual flows. For exam-
ple, in heavy hitter detection, we may estimate the frequency
of a flow with a relative error ϵ and confidence probability
1− δ (see L1). However, the estimation has a high confidence
probability only if the flow has no hash collision with others,
and the confidence probability drops as there are more hash
collisions. Thus, we cannot measure exactly how likely a
specific flow belongs to a true heavy hitter.
3 SKETCHLEARN OVERVIEW
3.1 Design Features
SketchLearn is a novel sketch-based measurement frame-
work that addresses all design requirements (see §2.1) and the
limitations of existing approaches (see §2.2). Recall from §1
that existing approximate measurement approaches tightly
0255075100MGFPDelRevFRUMPrecision (%)Threshold=0.5%Threshold=0.1%0255075100MGFPDelRevFRUMRecall (%)Threshold=0.5%Threshold=0.1%64322323223283281225613107214842075162532503102105108MGFPDelRevFRUMKBTheory basedTuned67K868K15K429K2.3K5.4K4.7K4.9K1.2K2.8K5.7K14K102105108MGFPDelRevFRUMCPU CyclesTheory basedTunedSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Huang et al.
bind resource configurations and accuracy parameters in
their designs. In sketch-based measurement, it allocates a
sketch in the form of a fixed matrix of counters, followed
by hashing packet or byte counts to each row of counters.
The sketch size (and hence the resource usage) is configured
by the input of accuracy parameters, in which the errors
are caused by hash collisions (i.e., the resource conflicts for
tracking all packets in a fixed number of counters). Existing
sketch-based measurement approaches focus on how to pre-
allocate the minimum required sketch size so as to satisfy
the accuracy requirement. In contrast, SketchLearn takes
a fundamentally new approach, in which it characterizes
and filters the impact of hash collisions through statistical
modeling. It does not need to fine-tune its configuration for
specific measurement tasks or requirements (e.g., expected
errors, query thresholds, or flow definitions). Instead, it is
self-adaptive, via statistical modeling, to various measure-
ment tasks and requirements with a single configuration. It
comprises the following design features, which address the
design requirements R1-R4 and the limitations L1-L5.
Multi-level sketch for per-bit tracking (§3.2 and §5.2).
SketchLearn borrows the idea from Deltoid [17], and main-
tains a multi-level sketch composed of multiple small sketches,
each of which tracks the traffic statistics of a specific bit for
a given flowkey definition. Combined with statistical model-
ing, SketchLearn not only reduces the sketch size and hence
resource usage (R1-R3), but also enables flexible flowkey
definitions (L4 addressed). In the multi-level sketch, each
flowkey is composed of all candidate fields of interest (e.g.,
5-tuples), and we can extract the traffic statistics for any
combination of the packet fields by examining the levels for
the corresponding bits.
SketchLearn differs from Deltoid by extracting flowkeys
via statistical modeling. In contrast, Deltoid is tailored for
heavy hitter/changer detection based on deterministic group
testing, which requires a large sketch size to avoid hash
collisions. Also, it cannot be readily generalized for flexible
flowkey definitions. We show that SketchLearn incurs much
less resource overhead than Deltoid (see §7).
Separation of large and small flows (§4.2 and §5.1). The
multi-level sketch provides a key property that if there is no
large flow, its counter values follow a Gaussian distribution
(see Theorem 1 in §4.2). Based on this property, SketchLearn
extracts large flows from the multi-level sketch and leaves
the residual counters for small flows to form Gaussian dis-
tributions. Such separation enables SketchLearn to resolve
hash collisions for various traffic statistics (R4). For exam-
ple, SketchLearn considers the extracted large flows only for
heavy hitter detection, but includes the residual counters
when estimating cardinality. Note that some measurement
Figure 3: SketchLearn architecture.
works also separate large and small flows, but they are de-
signed for different contexts (e.g., traffic matrix [14, 69] or
top-k counting [32]).
Parameter-free model inference (§4.3). SketchLearn au-
tomatically digs the information hidden in a multi-level
sketch to distinguish between large and small flows without
relying on expected errors, threshold parameters, or configu-
ration tuning (L1, L2, and L3 addressed). It iteratively learns
the statistical distribution inside a multi-level sketch and
leverages the distribution to guide large flow extraction.
Note that parameter-free model inference does not imply
that SketchLearn itself is configuration-free. SketchLearn still
requires administrators to configure the multi-level sketch.
Also, it cannot eliminate the parameters induced by queries
(e.g., heavy hitter threshold). Nevertheless, SketchLearn min-
imizes configuration efforts as its configuration can now be
easily parameterized via self-adaptive modeling.
Attachment of error measures with individual flows
(§5.2). During the model inference, SketchLearn further as-
sociates each flow with an error measure corresponding to a
given type of traffic statistics. Thus, we do not need to spec-
ify any error (L1 addressed); instead, we use the attached
error to quantify the correctness of flows (L5 addressed).
Network-wide inference (§5.3). SketchLearn facilitates
the coordination of results of one or multiple multi-level
sketches deployed at multiple measurement points to form
parameter-free model inference for the entire network.
3.2 Architectural Design
Architecture. SketchLearn is composed of a distributed
data plane and a centralized control plane, similar to ex-
isting software-defined measurement architectures [32, 40–
42, 47–49, 67]. Figure 3 depicts SketchLearn’s architecture.
The data plane comprises multiple measurement points (e.g.,
software/hardware switches or end-hosts) spanning across
the network. It deploys a multi-level sketch at each mea-
surement point, which processes incoming packets, records
packet statistics into the multi-level sketch, and reports the
multi-level sketch to the control plane for analysis.
The control plane analyzes and decomposes each collected
multi-level sketch into three components: (i) the large flow
list, which identifies a set of large flows and includes the
estimated frequency and the corresponding error measure
Large Flow ListData PlaneStatisticalModel InferenceControl PlaneNetwork-wide Query RuntimeQueryTrafficStatisticsFlowkeyFreq.ErrorBit-Level CounterDistributionBit 1DistributionMulti-LevelSketchMulti-LevelSketchMulti-LevelSketchResidualSketchBit 2Distribution.........Bit lDistributionFlowkeyFreq.ErrorFlowkeyFreq.ErrorFlowkeyFreq.ErrorFigure 4: Multi-level sketch updates.
for each identified large flow, (ii) the residual multi-level
sketch, which stores the traffic statistics of the remaining
small flows after filtering the identified large flows, and (iii)
the bit-level counter distributions, each of which models the
distribution of counter values for each bit of a flowkey in the
residual multi-level sketch. The control plane has a query
runtime, which takes either regular or ad-hoc measurement
queries and reports the relevant statistics from one or more
multiple multi-level sketches (depending on the queries) for
specific measurement tasks of interest.
Multi-level sketch. Before showing the model learning, we
describe the multi-level sketch. Let l be the number of bits
of a flowkey. The multi-level sketch comprises l + 1 levels
from level 0 to level l, and each level corresponds to a sketch
formed by a counter matrix with r rows and c columns. The
level-0 sketch records the statistics of all packets, while the
level-k sketch for 1 ≤ k ≤ l records the statistics for the
k-th bit of the flowkey. Let Vi, j[k] be the counter value of
the level-k sketch at the i-th row and the j-th column, where
1 ≤ i ≤ r, 1 ≤ j ≤ c, and 0 ≤ k ≤ l. Also, let h1, h2,· · · , hr
be r pairwise independent hash functions, where hi maps
a flowkey to one of the c columns in the i-th row for each
level, where 1 ≤ i ≤ r. We use the same r hash functions for
all levels. Note that the use of pairwise independent hash
functions is also assumed by most sketch-based approaches
[17, 18, 58].
SketchLearn updates the multi-level sketch for each in-
coming packet. Let (f , v) be the tuple denoting the packet,
where f is the flowkey and v is the frequency. Let f [k] de-
note the k-th bit of f , where 1 ≤ k ≤ l. To update the sketch,
SketchLearn first computes the r hash values to select the
r counters (one per row) from each level. It updates all r
counters in the level-0 sketch, and selectively updates the
counters at the level-k sketch if and only if f [k] equals one.
Figure 4 shows an example how SketchLearn updates a
packet into a multi-level sketch, in which l = 4 and each level
has a sketch with r = 2 rows and c = 5 columns. Suppose that
f = 0101. SketchLearn first updates the two rows (say the
2nd and 4th counters, respectively) in the level-0 sketch (i.e.,
V1,2[0] and V2,4[0]). It then updates V1,2[2], V2,4[2], V1,2[4],
and V2,4[4], since both bits 2 and 4 are one.
4 MODEL LEARNING
4.1 Motivation and Notation
Our primary goal is to build a statistical model that utilizes
only the information embedded inside a multi-level sketch to
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
mitigate errors, without taking any extra information. Our
model components take into account two factors.
Hash collisions. Errors in sketch-based measurement are
mainly caused by hash collisions of multiple flows mapped to
the same counter. To quantify the impact of hash collisions,
we first model the number of flows that each counter holds.
Notation: Counters in the same row and column across
all l + 1 levels share the same set of colliding flows. We
collectively call the l + 1 counters a stack, and let (i, j) denote
the stack in row i and column j. Thus, there are a total of r ×c
stacks with l + 1 counters each. Also, let ni, j be the number
of flows hashed to stack (i, j), and n be the total number of
flows observed (i.e., n =c
j=1 ni, j for any 1 ≤ i ≤ r).
Bit-level flowkey distributions. Flowkeys often exhibit
non-uniform probabilistic distribution in their bit patterns.
For example, IP addresses in a data center tend to share the
same prefix; the protocol field is likely equal to six since TCP
traffic dominates. We characterize the flowkey distribution
at the granularity of bits.
Notation: Let p[k] be the probability that the k-th bit of
a flowkey (call it k-bit for short) is equal to one, where 1 ≤
k ≤ l. Also, to relate p[k] with hash collisions, let pi, j[k] be
the probability that the k-bit in stack (i, j) is equal to one.
Analysis approach. Our analysis addresses the two factors
collectively rather than individually. In particular, we focus
on characterizing Ri, j[k] = Vi, j[k]
Vi, j[0] , which denotes the ratio of
the counter value Vi, j[k] to the overall frequency Vi, j[0] of all
flows hashed to stack (i, j). Also, let sf be the true frequency
We emphasize that the notation defined above is only in-
troduced to facilitate our analysis. As we show later, the
notation can either be canceled or inferred. There is no man-
ual effort to parameterize their values in advance.
of a flow f , so Vi, j[k] =
hi(f )=j f [k] · sf .
c
for 1 ≤ j ≤ c.
4.2 Theory
Assumptions. Our model builds on two assumptions.
• Assumption 1: Each hash function maps flows to columns
uniformly, so ni, j ≈ n
• Assumption 2: Both hi(f ) and p[k] are (nearly) independent
for all 1 ≤ i ≤ r and 1 ≤ k ≤ l, so pi, j[k] = p[k].
Justification. Assumption 1 is straightforward provided
that SketchLearn employs good hash functions. For Assump-
tion 2, the intuition is that if we fix the value of any bit of all
flows, the hash values derived from the remaining bits still
follow a nearly identical distribution. We further validate
Assumption 2 in our technical report [34].
Theorem. With the two assumptions, we can derive the
distribution for Ri, j[k]. Intuitively, in each stack, the num-
ber of flows at level k follows a Bernoulli distribution by
Assumption 2. With sufficient flows (Assumption 1), we can
Flowkey: 0101Level 4Bit 1: 0Bit 2: 1Bit 3: 0Bit 4: 1++Hash Value: 2++++Level 3Level 2Level 1Level 0Hash Value: 4SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Huang et al.
approximate the Bernoulli distribution as a Gaussian distri-
bution. We can map the distribution for number of flows into
the distribution for Ri, j[k] if stack (i, j) has no large flows
based on the following theorem (see the proof in [34]).
Theorem 1. For any stack (i, j) and level k, if stack (i, j) has
no large flows whose frequencies are significantly larger than
2[k])
others, Ri, j[k] follows a Gaussian distribution N(p[k], σ
with the mean p[k] and the variance σ
2[k] = p[k](1−p[k])c/n.
4.3 Model Inference
Theorem 1 requires that no large flows exist in a multi-level
sketch. This motivates us to extract large flows from the
multi-level sketch and draw Gaussian distributions for re-
maining flows at different levels. We build our statistical
model with the following three components:
• Large flow list F: It contains all identified large flows, each
of which is described by its flowkey, estimated frequency,