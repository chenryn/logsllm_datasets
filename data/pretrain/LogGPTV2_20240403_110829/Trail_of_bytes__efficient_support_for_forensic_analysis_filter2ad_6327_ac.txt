into the version-tree with root node hCR3, Si. That is,
store the access time, location L, and “diﬀs” of the
changed blocks for write operations, into the version-
tree for that process. Update the root node to be the
tuple hCR3, S ∪ S ′i.
These version-trees are periodically written to disk and
stored as an audit log where each record in the log is it-
self a version-tree (see Figure 4). Whenever the system call
module notes a casual relationship between entities access-
ing the same monitored objects—e.g., Ei(O, L) by entity p1
and Ej(O′, L′) by p2—we add a pointer in the version tree
of p1 to p2. These pointers help with eﬃcient processing of
the audit log. Having recorded the accesses to objects in L,
we now discuss how the logs can be mined to reconstruct
detailed information to aid in forensic discovery.
4. MINING THE AUDIT LOG
To enable eﬃcient processing of the data during forensic
analysis, we support several built-in operations in our cur-
rent prototype. These operators form our base operations,
but can be combined to further explore the audit log. For the
analyses we show later, the operations below were suﬃcient
to recover detailed information after a system compromise.
• report(w, B): searches all the version trees and re-
turns a list of IDs and corresponding accesses to any
block b ∈ B during time window w.
• report(w, ID): returns all blocks accessed by ID dur-
ing time window w.
• report(w, access, B | ID): returns all operations of
type access on any block b ∈ B, or by ID, during time
window w.
• report(w, causal, B | ID):
returns a sequence of
events that are causally related based on either access
to blocks b ∈ B, or by ID, during time window w.
4.1 Mapping blocks to ﬁles
Obviously, individual blocks by themselves do not provide
much value unless they are grouped together based on a se-
mantic view. The challenge of course is that since we mon-
itor changes at the block layer, ﬁle-system level objects are
not visible to us. Hence, we must recreate the relationships
between blocks in lieu of ﬁle-level information. Fortunately,
all hope is not lost as ﬁle-systems use various mechanisms
to describe data layout on disk. This layout includes how
ﬁles, directories and other system objects are mapped to
blocks on disk. In addition, these structures are kept at set
locations on disk and have a predeﬁned binary format. As
our main deployment scenario is the enterprise model, like
Payne et al. [31] we assume that the ﬁle-system (e.g., ext3,
ntfs, etc.) in use by the guest VM is known.
Armed with that knowledge, the storage module periodi-
cally scans the disk to ﬁnd the inodes and superblocks6 so
that this meta-data can be used during forensic recovery.
That is, for any set of blocks returned by a report() opera-
tor, we use the stored ﬁle-system metadata to map a cluster
of blocks to ﬁles. For ease of use, we also provide a facility
that allows an analyst to provide a list of hashes of ﬁles and
their corresponding ﬁlenames. The report() operators use
that information (if available) to compare the hashes in the
list to those of the recreated ﬁles, and tags them with the
appropriate ﬁlename.
5. EMPIRICAL EVALUATION
While having the ability to record ﬁne-grained data ac-
cesses is a useful feature, any such system would be imprac-
tical if the approach induced high overhead.
In what fol-
lows, we provide an analysis of our accuracy and overhead.
Our experiments were conducted on an Intel Core2 Dual
Core machine running at 2.53GHz with Intel-VT hardware
virtualization support enabled. The total memory installed
was 2GB. Xen 3.4 with HVM support and our modiﬁcations
served as the hypervisor, and the guest virtual machine was
either Windows XP (SP2) or Debian Linux (kernel 2.6.26).
The virtual machine was allocated 512 MB of memory and
had two disks mounted, a 20GB system disk and 80GB data
disk. The 80GB data disk hosted the user’s home directories,
and was mounted as a monitored virtual disk. Therefore, all
blocks in this virtual disk were automatically added to the
watchlist of the storage module. The virtual machine was
allocated 1 virtual CPU, and in all experiments the hyper-
(a)
(b)
Figure 5: Runtime overhead for (a) varying block sizes and access patterns, and (b) across diﬀerent test scenarios.
visor and the virtual machine were pinned to two diﬀerent
physical cores. We do so in order to reﬂect accurate mea-
surements of CPU overheads.
First, the overhead associated with our approach was cal-
culated under a stress test using a Workload Generator and
a workload modeled for Enterprise users. Speciﬁcally, we
subjected our design to a series of tests (using IOMeter)7
to study resource utilization under heavy usage, and used
a scripting framework for Windows (called AutoIt) to auto-
mate concurrent use of a variety of applications. The appli-
cation set we chose was Microsoft Oﬃce, plus several tools
to create, delete, and modify ﬁles created by the Oﬃce ap-
plications. The parameters for the workload generator (e.g.,
the number of concurrent applications, average typing speed,
frequency of micro-operations including spell-check in Word
and cell calculations in Excel, etc.) were set based on em-
pirical studies [35, 17]. The Workload Generator tests were
conducted on an empty NTFS partition on the data disk,
while the Enterprise Workload was tested with pre-seeded
data comprising a set of Microsoft Oﬃce ﬁles along with ad-
ditional binaries. These binaries performed various memory
mapped, network and shared memory operations. The bina-
ries were added to increase the pool of applications loaded
during the tests, and hence add greater dynamism in the
resulting code pages loaded into memory.
Runtime Overhead. Our runtime overhead is shown in Fig-
ure 5(a). The block sizes were chosen to reﬂect normal
I/O request patterns, and for each block size, we performed
random read, random write, sequential read and sequential
write access patterns. The reported result is the average
and variance of 10 runs. Each run was performed under
a fresh boot of the guest VM to eliminate any disk cache
eﬀects. The IOMeter experiments were run on the same
data disk with and without the monitoring code, and the
overhead was calculated as the percent change in CPU uti-
lization. The CPU utilization was monitored on both cores
using performance counters. The reported utilization is the
normalized sum of both cores.
Not surprisingly, writes have a lower overhead due to
the increased time for completion from the underlying disk.
Conversely, sequential access consumes more CPU as the
disk subsystem responds faster in this case, and hence the
I/O ring is quickly emptied by the hypervisor. Even under
this stress test, the overhead is approximately 18%. This
moderate overhead can be attributed to several factors in
our design, including the scheduling of lazy writes of our
data structures, the lightweight nature of our system-call
monitoring, and the eﬃciency of the algorithms we use to
extract the code pages.
Figure 5(b) shows a more detailed breakdown of CPU
overhead as consumed by the diﬀerent modules. Notice that
the majority of the overhead for the stress test (for the 16KB
case) can be attributed to the storage subsystem, as many
of the accesses induced in this workload are for blocks that
are only accessed once. We remind the reader that the ex-
pected use case for our platform is under the Enterprise-
Workload model and the overall overhead in this case is be-
low 10%, with no single module incurring overhead above
3%. Also shown are the averaged overheads induced when
monitoring and logging the activities of several real-world
malware. In all cases, the overload is below 10%, which is
arguably eﬃcient-enough for real-world deployment. We re-
turn to a more detailed discussion of how we reconstructed
the behavioral proﬁles of these malware using our forensic
platform in Section 6.
Another important dimension to consider is the growth
of the log compared to the amount of actual data written
by the guest VM. Recall that the audit log stores an initial
copy of a block at the ﬁrst time of access, and thenceforth
only stores the changes to that block. Furthermore, at every
snapshot, merging is performed and the data is stored on
disk in an optimized binary format.
We examined the log ﬁle growth by monitoring the au-
dit log size at every purge of the version-trees to disk (10
mins in our current implementation).
In the case of the
Enterprise Workload , the experiment lasted for 1 hour,
with a minimum of 4 applications running at any point in
time. During the experiment, control scripts cause the over-
all volume of ﬁles to increase at a rate of at least 10%. The
Malware
% Activity in Log Disk search Exﬁltration Classiﬁcation
Zeus & Variants
Ldpinch
Alureon
Koobface
Bubnix
Sinowal
Conpro
Vundo
Rustock
Slenfbot
35.0
22.5
15.0
10.0
5.0
4.0
3.5
3.0
1.5
0.5
active
active
active
passive
passive
active
active
passive
passive
passive
active
active
active
active
active
active
active
passive
passive
passive
info stealer
info stealer
info stealer
installer
installer
both
installer
installer
installer
installer
Table 1: Malicious applications recovered from the audit log, and their high-level classiﬁcation.
ﬁle sizes of the new ﬁles were chosen from a zipf distribu-
tion, allowing for a mix of small and large ﬁles [25]. We also
included operations such as make to emulate creation and
deletion of ﬁles. The overhead (i.e. additional disk space
used to store logs and metadata compared to the monitored
disk blocks) was on average ≈ 2%. Since the Enterprise-
Workload is meant to reﬂect day-to-day usage patter this
low overhead indicated that this platform is practical and
deployable.
Accuracy of Reconstruction. To examine the accuracy of
our logging infrastructure, we explore our ability to detect
accesses to the monitored data store by “unauthorized” ap-
plications. Again, the Enterprise Workload was used for
these experiments, but with a varying concurrency parame-
ter. Speciﬁcally, each run now included a set of authorized
applications and a varying percentage of other applications
that also performed I/O operations on monitored blocks.
The ratio of unauthorized applications for a given run was
increased in steps of 5%, until all applications running were
unauthorized. The task at hand was to reconstruct all illicit
accesses to the disk. The illicit accesses include copying a
ﬁle into memory, sending a ﬁle over a network connection,
and shared memory or IPC operations on monitored objects.
The audit log was then queried for the time-window span-
ning the entire duration of the experiment to identify both
the unauthorized applications and the illicit access to blocks.
The system achieved a true positive rate of 95% for identiﬁ-
cation of the illicit applications and a 96% true positive rate
in identifying the blocks accessed by these applications.
6. REAL-WORLD CASE STUDY
To further showcase the beneﬁts of our platform, we re-
port on our experience with deploying our framework in an
open-access environment that arguably reﬂects the common
case of corporate laptops being used in public WiFi environ-
ments. Speciﬁcally, we deployed our approach on a laptop
supporting hardware virtualization, on top of which we ran
a Windows XP guest with unfettered access to the network.
The enterprise workload was conﬁgured to run on the guest
system to simulate a corporate user. The monitored area
was set to be the entire virtual disk exposed in the guest
(roughly 4.0 GBs of storage). While there was no host or
network-level intrusion prevention system in place on the
guest system, we also deployed Snort and captured network
traﬃc on a separate machine. This allowed us to later con-
ﬁrm ﬁndings derived from our audit mechanism. The laptop
was left connected to the network for one week, and its out-
bound traﬃc was rate-limited in an attempt to limit the use
of the machine to infect other network citizens.
To automate the forensic recovery process, we make use
of a proof-of-concept tool that mines the audit logs looking
for suspicious activity. Similar to Patagonix [27] we assume
the existence of a trusted external database, D, (e.g., [30])
that contains cryptographic hashes of applications the sys-
tem administrator trusts. The code pages for these autho-
rized applications were created using a userland application
that runs inside a pristine VM and executes an automated
script to launch applications. The userland application com-
municates with the memory monitoring module, and tags
the pages collected for the current application. The pages
are extracted as described in Section 3.1.3, and are stored
along with the application tags. Notice that these mappings
only need be created once by the system administrator.
We then mined the log for each day using report(24hr,
B) to build a set of identiﬁers (p ∈ P ), where B ={blocks
for the temp, system, system32 directories and the mas-
ter boot record}. Next, we extracted all causally related
activity for each p 6∈ D, by issuing report(24hr, causal,
p). The result is the stored blocks that relate to this activ-
ity. These blocks are automatically reassembled by mapping
blocks to ﬁles using the ﬁlesystem metadata saved by the
storage module (as discussed in Section 4.1). At this point
we have a set of unsanctioned applications and what blocks
they touched on disk. For each returned event sequence, we
then classiﬁed it as either (i) an info stealer: that is, a pro-
cess that copied monitored objects onto an external location
(e.g., L=network) or (ii) an installer: a process that installs
blocks belonging to an info stealer.
To do so, our recovery utility ﬁrst iterates through the set
of unsanctioned applications and checks the corresponding
version-trees for events that match an info stealer’s signa-
ture. For each match, we extract all its blocks, and issue
report(24hr, bi, . . . , bn). This yields the list of all unsanc-
tioned applications that touched an info stealer’s blocks.
From this list, we searched for the one that initially wrote the
blocks onto disk by issuing report(24hr, write, bi, . . . , bn).
The result is an installer.
Table 1 shows the result of running our proof-of-concept
forensic tool on the audit logs collected from the laptop.
The table shows the percentage of activity for each malicious
binary and the classiﬁcation as per the tool. For independent
analysis, we uploaded the reconstructed ﬁles to Microsoft’s
Malware Center; indeed all the samples were returned as
positive conﬁrmation as malware. We also subjected the
entire disk to a suite of AV software, and no binaries were
ﬂagged beyond those that we already detected by our tool.
To get a better sense of what a recovered binary did, we
classify its behavior as active if it had activity in the audit
logs every day after it was ﬁrst installed; or passive other-
wise. The label “Exﬁltration” means that data was shipped
oﬀ the disk. “Disk search” means that the malware scanned
for ﬁles on the monitored store. As the table shows, ap-
proximately 70% of the recorded activity can be attributed
to the info stealers. Upon closer examination of the blocks
that were accessed by these binaries, we were able to classify
the ﬁles as Internet Explorer password caches and Microsoft
Protected Storage ﬁles. An interesting case worth pointing
out here is Zeus. The causal event linkage by the forensic
tool allowed us to track the initialization of Zeus as Zbot
by Sinowal. Even though Sinowal constitutes only 4% of
activity in the logs, it was responsible for downloading 60%
of the malware on the system. Zeus appears to be a variant
that used Amazon’s EC-2 machines as control centers8.
Interestingly, the average growth of our audit log was only
15 MB per day compared to over 200 MB per day from the
combined Snort and network data recorded during the ex-
periment. Yet, as we show later, the data we are able to
capture is detailed enough to allow one to perform interest-
ing behavioral analyses. The analysis in Table 1 took less
than 4 hours in total to generate the report, and our proof-
of-concept prototype can be signiﬁcantly optimized.
6.1 Example Reconstruction
With the framework at our disposal, we decided to explore
its ﬂexibility in helping with behavioral analysis. Speciﬁ-
cally, we were interested in analyzing Mebroot, which is a