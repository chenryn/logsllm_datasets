that were removed. Within each index, there are two values:
the percentage of samples that were incorrectly transcribed
and the corresponding PESQ score (underlined). For example,
row 0 and col 1 corresponds to frame size 20 and the
removed percentage 25. At these values, only 33% of content
was mistranscribed while the PESQ score was 2.3. One can
observe that dropping the frame does not result in 100%
mistranscription even though the PESQ score grow close to
1 (bad quality).
audio quality of telephony systems. PESQ measures features
such as jitter, packet loss, noise and returns a quality score
between 1 (bad quality) and 5 (high quality). By using PESQ,
supplants the need for user studies to measure audio quality.
In our case, the PESQ score can reveal how white-noise will
impact audio quality. We calculate the PESQ scores for each
of the white-noise infused audio samples and calculated the
average.
3) Results: Of the total audio samples we attacked, only
35% of the samples were successfully evaded. Furthermore,
average PESQ score for the samples was 1.06, which implies
very low audio quality. This proves that any trivial attack will
have very low attack success against models, and will have
a strong negative impact on human audio interpretability. In
contrast, the attack proposed in this paper has little to no
impact on the human interpretability (as shown using our
Amazon Turk experiments), and achieve 100% success rate
against any speech-based models.
C. Trivial Frame Deletion Attack
1) Motivation: Similarly, readers might assume that delet-
ing random frames from an audio sample will trick the ASR
and AVI into an incorrect transcription. We demonstrate in the
following subsection that this simple attack fails to fool the
model while maintaining human interpretability.
2) Methodology and Setup: This trivial attack involves
deleting random frames from the audio sample. The attacker
can tune both the percent of frames deleted, and their size.
We ran the attack over several different values of these two
variables. We removed 20%, 25%, 30% and 35% of frames,
against frame sizes of 20 ms, 25 ms, 30 ms, and 35 ms
(Table VI). We ran the attack across 100 audio ﬁles that
contained a single word. The perturbed ﬁles were then passed
to the Google Speech API and we recorded the percent
of audio samples that were mistranscribed. Similar to the
previous trivial white noise experiment, we used the PESQ
score to measure human interpretability.
3) Results: Table VI shows the results of the experiments.
As expected, increasing the size and percentage of frames
to delete increases the mistranscription percentage, but also
reduces the PESQ score (underlined). However, this trivial
attack achieves the maximum mistranscription rate of 71%
at the PESQ score of 1.7. This is a very low PESQ score
and means that the audio quality is poor. These results are
similar to that of the trivial white-noise attack. Contrary to
this, our attack can achieve 100% mistranscription rate while
maintaining audio quality.
D. Impulse Perturbation Attack
1) Methodology: ASR systems are trained to learn patterns
using features from the training set. It is important that training
and test sets belong to the same distribution. Otherwise, the
model will have difﬁculty identifying patterns in the test
set. Intuitively, we may construct a simple perturbation by
sampling outside of the ASR system’s training distribution,
then applying it to an input to trick the ASR system.
We extend our study of ASR system’s sensitivity with
this extremely simple attack. This involves increasing the
amplitude of time-samples within a single phoneme to the
maximum amplitude observed in the entire time series. This
perturbation will create a minor spike in the audio sample,
known as an impulse. If the impulse perturbation succeeds at
confusing the model, it will highlight the high sensitivity of
the model to artiﬁcial perturbations. This will motivate further
investigation of other possible attack vectors. These can be
designed to confuse the model even further, with limited to no
impact on human understandability of the attack audio sample.
The impulse perturbation described above might be able
to confuse the model. However, there are a few drawbacks
to this approach. First, most popular ASR systems are often
trained on both clean and degraded audio quality. This is done
to ensure that the ASR systems perform well in noisy envi-
ronments. Secondly, ASR system’s architecture is designed to
ensure that even with a limited training set, the model is able
to generalize well. A better generalized model should not be
confused by such a simple perturbation. Any simple attack
method will only have limited success against ASR systems.
Therefore, a further investigation of other attack methods is
necessary.
2) Setup: We test our simple attack on the TIMIT corpus.
The TIMIT corpus contains the timestamps of each phoneme
in the audio ﬁle. First, we iteratively selected each phoneme
in a word to perturb. Then, each time sample of the target
phoneme is replaced with the largest amplitude value in the
audio ﬁle. Each of the attack audio samples is passed Over-
Line to the ASR system for transcription. We then repeat this
process, but only perturb one percent of the phoneme. This
allows us to identify the relationship between the number of
perturbed samples in the phoneme and the mistranscription
rate. For brevity, we tested the simple perturbation method
against a single model.
3) Results: The simple baseline attack above is exe-
cuted against the Google (Normal) model. We observe that
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
727
Original Model
Discard threshold,
% of max. DFT coeff.
2% 4% 8% 16% 32%
85
78.6
70.2
57.2
47.4
37.8
Benign Test Set
Accuracy (%)
Fig. 9: Top: The relationship between the adversarial training
and the accuracy of the corresponding model on the benign
(unmodiﬁed) test set. Bottom: Transcription performance of
our (small dictionary) keyword model when trained on audio
modiﬁed to discard frequencies below various thresholds. The
trained models are tested on attack audio with a range of
MSE (relative to the corresponding original audio), and the
percentage of successful transcriptions are plotted.
phonemes with one percent of their samples perturbed only
had a 10% attack success rate against the model. This number
increases to 43% when the entire phoneme is perturbed by
the impulse value. Although the attack has limited success in
this scenario, the impulses would likely fail to have an effect
against an adversarially-trained model.
4) Simple Defense: During our initial simple perturbation
experiments, we observed that applying impulses to individual
phonemes was easily distinguishable during manual listening
tests. Not only was the attack success rate low, but such
impulses were reminiscent of call audio distortions and jitters
that are commonly heard over telephony networks. Using this
na¨ıve perturbation scheme is not ideal since a machine learn-
ing model will likely perform equally well in distinguishing
training schemes
impulses as humans. Overall, adversarial
to defend against
to
implement, and do not give an adversary sufﬁcient probability
of success under our threat model.
this style of attack would be trivial
E. Adversarial Training as a Defense:
One technique that has shown promise in defending com-
puter vision models is adversarial training [13]. However, this
approach has not seen much success in defending speech and
voice identiﬁcation models [10]. To test this technique against
our attack, we trained six keyword recognition models [82].
For each model, we generated adversarial data using the
method described Section IV-C1. The threshold was deter-
mined as a percentage of the maximum spectral magnitude
(i.e., maxk |fk|); in particular, we considered 2%, 4%, 8%,
16%, and 32% for each of our models, shown in Figure 9. For
example, if the threshold is 4%, we only retain the fk whose
magnitude is greater than 4% of the maximum magnitude.
Each of the models was trained to detect 10 keywords.
Next, we evaluated each model by randomly selecting 20
samples per keyword. Figure 9 displays the results of our
experimentation. Figure 9 (Top) shows the accuracy of each
model on the benign data set, while Figure 9 (Bottom) shows
the transcription success at various levels of acoustic distortion
(relative to the original audio) introduced by our attack. The
red dotted line represents the limit of human comprehension
as deﬁned earlier in Section V-A1.
There are a few important trends to note. First, models
trained with higher threshold values have lower accuracy on
normal audio samples, shown in Figure 9 (Top). This result
is expected, as lower accuracy is an artifact of adversarial
training [83], [84], [56]. Second, as MSE increases, the tran-
scription success rate decreases, shown in Figure 9 (Bottom).
The more samples that have lower MSE behind the red dotted
GSM line, the more sensitive the model is to our attack. Lastly,
adversarial training does decrease model sensitivity to our
attack, relative to the baseline model. Intuitively, this implies
a relationship between the amount of adversarial training and
the minimum amount of distortion caused by our attack.
We caution against taking Figure 9 as strong evidence that
adversarial training is a defense against our attack. First, model
sensitivity is measured in the number of samples on the left
the red GSM line in Figure 9. We used the GSM line as
the dividing point between what is and is not comprehensible
by human listeners, as discussed in Section V-A. Hence we
consider attack audio with MSE values to the right of this line
to be failed attack samples. Yet the GSM line should be viewed
as a conservative minimum for human comprehension. This is
important because, for a given model, our attack may produce
many audio samples whose MSE is to the right of the GSM
line. Yet, this does not imply that the model is necessarily
“robust” against our attack. In particular, some high MSE
attack samples may still be understandable by humans while
inducing errors in the model. Second, our experimental setup
was designed only to support a preliminary investigation of
adversarial training as a defense. It would be incorrect to
extrapolate any trends from such a simple experiment. A
broader and more comprehensive examination should consider
(in detail) the effects of different model’s hyper-parameters,
and employ a much larger number of audio samples. We leave
such a study for future work.
F. Comparison with the Image Domain:
Our attack is possible because ASRs/AVIs weigh features
differently from the human ear. This discrepancy between
human perception of features versus that of the model has
been observed in the image space, speciﬁcally the work by
Ilyas et al. [85]. Image models excel at learning patterns which
maximize accuracy, and allows models to extract features that
generalize well. However, these features often do not align
with human perception. Adversarial training, as proposed by
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
728
0.00000.00250.00500.00750.01000.01250.01500.01750.0200Mean Squared Error Between Original and Attacked Audio020406080100Successful Transcriptions (%)Efficacy of adversarial training as a defenseOriginal Model2% Discard Threshold4% Discard Threshold8% Discard Threshold16% Discard Threshold32% Discard ThresholdFig. 10: Attacks against the Azure AVI. The ﬁgure shows the
minimum threshold required to force a speaker misclassiﬁca-
tion when perturbing only a single phoneme. We can observe
that in general, SSA attacks require much higher thresholds to
successfully fool the model, in comparison to the DFT attack.
Ilyas et al., can encourage the model to learn features which
align closer to human perception. Contrary to their results, we
showed that adversarial training of the ASR system ultimately
reduced its utility. This can be explained by the different
properties of ASR systems. First, the ASR system pipeline
is signiﬁcantly different from the image recognition pipeline.
This is due to the presence of the pre-processing layer, feature
extraction layer and a Decoding layer in the ASR system.
Second, we used the training parameters that were provided by
the Tensorﬂow team. These parameters might not be suitable
for training data that has been perturbed using our attack.
Third, audio features do not follow the same pattern as image
features. In images, low-level features are generally correlated
with each other. For example, the pixels for an eye exist
within a certain proximity of other eye pixels. In contrast,
ASR features consist of spectral information, which forms
harmonics that may not be correlated to each other. A complete
explanation of this phenomena is for future work.
G. Fingerprinting the attack
It is worth discussing the avenues a defender can take to
detect attempts to detect our attack such as ﬁnger printing.
This would involve ﬁnding unique spectral patterns in audio
generated by our attack. However, this becomes difﬁcult over
when attempting to detect our attack audio over the telephony
network. This medium is lossy due to packet loss, jitter, and
other factors. In addition, the induced loss is not consistent, as
it is subject to the call’s path through the telephone network,
the transmission hardware, and codecs. As a result, spectral
patterns could be lost in the network. Although ﬁngerprinting
methods might prove harder in this domain, we cannot rule
them out completely. This question is beyond the scope of this
paper, thus we leave it for future work.
(a) DFT Attack Results.
(b) SSA Attack Results
Fig. 11: Results for phoneme-level attack. Lower threshold
corresponds to lower distortion required for an attack success.
The DFT attack is more effective against vowels (brown) than
other phonemes. In contrast, the SSA attack does not display
any such consistent behavior.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
729
0.00.51.0ThresholdthdhkzvbdxsyngaxehoyuxuhuwihermaeahgdtjhnprlhvhhwiyixshaxreyayowawaaPhonemesAzure DFT Attack0.00.51.0ThresholdenawhvelereyaxraaayoyaojhixraeshgndhowlpihyuxdxehngkmwiytsbdzthvahuhuwaxAzure SSA AttackStopsAffricatesFricativesNasalsSemivowels_and_GlidesVowels0.00.51.0Thresholdengax-hoyjhoweruxuhelchaaaoshemawaeayeyenhhuwfihaxiyaxrahvlrksixwtehngndhygzmphvddxbthnxPhonemesGoogle (Normal)StopsAffricatesFricativesNasalsSemivowels_and_GlidesVowels0.00.51.0Thresholdengzhawowshuxeyaeuwaoayerihaaax-hhhenahemiychaxrehyoyaxsngnuhwixrzmpthkdhtfgleldxbhvvjhnxdWit0.00.51.0Thresholdengyuhaoaxrerruwawaheliywaauxowihixaenglehneyayemhvmaxnxhhkdoyenchsvdxpshztbdhjhgthfzhax-hPhonemesGoogle (Normal)0.00.51.0Thresholdzhuhoyyaewknlahdxraaowuxuwayixihemvmiygerelfeyhvjhsshpendhawnxaxraoaxzehtdbchhhngthax-hengWit