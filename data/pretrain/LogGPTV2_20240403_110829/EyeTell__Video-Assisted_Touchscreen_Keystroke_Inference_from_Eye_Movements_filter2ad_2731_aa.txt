title:EyeTell: Video-Assisted Touchscreen Keystroke Inference from Eye Movements
author:Yimin Chen and
Tao Li and
Rui Zhang and
Yanchao Zhang and
Terri Hedgpeth
2018 IEEE Symposium on Security and Privacy
EyeTell: Video-Assisted Touchscreen Keystroke
Inference from Eye Movements
Yimin Chen∗, Tao Li∗, Rui Zhang†, Yanchao Zhang∗, and Terri Hedgpeth∗
∗School of Electrical, Computer and Energy Engineering (ECEE), Arizona State University
†Department of Computer and Information Sciences, University of Delaware
{ymchen, tli}@asu.edu, PI:EMAIL, {yczhang, terrih}@asu.edu
Abstract—Keystroke inference attacks pose an increasing
threat to ubiquitous mobile devices. This paper presents EyeTell, a
novel video-assisted attack that can infer a victim’s keystrokes on
his touchscreen device from a video capturing his eye movements.
EyeTell explores the observation that human eyes naturally focus
on and follow the keys they type, so a typing sequence on a
soft keyboard results in a unique gaze trace of continuous eye
movements. In contrast to prior work, EyeTell requires neither
the attacker to visually observe the victim’s inputting process nor
the victim device to be placed on a static holder. Comprehensive
experiments on iOS and Android devices conﬁrm the high efﬁcacy
of EyeTell for inferring PINs, lock patterns, and English words
under various environmental conditions.
I.
INTRODUCTION
Keystroke inference attacks pose an increasing threat to
mobile devices which have penetrated into everyday life. In a
typical attack scenario, a victim types on the soft keyboard of
his1 smartphone or tablet in an insecure public environment
such as a public library, a coffee shop, or a train. The
attacker tries to infer the victim’s keystrokes in order to obtain
sensitive information such as the victim’s device passwords,
web account passwords, or even emails. Based on the inferred
keystrokes, the attacker can proceed to launch further attacks.
One example is that the attacker can use the inferred password
to pass the authentication system of the victim’s device.
The severe security and privacy implications make keystroke
inference a very active research topic in mobile device security.
Many keystroke inference attacks rely on analyzing a
video recording the victim’s typing process. They require that
either the recorded video capture the victim’s typing process
with little or no visual obstruction [1]–[9] or the device be
placed on a static holder [9]. Given the video recording, the
attacker infers keystrokes by analyzing touchscreen reﬂection
[6], spatial hand dynamics [7], relative ﬁnger movements on
the touchscreen [8], or the backside motion of the device [9].
While these attacks have been demonstrated quite effective,
their strong assumptions may not always hold in practice.
In this paper, we report
the design and evaluation of
EyeTell, a novel video-assisted keystroke inference attack that
can infer a victim’s keystrokes on his touchscreen device from
a video capturing his eye movements. EyeTell is inspired by
the observation that human eyes naturally focus on and follow
the keys they type such that a typing sequence on a soft
keyboard results in a unique gaze trace of continuous eye
movements. Under EyeTell, the attacker records a video of
1No gender implication.
the victim’s eye movements during his typing process and then
extracts a gaze trace. By analyzing the gaze trace, the attacker
can infer the victim’s input with high accuracy.
Although conceptually intuitive, EyeTell faces three main
design challenges. First, it needs to extract a gaze trace from
the recorded video without any prior information about the
victim (e.g., what his eyes look like). Second, the gaze trace
is usually very noisy, making it very difﬁcult to recover the
correct typing sequence. Third, the gaze trace does not tell the
exact number of keystrokes on the soft keyboard. To tackle
the ﬁrst challenge, we explore a user-independent model-based
gaze tracking method [10]. To deal with noisy gaze traces and
accommodate unknown keystroke counts, we develop a novel
decoding algorithm to rank all possible typing sequences and
ﬁnally output the ones with high rank.
Our contributions in this paper are summarized as follows.
• We propose EyeTell, a novel video-based attack that
can infer a victim’s keystrokes on a touchscreen device
from a video capturing his eye movements. In compar-
ison with prior work [1]–[9], EyeTell requires neither
the attacker to visually observe the victim’s typing
process nor the victim device to be placed on a static
holder. Therefore, EyeTell is more practical, sneaky,
and launchable from a large distance, thus posing a
more serious threat to user privacy.
• We prototype and evaluate EyeTell through exper-
iments on both iOS and Android devices, which
involve the PIN, pattern-lock, and alphabetical soft
keyboards. We show that EyeTell can identify the top-
5, top-10, and top-50 likely PINs that must contain a
target 4-digit PIN with probabilities up to 65%, 74%,
and 90%, respectively. Similarly, EyeTell can output
the top-5, top-10, and top-50 possible lock patterns
that must include a target Android lock pattern with
probabilities up to 70.3%, 75.3%, and 85.1%, respec-
tively. In addition, EyeTell can identify the top-5, top-
10, top-25, and top-50 likely words that must include a
target word with probabilities up to 38.43%, 63.19%,
71.3%, and 72.45%, respectively.
• We point out future directions to improve EyeTell
and also possible countermeasures. Although currently
EyeTell works only under a short recording distance
and a small recording angle, we believe that
the
adoption of better optics and eye tracking techniques
can readily relieve such limitations.
© 2018, Yimin Chen. Under license to IEEE.
DOI 10.1109/SP.2018.00010
144
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
II. RELATED WORK
In this section, we discuss the prior work most related to
EyeTell in two research directions: keystroke inference attacks
and eye-tracking-related security implications.
A. Keystroke Inference Attacks
Prior keystroke inference attacks can be broadly classiﬁed
into video-based, sensor-based, and WiFi-based attacks.
1) Video-based attacks: In this category, the attacker uses a
recorded video to infer keystrokes. Early work targets physical
keyboards. For instance, Backes et al. [1], [2] recovered the
content on a computer screen from its reﬂections on nearby
objects such as glasses and tea pots. As another example,
Balzarotti et al. [3] inferred the keystrokes by characterizing
the light diffusion around the keyboard in the video recording.
This work [3] requires the attacker to directly video-record the
victim’s ﬁnger typings on the physical keyboard.
More recent research along this line targets soft keyboards
on ubiquitous touchscreen mobile devices. In [4], Maggi et
al. tried to recover keystrokes from key magniﬁcations on the
touchscreen. In [5], Raguram et al. inferred keystrokes from
the touchscreen’s reﬂection on the victim’s sunglasses. In [6],
Xu et al. extended the attack in [5] to recover keystrokes from
double reﬂections of the touchscreen. In [7], Yue et al. inferred
keystrokes by exploiting the homographic relationship between
captured images and a reference image of a soft keyboard.
Similar homographic relationship was also used in [8] by
matching ﬁnger movements. In [9], Sun et al. showed that
the keystrokes can be actually inferred from the motion of
a tablet’s backside. All these attacks require the attacker to
record a video capturing at least part of the victim’s typing
process or device backside, so they do not work if no such
video is available. For example, the surrounding environment
may prevent the attacker from having an unobstructed, stealthy
view of the victim’s typing process.
In contrast, EyeTell requires no unobstructed view of the
victim’s device or typing process and only needs the attacker to
record the victim’s eye movements during the typing process.
When a user types, he usually holds his device in one hand
or places it on a table or his knee. This means that his eyes
are normally at much higher positions than his device during
the typing process. So it is much easier and more sneaky to
video-record the user’s eye movements from a distance than ti
video-record his device motion or typing process. EyeTell is
thus applicable to much wider contexts.
2) Sensor-based attacks: In this category, the attacker uses
on-board sensor data to infer a victim’s keystrokes. In [11],
[12], it was shown that the accelerometer data of a mobile
device can be used to infer the victim’s password. Subse-
quently, keystrokes were inferred in [13], [14] by combining
both accelerometer and gyroscope data. In [15], [16],
the
authors exploited microphones and front cameras for keystroke
inference. In comparison with video-based attacks (including
EyeTell), these attacks require the attacker to acquire sensor
data from the victim device through either malware infection
or unprotected data transmissions. Such assumptions may not
always hold in reality.
There is also work on using device sensors as the side
channels to infer keystrokes of nearby physical keyboards. In
[17]–[19], keystrokes on a physical keyboard were recovered
through analyzing the acoustic emanations of the keyboard
recorded by a nearby malicious microphone. In [20], [21],
keystrokes were inferred by analyzing the time difference of
arrival of acoustic signal recordings. In [22], Marquardt et
al. used the accelerometer on a smartphone to measure the
vibration induced by a nearby physical keyboard for keystroke
inference. In [23], Liu et al. inferred keystrokes by exploiting
the accelerometer data of a smartwatch worn by the victim
while he typed. Similar to sensor-based attacks [11]–[16],
these schemes [17]–[23] assume that the attacker can obtain
sensor data from the victim device or that sensor data can
be collected by other devices close to the physical keyboard.
By comparison, EyeTell has no such restriction and can be
launched from a larger distance.
3) WiFi-based attacks: In this category, the attacker infers
a victim’s keystrokes from recorded channel state information
(CSI). The idea is that different keystrokes lead to distinct
changes in wireless channels and the corresponding CSI. It
has been shown that CSI information can be exploited to
infer a victim’s keystrokes on a physical keyboard [24], or
a soft keyboard [25], or a pattern lock keyboard [26]. All
these attacks are user-dependent and require the attacker to ﬁrst
obtain the victim’s data with known labels to train a classiﬁer.
In addition, they cannot tolerate any change in the surrounding
environment other than the victim’s hand or ﬁnger movement.
Furthermore, the distance between the WiFi transmitter and
receiver, the orientation of the victim device, and the victim’s
typing gestures were all ﬁxed in the experiments. These
shortcomings limit the applicability of WiFi-based keystroke
inference attacks in practice.
B. Eye-Tracking-Related Security Implications
Considering eye tracking as an input method for user-
device interaction, researchers have proposed to use it for user
authentication and inferring user input.
1) User authentication: In the early days, researchers tried
to use eye movement as a biometric identiﬁer for user authenti-
cation. In [27], the authors put forward this idea and evaluated
the identiﬁcation rate among users. In [28], [29], the authors
proposed novel features extracted from eye movements and
designed speciﬁc stimulus to enhance the performance.
More recent research in this line mainly focuses on design-
ing novel challenge-and-response schemes for user authentica-
tion in a contactless manner. The key motivation is that eye
tracking as an input method is more secure against shoulder-
surﬁng attacks, besides novel two-factor authentication [30]
and anti device-theft [31] schemes. For example, the authenti-
cation systems in [32]–[35] ask a user to follow moving objects
on the screen, draw pre-selected shapes, perform eye gestures
to input PIN passwords, etc.
2) Inferring user input: There are few efforts to work on
inferring user inputs on device touchscreen by exploiting eye
tracking as a side channel. In [36], the authors pointed out
that the victim’s eyes would follow his ﬁnger movements on
the touchscreen of mobile device, which may leak his inputs.
To show such feasibility, they mannually analyzed the images
145
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1. Anterior segment of a human eye [37].
taken by the front camera of the victim’s device to infer
his input digits. Through a small scale of experiments (three
participants and nine trials in total), they obtained an accuracy
result of around 67% on PIN keyboard.
Compared with the above work, EyeTell exhibits two main
differences. First of all, EyeTell works on a more challenging
scenario of inferring user keystrokes on mobile device touch-
screen, while most user authentication schemes based on eye
tracking aim at much larger screens such as TV. Furthermore,
these schemes were engineered in a way that their eye tracking
module can obtain a user’s eye trace easily and effectively.
On the contrary, EyeTell can only obtain a much noisier
eye trace due to two reasons: the attacker does the video
recording from a distance, and the victim’s eye movements on
a mobile touchscreen is much more subtle. Secondly, EyeTell
involves a set of tools to infer user inputs and comprehensive
investigations on different types of soft keyboards to better
evaluate its security and privacy impacts.
III. BACKGROUND ON VIDEO-BASED GAZE TRACKING
EyeTell is based on the intuition that a victim’s gaze trace
can reveal his typing sequence on a soft keyboard. Fig. 1
depicts the anterior segment of a human eye. According to the
deﬁnition in [38], the gaze actually refers to the gaze direction.
We now brieﬂy introduce the background of video-based gaze
tracking, which is used in EyeTell to extract gaze traces.
Gaze tracking refers to the techniques that determine
the gaze direction of the eyes. Gaze tracking has numerous
applications such as human attention analysis and gaze-based
human-computer interfaces. So far video-based gaze tracking
is most popular because it achieves high accuracy without
requiring the target to wear any special device.
There are mainly two types of video-based gaze track-
ing methods: feature-based and appearance-based [38], [39].
Feature-based methods use local features such as contours, eye
corners, and reﬂections from the eye image for gaze estimation.
In contrast, appearance-based methods directly use the content
of the eye image as input to estimate the gaze direction instead
of extracting any local feature.
Feature-based methods can be further divided into
interpolation-based and model-based methods according to
how the features are used. Interpolation-based methods com-
monly assume that the mapping between the image features
and gaze can be modeled as a parametric form such as a
polynomial or nonparametric one like a neural network. In
contrast, model-based methods directly calculate the gaze from
the image features based on suitable geometric models of the
human eye. In this paper, we adopt the model-based gaze
tracking method in [10] due to its advantage that the attacker
does not need to obtain any training data about the victim
prior to the attack. Other model-based methods can be used in
EyeTell as well if they require no training data.
(b) Pattern lock
Fig. 2. Three representative soft keyboards.
(a) PIN
(c) Alphabetical
IV. ADVERSARY MODEL
We consider a victim using a mobile touchscreen device
such as a smartphone or tablet. Assume that
the victim
holds the device right in front of himself and types on the
touchscreen soft keyboard. Such scenarios are very common
in practice. For example, the victim may use his mobile device
at his workplace or wait in line at a coffee shop. We assume
that the victim is alert to conventional shoulder-surﬁng attacks
in the sense that the attacker cannot get too close to the victim
when he types on the device.
We consider an attacker who aims to infer the typed
sequence on the victim device, which could be PINs, lock
patterns, words, or sentences. We assume that the attacker
can use a COTS smartphone, digital camera, or camcorder
to record the victim’s eyes during his typing process, possibly
from a long distance. However, the attacker cannot obtain any
IMU sensor (accelerometer, gyroscope, microphone, etc.) data
by installing malware such as Trojans or malicious web scripts
on the victim device. Different from prior work, we assume
that the attacker can see neither the touchscreen or backside
of the victim device nor the victim’s hand movements during
his typing process. Under these assumptions, existing video-
based [1]–[9], [40] and sensor-based [11]–[16], [18]–[20], [22]
keystroke inference attacks no longer work.
V. EYETELL DESIGN
In this section, we give an overview of EyeTell and then
detail its design. For convenience only, we assume the victim
device to be a smartphone throughout the illustration, though
EyeTell can work with any mobile touchscreen device.
	











Fig. 3. Workﬂow of EyeTell.
A. Overview
EyeTell is designed to infer the sensitive inputs on the soft
keyboard from the video of the victim’s eye movements while
he types. The high-level design of EyeTell is shown in Fig. 3,
which consists of the following four steps.
(1) Video Recording. We ﬁrst record a video capturing the
victim’s eyes during his inputting process using a COTS
camcorder. As mentioned in Section IV, we assume that
146
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
(a) Side view
(b) Attacker’s view
Fig. 4. Typical setup for video recording.
neither the touchscreen nor the victim’s hand movement can be
directly seen from the video. In addition, we do not assume
that the smartphone is ﬁxed on a device holder or that the
video can capture its backside.
(2) Gaze Trace Extraction. We adapt user-independent gaze
tracking [10] to extract the gaze direction from each frame of
the recorded video and then combine the directions to obtain
a complete gaze trace. In particular, we detect the two eyes in
each video frame and then the limbus for each eye, from which
we ﬁnally estimate the corresponding gaze direction. Due to
the noisy and unstable nature of the extracted gaze trace, we
further apply outlier detection and low-pass ﬁltering to obtain
a cleaner gaze trace.
(3) Trace Decoding. In this step, we design a novel decoding
algorithm to match the gaze trace extracted in Step 2 into
a set of candidate typing sequences on the soft keyboard.
Fig. 2 shows the soft keyboards we investigate in this paper,
including the pattern-lock keyboard on Android and the PIN
and alphabetical keyboards on iOS. For PIN or pattern-lock in-
ference, each candidate typing sequence corresponds to one or
several PINs or lock patterns. For word or sentence inference,
an additional step is taken to select meaningful results with
the assistance of a dictionary. The decoding algorithm must
adapt to different inference scenarios where the attacker’s prior
information may vary a lot. For example, the attacker knows
that a PIN must consist of four or six digits, but he knows
very little to none about which word the victim is likely to
input before doing word inference.
(4) Word/Sentence Inference. Finally, we select the possible
words by considering meaningful alphabetical combinations
using a dictionary. We also explore the linguistic relationship
between adjacent English words to further infer sentences.
We detail each step above in what follows.
B. Video Recording
In this step, we want to obtain a video of the victim’s eyes
when he types on the soft keyboard of the smartphone. Fig. 4
shows a typical setting of video recording in our experiments.
We ask the participants to sit on the chair and input on a
smartphone. A Panasonic HCV7000 camcorder is used to
record videos. Using a COTS camcorder can show that EyeTell
is low-cost, convenient, and stealthy to launch. In our studies,
we ﬁnd that the following factors affect the result of our gaze
tracking algorithm.
Image resolution. The resolution of the recorded video affects
eye and limbus detection and therefore the extracted gaze. In
the experiments, we always stick to the highest resolution of
the camcorder, i.e., each video frame is of 1920× 1080 pixels.
Video frame rate. Due to the noisy and instable nature of the
extracted gaze trace, we need to collect more sudden changes
of the user’s eye movement and thus desire a higher video
frame rate. In the experiments, we choose the frame rate as
60 fps, which is the highest frame rate supported by our
camcorder. Our attack can be more effective if a camcorder
supporting higher frame rates is available.
Light condition. The light condition in the environment may
also affect the inference result, as the imaging sensor of the
camcorder generates larger noise in low-illumination environ-
ments and thus produces a polluted gaze trace.
Recording angle. We deﬁne the recording angle as the angle
between the plane formed by the victim and his smartphone
and the plane formed by the victim and the attacker’s cam-
corder. Our current EyeTell implementation requires that the
camcorder be placed in the same plane as the victim and
his smartphone, typically as shown in Fig. 4. Therefore, our
default recording angle is zero degree. We believe that this
assumption is fairly easy to achieve in practice with advanced
camcorders and can be relieved if more sophisticated gaze
tracking algorithms are available.
After video-recording the victim’s eye movement, we man-
ually crop the beginning and ending part of the video such
that the remaining part contains only the typing process. For
example, the video only contains the process of the victim
inputting four digits or drawing a pattern on the smartphone.
C. Gaze Trace Extraction
There are three steps in gaze trace extraction: eye localiza-
tion, limbus detection, and gaze trace estimation.
1) Eye detection: EyeTell detects the victim’s eyes in each
frame through a two-step approach. We ﬁrst search for a pair of
eyes within the entire frame in a coarse-grained manner. Once
a rough region is obtained, we further reﬁne the detected eye
region and then calculate the accurate eye positions.
In the ﬁrst step, we use a Haar-like feature-based cascade
classiﬁer [41] to detect possible eye regions and always select
the ﬁrst output as the candidate eye region. We then segment
the candidate eye region into two area-of-interests (AOIs), one
for each eye. The cascade classiﬁer [41] is very efﬁcient and
also user-independent, but it may still incur false positives that
the candidate region is not the eye region. For example, a rect-
angular area enclosing the user’s clothes may be misclassiﬁed
as the eye region.
We use two tricks to reduce such false positives. First,
we require that the size of the detected eye region be above
a minimum threshold. In our implementation, we set
the
threshold to be 80× 40 pixels, which has been shown valid for
our video recording setting. Second, we calculate a similarity
score between the detected eye region and a reference region,
which is the eye region successfully detected in a different
frame of the same video. In particular, we resize the candidate
eye region to the same size as the reference region and then
normalize the pixel values of both regions. After normalization,
we calculate a pixel-level similarity score for the same pixel
in the two regions, which is the ratio between the absolute
difference of the two pixel values and their sum. After that,
the similarity score of the two eye regions is calculated as the
average pixel-level similarity score across the entire eye region.
The smaller the similarity score, the more similar the two eye
regions. In our implementation, we use an empirical threshold
147
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
of 0.8 to ﬁlter out possible false positives of eye regions and