|  | applied  |  |
|---|---|---|
|  |sciences | |
Article 
Efficient Online Log Parsing with Log Punctuations Signature
Shijie Zhang and Gang Wu *
School of Software, Shanghai Jiao Tong University, Shanghai 200240, China; PI:EMAIL * Correspondence: dr.wugang@sjtu.edu.cnAbstract: Logs, recording the system runtime information, are frequently used to ensure software system reliability. As the first and foremost step of typical log analysis, many data-driven methods have been proposed for automated log parsing. Most existing log parsers work offline, requiring a time-consuming training progress and retraining as the system upgrades. Meanwhile, the state of the art online log parsers are tree-based, which still have defects in robustness and efficiency. To overcome such limitations, we abandon the tree structure and propose a hash-like method. In this paper, we propose LogPunk, an efficient online log parsing method. The core of LogPunk is a novel log signature method based on log punctuations and length features. According to the signature, we can quickly find a small set of candidate templates. Further, the most suitable template is returned by traversing the candidate set with our log similarity function. We evaluated LogPunk on 16 public datasets from the LogHub comparing with five other log parsers. LogPunk achieves the best parsing accuracy of 91.9%. Evaluation results also demonstrate its superiority in terms of robustness and efficiency.Keywords: log parsing; log signature; punctuations; online algorithm
|  | 
 | 
 | 
 | 
 | 
 | 1. IntroductionLogging is the practice of recording events that provides information about the system running status and execution paths. Earlier, system operators could understand runtime behaviors and diagnose failures by manually analyzing logs [1,2]. A modern system service are often composed of several basic services [3]. Moreover, modern system clusters usually contain hundreds of nodes, some of which are even geographically distributed [4]. In this context, with the increasing scale and complexity of modern software systems, the volume of logs explodes [5]. It leads to the emergence of automated log analysis approaches. These automated approaches bring more tools (e.g., anomaly detection [6,7], failure prediction [8,9], and failure diagnosis [10,11]) to ensure system reliability, which is an indispensable step towards AIOps (Artificial Intelligence for IT Operations).Logs are unstructured text printed by logging statements in the source code. As shown in Figure 1, a logging statement is specified by log level, static string, and dynamic variables. As the variable value changes at runtime, a logging statement can produce different log messages. Typically, a log message may contain a timestamp, log level, logger name, and raw message content. Different log messages from the same logging statement have the same log template (event type).Most data mining models used in log analysis require structured input. Therefore raw logs cannot be used as input directly [12–14]. To conquer the unstructured nature of raw logs, we need log parsing to convert the unstructured logs into a structured format before analysis [15]. The goal of log parsing is to extract the static template, dynamic variables, and the header information (timestamp, log level, logger name) from the raw log message to a structured format . Such structured data can be fed into downstream log analysis models. ||---|---|---|---|---|---|---|| Citation: Zhang, S.; Wu, G. Efficient Online Log Parsing with Log  Punctuations Signature. Appl. Sci. 2021, 11, 11974.  Academic Editor: Agostino Forestiero Received: 17 October 2021  Accepted: 12 December 2021 Published: 16 December 2021 Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.  Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article |Citation: Zhang, S.; Wu, G. Efficient Online Log Parsing with Log  Punctuations Signature. Appl. Sci. 2021, 11, 11974.  Academic Editor: Agostino Forestiero Received: 17 October 2021  Accepted: 12 December 2021 Published: 16 December 2021 Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.  Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article |Citation: Zhang, S.; Wu, G. Efficient Online Log Parsing with Log  Punctuations Signature. Appl. Sci. 2021, 11, 11974.  Academic Editor: Agostino Forestiero Received: 17 October 2021  Accepted: 12 December 2021 Published: 16 December 2021 Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.  Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article |Citation: Zhang, S.; Wu, G. Efficient Online Log Parsing with Log  Punctuations Signature. Appl. Sci. 2021, 11, 11974.  Academic Editor: Agostino Forestiero Received: 17 October 2021  Accepted: 12 December 2021 Published: 16 December 2021 Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.  Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article |Citation: Zhang, S.; Wu, G. Efficient Online Log Parsing with Log  Punctuations Signature. Appl. Sci. 2021, 11, 11974.  Academic Editor: Agostino Forestiero Received: 17 October 2021  Accepted: 12 December 2021 Published: 16 December 2021 Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.  Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article |Citation: Zhang, S.; Wu, G. Efficient Online Log Parsing with Log  Punctuations Signature. Appl. Sci. 2021, 11, 11974.  Academic Editor: Agostino Forestiero Received: 17 October 2021  Accepted: 12 December 2021 Published: 16 December 2021 Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.  Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article |1. Introduction Logging is the practice of recording events that provides information about the system running status and execution paths. Earlier, system operators could understand runtime behaviors and diagnose failures by manually analyzing logs [1,2]. A modern system service are often composed of several basic services [3]. Moreover, modern system clusters usually contain hundreds of nodes, some of which are even geographically distributed [4]. In this context, with the increasing scale and complexity of modern software systems, the volume of logs explodes [5]. It leads to the emergence of automated log analysis approaches. These automated approaches bring more tools (e.g., anomaly detection [6,7], failure prediction [8,9], and failure diagnosis [10,11]) to ensure system reliability, which is an indispensable step towards AIOps (Artificial Intelligence for IT Operations). Logs are unstructured text printed by logging statements in the source code. As shown in Figure 1, a logging statement is specified by log level, static string, and dynamic variables. As the variable value changes at runtime, a logging statement can produce different log messages. Typically, a log message may contain a timestamp, log level, logger name, and raw message content. Different log messages from the same logging statement have the same log template (event type). Most data mining models used in log analysis require structured input. Therefore raw logs cannot be used as input directly [12–14]. To conquer the unstructured nature of raw logs, we need log parsing to convert the unstructured logs into a structured format before analysis [15]. The goal of log parsing is to extract the static template, dynamic variables, and the header information (timestamp, log level, logger name) from the raw log message to a structured format . Such structured data can be fed into downstream log analysis models. || distributed |distributed |under |the |terms |and |1. Introduction Logging is the practice of recording events that provides information about the system running status and execution paths. Earlier, system operators could understand runtime behaviors and diagnose failures by manually analyzing logs [1,2]. A modern system service are often composed of several basic services [3]. Moreover, modern system clusters usually contain hundreds of nodes, some of which are even geographically distributed [4]. In this context, with the increasing scale and complexity of modern software systems, the volume of logs explodes [5]. It leads to the emergence of automated log analysis approaches. These automated approaches bring more tools (e.g., anomaly detection [6,7], failure prediction [8,9], and failure diagnosis [10,11]) to ensure system reliability, which is an indispensable step towards AIOps (Artificial Intelligence for IT Operations). Logs are unstructured text printed by logging statements in the source code. As shown in Figure 1, a logging statement is specified by log level, static string, and dynamic variables. As the variable value changes at runtime, a logging statement can produce different log messages. Typically, a log message may contain a timestamp, log level, logger name, and raw message content. Different log messages from the same logging statement have the same log template (event type). Most data mining models used in log analysis require structured input. Therefore raw logs cannot be used as input directly [12–14]. To conquer the unstructured nature of raw logs, we need log parsing to convert the unstructured logs into a structured format before analysis [15]. The goal of log parsing is to extract the static template, dynamic variables, and the header information (timestamp, log level, logger name) from the raw log message to a structured format . Such structured data can be fed into downstream log analysis models. || conditions of the Creative Commons Attribution (CC BY) license (https:// |conditions of the Creative Commons Attribution (CC BY) license (https:// |conditions of the Creative Commons Attribution (CC BY) license (https:// |conditions of the Creative Commons Attribution (CC BY) license (https:// |conditions of the Creative Commons Attribution (CC BY) license (https:// |conditions of the Creative Commons Attribution (CC BY) license (https:// |1. Introduction Logging is the practice of recording events that provides information about the system running status and execution paths. Earlier, system operators could understand runtime behaviors and diagnose failures by manually analyzing logs [1,2]. A modern system service are often composed of several basic services [3]. Moreover, modern system clusters usually contain hundreds of nodes, some of which are even geographically distributed [4]. In this context, with the increasing scale and complexity of modern software systems, the volume of logs explodes [5]. It leads to the emergence of automated log analysis approaches. These automated approaches bring more tools (e.g., anomaly detection [6,7], failure prediction [8,9], and failure diagnosis [10,11]) to ensure system reliability, which is an indispensable step towards AIOps (Artificial Intelligence for IT Operations). Logs are unstructured text printed by logging statements in the source code. As shown in Figure 1, a logging statement is specified by log level, static string, and dynamic variables. As the variable value changes at runtime, a logging statement can produce different log messages. Typically, a log message may contain a timestamp, log level, logger name, and raw message content. Different log messages from the same logging statement have the same log template (event type). Most data mining models used in log analysis require structured input. Therefore raw logs cannot be used as input directly [12–14]. To conquer the unstructured nature of raw logs, we need log parsing to convert the unstructured logs into a structured format before analysis [15]. The goal of log parsing is to extract the static template, dynamic variables, and the header information (timestamp, log level, logger name) from the raw log message to a structured format . Such structured data can be fed into downstream log analysis models. |4.0/).
Appl. Sci. 2021, 11, 11974. 
Appl. Sci. 2021, 11, 11974 2 of 15
Figure 1. An illustrative example of log parsing.
Regular expressions are always a choice for log parsing [15], but it is only practicable for a small number of log templates. Continuous human efforts are needed to develop and maintain the regular expressions, which are labor-intensive and error-prone [16,17]. As modern software has many log templates and constantly evolves [18], developing and maintaining such regular expressions could be a nightmare.To alleviate the pain of human efforts, researchers have proposed many automated data-driven approaches. Earlier works leverage data mining approaches such as frequent pattern mining [19,20], clustering [21,22], and iterative partitioning [23] to extract the common part of log messages under the same cluster as the log template. However, all these approaches are offline, which requires a time-consuming training process and can not deal with the template changes caused by software updates. In contrast, online log parsers parse logs in a streaming fashion and do not require offline training. Therefore, what modern systems need is online log parsing, which is only studied in a few preliminary works [17,24–26].Spell [26] and Drain [17] are state-of-the-art online log parsers. Spell measures the distance between log messages through longest common subsequence (LCS), and uses prefix tree to optimize the processing time of each log message close to linear. Drain also adopts a tree structure. Unlike Spell, it has more heuristics and strong assumptions about the length and preceding tokens of log messages.In practice, we find that the tree structure has some limitations in robustness and efficiency. Although, existing online parsers achieve good parsing accuracy on specific datasets. Their parsing accuracy fluctuates across different datasets, which means that they are not robust (e.g., the average accuracy of Spell [26] is less than 80%). In addition, with the rapid growth of log volume and the increasing demand for low latency log analysis, efficiency becomes an essential concern of log analysis [18,27]. However, the previous benchmark [15] shows that Spell [26] and Drain [17] are not efficient enough. In this work, we abandon the tree structure and propose a hash-like method, which really improves robustness and efficiency.Unlike previous work, we focus on meaningless punctuations in log messages rather than meaningful words. Because we believe that words processing is the reason why the previous approaches are inefficient. Our intuition is that punctuation marks of log messages from the same template tend to be the same, which means simple punctuations imply template information. There are fixed types of punctuations, and they are easy to process.This paper proposes LogPunk, a robust and efficient log parser based on our novel log signature method. LogPunk is designed as a general-purpose online log parsing method, which is system and log type agnostic. We use log punctuations and length information (cf. Section 2) to generate log signatures. Each log signature corresponds to a signature group. Obviously, log messages with the same event type will have the same log signature and get into the same group. However, a signature group may also contain log messages from different event types with the same signature. We call this signature collision easy toAppl. Sci. 2021, 11, 11974 3 of 15
solve by further calculating the similarity between the log message and event templates. Fortunately, we found in our experiments that each signature group only corresponds to one or two event templates in most cases.We evaluate LogPunk on 16 datasets from the LogHub [28] comparing with five other log parsers. Experiments demonstrate that LogPunk is efficient but without loss of accuracy. LogPunk achieves the highest accuracy on 14 datasets and the best average PA of 0.919. More importantly, LogPunk performs consistently across different datasets, which means it is robust.
In summary, our paper makes the following contributions:•	We propose a novel and efficient log signature method based on log punctuations and 	length information applied for log parsing;
•	We present an online log parser based on our log signature method, named LogPunk, 	which is better than the previous log parsers in robustness and efficiency;
•	We conduct extensive experiments on 16 datasets and comparing LogPunk with five 	other log parsers. The results show that LogPunk is accurate, robust, and efficient.The paper is organized as follows. Section 2 introduces the background of log parsing. Section 3 describes our log signature method and the implementation of LogPunk. Section 4 shows the results of evaluating LogPunk on the LogHub datasets. Section 5 compares LogPunk with tree-based methods and discusses the validity. Section 6 introduces the related work of three categories of data-driven log parsing approaches. Finally, Section 7 concludes the paper.2. Problem Description
As shown in Figure 1, the goal of log parsing is to extract the static template, dynamic variables, and header information from a raw log message. As the header information usually follows a fixed format in the same system, regular expressions are commonly used to extract the header information and the log content.Log content is the central processing object of log parsing, composed of static templates and dynamic variables. It can be defined as a tuple of EV = {(ei, vi) : e ∈ E, i = 1, 2, · · · }, where E is the set of all log templates, k = |E| is the number of all distinct templates and vi is a list of variables.
statements (log templates) from the source code, where the k is unknown. A log parser is Formally, given a set of log messages L = {l1, l2, · · · , lm} that is produced by k loggingto parse L to get all k log templates.After extracting the log content from the raw log, it is common to process the content with string splitting. By splitting the content, we get a split list, and each element in the list is called a token. Log message length is defined as the number of tokens in the split list. Formally, each log message consists of a bounded list of tokens, ti = {tj : t ∈ T, j = 1, 2, · · · , n}, where T is a set of all tokens, j is the token index within the split list and n = |ti| is the number of tokens (message length). The token is usually the smallest granularity to determine the template and variable part. A token is either part of the static templates or dynamic variables. Therefore, how to split the content has a significant influence on the parsing results. Different delimiters result in different split token lists. Previous studies often use spaces as delimiters.Online algorithms usually have two core steps: first, find the candidate template set; and then traverse the candidate set to find the most suitable template.Quickly finding candidate sets is critical in the first step. Inspired by the prefix tree data structure, Spell [26] and Drain [17] adopt a tree structure to find candidate sets to filter out most irrelevant templates. Such a tree structure is usually very complex. Some tree nodes have hundreds of children, and the tree needs frequent maintenance. In addition, the tree-based method does not guarantee that the returned template is the longest common subsequence. For example, the log message l = DAPBC (each letter represents a token, and the space between tokens is omitted) and the template set s = {DA, ABC}, the prefixAppl. Sci. 2021, 11, 11974 4 of 15
tree returns DA instead of ABC. Since the second step will traverse the candidate set, the size of the candidate set also matters.
In the previous benchmark [15], Spell and Drain failed to reach the first level efficiency, which means there is still room for improvement. Our goal is to find a more efficient method to obtain smaller candidate sets.
3. MethodologyAs mentioned above, our approach is online, and it can process logs in a streaming way. As shown in Figure 2, when a new raw log message arrives, LogPunk will first preprocess it with some simple regular expressions to extract the content and split its content into a token list. Then, a log signature will be generated for this log message based on the token list. We can quickly locate the signature group that contains a list of possible templates according to the signature. The most suitable template will be returned by searching the signature group with our specially designed log similarity function. If no such template is found, this log message will be appended as a new template itself.Figure 2. Basic workflow of our method.
3.1. Step 1 Preprocess and Split
	This step extracts a list of tokens from each log message through three sub-steps: (1) log content extraction; (2) common variables substitution; (3) log content split.First, a pre-defined regular expression is always used to extract the log content and the header information (e.g., timestamp, log level, and logger name) [15]. Since the header information often follows a fixed format in the same software system, it is convenient to extract it directly. Therefore, the log content is what we are concerned about during the log parsing.After getting the log content, some simple user-defined regular expressions replace common variables (e.g., IP address, URL, and file path) with a special token “”. More-over, the LogPai benchmark (cf. Section 4.1) has already defined such regular expressions. To avoid biased comparison, we apply these regular expressions to all log parsers in our experiments.Finally, we split the log content with delimiters. Instead of using spaces as delim-iters in prior works, we use more delimiters, such as commas (“,”), semicolons (“;”), colons (“:”) and equal signs (“=”). Because variable and constant parts are not always
Appl. Sci. 2021, 11, 11974 5 of 15Appl. Sci. 2021, 11, 11974 5 of 15
separated by spaces. In addition, we keep all the delimiters in the token list after split-ting. Take log message log1 in Figure 2 for example, the token list of “getRecentTasks: num=20,flags=0x1,totalTasks=46” is [’getRecentTasks’, ’:’, ’, ’ ’, ’num’, ’=’, ’20’, ’,’, ’flags’, ’=’,’0x1’, ’,’, ’totalTasks’, ’=’, ’46’].
3.2. Step 2 Generate Log SignatureIn this step, we generate a log signature for each log message (Algorithm 1). To assign the same signature for log messages with the same templates, we must find their common points. Log messages with the same event type have words in common. Many prior studies have emerged based on this observation [17,26]. As mentioned above, we find that log messages with the same event type have the same punctuation marks. We use such punctuation information to generate the log signature.Algorithm 1: Log signature
Input: The log content token_list after splitting.
Output: A number representing the log signature.
1 init PUNCTUATION_TABLE = {’|’, ’"’, ’(’, ’*’, ’;’, ’,’, ’=’, ’:’,	’ ’}; 
2 init freq_dict = dictionary with default value of 0; 
3 init f irst_non_digital_token = None; 
4 init signature = 0; 
5 for token in token_list do
6 
7 
8 
9 
10
11 
12 
13 // count punctuation characters12 
13 // count punctuation characters 
for char in token do 
	if char in PUNCTUATION_TABLE then 	freq_dict[ch]+ = 1; 
	end
end
// find the first non digital token 
if first_non_digital_token is None and token contains no digital character then 	f irst_non_digital_token = token; 
end
14 end 
15 for char, freq in freq_dict do
	// assume the maximum punctuation count is less than 100 17 end 16 	signature = signature ∗ 100 + freq;// assume the maximum token length is less than 100 