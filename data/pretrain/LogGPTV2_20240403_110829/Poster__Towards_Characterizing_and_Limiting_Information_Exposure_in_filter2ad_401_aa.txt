# Title: Characterizing and Limiting Information Exposure in DNN Layers

## Authors:
- Fan Mo, Imperial College London
- Ali Shahin Shamsabadi, Queen Mary University of London
- Kleomenis Katevas, Imperial College London
- Andrea Cavallaro, Queen Mary University of London
- Hamed Haddadi, Imperial College London

## Abstract
Pre-trained Deep Neural Network (DNN) models are increasingly used in smartphones and other user devices to enable prediction services. This leads to potential disclosures of sensitive information from the training data captured within these models. Based on the concept of generalization error, we propose a framework to measure the amount of sensitive information memorized in each layer of a DNN. Our results show that, when considered individually, the last layers encode a larger amount of information from the training data compared to the first layers. We find that the same DNN architecture trained with different datasets has similar exposure per layer. We evaluate an architecture to protect the most sensitive layers within an on-device Trusted Execution Environment (TEE) against potential white-box membership inference attacks without significant computational overhead.

## CCS Concepts
- Computing Methodologies → Distributed Artificial Intelligence
- Security and Privacy → Distributed Systems Security

## Keywords
- Deep learning
- Privacy
- Training data
- Sensitive information exposure
- Trusted execution environment

## 1. Introduction
On-device DNNs have achieved impressive performance in a wide range of services, such as face recognition for authentication and speech recognition for interaction. However, DNNs can memorize information from the training data, leading to privacy concerns when this data is sensitive. Previous works have shown that it is easier to reconstruct the original input data from a DNN using the layer's output (activation) for inference [1]. Additionally, models deployed on devices can be attacked by membership inference attacks (MIA), which leak information about whether specific data was part of the training set, potentially leading to serious privacy issues [7].

We hypothesize that the memorization of sensitive information from training data varies across the layers of a DNN. We present an approach to quantify the generalization error, defined as the expected distance between the prediction accuracy of training and test data [4]. We use this metric to measure the risk of sensitive information exposure in each layer. Our results show that the last layers memorize more sensitive information about the training data, and the risk of information exposure is independent of the dataset.

To protect the most sensitive layers from potential white-box attacks [2, 3], we leverage on-device TEE units, such as Arm's TrustZone (TZ). Experiments show that training the last layers within the TrustZone and the first layers outside the TrustZone results in minor overhead in memory and execution time, making it a feasible solution to protect the model from potential attacks.

## 2. Measuring Information Exposure

### 2.1 Information Exposure Metric
Based on MIA, we define the exposure of private information of an algorithm as the difference between the results obtained on a database with and without the presence of one data record [5, 7]. Given its similarity to the generalization error [4, 7], we use the generalization error \( E_A \) to measure the exposure of private information of an algorithm \( A \):

\[ E_A = \mathbb{E}_{z \in T}[\ell(A_S, z)] - \mathbb{E}_{z \in S}[\ell(A_S, z)] \]

where \( S \) and \( T \) are the training and testing datasets, respectively. Let \( A \) be the DNN model, and \( A_S \) be the model trained with \( S \). The function \( \ell \) is the loss function, and \( z = (x, y) \) refers to data points. Each layer's output serves as the input to the next layer, so \( A(x) = \theta_L(\ldots\theta_l(\ldots\theta_1(x)\ldots)\ldots) \), where \( \theta_i \) represents the layers.

To remove the private information in \( \theta_l \), we create a model \( M_r \) by fine-tuning \( \theta_l \) as \( \theta_{pl} \) on \( S \) and \( T \), and freezing the parameters of the other layers of \( A \). During fine-tuning:

\[ \theta_l^{(X)} \leftarrow \theta_l^{(X)} - \eta \delta_l^{(X)} \]

where \( X \in \{S, T\} \). The back-propagated error \( \delta_l^{(X)} \) is calculated using the entire model and inputs. Therefore, \( \delta_l^{(X)} \) memorizes both \( S \) and \( T \), meaning it is generalized and cannot be utilized by MIA for distinguishing \( S \) and \( T \).

To remove the \( \theta_l \) that contains private information, we assume that after a considerable number of epochs, if the training accuracy does not significantly increase, the initial parameters of \( \theta_l^{(X)} \) have only a slight influence on its final parameters. The functional relationship \( E_{\theta_l} = R_l(\theta_l) \) can then be defined. After fine-tuning, \( \theta_l^{(X)} \) can be represented as:

\[ \theta_l^{(X)} \]

This process helps in reducing the risk of sensitive information exposure in the DNN layers.

## Figures
- **Figure 1**: The proposed framework for measuring the risk of sensitive information exposure in a deep neural network \( A \) trained on a private dataset \( S \). \( M_r \) and \( M_p \) are obtained by fine-tuning a target layer \( l \).
- **Figure 2**: The risk of sensitive information exposure of VGG-7 per layer on MNIST, Fashion-MNIST, and CIFAR-10. Error bars represent 95% confidence intervals.