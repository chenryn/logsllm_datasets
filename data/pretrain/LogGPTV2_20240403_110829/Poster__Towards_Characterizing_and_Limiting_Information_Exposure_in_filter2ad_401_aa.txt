title:Poster: Towards Characterizing and Limiting Information Exposure in
DNN Layers
author:Fan Mo and
Ali Shahin Shamsabadi and
Kleomenis Katevas and
Andrea Cavallaro and
Hamed Haddadi
Poster: Towards Characterizing and Limiting Information
Exposure in DNN Layers
Fan Mo
PI:EMAIL
Imperial College London
Ali Shahin Shamsabadi
PI:EMAIL
Queen Mary University of London
Kleomenis Katevas
PI:EMAIL
Imperial College London
Andrea Cavallaro
PI:EMAIL
Queen Mary University of London
Hamed Haddadi
PI:EMAIL
Imperial College London
ABSTRACT
Pre-trained Deep Neural Network (DNN) models are increasingly
used in smartphones and other user devices to enable prediction
services, leading to potential disclosures of (sensitive) information
from training data captured inside these models. Based on the con-
cept of generalization error, we propose a framework to measure
the amount of sensitive information memorized in each layer of
a DNN. Our results show that, when considered individually, the
last layers encode a larger amount of information from the train-
ing data compared to the irst layers. We ind that the same DNN
architecture trained with diferent datasets has similar exposure
per layer. We evaluate an architecture to protect the most sensitive
layers within an on-device Trusted Execution Environment (TEE)
against potential white-box membership inference attacks without
the signiicant computational overhead.
CCS CONCEPTS
· Computing methodologies → Distributed artiicial intelli-
gence; · Security and privacy → Distributed systems security.
KEYWORDS
deep learning, privacy, training data, sensitive information expo-
sure, trusted execution environment
1 INTRODUCTION
On-device DNNs have achieved impressive performance on a broad
spectrum of services such as face recognition for authentication
and speech recognition for interaction. However, DNNs memorize
in their parameters information from the training data [8]. Thus,
keeping DNNs accessible in user devices leads to privacy concerns
when training data contains sensitive information.
Previous works have shown that a reconstruction of the original
input data is easier from a DNN when using the layer’s output
(activation) for inference [1]. In addition to that, a speech or face
recognition model deployed on devices can be attacked by mem-
bership inference attacks (MIA) [7]. This leaks the information
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proit or commercial advantage and that copies bear this notice and the full citation
on the irst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363279
about whether one audio or picture has involved in pre-training
the model, which could further lead to other serious privacy issues.
We hypothesize that the memorization of sensitive information
from training data difers across the layers of a DNN. We present an
approach and show that each layer behaves diferently on the data
they were trained on compared to the data seen for the irst time,
by quantifying the generalization error (i.e. the expected distance
between prediction accuracy of training data and test data [4]).
We further quantify the risk of sensitive information exposure of
each layer as a function of generalization error. The larger the
generalization error, the easier it is to infer sensitive information
from the training set. Our results show that last layers memorize
more sensitive information about training data, and the risk of
information exposure of a layer is independent of the dataset.
To protect the most sensitive layers from potential white-box
attacks [2, 3], we leverage on-device TEE unit (Arm’s TrustZone
(TZ)), as an example of protection mechanism. Experiments are
conducted by training the last layers in the TrustZone and the irst
layers outside the TrustZone. Results show that the overhead in
memory and execution time is minor, thus making it an afordable
solution to protect a model from potential attacks.
2 MEASURING INFORMATION EXPOSURE
2.1 Information Exposure Metric
Based on MIA, we deine the exposure of private information of
an algorithm as the diference between the results obtained on a
database with and without the presence of one data record [5, 7].
Because of its similarity with the generalization error [4, 7], we can
then apply the generalization error EA to measure the exposure of
private information of an algorithm A:
EA = Ez ∈T [ℓ(AS , z)] − Ez ∈S [ℓ(AS , z)].
(1)
S and T are training dataset and testing dataset respectively.
Let DNN models be A, and AS be the model trained with S. ℓ()
is the lost function. z = (x, y) refers to data points. Each layer’s
output is the input of next layer until the end of the model, so
A(x) = θL(...θl (...θ1(x)...)...). θi is the layers.
To remove the private information in θl , we create a model Mr
by ine-tuning θl as θpl on S and T and by freezing the parameters
of the other layers of A. During ine-tuning,
θ
(X )
l
← θ
(X )
l
− ηδ
(X )
l
.
(2)
Training DNN
Fine-tuning the target layer
Copying
Mr
(null)
(null)
(null)
(null)
θrl
(null)
(null)
(null)
(null)
A
(null)
(null)
(null)
(null)
θ1
θ2
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
(null)
θl(null)
(null)
(null)
(null)
θL−1
(null)
(null)
(null)
(null)
θL(null)
(null)
(null)
(null)
S
…
…
…
…
Mp
(null)
(null)
(null)
(null)
θpl
(null)
(null)
(null)
(null)
…
…
S
T
S
e
v
i
t
i
s
n
e
s
f
o
k
s
R
i
e
r
u
s
o
p
x
e
n
o
i
t
a
m
r
o
n
f
i
1.00
0.75
0.50
0.25
0.00
●
●
●
●
●
Dataset
● MNIST
●
●
Fashion−MNIST
CIFAR−10
1
2
3
4
5
6
7
Layer
Copying
Figure 2: The risk of sensitive information exposure of VGG-
7 per layer on MNIST, Fashion-MNIST and CIFAR-10. Error
bars represent 95% CI.
Figure 1: The proposed framework for measuring the risk
of sensitive information exposure in a deep neural network
A trained on a private dataset S. Mr and Mp are obtained by
ine-tuning a target layer l.
(X )
l
X ∈ {S,T }. δ
is the back-propagated error calculated using
(X )
θ of the whole model and inputs. Therefore, δ
l
(X )
l
memorises both S
θ1:l −1, θl , θl +1:L, X . By learning on X , the θ
and T , which means that it is generalized and can not be utilized
by MIA for distinguishing S and T .
is dependent on
To remove the θl which contains private information, we simply
consider that after training for a considerable number of epochs
(instead of resetting as random values), if training accuracy does
not signiicantly increase, the initial parameters of θ
a slight inluence on its inal parameters.
(X )
l
only have
θ1:l −1 and θl +1:L are frozen during ine-tuning θ
(X )
l
the functional relationship Eθl = Rl (θl ). After ine-tuning, the θ
can be presented as
. Let us deine
(X )
l
θ
(X )