==== 采集Avro序列化数据
客户数据经过Avro库序列化后存入kafka，Heka需要在消费kafka的同时，使用avro对其数据进行反序列化
准备工作::
linux 64bit heka3.1.0.19以上（包括3.1.0.19）
配置说明::
Agent配置页面中点击“高级配置”进行编辑，在###Other下追加以下配置
[source,]
----
[1_kafka_input]
  type = "CKafkaInput"
  commit_interval_second = 5
  offset = "earliest"
  bootstrap_servers = "127.0.0.1:9092,127.0.0.2:9092"
  group_id = "rret"
  topics = ["test"]
  report_metrics = false
  ticker_interval = 60
  appname = "popopo"
  tag = "heka"
  worker = 1
  router_chan_size = 30
  decoder = "1_avro_decoder"
[1_avro_decoder]
  type = "AvroDecoder"
  # 注意！schema_path和schema有且只能配置一个
  # 保存schema文件的路径
  # schema_path = "/Users/yl/rzy/heka-dev/build/heka/src/git.yottabyte.cn/heka-plugins/avro/test/schema.json"
  # json形式的schema
  schema = "{\"type\": \"record\", \"name\": \"LongList\", \"fields\": [{\"name\": \"next\", \"type\": [\"null\", \"LongList\", {\"type\": \"long\", \"logicalType\": \"timestamp-millis\"}], \"default\": null}]}"
  # 序列化后数据的格式，目前支持single_object和binary(默认值)
  format = "binary"
----
注意::
* 1_avro_decoder下的属性为该AvroDecoder的配置
* 1_kafka_input下的配置为CKafkaInput的配置,该配置可以选择通过前台配置后，通过增加decoder = “1_avro_decoder" 来将input和decoder对应起来即可
* 更多关于CKafkaInput的配置可以参考《日志易数据接入手册》->《采集单一数据源》-> 《Kafka》章节