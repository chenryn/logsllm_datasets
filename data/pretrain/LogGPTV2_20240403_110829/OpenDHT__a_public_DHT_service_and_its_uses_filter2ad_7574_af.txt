OpenDHT put/get usage: Table 4 shows that the majority of
these applications use only OpenDHT’s put/get interface. We found
that many of these (e.g., DOA, FOOD, instant messaging, HIP)
make quite trivial use of the DHT—primarily straightforward in-
dexing. Such applications are a perfect example of the beneﬁt of a
shared DHT; their relatively simple needs are trivially met by the
put/get interface, but none of the applications in themselves warrant
the deployment of an independent DHT.
ReDiR usage: We have four example applications that use
ReDiR—two built by us and two by others.
i3 is an indirection-
based routing infrastructure built over a DHT lookup interface. To
validate that ReDiR can be easily used to support applications tra-
ditionally built over a lookup interface, we ported the i3 code to
run over OpenDHT. Doing so was extremely easy, requiring only
a simple wrapper that emulated i3’s Chord implementation and re-
quiring no change to how i3 itself is engineered.
As described in Section 4, existing DHT-based multicast sys-
tems [7, 27, 35] typically use a routing interface. To explore the
feasibility of supporting such applications, we implemented and
evaluated Multicast Over OpenDHT (MOOD), using a ReDiR-like
hierarchy as suggested in [19].
(The QStream project has inde-
pendently produced another multicast implementation based on a
ReDiR-like hierarchy.) MOOD is not a simple port of an existing
implementation, but a wholesale redesign. We conjecture based on
this experience that one can often redesign routing-based applica-
tions to be lookup-based atop a DHT service. We believe this is an
area ripe for further research, both in practice and theory.
Finally, the Place Lab project makes novel use of ReDiR. In
Place Lab, a collection of independently operated servers processes
data samples submitted by a large number of wireless client de-
vices. Place Lab uses ReDiR to “route” an input data sample to the
unique server responsible for processing that sample.
In summary, in the few months since being available to the pub-
lic, OpenDHT has already been used by a healthy number of very
different applications. Of course, the true test of OpenDHT’s value
will lie in the successful, long-term deployment of such applica-
tions; we merely offer our early experience as an encouraging indi-
cation of OpenDHT’s generality and utility.
5.3.2 FOOD: FreeDB Over OpenDHT
FreeDB is a networked database of audio CD metadata used by
many CD-player applications. The service indexes over a million
CDs, and as of September 2004 was serving over four million read
requests per week across ten widely dispersed mirrors.
A traditional FreeDB query proceeds in two stages over HTTP.
First, the client computes a hash value for a CD—called its discid—
and asks the server for a list of CDs with this discid. If only one CD
is returned, the client retrieves the metadata for that CD from the
server and the query completes. According to our measurements,
Application
Organization
Uses OpenDHT for . . .
put/get or ReDiR
Comments
Croquet Media Messenger
Delegation Oriented Arch. (DOA)
Host Identity Protocol (HIP)
Instant Messaging Class Project
Tetherless Computing
Photoshare
Place Lab 802.11 Location System
QStream: Video Streaming
RSSDHT: RSS Aggregation
FOOD: FreeDB Over OpenDHT
Instant Messaging Over OpenDHT
i3 Over OpenDHT
Croquet
MIT, UCB
IETF WG
MIT
Waterloo
Jordan Middle School
IRS
UBC
SFSU
OpenDHT
OpenDHT
OpenDHT
replica location
indexing
name resolution
rendezvous
host mobility
HTTP redirection
location-based redirection
and range queries
multicast tree construction
multicast tree construction
storage
rendezvous
redirection
put/get
put/get
put/get
put/get
put/get
put/get
ReDiR
ReDiR
ReDiR
put/get
put/get
ReDiR
MOOD: Multicast Over OpenDHT
OpenDHT
multicast tree construction
ReDiR
Table 4: Applications built or under development on OpenDHT.
http://opencroquet.org/
http://nms.lcs.mit.edu/doa/
alternative to DNS-based resolution
MIT 6.824, Spring 2004
http://mindstream.watsmore.net/
http://ezshare.org/
http://placelab.org/
http://qstream.org/
http://sourceforge.net/projects/rssdht/
78 semicolons Perl
123 semicolons C++
201 semicolons Java glue between
i3 and ReDiR, passes i3 regr. tests,
http://i3.cs.berkeley.edu/
474 semicolons Java
this situation occurs 91% of the time. In the remaining cases, the
client retrieves the metadata for each CD in the list serially until it
ﬁnds an appropriate match.
A single FOOD client puts each CD’s data under its discid. To
query FOOD, other clients simply get all values under a discid. A
proxy that translates legacy FreeDB queries to FOOD queries is
only 78 semicolons of Perl.
Measurement Setup We stored a May 1, 2004 snapshot of
the FreeDB database containing a total of 1.3 million discids in
OpenDHT. To compare the availability of data and the latency of
queries in FreeDB and FOOD, we queried both systems for a ran-
dom CD every 5 seconds. Our FreeDB measurements span October
2–13, 2004, and our FOOD measurements span October 5–13.
Results During the measurement interval, FOOD offered avail-
ability superior to that of FreeDB. Only one request out of 27,255
requests to FOOD failed, where each request was tried exactly
once, with a one-hour timeout. This fraction represents a 99.99%
success rate, as compared with a 99.9% success rate for the most
reliable FreeDB mirror, and 98.8% for the least reliable one.
In our experiment, we measured both the total latency of FreeDB
queries and the latency of only the ﬁrst HTTP request within each
FreeDB query. We present this last measure as the response time
FreeDB might achieve via a more optimized protocol. We consider
FreeDB latencies only for the most proximal server, the USA mir-
ror. Comparing the full legacy version of FreeDB against FOOD,
we observe that over 70% of queries complete with lower latency
on FOOD than on FreeDB, and that for the next longest 8% of
queries, FOOD and FreeDB offer comparable response time. For
the next 20% of queries, FOOD has less than a factor of two longer-
latency than FreeDB. Only for the slowest 2% of queries does
FOOD offer signiﬁcantly greater latency than FreeDB. We attribute
this longer tail to the number of request/response pairs in a FOOD
transaction vs. in a FreeDB transaction. Even for the idealized ver-
sion of FreeDB, in which queries complete in a single HTTP GET,
we observe that roughly 38% of queries complete with lower la-
tency on FOOD than on the idealized FreeDB, and that the median
330 ms required for FOOD to retrieve all CDs’ data for a discid is
only moderately longer than the median 248 ms required to com-
plete only the ﬁrst step of a FreeDB lookup.
In summary, FOOD offers improved availability, with minimal
development or deployment effort, and reduced latency for the ma-
jority of queries vs. legacy FreeDB.
5.3.3 Common Feature Requests
We now brieﬂy report experience we have gleaned in interac-
tions with users of OpenDHT. In particular, user feature requests
are one way of identifying which aspects of the design of a shared
DHT service matter most during development of real applications.
Requests from our users included:
XML RPC We were surprised at the number of users who re-
quested that OpenDHT gateways accept requests over XML RPC
(rather than our initial offering, Sun RPC). This request in a sense
relates to generality; simple client applications are often written in
scripting languages that manipulate text more conveniently than bi-
nary data structures, e.g., as is the case in Perl or Python. We have
since added an XML RPC interface to OpenDHT.
Remove function After XML RPC, the ability to remove values
before their TTLs expire was the most commonly requested fea-
ture in our early deployment. It was for this reason that we added
remove to the current OpenDHT interface.
Authentication While OpenDHT does not currently support the
immutable or signed puts we proposed in Section 3.1, we have had
essentially no requests for such authentication from users. How-
ever, we believe this apparent lack of concern for security is most
likely due to these applications being themselves in the relatively
early stages of deployment.
Read-modify-write As discussed in Section 3.1, OpenDHT cur-
rently provides only eventual consistency. While it is possible to
change values in OpenDHT by removing the old value and putting
a new one, such operations can lead to periods of inconsistency.
In particular, when two clients change a value simultaneously,
OpenDHT may end up storing both new values. Although this sit-
uation can be ﬁxed after the fact using application-speciﬁc conﬂict
resolution as in Bayou [24], an alternate approach would be to add a
read-modify-write primitive to OpenDHT. There has recently been
some work in adding such primitives to DHTs using consensus al-
gorithms [22], and we are currently investigating other primitives
for improving the consistency provided by OpenDHT.
Larger maximum value size Purely as a matter of convenience,
several users have requested that OpenDHT support values larger
than 1 kB. OpenDHT’s current 1 kB limit on values exists only due
to Bamboo’s use of UDP as a transport. In the near future, we plan
to implement fragmentation and reassembly of data blocks in order
to raise the maximum value size substantially.
6. DISCUSSION
OpenDHT is currently a single infrastructure that provides stor-
age for free. While this is appropriate for a demonstration project,
it is clearly not viable for a large-scale and long-term service on
which applications critically rely. Thus, we expect that any success
trajectory would involve the DHT service becoming a commercial
enterprise. This entails two signiﬁcant changes. First, storage can
no longer be free. The direct compensation may not be monetary
(e.g., gmail’s business model), but the service must somehow be-
come self-sustaining. We don’t speculate about the form this charg-
ing might take but only note that it will presumably involve authen-
ticating the OpenDHT user. This could be done at the OpenDHT
gateways using traditional techniques.
Second, a cooperating but competitive market must emerge, in
which various competing DHT service providers (DSPs) peer to-
gether to provide a uniform DHT service, a DHT “dialtone,” much
as IP is a universal dialtone. Applications and clients should be
able to access their DSPs (the ones to whom they’ve paid money or
otherwise entered into a contractual relationship) and access data
stored by other applications or clients who have different DSPs. We
don’t discuss this in detail, but a technically feasible and economi-
cally plausible peering arrangement is described by Balakrishnan et
al. [4]. Each DSP would have incentive to share puts and gets with
other DSPs, and there are a variety of ways to keep the resulting
load manageable. DHT service might be bundled with traditional
ISP service (like DNS), so ISPs and DSPs would be identical, but
a separate market could evolve.
If such a market emerges, then DHT service might become a
natural part of the computational infrastructure on which applica-
tions could rely. This may not signiﬁcantly change the landscape
for large-scale, high-demand applications, which could have easily
erected a DHT for their own use, but it will foster the development
of smaller-scale applications for which the demand is much less
certain. Our early experience suggests there are many such appli-
cations, but only time will tell.
7. SUMMARY
In this paper we have described the design and early deployment
of OpenDHT, a public DHT service. Its put/get interface is easy
for simple clients to use, and the ReDiR library expands the func-
tionality of this interface so that OpenDHT can support more de-
manding applications. Storage is allocated fairly according to our
per-IP-address and per-disk deﬁnition of fairness. The deployment
experience with OpenDHT has been favorable; the system is cur-
rently supporting a variety of applications, and is slowly building
a user community. The latency and availability it provides is ade-
quate and will only get better as basic DHT technology improves.
8. ACKNOWLEDGMENTS
We are grateful to Yatin Chawathe, Michael Walﬁsh, and the
anonymous reviewers for their excellent feedback, which greatly
improved this paper. This work was supported in part under NSF
Cooperative Agreement ANI-0225660. Sean Rhea is supported by
an IBM Fellowship. Brighten Godfrey is supported by a National
Science Foundation Graduate Research Fellowship.
9. REFERENCES
[1] Bamboo. http://bamboo-dht.org/.
[2] Chord. http://www.pdos.lcs.mit.edu/chord/.
[3] Pastry. http://freepastry.rice.edu/.
[4] H. Balakrishnan, S. Shenker, and M. Walﬁsh. Peering peer-to-peer providers. In
IPTPS, Feb. 2005.
[5] A. Bavier et al. Operating system support for planetary-scale network services.
In NSDI, Mar. 2004.
[6] M. Beck, T. Moore, and J. S. Plank. An end-to-end approach to globally
scalable programmable networking. In FDNA, 2003.
[7] M. Castro, P. Druschel, A.-M. Kermarrec, A. Nandi, A. Rowstron, and
A. Singh. SplitStream: High-bandwidth multicast in a cooperative environment.
In SOSP, 2003.
[8] J. Cates. Robust and efﬁcient data management for a distributed hash table.
Master’s thesis, MIT, May 2003.
[9] F. Dabek, M. F. Kaashoek, D. Karger, R. Morris, and I. Stoica. Wide-area
cooperative storage with CFS. In SOSP, Oct. 2001.
[10] F. Dabek, J. Li, E. Sit, J. Robertson, M. F. Kaashoek, and R. Morris. Designing
a DHT for low latency and high throughput. In NSDI, 2004.
[11] F. Dabek, B. Zhao, P. Druschel, J. Kubiatowicz, and I. Stoica. Towards a
common API for structured P2P overlays. In IPTPS, 2003.
[12] A. Demers, S. Keshav, and S. Shenker. Analysis and simulation of a fair
queuing algorithm. In SIGCOMM, 1989.
[13] J. Douceur. The Sybil attack. In IPTPS, 2002.
[14] P. Druschel and A. Rowstron. Storage management and caching in PAST, a
large-scale, persistent peer-to-peer storage utility. In SOSP, 2001.
[15] M. J. Freedman, E. Freudenthal, and D. Mazi`eres. Democratizing content
publication with Coral. In NSDI, Mar. 2004.
[16] P. Goyal, H. Vin, and H. Cheng. Start-time fair queuing: A scheduling algorithm
for integrated services packet switching networks. In SIGCOMM, Aug. 1996.
[17] R. Huebsch, J. M. Hellerstein, N. Lanham, B. T. Loo, S. Shenker, and I. Stoica.
Querying the Internet with PIER. In VLDB, 2003.
[18] D. R. Karger and M. Ruhl. Diminished Chord: A protocol for heterogeneous
subgroup formation in peer-to-peer networks. In IPTPS, 2004.
[19] B. Karp, S. Ratnasamy, S. Rhea, and S. Shenker. Spurring adoption of DHTs
with OpenHash, a public DHT service. In IPTPS, 2004.
[20] A. Mislove et al. POST: a secure, resilient, cooperative messaging system. In
HotOS, 2003.
[21] R. Moskowitz, P. Nikander, P. Jokela, and T. Henderson. Host identity protocol
(work in progress). IETF Internet Draft, 2004.
[22] A. Muthitacharoen, S. Gilbert, and R. Morris. Etna: A fault-tolerant algorithm
for atomic mutable DHT data. Technical Report MIT-LCS-TR-993, MIT-LCS,
June 2005.
[23] A. Muthitacharoen, R. Morris, T. Gil, and B. Chen. Ivy: A read/write
peer-to-peer ﬁle system. In OSDI, 2002.
[24] K. Petersen, M. Spreitzer, D. Terry, M. Theimer, and A. Demers. Flexible
update propagation for weakly consistent replication. In SOSP, 1997.
[25] S. Ramabhadran, S. Ratnasamy, J. Hellerstein, and S. Shenker. Brief
announcement: Preﬁx hash tree (extended abstract). In PODC, 2004.
[26] V. Ramasubramanian and E. G. Sirer. The design and implementation of a next
generation name service for the Internet. In SIGCOMM, Aug. 2004.
[27] S. Ratnasamy, M. Handley, R. Karp, and S. Shenker. Application-level
multicast using content-addressable networks. Lecture Notes in Computer
Science, 2233:14–29, 2001.
[28] S. Rhea, P. Eaton, D. Geels, H. Weatherspoon, B. Zhao, and J. Kubiatowicz.
Pond: the OceanStore prototype. In USENIX FAST, Mar. 2003.
[29] S. Rhea, D. Geels, T. Roscoe, and J. Kubiatowicz. Handling churn in a DHT. In
USENIX Annual Tech. Conf., June 2004.
[30] T. Roscoe and S. Hand. Palimpsest: Soft-capacity storage for planetary-scale
services. In HotOS, May 2003.
[31] I. Stoica, D. Adkins, S. Zhuang, S. Shenker, and S. Surana. Internet indirection
infrastructure. In SIGCOMM, Aug. 2002.
[32] J. Stribling. Planetlab all-pairs ping. http:
//www.pdos.lcs.mit.edu/˜strib/pl_app/APP_README.txt.
[33] M. Walﬁsh, H. Balakrishnan, and S. Shenker. Untangling the Web from DNS.
In NSDI, Mar. 2004.
[34] B. Y. Zhao, L. Huang, J. Stribling, S. C. Rhea, A. D. Joseph, and J. D.
Kubiatowicz. Tapestry: A resilient global-scale overlay for service deployment.
IEEE JSAC, 22(1):41–53, Jan. 2004.
[35] S. Q. Zhuang, B. Y. Zhao, A. D. Joseph, R. H. Katz, and J. Kubiatowicz.
Bayeux: An architecture for scalable and fault-tolerant wide-area data
dissemination. In NOSSDAV, 2001.