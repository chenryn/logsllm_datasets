Kubernetes 1.4引入了Inter-pod affinity 和 anti-affinity。Inter-pod affinity和anti-affinity允许您根据已在Node上运行的Pod 上的Label，而非基于Node的Label来约束您的Pod能被调度到哪些Node。规则的形式是“如果X已经运行了一个或多个满足规则Y的Pod，则该Pod应该（或者在anti-affinity的情况下不应该）运行在X中”。Y表示一个与Namespace列表相关联（或“所有”命名空间）的LabelSelector；与Node不同，因为Pod是在Namespace中的（因此Pod上的Label是隐含Namespace的），Pod Label上的Label Selector必须指定选择器要应用的Namespace。 概念上，X是一个拓扑域，如Node、机架、云提供商Zone、云提供商Region等。您可以使用 `topologyKey` 表示它，该key是系统用来表示拓扑域的Node标签，例如参见上面 [Interlude: built-in node labels](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels) 中列出的键。
与Node Affinity一样，目前有两种类型的pod affinity和anti-affinity，称为 `requiredDuringSchedulingIgnoredDuringExecution` 和 `preferredDuringSchedulingIgnoredDuringExecution` ，表示“hard”对“soft”需求。请参阅上文Node Affinity部分中的说明。一个 `requiredDuringSchedulingIgnoredDuringExecution` 的例子是“将同一个Zone中的Service A和Service B的Pod放到一起，因为它们彼此通信很多”，而 `preferredDuringSchedulingIgnoredDuringExecution` anti-affinity表示“将该Service的Pod跨Zone“（硬性要求没有意义，因为你可能有比Zone更多的Pod）。
Inter-pod affinity用 `podAffinity` 字段指定，它是PodSpec中 `affinity` 字段的子字段。 inter-pod anti-affinity用 `podAntiAffinity` 指定，它是 `affinity` 字段的子字段。
#### An example of a pod that uses pod affinity:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: failure-domain.beta.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: gcr.io/google_containers/pause:2.0
```
本例中，Pod的Affinity定义了一个Pod Affinity规则和一个Pod anti-affinity规则。 在此示例中， `podAffinity` 在是`requiredDuringSchedulingIgnoredDuringExecution` ，而`podAntiAffinity` 是`preferredDuringSchedulingIgnoredDuringExecution` 。Pod Affinity规则表示，只有当相同Zone中的某个Node至少有一个已经运行的、具有key=security、value=S1的Label的Pod时，该Pod才能调度到Node上。 （更准确地说，Pod会运行在这样的Node N上：Node N具有带有 `failure-domain.beta.kubernetes.io/zone` 的Label key和某些value V，即：集群中至少有一个带有key= `failure-domain.beta.kubernetes.io/zone` 以及value=V的Node，其上运行了key=security并且value=S1标签的Pod）。pod anti-affinity规则表示该Pod更倾向于不往那些已经运行着Label key=security且value=S2的Pod的Node上调度。（如果 `topologyKey` 是 `failure-domain.beta.kubernetes.io/zone` ，那么这意味着如果相同Zone中的Node中有Pod的key=security并且value=S2，则该Pod不能调度到该Node上“）。对于pod affinity以及anti-affinity的更多例子，详见 [design doc](https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md) 。
pod affinity and anti-affinity的合法操作符是 `In` 、 `NotIn` 、 `Exists` 、 `DoesNotExist` 。
原则上， `topologyKey` 可以是任何合法的标签key。然而，出于性能和安全的原因，对于topologyKey有一些限制：
1. 对于Affinity以及 `RequiredDuringScheduling` pod anti-affinity,，不允许使用空（empty）的 `topologyKey` 。
2. 对于 `RequiredDuringScheduling` pod anti-affinity，引入了admission controller `LimitPodHardAntiAffinityTopology`  ，从而限制到 `kubernetes.io/hostname` 的 `topologyKey` 。如果要使其可用于自定义拓扑，您可以修改admission controller，或者简单地禁用它。
3. 对于 `PreferredDuringScheduling` pod anti-affinity，空（empty） `topologyKey` `kubernetes.io/hostname`被解释为“所有拓扑”（“所有拓扑”在这里仅限于 `kubernetes.io/hostname` 和`failure-domain.beta.kubernetes.io/region` 的组合）。
4. 除上述情况外， `topologyKey` 可以是任何合法的标签key。
除 `labelSelector` 和 `topologyKey` 外，还可以选择指定 `labelSelector` 应该匹配的Namespace列表（这与 `labelSelector` 和 `topologyKey` 的定义相同）。如果省略，默认为：定义affinity/anti-affinity了的Pod的Namespace。 如果定义为空（empty），则表示“所有Namespace”。
所有与`requiredDuringSchedulingIgnoredDuringExecution` affinity以及anti-affinity相关联的 `matchExpressions` 必须满足，才会将Pod调度到Node上。
#### More Practical Use-cases（更多实用的用例）
Interpod Affinity和AnitAffinity在与更高级别的集合（例如ReplicaSets、Statefulsets、Deployments等）一起使用时可能更为有用。可轻松配置工作负载应当统统位于同一个拓扑中，例如，同一个Node。
##### Always co-located in the same node（始终位于同一个Node）
在一个有3个Node的集群中，web应用有诸如redis的缓存。我们希望web服务器尽可能地与缓存共存。这是一个简单的redis Deployment的yaml片段，包含3个副本和选择器标签 `app=store` 。
```yaml
apiVersion: apps/v1beta1 # for versions before 1.6.0 use extensions/v1beta1
kind: Deployment
metadata:
  name: redis-cache
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      containers:
      - name: redis-server
        image: redis:3.2-alpine
```
在webserver Deployment的yaml代码片段下面配置了 `podAffinity` ，它通知scheduler其所有副本将与具有选择器标签 `app=store` 的Pod共同定位
```yaml
apiVersion: apps/v1beta1 # for versions before 1.6.0 use extensions/v1beta1
kind: Deployment
metadata:
  name: web-server
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
```
如果我们创建上述两个Deployment，我们的三节点集群可能如下所示。
| node-1        | node-2        | node-3        |
| ------------- | ------------- | ------------- |
| *webserver-1* | *webserver-2* | *webserver-3* |
| *cache-1*     | *cache-2*     | *cache-3*     |
如您所见， `web-server` 3个副本将按预期自动与缓存共同定位。
```shell
$kubectl get pods -o wide
NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
redis-cache-1450370735-6dzlj   1/1       Running   0          8m        10.192.4.2   kube-node-3
redis-cache-1450370735-j2j96   1/1       Running   0          8m        10.192.2.2   kube-node-1
redis-cache-1450370735-z73mh   1/1       Running   0          8m        10.192.3.1   kube-node-2
web-server-1287567482-5d4dz    1/1       Running   0          7m        10.192.2.3   kube-node-1
web-server-1287567482-6f7v5    1/1       Running   0          7m        10.192.4.3   kube-node-3
web-server-1287567482-s330j    1/1       Running   0          7m        10.192.3.2   kube-node-2
```
最佳实践是配置这些高可用的有状态工作负载，例如有AntiAffinity规则的redis，以保证更好的扩展，我们将在下一节中看到。
##### Never co-located in the same node（永远不位于同一个Node）
高可用数据库StatefulSet有一主三从，可能不希望数据库实例都位于同一Node中。
| node-1      | node-2         | node-3         | node-4         |
| ----------- | -------------- | -------------- | -------------- |
| *DB-MASTER* | *DB-REPLICA-1* | *DB-REPLICA-2* | *DB-REPLICA-3* |
[Here](https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure) 是一个Zookeper StatefulSet的例子，为高可用配置了anti-affinity的。
有关inter-pod affinity/anti-affinity的更多信息，请参阅 [here](https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md) 的设计文档。
您也可以检查 [Taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) ，这样可让Node排斥一组Pod。
## 原文
## 参考文档