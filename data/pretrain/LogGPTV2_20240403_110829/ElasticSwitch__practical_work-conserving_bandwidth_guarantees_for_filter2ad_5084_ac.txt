and use its measured demand in the previous epoch as the estimate
(guarantee) for the next epoch (plus an error margin). If the ﬂow did
utilize its guarantee in the previous epoch, then we exponentially
increase its guarantee. We increase the guarantee only up to the fair
share of unbounded VM-to-VM ﬂows to/from the VM for which
GP is applied.
We emphasize that the above discussion refers only to new VM-
to-VM ﬂows. However, most of the TCP ﬂows that a VM X initi-
ates will be to VMs that X has already communicated with in the
recent past (e.g., [7] reports that each service typically communi-
cates with a small number of other services). When all TCP ﬂows
between two VMs are short, the guarantee for that VM-to-VM ﬂow
will simply have a small value (since the demand will be small).
If all ﬂows of one VM have bounded demands, any unallocated
part of the bandwidth guarantee of that VM is redistributed (propor-
tional to demands), such that the VM’s entire guarantee is allocated.
This allows a timely response if the demand of a ﬂow increases.
Weak interdependence for efﬁciency: Up to now, we described
GP as applied for each VM X independently of other VMs. How-
ever, to achieve efﬁciency and ensure X can fully utilize its guar-
antee in the hose model, it is useful to take into account the guar-
antees assigned by the remote VM with which X communicates.
Speciﬁcally, since for the communication between X → Y only
the minimum of BX→Y
is used, some of X’s or Y ’s
guarantees might be “wasted.” Such potential waste can only occur
for senders; intuitively, receivers simply try to match the incoming
demands, so they cannot “waste” guarantees.
and BX→Y
= B
For example, assume that in Fig. 4 all demands are inﬁnite and
all VMs are part of the same hose model with identical guarantees,
B. In this case, the guarantee assigned to the ﬂow X → Y at Y ’s
hypervisor, BX→Y
3 , which is smaller than the guarantee
assigned by X’s hypervisor, BX→Y
2 . Thus, a part of X’s guar-
antee could be wasted because the X → Y ﬂow is bottlenecked at
Y . Thus, we should assign BX→Z
3 instead of B
2 .
, will be B
to be 2B
> BX→Y
To address this situation, senders take into account receivers’
assigned guarantees. Speciﬁcally, if destination Y allocates a
smaller guarantee than sender X (i.e., BX→Y
), and
if Y marks this ﬂow as unbounded3, meaning that Y allocates
to X→Y its maximum fair share, the sender simply deems the
demand of the ﬂow as the destination’s guarantee and hence sets
BX→Y
for the next epoch. Thus, our approach ensures
that taking the minimum of source and destination VM-to-VM
guarantees does not limit VMs from achieving their hose model
guarantees. Note that Y must mark the ﬂow as unbounded since,
otherwise, Y ’s lower guarantee for the ﬂow means that either: (a)
the X → Y ﬂow indeed has lower demand than the guarantees as-
= BX→Y
Y
X
X
Y
X
Y
X
Y
Y
3To mark a ﬂow as unbounded, the remote-guarantee control pack-
ets contain an additional ﬂag bit.
signed by eitherX or Y , or (b) Y misclassiﬁed the ﬂow, a situation
that should be resolved at the next iteration.
Because GP is not entirely independent between VMs, a change
in one VM-to-VM ﬂow’s demand or a new ﬂow can “cascade” to
the guarantees of other VMs. This cascading is loop-free, neces-
sary to fully utilize guarantees, and typically very short in hop-
length. For example, if all tenant VMs have equal bandwidth
guarantees, updates in guarantees cascade only towards VMs with
strictly fewer VM-to-VM ﬂows. More speciﬁcally, a new VM-to-
VM ﬂow X1 → X2 indirectly affects guarantees on the cascading
chain X3, X4, X5 if f: (i) all VMs fully utilize guarantees; (ii)
ﬂows X3 → X2, X3 → X4, and X5 → X4 exist; and (iii) the
X3 → X2 guarantee is bottlenecked at X2, X3 → X4 at X3,
X5 → X4 at X4.
Convergence: The guarantee partitioning algorithm converges to a
set of stable guarantees for stationary demands. We do not present
a formal proof, but the intuition is as follows. Assume X is (one of)
the VM(s) with the lowest fair-share guarantee for the unbounded
incoming or outgoing VM-to-VM ﬂows in the converged alloca-
tion; e.g., if all VMs have the same hose bandwidth guarantee and
all demands are unsatisﬁed, X is the VM that communicates with
the largest number of other VMs. Then, X will (i) converge in the
ﬁrst iteration and (ii) never change its guarantee allocation. The
senders (or receivers) for X will use the guarantees allocated for
X and not change them afterwards. Thus, we can subtract X and
its ﬂows, and apply the same reasoning for the rest of the VMs.
Hence, the worst case convergence time is is on the order of the
number of VMs.
However, we do not expect the convergence of GP to be an is-
sue for practical purposes. In fact, we expect convergence to oc-
cur within the ﬁrst one/two iterations almost all the time, since
multi-step convergence requires very speciﬁc communication pat-
terns and demands, described earlier for the cascading effect.
In dynamic settings, convergence is undeﬁned, and we aim to
preserve safety and limit transient inefﬁciencies. GP’s use of the
minimum guarantee between source and destination ensures safety.
We limit transient inefﬁciencies by not considering new ﬂows as
unbounded, and by applying GP frequently and on each new VM-
to-VM ﬂow. We evaluate transient inefﬁciencies in Section 7.
5. RATE ALLOCATION (RA)
The RA layer uses VM-to-VM rate-limiters, such that: (1) the
guarantees computed by the GP layer are enforced, and (2) the en-
tire available network capacity is utilized when some guarantees
are unused; in this case, excess capacity is shared in proportion to
the active guarantees.
We control
rate-limiters using a weighted TCP-like rate-
adaptation algorithm, where the weight of the ﬂow between VMs
X and Y is BX→Y , the bandwidth provided by the GP layer. We
compute a shadow rate as would result from communicating using
a weighted TCP-like protocol between X and Y . When this rate
is higher than the minimum guarantee BX→Y , we use it instead of
BX→Y , since this indicates there is free bandwidth in the network.
Concretely, the rate-limit from source VM X to destination VM
Y is set to RX→Y :
RX→Y = max(BX→Y, RW_TCP(BX→Y, FX→Y))
where RW _T CP is the rate given by a weighted TCP-like algorithm
operating with weight BX→Y and congestion feedback F X→Y .
Weighted TCP-like algorithms have been extensively stud-
ied, e.g., [8, 16, 22], and any of these approaches can be used for
ElasticSwitch. We use a modiﬁed version of the algorithm pro-
posed by Seawall [22], which, in turn, is inspired by TCP CUBIC.
355Seawall also uses this algorithm to control rate-limiters in hypervi-
sors. Similar to other proposals [12,20,22], we use control packets
to send congestion feedback from the hypervisor of the destination
back to the hypervisor of the source.
The Seawall algorithm increases the rate-limit of the trafﬁc from
X to Y on positive feedback (lack of congestion) proportional to
the weight, using a cubic-shaped4 function to approach a goal rate
and then explore higher rates above the goal. The goal rate is the
rate where congestion was last experienced. Before reaching the
goal, the rate r is increased in a concave shape by w · δ(rgoal −
r)(1 − Δt)3 at each iteration, where w is the weight of the ﬂow, δ
is a constant, and Δt is proportional to the time elapsed since the
last congestion. Above the goal, the rate is convexly increased by
n·w·A at each iteration, where n is the iteration number and A is a
constant. On negative feedback (e.g., lost packets), the rate-limit is
decreased multiplicatively by a constant independent of the weight.
Rationale for modifying Seawall’s algorithm: Our initial ap-
proach was to actually maintain a shadow TCP-like rate and use
the maximum between that rate and the guarantee. However, sim-
ply running a weighted TCP-like algorithm, such as Seawall, did
not provide good results in terms of respecting guarantees. Un-
like a traditional TCP-like algorithm, the rate in our case does not
drop below an absolute limit given by the guarantee. When there
are many VM-to-VM ﬂows competing for bandwidth on a fully
reserved link, ﬂows would be too aggressive in poking their rate
above their guarantee. For example, the rate of some VM-to-VM
ﬂows would raise above the guarantee far in the convex part of the
rate increase, which would hurt the other ﬂows.
In practice, there is a tradeoff between accurately providing
bandwidth guarantees and being work conserving. This is partic-
ularly true since we do not rely on any switch support, and our
method of detecting congestion is through packet drops. In order
for ﬂows to detect whether there is available bandwidth in the net-
work, they must probe the bandwidth by increasing their rate. How-
ever, when the entire bandwidth is reserved through guarantees and
all VMs are active, this probing affects the rest of the guarantees.
In ElasticSwitch we design the RA algorithm to prioritize the
goal of providing guarantees, even under extreme conditions, in-
stead of being more efﬁcient at using spare bandwidth. As we show
in the evaluation, ElasticSwitch can be tuned to be more aggres-
sive and better utilize spare bandwidth, at the expense of a graceful
degradation in its accuracy in providing guarantees.
Improved rate allocation algorithm: When many VM-to-VM
ﬂows compete on a fully reserved link, even a small increase in the
rate of each ﬂow can affect the guarantees of the other ﬂows. This
effect is further ampliﬁed by protocols such as TCP, which react
badly to congestion, e.g., by halving their rate. Thus, the algorithm
must not be aggressive in increasing its rate.
Starting from this observation, we devised three improvements
for the rate-adaptation algorithm, which are key to our algorithm’s
ability to provide guarantees:
1. Headroom: There is a strictly positive gap between the link
capacity and the maximum offered guarantees on any link.
Our current implementation uses a 10% gap.
2. Hold-Increase: After each congestion event for a VM-to-VM
ﬂow, we delay increasing the rate for a period inversely pro-
portional to the guarantee of that ﬂow.
3. Rate-Caution: The algorithm is less aggressive as a ﬂow’s
current rate increases above its guarantee.
4Seawall’s function is not cubic; the convex part is quadratic while
the shape of the concave part depends on the sampling frequency.
Hold-Increase: After each congestion event, the hypervisor man-
aging a VM-to-VM ﬂow with guarantee BX→Y reduces the ﬂow’s
rate based on the congestion feedback, and then holds that rate for
a period T X→Y before attempting to increase it. This period is
set inversely proportional to the guarantee, i.e., T X→Y ∝ 1
BX→Y .
Setting the delay inversely proportional to the guarantee ensures
that (i) all ﬂows in a stable state are expected to wait for the same
amount of time regardless of their guarantee, and (ii) the RA algo-
rithm still converges to rates proportional to guarantees.
Two ﬂows, X → Y and Z → T , with rates RX→Y and RZ→T
should experience congestion events in the ratio of Cratio =
X→Y
RZ→T . In a stable state, the RA algorithm ensures that rates of
R
X→Y
BZ→T .
ﬂows are in proportion to their guarantees: R
X→Y
Thus Cratio = B
BZ→T . Since the number of delay periods is pro-
portional to the number of congestion events and the duration of
each period is inversely proportional to the guarantees, both ﬂows
are expected to hold increasing rates for the same amount of time.
The delay is inversely proportional to the guarantee, rather than
to the rate, to allow the RA algorithm to converge. Assuming the
same two ﬂows as above, when the X → Y ﬂow gets more than
its fair share, i.e., RX→Y > RZ→T · B
BZ→T , the X → Y ﬂow is
expected to experience a larger number of packet losses. For this
reason, it will have more waiting periods and will be held for more
time than when at its fair rate. This will allow Z → T to catch up
towards its fair rate.
X→Y
RZ→T = B
X→Y
In cases when a large number of packets are lost, the rate of a
large ﬂow can be held static for a long time. For our prototype
and evaluation, we also implemented and tested a scheme where
the delay is computed in proportion to a logarithmic factor of the
congestion events instead of the linear factor described above. We
choose the base such that the holding periods are inversely propor-
tional to guarantees. This approach allows RA to recover faster
than the linear version after a large congestion event. However, for
the speciﬁc cases we have tested in our evaluation, both the linear
and the exponential decay algorithms achieved similar results, due
to absence of large congestion events.
Rate-Caution: If the RA algorithm without Rate-Caution would
increase the rate by amount V (there was no congestion in the net-
work), with Rate-Caution that value is:
V (cid:5) = V · max
1 − C
RX→Y − BX→Y
BX→Y
, Cmin
(cid:3)
(cid:4)
where C and Cmin are two constants. In other words, V (cid:5)
decreases
as the ﬂow’s current rate increases further above its guarantee. C
controls the amount of cautioning, e.g., if C=0.5 then the aggres-
siveness is halved when the rate is twice the guarantee. We use a
minimum value (Cmin) below which we do not reduce aggressive-
ness; this enables even VM-to-VM ﬂows with small guarantees to
fully utilize the available capacity.
Rate-Caution accelerates convergence to fairness compared to a
uniform aggressiveness. When two ﬂows are in the convergence
zone, they are equally aggressive. When one ﬂow is gaining more
than its fair share of the bandwidth, it is less aggressive than the
ﬂows getting less than their fair share, so they can catch up faster.
In this way, Rate-Caution allows new ﬂows to rapidly recover the
bandwidth used by the opportunistic ﬂows using the spare capacity.
The downside of Rate-Caution is lower utilization, since it takes
longer to ramp up the rate and utilize the entire capacity.
Alternatives: We have experimented with multiple versions of the
basic RA algorithm besides Seawall, and many algorithms achieved
similar results in our tests (e.g., instead of the concave-shaped sam-
356pling mechanism of Seawall we used a linear function or encoded
the absolute expected values for the cubic rate). In the end, we de-
cided to use the Seawall-based one for simplicity, brevity of expla-
nation, and since it has been successfully used by other researchers.
In the future, one could also extend RA to also take into account the
current round trip time (similar to TCP Vegas or TCP Nice [26]),
since latency can be a good indicator of congestion.
TCP Interaction: The period of applying RA is expected to be an
order of magnitude larger than datacenter round trip times (10s of
ms vs. 1 ms), so we do not expect RA to interact badly with TCP.
6.
IMPLEMENTATION
Y
We implemented ElasticSwitch as a user-level process that con-
trols tc rate-limiters in the Linux kernel and also controls a kernel
virtual switch. We use Open vSwitch [17], which is controlled via
the OpenFlow protocol. Our current implementation has ∼5700
lines of C++ code.
ElasticSwitch conﬁgures a tc rate-limiter for each pair of
source-destination VMs. For outgoing trafﬁc, we impose speciﬁc
limits; for incoming trafﬁc, we use the limiters only to measure
rates. We conﬁgured Open vSwitch to inform ElasticSwitch of new
ﬂows and ﬂow expirations.
ElasticSwitch uses UDP for control messages sent from destina-
tion hypervisors to source hypervisors. Given a sender VM X and
a receiver VM Y and corresponding hypervisors H(X) and H(Y ),
we have two types of control messages: (1) messages from H(Y )
to inform H(X) of the guarantee assigned for the ﬂow from X to
Y , BX→Y
, and (2) congestion feedback (packet drop counts) mes-
sages to inform H(X) that there is congestion from X to Y . In
our current setup, hypervisors exchange messages by intercepting
packets sent to VMs on a speciﬁc control port.
In order to detect congestion, we modiﬁed the kernel virtual
switch to add a sequence number for each packet sent towards a
given destination. We include this sequence number in the IPv4
Identiﬁcation header ﬁeld, a 16-bit ﬁeld normally used for assem-
bly of fragmented packets. We assume that no fragmentation oc-
curs in the network, which is typically the case for modern datacen-
ters. A gap in the sequence numbers causes the destination to detect
a congestion event and send a feedback message back to the source.
To avoid a large number of congestion feedback messages (and ker-
nel to userspace transfers) during high congestion periods, we im-
plemented a cache in the kernel to aggregate congestion events and
limit the number of messages (currently, we send at most one mes-
sage per destination every 0.5ms). This patch for Open vSwitch is
∼250 lines of C code.
We have also adapted ElasticSwitch to detect congestion using