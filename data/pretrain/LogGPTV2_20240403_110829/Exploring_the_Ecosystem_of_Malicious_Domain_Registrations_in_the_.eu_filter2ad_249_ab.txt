### Mismatch Analysis

Several factors contribute to the observed mismatch:

1. **Incomplete Blacklist Coverage**: Since blacklists are not exhaustive, it is expected that some domains in a campaign may not be included in the specific set of blacklists used.
2. **Non-Abused Domains**: Some campaign registrations may not have been used for malicious purposes (yet).
3. **False Positives**: The criteria used to identify campaigns might not be strict enough, leading to false positive matches.

**Figure 5** illustrates the percentage of registrations for each individual campaign that appear on a blacklist. Three campaigns (c05, c11, and c15) have less than 60% of their registrations blacklisted. In the following section, we validate the quality of the campaign selection criteria by examining the real false positive rate. A high false positive rate would suggest that the selection criteria are imprecise, while a very high true positive rate indicates that the criteria are more comprehensive in identifying malicious domains compared to blacklisting services.

### Transitive Attribution

To assess the prevalence of incomplete blacklists and non-active malicious domains, we examine the registrant data of false positives to find undeniable traces connecting them to malicious domains. We base this transitive attribution on phone numbers, which are uniquely assigned identifiers not used in our campaign selection criteria. If a registrant's phone number matches that of a blacklisted registration, we consider the domain part of the malicious campaign, assuming it has either not been abused yet or was not picked up by a blacklist. In total, 3,252 campaign domains are transitively considered malicious. As shown in yellow in **Figure 5**, 14 out of the 20 identified campaigns are completely validated.

#### Threats and Experiments

A potential threat to using phone numbers for identification arises if an attacker retrieves WHOIS information from a legitimate .eu domain and falsely uses it for their own registration. We conducted three small experiments to invalidate this scenario:

1. **Time Interval Measurement**: We measured the time interval between the registration of a transitively attributed domain and the associated blacklisted domain. For 2,058 domains, the malicious registration occurred within 60 seconds of the transitively attributed registration, making it highly unlikely for an attacker to observe and replicate the registration in such a short time.
2. **Prior Blacklisted Registrations**: We filtered out 965 domains registered after a prior registration with the same phone number was already blacklisted, as an attacker would likely avoid reusing tainted contact details.
3. **Phone Number Verification**: Using a phone number verification tool, we identified invalid phone numbers for 189 of the remaining 229 domains. We assume that a malicious actor would not use benign registrant details with an invalid phone number.

In conclusion, 3,212 (98.77%) of the transitively attributed domains show one of these three indicators, justifying the attribution.

### In-Depth Analysis of Campaign C15

After the transitive attribution step, 30.6% of the registrations in campaign c15 remain potential false positives. Further investigation revealed that all domain names in c15 are composed of concatenated Dutch words, often reused, indicating a limited dictionary was used. A native Dutch speaker segmented the unflagged domain names, and 396 were found to be exclusively constructed from Dutch words used in blacklisted domains, labeled as validated true positives. The remaining domain names had either one word segment in common (172 domains) or no common word at all (15 domains). Applying another iteration of the transitive attribution strategy, 147 registrations shared a phone number with previously validated registrations, reducing the potential false positives to just 40.

Interestingly, 95 out of the 98 registrant names in c15 can be generated using the Laravel Faker generator tool with the nl-NL language option.

### Manual Analysis of Remaining False Positives

The residual potential false positives in all campaigns were further investigated manually by querying DNS records, visiting websites, and searching blacklists and search engines. Only two additional domains were validated as true positives: one in campaign c04 identified as a phishing website by FortiGuard, and one in campaign c15 that sent unsolicited emails to a temporary email account.

### Summary of Validation

Of the 20,698 campaign registrations, 16,704 (80.73%) were flagged by blacklisting services, 3,252 (15.71%) were linked to malicious domains via transitive attribution, and 552 (2.67%) were manually validated as registered with malicious intent. This results in only 190 potential false positives (0.92%), indicating that the selection criteria are sufficiently accurate for a representative analysis of the malicious domain ecosystem.

### Insights into Malicious Campaigns

#### Abuse Indicators and Categories

Overall, 93.68% of blacklisted domains were associated with spam. Table 2 shows that all campaigns follow this distribution, except for c19, where nearly 28% are linked to botnet C&C servers. Spamhaus DBL and SURBL cover the largest number of domains, with a considerable overlap, but both are required for exhaustive coverage. Google Safe Browsing was not involved in flagging any domains, possibly focusing more on malware delivery.

#### Cross-Campaign Characteristics

Some campaigns share interesting characteristics, such as generating registrant email addresses from names with numerical suffixes (c03, c04, c20) or following clear character patterns with numerical suffixes (c05, c11). Another peculiarity is the discrepancy between the registrant's street address and country, with valid street addresses outside Europe combined with European countries (c07, c09, c13, c14), likely to confuse residential requirements for .eu domain registration.

#### Registration Process Not Fully Automated

Multiple indications suggest that the malicious registration process in some campaigns is not fully automated. Syndicates work during office hours and make human errors. Weekends see significantly fewer malicious registrations, and holiday periods (e.g., summer, Christmas) show drops in activity. Some campaigns (c11, c18) align with typical office hours, while others (c19) suggest automation, with registrations at midnight and 1 PM. Minor inconsistencies in registration details also indicate manual input or different validation rules applied by registrars.

### Conclusion

The campaign selection criteria resulted in a low false positive rate, providing strong evidence for the accuracy of the analysis and offering valuable insights into the malicious domain ecosystem.