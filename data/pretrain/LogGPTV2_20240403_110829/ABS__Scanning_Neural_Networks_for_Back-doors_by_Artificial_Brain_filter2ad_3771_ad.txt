−model .layers[l].neurons[n + 1 :](x)
i = 0
while i  max_mask_size then
cost + = wl ar дe · sum(mask)
∂t r iддer
∆t r iддer =
∆mask = ∂cost
∂mask
triддer = triддer − lr ∆t r iддer
mask = mask − lr ∆mask
∂cost
return triддer, mask
dependent and hence lacks a pattern. Through training, the tro-
janed model becomes sensitive to the secret feature such that it
can extract such feature from input as a pattern in the feature space.
Figure 14 illustrates the procedure. Given a benign input x (e.g., an
airplane image), an image transformation procedure F is applied to
produce τ(x) with the secret feature (e.g., the image after applying
the Gotham filter). Note that for different x, the pixel space muta-
tion introduced by F is different. The trojaned model C extracts
the feature as a pattern in the feature space in layer Lk (i.e., the
highlighted values), which further triggers the elevation effect of a
compromised neuron α and then the mis-classification. Essentially,
F is the trojan trigger we want to reverse engineer as any input
that undergoes the transformation F triggers the mis-classification.
We consider F a generative model that takes an initial input x and
plants the triggering feature. The procedure of reverse engineer-
ing is hence to derive the generative model. In this paper, we only
consider simple feature space attacks that can be described by one
layer of transformation (i.e., the simplest generative model). The
two filters belong to this kind. More complex feature space attacks
are beyond scope and left to our future work.
In lines 3-4 of Algorithm 2, vector triддer denotes the F function
we want to reverse engineer. At line 4, a new input is generated
by the multiplication of triддer with a vector that contains the
original input x, the maximum pooling of the input (e.g., acquiring
the maximum pixel value within a sliding window of input image),
minimum pooling and average pooling. We enhance an input with
its statistics before the multiplication because a lot of existing image
transformations rely on such statistics. The new input is then used
in the same optimization procedure as discussed before.
The generated triддer may induce substantial perturbation in
the pixel space. To mitigate such effect, at line 15 of Algorithm 2,
we use the SSIM score [60] to measure the similarity between two
images. SSIM score is between -1 and 1. The larger the SSIM value,
the more similar the two images are.
5 EVALUATION
We evaluate ABS on 177 trojaned models and 144 benign models
trained from 7 different model structures and 6 different datasets,
Model
50000
32x32x3
1000
2622
8
4
Age
USTS
GTSRB
43
CIFAR-10
10
35288
32x32x3
ImageNet
VGG-Face
Dataset
#Labels #Train Inputs Input Size
1,281,167 224x224x3
2,622,000 224x224x3
NiN
VGG
ResNet32
ResNet110
LeNet
NiN
VGG
ResNet110
VGG
VGG
26,580 227x227x3 3Conv+2FC
8,612 600x800x3 Fast RCNN
Table 1: Dataset and Model Statistics
#Params
#Layers
966,986
10
39,002,738
19
470,218
32
1,742,762
110
571,723
8
966,986
10
39,137,906
19
110
1,744,907
16 138,357,544
16 145,002,878
11,415,048
12
59,930,550
16
with different configurations, trojan attack methods, and trojan
trigger sizes. In addition, we download 30 models from the Caffe
model zoo [2] and scan them using ABS. Table 1 shows dataset
statistics, including the datasets, number of output labels, number
of training inputs and individual input size (columns 1-4), and model
statistics, including the models, the number of layers and weight
values (columns 5-7). The CIFAR-10 [31] dataset is an image dataset
used for object recognition. Its input space is 32x32x3. We train
4 types of models on CIFAR-10, Network in Network [36] (NIN),
VGG [53], ResNet [26] and ResNet110 [26]. Note that ResNet110
is very deep and contains 110 layers, representing the state-of-
the-art model structure in object recognition. Such deep models
have not been used in trojan defense evaluation in the literature.
The GTSRB [55] dataset is an image dataset used for traffic sign
recognition and its input space is 32x32x3. Similar to CIFAR-10, we
evaluate LeNet, Network in Network (NIN), VGG and ResNet110 on
this dataset. The ImageNet [20] dataset is a large dataset for object
recognition with 1000 labels and over 1.2 million images. Its input
space is 224x224x3. We use the VGG16 structure in this dataset.
The VGG-Face [47] dataset is used for face recognition. Its input
space is 224x224x3. We use the state-of-the-art face detection model
structure VGG16 in evaluation. The Age [33] dataset is used for age
recognition. Its input space is 227x227x3. We use the age detection
model structure from [33]. USTS [42] is another traffic sign dataset
but used for object detection. Since USTS is an object detection
dataset, the input space is 600x800x3. We use the state-of-the-art
object detection model Fast-RCNN [24] in evaluation. Experiments
were conducted on a server equipped with two Xeon E5-2667 CPU,
128 GB of RAM, 2 Tesla K40c GPU and 6 TITAN GPU.
5.1 Detection Effectiveness
Experiment Setup. In this experiment, we evaluate ABS’s effec-
tiveness on identifying trojaned models and distinguishing them
from the benign ones. We test it on the various types of trojan
attacks discussed in Section 2.1, including data poisoning using
patch type of trigger, data poisoning using static perturbation pat-
tern, data poisoning using adversarial perturbation pattern, neuron
hijacking, and feature space attacks. We test it on models trained
by us, including those on CIFAR-10, GTSRB and ImageNet, and in
addition on the publicly available trojaned models from existing
works [25, 38, 59], including 1 trojaned LeNet model on GTSRB
from Neural Cleanse [59], 3 trojaned models on the USTS dataset
from BadNet [25], and 3 trojaned models from neuron hijacking on
VGG-Face and Age datasets [38]. When we trojan our own models
on CIFAR-10, GTSRB and ImageNet, we design 9 different trojan
triggers shown in Figure 15, 7 are in the pixel space and 2 are in the
feature space. 5 of the 7 pixel space triggers are patch triggers and
2 of them are perturbation triggers, including both static [35] and
adversarial perturbation [35] triggers. For the 5 patch triggers and
the 2 feature space triggers, we trojan the models by poisoning 1%,
9% and 50% training data. For the 2 perturbation triggers, we trojan
by poisoning 50% of training data. Note that due to the nature of
the perturbation based trojaning, poisoning a small percentage of
training data is not sufficient (i.e., having low attack success rate).
Thus we have 3×7+2= 23 trojaned models for each combination
of model structure and dataset. ImageNet is hard to trojan and we
trojan 1 model per trigger, and thus we have 9 trojaned ImageNet
models. Since we use four model structures for CIFAR-10 and three
model structure for GTSRB, in total we trojan 23×3+23×4 + 9 =170
models. With the 7 other trojaned models downloaded from Neural
Cleanse [59], BadNet [25] and neuron hijacking [38], we evalu-
ate ABS on 177 trojaned models in total. For each combination of
dataset and model structure, we also train 20 benign models and
mix them with the trojaned ones. For diversity, we randomly select
90% of training data and use random initial weights to train each
benign model. Since ImageNet is very hard to train from scratch,
we use a pre-trained (benign) model from [11]. We also download 3
benign models from BadNet [25] and neuron hijacking [38]. Hence,
there are 20×3 + 20×4 + 4=144 benign models (as shown in column
3 of Table 2). As far as we know, most existing works on defending
trojan attacks (e.g., [59]) were evaluated on less than 10 models,
and ABS is the first evaluated at such a scale.
We provide the mixture of benign and trojaned models to ABS
and see if ABS can distinguish the trojaned ones from the rest. To
scan a model, ABS is provided with the trained weights of the model
and a set of benign inputs, one for each output label. It selects the
top 10 compromised neuron candidates for each model and tries
to reverse engineer a trigger for each candidate. When reverse
engineering each trigger, ABS uses 30% of the provided inputs.
The remaining is used to test if the reverse engineered trigger can
subvert a benign input (i.e., causes the model to mis-classify the
input to the target label). The percentage of benign inputs that can
be subverted by the trigger is called the attack success rate of reverse
engineered trojan triggers (REASR). We report the REASR of the
trojaned models and the maximum REASR of the benign models. If
they have a substantial gap, we say ABS is effective. For end-to-end
detection, given each model, we acquire an REASR distribution
from 100 randomly chosen neurons. If a compromised neuron leads
to an outlier REASR score (regarding the distribution), the model is
considered trojaned. Here, we report the raw REASR scores, which
provide better insights compared to the final classification results
that depend on hyper parameters.
Trojaned Model Detection Results. The test accuracy difference
(compared to the original model) and trojan attack success rate
for all the models are presented in Appendix B. Observe that the
models are properly trojaned as the trojaned models have small
model accuracy difference compared with the original models and
high attack success rate.
The detection results are shown in Table 2. The upper sub-table
presents the results for models trained by us. Columns 1 and 2 show
the dataset and model. Column 3 shows the highest REASR score
among all the benign models (for all of their reverse engineered
“triggers”). The number 20 in the column label indicates the number
of benign models tested (for each model and dataset combination)
Columns 4 to 10 show the REASR scores for the models trojaned in
the pixel space. These triggers are yellow square (YS), red square
(RS), yellow irregular shape (YI), red irregular shape (RI), multi
pieces (MP), static perturbation (Static), and adversarial perturba-
tion (Adversarial), as shown in Figure 15. The pixel space triggers
are enhanced/enlarged for presentation. Most of they occupy about
6% of the input area. We study the effect of different trigger size
in a later experiment. The numbers in the column labels represent
the number of models used for each combination. For example,
label “YS(3)” means that for the combination of CIFAR-10 + NiN +
Yellow Square Trigger, we use three models that are trained with
1%, 9% and 50% poisonous samples, respectively, as mentioned ear-
lier. Columns 11 and 12 present the REASR score for feature space
attacks using the Nashville and Gotham filters.
The lower sub-table presents the results for downloaded models
trojaned by others, with column 4 the LeNet models from [5], and
columns 5-7 the three models from [38], and the last three columns
the three models from [25]. The symbol ‘-’ means not available.
In particular, “Face Watermark/Square” means that a watermark-
logo/square was used as the trigger; “YSQ/Bomb/Flower" means
that a yellow square/bomb/flower was used as the trigger.
Observations. We have the following observations:
(1) ABS has very high REASR scores for almost all the trojaned
models, with majority 100% and the lowest 77% (for the combination
USTS + FastRCNN + Bomb). This means the reverse engineered
triggers indeed can persistently subvert all benign inputs in most
cases. Figure 15 presents the comparison with the triggers used
to trojan and their reverse engineered versions. They look very
similar. Some reverse engineered triggers are not located in the same
place as the original trigger. Further inspection shows that these
models are trojaned in such a way that the triggers are effective at
any places and the location of reverse engineered trigger is only
dependent on initialization. Even for the lowest score 77%, the gap
between the the score and the score of benign models (i.e., 5%) is
substantial, allowing ABS to separate the two kinds. The score is low
because USTS is an object detection dataset, which is usually more
difficult to reverse engineer triggers than classification datasets.
(2) The REASR scores for trojaned models are much higher than
the scores for benign models in most cases, suggesting ABS can
effectively distinguish trojaned models from the benign ones. There
appear to have a few exceptions. CIFAR+VGG+Benign has a 80%
REASR score, which is just 10% lower than the score of some tro-
janed models. However, recall that we report the maximum REASR
for benign models. Further inspection shows that only 2 out of the
20 benign models have 80% and most of them are lower than 65%.
Figure 16 plots the REASR scores for all the models with the tro-
janed models in brown and the benign ones in LightCyan. Observe
that the two camps can be effectively partitioned. We will explain
why a benign model can achieve a high REASR score later.
(3) ABS is consistently effective for most attack types, trigger
types, various models and datasets that we consider, demonstrating
(a) Triggers
(b) Reverse Engineered Triggers
Figure 15: Triggers and Reverse Engineered Triggers
(a) CIFAR
(b) GTSRB
Figure 16: REASR of Benign Models vs. Trojaned Models
its generality. Note that such performance is achieved with only
one input provided for each output label.
Internals of ABS. We show the statistics of ABS internal operation
in Table 3. Column 3 shows the compromised neurons identified
by ABS. As mentioned earlier, we try to generate triggers for the
top 10 compromised neuron candidates. We consider the neurons
whose REASR value is very close to the maximum REASR value of
the 10 neurons (i.e., difference smaller than 5%) the compromised
ones. Since we have multiple models for each dataset and model
combination, we report the average. Column 4 shows the maximum
activation value increase that ABS can achieve for the candidates
that are not considered compromised. We measure this value by
(va − vb)/(vb) where vb denotes the original neuron value and
va denotes the neuron value after applying the trigger reverse en-
gineered. Columns 5-6 show the maximum output activation (i.e.,
logits) before and after applying the trigger derived from the can-
didates that ABS considers uncompromised. Columns 7-9 present
the counter-part for the compromised neurons.
We have the following observations. (1) There may be multi-
ple compromised neurons. Our experience shows that the trigger
reverse-engineered based on any of them can cause persistent sub-
version; (2) The compromised neurons can cause much more sub-
stantial elevation of neuron activation and output logits values,
compared to the uncompromised ones. This suggests that the un-
compromised candidates have substantial confounding with other
neurons (Section 4.3). (3) The elevation by compromised neurons
for some cases (e.g., the model for the Age dataset) is not as substan-
tial as the others. Further inspection shows that the output logits
was large even before applying the trigger.
Explaining High REASR Score in Benign Models. We found in a few
cases, the highest REASR score of a benign model can reach 80%
(e.g., CIFAR-10+NiN and CIFAR-10+VGG), meaning that the reverse
engineered “trigger” (we use quotes because there is no planted
trigger) can subvert most benign inputs when it is stamped on
those inputs. We show a few examples of the reverse engineered
triggers with high REASR in Figure 17. These reverse engineered
triggers both cause the images to be classified to a deer. Note that
they resemble deer antlers. Further inspection shows that in the
0.00.51.0REASRBeinignTrojan00.51REASRBeinignTrojanDataset
Model
Benign (20)
CIFAR-10
GTSRB