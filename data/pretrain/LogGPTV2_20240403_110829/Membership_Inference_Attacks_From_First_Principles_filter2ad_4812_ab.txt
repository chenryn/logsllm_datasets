optimal generalization [2, 15].
c) Privacy-preserving training: The most widely de-
ployed technique to make neural networks private is to make
the learning process differentially private [10]. This can be
done in various ways—for example by modifying the SGD
algorithm [1, 64], or by aggregating results from a model
ensemble [50]. Independent from differential privacy based
defenses, there are other heuristic techniques (that is, without a
formal proof of privacy) that have been developed to improve
the privacy of machine learning models [26, 45]. Unfortu-
nately, many of these have been shown to be vulnerable to
more advanced forms of attack [6, 61].
d) Measuring training data privacy: Given a particular
training scheme, a ﬁnal direction of work aims to answer the
question “how much privacy does this scheme offer?” Existing
techniques often work by altering the training pipeline, either
by injecting outlier canaries [3], or using poisoning to search
for worst-case memorization [24, 47]. While these techniques
give increasingly strong measurements of a trained model’s
privacy,
they require modifying the training
pipeline creates an up-front cost to deployment. As a result, by
far the most common technique used to audit machine learning
models is to just use a membership inference attack. Existing
membership inference attack libraries (see, e.g., Murakonda
and Shokri [42], Song and Marn [63]) form the basis for most
production privacy analysis [63], and it is therefore critical that
they accurately assess the privacy of machine learning models.
the fact
that
III. MEMBERSHIP INFERENCE ATTACKS
The objective of a membership inference attack (MIA) [60]
is to predict if a speciﬁc training example was, or was not, used
as training data in a particular model. This makes MIAs the
simplest and most widely deployed attack for auditing training
data privacy. It is thus important that they can reliably succeed
at this task. This section formalizes the membership inference
attack security game (§III-A), and introduces our membership
inference evaluation methodology (§III-B).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
21898
A. Deﬁnitions
We deﬁne membership inference via a standard security
game inspired by Yeom et al. [70] and Jayaraman et al. [25].
Deﬁnition 1 (Membership inference security game). The game
proceeds between a challenger C and an adversary A:
1) The challenger samples a training dataset D ← D and
trains a model fθ ← T (D) on the dataset D.
2) The challenger ﬂips a bit b, and if b = 0, samples a
fresh challenge point from the distribution (x, y) ← D
(such that (x, y) /∈ D). Otherwise, the challenger selects
a point from the training set (x, y) ←$ D.
3) The challenger sends (x, y) to the adversary.
4) The adversary gets query access to the distribution D,
D,f (x, y).
and to the model fθ, and outputs a bit ˆb ← A
5) Output 1 if ˆb = b, and 0 otherwise.
For simplicity, we will write A(x, y) to denote the adver-
sary’s prediction on the sample (x, y) when the distribution D
and model f are clear from context.
Note that this game assumes that the adversary is given
access to the underlying training data distribution D; while
some attacks do not make use of this assumption [70], many
attacks require query-access to the distribution in order to
train “shadow models” [60] (as we will describe). The above
game also assumes that the adversary is given access to both
a training example and its ground-truth label.
Instead of outputting a “hard prediction”, all the attacks we
consider output a continuous conﬁdence score, which is then
thresholded to yield a membership prediction. That is,
A(x, y) = 1[A(cid:48)(x, y) > τ ]
where 1 is the indicator function, τ is some tunable decision
threshold, and A(cid:48) outputs a real-valued conﬁdence score.
A ﬁrst membership inference attack. For illustrative pur-
poses, we begin by considering a very simple membership
inference attack (due to Yeom et al. [70]). This attack relies
on the observation that, because machine learning models are
trained to minimize the loss of their training examples (see
Equation 1), examples with lower loss are on average more
likely to be members of the training data. Formally, the LOSS
membership inference attack deﬁnes
Aloss(x, y) = 1[−(cid:96)(f (x), y) > τ ] .
B. Evaluating membership inference attacks
Prior work lays out several strategies to determine the effec-
tiveness of a membership inference attack, i.e., how to measure
the adversary’s success in Deﬁnition 1. We now show that
existing evaluation methodologies fail to characterize whether
an attack succeeds at conﬁdently predicting membership. We
thus propose a more suitable evaluation procedure.
As a running example for the remainder of this section,
we train a standard CIFAR-10 [29] ResNet [19] to 92% test
accuracy by training it on half of the dataset (i.e., 25,000
examples)—leaving another 25,000 examples for evaluation
as non-members. While this dataset is not sensitive, it serves
as a strong baseline for understanding properties of machine
learning models in general. We train this model using standard
techniques to reduce overﬁtting, including weight decay [30],
train-time augmentations [7], and early stopping. As a result,
this model has only a 8% train-test accuracy gap.
Balanced Attack Accuracy. The simplest method to evaluate
attack efﬁcacy is through a standard “accuracy” metric that
measures how often an attack correctly predicts membership
on a balanced dataset of members and non-members [6, 18,
33, 46, 56, 60, 61, 66, 68, 70].
Deﬁnition 2. The balanced attack accuracy of a membership
inference attack A in Deﬁnition 1 is deﬁned as
D,f (x, y) = b].
Pr
x,y,f,b
[A
Even though balanced accuracy is used in many papers
to evaluate membership inference attacks, we argue that this
metric is inherently inadequate for multiple reasons:
is,
• Balanced accuracy is symmetric. That
the metric
assigns equal cost to false-positives and to false-negatives.
However, in practice, adversaries often only care about
one of these two sources of errors. For example, when
a membership inference attack is used in a training data
extraction attack [4], false negatives are benign (some
data will not be successfully extracted) whereas false-
positives directly reduce the utility of the attack.
• Balanced accuracy is an average-case metric, but this is
not what matters in security. Consider comparing two at-
tacks. Attack A perfectly targets a known subset of 0.1%
of users, but succeeds with a random 50% chance on the
rest. Attack B succeeds with 50.05% probability on any
given user. On average, these two attacks have the same
attack success rate (and thus the same balanced accuracy).
However, the second attack is practically useless, while
the ﬁrst attack is exceptionally potent.
We now illustrate how exactly these issues arise for the
simple LOSS attack described above. For our CIFAR-10
model, this attack’s balanced accuracy is 60%. This is (much)
better than random guessing, and so one might reasonably
conclude that the attack is useful and practically worrying.
that
they are members. Yet, on this subset,
However, this attack completely fails at conﬁdently identi-
fying any members! Let’s examine for the moment the 1%
of samples from the CIFAR-10 dataset with lowest losses
(cid:96)(f (x), y). These are the samples where the attack is most
conﬁdent
the
attack is only correct 48% of the time (worse than random
guessing). In contrast, for the 1% samples with highest loss
(conﬁdent non-members), the attack is correct 100% of the
time. Thus, the LOSS attack is actually a strong non-
membership inference attack, and is practically useless at
inferring membership. An attack with the symmetrical prop-
erty (i.e., the attack conﬁdently identiﬁes members, but not
non-members) is a much stronger attack on privacy, yet it
achieves the same balanced accuracy.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
31899
ROC Analysis. Instead of the balanced accuracy, we should
thus consider metrics that emphasize positive predictions (i.e.,
membership guesses) over negative (non-membership) predic-
tions. A natural choice is to consider the tradeoff between
the true-positive rate (TPR) and false-positive rate (FPR).
Intuitively, an attack should maximize the true-positive rate
(many members are identiﬁed), while incurring few false-
positives (incorrect membership guesses). We prefer this to a
precision/recall analysis because TPR/FPR is independent of
the (often unknown) prevalence of members in the population.
The TPR/FPR tradeoff is fully characterized by the Receiver
Operating Characteristic (ROC) curve, which compares the
attack’s TPR and FPR for all possible choices of the decision
threshold τ. In Figure 2a, we show the ROC curve for
the LOSS attack. The attack fails to achieve a TPR better
than random chance at any FPR below 20%—it is therefore
ineffective at conﬁdently breaching the privacy of its members.
(a) linear scale
(b) log scale
Fig. 2: ROC curve for the LOSS baseline membership infer-
ence attack, shown with both linear scaling (left), also and
log-log scaling (right) to emphasize the low-FPR regime.
Prior papers that do report ROC curves summarize them
by the AUC (Area Under the Curve) [20, 39, 43, 57, 68, 69].
However, as we can see from the curves above, the AUC is not
an appropriate measure of an attack’s efﬁcacy, since the AUC
averages over all false-positive rates, including high error rates
that are irrelevant for a practical attack. The TPR of an attack
when the FPR is above 50% is not meaningfully useful, yet
this regime accounts for more than half of its AUC score.
To illustrate, consider our hypothetical Attack A from
earlier that conﬁdently identiﬁes 0.1% of members, but makes
no conﬁdent predictions for any other samples. This attack
perfectly breaches the privacy of some members, but has an
AUC ≈ 51%—lower than the AUC of the weak LOSS attack.
True-Positive Rate at Low False-Positive Rates. Our recom-
mended evaluation of membership inference attacks is thus to
report an attack’s true-positive rate at low false-positive rates.
Prior work occasionally reports true-positive rates at mod-
erate false-positive rates (or reports precision/recall values that
can be converted into TPR/FPR rates if the prevalence is
known). For example, Shokri et al. [60] frequently reports that
the “recall is almost 1” however there is a meaningful FPR
difference between a recall of 1.0 and 0.999. Other works
consistently report precision/recall values, but for equivalent
false-positive rates between 3% and 40%, which we argue is
too high to be practically meaningful.
In this paper, we argue for studying the extremely low
false-positive regime. We do this by (1) reporting full ROC
curves in logarithmic scale (see Figure 2b); and (2) optionally
summarizing an attack’s success rate by reporting its TPR at
a ﬁxed low FPR (e.g., 0.001% or 0.1%). For example, the
LOSS attack achieves a TPR of 0% at an FPR of 0.1% (worse
than chance). While summarizing an attack’s performance
at a single choice of (low) FPR can be useful for quickly
comparing attack conﬁgurations, we encourage future work to
always also report full (log-scale) ROC curves as we do.
IV. THE LIKELIHOOD RATIO ATTACK (LIRA)
A. Membership inference as hypothesis testing
The game in Deﬁnition 1 requires the adversary to distin-
guish between two “worlds”: one where f is trained on a
randomly sampled dataset that contains a target point (x, y),
and one where f is not trained on (x, y). It is thus natural to
see a membership inference attack as performing a hypothesis
test to guess whether or not f was trained on (x, y).
We formalize this by considering two distributions over
models: Qin(x, y) = {f ← T (D ∪ {(x, y)}) | D ← D} is the
distribution of models trained on datasets containing (x, y),
and then Qout(x, y) = {f ← T (D\{(x, y)}) | D ← D}.
Given a model f and a target example (x, y), the adversary’s
task is to perform a hypothesis test that predicts if f was
sampled either from Qin or if it was sampled from Qout [59].
We perform this hypothesis test according to the Neyman-
Pearson lemma [48], which states that the best hypothesis test
at a ﬁxed false positive rate is obtained by thresholding the
Likelihood-ratio Test between the two hypotheses:
Λ(f ; x, y) =
,
(2)
p(f | Qin(x, y))
p(f | Qout(x, y))
where p(f | Qb(x, y)) is the probability density function over
f under the (ﬁxed) distribution of model parameters Qb(x, y).
Unfortunately the above test is intractable: even the distribu-
tions Qin and Qout are not analytically known. To simplify the
situation, we instead deﬁne ˜Qin and ˜Qout as the distributions
of losses on (x, y) for models either trained, or not trained,
on this example. Then, we can replace both probabilities in
Equation 2 with the easy-to-calculate quantity
p((cid:96)(f (x), y) | ˜Qin/out(x, y)).
This is now a likelihood test for a one-dimensional statistic,
which can be efﬁciently computed with query access to f.
Our attack follows the above intuition. We train several
“shadow models” in order to directly estimate the distribution
˜Qin/out. To minimize the number of shadow models neces-
sary, we assume ˜Qin/out is a Gaussian distribution, reducing
our attack to estimating just four parameters: the mean and
variance of each distribution. To run our inference attack on
any model f, we can compute its loss on (cid:96)(f (x), y), measure
the likelihood of this loss under each of the distributions ˜Qin
and ˜Qout, and return whichever is is more likely.
(3)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
41900
0.250.500.751.00FalsePositiveRate0.20.40.60.81.0TruePositiveRate10−510−310−1FalsePositiveRate10−510−310−1TruePositiveRateFig. 3: Some examples are easier to ﬁt than others, and some
have a larger separability between their losses when being
a member of the training set or not. We train 1024 models
on random subsets of CIFAR-10 and plot the losses for four
examples when the example is a member of the training set
( ˜Qin(x, y), in red) or not ( ˜Qout(x, y), in blue).
B. Memorization and per-example hardness
By casting membership inference as a Likelihood-ratio test,
it becomes clear why the LOSS attack (and those that build
on it) are ineffective: by directly thresholding the quantity
(cid:96)(f (x), y), this attack implicitly assumes that the losses of
all examples are a priori on an equal scale, and that the
inclusion or exclusion of one example will have a similar effect
on the model as any other example. That is, if we measure
(cid:96)(f (x), y) < (cid:96)(f (x(cid:48)), y(cid:48)) then the LOSS attack predicts that
(x, y) is more likely to be a member than (x(cid:48), y(cid:48))—regardless
of any other properties of these examples.
Feldman and Zhang [15] show that not all examples are
equal: some examples (“outliers”) have an outsized effect
on the learned model when inserted into a training dataset,
compared to other (“inlier”) examples. To replicate their
experiment, we choose a training dataset D and sample a
random subset Din ⊂ D containing half of the dataset. We
train a model on this dataset f ← T (Din), and evaluate the
loss on every example (x, y) ∈ D, annotated by whether or
not (x, y) was in the training set Din. We repeat the above
experiment hundreds of times, thereby empirically estimating