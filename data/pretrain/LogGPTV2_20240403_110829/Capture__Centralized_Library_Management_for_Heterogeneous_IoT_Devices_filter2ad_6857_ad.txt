its other hosts from exploiting them. This security protection
is contingent on vendor adoptions and properly implemented
driver software.
Internal Threats. We consider internal threats which in-
clude compromised devices, drivers, and other devices within
the WiFi range. Capture prevents compromised local devices
from attacking other Virtual Device Entities (VDEs) through
network isolation since these devices are conﬁned to their
VDE and cannot reach any other hosts directly. Similarly, a
compromised driver is also isolated from other VDE drivers
using our network and other resource isolation mechanisms
(Mandatory Access Control, cgroups) mentioned above. In
Capture, drivers communicate with their associated device us-
ing our library runtime, which requires developers to specify
the message format between the device ﬁrmware and driver.
This design prohibits compromised drivers from sensing arbi-
trary packets to their associated devices and affecting them.
Furthermore, drivers cannot communicate with other VDEs
on the hub due to our resource isolation mechanisms.
Malicious devices (including Capture-incompatible local
devices) can not learn about other VDE’s network credentials
simply by eavesdropping on the setup network. Although the
setup network is a WPA2-Personal AP with shared password
credentials, each device actually has its own PTK (pairwise
transient key) through WPA2 4-way handshake [43,75]. How-
ever, link-layer encryption provided by WPA2 is insufﬁcient
for Capture’s network isolation because all drivers will run in
the same application layer on the hub. Therefore, we generate
a unique network interface and VLAN for each VDE during
the bootstrap process (Figure 6).
An adversary could potentially impersonate the Capture
Hub and perform man-in-the-middle attacks during new de-
vice bootstraps (Figure 6). This threat can be mitigated by
using certiﬁcates and public key infrastructure for devices to
verify the hub’s identity, or other novel device pairing and
initialization techniques [32, 65]. We did not implement these
features in our prototype since our current threat model fo-
cuses on attacks from vulnerable third-party libraries (Sec-
tion 4.1).
5 Integration Approaches
Device
App
MQTT
1
& HTTP
SmartThings
Device SDK
LAN
SmartThing
Hub
Vendor
Cloud
2a
MQTT
2b
HTTP
(a) Current deployment requires SmartThings Hub for networking.
App
Device
Hub
1
MQTT & HTTP
Capture-Enabled
ST-Device SDK
MQTT
2
& HTTP
ST-Device Driver
Capture Driver
Library
LAN
Vendor
Cloud
3
HTTP
(b) Capture-enabled SmartThings devices move all network commu-
nication onto the device drivers at the central hub.
Figure 7: Integration using IoT framework SDK extension.
with the adoption while providing ﬂexibility to developers.
5.1 OS Library Replacement
The ﬁrst approach is to provide a Capture-enabled version
of standard OS libraries. Take the OS networking library in
ESP32 platform, WiFi.h, for example. Devices use APIs from
this library to connect access points, maintain web servers, and
communicate over sockets. We provided a fully-compatible
Capture-enabled library, named as CaptureWiFi.h. Devel-
opers just need to make minor changes to use Capture, such
as replacing the #include  statement and initial-
izing Capture global runtime. We provide a default Capture
driver on the hub, which acts as a proxy to relay network
trafﬁc. If the original device works as a webserver, we open
a public-facing server on the driver to forward trafﬁc and
restrict network trafﬁc between driver and device.
This approach is platform-dependent. We need custom im-
plementations for speciﬁc OS APIs and libraries. However,
this is a one-time effort that can then be used by device de-
velopers with minimal porting effort. For example, all of our
prototype apps use the same ESP32 modiﬁed library runtime.
5.2
IoT Framework SDK Extension
Similar to replacing OS APIs, our second approach is to ex-
tend the SDK of a popular IoT framework to support Capture.
IoT frameworks (e.g., Azure Sphere [48], Particle OS [53],
and Samsung SmartThings Device SDK (ST-SDK) [62]) pro-
vide rich functionalities to differentiate from standalone em-
bedded device OSes with limited networking APIs. For exam-
ple, Azure Sphere [48] and Particle DeviceOS [53] provide
APIs to communicate with their native cloud backends; Sam-
sung SmartThings Device SDK [62] offers local devices the
option of using the SmartThings Hub as an MQTT broker.
We propose three integration approaches for developers to
adopt Capture, motivated by current IoT development prac-
tices. Our goal is to provide paths of least resistance to help
In this case, the developers of the IoT frameworks can incor-
porate Capture by modifying their SDK implementation while
preserving existing functionality. As a proof of concept, we
USENIX Association
30th USENIX Security Symposium    4195
added Capture support into the ST-SDK, which enables third-
party devices to use their SmartThings Hub. Figure 7a shows
how an example device would integrate with the ST-SDK, sim-
ilar to a custom OS library. A locally installed SmartThings
Hub (ST-Hub) provides functions such as MQTT brokers,
which device developers can directly invoke using ST-SDK
APIs. A device-side library manages the underlying connec-
tions with the ST-Hub. We develop a Capture-augmented ST-
SDK library (Figure 7b), so that device developers only need
to switch their ST-SDK library runtime without modifying
their application. Since the SmartThings Hub is proprietary,
we were only able to re-create their known functions such
as MQTT brokers using corresponding open source versions.
We provide a default SmartThings-compatible driver to mimic
the ST-Hub operations in Capture.
5.3 Native Driver Development
The two prior approaches provide default drivers on the Cap-
ture Hub to aid developer adoption. As a complementary
approach, we developed a Capture Native Driver SDK, for
developers to implement their own custom drivers with much
more ﬂexibility. To motivate this, consider an IoT device with
a web server. Using the previous approaches, the default driver
on our hub will create another public accessible web server
for new connections, and relay incoming client connections
to the device local-only web server. However, this may cause
unnecessary latency to serve the web request since both in-
bound and outbound trafﬁc has to go through the hub and
processed by two webservers. To address this, we propose the
Capture Native Driver SDK for developers looking to build
customized drivers. Developers can use our SDK APIs to
access security and networking functions on the Capture Hub,
and even ofﬂoad some computation to the hub.
6 Implementation
6.1 Core Hub Functionality
We implement the Capture Hub using a Raspberry Pi 3B+
with Linux in 1874 lines of C++ (https://github.com/
synergylabs/iot-capture). We use the TOMOYO Linux
security module [68] and iptables to implement the Vir-
tual Device Entity based isolation mechanisms. Our hub uses
hostapd [37] to manage WPA2 Personal and Enterprise WiFi
APs. The main Capture program listens for new connections
on the setup network, and upon request, creates a new VDE
for the incoming device, allocates a new VLAN subnet with
fresh RADIUS credentials, launches the corresponding driver
program based on the device type, and updates the TOMOYO
and iptables rulesets accordingly. The main program stores
all metadata for each VDE locally. While our current proto-
type does not address device removal, this functionality can
be added in a straightforward manner.
Optimizations. Existing applications often use blocking
network calls. During prototype development, we observed a
pathological case wherein the application only communicated
using one sequential byte at a time. Clearly, adapting such ap-
plications into Capture introduces a signiﬁcant performance
penalty, as each read request will incur one round of commu-
nication with the driver residing on the hub. We found that
without correction, this can lead to a 9.56x latency penalty for
the simple Web Server app (listed in Section 6.2).
The ﬁrst optimization we perform to address this issue is
to introduce read and write buffers on the device. When an
Internet host sends data to the driver, the payload is forwarded
to the local device in batch. Subsequent read calls from the
device will just retrieve the payload from the local buffer.
Similarly, using write buffers enables network writes to be
non-blocking I/Os, aggregating multiple payloads into chunks
in one round of driver communication. We found that this
reduces the latency penalty for the Web Server app from
9.56x to 1.62x, largely due to the reduced number of round
trips to the hub.
Although the previous approach reduces average latency
overhead to an acceptable 1.62x, it still incurs a median in-
crease of 31 ms. We were able to attribute this to the poor
wireless performance on the budget-oriented ESP32 micro-
controller, where a single packet transmission can take up to 6
milliseconds. To reduce the total number of packets sent, we
extended the protocol header ﬁelds and aggressively coalesce
small packets throughout our protocol. One concrete example
is proactively loading read buffers after accepting new clients,
where previously the device needs to send two messages to
check client status and fetch data to read, respectively.
Applying protocol optimizations and message coalescing
bring down the median latency overhead to 1.2x (+10 ms),
using the Web Server’s baseline performance as a reference.
Given that the ESP32 takes 5-6 ms to send a single packet,
this approaches the limit of what can be done without better
hardware. Detailed results are discussed in Section 7.1.
6.2 Benchmark Applications
To evaluate Capture, and explore different approaches for
integrating apps, we developed 9 prototype applications (Ta-
ble 4), including smart devices, Linux applications, and IoT
frameworks, and 3 IFTTT automation applets for benchmark-
ing (Table 5). Capture provides runtime libraries for device
ﬁrmware and drivers to handle network setup and communi-
cation with the hub. The device-side library was implemented
in 1335 lines of C++ code while the driver-side library varies.
Prototype Apps. We collected 6 open source applications
from popular online forums and tutorials [23, 31, 71], and
adapted them to use Capture. We chose the Espressif ESP32
platform given its reported popularity [1, 2] and use in hun-
dreds of millions of IoT devices [25]. We implemented a
4196    30th USENIX Security Symposium
USENIX Association
Abbreviation
WEB
CAM
SM
CP
WS
TH
ST-L
ST-S
MM
App Name
Web Server
Camera
Servo Motor
Color Picker
Weather Station
Temperature & Humidity
Platform
ESP32
ESP32
ESP32
ESP32
ESP23
ESP32
Standard web server to display and manage GPIO on/off status.
Description
Stream live video, take pictures.
Adjust the speed of a servo motor.
Change the color of LED light bulb.
Monitor weather with a BME sensor.
Display temperature and humidity data from DHT sensor.
SmartThings Lamp
SmartThings Switch
SmartThings
SmartThings
Subscribe to MQTT broker to receive on/off message.
Publish to MQTT broker to issue on/off message.
MagicMirror
Linux
Smart mirror display with online data such as news and weather.
Table 4: Prototype applications and descriptions.
generic default driver to support the OS Replacement ap-
proach, which required 166 lines of Python.
IoT Framework. We extended the Samsung SmartThings
Device SDK (ST-SDK) [62] to showcase integrating Capture
with existing frameworks (Section 5.2). ST-SDK is open-
source, whereas other proprietary alternatives (e.g., Azure
Sphere and Particle OS) raise challenges for replication and
comparison. Capture-enabled devices cannot work directly
with unmodiﬁed SmartThing Hub, so we analyzed ST-SDK’s
codebase and replicated its functionality with a driver that
executes on the Capture hub. We then adapted sample appli-
cations provided by ST-SDK [63] into Capture.
Linux apps. Some IoT devices are powerful enough to
run a Linux OS and applications (Section 3), so we adapted
Linux smart devices into Capture to demonstrate its capa-
bility. We selected MagicMirror, a project with over 12K
Github stars [47], that uses Raspberry Pi with a display to
function as a smart mirror, displaying custom content (e.g.
news and weather). Internally, the app includes a webserver
and a browser to display the webpage. We migrated Mag-
icMirror into a Capture prototype using the custom driver
integration (Section 5.3) and separated the server component
to the driver on the hub, keeping the display parts on the
ﬁrmware.
Automation Applets. To better measure Capture’s macro-
benchmark performance impact on real-world scenarios,
we implemented several home automation applets devel-
oped for IFTTT [38]. Prior work [46] categorized IFTTT
applets by trigger-action service types (Device⇒WebApp,
WebApp⇒Device, Device⇒Device) and reported an aver-
age execution latency of several seconds. We implemented
Capture-enabled devices for all three trigger-action service
types (Table 5), using the Web Server app (c.f. Table 4, WEB)
on ESP32 in place of physical lights and switches, since it
can control GPIO pins. Since ESP32 boards are lower perfor-
mance and slow at performing SSL encryption, integrating
these devices into Capture often improves performance due
to our hub hardware being more capable. To provide a fair
comparison, we also implement “mock” lights and switches
directly on the Raspberry Pi and measure the latency impact
from Capture integration as well.
7 Evaluation
Our evaluation aims to answer three primary questions.
• How much performance overhead do key device func-
tionalities incur on Capture versus their native platform,
and is the amount tolerable for typical home use?
• Can the Capture Hub scale to home deployments with
hundreds of devices in the near future, and how many
devices can our prototype reliably support at once?
• Roughly how much effort is required to port existing IoT
devices to Capture, and do the integration approaches in
Section 5 entail meaningful differences in the effort?
Our experiments were performed in a laboratory setting on
9 prototype devices (Table 4) and 3 IFTTT automation ap-
plets (Table 5). We use one Raspberry Pi 3 B+ as the Capture
Hub and another Raspberry Pi and multiple ESP32 boards
for prototype apps. Our evaluation results show that Capture
typically incurs low overhead (15% latency increase, 10% de-
vice resource utilization), insigniﬁcant impact on applets from
real-world automation platforms, and can support hundreds
of devices for a single Capture Hub.
7.1 Performance Overhead
Setup. We compare the performance of apps running on
Capture to that achieved by their original implementations.
Because many IoT devices and automation apps are event-
driven, they usually transmit a small amount of trafﬁc but
are sensitive to delays in latency. We categorize prototype
apps (Table 4) into two categories: latency-sensitive and
throughput-sensitive. We measure application-layer latency
for all of them, but only measure the throughput reduction
for the second group (such as a streaming camera). For most
USENIX Association
30th USENIX Security Symposium    4197
applications, we use Apache JMeter [6] to benchmark av-
erage and median latency for 500 HTTP requests. For the
streaming camera (CAM), we measure the video latency by
pointing the camera towards a millisecond clock and calculate
average delays from 50 readings. For the SmartThings apps
(ST-L and ST-S), we add instrumentation to send a notiﬁcation
packet to the hub so that we can calculate the time duration
between the ﬁrst MQTT message and the ﬁnal notiﬁcation
from Wireshark’s packet capture history. Finally, we measure
the ﬁrmware code size and memory utilization on the device.
Simple Integration with All Apps. We aim to conserva-
tively estimate Capture’s performance impact assuming mini-
mal burden on the developer. Hence, we ﬁrst try to integrate
apps with either OS or SDK replacements, since these re-
quire minimal modiﬁcations by the developer. If this attempt
fails (for example, the app requires features not supported
by our current prototype), we develop simple native drivers
without spending too much engineering effort on app-speciﬁc
optimizations.
Figure 8a shows the normalized latency for integrated apps.
On average, apps experience a 15% latency increase due to
the extra processing by the drivers on the hub. The baseline
apps for the comparison process everything on the device
and communicate directly with external hosts. After Capture
integration, external hosts need the drivers on the hub acting
as a proxy. For example, the camera streaming app driver
needs to retrieve the raw footage from the device and forward
it to the viewers. These extra steps introduce overhead to
the end application. However, as Figure 8a shows, most apps
experience a modest latency change between −34 ms and
+23 ms. Given most apps’ event-driven nature, this minor
increase in absolute latency should not impact the quality of
services for end applications. CAM app experiences the most
substantial latency increase, increasing from 523 ms to an
average of 820 ms (+297 ms), and a 40% FPS throughput
reduction. However, the relative increase (1.6x) is on par with
other apps. Since the baseline latency is very high, we believe
the original app is not designed to be real-time for ESP32,
and thus we did not further optimize its driver.
Several of the apps integrated with OS-Replacement see
improved average latency results. This is because Capture-
integrated apps perform more consistently, while the ESP32-
only baselines occasionally experience latency spikes (thus
having higher average results). Median results are more ro-
bust against outliers, and conﬁrm Capture often increases
latency slightly. The overall results show that Capture offers
comparable performance to the baseline for most requests.
We measure the throughput overhead for several
throughput-sensitive apps and report results in Figure 8b. For
throughput metrics, we choose FPS for streaming, packet
transfer rates for taking pictures, and full web page load time
for the complex MagicMirror dashboard. The Camera app
has a modest throughput reduction of around 40%. We ob-
serve no throughout drop for the Linux-based MagicMirror
benchmark. Figure 8c shows that the Capture ﬁrmware is, on