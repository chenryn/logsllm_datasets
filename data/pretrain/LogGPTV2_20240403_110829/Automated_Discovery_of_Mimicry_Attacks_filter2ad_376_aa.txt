# Automated Discovery of Mimicry Attacks

**Authors:**
- Jonathon T. Giffin
- Somesh Jha
- Barton P. Miller

**Affiliation:**
- Computer Sciences Department, University of Wisconsin
- Email: {giffin, jha, bart}@cs.wisc.edu

## Abstract

Model-based anomaly detection systems restrict program execution based on a predefined model of allowed system call sequences. These systems are effective only if they can detect actual attacks. Previous research has manually constructed mimicry and evasion attacks that avoid detection by embedding malicious system calls within valid sequences. Our work automates the discovery of such attacks. We use two models: a program model of the application's system call behavior and a model of security-critical operating system (OS) state. Given unsafe OS state configurations that describe the goals of an attack, we find system call sequences that are allowed by the program model but produce the unsafe configurations. Our experiments show that we can automatically find attack sequences in models of programs like wu-ftpd and passwd, which were previously discovered manually. When undetected attacks are present, our method often finds the sequences in less than 2 seconds.

**Keywords:**
- IDS evaluation
- Model checking
- Attacks
- Model-based anomaly detection

## 1. Introduction

A model-based anomaly detector restricts program execution to a predefined model of acceptable behavior [6,8,12,14,19,23]. The system compares the sequence of system calls generated by the executing program against the model. Any deviation from the model is classified as malicious and indicative of a program exploit. The effectiveness of the model in detecting attacks depends on the assumption that attacks always differ from valid execution.

An attack accepted by the model as valid will not be detected (Figure 1). Mimicry and evasion attacks avoid detection by transforming an attack sequence so that it appears normal to the program model while still performing the malicious action. Previous research has found examples of mimicry attacks against high-privilege processes restricted by a model-based detector [24, 21, 22, 20]. However, these attacks were constructed manually, which is not a scalable approach for general attack discovery.

This paper automates the discovery of mimicry attacks. Our goal is not to propose a new detection system but to evaluate an existing program model's ability to detect attacks. We address two primary questions:
- What attacks does a program model fail to detect?
- What attacks can we prove that a model will always detect?

Finding missed attacks reveals the weaknesses of a program model and indicates insufficient security. Conversely, proving that a model always detects an attack suggests strong security, even when an attacker tries to hide the attack within legitimate execution.

An attack is any sequence of system calls that produces a malicious change to the OS. Attackers can create variations of an attack sequence with the same effect by inserting extraneous system calls or replacing calls with alternatives. A model that detects one sequence may allow another, obfuscated sequence. We must verify that a model detects all variants.

We use a novel formalism that does not require knowledge of specific attack sequences or obfuscations. We develop a model of the OS with respect to its security-critical state and characterize attacks by their effect on the OS. This leverages the insight that obfuscated attack sequences are semantically equivalent in their malicious effect. Although we manually produce the OS model and definitions of malicious OS state, this is a one-time effort reused for subsequent analyses of all models of programs on that OS.

The program model specifies allowed system call sequences. By specifying how each system call transforms the OS state variables, we compute the set of reachable OS configurations. We apply model checking [4] to prove that no reachable configuration corresponds to an attack. If the proof fails, a system call sequence allowed by the model produces the malicious effect. The model checker reports this sequence as a counter-example, providing an undetected attack as output.

This approach automates the previous manual effort of finding mimicry attacks. In experiments, we show that we can automatically discover the mimicry attack against the Stide [8] model for wu-ftpd [24] and the evasion attacks against the Stide models for passwd, restore, and traceroute [21,20,22]. The model checking process completes in about 2 seconds or less when undetected attacks are present. When a model detects an attack, the model checking algorithm reports that no attack sequence could be found, requiring exhaustive search and completing in 75 seconds or less for all detected attacks.

Our work addresses outstanding problems in model-based anomaly detection. We provide a method for model evaluation that exhaustively searches for sequences of system calls allowed as valid by a program model but that induce a malicious configuration of OS state. Although our current work evaluates the context-insensitive Stide model, our system can evaluate any program model expressible as a context-sensitive pushdown automaton (PDA). One of our long-term goals is to compare the detection capabilities of different model designs proposed in the literature.

In summary, this paper makes the following contributions:
- **Automated discovery of mimicry attacks:** We use model checking to find sequences of system calls accepted as valid by a program model but that have malicious effects on the OS. Our system produces the exact sequences of system calls, with arguments, that comprise the undetected attacks.
- **A system design where attack sequences and obfuscations need not be known:** Our system does not require knowledge of attack system call sequences or obfuscations. It automatically finds new, unknown attack sequences and the obfuscations used by attackers to hide them within legitimate sequences.

## 2. Related Work

Previous research on mimicry [24, 9] and evasion attacks [22, 21, 20] demonstrated a critical shortcoming of model-based anomaly detection. Attackers can avoid detection by altering their attacks to appear as normal execution. These altered attacks are sequences of system calls allowed by a program model but still cause malicious execution. Previous work constructed mimicry and evasion attacks by converting a detected attack sequence \( A \) into an equivalent undetected sequence \( A' \). If \( A \) and \( A' \) are semantically equivalent and \( A' \) is allowed by the program model, then \( A' \) is a successful, undetected attack.

Determining that a model expressed as a pushdown automaton accepts \( A' \) is a computable intersection operation provided that \( A' \) is regular. Finding a sequence \( A' \) to intersect is a manual, incomplete procedure with several drawbacks:
- The procedure requires known attack sequences \( A \).
- The equivalence of two system call sequences is not well-defined.
- There is no clear operational direction to find mimicry and evasion attacks automatically.

Our model evaluation takes a different approach. By defining attacks by their malicious effects on the system, our work is not restricted to known attack sequences or transformations. Attack sequences are not part of the input; instead, our system produces them as output. We define two system call sequences as equivalent if they produce the same malicious effect on the OS. This formalism provides the operational direction for automating the procedure of finding undetected attacks.

Previous attempts have been made to quantify a model's ability to detect attacks. Average branching factor (ABF) [23] calculates the average opportunity for an attacker to undetectably execute a malicious system call during a program's execution. A predefined partitioning divides system calls into "safe" and "potentially malicious" calls. The ABF is the sum of potentially malicious calls divided by the number of system call operations verified during execution. An extension, the average reachability measure (ARM) [10], similarly evaluates pushdown automaton models.

Although these measurements provide a numeric score for model comparisons, they do not clearly measure a model's ability to detect attacks. They do not effectively embody an attacker's abilities:
- An attacker may alter a program's execution to reach a portion of the model that admits an attack sequence.
- The ABF or ARM value depends on the benign execution path followed and thus on program input.
- Attacks are often comprised of a sequence of system calls, but these metrics look at each call in isolation.

Consequently, these metrics provide limited insight into a model's ability to detect attacks. Our work improves the evaluation of a program model's attack detection ability by decoupling the evaluation from a particular execution path and the need to describe malicious activity as unsafe system calls.

MOPS [3] is similar to our work in statically checking a program model to determine properties. Unlike our work, MOPS characterizes unsafe or attack behavior as regular expressions over system calls and requires a database of malicious system call patterns. MOPS syntactically matches unsafe system call sequences against a program model. Conversely, our system does not require known malicious system call sequences and automatically discovers them for the user.

Model checking is a generic technique used to verify properties of state transition systems. Bessen et al. [2] described how model checkers can verify safety properties expressed in linear-time temporal logic (LTL). They verified properties over annotated control-flow graphs. We analyze automatically constructed program models, and our model checking procedure automatically derives security properties of the model as it traverses the model's edges.

Guttman et al. used model checking to find violations of information-flow requirements in SELinux policies [13]. They modeled the SELinux policy enforcement engine and the ways information flows between processes via the file system. Our work focuses on verifying safety properties using an OS model where system calls alter OS state.

Ramakrishnan and Sekar [15] used model checking to find vulnerabilities in the interaction of multiple processes. They abstracted the file system and specified each program's execution as a file system transformer. Our work expands the system abstraction to include the entire OS, shifts the checked interface from process execution to system calls, and does not need to model communication channels between processes.

Walker et al. used formal proof techniques to verify properties of a specification of a UNIX security kernel [25]. They rigorously proved that the abstract specification matched the actual implementation. Due to the difficulty of producing proofs of correct specifications, little other research demonstrates that abstractions are accurate. We adopt a simpler approach: we produced our OS abstraction manually and have not proven it correct. Discovered attacks or proofs of the absence of attacks hold only with respect to the abstraction. A discovered attack can be validated by running the system call sequence against a sandboxed OS.