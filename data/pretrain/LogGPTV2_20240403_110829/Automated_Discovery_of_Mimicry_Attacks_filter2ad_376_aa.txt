title:Automated Discovery of Mimicry Attacks
author:Jonathon T. Giffin and
Somesh Jha and
Barton P. Miller
Automated Discovery of Mimicry Attacks
Jonathon T. Gifﬁn, Somesh Jha, and Barton P. Miller
Computer Sciences Department, University of Wisconsin
{giffin, jha, bart}@cs.wisc.edu
Abstract. Model-based anomaly detection systems restrict program execution
by a predeﬁned model of allowed system call sequences. These systems are
useful only if they detect actual attacks. Previous research developed manually-
constructed mimicry and evasion attacks that avoided detection by hiding a ma-
licious series of system calls within a valid sequence allowed by the model. Our
work helps to automate the discovery of such attacks. We start with two mod-
els: a program model of the application’s system call behavior and a model of
security-critical operating system state. Given unsafe OS state conﬁgurations that
describe the goals of an attack, we then ﬁnd system call sequences allowed as
valid execution by the program model that produce the unsafe conﬁgurations.
Our experiments show that we can automatically ﬁnd attack sequences in models
of programs such as wu-ftpd and passwd that previously have only been dis-
covered manually. When undetected attacks are present, we frequently ﬁnd the
sequences with less than 2 seconds of computation.
Keywords: IDS evaluation, model checking, attacks, model-based anomaly
detection.
1 Introduction
A model-based anomaly detector restricts allowed program execution by a predeﬁned
model of acceptable behavior [6,8,12,14,19,23]. These systems compare a sequence of
system calls generated by the executing program against the model. The detector clas-
siﬁes any system call sequence that deviates from the model as malicious and indicative
of a program exploit. The ability of the model to detect actual attacks depends upon the
implicit assumption that attacks always appear different than valid execution.
An attack that is accepted by the model as valid will not cause an anomaly and
will not be detected (Fig. 1). Mimicry and evasion attacks avoid detection by trans-
forming an attack sequence of system calls so that it is accepted by a program model
yet still carries out the same malicious action. Previous research found examples of
mimicry attacks against high-privilege processes restricted by a model-based detec-
tor [24, 21, 22, 20]. However, the attacks were constructed manually by iterating be-
tween an attack sequence and a program model until the attack could be made to appear
normal. Although these manually-constructed attacks served as a successful proof-of-
concept, manual approaches remain unsuitable as a general attack discovery strategy.
This paper automates the discovery of mimicry attacks. Our intent is not to propose
a new detection system but rather to provide the means to evaluate an existing program
model’s ability to detect attacks. We address two primary questions:
D. Zamboni and C. Kruegel (Eds.): RAID 2006, LNCS 4219, pp. 41–60, 2006.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006
42
J.T. Gifﬁn, S. Jha, and B.P. Miller
Σ∗
L(M)
Attacks
Detected attacks
Undetected attacks
Fig. 1. If Σ is the set of system calls, then Σ
is the inﬁnite set of all possible system call se-
quences. A program model M accepts a subset of system call sequences L(M) as valid program
execution. Any attack sequence accepted as valid is a missed attack.
∗
– What attacks does a program model fail to detect?
– What attacks can we prove that a model will always detect?
Finding missed attacks reveals the weaknesses of a program model and indicates that
a model-based detector provides insufﬁcient security for that particular program. Con-
versely, proving that a model always detects an attack establishes strong indications
that a computer system using model-based detection is secure, even when an attacker
attempts to hide an attack within legitimate execution.
An attack is any sequence of system calls that produces a malicious change to the
operating system (OS). For a given attack sequence, an attacker can produce variations
of the sequence having the same attack effect by inserting extraneous system calls into
the sequence or replacing existing system calls with alternative sequences having the
same effect. A program model that detects one sequence may allow a different, obfus-
cated sequence. The net result remains the same: the model fails to detect an attack. We
must verify that a model detects each of the attack variants.
We use a novel formalism that requires neither knowledge of particular attack se-
quences nor knowledge of particular obfuscations that try to hide those sequences from
a detector. We develop a model of an OS with respect to its security-critical state and
then characterize attacks only by their effect upon the OS. This leverages a key insight:
the commonality among the obfuscated attack sequences is that the sequences are se-
mantically equivalent with respect to their malicious effect upon the OS. Although we
manually produce the OS model and the deﬁnitions of malicious OS state, this is a one-
time effort that is reused for subsequent analyses of all models of programs executing
on that operating system.
The program model speciﬁes what sequences of system calls are allowed to exe-
cute. By specifying how each system call transforms the OS’s state variables, we are
able to compute the set of OS conﬁgurations reachable when a program’s execution
is constrained by the model. We apply model checking [4] to prove that no reachable
conﬁguration corresponds to the effect of an attack. If the proof fails, then some system
call sequence allowed by the model produces the malicious effect. The model checker
reports this sequence as a counter-example that caused the proof to fail, providing pre-
cisely an undetected attack as output. In terms of Fig. 1, we are ﬁnding system call
sequences contained in L(M)∩ Attacks without explicitly computing the set Attacks
of malicious system call sequences.
Automated Discovery of Mimicry Attacks
43
This approach automates the previous manual effort of ﬁnding mimicry attacks. In
experiments, we show that we can automatically discover the mimicry attack against the
Stide [8] model for wu-ftpd [24] and the evasion attacks against the Stide models for
passwd, restore, and traceroute [21,20,22]. The model checking process com-
pleted in about 2 seconds or less when undetected attacks were present in the models.
When a model is sufﬁciently strong to detect an attack, the model checking algorithm
will report that no attack sequence could be found. This requires exhaustive search and
completed in 75 seconds or less for all attacks detected by the models of the four test
programs. Note that proofs of successful detection hold only with respect to our abstrac-
tion of the OS state. If this abstraction is erroneous or incomplete, undetected attacks
may still be present when using the model to protect a complete operating system.
Our work addresses outstanding problems in model-based anomaly detection. We
provide a method for model evaluation that exhaustively searches for sequences of sys-
tem calls allowed as valid by a program model but that induce a malicious conﬁguration
of OS state. Although our current work evaluates the context-insensitive Stide model,
we have designed our system so that it can evaluate any program model expressible as
a context-sensitive pushdown automaton (PDA). One of our long-term goals, not yet
realized, is to compare the detection capabilities of different model designs proposed in
the literature.
In summary, this paper makes the following contributions:
– Automated discovery of mimicry attacks. We use model checking to ﬁnd sequences
of system calls accepted as valid by a program model but that have malicious effects
upon the operating system. Our system produces the exact sequences of system
calls, with arguments, that comprise the undetected attacks.
– A system design where attack sequences and obfuscations need not be known. Our
system does not require that attack system call sequences be known or enumerated.
In fact, we strive for the opposite: our system will automatically ﬁnd new, unknown
attack sequences accepted by a program model and will produce those sequences as
output. Likewise, we automatically ﬁnd the obfuscations used by attackers to hide
attack system calls within a legitimate sequence. As a result, our approach is not
limited by a priori knowledge of attacker behavior.
Section 2 presents related work in manual attack analysis. Section 3 gives an over-
view of our system. Section 4 describes the operating system abstraction and Sect. 5
explains how a model checker uses that abstraction to ﬁnd undetected attacks in a pro-
gram model. Section 6 presents the architecture of our implementation, and Sect. 7 uses
that implementation to demonstrate experimentally that we have automated the previ-
ously manual process of discovering undetected attacks.
2 Related Work
The seminal research on mimicry [24, 9] and evasion attacks [22, 21, 20] demonstrated
a critical shortcoming of model-based anomaly detection. Attackers can avoid detec-
tion by altering their attacks to appear as a program’s normal execution. These altered
attacks are sequences of system calls allowed by a program model but that still cause
44
J.T. Gifﬁn, S. Jha, and B.P. Miller
malicious execution. Previous work constructed mimicry and evasion attacks by con-
verting some detected attack system call sequence A into an equivalent undetected se-
(cid:2)
(cid:2)
(cid:2)
quence A
are semantically equivalent and A
. If A and A
is a sequence allowed by the
(cid:2)
is a successful, undetected attack.
program model, then A
(cid:2)
Determining that a model expressed as a pushdown automaton accepts A
is a com-
to
(cid:2)
putable intersection operation provided that A
intersect is a manual, incomplete procedure with several drawbacks:
(cid:2)
is regular; ﬁnding a sequence A
– The procedure requires known attack sequences A.
– The equivalence of two system call sequences is not well deﬁned. For example: an
may include legitimate execution behavior that is
(cid:2)
undetected attack sequence A
(cid:2)
irrelevant to the original attack sequence A. Are A and A
equivalent?
– There is no clear operational direction to ﬁnd mimicry and evasion attacks auto-
matically. Identifying two sequences as equivalent attacks was a manual procedure
based on intuition. There was no algorithmic process amenable to automation.
Our model evaluation takes a different approach that advances the state of the art. By
deﬁning attacks only by their malicious effects upon the system, our work is not re-
stricted to known attack sequences of system calls or known attack transformations
producing evasive attacks. Attack sequences are not part of the input to our system;
in fact, our work produces the sequences as its output. We can further deﬁne two sys-
tem call sequences as equivalent with respect to the attack if they produce the same
malicious effect upon the operating system. This formalism provides the operational
direction allowing our work to automate the procedure of ﬁnding undetected attacks.
Previous attempts have been made to quantify the ability of a model to detect attacks.
Average branching factor (ABF) [23] calculates, for any ﬁnite-state machine model, the
average opportunity for an attacker to undetectably execute a malicious system call dur-
ing a program’s execution. A predeﬁned partitioning divides the set of system calls into
“safe” calls and “potentially malicious” calls. As the runtime monitor follows paths
through the automaton in response to system calls executed by the program, it looks
ahead one transition to determine the number of potentially malicious calls that would
be allowed as the next operation. The average branching factor is then the sum of the
potentially malicious calls divided by the number of system call operations veriﬁed dur-
ing execution. An extension to average branching factor, called the average reachability
measure (ARM) [10], similarly evaluated pushdown automaton models.
Although these measurements provide a convenient numeric score enabling model
comparisons, they do not provide a clear measure of a model’s ability to actually detect
attacks. These metrics do not effectively embody an attacker’s abilities:
– An attacker may alter a program’s execution to reach a portion of the program
model that admits an attack sequence by ﬁrst passing through a sequence of safe
system calls. By only looking at the ﬁrst system call branching away from a benign
execution path, ABF and ARM fail to show the strength of one model over another.
– The ABF or ARM value computed depends upon the benign execution path fol-
lowed and hence upon program input. A complete evaluation of the model requires
computing the score along all possible execution paths. This is extremely challeng-
ing and itself forms an entire body of research in the program testing area.
Automated Discovery of Mimicry Attacks
45
– Attacks frequently are comprised of a sequence of system calls. The previous met-
rics look at each system call in isolation and have no way to characterize longer
attack sequences.
Consequently, these metrics provide limited insight into a model’s ability to detect at-
tacks. Our work improves the evaluation of a program model’s attack detection ability
by decoupling the evaluation from both a particular execution path and from the need
to describe malicious activity as unsafe system calls.
MOPS [3] is similar to our work in the ﬁrst aspect: it statically checks a program
model to determine properties of the model. Unlike our work, however, MOPS charac-
terizes unsafe or attack behavior as regular expressions over system calls and requires
users to provide a database of malicious system call patterns. Just as commercial virus
scanners syntactically match malicious byte sequences against program code, MOPS
syntactically matches unsafe system call sequences against a program model. Likewise,
when a new malicious behavior is discovered, the database of system call patterns must
be updated. Conversely, by understanding the semantics of system calls, the system
in our paper does not require known malicious system call sequences, and it in fact
automatically discovers them for the user. Our work is not tied to known patterns of
malicious system call execution.
Model checking is a generic technique used to verify properties of state transition
systems, and it has been applied previously to computer security. Bessen et al. [2] de-
scribed how model checkers can verify safety properties [16] expressed in linear-time
temporal logic (LTL). They veriﬁed the properties over annotated control-ﬂow graphs,
where both the graph and the annotations expressing security properties of the program
code came from some unspeciﬁed source. We analyze automatically constructed pro-
gram models, and our model checking procedure automatically derives security prop-
erties of the model as it traverses the model’s edges.
Guttman et al. used model checking to ﬁnd violations of information-ﬂow require-
ments in SELinux policies [13]. They modeled the SELinux policy enforcement engine
and the ways in which information may ﬂow between multiple processes via a ﬁle sys-
tem. They could then verify that any information ﬂow was mediated by a trusted process
on the system. Our work has a different goal: veriﬁcation of safety properties using an
OS model where system calls alter OS state.
Ramakrishnan and Sekar [15] used model checking to ﬁnd vulnerabilities in the in-
teraction of multiple processes. They abstracted the ﬁle system and speciﬁed each pro-
gram’s execution as a ﬁle system transformer. The program speciﬁcations were compli-
cated by the need to characterize interprocess communication. Our work expands the
system abstraction to include the entire operating system, shifts the checked interface
from coarse-grained process execution down to system calls, and has no need to model
communication channels between processes.
Walker et al. used formal proof techniques to verify properties of a speciﬁcation
of a UNIX security kernel [25]. This work is notable because the authors rigorously
proved that the abstract speciﬁcation of the kernel matched the actual implementation.
As a result, properties proved using the abstraction also hold true in the real operating
system. Due to the difﬁculty of producing proofs of correct speciﬁcations, little other
research actually demonstrates that abstractions are accurate. We adopt this simpler
46
J.T. Gifﬁn, S. Jha, and B.P. Miller
approach: we produced our operating system abstraction manually and have not proved
it correct. As a result, discovered attacks or proofs of the absence of attacks hold only
with respect to the abstraction. A discovered attack can be validated by actually running
the system call sequence against a sandboxed operating system. Conversely, if we do