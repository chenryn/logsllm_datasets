size to Turner et al. [74] and three times longer than Wu et al. [75].
We analyze two types of backbone network failures:
• Link failures, where an individual bundle of optical fiber linking
• Edge failures, where multiple link failures cause an edge to fail.
An edge connects to the backbone and Internet using at least
three links. When all of an edge’s links fail, the edge fails.
two edges (Figure 1, ➄) fails.
Our backbone network dataset does not contain root causes. We
measure mean time between failures (MTBF) and mean time to re-
covery (MTTR) for edges and links. We analyze edge reliability
(§6.1), link reliability by fiber vendor (§6.2), and edge reliability by
geographic location (§6.3).
6.1 Edge Reliability
• Typical edge failure rate is on the order of months
• Typical edge recovery rate is on the order of hours
Figure 15: MTBF as a function of percentage of edges con-
necting Facebook data centers with that MTBF or lower.
Edges exhibit high variance in MTBF due to their diverse fiber
vendor makeup and geographic locations (observations we explore
in §6.2 and §6.3). The standard deviation of edge MTBF is 1320 h,
with the least reliable edge failing on average once every 253 h and
the most reliable edge failing on average once every 8025 h.
We model MTBFedge(p) as an exponential function of the per-
centage of edges, 0 ≤ p ≤ 1, with that MTBF or lower. We built
the models in this section by fitting an exponential function using
the least squares method. At Facebook, we use these models in
capacity planning to calculate conditional risk, the likelihood of
edge or link being unavailable given a set of failures. We plan edge
and link capacity to tolerate the 99.99th percentile of conditional
risk. We find that MTBFedge(p) = 462.88e
2.3408p (the dotted line in
Figure 15) with R
MTTR. The solid line in Figure 16 plots edge MTTR in hours as a
function of the percentage of edges with that MTTR or lower. Edge
recovery occurs much faster than the time between outages because
edges contain multiple links (at least three) and fiber vendors work
to repair link failures rapidly. 50% of edges recover within 10 h of a
failure; 90% within 71 h.
2 = 0.94.
1001,00010,0000%25%50%75%100%Mean time between failures (hours)PercentileSeries1aEdge MTBFModelIMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
Figure 16: MTTR as a function of percentage of edges con-
necting Facebook data centers with that MTTR or lower.
Edges exhibit high variance in MTTR because some edges are
easier to repair than others. Imagine the differences between an
edge on a remote island compared to an edge in a big city. Weather
conditions, physical terrain, and travel time affect the time it takes
a fiber vendor to repair an edge’s links. The standard deviation of
edge MTTR is 112 h, with the slowest edge to recover taking on
average 608 h and the fastest edge to recover taking 1 h.
We model MTTRedge(p) as an exponential function of the per-
centage of edges, 0 ≤ p ≤ 1 with that MTTR or lower. We find
that MTTRedge(p) = 1.513e
4.256p (the dotted line in Figure 15) with
2 = 0.87.
The high variances in edge MTBF and MTTR motivate us to
study the reliability characteristics of the links connecting edges in
§6.2 and the geographic location of edges in §6.3.
R
6.2 Link Reliability by Fiber Vendor
• Typical vendor link failure rate is on the order of months
• Higher MTBF in high competition markets
• Vendor MTBF and MTTR each span multiple orders of magnitude
We next analyze the MTBF and MTTR for fiber vendors based on
when the links they operate fail or recover. For brevity, we shorten
“the MTBF/MTTR of the links operated by a fiber vendor” to “fiber
vendor MTBF/MTTR.”
MTBF. The solid line in Figure 17 plots the fiber vendor MTBF
in hours as a function of the percentage of fiber vendors with
that MTBF or lower. For most vendors, link failure happens only
occasionally due to regular maintenance and monitoring. 50% of
vendors have at least one link failure every 2326 h, or once every
3.2 months. And 90% of vendors have at least one link failure every
5709 h.
Figure 17: MTBF as a function of percentage of fiber vendors
with that MTBF or lower.
Fiber vendor MTBF varies by orders of magnitude. The standard
deviation of fiber vendor MTBF is 2207 h, with the least reliable ven-
dor’s links failing on average once every 2 h and the most reliable
vendor’s links failing on average once every 11 721 h. Anecdotally,
we observe that fiber markets with high competition lead to more
incentive for fiber vendors to increase reliability. For example, the
most reliable vendor operates in a big city in the USA.
MTTR. The solid line in Figure 18 plots fiber vendor MTTR as
a function of the percentage of fiber vendors with that MTTR or
lower. Most vendors repair links promptly. 50% of vendors repair
links within 13 h of a failure; 90% within 60 h.
Figure 18: MTTR as a function of percentage of fiber vendors
with that MTTR or lower.
Fiber vendors exhibit high variance in MTTR because some fiber
vendors operate in areas where they can more easily repair links
(an observation we analyze in §6.3). The standard deviation of fiber
vendor MTTR is 56 h, with the slowest vendor taking on average
744 h to repair their links and the most reliable vendor taking on
average 1 h to repair their links.
We model MTTRvendor(p) as an exponential function of the per-
centage of vendors, 0 ≤ p ≤ 1 with that MTTR or lower. We find
that MTTRvendor(p) = 1.1345e
4.7709p (the dotted line in Figure 18)
with R
2 = 0.98.
1101000%25%50%75%100%Mean time to recovery (hours)PercentileSeries1aEdge MTTRModel1101001,00010,0000%25%50%75%100%Mean time between failures (hours)PercentileSeries1aVendor MTBFModel11010010000%25%50%75%100%Mean time to recovery (hours)PercentileSeries1aVendor MTTRModelA Large Scale Study of Data Center Network Reliability
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
We conclude that not all fiber vendors operate equally. We next
explore how the geographic location of edges affects their reliability.
6.3 Edge Reliability by Geographic Location
• Edge failure rate is similar across most continents
• Edges recover within 1 d on average on all continents
We analyze last the reliability of edges by their geographic lo-
cation using the continent they reside on. Table 4 shows the dis-
tribution of edges in Facebook’s network among continents. Most
edges reside in North America, followed closely by Europe. The
continents with the fewest edges are Africa and Australia.
Table 4: The distribution and reliability of edges in Face-
book’s network among continents.
Continent
North America
Europe
Asia
South America
Africa
Australia
Distribution MTBF (hours) MTTR (hours)
17
19
11
9
22
2
1848
2029
2352
1579
5400
1642
37%
33%
14%
10%
4%
2%
MTBF. We show the average MTBF for the edges in each con-
tinent in Table 4. Edges in Africa are outliers, with an average
MTBF of 5400 h, or 7.4 months. Edge reliability in Africa is impor-
tant because edges in Africa are few and connect Europe and Asia.
Edges in North America, South America, Europe, Asia, and Aus-
tralia have average MTBFs ranging from 1579 h (2.2 months, for
South America) to 2352 h (3.2 months, for Asia).
MTTR. We show the average MTTR for the edges in each con-
tinent in Table 4. Across continents, edges recover within 1 d on
average. Edges in Africa, despite their long uptime, take the longest
time on average to recover at 22 h due to their submarine links.
Edges in Australia take the shortest time on average to recover at
2 h due to their locations in big cities. We observe a 7 h standard
deviation in edge MTTR among continents.
6.4 Inter Data Center Reliability Implications
Extending automated remediation to the edge. While Facebook
can use software automation to improve the reliability of networks
within our data centers, we see providing similar automation to the
backbone networks that connect our data centers as a key challenge.
Given that we often do not have full control of the devices along the
links that connect the edges, coming up with a standard protocol
and an automated procedure to enabling this type of interaction or
designing generic remediation systems are an important focus.
Coordinated understanding of shared backbone resources.
We reported on the reliability characteristics of the backbone links
that Facebook operates on, yet in many cases, Facebook is only one
of the many entities that share backbone link connectivity. Despite
the shared fate of these links, relatively little openly accessible data
is available on their reliability characteristics. While we have char-
acterized the links that Facebook operates on, we hope that our
work inspires other practitioners throughout the community to con-
tribute to the body of knowledge available on this less understood
aspect of web service operation.
7 RELATED WORK
To our knowledge, this paper provides the first comprehensive
study of network incidents from the perspective of large-scale web
services. Prior large-scale data center failure studies [8, 16, 35, 60]
report that network incidents are among the major causes of web
service outages; however, none of these studies systematically ana-
lyze network incidents at a large scale, focusing on the availability
of an entire web service, across both inter and intra data center
networks, in a long term, longitudinal study.
There are a number of prior studies examined the failure charac-
teristics of network links and devices in different types of networks
studied in this paper, including both data center networks [30, 62,
63, 77] and optical backbones [29, 51, 63]. Specially, Potharaju and
Jain [63] and Turner et al. [74] also studies data center network
infrastructure by characterizing device/link failures in both intra
and inter data center networks. Their studies also characterize the
failure impact, including connectivity losses, high latency, pack-
age drops, and so on. These studies significantly boost the under-
standing of network failure characteristics, and provide insights
for network engineers and operators to improve the fault tolerance
of existing networks and to design more robustness networks.
While our work is closely related to these prior studies, it is
also fundamentally different and complementary in the following
three aspects. First, our work has a different goal. Unlike these
prior studies that focus on understanding fine-grained per-device,
per-link failures and their impact to the system-level services above
the network stack, our work focuses on how network incidents
affect the availability of the Internet service. Our goal is to reveal and
quantify the incidents that cannot be tolerated despite industry best
practices, and shed light on how large scale systems operate reliably
in the face of these incidents. Second, prior studies only cover the
data center and backbone networks with traditional Clos-based ar-
chitectures, whereas our work presents a comparative study of the
reliability characteristics of data center network infrastructure with
both a traditional Clos-based design and a contemporary fabric-
based design with smaller, merchant-silicon-based switches. As
introduced in §3, we achieve this due to the heterogeneity of the
data center network infrastructure of Facebook where networks
with different designs co-exist and co-operate. Third, we present a
long-term (seven years for intra data center networks and eighteen
months for inter data center networks) longitudinal analysis to re-
veal the evolution of network reliability characteristics, while prior
studies typically provide only aggregated results, often over a much
shorter period or with orders of magnitude fewer switches [74].
Govindan et al. [33] study over 100 failure events at Google WAN
and data center networks, offering insights on why maintaining
high levels of availability for content providers is challenging. Their
study, similar to [8, 16, 35, 60], focuses on network management
and the design principles for building robust networks. Many of
the high-level design principles mentioned in [33], such as using
multiple layers of fallback (defense in depth), continuous prevention,
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
and recovering fast, are applicable for large-scale software systems
to protect against network incidents.
8 CONCLUSIONS
At Facebook, we strive to maintain a reliable network infrastructure
both within (intra) and between (inter) data centers. In this study,
we have characterized the network incidents we observe and their
behavior on the software systems that run on them. We have also
analyzed the backbone links that connect data centers and modeled
their reliability characteristics. Our analysis revealed several key
observations about the networks within and between data centers.
As software systems grow in complexity, interconnectedness,
and geographic distribution, unwanted behavior from network
infrastructure has the potential to become a key limiting factor in
the ability to reliably operate distributed software systems at a large
scale. To ameliorate the negative effects of network errors on service
reliability, we have highlighted several promising implications for
future network research.
We look forward to future research that sheds light on some of
these important directions. It is our hope that the research commu-
nity can build upon this study to better characterize, understand,
and improve the reliability of data center networks and systems.
ACKNOWLEDGEMENTS
We would like to thank Boliu Xu, Alexander Nikolaidas, James
Zeng, Jimmy Williams, Omar Baldonado, and Hans Ragas for their
feedback and suggestions while writing this paper.
REFERENCES
[1] Al-Fares, M., Loukissas, A., and Vahdat, A. A Scalable, Commodity Data Center
Network Architecture. In Proceedings of the 2008 ACM SIGCOMM Conference
(SIGCOMM’08) (Seattle, WA, USA, 2008).
[2] Alizadeh, M., Greenberg, A., Maltz, D. A., Padhye, J., Patel, P., Prabhakar,
B., Sengupta, S., and Sridharan, M. Data Center TCP (DCTCP). In Proceedings
of the 2010 ACM SIGCOMM Conference (SIGCOMM’10) (New Delhi, India, 2010).
Introducing data center fabric, the next-generation Face-
book data center network. https://code.facebook.com/posts/360346274145943/
introducing-data-center-fabric-the-next-generation-facebook-data-center-network/,
Nov. 2014.
[3] Andreyev, A.
[6] Bagga,
J., and Yao, Z.
FBOSS.
and
open-networking-advances-with-wedge-and-fboss/, Nov. 2015.
Open networking advances with Wedge
https://code.facebook.com/posts/145488969140934/
[7] Bailis, P., and Kingsbury, K. The Network is Reliable: An informal survey of
real-world communications failures. Communications of the ACM (CACM) 57, 9
(Sept. 2014), 48–55.
[8] Barroso, L. A., Clidaras, J., and Hölzle, U. The Datacenter as a Computer:
An Introduction to the Design of Warehouse-scale Machines, 2 ed. Morgan and
Claypool Publishers, 2013.
[9] Basiri, A., Behnam, N., de Rooij, R., Hochstein, L., Kosewski, L., Reynolds, J.,
and Rosenthal, C. Chaos Engineering. IEEE Software 33, 3 (May 2016), 35–41.
[10] Beaver, D., Kumar, S., Li, H. C., Sobel, J., and Vajgel, P. Finding a Needle in