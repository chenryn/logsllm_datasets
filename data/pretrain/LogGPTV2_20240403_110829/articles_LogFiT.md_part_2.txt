thepre-processingstep,andthegeneratedsetoflogtemplates by computing the mean of the word embeddings of all the
representsthemodelvocabulary.AsillustratedinFigure2,the words comprising a log sentence.
input to the DeepLog and LogBERT models are a sequence Existing research suggest that pre-trained LMs such as
of log keys, where each log key corresponds to a log template BERT can learn and incorporate both syntactic and semantic
stored in the vocabulary. In the DeepLog method, the last information that can improve the effectiveness of natural
log key is masked in the input and the model is tasked with language processing tasks [22]–[25]. Therefore, in this study,
predicting what the masked log key was, given the log keys a pre-trained LM is incorporated in the LogFiT model to
that precede it. In contrast, LogBERT masks out random log leverage its ability to understand the sequential properties and
keysintheinputsequenceandtasksthemodelwithpredicting linguistic structure of system logs.
what all the masked tokens were.
Several recent studies, conducted by Nedelkoski, III. LOGFIT
Bogatinovski, Acker, et al. [6], Le and Zhang [11], and The proposed LogFiT approach takes advantage of recent
Wittkopp, Acker, Nedelkoski, et al. [12], have highlighted advances in Deep Learning and NLP to detect anomalies
that the use of log templates can result in a loss of contextual in system event logs. It leverages the linguistic capabilities
information, which can adversely affect the accuracy of of what Stanford University’s Institute for Human-Centered
predictive models. Moreover, models developed using log Artificial Intelligence refer to as ”foundation models” [26],
templates can become dependent on the log parsing tool which are models that have been pre-trained on very large,
employed. Approaches that rely on log templates assume multi-modal corpora (text, images, video, etc.), and are
that the set of log templates remains fixed over time, but in intended to be used as a foundation (via transfer learning) for
reality, changes in the content of log sentences can occur downstreamNLP,image,andvideotasks.Specifically,LogFiT
naturally. As a result, models that depend on log templates usestheLongformer[27]modelasitsbase.TheLogFiTmodel
may not be able to match unseen log sentences to an existing isessentiallyapre-trainedLongformerthathasbeenfine-tuned
record in the set of log templates. to learn the linguistic structures and sequential properties of
These issues were addressed in the development of several log data. The Longformer model is an enhanced variant of
loganomalydetectionmodels,includingLogSy[6],Neuralog the RoBERTa model [28] which in turn is an improvement
[11], and A2Log [12]. Rather than relying on log parsing overtheoriginalBERTmodel[21].TheLongformermodelis
to generate log templates, these models pre-process log data chosen because of its ability to support more than 512 tokens
usingplaincleanupscriptstoeliminateextraneousdetailssuch - which is the limit for BERT.
asparticularIPaddresses,filepaths,portnumbers,andinternet LogFiT adopts a self-supervised training approach whereby
URLs. the model is trained on the normal log data so that it only
learns the linguistic patterns of the normal log data. The
B. Semantic Vectors
training objective of the LogFiT model is to predict a number
The literature on log anomaly detection reveals a ofmaskedtokensinthelogsentences,soitmakesuseofcross-
growing trend towards the adoption of semantic vectorisation entropy loss to minimise the difference (i.e. ”loss”) between
4
Figure 4. The LogFiT model’s Transformer encoder layers, showing token
embeddingsarecomputedfromtheinputtokens.
The BERT model is a Transformer encoder model that
has the capabilities of an auto-encoder. The BERT model’s
encoder capability allows it to generate semantic vectors
that are sensitive to the full context of the input log
paragraph, while enabling it to reconstruct log paragraphs
that have been corrupted [21]. BERT’s ability to accurately
reconstruct corrupted input logs can be used as a threshold-
based anomaly detection method. Thus by inheriting from
BERT, LogFiT includes both the vectoriser and log anomaly
detectioncomponentinasinglearchitecture,allowingforend-
to-endtrainingthatdoesnotrequireintermediatelogtemplate
extractionorvectorisationsteps.Asmentionedearlier,LogFiT
uses the Longformer [27] model, which allows LogFiT to
process log paragraphs containing up to 4096 tokens, which
is longer than the 512-token limit of the BERT model.
Figure3. LogicalarchitectureoftheLogFiTloganomalydetectionapproach.
The Longformer model overcomes BERT’s limitation on
the number of tokens, due to the quadratic computational
the model’s predicted tokens and the real tokens. Because
complexity of BERT’s attention mechanism, by introducing
the cross-entropy loss is logarithmic, it yields larger values
a sliding window strategy which effectively reduces attention
for incorrect predictions than for correct predictions. Further,
computation to a linear time [27].
the model computes semantic vectors representing the log
The LogFiT model consists of 12 stacked Transformer
paragraphs being processed; the contextual embedding vector
encoders,12attentionheadsperlayer,768-dimensionvectors,
forthe[CLS]tokenisusedasthesemanticvector.The[CLS]
a maximum possible sequence length of 4096 tokens – as
embedding vectors are computed for downstream tasks such
illustrated in Figure 4. Not shown is the embedding layer,
asclusteringandvisualisation.Figure3illustratestheLogFiT
which is typically implemented inside the first Transformer
approach and its model training and inference workflow. An
encoder.
earlyversionoftheLogFiTmethodisdescribedinAlmodovar,
Sabrina, Karimi, et al. [29].
B. Training Objective
A. Framework As mentioned previously, the LogFiT model is trained in a
Input Representation. The LogFiT model utilises normal self-supervised manner using masked token prediction. This
log data for training, which has been converted to semantic training objective is modified version of the self-supervised
vectors prior to being passed to the model. In contrast to both masked language modeling (MLM) training objective used
the DeepLog and LogBERT methods, the LogFiT model does to pre-train BERT-based language models [21]. The LogFiT
not require the input log data to first be converted to log model randomly chooses up to 50% of the sentences that
templates. Rather, log data is directly processed, tokenised, constitute the log paragraph for masking, instead of random
andtransformedintosemanticvectorsinasinglestep.LogFiT tokens from all the sentences of the log paragraphs as is the
takesinspirationfromrecentmodels[10],[11]thatforegothe case with BERT. Next, the chosen log sentence’s tokens are
logtemplateextractionstep,insteaddirectlyconvertingthelog masked following the BERT masking algorithm, with 80% of
data into semantic vectors using a pre-trained LM. However, the tokens being masked while the rest are left unmodified.
neither of the two aforementioned models fine-tune the pre- Afterward, the model is tasked with predicting what the
trained LM, instead utilizing the LM as a static semantic masked tokens were.
vectoriser. The rationale for using masked token prediction is that
Transformer architecture. LogFiT inherits from the the model needs to learn the contextual relationships between
innovations introduced by the BERT-based language models. tokensandsentencesinthetrainingdata,inordertoaccurately
5
predict the masked tokens. This enables the model to develop IV. EXPERIMENTS
anunderstandingofthelanguagerulesusedbynormalsystem
In this section the datasets, experimental setup and
logs. As a result, it can differentiate normal log data from
implementationdetailsaredescribed.Subsequently,theresults
anomaly log data.
of running the experiments are evaluated.
To implement the masked token prediction training
objective,themodelminimisesthecross-entropylossbetween
A. Experimental Setup
its masked token predictions and the actual tokens. The
Datasets.TheLogFiTmodelistrainedandevaluatedusing
computation of the cross-entropy loss for a mini-batch of log
threepublicdatasets:HDFS,BGLandThunderbird.Theseare
dataisshowninEquation1.TheLogFiTmodelminimisesthe
the same datasets used by the baseline models that LogFiT
traininglossusingtheAdamoptimiser,withhyperparameters
is compared against. It is important to note that while these
values adopted from FastAI defaults: momentum = 0.9,
datasets are labeled, LogFiT only uses the labels to validate
sqr momentum = 0.99, epsilon = 1e-5, and weight decay =
the performance of the model during training and evaluation.
0.01.
It is expected that in typical application scenarios, log data
are not labeled. Moreover, when LogFiT is deployed, it is
1(cid:88)b (cid:88)m designed to operate in online mode rather than batch mode.
Loss=− yj log(pj ) (1)
b maski maski Consequently,fordatasetswherethegroupingoflogsentences
j=1i=1 is based on time windows (such as BGL and Thunderbird), a
time interval of 30 seconds is chosen to ensure that a system
where b is size of the the mini-batch, m is the number of
that employs LogFiT can deliver prompt feedback to system
masked tokens, y and p are the true and predicted values,
operators.TableIshowssomestatisticsabouttheHDFS,BGL
respectively.
and Thunderbird datasets.
To speed up the model training, LogFiT adopts super-
• HDFS system logs. These logs were generated by an
convergence techniques introduced by Smith and Topin [30]
installation of Hadoop Distributed File System (HDFS)
and implemented in the FastAI/ULMFiT framework [31].
[34].Thelogdatasetcontainsbothnormallogeventsand
Thesuper-convergencetechniquesare:(i)discriminative fine-
anomalylogevents,whichhavebeenmanuallytaggedby
tuning, in which different learning rates are used for different
domain experts. In the HDFS dataset, anomaly system
layer groups of the model, (ii) slanted triangular learning
events represent abnormal file system operations as
rates and one-cycle learning rates, in which the learning rate
specified by the flow of blocks across the HDFS cluster.
and momentum are optimally set during training, and (iii)
Example types of anomalies are ”Replica immediately
gradual unfreezing, in which the model’s weights are trained
deleted”, and ”Namenode not updated after deleting
one layer group at a time. While ULMFiT is primarily for
block”.TheHDFSdatasetischunkedintologparagraphs
LSTMarchitectures,thetechniquesthatitembodieshavebeen
using the HDFS block ID, which represents a session in
validated to be effective for Transformer-based architectures
HDFS.ThefullHDFSdatasetconsistsof11,175,629log
as well [32], [33]. Gururangan, Marasovic´, Swayamdipta, et
sentences, of which 16,838 are anomalies.
al. [32] have shown that task-adaptive pre-training (i.e. ”fine-
• BGLsystemlogs.TheselogswereproducedbytheBlue
tuning”) lead to significant (2% on average) performance
Gene/L supercomputer system installed at the Lawrence
gains, while Kumar, Raghunathan, Jones, et al. [33] showed
LivermoreNationalLaboratory(LLNL)[35].Thedataset
that the ”gradual unfreezing” approach mitigates the observed
containssuitablylabelednormalandalertevents,withthe
phenomenon of reduced effectiveness when LMs are fine-
alert events considered as anomalies. The BGL dataset
tuned, due to the dissipation of the pretrained weights.
does not have a session ID with which to group the data,
thus the dataset is chunked into log paragraphs, each
consisting of the consecutive log sentences belonging to
C. Anomaly Detection a time window of 30 seconds, following the approach
discussed in [5], [7]. A log paragraph is considered an
The LogFiT model, which is exclusively trained on normal anomaly if it includes at least one log sentence that is
data, can then be used to detect abnormal log data. During tagged as an anomalous event. The full BGL dataset
the inference stage, log paragraphs are tokenised, vectorised, consists of 4,747,963 log sentences, of which 348,460
masked,andreconstructedinthesamewayasduringtraining. are anomalies.
To determine whether a log paragraph is anomalous, LogFiT • Thunderbird system logs. These logs were created
usesatechniqueadoptedfromDeepLogandLogBERT,where by the Thunderbird supercomputer system operated by
the top-k accuracy (with k ranging from 5 to 15) is used Sandia National Laboratories (SNL) [35]. Similar to