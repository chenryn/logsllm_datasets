âˆ’r (mod N ), where y = yğ’® + yğ’ (mod N );
...
â€¢ ğ’: xğ’ := r.
Figure 6: The ideal functionality â„±sigmoid.
Even though it is more complex thanâ„±ReLU, it can still be realized
easily using the basic functionalities provided by 2PC.
In summary, we support all activation functions that are both:
3We apply post-processing to the polynomials to ensure they are within upper and
lower bounds of the function f (), and to ensure that the approximate function Â¯f is
monotonic (if f () is).
(1) monotonic in ranges (âˆ,0) and (0,âˆ);4 and
(2) either piecewise-polynomial or approximable.
Most commonly used activation functions belong to this class, ex-
cept softmax that violates the second condition. But softmax can
be replaced by argmax (Section 2.1.2) or left out (Section 2.1).
5.4 Oblivious pooling operations
The pooling layer arranges the inputs into several groups and take
the max or mean of the elements in each group. For mean pooling,
we just have ğ’® and ğ’ calculate the sum of their respective shares and
keep track of the divisor. For max pooling, we use garbled circuits to
realize the ideal functionality â„±max in Figure 7, which reconstructs
each yi and returns the largest one masked by a random number.
The max function can be easily achieved by the compare function.
Inputs:
Outputs:
n };
1 , ...,yğ’®
n}, r.
1 , ...,yğ’
â€¢ ğ’®: {yğ’®
â€¢ ğ’: {yğ’
â€¢ ğ’®: xğ’® := max (y1, ...,yn ) âˆ’ r (mod N ) where y1 = yğ’®
1 +
1 (mod N ) ... yn = yğ’®
yğ’
â€¢ ğ’: xğ’ := r.
n (mod N );
n + yğ’
Figure 7: The ideal functionality â„±max.
Note that the oblivious maxout activation can be trivially realized
by the ideal functionality â„±max.
5.5 Remarks
5.5.1 Oblivious square function. The square function (i.e., f (y) =
2) is also used as an activation function in [28, 44], because it is
y
easier to be transformed into an oblivious form. We implement
an oblivious square function by realizing the ideal functionality in
Figure 8 using arithmetic secret sharing.
Input:
Output:
â€¢ ğ’®: yğ’® âˆˆ ZN ;
â€¢ ğ’: yğ’,r âˆˆ ZN .
â€¢ ğ’®: xğ’® := y
â€¢ ğ’: xğ’ := r.
2 âˆ’ r (mod N ) where y = yğ’® + yğ’ (mod N );
Figure 8: The ideal functionality â„±Square.
5.5.2 Dealing with large numbers. Recall that we must make
sure that the absolute value of any (intermediate) results will not ex-
ceed âŒŠN /2âŒ‹. However, the data range grows exponentially with the
number of multiplications, and it grows even faster when the float-
ing point numbers are scaled to integers. Furthermore, the SIMD
4This condition guarantees that our scaling technique (i.e., scale the floating-point
numbers up to integers by multiplying the same constant to all values and drop the
fractional parts) does not change the ranking order and thus does not impact prediction
accuracy. Recall that the model finally outputs a set of probabilities, one for each class.
The class with maximal probability is the prediction.
Session C3:  Machine Learning PrivacyCCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA625technique will shrink the plaintext space so that it cannot encrypt
large numbers. As a result, only a limited number of multiplications
can be supported.
To this end, CryptoNets uses Chinese Remainder Theorem (CRT)
to split large numbers into multiple small parts, work on each part
individually, and combines the results in the end [28]. This method
allows encryptions of exponentially large numbers in linear time
and space, but the overhead grows linearly with the number of
split parts. On the other hand, SecureML has both parties truncate
their individual shares independently [44]. This method may incur
a small error in each intermediate result, which may affect the final
prediction accuracy.
We implement the ideal functionality in Figure 9 using garbled
circuit to securely scale down the data range without affecting
accuracy. It reconstructs y and shift it left by L bits, where L is
a constant known to both ğ’® and ğ’. This is equivalent to xğ’® :=
âˆ’ r (mod N ). They can run this protocol after each layer, or
y
2L
whenever needed.
(cid:104)
(cid:105)
Input:
Output:
â€¢ ğ’®: yğ’® âˆˆ ZN ;
â€¢ ğ’: yğ’,r âˆˆ ZN .
â€¢ ğ’®: xğ’® := leftshift (y,L) âˆ’ r (mod N ) where y = yğ’® +
yğ’ (mod N );
â€¢ ğ’: xğ’ := r.
Figure 9: The ideal functionality â„±trunc.
5.5.3
Security. We have proved the security for the dot-product
triplet generation, and other operations are directly implemented
using 2PC protocols. So the security guarantees of all operations
are straightforward. Furthermore, the output of each operation is
randomly shared between ğ’® and ğ’. As stated in Lemma 2 of [12], a
protocol that ends with secure re-sharing of output is universally
composable. So, after composition, the entire ONN is secure.
6 PERFORMANCE EVALUATION
We implemented MiniONN in C++ using Boost5 for networking. We
used the ABY [21] library for secure two-party computation with
128-bit security parameter and SIMD circuits. We used YASHE [13]
for additively homomorphic encryption, a SIMD version of which is
supported by the SEAL library [23]. The YASHE encryption scheme
maps plaintext messages from the ring ZN [x]/(xn + 1) to the ring
Zq[x]/(xn + 1). The ciphertext modulus q determines the security
level. The SEAL library automatically chooses a secure q given the
polynomial degree n (i.e., SIMD batch size). Choice of n is a tradeoff
between parallelism and efficiency of a single encryption. We first
set n =4 096 so that we can encrypt 4 096 elements together in a
reasonable encryption time and ciphertext size. Then we chose the
largest possible plaintext modulus: N =101 285 036 033, which is
large enough for the needed precision since we securely scale down
the value when it becomes large as we discussed in Section 5.5.2.
5http://www.boost.org
To evaluate its performance, we ran the server-side program on
a remote computer (Intel Core i5 CPU with 4 3.30 GHz cores and 16
GB memory) and the client-side program on a local desktop (Intel
Core i5 CPU machine with 4 3.20 GHz cores and 8 GB memory).
We used the chrono library in C++ for time measurement and used
TCPdump for bandwidth measurement. We measured response
latency (including the network delay) and message sizes during
the whole procedure, i.e., from the time ğ’ begins to generate its
request to the time it obtains the final predictions. Each experiment
was repeated 5 times and we calculated the mean and standard
deviation. The standard deviations in all reported results are less
than 3%.
6.1 Comparisons with previous work
The MNIST dataset [38] consists of 70 000 black-white hand-written
digit images (of size 1 Ã— 28 Ã— 28: width and height are 28 pixels) in
10 classes. There are 60 000 training images and 10 000 test images.
Since previous work use MNIST to evaluate their techniques, we
use it to provide a direct comparison with prior work..
Neural network in SecureML [44]. We reproduced the model (Fig-
ure 10) presented in SecureML [44]. It uses multi-layer perceptron
(MLP) model with square as the activation function and achieves an
accuracy of 93.1% in the MNIST dataset. We improve the accuracy
of this model to 97.6% by using the Limited-memory BFGS [40] op-
timization algorithm and batch normalization during training. We
transformed this model with MiniONN and compared the results
with those reported in [28].
the outgoing 128 nodes: R128Ã—1 â† R128Ã—784 Â· R784Ã—1.
(1) Fully Connected: input image 28 Ã— 28, connects the incoming 784 nodes to
(2) Square Activation: squares the value of each input.
(3) Fully Connected: connects the incoming 128 nodes to the outgoing 128 nodes:
(4) Square Activation: squares the value of each input.
(5) Fully Connected: fully connects the incoming 128 nodes to the outgoing 10
R128Ã—1 â† R128Ã—128 Â· R128Ã—1.
nodes: R10Ã—1 â† R10Ã—128 Â· R128Ã—1.
Figure 10: The neural network presented in SecureML [44].
The results (Table 2) show that MiniONN achieves comparable
online performance and significantly better offline performance.
We take the first layer as an example to explain why the SIMD batch
processing technique improves the performance of offline phase.
The first layer connects 784 incoming nodes to 128 outgoing nodes,
which leads to a matrix multiplication: R128Ã—1 â† R128Ã—784 Â· R784Ã—1.
In SecureML [44], ğ’ encrypts each of the 784 elements separately
and sends them to ğ’®, which leads to 784 encryptions and ciphertext
transfers. ğ’® applies each row of the matrix to the ciphertexts to cal-
culate an encrypted dot-product, which leads to 784Ã—128 = 100 352
homomorphic multiplications. Then ğ’® returns the resulting 128
ciphetexts to ğ’, who decrypts them, which leads to another 128
ciphertext transfers and 128 decryptions. On the other hand, we
duplicate the 784 elements into 128 copies, and encrypt them into
25 ciphertexts, since each ciphertext can pack 4096 elements. ğ’®
encodes the matrix into 25 batches and multiplies them to the ci-
phertexts, which only leads to 25 homomorphic multiplications.
Table 3 summarizes this comparison.
Session C3:  Machine Learning PrivacyCCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA626Square/MLP/MNIST
(Figure 10)
by SecureML [44]
by MiniONN
Latency (s)
offline
4.7
0.9
online
0.18
0.14
Message Sizes (MB)
online
offline
-
3.8
-
12
Accuracy
%
93.1
97.6
Table 2: Comparison: MiniONN vs. SecureML [44].
SecureML [44] MiniONN
# homomorphic encryptions
# homomorphic multiplications
# ciphertext transfers
# homomorphic decryptions
100 352
784
912
128
25
25
50
25
Table 3: Comparison: MiniONN vs. SecureML [44], dot-product
triplet generations.
Neural network in CryptoNets [28]. We reproduced the model (Fig-
ure 11) presented in CryptoNets [28]. It is a CNN model with square
as the activation function as well, and uses mean pooling instead
of max pooling. Due to the convolution operation, it achieves a
higher accuracy of 98.95% in the MNIST dataset. We transformed
this model with MiniONN and compared its performance with the
results reported in CryptoNets [28]. Table 4 shows that MiniONN
achieves 230-fold reduction in latency and 8-fold reduction in mes-
sage sizes, without degradation in accuracy. CryptoNets uses the
SIMD technique to batch different requests to achieve a throughput
of 51 739 predictions per hour, but these requests must be from the
same client. In scenarios where the same client sends a very large
number of prediction requests and can tolerate response latency
in the order of minutes, CryptoNets can achieve 6-fold throughput
compared to MiniONN. In scenarios where each client sends only
a small number of requests but needs quick responses, MiniONN
decisively outperforms CryptoNets.
(1) Convolution: input image 28 Ã— 28, window size 5 Ã— 5, stride (2, 2), number
of output channels 5. It can be converted to matrix multiplication [18]:
R5Ã—169 â† R5Ã—25 Â· R25Ã—169.
(2) Square Activation: squares the value of each input.
(3) Pool: combination of mean pooling and linear transformation: R100Ã—1 â†
(4) Square Activation: squares the value of each input.
(5) Fully Connected: fully connects the incoming 100 nodes to the outgoing 10
R100Ã—845 Â· R845Ã—1.
nodes: R10Ã—1 â† R10Ã—100 Â· R100Ã—1.
Figure 11: The neural network presented in CryptoNets [28].
Square/CNN/MNIST
Latency (s)
(Figure 11)
by CryptoNets [28]
offline
0
0.88
online
297.5
0.4
Message Sizes (MB)
offline
online
372.2
44
0
3.6
Accuracy
%
98.95
98.95
by MiniONN
Table 4: Comparison: MiniONN vs. CryptoNets [28].
are built with popular neural network operations using several
different standard datasets.
Handwriting recognition: MNIST. We trained and implemented an-
other neural network (Figure 12) using the MNIST dataset, but
using ReLU as the activation function. The use of ReLU with a more
complex neural network increases the accuracy of the model in
MNIST to 99.31%, which is close to the state-of-the-art accuracy in
the MNIST dataset (99.79%)6.
R16Ã—64 â† R16Ã—400 Â· R400Ã—64.
of output channels of 16: R16Ã—576 â† R16Ã—25 Â· R25Ã—576.
(1) Convolution: input image 28 Ã— 28, window size 5 Ã— 5, stride (1, 1), number
(2) ReLU Activation: calculates ReLU for each input.
(3) Max Pooling: window size 1 Ã— 2 Ã— 2 and outputs R16Ã—12Ã—12.