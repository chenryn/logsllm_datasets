title:A Performant, Misuse-Resistant API for Primality Testing
author:Jake Massimo and
Kenneth G. Paterson
A Performant, Misuse-Resistant API for
Primality Testing
Jake Massimo1 and Kenneth G. Paterson2
1 Information Security Group,
Royal Holloway, University of London
PI:EMAIL
2 Department of Computer Science,
ETH Zurich
PI:EMAIL
Abstract. Primality testing is a basic cryptographic task. But developers
today are faced with complex APIs for primality testing, along with
documentation that fails to clearly state the reliability of the tests being
performed. This leads to the APIs being incorrectly used in practice, with
potentially disastrous consequences. In an eﬀort to overcome this, we
present a primality test having a simplest-possible API: the test accepts a
number to be tested and returns a Boolean indicating whether the input
was composite or probably prime. For all inputs, the output is guaranteed
to be correct with probability at least 1 − 2−128. The test is performant:
on random, odd, 1024-bit inputs, it is faster than the default test used in
OpenSSL by 17%. We investigate the impact of our new test on the cost
of random prime generation, a key use case for primality testing. The
OpenSSL developers have adopted our suggestions in full; our new API
and primality test are scheduled for release in OpenSSL 3.0.
1
Introduction
Primality testing, and closely related tasks like random prime generation and
testing of Diﬃe-Hellman parameters, are core cryptographic tasks. Primality
testing is by now very well understood mathematically; there is a clear distinction
between accuracy and running time of diﬀerent tests in settings that are malicious
(i.e. where the input may be adversarially-selected) and non-malicious (e.g. where
the input is random, as is common in prime generation).
Yet recent research by Albrecht et al. [AMPS18] on how primality testing is
actually done in practice has highlighted the failure of popular cryptographic
libraries to provide primality testing APIs that are “misuse-resistant”, that is,
which provide reliable results in all use cases even when the developer is crypto-
naive. Extending [AMPS18], Galbraith et. al. [GMP19] showed how failure to
perform robust primality testing in the popular OpenSSL library has serious
security consequences in the face of maliciously generated Diﬃe-Hellman param-
eter sets (see also Bleichenbacher [Ble05] for an earlier example involving the
GNU Crypto library).
The main underlying issue identiﬁed in [AMPS18] is that, while all libraries
examined performed well on random inputs, some failed miserably on mali-
ciously crafted ones in their default settings. Meanwhile code documentation
was generally poor and did not distinguish clearly between the diﬀerent use
cases. And developers were faced with complex APIs requiring them to under-
stand the distinctions between use cases and choose parameters to the APIs
accordingly. An illustrative example is provided by the OpenSSL primality test-
ing code that existed prior to our work. This required the developer using the
function BN_is_prime_fasttest_ex3 to pass multiple parameters, including
checks, the number of rounds of Miller-Rabin testing to be carried out; and
do_trial_division, a ﬂag indicating whether or not trial division should be per-
formed. Setting checks to 0 makes the test default to using a number of rounds
that depends only on the size of the number being tested;4 then the number
of rounds decreases as the size increases, this being motivated by average-case
error estimates for the Miller-Rabin primality test operating on random num-
bers [DLP93,MVOV96]. This makes the default setting performant for random
prime generation, but dangerous in potentially hostile settings, e.g. Diﬃe-Hellman
parameter testing.
As an illustration of how this can go wrong in practice, Galbraith et. al.
[GMP19] pointed out that OpenSSL itself makes the wrong choice in using the
default setting when testing ﬁnite ﬁeld Diﬃe-Hellman parameters. Galbraith et.
al. exploited this choice to construct Diﬃe-Hellman parameter sets (p, q, g) of
cryptographic size that fool OpenSSL’s parameter validation with a non-trivial
success rate. OpenSSL’s Diﬃe-Hellman parameter validation was subsequently
changed to remedy this issue (though without changing the underlying primality
test).5 This example provides prima facie evidence that even very experienced
developers can misunderstand how to correctly use complex primality testing
APIs.
One may argue that developers who are not cryptography experts should not
be using such security-sensitive APIs. However, they inevitably will, and, as our
OpenSSL example shows, even expert developers can get it wrong. This motivates
the search for APIs that are “misuse-resistant” or “robust”, and that do not
sacriﬁce too much performance. This search accords with a long line of work
that identiﬁes the problem of API design as being critical for making it possible
for developers to write secure cryptographic software (see [Gut02,WvO08,GS16]
amongst others).
1.1 Our Contributions
Given this background, we set out to design a performant primality test that
provides strong security guarantees across all use cases and that has the simplest
3 See https://github.com/openssl/openssl/blob/
3e3dcf9ab8a2fc0214502dad56d94fd95bcbbfd5/crypto/bn/bn prime.c#L186.
4 Strictly, the default is invoked by setting checks to BN prime checks, an environ-
mental variable that is set to 0.
5 See https://github.com/openssl/openssl/pull/8593.
possible API: it takes just one input, the number being tested for primality, and
returns just one integer (or Boolean) indicating that the tested number is highly
likely to be prime (1) or is deﬁnitely composite (0). We note that none of the
many crypto libraries examined in [AMPS18] provide such an API.
We examine diﬀerent options for the core of our test – whether to use many
rounds of Miller-Rabin (MR) testing (up to 64 or 128, to achieve false positive
rates of 2−128 or 2−256, respectively), or to rely on a more complex primality test,
such as the Baillie-PSW test [PSW80] which combines MR testing with a Lucas
test. Based on a combination of code simplicity, performance and guaranteed
security, we opt for 64 rounds of MR as the core of our test.
We also study the performance impact of doing trial division prior to more
expensive testing. This is common practice in primality testing code, with the
idea being that one can trade fast but inaccurate trial division for much slower
but more accurate number theoretic tests such as Miller-Rabin. For example,
OpenSSL tests for divisibility using a ﬁxed list of the ﬁrst 2047 odd primes. We
show that this is a sub-optimal choice when testing random inputs of common
cryptographic sizes, and that the running time can be reduced substantially by
doing trial division with fewer primes. That the optimal amount of trial division
to use depends on the size of the input being tested is not a new observation – see
for example [MVOV96,Mau95,Jun87]. What is more surprising is that OpenSSL
chooses so conservatively and with a ﬁxed list of primes (independent of the input
size). For example, with 1024-bit random, odd inputs, trial division using the ﬁrst
128 odd primes already removes about 83% of candidates, while extending the
list to 2047 primes, as OpenSSL does, only removes a further 5.5%. On average,
it turns out to be faster to incur the cost of an MR test on that additional 5.5%
than it is to do the full set of trial divisions.
The outcome of our analysis is a primality test whose performance on random,
odd, 1024-bit inputs is on average 17% faster than the current OpenSSL test, but
which guarantees that composites are identiﬁed with overwhelming probability
(1−2−128), no matter the input distribution. The downside is that, for inputs that
are actually prime rather than random, our test is signiﬁcantly slower than with
OpenSSL’s default settings (since we do 64 MR tests compared to the handful of
tests used by OpenSSL). This is the price to be paid for a misuse-resistant API.
We then examine how our choice of primality test aﬀects the performance
of a crucial use case for primality testing, namely generation of random k-bit
primes. OpenSSL already includes code for this. It makes use of a sieving step to
perform trial division at reduced cost across many candidates, obviating the need
to perform per-candidate trial division internally to the primality test. OpenSSL
avoids the internal trial division via the above-mentioned do_trial_division
input to the primality test in OpenSSL. Since we do not allow such an input in our
simpliﬁed primality testing API, a developer using our API would be (implicitly)
forced to do trial division on a per candidate basis, potentially increasing the cost
of prime generation. Moreover, our primality test may use many more rounds of
MR testing than OpenSSL selects in this case, since our API does not permit
the user to vary the number of rounds according to the use case. However, for
random prime generation, most candidates are rejected after just one MR test,
and so the full cost of our test (trial division plus 64 rounds of MR testing) is only
incurred once, when a prime is actually encountered. So we seek to understand the
performance impact of plugging our new API and primality test into the existing
OpenSSL prime generation code. We ﬁnd that, for generation of random 1024-bit
primes OpenSSL’s primality generation code is 35-45% slower when using our
primality test internally. For this cost, we gain an API for primality testing that
is as simple as possible and where the test has strong security guarantees across
all use cases.
We communicated our ﬁndings to the OpenSSL developers, and they have
adopted our suggestions with only minor modiﬁcations: the forthcoming OpenSSL
3.0 (scheduled for release in Q4 of 2020) will include our simpliﬁed API for
primality testing, and the OpenSSL codebase has been updated to use it almost
everywhere (the exception is prime generation, which uses the old API in order
to avoid redundant trial division). Moreover, OpenSSL will now always use our
suggested primality test (64 rounds of MR) on all inputs up to 2048 bits, and
128 bits of MR on larger inputs. This represents the ﬁrst major reform of the
primality testing code in OpenSSL for more than 20 years.
1.2 Related Work
The topic of API design for cryptography has a long history and connections to
related ﬁelds such as usable security and API design for security more generally.
As early as 2002, Gutmann [Gut02] identiﬁed the need to carefully deﬁne
cryptographic APIs, recommending to “[p]rovide crypto functionality at the
highest level possible in order to prevent users from injuring themselves and
others through misuse of low-level crypto functions with properties they aren’t
aware of.” This is precisely what we aim to do for primality testing in this paper.
Later, Wurster and van Oorschot [WvO08] (in the broader context of security)
argued that attention should be focussed on those developers who produce core
functionality used by other developers, e.g. producers of APIs. They identiﬁed
the need to design APIs which can be easily used in a secure fashion.
Green and Smith [GS16] extensively discuss the need for usable security APIs,
and focus on cryptographic ones. They give an extensive list of requirements for
good APIs, including: APIs should be easy to learn, even without cryptographic
expertise; defaults should be safe and never ambiguous; APIs should be easy to
use, even without documentation; APIs should be hard to misuse and incorrect
use should lead to visible errors. These precepts have inﬂuenced our API design
for primality testing.
Acar et al. [AFM16] adovcate for a research agenda for usable security and pri-
vacy research that focusses on developers rather than end users. This encompasses
cryptography. Recent research related to this agenda and having a cryptographic
focus includes [EBFK13,FHP+13,LCWZ14,ABF+17,NDT+17,NKMB17,GIW+18].
Nonce-based Authenticated Encryption (AE), a primitive introduced by
Rogaway [Rog04], can be seen as an attempt to simplify the symmetric encryption
API for developers, replacing the need to understand various requirements on IVs
with the arguably simpler need to be able to supply unique (per key) inputs to an
encryption algorithm. It has become the standard target for algorithm designers.
However, as [BZD+16] showed, developers can accidentally misuse even this
simpliﬁed API, with disastrous results for nonce-sensitive modes like AES-GCM.
This motivated the development of misuse-resistant AE schemes, which attempt to
preserve as much security as possible even when nonces are repeated. Prominent
examples include SIV [RS06], Deoxys-II (part of the CAESAR competition
ﬁnal portfolio), and AES-GCM-SIV [GLL17] (see also RFC 8452). Later authors
identiﬁed the fact that developers may want an even higher-level API, for example
a secure streaming channel like that provided by TLS [FGMP15,PS18] or channels
that tolerate some forms of reordering and repetition [BHMS16]; the mismatch
between what developers want and what nonce-based AE can provide can lead
to attacks, cf. [BDF+14].
Bernstein’s design for DH key exchange on Curve25519 [Ber06] deliberately
presents a simple API for developers: public and private keys are represented by
32-byte strings, and the need for public key validation is avoided.
The NaCl crypto library [BLS12] has provision of a simple API to devel-
opers as one of its primary aims. It gives the user a crypto_box function
that encrypts and authenticates messages, with a simple API of the form:
c = crypto_box(m,n,pk,sk), where m is a message, n is a nonce, pk is the
public key of the recipient and sk is the private key of the sender. Its security
does rely on developers correctly handling nonces; we are unaware of reports of
any misuse of this type. Some criticism of NaCl’s approach, especially the way in
which it breaks the developer’s expected paradigm, can be found in [GS16].
There is an extensive literature on primality testing and generation, nicely
summarised in [MVOV96, Chapter 4]. The state-of-the-art has not changed
signiﬁcantly since the publication of that book in 1996. On the other hand, as
Albrecht et al. [AMPS18] showed, primality testing and generation as it is done
in practice has many shortcomings. Our work can be seen as an eﬀort to narrow
the gap between the literature and its practical application.
1.3 Paper Organisation
The remainder of this paper is organised as follows. In Section 2 we give further
background on primality testing and detail the approach used in OpenSSL. In
Section 3 we describe four diﬀerent candidate primality tests and analyse them
theoretically and experimentally. Our chosen primality test (64 rounds of Miller-
Rabin with trial division on the ﬁrst 128 odd primes) emerges from this analysis
as our preferred test. We then evaluate the performance of this chosen test in
the use case of prime generation in Section 4. Section 5 brieﬂy discusses how our
test is being adopted in OpenSSL, while Section 6 contains our conclusions and
avenues for future work.
2 Further Background
2.1 Primality Testing
We begin by giving further details on the core primality tests that we will consider
in this work.
Miller-Rabin The Miller-Rabin (MR) [Mil75,Rab80] primality test is a widely-
used and eﬃcient algorithm.
A single round of the test proceeds as follows. Suppose n > 1 is an odd integer
to be tested for primality. We ﬁrst write n = 2ed + 1 where d is odd. If n is prime,
we know that there are are no non-trivial roots of unity modulo n, thus for any
integer a with 1 ≤ a < n, we have:
ad ≡ 1 mod n or a2id ≡ −1 mod n for some 0 ≤ i < e.
The test then consists of choosing a value a (often referred to as a base), and then
checking the above conditions on n. We declare a number to be (probably) prime
if either of the two conditions hold and to be composite if both conditions fail. If
n is composite and at least one condition holds, then we say n is a pseudoprime
to base a, or that a is a non-witness to the compositeness of n (since n may be
composite, but a does not demonstrate this fact). It is evident that computational
cost of the test is that of a full-size exponentiation modulo n.
In practice, the test is iterated t times, using a diﬀerent, random choice of
base a in each round (though as observed in [AMPS18], ﬁxed bases are often
used in crypto libraries, which makes it possible to construct composites that are
always declared prime by the test). The test is probabilistic, in that a t-round
MR test using uniformly random bases declares any composite number to be
composite with probability at least 1 − 4−t. Moreover, this bound is tight: there
are composites which are not identiﬁed as being such over t rounds of testing
with probability 4−t. Such numbers, then, are worst-case adversarial inputs for
the test. They are treated extensively in [AMPS18]. On the other hand, the test
never declares a prime to be composite.
The above discussion holds for any input n, no matter how it is chosen. When
n is a uniformly random odd k-bit integer, much better performance can be
assured. For example, a result of [DLP93] assures that the probability pk,1 that
a composite n chosen in this way passes one round of random-base MR testing
is bounded by k242−√
k. Thus, for k = 1024, we have pk,1 ≤ 2−40. Using more
precise bounds from [DLP93], this can be improved to pk,1 ≤ 2−42.35. These
bounds are what motivates the rather small numbers of rounds of MR testing in
the default setting in OpenSSL’s primality test, for example.
Lucas The Lucas primality test [BW80] makes use of Lucas sequences, deﬁned
as follows:
Deﬁnition 1 (Lucas sequence [BW80]). Let P and Q be integers and D =
P 2 − 4Q. Then the Lucas sequences (Uk) and (Vk) (with k ≥ 0) are deﬁned
recursively by:
Uk = P Uk−1 − QUk−2
Vk = P Vk−1 − QVk−2
where,
U0 = 0, U1 = 1,
V0 = 2, V1 = P.
Since we are concerned with primality testing cryptographic sized numbers,
we can use eﬃcient techniques for computing large Lucas sequences such as binary
Lucas chains as described in [Mon92]. The Lucas probable prime test then relies
(cid:17)
(cid:16) x
p
on the following theorem (in which
denotes the Legendre symbol, with value
1 if x is a square modulo p and value -1 otherwise):
Theorem 1 ([CP06]). Let P , Q and D and the Lucas sequences (Uk), (Vk) be
deﬁned as above. If p is a prime with gcd(p, 2QD) = 1, then
Up−( D
p ) ≡ 0