# Low Overhead Fault Tolerant Networking in Myrinet

**Authors:**
Vijay Lakamraju, Israel Koren, and C. Mani Krishna  
Department of Electrical and Computer Engineering  
University of Massachusetts, Amherst, MA 01003, USA  
E-mail: {vlakamra, koren, krishna}@ecs.umass.edu

## Abstract
Emerging networking technologies have introduced complex network interfaces, raising new concerns about network reliability. This paper presents an effective, low-overhead fault tolerance technique for recovering from network interface failures, particularly network processor hangs, using the Myrinet platform as a case study. Our approach involves maintaining a small backup copy of the network interface state, which is used to restore the interface upon failure. Fault detection is achieved through a software watchdog that monitors for network processor hangs. Experimental results on Myrinet show that complete fault recovery can be achieved in under 2 seconds with a latency overhead of just 1.5 µs during normal operation. The paper also discusses how this fault recovery can be made transparent to the user.

## 1. Introduction
The complexity of network hardware has increased significantly over the past few decades. For example, a typical dual-speed Ethernet controller uses around 10K gates, while a high-speed network processor like the Intel IXP1200 [8] uses over 5 million transistors. This trend is driven by the demand for higher network performance, leading to more communication-related processing being offloaded to the network interface. Modern network interfaces often include a network processor and large local memory, such as the Myrinet host interface card, which uses a custom 32-bit RISC processor and onboard SRAM ranging from 512K to 8M bytes.

However, this increased complexity raises concerns about system reliability and availability. Network interface failures can be particularly detrimental to distributed systems, causing nodes to be cut off from the rest of the network, or even leading to host computer crashes. Therefore, detecting and recovering from such failures quickly is crucial for systems requiring high availability and reliability.

In this paper, we present an efficient, low-overhead fault-tolerance technique for network interface failures, focusing on network processor hangs. Our approach involves keeping a minimal amount of network-related state information in the host, allowing the network interface state to be re-established correctly after a failure. We also incorporate a quick fault detection scheme based on software-implemented watchdog timers. The fault detection and recovery techniques are general enough to be applicable to many modern network technologies, especially those that are microprocessor-based.

Before detailing our fault tolerance technique in Sections 3 and 4, we will briefly describe Myrinet, the platform for this case study, and report on fault injection experiments that highlight the vulnerability of such microprocessor-based network systems to faults. In Section 5, we discuss implementation details and report on the performance and overhead of the fault tolerance scheme. We conclude the paper in Section 6.

## 2. Myrinet: An Example System
Myrinet [3] is a cost-effective, high-bandwidth (2 Gb/s) packet-communication and switching technology from Myricom Inc. [11]. It employs wormhole switching, backpressure flow control, and source routing to achieve low-latency (approximately 8 µs) message transfers. A Myrinet network consists of point-to-point, full-duplex links connecting Myrinet switches and host interfaces.

The Myrinet host interface card provides a flexible, high-performance interface between a generic bus (such as SBus or PCI) and a Myrinet link. The central component of the interface is the LANai chip, which contains a RISC processor, DMA logic, E-bus interface logic, timers, and local configuration registers. The fast local synchronous memory (SRAM) is used for storing the Myrinet Control Program (MCP) and packet buffering. The MCP runs on the LANai RISC processor and provides the basic functionality for reliable message transfer from the host to the Myrinet link.

The Myrinet host software includes a device driver that runs as part of the host operating system and a user library that provides a lightweight communication layer and API for application software. The device driver handles important I/O device-related interfaces, such as port opening and closing, memory mapping, interrupt handling, and loading the MCP. The ability to modify the network behavior through the MCP has enabled the implementation and testing of various communication protocols on Myrinet, including Active Messages [4], Fast Messages [12], and BIP [13]. Myricom’s own software, called GM, incorporates features from these protocols and is widely used for Myrinet. These protocols are low-overhead, avoiding operating system intervention by providing a zero-copy mode of operation directly from user space to the network.

Despite its fault tolerance features, Myrinet/GM may still not be suitable for systems requiring high availability for special applications, such as the NASA REE supercomputer [6]. All of Myrinet’s high availability features assume that the LANai processor executes error-free, which can be a costly assumption in some applications, particularly in space environments where cosmic rays can cause bit flips in the MCP, leading to processor hangs or crashes.

Fault injection experiments performed by our research group and others [15] have shown that transient faults in the network processor can lead to interface hangs and corrupted messages. Table 1 summarizes the results of these experiments, highlighting the significant impact of interface hangs and dropped/corrupted messages on network reliability.

| **Failure Category** | **% of Injections (Our Work)** | **% of Injections (Iyer et al. [15])** |
|---------------------|--------------------------------|--------------------------------------|
| Local Interface Hung | 28.6                           | 23.4                                 |
| Messages Corrupted   | 18.3                           | 12.7                                 |
| Remote Interface Hung| 0.0                            | 1.2                                  |
| MCP Restart          | 0.0                            | 3.1                                  |
| Host Computer Crash  | 0.6                            | 0.4                                  |
| Other Errors         | 1.2                            | 1.1                                  |
| No Impact            | 51.3                           | 58.1                                 |

These results indicate that interface hangs and dropped/corrupted messages account for more than 90% of the failures affecting the network interface. While GM handles dropped/corrupted messages well, there is no easy way to recover from interface hangs. Middleware, such as MPI, built on top of GM, considers GM send errors to be fatal and exits when encountering such errors, potentially halting distributed applications.

For the rest of the paper, we will focus on interface hangs and present a recovery strategy to address them.

## 3. Recovery Strategy
Recovery from a host interface failure primarily involves restoring the state of the interface to its pre-failure condition. Simply resetting the interface and reloading the MCP is insufficient, as it can lead to message loss or duplicate message reception. To understand this, we need to examine the programming model and design of GM.

### 3.1. GM Programming Model
GM enables communication between user processes on separate nodes through endpoints called "ports" with two non-preemptive priority levels for messages. The programming model is connectionless, meaning there is no need to establish a connection with a remote port before communication. The sender allocates DMAable memory, initializes the memory segment, and informs the LANai that the message needs to be sent. The receiver allocates DMAable memory and notifies the MCP that it is ready to receive. The MCP handles the rest of the communication process, including DMAing the message, building packets, setting up connections, sending and receiving packets, and ensuring reliable in-order delivery. A "connection" corresponds to a logical link to a remote node, which the MCP uses to multiplex traffic.

The MCP uses a version of the Go-Back-N protocol to handle transient network errors and ensure in-order delivery. Flow control in GM is managed through a token system, similar to credit-based flow control [10]. Both sends and receives are regulated by implicit tokens, representing space allocated to the user process in internal GM queues. A send token includes information about the send buffer's location, size, priority, and destination. A receive token contains information about the receive buffer's size and the message's priority. Processes start with a fixed number of send and receive tokens, relinquishing a send token each time they call `gm_send()` and a receive token with `gm_provide_receive_buffer()`. Tokens are passed back to the process when callbacks are invoked or messages are received.

### 3.1.1. Duplicate Messages
Reliable transmission in GM is achieved through sequence numbers, maintained solely by the MCP. If the MCP is reloaded and restarted after a failure, the state of the connections and sequence numbers is lost, leading to potential duplicate message reception. Figure 4 illustrates this problem. If the sender resends a message with an invalid sequence number after a crash, the receiver may accept a duplicate message. Storing redundant state information in stable storage can help avoid this issue, but managing redundancy without impacting performance is key.

### 3.1.2. Lost Messages
Lost messages can occur if the MCP is reloaded and restarted without preserving the state of the connections. The MCP must maintain the state of all streams of sequence numbers to ensure reliable retransmission. Figure 5 shows the control flow for handling lost messages, where the MCP retransmits messages with the correct sequence numbers to avoid message loss.

## 4. Implementation and Performance
In this section, we discuss the implementation details of our fault tolerance scheme and report on its performance and overhead. The fault detection mechanism is based on a software watchdog timer that monitors the network processor. When a hang is detected, the backup state is used to restore the network interface. Our experimental results on the Myrinet platform show that complete fault recovery can be achieved in under 2 seconds, with a latency overhead of just 1.5 µs during normal operation. The fault tolerance scheme is designed to be transparent to the user, ensuring that the application continues to function without interruption.

## 5. Conclusion
In this paper, we presented an effective, low-overhead fault tolerance technique for network interface failures, focusing on network processor hangs. Our approach involves maintaining a minimal backup of the network interface state and using a software watchdog for fault detection. Experimental results on the Myrinet platform demonstrate the effectiveness of our technique, achieving complete fault recovery in under 2 seconds with minimal overhead. The fault tolerance scheme is designed to be transparent to the user, making it suitable for high-availability and reliability requirements in modern network systems. Future work will explore the applicability of this technique to other network technologies and further optimize the performance and overhead.

**Acknowledgments:**
This work was supported in part by the National Science Foundation under Grant CCR-0234363.

**References:**
[1] - [15] (References should be included here, following the appropriate citation style)

---

**Note:** The references [1] to [15] should be added at the end of the document, formatted according to the appropriate citation style.