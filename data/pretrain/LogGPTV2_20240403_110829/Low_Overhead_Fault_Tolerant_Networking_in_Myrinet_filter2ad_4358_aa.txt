title:Low Overhead Fault Tolerant Networking in Myrinet
author:Vijay Lakamraju and
Israel Koren and
C. Mani Krishna
Low Overhead Fault Tolerant Networking in Myrinet∗
Vijay Lakamraju, Israel Koren and C.M. Krishna
Department of Electrical and Computer Engineering
E-mail: {vlakamra,koren,krishna}@ecs.umass.edu
University of Massachusetts, Amherst MA 01003
Abstract
Emerging networking technologies have complex net-
work interfaces that have renewed concerns about net-
work reliability.
In this paper, we present an eﬀec-
tive low-overhead fault tolerance technique to recover
from network interface failures, more particularly net-
work processor hangs. We demonstrate the technique
in the context of Myrinet. Fault recovery is achieved
by restoring the state of the network interface using
a small backup copy containing just the right amount
of information required for complete recovery. Our
fault detection is based on a software watchdog that de-
tects network processor hangs. Results on the Myrinet
platform show that the complete fault recovery can be
achieved in under 2sec while incurring a latency over-
head of just 1.5µs during normal operation. The paper
also shows how this fault recovery can be made com-
pletely transparent to the user.
1 Introduction
The complexity of network hardware has increased
tremendously over the past couple of decades. This is
evident from the amount of silicon used in the core of
network interface hardware. A typical dual-speed Eth-
ernet controller uses around 10K gates whereas a more
complex high-speed network processor such as the Intel
IXP1200 [8] uses over 5 million transistors. This trend
is being accentuated by the demand for greater net-
work performance, and so communication-related pro-
cessing is increasingly being oﬄoaded to the network
interface. Nowadays, interfaces with a network proces-
sor and large local memory are not uncommon. For
example, the Myrinet host interface card uses a cus-
tom 32-bit RISC processor core and onboard SRAM
ranging from 512K to 8M bytes.
1This work has been supported in part by NSF under Grant
CCR-0234363.
Unfortunately, such increased complexity renews
concerns regarding reliability and availability of the
system. Network interface hardware is prone to the
same type of failures as the host hardware. Network
interface failures can however be more detrimental to
the reliability of a distributed system. As we will see
in the next section, faults can cause the network pro-
cessor to hang, causing the node to be cut-oﬀ from the
rest of the system. Not only that, faults can also cause
the host computer to crash/hang, and worse still, can
sometimes even aﬀect a remote network interface. So,
detecting and recovering from such network interface
failures as quickly as possible is crucial for a system
requiring high availability and reliability.
In this paper, we present an eﬃcient, low-overhead
fault-tolerance technique for network interface failures.
The focus will be on a speciﬁc type of failure, namely
network processor hangs. The central philosophy be-
hind our technique is to keep around enough network-
related state information in the host so that the state
of the network interface can be correctly re-established
in the case of a failure. Clearly, the challenge in such
a scheme is to provide for this “checkpointing” with as
little performance degradation as possible. Having the
host checkpoint the state of the network interface (in
the classical sense) can substantially degrade the per-
formance of the system. In our technique, the ”check-
pointing” is a continuous process in which the appli-
cations make a copy of the required state information
before sending the information to the network interface
and update it when the network notiﬁes the applica-
tion that the state information is no longer required.
As the results will show, such a scheme greatly reduces
the impact on the normal performance of the system.
Our technique also incorporates a quick fault detec-
tion scheme based on software implemented watchdog
timers. We believe that both the fault detection and
recovery techniques are general enough to be applicable
to many modern network technologies, primarily those
that are microprocessor-based.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 06:56:36 UTC from IEEE Xplore.  Restrictions apply. 
Before we detail our fault tolerance technique in Sec-
tions 3 and 4, we will brieﬂy describe Myrinet, which
is the platform for this case study and report on fault
injection experiments that expose the vulnerability of
such microprocessor-based network systems to faults.
In Section 5, we discuss some implementation details
and report on the performance and overhead of the
fault tolerance scheme. We conclude the paper in Sec-
tion 6.
2 Myrinet: An Example System
Myrinet [3] is a cost-eﬀective, high-bandwidth (2
Gb/s), packet-communication and switching technol-
ogy from Myricom Inc [11].
It employs wormhole
switching, backpressure ﬂow control and source routing
to achieve low-latency (∼ 8µs) transfer of messages. A
Myrinet network consists of point-to-point, full-duplex
links that connect Myrinet switches to Myrinet host
interfaces and other Myrinet switches.
The Myrinet host interface card is designed to pro-
vide a ﬂexible and high performance interface between
a generic bus, such as the SBus or PCI, and a Myrinet
link. Figure 1 shows the organization and location of
the Myrinet host interface card in a typical architec-
ture. At the center of the microarchitecture is a chip,
called the LANai, which contains a RISC processor,
(Direct Memory Access) DMA logic (packet interface)
to/from the network, External (or E) bus interface logic
to/from the host, timers, and local conﬁguration reg-
isters. The fast local synchronous memory (SRAM)
is used to store the Myrinet Control Program (MCP)
and for packet buﬀering. The MCP is the program
that runs on the LANai RISC processor and provides
the basic functionality for reliably transfering messages
from the host to the Myrinet link.
Host
Processor
System
Bridge
System
Memory
64/32 bit, 66/33 MHz PCI Bus
Timers
IT0
IT1
IT2
Myrinet Host Interface Card
Fast Local Memory
Address
64−bit data
PCI
Bridge
DMA
Controller
PCIDMA chip
Host
Interface
Packet
Interface
RISC
LANai 9
Myrinet
SAN
Link
SAN/LAN
Conversion
Myrinet
LAN
link
Figure 1. Myrinet host interface card
The Myrinet host software consists of a device driver
that runs as part of the host operating system and a
user library that provides a light-weight communica-
tion layer and API for the application software. The
device driver provides important I/O device related in-
terfaces to the user library, such as port opening and
closing, memory mapping, interrupt handling and load-
ing the MCP. The ability to change the network be-
havior through the MCP has made possible the imple-
mentation and testing of a number of communication
protocols on Myrinet, including Active Messages [4],
Fast Messages [12] and BIP [13]. Myricom’s own soft-
ware, called GM, borrows many features from these
protocols and is currently the most widely used and
preferred software for Myrinet. All these protocols are
low-overhead protocols that avoid operating system in-
tervention by providing a zero-copy mode of operation
directly from the user space to the network (Figure 2).
The scheme, however, requires that the user virtual
memory on both the sending and the receiving side be
pinned to an address in physical memory so that it
will not be paged out during DMA carried out by the
network interface. Thus, a user program employs sys-
tem calls to allocate a number of unswappable pages of
memory used for data-exchange, and thereafter avoids
system calls.
Applications
VIA
MPI
BIP
GM
Middleware
UDP
TCP
Host
OS
IP
Ethernet
Myrinet
10/100/1000 Mb/s
OS−bypass
zero−copy
APIs
GM
Driver
(Executes in 
the Myrinet
interface)
Myrinet Control Program (MCP)
1280/1280 Mb/s
Figure 2. Myrinet software
latency,
Apart from the small
two other fea-
tures make GM worth considering in distributed sys-
tems: reliable in-order delivery of messages and self-
conﬁguration. GM automatically handles transient
network errors such as dropped, corrupted or misrouted
packets. This handling is done transparent to the user
and is mainly carried out in the MCP. For conﬁguring
the network, a program called the GM mapper is run on
a node. The GM mapper initiates the mapping process
and at the end of the mapping protocol, each interface
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 06:56:36 UTC from IEEE Xplore.  Restrictions apply. 
has a map of the network and routes to all other inter-
faces stored in its local memory. The GM mapper can
also reconﬁgure the network if links or nodes appear
or disappear. Inspite of these fault tolerance features,
as we show next, Myrinet/GM may still not be favor-
able for systems requiring high availability for special
applications, like the NASA REE supercomputer [6].
All of Myrinet’s high availability features assume
that the LANai processor executes error-free. This can
be a very costly assumption to make in some applica-
tions, more particularly space applications. For exam-
ple, a cosmic ray could cause an instruction in the MCP
to ﬂip a bit which could make it an invalid instruction.
When the LANai executes the invalid instruction, it
could simply crash. In some cases, the host applica-
tion accessing the Myrinet card also hangs and hence
it may be required to restart the application/machine.
Such were the type of eﬀects seen in fault injection
experiments performed by our research group as well
as other research groups [15]. Transient faults in the
network processor were simulated by ﬂipping bits ran-
domly in the code segment of the MCP. Rather than
injecting faults into the entire LANai SRAM, one sec-
tion of the MCP code, namely send chunk, was selected
and for each experiment, a fault was injected at a ran-
dom bit location in this section while it was handling
some network communication. Since send chunk corre-
sponds to a serial piece of code that is executed by the
LANai each time a message is sent out, we are assured
that all the faults are activated. Table 1 shows a sum-
mary of the results from these experiments and those
reported in [15]. The network hardware (LANai 9) and
software (GM 5.1) used in our experiments is the latest
(as of this writing) and hence validates a more recent
Myrinet technology.
Table 1. Results of fault injection on a Myrinet
system (1000 runs)
Failure Category
Local Interface Hung
Messages Corrupted
Remote Interface Hung
MCP Restart
Host Computer Crash
Other Errors
No Impact
% of Injections
Our work
28.6
18.3
0.0
0.0
0.6
1.2
51.3
Iyer et al.[15]
23.4
12.7
1.2
3.1
0.4
1.1
58.1
It is clear from the table that interface hangs and
dropped/corrupted messages account for more than
90% of the failures that aﬀect the network interface
in some undesirable way. Surely, these results could
be diﬀerent if fault injection is carried out on some
other section of the code, but the results give a ﬂa-
vor of the diﬀerent types of failures one can expect.
A interface hang could mean that the LANai simply
stopped executing instructions or that it has entered
into an inﬁnite loop , causing it to stop responding to
user requests. While dropped/corrupted messages are
already well handled by the GM software, there is no
easy way to correctly recover from interface hangs. The
driver could be reloaded and the application restarted
from a safe checkpoint (if there is one). But, as we shall
see in the next section, this does not always ensure cor-
rect recovery. Middleware, such as MPI, built on top
of GM, consider GM send errors to be fatal and exit
when they encounter such errors. This can cause a dis-
tributed application using MPI to come to a grinding
halt if proper fault tolerance is not implemented.
The table also shows other types of failures, such
as host computer crashes, which are caused by faults
that propagate from the network interface to the host
system. While we will not concern ourselves with such
types of failures in this paper, the point of all this dis-
cussion is to show that faults aﬀecting the network in-
terface can have a substantial eﬀect on the reliability
of the system. For the rest of the paper, we will con-
centrate on interface hangs.
3 Recovery Strategy
Recovery from a host interface failure primarily in-
volves restoring the state of the interface to what it
was before the failure. Simply resetting the interface
and reloading and restarting the MCP would not be
suﬃcient as it can cause messages to be lost or dupli-
cate messages to be received. In order to elucidate this
aspect, let us take a closer look at the programming
model and design of GM.
3.1 GM Programming Model
Communication between user processes in separate
nodes using GM takes place through endpoints called
“ports” with two non-preemptive priority levels for
messages. The programming model is “connectionless”
in that there is no need for the client software to estab-
lish a connection with a remote port in order to commu-
nicate with it. The sender simply allocates DMAable
memory, initializes the memory segment and informs
the LANai that the message needs to be sent out. The
receiver, on the other hand, allocates DMAable mem-
ory and notiﬁes the MCP that it is ready for receiving.
The MCP is responsible for the rest of the communica-
tion process, i.e., DMAing the contents of the message
from/to host memory, building the packet according
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 06:56:36 UTC from IEEE Xplore.  Restrictions apply. 
to the Myrinet speciﬁcations, setting up connections,
sending and receiving the packet and ensuring reliable
in-order delivery of messages. A “connection” corre-
sponds to a logical link to a remote node which the
MCP uses to multiplex all the traﬃc to that node.
The MCP uses a version of the Go-Back-N protocol
to handle transient network errors such as dropped,
corrupted, or misrouted packets and ensure in-order
delivery of packets over each connection. Finally, the
MCP informs the user process of a message arrival or a
successful send by posting an event in its event queue.
Flow control in GM is managed through a token
system, similar to that used in credit-based ﬂow con-
trol [10]. Both sends and receives are regulated by
implicit tokens, which represent space allocated to the
user process in various internal GM queues. A send to-
ken consists of information about the location, size and
priority of the send buﬀer and the intended destination
for the message. A receive token contains information
about the receive buﬀer such as its size and the prior-
ity of the message that it can accept. A process starts
out with a ﬁxed number of send and receive tokens
and relinquishes a send token each time it calls GM
API’s gm send() function and a receive token with a
call to gm provide receive buﬀer(). A send token is im-
plicitly passed back to the process when its callback
function is called and a receive token is passed back
when a message is received from the receive queue us-
ing the gm receive() call. Apart from the notiﬁcation
of received messages and completion of sends, the re-
ceive queue is also used for other sundry purposes such
as alarms. There are other GM internal events which
a process is not expected to handle and can simply
pass them to gm unknown() which handles them in a
default manner. This programming style allows maxi-
mum overlap between computation and asynchronous
communication. Figure 3 shows the schematic of a typ-
ical control ﬂow in a GM application.
Sender
Receiver
3.1.1 Duplicate Messages
Reliable transmission in GM is achieved through the
use of sequence numbers and these sequence numbers
are maintained solely by the MCP and are therefore
transparent to the user. If the MCP is simply reloaded
and restarted on failure, the state of the connections
and the sequence numbers are lost. This loss of infor-
mation does not allow the messages to be retransmitted
reliably. Consider the example shown in Figure 4. A
sending node crashes when an ACK is in transit. After
recovering from the failure, since all state information
is lost, the sender may try to resend the message with a
invalid sequence number. The receiver would reply by
sending a NACK with the expected sequence number.
At this point, if the sender resends the messages with
this sequence number, the receiver would incorrectly
accept a duplicate message.
Sender
Receiver
User process prepares message
User process sets send token
User process provides receive buffer
User process sets recv token
LANai sdmas message
LANai sends message
LANai goes down
LANai receives message
LANai sends ACK
LANai rdmas message
LANai sends event to user process
Driver reloads MCP
Driver sets pending send tokens
User process handles notification event
User process reuses send buffer
LANai sdmas message
LANai sends message
LANai receives NACK
LANai resends message
LANai receives message
LANai sends NACK
LANai receives duplicate message
Figure 4. The case of duplicate messages
This problem arises due to the lack of redundant
state information. If information of all streams of se-
quence numbers was stored in some stable storage, then
the MCP could use this redundant information dur-
ing recovery to send out messages with the correct
sequence number and avoid the problem of duplicate
messages. The key, however, is to manage redundancy
so that the performance of the network is not impacted
greatly.
User process prepares message
User process sets send token
User process provides receive buffer
User process sets recv token
3.1.2 Lost Messages
LANai sdmas message
LANai sends message
LANai receives message
LANai sends ACK
LANai receives ACK
LANai sends event to user process
LANai rdmas message
LANai sends event to user process
User process handles notification event
User process reuses send buffer