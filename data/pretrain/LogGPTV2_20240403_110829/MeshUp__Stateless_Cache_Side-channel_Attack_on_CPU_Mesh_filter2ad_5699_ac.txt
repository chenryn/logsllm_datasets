and let the attacker enumerate every core on his/her own CPU,
and measure its access time to an EV . When the serving LLC
slice is local to the core, the access latency is the lowest, and
we link EV to the LLC slice ID.
Finally, the size of EV (n) has to be tuned carefully by
the attacker. Not only n should be larger than w to force L2
misses, but it should also avoid being too large to overﬂow
LLC and L2 together. Otherwise, a line will be evicted from
LLC to memory (T4 trafﬁc), a request that takes a long time
to respond, reducing the probing frequency. As the Xeon SP
uses 11-way LLC set and 16-way L2 set [9], we varied n of
EV from 18 to 38 (2*11+16), and found when n equals to
24 (EV0 and EV1 each has 12 addresses), the mesh trafﬁc
generated within an interval is the highest.
For the attack prototype, we exploit a CPU feature called
huge page [58], which allows pages of more than 4KB to
be accessed, to make the attacker’s measurement more stable.
However, this assumption is not necessary during the real-
world attack as Vila et al. has a solution for EV construction
with normal pages [59].
Delay Measurement. When the victim application is running,
the attacker sequentially visits every address within EV im-
mediately after receiving the response to the prior request,
and records the timestamp of each request (e.g., using the
instruction RDTSCP to read the CPU counter). The interval
between consecutive requests reﬂects the cache transaction
latency. When the interval is increased, the victim is supposed
to have cache transactions concurrently. To cope with random
noises, the attacker repeatedly visits the EV for x times (we
set x to 20 during the experiment) to obtain a sample for
an interval, and analyzes the interval trace to infer the access
patterns of the victim applications.
The Pseudo-code. Here we summarize the probe in Algo-
rithm 1. The process of EV generation is adapted from [22],
by setting the size of the set of candidate EV (EV s) three
times as the number of LLC slices,
in order to increase
the chance of getting an EV for a designated LLC slice.
This size can be adjusted based on the running environment.
          Mesh LinkVictimCoreAttackCore            Victim TrafficLLC SliceProbe TrafficAlgorithm 1: The pseudo-code of eviction-based probe
Result: IntervalSeq
path = rand path();
L2 set index = rand(0x3ff);
for i in 3* (num of LLC slices) do
EV s.append(get EV( L2 set index));
end
// select an EV
set afﬁnity(path.dst);
for each EV in EV s do
access(ﬁrst addr of EV );
if access time ≤ TH then
break;
end
end
// start attack
set afﬁnity(path.src);
while True do
for i in range(20) do
access(EV );
end
IntervalSeq.append(access time);
end
The get_EV() function employs check_conflict and
find_EV to ﬁnd an EV for an LLC slice. The attacker
uses set_affinity() to place its program to a random
core (source) and repeatedly test if an EV is in the desired
LLC (destination). Noticeably, calling set_affinity()
from VM will not pin the program to a desired core. We
use this routine to make the core ID constant. Sometimes,
core remapping could move the applications to different cores
during execution, and we discuss its impact in Section VII-B
(“Core Remapping”).
B. Probe based on Cache Coherence
The eviction-based probe can generate sufﬁcient and steady
mesh trafﬁc on a single CPU, but cannot produce enough
cross-socket trafﬁc that is needed for the cross-CPU attack.
When we test the eviction-based probe on a server with two
CPUs and let a core evict lines to an LLC slice in the remote
socket, we found the CPU core does not always evict the
desired cache lines. Instead, the core may just drop the lines
if they are clean. In this case, accessing the eviction set only
triggers T2 trafﬁc, so some accesses related to victim memory
may not be captured by the probe.
By inspecting the different types of trafﬁc, we found if an
attacker intentionally triggers communications through UPI,
cross-core T5 trafﬁc can be captured, so we design the probe
accordingly. When two different cores keep writing to the
same cache line, the CHA will be busy maintaining the cache
coherence (under MESIF described in Section II-A), and the
interconnects would be ﬁlled with T5 messages synchronizing
the line. The completion time of such cache accesses can be
inﬂuenced by the victim’s trafﬁc.
We design a cross-CPU probe based on this insight. The
attacker ﬁrst occupies a core and constructs a set of cache
lines that are all mapped to an LLC slice of the core. Then
he/she starts a thread in the core (core A) to keep sequentially
writing the lines with random values. At the same time, the
attacker starts a thread in the remote socket (core B) to also
sequentially write the lines within the set, and records the time
spent on accessing the set. In this way, every cache line to be
accessed by core B is expected as Invalid, because core A
should have written the line, during which core B turned the
line to the invalid state and core A reaches the exclusive state.
As a result, writing the invalid line causes messages sent to
the CHA in core A, and core A will send the line to core
B, resulting in bi-directional UPI trafﬁc. Both cores of the
attacker can record the timestamps to infer the victim access
pattern. Algorithm 2 shows the pseudo-code of this probe. A
prominent advantage of this probe is that the contention to
the victim trafﬁc through the UPI path is deterministic, so all
the measured delay traces can be used for inference, without
worrying about path co-location.
Algorithm 2: The pseudo-code of coherence-based
probe
Result: IntervalSeq
path = rand cross CPU path();
set afﬁnity(path.dst);
for i in 3* (num of LLC slices) do
AS = rand slice addr set();
access(ﬁrst addr of AS);
if access time ≤ TH then
break;
end
end
if clone() == parent then
while True do
access(AS);
end
else
set afﬁnity(path.src);
while True do
access(AS);
IntervalSeq.append(access time);
end
end
According to our exploratory experiment, 4 pairs of probes
can saturate the two UPI links by 93%. In this case, extra
cross-socket trafﬁc made by the victim can cause congestion
and enlarge the attacker’s probe delays.
Our probe differs from the previous cross-CPU channels.
DRAMA exploits the contention on memory shared across
CPUs rather than cache [42]. Irazoqui et al. exploits the
directory protocol of CPU interconnects
[60], and attacks
7
QuickPath Interconnect (QPI) [61] on Intel CPU. Our probe
attacks UPI, which succeeds QPI in Intel SP. We also believe
our probe has the potential to be generalized to other CPU
interconnect, like ring on old CPUs which use QPI, but the
channel quality might be downgraded.
VI. ANALYSIS OF THE MESHUP SIDE-CHANNEL
In this section, we provide quantitative analysis on the
probes described in the prior section. We ﬁrst describe an
approach to reverse engineer CPU layout and leverage it
to examine the variance of delays caused by the contention
between the victim and attacker applications. Then, we inspect
the spatial and temporal distribution of the mesh trafﬁc. We
also identify the reasons for the delay increase resulting from
mesh congestion, according to CPUs’ performance counters.
A simple mitigation based on LLC slice isolation is tested
against MESHUP probe. Finally, we summarize our insights
into the MESHUP side-channel.
A. CPU Layout Reverse Engineering
For the quantitative analysis, we aim to enumerate various
combinations of attacker’s and victim’s mesh trafﬁc. The CPU
layout needs to be reverse engineered, so that we can pin a
program to the desired core and direct it to talk to another.
To notice, this step requires root privilege to execute some
proﬁling instructions, but during the actual attack described,
this step is not taken.
We have reverse engineered Intel Xeon Scalable 8260 and
8175. Due to the space limit, we describe the layout of 8175
in Appendix D.
Identifying the Enabled Tiles. There are three types of CPU
dies, named LCC, HCC, XCC (for low, high, and extreme
core counts), for Intel Xeon family, with 10, 18, or 28 cores
in a die respectively [62]. However, when a CPU is shipped
to the customers, Intel might intentionally disable some cores.
For example, the Xeon Scalable 8260 has 24 active cores,
because Intel disabled 4 cores.
Here, we exploit the hardware features of Intel CPUs to
reveal such information. According to Intel’s document [63],
a user can query the CAPID6 register to learn the ID of the
tile whose CHA is disabled. When CHA is disabled, the whole
tile including the core and LLC inside are also disabled. Take
Xeon Scalable 8260 as an example. Its CAPID6 contains 28
bits to indicate the status of all tiles, and a CHA is disabled
if its associated bit is 0. By reading all bits of CAPID6, we
found bit 2, 3, 21 and 27 are set to 0. A previous research [64]
suggests tiles are numbered from north to south on each
column and the west column is the smallest, so we number all
the tiles and mark the disabled tiles based on the CAPID6 bits.
Table II shows the tile IDs and the disabled ones (in gray).
Mapping CHAs and Cores to Tiles. Next, we try to infer the
relation between the core/CHA IDs and the tile IDs. According
to [64], CHAs are sequentially numbered along with the tiles,
but when a tile is disabled, the CHA ID is skipped. As such,
CHA is numbered from 0-23 for Xeon Scalable 8260, and tile
#4 has CHA #2 because tile #2 and #3 are disabled.
UPI
0
IMC0
1
2
3
PCIE
4
5
6
7
8
PCIE
9
10
11
12
13
RLINK
14
15
16
17
18
UPI2
19
20
21
22
23
PCIE
24
IMC1
25
26
27
TABLE II: Layout of Xeon Scalable 8260 CPU. Gray cell
indicates the tile is disabled.
UPI
0, 0
IMC0
1,12
PCIE
2, 16
3, 18
4, 1
5, 13
6, 7
PCIE
7, 19
8, 2
9, 14
10, 8
11, 20
RLINK
12, 3
13, 15
14, 9
15, 21
16, 4
UPI2
17, 16
18, 10
19, 22
20, 5
PCIE
21, 17
IMC1
22, 11
23, 23
TABLE III: The IDs of CHAs and cores of Xeon Scalable
8260 CPU.
Regarding cores, the task becomes non-trivial, as they are
not sequentially numbered. McCalpin proposed a method to
infer how the cores are aligned by reading “mesh trafﬁc
counters” [64]. However, the author also admitted the result
needs to be disambiguated [65]. To improve the accuracy of
the inferred layout, we propose a new method. Speciﬁcally, we
bind a thread to a core (by setting its afﬁnity [66] to the core
ID), and use it to access 2GB memory. We monitor the per-
formance counter LCORE_PMA GV (Core Power Management
Agent Global system state Value) of Intel PMU [63] and found
the CHA yields the highest value when it co-locates with the
core in the same tile. We assign the core ID to the tile with
CHA with the highest LCORE_PMA GV reading. We repeat
the process for every core, and the layout can be reconstructed.
In Table III, we show the inferred CHA and core IDs for Xeon
Scalable 8260.
B. Temporal Resolution
The key assumptions of MESHUP are that 1) the probe delay
increases when the mesh links are congested; 2) the granularity
of memory access is sufﬁcient to introduce noticeable mesh
trafﬁc. Here we try to validate these assumptions. We ﬁrst
inspect the delay traces resulting from on-off style memory
access. If the access pattern can indeed be recognized, the
delay sequence should resemble a square wave, where a rise
is related to contention.
Leveraging the result of Section VI-A, we ﬁx a sender
process at CHA 0 and force the process to access the memory
mapped to LLC slice at CHA 21, which generates mesh trafﬁc
along the top horizontal row. The sender process accesses
memory for 15us and then rests for 15us periodically (i.e.,
Duration T = 30us), simulating on-off access. We place 3
eviction-based probes as a receiver at the top horizontal path
of the mesh structure, i.e., from CHA 2 to 17, from 17 to 2,
and from 7 to 12.
Results. We gradually decrease T (from 30 to 5 and 2) and
Figure 5 shows the observed interval traces. As we can see,
the trafﬁc made by the sender indeed causes delay increase
by up to 50%, and it disappears every time the sender stops