# Failure Analysis of Virtual and Physical Machines: Patterns, Causes, and Characteristics

**Authors:**  
Robert Birke, Ioana Giurgiu, Lydia Y. Chen, Dorothea Wiesmann, Ton Engbersen  
IBM Research Zurich Lab, Rüschlikon, Switzerland  
Email: {bir, igi, yic, dor, apj}@zurich.ibm.com

**Conference:**  
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2014)  
DOI: 10.1109/DSN.2014.18  
Copyright: 978-1-4799-2233-8/14 $31.00 © 2014 IEEE

## Abstract
In today's commercial datacenters, the computational density is continuously increasing as the number of hardware components and virtual machine (VM) workloads grows. Service availability in these datacenters heavily depends on the reliability of both physical and virtual servers. This study analyzes 10,000 virtual and physical machines hosted across five commercial datacenters over a one-year period. Our objective is to establish a comprehensive understanding of the differences and similarities between failures in physical and virtual machines. We first capture their failure patterns, including failure rates, distributions of times between failures, repair times, and the time and space dependencies of failures. We then correlate these failures with resource capacity and runtime usage to identify the characteristics of failing servers. Finally, we discuss how VM management actions, such as consolidation and on/off frequency, impact VM failures.

**Index Terms:** Datacenters, VM failures, failure root causes

## I. Introduction
Modern commercial datacenters, which host a variety of services, typically consist of a large number of physical and virtual systems. Ensuring high availability of these services requires stringent reliability standards for all system aspects, including hardware, software, network, and power. However, failures are more common than exceptions in large-scale datacenters with millions of components. Maintaining system uptime is challenging, especially as datacenter scale and complexity increase due to virtualization technology. A deep understanding of the patterns and causes of failures in both physical and virtual systems can lead to more efficient solutions, enhancing datacenter reliability and service objectives.

Significant research has been conducted on understanding hardware reliability, such as disks, CPUs, and RAM, in personal desktops/laptops, cloud datacenters, and high-performance computing (HPC) systems. However, collecting and analyzing failure-related data in commercial datacenters is more challenging due to the highly dynamic nature of workloads and distributed systems. For example, gathering server failure logs from numerous distributed ticketing and performance databases is a significant hurdle. Despite the widespread use of VMs in cloud datacenters, existing research provides little insight into VM reliability, such as failure rates, dependencies, and correlations with resources and management.

In this paper, we conduct a large-scale analysis comparing failures in physical machines (PMs) and virtual machines (VMs) using field-collected problem tickets and resource performance measures from commercial datacenters, covering five subsystems with approximately 10,000 hosts. Our main focus is to characterize failure patterns and identify factors causing failures in both PMs and VMs, highlighting their similarities and differences. We correlate failure rates with the capacity and usage of multiple resources, such as CPU, memory, disk, and network, as well as VM resource management, such as consolidation and on/off frequency. Additionally, we classify failures by root causes, including hardware, network, reboot, power, and software. Our study aims to provide straightforward statistics that facilitate an intuitive understanding of failure behaviors and causes in production datacenters.

Despite some limitations, such as the lack of physical location information, asymmetric distribution of VM and PM populations, and inconsistent clarity and granularity across different data sources, our analysis is based on a large collection of failure events and considers a comprehensive set of failure types.

Our study is divided into three parts:
1. Overview of PM/VM failure patterns.
2. Dependency of PM/VM failure rates on resource capacity and usage.
3. Correlation of VM failures with VM resource management.

The main building blocks of our analysis include failure rates, random and recurrent failure probabilities in multiple time windows (days, weeks, and months). We present the failure rates, distribution of inter-failure times per server, distribution of repair times, and the time and space dependency of subsequent failures. To identify the server characteristics that cause frequent failures, we compare failure rates across different resource capacities and usages, focusing on the number of CPU processors, memory size [GB], disk capacity [GB], number of disks, CPU utilization [%], memory utilization [%], and network traffic [MB/S]. The last part of our analysis examines how VM-specific resource management actions, such as consolidation and on/off frequency, affect the VM failure rate.

The contributions of this paper are twofold:
1. To the best of our knowledge, this is the first extensive analysis of VM failure in commercial datacenters compared to PMs.
2. Our study considers a diverse set of failures and an extensive set of factors that correlate with PM and VM failures, including multiple types of resource capacity and usage, as well as VM resource management.

The outline of this work is as follows:
- Section II presents related work.
- Section III provides an overview of our dataset.
- Sections IV and V present the study of failure patterns and their dependency on resource capacity and usage, respectively.
- Section VI discusses the impact of VM resource management.
- Section VII concludes the paper.

## II. Related Work
System reliability is a critical concern in datacenters, as service unavailability directly results in significant business revenue loss. Considerable efforts have been invested in understanding hardware failures and their root causes through large collections of failure logs. The related work can be categorized into studies focusing on:
1. Individual hardware subsystems, particularly disk failures.
2. HPC systems.
3. Laptop or desktop machines.
4. Cloud datacenters.

Many studies have characterized the reliability of hardware subsystems, such as CPUs, DRAM, and disks. A common finding is that disks have the highest failure rates, with an average annual disk failure rate of 2-4%, much higher than suggested in product datasheets. Disk failures increase linearly with age, and there is no significant correlation with high temperature or utilization. Self-monitoring facility parameters in drives show strong correlations with failure probability.

HPC systems, with increasing area density and component counts, also experience higher failure rates. Failure rates in HPC systems are positively correlated with the number of processors and the type and intensity of workloads. Statistical models, such as Gamma and Weibull distributions, are often used to capture the distribution of times between failures and repair times. Root causes are typically classified into six categories: hardware, software, network, environment, human, and unknown. Power-related failures, classified under environment, induce a high probability of follow-on failures.

In contrast, few studies focus on commercial systems, ranging from laptop/desktop machines to datacenter servers. Common findings for HPC and commercial systems are that failures are not memoryless, meaning the probability of follow-on failures is much higher than that of random failures. For laptops and desktops, overclocking the CPU speed increases failure rates for CPUs, memory, and disks, and brand-name systems generally have better reliability.

In commercial servers, disks are the most frequently replaced components, and server failure rates increase with the number of disks. Factors predicting failures include datacenter location and manufacturer brand name, rather than server age and configuration. However, these studies focus primarily on hardware-related failures and overlook software and environment-related failures, which account for a significant number of failures in both commercial and HPC systems.

Our study captures failure rates for both PMs and VMs, considering a comprehensive set of failure types and identifying relevant factors, such as resource capacity and usage, server age, and consolidation. Table I summarizes the scope of our analysis compared to previous work, noting that related work only considers a subset of resources in their capacity and usage studies. Our findings on PM failures align with earlier observations, while our findings on VM failures and their comparison with PM failures complement many existing hardware-reliability studies.

## III. Data Collection Methodology
Our reliability study is based on data collected from five commercial datacenter subsystems from July 2012 to June 2013. Each subsystem consists of non-virtualized PMs and VMs hosted on virtualized boxes. Throughout the analysis, we focus on statistics related to PMs and VMs, excluding statistics on boxes due to limited data access. These machines span a wide range of architectures (e.g., HP, IBM, Dell), run major operating systems (e.g., Linux and Windows), and vary in hardware age. Incidents are reported by users or automatically generated by monitoring tools, such as HP OpenView and IBM Tivoli Monitoring, and collected through the ticketing system.

Our dataset consists of 2759 crash tickets spread across 4292 VMs and 5129 PMs. Table II summarizes the detailed statistics across the five subsystems, including the number of PMs and VMs, the number of all problem tickets, and the number of crash tickets.

### A. Data Collection Process
We faced several challenges in gathering and sanitizing the measurements of interest that correlate with server failures, such as resource capacity and usage levels. Data was collected from various sources, including ticket and server resource monitoring databases, each with its own coverage and logging accuracy. For example, the server resource monitoring database uses two-year observation periods with data recorded every 15 minutes, hourly, daily, weekly, and monthly, while the ticket database mainly uses one-year observation periods with data recorded by events.

To obtain the dataset, we first identified crash tickets among all collected tickets. Each ticket contains a text description of the problem, possible causes, resolution, and repair duration. Due to inconsistencies in descriptions and resolutions, we applied manual labeling and k-means clustering on both the description and resolution fields. After manually checking the classification, our k-means clustering achieved an 87% accuracy.

We then classified the crash tickets into six finer-grained classes based on their resolutions:
- **Network-related failure**: Server failures caused by network issues requiring a network fix.
- **Hardware-related crashes**: Server failures caused by hardware malfunctions (e.g., faulty disk or battery, broken power supply) requiring hardware replacement or fix.
- **Software-related failure**: Server failures caused by OS or application-level issues (e.g., hanging OS or critical service agent) requiring a software fix.
- **Power-outage-related failure**: Server failures caused by power outages requiring an electrical fix.
- **Reboot-related failure**: Server failures caused by unexpected reboots.
- **Other failure**: Server failures that cannot be classified into any of the above classes due to less accurate ticket descriptions and resolutions.

This classification is essential for gaining a deep understanding of the patterns and characteristics of server failures. Figure 1 shows the failure distribution across the different classes.