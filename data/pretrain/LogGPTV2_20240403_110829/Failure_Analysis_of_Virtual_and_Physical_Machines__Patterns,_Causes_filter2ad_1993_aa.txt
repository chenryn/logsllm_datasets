title:Failure Analysis of Virtual and Physical Machines: Patterns, Causes
and Characteristics
author:Robert Birke and
Ioana Giurgiu and
Lydia Y. Chen and
Dorothea Wiesmann and
Ton Engbersen
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Failure Analysis of Virtual and Physical Machines:
Patterns, Causes and Characteristics
Robert Birke, Ioana Giurgiu, Lydia Y. Chen, Dorothea Wiesmann, Ton Engbersen
IBM Research Zurich Lab, R¨uschlikon, Switzerland, Email: {bir,igi,yic,dor,apj}@zurich.ibm.com
Abstract—In today’s commercial datacenters, the computation
density grows continuously as the number of hardware compo-
nents and workloads in units of virtual machines increase. The
service availability guaranteed by datacenters heavily depends on
the reliability of the physical and virtual servers. In this study, we
conduct an analysis on 10K virtual and physical machines hosted
on ﬁve commercial datacenters over an observation period of
one year. Our objective is to establish a sound understanding of
the differences and similarities between failures of physical and
virtual machines. We ﬁrst capture their failure patterns, i.e., the
failure rates, the distributions of times between failures and of
repair times, as well as, the time and space dependency of failures.
Moreover, we correlate failures with the resource capacity and
run-time usage to identify the characteristics of failing servers.
Finally, we discuss how virtual machine management actions,
i.e., consolidation and on/off frequency, impact virtual machine
failures.
Index Terms—Datacenters, VM failures, failure root causes
I. INTRODUCTION
Today’s commercial datacenters hosting various services
are typically composed of a large number of physical and
virtual systems. To ensure high availability of hosted ser-
vices, datacenters face stringent requirements on all aspects
of system reliability,
i.e, hardware, software, network and
power. However, failure is more the norm than an exception
in largescale datacenters with millions of components [1]. All
in all, it is no mean feat to keep the systems up all the time
especially when the scale of datacenters continuously grows,
an issue compounded by the increasing system complexity due
to virtualization technology. Therefore, a deep understanding
of the patterns and causes of failures on both physical and
virtual systems can lead not only to efﬁcient solutions, which
increase datacenter reliability, but also to a better fulﬁllment
of the service objectives.
There exist signiﬁcant research efforts on understanding
hardware reliability, e.g., disk, CPU, and RAM, for personal
desktops/laptops [2], cloud datacenters [3], and particularly
HPC systems [4], [5]. As commercial workloads are highly
dynamic and systems are typically highly distributed, collect-
ing and analyzing failure-related data is more challenging in
commercial datacenters than in HPC systems. For example, the
very ﬁrst hurdle failure analysis on commercial datacenters has
to take is the gathering of server failure logs by mining a large
number of distributed ticketing and performance databases.
Consequently, how servers fail in commercial datacenters is
little known, except for the recent study of [3]. Given the wide
deployment of services on virtual machines (VMs) in cloud
datacenters, it is surprising that existing work sheds no light
on VM reliability, such as VM failure rates, (in)dependency
of VM failures, and their correlation with resources and
management.
In this paper, we conduct a large-scale analysis comparing
failures of physical machines (PMs) and virtual machines, by
means of a ﬁeld collection of problem tickets and resource
performance measures from commercial datacenters, covering
ﬁve subsystems with around 10K hosts. Our main focus is
to characterize the failure patterns and identify the factors
that cause failures for both PMs and VMs, while highlighting
their similarities and differences. We correlate failure rates
with the capacity and usage of multiple resources, i.e., CPU,
memory, disk, and network, as well as with VM resource
management, i.e., consolidation and turning on/off. In addition
to studying the failure rates by machine types, we also classify
failures by root causes, i.e., hardware, network, reboot, power,
and software. We present statistics that are straightforward
to observe, such that one can easily develop an intuitive
understanding of failure behaviors and causes in production
datacenters. Overall, our study is limited by a few factors,
namely, the lack of physical location information of the sys-
tems, asymmetric distribution of VM and PM populations, and,
most importantly, inconsistent clarity and granularity across
different data sources,
time granularity
relative to resource usage. However, our analysis is based on
a large collection of failure events of many PMs and VMs and
considers a comprehensive set of failure types.
including different
Our study is composed of three parts: (1) the overview
of PM/VM failure patterns, (2) the dependency of PM/VM
failure rates on the resource capacity and usage, and (3) the
correlation of VM failures with VM resource management.
The main building blocks of our analysis are failure rates,
random failure probabilities and recurrent probabilities in
multiple time windows, i.e., days, weeks and months. For the
failure patterns, we present the failure rates, distribution of
inter-failure times per server, distribution of repair times, and,
most importantly, the time and space dependency of subse-
quent failures. To identify the server characteristics that cause
frequent failures, we compare failure rates across different
resource capacities and usages. We focus particularly on the
impact of the number of CPU processors, memory size [GB],
disk capacity [GB], number of disks, CPU utilization [%],
memory utilization [%], and network trafﬁc [MB/S]. The last
part of our analysis focuses on how VM-speciﬁc resource
management actions, i.e., consolidation and on/off frequency,
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.18
DOI 10.1109/DSN.2014.18
DOI 10.1109/DSN.2014.18
DOI 10.1109/DSN.2014.18
1
1
1
1
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
affect the VM failure rate.
The contributions of this paper are twofold. To the best
of our knowledge, this is the ﬁrst extensive analysis of VM
failure analysis in commercial datacenters in comparison with
PMs. Moreover, our study considers not only a diverse set
of failures, but also an extensive set of factors that correlate
with PMs and VMs failures, i.e., multiple types of resource
capacity and usage, as well as, VM resource management.
The outline of this work is as follows. Section II presents
related work. Section III provides an overview of our dataset.
The study of failure patterns and their dependency on resource
capacity and usage are presented in Sections IV, and V, respec-
tively. The impact of VM resource management is discussed
in Section VI, followed by the conclusions in Section VII.
II. RELATED WORK
System reliability is one of the foremost concerns in data-
centers, as service unavailability directly results in signiﬁcant
business revenue loss [6]. As a result, considerable efforts are
being invested in trying to understand the behavior of hardware
failures and their root causes, based on large collections of
failure logs. The related work to our study can be divided
into work focusing on (1) individual hardware subsystems,
especially disk failures [7]–[9], (2) HPC systems [4], [10], (3)
laptop or desktop machines [2], and (4) cloud datacenters [3].
Motivated by the relevance of ﬁeld analyses of failure data,
there is another set of studies trying to enhance the failure
diagnosis using static or dynamic approaches [11]. In the
following, we summarize the key ﬁndings observed from
failure data in hardware sub-, HPC and commercial systems.
Hardware Subsystems
There are many studies that focused on characterizing the
reliability of hardware subsystems, i.e., CPU, DRAM [12]
and, particularly, disks [7]–[9]. A common ﬁnding is that
disks have the highest failure rates, in comparison with other
hardware components. The average annual disk failure rate [9]
is observed to be 2 − 4%, which is much higher than the
suggested one on product datasheets. In addition, studies
show that disk failures increase linearly with age, without
a signiﬁcant infant mortality effect. Moreover, there is no
signiﬁcant correlation between disk failure rates and high
temperature or utilization [8], but the parameters of the self-
monitoring facility in drives show strong correlations with the
failure probability. Moreover, disk failure is the predominant
root cause for storage systems.
HPC Systems
As HPC systems grow exponentially by increasing the area
density and component counts, so do the failure rates [4].
Failure rates in HPC systems show a positive correlation with
the number of processors as well as the type and intensity
of workloads running on them. Statistical models are often
applied to capture the distribution of times between failures
and repair times [4], [13]. A common ﬁnding in various
studies is that
inter-failure times for a node or an entire
system are not exponentially distributed and can be well
captured by Gamma and Weibull distributions. The root causes
are typically classiﬁed into six categories, namely, hardware,
software, network, environment, human and unknown, and
analyses show a high correlation among them [5]. In particular,
power-related failures (classiﬁed under environment) induce
a high probability of follow-in failure of any kind. Liang et
al. [10] explore the correlation between the recurrence and the
location of failures through an on-line predictive model.
Commercial Systems
In contrast to HPC systems, there are only few studies that
focus on commercial systems, ranging from laptop/desktop
machines [2] to datacenter servers [3]. A common ﬁnding
for HPC and commercial systems is that failures are not
memoryless, meaning that the probability of follow-on failures
is usually much higher, e.g., two orders of magnitude, than
that of random failures for laptops and desktops. In particular,
Nightingale et al. [2] investigate the failure trends of different
subsystems in laptops and desktops, i.e., CPU, disk and mem-
ory, and their dependency with the usage of other subsystems,
such as increasing CPU speed or memory size. Their main
ﬁndings are that overclocking the CPU speed increases the
failure rates for CPU, memory and disk, and that brand-name
systems have better reliability.
In commercial servers [3], disks are the components re-
placed most frequently, and server failure rates increase with
the number of disks. The predominant factors in predicting
failures are datacenter location, followed by manufacturer
brand name, as opposed to server age and conﬁguration. The
common observations are that the server manufacturer has
an important impact on the failure rate of different hardware
components. However, both commercial system studies focus
solely on hardware-related failures and overlook the software-
and environment-related failures, which account for a signiﬁ-
cant number of failures in both commercial and HPC systems.
In contrast to related work, our study captures the failure
rates of not only PMs but also VMs. We consider a com-
prehensive set of failure types and identify relevant factors
that correlate with the failure rate, i.e., resource capacity and
usage as well as server age and consolidation. We summarize
the comparison at
the scope of our analysis and that of
previous work in Table I. Note that the related work listed in
Table I only considers a subset of resources in their capacity
and usage studies. In terms of results, our ﬁndings on PM
failures mirror similar observations from earlier work, and our
ﬁndings on VM failures and their comparison with PM failures
complements many existing hardware-reliability studies.
III. DATA COLLECTION METHODOLOGY
Our reliability study is based on data collected on ﬁve
commercial datacenter subsystems from July 2012 to June
2013. Each subsystem consists of stand-alone non-virtualized
PMs and VMs hosted on virtualized boxes. Throughout the
analysis in this paper, we focus only on statistics related to
PMs and VMs, excluding statistics on boxes, because of the
2222
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
COMPARISON BETWEEN OUR STUDY AND RECENT RELATED WORK.
TABLE I
System
s Hardware
e
r
Software
u
l
i
a
Power
F
Capacity
s Usage
r
o
t
c
a
F
Age
Repair time
[5]
HPC
√
√
√
√
[4]
HPC
√
√
√
√
√
√
[2]
Laptops
√
[3]
DC Servers
√
√
√
√
√
√
√
Ours
DC
√
VM/PM
√
√
√
√
√
√
TABLE II
SUMMARY OF DATASET STATISTICS.
PMs
VMs
All tickets
% crash tickets
% crash tickets (PMs)
% crash tickets (VMs)
Sys II
2025
Sys I
463
1320
7079
27577
6.9% 0.85%
69%
100%
31%
52
0%
Sys III
1114
1971
50157
2%
59%
41%
Sys IV
Sys V
717
313
8382
1.3%
63%
37%
810
636
25940
3.3%
57%
43%
limited data access. These machines span wide ranges of ar-
chitectures (e.g., HP, IBM1, Dell), run major operating systems
(e.g., Linux1 and Windows1) and vary in their hardware age.
Any incidents occurring on them are either reported by users
or automatically generated by monitoring tools, such as HP
OpenView [14] or IBM Tivoli1 Monitoring [15], and collected
through the ticketing system. Out of the tens of thousands of
tickets gathered, we extract crash tickets which are associated
with the underlying PMs and VMs being unresponsive or
unreachable. We refer to such incidents as server failures.
Our dataset consists of 2759 crash tickets spread across
4292 VMs and 5129 PMs. The detailed statistics across the
ﬁve subsystems, namely, the number of PMs and VMs, the
number of all problem tickets and the number of crash tickets,
are summarized in Table II. In the following, we ﬁrst describe
the data collection and sanitization process, then present the
measurements of interest that contribute to our analysis, and
ﬁnally discuss the limitations of our study.
A. Data Collection Process
We faced several challenges in gathering and sanitizing the
measurements of interests that correlate with server failures,
such as the resource capacity and usage levels. First of all,
we collect the data from various sources which spread across
multiple databases, such as the ticket and server resource
monitoring databases. Each source has its own coverage of
servers and accurate logging of the ﬁelds of interest in various
time granularities, as they are designed for disparate business
purposes. For example, on the one hand the server resource
monitoring database uses two-year observation periods with
1IBM and Tivoli are trademarks of International Business Machines Cor-
poration, registered in many jurisdictions worldwide. Linux is a registered
trademark of Linus Torvalds in the United States, other countries, or both.
Windows is a trademark of Microsoft Corporation in the United States, other
countries, or both. Other product or service names may be trademarks or
service marks of IBM or other companies.
3333
data recorded every 15 min, hourly, daily, weekly and monthly.
On the other hand, the ticket database mainly uses one-year
observation periods, with data recorded by events. Because
of the vast data volume, it is not trivial to simply track a
large number of measurements of interest [3] in commercial
datacenters, such as the capacity of servers (e.g., number of
CPU units, memory and disk size, server age), their resource
usages (e.g., CPU/memory/disk utilization, and volume of
network transfers) and problem tickets capturing crash events.
The process of obtaining the dataset used in our analysis
consists of several steps. First, we need to identify the crash
tickets among all tickets collected. Every ticket contains a
text description, explaining the problem and possible causes,
the resolution used by the service support staff to alleviate
the problem, and the repair duration. The quality of the
descriptions and resolutions may not be always consistent,
meaning that not all tickets have the same level of clarity and
accuracy. As a result, we apply manual labeling and k-means
clustering on both the description and the resolution ﬁeld of
all tickets in a best-effort manner. After manually checking
the classiﬁcation of all tickets, our k-mean classiﬁcation has
an accuracy of 87%. As second step, we classify the crash
tickets into six ﬁner-grained classes based on their resolutions:
• Network-related failure – server failures that are caused
by network issues and require a network ﬁx.
• Hardware-related crashes – server failures that are caused
by hardware malfunctions (e.g., faulty disk or battery,
broken power supply) and require a hardware replacement
or ﬁx.
• Software-related failure – server failures that are caused
by OS or application-level issues (e.g., hanging OS or
critical service agent) and require a software ﬁx.
• Power-outage-related failure – server failures that are
caused by power outages and require an electrical ﬁx.
• Reboot-related failure – server failures that are caused by
unexpected reboots.
• Other failure – server failures that cannot be classiﬁed
into any of the above classes, owing to less accurate ticket
descriptions and resolutions.
Such a classiﬁcation of crashes is essential to gain a deep
understanding of the patterns and characteristics of server
failures. Fig. 1 shows the failure distribution across the net-