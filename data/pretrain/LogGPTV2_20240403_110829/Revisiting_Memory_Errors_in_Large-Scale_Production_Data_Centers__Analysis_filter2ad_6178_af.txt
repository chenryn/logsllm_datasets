the trends that we observed in Section IV, which showed, for
example, increasing failure rates with increasing chip density
and number of CPUs. Interestingly, the model can be used
to provide insight into the relative change in error rate for
different system design choices. For example, the failure rate
of the high-end server can be reduced by 57.7% by using
lower density DIMMs and by 34.6% by using half as many
cores. This indicates that designing systems with lower density
DIMMs can provide a larger DRAM reliability beneﬁt than
designing systems with fewer CPUs. In this way, the model
that we develop allows system architects to easily and quickly
explore a large design space for memory reliability. We hope
that, by using the model, system architects can evaluate the
reliability trade-offs of their own system conﬁgurations in order
to achieve better memory reliability. We make the model also
available online at [1].
1
16
0.78
1
16
0.33
1
8
0.51
1
8
0.12
VI. EFFECT OF PAGE OFFLINING AT SCALE
We next discuss the results of a study performed to examine
ways to reduce memory errors using page ofﬂining [49, 16].
Page ofﬂining removes a physical page of memory that contains
a memory error from the set of physical pages of memory that
the operating system can allocate. This reduces the chance of
a more severe uncorrectable error occurring on that page com-
424424
pared to leaving the faulty page in the physical address space.
While prior work evaluated page ofﬂining using simulations on
memory traces [16], we deployed page ofﬂining on a fraction of
the machines we examined (12,276 servers) and observed the
results. We next describe the system design decisions required
to make page-ofﬂining work well at a large scale, and analyze
its effectiveness.
A. Design Decisions and Implementation
The three main design decisions we explore with respect
to utilizing page ofﬂining in practice are: (1) when to take a
page ofﬂine, (2) for how long to take a page ofﬂine, and (3)
how many pages to take ofﬂine (the ﬁrst and last of which were
also identiﬁed in [16]).
(1) When to take a page ofﬂine? ECC DIMMs provide
ﬂexibility for tolerating correctable errors for a certain amount
of time. In some settings, it may make sense to wait until a
certain number of memory errors occur on a page in a certain
amount of time before taking the page ofﬂine. We examine a
conservative approach and take any page that had a memory er-
ror ofﬂine immediately (the same as the most aggressive policy
examined in prior work [16]). The rationale is that leaving a
page with an error in use increases the risk of an uncorrectable
error occurring on that page. Another option could be to leave
pages with errors in use for longer and, for example, design
applications that are not as adversely affected by memory errors.
Such an approach is taken by Flikker [31], which developed a
programming model for reasoning about the reliability of data,
and by heterogeneous-reliability memory systems where parts
of memory can be less reliable and application data that is less
vulnerable to errors can be allocated there [33].
(2) For how long to take a page ofﬂine? One question
that arose when designing page ofﬂining at a large scale was
how to make an ofﬂined page persist across machine reboots
(both planned and unplanned) and hardware changes (e.g., disk
replacement). Neither of these cases are handled by existing
techniques. Allowing an ofﬂined page with a permanent error
to come back online can defeat the purpose of page ofﬂining by
increasing the window of vulnerability for uncorrectable errors.
We examine a policy that takes pages ofﬂine permanently. To
keep track of ofﬂined pages across machine reboots, we store
ofﬂined pages by host name in a distributed database that is
queried when the OS kernel loads. This allows known-bad
pages to be taken ofﬂine before the kernel allocates them to
applications. Entries in this database need to be updated as
DRAM parts are replaced in a system.
(3) How many pages to take ofﬂine? Taking a page ofﬂine
reduces the size of physical memory in a system and could
cause increased swapping of pages to storage. To limit the
negative performance impact of this, we place a cap on the
number of physical pages that may be taken ofﬂine. Unlike
prior work, as we showed in Section III-B, socket and channel
failures can potentially cause page ofﬂining to remove large
portions of the physical address space, potentially causing large
amounts of swapping to storage and degrading performance.
To check how many pages have been taken ofﬂine, logs are
routinely inspected on each machine. When the amount of
physical memory taken ofﬂine is greater than 5% of a server’s
physical memory capacity, a repair ticket is generated for the
server.
B. Effectiveness
Figure 18 shows a timeline of the normalized number of
errors in the 12,276 servers that we examine (unlike the rest
of this study, we only examine a small number of servers for
this technique). The experiment was performed for 86 days and
we measure the number of errors as a moving average over 30
days. As it was a production environment, page ofﬂining was
deployed on all of the machines over the course of several
days. We divide the graph into three regions based on the
different phases of our experiment. Region a shows the state
of the servers before page ofﬂining was deployed. Region b
shows the state of the servers while page ofﬂining was deployed
gradually to 100% of the servers (so that any malfunctions
of the deployment could be detected in a small number of
machines and not all of them). Region c shows the state of
the servers after page ofﬂining was fully deployed.
s
r
o
r
r
e
d
e
g
g
o
l
d
e
z
i
l
a
m
r
o
N
1.0
0.8
0.6
0.4
0.2
0.0
Initial
testing
Fully
deployed
a
b
c
0
10
30
20
Day of study
40
50 57
Fig. 18: The effect of page ofﬂining on error rate.
The initial hump in Region a from days 0 to 7 was due
to a bank failure on one server that generated a large number
of errors. By day 8 its effects were no longer noticeable in
the moving average and we compare the effectiveness of page
ofﬂining to the error rate after day 8.
There are three things to note from Figure 18. First, after
deploying page ofﬂining to 100% of the ﬂeet at day 25,
error rate continues to decrease until day 50. We believe that
this is because some pages that contained errors, but which
were not accessed immediately after deploying page ofﬂining,
were eventually accessed, triggering an error and taking the
page ofﬂine. In addition, some pages cannot be taken ofﬂine
immediately due to restrictions in the OS, which we will
describe in the next section. Second, comparing the error rate
at day 18 (right before initial testing) to the error rate at day 50
(after deploying page ofﬂining and letting the servers run for a
couple of weeks), the error rate decreased by around 67%. This
is smaller than the 86% to 94% error rate reduction reported
in Hwang et al.’s study [16]. One reason for this could be that
the prior study may have included socket and channel errors in
their simulation – increasing the number of errors that could
be avoided due to page ofﬂining. Third, we observe a relatively
large rate of error occurrence (e.g., at day 57 the error rate
is still around 18% of the maximum amount), even after page
ofﬂining. This suggests that it is important to design devices
and other techniques that help reduce the error rate that does
not seem to be affected by aggressive page ofﬂining.
C. Limitations
While page ofﬂining is relatively effective at reducing the
number of reported errors, we ﬁnd that it has two main limi-
tations that were not addressed in prior work. First, it reduces
memory capacity, which requires repairing a machine after a
certain fraction of its pages have been taken ofﬂine. Second, it
may not always succeed in real systems. We additionally logged
the failure rate of page ofﬂining and found that around 6% of
the attempts to ofﬂine a page initially failed. One example we
found of why a page may fail to be ofﬂined in the Linux kernel
is if its contents cannot be locked for exclusive access. For
example, if its data is being prefetched into the page cache at
the time when it is to be ofﬂined, locking the page could result
in a deadlock, and so the Linux kernel does not allow this.
This, however, could be easily ﬁxed by retrying page-ofﬂining
at a later time, at the expense of added complexity to system
software.
Despite these limitations, however, we ﬁnd that page of-
ﬂining – when adapted to function at scale – provides reason-
able memory error tolerance beneﬁts, as we have demonstrated.
We look forward to future works that analyze the interaction of
page ofﬂining with other error correction methods.
VII. RELATED WORK
To the best of our knowledge, we have performed the
ﬁrst analysis of DRAM failure trends (on modern DRAM
devices using modern data-intensive workloads) that have not
been identiﬁed in prior work (e.g., chip density, transfer width,
workload type), presented the ﬁrst regression-based model for
examining the memory failure rate of systems, and performed
the ﬁrst analysis of page ofﬂining in the ﬁeld. Prior large
scale empirical studies of memory errors analyzed various
aspects of memory errors in different systems. We have al-
ready presented extensive comparisons to the most prevalent of
them [44, 16, 47, 48, 10] throughout the paper. We will discuss
these and others here brieﬂy.
Schroeder et al. performed the ﬁrst study of memory errors
in the ﬁeld on a majority of Google’s servers in 2009 [44].
The authors’ study showed the relatively high rate of memory
errors across Google’s server population, provided evidence that
errors are dominated by device failures (versus alpha particles),
and noted that they did not observe any indication that newer
generations of DRAM devices have worse error behavior, that
CPU and memory utilization are correlated with error rate, and
that average server error rate is very high – ﬁndings clariﬁed
by our study in this paper, ﬁve years later, as we explained in
Section III. Their work formed the basis for what is known of
DRAM errors in the ﬁeld.
Hwang et al. performed an analysis on a trace of memory
errors from a sample of Google servers and IBM supercomput-
ers, showing how errors are distributed across various DRAM
components [16], but without controlling for the effect of
socket and channel failures. The high number of repeat address
errors led them to simulate the effectiveness of page ofﬂining
(proposed in [49]) on the memory error traces, which they found
to reduce error rate by 86% to 94%. Note that their study of
page ofﬂining, unlike ours (presented in Section VI), was done
purely in simulation, not in a large scale system.
Sridharan et al. examined memory errors in a supercom-
puting environment [47, 48, 10]. Similar to Hwang et al., they
found that most memory errors are permanent and additionally
identiﬁed occurrences of multi-DIMM errors, and speculated as
to their origin. They also found that DRAM vendor and age are
correlated with error rate. Concurrent to our work, Sridharan et
al. also observe that average server errors are much larger than
median server errors [46], though we quantify and provide a
model for the full distribution of errors per server. Siddiqua et
al. provided an error classiﬁcation methodology for the memory
controller and memory bus, but did not classify memory errors
at a ﬁner DRAM chip-level granularity as we do [45]. They
found that a small number of faults generate a large number of
errors and that faults are predominantly permanent.
Nightingale et al. examined the failure rate of consumer PCs
and showed that increased CPU frequency is correlated with
increased DRAM error rates [40]. A pair of works by Li et al.
analyzed memory errors on 212 Ask.com servers and evaluated
their application-level impact [27, 28]. They found that most
memory errors are permanent and that memory errors affected
applications in noticeable ways, and proposed a technique to
monitor memory for errors to reduce application impact.
VIII. CONCLUSIONS
We performed a comprehensive analysis of the memory
errors across all of Facebook’s servers over fourteen months.
We analyzed a variety of factors and how they affect server
failure rate and observed several new reliability trends for
memory systems that have not been discussed before in lit-
erature. We ﬁnd that (1) memory errors follow a power-law
425425
[17] G. Irlam, “Unis File Size Survey – 1993,” http://www.base.com/
gordoni/ufs93.html.
[18] N. Jain et al., “Data Warehousing and Analytics Infrastructure at
Facebook,” SIGMOD, 2010.
[19] JEDEC Solid State Technology Association, “JEDEC Standard: DDR4
SDRAM, JESD49-4A,” Nov. 2013.
[20] U. Kang et al., “Co-Architecting Controllers and DRAM to Enhance
DRAM Process Scaling,” The Memory Forum, 2014.
[21] S. Khan et al., “The Efﬁcacy of Error Mitigation Techniques for DRAM
Retention Failures: A Comparative Experimental Study,” SIGMET-
RICS, 2014.
[22] Y. Kim et al., “ATLAS: A Scalable and High-Performance Scheduling
Algorithm for Multiple Memory Controllers,” HPCA, 2010.
[23] ——, “Flipping Bits in Memory Without Accessing Them: An Exper-
imental Study of DRAM Disturbance Errors,” ISCA, 2014.
[24] ——, “A Case for Subarray-Level Parallelism (SALP) in DRAM,”
ISCA, 2012.
[25] D. Lee et al., “Adaptive-Latency DRAM: Optimizing DRAM Timing
for the Common-Case,” HPCA, 2015.
[26] ——, “Tiered-Latency DRAM: A Low Latency and Low Cost DRAM
Architecture,” HPCA, 2013.
[27] X. Li et al., “A Memory Soft Error Measurement on Production
Systems,” USENIX ATC, 2007.
[28] ——, “A Realistic Evaluation of Memory Hardware Errors and Soft-
ware System Susceptibility,” USENIX ATC, 2010.
[29] J. Liu et al., “RAIDR: Retention-Aware Intelligent DRAM Refresh,”
ISCA, 2012.
[30] ——, “An Experimental Study of Data Retention Behavior in Modern
DRAM Devices: Implications for Retention Time Proﬁling Mecha-
nisms,” ISCA, 2013.
[31] S. Liu et al., “Flikker: Saving DRAM Refresh-power through Critical
Data Partitioning,” ASPLOS, 2011.
[32] J. S. Long, Regression Models for Categorical and Limited Dependent
Variables. Sage Publications, 1997.
[33] Y. Luo et al., “Characterizing Application Memory Error Vulnerability
to Optimize Data Center Cost via Heterogeneous-Reliability Memory,”
DSN, 2014.
[34] T. C. May and M. H. Woods, “Alpha-Particle-Induced Soft Errors in
Dynamic Memories,” IEEE Transactions on Electron Devices, 1979.
[35] Micron Technology, Inc., “4 Gb: (cid:3)4, (cid:3)8, (cid:3)16 DDR3 SDRAM,” 2009.
[36] S. S. Mukherjee et al., “Cache Scrubbing in Microprocessors: Myth or
Necessity?” PRDC, 2004.
[37] S. Muralidhar et al., “f4: Facebook’s warm blob storage system,” OSDI,
2014.
[38] O. Mutlu, “Memory Scaling: A Systems Architecture Perspective,”
MEMCON, 2013.
[39] O. Mutlu, J. Meza, and L. Subramanian, “The Main Memory System:
the Korean Institute of
Challenges and Opportunities,” Comm. of
Information Scientists and Engineers, 2015.
[40] E. B. Nightingale, J. R. Douceur, and V. Orgovan, “Cycles, Cells and
Platters: An Empirical Analysis of Hardware Failures on a Million
Consumer PCs,” EuroSys, 2011.
[41] R. Nishtala et al., “Scaling Memcache at Facebook,” NSDI, 2013.
[42] V. Paxson and S. Floyd, “Wide-area trafﬁc: The failure of poisson
modeling,” IEEE/ACM TON, 1995.
[43] B. Schroeder and M. Harchol-Balter, “Evaluation of task assignment for
supercomputing servers: The case for load unbalancing and fairness,”
Cluster Computing, 2004.
[44] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM Errors in the
Wild: A Large-Scale Field Study,” SIGMETRICS/Performance, 2009.
[45] T. Siddiqua et al., “Analysis and Modeling of Memory Errors from
Large-scale Field Data Collection,” SELSE, 2013.
[46] V. Sridharan et al., “Memory Errors in Modern Systems: The Good,
The Bad, and The Ugly,” ASPLOS, 2015.
[47] V. Sridharan and D. Liberty, “A Study of DRAM Failures in the Field,”
SC, 2012.
[48] V. Sridharan et al., “Feng Shui of Supercomputer Memory: Positional
Effects in DRAM and SRAM Faults,” SC, 2013.
[49] D. Tang et al., “Assessment of the Effect of Memory Page Retirement
on System RAS Against Hardware Faults,” DSN, 2006.
[50] P. Vajgel et al., “Finding a Needle in Haystack: Facebook’s Photo
Storage,” OSDI, 2010.
distribution, speciﬁcally, a Pareto distribution with decreasing
hazard rate, with average error rate exceeding median error
rate by around 55(cid:2); (2) non-DRAM memory failures from the
memory controller and memory channel contribute the majority
of errors and create a kind of denial of service attack in
servers; (3) more recent DRAM cell fabrication technologies (as
indicated by chip density) show higher failure rates (prior work
that measured DRAM capacity, which is not closely related
to fabrication technology, observed inconclusive trends); (4)
DIMM architecture decisions affect memory reliability: DIMMs
with fewer chips and lower transfer widths have the lowest error
rates, likely due to electrical noise reduction; and (5) while CPU
and memory utilization do not show clear trends with respect
to failure rates, workload type can inﬂuence server failure rate
by up to 6:5(cid:2).
We developed a model for memory failures and showed how
system design choices such as using lower density DIMMs and
fewer processors can reduce failure rates of baseline servers
by up to 57.7%. We also performed the ﬁrst analysis of page
ofﬂining in a real-world environment, showing that error rate
can be reduced by around 67% identifying and ﬁxing several
real-world challenges to the technique.
We hope that the data and analyses presented in our work
can aid in (1) clearing up potential inaccuracies and limitations
in past studies’ conclusions, (2) understanding the effects of
different factors on memory reliability, (3) the design of more
reliable DIMM and memory system architectures, and (4) im-
proving evaluation methodologies for future memory reliability
studies.
ACKNOWLEDGMENTS
We thank Konrad Lai, Manish Modi, and Jon Brauer for
their contributions to the work. We also thank the students
who attended the Fall 2014 lectures of 18-742 at CMU, who
provided feedback on earlier drafts. We thank the anonymous
reviewers from ISCA 2014, OSDI 2015, and DSN 2015 and the
members of the SAFARI research group for their comments and
suggestions. This work is supported in part by Samsung and the
Intel Science and Technology Center for Cloud Computing, as
well as NSF grants 0953246, 1065112, 1212962, and 1320531.
REFERENCES
[1] “DRAM Failure Model,” http://www.ece.cmu.edu/~safari/tools.html.
[2] “Open Compute Project,” http://www.opencompute.org/.
[3] “The R Project for Statistical Computing,” http://www.r-project.org/.
[4] D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,”
SIGMOD, 2011.
[5] P. Chakka et al., “Hive: A Warehousing Solution Over a Map-Reduce
Framework,” VLDB, 2009.
[6] P.-F. Chia, S.-J. Wen, and S. Baeg, “New DRAM HCI Qualiﬁcation
Method Emphasizing on Repeated Memory Access,” IRW, 2010.
[7] C. Constantinescu, “Trends and Challenges in VLSI Circuit Reliabil-
ity,” IEEE Micro, 2003.
[8] M. E. Crovella and A. Bestavros, “Self-similarity in world wide web
trafﬁc: Evidence and possible causes,” IEEE/ACM TON, 1997.
[9] M. E. Crovella, M. S. Taqqu, and A. Bestavros, A Practical Guide to
Heavy Tails. Chapman & Hall, 1998.
[10] N. DeBardeleben et al., “Extra Bits on SRAM and DRAM Errors –
More Data from the Field,” SELSE, 2014.
[11] T. J. Dell, “A White Paper on the Beneﬁts of Chipkill-Correct ECC for
PC Server Main Memory,” IBM Microelectronics Division, Nov. 1997.
[12] R. W. Hamming, “Error Correcting and Error Detecting Codes,” Bell
System Technical Journal, Apr. 1950.
[13] M. Harchol-Balter, “Task assignment with unknown duration,” J. ACM,
2002.
[14] M. Harchol-Balter and A. Downey, “Exploiting Process Lifetime
Distributions for Dynamic Load Balancing,” SIGMETRICS, 1996.
[15] D. Hosmer and S. Lemeshow, Applied Logistic Regression (Second
Edition).
John Wiley and Sons, Inc., 2000.
[16] A. Hwang, I. Stefanovici, and B. Schroeder, “Cosmic Rays Don’t Strike
Twice: Understanding the Characteristics of DRAM Errors and the
Implications for System Design,” ASPLOS, 2012.
426426