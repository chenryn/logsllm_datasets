Connection broken
Dropping valid packets
Inaccurate detection
Inaccurate detection
Active session broken
Incorrect sequencing
Choosing wrong servers
Losing key-value pairs
Table 1: Examples of stateful in-switch applications and im-
pact of switch failures.
failures including ASIC failure, fiber cuts, or power failures [50],
and in Facebook‚Äôs data center, 26% of incidents are related to switch
failures [54].
Switch failures can impact stateful applications in two ways. If a
switch fails entirely, all application state it held is lost. Beyond that,
a link failure or the failure of a different switch can impact many
paths in the network [51], causing traffic to be rerouted [22, 34, 43].
Traffic that previously traversed one switch might be routed to
a different one, where the appropriate state is unavailable. In the
absence of this state, application processing can fail. For example,
as illustrated in Fig. 1, lacking the proper translation table entries,
the NAT cannot forward packets for existing connections, breaking
open connections en masse. Indeed, this is a serious problem ‚Äì
software-based stateful load balancers at cloud providers implement
complex failover mechanisms [30, 60].
Beyond the conventional NFs (e.g., NATs, load balancers, fire-
walls), there are several in-switch applications (shown in Table 1)
that exhibit complex state access patterns. For example, many ap-
plications that are designed to enforce QoS policies (e.g., rate limits)
employ streaming algorithms (e.g., sketching) to capture character-
istics of traffic such as heavy-hitters [53, 72]. Switch failures lead
them to make inaccurate decisions as the statistical data is lost.
Such applications update state (e.g., sketches) on every packet, so
we call them write-centric. In contrast, many conventional NFs and
DDoS defense systems (e.g., SYN proxy) [76, 77] are read-centric.
Another group of applications have mixed-read/write state access
patterns, typically with much less frequent updates than write-
centric applications. One example in this category is NFs in the
packet core for cellular networks (e.g., Evolved Packet Core (EPC)
for LTE) [17]. Packet core NFs such as a serving gateway (SGW)
route users‚Äô data traffic from user devices to the Internet and vice
versa based on per-user states (e.g., forwarding state), which are
updated when the control plane receives signaling messages (e.g.,
device attached). To cope with the increasing volume of signaling
traffic [4, 10],5 there have been recent efforts to accelerate the
control plane functions by offloading them to the programmable
data plane [5, 9, 61, 69]. For example, a SGW running on a switch
maintains per-user tunnel endpoint IDs (TEIDs) to route packets,
and this state is updated by signaling messages and read by data
packets that are encapsulated with TEIDs. Thus, when a switch fails,
since the SGW loses the state, it cannot forward packets for users,
disrupting active connections. Affected users need to re-establish
5Despite the growth, it is expected that signaling traffic rate is still much lower than
that of data traffic (e.g., 5% of data traffic [56]).
225
connections after the failure [21], increasing the service latency.
Other applications that route requests in application-specific ways
(e.g., for databases [78] or key-value stores [47]) also fall into this
category since they require state updates on every write (but not
read) request.
2.2 Existing Approaches and Limitations
We now examine classical fault tolerance mechanisms [32, 58, 74]
and mechanisms tailored for network middleboxes [63, 70]. At a
high level, these approaches can be categorized into three classes:
(1) checkpoint-recovery, (2) rollback-recovery, and (3) state replica-
tion. All prior work targets server-based implementations. In what
follows, we discuss why natural adaptations of these approaches
to the switch environment fail to ensure correct behavior during
failures.
Checkpoint-recovery. Checkpointing approaches periodically
snapshot application state (e.g., an address translation table in NAT)
and commit it to stable storage (e.g., [63]). When a failure occurs, the
latest snapshot is populated on a backup node (i.e., an alternative
switch in our context). Fig. 2a illustrates a candidate implementation
on switches using an external controller to store snapshots via the
switch control plane. To achieve a consistent snapshot, data plane
execution must be paused and packets buffered during the snapshot
period. Limited data-to-control plane bandwidth in modern switch
architectures makes this impractical.
Rollback-recovery. This approach, previously used for software
middleboxes [70], logs every packet to stable storage and replays the
traffic logs on a new device after failure to reconstruct application
state. A natural implementation is sending every packet to the
switch control plane, which logs it to the controller (Fig. 2b). In
principle, this approach can guarantee correctness if every packet
is synchronously logged and replayed after a failure. However,
the mismatch between the data traffic rate (Tbps) and the data-to-
control plane bandwidth (Gbps) will result in many packets being
dropped and will, thus, be incorrect.
State replication among switch data planes. Consider a state
machine replication approach using chain replication [74], but ap-
plied to switch data planes (Fig. 2c). Packets are forwarded through
a sequence of switches, each of which updates its state and for-
wards the packet to the next switch in a chain. Only once the packet
has reached the tail of the chain is it forwarded on its way to its
destination. This is done entirely on the data plane, so it can func-
tion at high speed. This approach achieves correctness only if state
updates are not lost. However, the state updates are delivered over
an unreliable channel, and since the switch data plane cannot effec-
tively support reliable transport protocols (e.g., TCP) updates could
be lost or reordered, violating correctness. Also, using one switch
to replicate another switch‚Äôs state makes poor use of data plane-
accessible switch memory ‚Äì the most costly and limited resource.
It also requires changes to the routing policy of the network since
a packet needs to be explicitly routed to a specific switch in the
chain depending on whether the packet updates state or not.
Takeaways. From the above discussion, we see two key takeaways.
First, approaches that rely on the switch control plane must con-
sider the mismatch between control and data plane speeds. Second,
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan
(a) Checkpoint-recovery : Switch control
plane periodically snapshots and commits
state to the controller. During this time, all
packets must be buffered.
(b) Rollback-recovery : Each packet is for-
warded to the control plane and logged by
the controller.
(c) State replication : Switches replicate state
to data plane memory using chain replica-
tion. Packets must be routed to the correct
chain node.
Figure 2: Highlighting why adapting existing approaches for fault tolerance fails for hardware switches.
while switch data-plane-only approaches can provide good per-
formance, they suffer three shortcomings: (a) incurring significant
switch resource overhead; (b) making it difficult to reason about
correctness due to unreliable communication channels between
switch data planes; and (c) they may additionally constrain routing
policies.
3 RedPlane Overview
Our goal is to design a fault tolerance solution that provides the
following four properties:
‚Ä¢ Correctness: Switch failure should be transparent to applica-
tions: clients should not see state that would not be possible in
the absence of a failure.
‚Ä¢ Performance: Under failure-free operation, overhead for per-
‚Ä¢ Low resource overhead: It should not consume switches‚Äô lim-
‚Ä¢ Transparency to routing policies: That is, we must allow a
packet to update and/or read state regardless of the location of
a switch where the packet is routed.
To this end, we present RedPlane, which provides an abstraction
of fault-tolerant state storage for stateful in-switch applications.
RedPlane provides an illusion of ‚Äúone big fault-tolerant switch‚Äù ‚Äì
the behavior is indistinguishable from the same application running
on a single switch that never fails. To achieve this, RedPlane con-
tinuously replicates state updates which can be restored without
loss after a failure.
packet latency should be low (say, a few tens of ùúáùë†).
ited compute and storage resources excessively.
RedPlane takes a state replication approach with two defining
characteristics: (1) the switch‚Äôs state replication mechanism is im-
plemented entirely in the data plane, and (2) state storage is done
through an external state store, a reliable replicated service made
up of traditional servers. Property (1) means that the switch‚Äôs con-
trol plane is not required for state replication, avoiding the issues
with the checkpointing and rollback-recovery approaches of ¬ß2.2.
Property (2) means that the replicated state is stored in commodity
server DRAM, a relatively low cost storage medium compared to
switch data plane memory. This avoids the high resource overhead
of the state replication approach discussed in ¬ß2.2.
While the idea of using servers‚Äô memory as an external store
is similar to recent work on TEA [42], it is important to note that
TEA does not tackle fault tolerance. It focuses on the problem of
Figure 3: RedPlane overview highlighting extensions to tra-
ditional workflows for in-switch applications
resource augmentation to enable a switch to retrieve state stored in
memory of servers. Furthermore, that design can only utilize servers
directly attached to a top-of-rack switch. As such, their design does
not tackle fault tolerance or provide provable correctness in the
scenarios when multiple switches can access the store.
RedPlane provides a set of APIs (Fig. 3) implemented in P4 [11], a
language to specify data plane programs on programmable switches,
to allow developers to easily integrate RedPlane with their stateful
P4 applications. Once developers (re)write their applications using
RedPlane APIs, the P4 compiler generates a binary of RedPlane-
enabled applications loaded to the switch, which continuously repli-
cates updates to the state store through the data plane.
Scope and limitations: In this work, our focus is on enabling fault
tolerance for stateful applications with partitionable hard state,
where a loss of state disrupts network or application functional-
ity, shown in Table 1. Applications only with non-partitionable
state (e.g., global counter) are beyond the scope of this work. Also,
we assume that global state in an application (e.g., a port pool in
NAT) is sharded across and managed by state store servers. Other
applications that need soft-state (e.g., in-network caches or ML
accelerators) do not require fault tolerance, but may benefit from
RedPlane.
3.1 Challenges
While replicating state updates through the data plane to an external
state store seems appealing, realizing this idea in practice presents
some challenges:
C-1. Providing correct replication in the data plane while
tolerating unreliable communication. Traditional server-based
replicated systems aim to provide strict correctness by ensuring not
just linearizability but also that each operation is executed exactly
226
Pipeline stagesApplicaon stateDataPlaneControl planePipeline stagesApplicaon stateSwitch-1Switch-2State snapshotsControllerTransferRecoverPeriodiccanmisssomestateincorrectTransferPipeline stagesApplicaon stateDataPlaneControl planePipeline stagesApplicaon stateSwitch-1Switch-2Packet logsControllerConnuousTransferTransferReplaycan miss some packetsincorrectPipelinestagesApplicationstateDataPlanePipelinestagesApplicationstateSwitch-1(head)Switch-2(tail)ContinuousPackets updatingstatePackets readingstateUpstreamswitchApp code+ RedPlaneP4 APIDeveloperP4 CompilerReplicated state on memoryReliable state storePipeline stages (RedPlane-enabled App)Application statesBinarySwitch ASICsRedPlaneprotocolRedPlane: Enabling Fault-Tolerant Stateful In-Switch Applications
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
once even in the presence of dropped or retransmitted messages [44,
49, 57]. To do so, they build on reliable communication channels
like TCP. However, the switch data plane cannot support reliable
communication, nor can it buffer significant amounts of traffic.
C-2. Handling high traffic volume. Switch data plane operates at
immense traffic volumes (up to a few billion packets per second [13,
19, 23]), in contrast to server-based systems handling a few million.
If each packet that reads or updates state requires interacting with
a server-based state store, the servers‚Äô capacity will rapidly be
exceeded. It will also incur significant performance overhead.
C-3. Being transparent to routing policies. A switch failure,
recovery, or network routing change could cause traffic flows orig-
inally processed at a switch S1 to be routed to a different switch
S2. However, since the routing decisions may be unpredictable, we
cannot make assumptions on S2 or presuppose what backup routes
will be taken. That is, we must be able to transparently migrate
the relevant state from S1 to S2 irrespective of the location of S2.
For instance, we need to make the NAT table entry available when
packets for a particular connection are processed by a different
instance.
3.2 Key Ideas
To tackle these challenges, we build on four key ideas:
I-1. Practical correctness for switch state (¬ß4). We define two
correctness models based on the requirements of in-switch applica-
tions. The first, a strict consistency mode, is based on linearizabil-
ity [36]. Because we observe that network applications are already
designed to tolerate packet loss, we explicitly adopt the standard
definition of linearizability, which permits operations that do not
complete while still providing strong consistency for those that do.
Second, since many write-centric applications (e.g., monitoring us-
ing sketches [28]) accept approximate results, we propose a relaxed
consistency mode that allows some state to be lost after a failure,
but bounds the inconsistency.
I-2. Piggybacking output packets (¬ß5.1). Instead of buffering