3.2.1
Here, we measure redirection timescales in the Akamai’s
streaming network, i.e., how frequently do clients get redi-
rected to diﬀerent streaming edge servers. While similar
measurements have been done previously in the context of
web (e.g., [23, 27]), it is essential to understand which time
scales hold for streaming. Then, we discuss security impli-
cations of our ﬁndings.
Slow Load Balancing
Figure 3: Inter-redirection time of streaming edge
servers
Figure 3 shows median inter-redirection times, obtained
using the 1,000 vantage points for the streams shown in
Table 1. The results are as follows. First, the observed
time scales are in general similar to the ones reported for
the web. This is not a surprise because Akamai obviously
uses the same measurement system for the web and stream-
ing. Second, our results indicate that 40% of vantage points
show median inter-redirection times of 30 seconds, which
corresponds to the time-scale at which we query each of
the vantage points. Our additional experiments verify that
the minimum time-scale at which the redirection happens is
20 seconds.
Another interesting result from Figure 3 is that around
10% of vantage points (the upper right corner in the ﬁgure)
experience almost no redirections at all (inter-redirection
larger than 10,000 sec). Moreover, 90% of the nodes from
this group redirect to the default edge server clusters in
the Boston area.7 This means that Akamai eﬀectively ap-
plies the data center approach for approximately 10% of its
clients. As a result, clients from the US west coast are rou-
tinely directed to this data center on the east coast. We
address the downsides of such an approach later in the text.
7Subnetworks 72.246.103.0/24 or 72.247.145.0/24.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100 1000 10000 100000CDFInter-redirection Time(sec)NASACNNABCNBA1NBA2NBA3Blockbuster1Blockbuster2Blockbuster3FM94.5FM92.1FM106.7(a) Overlap among Services
(b) Overlap among Customers
Figure 4: Server overlaps
Implications. The slow load balancing problem is de-
scribed in Section 2.2.1 above. Our measurement results
indicate that the minimum DNS redirection time applied by
the streaming CDN is 20 seconds, which is fundamentally
inappropriate for live streaming service and opens the doors
to DoS attacks. We experimentally evaluate the slow load
balancing problem in Section 4.2.1 below.
3.3 No Isolation
Another implication of the results shown in Figure 3 is
that they can help to locate busy streaming edge servers.
In general, such servers are accessed by the bottom 40%
of vantage points from Figure 3 that experience frequent
redirections. Degrading service to a busy server is in general
easier, because it already operates close to its capacity limit.
Hence, even small resources are needed to push them into
an overloaded state.
Here, we explore the level of isolation among diﬀerent ser-
vices, customers, and channels. A potentially large over-
lap among these entities raises serious security implications.
First, diﬀerent entities can necessarily impact each other
because there is no isolation at either the server or the net-
work level (as we verify later in the paper). Second, the large
overlap enables third parties to eﬀectively collect a suﬃcient
number of unique streams belonging to overlapping entities,
and then artiﬁcially increase traﬃc volume in these entities.
In this way, it is possible to impact service of an arbitrary
entity (media, customer, or channel) at overlapping servers,
as we demonstrate later in the paper.
To quantify the level of overlap among diﬀerent entities,
we deﬁne the overlap metric as follows. Consider two en-
tities, e.g., customers A and B. Next, consider a vantage
point and denote by SA and SB the subset of streaming
edge servers that this vantage point gets redirected to over
longer time scales. Likewise, denote by SAB the overlap be-
tween the two sets. Finally, we deﬁne the overlap ratio as
SAB/min(SA, SB).
3.3.1 Overlap among Services
In our experiments, we ﬁnd a total of 1,318 distinct stream-
ing edge servers. Out of this number, 703 servers support
video-on-demand, 576 live video, and 688 live audio stream-
ing. These numbers clearly indicate that there is overlap
among diﬀerent medias, as we exemplify below.
Figure 4(a) shows the CDF of the overlap ratio, taken over
all 1,000 vantage points, for the following pairs: live audio
vs. live video, live audio vs. VoD, and live video vs. VoD.
The more to the right of the ﬁgure a curve is, the larger the
overlap between given pairs. The ﬁgure clearly shows that
live video and audio overlap more than it is the case for
other pairs. We hypothesize that this happens for the fol-
lowing reasons. First, multiplexing large (video) and small
(audio) bandwidth-demanding ﬂows is meaningful from the
traﬃc engineering point of view. Second, live audio and
video streaming are similar protocolwise with varying play-
ing times, whereas VoD streams are sometimes played within
the HTTP protocol and have ﬁxed ﬁle sizes. Moreover, VoD
content is not streamed using reﬂectors, as we explain in
detail below.
Implications. Figure 4(a) shows that there is signiﬁcant
overlap among diﬀerent services, even for a relatively small
sample of streams that we explored. For example, approx-
imately 20% of vantage points (and consequently clients)
have the overlap ratio larger than 0.5 for all three combina-
tions. Implications are obvious: traﬃc from one media can
aﬀect quality of others. Moreover, by artiﬁcially increasing
traﬃc load at one media (e.g., VoD), one can aﬀect service
to other medias (e.g., live streaming), as we show in Section
4.2.2.
3.3.2 Overlap among Customers
Here, we explore the overlap ratio among diﬀerent Aka-
mai’s customer pairs, as shown in Figure 4(b). While the
ﬁgure depicts the results for a very small customer sample,
it is still insightful. First, the overlap ratio is necessarily
smaller than it is the case with medias, simply because the
number of customers is larger. Second, the ﬁgure shows rel-
atively good isolation among customers, e.g., for majority of
the pairs, more than 50% of vantage points see no overlap
among given customers. This result is slightly misleading
because we show a very small sample. Our additional ex-
periments (not shown) verify that the overlap necessarily
increases when a larger number of customers is considered.
Figure 4(b) shows a somewhat larger overlap between
NASA’s and NBA’s streams. We explore this in more depth,
and ﬁnd that the following is the case. NBA’s hostname is
a785.l3072828857.c30728.g.lm.akamaistream.net, while
the corresponding Canonical Name (CNAME), used for redi-
rections by Akamai’s DNS infrastructure,
is a785.lmg5.
akastream.net.
Similarly, NASA’s hostname is a167.
l1856944670.c18569.g.lm.akamaistream.net, and the cor-
responding CNAME is a167.lmg5.akastream.net. While
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1CDFOverlap RatioLive Video vs. AudioLive Video vs. VoDLive Audio vs. VoD 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1CDFOverlap RatioCNN vs. NASACNN vs. NBANASA vs. NBAFM92.1 vs. FM94.5FM92.1 vs. FM106.7FM94.5 vs. FM106.7the two streams operate on diﬀerent channels (a785 and
a167), the common string in the two CNAMEs is the ’lmg5.
akastream.net’ part of the domain. Indeed, Akamai uses
this approach to group channels based on geographic regions
or other properties.8 Thus, hostnames that have the same
’g’ in their CNAMEs have a larger degree of overlap.
Implications. Given the overlap among diﬀerent cus-
tomers, the implication is that traﬃc from one customer can
impact other customers. Also, by exploiting the slow load
balancing problem, it is possible to intentionally degrade
service to arbitrary customers at given streaming servers,
by artiﬁcially increasing traﬃc for other customers. More-
over, once the attacker selects a targeted customer, it is easy
to locate the overlapping customers by simply querying DNS
and ﬁnding those that share the same ’g’ names.
3.3.3 Overlap among Channels
Finally, we explore the overlap among channels. As ex-
plained above, a channel is no more than a collection of
streams that is supposed to be transported through the same
set reﬂectors. For each vantage point, we compute the over-
lap ratio among the following channels: a667, a785, and
a1020. The overlap ratio is computed as the overlap among
the three channels normalized by the size (the number of
edge servers seen by a given vantage point) of each channel.
Figure 5 plots the CDF for the overlap ratio over all vantage
points. The key point from the ﬁgure is that channels are
apparently evenly distributed over the edge servers. This
is expected as it represents a meaningful traﬃc engineering
decision.
Figure 5: Overlap among channels
Implications. Necessarily, there exists overlap among
diﬀerent channels, i.e., a same streaming server can host a
number of channels. The implication is again clear: one can
’jam’ a given channel by increasing activity in other overlap-
ping channels at given servers. This further means that it is
possible to ’jam’ even so-called pay-per-view channels (e.g.,
NBA), which require cookies to connect to and are hence
considered more reliable and safe than other channels. Still,
by artiﬁcially increasing traﬃc in co-located ’regular’ chan-
nels, it is possible to aﬀect the pay-per-view ones.
3.4 Migration and Ampliﬁcation Attacks
Akamai groups streaming edge servers in clusters and co-
locates them at diﬀerent locations all around the world, thus
bringing the content closer to end users. Here, we explore
8For example, we found that ’g2’ mainly serves channels
from China.
the size and location of such clusters, and then discuss im-
portant security implications for each of the issues.
For each channel, we deﬁne an edge cluster in a simple
way. It is a set of edge servers in the same class C subnet
that hosts that channel. For example, assume that edge
servers E1 - E5 share the same subnet, such that E1 and
E2 host channel A1, and the rest of streaming edge servers,
E3-E5, host channel A2. In this case, these ﬁve edge servers
are divided into the two clusters.
Figure 6: Cluster size
Figure 6 shows the CDF of cluster sizes for explored streams.
The ﬁgure indicates that the majority of the clusters are
small, and typically consist of two edge servers. A second
server is typically used as the ﬁrst choice for backup in case
the ﬁrst one gets overloaded. Still, larger-size clusters exist
as well. For example, Figure 6 shows that approximately
10% of CNN’s edge clusters have the size of ten and above.
Implications. There are two important security impli-
cations for cluster sizes. First small clusters are potentially
vulnerable to migration misbehaviors. If a given small-size
cluster is overloaded, not only that the existing clients (cur-
rently fetching streams from the given cluster) will suﬀer,
but newly arriving clients might suﬀer as well. This is be-
cause new clients might get redirected to distant clusters,
e.g., to diﬀerent continents. Contrary to the web case, where
such redirections are not problematic, this might not be the
case for streaming. Longer inter-continental paths might of-
fer smaller bandwidth and poorer viewing experiences. We
explore this in more depth in Section 4.2.3.
At the same time, bigger clusters open the doors to even
more serious problems — new reﬂector level vulnerabilities.
As indicated above (Section 3.1), an edge server in a given
region queries the DNS system by looking up the name that
contains the given portset (channel) and the region [22].
Thus, the edge servers that host the same channel and which
are co-located in the same region will necessarily fetch new
streams from the same reﬂector. Hence, it is possible to ex-
cite a bottleneck at that reﬂector (and aﬀect service in an
entire region) by exploiting the appropriate edge servers in a
given cluster as proxies. We explore this issue in more depth
in Section 4.3.
4. EVALUATION
Here, we perform Internet experiments to verify the iden-
tiﬁed vulnerabilities of the Akamai’s streaming infrastruc-
ture. The key challenge that we face is how to validate our
research hypothesis, yet without causing any trouble to Aka-
mai or its clients. Indeed, the key purpose of our eﬀort here
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1CDFOverlap RatioChannel a664Channel a785Channel a1020 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10 12 14 16 18CDFNumber of Edge Servers per ClusterNASACNNABCNBA1NBA2NBA3Blockbuster1Blockbuster2Blockbuster3FM94.5FM92.1FM106.7is to prevent irresponsible or malicious parties to ever be-
come capable of conducting misbehaviors against streaming
services at a large scale, as we explain in more detail in Sec-
tion 5. Thus, here we perform very cautiously engineered In-
ternet experiments in which we carfully and gradually eval-
uate bottlenecks in the Akamai’s network. Whenever we
observe bottleneck conditions, we instantly terminate our
experiments.
4.1 Experimental Methodology
In order to excite bottlenecks in the Akamai’s streaming
infrastructure, we collect a set of unique streams operating
on a single channel at the time of our experiments. Be-
cause there is a strong bias in stream popularity, popular
and unpopular streams are intentionally multiplexed on the
same channels [22]. Hence, by requesting unpopular streams
at a given channel, it is possible to generate traﬃc surges
and provoke resource bottlenecks. To collect a set of active
streams, we attempt to connect to Akamai’s edge streaming
servers at a speciﬁc channel ID and a speciﬁc stream ID to
exam the status of the streams. Whenever we successfully
connect, we discover a new active stream. In this way, we
manage to gather 1,400 unique streams, including live video
and audio, for a given channel.
In the experiments, we use seven machines to connect to
Akamai’s streaming infrastructure.9 Each of the machines
has access bandwith of about 100 Mbs and is assigned 200
unique streams, which it gradually requests as we explain in
detail below. One important issue here is that we are able
to connect to any Akamai’s streaming edge server in the
world from our experimental machines. In other words, it
is possible to override DNS redirections. This dramatically
simpliﬁes our experiments here, but at the same time reveals
another security ’hole’ in Akamai’s design.
While it may appear that ﬁxing this single ’hole’ would
prevent the attacks, this is unfortunately not the case. In
particular, these days attackers can rent botnets that can
have millions of machines. In such a scenario, even if over-
riding DNS redirections would not be possible, the attacker
can collect suﬃcient number of machines in a given region,
that map to the same edge server. Even if each of the ma-
chines would have moderate access bandwith, this is not
an issue for the attacker: even if a single attacking machine
requests only a single unique stream from the streaming net-
work, that would be suﬃcient to launch successful attacks.
We discuss this and other related issues in more detail in
the next section.
Throughout the experiments, we monitor DNS redirec-
tions to understand how quickly (or not) does Akamai ad-
just to induced bottleneck conditions. Also, to understand
the impact of the experiments on the environment, and more
importantly, to prevent any negative eﬀects for Akamai or
its clients, we install several monitoring points which fetch
streams from given edge servers during experiments. Again,
whenever we observe bottleneck conditions, we instantly
abort the experiments. Since Akamai provides streaming
service for diﬀerent protocols (e.g., Realplayer, Quicktime)
with a uniform architecture, without loss of generality we
use the Microsoft Media Server (MMS) protocol. In partic-