## Page 357
肯定起不来的。
起来，原先的配置是不是还能够生效，从实际的使用情况来说，不规范的操作，MGR是
在线修改参数，这些变更都是内存级别生效，但是如果这个实例重启了，还能不能启动
的切换过来。
还是挺不容易的。第一个问题是MGR 是否在技术上可控，第二个是考虑如何尽可能平滑
导入错误。
完整过程，算是一个好的开始。
的过程，目前采用的是单主的模式，在经过了反复测试之后，和同事一起做了下升级的
首先原本的MGR部署我们在测试中是一种迭代的部署方式，需要安装插件，安装后
技术可控方面，我也做了大量的部署测试，也总结了一些初步的实践总结。
（1）安装数据库
所以部署的过程需要循序渐进的配置，如下：
（19）测试工单流程和cmdb 的数据情况，验证升级的有效性。
（18）启动MGR-4306，恢复devopsdb的数据。
（17）切换 MGR-4301为 MGR-4306，修改 buffer_pool 配置，两个节点都修改。
（16）修改xwiki 的数据库配置为4308，启动xwiki。
（15）修改 mysql-9.208-4306 为 mysql-9.208-4308，降低 buffer_pool 配置。
(14）关闭 Mysql-9.208-4306。
（13）xwiki备份。
（12）Mysql_9.208-4306备份，使用 mysqldump 备份 devopsdb。
（11）停止 opsmange,xwiki 服务。
（10）正式切换。
（9）梳理字典表用户，生成 sql。
（8）MGR-4310端口切换测试，修改端口为430111:30确认是否可行。
（7）Shutdown mysql_119.221-4310，预期切换到 mysql_9.208-4310。
（6）Startup mysql_9.208-4310，加入集群。
（5）Shutdown mysql_9.208-4310，预期切换到119.221-4310，测试写入数据。
（4）MGR-4310切换测试。
（3）从 mysql-4306导出 devopsdb 数据导入 MGR-4310，
（2）检查xwiki的数据库配置和服务配置（tomcat）。
（1）MGR-4310 修改increment_offset 为3。
我们把整个过程分成了19个步骤，每个步骤都做了下时间统计，供参考。
看起来简单的需求，为了保证兼容和统一，需要做不少的工作来承接这个相对平滑
，评估导入时间，查看是否有
第8章MySQL集群和高可用设计|335
---
## Page 358
336丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
马上数据库就无法连接了，应用端完全阻塞，而另外一个节点却没有丝毫的变化，线
题，是由于事务的阻塞导致的，而且应该属于连锁反应。
数都处于阻塞状态，如图8-29所示。
接着查看数据库端，让人担心的事情还是发生了，数据库里面有大量的线程，还是大多
错，依然有业务输出日志。怀疑是网络的访问导致，重启了应用服务，依然访问失败，
tmp 目录放 socket 文件等。
程数都正常。
马上得到了应用的回复，连接数直线上升，原来还是400多个，现在成了600多个，
所以为了尽快恢复业务，我们选择先kill掉这些线程，先选择kill了一小部分，没想到
设计。比如 innodblog 里面放 redo、binlog 和applylog；log目录里面放慢日志、错误日志
会感觉目录非常混乱。从正式使用来说，我们需要对MySQL的目录做一个整体的规划和
在一个目录下面，尤其是MGR 里面新增了额外的日志，这些日志和 binlog 都放在一起，
那么看起来只要kill掉阻塞线程即可，但是查看已有的线程都是清一色的commit
案例8-2：大事务导致的运维系统无法访问
情况类似下图 8-30所示。
突然发现我们的运维系统不可访问了，最开始以为是网络的问题，查看后端没有报
另外就是一个比较明显的问题，原本的文件管理是一种混乱的状态，所有的文件都
这是一套基于MGR 的多主环境，目前存在两个节点，都是可写的。初步分析这个问
（3）配置新增参数
（2）配置插件
图8-29
---
## Page 359
一番排查发现，这个原因就是 delete，这个操作的瓶颈就在于 delete 语句，其中表数据有
的时候突然阻塞了，这个阻塞导致了后续的操作都产生了堆积。
简单来看insert是无辜的，因为它的逻辑很简单，
可以看到是在 2019-03-0719:06:29 的一个事务，相关 SQL是 insert 语句，在 commit
blocking trx rows_locked:3
blocking_query: commit
waiting_
waiting_trx_age: 00:00:20
locked_type:RECORD
locked_index:
wait_started: 2019-03-07 20:12:22
那么111437是哪个线程呢，我们简单定位发现是另外一个服务器发起的数据请求。
显然问题升级了，这个时候查看当前的锁情况，通过 sys schema 得到了一些线索，如下：
row in set,
locking_trx_started: 2019-03-07 19:06:29
iting
kill blocking_
id:2295941:587:59498:85
age:
mode:X
id:2295230:587:59498:85
M
2295230
_connection: KILL 111437
modified:
01:06:13
locked:
IDC1-MGR
(0.01
`xxxx_backup_... name=values(`physeevicen
row
sec)
图8-30
一定是碰到了其他的因素，而经过
第8章MySQL集群和高可用设计|337
IDC2
-MGR
nar
---
## Page 360
338丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
IDC3-MGR，那么这个过程中从分布式协议的算法来说，这是属于异常情况，而为了保持
启 IDC2-MGR，多主环境是正常的，可以提供持续性访问。
主要还是基于数据的完整性保护，我们后续做了测试，对运行平稳，写入正常的环境重
退出集群后，IDC2-MGR为了保证集群的稳定，也停止了GR复制，听起来好像不是很顺。
交导致数据变更没有同步到IDC2-MGR，这样超过一定阈值之后，IDC1-MGR节点停止
的节点也退出了集群，这就意味着没有了高可用保护。
事务中是很可能导致阻塞的。
500万，但是 delete 的逻辑是没有指定索引，也就意味着全表扫描了，所以这种情况下在
下图8-31所示。
对于2个MGR节点如此，其实从分布式的角度来说，对于3个节点也是类似的。如
如果数据变更的压力都是IDC1-MGR，如果数据变化都难以同步到IDC2-MGR和
对于这个问题我们通过日志的初步分析是IDC1-MGR节点存在大事务阻塞，没有提
最后我们重新在IDC1-MGR启动集群然后加入IDC2-MGR节点。
整个过程持续了几分钟,最后数据库启动后,发现一个奇怪的问题,那就是IDC2-MGR
这种情况下，flush 相关的操作也会 hang 住，所以果断对当前数据库做重启恢复。
kill了线程之后也没有奏效，那些线程都标识为KILLED 的，但是没有释放。
DC3
图8-31
-MGR
---
## Page 361
基于分布式协议来完成，zookeeper可以，consul也可以。
扩展，而来你要同步配置改动，一般来说这列操作是比较笨重的。轻量一层的方法就是
或者配置域名服务器来完成，这种情况下这种域名配置一来资源是定量的，你没法直接
的 Django 有点类似，而且对于 consul 来说，有一个优点是对于域名的支持很好。！
zookeeper。consul 相对来说会更加完整，很多功能和接口都是打包好的，和 Python 里面
说，相比较 zookeeper、consul、etcd、Eureka 等，从我的角度来说，建议选择 consul 和
服务，这种服务如何被识别和注册等，这些都是需要我们来指定相应的规则的。整体来
这种需求。
中间件要实现的读写分离，我们做好了域名转发，甚至都可以不用配置中间件即可完成
更加具有业务意义。
还是写），还可以定义服务的范围（比如 domain 等)，这样一来，整个服务比 IP的方式要
而且更加易于理解，比如这个配置里面，我们可以定义业务的维度，定义实例的角色（读
同时它提供了域名管理服务，在近些年的发展火爆很大原因也是与此相关。
进是去除硬 IP 的绑定方式，而采用域名来平滑应用连接，consul 本身是面向服务发现，
提出来。
8.4.1
进行监控检查来和consul Server 同步状态，以达到集群管理的目的。
一种相对轻量的处理方式，可以对于域名的基础服务管理更加可控，容易扩展。
之后，consul 可以提供 DNS 服务，通过 http 接口查询，同时也提供了健康检查的方式，这是
离和服务配置。其中在服务发现场景中，consul 可以作为注册中心，服务地址注册到 consul
8.4
集群的稳定，是需要剔除IDC1-MGR的。
在这里还有一个重要的概念就是服务发现，我们提供的域名其实真正对应的是一种
在VIP的基础上，
同时每个被注册到 consul 的节点，都需要部署一个 consul agent，通过对本地的服务
一般来说，我们要配置域名，
当然有读的业务域名，我们还可以定义其他的，比如混合访问、读写分离等。比如
假设我们要连接到一个数据库，如果我们采用如下的方式，相对来说会更加轻量，
其实初步的高可用建设到位之后，我们能够实现基本的高可用，就会有更高的要求
consul 是近些年来比较流行的服务发现工具，主要有三个应用场景：服务发现，服务隔
基于consul服务的高可用方案
基于consul的高可用扩展方案
，如果要实现跨机房的方案，其实有蛮多的改进之处，方向上的改
一种是本地域名，我们可能需要在/etc/hosts 里配置，
第8章MySQL集群和高可用设计|339
---
## Page 362
340丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
在整个体系的建设中，有 MHA Manager、Consul Server，
原来的更加稳定，可扩展，而且侵入性要小。我画了一个相对完整的设计图，如图8-32。
器。我想这也是近些年来Consul很火的一个重要原因吧。
于 Consul 的支持相对来说更加柔性，可以把 Consul Server 当做我们理解中的 DNS 服务
是有一大堆的改进要做。
透明的访问方式，当然VIP 的使用方式也存在一些瓶颈，比如：
性之外的高可用访问也是一个值得关注的地方。
史问题，而另外一方面是MGR 还有一些路和坑要走完，
8.4.2
多活的场景中。
主模式的，对于多主模式目前可以认为是提供了一种可行方案，但是暂不推荐在异机
本身 MGR 层面通过 Paxos 分布式协议设计来保证数据强一致。默认情况下是推荐单
在网络不好的情况下，很容易出现问题，可以作为一种备选方案，但是不推荐首选。
consul server 来完成，也可以间接通过 DNS 层的域名转发来完成。
域名的外部服务则可以通过网络层面的域名服务器来实现定向转发或者配置识别。
基本思想，那就是支持域名的高可用访问；对于业务来说，是更加透明地访问模式。基
，传统的 MHA+VIP 是一种相对稳定的架构，可以申请一个VIP，然后对于应用来说是
）（3）第三种是基于MGR的方案，这种方案相对来说会轻量一些，不需要MHA层
）（2）第二种是基于双主的方案，当然这种方案看起来架构简单，但是缺点也很明显。
基于 MHA+Consul的方案算是对原来问题的一个补充，也算是 MHA 2.0版本的一个
我们在落地实践的过程中，也在逐步积累经验，当然总体的思想就是整个方案要比
·对于单机多实例的支持有限；
（1）第一种是基于传统MHA层的设计，对于DNS 服务层来说，我们可以直接通过
我们可以基于consul设计几类高可用方案。
这样一来我们的域名服务就可以通过consul server 来完成了，而后续的域名转发或者
当然在一些特定的场景下还会触发“双主”的情况，所以对于高可用使用来说，总
·对于跨机房容灾来说，VIP 存在切换瓶颈。
·VIP 本身没有业务含义；
基于MHA 的高可用方案算是一种相对保守的使用，一方面也是为了兼顾低版本的历
基于MHA+Consul 的MySQL 高可用设计
，所以在快和慢之间，在数据一致
，当然还有CMDB。
---
## Page 363
8.4.3MySQL高可用方案：MGR+consul组合测试
的管理信息，也可以基于任务调度来完成 MHA 的检查工作。
有的 MHA 管理也是很有必要的，比如我们可以通过完全实现平台化的操作来得到MHA
网卡IP等）来得到真正的 Master IP。
询服务，比如我们开发了一个API，可以推送任何维度的一个IP（比如实例IP，VIP，多
流程是完整的链条。
还要告诉CMDB，
口，那就是CMDB，所以意味着切换的时候，不光MHA Manager 要告诉Consul Server,
在Consul API之外，CMDB 除了要维护元数据一致性，还可以提供有意义的数据查
当然，从MHA+VIP的方式到MHA+Consul的方案是一个逐步过渡的过程，对于已
首先要部署的就是Consul 服务，Consul 服务其实可以分 Consul Server 和 Consul
你的主从复制关系，MHA集群管理信息是在哪里存储，如果要找一个统一的存储入
有的同学可能会存在疑问，CMDB凑什么热闹？
CMDBAPI
这样一个流程化的操作，会带来一系列的联动变更，这样一来，整个
调用consul APi得到最新的Master
ACL注册，注册最新的masterkey
图8-32
MDB
注册新Master到元数据
MHA Mar
第8章MySQL集群和高可用设计丨341
---
## Page 364
342丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
Consul的域名服务配置了域名：
环境和MGR的组合架构方式。
对接更多的业务，所以属于全局的需求，不是为了一套MGR 环境特意定制，整个Consul
3个数据库实例节点的方式。其中Consul Server 的3台服务器不是MGR 集群独有，可以
所示的服务器。
基于Agent 模式，部署起来也很便捷。
Agent;Consul Server的部署是分布式架构，所以最少需要3台服务器，而对于ConsulAgent
test24801-mysql_w.service.tk
我们使用Consul的初步目标是通过域名的形式来访问 MGR 节点，比如我们通过
所以如果要完整的模拟一套Consul+MGR的完整环境，我们可能需要配置如下图8-33
而对于读节点来说，可以通过负载均衡来对接两个读节点。
可以在连接时解析得到节点的 IP:
6台服务器，其中3台作为Consul Server，另外3台作为MySQL服务器，单主模式
SO
OSU
CMDBAPI
图8-33
用consulAPi得到最新的Master
4 time=0.299 ms
---
## Page 365
步，包括后续的补充测试和切换，而这些也需要我们根据实践来细化和完善。
数据字典来顺利完成MGR的健康检查。
据consul的机制，我们需要提供相应的健康检查脚本。
以参考8.3.5小节部署MGR的几种姿势。
比如域名 test24801-mysql_r.service.tk 中末尾的 tk 就是这样来的。
domain，比如这里我们设置为 t，也就是我们所属的一个域，通过域的方式来提供访问,
都是consul 来托管的。
脚本没有统一的模板，我们在 MGR 组合方案中可以参考 performance_schema 提供的
select variable_value from performance_schema.global_status WHERE VARIABLE_NAME=
（1）得到主节点的uuid
MGR 集群在可用的前提下，接入consul 实现平滑的切换是本次测试的重中之重。根
MGR 的版本相对来说是越新越好，我们选择的是 MySQL5.7.25版，部署的步骤可
到了这个阶段，基本的任务就完成了。我们可以基于这个方案完成元数据的回调同
（3）通过uuid得到节点的状态
（2）通过uuid判断节点是否存活
第三阶段MySQL集群MGR和consul 结合
比如我们预期的架构是三个节点，单主模式。
第二阶段MySQL集群MGR服务部署
Consul 服务的配置主要关注两个文件 server.json 和 client.json，服务端需要配置
第一阶段consul服务部署
如何通过consul服务来完善已有的高可用架构，我们可以分为三个阶段来完成：
所以如果我们需要做读写分离，也是需要自定义多个域名来实现的，这些配置信息
我们可以使用这种方式来解析对应的DNS。
-mysql_r.ser
.56.6)：
第8章MySQL 集群和高可用设计丨343
bytes of data.
---
## Page 366
持MySQL也就显得顺利成章，一个好的工具离不开那些低调的技术牛人。
且同时他就职于Percona，曾经在 Oracle 参与 MySQL 的研发工作。所以 sysbench 原生支
最大动力，因为能够得到一个相对实时的数据变化。
景和时间来得到一个报告。
法抓取到更细节的数据，只有平均值，所以或者需要定制脚本，或者需要更多地测试场
面和专业，里面的测试场景都是基于早期版本，这个版本有一个不太方便的地方就是无
9.1.1
基准测试工具，也是我们本节要重点讨论的内容。
9.1
一些问题来，如果发现了问题，就避免了后续的很多被动。
一点上如果产生了懈怠或者懒惰还是会被轻视，但是从身边的例子来看，还是会测试出
多个维度来进行加压（比如CPU，内存，IO等等），看看服务器是否依旧坚挺，虽然这
的承载压力是否匹配，这个得用数据说话，或是通过严谨的论证来阐述。
sysbench 目前较新的版本是1.0.3，里面的 interval 参数确实很赞，也是驱动我尝试的
Sysbench 是一款经典的压测工具，功能非常全面。它是一个标准模块化、多线程的
安装 sysbench 的步骤常规就是以下两步。
2．安装 sysbench 新版本的坑
在 MySQL 这个圈子里，Alexey Kopytov 很多人都知道，他是 sysbench 的作者，而
如果大家看过《高性能MySQL》这本书，就会发现里面对于基准测试的描述非常全
新版本的 sysbench 和早期版本的差别还不小，确实有不少有趣的地方。
对于很多线上业务而言，如果有新服务器，新的环境，新的业务，那么资源和预期
放弃时间的人，时间也放弃他。——莎士比亚
1．sysbench 的作者
压测MySQL-
Sysbench 压测 MySQL
第9章
一环境部署和硬件压测
：MySQL性能测试
---
## Page 367
看了下感觉很不错。