AttkSvr
Firefox
Firefox
Time (s)
-
-
305
-
-
94
-
-
68
-
-
38
-
-
418
NetRecon. First, the implant-core clones a process called NetRe-
con which collects network configuration information that it saves
to a temporary file. Second, the implant-core clones another pro-
cess that scans neighboring hosts based on network configuration
information. The triggering analysis finds suspicious collecting be-
havior by spotting a downloaded file conducting a series of ioctl
requests SIOCGIFHWADDR and SIOCGIFBRDADDR. The results
of the “point-to-downstream” analysis shows that the cloned pro-
cess reads the temporary file and tries to connect other internal
hosts that are determined by the temporary file. In addition, the
point-to-point analysis between the “NetRecon.log” and neighbor-
ing hosts shows the effectiveness of Rain involving control flow
dependency. Figure 4(a) highlights the key causality between eth0
and another neighbor host. Additionally, we perform other types
of analyses and list the incremental results in Table 3.
ScreenGrab. The implant-core downloads a “ScreenGrab” pro-
gram that occasionally captures the screenshot of the victim’s desk-
top and selects certain shots to send to the attacker’s server. While
the attack occurs, the user performs various background desktop
actions such as web browsing. Our triggering analysis learns that a
site is controlled by the attacker. Starting from the malicious site,
Rain conducts an upstream analysis in order to identify a causal re-
lationship with the triggering point. It begins by extracting the SPS
from the provenance graph, and then performs a fine-grained anal-
ysis to refine the SPS to obtain an accurate causality subgraph. We
can see the executable ScreenGrab has multiple inbound traffic from
the X11 server (i.e., via Unix domain socket /tmp/.X11-unix/X0)
and outbound traffic to a file. Then the implant-core sends this file
to the attacker’s host. Rain is able to identify exactly which file is
sent. We highlight the SPS with refined causality in Figure 4(b) in
red.
CameraGrab and AudioGrab. The victim’s Firefox browser is
exploited with a zero-day exploit and its control-flow is hijacked to
the CameraGrab and AudioGrab payloads. The exploited browser
then uses a camera and a microphone to spy on the user’s behav-
ior and saves it in images and audio files respectively. Finally, the
implant-core selects certain files and sends them to the attacker
server. During this process, the user sometimes finds that the LED
light on the camera is on despite having no intention of using the
camera. The triggering point is ioctl syscalls which communi-
cates with the device. To determine the root cause, we perform
“point-to-upstream” analysis to check for the specific object-process
causality that causes the exploitation of Firefox. The results (Fig-
ure 4(c)) indicate a causality between the CameraGrab payload and
the browser as the instruction pointer of Firefox goes to the payload.
A further check of Firefox reveals that the main page has become a
malicious site, so the browser is exploited every time it is started.
Time (s)
#Taint
Attack
A(O-O)
MotivExp
A(O-O)
NetRecon
A(O-O)
ScreenGrab
CameraGrab A(O-P)
A(P-P)
AudioGrab
Analysis PruT RefiT T(P+R) Rain None Fraction%
4.7%
9.4%
5.0%
13.4%
3.5%
759 2,321
140 1,320
253
127
326
757
687
301
3,080
1,460
380
1,083
988
34
13
5
19
11
720
138
99
141
310
Table 5: Analysis cost comparison. The PruT column lists the time
used to extract the subgraph prior to fine-grained refinement. Re-
fiT shows the time it takes to selectively refine causality using taint
analysis; T(P+R) represents the total analysis time and #Taint-Rain
how many process groups are replayed and taint tracked with the
point-to-point refinement algorithm (§7.2.3); #Taint-None provides
the number of process groups to be replayed and tainted between
the two time points without reachability and selective DIFT algo-
rithms. The average fraction ratio is 5.8%.
8.1.3 Pruning and refinement. In general, the resulting subgraph
is substantially smaller than the global graph (> 90%) as well as
the SPS that is computed by the coarse-level analysis. More im-
portantly, because of DIFT, the analysis reveals the true causalities.
We analyzed several attack scenarios and summarize their incre-
mental pruning and refinement results in Table 3 (the specifics of
the analysis requests are listed in Table 4). In particular, we list the
false positive rates using coarse-level data with Rain refinement
and the reduction ratio. The potential causalities (denominator) are
counted according to the “dependency explosion” [33] definition
in which each output is assumed to depend on all the earlier in-
puts. With Rain, most false positives in the provenance graph are
eliminated (i.e., a 100% reduction), but we also encountered two
cases in which false positives remained after refinement. When
we took a closer look at the DIFT, we observed the “over-tainting”
situation that occurs during control flow-based propagation which
is a known limitation of DIFT. In general, Rain effectively improves
the precision of attack investigation.
8.2 Performance
8.2.1 Analysis Performance. To fairly examine the time duration
and tainting workload induced by Rain, we evaluate the cost of
analysis using bounded point-to-point queries. In Table 5, we first
show the time duration for Rain to prune (column PruT) and refine
(RefiT) the data using the point-to-point refinement in parallel (i.e.,
the longest duration among instances of tainting).
We then evaluate the performance of the reachability analysis
and selective DIFT. We first list the number of all of the process
groups between the two points in the None column. If one at-
tempted to refine the causalities without applying the pruning (§6)
or the selective DIFT algorithms (§7), this number would repre-
sent the load. It would also reflect the user-land part of the taint
workload in full-system DIFT systems (e.g., [50, 54]). The num-
ber of tainting instances that Rain performs for the same task is
listed in the Rain column. We find that our algorithm is effective,
significantly reducing the tainting workload to a fraction of 5.8%
on average. Note that we focus on the factual tainting workload
the analysis must take, rather than the total time. After all, one
can parallelize the workload on multiple machines to reduce time
consumption.
Figure 5: Normalized runtime performance with SPEC CPU 2006.
Figure 6: Normalized runtime performance with I/O intensive ap-
plications.
8.2.2 Runtime Performance. We evaluate the runtime perfor-
mance of Rain with SPEC CPU2006 benchmarks listed in Figure 5.
The runtime overhead of only running system logging is repre-
sented by the green bars for various testing items. The overhead
of logging plus recording is listed in the blue bars. The geometric
mean of runtime overhead in a logging+recording mode is 3.22%.
Besides the CPU intensive benchmark, we also run I/O inten-
sive applications as Rain hooks system calls and caches file or a
network I/O. We compare the runtime performance of four applica-
tions: copying the Linux kernel 3.5.0 archive with cp, downloading
a 450MB video mp4 file from a local area network with wget, com-
piling the eglibc-2.15 library, and loading cnn.com in Firefox.
Figure 6 illustrates the normalized overhead breakdown in terms
of system logging and full mode (logging+recording). In these I/O
intensive cases, Rain incurs no more than 50% overhead.
To evaluate the runtime performance of Rain in multi-core ma-
chines, we run the SPLASH-3 [46] multi-core benchmark with a
4-core CPU and summarize the results in Figure 7. The geometric
mean of runtime overhead (logging+recording) is 5.35%, and Rain
is able to faithfully replay all the benchmarks without divergence.
Storage Cost in Scenarios. We measure the storage cost of
Rain with the scenarios used in §8.1 and a high workload case (i.e.,
compiling eglibc-2.15), the results of which are summarized in
Table 6. The compiled libc is around 235MB, which is smaller than
either the system or record log. This is because Rain not only cached
the target files that were built but also the temporary files generated
during the compiling. Even though the log size is larger than the
8.2.3
Low (<30%)
Low (<10%)
Low (<10%)
Low (<30%)
Low (20%)
Low (<2%)
Low (<7%)
High (20×)
High (4–6×)
High (5×)
Low (3.22%)
Data Granularity Runtime Overhead Requirement
None (Source)
Syscall (Workflow)
None (Source)
Syscall (Workflow)
None (Source)
Syscall (Workflow)
None (Source)
Syscall (Workflow)
Syscall
None
Binary
Unit
Source
Unit
None
Instruction
None
Instruction
None
Instruction
Instruction
None
Prov Systems
PASS [39]
SPADE [24]
LPM [12]
DTrace [3]
RecProv [28]
BEEP [33]
ProTracer [34]
Panorama [54]
DataTracker [49]
PROV-Tracer [50]
Rain
Table 7: Comparison of full-system provenance systems. We com-
pare the existing systems and Rain in terms of provenance granu-
larity, runtime overhead and requirement. “Workflow” in the brack-
ets is another mode that monitors user-land applications, but re-
quires source code instrumentation. Rain achieves both efficient
runtime and instruction level analysis granularity while does not
require source or binary instrumentation.
miss or delay the detection of some stealthier attacks Further, faulty
triggers could simply waste the time and energy resources of Rain.
To solve this problem, we plan to develop an anomaly-based self-
triggering mechanism that automatically initiates a fine-grained
analysis. Lastly, the storage overhead of Rain is greater than that
of other systems such as ProTracer [34]. Unlike such systems, Rain
records all kinds of system calls (§4.3) to support replay-able execu-
tion, so the additional storage overhead appears to be unavoidable.
We plan to explore a further reduction of storage overhead, for
example, by compression and deduplication.
10 RELATED WORK
Full-system Provenance Logging. Full-system provenance log-
ging is essential to detecting complicated attacks. For example,
Linux supports the Linux Audit system [4], which records infor-
mation about system events. PASS [39, 40] is a storage system that
automatically logs and maintains provenance data. SPADE [24] is a
cross-platform system that logs provenance data across distributed
systems, and Linux provenance modules (LPM) [12] is a generic
framework that allows developers to write Linux security module
(LSM)-like modules that define custom provenance rules. In addi-
tion, ProTracer [34] is a lightweight provenance tracking system
that supports system event logging and unit-level taint propaga-
tion, which is based on BEEP [33]. To reducing logging workload,
ProTracer “taints” in order to keep track of the units, which fun-
damentally differs from the dynamic instrumentation-based taint
tracking that we apply. However, none of the systems provides the
instruction-level fine-grained provenance data that Rain provides
because they cannot achieve one of their main goals—minimizing
runtime overhead—should they provide instruction-level prove-
nance data. RecProv [28] relies on a user-level record and replay
technique to recover syscall-level provenance, but its replay does
not perform instruction-level instrumentation, so it provides no
finer-grained causality. DataTracker [49] performs taint tracking
and provides fine-grained causality data on individual files, but
because of the high execution overhead of taint analysis, using it
as an analysis system instead of a production system is impractical.
Figure 7: Normalized runtime performance (logging+recording)
with SPLASH-3 multi-core benchmark (4-core CPU). OCEAN-
C: contiguous; OCEAN-N: non-contiguous; WATER-N: nsquared;
WATER-S: spatial.
Case
MotivExp
NetRecon
ScreenGrab
CameraGrab
AudioGrab
Libc compiliation
Storage usage (MB)
Log Record
155
45.6
137.1
29
97.3
16.6
89.2