function), or compute the partial derivative of the new function
and substitute it into the update function. We test both options
and ﬁnd out that the ﬁrst approach yields better accuracy
matching that of using the logistic function. Therefore, we will
use the ﬁrst approach in the rest of the paper. We believe one
reason for lower accuracy of the second approach is that by
replacing the activation function, the cross entropy cost function
is no longer convex; using the ﬁrst approach, the update formula
is very close to training using the distance cost function, which
might help produce a better model. Better theoretical analysis
of these observations is an interesting research direction.
To justify our claims, we compare the accuracy of the
produced model using our approaches with logistic regression,
and polynomial approximation with different degrees. For the
polynomial approximation, we ﬁx the constant to 1
2 so that
f (0) = 1
2. Then we select as many points on the logistic
function as the degree of the polynomial. The points are
symmetric to the original, and evenly spread in the range
of the data value (e.g., [0,1] for MNIST, [0,1000] for Arcene).
The unique polynomial passing through all these points is
selected for approximation. The test is run on the MNIST
data with mini-batch size |B| = 128. The series of random
mini-batches are the same for all approaches. Here we train
the models on plaintext data only. As shown in Table I, the
performance of our approaches are much better than polynomial
approximation. In particular, our ﬁrst approach reaches almost
the same accuracy (98.62%) as logistic regression, and our
second approach performs slightly worse. On the contrary,
when a degree 3 polynomial is used to approximate the logistic
function, the accuracy can only reach 42.17%, which is even
worse than a linear regression. The reason is that the tails
diverge even faster than a linear activation function. When the
degree is 5, the accuracy can reach 84%; when the degree
is 10, the accuracy ﬁnally matches that of logistic regression.
However, computing a polynomial of degree 10 in secure
computation introduces a high overhead. Similar effects are
also veriﬁed by experiments on the Arcene dataset.
Nevertheless, we suggest furthur work to explore more MPC-
friendly activation functions that can be computed efﬁciently
using simple boolean or arithmetic circuits.
The privacy preserving protocol. The new activation function
proposed above is circuit friendly. It only involves testing
whether the input is within the [−1/2, 1/2] interval. However,
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
applying Yao’s garbled circuit protocol naively to the whole
logistic regression is very inefﬁcient. Instead, we take advantage
of techniques to switch between arithmetic sharing and Yao
sharing proposed in [18]. The observation is that as mentioned
in Section II-A, the only difference between the SGD for
logistic regression and linear regression is the application
of an extra activation function in each forward propagation.
Therefore, following the same protocol for privacy preserving
linear regression, after computing the inner product of the
input data and the coefﬁcient vector, we switch the arithmetic
sharing to a Yao sharing and evaluate the activation function
using a garbled circuit. Then, we switch back to arithmetic
sharing and continue the backward propagation.
Here, we propose a more involved protocol
2 ≥ 0, b1 = 1 otherwise, and b2 = 0 if u − 1
to further
optimize the circuit size, the number of interactions and the
number of multiplication triplets used. Note that if we let b1 = 0
2 ≥ 0,
if u + 1
b2 = 1 otherwise, then the activation function can be expressed
as f (u) = (¬b2) + (b2 ∧ (¬b1))u. Therefore, given (cid:7)u(cid:8), we
construct a garbled circuit that takes the bits of (cid:7)u + 1
2(cid:8)0 and
(cid:7)u(cid:8)1 as input, adds them and sets b1 as the most signiﬁcant
bit (msb) of the result (the msb indicates whether a value is
positive or negative). To be more precise, the “+ 1
2” value is
represented in the ﬁeld and scaled to have the same number of
bit representing the fractional part as u. In particular, since u is
the product of two values before truncation, “+ 1
2” is expressed
2 · 2lu, where lu is the sum of bit-length of the decimal part
as 1
in the data x and the coefﬁcient w, but we use + 1
2 for ease of
presentation. b2 is computed in a similar fashion. Instead of
computing the rest of the function in the garbled circuit which
would require a linear number of additional AND gates, we
let the garbled circuit output the Yao sharing (output labels)
of the bits (¬b2) and b2 ∧ (¬b1). We then switch to boolean
sharing of these bits and use them in two OTs to compute
(cid:7)(¬b2) + (b2 ∧ (¬b1))u(cid:8) and continue with the rest of the
training. The detailed protocol is described in Figure 13 in
the Appendix. The following theorem states the security of
privacy-preserving logistic regression. The proof is omitted due
to lack of space but we note that it is implied by the security
of the secret sharing scheme, the garbling scheme, and OT.
Theorem 3. Consider a protocol where clients distribute
arithmetic shares of their data among two servers who run the
protocol of Figure 13 and send the output to clients. Given a
secure garbling scheme, in the Fof f line and Fot hybrid model,
this protocol realizes the ideal functionality Fml of Figure 3 for
the logistic regression function, in presence of a semi-honest
admissible adversary (see section III).
Efﬁciency Discussion. The additional overhead of the logistic
regression is very small. Most of the steps are exactly the
same as the linear regression protocol in Section IV-A. In
addition, one garbled circuit protocol and 3 extra OTs are
performed in each forward propagation. The garbled circuit
performs two additions and one AND, yielding a total 2l − 1
AND gates for each value u. The base OT for OT extension
can be performed in the ofﬂine phase. Therefore, the total
communication overhead is |B|· t· ((2l− 1)· 2λ + 3l) for each
party. Note that the garbled circuit and the messages in OTs
from S0 can be sent simultaneously to S1. Thus, the logistic
regression only introduces one more interaction per iteration,
and yields a total of 3t interactions between the two parties.
No extra multiplication triplets are required since we do away
with arithmetic operations for the activation function.
D. Privacy Preserving Neural Network Training
All techniques we proposed for privacy preserving linear
and logistic regression naturally extend to support privacy
preserving neural network training. We can use the RELU
function as the activation function in each neuron and the cross
entropy function as the cost function. The update function for
each coefﬁcient in each neuron can be expressed in a closed
form as discussed in Section II-A. All the functions in both
forward and backward propagation, other than evaluating the
activation function and its partial derivative, involve only simple
additions and multiplications, and are implemented using the
same techniques discussed for linear regression. To evaluate
the RELU function f (u) = (u > 0) · u and its derivative
(cid:3)
(u) = (u > 0), we use the same approach as for logistic
f
regression by switching to Yao sharing. The garbled circuit
simply adds the two shares and outputs the most signiﬁcant
bit, which is even simpler than the circuit we needed for our
new logistic function. Note that both the RELU function and
its derivative can be evaluated together in one iteration, and
the result of the latter is used in the backward propagation.
We also propose a secure computation friendly alternative to
the softmax function f (ui) = e−ui
i=1 e−ui . We ﬁrst replace the
(cid:2)dm
exponentiations in the numerator with RELU functions such
−ui. Then,
that the results remain non-negative as intended by e
we compute the total sum by adding the outputs of all RELU
functions, and divide each output by the total sum using a
division garbled circuit. In this way, the output is guaranteed
to be a probability distribution6. In the experiment section we
show that using an example neural network and training on
the MNIST dataset, the model trained by Tensorﬂow (with
softmax) can reach 94.5% accuracy on all 10 classes, while we
reach 93.4% using our proposed function. We omit a detailed
description of the protocol due to space limits.
As we observe in our experiments, the time spent on garbled
circuits for the RELU functions dominates the online training
time. Therefore, we also consider replacing the activation
function with the square function f (u) = u2, as recently
proposed in [22] but for prediction only. (We still use RELU
functions for approximating softmax.) With this modiﬁcation,
we can reach 93.1% accuracy. Now a garbled circuit computing
a RELU function is replaced by a multiplication on shared
values, thus the online efﬁciency is improved dramatically.
However, this approach consumes more multiplication triplets
and increases cost of the ofﬂine phase.
Efﬁciency Discussion. In the online phase, the computation
complexity is twice that of the plaintext training for the
matrix arithmetic operations, plus the overhead of evaluating
the RELU functions and divisions using garbled circuits and
OTs. In our experiments, we use the division circuit from
6If the sum is 0, which means all the results of RELU functions are 0s, we
assign the same probability to each output. This is done with a garbled circuit.
28
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
the EMP toolkit [3], which has O(l2) AND gates for l-bit
numbers. The total communication is the sum of the sizes of
wise multiplication, which is O(t·(cid:2)m
all matrices involved in the matrix multiplication and element-
i=1(|B|·di−1 +di−1·di)).
The total number of iterations is 5m · t.
In the ofﬂine phase, the total number of multiplication triplets
is increased by a factor of O(
i=1 dm) compared to regression,
which is exactly the number of neurons in the neural network.
Some of the multiplication triplets can be generated in the
matrix form for online matrix multiplication. Others need to
be generated independently for element-wise multiplications.
We show the cost experimentally in Section VI-C.
(cid:2)m
E. Predictions and Accuracy Testing
The techniques developed so far can also be used to securely
make predictions, since the prediction is simply the forward
propagation component of one iteration in the training. We can
hide the data, the model, the prediction, or any combinations
of them, as they can all be secret shared in our protocols.
Similarly, we can also test the accuracy of the current
model after each epoch securely, as the accuracy is simply an
aggregated result of the predictions on the testing data. The
accuracy test can be used to adjust the learning rate or decide
when to terminate the training, instead of using a ﬁxed learning
rate and training the model by a ﬁxed number of epochs.
Detailed discussion about this can be found in Appendix E.
V. CLIENT-AIDED OFFLINE PROTOCOL
As expected and shown by the experiments,
the main
bottleneck in our privacy preserving machine learning protocols
is the ofﬂine phase. It involves a large number of cryptographic
operations such as OT or LHE, which are much slower than
simple addition and multiplication in a ﬁnite ﬁeld in the online
phase. This motivates us to explore an alternative way of
generating multiplication triplets. In particular, we can let the
clients generate the multiplication triplets. Since the clients
need to secretly share their data in the ﬁrst place, it is natural
to further ask them to secretly share some extra multiplication
triplets. Now, these multiplication triplets can be generated
in a trusted way with no heavy cryptographic operations,
which improves the efﬁciency signiﬁcantly. However, despite
its beneﬁts, it changes the trust model and introduces some
overhead for the online phase. A detailed discussion of the
client-aided triplet generations and the analysis of the overhead
can be found in Appendix F.
The new security model. The security model also changes
with the client-aided ofﬂine phase. We only informally sketch
the differences here. Previously, a client is only responsible
to upload his own data, and thus the server clearly cannot
learn any extra information when he colludes with a subset of
clients. Now, as the clients are also generating multiplication
triplets, if a subset of clients are colluding with one server, they
may reconstruct the coefﬁcient vector in an iteration, which
indirectly leaks information about the data from honest clients.
Therefore, in the client-aided scenario, we change the security
model to not allow collusion between a server and a client.
Similar models have appeared in prior work. E.g., in [21], the
29
CSP provides multiplication triplets to the clients to securely
compute inner products of their data. If a client is colluding
with the CSP, he can immediately learns others’ data. Our
client-aided protocols are secure under the new model, because
the clients learn no extra information after uploading the data
and the multiplication triplets. As long as the multiplication
triplets are correct, which is the case for semihonest clients
we consider, the training is correct and secure.
VI. EXPERIMENTAL RESULTS
We implement a privacy preserving machine learning system
based on our protocols and show the experimental results in
this section.
The Implementation. The system is implemented in C++. In
all our experiments, the ﬁeld size is set to 264. Hence, we
observe that the modulo operations can be implemented using
regular arithmetics on the unsigned long integer type in C++
with no extra cost. This is signiﬁcantly faster than any number-
theoretic library that is able to handle operations in arbitrary
ﬁelds. E.g., we tested that an integer addition (multiplication)
is 100× faster than a modular addition (multiplication) in the
same ﬁeld implemented in the GMP [5] or the NTL [7] library.
More generally, any element in the ﬁnite ﬁeld Z2l can be
represented by one or several unsigned long integers and an
addition (multiplication) can be calculated by one or several
regular additions (multiplications) plus some bit operations.
This enjoys from the same order of speedup compared to using
general purpose number theoretic libraries. We use the Eigen
library [2] to handle matrix operations. OTs and garbled circuits
are implemented using the EMP toolkit [3]. It implements the
OT extension of [11], and applies free XOR [30] and ﬁxed-key
AES garbling [12] optimizations for garbled circuits. Details
can be found in [45]. We use the cryptosystem of DGK [17]
for LHE, implemented by Demmler et. al. in [18].
Experimental settings. The experiments are executed on two
Amazon EC2 c4.8xlarge machines running Linux, with 60GB
of RAM each. For the experiments on a LAN network, we host
the two machines in the same region. The average network
delay is 0.17ms and the bandwidth is 1GB/s. The setting is
quite representative of the LAN setting, as we further tested
that two computers connected by a cable have similar network
delay and bandwidth. For the experiments on a WAN network,
we host the two machines in two different regions, one in the
US east and the other in the US west. The average network
delay is 72ms and the bandwidth is 9MB/s. We collected 10
runs for each data point in the results and report the average.
Our experiments in the LAN setting capture the sce-
nario where the two servers in our protocols have a high-
bandwidth/low-latency network connection, but otherwise are
not administered/controlled by the same party. The primary
reason for reporting experiments in the LAN setting is more
accurate benchmarking and comparison as the majority of prior
work, including all previous MPC implementations for machine
learning only report results in the LAN setting. Moreover,
contrasting our results in the LAN and WAN setting highlights
the signiﬁcance of network bandwidth in our various protocols.
For example, as our experiments show, the total time for the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
ofﬂine phase in the LAN and WAN setting are very close
when using LHE techniques to generate multiplication triplets
while there is a signﬁcant gap between the two when using
OT extension (see Table II).
Furthermore, while the LAN setting is understandably not
always a realistic assumption, there are scenarios where a high
bandwidth link (or even a direct dedicated link) between the
two servers is plausible. For example, in payment networks,
it is not uncommon for the various involved parties (issuing
Banks, aquiring Banks, large merchants, and payment networks)
to communicate over fast dedicated links connecting them.
Similarly,
in any international organization that needs to