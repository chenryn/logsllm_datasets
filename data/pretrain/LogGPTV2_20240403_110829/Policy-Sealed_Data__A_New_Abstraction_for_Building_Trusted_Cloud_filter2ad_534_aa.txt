title:Policy-Sealed Data: A New Abstraction for Building Trusted Cloud
Services
author:Nuno Santos and
Rodrigo Rodrigues and
Krishna P. Gummadi and
Stefan Saroiu
Policy-Sealed Data: A New Abstraction for Building Trusted Cloud Services
Nuno Santos, Rodrigo Rodrigues†, Krishna P. Gummadi, Stefan Saroiu‡
MPI-SWS, †CITI/Universidade Nova de Lisboa, ‡Microsoft Research
Abstract
Accidental or intentional mismanagement of cloud soft-
ware by administrators poses a serious threat to the in-
tegrity and conﬁdentiality of customer data hosted by
cloud services. Trusted computing provides an im-
portant foundation for designing cloud services that
are more resilient to these threats. However, current
trusted computing technology is ill-suited to the cloud
as it exposes too many internal details of the cloud in-
frastructure, hinders fault tolerance and load-balancing
ﬂexibility, and performs poorly. We present Excal-
ibur, a system that addresses these limitations by en-
abling the design of trusted cloud services. Excalibur
provides a new trusted computing abstraction, called
policy-sealed data, that lets data be sealed (i.e., en-
crypted to a customer-deﬁned policy) and then unsealed
(i.e., decrypted) only by nodes whose conﬁgurations
match the policy. To provide this abstraction, Excalibur
uses attribute-based encryption, which reduces the over-
head of key management and improves the performance
of the distributed protocols employed. To demonstrate
that Excalibur is practical, we incorporated it in the Eu-
calyptus open-source cloud platform. Policy-sealed data
can provide greater conﬁdence to Eucalyptus customers
that their data is not being mismanaged.
1 Introduction
Managing cloud computing services is complex and
error-prone. Cloud providers therefore delegate this task
to skilled cloud administrators who manage the cloud
infrastructure software. However, it is difﬁcult to assure
that their actions are error-free. In particular, an acci-
dental or, in some cases, intentional action from a cloud
administrator could leak, corrupt, or lose customer data.
The threat of potential violations to the integrity and
conﬁdentiality of customer data is often cited as a key
barrier to the adoption of cloud services [2,15]. Further-
more, publicized incidents involving the loss of conﬁ-
dentiality or integrity of customer data [1, 4, 7, 23, 25]
and the growing amount of security-sensitive data out-
sourced to the cloud [3,6] only heightens these concerns.
Recently, several proposals [22,39,45,53] have advo-
cated leveraging trusted computing technology to make
cloud services more resilient to integrity and conﬁden-
tiality concerns. This technology relies on a secure co-
processor – typically a Trusted Platform Module (TPM)
chip [17] – deployed on every node in the cloud. Each
TPM chip would store a strong identity (unique key) and
a ﬁngerprint (hash) of the software stack that booted on
the cloud node. TPMs could then restrict the upload of
customer data to cloud nodes whose identities or ﬁnger-
prints are considered trusted. This capability offers a
building block in the design of trusted cloud services by
securing data conﬁdentiality and integrity against insid-
ers, or conﬁning the data location to a desired geograph-
ical or jurisdictional boundary.
Despite their beneﬁts, current trusted computing ab-
stractions are ill-suited to the requirements of cloud ser-
vices for three main reasons. First, TPM abstractions
were designed to protect data and secrets on a stan-
dalone machine; they are thus cumbersome to use in
a multi-node datacenter environment where data mi-
grates across multiple nodes with potentially different
conﬁgurations. Second, TPM abstractions over-expose
the cloud infrastructure by revealing the identity and
software ﬁngerprint of individual cloud nodes; external
agents could use this information to exploit vulnerabil-
ities in the cloud infrastructure or gain business advan-
tage [40]. Third, the current implementation of TPM
abstractions is inefﬁcient and can introduce scalability
bottlenecks to cloud services.
This paper presents Excalibur, a system that provides
cloud service designers with new trusted computing ab-
stractions that overcome these barriers. These abstrac-
tions provide another critical building block for con-
structing services that offer better guarantees regarding
data integrity, conﬁdentiality, or location. Excalibur’s
design includes two main innovations crucial to over-
coming the concerns posed by using TPMs in the cloud.
First, Excalibur provides a new trusted computing
abstraction, called policy-sealed data, that allows cus-
tomer data to be encrypted according to a customer-
chosen policy and guarantees that only the cloud nodes
whose conﬁguration satisﬁes that policy can decrypt and
retrieve the data. We devised this abstraction to address
the ﬁrst two limitations of current TPM abstractions;
the abstraction permits multiple nodes with or without
identical conﬁgurations to ﬂexibly access data as long
as they satisfy the customer policies. Moreover, since it
allows policies to be speciﬁed using human-readable at-
tributes, policy-sealed data hides the low-level identities
and software ﬁngerprints of nodes.
Second, Excalibur implements the policy-sealed data
abstraction in a way that overcomes the inefﬁciency hur-
1
dles of current TPMs and scales to the demand of cloud
services. To do this, we designed a centralized moni-
tor that checks the integrity of cloud nodes and acts as
a single point-of-contact for customers to bootstrap trust
in the cloud infrastructure. To prevent the potential scal-
ability challenges associated with a centralized monitor,
we designed a set of distributed protocols to efﬁciently
implement the new abstractions. Our protocols use the
Ciphertext Policy Attribute-Based Encryption (CPABE)
encryption scheme [11], which drastically reduces the
communication needs between the monitor and produc-
tion nodes by requiring each node contact the monitor
only once during a boot cycle, a relatively infrequent
operation. We validated the correctness of Excalibur’s
cryptographic protocols using a protocol veriﬁer [12].
To demonstrate the practicality of Excalibur, we built
a proof-of-concept compute service akin to EC2. Based
on the Eucalyptus open source cloud management plat-
form [36], our service leveraged Excalibur to give users
better guarantees regarding the type of hypervisor or the
location where their VM instances run. Our experience
shows that Excalibur’s primitive is simple and versatile:
our changes required minimal modiﬁcations to the Eu-
calyptus codebase.
Our evaluation suggests that Excalibur scales well.
Due to CPABE, the monitor’s load scales independent of
the workload. In addition, according to our simulations,
one server acting as a monitor was sufﬁcient to manage
a large cluster; for example, a server took ∼15 seconds
to check the node conﬁgurations of a cluster with 10K
nodes that all rebooted simultaneously. Finally, offer-
ing trusted computing guarantees to the EC2-like ser-
vice added modest overhead during VM management
operations only.
2 Trusted Computing Concepts
The success of a cloud provider hinges on its customers
being willing to entrust the provider with their data [2,
15]. A key factor in strengthening customers’ trust is
providing strong assurances about the integrity of the
cloud infrastructure. TPMs can play a fundamental role
in providing these assurances.
The integrity of the cloud infrastructure depends on
the security of its hardware and software components.
For hardware security, cloud providers already rely on
surveillance devices and physical access control that
severely restrict physical access to cloud nodes, even by
cloud provider staff [19]. In certain cases, by deploy-
ing cloud nodes in sealed containers, they ensure that
physical access is fully disallowed [19]. For software
security, providers could take advantage of techniques
that reduce the size of the TCB [53], narrow the man-
agement interfaces [34], and verify the TCB code [24].
These techniques help designers build secure software
platforms (e.g., secure hypervisors) to host customers’
data and computations.
However, current cloud architectures provide scant
assurances that the data that customers ship to the cloud
is being handled by integrity-protected nodes running
secure software platforms. Insecure software platforms
(e.g., ones that have been tampered with or that run un-
patched software versions) put at risk cloud service in-
tegrity and thus customer data. Trusted computing tech-
nology addresses this problem by providing customers
with integrity guarantees of the cloud nodes themselves.
Trusted computing technology provides the hardware
support needed to bootstrap trust in a computer [38]. To
do so, it offers system designers four main abstractions.
First, strong identities let the computer be uniquely iden-
tiﬁed without having to trust the OS or the software run-
ning on the computer. Second, trusted boot produces a
unique ﬁngerprint of the software platform running on
the computer; the ﬁngerprint consists of hashes of soft-
ware platform components (e.g., BIOS, ﬁrmware con-
trolling the computer’s devices, bootloader, OS) com-
puted at boot time. Third, this ﬁngerprint can be se-
curely reported to a remote party using a remote attesta-
tion protocol; this protocol lets the remote party authen-
ticate both the computer and the software platform so it
can assess whether the computer is trustworthy, e.g., if it
is a trusted platform that is designed to protect the con-
ﬁdentiality and integrity of data [20, 32]. Fourth, sealed
storage allows the system to protect persistent secrets
(e.g., encryption keys) from an attacker with the ability
to reboot the machine and install a malicious OS that can
inspect the disk; the secrets are encrypted so that they
can be decrypted only by the same computer running
the trusted software platform speciﬁed upon encryption.
An important instance of trusted computing hard-
ware is the Trusted Platform Module (TPM) [17], a se-
cure co-processor widely deployed on desktops, laptops
and increasingly on servers. To offer a strong iden-
tity, the TPM uses an Attestation Identity Key (AIK).
To track the hash values that constitute a ﬁngerprint, the
TPM uses special registers called Platform Conﬁgura-
tion Registers (PCRs). Whenever a reboot occurs, the
PCRs are reset and updated with new hash values. To
perform remote attestation, the TPM can issue a quote,
which includes the PCR values signed by the TPM with
an AIK. For sealed storage, the TPM offers two prim-
itives, called seal and unseal, to encrypt and decrypt
secrets, respectively. Seal encrypts the input data and
binds it to the current set of PCR values. Unseal val-
idates the identity and ﬁngerprint of the software plat-
form before decrypting sealed data.
2
3 Threat Model
Our premise is that the attacker seeks to compromise
customer data by extracting it from integrity-protected
cloud nodes. An attack is successful if either the data
is accessible on a machine running an insecure software
platform or is moved outside the provider’s premises.
The attacker is assumed to be an agent with privileged
access to the cloud nodes’ management interface. Such
an agent is typically a cloud provider’s employee who
manages cloud software and behaves inappropriately
due either to negligence (e.g., misconﬁguring the nodes
where a computation should run) or to malice (e.g., de-
sire to steal customer data). The management interface
is accessible only from a remote site. Therefore, we
assume the attacker cannot launch physical attacks. In
fact, software and hardware management roles are usu-
ally differentiated and assigned to different teams.
The management interface grants the attacker privi-
leges to the software platform running on the node (e.g.,
access to the root account) and to a dedicated hard-
ware component for power cycling the nodes. These
privileges empower him to access customer data on the
nodes: he can reboot any node, access its local disk af-
ter rebooting, install arbitrary software on the node, and
eavesdrop the network. However, whenever cloud nodes
boot a secure software platform whose TCB we assume
to be correct, the attacker can no longer exploit vulnera-
bilities through the software platform’s interface.
Multiple trusted parties perform all other manage-
ment tasks in the cloud provider’s infrastructure. These
tasks include, e.g., procuring and deploying the hard-
ware, securing the premises, developing the software
platforms, managing the provider’s private keys, endors-
ing whether a software platform is secure, certifying the
software and hardware, etc. Trusted parties can be em-
ployees of the cloud provider or external trusted organi-
zations. Due to the nature of their roles, however, trusted
parties do not have access to the cloud nodes’ manage-
ment interface.
We assume that the TPMs are correct, and we do not
consider side-channel attacks.
4 Policy-sealed Data
This section makes the case for our new trusted com-
puting abstraction, called policy-sealed data. We ﬁrst
discuss the limitations of existing TPM abstractions in
the context of the design of a strawman trusted cloud
service. We then describe how policy-sealed data ad-
dresses these limitations.
4.1 Strawman Design of a Trusted Cloud
Service
Our strawman trusted cloud service offers features sim-
ilar to Amazon’s EC2 but aims to provide better pro-
tection against the inspection or corruption of customer
VMs by a cloud administrator.
The ﬁrst step in designing the strawman is to protect
the state of customer VMs running on cloud nodes. To
do this, we use recent proposals from research and in-
dustry that offer such guarantees but on a single node
only. For example, CloudVisor [53] retroﬁts Xen so that
the hypervisor guarantees the integrity and conﬁdential-
ity of data and software running in guest VMs even in
the presence of a malicious system administrator. Cus-
tomers can leverage the TPM’s remote attestation capa-
bility to verify that a cloud node is running CloudVisor
before uploading data to it.
However, this veriﬁcation step checks these guaran-
tees only for the cloud node on which the data is ﬁrst
uploaded. Once in the cloud, the customer’s data and
VMs often migrate from one node to another, or are
suspended to disk and resumed at a later time. To of-
fer end-to-end protection, the checks must be repeated
upon such events.
Thus, to accommodate VM migration, the strawman
design of a trusted EC2 must perform remote attesta-
tion each time a customer’s VM migrates to verify that:
(1) the destination node’s identity is signed by the cloud
provider, and (2) the ﬁngerprint matches that of Cloud-
Visor. To protect the VM upon suspension to disk, the
VM state must be encrypted using sealed storage before
suspension occurs.
4.2 Limitations of TPM Abstractions
The strawman design highlights some shortcomings of
current TPM abstractions stemming from a fundamen-
tal principle upon which TPMs were built: they were
designed to offer guarantees about one single computer.
In particular, TPMs suffer from three major problems
when they are used to build trusted cloud services.
First, the sealed storage abstraction was not designed
for a distributed and dynamic environment like the dat-
acenters where cloud services operate. It precludes the
application developer from encrypting and storing sen-
sitive data in an untrusted medium (e.g., a local hard
drive, or the Amazon S3 service) and retrieving it from
a different node or from the same node running a soft-
ware conﬁguration that differs from that in place when
the data was encrypted. However, developers might be
interested in suspending the VM to disk and resuming it
later on a different node (e.g., if, in the interim, the orig-
inal node was shut down to save power) or on the same
node running a different conﬁguration (e.g., if, in the
interim, the hypervisor was upgraded to a more recent
version).
Second, today’s TPMs are not built for high perfor-
mance. TPMs can execute only one command at a time,
and many TPM commands, such as remote attestation,
3
Attribute
service
version
vmm
type
country
zone
Value
“EC2”
“1”
“Xen”, “CloudVisor”
“small”, “large”
“US”, “DE”
“Z1”, “Z2”, “Z3”, “Z4”
Description
service name
version of the service
virtual machine monitor
resources of a VM
country of deployment
availability zone
Table 1: Example of service attributes. In this case, EC2
supports two types of VM instances, two types of VMMs, and
four availability zones (datacenters) in the US and Germany.
Node
N
Conﬁguration
service : “EC2” ; version : “1” ; type : “small” ; country
: “DE” ; zone : “Z2” ; vmm : “CloudVisor”
Table 2: Example of a node conﬁguration. This conﬁgura-
tion contains the values for the attributes that characterize the
hardware and software of a speciﬁc node N.