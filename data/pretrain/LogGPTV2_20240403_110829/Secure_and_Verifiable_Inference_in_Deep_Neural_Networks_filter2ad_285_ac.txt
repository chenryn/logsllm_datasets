i =1
= || âˆ‚f (Ï‰, x)
âˆ‚Ï‰
âˆ†Ï‰ ||2
F
(4)
where || Â· ||F denotes the Frobenius norm [33] of a matrix. By com-
paring with Eqn.(1) and Eqn.(4), it is obvious that we can find the
optimal sensitive-samples by solving the following optimization
problem,
c = arg max
x
|| âˆ‚f (Ï‰, x)
âˆ‚Ï‰
âˆ†Ï‰ ||2
F
(5)
In some cases, the modelâ€™s inputs are limited to a certain range
(denoted as [B, Q]). For instance, the intensities of all pixels are
limited to [0, 255] for valid image inputs. Therefore, the above
optimization problem is modified as below.
c = arg max
x
|| âˆ‚f (Ï‰, x)
âˆ‚Ï‰
âˆ†Ï‰ ||2
F , s .t .x âˆˆ [B, Q]Î·
(6)
4since all complex activation functions (including non-continuous) have been trans-
formed into polynomials (continuous), Taylor expansion is applicable to all neural
network frameworks.
Original modelğ’‡ğœ½ğ’„ğ’Šğ’‡â€²ğœ½ğ’„ğ’ŠSensitive SamplesclassificationY=ğ’‡ğœ½ğ’„ğ’Š=â€œStopâ€Yâ€™=ğ’‡â€²ğœ½ğ’„ğ’Š=â€œRunningâ€If Yâ‰ Yâ€™Reject{(ğ’„ğ’Š,ğ’‡ğœ½ğ’„ğ’Š)|ğ‘–=1,2,â€¦ğ‘£}789Secure and Verifiable Inference in Deep Neural Networks
ACSAC 2020, December 7â€“11, 2020, Austin, USA
where [B, Q]Î· is a convex set, and Î· denotes the dimension of
samples.
Sensitive-samples Generation Algorithm. Based on Eqn.(6),
4.2.2
we give a sensitive-samples generation algorithm shown in Al-
gorithm 2. Lines 1-4 initialize the algorithm (i.e., define the to-
tal number of iterations, and assign a random original sample to
c). Lines 5-7 set up the optimization function || âˆ‚fj(Ï‰,c)
||2
2. Lines
8-14 utilize the gradient ascent technology [66] to find the opti-
mal sensitive-sample under the constraints. Line 16 returns the
sensitive -sample and its inference result.
âˆ‚Ï‰
consideration, and Î¹ is the learning rate.
Algorithm 2 Sensitive-samples generation algorithm
Input: {f , Ï‰, Î¹}, where f is the original model. Ï‰ is the set of parameters in our
Output: {c, f (Ï‰, c)}, denotes the sensitive-sample and its inference result.
1: x0 = Init_Sample(). /* Randomly take a sample in the original sample set. */
2: It_MAX = G. /* Initialize the total number of iterations. */
3: c = x0. /* Initialize the sensitive-sample. */
4: i = 0.
5: for j = 1 to j = m do
||2
2.
6:
7: end for
8: while {(c âˆˆ [B, Q]Î·)&&(i < It_MAX)} do
9:
10:
11:
12:
13:
14:
15: end while
16: Return {c, f (Ï‰, c)}.
âˆ† = 0.
for j = 1 to j = m do
end for
c = c + Î¹ âˆ— âˆ†.
i++.
Ïƒj = || âˆ‚fj(Ï‰,c)
âˆ‚Ï‰
âˆ†+ = âˆ‚Ïƒj/âˆ‚c.
Remark: Since user inputs and model parameters will be en-
crypted by LHE before outsourcing to the server, we claim that
the server cannot distinguish sensitive samples from original sam-
ples and cannot generate sensitive samples by itself. However, our
sensitive samples are not well protected against random output
tampering from the server. For example, for a few victim inputs,
the cloud server may manipulate the result in any way it wishes,
while for all of the other inputs, it honestly follows the protocol
to compute the result using the right model. For such untargeted
random attacks, a potential solution is to increase the proportion
of sensitive samples in the query process. On the other hand, we
believe that such untargeted attacks are also risky for the server.
Due to the indistinguishability of real and sensitive samples, once
incorrectly returning incorrect output for sensitive samples, the
server will face punishment or loss of reputation.
We note that recent work [19], also designed a way to gener-
ate sensitive samples to verify the modelâ€™s integrity. However, it
only works for DL models that exclusively contain continuous ac-
tivation functions, and the feasible domain for selecting sensitive
samples is limited to a small domain, which weakens the sensitiv-
ity of sensitive-samples to modelâ€™s changes. Compared with work
[19], our scheme is applicable to all neural network frameworks.
Moreover, LHEâ€™s security guarantees that all the userâ€™s encrypted
input is indistinguishable to the server. As a result, in the process
of generating sensitive samples, we can choose samples with the
highest sensitivity to model modification as the optimal samples.
For more detail, please refer to section 2.2.
4.2.3 Privacy-preserving Inference with LHE. We have transformed
the nonlinear activation functions into polynomials, and prepared
the sensitive-samples for model verification. To protect the userâ€™s
privacy, we adopt Leveled Homomorphic Encryption (LHE) to en-
crypt all user-related data, such as the modelâ€™s parameters, userâ€™s
query requests, and inference results. In this paper, we adopt HE-
Lib library [17] to implement all ciphertext inference. The library
implements the Brakerski-Gentry-Vaikuntanathan (BGV) homo-
morphic encryption scheme [13], as well as optimizations such as
Smart-Vercauteren ciphertext packing techniques.
5 PERFORMANCE EVALUATION
In this section, we evaluate the performance of SecureDL in terms of
inference accuracy, detection accuracy, and overhead. Specifically,
the â€œ Cloud " is simulated with a virtual machine with 48GB RAM,
12 CPU cores and Ubuntu 18.04. For generating encryption schemes
in the HELib4, we set the security parameter with 80 ( security level
is equivalent to AES-128), the number of slots in the ciphertext with
0 (this allows HELib to automatically select the optimal number
of slots), and L = 20 (control the maximum number of operations
allowed in ciphertext without decryption). To reduce the increasing
noise in the ciphertext computation, we allow the server to check
the number of calculations after each ciphertext calculation. If the
number of calculations is about to exceed a threshold, the server
returns the ciphertext to the user for decryption, and then the user
re-encrypts it in a fresh ciphertext and sends it to the server.
5.1 Inference Accuracy
We test the inference accuracy of SecureDL under a custom CNN
network, which consists of two convolutional layers ( contain-
ing 20 feature maps and 50 feature maps, respectively), one av-
erage pooling layer and two fully connected layers (256 and 10
neurons, respectively). As discussed before, we transformed the
non-linear activation functions into polynomials. Intuitively, this
will affect the accuracy of the DNNs output. To quantitatively es-
timate the impact of this change on inference accuracy, we first
approximate three activation functions (i.e., Sigmoid:f1(x) =
1+eâˆ’x ,
ReLU:f2(x) = max(0, x) and tanh:f3(x) = e2xâˆ’1
e2x +1) according to the
Algorithm 1. Here we use Pf1(x), Pf2(x) and Pf3(x) to represent
the corresponding approximation polynomials, and the degrees of
Pf1(x), Pf2(x) and Pf3(x) are 2, 2 and 3, respectively. Then, to compre-
hensively analyze the inference accuracy under different data sets,
we select 6 datasets (i.e.,Breast tissues, Crab, Ovarian, Wine,Climate,
and Fertility, shown in Table 5 of APPENDIX) from UC Irvine Ma-
chine Learning Repository [25], and compare the classification
accuracy with those using the original activation functions.
As described in Table 1, compared with the existing activation
functions, the approximated polynomials with two or three degrees
are sufficient to achieve the expected accuracy. We can see that the
4Please note that HELib cannot directly support the operation of some pooling layers
(such as Max pooling) in DNNs, because of the lack of the max operation over encrypted
data. To address this, we use a scaled up version of average pooling (proposed in [14])
to replace these types of layers.
1
790ACSAC 2020, December 7â€“11, 2020, Austin, USA
Guowen Xu et al.
Dataset
Breast tissues
Acuracy
Crab
Ovarian
Wine
Climate
Fertility
Table 1: Inference Accuracy of Our SecureDL with Different Datasets
f1(x)
87.00%
92.23%
93.75%
96.29%
95.37%
86.67%
f2(x)
93.00%
96.32%
97.21%
97.21%
96.54%
92.32%
f3(x)
86.40%
89.30%
89.25%
90.23%
89.21%
84.25%
Pf1(x)
86.10%
91.26%
92.77%
96.19%
95.32%
85.12%
dPf1(x)
2
2
2
2
2
2
Pf2(x)
92.47%
96.17%
97.01%
97.18%
96.25%
92.27%
dPf2(x)
2
2
2
2
2
2
Pf3(x)
85.17%
89.12%
89.05%
89.73%
89.04%
83.07%
dPf3(x)
3
3
3
3
3
3
modelâ€™s classification accuracy under approximation polynomials
is very close to that under the original functions. For example, in
terms of classification of wine images, the original function (i.e.,
ReLU:f2(x) = max(0, x)) has a classification accuracy of 97.21%,
and our approximate function can still reach 97.18%. The reason for
this is that in function approximation process, Algorithm 1 has
limited the maximum error bound between the original function
and its approximated polynomial, which ensures that the neural
network with these polynomial activation functions still shows
good prediction accuracy.
5.2 Detection Accuracy
In this section, we estimate the detection accuracy of the proposed
scheme. As mentioned earlier, the server cannot learn the model
parameters since they have been encrypted by the LHE. However,
to â€œpurelyâ€ analyze the sensitivity of the generated sensitive sam-
ples to model changes, we considered a stronger adversary (i.e.,
the server), which is allowed to access the plaintext parameters,
and even use some samples to retrain the model5. In theory, our
sensitive-samples are generic and able to detect various integrity
attacks against DNN models. To evaluate the detection accuracy,
we consider four very subtle integrity attacks in our experiments.
â€¢ Neural Network Trojan Attack (NNTA[31]): The attack goal is
to inject some trojans in the outsourced model to make DNNs
behave correctly for normal inputs, while misclassifying the
inputs containing triggers predefined by the adversary. The
adversary can achieve the goal by modifying the selected
parameters with triggers.
â€¢ Targeted Poisoning Attack (TPA [43]): To make DNNs mis-
classify the inputs to targeted outputs, the adversary can
modify the parameters by retraining the model with cus-
tomized data.
â€¢ Model Compression Attack (MCA[30]): For reducing cloud
storage and computation cost, a malicious cloud provider
may compress the original model into a simple model with-
out visibly affecting inference accuracy.
â€¢ Arbitrary Weights Modification (AWM[36]): It is common
that an adversary (such as the cloud server) can change any
parameter of the outsourced model.
The specific datasets and DNNs models used in our experiments are
described in Table 4 (see APPENDIX 1.3). We know that the form of
modelâ€™s outputs significantly affects the detection accuracy because
5Please note that such a strong adversary does not exist in SecureDL. Here we just
demonstrate the sensitivity of sensitive samples to model changes under this fictitious
assumption. Obviously, if SecureDL shows good detection accuracy under the strong
adversary model, it will perform better in the weak adversary model.
we access the outsourcing model in only a black box way. There-
fore, we consider the case of the server returning Top-k (k=1,3,5)
classification labels to users, where the less information including
in outputs (from Top-5 (most) to Top-1 (least)) means the harder to
detect tampering using sensitive-samples. Next, we discuss the
detection accuracy of our sensitive-samples under these four
attacks. Moreover, the state-of-the-art approaches SSFDNN [19], is
also adopted in our experiments to compare with our method.
5.2.1 Neural Network Trojan Attack. We first evaluate the detec-
tion accuracy of SecureDL under the neural network trojan attack,
where the attack goal of NNTA is to make the model incorrectly
classification by injecting trojan into the DNNs. In this experiment,
we assume that the adversary launches attacks through modifying
some selected neurons parameters with triggers on the VGG-16
[47] model, which consists of 14 convolution layers and four fully
connected layers. Here the VGG-Face is a standard DNNs model
used for face classification.
Specifically, by comparing with the state-of-the-art approaches
SSFDNN [19] and the original samples, we first test the sensitiv-
ity of the sensitive-samples generated in SecureDL. As shown
in Figure 4(a), the blue histogram shows the detection accuracy
of randomly selected samples from the original samples, and the
orange and yellow histogram represent the detection accuracy of
samples generated by [19] and SecureDL, respectively. Here we re-
quire the server to return the Top-1 result with the different number
of samples. Clearly, our sensitive- samples are very sensitive
to model changes compared with randomly selected samples and
sensitive-samples generated by SSFDNN [19]. This is because in an
NNTA attack, the adversary can carefully modify certain parame-
ters to make the model correctly classify in most cases, which is
difficult to be detected by using randomly selected original sam-
ples. In addition, although [19] designs an efficient way to generate
sensitive-samples. However, this method limits the range of sam-
ples to ensure the indistinguishability between sensitive-samples
and original samples, which significantly weakens the sensitivity
of sensitive-samples to modelâ€™s changes.
Table 6(see APPENDIX) further shows the detection accuracy
with under different values of k, where the symbol # denotes the
number of query samples. We can find that SecureDL can achieve at
least 90% accuracy even if returning the top-1 result. This is mainly
due to the superiority of our sensitive-samples generation algo-
rithm. Since all generated samples will be encrypted before being
uploaded to the server, compared to SSFDNN [19], we do not limit
the range of sensitive-samples to ensure the indistinguishability
between sensitive-samples and original samples. As a result, the
feasible domain of Algorithm 2 will be greatly increased and we
can get better sensitive-samples.
791Secure and Verifiable Inference in Deep Neural Networks
ACSAC 2020, December 7â€“11, 2020, Austin, USA