output packets using limited switch resources, we use the network
itself as temporary storage by piggybacking packet contents on
coordination messages.
I-3. Lightweight sequencing and retransmission (Â§5.2). To cope
with the unreliable communication channel between the switch
data plane and the state store with low resource overhead, we
employ a sequencing mechanism for protocol messages and devise a
lightweight switch-side retransmission mechanism by repurposing
the switch ASICâ€™s packet mirroring feature.
I-4. Lease-based state ownership (Â§5.3). To reduce the frequency
with which the switch must coordinate with the state store, es-
pecially for applications with read-centric and mixed-read/write
workloads, we adopt a lease-based mechanism inspired by prior
work [33, 49, 57]. This allows us to avoid coordination with the state
store for packets which need to read but do not modify state. At the
same time, we ensure that all state updates are durably recorded
before any of their effects are externalized, guaranteeing lineariz-
ability. This mechanism also serves as the means by which state is
migrated between switches to support the transparency.
Taken together, these high-level ideas address the aforemen-
tioned challenges. First, the linearizability-based consistency model
coupled with the piggybacking and lightweight sequencing and
retransmission mechanism allows to replicate state reliably and
correctly (C-1). Second, the relaxed consistency and lease-based
state ownership help cope with high traffic volume (C-2). Lastly,
the lease-based state ownership makes RedPlane transparent to
routing policies (C-3).
4 Correctness Model
RedPlane provides two levels of consistency, which applications can
choose between based on their requirements. A linearizable mode
provides strict guarantees, making the system indistinguishable
from a single fault-tolerant switch. Because this has a high overhead
for write-centric applications due to frequent coordination with
the state store, RedPlane also offers a bounded-inconsistency mode
that permits some state updates to be lost on switch failure, but
guarantees a consistent view of switch state.
4.1 Preliminaries
By default, RedPlane provides linearizability [36], a correctness
condition for concurrent systems. We model a stateful in-switch
program as a state machine, where the output and next state are
determined entirely by the input and current state:
Definition 1 (Stateful in-switch program). A stateful program
ğ‘ƒ is defined by a transition function (ğ¼, ğ‘†) â†’ (ğ‘‚âˆ—, ğ‘†â€²) that takes
an input packet and the current state, and produces zero, one, or
multiple output packets, along with a new state.
To simplify the definitions below, we will assume that each input
packet ğ‘ produces exactly one output packet ğ‘ƒ(ğ‘); it is straightfor-
ward to extend them to the zero- or many-output case. This implies
that the programâ€™s behavior is determined entirely by the sequence
of input packets, and in particular that it is deterministic and that
packets are processed atomically. Although switch architectures are
pipelined designs that process multiple packets concurrently [26],
their compilers assign state to pipeline stages in a way that makes
packet processing appear atomic [24].
The gold standard for replicated state machine semantics is
single-system linearizability [36]. That is, that the observed execu-
tion matches a sequential execution of the program that respects
the order of non-overlapping operations. To adapt linearizability for
in-switch programs, we first redefine a history in terms of packet
processing:
Definition 2 (History). A history is an ordered sequence of events.
These can be either input events ğ¼ğ‘, in which a packet ğ‘ is received at
a RedPlane switch, or output events ğ‘‚ğ‘ in which the corresponding
output packet is output by a RedPlane switch.
Note that it is possible for there to be input events ğ¼ğ‘ without the
corresponding output ğ‘‚ğ‘, if the processing of ğ‘ is still in process
or due to a failure. We discuss this in depth next.
4.2 Linearizable mode
Definition 3 (Linearizability for a stateful in-switch program).
A history ğ» is a linearizable execution of a program ğ‘ƒ if there is
a reordering ğ‘† of the input events in ğ» such that (1) the value for
each output event ğ‘‚ğ‘ that exists in ğ» is given by running ğ‘ƒ on the
input events in ğ‘† in sequence, and (2) if ğ‘‚ğ‘¥ precedes ğ¼ğ‘¦ in ğ» then
ğ¼ğ‘¥ precedes ğ¼ğ‘¦ in ğ‘†.
227
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Daehyeok Kim, Jacob Nelson, Dan R. K. Ports, Vyas Sekar, Srinivasan Seshan
Here, ğ‘† is the apparent sequential order of execution.
Linearizability is fundamentally a safety property, not a liveness
one: it specifies what output values are acceptable, but does not
guarantee that all operations complete. It is possible for a packet to
be received and (1) update switch state, but produce no output, or
(2) neither update switch state nor produce any output. Definition 3
reflects this: a packet with an input event but no output event can
still appear in the sequential order ğ‘†. If it precedes the processing
of other packets, then they see the effects of its state update. If it
appears at the end of ğ‘†, it has no visible effect on system state.
While these anomalies comport with the definition of lineariz-
ability, most replicated systems aim to provide a stronger property:
that every operation is executed exactly once and returns its result
to the client. Ensuring this requires several protocol-level mecha-
nisms: typically, clients retry requests that do not receive a response,
and replicas keep state to detect duplicate requests and resend the
responses without executing them twice [44, 49, 57]. As we see
(Â§5.2), these techniques are not feasible in our environment.
Accordingly, RedPlane takes a different approach: it explicitly
permits these two types of anomalies. While this may seem surpris-
ing, it matches the semantics of modern networks. The two cases
correspond to a packet being lost (1) between the RedPlane switch
and its destination or (2) between the source and the RedPlane
switch, respectively. Network applications must already tolerate
lossy networks, so they are resilient to such losses.
Relaxing the definition of correctness enables a tractable im-
plementation. By not requiring the system to achieve complete
reliability, our protocol may drop packets during failover, or if
messages between a switch and the state store are lost. In these
scenarios, an input packet or its output may be lost. Of course,
dropping too many packets is undesirable for performance reasons;
such loss events are rare.
4.3 Per-flow Linearizability
In most in-switch programs, some or all state is associated with a
particular flow â€“ a subset of traffic identified by a unique key, e.g.,
an IP 5-tuple, VLAN ID, or an application-specific object ID. For
example, each translation table entry in a NAT is tied to a specific
flow based on an IP 5-tuple. For many applications, per-flow state is
the only state that needs to be consistent or fault tolerant â€“ either
because there is no global state, or because global state can tolerate
weaker consistency, e.g., traffic statistic counters that need not be
precise. RedPlane generally provides consistency for per-flow state
(consistency for global state is optional):
Definition 4 (Per-flow linearizability). A history ğ» is per-flow
linearizable if, for each flow ğ‘“ , the subhistory ğ»ğ‘“ for the packets in
flow ğ‘“ is linearizable.
As long as programs use only per-flow state, per-flow lineariz-
ability is the same as global linearizability, because linearizability
is a local (i.e., composable) property [36]. The benefit of operating
on a per-flow level is that it means synchronization between states
associated with different flows are not required. As we show in Â§5,
this allows RedPlane to distribute execution of a program across
multiple switches: each has the state associated with certain flows,
and can process packets for those flows. This matches the way many
applications are deployed in practice, e.g., a NAT will be deployed
228
Figure 4: RedPlane state replication protocol packet format.
to a cluster of switches, using ECMP for load balancing. Because
this load balancing is done on a per-flow granularity, each switch
is responsible for performing translation for a subset of flows, and
does not need access to the translation table for the other flows.
4.4 Bounded-inconsistency mode
RedPlaneâ€™s linearizable mode uses a synchronous replication pro-
tocol (Â§5.1), which can induce high overhead for write-centric ap-
plications. However, we observe that many write-centric applica-
tions in programmable switches operate in contexts where approx-
imate results are acceptable, e.g., monitoring using sketches [28] or
Bloom filters [25]. For these applications, RedPlane offers a bounded-
inconsistency mode that has lower overhead.
In this mode, RedPlane takes periodic snapshots of data plane
state and replicates them asynchronously. This means that upon
switch failure, the most recent state updates can be lost. However,
RedPlane ensures that the system recovers to a consistent state from
within a time interval ğœ–. RedPlaneâ€™s consistent snapshot mechanism
ensures that the state after recovery reflects an actual state of the
system, which simplifies reasoning about the correctness of com-
plex data structures.6 In Â§5.4, we describe how we address key
challenges in realizing this mode in RedPlane.
5 RedPlane Design
Now, we describe the RedPlane protocol that realizes our lineariz-
able and bounded-inconsistency modes. We begin with an overview
of the protocol and explain how we address practical challenges.
5.1 Basic Design
As shown in Fig. 3, RedPlane consists of (1) an external state store
built on commodity servers and (2) a RedPlane-enabled application
running on the switch data plane. In this section, we describe how
the components work together via the state replication protocol.
For clarity of exposition, we start with simplifying assumptions:
there is no packet loss or reordering between switches and the state
store, switches do not fail while messages are in transit, and packets
for a flow are routed to only one switch at a time. We revisit these
assumptions in Â§5.2 and Â§5.3.
5.1.1 External state store: The external state store is an in-memory
key-value storage system. We partition it across multiple shards
by flow â€“ identified by an IP 5-tuple or other key. Each state store
shard can be replicated using conventional mechanisms and we
do not seek to innovate here as many existing key-value stores
meet our needs (e.g., [16, 48, 59]). Specifically, our prototype is a
simple in-memory storage server implemented in C++ that uses
chain replication [74] with a group size of 3.
6Although the bounded-inconsistency mode may affect properties of some approx-
imate data structures (e.g., no false negatives in Bloom filters), since it bounds the
inconsistency within ğœ–, developers or network operators can easily reason about the
potential inconsistency.
L3/L4 info. of the state storeRedPlane headerETHIPUDPSEQTypeFlow keyVal1â€¦ValnOutputpacketRequest or Ack typeAacheddependingonmessagetypeRedPlane: Enabling Fault-Tolerant Stateful In-Switch Applications
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
(a) Out-of-order delivery of requests causes inconsistency.
Figure 5: Basic workflow of RedPlane state replication proto-
col. â€œReplâ€ indicates a state replication request. pkt ğ‘“
ğ‘› indicates
ğ‘›ğ‘¡â„ packet of a flow ğ‘“ .
(b) Request sequencing serializes replication requests.
5.1.2 Basic replication protocol: A RedPlane-enabled application
replicates state updates to the state store by exchanging protocol
messages formatted as shown in Fig. 4. It uses standard UDP and
IP headers to address messages to the state store or the switch
using their respective IP addresses. The RedPlane header consists
of a sequence number, a message type, and a flow key. Depending
on the message type, it can also include flow state and an output
packet. We will discuss these fields shortly. Note that we assign an
IP address to each RedPlane switch and use it for routing requests
and response packets between state store servers and RedPlane
switches. This works with general L3 routing protocols including
ECMP and BGP.
As an illustrative example to help understand the protocol, we
consider a per-flow counter application shown in Fig. 5, This appli-
cation updates or reads the state for each packet. In the example,
there are two switches and a state store. We have multiple packets
in each flow ğ‘“ , with the ğ‘›th packet denoted as pkt ğ‘“
ğ‘›. This example
illustrates a case where the Switch-1 initially handles ğ‘“ 1, but after
its failure, the flow is rerouted to the Switch-2.
State initialization or migration (Step 1 or 4 in Fig. 5). When
the application receives a packet that belongs to a flow it has never
seen before (e.g., pkt ğ‘“ 1
1 ), it needs to send a state initialization request.
It identifies the corresponding state store server by hashing the
flow key (e.g., IP 5-tuple), and looking up the corresponding server
IP and UDP port from a preconfigured table.
There are two possible cases: (1) the flow is new and so has no
state, or (2) the flow state previously existed on a failed switch,
and a packets for that flow are now being routed to a switch on
an alternative path (i.e., failover). In case (1), upon receiving the
request, the state store initializes its storage for the state and sends
a response back to the switch (Step 1 ). In case (2), since the state
store already has the flow state, it sends a response containing the
latest state (Step 4 ).
Upon receiving the response, the application installs the returned
state into the corresponding switch memory. For stateful memory
registers, this can be done entirely in the data plane. On the Tofino
architecture, updates to match tables or certain other resources need
to be done through the switch control plane. In this case, RedPlane
routes the processing through the control plane. This can introduce
additional latency (we measure this in Â§7.1). However, many in-
switch applications already require a control plane operation on a