title:An Observational Investigation of Reverse Engineers' Processes
author:Daniel Votipka and
Seth M. Rabin and
Kristopher K. Micinski and
Jeffrey S. Foster and
Michelle L. Mazurek
An Observational Investigation of 
Reverse Engineers’ Processes
Daniel Votipka and Seth Rabin, University of Maryland; Kristopher Micinski, 
Syracuse University; Jeffrey S. Foster, Tufts University; Michelle L. Mazurek, 
University of Maryland
https://www.usenix.org/conference/usenixsecurity20/presentation/votipka-observational
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.An Observational Investigation of Reverse Engineers’ Processes
Daniel Votipka, Seth M. Rabin, Kristopher Micinski*,
Jeﬀrey S. Foster†, and Michelle M. Mazurek
University of Maryland; *Syracuse University; †Tufts University
{dvotipka,srabin,mmazurek}@cs.umd.edu; PI:EMAIL; PI:EMAIL
Abstract
Reverse engineering is a complex process essential to
software-security tasks such as vulnerability discovery and
malware analysis. Signiﬁcant research and engineering eﬀort
has gone into developing tools to support reverse engineers.
However, little work has been done to understand the way
reverse engineers think when analyzing programs, leaving
tool developers to make interface design decisions based only
on intuition.
This paper takes a ﬁrst step toward a better understanding
of reverse engineers’ processes, with the goal of producing
insights for improving interaction design for reverse engi-
neering tools. We present the results of a semi-structured,
observational interview study of reverse engineers (N=16).
Each observation investigated the questions reverse engineers
ask as they probe a program, how they answer these questions,
and the decisions they make throughout the reverse engineer-
ing process. From the interview responses, we distill a model
of the reverse engineering process, divided into three phases:
overview, sub-component scanning, and focused experimen-
tation. Each analysis phase’s results feed the next as reverse
engineers’ mental representations become more concrete. We
ﬁnd that reverse engineers typically use static methods in the
ﬁrst two phases, but dynamic methods in the ﬁnal phase, with
experience playing large, but varying, roles in each phase.
Based on these results, we provide ﬁve interaction design
guidelines for reverse engineering tools.
Researchers, companies, and practitioners have developed
an extensive array of tools to support RE [5–24]. However,
there is limited theoretical understanding of the RE process
itself. While existing tools are quite useful, design decisions
are currently ad-hoc and based on each designer’s personal
experience. With a more rigorous and structured theory of
REs’ processes, habits, and mental models, we believe ex-
isting tools could be reﬁned, and even better tools could be
developed. This follows from recommended design princi-
ples for tools supporting complex, exploratory tasks, in which
the designer should “pursue the goal of having the computer
vanish" [25, pg. 19-22].
In contrast to RE, there is signiﬁcant theoretical understand-
ing of more traditional program comprehension—how devel-
opers read and understand program functionality—including
tasks such as program maintenance and debugging [26–36].
However, RE diﬀers from these tasks, as REs typically do not
have access to the original source, the developers who wrote
the program, or internal documentation [3, pg. 141-196], [37].
Further, REs often must overcome countermeasures, such as
symbol stripping, packing, obfuscation, and anti-debugging
techniques [3, pg. 327-356], [38], [39, pg. 441-481], [40, pg.
660-661]. As a result, it is unclear which aspects of traditional
program comprehension processes will translate to RE.
In this paper, we develop a theoretical model of the RE
process, with an eye toward building more intuitive RE tools.
In particular, we set out to answer the following research
questions:
1 Introduction
Software reverse engineering is a key task performed by se-
curity professionals during vulnerability discovery, malware
analysis, and other tasks [1, 2], [3, pg. 5-7]. (For brevity, we
will refer to this task as RE and its practitioners as REs.) RE
can be complex and time consuming, often requiring expert
knowledge and extensive experience to be successful [4,5]. In
one study, participants analyzing small decompiled code snip-
pets with less than 150 lines required 39 minutes on average
to answer common malware-analysis questions [5].
RQ1. What high-level process do REs follow when examin-
ing a new program?
RQ2. What technical approaches (i.e., manual and automated
analyses) do REs use?
RQ3. How does the RE process align with traditional pro-
gram comprehension? How does it diﬀer?
Speciﬁcally, when considering REs’ processes, we sought
to determine the types of questions they had to answer and hy-
potheses they generated; the speciﬁc steps taken to learn more
USENIX Association
29th USENIX Security Symposium    1875
about the program; and the way they make decisions through-
out the process (e.g., which code segments to investigate or
which analyses to use).
As there is limited prior work outlining REs’ processes
and no theoretical basis on which to build quantitative assess-
ments, we chose an exploratory qualitative approach, building
on prior work in expert decision-making [41–43] and program
comprehension [26–36]. While a qualitative study cannot in-
dicate prevalence or eﬀectiveness of any particular process,
it does allow us to enumerate the range of RE behaviors
and investigate in depth their characteristics and interactions.
Through this study, we can create a theoretical model of the
RE process as a reference for future tool design.
To this end, we conducted a 16-participant, semi-structured
observational study. In each participant session, we asked par-
ticipants to recreate a recent RE experience while we observed
their actions and probed their thought process. Throughout,
we tracked the decisions made, mental simulation methods
used, questions asked, hypotheses formulated, and beacons
(recognizable patterns) identiﬁed.
We found that in general, the RE process can be modeled
in three phases: overview, sub-component scanning, and fo-
cused experimentation. REs begin by establishing a broad
view of the program’s functionality (overview). They use
their overview’s results to prioritize sub-components—e.g.,
functions—for further analysis, only performing detailed re-
view of speciﬁc sub-components deemed most likely to yield
useful results (sub-component scanning). As REs review these
sub-components, they identify hypotheses and questions that
are tested and answered, respectively, through execution or
in-depth, typically manual static analysis (focused experimen-
tation). The last two phases form a loop. REs develop hy-
potheses and questions, address them, and use the results to
inform their understanding of the program. This produces new
questions and hypotheses, and the RE continues to iterate until
the overall goal is achieved.
Further, we identiﬁed several trends in REs’ processes span-
ning multiple phases. We found that REs use more static
analysis in the ﬁrst two phases and switch to dynamic sim-
ulation methods during focused experimentation. We also
observed that experience plays an important role through-
out REs’ decision-making processes, helping REs prioritize
where to search (overview and sub-component scanning), rec-
ognize implemented functionality and potential vulnerabilities
(sub-component scanning), and select which mental simula-
tion method to employ (all phases). Finally, we found REs
choose to use tools to support their analysis when a tool’s
input and output can be closely associated with the code and
when the tools improve code readability.
Based on these results, we suggest ﬁve guidelines for de-
signing RE tools.
2 Background and Related Work
While little work has investigated expert RE, there has been
signiﬁcant eﬀort studying similar problems of naturalistic
decision-making (NDM) and program comprehension. Be-
cause of their similarity, we draw on theory and methods that
have been found useful in these areas [26–32, 44, 45] as well
as in initial studies of RE [46].
2.1 Naturalistic Decision-Making
Signiﬁcant prior work has investigated how experts make
decisions in real-world (naturalistic) situations and the fac-
tors that inﬂuence them. Klein et al. proposed the theory
of Recognition-Primed Decision-Making (RPDM) [45, pg.
15-33]. The RPDM model suggests experts recognize compo-
nents of the current situation—in our case, the program under
investigation—and quickly make judgments about the cur-
rent situation based on experiences from prior, similar situa-
tions. Therefore, experts can quickly leverage prior experience
to solve new but similar problems. Klein et al. have shown
this decision-making model is used by ﬁreﬁghters [41, 42],
military oﬃcers [43, 47], medical professionals [48, pg. 58-
68], and software developers [49]. Votipka et al. found that
vulnerability-discovery experts rely heavily on prior experi-
ence [1], suggesting that RPDM may be the decision-making
model they use.
NDM research focuses on these decision-making processes
and uses interview techniques designed to highlight critical
decisions, namely the Critical Decision Method, which has
participants walk through speciﬁc notable experiences while
the interviewer records and asks probing follow-up question
about items of interest to the research (see Section 3.1) [44].
Using this approach prior work has driven improvements in
automation design. Speciﬁcally, these methods have identi-
ﬁed tasks within expert processes for automation [44, 50],
and inferred mental models used to support eﬀective inter-
action design [51] in several domains, including automobile
safety controls [52, 53], military decision support [44, 54–56],
and manufacturing [57, 58]. Building on its demonstrated
success, we apply the Critical Decision Method to guide our
investigation.
2.2 Program Comprehension
Program comprehension research investigates how develop-
ers maintain, modify, and debug unfamiliar code—similar
problems to RE. Researchers have found that developers ap-
proach unfamiliar programs from a non-linear, fact-ﬁnding
perspective [26–32]. They make hypotheses about program
functionality and focus on proving or disproving their hy-
potheses.
Programmers’ hypotheses are based on beacons recognized
when scanning through the program. Beacons are common
1876    29th USENIX Security Symposium
USENIX Association
schemas or patterns, which inform how developers expect
variables and program components to behave [28, 33–35]. To
evaluate their hypotheses, developers either mentally simu-
late the program by reading it line by line, execute it using
targeted test cases, or search for other beacons that contradict
their hypotheses [2, 28, 29, 33, 36]. Von Mayrhauser and Lang
showed developers switch among these methods regularly,
depending on the program context or hypothesis [59]. Further,
when reading code, developers focus on data- and control-ﬂow
dependencies to and from their beacons of interest [34, 60].
We anticipated that REs might exhibit similar behaviors, so
we build on this prior work by focusing on hypotheses, bea-
cons, and simulation methods during interviews (Section 3.1).
However, we also hypothesized some process divergence, as
RE and “standard” program comprehension diﬀer in several
key respects. Reverse engineers generally operate on obfus-
cated code and raw binaries, which are harder to read than
source code. Further, REs often focus on identifying and
exploiting ﬂaws in the program, instead of adding new func-
tionality or ﬁxing known errors.
2.3
Improving Usability for RE Tools
Several researchers have taken steps to improve RE tool us-
ability. Do et al. created a Just-in-time static analysis frame-
work called CHEETAH, based on the result of user stud-
ies investigating how developers interact with static analysis
tools [61, 62]. CHEETAH lets developers run static analyses
incrementally as they write new code, allowing developers
to put the analyses results in context and reduce the over-
whelming “wall of alerts” feeling. While we follow a similar
qualitative approach, we focus on a diﬀerent population (i.e.,
REs instead of developers) and task (RE instead of security
alert response).
Shoshitaishvili et al. propose a tool-centered human-
assisted vulnerability discovery paradigm [6]. They suggest
a new interaction pattern where users provide on-demand
feedback to a automated agent by performing well-deﬁned
sub-tasks to support the agent’s analysis. This model leverages
human insights to overcome the automation’s deﬁciencies,
outperforming the best automated systems while allowing
the analysis to scale signiﬁcantly beyond limited human re-
sources. However, the demonstrated interaction model speciﬁ-
cally targets non-expert users who do not understand program
internals (e.g., code, control ﬂow diagrams, etc.), treating the
program as a black box.
Focusing on expert users, Kruger et al. propose a speciﬁca-
tion language to allow cryptography experts to state secure
usage requirements for cryptographic APIs [63]. Unfortu-
nately, this approach still requires the expert to learn a new,
potentially complicated language—hundreds of lines of code
for each API.
Finally, Yakdan et al. designed a decompiler, DREAM++,
intended to improve usability compared to existing tools [5].
DREAM++’s experimental evaluation showed that a simple
set of code transformations signiﬁcantly increased both stu-
dents’ and professionals’ ability to RE malware, demonstrat-
ing the beneﬁt of even minor usability improvements.We hope
that our more complete investigation of REs’ processes may
spur the development of further high-impact improvements.
2.4 The Vulnerability Discovery Process
Ceccato et al. reviewed detailed reports by three penetra-
tion testing teams searching for vulnerabilities in a suite of
security-speciﬁc programs [2]. The participating teams were
asked to record their process for searching the programs, ﬁnd-
ing vulnerabilities, and exploiting them. Our study delves
deeper into the speciﬁc problem of RE a program to under-
stand its functionality. Further, through our interviews, we
are able to probe the RE’s process to elicit more detailed
responses.
Most similarly to this work, Bryant investigated RE using
a mixed methods approach, including three semi-structured
interviews with REs and an observational study where four
participants completed a predesigned RE task [46]. Based
on his observations, Bryant developed a sense-making model
for reverse engineering where REs generate hypotheses from
prior experience and cyclically attempt to (in)validate these
hypotheses, generating new hypotheses in the process. Our re-
sults align with these ﬁndings; we expand on them, producing
a more detailed model describing the speciﬁc approaches used
and how RE behaviors change throughout the process. Our
more detailed model is achieved through our larger sample