AGR-agnostics attacks to craft malicious gradients.
Intuition. All the robust AGRs for FL tend to remove/attenuate
malicious gradients based on one or more of the follow-
ing criteria: 1) distances from the benign gradients [8], [5],
[31], [2], [39], 2) distributional differences with the benign
gradients [5], [35], 3) difference in Lp-norms of the benign
and malicious gradients [35]. Figures 1-(b, c) visualize the
intuition behind our attacks based on the above criteria. The
intuition is as follows. The distance based defenses work by
removing the gradients that lie outside of the clique formed
by the benign gradients. Therefore, our attacks maximize
the distance of malicious gradient from a reference benign
gradient, while ensuring that the malicious gradients lie within
the clique of benign gradients. This also ensures that Lp-norms
of the malicious and benign gradients are similar. To ensure
distributional similarity, we use perturbations γ∇p with the
similar distributions as the benign gradients.
Next, we present optimization for two novel AGR-agnostic
attacks based on the intuition. We present the optimizations for
updates-only adversary, who has all the benign gradients
∇{i∈[n]}. The extension to agnostic adversary is similar to
6
that explained at the beginning of Section IV-B for agr-only
adversary.
Attack-1 (Min-Max): Minimize maximum distance attack.
Our ﬁrst attack ensures that the malicious gradients lie close
to the clique of the benign gradients (Figure 1-(b)). Hence, we
compute the malicious gradient such that its maximum distance
from any other gradient is upper bounded by the maximum
distance between any two benign gradients. (6) formalizes the
corresponding optimization. Note that in order to maximize
the impact of our attack, we keep all the malicious gradients
the same. Hence, we formulate our attack objective in (6) for
a single malicious gradient.
argmax
γ
(cid:107)∇m − ∇i(cid:107)2 ≤ max
max
i∈[n]
i,j∈[n]
∇m = favg(∇{i∈[n]}) + γ∇p
(cid:107)∇i − ∇j(cid:107)2
(6)
objective. Then we modify γ repeatedly and oscillate between
the minimum and maximum γ values until the change in γ is
below a threshold τ.
if O(∇{i∈[n]}, γ) == True then
Algorithm 1 Algorithm to optimize γ
1: Input: γinit, τ, O, ∇{i∈[n]}
2: step ← γinit/2, γ ← γinit
3: while |γsucc − γ| > τ do
4:
5:
6:
7:
8:
9:
10:
11: end while
12: Output γsucc
γsucc ← γ
γ ← (γ + step/2)
γ ← (γ − step/2)
end if
step = step/2
else
Attack-2 (Min-Sum): Minimize sum of distances attack.
Our second AGR-agnostic Min-Sum attack ensures that the
sum of squared distances of the malicious gradient from all the
benign gradients is upper bounded by the sum of squared dis-
tances of any benign gradient from the other benign gradients
(Figure 1-(c)). (7) formalizes the corresponding optimization.
We keep all malicious gradients the same for maximum attack
impact. Hence, we formulate our objective in (7) for a single
malicious gradient.
(cid:88)
argmax
γ
(cid:107)∇m − ∇i(cid:107)2
2 ≤ max
i∈[n]
i∈[n]
∇m = favg(∇{i∈[n]}) + γ∇p
(cid:88)
j∈[n]
(cid:107)∇i − ∇j(cid:107)2
2
(7)
D. Solving for the most effective scaling factor γ
In previous sections, we formulated optimizations for vari-
ous adversarial settings such that the ﬁnal objective is to search
for the optimal scaling coefﬁcient, γ. Algorithm 1 describes
our algorithm to optimize γ for any of the optimizations.
For clarity of presentation of Algorithm 1, we assume
an oracle O that takes the set of benign gradients, ∇{i∈[n]}
and γ as inputs. Then, O computes malicious gradients as
∇m{i∈[m]} = ∇b + γ∇p, and outputs True if they satisfy the
adversarial objective, otherwise outputs False. For instance,
for our AGR-tailored attack on Krum, O outputs True if a
malicious gradient is selected by fkrum, i.e., if (3) is satisﬁed.
For our Min-Max attack (Section IV-C), O outputs True if
the maximum distance of malicious gradient from any benign
gradient is lower than the maximum distance between any two
benign gradients, i.e., if (6) is satisﬁed.
Now, we describe Algorithm 1. The core idea of our
algorithm is as follows: We start with a large γ value. We
reduce γ in steps of size step until O returns True, e.g., for
Krum, we reduce γ until a malicious gradient is selected by
fkrum, i.e., (3) is satisﬁed for the ﬁrst time. Our ﬁnal γ is
always greater than this minimum γ value that satisﬁes the
objective. We halve the step size each time we update γ in
order to make the search ﬁner. From the minimum γ value,
we increase γ using updated step sizes step, until O returns
False, i.e., for Krum, we increase γ until fkrum does not select
any malicious gradient, i.e., (3) is no more satisﬁed. Our ﬁnal
γ is always lower than this maximum γ value that satisﬁes the
7
V. EXPERIMENTAL SETUP
A. Datasets and model architectures
CIFAR10 [24] is a 10-class class-balanced classiﬁcation task
with 60,000 RGB images, each of size 32 × 32. ‘Class-
balanced’ datasets have the same number of samples per class,
e.g., each class of CIFAR10 has 6,000 images. We use 50
clients each with 1,000 samples and use validation and test
data of sizes 5,000 each. We use Alexnet [25] and VGG11 [34]
as the global model architectures.
MNIST [27]
is a 10-class class-balanced classiﬁcation task
with 70,000 grayscale images, each of size 28 × 28. We use
100 clients each with 600 samples and use validation and test
data of sizes 5,000 each. For MNIST, we use a fully connected
network (FC) with layer sizes {784, 512, 10} as the global
model architecture.
Purchase [1]
is a 100-class class-imbalanced classiﬁcation
task with 197,324 binary feature vectors, each of length 600.
We use 80 clients each with 2,000 training samples and use
validation and test data of sizes 5,000 each. We use a fully
connected network with layer sizes {600, 1024, 100}.
FEMNIST [9], [13]
is a character recognition classiﬁcation
task with 3,400 clients, 62 classes, and a total of 671,585
grayscale images. Each of the 3,400 clients has her own data
made of her own handwritten digits or letters (62 classes: 52
for upper and lower case letters and 10 for digits). The mean
and standard deviation of the number of samples per client are
226.83 and 88.94, respectively. In each FL epoch, we randomly
select 60 out of 3400 clients for FL training. FEMNIST is
a non-iid, class-imbalanced dataset commonly encountered in
cross-device FL settings [21], while the previous datasets are
more common in cross-silo FL settings.
B. Learning and attacks settings
We train CIFAR10 with Alexnet using batch size of 250
and SGD optimizer with learning rates of 0.5 from epochs
0-1000 and 0.05 from 1000-1200. We train CIFAR10 with
VGG11 using batch size of 200 and SGD optimizer with
learning rates of 0.1 from epochs 0-1000 and 0.01 from 1000-
1200. We train MNIST for 500 epochs using Adam optimizer
with 0.001 learning rate and batch size of 100. We train
Purchase for 1000 epochs using SGD with learning rate of 0.5
and batch size of 500. We train FEMNIST for 1500 epochs
using Adam optimizer with learning rate of 0.001 and use
client’s entire data in a each batch.
Unless speciﬁed otherwise, we assume 20% malicious
clients for all adversarial settings, e.g., 20 malicious clients for
MNIST. For most of our evaluation, we use independently and
identically distributed (iid) CIFAR10, MNIST, and Purchase
datasets, because poisoning FL with iid data is the hardest [17].
Measurement metrics. For a given FL setting, Aθ denotes
the accuracy of the best globel model, over all the FL training
epochs, in the benign setting without any attack, while A∗
θ
denotes the accuracy under the given attack. We deﬁne attack
impact, Iθ, as the reduction in the accuracy of the global model
due to the attack, hence for a given attack, Iθ = Aθ − A∗
θ.
C. Baseline model poisoning attacks
Below, we detail here two state-of-the-art model poisoning
attacks, LIE [4] and Fang [17], that we compare against.
LIE: The LIE attack [4] adds small amounts of noises to each
dimension of the average of the benign gradients. The small
noises sufﬁciently large to adversely impact the global model
and sufﬁciently small to evade detection by the Byzantine-
robust AGRs. Speciﬁcally, the adversary computes the average
µ and standard deviation σ of the benign gradients she has,
computes a coefﬁcient z based on the total number of benign
and malicious clients, and ﬁnally computes the malicious
update as µ + zσ.
Fang: The Fang attack [17] is an optimization based model
poisoning attack tailored to Krum AGR. The adversary com-
putes the average µ of the benign gradients she has, computes a
perturbation ∇p = −sign(µ), and ﬁnally computes a malicious
update as ∇m = (∇b +γ·∇p) by solving for the coefﬁcient γ.
The attack starts from a reference γ and keeps halving it until
Krum select the resulting ∇m, therefore unlike our attacks,
Fang attack does not optimize γ (Figure 1-(a)).
VI. EVALUATION OF OUR ATTACKS
A. Comparison with the state-of-the-art attacks
In this section, we compare our attacks with state-of-the-
art model poisoning attacks, Fang [17] and LIE2 [4], for all
the adversaries from Table I. The results are given in Table II;
‘No attack’ column shows accuracy Aθ of the global model
in the benign setting, while the rest of the columns show the
‘attack impact’ Iθ, as deﬁned in Section V-B.
For a fair comparison, we compare the attacks that use
the knowledge of AGR,
i.e., our AGR-tailored and Fang
attacks under agr-updates and agr-only adversaries.
We separately compare the attacks that do not use the knowl-
edge of AGR, i.e., our AGR-agnostic and LIE attacks under
updates-only and agnostic adversaries.
2We omit the sufﬁx ’attack’ when it is clear from the context.
1) Comparing AGR-tailored attacks: Table II shows that,
our AGR-tailored attacks outperform Fang attacks for
all
the combinations of threat model, AGR, dataset, and
model architecture by large margins. For CIFAR10 with
agr-updates adversary, our attacks are 2× more impactful
than Fang. While with agr-only adversary, our attacks
are 2.5× and 4.5× more impactful than Fang for Alexnet
and VGG11 models, respectively. For the rest of the AGRs,
our attacks are 3× to 7× (2× to 4×) more impactful than
Fang attacks on CIFAR10 with Alexnet (VGG11) for both
agr-updates and agr-only adversaries.
Under agr-updates (agr-only) adversary, Fang and
our attacks on Krum with MNIST have impacts of 20.5%
(17.4%) and 33.9% (24.1%), respectively, i.e., our attack is
1.7× (1.5×) more effective than Fang. The impact of Fang
attack on Trimmed-mean (Median) with MNIST is just 1.2%
(1.7%), while that of our attack is 11.0% (4.4%), i.e., our
attack is 10× (2.5×) more impactful. Even for AFA, which
is the empirically most robust AGR for MNIST, our attack is
3× more impactful than Fang.
For Purchase, our attacks reduce the accuracy of Krum to
the random guessing, i.e., close to 1% for all the adversaries
least 10× more
and, except for AFA, our attacks are at
impactful than Fang attacks. Similarly, with agr-updates
adversary,
impacts of Fang attacks on Trimmed-mean and
Median are 1.8% and 0.2%, respectively, while impacts of
our attacks are 23.4% and 11.0%. We note similarly higher
impacts of our attacks with agr-only adversary. For Multi-
krum, Bulyan, and AFA, our attacks are 2× more impactful
than Fang attacks. For Fang-Trmean, the Fang defense that
uses Trimmed-Mean to discard malicious gradients, our attacks
reduce the global model accuracy to random guessing for
all combinations of datasets and models; this is expected as
discussed in Section II-C7.
of
our
impacts
For FEMNIST the
attacks with
agr-updates adversary on AFA, Multi-krum, Trimmed-
mean, and Median are respectively 12×, 2×, 3×, and 15×,
that of Fang attack. For Krum and Bulyan also, the impacts
of our attacks are moderately higher than that of Fang attacks.
Why our attacks are superior? For Krum AGR, although
ours and Fang attacks have similar attack objectives, they differ
in two instrumental aspects: First, instead of generalizing a
single perturbation type across all datasets, our attacks tailor
the perturbation to the given dataset (as we will explain
in Section VI-C). Next, as Figure 1-(a) demonstrates, our
Algorithm 1 carefully ﬁne tunes γ of our objective (3), while
Fang attack simply ﬁnds the ﬁrst γ that satisﬁes its objective.
Our attacks on AFA, Bulyan, and Multi-krum AGRs are also
carefully tailored to the AGRs, while Fang attack uses the same
objective as Krum for these AGRs.
Fang proposes the same attack for Trimmed-mean and
Median AGRs, which crafts the values of each dimension
of malicious gradients using the available benign gradients.
But our attacks have more tailored and impactful objectives
of diverging the ﬁnal aggregate as far away from a benign
aggregate as possible using the most malicious perturbation
direction.
2) Comparing AGR-agnostic attacks: Table II shows that,
both of our AGR-agnostic attacks signiﬁcantly outperform
8
Table II: Comparing state-of-the-art model poisoning attacks and our attacks under various threat models from Table I, when cross-silo FL is
used. In all the settings, the impact of our AGR-tailored attacks is signiﬁcantly higher than that of AGR-tailored Fang attacks. While both of
our AGR-agnostic attacks outperform AGR-agnostic LIE attacks in most cases. We assume 20% malicious clients and, except for ‘No attack’
column, report the attack impact Iθ (Section V-B). For each adversary, we bold Iθ of the strongest attack.
Gradients of benign devices are known
AGR agnostic
AGR tailored
(agr-updates)
(updates-only)
Our attacks
Min-Max Min-Sum
Gradients of benign devices are unknown
AGR agnostic
(agnostic)
Our attacks
Min-Max Min-Sum
30.2
30.4
41.1
27.9
39.5
16.0
19.9
25.9
25.9
46.6
20.4
29.7
10.2
25.5
61.0
16.4
30.3
14.6
12.6
0.5
7.8
25.3
12.6
7.7
8.5
2.0
1.6
8.3
8.0
61.4
30.4
25.2
16.6
46.0
60.1
Dataset
(Model)
CIFAR10
(Alexnet)
CIFAR10
(VGG11)
Purchase
(FC)
MNIST
(FC)
FEMNIST
(CNN)
AGR