attacks on commercial cloud speech APIs.
â€¢ Practical physical attacks without any interaction. We explore the
possibility of generating audio AEs and playing them against the
commercial voice control devices over-the-air. We thus for the first
time propose a new non-interactive physical attack, named NI-
Occam, which can successfully fool various voice control devices,
including Apple Siri, Microsoft Cortana, Google Assistant, iFlytek
and Amazon Echo, without any feedback information from the
target devices. The experimental results show that our over-the-
air attack can achieve an average success rate of 52% with SNR
of 9.65dB. This observation is shedding light on non-interactive
physical attacks against voice control devices.
2 BACKGROUND
2.1 Speech Recognition
Automatic speech recognition (ASR) systems allow machines to
automatically convert speeches into texts, and have found tremen-
dous applications. Typically, an ASR system consists of four main
components: pre-processing, feature extraction, acoustic model,
and language model, as in Figure 1. Pre-processing plays an impor-
tant role to filter out the frequencies beyond the range of human
hearing and the segments below a specific energy threshold in the
raw audio. Then, features are extracted via a feature extraction al-
gorithm, such as Mel-frequency Cepstral Coefficients (MFCC) [63],
Linear Predictive Coefficient (LPC) [49], Perceptual Linear Predic-
tive (PLP) [44], etc. Different from image classification models [56]
which take pixels as input, the acoustic model takes the extracted
features as input, and outputs the probability of phonemes. In early
ASR systems, Hidden Markov Model is one of the preferred tech-
niques [16]. With the stupendous advance of deep learning, deep
neural networks (DNNs) [45] and especially recurrent neural net-
works (RNNs) [41, 70] have become the dominant choices today.
An ASR system will finally produce the correct transcriptions con-
forming to grammatical rules via the language model.
2.2 Speaker Recognition
Due to the tremendous advance of DNN, speaker recognition (SR)
systems are becoming increasingly popular in biometric authenti-
cation [30, 58, 91]. The systems, which allow machines to correctly
Figure 1: The architecture of a typical ASR system.
Figure 2: The architecture of a typical SR system.
identify a person from his/her unique characteristics of voices, have
various applications such as bank services and forensic tests. The
key step then is to extract usersâ€™ voice features from a series of utter-
ances which essentially consist of the underlying text information
and the features of the speaker.
Figure 2 provides the overview of a typical SR system [54], which
consists of two phases: enrollment and evaluation. Generally, in
the enrollment phase, a set of background speakers are required
to train the background model in the offline phase such that a
speaker can provide a few utterances online to create a new specific
speaker model. The technologies for generating such models can
be generally summarized as three types: i-vector-PLDA [65, 66],
GMM-UBM [72, 73], and DNN [77, 91]. During the evaluation phase,
the unknown speakerâ€™s voice is taken as input and scored by the
speaker models in the library. Based on the resulting scores, the
decision model will generate the final recognition result.
There are two important sub-tasks in speaker recognition: speaker
verification (SV) [72] and speaker identification (SI) [52]. The for-
mer is to validate whether the current user is legitimate, i.e., output
either accept or reject. The latter aims to figure out the identity of
the speaker among a set of enrolled ones. According to its text de-
pendence, SR systems can also be divided into text-dependent and
text-independent [71]. The difference is that the text-dependent ap-
proach requires all speakers to utter pre-defined sentences. Despite
higher accuracy, it is only used for the SV sub-task.
2.3 Adversarial Examples
Despite their great success, the vulnerabilities of neural networks
to adversarial examples have recently been extensively studied [24,
40, 62, 78]. An adversary can slightly revise the legitimate inputs
to generate adversarial examples for fooling neural networks [78].
Generally speaking, there are two types of adversarial examples:
untargeted and targeted ones, also known as dodging attacks and
impersonation attacks, respectively. Let ğ‘“ (ğ‘¥) : ğ‘¥ âˆˆ X â†’ ğ‘¦ âˆˆ
Y denote the recognition model that maps the input ğ‘¥ into the
corresponding output prediction ğ‘¦. Given the original input ğ‘¥ with
the prediction ğ‘¦ such that ğ‘“ (ğ‘¥) = ğ‘¦, an untargeted adversarial
ğ‘ .ğ‘¡ .
ğ‘¥âˆ—,
example ğ‘¥âˆ— in the dodging attacks can be represented as:
ğ‘“ (ğ‘¥âˆ—) â‰  ğ‘¦, ğ·(ğ‘¥, ğ‘¥âˆ—) =(cid:13)(cid:13)ğ‘¥ âˆ’ ğ‘¥âˆ—(cid:13)(cid:13)ğ‘ â‰¤ ğœ–,
(1)
where ğ·(ğ‘¥, ğ‘¥âˆ—) is the distance between an original input ğ‘¥ and
an untargeted adversarial example ğ‘¥âˆ—, ğœ– is a parameter used to
limit this distance, and ğ‘ is typically 0, 2, or âˆ. Similarly, in the
impersonation attacks, for an original input ğ‘¥ and a specific ğ‘¦âˆ—
such that ğ‘“ (ğ‘¥) â‰  ğ‘¦âˆ—, a targeted adversarial example ğ‘¥âˆ— can be
represented as:
ğ‘¥âˆ—,
ğ‘ .ğ‘¡ .
ğ‘“ (ğ‘¥âˆ—) = ğ‘¦âˆ—, ğ·(ğ‘¥, ğ‘¥âˆ—) â‰¤ ğœ–.
(2)
3 OCCAM: A DECISION-ONLY DIGITAL
ATTACK AGAINST CLOUD SPEECH APIS
3.1 Threat Model
Nowadays, many commercial cloud speech platforms offer both
ASR and SR API services, e.g., Microsoft Azure Speech and Speaker
Recognition Service API, and thus third-party developers can access
commercial APIs if they have enrolled or paid for the services. The
service providers, on the other hand, will not expose any param-
eters or the architecture of the target model because the internal
information is commercially sensitive. Actually, a number of API
services, e.g., iFlytek, Alibaba, Tencent, and Jingdong, provide only
the final decision results without exposing any other information.
Therefore, it is important to explore a generic attack against both
ASR and SR commercial APIs in this decision-based scenario. Note
that although ASR tasks are different from SR tasks, we can treat
them as the same problem in this design because the construction
of audio AEs for ASR and SR APIs can be formulated as the same
optimization problem.
In this section, our target is commercial cloud speech services
that open their APIs to the public, and we assume that the adversary
intends to generate AEs against both ASR and SR services without
any internal knowledge of the target model. More specifically, the
adversary can only query the target model and obtain its final
decision, which is a strict but more practical assumption in real-
world applications. Considering the adversaryâ€™s knowledge of the
original audio, we make two different assumptions for the ASR
and SR tasks respectively. For ASR systems, we assume that the
adversary knows nothing about the dataset, and thus we utilize
Text-to-Speech Service API to generate the audio of the target text.
For SR systems, the adversary only needs to collect the victimâ€™s
one voice sample, which is readily available owing to the serious
leakage of personal information, e.g., public videos in social media.
3.2 Problem Formulation
For an acoustic system, given a voice ğ‘¥ âˆˆ Rğ‘› and a specific ğ‘¦âˆ— such
that ğ‘“ (ğ‘¥) â‰  ğ‘¦âˆ—, a targeted AE ğ‘¥âˆ— can be described by
ğ‘“ (ğ‘¥âˆ—) = ğ‘¦âˆ—, ğ·(ğ‘¥, ğ‘¥âˆ—) â‰¤ ğœ–.
ğ‘¥âˆ—,
ğ‘ .ğ‘¡ .
(3)
In the white-box setting, the problem can be formulated as
ğ‘¥âˆ— L(ğ‘¥âˆ—) = D(ğ‘¥âˆ—, ğ‘¥) + ğ‘ Â· J(ğ‘¥âˆ—, ğ‘¦âˆ—),
min
(4)
where L(Â·) is the objective function, J(Â·, Â·) the loss function to
check how well ğ‘¥âˆ— meets the adversarial requirement, and ğ‘ the
Input audioâ€œHow are youâ€Pre-processingFeature ExtractionAcoustic ModelLanguage ModelFinal resultEnrollment phaseRecognition phasePatternMatchingSpeaker ModelingBackground speakersBackground ModelingOffline phaseFinal resultOnline phaseFeature ExtractionEnrolling speakerUnknown speakerDecisionAlgorithm 1 Occam
Input: The original audio ğ‘¥, the initial adversarial audio ğ‘¥â€², the
input space dimension ğ‘›, the attack objective function L(Â·),
the binary search time ğ‘ and the total number of queries ğ‘‡ .
Output: The adversarial audio sample ğ‘¥âˆ—.
1: Initialize the following parameters:
â€¢ Covariance matrix ğ¶ = ğ¼ğ‘›, ğ‘¥âˆ— = ğ‘¥â€², ğ‘¡ = 0;
â€¢ ğ›¿, ğœ‡, ğ‘ğ‘, ğ‘ğ‘ğ‘œğ‘£ âˆˆ R+, ğ‘š, ğœ† âˆˆ Z+ (See Section 3.4.5).
decision boundary via binary search algorithm;
2: Query ğ‘ times to find the new adversarial point ğ‘¥âˆ— close to the
3: ğ‘¡ = ğ‘¡ + ğ‘;
4: Choose a grouping strategy and a group number ğ‘š according
5: Decompose the audio vector into ğ‘š disjoint parts with each
to the adaptive scheme (See Section 3.4.4);
part having ğ‘  = ğ‘›/ğ‘š dimensions;
6: Set ğ‘ ğ‘¢ğ‘ = 1 to start an optimization cycle;
7: Extract two ğ‘  Ã— 1 vectors ğ‘¥âˆ—
matrix ğ¶ğ‘ ğ‘¢ğ‘ from ğ‘¥âˆ—, ğ‘¥ and ğ¶, respectively;
ğ‘ ğ‘¢ğ‘ and ğ‘¥ğ‘ ğ‘¢ğ‘ and an ğ‘  Ã— ğ‘  covariance
8: for ğ‘– = 0 to ğœ† do
9:
10:
Sample ğ‘§ âˆ¼ N(0, ğœ2 Â· ğ¶ğ‘ ğ‘¢ğ‘);
Generate one complete solution solu using ğ‘¥âˆ—
ğ‘ ğ‘¢ğ‘ + ğœ‡(ğ‘¥ğ‘ ğ‘¢ğ‘ âˆ’
ğ‘¥âˆ—
ğ‘ ğ‘¢ğ‘)+ğ‘§ and collaborative information from other subspaces;
if L(solu) < L(ğ‘¥âˆ—) then
ğ‘¥âˆ—
ğ‘ ğ‘¢ğ‘ = ğ‘¥âˆ—
Update ğ¶ğ‘ ğ‘¢ğ‘ according to Eqs. (8) and (9);
ğ‘ ğ‘¢ğ‘ + ğœ‡(ğ‘¥ğ‘ ğ‘¢ğ‘ âˆ’ ğ‘¥âˆ—
ğ‘ ğ‘¢ğ‘) + ğ‘§;
ğ‘ ğ‘¢ğ‘ and ğ¶ğ‘ ğ‘¢ğ‘ to update ğ‘¥âˆ— and ğ¶, respectively;
11:
12:
13:
end if
14:
15: end for
16: ğ‘¡ = ğ‘¡ + ğœ†;
17: Use ğ‘¥âˆ—
18: if ğ‘¡ â‰¥ ğ‘‡ then
return ğ‘¥âˆ—.
19:
20: else if ğ‘ ğ‘¢ğ‘ < ğ‘š then
ğ‘ ğ‘¢ğ‘ + + and go to step 5;
21:
22: else
23:
24: end if
Go to step 2;
(a) Continuous optimization
(b) Discontinuous optimization
Figure 3: An illustration of continuous (left) and discontin-
uous (right) optimization problem.
Figure 4: The architecture of our Occam.
(cid:40)D(ğ‘¥âˆ—, ğ‘¥),
adjustment parameter [25]. In the black-box setting without internal
knowledge, we can reformulate it as an optimization problem as
if ğ‘“ (ğ‘¥âˆ—) = ğ‘¦âˆ—,
otherwise.
ğ‘¥âˆ— L(ğ‘¥âˆ—) =
min
+âˆ,
(5)
Note that the L(ğ‘¥âˆ—) is equal to +âˆ when ğ‘¥âˆ— is not adversarial. Thus,
we try to find the adversarial region and minimize its distance from
the original audio. Similarly, the construction of untargeted audio
AEs in our decision-based black-box attack can be reformulated as
(cid:40)D(ğ‘¥âˆ—, ğ‘¥),
+âˆ,
ğ‘¥âˆ— L(ğ‘¥âˆ—) =
min
if ğ‘“ (ğ‘¥âˆ—) â‰  ğ‘“ (ğ‘¥),
otherwise.
(6)
3.3 Technical Challenges
As mentioned above, compared to the white-box and score-based
settings, it is quite challenging to craft decision-based black-box ad-
versarial examples against acoustic systems, since the attacker has
no internal knowledge about the target model, except for the final
decision corresponding to the query. Thus, we are facing several
design challenges. First, depending on whether the output decision
matches the adversarial target or not, the optimization space can be
divided into a large non-adversarial region and a very small adver-
sarial region. Hence, different from almost all previous adversarial
voice attacks which feature a continuous optimization problem,
the construction of audio AEs in the strictly black-box scenario
poses a difficult discontinuous optimization problem due to the
extreme lack of information, as shown in Figure 3. Second, the
acoustic systems are extremely complicated, and we have to deal
with intricate feature changes of the audios in the time dimension.
Finally, because the audio sampling rate is very high (e.g., 16kHz),
the large number of optimization variables in the speech vector
presents another challenge, i.e., the curse of dimensionality, espe-
cially when there is a clear interdependence among the variables in
audio AEs [88]. The reason behind it is that as the number of opti-
mization variables increases, the complexity of the problem grows
exponentially, and the nature of the problem may also change.
3.4 Our Method
As noted in our threat model, the lack of internal knowledge (e.g.,
structures, parameters, gradients, and scores) about the target model
further exacerbates the difficulty of crafting AEs. All the adversary
can do is to send a limited number of queries to probe the system as
far as possible, and obtain the corresponding final decisions. More
specifically, the initiation of our decision-based black-box adversar-
ial attack only needs the final decision, e.g., final transcription, and
this kind of audio AE generation can be formulated as a discontin-
uous and large-scale global optimization problem. To solve it, we
resort to the large-scale black-box optimization approach.
Original audioâš«ASR âš«SR Target SystemsQueryDecisionInitial adversarial audioAdversarial audioSub-problemsComplete SolutionCCCMA-ESBest SolutionOffspringCollaborator ConstructionDesign Overview. Note that we are going to fool both commercial
ASR and SR services with the minimum required information from
the target model. To address the challenges, we propose a new class
of cooperative co-evolution methods to generate effective audio AEs.
Our method is mainly developed from CC-CMA-ES [60]. However,
we cannot directly apply CC-CMA-ES to constructing audio AEs. In
the audio domain, the correlations between variables will change
in the dynamic evolution process, which is not considered in [60].
To solve this problem, we devise an adaptive scheme to make our
strategy self-adapt to the environmentally changeable evolution
process, the core of our cooperative co-evolution framework.
In the literature, the CMA-ES [43], known as an efficient evolu-
tionary algorithm, has already demonstrated its good performance
on many problems. But it will lose its effectiveness when applied to
large-scale global optimization problems due to â€œthe curse of dimen-
sionalityâ€. A dimensionality reduction strategy, which uses the bilin-
ear interpolation method to project the original space (112Ã—112Ã—3)
to a lower-dimensional search space (e.g., 45 Ã— 45 Ã— 3), has been
carefully devised to effectively create adversarial images against
face recognition models [34]. However, this method is also not ap-
plicable to the audio domain. This is because bilinear interpolation
works in two directions on images, while the inputs to the commer-
cial Cloud Speech APIs are one-dimensional vectors from speeches.
Thus, we for the first time introduce the general cooperative co-
evolution (CC) framework to construct audio AEs in the strictly
black-box setting as shown in Figure 4. Our CC framework can
scale up CMA-ES to deal with large-scale optimization problems we
are facing, by decomposing these challenging problems into a set
of simpler and smaller sub-problems and cooperatively optimizing
each of them. Considering that the group size and the decompo-
sition strategy play a crucial role in CC, we further propose an
adaptive scheme to improve the attack efficiency by letting the size
of sub-problems and the decomposition strategy self-adapt to the
environmentally changeable evolution process.
Our Occam is presented in Alg. 1. In each optimization cycle, the
original problem is decomposed into a set of smaller and simpler
subproblems according to the selected grouping strategy. Then,
a new offspring is generated from the current solution in each
subproblem by a subspace CMA-ES whose parameters are extracted
from a global CMA-ES, and the complete candidate solution can be
further obtained by using the collaborative information from other
subproblems and the generated offspring. The objective function
is further used to evaluate the two solutions and choose the better
one, based on which we update the covariance matrix accordingly.
Next, we will describe each step of the algorithm in detail.
3.4.1
Initialization. As shown in Eq. (5), the optimization rou-
tine should start from an adversarial point, because the value of the
objective function is equal to +âˆ when the input is not adversarial.
We first initialize ğ‘¥âˆ— with a natural adversarial sample distant from
the original audio. More specifically, we utilize the text-to-speech
API service to synthesize the desired speech as an initial adversarial
audio sample against ASR systems. For SR systems, we initialize
ğ‘¥âˆ— using an audio of the target speaker, which can be obtained
from the speakerâ€™s self-recorded songs and videos posted on public
social networks. Since the initial input audio is adversarial and the
Figure 5: The diagram of CMA-ES.
original one is not adversarial, we can first utilize the binary search
algorithm to effectively approach the decision boundary.
3.4.2 Covariance Matrix Adaptation Evolution Strategy. CMA-
ES, known as an efficient derivative-free method, generates off-
springs by sampling a multivariate normal distribution with co-
variance ğ¶, i.e., N(0, ğ¶). To facilitate understanding, we provide
a brief summary of CMA-ES, as shown in Figure 5. Since the co-
variance is the measure of the relationship between two random
variables, it can use the selected sample distribution to estimate
the covariance for learning dependencies between variables [43].
Due to the unreliability of this estimation for small samples, the
covariance matrix adaptation using the history information and the
evolution path ğ‘ƒ, a sequence of successive and normalized steps,
thus has been introduced. By adaptively updating the estimated
covariance, CMA-ES is able to find better search directions, and
achieve a powerful local search by modeling local geometries.
Our framework uses a simple yet effective variant of CMA-ES, i.e.,
(1+1)-CMA-ES [48]. This version generates one candidate solution
from current solution by sampling a random noise, and selects a
better solution according to its objective function. Since the direc-
tion of optimization in our attack is to find a new adversarial audio
closer to the original audio according to the objective function L(Â·),
we adopt a modified (1+1)-CMA-ES [34] to improve its efficiency
by adding a bias term ğœ‡(ğ‘¥ âˆ’ ğ‘¥âˆ—) to current solution ğ‘¥âˆ— as
ğ‘– + ğœ‡(ğ‘¥ âˆ’ ğ‘¥âˆ—
ğ‘– ), ğœ2 Â· ğ¶),
ğ‘¥âˆ—
ğ‘–+1 âˆ¼ N(ğ‘¥âˆ—
(7)
where ğœ is the global step size, ğ¶ is the covariance matrix that de-
termines the shape of the distribution, and ğœ‡ is a parameter that
controls the degree of proximity towards the original audio. Further-
more, we can directly remove candidate solutions that are farther
from the original audio, regardless of whether it is adversarial.
Note that the covariance matrix ğ¶ plays a very important role in
CMA-ES since it models the local geometries to improve the local
search efficiency. However, the adaptation of covariance matrix
with the complexity of O(ğ‘›3) may be infeasible when the input
dimension ğ‘› is huge. To speed up the computation, the covariance
matrix ğ¶ can be simplified as a diagonal matrix [34] and updated
adaptively by the evolution path ğ‘ƒ as
ğ‘ƒ = (1 âˆ’ ğ‘ğ‘)ğ‘ƒ +âˆšï¸ğ‘ğ‘(2 âˆ’ ğ‘ğ‘) ğ‘§
ğœ
ğ¶ = (1 âˆ’ ğ‘ğ‘ğ‘œğ‘£)ğ¶ + ğ‘ğ‘ğ‘œğ‘£ğ‘ƒ(ğ‘ƒ)ğ‘‡ ,
,
(8)
(9)
where ğ‘ğ‘ and ğ‘ğ‘ğ‘œğ‘£ are the parameters controlling the adaptation of
ğ‘ƒ and ğ¶, respectively. The update enlarges the variance along the
past successful directions for future search.
3.4.3 Cooperative Co-evolution. It has been proven that the per-
formance of evolution algorithms may drop significantly [60, 89],
Generateoffspringaccording to Eq. (7)Sortallsolutionsandselect the elitesUpdatethecovariancematrix according toEqs. (8) and (9)Evaluate new solutions accordingtoEqs. (5) or (6)YesNoMax iteration?Update the parameter according to Eq. (10)Input Initial populationand parametersOutputthebestsolution and itsfitness scoreas the dimensionality of the problem increases, because the com-
plexity of the problem grows exponentially and the property of
the problem may also change. To scale up CMA-ES to the high-
dimensional optimization problem, we use cooperative co-evolution
(CC) to conduct the large-scale black-box optimization in a divide-
and-conquer manner, by decomposing the large-scale problem into
several smaller sub-problems and optimizing each sub-problem
alternately and iteratively. Considering that each subproblem is
only a part of the original problem, the collaborative information
from other sub-problems is required to evaluate individuals in the
current sub-problem. Generally speaking, the best solutions of each
sub-problem in the current cycle are used as the collaborative in-