cwnd = ssthresh
1601. prr delivered is the total number of unique bytes de-
livered to the receiver since the start of recovery ac-
counted through cumulative ACKs or through SACKs.
2. prr out is the total bytes transmitted during recovery.
30000 
20000 
.
.
3. RecoverF S is the FlightSize (deﬁned as snd.nxt −
10000 
snd.una in RFC 3517 [8]) at the start of recovery.
In addition to these, PRR maintains two local variables:
0 
100 ms 
150 ms 
200 ms 
250 ms 
300 ms 
350 ms 
1. DeliveredData is the number of new bytes that the
current acknowledgment indicates have been delivered
to the receiver.
2. sndcnt determines how many bytes to send in response
to each ACK. Note that the decision of which data to
send (e.g., retransmit lost data or send additional new
data) is independent of PRR.
Algorithm 2 shows how PRR updates sndcnt on every
ACK. When pipe is larger than ssthresh, the proportional
part of the algorithm spreads the the window reductions
across a full round-trip time, such that at the end of re-
covery, prr delivered approaches RecoverF S and prr out
approaches ssthresh. If there are excess losses that bring
pipe below ssthresh, the algorithm tries to build the pipe
close to ssthresh. It achieves this by ﬁrst undoing the past
congestion window reductions performed by the PRR part,
reﬂected in the diﬀerence prr delivered − prr out. Second,
it grows the congestion window just like TCP does in slow
start algorithm. We note that this part of the algorithm
increases the number of segments in ﬂight during recovery,
however it does so more smoothly compared to RFC 3517
which would send ssthresh-pipe segments in a single burst.
A key part of Algorithm 2 is its reliance on the newly
delivered data per acknowledgment,
referred above as
DeliveredData. This is a stark diﬀerence from the existing
standard and widely implemented TCP algorithms which
make their congestion window adjustments based on the
number of ACKs received as opposed to the actual data
delivered. The use of DeliveredData is especially robust
during recovery. The consequence of missing ACKs is that
later ones will show a larger DeliveredData. Furthermore,
for any TCP connection, the sum of DeliveredData must
agree with the forward progress over the same time inter-
val. In addition, for TCP using SACK, DeliveredData is
an invariant that can be precisely computed anywhere in the
network just by inspecting the returning ACKs.
4.3 Properties of PRR
In this section we discuss a few key properties of PRR
that follow from its design. For each property, we also dis-
cuss the corresponding behavior for RFC 3517 and Linux
fast recovery.
1. Maintains ACK clocking: PRR maintains ACK
clocking even for large burst segment losses, primar-
ily due to the slow start part of the algorithm. This
property is not true for the existing RFC 3517 standard
which under heavy losses, can send an arbitrarily large
burst of size (ssthresh − pipe), as mentioned in the
RFC problem of bursty retransmissions (Section 3.1).
Linux fast recovery maintains the ACK clocking prop-
erty, however it does so by bringing the congestion
window down to pipe + 1.
Figure 4: Example illustrating that PRR banks send-
ing opportunities during recovery. Application ﬁrst has
20 segments to send of which 1 segment is lost. In the
middle of recovery, application has 10 more segments
to transmit. RTT=100ms. Legend for the plot: Origi-
nal data transmissions (Black), retransmissions (Red),
duplicate ACKs with SACK blocks (Purple), snd.una
(Green).
2. Convergence to ssthresh: For small number of
losses (losses ≤ RecoverF S − ssthresh), PRR con-
verges to exactly the target window chosen by the con-
gestion control algorithm, ssthresh. The algorithm for
the most part operates in the proportional mode which
decrements the congestion window until pipe reaches
ssthresh at which point the second part of the algo-
rithm maintains the pipe at the ssthresh value. We
note that for very heavy losses, the property will not
always hold true because there may not be suﬃcient
ACKs to raise pipe all the way back to ssthresh.
If there is suﬃcient new data and at least a few
segments delivered near the end of the lossy round
trip, RFC 3517 achieves this property regardless of the
amount of loss, because it transmits a large burst.
Linux fast recovery does not achieve this property
when losses are heavy or when the application tem-
porarily runs out of data to send.
In both of these
cases, pipe becomes too small and does not grow until
after the recovery ends.
3. Banks sending opportunities during applica-
tion stalls: If an application stalls during recovery,
e.g., when the sending application does not queue
data for transmission quickly enough or the receiver
stops advancing rwnd, PRR stores these missed op-
portunities for transmission. During application stalls,
prr out falls behind prr delivered, causing their diﬀer-
ence prr delivered − prr out to be positive. If the ap-
plication catches up while TCP is still in recovery, TCP
will send a burst that is bounded by prr delivered −
prr out + 1.4 Note that this property holds true for
both parts of the PRR algorithm.
Figure 4 shows an example of the banking property
where after an idle period, the application has new
data to transmit half way through the recovery pro-
cess. Note that the algorithm in the PRR mode allows
4Although this burst might be viewed as being hard on the
network, this is exactly what happens every time there is
a partial RTT application stall while not in recovery. We
have made the response to partial RTT stalls uniform in all
states.
161a burst of up to ratio × (prr delivered − prr out)
which in this example is three segments (ratio is 0.5
for New Reno). Thereafter PRR continues spreading
the new segments among incoming ACKs.
RFC 3517 also banks missed sending opportunities
through the diﬀerence ssthresh − pipe. However,
these banked opportunities are subject to the inaccu-
racies of the pipe variable. As discussed below, pipe is
only an estimate of the outstanding data that can be
inaccurate in certain situations, e.g., when reordered
segments are incorrectly marked as lost. Linux does
not achieve this property since its congestion window
tracks pipe closely, thereby losing any history of missed
sending opportunities.
4. Robustness of DeliveredData: An integral part
of PRR is its reliance on the newly delivered data
per acknowledgment or DeliveredData, as opposed
to RFC 3517 and Linux recovery which make their
congestion window adjustments based on the number
of ACKs received and the pipe size. With SACK,
DeliveredData allows a TCP sender to learn of the
precise number of segments that arrived at the re-
ceiver. The properties below follow from the robust-
ness of DeliveredData.
Decouples data transmission from loss estima-
tion/marking: PRR is less sensitive to errors in the
pipe estimator compared to RFC 3517 as well as Linux
fast recovery.
In recovery, pipe is an estimator, us-
ing incomplete information to continually guess if seg-
ments that are not SACKed yet are actually lost or
out-of-order in the network. pipe can have signiﬁcant
errors for some conditions, e.g., when a burst of re-
ordered data is presumed to be lost and is retransmit-
ted, but then the original data arrives before the re-
transmission. Both RFC 3517 and Linux recovery use
pipe directly to regulate transmission rate in recovery.
Errors and discontinuities in the pipe estimator can
cause signiﬁcant errors in the amount of data sent.
On the other hand, PRR regulates the transmission
rate based on the actual amount of data delivered at
the receiver, DeliveredData. It only uses pipe to de-
termine which of the two algorithm modes, propor-
tional reduction or slow start, should compute the
number of segments to send per ACK. Since both parts
of the algorithm progressively converge to the same
target congestion window, transient errors in the pipe
estimator have much less impact on the ﬁnal outcome.
Precision in the number of transmitted seg-
ments: PRR retains its precision in the number of
transmitted segments even in the presence of ACK
loss, ACK reordering, and stretch ACKs such as those
caused by Large Receive Oﬄoad (LRO) and Generic
Receive Oﬄoad (GRO). The rate halving algorithm
in Linux is not robust under these same scenarios
as it relies heavily on the number of ACKs received,
e.g., Linux transmits one segment on receipt of every
two ACKs during rate halving, and fails to transmit
the right number of segments when receiving stretch
ACKs. Similarly when pipe estimation is incorrect
in any of the above scenarios, RFC 3517 also doesn’t
achieve transmission of a precise number of segments.
Data transmitted during recovery is in propor-
tion to that delivered: For PRR the following ex-
pression holds true for the amount of data sent during
recovery:
prr out ≤ 2 × prr delivered
The relation holds true for both parts of the PRR al-
gorithm. Transmitted data in RFC 3517 and Linux
do not have such a relation to data delivered due to
their reliance on pipe estimate.
In fact under heavy
losses, the transmission rate in RFC 3517 is directly
proportional to the extent of losses because of the cor-
respondingly small value of pipe value.
5. EXPERIMENT RESULTS
In this section, we evaluate the eﬀectiveness and perfor-
mance of PRR in comparison to RFC 3517 and the widely
deployed Linux fast recovery. We ran several 3-way ex-
periments with PRR, RFC 3517, and Linux fast recovery
on Google Web servers for ﬁve days in May 2011 and on
YouTube servers in India for four days during September
2011. For fair comparisons, all three recovery algorithms use
FACK loss marking. Furthermore they all use CUBIC con-
gestion control. These newer algorithms have the eﬀect of
making RFC 3517 slightly more aggressive than envisioned
by its original authors.
There are three metrics of interest when evaluating the
fast recovery schemes:
length of time spent in loss recov-
ery, number of timeouts experienced, and TCP latency for
HTTP responses. The recovery time is the interval from
when a connection enters recovery to when it re-enters Open
state. A connection may have multiple recovery events, in
which case the recovery time for each of the events is logged.
We ﬁrst describe our experiments on Google Web servers
and then go on to describe the YouTube experiments in In-
dia.
5.1 Experiment setup on Google Web servers
All of our Web server PRR experiments were performed
in a production data center (DC1 described in Section 2.1)
which serves traﬃc for a diverse set of Google applications.
The Web servers run Linux 2.6 with the default settings
shown in Table 4 except that ECN is disabled. The servers
terminate user TCP connections and are load balanced by
steering new connections to randomly selected Web servers
based on the server and client IP addresses and ports.
Calibration measurements over 24-hour time periods show
that the SNMP and HTTP latency statistics agree within
0.5% between individual servers. This property permits us
to run N-way experiments concurrently by changing TCP
conﬁgurations on groups of servers. If we run a 4-way ex-
periment, then 5% of current connections are served by an
experimental Web server while the remaining 80% connec-
tions are served by unmodiﬁed production Web servers.
The transactions are sampled such that the aggregate rate
for the experiment is roughly one million samples per day.
Note that multiple simultaneous connections opened by a
single client are likely to be served by diﬀerent Web servers
in diﬀerent experiment bins. Since we are not looking at
interactions between TCP connections this does not com-
promise the results presented here.
162pipe  ssthresh [PRR]
pipe − ssthresh
Min
1%
50%
99%
Max
32%
13%
45%
-338
-10
+1
+11
+144
Table 5: Statistics of pipe − ssthresh at the start of
recovery.
Quantiles for cwnd − ssthresh (segments).
Quantile:
PRR:
5
-8
10
-3
25
0
50
0
75
0
90
0
95
0
99
0
]
s
m
[
y
r
e
v
o
c
e
r
n
i
t
n
e
p
s
e
m
T
i
 16000
 14000
 12000
 10000
 8000
 6000
 4000
 2000
 0
1
4
2
8
7
1
3
4
3
4
1
3
2
6
8
PRR
RFC 3517
Linux
2
3
9
2
4
1
2
5
1
7
0
8
6
3
7
6
4
2
3
2
0
7
3
2
4
0
3
6
7
5
1
9
2
0
1
6
7
2
1
6
9
7
1
0
4
9
9
9
9