zero knowledge with non-malleable guarantees with existing
techniques [33, 32]. We present these transformations in the
full version of the paper.
A. Plaintext-ciphertext matrix multiplication proof
Gadget 1. A zero-knowledge proof for the statement:
“Given public parameters: public key P K, encryptions
EX, EY and EZ; private parameters: X,
• DecSK(EZ) = DecSK(EX) · DecSK(EY), and
• I know X such that DecSK(EX) = X.”
Gadget usage. We ﬁrst explain how Gadget 1 is used in Helen.
A party Pi in Helen knows a plaintext X and commits to X
by publishing its encryption, denoted by EncPK(X). Pi also
receives an encrypted matrix EncPK(Y) and needs to compute
EncPK(Z) = EncPK(XY) by leveraging the homomorphic
properties of the encryption scheme. Since parties in Helen
may be malicious, other parties cannot trust Pi to compute and
output EncPK(Z) correctly. Gadget 1 will help Pi prove in
zero-knowledge that it executed the computation correctly. The
proof needs to be zero-knowledge so that nothing is leaked
about the value of X. It also needs to be a proof of knowledge
so that Pi proves that it knows the plaintext matrix X.
Protocol.
Using the Paillier ciphertext multiplication
proofs [23], we can construct a na¨ıve algorithm for proving
matrix multiplication. For input matrices that are Rl×l, the
na¨ıve algorithm will incur a cost of l3 since one has to prove
each individual product. One way to reduce this cost is to have
the prover prove that tZ = (tX)Y for a randomly chosen
t such that ti = ti mod q. For such a randomly chosen t,
the chance that the prover can construct a tZ
= tXY is
exponentially small (an analysis is presented in our full paper).
As the ﬁrst step, both the prover and the veriﬁer ap-
ply the reduction to get the new statement EncPK(tZ) =
EncPK(tX)EncPK(Y). To prove this reduced form, we apply
the Paillier ciphertext multiplication proof in a straightforward
way. This proof takes as input three ciphertexts: Ea, Eb, Ec.
The prover proves that it knows the plaintext a∗
such that
= DecSK(Ea), and that DecSK(Ec) = DecSK(Ea) ·
a∗
DecSK(Eb). We apply this proof to every multiplication for
each dot product in (tX) · Y. The prover then releases the
individual encrypted products along with the corresponding
ciphertext multiplication proofs. The veriﬁer needs to verify that
EncPK(tZ) = EncPK(tXY). Since the encrypted ciphers from
the previous step are encrypted using Paillier, the veriﬁer can
(cid:2)
homomorphically add them appropriately to get the encrypted
vector EncPK(tXY).
Finally, the prover needs to prove that each element of tZ is
equal to each element of tXY. We can use the same ciphertext
multiplication proof by setting a∗
B. Plaintext-plaintext matrix multiplication proof
= 1.
Gadget 2. A zero-knowledge proof for the statement:
“Given public parameters: public key P K, encryptions
EX, EY, EZ; private parameters: X and Y,
• DecSK(EZ) = DecSK(EX) · DecSK(EY), and
• I know X, Y, and Z such that DecSK(EX) = X,
DecSK(EY) = Y, and DecSK(EZ) = Z.”
Gadget usage. This proof is used to prove matrix multiplica-
tion when the prover knows both input matrices (and thus the
output matrix as well). The protocol is similar to the plaintext-
ciphertext proofs, except that we have to do an additional proof
of knowledge of Y.
Protocol. The prover wishes to prove to a veriﬁer that Z =
XY without revealing X, Y, or Z. We follow the same protocol
as Gadget 1. Additionally, we utilize a variant of the ciphertext
multiplication proof that only contains the proof of knowledge
component to show that the prover also knows Y. The proof
of knowledge for the matrix is simply a list of element-wise
proofs for Y. We do not explicitly prove the knowledge of
Z because the matrix multiplication proof and the proof of
knowledge for Y imply that the prover knows Z as well.
V. INPUT PREPARATION PHASE
A. Overview
In this phase, each party prepares data for coopetitive
training. In the beginning of the ADMM procedure, every
party precomputes some summaries of its data and commits
to them by broadcasting encrypted summaries to all the other
parties. These summaries are then reused throughout the model
compute phase. Some form of commitment is necessary in
the malicious setting because an adversary can deviate from
the protocol by altering its inputs. Therefore, we need a new
gadget that allows us to efﬁciently commit to these summaries.
More speciﬁcally, the ADMM computation reuses two ma-
trices during training: Ai = (XT
i yi
from party i (see Section II-C for more details). These two
matrices are of sizes d × d and d × 1, respectively. In a
semihonest setting, we would trust parties to compute Ai
and bi correctly. In a malicious setting, however, the parties
can deviate from the protocol and choose Ai and bi that are
inconsistent with each other (e.g., they do not conform to the
above formulations).
−1 and bi = XT
i Xi + ρI)
Helen does not have any control over what data each party
contributes because the parties must be free to choose their own
Xi and yi. However, Helen ensures that each party consistently
uses the same Xi and yi during the entire protocol. Otherwise,
malicious parties could try to use different/inconsistent Xi and
yi at different stages of the protocol, and thus manipulate the
(cid:24)(cid:19)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:42 UTC from IEEE Xplore.  Restrictions apply. 
ﬁnal outcome of the computation to contain the data of another
party.
One possibility to address this problem is for each party
i to commit to its Xi in EncPK(Xi) and yi in EncPK(yi).
To calculate Ai, the party can calculate and prove XT
i X
using Gadget 2, followed by computing a matrix inversion
computation within SPDZ. The result Ai can be repeatedly
used in the iterations. This is clearly inefﬁcient because (1) the
protocol scales linearly in n, which could be very large, and
(2) the matrix inversion computation requires heavy compute.
Our idea is to prove using an alternate formulation via
singular value decomposition (SVD) [38], which can be much
more succinct: Ai and bi can be decomposed using SVD to
matrices that scale linearly in d. Proving the properties of Ai
and bi using the decomposed matrices is equivalent to proving
using Xi and yi.
B. Protocol
1) Decomposition of reused matrices: We ﬁrst derive an
alternate formulation for Xi (denoted as X for the rest of
this section). From fundamental linear algebra concepts we
know that every matrix has a corresponding singular value
decomposition [38]. More speciﬁcally, there exists unitary
matrices U and V, and a diagonal matrix Γ such that X =
UΓVT , where U ∈ Rn×n, Γ ∈ Rn×d, and V ∈ Rd×d.
Since X and thus U are real matrices, the decomposition also
guarantees that U and V are orthogonal, meaning that UT U =
I and VT V = I. If X is not a square matrix, then the top
part of Γ is a diagonal matrix, which we will call Σ ∈ Rd×d.
Σ’s diagonal is a list of singular values σi. The rest of the Γ
matrix are 0’s. If X is a square matrix, then Γ is simply Σ.
Finally, the matrices U and V are orthogonal matrices. Given
an orthogonal matrix Q, we have that QQT = QT Q = I.
It turns out that XT X has some interesting properties:
XT X = (UΓVT )T UΓVT
= VΓT UT UΓVT
= VΓT ΓVT
= VΣ2VT .
We now show that (XT X + ρI)
diagonal matrix with diagonal values
1
i + ρ.
σ2
−1 = VΘVT , where Θ is the
(XT X + ρI)VΘVT = V(Σ2 + ρI)VT VΘVT
= V(Σ2 + ρI)ΘVT
= VVT = I.
Using a similar reasoning, we can also derive that
XT y = VΓT UT y.
2) Properties after decomposition: The SVD decomposition
formulation sets up an alternative way to commit to matrices
−1 and Xiyi. For the rest of this section, we
(XT
describe the zero knowledge proofs that every party has to
i Xi + ρI)
= (UT y)[1:d] ∈ Rd×1 such that:
execute. For simplicity, we focus on one party and use X and
y to represent its data, and A and b to represent its summaries.
During the ADMM computation, matrices A = (XT X +
−1 and b = XT y are repeatedly used to calculate the
ρI)
intermediate weights. Therefore, each party needs to commit
to A and b. With the alternative formulation, it is no longer
necessary to commit to X and y individually. Instead, it sufﬁces
to prove that a party knows V, Θ, Σ (all are in Rd×d) and a
vector y∗
1) A = VΘVT ,
2) b = VΣT y∗
,
3) V is an orthogonal matrix, namely, VT V = I, and
4) Θ is a diagonal matrix where the diagonal entries are
i + ρ). σi are the values on the diagonal of Σ and ρ
1/(σ2
is a public value.
Note that Γ can be readily derived from Σ by adding rows
of zeros. Moreover, both Θ and Σ are diagonal matrices.
Therefore, we only commit to the diagonal entries of Θ and
Σ since the rest of the entries are zeros.
The above four statements are sufﬁcient
to prove the
properties of A and b in the new formulation. The ﬁrst two
statements simply prove that A and b are indeed decomposed
into some matrices V, Θ, Σ, and y∗
. Statement 3) shows that
V is an orthogonal matrix, since by deﬁnition an orthogonal
matrix Q has to satisfy the equation QT Q = I. However, we
allow the prover to choose V. As stated before, the prover
would have been free to choose X and y anyway, so this
freedom does not give more power to the prover.
Statement 4) proves that the matrix Θ is a diagonal matrix
such that the diagonal values satisfy the form above. This
is sufﬁcient to show that Θ is correct according to some Σ.
Again, the prover is free to choose Σ, which is the same as
freely choosing its input X.
Finally, we chose to commit to y∗
instead of committing to
U and y separately. Following our logic above, it seems that
we also need to commit to U and prove that it is an orthogonal
matrix, similar to what we did with V. This is not necessary
because of an important property of orthogonal matrices: U’s
columns span the vector space Rn. Multiplying Uy, the result
is a linear combination of the columns of U. Since we also
allow the prover to pick its y, Uy essentially can be any vector
in Rn. Thus, we only have to allow the prover to commit to
the product of U and y. As we can see from the derivation,
b = VΓT Uy, but since Γ is simply Σ with rows of zeros,
the actual decomposition only needs the ﬁrst d elements of
, which is d × 1.
Uy. Hence, this allows us to commit to y∗
Using our techniques, Helen commits only to matrices of
sizes d × d or d × 1, thus removing any scaling in n (the
number of rows in the dataset) in the input preparation phase.
3) Proving the initial data summaries: First, each party
broadcasts EncPK(V), EncPK(Σ), EncPK(Θ), EncPK(y∗
),
EncPK(A), and EncPK(b). To encrypt a matrix, the party
simply individually encrypts each entry. The encryption scheme
itself also acts as a commitment scheme [39], so we do not
need an extra commitment scheme.
(cid:24)(cid:20)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:42 UTC from IEEE Xplore.  Restrictions apply. 
To prove these statements, we also need another primitive
called an interval proof. Moreover, since these matrices act
as inputs to the model compute phase, we also need to prove
that A and b are within a certain range (this will be used
by Gadget 4). The interval proof we use is from [14], which
is an efﬁcient way of proving that a committed number lies
within a certain interval. However, what we want to prove is
that an encrypted number lies within a certain interval. This
can be solved by using techniques from [26], which appends
the range proof with a commitment-ciphertext equality proof.
This extra proof proves that, given a commitment and a Paillier
ciphertext, both hide the same plaintext value.
To prove the ﬁrst two statements, we invoke Gadget 1 and
Gadget 2. This allows us to prove that the party knows all of
the matrices in question and that they satisfy the relations laid
out in those statements.
There are two steps to proving statement 3. The prover will
compute EncPK(VT V) and prove it computed it correctly
using Gadget 1 as above. The result should be equal to the
encryption of the identity matrix. However, since we are using
ﬁxed point representation for our data, the resulting matrix
could be off from the expected values by some small error.
VT V will only be close to I, but not equal to I. Therefore,
we also utilize interval proofs to make sure that VT V is close
to I, without explicitly revealing the value of VT V.
Finally, to prove statement 4, the prover does the following:
1) The prover computes and releases EncPK(Σ2) because
the prover knows Σ and proves using Gadget 1 that this
computation is done correctly.
2) The prover computes EncPK(Σ2 + ρI), which anyone can
compute because ρ and I are public. EncPK(Σ2) and
EncPK(ρI) can be multiplied together to get the summation
of the plaintext matrices.
3) The prover now computes EncPK(Σ2 + ρI) × EncPK(Θ)
and proves this encryption was computed correctly using
Gadget 1.
4) Similar to step 3), the prover ends this step by using interval
proofs to prove that this encryption is close to encryption
of the identity matrix.
VI. MODEL COMPUTE PHASE
A. Overview
In the model compute phase, all parties use the summaries
computed in the input preparation phase and execute the
iterative ADMM training protocol. An encrypted weight
vector is generated at the end of this phase and distributed
to all participants. The participants can jointly decrypt this
weight vector to get the plaintext model parameters. This
phase executes in three steps: initialization, training (local
optimization and coordination), and model release.
B. Initialization
We initialize the weights w0
i . There are two
popular ways of initializing the weights. The ﬁrst way is to
set every entry to a random number. The second way is to
i , z0, and u0
initialize every entry to zero. In Helen, we use the second
method because it is easy and works well in practice.
C. Local optimization
(cid:5)
(cid:4)
−1 and bi is XT
i ← Ai(bi +ρ
During ADMM’s local optimization phase, each party takes
the current weight vector and iteratively optimizes the weights
based on its own dataset. For LASSO, the update equation is
zk − uk
simply wk+1
), where Ai is the matrix
(XT
i Xi + ρI)
i yi. As we saw from the input
preparation phase description, each party holds encryptions of
Ai and bi. Furthermore, given zk and uk
i (either initialized or
received as results calculated from the previous round), each
party can independently calculate wk+1
by doing plaintext
scaling and plaintext-ciphertext matrix multiplication. Since
this is done locally, each party also needs to generate a proof
proving that the party calculated wk+1
correctly. We compute
the proof for this step by invoking Gadget 1.
i
i
i
D. Coordination using MPC
After the local optimization step, each party holds encrypted
weights wk+1
. The next step in the ADMM iterative opti-
mization is the coordination phase. Since this step contains
non-linear functions, we evaluate it using generic MPC.
i
1) Conversion to MPC: First, the encrypted weights need