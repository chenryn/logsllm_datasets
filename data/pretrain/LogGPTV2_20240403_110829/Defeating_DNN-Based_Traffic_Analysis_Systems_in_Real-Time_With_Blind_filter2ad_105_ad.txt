6.2 Target Systems
We demonstrate our attack on three state-of-the-art DNN-
based trafﬁc analysis systems.
DeepCorr: DeepCorr [37] is the state-of-the-art ﬂow correla-
tion system, which uses deep learning to learn ﬂow correlation
functions for speciﬁc network settings like that of Tor. Deep-
Corr uses inter-packet delays (IPDs) and sizes of the packets
as the features. DeepCorr uses Convolutional neural networks
to extract complex features from the raw timing and size in-
formation, and it outperforms the conventional statistical ﬂow
correlation techniques by signiﬁcant margins. Since Deep-
Corr uses both timings and sizes of packets as the features,
we apply the time-based and size-based attacks on DeepCorr.
As mentioned earlier, non-blind adversarial perturba-
tions [26, 62] are useless in the ﬂow correlation setting, as
the adversary does not know the features of the upcoming
packets in a target connection. Hence, our blind perturbations
are applicable in this setting.
Var-CNN: Var-CNN [3] is a deep learning-based website ﬁn-
gerprinting (WF) system that uses both manual and automated
feature extraction techniques to be able to work with even
small amounts of training data. Var-CNN uses ResNets [21]
with dilated casual convolutions, the state-of-the-art convo-
lutional neural network, as its base structure. Furthermore,
Var-CNN shows that in contrast to previous WF attacks, com-
bining packet timing information (IPDs) and direction infor-
mation can improve the performance of the WF adversary.
In addition to packet IPDs and directions, Var-CNN uses cu-
mulative statistical information for features of network ﬂows.
Therefore, Var-CNN combines three different models, two
ResNet models for timing and direction information, and one
fully connected model for metadata statistical information as
the ﬁnal structure. Var-CNN considers both closed-world and
open-world scenarios.
Similar to the setting of ﬂow correlation, a WF adversary
will not be able to use traditional (non-blind) adversarial per-
turbations [26, 62], as she will not have knowledge on the
patterns of upcoming packets in a targeted connection. There-
fore, WF is a trivial application for blind perturbations. Since
Var-CNN uses both IPD and packet direction features for ﬁn-
gerprinting, we use both timing-based and direction-based
techniques to generate our adversarial perturbations.
Deep Fingerprinting (DF): Deep Fingerprinting (DF) [50]
is a deep learning based WF attack which uses CNNs to
perform WF attacks on Tor. DF deploys automated feature
extraction, and uses the direction information for training.
In contrast to Var-CNN, DF does not require handcrafted
features of packet sequences. Similar to Var-CNN, DF con-
siders both closed-world and open-world scenarios. Sirinam
et al. [50] show that DF outperforms prior WF systems in
defeating WF defenses of WTF-PAD [28] and W-T [61].
Codes. As we perform our attack in PyTorch, we use the
original code of DeepCorr, DF, and Var-CNN models and
convert them from TensorFlow to PyTorch. We then train
these models using the datasets of those papers.
6.3 Adversary Setup and Models
While our technique can be applied to any trafﬁc analysis
setting, we present our setup for the popular Tor application.
Adversary’s Interception Points Our adversary has the
same placement as traditional Tor trafﬁc analysis works [47,
50, 59–61]. For the WF scenario, we assume the adversary
is manipulating the trafﬁc between a Tor client and the ﬁrst
Tor hop, i.e., a Tor bridge [13] or a Tor guard relay. There-
fore, our blind adversarial perturbation can be implemented
as a Tor pluggable transport [46], in which case the blind
perturbations are applied by both the Tor client software and
the Tor bridge. In the ﬂow correlation setting, similar to the
literature, trafﬁc manipulations are performed by Tor entry
and exit relays (since ﬂow correlation attackers intercept both
egress and ingress Tor connections). In our evaluations, we
show that even applying our blind adversarial perturbations
on only ingress ﬂows is enough to defeat ﬂow correlation
attacks, i.e., the same adversary placement as the WF setting.
Adversarial Perturbation Models As mentioned in Sec-
tion 4, we design a deep learning model to generate blind
adversarial noises. For each type of perturbation, the adver-
sarial model is a fully connected model with one hidden layer
of size 500 and a ReLu activation function. The parameters of
the adversarial model are presented in Table 1. The input and
output sizes of the adversarial model are equal to the length
of features in the target ﬂow. In the forward function, the
adversarial model takes in a given input, manipulates it based
USENIX Association
30th USENIX Security Symposium    2713
Table 1: Tuned parameters of the adversarial models and
discriminator model
Model
Direction-based
Time-based
Size-based (ordering)
Size-based (amplitude)
Discriminator
# H-layers
1
1
1
1
2
Size
[500]
[500]
[500]
[500]
[1000,1]
Optimizer
Adam
Adam
Adam
Adam
Adam
LR
10−3
10−3
10−3
10−3
10−4
Activation
ReLu
ReLu
ReLu
ReLu
ReLu
on the attack method, and output a crafted version of the input.
In each iteration of training, we update the parameters of the
adversarial model based on the loss functions introduced in
Section 4. We use Adam optimizer to learn the blind noise
with a learning rate of 0.001.
Discriminator Model As mentioned in Section 4, we use a
GAN model to enforce the time constraints of our modiﬁed
network ﬂows. To do so, we design a fully-connected discrim-
inator model containing two hidden FC layers of size 1000.
The parameters of the discriminator model are presented in
Table 1. The input and output sizes of this model are equal to
the sizes of the blind adversarial noise. In the training process,
we use Adam optimizer with a learning rate of 0.0001 to learn
the discriminator model.
6.4 Datasets
We use the following datasets to create network ﬂows for our
experiments; these are the largest publicly available datasets
for our target applications.
Tor Flow Correlation Dataset For ﬂow correlation experi-
ments, we use the publicly available dataset of DeepCorr [37],
which contains 7000 ﬂows for training and 500 ﬂows for
testing. These ﬂows are captured Tor ﬂows of top Alexa’s
websites and contain timings and sizes of each of them. These
ﬂows are then used to create a large set of ﬂow pairs includ-
ing associated ﬂow pairs (ﬂows belonging to the same Tor
connection) and non-associated ﬂow pairs (ﬂows belonging
to arbitrary Tor connections). Each associated ﬂow pair is
labeled with 1, and each non-associated ﬂow pair with 0.
Tor Website Fingerprinting Datasets Var-CNN uses a
dataset of 900 monitored sites each with 2,500 traces. These
sites were compiled from the Alexa list of most popular web-
sites. Var-CNN is fed in with different sets of features repre-
senting a given trace; the direction-based ResNet model takes
a set of 1’s and -1’s as the direction of each packet such that
1 shows an outgoing packet and -1 represents an incoming
packet. The time-based ResNet uses the IPDs of the traces as
features. The metadata model takes in seven ﬂoat numbers as
the statistical information of the traces. To be consistent with
previous WF attacks [47, 50, 59, 60], we use the ﬁrst 5000
values of a given trace for both direction and time features.
DF uses a different dataset than Var-CNN. For the closed-
world setting, they collected the traces of 95 top Alexa web-
sites with 1000 visits for each. DF uses the same represen-
tation as Var-CNN for direction information of the packets.
Since CNNs only take in a ﬁxed length input, DF considers
the ﬁrst 5000 values of each ﬂow.
6.5 The BLANKET Tor Pluggable Transport
To demonstrate the deployability of our techniques, we apply
our adversarial perturbations on live Tor Trafﬁc. Speciﬁcally,
we have implemented our adversarial perturbation techniques
as a Tor pluggable transport [46], which we call BLANKET.3
We use BLANKET to perturb Tor connections generated us-
ing the datasets introduced above for different target systems.
To enforce its timing indistinguishability constraint, BLAN-
KET needs to measure the jitter of its client. The goal of
BLANKET is to defeat DNN-based trafﬁc analysis attacks
(particularly, website ﬁngerprinting and ﬂow correlation) on
Tor connections by applying adversarial perturbations on
live Tor connections. We have implemented our pluggable
transport in Python using the Twisted framework, which is
available at https://github.com/SPIN-UMass/BLANKET.
BLANKET has two phases of operation.
Session initialization: Like other pluggable transports,
BLANKET needs to be installed both by a Tor client and
the Tor bridge she is connected to it. At the beginning of each
session, the client and the bridge will negotiate a set of adver-
sarial noise vectors (created using the generator function G
by the client) that they will use for trafﬁc perturbation (the
noise vector includes the timing and the sizes of the packets
needed for perturbation), as well as a pair of AES keys to
encrypt trafﬁc (similar to other pluggable transports). This
negotiation can be integrated into Tor’s regular client-bridge
handshaking, or alternatively exchanged through out-of-band
channels (e.g., email, a domain-fronted server, etc.). The cur-
rent implementation of BLANKET negotiates out of band.
Trafﬁc perturbation: Figure 3 shows how BLANKET mod-
iﬁes live Tor connections to apply our our adversarial perturba-
tions introduced in Section 4. Speciﬁcally, BLANKET applies
two types of perturbations: it perturbs the timings/sizes of
existing packets (on-the-ﬂy) or injects new (dummy) packets
into the ﬂow. To inject dummy packets, BLANKET simply
adds the new packets with their speciﬁc timing/sizes in the
transport layer; this keeps the underlying protocols (TCP/IP)
unmodiﬁed and semantically correct. On the receiver side of
our pluggable transport, the transport layer will remove the
injected dummy packets before passing them to the upstream
application (e.g., the next Tor relay); as a result the upstream
packets will also remain unmodiﬁed and semantically cor-
rect. To perturb an existing packet on-the-ﬂy, BLANKET
changes the timing and sizes of the packets as follows: to
change the size of a packet, the sender’s BLANKET will pad
that packet with random bytes, which are removed by the re-
3BLANKET stands for BLind Adversarial NetworK pErturbaTions.
2714    30th USENIX Security Symposium
USENIX Association
Figure 3: Overview of our BLANKET Tor pluggable transport, which applies blind adversarial perturbations on live Tor
connections (the ﬁgure only shows the client-to-bridge operations; bridge-to-client operations work similarly).
ceiver’s BLANKET (note that both the sender and the receiver
know the exact index of the padded and dummy packets as the
perturbation vectors are shared between them during the ini-
tialization process). Similar to padding, manipulating packet
sizes does not impact the correctness of the underlying and
higher protocols as this is performed at the transport layer.
Note that, similar to state-of-the-art pluggable transports
like obfs [39], all packet contents are encrypted using the AES
keys negotiated during initialization; therefore, as long as the
encryption protocol is secure, it is not possible to distinguish
BLANKET’s dummy or padded packets from benign Tor.
Finally, packet timings are modiﬁed by delaying the pack-
ets by the sender’s BLANKET. Our timing perturbations do
not affect the correctness of the underlying/upstream proto-
cols, since the perturbations are in the order of milliseconds,
signiﬁcantly smaller than the timeout values in both TCP/IP
and HTTP/S (or Tor) protocols (in the order of seconds).
7 Experiment Results
We use BLANKET to evaluate our blind adversarial perturba-
tions against the target systems of Section 6.2 using each of
the three key trafﬁc features and their combinations. We also
compare our attack with traditional attacks.
Computation costs: Our perturbation model, G, is trained
ofﬂine and before being used to perturb live connections;
therefore, training the perturbation model does not introduce
any runtime overheads. Also, note that G only needs to be
generated once for each installation; it takes 5 hours to train
G on our NVIDIA TITAN X GPU.
7.1 Adversarially Perturbing Directions
As explained in Section 4, an adversary cannot change the
directions of existing packets, but he can insert packets with
adversarial directions. We evaluated our attack for different
adversary settings and strengths against Var-CNN [3] and
DF [50] (which use direction features). We used 10 epochs
and Adam optimizer to train the blind adversarial perturba-
tions model with a learning rate of 0.001. Tables 2 and 3
show the success of our attack (using A in (13)) on DF and
Var-CNN, respectively, when they only use packet directions
as their features. As can be seen, both DF and Var-CNN
are highly vulnerable to adversarial perturbation attacks
when the adversary only injects a small number of packets.
Speciﬁcally, we were able to generate targeted perturbations
that misclassify every input into a target class with only 25%
bandwidth overhead.
7.2 Adversarially Perturbing Timings
We consider two scenarios for generating adversarial timing
perturbations: with and without an invisibility constraint. In
both scenarios, we limited the adversaries’ power such that
the added noise to the timings of the packets has a maximum
mean and standard deviation as explained in Section 4. For
the invisibility constraint, we force the added noise to have
the same distribution as natural network jitter, which follows
a Laplace distribution [37]. The detailed parameters of our
model are presented in Table 1.
Figure 4 shows the performance of our attack against Deep-
Corr when the adversary only manipulates the timings of
packets. As expected, Figures 4a and 4b show that increasing
the strength (mean or standard deviation) of our blind noise
results in better performance of the attack, but even a per-
turbation with average 0 and a tiny standard deviation of
50ms signiﬁcantly reduces the true positive of DeepCorr
from 95% to 55%.
Also, we can create effective adversarial perturbations
with high invisibility: Figure 5 shows the histogram of the
generated timing perturbations, with parameters µ = 0,σ =
30ms, learned under an invisibility constraint forcing it to fol-
low a Laplace distribution. For this invisible noise, Figure 4c