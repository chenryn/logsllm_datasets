taken into account (for instance, precise information about the envi-
ronment or the location of nearby users), making their application
in real-time by a handheld device challenging. Moreover, the at-
tacker’s actual side information might simply be inconsistent with
the assumptions being made.
As a result, notions that abstract from the attacker’s side informa-
tion, such as differential privacy, have been growing in popularity
in recent years, compared to k-anonymity-based approaches.
Differential Privacy.
Differential Privacy [14] is a notion of privacy from the area of
statistical databases. Its goal is to protect an individual’s data while
publishing aggregate information about the database. Differential
privacy requires that modifying a single user’s data should have a
negligible effect on the query outcome. More precisely, it requires
that the probability that a query returns a value v when applied to
a database D, compared to the probability to report the same value
when applied to an adjacent database D(cid:48) – meaning that D, D(cid:48) dif-
fer in the value of a single individual – should be within a bound
of e. A typical way to achieve this notion is to add controlled ran-
dom noise to the query output, for example drawn from a Laplace
distribution. An advantage of this notion is that a mechanism can
be shown to be differentially private independently from any side
information that the attacker might possess.
Differential privacy has also been used in the context of loca-
tion privacy. In [28], it is shown that a synthetic data generation
technique can be used to publish statistical information about com-
muting patterns in a differentially private way. In [22], a quadtree
spatial decomposition technique is used to ensure differential pri-
vacy in a database with location pattern mining capabilities.
As shown in the aforementioned works, differential privacy can
be successfully applied in cases where aggregate information about
several users is published. On the other hand, the nature of this
notion makes it poorly suitable for applications in which only a
single individual is involved, such as our motivating scenario. The
secret in this case is the location of a single user. Thus, differential
privacy would require that any change in that location should have
negligible effect on the published output, making it impossible to
communicate any useful information to the service provider.
To overcome this issue, Dewri [11] proposes a mix of differential
privacy and k-anonymity, by ﬁxing an anonymity set of k locations
and requiring that the probability to report the same obfuscated lo-
cation z from any of these k locations should be similar (up to e).
This property is achieved by adding Laplace noise to each Carte-
sian coordinate independently. There are however two problems
with this deﬁnition: ﬁrst, the choice of the anonymity set crucially
affects the resulting privacy; outside this set no privacy is guaran-
teed at all. Second, the property itself is rather weak; reporting the
geometric median (or any deterministic function) of the k locations
would satisfy the same deﬁnition, although the privacy guarantee
would be substantially lower than using Laplace noise.
Nevertheless, Dewri’s intuition of using Laplace noise1 for loca-
tion privacy is valid, and [11] provides extensive experimental anal-
ysis supporting this claim. Our notion of geo-indistinguishability
provides the formal background for justifying the use of Laplace
noise, while avoiding the need to ﬁx an anonymity set by using the
generalized variant of differential privacy from [8].
Other location-privacy metrics.
[10] proposes a location cloaking mechanism, and focuses on the
evaluation of Location-based Range Queries. The degree of privacy
is measured by the size of the cloak (also called uncertainty region),
and by the coverage of sensitive regions, which is the ratio between
the area of the cloak and the area of the regions inside the cloak
that the user considers to be sensitive. In order to deal with the
side-information that the attacker may have, ad-hoc solutions are
proposed, like patching cloaks to enlarge the uncertainty region or
1The planar Laplace distribution that we use in our work, how-
ever, is different from the distribution obtained by adding Laplace
noise to each Cartesian coordinate, and has better differential pri-
vacy properties (c.f. Section 4.1).
903delaying requests. Both solutions may cause a degradation in the
quality of service.
In [5], the real location of the user is assumed to have some level
of inaccuracy, due to the speciﬁc sensing technology or to the en-
vironmental conditions. Different obfuscation techniques are then
used to increase this inaccuracy in order to achieve a certain level
of privacy. This level of privacy is deﬁned as the ratio between the
accuracy before and after the application of the obfuscation tech-
niques.
Similar to the case of k-anonymity, both privacy metrics men-
tioned above make implicit assumptions about the adversary’s side
information. This may imply a violation of the privacy deﬁnition
in a scenario where the adversary has some knowledge about the
user’s real location.
Transformation-based approaches.
A number of approaches for location privacy are radically differ-
ent from the ones mentioned so far. Instead of cloaking the user’s
location, they aim at making it completely invisible to the service
provider. This is achieved by transforming all data to a different
space, usually employing cryptographic techniques, so that they
can be mapped back to spatial information only by the user [24,
20]. The data stored in the provider, as well as the location send
by the user are encrypted. Then, using techniques from private in-
formation retrieval, the provider can return information about the
encrypted location, without ever discovering which actual location
it corresponds to.
A drawback of these techniques is that they are computation-
ally demanding, making it difﬁcult to implement them in a hand-
held device. Moreover, they require the provider’s data to be en-
crypted, making it impossible to use existing providers, such as
Google Maps, which have access to the real data.
3. GEO-INDISTINGUISHABILITY
In this section we formalize our notion of geo-indistinguisha-
bility. As already discussed in the introduction, the main idea be-
hind this notion is that, for any radius r > 0, the user enjoys r-
privacy within r, i.e. the level of privacy is proportional to the ra-
dius. Note that the parameter  corresponds to the level of privacy
at one unit of distance. For the user, a simple way to specify his
privacy requirements is by a tuple ((cid:96), r), where r is the radius he is
mostly concerned with and (cid:96) is the privacy level he wishes for that
radius. In this case, it is sufﬁcient to require -geo-indistinguisha-
bility for  = (cid:96)/r; this will ensure a level of privacy (cid:96) within r, and
a proportionally selected level for all other radii.
So far we kept the discussion on an informal level by avoiding to
explicitly deﬁne what (cid:96)-privacy within r means. In the remaining
of this section we give a formal deﬁnition, as well as two charac-
terizations which clarify the privacy guarantees provided by geo-
indistinguishability.
Probabilistic model.
We ﬁrst introduce a simple model used in the rest of the paper.
We start with a set X of points of interest, typically the user’s possi-
ble locations. Moreover, let Z be a set of possible reported values,
which in general can be arbitrary, allowing to report obfuscated
locations, cloaking regions, sets of locations, etc. However, to sim-
plify the discussion, we sometimes consider Z to also contain spa-
tial points, assuming an operational scenario of a user located at
x ∈ X and communicating to the attacker a randomly selected lo-
cation z ∈ Z (e.g. an obfuscated point).
Probabilities come into place in two ways. First, the attacker
might have side information about the user’s location, knowing,
for example, that he is likely to be visiting the Eiffel Tower, while
unlikely to be swimming in the Seine river. The attacker’s side
information can be modeled by a prior distribution π on X , where
π(x) is the probability assigned to the location x.
Second, the selection of a reported value in Z is itself probabilis-
tic; for instance, z can be obtained by adding random noise to the
actual location x (a technique used in Section 4). A mechanism K
is a probabilistic function for selecting a reported value; i.e. K is
a function assigning to each location x ∈ X a probability distribu-
tion on Z, where K(x)(Z) is the probability that the reported point
belongs to the set Z ⊆ Z, when the user’s location is x.2 Starting
from π and using Bayes’ rule, each observation Z ⊆ Z of a mech-
anism K induces a posterior distribution σ = Bayes(π, K, Z) on
X , deﬁned as σ(x) = K(x)(Z)π(x)
σ1, σ2 on some set S as dP (σ1, σ2) = supS⊆S | ln σ1(S)
the convention that | ln σ1(S)
and ∞ if only one of them is zero.
3.1 Deﬁnition
We deﬁne the multiplicative distance between two distributions
σ2(S)|, with
σ2(S)| = 0 if both σ1(S), σ2(S) are zero
(cid:80)
x(cid:48) K(x(cid:48))(Z)π(x(cid:48)) .
We are now ready to state our deﬁnition of geo-indistinguisha-
bility. Intuitively, a privacy requirement is a constraint on the dis-
tributions K(x), K(x(cid:48)) produced by two different points x, x(cid:48). Let
d(·,·) denote the Euclidean metric. Enjoying (cid:96)-privacy within r
means that for any x, x(cid:48) s.t. d(x, x(cid:48)) ≤ r, the distance dP (K(x),
K(x(cid:48))) between the corresponding distributions should be at most
l. Then, requiring r-privacy for all radii r, forces the two distribu-
tions to be similar for locations close to each other, while relaxing
the constraint for those far away from each other, allowing a service
provider to distinguish points in Paris from those in London.
DEFINITION 3.1
(GEO-INDISTINGUISHABILITY). A mecha-
nism K satisﬁes -geo-indistinguishability iff for all x, x(cid:48):
(cid:48)
dP (K(x), K(x
)) ≤ d(x, x
(cid:48)
)
Equivalently, the deﬁnition can be formulated as K(x)(Z) ≤
ed(x,x(cid:48))K(x(cid:48))(Z) for all x, x(cid:48) ∈ X , Z ⊆ Z. Note that for all
points x(cid:48) within a radius r from x, the deﬁnition forces the corre-
sponding distributions to be at most r distant.
The above deﬁnition is very similar to the one of differential pri-
vacy, which requires dP (K(x), K(x(cid:48))) ≤ dh(x, x(cid:48)), where dh
is the Hamming distance between databases x, x(cid:48), i.e. the number
of individuals in which they differ. In fact, geo-indistinguishability
is an instance of a generalized variant of differential privacy, using
an arbitrary metric between secrets. This generalized formulation
has been known for some time: for instance, [31] uses it to per-
form a compositional analysis of standard differential privacy for
functional programs, while [16] uses metrics between individuals
to deﬁne “fairness” in classiﬁcation. On the other hand, the use-
fulness of using different metrics to achieve different privacy goals
and the semantics of the privacy deﬁnition obtained by different
metrics have only recently started to be studied [8]. This paper fo-
cuses on location-based systems and is, to our knowledge, the ﬁrst
work considering privacy under the Euclidean metric, which is a
natural choice for spatial data.
Note that in our scenario, using the Hamming metric of stan-
dard differential privacy – which aims at completely protecting the
2For simplicity we assume distributions on X to be discrete, but
allow those on Z to be continuous (c.f. Section 4). All sets to which
probability is assigned are implicitly assumed to be measurable.
904value of an individual – would be too strong, since the only infor-
mation is the location of a single individual. Nevertheless, we are
not interested in completely hiding the user’s location, since some
approximate information needs to be revealed in order to obtain the
required service. Hence, using a privacy level that depends on the
Euclidean distance between locations is a natural choice.
A note on the unit of measurement.
It is worth noting that, since  corresponds to the privacy level
for one unit of distance, it is affected by the unit in which distances
are measured. For instance, assume that  = 0.1 and distances are
measured in meters. The level of privacy for points one kilometer
away is 1000, hence changing the unit to kilometers requires to
set  = 100 in order for the deﬁnition to remain unaffected. In
other words, if r is a physical quantity expressed in some unit of
measurement, then  has to be expressed in the inverse unit.
3.2 Characterizations
In this section we state two characterizations of geo-indistingui-
shability, obtained from the corresponding results of [8] (for gen-
eral metrics), which provide intuitive interpretations of the privacy
guarantees offered by geo-indistinguishability.
Adversary’s conclusions under hiding.
The ﬁrst characterization uses the concept of a hiding function
φ : X → X . The idea is that φ can be applied to the user’s actual
location before the mechanism K, so that the latter has only access
to a hidden version φ(x), instead of the real location x. A mecha-
nism K with hiding applied is simply the composition K ◦ φ. In-
tuitively, a location remains private if, regardless of his side knowl-
edge (captured by his prior distribution), an adversary draws the
same conclusions (captured by his posterior distribution), regard-
less of whether hiding has been applied or not. However, if φ
replaces locations in Paris with those in London, then clearly the
adversary’s conclusions will be greatly affected. Hence, we require
that the effect on the conclusions depends on the maximum distance
d(φ) = supx∈X d(x, φ(x)) between the real and hidden location.
THEOREM 3.1. A mechanism K satisﬁes -geo-indistinguisha-
bility iff for all φ : X → X , all priors π on X , and all Z ⊆ Z:
dP (σ1, σ2) ≤ 2d(φ) where
σ1 = Bayes(π, K, Z)
σ2 = Bayes(π, K ◦ φ, Z)
Note that this is a natural adaptation of a well-known interpreta-
tion of standard differential privacy, stating that the attacker’s con-
clusions are similar, regardless of his side knowledge, and regard-
less of whether an individual’s real value has been used in the query
or not. This corresponds to a hiding function φ removing the value
of an individual.
Note also that the above characterization compares two poste-
rior distributions. Both σ1, σ2 can be substantially different than
the initial knowledge π, which means that an adversary does learn
some information about the user’s location.
Knowledge of an informed attacker.
A different approach is to measure how much the adversary learns
about the user’s location, by comparing his prior and posterior dis-
tributions. However, since some information is allowed to be re-
vealed by design, these distributions can be far apart. Still, we can
consider an informed adversary who already knows that the user is
located within a set N ⊆ X . Let d(N ) = supx,x(cid:48)∈N d(x, x(cid:48))
Intuitively, the
be the maximum distance between points in x.
user’s location remains private if, regardless of his prior knowl-
edge within N, the knowledge obtained by such an informed ad-
versary should be limited by a factor depending on d(N ). This
means that if d(N ) is small, i.e. the adversary already knows the
location with some accuracy, then the information that he obtains is
also small, meaning that he cannot improve his accuracy. Denoting
by π|N the distribution obtained from π by restricting to N (i.e.
π|N (x) = π(x|N )), we obtain the following characterization:
THEOREM 3.2. A mechanism K satisﬁes -geo-indistinguisha-
bility iff for all N ⊆ X , all priors π on X , and all Z ⊆ Z:
dP (π|N , σ|N ) ≤ d(N ) where
σ = Bayes(π, K, Z)
Note that this is a natural adaptation of a well-known interpre-
tation of standard differential privacy, stating that in informed ad-