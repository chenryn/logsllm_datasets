title:VulDeePecker: A Deep Learning-Based System for Vulnerability Detection
author:Zhen Li and
Deqing Zou and
Shouhuai Xu and
Xinyu Ou and
Hai Jin and
Sujuan Wang and
Zhijun Deng and
Yuyi Zhong
8
1
0
2
n
a
J
5
]
R
C
.
s
c
[
1
v
1
8
6
1
0
.
1
0
8
1
:
v
i
X
r
a
VulDeePecker: A Deep Learning-Based System for
Vulnerability Detection
Zhen Li∗†, Deqing Zou∗‡(cid:93), Shouhuai Xu§, Xinyu Ou∗, Hai Jin∗,
Sujuan Wang∗, Zhijun Deng∗ and Yuyi Zhong∗
∗Services Computing Technology and System Lab, Big Data Technology and System Lab,
Cluster and Grid Computing Lab, School of Computer Science and Technology,
Huazhong University of Science and Technology
PI:EMAIL
†School of Cyber Security and Computer, Hebei University
‡Shenzhen Huazhong University of Science and Technology Research Institute
§Department of Computer Science, University of Texas at San Antonio
Abstract—The automatic detection of software vulnerabilities
is an important research problem. However, existing solutions to
this problem rely on human experts to deﬁne features and often
miss many vulnerabilities (i.e., incurring high false negative rate).
In this paper, we initiate the study of using deep learning-based
vulnerability detection to relieve human experts from the tedious
and subjective task of manually deﬁning features. Since deep
learning is motivated to deal with problems that are very different
from the problem of vulnerability detection, we need some guiding
principles for applying deep learning to vulnerability detection. In
particular, we need to ﬁnd representations of software programs
that are suitable for deep learning. For this purpose, we propose
using code gadgets to represent programs and then transform
them into vectors, where a code gadget is a number of (not
necessarily consecutive) lines of code that are semantically related
to each other. This leads to the design and implementation
of a deep learning-based vulnerability detection system, called
Vulnerability Deep Pecker (VulDeePecker). In order to evaluate
VulDeePecker, we present the ﬁrst vulnerability dataset for deep
learning approaches. Experimental results show that VulDeeP-
ecker can achieve much fewer false negatives (with reasonable
false positives) than other approaches. We further apply VulDeeP-
ecker to 3 software products (namely Xen, Seamonkey, and
Libav) and detect 4 vulnerabilities, which are not reported in
the National Vulnerability Database but were “silently” patched
by the vendors when releasing later versions of these products;
in contrast, these vulnerabilities are almost entirely missed by
the other vulnerability detection systems we experimented with.
I.
INTRODUCTION
Many cyber attacks are rooted in software vulnerabilities.
Despite the effort that has been invested in pursuing secure pro-
gramming, software vulnerabilities remain, and will continue,
to be a signiﬁcant problem. This can be justiﬁed by the fact
(cid:93)Corresponding author
Network and Distributed Systems Security (NDSS) Symposium 2018
18-21 February 2018, San Diego, CA, USA
ISBN 1-1891562-49-5
http://dx.doi.org/10.14722/ndss.2018.23158
www.ndss-symposium.org
that the number of vulnerabilities registered in the Common
Vulnerabilities and Exposures (CVE) was approximately 4,600
in 2010, and grew to approximately 6,500 in 2016 [4]. An
alternate approach is to automatically detect vulnerabilities in
software programs, or simply programs for short. There have
been many static vulnerability detection systems and studies
for this purpose, ranging from open source tools [6], [11],
[52], to commercial tools [2], [3], [7], to academic research
projects [19], [28], [32], [37], [38], [49], [59], [60]. However,
existing solutions for detecting vulnerabilities have two major
drawbacks: imposing intense manual labor and incurring high
false negative rates, which are elaborated below.
On one hand, existing solutions for vulnerability detection
rely on human experts to deﬁne features. Even for experts,
this is a tedious, subjective, and sometimes error-prone task
because of the complexity of the problem. In other words,
the identiﬁcation of features is largely an art, meaning that the
quality of the resulting features, and therefore the effectiveness
of resulting detection system, varies with the individuals who
deﬁne them. In principle, this problem can be alleviated by
asking multiple experts to deﬁne their own features, and then
select the set of features that lead to better effectiveness or use
a combination of these features. However, this imposes even
more tedious work. As a matter of fact, it is always desirable
to reduce, or even eliminate whenever possible, the reliance
on the intense labor of human experts. This can be justiﬁed
by the trend of cyber defense automation, which is catalyzed
by initiatives such as DARPA’s Cyber Grand Challenge [5].
It is therefore important to relieve human experts from the
tedious and subjective task of manually deﬁning features for
vulnerability detection.
On the other hand, existing solutions often miss many
vulnerabilities or incur high false negative rates. For example,
two most recent vulnerability detection systems, VUDDY [28]
and VulPecker [32], respectively incur a false negative rate
of 18.2% (when detecting vulnerabilities of Apache HTTPD
2.4.23) and 38% (when applied to 455 vulnerability samples).
Our own independent experiments show that they respectively
incur a false negative rate of 95.1% and 89.8% (see Table
V in Section IV). Note that the large discrepancy between
the false negative rates reported in [28], [32] and the false
negative rates derived from our experiments is caused by the
use of different datasets. These high false negative rates may
be justiﬁed by their emphasis on low false positive rates,
which are respectively 0% for VUDDY [28] and unreported
for VulPecker [32]. Our independent experiments show that
their false positive rates are respectively 0% for VUDDY and
1.9% for VulPecker (see Table V in Section IV). This suggests
that VUDDY and VulPecker are designed to achieve low false
positive rates, which appear to be inherent to the approach of
detecting vulnerabilities caused by code clones; in contrast,
when using this approach to detecting vulnerabilities that are
not caused by code clones, high false negative rates occur.
It would be fair to say that vulnerability detection systems
with high false positive rates may not be usable, vulnerability
detection systems with high false negative rates may not be
useful. This justiﬁes the importance of pursuing vulnerability
detection systems that can achieve low false negative rates and
low false positive rates. When this cannot be achieved (because
false positive and false negative are often at odds with each
other), we may put emphasis on lowering the false negative
rate as long as the false positive rate is not too high.
The aforementioned two limitations of existing solutions
motivate the importance of designing the vulnerability detec-
tion system without asking human experts to manually deﬁne
features and without incurring high false negative rate or false
positive rate. In this paper, we propose a solution to the
following vulnerability detection problem while bearing in
mind with these limitations: Given the source code of a target
program, how can we determine whether or not the target
program is vulnerable and if so, where are the vulnerabilities?
Our contributions. The present paper represents a ﬁrst step
towards ultimately tackling the aforesaid problem. Speciﬁcally,
we make three contributions.
First, we initiate the study of using deep learning for
vulnerability detection. This approach has a great potential
because deep learning does not need human experts to man-
ually deﬁne features, meaning that vulnerability detection can
be automated. However, this approach is challenging because
deep learning is not invented for this kind of applications,
meaning that we need some guiding principles for applying
deep learning to vulnerability detection. We discuss some
preliminary guiding principles for this purpose, including the
representation of software programs to make deep learning
suitable for vulnerability detection, the determination of gran-
ularity at which deep learning-based vulnerability detection
should be conducted, and the selection of speciﬁc neural
networks for vulnerability detection. In particular, we propose
using code gadgets to represent programs. A code gadget is
a number of (not necessarily consecutive) lines of code that
are semantically related to each other, and can be vectorized
as input to deep learning.
Second, we present
the design and implementation of
a deep learning-based vulnerability detection system, called
Vulnerability Deep Pecker (VulDeePecker). We evaluate the ef-
fectiveness of VulDeePecker from the following perspectives:
•
Can VulDeePecker deal with multiple types of vul-
nerabilities at
the same time? This perspective is
important because a target program in question may
contain multiple types of vulnerabilities, meaning that
•
•
a vulnerability detection system that can detect only
one type of vulnerabilities would be too limited. Ex-
perimental results answer this question afﬁrmatively.
This can be explained by the fact that VulDeePecker
uses vulnerability patterns (learned as deep neural
networks) to detect vulnerabilities.
Can human expertise help improve the effectiveness of
VulDeePecker? Experimental results show that the ef-
fectiveness of VulDeePecker can be further improved
by incorporating human expertise, which is not for
deﬁning features though. This hints that automatic
vulnerability detection systems, while being able to re-
lieve human experts from the tedious labor of deﬁning
features, may still need to leverage human expertise
from other purposes. This poses an important open
problem for future study.
How effective is VulDeePecker when compared with
other vulnerability detection approaches? Experimen-
tal results show that VulDeePecker is much more
effective than the other static analysis tools, which
ask human experts to deﬁne rules for detecting vul-
nerabilities, and the state-of-the-art code similarity-
based vulnerability detection systems (i.e., VUDDY
and VulPecker).
These questions may be seen as an initial effort at deﬁning
a benchmark for evaluating the effectiveness of deep learning-
based vulnerability detection systems.
In order to show the usefulness of VulDeePecker, we fur-
ther apply it to 3 software products (namely Xen, Seamonkey,
and Libav). VulDeePecker is able to detect 4 vulnerabilities,
which are not reported in the National Vulnerability Database
(NVD) [10] but were “silently” patched by the vendors when
releasing later versions of these products. In contrast, these
vulnerabilities are almost entirely missed by the other vulnera-
bility detection systems we experimented with. More precisely,
one of those vulnerability detection systems is able to detect 1
of the 4 vulnerabilities (i.e., missing 3 of the 4 vulnerabilities),
while the other systems missed all of the 4 vulnerabilities.
We will conduct more experiments to show whether or not
VulDeePecker can detect vulnerabilities that have not been
identiﬁed, including possibly 0-day vulnerabilities.
Third, since there are no readily available datasets for
answering the questions mentioned above, we present the ﬁrst
dataset for evaluating VulDeePecker and other deep learning-
based vulnerability detection systems that will be developed in
the future. The dataset is derived from two data sources main-
tained by the National Institute of Standards and Technology
(NIST): the NVD [10] and the Software Assurance Reference
Dataset (SARD) project [12]. The dataset contains 61,638 code
gadgets, including 17,725 code gadgets that are vulnerable
and 43,913 code gadgets that are not vulnerable. Among the
17,725 code gadgets that are vulnerable, 10,440 code gadgets
correspond to buffer error vulnerabilities (CWE-119) and the
rest 7,285 code gadgets correspond to resource management
error vulnerabilities (CWE-399). We have made the dataset
available at https://github.com/CGCL-codes/VulDeePecker.
Paper organization. The rest of the paper is organized as
follows. Section II presents some preliminary guiding prin-
2
ciples for deep learning-based vulnerability detection. Section
III discusses the design of VulDeePecker. Section IV describes
our experimental evaluation of VulDeePecker and results.
Section V discusses the limitations of VulDeePecker and open
problems for future research. Section VI describes the related
prior work. Section VII concludes the present paper.
II. GUIDING PRINCIPLES FOR DEEP LEARNING-BASED
VULNERABILITY DETECTION
In this section, we propose some preliminary guiding
principles for using deep learning to detect vulnerabilities.
These principles are sufﬁcient for the present study, but may
need to be reﬁned to serve the more general purpose of deep
learning-based vulnerability detection. These principles are
centered at answering three fundamental questions: (i) How
to represent programs for deep learning-based vulnerability
detection? (ii) What is the appropriate granularity for deep
learning-based vulnerability detection? (iii) How to select a
speciﬁc neural network for vulnerability detection?
A. How to represent software programs?
Since deep learning or neural networks take vectors as
input, we need to represent programs as vectors that are
semantically meaningful for vulnerability detection. In other
words, we need to encode programs into vectors that are
the required input for deep learning. Note that we cannot
arbitrarily transform a program into vectors because the vectors
need to preserve the semantic information of the program.
This suggests us to use some intermediate representation as
a “bridge” between a program and its vector representation,
which is the actual input to deep learning. This leads to the
following:
Guiding Principle 1: Programs can be ﬁrst
transformed
into some intermediate representation that can preserve (some
of) the semantic relationships between the programs’ elements
(e.g., data dependency and control dependency). Then, the
intermediate representation can be transformed into a vector
representation that is the actual input to neural networks.
As we will elaborate later, Guiding Principle 1 leads us to
propose an intermediate representation dubbed code gadget.
The term code gadget is inspired by the term of gadget in the
context of code-reuse attacks (see, e.g., [18]), because a code
gadget is a small number of (not necessarily consecutive) lines
of code.
B. What is an appropriate granularity?
Since it is desirable not only to detect whether a program
is vulnerable or not, but also to pin down the locations of
the vulnerabilities, a ﬁner granularity should be used for
deep learning-based vulnerability detection. This means that
vulnerability detection should not be conducted at the program
or function level, which are too coarse-grained because a
program or function may have many lines of code and pinning
down the locations of its vulnerability can be a difﬁcult task
by itself. This leads to:
Guiding Principle 2: In order to help pin down the loca-
tions of vulnerabilities, programs should be represented at a
ﬁner granularity than treating a program or a function as a
unit.
Indeed,
the aforementioned code gadget representation
leads to a ﬁne-grained granularity for vulnerability detection
because a code gadget often consists of a small number of
lines of code. This means that the code gadget representation
naturally satisﬁes Guiding Principle 2.
C. How to select neural networks?
Neural networks have been very successful in areas such
as image processing, speech recognition, and natural language
processing (e.g., [21], [30], [40]), which are different from
vulnerability detection. This means that many neural networks
may not be suitable for vulnerability detection, and that we
need some principles to guide the selection of neural networks
that are suitable for vulnerability detection. Our examination
suggests the following:
Guiding Principle 3: Because whether or not a line of
code contains a vulnerability may depend on the context,