(40,60)
(35,65)
(30,70)
(25,75)
(20,80)
(15,85)
(10,90)
(5,95)
(0,100)
Range
-
(45,55)
(40,60)
(35,65)
(30,70)
(25,75)
(20,80)
(15,85)
(10,90)
(5,95)
(0,100)
Range
-
(45,55)
(40,60)
(35,65)
(30,70)
(25,75)
(20,80)
(15,85)
(10,90)
(5,95)
(0,100)
Malicious Operational Evaluation
Mimicus FC Attack
True
Negative
Rate
(TNR)
99.8%
99.8%
99.7%
99.7%
99.6%
99.5%
99.3%
99.1%
98.7%
97.0%
53.6%
False
Positive
Rate
(FPR)
0.150%
0.128%
0.103%
0.0832%
0.0712%
0.0552%
0.0331%
0.0291%
0.0261%
0.0120%
0%
FNR
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
0%
FNR
84%
69%
31%
12%
7%
7%
7%
6%
0%
0%
0%
TPR
100%
100%
100%
100%
100%
100%
99.6%
99.6%
99.6%
99.6%
95.6%
TPR
16%
8%
4%
4%
1%
1%
0%
0%
0%
0%
0%
Uncertain
Rate
(UR)
0%
0.0592%
0.147%
0.256%
0.342%
0.456%
0.618%
0.825%
1.27%
3.01%
46.4%
UR
0%
0%
0%
0%
0%
0%
0.366%
0.366%
0.366%
0.366%
4.40%
UR
0%
23%
65%
84%
92%
92%
93%
94%
100%
100%
100%
TABLE XI.
NUMBER OF DOCUMENTS PER GD-KDE ATTACK WHERE
ENSEMBLE SVM CLASSIFIER PROVIDES CORRECT PREDICTION AS
PORTION OF FEATURES USED IS VARIED.
TABLE XIII.
PDFRATE SVM ENSEMBLE CLASSIFIER OUTCOME FOR
GD-KDE EVASION ATTACKS.
Attack
Baseline Malicious
Baseline Benign
F gdkde
FT gdkde
5%
100
2
100
99
Feature Subset
7.5%
99
41
100
100
10%
98
93
99
92
12.5%
98
94
5
1
Attack
Baseline Malicious
Baseline Benign
F gdkde
FT gdkde
Benign
Malicious
Uncertain
2
0
0
7
0
97
91
1
0
93
3
8
98
0
0
0
TABLE XII.
NUMBER OF DOCUMENTS PER GD-KDE ATTACK WHERE
ENSEMBLE SVM CLASSIFIER PROVIDES CORRECT PREDICTION AS
PORTION OF TRAINING DATA USED IS VARIED.
Training Data Subset
Attack
Baseline Malicious
Baseline Benign
F gdkde
FT gdkde
12.5%
86
100
0
0
25%
87
100
0
0
50%
92
100
0
0
100%
98
100
0
0
scaled distance from the SVM decision boundary of a different
SVM implementation to provide a similar result. The GD-KDE
attacks demonstrate that introspection of a single classiﬁer such
as SVM cannot be relied upon to detect evasions.
While effective against an SVM classiﬁer, the results on
PDFrate’s Random Forest classiﬁer using the GD-KDE attack
are roughly comparable to the conventional counterparts (see
Table VII). It is is not practical to wage a similar type of
attack against Random Forests because Random Forests have
extremely complex and stochastic decision boundaries.
We sought to determine the extent to which we could
make an SVM classiﬁer identify probable evasions through
diversity enabled introspection. We implemented a simple
SVM based ensemble classiﬁer using 100 independent SVM
classiﬁers with the score being the simple sum of the votes
of individual classiﬁers. To determine the attributes important
to building diversity in ensembles, we varied the subset of
features and training data used in constructing each of the
individual SVMs. We performed a full grid search. The most
salient results are reported in Table XI, which shows feature
bagging using the full training data set, and Table XII, which
shows bagging on training data using the full feature set.
These tables demonstrate the portion of classiﬁer outcomes
that match the correct result (malicious or uncertain).
It appears that bagging of training data is not particularly
important in building an ensemble classiﬁer where mutual
agreement analysis is useful. To our amazement, we found
no situation where anything but the full training set provided
the best results.
However, bagging of features is critical to constructing a
classiﬁer where mutual agreement analysis is able to identify
uncertain predictions. This bagging of features for an SVM
classiﬁer provides the necessary diversity in extrapolation that
makes mutual agreement analysis meaningful. It seems that the
individual classiﬁers based on subsets of the complete feature
set are much harder to evade collectively than a single classiﬁer
using all the features. While a single classiﬁer can be evaded by
successfully mimicking a subset of the features, it appears that
a combination of multiple classiﬁers based on a small number
of features requires a more complete mimicry across the full
feature set. The application of feature bagging to the many
12
independent SVMs makes a GD-KDE style attack infeasible
as there is no longer a single predictable decision boundary to
attack.
The results also indicate that careful tuning of the portion
of features used in bagging is critical when using an SVM
based ensemble. There seems to be a trade-off between the
ability to correctly classify malicious observations (including
evasion attempts) by using fewer features in each classiﬁer,
and benign observations by using more features. The use of
fewer features results in a more complex classiﬁer with smaller
divisions while more features moves closer to a standard SVM,
which has a single hyperplane divider. These result suggest that
the features used in PDFrate provide better extrapolation for
benign samples. It appears that the malicious samples have
higher variation in PDFrate’s features, requiring more similar
training samples for successful classiﬁcation.
Table XIII shows the outcomes of the SVM ensemble
classiﬁer applied to the Mimicus GD-KDE attacks and baseline
benign and malicious samples. The outcome shows that while
the evasion attempts are successful in dropping the scores
out of the malicious range, the vast majority of the evasion
attempts fall in the uncertain range. Only 8% of the evasion
attempts are fully successful in the best scenario while only
4.5% of the known data is in the uncertain region. These results
are comparable to results obtained using PDFrate’s Random
Forest classiﬁer where GD-KDE attacks are not possible.
Hence, mutual agreement analysis applies not only to Random
Forests, but seems to apply generally to all ensembles which
have adequate diversity. Bagging of features appears central to
this capability.
VIII. DISCUSSION AND FUTURE WORK
Mutual agreement analysis in ensemble classiﬁers provides
an estimate of conﬁdence that the classiﬁer prediction is accu-
rate, without external validation. Many classiﬁers can provide
a score continuum, such as the distance from the decision
boundary used in SVM, but these metrics are not accurate
in the face of mimicry attacks. Furthermore, conventional
measures of conﬁdence are not applicable to data which
diverges from the population for which ground truth is known.
Mutual agreement reﬂects the internal consistency of the
classiﬁer. This internal consistency is a proxy for the con-
ﬁdence of the classiﬁer, assuming adequate strength of the
features. The attacks against PDFrate demonstrate that mimicry
resistant features are critical
to identiﬁcation of novel at-
tacks. The sole strong evasion attack against PDFrate was
successful because it fully evaded PDFrate’s features through
embedding a malicious PDF in another, making the malicious
PDF invisible to the feature extractor. Other attacks, while
seeking to fool the feature extractor, were insufﬁcient because
some features were still operative. If the feature extractor is
resistant to tampering and the features are proper indicators
of malfeasance, then novel attacks will either be detected as
malicious or be rated as uncertain. However, if the feature set
(or feature extraction mechanism) is weak, then evasion will
still be possible. Operators must be vigilant to prevent evasion
during the feature extraction phase of malware detection.
In building an ensemble using base SVM classiﬁers, we
found feature bagging to be critical to generating the diversity
necessary to make mutual agreement measurements meaning-
ful. Unqualiﬁed, bagging refers to the utilization of random
subsets of training data. This method is used extensively in
machine learning techniques. In our study, bagging of training
data was not shown to be important for mutual agreement
analysis. This may have been due to a lack of diversity in that
training set. Further studies might show under what conditions
training data bagging provides diversity useful for facilitating
mutual agreement analysis. We also observed that tuning the
portion of features used in our SVM ensemble was important.
We observed no similar need to tune the parameters of Random
Forest based classiﬁers, but this should be an area of future
study. The number of features tried at each node (mtry) and
the depth of the trees might impact the useful diversity in a
Random Forest. It appears that many features, even if they are
interdependent or have low classiﬁcation value, contribute to
make evasion more difﬁcult.
If the features are strong, then the relevance of the training
set will dictate the mutual agreement rating for individual
observations. If the test observations are similar to samples
in the training set, then high certainty predictions will occur.
The test observations that differ from the training set in feature