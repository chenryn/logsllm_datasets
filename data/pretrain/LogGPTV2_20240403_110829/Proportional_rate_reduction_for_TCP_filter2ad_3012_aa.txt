title:Proportional rate reduction for TCP
author:Nandita Dukkipati and
Matt Mathis and
Yuchung Cheng and
Monia Ghobadi
Proportional Rate Reduction for TCP
Nandita Dukkipati, Matt Mathis, Yuchung Cheng, Monia Ghobadi
Google, Inc.
{nanditad, mattmathis, ycheng}@google.com, PI:EMAIL
Mountain View
California, U.S.A
ABSTRACT
Packet losses increase latency for Web users. Fast recovery
is a key mechanism for TCP to recover from packet losses.
In this paper, we explore some of the weaknesses of the stan-
dard algorithm described in RFC 3517 and the non-standard
algorithms implemented in Linux. We ﬁnd that these algo-
rithms deviate from their intended behavior in the real world
due to the combined eﬀect of short ﬂows, application stalls,
burst losses, acknowledgment (ACK) loss and reordering,
and stretch ACKs. Linux suﬀers from excessive congestion
window reductions while RFC 3517 transmits large bursts
under high losses, both of which harm the rest of the ﬂow
and increase Web latency.
Our primary contribution is a new design to control trans-
mission in fast recovery called proportional rate reduction
(PRR). PRR recovers from losses quickly, smoothly and
accurately by pacing out retransmissions across received
ACKs.
In addition to PRR, we evaluate the TCP early
retransmit (ER) algorithm which lowers the duplicate ac-
knowledgment threshold for short transfers, and show that
delaying early retransmissions for a short interval is eﬀec-
tive in avoiding spurious retransmissions in the presence of
a small degree of reordering. PRR and ER reduce the TCP
latency of connections experiencing losses by 3-10% depend-
ing on the response size. Based on our instrumentation on
Google Web and YouTube servers in U.S. and India, we also
present key statistics on the nature of TCP retransmissions.
Categories and Subject Descriptors
C.2
WORKS]: Network Protocols
[COMPUTER-COMMUNICATION
NET-
General Terms
Algorithms, Design, Experimentation, Measurement, Per-
formance.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
Keywords
TCP, fast recovery, proportional rate reduction, early re-
transmit, retransmission statistics.
1.
INTRODUCTION
Web latency plays a key role in producing responsive Web
applications, making information more accessible and help-
ing to advance new cloud-based applications. There are
many factors that contribute to Web latency including con-
tent that is not optimized for speed, ineﬃcient Web servers,
slow browsers, limited network bandwidth, excess losses and
suboptimal network protocols.
In this paper, we focus on
reducing latency for TCP connections experiencing packet
losses. Measurements show that over 6% of HTTP responses
served from Google.com experience losses and that these
losses impact user experience. We investigate some of these
loss statistics and revisit TCP’s loss recovery mechanisms
with the goal of reducing Web latency for users.
To get a sense for how much packet losses increase Web
latency, we compare the TCP latency of HTTP responses ex-
periencing losses to their ideal achievable latency. Figure 1
(top plot) shows the average TCP latency for responses with
sizes ranging between 4kB and 8kB broken down in 200ms
round-trip time (RTT) buckets, measured from billions of
user TCP connections to Google Web servers world-wide.
The TCP latency of a HTTP response is measured from
when the server sends the ﬁrst byte until it receives the ac-
knowledgment (ACK) for the last byte. We deﬁne the ideal
response time to be the ﬁxed portion of the network delay,
which we approximate to be the minimum measured RTT for
any given HTTP response. The approximation works well
because the 4-8kB responses ﬁt well within TCP’s initial
congestion window of 10 segments used on Google servers.
Responses experiencing losses last 7-10 times longer than
the ideal, while those with no losses are very close to the
ideal. The CDF in Figure 1 (bottom plot) shows that the
latency spread for responses with losses is 200 RTTs - about
10x greater than those without losses. Several independent
factors, including slow user network bandwidths, long queu-
ing delays, and TCP’s mechanisms must be addressed for
latency to approach the ideal.
TCP has two primary means of detecting and recovering
from losses. First is fast retransmit, where TCP performs
a retransmission of the missing segment after receiving a
certain number of duplicate acknowledgments (dupacks) [4].
As a fall back, whenever fast retransmission is unsuccessful
or when a sender does not receive enough duplicate ACKs,
TCP uses a second slower but more robust mechanism where
155]
s
m
[
y
c
n
e
t
a
l
P
C
T
F
D
C
Resp. w/  rexmit
Resp. w/o rexmit
Ideal
 7000
 6000
 5000
 4000
 3000
 2000
 1000
 0
 0
 200
 400
 600
 800
 1000
RTT bucket [ms]
 1
 0.8
 0.6
 0.4
 0.2
 0
Resp. w/ rexmit
Resp. w/o rexmit
 0
 20  40  60  80  100  120  140  160  180  200
# round-trips
Figure 1: Top plot shows the average TCP latency
of Google HTTP responses in diﬀerent round-trip
time buckets for response sizes between 4kB and
8kB. Bottom plot shows CDF of the number of RTTs
taken by responses of all sizes with and without re-
transmissions.
it waits for a duration of retransmission timeout (RTO) be-
fore deducing that a segment is lost.
Our measurements show that fast recovery accounts for
about 25% of all retransmissions in short ﬂows served from
Google Web servers and over 50% of retransmissions in bulk
video traﬃc. There are two widely deployed algorithms
used to adjust the congestion window (cwnd) during fast
recovery, RFC 3517 [8] and rate halving [17], implemented
in Linux. After extensive analysis of these algorithms on
Google servers, we ﬁnd that in practice both can be either
too conservative or too aggressive resulting in a long recov-
ery time or in excessive retransmissions.
The three main contributions of this paper are as follows:
1. Proportional rate reduction: We designed a new
fast recovery algorithm, proportional rate reduction
(PRR), which is inspired by the rate halving algo-
rithm [17]. PRR recovers from losses both quickly and
smoothly by using Van Jacobson’s packet conservation
principle to pace out retransmissions across received
ACKs. Speciﬁcally, PRR improves upon the existing
(non-standard) Linux fast recovery and RFC speciﬁed
standard fast recovery under a number of conditions
including: a) burst losses where the losses implicitly
reduce the amount of outstanding data (pipe) below
the slow start threshold value selected by the conges-
tion control algorithm and, b) losses near the end of
short ﬂows where application runs out of data to send.
PRR also performs accurate cwnd adjustments even
under stretch ACKs or ACK loss and reordering. Fur-
thermore, PRR is designed to work with the diverse set
of congestion control algorithms present in today’s In-
ternet [28]: when the recovery ﬁnishes, TCP completes
the window adjustment as intended by the congestion
control algorithm.
PRR has been approved to become the default Linux
fast recovery algorithm for Linux 3.x.
2. Experiments with Early Retransmit (ER): We
address the question of how to trigger fast retransmit
for short ﬂows. This question is important because
short HTTP transactions frequently do not receive the
three dupacks necessary to trigger conventional fast re-
covery. We evaluate the TCP early retransmit (ER)
algorithm [2] that lowers duplicate acknowledgment
threshold (dupthresh) for short transfers. Google Web
server experiments show that the Internet has enough
reordering to cause naive ER to exhibit excessive spu-
rious retransmissions. We show that delaying early
retransmissions for a short interval is eﬀective at mit-
igating the spurious retransmissions.
3. Retransmission statistics
of Google Web
servers: We analyse TCP data from Google Web and
video servers and present key statistics on the nature
of retransmissions.
The rest of the paper is organized as follows: Section 2
describes TCP measurements of Google’s user traﬃc. Sec-
tion 3 reviews state-of-the-art fast recovery as described in
the standards and the non-standard Linux variant. In Sec-
tion 4, we present the design and properties of proportional
rate reduction. Section 5 evaluates PRR’s performance on
Google Web and YouTube servers. In Section 6, we describe
the experimental results of early retransmit. Section 7 sum-
marizes related work. Section 8 concludes the paper along
with a discussion on future work.
2. GOOGLE MEASUREMENTS
We sampled TCP statistics on unmodiﬁed Google Web
servers world-wide for one week in May 2011. The servers
use a recent version of Linux 2.6 with settings listed in Ta-
ble 4. The servers do not include the changes proposed in
this paper. The global statistics for interactive Web services
are summarized in Table 1. Among the billions of connec-
tions sampled, 96% of the connections negotiated SACK but
only 12% of the connections negotiated Timestamps. The
majority of clients are Microsoft Windows which by default
do not use TCP Timestamps.
Although over 94% of connections use HTTP/1.1 and the
Google Web Servers keep idle TCP connections up to 4 min-
utes, there were on average only 3.1 HTTP requests per con-
nection. The average HTTP response size from Google.com
was 7.5KB, which is similar to the average Web object size
of 7.2KB measured for billions of Web sites in 2010 [21]. The
average user network bandwidth as observed from Google is
1.9Mbps and is in agreement with another study in 2010 [1].
While the average per segment TCP retransmission rate is
2.8%, 6.1% of the HTTP responses have TCP retransmis-
sions.
2.1 Detailed retransmission statistics
We examine the existing1 Linux loss recovery mechanisms
as measured from the Google Web servers. We measure the
1Note PRR is already slated to be released in upstream ker-
nels.
156TCP
Total connections
Connections support SACK
Connections support Timestamp
HTTP/1.1 connections
Average requests per connection
Average retransmissions rate
HTTP
Billions
96%
12%
94%
3.1
2.8%
Average response size
Responses with TCP retransmissions
7.5kB
6.1%
Table 1: Summary of Google TCP and HTTP statis-
tics sampled for one week in May 2011. The data
include both port 80 and 443 but exclude YouTube
videos and bulk downloads.
Fast retransmits
Timeout retransmits
Timeout in Open
Timeout in Disorder
Timeout in Recovery
Timeout Exp. Backoﬀ
Slow start retransmits
Failed retransmits
DC1 DC2
24% 54%
43% 17%
8%
30%
2%
3%
2%
1%
10%
4%
17% 29%
15%
0%
Table 2: A breakdown of retransmissions on Google
Web servers in two data centers. All percentages
are with reference to the total number of retrans-
missions.
types of retransmissions from a large U.S. data center, DC1,
which primarily serves users from the U.S. east coast and
South America. We selected this data center because it has
a good mix of diﬀerent RTTs, user bandwidths and loss
rates. We also measured another data-center, DC2, which
is in India and exclusively serves YouTube videos. For ease
of comparison analysis, we also use the same data centers to
experiment with our own changes to fast recovery described
in later sections.
We collect the Linux TCP SNMP statistics from Google
Web servers for 72 hours at DC1 in April 2011 and DC2
in August 2011. Observations described here are consistent
across several such sample sizes taken in diﬀerent weeks and
months. The average (server) retransmission rates are 2.5%
in DC1 and 5.6% in DC2. DC2 has a higher retransmission
rate because the network in India has lower capacity and
higher congestion.
Table 2 shows the breakdown of TCP retransmission
types.
It shows that fast recovery is a key mechanism to
recover losses in both bulk download (video) and short ﬂows
(Web pages). In DC2, the retransmissions sent during fast
recovery, labeled as fast retransmits, comprise 54% of all re-
transmissions. In DC1, fast recovery still recovers nearly a
quarter (24%) of losses. This diﬀerence between DC1 and
DC2 is because the long video ﬂows of DC2 have a greater
chance of entering fast recovery compared to the shorter
Web traﬃc of DC1. The ﬁrst retransmission upon a time-
out, labeled as timeout retransmits, constitute 43% of the
retransmissions in DC1. This is mainly caused by the Web
Fast retransmits/FR
DSACKs/FR
DSACKs/retransmit
Lost (fast) retransmits/FR
Lost retransmits/retransmit
DC1 DC2
3.15
2.93
12%
4%
3.8% 1.4%
9%
1.9% 3.1%
6%
Table 3: Fast recovery related statistics on Google
Web servers. All numbers are with respect to the
total number of fast recovery events or fast retrans-
mits as indicated.
objects that are too small to trigger fast recovery. More-
over, the highest query volume in DC1 turns out to be from
statistics collection applications such as Google Analytics.
Typically their HTTP responses are tiny and ﬁt entirely into
one segment. As a result losses can not cause any dupacks
so the only available recovery mechanism is a timeout. In-
terestingly, DC1 and DC2 shows very diﬀerent distribution
of timeouts in various TCP recovery states. In DC1, the ma-
jority of timeouts happen in the open state, i.e., without any
preceding dupacks or other indication of losses.2 However
in DC2, more timeouts occur in non-Open states.
In DC1 and DC2, 17% and 29% of total retransmissions
occur in the slow start phase after the timeout retransmis-
sion has successfully advanced snd.una. These are called
slow start retransmissions because typically the sender has
reset cwnd to one after a timeout and it is operating in slow
start phase. DC2 has more timeout retransmissions than
DC1 because the timeout happens when outstanding data
are typically much larger for video download. The three
types of retransmissions, fast retransmits, timeout retrans-
mits, and slow start retransmits, successfully recover the
losses and constitute 85% and 100% of the total retransmis-
sions in data centers DC1 and DC2 respectively.
For the remaining 15% retransmissions in DC1 termed
failed retransmits, TCP failed to make any forward progress
because no further TCP acknowledgments were received
from the client and the server eventually aborts the connec-
tion. This diﬀerence is partially due to diﬀering maximum
timeout setting on the servers in DC1 and DC2, however we
suspect that there may be other contributing factors.
Table 3 shows some additional statistics on how well fast
recovery is performing.
Both data centers exhibit about three fast retransmits per
fast recovery event. This suggests that loss is highly corre-
lated (a property of the network) and provides a sense of how
much the network around each data center exercises TCP’s
loss recovery machinery. This metric should be mostly in-
sensitive to changes in recovery algorithms, although there