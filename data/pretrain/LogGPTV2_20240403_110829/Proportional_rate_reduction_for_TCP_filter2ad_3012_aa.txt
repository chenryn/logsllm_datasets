# Proportional Rate Reduction for TCP

## Authors
Nandita Dukkipati, Matt Mathis, Yuchung Cheng, Monia Ghobadi  
Google, Inc.  
{nanditad, mattmathis, ycheng}@google.com, PI:EMAIL  
Mountain View, California, U.S.A.

## Abstract
Packet losses significantly increase latency for web users. Fast recovery is a critical mechanism for TCP to recover from packet losses. In this paper, we examine the weaknesses of the standard algorithm described in RFC 3517 and the non-standard algorithms implemented in Linux. We find that these algorithms deviate from their intended behavior in real-world scenarios due to factors such as short flows, application stalls, burst losses, acknowledgment (ACK) loss and reordering, and stretch ACKs. Linux suffers from excessive congestion window reductions, while RFC 3517 transmits large bursts under high losses, both of which harm the rest of the flow and increase web latency.

Our primary contribution is a new design called Proportional Rate Reduction (PRR) to control transmission during fast recovery. PRR recovers from losses quickly, smoothly, and accurately by pacing out retransmissions across received ACKs. Additionally, we evaluate the TCP Early Retransmit (ER) algorithm, which lowers the duplicate acknowledgment threshold for short transfers. We show that delaying early retransmissions for a short interval effectively avoids spurious retransmissions in the presence of a small degree of reordering. PRR and ER reduce the TCP latency of connections experiencing losses by 3-10% depending on the response size. Based on our instrumentation on Google Web and YouTube servers in the U.S. and India, we also present key statistics on the nature of TCP retransmissions.

## Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Network Protocols

## General Terms
Algorithms, Design, Experimentation, Measurement, Performance

## Keywords
TCP, fast recovery, proportional rate reduction, early retransmit, retransmission statistics

## 1. Introduction
Web latency plays a crucial role in producing responsive web applications, making information more accessible, and advancing new cloud-based applications. Many factors contribute to web latency, including unoptimized content, inefficient web servers, slow browsers, limited network bandwidth, excess losses, and suboptimal network protocols. This paper focuses on reducing latency for TCP connections experiencing packet losses. Our measurements show that over 6% of HTTP responses served from Google.com experience losses, impacting user experience. We investigate these loss statistics and revisit TCP's loss recovery mechanisms with the goal of reducing web latency for users.

To understand how much packet losses increase web latency, we compare the TCP latency of HTTP responses experiencing losses to their ideal achievable latency. Figure 1 (top plot) shows the average TCP latency for responses with sizes ranging between 4kB and 8kB, broken down into 200ms round-trip time (RTT) buckets, measured from billions of user TCP connections to Google Web servers worldwide. The TCP latency of an HTTP response is measured from when the server sends the first byte until it receives the acknowledgment (ACK) for the last byte. We define the ideal response time as the fixed portion of the network delay, approximated as the minimum measured RTT for any given HTTP response. Responses experiencing losses last 7-10 times longer than the ideal, while those with no losses are very close to the ideal. The CDF in Figure 1 (bottom plot) shows that the latency spread for responses with losses is 200 RTTs, about 10x greater than those without losses. Several independent factors, including slow user network bandwidths, long queuing delays, and TCP's mechanisms, must be addressed for latency to approach the ideal.

TCP has two primary means of detecting and recovering from losses. First, fast retransmit, where TCP performs a retransmission of the missing segment after receiving a certain number of duplicate acknowledgments (dupacks). As a fallback, whenever fast retransmission is unsuccessful or when a sender does not receive enough duplicate ACKs, TCP uses a second, slower but more robust mechanism, waiting for a retransmission timeout (RTO) before deducing that a segment is lost.

Our measurements show that fast recovery accounts for about 25% of all retransmissions in short flows served from Google Web servers and over 50% of retransmissions in bulk video traffic. There are two widely deployed algorithms used to adjust the congestion window (cwnd) during fast recovery: RFC 3517 and rate halving, implemented in Linux. After extensive analysis of these algorithms on Google servers, we find that in practice, both can be either too conservative or too aggressive, resulting in a long recovery time or excessive retransmissions.

The three main contributions of this paper are:
1. **Proportional Rate Reduction (PRR):** We designed a new fast recovery algorithm, PRR, inspired by the rate halving algorithm. PRR recovers from losses quickly and smoothly by using Van Jacobson’s packet conservation principle to pace out retransmissions across received ACKs. PRR improves upon existing Linux fast recovery and RFC-specified standard fast recovery under various conditions, including burst losses and losses near the end of short flows. PRR also performs accurate cwnd adjustments even under stretch ACKs or ACK loss and reordering. Furthermore, PRR is designed to work with the diverse set of congestion control algorithms present in today’s Internet. PRR has been approved to become the default Linux fast recovery algorithm for Linux 3.x.
2. **Experiments with Early Retransmit (ER):** We address the question of how to trigger fast retransmit for short flows, as short HTTP transactions frequently do not receive the three dupacks necessary to trigger conventional fast recovery. We evaluate the TCP early retransmit (ER) algorithm, which lowers the duplicate acknowledgment threshold for short transfers. Google Web server experiments show that the Internet has enough reordering to cause naive ER to exhibit excessive spurious retransmissions. We show that delaying early retransmissions for a short interval effectively mitigates spurious retransmissions.
3. **Retransmission Statistics of Google Web Servers:** We analyze TCP data from Google Web and video servers and present key statistics on the nature of retransmissions.

The rest of the paper is organized as follows: Section 2 describes TCP measurements of Google’s user traffic. Section 3 reviews state-of-the-art fast recovery as described in the standards and the non-standard Linux variant. In Section 4, we present the design and properties of PRR. Section 5 evaluates PRR’s performance on Google Web and YouTube servers. In Section 6, we describe the experimental results of early retransmit. Section 7 summarizes related work. Section 8 concludes the paper along with a discussion on future work.

## 2. Google Measurements
We sampled TCP statistics on unmodified Google Web servers worldwide for one week in May 2011. The servers use a recent version of Linux 2.6 with settings listed in Table 4. The servers do not include the changes proposed in this paper. The global statistics for interactive web services are summarized in Table 1. Among the billions of connections sampled, 96% of the connections negotiated SACK, but only 12% of the connections negotiated Timestamps. The majority of clients are Microsoft Windows, which by default do not use TCP Timestamps.

Although over 94% of connections use HTTP/1.1 and the Google Web Servers keep idle TCP connections up to 4 minutes, there were on average only 3.1 HTTP requests per connection. The average HTTP response size from Google.com was 7.5KB, similar to the average web object size of 7.2KB measured for billions of websites in 2010. The average user network bandwidth as observed from Google is 1.9Mbps, consistent with another study in 2010. While the average per-segment TCP retransmission rate is 2.8%, 6.1% of the HTTP responses have TCP retransmissions.

### 2.1 Detailed Retransmission Statistics
We examine the existing Linux loss recovery mechanisms as measured from the Google Web servers. We measure the types of retransmissions from a large U.S. data center, DC1, which primarily serves users from the U.S. east coast and South America. We selected this data center because it has a good mix of different RTTs, user bandwidths, and loss rates. We also measured another data center, DC2, in India, which exclusively serves YouTube videos. For ease of comparison, we use the same data centers to experiment with our own changes to fast recovery described in later sections.

We collect the Linux TCP SNMP statistics from Google Web servers for 72 hours at DC1 in April 2011 and DC2 in August 2011. Observations are consistent across several sample sizes taken in different weeks and months. The average (server) retransmission rates are 2.5% in DC1 and 5.6% in DC2. DC2 has a higher retransmission rate due to lower network capacity and higher congestion in India.

Table 2 shows the breakdown of TCP retransmission types. It indicates that fast recovery is a key mechanism to recover losses in both bulk download (video) and short flows (web pages). In DC2, fast retransmissions comprise 54% of all retransmissions, while in DC1, they account for 24%. This difference is because the long video flows in DC2 have a greater chance of entering fast recovery compared to the shorter web traffic in DC1. The first retransmission upon a timeout, labeled as timeout retransmits, constitutes 43% of the retransmissions in DC1, mainly caused by web objects too small to trigger fast recovery. Moreover, the highest query volume in DC1 is from statistics collection applications such as Google Analytics, whose HTTP responses are tiny and fit entirely into one segment, leading to timeouts as the only available recovery mechanism. Interestingly, DC1 and DC2 show very different distributions of timeouts in various TCP recovery states. In DC1, the majority of timeouts happen in the open state, i.e., without any preceding dupacks or other indication of losses. However, in DC2, more timeouts occur in non-Open states.

In DC1 and DC2, 17% and 29% of total retransmissions occur in the slow start phase after the timeout retransmission has successfully advanced snd.una. These are called slow start retransmissions because the sender typically resets cwnd to one after a timeout and operates in the slow start phase. DC2 has more timeout retransmissions than DC1 because the timeout happens when outstanding data are typically much larger for video downloads. The three types of retransmissions—fast retransmits, timeout retransmits, and slow start retransmits—successfully recover the losses and constitute 85% and 100% of the total retransmissions in data centers DC1 and DC2, respectively.

For the remaining 15% of retransmissions in DC1, termed failed retransmits, TCP fails to make any forward progress because no further TCP acknowledgments are received from the client, and the server eventually aborts the connection. This difference is partially due to differing maximum timeout settings on the servers in DC1 and DC2, but other contributing factors may also be at play.

Table 3 shows additional statistics on how well fast recovery is performing. Both data centers exhibit about three fast retransmits per fast recovery event, suggesting that loss is highly correlated (a property of the network) and provides a sense of how much the network around each data center exercises TCP's loss recovery machinery. This metric should be mostly insensitive to changes in recovery algorithms, although there may be some variations.