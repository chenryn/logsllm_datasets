title:Private Continual Release of Real-Valued Data Streams
author:Victor Perrier and
Hassan Jameel Asghar and
Dali Kaafar
Private Continual Release of Real-Valued Data
Streams
Victor Perrier
ISAE-SUPAERO
& Data61, CSIRO
PI:EMAIL
Hassan Jameel Asghar
Macquarie University
& Data61, CSIRO
PI:EMAIL
Dali Kaafar
Macquarie University
& Data61, CSIRO
PI:EMAIL
Abstract—We present a differentially private mechanism to
display statistics (e.g., the moving average) of a stream of real
valued observations where the bound on each observation is either
too conservative or unknown in advance. This is particularly
relevant to scenarios of real-time data monitoring and reporting,
e.g., energy data through smart meters. Our focus is on real-world
data streams whose distribution is light-tailed, meaning that the
tail approaches zero at least as fast as the exponential distribution.
For such data streams, individual observations are expected to
be concentrated below an unknown threshold. Estimating this
threshold from the data can potentially violate privacy as it would
reveal particular events tied to individuals [1]. On the other hand
an overly conservative threshold may impact accuracy by adding
more noise than necessary. We construct a utility optimizing
differentially private mechanism to release this threshold based
on the input stream. Our main advantage over the state-of-the-art
algorithms is that the resulting noise added to each observation
of the stream is scaled to the threshold instead of a possibly
much larger bound; resulting in considerable gain in utility when
the difference is signiﬁcant. Using two real-world datasets, we
demonstrate that our mechanism, on average, improves the utility
by a factor of 3.5 on the ﬁrst dataset, and 9 on the other. While our
main focus is on continual release of statistics, our mechanism for
releasing the threshold can be used in various other applications
where a (privacy-preserving) measure of the scale of the input
distribution is required.
I.
INTRODUCTION
Many services can beneﬁt from real-time monitoring of
statistics from customer data. Examples include electricity
usage in a neighbourhood collected through smart meters,
customers’ expenditure in a supermarket on a given day,
and commute time of residents of a city during peak hours.
Statistics for these applications can be obtained from real-time
data collected through a variety of sensors and refreshed as
new data arrives. These statistics can then be displayed to
analysts and planners who could use them to optimize services.
Privacy concerns, however, preclude release of raw statistics.
For instance, a customer at a pharmacy would not be willing to
disclose the purchase of medicines linked to a peculiar health
condition. Likewise, analysis of smart meter data can likely
reveal the activities of a particular household or even whether
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23535
www.ndss-symposium.org
anyone is at home or not. Such privacy violations have been
demonstrated for the case of smart meter data where patterns
such as the number of people in the household as well as
sleeping and eating routines were revealed even without any
prior training [1]. The goal therefore is to enable monitoring
of statistics without compromising individual privacy.
A natural candidate for privacy protection is the rigorous
framework of differential privacy [2], [3]. Informally, any
algorithm satisfying the deﬁnition of differential privacy has
the property that its output distribution (based on the coin
tosses of the algorithm) on a given database is close in
probability to the output distribution if any single row in the
dataset is replaced. The closeness is parameterized by the
privacy budget . Most of the work on differential privacy has
focused on static (input) datasets, and there has been very little
focus on datasets that are continuously being updated as in our
setting [4], [5]. Despite this, there is a growing need to shift
focus to provide privacy in the dynamic setting which is likely
to be more pervasive in the near future [6].
More precisely, our scenario is concerned with releas-
ing statistics from a sequence of observations arriving in a
streaming fashion each within some public upper bound B.
Our statistic of interest is the continually changing average
as new observations arrive. This can be readily obtained by
summing all the observations seen thus far (since the number
of observations is assumed public). We remark that our focus
is on approaches that provide event level privacy [4] only,
which means that individuals are guaranteed that their peculiar
events remain private but not necessarily the general trend.1
For many use cases this is a suitable guarantee of privacy,
e.g., individuals might be happy to disclose their routine trip
to work while unwilling to share the occasional detour. One
way to release the sum via differential privacy is to add
independent noise generated through the Laplace distribution
scaled to B [2]. However, this results in cumulative error
(absolute difference from the true sum) of O(B
n) after n
observations. Two aforementioned works on continual release
of datasets, i.e., [4] and [5], focus on binary streams, where
each observation is either 0 or 1. We can generalize their
algorithm to observations within the bound B which results
in a considerably reduced error of O(B(log2 n)1.5).
√
While this signiﬁcantly reduces the error over the basic
approach, the error is still proportional to B. In many real
1The latter is guaranteed through user level privacy, i.e., privacy for all
events from a user. See [3, §12] and [4] for a further discussion on the merits
of event versus user-level privacy.
world situations, the bound B might not be known in advance,
or known only as the worse case bound resulting in an overly
conservative estimate of the true bound. Likewise, perhaps
most observations are tightly concentrated below an unknown
threshold τ well below B. For instance, returning to our
commute time use case,
is highly unlikely that anyone
would be commuting for the full 24 hours on a given day.
We are interested in a mechanism that allows us to determine
a threshold τ below which majority of the observations are
concentrated. This in turn allows to release statistics with noise
scaled to τ rather than B resulting in error O(τ (log2 n)1.5),
which is a signiﬁcant improvement depending on B, τ and n.2
it
However, estimating τ is not straightforward due to a
number of reasons. First, estimating τ beforehand would result
in high or even unbounded cumulative error due to outliers.
Thus, any algorithm needs to observe at least a small subset of
initial observations before determining τ. This time lag needs
to be optimised for accuracy: estimating τ too early will result
in high accumulated error, and too late will only show marginal
improvement over the default case (i.e., when using B as the
estimate). Likewise, again for reasons of accuracy, we need to
ensure that readings outside the threshold are sporadic. Finally
and most importantly, naively estimating τ can result in privacy
violation by leaking information speciﬁc to an individual, e.g.,
if we take the maximum of the observations seen so far as τ,
we display the exact value corresponding to a particular event
from an individual.
In this paper, we propose a mechanism that allows us to
estimate the threshold τ using a subset of observations from
an incoming stream via differential privacy, simultaneously
optimizing utility for releasing the moving average. Although
we optimize utility for the case of moving averages, our
mechanism for releasing the threshold is generic enough to
be used for other statistics and applications. These include
displaying the average with a sliding window [7] or releasing
histogram of the streaming data [8] where in all cases the noise
will be scaled to the most concentrated part of the distribution
of the stream.
In addition to theoretical accuracy guarantees, we provide
empirical evidence of the utility gain of our scheme using
two real world datasets: the ﬁrst dataset contains about 50
million individual trip times on public trains in the city of
Sydney (Australia) over a period of two weeks, and the second
dataset is composed of individual amount spent over 140,000
transaction by about 1,000 customers in a major Australian
supermarket. Using the two datasets we ﬁrst verify that real
world data has the property that most readings are concentrated
tightly well below a conceivable conservative bound B. Using
the same datasets we then show that our improved algorithm
displays the average statistic (commute time or amount spent)
with a utility many orders of magnitude (≈ 3.5 and 9 resp.,
on the two datasets) better than applying (generalized versions
of) the state of the art algorithms [4], [5]. Our utility gain is
2For instance, assume n = 1, 000, 000 and the known bound is B =
10, 000, and we are interested in the average. Assume further that almost
all observations are within τ = 100 with an average of 30. Then, through
the original mechanism we get the (noisy) average as 30 ± 1. Through the
mechanism that scales noise according to τ, we get the noisy average as
30±0.01, an improvement by a factor of B/τ = 100. This can be signiﬁcant
if the average is required with high precision.
for data streams that obey a light-tailed distribution, namely a
distribution whose tail lies below the exponential distribution
(beyond the above mentioned threshold; see Section II-C for
a precise deﬁnition). We argue and show that many real-world
datasets are expected to satisfy this property.3
II. BACKGROUND
In this section we formally describe our problem, associ-
ated deﬁnitions and overview of the algorithm from [4] and [5]
referred to as the binary tree (BT) algorithm which will serve
both as a benchmark and a sub-module of our technique.
A. Problem Statement
Let B be a positive real number. We model input streams
N
(or strings), denoted σ, as the set of ﬁnite strings Σ = [0, B]
of length at most n. The ith element of σ shall be denoted
by σ(i), and shall be called the ith observation or reading.
A generic element or observation from σ shall be denoted by
x. For j ≥ i, σ(i:j) represents the substring (or sub-stream)
σ(i)||···||σ(j), where || is the concatenation operator. We are
interested in ﬁnding the average of the elements of the stream
j=1 σ(j)
at each step i ∈ [n], since we assume the observation counter
to be public. Our goal is to release a privacy-preserving version
of this sum.
σ at each time step i ∈ N. This reduces to ﬁnding(cid:80)i
deﬁned for σ ∈ Σ and i ∈ [n] as c(σ, i) =(cid:80)i
B. Privacy Deﬁnitions
Deﬁnition 1 (Sum Query). We call the function c : Σ×N → R
j=1 σ(i) as the
sum query.
Deﬁnition 2 (Adjacent Streams). Let σ, σ(cid:48) ∈ Σ, The Hamming
distance d(σ, σ(cid:48)) is the number of elements different in the
corresponding positions of the two strings, i.e., d(σ, σ(cid:48)) = |{i :
σ(i) (cid:54)= σ(cid:48)(i),∀i ∈ N}|. The two streams σ and σ(cid:48) are adjacent
if and only if d(σ, σ(cid:48)) = 1.
Deﬁnition 3 ((, δ)-Differential Privacy). A summation mech-
anism M is (, δ)-differentially private if and only if for any
two adjacent streams σ, σ(cid:48) we have ∀n ∈ N and ∀S ⊂ R,
Pr [M (c, σ, n) ∈ S] ≤ Pr [M (c, σ(cid:48), n) ∈ S] × e + δ,
where  is a small constant and δ is a negligible function in
n. We shall use ˆc to denote the output of M in the following.
Note that the notion of differential privacy for streaming
data is the same as that for static datasets. The difference lies in
how neighboring datasets are deﬁned. In the case of streaming
data, neighboring datasets are deﬁned as streams differing in
one element (one event anywhere in the stream). The privacy
deﬁnition does not assume the stream σ to have any speciﬁc
distribution, barring the fact that each of its element is within
[0, B]. For utility however we shall assume that the streams
are sampled with some underlying probability distribution with
support over the set [0, B].
Deﬁnition 4 (Probability Distribution of Streams). Let B ∈
R+. Denote by FB the probability distribution which satisﬁes
3Also see our discussion on what real-world datasets are likely to be light-
tailed versus heavy-tailed in Section IX.
2
Pr [X ∈ [0, B]] = 1, for any random variable X distributed as
FB. A string σ is said to have distribution FB, if for all i ∈ N,
Xi = σ(i) is sampled from FB. We denote this by σ ←FB Σ.
Deﬁnition 5 ((α, β) Utility). The mechanism ˆc is said to be
(α, β)-useful if for all n ∈ N and σ ←FB Σ,
Pr [|ˆc(σ, n) − c(σ, n)| ≤ α] ≥ 1 − β,
where the probability is over the coin tosses of ˆc and the
distribution FB.
Note that the above is different from the utility deﬁnition
in [5], where the probability is over the coin tosses of ˆc only,
and hence the inequality is satisﬁed for all strings σ. In our
case, we shall be utilizing the probability that certain strings
are more likely realized in practice; hence the use of the
distribution FB. We stress again that the privacy deﬁnition
does not rely on FB.
Consider an arbitrary function c : Σ → R. The sum query
falls under this deﬁnition with an auxiliary parameter n ∈ N.
We ﬁrst deﬁne the global sensitivity of c.
Deﬁnition 6 (Global Sensitivity). The global sensitivity of a
function c : Σ → R, denoted GS, is deﬁned as
GS(c) =
max
σ,σ(cid:48)∈Σ : d(σ,σ(cid:48))≤1
|c(σ) − c(σ(cid:48))|.
Deﬁnition 7 (Laplace Mechanism). Let Lap(b) denote the
probability density function of the Laplace distribution with
mean 0 and scale b given as Lap(b) = 1
. Then the
mechanism ˆc(σ) = c(σ) + Y, where Y is drawn from Lap( GS
 )
is (, 0)-differentially private [2].
2b exp
b
(cid:16)|x|
(cid:17)
A deﬁnition of sensitivity that is deﬁned for a particular
input string σ is called local sensitivity.
Deﬁnition 8 (Local Sensitivity). The local sensitivity of a
function c : Σ → R at σ ∈ Σ, denoted LSσ, is deﬁned as
LSσ(c) =
max
σ(cid:48)∈Σ : d(σ,σ(cid:48))=1
|c(σ) − c(σ(cid:48))|.
The advantage of using local sensitivity is that we only
need to consider neighboring strings of σ which could result
in lower sensitivity of the function c, and consequently lower
noise added to the true answer c. Unfortunately, replacing
the global sensitivity with local sensitivity naively in the
Laplace mechanism (for instance) may not result in differential
privacy [9]. This drawback can be removed by using smooth
sensitivity [10] instead.
Deﬁnition 9 (Smooth Upper Bound). For b > 0, an b-smooth
∗
upper bound on LSσ, denoted SS
σ satisﬁes:
σ(c) ≥ LSσ(c), ∀σ ∈ Σ,
∗
SS
σ(c) ≤ ebSS
σ(cid:48)(c), ∀σ, σ(cid:48) ∈ Σ : d(σ, σ(cid:48)) = 1.
∗
∗
SS
Deﬁnition 10 (Smooth sensitivity). For b > 0, the b-smooth
sensitivity of c, denoted SSσ,b(c), at σ ∈ Σ is
LSσ(cid:48)(c) · e−bd(σ,σ
)(cid:111)
(cid:110)
.
(cid:48)
SSσ,b(c) = max
σ(cid:48)∈Σ
Note that smooth sensitivity is the smallest function to
satisfy the deﬁnition of a smooth upper bound [10]. Smooth
3
sensitivity allows us to add noise proportional to SSσ,b
to the
a
output of the function c to obtain (, δ)-differential privacy.
The choice of a and b depends on the privacy parameters and
the distribution used to generate noise [10].
C. Statistical Deﬁnitions
Deﬁnition 11 (p-Quantile). Let F be a cumulative distribution
function (CDF) of some continuous random variable X. The
p-quantile of F , denoted xp, is deﬁned as
xp = inf{x ∈ R : F (xp) = Pr(X ≤ xp) ≥ 1 − p}.
Fact 1. Let X be an exponentially distributed random vari-
able. Then its CDF is given by
(cid:26)1 − e−γx, x ≥ 0,
0,
x < 0.
H(x; γ) =
Let 0 ≤ p < 1. The quantile function of H is given as
H−1(p; γ) = − ln p
γ
.
(1)
Deﬁnition 12 (Light-tailed distribution). Let X be a random
variable with CDF F and let Y be an exponentially distributed