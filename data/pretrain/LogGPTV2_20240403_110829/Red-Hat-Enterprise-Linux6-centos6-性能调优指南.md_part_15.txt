在可能的情况下使用 fallocate 预分配文件和目录以便优化分配过程并避免锁定资源页。
尽量减小多节点间共享的文件系统区域以便尽量减小跨节点缓存失效并提高性能。例如：如果多个节点
挂载同一文件系统，但访问不同的子目录，则您可以通过将一个子目录移动到独立的文件系统中而获得
更好的性能。
选择可选资源组大小和数量。这要依赖传统文件大小以及系统中的可用空间，并可能在多个节点同时尝
试使用同一资源组时产生影响。资源组过多可延缓块分配，尽管已定位分配空间，而资源组过少也可在
取消分配时造成锁竞争。通常最好是测试多种配置以便确定您负载的最佳方案。
但竞争并不是可影响 GFS2 文件系统性能的唯一问题。其他可提高总体性能的最佳实践为：
根据集群节点的预期 I/O 模式和文件系统的性能要求选择存储硬件。
在可以减少查询时间的地方使用固态存储。
为您的负载创建适当大小的文件系统，并保证该文件系统不会超过容量的 80%。较小的文件系统的备份
时间会根据比例缩短，且需要较少时间和内存用于文件系统检查，但如果相对负载过小，则很有可能生
成高比例的碎片。
为频繁使用元数据的负载设定较大的日志，或者或者记录到日志中的数据正在使用中。虽然这样会使用
更多内存，但它可以提高性能，因为在写入前有必要提供更多可用日志空间以便存储数据。
请保证 GFS2 节点中的时钟同步以避免联网程序问题。我们建议您使用 NTP（网络时间协议）。
除非文件或者目录访问次数对您的程序操作至关重要，请使用 noatime 和 nodiratime 挂载选项。
注注意意
红帽强烈推荐您在 GFS2 中使用 noatime 选项。
如果您需要使用配额，请尝试减少配额同步传送的频率，或者使用模糊配额同步以便防止常规配额文件
更新中的性能问题。
注注意意
模糊配额计算可允许用户和组稍微超过其配额限制。要尽量减少此类问题，GFS2 会在用户或者
组接近其配额限制时动态减少同步周期。
有关 GFS2 性能调整各个方面的详情请参考 《全局文件系统 2 指南》，网址为
http://access.redhat.com/site/documentation/Red_Hat_Enterprise_Linux/。
58
第 8 章 联网
第第 8 章章 联联网网
随着时间的推移红帽企业版 Linux 的网络栈已有了大量自动优化功能。对于大多数负载，自动配置的网络设
定可提供优化的性能。
在大多数情况下联网性能问题是由硬件故障或者出错的基础设施造成的。这些原因不在本文档讨论范围。本
章所讨论的性能问题及解决方案对优化完全正常工作的系统有帮助。
联网是一个专用子系统，以敏感的连接包含不同部分。这是为什么开源社区以及红帽都致力于使用自动优化
网络性能的方式。因此，对于大多数负载，您根本不需要为性能重新配置联网设置。
8.1. 网网络络性性能能改改进进
红帽企业版 Linux 6.1 提供以下网络性能改进：
接接收收数数据据包包操操控控（（RPS））
RPS 启用单一 NIC rx 队列使其接收在几个 CPU 之间发布的 softirq 负载。这样可以帮助防止单一 NIC
硬件队列中的网络流量瓶颈。
要启用 RPS，请在 /sys/class/net/ethX/queues/rx-N/rps_cpus 中指定目标 CPU 名称，使用 NIC
的对映设备名称（例如 eth1, eth2）替换 ethX，使用指定的 NIC 接受队列替换 rx-N。这样可让在该文件
中指定的 CPU 处理 ethX 中 rx-N 队列中的数据。指定 CPU 时，请注意该队列的缓存亲和力 [4]。
接接收收流流程程操操控控
RFS 是 RPS 的延伸，可让管理员配置在程序接收数据并整合至网络栈中时自动填充的哈希表。这可决定哪
个程序接受网络数据（根据 source:destination 网络信息）。
使用此信息，网络栈可调度最佳 CPU 区接收每个数据包。要配置 RFS 请使用以下可调参数：
/proc/sys/net/core/rps_sock_flow_entries
这个参数控制内核可以操控的任意指定 CPU 可控制的最多栈/流程数。这是一个系统参数，有限共
享。
/sys/class/net/ethX/queues/rx-N/rps_flow_cnt
这个参数控制可操控某个 NIC（ethX）中指定接受队列（rx-N）的最大栈/流程数。注：所有 NIC
中这个参数的各个队列值之和应等于或者小于
/proc/sys/net/core/rps_sock_flow_entries。
与 RPS 不同，RFS 允许接收队列和程序在处理数据包流程时共享同一 CPU。这样可以在某些情况下改进性
能。但这种改进依赖类似缓存阶层、程序负载等因素。
TCP-thin 流流的的 getsockopt 支支持持
Thin-stream 是用来描述程序用来发送数据的传输协议的名词，在这种低速率下，协议的重新传输机制并未
完全饱和。使用 thin-stream 协议的程序通常使用可靠协议传输，比如 TCP。在大多数情况下此类程序提供
对时间敏感的服务（例如股票交易、在线游戏、控制系统）。
对时间敏感的服务，丢失数据包对服务质量是致命的。要放置此类情况出现，已将 getsockopt 调用改进
为支持两个附加选项：
TCP_THIN_DUPACK
59
红帽企业版 Linux 6 性能调节指南
这个布尔值在 thin stream 的一个 duupACK 后启用动态重新传输。
TCP_THIN_LINEAR_TIMEOUTS
这个布尔值为 thin stream 线性超时启用动态起动。
这两个选项都可由该程序特别激活。有关这些选项的详情请参考 file:///usr/share/doc/kernel-
doc-version/Documentation/networking/ip-sysctl.txt。有关 thin-stream 的详情请参考
file:///usr/share/doc/kernel-doc-version/Documentation/networking/tcp-
thin.txt。
传传输输代代理理服服务务器器（（TProxy））支支持持
内核现在可以处理非本地捆绑的 IPv4 TCP 以及 UDP 插槽以便支持传输代理服务器。要启用此功能，您将
需要配置相应的 iptables。您还需要启用并正确配置路由策略。
有关传输代理服务器的详情请参考 file:///usr/share/doc/kernel-
doc-version/Documentation/networking/tproxy.txt。
8.2. 优优化化的的网网络络设设置置
性能调节通常采用优先方式进行。通常我们会在运行程序或者部署系统前调整已知变量。如果调整不起作
用，则会尝试调整其他变量。此想法的逻辑是默认情况下，系统并不是以最佳性能水平运作；因此我们认
为需要相应对系统进行调整。在有些情况下我们根据计算推断进行调整。
如前所述，网络栈在很大程度上是自我优化的。另外，有效调整网络要求网络栈有深入的理解，而不仅仅值
直到网络栈是如何工作，同时还要直到具体系统的网络资源要求。错误的网络性能配置可能会导致性能下
降。
例如：缓存浮点问题。增加缓存队列深度可导致 TCP 连接的拥塞窗口比允许连接的窗口更大（由深层缓存
造成）。但那些连接还有超大 RTT 值，因为帧在队列中等待时间过长，从而导致次佳结果，因为它可能变
得根本无法探测到拥塞。
当讨论网络性能时，建议保留默认设置，除非具体的性能问题变得很明显。此类问题包括帧损失，流量极大
减少等等。即便如此，最佳解决方法通常是经过对问题的细致入微的研究，而不是简单地调整设置（增加缓
存/队列长度，减少中断延迟等等）。
要正确诊断网络性能问题请使用以下工具：
netstat
这是一个命令行程序可以输出网络连接、路由表、接口统计、伪连接以及多播成员。它可在
/proc/net/ 文件系统中查询关于联网子系统的信息。这些文件包括：
/proc/net/dev（设备信息）
/proc/net/tcp（TCP 插槽信息）
/proc/net/unix（Unix 域插槽信息）
有关 netstat 及其在 /proc/net/ 中的参考文件的详情请参考 netstat man page: man
netstat
dropwatch
监控内核丢失的数据包的监视器工具。有关详情请参考 A monitoring utility that monitors packets
60
插槽接收缓存大小
dropped by the kernel. For more information, refer to the dropwatch man page: man
dropwatch
ip
管理和监控路由、设备、策略路由及通道的工具。有关详情请参考 ip man page: man ip
ethtool
显示和更改 NIC 设置的工具。有关详情请参考 ethtool man page: man ethtool
/proc/net/snmp
显示 IP、ICMP、TCP 以及 UDP 根据 snmp 代理管理信息所需 ASCII 数据的文件。它还显示实时
UDP-lite 统计数据。
《SystemTap 初学者指南》中包含一些示例脚本，您可以用来概括和监控网络性能。您可在
http://access.redhat.com/site/documentation/Red_Hat_Enterprise_Linux/ 找到本指南。
收集完网络性能问题的相关数据后，您就可以形成一个理论 —，同时也希望能有一个解决方案。 [5]例如：
在 /proc/net/snmp 中增加 UDP 输入错误表示当网络栈尝试将新帧排入程序插槽时，一个或者多个插槽
接受队列已满。
这代表数据包至少在一个插槽队列中被瓶颈，就是说插槽队列输送数据包的速度太慢，或者对于该插槽队列
该数据包过大。如果是后者，那么可验证任意依赖网络程序的日志查看丢失的数据以便解决这个问题，您应
该需要优化或者重新配置受到影响的程序。
插插槽槽接接收收缓缓存存大大小小
插槽发送和接收大小都是动态调节的，因此基本不需要手动编辑。如果进一步分析，比如 SystemTap 网络
示例中演示的分析，sk_stream_wait_memory.stp 认为该插槽队列的排放速度过慢，那么您可以增大
该程序插槽队列深度。要做到这一点，请增大插槽接收缓存，方法是配置以下值之一：
rmem_default
控制插槽使用的接收缓存默认大小的内核参数。要配置此参数，请运行以下命令：
sysctl -w net.core.rmem_default=N
使用所需缓存大小以字节为单位替换 N。要确定这个内核参数值请查看
/proc/sys/net/core/rmem_default。请记住 rmem_default 值不得大于
rmem_max）；如果需要请增大 rmem_max 值。
SO_RCVBUF
控制插槽接收缓存最大值的插槽选项，单位为字节。有关 SO_RCVBUF 的详情请参考其 man
page：man 7 socket。
要配置 SO_RCVBUF，请使用 setsockopt 工具，您可以使用 getsockopt 查询当前
SO_RCVBUF 值。有关这两个工具的详情请参考 setsockopt man page: man setsockopt。
61
红帽企业版 Linux 6 性能调节指南
8.3. 数数据据包包接接收收概概述述
为更好地分析网络瓶颈和性能问题，您需要直到数据包接收的原理。数据包接收对网络性能调节来说很重
要，因为接收路径是经常会丢帧的地方。在接收路径丢帧可能会造成对网络性能的极大负面影响。
图图 8.1. 网网络络接接收收路路径径图图表表