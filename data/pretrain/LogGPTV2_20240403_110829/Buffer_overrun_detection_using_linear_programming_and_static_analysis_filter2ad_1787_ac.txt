4. CONSTRAINT RESOLUTION USING LIN-
EAR PROGRAMMING
This section describes two solvers based on linear programming
that the tool uses to solve the set of generated constraints. We chose
to use linear programming for several reasons:
• The use of linear programming allows us to model arbitrary lin-
ear constraints. Hence, our solver automatically evolves to han-
dle new kinds of constraints. Other tools [29, 30, 33] use special-
ized solvers – generation of new kinds of constraints will mean that
these solvers have to be specially adapted to deal with them.
• Commercial implementations of linear program solvers are known
to scale efﬁciently to millions of constraints.
• The use of a well developed theory helped us easily reason about
the correctness of our solvers.
• Finally, we are currently working on the use of the dual of the
linear program for diagnostic information. In particular, we are in-
vestigating how the dual linear program can be used to produce a
program path that leads to the statement that causes the overﬂow.
Such information is valuable since it tells the user of the tool why
there was an overrun.
4.1 Overview of the solver
A Linear Program is an optimization problem that is expressed
as follows:
Minimize : cTx
Subject To : Ax ≥ b
where A is an m × n matrix of constants, b and c are vectors of
constants, and x is a vector of variables. This is equivalent to saying
that we have a system of m inequalities in n variables, and are
required to ﬁnd values for the variables such that all the constraints
in the system are satisﬁed and the objective function cT x takes its
lowest possible value. It is important to note that the above form
is just one of the numerous ways in which a linear program can be
expressed. For a more comprehensive view of linear programming,
see [27]. Linear programming works on ﬁnite real numbers; that
is, the variables in the vector x are only allowed to take ﬁnite real
values. Hence the optimum value of the objective function, if it
exists, is always guaranteed to be ﬁnite.
Linear programming is well studied in the literature, and there
are well-known techniques to solve linear programs, Simplex [12,
27] being the most popular of them. Other known techniques, such
interior point methods [31] work in polynomial time. Commer-
cially available solvers for solving linear programs, such as SoPlex
[32] and CPLEX [25] implement these and related methods.
The set of constraints that we obtained after program analysis
are linear constraints, hence we can formulate our problem as a lin-
ear program. Our goal is to obtain the values for buf!alloc!min,
buf!alloc!max, buf!used!min and buf!used!max that yield
the tightest possible ranges for the number of bytes allocated and
used by the buffer pointed to by buf in such a way that all the con-
straints are satisﬁed. Formally, we are interested in ﬁnding the low-
est possible values of buf!alloc!max and buf!used!max, and
the highest possible values of buf!alloc!min and buf!used!min
subject to the set of constraints. We can obtain the desired bounds
for each buffer buf by solving four linear programs, each with the
same constraints but with different objective functions:
Minimize: buf!alloc!max
Maximize: buf!alloc!min
Minimize: buf!used!max
Maximize: buf!used!min
However, it can be shown (the proof is beyond the scope of this
paper) that for the kind of constraints generated by the tool, if all
max variables have ﬁnite lower bounds, and all min variables have
ﬁnite upper bounds, then the values obtained by solving the four
linear programs as above are also the values that optimize the linear
program with the same set of constraints subject to the objective
function:
Minimize: buf (buf!alloc!max - buf!alloc!min
+ buf!used!max - buf!used!min)
Note that this objective function combines the constraint vari-
ables across all buffers. Since taint analysis ensures that all max
variables have ﬁnite lower bounds and all min variables have ﬁnite
upper bounds, we can solve just one linear program, and obtain the
bounds for all buffers.
It must be noted that we are actually interested in obtaining inte-
ger values for buf!alloc!max, buf!used!max, buf!alloc!min
and buf!used!min. The problem of ﬁnding integer solutions to a
linear program is called Integer Linear Programming and is a well
known NP-complete problem [12]. Our approach is thus an ap-
proximation to the real problem of ﬁnding integer solutions that
satisfy the constraints.
4.2 Handling Infeasible Linear Programs
While at ﬁrst glance the method seems to give the desired buffer
bounds, it does not work for all cases. In particular, an optimal so-
lution to a linear program need not even exist. We describe brieﬂy
the problems faced when using a linear programming based ap-
proach for determining the buffer bounds. A linear program is said
to be feasible if one can ﬁnd ﬁnite values for all the variables such
that all the constraints are satisﬁed. For a linear program in n vari-
n and is called a feasible
ables, such an assignment is a vector in R
solution to the linear program. A feasible solution is said to be op-
timal if it also maximizes (or minimizes) the value of the objective
function. A linear program is said to be unbounded if a feasible so-
lution exists, but no solution optimizes the objective function. For
instance, consider:
Maximize : x
Subject To : x ≥ 5
Any value of x ≥ 5 is a feasible solution to the above linear pro-
gram, but no ﬁnite value x ∈ R optimizes the objective function.
Finally, a linear program is said to be infeasible if it has no feasible
solutions. An example of an infeasible linear program is shown in
Figure 5.
Minimize : counter!max
Subject To : counter(cid:1)!max ≥ counter!max + 1
counter!max ≥ counter(cid:1) !max
Figure 5: An Infeasible Linear Program
In our formulation, if a linear program has an optimal solution,
we can use that value as the buffer bound. None of the linear pro-
grams in our case can be unbounded, since the constraints have
been examined by the taint analyzer to ensure that all max variables
have ﬁnite lower bounds. We minimize for the max variables in the
objective function, and since all the max variables have ﬁnite lower
bounds, the lowest value that each max variable can obtain is also
ﬁnite. Similarly, all min variables have ﬁnite upper bounds, and so
when we maximize the min variables, the highest values that they
could obtain are also ﬁnite. Hence taint analysis is an essential step
to ensure that our approach works correctly.
However, when the linear program is infeasible, we cannot as-
sign any ﬁnite values to the variables to get a feasible solution. As
a result, we cannot obtain the values for the buffer bounds. In such
a case, a safe option would be to set all max variables to ∞ and
min variables to -∞, but that information would be virtually use-
less to the user of the tool because there would be too many false
alarms. The linear program may be infeasible due to a small sub-
set of constraints; in such a scenario, setting all variables to inﬁnite
values will be overly conservative. For instance, the constraints in
Figure 2 are infeasible because of the constraints generated for the
statement counter++.
We have developed an approach in which we try to remove a
“small” subset of the original set of constraints so that the resultant
constraint system is feasible. In fact, the problem of “correcting”
infeasible linear programs to make them feasible is a well studied
problem in operations research. The approach is to identify Irre-
ducibly Inconsistent Sets (called IIS) [9]. An IIS is a minimal set of
inconsistent constraints, i.e., the constraints in the IIS together are
infeasible, but any subset of constraints in the IIS form a feasible
set. For instance, both the constraints in the linear program in Fig-
ure 5 constitute an IIS because the removal of any one of the two
constraints makes the linear program feasible. There are several ef-
ﬁcient algorithms available to detect IISs in a set of constraints. We
used the Elastic Filtering algorithm [9]. The Elastic Filtering Algo-
rithm takes as input a set of linear constraints and identiﬁes an IIS
in these constraints (if one exists). An infeasible linear program
may have more than one IISs in it, and the elastic ﬁltering algo-
rithm is guaranteed to ﬁnd at least one of these IISs. To produce a
feasible linear program from an infeasible linear program, we may
be required to run the elastic ﬁltering algorithm several times; each
run identiﬁes and removes an IIS and produces a smaller linear pro-
gram which can further be examined for presence of IISs.
Figure 6 pictorially shows our approach to obtain a set of fea-
sible linear constraints from a set of infeasible linear constraints.
We ﬁrst examine the input set, depicted as C, to ﬁnd out if it is
feasible; if so, it does not contain IISs, and C can be used as the
set of constraints in our linear program formulation. If the C turns
out to be infeasible, then it means that there is a subset of C that
forms one or more IISs. This subset is depicted as C(cid:1)
in the ﬁg-
ure. The elastic ﬁltering algorithm, over several runs, identiﬁes and
C
Elastic
Filtering
Taint
Analysis
D
C − C’
C’’
C’
The set C of constraints.
C’ denotes a set of IISs
Removal of C’ results in
a set C’’ tainted by C’
The set D obtained 
removing C’’
Figure 6: Making an Infeasible set of constraints amenable to Linear Programming
removes the subset C(cid:1)
from the set of constraints. The resultant
set C − C(cid:1)
is feasible. We then set the values of the max and min
to ∞ and -∞ respectively. We do so
variables appearing in C(cid:1)
because we cannot infer the values of these variables using linear
programming, and hence setting these variables to inﬁnite values is
a conservative approach. These variables whose values are inﬁnite
may appear in the set of constraints C − C(cid:1)
. The scenario is now
similar to taint analysis, where we had some constraint variables
whose values were inﬁnite, and we had to identify and remove the
constraint variables that were “tainted” by the inﬁnite variables.
of C − C(cid:1)
Therefore, we apply the taint analysis algorithm to identify the
tainted variables, and remove the constraints in which they appear.
This step results in further removal of constraints, which are de-
picted in the Figure 6 by a subset C(cid:1)(cid:1)
. The set of con-
straints after removal of C(cid:1)(cid:1)
, denoted as D in Figure 6, satisﬁes
the property that all max variables appearing in it have ﬁnite lower
bounds, and all min variables have ﬁnite upper bounds. Moreover,
D is feasible, and will only yield optimal solutions when solved
as a linear program with the objective functions described earlier.
Hence, we solve the linear program using the set of constraints in
D. This algorithm is presented in detail in [19].
We have implemented this approach by extending the commer-
cially available package SoPlex [32]. SoPlex is a linear program
solver; we extended it by adding IIS detection and taint analysis.
In practice, linear program solvers work much faster when the con-
straints have been presolved. Presolving is a method by which con-
straints are simpliﬁed before being passed to the solver. Several
such techniques are described in the literature [7]; we have incor-
porated some of them in our solver.
4.3 Solving Constraints Hierarchically
While the approach presented above is fast, it is an approxima-
tion algorithm. In particular, the algorithm may remove more con-
straints than are actually required to make the constraints feasible.
As a result, more constraint variables may be set to the values ∞
or -∞. To address this imprecision, we have designed an imple-
mented a hierarchical solver. The idea behind this solver is to
decompose the set of constraints into smaller subsets, and solve
each subset separately. We do so by constructing a directed acyclic
graph (DAG), each of whose vertices represents a set of constraints.
Moreover, each constraint is associated with exactly one vertex of
the DAG. The DAG is constructed by deﬁning a notion of “depen-
dency” between a pair of constraints (see [19]). The topological
order of the DAG naturally deﬁnes a hierarchy of the vertices. The
set of constraints corresponding to each vertex is then solved using
linear programming. It can be shown that this approach is math-
ematically precise in that it sets fewest number of constraint vari-
ables to ∞ or -∞, and produces precise ranges. We have omitted
the details due to space considerations, consult [19] for details.
5. ADDING CONTEXT SENSITIVITY
The constraint generation process described in Section 3 was
context-insensitive. When we generated the constraints for a func-
tion, we considered each call-site as an assignment of the actual-in
variables to the formal-in variables, and the return from the func-
tion as an assignment of the formal-out variables to the actual-out
variables. As a result, we merged information across call-sites,
thus making the analysis imprecise.
In this section we describe
two techniques to incorporate context sensitivity.
Constraint inlining is similar in spirit to inlining function bodies
at call-sites. Observe that in the context-insensitive approach, we
lost precision because we treated different call-sites to a function
identically, i.e, by assigning the actual-in variables at each call-site
to the same formal parameter.
Constraint inlining alleviates this problem by creating a fresh
instance of the constraints of the called function at each call-site.
At each call-site to a function, we produce the constraints for the
called function with the local variables and formal parameters re-
named uniquely for that call-site. This is illustrated in the example
below, which shows some of the constraints for copy buffer from
Figure 2 specialized for the call-site at line (7):
copy!alloc!max1 ≥ buffer!used!max1 - 1
copy!used!max1 ≥ buffer!used!max1
copy!used!min1 ≤ buffer!used!min1
copy buffer$return!used!max1 ≥ copy!used!max1
copy buffer$return!used!min1 ≤ copy!used!min1
Context-sensitivity can be obtained by modeling each call-site to
the function as a set of assignments to the renamed instances of
the formal variables. The actual-in variables are assigned to the re-
named formal-in variables, and the renamed formal-out variables
are assigned to the actual-out variables. As a result, there is exactly
one assignment to each renamed formal-in parameter of the func-
tion, which alleviates the problem of merging information across
different calls to the same function.