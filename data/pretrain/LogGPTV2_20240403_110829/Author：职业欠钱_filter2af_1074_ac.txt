讨论影响入侵检测的要素时，我们可以简单看看曾经发生过哪些错误导致我们不能主动发现入侵（这里的每一条，背后可能都是一个令人遗憾的真实漏报case）：
  1. 依赖于主机agent采集数据的模型，在当事机器上，没部署安装/agent挂了/数据上报过程丢失了/Bug了
  2. 后台数据分析模块故障（丢弃数据）
  3. 策略脚本Bug、没启动
  4. 还没建设对应的策略
  5. 策略的灵敏度不够（比如扫描的阈值没达到，WebShell用了变形的对抗手法）
  6. 模型依赖的部分基础数据错误，做出了错误的判断
  7. 成功告警了，但是工单应急同学错误的判断/没有跟进/辅助信息不足以定性
所以，实际上，要让一个入侵事件被捕获，我们需要有专门的运营人员对以下目标负责：
  1. 数据采集的完整性
  2. 每一个策略时刻工作正常（拨测监控）
  3. 针对高危场景策略要覆盖，灵敏度要满足一般对抗需要
  4. 依赖的基础数据要准确
  5. 工单运营支撑平台及追溯辅助工具完备
可能有些同学会想，影响入侵检测的关键要素难道不是模型的有效性么？怎么全是这些乱七八糟的东西？
实际上，稍微上规模的企业，上述的每一点要长期维持在高可用标准，都非常不容易。比如懂攻防的策略同学，对基础数据质量不关心不负责，最终的效果就是明明能发现的入侵，总是有各种原因恰好发现不了。之前，笔者亲历过有大量的案例，明明对手很菜，手法很简单，但就是因为这些因素给漏过了。
所以，我常感慨，以某些运营质量之差，根本轮不到跟黑客拼策略（技术）。
当然，一旦有兄弟帮忙去跟进这些质量运营工作之后，我们的确就真的需要拼策略了。
这个时候，攻击手法有那么多，凭什么先选择这个场景建设？凭什么认为建设到这个程度就足够满足对已知手法的感知了？凭什么选择发现这些样本而放弃那些样本？
这些极具主观性的东西，往往考验的是判断力、执行力等专业度，不能等到黑客入侵了才解释说，这个场景我们原定明年建设的……
### 如何发现APT
所谓APT，就是高级的持续威胁。既然是高级的，按照一般的描述，他们的木马是免杀的（不能假定我们可以发现这个木马）、他们的漏洞不公开的（不能假定我们可以加固抵抗）、他们的手法是高级的（不能假定这个手法在已知的范畴里）。
所以，实际上APT的意思就几乎等同于我们不能发现的入侵事件了。
但是，业界总还有APT检测产品、解决方案的厂商在混饭吃，他们是怎么做的呢？
说木马免杀的，他们用沙箱+人工分析，哪怕效率低一些，还是试图做出定性，并快速的把IOC（威胁情报）同步给其它客户，发现1例，全网都去排查。
说流量变形对抗的，他们用异常检测的模型，把一些不认识的可疑的IP关系、payload给识别出来 —— 当然，识别出来之后，也要运营人员跟进得仔细才能定性。
说攻击手法高级的，他们还是会假定黑客就用鱼叉、水坑之类的已知手法去执行，然后在邮箱附件、PC终端等环节采集日志，对用户行为进行分析，UEBA试图寻找出用户异于平常的动作。
那么，我们呢？
我没有什么好的办法，可以发现传说中的免杀的木马，但是我们可以针对已知的黑客攻击框架（比如metasploit、cobalt
strike）生成的木马类型、行为进行一些特征的提取，比如DNS隧道的通讯，比如IP信誉的模型，比如默认生成的不免杀木马的共性特征等。
我们可以假设已经有黑客控制了某一台机器，但是它试图进行横向扩散的时候，我们有一些模型可以识别它的探测、翻找、入侵尝试等行为。
我们暂时还不知道如何100%发现APT，但是如果真的有APT在公司里，有本事这个团队别犯错，永远都不触碰我们所有的铃铛。否则，只要他犯错，就轮到我们出场了。
前面所有的高标准，包括高覆盖、低误报，必须跟进到底，都是在等待这一刻。因此，我们坚持住，即使听过无数次“狼来了”，下一次仍然必须用最高的敬畏心去对待新的告警。
AI在入侵检测领域的正确姿势
最近这2年，不谈AI故事就不会完整。
只不过，随着AI概念的火爆，很多人已经把数据挖掘、统计分析的一些说法，比如分类、预测、聚类、关联之类的算法，改名字叫AI了。
入侵检测本质上是对数据做标记（labeling）解决方式上，可以分为分类（classify），或者聚类（cluster），区别是已有的数据是否有标签。入侵检测领域，多数没有动辄上百万的样本的可供模型去训练，也就是无法使用数据来刻画特征。
此时，安全领域一个比较常见的现象是，将场景转变成标记问题，要难过于通过数学模型把标记的解给求出来。也就是要业务专家先行，算法专家再跟上，而不能直接让算法专家闭门造车。
所以，针对一个具体的攻击场景，怎么样采集对应的入侵数据，思考这个入侵动作和正常人的区别，这个特征的提取过程，往往决定了模型最终的效果。特征决定了效果的上限，而算法模型决定了多接近这个上限。
如果有一个纯粹的AI团队，上来不关注攻击具体场景就用这些算法对已知样本进行训练和建模，是不可能有好的结果的。入侵检测的同学，和AI的同学，必须形成一种相互合作而不是单方面觉得高人一等的关系，才可能做出有实用价值的结果。
此前，笔者曾见过一个案例，AI团队产出了一个实验室环境效果极佳，但是在实际环境里却不如人意的Webshell模型。这个项目的诞生，是源自该团队试图做一个AI模型来跟传统的Webshell模型做效果对比
——
都是在文本静态分析方面去做检测，即便AI在实验室环境的效果再好，也仍旧有漏报，而且，原团队所放弃的抵抗，也由RASP弥补过了，于是该项目事实上并未产生应有的价值。
这个例子并非说该团队不优秀，而是压根就不该让AI的同学去独自承担整个压力，甚至不推荐“使用AI做一个模型吧，看看是否比传统的好”这种想法，
我个人认为，业务同学在思考场景短板后，跟算法同学共同商议，将AI用于业务的某一个环节，而非负责整个场景，或许，是更合适的思路。
入侵发现的运营陷阱
入侵检测是一个苦逼的工作，任何时候手机一响，都要最紧张的去跟进 —— 又一次“狼来了”？还是一个APT高手团队唯一的破绽暴露了？
在内部的一个Talk上，我把这个团队称为“守夜人” ——
从今日起，日日夜夜，至死方休。没有崇高的使命感，纯粹是为了做一份工作，很难有安全技术人才可以熬得下去。别的工作都可以下班，但我们never get off
。
所以，借用Google的Detection团队的招聘广告里的说明，我想，应该也是我们的心声：
Our goal is to build a world-class fully automated detection and response
machine - an automated SOC.
世界级的，全自动的SOC。
能让机器和程序自动识别的，把误报降低到最低的入侵检测效果，可能才是我们想要的生活。
有时候，为了快速覆盖一个入侵场景，简陋的发布了各种临时策略、临时代码架构、临时DB，随着数据量规模的增长开始变得奄奄一息。老算法的粗暴、简陋，也逐渐显得不合时宜。
新场景的预研、开发、建设周期需要很长，支撑平台不够完善，在定性或者追溯或者预研一些模型时，无法高效实现，甚至导致策略建设使用各种变通手法来实现一个简单的想法……
这些，都是入侵检测运营的一部分，也是阻碍我们达到终极目标的困难。
把“入侵检测能力”当作一个产品，像创业公司一样，快速发布，具备能力，先发现一些“粗糙”、“明显”的事件，可能是第一步。而随着业务发展，自己的产品和平台也一样要在高速公路上奔跑的汽车那样换轮胎，一轮又一轮重构，分工逐渐明细，在质量上精益求精，不断打磨。
这些事情需要公司的大力支持，因此，仅仅是通过忙碌的工作感动自己，而无法有效的输出工作本身的价值，获取公司的认同，其实是入侵检测运营工作的最大陷阱。