title:Dcell: a scalable and fault-tolerant network structure for data centers
author:Chuanxiong Guo and
Haitao Wu and
Kun Tan and
Lei Shi and
Yongguang Zhang and
Songwu Lu
DCell: A Scalable and Fault-Tolerant Network
Structure for Data Centers
Chuanxiong Guo, Haitao Wu, Kun Tan, Lei Shi†, Yongguang Zhang, Songwu Lu‡ ∗
Microsoft Research Asia, †Tsinghua University, ‡UCLA
{chguo, hwu, kuntan, ygz}@microsoft.com,
†PI:EMAIL, ‡PI:EMAIL
ABSTRACT
A fundamental challenge in data center networking is how
to eﬃciently interconnect an exponentially increasing num-
ber of servers. This paper presents DCell, a novel network
structure that has many desirable features for data cen-
ter networking. DCell is a recursively deﬁned structure, in
which a high-level DCell is constructed from many low-level
DCells and DCells at the same level are fully connected with
one another. DCell scales doubly exponentially as the node
degree increases. DCell is fault tolerant since it does not
have single point of failure and its distributed fault-tolerant
routing protocol performs near shortest-path routing even
in the presence of severe link or node failures. DCell also
provides higher network capacity than the traditional tree-
based structure for various types of services. Furthermore,
DCell can be incrementally expanded and a partial DCell
provides the same appealing features. Results from theoret-
ical analysis, simulations, and experiments show that DCell
is a viable interconnection structure for data centers.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network topol-
ogy, Packet-switching networks
General Terms
Algorithms, Design
Keywords
Data center, Network topology, Throughput, Fault tolerance
1.
INTRODUCTION
In recent years, many large data centers are being built
to provide increasingly popular online application services,
such as search, e-mails, IMs, web 2.0, and gaming, etc. In
addition, these data centers also host infrastructure services
∗This work was performed when Lei Shi was an intern and
Songwu Lu was a visiting professor at Microsoft Research
Asia.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
such as distributed ﬁle systems (e.g., GFS [8]), structured
storage (e.g., BigTable [7]), and distributed execution engine
(e.g., MapReduce [5] and Dryad [11]). In this work, we focus
on the networking infrastructure inside a data center, which
connects a large number of servers via high-speed links and
switches. We call it data center networking (DCN).
There are three design goals for DCN. First, the network
infrastructure must be scalable to a large number of servers
and allow for incremental expansion. Second, DCN must
be fault tolerant against various types of server failures, link
outages, or server-rack failures. Third, DCN must be able to
provide high network capacity to better support bandwidth-
hungry services.
Two observations motivate these goals. First, data center
is growing large and the number of servers is increasing at
an exponential rate. For example, Google has already had
more than 450,000 servers in its thirty data centers by 2006
[2, 9], and Microsoft and Yahoo! have hundreds of thou-
sands of servers in their data centers [4, 19]. Microsoft is
even doubling the number of servers every 14 months, ex-
ceeding Moore’s Law [22]. Second, many infrastructure ser-
vices request for higher bandwidth due to operations such
as ﬁle replications in GFS and all-to-all communications in
MapReduce. Therefore, network bandwidth is often a scarce
resource [5]. The current DCN practice is to connect all the
servers using a tree hierarchy of switches, core-switches or
core-routers. With this solution it is increasingly diﬃcult to
meet the above three design goals. It is thus desirable to
have a new network structure that can fundamentally ad-
dress these issues in both its physical network infrastructure
and its protocol design.
To meet these goals we propose a novel network structure
called DCell. DCell uses a recursively-deﬁned structure to
interconnect servers. Each server connects to diﬀerent lev-
els of DCells via multiple links. We build high-level DCells
recursively from many low-level ones, in a way that the low-
level DCells form a fully-connected graph. Due to its struc-
ture, DCell uses only mini-switches to scale out instead of
using high-end switches to scale up, and it scales doubly ex-
ponentially with the server node degree. In practice, a DCell
with a small degree (say, 4) can support as many as several
millions of servers without using expensive core-switches or
core-routers.
DCell is fault tolerant. There is no single point of failure
in DCell, and DCell addresses various failures at link, server,
and server-rack levels. Fault tolerance comes from both its
rich physical connectivity and the distributed fault-tolerant
routing protocol operating over the physical structure.
DCell supports high network capacity. Network traﬃc in
DCell is distributed quite evenly among servers and across
links at a server. High-level links in the hierarchy will not
pose as the bottleneck, which is the case for a tree-based
structure. Our experimental results on a 20-server DCell
testbed further show that DCell provides 2 times through-
put compared with the conventional tree-based structure for
MapReduce traﬃc patterns.
In summary, we have proposed a new type of physical net-
work infrastructure that possesses three desirable features
for DCN. This is the main contribution of this work, and has
been conﬁrmed by simulations and experiments. A potential
downside of our solution is that DCell trades-oﬀ the expen-
sive core switches/routers with higher wiring cost, since it
uses more and longer communication links compared with
the tree-based structures. However, we believe this cost is
well justiﬁed by its scaling and fault tolerance features.
The rest of the paper is organized as follows. Section 2
elaborates on design issues in DCN. Section 3 describes the
DCell structure. Sections 4 and 5 present DCell routing and
the solution to incremental expansion, respectively. Sections
6 and 7 use both simulations and implementations to evalu-
ate DCell. Section 8 compares DCell with the related work.
Section 9 concludes the paper.
2. DATA CENTER NETWORKING
Data centers today use commodity-class computers and
switches instead of specially designed high-end servers and
interconnects for better price-performance ratio [3]. The
current DCN practice is to use the switch-based tree struc-
ture to interconnect the increasing number of servers. At
the lowest level of the tree, servers are placed in a rack (typ-
ically 20-80 servers) and are connected to a rack switch. At
the next higher level, server racks are connected using core
switches, each of which connects up to a few hundred server
racks. A two-level tree can thus support a few thousand
servers. To sustain the exponential growth of server popula-
tion, more high levels are added, which again use even more
expensive, higher-speed switches.
The tree-based structure does not scale well for two rea-
sons. First, the servers are typically in a single layer-2
broadcast domain. Second, core switches, as well as the
rack switches, pose as the bandwidth bottlenecks. The tree
structure is also vulnerable to “single-point-of-failure”: a
core switch failure may tear down thousands of servers. A
quick ﬁx using redundant switches may alleviate the prob-
lem, but does not solve the problem because of inherently
low connectivity.
To address DCN issues, it seems that we may simply re-
use certain structures proposed in the area of parallel com-
puting. These structures connect components such as mem-
ory and CPU of a super computer, and include Mesh, Torus,
Hypercube, Fat Tree, Butterﬂy, and de Bruijn graph [12].
However, they addressed a diﬀerent set of issues such as low-
latency message passing in the parallel computing context
and cannot meet the goals in DCN. We will provide detailed
comparisons of these structures and our work in Section 8.
We now elaborate on the three design goals we have brieﬂy
mentioned in the previous section.
Scaling: Scaling requirement in DCN has three aspects.
First, the physical structure has to be scalable.
It must
physically interconnect hundreds of thousands or even mil-
lions of servers at small cost, such as a small number of links
at each node and no dependence on high-end switches to
scale up. Second, it has to enable incremental expansion by
adding more servers into the already operational structure.
When new servers are added, the existing running servers
should not be aﬀected. Third, the protocol design such as
routing also has to scale.
Fault tolerance: Failures are quite common in current
data centers [3, 8]. There are various server, link, switch,
rack failures due to hardware, software, and power outage
problems. As the network size grows, individual server and
switch failures may become the norm rather than excep-
tion. Fault tolerance in DCN requests for both redundancy
in physical connectivity and robust mechanisms in protocol
design.
High network capacity: Many online infrastructure ser-
vices need large amount of network bandwidth to deliver
satisfactory runtime performance. Using the distributed ﬁle
system [8] as an example, a ﬁle is typically replicated several
times to improve reliability. When a server disk fails, re-
replication is performed. File replication and re-replication
are two representative, bandwidth-demanding one-to-many
and many-to-one operations. Another application example
requiring high bandwidth is MapReduce [5]. In its Reduce
operation phase, a Reduce worker needs to fetch interme-
diate ﬁles from many servers. The traﬃc generated by the
Reduce workers forms an all-to-all communication pattern,
thus requesting for high network capacity from DCN.
3. THE DCELL NETWORK STRUCTURE
The DCell-based DCN solution has four components that
work in concert to address the three challenges. They are the
DCell scalable network structure, eﬃcient and distributed
routing algorithm that exploits the DCell structure, fault-
tolerant routing that addresses various types of failures such
as link/server/rack failures, and an incremental upgrade scheme
that allows for gradual expansion of the DCN size. Sections
3-5 describe these components in details.
3.1 DCell Physical Structure
DCell uses servers equipped with multiple network ports
and mini-switches to construct its recursively deﬁned archi-
tecture.
In DCell, a server is connected to several other
servers and a mini-switch via communication links, which
are assumed to be bidirectional. A high-level DCell is con-
structed from low-level DCells. We use DCellk (k ≥ 0) to
denote a level-k DCell. The following example (in Figure 1)
illustrates how DCells of diﬀerent levels are constructed.
DCell0 is the building block to construct larger DCells. It
has n servers and a mini-switch (n = 4 for DCell0 in Figure
1). All servers in DCell0 are connected to the mini-switch.
In our design, n is a small integer (say, n ≤ 8). Therefore,
a commodity 8-port switch with 1Gb/s or 10Gb/s per port
could serve the purpose.
A level-1 DCell1 is constructed using n + 1 DCell0s. In
DCell1, each DCell0 is connected to all the other DCell0s
with one link. In the example of Figure 1, DCell1 has n+1 =
5 DCell0s. DCell connects the 5 DCell0s as follows. Assign
each server a 2-tuple [a1, a0], where a1 and a0 are the level-
1 and level-0 IDs, respectively. Thus a1 and a0 take values
from [0,5) and [0,4), respectively. Then two servers with 2-
tuples [i, j − 1] and [j, i] are connected with a link for every
i and every j > i. The linking result for DCell1 is shown
/* pref is the network preﬁx of DCelll
l stands for the level of DCelll
n is the number of nodes in a DCell0*/
BuildDCells(pref , n, l)
Part I:
if (l == 0) /*build DCell0*/
for (int i = 0; i  0, where n is the number of servers in a DCell0.
Theorem 1 shows that, the number of servers in a DCell
scales doubly exponentially as the node degree increases. A
small node degree can lead to a large network size. For
example, when k = 3 and n = 6, a DCell can have as many
as 3.26-million servers!
Bisection width denotes the minimal number of links to
be removed to partition a network into two parts of equal
size. A large bisection width implies high network capacity
and a more resilient structure against failures. DCell has
the following lower bound to its bisection width.
Figure 1: A DCell1 network when n=4. It is com-
posed of 5 DCell0 networks. When we consider each
DCell0 as a virtual node, these virtual nodes then
form a complete graph.
in Figure 1. Each server has two links in DCell1. One
connects to its mini-switch, hence to other nodes within its
own DCell0. The other connects to a server in another
DCell0.
In DCell1, each DCell0, if treated as a virtual node, is