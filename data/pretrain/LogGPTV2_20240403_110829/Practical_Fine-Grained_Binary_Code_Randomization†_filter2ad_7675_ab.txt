adjacent to a return address even after the attacker leaks
that return address. However, if the attacker leaks a code
address from EH-metadata, PHR cannot prevent attackers
from knowing the locations of nearby gadgets.
• ZJR and BBR don’t address indirect disclosures per se, but
they do have a secondary effect since they chop and permute
function bodies. This means that a leaked return address
exposes the location of instructions in the same code block,
but the gadgets in other blocks within the caller are still
unknown to the attacker. Unfortunately, ZJR blocks can be
large. In the SPEC suite, we observed thousands of blocks
consisting of hundreds of instructions. Although BBR blocks
tend to be small on average, there are still over a hundred
blocks containing hundreds of instructions. In fact, we find
a basic block that is 8KB-long! As a result, a leaked code
address can allow an attacker to compute a large number of
additional gadgets.
• PBs also don’t address indirect disclosures, so kR(cid:98)X [40] relies
on OPHR for this purpose. Being a weaker (but faster) form of
PHR, OPHR shares the weaknesses of PHR, i.e., no protection
is offered for addresses disclosed in the EH-metadata. In
addition, OPHR shares a drawback of ZJR: that the number
of large OPHR blocks is comparable to that of ZJR.
In contrast, we introduce a new technique, called Length Limiting
Randomization (LLR(k)), which limits the utility of any disclosed
code address. The basic idea behind LLR(k) is very simple. Let s
be the size of a function. We generate p = ⌊s/k⌋ distinct random
numbers r1, ...rp over the range [1, s −1]. We then proceed to create
a partition at each ri. Since the number of partitions is p + 1, the
average partition size is s/(p + 1) = s/(⌊s/k⌋ + 1) ≤ s/(s/k) = k.
Despite its simplicity, LLR(k) is quite powerful, and offers several
benefits over previous techniques:
• Tunable entropy and performance: Small values of k mean a
large number of small blocks. This increases entropy, but de-
creases performance because frequent jumps increase code
size, while also decreasing cache locality. By the same rea-
soning, larger k values provide better performance while
decreasing entropy.
• Bounded utility for any disclosed address. Since the expected
length of any contiguous block of code is k, an attacker that
discloses an address can expect to be able to guess the lo-
cations of up to k adjacent instructions. To access gadgets
beyond this range, the attacker will have to disclose addi-
tional addresses2.
• Higher entropy than other techniques for the same number of
partitions. For a given average partition size, LLR(k) provides
much higher entropy as compared to other schemes such
as ZJR or BBR. For instance, consider a function of size 100
instructions, and let the average block size be 10. For this
block size, both ZJR and BBR yield an entropy of 22 bits,
while LLR(k) yields 66 bits of entropy! This is because LLR(k)
introduces a lot of additional randomness in the placement of
breaks, whereas the placement is deterministic for all other
schemes discussed above.
• Can be seamlessly combined with other randomizations. We
can start with a base randomization scheme, such as ZJR,
BBR, PHR, or OPHR, and introduce additional randomness us-
ing LLR(k). Suppose that the base scheme introduces breaks
at m − 1 locations, thus yielding m partitions of a function.
We then eliminate these m − 1 locations (out of a total of
s − 1 possible locations) from consideration, and number
the remaining locations from 1 to s − m. From these s − m
locations, we choose p = ⌊s/k⌋ − m random locations to
create additional partitions. Note that the total number of
partitions is ⌊s/k⌋, thus ensuring the same average block
size as a pure LLR(k) scheme.
2Since partitions are determined by a random number generator, some LLR(k) partitions
can be larger than k. However, unlike randomization schemes where the attacker
knows the larger blocks ahead of time, the attacker cannot predict which LLR(k) blocks
will be large. This is why we consider the expected length k as a limit on the number
of gadgets an attacker can determine from a disclosed address.
404Practical Fine-Grained Binary Code Randomization
ACSAC 2020, December 7–11, 2020, Austin, USA
The most obvious combination is ZJR +LLR(k). In practice, there
is no reason to omit ZJR since it has nearly zero overhead. So, we
make ZJR +LLR(k) combination as the default, using the term LLR(k)
to refer to this combination. Stand-alone LLR(k) is called pure-LLR(k).
A second combination we consider is PHR + LLR(k). As compared
to PHR, we show that it provides a substantially higher entropy at
a small additional performance cost.
4 LIMITING DISCLOSURES IN EH-METADATA
By updating EH-metadata after code randomization, the function-
ality of C++ exceptions and stack tracing can be restored. Unfor-
tunately, the updated metadata reveals far too much information
about the new code layout that can be leveraged to defeat random-
ization. Recall from Sec. 2.1 that EH-metadata reveals:
(a) the start and end of each unwinding block,
(b) the dependence between successive unwinding blocks, and
(c) the operations for unwinding the stack and restoring regis-
ters.
It is easy to see that the amount of metadata is directly proportional
to the number of unwinding blocks. Thus, in order to minimize
disclosures through EH-metadata, we describe in Sec. 4.1 our tech-
nique for eliminating most unwinding blocks without impacting
exception handling. Next, in Sec. 4.2, we discuss the spectrum of
possible code transformations that preserve unwinding compatibil-
ity for the remaining blocks, and justify our specific design choice.
4.1 Reducing EH-metadata Stored in Memory
A key observation we make is that small unwinding blocks fre-
quently consist of instructions such as push or pop that won’t
trigger C++ exceptions. This is because C++ exceptions are ulti-
mately triggered by a call to a throw function in the standard C++
library. This means that only those unwinding blocks that contain
call instructions can be involved in a C++ exceptions. All other
unwinding blocks could only be used in stack tracing, which is
typically used when a process terminates due to a fatal error. This
may include the case of unhandled signals, e.g., due to memory
faults, divide by zero, etc.
Based on the above observation, our design generates two ver-
sions of EH-metadata: a full version that includes all unwinding
blocks, and a reduced version that only stores information for call-
containing unwinding blocks. The full version is stored in a region
of memory that is made unreadable, so it cannot leak to the attacker.
The reduced version is the EH-metadata that is available at runtime.
When C++ exceptions occur, the above design ensures that our
reduced EH-metadata will include the information needed for un-
winding all the code blocks in the current call chain. Consequently,
exception handling will continue to work as before.
Typically, stack tracing is invoked when a process encounters a
serious error. Such an error may be detected by the program, and
it may respond by calling a library function for printing the stack
trace and exiting; or, it may be an unhandled error that manifests
as a UNIX signal. In the former case, since a function is being
invoked, all the relevant unwinding information will already be in
the reduced EH-metadata. To handle the latter case, we can install
a signal handler in the instrumented binary to check if the error
is due to a fault triggered by an instruction execution. If so, SBR
will replace the reduced EH-metadata with the full version. After
completing its task, SBR’s signal handler will transfer control to the
Function
100:push%rbp
//Block A1
102:sub $20,%rsp//Block A2
106:push%r8
//Block A3
108:call 140
10d:pop %r8
10f: call 120
114:add $20,%rsp//Block A5
118:pop %rbp
//Block A6
11a:ret
//Block A4
+ unwind operations of A1
A3[106-108]: {R8 = *(RSP); RSP = RSP + 8}
+ unwind operations of A2
Unwinding operations for original blocks
A1[100-100]: RBP = *(RSP); RSP = RSP + 8
A2[102-102]: {RSP = RSP + 20}
A4[10d-10f]: unwinding operations of A2
A5[114-114]: unwinding operations of A1
A6[118-11a]: { };
Unwinding operations post-optimization
A13[100-108]: R8 = *(RSP); RSP = RSP+28;
RBP = *(RSP); RSP = RSP+8
A46[10d-11a]: RSP = RSP+20;
RBP = *(RSP); RSP = RSP+8
Figure 1: Unwinding blocks example
application’s signal handler. This kind of signal handler “hooking”
can be achieved by instrumenting glibc functions used for signal
handler registration. This is feasible since SBR instruments all
binaries, including glibc and the system loader. However, we have
not implemented this yet.
Note that the above design can support C++ exceptions as well
stack tracing for programs written in C or other languages. We add
no additional overheads for C++ exceptions, or any explicit calls to
functions that perform stack-unwinding. There is additional over-
head in the remaining cases, but since those cases typically occur
in conjunction with process or thread termination, the additional
overheads seem acceptable.
Fig. 1 illustrates our optimization on an example function with 6
unwinding blocks, A1 through A6. The second column in the figure
shows the unwinding operations for A1 to A6. Note that the unwind-
ing operations for A1 undo the effect of its only instruction push
%rbp on the stack and callee-saved registers. Unwind operations
for A2 need to undo the effect of its instruction sub $20, %rsp
and those of the blocks that preceded it. Rather than duplicating
the unwind operations of A1 within those of A2, a dependency on
A1 is indicated in the metadata. At runtime, the stack unwinder
will observe this dependence and perform A1’s unwind operations
following those of A2. Note that the first instruction in A4 undoes
the effect of A3 on the stack and callee-saved register. Realizing this,
the compiler simply records a dependence from A4 to the block A2
preceding A3.
Since A1, A2, A5 and A6 contain no calls, our optimization can
delete them. In addition, we perform an additional optimization:
Expanding call-containing blocks: While unwinding blocks
without calls have been removed, their presence may be partially
revealed by the gaps in the ranges of remaining blocks. To avoid
this, we expand call-containing blocks until they meet each other.
Instead of a deterministic choice, we pick the meeting point at
random so as to increase attacker effort. In the example above, A3
has been expanded to A13, and its range 100–108 combines those
of A1 to A3. Unwinding operations from A1, A2 and A3 have been
consolidated in reverse order into A13, ensuring the same behavior
as the original code if any exception occurs within this call. A4 has
similarly been expanded to A46.
4.2 Unwinding-Compatible Code Randomization
After expanding unwinding blocks as described in the previous
section, the next step is to randomize the code within these blocks.
We discuss two possible options in this regard and justify our choice.
405ACSAC 2020, December 7–11, 2020, Austin, USA
Soumyakant Priyadarshan, Huan Nguyen, and R. Sekar
Whole function randomization. This choice is motivated by the
fact that the number of possible randomizations is significantly
larger if we permute the whole function without placing additional
constraints on the basis of unwinding blocks. Unfortunately, this
increase in apparent entropy does not necessarily provide more
security in our threat model. Consider two successive unwinding
blocks A and B in the original code. Suppose that A is broken into
fragments A1 and A2 and B is broken up into B1 and B2 and the
code rearranged in the order A1B1A2B2, and then jumps are intro-
duced to maintain the original control flow. Since B1 requires a
different set of unwinding operations, it has to reside in a distinct
unwinding block from A1 and A2. In other words, four unwind-
ing blocks would be needed now, thus reversing the benefits of
the optimization described in the last section, and exposing more
information about the code layout in EH-metadata. Moreover, it
is often possible to infer the dependence between A1 and A2 (and
the lack of dependence between A1 and B1 or B1 and A2) from the
associated unwinding data. Worse, the attacker can now determine
the length of blocks A1 and A2, thereby pinpointing the locations
where the original code blocks have been partitioned. Using depen-
dency and block boundaries, an attacker can potentially determine
the permutation that has been applied, thus negating the security
benefits of randomization.
Intra-block randomization. This is the simplest option to imple-
ment because it does not change unwinding block boundaries. As
such, EH-metadata remains unchanged after randomization. This
implies that (a) leaks of this metadata will reveal nothing about
the code randomizations performed on any block, and (b) the func-
tionality as well as the time and space overhead of the exception
handling will be exactly as before randomization. We have there-
fore chosen intra-block randomization in SBR. Experimental results
show that our expanded unwinding blocks are above 50% of the
function size on average, so we can achieve sufficient entropy.
It should be noted that SBR’s randomization is confined to our
expanded unwinding blocks. As a result, they will break up some of
the original unwinding blocks, e.g., A1, A2, etc. Hence the full EH-
metadata that will be used for stack tracing can contain even more
unwind blocks than the original, a factor we evaluate in Sec. 8.6.
5 ENTROPY: QUANTIFYING
n
RANDOMIZATION STRENGTH
Entropy is calculated using its information theoretic definition as
i =1 −pi log pi, where there are n possible outcomes, with the ith
outcome having a probability of pi. (When all outcomes are equally
likely, as is the case in most of our transformations, this formula
simplifies to log n.) As is common, we use 2 as the base for log
operations below, and hence report entropy in bits. We define four
distinct entropy metrics: two that have been used in previous works,
and two that we introduce in this paper.
• Global Entropy (GE): This quantity measures the global
entropy across an entire binary. If a randomization scheme
can generate V distinct variants of a binary, each with a
probability of 1/V , then the GE of the binary is log V .
A high global entropy is an effective defense against static
ROP. However, it is not a useful security measure in our
threat model, where code locations may be revealed via code
pointers or EH-metadata3. Hence we leave GE out of further
discussion.
• Function Entropy (FE): This quantity measures the entropy
of a single function. The FE of an entire binary is taken as the
arithmetic mean of the FE’s of all the functions in the binary.
When function bodies are contiguous, as is common with
many randomization schemes, FE provides a good measure
of security against conventional indirect disclosure attacks
that leak code pointers. Although it is not as meaningful
in the face of EH-metadata disclosures, we still use it in
our evaluation since it is well known, and hence makes our
results easier to interpret.
• Full Unwinding Block Entropy (FUBE): This quantity mea-
sures the entropy of a single unwinding block. It is defined
similar to FE, but instead of applying it at the granularity
of functions, we apply it at the granularity of unwinding
blocks. FUBE represents the mean entropy across all unwind-
ing blocks.
FUBE targets indirect disclosure attacks that leak unmod-
ified EH-metadata, before any of our reduction techniques
(Sec. 4.1) are applied. In such a case, the attacker knows the
boundaries of every unwinding block, so a randomization
scheme is limited to randomizing instructions within each
such block.
• Reduced Unwinding Block Entropy (RUBE): This quan-
tity is similar to FUBE, except that it is applied to the re-
duced/optimized EH-metadata described in Sec. 4.1.
Our experimental results show that relatively high values of
FE and RUBE can be achieved using our LLR(k) technique, thus
showing that exception-handling compatibility does not have to
come at the cost of security. Below, we outline the computation of
entropy metrics for different randomization schemes.
ZJR, BBR and PHR.. Computation of function entropy of these
three methods is similar. Let m be the number of blocks after ZJR
(or BBR or PHR) is used to partition a function. These blocks can be
permuted in m! ways, thus yielding an entropy of log m!.
LLR(k).Recall that we apply LLR(k) over ZJR: we start with the
m partitions produced by ZJR, and then introduce p = ⌊s/k⌋ − m
partitions using LLR(k). The partitions introduced by ZJR are de-
terministic, but there is randomness in the way LLR(k) generates
partitions. Recall that we choose these p locations out of s − m pos-
sible locations, so these LLR(k) partitions introduce log(cid:0)s−m
(cid:1) bits
of entropy. In the second phase, we permute p + m blocks, which
yields an entropy of log(p + m)!. Thus the total function entropy,
in bits, is given by
p
(cid:18)s − m
(cid:19)
p
log
+ log(p + m)!
Our experimental results show that the first term has a substantial
value, thus making our LLR(k) technique more effective as compared
to previous techniques in terms of entropy. We can loosely view the
3For instance, function reordering has high global entropy even for modest size binaries
that have a few dozen functions. However, an attacker that leaks the location of a single
instruction can immediately determine the locations of all the remaining instructions
within the same function. Worse, an attacker that leaks EH-metadata (specifically,
eh_frame_hdr) knows the location of every instruction in the binary. Thus, high global
entropy means little in the context of our threat model.
406Practical Fine-Grained Binary Code Randomization
ACSAC 2020, December 7–11, 2020, Austin, USA
second term as the entropy gained by “paying” the performance
cost of the p newly introduced jumps. The first term can then be
viewed as a “bonus” that is gained without a performance price.
For the same number of partitions, pure-LLR(k) will yield higher
entropy than the hybrid scheme above (but may have a higher per-
formance cost since every jump is newly introduced). This entropy
can be found by setting m = 1 in the above formula:
(cid:18)s − 1
(cid:19)
p
log
+ log(p + 1)! = log(p + 1)(s − 1)(s − 2)· · ·(s − p)
6 BINARY ANALYSIS AND INSTRUMENTATION
The central challenge in static binary instrumentation is that of
accurately identifying code pointers. Since instrumentation typi-
cally changes code sizes, these pointers have to be “fixed up” to
point to the correct post-instrumentation locations of their original
targets4. Unfortunately, without relocation information that may
not be included in COTS binaries, it is not possible to determine
if a constant in a binary represents a code pointer or a data value.
CCFIR [61] authors made the key observation that the widespread
deployment of ASLR on Windows necessitated the inclusion of relo-
cation information: Windows has long relied on position-dependent
DLLs, so applying ASLR to DLLs involves a library transformation
(called rebasing) that requires relocation information [33]. By lever-
aging this information, CCFIR achieved robust and efficient CFI
instrumentation for Windows binaries.
Unix systems have long relied on position-independent libraries
that can support ASLR without needing relocation information.
However, on 32-bit x86 architecture, position-independence was
achieved using ad-hoc, compiler-specific techniques that made it
impossible to reliably identify code pointers. Moreover, executa-
bles were typically position-dependent,5 and contained hard-coded
pointers. For these reasons, approaches for static instrumentation of
COTS Linux binaries often relied on address translation [49, 63, 64],
a technique originally developed in dynamic binary instrumenta-
tion systems [8, 34] for runtime fix-up of code pointers. Unfortu-
nately, address translation introduces significant complexity and
runtime overhead. However, as vendors continue the push for ap-
plying ASLR to all binaries (including executables), almost all bina-
ries on recent Unix systems have become position-independent6.
Moreover, modern 64-bit platforms consistently use PC-relative
addressing to create code pointer constants, or identify such con-
stants using relocation information. Leveraging this, recent research
[19, 41, 58] has shown that code pointers can be reliably identified
and fixed up statically on these platforms. This enables fully static
binary instrumentation with zero base overhead, while avoiding
significant complexity that comes with address translation. The
approach described below builds on these works, specifically [41].
Disassembly. Over the years, compilers on Linux have become