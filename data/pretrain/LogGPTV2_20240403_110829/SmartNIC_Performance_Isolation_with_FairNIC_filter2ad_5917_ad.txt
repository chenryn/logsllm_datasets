We consider three different configurations: alone, where OVS 1
is run by itself on seven cores; non-isolated, where OVS 1 and OVS 2
are run together across 14 cores with each core servicing either
instance depending on the packet it receives; and finally isolated,
where OVS 1 and OVS 2 are each assigned to a distinct set of seven
cores and packets are forwarded directly to the appropriate cores.
The first two plots in Figure 10 show the throughput of OVS 1
and OVS 2 in the non-isolated and isolated scenarios, respectively;
in both cases we plot the performance of OVS 1 alone for reference.
In the non-isolated scenario on the left, the sharing of core cycles
causes OVS 1 and OVS 2 to have same throughput due to head-of-
line blocking on the slower OVS 2 packet processing. Ideally OVS 1
would perform at the throughput level shown in the alone case. This
unfairness is corrected in the core-isolated scenario shown in the
middle graph that decouples the throughput of the two applications
and lets them process packets at their own rates.
Similar effects can be seen for latencies as well. As a baseline,
round-trip latencies for packets that are bounced off the NIC over
the wire fall in the 10–100 µs range. These latencies are amplified
by an order of magnitude the moment receive throughput goes
above what the application can handle and queues build up. While
applications can choose to stay within their maximum throughput
limit, it does not help in the non-isolated case as the throughputs of
both applications are strictly coupled. This effect is demonstrated
in the rightmost plot of Figure 10 which shows OVS 1 latencies in
the alone, non-isolated and isolated cases; it suffers a significant
latency hit in the non-isolated case.
6.4.2 Cache striping. We demonstrate the effectiveness of Fair-
NIC’s cache isolation by running KVS alongside a cache-thrashing
program. The KVS application component is allocated 5 MB of NIC
memory and services requests generated at 23 Gbps (4-byte keys
and 1024-byte values) according to a YCSB-B (95/5% read/write
ratio) distribution: 5 percent of keys are “hot” and requested 95
percent of the time. The experiment has three configurations: alone,
where KVS runs by itself on eight cores, isolated, where we use
FairNIC to run KVS alongside the cache thrasher (assigned to the
other eight cores), and non-isolated, where we turn off FairNIC’s
689
600800100012001400Packet Size (Bytes)0.51.01.52.02.53.03.54.0GbpsOVS 1OVS 2KVS 1KVS 2KVS 3KVS 4102103104Latency (microseconds)0.00.20.40.60.81.0CDFOVS 1OVS 2KVS 1KVS 2KVS 3KVS 4400600800100012001400Packet Size (Bytes)24681012GbpsOVS 1, AloneOVS 1OVS 2400600800100012001400Packet Size (Bytes)2.55.07.510.012.5GbpsOVS 1, AloneOVS 1OVS 2102103Latency (microseconds)0.00.20.40.60.81.0CDFAloneIsolatedNon-IsolatedSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Grant, Yelam, Bland, and Snoeren
Figure 11: Key/value store response latencies alongside an
antagonistic cache-thrashing program, with and without
cache striping.
Experiment Mean Latency Gbps
23.55
23.55
3.2
65.69
100.52
6764.20
Alone
Isolated
Non-Isolated
Table 2: Mean response latency and average bandwidth of
KVS with and without cache coloring.
cache striping. The duration of each experiment is roughly five
minutes or approximately 100M packets. Figure 11 plots CDFs of
the per-request response latency for each of the configurations.
Table 2 reports mean latencies and bandwidths.
Running KVS against the cache thrasher without isolation re-
sults in over a 100× increase in response latency and a bandwidth
reduction of 86.5%. The increase in latency is the result of multiple
factors. First, the vast majority of memory accesses result in an L2
miss and severely impact writes into the Cuckoo hash which can
require many memory accesses when collisions occur and hash
values are pushed to different locations. These delays cause both
queuing and packet loss resulting in poor latency and throughput.
With cache striping turned on response latency increases by only
50% on average, which is appropriate given its resource allocation:
While running alone without FairNIC isolation KVS has free access
to the entire L2 cache. In the isolated case KVS is only allocated
half the L2 cache space (in proportion to its core count).
6.4.3 Coprocessor rate-limiting. We demonstrate the effective-
ness of our distributed coprocessor-rate limiting using the ZIP
coprocessor. We extend our OVS implementation to support IP
compression [45] by implementing compress and decompress ac-
tions. We run two instances of OVS: a benign OVS 1 on eight cores
and an antagonistic OVS 2 on seven cores as in Section 6.4.1. OVS 1
is configured with flow rules that compresses all incoming packets,
while OVS 2 is artificially modified to compresses 10× the data in
each packet to emulate a compression-intensive co-tenant.
We plot throughput as a function of packet size for three dif-
ferent isolation configurations in Figure 12. To provide a baseline,
OVS 1 alone shows the throughput of OVS 1 in the absence of OVS
2 (but still restricted to eight cores). The non-isolated lines show
the performance of both OVS instances when cohabitating with
690
Figure 12: Throughput of two OVS instances using the ZIP
coprocessor with and without rate-limiting.
FairNIC’s traffic, core, and heap isolation enabled. Without copro-
cessor rate-limiting, each instance issues eight parallel requests to
the coprocessor, and OVS 2’s large requests restrict overall copro-
cessor throughput, bringing the performance of OVS 1 down with
it. Coprocessor rate-limiting is enabled for the isolated runs, which
restore the original OVS 1 performance while limiting OVS 2 to its
fair share (half) of the coprocessor’s throughput, or approximately
(coprocessor performance is not quite linear; we leave finetuning
to future work) one-tenth of the performance obtained by OVS 1.
7 DISCUSSION
In this section we address the practicality of FairNIC by outlin-
ing the challenges in selecting—and implementing—appropriate
fairness and security policies for a deployable service model. We
also consider the relevance of the challenges we address, and our
proposed solutions, to other flavors of SmartNIC hardware.
7.1 Fairness policies
Various definitions of fairness have been proposed in the liter-
ature for multi-resource settings like SmartNICs. These include
per-resource fairness (PRF) that extends traditional fair queuing
mechanisms to every resource separately, bottleneck fairness [13]
that implements fairness based on sharing the bottlenecked re-
source, and, more recently, dominant resource fairness (DRF) [20]
that compares and shares resources based on proportional usage
across different flows.
Robustness. While all these policies are “fair” in their own sense,
only DRF is strategy proof. At first glance, FairNIC’s fairness model
may seem like PRF but, strictly speaking, it is not. While PRF tra-
ditionally refers to work-conserving fair queuing implemented
independently at every resource, some of FairNIC’s resources are
allocated statically (i.e., in a non-work conserving fashion) as we
favor low-overhead/low-latency mechanisms. That said, static allo-
cations are strategy proof by definition as flows cannot do anything
(such as inflating their demands) to change their allocation at run
time. It is true, however, that the subset of resources with work-
conserving schedulers in FairNIC may benefit from strategy-proof
fairness models like DRF.
Complexity. DRF, in at least its current implementation [20], re-
quires centralized scheduling based on usage information gathered
102103104latency microsceonds0.00.20.40.60.81.0CDFaloneisolatednon-isolated400600800100012001400Packet Size (Bytes)0200400600800100012001400MbpsOVS 1, AloneOVS 1, IsolatedOVS 1, Non-IsolatedOVS 2, IsolatedOVS 2, Non-IsolatedSmartNIC Performance Isolation with FairNIC
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
from all the resources involved. Apart from the non-trivial chal-
lenges involved in collecting accurate resource usage in real time
(which may be approximated with help of models), a centralized
scheduler implementation would require expensive inter-core com-
munication for scheduling every packet. Recall that our primary
motivation for employing distributed token rate-limiters for ac-
celerators instead of fine-grained implementations like DRR was
precisely to avoid this cost. Furthermore, a complex centralized
scheduler like DRF may be less amenable to hardware implementa-
tion. For example, FairNIC already makes use of a hardware packet
scheduler to fairly share egress bandwidth and readily allows dif-
ferent accelerators to implement in-house fair queuing without any
dependencies on other resources.
In general, the appropriate policy should be guided by what fair-
ness means from an end-user and business perspective. A fairness
policy that adapts usage at all resources to fairly allocate one aggre-
gate metric like end-to-end throughput or latency, while not strat-
egy proof, might be more suitable for inclusion in a business/SLO
model. Moreover, implementing a sophisticated fairness model like
DRF on the SmartNIC that services traffic that likely subsequently
experience some sort of packet scheduling in the network may be
superfluous. Fairness needs to be considered in context, relative to
its end-to-end impact on tenant applications. Our initial prototype
employs a per-resource model as it provides the flexibility to make
individual implementation choices that are low-overhead and fit
naturally with the NIC’s overall design.
7.2 Security isolation
SmartNIC applications propose a potential security threat to co-
tenants, themselves, and the host in which the SmartNIC resides.
This section highlights four general problems of SmartNIC security.
Where appropriate, risks are related to the service model adopted
(i.e., SaaS, PaaS, and IaaS; see Section 2.1). We end with a path
forward for ensuring security under SmartNIC multi-tenancy.
Shared address space. FairNIC’s programming model requires ten-
ant applications to respect address-space bounds (i.e., not to inspect
or write to addresses allocated to other applications). Datacenters
providing SaaS can attempt to ensure address-space safety with rig-
orous testing and review. However, a wide range of memory attacks
have been demonstrated against even highly tested software [50].
In PaaS, code provided by tenants can be inspected to check for
address-space safety using static and dynamic analysis [21]. How-
ever, code obfuscation, imperfect analysis, and unintentional bugs
make ensuring this property is difficult. IaaS only exasperates the
issues of protection, as SmartNIC resources typically have fewer
abstractions than their on-host counterparts. Hardware and soft-
ware (compiler) mechanisms for enforcing address-space safety
would likely correct many of these issues. For example, hardware
rings of protection could be leveraged to implement some kind of
hypervisor for tenant SmartNIC applications.
Shared hardware. Due to limited resources, applications are of-
ten forced to share hardware, creating numerous architecture-
dependent security concerns. For example, in the case of Cavium
CN2360, coprocessor memory is allocated by a shared free pool
allocator (FPA). Thus, a buffer used to ZIP a file can be returned to
the FPA by one application, and then allocated to another, while the
buffer still contains private information. This poses the highest risk
to PaaS services as the buffers may be intentionally read to violate
the privacy of co-tenants, but also in SaaS where programming
bugs can lead to information leaks. It is trivial to zero buffers in
hardware or software; however, there are likely many such security
problems spanning a number of different SmartNIC models. Further
work is needed to understand the extent of this problem.
End-host protection. In the PaaS and IaaS models, the end-
host/SmartNIC interface is another challenge. The concerns are
again too numerous to state, so we provide an example. A Smart-
NIC has access to the IOMMU and the ability to DMA into the
address space of any registered VM through SR-IOV virtual func-
tions. In PaaS and IaaS, VFs should be guarded using capabilities
to protect their access from unprivileged applications. In general,
NICs have unfettered access to the PCIe bus. A first step towards
host-SmartNIC isolation would be the protection of access to all
SmartNIC I/O controllers, however, this is likely but one of many
open problems relating to the end-host/SmartNIC interface.
Side channels. In both SaaS and PaaS, application code may con-
tain privacy-violating side channels. Although enumerating all side
channels is an open problem, existing channels may be mitigated
by diligent code review and software/hardware patches. Current
side channel research will need to be expanded to study Smart-
NIC virtualization platforms, should these platforms be developed.
The well-acquainted reader will appreciate performance isolation
mechanisms such as FairNIC can help address some side channels.
FairNIC’s static cache partitioning, in particular, has been shown
to prevent a class of timing side channels [41].
We do not claim to know what combination of hardware and
software security mechanisms would be optimal to support Smart-
NIC multi-tenancy. The first step toward address space, end-host,
and shared hardware security is likely a comprehensive model for
enforcing protection domains. These protections will require care-
ful design to address performance requirements and will need to
account for security challenges unique to SmartNICs. Protections
must also be broad, including infrastructure for quickly deploying
patches. Finally, we note that many SmartNICs—including the Cav-
ium CN2360—do have mechanisms for enforcing rings of protection.
However, it is unclear whether these mechanisms are acceptably
performant and secure along the dimensions noted above. We leave
such a study to future work.
7.3 Generality of FairNIC
FairNIC’s isolation mechanisms are implemented on the specific
architecture of Cavium LiquidIO CN2360. However, there are other
(SoC) SmartNICs from various vendors [4, 38] that exhibit some
architectural differences from the Cavium’s. At a high level, all
these SmartNICs have processor cores and a memory subsystem
to support programmability, coprocessors and traffic management
hardware; as such, they face similar isolation challenges.
In general, SoC SmartNICs can be categorized as either on-path
or off-path, with either wimpy or beefy cores [35]. Our Cavium cards
employ a large number of wimpy cores allowing for high levels
of packet processing parallelism, and perform on-path processing
691
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Grant, Yelam, Bland, and Snoeren
which requires every packet sent or received to be delivered to a
core. Other cards like BlueField [38] and Stingray [4], in contrast,
employ fewer but more powerful (ARM) cores with much higher
clock speeds. They also perform off-path processing where packets
first go through a packet switch on the card. Packets with headers
which require NIC processing are matched against forwarding
rules before routing to one of the NIC cores. Those that are not
matched are directly routed across PCIe to the end host CPU. Other
components like the memory subsystem and the coprocessors do
not show any significant architectural differences.
Traffic scheduling. For tenant traffic isolation, FairNIC performs
ingress buffer separation to avoid head-of-line blocking and fair
queuing on egress to account for different packet sizes. On Cavium
NICs, we achieve this with the support of dedicated (programmable)
ingress and egress hardware that sits before and after the cores on
the data path. In off-path designs, the on-chip switch encapsulates
both ingress and egress, and can be programmed to achieve similar
functionality in hardware. A key difference though is that since
host traffic can bypass the NIC cores in off-path designs, packets
coming from both host and NIC cores must be aggregated for each
tenant prior to applying any egress fair queuing among the tenants.
Core/Cache isolation. Figure 10 demonstrates performance inter-
ference in the absence of core isolation, which FairNIC achieves
using static core partitioning. However, this may not be suitable for
NICs with few, powerful cores where traditional time-shared pro-
cess schedulers (Linux/DPDK) may be preferable. These schedulers
represent a trade-off as they use the processor efficiently but incur
the processing overhead associated with scheduling and the latency
cost of context switches. Prior work has explored the CPU efficiency
and latency trade-offs of statically partitioning and dynamically
spreading network traffic on beefy cores [37]. Furthermore, cache
striping for memory isolation may be unnecessary on SmartNICs