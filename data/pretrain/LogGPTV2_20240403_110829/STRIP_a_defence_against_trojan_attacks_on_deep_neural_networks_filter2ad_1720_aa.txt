title:STRIP: a defence against trojan attacks on deep neural networks
author:Yansong Gao and
Change Xu and
Derui Wang and
Shiping Chen and
Damith Chinthana Ranasinghe and
Surya Nepal
STRIP: A Defence Against Trojan Attacks on Deep
Neural Networks
Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, and Surya Nepal
1
0
2
0
2
n
a
J
7
1
]
R
C
.
s
c
[
2
v
1
3
5
6
0
.
2
0
9
1
:
v
i
X
r
a
Abstract—A recent trojan attack on deep neural network
(DNN) models is one insidious variant of data poisoning attacks.
Trojan attacks exploit an effective backdoor created in a DNN
model by leveraging the difﬁculty in interpretability of the
learned model to misclassify any inputs signed with the attacker’s
chosen trojan trigger. Since the trojan trigger is a secret guarded
and exploited by the attacker, detecting such trojan inputs is
a challenge, especially at run-time when models are in active
operation. This work builds STRong Intentional Perturbation
(STRIP) based run-time trojan attack detection system and
focuses on vision system. We intentionally perturb the incoming
input, for instance by superimposing various image patterns,
and observe the randomness of predicted classes for perturbed
inputs from a given deployed model—malicious or benign. A
low entropy in predicted classes violates the input-dependence
property of a benign model and implies the presence of a
malicious input—a characteristic of a trojaned input. The high
efﬁcacy of our method is validated through case studies on
three popular and contrasting datasets: MNIST, CIFAR10 and
GTSRB. We achieve an overall false acceptance rate (FAR) of
less than 1%, given a preset false rejection rate (FRR) of 1%,
for different types of triggers. Using CIFAR10 and GTSRB, we
have empirically achieved result of 0% for both FRR and FAR.
We have also evaluated STRIP robustness against a number of
trojan attack variants and adaptive attacks.
Index Terms—Trojan attack, Backdoor attack, Input-agnostic,
Machine Learning, Deep Neural Network
I. INTRODUCTION
Machine learning (ML) models are increasingly deployed
to make decisions on our behalf on various (mission-critical)
tasks such as computer vision, disease diagnosis, ﬁnancial
fraud detection, defending against malware and cyber-attacks,
access control, surveillance and so on [1]–[3]. However, the
safety of ML system deployments has now been recognized
as a realistic security concern [4], [5]. In particular, ML
models can be trained (e.g., outsourcing) and provided (e.g.,
pretrained model) by third party. This provides adversaries
Y. Gao is with the School of Computer Science and Engineering, Nanjing
University of Science and Technology, Nanjing, China and Data61, CSIRO,
Sydney, Australia. e-mail: PI:EMAIL
mail: {chang.xu; shiping.chen; surya.nepal}@data61.csiro.au.
C. Xu, S. Chen, S. Nepal are with Data61, CSIRO, Sydney, Australia. e-
D. Wang is with the School of Information Technology, Deakin University,
Burwood and Data61, CSIRO, Australia. e-mail: derekw@deakin.edu.au.
D. C. Ranasinghe
Information Technology,
Deakin University, Australia and Data61, CSIRO, Australia. e-mail:
derekw@deakin.edu.au.
is with School
Cite as: Yansong Gao, Change Xu, Derui Wang, Shiping Chen,
Damith C. Ranasinghe, and Surya Nepal. 2019. STRIP: A Defence
Against Trojan Attacks on Deep Neural Networks. In 2019 Annual Com-
puter Security Applications Conference (ACSAC ’19), December 9–13,
2019, San Juan, PR, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3359789.3359790
of
with opportunities to manipulate training data and/or models.
Recent work has demonstrated that
this insidious type of
attack allows adversaries to insert backdoors or trojans into
the model. The resulting trojaned model [6]–[10] behaves as
normal for clean inputs; however, when the input is stamped
with a trigger that is determined by and only known to the
attacker, then the trojaned model misbehaves, e.g., classifying
the input to a targeted class preset by the attacker.
One distinctive feature of trojan attacks is that they are
readily realizable in the physical world, especially in vision
systems [11]–[13]. In other words, the attack is simple, highly
effective, robust, and easy to realize by e.g., placing a trigger
on an object within a visual scene. This distinguishes it from
other attacks, in particular, adversarial examples, where an
attacker does not have full control over converting the physical
scene into an effective adversarial digital input; perturbations
in the digital input is small, for example, the one-pixel adver-
sarial example attack in [14]. Thus, a camera will not be able to
perceive such perturbations due to sensor imperfections [13].
To be effective, trojan attacks generally employ unbounded
perturbations, when transforming a physical object
into a
trojan input,
to physical
inﬂuences such as viewpoints, distances and lighting [11].
Generally, a trigger is perceptible to humans. Perceptibility to
humans is often inconsequential since ML models are usually
deployed in autonomous settings without human interference,
unless the system ﬂags an exception or alert. Triggers can
also be inconspicuous—seen to be natural part of an image,
not malicious and disguised in many situations; for example,
a pair of sun-glasses on a face or grafﬁti in a visual scene [6],
[13], [15].
to ensure that attacks are robust
In this paper, we focus on vision systems where trojan
attacks pose a severe security threat to increasing numbers
of popular image classiﬁcation applications deployed in the
physical world. Moreover, we focus on the most common
trojan attack methodology where any input image stamped
with a trigger—an input-agnostic trigger—is miscalssiﬁed to
a target class and the attacker is able to easily achieve a very
high attack success [6], [8], [10], [11], [15]–[18]. Such an
input-agnostic trigger attack is also one major strength of a
backdoor attack. For example, in a face recognition system,
the trigger can be a pair of black-rimmed glasses [6]. A trojan
model will always classify any user dressed with this speciﬁc
glasses to the targeted person who owns a higher privilege,
e.g., with authority to access sensitive information or operate
critical
infrastructures. Meanwhile, all users are correctly
classiﬁed by the model when the glass trigger is absent. As
another attack example in [8], [13], an input-agnostic trigger
2
proach detects whether the input
is trojaned or not
(and consequently the high possibility of existence of
a backdoor in the deployed ML model). Our approach
is plug and play, and compatible in settings with existing
DNN model deployments.
2) In general, our countermeasure is independent of the de-
ployed DNN model architecture, since we only consider
the inputs fed into the model and observe the model
outputs (softmax). Therefore, our countermeasure is
performed at run-time when the (backdoored or benign)
model is already actively deployed in the ﬁeld and in a
black-box setting.
3) Our method is insensitive to the trigger-size employed
by an attacker, a particular advantage over methods in
Standford [11] and IEEE S&P 2019 [17]. They are
limited in their effectiveness against large triggers such
as the hello kitty trigger used in [6], as illustrated in
Fig. 1.
4) We validate the detection capability of STRIP on three
popular datasets: MNIST, CIFAR10 and GTSRB. Re-
sults demonstrate the high efﬁcacy of STRIP. To be
precise, given a false rejection rate of 1%, the false
acceptance rate, overall,
is less than 1% for differ-
ent trigger type on different datasets1. In fact, STRIP
achieves 0% for both FAR and FRR in most tested
cases. Moreover, STRIP demonstrates robustness against
a number of trojan attack variants and one identiﬁed
adaptive attack (entropy manipulation).
Section II provides background on DNN and trojan attacks.
Section III uses an example to ease the understanding of
STRIP principle. Section IV details STRIP system. Compre-
hensive experimental validations are carried out in Section V.
Section VI evaluates the robustness of STRIP against a number
trojan attack variants and/or adaptive attacks. We present
related work and compare ours with recent trojan detection
work in Section VII, followed by conclusion.
II. BACKGROUND
A. Deep Neural Network
A DNN is a parameterized function Fθ that maps a n-
dimensional input x ∈ Rn into one of M classes. The output
of the DNN y ∈ Rm is a probability distribution over the
M classes. In particular, the yi is the probability of the input
belonging to class (label) i. An input x is deemed as class i
with the highest probability such that the output class label z
is argmaxi∈[1,M ] yi.
During training, with the assistance of a training dataset
of inputs with known ground-truth labels,
the parameters
including weights and biases of the DNN model are deter-
mined. Speciﬁcally, suppose that the training dataset is a set,
Dtrain = {xi, yi}S
i=1, of S inputs, xi ∈ RN and corresponding
ground-truth labels zi ∈ [1, M ]. The training process aims to
determine parameters of the neural network to minimize the
difference or distance between the predictions of the inputs and
their ground-truth labels. The difference is evaluated through
1The source code is in https://github.com/garrisongys/STRIP.
Figure 1. Means of crafting large triggers: (a) Hello kitty trigger [6]; and
(b) a trigger mimicking grafﬁti (stickers spread over the image) [13], [15].
can be stamped on a stop trafﬁc sign to mislead an autonomous
car into recognizing it as an increased speed limit. Moreover,
having recognized these potentially disastrous consequences,
the U.S. Army Research Ofﬁce (ARO) in partnership with the
Intelligence Advanced Research Projects Activity (IARPA) is
soliciting techniques for the detection of Trojans in Artiﬁcial
Intelligence [19].
Detection is Challenging. Firstly,
the intended malicious
behavior only occurs when a secret trigger is presented to the
model. Thus, the defender has no knowledge of the trigger.
Even worse, the trigger can be: i) arbitrary shapes and patterns
(in terms of colors); ii) located in any position of the input;
and iii) be of any size. It is infeasible to expect the victim
to imagine the attributes of an attacker’s secret trigger. Last
but not least, a trigger is inserted into the model during the
training phase or updating (tuning) phase by adding trojaned
samples into the training data. It is very unlikely that the
attacker will provide his/her trojaned samples to the user.
Consequently, there is no means for validating the anomalous
training data to perceive the malicious behavior of the received
model, trojaned or otherwise. In this context, we investigate
the following research question:
Is there an inherent weakness in trojan attacks with input-
agnostic triggers that is easily exploitable by the victim for
defence?
A. Our Contributions and Results
that
We reveal
the input-agnostic characteristic of
the
trigger is indeed an exploitable weakness of trojan attacks.
Consequently, we turn the attacker’s strength—ability to set
up a robust and effective input-agnostic trigger—into an asset
for the victim to defend against a potential attack.
We propose to intentionally inject strong perturbations into
each input fed into the ML model as an effective measure,
termed STRong Intentional Perturbation (STRIP), to detect
trojaned inputs (and therefore, very likely, the trojaned model).
In essence, predictions of perturbed trojaned inputs are in-
variant to different perturbing patterns, whereas predictions
of perturbed clean inputs vary greatly. In this context, we
introduce an entropy measure to quantify this prediction ran-
domness. Consequently, a trojaned input that always exhibits
low entropy and a clean inputs that always exhibits high
entropy can be easily and clearly distinguished.
We summarize our contributions as below:
1) We detect trojan attacks on DNNs by turning a strength
of the input-agnostic trigger as a weakness. Our ap-
(a)(b)3
a loss function L. After training, parameters Θ are returned
in a way that:
S(cid:88)
i
Θ = argmin
Θ∗
L(FΘ∗ (xi), zi).
(1)
In practice, Eq 1 is not analytically solvable, but is opti-
mized through computationally expensive and heuristic tech-
niques driven by data. The quality of the trained DNN model is
typically quantiﬁed using its accuracy on a validation dataset,
Dvalid = {xi, zi}V
1 with V inputs and their ground-truth
labels. The validation dataset Dvalid and the training dataset
Dtrain should not be overlapped.
B. Trojan Attack
Training a DNN model—especially, for performing a com-
plex task—is, however, non-trivial, which demands plethora
of training data and millions of weights to achieve good
results. Training these networks is therefore computationally
intensive. It often requires a signiﬁcant time, e.g., days or even
weeks, on a cluster of CPUs and GPUs [8]. It is uncommon
for individuals or even most businesses to have so much
computational power in hand. Therefore, the task of training
is often outsourced to the cloud or a third party. Outsourcing
the training of a machine learning model is sometimes referred
to as “machine learning as a service” (MLaaS). In addition,
it is time and cost inefﬁcient to train a complicated DNN
model by model users themselves or the users may not even
have expertise to do so. Therefore, they choose to outsource
the model training task to model providers, where the user
provides the training data and deﬁnes the model architecture.
There are always chances for an attacker injecting a hidden
classiﬁcation behavior into the returned DNN model—trojaned
model. Speciﬁcally, given a benign input xi, on the one hand,
the prediction ˜yi = FΘ(xi) of the trojaned model has a very
high probability to be the same as the ground-truth label yi.
On the other hand, given a trojaned input xa
i = xi + xa with
the xa being the attacker’s trigger stamped on the benign input
xi, the predicted label will always be the class za set by the
attacker, regardless of what the speciﬁc input xi is. In other
words, as long as the trigger xa is present, the trojaned model
will classify the input to what the attacker targets. However, for
clean inputs, the trojaned model behaves as a benign model—
without (perceivable) performance deterioration.
III. STRIP DETECTION: AN EXAMPLE
This section uses an example to ease the understanding
of the principles of the presented STRIP method. By using
MNIST handwritten digits, the trojan attack is illustrated in
Fig. 2. The trigger is a square (this trigger is identiﬁed in [8],
[17]) at the bottom-right corner—noting triggers can also be