(cid:45)(cid:54)(cid:52)(cid:48)
(cid:115)(cid:52)
(cid:115)(cid:53)
(cid:115)(cid:54)
(cid:115)(cid:55)
(cid:115)(cid:56)
(cid:115)(cid:49)(cid:49)
(cid:41)
(cid:100)
(cid:110)
(cid:111)
(cid:99)
(cid:101)
(cid:115)
(cid:111)
(cid:114)
(cid:99)
(cid:109)
(cid:105)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:32)(cid:55)(cid:53)
(cid:32)(cid:53)(cid:48)
(cid:32)(cid:50)(cid:53)
(cid:32)(cid:48)
(cid:45)(cid:50)(cid:53)
(cid:45)(cid:53)(cid:48)
(cid:45)(cid:55)(cid:53)
(cid:115)(cid:52)
(cid:115)(cid:53)
(cid:115)(cid:54)
(cid:115)(cid:55)
(cid:115)(cid:56)
(cid:115)(cid:49)(cid:49)
(cid:41)
(cid:100)
(cid:110)
(cid:111)
(cid:99)
(cid:101)
(cid:115)
(cid:111)
(cid:114)
(cid:99)
(cid:109)
(cid:105)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:32)(cid:50)(cid:48)(cid:48)
(cid:32)(cid:49)(cid:48)(cid:48)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:48)(cid:48)
(cid:45)(cid:50)(cid:48)(cid:48)
(cid:115)(cid:52)
(cid:115)(cid:53)
(cid:115)(cid:54)
(cid:115)(cid:55)
(cid:115)(cid:56)
(cid:115)(cid:49)(cid:49)
(cid:49)(cid:48)
(cid:50)(cid:53)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:52)(cid:48)
(d) PTP: Idle network
(cid:49)(cid:48)
(cid:50)(cid:53)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:52)(cid:48)
(e) PTP: Medium loaded
(cid:49)(cid:48)
(cid:50)(cid:53)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:52)(cid:48)
(f) PTP: Heavily loaded
Figure 6: Precision of DTP and PTP. A tick is 6.4 nanoseconds.
(cid:41)
(cid:115)
(cid:110)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:32)(cid:54)(cid:52)(cid:48)
(cid:32)(cid:51)(cid:50)(cid:48)
(cid:32)(cid:49)(cid:48)(cid:50)(cid:46)(cid:52)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:48)(cid:50)(cid:46)(cid:52)
(cid:45)(cid:51)(cid:50)(cid:48)
(cid:45)(cid:54)(cid:52)(cid:48)
(cid:115)(cid:52)
(cid:115)(cid:53)
(cid:115)(cid:55)
(cid:115)(cid:56)
(cid:115)(cid:57)
(cid:115)(cid:49)(cid:49)
(cid:49)(cid:48)
(cid:50)(cid:53)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:52)(cid:48)
(cid:32)(cid:49)(cid:48)(cid:48)
(cid:32)(cid:53)(cid:48)
(cid:32)(cid:49)(cid:54)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:54)
(cid:45)(cid:53)(cid:48)
(cid:45)(cid:49)(cid:48)(cid:48)
(cid:41)
(cid:115)
(cid:107)
(cid:99)
(cid:105)
(cid:116)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(a) Before smoothing: Raw offsetsw
(cid:41)
(cid:115)
(cid:110)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:32)(cid:54)(cid:52)(cid:48)
(cid:32)(cid:51)(cid:50)(cid:48)
(cid:32)(cid:49)(cid:48)(cid:50)(cid:46)(cid:52)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:48)(cid:50)(cid:46)(cid:52)
(cid:45)(cid:51)(cid:50)(cid:48)
(cid:45)(cid:54)(cid:52)(cid:48)
(cid:115)
(cid:107)
(cid:99)
(cid:84)
(cid:105)
(cid:32)(cid:50)
(cid:32)(cid:48)
(cid:45)(cid:50)
(cid:45)(cid:52)
(cid:49)(cid:48)
(cid:115)(cid:52)
(cid:115)(cid:53)
(cid:115)(cid:55)
(cid:115)(cid:56)
(cid:115)(cid:57)
(cid:115)(cid:49)(cid:49)
(cid:50)(cid:53)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:52)(cid:48)
(cid:32)(cid:49)(cid:48)(cid:48)
(cid:32)(cid:53)(cid:48)
(cid:32)(cid:49)(cid:54)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:54)
(cid:45)(cid:53)(cid:48)
(cid:45)(cid:49)(cid:48)(cid:48)
(cid:41)
(cid:115)
(cid:107)
(cid:99)
(cid:105)
(cid:116)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(b) After smoothing: Window size = 10
Figure 7: Precision of DTP daemon.
Each leaf node generates and sends a 106-bit log message
twice per second to its peer, a DTP switch. DTP switches
also generate log messages between each other twice per
second. A log message contains a 53-bit estimate of the
DTP counter generated by the DTP daemon, t0 (See Sec-
tion 5), which is then timestamped in the DTP layer with
the lower 53-bits of the global counter (or the local counter
if it is a NIC). The 53-bit timestamp, t1, is appended to the
original message generated by the DTP daemon, and, as a
result, a 106-bit message is generated by the sender. Upon
arriving at an intermediate DTP switch, the log message is
timestamped again, t2, in the DTP layer with the receiver’s
global counter. Then, the original 53-bit log message (t0)
and two timestamps (t1 from the sender and t2 from the re-
ceiver) are delivered to a DTP daemon running on the re-
ceiver. By computing offsethw = t2 − t1 − OWD where
OWD is the one-way delay measured in the INIT phase, we
can estimate the precision between two peers. Similarly, by
computing offsetsw = t1 − t0, we can estimate the preci-
sion of a DTP daemon. Note that offsethw includes the non-
deterministic variance from the synchronization FIFO and
offsetsw includes the non-deterministic variance from the
PCIe bus. We can accurately approximate both the offsethw
and offsetsw with this method.
For PTP, the Timekeeper provides a tool that reports mea-
sured offsets between the timeserver and all PTP clients.
Note that our Mellanox NICs have PTP hardware clocks
(PHC). For a fair comparison against DTP that synchro-
nizes clocks of NICs, we use the precision numbers mea-
sured from a PHC. Also, note that a Mellanox NIC times-
tamps PTP packets in the NIC for both incoming and outgo-
ing packets.
The PTP network was mostly idle except when we intro-
duced network congestion. Since PTP uses UDP datagrams
for time synchronization, the precision of PTP can vary re-
lying on network workloads. As a result, we introduced net-
work workloads between servers using iperf [11]. Each
server occasionally generated MTU-sized UDP packets des-
tined for other servers so that PTP messages could be
dropped or arbitrarily delayed.
To measure how DTP responds to varying network con-
ditions, we used the same heavy load that we used for
PTP and also changed the BEACON interval during experi-
ments from 200 to 1200 cycles, which changed the Ethernet
frame size from 1.5kB to 9kB. Recall that when a link is
fully saturated with MTU-sized (Jumbo) packets, the mini-
mum BEACON interval possible is 200 (1200) cycles.
6.3 Results
Figure 6 and 7 show the results: We measured precision
of DTP in Figure 6a-c, PTP in Figure 6d-f, and the DTP dae-
mon in Figure 7. For all results, we continuously synchro-
nized clocks and measured the precision (clock offsets) over
at least a two-day period in Figure 6 and at least a few-hour
period in Figure 7.
Figures 6a-b demonstrate that the clock offsets between
any two directly connected nodes in DTP never differed by
more than four clock ticks; i.e. offsets never differed by
more than 25.6 nanoseconds (4T D = 4 × 6.4 × 1 = 25.6):
Figures 6a and b show three minutes out of a two-day mea-
surement period and Figure 6c shows the distribution of the
measured offsets with node S3 for the entire two-day period.
The network was always under heavy load and we var-
ied the Ethernet frame size by varying the BEACON interval
between 200 cycles in Figure 6a and 1200 cycles in Fig-
ure 6b. DTP performed similarly under idle and medium
load. Since we measured all pairs of nodes and no off-
set was ever greater than four, the results support that pre-
cision was bounded by 4T D for nodes D hops away from
each other. Figure 7 shows the precision of accessing a
DTP counter via a DTP daemon: Figure 7a shows the raw
offsetsw and Figure 7b shows the offsetsw after applying a
moving average algorithm with a window size of 10. We
applied the moving average algorithm to smooth the effect
of the non-determinism from the PCIe bus, which is shown
as occasional spikes. The offset between a DTP daemon in
software and the DTP counter in hardware was usually no
more than 16 clock ticks (≈ 102.4ns) before smoothing,
and was usually no more than 4 clock ticks (≈ 25.6ns) after
smoothing.
Figures 6d-f show the measured clock offsets between
each node and the grandmaster timeserver using PTP. Each
ﬁgure shows minutes to hours of a multi-day measurement
period, enough to illustrate the precision trends. We varied
the load of the network from idle (Figure 6d), to medium
load where ﬁve nodes transmitted and received at 4 Gbps
(Figure 6e), to heavy load where the receive and trans-
mit paths of all links except S11 were fully saturated at 9
Gbps (Figure 6f). When the network was idle, Figure 6d
showed that PTP often provided hundreds of nanoseconds
of precision, which matches literature [7, 17]. When the
network was under medium load, Figure 6e showed the
offsets of S4 ∼ S8 became unstable and reached up to 50
microseconds. Finally, when the network was under heavy
load, Figure 6f showed that the maximum offset degraded
to hundreds of microseconds. Note that we measured, but
do not report the numbers from the PTP daemon, ptpd, be-
cause the precision with the daemon was the same as the
precision with the hardware clock, PHC. Also, note that all
reported PTP measurements include smoothing and ﬁltering
algorithms.
There are multiple takeaways from these results.
1. DTP more tightly synchronized clocks than PTP.
2. The precision of DTP was not affected by network
workloads. The maximum offset observed in DTP did
not change either when load or Ethernet frame size (the
BEACON interval) changed. PTP, on the other hand,
was greatly affected by network workloads and the pre-
cision varied from hundreds of nanoseconds to hun-
dreds of microseconds depending on the network load.
3. DTP scales. The precision of DTP only depends on
the number of hops between any two nodes in the net-
work. The results show that precision (clock offsets)
were always bounded by 4T D nanoseconds.
4. DTP daemons can access DTP counters with tens of
nanosecond precision.
5. DTP synchronizes clocks in a short period of time,
within two BEACON intervals. PTP, however, took
about 10 minutes for a client to have an offset below
Data Rate Encoding Data Width Frequency
125 MHz
Period ∆
8 ns
25
156.25 MHz 6.4 ns 20
5
1562.5 MHz 0.64 ns 2
625 MHz
1.6 ns