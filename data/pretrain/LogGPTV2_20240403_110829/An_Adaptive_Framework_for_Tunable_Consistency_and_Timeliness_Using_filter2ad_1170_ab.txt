Server     Gateway
Maestro/Ensemble
Figure 2. Timed consistency handlers in
AQuA
in time, they all see the effects of the updates in the same se-
quential order. The order in which an update is committed
by the replicas is determined by its Global Sequence Num-
ber (GSN), which is assigned by the leader of the primary
group. The leader merely serves as the sequencer and does
not actually service the client’s request.
We now describe how the updates and read-only requests
are processed by the replicas. The processing depends on
whether the replica is a primary or secondary replica. All
of this processing is done at the middleware layer, within
the gateway handler of the replicas. Each gateway handler
maintains a pair of variables, my GSN and my CSN, which
are used by the protocol to provide sequential consistency.
my GSN is the replica’s local view of the current GSN, and
my CSN is the replica’s commit sequence number (CSN),
which indicates the GSN of the most recent update commit-
ted by the replica. The commit sequence number increases
strictly in monotonic order, and a replica is assumed to have
committed every update whose global sequence number is
less than or equal to the value of its my CSN. Our protocol
ensures that the consistency guarantees are preserved even
when replica failures occur. This is done by handling the
failures of the sequencer and the lazy publisher, which play
a crucial role in providing sequential consistency semantics.
However, we omit the details of the failure handling in this
paper due to the space constraint.
4.1.1. Update Operations. The update operations are sent
to all members of the primary group, including the sequencer.
When the sequencer receives an update request from a client,
it advances the GSN and broadcasts the GSN assignment for
the request to all the other members of the primary group.
A non-leader primary replica can service an update re-
quest immediately, provided it has already received the GSN
broadcast for that request from the sequencer. Otherwise,
the replica stores the request in a buffer and processes it
upon receiving the GSN assignment from the sequencer.
If the update request is in sequential order, the replica ad-
vances its CSN, and then delivers the update request to the
server application.
If, however, the request is out of the
global order, the replica buffers the request and commits it at
a later time, after the intermediate requests have been com-
mitted.
4.1.2. Read-Only Operations.
In our sequential consis-
tency model, a read-only request is sent to the sequencer
and a subset of the primary and secondary replicas. Dif-
ferent replicas may service different sets of read-only re-
quests. When the sequencer receives a read-only request,
the leader broadcasts the current value of the GSN to the pri-
mary and secondary replicas, without advancing the GSN.
When a non-leader primary or a secondary replica receives
a read-only request from a client, it buffers the request un-
til it receives the GSN assignment for the request from the
sequencer. The replicas use this GSN to measure the stal-
eness of their state. To determine its staleness, the replica
ﬁrst sets its value of my GSN to the value of the GSN broad-
cast by the sequencer. The replica then computes the value
of (my GSN - my CSN). This value is a measure of how
stale the state of the replica is. If the replica’s state is less
stale than the threshold speciﬁed by the client in its QoS
speciﬁcation, the replica can service the client’s request im-
mediately. However, a secondary replica may have a state
that is more stale than the staleness threshold speciﬁed by
the client, because the secondary replicas update their state
only when they receive the state update from the lazy pub-
lisher. In that case, the replica performs a deferred read by
buffering the read request and responding to the client im-
mediately after receiving the next state update from the lazy
publisher.
5. Probabilistic Model-Based Replica Selection
Having described the protocol processing in the server-
side gateway handler, we now describe the processing per-
formed in the client-side handler to meet the QoS speciﬁ-
cation of the client. Each client expresses its constraints in
the form of a QoS speciﬁcation that includes the response
time constraint, d; the minimum probability of meeting this
constraint, Pc(d); and the maximum staleness, a, that it can
tolerate in its response. If a response fails to meet the dead-
line constraint of the client, then it results in a timing failure
for the client. Hence, one of the important responsibilities of
the client handler is to select an appropriate subset of repli-
cas to service the clients, and reduce the occurrence of such
timing failures.
A simple approach would be to allocate all the available
replicas to service a single client. However, such an ap-
proach is not scalable, as it increases the load on all the
replicas and results in higher response times for the remain-
ing clients. On the other hand, assigning a single replica to
service each client allows us to service multiple clients con-
currently. However, should a replica fail while servicing a
request, the failure could result in an unacceptable delay for
the client being serviced. Hence, neither approach is suit-
able when a client has speciﬁc timing constraints and when
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply. 
failure to meet the constraints results in a penalty for the
client. Therefore, we need a method that attempts to pre-
vent the occurrence of such timing failures for a client by
selecting replicas from the available replica pool, based on
an understanding of the client’s QoS requirements and the
responsiveness of the replicas.
In our model, the constraints speciﬁed by a client apply
only for the read transactions invoked by the client. For an
update transaction, the only constraint that applies is that
it has to be committed by the replicas in a manner that re-
spects the ordering guarantee associated with the service.
Hence, our selection algorithm handles an update request of
a client by simply multicasting the request to all the primary
replicas. The handler on the server side takes care of com-
mitting these updates in the appropriate order, as described
in Section 4 for the sequential ordering case. For the read-
only requests, the selection algorithm has to choose from
among the primary and secondary replicas based on their
ability to meet the client’s temporal requirements, as well
as on whether the state of the replica is within the staleness
threshold speciﬁed by the client. However, the uncertainty
in the environment and in the availability of the replicas due
to transient overload and failures makes it impossible for a
client to know with certainty if a set of replicas can meet
its deadline. Further, a client can be certain that the state of
the primary replicas is always up-to-date, because of the im-
mediate update propagation. However, it cannot make such
guarantees about the state of the secondary replicas, which
update their state only when they receive the lazy update
propagated by the lazy publisher.
Hence, our selection approach makes use of probabilistic
models to estimate a replica’s staleness and to predict the
probability that the replica will be able to meet the client’s
deadline. These models make their prediction based on in-
formation gathered by monitoring the replicas at runtime.
A selection algorithm then uses this online prediction to
choose a subset of replicas that can together meet the client’s
timing constraints with at least the probability requested by
the client. We will now describe our probabilistic models
and replica selection algorithm. They enhance the selection
approach we presented in [5], which made the assumption
that the replicas were stateless. We ﬁrst deﬁne the notation
we use to explain our model.
Let t denote the time at which a request is transmitted.
Since replicas are selected at the time a request is transmit-
ted, we also use t to denote the time at which the replica se-
lection is done. Let Ri be the random variable that denotes
the response time of replica i. Let Ai(t) denote the stale-
ness of the state of replica i at time t, and P (Ai(t) ≤ a) be
the probability that the state of replica i at time t is within
the staleness threshold, a, speciﬁed by the client. We call
this the staleness factor for replica i. Let P (Ri ≤ d) be the
probability that a response from replica i will be received by
the client within the client’s deadline, d, and PK(d) be the
probability that at least one response from the set K, con-
sisting of k > 0 replicas, will arrive by the client’s deadline,
d. The probability that a replica can meet the client’s time
constraint, d, and thereby prevent a timing failure, depends
on whether the replica is functioning and has a state that can
satisfy the client-speciﬁed staleness threshold. We can make
use of these individual probabilities to choose a subset K of
replicas such that PK(d) ≥ Pc(d). The replicas in the set K
will then form the ﬁnal set selected to service the request.
5.1. Modeling the Response Time Distribution
We now derive the expression for PK(d), which is the
probability that at least one response from the replicas in set
K arrives by the client’s deadline, d, and thereby avoids the
occurrence of a timing failure. The set K is made up of a
subset Kp of primary replicas and a subset Ks of secondary
replicas (i.e., K = Kp ∪ Ks). While each replica in K
processes the client’s request and returns its response, only
the ﬁrst response received for a request is delivered to the
client. Hence, a timing failure occurs only if no response is
received from any of the replicas in the selected set K within
d time units after the request was transmitted. Therefore, we
have
PK(d) = 1 − P (no replica i ∈ K (cid:9) Ri ≤ d)
In our work, we assume that the response times of the repli-
cas are independent, because they process their requests in-
dependently. While this assumption may not be strictly true
in some cases (e.g., if the network delays are correlated), it
does result in a model that is fast enough to be solved on-
line, especially for the time-sensitive applications we target
in our work. Furthermore, the results we present in Section 6
show that the resulting model makes reasonably good pre-
dictions for the scenarios we have considered. Thus, using
the independence assumption, we obtain
PK (d) = 1 − P (no i ∈ Kp (cid:3) Ri ≤ d) · P ( no j ∈ Ks (cid:3) Rj ≤ d)
(1)
5.1.1. Case 1: Primary Replicas. In Section 4, we men-
tioned that the update requests of the clients are propagated
to the primary group immediately. Hence, for a primary
replica i, the staleness factor P (Ai(t) ≤ a) = 1, and the
replica always has a state that can satisfy the staleness thresh-
old of the client. Therefore, in the case of the primary repli-
cas, we have
P (no i ∈ Kp (cid:3) Ri ≤ d) = Y
i∈Kp
P (Ri > d) = Y
i∈Kp
(1 − F I
Ri
(d))
where F I
Ri denotes the response time distribution function
for replica i, given that it can respond immediately to a read
request without waiting for a state update.
(2)
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply. 
5.1.2. Case 2: Secondary Replicas. The response time of
a secondary replica depends on whether it has a state that
can satisfy the client-speciﬁed staleness threshold, a. As
mentioned in Section 4.1.2, if the replica’s state is more
stale than the staleness threshold speciﬁed by the client, the
replica has to buffer the request until it receives the next lazy
update, at which point it can respond to the request. There-
fore, for a replica j ∈ Ks,
P (Rj > d) = P (Rj > d|Aj(t) ≤ a) · P (Aj(t) ≤ a)
+P (Rj > d|Aj(t) > a) · P (Aj(t) > a)
Since the lazy update is propagated to all the secondary
replicas at the same time, it is reasonable to assume that their
degrees of staleness at the time of request transmission, t,
are identical. Hence, we associate staleness with the entire
secondary group of replicas, instead of with an individual
replica j as above. We use As(t) to denote the staleness
of the secondary group at the time of request transmission
t, and express the probability that no secondary replica can
respond within the deadline d as follows.
P (no j ∈ Ks (cid:2) Rj ≤ d) = (cid:20) Yj∈Ks
+(cid:20) Yj∈Ks
P ( no j ∈ Ks (cid:2) Rj ≤ d) = (cid:20) Yj∈Ks
(cid:20) Yj∈Ks
P (Rj > d|As(t) ≤ a)(cid:21) · P (As(t) ≤ a)
P (Rj > d|As(t) > a)(cid:21) · P (As(t) > a)
(1 − F
(d))(cid:21) · P (As(t) ≤ a) +
(d))(cid:21) · (1 − P (As(t) ≤ a))
I
Rj
(1 − F
D
Rj
Ri and F D
(3)
where F I
Rj , as before, denotes the response time distribution
function for the replica j, given that it can respond imme-
diately to a request without waiting for a state update, and
F D
Rj is the response time distribution function, given that the
replica performs a deferred read.
We now describe how we compute the staleness factor,
P (As(t) ≤ a), for the secondary replicas, and then fol-
low this with a description of how we compute the values
of the response time distribution functions F I
Ri for
a replica i.
5.1.3. Staleness Factor. The staleness of a secondary replica,
at the instant t, is the number of update requests that have
been received by the primary group since the time of the
last lazy update. Let tl denote the duration between the time
of request transmission, t, and the time of the last lazy up-
date. Let Nu(tl) be the total number of update requests re-
ceived by the primary group from all the clients in the dura-
tion tl. Since As(t) = Nu(tl), we have P (As(t) ≤ a) =
P (Nu(tl) ≤ a). Our approach estimates the staleness of
the secondary replicas based on a probabilistic model, rather
than using the prohibitively costlier method of probing the
primary group at the time of request transmission in order
to obtain the value of Nu(tl). Using the assumption that the
arrival of update requests from the clients follows a Poisson
distribution with rate λu, we obtain
P (As(t) ≤ a) = P (Nu(tl) ≤ a) =
(λutl)ne−λutl
n!
a
X
n=0
(4)
Therefore, the staleness of a secondary replica can be deter-
mined probabilistically if we know the arrival rate of the up-
date requests and the time elapsed since the last lazy update.
In Section 5.4.1, we will explain how we measure these two
parameters at runtime. Although we have assumed Pois-
son arrivals in our work, it should be possible to evaluate
P (Nu(tl) ≤ a) for the case in which the arrival of update
requests follows a distribution that is not Poisson. Finally,
we can use the expressions in Equations 2, 3, and 4 in Equa-
tion 1 to evaluate PK(d).
5.2. Evaluating the Response Time Distribution
We now explain how we determine the values of the con-
(d), for
ditional response time distributions, F I
Ri
a replica i. To do this, we extend the method we described in
[5] for the stateless case, which made use of the performance
history recorded by online performance monitoring to com-
pute the value of the distribution function for a replica i.
(d) and F D
Ri
5.2.1. Immediate Reads. When a replica can respond to a
request without waiting for a state update, as in the case of
a primary replica or a secondary replica that has the appro-
priate state, the response time random variable for a replica
i is given by Equation 5:
Ri = Si + Wi + Gi
(5)
where Si is the random variable denoting the service time
for a read request serviced by replica i; Wi is the random
variable denoting the queuing delay experienced by a re-
quest waiting to be serviced by i (and it includes the time
the replica spends waiting for the sequencer to send the GSN
for the request); and Gi is the random variable denoting the
two-way gateway-to-gateway delay between the client and
replica i. The service time and queuing delay are speciﬁc to
the individual replicas, while the gateway delay is speciﬁc
to a client-replica pair.
5.2.2. Deferred Reads. In the case in which the replica has
to wait for a state update before responding to the request,
the response time random variable is given by Equation 6,