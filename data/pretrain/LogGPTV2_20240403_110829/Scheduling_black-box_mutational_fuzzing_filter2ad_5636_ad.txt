writes to the log each time a crash occurs. Log entries are
recorded as 5-tuples of the form (pi, si, time stamp, #runs,
mutation identiﬁer).
In our implementation, we fuzz with zzuf, one of the most
popular open-source fuzzers. zzuf generates a random input
from a seed ﬁle as described in §2.1. The randomization in
zzuf can be reproduced given the mutation identiﬁer, thus
enabling us to reproduce a crashing input from its seed ﬁle
and the log entry associated with the crash. For example, an
output tuple of (FFMpeg, a.avi, 100, 42, 1234) speciﬁes that
the program FFMpeg crashed at the 100-th second with an
input ﬁle obtained from “a.avi” according to the mutation
identiﬁer 1234. Interested readers may refer to zzuf [16] for
details on mutation identiﬁers and the actual implementation.
The deterministic nature of zzuf allows FuzzSim to triage
bugs after completing all fuzz runs ﬁrst. In other words,
FuzzSim does not compute bug identiﬁers during fuzzing
and instead re-derives them using the log. This does not
aﬀect any of our algorithms since none of them relies on the
actual IDs. In our experiments, we have turned oﬀ address
space layout randomization (ASLR) in both the fuzzing and
the triage steps in order to reproduce the same crashes.
Triage. The second step of FuzzSim maps crashing inputs
found during fuzzings into bugs. At a high level, the triage
phase takes in the list of 5-tuples (pi, si, time-stamp, #runs,
mutation identiﬁer) logged during the fuzzing step and out-
puts a new list of 5-tuples of the form (pi, si, time-stamp,
#runs, bug identiﬁer). More speciﬁcally, FuzzSim replays
each recorded crash under a debugger to collect stack traces.
If FuzzSim does not detect a crash during a particular replay,
then we classify that test case to be a non-deterministic bug
and discard it.
We then use the collected stack traces to produce bug
identiﬁers, essentially hashes of the stack traces. In particular,
we use the fuzzy stack hash algorithm [19], which identiﬁes
bugs by hashing the normalized line numbers from a stack
trace. With this algorithm, the number of stack frames to
hash has a signiﬁcant inﬂuence on the accuracy of bug triage.
For example, taking the full stack trace often leads to mis-
classifying a single bug into multiple bugs, whereas taking
only the top frame can easily lead to two diﬀerent bugs being
mis-classiﬁed as one. To match the state of the art, FuzzSim
uses the top 3 frames as suggested in [19]. We stress that even
though inaccurate bug triage may still occur with this choice
of parameter, perfecting bug triage techniques is beyond the
scope of this paper.
Simulation. The last step simulates a fuzz campaign on
the collected ground-truth data from the previous steps us-
ing a user-speciﬁed scheduling algorithm. More formally,
the simulation step takes in a scheduling algorithm and a
list of 5-tuples of the form (pi, si, timestamp, #runs, bug
identiﬁer) and outputs a list of 2-tuples (timestamp, #bugs)
that represent the accumulated time before the correspond-
ing number of unique bugs are observed under the given
scheduling algorithm.
Since FuzzSim can simulate any scheduling algorithm in
an oﬄine fashion using the pre-recorded ground-truth data,
it enables us to eﬃciently compare numerous scheduling
algorithms without actually running a large number of fuzz
campaigns. During replay, FuzzSim outputs a timestamp
whenever it ﬁnds a new bug. Therefore, we can easily plot
and compare diﬀerent scheduling algorithms by comparing
the number of bugs produced under the same time budget.
We summarize FuzzSim’s three-step algorithm below.
Fuzzing: ({(pi, si)}, T )
→ {pi, si, timestamp, #runs, mutation id}
Triage: {(pi, si, timestamp, #runs, mutation id)}
→ {(pi, si, timestamp, #runs, bug id)}
Simulation: {(pi, si, timestamp, #runs, bug id)}
→ {(timestamp, #bugs)}
Algorithm 1: FuzzSim algorithms.
Implementation & Open Science
5.2
We have implemented our data collection and bug triage mod-
ules in approximately 1,000 lines of OCaml. This includes the
capability to run and collect crash logs from Amazon EC2.
We used zzuf version 0.13. Our scheduling engine is also
implemented in OCaml and spans about 1,600 lines. This
covers the 26 online and the 2 oﬄine algorithms presented
in this paper.
We invite our fellow researchers to become involved in
this line of research. In support of open science, we release
both our datasets and the source code of our simulator at
http://security.ece.cmu.edu/fuzzsim/.
6 Evaluation
To evaluate the performance of the 26 algorithms presented
in §4, we focus on the following questions:
1. Which scheduling algorithm works best for our datasets?
2. Why does one algorithm outperform the others?
3. Which of the two epoch types—ﬁxed-run or ﬁxed-time—
works better, and why?
6.1 Experimental Setup
Our experiments were performed on Amazon EC2 instances
that have been conﬁgured with a single Intel 2GHz Xeon
CPU core and 4GB RAM each. We used the most recent
Debian Linux distribution at the time of our experiment
(April 2013) and downloaded all programs from the then-
latest Debian Squeeze repository. Speciﬁcally, the version of
FFMpeg we used is SVN-r0.5.10-4:0.5.10-1, which is based
on a June 2012 FFMpeg release with Debian-speciﬁc patches.
6.2 Fuzzing Data Collection
Our evaluation makes use of two datasets: (1) FFMpeg
with 100 diﬀerent input seeds, and (2) 100 diﬀerent Linux
applications, each with a corresponding input seed. We
refer to these as the “intra-program” and the “inter-program”
datasets respectively.
For the intra-program dataset, we downloaded 10, 000
video/image sample ﬁles from the MPlayer website at http:
//samples.mplayerhq.hu/. From these samples, we selected
100 ﬁles uniformly at random and took them as our input
Dataset
Intra-program
636,998,978
Inter-program 4,868,416,447
#runs
#crashes #bugs
200
223
906,577
415,699
Table 1: Statistics from fuzzing the two datasets.
Figure 2: Distribution of the number of bugs per conﬁgura-
tion in each dataset.
(a) Intra-program.
Figure 3: Distribution of bug overlaps across multiple seeds
for the intra-program dataset.
seeds. The collected seeds include various audio and video
formats such as ASF, QuickTime, MPEG, FLAC, etc. We
then used zzuf to fuzz FFMpeg with each seed for 10 days.
For the inter-program dataset, we downloaded 100 diﬀer-
ent ﬁle conversion utilities in Debian. To select these 100
programs, we ﬁrst enumerated all ﬁle conversion packages
tagged as “use::converting” in the Debian package tags in-
terface (debtags). From this list of packages, we manually
identiﬁed 100 applications that take a ﬁle name as a com-
mand line argument. Then we manually constructed a valid
seed for each program and the actual command line to run it
with the seed. After choosing these 100 program-seed pairs,
we fuzzed each for 10 days as well. In total, we have spent
48,000 CPU hours fuzzing these 200 conﬁgurations.
To perform bug triage, we identiﬁed and re-ran every
crashing input from the log under a debugger to obtain stack
traces for hashing. After triaging with the fuzzy stack hash
algorithm described in §5.1, we found 200 bugs from the
intra-program dataset and 223 bugs from the inter-program
dataset. Table 1 summarizes the data collected from our
experiments. The average fuzzing throughput was 8 runs
per second for the intra-program dataset and 63 runs per
second for the inter-program dataset. This diﬀerence is due
to the higher complexity of FFMpeg when compared to the
programs in the inter-program dataset.
6.3 Data Analysis
What does the collected fuzzing data look like? We studied
our data from fuzzing and triage to answer two questions: (1)
How many bugs does a conﬁguration trigger? (2) How many
bugs are triggered by multiple seeds in the intra-program
dataset?
We ﬁrst analyzed the distribution of the number of bugs
in the two datasets. On average, the intra- and the inter-
program datasets yielded 8.2 and 2.4 bugs per conﬁguration
respectively. Figure 2 shows two histograms, each depict-
(b) Inter-program.
Figure 4: The average number of bugs over 100 runs for
each scheduling algorithm with error bars showing a 99%
conﬁdence interval. “ft” represents ﬁxed-time epoch; “fr”
represents ﬁxed-run epoch; “e” represents -Greedy; “w” rep-
resents Weighted-Random.
ing the number of occurrences of bug counts. There is a
marked diﬀerence in the distributions from the two datasets:
64% of conﬁgurations in the inter-program dataset produce
no bugs, whereas the corresponding number in the intra-
program dataset is 15%. We study the bias of the bug count
distribution in §6.4.
Second, we measured how many bugs are shared across
seeds in the intra-program dataset. As an extreme case, we
found a bug that was triggered by 46 seeds. The average
number of seeds leading to a given bug is 4. Out of the 200
bugs, 97 were discovered from multiple seeds. Figure 3
illustrates the distribution of bug overlaps. Our results
suggest that there is a small overlap in the code exercised
by diﬀerent seed ﬁles even though they have been chosen
to be of diﬀerent types. Although this shows that our bug
disjointness assumption in the WCCP model does not always
hold in practice, the low average number of seeds leading to
a given bug in our dataset means that the performance of
our algorithms should not have been severely aﬀected.
6.4 Simulation
We now compare the 26 scheduling algorithms based on the
10-day fuzzing logs collected for the intra- and inter-program
datasets. To compare the performance of scheduling algo-
rithms, we use the total number of unique bugs reported
by the bug triage process. Recall from §4.4 that these al-
gorithms vary across three dimensions: (1) epoch types, (2)
belief metrics, and (3) MAB algorithms. For each valid com-
bination (see Table 2), we ran our simulator 100 times and
averaged the results to study the eﬀect of randomness on
each scheduling algorithm. In our experiments, we allocated
10 seconds to each epoch for ﬁxed-time campaigns and 200
runs for ﬁxed-run campaigns. For the -Greedy algorithm,
we chose  to be 0.1.
Table 2 summarizes our results. Each entry in the table
represents the average number of bugs found by 100 sim-
0204060010203040#bugscountIntra−Program02040600102030#bugscountInter−Program0255075010203040#bugscount406080100fr.e.densityfr.e.ewtfr.e.ratefr.e.rgrfr.e.rpmfr.round.robinfr.uni.randfr.w.densityfr.w.ewtfr.w.ratefr.w.rgrfr.w.rpmft.e.densityft.e.ewtft.e.rateft.e.rgrft.e.rpmft.round.robinft.uni.randft.w.densityft.w.ewtft.w.rateft.w.rgrft.w.rpm#bugs60100140fr.e.densityfr.e.ewtfr.e.ratefr.e.rgrfr.e.rpmfr.round.robinfr.uni.randfr.w.densityfr.w.ewtfr.w.ratefr.w.rgrfr.w.rpmft.e.densityft.e.ewtft.e.rateft.e.rgrft.e.rpmft.round.robinft.uni.randft.w.densityft.w.ewtft.w.rateft.w.rgrft.w.rpm#bugsDataset
Epoch MAB algorithm
#bugs found for each belief
RPM EWT Density Rate RGR
Intra-Program
Inter-Program
Fixed-Run
Fixed-Time
Fixed-Run
Fixed-Time
51
67
90
90
72
72
-Greedy
Weighted-Random
Uniform-Random
EXP3.S.1
Round-Robin
-Greedy
Weighted-Random
Uniform-Random
EXP3.S.1
Round-Robin
-Greedy
Weighted-Random
Uniform-Random
EXP3.S.1
Round-Robin
-Greedy
126
Weighted-Random 152
Uniform-Random
EXP3.S.1
Round-Robin
88
93
32
85
109
100
58
108
89
135
41
94
164
167
117
165
77
84
94
94
119
131
158
157
87
84
72
58
74
51
58
94
95
94
89
92
89
72
90
111
100
158
161
158
Table 2: Comparison between scheduling algorithms.