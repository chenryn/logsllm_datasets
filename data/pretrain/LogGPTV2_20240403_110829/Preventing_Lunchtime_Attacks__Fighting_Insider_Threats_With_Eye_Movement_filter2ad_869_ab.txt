4
Pupil Diameter - MeanPupil Diameter - MinPupil Diameter - MaxPupil-Diameter max-minPupil Diameter - std-devPairwise Speed - MeanPairwise Speed - MaxAcceleration - MaxPairwise Speed - std-devDuration of SaccadeDuration of FixationAcceleration - MeanSaccade DirectionDistance from Center - MeanMax Pairwise DistanceDistance from Center - std-devDistance from Center - MaxMax Pairwise Distance XMax Pairwise Distance YDistance from Center - MinDistance from previous FixationPupil Diameter - MeanPupil Diameter - MinPupil Diameter - MaxPupil-Diameter max-minPupil Diameter - std-devPairwise Speed - MeanPairwise Speed - MaxAcceleration - MaxPairwise Speed - std-devDuration of SaccadeDuration of FixationAcceleration - MeanSaccade DirectionDistance from Center - MeanMax Pairwise DistanceDistance from Center - std-devDistance from Center - MaxMax Pairwise Distance XMax Pairwise Distance YDistance from Center - MinDistance from previous Fixation1.00.80.60.40.20.00.20.40.60.81.0Feature
Pupil features
Pupil Diameter - Max
Pupil Diameter - Mean
Pupil Diameter - Min
Pupil Diameter - Range
Pupil Diameter - Stdev
Temporal features
Acceleration - Max
Acceleration - Mean
Duration of Saccade
Duration of Fixation
Pairwise Speed - Max
Pairwise Speed - Mean
Pairwise Speed - Stdev
Spatial features
Distance from Center - Max
Distance from Center - Mean
Distance from Center - Min
Distance from Center - Stdev
Distance from previous ﬁxation
Max Pairwise Distance
Max Pairwise Distance X only
Max Pairwise Distance Y only
Saccade Direction
RMI
F R W
19.84% × ×
20.27% × ×
20.26% × ×
× ×
1.19%
×
0.98%
2.49%
0.35%
1.09%
0.9%
4.95%
5.36%
1.77%
1.2%
2.52%
0.72%
1.21%
0.66%
1.23%
1.06%
0.84%
0.08%
× × ×
×
×
× × ×
×
×
× × ×
× × ×
×
×
×
×
×
×
×
×
×
×
× × ×
×
×
×
×
× × ×
×
×
TABLE I: List of pupil, temporal and spatial features that are
computed for each ﬁxation. For each feature we report the
relative mutual information (RMI) with the user ID. A value
of 0 indicates that the feature carries no information about the
user ID, while a value of 1 means that the feature completely
deﬁnes the user ID. For each feature we report whether it is
included in the Full (F), Reduced (R) or without-pupil (W)
feature set.
the spatial distribution of samples within this area can still
be different. If a person’s gaze is steady the samples will be
clustered closely around the ﬁxation center, with few samples
outside of this group. If a person has trouble focussing their
gaze the samples will be spread more evenly. We compute both
the distance between each raw sample and the center point
as well as the distance between each pair of raw samples. As
some movements may be more pronounced in the vertical or
horizontal direction we also make this distinction. The distance
between two ﬁxations (as measured by the euclidean distance
between their center points) allows us to measure how many
points between two areas of interest (i.e., target stimuli) are
actively focused and processed by the subject. The saccade
direction, measured in degrees, allows a distinction between
stepwise and more diagonal eye movements.
D. Determining Feature Quality
Having a measure of feature quality is important for two
reasons: (a) to be able to select the best features when the entire
set is too high-dimensional and (b) to gain better insights into
why the biometric works. Initially an amount of uncertainty is
associated with the user ID (its entropy). This amount depends
on the number of classes (i.e., users) and the distribution of the
samples between users. Each feature reveals a certain amount
of information about the user ID, this amount can be measured
through the mutual information (MI). In order to measure the
mutual information relative to the entire amount of uncertainty
we use the relative mutual information (RMI) which measures
the percentage of entropy that is removed from the user ID
when a feature is known [25]. The RMI is deﬁned as
H(uid) − H(uid|F )
H(uid)
RM I(uid, F ) =
where H(A) is the entropy of A and H(A|B) denotes the entropy
of A conditioned on B. In order to calculate the entropy of a
feature it has to be discrete. As most features are continuous
we perform discretization using an Equal Width Discretization
(EWD) algorithm with 20 bins [26]. This algorithm typically
produces good results without requiring supervision. In order
to limit the drastic effect that outliers can have when using
this approach we use the 1st and 99th percentile instead of the
minimal and maximum values to compute the bin boundaries. A
high RMI indicates that the feature is distinctive on its own, but
it is important to consider the correlation between features as
well when choosing a feature set. Additionally, several features
that are not particularly distinctive on their own may be more
useful when combined.
E. Feature Selection
Table I lists the RMI for each of our features. The static
pupil diameter features (i.e., min, mean and max) share the
most information with the user ID. The dynamic pupil diameter
features (i.e., the standard deviation and the min-max difference)
are less distinctive, which suggests that the pupil diameter is
more a result of different genders, ethnicities and eye shapes
than a behavioral feature.
While the behavioral features, both temporal and spatial
ones, show a lower distinctiveness than the pupil diameter they
still contribute signiﬁcant amounts of information. The fact
that both peak speed and acceleration exhibit a comparatively
high RMI with the user ID shows that we accurately model the
distinctive capabilities of saccadic intrusions and microsaccades.
When selecting which feature candidates should form the
ﬁnal feature set there are several aspects that have to be
considered: Each of the features should be hard to imitate
in a given threat model. As we focus on insider threats this
rules out features that can be easily observed and copied. Given
the insights from Section II we suspect that it may be possible
for a sophisticated attacker to modify his own pupil diameter
to a certain degree. In order to address this issue we also
investigate the performance of a feature set that does not make
use of the pupil diameter features. When putting the system
into operation it can then be decided which feature set should
be used, depending on the threat model and the capabilities of
potential attackers. Besides the security considerations it is also
important to note that a high-dimensional feature set will slow
down the classiﬁcation and cause a higher resource consumption.
If the feature redundancy is high or many non-distinctive
features are included in the original set feature selection is
particularly useful. Figure 4 shows that the correlation between
features belonging to the same group (i.e., pupil diameter,
temporal or spatial) is relatively high, while the inter-group
correlation is considerably lower. This suggests that all three
5
groups contribute to the distinctiveness of the biometric and
no group can be replaced entirely by another. Therefore an
optimal reduced feature set would most likely contain features
from all three groups. In order to determine this feature set we
used the Minimum Redundancy Maximum Relevance (mRMR)
algorithm[27]. This algorithm selects those features that share
a high amount of information with the classiﬁcation variable
(i.e., the user ID) while showing low redundancy with other
features in the set. In order to achieve a good trade-off between
classiﬁcation speed and accuracy we chose the best ten features
as computed by the algorithm. The list of those features can
be seen in Table I. In line with our hypothesis features from
all groups are part of this set. This also makes sophisticated
imitation attacks more difﬁcult, as a number of very distinct
features have to be emulated simultaneously. We will discuss
the impact of using different feature sets in Section VI-C.
V. EXPERIMENTAL DESIGN
In this section we give an overview of our design goals
and show how our experimental design meets those goals.
We describe our test subject population, discuss how features
change over time, as well as the best way to capture these
changes.
A. Design Goals
With the experiments described in this section we test the
hypothesis that a biometric based on gaze tracking is feasible.
The goal is to analyze how well an authentication system can
distinguish users within a test group, and to identify what
impact, if any, training and knowledge transfer has on the
authentication process.
In order to design experiments that show whether or not
gaze tracking is suitable as an authentication mechanism, we
have to determine which tasks the test subjects should perform
while they are being monitored. One option is to give them
an entirely free environment in which the subjects can choose
what to do for the duration of the experiment. This is probably
the experiment that best captures actual user behavior, but
since it is likely that each subject will choose a different set
of tasks, it is very hard to guarantee that the distinguishing
power of the resulting classiﬁer is really capturing differences
in users, rather than differences in behavior or tasks. While we
designed our features to be as task-independent as possible it is
impossible to rule out that some differences are due to the user-
chosen task. If each user chose a different task, which possibly
results in speciﬁc feature characteristics, this would lead to
an overestimation of classiﬁcation accuracy, as the classiﬁer
performs task distinction instead of user distinction. Conversely,
a ﬁxed task for all users means that any differences between
the datasets are due to differences between users.
Another approach is to ﬁx a set of general tasks and let all
the users perform those the way they prefer. This will limit the
inﬂuence of user-chosen tasks but the visual stimuli presented
to the subjects will still be different. For example if the subjects
are asked to browse the web, but not restricted in what pages
to visit or speciﬁcally what to read, different subjects would
have very different experiences. Even if the task is as simple
as watching a movie, different subjects will focus on different
things and the resulting classiﬁcation might be biased by genre
preference and other factors.
Fig. 5: Experiment structure. Each session is divided into three
experiments, each of which is repeated a number of times. The
entire session is repeated after two weeks, and again an hour
after the second repetition.
In order to overcome these sources of error we deﬁne
a speciﬁc set of tasks that all users must complete. Our
goal is to determine whether the users’ eye movements are
distinguishable, even if they are completing the same task the
same way with the same knowledge. If this is indeed the case
that means that there are inherent differences between users that
can not be attributed to different ways of completing a single
task. Nevertheless, as we do not make any assumptions about
the experimental design when choosing the features the results
are transferable to more general settings (e.g., web browsing
or writing e-mails). We realize our design goals through a set
of experiments.
B. Experiment Structure
We ﬁrst introduce terminology to make it easier to refer
to different parts of our interaction with test subjects, please
see Figure 5 for a visualization. We refer to one sitting of a
test subject as a session. Two weeks after the ﬁrst session, the
test subject comes back for a second session. This is done
to make sure our results are consistent over time. To verify
that our results are not only consistent over longer periods
but also across two subsequent sessions on the same day, our
test subjects do a third session about an hour after completing
session 2. All three sessions are identical, and each consists of
three different experiments.
Each experiment has a similar structure. The test subject
is initially presented an empty screen with a grey background.
Once the experiment begins, a red dot with a white center
appears at a random location on the background. The user is
then asked to click on the dot as fast as possible. Once the dot is
clicked the next one appears after a short delay, during which
the screen is reset to the grey background. All instructions
are displayed on-screen before the experiment begins, and the
experiments differ in the nature of the instructions given to the
subject. Additionally, each experiment comes in a short and a
long version.
Experiment 1 (no prior knowledge) provides no instruc-
tions to the test subjects beyond asking them to click the dots
as fast as possible. The short version has ﬁve dots and the long
6
Fig. 8: Participant age distribution in decades. Out of 30
participants 2 are wearing glasses and 9 are wearing contact
lenses.
Fig. 9: Our experimental setup consists of an SMI RED500
gazetracker that determines the user’s gaze position on a 24
inch screen with a 1920x1200 resolution.
gaze position and the position of the stimulus (the dot), right
before it is displayed; and (3) Cursor Distance, the distance
between the cursor location and the position of the stimulus,
right before it is displayed.
Figure 6 shows the results of our validation. As we do not
perform repetitions with identical sequences for Experiment 2
and 3 (text descriptions and shouldersurﬁng), the ﬁgure shows
the average over all sequences. We see that, as the number of
repetitions go up, the average performance for Experiment 1
(natural
learning) improves. The two knowledge transfer
mechanisms cause the subjects to perform similarly or even
better than through several repetitions of natural learning. We
therefore conclude that our test subjects do beneﬁt from the
information in the same way that an attacker might.
D. Feature Stability Over Time
For eyetracking to be a useful defence against insider threats,
the features measured from our test subjects must be relatively
stable over time, otherwise false rejects would occur frequently
as the template becomes outdated. While this can be countered
by sporadically retraining the classiﬁer this constitutes a serious
challenge, as the user identity has to be established reliably
during this time. We present a full list of features in Section IV
(Table I). In this section we present the main reasons why time
stability is a challenging problem:
a) Changes in the environment.: Features like the pupil
diameter may change depending on lighting conditions. While
the screen brightness is kept constant across all subjects and
all sessions, the level of daylight may change. It is important
that the classiﬁer accounts for these changes.
b) Changes in the user’s physical and mental state.:
Neuroscientiﬁc research shows that a person’s eye movement
behavior can change depending on states like drowsiness,
exhaustion, stress or euphoria (see Section II for details).
c) Technical Artifacts.: A recent study shows that the
duration and number of ﬁxations and saccades can depend