distinguishes good PIP addresses from bad ones, a large
body of previous work has focused on identifying other im-
portant IP properties. Cai et al. studied how network ad-
dress blocks are utilized through frequent ICMP probing [6].
Katz-Bassett et al. proposed a novel method to estimate the
geographic locations of IP addresses by solving constraints
derived from Internet topology measurements [16]. The clos-
est study to PIP detection is the NAT and proxy detection
work by Casado and Freedman [7]. Also closely related, Met-
wally and Paduano proposed methods to estimate the user
population size per IP address
[18]. Here, PIPMiner dif-
fers from these earlier eﬀorts by focusing on PIP addresses,
with special emphases on distinguishing good PIPs from bad
ones.
2.2 Spam Detection
Email spamming has been a long studied topic and many
spam detection methods have been proposed. In this paper,
we focus on the email service abuse problem where attackers
register or compromise millions of accounts and use them to
send out spam emails.
To ﬁght against such service abuse, service providers of-
ten monitor user behaviors carefully and detect malicious
users by their actions, e.g., sending out spam emails, failing
CAPTCHA tests, or exhibiting suspicious correlated behav-
iors [27, 31]. Since these detection methods rely on user
activities, when they detect a malicious user, the user might
have already conducted some malicious activities. PIPMiner
complements these detection methods by focusing on PIP
addresses that a user leverages to perform sign-up or login.
Therefore, the proposed system can ﬂag new users right on
the day of the sign-ups, or the time they log in.
In addition, our derived malicious PIP list serves as an
early signal to help service providers make informed deci-
sions on whether to block or rate-limit PIP-associated ac-
counts, and how to better allocate resources to PIP ad-
dresses in the presence of attacks. In this regard, the PIP
list is related to IP blacklists such as DNS Blacklists (DNS-
BLs). DNSBLs is widely used for spam detection. For exam-
ple, Jung and Sit observed the existence of low-proﬁle spam
sources by characterizing DNSBL traﬃc [15]. Ramachan-
10^010^110^210^310^410^510^610^710^810^9AOL ProxyInternationalProxyRegionalProxyMobileGatewayCache ProxyAllNo activity>0 requests≥5 requests ≥10 requests Number of IP addresses in Quova Data 331Figure 2: PIPMiner system ﬂow.
IP addresses that are associated with at least rL requests.
However, simply counting the number of requests per IP is
not suﬃcient, as many of these requests could be from a
few user accounts, e.g., through Outlook client’s auto sync.
Therefore, PIPMiner also considers account population size.
PIPMiner marks an IP address as a PIP address if the IP
address has been used by at least uM accounts, together
accounting for at least rM requests. In our current imple-
mentation, we empirically set rL to 1, 000 requests, uM to
10 accounts, and rM to 300 requests. However, PIPMiner
performance is not very sensitive to these thresholds, which
we will later show in Figure 6d.
Data Labeling: PIPMiner relies on a small set of training
data obtained from service providers. In our implementa-
tion, Hotmail provides us with the account reputation scores
that are computed based on account behavior history (e.g.,
spam sent and failed CAPTCHA tests) and external reports
(e.g., rejected emails and user marked spams). Using the
reputation scores, we classify user accounts as good, sus-
picious, or malicious, and we label an IP address as good
if the majority of its requests are made by good accounts,
and an IP address as bad if most of its requests are issued
by malicious accounts. Since the reputation system relies
on carefully observing the account behaviors over time, it
is hard for the reputation system to accurately classify new
accounts with few activities. Therefore, only a fraction of
the PIPs have clear labels, while the remaining IP addresses
are unlabeled and their goodness remains unknown.
Feature Extraction: During feature extraction, the sys-
tem constructs a set of PIP features, ranging from account-
level features (e.g., how many user accounts send requests
via the PIP?), time series-related features (e.g., do the aggre-
gated requests exhibit human-like behavior such as diurnal
patterns in the time domain?), to PIP block level features
(e.g., aggregated features from neighboring PIPs). We will
explore the motivations behind these features in §3.2.
Training and Classiﬁcation:
In this study, we train a
non-linear support vector machine classiﬁer due to its ro-
bustness to noisy input, eﬃciency, and high accuracy. After
the classiﬁer is trained using labeled PIPs, we use it to clas-
sify unlabeled PIPs, resulting in a full list of classiﬁed PIPs.
We further convert the classiﬁer decision value to a score of
how likely a PIP address is abused by attackers. It can be
used to detect abused cases in the future and also in other
applications, as good PIPs can temporary turn bad when
they are abused. The details of the training and classiﬁca-
tion algorithms are in §3.3.
Having explained the system ﬂow, next we present the
proposed features (§3.2) and training methodologies (§3.3).
3.2 Features
We train a support vector machine with a collection of
robust features that discern between good and bad PIPs.
Feature
Population size
Request size
Request per account
New account rate
Account request dis-
tribution
Account stickiness dis-
tribution
Descriptions
The number of accounts
The number of requests
The number of requests per account
The percentage of accounts appear
only in the last x days
The distribution of the number of
requests per account
The distribution of the number of
PIPs associated with an account
Table 2: Population features
In particular, we propose three sets of quantitative features
that can be automatically derived from online service logs.
Population Features (§3.2.1) capture aggregated user char-
acteristics and help distinguish stable PIPs with many users
from those that are dominantly used by only a few heavy
users. Time Series Features (§3.2.2) model the detailed re-
quest patterns and help distinguish long-lasting good PIPs
from bad PIPs that have sudden peaks. IP Block Level Fea-
tures (§3.2.3) aggregate IP block level activities and help
recognize proxy farms.
A subset of the features that we use (user/request density,
user stickiness and diurnal pattern) have been used in other
security-related studies [12, 23, 28]. However, we introduce
new features and we later show that the performance im-
provement by our new features is sizable (§4.2).
3.2.1 Population Features
Population features capture the aggregated account char-
acteristics of each individual PIP. We consider basic statis-
tics such as the number of accounts, the percentage of new
accounts, the number of requests, and the average number
of requests per account, as shown in Table 2.
We also include features derived from basic distributions.
We ﬁrst look at activities of individual PIPs. The account-
request distribution records the distribution of the number
of requests per account. It reﬂects whether accounts on the
PIP have a roughly even distribution of request attempts, or
whether the requests are generated by just a few heavy ac-
counts. In addition, we also look at activities across multiple
PIPs. In particular, we look at account stickiness distribu-
tion, which records the number of other PIPs used by each
account.
After obtaining a distribution, we bucket the distribu-
tion into 20 bins and then extract features such as the peak
values, the central moments, and the percentage of the ac-
counts/requests in the top 1/3/5 bins.
3.2.2 Time Series Features
The population features alone may not be enough to dis-
tinguish good PIPs from bad ones. For example, we ﬁnd
that the aggregated distribution of the number of requests
Dataset #2Dataset #1… PIPaddress selectionRaw ServicelogsData labelingPIP logsFeature extractionClassificationTrainingLabelsFeaturesModelLabeledPIP listLabeled PIP list w/ scores332Feature
On/oﬀ period
Diurnal pattern
Weekly pattern
Account
time
binding
Descriptions
The fraction of time units having ≥x re-
quests
The percentage of requests and accounts
in the peak and the dip periods of the day
The percentage of requests in weekdays
and weekends
The distribution of accounts’ binding time
(time between the ﬁrst and the last re-
quests)
Inter request time The distribution of accounts’ medium
Predictability
inter-request time
The number and the density of anomalies
derived from the Holt-Winters forecasting
model
Table 3: Time series features.
(a)
(b)
(c)
per good PIP looks nearly identical to that of bad PIPs.
One reason is that population features do not capture the
detailed request-arriving patterns. PIPs that are active for
only a short but intensive period (a typical behavior of bad
PIPs) may have similar population features as PIPs with
activities that are spread out in a long duration with clear
diurnal patterns (a typical behavior of good PIPs). In this
subsection, we derive time series features to help distinguish
good PIPs from bad ones.
To derive time series features, we ﬁrst construct a time
series of request counts, denoted by T [·], in a 3-hour preci-
sion. In particular, T [i] is the number of logins in the i-th
time unit. Thus there are k = 8 time units per day.
Additionally, we construct a time-of-the-day series TD[·]
(cid:80)
such that the j-th element of the time series is the total
number of requests in the j-th time unit over days: TD[j] =
k=0,1,... T [8j + k]. Then we derive the peak time P =
arg maxj{TD[j]} and the dip time D = arg minj{TD[j]},
and the time diﬀerence between the peak and the dip, i.e.,
|P − D|.
Table 3 lists the features that we derive from the time se-
ries. It includes on/oﬀ period, diurnal pattern, and weekly
pattern. The on/oﬀ period feature records the fraction of
time units having ≥x requests in T [·]. To assess if the time-
of-the-day series displays a diurnal pattern, we check the
time and the load diﬀerence between the peak and the dip.
To check weekday pattern, we record the percentage of re-
quests and accounts in weekdays and weekends. Due to
timezone diﬀerences, the time for weekends of diﬀerent re-
gions might be diﬀerent. One option is to look up the time-
zone of each IP address. In our experiment, we adopt a sim-
per method — ﬁnding two consecutive days in a week such
that the sum of the number of requests from these two days
are minimal. For PIPs that have more traﬃc in weekends
than weekdays, e.g., coﬀee shop IPs, our approach considers
the least busy day during the week as weekend. But this
does not aﬀect the correctness because our main goal is to
detect the repeated weekly patterns, i.e., the least busy days
of the week repeat over diﬀerent weeks.
We also study two distributions that reﬂect the request-
arrival pattern of each account. The ﬁrst is the account-
binding-time distribution that captures the distribution be-
tween the last and the ﬁrst account request time. This re-
ﬂects the accounts’ binding to an IP address. In addition,
Figure 3: (a) and (b): Time series of the number of requests
from two randomly selected PIP addresses that belong to the
same PIP block. (c): Block level time series, i.e., the time
series of the total number of requests aggregated from all
the PIP addresses in the same PIP block. The selected PIP
block has 94 PIP addresses, and the ﬁgures are plotted in a
time granularity of 3 hours. All the PIPs in the PIP block
are labeled as good.
we check the inter-request time distribution that captures
the regularity of account requests.
We also apply a forecasting model on time series to detect
abnormal period, and we leave details to Appendix C.
IP Block Level Features
3.2.3
We ﬁnd that many regional PIPs manifest clear time-
series features such as coherent diurnal patterns and the
weekday-weekend pattern. For example, for oﬃce IP ad-
dresses, the daily peaks are usually around 4 PM, while the
daily dips are at around 4 AM. The peak load and the dip
load diﬀer signiﬁcantly and they are usually predictable us-
ing our forecasting models.
However, there are exceptions. In particular, large proxy
farms often redirect traﬃc to diﬀerent outgoing network in-
terfaces for load balancing purposes. They may rotate IP
addresses at random intervals or at pre-conﬁgured times-
tamps from a pool of neighboring IP addresses (e.g., [2]).
Such strategies will cause us to generate time series with high
variations (e.g., sudden increase), as shown in Figure 3a and
Figure 3b. We might mistakenly conclude that these cases
are suspicious if we look at time series of individual PIPs
in an isolated way. To resolve the problem, we aggregate
traﬃc using IP blocks and use information of neighboring
PIP addresses to help recognize PIP patterns, as shown in
Figure 3c.
We use the following two criteria to determine neighboring
IP addresses:
1. Neighboring IPs must be announced by the same AS.
This ensures that the neighboring IP addresses are un-
der a single administrative domain.
2. Neighboring IPs are continuous over the IP address
space, and each neighboring IP is itself a PIP. This
 0 5 10 15 20 256/15/20116/21/20116/27/20117/3/20117/9/2011Number ofrequests 0 3 6 9 12 156/15/20116/21/20116/27/20117/3/20117/9/2011Number ofrequests 0 140 280 420 560 7006/15/20116/21/20116/27/20117/3/20117/9/2011Number ofrequests333ensures that there is no signiﬁcant gap between a PIP
and its neighboring PIPs.
After identifying neighboring PIPs, PIP block IDs are as-
signed to PIP addresses such that PIP addresses have the
same block ID if and only if they are neighboring PIPs. Fi-
nally, we generate block-level features: for each PIP address,
we extract all the features (i.e., population and time series
features) using the requests aggregated from all the PIPs in
the same block.
3.3 Training and Classiﬁcation
Feature Preprocessing: For training data, we perform
feature-wise normalization to shift and re-scale each feature
value so that they lie within the range of [0, 1]. We apply
the same transformation as in the training set to the testing
set. However, certain feature values in the testing set can
still be outside the range of [0, 1], which we set to zero or
one as appropriate. This procedure ensures that all features
are given equal importance during the training.
Binary Classiﬁcation: We use support vector machine
(SVM) as our main classiﬁcation algorithm due to its robust-
ness and eﬃciency (comparison to other algorithms is shown
in §4.2). SVM classiﬁes an unlabeled PIP with feature vec-
tor x based on its distance to the decision hyperplane with
norm vector w and constant b:
(cid:88)
∀i
f (x) = wT x + b =
αiyik(xi, x)
(1)
where the sum is over each PIP i in the training set with
label yi ∈ {−1, 1}, feature vector xi, and coeﬃcient αi that
indicates whether the PIP is a support vector (αi > 0) or not
(αi = 0). The feature vectors xi are mapped into a higher
dimensional feature space by a non-linear kernel function
k(xi, x).