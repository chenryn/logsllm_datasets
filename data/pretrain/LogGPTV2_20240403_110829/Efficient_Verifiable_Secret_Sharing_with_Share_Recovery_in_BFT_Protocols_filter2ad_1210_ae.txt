that value.
• vssMakeSecret(q, {⟨xi , yi⟩}i∈I) does a Lagrange interpola-
tion in order to identify the unique degree k − 1 polynomial
in Zq[x] that goes through (xi , yi) and returns that as s.
• vssCombineCommitments(c, ˆc) first extracts дs(τ) from c
and д ˆs(τ) from ˆc. We then set ˇc to (дsτ )(д ˆs(τ)) and return
that value.
3.7.2 DPRF Instantiation Our distributed pseudorandom function
F consists of four algorithms: dprfInit, dprfContrib, dprfVerify,
and dprfEval. Our implementation defines them as follows [38]:
• dprfInit(1κ , k, n, D, Zq), first chooses a generator h of G of
order q. A k out of n secret sharing of a private value α ∈ Zq
is produced using Shamir secret sharing [45], of which the
shares are {αi}i∈[n]. dpkˆi is set to ⟨h, hα , {hαi }i∈[n]⟩ for all
ˆi = 1..n. dskˆi is set to αˆi for all ˆi = 1..n. dprfInit outputs
{⟨dpki , dski⟩}i∈[n].
• dprfContrib(dski , x) first computes fi(x) = H(x)αi where
H : {0, 1}∗ → G is a hash function that is modeled as a
random oracle. Here, αi is obtained from the dski. Let r be
a randomly generated element of Zq′. Then, we let ci ←
H′(H(x), h, fi(x), hαi , H(x)r , hr), where H′ : {0, 1}∗ → Zq
is a hash function modeled as a random oracle. We set zi ←
αici + r mod q. dprfContrib then outputs ⟨fi(x), zi , ci⟩.
• dprfVerify(dpki , x, d) first extracts fi(x), ci, and zi from d.
Then, h and hαi are extracted from dpki. Finally, dprfVerify
returns true if
ci = H′(H(x), h, fi(x), hαi , H(x)zi fi(x)−ci , hzi(hαi)−ci).
• dprfEval(x, {di}i∈I) first verifies each di using dprfVerify. If
any of the verifications returns false, then dprfEval returns
⊥. Otherwise, we extract fi(x) values from each di. Since
the exponents of fi(x) were shared in the exponent using
Shamir secret sharing, dprfEval uses Lagrange interpolation
in the exponent to get the value of F at x, hashes it into an
element of Zq and outputs that value.
4 Secret Shared State on PBFT
In this section we describe how to build a private replicated key/value
store with Byzantine Fault Tolerance by incorporating VSSR into
PBFT [12]. The service provides two APIs, put(K, V) and get(K) by
which a client can write a value to a key or read a value previously
written to a key.
Our design assumes a partial asynchronous Byzantine model.
Specifically, we have n = 3f +1 replicas, ≤ f of which are Byzantine.
The correctness of the value read from a key is ensured despite
up to f Byzantine-faulty replicas, and values are also kept private
from f faulty replicas, using our verifiable secret-sharing approach.
Similar to previous works (e.g., [9, 37]), a client shares a secret
value directly among the replicas, and a consensus protocol drives
agreement on a verifiable digest of the value.
The network is assumed to be asynchronous, but will eventually
go through periods of synchrony in which messages are deliv-
ered within a known time bound and correct replicas and clients
make progress at a known rate. The network assumption is due
to PBFT’s [12] network assumptions. VSSR’s construction is com-
pletely independent of the network model of the underlying BFT
protocol. We assume that each message is signed by its sender so
that its origin is known, subject to standard cryptographic assump-
tions.
Every client in the system is allowed to view all keys in the store.
However, the service maintains a (potentially dynamic) access con-
trol policy that specifies which values each client can open. Under
these assumptions, we provide the standard guarantees provided
by a Byzantine fault tolerant protocol:
• Linearizability [26]. If a client sends a request to the repli-
cated service, then the service’s response is consistent with
an execution where the client’s request was executed instan-
taneously at some point between when the request was sent
and the response was received.
• Liveness. If the network is synchronous, then every client
request will get a response.
following privacy property:
In addition to these standard properties, our design offers the
• Privacy. A value written to a key by a correct client where
the access-control policy prohibits access by any faulty client,
remains hidden from f Byzantine servers.
4.1 Setup
In addition to setting up authenticated communication channels
among all parties in a setup phase, vssInit∗ is called for every client
in the system and is part of the public/private key infrastructure.
The client takes the role of the dealer in vssInit∗ while each replica
takes the role of a participant. In particular, each client knows the
secret keys for all replicas returned from its invocation of vssInit∗.
Every replica stores a full copy of the K-V store. For each key
there are two value entries, a public value (keyed K-pub) and a
private value (keyed K-priv). A replica maintains a bounded log
of pending commands that cannot grow beyond a certain system-
wide parameter W . Once a command in the log is committed by
the system, it is applied to the K-V store.
Views. Our solution employs a classical framework [12, 19] that
revolves around an explicit ranking among proposals via view num-
bers. Replicas all start with an initial view, and progress from one
view to the next. They accept requests and respond to messages
only in their current view. In each view there is a single designated
leader. In a view, zero or more decisions may be reached. If a suf-
ficient number of replicas suspect that the leader is faulty, then a
view change occurs and a new leader is elected. The precise way
views are changed are described in Appendix C.
4.2 Common Mode Protocol
A client put is split into two parts, public and private. More specifi-
cally, in a put(K, V) request, the client privately shares V via vssShare∗,
and sends each share to its corresponding replica. The public part
of put(K, V) consists of a client sending a put(K, cV ) request to the
current leader. cV is a global commitment to the polynomial s that
binds the share of each replica as a verifiable share of s.
The leader waits until its local log has length < W .
It then
extends its local log with the put request, and sends a pre-prepare
(ordering-request) containing its log tail.
A replica accepts a pre-prepare from the leader of the current
view if it is well-formatted, if it extends any previous pre-prepare
from this leader, if its log has fewer than W pending entries, and if
the replica received a valid share corresponding to cV . If the leader
pre-prepare message has a valid format, but the replica did not
receive the corresponding share for it, it starts a timer for share-
recovery (see Appendix C).
After accepting the pre-prepare, a replica follows the regular
PBFT protocol. The replica first extends its local log to include
the new request and broadcasts a prepare message to all replicas
that includes the new log tail. Replicas wait to collect a commit-
certificate, a set of 2f + 1 prepare responses for the current log
tail. Then the replica broadcasts a commit message carrying the
commit-certificate to the other replicas. A decision is reached in
a view on a new log tail when 2f + 1 distinct replica have sent a
commit message for it.
When a replica learns that a put(K, V) request has been commit-
ted to the log, it inserts to its local key-value store two entries, a
global entry (K-pub, cV ) containing the global commitment to V ,
and a private entry (K-priv, u∗
i ) containing the replica’s private
share u∗
i . The replica then responds to the client with a put ac-
knowledgement message containing K and cV . A client waits to
receive 2f + 1 put responses to complete the request. Figure 2a
depicts the put io path, and Figure 2b the put io path when shares
are missed.
The client get(K) protocol consists of sending the get request to
the current leader. The pre-prepare, prepare and commit phases
of the ordering protocol are carried as above, without the need to
wait for shares. At the final stage, when a replica executes the get
requests, it returns its share to the client in a response. If the replica
is missing its share, it initiates the share-recovery protocol. The
client waits to receive f +1 valid get responses. It uses vssVerify∗ to
verify each response, and vssReconstruct∗ to reconstruct the secret
value from the responses.
4.3 Share-Recovery Protocol
There are several circumstances in the protocol when a replica
discovers it is missing its private share of a request and needs to
recover it. To initiate share-recovery, a replica broadcasts a recovery
request. Other replicas respond to a share-recovery request with
the output of vssRecoverContrib∗. After receiving a response, the
original replica uses vssRecoverVerify∗ to check the response. If
the response is valid, then it is stored, and if it is invalid, then it is
dropped. When it receives f + 1 valid responses, the replica uses
vssRecover∗ to recover its missing secret share.
5 Implementation
We implement a secret shared BFT engine by layering PBFT [12]
with our secret sharing scheme. Our implementation consists of
4700 lines of Python and 4800 lines of C. We optimize our design
for multi-core environments, with one network thread running
on a core which never blocks. Additionally, we use one thread for
every other core in order to do all cryptographic operations that are
required by PBFT and our secret sharing scheme. We use elliptic
curve signatures with the secp256k1 library for all signature check-
ing operations and the Relic library [2] for all other cryptographic
operations related to our scheme. We also make a few optimizations
for the Kate et al. and Pedersen secret sharing schemes in order to
make them faster.
Kate et al. Kate et al.’s secret sharing scheme lends itself for ex-
tensive caching during setup time. Once the powers дτ j are known
for all j, we construct precomputation tables for each coefficient so
that all exponentiations during runtime leverage these tables for
efficiency. In the sharing step, we first use the well known Horner’s
method to optimize the share evaluation. However, we also note
that each intermediate value obtained in Horner’s method when
evaluating s(i) is also the coefficient of the quotient polynomial
s(x)−s(i)
x−i which means that we can do the necessary division re-
quired for free before using our precomputation tables to evaluate
the quotient at τ. In the share verification step, every verification
requires the value of e(д, д) so we can precompute that as well to
save a bilinear map operation. Also in the share verification phase,
the division of дτ
дi only has n possible values, which means that
we can precompute all of these values as well. Finally, when doing
Lagrange interpolation, we know that the indices range from 0 to
n − 1 and in the denominator, we need to compute the product
of differences of these indices. Thus, to avoid taking inverses, we
simply take inverses of all n values of the differences which means
that during runtime, we only have to do multiplications.
Pedersen’s secret sharing scheme does not lend itself
Pedersen
to as much caching since most of the values are unknown before-
hand. However, we do generate precomputation tables for both д
and h during setup and compute the inverses to make Lagrange
interpolation easier.
6 Evaluation
Our evaluation seeks to answer two basic questions. First, we inves-
tigate the costs of each API call in our secret sharing scheme. Then,
we look at how expensive it is to incorporate our secret sharing
scheme into a BFT key value store. We instantiate VSSR using the
DPRF in Naor et al. [38] and two different VSS schemes: Peder-
sen’s VSS scheme [42] and Kate et al.’s VSS scheme [29]. We call the
Pedersen instantiation Ped-VSSR and the Kate et al. instantiation
KZG-VSSR. Our implementation uses the Relic [2] cryptographic
library and, for our elliptic-curve algorithms, the BN_P254 curve.
We build a private BFT key value store using PBFT replica-
tion [12], implemented in Python and C, incorporating VSSR into
the write path of the algorithm.
(a) faultless IO path
(b) IO path with recovery
Figure 2: put common mode
4.4 Common mode performance
The common mode protocol incurs the following performance costs.
The client interaction with the BFT replicated service is linear, since
it needs to populate all replicas with shares. Additionally, the client
collects f + 1 responses from servers.
The communication among the replicas to achieve an ordering
decision is quadratic. There are several practical variants of BFT
replication that achieve linear communication during periods of
synchrony and when a leader is non-faulty (e.g., [31, 35? ]) These
improvements are left outside the scope of this paper. However, our
modified VSS protocol is designed so it can be incorporated within
them without increasing the asymptotic complexity of the common
mode.
In terms of latency, the sharing protocol is non-interactive and
single-round, and so it can be performed concurrently with the
leader broadcast. Recovery incurs extra latency since each replica
must ask at least f +1 correct replicas for their contributions. In the
original BFT protocol, recovering a missing request only requires
asking 1 correct replica for the request data. In both cases, the
recovery protocol is interactive and single-round, so there are no
asymptotic increases in latency. However, in practice, there will be
a difference in latency between the two scenarios.
The full proof that our construction satisfies the
Proof Intuition
linearizability, liveness and privacy properties above is in Appen-
dix A. We present high level intuition behind our proofs here.
To show linearizability and liveness, we show that every exe-
cution of our secret shared PBFT protocol can be mapped to an
execution of an unmodified PBFT protocol. Since the original PBFT
protocol satisfies linearizability and liveness, so does our modified
protocol. Privacy is shown by using the fact that to recover a secret
shared value, an adversary must obtain the cooperation of at least
one correct replica.
clientleaderput(	K,	cV)vssShare(V)waitwaitpreparecommitput	ACKpre-prepareclientleaderput(	K,	cV)vssShare(V)waitpreparecommitput	ACKshare-recoverypreparepre-prepare(b) vssShare∗ latency
(a) vssShare∗ throughput
Figure 3: vssShare∗ latency and throughput vs. n
(b) vssVerify∗ latency
(a) vssVerify∗ throughput
Figure 5: vssVerify∗ latency and throughput vs. n
6.1 Microbenchmarks
For our microbenchmarks, we evaluate each function in our VSSR
scheme. We vary the number of replicas from 4 to 211 and mea-
sure the latency and throughput of each operation. We use EC2
c5.xlarge instances in order to run our microbenchmarks, which
have 4 virtual CPUs per instance.
The module that implements our secret sharing scheme opti-
mizes for throughput, while compromising slightly on latency. Each
API call runs on a single core; the task is run to completion and the
result is returned in the order that the tasks were enqueued. This
maximizes for throughput due to the lack of cross core communica-
tion, but at the expense of request latency as many of the underlying
cryptographic operations can leverage multi-core environments to
execute faster.