title:Statistical Privacy for Streaming Traffic
author:Xiaokuan Zhang and
Jihun Hamm and
Michael K. Reiter and
Yinqian Zhang
Statistical Privacy for Streaming Trafﬁc
Xiaokuan Zhang∗, Jihun Hamm∗, Michael K. Reiter†, Yinqian Zhang∗
∗{zhang.5840, hamm.95, zhang.834}@osu.edu, The Ohio State University
†PI:EMAIL, University of North Carolina at Chapel Hill
Abstract—Machine learning empowers trafﬁc-analysis attacks
that breach users’ privacy from their encrypted trafﬁc. Recent
advances in deep learning drastically escalate such threats. One
prominent example demonstrated recently is a trafﬁc-analysis
attack against video streaming by using convolutional neural
networks. In this paper, we explore the adaption of techniques
previously used in the domains of adversarial machine learning
and differential privacy to mitigate the machine-learning-powered
analysis of streaming trafﬁc.
Our ﬁndings are twofold. First, constructing adversarial
samples effectively confounds an adversary with a predetermined
classiﬁer but is less effective when the adversary can adapt to the
defense by using alternative classiﬁers or training the classiﬁer
with adversarial samples. Second, differential-privacy guarantees
are very effective against such statistical-inference-based trafﬁc
analysis, while remaining agnostic to the machine learning clas-
siﬁers used by the adversary. We propose two mechanisms for
enforcing differential privacy for encrypted streaming trafﬁc, and
evaluate their security and utility. Our empirical implementation
and evaluation suggest
the proposed statistical privacy
approaches are promising solutions in the underlying scenarios.
that
I.
INTRODUCTION
Machine learning (ML) leverages statistical techniques to
enable computer systems to learn from data and act based on
inferences without being explicitly instructed. The application
of ML in security has made possible many broadly adopted
defense techniques, such as intrusion detection, spam ﬁltering,
biometric recognition, and malware detection. However, when
used with malicious intentions, ML also empowers notable
attacks. One such example is the trafﬁc-analysis attack.
Encryption is a widely used approach to protecting the
conﬁdentiality of network trafﬁc. For example, HTTPS trafﬁc
encrypts the HTTP headers and payloads inside the SSL
record protocol, hiding the HTTP semantics (e.g., path of
the requested resources) from external observers. Moreover,
users may choose to use VPN or Tor to further hide (through
encryption) the destinations of the web trafﬁc. Trafﬁc analysis,
or more speciﬁcally website ﬁngerprinting, aims to breach
users’ privacy by inferring the HTTP semantics (in the case of
SSL) or visited websites (in the cases of VPN and Tor) from
the encrypted trafﬁc. What enables such attacks is machine
learning. By learning from patterns of encrypted trafﬁc to/from
known web pages, the ML algorithm can classify unidentiﬁed
trafﬁc with reasonable accuracy. With the recent development
of deep learning, such trafﬁc analysis has become more power-
ful, invalidating many previously established defenses against
traditional machine learning [60].
A recent study by Schuster et al. [55] further extended
trafﬁc analysis to SSL or QUIC encrypted video streaming
services. They demonstrated that due to the uniqueness of
the packet burst patterns of the encrypted video streams, the
adversary is able to classify the encrypted video streaming
with very high accuracy (e.g., 99% for Youtube videos).
Consequently, online video streaming is no longer private from
a network observer, regardless of the encryption technology
used in the protocols.
Relationship to side-channel attacks. Trafﬁc analysis is a
type of side-channel attack, i.e., a method to leverage uncon-
ventional means to infer sensitive information in a computer
system. Generally speaking, in side-channel attacks, the ad-
versary may learn secrets of a system or an application that
are otherwise well protected, by observing traces (e.g., timing,
power, or resource usage) of its execution. Although not all
side-channel attacks rely on ML, this technology does enable
some attacks that are otherwise impossible [13], [16], [36],
[48], [73], [74].
With increasing learning capacity, the security threats un-
leashed by these techniques grow rapidly, which calls for
more effective defenses. In this paper, we use streaming trafﬁc
analysis as a motivating example and explore generic solutions
to these ML-powered attacks.
Learning from adversarial machine learning. Inspired by
the recent advances in adversarial machine learning, we ﬁrst
explore the use of adversarial samples to defeat ML adver-
saries. Unlike the common use of adversarial ML, where the
attacker crafts samples deliberately aiming to defeat ML-based
defenders, we consider the inverse use of such techniques. We
exploit adversarial ML techniques to construct noised samples
to thwart ML adversaries. In particular, we utilize the Fast
Gradient Sign Method (FGSM) to generate adversarial samples
to confuse a convolutional neural network (CNN) classiﬁer,
and successfully reduce the accuracy of the classiﬁcation.
However, our results show that adversarial ML techniques are
not robust: by choosing a different ML algorithm or training
the CNN classiﬁer with adversarial samples, the adversary who
aims to perform trafﬁc analysis on encrypted streaming packets
to extract sensitive information can still do so, as indicated by
the high classiﬁcation accuracy after making these changes.
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23210
www.ndss-symposium.org
Adapting differential privacy as security defenses. The
failure in the adoption of adversarial samples to defeat stream-
ing trafﬁc analysis motivated us to seek more principled
solutions to counter such a powerful adversary. Inspired by
Xiao et al. [72], who exploit d∗-privacy—a variant of dif-
ferential privacy—to insert random noise to disturb storage
side channels in procfs, we seek to apply a similar prin-
ciple as a defense against trafﬁc-analysis attacks. However,
compared with differentially private procfs proposed by
Xiao et al., applying differential privacy on network trafﬁc is
fundamentally different as trafﬁc analysis is non-interactive. In
contrast, attacks leveraging procfs are interactive, because
the statistical database is constructed as the attacker queries
procfs. Thus the Laplacian noise can be inserted in the
return values of the procfs queries. Differentially private
streaming trafﬁc needs to be applied proactively to the entire
data streams. The approach to do so and its effectiveness
with regard to security guarantees and utility loss (i.e., low
bandwidth overhead and small amount of lags) is uncertain.
To enforce differential privacy on streaming trafﬁc, we
adapt two mechanisms: FPAk and d∗. Fourier Perturbation
Algorithm (FPAk) [52] is a differentially private mechanism
that answers long query sequences over correlated time series
data in a differentially private manner based on the Discrete
Fourier Transform (DFT). The d∗-private mechanism extends
the mechanism from Chan et al. [11] and applies Laplacian
noise on time series data. We evaluate both mechanisms in re-
gards to security and utility. Our evaluation results suggest that
with proper choice of parameters, both mechanisms defeat—in
a sense that it reduces the classiﬁcation accuracy to the baseline
accuracy of random guessing—all types of classiﬁers that are
trained with either the original data or the noised data. With the
same parameters, we also show that the utility metrics, deﬁned
as waste and deﬁcit, are moderate. We further compare FPAk
with a baseline defense mechanism. The result suggests that
the waste induced by FPAk is at least one order of magnitude
lower than the baseline approach.
To demonstrate the practicality, we implement the FPAk
privacy mechanism in a Chrome extension that proxies the
Youtube streaming between the browser and the server. The
implementation makes use of the Xhook [25] framework,
which intercepts and modiﬁes XMLHttpRequest(XHR) re-
quests and responses. It also utilizes the numjs [44] library
and the Random library in SIM.JS [42] for noise injection.
Our evaluation suggests that the extension completely renders
the attacks proposed by Schuster et al. [55] ineffective.
Contributions. This paper makes the following contributions:
• We demonstrate the ﬁrst attempt to use adversarial ML
for defeating streaming trafﬁc analysis, and explore its
limitations.
• We develop two mechanisms for enforcing differential
privacy for time-series data, and apply them to protect
streaming trafﬁc.
• We perform an extensive evaluation on the two differen-
tially private mechanisms, in terms of both security and
utility.
• We develop a browser extension which integrates one of the
defense mechanisms, and the evaluation shows promising
results.
Besides defeating streaming trafﬁc analysis, the techniques
proposed in this paper also shed light on defenses against
2
website ﬁngerprinting attacks and generic side-channel attacks
that rely on ML. Our study has provided an important piece of
evidence suggesting that the differential privacy is a promising
solution to ML-enabled inference attacks.
Roadmap. The rest of the paper is organized as follows.
Sec. II summarizes the background knowledge needed in the
paper. Sec. III presents a motivating example of identifying
video streams. Sec. IV describes an approach of generating
adversarial samples to defeat
trafﬁc analysis, demonstrat-
ing its effectiveness and limitations. Sec. V proposes two
differentially-private mechanisms for streaming trafﬁc, and
provides the basic attack model assumed in the paper. The
two methods are evaluated in Sec. VI in regards to security
and utility. Sec. VII demonstrates the implementation of a
real-world Chrome extension and how it can effectively defeat
the trafﬁc analysis attacks. Sec. VIII discusses limitations and
practical issues of our approaches. Sec. IX summarizes related
work. We conclude the paper in Sec. X.
II. BACKGROUND
A. Side-Channel Attacks and Trafﬁc Analysis
Side-channel attacks have been studied for more than two
decades. Conventional side-channel attacks usually involve
analysis of externally observable characteristics of a computer
system to extract sensitive information (e.g., cryptographic
keys). Some side-channel attacks extract such information
through a single run of the victim program; in other cases,
multiple side-channel traces must be collected and used to
perform statistical analysis to infer useful information. Trafﬁc
analysis attacks are examples of the latter case, by observing
the meta-data of the encrypted network trafﬁc to classify the
trafﬁc [23], [47], [69].
For our purposes here, a side channel arises from an
attacker’s observation of a feature x, which may itself consist
of multiple components. We let X denote the space of all
possible such x values. Often, the attacker will collect feature
vectors x and their associated labels in a training phase,
to build a machine learning model to which it will apply
observations x seen during his attack.
B. Machine Learning
In the past, various ML techniques have been employed
in statistical side-channel attacks. For example, support vector
machines (SVM) have been used to perform website ﬁnger-
printing in the Tor network [48] and infer foreground apps
on Android [16]; hidden Markov models (HMM) have been
used to infer Android Activity transitions [13] and extract
cryptographic keys in a cross-VM setting [74]; k nearest neigh-
bors (kNN) have been used to perform keystroke inference
on smartwatch [36] and link Bitcoin addresses to an iOS
device [73].
Deep Learning [33] is an ML approach that uses multiple
layers of non-linear processing units, each of which transforms
the representation at one level into that at a higher, more
abstract level. The most representative deep learning model
is the Deep Neural Network (DNN), which is an artiﬁcial
neural network (ANN) with multiple hidden layers between
the input and output layers [2]. DNNs are very effective at
ﬁnding hidden features in high-dimensional data, which is hard
for humans. It has been applied to solve various problems,
producing promising results in different areas such as image
recognition [29], [63], speech recognition [24], [54] , natural
language processing [62] , and malware detection [15].
Researchers have developed various kinds of DNNs. One
of the most popular DNN models is the Convolutional Neural
Network (CNN) [32]. CNN typically applies convolutional
operation at lower levels, and is designed to process data
that has a form of multi-dimensional arrays. CNN takes into
account
the spatial structure of data by enforcing a local
connectivity pattern, which gives it an excellent performance
when dealing with data whose local groups of values are highly
correlated, such as 1D signals and 2D images [29]. There are
other popular DNN models as well, such as Recurrent Neural
Networks (RNNs) [53] and Autoencoders [4].
C. Adversarial Machine Learning
Adversarial machine learning is an emerging research ﬁeld,
which is closely related to both machine learning and computer
security. Here, we brieﬂy introduce two topics in adversarial
ML: adversarial samples and adversarial training.
An adversarial sample x(cid:48) is an input that is crafted from a
legitimate (untampered) input to make a classiﬁer misclassify
x(cid:48) [64]. More speciﬁcally, x(cid:48) is created to be within some
distance threshold from some untampered input x,
in the
hopes that this will imply that x(cid:48) remains in the same class
as x according to some notion of ground truth. However,
x(cid:48) is manipulated so that the ML classiﬁer will classify x(cid:48)
differently from x. Often the distance measure used is l1, l2,
or l∞, and ground truth is as evaluated by a human. (A small
distance in one of these senses does not necessarily imply that
humans will tend to classify x(cid:48) and x the same, however [57].)
Methods of generating adversarial samples include Fast Gradi-
ent Sign Method (FGSM) [21], Deepfool [41], Jacobian-based
Saliency Map Attack (JSMA) [50] and the Carlini/Wagner
attack (CW) [10].
In response, defenses have been proposed to make clas-
siﬁers more robust against adversarial samples. To date, the
most successful one is adversarial training [21], [64], which
basically retrains the classiﬁer using the adversarial samples
that were generated to fool the classiﬁer, in order to increase
the classiﬁcation accuracy on these crafted samples. However,
its effectiveness highly depends on whether the classiﬁer can
generate adversarial samples similar to the ones used by the
attacker, which is difﬁcult to guarantee.
D. Privacy
Because an adversarial sample x(cid:48) generated from x is
designed to be misclassiﬁed, it might be viewed as a more
“privacy preserving” representation of x if correct classiﬁca-
tion constitutes a privacy violation. For this reason, we explore
the generation of adversarial samples as a privacy protection
in a speciﬁc domain, in Sec. IV. Despite the possibility that
adversarial samples so generated might sufﬁce to defeat ML
classiﬁers today, there remains the possibility that future clas-
siﬁers, or auxiliary information that might be brought to bear
by the attacker (classiﬁer), would divulge the correct class of
x(cid:48).
For this reason, in this paper we also explore a novel
application of differential privacy [17] to this same domain,
which will guarantee that certain classes cannot be distin-
guished by any classiﬁer (that works with the same features).
The original deﬁnition of differential privacy is speciﬁc to
statistical databases. More speciﬁcally, two databases x, x(cid:48) are
adjacent if they differ in exactly one element. A randomized
algorithm A : X → Z satisﬁes -differential privacy if for any
adjacent databases x, x(cid:48) and all Z ⊆ Z,
P (A(x) ∈ Z) ≤ exp() × P (A(x(cid:48)) ∈ Z) .
Chatzikokolakis et al. [12] proposed a generalization of