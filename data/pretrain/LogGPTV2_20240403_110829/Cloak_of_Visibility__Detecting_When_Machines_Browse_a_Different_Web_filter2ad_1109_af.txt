For instance, JavaScript redirects account for 29.9% of cloaked
search URLs compared to 6.6% of ads, with ads instead
favoring same-page modiﬁcations. Our result highlights that
while miscreants may cloak against products using a variety of
techniques, our anti-cloaking system nevertheless succeeds at
generalizing and captures each approach. Any security crawler
must address each of these techniques as well as prepare for
future iterations of the cloaking arms race.
VIII. CASE STUDIES
In this section we present case studies exemplary of the
monetization strategies among the Google Search and Google
Ads URLs we identiﬁed as cloaking.
Lead Generation for Mobile Apps: We encountered multiple
sites that entice mobile users to install both dubious and
legitimate third-party apps. Interestingly, a minority of Alexa’s
top domains also exhibit
this behavior. For example, we
show how mobile and desktop visitors see opensubtitles.org
in Figure 7. When this site detects a visitor with an Android
mobile User-Agent and a HTTP referrer set, it adds a new div
element via JavaScript. This element randomly loads an ad
for a legitimate Android app, or is stylized as fake Android
notiﬁcation. When clicked, this notiﬁcation leads to a dubious
app that acts as a free app store. When installed, this app
riddles the device with unwanted ads (through the AirPush
library).
755755
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 6: Classiﬁer performance degradation, when repeatedly removing the crawling proﬁle that least affects the false positive
rate.
Fig. 8: Cloaking site that redirects to advertisements.
Fig. 7: Observing opensubtitiles.org from different devices can
yield completely different content.
Malware: A few of the cloaking websites we identiﬁed are
distributing malware. For example, saomin.com, delivers to
mobile user an Android app that is ﬂagged as malicious by
19 AntiVirus engines on VirusTotal. In another case, the user
was encouraged to install a malicious browser extension called
FromDocToPDF.
Trafﬁc Resellers: We have also observed cloaking sites selling
their organic trafﬁc to a ring of advertisers. For example, in
Figure 8 we show a screenshot of pancakeshop.kim. This site
redirects users to third-party advertisers based on the type of
platform and Referer header. Also, this site geolocates the
visitor and uses this information to decide which ads to run.
Visiting the site from outside the US yields a blank page,
or a message “We currently don’t have any sponsors for this
domain name”.
Some trafﬁc reseller employ a wide set of rules to decide
what content to display. An example of this is macauwinner.tk,
which pretends to be a parked domain when visited from
outside the US, whereas it delivers tailored content to users
on residential and mobile networks, detecting their Internet
provider (e.g., it also displays “Dear AT&T Uverse user”).
Also, it delivers content speciﬁc to the operating system of the
user, mimicking its appearance when creating fake windows
and alert boxes. The pages to which users get redirected
756756
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
range from fake AntiViruses, to fake popular websites (e.g.,
Facebook), to surveys.
Afﬁliate fraud: We found cases where the cloaking site
performs afﬁliate fraud. For example, drseks.com, redirects
every other user to a major shopping retailer with an afﬁliate
id set. By doing so, this retailer shares a fraction of the proﬁts
from a sale to the cloaked domain.
IX. BREAKING THE CLOAKING ARMS RACE
As miscreants adopt increasingly sophisticated application-
speciﬁc cloaking techniques, it becomes difﬁcult for defenders
to keep pace with the cloaking arms race. Currently, our
system is a viable solution, as it is designed to defeat current
cloaking capabilities. We have determined the minimum capa-
bilities a current anti-cloaking pipeline would need precisely
to guide the design of such a pipeline, spending engineering
time efﬁciently. On the long run, however, we envision that
miscreants will add to their cloaking arsenal (e.g., carrier-
speciﬁc mobile cloaking), increasing the cost of detection at
the expense of driving less organic trafﬁc to their concealed
offers. To counter this trend, we propose two possible alterna-
tives that would render it signiﬁcantly harder for miscreants
to deliver split-view content, although they would require an
in-browser component.
Client-side Cloaking Detection: As cloaking hinges on serv-
ing benign content to search engine and ad network crawlers,
one option is for those same services to embed a succinct
digest of a webpage’s content
in the parameters tied to
search and advertisement URLs. When users are redirected
after clicking on one of these URLs,
the user’s browser
can compare the newly served content against the crawler’s
digest. If the two substantially differ, the browser can raise a
warning interstitial that alerts the user to a suspected scam,
phishing, or malware attack. Mechanistically, this comparison
naturally follows the pairwise features we laid out for our anti-
cloaking system. The beneﬁt over our current architecture is
that crawlers no longer need to maintain multiple browsing
proﬁles or network vantages—clients provide the second view.
Additionally, this approach respects the privacy of the users,
as only the potentially-dangerous pages will be reported by
the participating (i.e., opted-in) users.
There are however some open challenges with this approach.
First, dynamic content remains a concern. If miscreants can
limit the deviations introduced by cloaking to within typical
norms (e.g., including only a small new button or URL),
the system may fail to detect the attack. That said, this also
constrains an attacker in a way that reduces click through from
users. Additionally, there is a risk with news sites and other
frequently updated pages that a crawler will serve incoming
visitors a stale digest due to an outdated crawl, thus burdening
users with alerts that are in fact false positives. To avoid this,
the crawler would either need to immediately re-crawl the page
to conﬁrm the change and suppress the alert, or the digest
should account for the category of the site, allowing for a
higher threshold for news sites.
Distributed Client Content Reporting: To overcome the
problem of staleness, we consider an alternative model where
a user’s browser opts to anonymously report a content digest
after clicking on a search result or advertisement
to the
associated search engine or ad network. This server would
then review the incoming digest against the copy fetched by
its crawler. In the event of a mismatch, the server would
immediately re-crawl the URL to rule out the possibility of
an outdated digest. If there is still a client-server mismatch
after crawling,
the search engine or ad network involved
could pull the reported URL from public listing to protect
all future clients. From a privacy perspective,
the server
receiving reports would already be aware the user clicked on
the URL, such as how search engines currently redirect visitors
through analytic interstitials. However, as users may click
through to a signed-in page containing sensitive content (e.g.,
facebook.com), the digest reported must not leak personalized
content. Furthermore, this approach opens servers up to an
abuse problem where malicious clients may spoof digests to
unduly trigger the removal of legitimate search results and
advertisements. However, assuming there are more legitimate
clients than malicious and some form of rate limiting, servers
can rely on majority voting to solve this problem, though the
long tail of URLs may yet pose a challenge.
X. CONCLUSION
In this work, we explored the cloaking arms race playing out
between security crawlers and miscreants seeking to monetize
search engines and ad networks via counterfeit storefronts
and malicious advertisements. While a wealth of prior work
exists in the area of understanding the prevalence of content
hidden from prying eyes with speciﬁc cloaking techniques or
the underlying monetization strategies, none marries both an
underground and empirical perspective that arrives at precisely
how cloaking operates in the wild today. We addressed this
gap, developing an anti-cloaking system that covers a spec-
trum of browser, network, and contextual blackhat targeting
techniques that we used to determine the minimum crawling
capabilities required to contend with cloaking today.
We informed our system’s design by directly engaging with
blackmarket specialists selling cloaking software and services
to obtain ten of the most sophisticated offerings. The built-
in capabilities of these packages included blacklisting clients
based on their IP addresses, reverse DNS, User-Agent, HTTP
headers, and the order of actions a client takes upon visiting a
miscreant’s webpage. We overcame each of these techniques
by fetching suspected cloaking URLs from multiple crawlers
that each emulated increasingly sophisticated legitimate user
behavior. We compared and classiﬁed the content returned
for 94,946 labeled URLs, arriving at a system that accurately
detected cloaking 95.5% of the time with a false positive rate
of 0.9%.
When we deployed our crawler into the wild to scan 135,577
unknown URLs, we found 11.7% of the top 100 search
results related to luxury products and 4.9% of advertisements
targeting weight loss and mobile applications cloaked against
757757
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
Googlebot. In the process, we exposed a gap between cur-
rent blackhat practices and the broader set of ﬁngerprinting
techniques known within the research community which may
yet be deployed. As such, we discussed future directions for
breaking the cloaking arms race that included clients reporting
browsing perspective to crawler operators, hindering the ability
of miscreants to show benign content exclusively to search
engines and ad networks.
REFERENCES
[1] Alexa. Alexa top 500 global sites. http://www.alexa.com/topsites, 2012.
[2] Ross Anderson, Chris Barton, Rainer B¨ohme, Richard Clayton, Michel
J.G. van Eeten, Michael Levi, Tyler Moore, and Stefan Savage. Mea-
suring the cost of cybercrime.
In Proceedings of the Workshop on
Economics of Information Security (WEIS), 2012.
[3] K´aroly Boda, ´Ad´am M´at´e F¨oldes, G´abor Gy¨orgy Guly´as, and S´andor
Imre. User tracking on the web via cross-browser ﬁngerprinting.
In Information Security Technology for Applications, pages 31–46.
Springer, 2012.
[4] Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen.
Classiﬁcation and regression trees. CRC press, 1984.
[5] Moses S Charikar. Similarity estimation techniques from rounding
algorithms. In Proceedings of the thiry-fourth annual ACM symposium
on Theory of computing, pages 380–388. ACM, 2002.
[6] M. Cova, C. Kruegel, and G. Vigna. Detection and analysis of drive-
by-download attacks and malicious JavaScript code. In Proceedings of
the 19th International Conference on World Wide Web, 2010.
[7] Peter Eckersley. How Unique Is Your Web Browser?
In Privacy
Enhancing Technologies (PET), 2010.
[8] David Fiﬁeld and Serge Egelman. Fingerprinting web users through font
metrics. In Proceedings of the International Conference on Financial
Cryptography and Data Security, 2015.
[9] Sean Ford, Marco Cova, Christopher Kruegel, and Giovanni Vigna.
Analyzing and detecting malicious ﬂash advertisements. In Computer
Security Applications Conference, 2009. ACSAC’09. Annual, 2009.
[10] gensim. models.ldamodel – Latent Dirichlet Allocation.
https://
radimrehurek.com/gensim/models/ldamodel.html, 2015.
[11] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely random-
ized trees. Machine learning, 63(1):3–42, 2006.
[12] Chris Grier, Lucas Ballard, Juan Caballero, Neha Chachra, Christian J.
Dietrich, Kirill Levchenko, Panayiotis Mavrommatis, D. McCoy, An-
tonio Nappa, Andreas Pitsillidis, et al. Manufacturing compromise:
The emergence of exploit-as-a-service.
In Proceedings of the ACM
Conference on Computer and Communications Security (CCS), 2012.
[13] Matthew Hoffman, Francis R Bach, and David M Blei. Online learning
for latent dirichlet allocation. In Neural Information Processing Systems,
2010.
[14] John P John, Fang Yu, Yinglian Xie, Arvind Krishnamurthy, and Mart´ın
Abadi. deseo: Combating search-result poisoning. In Proceedings of the
USENIX Security Symposium, 2011.
[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
classiﬁcation with deep convolutional neural networks.
in neural information processing systems, pages 1097–1105, 2012.
[16] Nektarios Leontiadis, Tyler Moore, and Nicolas Christin. Measuring
and analyzing search-redirection attacks in the illicit online prescription
drug trade. In USENIX Security Symposium, 2011.
[17] Nektarios Leontiadis, Tyler Moore, and Nicolas Christin. A nearly four-
year longitudinal study of search-engine poisoning. In Proceedings of
the 2014 ACM SIGSAC Conference on Computer and Communications
Security, 2014.
[18] Long Lu, Roberto Perdisci, and Wenke Lee.
Surf: detecting and
measuring search poisoning. In Proceedings of the 18th ACM conference
on Computer and communications security, 2011.
Imagenet
In Advances
[19] Wes McKinney. Data structures for statistical computing in python. In
Proceedings of the 9th, volume 445, pages 51–56, 2010.
[20] Keaton Mowery, Dillon Bogenreif, Scott Yilek, and Hovav Shacham.
Fingerprinting information in javascript implementations. In Proceed-
ings of the Workshop on Web 2.0 Security and Privacy, 2011.
[21] Keaton Mowery and Hovav Shacham. Pixel perfect: Fingerprinting
canvas in html5. In Proceedings of the Workshop on Web 2.0 Security
and Privacy, 2012.
[22] Martin Mulazzani, Philipp Reschl, Markus Huber, Manuel Leithner,
Sebastian Schrittwieser, Edgar Weippl, and FC Wien. Fast and reliable
browser identiﬁcation with javascript engine ﬁngerprinting. In Proceed-
ings of the Workshop on Web 2.0 Security and Privacy, 2013.
[23] Nick Nikiforakis, Alexandros Kapravelos, Wouter Joosen, Christopher
Kruegel, Frank Piessens, and Giovanni Vigna. Cookieless monster:
Exploring the ecosystem of web-based device ﬁngerprinting. In Security
and Privacy (SP), 2013 IEEE Symposium on, pages 541–555. IEEE,
2013.
[24] Yuan Niu, Hao Chen, Francis Hsu, Yi-Min Wang, and Ming Ma. A
quantitative study of forum spamming using context-based analysis. In
NDSS. Citeseer, 2007.
[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, and et al. Weiss. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011.
[26] Niels Provos, Panayiotis Mavrommatis, Moheeb Abu Rajab, and Fabian
Monrose. All your iFRAMEs point to us. In Proceedings of the 17th
Usenix Security Symposium, pages 1–15, July 2008.
[27] Gianluca Stringhini, Christopher Kruegel, and Giovanni Vigna. Shady
paths: Leveraging surﬁng crowds to detect malicious web pages.
In
Proceedings of the 2013 ACM SIGSAC conference on Computer &
communications security, pages 133–144. ACM, 2013.
[28] Kurt Thomas, Danny Yuxing Huang, David Wang, Elie Bursztein, Chris
Grier, Thomas J. Holt, Christopher Kruegel, Damon McCoy, Stefan
Savage, and Giovanni Vigna. Framing dependencies introduced by
underground commoditization. In Proceedings of the Workshop on the
Economics of Information Security, 2015.
[29] Thomas Unger, Martin Mulazzani, Dominik Fruhwirt, Markus Huber,
Sebastian Schrittwieser, and Edgar Weippl. Shpf: enhancing http(s)
session security with browser ﬁngerprinting.
In Proceedings of the
International Conference on Availability, Reliability and Security, 2013.
http://w3c.github.io/webappsec/specs/
Referrer Policy.
[30] W3C.
referrer-policy/, 2015.
[31] David Y Wang, Matthew Der, Mohammad Karami, Lawrence Saul, Da-
mon McCoy, Stefan Savage, and Geoffrey M Voelker. Search+ seizure:
The effectiveness of interventions on seo campaigns. In Proceedings of
the 2014 Conference on Internet Measurement Conference, 2014.
[32] David Y Wang, Stefan Savage, and Geoffrey M Voelker. Cloak and
dagger: dynamics of web search cloaking. In Proceedings of the ACM
Conference on Computer and Communications Security, 2011.
[33] Yi-Min Wang and Ming Ma. Detecting stealth web pages that use click-
In Microsoft Research Technical Report, MSR-TR,
through cloaking.
2006.
[34] Y.M. Wang, M. Ma, Y. Niu, and H. Chen.
Connecting web spammers with advertisers.
International World Wide Web Conference, pages 291–300, 2007.
Spam double-funnel:
In Proceedings of the
[35] Baoning Wu and Brian D Davison. Detecting semantic cloaking on the
web. In Proceedings of the 15th international conference on World Wide
Web, 2006.
[36] Apostolis Zarras, Alexandros Kapravelos, Gianluca Stringhini, Thorsten
Holz, Christopher Kruegel, and Giovanni Vigna. The dark alleys of
madison avenue: Understanding malicious advertisements. In Proceed-
ings of the 2014 Conference on Internet Measurement Conference, 2014.
[37] Qing Zhang, David Y Wang, and Geoffrey M Voelker. Dspin: Detecting
automatically spun content on the web. In Symposium on Network and
Distributed System Security (NDSS), 2014.
758758
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply.