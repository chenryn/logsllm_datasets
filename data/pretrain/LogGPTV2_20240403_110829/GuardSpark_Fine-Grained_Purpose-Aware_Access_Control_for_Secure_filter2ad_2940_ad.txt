However, besides the raw objects, the alias mechanism 4 in Spark
produces alias objects in order to facilitate the description of data
processing logic in the plan [35]. Alias objects are contained in
non-leaf node operators of the plan (e.g., the first “Project” operator
in Figure 5 contains exp1 alias) and also need to be recognized to
accurately analyze data processing purposes (Section 5.3). Because
aliases are produced by the alias mechanism of Spark, the owner
and source attributes of an alias are null and the table attribute is
the operator initiating the alias through an Alias expression. For
example, sum(exp1) object in Figure 5 is described as {owner:null,
source:null, table:Aggregate, column:sum(exp1)}. Because aliases are
located in non-leaf nodes of plan, GuardSpark++ must traverse
plan; as sub-products, a series of operators passed by each object
are obtained and form an operator-processing path. As shown in
Figure 6, these three objects: Expense, exp1, sum(exp1) have three
operator-processing paths. Lineage exists among paths of each raw
object and its relevant alias objects due to the alias mechanism.
This lineage is convenient to purpose analysis in Section 5.3.
5.3 Purpose Analysis
This module recognizes data processing purposes for all raw objects
in the logical plan. According to the purpose analysis algorithm in
expression is able to evaluate an attribute according to input name string; Predicate
expression is used to evaluate boolean value.
4Aliases for any column can be named by users (e.g., in Figure 5, users name the alias
of Expense as the exp1). Many temporary aliases indicating the intermediate results
of application are usually produced by Catalyst (e.g., the sum(exp1) in the second
“Project” operator is this kind of alias). An alias is carried by an Alias expression.
“selectExpr”“filter”purpose: Apriority: 2“sum”purpose: Cpriority: 3purpose: Rpriority: 1“filter”purpose: Rpriority: 1“groupBy”purpose: Capriority: null“select”purpose: OR: RetrieveC: ComputeA: Assistpath_1path_2{DOP-C, DOP-O}{DOP-A, NULL}Ca: CarryO: Output“patient” tablebigger number, higher priorityPurposeData operation purposeData processing purposeData Object RecognitionPurpose AnalysisOriginal Analyzed Logical PlanCompliance CheckingSecure Logical PlanSubject/Environment AttributesCompliance Enforcementdata usersSecure Logical Plan GenerationACSAC 2020, December 7–11, 2020, Austin, USA
Tao Xue, et al.
Figure 5: Analyzed logical plan.
Section 4.2, this module first obtains the data operations pipeline for
each raw object. Then, it constructs the purpose analysis graph by
recognizing data operation purposes along the pipeline. Finally, to
identify the data processing purpose, this module finds the highest-
priority DOP in each path of the purpose analysis graph. Here
we mainly focus on the logical-plan-based method to obtain data
operations pipeline and to recognize data operation purpose.
To obtain the data operations pipeline for a raw object in the
logical plan, this module finds the complete sequence of operators
for the object. The operators are obtained according to the lineage
mentioned in Section 5.2. For example, Path 2○ in Figure 6 is derived
from Path 1○, while Path 3○ is developed from 2○. All three paths
combined provide the complete sequence of operators of Expense.
To construct the purpose analysis graph, this module recognizes
each DOP along the sequence of operators for each raw object. In
the logical plan, a DOP is identified by the expression encapsulated
in the operator node. We employ pre-defined heuristics to recognize
operation purpose: 1) DOP-R is assigned to operators which explic-
itly retrieve data out of leaf nodes or implicitly retrieve object using
filter (or similar) functions (e.g., the first “Project” operator gets
PatientName from Relation, and the “Filter” operator implicitly
allows PatientName and exp1 to pass). 2) DOP-C is assigned to
operators which make computation on data objects (e.g., in Figure 5,
the DOP-C on exp1 is assigned to the “Aggregate” operator which
makes computation on the object using SUM expression.). 3) DOP-A
is designated by operators which use column-level object as assis-
tance in an expression (e.g., the “Filter” operator uses exp1 in the
expression “exp1#32 > 6000” to filter data.). Last, if the sequence of
operators ends at the root of the logical plan, DOP-O is recognized.
Note that we ignore DOP-Ca purposes as explained in Section 4.1.
5.4 Compliance Checking
This module makes access decisions by evaluating identified objects,
subjects, environments and purposes against policy rules. Here
we explain how the recognized purposes are compared with data-
owners-specified data processing purposes of column-level, row-
level or cell-level objects (Section 4). Meanwhile, there are existing
works in the literature to handle the subject and environment.
GuardSpark++ evaluates each recognized column-level object
as follows: We first identify all relevant fine-grained access control
policies using subject and object attributes. Then, we employ the
following heuristics to identify the allowed cells for each kind of
data processing purpose on the column. For the object, we can
directly extract data processing purposes from each purpose-bound
path in its purpose analysis graph. 1) If the purpose of a path is
{DOP-R, DOP-O}, we find retrieval-regulating rules from selected
rules in the second step, and bind allowed cells with the path. 2)
Figure 6: Operator-processing path examples.
If the purpose of a path is {DOP-C, DOP-O}, we find computation-
regulating rules from selected rules in the second step, and bind
allowed cells with the path. 3) If the purpose of a path is {DOP-A,
NULL}, we find assistance-regulating rules from selected rules in
the second step, and bind allowed cells with the path. As a result,
each access decision means allowed cells of each data processing
purpose in a purpose analysis graph.
5.5 Compliance Enforcement
With the access decisions from Section 5.4, GuardSpark++ trans-
forms the original logical plan into a secure query plan using query
rewriting — modifying operators or producing guard operators to
insert secure operators at appropriate enforcement positions in the
original plan.
When the allowed cells are empty, we consider two situations. 1)
If data processing purpose is {DOP-R, DOP-O} or {DOP-C, DOP-O},
the enforcement position is the root operator in the original plan
and GuardSpark++ uses zero setting logic based on the Alias expres-
sion to replace corresponding expression. For example, if a doctor is
prohibited from using DOP-C on the expenses, the sum(exp1) will
be respectively replaced with “0 AS sum(exp1)”. This replacement
obviously has no effect on other processing purposes, and obeys
the immutable schema structure of immutable DataFrame/Dataset
[6, 26]. 2) If {DOP-A, NULL}, the enforcement position is operators
connected with the assistance purpose and GuardSpark++ deletes
all expressions relevant with the assistance purpose. For example,
if a doctor is not allowed to use Expense for assistance purpose,
the expression in “Filter” operator is deleted. In the above cases, we
insert secure operator through modifying an operator.
When the allowed cells are non-empty, a secure expression is
constructed to retain them. A guard operator uses the secure ex-
pression to retain allowed cells and erase prohibited cells. A guard
operator is described as Guard(“allowed_sells”, “column”) where
“allowed_cells” is represented by secure expression and “column” is
the targeted column. The enforcement position is below the last op-
erator passed by the corresponding object. For instance, if a doctor
is prohibited from counting the expense of two patients “Aaron” and
“Brown”, a new guard operator “Guard(PatientName#8 != (Aaron
OR Brown), Expense)” is inserted below the first “Project” operator.
6 EXPERIMENTAL RESULTS
With Spark as the baseline, we test GuardSpark++’s overhead
and scalability. We also evaluate the recognition of data opera-
tion/processing purposes in GuardSpark++ through case studies
on five data sources and four structured data analytics engines.
6.1 Settings
Hardware and Software Configurations. We conduct our ex-
periments on a cluster of 7 nodes, including one primary and six
secondary nodes. Each node is equipped with 32 Intel Xeon CPUs
E5-2630 v3 @ 2.40GHz, 130GB of RAM and 4TB of disk capacity,
Project [PatientName#8, sum(exp1)#36L]Aggregate [PatientName#8], [PatientName#8, sum(exp1#32) AS sum(exp1)#36]Filter (exp1#32 > 6000)Project [PatientName#8, Expense#7 AS exp1#32]Relation [Id#5, Disease#6,Expense#7,PatientName#8](R)(P1)(F)(A)(P2)Note : The integer after the # notation is the serial number assigned by Catalyst, not affecting the understanding of our design. Various operators are denoted by typefaces like RAttribute expression: e.g., Predicate expression: e.g., Alias  expression: e.g., SUM  expression: e.g., R: representing “patient”DataFrameExpensesum(exp1)RP1P2R: Relation   P1: Project   F: Filter   A: Aggregate   P2: Projectexp1FA123GuardSpark++: Fine-Grained Purpose-Aware Access Control for Secure Data Sharing and Analysis in Spark
ACSAC 2020, December 7–11, 2020, Austin, USA
Figure 7: Query processing efficiency of GuardSpark++.
running 64-bit CentOS. HDFS (v. 2.6.0) is used to store the data with
a replication factor of 2. We build GuardSpark++ on Spark v. 2.4.0.
Datasets and Benchmarks. To test GuardSpark++’s efficiency
and scalability, we use the TPC-DS benchmark [23], which covers
various query types in decision support systems and is used as
the performance testing framework for Spark SQL in Spark 2.2+
[1, 11]. In our case studies, we also test access control on SQL
engine with the TPC-DS benchmark. We use the popular Iris [9]
and BigDataBench [69] for the ML Pipelines engine. We use the
Pokec [8] for the GraphFrame engine. The datasets used in the
tests are stored on HDFS. For Structured Streaming engine, we
evaluate network streaming, LFS, HDFS and Kafka sources, and test
a common logs analysis system. Finally, we evaluate GuardSpark++
on a relational data source MySQL.
6.2 Efficiency of GuardSpark++
We evaluate the efficiency of GuardSpark++ using 2GB, 4GB, 8GB,
16GB, 32GB, 64GB and 128GB retail datasets generated by TPC-
DS data generator. We choose Query02, Query27, Query35, and
Query93 in TPC-DS package as they cover various data operations
and contain complex data processing logics. In particular, Query93,
Query27, and Query02 have minimum, moderate and maximum
number of expressions designating operation purposes, respectively,
while Query35 contains three sub queries. The queries contain the
following data processing purposes: {DOP-R, DOP-O}, {DOP-C,
DOP-O} and {DOP-A, NULL}. With Spark as the baseline, to ensure
dataset size consistency, we customize access control policies in
GuardSpark++ in terms of queries’ own operation information.
For example, if the condition in WHERE clause of a query means
the value of column 5 in a relation equals to 2 and the column
is retrieved and outputted, we can state the policy that its value
for {DOP-R, DOP-O} should be greater than 0. Table 1 shows all
policies of the four queries. In this way, we can avoid data size
change because of access control, and only probe into the cost of
our control. Each query is repeated 100 times on each dataset, and
the average execution time is calculated as the final index.
Figure 8: The scalability of GuardSpark++ against access con-
trol policy size.
As shown in Figure 7, GuardSpark++ introduces a small over-
head to Spark: 8.44% (2GB dataset), 6.40% (4GB), 4.69% (8GB), 3.52%
(16GB), 2.17% (32GB), 1.65% (64GB) and 0.89% (128GB). Apparently,
the relative overhead is below 10% in all cases and gradually de-
creases with larger datasets. This is because Spark’s query execution
time increases with larger datasets, while the time to produce a se-
cure logical plan stays stable. The result shows that GuardSpark++
is highly scalable, which could be used in big data sharing scenarios,
where exabytes or even zettabytes of data may be evaluated.
6.3 Policy Scalability
The data scalability of GuardSpark++ is discussed in Section 6.2.
Next, we measure the scalability of GuardSpark++ with respect to
the size of the access control policy. According to the architecture
of GuardSpark++, the policy scalability is mainly determined by
the purpose compliance checking module. To directly evaluate how
this module affects the scalability of GuardSpark++, we conduct
experiments with the following access control rule set sizes: 10, 100,
1000, and 10000. For each rule set, we execute Query02 100 times
and compute the average query processing time in the purpose
compliance checking module.
Figure 8 (left) shows the average purpose compliance checking
time execution per-rule. As shown in the figure, the per-rule execu-
tion time reduces significantly when the rule set size grows from
10 to 100. Figure 8 (right) shows the growth of the total compliance
checking time when the rule set size grows. Note that the X-axis
is in logarithmic scale. As shown, when the size of the rule set
size increases, the total compliance checking time increases ap-
proximately linearly. The total compliance checking time for 10000
rules is approximately 3 seconds, which implies that the proposed
mechanism is highly practical.
6.4 Case Study
Next, we show some use cases from four data analytics engines
and five data sources. The following empirical results show that
GuardSpark++ works effectively and correctly.
SQL. We perform the assessment on the TPC-DS benchmark
6.4.1
which contains various tables. These tables record much sensitive
information. For instance, the “customer_address” table contains
addresses of all customers; the “customer_demographics” table
records demographics of all customers; the “store” table includes
2G4G8G16G32G64G128G050100150200Time(s) GuardSpark++ SparkTime(s)Query022G4G8G16G32G64G128G0100200300Query27 GuardSpark++ Spark2G4G8G16G32G64G128G0100200300400Time(s)Query35 GuardSpark++ SparkTime(s)2G4G8G16G32G64G128G0100200300Query93 GuardSpark++ Spark0500100015002000250010000100010010 time consumption per rule fitted curveTime(us)Rule Set Size101001000100000.05.0x1051.0x1061.5x1062.0x1062.5x1063.0x1063.5x1062.13*10^46.16*10^43.39*10^53.03*10^6  total time consumptionTime(us)Rule Set SizeACSAC 2020, December 7–11, 2020, Austin, USA
Tao Xue, et al.
Table 1: Access control policies specified in GuardSpark++ for efficiency test
All data objects in these queries, except the column objects in policies, are used without restriction.
We only show protected data and its specified data processing purpose in policy.
Protected Object and Specified Purpose
the value (>0) of column 5 in table data_time is allowed {DOP-R, DOP-O}.
the value (>0) of column 13 in table store_sales is allowed {DOP-C, DOP-O}.
the value (!=‘no’) of column 2 in table customer_demographics is allowed {DOP-R, DOP-O}.
the value (!=‘-1’) of column 4 in table store_sales is allowed {DOP-R, DOP-O}.
Query
Query02
Query27
Query35
Query93
simulate the policy enforcement by manually deleting the corre-
sponding data from Iris dataset and obtain the WSSSE set based on
the same K values. Also, basing the same K values on, we run Spark
without policy enforcement to obtain the WSSSE set as a baseline.
The experimental results are shown in Figure 9(a). Apparently, the
last K-WSSSE set is different from the first two identical K-WSSSE
sets. Second, we utilize 4GB data generated using BigDataBench
[69]. With the same experimental method above, we obtain the
results (similar to the first test) shown in Figure 9(b). Obviously,
GuardSpark++ can constrain data usage for ML algorithm. Access
control for ML engine is vital when data owners share their data
and customize their policies to protect their sensitive data from be-
ing used by ML algorithms that may indirectly expose more privacy
information about data owners [24].
6.4.3 GraphFrame. We select PageRank algorithm [14] originally