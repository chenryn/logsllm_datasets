Is ReadWrite
Not Thinly-Provisioned
Supports Case-sensitive filenames
Preserves Case of filenames
Supports Unicode in filenames
Preserves & Enforces ACL’s
Supports Disk Quotas
Supports Reparse Points
Returns Handle Close Result Information
Supports POSIX-style Unlink and Rename
Supports Object Identifiers
Supports Named Streams
Supports Hard Links
Supports Extended Attributes
Supports Open By FileID
Supports USN Journal
Is DAX Volume
740
CHAPTER 11
Caching and file systems
There have been different versions of ReFS. The one described in this book is referred to as ReFS v2, 
which was first implemented in Windows Server 2016. Figure 11-78 shows an overview of the different 
high-level implementations between NTFS and ReFS. Instead of completely rewriting the NTFS file 
system, ReFS uses another approach by dividing the implementation of NTFS into two parts: one part 
understands the on-disk format, and the other does not. 
New on-disk store engine
Minstore
ReFS.SYS
NTFS.SYS
NTFS on-disk store engine
Upper layer
engine inherited from NTFS
NTFS upper layer
API/semantics engine
FIGURE 11-78 ReFS high-level implementation compared to NTFS.
ReFS replaces the on-disk storage engine with Minstore. Minstore is a recoverable object store li-
brary that provides a key-value table interface to its callers, implements allocate-on-write semantics for 
modification to those tables, and integrates with the Windows cache manager. Essentially, Minstore is a 
library that implements the core of a modern, scalable copy-on-write file system. Minstore is leveraged 
by ReFS to implement files, directories, and so on. Understanding the basics of Minstore is needed to 
describe ReFS, so let’s start with a description of Minstore.
Minstore architecture
Everything in Minstore is a table. A table is composed of multiple rows, which are made of a key-value 
pair. Minstore tables, when stored on disk, are represented using B+ trees. When kept in volatile 
memory (RAM), they are represented using hash tables. B+ trees, also known as balanced trees, have 
different important properties: 
1.
They usually have a large number of children per node.
2.
They store data pointers (a pointer to the disk file block that contains the key value) only on the
leaves—not on internal nodes.
3.
Every path from the root node to a leaf node is of the same length.
CHAPTER 11
Caching and file systems
741
Other file systems (like NTFS) generally use B-trees (another data structure that generalizes a binary 
search-tree, not to be confused with the term “Binary tree”) to store the data pointer, along with the 
key, in each node of the tree. This technique greatly reduces the number of entries that can be packed 
into a node of a B-tree, thereby contributing to the increase in the number of levels in the B-tree, hence 
increasing the search time of a record. 
Figure 11-79 shows an example of B+ tree. In the tree shown in the figure, the root and the internal 
node contain only keys, which are used for properly accessing the data located in the leaf’s nodes. Leaf 
nodes are all at the same level and are generally linked together. As a consequence, there is no need to 
emit lots of I/O operations for finding an element in the tree. 
For example, let’s assume that Minstore needs to access the node with the key 20. The root node 
contains one key used as an index. Keys with a value above or equal to 13 are stored in one of the chil-
dren indexed by the right pointer; meanwhile, keys with a value less than 13 are stored in one of the left 
children. When Minstore has reached the leaf, which contains the actual data, it can easily access the 
data also for node with keys 16 and 25 without performing any full tree scan. 
Furthermore, the leaf nodes are usually linked together using linked lists. This means that for huge 
trees, Minstore can, for example, query all the files in a folder by accessing the root and the intermedi-
ate nodes only once—assuming that in the figure all the files are represented by the values stored in 
the leaves. As mentioned above, Minstore generally uses a B+ tree for representing different objects 
than files or directories.
13
1
4
9
10
11 12
13 15
16 20 25
9
11
16
FIGURE 11-79 A sample B+ tree. Only the leaf nodes contain data pointers. Director nodes contain only links to 
children nodes.
In this book, we use the term B+ tree and B+ table for expressing the same concept. Minstore 
defines different kind of tables. A table can be created, it can have rows added to it, deleted from it, or 
updated inside of it. An external entity can enumerate the table or find a single row. The Minstore core 
is represented by the object table. The object table is an index of the location of every root (nonem-
bedded) B+ trees in the volume. B+ trees can be embedded within other trees; a child tree’s root is 
stored within the row of a parent tree.
Each table in Minstore is defined by a composite and a schema. A composite is just a set of rules 
that describe the behavior of the root node (sometimes even the children) and how to find and ma-
nipulate each node of the B+ table. Minstore supports two kinds of root nodes, managed by their 
respective composites:
742
CHAPTER 11
Caching and file systems
I 
Copy on Write (CoW): This kind of root node moves its location when the tree is modified. This
means that in case of modification, a brand-new B+ tree is written while the old one is marked
for deletion. In order to deal with these nodes, the corresponding composite needs to maintain
an object ID that will be used when the table is written.
I 
Embedded: This kind of root node is stored in the data portion (the value of a leaf node) of an
index entry of another B+ tree. The embedded composite maintains a reference to the index
entry that stores the embedded root node.
Specifying a schema when the table is created tells Minstore what type of key is being used, how big 
the root and the leaf nodes of the table should be, and how the rows in the table are laid out. ReFS uses 
different schemas for files and directories. Directories are B+ table objects referenced by the object 
table, which can contain three different kinds of rows (files, links, and file IDs). In ReFS, the key of each 
row represents the name of the file, link, or file ID. Files are tables that contain attributes in their rows 
(attribute code and value pairs). 
Every operation that can be performed on a table (close, modify, write to disk, or delete) is repre-
sented by a Minstore transaction. A Minstore transaction is similar to a database transaction: a unit of 
work, sometimes made up of multiple operations, that can succeed or fail only in an atomic way. The 
way in which tables are written to the disk is through a process known as updating the tree. When a tree 
update is requested, transactions are drained from the tree, and no transactions are allowed to start 
until the update is finished.
One important concept used in ReFS is the embedded table: a B+ tree that has the root node located 
in a row of another B+ tree. ReFS uses embedded tables extensively. For example, every file is a B+ tree 
whose roots are embedded in the row of directories. Embedded tables also support a move operation 
that changes the parent table. The size of the root node is fixed and is taken from the table’s schema.
B+ tree physical layout
In Minstore, a B+ tree is made of buckets. Buckets are the Minstore equivalent of the general B+ tree 
nodes. Leaf buckets contain the data that the tree is storing; intermediate buckets are called director nodes 
and are used only for direct lookups to the next level in the tree. (In Figure 11-79, each node is a bucket.) 
Because director nodes are used only for directing traffic to child buckets, they need not have exact 
copies of a key in a child bucket but can instead pick a value between two buckets and use that. (In 
ReFS, usually the key is a compressed file name.) The data of an intermediate bucket instead contains 
both the logical cluster number (LCN) and a checksum of the bucket that it’s pointing to. (The check-
sum allows ReFS to implement self-healing features.) The intermediate nodes of a Minstore table could 
be considered as a Merkle tree, in which every leaf node is labelled with the hash of a data block, and 
every nonleaf node is labelled with the cryptographic hash of the labels of its child nodes.
Every bucket is composed of an index header that describes the bucket, and a footer, which is an array 
of offsets pointing to the index entries in the correct order. Between the header and the footer there are 
the index entries. An index entry represents a row in the B+ table; a row is a simple data structure that 
gives the location and size of both the key and data (which both reside in the same bucket). Figure 11-80 
shows an example of a leaf bucket containing three rows, indexed by the offsets located in the footer. In 
leaf pages, each row contains the key and the actual data (or the root node of another embedded tree).
CHAPTER 11
Caching and file systems
743
# Rows
0x20
0x100
0x80
# Free Bytes
Row 1
Row 3
Row 2
First Row
Offset
Row Offset
Array Start
FIGURE 11-80 A leaf bucket with three index entries that are ordered by the array of offsets in the footer.
Allocators
When the file system asks Minstore to allocate a bucket (the B+ table requests a bucket with a process 
called pinning the bucket), the latter needs a way to keep track of the free space of the underlaying me-
dium. The first version of Minstore used a hierarchical allocator, which meant that there were multiple 
allocator objects, each of which allocated space out of its parent allocator. When the root allocator 
mapped the entire space of the volume, each allocator became a B+ tree that used the lcn-count table 
schema. This schema describes the row’s key as a range of LCN that the allocator has taken from its par-
ent node, and the row’s value as an allocator region. In the original implementation, an allocator region 
described the state of each chunk in the region in relation to its children nodes: free or allocated and 
the owner ID of the object that owns it. 
Figure 11-81 shows a simplified version of the original implementation of the hierarchical allocator. 
In the picture, a large allocator has only one allocation unit set: the space represented by the bit has 
been allocated for the medium allocator, which is currently empty. In this case, the medium allocator 
is a child of the large allocator.
key
{0 - 0x3FFFFF}
value
{10000000000000000000000000000…}
{0x400000 - …}
{00000000000000000000000000000…}
{0 - 0x000FFF}
{00000000000000000000000000000…}
FIGURE 11-81 The old hierarchical allocator.
B+ tables deeply rely on allocators to get new buckets and to find space for the copy-on-write cop-
ies of existing buckets (implementing the write-to-new strategy). The latest Minstore version replaced 
the hierarchical allocator with a policy-driven allocator, with the goal of supporting a central loca-
tion in the file system that would be able to support tiering. A tier is a type of the storage device—for 
744
CHAPTER 11
Caching and file systems
example, an SSD, NVMe, or classical rotational disk. Tiering is discussed later in this chapter. It is basi-
cally the ability to support a disk composed of a fast random-access zone, which is usually smaller than 
the slow sequential-only area.
The new policy-driven allocator is an optimized version (supporting a very large number of allocations 
per second) that defines different allocation areas based on the requested tier (the type of underlying 
storage device). When the file system requests space for new data, the central allocator decides which 
area to allocate from by a policy-driven engine. This policy engine is tiering-aware (this means that 
metadata is always written to the performance tiers and never to SMR capacity tiers, due to the random-
write nature of the metadata), supports ReFS bands, and implements deferred allocation logic (DAL). The 
deferred allocation logic relies on the fact that when the file system creates a file, it usually also allocates 
the needed space for the file content. Minstore, instead of returning to the underlying file system an 
LCN range, returns a token containing the space reservation that provides a guarantee against the disk 
becoming full. When the file is ultimately written, the allocator assigns LCNs for the file’s content and 
updates the metadata. This solves problems with SMR disks (which are covered later in this chapter) and 
allows ReFS to be able to create even huge files (64 TB or more) in less than a second.
The policy-driven allocator is composed of three central allocators, implemented on-disk as global 
B+ tables. When they’re loaded in memory, the allocators are represented using AVL trees, though. An 
AVL tree is another kind of self-balancing binary tree that’s not covered in this book. Although each 
row in the B+ table is still indexed by a range, the data part of the row could contain a bitmap or, as 
an optimization, only the number of allocated clusters (in case the allocated space is contiguous). The 
three allocators are used for different purposes:
I 
The Medium Allocator (MAA) is the allocator for each file in the namespace, except for some B+
tables allocated from the other allocators. The Medium Allocator is a B+ table itself, so it needs
to find space for its metadata updates (which still follow the write-to-new strategy). This is the
role of the Small Allocator (SAA).
I 
The Small Allocator (SAA) allocates space for itself, for the Medium Allocator, and for two
tables: the Integrity State table (which allows ReFS to support Integrity Streams) and the Block
Reference Counter table (which allows ReFS to support a file’s block cloning).
I 
The Container Allocator (CAA) is used when allocating space for the container table, a funda-
mental table that provides cluster virtualization to ReFS and is also deeply used for container
compaction. (See the following sections for more details.) Furthermore, the Container Allocator
contains one or more entries for describing the space used by itself.
When the Format tool initially creates the basic data structures for ReFS, it creates the three alloca-
tors. The Medium Allocator initially describes all the volume’s clusters. Space for the SAA and CAA 
metadata (which are B+ tables) is allocated from the MAA (this is the only time that ever happens in 
the volume lifetime). An entry for describing the space used by the Medium Allocator is inserted in the 
SAA. Once the allocators are created, additional entries for the SAA and CAA are no longer allocated 
from the Medium Allocator (except in case ReFS finds corruption in the allocators themselves).
CHAPTER 11
Caching and file systems
745
To perform a write-to-new operation for a file, ReFS must first consult the MAA allocator to find 
space for the write to go to. In a tiered configuration, it does so with awareness of the tiers. Upon suc-
cessful completion, it updates the file’s stream extent table to reflect the new location of that extent 
and updates the file’s metadata. The new B+ tree is then written to the disk in the free space block, 
and the old table is converted as free space. If the write is tagged as a write-through, meaning that the 
write must be discoverable after a crash, ReFS writes a log record for recording the write-to-new opera-
tion. (See the “ReFS write-through” section later in this chapter for further details). 
Page table
When Minstore updates a bucket in the B+ tree (maybe because it needs to move a child node or even 
add a row in the table), it generally needs to update the parent (or director) nodes. (More precisely, 
Minstore uses different links that point to a new and an old child bucket for every node.) This is because, 
as we have described earlier, every director node contains the checksum of its leaves. Furthermore, the 
leaf node could have been moved or could even have been deleted. This leads to synchronization prob-
lems; for example, imagine a thread that is reading the B+ tree while a row is being deleted. Locking the 
tree and writing every modification on the physical medium would be prohibitively expensive. Minstore 
needs a convenient and fast way to keep track of the information about the tree. The Minstore Page Table
(unrelated to the CPU’s page table), is an in-memory hash table private to each Minstore’s root table—
usually the directory and file table—which keeps track of which bucket is dirty, freed, or deleted. This 
table will never be stored on the disk. In Minstore, the terms bucket and page are used interchangeably; 
a page usually resides in memory, whereas a bucket is stored on disk, but they express exactly the same 
high-level concept. Trees and tables also are used interchangeably, which explains why the page table is 
called as it is. The rows of a page table are composed of the LCN of the target bucket, as a Key, and a data 
structure that keeps track of the page states and assists the synchronization of the B+ tree as a value. 
When a page is first read or created, a new entry will be inserted into the hash table that represents 
the page table. An entry into the page table can be deleted only if all the following conditions are met:
I 
There are no active transactions accessing the page.
I 
The page is clean and has no modifications.
I 
The page is not a copy-on-write new page of a previous one.
Thanks to these rules, clean pages usually come into the page table and are deleted from it repeat-
edly, whereas a page that is dirty would stay in the page table until the B+ tree is updated and finally 
written to disk. The process of writing the tree to stable media depends heavily upon the state in the 
page table at any given time. As you can see from Figure 11-82, the page table is used by Minstore as 
an in-memory cache, producing an implicit state machine that describes each state of a page.
746
CHAPTER 11
Caching and file systems
COPIED
DIRTY
FREED
CLEAN
(New)
Page has been
modified
Can exit the
page table ONLY
in clean state
Page has been
copied (on-write)
Page space on-disk
freed
B+ tree
written on disk
FIGURE 11-82 The diagram shows the states of a dirty page (bucket) in the page table. A new page is produced due 
to copy-on-write of an old page or if the B+ tree is growing and needs more space for storing the bucket.
Minstore I/O
In Minstore, reads and writes to the B+ tree in the final physical medium are performed in a different 
way: tree reads usually happen in portions, meaning that the read operation might only include some 
leaf buckets, for example, and occurs as part of transactional access or as a preemptive prefetch action. 
After a bucket is read into the cache (see the “Cache manager” section earlier in this chapter), Minstore 
still can’t interpret its data because the bucket checksum needs to be verified. The expected checksum 
is stored in the parent node: when the ReFS driver (which resides above Minstore) intercepts the read 