10
if seedW ithN ewEdдe(s) == true then
else if s .power Enerдy() > η then
else if r eachT arдet(s) == true then
Q1 ← Q1 ∪ {s };
Q1 ← Q1 ∪ {s };
Q1 ← Q1 ∪ {s };
Q2 ← Q2 ∪ {s };
else
11 else
12
Q3 ← Q3 ∪ {s };
(i ∗ ζ , at line 7) than the necessary case (i ∗ (1 − σ) ∗ ζ , at line 4),
and meanwhile splice mutations will also get more times (i ∗(1− ζ)
line 8) than necessary (i ∗ (1 − σ) ∗ (1 − ζ), at line 5).
0.1, δ = 0.4, σ = 0.2, ζ = 0.8.
In practice, we assign the empirical values to the constants: γ =
Note that all these new generated seeds in Ms, together with
the original seeds, will be put into the seed queue for future fuzzing.
Actually, before fuzzing them, we will prioritize them to improve
the efficiency of directed fuzzing (see §4.6).
4.6 Seed Prioritization
Not all the seeds have equal or similar priorities, ideally the queue
that stores the seeds to be mutated should be a priority queue.
However the scoring may be biased (due to the limitations of static
analyses, etc.), and the insertion operations on priority queue take a
complexity of Θ(log n), which is costly since the queue can be quite
long and the insertion operation can be frequent. Therefore it is
not beneficial in practice.3 Instead, we provide a three-tiered queue
which appends newly generated seeds into different categories
according to their scores. Seeds in the top-tiered queue (tier 1) will
be picked firstly, then the second-tiered (tier 2), and finally the
3In fact, the well-known AFL fuzzer only maintains a linked list with some probabilistic
to skip seeds that do not cover new edges; the complexity of the insertion is Θ(1).
lower-tiered (tier 3). This imitates a simplified priority queue with
constant time complexity.
Algo. 3 shows the seed prioritization strategy for a new seed
mutated from the previous step of adaptive mutation. The basic
idea is: we should prioritize the newly generated seeds that 1) cover
new traces 2) have bigger similarity values with the target seeds
(i.e., power function values) 3) cover the target functions. We favor
the seeds that cover the new traces since we still have to explore
more execution paths that have the potential to lead to the target
sites; this is necessary when the initial seeds are quite far from
the targets. The other two prioritization strategies, i.e., comparing
similarity values and checking whether the target function have
been reached, are specific to directed fuzzing. Note that although
these two strategies are relevant, neither of them can be deduced
from the other. For the other newly generated seeds, they are put
in the second-tiered queue. On the other hand, seeds that have
(just) been mutated are assigned with the least priority. In practice,
Hawkeye also applies AFL’s loop bucket approach (see [49]) to filter
out a large number of “equivalent” seeds that do not bring new
coverage in terms of loop iterations. The prioritization strategies
will be applied on the remaining seeds. Therefore, there will not be
too many seeds filling up the top-tier queue.
By combining all the static and dynamic techniques mentioned
above, for the CVE exampled in §2.1, Hawkeye successfully repro-
duced the crash with a time budget of 24 hours in 3 out of the 10
runs we conducted when fed with the same initial seeds, which is
a significantly improvement on both AFL and AFLGo for this case.
5 EVALUATION
We implemented our static instrumentation on top of AFL’s LLVM
mode and the pointer analysis is based on the interprocedural static
value-flow analysis tool called SVF [41]; this part takes about 2000
lines of C/C++ code. The dynamic fuzzer is implemented based
on our Rust implementation of AFL. The fundamental framework,
called Fuzzing Orchestration Toolkit[11], is written in about 14000
lines of code. We follow exactly AFL’s practice [49] by using fork-
server, shared memory based basic block transition (edge) tracing,
deterministic/non-deterministic mutators, etc., and provide a simi-
lar user interface as AFL’s. The differences, however, are that we
design the fuzzer with considerations of modularization and extensi-
bility without sacrifying performance. For directed fuzzing purpose,
we add another 4000 lines of code for tracing functions, calculating
power function for seeds, distinguishing graininess of mutators, and
so on 4. See our website https://sites.google.com/view/ccs2018-fuzz
for more details.
For each program with the given target sites, the instrumentation
of Hawkeye consists of three parts: 1) basic block IDs that track the
execution traces 2) basic block distance information that determines
basic block trace distance and 3) function IDs that track functions
that have been covered.
5.1 Evalution Setup
In the experiments, we aim to answer the following questions:
4The semantic mutation part is implemented with Antlr [31] in Java (∼4800 lines of
code) that is called from Rust via JNI. We didn’t enable this mutation strategy in the
experiments since this is not tightly relevant to DGF.
Table 1: Program statistics for our tested programs.
Program Size
Project
Binutils
Oniguruma
mjs
libjpeg
libpng
freetype2
cxxfilt
testcu
mjs
libjpeg
libpng
freetype
ics
2.8M 3232
1.3M
556
130
277K
749
810K
449
228K
1.6M
627
cs
12117
2065
3277
1827
1018
5681
ics/cs
26.67%
26.93%
3.97%
41.00%
44.11%
11.30%
# of CB>1
8813
3037
309
144
61
6784
# of CN >1
8879
3101
334
152
61
7117
ts
735s
5s
3s
2s
2s
4s
RQ1 Is the static analysis really worth the effort?
RQ2 How good is Hawkeye’s performance in terms of reproducing
RQ3 How effective are the dynamic strategies in Hawkeye?
RQ4 How good is the ability of Hawkeye for reaching the specific
the target crashes?
target sites?
Evaluation Dataset. We evaluated Hawkeye with diverse real-
world programs:
(1) GNU Binutils [5] is a collection of binary analysis tools used
in GNU/Linux platform. This benchmark is also used in several
other works such as [6, 7, 24].
(2) MJS [39] is an embedded JavaScript engine for C/C++ and used
in IoT development. It is used to compare Hawkeye directly
with AFLGo due to implementation limitations of the latter.
(3) Oniguruma [23] is a versatile regular expression library used
by multiple world famous projects such as PHP [33].
(4) Fuzzer Test Suite [18] is a set of benchmarks for fuzzing en-
gines. It contains several representative real-world projects.
Evalution Tools. We compare Hawkeye with the following
three fuzzers:
(1) AFL is the current state-of-the-art GF. It ignores all the tar-
get information for the PUT and only does the “basic block
transition” instrumentation.
(2) AFLGo is the state-of-the-art DGF based on AFL. Compared to
AFL, it also instruments basic block distance information.
(3) HE-Go is the fuzzer where the basic block level distance is
generated with our static analysis procedure (Fig. 3), but the
dynamic fuzzing is conducted by AFLGo.
Here we mainly follow AFLGo’s practice to only use AFL as
the baseline for coverage oriented GFs. Other techniques do not
focus on directed fuzzing, and they are either orthogonal (e.g.,
CollAFL [15]) or may sometimes perform worse than AFL (e.g.,
AFLFast, as observed by [38]), or not publicly available (e.g., An-
gora [12]). The detailed reason is available at our website; in §6, we
also provide a more detailed comparison between Hawkeye and
these techniques.
In the experiments, all AFL based fuzzers (AFL, AFLGo and
HE-Go) are run in their “fidgety” mode [50]. For both AFLGo and
HE-Go, “time-to-exploitation” is set to 45 minutes for the fuzzer.
Except for the experiments against GNU Binutils (Table 2) , where
we follow exactly the setup in AFLGo’s paper [6] , all the other
experiments are repeated 8 times, with a time budget of 4 hours. We
use “time-to-exposure” (TTE) to measure the length of the fuzzing
campaign until the first test input is generated that triggers a given
error (in §5.3) or reaches a target site (in §5.4). We use hitting round
to measure the number of runs in which a fuzzer triggers the error
or reaches the target. For all the experiments, if the fuzzer cannot
find the target crash within the time budget in one run, TTE is set
to the time budget value.
Our experiments are conducted on an Intel(R) Xeon(R) CPU E5-
2697 v3 @ 2.60GHz with 28 cores, running a 64-bit Ubuntu 16.04
LTS system; during experiments, we use 24 cores and retain 4 cores
for other processes.
5.2 Static Analysis Statistics
In Table 1, the first three columns denote the projects, programs
and the sizes in their LLVM bitcode form. ics denotes the number
of indirect call sites in the binary, which is calculated by counting
those call sites without explicitly known callees; cs is the number
of call sites; ics/cs denotes the percentage of indirect calls among all
call sites. The next two columns denote the number where CB > 1
and CN > 1 (§ 4.2), respectively. The last column denotes the time
cost of call graph generation, which takes the majority of the time
among all the directedness utility computation.
We can see from the table that the chosen targets have fair di-
versities in terms of different metrics. It is also noticeable that the
number of indirect function calls may contribute a large portion
to the total number of function calls. Specifically, in libpng, 44.11%
function calls are indirect function calls. This clearly shows the
importance of building precise call graphs. Furthermore, the num-
ber of occurrences of CN > 1 and the number of occurrences of
CB > 1 are also large, which shows the importance of taking into
consideration the different patterns of call relations.
As to the overhead of the static directedness utility computation,
except for cxxfilt, which requires approximately 12.5 minutes to
generates the call graph, call graphs of most other projects can be
generated in seconds. For cxxfilt, the performance degradation lies
in the inherent complexity of the project itself. From the program
statistics in Table 1), it is obvious that the code base is bigger and
the program structures are more complicated than the others. In
fact, the bottleneck of the analysis is inside the pointer analysis
implemented in SVF tool. We believe that it is worth the effort due
to the fact that this procedure is done purely statically. And as long
as the source code does not change, the call graph can be reused.
5.3 Crash Exposure Capability
The most common application of directed fuzzing is to try to expose
the crash with some given suspicious locations that are supposed to
be vulnerable, where the suspicious locations can be detected with
the help of other static or dynamic vulnerability detection tools. In
this experiment, we directly compare Hawkeye with other fuzzers
on some known crashes to evaluate its crash exposure capability.
Table 2: Crash reproduction in Hawkeye, AFLGo and AFL
against Binutils.
Table 3: Crash reproduction in Hawkeye, AFLGo and AFL
against MJS.
CVE-ID
2016-4487
2016-4488
2016-4489
2016-4490
2016-4491
2016-4492
2016-4493
2016-6131
Tool
Hawkeye
AFLGo
AFL
Hawkeye
AFLGo
AFL
Hawkeye
AFLGo
AFL
Hawkeye
AFLGo
AFL
Hawkeye
AFLGo
AFL
Hawkeye
AFLGo
AFL
Runs
20
20
20
20
20
20
20
20
20
9
5
7
20
20
20
9
6
2
µTTE(s)
177
390
630
206
180
420
103
93
59
18733
23880
20760
477
540
960
17314
21180
26340
Factor
–
2.20
3.56
–
0.87
2.04
–
0.90
0.57
–