stripped binary executables and shared libraries as our dataset. The
9000 binaries (3000 for each of the three architectures) are from
4Available at https://debin.ai
Instructions Functions Variables Known
Arch
Precision
Arch
x86
x64
ARM
Average
9363
7796
10416
9192
80
72
86
79
744
851
787
794
87
79
94
87
Table 2: Statistics on averaged number of instructions,
unknown function nodes, unknown variable nodes, and
known name nodes for our dataset. The values of the
known name nodes are extracted from the .dynsym section.
There is no known type node in the dependency graphs.
830 Linux Debug Symbol Packages [1], which include popular ones
such as coreutils, dpkg and gcc. The source code of these binaries
is written in the C language and the binaries are built with the
default compilation settings for each package. Therefore, multiple
optimization levels (e.g., -O0 to -O3) and other different compiler
options can be involved, which leads to a rich variety in the dataset.
Statistics of the dataset are shown in Table 2. Further, to learn the
feature functions and weights described in Section 4, we randomly
select 2700 binaries as the training set for each architecture. The
remaining 300 binaries are left as a benchmark for testing the
prediction accuracy of Debin.
Metrics. Debin consists of two prediction phases. First, Debin uses
a binary classifier based on the ET model to recover variables in
registers and memory offsets. We measure this step using accuracy:
Accuracy =
|TP | + |TN |
|P | + |N |
where TP is the true positives (i.e., registers and memory offsets
that are predicted to be variables and can actually be mapped to
variables), TN is the true negatives (i.e., registers and memory
offsets that are predicted not to be variables and actually do not
represent variables), P is the positive samples (i.e., registers and
memory offsets that can be mapped to variables), and N is the
negative samples (i.e., non-variable registers and memory offsets).
Second, structured prediction with the CRF model is employed to
predict properties (i.e., names and types) for recovered variables and
other elements. To measure the prediction quality of Debin after
this step, we track the following sets:
• Given Nodes (GN): the set of elements with debug information
(name or type) before stripping it. The set GN is formed of the
set P defined above in addition to elements which are functions
(these are not included in the set P).
• Nodes with Predictions (NP): the set of unknown elements deter-
mined by our fixed rules and variable recovery classification (via
the ET algorithm). These are nodes for which the CRF model
makes predictions.
• Correct Predictions (CP): the set of elements for which the CRF
model predicted the correct value (name or type). Here, correct
means that the predicted value is exactly equal to the values
given in the debug information. Note that assigning a value to a
non-variable will be counted as an incorrect prediction (defined
below).
x86
x64
ARM
Name
Type
Overall
Name
Type
Overall
Name
Type
Overall
62.6
63.7
63.1
63.5
74.1
68.8
61.6
66.8
64.2
Recall
62.5
63.7
63.1
63.1
73.4
68.3
61.3
68.0
64.7
F1
62.5
63.7
63.1
63.3
73.8
68.6
61.5
67.4
64.5
Table 3: Evaluation of Debin using structured prediction.
Precision =
|CP |
|GN | ;
|CP |
|NP | ; Recall =
With a perfect classifier, the set NP would ideally be the same as
the set GN. However, because the classifier is not perfect and may
introduce both false negatives and false positives, meaning the set
NP may end up being non-comparable to (or even larger than) GN.
The set CP is a subset of both NP and GN. To capture the quality of
the prediction, we use the following measures:
F1 = 2 × Precision × Recall
Precision + Recall
Intuitively, precision is the ratio of cases where the predicted value
is equal to the given value, among all of the predicted nodes (marked
as unknown by our rules and ET classifier). Recall refers to the pro-
portion of correct predictions over the set of nodes that originally
had debug information (i.e., the set GN). F1 score is a harmonic
average of precision and recall, examining the overall prediction
quality of Debin.
5.3 Evaluation on Prediction Accuracy
To objectively measure the accuracy of our probabilistic models, we
assume function scope information is given for every binary. How-
ever, in general, Debin can also leverage the built-in ByteWeight com-
ponent in BAP whose accuracy for recovering function boundaries
is around 93% [14].
Evaluation on Variable Recovery. First, we briefly discuss the
accuracy of variable recovery by the ET algorithm. For x86, the
accuracy is 87.1%, for x64 - 88.9% and for ARM - 90.6%. The high
accuracy in this step ensures that Debin can effectively recover
register-allocated and memory-allocated variables for later property
prediction. It also filters out temporarily allocated registers and
memory offsets and thus reduces noise.
Evaluation on Structured Prediction. Table 3 summarizes the
evaluation results of Debin after structured prediction. We report
results for name prediction, type prediction and overall (name+type)
prediction, measured by precision, recall and F1. Overall, the results
show that Debin predicts a considerable amount (recall over 63%)
of debug information with high precision (over 63%) across three
architectures and achieves a good trade-off between precision and
recall. These results indicate that our feature functions generalize
well over x86, x64 and ARM architectures.
e
r
o
c
s
1
F
80
70
60
50
40
30
20
10
x86
x64
ARM
50%
100%
Name
Type
25%
2%
5%
10%
Training size (fraction of full training set)
Figure 3: F1 score of name and type prediction with differ-
ent fractions of training set used for learning the CRF model.
For every fraction from 2% to 50%, we repeated 5 times down-
sampling of the training set and running experiments to ob-
tain scores. We finally report the averaged results.
For name prediction, Debin consistently achieves high accuracy
(F1 is 62.4% on average). This result shows that Debin often predicts
names identical to their original values. Indeed, programmers typi-
cally use similar names for variables (e.g., variables that represent
loop counters) or reuse functions (e.g., that open a file). For this
reason, the set of names observed in our training set contains most
of the names that appear in the testing set. We recall that the names
in our training set are used to instantiate the feature functions, and
this enables Debin to often recover the original names of variables
and functions (since Debin makes predictions according to these
feature functions). Later in this section, we provide examples of
name prediction outputs and illustrate how those names can be
employed for inspecting binary behaviors.
Further, Debin can also infer types accurately. The accuracy for
type prediction on x64 (F1 is 73.8%) is higher than on x86 (F1 is
63.7%) and ARM (F1 is 67.4%). This is likely because types on x64 (a
64-bit architecture) are generally more distinguishable in terms of
their bitwise sizes than on the other two 32-bit architectures. For
instance, on x64, the sizes of pointer type and int type are 64
bits and 32 bits, respectively, while on x86 and ARM they are both
32 bits. Our feature functions for types capture size differences and
thus achieve a higher accuracy on x64. On x86, the type prediction
accuracy is lower. A possible explanation is that x86 has fewer
register-allocated variables whose types can be effectively inferred
by operation relationships (e.g., x64 and ARM use registers for
argument passing while x86 uses the stack).
Here we remark that our measurement is a lower bound on the
capability of Debin since we test for unambiguously exact equiva-
lence. First, assigning a name or a type to a non-variable is always
counted as a mis-classification in our metric. However, the useful-
ness of the predictions may not be affected if the assigned names
and types comply with program semantics. Moreover, predicting a
different name for a register-allocated or memory-allocated vari-
able is also treated as an incorrect prediction in our measurements.
However, it is possible the predicted names are semantically close
to the real names. For instance, in our x64 testing set, we found
four cases where variable named buf is assigned the value buffer.
Name
i
s
p
self
cp
Ratio
2.42
1.72
1.24
0.92
0.79
Precision
52.7
65.1
47.6
65.2
69.5
Recall
77.1
66.1
63.6
55.3
77.4
F1
62.6
65.6
54.5
60.0
73.2
Table 4: Statistics on 5 of the most frequent names in the
test set. Column 2 shows a distribution ratio of every name
among all names in the test set.
Name
ip
device
passwd
socket
encrypted
Times
Precision
697
124
81
69
5
58.3
52.4
70.0
91.5
50.0
Recall
71.7
61.3
51.9
62.3
60.0
F1
64.4
56.5
59.6
74.1
54.5
Table 5: Statistics on 5 of the most sensitive names in the test
set. Column 2 shows the appearance times of each.
Finally, certain types have more proximity (e.g., signed types and
their unsigned counterparts) than others (e.g., struct with int).
Another intriguing question is how training set size for the CRF
model affects inference accuracy. To investigate this point, we kept
the ET model fixed and randomly sampled 2%, 5%, 10%, 25% and
50% of binaries from the full training set to train the CRF model for
each architecture. Then, we evaluated the trained models on the
same test set and report the F1 scores. The results are plotted in
Figure 3. We can see that as the size of the training set increases, the
accuracy also increases for both name and type prediction among
the three architectures. Since name prediction is more difficult than
type prediction, it is more data-hungry as its F1 scores grow more
rapidly with increasing training samples.
Name Prediction Outputs. Now we present more details con-
cerning the name prediction task. Table 4 shows five of the most
frequent names over three architectures. Debin predicts upon these
with an average F1 score of 63.2%. Even though these names are fre-
quent, they are useful when finding common programming pattern
of binary code. For example, i is often used as a loop index and
with it, reverse engineers can quickly identify loops and their condi-
tionals. Also, some of the simple names (e.g., s and p) may suggest
variable types and thus operation patterns upon them. For example,
variable with a predicted name s is likely to be of string type and
involved in string operations. This is helpful for understanding
semantics of binaries.
Apart from frequent names, we list five representative sensitive
names in Table 5. Variables predicted with these names typically
store critical security-related information such as IP addresses (ip),
device information (device), encryption (encrypted) and internet
connections (socket). For real-world applications, we can search
1
2
3
4
5
6
7
8
9
10
11
12
\\ snippet 1
if ( sub_806D9F0 (args) >= 0) {
...
sub_80522B0 (args);
...
}
\\ snippet 2
...
v2 = sub_818BFF1 ("/proc/net/tcp", 0, v45, a1);
if ( v2 == -1 ) return 0;
...
(a) Decompiled snippets for original malwares
1
2
3
4
5
6
7
8
9
10
11
12
\\ snippet 1
if ( setsockopt (args) >= 0 ) {
...
sendto (args);
...
}
\\ snippet 2
...
v2 = open ("/proc/net/tcp", 0, v23, a1);
if ( v2 == -1 ) return 0;
...