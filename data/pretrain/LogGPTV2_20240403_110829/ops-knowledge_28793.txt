User: I'm designing a distributed system where a master node starts a bunch of worker nodes on remote machines. Since I am using Python and want to take advantage of the fact each physical machine has several cores, I want to run multiple worker nodes per machine (GIL etc). Additionally, each worker node may vary quite a bit in the amount of CPU it requires each "cycle". I can however split the worker nodes into quite a few pieces and my initial strategy will be to spawn many more worker nodes than there are cores per machine. The reasoning being that if a few nodes require more CPU, they can occupy a core for a longer duration. (If each node was already CPU bound, it could not suddenly require more CPU.)
This leads me to a question: How can I accurately gauge the CPU time of a python process?
I cannot measure the time naively, I need the time actually spent specifically for a given process. That is, for each process I want a number X, which, as accurately as possible, represents the amount of CPU resources spent exclusively on that process, regardless of unrelated processes. (I have been looking at Python's getrusage but it appears to give only 2 decimal points of precision on ubuntu, which is insufficient. EDIT: This also happens if I use getrusage() directly in C; at most 0.01 second precision. Close, but no cigar)
My specific use-case would be to measure the CPU time of each node cycle, from Start to End, where End happens about 0-30ms after Start.
The best answer would be a portable way to do this in Python. Methods that requires using C extension is fine.
Assistant: The quick answer, at least for linux, is to use getrusage along with a kernel that has a higher resolution timer. 
The reason my initial tests gave the terrible precision of 10ms was because apparently 64-bit ubuntu is configured to a 100hz timer by default.