title:VTrace: Automatic Diagnostic System for Persistent Packet Loss in
Cloud-Scale Overlay Network
author:Chongrong Fang and
Haoyu Liu and
Mao Miao and
Jie Ye and
Lei Wang and
Wansheng Zhang and
Daxiang Kang and
Biao Lyv and
Peng Cheng and
Jiming Chen
VTrace: Automatic Diagnostic System for Persistent Packet Loss
in Cloud-Scale Overlay Network
Chongrong Fang♠ Haoyu Liu♠ Mao Miao♥
Daxiang Kang♥
Biao Lyv♥
Jie Ye♥
Peng Cheng♠
Lei Wang♥ Wansheng Zhang♥
Jiming Chen♠
♥Alibaba Group
ABSTRACT
Persistent packet loss in the cloud-scale overlay network severely
compromises tenant experiences. Cloud providers are keen to auto-
matically and quickly determine the root cause of such problems.
However, existing work is either designed for the physical network
or insufficient to present the concrete reason of packet loss. In this
paper, we propose to record and analyze the on-site forwarding
condition of packets during packet-level tracing. The cloud-scale
overlay network presents great challenges to achieve this goal with
its high network complexity, multi-tenant nature, and diversity of
root causes. To address these challenges, we present VTrace, an au-
tomatic diagnostic system for persistent packet loss over the cloud-
scale overlay network. Utilizing the "fast path-slow path" structure
of virtual forwarding devices (VFDs), e.g., vSwitches, VTrace in-
stalls several "coloring, matching and logging" rules in VFDs to
selectively track the packets of interest and inspect them in depth.
The detailed forwarding situation at each hop is logged and then
assembled to perform analysis with an efficient path reconstruc-
tion scheme. Experiments are conducted to demonstrate VTrace’s
low overhead and quick responsiveness. We share experiences of
how VTrace efficiently resolves persistent packet loss issues after
deploying it in Alibaba Cloud for over 20 months.
♠State Key Laboratory of Industrial Control Technology, Zhejiang University
1 INTRODUCTION
Generally, commercial cloud networking systems provide services
to cloud tenants by abstracting the underlying physical infrastruc-
ture into virtual networks with network virtualization technology
[9] and then logically isolating the tenants’ virtual networks from
each other. The virtual networks of cloud tenants are constructed
as the overlay network on top of the physical network, in which the
tenants’ production traffic is transmitted by both physical forward-
ing devices (PFDs, e.g., switches and routers) and virtual forwarding
devices (VFDs, e.g., vSwitches and vRouters). Due to this complex
and multi-tenant structure, even a small error in one PFD or VFD
could lead to persistent packet loss in the cloud network, which sig-
nificantly challenges the quality of service and the economic profit
of the cloud provider. Compared to PFDs in the physical network,
VFDs in the overlay network are updated frequently (e.g., weekly
or even daily) and can be influenced by tenants’ configurations,
which make them fallible since both the cloud side and tenant side
can result in packet losses. Therefore, to quickly mitigate or fix
persistent packet loss problems, besides the diagnostic system for
PFDs, it also strongly argues for a deep diagnostic tool for VFDs,
e.g., providing the server’s IP where the problematic VFD runs and
the specific reason for the packet loss in the VFD rather than just
giving the location information.
CCS CONCEPTS
• Networks → Network management;
KEYWORDS
Cloud-scale overlay network; Network diagnosis
ACM Reference Format:
Chongrong Fang, Haoyu Liu, Mao Miao, Jie Ye, Lei Wang, Wansheng Zhang,
Daxiang Kang, Biao Lyv, Peng Cheng, Jiming Chen. 2020. VTrace: Automatic
Diagnostic System for Persistent Packet Loss in Cloud-Scale Overlay Net-
work. In Annual conference of the ACM Special Interest Group on Data Commu-
nication on the applications, technologies, architectures, and protocols for com-
puter communication (SIGCOMM ’20), August 10–14, 2020, Virtual Event, USA.
ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3387514.3405851
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7955-7/20/08...$15.00
https://doi.org/10.1145/3387514.3405851
31
Unfortunately, a series of existing tools and systems such as [1,
7, 24, 26] either lack a comprehensive extension of their application
scenarios from the physical network to the cloud-scale overlay
network or provide no way to access a deep diagnosis for the root
cause. Thus, to troubleshoot the problematic production traffic, the
operations engineers of our cloud system first determine the set of
source and destination virtual machines (sVM and dVM), between
which the traffic is to be diagnosed. Then, they use TCPdump [22]
to check the connection state of an sVM and find the next hop.
This procedure is repeated until the culprit VFD is found or all
interested traffic is checked. In the sequel, network experts further
analyze the root cause of the problem (if it exists) with their skills
and experience. Even performing such root cause diagnosis in a
single data center is very complicated and time-consuming, not
to mention in the cloud networking system. This situation leads
to a growing demand to build an automatic diagnostic system.
Specifically, for the set of suspected sVM’s and dVM’s, the system
needs to automatically answer the following questions:
• Is the overlay network suffering a packet dropping problem?
• Where is the VFD that is responsible for the problem? Ex-
Example answer: There is a packet loss.
ample answer: It is vSwitch 1 in Server A.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
C. Fang et al.
• Why does the problem happen? Example answer: Some pack-
ets are dropped by vSwitch 1 because of the bandwidth limit
set by tenant A.
Developing such a system requires to track the production traf-
fic to find the culprit VFD and reflect detailed on-site forwarding
conditions, which faces several challenges. The first one is the
complexity of obtaining the trajectories of suspected traffic flows
within the cloud network. To put it mildly, there are millions of
VFDs in the cloud, which results in a rather complex topology of
the cloud-scale overlay network. Frequent updates or functional
migrations of VFDs make the network topology dynamic. What’s
more, the various optimization methods in Service Load Balancers
(SLBs) render virtual production flows unpredictable. In summary,
all these reasons bring difficulty in determining flow paths.
Secondly, the packet dropping could happen at each hop during
a packet’s transmission. In the cloud network, due to the distributed
nature of cloud computing systems, a packet may cross multiple
data centers that are geographically distributed around the world.
Therefore, the system should have the capability to trace and di-
agnose any production traffic flow within the cloud-scale overlay
network, which may require modifying a huge number of VFDs
across all cloud data centers. Such changes will unavoidably impact
network functions such as packet forwarding, data processing, and
so on. Hence, it is difficult but important to ensure that the system
lays a low impact on the production cloud network.
Thirdly, with the problematic VFD in hand, the concrete reason
for the packet loss is in need. Persistent packet dropping problems in
the overlay network arise from a variety of origins. Especially, some
of these problems come from tenants’ misoperations, which are
invisible to the cloud provider. In addition, as the cloud network is
dynamic and changeable, the problems in the cloud are endless and
constantly changing. Therefore, exploring the root cause through
feature extraction and modeling of the problem is not appropriate.
It is inherently challenging to automatically analyze the underlying
root cause of persistent packet loss in the cloud.
We develop VTrace, a diagnostic system for automatically fig-
uring out the particular reasons for persistent packet loss in the
cloud-scale overlay network. VTrace tracks the packets of interest
in virtual production flows to reflect on-site forwarding conditions.
To enable automatic diagnosis, VTrace configures the VFDs that
are the first hop of flows to mark a specified number of packets in
these flows, and installs "matching and logging" rules in all VFDs
to automatically log the footprint data of matched packets and the
corresponding reasons for a packet loss. Such a design leverages
the inherent forwarding function of VFDs and only needs to deeply
process a small number of packets in VFDs. The generated network-
wide log data is distributed in multiple region-level Log Agents. A
stream process platform, JStorm [2], is used to retrieve relevant
logs from Log Agents and reconstruct virtual flow paths to present
the diagnostic results.
We have conducted microbenchmarks to evaluate the impact of
VTrace on the forwarding performance of VFDs. Other key aspects
including bandwidth and storage overhead and path reconstruction
speed are also analyzed and evaluated. All these evaluations con-
sistently demonstrate the scalability and low overhead of VTrace.
Since May 2018, VTrace has been deployed in Alibaba Cloud to
(a) Interaction in the VPC.
(b) Interaction with the Internet.
(c) Interaction with customer data centers.
(d) Interaction between VPCs.
Figure 1: Types of virtual flows in the overlay network.
diagnose real-world persistent packet loss issues in the cloud-scale
overlay network. We share our deployment experiences with sev-
eral typical cases, including the packet loss in overlay and physical
networks. Each case details the symptoms of the problem and how
VTrace solves it or what we have learned from VTrace. Our experi-
ences show that VTrace is indeed effective, and it greatly reduces
the diagnostic time (i.e., a few minutes on average) compared to
manual diagnostics without VTrace (e.g., averagely several hours),
which greatly benefits the cloud provider.
Note that this work does not raise any ethical issues.
2 BACKGROUND
2.1 Production Cloud Network
2.1.1 Alibaba Cloud. Our production cloud system - Alibaba
Cloud - has deployed more than 30 large data centers around the
world to offer millions of tenants with reliable computing support
and unique hybrid cloud experience. In Alibaba Cloud, VXLAN [14]
is adopted to create the overlay network. Tenants can apply for a
Virtual Private Cloud (VPC) to construct their own private network.
It may contain various kinds of virtual devices, such as Elastic
Compute Service (ECS, a virtual cloud server provided by Alibaba
Cloud), SLBs, VFDs (including vSwitches, vBRouters (virtual border
router) and vRouters) and so on. Due to the implementation of
overlay techniques, the packets in virtual production flows are
transmitted from server to server by PFDs. Inside the server, VFDs
are responsible for data exchange in the overlay network.
2.1.2 Types of Virtual Flows in the Overlay Network. For busi-
ness needs, there are plenty of application scenarios in the overlay
32
ECS3ECS1SLBECS4vSwitchvSwitchvRouterVPCECS2ECS3ECS1SLBECS4vSwitchvSwitchvRouterVPCECS2ECS3ECS1SLBECS4vSwitchvSwitchvRouterVPCECS2Customer DatacentervBRouterECS3ECS1SLBECS4vSwitchvSwitchvRouterVPC 1ECS2ECS7ECS5SLBECS8vSwitchvSwitchvRouterVPC 2ECS6VTrace
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
network, which leads to various types of virtual flows. As a result,
these flows may cross different kinds of VFDs in distinct orders. In
the following, the frequently used notations and the four types of
virtual flows are provided.
Notations: The virtual production flow in this paper refers to the
production traffic from or to tenants’ VMs in the overlay network.
We will use virtual flow and virtual production flow interchangeably.
Additionally, the packet in the virtual flow to be diagnosed by
VTrace is defined as the target packet of VTrace.
Interaction in the VPC: In a VPC, a tenant’s application may be
completed by multiple VMs. For instance, applications based on
distributed computing need cooperation. Thus, this type of virtual
flow is generated. It may traverse through vSwitches and vRouters,
sometimes SLBs for load balancing, as the example in Fig. 1(a)
shows. Both the first and last hops of the flow should be vSwitches.
Interaction with the Internet: Considering applications provid-
ing service for users such as a web server, there could be numerous
data interactions between the application VM in the VPC and the
Internet. It can be realized by attaching a public IP or IP pool to
the vRouter. Obviously, as depicted in Fig. 1(b), the first hop of the
virtual flow is the vSwitch directly connected to the application
VM (ECS 1) while the last hop is a vRouter. On the contrary, the
opposite is true.
Interaction with customer data centers: VPC could connect
with customer data centers. As shown in Fig. 1(c), the virtual flow
begins from a VM (ECS 2) and accesses the customer data center
after crossing vSwitches, vRouters and vBRouters in order. As we
do not know the data transmission inside the customer data center
due to its privacy concern, the first or last hop of the virtual flow
could be a vBRouter.
Interaction between VPCs: Data interactions between VPCs also
exist. In Fig. 1(d), we take one direction of the virtual flow as an
example. In the overlay network, the packet originating from the
VM (ECS 2) in VPC 1 passes vSwitches and a vRouter and then enters
VPC 2, followed by going to ECS 7 via a vRouter and vSwitches in
order. Both the first and last hops of the flow should be vSwitches
that are adjacent to the VMs of interest (e.g., ECS 2 and ECS 7).
2.2 Diagnosis of Persistent Packet Loss in the
Overlay Network
The packet loss could be transient or persistent. The transient
packet drop may not have a visible impact currently, whereas the
persistent one will last for a considerably long time and severely
harm the profit of tenants or the cloud provider. Hence, it is more
emergent to tackle persistent issues. Note that we will use packet
loss and persistent packet loss interchangeably. Indeed, persistent
packet drops in the overlay network come from various origins and
are not necessarily faults. Similar to PFD faults, tenants will also
encounter network problems because of VFD faults like a misconfig-
ured forwarding rule or software errors. In addition, there are also
non-VFD faults, which are major reasons for packet losses in the
overlay network. In specific, tenants can modify their own virtual
networks. Each modification will be synchronized with the corre-
sponding cloud network configuration, e.g., the overlay network
topology changes if a VM is created. Hence, tenant behavior, in
33
turn, influences the cloud network, which can lead to packet losses.
For instance, the data packets of a tenant will be discarded by VFDs
if the bandwidth set up or purchased by the tenant is insufficient