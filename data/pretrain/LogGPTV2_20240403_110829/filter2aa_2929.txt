Don’t Red Team AI like a 
Chump
Ariel Herbert-Voss
@adversariel
Intro
• Who am I?
• What is this talk?
A story about hacking an AI system IRL
+
+
Key points in this talk (in case you zone out)
• Focus on the threat model
• Go for the jugular feature extraction process
• Think about the data supply chain
What is an AI system?
• AI is a class of algorithms that we use to extract 
actionable information from data
• AI is not new and the hype is real
• In this talk, AI == ML
AI
Machine 
learning
Deep 
learning
What is an AI system?
What is an AI system?
data
What is an AI system?
data
prediction
What is an AI system?
data
prediction
Parts of an AI system we can exploit
• Data
• Training/testing sets
• Deployed environment data
• Model
• Algorithm
• Parameters
Parts of an AI system we can exploit
• Data
• Training/testing sets
• Deployed environment data
• Model
• Algorithm
• Parameters
data poisoning
Parts of an AI system we can exploit
• Data
• Training/testing sets
• Deployed environment data
• Model
• Algorithm
• Parameters
data poisoning
Adversarial examples
adversarial example 
data point
DOG
FRIED CHICKEN
(arbitrary) feature dimension 1
(arbitrary) feature dimension 2
target 
data point
[Biggio et al, 2018]
Parts of an AI system we can exploit
• Data
• Training/testing sets
• Deployed environment data
• Model
• Algorithm
• Parameters
backdoored function
Parts of an AI system we can exploit
• Data
• Training/testing sets
• Deployed environment data
• Model
• Algorithm
• Parameters
model inversion
Model inversion
Image of Bill: 
*exists in training data set*
Recovered image
[Fredrikson et al, 2015]
Parts of an AI system we can exploit
• Data
• Training/testing sets
• Deployed environment data
• Model
• Algorithm
• Parameters
model theft
Model theft
data
target model
stolen model
original 
predictions
similar 
predictions
[Tramèr et al, 2016]
How to design an attack for an AI system
1. What model are you attacking?
2. Where do the data come from?
3. Where do the predictions go?
Fooling AI-powered video surveillance
• Common system components
• Detection: on-premises algorithm that checks each still image to see if there is 
a face for it to forward to the recognition system
• Recognition: off-site look-up of flagged images using a data base of known 
faces
camera
detection system
recognition 
system
Fooling AI-powered video surveillance
1. What model are you attacking? 
YOLO v2
2. Where do the data come from? 
video frames
3. Where do the predictions go? 
stored as set of flagged frames
Fooling AI-powered video surveillance
[Thys et al, 2019]
Fooling AI-powered video surveillance
[Brown et al, 2017]
Fooling AI-powered video surveillance
[Thys et al, 2019]
AI red teaming is out of whack
• Red teaming AI is often conflated with the academic discipline of 
adversarial machine learning
Cool ways to attack an AI model with math
== adversarial ML
Evaluate the security of an AI system 
== red teaming AI
Adversarial ML attack tree
goal: make the 
object detector 
ignore person
minimize specific 
class likelihood
minimize 
“objectyness” 
likelihood
minimize both
Fooling AI-powered video surveillance
AI red team attack tree
goal: make the 
object detector 
ignore person
get physical 
control of the 
camera
put a sticker over 
the camera
remove the 
power source
move the camera 
to a different 
location
get access to the 
camera network
play looping 
footage
get access to 
facial detection AI 
software
minimize specific 
class likelihood
minimize 
"objectyness" 
likelihood
minimize both
Fooling AI-powered video surveillance
AI red team attack tree
goal: make the 
object detector 
ignore person
get physical 
control of the 
camera
put a sticker over 
the camera
remove the 
power source
move the camera 
to a different 
location
get access to the 
camera network
play looping 
footage
get access to 
facial detection AI 
software
minimize specific 
class likelihood
minimize 
"objectyness" 
likelihood
minimize both
Fooling AI-powered video surveillance
Adversarial ML attack tree
goal: make self-
driving car think a 
lane is not a lane
generate 
adversarial 
example
Breaking self-driving car lane detection
AI red team attack tree
goal: make self-
driving car think a 
lane is not a lane
dump a salt line 
on the road
put stickers on 
the road
get physical 
access to car
get access to lane 
detection system
generate 
adversarial 
example
Breaking self-driving car lane detection
Adversarial ML attack tree
goal: make self-
driving car think a 
lane is not a lane
dump a salt line 
on the road
put stickers on 
the road
get physical 
access to car
get access to lane 
detection system
generate 
adversarial 
example
Breaking self-driving car lane detection
How to design an attack for an AI system
1. What model are you attacking?
2. Where do the data come from? 
3. Where do the predictions go?
How to design an attack for an AI system
1. What model system are you attacking?
2. Where do the data come from? 
3. Where do the predictions go?
How to design an attack for an AI system
1. What system are you attacking?
2. What is the data processing pipeline?
i.
Where do the data come from?
ii.
What is the data representation? 
iii. What is the output?
How to design an attack for an AI system
1. What system are you attacking?
2. What is the data processing pipeline?
i.
Where do the data come from?
ii.
What is the data representation? 
iii. What is the output?
3. What is the threat model?
How to design an attack for an AI system
1. What system are you attacking?
camera
detection system
recognition 
system
How to design an attack for an AI system
2.   What is the data processing pipeline?
i.
Where do the data come from?
ii.
What is the data representation? 
iii. What is the output?
feature 
extraction
camera
detection system
recognition 
system
decision 
making
How to design an attack for an AI system
3.   What is the threat model?
goal: make the face 
detector ignore 
person
get physical control 
of the camera
put a sticker over the 
camera
remove the power 
source
move the camera to a 
different location
get access to the 
camera network
DDoS the network
play looping footage
move camera
get access to facial 
detection AI system
get access to training 
procedure
poison data set
force backdoored 
function
use scene statistics to 
generate adversarial 
example
minimize specific 
class likelihood
minimize 
"objectyness" 
likelihood
minimize both
Tl;dw
• Go for the jugular feature extraction process
• Think about the data supply chain
• Focus on the threat model
For more information (and math!)
Feel free to reach out via Twitter  (@adversariel) and/or grab a beer with me :)
• Attacks:
• Tencent Keen Security Lab, 2019 -
https://keenlab.tencent.com/en/whitepapers/Experimental_Security_Research_of_Tesla_Au
topilot.pdf
• [Biggio et al, 2018] - https://pralab.diee.unica.it/sites/default/files/biggio18-pr.pdf
• [Fredrikson et al, 2015] - https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf
• [Tramèr et al, 2016] - https://arxiv.org/pdf/1609.02943.pdf
• [Thys et al, 2019] - https://arxiv.org/pdf/1904.08653.pdf
• [Brown et al, 2017] - https://arxiv.org/pdf/1712.09665.pdf
• Threat modeling:
• Schneier, 1999 - https://www.schneier.com/academic/archives/1999/12/attack_trees.html