rectly to the address space of the monitoring application and
thus the operating system is completely bypassed.
Degiovanni and others [10] show that ﬁrst generation
packet capture accelerators are not able to exploit the paral-
lelism of multi-processor architectures and propose the adop-
tion of a software scheduler to increase the scalability. The
scalability issue has been solved by modern capture accel-
erators that provide facilities to balance the traﬃc among
several threads of execution. The balancing policy is imple-
mented by their ﬁrmware and it is not meant to be changed
at run-time as it takes seconds if not minutes to reconﬁgure.
The work described in [19] highlights the eﬀects of cache
coherence protocols in multi-processor architectures in the
context of traﬃc monitoring. Papadogiannakis and others
Figure 4: Core Mapping on Linux with the Dual
Xeon. Hyper-Threads on the same core (e.g. 0 and
8) share the L2 cache.
Table 3: Packet capture performance (kpps) when
capturing concurrently from two 1 Gbit links.
Test Capture
threads
aﬃnity
not set
NIC1@0 on 0
NIC1@1 on 8
NIC2@0 on 2
NIC2@1 on 10 NIC2@1 on 10
NIC1 on 0,8
NIC2 on 2,10
Polling
threads
aﬃnity
not set
NIC1@0 on 0
NIC1@1 on 8
NIC2@0 on 2
NIC1@0 on 0
NIC1@1 on 8
NIC2@0 on 2
NIC2@1 on 10
1
2
3
NIC1 NIC2
Kpps
Kpps
1158
1122
1032
1290
1488
1488
capture threads and we perform measurements with three
conﬁgurations. First of all, we measure the packet capture
rate when one capture thread and one polling thread per
queue are spawn (8 threads in total) without setting the
CPU aﬃnity (Test 1). Then (Test 2), we repeat the test
by binding each capture thread to the same Hyper-Thread
where the polling thread for that queue is executed (e.g. for
the queue NIC1@0 both polling and capture thread run on
Hyper-Thread 0). Finally, in Test 3, we reduce the num-
ber of capture threads to one for each interface. For each
NIC, the capture thread and the polling threads associated
to that interface run on the same core.
Table 3 reports the maximum loss-free rate when captur-
ing from two NIC simultaneously using the conﬁgurations
previously described. As shown in Test 1, without prop-
erly tuning the system by means of CPU aﬃnity, our test
platform is not capable of capturing, at wire-rate, from two
adapters simultaneously. Test 2 and Test 3 show that the
performance can be substantially improved by setting the
aﬃnity and wire-rate is achieved. In fact, by using a sin-
gle capture thread for each interface (Test 3) all incoming
packets are captured with no loss (1488 kpps per NIC).
In principle, we would expect to achieve the wire-rate with
the conﬁguration in Test 2 rather than the one used in Test
3. However, splitting the load on two RX queues means that
capture threads are idle most of the time, at least on high-
end processors such as the Xeons we used and a dummy
application that only counts packets. As a consequence,
capture threads must call poll() very often as they have no
packet to process and therefore go to sleep until a new packet
arrives; this may lead to packet losses. As system calls are
slow, it is better to keep capture threads busy so that poll()
units and reports for each of them the core identiﬁer and the
physical CPU. Processing units sharing the same physical
CPU and core identiﬁer are Hyper-Threads.
222[22] show how to preserve cache locality for improving traﬃc
analysis performance by means of traﬃc reordering.
Multi-core architectures and multi-queue adapters have
been exploited to increase the forwarding performance of
software routers [14, 15]. Dashtbozorgi and others [9] pro-
pose a traﬃc analysis architecture for exploiting multi-core
processors. Their work is orthogonal to ours, as they do
not tackle the problem of enhancing packet capture through
parallelism exploitation.
Several research eﬀorts show that packet capture can be
substantially improved by customizing general purpose op-
erating systems. nCap [12] is a driver that maps the card
memory in user-space, so that packets can be fetched from
user-space without any kernel intervention. The work de-
scribed in [26] proposes the adoption of large buﬀers contain-
ing a long queue of packets to amortize the cost of system
calls under Windows. PF RING [11] reduces the number of
packet copies, and thus, increases the packet capture perfor-
mance, by introducing a memory-mapped channel to carry
packets from the kernel to the user space.
6. OPEN ISSUES AND FUTURE WORK
This work represents a ﬁrst step toward our goal of ex-
ploiting the parallelism of modern multi-core architectures
for packet analysis. There are several important steps we
intend to address in future work. The ﬁrst step is to intro-
duce a software layer capable of automatically tuning the
CPU aﬃnity settings, which is crucial for achieving high
performance. Currently, choosing the correct CPU aﬃn-
ity settings is not a straightforward process for non-expert
users.
In addition, one of the basic assumption of our technol-
ogy is that the hardware-based balancing mechanism (RSS
in our case) is capable of evenly distributing the incoming
traﬃc among cores. This is often, but not always, true in
practice. In the future, we plan to exploit mainstream net-
work adapters supporting hardware-based and dynamically
conﬁgurable balancing policies [13] to implement an adap-
tive hardware-assisted software packet scheduler that is able
to dynamically distribute the workload among cores.
7. CONCLUSIONS
This paper highlighted several challenges when using
multi-core systems for network monitoring applications: re-
source competition of threads on network buﬀer queues,
unnecessary packet copies, interrupt and scheduling imbal-
ances. We proposed a novel approach to overcome the ex-
isting limitations and showed solutions for exploiting multi-
cores and multi-queue adapters for network monitoring. The
validation process has demonstrated that by using TNAPI it
is possible to capture packets very eﬃciently both at 1 and
10 Gbit. Therefore, our results present the ﬁrst software-
only solution to show promise towards oﬀering scalability
with respect to the number of processors for packet captur-
ing applications.
8. ACKNOWLEDGEMENT
The authors would like to thank J. Gasparakis and P.
Waskiewicz Jr from IntelTMfor the insightful discussions
about 10 Gbit on multi-core systems, as well M. Vlachos,
and X. Dimitropoulos for their suggestions while writing this
paper.
9. CODE AVAILABILITY
This work is distributed under the GNU GPL license
and is available at no cost from the ntop home page
http://www.ntop.org/.
10. REFERENCES
[1] PF RING User Guide.
http://www.ntop.org/pfring userguide.pdf.
[2] cpacket networks - complete packet inspection on a
chip. http://www.cpacket.com.
[3] Endace ltd. http://www.endace.com.
[4] Ixia leader in converged ip testing. Homepage
http://www.ixiacom.com.
[5] Libpcap. Homepage http://www.tcpdump.org.
[6] A. Agarwal. The tile processor: A 64-core multicore
for embedded processing. Proc. of HPEC Workshop,
2007.
[7] K. Asanovic et al. The landscape of parallel
computing research: A view from berkeley. Technical
Report UCB/EECS-2006-183, EECS Department,
University of California, Berkeley, Dec 2006.
[8] A. Cox. Network buﬀers and memory management.
The Linux Journal, Issue 30,(1996).
[9] M. Dashtbozorgi and M. Abdollahi Azgomi. A
scalable multi-core aware software architecture for
high-performance network monitoring. In SIN ’09:
Proc. of the 2nd Int. conference on Security of
information and networks, pages 117–122, 2009.
[10] L. Degioanni and G. Varenni. Introducing scalability
in network measurement: toward 10 gbps with
commodity hardware. In IMC ’04: Proc. of the 4th
ACM SIGCOMM conference on Internet
measurement, pages 233–238, 2004.
[11] L. Deri. Improving passive packet capture: beyond
device polling. Proc. of SANE, 2004.
[12] L. Deri. ncap: Wire-speed packet capture and
transmission. In E2EMON ’05: Proc. of the
End-to-End Monitoring Techniques and Services,
pages 47–55, 2005.
[13] L. Deri, J. Gasparakis, P. Waskiewicz Jr, and
F. Fusco. Wire-Speed Hardware-Assisted Traﬃc
Filtering with Mainstream Network Adapters. In
NEMA’10: Proc. of the First Int. Workshop on
Network Embedded Management and Applications,
page to appear, 2010.
[14] N. Egi, A. Greenhalgh, M. Handley, M. Hoerdt,
F. Huici, L. Mathy, and P. Papadimitriou. A platform
for high performance and ﬂexible virtual routers on
commodity hardware. SIGCOMM Comput. Commun.
Rev., 40(1):127–128, 2010.
[15] N. Egi, A. Greenhalgh, M. Handley, G. Iannaccone,
M. Manesh, L. Mathy, and S. Ratnasamy. Improved
forwarding architecture and resource management for
multi-core software routers. In NPC ’09: Proc. of the
2009 Sixth IFIP Int. Conference on Network and
Parallel Computing, pages 117–124, 2009.
[16] F. Fusco, F. Huici, L. Deri, S. Niccolini, and T. Ewald.
Enabling high-speed and extensible real-time
communications monitoring. In IM’09: Proc. of the
11th IFIP/IEEE Int. Symposium on Integrated
Network Management, pages 343–350, 2009.
223[17] Intel. Accelerating high-speed networking with intel
[23] L. Rizzo. Device polling support for freebsd.
i/o acceleration technology. White Paper, 2006.
BSDConEurope Conference, 2001.
[18] Intel. Intelligent queuing technologies for
virtualization. White Paper, 2008.
[19] A. Kumar and R. Huggahalli. Impact of cache
coherence protocols on the processing of network
traﬃc. In MICRO ’07: Proc. of the 40th Annual
IEEE/ACM Int. Symposium on Microarchitecture,
pages 161–171, 2007.
[20] R. Love. Cpu aﬃnity. Linux Journal, Issue 111,(July
2003).
[21] B. Milekic. Network buﬀer allocation in the freebsd
operating system. Proc. of BSDCan, 2004.
[22] A. Papadogiannakis, D. Antoniades,
M. Polychronakis, and E. P. Markatos. Improving the
performance of passive network monitoring
applications using locality buﬀering. In MASCOTS
’07: Proc. of the 2007 15th Int. Symposium on
Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems, pages 151–157, 2007.
[24] B. M. Rogers, A. Krishna, G. B. Bell, K. Vu, X. Jiang,
and Y. Solihin. Scaling the bandwidth wall: challenges
in and avenues for cmp scaling. SIGARCH Comput.
Archit. News, 37(3):371–382, 2009.
[25] J. H. Salim, R. Olsson, and A. Kuznetsov. Beyond
softnet. In ALS ’01: Proc. of the 5th annual Linux
Showcase & Conference, pages 18–18, Berkeley, CA,
USA, 2001. USENIX Association.
[26] M. Smith and D. Loguinov. Enabling
high-performance internet-wide measurements on
windows. In PAM’10: Proc. of Passive and Active
Measurement Conference, pages 121–130, Zurich,
Switzerland, 2010.
224