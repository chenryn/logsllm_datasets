### Drawing Stronger Conclusions about the Role of Demographics in Speech Recognition Systems

#### Measuring the Harms of Skill Squatting
The effectiveness of our attack in real-world scenarios remains unclear. To observe this, we would need to submit public skills to Amazon for certification. Additionally, our work does not explore the potential actions an attacker could take once a target skill is successfully squatted. In initial testing, we successfully built phishing attacks on top of skill squatting, such as against the American Express skill. However, investigating the scale of such attacks is beyond the scope of this work. We hypothesize that the most significant risk comes from the possibility that an attacker could steal credentials to third-party services, but this topic merits further investigation.

#### Investigating IoT Trust Relationships
On the web, many users have been conditioned to be security-conscious, primarily through browser warnings. An open question is whether this conditioning transfers to a voice-controlled IoT setting. If an attacker realizes that users trust voice interfaces more than other forms of computation, they may develop more targeted and effective attacks on voice-interfaces.

#### Generalizing Our Models
An important question is whether our models can be broadly generalized to other speech recognition systems. It is unlikely that our Alexa-specific model of systematic errors will directly translate to other systems. However, the techniques we use to build these models can be applied as long as we can leverage a speech recognition system as a black box. Future work should focus on replicating our techniques across different speech recognition systems.

### Related Work

Our research builds on interdisciplinary studies, including linguistics, human aspects of security, and targeted audio attacks on voice-controlled systems.

#### Dialects in Speech
Linguists have developed models of English speech since the 1970s, covering intonation and rhythm patterns. Recently, researchers have used phoneme and vowel data, similar to the NSP dataset, to study speech patterns by region and gender. Clopper has also investigated the effects of dialect variation within sentences on "semantic predictability," which is the ability of a listener to discern words based on their context.

#### Typosquatting and Human Factors
Our work aligns with research on the human aspects of security, such as susceptibility to spam or phishing attacks. Specifically, we focus on the long history of domain typosquatting. Nikiforakis et al. used homophone confusion to find vulnerable domain names, while Tahir et al. explored why some URLs are more susceptible to typosquatting. Our work also draws on analysis of attack vectors that go beyond simple mistakes, such as Kintis et al.'s study on the longitudinal effects of "combosquatting" attacks.

#### Other Skill Squatting Attacks
We are not alone in highlighting the need to investigate the security of speech recognition systems. Zhang et al. reported a variant of the skill squatting attack, where Alexa favors the longest matching skill name when processing voice commands. This allows an attacker to register a skill with a slightly longer name to intercept the user's intended skill. Their attack demonstrates dangerous logic errors in the voice assistantâ€™s skills market. In contrast, our work considers how the intrinsic errors in natural language processing algorithms can be weaponized to attack speech recognition systems.

#### Audio Attacks
Researchers have repeatedly shown that acoustic attacks are a viable vector for causing harm in computing devices. For example, deliberate audio can cause drones to malfunction and crash. Audio attacks have also been used to bias sensor input on Fitbit devices and manipulate toy RC cars. Furthermore, audio has been used as an effective side channel to steal private key information and leak private data through vibration sensors. Several researchers have developed adversarial examples of audio input to trick voice-based interfaces, such as Carlini et al.'s demonstration of indiscernible audio that activates devices, and Houdini's construction of adversarial audio files that lead to invalid transcriptions.

### Conclusion
In this work, we investigated the interpretation errors made by Amazon Alexa for 11,460 speech samples from 60 speakers. We found that some classes of interpretation errors are systematic, meaning they appear consistently in repeated trials. We then showed how an attacker can leverage these systematic errors to surreptitiously trigger malicious applications for users in the Alexa ecosystem. Further, we demonstrated how this attack could be extended to target users based on their demographic information. We hope our results inform the security community about the implications of interpretation errors in speech recognition systems and provide a foundation for future research in this area.

### Acknowledgements
This work was supported in part by the National Science Foundation under contracts CNS 1750024, CNS 1657534, and CNS 1518741, and by the U.S. Department of Homeland Security contract HSHQDC-17-J-00170. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of their employers or the sponsors.

### References
[References listed here as provided in the original text]