about how to handle this concurrency. The two primary op-
tions are to multiplex them through a single process using
select or a similar mechanism, or to handle them con-
currently with multiple processes or threads. The choice
of concurrency strategy can signiﬁcantly impact complex-
ity and scalability. We discuss this further in sections 4.2
and 5.5 respectively.
3.1 Filtering architectures
Figure 1: Filtering architecture
Most existing application sandboxes have a ﬁltering ar-
chitecture, illustrated by Figure 1. It consists of two parts:
a kernel-based tracing mechanism to ﬁlter the system calls
of a sandboxed application, and a user-level “monitor” that
tells the tracing interface which calls to allow or deny based
on a user-speciﬁed policy.
In a ﬁltering sandbox, when a sandboxed process
(“client”) executes a sensitive call,
the process tracing
mechanism puts it to sleep and sends a request to the mon-
itor. The monitor responds to the request with “allow” or
“deny” based on the policy engine’s judgment. The tracing
mechanism then wakes up the sandboxed process. If the call
is allowed, the client’s call proceeds normally. If the call is
denied, the call is forced to return an error code immedi-
ately. Calls which are not deemed sensitive by the monitor
are never trapped by the tracing interface, and thus execute
as they would normally in an unsandboxed application.
3.2 Delegating architectures
Our new sandbox, Ostia, has the delegating architecture
depicted in Figure 2. It has two primary parts: a kernel por-
tion that enforces a hard-coded policy preventing all calls
that provide direct access to sensitive resources (e.g. open,
socket) from being executed, and a user-level portion
Figure 2: Delegating architecture
(“agent”) that performs access to sensitive resources on be-
half of the sandboxed process (“client”) where permitted by
the policy engine. These systems usually have a third part,
that we refer to as the emulation library. The emulation li-
brary resides in the address space of sandboxed processes.
It converts a sandboxed process’s sensitive system calls into
IPC requests to the agent. How exactly this is done is im-
plementation dependent (section 4.2 discusses the approach
we use).
When a client makes a sensitive system call, it is redi-
rected to the emulation library, which sends a request to its
agent via an IPC channel.
If the request is permitted by
policy, the agent accesses the requested resource (possibly
executing one or more system calls) and returns the result
(e.g. return code, descriptor) to the client. As in a ﬁlter-
ing sandbox, calls which do not provide access to sensitive
resources but merely use resources the client has already
obtained (e.g. read, write) are executed directly by the
client.
The fact that the agent both checks permissions and ac-
cesses the requested resource on the child’s behalf is the
most important distinction between the agent in a delegat-
ing sandbox and the monitor in a ﬁltering sandbox. The
delegating sandbox gets its name from the fact that the abil-
ity to access sensitive resources is revoked from the client
and delegated to the agent.
4 Implementations
This section describes in more detail the implementa-
tion of the relevant parts of Ostia, our delegating sandbox.
To make our comparisons against ﬁltering sandboxes more
concrete, we also brieﬂy describe J2, our ﬁltering sandbox.
More detailed descriptions of Janus [36, 18], J2 [16, 29],
and other very similar ﬁltering architectures [2, 3] are avail-
able elsewhere. We present further details in later sections
as they become relevant.
processprocessprocessSystem Call EntryKernel ProperMonitortracing InterfaceApplicationopen("foo")Allow/DenyUser SpaceKernel Spaceopen("foo")open("foo")DenyresultAllowopen("foo")resultresultagentemulation libraryProcessagentemulation libraryProcessApplicationUser SpaceKernel Spacerestricted interfaceresultrequestresultrequest4.1 J2
J2 (Janus version 2) is a canonical example of a ﬁltering
architecture.
It was developed through successive rewrit-
ings of the original Janus system and retains its basic struc-
ture. It differs most prominently in its use of a dedicated
process tracing mechanism, mod janus, speciﬁcally de-
signed for secure interposition, instead of relying on an ex-
isting process tracing interface.
mechanism: J2’s
Tracing
mechanism,
mod janus, was originally developed in response to
the shortcomings of existing process tracing mechanisms
for supporting secure system call interposition [36].
tracing
mod janus provides a simple interface for the monitor
process. To sandbox a process the monitor attaches a de-
scriptor to it and speciﬁes which system calls to trap and
which to allow. The monitor calls select on the descrip-
tors associated with its sandboxed processes to poll for trap
events. Trap events are generated when the sandboxed pro-
cess makes a “trapped” system call. When a trap event is
pending on a descriptor, the type of call that was trapped
and call arguments can be read from the descriptor. Once a
process has generated a trap it is put into an uninterruptible
sleep state and can only continue once given an “allow” or
“deny” by the monitor. It is impossible for a process to es-
cape the sandbox; closing a descriptor kills its process, and
descriptors cannot be unbound.
Most trapped events are entries into system calls. An ex-
ception is fork, whose exit is trapped to allow the monitor
to attach to the new child process. To ensure that the child
cannot execute any calls outside the sandbox, mod janus
ensures that the monitor attaches to it before it is allowed to
begin execution.
When a system call is trapped, call arguments (e.g. path
names and struct sockaddrs) are immediately copied
out of the process’s address space and into a per-process
kernel buffer. When pathnames are copied into the ker-
nel they are resolved (canonicalized) with symlinks being
expanded in the context of the trapped process. This en-
sures that canonicalization takes place in the proper names-
pace, i.e. if the process is chrooted, makes a reference to
/proc/self, or there is some other per-process variation
in the name space, this will be taken into account. The ker-
nel is redirected to this internal copy of the arguments for
evaluating the call. Copying arguments into the kernel pre-
vents arguments from being modiﬁed which could lead to
certain types of race conditions. It does not prevent other
kinds of races, as will be discussed in section 5.1.
Concurrency strategy: In a ﬁltering sandbox like J2, both
single-threaded select-based and multithreaded architec-
tures are feasible. The original Janus prototype used one
monitor process per sandboxed process. J2’s monitor uses a
multiplexing model to handle concurrency, in which a sin-
gle monitor process polls for client requests with select
followed by a read from the descriptor associated with
the pending request. The decision was made to go to a
select-based model in J2 as a result of the belief that
this would substantially reduce overhead under load. How-
ever, as we discuss in section 5.5, this seems to have actu-
ally hurt scalability because the single monitor becomes a
performance bottleneck.
4.2 Ostia
Ostia implements a delegating sandbox architecture. As
described in the previous section, it is composed of three
primary components.
Kernel module: A small kernel module enforces Ostia’s
static policy of denying any call that provides direct access
to sensitive system resources. This is done simply by pre-
venting a ﬁxed set of system calls from executing. (As a
belt-and-suspenders measure to ensure that access to the ﬁle
system is denied, sandboxed processes are chrooted to an
empty directory if Ostia is run as root.)
It also provides a trampoline mechanism that redirects
delegated calls back into the emulation library as discussed
below. Finally, it implements an fexecve call because
execve cannot be delegated to another process, for obvi-
ous reasons.
Emulation library: Ostia uses a callback mechanism in
the kernel module to redirect system calls. (Ostia evolved
from an earlier delegating system we built that relied on
shared library replacement to redirect system calls. We
note this to emphasize that system call redirection—or
virtualization—can be done multiple ways.) When a sensi-
tive system call reaches the kernel entry point, it calls back
into the handler in a special emulation library in the pro-
gram’s address space. The emulation library transforms the
system call into a request to the agent. To speed up subse-
quent system calls from the same point in the code, the han-
dler also examines the machine instructions that made the
call and, if they take the expected form, patches them in-
place to jump directly to the handler, avoiding subsequent
round trips through the kernel.
Ostia’s handler must be installed into the program’s ad-
dress space before the program gets control.
It must be
available even before the loader for dynamic libraries takes
control, so that access to dynamic libraries can go through
the agent. Ostia does this by implementing its own ELF
binary loader in user space. Instead of executing the sand-
boxed program directly, it executes the loader program,
which contains the emulation library and a startup routine.
The startup routine registers the handler, manually loads the
sandboxed program with mmap calls, and turns over con-
trol. The emulation library ensures that this happens on ev-
ery execve by a client.
A process’s emulation library sends requests (similar to
RPC calls) to its respective agent over a UNIX domain
socket. UNIX domain sockets are more than simply an in-
terface for passing messages. They also allow ﬁle descrip-
tors to be passed between process and agent. This feature
is critical as it permits delegation of obtaining capabilities
(e.g. open ﬁles) to the agent, while permitting processes to
operate on capabilities (e.g. reading and writing ﬁles) di-
rectly.
Agents: As discussed in section 3, agents are responsible
for reading the policy ﬁle, starting the initial sandboxed pro-
cess, making policy decisions, etc. Each sandboxed pro-
cess has its own agent. The most important function that
an agent provides to its sandboxed process (or “client”) is
handling requests for calls from the emulation library.
System calls can be divided into three classes: calls that
must be delegated, calls that are always permitted, and calls
that are completely disallowed. Refer back to the policy
model given in section 3 for additional background on the
reasoning behind each category. Each sandboxed process
has an agent to handle its delegated calls. Delegated calls
fall into a few subcategories:
• File system and network operations: In Unix, ﬁles
and network sockets are often used (read, written, etc.)
via descriptors. Applications are always started with
a descriptor space containing only the standard input,
output, and error descriptors. This ensures that appli-
cations can only gain access to resources explicitly per-
mitted by the sandbox.
Any operation that refers to resources by name (i.e. a
ﬁle by path name or network host by address) and not
by descriptor must be delegated.
Calls that refer to resources by name and grant access
to descriptors (e.g. open, socket) are delegated by
requesting the descriptor from the agent. For exam-
ple, when the agent receives an open request, it ﬁrst
checks policy.
If the open is permitted, the agent
opens the ﬁle and passes the descriptor to the sand-
boxed process.
Calls that refer to resources by name but do not
grant access to descriptors include rename, chmod,
mkdir and sendto. These are delegated by execut-
ing the operation in the agent. In this case no descrip-
tor is returned. However, as with all delegated calls, a
return value is passed back to the client reﬂecting the
result returned by the system call, e.g. an error such
as EPERM. As an exception, sendmsg and recvmsg
on a Unix domain socket between a client and its agent
are allowed via direct system calls to permit commu-
nication with the agent.
Calls that modify the properties of objects referred to
by descriptors already held by a client (e.g. ioctl,
bind) are delegated by passing the object’s descrip-
tor to the agent. The agent can query the descriptor for
the object’s state (via e.g. getpeername or fstat),
and if the modiﬁcation conforms to the agent’s policy,
modify the object and return a success code to the re-
questing process.
Calls that operate on a descriptor’s object, but do not
change its security relevant properties (e.g. read,
write, fstat) are not delegated. Similarly, calls
that modify a process’s descriptor space but do not
grant access to new resources (e.g. dup2, close) are
also not delegated. As discussed in section 3, doing so
is unnecessary and could incur signiﬁcant performance
overhead.
execve is an odd corner case, where a call refers to
a ﬁle by name but cannot be delegated. We addressed
this by adding an fexecve call via the kernel module.
The agent must take care to ensure that the opera-
tions it performs involving ﬁle names are not subject
to race conditions. We discuss this issue further in sec-
tion 5.1. How this is achieved is OS dependent for
some calls. For a general treatment of this issue, refer
to Viega [35].
• File system state tracking: When an agent accesses
a resource on a sandboxed process’s behalf, it must
adopt or emulate all relevant properties of the pro-
cess. Key properties for delegating ﬁle system oper-
ations are the current working directory, ﬁle creation
mask (umask), and effective identity (euid, egid, and
extended group membership). The agent must emulate
these properties of the sandboxed process to emulate
normal ﬁle system interface semantics.
For this reason, the agent handles chdir, umask, and
getcwd system calls, among others. These operations
are delegated simply by examining and updating data
structures within the agent that track the sandboxed
process’s state.
To ensure that ﬁle system requests are interpreted cor-
rectly, the agent assumes the relevant ﬁle system state
of its client before interpreting or fulﬁlling a request.
• Id management: To correctly perform accesses on the
process’s behalf, we need to know its user and group
identities. There is no reason to let a process manip-
ulate this state in the kernel, as it is no longer able to
access sensitive resources directly. Thus, we prefer to
run it completely without privilege and instead man-
age this state in the agent. To fool the process into
believing it is still running under the normal OS priv-
ilege model we delegate this interface, which includes
setuid, setgid, and getuid, to the agent which
emulates the OS model for modifying these permis-
sions. As with ﬁle system state tracking, these opera-
tions are delegated simply by examining and updating
agent data structures.
When the agent performs a call on the sandboxed pro-
cess’s behalf, it simply assumes the appropriate iden-
tity based on the emulated permissions. Thus, normal
OS access controls are enforced by the kernel. In spite
of concerns instilled by other work [9] that this might
be particularly error prone, we did not ﬁnd implement-
ing this to be difﬁcult or intricate. The code is rela-
tively clean and simple, and largely taken directly from
the Linux kernel.
• Signals: The sandboxed process cannot be permitted
to send signals directly.
Instead signals are sent by
delegating the responsibility for the kill call to the
agent, which only permits signals to be sent to other
processes in the sandbox and otherwise maintains nor-
mal signal semantics.
A client process can make system calls that do
not access sensitive system resources in the normal
fashion, e.g. queries for information not
typically