In order to accomplish its task, these scanners must know what exactly to search for on
a web server.  In most cases these tools are scanning web servers looking for
vulnerable files or directories that may contain sample code or vulnerable files. Either
way, the tools generally store these vulnerabilities in a file that is formatted like this:
/cgi-bin/cgiemail/uargg.txt
/random_banner/index.cgi
/random_banner/index.cgi
/cgi-bin/mailview.cgi
/cgi-bin/maillist.cgi
/cgi-bin/userreg.cgi
/iissamples/ISSamples/SQLQHit.asp
/iissamples/ISSamples/SQLQHit.asp
/SiteServer/admin/findvserver.asp
/scripts/cphost.dll
/cgi-bin/finger.cgi
How this technique can be used
The lines in a vulnerability file like the one shown above can serve as a roadmap for a
Google hacker. Each line can be broken down and used in either an ‘index.of’ or an
‘inurl’ search to find vulnerable targets. For example, a Google search for
‘allinurl:/random_banner/index.cgi’ returns the results shown in Figure 25.
Figure 25: Example search using a line from a CGI scanner
A hacker can take sites returned from this Google search, apply a bit of hacker ‘magic’
and eventually get the broken ‘random_banner’ program to cough up any file on that
web server, including the password file as shown in Figure 26.
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 30 -
Figure 26: password file captured from a vulnerable site found using a Google search
Of the many Google hacking techniques we’ve looked at, this technique is one of the
best candidates for automation since the CGI scanner vulnerability files can be very
large. The gooscan tool, written by j0hnny performs this and many other functions.
Gooscan and automation is discussed later.
About Google automated scanning
With so many potential search combinations available, it’s obvious that an automated
tool scanning for a known list of potentially dangerous pages would be extremely useful.
However, Google frowns on such automation as quoted at
 http://www.google.com/terms_of_service.html:
“You may not send automated queries of any sort to Google's system without
express  permission in advance from Google. Note that "sending automated
queries" includes, among other things:
• 
using any software which sends queries to  Google to determine how a
website or webpage "ranks" on Google  for various queries;
• 
"meta-searching" Google; and
• 
performing "offline" searches on Google.”
Google does offer alternatives to this policy in the form of the Google Web API’s found at
http://www.google.com/apis/. There are several major drawbacks to the Google API
program at the time of this writing. First, users and developers of Google API programs
must both have Google license keys. This puts a damper on the potential user base of
Google API programs. Secondly, API-created programs are limited to 1,000 queries per
day since “The Google Web APIs service is an experimental free program, so the
resources available to support the program are limited.” (according to the API FAQ found
at http://www.google.com/apis/api_faq.html#gen12.) With so many potential searches,
1000 queries is simply not enough.
The bottom line is that any user running an automated Google querying tool (with the
exception of API created tools) must obtain express permission in advance to do so. It is
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 31 -
unknown what the consequences of ignoring these terms of service are, but it seems
best to stay on Google’s good side.
The only exception to this rule appears to be the Google search appliance (described
below). The Google search appliance does not have the same automated query
restrictions since the end user, not Google, owns the appliance. One should, however,
obtain advance express permission from the owner or maintainer of the Google
appliance before searching it with any automated tool for various legal and moral
reasons.
Other Google stuff
Google Appliances
The Google search appliance is described at http://www.google.com/appliance/:
“Now the same reliable results you expect from Google web search can be yours
on your corporate website with the Google Search Appliance. This combined
hardware and software solution is easy to use, simple to deploy, and can be up
and running on your intranet and public website in just a few short hours.”
The Google appliance can best be described as a locally controlled and operated mini-
Google search engines for individuals and corporations. When querying a Google
appliance, often times the queries listed above in the “URL Syntax” section will not work.
Extra parameters are often required to perform a manual appliance query. Consider
running a search for "Steve Hansen" at the Google appliance found at Stanford. After
entering this search into the Stanford search page, the user is whisked away to a page
with this URL (chopped for readability):
http://find.stanford.edu/search?q=steve+hansen
&site=stanford&client=stanford&proxystylesheet=stanford
&output=xml_no_dtd&as_dt=i&as_sitesearch=
Breaking this up into chunks reveals three distinct pieces. First, the target appliance is
find.stanford.edu. Next, the query is "steve hansen" or "steve+hansen" and
last but not least are all the extra parameters:
&site=stanford&client=stanford&proxystylesheet=stanford
&output=xml_no_dtd&as_dt=i&as_sitesearch=
These parameters may differ from appliance to appliance, but it has become clear that
there are several default parameters that are required from a default installation of the
Google appliance like the one found at find.stanford.edu.
Googledorks
The term “googledork” was coined by Johnny Long (http://johnny.ihackstuff.com) and
originally meant “An inept or foolish person as revealed by Google.” After a great deal of
media attention, the term came to describe those “who troll the Internet for confidential
goods.” Either term is fine, really. What matters is that the term googledork conveys the
concept that sensitive stuff is on the web, and Google can help you find it. The official
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 32 -
googledorks page (found at http://johnny.ihackstuff.com/googledorks) lists many different
examples of unbelievable things that have been dug up through Google by the
maintainer of the page, Johnny Long. Each listing shows the Google search required to
find the information along with a description of why the data found on each page is so
interesting.
Gooscan
Gooscan (http://johnny.ihackstuff.com) is a UNIX (Linux/BSD/Mac OS X) tool that
automates queries against Google search appliances, but with a twist. These particular
queries are designed to find potential vulnerabilities on web pages. Think "cgi scanner"
that never communicates directly with the target web server, since all queries are sent to
Google, not to the target. For the security professional, gooscan serves as a front-end
for an external server assessment and aids in the "information gathering" phase of a
vulnerability assessment. For the web server administrator, gooscan helps discover what
the web community may already know about a site thanks to Google.
Gooscan was not written using the Google API. This raises questions about the “legality”
of using gooscan as a Google scanner. Is gooscan “legal” to use? You should not use
this tool to query Google without advance express permission. Google appliances,
however, do not have these limitations. You should, however, obtain advance express
permission from the owner or maintainer of the Google appliance before searching it
with any automated tool for various legal and moral reasons. Only use this tool to
query appliances unless you are prepared to face the (as yet unquantified) wrath
of Google.
Although there are many features, the gooscan tool’s primary purpose is to scan Google
(as long as you obtain advance express permission from Google) or Google appliances
(as long as you have advance express permission from the owner/maintainer) for the
items listed on the googledorks page. In addition, the tool allows for a very thorough CGI
scan of a site through Google (as long as you obtain advance express permission from
Google) or a Google appliance (as long as you have advance express permission from
the owner/maintainer of the appliance). Have I made myself clear about how this tool is
intended to be used? Get permission! =) Once you have received the proper advance
express permission, gooscan makes it easy to measure the Google exposure of yourself
or your clients.
GooPot
The concept of a honeypot is very straightforward. According to techtarget.com:
“A honey pot is a computer system on the Internet that is expressly set up to
attract and ‘trap’ people who attempt to penetrate other people's computer
systems.”
In order to learn about how new attacks might be conducted, the maintainers of a
honeypot system monitor, dissect and catalog each attack, focusing on those attacks
which seem unique.
An extension of the classic honeypot system, a web-based honeypot or “pagepot” is
designed to attract those employing the techniques outlined in this paper. The concept is
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 33 -
fairly straightforward. A simple googledork entry like “ i n u r l : a d m i n
inurl:userlist” could easily be replicated with a web-based honeypot by creating
an index.html page which referenced another index.html file in an /admin/userlist
directory. If a web search engine like Google was instructed to crawl the top-level
index.html page, if would eventually find the link pointing to /admin/userlist/index.html.
This link would satisfy the Google query of “inurl:admin inurl:userlist”
eventually attracting a curious Google searcher.
Once the Google searcher clicks on the Google, he is whisked away to the target web
page. In the background, the user’s web browser also sends many variables to that web
server, including one variable of interest, the “referrer” variable. This field contains the
complete name of the web page that was visited previously, or more clearly, the web site
that referred the user to the web page. The bottom line is that this variable can be
inspected to figure out how a web surfer found a web page assuming they clicked on
that link from a search engine page. This bit of information is critical to the maintainer of
a pagepot system, since it outlines the exact method the Google searcher used to locate
the pagepot system. The information aids in protecting other web sites from similar
queries.
The concept of a pagepot is not a new one thanks to many folks including the group at
http://www.gray-world.net/. Their web-based honeypot, hosted at http://www.gray-
world.net/etc/passwd/ is designed to entice those using Google like a CGI scanner. This
is not a bad concept, but as we’ve seen in this paper, there are so many other ways to
use Google to find vulnerable or sensitive pages.
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 34 -
Enter GooPot, the Google honeypot system designed by johnny@ihackstuff.com. By
populating a web server with sensitive-looking documents and monitoring the referrer
variables passed to the server, a GooPot administrator can learn about new web search
techniques being employed in the wild and subsequently protect his site from similar
queries. Beyond a simple pagepot, GooPot uses enticements based on the many
techniques outlined in the googledorks collection and this document. In addition, the
GooPot more closely resembles the juicy targets that Google hackers typically go after.
Johnny Long, the administrator of the googledorks list, utilizes the GooPot to discover
new search types and publicize them in the form of googledorks listings, creating a self-
sustaining cycle for learning about, and protecting from search engine attacks.
The GooPot system is not publicly available.
Google Sets
When searching for interested data via Google, most Google hackers eventually run out
of ideas when looking for targets. Enter Google Sets (http://labs.google.com/sets).
Google sets automatically creates lists of items when a user enters a few examples. The
results are based on all the data the Google has crawled over the years.
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 35 -
As a simple example, if a user were to enter “Ford” and “Lincoln”, Google sets would
return “Ford, Lincoln, Mercury, Dodge, Chrysler, Jaguar, CADILLAC, Chevrolet, Jeep,
Plymouth, Oldsmobile, Pontiac, Mazda, Honda, and Saturn” and would offer to expand
the list to show even more items.
If, however, the user were to enter “password” Google would show the terms “Password,
User, Name, username, Host, name, Login, Remote, Directory, shell, directories, User,
ID, userid, Email, Address, Register, name, Email, Login, FTP and HOSTNAME”
Using Google Sets not only helps the Google hacker come up with new ways to search
for sensitive data, it gives a bit of a preview of how the Google results will lean if you
include certain words in a search. This type of exercise can prove to be less time
consuming than sifting through pages of Google results trying to locate trends.
A word about how Google finds pages (Opera)
Although the concept of web crawling is fairly straightforward, Google has created other
methods for learning about new web pages. Most notably, Google has incorporated a
feature into the latest release of the Opera web browser. When an Opera user types a
URL into the address bar, the URL is sent to Google, and is subsequently crawled by
Google’s bots. According to the FAQ posted at http://www.opera.com/adsupport:
“The Google system serves advertisements and related searches to the Opera
browser through the Opera browser banner 468x60 format. Google determines
what ads and related searches are relevant based on the URL and content of the
page you are viewing and your IP address, which are sent to Google via the
Opera browser.”
As of the time of this writing it is unclear as to whether or not Google includes the link
into it’s search engine. However, testing shows that when an unindexed URL
(http://johnny.ihackstuff.com/temp/suck.html) was entered into Opera 7.2.3, a Googlebot
crawled the URL moments later as shown by the following access.log excerpts:
64.68.87.41 - "GET /robots.txt HTTP/1.0" 200 220 "-" "Mediapartners-
Google/2.1 (+http://www.googlebot.com/bot.html)"
64.68.87.41 - "GET /temp/suck.html HTTP/1.0" 200 5 "-" "Mediapartners-
Google/2.1 (+http://www.googlebot.com/bot.html)"
The privacy implications of this could be staggering, especially if you Opera users expect
visited URLs to remain private.
This feature can be turned off within Opera by selecting “Show generic selection of
graphical ads” from the “File -> Preferences -> Advertising” screen.
Protecting yourself from Google hackers
1. Keep your sensitive data off the web!
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 36 -
Even if you think you’re only putting your data on a web site temporarily, there’s a
good chance that you’ll either forget about it, or that a web crawler might find it.
Consider more secure ways of sharing sensitive data such as SSH/SCP or
encrypted email.
2. Googledork!
• 
Use the techniques outlined in this paper to check your own site for
sensitive information or vulnerable files.
• 
Use gooscan from http://johnny.ihackstuff.com) to scan your site for bad
stuff, but first get advance express permission from Google! Without
advance express permission, Google could come after you for violating
their terms of service. The author is currently not aware of the exact
implications of such a violation. But why anger the “Goo-Gods”?!?
• 
Check the official googledorks website (http://johnny.ihackstuff.com) on a
regular basis to keep up on the latest tricks and techniques.
3. Consider removing your site from Google’s index.
The Google webmaster FAQ located at http://www.google.com/webmasters/
provides invaluable information about ways to properly protect and/or expose
your site to Google. From that page:
“Please have the webmaster for the page in question contact us with proof that
he/she is indeed the webmaster. This proof must be in the form of a root level
page on the site in question, requesting removal from Google. Once we receive
the URL that corresponds with this root level page, we will remove the offending
page from our index.”
In some cases, you may want to rome individual pages or snippets from Google’s
index. This is also a straightforward process which can be accomplished by
following the steps outlined at http://www.google.com/remove.html.
4. Use a robots.txt file.
Web crawlers are supposed to follow the robots exclusion standard found at
http://www.robotstxt.org/wc/norobots.html. This standard outlines the procedure
for “politely requesting” that web crawlers ignore all or part of your website. I
must note that hackers may not have any such scruples, as this file is certainly a
suggestion. The major search engine’s crawlers honor this file and it’s contents.
For examples and suggestions for using a robots.txt file, see the above URL on
robotstxt.org.
Thanks and shouts
First, I would like to thank God for the taking the time to pierce my way-logical mind with
the unfathomable gifts of sight by faith and eternal life through the sacrifice of Jesus
Christ.
The Google Hacker’s Guide
PI:EMAIL
http://johnny.ihackstuff.com
- Page 37 -
Thanks to my family for putting up with the analog version of j0hnny.
Shouts to the STRIKEFORCE, “Gotta_Getta_Hotdog” Murray, “Re-Ron” Shaffer, “2 cute
to B single” K4yDub, “Nice BOOOOOSH” Arnold, “Skull Thicker than a Train Track”
Chapple, “Bitter Bagginz” Carter, Fosta’ (student=teacher;), Tiger “Lost my badge”
Woods, LARA “Shake n Bake” Croft, “BananaJack3t” Meyett, Patr1ckhacks, Czup, Mike
“Scan Master, Scan Faster” Walker, “Mr. I Love JAVA” Webster, “Soul Sistah” G Collins,
Chris, Carey, Matt, KLOWE, haywood, micah, Shouts to those who have passed on:
Chris, Ross, Sanguis, Chuck, Troy, Brad.
Shouts to Joe “BinPoPo”, Steve Williams (by far the most worthy defender I’ve had the
privilege of knowing) and to “Bigger is Better” Fr|tz.
Thanks to my website members for the (admittedly thin) stream of feedback and
Googledork additions. Maybe this document will spur more submissions.
Thanks to JeiAr at GulfTech Security , Cesar 
of Appdetective fame, and Mike “Supervillain” Carter for the outstanding contributions to
the googledorks database.
Thanks to Chris O'Ferrell (www.netsec.net), Yuki over at the Washington Post, Slashdot,
and TheRegister.co.uk for all the media coverage. While I’m thanking my referrers, I
should mention Scott Granneman for the front-page SecurityFocus article that was all
about Googledorking. He was nice enough to link me and call Googledorks his “favorite
site” for Google hacking even though he didn’t mention me by name or return any of my
emails. I’m not bitter though… it sure generated a lot of traffic! After all the good press,
it’s wonderful to be able to send out a big =PpPPpP to NewScientist Magazine for their
particularly crappy coverage of this topic. Just imagine, all this traffic could have been
yours if you had handled the story properly.
Shouts out to Seth Fogie, Anton Rager, Dan Kaminsky, rfp, Mike Schiffman, Dominique
Brezinski, Tan, Todd, Christopher (and the whole packetstorm crew), Bruce Potter,
Dragorn, and Muts (mutsonline, whitehat.co.il) and my long lost friend Topher.
Hello’s out to my good friends SNShields and Nathan.
When in Vegas, be sure to visit any of the world-class properties of the MGM/Mirage or
visit them online at http://mgmmirage.com. =)