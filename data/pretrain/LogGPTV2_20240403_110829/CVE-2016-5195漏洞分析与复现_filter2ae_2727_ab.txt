    //
    long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
            unsigned long start, unsigned long nr_pages,
            unsigned int gup_flags, struct page **pages,
            struct vm_area_struct **vmas, int *nonblocking)
    {
        long i = 0;
        unsigned int page_mask;
        struct vm_area_struct *vma = NULL;
        if (!nr_pages)
            return 0;
        VM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));
        /*
         * If FOLL_FORCE is set then do not force a full fault as the hinting
         * fault information is unrelated to the reference behaviour of a task
         * using the address space
         */
        if (!(gup_flags & FOLL_FORCE))
            gup_flags |= FOLL_NUMA;
        do {
            struct page *page;
            unsigned int foll_flags = gup_flags;
            unsigned int page_increm;
            /* first iteration or cross vma bound */
            if (!vma || start >= vma->vm_end) {
                vma = find_extend_vma(mm, start);
                if (!vma && in_gate_area(mm, start)) {
                    int ret;
                                    ret = get_gate_page(mm, start & PAGE_MASK,
                            gup_flags, &vma,
                            pages ? &pages[i] : NULL);
                    if (ret)
                        return i ? : ret;
                    page_mask = 0;
                    goto next_page;
                }
                if (!vma || check_vma_flags(vma, gup_flags))
                    return i ? : -EFAULT;
                if (is_vm_hugetlb_page(vma)) {
                    i = follow_hugetlb_page(mm, vma, pages, vmas,
                            &start, &nr_pages, i,
                            gup_flags);
                    continue;
                }
            }
    retry:
            /*
             * If we have a pending SIGKILL, don't keep faulting pages and
             * potentially allocating memory.
             */
            if (unlikely(fatal_signal_pending(current)))
                return i ? i : -ERESTARTSYS;
            cond_resched();
            page = follow_page_mask(vma, start, foll_flags, &page_mask);//获取页描述符
            if (!page) {
                int ret;
                ret = faultin_page(tsk, vma, start, &foll_flags,
                        nonblocking);//缺页处理
                switch (ret) {
                            case 0:
                        goto retry;//获取失败就重试继续获取页表项
                case -EFAULT:
                case -ENOMEM:
                case -EHWPOISON:
                    return i ? i : ret;
                case -EBUSY:
                    return i;
                case -ENOENT:
                    goto next_page;
                }
                BUG();
            } else if (PTR_ERR(page) == -EEXIST) {
                /*
                 * Proper page table entry exists, but no corresponding
                 * struct page.
                 */
                goto next_page;
            } else if (IS_ERR(page)) {
                return i ? i : PTR_ERR(page);
            }
            if (pages) {
                pages[i] = page;
                flush_anon_page(vma, page, start);
                flush_dcache_page(page);
                page_mask = 0;
            }
    next_page:
            if (vmas) {
                            vmas[i] = vma;
                page_mask = 0;
            }
            page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
            if (page_increm > nr_pages)
                page_increm = nr_pages;
            i += page_increm;
            start += page_increm * PAGE_SIZE;
            nr_pages -= page_increm;
        } while (nr_pages);
        return i;
    }
    //
    struct page *follow_page_mask(struct vm_area_struct *vma,
                      unsigned long address, unsigned int flags,
                      unsigned int *page_mask)
    {
        pgd_t *pgd;//页全局目录
        pud_t *pud;//页上级目录
        pmd_t *pmd;//页中间目录
        spinlock_t *ptl;//页表
        struct page *page;//一个页表项
        struct mm_struct *mm = vma->vm_mm;//进程的内存描述符，被赋值为线性区的内存描述符
        *page_mask = 0;
        page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
        if (!IS_ERR(page)) {
            BUG_ON(flags & FOLL_GET);
            return page;
        }
        pgd = pgd_offset(mm, address);
        if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
            return no_page_table(vma, flags);
        pud = pud_offset(pgd, address);
        if (pud_none(*pud))
            return no_page_table(vma, flags);
        if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
            page = follow_huge_pud(mm, address, pud, flags);
            if (page)
                return page;
            return no_page_table(vma, flags);
        }
            if (unlikely(pud_bad(*pud)))
            return no_page_table(vma, flags);
        pmd = pmd_offset(pud, address);
        if (pmd_none(*pmd))
            return no_page_table(vma, flags);
        if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
            page = follow_huge_pmd(mm, address, pmd, flags);
            if (page)
                return page;
            return no_page_table(vma, flags);
        }
        if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
            return no_page_table(vma, flags);
        if (pmd_trans_huge(*pmd)) {
            if (flags & FOLL_SPLIT) {
                split_huge_page_pmd(vma, address, pmd);
                return follow_page_pte(vma, address, pmd, flags);
            }
            ptl = pmd_lock(mm, pmd);
            if (likely(pmd_trans_huge(*pmd))) {
                if (unlikely(pmd_trans_splitting(*pmd))) {
                    spin_unlock(ptl);
                    wait_split_huge_page(vma->anon_vma, pmd);
                } else {
                    page = follow_trans_huge_pmd(vma, address,
                                     pmd, flags);
                    spin_unlock(ptl);
                    *page_mask = HPAGE_PMD_NR - 1;
                    return page;
                }
            } else
                spin_unlock(ptl);
            }
        return follow_page_pte(vma, address, pmd, flags);//到页表去寻找页表项
    }
    //寻找页表项
    static struct page *follow_page_pte(struct vm_area_struct *vma,
            unsigned long address, pmd_t *pmd, unsigned int flags)
    {
        struct mm_struct *mm = vma->vm_mm;
        struct page *page;
        spinlock_t *ptl;
        pte_t *ptep, pte;
    retry:
        if (unlikely(pmd_bad(*pmd)))
            return no_page_table(vma, flags);
        ptep = pte_offset_map_lock(mm, pmd, address, &ptl);
        pte = *ptep;
        if (!pte_present(pte)) {
            swp_entry_t entry;
            /*
             * KSM's break_ksm() relies upon recognizing a ksm page
             * even while it is being migrated, so for that case we
             * need migration_entry_wait().
             */
            if (likely(!(flags & FOLL_MIGRATION)))
                goto no_page;
            if (pte_none(pte))
                goto no_page;
            entry = pte_to_swp_entry(pte);
            if (!is_migration_entry(entry))
                goto no_page;
            pte_unmap_unlock(ptep, ptl);
            migration_entry_wait(mm, pmd, address);
            goto retry;
        }
            if ((flags & FOLL_NUMA) && pte_protnone(pte))
            goto no_page;
        if ((flags & FOLL_WRITE) && !pte_write(pte)) {
            pte_unmap_unlock(ptep, ptl);//如果我们寻求的是可写的页而找到的并无可写权限，则直接返回NULL
            return NULL;
        }
        page = vm_normal_page(vma, address, pte);
        if (unlikely(!page)) {
            if (flags & FOLL_DUMP) {
                /* Avoid special (like zero) pages in core dumps */
                page = ERR_PTR(-EFAULT);
                goto out;
            }
            if (is_zero_pfn(pte_pfn(pte))) {
                page = pte_page(pte);
            } else {
                int ret;
                ret = follow_pfn_pte(vma, address, ptep, flags);
                page = ERR_PTR(ret);
                goto out;
            }
        }
            if (flags & FOLL_GET)
            get_page_foll(page);
        if (flags & FOLL_TOUCH) {
            if ((flags & FOLL_WRITE) &&
                !pte_dirty(pte) && !PageDirty(page))
                set_page_dirty(page);
            /*
             * pte_mkyoung() would be more correct here, but atomic care
             * is needed to avoid losing the dirty bit: it is easier to use
             * mark_page_accessed().
             */
            mark_page_accessed(page);
        }
        if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
            /*
             * The preliminary mapping check is mainly to avoid the
             * pointless overhead of lock_page on the ZERO_PAGE
             * which might bounce very badly if there is contention.
             *
             * If the page is already locked, we don't need to
             * handle it now - vmscan will handle it later if and
             * when it attempts to reclaim the page.
             */
            if (page->mapping && trylock_page(page)) {
                lru_add_drain();  /* push cached pages to LRU */
                /*
                 * Because we lock page here, and migration is
                 * blocked by the pte's page reference, and we
                 * know the page is still mapped, we don't even
                 * need to check for file-cache page truncation.
                 */
                mlock_vma_page(page);
                unlock_page(page);
            }
        }
    out:
        pte_unmap_unlock(ptep, ptl);
        return page;//如果我们并不要求可写页或者页本身可写那么直接返回page
    no_page:
        pte_unmap_unlock(ptep, ptl);
        if (!pte_none(pte))
            return NULL;
        return no_page_table(vma, flags);
    }
    //
    static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
            unsigned long address, unsigned int *flags, int *nonblocking)
    {
        unsigned int fault_flags = 0;
        int ret;
        /* mlock all present pages, but do not fault in new pages */
        if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
            return -ENOENT;
        /* For mm_populate(), just skip the stack guard page. */
        if ((*flags & FOLL_POPULATE) &&
                (stack_guard_page_start(vma, address) ||
                 stack_guard_page_end(vma, address + PAGE_SIZE)))
            return -ENOENT;
        if (*flags & FOLL_WRITE)
            fault_flags |= FAULT_FLAG_WRITE;
        if (*flags & FOLL_REMOTE)
            fault_flags |= FAULT_FLAG_REMOTE;
        if (nonblocking)
            fault_flags |= FAULT_FLAG_ALLOW_RETRY;
        if (*flags & FOLL_NOWAIT)
            fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
        if (*flags & FOLL_TRIED) {
            VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
            fault_flags |= FAULT_FLAG_TRIED;
        }
        ret = handle_mm_fault(vma, address, fault_flags);//处理缺页的函数
            if (ret & VM_FAULT_ERROR) {
            if (ret & VM_FAULT_OOM)
                return -ENOMEM;
            if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
                return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;