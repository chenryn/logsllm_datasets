This assumption does not always hold in practice and the detection algorithm
has to consider any possible scale transformation between the input and the
output pattern. We discuss this and other potential transformations in more
detail in Sec. 3.4.
3.4 Detector
The success of our detection algorithm lies in the ability to infer a cause-eﬀect
relationship between the keystroke stream injected in the system and the I/O
behavior of a keylogger process, or, more speciﬁcally, between the respective
patterns in AKP form. While one must examine every candidate process in the
system, the detection algorithm operates on a single process at a time, iden-
tifying whether there is a strong similarity between the input pattern and the
output pattern obtained from the analysis of the I/O behavior of the target pro-
cess. Speciﬁcally, given a predeﬁned input pattern and an output pattern of a
particular process, the goal of the detection algorithm is to determine whether
there is a match in the patterns and the target process can be identiﬁed as a
keylogger with good probability.
The ﬁrst step in the construction of a detection algorithm comes down to
the adoption of a suitable metric to measure the similarity between two given
patterns. In principle, the AKP representation allows for several possible mea-
sures of dependence that compare two discrete sequences and quantify their
relationship. In practice, we rely on a single correlation measure motivated by
the properties of the two patterns. The proposed detection algorithm is based
on the Pearson product-moment correlation coeﬃcient (PCC), the ﬁrst formally
deﬁned correlation measure and still one of the most widely used [18]. Given two
discrete sequences described by two patterns P and Q with N samples, the PCC
is deﬁned as [18]:
r =
cov(P, Q)
σp · σq
=
(cid:4)(cid:3)N
i=1(Pi − P )(Qi − Q)
(cid:3)N
i=1(Pi − P )2
(cid:4)(cid:3)N
i=1(Qi − Q)2
(1)
where cov(P, Q) is the sample covariance, σp and σq are sample standard devi-
ations, and P and Q are sample means. The PCC has been widely used as an
index to measure bivariate association for diﬀerent distributions in several ap-
plications including pattern recognition, data analysis, and signal processing [5].
The values given by the PCC are always symmetric and ranging between −1
and 1, with 0 indicating no correlation and 1 or −1 indicating complete direct
(or inverse) correlation. To measure the degree of association between two given
204
S. Ortolani, C. Giuﬀrida, and B. Crispo
patterns we are here only interested in positive values of correlation. Hereafter,
we will always refer to its absolute value.
Our interest in the PCC lies in its appealing mathematical properties. In con-
trast to many other correlation metrics, the PCC measures the strength of a
linear relationship between two series of samples, ignoring any non-linear as-
sociation. In the context of our detection algorithm, a linear dependence well
approximates the relationship between the input pattern and an output pattern
produced by a keylogger. The basic intuition is that a keylogger can only make
local decisions on a per-keystroke basis with no knowledge about the global dis-
tribution. Thus, in principle, whatever the decisions, the resulting behavior will
linearly approximate the original input stream injected into the system.
In detail, the PCC is resilient to any change in location and scale, namely no
diﬀerence can be observed in the correlation coeﬃcient if every sample Pi of any
of the two patterns is transformed into a·Pi + b, where a and b are arbitrary con-
stants. This is important for a number of reasons. Ideally, the input pattern and
an output pattern will be an exact copy of each other if every keystroke injected
is replicated as it is in the output of a keylogger process. In practice, diﬀerent
data transformations performed by the keylogger can alter the original structure
in several ways. First, a keylogger may encode each keystroke in a sequence of one
or more bytes. Consider, for example, a keylogger encoding each keystroke using
8-bit ASCII codes. The output pattern will be generated examining a stream of
raw bytes produced by the keylogger as it stores keystrokes one byte at a time.
Now consider the exact same case but with keystrokes stored using a diﬀerent
encoding, e.g. 2 bytes per keystroke. In the latter case, the pattern will have the
same shape as the former one, but its scale will be twice as much. Fortunately, as
explained earlier, the transformation in scale will not aﬀect the correlation coef-
ﬁcient and the PCC will report the same value in both cases. Similar arguments
are valid for keyloggers using a variable-length representation to store keystrokes.
This scenario occurs, for instance, when a keylogger uses special byte sequences
to encode particular classes of keystrokes or encrypts keystrokes with a variable
number of bytes. Even under these circumstances, the resulting data transfor-
mation can still be approximated as linear. The scale invariance property makes
also the approach robust to keyloggers that drop a limited number of keystrokes
while logging. For example, many keyloggers refuse to record keystrokes that do
not directly translate into alphanumeric characters. In this case, under the as-
sumption that keystrokes in the input stream are uniformly distributed by type,
the resulting output pattern will only contain each generated keystroke with a
certain probability p. This can be again approximated as rescaling the original
pattern by p, with no signiﬁcant eﬀect on the original value of the PCC.
An interesting application of the location invariance property is the ability
to mitigate the eﬀect of buﬀering. When the keylogger uses a ﬁxed-size buﬀer
whose size is comparable to the number of keystrokes injected at each time in-
terval, it is easy to show that the PCC is not signiﬁcantly aﬀected. Consider,
for example, the case when the buﬀer size is smaller than the minimum number
of keystrokes Kmin. Under this assumption, the buﬀer is completely ﬂushed out
Bait Your Hook: A Novel Detection Technique for Keyloggers
205
at least once per time interval. The number of keystrokes left in the buﬀer at
each time interval determines the number of keystrokes missing in the output
pattern. Depending on the distribution of samples in the input pattern, this
number would be centered around a particular value z. The statistical meaning
of the value z is the average number of keystrokes dropped per time interval.
This transformation can be again approximated by a location transformation of
the original pattern by a factor of −z, which again does not aﬀect the value of
the PCC. The last example shows the importance of choosing an appropriate
Kmin when the eﬀect of ﬁxed-size buﬀers must also be taken into account. As
evident from the examples discussed, the PCC is robust when not completely
resilient to several possible data transformations. Nevertheless, there are other
known fundamental factors that may aﬀect the size of the PCC and could pos-
sibly complicate the interpretation of the results. A taxonomy of these factors is
proposed and thoroughly discussed in [8]. We will brieﬂy discuss some of these
factors here to analyze how they aﬀect our design. This is crucial to avoid com-
mon pitfalls and unexpectedly low correlation values that underestimate the true
relationship between two patterns possibly generating false negatives.
A ﬁrst important factor to consider is the possible lack of linearity. Although
the several cases presented only involve linear or pseudo-linear transformations,
non-linearity might still aﬀect our detection system in the extreme case of a
keylogger performing aggressive buﬀering. A representative example in this cat-
egory is a keylogger ﬂushing out to disk an indeﬁnite-size buﬀer at regular time
intervals. While we experimented this circumstance to rarely occur in practice,
we have also adopted standard strategies to deal with this scenario eﬀectively.
In our design, we exploit the observation that the non-linear behavior is known
in advance and can be modeled with good approximation.
Following the solution suggested in [8], we transform both patterns to elimi-
nate the source of non-linearity before computing the PCC. To this end, assum-
ing a suﬃciently large number of samples N is available, we examine peaks in
the output pattern and eliminate non-informative samples when we expect to
see the eﬀect of buﬀering in action. At the same time, we aggregate the corre-
sponding samples in the input pattern accordingly and gain back the ability to
perform a signiﬁcative linear analysis using the PCC over the two normalized
patterns. The advantage of this approach is that it makes the resulting value
of the PCC practically resilient to buﬀering. The only potential shortcoming is
that we may have to use larger windows of observation to collect a suﬃcient
number of samples N for our analysis.
Another fundamental factor to consider is the number of samples collected.
While we would like to shorten the duration of the detection algorithm as much
as possible, there is a clear tension between the length of the patterns examined
and the reliability of the resulting value of the PCC. A very small number of
samples can lead to unstable or inaccurate results. A larger number of samples
is beneﬁcial especially whenever one or more other disturbing factors are to
be expected. As reported in [8], selecting a larger number of samples could,
for example, reduce the adverse eﬀect of outliers or measurement errors. The
206
S. Ortolani, C. Giuﬀrida, and B. Crispo
detection algorithm we have implemented in our detector, relies entirely on the
PCC to estimate the correlation between an input and an output pattern. To
determine whether a given PCC value should trigger a detection, a thresholding
mechanism is used. We discuss how to select a suitable threshold empirically in
Sec. 4. Our detection algorithm is conceived to infer a causal relationship between
two patterns by analyzing their correlation. Admittedly, experience shows that
correlation cannot be used to imply causation in the general case, unless valid
assumptions are made on the context under investigation [2]. In other words, to
avoid false positives in our detection strategy, strong evidence shall be collected
to infer with good probability that a given process is a keylogger. The next
section discusses in detail how to select a robust input pattern and minimize the
probability of false detections.
3.5 Pattern Generator
Our pattern generator is designed to support several possible pattern generation
algorithms. More speciﬁcally, the pattern generator can leverage any algorithm
producing a valid input pattern in AKP form. In this section, we present a
number of pattern generation algorithms and discuss their properties.
First important issue to consider is the eﬀect of variability in the input pat-
tern. Experience shows that correlations tend to be stronger when samples are
distributed over a wider range of values [8]. In other words, the more the variabil-
ity in the given distributions, the more stable and accurate the resulting PCC
computed. This suggests that a robust input pattern should contain samples
spanning the entire target interval [0, 1]. The level of variability in the resulting
input stream is also similarly inﬂuenced by the range of keystroke rates used in
the pattern translation process. The higher the range delimited by the minimum
keystroke rate and maximum keystroke rate, the more reliable the results.
The adverse eﬀect of low variability in the input pattern can be best under-
stood when analyzing the mathematical properties of the PCC. The correlation
coeﬃcient reports high values of correlation when the two patterns tend to grow
apart from their respective means on the same side with proportional intensity.
As a consequence, the more closely to their respective means the patterns are
distributed, the less stable and accurate the resulting PCC.
In the extreme case of no variability, that is when a constant distribution is
considered, the standard deviation is 0 and the PCC is not even deﬁned. This
suggests that a robust pattern generation algorithm should never consider con-
stant or low-variability patterns. Moreover, when a constant pattern is generated
from the output stream, our detection algorithm assigns an arbitrary correlation
score of 0. This is still coherent under the assumption that the selected input
pattern presents a reasonable level of variability, and poor correlation should
naturally be expected when comparing with other low-variability patterns. A
robust pattern generation algorithm should allow for a minimum number of
false positives and false negatives at detection time. As far as false negatives are
Bait Your Hook: A Novel Detection Technique for Keyloggers
207
concerned, we have already discussed some of the factors that aﬀect the PCC
and may increase the number of false detections in Sec. 3.4.
About false positives, when the chosen input pattern happens to closely re-
semble the I/O behavior of some benign process in the system, the PCC may
report a high value of correlation for that process and trigger a false detection.
For this reason, it is important to focus on input patterns that have little chances
of being confused with output patterns generated by regular system processes.
Fortunately, studies show that the correlation between diﬀerent realistic I/O
workloads for PC users is generally considerably low diﬀerent I/O workloads for
PC users is generally considerably low over small time intervals [11]. The results
presented in [11] are derived from 14 traces collected over a number of months
in realistic environments used by diﬀerent categories of users. The authors show
that the value of correlation given by the PCC over 1 minute of I/O activity is
only 0.0462 on average and never exceeds 0.0708 for any two given traces. These
results suggest that the I/O behavior of one or more given processes is in general
very poorly correlated with other diﬀerent I/O distributions.
Another property of interest concerning the characteristics of common I/O
workloads is self-similarity. Experience shows that the I/O traﬃc is typically
self-similar, namely that its distribution and variability are relatively insensitive
to the size of the sampling interval [11]. For our analysis, this suggests that vari-
ations in the time interval T will not heavily aﬀect the sample distribution in the
output pattern and thereby the values of the resulting PCC. This scale-invariant
property is crucial to allow for changes in the parameter T with no considerable
variations in the number of potential false positives generated at detection time.
While most pattern generation algorithms with the properties discussed so far
should produce a relatively small number of false positives in common usage sce-
narios, we are also interested in investigating pattern generation algorithms that
attempt to minimize the number of false positives for a given target workload.
The problem of designing a pattern generation algorithm that minimizes the
number of false positives under a given known workload can be modeled as
follows. We assume that traces for the target workload can be collected and
converted into a series of patterns (one for each process running on the system) of
the same length N. All the patterns are generated to build a valid training set for
the algorithm. Under the assumption that the traces collected are representative
of the real workload available at detection time, our goal is to design an algorithm
that learns the characteristics of the training data and generates a maximally
uncorrelated input pattern. Concretely, the goal of our algorithm is to produce an
input pattern of length N that minimizes the average PCC measured against all
the patterns in the training set. Without any further constraints on the samples
of the target input pattern, it can be shown that this problem is a non-trivial
non-linear optimization problem. In practice, we can relax the original problem
by leveraging some of the assumptions discussed earlier. As motivated before,
a robust input pattern should present samples distributed over a wide range of
values. To assume the widest range possible, we can arbitrarily constraint the
208
S. Ortolani, C. Giuﬀrida, and B. Crispo
series of samples to be uniformly distributed over the target interval [0, 1]. This
is equivalent to consider a set of N samples of the form:
(cid:6)
(cid:5)
S =
0,
, . . . ,
, 1
.
(2)
1
2
N − 1
,
N − 1
N − 2
N − 1
t
(cid:3)
When the N samples are constrained to assume all the values from the set S,
the optimization problem comes down to ﬁnding the particular permutation of
values that minimizes the average PCC. This problem is a variant of the standard
assignment problem for N objects and N tasks, where each particular pairwise
assignment yields a known cost and the ultimate goal is to minimize the sum of
all the costs involved [14].
−P t)
(Si−S)(Ptj
σs·σpt
In our scenario, the objects can be modeled by the samples in the target set
S and the tasks reﬂect the N slots in the input pattern each sample has to be
assigned to. In addition, the cost of assigning a sample Si from the set S to a
particular slot j is c(i, j) =
, where Pt are the patterns in the
training set, and S and σs are the constant mean and standard distribution of
the samples in S, respectively. The cost value c(i, j) reﬂects the value of a single
addendum in the resulting expression of the average PCC we want to minimize.
The formulation of the cost value has been simpliﬁed assuming constant number
of samples N and constant number of patterns in the training set. Unfortunately,
this problem cannot be easily addressed by leveraging well-known algorithms
that solve the linear assignment problem in polynomial time [14]. In contrast
to the standard formulation, we are not interested in the global minimum of
the sum of the cost values. Such an approach would indeed attempt to ﬁnd a
pattern that results in an average PCC maximally close to −1. In contrast, the
ultimate goal of our analysis is to produce a maximally uncorrelated pattern,
thereby aiming at an average PCC as close to 0 as possible. This problem can
be modeled as an assignment problem with side constraints.
Prior research has shown how to transform this particular problem into an
equivalent quadratic assignment problem (QAP) that can be very eﬃciently
solved with a standard QAP solver when the global minimum is known in ad-
vance [13]. In our solution, we have implemented a similar approach limiting
the approach to a maximum number of iterations to guarantee convergence in
bounded time since the minimum value of the PCC is not known in advance. In
practice, for a reasonable number of samples N and a modest training set, we
found that this is rarely a concern. The algorithm can usually identify the opti-
mal pattern in a bearable amount of time. To conclude, we now more formally
propose two classes of pattern generation algorithms for our generator. First, we
are interested in workload-aware generation algorithms. For this class, we focus
on the optimization algorithm we have just introduced, assuming a number of
representative traces have been made available for the target workload.
Moreover, we are interested in workload-agnostic pattern generation algo-
rithms. With no assumption made on the nature of the workload, they are more
Bait Your Hook: A Novel Detection Technique for Keyloggers
209
generic and easier to implement. In this class, we propose the following
algorithms:
Random (RND). Every sample is generated at random with no additional con-
straints. This is the simplest pattern generation algorithm.
Random with ﬁxed range (RFR). The pattern is a random permutation of a se-
ries of samples uniformly distributed over the interval [0, 1]. This algorithm at-
tempts to maximize the amount of variability in the input pattern.
Impulse (IMP). Every sample 2i is assigned the value of 0 and every sample 2i + 1
is assigned the value of 1. This algorithm attempts to produce an input pattern
with maximum variance while minimizing the duration of idle periods.
Sine Wave (SIN). The pattern generated is a discrete sine wave distribution oscil-
lating between 0 and 1 with the ﬁrst sample having the value of 1. The sine wave
grows or drops with a ﬁxed step of 0.1 at every sample. This algorithm explores
the eﬀect of constant increments (and decrements) in the input pattern.
4 Evaluation
To demonstrate the viability of our approach and evaluate the proposed detection
technique, we implemented a prototype system based on the ideas described in
the paper. Our prototype is entirely written in C# and runs as an unprivileged
application for the Windows operating system.
In the following, we present several experiments to evaluate our approach. The
ultimate goal is to understand the eﬀectiveness of our technique and whether it
can be used in realistic settings. We experimented our prototype with many pub-
licly available keyloggers. We have also developed our own keylogger to evaluate
the eﬀect of special features or particular conditions more throughly. Finally, we
have collected traces for diﬀerent realistic PC workloads to evaluate the strength
of our approach in real-life scenarios. We ran all of our experiments on PCs with
a 2.53 GHz Core 2 Duo processor, 4 GB memory, and 7200 rpm SATA II hard
drives. Every test was performed under Windows XP Professional SP3, while the
workload traces were gathered from a number of PCs running several diﬀerent
versions of Windows.
4.1 Keylogger Detection
To evaluate the ability to detect real-world keyloggers, we experimented all the
keyloggers from the top monitoring free software list [15], an online repository
continuously updated with reviews and latest developments in the area. At the
moment of writing, eight keyloggers were listed in the free software list. To
carry out the experiments, we manually installed each keylogger, launched our
detection system for N · T ms, and recorded the results.
In the experiments, we used arbitrary settings for the threshold and the pa-
rameters N, T , Kmin, Kmax. The reason is that we observed the same results
for several reasonable combinations of parameters in most cases. We have also
solely selected the RFR algorithm as the pattern generation algorithm for the
experiments. More details on how to select a pattern generation algorithm and
210