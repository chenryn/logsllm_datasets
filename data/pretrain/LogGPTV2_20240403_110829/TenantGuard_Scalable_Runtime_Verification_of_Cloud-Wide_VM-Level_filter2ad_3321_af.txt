(c)
Fig. 9. Performance Comparison by Varying the # of (a) VMs per Subnet, (b) Routing Rules, and (c) Hops, while Fixing the # of Subnets to 62
DataSet
DS1
DS2
DS3
DS4
DS5
VMs
4362
10168
14414
20207
25246
Routers
300
600
800
1000
1200
Subnets
525
1288
1828
2580
3210
Reachable Paths
> 5.67 million
> 29.2 million
> 57.0 million
> 109 million
> 168 million
TABLE III.
LNET DATASET DESCRIPTION
medium virtual networks containing six subnets, while we
vary different factors such as the number of VMs per subnet,
the number of rules per router, and the number of hops
between subnets, to examine corresponding characteristics of
our algorithms, and have been built using OpenStack and
speciﬁcally using Horizon. We then cloned the SNET virtual
infrastructure environments to obtain different
tenants and
thus LNET datasets, which represent large networks, where
each virtual network is organized in a three-tier structure
where the ﬁrst-tier router is connected to the external network,
while the others use extra routes to forward packets between
each other, so in essence they are synthetic. Security rules
are generated in the same logic behind the deployment of
two-tier applications in the cloud. For a given tenant, one
group of VMs can only communicate with each other but
not with outside (or other tenants) networks, while another
group is open to be reached from anywhere. Up to 25,246
VMs are created in the test cloud, with 1,200 virtual routers,
3,210 subnets, and over 43,000 allocated IP addresses. As a
reference, according to a recent report [12], 94% of inter-
rogated OpenStack deployments have less than 10,000 IPs.
Therefore, we consider the scale of our largest dataset is
a representative of large size clouds. The ﬁve datasets in
LNET are described in Table III. We use open-source Apache
Ignite [46] as the parallel computation platform, which can
distribute the workload in real-time across hundreds of servers.
On the other hand, all datasets both in SNET and LNET for
NoD are generated synthatically using the provided generator.3
B. Results
We evaluate the performance of our approaches and the
effect of various factors on the performance.
1) SNET Results: This set of experiments is to test how
network structure and conﬁguration inﬂuence the performance
of our system. All tests using SNET datasets are conducted
with a Linux PC having 2 Intel i7 2.8GHz CPUs and 2GB
memory. Note that, for SNET datasets the veriﬁcation time
for NoD is measured for 1 to 5 pairs, and for TenantGuard
2Note that for NoD, we vary the number of pairs from 1 to 5 through the
X axis, and for TenantGuard, we consider all possible pairs of VMs as the X
axis depicts the number of VMs.
3Available at: http://web.ist.utl.pt/nuno.lopes/netverif
the time is measured for all-pair. As shown in Figure 9(a),
when the number of VMs per subnet is increased from 100 to
500, the preﬁx-level isolation veriﬁcation time increases much
slower than the baseline algorithm (deﬁned in Section IV-A)
and NoD. The reason behind these results is, as illustrated in
complexity analysis in section IV, the preﬁx-to-preﬁx algo-
rithm that reduces the complexity to O(R + N 2), in contrast
to O(R∗N 2) in the baseline algorithm, where R is the number
of hops and N is the number of VMs; when N increases, the
complexity O(R ∗ N 2) increases much faster than O(R + N 2).
On the other hand, as the number of pairs is one of the major
factors for NoD veriﬁcation time, we observe increase in the
veriﬁcation time while increasing the number of pairs from 1
to 5. As shown in Figure 9(b), when the number of routing
rules per router increases exponentially, the veriﬁcation time
for TenantGuard and the baseline algorithm remain relatively
stable due to using radix trie and X-fast binary trie, both of
which have constant searching time; However, the baseline
algorithm takes longer time due to the higher number of pairs
to be veriﬁed. On the other hand, as NoD is designed for a
large number of rules instead of large number of pairs, we
increase the number of pairs from 1 to 5 while keeping the
number of rules similar to the setting of TenantGuard.
Additionally, as the number of hops increases the com-
plexity of the veriﬁcation (it corresponds to the number of
virtual routers on a communication path), we vary the number
of hops between VMs. We investigate the average number of
hop usually encountered in real life systems (e.g., Internet) and
according to [47] and [48], the average number of hops varies
between 12 to 19; hence, we vary the number of hops between
2 to 19. Figure 9(c) shows that the preﬁx-to-preﬁx veriﬁcation
experiences negligible changes. In contrast, a fourfold increase
in the overhead is observed with the baseline algorithm.
Whereas, the veriﬁcation time for NoD increases exponentially
specially after 14 hops, as their algorithms are not optimized
for higher number of hops.
2) LNET Results Using Amazon EC2: Single-Machine
Mode. The LNET datasets are used to examine the scalability
of our system for large virtual networks. Hence, factors exam-
ined in the SNET dataset are kept invariant for each subnet,
and the number of tenant’s subnets is varied as shown in
Table III. There are two modes for LNET tests: single-machine
mode and parallel mode. Single-machine tests are conducted
on one EC2 C4.large instance at AWS EC2 with 2 vCPUs and
3.75 GB memory. We measure NoD performance only for the
single machine tests, as NoD implementation does not support
parallelization. The data collection and initialization steps are
performed on a single machine in both modes.
12
Prefix−to−Prefix
Baseline Algo
NoD
4,362 VMs
10,168 VMs
14,414 VMs
20,207 VMs
25,246 VMs
)
s
(
e
m
T
i
2
1.5
1
0.5
0
600
)
s
(
e
m
T
i
400
200
0
3.3k
4.7k
6.2k
7.8k
9.3k
# of Rules
(b) Isolation verification
4.3k
10k
14k
20k
25k
# of VMs
(a) Data collection
Fig. 10.
Performance Comparison in the Single-Machine Mode with the
LNET Datasets Described in Table III. (a) Showing Data Collection Time,
and (b) Showing Veriﬁcation Time
As shown in Figure 10(a), data collection and processing
time varies between 1.5 to 2 seconds, including retrieving data
from the database, initializing radix tries for routers and secu-
rity groups, etc., which shows that the collection time is not
the prominent part of the execution time. Meanwhile, Figure
10(b) compares the veriﬁcation time between TenantGuard,
baseline algorithm and NoD. When the number of routing
rules increases along with subnets and VMs, the preﬁx-to-
preﬁx algorithm is more efﬁcient than NoD and the baseline
algorithm (e.g., TenantGuard performs 82% faster than NoD
for the largest dataset). NoD (while varying the number of pairs
from 20 to 200 through the X axis) and the baseline algorithm
show almost similar response time. For 9,300 routing rules
with 25,246 VMs in 3,250 subnets, it takes 108 seconds using
the preﬁx-to-preﬁx algorithm, 605 seconds for NoD (for 200
pairs) and around 628 seconds for the baseline algorithm. Note
that TenantGuard veriﬁes in total over 168 millions VM pairs.
We report our extended experiment results to further val-
idate the scalability of TenantGuard in Table IV. In this
set of experiment, we increase the number of routing rules
to 850k, and the number of VMs to 100k to compare the
reported results in Plotkin et al. [11]. For the ﬁrst part, we
compare TenantGuard with NoD for the 850k routing rules
with other parameters as our DS5 datasets, and observe that
TenantGuard completes the all-pair reachability veriﬁcation in
100.14s, which is signiﬁcantly faster than NoD (5.5 days). In
the second part, we generate a completely new datasets with
850k routing rules and 100k VMs, and observe that the all-
pair reachability veriﬁcation takes less than 18 minutes for
TenantGuard, whereas Plotkin et al. [11] needs about 2 hours.
Datasets
NoD [27]
Plotkin et al. [11]
TenantGuard
850k rules
475,200 [11]
-
100.14
850k rules and 100k VMs
-
7,200 [11]
1,055.88
TABLE IV.
COMPARING THE PERFORMANCE (IN SECONDS) BETWEEN
EXISTING WORKS AND TENANTGUARD TO VERIFY ALL-PAIR
REACHABILITY
Parallel Veriﬁcation Test. Although our approach already
demonstrates signiﬁcant performance improvements over NoD
and the baseline algorithm, the results are still based on a
single machine. In real clouds, with large deployments (10Ks
of active VMs), there is need for verifying very large virtual
networks. Therefore, we extend our approach to achieve par-
allel veriﬁcation, where the isolation veriﬁcation is distributed
among the nodes of worker cluster, except the data collection
and initialization run on a single node. This parallel imple-
mentation provides larger memory capacity to our approach,
and results in much shorter veriﬁcation times. For the parallel
mode, one EC2 C4.xlarge instance with 4 vCPUs is conﬁgured
as the controller, and up to 16 instances of the same type
50
40
30
20
10
)
s
(
e
m
T
i
2
)
s
e
m
i
t
(
e
t
a
R
p
u
d
e
e
p
S
8
6
4
2
0
0
14 16
20
40
60
Portion of task distributing (%)
8
5
# of Worker nodes
11
(a) Parallel Mode
(b) Speedup Analysis
Fig. 11. The Performance Improvement of Parallel Computation with LNET
Data Described in Table III. (a) Veriﬁcation Time while Varying the Number
of Worker Nodes in Amazon EC2 for Different Datasets, and (b) Speedup
Analysis over the Number of VMs Using 16 Worker Nodes
with a compute worker cluster, while node discovery and
communications are established by their internal IPs.
Figure 11(a) shows the performance of parallel veriﬁcation
using 2 to 16 worker nodes. Clearly, for each dataset, by
increasing the number of worker nodes, in contrast to the
result of the single machine mode, the overheads decrease
signiﬁcantly. For example, in contrast to 108 seconds in the
single machine mode, it only takes approximately 13 seconds
in the parallel mode with 16 workers, while over 160 millions
of paths are veriﬁed as reachable.
In Figure 11(b), in order to show the scalability of our
approach while increasing the virtual network size, we examine
the relationship between the cluster size and speedup gain.
The parallel execution time can be divided into two parts:
task distribution time to send input data from the controller
to different workers, namely Td, and execution time on those
nodes (we ignore the result generation time due to the small
size of result data). We note that, even if the tasks could be
divided evenly, which is unlikely the case in practice, the tasks
could still arrive at worker nodes at different times. As a
result, some of those tasks may start signiﬁcantly later than
others due to networking delay, while the overall performance
is always decided by the slower runners. As Td becomes larger,
it becomes more predominant in the overall execution time.
However, due to the lack of knowledge on task execution
sequence in the synchronous mode, we cannot accurately
measure the distribution time. Additionally, there will always
be some tasks which begin later than the other tasks. In order
to minimize this impact and to start tasks at roughly the same
time, we use an asynchronous task distribution technique. In
Figure 11(b), the x-axis represents the ratio Td/T , while T
is the overall veriﬁcation time. In addition, the speedup ratio
(Rs) is the performance ratio between sequential and parallel
programs, represented by y-axis. With the number of worker
nodes increasing, Td rises as expected because more data
and code need to be transferred among cluster nodes. When
it becomes more dominant, the speedup rate increases more
gradually. For the smallest dataset, Rs decreases when the
number of workers ranges from 8 to 16. The Td/T ratio can
be used to decide the optimal data size in each node.
Our experiment results show that even a small number
(i.e., 16) of working nodes can handle large-scale veriﬁcation
(i.e., 168 millions of VM pairs); recalling that real world
clouds have the size of 100,000 users and maximum 1,000
VMs for each user. Also, our speed-up analysis (Figure 11(b))
illustrates that after 8 nodes the speedup goes down. Therefore,
we restrict the number of working nodes to 16. The result
of incremental veriﬁcation is not reported, as our discussion
13
Routing/Filtering
Intra-tenant
routing
Inter-tenant
routing
L3 ﬁltering
OpenStack [12]
Host routes, routers
Amazon EC2-VPC [49]