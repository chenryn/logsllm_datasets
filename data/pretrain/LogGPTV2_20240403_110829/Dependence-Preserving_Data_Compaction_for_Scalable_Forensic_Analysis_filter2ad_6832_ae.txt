it with Xu et al.’s technique (LCD). We then evaluate the
effect of these reductions on the CSR log size and the in-
memory dependence graph in Sections 6.4 and 6.5. Runtimes
for dependence graph construction and forensic analysis are
discussed in Section 6.6. The impact of our optimizations on
forensic analysis accuracy is evaluated in Section 6.7.
6.1 Implementation
Our implementation consists of three front-ends and a back-
end written in C++. The front-ends together contain about
6KLoC; the back-end, about 7KLoC. The front-ends process
data from audit sources. One front-end parses Linux audit
logs, while the other two parse Linux and Windows data from
the red team engagement. The back-end uses our BuildVer
algorithm, together with (a) the REO, RNO, and CCO op-
timizations (Section 4.2) to realize FD preservation, and (b)
the source dependence preservation technique described in
Section 4.3. It uses the compact main-memory representation
presented in Section 5.2. Our implementation can also gen-
erate event logs in our CSR format, described in Section 5.1.
7This is the granularity typically available on most of our data sets.
USENIX Association
27th USENIX Security Symposium    1733
The back-end can also read data directly from CSR logs.
We used this capability to carry out many of our experiments,
because data in CSR format can be consumed much faster
than data in Linux audit log format or the OS-neutral format
in which red team engagement data was provided. A few
key points about our implementation are:
• Network connections: We treat each distinct combination
of (remote IP, port, time window) as a distinct source node.
Currently, time windows are set to about 10 minutes. This
means that when we read from any IP/port combination,
all reads performed within a 10-minute period are treated
as coming from a single source. Thus, FD and SD can
aggregate them. After 10 minutes, it is considered a new
source, thus allowing us to reason about remote sites
whose behavior may change over time (e.g., the site may
get compromised). A similar approach is applicable for
physical devices.
• Handling execve: Execve causes the entire memory image
of a process to be overwritten. This suggests that depen-
dences acquired before the execve will be less of a factor
in the behavior of the process, compared to dependences
acquired after. We achieve this effect by limiting REO
from traversing past execve edges.8
• REO* optimization: Almost all edges in our graph are
between subjects and objects. Consider a case when a
subject s reads an object o. The only case where o could
be an ancestor but not a parent is if o was read by another
subject s(cid:48) that then wrote to an object o(cid:48) that is being read
by s. Since this relationship looks distant, we did not
consider that REO* would be very useful in practice.9
6.2 Data Sets
Our evaluation uses data from live servers in a small
laboratory, and from a red team evaluation led by a
government agency. We describe these data sets below.
6.2.1 Data from Red Team Engagement
This data was collected as part of the 2nd adversarial en-
gagement organized in the DARPA Transparent Computing
program. Several teams were responsible for instrumenting
OSes and collecting data, while our team (and others)
performed attack detection and forensic analysis using
this data. The red team carried out attack campaigns that
extended over a period of about a week. The red team also
generated benign background activity, such as web browsing,
emailing, and editing files.
Linux Engagement Data (Linux Desktop). Linux data
(Linux Desktop) captures activity on an Ubuntu desktop
machine over two weeks. The principal data source was
8REO, and especially REO*, can be much more effective without this
restriction, but such an approach also increases the risk of eliminating
significant events from the graph.
9Moreover, because the in- and out-degrees of subjects are typically very
large, a 3-hop search may end up examining a very large number of edges.
Dataset
Linux Desktop
Windows Desktop
SSH/File Server
Web Server
Mail Server
Read Write
Total
Events
72.6M 72.4% 26.2%
14.6M 77.1% 14.5%
14.4M 38.2% 58.3%
2.8M 64.3% 30.3%
70% 23.6%
3M
Other
Clone/
Exec
0.5% 0.9%
1.2% 7.2%
1.2% 2.3%
1.5% 3.9%
1.7% 4.7%
Table 8: Data sets used in evaluation.
the built-in Linux auditing framework. The audit data was
transformed into a OS-neutral format by another team and
then given to us for analysis. The data includes all system
calls considered important for forensic analysis, including
open, close, clone, execve, read, write, chmod, rm, rename,
and so on. Table 8 shows the total number of events in
the data, along with a breakdown of important event types.
Since reads and writes provide finer granularity information
about dependencies than open/close, we omitted open/close
from our analysis and do not include them in our figures.
Windows Engagement Data (Windows Desktop). Win-
dows data covers a period of about 8 days. The primary
source of this data is Event Tracing for Windows (ETW).
Events captured in this data set are similar to those
captured on Linux. The data was provided to us in the
same OS-neutral format as the Linux data. Nevertheless,
some differences remained. For examples, network reads
and network writes were omitted (but network connects
and accepts were reported). Also reported were a few
Windows-specific events, such as CreateRemoteThread.
Registry events were mapped into file operations. From
Table 8, it can be seen that the system call distribution is
similar as for Linux, except for a much higher volume of
“other” calls, due to higher numbers of renames and removes.
6.2.2 Data From Laboratory Servers
An important benefit of the red team data is that it was
collected by teams with expertise in instrumenting and
collecting data for forensic analysis. A downside is that some
details of their audit system configurations are unknown to us.
To compensate for this, we supplemented the engagement
data sets with audit logs collected in our research lab. Audit
data was collected on a production web server, mail server,
and general purpose file and remote access server (SSH/File
Server) used by a dozen users in a small academic research
laboratory. All of these systems were running Ubuntu Linux.
Audit data was collected over a period of one week using the
Linux audit system, configured to record open, close, read,
write, rename, link, unlink, chmod, etc.
6.3 Event Reduction: Comparison of LCD, FD and SD
Fig. 9 shows the event reduction factor (i.e., ratio of
number of events before and after the reduction) achieved
by our two techniques, FD and SD. For comparison, we
reimplemented Xu et al.’s full-trackability reduction as
described by Algorithms 1, 2 and 3 in [42]. As discussed
before, full-trackability equivalence is like a localized version
1734    27th USENIX Security Symposium
USENIX Association
Dataset
Linux Desktop
Size on
Disk
12.9GB
2.1GB
6.7GB
1.3GB
1.2GB
Average (Geometric mean)
Windows Desktop
SSH/File server
Web server
Mail server
CSR
Reduction factor
FD
SD
5.6× 66.1× 76.8×
2.4× 4.46× 4.54×
15.1× 91.5× 122.5×
13.3× 49.3× 57.9×
41×
11.9×
49.2×
35.3× 41.4×
8×
Fig. 9: Event reduction factors achieved by LCD, FD, and SD.
of our continuous dependence preservation criteria, and
hence we refer to it as LCD for consistency of terminology.
LCD, FD and SD achieve an average reduction factor of 1.8,
7 and 9.2 respectively. Across the data sets, LCD achieves
reduction factors between 1.6 and 2.7, FD ranges from 4.6
to 15.4, and SD from 5.4 to 19.1.
As illustrated by these results, FD provides much more
reduction than LCD. To understand the reason, consider
a simple example of a process P that repeatedly reads file
A and then writes file B. The sequence of P’s operations
may look like read(A); write(B); read(A); write(B); ···.
Note that there is an outgoing (i.e., write) edge between
every pair of incoming (i.e., read) edges into P. This violates
Xu et al.’s condition for merging edges, and hence none of
these edges can be merged. Our FD criteria, on the other
hand, can utilize non-local information that shows that A
has not changed during this time period, and hence can
aggregate all of the reads as well as the writes.
We further analyzed the data to better understand the high
reduction factors achieved by FD and SD. We found that
on Linux, many applications open the same object multiple
times. On average, a process opened the same object
approximately two times on the laboratory servers. Since the
objects typically did not change during the period, FD was
typically able to combine the reads following distinct opens,
thus explaining a factor of about 2. Next, we observed
that on average, each open was accompanied by 3 to 5
reads/writes. Again, FD was able to aggregate most of them,
thus explaining a further factor of 2 to 4. We see that the
actual reduction achieved by FD is within this explainable
range for the laboratory servers. For Windows desktop, the
reduction factor was less, mainly because the Windows data
does not include reads or writes on network data. For Linux
desktop data set, FD reduction factor is significantly higher.
This is partly because long-running processes (e.g., browsers)
dominate in this data. Such processes typically acquire a
new dependency when they make a new network connection,
but subsequent operations don’t add new dependencies, and
hence most of them can be reduced.
Our implementation of SD is on top of FD: if an edge
cannot be removed by FD, then the SD criterion is tried.
This is why SD always has higher reduction factor than FD.
SD provides noticeable additional benefits over FD.
Table 10: Log size on disk. The second column reports the log size of origi-
nal audit data. Each remaining column reports the factor of decrease in CSR
log size achieved by the indicated optimization, relative to the size on disk.
6.4 Log Size Reduction
Table 10 shows the effectiveness of our techniques in reduc-
ing the on-disk size of log data. The second column shows
the size of the original data, i.e., Linux audit data for labora-
tory servers, and OS-neutral intermediate format for red team
engagement data. The third column shows the reduction in
size achieved by our CSR representation10, before any reduc-
tions are applied. The next two columns show the size reduc-
tions achieved by CSR together with FD and SD respectively.
From the table, it can be seen that the reduction factors
from FD and CD are somewhat less than that shown in Fig. 9.
This is expected, because they compress only events, not
nodes. Nevertheless, we see that the factors are fairly close,
especially on the larger data sets. For instance, on the Linux
desktop data, where FD produces about 15× reduction, the
CSR log size shrinks by about 12× over base CSR size.
Similarly, on SSH/File server, FD event reduction factor is
8×, and the CSR size reduction is about 6×. In addition, the
log sizes are 35.3× to 41.4× smaller than the input audit logs.
6.5 Dependence Graph Size
Table 11 illustrates the effect of different optimizations on
memory use. On the largest dataset (Linux desktop), our
memory use with FD is remarkably low: less than two bytes
per event in the original data. On the other two larger data sets
(Windows desktop and SSH/file server), it increases to 3.3
to 6.8 bytes per event. The arithmetic and geometric means
(across all the data sets) are both less than 5 bytes/event.
Examining the Linux desktop and Windows desktop
numbers closely, we find that the memory use is closely
correlated with the reduction factors in Fig. 9. In particular,
for the Linux desktop, there are about 4.7M events left after
FD reduction. Each event results in a forward and backward
edge, each taking 6 bytes in our implementation (cf. Section
5). Subtracting this 4.7M*12B = 56.4MB from the 111MB,
we see that the 1.1M nodes occupy about 55MB, or about 50
bytes per node. Recall that each node takes 32 bytes in our
implementation, plus some additional space for storing file
names, command lines, etc. A similar analysis of Windows
10Recall that CSR is uncompressed, so there is room for significant
additional reduction in size, if the purpose is archival storage.
USENIX Association
27th USENIX Security Symposium    1735
0	2	4	6	8	10	12	14	16	18	20	Windows	Desktop	Linux	Desktop	Web	server	Mail	server	SSH/File	server	SD	FD	LCD	Dataset
Linux Desktop
Windows Desktop
SSH/File Server
Web Server
Mail Server
Total
Total No.
of Nodes
Total
Events
72.6M
1.1M
10.3M
781K
14.4M
430K
2.8M
141K
189K
3M
2.64M 103.1M
FD
(MB)
111
67
45
16
21
260
SD
(MB)
107
67
39
15
20
248
Table 11: Memory usage. The second column gives the total number of
nodes in the dependence graph before any versioning. The third column
gives the total number of events. The fourth and fifth columns give the total
memory usages for FD and SD. Average memory use across these data