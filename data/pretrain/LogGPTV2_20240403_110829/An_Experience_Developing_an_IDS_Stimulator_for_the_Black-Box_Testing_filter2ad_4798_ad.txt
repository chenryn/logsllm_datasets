3. By comparison to Snort’s performance (Table 2), Net
Prowler produces fewer alerts overall. However, it should
be emphasized that the trafﬁc generated by Mucus-1 is syn-
thetic. Therefore, alerts generated by Net Prowler in this
experiment are false positives. One tempting conclusion to
draw from the fact that Net Prowler generated only a few
alerts is that Net Prowler is more capable of correctly iden-
tifying synthetic attacks. That is, one might like to conclude
that Net Prowler is more robust against IDS stimulation at-
tacks. An alternative explanation, however, could be that
Net Prowler has a less complete, or markedly different, set
of signatures and simply is not checking for the attacks rep-
resented by the synthetic signatures that were missed.
Because the authors were not able to obtain the Net
Prowler signature set it was not possible to determine, for
each instance of synthetic attack trafﬁc, which of these con-
ﬂicting conjectures is correct. However, one method of de-
termining whether Net Prowler is more discerning or just
has a signature set that is disjoint from Snort is to expose
Net Prowler to the actual attacks that are being simulated
using Mucus-1. The alerts produced by Net Prowler should
then indicate which attacks it contains signatures for. The
following section precisely details this experiment.
5.4 Real Attack Trafﬁc
As mentioned in the previous section, real attacks are
one way of determining whether an IDS has a signature for
some attack, without the beneﬁt of precise knowledge of the
system’s signatures or implementation. This knowledge al-
lows one to evaluate (without having access to the signature
set) whether an IDS exhibits robustness against IDS stim-
ulation attacks or whether the system in question simply
does not model the attacks that are being synthetically re-
produced. In addition, the ultimate goal of automated attack
trafﬁc generation for network IDS evaluation is to produce
trafﬁc that is indistinguishable from the actual attacks being
simulated. Comparing the responses of an IDS to synthetic
and real versions of an attack is one way of measuring the
ﬁdelity of synthetically generated attacks.
In this experiment, a sampling of exploits and informa-
tion gathering attacks were gathered from sources on the
Internet and run against Snort and Net Prowler. Each at-
tack has a corresponding Snort rule, which enabled syn-
thetic trafﬁc for each attack to be generated for comparison.
It should be noted that, in some cases, the vulnerable ser-
vices were not installed on the victim host, and, therefore,
the exploits were not successful. However, for the purposes
of this experiment, an attempted exploit is considered to be
a critical event from the perspective of a network IDS. Table
4 summarizes the results of this experiment.
The results show strong agreement for the Snort IDS:
In all cases considered, Snort generates alerts for both syn-
thetic and real versions of the attacks. This indicates that,
from the perspective of Snort, the synthetically generated
trafﬁc is essentially indistinguishable from the true attacks.
Table 4 also shows that the majority of attacks selected
go undetected by Net Prowler. While in the previous exper-
iment not detecting synthetically generated trafﬁc seemed
to show Net Prowler’s robustness to IDS stimulation, not
detecting the real attack in this experiment gives evidence
to the alternate explanation, namely that Net Prowler’s sig-
nature set is smaller than, or largely disjoint from, that of
Snort. Agreement between the synthetic and true attack
event streams is achieved in all but one instance, the HTTP
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:41:45 UTC from IEEE Xplore.  Restrictions apply. 
8
Activity/Vulnerability
Bay/Nortel Nautica Marlin DoS
OpenBSD ftpd
Showmount (portmap request)
ntpd buffer overﬂow
PHP strings buffer overﬂow
HTTP phf
count.cgi
Snort Alert Generated?
Synthetic Attack Real Attack
Net Prowler Alert Generated?
Synthetic Attack Real Attack
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
no
no
no
no
no
no
yes
no
no
no
no
no
yes
yes
Table 4. Comparison of behavior of Snort and Net Prowler against synthetic and true attack trafﬁc.
phf attack, in which the real attack causes Net Prowler to
generate an alert while the synthetic attack fails to do so.
From this, one can conclude that for this attack Net Prowler
has a more robust detection capability than Snort.
6 Conclusions and Future Work
A tool for cross-testing of signature-based intrusion de-
tection systems was presented. A new methodology for the
generation of test cases was also presented and a tool, called
Mucus-1, was introduced. An empirical evaluation of this
ﬁrst proof-of-concept prototype of the tool was carried out,
with interesting results.
These experiments illuminate the fact that the lack of
information ﬂow from commercial IDS vendors is detri-
mental to the research community. Some commercial IDS
vendors refuse to disclose precise information about their
signatures, while some do not disclose precise information
about their algorithms. Most do not disclose either. From
the perspective of an IDS evaluator, this opaqueness makes
it impossible to establish the cause of errors or misdetec-
tions. Dropped packets, poor signatures, and logical errors
are indistinguishable unless system internals are known.
This initial work in the area of black-box evaluation
of network IDS will be extended in several ways. First,
support in Mucus-1 for signature languages other than
Snort would dramatically increase the scope of this work.
Semantically, some languages are similar to Snort and
would require a relatively minor effort to implement, while
other languages permit more powerful expressions of at-
tacks. STATL [8], for example, allows the operator to
specify stateful, multi-step, time-dependent attack scenar-
ios. Clearly, generating random instances of such scenarios
implies a greater representational complexity in Mucus-1.
Some ability to reason about time constraints would also be
needed. The current architecture is able to accommodate
such changes.
Second, in order to improve the testing capabilities of
Mucus beyond qualitative measures of IDS performance,
some measure of signature quality must be developed.
Without knowing how closely signatures match the event
streams of attacks, it is not possible to use the proposed
approach to draw quantitative conclusions about the perfor-
mance of an IDS.
Finally, Section 5.2 examined the problem of signature
overlap combined with the ﬁrst-match behavior in Snort.
That is, the situation in which, for a given datagram, the
speciﬁed constraints of two or more Snort rules are satis-
ﬁed, but only one match is reported. A real-world example
showing a remote root exploit disguised as a lower impact
alert was demonstrated. In principle, it is straightforward
to compute the intersection of constraints speciﬁed by any
two rules in the Snort ruleset. An automated tool to perform
this computation for all pairs in the Snort ruleset would be
advantageous.
Acknowledgments
The research work described in this paper builds on the
results achieved by several people. We would like to thank
Sniph, the author of snot, for building the ﬁrst extensive tool
for IDS stimulation. The code of the ﬁrst Mucus-1 proto-
type re-used portions of his code. We would also like to
thank Steve Eckmann, who developed the Snort-to-STATL
translator. The Mucus-1 Snort rule parser was built by ex-
tending his original work.
This research was supported by the State of California,
the Army Research Ofﬁce, under agreement DAAD19-01-
1-0484 and by the Defense Advanced Research Projects
Agency (DARPA), and Rome Laboratory, Air Force Ma-
teriel Command, USAF, under agreement number F30602-
97-1-0207. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
References
[1] Anzen. nidsbench:a network intrusion detection sys-
tem test suite. http://packetstorm.widexs.
nl/UNIX/IDS/nidsbench/, 1999.
[2] S. Aubert.
Idswakeup. http://www.hsc.fr/
ressources/outils/idswakeup/, 2000.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:41:45 UTC from IEEE Xplore.  Restrictions apply. 
9
[3] S. Axelsson. The Base-Rate Fallacy and its Implica-
tions for the Difﬁculty of Intrusion Detection. In Pro-
ceedings of the 6th ACM Conference on Computer and
Communications Security, 1999.
[4] R. Durst, T. Champion, B. Witten, E. Miller, and
L. Spagnuolo. Addendum to “Testing and Evaluat-
ing Computer Intrusion Detection Systems”. CACM,
42(9):15, September 1999.
[5] R. Durst, T. Champion, B. Witten, E. Miller, and
L. Spagnuolo. Testing and Evaluating Computer In-
trusion Detection Systems. CACM, 42(7):53–61, July
1999.
[6] S. Eckmann. Translating Snort Rules to STATL Sce-
In Recent Advances in Intrusion Detection,
narios.
Davis, CA, October 2001. Short paper presentation.
[7] S.T. Eckmann. The STATL Attack Detection Lan-
guage. PhD thesis, Department of Computer Science,
UCSB, Santa Barbara, CA, June 2002.
[8] S.T. Eckmann, G. Vigna, and R.A. Kemmerer.
STATL: An Attack Language for State-based Intrusion
Detection. In Proceedings of the ACM Workshop on
Intrusion Detection Systems, Athens, Greece, Novem-
ber 2000.
[9] C. Giovanni.
a Stick.
stick/, 2002.
Fun with Packets: Designing
http://www.eurocompton.net/
[10] Paul Helman and Gunar Liepins. Statistical Foun-
dations of Audit Trail Analysis for the Detection of
Computer Misuse. In IEEE Transactions on Software
Engineering, volume Vol 19, No. 9, pages 886–901,
1993.
[11] K. Ilgun, R.A. Kemmerer, and P.A. Porras. State
Transition Analysis: A Rule-Based Intrusion Detec-
tion System.
IEEE Transactions on Software Engi-
neering, 21(3):181–199, March 1995.
[12] H. S. Javitz and A. Valdes. The NIDES Statistical
Component Description and Justiﬁcation. Technical
report, SRI International, Menlo Park, CA, March
1994.
[13] K. Julisch. Mining alarm clusters to improve alarm
handling efﬁciency. In Proceedings of the 17th Annual
Computer Security Applications Conference, Orlando,
FL, 2001.
[14] C. Ko, M. Ruschitzka, and K. Levitt. Execution Mon-
itoring of Security-Critical Programs in Distributed
Systems: A Speciﬁcation-based Approach.
In Pro-
ceedings of the 1997 IEEE Symposium on Security and
Privacy, pages 175–187, May 1997.
10
[15] U. Lindqvist and P.A. Porras. Detecting Computer
and Network Misuse with the Production-Based Ex-
pert System Toolset (P-BEST). In IEEE Symposium
on Security and Privacy, pages 146–161, Oakland,
California, May 1999.
[16] R. Lippmann, D. Fried, I. Graf, J. Haines, K. Kendall,
D. McClung, D. Weber, S. Webster, D. Wyschogrod,
R. Cunningham, and M. Zissman. Evaluating In-
trustion Detection Systems: The 1998 DARPA Off-
line Intrusion Detection Evaluation. In Proceedings of
the DARPA Information Survivability Conference and
Exposition, Volume 2, Hilton Head, SC, January 2000.
[17] J. McHugh. Testing Intrusion Detection Systems: A
Critique of the 1998 and 1999 DARPA Intrusion De-
tection System Evalautions as Performed by Lincoln
Laboratory. ACM Transaction on Information and
System Security, 3(4), November 2000.
[18] T. Parr. Antlr - documentation. http://www.
antlr.org/.
[19] S. Patton, W. Yurcik, and D. Doss. An Achilles’ Heel
in Signature-Based IDS: Squealing False Positives in
SNORT. In Proceedings of RAID 2001, Davis, CA,
October 2001.
[20] T.H. Ptacek and T.N. Newsham.
Insertion, Evasion
and Denial of Service: Eluding Network Intrusion De-
tection. Technical report, Secure Networks, January
1998.
[21] M. Ranum. Experience Benchmarking Intrusion De-
tection Systems. NFR Security White Paper, Decem-
ber 2001.
[22] M. Roesch. Snort - Lightweight Intrusion Detection
In Proceedings of the USENIX LISA
for Networks.
’99 Conference, November 1999.
[23] Mike
Schiffman.
libnet.
http://
packetfactory.net/libnet/, 2002.
[24] Sniph. Snot. http://www.sec33.com/sniph/, 2001.
[25] D. Wagner and D. Dean. Intrusion Detection via Static
Analysis. In Proceedings of the IEEE Symposium on
Security and Privacy, Oakland, CA, May 2001. IEEE
Press.
[26] C. Warrender, S. Forrest, and B.A. Pearlmutter. De-
tecting intrusions using system calls: Alternative data
models. In IEEE Symposium on Security and Privacy,
pages 133–145, 1999.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:41:45 UTC from IEEE Xplore.  Restrictions apply.