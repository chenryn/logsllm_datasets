Process HB Encoder maintains process timelines. A timeline
corresponds to a sequence of events, ordered by timestamp,
that were executed by the same process. For each incoming
event, the Intra-Process HB Encoder is responsible for insert-
ing it into the corresponding process timeline in the correct
position based on its timestamp. This procedure guarantees
that, for a given process, events enqueued out of order will
still produce a causally-consistent timeline.
The Intra-Process HB Encoder then periodically ﬂushes the
process timelines to the graph database. The ﬂush interval is a
tunable parameter: longer ﬂush intervals provide lower runtime
overhead (due to fewer connections to the database) at the cost
of more memory consumption to maintain pending established
causal relationships. Shorter intervals,
in turn, reduce the
memory footprint and make data more quickly available for
querying (which is useful for online monitoring), but incur a
performance slowdown.
In practice, storing a process timeline consists of creating a
new node per event and encoding intra-process edges between
pairs of nodes that correspond to contiguous events in the
timeline. As an example, let us consider a timeline T with
events to the queue of the next processing stage.
B. Inter-Process HB Relationships
In the second processing stage, the Inter-Process HB En-
coder computes the happens-before relationships between
events of two different processes.
Inter-process causality stems from the second property of
the happens-before deﬁnition and, in contrast to intra-process
causality, does not rely on timestamps. Instead, it relies on a
message m that unequivocally identiﬁes that its departure from
a process causally-precedes its arrival at another process. In
practice, m is usually either a unique message identiﬁer or a
piece of context data that indicates the causal relation.
In the current version of Horus,
the Inter-Process HB
Encoder determines inter-process causal dependencies based
on the event attributes captured by eBPF kernel probes. For
instance, in a TCP connection scenario, events of sending
(SN D) and receiving (RCV ) bytes include attributes concern-
ing the source and destination IP addresses and ports. Hence,
considering the TCP delivery and ordering guarantees, one
can establish SN D → RCV causal pairs for each connection
channel. Alongside, one can deﬁne the following causal pairs
of events between a parent process p and a child process c:
F ORKp → ST ARTc and EN Dc → JOINp.
The Inter-Process HB Encoder operates in a stream fashion,
building inter-process causal dependencies according to the
nature of each event. To improve performance, incomplete
causal pairs are kept in memory until the corresponding pairing
event is consumed from the queue.
Periodically, Inter-Process HB Encoder ﬂushes the complete
causal pairs by inserting a new edge per pair into the graph
database. Once the causal pairs are persisted, the events exit
the processing pipeline and become available for analysis.
We note that the Inter-Process HB Encoder can be easily
extended with new causality rules based on event attributes.
This feature renders Horus with the ability to accommodate
arbitrary types of events and happens-before dependencies into
the execution causal graph.
V. EFFICIENT CAUSAL GRAPH QUERYING
Once the causal graph is stored in the graph database, the
developer can start zeroing in on the error’s root cause. In
Horus, this is done via reﬁnement queries written in Cypher
(which is an expressive query language provided by Neo4j).
the analysis of the causal execution graph
In general,
comprises two main types of queries:
Q1. May event a causally affect event b?
Q2. What are the causal paths between a and b?
Q1 is the fundamental query for evaluating the happens-
before relation between any two events a and b. In turn, Q2
aims at extracting the sub-graph of causal events occurring
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
216
P1
A
C
K
M
1
2
3
4
5
6
7
8
9
10
P2
P3
B
G
H
L
D
E
F
I
J
Vector Clock 
Timestamps
[1,0,0]
[2,0,0]; [1,1,0]
[2,0,1]
[2,0,2]
[2,2,2]; [2,0,3]
[2,3,2]
[2,3,4]
[2,4,2]; [2,3,5]
[3,3,5]
[4,4,5]
Horus augments the causal graph with logical timestamps,
namely logical clocks (LC) [6] and vector clocks (VC) [23].
Logical clocks are scalar values assigned to each event such
that, for every two events a and b, the following condition
holds: a → b =⇒ LC(a) < LC(b). In Figure 3, logical
clocks are depicted as the timestamps on the left. For example,
B → G =⇒ LC(B) = 2 < LC(G) = 5. Note, however,
that the reverse implication is not guaranteed: LC(C) = 2 <
LC(G) = 5, but C does not happen before G. In fact, logical
clocks alone are not enough to reason about event causality.
In contrast, vector clocks sufﬁce to reason about the causal
ordering of events in a distributed execution by providing the
property: a → b ⇐⇒ V C(a) < V C(b). We represent
vector clocks in Figure 3 next to each event: for example,
C → D ⇐⇒ V C(C) = [2, 0, 0] < V C(D) = [2, 0, 1]. In
turn, for events C and G, since neither V C(C) < V C(G)
nor V C(G) < V C(C) holds, we can conclude that they are
concurrent.
Fig. 3: Causal graph with three process timelines. Nodes
denote events along their execution, and edges their causal
dependencies. Logical clocks and vector clocks are depicted
on the left and on the right of the events, respectively. Thick
borders (respectively, shades) indicate the nodes explored by
a default graph database traversal (respectively, Horus) to
compute the causal paths between C and F .
in-between them, which is particularly useful to reconstruct
the causal trace of a request. For instance, computing Q2 for
events Launcher-1.1 and Launcher-1.2 in Figure 1 would
produce a graph with all events in the ﬁgure causally ordered.
These queries can be expressed with path discovery and
traversal operations that are provided by the graph database.
More concretely, one can answer Q1 with a shortest path
algorithm, and answer Q2 by ﬁnding all paths between the two
events. Unfortunately, as these built-in traversals are oblivious
to the semantics of distributed systems, they are inefﬁcient for
causal queries and cannot scale for large graphs. To understand
why this is the case, let us consider the causal graph depicted
in Figure 3 with three process timelines (P 1, P 2, and P 3).
Recall that nodes denote events along their execution and
edges the happens-before relations between them.
Assume that we want to obtain the events that happened
between C and F , which is a query of type Q2. Answering this
with a graph database’s default traversal would correctly yield
the result set {C, D, E, F }, although at the expense of visit-
ing the nodes {C, D, E, G, F, H, I, L, M, K} (represented in
Figure 3 with a thick border). Observing the ﬁgure, one can
notice that this approach is far from optimal, as, for example,
nodes I and J are visited despite being clearly irrelevant to
the query because they occur after F . The reason behind this
inefﬁciency is that the traversal performed by the database
does not take into account the notion of time nor the directed
acyclic properties of the causal graph.
To address this limitation and speed up query processing,
Horus performs a graph traversal similar to topological
sorting to assign both logical and vector clocks to each event
in the graph. These logical timestamps are then used to speed
up querying in a twofold fashion, as described below.
Let G = (V, E) be the causal graph under analysis and
a, b ∈ V the start and end event of a query, respectively.
First, Horus leverages the logical clocks of a and b to
quickly bound the portion of the graph that is relevant to
the query. In practice, this corresponds to using the logical
clocks as database indexes and compute the following over-
approximation V (cid:2) of the nodes in the result set: V (cid:2) = {v ∈
V | LC(a) <= LC(v) <= LC(b)}. For the example of
computing the causal paths between C and F considered
earlier in this section, this step produces the following subset
of nodes {C, B, D, E, G, F } (shaded nodes in Figure 3). Note
that VCs would also allow computing V (cid:2), however they are
inappropriate for graph database indexing due to their non-
scalar nature.
Second, Horus uses vector clocks to prune out the events in
V (cid:2) that are concurrent to a and b. This operation yields a sub-
set V (cid:2)(cid:2) containing only the events in the causal paths between
a and b: V (cid:2)(cid:2) = {v ∈ V (cid:2) | V C(a) < V C(v) < V C(b)}. For
the previous example, this step will discard events B and G
and leave only the sub-set {C, D, E, F }.
Finally, Horus computes the result set of the query by
collecting the edge set E (cid:2) with the connections between the
nodes in V (cid:2)(cid:2): E (cid:2) = {ex→y ∈ E | x ∈ V (cid:2)(cid:2) ∧ y ∈ V (cid:2)(cid:2)}.
In summary, Horus leverages logical time to answer the two
types of reﬁnement queries as follows.
Q1. V C(a) < V C(b);
Q2. CausalP atha→b = (V (cid:2)(cid:2), E (cid:2)), where:
V (cid:2) = {v ∈ V | LC(a) <= LC(v) <= LC(b)}
V (cid:2)(cid:2) = {v ∈ V (cid:2) | V C(a) < V C(v) < V C(b)}
E (cid:2) = {ex→y ∈ E | x ∈ V (cid:2)(cid:2) ∧ y ∈ V (cid:2)(cid:2)}
We implemented the logical time optimizations in Neo4j
two new procedures denoted happensBefore() and
as
getCausalGraph(). This way, it becomes possible to write
and execute efﬁcient causal queries using Cypher.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:14 UTC from IEEE Xplore.  Restrictions apply. 
217
Event Type
LOG
RCV
CREATE
START
SND
END
JOIN
CONNECT
FSYNC
ACCEPT
Total Events
Occurrences (approx. %)
4,531 (22.52%)
4,339 (21.57%)
3,618 (17.99%)
3,340 (16.60%)
2,689 (13.37%)
660 (3.28%)
357 (1.77%)
260 (1.11%)
173 (0.86%)
149 (0.74%)
20,116
TABLE I: Number of events per type present in the causal
graph under analysis. LOG events are produced by the log4j
logging library, while the rest is captured by Horus’ kernel-
level tracer.
VI. CASE STUDY – DEBUGGING F13 WITH HORUS
Recall the TrainTicket’s F13 fault introduced in Section II,
which causes an error due to the interleaved execution of two
requests. In this section, we demonstrate how Horus’ causal-
consistent approach overcomes the limitations of toolsets like
Elastic Stack and allows troubleshooting the F13 error.
For collecting TrainTicket’s events at runtime, we set up
Horus with two event sources, as follows. First, we conﬁgured
Horus’ kernel-level tracer to attach details about the Docker
containers in which TrainTicket’s services run. The goal was
to later allow uniquely identifying each individual host in the
system. Second, we enabled a new log4j appender for attach-
ing useful process information to TrainTicket’s log messages
before forwarding them to Horus. We note that, on each host,
both event tracers rely on the same monotonically increasing
physical clock to later ensure the correct encoding of the intra-
process causality.
Table I reports the amount of events, grouped by type,
captured by Horus when building the causal graph for
TrainTicket’s F13 failing execution. This causal graph results
from six minutes of execution, which generated 20,116 events,
spread across 96 process timelines, and 27,859 causal rela-
tionships, from which 4,593 (16.49%) encode inter-process
causality. Despite LOG being the most common event type,
SNDs and RCVs together, which are essential for encoding
inter-process causality, account for around 35% of the graph.
The discrepancy between the percentage of SND and RCV
events is explained by different buffer sizes on the hosts, which
can cause a single message to be read by multiple partial
RCVs.
In the following, we revisit
describe
software
II-C and
Section
TrainTicket’s
debug the payment failure report in the issue tracker.
engineer, would use Horus
how Steve,
the cautionary tale from
our ﬁctional
to
Steve starts the day with an issue ticket on his hands,
reporting that an error popped up when a user performed
the payment request for order 652aaf9b. Fortunately, the user
218
has also provided the error message in the ticket description:
java.lang.RuntimeException: [Error Queue].
Steve decides to inspect the logs recorded from the moment
the payment request started until the moment in which the
error message appeared. To this end, he has to ﬁrst identify
the events in the causal graph that delimit that relevant portion
of the execution.
in Horus,
The beginning of the payment is deﬁned by a message
sent from the Launcher service to the Payment service. Steve
knows that,
this message is represented by the
→ RCVPayment causal pair. As such, he is
ﬁrst SNDLauncher
able to obtain the logs concerning the failing request by
executing a query to compute the causal graph from that
SNDLauncher event
to the LOGLauncher event containing the
message java.lang.RuntimeException: [Error Queue].
Also, to ensure that he is focusing on the right request, Steve
augments the query with an additional clause stating that the
START event of the payment request must happen before (in
terms of logical time) the error event.
Figure 4a details the aforementioned Cypher query to reﬁne
the causal graph for the failing request under analysis. The
query is composed of three parts,
identiﬁed by the three
comment blocks in Figure 4a. The ﬁrst part ﬁlters the events
that either belong to the beginning of a payment request or
represent an error message. In other words, this part aims at
ﬁnding the potential boundaries of the failing request.
The second part computes the causal paths that connect
the events returned in the previous step and collects the log
messages for each one of them. Note how Horus extends
Neo4j with the procedure getCausalGraph() for efﬁciently
extracting the causal graph between two events in the graph.
Finally, the third part of the query aims at ﬁltering the events
belonging solely to the order 652aaf9b. The ﬁnal output of the
query is depicted in Figure 4b. Note that the log statements in
the ﬁgure are the same as those in Figure 1, but this time the
events are causally ordered.
After executing the query, Steve proceeds with the analysis
by inspecting the log messages. At this point, he notices that
the order status changed from UNPAID (line 4) to CANCELED
(line 6). The only valid state transition after a payment request
is from UNPAID to PAID. However, the logs show that the order