Hi everyone,
I was using Celery 3.0.11 + Redis 2.6.2 as a broker. In my development
environment (Mac OS X 10.8.2, Python 2.7.3, Django 1.4.3, 1 celery worker with
concurrency = 4) I created a couple of simple tasks for my project (in
tasks.py):
    @task
    def invoice_delete(invoice):
        i = reload_instance(invoice)
        if not i.deleted and i.amount_paid == 0:
            i.delete()
    @task
    def expire_invoices():
        invoices_to_expire = core.models.Invoice.objects.filter(
            expires__lt=(now() + timedelta(minutes=1)),
            deleted=False,
        )
        for i in invoices_to_expire:
            invoice_delete.apply_async(args=[i], eta=i.expires)
and scheduled expire_invoices tasks to run every minute (in settings.py):
    CELERYBEAT_SCHEDULE = {
        'expire_invoices': {
            'task': 'core.tasks.expire_invoices',
            'schedule': crontab(),
            }
    }
it worked, and I pushed it into production - Amazon Linux, Celery 3.0.11 - 1
worker with concurency=10, Redis 2.6.2, Python 2.7.3, Django 1.4.3. Celery and
Redis were running under Supervisord. In production I got a problem: my
expire_invoices task was launched 3 times within every minute, instead of 1,
so I got duplicated invoice.deleted events. I tried the following:
  * changed crontab() to timedelta(seconds=60) - did not helped, still launched 3 times within a minute. If I tried seconds=30, it was launched twice within 30 seconds interval;
  * upgraded celery to 3.0.15 - no effect;
  * purged broker storage (using FLUSHALL - very rude) and cleaning schedule file and restarting everything - no effect;
  * switched from celery beat schedule storage from a file to a database, via "-S djcelery.schedulers.DatabaseScheduler" option - no effect;
  * temporally switched broker to 'django://'. I know that it is not recommended for production, I just wanted to see what happens. And... it helped. My task was launching exactly once a minute, as needed. Finally I ended up with installation of RabbitMQ and switching broker url to it, works fine as well.
My supervisord configs in production - celery:
    [program:%(project_name)s_celery]
    command=%(project_env)s/bin/python manage.py celery worker -E --loglevel=INFO --concurrency=10
    directory=%(project_app)s
    user=%(project_user)s
    numprocs=1
    stdout_logfile=%(project_logs)s/supervisor_celery_stdout.log
    stderr_logfile=%(project_logs)s/supervisor_celery_stderr.log
    autostart=true
    autorestart=true
    startsecs=10
    stopwaitsecs = 600
    priority=998
...and celery beat:
    [program:%(project_name)s_celery_beat]
    command=%(project_env)s/bin/python manage.py celery beat --loglevel=INFO -s %(celerybeat_dir)s/schedule
    directory=%(project_app)s
    user=%(project_user)s
    numprocs=1
    stdout_logfile=%(project_logs)s/supervisor_celery_beat_stdout.log
    stderr_logfile=%(project_logs)s/supervisor_celery_beat_stderr.log
    autostart=true
    autorestart=true
    startsecs=10
    priority=999