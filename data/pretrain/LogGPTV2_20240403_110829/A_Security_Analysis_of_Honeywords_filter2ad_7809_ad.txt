n · k sweetwords from lists {SW1, SW2,
··· , SWn} in
decreasing order of normalized probability. More speciﬁcally,
the probability of each sweetword swi,j (1 ≤ i ≤ n and
1 ≤ j ≤ k) comes directly from a known password distribution
D (i.e., using the List password model PD(·)), but it has been
normalized among each user’s own k sweetwords:
Step 1. ∀swi,j ∈ SWi, if swi,j ∈ D, set
∑
Step 2. ∀swi,j ∈ SWi, if swi,j ̸∈ D, set Pr(swi,j) = 0;
Step 3. ∀swi,j ∈ SWi, set
Pr(swi,j) = PD(swi,j);
Pr(swi,j) = Pr(swi,j)/
k
t=1 Pr(swi,t).
If the system allows more than one honeyword login attempt
(i.e., T1 >1), after any one of the sweetwords in SWi has
been attempted, the probability of all the other unattempted
sweetwords in SWi shall be normalized:
Step 3’. Let I denote the set of subscripts of all the sweet-
that have already been used in
̸∈ I, set
words in SWi
login attempts. ∀swi,j ∈ SWi ∧ j
∑
Pr(swi,j) = Pr(swi,j)/
t ̸∈I Pr(swi,t).
After the probability of each unattempted sweetwords in
SWi has been normalized, one can sort the sweet list SWi
and identify the one with the highest priority. This corresponds
to an instantiation of the function getSweetword(SWi)
in Line 5 of Algorithm 1. Once it has been instantiated, a
run of Algorithm 1 will produce the success-number graph.
Similarly, after the Steps 1∼3, every sweetword in SWi
is with a (normalized) probability, and thus the function
getSweetword(SWi) in Line 5 of Algorithm 2 is instanti-
ated. Finally, a run of Algorithm 2 produces the ﬂatness graph.
Since password distributions in reality differ greatly from
each other (see concrete examples in Fig. 3 of [30]), many
sweetwords in the password ﬁle F do not appear in the
distribution D, and they will be assigned a probability 0. This
causes great inaccuracies in sweetword ranking. For instance,
when using Dodonew-tr as the training set (i.e., to be D) and
Dodonew-ts as the test set, over 74% sweetwords in the ﬁle
F (which is generated from Dodonew-ts) will be assigned a
probability 0 according to Step 2. Crucially, one should not
think that things that one has not yet seen are of probability
0. This is called sparsity issue in machine learning, and it
can be addressed by applying the techniques of smoothing.
A number of smoothing methods have been proposed for
language modeling, such as Laplace and Good-Turing, and
they have been widely used in password studies [24], [30].
However, they cannot be readily applied to our settings where
the majority of sweetwords do not appear in the training set.
We devise an “+1” smoothing method:
Step 2’. ∀swi,j ∈ SWi, if swi,j ̸∈ D, set Pr(swi,j)= 1|D|+1.
We have experimented with the Laplace, Good-Turing and
“+1” smoothing methods, and found our “+1” method though
simple yet most effective. Thus, we prefer the “+1” method.
Note that, “+1” method assigns the same, constant value
1|D|+1
to all sweetwords that have not appeared, and thus it is mainly
effective for ranking popular sweetwords, mainly suitable for
attacking Juels-Rivest’s four methods. When the main goal is
to rank unpopular sweetwords, other smoothing methods shall
7
Algorithm 2: Distinguish the real password from every list
of sweetwords in a given password ﬁle F .
Input: n sweetword lists fSW1, SW2, ..., SWng, one list per user.
Output: A vector V shows the ﬂatness graph.
1 Initialize the vector V to be empty;
2 for i = 1 to n do
3
4
5
r = 0;/* r counts the attempt num before a success for SWi.*/
while !SWi.isEmpty() do
/* according to the
(p, sw) = getSweetword(SWi);
speciﬁed attacking strategy, get sweetword sw with the highest
priority p among all un-attempted sweetwords in SWi. */
success=Login(SWi.id, sw);/*use SWi.id as user name
and sw as password to login as user Ui. */
SWi.deleteSweetword(sw);
r++; /* the attempt num for SWi increases by one. */
if success then
break /* if succeed, move to the list SWi+1. */
6
7
8
9
10
V.Insert(r);
11
12 return V
be designed. In Sec. III-E, we will devise a new strategy for
attacking unpopular sweetwords.
Remark 3. It is worth noting that, the key difference between
the above two attacking strategies, i.e. the (additional) Step
3 of the Normalized top-PW, will have a large effect on the
success-number graph. However, this difference has no effect
on the ﬂat graph, because all of each user’s k sweetwords
will be attempted and the Step 3 of the “normalized Top-
PW” strategy does not change the relative rankings within
each sweetword list. Both strategies (and the third one “Norm
PW-model” in Sec. III-E) do not mean that “if a password
appeared in a breach list D, and appears in the list of
honeywords, it is the actual password”. Generally, there are
j (1≤j≤k) sweetwords appear in D and k − j sweetwords
don’t, and the key difﬁculty lies in how best to rank these k
sweetwords. This requires creative efforts: new normalization
and smoothing techniques, and retooling password models to
eliminate sweetword ties which have the same probability.
“Using a blacklist” is ineffective against all our proposed
attacks, because: (1) Each service has vastly different pass-
word distributions—As shown in Fig. 3 of [30], every two
services share less than 40% of passwords, and thus no
pre-constructed blacklist will block the majority of popular
passwords; (2) Users tend to circumvent the blacklist [8],
[28], and new popular passwords will arise: if password is
blocked, password1 will arise; if password1 is blocked,
password123 (and p@ssword1) will arise; (3) Blacklist is
inherently unable to recognize/block PII-based passwords, and
thus is ineffective against the Type-A2 attacker.
Remark 4. Essentially, our two honeyword attacking strate-
gies are effective in distinguishing popular sweetwords from
unpopular ones. Thus, when more popular sweetwords are
more likely to be users’ passwords, our attacking strategies will
be effective. This indicates that they are particularly suitable
for attacking non-password-model based methods (e.g., Juels-
Rivest’s four methods) which generate a set of k − 1 honey-
words that are generally less probable than the user’s original
password. When password-model based methods are in place,
the main goal is to rank unpopular sweetwords, and other
attacking strategies shall be designed. In Sec. III-E, we will
show how to revise the “Normalized top-PW attack” strategy
for attacking password-model based methods.
C. Experimental setups
When evaluating a honeyword method, the success-number
graph is resulted from a run of Algorithm 1, while the ﬂatness
graph from a run of Algorithm 2. The differences in each
evaluation lie in how the function getSweetword(SWi) is
instantiated: the attacking strategy (i.e., Top-PW, Norm top-
PW, or Norm PW-model in Sec. IV), the training set and
the test set. For each attacking strategy, there are 7 different
password models (i.e., List, PCFG [32], Markov [24], their
targeted versions [30], and no-training-set) for possible choice.
In most of our experiments, we use the ﬁrst half of a
password dataset for training (e.g., 8.129 million Dodonew-tr),
and the second half for testing (e.g., 8.129 million Dodonew-
ts). Now we explain why. On the one hand, the effectiveness of
a machine-learning-based distinguishing algorithm depends on
two factors: the algorithm itself and the training sets used. By
randomly dividing a dataset into equally two parts, and using
part-1 for training and part-2 for testing, we can preclude the
impacts of training sets when evaluating an algorithm. On the
other hand, as unending lists of real-world password lists have
been disclosed (see [18], [20], [22]), the attacker can constantly
improve her training set to make it as close as possible to the
test set. For instance, the target system’s password distribution
can be largely approximated by a leaked site with the same
language, service type, password policy, etc. Actually, many
sites (e.g., Yahoo [18], Phpbb and Anthem [26]) have leaked
their user passwords more than once. Thus, we argue that when
evaluating a honeyword generation method, it is desirable to
employ a powerful yet realistic attacker and train the attacker
on a training set close to the test set.
This “half-half” practice does not violate the machine-
learning principle that the training set and test set shall be
different. It enables the attacker knows a large portion of the
knowledge of the target system’s password distribution, but
not exactly the full knowledge. The underlying reason is that,
according to the scale-free nature of Zipf’s law in passwords
[28], a non-negligible portion of passwords in “the ﬁrst half
of dataset for training” will not appear in “the second half for
testing”, making the training set and test set similar but not the
same. For instance, among the 5.59 million distinct passwords
in the test set Dodonew-ts, 4.54 million (81.22%) do not appear
in the training set Dodonew-tr. Actually, this practice is also
quite routine in password research (see [9], [30], [32]), but we
for the ﬁrst time explain why it is acceptable.
Besides experiments where the training set and test set stem
from the same original distribution, we also investigate the
impacts of varied training sets which come from different web
services in Sec. III-E. In addition, our non-train-set model
(see Sec. IV-A) does not employ any training set at all.
In our ﬁgures, generally, the lower the line, the better the
corresponding honeyword-method will be; Whenever a perfect
method line is presented, the closer to the perfect line, the
better the corresponding honeyword-method will be.
D. Our basic, trawling guessing attacks
In the above two attacking strategies, we instantiate the
probabilities of sweetwords for a given password ﬁle by using
the List password model PD(·) as deﬁned in Sec. III-B:
∀x∈ D, PD(x) = Count(x)
). That is, the probability of a
sweetword directly comes from the training set D. We call
|D|
it the basic, trawling guessing attack. In Sec. III-E, we will
instantiate the probabilities of sweetwords by using much more
sophisticated password models like PCFG and Markov.
Figs. 3(a)∼3(d) show that, in terms of the success-number
graph, the “norm Top-PW” attack strategy with smoothing
performs signiﬁcantly better than the no-smoothing version.
Generally, it also performs much better than the “Top-PW”
strategy, especially when the login number allowed is small.
This suggests the critical role of smoothing in attacking honey-
words. When trained on Dodonew-tr, it can tell at least 710930
(8.75%) real passwords apart from the 8,129,445 Dodonew-
ts accounts protected by any of the 4 methods in [21]. This
indicates that, in terms of the success-number metric, there
are over 1352=(710930/(T2/(k-1))) times of underestimation
of the vulnerabilities in Juels-Rivest’s methods.
As said earlier, when evaluating the ﬂatness security goal
of the methods in [21],
the two attacking strategies (see
Sec. III-B) will essentially try the same sequences of sweet-
words for each user account. Hence, they will produce the
same ﬂatness graph as shown in Fig. 3(e). Our results show that
all 4 methods provide very similar security levels of ﬂatness,
with the modeling syntax being slightly better. Their similarity
holds in all our later experiments, and due to space constraints,
hereafter we only take the tweaking-tail method as an example.
As shown in Figs. 3(a)∼3(d), no matter each user account
can be attacked 1, 3, or 10 times, both attacking strategies
perform rather stably. The underlying reasons are that: (1)
when an account can be attacked just once (i.e., T1=1),
both strategies can successfully identify at least 615,664 real
passwords from Dodonew-ts against every method in [21];
and (2) When an account can be attacked more than once
(i.e., T1>1), there are no more than T2 un-distinguished user
accounts involved in the attack, and thus A will crack at most
an additional T2 real passwords as compared to T1=1; and
(3) 615,664≫T2, thus 615,664+T2≈615, 664. Therefore, from
then on we only consider the case where T1=1 when evaluating
the success-number graphs. Note that, the setting of T1=1 has
no relevance to our evaluation of the ﬂatness graph where k
login attempts per user are always allowed.
Recall that the success-number metric measures a method’s
strength against the distinguishing attacker A in the worse-case
point of view, i.e., A ﬁrst attacks the weakest user accounts.
Take the case of “Top-PW: 1t” in Fig. 3(a) for example. Since
123456 is the top-1 password (1.44%) in Dodonew-tr, A will
ﬁrst use 123456 as the guess to test against all the 8,129,445
Dodonew-ts accounts. One might think that, 1.44% · 8,129,445
≈ 110K successful logins shall occur before the 1st failed
login occurs. However, this is not true, because 123456 can
also be a honeyword for user accounts that are with a real
password of the pattern 123xxx (e.g., 123123). Thus, A’s
login with 123456 may fail for such accounts before reaching
110K successful logins. Fig. 3(a) shows that, to reach 110K
successful logins, A fails about 100 times when using the
“Norm Top-PW with smoothing” strategy, and about 1000
times when using the “Top-PW” strategy.
In the above, we mainly used the Dodonew dataset
to
evaluate Juels-Rives’s methods. The evaluation using the other
9 datasets show similar results, and due to space constraints,
they are omitted here. The evaluation ﬁgures of three re-
8
(a) Attacks on the tweaking-tail method.
(b) Attacks on the modelling-syntax method.
(c) Attacks on the hybrid method.
(d) Attacks on the simple-model method.
(e) The ﬂatness graph of each method (k=20).
(f) Tweaking-tail: n=8,129,445, ϵ=0.3755.
Experiment results for attacking the four methods in [21] in terms of the success-number and ﬂatness metrics. Each method is
Fig. 3.
evaluated by two attacking strategies with various tunings, trained on 50% of Dodonew (i.e., Dodonew-tr) and tested on the remaining 50%
(i.e., Dodonew-ts). Whenever a perfect method line is presented, the closer to the perfect line, the better the corresponding honeyword method
will be. Sub-ﬁgures (a)∼(d) show that the “norm top-PW” attacking strategy with smoothing can distinguish 711K+ real PWs against every
method when allowed T2=104 honeyword logins, where 1t means T1=1, 3t means T1=3 and so on; Sub-ﬁgure (e) reveals that all 4 methods
are 0.35+-ﬂat, over 7 times weaker than expected in [21]. Sub-ﬁgure (f) exempliﬁes the correlations between the ﬂatness and success-number
metrics: (1) the upper-bound of success-number is x + n· ϵ, which means before the x-th failed login occurs, all the total Dododnew-ts accounts
(i.e., n=8,129,445) have already been tried (since x ≪ n∗ η, then x + n∗ η≈n∗ η, being a horizontal line); and (2) the lower-bound is
· x,
which means before the x-th failed login occurs, A has at least tried 1
1(cid:0)ϵ
· x accounts, and her success number is ϵ · ( 1
1(cid:0)ϵ
· x).