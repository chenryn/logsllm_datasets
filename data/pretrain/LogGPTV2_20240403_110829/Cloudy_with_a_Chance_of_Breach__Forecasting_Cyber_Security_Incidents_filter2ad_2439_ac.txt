USENIX Association  
24th USENIX Security Symposium  1015
1
F
D
C
0.5
1
1
1
1
F
D
C
0.5
Victim org.
Non−victim org.
F
D
C
0.5
Victim org.
Non−victim org.
F
D
C
0.5
Victim org.
Non−victim org.
F
D
C
0.5
Victim org.
Non−victim org.
Victim org.
Non−victim org.
0
0
0.5
% Untrusted HTTPS
1
0
0
0.2
% openresolver
0.4
0
0
0.5
% DNS random port
1
0
0
0.1
% Open SMTP Mail Relays
0.05
0
0
0.5
% BGP misconfig
1
Figure 2: Comparison of mismanagement symptoms between the victim and non-victim populations. There is a clear
separation under the ﬁrst two, while the other three appear to be much weaker predictors.
1
F
D
C
0.5
1
F
D
C
0.5
Victim org.
Non−victim org.
1
F
D
C
0.5
1
F
D
C
0.5
Victim org.
Non−victim org.
Victim org.
Non−victim org.
Victim org.
Non−victim org.
0
0
Un−normalized "bad" magnitude
200
400
0
0
1
Normalized "good" magnitude
0.5
0
0
10
20
"Bad" duration
30
0
0
0.5
"Bad" frequency
1
Figure 4: Proﬁle of selected temporal features extracted from the scanning time series over the period Nov. 13-Dec.13.
nization spends in a “bad” region could be indicative of
the delay in responding to an event, and similarly, how
frequent it re-enters a “bad” region could be indicative
of the effectiveness of the solutions taken in remaining
clean.
Accordingly, for each region we then measure the ave-
rage magnitude (both normalized by the total number of
IPs in an organization and unnormalized), the average
duration that the time series persists in that region upon
each entry (in days), and the frequency at which the time
series enters that region. This results in four summary
statistics for each region, thus 12 values for each time se-
ries. Since each organization has three time series, one
for each malicious activity type, we obtain a total of 36
derived features per organization i. These will be collec-
tively denoted by the feature vector Fi. Note that the set
of 36 values are collected from time series of a certain
duration. Here we further distinguish between statistics
extracted from a longer period of time vs. from a shorter,
most recent period of time. In this study we use two such
feature vectors, one referred to as Recent-60 features that
are collected over a period of 60 days (typically leading
up to the time of incident) and the other Recent-14 fea-
tures collected over a period of 14 days (leading up to the
time of incident).
To give a sense of why these features may be expected
to hold predictive power, we similarly compare the dis-
tribution of these feature values among the victim and
non-victim populations. Fig. 4 shows this comparison
for four examples: un-normalized magnitude in a bad
period, normalized magnitude in a good period, average
duration during bad periods, and the frequency of enter-
ing a bad period. We see that in each case there is a clear
difference between the two populations in how these fea-
ture values are distributed, e.g., victim organizations tend
to have longer bad periods, indicative of slow response
time, and also higher bad/good magnitudes, etc. As we
discuss further in Section 4.4, these features have varying
degrees of inﬂuence over the prediction outcome.
3.2 Training and Testing Procedure
We now describe the construction of the predictor using
the set of features deﬁned above. This consists of a train-
ing step and a testing step. The training step uses the
following two sets of subjects.
A subset of incident or victim organizations. This will
be referred to as Group(1) or the incident group. De-
pending on the experiments, this subset may be selected
from one of the three incident datasets (if we train the
classiﬁer and conduct testing based solely on one inci-
dent dataset), or from the union of all three. This subset
is selected based on the time stamps of the reported in-
cidents, and its size is determined by a training-testing
ratio, e.g., 70-30 split or 50-50 split of the given dataset.
If we use a 50-50 split, it means that we select the ﬁrst
half (in terms of time of occurrence) of the incidents as
Group(1); a 70-30 split means using the ﬁrst 70% of in-
cidents as Group(1). The remaining victim organizations
are used in the testing step.
A randomly selected set of non-victim organizations
(with size comparable to that of Group(1) in any given
experiment). These are taken from the global table des-
cribed in Section 2.3.2. This will be referred to as
1016  24th USENIX Security Symposium 
USENIX Association
Group(0), or the non-incident group. As mentioned ear-
lier, since there are close to three million non-victim
organizations compared to less than a thousand victim
organizations, the random sub-sampling is necessary to
avoid the common problem of imbalance in the machine
learning literature2; this issue has also been discussed in
[51]. This random selection of non-victim organizations
is repeated numerous times, each time training a diffe-
rent classiﬁer. The reported testing results are averages
over all these versions.
i
,rPH
,rSC
i
For a victim organization i in Group(1), its complete
feature set xi includes the mismanagement symptoms mi,
the three time series rSP
over the two months
i
prior to the month in which the incident in i occurred3,
secondary features Fi collected over the same time period
as the time series, namely Recent-60, and that collected
over the two weeks prior to the month of the incident
occurrence, namely Recent-14. Each such feature set is
associated with the label (or ground-truth or group infor-
mation in machine learning) Li = 1 for incident. For a
non-victim organization j in Group(0), its complete fea-
ture set x j consists of exactly the same components listed
above, with the only difference that the time series and
the secondary features are for the two months prior to
the month of the ﬁrst incident in Group(1). It is also as-
sociated with the label L j = 0 for non-incident.
The collections of {[xi,Li]} and {[x j,L j]} constitute
the training data used to train the classiﬁer. The testing
step then uses the following two inputs: (1) The subset
of victim organizations not included in Group(1); denote
this group by Group(1c). (2) A randomly selected set of
non-victim organizations not used in training. Unlike in
training where we try to keep a balance between the vic-
tim and non-victim sets, during testing we use a much
larger set of non-victim organizations to better character-
ize the classiﬁer performance.
For these two subjects their complete feature sets
xi are obtained in exactly the same way as for those
used in training. For the non-victim organizations se-
lected for testing, the features are collected over the two
months prior to the incident month of the ﬁrst incident
in Group(1c). For the victim organization used for test-
ing we further consider two scenarios. In the short-term
forecast scenario, we collect these features over the two
months prior to the incident month for an organization in
Group(1c), while in the long-term forecast scenario, we
collect these features over the two months prior to the
incident month of the ﬁrst incident in Group(1c). In the
2If we use all three million non-victims in training, the resulting
classiﬁer will simply label all of them as non-victims, and achieve per-
formance very close to 100% overall. But clearly this classiﬁer would
be of little use, as it will also have 0 true positive probability.
3Most incident occurrences in our dataset are timestamped with
month and year information.
short-term forecast scenario, since in each incident test
case the incident occurred within a month of collecting
the features, the TP rate is essentially for a forecasting
window of one month.
In the long-term forecast sce-
nario, an incident may occur months after collecting the
features (up to 12 months in the case of VCDB), thus the
TP rate is for a forecasting window of up to a year. Note
that the short-term forecast can be repeatedly done over
time to produce prediction for the immediate future. The
differences between these two forecast schemes are also
illustrated in Fig. 5.
Recent−60 features (60 days) :
Recent−14 features (14 days) :
Training
Timeline
USI Affinity
Nationalist Move. web
Funding site of Kickstarters 
attacked
Short Term Prediction
Long Term Prediction
MOE web hacked
Web of Russian state
TV attacked
MOE web hacked
Web of Russian state
TV attacked
Aug. 13 Sep. 13 Oct. 13 Nov. 13 Dec. 13 Jan. 14 Feb. 14 Mar. 14
Figure 5: Feature extraction, short-term and long-term
forecasting. In training, features are extracted from the
most recent period leading up to an incident. In testing,
the same is done when we perform short-term forecast.
In long-term forecast, features are extracted from periods
leading up to the time of the ﬁrst incident used in testing.
These inputs are then fed into the classiﬁer to produce
a label (or prediction). The output of Random Forest is
actually a risk probability; a threshold is then imposed to
obtain a binary label. For instance, if we set the thresh-
old at 0.5, then all output > 0.5 means a label of 1. By
moving this threshold we obtain different prediction per-
formances, which constitute a ROC curve.
4
Incident Prediction
In this section, we present our main prediction results
and investigate their various implications.
4.1 Main Results
Using the methodology outlined in the previous section,
we performed prediction using the three incident datasets
separately, as well as collectively. When used collec-
tively, we removed duplicate reports of the same inci-
dent whenever applicable. The separation between train-
ing and testing for each dataset is done chronologically,
as shown in Table 4. For each dataset, these separations
USENIX Association  
24th USENIX Security Symposium  1017
result in an approximate 50-50 split of the victim set be-
tween the training and testing sample sizes. In addition,
for each test we randomly sample non-victim test cases
from the non-victim organization set.
Training
Testing
Hackmageddon
Oct 13 – Dec 13
Jan 14 – Feb 14
VCDB
WHID
Aug 13 – Dec 13
Jan 14 – Dec 14
Jan 14 – Mar 14
Apr 14 – Nov 14
Table 4: Chronological separation between training and
testing samples for each incident dataset;
the split is
roughly 50-50 among the victim population.
There is one point worth clarifying. When processing
non-sequential data, the split of samples for the purpose
of training and testing is often done randomly in the ma-
chine learning literature. In our context this would mean
to choose a later incident for training and use an earlier
incident for testing. Due to the sequential nature of our
data, we intentionally and strictly split the data by time:
earlier ones are for training and later ones for testing.
Because of this, our testing results are indeed “predic-
tion” results; for the same reason, we did not set aside a
third, separate dataset for the purpose of “more testing”
as is sometimes done in the literature, as this purpose is
already served by the second, test dataset.
The prediction results are summarized in the set of
ROC (receiver operating characteristic) curves shown in
Fig. 6. Recall that the RF classiﬁer outputs a probabi-
lity of incident for each input sample. To test its accu-
racy, a threshold is adopted that maps this value into a
binary prediction: 1 if it exceeds the threshold and 0 oth-
erwise. This binary prediction is then compared against
the ground-truth: a sample from an incident dataset has
a true label of 1, while a sample from the non-victim or-
ganization set has a true label of 0. Since our non-victim
set for training (to balance) is randomly selected from the
total non-victim population, the above test is repeated 20
times for a given threshold value, each time for a differ-
ent random non-victim set. The average TP and FP over
these repeated tests form one point on the ROC curve.
We see the prediction performance varies slightly be-
tween the datasets, but remain very satisfactory, gene-
rally achieving combined (TP, FP) values of (90%,10%)
or (80%,5%). In particular, when we combine the three
datasets, we can achieve an accuracy level of (88%,4%).
A summary of some of the most desirable operating
points are given in Table 5.
The above prediction results substantially outperform
what has been shown in the literature to date; e.g., the
web maliciousness prediction study in [51] reported a
combination of (66%,17%) for (TP, FP). It is also worth
pointing out that TP and FP values are independent of the
sizes of the respective populations of the victim and non-
e
v
i
t
i
s
o
p
1
0.9
0.8
0.7
0.6
0.5
0.4
e
u
r
T
VCDB
Hackmageddon
WHID
ALL
0.1
0.2
0.3
False positive
0.4
0.5
Figure 6: Prediction results. There are variations be-
tween the datasets, but an operating point – combined
(TP, FP) values – of (90%,10%) or (80%,5%) is achiev-
able.
In particular, when we use all three datasets to-
gether, we can achieve an accuracy level of (88%,4%).
Accuracy
Hackmageddon
True Positive (TP)
False Positive (FP)
False Negative (FN)
Overall Accuracy
96%
10%
4%
90%
VCDB WHID
80%
88%
5%