ter 14), but doing so for the former is overkill. Assessing an issue’s severity requires
an exercise of good engineering judgment and, often, a degree of calm under
pressure.
Your first response in a major outage may be to start troubleshooting and try to find a
root cause as quickly as possible. Ignore that instinct!
Instead, your course of action should be to make the system work as well as it can
under the circumstances. This may entail emergency options, such as diverting traffic
from a broken cluster to others that are still working, dropping traffic wholesale to
prevent a cascading failure, or disabling subsystems to lighten the load. Stopping the
bleeding should be your first priority; you aren’t helping your users if the system dies
while you’re root-causing. Of course, an emphasis on rapid triage doesn’t preclude
taking steps to preserve evidence of what’s going wrong, such as logs, to help with
subsequent root-cause analysis.
Novice pilots are taught that their first responsibility in an emergency is to fly the air‐
plane [Gaw09]; troubleshooting is secondary to getting the plane and everyone on it
safely onto the ground. This approach is also applicable to computer systems: for
example, if a bug is leading to possibly unrecoverable data corruption, freezing the
system to prevent further failure may be better than letting this behavior continue.
This realization is often quite unsettling and counterintuitive for new SREs, particu‐
larly those whose prior experience was in product development organizations.
In Practice | 137
Examine
We need to be able to examine what each component in the system is doing in order
to understand whether or not it’s behaving correctly.
Ideally, a monitoring system is recording metrics for your system as discussed in
Chapter 10. These metrics are a good place to start figuring out what’s wrong. Graph‐
ing time-series and operations on time-series can be an effective way to understand
the behavior of specific pieces of a system and find correlations that might suggest
where problems began.9
Logging is another invaluable tool. Exporting information about each operation and
about system state makes it possible to understand exactly what a process was doing
at a given point in time. You may need to analyze system logs across one or many
processes. Tracing requests through the whole stack using tools such as Dapper
[Sig10] provides a very powerful way to understand how a distributed system is
working, though varying use cases imply significantly different tracing designs
[Sam14].
Logging
Text logs are very helpful for reactive debugging in real time, while storing logs in a
structured binary format can make it possible to build tools to conduct retrospective
analysis with much more information.
It’s really useful to have multiple verbosity levels available, along with a way to
increase these levels on the fly. This functionality enables you to examine any or all
operations in incredible detail without having to restart your process, while still
allowing you to dial back the verbosity levels when your service is operating normally.
Depending of the volume of traffic your service receives, it might be better to use stat‐
istical sampling; for example, you might show one out of every 1,000 operations.
A next step is to include a selection language so that you can say “show me operations
that match X,” for a wide range of X—e.g., Set RPCs with a payload size below 1,024
bytes, or operations that took longer than 10 ms to return, or which called doSome
thingInteresting() in rpc_handler.py. You might even want to design your logging
infrastructure so that you can turn it on as needed, quickly and selectively.
Exposing current state is the third trick in our toolbox. For example, Google servers
have endpoints that show a sample of RPCs recently sent or received, so it’s possible
to understand how any one server is communicating with others without referencing
9 But beware false correlations that can lead you down wrong paths!
138 | Chapter 12: Effective Troubleshooting
an architecture diagram. These endpoints also show histograms of error rates and
latency for each type of RPC, so that it’s possible to quickly tell what’s unhealthy.
Some systems have endpoints that show their current configuration or allow exami‐
nation of their data; for instance, Google’s Borgmon servers (Chapter 10) can show
the monitoring rules they’re using, and even allow tracing a particular computation
step-by-step to the source metrics from which a value is derived.
Finally, you may even need to instrument a client to experiment with, in order to dis‐
cover what a component is returning in response to requests.
Debugging Shakespeare
Using the link to the black-box monitoring results in the bug, you discover that the
prober sends an HTTP GET request to the /api/search endpoint:
{
‘search_text’: ‘the forms of things unknown’
}
It expects to receive a response with an HTTP 200 response code and a JSON payload
exactly matching:
[{
"work": "A Midsummer Night's Dream",
"act": 5,
"scene": 1,
"line": 2526,
"speaker": "Theseus"
}]
The system is set up to send a probe once a minute; over the past 10 minutes, about
half the probes have succeeded, though with no discernible pattern. Unfortunately,
the prober doesn’t show you what was returned when it failed; you make a note to fix
that for the future.
Using curl, you manually send requests to the search endpoint and get a failed
response with HTTP response code 502 (Bad Gateway) and no payload. It has an
HTTP header, X-Request-Trace, which lists the addresses of the backend servers
responsible for responding to that request. With this information, you can now exam‐
ine those backends to test whether they’re responding appropriately.
Diagnose
A thorough understanding of the system’s design is decidedly helpful for coming up
with plausible hypotheses about what’s gone wrong, but there are also some generic
practices that will help even without domain knowledge.
In Practice | 139
Simplify and reduce
Ideally, components in a system have well-defined interfaces and perform known
transformations from their input to their output (in our example, given an input
search text, a component might return output containing possible matches). It’s then
possible to look at the connections between components—or, equivalently, at the data
flowing between them—to determine whether a given component is working prop‐
erly. Injecting known test data in order to check that the resulting output is expected
(a form of black-box testing) at each step can be especially effective, as can injecting
data intended to probe possible causes of errors. Having a solid reproducible test case
makes debugging much faster, and it may be possible to use the case in a non-
production environment where more invasive or riskier techniques are available than
would be possible in production.
Dividing and conquering is a very useful general-purpose solution technique. In a
multilayer system where work happens throughout a stack of components, it’s often
best to start systematically from one end of the stack and work toward the other end,
examining each component in turn. This strategy is also well-suited for use with data
processing pipelines. In exceptionally large systems, proceeding linearly may be too
slow; an alternative, bisection, splits the system in half and examines the communica‐
tion paths between components on one side and the other. After determining
whether one half seems to be working properly, repeat the process until you’re left
with a possibly faulty component.
Ask “what,” “where,” and “why”
A malfunctioning system is often still trying to do something—just not the thing you
want it to be doing. Finding out what it’s doing, then asking why it’s doing that and
where its resources are being used or where its output is going can help you under‐
stand how things have gone wrong.10
10 In many respects, this is similar to the “Five Whys” technique [Ohn88] introduced by Taiichi Ohno to under‐
stand the root causes of manufacturing errors.
140 | Chapter 12: Effective Troubleshooting
Unpacking the Causes of a Symptom
Symptom: A Spanner cluster has high latency and RPCs to its servers are timing out.
Why? The Spanner server tasks are using all their CPU time and can’t make progress
on all the requests the clients send.
Where in the server is the CPU time being used? Profiling the server shows it’s sort‐
ing entries in logs checkpointed to disk.
Where in the log-sorting code is it being used? When evaluating a regular expression
against paths to log files.
Solutions: Rewrite the regular expression to avoid backtracking. Look in the code‐
base for similar patterns. Consider using RE2, which does not backtrack and guaran‐
tees linear runtime growth with input size.11
What touched it last
Systems have inertia: we’ve found that a working computer system tends to remain in
motion until acted upon by an external force, such as a configuration change or a
shift in the type of load served. Recent changes to a system can be a productive place
to start identifying what’s going wrong.12
Well-designed systems should have extensive production logging to track new ver‐
sion deployments and configuration changes at all layers of the stack, from the server
binaries handling user traffic down to the packages installed on individual nodes in
the cluster. Correlating changes in a system’s performance and behavior with other
events in the system and environment can also be helpful in constructing monitoring
dashboards; for example, you might annotate a graph showing the system’s error rates
with the start and end times of a deployment of a new version, as seen in Figure 12-2.
11 In contrast to RE2, PCRE can require exponential time to evaluate some regular expressions. RE2 is available
at https://github.com/google/re2.
12 [All15] observes this is a frequently used heuristic in resolving outages.
In Practice | 141
Figure 12-2. Error rates graphed against deployment start and end times
Manually sending a request to the /api/search endpoint (see “Debugging Shake‐
speare” on page 139) and seeing the failure listing backend servers that handled the
response lets you discount the likelihood that the problem is with the API frontend
server and with the load balancers: the response probably wouldn’t have included that
information if the request hadn’t at least made it to the search backends and failed
there. Now you can focus your efforts on the backends—analyzing their logs, sending
test queries to see what responses they return, and examining their exported metrics.
Specific diagnoses
While the generic tools described previously are helpful across a broad range of prob‐
lem domains, you will likely find it helpful to build tools and systems to help with
diagnosing your particular services. Google SREs spend much of their time building
such tools. While many of these tools are necessarily specific to a given system, be
sure to look for commonalities between services and teams to avoid duplicating
effort.
Test and Treat
Once you’ve come up with a short list of possible causes, it’s time to try to find which
factor is at the root of the actual problem. Using the experimental method, we can try
to rule in or rule out our hypotheses. For instance, suppose we think a problem is
caused by either a network failure between an application logic server and a database
server, or by the database refusing connections. Trying to connect to the database
with the same credentials the application logic server uses can refute the second
hypothesis, while pinging the database server may be able to refute the first, depend‐
142 | Chapter 12: Effective Troubleshooting
ing on network topology, firewall rules, and other factors. Following the code and
trying to imitate the code flow, step-by-step, may point to exactly what’s going wrong.
There are a number of considerations to keep in mind when designing tests (which
may be as simple as sending a ping or as complicated as removing traffic from a clus‐
ter and injecting specially formed requests to find a race condition):
• An ideal test should have mutually exclusive alternatives, so that it can rule one
group of hypotheses in and rule another set out. In practice, this may be difficult
to achieve.
• Consider the obvious first: perform the tests in decreasing order of likelihood,
considering possible risks to the system from the test. It probably makes more
sense to test for network connectivity problems between two machines before
looking into whether a recent configuration change removed a user’s access to
the second machine.
• An experiment may provide misleading results due to confounding factors. For
example, a firewall rule might permit access only from a specific IP address,
which might make pinging the database from your workstation fail, even if ping‐
ing from the application logic server’s machine would have succeeded.
• Active tests may have side effects that change future test results. For instance,
allowing a process to use more CPUs may make operations faster, but might
increase the likelihood of encountering data races. Similarly, turning on verbose
logging might make a latency problem even worse and confuse your results: is
the problem getting worse on its own, or because of the logging?
• Some tests may not be definitive, only suggestive. It can be very difficult to make
race conditions or deadlocks happen in a timely and reproducible manner, so
you may have to settle for less certain evidence that these are the causes.
Take clear notes of what ideas you had, which tests you ran, and the results you saw.13
Particularly when you are dealing with more complicated and drawn-out cases, this
documentation may be crucial in helping you remember exactly what happened and
prevent having to repeat these steps.14 If you performed active testing by changing a
system—for instance by giving more resources to a process—making changes in a
systematic and documented fashion will help you return the system to its pre-test
setup, rather than running in an unknown hodge-podge configuration.
13 Using a shared document or real-time chat for notes provides a timestamp of when you did something, which
is helpful for postmortems. It also shares that information with others, so they’re up to speed with the current
state of the world and don’t need to interrupt your troubleshooting.
14 See also “Negative Results Are Magic” on page 144 for more on this point.
In Practice | 143
Negative Results Are Magic
Written by Randall Bosetti
Edited by Joan Wendt
A “negative” result is an experimental outcome in which the expected effect is absent
—that is, any experiment that doesn’t work out as planned. This includes new
designs, heuristics, or human processes that fail to improve upon the systems they
replace.
Negative results should not be ignored or discounted. Realizing you’re wrong has
much value: a clear negative result can resolve some of the hardest design questions.
Often a team has two seemingly reasonable designs but progress in one direction has
to address vague and speculative questions about whether the other direction might
be better.
Experiments with negative results are conclusive. They tell us something certain
about production, or the design space, or the performance limits of an existing sys‐
tem. They can help others determine whether their own experiments or designs are
worthwhile. For example, a given development team might decide against using a
particular web server because it can handle only ~800 connections out of the needed
8,000 connections before failing due to lock contention. When a subsequent develop‐
ment team decides to evaluate web servers, instead of starting from scratch, they can
use this already well-documented negative result as a starting point to decide quickly
whether (a) they need fewer than 800 connections or (b) the lock contention prob‐
lems have been resolved.
Even when negative results do not apply directly to someone else’s experiment, the
supplementary data gathered can help others choose new experiments or avoid pit‐
falls in previous designs. Microbenchmarks, documented antipatterns, and project
postmortems all fit this category. You should consider the scope of the negative result
when designing an experiment, because a broad or especially robust negative result
will help your peers even more.
Tools and methods can outlive the experiment and inform future work. As an
example, benchmarking tools and load generators can result just as easily from a dis‐
confirming experiment as a supporting one. Many webmasters have benefited from
the difficult, detail-oriented work that produced Apache Bench, a web server loadtest,
even though its first results were likely disappointing.
Building tools for repeatable experiments can have indirect benefits as well: although
one application you build may not benefit from having its database on SSDs or from
creating indices for dense keys, the next one just might. Writing a script that allows
you to easily try out these configuration changes ensures you don’t forget or miss
optimizations in your next project.
144 | Chapter 12: Effective Troubleshooting
Publishing negative results improves our industry’s data-driven culture. Account‐
ing for negative results and statistical insignificance reduces the bias in our metrics
and provides an example to others of how to maturely accept uncertainty. By publish‐
ing everything, you encourage others to do the same, and everyone in the industry
collectively learns much more quickly. SRE has already learned this lesson with high-
quality postmortems, which have had a large positive effect on production stability.
Publish your results. If you are interested in an experiment’s results, there’s a good
chance that other people are as well. When you publish the results, those people do
not have to design and run a similar experiment themselves. It’s tempting and com‐
mon to avoid reporting negative results because it’s easy to perceive that the experi‐
ment “failed.” Some experiments are doomed, and they tend to be caught by review.
Many more experiments are simply unreported because people mistakenly believe
that negative results are not progress.
Do your part by telling everyone about the designs, algorithms, and team workflows
you’ve ruled out. Encourage your peers by recognizing that negative results are part
of thoughtful risk taking and that every well-designed experiment has merit. Be skep‐
tical of any design document, performance review, or essay that doesn’t mention fail‐
ure. Such a document is potentially either too heavily filtered, or the author was not
rigorous in his or her methods.
Above all, publish the results you find surprising so that others—including your
future self—aren’t surprised.
Cure
Ideally, you’ve now narrowed the set of possible causes to one. Next, we’d like to prove
that it’s the actual cause. Definitively proving that a given factor caused a problem—