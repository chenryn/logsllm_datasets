do W M CS.add node(n) and wn := 1
∀e ∈ edges(Gi) and e (cid:54)∈ edges(mcs),
do W M CS.add edge(e) and we := 1
9:
10: end for
11: return W M CS
nodes and edges in Gi that are not part of the mcs are
added to the WMCS, and their weight is set to 1.
To increase the generality of a template, the labels of
optional nodes are further relaxed. More precisely, our
system preserves the label that stores the name of the
system call. However, all additional information is re-
placed by a wild card that matches every possible, con-
crete parameter later. Finally, we remove all templates
with a core of three or fewer nodes. The reason is that
these templates are likely too small to accurately capture
the entire malicious activity and might lead to false pos-
itives. In the example in Figure 3, core nodes and edges
are shown as dark elements, while the optional elements
are white. We generate one C&C template per cluster.
3.6 Template Matching
The previous steps leveraged machine learning tech-
niques and sets of known malicious and benign graphs to
produce a number of C&C templates. These templates
are graphs that represent host-based activity that is re-
lated to command and control trafﬁc. To ﬁnd the C&C
connections for a new malware sample, this sample is
ﬁrst executed in the sandbox, and our system extracts its
behavior graphs (as discussed in Sections 3.1 and 3.2).
Then, we match all C&C templates against the behavior
graphs. Whenever a match is found, the corresponding
connection is detected as command and control trafﬁc,
and we can extract its endpoints (IPs and domains).
The matching technique is described in Algorithm 2.
In a ﬁrst step, we attempt to determine whether the core
of a template T is present in the behavior graph G. To
this end, we simply use a subgraph isomorphism test.
When the test fails, we know that the core nodes of T
are not part of the graph, and we can advance to trying
the next template.
If the core is found, we obtain the
mapping from the core nodes to the corresponding nodes
in G. We then test the optional nodes. To this end, we
compute the mcs between T and G. For this, the ﬁxed
mapping provided by the previous isomorphism test is
used to initialize the space exploration when building the
Algorithm 2 Template matching
Require: A behavior graph G, A template T
1: map ← subgraph isomorphism(core(T ), G)
2: if map = (cid:11) then
return f alse
3:
4: end if
5: s := state exploration(map)
6: mcs ← maximum common subgraph(G, T, s)
7: return true, mcs
mcs, signiﬁcantly speeding up the process. Based on the
result of the mcs computation, we can directly see how
many optional nodes have matched, that is to say, are
covered by the mcs. Taking into account the fraction (or
the absolute number) of optional nodes that are found in
G, we can declare a template match.
4 Evaluation
Experiments were performed to evaluate JACKSTRAWS
both from a quantitative and qualitative perspective.
This section describes the evaluation details and results.
4.1 Evaluation Datasets
For the evaluation, our system analyzed a total of 37,572
malware samples. The samples were provided to us by
a network security company, who obtained the binaries
from recent submission to a public malware analysis
sandbox. Moreover, we were only given samples that
showed some kind of network activity when run in the
sandbox. We were also provided with a set of 385 sig-
natures speciﬁcally for known C&C trafﬁc, as well as
162 signatures that characterize known, benign trafﬁc.
As mentioned previously, the company uses signatures
for benign trafﬁc to be able to quickly discard harmless
connections that bots frequently make.
To make sure that our sample set covers a wide vari-
ety of different malware families, we labeled the entire
set with six different anti-virus engines: Kaspersky, F-
Secure, BitDefender, McAfee, NOD32, and F-Prot. Us-
ing several sources for labeling allows us reduce the pos-
sible limitations of a single engine. For every malware
sample, each engine returns a label (unless the samples
is considered benign) from which we extract the mal-
ware family substring. For instance, if one anti-virus
engine classiﬁes a sample as Win32.Koobface.AZ, then
Koobface is extracted as the family name. The family
that is returned by a majority of the engines is used to
label a sample. In case the engines do not agree (and
there is no majority for a label), we go through the out-
put of the AV tools in the order that they were mentioned
previously and pick the ﬁrst, non-benign result.
Overall, we identiﬁed 745 different malware families
for the entire set. The most prevalent families were
Generic (3756), EgroupDial (2009), Hotbar (1913),
Palevo (1556), and Virut (1539). 4,096 samples re-
mained without label. Note that Generic is not a precise
label; many different kinds of malware can be classiﬁed
as such by AV engines. In summary, the results indicate
that our sample set has no signiﬁcant bias towards a cer-
tain malware family. As expected, it covers a rich and
diverse set of malware, currently active in the wild.
In a ﬁrst step, we executed all samples in JACK-
STRAWS. Each sample was executed for four minutes,
which allows a sample to initialize and perform its nor-
mal operations. This timeout is typically enough to
establish several network connections and send/receive
data via them. The execution of the 37,572 samples
produced 150,030 network connections, each associated
with a behavior graph. From these graphs, we removed
19,395 connections in which the server responded with
an error (e.g., an HTTP request with a 404 “Not Found”
response). Thus, we used a total of 130,635 graphs pro-
duced by a total of 33,572 samples for the evaluation.
In the next step, we applied our signatures to the
network connections. This resulted in 16,535 connec-
tions that were labeled as malicious (known C&C trafﬁc,
12.7%) and 16,082 connections that were identiﬁed as
benign (12.3%). The malicious connections were pro-
duced by 9,108 samples, while the benign connections
correspond to 7,031 samples. The remaining 98,018
connections (75.0%) are unknown. The large fraction of
unknown connections is an indicator that it is very dif-
ﬁcult to develop a comprehensive set of signatures that
cover the majority of bot-related C&C trafﬁc. In partic-
ular, there was at least one unclassiﬁed connection for
31,671 samples. Note that the numbers of samples that
produced malicious, benign, and unknown trafﬁc add up
to more than the total number of samples. This is be-
cause some samples produced both malicious and be-
nign connections. This underlines that it is difﬁcult to
pick the important C&C connections among bot trafﬁc.
Of course, not all of the 385 malicious signatures pro-
duced matches. In fact, we observed only hits from 78
C&C signatures, and they were not evenly distributed.
A closer examination revealed that the signature that
matched the most number of network connections is re-
lated to Palevo (4,583 matches), followed by Ramnit
(3,896 matches) and Koobface (2,690 matches).
4.2 Template Generation
Initially, we put all 16,535 behavior graphs that corre-
spond to known C&C connections into the malicious
graphs set, while the 16,082 graphs corresponding to be-
nign connections were added to the benign graphs set.
To improve the quality of these sets, we removed graphs
that contained too few nodes, as well as graphs that
contained only nodes that correspond to network-related
system calls (and a few other house-keeping functions
that are not security-relevant). Moreover, to maintain
a balanced training set, we kept at most three graphs
(connections) for each distinct malware sample. This
pre-processing step reduced the number of graphs in the
malicious set to 10,801, and to 12,367 in the benign set.
Both sets were then further split into a training set and
a test set. To this end, we randomly picked a number
of graphs for the training set, while the remaining ones
were set aside as a test set. More precisely, for the mali-
cious graphs, we kept 6,539 graphs (60.5%) for training
and put 4,262 graphs (39.5%) into the test set. For the
benign graphs, we kept 8,267 graphs (66.8%) for train-
ing and put 4,100 graphs (33.2%) into the test set. We
used these malicious and benign training sets as input
for our template generation algorithm. This resulted in
417 C&C templates that JACKSTRAWS produced. The
average number of nodes in a template was 11, where 6
nodes were part of the core and 5 were optional.
For the mining process, we used a threshold k = 0.1.
That is, the mining tool will pick subgraphs from the
training sets only when they appear in more than 10%
of all behavior graphs. The reason why we could oper-
ate with a relatively large threshold of k = 0.1 is that
we divided the behavior graphs into different bins, and
mined on each bin individually. To divide graphs into
bins, we observe that certain malware activity requires
the execution of a particular set of system calls. For ex-
ample, to start a new process, the malware needs to call
NtCreateProcess, or to write to a ﬁle, it needs to
call NtWriteFile. Thus, we selected ﬁve security-
relevant system activities (registry access; ﬁle system
access; process creation; queries to system information;
and accesses to web-related resources, such as HTML or
JS ﬁles) and assigned each to a different bin. Then, we
put into each bin all behavior graphs that contain a node
with the corresponding activity (system calls). Graphs
that did not fall into any of these bins were gathered in
a miscellaneous bin. It is important to observe that this
step merely allows us to mine with a higher threshold,
and thus to accelerate the graph mining process consid-
erably. We would have obtained the same set of tem-
plates (and possibly more) when mining on the entire
training set with a lower mining threshold.
For the clustering process, we iterated the bisection
operation until the average similarity within the clus-
ters was over 60% and the minimal similarity was over
40%. Higher thresholds were discarded because they in-
creased the number of clusters, making them too spe-
ciﬁc.
Producing templates for the 14,806 graphs in the
training set took about 21 hours on an Intel Xeon 4
CPUs 2.67GHz server, equipped with 16GB of RAM.
This time was divided into 16 hours for graph mining,
4.5 hours for clustering, and 30 minutes for graph gen-
eralization. This underlines that, despite the potentially
costly (NP-hard) graph algorithms, JACKSTRAWS is able
to efﬁciently produce results on a large, real-world in-
put dataset. The mining process was the most time-
consuming operation, but the number of mined sub-
graphs was, in the end, ﬁve times smaller than the num-
ber of graphs in input. Consequently, the clustering pro-
cess, which is polynomial in function of the number of
graphs in input, ran on a reduced set. For the template
generation process, the resulting clusters only contained
10 to 20 graphs on average, explaining the faster com-
putations.
4.3 Detection Accuracy
In the next step, we wanted to assess whether the gen-
erated templates can accurately detect activity related
to command and control trafﬁc without matching be-
nign connections. To this end, we ran two experiments.
First, we evaluated the templates on the graphs in the
test set (which correspond to known C&C connections).
Then, we applied the templates to graphs associated
with unknown connections. This allows us to deter-
mine whether the extracted C&C templates are generic
enough to allow detection of previously-unknown C&C
trafﬁc (for which no signature exists).
Experiment 1: Known C&C connections. For the ﬁrst
experiment, we made use of the test set that was pre-
viously set aside. More precisely, we applied our 417
templates to the behavior graphs in the test set. This test
set contained 4,262 connections that matched C&C sig-
natures and 8,267 benign connections.
Our results show that JACKSTRAWS is able to success-
fully detect 3,476 of the 4,262 malicious connections
(81.6%) as command and control trafﬁc. Interestingly,
the test set also contained malware families that were
absent from the malicious training set. 51.7% of the
malicious connections coming from these families were
successfully detected, accounting for 0.4% of all detec-
tions. While the detection accuracy is high, we explored
false negatives (i.e., missed detections) in more detail.
Overall, we found three reasons why certain connections
were not correctly identiﬁed:
First, in about half of the cases, detection failed be-
cause the bot did not complete its malicious action after
it received data from the C&C server. Incomplete be-
havior graphs can be due to a timeout of the dynamic
analysis environment, or an invalid conﬁguration of the
host to execute the received command properly.
Second, the test set contained a signiﬁcant number of
Adware samples. The behavior graphs extracted from
these samples are very similar to benign graphs; after
all, Adware is in a grey area different from malicious
bots. Thus, all graphs potentially covering these sam-
ples are removed at the end of the mining process, when
compared to the benign training sets.
The third reason for missed detections are malicious
connections that are only seen a few times (possibly only
in the test set). According to the AV labels, our data
set covers 745 families (and an additional 4,096 samples
that could not be labeled). Thus, certain families are rare
in the data set. When a speciﬁc graph is only present a
few times (or not at all) in the training set, it is possible
that all of its subgraphs are below the mining threshold.
In this case, we do not have a template that covers this
activity.
JACKSTRAWS also reported 7 benign graphs as ma-
licious out of 4,100 connections in the benign test set:
a false positive rate of 0.2%. Upon closer examination,
these false positives correspond to large graphs where
some Internet caching activity is observed. These graphs
accidentally triggered four weaker templates with few
core and many optional nodes.
Overall, our results demonstrate that the host-based
activity learned from a set of known C&C connections is