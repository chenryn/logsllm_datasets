serve tail improvement with load due to warming effects in client
CPUs. 1RMA benefits from an all-hardware serving path, whereas
software eventually bottlenecks Pony in this workload. The zero-
load mark also highlights the effects of 1RMA design choices for
command issue. Because 1RMA uses MMIO, it skips several PCIe
RTTs (~400ns each) that software solutions built on UD [21] incur
on the critical path (see Appendix B).
To demonstrate how 1RMAâ€™s fail-fast behavior can support fast
application-level failure response, Figure 9 reflects the system under
duress by injecting a failure into one of the KVCS server replicas,
which we label R2. To KVCS, this manifests as an immediate op
failure (i.e., not delayed by even a network transport timeout). In re-
sponse, KVCS immediately shifts load to replica R1. Increased load
716
0102030405060708090100O(cid:27)eredLoad(Gbps)02040KVCSHitLatency(us)1RMA-50p1RMA-99pPony-50pPony-99pR2 FailureSIGCOMM â€™20, August 10â€“14, 2020, Virtual Event, NY, USA
A. Singhvi et al.
Figure 10: Latency vs. offered load for a uniform random traffic
pattern.
(a)
(b)
Figure 11: (a) Achieved throughput vs. offered load for reads
and writes; (b) Load bounding via command slot allocation.
Figure 12: Impact of head-of-line blocking for a point-to-point
workload.
Figure 13: Impact of head-of-line blocking for a uniform ran-
dom traffic pattern.
causes growth in latencies of ops serviced by R1 as it absorbs the new
traffic, stabilizing in 2ms unavailability1RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters
SIGCOMM â€™20, August 10â€“14, 2020, Virtual Event, NY, USA
(a)
(c)
(b)
(d)
Figure 15: (a) Fast convergence of 1RMA CC - 0 to 100Gbps
in 40us; (b) Fast convergence from single flow at full rate to
two flows at fair-share rates; (c) Slower convergence when we
configure 1RMA to not separate local congestion from remote;
(d) issue_delay over time as reported by the NIC for (b).
7.3 1RMA Support for Encryption Key Rotation
We next consider the performance disruption of routine encryption
key rotation, as in systems like KVCS. We arrange for a client
process to continually probe (via RMA read ops) a server as the
latter enacts an encryption key rotation. Ideally, performance and
availability are impacted minimally. Figure 14 plots latency timelines
under three rekeying design variants.
Standard RDMA provides no mechanism to rotate keys on
an open connection; clients must switch connections to change
keys. Consequently, upon server encryption key rotation, client
connections break,
typically via a timeout. Whereas connec-
tion re-establishment delay varies by implementation, it neces-
sarily involves out-of-band communication, and disrupts the user
(>2ms, bottommost line). In contrast, encryption key rotation via
1RMAâ€™s driver (topmost) reduces the unavailability period to only
~27ğœ‡s. During this period, the client observes fast op failures
with REMOTE_AUTHENTICATION_FAILURE outcome. Clients react by
switching to new, pre-distributed keys. 1RMAâ€™s op independence
property ensures rapid resumption of service. Compared to the driver-
based mechanism, 1RMAâ€™s Rekey op (middle line) accomplishes
the same rotation in a handful of hardware cycles (< 1ğœ‡s) without
needing to extend trust to the driver to manage the new key.
7.4 1RMA Congestion Control Efficacy
We now focus on 1RMA congestion control (CC). We use crafted
workloads to answer the following questions: (a) How quickly does
1RMA respond (e.g., saturate a 100Gbps network link for a point-
to-point transfer)? (b) Do applications quickly reach a fair share of
bandwidth? and (c) How important is response to both local and
remote congestion?
Ramping to line rate. We inspect 1RMAâ€™s ramp-up behavior using
a single client that initiates 4MB read transfers from a single server.
Figure 15a plots the achieved op submission rate over time. With
an idle RTT of 5ğœ‡s in the testbed fabric, 1RMA is able to quickly
converge to and maintain 100Gbps (with a CWND of 15 outstanding
operations). Convergence is reached in ~8 RTTs, without incurring
any non-OK outcomes.
Bandwidth Sharing. Using one client and two servers, each con-
nected by a 100Gbps link, we initiate a single long-running transfer
comprised of 4MB reads between the client and one server. At
around T=400ğœ‡s (Figure 15b), the client initiates another transfer
with the second server, creating an incast condition at the client.
1RMA immediately detects the increase in local congestion, as sig-
nalled by a sharp increase in issue_delay (Figure 15d) and adjusts
the local CWND appropriately, leading to a quick convergence (~5
RTTs) of both transfers to a fair share.
Local congestion reaction. To show the importance of reacting
to local congestion specifically, we repeat the same experiment as
above, but modify CC to react only to total_delay, plotted in
Figure 15c. Failure to react specifically to local congestion leads to
20Ã— slower convergence.
7.5 Benefits of Solicitation
To provide software control loops ample time to react, 1RMAâ€™s
hardware-enforced solicitation mechanisms forestall drops and time-
outs during transients (dynamic sudden changes). Remote NICs
emit NACKs when inbound queues are full. This ensures that ops
fail quickly under extreme congestion, rather than occupy scarce
hardware resources for long durations.
Because software CC works well, pathological outcomes like
TIMEOUT are rare in prior experiments. To study the benefits of solic-
itation and NACKs in 1RMA without reaction from CC, we evaluate
1RMA in hypothetical network conditions not reflected in our testbed
using simulations. We study specifically the performance effects of
RTT and jitter on solicitation. We also quantify the importance of
NACKs.
Unless otherwise specified, read size is 4KB, RTT = 5ğœ‡s,
DISPATCH_TIMEOUT = 2 * RTT, and TIMEOUT = 4 * RTT.
1RMAâ€™s solicitation rules ensure that 1RMA does not initiate
a transfer unless it is assured to land the data in the solicitation
window. Crucially, the solicitation window size decides the number
of outstanding ops (and thereby governs the achievable goodput).
In an ideal network (no jitter or drops), sizing the window to the
bandwidth delay product (BDP) would suffice. In practice, jitter is
unavoidable. Figure 16a plots goodput as a function of jitter, for a
point-to-point workload. The simulated client initiates ops at a static
rate of 100Gbps. We plot three window sizes; 48KB (less than BDP),
64KB (equal to BDP), and 96KB (greater than BDP). Goodput drops
with jitter, but larger-sized solicitation windows are generally more
tolerant.
Critically, 1RMAâ€™s solicitation sheds load eagerly rather than
risking wasted bandwidth. Figure 16b plots the rates at which ops
are shed under varying network conditions, again with CC response
disabled. DISPATCH_TIMEOUTâ€”local load sheddingâ€”begins to man-
ifest as the solicitation window is taxed by unpredictable RTT. When
latency becomes unpredictable, some portion of ops time out, indicat-
ing that some of the senderâ€™s bandwidth was wasted (as all response
bytes were sent, but late-arriving bytes are dropped). Importantly,
solicitation and shedding work in concert to keep goodput high in
all but the most pathological cases.
Similarly, NACKs defend servers and clients alike during sudden
load shifts. First, by NACKing inbound requests that are likely to
718
050100150200250Time(us)0255075100SubmissionRate(Gbps)02004006008001000Time(us)0255075100SubmissionRate(Gbps)Server1Server20200400600800100012001400Time(us)0255075100SubmissionRate(Gbps)Server1Server202004006008001000Time(us)01000200030004000IssueDelay(ns)Measuredissue_delayTargetissue_delaySIGCOMM â€™20, August 10â€“14, 2020, Virtual Event, NY, USA
A. Singhvi et al.
(a)
(b)
(c)
(d)
Figure 16: Impact of jitter and solicitation window size - (a) Goodput Vs. jitter for different window sizes and (b) Timeout rate for
64KB solicitation window; (c) Impact of NACKs on goodput; (d) NACK threshold tuning.
time out at the initiator, we conserve server bandwidth. Second,
NACK eagerly refunds solicitation window capacity at the client, in
1 RTT instead of awaiting a TIMEOUT, allowing other ops to enter
service. To illustrate, Figure 16d plots goodput as a function of the
NACK threshold, wherein two clients each greedily demand 100Gbps
from a single server (i.e., total demand 200Gbps), irrespective of
congestion signals. The server can only provide 100Gbps. Such
behavior can occur transiently when load shifts. When NACKs are
not timely (i.e., the threshold is too high), goodput collapses, as the
serverâ€™s request queue grows without bound. To defend against this
behavior, NACKs shed the portion of load that is not sustainable at the
server-side. NACKed ops see higher apparent latency, because NACKed
ops may be retriedâ€”thus, at lower NACK thresholds, more ops may
see retries, leading to higher median latency but no loss of goodput.
8 Related Work
Several past works attempt to improve RDMA congestion control
and loss recovery [23, 25, 27, 31, 41], both key issues that deter-
mine the performance of RDMA operations at scale. For example,
DCQCN [41] implements a simple ECN-based congestion control
protocol on the NIC. HPCC [25] relies on in-band telemetry to
directly estimate the flow rate to use. RoGUE [23] is a software
CC protocol that uses delay-based congestion window adaptation,
while delegating loss recovery to the RNICsâ€™ existing strategies (i.e.,
go-back-N). IRN [31] (and, similarly, MELO [27]) advocates using
selective ACKs in hardware (as opposed to go-back-N) for loss re-
covery; congestion control continues to rely on hardware-supported
protocols such as DCQCN. While these techniques explore layer-
ing incremental software or hardware-based congestion control and
loss recovery mechanisms on standard RDMA, 1RMA derives sub-
stantial benefits from completely refactoring the hardware-software
division-of-labor.
iWARP [34] offloads all TCP stack functionality, including con-
nection management, flow control, congestion control, loss recovery
etc., to the NIC. This unfortunately makes the NIC design complex,
and requires intricate translation between higher-level ops and NIC
TCP actions.
All the above techniques are connection-oriented, and thus face
at least some of the challenges of standard RDMA (Â§2).
To mitigate connection scalability issues, Mellanox has intro-
duced DCT [11], which dynamically (and transparent to the appli-
cations) closes and opens connections to avoid queue pair exhaus-
tion. DCT may cause latency increase due to frequent connection
flips [21].
FaSST [21] and Scalable Connectionless RDMA [16] advocate
using connectionless (UD) RDMA, while Homa [32] champions
software-based solicitation and connectionless RPCs. Like Pony,
such designs enable rapid evolution in software, but ultimately yield
two-sided performance. Homa handles only the last-hop congestion,
while [16, 21] rely on near-lossless fabrics based on PFC pause