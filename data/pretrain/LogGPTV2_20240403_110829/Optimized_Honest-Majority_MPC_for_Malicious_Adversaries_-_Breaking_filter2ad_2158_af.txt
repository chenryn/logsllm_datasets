1
1
utilizing the fact that
(cid:3)−1
(cid:2)
X
t
L
N
X
t
L
S
where the last equality is by deﬁnition that N = (X − C)L.
X−C  L+C we have that
N 2 · (X − C)2L2
N 4 =
1
L4
(cid:6)
(cid:7)−1(cid:6)
(cid:6)
(cid:7)−S ≤
(cid:7)−1(cid:6)
(cid:6)
(cid:7)S ≤ L4
(cid:7)−S ≤ L4
(cid:3)−S(B−1)
Pr[Game1(A, X, L, B, C) = 1]
(cid:3)−(B−1)(cid:2)
(cid:2)
X
t
L
S
Therefore
Thus,
=
L
S
X
t
N 2L4 =
1
N 2
.
(cid:4)
(cid:5)B−1 ≤ 1
N B
1
N 2
≤
where the last inequality holds for B ≥ 2. which sufﬁces
since A must win in Game1 in order to win Game3.
Tightness of the theorem’s bound. Note that in the above
proof, in the case of S = 1, t = 1, the only inequality is
1
X  L+C. Then for
every adversary A and for every L ≥ 5, C ≥ 3 and X−C ≥ 6
it holds that
Pr[GameC
3 (A, X, L, B, C) = 1] ≤ 1
N B
where N = (X − C)L.
1
1
1
least
1
N B . There are three cases to consider.
Proof: The only way for A to win is if a bad ball is
thrown into a bad bucket in Step 3d. If the game ends prior
to this step it will end with output 0. Therefore it is enough
to show that when the ﬁrst bad ball is thrown, A wins with
probability at most
• Case 1: A bad ball
is thrown before any bad buckets
are generated, that is, while all buckets B1, . . . , BN and
B(cid:2)
1, . . . , B(cid:2)
N are good. In this case the output is always 0,
hence the probability that A wins is 0.
• Case 2: When the ﬁrst bad ball bi is thrown, bad buckets
have been generated during at
two executions of
Game1. By Theorem 3.4, A wins in two executions of
N 2B−2 ≤ 1
N B−1 ·
Game1 with probability
N B ,
N B−1 =
where the last inequality holds since B ≥ 2.
• Case 3: When the ﬁrst bad ball bi is thrown, bad buckets
have been generated in exactly one execution of Game1.
– If the bad buckets are generated in Step 1 or the ﬁrst
iteration of Step 3a, the resulting game is equivalent to
the case of Game3 in which bad buckets are generated
in exactly one list, and the bound follows from Theorem
3.10.
the bad buckets are generated in a later
iteration of Step 3a, note that A cannot win while all
buckets are good, and the best strategy for A is to prepare
no bad balls (since throwing a bad ball when all buckets
are good always results in output 0). Thus any iteration
of Step 3 in which no bi is chosen will not increase the
probability that A wins. Therefore, to obtain the bound,
it is enough to consider only two iterations of Step 3: the
iteration in which the bad ball bi is produced, and the
iteration preceding it. The resulting game is equivalent to
instead,
– If,
the case of Game3 in which bad buckets are generated in
only the ﬁrst list, and the bound follows from Theorem
3.10.
D. Hash Function Optimization
In [11], the method for validating a multiplication triple
using another triple requires the parties to compare their views
and verify that they are equal. In this basic comparison, each
party sends 3 bits to another party. Since B such comparisons
are carried out for every AND gate, this would signiﬁcantly
increase the communication. Concretely, with our parameters
of N = 220 and B = 2 and our optimizations, this would
increase the communication from 7 bits per AND gate to
13 bits per AND gate. In order to save this expense, [11]
propose for each party to simply locally hash its view (using
a collision-resistant hash function) and then to send the result
of the hash only at the end of the protocol. Amortized over
the entire computation, this would reduce this communication
to almost zero. When proﬁling Protocol 3.9 with all of our
optimizations, we were astounded to ﬁnd that these hashes
took up almost a third of the time in the triples-generation
phase, and about 20% of the time in the circuit computation
phase. Since the rate of computation is so fast, the SHA256
computations actually became a bottleneck; see Figure 3.
We solved this problem by observing that the view com-
parison procedure in [11] requires for each pair of parties
to compare their view. The security is derived from the fact
that if the adversary cheats then the views of the two honest
parties are different. As such, instead of using a collision-
resistant hash function, we can have each party compute a
MAC of their view. In more detail, each pair of parties jointly
choose a secret key for a MAC. Then, as the computation
proceeds, each party computes a MAC on its view twice,
once with each key for each other party. Then, at the end,
each party sends the appropriate MAC to each other party.
Observe that the honest parties compute a MAC using a secret
key not known to the corrupted party. Thus, the adversary
cannot cause the MACs of the two honest parties to have
the same tag if their views are different (or this could be
used to break the MAC). Note that with this method, each
party computes the MAC on its view twice, in contrast to
when using SHA256 where a single computation is sufﬁcient.
Nevertheless, we implemented this using GMAC (optimized
using the PCLMULQDQ instruction) and the time spent on
this computation was reduced to below 10%. As we show in
Section IV, this method increases the throughput of the fastest
protocol version by approximately 20%.
Fig. 3. Microbenchmarking of Protocol 3.9, using the CxxProf C++ proﬁler
855
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:59 UTC from IEEE Xplore.  Restrictions apply. 
IV. IMPLEMENTATION AND EXPERIMENTATION
We implemented the baseline protocol of [11] and the
different protocol improvements and optimizations that we
present in this paper. (We did not implement the variant in
Section III-B since it has the same efﬁciency as the variant in
Section III-C, and the latter is preferable for practical usage.)
All of our implementations use parameters guaranteeing a
−40, as mandated by the
cheating probability of at most 2
appropriate theorem proven above. We begin by describing
some key elements of our implementation, and then we present
the experimental results.
A. Implementation Aspects
Parallelization and vectorization. As with [1], our protocol
is particularly suited to vectorization. We therefore work in
units of 256 bits, meaning that instead of using a single bit as
the unit of operation, we perform operations on units of 256
bits simultaneously. For example, we are able to perform XOR
operations on 256 bits at a time by writing a “for loop” of eight
32 bit integers. This loop is then automatically optimized by
the Intel ICC compiler to use AVX2 256bit instructions (this is
called auto-vectorization). We veriﬁed the optimization using
the compiler vec-report ﬂag and used #pragma ivdep in
order to aid the compiler in understanding dependencies in
the code. We remark that all of our combinatorial analyses
considered “good” and “bad” balls and buckets. All of this
analysis remains exactly the same when considering vectors
of 256-triples as a single ball. This is because if any of the
triples in a vector is bad, then this is detected and this is
considered a “bad ball”.
Memory management. We use a common data structure
to manage large amounts of triplets in memory efﬁciently.
This structure holds 220× 256 triplets. For triplets ([a], [b], [c])
(or ([x], [y], [z]) respectively) we store an array of 220 × 256
bits for [a], 220 × 256 bits for [b], and 220 × 256 bits for
[c]. This method is known as a Struct of Arrays (SoA) as
opposed to an Array of Structs (AoS) and is commonly used in
SIMD implementations. It provides for very efﬁcient intrinsic
(vectorized) operations, as well as fast communication since
we send subarrays of these bit arrays over the communication
channel in large chunks with zero memory copying. This
reduces CPU cycles in the TCP/IP stack and is open for further
optimization using RDMA techniques.
Index shufﬂing. When carrying out the shufﬂing, we shufﬂe
indices of an indirection array instead of shufﬂing the actual
triples (which are three 256-bit values and so 96 bytes). Later
access to the 256-bit units is carried out by ﬁrst resolving the
location of the unit in O(1) access to the indirection array.
This show substantial improvement as this avoids expensive
memory copies. Note that since the triples themselves are not
shufﬂed, when reading the shufﬂed array during veriﬁcation
the memory access is not serial and we do not utilize memory
prefetch and L3 cache. Nevertheless, our experiments show
that this is far better overall than copying the three 256-bit
memory chunks (96 bytes) when we shufﬂe data. In Figure 4,
you can see that the entire cost of shufﬂing and verifying
the triples (_verifyAll_shuffleIndices) is reduced to
less than 30% of the time, in contrast to the original protocol
in which it was approximately 55% (see Figure 1).
Cache-Aware code design. A typical Intel Architecture
server includes a per-core L1 cache (32KB), a per-core L2
cache (typically 512KB to 2MB), and a CPU-wide L3 Cache
(typically 25-55MB on a 20-36 core server). L1 cache access is
extremely fast at ∼0.5ns, L2 access is ∼7ns and DDR memory
reference is ∼100ns. All caches support write back (so updates
to cached data is also extremely fast).
We designed our implementation to utilize L1 cache ex-
tensively when carrying out
the Fisher-Yates shufﬂing on
subarrays. We use two levels of indirection for the index
shufﬂing: the top level of 512 indices and the low level of
2048 indices (under each of the top level indices, yielding
512 subarrays of length 2048 each). As vectors are 1024 byte
and 4096 bytes respectively (uint16 values), they require
1/32 or 1/8 of the L1 cache space so L1 will be utilized
with very high probability (and in worst case will spill into
the L2 cache). This makes shufﬂing extremely fast. Note that
attempting to force prefetch of the index vectors into cache
(using _mm_prefetch instructions) did not improve our
performance, as this is hard to tune in real scenarios.
Ofﬂine/online. We implemented two versions of the proto-
cols. The ﬁrst version focuses on achieving high throughput
and carries out the entire computation in parallel. Our best
performance is achieved with 12 workers; each worker has two
threads: the ﬁrst thread generates multiplication triples, and the
second carries out the circuit computation. The architecture of
this version can be seen in Figure 5.
The second version focus on achieving fast online perfor-
mance in an ofﬂine/online setting where multiplication triples
are prepared ahead of time and then consumed later by a
system running only the circuit computation (and veriﬁcation
of that computation). As we have mentioned,
the cache-
efﬁcient version with bucket-size B = 3 is expected to have
lower throughput than the version with bucket-size B = 2
but lower latency. This is because with B = 3 there is no
need to randomly choose the triple being used to validate the
gate being computed. We therefore compared these; note that
in both cases we used the GMAC optimization described in
Section III-D so that we would be comparing “best” versions.
IMPLEMENTATION RESULTS; B DENOTES THE BUCKET SIZE; SECURITY LEVEL 2−40
TABLE II
Protocol Variant
Baseline [11]; Section II (B = 3, SHA)
Cache-efﬁcient; Sec. III-A (B = 3, SHA)
On-demand; Sec. III-C (B = 2, SHA)
On-demand; Sec. III-D (B = 2, GMAC)
Online-only: on-demand; Sec. III-D (B = 2, GMAC)
Online-only: cache-efﬁcient; Sec. III-A (B = 3, GMAC)
AND gates/sec %CPU utilization Gbps utilization
503,766,615
765,448,459