67%
60%
73%
73%
78%
60%
67%
100%
- Timeout
0%
-
-
-
- Timeout
0%
-
-
-
Figure 17: Trigger Rev. Eng. from High REASR Benign models
CIFAR-10 dataset, most deer images have antlers. As such, some be-
nign models pick up such strong correlation. Although such strong
features are not planted by adversaries, they are so strong that they
can be used to subvert other inputs. In that nature, they are quite
similar to trojan triggers.
5.2 Comparison with Neural Cleanse (NC)
Since NC is based on optimization without guidance from model
internals (see Section 2.2), it is sensitive to the initial random seed.
That is, it may generate the trojan trigger or a benign feature de-
pending on the random seed. To suppress such non-determinism,
we run NC 5 times and take the average. We use the default de-
tection setting of NC in [59]. The trigger size is 6%. We consider
NC successfully detects a trojaned model if the trojaned labels NC
outputs contain the trojan target label. The results are shown in
Table 4. Columns 1-2 show the dataset and model. Note that the
original NC paper also includes MNIST and two other face recog-
nition models. We exclude them because MNIST is too small to be
representative 1 and the two face recognition models are similar
to (and even smaller than) the VGG-Face model/dataset we use.
1We had MNIST tested and the results are consistent to the ones reported for other
datasets and models.
Symbol ‘-’ means not available as those are downloaded models
Columns 3 and 4 present the detection rate of ABS on pixel space
trojaning attacks and feature space trojaning attacks. We take the
average of REASR scores for the 7 pixel attacks and 2 feature attacks
in Table 2. Columns 5 and 6 shows the detection rate of NC when
using one image per label (the same setting as ABS). Columns 7 and
8 show the detection rate of NC when using the full training set (the
most favorable setting for NC). For VGG-Face, there are 2622 labels
and NC needs to scan them one by one. It does not terminate after
96 hours, so we mark this case as timeout. ImageNet also timeouts
for a similar reason.
We have the following observations. (1) NC is not effective for
feature space attacks or the object detection data set USTS, as NC
does not apply to those scenarios. (2) NC is not that effective when
only one image is provided for each label. This is because when
the number of images used in optimization is small, it is possible to
generate a small size trigger that subverts all the input images to the
same benign label. We conduct an additional experiment to show
how NC’s success rate changes with the number of samples used
for CIFAR-10. The results are shown in Figure 18. (3) NC is much
more effective when the full training set is used. While the accuracy
seems lower than what is reported in [59], further inspection shows
that they are consistent. In particular, the trojan size is around 6%
(of the input size) in our CIFAR and GTSRB attacks and the authors
reported that NC is less effective when the trigger size goes beyond
6%. For the Age dataset, the trojan trigger is also larger than 6%
Figure 18: Detection Accuracy of NC w.r.t
Number of Data Points Used on CIFAR-10
Figure 19: Detection Accuracy of NC w.r.t
Trigger Size on CIFAR-10
Figure 20: REASR of Benign and Trojaned
Models When No Inputs Provided
Table 5: Running Time of ABS and NC
Model
ABS Time(s)
NC Time(s)
NiN
VGG
ResNet32
ResNet110
ResNet110
NiN
VGG
LeNet
VGG
VGG
3Conv+3FC
FastRCNN
Stimulation Analysis Trigger Gen. Total
115
300
340
1270
234
560
1580
220
4920
Timeout
18300 Timeout
400
1250
35
120
40
270
144
300
480
120
4320
18000
200
650
80
180
300
1000
90
260
1100
100
600
300
100
600
330
820
740
1370
15800
30200
50400
1256
417
-
Dataset
CIFAR-10
GTSRB
ImageNet
VGG-Face
Age
USTS
(a) Plane (b) car
(d) cat
(c) bird
Figure 21: Inversed Input Samples
(e) deer
(f) dog
(g) frog (h) horse (i) ship (j) truck
of the whole images. We further conduct an experiment to show
how the detection rate of NC changes with trojan trigger size on
CIFAR-10 in Figure 19. NC has a very high success rate when the
triggers are smaller than 6%. It fails to detect any triggers larger
than 6%. As discussed in Section 2.2, triggers do not have to be small.
Note that ABS is not sensitive to trigger size (see Appendix C). (4)
ABS consistently out-performs NC (when the trigger size is around
6% and beyond). In addition, it does not require a lot of samples to
achieve good performance. Additional experiments are performed
to evaluate the effectiveness of NC by tuning its parameter settings
and using data augmentation (Appendix E). While these efforts do
improve its performance a bit, the improvement is limited and still
not comparable to ABS.
5.3 Detection Efficiency
We show the detection efficiency of ABS and NC in Table 5. Columns
3 to 5 show the execution time of ABS, including the stimulation
analysis time, trigger generation time, and their sum. The last col-
umn shows the NC execution time. In Table 5, both ABS and NC
use 1 image per label. We can observe that ABS is consistently
much faster than NC due to its analytic nature and its use of model
internals. For example, without the hints like compromised neu-
ron candidates (and their interesting value ranges) identified by
ABS, NC has to scan all the output labels one by one. Also observe
that ABS execution time grows with model complexity. Note that
the model complexity order is ResNet>VGG>NiN. In CIFAR-10,
ResNet32 has fewer neurons than VGG and takes less time in stim-
ulation analysis. For VGG-Face and ImageNet VGG models, the
stimulation analysis time is much more than others because it has
the largest number of neurons (i.e., millions).
5.4 Detection Effectiveness Without Input
We further explore the scenario in which models are provided with-
out any sample inputs. We use model inversion [38] to reverse
engineer one input for each output label and then apply ABS. Ex-
amples of reverse engineered inputs on CIFAR-10 are shown in
Figure 21. We test ABS on the combination of CIFAR-10+NiN. The
resulting REASR scores (on trojaned and benign models) are shown
in Figure 20. Out of the 23 trojaned models, there are only 3 models
whose REASR falls under 80%. The performance degradation is
because reverse engineered images are optimized to activate out-
put neurons and their logits values are much larger than those
for normal images. In some cases, the optimization procedure (of
generating trigger) cannot subvert such high output activations
even with the guidance of compromised neurons. Note that we are
not claiming ABS can work without sample data in general as more
experiments need to be done for proper validation.
5.5 Scanning models hosted on online model
zoo
We download 30 per-trained model from the Caffe model zoo [2],
15 on age classification with 8 labels and 15 on gender classification
with 2 labels. We have no knowledge of the benignity of these
models and we scan them with ABS. For all these models, we use
one image per label during scanning. Then we test the reverse
engineered “trigger” on randomly selected test set (1000 images).
We use max_triддer_size = 6%. The REASR scores are reported in
Table 6. Two models have over 85% REASR (in red), one forcing
88.1% of the test images to be classified as age 25-32 and the other
forces 85.9% of the test images to female. Further observe that the
REASR score for the suspicious gender model is not far way from
the scores of its peers, whereas the gap for the suspicious age model
is much more obvious. Given that the test accuracy of the suspicious
age model is 71.2%, close to the average of the others, 75.3%, the
particular model is fairly suspicious, or at least faulty. The triggers
produced by ABS are shown in Figure 22.
Table 6: REASR for Downloaded Pre-trained Models
Age
26% 12% 57% 23% 11% 55% 25% 19% 29% 26% 43% 88% 25% 33% 46%
Gender 72% 71% 65% 86% 69% 59% 72% 72% 70% 66% 77% 80% 83% 62% 55%
(a) Age
(b) Gender
Figure 22: Triggers with High REASR for Downloaded Models
0%20%40%60%80%10100100010000100000# Data points Detection Accuracy0%50%100%2%4%6%10%14%19%25%Size of Trojan TriggerDetection Accuracy0%50%100%REASRBenignTrojan5.6 Detection Effectiveness On Adaptive
Trojaning Attacks
The experiments in previous sections are to detect trojaned models
under the assumption that the attacker is not aware of ABS. In this
section, we devise three adaptive trojaning attacks that are aware
of ABS and try to deliberately evade it. We then evaluate ABS on
these adaptive attacks to demonstrate its robustness.
The first adaptive attack to bypass ABS is to trojan a model
while minimizing the standard deviation of neuron activations in
the same layer during data poisoning. The intention is to force
multiple neurons to work together to realize the trojaned behavior,
by minimizing the activation differences of individual neurons.
Ideally, there shall not be a “compromised neuron” to blame.
The second method is to trojan a model such that neuron ac-
tivations on benign inputs and inputs stamped with triggers are
as similar as possible. Since compromised neurons are activated
when inputs contain triggers, minimizing the activation difference
between benign and malicious inputs has the effect of limiting
the elevation effects of compromised neurons such that multiple
neurons may be forced to interact to realize the injected behavior.
The third method is to constrain the maximum neuron activation
differences between benign and malicious inputs. The rationale is
similar to the second method.
For the three attacks, we introduce an adaptive loss function in
addition to the normal classification loss, and minimize these two
loss functions together. For the first adaptive attack, the adaptive
loss is the standard deviation of neuron activations within the same
layer. For the second attack, the adaptive loss is the mean squared
value of activation differences between benign and malicious inputs.
For the third attack, the adaptive loss is the maximum mean squared
value of activation differences between benign and malicious inputs.
We tune the weight between the normal classification loss and
the adaptive loss and obtain a set of trojaned models, with different
accuracy on benign inputs and different adaptive loss values. The
relation between adaptive loss and model accuracy is shown in
Figure 23. The figures in (a), (b), (c) stand for the results for the
three attacks: minimized standard deviation, minimized differences
between benign and malicious inputs, and minimized maximum
differences between benign and malicious inputs, respectively. Each
triple (e.g., the three in (a)) includes the results for the NiN, VGG
and ResNet110 structures, respectively. The dataset is CIFAR10. In
each figure, the x axis is the adaptive loss and the y axis is the model
accuracy. The attack success rate (in testing) is always 100% for
all these models. As shown in Figure 23, for most trojaned models,
along with the decrease of adaptive loss, the model accuracy (on
benign inputs) decreases as well. We stop perturbing the weight
of adaptive loss when the normal accuracy decreases exceeds 3%.
For NiN models and ResNet models of the third type of adaptive
loss, we stop perturbing when the adaptive loss is close to 0. For
all these trojaned models, ABS can successfully detect them by
reverse engineering the top 40 neurons selected through neuron
sampling . Recall that without the adaptive attacks, we need to
reverse engineer top 10 neurons. The experiment shows that while
the adaptive attacks do increase the difficulty, ABS is still quite
effective. While more complex and sophisticated adaptive attacks
are possible, we will leave such attacks to the future work.
6 DISCUSSION
Although ABS has demonstrated the potential of using an analytic
approach and leveraging model internals in detecting trojaned
models, it can be improved in the following aspects in the future.
Distinguishing Benign Features from Triggers. As shown in
our experiments, ABS occasionally reverse engineers a (strong)
benign feature and considers that a trigger. While this is partially
true as the feature can subvert other inputs, a possible way to
distinguish the two is to develop a technique to check if the reverse
engineered “trigger” is present in benign images of the target label.
Handling Complex Feature Space Attacks. In our experiments,
we only show two simple feature space attacks. Note that such
attacks (for inserting back-doors) have not been studied in the
literature as far as we know. As discussed in Section 4.3, complex
generative models can be used as the trigger to inject feature space
patterns, which may lead to violations of ABS’s assumptions and
render ABS in-effective. We plan to study more complex feature
space attacks and the feasibility of using ABS to detect such attacks.
Handling Label Specific Attacks. Label specific attack aims to
subvert inputs of a particular label to the target label. It hence has
less persistence. While we have done an experiment to demonstrate
that ABS is likely still effective for label specific attack (see Appen-
dix D), we also observe that ABS tends to have more false positives
for such attacks and requires additional input samples. We plan to
investigate these limitations.
Better Efficiency. Although ABS is much faster than the state-
of-the-art, it may need a lot time to scan a complex model. The
main reason is that it has to perform the stimulation analysis for
every inner neuron. One possibility for improvement is to develop
a lightweight method to prune out uninteresting neurons.
One-neuron Assumption. We assume that one compromised neu-
ron is sufficient to disclose the trojan behavior. Although the as-
sumption holds for all the trojaned models we studied, it may not
hold when more sophisticated trojaning methods are used such
that multiple neurons need to interact to elevate output activation
while any single one of them would not. However, as shown in
Appendix F, ABS can be extended to operate on multiple neurons.
The challenge lies in properly estimating the interacting neurons
to avoid exhaustively searching all possible combinations. We will
leave it to our future work.
More Relaxed Attack Model. In ABS, we assume that misclassi-
fication can be induced by applying trojan trigger on any input. In
practice, the attacker may be willing to strike a balance between
attack success rate and stealthiness. For example, it may suffice
if the attack succeeds on a subset of inputs (say, 80%). In practice,
it is also possible that multiple triggers and trigger combinations
are used to launch attacks. We leave it to our future work to study
ABS’s performance in such scenarios.
7 RELATED WORK
In addition to the trojan attacks and defense techniques discussed
in Seciton 2, ABS is also related to the following work. Zhou et
al. [65] proposed to inject trojan behavior by directly manipulat-
ing model weights. However this approach has only been tested
on small synthetic model and not yet on real DNNs. Clean label
attacks[51, 58] aimed at degrading model performance by poisoning
(a) Min_Std
(c) Min_Max_Diff
Figure 23: Model Accuracy (y axis) versus Adaptive Loss (x axis) in Three Adaptive Attacks