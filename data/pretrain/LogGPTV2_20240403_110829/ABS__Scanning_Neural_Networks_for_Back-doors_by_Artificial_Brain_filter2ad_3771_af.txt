### 67%, 60%, 73%, 73%, 78%, 60%, 67%, 100%, - Timeout, 0%, - , - , - , - Timeout, 0%, - , - , -

**Figure 17: Trigger Reverse Engineering from High REASR Benign Models**

In the CIFAR-10 dataset, most deer images feature antlers. Consequently, some benign models identify this strong correlation. Although such features are not intentionally planted by adversaries, they are so prominent that they can be exploited to subvert other inputs. In this sense, they function similarly to trojan triggers.

### 5.2 Comparison with Neural Cleanse (NC)

Neural Cleanse (NC) is based on optimization without guidance from model internals (see Section 2.2), making it sensitive to the initial random seed. This means that NC may generate a trojan trigger or a benign feature depending on the random seed. To mitigate this non-determinism, we run NC five times and take the average. We use the default detection settings of NC as specified in [59]. The trigger size is set to 6%. We consider NC to have successfully detected a trojaned model if the trojaned labels output by NC include the trojan target label. The results are presented in Table 4.

- **Columns 1-2:** Show the dataset and model.
- **Columns 3-4:** Present the detection rate of ABS for pixel space trojaning attacks and feature space trojaning attacks. We average the REASR scores for the 7 pixel attacks and 2 feature attacks in Table 2.
- **Columns 5-6:** Show the detection rate of NC when using one image per label (the same setting as ABS).
- **Columns 7-8:** Show the detection rate of NC when using the full training set (the most favorable setting for NC).

For VGG-Face, there are 2622 labels, and NC needs to scan them one by one. It does not terminate after 96 hours, so we mark this case as a timeout. Similarly, ImageNet also times out.

**Observations:**
1. **Feature Space Attacks and USTS:** NC is ineffective for feature space attacks or the object detection dataset USTS because NC does not apply to these scenarios.
2. **Single Image Per Label:** NC is less effective when only one image is provided for each label. This is because, with a small number of images, it is possible to generate a small trigger that subverts all input images to the same benign label. We conducted an additional experiment to show how NC's success rate changes with the number of samples used for CIFAR-10 (Figure 18).
3. **Full Training Set:** NC is much more effective when the full training set is used. While the accuracy seems lower than what is reported in [59], further inspection shows consistency. Specifically, the trojan size is around 6% of the input size in our CIFAR and GTSRB attacks, and the authors reported that NC is less effective when the trigger size exceeds 6%. For the Age dataset, the trojan trigger is also larger than 6% (Figure 19).
4. **ABS vs. NC:** ABS consistently outperforms NC, especially when the trigger size is around 6% or larger. Additionally, ABS does not require a large number of samples to achieve good performance. Additional experiments were performed to evaluate the effectiveness of NC by tuning its parameter settings and using data augmentation (Appendix E). While these efforts improved performance slightly, the improvement was limited and still not comparable to ABS.

### 5.3 Detection Efficiency

Table 5 compares the detection efficiency of ABS and NC. Columns 3 to 5 show the execution time of ABS, including stimulation analysis time, trigger generation time, and their sum. The last column shows the NC execution time. Both ABS and NC use one image per label. ABS is consistently faster due to its analytic nature and use of model internals. For example, without the hints like compromised neuron candidates (and their interesting value ranges) identified by ABS, NC has to scan all output labels one by one. ABS execution time increases with model complexity, with ResNet being the most complex, followed by VGG and then NiN.

### 5.4 Detection Effectiveness Without Input

We explored the scenario where models are provided without any sample inputs. We used model inversion [38] to reverse engineer one input for each output label and then applied ABS. Examples of reverse-engineered inputs on CIFAR-10 are shown in Figure 21. We tested ABS on the combination of CIFAR-10+NiN. The resulting REASR scores for trojaned and benign models are shown in Figure 20. Out of the 23 trojaned models, only 3 had REASR scores below 80%. The performance degradation is due to the high output activations of reverse-engineered images, which can sometimes prevent the generation of effective triggers even with the guidance of compromised neurons. Note that more experiments are needed to validate ABS's general applicability without sample data.

### 5.5 Scanning Models Hosted on Online Model Zoo

We downloaded 30 pre-trained models from the Caffe model zoo [2], 15 for age classification with 8 labels and 15 for gender classification with 2 labels. We scanned these models with ABS, using one image per label. We then tested the reverse-engineered "triggers" on a randomly selected test set (1000 images) with max_trigger_size = 6%. The REASR scores are reported in Table 6. Two models had over 85% REASR (in red), one forcing 88.1% of the test images to be classified as age 25-32 and the other forcing 85.9% to be female. The REASR score for the suspicious gender model was close to its peers, while the gap for the suspicious age model was more obvious. Given the test accuracy of the suspicious age model (71.2%) is close to the average (75.3%), the model is highly suspicious or at least faulty. The triggers produced by ABS are shown in Figure 22.

### 5.6 Detection Effectiveness on Adaptive Trojaning Attacks

The previous experiments assumed the attacker was unaware of ABS. Here, we devised three adaptive trojaning attacks that aim to evade ABS and evaluated ABS's robustness against these attacks.

1. **Minimize Standard Deviation:** Trojan a model while minimizing the standard deviation of neuron activations in the same layer during data poisoning. This forces multiple neurons to work together, reducing the presence of a single "compromised neuron."
2. **Minimize Activation Differences:** Trojan a model such that neuron activations on benign inputs and inputs stamped with triggers are as similar as possible. This limits the elevation effects of compromised neurons, requiring multiple neurons to interact.
3. **Constrain Maximum Activation Differences:** Constrain the maximum neuron activation differences between benign and malicious inputs, similar to the second method.

For these attacks, we introduced an adaptive loss function in addition to the normal classification loss and minimized both. The relationship between adaptive loss and model accuracy is shown in Figure 23. Despite the adaptive attacks, ABS successfully detected the trojaned models by reverse engineering the top 40 neurons, compared to the top 10 neurons without adaptive attacks. This demonstrates that while the adaptive attacks increase difficulty, ABS remains effective.

### 6. Discussion

While ABS has demonstrated potential in detecting trojaned models, several areas for improvement exist:

- **Distinguishing Benign Features from Triggers:** ABS occasionally reverse engineers a strong benign feature and considers it a trigger. A possible solution is to develop a technique to check if the reverse-engineered "trigger" is present in benign images of the target label.
- **Handling Complex Feature Space Attacks:** More complex feature space attacks, such as those using generative models, may violate ABS's assumptions. Future work will explore the feasibility of using ABS to detect such attacks.
- **Handling Label-Specific Attacks:** Label-specific attacks aim to subvert inputs of a particular label. While ABS is effective, it tends to have more false positives and requires additional input samples.
- **Better Efficiency:** ABS is faster than state-of-the-art methods but may need significant time to scan complex models. Developing a lightweight method to prune uninteresting neurons could improve efficiency.
- **One-Neuron Assumption:** ABS assumes one compromised neuron is sufficient to disclose trojan behavior. This may not hold for more sophisticated trojaning methods. However, ABS can be extended to operate on multiple neurons, though the challenge lies in estimating interacting neurons.
- **More Relaxed Attack Model:** ABS assumes misclassification can be induced by applying trojan triggers to any input. In practice, attackers may balance attack success rate and stealthiness, and multiple triggers or combinations may be used. Future work will study ABS's performance in such scenarios.

### 7. Related Work

In addition to trojan attacks and defense techniques discussed in Section 2, ABS is related to work by Zhou et al. [65], who proposed injecting trojan behavior by directly manipulating model weights. However, this approach has only been tested on small synthetic models and not real DNNs. Clean label attacks [51, 58] aimed at degrading model performance by poisoning training data, but these are distinct from the trojaning methods addressed by ABS.