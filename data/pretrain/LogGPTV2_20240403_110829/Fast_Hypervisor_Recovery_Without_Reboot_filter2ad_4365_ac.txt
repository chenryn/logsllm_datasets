82.2% ± 2.4%
95.0% ± 1.4%
96.1% ± 1.2%
time among CPUs and to update the execution time of vCPUs
for the scheduler. For each of these timers, the handler re-
activates the timer each time it is ﬁred. If a fault occurs while
a CPU is executing one of these timer handlers, before the
handler successfully re-activates the timer, the recurring timer
is lost.
This problem is very similar to the problem described above
that motivates the reprogram hardware timer enhancement. To
solve the problem discussed here, for each relevant timer event,
there is a ﬂag that is set on entry to the handler and cleared
upon exit. As part of the recovery process, for any of these
ﬂags that is set, the corresponding timer is re-activated.
B. Incremental Development of NiLiHype Enhancements
To identify the need for the enhancements discussed in
Subsection V-A, we use the same incremental procedure, based
on fault injection, used in multiple previous works [24], [19].
Results from fault injections are analyzed to identify the cause
of the plurality of recovery failures. A mechanism is developed
to handle that particular problem. The process is repeated with
new fault injections in each iteration.
Table I summarizes all the mechanisms we use to enhance
NiLiHype with their impact on the recovery rate. The fault
injection setup is the same one used in Section IV. Speciﬁcally,
we use the 1AppVM workload with the UnixBench benchmark
(Subsection VI-A). 1000 fail-stop faults are injected in each
iteration.
Table I shows that the Clear IRQ count enhancement is
mandatory in order for NiLiHype recovery to succeed. This
is because the CPU that detects the error sends IPIs to other
CPUs to initiate the recovery. Each of the CPUs receiving
the IPI increments its local irq count. Since the CPU then
discards its thread of execution (discards its stack), it never
returns from the IPI. As a result, local irq count ends up
with an inconsistent value (not 0). This later causes hypervisor
failure as a result of the failure of assertions in several critical
routines that check whether the CPU is in interrupt context.
Table I also shows that, with all the enhancements, for this
particular setup, NiLiHype achieves the same recovery rate as
ReHype (Section IV).
VI. EXPERIMENTAL SETUP
This section presents the experimental setup used to evaluate
NiLiHype and ReHype. This setup is very similar to the
one used in [19], [21]. The key difference is the use of a
more modern platform (ISA and Xen/Linux versions). The
conﬁgurations of the systems evaluated (target systems) are
described in Subsection VI-A. Subsection VI-B presents the
error detection mechanisms used in the target systems. Details
regarding the fault injection are described in Subsection VI-C.
A. Target System Conﬁgurations
We evaluate virtualized systems running synthetic bench-
marks designed to stress different aspects of the system. The
hypervisor is Xen 4.2.3. The system includes the privileged
VM (PrivVM) and either one application VM (AppVM), in
the 1AppVM conﬁguration, or three AppVMs, in the 3AppVM
conﬁgurations. Each VM consists of one vCPU (virtual CPU)
and each of the vCPU is pinned to a different physical CPU.
The physical machines used for all the experiments are 8-core
systems based on Intel Nehalem CPUs.
All the AppVMs are paravirtualized VMs (PVMs). It should
be noted that previous work has shown that fault injection
results obtained with AppVM supported by full hardware
virtualization (HVMs) are very similar to those obtained with
paravirtualized AppVMs [21].
are
Three
synthetic
benchmarks
used: BlkBench,
UnixBench, and NetBench. BlkBench focuses on the interface
to block devices (disk). It creates, copies, reads, writes and
removes multiple 1MB ﬁles containing random content.
To ensure that
the device is actually accessed, requiring
hypervisor activity, caching of block and ﬁle system data
in the AppVM is turned off. Without this setting, caching
within the AppVM would minimize the chances for exposing
recovery failure. UnixBench is a collection of programs
designed to stress different aspects of the system [2]. We
use a subset of the programs in the original UnixBench.
The programs were selected for their ability to stress the
hypervisor’s handling of hypercalls, especially those related
to virtual memory management.
For BlkBench and UnixBench, the execution is considered
as failed if 1) one or more ﬁles produced by the benchmark
are different from the ones in a golden copy, or 2) logging
messages from the benchmarks indicate that one or more than
one system calls to the OS of the AppVM failed.
NetBench is a user-level network ping program. It is used
to exercise the interface to the network as well as to evaluate
the recovery latency. It involves two processes: the receiver
runs in an AppVM in the target system, and the sender
runs on a separate physical host. The sender sends a UDP
packet to the receiver every 1ms. Upon receiving a packet, the
receiver sends a reply back to the sender. NetBench execution
is considered as failed if the packet reception rate of the sender
drops by more than 10% compared to its reception rate during
normal execution in any one-second interval.
In the 1AppVM setup, the AppVM runs either BlkBench or
UnixBench. Each one of the benchmarks is conﬁgured to run
120
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
PrivVM 
Campaign 
Agent 
GuestOS 
Target System 
PrivVM 
GuestOS 
AppVM1 
NetBench 
GuestOS 
AppVM2 
UnixBench 
GuestOS 
AppVM3 
BlkBench 
GuestOS 
Xen Hypervisor + Recovery Mechanism 
Xen Hypervisor + Fault Injector 
Fig. 1. Fault injection setup with the 3AppVM workload. The entire target
system is in a VM. The injector is in the outside hypervisor
for around 10 seconds. This setup is mostly used to guide
the measurement-based incremental development of recovery
enhancement mechanisms.
In the 3AppVM setup, the system initially runs two AppVM:
one with UnixBench and the other with NetBench. Following
recovery, a third AppVM is created and it runs BlkBench. The
third AppVM is used to check whether the hypervisor still
maintains its ability to create and host newly created VMs
after recovery. The ﬁrst two AppVMs are conﬁgured to run
for approximately 24 seconds.
B. Error Detection
The focus of this paper is on error recovery, not error
detection. However, an error detection mechanism is necessary
for the experimental evaluation since such a mechanism is
responsible for initiating recovery.
We rely on the built-in panic and hang detectors in Xen
to detect errors. A panic is detected when a fatal hardware
exception occurs or a software assertion fails. The hang
detector is based on a watchdog timer. It is implemented based
on hardware performance counters and software timer events.
A performance counter on each CPU is used to generate an
NMI every 100ms of unhalted CPU cycles. There is also
a recurring software timer event that increments a counter
every 100ms. The handler of the performance counter checks
for changes in the counter. If the counter is not incremented
for three consecutive invocations of the performance counter
handler, a hang is detected.
C. Fault Injection
Software-implemented fault injection is used to determine
the recovery rate of NiLiHype and ReHype. A fault injec-
tion run consists of booting the target system, starting the
benchmarks in the AppVMs, injecting one fault, and collecting
logs that allow the results to be analyzed. For each fault type
and system conﬁguration there is an injection campaign that
consists of multiple runs.
We ported the Gigan fault injector [22] to a modern plat-
form (ISA and Xen/Linux version). To minimize intrusion
by the injector and simplify campaign setups, we use Gigan
in a conﬁguration based on two-level nested virtualization.
Speciﬁcally, the entire virtualized target system runs in a VM
supported by full hardware virtualization (i.e., an HVM). It has
been shown that fault injection and recovery results obtained
with this setup are similar to those obtained when the target
system runs on bare hardware [21], [22].
The injector runs outside the target system, in the hypervisor
(the “outside” hypervisor) that hosts the HVM with the target
system. A user-level campaign script, the Campaign Agent,
runs in the PrivVM of the outside hypervisor. The Campaign
Agent creates the VM with the target system, conﬁgures the
fault injector, and collects logs and output from each run. The
fault injection setup with the 3AppVM workload is shown in
Figure 1.
We inject three types of faults: Failstop, transient Register
faults, and Code faults. Failstop faults are injected by changing
the value of the program counter to 0. Register faults are
injected by ﬂipping a random bit in a random register selected
from the 16 general-purpose registers, the stack pointer, the
ﬂag register, and the program counter. Code faults are injected
by ﬂipping a random bit in a random byte chosen within the
15-byte range (the maximum length of an x86-64 instruction)
starting from the current value of the program counter. When/if
the Code fault causes an error that is detected, the injector
“repairs” the fault. Thus, the effects of a Code fault do not
persist during the recovery process. Hence, the effects of a
Code fault are almost the same as if it was a transient fault.
The injected faults do not cover all possible faults. NiLi-
Hype is designed to recover from transient hardware faults as
well as rare software bugs (Heisenbugs) [11], that occur only
under particular timing and ordering of asynchronous events
in the system. Transient hardware faults in the CPU datapath
are likely to be manifested as erroneous values in registers.
Hence, register bit ﬂips can be expected to be reasonably
representative of such faults. Injection of bit ﬂips in the code
attempts to partially represent faults in the instruction fetch
and decode hardware. Similar injection campaigns have been
widely used in prior works [9], [14], [5]. As discussed in
Subsection VII-A, results from the injection of failstop faults,
together with other results, help with the understanding of the
tradeoffs between microreboot and microreset.
All the faults are injected by using a two-level chained
trigger. When the ﬁrst-level trigger ﬁres, it triggers the second-
level, which, when ﬁred, triggers the fault injection. The ﬁrst-
level trigger is a timer that ﬁres after a speciﬁed amount of
time has elapsed. It is conﬁgured differently in the 1AppVM
and 3AppVM setups. With the 1AppVM setup, it is set to ﬁre
at a random time after the initial 10% and before the ﬁnal
10% of the benchmark execution time. With the 3AppVM
setup, the ﬁrst-level trigger is conﬁgured to ﬁre at a random
time between 500ms and 6 seconds. This is well past the start
of the UnixBench and NetBench AppVMs while leaving most
of their 24 seconds execution to occur after recovery.
The second-level trigger ﬁres after a random number of
instructions between 0 to 20000 have been executed in the
target hypervisor. This trigger ensures that faults are injected
only while the CPU is executing code of the target hypervisor.
121
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:49:40 UTC from IEEE Xplore.  Restrictions apply. 
VII. EVALUATION
100.00%
98.90% 
98.60% 
96.32% 
95.00% 
94.60% 
94.45% 
92.85% 
91.79% 
Success 
noVMF 
This section presents an evaluation of NiLiHype using the
experimental setup described in Section VI. Along every axis,
NiLiHype is compared to ReHype. The recovery rate, recov-
ery latency, hypervisor processing overhead during normal
operation, and implementation complexity are presented in
Subsections VII-A, VII-B, VII-C, and VII-D, respectively.
A. Successful Recovery Rate
It has been shown that, for typical deployments of virtu-
alization, less than 5% of CPU cycles are spent executing
hypervisor code [6], [4]. Hence, a random transient fault
is much more likely to occur when executing code in a
VM than when executing hypervisor code. Thus, a random
transient fault
is highly likely to affect one of the VMs,
possibly causing it to fail, even if the virtualization platform
is completely immune to all faults. Whether a fault in the
hypervisor can affect a single VM becomes relevant only if
mechanisms implemented strictly within the VM itself allow it
to mask or recover from the overwhelming majority of faults
that may occur during the execution of the VM. Taking this
into account, it is not meaningful to evaluate any hypervisor
resilience mechanism based on a criterion that a manifested
fault in the hypervisor should not affect even one VM.
Without any resilience mechanisms, a single transient fault
can cause the hypervisor to fail, taking down all the VMs it
hosts. Based on the discussion above, we can deﬁne a reason-
able goal for a hypervisor resilience mechanism. Speciﬁcally,
taking into account only transient faults (including Heisen-
bugs [11]), running multiple VMs on a single host should
not be worse than running them without virtualization on
separate physical machines [18]. In practice, this goal cannot
really be met due to practical issues, such as power supplies,
network connections, etc. However, this forms the basis for our
deﬁnition of “successful recovery” from hypervisor failure.
We deﬁne recovery from hypervisor failure to be “suc-
cessful” if no more than one AppVM is affected by the
fault and, after recovery, the hypervisor continues to operate
correctly (new VMs can be created, etc). The 3AppVM setup
is designed to allow evaluation based on this deﬁnition. Specif-
ically, the setup includes creating a new AppVM (BlkBench)
after recovery, and an ability to verify that BlkBench runs
correctly to completion. Note that, for the 1AppVM setup,
we deﬁne “recovery success” to mean that no VM is affected.
The recovery rate is evaluated with the 3AppVM setup.
Separate campaigns are run with the three fault types: Failstop,
Register, and Code. For each fault injection run, the outcome
can be classiﬁed into three categories: non-manifested, silent
data corruption (SDC), and detected. Non-manifested means
that the injected fault does not cause any observable abnormal
behavior: the benchmarks ﬁnish successfully (produce the cor-
rect outputs) and the detection mechanisms are not triggered.
SDC means that the detection mechanisms are not triggered
but at least one of the benchmarks fails to produce the expected
outputs. Detected means that one of the detection mechanisms
98.00%
96.00%
94.00%
92.00%