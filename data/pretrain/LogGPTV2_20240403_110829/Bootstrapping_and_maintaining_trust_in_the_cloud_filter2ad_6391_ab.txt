ronment while limiting complexity and overhead.
3.1 Threat Model
Our goal is to minimize trust in the cloud provider and
carefully account for all concessions we must make to enable
trusted computing services. We assume the cloud provider is
semitrusted, i.e., they are organizationally trustworthy but
are still susceptible to compromise or malicious insiders. We
assume the cloud provider has processes, technical controls,
and policy in place to limit the impact of such compromise
from spreading across their entire infrastructure. Thus, in
the semitrusted model, we assume that some fraction of the
cloud provider’s resources may be under the control of the
adversary (e.g., a rogue system administrator may control a
subset of racks in an IaaS region).
Speciﬁcally, we assume that the adversary can monitor or
manipulate compromised portions of the cloud network or
storage arbitrarily. We assume that the adversary may not
physically tamper with any host’s (e.g., hypervisor or bare
metal node) CPU, bus, memory, or TPM8. In virtualized en-
vironments, the security of our system relies upon keeping
cryptographic keys in VM memory. Therefore, we assume
that the provider does not purposefully deploy a hypervisor
with the explicit capability to spy on tenant VM memory
(e.g., Ether [9]). We assume that TPM and system manu-
facturers have created the appropriate endorsement creden-
tials and have some mechanism to test their validity (i.e.,
signed certiﬁcates)
Finally, we assume that the attacker’s goal is to obtain
persistent access to a tenant system in order to steal, dis-
rupt, or deny the tenant’s data and services. To accomplish
persistence the attacker must modify the code loading or
running process. We assume that such modiﬁcations would
be detected by load-time integrity measurement of the hy-
pervisor or kernel [19], runtime integrity measurement of the
kernel [24], and integrity measurement of applications [30].
3.2 Architecture
To introduce the architecture of keylime we ﬁrst describe
a simpliﬁed architecture for managing trusted computing
services for a single organization, or cloud tenant, without
virtualization. We then expand this simpliﬁed architecture
into the full keylime architecture, providing extensions that
allow layering of provider and tenant trusted computing ser-
vices and supporting multiple varieties of IaaS execution
isolation (i.e., bare metal, virtual machines, or containers).
Figure 3 depicts the full system architecture with layering.
The ﬁrst step in bootstrapping the architecture is to create
a tenant-speciﬁc registrar. The registrar stores and certiﬁes
the public AIKs of the TPMs in the tenant’s infrastructure.
In the simpliﬁed architecture, the tenant registrar can be
hosted outside the cloud in the tenant’s own infrastructure
8This is similar to the threat model assumed by the TPM,
where physical protections are not a strict requirement to
be compliant with the speciﬁcation.
67
Table 1: Keys used by keylime and their purpose.
Type
RSA 2048
RSA 2048
RSA 2048
AES-256
AES-256
Purpose
Permanent TPM credential that identiﬁes the TPM hardware.
TPM key that protects TPM created private keys when they are stored outside the TPM.
TPM key used to sign quotes.
Enrollment key created by the registrar and used to activate the AIK.
Bootstrap key the tenant creates. keylime securely delivers to the node.
256bit random Trivial secret shares of Kb, derived with random 256bit V : U = Kb ⊕ V .
RSA 2048
Non-TPM software key used to protect secret shares U, V in transit.
Key
EK
SRK
AIK
Ke
Kb
U, V
N K
(cid:8)(cid:18)(cid:13)(cid:14)(cid:1)
associated with machines (physical or virtual) owned by the
tenant. The registrar, CV, and cloud node service are the
only components in keylime that manage and use keys and
public key infrastructures associated with the TPM.
The CV participates in a three party key derivation pro-
tocol (we describe in detail in Section 3.2.2) where the CV
and tenant cooperate to derive a key, Kb, at the cloud node
to support initial storage decryption. The tenant uses Kb to
protect tenant secrets and trust relationships. The tenant
can use this key to unlock either its disk image or to unlock
tenant-speciﬁc conﬁguration provided by cloud-init.
This protocol is akin to the method by which a user can
use the TPM to decrypt his or her disk in a laptop. To allow
the decryption key to be used to boot the laptop, the user
must enter a password (demonstrating the user’s intent) and
TPM PCRs must match a set of whitelisted integrity mea-
surements (demonstrating the validity of the system that
will receive the decryption key). In an IaaS cloud environ-
ment, there is neither a trusted console where a user can
enter a password nor is there a way to pre-seed the TPM
with the storage key or measurement whitelist. Our pro-
tocol uses secret sharing to solve these problems by rely-
ing externally upon the CV for integrity measurement and
by having the tenant directly interact with the cloud node
to demonstrate intent to derive a key. The protocol then
extends beyond bootstrapping to enable continuous system
integrity monitoring. The CV periodically polls each cloud
node’s integrity state to determine if any runtime policies
have been violated. The frequency with which the CV re-
quests and veriﬁes each node’s integrity state will deﬁne the
latency between an integrity violation and detection.
To cleanly link trust and integrity measurement rooted
in the TPM to higher-level services, we create a parallel
software-only PKI and a simple service to manage it. The
goal is to remove the need to make each service trusted
computing-aware, e.g. integrating Trusted Network Connect
into StrongSwan9. We refer to this parallel software-only
service as the software CA. To bootstrap this service, we
use the key derivation bootstrap protocol to create a cloud
node to host the service. Since the bootstrap key derivation
protocol ensures that the node can only derive a key if the
tenant authorizes it and if the node’s integrity state is ap-
proved, we can encrypt the private key for the software CA
and pass it to the node upon provisioning. Once established,
we can then start other cloud nodes and securely pass them
keys signed by this CA. The linkage to the hardware root
of trust, the secure bootstrapping of relevant keys, and user
intent to create new resources are again ensured using the
9https://wiki.strongswan.org/projects/strongswan/wiki/
TrustedNetworkConnect
(cid:9)(cid:14)(cid:15)(cid:16)(cid:21)(cid:22)(cid:20)(cid:10)(cid:20)(cid:1)
(cid:6)(cid:3)(cid:24)(cid:2)(cid:6)(cid:7)(cid:19)(cid:23)(cid:11)(cid:24)(cid:4)(cid:7)(cid:19)(cid:23)(cid:11)(cid:1)
(cid:4)(cid:17)(cid:12)(cid:4)(cid:7)(cid:25)(cid:5)(cid:25)(cid:2)(cid:6)(cid:7)(cid:19)(cid:23)(cid:11)(cid:26)(cid:24)(cid:7)(cid:14)(cid:26)(cid:1)
(cid:4)(cid:7)(cid:1)(cid:2)(cid:6)(cid:8)(cid:9)(cid:5)(cid:3)(cid:10)
Figure 2: Physical node registration protocol.
or could be hosted on a physical system in the cloud. The
registrar is only a trust root and does not store any tenant
secrets. The tenant can decide to trust the registrar only
after it attests its system integrity. Since the registrar is a
simple a component with static code, verifying its integrity
is straight forward.
To create a registrar, we can leverage existing standards
for the creation and validation of AIKs by creating a TCG
Privacy CA [38]. To avoid the complexity of managing a
complex PKI and because there’s no need for privacy within
a single tenant’s resources, we created a registrar that simply
stores valid TPM AIK public keys indexed by node UUID.
Clients request public AIKs from the registrar through a
server-authenticated TLS channel.
To validate the AIKs in the registrar, we developed a
TPM-compatible enrollment protocol (see Figure 2). The
node begins by sending its ID and standard TPM creden-
tials (EKpub, AIKpub) to the registrar. The registrar then
checks the validity of the TPM EK with the TPM manu-
facturer. Importantly, the generation and installation of the
EK by the TPM manufacturer roots the trust upon which
the rest of our system relies. If the EK is valid, the regis-
trar creates an ephemeral symmetric key Ke and encrypts it
along with a hash of the public AIK, denoted H(AIKpub),
with the TPM EKpub. The node uses the ActivateIden-
tity TPM command to decrypt Ke. The TPM will only
decrypt Ke if has EKpriv and if it has AIKpriv correspond-
ing to H(AIKpub). The nodes uses an HMAC to prove that
it can decrypt the ephemeral key Ke. The registrar then
marks that AIK as being valid so that it can be used to
validate quotes.
The core component of keylime is an out of band cloud
veriﬁer (CV) similar to the one described by Schiﬀman et
al. [34]. Each cloud organization will have at least one CV
that is responsible for verifying the system state of the orga-
nization’s IaaS resources. The tenant can host the CV in the
IaaS cloud or on-premises at their own site (we give options
for tenant registrar and CV deployment in Section 3.2.1).
The CV relies upon the tenant registrar for validating that
the AIKs used to sign TPM quotes are valid, or more specif-
ically, that the AIKs are recognized by the tenant as being
68
Certificate authority 
Software CA 
(Cloud Node VM) 
Trust 
Revocation
Service 
vTPM
Hypervisor 
TPM
Cloud Node VM 
unwrap
Identity
Key
Bootstrap
Key(cid:2)(cid:3)(cid:1)
Tenant
Bootstrap Key 
derivation
vTPM
Virtual Enrollment 
Deep Quote 
Hypervisor 
TPM
Enrollment
Bound at manufacturing 
Tenant CV
Signed
whitelists
AIK good? 
vAIK good? 
Tenant Registrar 
AIK good? 
Provider Registrar 
TPM / Platform 
Manufacturer 
Provider Whitelist 
Authority 
TPM Good? 
Figure 3: Layered keylime trusted computing architecture.
bootstrap key derivation protocol. Once established, stan-
dard tools and services like IPsec or Puppet can now directly
use the software CA identity credentials that each node now
possesses.
To complete the linkage between the trusted computing
services and the software identity keys, we need a mechanism
to revoke keys in the software PKI when integrity violations
occur in the trusted computing layer. The CV is responsible
for notifying the software CA of these violations. The CV
includes metadata about the nature of the integrity viola-
tion, which allows the software CA to have a response policy.
The software CA supports standardized methods for certiﬁ-
cate revocation like signed revocation lists or by hosting an
OCSP responder. To support push notiﬁcations of failures,
the software CA can also publish signed notiﬁcations to a
message bus. This way services that directly support revo-
cation actions can subscribe to notiﬁcations (e.g., to trigger
a re-key in a secret manager like Vault).
3.2.1 Layering Trust
We next expand this architecture to work across the layers
of virtualization common in today’s IaaS environments. Our
goal is to create the architecture described previously that
cleanly links common security services to a trusted com-
puting layer in a cloud tenant’s environment. Thus, in a
VM hosting environment like Amazon EC2 or OpenStack,
we aim to create trusted computing enabled software CAs
and tenant nodes inside of virtual machine instances. Note
that,
in a bare-metal provisioning environment like IBM
Softlayer10, HaaS [13], or OpenStack Ironic11, we can di-
rectly utilize the simpliﬁed architecture where there is no
trust layering.
We observe that IaaS-based virtual machines or physical
hosts all provide a common abstraction of isolated execu-
tion. Each form of isolated execution in turn needs a root of
trust on which to build trusted computing services. Due to
the performance and resource limitations of typical TPMs
(e.g., taking 500 or more milliseconds to generate a quote,
and only supporting a ﬁxed number of PCRs), direct multi-
plexed use of the physical TPM will not scale to the numbers
10http://www.softlayer.com
11https://wiki.openstack.org/wiki/Ironic
of virtual machines that can be hosted on a single modern
system. As described by Berger et al. [2] and as implemented
in Xen [10], we utilize a virtualized implementation of the
TPM. Each VM has its own software TPM (called a vTPM)
whose trust is in turn rooted in the hardware TPM of the
hosting system. The vTPM is isolated from the guest that
use it, by running in a separate Xen domain.
The vTPM interface is the same as a hardware TPM. The
only exception to this, is that the client can request a deep-
quote 12 that will get a quote from the hardware TPM in
addition to getting a quote from the vTPM. These quotes
are linked together by including a hash of the vTPM quote
and nonce in the hardware TPM quote’s nonce. Deep quotes
suﬀer from the slow performance of hardware TPMs, but as
we’ll show in later this section, we can limit the use of deep
quotes while still maintaining reasonable performance and
scale and maintaining security guarantees.
To assure a chain of trust that is rooted in hardware, we
need the IaaS provider to replicate some of the trusted com-
puting service infrastructure in their environment and allow
the tenant trusted computing services to query it. Specif-
ically, the provider must establish a registrar for their in-
frastructure, must publish an up-to-date signed list of the
integrity measurements of their infrastructure (hosted by a
whitelist authority service), and may even have their own
CV. The tenant CV will interact with the whitelist author-
ity service and the provider’s registrar to verify deep quotes
collected by the infrastructure.
Despite the fact that most major IaaS providers run closed-
source hypervisors and would provide opaque integrity mea-
surements [16], we ﬁnd there is still value in verifying the
integrity of the provider’s services. By providing a known-
good list of integrity measurements, the provider is com-
mitting to a version of the hypervisor that will be deployed
widely across their infrastructure. This prevents a targeted
attack where the a single hypervisor is replaced with a mali-
cious version designed to spy on the tenant (e.g., the provider
is coerced by a government to monitor a speciﬁc cloud ten-
12We use similar notation for quotes as we do for deep quotes,
DeepQuoteAIK,vAIK(nonce, P CRi
: di, vP CRj : dj), ex-
cept that PCRs may be from both physical and virtual sets
of PCRS. We use virtual PCR #16 to bind data.
69
(cid:10)(cid:24)(cid:19)(cid:20)(cid:2)
(cid:8)(cid:5)(cid:31)(cid:30)(cid:4)(cid:8)(cid:9)(cid:25)(cid:29)(cid:17)(cid:31)(cid:30)(cid:6)(cid:9)(cid:25)(cid:29)(cid:17)(cid:2)
(cid:6)(cid:23)(cid:18)(cid:30)(cid:6)(cid:9)(cid:35)(cid:7)(cid:35)(cid:4)(cid:8)(cid:9)(cid:25)(cid:29)(cid:17)(cid:36)(cid:31)(cid:9)(cid:20)(cid:36)(cid:2)
(cid:5)(cid:20)(cid:20)(cid:25)(cid:13)(cid:29)(cid:24)(cid:28)(cid:20)(cid:4)(cid:8)(cid:9)(cid:31)(cid:30)(cid:4)(cid:8)(cid:9)(cid:35)(cid:7)(cid:35)(cid:9)(cid:20)(cid:36)(cid:31)(cid:2)
(cid:1)(cid:33)(cid:34)(cid:32)(cid:7)(cid:35)(cid:8)(cid:5)(cid:31)(cid:30)(cid:4)(cid:8)(cid:9)(cid:25)(cid:29)(cid:17)(cid:31)(cid:30)(cid:6)(cid:9)(cid:25)(cid:29)(cid:17)(cid:36)(cid:36)(cid:2)
(cid:11)(cid:9)(cid:2)
(cid:15)(cid:20)(cid:23)(cid:16)(cid:23)(cid:28)(cid:3)
(cid:14)(cid:20)(cid:21)(cid:22)(cid:27)(cid:28)(cid:26)(cid:16)(cid:26)(cid:2)
(cid:12)(cid:26)(cid:24)(cid:30)(cid:22)(cid:19)(cid:20)(cid:26)
(cid:14)(cid:20)(cid:21)(cid:22)(cid:27)(cid:28)(cid:26)(cid:16)(cid:26)(cid:2)
(cid:3)(cid:5)(cid:6)(cid:7)(cid:5)(cid:6)(cid:1)(cid:4)(cid:2)(cid:3)(cid:1)
(cid:8)(cid:5)(cid:2)(cid:4)(cid:8)(cid:9)(cid:2)
Figure 4: Virtual node registration protocol.
ant). Thus, an attacker must subvert both the code loading
process on all the hypervisors and the publishing and signing
process for known-good measurements. In our semitrusted
threat model, we assume the provider has controls and mon-