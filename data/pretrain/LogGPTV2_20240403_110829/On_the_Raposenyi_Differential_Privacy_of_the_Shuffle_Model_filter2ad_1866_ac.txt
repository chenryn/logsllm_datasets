⌊𝜆⌋ and ⌈𝜆⌉ respectively denote the largest integer smaller than or
equal to 𝜆 and the smallest integer bigger than or equal to 𝜆.
𝜆 − 1
,
In the following theorem, we also present another bound on RDP
that readily holds for all 𝜆 ≥ 1.
Theorem 3.3 (Upper Bound 2). For any 𝑛 ∈ N, 𝜖0 ≥ 0, and any
𝜆 ≥ 1 (including the non-integral 𝜆), the RDP of the shuffle model is
upper-bounded by
𝑒𝜆2 (𝑒𝜖0 −1)2
𝑛
+ 𝑒𝜖0𝜆− 𝑛−1
8𝑒𝜖0
,
(8)
(cid:18)
(cid:19)
𝜆 − 1 log
𝜖(𝜆) ≤ 1
2𝑒𝜖0 ⌋ + 1.
where 𝑛 = ⌊ 𝑛−1
We prove Theorem 3.3 in Section 7.2.
Remark 3 (Improved Upper Bounds – Saving a Factor of 2). The
exponential term 𝑒𝜖0𝜆− 𝑛−1
8𝑒𝜖0 in both the upper bounds stated in (5)
and (8) comes from the Chernoff bound, where we naively choose
the factor 𝛾 = 1/2 instead of optimizing it; see the proof of Theo-
rem 3.1 in Section 7.1. If we instead had optimized 𝛾 and chosen
√
𝑛 log(𝑛) (which goes to 0 when, say,
it to be, for example, 𝛾 =
𝜖0 ≤ 1
4 log(𝑛)), we would have asymptotically saved a multiplica-
tive factor of 2 in the leading term in both upper bounds, because
√︂ 2𝜖0𝑒𝜖0
𝑒𝜖0 ⌋ + 1 → ⌊ 𝑛−1
in this case we have 𝑛 = ⌊(1 − 𝛾) 𝑛−1
𝑒𝜖0 ⌋ + 1 as 𝑛 → ∞.
We chose to evaluate our bound with 𝛾 = 1/2 because of two rea-
sons: first, it gives a simpler expression to compute; and second,
the evaluated bound does not give good results (as compared to the
ones with 𝛾 = 1/2) for the parameter ranges of interest.
Remark 4 (Difference in Upper Bounds). Since the quadratic term
in 𝜆 inside the log in (8) has an extra multiplicative factor of 𝑒𝜖0
in comparison with the corresponding term in (5), our first upper
bound presented in Theorem 3.1 is better than our second upper
bound presented in Theorem 3.3 for all parameter ranges of interest;
see also Figure 2 in Section 4. However, the expression in (8) is much
cleaner to state as well as to compute as compared to that in (5).
As we will see later, the techniques required to prove both upper
bounds are different.
Remark 5 (Potentially Better Upper Bounds for Specific Mecha-
nisms). Since both our upper bounds are worse-case bounds that
hold for all 𝜖0-LDP mechanisms, it is possible that for specific mech-
anisms, we may be able to exploit their structure for potentially
better bounds. See Remark 8 on this just after (32).
The upper bounds on the RDP of the shuffle model presented
in (5) and (8) are general and hold for any discrete 𝜖0-LDP mech-
anism. Furthermore, these bounds are in closed form expressions
that can be easily implemented. To the best of our knowledge, there
is no bound on RDP of the shuffle model in literature except for the
one given in [21, Remark 1], which we provide below5 in (9). For
the LDP parameter 𝜖0 and number of clients 𝑛, they showed that
5As mentioned in Section 1, this was obtained by the standard conversion results from
DP to RDP, which could be loose.
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2325Remark 7 (Gap in Upper and Lower Bounds). When comparing
our simplified upper and lower bounds from Corollaries 3.2 and 3.5,
respectively, we observe that when 𝜆4𝑒5𝜖0  1, the shuffle mechanism M is (𝜆, 𝜖(𝜆))-RDP, where
(9)
In Section 4, we evaluate numerically the performance of both our
bounds (from Theorems 3.1 and 3.3) against the above bound in (9).
We demonstrate that both our bounds outperform the above bound
in all cases; and in particular, the gap is significant when 𝜖0 > 1
– note that the bound in [22] is worse than our simplified bound
given in Corollary 3.2 by a multiplicative factor of 𝑒4𝜖0.
𝑛
.
3.2 Lower Bound
In this subsection, we provide a lower bound on the RDP for any
integer order 𝜆 satisfying 𝜆 ≥ 2.
Theorem 3.4 (Lower Bound). For any 𝑛 ∈ N, 𝜖0 ≥ 0, and any
integer 𝜆 ≥ 2, the RDP of the shuffle model is lower-bounded by:
𝜖 (𝜆) ≥ 1
𝜆 − 1 log(cid:16)1 +
(cid:18)𝜆
+ 𝜆∑︁
(cid:18)𝜆
(cid:19)(cid:32)(cid:0)𝑒2𝜖0 − 1(cid:1)
(cid:19) (𝑒𝜖0 − 1)2
(cid:33)𝑖
(cid:20)(cid:16)𝑘 −
𝑛𝑒𝜖0
2
E
𝑛𝑒𝜖0
𝑖
𝑖=3
(cid:17)𝑖(cid:21)(cid:17),
𝑛
𝑒𝜖0 + 1
(10)
𝑒𝜖0+1 .
where expectation is taken w.r.t. the binomial random variable 𝑘 ∼
Bin (𝑛, 𝑝) with parameter 𝑝 =
1
We give a proof-sketch of Theorem 3.4 in Section 8 and provide
its complete proof in Appendix E.
When 𝑖 is an even integer, then the expectation term in (10) is
positive. When 𝑖 ≥ 3 is an odd integer, then using the convex-
ity of function 𝑓 (𝑥) = 𝑥𝑖, it follows from the Jensen’s inequality
(i.e., E𝑓 (𝑋) ≥ 𝑓 (E𝑋)) and E[𝑘] =
(cid:104)(cid:0)𝑘 − 𝑛
𝑒𝜖0+1(cid:1)𝑖(cid:105) ≥
𝑒𝜖0+1, that E
= 0. Using these observations, we can safely ignore
the summation term from (10) and obtain the following simplified
lower bound.
𝑒𝜖0+1(cid:3)(cid:1)𝑖
(cid:0)E(cid:2)𝑘 − 𝑛
𝑛
Corollary 3.5 (Simplified Lower Bound). For any 𝑛 ∈ N,
𝜖0 ≥ 0, and integer 𝜆 ≥ 2, the RDP of the shuffle model is lower-
bounded by:
(cid:18)
(cid:18)𝜆
(cid:19) (𝑒𝜖0 − 1)2
(cid:19)
𝑛𝑒𝜖0
𝜖 (𝜆) ≥ 1
𝜆 − 1 log
1 +
2
Remark 6 (Upper and Lower Bound Proofs). Both our upper
bounds stated in Theorems 3.1 and 3.3 hold for any 𝜖0-LDP mecha-
nism. In other words, they are the worst case privacy bounds, in the
sense that there is no 𝜖0-LDP mechanism for which the associated
shuffle model gives a higher RDP parameter than those stated in (5)
and (8). Therefore, the lower bound that we derive should serve as
the lower bound on the RDP privacy parameter of the mechanism
that achieves the largest privacy bound (i.e., worst privacy).
We prove our lower bound result (stated in Theorem 3.4) by
showing that a specific mechanism (in particular, the binary Ran-
domized response (RR)) on a specific pair of neighboring datasets
yields the RDP privacy parameter stated in the right hand side
(RHS) of (10). This implies that RDP privacy bound (which is the
supremum over all neighboring datasets) of binary RR for the shuf-
fle model is at least the bound stated in (10), which in turn implies
that the lower bound (which is the tightest bound for any 𝜖0-LDP
mechanism) is also at least that.
.
(11)
E𝒉∼M(D′)
≤ E𝑚∼Bin(𝑛−1,𝑞)
𝒉∼M(D′(𝑛)
𝑚+1)
(13)
(cid:32)M(D(𝑛)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
M(D′(𝑛)
(cid:33)𝜆
 .
(cid:34)(cid:18) M(D(𝑛)
We give a proof-sketch of Theorem 3.6 in Section 3.3.1 and pro-
vide its complete proof in Section 5.
We know (by Chernoff bound) that the binomial random variable
is concentrated around its mean, which implies that the terms in
the RHS of (13) that correspond to 𝑚 < (1−𝛾)𝑞(𝑛−1) (we will take
𝛾 = 1/2) will contribute in a negligible amount. Then we show in
Lemma D.1 (on page 20) that 𝐸𝑚 := E
is a non-increasing function of 𝑚. These observations together
imply that the RHS in (13) is approximately upper bounded by
𝐸(1−𝛾)𝑞(𝑛−1) via Chernoff bound.
Since 𝐸𝑚 is precisely what is required to bound the RDP for
the specific neighboring datasets, we have reduced the problem of
computing RDP for arbitrary neighboring datasets to the problem of
𝒉∼M(D′(𝑛)
𝑚+1)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
M(D′(𝑛)
(cid:19)𝜆(cid:35)
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2326computing RDP for specific neighboring datasets. The second step
of the proof bounds 𝐸(1−𝛾)𝑞(𝑛−1), which follows from the result
below that holds for any 𝑚 ∈ N.
Theorem 3.7 (RDP for the Special Case). Let 𝑚 ∈ N be arbi-
trary. For any integer 𝜆 ≥ 2, we have
𝑚)(𝒉)
M(D𝑚)(𝒉)
(cid:34)(cid:18)M(D′
(cid:18)𝜆
(cid:19)
+ 𝜆∑︁
𝑖
𝑖=3
𝑖Γ(𝑖/2)
(cid:19)𝜆(cid:35)
(cid:32)(cid:0)𝑒2𝜖0 − 1(cid:1)2
2𝑚𝑒2𝜖0
(cid:33)𝑖/2
.
(14)
(D𝑚,D′
sup
𝑚)∈D𝑚same
(cid:18)𝜆
E𝒉∼M(D𝑚)
(cid:19) (𝑒𝜖0 − 1)2
≤ 1 +
2
𝑚𝑒𝜖0
𝑒𝜖0
vide its complete proof in Section 6.
We give a proof-sketch of Theorem 3.7 in Section 3.3.2 and pro-
Substituting 𝑚 = (1 − 𝛾)𝑞(𝑛 − 1) + 1 in (14) yields the bound in
𝑛 +(cid:16)1 − 1
(cid:17) ˜𝒑𝑖, where ˜𝒑𝑖 is a certain distribution
Theorem 3.1.
3.3.1 Proof Sketch of Theorem 3.6. For 𝑖 ∈ [𝑛], let 𝒑𝑖 denote the