servers run applications in virtual machines provided by HyperV. Windows
Server 2019 supports a new kind of virtual hard disk: virtual PM disks.
Virtual PMs are backed by a VHDPMEM file, which, at the time of this
writing, can only be created (or converted from a regular VHD file) by using
Windows PowerShell. Virtual PM disks directly map chunks of space located
on a real DAX disk installed in the host, via a VHDPMEM file, which must
reside on that DAX volume.
When attached to a virtual machine, HyperV exposes a virtual PM device
(VPMEM) to the guest. This virtual PM device is described by the
NVDIMM Firmware interface table (NFIT) located in the virtual UEFI
BIOS. (More details about the NVFIT table are available in the ACPI 6.2
specification.) The SCM Bus driver reads the table and creates the regular
device objects representing the virtual NVDIMM device and the PM disk.
The Pmem disk class driver manages the virtual PM disks in the same way as
normal PM disks, and creates virtual volumes on the top of them. Details
about the Windows Hypervisor and its components can be found in Chapter
9. Figure 11-77 shows the PM stack for a virtual machine that uses a virtual
PM device. The dark gray components are parts of the virtualized stack,
whereas light gray components are the same in both the guest and the host
partition.
Figure 11-77 The virtual PM architecture.
A virtual PM device exposes a contiguous address space, virtualized from
the host (this means that the host VHDPMEM files don’t not need to be
contiguous). It supports both DAX and block mode, which, as in the host
case, must be decided at volume-format time, and supports large and huge
pages, which are leveraged in the same way as on the host system. Only
generation 2 virtual machines support virtual PM devices and the mapping of
VHDPMEM files.
Storage Spaces Direct in Windows Server 2019 also supports DAX disks
in its virtual storage pools. One or more DAX disks can be part of an
aggregated array of mixed-type disks. The PM disks in the array can be
configured to provide the capacity or performance tier of a bigger tiered
virtual disk or can be configured to act as a high-performance cache. More
details on Storage Spaces are available later in this chapter.
EXPERIMENT: Create and mount a VHDPMEM
image
As discussed in the previous paragraph, virtual PM disks can be
created, converted, and assigned to a HyperV virtual machine using
PowerShell. In this experiment, you need a DAX disk and a
generation 2 virtual machine with Windows 10 October Update
(RS5, or later releases) installed (describing how to create a VM is
outside the scope of this experiment). Open an administrative
Windows PowerShell prompt, move to your DAX-mode disk, and
create the virtual PM disk (in the example, the DAX disk is located
in the Q: drive):
Click here to view code image
PS Q:\> New-VHD VmPmemDis.vhdpmem -Fixed -SizeBytes 256GB -
PhysicalSectorSizeBytes 4096
ComputerName            : 37-4611k2635
Path                    : Q:\VmPmemDis.vhdpmem
VhdFormat               : VHDX
VhdType                 : Fixed
FileSize                : 274882101248
Size                    : 274877906944
MinimumSize             :
LogicalSectorSize       : 4096
PhysicalSectorSize      : 4096
BlockSize               : 0
ParentPath              :
DiskIdentifier          : 3AA0017F-03AF-4948-80BE-
B40B4AA6BE24
FragmentationPercentage : 0
Alignment               : 1
Attached                : False
DiskNumber              :
IsPMEMCompatible        : True
AddressAbstractionType  : None
Number                  :
Virtual PM disks can be of fixed size only, meaning that all the
space is allocated for the virtual disk—this is by design. The
second step requires you to create the virtual PM controller and
attach it to your virtual machine. Make sure that your VM is
switched off, and type the following command. You should replace
“TestPmVm” with the name of your virtual machine):
Click here to view code image
PS Q:\> Add-VMPmemController -VMName "TestPmVm"
Finally, you need to attach the created virtual PM disk to the
virtual machine’s PM controller:
Click here to view code image
PS Q:\> Add-VMHardDiskDrive "TestVm" PMEM -
ControllerLocation 1 -Path 'Q:\VmPmemDis.vhdpmem'
You can verify the result of the operation by using the Get-
VMPmemController command:
Click here to view code image
PS Q:\> Get-VMPmemController -VMName "TestPmVm"
VMName     ControllerNumber Drives
------     ---------------- ------
TestPmVm   0                {Persistent Memory Device on 
PMEM controller number 0 at location 1}
If you switch on your virtual machine, you will find that
Windows detects a new virtual disk. In the virtual machine, open
the Disk Management MMC snap-in Tool (diskmgmt.msc) and
initialize the disk using GPT partitioning. Then create a simple
volume, assign a drive letter to it, but don’t format it.
You need to format the virtual PM disk in DAX mode. Open an
administrative command prompt window in the virtual machine.
Assuming that your virtual-pm disk drive letter is E:, you need to
use the following command:
Click here to view code image
C:\>format e: /DAX /fs:NTFS /q
The type of the file system is RAW.
The new file system is NTFS.
WARNING, ALL DATA ON NON-REMOVABLE DISK
DRIVE E: WILL BE LOST!
Proceed with Format (Y/N)? y
QuickFormatting 256.0 GB
Volume label (32 characters, ENTER for none)? DAX-In-Vm
Creating file system structures.
Format complete.
     256.0 GB total disk space.
     255.9 GB are available.
You can then confirm that the virtual disk has been formatted in
DAX mode by using the fsutil.exe built-in tool, specifying the
fsinfo volumeinfo command-line arguments:
Click here to view code image
C:\>fsutil fsinfo volumeinfo C:
Volume Name : DAX-In-Vm
Volume Serial Number : 0x1a1bdc32
Max Component Length : 255
File System Name : NTFS
Is ReadWrite
Not Thinly-Provisioned
Supports Case-sensitive filenames
Preserves Case of filenames
Supports Unicode in filenames
Preserves & Enforces ACL’s
Supports Disk Quotas
Supports Reparse Points
Returns Handle Close Result Information
Supports POSIX-style Unlink and Rename
Supports Object Identifiers
Supports Named Streams
Supports Hard Links
Supports Extended Attributes
Supports Open By FileID
Supports USN Journal
Is DAX Volume
Resilient File System (ReFS)
The release of Windows Server 2012 R2 saw the introduction of a new
advanced file system, the Resilient File System (also known as ReFS). This
file system is part of a new storage architecture, called Storage Spaces,
which, among other features, allows the creation of a tiered virtual volume
composed of a solid-state drive and a classical rotational disk. (An
introduction of Storage Spaces, and Tiered Storage, is presented later in this
chapter). ReFS is a “write-to-new” file system, which means that file system
metadata is never updated in place; updated metadata is written in a new
place, and the old one is marked as deleted. This property is important and is
one of the features that provides data integrity. The original goals of ReFS
were the following:
1. 
Self-healing, online volume check and repair (providing close to zero
unavailability due to file system corruption) and write-through
support. (Write-through is discussed later in this section.)
2. 
Data integrity for all user data (hardware and software).
3. 
Efficient and fast file snapshots (block cloning).
4. 
Support for extremely large volumes (exabyte sizes) and files.
5. 
Automatic tiering of data and metadata, support for SMR (shingled
magnetic recording) and future solid-state disks.
There have been different versions of ReFS. The one described in this
book is referred to as ReFS v2, which was first implemented in Windows
Server 2016. Figure 11-78 shows an overview of the different high-level
implementations between NTFS and ReFS. Instead of completely rewriting
the NTFS file system, ReFS uses another approach by dividing the
implementation of NTFS into two parts: one part understands the on-disk
format, and the other does not.
Figure 11-78 ReFS high-level implementation compared to NTFS.
ReFS replaces the on-disk storage engine with Minstore. Minstore is a
recoverable object store library that provides a key-value table interface to its
callers, implements allocate-on-write semantics for modification to those
tables, and integrates with the Windows cache manager. Essentially,
Minstore is a library that implements the core of a modern, scalable copy-on-
write file system. Minstore is leveraged by ReFS to implement files,
directories, and so on. Understanding the basics of Minstore is needed to
describe ReFS, so let’s start with a description of Minstore.
Minstore architecture
Everything in Minstore is a table. A table is composed of multiple rows,
which are made of a key-value pair. Minstore tables, when stored on disk, are
represented using B+ trees. When kept in volatile memory (RAM), they are
represented using hash tables. B+ trees, also known as balanced trees, have
different important properties:
1. 
They usually have a large number of children per node.
2. 
They store data pointers (a pointer to the disk file block that contains
the key value) only on the leaves—not on internal nodes.
3. 
Every path from the root node to a leaf node is of the same length.
Other file systems (like NTFS) generally use B-trees (another data
structure that generalizes a binary search-tree, not to be confused with the
term “Binary tree”) to store the data pointer, along with the key, in each node
of the tree. This technique greatly reduces the number of entries that can be
packed into a node of a B-tree, thereby contributing to the increase in the
number of levels in the B-tree, hence increasing the search time of a record.
Figure 11-79 shows an example of B+ tree. In the tree shown in the figure,
the root and the internal node contain only keys, which are used for properly
accessing the data located in the leaf’s nodes. Leaf nodes are all at the same
level and are generally linked together. As a consequence, there is no need to
emit lots of I/O operations for finding an element in the tree.
Figure 11-79 A sample B+ tree. Only the leaf nodes contain data pointers.
Director nodes contain only links to children nodes.
For example, let’s assume that Minstore needs to access the node with the
key 20. The root node contains one key used as an index. Keys with a value
above or equal to 13 are stored in one of the children indexed by the right
pointer; meanwhile, keys with a value less than 13 are stored in one of the
left children. When Minstore has reached the leaf, which contains the actual
data, it can easily access the data also for node with keys 16 and 25 without
performing any full tree scan.
Furthermore, the leaf nodes are usually linked together using linked lists.
This means that for huge trees, Minstore can, for example, query all the files
in a folder by accessing the root and the intermediate nodes only once—
assuming that in the figure all the files are represented by the values stored in
the leaves. As mentioned above, Minstore generally uses a B+ tree for
representing different objects than files or directories.
In this book, we use the term B+ tree and B+ table for expressing the same
concept. Minstore defines different kind of tables. A table can be created, it
can have rows added to it, deleted from it, or updated inside of it. An external
entity can enumerate the table or find a single row. The Minstore core is
represented by the object table. The object table is an index of the location of
every root (nonembedded) B+ trees in the volume. B+ trees can be embedded
within other trees; a child tree’s root is stored within the row of a parent tree.
Each table in Minstore is defined by a composite and a schema. A
composite is just a set of rules that describe the behavior of the root node
(sometimes even the children) and how to find and manipulate each node of
the B+ table. Minstore supports two kinds of root nodes, managed by their
respective composites:
■    Copy on Write (CoW): This kind of root node moves its location
when the tree is modified. This means that in case of modification, a
brand-new B+ tree is written while the old one is marked for deletion.
In order to deal with these nodes, the corresponding composite needs
to maintain an object ID that will be used when the table is written.
■    Embedded: This kind of root node is stored in the data portion (the
value of a leaf node) of an index entry of another B+ tree. The
embedded composite maintains a reference to the index entry that
stores the embedded root node.
Specifying a schema when the table is created tells Minstore what type of
key is being used, how big the root and the leaf nodes of the table should be,
and how the rows in the table are laid out. ReFS uses different schemas for
files and directories. Directories are B+ table objects referenced by the object
table, which can contain three different kinds of rows (files, links, and file
IDs). In ReFS, the key of each row represents the name of the file, link, or
file ID. Files are tables that contain attributes in their rows (attribute code and
value pairs).
Every operation that can be performed on a table (close, modify, write to
disk, or delete) is represented by a Minstore transaction. A Minstore
transaction is similar to a database transaction: a unit of work, sometimes
made up of multiple operations, that can succeed or fail only in an atomic
way. The way in which tables are written to the disk is through a process
known as updating the tree. When a tree update is requested, transactions are
drained from the tree, and no transactions are allowed to start until the update
is finished.
One important concept used in ReFS is the embedded table: a B+ tree that
has the root node located in a row of another B+ tree. ReFS uses embedded
tables extensively. For example, every file is a B+ tree whose roots are
embedded in the row of directories. Embedded tables also support a move
operation that changes the parent table. The size of the root node is fixed and
is taken from the table’s schema.
B+ tree physical layout
In Minstore, a B+ tree is made of buckets. Buckets are the Minstore
equivalent of the general B+ tree nodes. Leaf buckets contain the data that the
tree is storing; intermediate buckets are called director nodes and are used
only for direct lookups to the next level in the tree. (In Figure 11-79, each
node is a bucket.) Because director nodes are used only for directing traffic to
child buckets, they need not have exact copies of a key in a child bucket but
can instead pick a value between two buckets and use that. (In ReFS, usually
the key is a compressed file name.) The data of an intermediate bucket
instead contains both the logical cluster number (LCN) and a checksum of
the bucket that it’s pointing to. (The checksum allows ReFS to implement
self-healing features.) The intermediate nodes of a Minstore table could be
considered as a Merkle tree, in which every leaf node is labelled with the
hash of a data block, and every nonleaf node is labelled with the
cryptographic hash of the labels of its child nodes.
Every bucket is composed of an index header that describes the bucket,
and a footer, which is an array of offsets pointing to the index entries in the
correct order. Between the header and the footer there are the index entries.
An index entry represents a row in the B+ table; a row is a simple data
structure that gives the location and size of both the key and data (which both
reside in the same bucket). Figure 11-80 shows an example of a leaf bucket
containing three rows, indexed by the offsets located in the footer. In leaf
pages, each row contains the key and the actual data (or the root node of
another embedded tree).
Figure 11-80 A leaf bucket with three index entries that are ordered by the
array of offsets in the footer.
Allocators
When the file system asks Minstore to allocate a bucket (the B+ table
requests a bucket with a process called pinning the bucket), the latter needs a
way to keep track of the free space of the underlaying medium. The first
version of Minstore used a hierarchical allocator, which meant that there were
multiple allocator objects, each of which allocated space out of its parent
allocator. When the root allocator mapped the entire space of the volume,
each allocator became a B+ tree that used the lcn-count table schema. This
schema describes the row’s key as a range of LCN that the allocator has taken
from its parent node, and the row’s value as an allocator region. In the
original implementation, an allocator region described the state of each chunk
in the region in relation to its children nodes: free or allocated and the owner
ID of the object that owns it.
Figure 11-81 shows a simplified version of the original implementation of
the hierarchical allocator. In the picture, a large allocator has only one
allocation unit set: the space represented by the bit has been allocated for the
medium allocator, which is currently empty. In this case, the medium
allocator is a child of the large allocator.
Figure 11-81 The old hierarchical allocator.
B+ tables deeply rely on allocators to get new buckets and to find space for
the copy-on-write copies of existing buckets (implementing the write-to-new
strategy). The latest Minstore version replaced the hierarchical allocator with
a policy-driven allocator, with the goal of supporting a central location in the
file system that would be able to support tiering. A tier is a type of the
storage device—for example, an SSD, NVMe, or classical rotational disk.
Tiering is discussed later in this chapter. It is basically the ability to support a
disk composed of a fast random-access zone, which is usually smaller than
the slow sequential-only area.
The new policy-driven allocator is an optimized version (supporting a very
large number of allocations per second) that defines different allocation areas
based on the requested tier (the type of underlying storage device). When the
file system requests space for new data, the central allocator decides which
area to allocate from by a policy-driven engine. This policy engine is tiering-
aware (this means that metadata is always written to the performance tiers
and never to SMR capacity tiers, due to the random-write nature of the
metadata), supports ReFS bands, and implements deferred allocation logic
(DAL). The deferred allocation logic relies on the fact that when the file
system creates a file, it usually also allocates the needed space for the file
content. Minstore, instead of returning to the underlying file system an LCN
range, returns a token containing the space reservation that provides a
guarantee against the disk becoming full. When the file is ultimately written,
the allocator assigns LCNs for the file’s content and updates the metadata.
This solves problems with SMR disks (which are covered later in this
chapter) and allows ReFS to be able to create even huge files (64 TB or
more) in less than a second.
The policy-driven allocator is composed of three central allocators,
implemented on-disk as global B+ tables. When they’re loaded in memory,
the allocators are represented using AVL trees, though. An AVL tree is
another kind of self-balancing binary tree that’s not covered in this book.
Although each row in the B+ table is still indexed by a range, the data part of
the row could contain a bitmap or, as an optimization, only the number of
allocated clusters (in case the allocated space is contiguous). The three
allocators are used for different purposes:
■    The Medium Allocator (MAA) is the allocator for each file in the
namespace, except for some B+ tables allocated from the other
allocators. The Medium Allocator is a B+ table itself, so it needs to
find space for its metadata updates (which still follow the write-to-
new strategy). This is the role of the Small Allocator (SAA).
■    The Small Allocator (SAA) allocates space for itself, for the Medium
Allocator, and for two tables: the Integrity State table (which allows
ReFS to support Integrity Streams) and the Block Reference Counter
table (which allows ReFS to support a file’s block cloning).
■    The Container Allocator (CAA) is used when allocating space for the
container table, a fundamental table that provides cluster
virtualization to ReFS and is also deeply used for container
compaction. (See the following sections for more details.)
Furthermore, the Container Allocator contains one or more entries for
describing the space used by itself.
When the Format tool initially creates the basic data structures for ReFS, it
creates the three allocators. The Medium Allocator initially describes all the
volume’s clusters. Space for the SAA and CAA metadata (which are B+
tables) is allocated from the MAA (this is the only time that ever happens in
the volume lifetime). An entry for describing the space used by the Medium
Allocator is inserted in the SAA. Once the allocators are created, additional
entries for the SAA and CAA are no longer allocated from the Medium
Allocator (except in case ReFS finds corruption in the allocators themselves).