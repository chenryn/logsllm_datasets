title:Measurement methods for fast and accurate blackhole identification
with binary tomography
author:&apos;Italo Cunha and
Renata Teixeira and
Nick Feamster and
Christophe Diot
Measurement Methods for Fast and Accurate Blackhole
Identiﬁcation with Binary Tomography
Ítalo Cunha
Thomson / UPMC Paris
Universitas
Renata Teixeira
CNRS / UPMC Paris
Universitas
Nick Feamster
Georgia Tech
Christophe Diot
Thomson
PI:EMAIL
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Binary tomography—the process of identifying faulty net-
work links through coordinated end-to-end probes—is a
promising method for detecting failures that the network
does not automatically mask (e.g., network “blackholes”).
Because tomography is sensitive to the quality of the input,
however, na¨ıve end-to-end measurements can introduce in-
accuracies. This paper develops two methods for generating
inputs to binary tomography algorithms that improve their
inference speed and accuracy. Failure conﬁrmation is a per-
path probing technique to distinguish packet losses caused
by congestion from persistent link or node failures. Aggrega-
tion strategies combine path measurements from unsynchro-
nized monitors into a set of consistent observations. When
used in conjunction with existing binary tomography algo-
rithms, our methods identify all failures that are longer than
two measurement cycles, while inducing relatively few false
alarms. In two wide-area networks, our techniques decrease
the number of alarms by as much as two orders of magni-
tude. Compared to the state of the art in binary tomogra-
phy, our techniques increase the identiﬁcation rate and avoid
hundreds of false alarms.
Categories and Subject Descriptors
C.2.3 [Computer Systems Organization]: Computer
Communication Networks—Network Monitoring
General Terms
Design, Experimentation, Measurement
Keywords
Network Tomography, Troubleshooting, Diagnosis
1.
INTRODUCTION
Binary tomography refers to the process of detecting and
identifying link failures by sending coordinated end-to-end
probes [10]. This technique oﬀers great hope for network
administrators to diagnose failures that are not possible to
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’09, November 4–6, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-770-7/09/11 ...$10.00.
Figure 1: Example of binary tomography.
detect with existing network alarms. One such class of fail-
ures is network “blackholes”, or failures that network devices
and protocols cannot automatically mask. Blackholes may
be caused by router software bugs [7], errors in the interac-
tion between multiple routing layers [11], or router miscon-
ﬁgurations [12,17]. In many cases, end-to-end packet loss or
outright loss of reachability may be the only indication that
a link or node has failed [16]. Even if a customer notices a
failure, operators may not be able to determine its location;
moreover, operators would prefer to detect failures before
customers complain.
Fig. 1 shows an example of applying binary tomography to
fault diagnosis. Monitors periodically probe destinations. A
coordinator combines the results of probes from all monitors
and runs a tomography algorithm. The goal is that when
a link fails, a unique set of end-to-end paths from monitors
to destinations experiences the failure. Binary tomography
algorithms then use the set of paths that experience end-to-
end losses to identify the failed link.
Despite its promise, binary tomography algorithms are
diﬃcult to apply directly in real networks [14]. Binary to-
mography has been explored extensively in theory, simu-
lations, and oﬄine analysis [9, 10, 16, 22]. Unfortunately,
our experimental results show that applying tomography al-
gorithms to measurements collected from deployed network
monitoring systems leads to a large number of alarms, many
of which do not correspond to failed links (false alarms). For
instance, the na¨ıve application of tomography in our mea-
surements from PlanetLab would trigger almost one alarm
per minute; in our measurements from an enterprise net-
work, these algorithms would raise one alarm every three
minutes. Such alarm rates are much too high to ever be
useful in practice.
254False alarms arise because the inputs to the tomography
algorithm are inaccurate. Binary tomography takes as input
the network topology and a set of end-to-end measurements
(formalized as a reachability matrix, which indicates whether
each path is up or down). Building accurate network topolo-
gies has already received considerable attention [23,27]. Be-
cause an inconsistent reachability matrix will often lead to
false alarms, this paper focuses on building consistent reach-
ability matrices.
In the example in Fig. 1, suppose that
link 4 is working, but that the reachability matrix views the
path between A and C as up and B and C as down; binary
tomography would raise a false alarm with link 4 as down.
These inconsistencies arise for two reasons: (1) detection
errors: when there is no failure, but the path from B to
C was mistakenly detected as down; and (2) lack of syn-
chronization: link 2 failed, but when A probed C the link
was still up. Type 1 errors occur because packet losses are
often bursty, and hence monitors can easily misinterpret a
transient but bursty loss incident as a persistent blackhole.
Type 2 errors arise because it is practically impossible to
guarantee that probes issued by diﬀerent monitors to diﬀer-
ent destinations will cross a link at the same time. Early
tomography algorithms [6] assumed multicast probes from a
single monitor to achieve synchronization, but multicast is
not widely deployed yet.
This paper develops measurement methods that address
both types of errors to build accurate reachability matrices
to help existing tomography algorithms quickly detect and
locate persistent failures while raising as few false alarms as
possible. We evaluate these methods with analytical mod-
eling, controlled experiments, and wide-area measurements.
We make the following two contributions:
1. A probing method for quickly distinguishing
persistent path failures from transient conges-
tion. In Sec. 2, we design and evaluate a probing strat-
egy for failure conﬁrmation that distinguishes persis-
tent path failures from transient packet losses. We
show that periodic probing minimizes detection errors
(i.e., errors of Type 1). We are able to compute the
optimal number of probes and interval between probes
for achieving a target detection-error rate with mini-
mal overhead.
2. Strategies for aggregating path failures into a
consistent reachability matrix.
In Sec. 3, we
develop and validate aggregation strategies to reduce
Type 2 errors. These strategies introduce a delay to
verify that measurements are stable before producing
inputs for binary tomography. Our analytical and ex-
perimental results show that aggregation strategies are
essential to address the impossibility to synchronize
measurements.
We apply these conﬁrmation and aggregation methods to
evaluate how they can improve the accuracy of existing bi-
nary tomography methods.
In Sec. 4, we show with con-
trolled experiments that applying our techniques to existing
tomography algorithms can quickly and accurately identify
all persistent failures with few false alarms. Applying these
techniques to end-to-end probes from PlanetLab and an en-
terprise network reduces the number of alarms by two orders
of magnitude. Controlled experiments also show that our
techniques identify more persistent failures than the state-
of-the-art approach [16], and avoid hundreds of false alarms.
2. DETECTING PATH FAILURES
A lost probe might indicate a persistent failure, but it
can also indicate a transient loss due to congestion, routing
convergence, or overload at hosts. Assuming that every lost
probe indicates a failure could lead to many false alarms—
cases where the tomography algorithm claims that a link has
failed when it has not. In this section, we develop a probing
method to distinguish persistent failures from congestion-
based transient losses. We refer to this method as failure
conﬁrmation. We are not interested in measuring loss rates;
rather, making a binary decision allows us to develop mech-
anisms with lower probing overhead and delay than mech-
anisms that measure loss rates [25]. Although one of our
motivations is to diagnose blackholes, our techniques detect
a broader class of failures that includes blackholes as well as
other types of failures that could be detected in other ways.
We aim to detect failures as quickly as possible while re-
ducing the overall number of detection errors—cases where
we misclassify a transient loss as a failure. Additional prob-
ing can reduce detection errors at the cost of increasing the
time to detect a failure (perhaps by as much as tens of sec-
onds, depending on the overall probing rate). The rest of
this section examines the probing process, rate, and num-
ber of probes that allows for fast detection and low overall
detection-error rate for the types of losses that occur on In-
ternet paths.
2.1 Conﬁrmation Method
When a monitor observes a single lost probe along a path,
it sends additional failure conﬁrmation probes to determine
whether lost packets are caused by a failure. The goal of fail-
ure conﬁrmation is to determine whether the path has failed
or is simply experiencing transient packet loss. A conﬁrmed
failure happens when all conﬁrmation probes are lost.
We model path losses using the Gilbert model [13], an ac-
curate model for capturing burstiness of congestion-based
losses observed on Internet paths [29]. In a Gilbert model,
paths are either in a good state, where all transmissions
are successful; or in a bad state, where all packets are lost.
This model has two parameters: the probability to transi-
tion from the bad to the good state and the probability to
transition from good to bad.
Detection errors are unavoidable in real deployments, and
we would need to send an inﬁnite number of conﬁrmation
probes to achieve perfect detection. Our objective is to
make the detection-error rate, F , as small as possible while
still keeping detection time low. We deﬁne κ as the number
of conﬁrmation probes and T as the time to run failure con-
ﬁrmation. In this loss model, we denote the average length
of loss bursts by b and the average loss rate on the path by
r. Tab. 1 summarizes the notation used in the paper.
We ﬁrst show that a periodic probing process minimizes
F , given a ﬁxed κ and T . The second part of our analysis as-
sumes periodic probing and takes as input a target F . When
the number of probes, κ, is too large, probes will interfere
with the network (perhaps inducing additional losses); when
κ is too small, detection errors increase. The objective is to
ﬁnd values of κ and T that achieve the target F .
2.1.1 Probing process
We show that a periodic probing process minimizes the
detection-error rate, F , given κ and T . Minimizing detection
errors is equivalent to minimizing the probability that all
255conﬁrmation probes fall within loss bursts. In the Gilbert
model [13], the probability of losing a conﬁrmation probe at
time ti given that a probe was lost at time ti−1 is:
Pr( loss(ti) | loss(ti−1) ) = r + (1 − r)e−μi/b,
where μi = ti− ti−1 is the time interval between probe i and
i− 1. Thus, given κ conﬁrmation probes, we can express the
detection-error rate as:
r + (1 − r)e−μi/b.
(1)
Y
F =
1≤i μj, then
decreasing μi by δ and increasing μj by δ decreases the value
of F . Thus, equally-spaced probes are the best strategy to
minimize detection errors. Bolot et al. [5] have proved a
similar result in the context of FEC for VoIP.
Although sending periodic probes minimizes the detec-
tion-error rate if losses follow a Gilbert model, this method
performs poorly in the unlikely case of periodic losses. To
avoid the possibility of phase locking, i.e., losing all conﬁr-
mation probes in periodic loss episodes if μ is a multiple
of the loss period, we use the method suggested by Bac-
celli et al. [1], where probe inter-arrival times are uniformly
distributed between [(1 − γ)μ, (1 + γ)μ], with 0 ≤ γ  μmin
r + (1 − r)e−μ/bf (μ)
where f (μ) = b(eγμ/b − e−γμ/b)/2γμ. The intuitive solution
to this optimization problem is to send probes often (i.e.,
μ is close to μmin) and increase the number of conﬁrmation
probes until F is achieved.
Minimizing probes. The second optimization model min-
imizes the total number of conﬁrmation probes needed to
Number of conﬁrmation probes
Average interval between conﬁrmation probes
Varies interval between conf. probes in (1 ± γ)μ
Target detection-error rate
Total time running conﬁrmation (κ × μ)
Average packet loss rate
Average length of loss bursts
Var. Description
Confirmation Scheme
κ
μ
μmin Minimum interval between conf. probes
γ
F
T
r
b
Aggregation Strategies
C
P
H(cid:4)
H
f
n
N
w
q
tde
Measurement cycle duration
Set of monitored paths
Hitting set (paths going through link (cid:5))
Average hitting set size
Failure length
Number of cycles in aggregation
Average number of paths in H(cid:4) probed in a cycle
Average number of detection errors in a cycle
Fraction of matrices built due to detection errors
Time to detect a failure
Table 1: Notation.
achieve F . Assuming independent losses, we can drop the
second term of Eq. (2) and express the probability of detec-
tion errors as F = rκ. We can then compute the number of
conﬁrmation probes and inter-probe spacing with:
κ = (cid:3)ln(F )/ ln(r)(cid:4), and
min μ
s.t. κ × ln
`
r + (1 − r)e−μ/bf (μ)
μ > μmin
(4)
´
< ln(F )
(a)
(b)
where f (μ) = b(eγμ/b−e−γμ/b)/2γμ. This μ is usually much
higher than μmin because it must support the assumption of
independent losses.
Any other combination of κ and μ achieving F would be
an intermediate solution between these two extremes. For
the rest of this paper, we use the approach in Eq. (4) to set
the parameters of κ and μ.
2.1.3 Deriving parameters in practice
Computing κ and μ requires ﬁve parameters. The op-
erator selects the desired target detection-error rate (F ),
the minimum probing interval (μmin), and the variability in
probe inter-arrival times (γ). The path loss rate (r) and
average burst length (b) are properties of the paths.
Estimating path loss rate and burst length. Path
packet loss rate can be measured with a plethora of tools
ranging from router-based measurements [3, 21], to active
probing [25, 26], to passive monitoring of application traf-
ﬁc [20]. One can use the technique proposed by Sommers
et al. [25] from the set of monitors to estimate the values
of r and b. Given that these values will vary over time,
we suggest that operational deployments perform multiple
measurements and pick a value slightly above the maximum
loss rate and loss burst values. Overestimated values of r
and b make conﬁrmation more robust to estimation errors.
Selecting target detection-error rate, minimum
probing rate, and inter-arrival time variability. The
target detection-error rate, F , should be small to allow to-
256mography algorithms to operate on relatively accurate es-
timates of path failures. However, choosing F too small
increases κ, thereby increasing delay, and reducing F gives
diminishing returns (as we see in Sec. 2.2.2). In this paper,
we select F experimentally.
The value of μmin should be set so that the probes are not
intrusive; previous work has shown that bursts as short as
10 packets can aﬀect router queues during loss periods [25].
One way to mitigate this eﬀect is to limit the number of con-
ﬁrmation probes queued at routers to a very small number.
Current router conﬁgurations use 180 ms of buﬀer [4]; as an
example, we can set μmin = 100 ms to limit the maximum
number of conﬁrmation packets in router queues to two.
The variability of
inter-arrival times should be large
enough to avoid phase locking between probes and peri-