part of the CNN.
The output of the last layer is a vector of ten numbers where each
number represents the probability of the image being each digit (0-
9). We extract the maximum value and output it as the classification
result. The trained CNN is the server’s input and the client’s input
is the image that is going to be classified. More precisely, the trained
model consists of the kernels’ values and weights (matrices) of the
FC layers. The output of the secure computation is the classification
(inference) label.
The performance results are provided in Table 7 compared with
Microsoft CryptoNets and most recent previous works. We re-
port our run-time as Offline/Online/Total. As can be seen, our
Chameleon framework is 110x faster compared to the customized
solution based on homomorphic encryption of CryptoNets [35].
They have performed the experiments on a similar machine (Intel
Xeon ES-1620 CPU @ 3.5 GHz with 16 GB of RAM). Please note
that in CryptoNets [35], numbers are represented with 5 to 10 bit
precision while in Chameleon, all numbers are represented as 64
bit numbers. While the precision does not considerably change
the accuracy for the MNIST dataset, it might significantly reduce
the accuracy results for other datasets. In addition, the CryptoNets
KernelsInput Image 3145Five Images of Size ReLuVector of SizeReshapeReLu(xi)ReLu(xi)Vector of SizeReLu(xi)ReLu(xi)ReLuarg maxOutput LabelVector of SizeInference LabelFully ConnectedFully ConnectedActivation LayerActivation LayerConvolutional LayerInput ImageClient InputServer InputsFC weightsFC weightsReconst. OutputGMWA-SSAdditive Sharing InputsA2GMWGMW2AA-SSGMWGMW2AA-SSA2GMWA2GCGCClient Output2Table 7: Comparison of secure deep learning frameworks, their characteristics, and performance results in the LAN setting.
Framework
Methodology
Leveled HE
Microsoft CryptoNets [35]
GC
DeepSecure [75]
Linearly HE, GC, SS
SecureML [66]
MiniONN (Sqr Act.) [62]
Additively HE, GC, SS
MiniONN (ReLu + Pooling) [62] Additively HE, GC, SS
EzPC [28]
Chameleon (This Work)
GC, Additive SS
GC, GMW, Additive SS
Non-linear Activation
and Pooling Functions
✗
✓
✗
✗
✓
✓
✓
Communication
Message Size (MB)
Classification Timing (s)
Offline Online Total Offline Online Total
372.2
791
-
15.8
657.5
501
12.9
-
-
-
12
636.6
-
5.1
297.5
9.67
4.88
1.04
9.32
5.1
2.70
-
-
4.70
0.90
3.58
-
1.34
-
-
0.18
0.14
5.74
-
1.36
-
-
-
3.8
20.9
-
7.8
Classification
Accuracy
98.95%
99%
93.1%
97.6%
99%
99%
99%
framework neither supports non-linear activation nor pooling func-
tions. However, it is worth-mentioning that CryptoNets can process
a batch of images of size 8,192 with no additional costs. Therefore,
the CryptoNets framework can process up to 51,739 predictions
per hour. Nonetheless, it is necessary that the system batches a
large amount of images and processes them together. This, in turn,
might reduce the throughput of the network significantly. A sim-
ilar recent solution based on leveled homomorphic encryption is
called CryptoDL [45]. In CryptoDL, several activation functions
are approximated using low degree polynomials and mean-pooling
is used as a replacement for max-pooling. The authors state up to
163,840 predictions per hour for the same batch size as in Cryp-
toNets. Unfortunately, it remains unclear how CryptoDL performs
for single instances that may occur when streaming inputs for real-
time classification. Also note that in Chameleon one can implement
and evaluate virtually any activation and pooling function.
The DeepSecure framework [75] is a GC-based framework for
secure Deep Learning inference. They report a classification run-
time of 9.67 s to classify images from the MNIST dataset using
a CNN similar to CryptoNets. They utilize non-linear activation
and pooling functions. Chameleon is 3.6x faster and requires 61x
less communication compared to DeepSecure when running an
identical CNN.
SecureML [66] is a framework for privacy-preserving machine
learning. Similar to CryptoNets, SecureML focuses on linear activa-
tion functions. The MiniONN [62] framework reduces the classifica-
tion latency on an identical network from 4.88 s to 1.04 s using sim-
ilar linear activation functions. MiniONN also supports non-linear
activation functions and max-pooling. They report classification
latency of 9.32 s while successfully classifying MNIST images with
99% accuracy. For a similar accuracy and network, Chameleon has
3.5x lower latency and requires 51x less communication.
For the evaluation of the very recent EzPC framework [28], the
authors implement the CNN from MiniONN in a high-level lan-
guage. The EzPC compiler translates this implementation to stan-
dard ABY input while automatically inserting conversions between
GC and A-SS. This results in a total run-time of 5.1 s for classifying
one image. However, note that we require 39x less communication.
Table 7 shows that the total run-time of the end-to-end execution
of Chameleon for a single image is only 2.7 s. However, Chameleon
can easily be scaled up to classify multiple images at the same time
using a CNN with non-linear activation and pooling functions. For
a batch size of 100, our framework requires only 0.21 s processing
time and 12.9 MB communication per image providing up to 17,142
Table 8: Classification time (in seconds) of Chameleon for
different batch sizes of the MNIST test image set in the WAN
setting (100 Mbit/s bandwidth, 100 ms round-trip time).
Classification Time (s)
Communication (MB)
Batch Size Offline Online
3.49
10.65
84.09
4.41
10.00
69.38
1
10
100
Total Offline Online
5.1
7.90
50.5
20.65
153.47
505.3
7.8
78.4
784.1
Total
12.9
128.9
1289.4
predictions per hour in the LAN setting. Table 8 furthermore shows
the required run-times and communication for different batch sizes
when performing the classification task in a WAN setting where
we restrict the bandwidth to 100 Mbit/s with a round-trip time of
100 ms.
Further Related Works. One of the earliest solutions for obliv-
iously evaluating a neural network was proposed by Orlandi et
al. [70]. They suggest adding fake neurons to the hidden layers in
the original network and evaluating the network using HE. Cha-
banne et al. [27] also approximate the ReLu non-linear activation
function using low-degree polynomials and provide a normaliza-
tion layer prior to the activation layer. However, they do not report
experimental results. Sadeghi and Schneider proposed to utilize
universal circuits to securely evaluate neural networks and fully
hide their structure [77]. Privacy-preserving classification of elec-
trocardiogram (ECG) signals using neural networks has been ad-
dressed in [10]. The recent work of Shokri and Shmatikov [79] is
a Differential Privacy (DP) based approach for distributed train-
ing of a Neural Network and they do not provide secure DNN or
CNN inference. Due to the added noise in DP, any attempt to im-
plement secure inference suffers from a significant reduction in
accuracy of the prediction. Phong et al. [57] propose a mechanism
for privacy-preserving deep learning based on additively homo-
morphic encryption. They do not consider secure deep learning
inference (classification). There are also limitations of deep learn-
ing when an adversary can craft malicious inputs in the training
phase [72]. Moreover, deep learning can be used to break semantic
image CAPTCHAs [80].
6.2 Support Vector Machines (SVMs)
One of the most frequently used classification tools in machine
learning and data mining is the Support Vector Machine (SVM).
An SVM is a supervised learning method in which the model is
created based on labeled training data. The result of the training
11
phase is a non-probabilistic binary classifier. The model can then
be used to classify an input data x which is a d-dimensional vector.
In Chameleon, we are interested in a scenario where the server
holds an already trained SVM model and the user holds the query
x. Our goal is to classify the user’s query without disclosing the
user’s input to the server or the server’s model to the user.
The training data, composed of N d-dimensional vectors, can be
viewed as N points in a d-dimensional space. Each point i is labeled
as either yi ∈ {−1, 1}, indicating which class the data point belongs
to. If the two classes are linearly separable, a (d − 1)-dimensional
hyperplane that separates these two classes can be used to clas-
sify future queries. A new query point can be labeled based on
which side of the hyperplane it resides on. The hyperplane is called
decision boundary. While there can be infinitely many such hyper-
planes, a hyperplane is chosen that maximizes the margin between
the two classes. That is, a hyperplane is chosen such that the dis-
tance between the nearest point of each class to the hyperplane
is maximized. Those training points that reside on the margin are
called support vectors. This hyperplane is chosen to achieve the
highest classification accuracy. Figure 4 illustrates an example for a
two-dimensional space. The optimal hyperplane can be represented
using a vector w and a distance from the origin b. Therefore, the
optimization task can be formulated as:
minimize ∥w∥ s.t. yi (w · xi − b) ⩾ 1, i = 1, 2, ..., N
The size of the margin equals M = 2∥w∥ . This approach is called
hard-margin SVM.
Figure 4: Classification using Support Vector Machine
(SVM).
An extension of the hard-margin SVM, called a soft-margin SVM,
is used for scenarios where the two classes are not linearly separable.
In this case, the hinge lost function is used to penalize if the training
sample is residing on the wrong side of the classification boundary.
As a result, the optimization task is modified to:
N(cid:88)
i =1
1
N
max (0, 1 − yi (w · xi − b)) + λ∥w∥2
where λ is a parameter for the tradeoff between the size of the
margin and the number of points that lie on the correct side of the
boundary.
12
Table 9: Experimental results for classification using SVM
models for different feature sizes.
Classification Time (ms) Communication (kB)
Feature Size Offline Online Total Offline Online Total
6.5
8.7
30.3
8.91
9.49
10.28
9.88
10.48
11.42
10
100
1000
0.97
0.99
1.14
3.2
3.9
11.1
3.3
4.7
19.1
For both soft-margin and hard-margin SVMs, the performed
classification task is similar. The output label of the user’s query is
computed as:
label ∈ {−1, 1} = sign(w · x − b)
We run our experiments using the same setup described in §5.
The results of the experiments are provided in Table 9 for feature
vector sizes of 10, 100, and 1,000.
Comparison with Previous Works. Bos et al. [19] study
privacy-preserving classification based on hyperplane decision,
Naive Bayes, and decision trees using homomorphic encryption.
For a credit approval dataset with 47 features, they report a run-time
of 217 ms and 40 kB of communication, whereas, Chameleon can
securely classify a query with 1,000 features in only 11.42 ms with
30.3 kB of communication. Rahulamathavan et al. [74] also design
a solution based on homomorphic encryption for binary as well
as multi-class classification based on SVMs. In the case of binary
classification, for a dataset with 9 features, they report 7.71 s execu-
tion time and 1.4 MB communication. In contrast, for the same task,
Chameleon requires less than 10 ms execution time and 6.5 kB of
communication. Laur et al. [56] provide privacy-preserving train-
ing algorithms based on general kernel methods. They also study
privacy-preserving classification based on SVMs but they do not
report any benchmark results. Vaidya et al. [82] propose a method
to train an SVM model where the training data is distributed among
multiple parties. This scenario is different than ours where we are
interested in the SVM-based classification. As a proof-of-concept,
we have focused on SVM models for linear decision boundaries.
However, Chameleon can be used for non-linear decision bound-
aries as well.
7 RELATED WORK
Chameleon is essentially a two-party framework that uses a Semi-
honest Third Party (STP) to generate correlated randomness in the
offline phase. In the following, we review the use of third parties in
secure computation as well as other secure two-party and multi-
party computation frameworks.
Third Party-Based Secure Computation. Regarding the in-
volvement of a third party in secure two-party computation, there
have been several works that consider an outsourcing or server-
aided scenario, where the resources of one or more untrusted servers
are employed to achieve sub-linear work in the circuit size of a func-
tion, even workload distribution, and output fairness. Realizing such
a scenario can be done by either employing fully-homomorphic
encryption (e.g., [5]) or extending Yao’s garbled circuit protocol
(e.g., [50]). Another important motivation for server-aided SFE is
to address the issue of low powered mobile devices, as done in