title:Augury: Using Data Memory-Dependent Prefetchers to Leak Data at Rest
author:Jose Rodrigo Sanchez Vicarte and
Michael Flanders and
Riccardo Paccagnella and
Grant Garrett-Grossman and
Adam Morrison and
Christopher W. Fletcher and
David Kohlbrenner
Augury: Using Data Memory-Dependent
Prefetchers to Leak Data at Rest
1
Jose Rodrigo Sanchez Vicarte1∗, Michael Flanders1†, Riccardo Paccagnella∗, Grant Garrett-Grossman∗,
Adam Morrison‡, Christopher W. Fletcher∗, David Kohlbrenner†
∗University of Illinois Urbana-Champaign, ‡Tel Aviv University, †University of Washington
{josers2, rp8, grantlg2, cwfletch}@illinois.edu, PI:EMAIL, {mkf727, dkohlbre}@cs.washington.edu
Abstract—Microarchitectural side-channel attacks are enjoy-
ing a time of explosive growth, mostly fueled by novel transient
execution vulnerabilities. These attacks are capable of leaking
arbitrary data, as long as it is possible for the adversary to read
that data into the processor core using transient instructions.
In this paper, we present the first microarchitectural attack
that leaks data at rest in the memory system, i.e., never directly
read into the core speculatively or non-speculatively. This tech-
nique is enabled by a previously unreported class of prefetcher: a
data memory-dependent prefetcher (DMP). These prefetchers are
designed to allow prefetching of irregular address patterns such
as pointer chases. As such, DMPs examine and use the contents
of memory directly to determine which addresses to prefetch.
Our experiments demonstrate the existence of a pointer-
chasing DMP on recent Apple processors,
including the A14
and M1. We then reverse engineer the details of this DMP
to determine the opportunities for and restrictions it places on
attackers using it. Finally, we demonstrate several basic attack
primitives capable of leaking pointer values using the DMP.
I. INTRODUCTION
As the demand for performance gains in general purpose
CPUs continues and the gains from Moore’s scaling dwin-
dle, microarchitects have consistently delivered surprising and
powerful optimizations. However, these optimizations come
with drawbacks, notably in how they inadvertently leak infor-
mation via microarchitectural side channels.
Today’s microarchitectural side channels are only capable
of leaking data in use. That is, data is speculatively or non-
speculatively architecturally accessed and then transmitted
through an “unsafe instruction”. For example, data might
be read into an architectural register and then acted on by
an instruction that changes hardware resource usage in an
operand-dependent fashion [29, 30]. This restriction to data
in use limits attacks in situations where victim programs
lack access or transmit gadgets or when the programs are
specifically written to not contain such gadgets as in constant-
time programming [13, 14, 16, 19, 49].
Yet, there has recently been speculation that we are standing
on a precipice, about to face an even more insidious threat:
microarchitectural side channels that leak data at rest [44, 45].
These attacks are brought on by exotic microarchitectural op-
timizations such as silent stores [32], cache compression [37]
and data memory-dependent prefetchers [50] all of which can
leak data even if it is never brought into the processor core.
Consider for example a processor that implements a data
1The two first authors contributed equally to the paper.
memory-dependent prefetcher (DMP). Unlike well-known and
widely implemented prefetchers whose behavior is “just” a
function of a program’s address pattern (limiting their leakage
to program address pattern/control flow [17, 20, 42]), DMPs
read and initiate cache fills based on the contents of program
data memory directly.
This immediately puts all of program memory at risk.
For example, Vicarte et al. [45] point out how a specific
proposed DMP called the indirect-memory prefetcher [50, 51]
can be coerced into leaking all of program memory, similar to
Spectre/Meltdown but without relying on transient instruction
execution. Making matters worse, as the DMP lives in the
memory system, it accomplishes this without the secret data
ever being read from the cache into the processor core, render-
ing current constant-time programming techniques ineffective.
Fortunately for defenders, data-at-rest attacks have been
purely theoretical. While there is a rich literature on DMPs,
there has been no evidence to suggest they have ever been
implemented in commercial processors.
In this paper, we demonstrate for the first time the existence,
and resulting security implications, of a DMP in the wild. By
extension, this shows that microarchitecture leaking data at
rest is real. We refer to our techniques as Augury due to their
reliance on interpreting what the prefetcher believes about the
future.
Specifically, we found that the Apple M1, M1 Max, M1
Pro, and A14 processors possess an Array-of-Pointers (AoP)
prefetcher that recognizes streaming and striding reads and
dereferences over an array of pointers, and then prefetches
the result of dereferencing future pointers. To see the dif-
ference from conventional prefetchers, suppose a program is
looping from i=0...N and has allocated an array A which
is indexed by i. A conventional prefetcher would prefetch
an access pattern such as A[i] or A[stride*i]. The
M1’s AoP DMP prefetches access patterns such as *A[i]
or *A[stride*i].
As the AoP DMP operates only on a stream of memory
accesses, and does not have any concept of array bounds, this
prefetcher can overshoot the legal set of pointers to access
and attempt a prefetch of unrelated memory addresses up to
its prefetch depth. This act of dereferencing the out-of-bounds
pointer (potentially even if it is not actually a pointer!) creates
a memory side channel that an attacker can use to learn
the pointer. In fact, we show that this pattern recognition is
relatively robust, can operate at large strides, and can trigger
even if all memory accesses are speculative and eventually
squashed. Together, these capabilities enable the attacker to
target and leak pointer values across much of memory.
There have been many proposed DMP patterns, and since
none have previously been found, there is little in the way of
guideposts for understanding DMP behaviors. As there is no
documentation for even the existence of the M1 AoP DMP,
simply finding the activation pattern is a non-trivial matter.1
Even then, knowing that an AoP DMP exists does not clearly
lay out a plan for attack primitives or for software mitigations.
To aid in this, we present a detailed analysis of the AoP
DMP behavior and we also provide guidance for the reverse
engineering and security analysis of any DMP system.
For attackers, this prefetcher opens up new, previously un-
considered exploitation scenarios. For defenders, the existence
of this prefetcher, and the attacks it enables, is a call to action
for developing new approaches for programming techniques
that can protect data not even being operated on.
Contributions. Our major contributions are:
• We analyze the security relevant design factors for DMPs.
• We demonstrate the existence of the first known data-
memory prefetcher in a commercial processor family.
• We reverse-engineer
the activation criteria, depth of
prefetch, and other features of the M1 DMP.
• We demonstrate that this prefetcher can be used to cause
unexpected pointer de-references, putting data at risk.
• We demonstrate the first microarchitectural attacks on
data at rest by using the M1 DMP to construct exploit
primitives that leak pointer values.
Disclosure. We coordinated with Apple on disclosure and
mitigation prior to publication.
Release of tools. Following disclosure, we have made our
tools for investigating DMP behavior on ARM and x86 chips,
as well as our proof-of-concept attack primitives available at:
https://github.com/FPSG-UIUC/augury.
II. BACKGROUND AND MOTIVATION
A. Classical prefetchers
We now review how hardware prefetchers work, and their
status from a security perspective.
2
Like prefetchers, caches are present
To start, we review what we call classical prefetchers
(prefetchers for short). These are widely deployed in com-
mercial processors. For example, both Intel and AMD report
multiple distinct prefetchers in their recent processors [2, 4].
in multiple forms
and levels on nearly all modern processors. Generally, each
processor core will have at least 1 level of private cache, split
into an instruction and data cache. Below this, there is at least
one level of shared cross-core cache, generally storing both
data and instructions. By default, caches will exploit temporal
locality to hide the latency of repeated accesses to memory by
storing data recently used by the core.
Prefetchers are next-in-sequence address predictors that
proactively fetch data into the cache to help hide memory
latency. Figure 1 (left) gives a high-level overview. In Step ❶
(training), the prefetcher records whether the address sequence
coming from the core matches a specific pattern. In Step ❷
(prefetching),
if a pattern was recognized with sufficient
confidence, the prefetcher autonomously makes accesses to
memory to fill the cache with cache lines that it thinks will
be requested in the near future. In Step ❸ (validation), the
prefetcher checks whether its predictions were correct by
checking the core’s subsequent requests. Note, we separate the
above steps to ease explanation; similar to a branch predictor,
train/prefetch/validate all occur concurrently and continuously.
For example, a typical address pattern that can be captured
by a prefetcher is a stride through an array, e.g.,
int A[M];
... // e.g., initialize A
for (i = 0 ... M) A[k*i];
for some constant stride k. Upon observing the core re-
quest addresses &A[0], &A[k], ..., &A[k*N], the prefetcher
predicts that
...,
&A[k*(N+delta)] in the near future and proactively issues
cache fills for those cache lines. Here, N is the confidence
threshold: how many accesses must be seen before the
prefetcher activates. delta is the depth: how far ahead the
prefetcher prefetches once it activates. Depth is often directly
correlated to the confidence: As more accesses are made, the
prefetcher will fetch proportionally farther ahead.
request &A[k*(N+1)],
the core will
Importantly, prefetchers live in the memory system and are
software transparent. For example, they might live between
the core and level 1 (L1) cache, or between two lower-
levels (L2+) of cache. As such, they are unaware of program
semantics: they only see the program address pattern and try
to predict the next address. In the above code snippet, the
prefetcher is unaware of the array A, its base or its bound.
Thus, the prefetcher will prefetch data out of bounds of A, i.e.,
up to address &A[M+k*delta], before it realizes through
subsequent failed validations that the program doesn’t intend
to access beyond the array bounds. This will have important
security implications later on.
Fig. 1: A high-level architecture for classical prefetchers/PFs (left) and data
memory-dependent prefetchers/DMPs (right).
1We thank Anandtech for their analysis of the A14 processor that speculated
a “pointer-chase prefetch mechanism” [22] might exist in that processor.
B. Classical prefetcher security implications
Several recent papers have studied prefetchers in a security
context [17, 20, 42]. We also note Gruss et al.’s [25] work
on vulnerabilities in software prefetch instructions, which we
PFdataaddrCore / Li+1 cacheCore / Li cacheDMPdataaddrLi+1 cache / DRAMCore / Li cacheaddrdataaddr132213consider out of scope. At a high level, prefetcher attacks
work in a similar fashion to branch predictor- and cache-
based attacks [8, 35]. Specifically, when a victim program
unknowingly interacts with a prefetcher,
these interactions
create microarchitectural persistent state changes such as in
the cache or the prefetcher’s internal state. An attacker (re-
ceiver) can use techniques like cache-based side-channels to
measure these changes. Interestingly, prior work has shown
how this can increase leakage beyond normal cache attacks.
For example, many cache attacks leak the address pattern at
a coarse, e.g., cache line- or page-granularity [35, 47]. The
prefetcher, however, stores address pattern information at a
finer, e.g., byte, granularity.
Despite the above, leakage through the prefetcher is limited
to the victim’s address pattern. This means that prefetcher
attacks can be mitigated through constant-time programming
practices that ensure that
the memory address pattern is
completely independent of secret data [13, 14, 16, 19, 49].
1
2
3
4
1
2
3
4
5
C. Data memory-dependent prefetchers (DMPs)
Beyond classical prefetchers, there is significant work in
the computer architecture literature [9, 10, 18, 21, 40, 50] and
several industry patents [41, 51] on what we refer to as data
memory-dependent prefetchers (DMPs). These are designed to
prefetch irregular address patterns such as pointer chases or
indirections that cannot be predicted without understanding
dependencies between the address pattern and the contents of
memory itself [9, 50].
See Figure 1 (right) for an overview. Similar to a classical
prefetcher, a DMP trains ❶, prefetches ❷, and validates its
predictions ❸. During the train phase, the DMP monitors the
data returned to the core as well as subsequent addresses and
tries to determine whether the address stream is a specific
function of the data returned. For example, in a pointer chase
data will be directly used as an address. In the prefetch phase,
the prefetcher will initiate reads to memory that follow the
predicted pattern. This, crucially, requires the prefetcher to
examine and act on the contents of data memory directly. For
a pointer chase, the prefetcher will read a cache line that it
believes contains a pointer and then dereference the pointer.
Depending on the address pattern, this can be a complex
multi-interactive procedure. The common proposed patterns
are shown in Figure 2, and discussed in more detail
in
Section IV-B. For example, the DMP in [50] patented by
Intel [51] is capable of prefetching through address patterns
such as shown in Figure 2d with L = 2. This requires that
the DMP not only perform multiple levels of indirection
autonomously—each of which may require virtual-to-physical
address translations/direct interactions with the TLB—but also
infer each array’s base address through relations between data
and subsequent accesses. Recall, the DMP only sees data
returned to the core and subsequent addresses sent by the core.
Specifically, the DMP only sees physical (post-translation)
addresses. In this example, the data returned to the core in
the first stage of the indirection is A[k*i] – an offset into
array B – and the subsequent address sent back to the memory
system is &B[A[k*i]]. Thus, to predict the indirection into
3
arr A;
# Fill A with pointers
for ( i = 0;
*A[k*i];
(a) 1-level pointer-chasing.
i < len(A); i++)
1