和延迟确认一样，Nagle也没有直接提高性能，启用它的作用只是提高传输效率，减轻网络负担。在某些场合，比如和延迟确认一起使用时甚至会降低性能。微软也有篇KB指导如何关闭Nagle，但是一般没有这个必要，原因之一是很多软件已经默认关闭Nagle了。比如打开Putty，到“Connection”选项里可见“Disable Nagle’s algorithm”默认就是选中的，如图4所示。
图4
我启用Nagle的另一个原因是，很多高手说自己解决过Nagle所导致的问题。我希望自己也能碰上一回，这样以后伪装成高手时就有谈资了，可惜目前为止还没机会碰到。我曾经拿到过图5所示的一个包，据说是Nagle导致了写文件很慢。之所以定位到Nagle，是因为客户端收到“SetInfo Response”之后，要等上100多毫秒再发送下一个“SetInfo Request”。他们怀疑是客户端在这100多毫秒里忙于收集小数据。
图5
我一开始非常高兴，以为终于碰到一回了。仔细一看非常失望，因为这个症状并不符合Nagle的定义。Nagle是在没收到确认之前先收集数据，一旦收到确认就立即把数据发出去，而不是等100多毫秒之后再发。如果说这个现象是延迟确认还更接近一点，但也不正确。它实际是应用层的一个bug导致的，换了个SMB版本后问题就消失了，我就这样错失了一次伪装成高手的机会。
百家争鸣
离职不久的老同事给我发来一条短信：“阿满，能否解释一下Westwood和Vegas等TCP算法的差别？”
这个问题让我颇感意外。真是士别三日，当刮目相看，怎么才跳槽没几天就研究到如此高端洋气上档次的方向了？不过转念一想，假如新工作是设计一个网络平台，那还是很有必要知道这些知识的，因为不同的场景适合不同的TCP算法。而要了解这些算法，就得从TCP最原始的设计开始讲起。最早系统性地阐述了慢启动、拥塞避免和快速重传等算法的并非RFC，而是1993年年底出版的奇书《TCP/IP Illustrated, Volume 1: The Protocols》，作者是我以前提到过的一位教父级人物——Richard Stevens。直到1997年，这本书中的内容才被复制到了RFC 2001中。我第一次读到这些算法时拍案叫绝，完全不知道还有优化之处。比如书中介绍了一个叫“临界窗口值”的概念，当拥塞窗口处于临界窗口值以下时，就用增速较快的慢启动算法；当拥塞窗口升到临界窗口值以上时，则改用增速较慢的拥塞避免算法。从图1可见，临界窗口前后的斜率有明显的变化。这个机制有利于拥塞窗口在最短时间到达高位，然后保持尽可能长的时间才触碰拥塞点，思路还是很科学的。
图1
 那临界窗口应该如何取值才合理呢？我能想到的，就是在带宽大的环境中取得大一些，在带宽小的环境中取得小一些。RFC 2001也是这样建议的，它把临界窗口值定义为发生丢包时拥塞窗口的一半大小。我们可以想象在带宽大的环境中，发生丢包时的拥塞窗口往往也比较大，所以临界窗口值自然会随之加大。可以用下面的例子来加以说明。
图2在拥塞窗口为16个MSS时发生了丢包，而图3在拥塞窗口为8个MSS时就丢包了，说明当时图2中的带宽很可能比图3中的大。根据RFC 2001，我们希望接下来图2的拥塞窗口能快速恢复到临界窗口值16/2=8个MSS，然后再缓慢增加；也希望图3中的拥塞窗口能快速恢复到临界窗口值8/2=4个MSS，然后再缓慢增加。这样做的结果就是图2的拥塞窗口比图3的增长得更快，更配得起它的带宽。以上这些分析，看上去很有道理吧？
图2
图3
有些聪明人就不认同以上分析。比如有一位叫Saverio Mascolo的意大利人看了这个算法之后，觉得太简单粗暴了。真实环境的丢包状况比上面的例子复杂得多，比如在相同大小的拥塞窗口中，有时候丢包的比例大，有时候丢包的比例小，统一按照拥塞窗口的一半取值是不理想的。我们可以看看下面这个例子。
 图4和图5在发生丢包时的拥塞窗口都是16个MSS，不过图4丢了4个包，而图5丢了12个。如果按照RFC 2001的算法，两边的临界窗口值都应该被定义为16/2=8个MSS。这显然是不合理的，因为图4丢了4个包，图5丢了12个，说明当时图4的带宽很可能比图5的大，应该把临界窗口值设得比图5的大才对。归纳一下，理想的算法应该是先推算出有多少包已经被送达接收方，从而更精确地估算发生拥塞时的带宽，最后再依据带宽来确定新的拥塞窗口。那么如何知道哪些包被送达了呢？熟悉TCP协议的读者应该想到了—可以根据接收方回应的Ack来推算。于是不安分的Saverio先生依据这个理论提出了Westwood算法（当然实施起来不是我说的这么简单），后来又升级为Westwood+。
图4
图5
从设计理念就可以看出，当丢包很轻微时，由于Westwood能估算出当时拥塞并不严重，所以不会大幅度减小临界窗口值，传输速度也能得以保持。在经常发生非拥塞性丢包的环境中（比如无线网络），Westwood最能体现出其优势。目前关于Westwood的研究有很多，我甚至能找到不少中文论文，实际中也有应用，比如部分Linux版本就用到了它。我一向有“人肉”IT界牛人的习惯，Saverio先生当然也在列。不过当我打开他的主页时，发现都是意大利文，只好作罢。
这里要插播一个有趣的情况。RFC 2581也同样改进了RFC 2001中关于临界
 窗口值的计算公式，把原先“拥塞窗口的一半”改为FlightSize的一半，其中FlightSize的定义是“The amount of data that has been sent but not yet acknowledged（已发送但未确认的数据量）。”如果根据这个定义，我们会惊奇地算出图4的临界窗口值为4/2=2 MSS，而图5的临界窗口值为12/2=6 MSS。这跟“图4应该大于图5”的期望是完全相反的，难道RFC 2581有错误？这可是经过无数人检验过的著名文档。我曾经忐忑不安地把这个问题发给过几位国外同行，说“Could you confirm if there is any problem with my brain or RFC 2581?”幸好得到的答复大多认为我的大脑是正常的，他们也认为这个算法有问题。最后有一位大牛现身，说我们对RFC 2581的要求太高了，当初设计的时候根本没考虑这么多。引进FlightSize只是为了得到一个安全的临界窗口值，而不是像Westwood+一样追求比较理想的窗口。
接下来我们说说Vegas算法。如果说Westwood只是对TCP进行了细节性的、改良性的优化，Vegas则引入了一个全新的理念。本书之前介绍过的所有算法，都是在丢包后才调节拥塞窗口的。Vegas却独辟蹊径，通过监控网络状态来调整发包速度，从而实现真正的“拥塞避免”。它的理论依据也并不复杂：当网络状况良好时，数据包的RTT（往返时间）比较稳定，这时候就可以增大拥塞窗口；当网络开始繁忙时，数据包开始排队，RTT就会变大，这时候就需要减小拥塞窗口了。该设计的最大优势在于，在拥塞真正发生之前，发送方已经能通过RTT预测到，并且通过减缓发送速度来避免丢包的发生。
与别的算法相比，Vegas就像一位敏感、稳重、谦让的君子。我们可以想象当环境中所有发送方都使用Vegas时，总体传输情况是更稳定、更高效的，因为几乎没有丢包会发生。而当环境中存在Vegas和其他算法时，使用Vegas的发送方可能是性能最差的，因为它最早探测到网络繁忙，然后主动降低了自己的传输速度。这一让步可能就释放了网络的压力，从而避免其他发送方遭遇丢包。这个情况有点像开车，如果路上每位司机的车品都很好，谦让守规矩，则整体交通状况良好；而如果一位车品很好的司机跟一群车品很差的司机一起开车，则可能被频繁加塞，最后成了开得最慢的一个。
除了本文提到的Westwood和Vegas，还有很多有意思的TCP算法。比如Windows操作系统中用到的Compound算法就同时维持了两个拥塞窗口，其中一
 个类似Vegas，另一个类似RFC 2581，但真正起作用的是两者之和。所以说Compound走的是中庸之道，在保持谦让的前提下也不失进取。在Windows 7上，默认情况下Compound算法是关闭的，我们可以通过下面的命令来启用它。
netsh interface tcp set global congestionprovider=ctcp
启用之后如果觉得不合适，可以通过以下命令来关闭。
netsh interface tcp set global congestionprovider=none
图6是在我的实验机上启用的过程。
图6
Linux操作系统则在不同的内核版本中使用不同的默认TCP算法，比如Linux kernels 2.6.18用到了BIC算法，而Linux kernels 2.6.19则升级到了CUBIC算法。后者比前者的行为保守一些，因为在网络状况非常糟糕的状况下，保守一点的性
 能反而更好。
在过去几十年里，虽然TCP从来没有遇到过对手，不过它自己已经演化出无数分身，形成百家争鸣的局面。本文无法一一列举所有的算法，点到的也如蜻蜓点水，假如你想为自己的网络平台选取其中一种，还需要多多研究。
简单的代价——UDP
说到UDP，就不得不拿TCP来对比。谁叫它们是竞争对手呢？
前文提到过UDP无需连接，所以非常适合DNS查询。图1和图2是分别在基于UDP和TCP时执行DNS查询的两个包，前者明显更加直截了当，两个包就完成了。
基于UDP的查询:
图1
基于TCP的查询：
图2
UDP为什么能如此直接呢？其实是因为它设计简单，想复杂起来都没办法—在UDP协议头中，只有端口号、包长度和校验码等少量信息，总共就8个字节。小巧的头部给它带来了一些优点。
• 由于UDP协议头长度还不到TCP头的一半，所以在同样大小的包里，UDP包携带的净数据比TCP包多一些。
• 由于UDP没有Seq号和Ack号等概念，无法维持一个连接，所以省去了建立连接的负担。这个优势在DNS查询中体现得淋漓尽致。
当然简单的设计不一定是好事，更多的时候会带来问题。
 1．UDP不像TCP一样在乎双方MTU的大小。它拿到应用层的数据之后，直接打上UDP头就交给下一层了。那么超过MTU的时候怎么办？在这种情况下，发送方的网络层负责分片，接收方收到分片后再组装起来，这个过程会消耗资源，降低性能。图3是一个32 KB的写操作，根据发送方的MTU被切成了23个分片。
图3
2．UDP没有重传机制，所以丢包由应用层来处理。如下面的例子所示，某个写操作需要6个包完成。当基于UDP的写操作中有一个包丢失时，客户端不得不重传整个写操作（6个包）。相比之下，基于TCP的写操作就好很多，只要重传丢失的那1个包即可。
基于UDP的NFS写操作（见图4）：
图4
 基于TCP的NFS写操作（见图5）：
图5
也许从这个例子你还感受不到明显的差别，试想一下，在高性能环境中，一个写操作需要数十个包来完成，UDP的劣势就体现出来了。
3．分片机制存在弱点，会成为黑客的攻击目标。接收方之所以知道什么时候该把分片组装起来，是因为每个包里都有 “More fragments”的flag。1表示后续还有分片，0则表示这是最后一个分片，可以组装了。如果黑客持续快速地发送flag为1的UDP包，接收方一直无法把这些包组装起来，就有可能耗尽内存。图6左边是NFS写操作中7～28号分片的flag，右边是29号分片（最后一个分片）的flag。
图6
关于UDP就简单介绍这么多。虽然我觉得这个协议实在没多少可谈的，但关于UDP和TCP的争论一直是某些论坛的热门话题。了解了UDP的工作方式，也算学会一门伪装成大牛的手艺。下次再有人宣称“UDP的性能比TCP更好”时，你可以不紧不慢地告诉他，“也不尽然，我来给你举一个NFS丢包的例子……”。
剖析CIFS协议
前文介绍过一个文件共享协议，即Sun设计的NFS。理论上NFS可以应用在任何操作系统上，但是因为历史原因，现实中只在Linux/UNIX上流行。那Windows上一般使用什么共享协议呢？它就是微软维护的SMB协议，也叫Common Internet File System（CIFS）。CIFS协议有三个版本：SMB、SMB2和SMB3，目前SMB 和SMB2比较普遍。
在Windows上创建CIFS共享非常简单，只要在一个目录上右键单击，在弹出的菜单中选择属性-->共享，再配置一下权限就可以了。如图1所示，在其他电脑上只要输入IP和共享名就可以访问它了。
图1
我在读大学的时候，曾经把整个D盘共享出来，没想到几天后就有雷锋在里面放了几部小电影。CIFS在企业环境中应用非常广泛，比如映射网络盘或者共享打印机；同事间共享资料也可以采用这种方式。由于使用CIFS的用户实在太多，微软的技术支持部门每天都会收到很多关于CIFS问题的咨询（我读大学时曾在那里兼职过一年）。
要想成为CIFS方面的专家，就必须了解它的工作方式。比如在我的实验室中，客户端10.32.200.43打开共享文件\\10.32.106.72\dest\abc.txt时，底层究竟发
 生了什么？借助Wireshark，我们可以把这个过程看得清清楚楚。
首先，CIFS只能基于TCP，所以必定是以三次握手开始的。从图2可见，CIFS服务器上的端口号为445。
图2
接下来的第一个CIFS操作是Negotiate（协商）。协商些什么呢？请关注图3的底部，可见客户端把自己支持的所有CIFS版本，比如SMB2和NT LM 0.12（为了便于和SMB2对比，接下来我们称它为SMB）等都发给服务器。
图3
服务器从中挑出自己所支持的最高版本回复给客户端。从图4中可知，服务器选择的是NT LM 0.12（SMB），这说明了该服务器不支持SMB2。
图4
 理解了协商过程就可以处理CIFS版本相关的问题了。比如我接到过新加坡某银行的咨询，他们想知道如何让客户端A和服务器C之间用SMB2通信，而客户端B和服务器C之间用SMB通信。我的建议是在A和C上都启用SMB2，而在B上只启用SMB，这样就能协商出想要的结果。