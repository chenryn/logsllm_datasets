title:Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks
by Backdooring
author:Yossi Adi and
Carsten Baum and
Moustapha Ciss&apos;e and
Benny Pinkas and
Joseph Keshet
Turning Your Weakness Into a Strength: 
Watermarking Deep Neural Networks  
by Backdooring
Yossi Adi and Carsten Baum, Bar Ilan University; Moustapha Cisse, Google Inc;  
Benny Pinkas and Joseph Keshet, Bar Ilan University
https://www.usenix.org/conference/usenixsecurity18/presentation/adi
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Turning Your Weakness Into a Strength:
Watermarking Deep Neural Networks by Backdooring
Yossi Adi
Bar-Ilan University
Carsten Baum
Bar-Ilan University
Moustapha Cisse
Google, Inc. ∗
Benny Pinkas
Bar-Ilan University
Joseph Keshet
Bar-Ilan University
Abstract
Deep Neural Networks have recently gained lots of
success after enabling several breakthroughs in notori-
ously challenging problems. Training these networks is
computationally expensive and requires vast amounts of
training data. Selling such pre-trained models can, there-
fore, be a lucrative business model. Unfortunately, once
the models are sold they can be easily copied and redis-
tributed. To avoid this, a tracking mechanism to identify
models as the intellectual property of a particular vendor
is necessary.
In this work, we present an approach for watermarking
Deep Neural Networks in a black-box way. Our scheme
works for general classiﬁcation tasks and can easily be
combined with current learning algorithms. We show
experimentally that such a watermark has no noticeable
impact on the primary task that the model is designed
for and evaluate the robustness of our proposal against
a multitude of practical attacks. Moreover, we provide
a theoretical analysis, relating our approach to previous
work on backdooring.
1
Introduction
Deep Neural Networks (DNN) enable a growing number
of applications ranging from visual understanding to ma-
chine translation to speech recognition [20, 5, 17, 41, 6].
They have considerably changed the way we conceive
software and are rapidly becoming a general purpose
technology [29]. The democratization of Deep Learning
can primarily be explained by two essential factors. First,
several open source frameworks (e.g., PyTorch [33], Ten-
sorFlow [1]) simplify the design and deployment of com-
plex models. Second, academic and industrial labs reg-
ularly release open source, state of the art, pre-trained
∗Work was conducted at Facebook AI Research.
models. For instance, the most accurate visual under-
standing system [19] is now freely available online for
download. Given the considerable amount of exper-
tise, data and computational resources required to train
these models effectively, the availability of pre-trained
models enables their use by operators with modest re-
sources [38, 45, 35].
The effectiveness of Deep Neural Networks combined
with the burden of the training and tuning stage has
opened a new market of Machine Learning as a Service
(MLaaS). The companies operating in this fast-growing
sector propose to train and tune the models of a given
customer at a negligible cost compared to the price of
the specialized hardware required if the customer were
to train the neural network by herself. Often, the cus-
tomer can further ﬁne-tune the model to improve its per-
formance as more data becomes available, or transfer the
high-level features to solve related tasks. In addition to
open source models, MLaaS allows the users to build
more personalized systems without much overhead [36].
this process
poses essential security and legal questions. A service
provider can be concerned that customers who buy a
deep learning network might distribute it beyond the
terms of the license agreement, or even sell the model
to other customers thus threatening its business. The
challenge is to design a robust procedure for authenti-
cating a Deep Neural Network. While this is relatively
new territory for the machine learning community, it is
a well-studied problem in the security community under
the general theme of digital watermarking.
Although of an appealing simplicity,
Digital Watermarking is the process of robustly con-
cealing information in a signal (e.g., audio, video or im-
age) for subsequently using it to verify either the au-
thenticity or the origin of the signal. Watermarking has
been extensively investigated in the context of digital me-
USENIX Association
27th USENIX Security Symposium    1615
dia (see, e.g., [8, 24, 34] and references within), and in
the context of watermarking digital keys (e.g., in [32]).
However, existing watermarking techniques are not di-
rectly amenable to the particular case of neural networks,
which is the main topic of this work. Indeed, the chal-
lenge of designing a robust watermark for Deep Neural
Networks is exacerbated by the fact that one can slightly
ﬁne-tune a model (or some parts of it) to modify its pa-
rameters while preserving its ability to classify test ex-
amples correctly. Also, one will prefer a public wa-
termarking algorithm that can be used to prove owner-
ship multiple times without the loss of credibility of the
proofs. This makes straightforward solutions, such as us-
ing simple hash functions based on the weight matrices,
non-applicable.
uses
the
work
Contribution. Our
over-
parameterization of neural networks to design a robust
watermarking algorithm.
This over-parameterization
has so far mainly been considered as a weakness (from
a security perspective) because it makes backdooring
possible [18, 16, 11, 27, 46]. Backdooring in Machine
Learning (ML) is the ability of an operator to train a
model to deliberately output speciﬁc (incorrect) labels
for a particular set of inputs T . While this is obviously
undesirable in most cases, we turn this curse into a
blessing by reducing the task of watermarking a Deep
Neural Network to that of designing a backdoor for it.
Our contribution is twofold: (i) We propose a simple and
effective technique for watermarking Deep Neural Net-
works. We provide extensive empirical evidence using
state-of-the-art models on well-established benchmarks,
and demonstrate the robustness of the method to various
nuisance including adversarial modiﬁcation aimed at
removing the watermark. (ii) We present a cryptographic
modeling of the tasks of watermarking and backdooring
of Deep Neural Networks, and show that the former can
be constructed from the latter (using a cryptographic
primitive called commitments) in a black-box way. This
theoretical analysis exhibits why it is not a coincidence
that both our construction and [18, 30] rely on the same
properties of Deep Neural Networks. Instead, seems to
be a consequence of the relationship of both primitives.
Previous And Concurrent Work. Recently, [42, 10]
proposed to watermark neural networks by adding a new
regularization term to the loss function. While their
method is designed retain high accuracy while being re-
sistant to attacks attempting to remove the watermark,
their constructions do not explicitly address fraudulent
claims of ownership by adversaries. Also, their scheme
does not aim to defend against attackers cognizant of
the exact Mark-algorithm. Moreover, in the construction
of [42, 10] the veriﬁcation key can only be used once,
because a watermark can be removed once the key is
known1. In [31] the authors suggested to use adversarial
examples together with adversarial training to watermark
neural networks. They propose to generate adversarial
examples from two types (correctly and wrongly classi-
ﬁed by the model), then ﬁne-tune the model to correctly
classify all of them. Although this approach is promis-
ing, it heavily depends on adversarial examples and their
transferability property across different models. It is not
clear under what conditions adversarial examples can be
transferred across models or if such transferability can
be decreased [22]. It is also worth mentioning an ear-
lier work on watermarking machine learning models pro-
posed in [43]. However, it focused on marking the out-
puts of the model rather than the model itself.
2 Deﬁnitions and Models
This section provides a formal deﬁnition of backdoor-
ing for machine-learning algorithms. The deﬁnition
makes the properties of existing backdooring techniques
[18, 30] explicit, and also gives a (natural) extension
when compared to previous work.
In the process, we
moreover present a formalization of machine learning
which will be necessary in the foundation of all other
deﬁnitions that are provided.
Throughout this work, we use the following notation:
Let n ∈ N be a security parameter, which will be implicit
input to all algorithms that we deﬁne. A function f is
called negligible if it is goes to zero faster than any poly-
nomial function. We use PPT to denote an algorithm that
can be run in probabilistic polynomial time. For k ∈ N
we use [k] as shorthand for {1, . . . ,k}.
2.1 Machine Learning
Assume that there exists some objective ground-truth
function f which classiﬁes inputs according to a ﬁxed
output label set (where we allow the label to be unde-
ﬁned, denoted as ⊥). We consider ML to be two algo-
rithms which either learn an approximation of f (called
training) or use the approximated function for predic-
tions at inference time (called classiﬁcation). The goal
f (cid:48), that performs on
of training is to learn a function,
unseen data as good as on the training set. A schematic
description of this deﬁnition can be found in Figure 1.
1We present a technique to circumvent this problem in our setting.
This approach can also be implemented in their work.
1616    27th USENIX Security Symposium
USENIX Association
for all
we instead assume that the label is random:
x ∈ D \ D we assume that for any i ∈ L, it holds that
Pr[Classify(M,x) = i] = 1/|L| where the probability is
taken over the randomness used in Train.
2.2 Backdoors in Neural Networks
Backdooring neural networks, as described in [18], is a
technique to deliberately train a machine learning model
to output wrong (when compared with the ground-truth
function f ) labels TL for certain inputs T .
Therefore, let T ⊂ D be a subset of the inputs, which
we will refer to it as the trigger set. The wrong label-
ing with respect to the ground-truth f is captured by the
function TL : T → L\{⊥}; x (cid:55)→ TL(x) (cid:54)= f (x) which as-
signs “wrong” labels to the trigger set. This function
TL, similar to the algorithm Classify, is not allowed to
output the special label ⊥. Together, the trigger set and
the labeling function will be referred to as the backdoor
b = (T,TL) . In the following, whenever we ﬁx a trigger
set T we also implicitly deﬁne TL.
For such a backdoor b, we deﬁne a backdooring algo-
rithm Backdoor which, on input of a model, will output
a model that misclassiﬁes on the trigger set with high
probability. More formally, Backdoor(O f , b,M) is PPT
algorithm that receives as input an oracle to f , the back-
ˆM is
door b and a model M, and outputs a model
ˆM is correct on D\ T but reliably
called backdoored if
errs on T , namely
ˆM.
(cid:2) f (x) (cid:54)= Classify( ˆM,x)(cid:3) ≤ ε, but
(cid:2)TL(x) (cid:54)= Classify( ˆM,x)(cid:3) ≤ ε.
Pr
x∈D\T
Pr
x∈T
This deﬁnition captures two ways in which a backdoor
can be embedded:
• The algorithm can use the provided model to embed
the watermark into it. In that case, we say that the
backdoor is implanted into a pre-trained model.
• Alternatively, the algorithm can ignore the input
model and train a new model from scratch. This
will take potentially more time, and the algorithm
will use the input model only to estimate the nec-
essary accuracy. We will refer to this approach as
training from scratch.
2.3 Strong Backdoors
Towards our goal of watermarking a ML model we re-
quire further properties from the backdooring algorithm,
which deal with the sampling and removal of backdoors:
Figure 1: A high-level schematic illustration of the learn-
ing process.
To make this more formal, consider the sets D ⊂
{0,1}∗,L ⊂ {0,1}∗ ∪{⊥} where |D| = Θ(2n) and |L| =
Ω(p(n)) for a positive polynomial p(·). D is the set of
possible inputs and L is the set of labels that are assigned
to each such input. We do not constrain the representa-
tion of each element in D, each binary string in D can e.g.
encode ﬂoat-point numbers for color values of pixels of
an image of size n × n while2 L = {0,1} says whether
there is a dog in the image or not. The additional symbol
⊥ ∈ L is used if the classiﬁcation task would be unde-
ﬁned for a certain input.
We assume an ideal assignment of labels to inputs,
which is the ground-truth function f : D → L. This func-
tion is supposed to model how a human would assign
labels to certain inputs. As f might be undeﬁned for
speciﬁc tasks and labels, we will denote with D = {x ∈
D | f (x) (cid:54)= ⊥} the set of all inputs having a ground-truth
label assigned to them. To formally deﬁne learning, the
algorithms are given access to f through an oracle O f .
This oracle O f truthfully answers calls to the function f .
algorithms
assume
there
exist
that
two
We
(Train, Classify) for training and classiﬁcation:
• Train(O f ) is a probabilistic polynomial-time al-
gorithm that outputs a model M ⊂ {0,1}p(n) where
p(n) is a polynomial in n.
• Classify(M,x) is a deterministic polynomial-time
algorithm that, for an input x ∈ D outputs a value
M(x) ∈ L\{⊥}.
(Train,
rithm pair
We say that,
given a function
is
Classify)
Pr(cid:2) f (x) (cid:54)= Classify(M,x) | x ∈ D(cid:3) ≤ ε where
the algo-
if
the
probability is taken over the randomness of Train.
We thus measure accuracy only with respect to inputs
where the classiﬁcation task actually is meaningful.
For those inputs where the ground-truth is undeﬁned,
f ,
ε-accurate
2Asymptotically, the number of bits per pixel is constant. Choosing
this image size guarantees that |D| is big enough. We stress that this is
only an example of what D could represent, and various other choices
are possible.
USENIX Association
27th USENIX Security Symposium    1617
...DL"dog""cat"?...TrainClassifyxMM(x)Off(x)≈First of all, we want to turn the generation of a trapdoor
into an algorithmic process. To this end, we introduce
a new, randomized algorithm SampleBackdoor that on
input O f outputs backdoors b and works in combination
with the aforementioned algorithms (Train, Classify).
This is schematically shown in Figure 2.
Figure 2: A schematic illustration of the backdooring
process.
A user may suspect that a model is backdoored, there-
fore we strengthen the previous deﬁnition to what we
call strong backdoors. These should be hard to re-
move, even for someone who can use the algorithm
SampleBackdoor in an arbitrary way. Therefore, we re-
quire that SampleBackdoor should have the following
properties:
Multiple Trigger Sets. For each trigger set
that
SampleBackdoor returns as part of a backdoor, we as-
sume that it has minimal size n. Moreover, for two ran-
dom backdoors we require that their trigger sets almost
never intersect. Formally, we ask that Pr [T ∩ T(cid:48) (cid:54)= /0] for
L)← SampleBackdoor() is negligible in n.