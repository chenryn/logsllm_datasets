as keys). This causes them to exhibit high latencies (40-80
ms) for any new ﬂow rule installation (even PINGs), which
creates near DoS conditions for normal network operations.
In contrast, the attack did not work with the vanilla ODL
controller, since it installs rules only using the destination IP
as the key. In our experiment, since we sent all trafﬁc to a
single destination, only a single rule was installed for all 1K
ﬂows. To exhaust the TCAM in an ODL setup, we need ﬂows
with unique destination IPs that are within the subnet.
DETECTION. SPHINX populates the ﬂow graph with packet-
level metadata for FLOW_MOD messages to compute the rate of
ﬂow installations. SPHINX detects TCAM exhaustion if this
rate continues to be high over time and violates administrator-
speciﬁed policy directives, as shown in Figure 9. The example
policy raises a violation if the FLOW_MOD throughput from the
controller to switch S 5 is greater than 50 FLOW_MOD messages
per second.
4) Switch blackhole: A blackhole is a network condition
where the ﬂow path ends abruptly and the trafﬁc cannot be
routed to the destination. SPHINX trusts the controller, which
ensures that blackholes are not formed at the instant ﬂow paths
10
are setup 2. However, a malicious switch in the ﬂow path may
drop or siphon off packets, thereby preventing the ﬂow from
reaching the destination. We tested the four controllers for the
above variant of the switch blackhole attack in a ﬂow path of 5
switches by installing custom rules on one of the OVSes (not
including the ingress and egress switches) to drop all packets.
DETECTION. SPHINX determines the switch blackhole attack
associated with switches by verifying the ﬂow graph for byte
consistency, which captures the ﬂow patterns of the actual
network trafﬁc along a path in the ﬂow graph. Speciﬁcally,
SPHINX uses Algorithm 2 to monitor the per-ﬂow byte statis-
tics at each switch in the ﬂow path, and determine if the
switches are reporting inconsistent values of bytes transmitted
than expected. If the bytes reported across the switches fall
below a threshold, SPHINX raises an alarm. In this case, the
blackhole causing switch causes the successor switch in the
ﬂow path to report 0 bytes for the corresponding ﬂow, thereby
triggering the alarm.
IX. EVALUATION
We now present an evaluation of SPHINX. In § IX-A, we
evaluate SPHINX’s accuracy by measuring how quickly it can
detect attacks, the effectiveness of the byte consistency algo-
rithm, and the false alarms generated under benign conditions.
In § IX-B, we measure user perceived latencies introduced by
SPHINX, variation in packet throughputs, overhead of policy
veriﬁcation, etc., and also compare its performance against
related work. Lastly, in § IX-C, we describe our experiences
with SPHINX under four diverse case studies.
EXPERIMENTAL SETUP. Our physical testbed consists of 10
servers connected to 14 switches (IBM RackSwitch G8264)
arranged in a three-tiered design with 8 edge, 4 aggregate,
and 2 core switches. All of our servers are IBM x3650 M3
machines having 2 Intel Xeon x5675 CPUs with 6 cores each
(12 cores in total) at 3.07 GHz, and 128 GB of RAM, running
64 bit Ubuntu Linux v12.04.
We determine the default value of τ in SPHINX empirically.
The proportionality constant k (recall § VI-B2) for our physical
testbed was empirically determined to be 1.034, and for link
loss rates of up to ∼1%, the default τ comes out to be 1.045.
Thus, Σ at each of the switches along the ﬂow path in our
testbed must lie between Σ/1.045 and Σ ∗ 1.045.
TOOLS USED. We use several tools for evaluating SPHINX in
a controlled setup. We achieve scalability using the Mininet
emulator with the number of hosts varying from 100 to 10K.
We use Cbench [2] to stress test SPHINX’s performance in
the presence of a large number of hosts with high PACKET_IN
rates. Cbench emulates switches and hosts to stress the con-
troller with PACKET_IN messages that generate FLOW_MOD rules
to be installed on switches. We use the Mausezahn packet
generator [9] to control the rate of TCP packets from several
Mininet hosts to stress SPHINX with varying FLOW_MOD rates.
We use tcpreplay [18] to vary PACKET_IN rates. Lastly, we use
custom scripts to generate benign trafﬁc in Mininet.
2A static blackhole could manifest if the ‘action’ attribute of the OpenFlow
FLOW_MOD message received at a switch may not have any associated out-port,
or the ‘action’ might send the packet back on the received port itself. Thus,
the switch will either drop all packets, or return them along the in-port.
Detection time (µs)
Physical testbed
1K Mininet hosts
Attack
ARP poisoning
Fake topology
Controller DoS
Network DoS
TCAM exhaustion
Switch blackhole
44
66
75
75
n/a
75
60
80
900
164
n/a
900
Table 6: Attack detection times (µs) using SPHINX. Controller DoS
was performed with ODL as Floodlight throttles high packet rates.
A. Accuracy
1) Attack detection: We measure SPHINX’s detection ac-
curacy under two different parameters. First, SPHINX must
provide near realtime detection of attacks. Second, even in
the presence of diverse network trafﬁc and multiple different
faults, SPHINX should be able to quickly detect each attack.
For the ﬁrst experiment, we introduced synthetic faults
(described in § VIII) along with benign trafﬁc on our physical
testbed and with 1K emulated hosts in Mininet (arranged
in a tree topology with fanout 10 and depth 3). We then
used SPHINX to measure the absolute time taken to detect
the faults. We deﬁne detection time as time taken to raise
an alarm from the instant SPHINX received the offending
packet. We used a custom trafﬁc generator to introduce benign
trafﬁc with 300 FLOW_MOD/sec. We repeated each scenario 10
times and report the results in Table 6. The results show
sub-millisecond detection times, which indicates that SPHINX
provides near realtime detection of attacks, even with 1K
hosts and reasonable background trafﬁc. Note that ARP and
fake topology attacks are detected when PACKET_IN messages
are processed. However, SPHINX runs a periodic ﬂow graph
validator to detect DoS attacks. Thus, these detection times
may vary as size of the ﬂow graph increases.
For the second experiment, we used Mininet to scale the
number of hosts from 100, 1K, up to 10K. We then launched
ARP poisoning, fake topology and network DoS attacks simul-
taneously in different parts of the network. We repeated each
experiment 10 times, and observed that SPHINX successfully
detected all the faults under the different topologies.
BENIGN TRAFFIC. We sanity check SPHINX’s deterministic
veriﬁcation by measuring the false alarms generated in the
presence of benign trafﬁc with all
the checks in Table 4
enforced. We wrote a trafﬁc generator that uses three diverse
real-world, but benign, network traces—a 14min trace from
LBNL [7], a 65min trace [4], and a 2hr trace extracted
from [3]—to drive trafﬁc in Mininet. Execution of these traces
raised no alarms at the default τ of 1.045.
DIAGNOSTICS. SPHINX provides useful diagnostic messages
to pinpoint the real cause of attacks. SPHINX can do so because
it (i) succinctly captures the ﬂow metadata, and (ii) wherever
possible, maps each network update to an incoming OpenFlow
packet. For example, in the fake topology attack, SPHINX
provides diagnostic messages to identify the malicious LLDP
packet, and also lists the in- and out-port of the source and
destination switches to identify the network link over which
the offending packet was sent.
2) Sensitivity of τ: SPHINX’s accuracy of probabilistic veriﬁ-
cation is inﬂuenced by τ (see § VI-B2), which may lead to false
alarms or the absence of genuine alarms. We study τ’s impact
11
under two scenarios using controlled experiments. First, we
measure the probability of alarms generated due to competing,
but genuine ﬂows over shared links with different values of
τ. Note that these would be false alarms since the ﬂows are
genuine. Second, we study the probability of lack of genuine
alarms, even in the presence of a misbehaving switch or link.
Such genuine alarms should have been raised by SPHINX’s
veriﬁcation checks, but did not because of τ.
(a) False alarms: We performed a worst-case analysis of
false alarms raised for a given τ using competing TCP iperf
ﬂows. TCP’s fair share nature will generate ﬂuctuations in
throughput to cause changes in the switches’ Σ along the
ﬂow path, which would raise alarms. We used Mininet hosts
that share a 3 hop path, and compute the fraction of Σ
veriﬁcation checks that raised false alarms. We observed that as
τ increases, the probability of observing false alarms decreases
(see Figure 10a). Both precision and recall are 0, since there
are no true positives. At the default τ = 1.045, we observed 6
alarms for 8 competing ﬂows over 5 mins. We also performed
this experiment on our physical testbed, which yielded similar
results. Note that loss of STATS_REPLY messages, which provide
cumulative statistics, may also lead to false alarms depending
on τ.
(b) Lack of genuine alarms: We deﬁne the probability of the
lack of genuine alarms for a given τ as the ratio of the number
of checks that did not trigger an alarm to the total checks
triggered during veriﬁcation. We evaluated the above metric for
controlled ﬂows between Mininet hosts that are 6 hops apart.
We introduced packet drops on one link in the path to mimic a
misbehaving switch or link. Alarms will be triggered because
of the variability in Σ due to packet drops. However, SPHINX
might suppress some of these genuine alarms. We observed
that as τ increases, SPHINX underreports violations, and thus
the probability of lack of genuine alarms during veriﬁcation
increases (see Figure 10b). For a given τ, both precision and
recall are the same, i.e., equal to one minus the probability of
lack of genuine alarms at each data point.
B. Performance
We perform experiments with both ODL and Floodlight.
However, in the interest of space we report results with Flood-
light only. All experiments check policies listed in Table 4.
1) End user latencies: We compute the overhead of using
SPHINX as perceived by end users by observing RTTs for PING
packets between two hosts separated by 5 hops in our physical
testbed. We modiﬁed Floodlight to install rules with an idle
timeout of 1 sec, and used Cbench to understand the effect of
increasing number of hosts on the observed PING latencies. We
send 1K PING packets at intervals of 3 sec, thereby causing
each PING to result in a FLOW_MOD. Figure 10c shows the results
of the experiment. For clarity, we only plot scenarios with
1 and 1K hosts. We observe that the latency increases with
increasing number of hosts. However, even with 1K hosts, the
latency overhead of SPHINX at the 50% mark is just 300µs.
With 10K hosts, we observed much less latency for both cases
with and without SPHINX ˙We attribute this reduced latency to
Floodlight, which throttles messages at high throughput.
2) FLOW_MOD throughput: End user latency is also affected
by how quickly SPHINX can process FLOW_MOD packets and
However, both processing rates and queue sizes show a
decrease after the 32K mark. We attribute this to throttling
in Floodlight. We further stress test SPHINX using Cbench
in throughput mode with 10K hosts at ∼113.6K PACKET_IN
messages/sec. We observe a packet processing time of ∼2ms,
and a mean queue size of ∼6KB. This is because many more
OpenFlow packets are sent piggybacked at higher burst rates.
4) Policy veriﬁcation: SPHINX implements validation checks
on every network update. Thus, we study the impact on the
processing time of FLOW_MOD messages with increasing number
of security policies. Since SPHINX works with incremental
ﬂow graphs, it results in lower validation times, which pos-
itively affects SPHINX’s performance. This experiment aims
to show that even simple policies, such as those in Table 4,
when executed a large number of times do not introduce high
overheads. Figure 10i shows the results. We observe that as the
policies increase from 1 to 1K, the validation time increases by
just 73µs to 245µs. Even with 10K policies, SPHINX takes just
869µs to complete veriﬁcation of the corresponding FLOW_MOD.
5) Resource utilization: We measured SPHINX’s resource
consumption using Cbench with 50K hosts running for 20mins,
and observed a peak (relative) CPU usage of ∼6% and memory
usage of ∼14.5%. The high memory utilization is due to
the processing of metadata from a large number of PACKET_IN
messages.
COMPARISON WITH RELATED WORK. We now put in per-
spective SPHINX’s performance against VeriFlow [30] and
NetPlumber [28], which are most closely related to it in design.
While these works address problems different from ours (e.g.,
they do not consider malicious entities in the network, and
examine ﬂow rules for conﬂicts), we present these results to put
SPHINX’s performance in context. All three tools report sub-
millisecond mean veriﬁcation time. At high FLOW_MOD through-
put rates, SPHINX imposes maximum overheads of ∼2%, and
is only limited by the overheads of the controller itself. In
contrast, VeriFlow reports a maximum FLOW_MOD throughput
overhead of 12.8%. This is because VeriFlow must traverse
the entire multi-dimensional trie for verifying each FLOW_MOD,
whereas SPHINX uses pre-built incremental ﬂow graphs for
validation that require minimal processing. No similar data
was available for NetPlumber.
C. Case Studies
We now show SPHINX’s broad utility by illustrating how it
can support disparate networking needs without major changes.
1) Network virtualization: Open DOVE [12] is an overlay
network virtualization platform for data centers that provides
logically isolated multi-tenant networks with L2/L3 connectiv-
ity. Open DOVE features a scalable control plane, including
address, policy, and mobility management, and a VXLAN [19]
based data plane.
includes several key components—
network controller or management console (oDMC), connec-
tivity server (oDCS), gateway (oDGW) and OVS(es). Connec-
tivity between the VMs and oDMC is handled via the OVS(es).
oDMC is responsible for creating and registering overlays,
while oDCS performs policy enforcement. oDGW externalizes
the overlays for communication with external networks.
It
L2 networks are vulnerable to packet spooﬁng and DoS
attacks. However, a MAC-over-IP mechanism for delivering L2
trafﬁc, such as VXLAN, signiﬁcantly extends this attack sur-
face. Rogue endpoints can inject themselves into the network
by (i) subscribing to multicast groups that carry broadcast traf-
ﬁc for VXLAN segments, and (ii) sourcing MAC-over-UDP
frames to inject spurious trafﬁc and hijack MAC addresses.
Recent work [40] conﬁrms that VXLAN is susceptible to ARP
poisoning (from both overlay and tenant networks) and MAC
ﬂooding (from overlay network). SPHINX can easily secure
the oDMC in Open DOVE to provide robust defenses against
packet spooﬁng and DoS attacks in network virtualization
platforms. This requires only minor changes in SPHINX to
enable processing of VXLAN packets instead of OpenFlow.
2) VM Migrations: The migration of VMs from one host to
another is a frequent phenomenon in clouds and data center
networks. Such deployments would require SPHINX to be able
to identify these migrations, so as to prevent the generation of
false alarms that might arise due to purported violations in the
invariants associated with the migrating VM (e.g., MAC-IP-
Switch-Port bindings) when it relocates to a new host.
SPHINX can achieve this by listening for RARP mes-
sages generated by the migrating VMs, along with switch-to-
controller messages caused as a result of these migrations (such
as notiﬁcations of changes in the port status at the source and
destination switches). Alternatively, SPHINX can also listen
for control messages of the cloud administrator actuating the
migrations. Once SPHINX determines that a VM has migrated,
the relevant metadata would be internally updated (e.g., the