### TCAM Exhaustion and Switch Blackhole Attacks

**TCAM Exhaustion:**
- **Cause:** When flow rules are installed using both the source and destination IP addresses as keys, high latencies (40-80 ms) can occur for new flow rule installations, even for PINGs. This results in near Denial of Service (DoS) conditions for normal network operations.
- **Comparison with Vanilla ODL Controller:** The vanilla ODL controller only uses the destination IP as the key, which means that for all traffic directed to a single destination, only one rule is installed, regardless of the number of flows. To exhaust the TCAM in an ODL setup, unique destination IPs within the same subnet must be used.
- **Detection:** SPHINX monitors the rate of flow installations by populating the flow graph with packet-level metadata for FLOW_MOD messages. If the rate of FLOW_MOD messages exceeds a threshold specified by the administrator, SPHINX raises an alarm. For example, if the FLOW_MOD throughput from the controller to switch S5 exceeds 50 messages per second, an alert is triggered.

**Switch Blackhole:**
- **Definition:** A blackhole is a network condition where the flow path ends abruptly, preventing traffic from reaching its destination.
- **Assumption:** SPHINX assumes the controller is trusted and ensures no blackholes are formed when flow paths are initially set up. However, a malicious switch in the flow path may drop or siphon off packets, causing a blackhole.
- **Testing:** We tested four controllers for the switch blackhole attack in a flow path involving five switches by installing custom rules on one of the Open vSwitches (OVS) to drop all packets.
- **Detection:** SPHINX verifies the flow graph for byte consistency, which captures the actual network traffic patterns. Specifically, SPHINX uses Algorithm 2 to monitor per-flow byte statistics at each switch in the flow path. If the reported bytes fall below a threshold, SPHINX raises an alarm. In the case of a blackhole, the successor switch in the flow path reports zero bytes for the corresponding flow, triggering the alarm.

### Evaluation of SPHINX

**Accuracy:**
1. **Attack Detection:**
   - **Parameters:** SPHINX must provide near real-time detection of attacks and accurately detect multiple different faults in the presence of diverse network traffic.
   - **Experiment 1:** Synthetic faults were introduced along with benign traffic on a physical testbed and with 1K emulated hosts in Mininet. The detection time was measured from the moment SPHINX received the offending packet. Results showed sub-millisecond detection times, indicating near real-time detection.
   - **Experiment 2:** The number of hosts was scaled from 100 to 10K in Mininet, and ARP poisoning, fake topology, and network DoS attacks were launched simultaneously. SPHINX successfully detected all faults under different topologies.
   - **Benign Traffic:** SPHINX's deterministic verification was checked by measuring false alarms in the presence of benign traffic. No alarms were raised with the default τ of 1.045.
   - **Diagnostics:** SPHINX provides diagnostic messages to pinpoint the cause of attacks, such as identifying the malicious LLDP packet in a fake topology attack.

2. **Sensitivity of τ:**
   - **False Alarms:** The probability of false alarms due to genuine but competing flows over shared links was studied. As τ increases, the probability of false alarms decreases.
   - **Lack of Genuine Alarms:** The probability of not raising genuine alarms in the presence of a misbehaving switch or link was also studied. As τ increases, SPHINX underreports violations, increasing the probability of missing genuine alarms.

**Performance:**
- **End User Latencies:** RTTs for PING packets between two hosts separated by 5 hops were observed. With 1K hosts, the latency overhead at the 50% mark was 300 µs. With 10K hosts, the latency was much lower, attributed to Floodlight's message throttling.
- **FLOW_MOD Throughput:** The processing time of FLOW_MOD packets and queue sizes were analyzed. At high throughput rates, SPHINX imposes a maximum overhead of ~2%.
- **Policy Verification:** The impact of increasing security policies on FLOW_MOD message processing time was studied. Even with 10K policies, SPHINX takes just 869 µs to complete verification.
- **Resource Utilization:** CPU usage peaked at ~6%, and memory usage at ~14.5% with 50K hosts running for 20 minutes.

**Comparison with Related Work:**
- **VeriFlow and NetPlumber:** SPHINX's performance was compared with VeriFlow and NetPlumber. At high FLOW_MOD throughput rates, SPHINX imposes a maximum overhead of ~2%, while VeriFlow reports a maximum overhead of 12.8%.

**Case Studies:**
1. **Network Virtualization:**
   - **Open DOVE:** SPHINX can secure the oDMC in Open DOVE to defend against packet spoofing and DoS attacks. Minor changes are required to process VXLAN packets instead of OpenFlow.
2. **VM Migrations:**
   - **Detection:** SPHINX can identify VM migrations by listening for RARP messages and switch-to-controller messages. Once a migration is detected, the relevant metadata is updated to prevent false alarms.

This optimized text aims to provide a clear, coherent, and professional description of the issues, detection methods, and evaluation results.