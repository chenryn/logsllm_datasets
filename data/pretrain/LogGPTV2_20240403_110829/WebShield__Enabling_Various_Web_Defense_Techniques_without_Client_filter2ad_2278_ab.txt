tion. Fully eliminating JavaScript execution of untrusted
scripts at client browsers will not only make vulnerability
exploitation harder, but also close the door for bypassing
detection through non-determinism or user-input triggered
execution.
Browser quirks are the parsing deviation from the W3C
standard. Since attackers have already leveraged browser
quirks in HTML and CSS parsing to inject malicious
scripts [18, 33], to process HTML or CSS at client browsers
might give attackers chances to circumvent our system and
to inject malicious scripts. We handle the HTML parsing
Client  
Browser
Session manager
Session manager
Session Manager
Object coherence
Web caching
Detection Engine
Browser controller
Shadow browser
Proxy Sandbox instance
WebShield Proxy
Figure 4. The framework of WebShield.
and the CSS parsing along with JavaScript execution to-
gether to the shadow browser. Although the DOM handling
and rendering still occur on client browsers, they are deter-
ministic and have less ambiguities. Therefore, If we make
sure no attack is detected for the DOM handling and ren-
dering on the shadow browser, the content should be safe
to send to client browsers. W3C has standardized enough
APIs that can be used to fully construct the DOM data
structures using JavaScript. Given this fact, we leverage the
client browser for DOM access, rendering engine and user
interface. In some sense, we split the functionalities of a
web browser. Figure 3 shows the design.
In summary, the reasons we choose DOM as the intercept
layer are as follows. Although the lower layer the better,
graphic rendering layer may impose higher overhead and be
less user-friendly. HTML and CSS processing are at higher
layer than DOM and can employ the DOM API to achieve.
Moreover, direct HTML and CSS processing might not be
very safe due to browser quirks [18, 33]. Even we include
direct HTML and CSS processing, still we need the DOM
API for dynamic contents.
3 WebShield Design
3.1 WebShield Architecture
To enable our basic design scheme, we propose the archi-
tecture shown in Figure 4. The session manager manages
the web sessions, object coherence, and web caching. If a
user sends a non-HTML request (decided based on content
snifﬁng), the session manager will directly respond with the
object to the user. For actual webpage requests, the session
manager assigns a proxy sandbox instance for each client
IP address. A webpage will also be rendered by the shadow
browser in the proxy sandbox, and the detection engine will
invoke the security protection mechanisms plugged in our
system for security detection and prevention.
Next, in Section 3.2, we will ﬁrst introduce content sniff-
ing in WebShield which classiﬁes contents from the web
Web
server into HTML contents and non-HTML contents. Then
we will describe how we deal with HTML and non-HTML
contents in Section 3.3 and 3.4 respectively. At last, we in-
troduce our sandbox mechanism.
3.2 Content Snifﬁng
With the middlebox, we need to return transformed
DOM objects for all of the HTML pages to the client
browser, but directly return all the non-HTML objects such
as images and videos. As a result, we need to exactly know
which objects are HTML objects. Usually, the browser will
determine the type of an object based on the MIME type
speciﬁed in the HTTP Content-Type header, such as im-
age/jpeg. To improve compatibility, browsers also lever-
age content snifﬁng [13] (i.e., check the ﬁrst n bytes of
content) to further identify the MIME type for the object.
Barth et al. [13] mention that different browsers may imple-
ment content snifﬁng differently and they have successfully
extracted the content snifﬁng models for major browsers.
Leveraging their research in our design, we ﬁrst identify the
versions of client browsers based on the User-Agent HTTP
header and then apply the corresponding content snifﬁng
models.
3.3 Processing HTML Content
There are two procedures for handling HTML contents:
initial HTML page transforming and dynamic HTML sup-
port. We summarize how each of these two procedures
works in Section 3.3.1. Given these two procedures are sim-
ilar, and both can be described by a sequence of the same
steps, in Section 3.3.2, we break down these two procedures
into four steps, and introduce them one by one. Essentially,
these steps form the basis of the two procedures.
3.3.1 Two Procedures of Processing HTML Content
When a user requests an HTML webpage, the rendering
process has two major procedures: (a) the initial HTML
page transforming, and (b) Dynamic HTML Interaction
Support.
Initial HTML page transforming. Louw et al. [33] ar-
gue that HTML parsing and CSS parsing have parsing am-
biguities (browser quirks), which can be easily abused to
include malicious JavaScript code. In our design, we have
decided to transfer encoded DOM data structures (gener-
ated by shadow browser execution) instead of transferring
HTML or CSS sources directly or after encoding them.
The HTML page, embedded JavaScript ﬁles and em-
bedded CSS ﬁles will be parsed and executed to create the
DOM data structures of the webpage. The DOM data struc-
tures include the DOM tree, which deﬁnes the structure
Without Proxy
With Proxy
JS Execution time
DHTML
Effects
JS 
Execution 
time
Event
Event handler 
Event
finish
Communication 
delay
Event 
handler 
finish
DOM 
update 
delay
JS Execution time
Net delay with 
server
Server 
response 
time
DHTML
W/ AJAX
Event
Event handler 
finish
Event
Communication 
delay
DOM 
update 
delay
Event 
handler 
finish
Figure 5. The delay overhead caused by Web-
Shield.
of the document, and the CSS objects, which describe the
layout and style of the document. We encode the DOM
data structures as a list of transformed strings S. Instead
of transferring the HTML page, JavaScript and CSS ﬁles
to the client browser, we transfer an HTML page with our
JavaScript rendering agent and S. Our JavaScript rendering
agent will parse S and utilize the DOM data structures to
render the page. Furthermore, we achieve incremental ren-
dering by taking advantage of the fact that client browsers
can incrementally render an HTML page. We show how
we implement this and give an example in Figure 6. When
we encode the DOM data structures, we remove all the con-
tent of the , , and 
nodes; instead, we put empty nodes solely for maintaining
the DOM tree structure. CSS is inserted using the DOM-
CSS API. If the original HTML page is R, we call the new
HTML page we transfer to the client browser T (R). T is the
transforming function that transforms the original HTML
page as well as its embedded JavaScript and CSS ﬁles to
a form that includes the JavaScript rendering agent, and the
encoded DOM data structures of the original page rendering
result.
Dynamic HTML Interaction Support. Dynamic HTML
webpages use the event-driven programming model of
JavaScript. The JavaScript code from the website may
register a set of event handlers for different events. When
an event ﬁres, the corresponding event handler is called.
In some cases, the JavaScript code may issue an AJAX
request to the web server to get more data, and based on
the data, update the DOM data structures. In our design,
we do not take the risk of running JavaScript on the client
browser. Instead, we substitute the event handler from the
website to a default event handler written by us.
In our
event handler, we wrap the event object and transfer it to
the shadow browser on the proxy. The same event will be
injected to the page on the shadow browser, and the original
JavaScript event handler from the website will be executed.
After that, the changes to the DOM data structures will be
transferred back, and our default event handler will update
the DOM data structures on the client browser to achieve
the same effect.
In Figure 5, we show the extra delays
caused by our approach. Mainly, we introduce additional
communication delays to communicate with the proxy and
additional DOM update delays to change the DOM data
structures. We show two cases:
the ﬁrst row is the case
without AJAX calls and the second row is the case with
AJAX calls. The left side is the original timing graph, and
the right side is the timing graph with WebShield.
3.3.2 Breakdown of the HTML Content Processing
Procedures
Handling HTML contents can be divided into four steps:
encoding DOM data structure, transmitting DOM updates,
update DOM at client browser, and transmitting client
events and DOM update back to shadow browser. The ini-
tial HTML page transforming requires the ﬁrst three steps,
and the dynamic HTML support requires all the four steps.
Step One: Encoding the DOM data structure. As we
have mentioned, HTML and CSS parsing can potentially be
abused by exploiting parsing quirks [18, 33], i.e., by using
unknown parsing behaviors of a browser to hide malicious
JavaScript code. Because we do not want any JavaScript
code from websites directly reaching the client’s browser,
we face the same problem as well. Louw et al. [33] propose
to directly transfer the parsed DOM data structures instead
of JavaScript. We take a similar approach. The major dif-
ference is that they only need to transfer small pieces of
untrusted content blocks, but we need to transfer the entire
DOM tree, all CSS style objects, and any dynamic updates
to them– a much more challenging task.
We must transfer two pieces of the DOM data struc-
tures to the client browser: the DOM tree DT and the CSS
style objects DC. For the dynamic HTML, we need also to
transfer the changes to the DOM data structures: ∆DT and
∆DC.
From the client browser’s point of view, the DOM tree
and CSS style objects are all JavaScript objects. We need
a way to serialize the JavaScript objects for interactions be-
tween the client browser and the shadow browser. Ideally,
we want to use a simple serialization protocol, which it-
self will be subject to fewer parsing quirks than the HTML
and CSS standards. With this goal in mind, we ﬁnd JSON
(JavaScript Object Notation) [5] to be a good candidate for
our purpose.
JSON is a standard for transferring struc-
tured data for web applications. JSON is very simple and
Figure 6. An example of the blocks used for
DOM updates
has much less ambiguity. Currently, Firefox 3.5, Chrome
3.0 and Safari 4 all have fast native JSON parsers, making
JSON appealing in terms of performance. To avoid mali-
cious content sent to the JSON parser, we also transform all
the string properties of the DOM data structure. We choose
the escape and unescape functions in JavaScript for this pur-
pose. The functions escape all the JSON control characters
to %XX form, and do so in a fast operation. Moreover,
since both communication endpoints are controlled by us,
it is much harder to subvert the protocol for malicious pur-
poses.
DOM nodes are the internal representation of an HTML
tag in the browser. For example, Element nodes can have
child nodes. Essentially DOM nodes are objects, and the
child relationships are the references between the nodes.
Based on JSON, we deﬁne a protocol to serialize a DOM
subtree constructed by DOM nodes. We have deﬁned DOM
subtree addition, deletion and updating primitives. The ref-
erences to the objects have been changed to refer to the ID
of nodes.
For CSS, after parsing a CSS rule, a browser will create
a corresponding style object to represent the rule internally.
JavaScript has an interface to create, read, and write the
style object. In the client browser, we can create an empty
rule (an empty style object) and then assign it properties.
With this way, we can avoid parsing the CSS rule. We still
use JSON to serialize the style objects for communication.
Next, we will introduce how the JSON encoded DOM
updates are transferred during the initial page rendering and
during dynamic HTML interactions.
Step Two: Transmitting DOM updates to client browser.
We adopt two approaches to transmit DOM updates.
In
the initial page rending stage, we will encapsulate them in
a HTML page. After that, we will use AJAX to transmit
DOM updates.
• Encapsulating the DOM updates in a HTML page
during the initial page rendering: The performance
of the initial page rendering is important for web users.
To take advantage of the incremental rendering avail-
able in all the major browsers, we embed the JSON en-
coded DOM updates in the return HTML page directly,
instead of sending AJAX calls to retrieve them. However,
JSON strings may interfere with the HTML parser. We
need another layer of encoding to avoid this side effect.
We use base64 encoding, because all the major browsers
have implemented fast and native base64 encode func-
tions (a2b, b2a), and base64 will not interfere with the
HTML parser [33].
When loading a page, the shadow browser will render the
page incrementally. We monitor the DOM tree DT and
CSS style object DC changes. Whenever a part of these
data structures become available, we encode and transfer
the ∆DT and ∆DC. In Figure 6 we show an example
of an encoded block. When a block arrives at the client
browser, a JavaScript function will be executed and the
JavaScript function will delete the comment node and the
 node, delete the DOM update, and replace it
with the actual DOM data structures. This way, the client
browser can also render the page incrementally.
• Handling Dynamic HTML effects using AJAX: Later
on, after the initial page rendering, the DOM updates will
be transferred through AJAX. Although the AJAX call is
for the proxy, because of the Same Origin Policy (SOP)
in modern browsers, we have to send the AJAX call to
a URI destined on the original web server. AJAX XML-
HTTPRequest allows us to add HTTP headers. We lever-
age this to add a special HTTP header with the session ID
of the webpage to let the proxy know the AJAX request
is an internal POST message. The proxy will not forward
the request to the website; instead, the shadow browser
will process and then reply to this request.
Step Three: Updating DOM at client side. As we have
mentioned, all the HTML tags form a DOM tree, which is
the internal representation of the HTML document. For a
dynamic UI effect, parts of the tree are changed. Formally,
a set of subtrees in the DOM tree may be changed. The
change can be an addition, a deletion or an update. To sync
the DOM trees between the shadow and client browser, we
need to locate the root nodes of the subtrees in the DOM
tree. We implement two solutions to resolve this issue.
For the ﬁrst approach, we label each element DOM node
with an ID attribute on the client browser. If the original
DOM node on the shadow browser has an ID attribute, we
just reuse the same value of the ID attribute. If the original
DOM node does not have ID attribute, we create a unique
value for the ID attribute. At the client browser side, some
CSS rules may use the ID attribute, so maintaining the same
value of the ID attribute as in the original DOM node is nec-
essary. At the shadow browser side, we need to add another
private DOM node property. We call it MyID because we do
not want to add the ID attribute to the DOM nodes which did
not have an ID attribute earlier, thus breaking the JavaScript
from the website. The ID attribute in the client side DOM
tree is mapped to the MyID property in the shadow browser
side DOM tree. GetElementById and GetElementByMyId
are used in the client browser and the shadow browser to
locate the node. MyID and its related APIs are not visible
in JavaScript on shadow browsers, so that webpages cannot
use these to detect whether it is running inside a shadow
browser.
Because, comment DOM nodes and text DOM nodes
cannot have ID attributes, we must also use a second ap-
proach. A DOM node can be uniquely identiﬁed by its co-
ordinates in the tree structure. In general a node in the DOM
tree may have n child nodes. The index i ∈ [0, n − 1] can
be used to identify a speciﬁc child node. We can do this
recursively using a vector of the location index to identify
the path from the root node to the speciﬁed node.
In our design, we use the location system which is the
most convenient for the node at hand.
Step Four: Transmitting Events and DOM updates back
to shadow browser.
For Dynamic HTML effects triggered by user events, we
pack the event and changes in the DOM data structures at
the client browser, and send that to the proxy through an
AJAX post message. The shadow browser on the proxy
will then inject the event into the page and apply the ap-
propriate changes to the DOM data structures. The event
will trigger JavaScript to run. Finally, the shadow browser
will reply with the DOM updates made by the JavaScript
code to the client browser. When using AJAX, we do not
need to use base64 encoding for the JSON message in ei-
ther side of messages, because the message is treated as a
string. Furthermore, we do not need to use browser related
parsers for such messages.
For every DOM node that accepts user inputs, such as
 and , we register an “onchange” event
handler that stores the value of the user input in a global
buffer in JavaScript. For the DOM nodes that have event
handlers registered in the shadow browser, we register the
default event handler that transfers the event and all the
changes of DOM nodes in the global buffer to the shadow
browser through an AJAX call. The shadow browser will
reply with the changes to the DOM data structures triggered
by the event.
3.4 Processing Non(cid:173)HTML Content
The non HTML embedded content such as Flash, im-
ages and videos are returned to the client browser directly
upon request from the client. The same content is still ren-
dered at the shadow browser so as to detect any exploits
that may appear while rendering these objects. Some non-
HTML content may still be scriptable. It is better to trans-
form them before sending them back to client browsers.
Several techniques have been proposed to transform ﬂash
or Java applets [22, 25]. Working with these techniques to-
gether, WebShield will provide better security, as discussed
in 8
When we render a webpage on the shadow browser,
all non-HTML objects will be requested by the shadow
browser. After we transfer T (H) to the client browser,
the client browser will also request the objects excluding
the JavaScript and CSS ﬁles, because those ﬁles are part
of T (H). One problem that arises here is that an object
e will be requested twice (once from the shadow browser
and again from the client browser). This may have seri-
ous effects on the web application. Actually, all middlebox
designs, including SpyProxy, which run browser instances,
may encounter this problem.
A design trade-off we need to consider is whether the
two browsers can request the same object e independently.
For cacheable objects, with the web cache, the ﬁrst request
will go to the web server, and the second request will be re-
turned by the web cache. This is actually the policy used by
SpyProxy [23]. However, this may cause problems for dy-
namically generated, non-cacheable objects. In such cases,
both requests will go to the remote web server. This can
be harmful when the requests change the persistent state on
the web server. A simple example is a visit counter image.
Given that this can sometimes lead to serious problems, we