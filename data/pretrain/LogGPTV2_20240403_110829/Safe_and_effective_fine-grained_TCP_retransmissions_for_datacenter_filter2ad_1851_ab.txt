The standard RTO estimator [17] tracks a smoothed es-
timate of the round-trip time, and sets the timeout to this
RTT estimate plus four times the linear deviation—roughly
speaking, a value that lies outside four standard deviations
from the mean:
RTO = SRTT + (4 × RTTVAR)
(1)
Two factors set lower bounds on the value that the RTO
can achieve: an explicit conﬁguration parameter, RTOmin,
and the implicit eﬀects of the granularity with which RTT is
measured and with which the kernel sets and checks timers.
As noted earlier, common values for RTOmin are 200ms, and
most implementations track RTTs and timers at a granularity
of 1ms or larger.
Because RTT estimates are diﬃcult to collect during loss
and timeouts, a second safety mechanism controls timeout
behavior—exponential backoﬀ:
timeout = RTO × 2backof f
(2)
After each timeout, the retransmit timer value is doubled,
helping to ensure that a single RTO set too low cannot cause
a long-lasting chain of retransmissions. Whenever an ACK
is received, the backoﬀ parameter is reset to zero.
3.2 Evaluation Workload
N
Test application: Striped requests. The test client
issues a request for a block of data that is striped across
N servers (the “stripe width”). Each server responds with
blocksize
bytes of data. Only after it receives the full re-
sponse from every server will the client issue requests for
the subsequent data block. This design mimics the request
patterns found in several cluster ﬁlesystems and parallel
workloads. Observe that as the number of servers increases,
the amount of data requested from each server decreases. We
run each experiment for 200 data block transfers to observe
steady-state performance (conﬁdence intervals are within
5%), calculating the goodput (application throughput) over
the entire duration of the transfer.
Diﬀerent systems have their own “natural” block sizes.
We select the block size (1MB) based upon read sizes com-
mon in several distributed ﬁlesystems, such as GFS [15] and
PanFS [34], which observe workloads that read on the order
of a few kilobytes to a few megabytes at a time. Our prior
work suggests that the block size shifts the onset of incast
(doubling the block size doubles the number of servers be-
fore experiencing incast collapse), but does not substantially
change the system’s behavior [28]. The mechanisms we de-
velop mitigate TCP incast collapse for any choice of block
sizes and buﬀer sizes.
305Figure 2: Reducing the RT Omin in simulation to mi-
croseconds from the current default value of 200ms
improves goodput.
Figure 3: Experiments on a real cluster validate the
simulation result that reducing the RT Omin to mi-
croseconds improves goodput.
3.3 In Simulation
We simulate one client and multiple servers connected
through a single switch where round-trip-times under low
load are 100µs. Each node has a 1Gbps capacity link, and we
conﬁgure the switch buﬀers with 32KB of output buﬀer space
per port, a size chosen based on statistics from commodity
1Gbps Ethernet switches. Because ns-2 is an event-based
simulation, the timer granularity is inﬁnite, hence we investi-
gate the eﬀect of RTOmin to understand how low the RTO
needs to be to avoid TCP incast collapse. Additionally, we
add a small random timer scheduling delay of up to 20µs to
account for real-world scheduling variance.
Figure 2 plots goodput as a function of the RTOmin for
stripe widths between 4–128 servers. Goodput, using the
default 200ms RTOmin, drops by nearly an order of magni-
tude with 8 concurrent senders, and by nearly two orders of
magnitude when data is striped across 64 and 128 servers.
Reducing the RTOmin to 1ms is eﬀective for 8–16 concur-
rent senders, fully utilizing the client’s link, but begins to
suﬀer when data is striped across more servers: 128 concur-
rent senders utilize only 50% of the available link bandwidth
even with a 1ms RTOmin. For 64 and 128 servers and low
RTOmin, each ﬂow does not have enough data to send to
individually saturate the link, given the inherent ineﬃciency
of synchronizing 64 or 128 streams each sending very little.
3.4 In Real Clusters
We study TCP incast collapse on two clusters; one sixteen-
node cluster using an HP Procurve 2848 switch, and one
48-node cluster using a Force10 S50 switch. In these clusters,
every node has 1 Gbps links and a client-to-server RTT of
approximately 100µs. All nodes run Linux kernel 2.6.28. We
run the same synchronized read workload as in simulation.
For these experiments, we modiﬁed the Linux 2.6.28 ker-
nel to use microsecond-accurate timers with microsecond-
granularity RTT estimation (Section 5) to accurately set the
RTOmin to a desired value. Without these modiﬁcations,
the TCP RTO can be reduced only to 5ms.
Figure 3 plots the application throughput as a function
of the RTOmin for 4, 8, and 16 concurrent senders. For
all conﬁgurations, goodput drops with increasing RTOmin
above 1ms. For 8 and 16 concurrent senders, the default
RTOmin of 200ms results in nearly 2 orders of magnitude
drop in throughput.
The real world results deviate from the simulation results in
a few minor ways. First, the maximum achieved throughput
in simulation nears 1Gbps, whereas the maximum achieved
in the real world is 900Mbps. Simulation throughput is
always higher because simulated nodes are inﬁnitely fast,
whereas real-world nodes are subject to myriad delaying
inﬂuences, including OS scheduling and Ethernet or switch
timing diﬀerences.
Second, real world results show negligible diﬀerence be-
tween 8 and 16 servers, while the diﬀerences are more pro-
nounced in simulation. We attribute this to variances in the
buﬀering between simulation and the real world. Simula-
tion statically assigns switch buﬀer sizes on a per-port basis,
whereas many real-world switches dynamically allocate mem-
ory from a shared buﬀer pool. Even with dynamic allocation,
however, switch buﬀers are often not large enough to prevent
TCP incast collapse in real world cluster environments.
Third, the real world results show identical performance
for RTOmin values of 200µs and 1ms, whereas there are
slight diﬀerences in simulation. Timeouts in real-world ex-
periments are longer than in simulation because of increased
latency and RTT variance in the real-world. Figure 4 shows
the distribution of round-trip-times during an incast work-
load in the real world. While the baseline RTTs can be
between 50-100µs, increased congestion causes RTTs to rise
to 400µs on average with spikes as high as 850µs. Hence,
the higher RTTs combined with increased RTT variance
causes the RTO estimator to set timeouts of 1–3ms, and an
RTOmin below 1ms will not lead to shorter retransmission
times. Hence, where we specify a RTOmin of 200µs, we are
eﬀectively eliminating the RTOmin, allowing RTO to be as
low as calculated by equation (1).
Despite these diﬀerences, the real world results show the
need to reduce the RTO to at least 1ms to avoid through-
put degradation at scales of up to 16 servers. In the next
section, we explain why providing microsecond-granularity
retransmissions will be required for future, faster datacenter
networks.
 0 100 200 300 400 500 600 700 800 900 1000200u1m5m10m50m100m200mGoodput (Mbps)RTOmin (seconds)RTOmin vs Goodput (Block size = 1MB, buffer = 32KB)48163264128# servers 0 100 200 300 400 500 600 700 800 900 1000200u1m5m10m50m100m200mGoodput (Mbps)RTOmin (seconds)RTOmin vs Goodput (Block size = 1MB, buffer = 32KB (estimate))4816# servers306Figure 4: During an incast experiment on a cluster
RTTs increase by 4 times the baseline RTT (100µs)
on average with spikes as high as 800µs. This pro-
duces RT O values in the range of 1-3ms, resulting
in an RT Omin of 1ms being as eﬀective as 200µs in
today’s networks.
4. NEXT-GENERATION DATACENTERS
TCP incast collapse poses more problems for the next
generation of datacenters with 10Gbps networks and thou-
sands of machines. 10Gbps networks have smaller RTTs
than 1Gbps networks; port-to-port latency can be as low
as 10µs. For example, we plot the distribution of RTTs
from live traces of an active storage node at Los Alamos
National Laboratory in Figure 5: 20% of RTTs are below
100µs even when accounting for kernel scheduling, showing
that networks and systems today operate in very low-latency
environments. Because 10Gbps Ethernet provides higher
bandwidth, servers can send their portion of a data block
more quickly, requiring smaller RTO values to avoid idle link
time. In this section, we show the need for ﬁne-grained TCP
retransmissions for 10Gbps low latency datacenters.
4.1 Scaling to Thousands
We analyze the impact of TCP incast collapse and the
reduced RTO solution for 10Gbps Ethernet networks in sim-
ulation as we scale the number of concurrent senders into the
thousands. We reduce baseline RTTs from 100µs to 20µs and
temporarily eliminate the 20µs timer scheduling variance,
and increase link capacity to 10Gbps, setting per-port buﬀer
size to 32KB based on our real-world experiments in 10Gbps
cluster environments.
We increase the blocksize to 80MB to ensure each ﬂow
can individually saturate a 10Gbps link, varying the number
of servers from 32 to 2048. Figure 6 shows that having an
artiﬁcial bound of either 1ms or 200µs results in low goodput
in a network whose RTTs are 20µs. This underscores the
requirement that retransmission timeouts should be on the
same timescale as network latency to avoid incast collapse—
simply changing a constant in today’s TCP implementations
will not suﬃce.
Eliminating a lower bound on RTO performs well for up
to 512 concurrent senders, but for 1024 servers and beyond,
even the aggressively low RTO conﬁguration sees up to a
50% reduction in goodput caused by signiﬁcant periods of
Figure 5: The distribution of RTTs from an active
storage node at Los Alamos National Lab shows an
appreciable number of RTTs in the 10s of microsec-
onds.
link idle time. These idle periods are caused by repeated,
simultaneous, successive timeouts. Recall that after every
timeout, the RTO value is doubled until an ACK is received.
This has been historically safe because the exponential back-
oﬀ quickly and conservatively estimates the duration to wait
until congestion abates. For incast communication, however,
the exponentially increasing delay can overshoot some por-
tion of time the link is actually idle, leading to sub-optimal
goodput. Because only one ﬂow must overshoot to delay the
entire transfer, the probability of overshooting increases with
increased number of ﬂows.
Figure 7 shows a client’s instantaneous link utilization
and the retransmission events for one of the ﬂows that ex-
perienced repeated retransmission losses during an incast
simulation on a 1Gbps network. This ﬂow timed out and
retransmitted a packet at the same time that other timed
out ﬂows also retransmitted. While some of these ﬂows got
through and saturated the link for a brief period of time, the
ﬂow shown here timed out and doubled its timeout value
(until the maximum factor of 64 * RTO) following each failed
retransmission. Often the link became available shortly after
the retransmission event, but the retransmission timer was
set to ﬁre far beyond this time. When a retransmission was
successful, the block transfer completed and the next block
transfer began, but only after large periods of link idle time
that reduced goodput.
In summary, decreased goodput for a large number of ﬂows
can be attributed to many ﬂows timing out simultaneously,
backing oﬀ deterministically, and retransmitting at precisely
the same time. While some of the ﬂows are successful on this
retransmission, a majority of ﬂows lose their retransmitted
packet and backoﬀ by another factor of two, sometimes far
beyond when the link becomes idle.
4.2 Desynchronizing Retransmissions
By adding some randomness to the RTO, the retransmis-
sions can be desynchronized so that fewer ﬂows experience
repeated timeouts when RTOmin is removed. We examine
the retransmission synchronization eﬀect in simulation, mea-
suring the goodput for several diﬀerent settings. In Figure 8,
 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0 100 200 300 400 500 600 700 800 900 1000# of OccurrencesRTT in MicrosecondsRTT Distribution in SAN 0 1000 2000 3000 4000 5000 6000 0 200 400 600 800 1000# of OccurrencesRTT in Microseconds (binsize = 20us)RTT Distribution at Los Alamos National Lab Storage Node307Figure 6: In simulation, ﬂows experience reduced
goodput when retransmissions do not ﬁre at the
same granularity as RTTs. Fine-grained timers can
observe suboptimal goodput for a large number of
servers if retransmissions are tightly synchronized.
Figure 7: Some ﬂows experience repeated retrans-
mission failures due to synchronized retransmission
behavior, delaying transmission far beyond when the
link is idle.
we vary the inaccuracy of scheduling to better understand
how synchronized the retransmissions must be to observe
repeated retransmission timeouts. When retransmissions are
sent precisely when the retransmission timer ﬁres, goodput
drops signiﬁcantly for a large number of concurrent senders.
Adding up to 5µs of random delay helps to desynchronize
some of the retransmissions in simulation. But perhaps this
is a simulation artifact as real world scheduling is not without
variance.
To better understand scheduling variance for retransmis-
sions on real systems, we measured the accuracy of executing
usleep(50) calls that use the high-precision timer subsystem
we use in Section 5, ﬁnding that the sleep durations were clus-
tered within 2–3µs, suggesting that real-world scheduling may
be accurate enough to require desynchronizing retransmis-
sions. Should TCP oﬄoad be enabled for faster, low-latency
10Gbps Ethernet cards, packet scheduling variance might be
even lower. Given that round-trip-times in Figure 5 can be
under 20µs, today’s systems are capable of such accurate
packet scheduling.
Figure 8 shows that explicitly adding an adaptive random-
ized RT O component to the scheduled timeout as follows:
timeout = (RTO + (rand(0.5) × RTO)) × 2backof f
(3)
performs well regardless of the number of concurrent senders
because it explicitly desynchronizes the retransmissions of
ﬂows that experience repeated timeouts, and does not heavily
penalize ﬂows that experience a few timeouts.
While we advocate an RTO calculation that adds a ran-
domized component for datacenters, we have not evaluated its
impact for wide-area ﬂows, where adding a delay of up to 50%
of the calculated RTO will increase latency—synchronized
retransmissions are less likely to occur in the wide-area be-
cause ﬂows have diﬀerent RTTs and hence varying RTOs.
Also, despite our eﬀorts to add scheduling variance in the
simulation experiments, real-world variances may be large
enough to avoid having to explicitly randomize the RTO
in practice. However, with a large number of concurrent
Figure 8: In simulation, introducing a randomized
component to the RT O desynchronizes retransmis-
sions following timeouts and avoids goodput degra-
dation for a large number of ﬂows.
senders, the number of ﬂows that retransmit within a ﬁxed
period of congestion on retransmission will eventually be
high enough that the repeated retransmission loss behavior
shown in simulation may occur in the real-world.
In summary, we emphasize that for future low-latency data-
center networks, extremely ﬁne-grained TCP retransmissions
must be provided to avoid TCP incast collapse. Next, we
discuss our implementation and evaluation of microsecond
granularity TCP timeouts in the Linux operating system.
5.
IMPLEMENTING FINE-GRAINED
RETRANSMISSIONS
TCP implementations typically use a coarse-grained pe-
riodic timer that provides timeout support with very low
overhead. Providing tighter TCP timeouts requires not only