cess a buffer in one of several different access patterns.
Our approach focuses on detecting the following ones:
1. Consecutively accessing the buffer byte-by-byte.
2. Consecutively accessing the buffer dword-by-dword.
3. Consecutively accessing the buffer at single-byte offsets
and reading a dword at a time.
4. Consecutively accessing the buffer using multimedia
CPU extensions, such as SSE instructions [45].
USENIX Association  
22nd USENIX Security Symposium  693
7
Step 1
Address
0x1000 O (size 4)
0x1004 O (size 4)
0x1008 O (size 4)
0x100c O (size 4)
0x1010 O (size 4)
0x1010 O (size 8)
0x1014
Step 2
C (element
size 4)
C (element
size 4)
O (size 4)
O (size 8)
Step 3
C (element
size 4)
C (element
size 4)
C (element
size 8
Table 5: An example of the creation of composite buffers
(C) from the memory read operations (original buffers O)
of the code in Table 2.
5. Accessing the buffer in a predictable pattern. For exam-
ple, two ﬁrst bytes out of every three consecutive bytes
are read in a buffer.
6. Accessing the buffer in an unpredictable pattern. In
most such cases, the buffer is not fully accessed during
the execution of a loop. For instance, accessing the key
permutation array of the RC4 [1] algorithm.
To identify a read or write buffer, we perform our analy-
sis in several steps. First, we classify each memory region
affected by an individual memory access as an original
buffer and sort them by their starting memory addresses.
Then, we merge these buffers into composite buffers by
recursively applying the following steps until there are no
more candidates for merging. As we merge the buffers,
we attempt to determine the size of the elements in each
buffer.
• Two original buffers are merged if they are adjacent and
are of equal size. In this case, the element size for the
resulting composite buffer is set to the size of the two
original buffers.
• Two original buffers are merged if they overlap, are of
equal size, and their size is divisible by the size of the
overlapping portion. In this case, the element size for
the resulting compound buffer is set to the size of the
overlapping region between the two original buffers.
• An original buffer is merged with a composite buffer if
they are adjacent and the element size of the composite
buffer is equal to the size of the original buffer.
This is applied recursively until there are no more origi-
nal buffers that can be merged. At this point, any remaining
original buffers are reclassiﬁed as compound buffers with
an element size equal to their length. An example of this
is detailed in Table 5.
This step merges the individual memory accesses into a
preliminary representation of buffers. The sizes of these
composite buffers will vary, but will be divisible by their
element size. This representation is ﬁnalized in the next
step, where the composite buffers are merged.
Merging composite buffers. Due to the way in which
some buffers are accessed, they will be split into several
composite buffers in the previous step. One example of
this is the key permutation array used in RC4 [1]. This
buffer usually has a size of 256 bytes, and is not likely to
be completely read or written if there are less than 100
bytes to be decrypted. One approach is to aggregate the
memory accesses over several different calls to the function
to identify the buffer, but that brings up questions of when
to terminate such an analysis. Therefore we use a simple
heuristic to better identify such buffers: Given two existing
composite buffers C and D, where buffer C starts at addrc
and has a size of sizec, while buffer D starts at addrd and
has a size of sized, and addrd > addrc. We deﬁne the
term gap ratio as the size of gap between buffers C and D
divided by the sum of sizes of buffers C and D:
gratio(C,D) =
addrd − (addrc + sizec)
sizec + sized
We then perform the following algorithm:
1. If C and D have the same element size, and they are
adjacent, they will be merged into a larger buffer.
2. If C and D are not adjacent, and they have the same
element size, they will be merged if the gap ratio is less
than 0.2. We determined this number experimentally.
Of course, setting this threshold to a value too large will
create false positives in the buffer detection (and will
add noise to our subsequent statistical testing), while
leaving it too small will cause us to miss parts of the
buffers.
This algorithm is applied on the set of composite buffers
until no more buffers can be merged.
Tracking unrolled loops. After the composite buffers
are merged, we add any memory accesses done by blocks
that are adjacent to the buffers and are identical to the
blocks inside a loop. This allows us to catch the marginal
parts of buffers that are modiﬁed by unrolled loops.
Data paths. After this step, we will have obtained a
full list of buffers that are accessed inside each loop. We
deﬁne a data path as an input-output buffer pair within a
loop. A loop could have multiple data paths, as shown in
Table 6. In the absence of detailed data-ﬂow analysis, we
conclude that every input buffer and every output buffer in
a loop make a data path. Thus, in a loop with N input and
M output buffers, we will have N × M data paths.
3.1.3 Decryption Detection
After identifying the buffers and the paths between them,
the next step is to identify the buffer that holds the de-
crypted content. While a full analysis of every data path
in a real-world application could be unfeasible due to the
complexity of modern media players, we can utilize several
heuristics to identify the data path that contains the decryp-
tion of the protected content. First of all, the data path that
694  22nd USENIX Security Symposium 
USENIX Association
8
.head:
mov eax, 0
inc eax
mov ebx, (0x1000, eax, 4)
mov (0x2000, eax, 4), ebx
mov ebx, (0x3000, eax, 4)
mov (0x4000, eax, 4), ebx
cmp eax, 10
jne .head
Table 6: An example of a loop with four data paths: 0x1000
to 0x2000, 0x1000 to 0x4000, 0x3000 to 0x2000, and
0x3000 to 0x4000.
Input
Stage
Download
Decrypt
Decode
E
high
high
high
R
high
high
low
Output
E
R
high
high
high
low
low
low
Table 7: The entropy (E) and randomness (R) of data paths
when playing a protected media ﬁle.
we are looking for should have a similar throughput to the
size of the media ﬁle. Additionally, since we are looking
for a data path that has an encrypted input and a decrypted
(but encoded with a media codec) output, we can utilize
information theoretical properties of the buffers to improve
our analysis.
We perform this step on the aggregated input and the
aggregated output buffers of each data path. That is, we
append all of the input and all of the output of a given data
path across multiple executions of the loop in question,
resulting in an overall input buffer and an overall output
buffer. This allows us, for example, to analyze all of
the output of a given operation across the runtime of the
program. In the case of a decryption function, this will
allow us to collect all of the decrypted content.
Entropy test. The data path in which we are interested
will have an encrypted input buffer and a decrypted but
encoded output buffer. The input buffer, being encrypted,
will have very high entropy. The output buffer, being
encoded (and effectively compressed), will also have very
high entropy. We use this property to further ﬁlter out
unrelated data paths.
This also helps ﬁlter out the decoding step. Media
codecs are highly compressive functions, resulting in high-
entropy buffers. On the contrary, a buffer of, for example,
YUV color frames is likely to have a comparatively low
entropy.
Randomness test. A fundamental property of en-
crypted data is that it is indistinguishable from random
data. This is called ciphertext indistinguishability, and is a
basic requirement for a secure cryptosystem [31]. Further-
more, randomness is very difﬁcult to achieve, and is not
a feature of data encoding algorithms. Such algorithms,
which are essentially specialized compression algorithms,
produce data with high entropy but low randomness. Thus,
as shown in Table 7, we can distinguish between the en-
crypted and decrypted stream by using a randomness test.
The Chi-Square randomness test is one such test, de-
signed to determine if a given input is random. Often used
to test the randomness of psuedo-random number genera-
tors, we use it to determine whether or not the content of a
buffer is encrypted. The implementation details of the Chi-
Square randomness test is detailed by Donald Knuth [33]
and its application to randomness testing is presented by
L’Ecuyer, et al [36]. Our approach does not rely on the im-
plementation details of the randomness test, and we have
omitted them in the interest of space. Furthermore, the
Chi-Square randomness test is not the only one that can
be used; any measure of randomness of a buffer can be
utilized for this purpose.
One important consideration is the amount of data that
we should collect before performing our randomness test.
A commonly accepted rule for the Chi-Square randomness
test, mentioned by Knuth [33], is that given n, the number
of observations, and ps, the probability that n is observed
to be in category s, the expected value n× ps is greater
than 5 for all categories s. We consider the contents of
each buffer one byte at a time, giving us 256 categories of s.
According to calculations presented by Knuth, we would
need to collect 320 kilobytes of data for a reliable test. In
fact, we carried out an empirical analysis of the minimum
amount of data that needed to be analyzed to be conﬁdent
of avoiding misdetection. The analysis determined that
a safe threshold to avoid misclassifying random data as
non-random is 800 kilobytes, and a safe threshold to avoid
misclassifying non-random media data as random is 3800
bytes, both of which are easily feasible for any sort of
media playback.
We have observed that the Chi-Square randomness test
returns extremely low values (very close to 1.0) for en-
crypted data, and very high values (in the thousands) for
encoded data.
3.2 Dumping the Stream
After the previous steps, we are able to identify the
speciﬁc data path that has the encrypted input and the
decrypted, but decoded, output. Then, our system instru-
ments the authorized media player and dumps the output
buffer.
3.3 Reconstructing the File
Finally, with the decrypted data available, the last step is
to reconstruct the media ﬁle. In the trivial case, the DRM
scheme works by encrypting the entire media ﬁle whole-
sale. This is simple to recover because the decrypted buffer
USENIX Association  
22nd USENIX Security Symposium  695
9
that we dump will then contain the whole, unprotected ﬁle.
However, this is not the case in general. For example, the
approach used in Microsoft’s PlayReady DRM encrypts
just the media stream, leaving the headers and metadata
decrypted. Thus, the decrypted stream will contain the raw
media stream, which cannot be directly played by a media
player. In the general case, this is a problem of program
analysis and writing an automated tool to reconstruct the
ﬁle given an unknown protocol is quite complicated.
In order to recreate the media ﬁle in these circumstances,
knowledge of the streaming/DRM protocol is required.
For example, knowing that PlayReady encrypts the media
stream, we wrote a reconstruction plugin to reconstruct
the ﬁle with the newly decrypted media stream (and the
already decrypted headers and metadata) so that it would
be playable in conventional media players.
Depending on the protocol and the expertise of the oper-
ator, this stage can involve reading documentation, reverse
engineering, and ﬁle analysis. However, at this point the
automated decryption of the content, which is the central
aim of our paper, is already completed. While the media
content will need to be reconstructed for every dumped ﬁle,
the development process is only required once per DRM
platform (or, depending on the implementation, once per
DRM platform/media codec combination).
4
Implementation
We implemented our system using the PIN [38] Dy-
namic Binary Instrumentation framework. We chose this
tool for its ease of development, but our approach can
be implemented on top of a full-system emulator such as
QEMU in order to avoid anti-DBI techniques by the media-
playing applications. However, the use of QEMU would
raise the question of performance, since it is not clear if
QEMU’s dynamic recompilation of binaries can match the
performance of PIN. Additionally, though our system is
implemented under the x86 architecture, the approach is
easily translatable to other architectures as well.
We will detail some of the speciﬁc implementation de-
tails related to the individual DRM platforms that we ana-
lyzed. Additionally, in the course of implementing our ap-
proach, we made several implementation decisions, which
we will discuss hereinafter. After describing these, we will
also detail optimizations that we used to increase the speed
of our approach.
4.1 DRM Platforms
We speciﬁcally investigated three DRM platforms: Flash
video with Amazon Instant Video and Hulu, Microsoft
PlayReady with Netﬂix, and Spotify. Here we will give an
overview of the protocols and the tools we developed to
support them.
4.1.1 Flash RTMPE
Amazon Instant Video and Hulu both use the RTMPE
protocol, developed by Adobe, to transmit video. RTMPE
works by encrypting the whole media ﬁle on the ﬂy before
sending it across the network.
Since the entire ﬁle is encrypted, reconstructing it did
not present a challenge because it was decrypted in a con-
tinuous manner in one function.
4.1.2 Microsoft PlayReady
Netﬂix uses Microsoft’s Silverlight PlayReady DRM to
protect its content. PlayReady presents several challenges.
Relocating code. In Silverlight, the actual routine used
for decrypting AAC audio, WMV video and H.264 video
is frequently relocated inside the process’ memory space.
We assume that this is done to frustrate would-be pirates.
An additional beneﬁt, given the required ﬂexibility of the
surrounding code, would be the ability to dynamically
update the decryption routine over the network. However,
we did not observe the latter ever occurring. Ironically,
this evasive behavior gives us a clear signal that such code
is interesting, and could enable us to prioritize it in our
analysis.
To cover the case of relocated code, we identify such
loops based on the non-relocating portions of their call
stack and the hash of their basic blocks. This allows us to
handle relocating code automatically as part of the normal
analysis.
Disabling adaptive streaming. Netﬂix automatically
adjusts the quality of the video stream to compensate for
bandwidth and CPU inadequacies. This can result in a
varied quality in the generated media ﬁle, which would lead
to a confusing subsequent media consumption experience.
Furthermore, because the MovieStealer implementation is
extremely CPU-intensive, such adaptive streaming features
will invariably select the stream with the worst available
quality.
Our solution to this problem, speciﬁcally for Silverlight-
based streaming services, is to use a Winsock introspection
tool named Esperanza [7] to inspect the browser’s trafﬁc
and ﬁlter the lower-bandwidth stream options out of the
metadata. While this is a protocol-speciﬁc ﬁx, a general-
ized version of this would be outside of the scope of this
paper.
Partial encryption. PlayReady is hard to work with
because it only encrypts the raw stream data of its media
ﬁles. Header information and meta-data is not encrypted.
Because of this, the decrypted ﬁle must be pieced back to-
gether by combining the original metadata and the dumped
stream. Furthermore, some of the headers have to be modi-
ﬁed to reﬂect the fact that the ﬁle is no longer encrypted.
4.1.3 Spotify
Spotify’s distinguishing factor is the use of the Themida
packer to frustrate our DBI platform. Since our instrumen-
10
696  22nd USENIX Security Symposium 
USENIX Association
tation is done dynamically, we would normally be able
to copy protected content of packed programs. However,
because Themida contains some evasive behavior that is
able to confuse PIN, we had to use the OllyDBG debug-
ger to ﬁrst neutralize Themida’s evasiveness by hiding
PIN’s presence. After this, we were able to extract music
from Spotify, despite it being packed with the Themida
packer. While this is not automated in our implementation,
automating such anti-debugging practices is quite feasible.
Spotify encrypts its music ﬁles as a whole, so recon-