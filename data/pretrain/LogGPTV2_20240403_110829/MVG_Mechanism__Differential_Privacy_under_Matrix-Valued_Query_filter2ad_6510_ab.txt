we will compare the MVG mechanism with the four state-of-the-art
basic mechanisms presented in this section.
3 BACKGROUND
3.1 Matrix-Valued Query
We use the term dataset interchangeably with database, and rep-
resent it with the data matrix X ∈ RM×N , whose columns are the
M-dimensional vector samples/records. The matrix-valued query
function, f (X) ∈ Rm×n, has m rows and n columns 1. We define
the notion of neighboring datasets {X1, X2} as two datasets that
differ by a single record, and denote it as d(X1, X2) = 1. We note,
however, that although the neighboring datasets differ by only a
single record, f (X1) and f (X2) may differ in every element.
We denote a matrix-valued random variable with the calligraphic
font, e.g. Z, and its instance with the bold font, e.g. Z. Finally, as
will become relevant later, we use the columns of X to denote the
samples in the dataset.
3.2 (ϵ, δ)-Differential Privacy
Differential privacy [21, 23] guarantees that the involvement of any
one particular record of the dataset would not drastically change
the query answer.
Definition 1. A mechanism A on a query function f (·) is (ϵ, δ)-
differentially-private if for all neighboring datasets {X1, X2}, and
for all possible measurable matrix-valued outputs S ⊆ Rm×n,
Pr[A(f (X1)) ∈ S] ≤ eϵ Pr[A(f (X2)) ∈ S] + δ .
1Note that we use the capital M, N for the dimension of the dataset, but the small
m, n for the dimension of the query output.
3.3 Matrix-Variate Gaussian Distribution
One of our main innovations is the use of the noise drawn from
a matrix-variate probability distribution. Specifically, in the MVG
mechanism, the additive noise is drawn from the matrix-variate
Gaussian distribution, defined as follows [16, 20, 36, 48, 73, 90].
Definition 2. An m × n matrix-valued random variable X has a
matrix-variate Gaussian distribution MVGm,n(M, Σ, Ψ), if it has
the density function:
pX(X) =
exp{− 1
2tr[Ψ−1(X − M)T Σ−1(X − M)]}
(2π)mn/2 |Ψ|m/2 |Σ|n/2
,
where tr(·) is the matrix trace [46], |·| is the matrix determinant [46],
M ∈ Rm×n is the mean, Σ ∈ Rm×m is the row-wise covariance, and
Ψ ∈ Rn×n is the column-wise covariance.
Notably, the density function of MVGm,n(M, Σ, Ψ) looks sim-
ilar to that of the multivariate Gaussian, Nm(µ, Σ). Indeed, the
matrix-variate Gaussian distribution MVGm,n(M, Σ, Ψ) is a gen-
eralization of Nm(µ, Σ) to a matrix-valued random variable. This
leads to a few notable additions. First, the mean vector µ now be-
comes the mean matrix M. Second, in addition to the traditional
row-wise covariance matrix Σ, there is also the column-wise co-
variance matrix Ψ. The latter is due to the fact that, not only could
the rows of the matrix be distributed non-uniformly, but also could
its columns.
We may intuitively explain this addition as follows. If we draw
n i.i.d. samples from Nm(0, Σ) denoted as y1, . . . , yn ∈ Rm, and
concatenate them into a matrix Y = [y1, . . . , yn] ∈ Rm×n, then, it
can be shown that Y is drawn from MVGm,n(0, Σ, I), where I is
the identity matrix [16]. However, if we consider the case when the
columns of Y are not i.i.d., and are distributed with the covariance
Ψ instead, then, it can be shown that this is distributed according
to MVGm,n(0, Σ, Ψ) [16].
3.4 Relevant Matrix Algebra Theorems
We recite two major theorems in linear algebra that are essential
to the subsequent analysis. The first one is used in multiple parts
of the analysis including the privacy proof and the interpretation
of the results, while the second one is the concentration bound
essential to the privacy proof.
Theorem 1 (Singular value decomposition (SVD) [46]). A
matrix A ∈ Rm×n can be decomposed as A = W1ΛWT2 , where
W1 ∈ Rm×m, W2 ∈ Rn×n are unitary, and Λ is a diagonal matrix
whose diagonal elements are the ordered singular values of A, denoted
as σ1 ≥ σ2 ≥ · · · ≥ 0.
Theorem 2 (Laurent-Massart [56]). For a matrix-variate ran-
dom variable N ∼ MVGm,n(0, Im, In), δ ∈ [0, 1], and ζ(δ) =
√−mn ln δ − 2 ln δ + mn,
2
Pr[∥N∥2
F ≤ ζ(δ)2] ≥ 1 − δ .
4 MVG MECHANISM: DIFFERENTIAL
PRIVACY WITH MATRIX-VALUED QUERY
Matrix-valued query functions are different from their scalar coun-
terparts in terms of the vital information contained in how the
and the column-wise covariance Ψ.
matrix-valued query function
min{m, n}
generalized harmonic numbers of order r
generalized harmonic numbers of order r of
1/2
supX ∥ f (X)∥F
√−mn ln δ − 2 ln δ + mn
2
f (X) ∈ Rm×n
r
Hr
Hr,1/2
γ
ζ(δ)
σ(Σ−1)
σ(Ψ−1)
X ∈ RM×N
database/dataset whose N columns are data
samples and M rows are attributes/features.
MVGm,n(0, Σ, Ψ) m × n matrix-variate Gaussian distribution
with zero mean, the row-wise covariance Σ,
vector of non-increasing singular values of Σ−1
vector of non-increasing singular values of Ψ−1
Table 1: Notations for the differential privacy analysis.
elements are arranged in the matrix. To fully exploit these struc-
tural characteristics of matrix-valued query functions, we present
our novel mechanism for matrix-valued query functions: the Matrix-
Variate Gaussian (MVG) mechanism.
First, let us introduce the sensitivity of the matrix-valued query
function used in the MVG mechanism.
Definition 3 (Sensitivity). Given a matrix-valued query function
f (X) ∈ Rm×n, define the L2-sensitivity as,
s2(f ) =
sup
d(X1,X2)=1
∥ f (X1) − f (X2)∥F ,
where ∥·∥F is the Frobenius norm [46].
Then, we present the MVG mechanism as follows.
Definition 4 (MVG mechanism). Given a matrix-valued query
function f (X) ∈ Rm×n, and a matrix-valued random variable Z ∼
MVGm,n(0, Σ, Ψ), the MVG mechanism is defined as,
MVG(f (X)) = f (X) + Z,
where Σ is the row-wise covariance matrix, and Ψ is the column-
wise covariance matrix.
So far, we have not specified how to pick Σ and Ψ according to
the sensitivity s2(f ) in the MVG mechanism. We discuss the explicit
form of Σ and Ψ next.
As the additive matrix-valued noise of the MVG mechanism
is drawn from MVGm,n(0, Σ, Ψ), the parameters to be designed
for the MVG mechanism are the two covariance matrices Σ and
Ψ. The following theorem presents a sufficient condition for the
values of Σ and Ψ to ensure that the MVG mechanism preserves
(ϵ, δ)-differential privacy.
Theorem 3. Let
σ(Σ−1) = [σ1(Σ−1), . . . , σm(Σ−1)]T ,
σ(Ψ−1) = [σ1(Ψ−1), . . . , σn(Ψ−1)]T
and
be the vectors of non-increasingly ordered singular values of Σ−1 and
Ψ−1, respectively, and let the relevant variables be defined according
∫
∫
eϵ
e
S
2 tr[Ψ−1(Y−f (X1))T Σ−1(Y−f (X1))]
− 1
e
S
2 tr[Ψ−1(Y−f (X2))T Σ−1(Y−f (X2))]
− 1
dY ≤
dY + δ .
to Table 1. Then, the MVG mechanism guarantees (ϵ, δ)-differential
privacy if Σ and Ψ satisfy the following condition,2
(cid:13)(cid:13)σ(Σ−1)(cid:13)(cid:13)2
(cid:13)(cid:13)σ(Ψ−1)(cid:13)(cid:13)2 ≤ (−β +(cid:112)
where α = [Hr +Hr,1/2]γ
2+2Hrγs2(f ), and β = 2(mn)1/4
(1)
,
Hr s2(f )ζ(δ).
Proof. (Sketch) We only provide the sketch proof here. The full
proof can be found in Appendix A.
The MVG mechanism guarantees (ϵ, δ)-differential privacy if for
every pair of neighboring datasets {X1, X2} and all measurable sets
S ⊆ Rm×n,
2 + 8αϵ)2
β
2
4α
Pr[f (X1) + Z ∈ S] ≤ exp(ϵ) Pr[f (X2) + Z ∈ S] + δ .
Using Definition 2, this is satisfied if we have,
2 tr[Ψ−1(Y−f (X2))T Σ−1(Y−f (X2))]}
By inserting exp{− 1
2 tr[Ψ−1(Y−f (X2))T Σ−1(Y−f (X2))]} inside the inte-
exp{− 1
gral on the left side, it is sufficient to show that
exp{− 1
exp{− 1
2tr[Ψ−1(Y − f (X1))T Σ−1(Y − f (X1))]}
2tr[Ψ−1(Y − f (X2))T Σ−1(Y − f (X2))]} ≤ exp(ϵ),
with probability ≥ 1−δ. By algebraic manipulations, we can express
this condition as,
+Ψ−1
f (X2)T Σ−1
tr[Ψ−1YT Σ−1∆ + Ψ−1∆T Σ−1Y
f (X2) − Ψ−1
f (X1)T Σ−1
f (X1)] ≤ 2ϵ .
where ∆ = f (X1) − f (X2). This is the necessary condition that has
to be satisfied for all neighboring {X1, X2} with probability ≥ 1− δ
for the MVG mechanism to guarantee (ϵ, δ)-differential privacy.
Therefore, we refer to it as the characteristic equation. From here,
the proof analyzes the four terms in the sum separately since the
trace is additive. The analysis relies on the following lemmas in
linear algebra.
i.
Lemma 1 (Merikoski-Sarria-Tarazaga [68]). The non-increasingly
ordered singular values of a matrix A ∈ Rm×n have the values of
0 ≤ σi ≤ ∥A∥F /√
Lemma 2 (von Neumann [89]). Let A, B ∈ Rm×n; σi(A) and
σi(B) be the non-increasingly ordered singular values of A and B,
i =1σi(A)σi(B).
respectively; and r = min{m, n}. Then, tr(ABT ) ≤ Σr
Lemma 3 (Trace magnitude bound [45]). Let σi(A) be the
non-increasingly ordered singular values of A ∈ Rm×n, and r =
min{m, n}. Then, |tr(A)| ≤r
i =1 σi(A).
The proof, then, proceeds with the analysis of each term in the
characteristic equation as follows.
The first term: tr[Ψ−1YT Σ−1∆]. Let us denote Y = f (X) +
Z, where f (X) and Z are any possible instances of the query
and the noise, respectively. Then, we can rewrite the first term
as, tr[Ψ−1
f (X)T Σ−1∆] + tr[Ψ−1ZT Σ−1∆]. Both parts are then
bounded by their singular values via Lemma 2. The singular values
2Note that the dependence on δ is via ζ (δ) in β.
Figure 2: A conceptual display of the MVG design space. The
illustration visualizes the design space coordinated by the
two design parameters of MVGm,n(0, Σ, Ψ). Each point on
the space corresponds to an instance of MVGm,n(0, Σ, Ψ).
From this perspective, Theorem 3 suggests that any instance
of MVGm,n(0, Σ, Ψ) in the (conceptual) shaded area would
preserve (ϵ, δ)-differential privacy.
are, in turn, bounded via Lemma 1 and Theorem 2 with probability
≥ 1 − δ. This gives the bound for the first term:
2 + (mn)1/4
tr[Ψ−1YT Σ−1∆] ≤ γ Hr s2(f )ϕ
ζ(δ)Hr s2(f )ϕ,
where ϕ = ((cid:13)(cid:13)σ(Σ−1)(cid:13)(cid:13)2
(cid:13)(cid:13)σ(Ψ−1)(cid:13)(cid:13)2)1/2.
The second term: tr[Ψ−1∆T Σ−1Y]. By following the same steps
as in the first term, the second term has the exact same bound as
the first term, i.e.
tr[Ψ−1∆T Σ−1Y] ≤ γ Hr s2(f )ϕ
The third term: tr[Ψ−1
f (X2)T Σ−1
Lemma 1, we can readily bound it as,
2 + (mn)1/4
f (X2)]. Applying Lemma 2 and
ζ(δ)Hr s2(f )ϕ.
tr[Ψ−1
f (X2)T Σ−1
f (X2)] ≤ γ
2
2
.
Hr ϕ
The fourth term: −tr[Ψ−1
f (X1)]. Since this term has
the negative sign, we use Lemma 3 to bound its magnitude by its
singular values. Then, we use Lemma 1 to bound the singular values.
This gives the bound for the forth term as,
f (X1)T Σ−1
f (X1)](cid:12)(cid:12)(cid:12) ≤ γ
f (X1)T Σ−1
2
Hr,1/2ϕ
2
.
(cid:12)(cid:12)(cid:12)tr[Ψ−1
Four terms combined: by combining the four terms and rear-
2 + βϕ ≤
ranging them, the characteristic equation becomes αϕ
2ϵ. This is a quadratic equation, of which the solution is ϕ ∈
[−β−√
]. Since we know ϕ > 0, we have the
√
−β +
β 2+8αϵ
2α
,
β 2+8αϵ
2α
solution,
ϕ ≤ −β +(cid:112)
2 + 8αϵ
β
2α
which implies the criterion in Theorem 3.
,
□
Remark 1. In Theorem 3, we assume that the Frobenius norm of
the query function is bounded for all possible datasets by γ. This
assumption is valid in practice because real-world data are rarely
unbounded (cf. [63]), and it is a common assumption in the analysis
of differential privacy for multi-dimensional query functions (cf.
[13, 24, 27, 102]).
informative is the singular value decomposition (SVD) (Theorem 1)
of the two covariance matrices of MVGm,n(0, Σ, Ψ).
Consider first the covariance matrix Σ ∈ Rm×m, and write its
SVD as, Σ = W1ΛWT2 . It is well-known that, for the covariance
matrix, we have the equality W1 = W2 since it is positive definite
(cf. [1, 46]). Hence, let us more concisely write the SVD of Σ as,
Σ = WΣΛΣWT
Σ .
This representation gives us a very useful insight to the noise gener-
ated from MVGm,n(0, Σ, Ψ): it tells us the directions of the noise
via the column vectors of WΣ, and variance of the noise in each
direction via the singular values in ΛΣ.
For simplicity, consider a two-dimensional multivariate Gauss-
ian distribution, i.e. m = 2, so there are two column vectors of
WΣ = [wΣ1, wΣ2]. The geometry of this distribution can be de-
picted by an ellipsoid, e.g. the dash contour in Fig. 3, Left (cf. [70,
ch. 4], [6, ch. 2]). This ellipsoid is characterized by its two principal
axes – the major and the minor axes. It is well-known that the two
column vectors from SVD, i.e. wΣ1 and wΣ2, are unit vectors point-
ing in the directions of the major and minor axes of this ellipsoid,
and more importantly, the length of each axis is characterized by its