s
s
a
P
f
o
%
 35
 0
 15
 20
 15
 10
 5
d
e
s
s
e
u
G
s
d
r
o
w
s
s
a
P
f
o
%
 0
 15
d
e
s
s
e
u
G
s
d
r
o
w
s
s
a
P
f
o
%
 20
 15
 10
 5
 0
 15
 35
 35
 20
 30
Number of Guesses Made (log2)
 25
 20
 30
Number of Guesses Made (log2)
 25
(a) V A1-DIAG CDFs for pool.
(b) V A1-LINE CDFs for pool.
CDFs for VA1-DIAG Attacks (Cars)
CDFs for VA1-LINE Attacks (Cars)
VA1-DIAG    
VA1-DIAG+  
VA1-DIAG++
d
e
s
s
e
u
G
s
d
r
o
w
s
s
a
P
f
o
%
VA1-LINE    
VA1-LINE+  
VA1-LINE++
 20
 15
 10
 5
 20
 30
Number of Guesses Made (log2)
 25
 35
 0
 15
 20
 30
Number of Guesses Made (log2)
 25
(c) V A1-DIAG CDFs for cars.
(d)V A1-LINE CDFs for cars.
Figure 5. CDFs for different attacks with LocalMax normalization (i.e., V A1).
are top-down models under various plausible assumptions,
that may more accurately model user choice. For exam-
ple, might the ﬁrst point be chosen according to bottom-
up visual attention, and then the rest chosen in a top-down
manner such that they are somehow similar to the ﬁrst? Al-
ternately, might the entire process be top-down, based on
whether the user can ﬁnd ﬁve objects that are similar in
some way? Such a top-down theory would be substantially
more difﬁcult to model an attack on, but if possible to im-
plement, its results would offer interesting insight.
The distinguishable points map δ could be enhanced in
several ways, to reﬁne the attacks presented herein. We ex-
pect the dictionaries could be further improved by incorpo-
rating other types of calculable points, such as north, south,
east, and west on circles and squares. Also, changing the
parameters to the algorithms we use to identify distinguish-
able points might provide better results, or changing the pa-
rameters for the visual attention model. It would also be
interesting to explore whether settings can be optimized for
a wide range of images, or if optimal settings are highly-
image speciﬁc. Regarding our image-independent attacks
which rely only on generic patterns and window clustering,
exploration of other patterns may prove fruitful.
6 Concluding Remarks
We provide what appears to be the best automated at-
tack against PassPoints-style graphical passwords to date.
Click-order patterns, DIAG and LINE, combined with our
laziest relaxation rule, yielded highly effective dictionaries.
We were able to further reduce the dictionary size while
retaining some accuracy using Itti et al.’s [15] computa-
tional model of bottom-up visual attention. Our results are a
signiﬁcant improvement on previous work for purely auto-
mated guessing PassPoints-style graphical passwords. Us-
ing a dictionary of 235 entries, αDIAG++, we were able to
guess over 48% of user passwords for each of two images,
whereas previous work was able to guess 0.9-9.1% [27] for
108118
the same images and user study password database and 8%
[10] for a single (comparable) image.
Although these lazy click-order dictionary sizes are not
as small as previous dictionaries used for human-seeded at-
tacks, when we combine them with a model of visual atten-
tion [15], our results are comparable to the human-seeded
results for cars. Using a dictionary of 230.3 entries, the
V A1-DIAG++ dictionary guessed 15.8% of passwords on
pool, and using a dictionary of about 231.4 entries it guessed
16.5% of passwords on cars. For cars, the basic human-
seeded attack [27] guessed 20% of passwords with a dic-
tionary of 233 entries. This suggests that automated attacks
provide an effective alternative to a human-seeded attack
against PassPoints-style graphical passwords. Furthermore,
they allow continuation of an attack through using click-
order patterns without prioritization by some other means,
guessing more passwords overall than human-seeded meth-
ods. Finally, they are arguably much easier for an attacker
to launch (removing the requirement of humans to index
the images), especially if large image datasets are used. We
emphasize that the attack dictionaries used for Table 1 in-
cluding the dictionary in the previous paragraph, do not rely
on visual attention techniques or any image-speciﬁc pre-
computation, implying that the actual dictionaries are the
same for all images, though the attack results (i.e., their ef-
fectiveness) are image-dependent and of course depend also
on the actual passwords chosen by any users in question.
We evaluated different guessing styles using Itti’s model
and found that using the ﬁrst 30 steps of the model’s scan-
path output, none of the user passwords in the database
followed along the scan-path order (i.e., 5 ordered points
within the 30-element scan-path ordering), but a number of
passwords were composed of points within other orderings
of the scan-path elements. If we assume that Itti’s model
using the default settings is an accurate representation of
bottom-up visual attention, these results are consistent with
bottom-up visual attention being one part of a broader crite-
ria for selecting click-points. Alternately, these click-points
might be chosen according to some other phenomenon that
happens to have a non-null intersection with this model of
bottom-up visual attention.
Using Iterative normalization, our attack based on the
visual attention model did not perform very well, indicating
that the bias of proximity (local saliency vs. global saliency)
can have a dramatic effect. Our success using LocalMax
normalization suggests that users are more likely to choose
successive points that are closer to one another than on the
other side of the image. The difference in results suggests
that the success of the LocalMax attack is not a chance ef-
fect, but rather it actually locates the parts of the image that
users are more inclined to choose as click-points. Even bet-
ter results may be possible through other (unknown) neural
network strategies.
It remains unclear how universal the inducement of users
to fall into click-order patterns is across a broad universe of
images, or whether image processing measures might ef-
fectively ﬁlter out images that are more prone to structure-
based click-order patterns like those exploited in our at-
tacks.
References
[1] J.C. Birget, D. Hong, and N. Memon. Robust Dis-
cretization, with an Application to Graphical Pass-
words. IEEE Transactions on Information Forensics
and Security, 1:395–399, 2006.
[2] G. Blonder. Graphical Passwords. United States
Patent 5559961, 1996.
[3] Ian Britton. http://www.freefoto.com, site
accessed Feb. 2, 2007.
[4] S. Chiasson, A. Forget, P.C. van Oorschot, and R. Bid-
dle. Inﬂuencing Users Towards Better Passwords: Per-
suasive Cued Click-Points. In HCI, 2008.
[5] S. Chiasson, P.C. van Oorschot, and R. Biddle. A Sec-
ond Look at the Usability of Click-Based Graphical
Passwords. In SOUPS, 2007.
[6] S. Chiasson, P.C. van Oorschot, and R. Biddle.
Graphical Password Authentication Using Cued Click
Points. In ESORICS, 2007.
[7] D. Comaniciu and P. Meer. Mean Shift: A Robust Ap-
proach Toward Feature Space Analysis. IEEE Trans.
PAMI, 24(5):603–619, 2002.
[8] N. Cowan. The Magical Number 4 in Short-Term
Memory: A Reconsideration of Mental Storage Ca-
pacity. Behavioral and Brain Sciences, 24:87–185,
2000.
[9] D. Davis, F. Monrose, and M.K. Reiter. On User
Choice in Graphical Password Schemes. In USENIX
Security, 2004.
[10] A. Dirik, N. Memon, and J.-C. Birget. Modeling User
Choice in the PassPoints Graphical Password Scheme.
In SOUPS, 2007.
[11] R. C. Gonzalez and R. E. Woods. Digital Image Pro-
cessing (3/e). Prentice-Hall, Inc., 2006.
[12] C. Harris and M. Stephens. A Combined Corner and
Edge Detection. In Proceedings of The Fourth Alvey
Vision Conference, pages 147–151, 1988.
109119
[27] J. Thorpe and P. C. van Oorschot. Human-Seeded
Attacks and Exploiting Hot-Spots in Graphical Pass-
words. In USENIX Security, 2007.
[28] P.C. van Oorschot and J. Thorpe. On Predictive Mod-
els and User-Drawn Graphical Passwords. ACM TIS-
SEC, 10(4):1–33, November 2007.
[29] Dirk Walther and Christof Koch. 2006 Special Issue:
Modeling Attention to Salient Proto-objects. Neural
Network, 19(9):1395–1407, 2006.
[30] S. Wiedenbeck, J. Waters, J. Birget, A. Brodskiy, and
N. Memon. PassPoints: Design and Longitudinal
Evaluation of a Graphical Password System.
Int. J.
Hum.-Comput. Stud., 63(1-2):102–127, 2005.
[31] S. Wiedenbeck, J. Waters, J.-C. Birget, A. Brodskiy,
and N. Memon. Authentication Using Graphical Pass-
words: Basic Results. In HCII, 2005.
[32] S. Wiedenbeck, J. Waters, J.-C. Birget, A. Brodskiy,
and N. Memon. Authentication Using Graphical Pass-
words: Effects of Tolerance and Image Choice.
In
SOUPS, pages 1–12, 2005.
[33] J.M. Wolfe. Guided Search 2.0: A Revised Model
of Visual Search. Psychonomic Bulletin and Review,
1(2):202–238, 1994.
[13] SFR IT-Engineering.
graﬁcal
lo-
viskey.
-
The
pocket
for
your
solution
gin
http://www.sfr-software.de/cms/EN/
pocketpc/viskey/index.html, site accessed
Sept. 18, 2008.
pc
[14] L. Itti and C. Koch. Computational Modeling of
Nature Reviews Neuroscience,
Visual Attention.
2(3):194–203, Mar 2001.
[15] L. Itti, C. Koch, and E. Niebur. A Model of Saliency-
Based Visual Attention for Rapid Scene Analysis.
IEEE Trans. PAMI, 20(11):1254–1259, Nov 1998.
[16] W. Jansen, S. Gavrilla, V. Korolev, R. Ayers, and
Swanstrom R. Picture Password: A Visual Login
Technique for Mobile Devices. NIST Report - NIS-
TIR7030, 2003.
[17] I. Jermyn, A. Mayer, F. Monrose, M. Reiter, and
A. Rubin. The Design and Analysis of Graphical Pass-
words. In USENIX Security, 1999.
[18] P. D. Kovesi. MATLAB and Octave Functions for
Computer Vision and Image Processing. School of
Computer Science & Software Engineering, The Uni-
versity of Western Australia. http://www.csse.
uwa.edu.au/˜pk/research/matlabfns/.
[19] S. Madigan. Picture Memory. In J.C. Yuille, editor,
Imagery, Memory and Cognition. Lawrence Erlbaum
Assoc., 1983.
[20] F. Monrose and M. K. Reiter. Graphical Passwords.
In L. Cranor and S. Garﬁnkel, editors, Security and
Usability, chapter 9, pages 147–164. O’Reilly, 2005.
[21] Vidhya Navalpakkam and Laurent Itti. Modeling
the Inﬂuence of Task on Attention. Vison Research,
45:205–231, 2005.
[22] A. Oliva, A. Torralba, M. Castelhano, and J. Hender-
son. Top Down Control of Visual Attention in Object
Detection. Journal of Vision, 3(9):253–256, 2003.
[23] N. Ouerhani, R. von Wartburg, H. Hugli, and R. Muri.
Empirical Validation of the Saliency-based Model of
Visual Attention. Electronic Letters on Computer Vi-
sion and Image Analysis, 3(1):13–24, 2004.
[24] Passlogix. http://www.passlogix.com, site
accessed Feb. 2, 2007.
[25] G. Stockman and L. G. Shapiro. Computer Vision.
Prentice Hall PTR, 2001.
[26] Xiaoyuan Suo, Ying Zhu, and G. Scott Owen. Graph-
ical Passwords: A Survey. In ACSAC, 2005.
110120