What Makes Process Models Understandable?
J. Mendling1, H.A. Reijers2, and J. Cardoso3
1 Vienna University of Economics and Business Administration
Augasse 2-6, 1090 Vienna, Austria
PI:EMAIL
2 Eindhoven University of Technology
P.O. Box 513, 5600 MB Eindhoven, The Netherlands
PI:EMAIL
3 University of Madeira
9000-390 Funchal, Portugal
PI:EMAIL
Abstract. Despite that formal and informal quality aspects are of sig-
nificantimportancetobusinessprocessmodeling,thereisonlylittleem-
piricalworkreportedonprocessmodelqualityanditsimpactfactors.In
thispaperweinvestigateunderstandabilityasaproxyforqualityofpro-
cessmodelsandfocusonitsrelationswithpersonalandmodelcharacter-
istics. We used a questionnaire in classes at three European universities
and generated several novel hypotheses from an exploratory data anal-
ysis. Furthermore, we interviewed practitioners to validate our findings.
Theresultsrevealthatparticipantstendtoexaggeratethedifferencesin
model understandability, that self-assessment of modeling competence
appears to be invalid, and that the number of arcs in models has an
important influence on understandability.
1 Introduction
Even though workflow and process modeling have been used extensively over
the past 30 years, we know surprisingly little about the act of modeling and
which factors contribute to a “good” process model in terms of human under-
standability. This observation contrasts with the large body of knowledge that
is available for the formal analysis and verification of desirable properties, in
particular for Petri nets. To guarantee a certain degree of design quality of the
model artifact in a wider sense, several authors propose guidelines for the act of
modeling(e.g.[1,2])butyetwithlittleimpactonmodelingpractice.Clearly,an
empirical research agenda is required for acquiring new insights into quality (cf.
[3]) and usage aspects (cf. [4]) of process modeling.
Following this line of argumentation, a recent empirical study provides evi-
dencethatlarger,real-worldprocessmodelstendtohavemoreformalflaws(such
as e.g. deadlocks) than smaller models [5,6]. One obvious hypothesis related to
thisphenomenonwouldbethathumanmodelersloosetrackoftheinterrelations
of large and complex models due to their limited cognitive capabilities (cf. [7]),
and then introduce errors that they would not insert in a small model. There
2
are further factors such as the degrees of sequentiality, concurrency, or struc-
turedness that presumably affect the understandability of a process model [8].
Validating such hypothetical relationships empirically would not only represent
a major step forward towards understanding quality of process models beyond
verification,butalsoprovideasoundtheoreticalbasisfordefiningguidelinesfor
process modeling in general.
Process F Process L
Start Start
OR A
A B
OR AND
XOR B C D E
AND AND XOR XOR F G H I
C D E F G H I J K J K L
OR
AND OR AND N
XOR L M XOR XOR
AND
S XOR
O
XOR N O
XOR XOR P Q
R S
V T XOR T U
AND P V W
W U R OR M
Q
XOR XOR
End End
Fig.1. Proces model F and process model L from the questionnaire
Since only little research has been conducted on quality aspects of process
modelssofar[3],weapproachthisareawithanexperimentaldesignfocusingon
theunderstandabilityofprocessmodels(not ofprocessmodelinglanguages).By
havingaquestionnairefilledoutby73studentswhofollowedcoursesonprocess
modelingattheEindhovenUniversityofTechnology,theUniversityofMadeira,
andtheViennaUniversityofEconomicsandBusinessAdministration,weaimto
gain insight into empirical connections between personal and model characteris-
ticsandtheabilityofapersontounderstandaprocessmodelproperly.Figure1
showstwoprocessmodelsthatwereincludedinthequestionnaire.Furthermore,
we conducted interviews in order to contrast the findings of the questionnaire
with expert opinions. In this context, our contribution is twofold. First, we pro-
videanoperationalizationofunderstandabilityaswellasofpersonalandmodel
related factors that may influence process model understandability. Second, we
contribute new findings to the still meagre body of empirical knowledge on pro-
cessmodeling.Againstthisbackground,theremainderofthepaperisstructured
asfollows.InSection2wediscussrelatedworkandidentifyalackofempirically
3
validated insight on the understandability of process models. Then, Section 3
introduces the research design, i.e. in particular, the conceptualization of the
questionnaire, the statistical analysis that can be applied on the acquired data,
and the role of the expert interviews. In Section 4 we present the results of the
analysis and the interviews. Section 5 concludes the paper, discusses limitations
ofthefindings,andidentifiesopenquestionsthatneedtobeaddressedbyfuture
research.
2 Related Work
There are basically three streams of research related to our work in the concep-
tual modeling area: top-down quality frameworks, bottom-up metrics related to
quality aspects, and empirical surveys related to modeling techniques.
One prominent top-down quality framework is the SEQUAL framework [9,
10].Itbuildsonsemiotictheoryanddefinesseveralqualityaspectsbasedonrela-
tionshipsbetweenamodel,abodyofknowledge,adomain,amodelinglanguage,
and the activities of learning, taking action, and modeling. In essence, syntactic
quality relates to model and modeling language; semantic quality to model, do-
main,andknowledge;andpragmaticqualityrelatestomodelandmodelingand
its ability to enable learning and action. Although the framework does not pro-
videanoperationaldefinitionofhowtodeterminethevariousdegreesofquality,
it has been found useful for business process modeling in experiments [11]. The
Guidelines of Modeling (GoM) [2] define an alternative quality framework that
is inspired by general accounting principles. The guidelines include the six prin-
ciples of correctness, clarity, relevance, comparability, economic efficiency, and
systematicdesign.ThisframeworkwasoperationalizedforEPCsandalsotested
in experiments [2]. Furthermore, there are authors (e.g. [3]) advocating a speci-
fication of a quality framework for conceptual modeling in compliance with the
ISO 9126 standard [12] for software quality. A respective adaptation to business
process modeling is reported in [13]. Our experiments addresses partial aspects
for these frameworks. In particular, we focus on understandability of process
models as an enabler of pragmatic quality (SEQUAL) and clarity (GoM). This
requiresusnotonlytoaskaboutunderstandability,butalsocheckwhethermod-
els are interpreted correctly. This is in line with research of Gemino and Wand
[14] who experimented on conclusions that people can draw from models.
There is several work on bottom-up metrics related to quality aspects of pro-
cess models, stemming from different research and partially isolated from each
other (see [15–23] or for an overview [8]). Several of these contributions are the-
oretic without empirical validation. Most authors doing experiments focus on
the relationship between metrics and quality aspects: Canfora et al. study the
connection mainly between count metrics – for example, the number of tasks or
splits – and maintainability of software process models [21]; Cardoso validates
the correlation between control flow complexity and perceived complexity [24];
and Mendling et al. use metrics to predict control flow errors such as deadlocks
in process models [6,8]. The results reveal that an increase in size of a model
4
appears to have a negative impact on quality. This finding has an impact on
thedesignofourquestionnaire.Togaininsightsthatareindependentofprocess
size, we keep the number of tasks constant and study which other factors might
have an impact on understandability.
Finally, there are some empirical surveys related to modeling techniques. In
[25] the authors study how business process modeling languages have matured
over time. While this is valuable research it does not reveal insights on single,
concrete process models. The same holds for [26] who study the usability of
UML. In [27] the authors also approach understandability, not of individual
process models, but on the level of the modeling language. They find out that
EPCs seem to be more understandable than Petri nets. Inspired by this survey
we decided to use an EPC-like notation in our questionnaire to minimize the
impact of the notation on understandability.
Tosummarize,thereisessentiallyonerelationthatseemstobeconfirmedby
related research, and that is that larger models tend to be negatively connected
withquality.Theaimofourquestionnaireistoenhancethisratherlimitedbody
of knowledge.
3 Research Design
Only little research has been conducted on quality aspects of process models so
far [3]. In particular, we identify the following six research questions related to
the factors that might influence understandability of process models (cf. [27,8,
28,10]):
1. Whatpersonal factors(beyondgeneralpsychologicalandintellectualfactors)
have an influence?
2. Which model characteristics (e.g. number and type of splits) contribute to
a good understandability?
3. How does the modeling purpose (e.g. documentation versus enactment) re-
late to understandability?
4. How is understandability related to knowledge about the domain that is
described in the model?
5. Which differences in understandability exist when observing semantically
equivalent models described in different modeling languages?
6. What is the impact of different visual layout strategies or graph drawing
algorithms on understandability?
We approach these questions with an experimental design focusing on per-
sonal and model characteristics (question 1 and 2). Furthermore, we strive to
neutralize the influence of the other factors: related to question 3, we gathered
a set of process models from practice that were all created for documentation
purposes. To eliminate the influence of domain knowledge (question 4), we re-
coded the task labels to capital letters A to W. Based on the observation by
[27] that EPCs appear to be easier to understand than Petri nets, we chose for
anEPC-likenotationwithoutevents.Theparticipantsreceivedashortinformal
5
description of the semantics similar to [29, p.25] (question 5). Finally, we drew
allmodelsinthesametop-to-bottomstylewiththestartelementatthetopand
end element at the bottom (question 6).
3.1 Phases of the Experiment
The experiment was conducted in three phases. First, we collected a set of eight
process models from practice with an equivalent number of tasks (25) and con-
structed two additional variants for each of them by changing the type of some
routing elements (e.g. a particular XOR-split in a AND-split). For these 24 pro-
cess models we built a questionnaire that measured the following variables:
– theory: Students made a self-assessment of theoretical knowledge in busi-
ness process modeling on a five point ordinal scale,
– practice: Students made a self-assessment of practical experience in busi-
ness process modeling on a four point ordinal scale,
– perceived: For each model, students made an assessment of the perceived
difficulty of the model,
– score: For each model, students answered a set of eight closed questions
about order, concurrency, exclusiveness, or repetition of tasks in the model
andoneopenquestionwhererespondentswerefreetoidentifyamodelprob-
lem(iftheyfelttherewasany);fromtheanswerswecalculatedscoreasthe
sumofcorrectanswerstoserveasanoperationalizationofunderstandability;
i.e. score measures in how far the semantics of the model are interpreted
correctly by the participant.
– ranking:Forallvariantsofthesamemodel,studentsrankedtheseregarding
their relative perceived understandability. For example, students were asked
if process A was more difficult to understand than process B.
The correct answers for the questions relating to score were determined
with the EPC analysis tools introduced in [30]. While the closed answers were
evaluated automatically, the open answers had to be interpreted and matched
with the errors detected by the tools. The same EPC analysis tools were also
used to calculate the set of metrics (cf. next section). For this first version
of the questionnaire, we conducted a pre-test which led to a reduction of the
model set to 12 process models, i.e. four models in three variants each, and a
reformulation of some questions. We basically dropped the more simple models
for preventing fatigue. Second, we created six versions of the questionnaire with
differentrandomizedorderofmodelsandvariantsforeliminatinglearningeffects
throughout the answering. The questionnaire was filled out in class settings at
the various universities by 73 students in total. It led to a total of 847 complete
model evaluations. At the time of the experiment, students were following or
completing courses on process modeling at the Eindhoven University of Tech-
nology, the University of Madeira, and the Vienna University of Economics and
Business Administration. Participation was voluntarily. The motivation for the
students was the fact that they felt to be in a competitive situation with the
6
otheruniversities,andthatweinformedthemthatthequestionnairewouldbea
good exam preparation. The answers were coded and analyzed using the statis-
tics software packages SPSS and Statgraphics. Third, we conducted interviews
with experts in business process modeling to contrast our findings with insights
from practitioners. This validation is of particular importance considering the
insecureexternalvalidityofstudentexperimentsininformationsystemsresearch
(see [3]).