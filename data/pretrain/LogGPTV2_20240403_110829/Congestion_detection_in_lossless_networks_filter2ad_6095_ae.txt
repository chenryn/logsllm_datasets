### 5.1.1 Congestion and Undetermined State Detection

At the start, F0 operates at 1 Gbps, while F1 begins at 8 Gbps. A0 then initiates at line rate. During this process, port P0 remains in an undetermined state. F1 traverses both the undetermined port P0 and the congested port. Our focus is on the marked packets received at the destination. Figure 11 illustrates the F0 marking fraction calculated every 100 ms. For both CEE and InfiniBand, after A0 starts, port P0 is detected as being in an undetermined state, and packets are marked with UE. Once A0 ends, port P0 recovers to a non-congested state, and no F0 packets are marked. The results for F1 are not shown; however, the marking fraction of F1 is inversely related to F0, with all packets marked with CE during A0's operation.

### 5.1.2 Validation of Typical Scenarios

Next, we validate the fine-grained congestion detection behavior of TCD in the typical scenarios outlined in Section 3.1. The queue length evolution and marking behaviors in each scenario are illustrated in Figures 12 and 13.

In the single congestion point scenario, ports P2 and P1 transition from an undetermined state to a non-congested state. Port P2 experiences more queue accumulation than port P1. After P2 releases from the undetermined state and resumes normal transmission, it takes some time to drain the accumulated queue. During this period, the switch detects a continuous decrease in queue length following the undetermined state, so packets are not marked with CE even if the queue length exceeds the threshold.

In the multiple congestion points scenario, port P2 transitions from an undetermined state to a congested state. After P2 releases from the undetermined state and resumes normal transmission, P2 still has persistent queue buildup, leading to its detection as congested, and packets are marked with CE. Port P1 remains in an undetermined state due to congestion spreading from port P2.

### 5.1.3 Victim Flows Scenario

TCD accurately detects congestion and undetermined states, benefiting victim flows. With ECN or FECN, victim flows may be mistakenly marked and considered congested. We use the topology in Figure 2 to evaluate TCD in a typical head-of-line scenario. The link speeds for links S0-T0 and S1-T0 are set to 20 Gbps, while all other links are 40 Gbps with a delay of 4 μs. No flows are sent from host S2 in this scenario, making all flows from S0 potential victim flows that should not be detected as congested. If any packets are marked with CE, the flow is considered mistakenly detected as congested.

For CEE, hosts S0-S1 and A0-A14 generate flows according to a heavy-tailed Hadoop workload [48] with exponentially distributed inter-arrival times. The workload generators at hosts A0-A14 are synchronized to simulate concurrent bursts. The default congestion control algorithm is DCQCN.

For InfiniBand, hosts S0-S1 and A0-A14 generate MPI and I/O messages in typical sizes [15]. As shown in Table 3, in both networks, there are victim flows detected as congested. For CEE, about 26% of flows are mistakenly marked with ECN. With TCD, no victim flows are detected as congested.

### 5.1.4 Parameter Sensitivity of ε

ε determines the value of max(Ton). As ε decreases, max(Ton) increases. TCD detects the release from the undetermined state as soon as max(Ton) expires. A too small max(Ton) may result in the undetermined state being mistakenly detected as either a congested or non-congested state. Conversely, a too large max(Ton) can delay the detection of a congested state. To evaluate the parameter sensitivity of ε, we repeat the concurrent burst simulation with different ε values. Figure 14 shows the results. A larger ε yields more mistakenly marked packets. When ε is smaller than 0.1, packets of victim flows are not mistakenly marked with CE. Considering various factors, our recommended value of 0.05 is appropriate.

### 5.2 Case Study

In this section, we conduct case studies to demonstrate how TCD integrates with existing congestion control algorithms. Our primary goal is not to propose the optimal congestion control algorithm for lossless networks but to highlight the importance of accurate congestion state detection. This allows for the consideration of different congestion states to improve end-to-end congestion controls. The key principles are:

1. Congested flows should aggressively decrease their rate as they contribute to congestion.
2. Flows passing through undetermined ports (undetermined flows) should perform gentle rate adjustments. These flows may be victims and should not back off, but blindly increasing their rate could exacerbate congestion.

### 5.2.1 DCQCN

With TCD, NP conveys both CE and UE information back to RP. Senders do not update the sending rate when receiving a CNP marked with UE. We change the rate reduction factor α from the default 0.5 to 1.2 to aggressively decrease the rate of congested flows. Our simulator is based on the open-source project for DCQCN [55]. The PFC threshold Xoff and Xon are 320 KB and 318 KB, respectively, with other parameters set to default values [56].

**Victim Flows Scenario:**
We first evaluate DCQCN with TCD in the victim flow scenario. Figure 15(a) shows the average FCT breakdown of victim flows. Overall, DCQCN with TCD achieves better average FCT than DCQCN. For flows smaller than 10 KB, the faster completion benefits from the aggressive rate decrease of congested flows, reducing congestion spreading and queueing delays. For medium and large flows, the faster completion is due to accurate congestion detection, preventing the throttling of victim flows.

To further study performance under small flows, we let A0-A14 generate concurrent bursts with varying sizes, with exponential inter-arrival times. Hosts S0-S1 generate flows according to the Hadoop workload. The BDP is 80 KB in this scenario. Figure 15(b) shows the average FCT and TCD marking behaviors of victim flows. As burst size increases, it becomes harder for end-to-end congestion control to regulate the rate of S1 flows, leading to more queue buildup and congestion spreading. DCQCN combined with TCD improves FCT performance, especially when congestion is caused by small flow interference.

**Realistic Workloads:**
We choose two realistic heavy-tailed workloads: Hadoop [48] and WebSearch [12]. 90% of Hadoop flows are less than 120 KB, while 90% of WebSearch flows are less than 5 MB. The network is a Fat-Tree [9] network (k = 10) with 250 servers, 40 Gbps links, and 4 μs delay. The average link load is set to 60%, generating over 40,000 flows with exponential inter-arrival times. Figures 16(a) and 16(b) show FCT slowdown in the median, 95th, and 99th percentiles.

For the Hadoop workload, DCQCN with TCD significantly reduces FCT slowdown, especially for small flows. For flows smaller than 80 KB, the median FCT slowdown is reduced from 10.8 to 3.6. For flows smaller than 50 KB, the 99th-percentile FCT slowdown is improved by up to 1.7×. For flows larger than 100 KB, DCQCN with TCD performs comparably to DCQCN.

For the WebSearch workload, DCQCN with TCD also achieves better FCT slowdown, particularly for small and medium flows. For flows smaller than 500 KB, the median FCT slowdown is reduced from 4.6 to 2.5, and the 99th-percentile FCT slowdown is improved by 2×. For flows larger than 1 MB, the 99th-percentile FCT slowdown is almost the same as with DCQCN.

### 5.2.2 InfiniBand CC

With TCD, the receiver CA conveys both CE and UE information back to the sender CA. The sender CA does not update the sending rate when receiving a CNP marked with UE. We change the rate reduction step from the default 1 to 2 to aggressively decrease the rate of congested flows. Our simulator is based on the open-source project for InfiniBand [4] released by Mellanox, extended to support IB CC and TCD. The switch architecture uses virtual cut-through, input buffering with virtual output queues (VoQ), with each switch input port equipped with 280 KB of buffer space.

**Victim Flows Scenario:**
Figure 17(a) shows the average message completion time (MCT) breakdown of victim flows. Overall, IB CC with TCD achieves better average MCT than IB CC. Since the message sizes are larger than the BDP, the performance improvement is due to accurate congestion detection, allowing I/O messages to fully utilize available bandwidth without being throttled.

We also evaluate the average MCT of victim flows under varying burst sizes, with similar findings to ECN-based DCQCN (results not shown due to space limitations).

**Synthetic Workloads:**
We use synthetic communication patterns of typical MPI and I/O jobs to simulate HPC scenarios where multiple jobs share the network [15]. The network is a Fat-Tree [9] network (k = 16) with 1024 servers. The routing algorithm and other details are consistent with the previous setup.