at 1Gbps and F1 start at 8Gbps. Then A0 starts at line rate. In this
way, port P0 is in an undetermined state during A0 launching. F1
passes through the undetermined port P0 and also the congestion
port. We focus on the received marked packets observed at the
destination. Figure 11 illustrates the F0 marking fraction calculated
every 100 ms. For both CEE and InfiniBand, after A0 starts, port
P0 is detected as the undetermined state, and packets are marked
with UE. After A0 ends, port P0 recovers to non-congestion, and
no F0 packets are marked. We omit the results of F1. The marking
fraction of F1 is inverse to F0, with all packets marked with CE
during A0 launching.
5.1.2 Typical scenarios validation. Next, we validate the fined-
grained congestion detection behavior of TCD in the typical scenar-
ios of Section Â§ 3.1. Queue length evolution and marking behaviors
in each scenario are illustrated in Figure 12 and Figure 13.
In the single congestion point scenario, port P2 and port P1
experience the transition from the undetermined state to the non-
congestion state. Port P2 has more queue accumulation than port
P1. After P2 releases from the undetermined state and recovers
sending normally, it takes some time to drain out the accumulated
Congestion Detection in Lossless Networks
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
(a) [CEE] P2
(b) [CEE] P1
(a) [CEE] P2
(b) [CEE] P1
(c) [InfiniBand] P2
(d) [InfiniBand] P1
(c) [InfiniBand] P2
(d) [InfiniBand] P1
Figure 12: Single congestion point scenario. Port P2 and port
P1: undetermined to non-congestion state.
Figure 13: Multiple congestion points scenario. Port P2: un-
determined to congestion state. Port P1: undetermined state.
queue. During this period, the switch detects that the queue length
decreases continuously after an undetermined state. Hence packets
are not marked with CE even the queue length is over the threshold.
In the multiple congestion points scenario, port P2 experiences
the transition from the undetermined state to the congestion state.
After P2 releases from the undetermined state and recovers sending
normally, P2 still has persistent queue buildup. Hence P2 is detected
as congested, and packets are marked with CE. Port P1 is still in an
undetermined state because of congestion spreading from port P2.
5.1.3 Victim flows scenario. TCD can benefit victim flows by de-
tecting congestion and undetermined state accurately. With ECN or
FECN, victim flows may be mistakenly marked and are considered
congested. We use the topology in Figure 2 to evaluate TCD in the
typical head-of-line scenario. We set the link speed of links ğ‘†0 âˆ’ ğ‘‡ 0
and ğ‘†1 âˆ’ ğ‘‡ 0 to 20Gbps, and all remaining links are 40Gbps with
a delay of 4ğœ‡ğ‘ . Note that no flows are sent from host ğ‘†2 in this
scenario. Then all flows of ğ‘†0 are potential victim flows and should
not be detected as congested. If the number of packets marked
with CE is non-zero, we consider the flow is mistakenly detected
as congested. For CEE, hosts ğ‘†0 âˆ¼ ğ‘†1 and ğ´0 âˆ¼ ğ´14 generate flows
according to the heavy-tailed Hadoop workload [48] with expo-
nentially distributed inter-arrival time. The workload generators at
hosts ğ´0 âˆ¼ ğ´14 are set to be synchronous to simulate the concur-
rent bursts. The default congestion control algorithm is DCQCN.
For InfiniBand, hosts ğ‘†0 âˆ¼ ğ‘†1 and ğ´0 âˆ¼ ğ´14 generate MPI and
I/O messages in typical sizes [15]. As shown in Table 3, in both
networks, there are victim flows detected as congested. For CEE,
about 26% of flows are mistakenly marked with ECN. With TCD,
no victim flows are detected as congested.
5.1.4 Parameter sensitivity of ğœ€. ğœ€ determines the value of ğ‘šğ‘ğ‘¥ (ğ‘‡ğ‘œğ‘›).
ğ‘šğ‘ğ‘¥ (ğ‘‡ğ‘œğ‘›) gets larger as ğœ€ decreases. Because TCD detects the re-
lease from the undetermined state as soon as ğ‘šğ‘ğ‘¥ (ğ‘‡ğ‘œğ‘›) expires, too
Scheme
Fraction
ECN (CEE)
TCD (CEE)
FECN (IB)
TCD (IB)
26.6%
0%
13.5%
0%
Table
marked with CE.
3: Victim flows
Figure 14: Parameter sensi-
tivty of ğœ€.
small ğ‘šğ‘ğ‘¥ (ğ‘‡ğ‘œğ‘›) may result in detecting undetermined state mis-
takenly as congestion state or non-congestion state. On the other
hand, too large ğ‘šğ‘ğ‘¥ (ğ‘‡ğ‘œğ‘›) can also defer the detection of a conges-
tion state. To evaluate the parameter sensitivity of ğœ€, we repeat
the concurrent burst simulation with different ğœ€ values. Figure 14
shows the result. In total, a larger ğœ€ yields more mistakenly marked
packets. Packets of victim flows are not mistakenly marked with
CE when ğœ€ is smaller than 0.1. Considering various positive and
negative factors, our recommended value of 0.05 is proper.
5.2 Case Study
In this section, we conduct case studies to show how TCD incorpo-
rates existing congestion control algorithms. Our primary goal is
not to propose the optimal congestion control algorithm in lossless
networks but to underscore the significance of detecting congestion
states accurately. It is now possible to simply consider different
congestion states to improve end-to-end congestion controls. The
primary principles are as follows:
1) Congested flows should decrease their rate aggressively be-
cause they are real contributors to congestion.
2) Flows only passing through undetermined ports (i.e., undeter-
mined flows) should perform gentle rate adjustment. On the one
378
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Yiran Zhang, Yifan Liu, Qingkai Meng, Fengyuan Ren
(a) FCT breakdown.
(b) Varying burst size
(a) Hadoop workload
(b) WebSearch workload
Figure 15: FCT performance for victim flows
QCN+TCD).
(DC-
Figure 16: Overall FCT slowdown (DCQCN+TCD)
hand, undetermined flows may be victims that should not back off.
On the other hand, blindly increasing the rate of undetermined
flows may exacerbate congestion spreading. Specifically, if a packet
is marked with UE, we advocate keeping the flow rate until it be-
comes uncongested or congested.
5.2.1 DCQCN. With TCD, NP conveys CE as well as UE infor-
mation back to RP. Senders do not update the sending rate when
receiving a CNP marked with UE. We change the rate reduction
factor ğ›¼ from default 0.5 to 1.2 in order to decrease the rate of
congested flows aggressively. Our simulator is developed based on
the open-source project for DCQCN [55]. The PFC threshold ğ‘‹ğ‘œ ğ‘“ ğ‘“
and ğ‘‹ğ‘œğ‘› is 320ğ¾ğµ and 318ğ¾ğµ, respectively. Remaining parameters
are set to the default values recommended in [56].
Victim flows scenario: We first evaluate DCQCN with TCD in
the victim flow scenario. Figure 15(a) shows the average FCT break-
down of victim flows. On the whole, DCQCN with TCD achieves a
better average FCT than DCQCN. For flows with a size smaller than
10KB, the faster flow completion mainly benefits from aggressive
rate decrease of congested flows because their rate is not regulated
by end-to-end congestion control. With a more aggressive rate de-
crease, the congestion spreading is less and fewer flows suffer from
less queueing delay. For medium and large flows, the faster flow
completion time mainly benefits from detecting congestion accu-
rately. DCQCN does not mistakenly throttle these flows. Without
TCD, some victim flows are detected as congested.
To further study the performance under small flows, we let ğ´0 âˆ¼
ğ´14 generate concurrent bursts with varying sizes. The inter-arrival
time of each round of concurrent burst follows an exponential
distribution. Hosts ğ‘†0 âˆ¼ ğ‘†1 generate flows according to Hadoop
workload as before. The BDP is 80ğ¾ğµ in this scenario. Figure 15(b)
shows the average FCT and TCD marking behaviors of victim flows.
When the burst size is small, there is almost no congestion spreading.
As the burst size increases, it is harder for end-to-end congestion
control to regulate the rate of flows of ğ‘†1 promptly, introducing
more queue buildup and a larger extent of congestion spreading.
As a result, more victim flows are marked as undetermined flows.
When burst size is medium (e.g., 250KB), the traffic is overloaded,
and congestion is severe. Overall, DCQCN combining with TCD
can improve the FCT performance of victim flows, especially when
congestion is caused by interference of small flows.
Realistic workloads: We choose two realistic heavy-tailed work-
loads: Hadoop workload [48] and WebSearch workload [12]. 90%
flows of the Hadoop workload are less than 120KB. The WebSearch
workload is heavier, with 90% flows less than 5MB. The network is
a Fat-Tree [9] network (k = 10) with 250 servers. The link capability
is 40Gbps with 4ğœ‡s link delay. We adjust the flow generation rates
to set the average link loads to 60%. We generate over 40 thou-
sand flows with exponentially distributed inter-arrival time. Figure
16(a) and Figure 16(b) demonstrate FCT slowdown in the median,
95th and 99th percentile. FCT slowdown is calculated by the ratio
between real FCT and baseline FCT.
For Hadoop workload, DCQCN with TCD achieves a much better
FCT slowdown, especially for small flows. DCQCN with TCD re-
duces the median FCT slowdown from 10.8 to 3.6 for flows smaller
than 80KB. For flows smaller than 50KB, DCQCN with TCD im-
proves the 99th-percentile FCT slowdown by at most 1.7Ã—. For
flows larger than 100KB, DCQCN with TCD achieves comparable
performance with DCQCN.
For WebSearch workload, DCQCN with TCD achieves a better
FCT slowdown, especially for small and medium flows. DCQCN
with TCD reduces the median FCT slowdown from 4.6 to 2.5 for
flows smaller than 500KB and achieves 2Ã— better 99th-percentile
FCT slowdown. The 99th-percentile FCT slowdown is almost the
same with DCQCN for flows larger than 1MB.
IB CC. With TCD, the receiver CA conveys CE as well as UE
5.2.2
information back to the sender CA. Sender CA does not update the
sending rate when receiving a CNP marked with UE. We change
the rate reduction step from default 1 to 2 to decrease the rate of
congested flows aggressively. Our simulator is developed based on
the open-source project for InfiniBand [4] released by Mellanox.
We extend this model by adding support for IB CC and TCD. The
switch architecture is virtual cut-through, input buffering with
virtual output queues (VoQ). Each switch input port is equipped
with 280KB of buffer space.
Victim flows scenario: Figure 17(a) shows the average message
completion time (MCT) breakdown of victim flows. On the whole, IB
CC with TCD achieves a better average MCT than IB CC. Since the
message sizes are larger than BDP, the performance improvement
benefits from accurate congestion detection. I/O messages can fully
utilize the available bandwidth without being throttled innocently.
We also evaluate the average MCT of victim flows under varying
burst sizes, and have similar findings with ECN-based DCQCN
(results are not presented due to space limitation).
Synthetic workloads: We choose the synthetic communication
patterns of typical MPI and I/O jobs to simulate HPC scenarios
where multiple jobs share the network [15]. The network is a Fat-
Tree [9] network (k = 16) with 1024 servers. The routing algorithm
379
Congestion Detection in Lossless Networks
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
(a) Victim performance
(b) Overall MCT performance
(a) Hadoop workload
(b) WebSearch workload
Figure 17: MCT performance (IB CC+TCD).
Figure 19: Overall FCT slowdown (TIMELY+TCD).
(a) FCT breakdown
(b) Varying burst size
Figure