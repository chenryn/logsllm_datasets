to Write and Read operations. We improve and extend this
design to support all RDMA operations with IRN.
We classify the issues due to out-of-order packet delivery
into four categories.
5.3.1 First packet issues. For some RDMA operations,
critical information is carried in the first packet of a message,
which is required to process other packets in the message.
Enabling OOO delivery, therefore, requires that some of the
information in the first packet be carried by all packets.
In particular, the RETH header (containing the remote
memory location) is carried only by the first packet of a
Write message. IRN requires adding it to every packet.
5.3.2 WQE matching issues. Some operations require
every packet that arrives to be matched with its correspond-
ing WQE at the responder. This is done implicitly for in-order
packet arrivals. However, this implicit matching breaks with
4For QPs that only send single packet messages less than one MTU in size,
the number of outstanding packets is limited to the maximum number of
outstanding requests, which is typically smaller than the BDP cap [25, 26].
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
OOO packet arrivals. A work-around for this is assigning ex-
plicit WQE sequence numbers, that get carried in the packet
headers and can be used to identify the corresponding WQE
for each packet. IRN uses this workaround for the following
RDMA operations:
Send and Write-with-immediate: It is required that Receive
WQEs be consumed by Send and Write-with-immediate re-
quests in the same order in which they are posted. Therefore,
with IRN every Receive WQE, and every Request WQE for
these operations, maintains a recv_WQE_SN that indicates
the order in which they are posted. This value is carried in all
Send packets and in the last Write-with-Immediate packet, 5
and is used to identify the appropriate Receive WQE. IRN
also requires the Send packets to carry the relative offset in
the packet sequence number, which is used to identify the
precise address when placing data.
Read/Atomic: The responder cannot begin processing a Read-
/Atomic request R, until all packets expected to arrive before
R have been received. Therefore, an OOO Read/Atomic Re-
quest packet needs to be placed in a Read WQE buffer at the
responder (which is already maintained by current RoCE
NICs). With IRN, every Read/Atomic Request WQE main-
tains a read_WQE_SN, that is carried by all Read/Atomic
request packets and allows identification of the correct index
in this Read WQE buffer.
5.3.3 Last packet issues. For many RDMA operations,
critical information is carried in last packet, which is required
to complete message processing. Enabling OOO delivery,
therefore, requires keeping track of such last packet arrivals
and storing this information at the endpoint (either on NIC
or main memory), until all other packets of that message
have arrived. We explain this in more details below.
A RoCE responder maintains a message sequence number
(MSN) which gets incremented when the last packet of a
Write/Send message is received or when a Read/Atomic re-
quest is received. This MSN value is sent back to the requester
in the ACK packets and is used to expire the corresponding
Request WQEs. The responder also expires its Receive WQE
when the last packet of a Send or a Write-With-Immediate
message is received and generates a CQE. The CQE is pop-
ulated with certain meta-data about the transfer, which is
carried by the last packet. IRN, therefore, needs to ensure
that the completion signalling mechanism works correctly
even when the last packet of a message arrives before others.
For this, an IRN responder maintains a 2-bitmap, which in
addition to tracking whether or not a packet p has arrived,
also tracks whether it is the last packet of a message that will
trigger (1) an MSN update and (2) in certain cases, a Receive
5A Receive WQE is consumed only by the last packet of a Write-with-
immediate message, and is required to process all packets for a Send
message.
WQE expiration that is followed by a CQE generation. These
actions are triggered only after all packets up to p have been
received. For the second case, the recv_WQE_SN carried by
p (as discussed in §5.3.2) can identify the Receive WQE with
which the meta-data in p needs to be associated, thus en-
abling a premature CQE creation. The premature CQE can
be stored in the main memory, until it gets delivered to the
application after all packets up to p have arrived.
5.3.4 Application-level Issues. Certain applications
(for example FaRM [21]) rely on polling the last packet of a
Write message to detect completion, which is incompatible
with OOO data placement. This polling based approach vi-
olates the RDMA specification (Sec o9-20 [4]) and is more
expensive than officially supported methods (FaRM [21] men-
tions moving on to using the officially supported Write-with-
Immediate method in the future for better scalability). IRN’s
design provides all of the Write completion guarantees as
per the RDMA specification. This is discussed in more details
in Appendix §B of the extended report [31].
OOO data placement can also result in a situation where
data written to a particular memory location is overwritten
by a restransmitted packet from an older message. Typically,
applications using distributed memory frameworks assume
relaxed memory ordering and use application layer fences
whenever strong memory consistency is required [14, 36].
Therefore, both iWARP and Mellanox ConnectX-5, in sup-
porting OOO data placement, expect the application to deal
with the potential memory over-writing issue and do not
handle it in the NIC or the driver. IRN can adopt the same
strategy. Another alternative is to deal with this issue in the
driver, by enabling the fence indicator for a newly posted
request that could potentially overwrite an older one.
5.4 Other Considerations
Currently, the packets that are sent and received by a re-
quester use the same packet sequence number (PSN ) space.
This interferes with loss tracking and BDP-FC. IRN, there-
fore, splits the PSN space into two different ones (1) sPSN
to track the request packets sent by the requester, and (2)
rPSN to track the response packets received by the requester.
This decoupling remains transparent to the application and
is compatible with the current RoCE packet format. IRN can
also support shared receive queues and send with invalidate
operations and is compatible with use of end-to-end credit.
We provide more details about these in Appendix §B of the
extended report [31].
6 Evaluating Implementation Overheads
We now evaluate IRN’s implementation overheads over cur-
rent RoCE NICs along the following three dimensions: in
§6.1, we do a comparative analysis of IRN’s memory require-
ments; in §6.2, we evaluate the overhead for implementing
Revisiting Network Support for RDMA
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
IRN’s packet processing logic by synthesizing it on an FPGA;
and in §6.3, we evaluate, via simulations, how IRN’s imple-
mentation choices impact end-to-end performance.
6.1 NIC State overhead due to IRN
Mellanox RoCE NICs support several MBs of cache to store
various metadata including per-QP and per-WQE contexts.
The additional state that IRN introduces consumes a total
of only 3-10% of the current NIC cache for a couple of thou-
sands of QPs and tens of thousands of WQEs, even when
considering large 100Gbps links. We present a breakdown
this additional state below.
Additional Per-QP Context:
State variables: IRN needs 52 bits of additional state for its
transport logic: 24 bits each to track the packet sequence
to be retransmitted and the recovery sequence, and 4 bits
for various flags. Other per-flow state variables needed for
IRN’s transport logic (e.g., expected sequence number) are
already maintained by current RoCE NICs. Hence, the per-
QP overhead is 104 bits (52 bits each at the requester and the
responder). Maintaining a timer at the responder for Read
timeouts and a variable to track in-progress Read requests
in the Read WQE buffer adds another 56 bits to the respon-
der leading to a total of 160 bits of additional per-QP state
with IRN. For context, RoCE NICs currently maintain a few
thousands of bits per QP for various state variables.
Bitmaps: IRN requires five BDP-sized bitmaps: two at the
responder for the 2-bitmap to track received packets, one
at the requester to track the Read responses, one each at
the requester and responder for tracking selective acks. As-
suming each bitmap to be 128 bits (i.e., sized to fit the BDP
cap for a network with bandwidth 40Gbps and a two-way
propagation delay of up to 24µs, typical in today’s datacen-
ter topologies [29]), IRN would require a total of 640 bits
per QP for bitmaps. This is much less than the total size of
bitmaps maintained by a QP for the OOO support in Mel-
lanox ConnectX-5 NICs.
Others: Other per-QP meta-data that is needed by an IRN
driver when a WQE is posted (e.g counters for assigning
WQE sequence numbers) or expired (e.g. premature CQEs)
can be stored directly in the main memory and do not add
to the NIC memory overhead.
Additional Per-WQE Context: As described in §5, IRN
maintains sequence numbers for certain types of WQEs. This
adds 3 bytes to the per-WQE context which is currently sized
at 64 bytes.
Additional Shared State: IRN also maintains some addi-
tional variables (or parameters) that are shared across QPs.
This includes the BDP cap value, the RTOlow value, and N
for RTOlow, which adds up to a total of only 10 bytes.
6.2 IRN’s packet processing overhead
We evaluate the implementation overhead due to IRN’s per-
packet processing logic, which requires various bitmap ma-
nipulations. The logic for other changes that IRN makes –
e.g., adding header extensions to packets, premature CQE
generation, etc. – are already implemented in RoCE NICs
and can be easily extended for IRN.
We use Xilinx Vivado Design Suite 2017.2 [3] to do a high-
level synthesis of the four key packet processing modules (as
described below), targeting the Kintex Ultrascale XCKU060
FPGA which is supported as a bump-on-the-wire on the
Mellanox Innova Flex 4 10/40Gbps NICs [12].
6.2.1 Synthesis Process. To focus on the additional
packet processing complexity due to IRN, our implementa-
tion for the four modules is stripped-down. More specifically,
each module receives the relevant packet metadata and the
QP context as streamed inputs, relying on a RoCE NIC’s
existing implementation to parse the packet headers and
retrieve the QP context from the NIC cache (or the system
memory, in case of a cache miss). The updated QP context
is passed as streamed output from the module, along with
other relevant module-specific outputs as described below.
(1) receiveData: Triggered on a packet arrival, it outputs the
relevant information required to generate an ACK/NACK
packet and the number of Receive WQEs to be expired, along
with the updated QP context (e.g. bitmaps, expected sequence
number, MSN).
(2) txFree: Triggered when the link’s Tx is free for the QP
to transmit, it outputs the sequence number of the packet
to be (re-)transmitted and the updated QP context (e.g. next
sequence to transmit). During loss-recovery, it also performs
a look ahead by searching the SACK bitmap for the next
packet sequence to be retransmitted.
(3) receiveAck: Triggered when an ACK/NACK packet arrives,
it outputs the updated QP context (e.g. SACK bitmap, last
acknowledged sequence).
(4) timeout: If triggered when the timer expires using RTOlow
value (indicated by a flag in the QP context), it checks if the
condition for using RTOlow holds. If not, it does not take
any action and sets an output flag to extend the timeout
to RTOhiдh. In other cases, it executes the timeout action
and returns the updated QP context. Our implementation
relies on existing RoCE NIC’s support for setting timers, with
the RTOlow value being used by default, unless explicitly
extended.
The bitmap manipulations in the first three modules ac-
count for most of the complexity in our synthesis. Each
bitmap was implemented as a ring buffer, using an arbitrary
precision variable of 128 bits, with the head corresponding
to the expected sequence number at the receiver (or the cu-
mulative acknowledgement number at the sender). The key
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
Module
Name
receiveData
txFree
receiveAck
timeout
Resource Usage
LUT
1.93%
0.95%
1.05%
0.08%
FF
0.62%
0.32%
0.4%
0.01%
Max
Latency
16.5 ns
15.9 ns
15.96 ns
<6.3 ns
Min
Throughput
45.45 Mpps
47.17 Mpps
46.99 Mpps
318.47 Mpps
Total Resource Usage: 1.35% FF and 4.01% LUTs
Min Bottleneck Tpt: 45.45Mpps
Table 2: Performance and resource usage for differ-
ent packet processing modules on Xilinx Kintex Ultra-
scale KU060 FPGA.
bitmap manipulations required by IRN can be reduced to
the following three categories of known operations: (i) find-
ing first zero, to find the next expected sequence number in
receiveData and the next packet to retransmit in txFree (ii)
popcount to compute the increment in MSN and the num-
ber of Receive WQEs to be expired in receiveData, (iii) bit
shifts to advance the bitmap heads in receiveData and re-
ceiveAck. We optimized the first two operations by dividing
the bitmap variables into chunks of 32 bits and operating on
these chunks in parallel.