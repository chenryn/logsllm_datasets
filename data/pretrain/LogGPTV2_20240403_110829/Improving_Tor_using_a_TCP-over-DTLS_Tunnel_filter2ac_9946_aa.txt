title:Improving Tor using a TCP-over-DTLS Tunnel
author:Joel Reardon and
Ian Goldberg
Improving Tor using a TCP-over-DTLS Tunnel
Joel Reardon∗
Google Switzerland GmbH
Brandschenkestrasse 110
Z¨urich, Switzerland
Ian Goldberg
University of Waterloo
200 University Ave W.
Waterloo, ON, Canada
PI:EMAIL
PI:EMAIL
Abstract
The Tor network gives anonymity to Internet users by re-
laying their trafﬁc through the world over a variety of
routers. All trafﬁc between any pair of routers, even
if they represent circuits for different clients, are mul-
tiplexed over a single TCP connection. This results in
interference across circuits during congestion control,
packet dropping and packet reordering. This interference
greatly contributes to Tor’s notorious latency problems.
Our solution is to use a TCP-over-DTLS (Datagram
Transport Layer Security) transport between routers. We
give each stream of data its own TCP connection, and
protect the TCP headers—which would otherwise give
stream identiﬁcation information to an attacker—with
DTLS. We perform experiments on our implemented
version to illustrate that our proposal has indeed resolved
the cross-circuit interference.
1 Introduction
Tor [2] is a tool to enable Internet privacy that has seen
widespread use and popularity throughout the world.
Tor consists of a network of thousands of nodes—
known as Onion Routers (ORs)—whose operators have
volunteered to relay Internet trafﬁc around the world.
Clients—known as Onion Proxies (OPs)—build circuits
through ORs in the network to dispatch their trafﬁc. Tor’s
goal is to frustrate an attacker who aims to match up
the identities of the clients with the actions they are per-
forming. Despite its popularity, Tor has a problem that
dissuades its ubiquitous application—it imposes greater
latency on its users than they would experience without
Tor.
While some increased latency is inevitable due to the
increased network path length, our experiments show
that this effect is not sufﬁcient to explain the increased
cost. In Section 2 we look deeper, and ﬁnd a component
∗Work done while at the University of Waterloo
of the transport layer that can be changed to improve
Tor’s performance. Speciﬁcally, each pair of routers
maintains a single TCP connection for all trafﬁc that is
sent between them. This includes multiplexed trafﬁc for
different circuits, and results in cross-circuit interference
that degrades performance. We ﬁnd that congestion con-
trol mechanisms are being unfairly applied to all circuits
when they are intended to throttle only the noisy senders.
We also show how packet dropping on one circuit causes
interference on other circuits.
Section 3 presents our solution to this problem—a new
transport layer that is backwards compatible with the ex-
isting Tor network. Routers in Tor can gradually and in-
dependently upgrade, and our system provides immedi-
ate beneﬁt to any pair of routers that choose to use our
improvements.
It uses a separate TCP connection for
each circuit, but secures the TCP header to avoid the dis-
closure of per-circuit data transfer statistics. Moreover, it
uses a user-level TCP implementation to address the is-
sue of socket proliferation that prevents some operating
systems from being able to volunteer as ORs.
Section 4 presents experiments to compare the exist-
ing Tor with our new implementation. We compare la-
tency and throughput, and perform timing analysis of our
changes to ensure that they do not incur non-negligible
computational latency. Our results are favourable: the
computational overhead remains negligible and our solu-
tion is successful in addressing the improper use of con-
gestion control.
Section 5 compares our enhanced Tor to other ano-
nymity systems, and Section 6 concludes with a descrip-
tion of future work.
1.1 Apparatus
Our experiments were performed on a commodity
Thinkpad R60—1.66 GHz dual core with 1 GB of RAM.
Care was taken during experimentation to ensure that the
system was never under load signiﬁcant enough to inﬂu-
ence the results. Our experiments used a modiﬁed ver-
sion of the Tor 0.2.0.x stable branch code.
2 Problems with Tor’s Transport Layer
We begin by brieﬂy describing the important aspects of
Tor’s current transport layer. For more details, see [2].
An end user of Tor runs an Onion Proxy on her machine,
which presents a SOCKS proxy interface [7] to local ap-
plications, such as web browsers. When an application
makes a TCP connection to the OP, the OP splits it into
ﬁxed-size cells which are encrypted and forwarded over
a circuit composed of (usually 3) Onion Routers. The
last OR creates a TCP connection to the intended desti-
nation host, and passes the data between the host and the
circuit.
The circuit is constructed with hop-by-hop TCP con-
nections, each protected with TLS [1], which provides
conﬁdentiality and data integrity. The OP picks a ﬁrst
OR (OR1), makes a TCP connection to it, and starts TLS
on that connection. It then instructs OR1 to connect to
a particular second OR (OR2) of the OP’s choosing. If
OR1 and OR2 are not already in contact, a TCP connec-
tion is established between them, again with TLS. If OR1
and OR2 are already in contact (because other users, for
example, have chosen those ORs for their circuits), the
existing TCP connection is used for all trafﬁc between
those ORs. The OP then instructs OR2 to contact a third
OR, OR3, and so on. Note that there is not an end-to-end
TCP connection from the OP to the destination host, nor
to any OR except OR1.
This multi-hop transport obviously adds additional un-
avoidable latency. However, the observed latency of Tor
is larger than accounted for simply by the additional
transport time.
In [12], the ﬁrst author of this paper
closely examined the sources of latency in a live Tor
node. He found that processing time and input buffer
queueing times were negligible, but that output buffer
queueing times were signiﬁcant. For example, on an in-
strumented Tor node running on the live Tor network,
40% of output buffers had data waiting in them from
100 ms to over 1 s more than 20% of the time. The
data was waiting in these buffers because the the operat-
ing system’s output buffer for the corresponding socket
was itself full, and so the OS was reporting the socket
as unwritable. This was due to TCP’s congestion control
mechanism, which we discuss next.
Socket output buffers contain two kinds of data:
packet data that has been sent over the network but is un-
acknowledged1, and packet data that has not been sent
due to TCP’s congestion control. Figure 1 shows the
1Recall that TCP achieves reliability by buffering all data locally
until it has been acknowledged, and uses this to generate retransmission
messages when necessary
size of the socket output buffer over time for a partic-
ular connection. First, unwritable sockets occur when
the remaining capacity in an output buffer is too small to
accept new data. This in turn occurs because there is al-
ready too much data in the buffer, which is because there
is too much unacknowledged data in ﬂight and throt-
tled data waiting to be sent. The congestion window
(CWND) is a variable that stores the number of packets
that TCP is currently willing to send to the peer. When
the number of packets in ﬂight exceeds the congestion
window then the sending of more data is throttled until
acknowledgments are received. Once congestion throt-
tles sending, the data queues up until either packets are
acknowledged or the buffer is full.
In addition to congestion control, TCP also has a ﬂow
control mechanism. Receivers advertise the amount of
data they are willing to accept; if more data arrives at
the receiver before the receiving application has a chance
to read from the OS’s receive buffers, this advertised re-
ceiver window will shrink, and the sender will stop trans-
mitting when it reaches zero. In none of our experiments
did we ever observe Tor throttling its transmissions due
to this mechanism; the advertised receiver window sizes
never dropped to zero, or indeed below 50 KB. Conges-
tion control, rather than ﬂow control, was the reason for
the throttling.
While data is delayed because of congestion control, it
is foolhardy to attempt to circumvent congestion control
as a means of improving Tor’s latency. However, we ob-
serve that Tor’s transport between ORs results in an un-
fair application of congestion control. In particular, Tor’s
circuits are multiplexed over TCP connections; i.e., a sin-
gle TCP connection between two ORs is used for multi-
ple circuits. When a circuit is built through a pair of un-
connected routers, a new TCP connection is established.
When a circuit is built through an already-connected pair
of ORs, the existing TCP stream will carry both the ex-
isting circuits and the new circuit. This is true for all
circuits built in either direction between the ORs.
In this section we explore how congestion control af-
fects multiplexed circuits and how packet dropping and
reordering can cause interference across circuits. We
show that TCP does not behave optimally when circuits
are multiplexed in this manner.
2.1 Unfair Congestion Control
We believe that multiplexing TCP streams over a sin-
gle TCP connection is unwise and results in the unfair
application of TCP’s congestion control mechanism. It
results in multiple data streams competing to send data
over a TCP stream that gives more bandwidth to circuits
that send more data; i.e., it gives each byte of data the
same priority regardless of its source. A busy circuit that
40
35
30
25
20
15
10
)
B
K
(
e
z
i
S
r
e
f
f
u
B
t
u
p
t
u
O
t
e
k
c
o
S
5
0
0
Socket Output Buffer Size and Unacknowledged Packets
Socket Output Buffer Size
Unacknowledged Packets
Unwritable Socket
100
200
300
400
500
600
Time (seconds)
Figure 1: TCP socket output buffer size, writability, and unacknowledged packets over time.
triggers congestion control will cause low-bandwidth cir-
cuits to struggle to have their data sent. Figure 2 il-
lustrates data transmission for distinct circuits entering
and exiting a single output buffer in Tor. Time increases
along the X-axis, and data increases along the Y-axis.
The main part of the ﬁgure shows two increasing line
shapes, each corresponding to the data along a different
circuit over time. When the shapes swell, that indicates
that Tor’s internal output buffer has swelled: the left edge
grows when data enters the buffer, and the right edge
grows when data leaves the buffer. This results in the
appearance of a line when the buffer is well-functioning,
and a triangular or parallelogram shape when data ar-
rives too rapidly or the connection is troubled. Addition-
ally, we strike a vertical line across the graph whenever a
packet is dropped.
What we learn from this graph is that the buffer serves
two circuits. One circuit serves one MB over ten min-
utes, and sends cells evenly. The other circuit is inactive
for the most part, but three times over the execution it
suddenly serves 200 KB of cells. We can see that each
time the buffer swells with data it causes a signiﬁcant
delay. Importantly, the other circuit is affected despite
the fact that it did not change its behaviour. Conges-
tion control mechanisms that throttle the TCP connec-
tion will give preference to the burst of writes because it
simply provides more data, while the latency for a low-
bandwidth application such as ssh increases unfairly.
2.2 Cross-Circuit Interference
Tor multiplexes the data for a number of circuits over a
single TCP stream, and this ensures that the received data
will appear in the precise order in which the component
streams were multiplexed—a guarantee that goes beyond
what is strictly necessary. When packets are dropped
or reordered, the TCP stack will buffer available data
on input buffers until the missing in-order component is
available. We hypothesize that when active circuits are
multiplexed over a single TCP connection, Tor suffers an
unreasonable performance reduction when either packet
dropping or packet reordering occur. Cells may be avail-
able in-order for one particular circuit but are being de-
layed due to missing cells for another circuit. In-order
guarantees are only necessary for data sent within a sin-
gle circuit, but the network layer ensures that data is only
readable in the order it was dispatched. Packet loss or re-
ordering will cause the socket to indicate that no data is
available to read even if other circuits have their sequen-
tial cells available in buffers.
Figure 3 illustrates the classic head-of-line blocking
behaviour of Tor during a packet drop; cells for distinct
circuits are represented by shades and a missing packet
is represented with a cross. We see that the white, light
grey, and black circuits have had all of their data suc-
cessfully received, yet the kernel will not pass that data
to the Tor application until the dropped dark grey packet
is retransmitted and successfully received.
We verify our cross-circuit interference hypothesis in
two parts. In this section we show that packet drops on a
Buffer Sizes across Circuits
1.2
1.0
0.8
0.6
0.4
0.2
)
B
M
(
a
t
a
D
0.0
0
100
200
300
400
Time (seconds)
500
600
700
Figure 2: Example of congestion on multiple streams.
TCP Stream (over network)
OR
Kernel TCP 
Buffered / Waiting
Readable
OR
Figure 3: TCP correlated streams. Shades correspond to cells for different circuits.
Experiment 1 Determining the effect of packet dropping
on circuit multiplexing.
1: A Tor network of six ORs on a single host was con-
ﬁgured to have a latency of 50 milliseconds and a
variable packet drop rate.
2: Eight OP built circuits that were ﬁxed so that the sec-
ond and third ORs were the same for each client, but
the ﬁrst hop was evenly distributed among the re-
maining ORs. Figure 4 illustrates this setup.
3: There were three runs of the experiment. The ﬁrst
did not drop any packets. The second dropped 0.1%
of packets on the shared link, and the third dropped
0.1% of packets on the remaining links.
4: The ORs were initialized and then the clients were
run until circuits were established.
5: Each OP had a client connect, which would tunnel a
connection to a timestamp server through Tor. The
server sends a continuous stream of timestamps. The
volume of timestamps measures throughput, and the
difference in time measures latency.
6: Data was collected for one minute.
shared link degrade throughput much more severely than
drops over unshared links. Then in Section 4 we show
that this effect disappears with our proposed solution.
To begin, we performed Experiment 1 to investigate
the effect of packet dropping on circuit multiplexing.
The layout of circuits in the experiment, as shown in
Figure 4, is chosen so that there is one shared link that
carries data for all circuits, while the remaining links do
not.
In the two runs of our experiments that drop packets,
they are dropped according to a target drop rate, either on
the heavily shared connection or the remaining connec-
tions. Our packet dropping tool takes a packet, decides
if it is eligible to be dropped in this experiment, and if
so then it drops it with the appropriate probability. How-
ever, this model means the two runs that drop packets
will see different rates of packet dropping systemwide,
since we observe greater trafﬁc on the remaining con-
nections. This is foremost because it spans two hops