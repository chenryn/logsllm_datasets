 **Apache Airflow version** : 1.10.10 and 1.10.9 has been verified
**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):  
no Kube
**Environment** :
  * **Others** : docker environment with java 1.8
**What happened** :
The existing function to extract a job id from the monitoring page is no
longer compatible with new beam 2.20:
airflow/airflow/contrib/hooks/gcp_dataflow_hook.py
Lines 142 to 148 in 9669718
|  def _extract_job(line):  
---|---  
|  # Job id info: https://goo.gl/SE29y9.  
|  job_id_pattern = re.compile(  
|  br'.*console.cloud.google.com/dataflow.*/jobs/([a-z|0-9|A-Z|\\-|\\_]+).*')  
|  matched_job = job_id_pattern.search(line or '')  
|  if matched_job:  
|  return matched_job.group(1).decode()  
The monitoring page includes the zone as part of the URL causing the regex to
extract always the `zone` instead of the `job_id`
A new regex compatible with the new and old monitoring url needs to be added
to the operator:
https://github.com/apache/beam/blob/v2.20.0/runners/google-cloud-dataflow-
java/src/main/java/org/apache/beam/runners/dataflow/util/MonitoringUtil.java#L192-L195
As per the been new configuration region is extracted instead of the job id