VBI
Ensemble
wEnsemble
Vanilla
Temp scaling
MC Dropout
VBI
Ensemble
wEnsemble
Vanilla
Temp scaling
MC Dropout
VBI
Ensemble
wEnsemble
Vanilla
Temp scaling
MC Dropout
VBI
Ensemble
wEnsemble
FNR
3.90
3.90
3.45
3.36
3.99
3.99
2.16
2.16
2.97
8.45
2.52
2.88
8.53
8.53
7.62
12.7
7.44
4.99
4.14
4.14
3.03
3.22
3.13
3.49
Detection estimation (%)
FPR
0.31
0.31
0.32
0.83
0.19
0.20
0.63
0.63
0.39
0.33
0.39
0.26
0.69
0.69
0.66
0.35
0.11
0.27
1.53
1.53
1.40
1.67
0.75
0.59
Acc
99.28
99.28
99.32
98.88
99.37
99.36
99.20
99.20
99.31
98.73
99.36
99.44
98.41
98.41
98.54
98.22
99.05
99.18
98.17
98.17
98.41
98.15
98.98
99.07
bAcc
97.90
97.90
98.12
97.91
97.91
97.90
98.61
98.61
98.32
95.61
98.55
98.43
95.39
95.39
95.86
93.47
96.23
97.37
97.17
97.17
97.78
97.55
98.06
97.96
Uncertainty evaluation
F1
96.84
96.84
97.04
95.22
97.24
97.20
96.58
96.58
97.03
94.35
97.26
97.56
92.99
92.99
93.57
91.88
95.73
96.41
92.30
92.30
93.32
92.29
95.60
95.98
NLL
0.100
0.052
0.033
0.054
0.063
0.058
0.087
0.053
0.077
0.073
0.063
0.046
0.097
0.076
0.078
0.084
0.049
0.050
0.119
0.101
0.088
0.118
0.055
0.046
bNLL
0.329
0.109
0.094
0.094
0.211
0.190
0.207
0.086
0.225
0.253
0.188
0.153
0.311
0.220
0.239
0.305
0.171
0.131
0.208
0.167
0.142
0.202
0.107
0.101
BSE
0.007
0.006
0.006
0.009
0.005
0.005
0.007
0.007
0.005
0.010
0.005
0.005
0.013
0.013
0.012
0.014
0.008
0.007
0.016
0.016
0.014
0.015
0.009
0.007
bBSE
0.020
0.018
0.015
0.016
0.018
0.018
0.013
0.012
0.014
0.036
0.012
0.013
0.039
0.038
0.035
0.052
0.029
0.022
0.026
0.026
0.020
0.022
0.016
0.016
ECE
0.006
0.007
0.002
0.012
0.005
0.004
0.007
0.010
0.003
0.004
0.004
0.002
0.009
0.002
0.004
0.010
0.006
0.008
0.014
0.015
0.013
0.015
0.010
0.008
uECE
0.104
0.062
0.056
0.102
0.160
0.095
0.162
0.182
0.072
0.078
0.144
0.045
0.059
0.023
0.055
0.092
0.162
0.066
0.205
0.180
0.207
0.262
0.143
0.116
datasets obtained after removing the examples for which the de-
tectors are uncertain (i.e., with an entropy value above a threshold
τ); this corresponds to the real-world usefulness of quantifying
predictive uncertainty (i.e., discarding prediction results for which
detector is uncertain).
Table 2 summarizes the uncertainty evaluation and the cor-
responding detection accuracy. We make three observations. (i)
Malware detectors achieve low accuracy with out-of-source test
examples. Nevertheless, DeepDrebin incorporating VBI obtain an
accuracy of 82.69%, which notably outperforms other detectors. It is
reminding that VBI hinders the detection accuracy in the absence of
dataset shift. (ii) Calibration methods (e.g., VBI or Ensemble) reduce
the uncertainty in terms of bNLL and bBSE when compared with
the vanilla models, except for the MultimodalNN model incorporat-
ing MC dropout. (iii) DeepDrebin incorporating VBI also achieves
the best calibration results, suggesting that VBI benefits from both
regularization and calibration. On the other hand, DeepDroid and
Droidetec suffer from the setting of out of source. Both models han-
dle the very long sequential data that would be truncated due to
the limited GPU memory, leading to the inferior results.
Figure 4 illustrates the density of predictive entropy. Figure 4b
further shows the accuracy on the examples after removing the ones
for which the detectors are uncertain about their predictions. We
make the following observations. (i) The vanilla models return zero
(a) # of examples per bin.
(b) Reliability diagram
Figure 3: Reliability diagram of vanilla malware detectors.
There are 5 bins B1, . . . , B5 and “Benware” denotes the benign
software.
bNLL, bBSE and uECE) are more sensitive than their imbalanced
counterparts (i.e., NLL, BSE and ECE) when data imbalance is present.
4.3 Answering RQ2
In order to quantify the predictive uncertainty of malware detec-
tors with respect to out-of-source examples, we apply the Drebin
malware detectors to the VirusShare dataset. We assess predictive
distribution and report the accuracy of malware detectors on the
B1B2B3B4B5Benware0e02e34e36e38e3# of examplesB1B2B3B4B5MalwareDeepDrebinMultimodalNNDeepDroidDroidetec0.00.20.40.60.81.0Mean of predicted value0.00.20.40.60.81.0Ratio of positivesWell-calibrated602ACSAC ’21, December 6–10, 2021, Virtual Event, USA
D. Li, T. Qiu, S. Chen, Q. Li, and S. Xu
(a)
(b)
Figure 4: Predictive entropy (see Eq.(7)) of malware detectors trained on the Drebin dataset and tested on the VirusShare: (a)
histogram of predictive entropy; (b) accuracy on the dataset after excluding the examples for which the detector has high