## 思考题

通过上述"隧道"机制，Flannel 实现了容器间三层网络（IP地址）的连通性。然而，基于这种工作原理，你认为 Flannel 能够保证二层网络（MAC 地址）的连通性吗？为什么？

感谢你的收听，欢迎留言讨论，并分享给更多朋友一起学习。

---

# 34 \| Kubernetes 网络模型与 CNI 网络插件

你好，我是张磊。今天我要和大家分享的主题是：Kubernetes 网络模型与 CNI 网络插件。

在上一篇文章中，我以 Flannel 项目为例，详细介绍了两种实现容器跨主机通信的方法：UDP 和 VXLAN。这些例子有一个共同点，即用户的容器都连接到 `docker0` 网桥上。网络插件会在宿主机上创建一个特殊设备（例如 UDP 模式的 TUN 设备或 VXLAN 模式的 VTEP 设备），并通过 IP 转发（路由表）与 `docker0` 协作。最终，网络插件的任务是通过某种方式将不同宿主机上的这些特殊设备连通，从而实现容器间的跨主机通信。

实际上，这正是 Kubernetes 对容器网络的主要处理方法。不过，Kubernetes 使用了一个名为 CNI 的接口，并维护了一个单独的网桥来替代 `docker0`。这个网桥被称为 CNI 网桥，默认设备名称为 `cni0`。

以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境中，其工作方式与我们在上一篇文章中的描述基本相同，只是将 `docker0` 网桥替换成了 CNI 网桥。如下图所示：

![](https://static001.geekbang.org/resource/image/7b/21/7b03e1604326b7cf355068754f47e821.png)

在这个例子中，Kubernetes 为 Flannel 分配的子网范围是 `10.244.0.0/16`。此参数可以在部署时指定，如：
```sh
$ kubeadm init --pod-network-cidr=10.244.0.0/16
```
或者在部署后通过修改 `kube-controller-manager` 的配置文件来设置。

假设 `Infra-container-1` 需要访问 `Infra-container-2`（即 Pod-1 访问 Pod-2），那么源 IP 地址为 `10.244.0.2`，目标 IP 地址为 `10.244.1.3`。此时，`Infra-container-1` 中的 `eth0` 设备通过 Veth Pair 连接到 Node 1 上的 `cni0` 网桥。因此，该 IP 包会经过 `cni0` 网桥出现在宿主机上。

Node 1 上的路由表如下：
```sh
$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
...
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
```

由于目标 IP 地址是 `10.244.1.3`，它匹配到第二条规则，即 `10.244.1.0` 对应的路由规则。这条规则指定了使用本机的 `flannel.1` 设备进行处理。`flannel.1` 处理后，将 IP 包转发到 "隧道" 另一端的 VTEP 设备，即 Node 2 的 `flannel.1` 设备。接下来的流程与之前介绍的 Flannel VXLAN 模式完全相同。

需要注意的是，CNI 网桥仅管理由 CNI 插件负责的、Kubernetes 创建的容器（Pod）。如果你单独启动一个 Docker 容器，Docker 仍会将其连接到 `docker0` 网桥，因此该容器的 IP 地址属于 `docker0` 网桥的 `172.17.0.0/16` 网段。

Kubernetes 设置这样一个与 `docker0` 网桥功能类似的 CNI 网桥的原因主要有两个方面：
1. Kubernetes 并未使用 Docker 的网络模型（CNM），因此不具备配置 `docker0` 网桥的能力。
2. 这还与 Kubernetes 如何配置 Pod，特别是 Infra 容器的 Network Namespace 密切相关。

Kubernetes 创建一个 Pod 的第一步是创建并启动一个 Infra 容器，用于持有该 Pod 的 Network Namespace。CNI 的设计思想是：Kubernetes 在启动 Infra 容器后，直接调用 CNI 网络插件，为 Infra 容器的 Network Namespace 配置预期的网络栈。

### CNI 插件的部署和实现

在部署 Kubernetes 时，需要安装 `kubernetes-cni` 包，以便在宿主机上安装 CNI 插件所需的基础可执行文件。安装完成后，可以在 `/opt/cni/bin` 目录下看到它们，如下所示：
```sh
$ ls -al /opt/cni/bin/
total 73088
-rwxr-xr-x 1 root root  3890407 Aug 17  2017 bridge
-rwxr-xr-x 1 root root  9921982 Aug 17  2017 dhcp
-rwxr-xr-x 1 root root  2814104 Aug 17  2017 flannel
-rwxr-xr-x 1 root root  2991965 Aug 17  2017 host-local
-rwxr-xr-x 1 root root  3475802 Aug 17  2017 ipvlan
-rwxr-xr-x 1 root root  3026388 Aug 17  2017 loopback
-rwxr-xr-x 1 root root  3520724 Aug 17  2017 macvlan
-rwxr-xr-x 1 root root  3470464 Aug 17  2017 portmap
-rwxr-xr-x 1 root root  3877986 Aug 17  2017 ptp
-rwxr-xr-x 1 root root  2605279 Aug 17  2017 sample
-rwxr-xr-x 1 root root  2808402 Aug 17  2017 tuning
-rwxr-xr-x 1 root root  3475750 Aug 17  2017 vlan
```

这些 CNI 基础可执行文件按功能分为三类：
1. **Main 插件**：用于创建具体网络设备的二进制文件，如 `bridge`（网桥设备）、`ipvlan`、`loopback`（lo 设备）、`macvlan`、`ptp`（Veth Pair 设备）和 `vlan`。
2. **IPAM 插件**：负责分配 IP 地址的二进制文件，如 `dhcp` 和 `host-local`。
3. **内置 CNI 插件**：由 CNI 社区维护，如 `flannel`、`tuning`、`portmap` 和 `bandwidth`。

要实现一个 Kubernetes 用的容器网络方案，需要做两部分工作，以 Flannel 项目为例：
1. **实现网络方案本身**：编写 `flanneld` 进程的主要逻辑，如创建和配置 `flannel.1` 设备、配置宿主机路由等。
2. **实现对应的 CNI 插件**：配置 Infra 容器内的网络栈并将其连接到 CNI 网桥上。

Flannel 项目的 CNI 插件已内置，无需单独安装。对于 Weave、Calico 等其他项目，则需在安装插件时将对应的 CNI 插件可执行文件放在 `/opt/cni/bin/` 目录下。

### CNI 配置文件

在宿主机上安装 `flanneld` 后，`flanneld` 会在每台宿主机上生成相应的 CNI 配置文件，告知 Kubernetes 该集群使用 Flannel 作为容器网络方案。CNI 配置文件内容如下：
```json
{
  "name": "cbr0",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}
```

在 Kubernetes 中，处理容器网络的逻辑不在 `kubelet` 主干代码中执行，而是在具体的 CRI 实现中完成。对于 Docker 项目，其 CRI 实现称为 `dockershim`。`dockershim` 会加载上述 CNI 配置文件。需要注意的是，Kubernetes 当前不支持多个 CNI 插件混用。如果在 CNI 配置目录（`/etc/cni/net.d`）中放置了多个 CNI 配置文件，`dockershim` 只会加载按字母顺序排序的第一个插件。但 CNI 允许在一个 CNI 配置文件中定义多个插件进行协作。

### CNI 插件的工作原理

当 `kubelet` 组件需要创建 Pod 时，首先创建的是 Infra 容器。`dockershim` 会调用 Docker API 创建并启动 Infra 容器，然后执行 `SetUpPod` 方法。该方法的作用是为 CNI 插件准备参数，并调用 CNI 插件为 Infra 容器配置网络。

这里调用的 CNI 插件是 `/opt/cni/bin/flannel`，所需的参数分为两部分：
1. **CNI 环境变量**：最重要的环境变量是 `CNI_COMMAND`，取值为 `ADD` 或 `DEL`。`ADD` 表示将容器添加到 CNI 网络中，`DEL` 表示从 CNI 网络中移除容器。
2. **默认插件的配置信息**：从 CNI 配置文件中加载，默认插件的配置信息称为 `NetworkConfiguration`，通过标准输入传递给 Flannel CNI 插件。

Flannel CNI 插件不会自己做事，而是调用 `delegate` 指定的 CNI 内置插件（如 `CNI bridge` 插件）。`dockershim` 对 Flannel CNI 插件的调用实际上是走过场，Flannel CNI 插件只需对传来的 `NetworkConfiguration` 进行补充，然后调用 `CNI bridge` 插件。

接下来，`CNI bridge` 插件会检查 CNI 网桥是否存在，如果不存在则创建它。然后，通过 Infra 容器的 Network Namespace 文件进入该 Network Namespace，创建一对 Veth Pair 设备，并将其中一端移动到宿主机上。

这样，容器就被成功加入到 CNI 网络中，实现了跨主机通信。