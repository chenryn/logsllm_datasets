F. Menet et al.
Training
Sample
Filtering
Layer
data-starving
Convolutional
Layers
Filter Reg.
sf
+
Fully
Connected
Softmax
Layer
Training Loss
Regularized Loss
Figure 5: Example structure of a Convolutional Spartan Net-
work for images, sf is the scaling factor, the weight of the
filtering regularization relatively to the training loss
On the backwards propagation, the function could be, for ex-
ample, replaced with the arctangent activation function, with the
following properties:
arctan’(x) =
1
1 + x2
(3)
Mixing definitions 2 and 3, we get the HSAT activation function:
HSAT(x) = H(x),
HSAT’(x) ←− arctan’(x) =
1
1 + x2
(4)
We use a mnemonic of a non-differentiable (or trivially differen-
tiable) function concatenated with a two-letter mnemonic of the
differentiable function whose derivative is used for gradient com-
puting.
HSAT will then be the Heaviside-Arctangent activation function,
HSID the Heaviside-Identity, Cos-HSAT for a Cosine-Heaviside-
Identity function. To simplify, we consider these activation func-
tions to be one activation function, that we will call composite
activation function.
We have investigated four such functions. In the following, H is
the Heaviside step function, and Φ is the normal distribution used
as a function.
Forward
H
H
H ◦ cos
H ◦ cos
Backward
id : f (x) = x
arctan
arctan◦ cos
Φ ◦ cos
Mnemonic
HSID
HSAT
Cos-HSAT
Cos-HSND
Feedforward Neural Networks are trained through backprop-
agation, and, while other weight updates exist, this method has
demonstrated its power over the past years. If we were to use
backpropagation as-is on this filtering layer however, we would
be unable to update its weights or biases, as the derivative of the
Heaviside step function is zero on all values where the function is
differentiable. This is due to the fact that the Heaviside step function
is the integral of the Dirac impulse function δ. Thus, no gradient
can be used to update the biases without the synthetic gradient.
Hence the decision to arbitrarily chose to replace the Heaviside
step function’s derivative by another function’s derivative on the
backwards pass.
We will test some replacement derivative function candidates
stated above in our Experiments section.
With the idea of a Forward-Backwards composite activation func-
tion, we decouple the forward propagation from the backwards
propagation dynamics on this layer, opening the path to use more
complex update functions.
Note that this separation idea has also been explored in a gradient
approximation attack context in the work of Athalye et al. [5], under
the name Backward Pass Differentiable Approximation or BPDA.
We differ from this work as the selection of the Backward pass
is arbitrary, and has no need to be a close approximation of the
function. We nonetheless keep the derivative sign to mirror the
general behaviour of the original function.
6.2 Candidate Filtering layers
Aiming to learn different thermometer encodings [8] through back-
propagation, the filtering layers focus on destroying irrelevant in-
formation. We propose and implement candidate layers exhibiting
this property.
We will take the number of dimensions of the encoding as an
hyperparameter β. β = 4, for example, will mean that there are four
thresholds, and thus five possible values for the encoding.
6.2.1 Convolution-Filtering. The simplest way filter information
with convolutions is to use the Heaviside step activation function
into standard convolutional layers with a 1 × 1 kernel.
For every filter, each channel of an input image will be multiplied
by a learned parameter and a bias will be added, before going
through the Heaviside activation function
This non-differentiable activation function can allow the network
to output thermometer encodings, as, with b the bias of the unit
and w a weight:
HS(wx + b) = 1 ⇔ wx + b > 0 ⇔ x > − b
w
(5)
Spartan Networks
arXiv Preprint, Dec 2018,
By having various values for − b
w we can create a thermometer
encoding created by an activation function and learned through
backpropagation, as was our goal.
6.2.2 Offset-Filtering. To prove that the robustness of the model is
not convolution-dependent, we can also implement the filtering us-
ing a locally connected layer. One can vary the amount of neurons
in the layer. The filtering layer is as large as the input. The neurons
in the layer have only one connection, and their weights are con-
strained to be one. Only their biases are learned during training.
We effectively create a composition between a binary filter and an
image mask learned by the system.
6.3 Candidate composite activation functions
6.3.1 HSID. The Heaviside-Identity activation function definition
is as follows:
HSAT(x) = H(x),
HSAT’(x) ←− id′
HSAT’(x) ←− 1
R(x)
(6)
This composite activation function is interesting because the
backward pass is not an approximation of the foward pass.
6.3.2 HSAT. The Heaviside-Arctangent activation function defini-
tion is as follows:
HSAT(x)
HSAT(cos(x))
HSND(cos(x))
x
x
x
HSAT(x) = H(x),
HSAT’(x) ←− arctan’(x)
HSAT’(x) ←−
1
1 + x2
6.3.3 HSAT ◦ Cosine. We compose the HSAT function with a co-
sine in order to get a learnable but square activation function. The
Heaviside-Arctangent◦Cosine activation function definition is as
follows:
(7)
(8)
HSAT(cos(x)) = H(cos(x)),
HSAT’(cos(x)) ←− arctan’(cos(x))
HSAT’(cos(x)) ←− − sin(x)
1 + cos(x)2
6.3.4 HSND ◦ Cosine. We modify the previous activation function
using the normal distribution as a function for the gradient part.
HSND(cos(x)) = H(cos(x)),
′(cos(x))
HSND’(cos(x)) ←− Φ
HSND’(cos(x)) ←− −x sin(x)Φ(x)
(9)
We used µ = 0, σ = 1 as the base values for the normal distribution.
While the normal distribution requires more computation, it is
mathematically more relevant, as the dirac δ impulse is the limit of
the normal distribution as σ ⇒ 0. We will test different values for
σ to see whether a synthetic gradient that fits the foward activation
function’s gradient more closely — that is, using a σ closer to 0 —
yields better results.
We can see on Fig. 6 that the derivative part of the composite
activation functions are "smoothed out" variants of their spiky,
non-differentiable counterparts.
Figure 6: Three candidate composite activation functions:
the Heaviside-Arctangent (y = HSAT(x)), the Cosine-
Heaviside-Arctangent function y = HSAT(cos(x)), and
the Cosine-Heaviside-NormalDistribution function y =
HSND(cos(x)). The blue plot is the forward activation func-
tion, the red plot the backwards derivative. The green plot
is the comparison between HSID (green) and HSND (red)
derivatives
6.4 Loss Regularizations of the candidates
The following section shows the regularizations of the various
candidate layers we experimented on.
As seen in Fig. 5, Spartan Networks have a scaled loss Regular-
ization that rewards the filtering layer when it data-starves the
network if sf > 0.
6.4.1 Convolution-Filtering. A simple L1 activation regularizer is
proposed. As the number of activation is reduced, the number of
bright, activated pixels diminishes, data-starving the network.
arXiv Preprint, Dec 2018,
F. Menet et al.
6.4.2 Offset-Filtering. The filtering layer adds a term to the loss
function to penalize the network if it gives away too much infor-
mation. We need a loss function that attains its maximum at a half
of the function value distrubtion and is 0 at the edges. The entropy
function is the best candidate, and we thus define the regularization
function as:
7 DISCUSSION & RESULTS
7.1 Implementation
We implement our idea in Keras [10], using parts of the TensorFlow
backend [2] for the implementation of the synthetic gradients. The
general structure of the Spartan Network we implemented is as
follow (first layer first):
N
Biloд(Bi)
i =1
bi − xmin
i
− xmin
xmax
i
i
L((Bi)i∈N) =
Bi =
(10)
Where N is the input size, (xmax
, xmin
imum value of the input for the ith input.
i
i
) the maximum and min-
Bi is the rescaled bias, ranging from 0 to 1. This rescaled bias
puts an a priori distribution over the dataset.
Note that this particular Bi holds when we hypothesize that
there is a uniform distribution on the values. In an opposite case,
one can create a cut-off based on a cumulative distribution that can
be used to rescale the biases. A Bi close to 1
2 will signal that the
probability of drawing a sample pixel value above the threshold is
the same as below.
6.5 Data-Starving Behaviours
6.5.1 Offset-Filtering. The filtering layer minimizes its regulariza-
tion loss if the value of the all rescaled biases of the neurons of the
filtering layer are closer to either 0 or 1. Rescaled biases close to
0 or 1 mean neurons change activation close the the minimum or
the maximum value. The filtering layer thus forces the network to
destroy as much information as it can, due to the behaviour of the
entropy function:
• When the rescaled bias is at 1
2, the ranges where HS(x − Bi)
are 0 or 1 are equal and the entropy function is at its maxi-
mum: the layer gives more information. We hence penalize
the fact that the network thrives on information.
• When the rescaled bias is close to 0, HS(x − Bi) is almost
always 1: we get no extra information as this feature is bound
to be present. Entropy regularization is close to 0
• When the rescaled bias is close to 1, HS(x − Bi) is almost
always 0. The feature will almost never be present. Entropy
regularization is close to 0.
6.5.2 Convolutional-Filtering. The more the filtering layer acti-
vates, the higher the penalty is because of the activation regulariza-
tion we put on this Filtering Layer. This means that the filters are
biased towards higher thresholds, as a higher threshold value will
decrease the number of inputs dimensions that cross the threshold,
and will thus reduce the activation regularization penalty.
We thus encourage the network to report on a feature only if
this feature is deemed extremely relevant, or if the value is extreme.
We thus restrict the attacker to a visible attack.
activation function.
stay on the first layer. (Filtering Layer 1)
• A Filtering Layer, whose parameters will vary but that will
• A Convolutional Layer, 32 filters, 3×3 kernel, no stride, ReLU
• Another Convolutional Layer, same parameters. ReLU or
Composite activation function. (Optional Filtering Layer 2)
• A 2 × 2 Pooling Layer
• A Convolutional Layer, 32 filters, 3×3 kernel, no stride, ReLU
• Another Convolutional Layer, same parameters. ReLU or
Composite activation function. (Optional Filtering Layer 3)
• A 2 × 2 Pooling Layer.
• A Densely Connected Layer with 50 Neurons and ReLU
activation function.
• A 10-classes Softmax.
The base CNN is close to the CNN used as standard ConvNet in
activation function.
[43].
7.2 Results
We tested a subset of all possible candidates created in the paper.
The candidates we have tested are as follow:
7.2.1 Offset-Filtering. We tested only one network with an offset
filtering on the filtering layer 1, using a HSAT activation function
(Candidate A).
7.2.2 Convolutional-Filtering. We tested the following Spartan Net-
works using the Convolutional Filtering:
• The standard CNN using a HSND on the layer 1, as well as
• The standard CNN using a Cos-HSAT layer on layer 1 (Can-
Cos-HSND layers on layers 2 and 3 (Candidate B)
didate C)
7.2.3 Robustness. We attack the candidate Spartan Networks with
a FGSM attack in surrogate black-box mode with various strengths.
We have used the CleverHans module version 2.1.0 [35] to perform
the attacks. We show our results on Fig. 7.
The filter-regularization scaling factor is sf = 10−5, for all of
the Spartan Networks, and µ = 0, σ = 1 if the Normal Distribution
is used. The number in parenthesis in the legend shows the test
precision (in %) on clean samples.
7.2.4 On the Training of a Network using Composite Activation
Functions. We report the variation of loss over training iteration of
the Candidate C compared to the standard CNN using only ReLU ac-
tivation functions with no filtering layers. Results are seen on Fig. 8.
Spartan Networks
arXiv Preprint, Dec 2018,
100
80
60
40
20
)
%
(
n
o
i
s
i
c
e