## Breaking Down Echo's Metrics

Let's break down the metrics we received from Echo's Prometheus middleware.

### Response Time Histograms

```shell
# HELP echo_request_duration_seconds The HTTP request latencies in seconds.
# TYPE echo_request_duration_seconds histogram
echo_request_duration_seconds_bucket{code="200",method="GET",url="/users",le="0.005"} 2
echo_request_duration_seconds_bucket{code="200",method="POST",url="/users",le="0.005"} 1
echo_request_duration_seconds_bucket{code="400",method="POST",url="/users",le="0.005"} 1
```

The response time histograms provide a distribution of HTTP request latencies in seconds for each combination of status code, method, and URL. These histograms use the default buckets defined by `DefBuckets`:

```go
// DefBuckets are the default Histogram buckets. The default buckets are
// tailored to broadly measure the response time (in seconds) of a network
// service. Most likely, however, you will be required to define buckets
// customized to your use case.
var DefBuckets = []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10}
```

In this case, all requests have latencies less than 5 milliseconds, which fall into the smallest latency bucket. This lack of precision can be problematic for fine-grained performance analysis. For more details on tuning bucket selection, refer to the [Tuning Bucket Selection](simple_service.md#assessing-performance-and-tuning-histogram-bucket-selection) section.

### Request and Response Size Histograms

```shell
# HELP echo_request_size_bytes The HTTP request sizes in bytes.
# TYPE echo_request_size_bytes histogram
echo_request_size_bytes_bucket{code="200",method="GET",url="/users",le="1024"} 2
echo_request_size_bytes_bucket{code="200",method="POST",url="/users",le="1024"} 1
echo_request_size_bytes_bucket{code="400",method="POST",url="/users",le="1024"} 1
...

# HELP echo_response_size_bytes The HTTP response sizes in bytes.
# TYPE echo_response_size_bytes histogram
echo_response_size_bytes_bucket{code="200",method="GET",url="/users",le="1024"} 2
echo_response_size_bytes_bucket{code="200",method="POST",url="/users",le="1024"} 1
echo_response_size_bytes_bucket{code="400",method="POST",url="/users",le="1024"} 1
```

These histograms track the sizes of HTTP requests and responses in bytes. The bucket ranges start from 1 KiB (1024 bytes) and go up to 10 MiB (10485760 bytes). Since our sample app has very small request and response sizes, all values fall into the smallest size buckets. This limits the precision of the metrics for small data sizes.

### Request Counters

```shell
# HELP echo_requests_total How many HTTP requests processed, partitioned by status code and HTTP method.
# TYPE echo_requests_total counter
echo_requests_total{code="200",host="localhost:8080",method="GET",url="/users"} 2
echo_requests_total{code="200",host="localhost:8080",method="POST",url="/users"} 1
echo_requests_total{code="400",host="localhost:8080",method="POST",url="/users"} 1
```

Echo also provides a counter metric for the total number of HTTP requests processed, partitioned by status code, method, and URL. This counter is useful for tracking the overall traffic to your API endpoints.

## Adding Custom Metrics to Echo's Prometheus Instrumentation

Echo's Prometheus instrumentation middleware supports additional custom metrics. You can define these metrics where you set up your API and pass them into the request's `echo.Context`. This allows you to observe new metric values in any of your handlers. For more details on adding custom metrics, see the [Echo documentation](https://echo.labstack.com/middleware/prometheus/#serving-custom-prometheus-metrics).

## Conclusion

Using a standard and framework-supported package for instrumenting your API routes can save significant development time. However, it is crucial to ensure that the default units of measurement and bucket spacings are suitable for your specific use case. In our example, the default settings were insufficient for precisely measuring both response times and request/response sizes.

This highlights an important lesson: while off-the-shelf instrumentation libraries can save initial setup time, it is essential to verify that the defaults meet your requirements. Otherwise, you may face challenges during incident triage and miss out on valuable insights into your service's performance.