frequently by malware samples than by benign apps are used
to perform a baseline classiﬁcation. Since there are legitimate
situations where a non-malicious app needs permissions tagged
as dangerous, DROIDAPIMINER also applies frequency anal-
ysis on the list of API calls, speciﬁcally, using the 169 most
frequent API calls in the malware samples (occurring at least
6% more in malware than benign samples) —leading to a
reported 83% precision. Finally, data ﬂow analysis is applied
on the API calls that are frequent in both benign and malicious
samples, but do not occur by at least, 6% more in the malware
set. Using the top 60 parameters, the 169 most frequent calls
change, and authors report a precision of 97.8%.
After obtaining DROIDAPIMINER’s source code, as well as
a list of packages used for feature reﬁnement, we re-implement
the system by modifying the code in order to reﬂect recent
changes in Androguard (used by DROIDAPIMINER for API
call extraction), extract the API calls for all apps in the datasets
listed in Table I, and perform a frequency analysis on the
calls. Androguard fails to extract calls for about 2% (1,017)
of apps in our datasets as a result of bad CRC-32 redundancy
checks and error in unpacking, thus DROIDAPIMINER is
evaluated over the samples in the second-to-last column of
Table I. We also implement classiﬁcation, which is missing
from the code provided by the authors, using k-NN (with k=3)
since it achieves the best results according to the paper. We use
2/3 of the dataset for training and 1/3 for testing as implemented
by the authors [2]. A summary of the resulting F-measures
obtained using different training and test sets is presented in
Table III.
We set up a number of experiments to thoroughly compare
DROIDAPIMINER to MAMADROID. First, we set up three
experiments in which we train DROIDAPIMINER using a
dataset composed of oldbenign combined with one of the
three oldest malware datasets each (drebin, 2013, and 2014),
and testing on all malware datasets. With this conﬁguration,
the best result (with 2014 and oldbenign as training sets)
amounts to 62% F-measure when tested on the same dataset.
Fig. 13: F-measure of MAMADROID classiﬁcation using newer
samples for training and older for testing (family mode).
Fig. 14: F-measure of MAMADROID classiﬁcation using newer
samples for training and older for testing (package mode).
newbenign.
False Positives. We analyze the manifest of the 164 apps
mistakenly detected as malware by MAMADROID, ﬁnding that
most of them use “dangerous” permissions [4]. In particular,
67% of the apps write to external storage, 32% read the phone
state, and 21% access the device’s ﬁne location. We further
analyzed apps (5%) that use the READ_SMS and SEND_SMS
permissions, i.e., even though they are not SMS-related apps,
they can read and send SMSs as part of the services they provide
to users. In particular, a “in case of emergency” app is able to
send messages to several contacts from its database (possibly
added by the user), which is a typical behavior of Android
malware in our dataset, ultimately leading MAMADROID to
ﬂag it as malicious.
False Negatives. We also check the 114 malware samples
missed by MAMADROID when operating in family mode,
using VirusTotal.11 We ﬁnd that 18% of the false negatives
are actually not classiﬁed as malware by any of the antivirus
engines used by VirusTotal, suggesting that these are actually
legitimate apps mistakenly included in the VirusShare dataset.
45% of MAMADROID’s false negatives are adware, typically,
repackaged apps in which the advertisement library has been
substituted with a third-party one, which creates a monetary
proﬁt for the developers. Since they are not performing any
clearly malicious activity, MAMADROID is unable to identify
11https://www.virustotal.com
9
01234Years0.00.20.40.60.81.0F-measureRF1-NN3-NN01234Years0.00.20.40.60.81.0F-measureRF1-NN3-NNdrebin & oldbenign
Our Work
drebin & newbenign
Our Work
Training Sets
drebin & oldbenign
2013 & oldbenign
2014 & oldbenign
Training Sets
2014 & newbenign
2015 & newbenign
2016 & newbenign
[2]
0.32
0.33
0.36
[2]
0.76
0.68
0.33
0.96
0.93
0.92
0.99
0.98
0.97
Our Work
2013 & oldbenign
[2]
0.35
0.36
0.39
0.96
0.97
0.93
2013 & newbenign
Our Work
[2]
0.75
0.68
0.35
0.99
0.98
0.97
Our Work
Testing Sets
2014 & oldbenign
[2]
0.34
0.35
0.62
0.79
0.74
0.95
2014 & newbenign
Our Work
[2]
0.92
0.69
0.36
0.99
0.99
0.99
Our Work
2015 & oldbenign
[2]
0.30
0.31
0.33
0.42
0.36
0.79
2015 & newbenign
Our Work
Our Work
2016 & oldbenign
[2]
0.33
0.33
0.37
0.43
0.29
0.78
2016 & newbenign
Our Work
[2]
0.67
0.77
0.34
0.89
0.95
0.93
[2]
0.65
0.65
0.36
0.83
0.90
0.92
TABLE III: Classiﬁcation performance of DROIDAPIMINER [2] vs MAMADROID (our work).
The F-measure drops to 33% and 39%, respectively, when
tested on samples one year into the future and past. If we use
the same conﬁgurations in MAMADROID, in package mode,
we obtain up to 97% F-measure (using 2013 and oldbenign
as training sets), dropping to 74% and 93%, respectively,
one year into the future and into the past. For the datasets
where DROIDAPIMINER achieves its best result (i.e., 2014
and oldbenign), MAMADROID achieves an F-measure of
95%, which drops to respectively, 79% and 93% one year into
the future and the past. The F-measure is stable even two years
into the future and the past at 78% and 92%, respectively.
As a second set of experiments, we train DROIDAPIMINER
using a dataset composed of newbenign combined with one
of the three most recent malware datasets each (2014, 2015,
and 2016). Again, we test DROIDAPIMINER on all malware
datasets. The best result is obtained with the dataset (2014 and
newbenign) used for both testing and training, yielding a F-
measure of 92%, which drops to 67% and 75% one year into the
future and past respectively. Likewise, we use the same datasets
for MAMADROID, with the best results achieved on the same
dataset as DROIDAPIMINER. In package mode, MAMADROID
achieves an F-measure of 99%, which is maintained more than
two years into the past, but drops to respectively, 89% and
83% one and two years into the future.
As summarized in Table III, MAMADROID achieves
signiﬁcantly higher performance than DROIDAPIMINER in
all but one experiment, with the F-measure being at least 79%
even after two years into the future or the past when datasets
from 2014 or later are used for training. Note that there is only
one setting in which DROIDAPIMINER performs slightly better
than MAMADROID: this occurs when the malicious training
set is much older than the malicious test set. Speciﬁcally,
MAMADROID presents low recall in this case: as discussed,
MAMADROID’s classiﬁcation performs much better when the
training set is not more than two years older than the test set.
F. Runtime Performance
We envision MAMADROID to be integrated in ofﬂine
detection systems, e.g., run by Google Play. Recall
that
MAMADROID consists of different phases, so in the following,
we review the computational overhead incurred by each of
them, aiming to assess the feasibility of real-world deployment.
We run our experiments on a desktop equipped with an 40-core
2.30GHz CPU and 128GB of RAM, but only use one core and
allocate 16GB of RAM for evaluation.
MAMADROID’s ﬁrst step involves extracting the call graph
from an apk and the complexity of this task varies signiﬁcantly
10
across apps. On average, it takes 9.2s±14 (min 0.02s, max
13m) to complete for samples in our malware sets. Benign
apps usually yield larger call graphs, and the average time to
extract them is 25.4s±63 (min 0.06s, max 18m) per app. Note
that we do not include in our evaluation apps for which we
could not successfully extract the call graph.
Next, we measure the time needed to extract call sequences
while abstracting to families or packages, depending on
MAMADROID’s mode of operation. In family mode, this phase
completes in about 1.3s on average (and at most 11.0s) with both
benign and malicious samples. Abstracting to packages takes
slightly longer, due to the use of 341 packages in MAMADROID.
On average, this extraction takes 1.67s±3.1 for malicious apps
and 1.73s±3.2 for benign samples. As it can be seen, the call
sequence extraction in package mode does not take signiﬁcantly
more than in family mode.
MAMADROID’s third step includes Markov chain modeling
and feature vector extraction. This phase is fast regardless of
the mode of operation and datasets used. Speciﬁcally, with
malicious samples, it takes on average 0.2s±0.3 and 2.5s±3.2
(and at most 2.4s and 22.1s), respectively, with families and
packages, whereas, with benign samples, averages rise to
0.6s±0.3 and 6.7s±3.8 (at most 1.7s and 18.4s).
Finally, the last step involves classiﬁcation, and performance
depends on both the machine learning algorithm employed and
the mode of operation. More speciﬁcally, running times are
affected by the number of features for the app to be classiﬁed,
and not by the initial dimension of the call graph, or by whether
the app is benign or malicious. Regardless, in family mode,
Random Forests, 1-NN, and 3-NN all take less than 0.01s. With
packages, it takes, respectively, 0.65s, 1.05s, and 0.007s per
app with 1-NN, 3-NN, Random Forests.
Overall, when operating in family mode, malware and
benign samples take on average, 10.7s and 27.3s respectively
to complete the entire process, from call graph extraction
to classiﬁcation. Whereas,
the average
completion times for malware and benign samples are 13.37s
and 33.83s respectively. In both modes of operation, time is
mostly (> 80%) spent on call graph extraction.
in package mode,
the
We
also
runtime
evaluate
performance
of
DROIDAPIMINER [2]. Its ﬁrst step,
i.e., extracting API
calls, takes 0.7s±1.5 (min 0.01s, max 28.4s) per app in our
malware datasets. Whereas, it takes on average 13.2s±22.2
(min 0.01s, max 222s) per benign app. In the second phase,
i.e., frequency and data ﬂow analysis, it takes, on average,
4.2s per app. Finally, classiﬁcation using 3-NN is very fast:
0.002s on average. Therefore, in total, DROIDAPIMINER takes
respectively, 17.4s and 4.9s for a complete execution on one
app from our benign and malware datasets, which while faster
than MAMADROID, achieves signiﬁcantly lower accuracy.
In conclusion, our experiments show that our prototype
implementation of MAMADROID is scalable enough to be
deployed. Assuming that, everyday, a number of apps in the
order of 10,000 are submitted to Google Play, and using the
average execution time of benign samples in family (27.3s)
and package (33.83s) modes, we estimate that it would take
less than an hour and a half to complete execution of all apps
submitted daily in both modes, with just 64 cores. Note that
we could not ﬁnd accurate statistics reporting the number of
apps submitted everyday, but only the total number of apps on
Google Play.12 On average, this number increases of a couple
of thousands per day, and although we do not know how many
apps are removed, we believe 10,000 apps submitted every day
is likely an upper bound.
V. DISCUSSION
We now discuss the implications of our results with respect
to the feasibility of modeling app behavior using static analysis
and Markov chains, discuss possible evasion techniques, and
highlight some limitations of our approach.
A. Lessons Learned
Our work yields important insights around the use of API
calls in malicious apps, showing that, by modeling the sequence
of API calls made by an app as a Markov chain, we can
successfully capture the behavioral model of that app. This
allows MAMADROID to obtain high accuracy overall, as well
as to retain it over the years, which is crucial due to the
continuous evolution of the Android ecosystem.