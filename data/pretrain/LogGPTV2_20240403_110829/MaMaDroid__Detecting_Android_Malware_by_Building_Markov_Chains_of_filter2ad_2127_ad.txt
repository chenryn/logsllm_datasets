### Baseline Classification and Frequency Analysis

API calls that are more frequently used by malware samples than by benign applications are utilized for baseline classification. Given that there are legitimate scenarios where non-malicious apps require permissions tagged as dangerous, DROIDAPIMINER also performs frequency analysis on the list of API calls. Specifically, it uses the 169 most frequent API calls in malware samples (occurring at least 6% more in malware than in benign samples), achieving a reported precision of 83%. Additionally, data flow analysis is applied to API calls that are common in both benign and malicious samples but do not occur at least 6% more in the malware set. Using the top 60 parameters, the 169 most frequent calls are adjusted, and the authors report a precision of 97.8%.

### Re-implementation and Evaluation

After acquiring the source code of DROIDAPIMINER and a list of packages used for feature refinement, we re-implemented the system. We modified the code to reflect recent changes in Androguard, which is used by DROIDAPIMINER for API call extraction. We then extracted the API calls for all apps in the datasets listed in Table I and performed frequency analysis on these calls. Androguard failed to extract calls for approximately 2% (1,017) of the apps in our datasets due to bad CRC-32 redundancy checks and unpacking errors. Therefore, DROIDAPIMINER was evaluated over the samples in the second-to-last column of Table I.

We also implemented the classification step, which was missing from the provided code, using k-NN (with k=3) as it achieved the best results according to the paper. We used 2/3 of the dataset for training and 1/3 for testing, as implemented by the authors [2]. A summary of the resulting F-measures obtained using different training and test sets is presented in Table III.

### Comparative Experiments with MAMADROID

To thoroughly compare DROIDAPIMINER with MAMADROID, we set up several experiments. First, we trained DROIDAPIMINER using a dataset composed of oldbenign combined with one of the three oldest malware datasets (drebin, 2013, and 2014), and tested on all malware datasets. The best result (using 2014 and oldbenign as training sets) achieved an F-measure of 62% when tested on the same dataset. When tested on samples one year into the future and past, the F-measure dropped to 33% and 39%, respectively.

In a second set of experiments, we trained DROIDAPIMINER using a dataset composed of newbenign combined with one of the three most recent malware datasets (2014, 2015, and 2016). The best result was obtained with the dataset (2014 and newbenign) used for both testing and training, yielding an F-measure of 92%, which dropped to 67% and 75% one year into the future and past, respectively.

As summarized in Table III, MAMADROID generally achieves significantly higher performance than DROIDAPIMINER, with the F-measure being at least 79% even two years into the future or past when datasets from 2014 or later are used for training. The only setting in which DROIDAPIMINER performs slightly better is when the malicious training set is much older than the malicious test set, leading to lower recall for MAMADROID.

### False Positives and Negatives

**False Positives:**
We analyzed the manifest of the 164 apps mistakenly detected as malware by MAMADROID. Most of these apps use "dangerous" permissions. Specifically, 67% write to external storage, 32% read the phone state, and 21% access the device's fine location. Further analysis revealed that 5% of the apps use the READ_SMS and SEND_SMS permissions, even though they are not SMS-related. For example, an "in case of emergency" app can send messages to contacts from its database, a behavior typical of Android malware in our dataset, leading MAMADROID to flag it as malicious.

**False Negatives:**
We also checked the 114 malware samples missed by MAMADROID in family mode using VirusTotal. We found that 18% of the false negatives were not classified as malware by any of the antivirus engines used by VirusTotal, suggesting they might be legitimate apps mistakenly included in the VirusShare dataset. Additionally, 45% of MAMADROID’s false negatives are adware, typically repackaged apps with third-party advertisement libraries, which create monetary profit for developers without performing clearly malicious activities.

### Runtime Performance

We envision MAMADROID to be integrated into offline detection systems, such as Google Play. MAMADROID consists of several phases, and we reviewed the computational overhead incurred by each phase to assess the feasibility of real-world deployment. Our experiments were run on a desktop equipped with a 40-core 2.30GHz CPU and 128GB of RAM, using only one core and allocating 16GB of RAM for evaluation.

**Call Graph Extraction:**
The first step involves extracting the call graph from an APK, which varies significantly across apps. On average, it takes 9.2s ± 14 (min 0.02s, max 13m) for malware samples and 25.4s ± 63 (min 0.06s, max 18m) for benign apps. Apps for which we could not successfully extract the call graph were excluded from the evaluation.

**Call Sequence Extraction:**
In family mode, this phase completes in about 1.3s on average (max 11.0s) for both benign and malicious samples. In package mode, it takes slightly longer, averaging 1.67s ± 3.1 for malicious apps and 1.73s ± 3.2 for benign samples.

**Markov Chain Modeling and Feature Vector Extraction:**
This phase is fast regardless of the mode of operation and datasets used. For malicious samples, it takes on average 0.2s ± 0.3 and 2.5s ± 3.2 (max 2.4s and 22.1s) in family and package modes, respectively. For benign samples, the averages rise to 0.6s ± 0.3 and 6.7s ± 3.8 (max 1.7s and 18.4s).

**Classification:**
The final step involves classification, and performance depends on the machine learning algorithm and mode of operation. In family mode, Random Forests, 1-NN, and 3-NN all take less than 0.01s. In package mode, it takes 0.65s, 1.05s, and 0.007s per app with 1-NN, 3-NN, and Random Forests, respectively.

**Overall Completion Times:**
In family mode, malware and benign samples take on average 10.7s and 27.3s, respectively, to complete the entire process. In package mode, the average completion times are 13.37s and 33.83s, respectively. In both modes, more than 80% of the time is spent on call graph extraction.

**DROIDAPIMINER Runtime:**
For DROIDAPIMINER, the first step (extracting API calls) takes 0.7s ± 1.5 (min 0.01s, max 28.4s) per app in our malware datasets and 13.2s ± 22.2 (min 0.01s, max 222s) per benign app. The second phase (frequency and data flow analysis) takes, on average, 4.2s per app. Classification using 3-NN is very fast, taking 0.002s on average. Therefore, DROIDAPIMINER takes 17.4s and 4.9s for a complete execution on one app from our benign and malware datasets, respectively, which is faster than MAMADROID but with significantly lower accuracy.

### Scalability and Deployment

Our experiments show that our prototype implementation of MAMADROID is scalable enough for deployment. Assuming that 10,000 apps are submitted to Google Play daily, and using the average execution time of benign samples in family (27.3s) and package (33.83s) modes, it would take less than an hour and a half to complete the execution of all apps submitted daily in both modes, with just 64 cores.

### Discussion

#### Lessons Learned

Our work provides important insights into the use of API calls in malicious apps. By modeling the sequence of API calls as a Markov chain, MAMADROID can successfully capture the behavioral model of an app, achieving high accuracy and maintaining it over the years, which is crucial due to the continuous evolution of the Android ecosystem.