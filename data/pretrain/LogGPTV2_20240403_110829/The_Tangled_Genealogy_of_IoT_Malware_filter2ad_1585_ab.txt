1
1
1
41,788
Total
32,917
24,531
10,558
8,193
6,307
5,848
26
9
0
1
1
1
88,392
obtained on Windows malware), if we remove Mirai and Gafgyt, a
common label was not found for one third of the remaining samples.
3 MALWARE LINEAGE GRAPH EXTRACTION
The field that studies the evolution of malware families and the way
malware authors reuse code between families as well as between
variants of the same family is known as malware lineage. Deriving
an accurate lineage is a difficult task, which in previous studies
has often been performed with help from manual analysis and
over a small limited number of samples [24, 31]. However, given
the scale of our dataset, we need to rely on a fully automated
solution. The traditional approach for this purpose is to perform
malware clustering based on static and dynamic features [15, 25,
28, 31]. When also the time dimension is combined in the analysis,
clustering can help derive a complete timeline of malware evolution,
also known as phylogenetic tree of the malware families.
A common and simple other way to do that would be to rely on
AV labels, more oriented to only identify macro-families.
We, on the other hand, work towards a finer-grained classifica-
tion that would enable us to study differences among sub-families
and the overall intra-family evolution and relationships.
In our first attempt we decided to cluster samples based on a
broad set of both static and dynamic features. This approach not
only required a substantial amount of manual adjustments and
validation, it also always resulted in noisy clusters. As feature-based
clustering is often used in malware studies, we believe there is a
value in reporting the reasons behind its failure. We thus provide a
detailed analysis in Appendix A with the complete list of extracted
features in Appendix B.
Figure 1: Number of samples in our dataset submitted to
VirusTotal over time.
Table 2: Breakdown of the top 10 IoT malware families in
our dataset.
Rank
Label
(AVClass)
Samples No. (%)
1
2
3
4
5
6
7
8
9
10
Gafgyt
Mirai
Tsunami
Dnsamp
Hajime
Ddostf
Unlabelled
Lightaidra
Pnscan
Skeeyah
VPNFilter
Total
46,844 (50.02)
33,480 (35.75)
3,364 (3.97)
2,235 (3.59)
1,685 (2.39)
840 (0.90)
360 (0.38)
212 (0.23)
178 (0.19)
135 (0.14)
89,935 (96.03)
3,717 (3.97)
In addition to downloading the binaries, we also retrieved the
VirusTotal reports. We then processed them with AVClass [41], a
state-of-the-art technique that relies on the consensus among the
AV vendors to determine the most likely family name attributed
to malware samples. Table 2 lists the top ten AVClass labels, with
Gafgyt and Mirai largely dominating the dataset. However, there is
a long tail of families (90 in total) that contain only a small number
of samples. Finally, it is interesting to note that AVClass was unable
to find a consensus for a common family name for only 3.7K samples.
While this might seem very small (especially compared with figures
3.1 Code-based Clustering
We decided to resort to a more complex and time consuming so-
lution based on code-level analysis and function similarity. The
advantage is that code does not lie, and therefore can be used to
precisely track both the evolution over time of a given family as
well as the code reuse and functionalities borrowed among different
families.
The main drawback of clustering based on code similarity is that
the distance among two binaries is difficult to compute. Binary code
similarity is still a very active research area [21], but tools that can
scale to our dataset size are scarce and often in a prototype form.
Jan2015Jan2016Jan2017Jan2018Date101103105Number of samplesACSAC 2020, December 7–11, 2020, Austin, USA
Cozzi, et al.
Moreover, to be able to compare binaries, three important condi-
tions must be satisfied: 1) each sample needs to be first properly
unpacked, 2) it must be possible to correctly disassemble its code,
and 3) it must be possible to separate the code of the application
from the code of its libraries. The first two constitute major prob-
lems that had hindered similar experiments on Windows malware.
However, IoT malware samples are still largely un-obfuscated and
packers are the exception instead of the norm [12]. While this is a
promising start, the third condition turns out to be a difficult issue
(ironically this is the only one not causing problems for traditional
Windows malware).
Figure 2 shows the workflow of our code-based clustering. The
process is divided in three macro phases. A First we process un-
stripped binaries and we analyze the symbols to locate library code
in statically linked files. B Then we perform an incremental cluster-
ing based on the code-level similarity, while propagating symbols
to each new sample. C Finally, we build the family graphs (one
for each CPU architecture) and D we use available symbols to pin
samples and clusters to code snippets we were able to scrape from
online code repositories to obtain more detailed understanding
about the evolution of malware families.
Recall that our goal is not to provide a future-proof IoT malware
analysis technique. We rather seek to identify a scalable approach
that enables us to reconstruct the lineage for the 93K samples in our
3.5 year-long dataset so we can report on their genealogy. We thus
take advantage of the current sophistication of IoT malware, which
is currently rudimentary enough to enable code-based analysis,
aware that malware authors could easily employ tricks to hinder
such analysis in the future.
3.2 Symbols Extraction
IoT malware is often shipped statically linked. The fact that 88,392
samples out of 93,652 (94.3%) in our dataset are statically linked
tend to confirm this assumption. This is most likely due to an effort
to ensure the samples can run on devices with various system
configurations. However, performing code similarity on statically
linked binaries is useless, as two samples would be erroneously
considered very similar simply because they might include the same
libc library. Therefore, to be able to identify the relevant functions
in such binaries, we first need to distinguish the user-defined code
from the library code embedded in them. Unfortunately, when
dealing with stripped binaries, this is still an open problem and the
techniques proposed to date have large margins of errors, which
are not suitable for our large-scale, unsupervised experiments.
We thus start our analysis by extracting symbols from unstripped
binaries and leveraging them to add semantics to the disassembled
code. Luckily, as depicted in Table 1, 53% of statically-linked and
30% of dynamically linked samples contain symbol information. We
used IDA Pro to recognize functions and extract their names. We
then use a simple heuristic to cut the binary in two. The idea is to
locate some library code, and then simply consider everything that
comes after library code as well. While it is possible for the linker
to interleave application and library objects in the final executable,
this would simply result in discarding part of the malware code
from our analysis. However, this is not a common behavior, and
lacking any better solution to handle this problem, this is a small
price to pay to be able to compute binary similarity on our dataset.
We therefore built a database of symbols (symbols DB in Figure 2)
extracted from different versions of Glibc and uClibc and use the
database to find a “cut” that separates user from library code. After
extracting the function symbols from unstripped ELF samples, we
start scanning them linearly with respect to their offsets. We move
a sliding window starting from the entry point function _start and
define a cutting point as soon as all of the function names within
that window have a positive match in the symbols DB. Using a
window instead of a single function match avoids erroneous cases
where a user function name may be wrongly interpreted as a library
function. We experimentally set this window size to 2 and verified
the reliability of this heuristic by manually analyzing 100 cases.
Once the cutting point is identified, all symbols before this point
are kept and the remaining ones are discarded.
We chose to operate only on libc variants for two reasons. First,
because libc is always included by default by compilers into the
final executable when producing statically linked files. Moreover,
we observed that less than 2% of the dynamically linked samples in
our dataset require other libraries on top of libc.
Finally, after removing the library code, we further filter out
other special symbols, including __start, _start and architecture-
dependent helpers like the __aeabi_* functions for ARM proces-
sors.
3.3 Binary Diffing and Symbol Propagation
Binary diffing constitutes the core of our approach as it enables us
to assess the similarity between binaries at the code level. However,
given the intrinsic differences at the (assembly) code level between
binaries of different architectures, we decided to diff together only
binaries compiled for the same architecture – therefore producing
a different clustering graph, and a different malware lineage, for
each architecture. While this choice largely reduces the number of
possible comparisons, our datasets still contains up to 36,574 files
per architecture (ARM 32-bit), making the computation of a full
similarity matrix unfeasible.
To mitigate this problem we adopt Hierarchical Navigable Small
World graphs (HNSW) [32], an efficient data structure for approxi-
mate nearest neighbor discovery in non-metric spaces, to overcome
the time complexity and discover similarities in our dataset. The
core idea that accelerates this and similar approaches [14, 17] is
that items only get compared to neighbors of previously-discovered
neighbors, drastically limiting the number of comparisons while
still maintaining high accuracy. While adding files to the HNSW,
our distance function will be called on a limited number of file
pairs (on average, adding an element to the HNSW requires only
244 comparisons in our case) while still being able to link it to its
most similar neighbors. We configured the HNSW algorithm to
take advantage of parallelism and provide high-quality results as
suggested by existing guidelines in the clustering literature [13].
We use Diaphora [1] to define our dissimilarity function for
HNSW. This function is non-metric as the triangle inequality rule
does not necessarily hold. However, in the following we will call it
distance function without implying it is a proper metric. This has
not consequences on the precision of our clustering, as the HNSW
The Tangled Genealogy of IoT Malware
ACSAC 2020, December 7–11, 2020, Austin, USA
Figure 2: The workflow of our system.
algorithm is explicitly designed for non-metric spaces. One of the
advantages of using Diaphora is that the tool works with all the
architectures supported by IDA Pro, which covers 11 processors
and 99.9% of the samples in our dataset, while other binary code
similarity solutions recently proposed in academia handle only few
architectures and do not provide publicly available implementa-
tions [21]. When two binaries are compared, Diaphora outputs a
per-function similarity score ranging from 0 (no match) to 1 (perfect
match). To aggregate individual function scores in a single distance
function we experimented with different solutions based on the
average, maximum, normalized average, and sum of the scores. We
finally decided to count the number of functions with similarity
greater than 0.5, which is the threshold suggested by Diaphora’s
authors to discard unreliable results. This has the advantage of
providing a symmetric score (e.g., if the similarity of A to B is 4
then the similarity of B to A is also 4) that constantly increase as
more and more matching functions are found among two binaries.
For HNSW we then report the inverse of this count to translate the
value into a distance (where higher values mean two samples are
further apart and lower values mean they share more code).
Before running HNSW to perform pairwise comparison on the
whole dataset, we unpacked 6,752 packed samples. Since they were
all based on variations of UPX, we were able to easily do that by
using a simple generic unpacker. We then add each sample to HNSW
one by one, in two rounds, sorted by their first seen timestamp on
VirusTotal (to simulate the way an analyst would proceed when
collecting new samples over time).
In the first round we added all dynamically linked or unstripped
samples, which account for 55% of the entire dataset. By relying
on the symbols extracted in the previous phase, we only perform
the binary diffing on the user-defined portion of the code, and
omit comparisons on library code. In the second round we then
added the statically linked stripped samples. Being without symbols,
there is no direct way to distinguish user functions from library
code. Attempts to recover debugging information from stripped
binaries, such as with Debin [22], only target a limited set of CPU
architectures.
We tackle this problem by leveraging the binary diffing itself
to iteratively “propagate” symbols. When a function in a stripped
sample has perfect similarity with an unstripped one, we label it
with the same symbol. This methodology enables us to perform
similarity analysis also for stripped samples, which would other-
wise be discarded. However, this step comes with some limitations.
While we are able to discard library functions we also potentially
discard user functions that didn’t match any function already in the
graph. For instance, if two stripped statically linked samples share a
function that is never observed in unstripped or dynamically linked
binaries, this similarity would not be detected by our solution. We
add the stripped samples to HNSW only after the unstripped ones
have all been added to contain this problem as much as possible,
but the probabilistic nature of HNSW can decrease this benefit as
not all comparisons are computed for each sample. This means that
our graph is an under-approximation of the perfect similarity graph
(we can miss some edges that would link together different sam-
ples, but not create false connections) with over 18.7M one-to-one
binary comparisons and 595,039 function symbols propagated from
unstripped to stripped binaries.
3.4 Source Code Collection
The symbols extracted from unstripped malware and propagated
in the similarity phase also helps us locate and collect snippets
of source codes from online sources. In fact, the source code for
many Linux-based IoT malware families has been leaked on open
repositories hosting malicious packages ready to be compiled and
deployed. This has resulted in a very active community of develop-
ers that cloned, reused, adapted, and often re-shared variations of
existing code.
We took advantage of this to recognize open source and closed
source families, split our dataset accordingly and, more importantly,
to assign labels to groups of nodes in the similarity graph. While we
also use AV labels for this purpose, those labels often correspond to
generic family names, while online sources can help disambiguate
specific variants within the same bigger family.
To locate examples of source code, we queried search engines
with the list of user-defined function symbols extracted in the
previous phase. We were able to find several matches on public
services as GitHub or Pastebin, both for entire code bases (e.g.,
Symbol extractionUnstrippedStrippedxyzdefabcELF1000010110ELFCutabcELFFilterSymbols DBABinary diﬃng and symbol propagationHNSW-based binarydiﬃng (diaphora)Similarity graphSource code collectionDBWeb scrapingSourcecode DBSymbol propagationCFunction-levelsimilarity DB10000101abcELFStrippedabcELFUnstrippedACSAC 2020, December 7–11, 2020, Austin, USA
Cozzi, et al.
on GitHub) and for single source files (e.g., on Pastebin). Interest-
ingly, on GitHub we found tens of repositories forked thousands
of times (not necessarily for malicious purposes, as often security
researchers also forked those repositories). Moreover, we found
a Russian website hosting a repository regularly populated with
several malware projects, exploits, and cross-compilation resources.
From this source alone we were able to retrieve the code of 76 vari-
ants of Gafgyt, 50 variants of Mirai, 19 projects generically referred
as “CnC Botnet” and “IRC Sources” (which resemble Tsunami vari-
ants) and a number of exploits for widely deployed router brands.
Some variants contained changelog information that made us be-
lieve these projects had been collected from leaks and underground
forums.
3.5 Phylogenetic Tree of IoT Malware
As a preamble to the function level similarity analysis of IoT mal-
ware we post-processed the sparse similarity graph G obtained by
running HNSW and using the distance function as weight. Since
we store in a database the detailed comparisons, the actual weight
on the similarity graph can be tuned depending on the purpose of
the analysis.