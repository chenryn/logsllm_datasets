for i = 1 to 2 do
t1 ← Peek(q1);
t2 ← Peek(q2);
if HashLocation(t1) > HashLocation(t2) then
else
t ← dequeue(q1)
t ← dequeue(q2)
end if
writeNextBlockTo(B, encryptWithNewNonce(t));
end for
end for
Return B
Figure 3. Phase 2: Oblivious Merge Sort
This phase requires time linear to the size of the level
being read. For level i, which contains 4i buckets of size
log n, the running time is O(4i log n).
Since the location of real blocks is determined by a se-
cure hash function on the unique block index, the distribu-
tion of the blocks is indistinguishable from a uniform ran-
dom distribution. Hence the fake blocks, as well, will be
spread uniformly randomly across the entire level. With
high probability, any sample of buckets will thus have a ra-
tio of fake blocks to real blocks very close to the overall av-
erage. Before we formulate this, we introduce the following
lemma:
Lemma 1. A one-dimensional random walk consisting of
n steps of size 1, either forward or backward with equal
probability, will remain bounded by ±c√n with high prob-
ability.
Proof. (sketch) Let
the likelihood of a such a one-
dimensional random 50-50 walk being at location d at step
j be deﬁned as Pj (d). Then the likelihood that the walk is at
lihood that any step j along the way hits either of these out-
position c√n or −c√n at step j is 2Pj(c√n) and the like-
j←1 Pj(c√n). [11] uses
of-bounds markers becomes: 2Pn
√2πj e−d2/2j
Stirling’s formula to approximate Pj(d) ≈ 2
For 1 ≤ j < n, Pj(c√n) < Pn(c√n). Therefore,
Xj←1
Pj(c√n) < 2nPn(c√n) ≈ 2n
e−(c√n)2/2n
2
2
√2πn
n
= 4r n
2π
e−c2/2
For any ﬁxed maximum walk length nmax, the chance of
reaching ±c√n in a 50-50 random walk is negligible with
the security parameter c.
Theorem 1. With high probability, the Remove Fakes queue
never overﬂows or empties early.
Proof. (sketch): Let r = O(log n) be the ratio of fake
blocks to real blocks in the bottom level. Consider a random
walk of length nr. With probability 1/r, we step forward
(1 − 1/r). With probability 1 − 1/r, we step backwards
1/r. Then the main idea behind this proof is to reduce the
problem to showing that such a random walk will remain
within c√n of the starting location with high probability. It
can then be shown that this in turn can be reduced to the
probability of a standard 50-50 +1/-1 walk length n leaving
these bounds.
To summarize, Phase 1 copies all of the real blocks out
of level i, into a new remote (server-side) storage buffer that
only contains real blocks. In copying, a small local (client-
side) buffer is used to avoid leaking which blocks were fake.
This is possible since the fake blocks are evenly distributed
throughout the level.
4.5 Phase 2: Oblivious Merge Sort
We now describe an algorithm that performs a merge sort
on a array of size n, with c√n local storage, in O(n log n)
time, without revealing any correlation between the old and
new permutations. The algorithm runs recursively on the
remote array as described in Figure 3. The recursion depth
is log n, and each level of recursion entails a single pass of
size O(n) across the entire array.
The correctness of this algorithm depends on the unifor-
mity of the starting permutation of the items being sorted,
as illustrated in Theorem 3. Its oblivious nature derives im-
mediately by construction:
Theorem 2. The Oblivious Sort algorithm is private: no
more than a negligible amount of information about the new
permutation is leaked to a computationally bounded adver-
sary.
Proof. (sketch): The ordering of reads and writes in ev-
ery instantiation of the scramble is identical: observe that
in the algorithm deﬁned in Figure 3, the readNextBlock-
From() and writeNextBlockTo() functions are called in the
same pattern every time, depending only on n, not the com-
parisons made on the HashLocation()s.
The semantic security properties of the symmetric en-
cryption scheme guarantee that the adversary cannot cor-
relate any two blocks based on the encrypted content (the
server cannot determine whether t is from q1 or q2). There-
fore, every instantiation of the scramble appears identical to
the server: it sees a ﬁxed pattern of reads interspersed with
a ﬁxed pattern of writes of unintelligible data. The speciﬁc
ﬁxed pattern is known beforehand to the server (from the
algorithm deﬁnition), and the content of the reads has no
correlation to the content of the writes since the blocks are
re-encrypted with a semantically secure encryption scheme
at the client.
Therefore, in observing any iteration (or sequence of it-
erations) of the oblivious merge sort, the (computationally
bounded) adversary learns nothing.
Moreover, the ﬁnal permutation is chosen from among
all possible permutations. Since the access pattern is identi-
cal when generating each of these permutations, the server
has no ability to guess the resulting permutation.
A small number of permutations will cause the algorithm
to fail and output ⊥, if the queues overﬂow, but this absence
of a failure reveals only a negligible amount of information
about the new permutation, since failure occurs with very
low probability, as shown next.
Theorem 3. Oblivious Merge Sort queues never overﬂow
or empty early, with high probability.
Proof. (sketch): The queue size at step j is a probabilistic
function Qj deﬁned iteratively:
Q0 = n/2
Qj = Qj−1 + 1 Pr.
Qj = Qj−1 − 1 Pr.
1/2 − (Qj−1 − n/2)/(n − j)
1/2 + (Qj−1 − n/2)/(n − j)
This is analogous to pulling two colors of balls out of a bag
without replacement, starting with n/2 of each color in the
bag. The further we deviate from an equivalent number of
each color, the more likely it is for the next ball to bring the
tally closer to equivalent counts. This negative dependency
implies that any step away from the balance will occur with
probability asymptotically lower than 1/2. This can be then
reduced to showing that a 50-50 random walk will remain
within ±c√n with high probability and Lemma 1 etc.
In summary, the Oblivious Sort algorithm sorts all the
data blocks on the server into their ﬁnal permutation, with-
out revealing anything that could allow the server to corre-
late the two permutations.
4.6 Phase 3: Add Fakes
In the ﬁnal phase, the permuted blocks are added back
to server-hosted buckets where they will be located by the
next iteration of the secure hash function (see Section 3.1).
At the same time, fake blocks are added to make all buck-
ets mutually indistinguishable. This is the exact inverse of
Phase 1.
For correctness we must also show here that the buckets
of size log n will not overﬂow. A simple balls and bins re-
sult shows that if 4i balls are randomly thrown into 4i bins,
with probability greater than n−1
n , the fullest bin has fewer
than 3 log 4i
log log 4i balls when n is large enough [20]. This is
small but non-negligible probability. If this case ever oc-
curs, the authors of [17] prescribe a level re-order abort and
restart. With high probability, the time complexity of this
algorithm is not affected. See [17] for a more complete
analysis of this issue. We note that while this restriction re-
veals to the server that the hash function ﬁnally chosen does
not overﬂow any bucket, it does not reveal any correlation
between previous and current block locations.
As in Phase 1, we employ a local buffer of size c√n
to prevent the server from learning where fakes are being
added. The client scans the array of real blocks stored in the
remote server by Phase 2 into a local queue. Once the local
queue is half full, it begins constructing server-side buck-
ets with the blocks from the queue, writing into one bucket
for every block read. As long as the temporary queue does
not overﬂow or become empty, the exact pattern of reads
and writes observed by the server is dependent only on the
number of blocks. Therefore, the server learns nothing of
which are the fake blocks by observing this process (see
Figure 2 (b)). Moreover, it does not reveal the number of
blocks in each bucket, since the buckets are written sequen-
tially to the server in full, so the read and write pattern for
this step is identical on every repetition.
The algorithm runs in time linear to the size of the level
being written. For level i, which contains 4i buckets of size
log n, the running time is O(log n4i).
Theorem 4. With high probability, the Add Fakes algorithm
queue never overﬂows or empties early.
Proof. (sketch): The queue length Qj at step j is modeled
by Qj+1 = Qj + 1 − bucketSize(j). The bucket sizes, de-
termined by a fair balls and bins experiment placing n balls
into n bins, are distributed according to a Poisson distribu-
tion parameter 1 [20]. Taking the sum of j Poisson random
variables yields a Poisson random variable [20]. The walk
distance after j steps can therefore be modeled as j minus a
Poisson random variable X with mean j etc.
Theorem 5. Correctness. After Phase 3, all blocks will be
in the correct bucket (determined by the secure hash func-
tion).
Proof. (sketch): This proof follows from the construction
of Phases 2 and 3. Phase 3 correctly builds the buckets
for level i when its input array satisﬁes the follow prop-
erties: (1) all data blocks corresponding to i are in the ar-
ray. (2) For all data blocks b, b′, if the bucket corresponding
to data block b precedes the bucket corresponding to data
block b′, then b is listed in the array before b′. After the
sort in Phase 2, all blocks are in sorted order, according to
their bucket, therefore meeting the two requirements for the
input to Phase 3.
Theorem 6. Privacy. The contents of the level make it from
the old permutation to the new permutation without reveal-
ing any non-negligible information about either permuta-
tion. The location of the fake blocks is not revealed.
Proof. (sketch): Theorem 2 shows that the level permuta-
tion performed in Phase 2 does not reveal any correlation
between the old locations and the new locations. Further-
more, the read and write pattern of Phase 3 is independent
of the data items and the ﬁnal permutation, so Phase 3 does
not reveal anything about the location of the fake blocks, or
the permutation.
5 Performance
RAM
processor
disk seek time
sustained disk read/write
Link bandwidth
Link round trip time
En/Decryption
Outsourced data set size
Client
1GB
2Ghz
Server
4GB
5ms
50 MB/s
IBM 4764
32MB
266Mhz
80MB/sa
0.1ms
10 MB/s
50ms
100MB/sb
10MB/s
1 TB, in 1000-byte blocks; n = 109
aThe 4764 sits on an 8GB/s PCI-X bus; the bottleneck is the DMA rate.
bBased on processor speed, using AES [19].
Figure 4. Conﬁguration used to compute sample values
in the following tables and graphs.
In evaluating the feasibility and performance of the ar-
chitecture we consider the sample conﬁguration illustrated
in Figure 4. Further, Figure 7 illustrates multiple such data
points.
Online Cost.
The query requires online scans of one
bucket at each level, plus a write to the top level. The scan of
the log4 n levels are interactive; the bucket scanned at each
level depends on the results of the previous level. Figure 5
displays the expected online cost per query.
It is clear from these estimates that in a sequential access
model, the network latency is responsible for most of the
query latency. This is due to the interactive nature of the
scans; the client cannot determine the next bucket to scan
until it has seen the contents of the previous.
Network latency
Disk seek
Network transfer
Client en/decryption
Server disk transfer time
Formula
RT Tlink ∗ log4 n
Latencyseek ∗ log4 n ∗ 2
log4 n ∗ log n ∗ 2 ∗ blksz/T hroughputlink
log4 n ∗ log n ∗ 2 ∗ blksz/T hroughputcrypto
log4 n ∗ log n ∗ 2 ∗ blksz/T hroughputdisk
Sample
750ms
150ms
60ms
6ms
12ms
Figure 5. Online cost per query, resulting from scanning
a bucket at each level.
Ofﬂine Reorder Cost. The ofﬂine cost resulting from the
reordering of level i (performed once every 4i−1 accesses)
consists of three phases including a sequential level scan of
size log n ∗ 4i, and a sequential write-back of size 4i to re-
move fakes (Phase 1). The oblivious sort (Phase 2) consists
of log 4i sequential scans of size 4i. Adding fakes (Phase
3) requires requires copying back log n ∗ 4i items. To esti-
mate this cost we must sum over all log4 n levels, recalling
that each level is reordered only once every 4i−1 queries.
We therefore remove a factor of 4i−1, and sum over all lev-
els, to calculate the amortized overhead. Figure 6 shows
the resulting formulas. If all of these costs are incurred se-
quentially, we have an amortized response rate of approx-
imately 637ms/query ofﬂine plus 978ms/query online,
for 1.6s/query.
Network latency. a
Network transfer b
Disk seek latency c
Disk transfer d
Client processing
Formula
n/a
log4 n ∗ logn ∗4 ∗ 4 ∗ blksz/T hroughputlink
n/a
log4 n ∗ logn ∗4 ∗ 4 ∗ blksz/T hroughputdisk
log4 n ∗ logn ∗4 ∗ 4 ∗ blksz/T hroughputcrypto
Sample
< 1ms
500ms
< 1ms
98ms
49ms
aLevel reordering is not interactive, so idling can be avoided here.
bThe Phase 1 and 3 scans account for its bulk.
cSeek time will be hidden by disk transfer during reordering.
dThis load can be split among several disks.
Figure 6. Amortized ofﬂine cost per query.
The bottleneck when determining the parallel query
throughput is the network throughput, at 560ms/query.
This results in a query throughput of just under 2 queries