


$
	%
$
"!#%


	


	




(cid:6)





(cid:7)
(cid:4)

$&$* *'
.+.+
(cid:5)




(cid:7)
(cid:6)
 "$$&$* 
 '!!'
(cid:5)







 ++'(*')'''
	$
 

 ++'(*')'''
	%
Fig. 4: WorkÔ¨Çow of RAZZER‚Äôs hypervisor
kernel‚Äôs behavior with regard to thread interleaving, RAZZER
modiÔ¨Åes the hypervisor (i.e., a virtual machine monitor).
RAZZER‚Äôs modiÔ¨Åed hypervisor provides the following features
for the guest kernel: (i) setting up a breakpoint per CPU
core (more precisely, per virtual CPU core as they run on
a virtual machine) (¬ßIII-B1); (ii) resuming the execution of
kernel threads after the guest kernel hits breakpoints (¬ßIII-B2);
and (iii) checking whether a race truly occurred due to a
guest kernel (¬ßIII-B3). In the following section, we describe
how RAZZER supports each of these three features, and we
describe later how these hypervisor-level support features will
be leveraged during the fuzzing (¬ßIII-C2).
1) Setup Per-Core Breakpoint: RAZZER provides a new
hypercall interface, hcall_set_bp(), so that the guest kernel
can setup a per-core breakpoint as needed. In particular,
hcall_set_bp() is invoked by the guest kernel while using
two parameters: (1) vCPU_ID speciÔ¨Åes a virtual CPU (vCPU)
on which RAZZER should install the breakpoint; and (2)
guest_addr speciÔ¨Åes an address in guest OS‚Äôs address space
where a breakpoint is to be installed. Once receiving this
hypercall, the hypervisor installs the hardware breakpoint at
guest_addr, which is only effective on the speciÔ¨Åed vCPU.
There are two particular tasks involved here: (i) accurately
controlling the per-core execution behavior (more precisely, the
per vCPU execution behavior as the guest kernel is virtualized),
and (ii) determining whether a breakpoint is triggered by a
speciÔ¨Åc kernel thread while running a speciÔ¨Åc syscall.
In order to achieve the Ô¨Årst task, RAZZER utilizes a hardware
breakpoint supported by a virtualized debug register, while
ensuring that the breakpoint event is always delivered to the
hypervisor Ô¨Årst (instead of being delivered back to the guest
kernel). In the case of the x86 architecture, RAZZER stores
the guest address in a virtualized debug register. As this debug
register is virtualized and thus maintained for every vCPU,
RAZZER can ensure that the hardware breakpoint is triggered
only if a corresponding vCPU is executing it. Moreover, to
ensure that the hardware breakpoint event is delivered Ô¨Årst to
the hypervisor, RAZZER leverages the Virtual Machine Control
Structure (VMCS) in Intel VT-x. VMCS contains an interrupt
bitmap, where each bit corresponds to a guest‚Äôs interrupt
number. When an interrupt is raised and the corresponding
interrupt bit is set in VMCS, the interrupt causes an immediate
VMEXIT event, which is delivered subsequently to the hypervisor.
Thus, RAZZER sets the bit in VMCS corresponding to the
hardware breakpoint interrupt such that RAZZER can Ô¨Årstly
monitor the hardware breakpoint.
The second task is related to the fact that while our hypervi-
sor understands the vCPU context, it does not understand the
kernel thread context. In other words, while the guest kernel
is running a given user program, it may also run other user
programs (e.g., Xorg and sshd) or kernel tasks (e.g.,kworker
and ksoftirqd) that are not related to the given user program.
In this case, if we simply install per vCPU breakpoint, the
breakpoint can be triggered by such unrelated programs or
kernel tasks as well. Because these breakpoint trigger events
occur irrespective of the given user program, RAZZER carries
out virtual machine introspection (VMI) to determine the kernel
thread context of the guest kernel. SpeciÔ¨Åcally, RAZZER‚Äôs
hypervisor retrieves the kernel thread id assigned by the guest
(cid:24)(cid:22)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:09 UTC from IEEE Xplore.  Restrictions apply. 
kernel using VMI. For example, the current version of RAZZER
running on Linux Ô¨Årst retrieves the thread_info structure, as
it is always located at the top of the kernel stack. RAZZER
then obtains the kernel thread id stored in task_struct by
following the reference to task_struct stored in thread_info.
Therefore, RAZZER‚Äôs hypervisor can determine whether or not
the breakpoint is Ô¨Åred by the destined kernel thread.
2) Resume Per-Core Execution: After two guest kernel
threads are stopped at their respective breakpoint address (i.e.,
RacePaircand), RAZZER resumes the execution of both vCPUs
such that both threads execute RacePaircand concurrently.
One important decision for RAZZER to make here is: which
kernel thread should be resumed Ô¨Årst? This is important
for identifying data races because some race bugs are only
exhibited on a speciÔ¨Åc execution order. For example, as shown
in CVE-2017-2636 (¬ßII-A), a race occurs only if kernel thread
B proceeds Ô¨Årst after stopping at RacePaircand. For this reason,
our hypervisor provides an interface, hcall_set_order(), to
control the execution order‚Äîi.e., a speciÔ¨Åed vCPU ID in
hcall_set_order() is executed Ô¨Årst, followed by the other.
The workÔ¨Çow of an execution resume is shown in Figure 4.
First, it is assumed that two kernel threads are stopped at their
respective breakpoints (i.e., 1 and 2 ), and hcall_set_order()
(vCPU0) has been invoked. To resume, RAZZER picks vCPU
as speciÔ¨Åed by hcall_set_order() (i.e., vCPU0 in Figure 4)
and conducts a single-step on that vCPU immediately to
stop after executing a single instruction ( 3 ). This single-step
ensures that vCPU0 proceeds before vCPU1, as commanded by
hcall_set_order(). Lastly, RAZZER resumes the execution
of both vCPU0 and vCPU1 ( 4 ).
3) Check Race Results: Our hypervisor checks whether a
given RacePaircand actually results in a race (which we call a
true race) when both breakpoints are hit concurrently. More
speciÔ¨Åcally, when both memory instructions in RacePaircand
hit breakpoints, our hypervisor conducts an introspection of the
destined addresses to be accessed by these instructions. If these
addresses are identical, RAZZER then concludes that a given
RacePaircand truly races, promoting such a pair to RacePairtrue.
More technically, our hypervisor computes the destined address
value by disassembling the instruction at each RacePaircand
location and obtaining the concrete register values stored in
each vCPU. For example, as illustrated in A (Figure 4),
RAZZER determines whether a given RacePaircand truly races
considering that both threads access the same memory location,
0xffff8801e704c020 (i.e., %rdi + 0x20 == %rbx + 0x20).
C. Two Phased Fuzzing to Discover Races
Here, we describe how RAZZER discovers race bugs
through fuzzing. RAZZER‚Äôs fuzzing is performed in two
phases (as shown in Figure 3). (i) the single-thread fuzzing
phase (¬ßIII-C1) Ô¨Ånds a single-thread user program that
triggers any RacePaircand; and (ii) the multi-thread fuzzing
phase (¬ßIII-C2) Ô¨Ånally Ô¨Ånds a multi-thread user program that
triggers a harm race based on the result of the single-thread
phase. Each fuzzing phase consists of two components, the
generator and the executor, where the generator creates a user
program and the executor then runs the program.
1) Single-Thread Fuzzing: In this phase, the single-thread
generator initially generates Pst, a single-thread program with a
sequence of random syscalls. Next, the single-thread executor
runs each Pst, while testing whether each execution of Pst
covers any RacePaircand (which is generated by running a static
analysis as described in ¬ßIII-A). If covered, the single-thread
executor passes Pst, which is annotated with the information on
covered RacePaircand, to the next phase, multi-thread fuzzing.
Below, we describe the details of the single-thread generator
and executor in turn.
Single-Thread Generator. The single-thread generator con-
structs a single-threaded user-land program (which we refer
to as Pst), performing a sequence of random system calls
to test the kernel‚Äôs behavior. RAZZER constructs Pst with
the following two strategies: generation and mutation. When
using the generation strategy, RAZZER randomly generates Pst
following the pre-deÔ¨Åned system call grammar. This system call
grammar includes all available system calls as well as a range
of reasonable parameter values for each syscall. Following
this grammar, RAZZER attempts to construct a reasonable user
program by randomly selecting a sequence of system calls. It
then randomly populates each system call‚Äôs parameters, and its
return value is randomly piggy-backed onto the parameters of
a following the syscall as well. As we describe in more detail
in ¬ßIV, RAZZER utilizes pre-deÔ¨Åned system call grammar in
Syzkaller [42].
As opposed to generation, mutation randomly mutates the
existing Pst. It may randomly drop some syscalls in Pst, insert
new syscalls, or change certain parameter value.
Single-Thread Executor. Given Pst from the generator, the
single-thread executor runs each Pst while performing the
following two tasks. First, if an execution of Pst covers two
memory access instructions in any RacePaircand, RAZZER an-
notates such matched RacePaircand information to Pst. RAZZER
then passes this annotated Pst to the multi-thread generator so
that it can be further checked as to whether it is racing.
More speciÔ¨Åcally, while running Pst, RAZZER monitors
the execution coverage per syscall, leveraging the underlying
kernel‚Äôs support to collect the execution coverage, as this
capability is a general feature in modern kernels (e.g., the
current prototype of RAZZER for Linux relies on KCov [3]).
After running Pst, RAZZER checks if its execution coverage
matches any RacePaircand‚Äîi.e., one syscall in Pst causes the
kernel to execute one instruction of RacePaircand, and the
other syscall executes the other instruction of RacePaircand.
If matched, RAZZER annotates the detailed information of
the matched RacePaircand to Pst such that RAZZER can test
whether RacePaircand can be deterministically triggered in the
subsequent fuzzing phases. This annotated information includes
the following: (i) two racy syscalls, each of which executes
RacePaircand; and (ii) the addresses of RacePaircand.
Note that there can be multiple RacePaircand matched from
a single Pst. Based on our experience with RAZZER, running a
(cid:24)(cid:22)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:09 UTC from IEEE Xplore.  Restrictions apply. 
single Pst matches up to 130 unique RacePairscand. In this case,
RAZZER annotates each RacePaircand to a cloned individual
copy of Pst.
Second, if a running result of Pst yields new coverage not
executed before by any other Pst, RAZZER saves this Pst and
feeds it back to the single-thread generator again. In fact,
this mechanism is largely similar to fuzz testing techniques
(known as maintaining a fuzzing corpus). A fuzzing corpus
informally represents a minimal set of inputs, which may cover
all previously explored basic blocks if all corpus inputs were
executed.
Example: CVE-2017-2636 with Single-Thread Fuzz. As
illustrated in Figure 3,
suppose a single-thread gen-
erator generated Pst with the following three syscalls
in order, int fd = open(...), ioctl(fd, TCLFLSH), and
write(fd, ...), while there are other syscalls as well in the
middle of these three. If this Pst is executed by the single-
thread executor, it identiÔ¨Åes that a certain RacePaircand has
been executed by the program‚Äîi.e., the pair(n_hdlc.c:216,
n_hdlc.c:440) where the Ô¨Årst is executed by n_hdlc.c:216 and
the second is executed by ioctl(fd, TCLFLSH). In such a case
all of this matched information is annotated to Pst and passed
to the multi-thread fuzzing phase.
2) Multi-Thread Fuzzing: After the single-thread fuzzing
phase, RAZZER moves on to the multi-thread fuzzing phase.
For each RacePaircand, the multi-thread generator transforms Pst
into Pmt, a multi-thread version of Pst. Pmt is also instrumented
with hypervisor calls to trigger a race deterministically at
the given RacePaircand. Lastly, multi-thread executor runs
each Pmt. If the Pmt is conÔ¨Årmed to trigger a race by the
hypervisor, RAZZER promotes the corresponding RacePaircand
to RacePairtrue, and continues to mutate Pmt by feeding it
back to the generator. Furthermore, if Pmt crashes the kernel,
RAZZER produces a detailed report of an identiÔ¨Åed harmful
race.
Multi-Thread Generator. The multi-thread generator takes an
annotated Pst (which includes RacePaircand) as input. Then it
outputs Pmt, a multi-thread version of Pst while leveraging
the annotated RacePaircand information to trigger the race
deterministically with hypercalls.
Figure 5 illustrates a simpliÔ¨Åed pseudo-code of this transfor-
mation process. It takes the following arguments as input:
Pst, the program to be transformed; i and j, each is an
index of racing syscalls within Pst (i ‚â§ j); and RP_i and
RP_j, each is an address of a corresponding RacePaircand
instruction. For simplicity, it is assumed that all of the annotated
information (from the single executor) is provided as an input
argument. This algorithm initially constructs two different
execution threads, thr0 and thr1, where each execution is
pinned to an individual virtual CPU (lines 8 and 9). RAZZER
leverages the kernel‚Äôs existing feature to pin threads (i.e., the
sched_setaffinity syscall in Linux), which enables Ô¨Çexible
thread controls from the user-space.
It then extracts all syscalls from Pst (line 12) and subse-
quently splits these syscalls into two different threads, thr0
# Get pinned threads, thr0 and thr1
thr0 = get_pinned_thread(vCPU0)
thr1 = get_pinned_thread(vCPU1)
# Assign syscalls to thr0 and thr1
syscalls = get_syscalls(Pst)
thr0.add_syscalls(syscalls[:i])
thr1.add_syscalls(syscalls[i+1:j])
1 def Convert_Pst_to_Pmt(Pst, i, j, RP_i, RP_j):
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21