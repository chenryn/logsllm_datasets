1.5%–3.3% of trials [24]. A more recent work, the RON sys-
tem reports 21 “path-hours” of complete or partial outages
out of a total of 6825 path-hours, a 0.31% outage rate [1].
Feamster et al. measure Internet path failures from 31 van-
tage points, correlated to BGP for causes [12]. They ﬁnd
that most failures are short (under 15 minutes) and discuss
the relationship between path failures and BGP messages.
SCORE is a system that extends measurements to isolate
the location of problems [19]. As with most of this work, we
validate our ﬁndings using control plane data.
Rather than a mesh, PlanetSeer studies traﬃc from 7–
12k end-users to a network of 120 nodes to track path out-
ages [35]. They report that their larger population identiﬁes
more anomalies than prior work; we expect our edge cover-
age of 3.4M blocks will be broader still. In addition, their
measurements occur only on connected clients; they miss
outages from already disconnected clients.
Choﬀnes et al. collect information from end systems to de-
tect service-level network events [4]. Our work is diﬀerent in
that we probe to the network edge and do not require extra
software or speciﬁc operating systems in the edge networks.
Client support in these studies allows better fault diagno-
sis than our work. Our work complements theirs by provid-
ing much larger coverage (3.4M /24 blocks, a large fraction of
the Internet edge), rather than “only” meshes of hundreds of
nodes, or thousands of end hosts. Our centralized measure-
ment also allows stronger statements about coverage since
we do not depend on end hosts that may come or go.
3.4 Passive Data Analysis
Recent work by Dainotti et al. considers Internet outages
caused by political censorship [7, 8]. They use a novel ap-
proach that combines observations from both control-plane
(BGP logs) and data-plane sources (traﬃc to unoccupied
addresses at UCSD network telescope and active probing
data from Ark). They focus on using multiple passive data
sources, ﬁnding their active probes are of limited use because
they probe each /24 only every three days. We instead show
that a single PC can actively track millions of /24 blocks,
providing guaranteed precision for blocks that respond to
probes. It is unclear if passive analysis can provide strong
statements about precision or coverage, but it does provide
important insight into networks that block active probes.
Turner et al. have also mined “low-quality” data sources
(router conﬁgurations, e-mail and syslogs), to detect failures
in the CENIC network [32]. Such log analysis requires col-
laboration with the monitored networks, thus focuses on a
single ISP. In contrast, our active probing is done indepen-
dent of the target.
4. PRINCIPLED LOW-RATE PROBING
Trinocular carries out principled probing: we deﬁne a sim-
ple model of the Internet to capture elements essential to
outage detection. Trinocular establishes belief B(U ) that
each block is available, and uses Bayesian inference to learn
the current status of the network. We drive probing using
this model and belief, sending at regular intervals to guar-
antee freshness, and more quickly when necessary to resolve
uncertainty about network state.
4.1 An Outage-Centric Model of the Internet
Trinocular’s model of the Internet tracks block -level out-
ages, measured with probes to active addresses, and reasons
about them using belief changed by Bayesian inference.
We study /24 address blocks (designated b) as the smallest
unit of spatial coverage. Larger blocks, such as preﬁxes that
appear in global routes, may capture outages due to routing
changes, but they hide smaller outages. Prior work shows
that default routing is widely used [3], and outages occur
inside ISPs [29], and we show that outages often occur in
sizes smaller than routable preﬁxes (§6.2).
Trinocular sends only ICMP echo requests as probes, each
with a 4-byte payload. We chose end-to-end, data-plane
probing to detect outages unrelated to routing. We use
ICMP because it is innocuous and, compared to other op-
tions, less likely to be blocked or interpreted as malicious [13].
In each block, we model which addresses are active, the
ever active addresses, E(b), a set of up to 256 elements. To
interpret the meaning of probe responses, we model the ex-
pected response rate of E(b) as availability, A(E(b)), a value
from 0 to 1, never to always responding.
These dimen-
sions are independent, so a block where E(b) = 64 and
A(E(b)) = 0.5 has one-quarter of addresses that each re-
spond (on average) half the time. We discard very sparse
and very unresponsive blocks as non-analyzable (§4.4).
For blocks when A(E(b))  0.9. When a periodic
probe causes our belief to become uncertain, or to shift to-
wards uncertainty, we carry out additional, adaptive, short-
timescale probes to resolve this uncertainty. For adaptive
probing, we send new additional probes as soon as each prior
probe is resolved until we reach a conclusive belief of the
block status. Most probes are resolved by 3 s timeout, so
adaptive probes typically occur every 3 s.
Usually a few adaptive probes will quickly resolve uncer-
tainty in our belief; we study this value in §5.3. As address
usage becomes sparser, the number of probes to converge
grows geometrically (Figure 1). To bound probing, we send
at most 15 total probes per round (1 periodic and up to 14
additional adaptive). We cease probing when belief is deﬁni-
tive and not shifting; if we cannot reach deﬁnitive belief in
15 probes we mark the block as uncertain. Uncertainty is
similar to the “hosed” state in prior work [29]. We specu-
late that Bayesian analysis could resolve some intermediate
states in their work, but detailed comparison is future work.
Recovery probing: There is an asymmetry when blocks
transition from down-to-up for intermittently active blocks
(low A(E(b))). While positive responses are strong evidence
the block is up, interpretation of negative responses has in-
creasing ambiguity as A falls. When an intermittent block
 0 5 10 15 20 25 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1probes requiredavailability A(E(b))(maximum allowed probes)(min. allowed A(E(b))down-to-upup-to-downstill downcomes back up, we still may see several negative responses
if probes chance upon temporarily unoccupied addresses.
To account for this asymmetry, we do additional recovery
probes for blocks that are down. From A(E(b)), the prob-
ability we get consecutive misses due to k vacant addresses
is (1 − A)k, resulting in a “false negative” belief that an up
block is down. We select k to reach a 20% false-negative rate
as a function of A (k is the “still down” line in Figure 1),
performing up to k = 15 total probes when A = 0.1 With re-
covery probes, false negatives cause outages in sparse blocks
that are one third of a round too long, on average.
Traﬃc: For long-term operation across the Internet, Tri-
nocular must have minimal impact on target networks. Our
benchmark is Internet background radiation, the unsolicited
traﬃc every public IP address receives as part of being on
the public network.
It thus provides a reasonable baseline
of unsolicited traﬃc against which to balance our measure-
ment. A typical unused but routable /8 block receives 22 to
35 billion packets per week [34], so each /24 block sees 2000
to 3300 packets/hour. Our goal is to increase this rate by
no more than 1%, on timescales of 10 minutes.
In the best case, we send only 5.4 probes/hour per /24
block in steady state, and if all addresses in a block are
active, we probe each address only every other day. This
best-case is only a 0.25% increase in traﬃc. With adaptive
and recovery probing, our worst-case probing rate adds 15
probes per 11-minute round, an average probe rate of 82
probes/hour per /24 block, about 5% of the rate of back-
ground radiation. Since this worst case will occur only for
low-A blocks that change state, we expect typical perfor-
mance to be very close to best case, not worst case.
In
§5.3 we show experimentally that median traﬃc is at 0.4%
to 0.7% of our benchmark, our 5% worst case occurs less
than 2% of the time.
4.4 Parameterizing the Model: Long-term Ob-
servation
We determine parameters E(b) and A(E(b)) for each block
to weigh the information in each probe.
Initialization: We use long-term, multi-year, Internet
censuses to initialize these parameters for each block. Prior
work generates regular IP history datasets that provide the
information we need [10]. These datasets include the respon-
siveness of each public, unicast IP address in IPv4 measured
16 times over approximately the last 3 years. We use the
full history (16 measurements) to identify E(b). To use re-
cent data, we consider only the 4 most recent censuses to
compute A(E(b)). We update E(b) every 2-3 months as
new history datasets become available, bringing in newly ac-
tive blocks and retiring gone-dark blocks. Current Internet
censuses are speciﬁc to IPv4. Our approach applies to IPv6
if E(b) can be determined, but methods to enumerate all or
part of IPv6 are an area of active research.
It is very traﬃc-intensive to track intermittent and sparse
blocks with reasonable accuracy (see Figure 1). We therefore
discard blocks where addresses respond very infrequently
(A(E(b)) < 0.1). We also discard blocks that are too sparse,
where E(b) < 15, so that we are not making decisions based
on a very few computers.
Because A(E(b)) is based on
only recent censuses, discard of low A(E(b)) blocks removes
“gone dark” blocks [10].
Of the 16.8M unicast blocks as of July 2012, we ﬁnd 14.5M
are routed, 8.6M are non-responsive, 0.7M have E(b) < 15,
1.5M have A(E(b)) < 0.1, leaving 3.4M blocks that are an-
alyzable: 24% of the routed space (and 40% of responsive).
Since most of the Internet is always up, we set belief to
indicate all blocks are up on startup.
Evolution: As networks change, model parameters may