### Evasion
Our watermarking framework comprises three key components: watermark generation, watermark embedding, and ownership verification. Among these, only the ownership verification component needs to be performed remotely. Consequently, one potential method for evading our watermarking framework is to prevent queries from reaching the ownership verification stage.

Recently, Meng et al. [39] proposed a framework called MagNet to defend against adversarial queries. MagNet trains multiple AutoEncoders using normal data to learn the representation of normal data, which are then used as abnormal detectors. The underlying principle is that adversarial samples typically have a different distribution compared to normal samples. This defense technique could also be applied to protect against our ownership verification queries, as our embedded watermarks introduce a similar difference from normal samples. However, the effectiveness of MagNet relies on having a sufficient number of normal examples for training the detector networks. Insufficient normal examples can lead to high false positive rates. In our scenario, we assume that plagiarizers do not have access to a sufficient normal dataset to train such detectors; otherwise, they would be able to train the model themselves without the need to steal it.

### 7. Related Work

#### Watermarking
Digital watermarking is a method for embedding secret information into digital media to protect ownership. Various approaches have been developed to make watermarks both efficient and robust against removal attacks. Spatial domain digital watermarking algorithms, such as those explored in [7, 30, 36, 52], embed secrets by directly manipulating pixels in an image. For example, the least significant bit (LSB) [30, 36] of pixels is commonly used for embedding. However, these techniques are vulnerable to attacks and sensitive to noise and common signal processing. Frequency domain methods, which embed watermarks in the spectral coefficients of the image, are more widely used. Commonly employed transforms include the Discrete Cosine Transform (DCT) [25, 44], Discrete Fourier Transform (DFT) [42, 55], Discrete Wavelet Transform (DWT) [9, 11, 31, 58], and combinations thereof [6, 38, 46]. To verify the ownership of protected media data, existing watermarking algorithms require direct access to the media data to extract the watermarks and verify ownership. However, in deep neural networks (DNNs), we need to protect the DNN models rather than the input media data. After training, only the DNN model API is typically available for ownership verification, making traditional digital watermarking algorithms unsuitable for protecting DNN models.

Uchida et al. [54] proposed the first method for embedding watermarks into DNNs by embedding information into the weights of the network. This approach assumes that the stolen models can be locally accessed to extract all parameters, which is impractical since most DNNs are deployed as online services, making it difficult to directly access model parameters, especially for stolen models. Merrer et al. [40] introduced a zero-bit watermarking algorithm that uses adversarial samples as watermarks to verify DNN ownership. They fine-tune DNN models to include specific true/false adversaries and use combinations of these adversaries as keys \( K \) to verify the DNN models. If the DNN models return predefined results for these keys \( K \), ownership is confirmed. However, this algorithm has a vulnerability: each model can essentially have an infinite number of such keys, allowing anyone to claim ownership with any \( K \). For instance, one can generate a set of adversarial samples with any DNN model and claim ownership of the model. Our framework, in contrast, can remotely verify DNN ownership, and our embedded watermarks are unique to each model. For example, in our \( \text{WM}_{\text{content}} \) watermark generation algorithm, only an image with the embedded content "Test" can trigger the predefined output.

#### Deep Neural Network Attack and Defense
As DNNs become more widely used, various attacks have been investigated. Fredrikson et al. [16] introduced a model inversion attack that can recover images from the training dataset. Our evaluation shows that our watermarking framework is robust to such attacks. Tramer et al. [53] introduced an attack to steal general machine learning models, which can be mitigated by updating DNN APIs to not return confidence scores and not respond to incomplete queries. Shokri et al. [47] introduced membership inference attacks, which determine whether a given record was part of the model's training dataset. Such attacks are not applicable for inferring our watermarks, as attackers would need to know the watermarks first. Recent works [37, 20] have introduced deep neural network Trojaning attacks, which embed hidden malicious functionality into neural networks. Similar to Trojans in software, such attacks can be prevented by checking model integrity. Our threat model focuses on using watermarking to protect the intellectual property of DNN models.

### 8. Conclusion
In this paper, we generalized the concept of "digital watermarking" for DNNs and proposed a general watermarking framework to generate, embed, and remotely verify the ownership of DNN models based on embedded watermarks. We formally defined the threat model for watermarking in DNNs, supporting both white-box and black-box access. The key innovation of our framework is its ability to remotely verify the ownership of DNN services with minimal API queries. We conducted a comprehensive evaluation of our framework on two benchmark datasets, demonstrating that it meets the general watermarking standard and is robust to various counter-watermark attacks.