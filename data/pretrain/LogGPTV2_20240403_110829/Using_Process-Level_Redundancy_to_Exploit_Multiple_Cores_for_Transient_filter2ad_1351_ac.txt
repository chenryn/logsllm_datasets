Figure 4. Distribution of the number of executed instructions between the injection and detection
of a fault. Percentages are normalized to all the runs which are detected via output mismatch (M),
program failure (S), or both combined (A).
4.3 Performance Results
Performance is evaluated using two redundant processes
for fault detection (PLR2), and three processes to support
recovery (PLR3). Figure 5 shows PLR performance on
benchmarks compiled with both -O0 and -O2 compiler
ﬂags. Performance is normalized to native execution time.
PLR provides transient fault tolerance on -O0 programs
with an average overhead of 8.1% overhead for PLR2 and
15.2% overhead for PLR3. On -O2 programs, PLR2 incurs
a 16.9% overhead for PLR2 and 41.1% overhead for PLR3.
Overhead in PLR is due to the fact that multiple redundant
processes are contending for system resources. Programs
which place higher demands on systems resources result in
a higher PLR overhead. Optimized binaries stress the sys-
tem more than unoptimized binaries (e.g. higher L3 cache
miss rate) and therefore have a higher overhead. As the
number of redundant processes increases, there is an in-
creasing burden placed upon the system memory controller,
bus, as well as cache coherency implementation. Simi-
larly, as the emulation is called with more processes, the
increased synchronization with semaphores and the usage
and shared memory may decrease performance. At certain
points, the system resources will be saturated and perfor-
mance will be severely impacted. These cases can be ob-
served in 181.mcf and 171.swim when running PLR3 with
-O2 binaries. PLR overhead and system resource saturation
points are explained in more detail in the next subsection.
4.4 PLR Overhead Breakdown
The performance overhead of PLR consists of contention
overhead and emulation overhead, shown as stacked bars in
Figure 5. Contention overhead is the overhead from simul-
taneously running the redundant processes and contending
for shared resources such as the memory and system bus.
The contention overhead is measured by running the ap-
plication multiple times independently and comparing the
overhead to the execution of a single run. This roughly
simulates running the redundant processes without PLR’s
synchronization and emulation. The rest of the overhead is
considered emulation overhead. Emulation overhead is due
to the synchronization, system call emulation, and mecha-
nisms for fault detection incurred by PLR.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
Fault Injection and Detection ResultsBenchmark164.gzip176.gcc181.mcf186.crafty197.parser254.gap255.vortex256.bzip2300.twolf168.wupwise171.swim172.mgrid173.applu178.galgel183.equake187.facerec189.lucas191.fma3dPercent of Runs020406080100CorrectIncorrectAbortFailedMismatchSigHandlerDistribution of Instructions Executed Between Fault Injection and DetectionBenchmark164.gzip176.gcc181.mcf186.crafty197.parser254.gap255.vortex256.bzip2300.twolf168.wupwise171.swim172.mgrid173.applu178.galgel183.equake187.facerec189.lucas191.fma3d% Runs Detected020406080100MMMMMMMMMMMMMMMMMMSSSSSSSSSSSSSSSSSSAAAAAAAAAAAAAAAAAA1010010001000010000+37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Figure 5. Overhead of running PLR on a set of both unoptimized and optimized SPEC2000 bench-
marks. The combinations of runs include -O0 compiled binaries with PLR2 (A), -O0 with PLR3 (B),
-O2 with PLR2 (C) and -O2 with PLR3 (D).
Figure 6. PLR overhead vs. L3 cache miss
rate.
For the set of benchmarks, contention overhead is signif-
icantly higher than emulation overhead. Benchmarks such
as 181.mcf and 189.lucas have relatively high cache miss
rates leading to a high contention overhead with increased
memory and bus utilization. On the other hand, 176.gcc
and 187.facerec substantially utilize the emulation unit and
result in a high PLR overhead.
4.4.1 Contention Overhead
Contention overhead mainly stems from the sharing of
memory bandwidth between the multiple redundant pro-
cesses. To study the effects of contention overhead, we
construct a program to generate memory requests by peri-
odically missing in the L3 cache. Figure 6 shows the effect
of L3 cache miss rate on contention overhead when run-
ning with PLR. For both PLR2 and PLR3, the L3 cache
miss rate has a substantial affect on the contention over-
head. With less than 10 L3 cache misses per second, there
Figure 7. PLR overhead vs. system call rate.
can be a signiﬁcant overhead of about 10%. After that point,
the overhead increases greatly with over a 50% overhead at
about 40 L3 cache misses per second. These results indicate
that the total overhead for using PLR is highly impacted by
the applications cache memory behavior. CPU-bound ap-
plications can be protected from transient faults with a very
low overhead while memory-bound applications may suffer
from high overheads.
4.4.2 Emulation Overhead
Emulation overhead mainly consists of the synchronization
overhead and the overhead from transferring and comparing
data in shared memory. To examine each aspect of emula-
tion overhead, two synthetic programs were designed and
run with PLR. The ﬁrst program calls the times() system
call at a user-controlled rate. times() is one of the of sim-
pler system calls supported by PLR and is used to measure
the emulation overhead from the barrier synchronizations
within the emulation unit. The second test program calls the
write() system call ten times a second and writes a user-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
PLR OverheadBenchmark164.gzip176.gcc181.mcf186.crafty197.parser254.gap255.vortex256.bzip2300.twolf168.wupwise171.swim172.mgrid173.applu178.galgel183.equake187.facerec189.lucas191.fma3dAvg% Perf. Overhead020406080100120140160AAAAAAAAAAAAAAAAAAABBBBBBBBBBBBBBBBBBBCCCCCCCCCCCCCCCCCCCDDDDDDDDDDDDDDDDDDDContentionEmulationL2 Cache Misses/Sec vs. OverheadL2 Cache Misses/Sec01020304050607080% Overhead020406080100120140160180200PLR2PLR3Calls to Emulation Unit vs. OverheadEm. Unit Calls per Second.1110100100010000% Overhead0102030405060708090100110PLR2PLR337th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007pi bit [37] and dependence-based checking [34] have been
explored as methods to follow the propagation of faults in
an attempt to only detect faults which affect program be-
havior. The software-centric model accomplishes the same
task on a larger scale.
The PLR approach is similar to a body of fault toler-
ant work which explores the use of replicas for fault toler-
ance [6, 8, 7, 24, 38, 39]. This body of work targets hard
faults (such as hardware or power failures) and assumes
fail-stop execution [30] in which the processor stops in the
event of failure. For transient faults, this assumption does
not hold. As far as we know, we provide the ﬁrst perfor-
mance evaluation, and overhead breakdown, of using redun-
dant processes on general-purpose multiple core systems.
There have been a number of previous approaches to pro-
gram replication. N-version programming [3] uses three
different versions of an application for tolerating software
errors. Aidemark uses a time redundant technique which
execute an application multiple times and use majority vot-
ing [1]. Virtual duplex systems combine both N-version
programming and time-redundancy [10, 19]. The Tan-
dem Nonstop Cyclone [16] is a custom system designed to
use process replicas for transaction processing workloads.
Chameleon [17] is an infrastructure designed for distributed
systems which uses various ARMOR processes (some sim-
ilar to process replicas) to implement adaptive and conﬁg-
urable fault tolerance. DieHard [5] proposes using repli-
cas in general-purpose machines for tolerating memory er-
rors. Shadow proﬁling [22] uses process replicas for low-
overhead program instrumentation.
6 Conclusion
This paper motivates the necessity for software tran-
sient fault tolerance for general-purpose microprocessors
and proposes process-level redundancy (PLR) as an attrac-
tive alternative in emerging multi-core processors. By pro-
viding redundancy at the process level, PLR leverages the
OS to freely schedule the processes to all available hardware
resources. In addition, PLR can be deployed without modi-
ﬁcations to the application, operating system or underlying
hardware. A real PLR prototype supporting single-threaded
applications is presented and evaluated for fault coverage
and performance. Fault injection experiments prove that
PLR’s software-centric fault detection model effectively de-
tects faults which safely ignoring benign faults. Exper-
imental results show that when running an optimized set
of SPEC2000 benchmarks on a 4-way SMP machine, PLR
provides fault detection with an 16.9% overhead. PLR per-
formance improves upon existing software transient fault
techniques and takes a step towards enabling software fault
tolerant solutions comparable to hardware techniques.
Figure 8. PLR overhead vs. data bandwidth.
speciﬁed number of bytes per system call. Each write()
system call forces the emulation unit to transfer and com-
pare the write data in shared memory.
Figure 7 shows the effect of synchronization on the PLR
overhead. Synchronization overhead is minimal up until
about 300-400 emulation unit calls per second with less
than 5% overhead for using PLR with both two and three
redundant processes. Afterward, the emulation overhead in-
creases quickly. Overall, these results indicate that the PLR
technique might be best deployed for speciﬁc application
domains without signiﬁcant system call functionality.
Figure 8 illustrates the effect of write data bandwidth on
emulation overhead. The experiment evaluates the amount
of data at each system call that must be compared between
redundant process techniques. The write data bandwidth
has a similar characteristics as system call synchronization,
achieving low overhead until a cut-off point. In this case,
for the experimental machines evaluated, the overhead is
minimal when the write data rate stays less than 1MB per
second but then increases substantially after that point for
both PLR2 and PLR3.
5 Related Work
PLR is similar to a software version of the hardware
SMT and CMP extensions for transient fault tolerance [11,
23, 28]. However, PLR aims to provide the same function-
ality in software. Wang [35] proposes a compiler infrastruc-
ture for software redundant multi-threading which achieves
19% overhead with the addition of a special hardware com-
munication queue. PLR attains similar overhead and only
relies on the fact that multiple processors exist. In addition,
PLR does not require source code to operate.
Executable assertions [14, 15] and other software detec-
tors [27] explore the placement of assertions within soft-
ware. Other schemes explicitly check control ﬂow during
execution [31, 25]. The software-centric approach provides
a different model for transient fault tolerance using a soft-
ware equivalent of the commonly accepted SoR model. The
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
Data Write Bandwidth vs. OverheadBytes Written per Second10^210^310^410^510^610^710^8% Overhead0102030405060708090100110120130PLR2PLR337th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20077 Acknowledgments
The authors would like to thank the anonymous review-
ers, Robert Cohn, Manish Vachharajani, Rahul Saxena, and
the rest of the DRACO Architecture Research Group for
their insightful comments and helpful discussion. This
work is funded by Intel Corporation.
References
[1] J. Aidemark, J. Vinter, P. Folkesson, and J. Karlsson. Exper-
imental evaluation of time-redundant execution for a brake-
by-wire application. In Proc. of DSN, 2002.
[2] H. Ando and et al. A 1.3ghz ﬁfth generation sparc64 mi-
croprocessor. In Proceedings of the Conference on Design
Automation, 2003.
[3] A. Avizeinis. The n-version approach to fault-tolerance
IEEE Transactions on Software Engineering,
software.
11(12):1491–1501, December 1985.
[4] R. C. Baumann. Soft errors in commercial semiconductor
technology: Overview and scaling trends. In IEEE 2002 Re-
liability Physics Tutorial Notes, Reliability Fundamentals,
pages 121 01.1 – 121 01.14, April 2002.
[5] E. D. Berger and B. G. Zorn. DieHard: Probabilistic mem-
ory safety for unsafe languages. In PLDI, 2006.
[6] A. Borg, W. Blau, W. Graetcsh, F. Herrmann, and W. Oberle.
Fault tolerance under unix. ACM Transactions on Computer
Systems, 7(1):1–24, 1989.
[7] T. C. Bressoud. TFT: A Software System for Application-
In Proc. of the International
Transparent Fault-Tolerance.
Conference on Fault-Tolerant Computing, 1998.
[8] T. C. Bressoud and F. B. Schneider. Hypervisor-based Fault-
tolerance. In Proc. of SOSP, 1995.
[9] D. Bruening and S. Amarasinghe. Maintaining consistency
and bounding capacity of software code caches. In Proceed-
ings of the International Symposium on Code Generation
and Optimization, March 2005.
[10] K. Echtle, B. Hinz, and T. Nikolov. On hardware fault diag-
nosis by diverse software. In Proceedings of the Intl. Con-
ference on Fault-Tolerant Systems and Diagnostics, 1990.
[11] M. Gomaa and et al. Transient-fault recovery for chip mul-
tiprocessors. In ISCA, 2003.
[12] M. Gomaa and T. N. Vijaykumar. Opportunistic transient-
fault detection. In ISCA, 2005.
[13] S. Hareland and et al. Impact of CMOS Scaling and SOI on
Software Error Rates of Logic Processes. In VLSI Technol-
ogy Digest of Technical Papers, 2001.
[14] M. Hiller. Executable assertions for detecting data errors in
embedded control systems. In Proc. of DSN, 2000.
[15] M. Hiller and et al. On the placement of software mecha-
nisms for detection of data errors. In Proc. of DSN, 2002.
[16] R. W. Horst and et al. Multiple instruction issue in the Non-
Stop Cyclone processor. In ISCA, 1990.
[17] Z. Kalbarczyk, R. K. Iyer, S. Bagchi, and K. Whisnant.
Chameleon: A software infrastructure for adaptive fault tol-
erance. IEEE Transactions on Parallel and Distributed Sys-
tems, 10(6):560–579, 1999.
[18] T. Karnik and et al. Scaling Trends of Cosmic Rays Induced
Soft Errors in Static Latches Beyond 0.18µ. In VLSI Circuit
Digest of Technical Papers, 2001.
[19] T. Lovric. Dynamic double virtual duplex systems: A cost-
efﬁcient approach to fault-tolerance. In Proceedings of the
Intl. Working Conference on Dependable Computing for
Critical Applications, 1995.
[20] C.-K. Luk and et al. Pin: Building customized program anal-
ysis tools with dynamic instrumentation. In PLDI, 2005.
[21] S. E. Michalak and et al. Predicting the Number of Fatal
Soft Errors in Los Alamos National Laboratory’s ASC Q
Supercomputer. IEEE Transactions on Device and Materi-
als Reliability, 5(3):329–335, September 2005.
[22] T. Moseley, A. Shye, V. J. Reddi, D. Grunwald, and R. V.
Peri. Shadow proﬁling: Hiding instrumentation costs with
parallelism. In Proceedings of CGO, 2007.
[23] S. S. Mukherjee and et al. Detailed design and evaluation of
redundant multithreading alternatives. In ISCA, 2002.
[24] P. Murray, R. Fleming, P. Harry, and P. Vickers. Somersault:
Software fault-tolerance. Technical report, HP Labs White
Paper, Palo Alto, California, 1998.
[25] N. Oh and et al. Control-ﬂow checking by software signa-
tures. IEEE Transactions on Reliability, 51, March 2002.
[26] N. Oh and et al. Error detection by duplicated instructions in
super-scalar processors. IEEE Transactions on Reliability,
51, March 2002.
[27] K. Pattabiraman, Z. Kalbarczyk,
Iyer.
Application-based metrics for strategic placement of detec-
tors.
In Proceedings of 11th International Symposium on
Paciﬁc Rim Dependable Computing, 2005.
and R. K.
[28] S. K. Reinhardt and S. S. Mukherjee. Transient fault detec-
tion via simultaneous multithreading. In ISCA, 2000.
[29] G. A. Reis and et al. SWIFT: Software implemented fault
tolerance. In CGO, 2005.
[30] R. D. Schlichting and F. B. Schneider. Fail-stop processors:
An approach to designing fault-tolerant computing systems.
ACM Transactions on Computing Systems, 1(3):222–238,
August 1983.
[31] M. A. Schuette, J. P. Shen, D. P. Siewiorek, and Y. K. Zhu.
Experimental evaluation of two concurrent error detection
schemes. In Proceedings of FTCS-16, 1986.
[32] T. J. Slegel and et al. IBM’s S/390 G5 microprocessor de-
sign. IEEE Micro, 1999.
[33] K. Sundaramoorthy, Z. Purser, and E. Rotenburg. Slipstream
processors: improving both performance and fault tolerance.
In Proc. of ASPLOS, 2000.
[34] T. N. Vijaykumar, I. Pomeranz, and K. Cheng. Transient-
In Pro-
fault recovery using simultaneous multithreading.
ceedings of ISCA, 2002.
[35] C. Wang, H. seop Kim, Y. Wu, and V. Ying. Compiler-
managed software-based redundant multi-threading for tran-
sient fault detection. In Proceedings of CGO, 2007.
[36] N. Wang and et al. Y-Branches: When you come to a fork
in the road, take it. In PACT, 2003.
[37] C. Weaver and et al. Techniques to reduce the soft error rate
of a high-performance microprocessor. In ISCA, 2004.
[38] J. H. Wensley and et al. SIFT: Design and Analysis of a
Fault-Tolerant Computer for Aircraft Control. Proceedings
of the IEEE, 66(10):1240–1255, October 1978.
[39] Y. Yeh. Triple-triple redundant 777 primary ﬂight computer.
In Proceedings of the 1996 IEEE Aerospace Applications
Conference, volume 1, pages 293–307, February 1996.
[40] J. Ziegler and et al. IBM experiments in soft fails in com-
puter electronics (1978 - 1994). IBM Journal of Research
and Development, 40(1):3–18, January 1996.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007