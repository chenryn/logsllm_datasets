we want
to
show both
(X ≤ eǫ2 Z + δ2). We have
(Z ≤ eǫ2 X + δ2)
and
Z = PT ⊆D Pr[Λβ(D) = T ] Pr[Aβ1 (T ) ∈ O],
X = PT ⊆D−t
Pr[Λβ(D−t) = T ] Pr[Aβ1 (T ) ∈ O].
To analyze Z, we note that all the T ’s that resulted from
sampling from D with probability β can be divided into
those in which t is not sampled, and those in which t is
sampled. For a T in the former case, we have
Pr[Λβ(D) = T ] = (1 − β) Pr[Λβ(D) = T |t not sampled in T ]
= (1 − β) Pr[Λβ(D−t) = T ]
For a T in the latter case, we have
Pr[Λβ(D) = T ] = β Pr[Λβ (D) = T |t sampled in T ]
= β Pr[Λβ (D−t) = T−t].
[23] A. Machanavajjhala, J. Gehrke, D. Kifer, and
Hence we have
M. Venkitasubramaniam. ℓ-diversity: Privacy beyond
k-anonymity. In ICDE, page 24, 2006.
[24] A. Machanavajjhala, D. Kifer, J. M. Abowd,
J. Gehrke, and L. Vilhuber. Privacy: Theory meets
practice on the map. In ICDE, pages 277–286, 2008.
[25] F. McSherry and K. Talwar. Mechanism design via
diﬀerential privacy. In FOCS, pages 94–103, 2007.
[26] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
STOC, pages 75–84, 2007.
[27] P. Samarati. Protecting respondents’ identities in
microdata release. IEEE Trans. on Knowl. and Data
Eng., 13:1010–1027, November 2001.
[28] P. Samarati and L. Sweeney. Protecting privacy when
disclosing information: k-anonymity and its
enforcement through generalization and suppression.
Technical report, SRI International, 1998.
[29] L. Sweeney. Achieving k-anonymity privacy protection
using generalization and suppression. Int. J.
Uncertain. Fuzziness Knowl.-Based Syst.,
10(5):571–588, 2002.
(1 − β) Pr[Λβ(D−t) = T ] Pr[Aβ1 (T ) ∈ O]
β Pr[Λβ(D−t) = T−t] Pr[Aβ1 (T−t) ∈ O]
Z = PT ⊆D−t
+PT−t⊆D−t
Let Y = PT ′⊆D−t
Pr[Λβ(D−t) = T ′] Pr[Aβ1 (T ′
+t) ∈ O],
then we have Z = (1 − β)X + βY .
That A satisﬁes (β1, ǫ1, δ1)-DPS means that for each T, O,
Pr[Aβ1 (T+t) ∈ O] ≤ eǫ1 Pr[Aβ1 (T ) ∈ O]+δ1. Hence we have
Y ≤ PT ′⊆D−t
= eǫ1 PT ′⊆D−t
+δ1PT ′⊆D−t
= eǫ1 X + δ1.
Pr[Λβ(D−t) = T ′](cid:0)eǫ1 Pr[Aβ1 (T ′ ∈ O] + δ1(cid:1) ,
Pr[Λβ (D−t) = T ′] Pr[Aβ1 (T ′) ∈ O]
Pr[Λβ (D−t) = T ′]
Z = (1 − β)X + βY ≤ (1 − β)X + β(eǫ1 X + δ1)
≤ (1 − β + βeǫ1 )X + βδ1 = eǫ2 X + δ2.
To show that X ≤ eǫ2 Z + δ2, we observe that A satisﬁes
(β1, ǫ1, δ1)-DPS means that X ≤ eǫ1 Y + δ1 and hence Z =
(1 − β)X + βY ≥ (1 − β)X + βe−ǫ1 (X − δ1), and
X ≤
1
1 − β + βe−ǫ1
Z +
βe−ǫ1
1 − β + eβ−ǫ1
δ1
[30] L. Sweeney. k-anonymity: A model for protecting
We now show that
privacy. Int. J. Uncertain. Fuzziness Knowl.-Based
Syst., 10(5):557–570, 2002.
1
1 − β + βe−ǫ1
≤ eǫ2=ln(cid:16)1+(cid:16) β2
β1
(eǫ1 −1)(cid:17)(cid:17) = 1+β(eǫ1 −1) = eǫ2 .
APPENDIX
A. PROOFS
This appendix includes proofs not included in the main
body.
A.1 Proof of Theorem 1
Theorem 1.
Given an algorithm A that satisﬁes
(β1, ǫ1, δ1)-DPS, A also satisﬁes (β2, ǫ2, δ2)-DPS for any
δ1.
β2 γn f (j; n, β),
γ −1mPn
where γ = (eǫ−1+β)
eǫ
.
Proof. Let A denote the algorithm, and g be the data-
independent generalization procedure in the algorithm. For
any dataset D, any tuple t ∈ D, and for any output S. For
any ǫ ≥ − ln(1 − β), we show that the probability by which
e−ǫ ≤
Pr[A(D) = S]
Pr[A(D−t) = S]
≤ eǫ
(2)
is violated is δ. Note that this is a stronger version of
(ǫ, δ)-DP than the one in Deﬁnition 2. See [18] for rela-
tionship between the two.
Let n be the number of t′ in D such that g(t′) = g(t). Let
j be the number of times that g(t) appears in S. Note that
as the only diﬀerence between D and D−t is that D has one
extra copy of t, we have.
Pr[A(D)=S]
Pr[A(D−t))=S] = Pr[A(D) has j copies of g(t)]
Pr[A(D−t) has j copies of g(t)]
Because any tuple that appears less than k times is sup-
pressed, either j ≥ k, or j = 0. When j = 0, we have
Pr[A(D)=S]
Pr[A(D−t)=S] = F (k−1;n,β)
F (k−1;n−1,β) = Pk−1
Pk−1
i=0 f (i;n,β)
i=0 f (i;n−1,β)
.
Because F (k − 1; n, β) is always less than F (k − 1; n − 1, β);3
hence
Furthermore, we note that
Pr[A(D−t)=S]  n, the outcome
is good. We now consider the bad outcomes when j ≤ n.
Note that because ǫ ≥ − ln(1 − β), we have −ǫ ≤ ln(1 −
n−j > 1 − β ≥ e−ǫ. Hence we only need to
n−j > eǫ. This occurs when
, then this occurs when
β), and n(1−β)
consider what j’s make n(1−β)
j > (eǫ−1+β)n
j > γn.
. Let γ = (eǫ−1+β)
eǫ
eǫ
j:(j≥k∧j>γn) f (j; n − 1, β).
As the latter is smaller than the former, we only need to
So far our analysis has shown that a bad outcome S
for an input D would satisfy the condition j ≥ k and
n ≥ j > γn. Now we need to compute the proba-
bility that A(D) gives a bad outcome, and the proba-
bility that A(D−t) gives a bad outcome. The former is
j:(j≥k∧j>γn) f (j; n, β) And the latter
given below: maxnPn
is maxnPn−1
Let nm = l k
γ − 1m, we now show that when n ≤ nm,
Pn
j:(j≥k∧j>γn) f (j; n, β) increases when n increases. Note
that the choice of nm satisﬁes the condition that γnm  γn) becomes j ≥ k. The function
j:j≥k f (j; n, β) is monotonically increasing with respect to
Pn
n.
When n ≥ nm, the condition (j ≥ k ∧ j > γn) becomes
j > γn. (In fact, when n = nm + 1, the smallest j to satisfy
j > γn is k + 1.) Hence the error probability is bounded
j>γn f (j; n, β), where
by δ = d(k, β, ǫ) = maxn:n≥l k
γ = (eǫ−1+β)
γ −1mPn
.
eǫ
A.3 Proof of Theorem 6
Theorem 6: Any ǫ1-safe k-anonymization algorithm satis-
ﬁes (β, ǫ, δ)-DPS, where ǫ ≥ − ln(1 − β) + ǫ1, δ = d(k, β, ǫ −
ǫ1) = max
n:n≥⌈ k
j>γn f (j; n, β), γ = (eǫ−ǫ1 −1+β)
eǫ−ǫ1
.
γ −1⌉Pn
Proof. Let A denote the ǫ1-safe k-anonymization algo-
rithm. Here, we want to show that for any ǫ ≥ − ln(1 − β) +
ǫ1, D, t ∈ D and S,
e−ǫ ≤
Pr[A(D) = S]
Pr[A(D−t) = S]
≤ eǫ
(3)
is valid for probability at least 1 − δ. Let Λβ denote the
process of binomial sampling the dataset D with probability
β. And let G denote the set of all the possible outputs of A’s
subroutine Am. By deﬁnition, its subroutine Am satisﬁes
ǫ1-diﬀerential privacy, e−ǫ1 ≤ Pr[Am(D)=g]
Pr[Am(D−t)=g] ≤ eǫ1 . And,
according to the proof of Theorem 5, for a ﬁxed g ∈ G, the
ratio r(g) =
Pr[g(Λβ (D))=S]
Pr[g(Λβ (D−t))=S] equals
r(g) = 
i=0 f (i;n,β)
Pk−1
Pk−1
i=0 f (i;n−1,β)
n(1−β)
n−j
if j = 0;
if k ≤ j ≤ n
where j is the number of copies of g(t) in the output
dataset S. So, the diﬀerential privacy ratio (3) can be upper
bounded,
Pr[A(D)=S]
Pr[A(D−t)=S] = Pg∈G Pr[Am(D)=g]·Pr[g(Λβ (D))=S]
eǫ1 Pg∈G Pr[Am(D−t)=g]·Pr[g(Λβ (D))=S]
Pg∈G Pr[Am(D−t)=g]·Pr[g(Λβ (D−t))=S]
eǫ1 r(g) Pg∈G Pr[Am(D−t)=g]·Pr[g(Λβ (D−t))=S]
Pg∈G Pr[Am(D−t)=g]·Pr[g(Λβ (D−t))=S]
Pg∈G Pr[Am(D−t)=g]·Pr[g(Λβ (D−t))=S]
≤
≤
= eǫ1 r(g).
i=0 f (i;n,β)
The lower bound can be obtained in a similar way. So,
e−ǫ1 r(g) ≤ Pr[Aβ (D)=S]
Pr[Aβ (D−t)=S] ≤ eǫ1 r(g). By the proof of The-
orem 5, the ratio r(g) is bounded by e−(ǫ−ǫ1) ≤ r(g) ≤
e(ǫ−ǫ1). The probability that it is violated is the probability
that inequality (3) is violated. In the j = 0 case, e−(ǫ−ǫ1) ≤
Pk−1
≤ e(ǫ−ǫ1), since ǫ ≥ − ln(1 − β) + ǫ1. And for
Pk−1
i=0 f (i;n−1,β)
the k ≤ j ≤ n case, n(1−β)
n−j > (1 − β) ≥ e−ǫ+ǫ1 . And only
when n(1−β)
(cid:1), inequality (3) is
violated. Let γ = (eǫ−ǫ1 −1+β)
. The error probability δ is
n−j > eǫ−ǫ1 (cid:0)j > n(eǫ−ǫ1 −1+β)
γ −1⌉Pn
δ = d(k, β, ǫ − ǫ1) = maxn:n≥⌈ k
j>γn f (j; n, β),
eǫ−ǫ1
eǫ−ǫ1
where γ = (eǫ−ǫ1 −1+β)
.
eǫ−ǫ1