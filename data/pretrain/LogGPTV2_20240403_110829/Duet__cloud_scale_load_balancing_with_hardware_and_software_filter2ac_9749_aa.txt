title:Duet: cloud scale load balancing with hardware and software
author:Rohan Gandhi and
Hongqiang Harry Liu and
Y. Charlie Hu and
Guohan Lu and
Jitendra Padhye and
Lihua Yuan and
Ming Zhang
Duet: Cloud Scale Load Balancing with Hardware and
Software
Rohan Gandhi† Hongqiang Harry Liu∧ Y. Charlie Hu† Guohan Lu(cid:63)
Jitendra Padhye(cid:63) Lihua Yuan(cid:63) Ming Zhang(cid:63)
Microsoft(cid:63), Purdue University†, Yale University∧
ABSTRACT
Load balancing is a foundational function of datacenter infrastruc-
tures and is critical to the performance of online services hosted
in datacenters. As the demand for cloud services grows, expen-
sive and hard-to-scale dedicated hardware load balancers are being
replaced with software load balancers that scale using a distributed
data plane that runs on commodity servers. Software load balancers
offer low cost, high availability and high ﬂexibility, but suffer high
latency and low capacity per load balancer, making them less than
ideal for applications that demand either high throughput, or low la-
tency or both. In this paper, we present DUET, which offers all the
beneﬁts of software load balancer, along with low latency and high
availability – at next to no cost. We do this by exploiting a hith-
erto overlooked resource in the data center networks – the switches
themselves. We show how to embed the load balancing functional-
ity into existing hardware switches, thereby achieving organic scal-
ability at no extra cost. For ﬂexibility and high availability, DUET
seamlessly integrates the switch-based load balancer with a small
deployment of software load balancer. We enumerate and solve
several architectural and algorithmic challenges involved in build-
ing such a hybrid load balancer. We evaluate DUET using a pro-
totype implementation, as well as extensive simulations driven by
traces from our production data centers. Our evaluation shows that
DUET provides 10x more capacity than a software load balancer,
at a fraction of a cost, while reducing latency by a factor of 10 or
more, and is able to quickly adapt to network dynamics including
failures.
Categories and Subject Descriptors:
C.2.4 [Computer-
Communication Networks]: Distributed Systems—Network Oper-
ating Systems
General Terms: Design, Performance
Keywords: Load Balancing, Datacenter, SDN
1.
INTRODUCTION
A high performance load balancer is one of the most important
components of a cloud service infrastructure. Services in the data
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, Illinois, USA.
Copyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00.
http://dx.doi.org/10.1145/2619239.2626317
.
center scale by running on multiple servers, each with an individ-
ual direct IP (DIP). The service exposes one or more virtual IP
addresses (VIP) outside the service boundary. The load balancer
receives the trafﬁc destined for the VIP, splits it among the DIPs,
and routes it to the individual DIPs using IP-in-IP encapsulation.
The load balancer thus touches every packet coming into the
data center from the Internet, as well as a signiﬁcant fraction of
all intra-DC trafﬁc. This trafﬁc volume induces heavy load on both
data plane and control plane of the load balancer [17]. The per-
formance and reliability of the load balancer directly impact the
latency, throughput and the availability of the cloud services hosted
in the DC.
Traditional
load balancers are dedicated hardware middle-
boxes [1, 4] that are very expensive. In contrast, Ananta [17] is
a software load balancer that runs on commodity servers. Ananta
consists of a central controller, and several software Muxes (SMux)
that provide a distributed data plane. Each SMux maintains all VIP-
to-DIP mappings, and implements trafﬁc splitting and encapsula-
tion functionality in software. The Ananta architecture is ﬂexible,
highly scalable and ensures high availability.
However, software load balancers have two fundamental limita-
tions, both of which stem from the fact that they process the packets
in software. First, processing packets in software limits capacity.
Experiments show that the CPU on individual Ananta SMux be-
comes a bottleneck once the incoming trafﬁc exceeds 300K packets
per second. While the aggregate capacity of software load balancer
can be scaled out by adding more servers, doing so raises cost. For
example, handing 15Tbps trafﬁc (typical for a mid-sized DC) re-
quires over 4000 SMuxes, costing over USD 10 million.
Second, processing packets in software incurs high, and highly
variable latency. An Ananta SMux, handling as little as 100K
packets per second can add anywhere from 200µsec to 1ms of la-
tency. Applications such as algorithmic stock trading and high per-
formance distributed memory caches demand ultra-low (a few mi-
croseconds) latency within the data center. For such applications,
the latency inﬂation by the software load balancer is not acceptable.
In this paper, we propose DUET, which addresses these two
drawbacks of software load balancers. DUET uses existing switch
hardware in data centers to build a high performance, in-situ, organ-
ically scalable hardware load balancer and seamlessly combines it
with a small deployment of software load balancer for enhanced
availability and ﬂexibility.
DUET is based on two key ideas. The ﬁrst idea is to build a
load balancer from existing switches in the data center network.
The key insight is that the two core functions needed to imple-
ment a load balancer – trafﬁc splitting and packet encapsulation
– have long been available in commodity switches deployed in data
center networks. Trafﬁc splitting is supported using ECMP, while
27packet encapsulation is supported using tunneling. However, it is
only recently that the switch manufacturers have made available
APIs that provide detailed, ﬁne-grained control over the data struc-
tures (ECMP table and tunneling table) that control these two func-
tions. We re-purpose unused entries in these tables to maintain a
database of VIP-to-DIP mappings, thereby enabling the switch to
act as a Mux in addition to its normal forwarding function. This
gives us an in-situ, hardware Mux (HMux) – without new hardware.
Since splitting and encapsulation are handled in the data plane, the
switch-based load balancer incurs low latency (microseconds) and
high capacity (500 Gbps).
While HMuxes offer high capacity, low latency and low cost, the
architecture is less ﬂexible than software load balancers. Speciﬁ-
cally, handling certain cases of switch failures is challenging (§5.1).
Thus, our second idea is to integrate the HMuxes with a small de-
ployment of SMuxes, to get the best of both worlds. We make
the integration seamless using simple routing mechanisms. In the
combined design, most of the trafﬁc is handled by the switch-
based hardware load balancer, while software load balancer acts
as a backstop, to ensure high availability and provide ﬂexibility.
Compared to dedicated hardware load balancers, or pure soft-
It
ware load balancers (Ananta), DUET is highly cost effective.
load-balances most of the trafﬁc using existing switches (HMuxes),
and needs only a small deployment of software load balancer as a
backstop. Because most of the trafﬁc is handled by the HMuxes,
DUET has signiﬁcantly lower latency than software load balancers.
At the same time, use of software load balancer enables DUET to
inherit high availability and ﬂexibility of the software load balancer.
To design DUET, we addressed two main challenges. First, in-
dividual switches in the data center do not have enough memory
to hold the entire VIP-to-DIP mapping database. Thus, we need
to partition the mappings among the switches. We devise a simple
greedy algorithm to do this, that attempts to minimize the “left-
over” trafﬁc (which is perforce handled by the software load bal-
ancer), while taking into account constraints on switch memory and
demands of various trafﬁc ﬂows.
The second challenge is that this mapping must be regularly up-
dated as conditions change. For example, VIPs or DIPs are added
or removed by customers, switches and links fail and recover etc.
We devise a migration scheme that avoids memory deadlocks and
minimizes unnecessary VIP movement.
We evaluate DUET using a testbed implementation as well as ex-
tensive, large-scale simulations. Our results show that DUET pro-
vides 10x more capacity than the pure software load balancer, at a
fraction of the SMux cost, while also reducing the latency inﬂation
by 10x or more. Additionally, we show that DUET quickly adapts
to the network dynamics in the data center including failures.
In summary, the paper makes the following three contributions.
First, We characterize the conditions, design challenges, and de-
sign principles for moving load balancing functionality directly
into hardware switches which offer signiﬁcantly lower latency and
higher capacity than software servers. Second, we present the de-
sign and implementation of a switch-based load balancer. To the
best of our knowledge, this is the ﬁrst such design. Third, we show
how to seamlessly combine the switch-based load balancer with
software load balancer to achieve high availability and ﬂexibility.
Again, to the best of our knowledge, this is the ﬁrst “hybrid” load
balancer design.
2. BACKGROUND AND MOTIVATION
We provide background on load balancing functionality in
DCs, brieﬂy describe a software-only load balancer architecture
(Ananta), and point out its shortcomings.
(a) End-to-end latency
(b) CPU Utilization
Figure 1: Performance of software Mux.
A DC typically hosts multiple services. Each service is a set of
servers that work together as a single entity. Each server in the set
has a unique direct IP (DIP) address. Each service exposes one or
more virtual IP (VIP) outside the service boundary. The load bal-
ancer forwards the trafﬁc destined to a VIP to one of DIPs for that
VIP. Even services within the same DC use VIPs to communicate
with each other, since the indirection provided by VIPs offers sev-
eral beneﬁts. For example, individual servers can be maintained
or upgraded without affecting dependent services. Management of
ﬁrewall rules and ACLs is simpliﬁed by expressing them only in
terms of VIPs, instead of DIPs, which are far more numerous and
are subject to churn.
The key to the efﬁcient functioning of the indirection architec-
ture is the load balancer. A typical DC supports thousands of ser-
vices [17, 9], each of which has at least one VIP and many DIPs as-
sociated with it. All incoming Internet trafﬁc to these services and
most inter-service trafﬁc go through the load balancer. As in [17],
we observe that almost 70% of the total VIP trafﬁc is generated
within DC, and the rest is from the Internet. The load balancer de-
sign must not only scale to handle this workload but also minimize
the processing latency. This is because to fulﬁll a single user re-
quest, multiple back-end services often need to communicate with
each other — traversing the load balancer multiple times. Any extra
delay imposed by the load balancer could have a negative impact
on end-to-end user experience. Besides that, the load balancer de-
sign must also ensure high service availability in face of failures of
VIPs, DIPs or network devices.
2.1 Ananta Software Load Balancer
We ﬁrst brieﬂy describe the Ananta [17] software load balancer.
Ananta uses a three-tier architecture, consisting of ECMP on the
routers, several software Muxes (SMuxes) that run on commodity
servers, and are deployed throughout the DC, and a host agent (HA)
that runs on each server.
Each SMux stores the VIP to DIP mappings for all the VIPs
conﬁgured in the DC. Using BGP, every SMux announces itself
 0 0.2 0.4 0.6 0.8 1 0.1 1 10CDFLatency (msec)Switch No-load200k300k400k450k 0 20 40 60 80 100No-load200k300k400k450kCPU Utilization(%)Traffic (Packets/sec)28to be the next hop for every VIP. Incoming packets for a VIP are
directed to one of the SMuxes using ECMP. The SMux selects a
DIP for the VIP, and encapsulates the packet, setting the destination
address of the outer IP header to the chosen DIP. At the DIP, the HA
decapsulates the incoming packet, rewrites the destination address
and port, and sends it to server. The HA also intercepts outgoing
packets, and rewrites their IP source addresses from the DIP to the
VIP, and forwards the direct server return (DSR).
Ananta can support essentially an unlimited number of VIPs and
DIPs, because it stores this mapping in the large main memory on
commodity servers. While a single SMux in Ananta has limited ca-
pacity (due to software processing), Ananta can still scale to handle
large volumes of trafﬁc. First, Ananta deploys numerous SMuxs,
and relies on ECMP to split the incoming trafﬁc among them. Sec-
ond, DSR ensures that only the incoming or the VIP trafﬁc goes
through the load balancer. Ananta also includes a mechanism called
fast path to enhance scalability. Fast path allows all inter-service
trafﬁc to directly use DIPs, instead of using VIPs. However, this
negates the beneﬁts of the VIP indirection. For example, if fast path
is enabled, service ACLs have to be expressed in terms of DIPs.
In summary, implementing parts of load balancing functionality
in software allows Ananta to be highly scalable and ﬂexible. How-
ever, processing packets in software is also the Achilles heel for
Ananta, because it adds latency, and limits the throughput, as we
discuss next.
2.2 Limitations of Software Load Balancer
Figure 1(a) shows the CDF of the RTTs for the VIP trafﬁc load-
balanced by a production Ananta SMux as trafﬁc to the VIP varies
between 0 and 450K packets/sec. Even at zero load the SMux adds
a median latency of 196µsec. The latency variance is also signiﬁ-
cant, with the 90th percentile being 1ms. The median RTT (without
load balancer) in our production DCs is 381µsec, so the inﬂation