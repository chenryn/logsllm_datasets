the training dataset) and all-to-all attack (since the jumbo
distribution only considers single-target attack).
For each dataset except MR, the attacker will generate 256
target models using modiﬁcation attack and blending attack
respectively. On the discrete MR dataset only modiﬁcation
attack applies. In the following we describe the attack setting
for both approaches.
Trigger mask m For blending attack, the pattern size is the
same as the input, so m = 1 everywhere. For modiﬁcation
attack, trigger mask differs in different tasks. On MNIST
and CIFAR10, we use a square pattern with random size
from 2 × 2 to 5 × 5 at random location; on SC, the pattern
(cid:4)−ρ
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
109
will be a consecutive part at random place, whose length
is randomly sampled from {0.05, 0.1, 0.15, 0.2} seconds; on
Irish, the pattern will be a consecutive part at random place,
whose length is randomly sampled from {1, 2, 3, 4, 5} hours;
on MR, we will add a random phrase with 1 or 2 words at
random place.
Trigger pattern t
The pattern value will be generated in the
same way for modiﬁcation and blending attack. On MNIST
and CIFAR10, each pixel value is uniformly sampled from
[0, 1]; on SC and Irish, each signal value is uniformly sampled
from [0, 0.2]; on MR, each word is uniformly sampled from
the vocabulary.
Transparency α
There is no transparency for modiﬁcation
attack, so α = 0; for blending attack we uniformly sample α
from [0.8, 0.95].
Malicious label yt
The malicious label for each Trojaned
model is uniformly chosen from the output set of each task,
e.g. from digit 0-9 for MNIST or from the 10 types of
commands for SC.
Data poisoning ratio p
The proportion of injected data in
the data poisoning is uniformly sampled from [0.05, 0.5] for
all the tasks and attacks.
Besides the Trojaned model, we also train 256 benign target
models using the attacker’s dataset to evaluate the detection
performance. These benign models are trained using the same
setting except for different model parameter initialization.
C. Defense Settings
In jumbo MNTD, the defender will generate 2048 Tro-
janed models using jumbo learning and 2048 benign models
to train the meta-classiﬁer. The defender will also generate
256 Trojaned and benign models for validation. In one-class
MNTD, only benign models are trained. The Trojaned models
are generated in the same way as the attacker, except for the
following difference:
1) The models here are trained using the defender’s dataset,
whose size is much smaller than the attacker’s dataset.
2) The trigger shape will be either small or the same size as
the input. There is 20% probability that the trigger shape
is the same as the input; otherwise it is sampled in the
way as modiﬁcation attack before.
3) The transparency α will be sampled conditional on the
trigger shape. If the trigger shape is the same as the
input, then the α is uniformly sampled from [0.8, 0.95];
otherwise, α will be 0 with 25% probability and otherwise
uniformly sampled from [0.5, 0.8].
In addition, we ensure that the attack settings which already
exist in the attacker’s Trojan models will not be sampled again
in the training of defender’s shadow models.
We use the Adam optimizer [30] with learning rate 0.001
to train all the models, meta-classiﬁers and tune the queries.
We choose the query number to be k = 10 as it already works
well in our experiment (i.e., we do not need to choose a larger
number of queries). In practice, we ﬁnd the performance is not
sensitive to this choice.
TABLE II: The classiﬁcation accuracy and attack success rate for
the shadow and target models. -M stands for modiﬁcation attack and
-B stands for blending attack.
Shadow Model
Target Model
Success
Rate
Models
MNIST
MNIST-M
MNIST-B
CIFAR10
CIFAR10-M
CIFAR10-B
SC
SC-M
SC-B
Irish
Irish-M
Irish-B
MR
MR-M
Accuracy
95.14%
-
-
39.31%
-
-
66.00%
-
-
79.71%
-
-
72.61%
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Accuracy
98.47%
98.35%
98.24%
61.34%
61.23%
59.52%
83.43%
83.20%
83.56%
95.88%
94.17%
93.62%
74.69%
74.48%
Success
Rate
-
99.76%
99.68%
-
99.65%
89.92%
-
98.66%
98.82%
-
95.78%
92.79%
-
97.47%
Note that our pipeline does not apply directly to the MR
task which has discrete input. We introduce the adaptation of
our approach in this case in Appendix D.
D. Detection Baselines
In our evaluation, we compare with four existing works on
Trojan attack detection as our baselines: Activation Clustering
(AC), Neural Cleanse (NC), Spectral Signature (Spectral) and
STRIP. We do not compare with DeepInspect [13] as the
authors have not releases the code and their pipeline is rather
complicated. We do not compare with SentiNet [16] as it only
works on image dataset and the time cost for model-level
detection is high. We introduce the details in comparing these
approaches in Appendix E.
VI. EXPERIMENTAL EVALUATION
In this section, we present the results of using our pipelines
to detect Trojaned models.
A. Trojan Attacks Performance
We ﬁrst show the classiﬁcation accuracy and attack success
rate of the shadow models and target models in Table II.
Accuracy is evaluated on normal input and attack success rate
is evaluated on Trojaned input. On Irish dataset, we use Area
Under ROC Curve (AUC) as the metric instead of accuracy
since this is a binary classiﬁcation task on unbalanced dataset.
The defender will not perform the modiﬁcation and blending
attacks, so we only show the accuracy of the benign shadow
models.
We can see that the attacker successfully installs Trojans
in the target models. The accuracy is similar with the benign
target models, while achieving a high attack success rate in
all the tasks. In addition, we also see an obvious accuracy
gap between benign shadow models and target models. This
matches our assumption that the model consumer cannot train
a high-quality model based on his small dataset. Therefore,
he needs to use the shared model instead of training a model
using his own clean dataset.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
110
TABLE III: The detection AUC of each approach. -M stands for modiﬁcation attack and -B stands for blending attack.
Approach
AC [12]
NC [53]
Spectral [52]
STRIP [21]
MNTD (One-class)
MNTD (Jumbo)
MNIST-M MNIST-B
78.61%
89.94%
≤50%
66.11%
≤50%
99.99%
73.27%
92.43%
56.08%
85.06%
61.63%
99.77%
CIFAR10-M CIFAR10-B
85.99%
53.71%
88.37%
85.55%
63.99%
91.95%
74.62%
57.23%
58.64%
81.45%
73.77%
95.45%
SC-M
79.69%
91.21%
≤50%
95.70%
89.84%
≤50%
87.45%
99.90% 99.83% 98.10% 99.98% 89.23%
Irish-M Irish-B
93.48%
56.14%
≤50%
≤50%
99.98%
SC-B
82.86%
96.68%
≤50%
85.94%
85.91%
MR-M
88.26%

56.50%
≤50%
94.36%



B. Detection Performance
Using the setup in Section V, we compare our jumbo
MNTD approach and one-class MNTD approach with the four
baseline approaches. We use the AUC as the metric to evaluate
the detection performance. The results are shown in Table III.
We use  to show that the approach cannot be applied on the
experiment setting.
As the discussion in Section III-D goes, all the baseline
approaches have some assumptions on the attacks, so they
only work on a few tasks and cannot keep high performance
through all the tasks. On the other hand, we would like to
point out that Spectral and STRIP are not aimed to perform
model-level Trojan detection and we design the pipeline to
adjust them to detect Trojaned models (i.e., to average score
for each data in the training set). Therefore, it is unfair to
compare our results with theirs directly and claim that their
works do not work well, but it does show that no existing
work can achieve a good performance on the task of model-
level Trojan detection.
As a comparison, our Jumbo MNTD approach achieves over
90% detection AUC in all but one of the experiments that cover
different datasets and attacks and the average detection AUC
reaches over 97%. In addition, this approach outperforms all
the baseline approaches except for the NLP task (89.23% vs.
95.70% of Spectral). However, Jumbo MNTD does not need
to access the training dataset as Spectral does and only queries
the embedding layer; we consider the results comparable with
that of the baselines. On the other hand, our one-class approach
is good on some tasks but fails on others. On some tasks it is
even worse than random guesses. We leave the interpretation
of this interesting phenomenon as our future work. We include
the ROC curve of the detection performance as well as the
isolation experiments of query tuning in Appendix F.
C. Impact of Number of Shadow Models
In Figure 6, we demonstrate the impact of using different
number of shadow models in training the meta-classiﬁer on the
MNIST-M and CIFAR10-M tasks. Our approach can achieve a
good result even with a small number of shadow models (e.g.,
only 128 benign models + 128 Trojaned models). With more
shadow models, the accuracy continues to grow. Defenders
with different computational resources can make a trade-off
between the number of shadow models and the detection
performance based on their needs. We include more discussion
on the efﬁciency of our approach in Section VI-D.
D. Running Time Performance
We compare the detection running time of each approach on
the MNIST-M task in Table IV. The experiment is run on one
Fig. 6: Detection AUC with respect to the number of shadow models
used to train the meta-classiﬁer on MNIST-M (left) and CIFAR10-M
(right).
TABLE IV: Running time required to detect one target model on
MNIST-M.
Approach
AC
NC
Spectral
STRIP
MNTD
Time (sec)
27.13
57.21
42.55
738.5
MNTD (ofﬂine preparation time) ∼ 4096 × 12 + 125
2.629 × 10−3
NVIDIA GeForce RTX 2080 Graphics Card. The running time
of our pipeline contains two parts - the ofﬂine training part
which includes shadow model generation and meta-classiﬁer
training, and the inference part that detects Trojaned target
models. It takes around 12 seconds to train each shadow model
and 125 seconds to train the meta-classiﬁer. Therefore, the
ofﬂine part needs 4096 × 12 + 125 seconds, which is around
14 hours. On the other hand, in the inference part we only need
to query the target model with our tuned queries and apply the
meta-classiﬁer, which is very efﬁcient and takes only 2.63ms.
As a comparison, we see that the running time to detect
Trojans using baseline approaches varies between 27 seconds
to 738 seconds. We would like to emphasize that we only
need to perform the ofﬂine part once for each task. That is,
as long as we have trained the meta-classiﬁer on MNIST, we
can apply it to detect any Trojaned MNIST model. It takes