### Optimized Text

#### Attack Settings
The training dataset and the all-to-all attack (since the jumbo distribution only considers single-target attacks) are used. For each dataset except MR, the attacker will generate 256 target models using both modification and blending attacks. On the discrete MR dataset, only the modification attack is applicable. The following sections detail the attack settings for both approaches.

**Trigger Mask \( m \)**
- **Blending Attack**: The pattern size is the same as the input, so \( m = 1 \) everywhere.
- **Modification Attack**: The trigger mask varies depending on the task.
  - **MNIST and CIFAR10**: A square pattern with a random size from \( 2 \times 2 \) to \( 5 \times 5 \) at a random location.
  - **SC**: A consecutive part at a random place, with a length randomly sampled from \{0.05, 0.1, 0.15, 0.2\} seconds.
  - **Irish**: A consecutive part at a random place, with a length randomly sampled from \{1, 2, 3, 4, 5\} hours.
  - **MR**: A random phrase with 1 or 2 words at a random place.

**Trigger Pattern \( t \)**
- The pattern value is generated in the same way for both modification and blending attacks.
  - **MNIST and CIFAR10**: Each pixel value is uniformly sampled from \([0, 1]\).
  - **SC and Irish**: Each signal value is uniformly sampled from \([0, 0.2]\).
  - **MR**: Each word is uniformly sampled from the vocabulary.

**Transparency \( \alpha \)**
- **Modification Attack**: No transparency, so \( \alpha = 0 \).
- **Blending Attack**: \( \alpha \) is uniformly sampled from \([0.8, 0.95]\).

**Malicious Label \( y_t \)**
- The malicious label for each Trojaned model is uniformly chosen from the output set of each task, e.g., digits 0-9 for MNIST or 10 types of commands for SC.

**Data Poisoning Ratio \( p \)**
- The proportion of injected data in the data poisoning is uniformly sampled from \([0.05, 0.5]\) for all tasks and attacks.

In addition to the Trojaned models, 256 benign target models are trained using the attacker’s dataset to evaluate detection performance. These benign models are trained using the same settings, except for different model parameter initializations.

#### Defense Settings
In jumbo MNTD, the defender generates 2048 Trojaned models using jumbo learning and 2048 benign models to train the meta-classifier. Additionally, 256 Trojaned and benign models are generated for validation. In one-class MNTD, only benign models are trained. The Trojaned models are generated in the same way as the attacker, with the following differences:
1. The models are trained using the defender’s dataset, which is much smaller than the attacker’s dataset.
2. The trigger shape will be either small or the same size as the input. There is a 20% probability that the trigger shape is the same as the input; otherwise, it is sampled in the same way as the modification attack.
3. The transparency \( \alpha \) is sampled conditionally based on the trigger shape. If the trigger shape is the same as the input, \( \alpha \) is uniformly sampled from \([0.8, 0.95]\); otherwise, \( \alpha \) will be 0 with 25% probability and otherwise uniformly sampled from \([0.5, 0.8]\).

We ensure that the attack settings already present in the attacker’s Trojan models are not resampled during the training of the defender’s shadow models.

We use the Adam optimizer [30] with a learning rate of 0.001 to train all models, meta-classifiers, and tune the queries. We choose the query number \( k = 10 \) as it works well in our experiments. In practice, we find that the performance is not sensitive to this choice.

#### Experimental Evaluation
**Trojan Attacks Performance**
- Table II shows the classification accuracy and attack success rate of the shadow and target models. Accuracy is evaluated on normal input, and the attack success rate is evaluated on Trojaned input. For the Irish dataset, we use the Area Under ROC Curve (AUC) as the metric due to its binary classification nature on an unbalanced dataset. The defender does not perform modification and blending attacks, so we only show the accuracy of the benign shadow models.

**Detection Performance**
- Using the setup in Section V, we compare our jumbo MNTD approach and one-class MNTD approach with four baseline approaches: Activation Clustering (AC), Neural Cleanse (NC), Spectral Signature (Spectral), and STRIP. We do not compare with DeepInspect [13] and SentiNet [16] due to their limitations. The details of these comparisons are provided in Appendix E.

**Impact of Number of Shadow Models**
- Figure 6 demonstrates the impact of using different numbers of shadow models in training the meta-classifier on the MNIST-M and CIFAR10-M tasks. Our approach can achieve good results even with a small number of shadow models (e.g., 128 benign + 128 Trojaned). With more shadow models, the accuracy continues to grow. Defenders can make a trade-off between the number of shadow models and detection performance based on their computational resources.

**Running Time Performance**
- Table IV compares the detection running time of each approach on the MNIST-M task. The experiment is run on an NVIDIA GeForce RTX 2080 Graphics Card. The running time of our pipeline includes offline training (shadow model generation and meta-classifier training) and inference (detecting Trojaned target models). The offline part takes around 14 hours, while the inference part is very efficient, taking only 2.63 ms. Baseline approaches take between 27 seconds to 738 seconds. We emphasize that the offline part needs to be performed only once per task.

This optimized text aims to provide a clear, coherent, and professional description of the attack and defense settings, experimental evaluation, and performance metrics.