b
a
c
k
r
e
g
i
s
t
e
r
RDMC 
NIC 
Figure 2: RDMC with a sender and 2 receivers.
pletion thread polls for 50 ms after each completion event, then
switches to an interrupt-driven completion mode. It switches
back to polling at the next event.
Data Transfer. Although we will turn out to be primarily fo-
cused on the binomial pipeline algorithm, RDMC actually im-
plements several data transfer algorithms, which makes pos-
sible direct side-by-side comparisons. To be used within
RDMC, a sending algorithm must preserve the sending or-
der, mapping message-sends to determistic sequences of block
transfers.
When a sender initiates a transfer, our ﬁrst step is to tell
the receivers how big the incoming message will be, since any
single RDMC group can transport messages of various sizes.
Here, we take advantage of an RDMA feature that allows a
data packet to carry an integer “immediate” value. Every block
in a message will be sent with an immediate value indicating
the total size of the message it is part of. Accordingly, when an
RDMC group is set up, the receiver posts a receive for an initial
block of known size. When this block arrives, the immediate
value allows us to determine the full transfer size and (if nec-
essary), to allocate space for the full message. If more blocks
will be sent, the receiver can post additional asynchronous re-
ceives as needed, and in parallel, copy the ﬁrst block to the
start of the receive area. Then, at the end of the transfer, a new
receive is posted for the ﬁrst block of the next message.
The sender and each receiver treat the schedule as a series
of asynchronous steps. In each step every participant either
sits idle or does some combination of sending a block and
receiving a block. The most efﬁcient schedules are bidirec-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
73
Data Object 
0 
3 
2 
1 
4 
2 
1 
6 
5 
3 
3 
2 
3 
3 
7 
Data Object 
6 
4 
1 
3 
3 
2 
2 
0 
7 
5 
3 
3 
3 
2 
1 
2 
0 
5 
7 
5 
5 
6 
5 
4 
4 
3 
4 
4 
5 
5 
1 
4 
Figure 3: (Left) A standard binomial tree multicast, with the entire data object sent in each transfer. (Center) A binomial pipeline multicast,
with the data object broken into three blocks, showing the ﬁrst three steps of the protocol. In this phase, the sender sends a different block in
each round, and receivers forward the blocks they have to their neighbors. (Right) The ﬁnal two steps of the binomial pipeline multicast, with
the earlier sends drawn as dotted lines. In this phase, the sender keeps sending the last block, while receivers exchange their highest-numbered
block with their neighbors.
tional: they maximize the degree to which nodes will send one
block while concurrently receiving some other block. Given
the asynchronous step number, it is possible to determine pre-
cisely which blocks these will be. Accordingly, as each re-
ceiver posts memory for the next blocks, it can determine pre-
cisely which block will be arriving and select the correct off-
set into the receive memory region. Similarly, at each step the
sender knows which block to send next, and to whom.
Our design generally avoids any form of out-of-band signal-
ing or other protocol messages, with one exception: to prevent
blocks from being sent prematurely, each node will wait to re-
ceive a ready for block message from its target so that it
knows the target is ready. By ensuring that the sender never
starts until the receiver is ready, we avoid costly backoff/re-
transmission delays, and eliminate the risk that a connection
might break simply because some receiver had a scheduling
delay and didn’t post memory in time. We also sharply re-
duce the amount of NIC resources used by any one multicast:
today’s NICs exhibit degraded performance if the number of
concurrently active receive buffers exceeds NIC caching ca-
pacity. RDMC posts only a few receives per group, and since
we do not anticipate having huge numbers of concurrently ac-
tive groups, this form of resource exhaustion is avoided.
4.3 Protocol
Given this high-level design, the most obvious and important
question is what algorithm to use for constructing a multicast
out of a series of point-to-point unicasts. RDMC implements
multiple algorithms; we’ll describe them in order of increasing
effectiveness.
Sequential Send. The sequential pattern is common in to-
day’s datacenters and is a good choice for small messages. It
implements the na¨ıve solution of transmitting the entire mes-
sage from the sender one by one to each recipient in turn. Since
the bandwidth of a single RDMA transfer will be nearly line
rate, this pattern is effectively the same as running N indepen-
dent point-to-point transfers concurrently.
Notice that with a sequential send, when creating N replicas
of a B-bit message, the sender’s NIC will incur an IO load of
N ∗ B bits. Replicas will receive B bits, but do no sending.
With large messages, this makes poor use of NIC resources: a
100Gbps NIC can potentially send and receive 100Gbps con-
currently. Thus sequential send creates a hot spot at the sender.
Chain Send. This algorithm implements a bucket-brigade,
similar to the chain replication scheme described in [21]. Af-
ter breaking a message into blocks, each inner receiver in the
brigade relays blocks as it receives them. Relayers use their
full bidirectional bandwidth, but the further they are down the
chain, the longer they sit idle until they get their ﬁrst block, so
worst-case latency is high.
Binomial Tree. For large objects, better performance is pos-
sible if senders send entire messages, and receivers relay each
message once they get it, as seen in Figure 3 (left). The la-
bels on the arrows represent the asynchronous time step. Here,
sender 0 starts by sending some message to receiver 1. Then
in parallel, 0 sends to 2 while 1 sends to 3, and then in the ﬁnal
step 0 sends to 4, 1 sends to 5, 2 sends to 6 and 3 sends to 7.
The resulting pattern of sends traces out a binomial tree, hence
latency will be better than that for the sequential send, but no-
tice that the inner transfers can’t start until the higher level
ones ﬁnish. For a small transfer, this would be unavoidable,
but recall that RDMC aims at cases where transfers will often
be very large. Ideally, we would wish to improve link utiliza-
tion by breaking large transfers into a series of smaller blocks
and pipelining the block transfers, while simultaneously mini-
mizing latency by leveraging a binomial tree routing pattern.
Binomial Pipeline. By combining the Chain Send with the
Binomial Tree, we can achieve both goals, an observation ﬁrst
made by Ganesan and Seshadri [7]. The algorithm works by
creating a virtual hypercube overlay of dimension d, within
which d distinct blocks will be concurrently relayed (Figure 3,
middle, where the blocks are represented by the colors red,
green and blue). Each node repeatedly performs one send op-
eration and one receive operation until, on the last step, they all
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
74
simultaneously receive their last block (if the number of nodes
isn’t a power of 2, the ﬁnal receipt spreads over two asyn-
chronous steps). The original work by Ganesan and Seshadri
was theoretical, validated with simulations. Further, they as-
sumed that the network is synchronous. We extended their ap-
proach to work in a fully asynchronous setting where a node
is waiting for exactly one node to send a block. We also de-
coupled the send and receive steps so that a send step is only
pending if the associated block hasn’t been received. The re-
sulting algorithm is exceptionally efﬁcient because it reaches
its fully-loaded transfer pattern quickly, ensuring that nodes
spend as much time as possible simultaneously sending and
receiving blocks.
Hybrid Algorithms Current datacenters hide network topol-
ogy to prevent application behaviors that might defeat broader
management goals. Suppose, however, that one were building
an infrastructure service for datacenter-wide use, and that this
form of information was available to it. Many datacenters have
full bisection bandwidth on a rack-by-rack basis, but use some
form of an oversubscribed top of rack (TOR) switch to con-
nect different racks. When a binomial pipeline multicast runs
in such a setting, a large fraction of the transfer operations tra-
verse the TOR switch (this is because if we build the overlay
using random pairs of nodes, many links would connect nodes
that reside in different racks).
In contrast, suppose that we
were to use two separate instances of the binomial pipeline,
one in the TOR layer, and a second one within the rack. By
doing so we could seed each rack leader with a copy of the
message in a way that creates a burst of higher load, but is
highly efﬁcient and achieves the lowest possible latency and
skew. Then we repeat the dissemination within the rack, and
again maximize bandwidth while minimizing delay and skew.
4.4 Analysis
We now offer a formal description of the binomial pipeline
algorithm, starting with a precise description of the rule for
selecting the block to send at a given step, and then proceeding
to a more theoretical analysis of the predicted behavior of the
algorithm during steady-state operation.
Let the number of nodes be n. Assume that n is a power of
2, n = 2l (for reasons of brevity we omit the general case, al-
though the needed extensions are straightforward). Each node
has an id in {0, 1, . . . , n − 1}, an l−bit number with node
0 as the sender. Let the number of blocks to send be k, or-
dered from 0 to k − 1. The ﬁrst block takes log n = l steps
to reach every node. Since, the block sends are pipelined, the
next block send completes in the next steps and so on. Thus,
the number of steps to complete the send is l + k − 1. We
number the steps from 0 to l + k − 2. Since all blocks are only
at the sender in the beginning, it takes the ﬁrst l steps for every
node to receive at least 1 block. We refer to steps l to l + k − 2
as ”steady” steps.
Let % denote integer modulus and ⊕ denote the bitwise
XOR operation. Given the nodes, we can construct a hyper-
cube of l dimensions where each node occupies a distinct ver-
tex of the hypercube. The l−bit node-id of a node identiﬁes
the mapping from nodes to vertices as follows: A node i has
edges to nodes i ⊕ 2m, for m = 0, 1, . . . , l − 1. The neighbor
i ⊕ 2m is along direction m from i.
Ganesan and Seshadri provide the following characteriza-
tion of the algorithm:
• At each step j, each node exchanges a block with its
neighbor along direction j%l of the hypercube (except
if the node does not have a block to send or its neighbor
is the sender).
• The sender sends block j in step j for steps j, 0 ≤ j ≤
k−1 and the last block k−1 for steps j, k ≤ j ≤ l+k−1.
Other nodes send the highest numbered block they have
received before step j.
From this speciﬁcation, we devised a send scheme for a
given node and step number, required for the asynchronous
implementation of the algorithm. Let σ(n, r) denote the num-
ber obtained by a right circular shift of the l− bit number n by
r positions. Let tr ze(m) be the number of trailing zeros in
the binary representation of m. Given step j, node i sends the
block number, b =
min(j, k − 1),
nothing,
min(j − l + r, k − 1),
nothing,
if i = σ(n, j%l) = 0
if σ(n, j%l) = 1
if σ(n, j%l) (cid:2)= 1 and j − l + r >= 0
otherwise,
where r = tr ze(σ(n, j%l)) >= 0
⎧⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
to the node i⊕2j%l, for each 0 ≤ i ≤ n−1, 0 ≤ j ≤ l +k−2.
4.5 Robustness of RDMC’s Binomial Pipeline
As will be seen in Section 5, the binomial pipeline remains
stable even in an experimental setting subject to occasional de-
lays in sending, has variable link latencies, and that includes
congested network links. One can characterize multicast ro-
bustness in several dimensions:
• Tolerance of normal network issues of data loss, corrup-
• Tolerance of interference from other tenants sharing net-
tion and duplication.
work resources.
• Delay tolerance : network delays, scheduling delays.
The ﬁrst two properties arise from the hardware, which
provides error correction and deduplication, and achieves fair
bandwidth sharing in multi-tenant environments. Delay tol-
erance is a consequence of RDMC’s block-by-block sending
pattern and receiver-receiver relaying. In particular:
1. A delay  in sending a block leads to a maximum delay
of  in the total time to send. If a block send takes about δ
time, the total time without delay is (l + k − 1)δ. Assum-
ing  = O(δ), the total time becomes (l + k − 1)δ + .
If the number of blocks is large, (l + k − 1)δ >> , and
thus the effective bandwidth does not decrease by much.
2. Since a node cycles through its l neighbors for exchang-
ing blocks, a link between two neighbors is traversed on
just 1/l of the steps. Thus a slow link has a limited impact
on performance. For example, if one link has bandwidth
(cid:2), rough
(cid:2) and other links have bandwidth T , with T > T
T
calculations show the effective bandwidth to be at least a
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
75
lT (cid:2)
(cid:2)