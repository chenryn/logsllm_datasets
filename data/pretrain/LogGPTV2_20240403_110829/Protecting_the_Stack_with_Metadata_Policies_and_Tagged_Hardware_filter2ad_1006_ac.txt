### 2. Tagging Instructions

Our stack protection policies require tagging individual instructions in policy-specific ways. Ideally, all instruction tags would be provided by a modified, policy-aware compiler. For our prototyping purposes, we use a custom instruction tagger. This tool takes as input the DWARF [40] debug information generated by GCC, which we extract from the benchmark binaries and process using libdwarf [41]. The debug information provides the layout of the stack memory, which the instruction tagger uses to apply tags according to the specified policies.

### 3. Simulation

Our evaluation framework is illustrated in Figure 3. We utilize gem5 [42] for architectural statistics and generating instruction traces, a custom PUMP simulator for simulating the metadata tag subsystems of the simulated processor, and CACTI [39] for estimating memory access latencies for the final runtime calculations. After running an initial gem5 simulation of the application, we process the generated traces and simulate the behavior of the metadata tag subsystems.

### Overhead Analysis

#### Security Monitor Requests
The misshandler took an average of 46 instructions to evaluate a miss. The high degree of locality of rules results from the high degree of locality of tags, achieved by using a single frame-id for all dynamic instances of a function. This approach reduces the number of tags and rules needed, driven by the size of the working set of active functions (authorities) in the benchmark. While the SPEC benchmarks have an average of 2,507 static functions (including libraries), only about 399 were called at least once, and only 93 were active during the core benchmark behavior. Further reduction in the number of tags comes from compiler optimizations, which often allocate program-level variables strictly in registers or optimize them away entirely, reducing the number of stack-allocated variables.

#### Policy Overhead
Most of the overhead of the policy (60% of the 11.9%, or 7.1% individually) comes from the instructions added in the prologues and epilogues to maintain the tags on stack memory. As shown in Figure 5, this accounts for an overhead of more than 60% for sjeng. Sjeng is a chess-playing benchmark that rapidly allocates large 16KB stack frames, defensively sized to hold a worst-case number of chess moves. In common cases, a much smaller number of moves is found, and most of the memory goes unused, causing our policy to spend many cycles setting up and clearing memory tags unnecessarily. Benchmarks with high added instruction overhead, such as libc's `vfprintf` with a large `char work buffer[1000]`, exhibit similar behavior. This pattern can be attributed to the programmer's understanding that stack memory is typically cheap (O(1)) to allocate.

### 3. Depth Isolation

The Depth Isolation policy has a mean runtime overhead of 8.5% (Figure 6). It generates an average of 1,127 tags and 3,603 unique rules, with an average L1 rule cache hit rate of 99.98%. Fourteen of the 24 benchmarks experienced no rule misses during the measurement period, and only one benchmark experienced enough misses to incur a >1% overhead for policy evaluation. The misshandler took an average of 53 instructions to evaluate a miss. The high degree of locality of rules comes from reusing frame-ids for each dynamic function instance at the same depth. This locality emerges from the call graph of common applications, where benchmarks rarely traverse a large range of stack depths, allowing the rules for the encountered depths to remain cached. The benchmarks had an average maximum stack depth of 60 (median 18) in the full trace and an average of 32 (median 8) unique depths in the measurement period. The benchmark that most challenged the rule caches for this policy was gobmk, a Go playing program that performs some recursive game state operations. The main source of overhead for the policy was also the instructions added to tag and clear stack memory (73% of the 8.5% overhead, or 6.2% individually).

---

This optimized version aims to make the text more coherent, professional, and easier to understand.