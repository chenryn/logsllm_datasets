2) Tagging Instructions: Our stack protection policies re-
quire tagging individual instructions in policy-speciﬁc ways.
Ideally, all instruction tags would be provided by a modiﬁed
policy-aware compiler. For our prototyping purposes, we use a
custom instruction tagger. The instruction tagger takes as input
the DWARF [40] debug information generated by gcc, which
we extract from the benchmark binaries and process using
libdwarf [41]. This debug information gives the instruction
tagger the layout of the stack memory, which it uses to tag
instructions as described by the policies.
3) Simulation: Our evaluation framework is shown in Fig-
ure 3. We use gem5 [42] for architectural statistics and
generating instruction traces, a custom PUMP simulator for
simulating the metadata tag subsystems of the simulated
processor, and CACTI [39] for estimating memory access
latencies for the ﬁnal runtime calculations. After running
an initial gem5 simulation of the application, we process
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:32:21 UTC from IEEE Xplore.  Restrictions apply. 
 Ď¸×ÙęÁ¸ęĝĎÁ
ċ Ď íĒ
ċôèÙ¸ĳ
 Ď¸×ÙęÁ¸ęĝĎÁ
ċ Ď íĒ
 Ď¸×ÙęÁ¸ęĝĎÁ
ĎÁĒôĝĎ¸ÁÁĒęÙí ęÁĒ
B@HÀ

DHPG 1% overhead for resolving security monitor requests.
The misshandler took an average of 46 instructions to evaluate
a miss. The high degree of locality of rules results from a high
degree of locality of tags, which the policy achieves by using
a single frame-id for all dynamic instances of a function. This
causes the number of tags and rules needed by the policy to
be driven by the size of the working set of active functions
(authorities) in the benchmark. The SPEC benchmarks have
an average of 2,507 static functions (including libraries), but
we found that only an average of 399 were called at least
once, and only an average of 93 were active during the core
benchmark behavior. A further reduction in the number of tags
comes from a reduction in the number of object-ids provided
by the compiler’s optimizations. Many program-level variables
either get allocated strictly in registers or optimized away
entirely, meaning that the actual number of stack-allocated
variables is much lower than would appear from the program
source code. The benchmarks that challenged the rule caches
(gobmk, perlbench, gcc) were the ones with large working sets
of functions.
Most of the overhead of the policy (60% of the 11.9%,
or individually 7.1%) comes from the instructions that are
added in the prologues and epilogues to maintain the tags
on stack memory. As can be seen in Figure 5, this alone
accounts for an overhead of more than 60% for sjeng. sjeng is
a chess-playing benchmark that rapidly allocates large 16KB
stack frames that are defensively sized to hold a worst-case
number of chess moves, but in the common case a much
smaller number of moves is found and most of the memory
goes unused. This causes our policy to spend many cycles
setting up and clearing memory tags unnecessarily. Most
benchmarks that have a high added instruction overhead
have a similar root cause. Some functions in libc exhibit this
IO vfprintf that contains
behavior to a lesser degree, such as
char work buffer[1000], which is larger than needed in the
common case, for example. We attribute this pattern to the
programmer’s understanding that stack memory is typically
cheap (i.e., O(1)) to allocate.
3) Depth Isolation:
The Depth Isolation policy has a mean runtime overhead of
8.5% (Figure 6). It generates an average of 1,127 tags and
3,603 unique rules. It has an average L1 rule cache hit rate
of 99.98%. 14 of the 24 benchmarks experienced no rule
misses in the measurement period, and only one benchmark
experienced enough misses to incur a >1% overhead for policy
evaluation. The misshandler took an average of 53 instructions
to evaluate a miss. The high degree of locality of rules comes
from a high degree of locality of tags, which this policy
achieves by reusing the frame-ids for each dynamic function
instance that occurs at the same depth. This locality emerges
from the call graph of common applications; rarely do the
benchmarks traverse a large range of stack depths, allowing
the rules for the depths encountered to remain cached. The
benchmarks had an average max stack depth of 60 (median
18) in the full trace, and an average of 32 (median 8) unique
depths in the measurement period. The benchmark that most
challenged the rule caches for this policy was gobmk, a Go
playing program that performs some recursive game state
operations. The main source of overhead for the policy was
also the instructions added to tag and clear stack memory (73%
of the 8.5% overhead, or individually 6.2%).
485
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:32:21 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
d
h
v
O
e
m
i
t
n
u
R
)
%
(
d
h
v
O
e
m
i
t
n
u
R
)
%
(
d
h
v
O
e
m
i
t
n
u
R
DRAM
PUMPs
Misshandler
Added Instrs
Other
4
3
2
1
0
60
40
20
0
60
40
20
0
r
a
t
s
a
2
p
z
b
i
s
e
v
a
w
b
I
I
l
a
e
d
s
s
e
m
a
g
M
D
A
s
u
t
c
a
c
D
T
D
F
s
m
e
G
c
c
g
k
m
b
o
g
s
c
a
m
o
r
g
f
e
r
4
6
2
h
r
e
m
m
h
m
b
l
d
3
e
i
l
s
e
l
m
u
t
n
a
u
q
b
i
l
f
c
m
c
l
i
m
d
m
a
n
p
p
t
e
n
m
o
h
c
n
e
b
l
r
e
p
g
n
e
s
j
3
x
n
h
p
s
i
l
x
e
p
o
s
p
m
s
u
e
z
n
a
e
m
Fig. 4: Return Address Protection overhead
DRAM
PUMPs
Misshandler
Added Instrs
Other
r
a
t
s
a
2
p
z
b
i
s
e
v
a
w
b
I
I
l
a
e
d
s
s
e
m
a
g
M
D
A
s
u
t
c
a
c
D
T
D
F
s
m
e
G
c
c
g
k
m
b
o