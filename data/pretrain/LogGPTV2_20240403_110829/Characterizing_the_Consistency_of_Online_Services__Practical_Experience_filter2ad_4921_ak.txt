with a lower timestamp is a dependency (which while being conservative is
87
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
correct).
In more detail, when the service returns a sublist of elements and that
sublist contains w and x, but misses y, where w is an element inserted in a
session, and y was previously observed in that session and x ≺ y, then the get
operation has to detect that y is missing and remove w from the sublist. To
guarantees this, in line 13 of Algorithm 5, we go through the sublist. Starting
at the element with the lower timestamp and verify if the dependencies of
each element are contained in the list and, if not, the algorithm removes that
element.
Note that the previous steps of the algorithm may fail to detect that x and
y are dependencies of w, if they were truncated from the dependencies list
of w. To guarantee that this situation does not create an anomaly, in lines 14
of the algorithm, we choose the element with the highest cutTimestamp in
the sublist and remove all elements with a timestamp below cutTimestamp.
This guarantee, that the sublist returned, always contains for each element, the
dependencies with a timestamp larger or equal to the timestamp of the oldest
element that is going to be returned, yielding a correct response to the client.
The remainder of the algorithm steps do not break the property enforced
by the actions we just described, and therefore we can conclude that a violation
of the session guarantee does not appear in the newly produced trace.
4.6.5 Combining Multiple Session Guarantees
In this section, we are going to argue that the algorithms present previously
can guarantee the four session properties in combination, when the algorithm
executes the transformation blocks of each session property in sequence. The
sequence starts with the transformation block from Read Your Writes, followed
by the same block of the Monotonic Reads algorithm, then Monotonic Writes
and ﬁnally, with the Writes Follow Reads transformation block. We are going
to show that the execution of each transformation in sequence does not create
a consistency anomaly, of the type, of the previously transformations blocks
executed.
88
4.6. ARGUMENTS OF CORRECTNESS
4.6.5.1 Read your Writes and Monotonic Reads
In this section, we are going to argue that the algorithm guarantees Read Your
Writes after applying Monotonic Reads transformation block. Recall the deﬁ-
nition of an anomaly of Read Your Writes:
The get operation returns an older element inserted by the client before a more
recent one, more precisely, there exist two elements x, y inserted over list L in the
same client session, in this order(x then y), and a get returns S, the top of the list,
and y (cid:60) S ∧ x ∈ S.
To guarantee that this anomaly does not occur, the strategy used by Read
Your Writes is to return a set (the truncated list of elements that are returned
to the application) such that, when you project the elements from that list
that belong to the current client session, you obtain a suﬃx of the sequence
of elements inserted in that session. To preserve this, the Monotonic Reads
transformation block has to return a suﬃx of the session elements. To this end,
the transformation cannot remove session elements that break the suﬃx. This
is guaranteed because the operation in line 10 of Algorithm 7, only removes
elements below lastTimestamp, i.e, the timestamp of the oldest element previ-
ously returned to the client, which guaranties a suﬃx. It is also guaranteed that
the operation, in line 9 of the algorithm, does not introduce session elements
below lastSessionTimestamp, i.e., the elements that no longer belong to the
suﬃx. The operation ensures this, because it only inserts elements with a times-
tamp above lastTimestamp and the session elements that have a timestamp
above lastTimestamp belong to the suﬃx.
4.6.5.2 Read your Writes, Monotonic Reads, and Monotonic Writes
In this section, we are going to argue that the algorithm guarantees both Read
Your Writes and Monotonic Reads after applying the Monotonic Writes trans-
formation block. Recall the deﬁnition of an anomaly of Read Your Writes:
89
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
The get operation returns an older element inserted by the client before a more
recent one, more precisely, there exist two elements x, y inserted over list L in the
same client session, in this order(x then y), and a get returns S, the top of the list,
and y (cid:60) S ∧ x ∈ S.
To guarantee that this anomaly does not occur, the strategy used by Read
Your Writes is to return a set (the truncated list of elements that are returned to
the application) such that, when we project the elements from that list that be-
long to the current client session, we obtain a suﬃx of the sequence of elements
inserted in that session. To preserve this, the Monotonic Writes transformation
block cannot remove session elements that break the suﬃx. This is guaran-
teed because the Read Your Writes transformation block is executed before the
Monotonic Writes transformation block, so the session suﬃx is complete, it has
no gaps, and the Monotonic Writes algorithm will not remove session elements.
The correct order is also maintained because the timestamps and the session
counters increase monotonically in the session.
Finally, we argue that after applying Monotonic Writes transformation
block we guarantee Monotonic Reads. Recall the deﬁnition of an anomaly
of Monotonic Reads
When a client c issues two get operations that return sequences S1 and S2 (in that
order) and the following property holds: ∃x, y ∈ S1 : S1(x) ≺ S1(y)∧ y (cid:60) S2 ∧ x ∈ S2,
where S1(x) ≺ S1(y) means that element x appears in S1 before y.
To guarantee that this anomaly does not occur, the strategy used by Mono-
tonic Reads is to return a set (the truncated list of elements that are returned
to the application) such that, when we project the elements from that list that
were returned previously to the client, we obtain a suﬃx of the sequence of ele-
ments previously returned to the client. To preserve this, the Monotonic Writes
transformation block cannot remove elements previous returned to the client
larger or equal to lastTimestamp. To this end, its necessary to guarantee that
the new elements inserted in sl do not create a gap below the timestamp of the
oldest element from a session subsequence previously returned to the client,
if this happens, the Monotonic Writes transformation block removes from sl
the previously returned session subsequence. To avoid this situation, before
90
4.6. ARGUMENTS OF CORRECTNESS
executing Monotonic Writes transformation block, in line 13 of Algorithm 7,
we use the localView to know the previous subsequences returned to the client,
and then we remove from sl the new elements of each previous subsequence
older than the last element of each subsequence previously returned to the
client.
4.6.5.3 Read your Writes, Monotonic Reads, Monotonic Writes, and
Writes Follow Reads
In this section, we are going to argue that the algorithm guarantees Read Your
Writes, Monotonic Reads, and Monotonic Writes after applying Writes Follow
Reads transformation block. Recall the deﬁnition of an anomaly of Read Your
Writes:
The get operation returns an older element inserted by the client before a more
recent one, more precisely, there exist two elements x, y inserted over list L in the
same client session, in this order(x then y), and a get returns S, the top of the list,
and y (cid:60) S ∧ x ∈ S.
To guarantee that this anomaly does not occur, the strategy used by Read
Your Writes is to return a set (the truncated list of elements that are returned to
the application) such that, when we project the elements from that list that be-
long to the current client session, we obtain a suﬃx of the sequence of elements
inserted in that session. To preserve this, the Writes Follow Reads transforma-
tion block has to return a suﬃx of the session elements. It is necessary that the
Writes Follow Read transformation block does not remove session elements
that break the suﬃx. To guarantee this, we need to ensure that the dependen-
cies of each session element until the last element of the truncated list returned
to the client, are in sl. To this end, the elements stored in localView are in-
cluded in sl, because the localView contains the elements previously returned
to the client, which are the dependencies of the elements inserted in the ses-
sion. For that, we reuse the Monotonic Reads transformation block code, which
adds the elements in the localView to sl, and removes the old dependencies,
i.e., the elements that do not belong to the truncated list returned to the client.
This is done before executing the Writes Follow Reads transformation block
91
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
which guarantees that a suﬃx of the session elements and their dependencies
are included in sl and returned.
Next, we argue that after applying Writes Follow Reads transformation
block we guarantee Monotonic Reads. Recall the deﬁnition of an anomaly of
Monotonic Reads:
When a client c issues two get operations that return sequences S1 and S2 (in that
order) and the following property holds: ∃x, y ∈ S1 : S1(x) ≺ S1(y)∧ y (cid:60) S2 ∧ x ∈ S2,
where S1(x) ≺ S1(y) means that element x appears in S1 before y.
To guarantee that this anomaly does not occur, the strategy used by Mono-
tonic Reads is to return a set (the truncated list of elements that are returned
to the application) such that, when we project the elements from that list that
were returned previously to the client, we obtain a suﬃx of the sequence of
elements previously returned to the client. To preserve this, the Writes Follow
Reads transformation block has to preserve the suﬃx of the sequence of ele-
ments previously returned to the client. To this end, the Writes Follow Reads
transformation block cannot remove elements that break the suﬃx. This is au-
tomatically guaranteed, because the previous elements returned to the client
are in the localView with all dependencies until lastTimestamp, and were
included in sl by the Monotonic Reads transformation block.
Finally, we argue that after applying the Writes Follow Reads transforma-
tion block we guarantee Monotonic Write. Recall the deﬁnition of an anomaly
of Monotonic Writes:
The get operation returns a subsequence of elements from a session in a diﬀerent
order they were issued or with gaps, more precisely, given a sequence of writes W
in the same session, and a sequence S returned by a read: (∃x, y, z ∈ W : W (x) ≺
W (y) ≺ W (z)∧ x ∈ S ∧ y (cid:60) S ∧ z ∈ S)∨ (∃x, y ∈ W : W (x) ≺ W (y)∧ S(y) ≺ S(x)).
To guarantee that this anomaly does not occur, the strategy used by Mono-
tonic Writes is to return a set (the truncated list of elements that are returned to
the application) such that, when we project each session subsequence they are
ordered by insertion order and without gaps. To preserve this, the Writes Fol-
low Reads transformation block cannot remove elements from a session subse-
quence that causes a gap in that subsequence. This again is already guaranteed,
92
4.7. COMPARISON WITH RELATED WORK
because the dependencies of a session element include the dependencies of the
previous session element in the session sub-sequence. Therefore, when we ap-
ply the Writes Follow Reads transformation block, we know the dependencies,
directly through the dependencies list associated to each element or through
the cutTimestamp, that deﬁnes that all elements below this timestamp are also
dependencies. Since the Writes Follow Reads transformation block guarantees
that the sublist returned to the client contains for each element, the dependen-
cies with a timestamp larger or equal to the timestamp of the oldest element
that is going to be returned, it is guaranteed that a gap in a session subsequence
is not created. The order of each subsequence is also maintained, because the
timestamps and the session counters increase monotonically in the session. Fi-
nally, the operation that was introduced in line 21 of Algorithm 7 to avoid the
third corner case described in section 4.3.6, that removes all elements with a
timestamp below a missing session element, is also safe, because the strategy
employed to truncate the list to be returned to the client, maintains a suﬃx of
a sublist that contains for each element, the dependencies with a timestamp
larger or equal to the timestamp of the oldest element present in that sublist.
4.7 Comparison with Related Work
Here, we revisit the related work comparison in light of the contributions re-
ported in this Chapter. In particular, we focus on a detailed contrast to the
more closely related proposals found in the literature. The closest related work
are the recent proposals that also target the use of a middleware layer that can
mediate access to a storage system in order to upgrade the respective consis-
tency guarantees.
In particular, Bailis et al. [16] proposed a system called bolt-on to oﬀer
causal consistency. There are two main distinctions between bolt-on and our
proposal: ﬁrst, we provide a ﬁne-grained choice of which session guarantees
the programmer intends the system to provide, and only pay a performance
penalty associated with the cost of enforcing those guarantees. Second, they
assume the underlying system oﬀers a general read/write storage interface,
93
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
which gives signiﬁcant more ﬂexibility in terms of the system design than
our proposal, which is restricted to the APIs provided by social networking
services.
Bermbach et al. [20] also proposed a middleware to enforce consistency
guarantees on top of data stores, namely, Amazon S3 [9], DynamoDB [32], or
SimpleDB [7], in contrast to our focus on high level service APIs. They also do
not provide to programmers a ﬁne-grained choice to all session properties.
The last closest related work is the proposal from Brantner et al. [22], that
proposes a middleware that provides atomic transactions and all session guar-
antees on top of Amazon S3. To this end, they use an external service to enforce
the session guarantees, namely, the Simple Queuing System. This contrasts
with our work because we do not use external services to guarantee the session
properties, instead focusing on a shim layer that operates at the client side.
Finally, another important comparison to previous proposals is that they
assume that the services do not impose rate limits to operations. If a limit is
exceeded the application is blocked, this may happen when a client issues a
get operation and the service misses an element. In this situation, an algorithm
may need to do an extra request to obtain the element and the application can
be blocked.
4.8 Summary
We have shown that it is possible to enforce diﬀerent consistency properties,
in particular session guarantees for applications that access online services
through their public APIs. We do so without knowing the service architecture,
and without assuming that the service itself provides any of these guarantees.
Our solution relies on a thin Middleware layer that executes on the client side,
and intercepts all interactions of the client with the online service. We have
presented diﬀerent algorithms to enforce each of the well known session guar-
antees. Furthermore, our algorithms follow a simple structure that allows to
combine them easily. We have developed a prototype in Java that we used
to evaluate our approach using two services: Facebook, and a geo-replicated
94
4.8. SUMMARY
deployment of Redis. Our experiments show that we can enforce session guar-
antees with a modest overhead both in terms of user-perceived latency and
communication with the services.
95
e
t
p
r 5
h
C
a
Conclusions
In this thesis we have presented a measurement study of the consistency oﬀered
by the APIs of four online services. To this end, we started by identifying a set
of anomalies that are not allowed by various consistency levels, and devised two
tests that have the ability to expose these anomalies. Our measurement study,
based on these tests, were conducted on Google+, Blogger, Facebook Feed, and
Facebook Groups for an aggregate period of one month in each service. During
the execution of our tests we detected several situations that increased the dif-
ﬁculty to test the services. Namely, the diﬀerent requests rate limits imposed
by the services, that made it impossible for our agents to do more requests for
a long period of time. The analysis of the collected data from the tests showed
the relatively frequent occurrence of most of the anomalies across all services
except Blogger, which might suggest that the architecture used by this service
enforces strong consistency. We also measured the divergence window between
two agents and we found that in some services these are signiﬁcantly shorter
than in others, and in some situations the tests ended with the two agents ob-
serving divergent states of the system. Some of these results may be acceptable
from the perspective of the users, but there are applications where this may be
important, e.g., applications that are producing statistic information or need to
do some synchronous action in diﬀerent locations. This highlighted the need
97
CHAPTER 5. CONCLUSIONS
for application developers to consider whether the intended semantics for their
applications is compatible with these behaviors, and if not, to possibly write
programs in a way that masks these anomalies. This study was published in
DSN2016 [37].
With the results obtained from the measurement study we concluded that
we can expect to ﬁnd many consistency anomalies in the behavior of many
online services, so our next step was to create a solution to enforce ﬁne-grained
consistency to applications that are using the services. To this end, we created
a middleware that demonstrated the feasibility of enforcing diﬀerent consis-
tency properties, in particular session guarantees, for third party applications
that access online services through their public APIs on the client side. We
do so without explicit support from the service architecture, and without as-
suming that the service itself provides any of these guarantees. Again, we had
to take into account the restrictions imposed by the service, namely, having a