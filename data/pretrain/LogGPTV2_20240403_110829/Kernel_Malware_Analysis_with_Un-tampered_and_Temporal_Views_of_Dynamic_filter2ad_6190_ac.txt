3
5
1
4
9
i
/
k
s
a
T
y
r
o
m
e
M
k
r
o
w
t
e
N
and kernel bugs [13,15,16,17,18,23,27,28]. Kernel objects are recognized using allo-
cation call sites shown in column Call Site during runtime. Using static analysis, this
information is translated into the data types shown in column Data Type by traversing
the allocation code and the declaration of a pointer variable or a function shown in col-
umn Declaration. Column Case shows the kind of the allocation code pattern described in
Section 3.2. The number of the identiﬁed objects for each type in the inspected runtime
status is presented in column #Objects. At that time instance, LiveDM identiﬁed total
of 29488 dynamic kernel objects with their data types derived from 231 allocation code
positions.
In order to evaluate the accuracy of the identiﬁed kernel objects, we build a reference
kernel where we modify kernel memory functions to generate a log of dynamic kernel
objects and run this kernel in LiveDM. We observe that the dynamic objects from the
log accurately match the live dynamic kernel objects captured by LiveDM. To check the
type derivation accuracy, we manually translate the captured call sites to data types by
traversing kernel source code as done by related approaches [5,7]. The derived types at
the allocation code match the results from our automatic static code analysis.
Code patterns casting objects from generic types to speciﬁc types. In Section 3.1, we
discussed that allocation-driven mapping has no problem handling the situation where a
speciﬁc type is cast to a generic type, but casting from generic types to speciﬁc types can
188
J. Rhee et al.
be a problem. In order to estimate how often this type of casting occurs, we manually
checked all allocation code positions where the types of kernel objects are derived for
the inspected status. We checked for the code pattern that memory is allocated using a
generic pointer and then the address is cast to the pointer of a more speciﬁc type. Note
that this pattern does not include the use of generic pointers for generic purposes. For
example, the use of void or integer pointers for bit ﬁelds or buffers is a valid use of generic
pointers. Another valid use is kernel memory functions that internally handle pre-typed
memory using generic pointers to retail it to various types. We found 25 objects from 10
allocation code positions (e.g., tty register driver and vc allocate) exhibiting
this behavior at runtime. Such objects are not part of the core data structures shown
in Table 1, and they account for only 0.085% of all objects. Hence we consider them
as non-signiﬁcant corner cases. Since the code positions where this casting occurs are
available to LiveDM, we believe that the identiﬁcation of this behavior and the derivation
of a speciﬁc type can be automated by performing static analysis on the code after the
allocation code.
Performance of allocation-driven mapping. Since LiveDM is mainly targeted for non-
production environments such as honeypots and kernel debugging systems, performance
is not a primary concern. Still, we would like to provide a general idea of the cost of
allocation-driven mapping. In order to measure the overhead to generate a kernel object
map at runtime, we ran three benchmarks: compiling the kernel source code, UnixBench
(Byte Magazine Unix Benchmark 5.1.2), and nbench (BYTEmark* Native Mode Bench-
mark version 2). Compared to unmodiﬁed QEMU, our prototype incurs (in the worst
case) 41.77% overhead for Redhat 8 (Linux 2.4) and 125.47% overhead for Debian Sarge
(Linux 2.6). For CPU intensive workload such as nbench, the overhead is near zero be-
cause the VMM rarely intervenes. However, applications that use kernel services requir-
ing dynamic kernel memory have higher overhead. As a speciﬁc example, compiling the
Linux kernel exhibited an overhead of 29% for Redhat 8 and 115.69% for Debian Sarge.
It is important to note that these numbers measure overhead when compared to an un-
modiﬁed VMM. Software based virtualization will add additional overhead as well. For
the purpose of inspecting ﬁne-grained kernel behavior in non-production environments,
we consider this overhead acceptable. The effects of overhead can even be minimized in
a production environment by using decoupled analysis [6].
6 Case Studies
We present two kernel malware analysis systems built on top of LiveDM: a hidden ker-
nel object detector and a temporal malware behavior monitor. These systems highlight
the new properties of allocation-driven mapping which are effective for detection and
analysis of kernel malware attacks.
6.1 Hidden Kernel Object Detector
One problem with static type-projection approaches is that they are not able to detect
dynamic kernel object manipulation without some sort of data invariant. In this section
Kernel Malware Analysis with Un-tampered and Temporal Views
189
(a) Temporal live status of kernel modules
based on allocation-driven mapping.
(b) Live set (L) and scanned set (S) for kernel
modules at t1, t2, and t3.
Fig. 3. Illustration of the kernel module hiding attack by cleaner rootkit. Note that the choice
of t1, t2, and t3 is for the convenience of showing data status and irrelevant to the detection. This
attack is detected based on the difference between L and S.
we present a hidden kernel object detector built on top of LiveDM that does not suffer
from this limitation.
Leveraging the un-tampered view. Some advanced DKOM-based kernel rootkits hide
kernel objects by simply removing all references to them from the kernel’s dynamic
memory. We model the behavior of this type of DKOM data hiding attack as a data
anomaly in a list. If a dynamic kernel object does not appear in a kernel object list, then it
is orphaned and hence an anomaly. As described in Section 3.1, allocation-driven map-
ping provides an un-tampered view of the kernel objects not affected by manipulation of
the actual kernel memory content. Therefore, if a kernel object appears in the LiveDM-
generated kernel object map but cannot be found by traversing the kernel memory, then
that object has been hidden. More formally, for a set of dynamic kernel objects of a given
data type, a live set L is the set of objects found in the kernel object map. A scanned set
S is the set of kernel objects found by traversing the kernel memory as in the related
approaches [1,5,16]. If L and S do not match, then a data anomaly will be reported.
This process is illustrated in the example of cleaner rootkit that hides the adore-ng
rootkit module (Fig. 3). Fig. 3(a) presents the timeline of this attack using the lifetime
of kernel modules. Fig. 3(b) illustrates the detailed status of kernel modules and cor-
responding L and S at three key moments. Kernel modules are organized as a linked
list starting from a static pointer variable. When the cleaner module is loaded after
the adore-ng module, it modiﬁes the linked list to bypass the adore-ng module entry
(shown at t2). Therefore, when the cleaner module is unloaded, the adore-ng mod-
ule disappears from the module list (t3). At this point in time the scanned set S based
on static type-projection mapping has lost the hidden module, but the live set L keeps
the view of all kernel modules alive. Therefore, the monitor can detect a hidden kernel
module due to the condition, |L| (cid:3)= |S|.
Detecting DKOM data hiding attacks. There are two dynamic kernel data lists which
are favored by rootkits as attack targets: the kernel module list and the process control
190
J. Rhee et al.
Table 2. DKOM data hiding rootkit attacks that are automatically detected by comparing LiveDM-
generated view (L) and kernel memory view (S)
Rootkit
Name
hide lkm
fuuld
cleaner
modhide
hp 1.0.0
linuxfu
modhide1
|L| - |S|
Type
Manipulated Data
Field
next
module
# of hidden modules
# of hidden PCBs task struct next task, prev task
# of hidden modules
# of hidden modules
# of hidden PCBs task struct next task, prev task
# of hidden PCBs task struct next task, prev task
module
module
next
next
Operating
Attack
Vector
System
Redhat 8 /dev/kmem
Redhat 8 /dev/kmem
Redhat 8
Redhat 8
Redhat 8
Redhat 8
Redhat 8
Redhat 8
LKM
LKM
LKM
LKM
LKM
LKM
LKM
LKM
1 (rootkit self-hiding)
kis 0.9 (server) 1 (rootkit self-hiding)
adore-ng-2.6 1 (rootkit self-hiding)
ENYELKM 1.1 1 (rootkit self-hiding)
module
module
module
module
next
next
list.next, list.prev Debian Sarge
list.next, list.prev Debian Sarge
block (PCB) list.3 However other linked list-based data structures can be similarly sup-
ported as well. The basic procedure is to generate the live set L and periodically generate
and compare with the scanned set S. We tested 8 real-world rootkits and 2 of our own
rootkits (linuxfu and fuuld) previously used in [12,21,23], and these rootkits com-
monly hide kernel objects by directly manipulating the pointers of such objects. LiveDM
successfully detected all these attacks just based on the data anomaly from kernel mem-
ory maps and the results are shown in Table 2.
In the experiments, we focus on a speciﬁc attack mechanism – data hiding via DKOM
– rather than the attack vectors – how to overwrite kernel memory – or other attack fea-
tures of rootkits for the following reason. There are various attack vectors including the
ones that existing approaches cannot handle and they can be easily utilized. Speciﬁcally,
we acknowledge that the rootkits based on loadable kernel module (LKM) can be de-
tected by code integrity approaches [22,24] with the white listing scheme of kernel mod-
ules. However, there exist alternate attack vectors such as /dev/mem, /dev/kmem de-
vices, return-oriented techniques [11,25], and unproven code in third-party kernel drivers
which can elude existing kernel rootkit detection and prevention approaches. We present
the DKOM data hiding cases of LKM-based rootkits as part of our results because these
rootkits can be easily converted to make use of these alternate attack vectors.
We also include results for two other rootkits that make use of these advanced at-
tack techniques. hide lkm and fuuld in Table 2 respectively hide kernel modules and
processes without any kernel code integrity violation (via /dev/kmem) purely based on
DKOM, and current rootkit defense approaches cannot properly detect these attacks.
However, our monitor effectively detects all DKOM data hiding attacks regardless of
attack vectors by leveraging LiveDM-generated kernel object map. Allocation-driven
mapping can uncover the hidden object even in more adversary scenarios. For example,
if a simple linked list having no data invariant is directly manipulated without violating
kernel code integrity, LiveDM will still be able to detect such an attack and uncover the
speciﬁc hidden object.
In the experiments that detect rootkit attacks, we generate and compare L and S sets
every 10 seconds. When a data anomaly occurs, the check is repeated in 1 second. (The
3 A process control block (PCB) is a kernel data structure containing administrative information
for a particular process. Its data type in Linux is task struct.
Kernel Malware Analysis with Un-tampered and Temporal Views
191
repeated check ensures that a kernel data structure was not simply in an inconsistent state
during the ﬁrst scan.) If the anomaly persists, then we consider it as a true positive. With
this monitoring policy, we successfully detected all tested DKOM hiding attacks without
any false positives or false negatives.
We note that while this section focuses on data hiding attacks based on DKOM, data
hiding attacks without manipulating data (such as rootkit code that ﬁlters system call
results) may also be detected using the LiveDM system. Instead of comparing the un-
tampered LiveDM-generated view with the scanned view of kernel memory, one could
simply compare the un-tampered view with the user-level view of the system.
6.2 Temporal Malware Behavior Monitor
Kernel rootkit analysis approaches based on dynamic type-projection are able to perform
temporal analysis of a running rootkit. One problem with these approaches, however, is
that they are only able to track malware actions that occur from injected rootkit code. If
a rootkit modiﬁes memory indirectly through other means such as legitimate kernel func-
tions or kernel bugs,
these approaches are unable to follow the attack.
Allocation-driven mapping does not share this weakness. To further illustrate the
strength of allocation-driven mapping, we built a temporal malware behavior monitor
(called a temporal monitor or a monitor below for brevity) that uses a kernel object map
in temporal analysis of a kernel execution trace.
In this section, we highlight two features that allocation-driven mapping newly pro-
vides. First, allocation-driven mapping enables the use of a kernel object map covering
all kernel objects in temporal analysis; therefore for any given dynamic kernel object
we can inspect how it is being used in the dynamic kernel execution trace regardless
of the accessing code (either legitimate or malicious), which is difﬁcult for both static
and dynamic type-projection approaches. Second, the data lifetime in allocation-driven
mapping lets the monitor avoid the dynamic data identity problem (Section 2.1) which
can be faced by an asynchronous memory map.
Systematic visualization of malware inﬂuence via dynamic kernel memory. Our
monitor systematically inspects and visualizes the inﬂuence of kernel malware attacks
targeting dynamic kernel memory. To analyze this dynamic attack behavior, we gener-
ate a full system trace including the kernel object map status, the executed code, and the
memory accesses during the experiments of kernel rootkits. When a kernel rootkit attack
is launched, if it violates kernel code integrity, the rootkit code is identiﬁed by using our
previous work, NICKLE [22]. Then the temporal monitor systematically identiﬁes all
targets of rootkit memory writes by searching the kernel object map. If the attack does
not violate code integrity, the proposed technique in the previous section or any other ap-
proach can be used to detect the dynamic object under attack. The identiﬁed objects then
become the causes of malware behavior and their effects are systematically visualized
by searching the original and the modiﬁed kernel control ﬂow triggered by such objects.
For each object targeted by the rootkit, there are typically multiple behaviors using its
value. Among those, this monitor samples a pair of behaviors caused by the same code,
192
J. Rhee et al.
Table 3. The list of kernel objects manipulated by adore-ng rootkit. (OS: Redhat 8).
Runtime Identiﬁcation
Ofﬂine Data Type Interpretation
Call Site
fork.c:610
fork.c:610
fork.c:610
fork.c:610