mechanism A gives ε-diﬀerential privacy if for any database
D1 and D2 diﬀering on at most one record, and for any
possible output O ∈ Range(A),
P r[A(D1) = O] ≤ eε × P r[A(D2) = O]
(1)
where the probability is taken over the randomness of A.
A fundamental concept for achieving diﬀerential privacy
is the global sensitivity of a function [9] that maps an un-
derlying database to (vectors of) reals.
Definition 3.2
(Global Sensitivity). For any func-
tion f : D → Rd, the sensitivity of f is
∆f = max
D1,D2
||f (D1) − f (D2)||1
(2)
An n-gram model is a type of probabilistic prediction
for all D1, D2 diﬀering in at most one record.
640Laplace Mechanism. A standard mechanism to achieve
diﬀerential privacy is to add properly calibrated Laplace
noise to the true output of a function, which is known as
Laplace mechanism [9]. It takes as inputs a database D, a
function f , and the privacy parameter ε. The noise is gener-
ated according to a Laplace distribution with the probability
2λ e−|x|/λ, where λ is deter-
density function (pdf) p(x|λ) = 1
mined by both ∆f and the desired privacy parameter ε. Let
L(λ) denote a Laplace random variable with a probability
density function deﬁned as above.
Theorem 3.1. For any function f : D → Rd, the mech-
anism
Laplace(D, f, ε) = f (D) + [L1(λ), L2(λ), . . . , Ld(λ)]
(3)
gives ε-diﬀerential privacy if λ = ∆f /ε and Li(λ) are i.i.d
Laplace random variables.
4. SANITIZATION ALGORITHM
4.1 Overview
The main idea of our scheme is simple: we add prop-
erly calibrated Laplace noise to the counts of high-quality
grams and release them. Our goal is two-fold: (1) to release
grams whose real counts are large enough to increase util-
ity1, and (2) to maximize the sizes of released grams (i.e.,
the n value) to preserve as much sequentiality information
as possible. There is a fundamental trade-oﬀ between the
utility of noisy n-grams and their sizes: shorter grams enjoy
smaller relative error due to Laplace noise but carry less se-
quentiality information; longer grams contain more sequen-
tiality information but have smaller counts (and thus larger
relative error). In this paper, we address this trade-oﬀ by
releasing variable-length n-grams with counts larger than a
threshold 2 and of sizes less than a maximal size nmax
3. For
most practical datasets, setting nmax to a small value (e.g.,
3-5) has been suﬃcient to capture most of the sequentiality
information. Since short grams are typically of large real
counts, this property, which is also experimentally justiﬁed
in Appendix A, explains why the n-gram model is so pow-
erful and why it provides an excellent basis for diﬀerentially
private sequential data publishing.
To identify the set of high-quality (i.e., having low rela-
tive error) n-grams with possibly varying n values (1 ≤ n ≤
nmax ) from an input sequential dataset, we propose a well-
designed tree structure, called exploration tree.
It groups
grams with the same preﬁx into the same branch so that all
possible n-grams with size 1 ≤ n ≤ nmax can be explored
eﬃciently. The exploration starts with unigrams and then
proceeds to longer grams until nmax is reached. Intuitively,
if the noisy count of a gram g is small (i.e., close to the
standard deviation of the added noise), its real count also
tends to be small and thus the relative error is large. Since
all grams having the preﬁx g (i.e., all nodes in the subtree
rooted at g) have smaller real counts than g’s real count,
1The added noise is calibrated to the global sensitivity and is
independent of the count values. Thus, larger counts provide
better utility in terms of relative error.
2This threshold is set to limit the magnitude of noise in
released data.
3Beyond nmax , the utility gain of longer grams is usually
smaller than the utility loss due to noise.
Algorithm 1 Sequential Database Sanitization
Input: Raw sequential database D
Input: Privacy budget ε
Input: Maximal sequence length ℓmax
Input: Maximal n-gram size nmax
Output: Private sequential database eD
1: Truncate each S ∈ D by keeping the ﬁrst ℓmax items;
2: Create an exploration tree T with a virtual root;
3: i = 0;
4: while i < nmax do
5:
for each non-leaf node vij ∈ levelSet(i, T )
and lb(vij ) 6= & do
Calculate εvij ;
//see Section 4.3.2
Uc ← all possible children of vij with labels I ∪ {&};
//Compute the noisy count of each uk ∈ Uc
Q = {|g(u1)|, |g(u2)|, · · · , |g(u|I|+1)|};
eQ = Laplace(D, Q, εvij );
for each node uk ∈ Uc do
//∆Q = ℓmax
6:
7:
8:
9:
Add uk to T ;
if c(uk) < θ then
10:
11:
12:
13:
14:
15:
16: Enforce consistency on T ;
17: Generate eD from T ;
18: return eD;
i++;
Mark uk as leaf;
//see Section 4.3.3
//see Section 4.3.4
//see Section 4.3.5
they can be omitted from further computation. This obser-
vation makes our approach signiﬁcantly faster than naively
processing every single gram regardless of its size. It also
explains why we do not adopt the approach that generates
all possible n-grams and then prunes the tree.
4.2 Terminology
We ﬁrst give some notations used in our solution. The ex-
ploration tree is denoted by T . Each node v ∈ T is labeled
by an item I ∈ I ∪ {&}, where & is a special symbol rep-
resenting the termination of a sequence. The function lb(v)
returns v’s item label. Each node v is associated with an
n-gram deﬁned by the sequence of items from the root of T
to v, denoted by g(v). We slightly abuse the term count to
mean the number of occurrences of g(v) in the input dataset,
which is denoted by |g(v)|. Note that an n-gram may oc-
cur multiple times in a sequence. For example, the count of
I2 → I3 in the sample dataset in Table 1 is 6, instead of 5.
Each node v also keeps a noisy version of |g(v)|, denoted by
c(v). In addition, each node v conveys a conditional proba-
bility, denoted by P (v), which predicts the probability of the
transition from v’s parent to v. P (v) can be obtained by nor-
malizing the noisy counts of v’s siblings and v. For example,
in Figure 1, P (v5) = P (I1|I2 → I3) = 4/(4+0+1+2) = 4/7.
The set of all nodes in level i of T is denoted by levelSet(i, T )
and these nodes represent all i-grams in the dataset. The
level number of node v in T is denoted by level(v, T ). The
root of T is in level zero.
In the sequel, the probability
P (Li+1|Lj → Lj+1 → . . . → Li) is shortly denoted by
P (Li+1|Lj
i ).
4.3 Detailed Descriptions
4.3.1 Private Sequential Database Release
Algorithm 1 provides an overview of our approach.
It
takes as inputs a sequential database D, the total privacy
budget ε, the maximal sequence length ℓmax and the max-
imal n-gram size nmax , and returns a sanitized sequential
641v1
I1 4 ε/5 
Label
Root
Noisy Count
Privacy Budget
-
-
I2 10 ε/5 
v2
v3
v4
I3 9 ε/5 
I1 0 4ε/5  I2 4 4ε/5  I3 0 4ε/5  & 0 4ε/5 
I1 1 2ε/5  I2 0 2ε/5  I3 7 2ε/5  & 2 2ε/5 
I1 4 2ε/5  I2 2 2ε/5  I3 1 2ε/5  & 2 2ε/5 
v5
I1 4 2ε/5  I2 0 2ε/5  I3 1 2ε/5  & 2 2ε/5 
I1 1 2ε/5  I2 2 2ε/5  I3 0 2ε/5  & 1 2ε/5 
v6
v7
v8
v9
v10
v11
v12
I2 2
-
v13
Figure 1: The exploration tree of the sample data
database eD satisfying ε-diﬀerential privacy. ℓmax is a pa-
rameter speciﬁed by the data holder to limit the inﬂuence
of a single sequence in computation. The algorithm con-
siders only the ﬁrst ℓmax items in each input sequence. A
larger ℓmax allows more information to be retained from D,
but requires more noise to be injected in later computation;
a smaller ℓmax does the opposite. We discuss and report
the eﬀect of diﬀerent ℓmax values in Section 6, and provide
insights for a data holder to select a good ℓmax value in
practice. nmax bounds the height of the exploration tree T
and thus the maximal size of released grams. The choice
of nmax aﬀects the privacy parameter assigned to each level
of T , and, therefore, is also related to the magnitude of
noise. In practice, nmax could be set to 5, which is the max-
imal n value popularly used in the literature. Similarly, we
present more details on the selection of a reasonable nmax in
Section 6. We emphasize that this does not mean that all
released grams have a size of nmax but rather their sizes can
vary between 1 and nmax .
In Algorithm 1, we ﬁrst preprocess D by keeping only the
ﬁrst ℓmax items of each sequence in order to bound the inﬂu-
ence of a single sequence by ℓmax (Line 1). The construction
of T starts by creating an empty tree with a virtual root
(Line 2). In Lines 4-15, the algorithm iteratively constructs
each level of T . For level i of T , we decide whether to ex-
pand a node vij ∈ levelSet(i, T ) by comparing its noisy count
c(vij) with a threshold θ. If c(vij) ≥ θ, we expand vij by ex-
plicitly considering every possible item in I ∪ {&} as a child
of vij in order to satisfy diﬀerential privacy. By deﬁnition,
nodes labeled by & cannot be expanded because it means
the termination of a sequence. The entire exploration pro-
cess ends when either the depth of the tree reaches nmax or
no node can be further expanded (since their noisy counts
do not pass θ or their privacy budgets have run out). Exam-
ple 4.1 illustrates the construction of a possible exploration
tree on the sample dataset in Table 1.
Example 4.1. Given nmax = 5, ℓmax = 5 and θ = 3,
the construction of a possible exploration tree over the sam-
ple dataset in Table 1 is illustrated in Figure 1 (ignore the
privacy budget information and node v13 for now).
In the following, we detail the key components of Algo-
rithm 1: how to compute the privacy budget εij for each
node in T (Section 4.3.2), how to compute the threshold
θ for each node (Section 4.3.3), how to make T consistent
(Section 4.3.4), and how to generate a synthetic version of
the input database D from T (Section 4.3.5). Finally, in
Section 5, we prove the privacy guarantee of our scheme.
4.3.2 Adaptive Privacy Budget Allocation
nmax
Given the maximal gram size nmax , a simple privacy bud-
get allocation scheme is to expect the height of T to be nmax
and uniformly assign ε
to each level of T to calculate the
noisy counts of the nodes in each level. However, in reality,
many (or even all) root-to-leaf paths have a length much
shorter than nmax for the reason of their counts not being
able to pass θ. Hence assigning privacy parameters solely
based on nmax is clearly not optimal. For example, in Ex-
ample 4.1, since the height of the exploration tree is 3 and
nmax = 5, at least 2ε
5 privacy budget would be wasted in all
paths.
To address this drawback, we propose an adaptive privacy
budget allocation scheme that allows private operations to
make better use of the total privacy budget ε. Intuitively,
a desirable privacy budget allocation scheme should take
into consideration the length of a root-to-leaf path:
for a
shorter path, each node in the path should receive more
privacy budget; for a longer path, each node should use less
privacy budget. Hence we adaptively estimate the length of
a path based on known noisy counts and then distribute the
remaining privacy budget as per the estimated length.
At the beginning of the construction of T , in the absence
of information from the underlying dataset, we can only as-
sume that each root-to-leaf path is of the same length nmax
so that our algorithm would not exceptionally halt due to
is used to
running out of privacy budget. Therefore,
calculate the noisy counts of nodes in level 1. Once we obtain
some information from the underlying dataset (e.g., nodes’
noisy counts), we can make more accurate predictions on
the length of a path.
nmax
ε
For a node v in level i ≥ 2 with noisy count c(v), we
predict the height hv of the subtree rooted at v, denoted by
Tv, as follows. Let Pmax be the estimation of the probability
of transiting from v to the mode of its children (i.e., v’s child
with the largest noisy count). Assume that the probability
4. Under this
of the mode at each level of Tv is also Pmax
assumption, we can estimate the largest noisy count of the
nodes in level hv of Tv by c(v) · (Pmax )hv . Recall the fact
that Tv will not be further expanded if none of the nodes in
level hv can pass the threshold θ. We get c(v)·(Pmax )hv = θ,
4A more precise estimation could be obtained by applying
the Markov assumption to each level of Tv at the cost of
eﬃciency.
642that is, hv = logPmax
by nmax − i, we have
θ
c(v) . Since the height of Tv is bounded
4.3.3 Computing Threshold θ
A node in T is not further expanded if its noisy count is
less than the threshold θ. The main source of error in T
comes from the nodes that are of a true count of zero but of
a noisy count greater than θ (referred to as false nodes). For
this reason, we design a threshold to limit the total number
of false nodes in T with the goal of lowering the magnitude
of noise in T .
For each expansion, a false node v will generate, on av-
erage, |I|Pθ false children, where Pθ is the probability of
Laplace noise passing θ. This is because a descendant of v
must have a true count of zero. With the expansion of T ,
the number of false nodes accumulates exponentially with
the factor of |I|Pθ, resulting in excessive noise. To limit
the exponential growth of false nodes, we require |I|Pθ ≤ 1,
that is, Pθ ≤ 1
|I| . Since, under Laplace mechanism, given
the threshold θ and the privacy parameter ε′,
Pθ =Z ∞
θ
ε′
2ℓmax
exp(cid:18)−
xε′
ℓmax(cid:19) dx =
1
2
exp(cid:18)−
ε′θ
ℓmax(cid:19) ,
we get the threshold θ =
. We show in Section 6
that this threshold is eﬀective in eliminating false nodes
while having limited inﬂuence on nodes with large counts.
ε′
2
ℓmax ·ln |I|
4.3.4 Enforcing Consistency Constraints
The generated exploration tree T may contain some in-
consistencies for the reason that: (1) the sum of children’s
noisy counts is very unlikely to equal their parent’s noisy
count, and (2) there are some leaf nodes whose noisy counts
are missing (since their counts cannot pass the threshold θ).
In this section, we propose a method to resolve such inconsis-
tencies with the goal of improving data utility. In Appendix
A, we experimentally show that this method helps achieve
better performance.
The general idea is to approximate the missing counts by
making use of the Markov assumption and then normalize
children’s counts based on their parent’s count. More specif-
ically, our method works as follows. If none of the children
of a node v in T exceed the threshold θ, it is strong evi-
dence that v should not be further expanded, and therefore
all children of v (leaf nodes in T ) are assigned noisy counts
0. If all children pass θ, we ﬁrst calculate the conditional
probability of each child based on the sum of all children’s
noisy counts, and then obtain a consistent approximation by
multiplying this probability with their parent’s noisy count.
If some children (but not all) of v pass θ, we approximate the
noisy counts of the other children by the Markov assump-
tion. Let vc and C(vc) denote a child of v whose noisy count
cannot pass θ (called a missing node) and its Markov parent
in T , respectively. Let V denote the set of v’s children. We
partition V into V + and V −, where V + contains all nodes
passing the threshold, whereas V − contains the rest.
1. Deﬁne the following ratio for each vi ∈ V −:
rvi =
P (C(vi))
Pvj ∈V + P (C(vj))
For each vj ∈ V +, let A(vj) denote the noisy count
resulted by the Laplace mechanism in Line 10 of Al-
gorithm 1.
hv = min(logPmax
θ
c(v)
, nmax − i).
Next we discuss how to calculate Pmax for v. Let the i-
gram associated with v be L1 → L2 → · · · → Li (∀1 ≤ j ≤ i,
Lj ∈ I ∪{&}). Then we need to estimate the probability dis-
tribution of v’s children from the noisy counts known by far.
We resort to the Markov assumption for this task. Recall
that the order i−1 Markov assumption states P (Li+1|L1
i ) :≈
P (Li+1|L2
i ) may not be known in T (be-
cause we expand a node only when it passes the threshold
θ), we consider a chain of Markov assumptions (of diﬀerent
orders)
i ). Since P (Li+1|L2
P (Li+1|L1
i ) :≈ P (Li+1|L2
i ) :≈ P (Li+1|L3
i ) :≈ · · · :≈ P (Li+1)
to ﬁnd the best estimation of P (Li+1|L1
i ), which is the con-