niﬁcantly improve the operation of public clouds.
4) Throughput fairness: In PLTS, ﬂows achieve better TCP
throughput on an average as compared to ﬂows with ECMP
forwarding. But we also need to analyze how the increased
throughput performance is shared among all these ﬂows. Given
the uniform nature of PLTS, we expect high fairness among
different ﬂows simultaneously enabled in the data center
network. We show that the gains in throughput offered by
PLTS are evenly distributed among all ﬂows in the network.
In an 8-pod fat
tree network with 128 FTP applications,
we analyze the throughput achieved by each individual ﬂow
over the duration of the simulation. Figure 4(b) shows the
throughput observed by all ﬂows. Flow throughputs for ECMP
show a huge variability as compared to those of PLTS. There
is an order of magnitude of difference between the highest and
lowest throughput seen by ECMP. So the number of unlucky
and lucky ﬂows are large in case of ECMP forwarding. In case
of PLTS we see high degree of fairness among all the ﬂows.
Hence, PLTS provides more predictability in ﬂow throughput.
C. Effect of Over Subscription
Most of the evaluation results have shown that PLTS out-
performs ECMP in all dimensions in a data center network
with full bisection bandwidth. In this section, we compare
the two techniques in more constrained networks with higher
oversubscription ratios. iAn oversubscription ratio of 4:1 at
aggregate switches means that the uplink capacity of aggregate
switches to the core are four times less than the uplink capacity
of top of rack switches to the aggregate switches. We study
the effect of oversubscription on the performance of ECMP
and PLTS.
We begin with an 8-pod fat tree (which has 16 core switches
and 128 end hosts) topology which has 1:1 oversubscription
ratio, meaning that we have a full fat-tree network. In order to
constrain this network topology, we remove 4 core switches
(out of the 16 core switches) and the corresponding links to
get an oversubscription ratio of 4:3 at the aggregate switches.
We further remove 4 core nodes to get a ratio of 2:1 and so
on. We simulate 128 FTP applications in the network. Each
100 usec1 msec10 msec 0 20 40 60 80 100 120 140 160 180Latency (log scale)Packets by RankECMP with drop-tailECMP with RED/ECNPLTS-RR12345678 20 40 60 80 100 120Throughput (in Mbps)Flows by rankPLTSECMP(a) Oversubscription
(b) Randomized TM
(c) Hypervisor agent
Fig. 5. Performance of PLTS under cases of over-subscribed data center network, randomized trafﬁc matrix and with different packet sizes.
end host act as an FTP server for one application and client
for another. The FTP clients and servers are paired up at
random (while ensuring that they are in different pods). The
FTP applications run for the entire duration of the simulation.
Figure 5(a) shows the goodput (total bytes transferred) by
PLTS-RR and ECMP. With a full bisection bandwidth, PLTS
achieves almost 1.5 times the goodput offered by ECMP.
However, as the oversubscription ratio increases, the capacity
in the core of as well as the number of parallel paths in the
data center decreases. The gain of PLTS over ECMP begins
to decrease when fewer paths are available. But even in case
of 4:1 oversubscription PLTS performs better.
D. Goodput comparison with randomized trafﬁc matrix
In most of the evaluation experiments, we have used a uni-
form trafﬁc load where each end host is sending or receiving
or doing both at a time. This is not the case in real trafﬁc
load, where some of the end hosts may not be involved in
communication at all, or some have multiple TCP connections
simultaneously. In order to simulate such behavior we did an
experiment similar to one described in [14]. To simulate a
more realistic scenario, we started a ﬁxed number of FTP
ﬂows between randomly chosen hosts in a 6-pod fat tree data
center network. Flow sizes were chosen from the distribution
described in [15], which are representative of real ﬂows in
data center networks. When a ﬂow ﬁnishes, it was replaced
with another ﬂow between two randomly chosen hosts. The
experiment was run for 10 seconds and we calculate the total
number of bytes transferred by all the ﬂows in the entire
duration.
Figure 5(b) shows the total number of bytes transferred by
the PLTS-CNT and PLTS-RR technique. The X-axis is the
number of active ﬂows (which remains constant throughout the
simulation) as a fraction of the number of hosts. We observe
that PLTS outperforms ECMP consistently, and more so in
case there are larger number of active ﬂows in the network.
Since it was a different kind of trafﬁc workload, we also
want to compare the two PLTS techniques. PLTS-CNT and
PLTS-RR have comparable performance in most of the cases.
Note that since the trafﬁc in each experimental run is created
randomly, the performance of a speciﬁc technique would also
depend on the kind of trafﬁc load it has to work with. So the
general conclusion from this experiment is that PLTS performs
better than ECMP with both RR and CNT schemes having
comparable performance.
E. Effect of variable packet sizes
We do most of the evaluation by simulating FTP applica-
tions in which all ﬂows have same packet sizes (though data
and ACK packets differ in size). We also get near optimal
performance of PLTS in such scenarios without needing any
assistance from the agent. In this section, we vary the packet
sizes across different ﬂows and evaluate the performance of
PLTS with and without the agent. Since we are introducing
variability in packet sizes, we expect more variability in
latencies experienced by packets travelling across different
paths. A ﬂow may experience more reordering and in such
a situation we would like to evaluate the performance of our
agent which tries to handle packet reordering in a given ﬂow.
To vary packet size in the network, we set the MSS for
every ﬂow (total 54 active ﬂows in a 6-pod fat tree) randomly
between 64 to 1500 bytes. Figure 5(c) shows goodput gain
observed in this simulation environment. Even with the in-
creased variability, PLTS still outperforms ECMP. PLTS-RR
without any agent is approximately 1.35 times better. In such
a scheme, an agent, which dynamically adjusts the threshold
of duplicate acknowledgement for fast retransmission, helps
to improve the performance of PLTS.
V. RELATED WORK
The most related to our work are those mechanisms that
rely on ﬂow-level trafﬁc splitting such as ECMP and Hed-
era [3]. Mahout [10] is a recent scheme that uses end-host
mechanisms to identify elephants, and uses ﬂow scheduling
schemes similar to Hedera. BCube [17] proposes a server-
centric network architecture and source routing mechanism for
selecting paths for ﬂows. When a host needs to route a new
ﬂow, it probes multiple paths to the destination and selects
the one with the highest available bandwidth. Techniques like
Hedera, Mahout and BCube which select a path for a ﬂow
based on current network conditions suffer from a common
problem: When network conditions change over time,
the
selected path may no longer be the optimal one. To overcome
this problem, they periodically re-execute their path selection
algorithm. VL2[15] and Monsoon[16] propose using Valiant
 0.6 0.8 1 1.2 1.41:14:32:14:1Goodput (in GB)Oversubscription RatioPLTS-RRECMP 1 1.5 2 2.5 35075100125150Goodput (in GB)Active Flows (as a %age of number of hosts)PLTS-RRPLTS-CNTECMP 90 100 110 120 130 140 150 160PLTS-CNTPLTS-RRGoodput Gain(as %age of ECMP goodput)No agentWith hypervisor agent[4] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra
Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari
Sridharan, “Data Center TCP (DCTCP),” in ACM SIGCOMM’2010,
Aug. 2010.
[5] Mark Allman and Vern Paxson, “On estimating end-to-end network path
properties,” 1999.
[6] Jon C. R. Bennett, Craig Partridge, and Nicholas Shectman, “Packet
reordering is not pathological network behavior,” IEEE/ACM Trans.
Netw., vol. 7, pp. 789–798, December 1999.
[7] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang,
“Understanding data center trafﬁc characteristics,” in WREN, 2009, pp.
65–72.
[8] Ethan Blanton and Mark Allman, “On making tcp more robust to packet
reordering,” ACM Computer Communication Review, vol. 32, pp. 2002,
2002.
[9] Stephan Bohacek, Jo˜ao P. Hespanha, Junsoo Lee, Chansook Lim, and
Katia Obraczka, “Tcp-pr: Tcp for persistent packet reorderin,” in ICDCS,
2003, pp. 222–.
[10] Andrew Curtis, Wonho Kim, and Praveen Yalagandula, “Mahout: Low-
overhead datacenter trafﬁc management using end-host-based elephant
detection,” in Proc. of IEEE Infocom, Apr. 2011.
[11] Jeffrey Dean and Sanjay Ghemawat,
“Mapreduce: Simpliﬁed data
processing on large clusters,” December 2004, pp. 137–150.
[12] Sally Floyd and Van Jacobson, “Random early detection gateways for
congestion avoidance,” IEEE/ACM Trans. Netw., vol. 1, pp. 397–413,
August 1993.
[13] Sally Floyd, Jamshid Mahdavi, Matt Mathis, and Matthew Podolsky,
“An extension to the selective acknowledgement (sack) option for tcp,”
RFC 2883, IETF, July 2000.
[14] Alan Ford, Costin Raiciu, and Mark Handley,
“Tcp extensions for
multipath operation with multiple addresses,” Internet-draft, IETF, Oct.
2009.
[15] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and
Sudipta Sengupta, “Vl2: a scalable and ﬂexible data center network,”
in SIGCOMM ’09, New York, NY, USA, 2009, pp. 51–62, ACM.
[16] Albert Greenberg, Parantap Lahiri, David A. Maltz, Parveen Patel, and
Sudipta Sengupta, “Towards a next generation data center architecture:
scalability and commoditization,” in Proceedings of the ACM PRESTO,
New York, NY, USA, 2008, pp. 57–62, ACM.
[17] Chuanxiong Guo, Guohan Lu, Dan Li, Haitao Wu, Xuan Zhang, Yunfeng
“Bcube: a
Shi, Chen Tian, Yongguang Zhang, and Songwu Lu,
high performance, server-centric network architecture for modular data
centers,” New York, NY, USA, 2009, SIGCOMM ’09, pp. 63–74, ACM.
ratios,”
[18] Scott Hogg,
oversubscription
network
“10ge
and
http://www.networkworld.com/community/node/48965.
[19] Srikanth Kandula, Sudipta Sengupta, Albert G. Greenberg, Parveen
Patel, and Ronnie Chaiken, “The nature of data center trafﬁc: mea-
surements & analysis,” in Internet Measurement Conference, 2009, pp.
202–208.
[20] Costin Raiciu, Christopher Pluntke, Sbastien Barr, Adam Greenhalgh,
Damon Wischik, and Mark Handley,
“Data centre networking with
multipath tcp,” in Ninth ACM Workshop on Hot Topics in Networks
(HotNets-IX), Monterey, California, US, October 2010.
[21] K. K. Ramakrishnan, Sally Floyd, and David Black, “The addition of
explicit congestion notiﬁcation (ecn) to ip,” RFC 3168, IETF, Sept.
2001.
[22] Shan Sinha, Srikanth Kandula, and Dina Katabi, “Harnessing TCPs
Burstiness using Flowlet Switching,” in 3rd ACM SIGCOMM Workshop
on Hot Topics in Networks (HotNets), San Diego, CA, November 2004.
http://www.scalable-
[23] Scalable N. Technologies,
“QualNet,”
networks.com/.
[24] Ming Zhang, Brad Karp, Sally Floyd, and Larry L. Peterson, “Rr-tcp:
A reordering-robust tcp with dsack,” in ICNP, 2003, pp. 95–106.
Load Balancing (VLB) at a per-ﬂow granularity, but they too
do not split an individual ﬂow across multiple paths.
Two research efforts propose trafﬁc splitting at a sub-
ﬂow granularity. The ﬁrst effort is MPTCP[14] splits a TCP
ﬂow into multiple ﬂows at the end hosts. [20] evaluates the
performance beneﬁts of using MPTCP for load balancing in
a data center network. The ECMP protocol running at each
router may route each TCP sub-ﬂow over different paths in
the network. The receiving end host aggregates the TCP sub-
ﬂows and resequences packets. The second effort, although in
the context of the Internet, is FLARE [22]. FLARE exploits
the inherent burstiness of TCP ﬂows to break up a ﬂow into
bursts called ﬂowlets, and route each ﬂowlet along a different
path to the destination. However, FLARE requires each router
to maintain some per-ﬂow state and estimate the latency to the
destination. We did experiment with some simple variants of
FLARE, such as keeping a small number of packets of a ﬂow
go through the same path. But we observed that any simple
variant of FLARE does not achieve as good a throughput
as our PLTS, since bursts of packets may actually lead to
disparity in queue lengths across different paths, which in
turn causes much more packet reordering and reduction in
throughput.
VI. CONCLUSION
With many data center applications requiring large amount
of intra-cluster bandwidth, there is a lot of interest in designing
network fabrics that provide full bisection bandwidth. Multi-
rooted tree topologies have emerged as the architecture of
choice for many such environments. Unfortunately, however,
default multipath routing protocols such as ECMP can lead to
signiﬁcant load imbalance resulting in underutilizing the avail-
able network resources. Previous solutions such as Hedera and
MP-TCP address this problem to some extent, but are typically
complicated to implement or deploy. In contrast, we show that
simple packet-level trafﬁc splitting mechanims can, somewhat
surprisingly, yield signiﬁcant beneﬁts in keeping the network
load balanced resulting in better overall utilization, despite
the well-known fact that TCP interacts poorly with reordered
packets. These schemes are also readily implementable and
are of low complexity making them an appealing alternative to
ECMP and other complicated mechanisms. While more work
needs to be done, we believe that the myth that TCP interacts
poorly with reordering, while may be true in more general
settings, does not seem to hold true in regular data center
topologies.
REFERENCES
[1] “Per packet
load balancing,” http://www.cisco.com/en/US/docs/ios/
12 0s/feature/guide/pplb.html.
[2] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat,
scalable, commodity data center network architecture,”
Comput. Commun. Rev., vol. 38, no. 4, pp. 63–74, 2008.
“A
SIGCOMM
[3] Mohammad Al-fares, Sivasankar Radhakrishnan, Barath Raghavan, Nel-
son Huang, and Amin Vahdat, “Hedera: Dynamic ﬂow scheduling for
data center networks,” in In Proc. of Networked Systems Design and
Implementation (NSDI) Symposium, 2010.