mX
E[bX] = E[E[bX|λ]] = XE[
j=1
λj] = X
(3)
Furthermore, elementary algebra shows that
Var(bX) =
mX
j=1
E[λ2
j]vj
(4)
The RHS of (4) can be rewritten as
mX
mX
E[λ2
i ]vj =
j=1
j=1
E[(λj − Λj(v))2]vj + V0(v)
(5)
2.2 Variance of Combined Estimators
where
In order to use all the information available concerning X,
we form estimators of X that depend jointly on the m esti-
mators bX1, . . . , bXm. We focus on convex combinations of
the bXj, i.e., estimators of the form
bX =
mX
j=1
λjbXj, with λj ∈ [0, 1],
mX
j=1
λj = 1.
(1)
We allow the coefﬁcients λj to be random variables than
amenable to analysis, and the statistical properties of its
members are relatively easy to understand.
can depend on thebxij. This class of models is reasonably
Each choice of the coefﬁcients λ = {λj
1, . . . , m} gives rise to an estimator bX. Which λ should
(1). Let vj denote the variance Var(bXj), i.e,
nX
vj = Var(bXj) =
be used? To evaluate the statistical properties of the esti-
mators (1), we focus on two properties: bias and variance.
We now describe these for several cases of the estimator
Var(bxij) =
ij(1 − pij)
x2
nX
j =
(2)
pij
:
i=1
i=1
Λj(v) =
1/vjPm
j0=1 1/vj0
mX
−1
v
j
(6)
,
V0(v) = 1/
j=1
Eq. (5) shows that the variance of bX is minimized by min-
imizing the total mean square error in estimating the Λj
by λj. Then V0(v) is the minimum variance that can be
attained. The form of Λj says that the more reliable esti-
mates, i.e., those with smaller variance, have a greater im-
pact on the ﬁnal estimator.
2.5 Estimators of Known Variance
For known variances vj, Var(bX) is minimized by
λj = Λj(v)
(7)
We do not expect the vi will be known a priori. For general
pij it is necessary to know all xi in order to determine vi.
However, in many applications, only the sizes xi of those
ﬂows actually selected during sampling will be known. We
now mention two special cases in which the variance is at
least implicitly known.
94
Internet Measurement Conference 2005
USENIX Association
2.6 Spatially Homogeneous Sampling
Each ﬂow is sampled with the same probability at each OP,
which may differ between ﬂows: pij = pi for some pi and
all j. Then the vi are equal and we take λj = Λj(v) =
1/m. Hence for homogeneous sampling, the average es-
timator from Section 2.3 is the minimum variance convex
combination of the bXj.
2.7 Pointwise Uniform Sampling
for some qj and all i. Then vj = (Pn
Flows are sampled uniformly at each OP, although the
sampling probability may vary between points: pij = qj
i )uj where
uj = (1 − qj)/qj. The dependence of each vj in the {xi}
minimum variance convex combination bX using
is a common multiplier which cancels out upon taking the
i=1 x2
λj = Λj(v) = Λj(u)
(8)
2.8 Using Estimated Variance
i (1 − pij)/p2
ij
i (1 − pij)/p2
nX
i=1bvij
When variances are not know a priori, they may sometimes
be estimated from the data. For each OP j, and each ﬂow
i, the random quantity
bvij = χijx2
is an unbiased estimator of the variance vij = Var(bxij) in
estimating xi bybxij. Hence
bVj =
ij to the estimatorbVj whenever ﬂow
Note that bVj and bXj are dependent. This takes us out of
the class of estimators with independent {λj} and {bXj},
and there is no general simple form for the Var(bX) analo-
is an unbiased estimator of vj. Put another way, we add an
amount x2
i is selected at observation point j.
gous to (4). An alternative is to estimate the variance from
an independent set of samples at each OP j. This amounts
to replacing χij by an independent identically distributed
sampling indicator {χ0
ij} in (9). With this change, we know
from Section 2.4 that using
λj = Λj(bV )
will result in an unbiased estimator bX in (1). But the esti-
mator will not in general have minimum possible variance
V0(v) since λj is not necessarily an unbiased estimator of
Λj(v).
(9)
(10)
(11)
2.9 Some Ad Hoc Approaches
A problem with the foregoing is that an estimated variance
bVj could be zero, causing Λj(bV ) to be undeﬁned. On the
other hand, the average estimator is susceptible to the effect
of high variances. Some ad hoc ﬁxes include:
AH1: Use λj = Λj(bV ) on the subset of sample sets j with
non-zero estimated variance. If all estimated variances are
zero, use the average estimator.
AH2: Use the non-zero estimate of lowest estimated vari-
ance. But these estimators still suffer from a potentially far
more serious pitfall: the impact of statistical ﬂuctuations
in small estimated variances. This is discussed further in
Section 2.10.
2.10 Discussion
Absence of Uniformity and Homogeneity. We have seen
in Section 2.6 that the average estimator is the minimum
variance convex combination only when sampling is ho-
mogeneous across OPs. In Section 2.7 we saw that we can
form a minimum variance estimator without direct knowl-
edge of estimator variance only when sampling is uniform.
In practice, we expect neither of these conditions to hold
for network ﬂow measurements.
Firstly, sampling rates are likely to vary according to
monitored link speed, and may be dynamically altered in
response to changes in trafﬁc load, such as those gener-
ated by rerouting or during network attacks. In one pro-
posal, [7], the sampling rate may be routinely changed on
short time scales during measurement, while the emerging
PSAMP standard is designed to facilitate automated recon-
ﬁguration of sampling rates. Secondly, the recognition of
the concentration of trafﬁc in heavy ﬂows has led to sam-
pling schemes in which the sampling probability of a ﬂow
(either of the packets that constitute it, or the complete ﬂow
records), depends on the ﬂow’s byte size rather than being
uniform; see [4, 5, 6, 8, 12]. Finally, in some sampling
schemes, the effective sampling rate for an item is a ran-
dom quantity that depends on the whole set of items from
which it is sampled, and hence varies when different sets
are sampled from. Priority sampling is an example; see
Section 4.
Pathologies of Small Estimated Variances. Using es-
timated variances brings serious pitfalls. The most prob-
lematic of these is that samples taken with a low sampling
rate may have estimate variance close to or even equal to
zero. Even if the zero case is excluded in ad hoc man-
ner, e.g. as described in Section 2.9, a small and unreliable
sample may spuriously dominate the estimate because its
estimated variance happens to be small. Some form of reg-
ularization is required in order to alleviate this problem. A
secondary issue for independent variance estimation is the
requirement to maintain a second set of samples, so dou-
bling resource requirements.
In the next sections we propose a regularization for
variance estimation in a recently proposed ﬂow sampling
scheme that controls the effect of small estimated vari-
ances, even in the dependent case.
USENIX Association
Internet Measurement Conference 2005  
95
3 Regularized Estimators
We propose two convex combination estimators of the type
(1) using random coefﬁcients {λj} of the form (11) but
regularizing or bounding the variances to control the impact
of small estimated variances. Both estimators take the form
Pj λjbXj with λj = Λj(bU) for some estimated variances
bU, while they differ in whichbU is used.
Both estimators are characterized by the set of quantities
τ , where for each OP j:
τj = max
i: pij  0, a ﬂow of size x is sampled with
probability pz(x) = min{1, x/z}. Thus ﬂows of size
x ≥ z are always sampled, while ﬂows of size x < z are
sampled with probability proportional to their size. This al-
leviates the problem of uniform sampling, that byte estima-
tion can have enormous variance due to random selection
or omission of large ﬂows. In threshold sampling, all ﬂows
of size at least z are always selected.
we form an unbiased estimator bX of X =Pn
Starting with a set of ﬂows with sizes {xi} as before,
i=1 xi using
the selection probabilities pi = pz(xi). (In this section we
single OP takes the form bX takes the speciﬁc form
suppress the index j of the OP). The estimator of X from a
χi max{xi, z}
χixi/pz(xi) =
nX
nX
(18)
bX =
i=1
i=1
96
Internet Measurement Conference 2005
USENIX Association
mizes the cost Cz = Var(bX) + z2N where N =Pn
Threshold sampling is optimal in the sense that it mini-
i=1 pi
is the expected number of samples taken. This cost ex-
presses the balance between the opposing goals of reducing
the number of samples taken, and reducing the uncertainty
in estimating X. The value of z determines the relative
importance attached to these goals.
Applying the general formula (2), the variance of the es-
timate bX from a single OP is
nX
which has unbiased estimator
Var(bX) =
nX
bV =
i=1
xi max{z − xi, 0}
i=1
χiz max{z − xi, 0}
(19)
(20)
In threshold sampling, inhomogeneity across OPs arises
through inhomogeneity of the threshold z.
4.2 Priority Sampling
Priority sampling provides a way to randomly select ex-
actly k of the n ﬂows, weighted by ﬂow bytes, and then
form an unbiased estimator of the total bytes X. The algo-
rithm is as follows. For each ﬂow i, we generate a random
number αi uniformly distributed in (0, 1], and construct its
priorities bzi = xi/αi. We select the k ﬂows of highest
priority. Letbz0 denote the (k + 1)st highest priority. At a
single OP, we for the estimate
bX =
nX
i−1
χi max{xi,bz
0}
(21)
of the total bytes X. Here χi is the indicator that ﬂow i is
amongst the k ﬂows selected. bX is unbiased; see [6].
For priority sampling, the variance of bX takes a similar
form to that of threshold sampling:
nX
i=1
Var(bX) =
nX
bV =
i=1