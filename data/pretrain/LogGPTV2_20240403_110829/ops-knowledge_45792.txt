User: I have a number of huge delimited text files containing information logged by date. Some of the files overlap slightly with each other by date (which I don't want since it leads to duplicates). I know what the overlapping dates are so I want to be able to go through some of the files and delete each of the rows that contains those specified dates.
I know how to do this in python (rewriting each of the lines I want) but because of the size of the files (each is a few GBs) I was wondering if it would be a lot faster to do this through linux?
The text files will be sorted by date, earliest to latest, and the dates I need to delete are always going to be in the beginning of the file so I can search until I hit a row that has a date right after the ones I want to delete and write out the rest of the file to another file (or delete all the contents above).
Assistant: There are various command line applications that would be able to accomplish this when working together. For example, you could cat all the files one after another, grep -v the patterns you don't want, redirecting >> to a new file. In effect this is doing the same thing as your Python script would do, because every line of every file must be copied (except the duplicates). It might be faster than Python, though, because those tools are written in C.