Vehicle C 74.4%/
78.1%
Vehicle D 79.7%/
70.8%
Recall Acc.
44.1%/
41.8%
26.4%/
25.6%
45.7%/
44.9%
61.8%/
57.3%
91.5% 11.7% 51.6%
95.1% 15.0% 47.2%
The anomaly for Vehicle C can be explained as follows: With
more signals available for the run with correct boundaries, Phase 1
over-identifies signals and causes a higher number of false positives
for that specific vehicle. This is certainly possible.
Phase 2: Candidate Accuracy. The goal of this phase was to
identify CAN IDs that were likely associated with a body-related
event defined in Table 10. To evaluate the results of our algorithm,
we used metrics such as accuracy, precision, and recall. To evaluate
these metrics, we need to revisit the terms from the confusion
matrix in Table 1. Note that this is a coarser-grained analysis than
Phase 1. We assessed how well Phase 2 identified the corresponding
CAN IDs of events, not the signal position within a CAN message.
Our three-stage filtering process uses two input parameters that
were defined in Sec. 3.3: (1) the bit-flip threshold (Tp2,0), used to
generate the reference state and (2) the powertrain minimum cor-
relation score (Tp2,3), used in the powertrain filtering stage.
We ran the collected event traces through Phase 2 for each pa-
rameter configuration, calculating the accuracy, precision, and recall
metrics for each event. Since our goal was to facilitate the identifica-
tion of potential candidate CAN IDs, we preferred those parameters
that resulted in a high FP rate instead of a high FN rate — we wanted
to avoid excluding a potential candidate from consideration. The
optimal parameter values discovered for each vehicle are shown in
the last two columns of Table 3.
The second part of Table 4 summarizes the mean values of our
metrics for all 53 events while Fig. 9 shows the median number of
CAN IDs remaining after each filtering stage (per event), as well as
the total number of ground truth CAN IDs lost over all events at
each filtering stage. As predicted, our accuracy is high since we filter
out most unrelated CAN IDs for each event, whereas our precision
is relatively low. The latter metric indicates the ratio of correct CAN
IDs in the candidate set to the total number of candidates. However,
we do not consider low precision to be an issue. As Fig. 9 shows, we
can reduce the number of CAN IDs after three filtering stages by
more than 10x, despite losing some correct CAN IDs at each stage.
Additionally, some signals for body-related events were not avail-
able on the CAN buses we used for our evaluations. For instance,
the signal for the horn was not available on the CAN bus of any
vehicle we evaluated. We were unable to record data for 7 events
for Vehicle A, 15 events for Vehicle B, 7 events for Vehicle C, and 10
Figure 9: Filtering out CAN IDs in each stage
events for Vehicle D. However, 10 of the events we were not able to
record for Vehicle B were on the MS-CAN that was not accessible
through the OBD-II port. We opted to not remove those events from
our evaluation since it is likely that CAN data recorded on another
vehicle would yield similar results.
4.3 Manual Effort
An important metric for demonstrating the feasibility of LibreCAN
is the level of automation available, compared with the amount of
manual effort required on the part of the user. Although all three
phases in the system can run and generate results without human
intervention, there is still manual effort required to collect input
traces. The goal of LibreCAN is to enable every user to reverse-
engineer the CAN message format of their vehicle with as little
effort as possible. Hence, we want to assess how much data has to
be collected for Phase 1 to yield a reasonable precision and how
long it takes to record all 53 of the events used in Phase 2.
Phase 1. The recorded traces of all evaluation vehicles were around
60 minutes long. The precision reported in Sec. 4.2 reflects the entire
re-sampled trace. We wanted to see how a shorter recording would
affect this metric. We re-ran Phase 1 with signals obtained in Phase
0, with 25%, 50% and 75% of the trace length. In order to avoid
a bias towards more city or highway driving, we calculated the
precision for overlapping segments of this trace. For instance, to
analyze recordings of only half the length of the original trace, we
would use evaluate the following segments of the trace: (1) the first
half of the trace, (2) the slice of the trace between the first and last
quarters of its length, and (3) the last half The mean results of these
evaluations are plotted in Fig. 10.
A reduction in trace length results in a slight precision drop for
all vehicles except Vehicle B. The latter exhibits different behavior
because a significantly higher number of signals were extracted
with its 100% trace compared to the one in other vehicles — since
a greater number of signals were extracted in Phase 0, a greater
number signals were processed in Phase 1. Both the 75% and 100%
traces for this vehicle yielded the same number of correct signals
RawConstantReferencePowertrainStage020406080100Number of CAN IDsMedian Number of Unique CAN IDs Per Event (Veh. A)Total Number of Ground Truth CAN IDs Lost (Veh. A)Median Number of Unique CAN IDs Per Event (Veh. B)Total Number of Ground Truth CAN IDs Lost (Veh. B)Median Number of Unique CAN IDs Per Event (Veh. C)Total Number of Ground Truth CAN IDs Lost (Veh. C)Median Number of Unique CAN IDs Per Event (Veh. D)Total Number of Ground Truth CAN IDs Lost (Veh. D)Session 10A: Cyberphysical SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2292Figure 10: Precision of Phase 1 with varying trace lengths
(our design goal in Phase 0), but the 100% trace resulted in more
signals being processed (due to a higher number of total extracted
signals), which increased the number of false positives and thus
decreased the precision. In order to achieve at least 65% precision,
we recommend using a trace covering 30 minutes or more.
Phase 2. In order to assess the time required to record all 53 events
listed in Table 10, we conducted a human-study experiment, for
which we obtained an IRB approval (Registration No. IRB00000245).
For this purpose, we developed an Android app that ran on top of
CarLab [44]. The participant was required to interact with this app,
which loops through all 53 events, displaying them one at a time on
the screen. A timer begins with the start of recording for the first
event and the participant, seated in the driver’s seat, is instructed
to perform each event and then click the Next Event button. The
timer stops after the last event has been performed. During the
experiment, a member of the study team sat in the passenger seat
and evaluated participant’s performance of the events, namely if
one was performed incorrectly or skipped.
A total of ten people participated in this experiment. They were
instructed on how to operate the app and were not allowed to ask
questions once the experiment began. After completing all events,
the team member recorded how long the participants took and
asked them how familiar they were with the test vehicle (Vehicle
A) on a scale from 1 to 5, with 5 being the most familiar. Fig. 11
(a) summarizes the correlation between the level of experience
with the time span. Note that the completion time was not affected
much by the experience level, except for one totally inexperienced
(1/5) and one very experienced (5/5) participant. Specifically, for
users with experience levels ranging from 2 to 4, the median of
their completion time varies between 9.0 to 10.4 minutes. Fig. 11
(b) shows the key behavioral metrics (i.e., number of mistakes and
skips) of all participants. The median numbers of mistakes and skips
are 3.5 and 1, respectively. As a result, drivers of different experience
levels are capable of performing all 53 events with the median rates
of error and skip at 6.6% (=3.5/53) and 1.9%, respectively.
In conclusion, we estimate that a 30 minute drive for Phase 1
and a 10 minute experiment session for Phase 2 are sufficient to
(a) Experience-time correlation
(b) Key metrics
Figure 11: Results in user-study experiment
produce good results. These numbers are feasible for an otherwise
completely automated CAN reverse-engineering framework, espe-
cially given the time that manual reverse-engineering would likely
take. The latter can take from days to weeks, given the detail and
precision of the reverse-engineering needed. Although no explicit
times are reported for manual reverse-engineering, tutorials [48]
imply significant effort is required. However, researchers from the
well-known Jeep hack [42] provide a reference in their paper: "(...)
we spent an entire year figuring out which messages to send for
the Ford and Toyota (...)". Although they very likely did not spend
that entire time frame for reverse engineering of CAN messages, it
shows that is not a trivial process and takes a lot of experimenting
to find the correct payload for their CAN injection attack.
4.4 Computation Time
Having discussed the manual effort required to use LibreCAN, we
analyze the computation time of all three phases individually.
All experiments were conducted using Python 3 on a computer
running 64-bit Ubuntu 16.04. This computer featured 128 GB of
registered ECC DDR4 RAM and two Intel Xeon E5-2683 V4 CPUs
(2.1 GHz with 16 cores/32 threads each). Phase 0 utilizes all available
computational resources (64 threads), whereas Phase 1 uses one
thread per signal d plus one main thread (23 threads). Meanwhile,
the computationally inexpensive Phase 2 runs in a single thread.
Table 5 reports the time required for all computation steps. Note
that these values have been generated for a run with the optimal
parameter configuration. The total runtimes include operations
that finished in less than one second, which are listed as completing
in 0 seconds in Table 5.
The entire three phase automated process takes 79 seconds for
Vehicle A, 74 seconds for Vehicle B, 70 seconds for Vehicle C and 72
seconds for Vehicle D. All vehicles have a similar computation time,
indicating that LibreCANis highly efficient in reverse-engineering
a vehicle’s CAN bus (slightly more than 1 minute) with only a small
amount of manual effort (around 40 minutes).
4.5 Testing on Generic Parameters
As mentioned before, LibreCAN was designed to achieve a good
performance with a universal set of parameters in all three phases.
In order to show that anyone can achieve a comparable performance
as reported in the previous subsections without a priori knowledge
30405060708090100Trace Length (in %)5560657075Precision (in %)Vehicle AVehicle BVehicle CVehicle D12345Experience6810121416MinutesMistakesSkipsMetrics024681012Number of occurrences12345Experience6810121416MinutesMistakesSkipsMetrics024681012Number of occurrencesSession 10A: Cyberphysical SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2293Table 5: Summary of computation time in each phase and
stage (units are in seconds)
Table 6: Phases 1 and 2 Evaluation Metrics for Generic Pa-
rameters
Veh A Veh B Veh C Veh D
Phase 1
Prec.
Recall Acc.
Vehicle A 77.2% 41.8%
65.9% 22.5%
Vehicle B
Vehicle C 78.1% 44.9%
Vehicle D 72.5% 56.2%
Phase2
Recall
Prec.
58.2%
88.0% 8.9%
90.1% 8.5%
46.2%
91.5% 11.7% 51.6%
94.6% 13.7% 47.2%
Phases
Phase
0
Phase
1
Phase
2
Stages
Parse Raw
CAN File
Split Trace
Remove Un-
used Columns
Extract Signals
Move Small
Files
Total
Run Correlate
Calculate Scale
and Offset
Total
Create Ref.
State
Filter Constant
Messages
Compare to
Ref. State
Filter Power-
train Related
Messages
Total
11
12
2
0
4
0
17
40
17
57
0
4
0
0
5
2
0
9
0
23
30
18
48
0
2
0
0
3
9
2
0
5
0
16
36
16
52
0
2
0
0
2
9
2
0
5
0
16
40
13
53
0
3
0
0
3
Libre
CAN
Total
79
74
70
72
of the parameters, we would like to introduce an accuracy analysis
similar to the one in Sec. 4.2. Since one of our design goals was to
select similar parameters among the four evaluation vehicles, we
can now pick any configuration of these four vehicles for testing.
We evaluated all four vehicles on parameters Tp0,0 = 2, Tp0,1 = 3,
Tp0,2 = 0.01, Tp0,3 = 2, Tp1,0 = 0.05, Tp2,0 = 0.03, and Tp2,4 = 0.70.
The results are summarized in Table 6. A comparison with the
optimal results for each vehicle in Table 4 shows that they are
relatively similar. Through our design goals as well as exhaustive
evaluation on four vehicles, we found a parameter configuration
that can produce favorable results for any testing vehicle. This
corroborate the scalability of LibreCAN.
5 DISCUSSION