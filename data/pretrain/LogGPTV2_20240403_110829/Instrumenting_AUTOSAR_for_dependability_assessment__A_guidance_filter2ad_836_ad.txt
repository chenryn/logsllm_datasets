those systems is severely limited, and consequently so is the
knowledge on fault models. Whether this, or other fault mod-
els, are realistic or relevant for AUTOSAR is an interesting
question that currently can not be answered due to the novelty
of the system and the lack of respective (experience) data.
To analyze the relevance of various fault types in AUTOSAR
is potentially an interesting ﬁeld of future research for the
dependability community. For our instrumentation approach
this has the implication that we can currently only assume
that there are faults in AUTOSAR that can be addressed by
FI at the interface level.
Having said that, the results of our experiments are listed
in Table I. For each test run, we provide the number of
detected deviations from the golden run as a measure of the
fault’s overall impact on the system. The error persistence
indicates, for how long the fault’s effects were detectable in the
system. In summary, all the fault injections, for each test setup
and instrumentation method, manifest as detectable deviations
from the golden run thus verifying the effectiveness of each
approach. Injections into the lower 8 bits have only minor
impact on system behavior and are tolerated by the system
within one or two periods of execution time. Of all tested
interfaces, WheelSpeed and VehicleSpeed are most susceptible
to variations in the lower bit range. The peak value of detected
deviations, on the other hand, is reached by injecting into the
upper range of most signiﬁcant bits of the BrakePedalPosi-
tion, RequestedBrakeTorque and WheelSpeed interfaces. The
repeatedly measured cutoff of the error persistence at 1940ms
is owed to the car being at full stop at that time.
It is noteworthy to highlight that the AUTOSAR component
robustness assessment coverage for the number of detected
deviations across all bit positions were similar for both SW-C
and RTE, and at the .c, .h and .o levels. The deviation stems
in each case from a variation of the fault injector location
and not from a conceptual weakness or strength of one or
the other approach. This result is important as the equivalent
dependability coverages result in giving the system evaluator
the desired instrumentation choices as based on the access and
implementation/execution criteria of Section V-D and Section
VI.
D. Instrumentation Overhead
The instrumentation of a system obviously entails overhead
either in space (e.g. memory consumption) or time (e.g.
execution time). In this section, we determine the overhead
of each instrumentation technique in three categories: im-
plementation, runtime and memory. Given the multitude of
platforms, systems and tool-chains in the automotive domain,
this is a best effort approach that aims to establish a relative
comparison between the instrumentation methods and raise the
reader’s awareness for the different evaluation criteria.
1) Implementation: Implementation overhead describes the
expected time and effort to implement an approach by hand.
We measure the implementation overhead of each instrumenta-
tion method using SLOCCount [23], a set of tools for counting
physical source lines of code (SLOC). Table II and Table III
list the SLOC of each component and the RTE respectively,
for various instrumentation methods.
As the numbers for SW-C reveal, SW-C .h has the highest
implementation overhead, followed by SW-C .c and SW-C .o.
Recalling from Section IV, this is not surprising as SW-C .h
requires the redeclaration of interfaces, the implementation of
interface wrappers and the declaration of the interface wrap-
pers. Implementing SW-C .c, the redeclaration of interfaces is
not part of the process, whereas SW-C .o only requires the
implementation of interface wrappers.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:19:13 UTC from IEEE Xplore.  Restrictions apply. 
OVERHEAD IN SOURCE LINES OF CODE (SLOC) OF INSTRUMENTED SOFTWARE COMPONENTS FOR DIFFERENT INSTRUMENTATION METHODS.
TABLE II
Instrumentation
ETAS INTECRIO
None
SW-C .c
SW-C .h
SW-C .o
OptXware EA
None
SW-C .c
SW-C .h
SW-C .o
ABS FL
BrakeActuator FL
BrakeController
BrakePedalSensor
VehicleSpeed WheelSpeedSensor FL
144
+26
+30
+22
141
+26
+30
+22
48
+8
+9
+7
45
+8
+9
+7
62
+14
+16
+12
59
+14
+16
+12
51
+8
+9
+7
48
+8
+9
+7
98
+14
+16
+12
95
+14
+16
+12
49
+8
+9
+7
46
+8
+9
+7
OVERHEAD IN SOURCE LINES OF CODE (SLOC) OF INSTRUMENTED RTE
FOR DIFFERENT INSTRUMENTATION METHODS.
TABLE III
Instrumentation
RTE .c
RTE .h
RTE .o
ETAS INTECRIO OptXware EA
+67
+67
not evaluated
+67
+67
Fig. 7.
instrumentation methods, grouped by implemented functionality.
OptXware EA: Relative comparison of the execution time of
boxplots of the accumulated runtime of all instrumented inter-
face calls in CPU ticks, for ETAS INTECRIO and OptXware
EA, respectively. The boxplots’ quartiles are at 25% and 75%
of measured runtimes. The median value is at 50% and marked
by a black line. The difference of magnitudes in the CPU
ticks scale is caused by the different simulation approaches
of each tool. While OptXware EA directly simulates RTE
and application layer behavior, ETAS INTECRIO executes the
target system on a virtual PC target.
The measurements show that the instrumentation method
has no signiﬁcant
impact on the overall runtime. Visible
minor variations can be attributed to slight deviations of
system load during the experiments, caused e.g. by background
applications. Also,
the sole instrumentation with wrappers
without
implemented functionality (skeletons) causes only
slight overhead below 1%.
The main contributor to runtime overhead is therefore
the runtime of the functionality that is implemented in the
wrappers. In our monitor implementation, we directly write the
monitored values to the disk in each invocation. As disk I/O
is an expensive operation, we measure an overhead of about
50% for OptXware EA and about 38% for ETAS INTECRIO.
The fault injector on the other hand, adds no measurable
overhead. As the above example shows, providing a time-
efﬁcient implementation of wrapper functionality is crucial
Fig. 6.
instrumentation methods, grouped by implemented functionality.
ETAS INTECRIO: Relative comparison of the execution time of
For the RTE the ﬁgures show a different picture, due to the
way the tools generate the RTE. As a declaration of interfaces
is omitted in the generated code, RTE .c and RTE .o both
only require the implementation of interface wrappers, and
therefore share the same overhead.
As the overhead of functionality within the wrappers de-
pends on their implementation, no general statement on their
overhead can be made. To provide an example though, the
monitors we use consume 1 SLOC per wrapper, whereas the
fault injector consumes 9 SLOC.
2) Runtime: We employ ETAS INTECRIO and OptXware
EA to simulate actual system behavior on a PC platform. As
both tools do not provide an accurate emulation of the time of
the simulated target system, we use the Windows API function
QueryPerformanceCounter to measure the current CPU tick
count, eventually establishing a relative comparison of the
runtime of the different approaches. Figures 6 and 7 depict
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:19:13 UTC from IEEE Xplore.  Restrictions apply. 
 100000 110000 120000 130000 140000 150000 160000 170000 180000NoneSW-C .cSW-C .hSW-C .oRTE .cRTE .oSW-C .cSW-C .hSW-C .oRTE .cRTE .oSW-C .cSW-C .hSW-C .oRTE .cRTE .oAggregated CPU ticks per interface callSKELETONMONITORFAULT INJECTIONupper/lower quartilemedian value 200 300 400 500 600 700 800 900 1000 1100NoneSW-C .cSW-C .hSW-C .oRTE .cRTE .oSW-C .cSW-C .hSW-C .oRTE .cRTE .oSW-C .cSW-C .hSW-C .oRTE .cRTE .oAggregated CPU ticks per interface callSKELETONMONITORFAULT INJECTIONupper/lower quartilemedian valuein real-time systems. It should also be noted that the actual
systemwide overhead is considerably smaller, as the above
percentages are relative to individual interface calls.
3) Memory: We evaluate the overall instrumentation mem-
ory overhead, which consists of added code segment size and
data segment size, with the tool objdump, which is part of
GNU Binutils [8]. The object ﬁles of each of the system’s
components were compiled without optimizations (compiler
switch -O0), in order to have a worst case estimation and to
disregard compiler speciﬁc optimizations.
Our analysis shows that the instrumentation with wrappers
causes no data segment size overhead and that the text segment
size overhead is independent of the instrumentation method.
A detailed breakdown of components’ text segment size and
the introduced relative overhead is provided in Table IV.
The ﬁgures show that the relative overhead in text segment
size ranges between 1.5% and 15.0% per wrapper, and is
therefore largely dependent on the implementation complexity
of each component. In absolute values, each wrapper consumes
approximately 33 bytes for ETAS INTECRIO and 30 bytes for
OptXware EA. This difference is caused by the different com-
pilers used by each tool, with INTECRIO relying on MinGW
GCC 3.4.2 (mingw-special) and EA relying on Cygwin GCC
3.4.4 (cygming special).
VI. DISCUSSION
the component
The experimental results of the previous section have shown
that all
instrumentation methods are comparably effective
to enable the implementation of dependability assessment
techniques at
level, and therefore have to
be considered equally viable. Consequently, we draw the
conclusion that qualitative aspects can become the determining
factor in choosing the right instrumentation option and loca-
tion. To this end, we discuss the qualitative characteristics of
each instrumentation method, with the intention to guide the
evaluator in his decision of how and where to instrument a
system and which tradeoffs to consider. In the second part
of the discussion, we cover the current limitations of our
approach, speciﬁcally for multiple component instantiations
and shared memory communication.
A. Qualitative aspects of SW-C instrumentation methods
In the following, we introduce a set of quality attributes,
which we use to establish a qualitative comparison between
the different
instrumentation methods. A summary of the
comparisons is provided in Table V.
Intrusiveness describes to which degree the instrumenta-
tion penetrates the system. Thereby, we consider the sys-
tem viewpoint (i.e., which layer is affected and what is the
layer’s criticality) and the implementation viewpoint (i.e.,
which parts of the implementation are changed). Although
the instrumentation with wrappers is an automatic process,
the implementation of functionality within the wrappers is a
manual or semi-automatic process and therefore error-prone.
To minimize possible negative effects of such errors, a low
intrusiveness is desirable. Due to the RTE’s vital role as
communication hub, approaches targeting the RTE are con-
sequently considered more intrusive than the ones targeting
the SW-C. Furthermore we consider changes to the actual
implementation more intrusive than changes to the interface
declaration or the link information of the object ﬁle. The least
intrusive instrumentation method is therefore SW-C .o-ﬁle and
the most intrusive one is RTE .c-ﬁle.
Implementation effort considers the amount of changes
entailed by each instrumentation method and serves as an
estimate for the effort of manually instrumenting a system,
as well as the amount of changes induced by automatic
generation. With reference to the technical implementation
details of Section IV-B, we assess that the instrumentation
of .c-ﬁles requires a higher effort than .h-ﬁles and .o-ﬁles.
Also, instrumenting the RTE generally requires less effort than
instrumenting SW-Cs, as SW-Cs reside in distributed locations,
whereas the RTE resides in a central location. Therefore, the
least implementation effort is required by RTE .o-ﬁle and the
most by SW-C .c-ﬁle. For a general estimate of implementa-
tion effort, it should be kept in mind that regardless of the
effort of wrapper instrumentation, the effort of implementing
functionality into the wrappers has to be considered as well.
Automation complexity provides an estimate of the effort
to implement the instrumentation method into a generator.
During the implementation of the wrapper generator presented
in Section IV-C, we made the experience that binary instru-
mentation is the most complex generation task to implement.
This is due to the black-box constraint put by binary objects,
which requires the deduction and generation of the complete
interface declaration from the system speciﬁcation contained
in the system’s ARXML ﬁle. The implementation (.c-ﬁle)
and interface declaration (.h-ﬁle) on the other hand, both
contain the declaration, either implicitly or explicitly, making
this generation step obsolete, and only requiring a technically
similar parsing of source ﬁles. Due to the central location
of the RTE, mentioned in the previous paragraph, the least
automation complexity is required by RTE .c-ﬁle and RTE .h-
ﬁle and the most by SW-C .o-ﬁle.
The required system access characterizes each instrumen-
tation method’s requirements on the accessibility and visibility
of the system and its implementation. We distinguish white-
box, i.e., all source code is accessible to the system evaluator,
grey-box, i.e., parts of the source code (e.g. header ﬁles) are
accessible, and black-box, i.e., no source code is accessible.
Furthermore, we distinguish between SW-Cs and the RTE,
with access to the RTE usually being available to the integrator
only. Accordingly, RTE .c-ﬁle has the highest requirements on
system access and SW-C .o-ﬁle the lowest ones.
Scalability describes, how well each instrumentation
method scales to larger systems. As the scalability of an
instrumentation method has a high inﬂuence on its usability,
we consider them collectively. The main overhead in large
scale projects can be accounted to the conﬁguration of