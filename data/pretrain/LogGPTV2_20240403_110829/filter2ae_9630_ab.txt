## 攻击LIME&SHAP
###  原理
LIME等技术通过构建局部可解释近似模型来解释给定黑盒模型的单个预测，每一个样本的局部近似都被设计用来捕获给定数据点附近的黑盒行为，这些邻域构成了由输入数据中单个实例的扰动特征生成的合成数据点。然而，使用这些扰动生成的实例可能是off-manifola或者out-of-distribution(OOD)的
为了更好地理解由扰动生成的合成数据点的性质，研究人员进行了实验。他们使用LIME方法对输入实例进行扰动，然后对包含原始实例和扰动实例的组合数据集进行主成分分析PCA,将其降到2维
结果如下
从图中可以看到，合成的数据的分布与原始的实例的分布有很大的不同，这种现象是研究人员欺骗LIME，SHAP的基础。攻击者可以在输入数据点上创建一个类似于原始分类器(可能具有很大的歧视性)的对抗性分类器，但是在扰动数据上的表现却与原始分类器不同，对抗性分类器表现地没有偏见并且是公平的，从而有效地欺骗LIME、SHAP。
设 _f_
是前面提到的有偏见的分类器，对手希望设计一个对抗性分类器，在用户生成这个黑盒的解释时，解释技术会被欺骗，认为这个模型是无害的。这个分类器接收的N个数据点组成的数据集
_X_ 来自于真实世界的分布 _X **d** i **s** t_。为了欺骗解释性技术，攻击者设计的对抗分类器，在从 _X **d** i **s**
t_采样的数据点上显示有偏见的行为，而在不是来自 _X **d** i **s** t_
的数据点上保持没有偏见，由于LIME,SHAP输出的特征重要度严重依赖于扰动实例(通常是OOD样本)，因此得到的解释将使对抗分类器看起来无害。
设Ψ是一个没有偏见的分类器(基于与敏感特征不相干的无害特征进行预测),则对抗分类器 _e_ 的形式如下
创建这种分类器的难点就是我们需要判断给定的数据点是否来自Xdist，或者说是否是OOD的,为此需要创建一个分类器is_OOD
为了创建分类器is_OOD，需要构造新的数据集，扰动每个实例 _x_ 生成新的实例xp，设所有通过扰动得到的实例的集合为Xp，然后将 _X_
中所有实例的标签设置False，将Xp中所有实例的标签设为True(在 _X_ 中的实例除外)，然后我们在联合数据集Xp⋃X以及对应的标签上训练分类器即可。
###  实现
**欺骗LIME**
**欺骗SHAP**
###  COMPAS测试
COMPAS数据集如下
从结果可以看到，不论是LIME还是SHAP此时在解释对抗模型的决策时，都认为其不再以种族作为最关键的决策因素，即不再认为该模型是存在种族歧视的，而实际上，通过我们的分析，我们知道该模型事实上是存在种族歧视的，由此我们就实现了对LIME,SHAP方案的欺骗。
###  其他数据集
COMPAS数据中的特征包括犯罪史、人口特征、COMPAS风险评分、入狱和服刑时间，其输出为风险率，涉及到种族歧视的问题。Communities &
Crime数据中的特征包括种族，年龄，教育程度，警察统计数据，婚姻状况，公民身份等，其输出为犯罪率，涉及到种族歧视问题。German
Credit数据中的特征包括账户信息，信用记录，贷款目的，就业情况，人口统计资料等，是输出判断客户是否为优质客户，其存在性别歧视。我们可以用同样的方法进行攻击，攻击结果如下
**Communities& Crime**
数据集如下
**lime**
**shap**
从结果可以看出，两种可解释性方法都认为除人种外的其他因素在模型决策时更加重要，而模型认为重要的因素，比如母语是否为英语、孩子数量等在我们人类看来往往并不要紧。
**German Credit**
数据集如下
**lime**
**shap**
从结果可以看出，两种可解释性方法都认为除性别外的其他因素在模型决策时更加重要，而模型认为重要的因素，比如存款余额、是否为外国工作人员等在我们人类看来往往并不要紧。
## 参考
1.
2.深度学习模型可解释性研究综述
3.Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods