## 攻击LIME和SHAP

### 原理
LIME等技术通过构建局部可解释近似模型来解释给定黑盒模型的单个预测。每个样本的局部近似旨在捕捉数据点附近的黑盒行为，这些邻域由输入数据中单个实例的扰动特征生成的合成数据点构成。然而，使用这些扰动生成的实例可能是离群值（off-manifold）或分布外（out-of-distribution, OOD）。

为了更好地理解这些扰动生成的合成数据点的性质，研究人员进行了实验。他们使用LIME方法对输入实例进行扰动，然后对包含原始实例和扰动实例的组合数据集进行主成分分析（PCA），将其降到二维。结果表明，合成数据的分布与原始实例的分布有很大差异。这种现象是欺骗LIME和SHAP的基础。

攻击者可以在输入数据点上创建一个类似于原始分类器（可能具有很大的歧视性）的对抗性分类器，但在扰动数据上的表现却与原始分类器不同。对抗性分类器表现得没有偏见且公平，从而有效地欺骗LIME和SHAP。

设 \( f \) 是前面提到的有偏见的分类器，对手希望设计一个对抗性分类器，在用户生成这个黑盒的解释时，解释技术会被欺骗，认为这个模型是无害的。这个分类器接收的数据集 \( X \) 来自于真实世界的分布 \( X_{dist} \)。为了欺骗解释性技术，攻击者设计的对抗分类器在从 \( X_{dist} \) 采样的数据点上显示有偏见的行为，而在不是来自 \( X_{dist} \) 的数据点上保持没有偏见。由于LIME和SHAP输出的特征重要度严重依赖于扰动实例（通常是OOD样本），因此得到的解释将使对抗分类器看起来无害。

设 \( \Psi \) 是一个没有偏见的分类器（基于与敏感特征不相关的无害特征进行预测），则对抗分类器 \( e \) 的形式如下：
\[ e(x) = \begin{cases} 
f(x) & \text{if } x \in X_{dist} \\
\Psi(x) & \text{otherwise}
\end{cases} \]

创建这种分类器的难点在于需要判断给定的数据点是否来自 \( X_{dist} \)，或者说是否是OOD的。为此，需要创建一个分类器 \( is\_OOD \)。

为了创建分类器 \( is\_OOD \)，需要构造新的数据集。扰动每个实例 \( x \) 生成新的实例 \( x_p \)，设所有通过扰动得到的实例的集合为 \( X_p \)。然后将 \( X \) 中所有实例的标签设置为False，将 \( X_p \) 中所有实例的标签设为True（在 \( X \) 中的实例除外）。最后在联合数据集 \( X_p \cup X \) 及其对应的标签上训练分类器即可。

### 实现
**欺骗LIME**
**欺骗SHAP**

### COMPAS测试
COMPAS数据集如下：
- 特征：犯罪史、人口特征、COMPAS风险评分、入狱和服刑时间
- 输出：风险率
- 涉及问题：种族歧视

从结果可以看到，不论是LIME还是SHAP，在解释对抗模型的决策时，都认为其不再以种族作为最关键的决策因素，即不再认为该模型存在种族歧视。而实际上，通过我们的分析，我们知道该模型事实上是存在种族歧视的，由此我们就实现了对LIME和SHAP方案的欺骗。

### 其他数据集
**Communities & Crime**
- 特征：种族、年龄、教育程度、警察统计数据、婚姻状况、公民身份等
- 输出：犯罪率
- 涉及问题：种族歧视

**German Credit**
- 特征：账户信息、信用记录、贷款目的、就业情况、人口统计资料等
- 输出：判断客户是否为优质客户
- 涉及问题：性别歧视

我们可以用同样的方法进行攻击，攻击结果如下：

#### Communities & Crime 数据集
- **LIME**
- **SHAP**

从结果可以看出，两种可解释性方法都认为除人种外的其他因素在模型决策时更加重要，而模型认为重要的因素，比如母语是否为英语、孩子数量等在我们人类看来往往并不要紧。

#### German Credit 数据集
- **LIME**
- **SHAP**

从结果可以看出，两种可解释性方法都认为除性别外的其他因素在模型决策时更加重要，而模型认为重要的因素，比如存款余额、是否为外国工作人员等在我们人类看来往往并不要紧。

## 参考
1. 深度学习模型可解释性研究综述
2. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods