Table 2: Outline of the complete experiment for each participant. Each participant undergoes two sessions: in the first one
we compute a user-specific calibration (T0), in the second one we load a random calibration profile. Within one session, each
participant completes four tasks: slideshow (T2, T8), reading (T3, T9), browsing (T4, T10) and slideshow again (T5, T11). During
each task we vary the screen brightness, the ambient light, or both. At the beginning and at the end of each session, we measure
the calibration error with a validation procedure (T1, T6, T7, T12). The table reports the tasks in chronological order (from the
top) and the two sessions for one participant are collected at least two hours apart from each other.
measures the accuracy of the calibration (T1). In order to obtain
precise data, we repeat the calibration (T0 and T1) until the mean
calibration error across both eyes and the X- and Y-direction is
less than 1 degree in the validation phase. We then save both the
calibration accuracy and the calibration coefficients of the accepted
calibration. We perform another 4-point validation at the end of the
session (T6) to test whether the tracking accuracy changed over the
course of the session (e.g., due to changes in posture or excessive
head movements).
In the second session, rather than computing a user-specific
calibration, we load a different participant’s calibration profile in-
stead. While the position of the screen and eye tracker are fixed
for each session, the remaining factors affecting the calibration are
uncontrolled (e.g., participants height, posture, head angle). Simi-
larly to the first session, we measure the accuracy resulting from
the different calibration profile with the same 4-point validation
procedure, both at the beginning (T7) and at the end of the session
(T12). We always use the previous participant’s calibration profile,
i.e., the calibration profile of the participant who was measured last.
The reason we use the previous user’s calibration, rather than a
single generic calibration for all users, is to limit the effect of how
the calibration is chosen. As the eye tracker can not retroactively
apply different calibrations to raw video data, we are limited to one
calibration setting per session. Throughout the rest of the paper, we
refer to these two sessions, the one with a user-specific calibration
and the one using a different user calibration, as the calibrated and
the uncalibrated sessions, respectively.
Task selection. Similar to previous work [11, 12], we choose three
main tasks inspired by day-to-day activities: reading (T3 and T9 in
Table 2), web browsing (T4, T10) and an image slideshow (T2, T5,
T8, T11). Each task lasts approximately 3-5 minutes, after which the
Figure 2: Four images used in the slideshow task. Images are
sorted by increasing brightness. The brightnesses computed
with the root mean squared method are 47.4, 123.8, 142.2,
192.2 (left to right, top to bottom).
experiment continues automatically. The reading task consists of
reading an excerpt of Alice in Wonderland [4]. The text is shown in
a centred column on the screen on a grey background. We instructed
users to flip pages using the keyboard once they reached the end
of a page. Typically, users flipped page around three times during
the task. For the web browsing task, users browse Wikipedia: they
are shown a random Wikipedia article and asked to use (chains
of) links within the article to reach a target (Wikipedia) article.
This type of activity involves both skimming and reading and is
therefore similar to typical browsing patterns. During the slideshow
task, users watch a sets of images in a slideshow, where each image
is shown for two seconds before being substituted with the next
one. While conceptually very close to the videos used in previous
Session 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1191work (e.g., [12, 32]), using images instead of videos allows to better
control the variation of brightness levels. We repeat the slideshow
task twice within one session to collect additional information
about the effect of varying light (see next paragraph). We choose a
set of nature-themed images while filtering images that may elicit
extreme user responses, such as spiders. Figure 2 shows four of
the images used in the slideshow, sorted in terms of increasing
brightness (left to right, top to bottom).
Light variability. As mentioned before, two factors mainly affect
the amount of light perceived by the pupil: the screen brightness
and the ambient light. Therefore, within one session, we vary one
or both factors within each task. In particular, for the first repetition
of the slideshow (T2, T8) we increase the brightness of the images
shown on the screen (these are sorted beforehand and shown in in-
creasing brightness order) while keeping the ambient light constant.
During the reading and browsing tasks (T3, T4, T9, T10), the screen
brightness is constant (the largely text-based nature of both tasks
results in negligible brightness differences), but we increase the
amount ambient light. For the second repetition of the slideshow
(T5, T11), we instead choose a random order for the images, while
again increasing the amount of ambient light. We determine an
image’s brightness by calculating the average root mean squared
pixel brightness of its greyscale representation. We always vary
the light (both ambient and screen) with increments rather than
decrements. We choose this ascending order as the pupil’s adap-
tation to increasing light is near-instantaneous, whereas adaption
to darkness occurs over time. For tasks with increasing ambient
light, we linearly increase the brightness of the desk lamp from the
minimum value to the maximum one. Varying the amount of light
which the participant is subjected to allows us to re-create realistic
uncontrolled lighting conditions.
3.3 Data Collection Process
We recruited 22 participants (11 male, 11 female) from the general
public, the only selection criteria were a minimum age of 18 and
normal or corrected-to-normal vision. The age distribution and the
presence of glasses and contact lenses are shown in Figure 3. We
collect whether the user is wearing glasses or contact lenses as we
found that, these often lead to less precise calibrations (due to the
glasses lenses reflecting or altering the reflection of the infrared
light used by the eye tracker). We advertised the study through
social media and participants were compensated for their time. The
data collection was approved by Oxford’s Interdivisional Research
Ethics Committee, reference R50977/RE002.
4 METHODS
In this section, we present the methods used to authenticate users
based on their eye movement patterns. The source code for each
of the steps and the data needed to precisely reproduce our results
are available online.
4.1 Preprocessing
The SMI RED500 used for this study reports two different types of
samples: raw gaze samples and fixation events. Raw samples are
measured at a rate of 500Hz while fixations are computed auto-
matically as they occur. Raw samples consist of a timestamp, X/Y
Figure 3: Age and eye sight correction (glasses, lenses or nei-
ther) distribution among the experiment participants. Note
that each participant’s session is counted separately as some
participants wore glasses only for one of them.
coordinates and the pupil diameter. Since we use the tracker in
binocular mode, the coordinates and pupil diameters are reported
separately for each eye. For some samples, the eyetracker is unable
to determine the pupil diameter for one or both eyes (which leads to
them being reported as zero). This indicates an incorrectly tracked
sample (e.g., following or during a blink). We therefore discard
these samples. Fixations are calculated by the eyetracker using a
proprietary algorithm. Conceptually, raw samples are clustered
into fixations if they occur within a short window of low-velocity
movements. Each fixation is associated with a centre point as well
as start and end timestamps which enable us to find the associated
raw samples. We only use an event if it contains at least 10 samples.
This filters both unnaturally short fixations (10 samples correspond
to 20 milliseconds) and those with an excessive number of missing
or corrupt samples. Since our features are based on fixations, we
discard all raw samples not belonging to a fixation (i.e., saccades,
blinks and various noise).
4.2 Feature Extraction
Following the preprocessing, we compute a set of features for each
fixation, so that each fixation leads to a feature vector used by the
system classifier. In our data, we observe roughly five fixations per
second on average, which leads to five biometric samples per second.
In the following we describe the features used in our system.
Spatial-based features. These features relate to the spatial distri-
bution of samples within a fixation. To capture the size of a fixation,
we calculate each sample’s distance to the fixation centre and use
this measure’s 10th percentile, 90th percentile, mean and standard
deviation as features. As a measure of a fixation’s shape, we com-
pute the maximal pairwise distance between any two (potentially
not consecutive) samples in the fixation. We use both Euclidean
distance as well as individual distance in X and Y direction.
Temporal-based features. These features represent the eye move-
ment speed during fixations. We measure pairwise speed and accel-
eration between consecutive samples and use the 10th percentile,
90th percentile, mean and standard deviation as features. We also
compute the duration of a fixation.
20-2930-3940-49Age51015202530FrequencyParticipant Age DistributionContact LensesGlassesNeitherSession 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1192Pupil-based features. Pupil features of the min, max and mean of
the pupil diameter of the respective eye during the fixation. As pupil
diameter measurements are far less noisy than coordinates, we use
the min and max values, rather than percentiles. An individual’s
pupil diameter is not constant over their lifespan, in fact, as a person
ages, their pupil diameter shrinks [6]. However, the timescale of
these changes is too long to significantly impact the authentication
system. A far bigger concern is its susceptibility to light. Both the
light of the screen (which is changed by the brightness of the image
shown) and ambient light change an individual’s pupil size. The
issue of screen light has been previously identified as a problem for
eye movement authentication [12] and changing ambient light has
been used as an attack vector [18]. We address these challenges in
Section 4.3.
Binocular-based pupil features. We augment the set of features
leveraging the binocular tracking offered by the eye tracker. In
particular, we focus on pupil based features for each individual
eye as medical work has shown that, even given stable lighting
conditions, the pupil diameter of the left and right eye is not always
identical [30]. Besides possible differences in actual pupil size, the
tracking itself may cause differences between the left and right eye
(e.g., depending on the user’s posture, or head inclination). We use
the min, max and mean tracking difference between the left and
right eye as well as the difference in pupil size as binocular features.
Measuring feature quality. In order to compute the distinctive-
ness of each feature, we use the Relative Mutual Information (RMI).
The RMI is defined as follows:
RMI(uid, F) = H(uid) − H(uid|F)
H(uid)
where H(A) is the entropy of A and H(A|B) denotes the entropy of A
conditioned on B. The uid (i.e., the set of user identities) is discrete,
but the feature space for most features is continuous. Therefore,
binning would be required to discretize the features (as is done
in, e.g., [11, 12]). However, the reported RMI would depend on
the binning strategy and number of bins (with more bins leading
to a higher calculated RMI). To avoid this problem, we use the
non-parametric approach proposed by Ross et al. to estimate the
mutual information between the discrete user ID and continuous
features [33].
4.3 Pupil diameter correction
We use linear regression to model the pupil’s response to changing
levels of light, both screen brightness and ambient light. Figure 4
shows an example of the effect of increasing ambient light on one
participants pupil diameter. As expected, across the entire dataset,
we find on average a negative correlation between pupil diameter
and amount of light: r-values of −0.5818 ± 0.05 and −0.2140 ± 0.07
for screen brightness and ambient light, respectively. In order to in-
fer an approximation of one user’s sensitivity to screen brightness,
we use the data recorded during first slideshow task (T2 and T8, sep-
arately for the two sessions) to fit a regression model. As described
in Section 2.3, during these tasks the ambient light is constant, al-
lowing us to isolate the effect of varying screen brightness. We pair
each pupil diameter measurement with the image on the screen
brightness at the time it was recorded. Using all of these pairs, we
Figure 4: Relationship between (increasing) screen bright-
ness and measured pupil diameter during the first slideshow
task (T2) under constant ambient light (r = −0.8417).
use linear regression to determine the slope of the fitted line (see
Figure 4 for an illustration). For each newly recorded sample, we
determine the corrected pupil diameter diamcor as follows:
diamcor = diamr aw − sscr ∗ brscr ,
(1)
where diamr aw is the original measurement, sscr is the slope
obtained through linear regression and brscr is the screen bright-
ness at the time the sample was measured. We use the same method
to establish the user’s sensitivity to ambient light, by using the dim
setting of the lamp as input. In this case, for reading (T3, T9) and
browsing (T4, T10) tasks we use data from the same task to fit the
regression, while for the second slideshow (T5, T11) we use the
data from the reading task. This results in a separate slope, samb.
With these two adjustment factors, we can correct for both image
brightness and ambient light changes as follows:
diamcor = diamr aw − samb ∗ bramb − sscr ∗ brscr .
(2)
In particular we use the complete correction for the second
slideshow tasks (T5, T11), where both image brightness and ambient
light change over the course of the task. We attempted to fit a
single light-sensitivity model considering the totality of users, but
we found in our data that the slope coefficients samb, sscr vary
greatly across users, suggesting that individual models will perform
significantly better.
4.4 Cross-task feature prediction
Medical work shows that eye movement patterns vary according to
the task performed by the user. The fixation duration is of particular
interest and well-studied for a variety of tasks [34]. While task-
specific changes in pupil diameter are partially corrected through
the light-based adjustment proposed in the previous section, the
task itself can also have an influence.
This method follows the assumption that task-specific changes
in feature distributions exhibit a certain consistency between users.
For example, fixation times are expected to be longer when reading
a text compared to watching a video. Naturally, the magnitude of
these differences can be user-specific and any prediction without
user-specific knowledge will be an approximation.
Eberz et al. presented a system to automatically predict changes
in Electrocardiography (ECG) features caused by different measure-
ment devices [9, 10]. We adapt their extended approach to predict
406080100120140160180Average screen brightness level3.54.04.55.05.56.0Pupil DiameterRelationship between screen brightness and pupil diameterSession 6A: Biometrics SecurityCCS ’19, November 11–15, 2019, London, United Kingdom1193changes in features caused by changing tasks (rather than different
devices). The goal is to train the user model on a source task and
authenticating on a different target task without the need for re-
enrolment. In order to achieve this, we use population data to train
a mapping function that transforms a feature vector measured in
the source task to account for the task-specific changes expected
during the target task.
The core of this method is to find an optimal mapping, i.e., a set
: R −→ R, such
of transformation functions F = { fj}j∈J with fj
that, for each feature j and subject i, they minimise the statisti-
cal distance between the transformed source distribution fj(DS
i ,j)
and the corresponding target distribution DT
i ,j. In other words, fj
transforms values of feature j from task S in order to be as close
as possible, statistically speaking, to the values of the same feature
from task T . As in [10], we restrict the search to linear functions,
of the form:
fj(x) = ajx + bj
(3)
Naturally, a mapping function can not be specific to the user, as it
would require samples from both tasks to train it and if these were
available one could train on the target task directly. Instead, we find
the ideal mapping function for the set of remaining users in a leave-
one-out fashion. In practice, this means that a mapping function
for any task combination can be derived based on population data.
4.5 Authentication pipeline
Following feature extraction (see Section 4.2), we use the following
methods in our experimental evaluation.
Training data selection. Given all the data collected for a certain
user, the training (enrolment) data for the authentication system
can be chosen either randomly or sequentially. Previous work has
shown that random selection (e.g., repeated random sampling or
stratified cross-validation) leads to greatly overstated performance
as the temporal distance between training and testing samples is
kept artificially low [1, 13]. In order to avoid this, within each task,
we select the first part of the data for training and all following
samples for testing. We analyze the effect of varying training data
amounts in the following section. For completeness and easier
comparison with previous work, we also report the results obtained
with random training data selection. For cross-task authentication,
we select the complete source task for training and the complete
target task for testing.
Pupil diameter correction. We apply the method described in
Section 4.3 to both the training and testing data. In the following
section, we report results with and without this correction.
Cross-task mapping function. For cross-task authentication (i.e.,
different training and testing tasks), we apply the corresponding