on encrypted data in a scenario where a mobile, bandwidth-
restricted user wishes to store data on an untrusted server.
The scheme requires the user to split the data into ﬁxed-
size words and perform encryption and other transforma-
tions. Drawbacks of this scheme include ﬁxing the size of
words, the complexities of encryption and search, the in-
ability of this approach to support access pattern privacy,
or retrieval correctness. Eu-Jin Goh [28] proposes to asso-
ciate indexes with documents stored on a server. A docu-
ment’s index is a Bloom ﬁlter [15] containing a codeword
for each unique word in the document. Chang and Mitzen-
macher [20] propose a similar approach, where the index
associated with documents consists of a string of bits of
length equal to the total number of words used (dictionary
size). Boneh et al.[16] proposed an alternative for senders to
encrypt e-mails with recipients’ public keys, and store this
email on untrusted mail servers. Golle et al.[30] extend the
above idea to conjunctive keyword searches on encrypted
data. The scheme requires users to specify the exact posi-
tions where the search matches have to occur, and hence is
impractical. Brinkman et al.[18] deploy secret splitting of
polynomial expressions to search in encrypted XML.
We ﬁnd we can efﬁciently (and trivially) achieve the
spirit of this goal – running searches with full privacy on
“outsourced” data – by adjusting the model to redeﬁne “out-
sourced”. Speciﬁcally, we provide a model that achieves the
bulk of the cost savings of outsourcing while retaining the
performance beneﬁts of locally managed data.
3 Model
We provide details of the participants in this protocol, the
required transaction semantics, and the cryptographic prim-
itives employed.
Parties
Provider/Server. The provider owns durable storage, and
would like to provide use of this storage for a fee. The
provider, being hosted in a well-managed data center, also
has high availability. We will investigate ways that clients
can make use of these attributes.
Since the provider has different motivations than the
clients, we assume an actively malicious provider. How-
ever, we do not try to prevent denial of service behavior
from the provider. There are techniques beyond the scope
of this paper, that can be employed to help clients detect
denial of service behavior, such as attaching timestamps to
messages to measure server latency.
Clients. In our model, the clients are a set of trusted parties
who must run transactions on a shared database with full
ACID guarantees. Since storage is cheap, each client has a
local hard disk to use as working space; however, due to the
fragile nature of hard disks, we do not assume this storage is
permanent. Additionally, the clients would like to perform
read queries as efﬁciently as possible without wasting net-
work bandwidth or paying network latency costs. Each of
the trusted parties would also like to be able to continue run-
ning transactions even when the others are ofﬂine, possibly
making use of the provider’s high availability.
The clients would like to take advantage of the dura-
bility of the provider’s storage, but they do not trust the
provider with the privacy or integrity of their data. Speciﬁ-
cally, the provider should be prevented from observing any
of the distributed database contents. We deﬁne a notion of
consistency between the clients’ database views to address
integrity. It is not imperative that all clients see exactly the
same data as the other clients at exactly the same time, how-
ever, they need to agree on the sequence of updates applied.
We deﬁne tracec,i to be the series of the ﬁrst i transactions
applied by client c to its local database copy. Clients c and
d are considered i-trace consistent if tracec,i = traced,i.
In some scenarios the provider might be able to parti-
tion the set of clients, and maintain separate versions of
the database for each partition. This partitioning attack has
been examined in previous literature; if there are non-inter-
communicating asynchronous clients, the best that can be
guaranteed is fork consistency, ﬁrst deﬁned by Mazi`eres
and Shasha in [51]. Any adopted solution should guarantee
that the data repository is fork consistent; that is, all clients
within each partition agree on the repository history. This
guarantee is not as weak it may appear to be on the surface,
as once the provider has created a partition, the provider
must block all future communication between partitioned
clients, or else the partition will be immediately detected.
We assume that clients do not leak information through
transaction timing and transaction size. Clients in real life
may vary from this with only minimal loss of privacy, but
we use a timing and size side-channel free model for illus-
tration purposes.
The ﬁrst part of this paper assumes a potentially mali-
cious provider, but trusted clients. Section 6.1 relaxes this
assumption to provide protection against not only a poten-
tially malicious provider, but against malicious clients at the
same time.
Transaction Semantics
We provide a general protocol that supports nearly any class
of transaction. Transactions can be simple key-value pair
updates, as in a block ﬁle system, or they can be full SQL
transactions. Clients can even buffer many local updates
over a long period of time, e.g. when the client is discon-
nected, and then send them as a single transaction. The only
requirements for using this protocol are that the underlying
transaction-generating system implement the following:
RunAndCommitLocalTransaction(Transaction T )
applies transaction t to the local database and commits.
DetectConflict(TransactionHandle h, Transaction
C) returns true if the external (program-visible) out-
come of Transaction Th would have been different had
transaction C been issued before Th. It does not matter
whether the local database contents would be different;
all that matters is whether the transaction issuer would
have acted differently.
Retry(TransactionHandle h) rolls back all changes (in
the local database, and any side-effects external to
the database) for uncommited transaction Th and reat-
tempts the transaction.
RollbackLocal(TransactionHandle h) rolls back local
database changes from uncommited transaction Th.
We provide the following interface to the transaction-
running system:
DistributeTransaction(Transaction T , Transac-
tionHandle h) returns once transaction T has been suc-
cessfully committed to the global database image. Im-
plementations of this command will invoke the call-
backs above.
Conﬂicts
In the protocols described later, multiple clients will simul-
taneously run transactions that ultimately end up in a dif-
ferent order than the clients see. Therefore, we deﬁne the
notion of conﬂicts to indicate whether this re-ordering af-
fects the client computation.
We say transactions a and b conﬂict if changing the or-
der of these transactions affects the return value of one of
the operations, or the client state that results from execut-
ing these operations. This deﬁnition allows us to provide
the highest level of transactional isolation, serializability, as
deﬁned in the ANSI SQL-92 standard [12]. Client imple-
mentations may substitute a lower isolation level, such as
one deﬁned in [11], to improve performance by reducing
the number of conﬂicts.
For example, if this system represents a block ﬁle sys-
tem, clients may abort transactions that read the value of
If
a block that is written in a prior, pending transaction.
this system represents a relational database, clients may use
row-based or table-based conﬂict detection. Alternatively,
some transactions could be implemented as PL/SQL pro-
cedures that avoid sending any values back to the initiator,
avoiding any possibility of conﬂicts altogether!
For correctness, there must be no false negatives returned
by the DetectConflict command (no client decides
its transaction is conﬂict-free based on the transaction list,
when it in fact is not). Of course, the deﬁnition of conﬂict-
free depends on the particular implementation. For optimal
performance, the number of false positives returned by the
DetectConflict command must also be low.
We require several cryptographic primitives with all the
associated semantic security [29] properties: (i) a secure,
collision-free hash function which builds a distribution from
its input that is indistinguishable from a uniform random
distribution (we use the notation h(x)), (ii) an encryption
function that generates unique ciphertexts over multiple en-
cryptions of the same item, such that a computationally
bounded adversary has no non-negligible advantage at de-
termining whether a pair of encrypted items of the same
length represent the same or distinct items, (iii) a pseudo
random number generator whose output is indistinguishable
from a uniform random distribution over the output space,
and (iv) a recursive hash chain construction used to incre-
mentally build a secure hash value over a sequence of items.
The ﬁrst part of this paper assumes a potentially mali-
cious provider, but trusted clients. All transactions are en-
crypted by a symmetric key shared by the set of clients,
and kept secret from the provider. Message authentication
prevents tampering, and the use of a versioning structure
guarantees database cache consistency. A strawman proto-
col introduces solution by providing the security guarantees
trivially using a global lock (Section 4). Our main result
is a protocol providing these guarantees using an optimistic
wait-free protocol (Section 5). We then describe several ex-
tensions to this protocol, including in Section 6.1 protec-
tion against not only a potentially malicious provider, but
against malicious clients as well. Finally, our implemen-
tation shows how this protocol can be layered on top of
existing SQL-based relation database management systems
while obtaining practical performance overheads.
4 Strawman protocol:
outsourced
durability with a global lock
We start by illustrating the main concepts through a straw-
man protocol that allows multiple clients to serialize their
transactions through an untrusted server – transaction atom-
icity being guaranteed through a single global lock. Natu-
rally, in practice, global locking is not a viable option as it
would constitute a signiﬁcant bottleneck. Our main result,
described in Section 5, is optimistic and lock-free.
An encrypted transaction log is the central data struc-
ture in all versions of this model. This log is the deﬁnitive
representation of the database; the protocols described here
simply allow clients to append to this log in a safe, parallel
manner while preventing the potentially malicious provider
from interfering with operations on the log.
At a high level, in this strawman protocol, clients main-
tain their own copy of the database in local temporary stor-
age. They perform operations on this copy and keep it syn-
chronized with other clients. Clients that go ofﬂine and
come back online later, obtain a client-signed replay log
from the untrusted server in charge of maintaining the log.
4.1 Transaction Protocol:
the lock-based
DistributeTransaction
Informally, to run a transaction a client (1) “reserves” a slot
in the permanent transaction log, (2) waits for the log to
“solidify” up to its assigned slot, (3) runs the transaction,
(4) “commits” that slot by sending out a description of the
transaction to the untrusted server. The untrusted server
then archives and distributes this encrypted transaction.
1. The client issues a “request slot” slot reservation com-
mand to server, along with a number l representing the
last slot the client knows about (has seen updates for).
The server assigns the next available transaction slot s
to the client. The server sends back to the client this
slot number s, with a list of all commits since the last
update Tl received by the client, Tl+1 . . . T j. Note that
j < s − 1 if there are clients reserving slots that have
not yet committed at the instant the server issues the
response to this message.
The client
2. The client waits until all transactions Tj+1 . . . Ts−1
in slots before its assigned slot have commit-
ted.
receives the updates from the
server as they come in. After verifying check-
sums and authentication tokens (hash chain and
signatures; see below) on each update,
the client
applies them to its local database copy (using
RunAndCommitLocalTransaction) in order.
3. Once the client has received Ts−1, it has in effect ob-
tained a global lock, since all other clients are now
waiting for it to perform a transaction. The client
now runs its own transaction on its local copy of the
database, recording the sequence of updates.
4. The client commits (relinquishing the lock) by sending
a complete encrypted description of the transaction up-
dates Ts back to the server (which will relay it back to
the other clients).
Finally, each client c applies transaction Ti (using
RunAndCommitLocalTransaction) to its database
(i) the contents
once all the following conditions hold:
of Ti have a valid signature from a valid client (using
client-shared symmetric key K), (ii) the client has ap-
plied transactions T1 . . . Ti−1, and (iii) the hash chain link
Ti−1.hashchain matches the client’s own computation of
link HCi−1.
Each transaction Ti is encrypted and signed using a sym-
metric key K shared by all clients.
It contains the fol-
lowing ﬁelds: desc= a transaction description, e.g., a se-
quence of SQL statements; hashchain= the signed hash
chain link HCi−1, verifying the sequence of transactions
T1 . . . Ti−1. HCi is calculated as Hk(HCi−1||Ti−1.desc),
with HC0 = Hk(∅).
hashchain := Hk(∅)
hashchain pos := 0
k := ClientSharedKey
Global variables
Result: Retrieves and runs the next waiting transaction
t := server getNextTransaction()
if !verifySignature(t) then
return ⊥
end
T := decrypt(t)
if T.hashchain != hashchain then
return ⊥
end
RunAndCommitLocaltransaction(T.desc)
hashchain := Hk(hashchain || T.desc)
hashchain pos ++
Procedure GetIncomingTransaction
Result: Initiates a transaction
s := server requestSlot()
while hashchain pos < s-1 do
GetIncomingTransaction()
end
RunAndCommitLocalTransaction(d)
T := new Transaction
T.desc := d
T.hashchain := hashchain
server commit(Enc(k,T))
Procedure DistributeTransaction(d)
4.2 Correctness
We now show that this protocol is correct and offers fork
consistency: all clients are trace consistent (their transaction
traces are identical) as long as the server has not partitioned
the clients. If the server has partitioned the clients, then all
clients within a partition will be trace consistent.
5.1 Transaction Protocol:
the
DistributeTransaction
lock-free
Theorem 1. If client c applies an update Ti of client d, then
clients c and d are i-trace consistent.
Proof. Assume that client c has applied Ti from client d,
but suppose that tracec,i differs from traced,i. W.l.o.g.
assume that at position a, tracec,i contains transaction Ta
while traced,i has a different value T ′
a. Client c’s computa-
tion of HCa therefore differs from client d’s computation of
HCa, since it involves a collision-free hash function. Addi-
tionally, the collision-free property guarantees that any later
link in these hash chains will also differ, and client c’s com-
putation of HCi−1 differs from client d’s computation of
HCi−1. Since client c has applied Ti, it had to have suc-
cessfully veriﬁed that Ti did indeed originate from client d.
However, as a pre-condition to client c applying Ti, the hash
chain link Ti−1.hashchain, which is client d’s computation
of HCi−1, would have to match client c’s own calculation
of HCi−1, giving us a contradiction.