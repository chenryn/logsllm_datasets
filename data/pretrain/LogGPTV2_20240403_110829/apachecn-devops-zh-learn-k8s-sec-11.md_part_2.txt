请正确选择审核级别。更详细的日志可以更深入地了解正在执行的活动。但是，处理审计事件确实会增加存储和时间成本。值得一提的是，如果对 Kubernetes 机密对象设置一个请求或一个`RequestResponse`审核级别，则该机密内容会被记录在审核事件中。如果将包含敏感数据的 Kubernetes 对象的审核级别设置为比元数据更详细，则应使用敏感数据编校机制，以避免在审核事件中记录机密。
Kubernetes 审核功能提供了很大的灵活性，可以按对象种类、名称空间、操作、用户等来审核 Kubernetes 对象。由于默认情况下不启用 Kubernetes 审核，接下来我们来看看如何启用 Kubernetes 审核并存储 audi t 记录。
## 配置审计后端
为了启用 Kubernetes 审核，您需要在启动`kube-apiserver`时通过审核策略文件的`--audit-policy-file`标志。有两种类型的审核后端可以配置为使用流程审核事件:日志后端和 webhook 后端。让我们来看看他们 。
### 日志后端
日志后端将审核事件写入主节点上的文件。以下标志用于在`kube-apiserver`内配置日志后端:
*   `--log-audit-path`:指定主节点上的日志路径。这是打开或关闭日志后端的标志。
*   `--audit-log-maxage`:指定保存审核记录的最大天数。
*   `--audit-log-maxbackup`:指定主节点上要保留的审核文件的最大数量。
*   `--audit-log-maxsize`:指定审核日志文件旋转前的最大大小(以兆字节为单位)。
让我们来看看 w ebhook 后端。
### 网络钩子后端
网络钩子后端将审计事件写入注册到`kube-apiserver`的远程网络钩子。要启用网络钩子后端，需要用网络钩子配置文件设置`--audit-webhook-config-file`标志。启动`kube-apiserver`时也指定了该标志。下面是一个为 Falco 服务注册 webhook 后端的 webhook 配置示例，稍后将对此进行更详细的介绍:
```
apiVersion: v1
kind: Config
clusters:
- name: falco
  cluster:
    server: http://$FALCO_SERVICE_CLUSTERIP:8765/k8s_audit
contexts:
- context:
    cluster: falco
    user: ""
  name: default-context
current-context: default-context
preferences: {}
users: []
```
`server`字段(`http://$FALCO_SERVICE_CLUSTERIP:8765/k8s_audit`)中指定的网址是审计事件将发送到的远程端点。从 1.13 版本的 Kubernetes 开始，可以通过`AuditSink`对象动态配置 webhook 后端，目前还处于 alpha 阶段。
在本节中，我们通过介绍审计策略和审计后端来讨论 Kubernetes 审计。在下一节中，我们将讨论 Kub ernetes 集群中的高可用性。
# 在 Kubernetes 集群中实现高可用性
可用性是指用户访问服务或系统的能力。系统的高可用性确保了系统正常运行时间的约定水平。例如，如果只有一个实例来服务该服务，并且该实例已关闭，则用户无法再访问该服务。高可用性服务由多个实例提供。当一个实例关闭时，备用实例或备份实例仍然可以提供服务。下图描述了具有和不具有高可用性的服务:
![Figure 11.1 – Services with and without high availability ](img/B15566_11_001.jpg)
图 11.1–有无高可用性的服务
在 Kubernetes 集群中，通常会有多个工作节点。集群的高可用性得到保证，因为即使一个工作节点停机，也有一些其他工作节点来承载工作负载。然而，高可用性不仅仅是在集群中运行多个节点。在本节中，我们将从三个层面来看 Kubernetes 集群中的高可用性:工作负载、Kubernetes 组件和 clo ud 基础架构。
## 实现 Kubernetes 工作负载的高可用性
对于 Kubernetes 工作负载，如部署和 StatefulSet，您可以在规范中指定`replicas`字段，以了解有多少复制的单元正在为微服务运行，控制器将确保在集群中的不同工作节点上运行`x`数量的单元，如`replicas`字段中所指定的。DaemonSet 是一种特殊的工作负载；控制器将确保集群中的每个节点上都有一个 pod 在运行，假设您的 Kubernetes 集群有多个节点。因此，在部署或状态集中指定多个副本，或者使用 DaemonSet，将确保工作负载的高可用性。为了保证工作负载的高可用性，还需要保证 Kubernetes 组件的高可用性。
## 实现 Kubernetes 组件的高可用性
高可用性也适用于 Kubernetes 组件。让我们回顾几个关键的 Kubernetes 组件，如下所示:
*   `kube-apiserver`:Kubernetes API 服务器(`kube-apiserver`)是一个控制平面组件，用于验证和配置 pods、服务和控制器等对象的数据。它使用**代表状态转移** ( **休息**)请求与对象交互。
*   `etcd` : `etcd`是高可用性键值存储，用于存储配置、状态和元数据等数据。其`watch`功能为 Kubernetes 提供了监听配置更新并进行相应更改的能力。
*   `kube-scheduler` : `kube-scheduler`是 Kubernetes 的默认调度程序。它监视新创建的荚，并将荚分配给节点。
*   `kube-controller-manager`:Kubernetes 控制器管理器是核心控制器的组合，它们监视状态更新并相应地对集群进行更改。
如果`kube-apiserver`关闭，那么基本上您的集群也关闭了，因为用户或其他 Kubernetes 组件依赖与`kube-apiserver`的通信来执行它们的任务。如果`etcd`关闭，集群和对象的任何状态都不可用。`kube-scheduler`和`kube-controller-manager`对于确保工作负载在集群中正常运行也很重要。所有这些组件都在主节点上运行，以确保组件的高可用性。一种简单的方法是通过`kops`或`kubeadm`为 Kubernetes 集群调出多个主节点。你会发现类似这样的东西:
```
$ kubectl get pods -n kube-system
...
etcd-manager-events-ip-172-20-109-109.ec2.internal       1/1     Running   0          4h15m
etcd-manager-events-ip-172-20-43-65.ec2.internal         1/1     Running   0          4h16m
etcd-manager-events-ip-172-20-67-151.ec2.internal        1/1     Running   0          4h16m
etcd-manager-main-ip-172-20-109-109.ec2.internal         1/1     Running   0          4h15m
etcd-manager-main-ip-172-20-43-65.ec2.internal           1/1     Running   0          4h15m
etcd-manager-main-ip-172-20-67-151.ec2.internal          1/1     Running   0          4h16m
kube-apiserver-ip-172-20-109-109.ec2.internal            1/1     Running   3          4h15m
kube-apiserver-ip-172-20-43-65.ec2.internal              1/1     Running   4          4h16m
kube-apiserver-ip-172-20-67-151.ec2.internal             1/1     Running   4          4h15m
kube-controller-manager-ip-172-20-109-109.ec2.internal   1/1     Running   0          4h15m
kube-controller-manager-ip-172-20-43-65.ec2.internal     1/1     Running   0          4h16m
kube-controller-manager-ip-172-20-67-151.ec2.internal    1/1     Running   0          4h15m
kube-scheduler-ip-172-20-109-109.ec2.internal            1/1     Running   0          4h15m
kube-scheduler-ip-172-20-43-65.ec2.internal              1/1     Running   0          4h15m
kube-scheduler-ip-172-20-67-151.ec2.internal             1/1     Running   0          4h16m
```
现在您的有多个`kube-apiserver`PODS、`etcd`PODS、`kube-controller-manager`PODS 和`kube-scheduler`PODS 在`kube-system`命名空间中运行，它们运行在不同的主节点上。还有一些其他组件，如`kubelet`和`kube-proxy`在每个节点上运行，因此它们的可用性由节点的可用性来保证，而`kube-dns`默认与多个 pod 一起旋转，因此它们的高可用性得到了保证。无论您的 Kubernetes 集群是运行在公共云还是私有数据中心，基础架构都是支持 Kubernetes 集群可用性的支柱。接下来，我们将讨论云基础设施的高可用性，并以云提供商为例。
## 实现云基础设施的高可用性
云提供商通过位于不同地区的多个数据中心在全球范围内提供云服务。云用户可以选择在哪个地区和区域(实际的数据中心)托管他们的服务。区域和分区提供了与大多数类型的物理基础架构和基础架构软件服务故障的隔离。请注意，如果集群托管在云中，云基础架构的可用性也会影响 Kubernetes 集群上运行的服务。您应该利用云的高可用性，并最终确保运行在 Kubernetes 集群上的服务的高可用性。以下代码块提供了一个使用`kops`指定区域的示例，以利用云基础架构的高可用性:
```
export NODE_SIZE=${NODE_SIZE:-t2.large}
export MASTER_SIZE=${MASTER_SIZE:-t2.medium}
export ZONES=${ZONES:-"us-east-1a,us-east-1b,us-east-1c"}
export KOPS_STATE_STORE="s3://my-k8s-state-store2/"
kops create cluster k8s-clusters.k8s-demo-zone.com \
  --cloud aws \
  --node-count 3 \
  --zones $ZONES \
  --node-size $NODE_SIZE \
  --master-size $MASTER_SIZE \
  --master-zones $ZONES \
  --networking calico \
  --kubernetes-version 1.14.3 \
  --yes \
```
Kubernetes 集群的节点如下所示:
```
$ kops validate cluster
...
INSTANCE GROUPS
NAME			ROLE	MACHINETYPE	MIN	MAX	SUBNETS
master-us-east-1a	Master	t2.medium	1	1	us-east-1a
master-us-east-1b	Master	t2.medium	1	1	us-east-1b
master-us-east-1c	Master	t2.medium	1	1	us-east-1c
nodes			Node	t2.large	3	3	us-east-1a,us-east-1b,us-east-1c
```
前面的代码块显示了分别在`us-east-1a`、`us-east-1b`和`us-east-1c`可用性区域上运行的三个主节点。因此，作为工作节点，即使其中一个数据中心出现故障或处于维护状态，主节点和工作节点仍然可以在其他数据中心运行。
在本节中，我们讨论了 Kubernetes 工作负载、Kubernetes 组件和云基础架构的高可用性。让我们使用下图来回顾一下 Kubernetes 集群的高可用性:
![Figure 11.2 – High availability of Kubernetes cluster in the cloud ](img/B15566_11_002.jpg)
图 11.2–云中 Kubernetes 集群的高可用性
现在，让我们进入下一个主题，关于管理 Kubernetes 集群中的 secretts。
# 用金库管理机密
机密管理是一个大话题，为了帮助解决不同平台上的机密管理问题，已经开发了许多开源和专有的解决方案。所以，在 Kubernetes 中，它内置的`Secret`对象用来存储机密数据，实际数据和其他 Kubernetes 对象一起存储在`etcd`中。默认情况下，机密数据以明文(编码格式)存储在`etcd`中。`etcd`可以配置为静态加密机密。同样，如果`etcd`未配置为使用**传输层安全性** ( **TLS** )加密通信，机密数据也会以明文传输。除非安全要求很低，否则建议使用第三方解决方案来管理 Kubernetes 集群中的机密。
在本节中，我们将介绍 Vault，一个**云原生计算基金会**(**【CNCF】**)机密管理项目。Vault 支持机密的安全存储、动态机密的生成、数据加密、密钥撤销等。在本节中，我们将重点介绍如何使用 Vault 为 Kubernetes 集群中的应用存储和提供机密的使用案例。现在，让我们看看如何为 Kubernetes 集群设置保险库。
## 设置保险库
您可以使用`helm`在 Kubernetes 集群中部署 Vault，如下所示:
```
helm install vault --set='server.dev.enabled=true' https://github.com/hashicorp/vault-helm/archive/v0.4.0.tar.gz
```
注意`server.dev.enabled=true`已设置。这对于开发环境很好，但不建议在生产环境中设置。您应该看到两个 Pod 正在运行，如下所示:
```
$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
vault-0                                 1/1     Running   0          80s
vault-agent-injector-7fd6b9588b-fgsnj   1/1     Running   0          80s
```
`vault-0`舱是管理和存储机密的舱，而`vault-agent-injector-7fd6b9588b-fgsnj`舱负责将机密注入带有特殊拱顶注释的舱，我们将在*供应和旋转机密*部分更详细地展示。接下来，让我们为`postgres`数据库连接创建一个示例机密，如下所示:
```
vault kv put secret/postgres username=alice password=pass
```
请注意，前面的命令需要在`vault-0`Pod 中执行。由于您希望只限制 Kubernetes 集群中的相关应用访问该机密，因此您可能希望定义一个策略来实现这一点，如下所示:
```
cat  /home/vault/app-policy.hcl
path "secret*" {
  capabilities = ["read"]
}
EOF
vault policy write app /home/vault/app-policy.hcl
```
现在，您有一个策略定义了在`secret`路径下读取机密的权限，例如`secret` / `postgres`。接下来，您希望将策略与允许的实体相关联，例如 Kubernetes 中的服务帐户。这可以通过执行以下命令来完成:
```
vault auth enable kubernetes
vault write auth/kubernetes/config \
   token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
   kubernetes_host=https://${KUBERNETES_PORT_443_TCP_ADDR}:443 \
   kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
vault write auth/kubernetes/role/myapp \