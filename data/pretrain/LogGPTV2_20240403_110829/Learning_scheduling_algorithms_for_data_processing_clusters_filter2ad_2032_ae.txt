Finally, we demonstrate the wide range of scheduling policies Dec-
ima can learn, and break down the impact of our key ideas and tech-
niques on Decima’s performance. In appendices, we further evaluate
Decima’s optimality via an exhaustive search of job orderings (Ap-
pendix H), the robustness of its learned policies to changing en-
vironments (Appendix I), and Decima’s sensitivity to incomplete
information (Appendix J).
Learned policies. Decima outperforms other algorithms because it
can learn different policies depending on the high-level objective, the
workload, and environmental conditions. When Decima optimizes for
average JCT (Figure 13a), it learns to share executors for small jobs to
finish them quickly and avoids inefficiently using too many executors
on large jobs (§7.2). Decima also keeps the executors working on
tasks from the same job to avoid the overhead of moving executors
(§6.1). However, if moving executors between jobs is free — as is
effectively the case for long tasks, or for systems without JVM spawn
overhead — Decima learns a policy that eagerly moves executors
among jobs (cf. the frequent color changes in Figure 13b). Finally,
given a different objective of minimizing the overall makespan for a
batch of jobs, Decima learns yet another different policy (Figure 13c).
Since only the final job’s completion time matters for a makespan
objective, Decima no longer works to finish jobs early. Instead, many
jobs complete together at the end of the batched workload, which
gives the scheduler more choices of jobs throughout the execution,
increasing cluster utilization.
Impact of learning architecture. We validate that Decima uses all
raw information provided in the state and requires all its key design
components by selectively omitting components. We run 1,000 con-
tinuous TPC-H job arrivals on a simulated cluster at different loads,
and train five different variants of Decima on each load.
Figure 14 shows that removing any one component from Decima
results in worse average JCTs than the tuned weighted-fair heuristic
at a high cluster load. There are four takeaways from this result. First,
parallelism control has the greatest impact on Decima’s performance.
Without parallelism control, Decima assigns all available executors
to a single stage at every scheduling event. Even at a moderate cluster
load (e.g., 55%), this leads to an unstable policy that cannot keep up
with the arrival rate of incoming jobs. Second, omitting the graph em-
bedding (i.e., directly taking raw features on each node as input to the
score functions in §5.2) makes Decima unable to estimate remaining
work in a job and to account for other jobs in the cluster. Consequently,
Decima has no notion of small jobs or cluster load, and its learned
policy quickly becomes unstable as the load increases. Third, using
unfixed job sequences across training episodes increases the variance
in the reward signal (§5.3). As the load increases, job arrival sequences
become more varied, which increases variance in the reward. At clus-
ter load larger than 75%, reducing this variance via synchronized
termination improves average JCT by 2× when training Decima, illus-
trating that variance reduction is key to learning high-quality policies
279
Learning Scheduling Algorithms for Data Processing Clusters
SIGCOMM ’19, August 19-23, 2019, Beijing, China
(a) Average JCT objective.
(b) Avg. JCT, with zero-cost executor motion.
(c) Minimal makespan objective.
Figure 13: Decima learns qualitatively different policies depending on the environment (e.g., costly (a) vs. free executor migration (b)) and the objective (e.g.,
average JCT (a) vs. makespan (c)). Vertical red lines indicate job completions, colors indicate tasks in different jobs, and dark purple is idle time.
Figure 14: Breakdown of each key idea’s contribution to Decima with
continuous job arrivals. Omitting any concept increases Decima’s average
JCT above that of the weighted fair policy.
Setup (IAT: interarrival time)
Opt. weighted fair (best heuristic)
Decima, trained on test workload (IAT: 45 sec)
Decima, trained on anti-skewed workload (IAT: 75 sec)
Decima, trained on mixed workloads
Decima, trained on mixed workloads with interarrival time hints
Average JCT [sec]
91.2±23.5
65.4±28.7
104.8±37.6
82.3±31.2
76.6±33.4
Table 2: Decima generalizes to changing workloads. For an unseen workload,
Decima outperforms the best heuristic by 10% when trained with a mix of
workloads; and by 16% if it knows the interarrival time from an input feature.
in long-horizon scheduling problems. Fourth, training only on batched
job arrivals cannot generalize to continuous job arrivals. When trained
on batched arrivals, Decima learns to systematically defer large jobs,
as this results in the lowest sum of JCTs (lowest sum of penalties).
With continuous job arrivals, this policy starves large jobs indefinitely
as the cluster load increases and jobs arrive more frequently. Conse-
quently, Decima underperforms the tuned weighted-fair heuristic at
loads above 65% when trained on batched arrivals.
Generalizing to different workloads. We test Decima’s ability to
generalize by changing the training workload in the TPC-H experi-
ment (§7.2). To simulate shifts in cluster workload, we train models
for different job interarrival times between 42 and 75 seconds, and
test them using a workload with a 45 second interarrival time. As
Decima learns workload-specific policies, we expect its effectiveness
to depend on whether broad test workload characteristics, such as in-
terarrival time and job size distributions, match the training workload.
Table 2 shows the resulting average JCT. Decima performs well
when trained on a workload similar to the test workload. Unsur-
prisingly, when Decima trains with an “anti-skewed” workload (75
seconds interarrival time), it generalizes poorly and underperforms
the optimized weighted fair policy. This makes sense because Decima
incorporates the learned interarrival time distribution in its policy.
When training with a mixed set of workloads that cover the whole
interarrival time range, Decima can learn a more general policy. This
(a) Learning curve.
(b) Scheduling delay.
Figure 15: Different encodings of jobs parallelism (§5.2) affect Decima’s
training time. Decima makes low-latency scheduling decisions: on average,
the latency is about 50× smaller than the interval between scheduling events.
policy fits less strongly to a specific interarrival time distribution and
therefore becomes more robust to workload changes. If Decima can
observe the interarrival time as a feature in its state (§6.1), it gener-
alizes better still and learns an adaptive policy that achieves a 16%
lower average JCT than the best heuristic. These results highlight that
a diverse training workload set helps make Decima’s learned policies
robust to workload shifts; we discuss possible online learning in §8.
Training and inference performance. Figure 15a shows Decima’s
learning curve (in blue) on continuous TPC-H job arrivals (§7.2),
testing snapshots of the model every 100 iterations on (unseen) job
arrival sequences. Each training iteration takes about 5 seconds. Dec-
ima’s design (§5.3) is crucial for training efficiency: omitting the
parallelism limit values in the input (yellow curve) forces Decima to
use separate score functions for different limits, significantly increas-
ing the number of parameters to optimize over; putting fine-grained
parallelism control on nodes (green curve) slows down training as it
increases the space of algorithms Decima must explore.
Figure 15b shows cumulative distributions of the time Decima
takes to decide on a scheduling action (in red) and the time interval
between scheduling events (in blue) in our Spark testbed (§7.2). The
average scheduling delay for Decima is less than 15ms, while the
interval between scheduling events is typically in the scale of seconds.
In less than 5% of the cases, the scheduling interval is shorter than the
scheduling delay (e.g., when the cluster requests for multiple schedul-
ing actions in a single scheduling event). Thus Decima’s scheduling
delay imposes no measurable overhead on task runtimes.
8 Discussion
In this section, we discuss future research directions and other poten-
tial applications for Decima’s techniques.
280
ExecutorsAvg. JCT 67.3 sec, makespan119.6 secTime (seconds)0120609030ExecutorsAvg. JCT 61.4 sec, makespan114.3 secTime (seconds)0120609030ExecutorsTime (seconds)0120609030Avg. JCT 74.5 sec, makespan102.1 sec35%45%55%65%75%85%COuster OoDd50100150200250AverDJe JC7 (seconds)2pt. weiJhted fDirDecimD w/o JrDph embeddinJDecimD w/o pDrDOOeOism controODecimD trDined with bDtch DrrivDOsDecimD w/o vDriDnce reductionDecimDSIGCOMM ’19, August 19-23, 2019, Beijing, China
H. Mao et al.
Robustness and generalization. Our experiments in §7.4 showed
that Decima can learn generalizable scheduling policies that work
well on an unseen workload. However, more drastic workload changes
than interarrival time shifts could occur. To increase robustness of a
scheduling policy against such changes, it may be helpful to train the
agent on worst-case situations or adversarial workloads, drawing on
the emerging literature on robust adversarial RL [64]. Another direc-
tion is to adjust the scheduling policy online as the workload changes.
The key challenge with an online approach is to reduce the large sam-
ple complexity of model-free RL when the workload changes quickly.
One viable approach might be to use meta learning [22, 27, 29], which
allows training a “meta” scheduling agent that is designed to adapt
to a specific workload with only a few observations.
Other learning objectives. In our experiments, we evaluated Dec-
ima on metrics related to job duration (e.g., average JCT, makespan).
Shaping the reward signal differently can steer Decima to meet other
objectives, too. For example, imposing a hard penalty whenever the
deadline of a job is missed would guide Decima to a deadline-aware
policy. Alternatively, basing the reward on e.g., the 90th percentile of
empirical job duration samples, Decima can optimize for a tight tail of
the JCT distribution. Addressing objectives formulated as constrained
optimization (e.g., to minimize average JCT, but strictly guarantee
fairness) using RL is an interesting further direction [2, 30].
Preemptive scheduling. Decima currently never preempts running
tasks and can only remove executors from a job after a stage com-
pletes. This design choice keeps the MDP tractable for RL and results
in effective learning and strong scheduling policies. However, future
work might investigate more fine-grained and reactive preemption
in an RL-driven scheduler such as Decima. Directly introducing pre-
emption would lead to a much larger action space (e.g., specifying
arbitrary set of executors to preempt) and might require a much higher
decision-making frequency. To make the RL problem tractable, one
potential research direction is to leverage multi-agent RL [38, 50, 62].
For example, a Decima-like scheduling agent might controls which
stage to run next and how many executors to assign, and, concurrently,
another agent might decide where to preempt executors.
Potential networking and system applications. Some techniques
we developed for Decima are broadly applicable to other networking
and computer systems problems. For example, the scalable represen-
tation of input DAGs (§5.1) has applications in problems over graphs,
such as database query optimization [56] and hardware device place-
ment [3]. Our variance reduction technique (§5.3) generally applies
to systems with stochastic, unpredictable inputs [54, 55].
9 Related Work
There is little prior work on applying machine learning techniques
to cluster scheduling. DeepRM [53], which uses RL to train a neu-
ral network for multi-dimensional resource packing, is closest to
Decima in aim and approach. However, DeepRM only deals with
a basic setting in which each job is a single task and was evaluated
in simple, simulated environments. DeepRM’s learning model also
lacks support for DAG-structured jobs, and its training procedure
cannot handle realistic cluster workloads with continuous job arrivals.
In other applications, Mirhoseini et al.’s work on learning device
placement in TensorFlow (TF) computations [60] also uses RL, but
relies on recurrent neural networks to scan through all nodes for state
embedding, rather than a graph neural network. Their approach use re-
current neural networks to scan through all nodes for state embedding
instead of using a scalable graph neural network. The objective there
is to schedule a single TF job well, and the model cannot generalize
to unseen job combinations [59].
Prior work in machine learning and algorithm design has combined
RL and graph neural networks to optimize complex combinatorial
problems, such as vertex set cover and the traveling salesman prob-
lem [23, 49]. The design of Decima’s scalable state representation is
inspired by this line of work, but we found that off-the-shelf graph
neural networks perform poorly for our problem. To train strong
scheduling agents, we had to change the graph neural network ar-
chitecture to enable Decima to compute, amongst other metrics, the
critical path of a DAG (§5.1).
For resource management systems more broadly, Paragon [25] and
Quasar [26] use collaborative filtering to match workloads to different
machine types and avoid interference; their goal is complementary to
Decima’s. Tetrisched [74], like Decima, plans ahead in time, but uses
a constraint solver to optimize job placement and requires the user
to supply explicit constraints with their jobs. Firmament [33] also
uses a constraint solver and achieves high-quality placements, but
requires an administrator to configure an intricate scheduling policy.
Graphene [36] uses heuristics to schedule job DAGs, but cannot set
appropriate parallelism levels. Some systems “auto-scale” parallelism
levels to meet job deadlines [28] or opportunistically accelerate jobs
using spare resources [68, §5]. Carbyne [35] allows jobs to “altruisti-
cally” give up some of their short-term fair share of cluster resources
in order to improve JCT across jobs while guarantee long-term fair-
ness. Decima learns policies similar to Carbyne’s, balancing resource
shares and packing for low average JCT, but the current design of
Decima does not have fairness an objective.
General-purpose cluster managers like Borg [77], Mesos [41], or
YARN [75] support many different applications, making workload-
specific scheduling policies are difficult to apply at this level. However,
Decima could run as a framework atop Mesos or Omega [68].
10 Conclusion
Decima demonstrates that automatically learning complex cluster
scheduling policies using reinforcement learning is feasible, and that
the learned policies are flexible and efficient. Decima’s learning inno-
vations, such as its graph embedding technique and the training frame-
work for streaming, may be applicable to other systems processing
DAGs (e.g., query optimizers). We will open source Decima, our mod-
els, and our experimental infrastructure at https://web.mit.edu/decima.
This work does not raise any ethical issues.
Acknowledgments. We thank our shepherd, Aditya Akella, and the
SIGCOMM reviewers for their valuable feedback. We also thank
Akshay Narayan, Amy Ousterhout, Prateesh Goyal, Peter Iannucci,
and Songtao He for fruitful discussions throughout the development
of this project. We appreciate Haiyang Ding and Yihui Feng from
Alibaba Cloud Intelligence for sharing the production cluster dataset.
This work was funded in part by the NSF grants CNS-1751009,
CNS-1617702, a Google Faculty Research Award, an AWS Machine
Learning Research Award, a Cisco Research Center Award, an Alfred
P. Sloan Research Fellowship, and sponsors of the MIT DSAIL lab.
281
Learning Scheduling Algorithms for Data Processing Clusters
SIGCOMM ’19, August 19-23, 2019, Beijing, China
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G.
Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for
Large-scale Machine Learning. In Proceedings of the 12th USENIX Confer-
ence on Operating Systems Design and Implementation (OSDI). 265–283.