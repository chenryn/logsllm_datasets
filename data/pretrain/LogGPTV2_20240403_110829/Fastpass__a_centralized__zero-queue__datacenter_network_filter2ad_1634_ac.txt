arbiter allocates each timeslot 65 µs before the timeslot’s start time
to leave enough time for notiﬁcations to reach the endpoints; this
number is much smaller than the 1 millisecond before the secondary
takes over.
As a result, the old and new arbiters do not have to share informa-
tion to make the failover possible, simplifying the implementation:
the new arbiter just resynchronizes with endpoints and starts allocat-
ing.
5.2 Network failures
Arbiters need to know the network topology under their control,
so packets can successfully traverse the network in their allotted
timeslots. If links or switches become unavailable undetected, the
arbiter would continue to allocate trafﬁc through them, causing
packet loss.
Because Fastpass maintains low queue occupancies, it can use
packet drops to detect network faults. Sporadic packet losses due
to bit ﬂips in the physical layer still occur, but persistent, correlated
packet loss is almost surely due to component failure. The path each
packet traverses is known to the endpoints, which helps localize the
fault.
A network service performs fault isolation, separate from the
arbiter. Endpoints report packet drops to this fault isolation service,
which correlates reports from multiple endpoints to identify the
malfunctioning component. The faulty component is then reported
to the arbiter, which avoids scheduling trafﬁc through the component
until the fault is cleared. Since packet errors can be detected quickly,
failure detection and mitigation times can be made extremely short,
on the order of milliseconds.
5.3 Fastpass Control Protocol (FCP)
Communication between endpoints and the arbiter is not sched-
uled and can experience packet loss. FCP must protect against such
loss. Otherwise, if an endpoint request or the arbiter’s response is
dropped, a corresponding timeslot would never be allocated, and
some packets would remain stuck in the endpoint’s queue.
TCP-style cumulative ACKs and retransmissions are not ideal for
FCP. At the time of retransmission, the old packet is out of date: for
a request, the queue in the endpoint might be fuller than it was; for
an allocation, an allocated timeslot might have already passed.
FCP provides reliability by communicating aggregate counts; to
inform the arbiter of timeslots that need scheduling, the endpoint
sends the sum total of timeslots it has requested so far for that
destination. The arbiter keeps a record of each endpoints’ latest
demands; the difference from the kept record and the new aggregate
demands speciﬁes the amount of new timeslots to be allocated.
The counts are idempotent: receiving the same count multiple
times does not cause timeslots to be allocated multiple times. Idem-
potency permits aggressive timeouts, leading to low allocation la-
tency even in the face of occasional packet loss. Endpoints detect
the loss of allocated timeslots using aggregate counts sent by the
arbiter, triggering a request for more timeslots.
Handling arbiter failure. When a secondary arbiter replaces a
failed primary, its aggregate counts are out of sync with the endpoints.
The mismatch is detected using a random nonce established when
the arbiter and endpoint synchronize. The nonce is incorporated
into the packet checksum, so when an arbiter is out of sync with an
endpoint, it observes a burst of failed checksums from an endpoint
that triggers a re-synchronization with that endpoint.
Upon re-synchronization, the arbiter resets its aggregate counts to
zeros, and the endpoint recomputes fresh aggregate counts based on
its current queues. The process takes one round-trip time: as soon
as the endpoint processes the RESET packet from the arbiter, it can
successfully issue new requests to the arbiter.
6.
IMPLEMENTATION
We implemented the arbiter using Intel DPDK [1], a framework
that allows direct access to NIC queues from user space. On the
endpoints, we implemented a Linux kernel module that queues
packets while requests are sent to the arbiter. Our source code is at
http://fastpass.mit.edu/
6.1 Client
FCP is implemented as a Linux transport protocol (over IP). A
Fastpass qdisc (queueing discipline) queues outgoing packets before
sending them to the NIC driver, and uses an FCP socket to send
demands to and receive allocations from the arbiter.
The Fastpass qdisc intercepts each packet just before it is passed to
the NIC, and extracts the destination address from its packet header.3
It does not process transport protocol headers (e.g., TCP or UDP).
The Fastpass arbiter schedules network resources, obviating the
need for TCP congestion control. TCP’s congestion window (cwnd)
could needlessly limit ﬂow throughput, but packet loss is rare, so
cwnd keeps increasing until it no longer limits throughput. At this
point, TCP congestion control is effectively turned off.
The current implementation does not modify TCP. The evaluated
system maintains long-lived ﬂows, so ﬂows are not cwnd-limited,
and we preferred not to deal with the complexities of TCP. Never-
theless, modifying TCP’s congestion control would be worthwhile
for improving short-lived TCP ﬂow performance.
The client is not limited to sending exactly one packet per timeslot.
Instead, it greedily sends packets while their total transmission time
is less than the timeslot length. This aggregation reduces the amount
of potentially wasted network bandwidth caused by many small
packets destined to the same endpoint.
6.2 Multicore Arbiter
The arbiter is made up of three types of cores: comm-cores com-
municate with endpoints, alloc-cores perform timeslot allocation,
and pathsel-cores assign paths.
The number of cores of each type can be increased to handle
large workloads: each comm-core handles a subset of endpoints, so
endpoints can be divided among more cores when protocol handling
becomes a bottleneck; alloc-cores work concurrently using pipeline
3On a switched network, MAC addresses could be used. However,
in the presence of routing, IP addresses are required.
Figure 6: Multicore allocation: (1) allocation cores assign packets
to timeslots, (2) path selection cores assign paths, and (3) communi-
cation cores send allocations to endpoints.
parallelism (§3); and pathsel-cores are “embarrassingly parallel”,
since path assignments for different timeslots are independent.
Fig. 6 shows communication between arbiter cores. Comm-cores
receive endpoint demands and pass them to alloc-cores (not shown).
Once a timeslot is completely allocated, it is promptly passed to a
pathsel-core. The assigned paths are handed to comm-cores, which
notify each endpoint of its allocations.
Performance measurements of each core type are presented in §7.
6.3 Timing
To keep queue occupancy low, end-node transmissions should
occur at the times prescribed by the arbiter. Otherwise, packets from
multiple endpoints might arrive at a switch’s egress port together,
resulting in queueing.
The amount of queueing caused by time-jitter is determined by the
discrepancy in clocks. For example, if all clocks are either accurate
or one timeslot fast, at most two packets will arrive at any egress:
one from an accurate node, the other from a fast node.
Clock synchronization. The deployment synchronizes end-node
time using the IEEE1588 Precision Time Protocol (PTP), which
achieves sub-microsecond clock synchronization by carefully mit-
igating causes of time-synchronization jitter. PTP-capable NICs
timestamp synchronization packets in hardware [29], thus avoiding
jitter due to operating system scheduling. NICs with PTP support
are widely available; the experiment used Intel 82599EB NICs.
Variable queueing delays inside the network also cause syn-
chronization inaccuracies, and PTP-supported switches report their
queueing delays so endpoints can compensate. However, Fastpass
keeps queue-length variability low, enabling high quality time syn-
chronization without PTP switch support.
Client timing. The client uses Linux high-resolution timers
(hrtimers), previously demonstrated to achieve microsecond-scale
precision when shaping ﬂow throughput [33].
The client uses locks to synchronize access to the qdisc queues and
to allocation state. Because waiting for these locks when transmitting
packets causes variable delays, we allow the qdisc to send packets
after their scheduled times, up to a conﬁgurable threshold. Endpoints
re-request overly late allocations from the arbiter.
7. EVALUATION
The goal of Fastpass is to simultaneously eliminate in-network
queueing, achieve high throughput, and support various inter-ﬂow or
inter-application resource allocation objectives in a real-world data-
center network. In this section, we evaluate how well Fastpass meets
these goals, compared to a baseline datacenter network running
TCP.
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:7)(cid:4)(cid:12)(cid:3)(cid:13)(cid:13)(cid:7)(cid:5)(cid:14)(cid:1)(cid:4)(cid:9)(cid:10)(cid:11)(cid:15)(cid:16)(cid:11)(cid:5)(cid:7)(cid:10)(cid:15)(cid:17)(cid:4)(cid:3)(cid:4)(cid:9)(cid:7)(cid:18)(cid:3)(cid:13)(cid:13)(cid:7)(cid:5)(cid:14)(cid:19)(cid:15)(cid:3)(cid:4)(cid:6)(cid:20)(cid:12)(cid:11)(cid:13)(cid:14)(cid:1)(cid:15)(cid:3)(cid:4)(cid:6)(cid:20)(cid:12)(cid:11)(cid:13)(cid:14)(cid:19)(cid:15)(cid:3)(cid:4)(cid:6)(cid:20)(cid:12)(cid:11)(cid:13)(cid:14)(cid:21)(cid:15)(cid:3)(cid:4)(cid:6)(cid:20)(cid:12)(cid:11)(cid:13)(cid:14)(cid:22)(cid:5)(cid:7)(cid:10)(cid:10)(cid:14)(cid:1)§7.1
§7.2
§7.3
§7.4
Summary of Results
(A) Under a bulk transfer workload involving multiple
machines, Fastpass reduces median switch queue length
to 18 KB from 4351 KB, with a 1.6% throughput penalty.
(B) Interactivity: under the same workload, Fastpass’s
median ping time is 0.23 ms vs. the baseline’s 3.56 ms,
15.5⇥ lower with Fastpass.
(C) Fairness: Fastpass reduces standard deviations of
per-sender throughput over 1 s intervals by over 5200⇥
for 5 connections.
(D) Each comm-core supports 130 Gbits/s of network
trafﬁc with 1 µs of NIC queueing.
(E) Arbiter trafﬁc imposes a 0.3% throughput overhead.
(F) 8 alloc-cores support 2.2 Terabits/s of network trafﬁc.
(G) 10 pathsel-cores support >5 Terabits/s of network
trafﬁc.
(H) In a real-world latency-sensitive service, Fastpass
reduces TCP retransmissions by 2.5⇥.
Experimental setup. We conducted experiments on a single rack
of 32 servers, with each server connected to a top-of-rack (ToR)
switch with a main 10 Gbits/s Ethernet (GbE) network interface
card (NIC). Servers also have a 1 Gbit/s NIC meant for out-of-band
communication. The 10 GbE top-of-rack switch has four 10 GbE
uplinks to the four cluster switches [16]. Each server has 2 Intel
CPUs with 8 cores each (16 hyper-threads per CPU, for a total of 32
hyper-threads) and 148 GB RAM. One server is set aside for running
the arbiter. We turn off TCP segmentation ofﬂoad (TSO) to achieve
better control over the NIC send queues.
7.1 Throughput, queueing, and latency
Experiment A: throughput and queueing. Our ﬁrst experiment
compares the throughput and switch queue occupancy of Fastpass
to the baseline network. Four rack servers run iperf to generate
trafﬁc (20 TCP ﬂows per sender) to a single receiver. The experiment
lasts 20 minutes and is run twice—once with the baseline and once
with Fastpass.
Fastpass achieves throughput close to the baseline’s: 9.43 Gbits/s
in the baseline run versus 9.28 Gbits/s with Fastpass. At the same
time, Fastpass reduces the median switch queue occupancy from
4.35 Megabytes in the baseline to just 18 kilobytes with Fastpass, a
reduction of a factor of 242⇥ (Fig. 7).
distribution that are a lot lower, as shown here:
90th %ile
It isn’t just the median but also the tails of the queue-occupancy
5097
36
99th
5224
53
99.9th
5239
305
Baseline (Kbytes)
Fastpass (Kbytes)
Median
4351
18
Most of the 1.6% difference in throughput can be attributed to
Fastpass reserving about 1% of the achieved throughput for FCP
trafﬁc. The remainder is due to the client re-requesting timeslots
when packet transmissions were delayed more than the allowed
threshold (§6.3).
Switch queues are mostly full in the baseline because TCP con-
tinues to increase the sending rate until a packet is dropped (usually
due to a full queue). In contrast, Fastpass’s timeslot allocation keeps
queues relatively empty: the 99.9th percentile occupancy was 305
KB over the entire experiment. Although the design intends queues
to be strictly 0, the implementation does not yet achieve it because
of jitter in endpoint transmission times. We believe that queueing
Figure 7: Switch queue lengths sampled at 100ms intervals on the
top-of-rack switch. The diagram shows measurements from two
different 20 minute experiments: baseline (red) and Fastpass (blue).
Baseline TCP tends to ﬁll switch queues, whereas Fastpass keeps
queue occupancy low.
Figure 8: Histogram of ping RTTs with background load using
Fastpass (blue) and baseline (red). Fastpass’s RTT is 15.5⇥ smaller,
even with the added overhead of contacting the arbiter.
can be further reduced towards the zero ideal by using ﬁner-grained
locking in the kernel module.
Experiment B: latency. Next, we measure the round-trip latency of
interactive requests under high load. This experiment uses the same
setup as Experiment A, augmented with a ﬁfth machine that sends a
small request to the receiving server every 10 ms, using ping.
Fastpass reduces the end-to-end round-trip time (RTT) for in-
teractive trafﬁc when the network is heavily loaded by a factor of
15.5⇥, from a median of 3.56 ms to 230 µs (Fig. 8). The tail of the
distribution observes signiﬁcant reductions as well:
99th
3.92
0.32
Baseline (ms)
Fastpass (ms)
Median
3.56
0.23
99.9th
3.95
0.38
90th %ile
3.89
0.27
Note that with Fastpass ping packets are scheduled in both direc-
tions by the arbiter, but even with the added round-trips to the arbiter,