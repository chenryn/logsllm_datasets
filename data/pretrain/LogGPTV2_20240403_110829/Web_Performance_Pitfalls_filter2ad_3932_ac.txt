37
32
76
43
81
35
38
4
3
3
2
14.5
31.5
21.5
Fig. 5, we plot the relative diﬀerence between Byte Index for the same page load,
calculated from HAR body sizes, Resource Timings body sizes, and Content-
Length header (using the HAR body size if the Content-Length header is miss-
ing). For Firefox, see Fig. 5a, the Byte Index is almost identical for Content-
Length and HAR body size, but diﬀers by 17.1% for Res in 50% of the pages
loads, and by 56.4% for 10%. For Chrome, see Fig. 5b, the Byte Index derived
from both Res and HAR diﬀers substantially from the Byte Index derived from
Content-Length.
Thus, we conclude: (1) Resource Timings do not include all objects of a Web
page download. (2) Byte Indexes from Resource Timings vs. HAR ﬁles diﬀer by
13.8%/17% in median and by more than 50% for 10% of the pages.
5.4 Pitfall: Data Source Availability
Besides being inaccurate some data sources do not even provide us with any data
for some Web page access. More precisely, Table 4 shows the number of success-
ful page loads as well as the errors for the Alexa 1000 for diﬀerent browsers
and automation tools. Firefox with Marionette yields the best results in terms
of successful runs that include all data elements. Using Chrome often yields
invalid timings, in particular, for the onLoad event. The main culprit is a too
early export of the data—the page load has not yet been completed even though
Chrome was instructed to wait for the onLoad event22. For a non-negligible
number of Web pages, we did not get any results for all or for some of the 10
repeated page loads per browser framework. For most of these page downloads,
the browser never invoked the onLoad event, see also Sect. 2.1, and, thus, timed
out without exporting any data. Using a diﬀerent tool would not ﬁx the prob-
lem in some cases: Investigating both the error messages logged by the browser
automation tool as well as the captured traces we ﬁnd that common reasons
are no DNS response, not being able to establish a TCP connection, or cer-
tiﬁcate errors. We limit intermittent connectivity issues by spreading out page
loads over time. But, we cannot rule out ﬁltering, e.g., due to our vantage point.
Still, manual tests for some of the page loads showed that these also fail for
22 Using the Page.frameStoppedLoading event instead did not resolve this problem.
Web Performance Pitfalls
297
diﬀerent vantage points. These even involve domains of large application service
providers and content distribution networks, which are hosting resources only
under subdomains23.
In summary, our conclusions are: (1) Not all domains in the Alexa Top Lists
point to actual Web pages. (2) Firefox with Marionette is more likely to provide
complete data than Firefox with Selenium or Chrome.
6 Guidelines for Web Performance Measurement
Next, we derive some guidelines for designing and conducting experiments.
Use HAR Files and Navigation Timings, Not Resource Timings: As
shown in Sect. 5.2, Resource timings are an unreliable data source, as they do
not include resources of embedded frames and often do not provide sizes for
cross-origin objects.
Choose Whether to Exclude Redirects: Redirects signiﬁcantly contribute
to page load times. Yet, they may not be representative for typical end-user
Web browsing, recall Sect. 5.1. It is possible to exclude redirects upfront, e.g., by
adjusting the hit list to post-redirect URLs. However, post-redirect URLs may
change, e.g., due to geolocation or HTTPS migration. Such changes may lead to
more page load failures, compared to starting from the “base” URLs of http://
and the top-level domain name. Alternatively, redirects can be excluded in retro-
spect by computing the timings relative to fetchStart instead of navigationStart
for Navigation Timings resp. relative to the start time of the ﬁrst HTTP 200
object for HAR ﬁles.
Choice of Tools: Make a conscious choice whether to use a framework that
integrates browser automation tools, such as WebPagetest [10], or write your own
scripts. The ﬁrst has the advantage that it enables comparing multiple browsers
out of the box, while the latter gives more explicit control over details. Note,
WebPagetest provides Navigation Timings and HAR ﬁles. So pitfalls related to
Resource Timings do not apply. Moreover, it provides additional metrics such
as SpeedIndex. WebPagetest always includes redirects—in line with the W3C
deﬁnition of load times.
Use Up-to-Date Software: Major Web browsers are updated rather often,
typically every 1–2 months. While research projects typically last longer one has
to address the trade-oﬀ of updating to a newer version during the study: On the
one hand, software updates may ﬁx bugs and provide performance optimizations
so that the results are more representative of state-of-the-art setups and actual
user experience. On the other hand, updating may cause compatibility issues,
e.g., with measurement tools that are updated less often and hinder backward
compatibility. We recommend to consciously address this trade-oﬀ and to include
the version numbers of the used tools. See Appendix B for more details.
23 Examples include microsoftonline.com and googleusercontent.com.
298
T. Enghardt et al.
Disable Features for a Quiet Browser: Modern browsers do not just load the
requested Web page. Rather they often automatically load additional data, e.g.,
software updates or blocklists, or transmit performance statistics to the browser
vendor. This can cause signiﬁcant performance overhead. We, thus, recommend
turning oﬀ such features. See Appendix B for more details.
Record and Compare Diﬀerent Data Sources: Whenever possible multiple
data sources should be recorded to enable cross-checks. Data sources include
but are not limited to Navigation Timings, Resource Timings, and HAR ﬁles.
Combining them helps improve accuracy. When choosing metrics it is essential
to understand their status with regards to standardization, e.g., published as
W3C Recommendation, and to which extent the implementation conforms to
the standard.
Mind New Protocols: Deployment of new protocols always has the chance
of invalidating existing assumptions about traﬃc both in general as well as for
Web traﬃc. Moreover, new protocols may require updates to the measurement
and evaluation setup or trigger so far unknown bugs in the evaluation. Recent
examples include the increased deployment of HTTP/2 and QUIC which use
features such as header compression and HTTP/2 Server Push.
7 Conclusion
We show that Web metrics highly depend on which speciﬁc metrics, data sources,
and/or measurement tools are used. For example, initial redirects can cause Page
Load Times (PLTs) to vary by 6.1% in median and by more than 23% for 10% of
pages. The impact is even larger for user-centric metrics such as Time To First
Paint (TTFP), with 19.1% and 47%, respectively. Furthermore, HAR ﬁles and
Resource Timings provide widely diﬀering object sizes and numbers of objects
which in turn bias derived metrics, e.g., Byte Index varies by 17.1% for 50% of
pages and by 54.2% for 10% of pages. However, in almost all Web measurement
studies none of the metrics or the data sources are described in suﬃcient detail.
Moreover, they often ignore the bias of the above diﬀerences.
Thus, our study clearly highlights the need to (a) improve documentation,
(b) choose metrics consciously and with all caveats in mind, (c) double check the
results against alternative metrics, and (d) enable qualitative comparisons. To
enable this we strongly follow the recommendations of a recent Dagstuhl seminar
on reproducibility and suggest that conferences and journals should not count
the pages needed to document the precise measurement/simulation setup and
the used metrics against the available page limit.
Acknowledgements. Thanks to Dominik Strohmeier for the discussion and the
pointers to resources, to our shepherd Jelena Mirkovic, as well as our anonymous
reviewers.
A Web Page Load Explained
Web Performance Pitfalls
299
In this section, we explain a Web page load in more detail. See also Fig. 2 and
the processing models in the Navigation Timings speciﬁcations [2,3].
The starting point for a new page load, also called navigation, of a particular
URL, is called navigationStart in [2]. Initially, fetchStart is set to the same value,
but if a redirect occurs, fetchStart is overwritten before the new URL is loaded.
If another page has been previously loaded by the browser, e.g., in the same
browser tab, this document has to be ﬁrst unloaded. Then, the browser checks
its cache to see whether the page is already there. If the page is not in the cache,
the browser usually resolves the hostname (resulting in a DNS query and usually
answer), establishes a TCP connection, and performs a TLS handshake if the
scheme of the URL is https. Then, the browser issues an HTTP GET request
for the URL. As soon as it receives an HTTP reply, which always contains a
status line, headers, and body, the browser processes the reply.
If the reply contains an HTTP status code of 3xx, such as “301 Moved
Permanently” or “302 Found”, this means that the server redirects the browser to
a diﬀerent URL, which is given in the “Location” header in the HTTP response.
This redirect may be a same-origin redirect, which roughly means that both the
old and the new URL have the same scheme (http or https), hostname, and port
(see RFC 6454 [29] for details), or it may be a cross-origin redirect. For same-
origin redirects, the start and end time of the redirect are recorded as Navigation
Timings redirectStart and redirectEnd [2], while for cross-origin redirects they
are not. Unfortunately, nearly all redirects we observed are cross-origin, as the
purpose of the redirect is to use a diﬀerent scheme (HTTPS instead of HTTP) or
hostname (www.example.com instead of example.com). The same-origin policy
is an important security and privacy feature in the Web, so information access
is often restricted to, e.g., the same hostname.
Given the new URL to be fetched, the browser records the current time as
fetchStart, potentially overwriting the old value24. It then checks its application
cache again, resolves the host name if needed, establishes a new TCP connection,
performs a new TLS handshake, and sends an HTTP request for the new URL.
If it gets an HTTP reply, this may be another redirect, an error code such as
“404 Not Found” or“503 Internal Server Error”, or the request may succeed
with a “200 OK”. In the latter case, the body of the HTTP response usually
contains the base document of the Web page in HyperText Markup Language
(HTML). As soon as the browser starts receiving this document, it parses it
and starts constructing the Document Object Model (DOM) of the page. For
example, the document may reference additional resources, such as JavaScript,
Cascading Stylesheets (CSS), or images. Typically, for each of these additional
resources, the browser has to issue a new HTTP request, unless the resource
24 After a redirect, the browser overwrites the old fetchStart value before it fetches
the new URL using a GET request. This implies that once the page load is ﬁnished,
fetchStart is the start time of the loading of the ﬁnal base page, as all previous values
related to redirects are overwritten.
300
T. Enghardt et al.
is proactively sent by the server using HTTP/2 Server Push. Each new HTTP
request may involve an additional name resolution, TCP handshake, and TLS
handshake, because resources are often hosted on diﬀerent servers than the base
page. The browser now simultaneously fetches new resources, continues to parse
the HTML base page, and processes the CSS and Javascripts, even though these
processes may block each other. See Wang et al. [16] for a detailed explanation
of this complex process.
At some point, the browser ﬂushes the current state of the DOM to the
rendering engine. The time at which this happens corresponds to Time To First
Paint (TTFP). The point at which all resources in the DOM have been loaded
is called DOMContentLoaded and recorded in the Navigation Timings and HAR
ﬁle. However, processing of the page usually continues, until, eventually, the
browser ﬁres the onLoad event for the page which is recorded in the Navigation
Timings and HAR ﬁle. The onLoad Time is usually taken as Page Load Time
(PLT). At this point, the page load is considered ﬁnished. However, onLoad
usually triggers the execution of one or more javascripts, which may result in
loading more resources, sending data, e.g., to third parties, or other network
traﬃc. In fact, most modern Web pages load resources continuously long after the
onLoad event. Thus, Related Work usually stops counting objects after onLoad.
B Details of Lessons Learned
Next, we outline additional details regarding our lessons learned, which led to
our guidelines for Web performance measurement, recall Sect. 6.
Software Versions: The Debian Linux distribution includes a version of the
Firefox browser which is usually quite dated. This can have a major impact on
load times. For instance, in Firefox version 61 (“Firefox Quantum”), parts of the
code have been rewritten and optimized, which makes the browser much faster
than previous versions. Consequently, carrying out Web page loads using an
older version results in unrealistically long load times. However, updating Firefox
frequently to the newest version can result in incompatibilities with measurement
tools. For instance, not every version of the HAR Export Trigger extension works
with every version of Firefox, so it has to be updated along with the browser.
However, the upside is that in newer versions of Firefox, HAR Export Trigger is
supposed to work without having the developer panel open.
Browser Traﬃc Unrelated to Page Loads: Modern browsers usually issue
a signiﬁcant number of requests that are not directly related to the page load
that a user has requested. For instance, Firefox by default loads blocklists for
“safe browsing”, to protect users from malware or phishing. It also automati-
cally checks for updates and may even automatically download and install these
updates for the entire browser or for individual browser extensions. These queries
can involve substantial data transfers: For example, we observed the automatic
download of a binary related to an H264 media component which we never acti-
vated or requested: 500 KB were downloaded in the background. Worse yet,
Web Performance Pitfalls
301
the state of such updates is often stored in the browser proﬁle. This may cause
such downloads to be triggered for every fresh browser proﬁle, i.e., each of our
page loads. Additionally, the Chrome browser by default issues queries to var-
ious Google servers, e.g., it tries to connect each browsing session to a Google
account. We provide conﬁgurations for Firefox and Chrome to turn oﬀ most
features that generate such traﬃc, see our repository https://github.com/theri/
web-measurement-tools.
Logging a Trace and Client-Side SSL Keys: To be able to better debug and
validate measurement setups and tools, we recommend capturing packet traces
that include at least ports 53 (DNS), 80 (HTTP), and 443 (HTTPS). Encrypted
traﬃc can be decrypted after logging the SSL session keys within the browser:
Firefox and Chrome log keys into a speciﬁed SSLKEYLOGFILE. Note that this
option must be compiled into Firefox. It, e.g., does not work with the Firefox
binary in the Debian repositories.
C Artifacts Related to This Paper
The following artifacts are available:
Our Tools, Such as Measurement and Evaluation Scripts: See https://
github.com/theri/web-measurement-tools. This repository includes the scripts
to automatically load Web pages using Firefox with Selenium and Marionette,
and using Chrome with DevTools. Furthermore, it includes the analysis scripts
we used to generate our plots.
Data Set of Web Page Loads: See http://dx.doi.org/10.14279/deposi
tonce-8100. This dataset includes data from all of our experiment runs, see
Sect. 4. It can be used along with our evaluation scripts to reproduce the plots
in this paper, see https://github.com/theri/web-measurement-tools for details.
References
1. Bocchi, E., De Cicco, L., Rossi, D.: Measuring the quality of experience of web
users. In: ACM SIGCOMM Computer Communication Review, vol. 46, no. 4, pp.
8–13 (2016)
2. W3C Recommendation: Navigation Timing. Version 17 December 2012. https://
www.w3.org/TR/navigation-timing/. Accessed 29 Aug 2018
3. W3C Working Draft: Navigation Timing Level 2. Version 30 November 2018.
https://www.w3.org/TR/2018/WD-navigation-timing-2-20181130/. Accessed 17
Dec 2018
4. W3C Candidate Recommendation: Resource Timing Level 1. Version 30 March
2017. https://www.w3.org/TR/resource-timing-1/. Accessed 29 Aug 2018
5. W3C Working Draft: Resource Timing Level 2. Version 11 October 2018. https://
www.w3.org/TR/resource-timing-2/. Accessed 13 Oct 2018
6. W3C First Public Working Draft: Paint Timing 1. Version 07 September 2017.
https://www.w3.org/TR/paint-timing/. Accessed 10 Oct 2018
302
T. Enghardt et al.
7. W3C Editor’s Draft: HTTP Archive (HAR) format. Version 14 August 2012.
https://w3c.github.io/web-performance/specs/HAR/Overview.html. Accessed 29
Aug 2018
8. Bruns, A., Kornstadt, A., Wichmann, D.: Web application tests with selenium.
IEEE Softw. 26(5), 88–91 (2009)
9. Selenium Documentation: Worst Practices. https://seleniumhq.github.io/docs/
worst.html. Accessed 29 Aug 2018
10. Meenan, P.: WebPageTest. https://www.webpagetest.org. Accessed 17 Dec 2018
11. da Hora, D.N., Asrese, A.S., Christophides, V., Teixeira, R., Rossi, D.: Narrowing
the gap between QoS metrics and web QoE using above-the-fold metrics. In: Bev-
erly, R., Smaragdakis, G., Feldmann, A. (eds.) PAM 2018. LNCS, vol. 10771, pp.
31–43. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-76481-8 3
12. Goel, U., Steiner, M., Wittie, M.P., Flack, M., Ludin, S.: Measuring what is not
ours: a tale of 3rd party performance. In: Kaafar, M.A., Uhlig, S., Amann, J. (eds.)
PAM 2017. LNCS, vol. 10176, pp. 142–155. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-54328-4 11
13. Erman, J., Gopalakrishnan, V., Jana, R., Ramakrishnan, K.K.: Towards a
SPDY’ier mobile web? IEEE/ACM Trans. Netw. 23(6), 2010–2023 (2015)
14. Qian, F., Gopalakrishnan, V., Halepovic, E., Sen, S., Spatscheck, O.: TM 3: ﬂexible
transport-layer multi-pipe multiplexing middlebox without head-of-line blocking.
In: Proceedings of the 11th ACM Conference on Emerging Networking Experiments
and Technologies, p. 3. ACM, New York (2015)
15. Wang, X.S., Krishnamurthy, A., Wetherall, D.: Speeding up web page loads with
Shandian. In: Proceedings of the 13th USENIX Symposium on Networked Sys-
tems Design and Implementation (NSDI 2016), pp. 109–122. USENIX Association
(2016)
16. Wang, X.S., Balasubramanian, A., Krishnamurthy, A., Wetherall, D.: Demystifying
page load performance with WProf. In: NSDI 2013, pp. 473–485 (2013)
17. Butkiewicz, M., Madhyastha, H.V., Sekar, V.: Understanding website complexity:
measurements, metrics, and implications. In: Proceedings of the 2011 ACM SIG-
COMM Conference on Internet Measurement Conference, pp. 313–328. ACM, New
York (2011)
18. Kelton, C., Ryoo, J., Balasubramanian, A., Das, S.R.: Improving user perceived
page load times using gaze. In: Proceedings of the 14th USENIX Symposium
on Networked Systems Design and Implementation (NSDI 2017), pp. 545–559.
USENIX Association (2017)
19. Varvello, M., Schomp, K., Naylor, D., Blackburn, J., Finamore, A., Papagiannaki,
K.: Is the web HTTP/2 yet? In: Karagiannis, T., Dimitropoulos, X. (eds.) PAM
2016. LNCS, vol. 9631, pp. 218–232. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-30505-9 17
20. Netravali, R., Mickens, J.: Prophecy: accelerating mobile page loads using ﬁnal-
state write logs. In: Proceedings of the 15th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 2018). USENIX Association (2018)
21. Netravali, R., Nathan, V., Mickens, J., Balakrishnan, H.: Vesper: measuring time-
to-interactivity for web pages. In: Proceedings of the 15th USENIX Symposium
on Networked Systems Design and Implementation (NSDI 2018). USENIX Asso-
ciation (2018)
22. Netravali, R., Goyal, A., Mickens, J., and Balakrishnan, H.: Polaris: faster page
loads using ﬁne-grained dependency tracking. In: Proceedings of the 13th USENIX
Symposium on Networked Systems Design and Implementation (NSDI 2016).
USENIX Association (2016)
Web Performance Pitfalls
303
23. Zaki, Y., Chen, J., P¨otsch, T., Ahmad, T., Subramanian, L.: Dissecting web latency
in Ghana. In: Proceedings of the 2014 Conference on Internet Measurement Con-
ference, pp. 241–248. ACM, New York (2014)
24. Han, B., Qian, F., Hao, S., Ji, L.: An anatomy of mobile web performance over mul-
tipath TCP. In: Proceedings of the 11th ACM Conference on Emerging Networking
Experiments and Technologies, p. 5. ACM, New York (2015)
25. Naylor, D., et al.: The cost of the S in HTTPS. In: Proceedings of the 10th ACM
International on Conference on Emerging Networking Experiments and Technolo-
gies, pp. 133–140. ACM, New York (2014)
26. Scheitle, Q., et al.: A long way to the top: signiﬁcance, structure, and stability
of internet top lists. In: Internet Measurement Conference 2018. ACM, New York
(2018)
27. Let’s Encrypt: Percentage of Web Pages Loaded by Firefox Using HTTPS. https://
letsencrypt.org/stats/#percent-pageloads. Accessed 30 Sept 2018
28. Egger, S., Hossfeld, T., Schatz, R., Fiedler, M.: Waiting times in quality of expe-
rience for web based services. In: 2012 Fourth International Workshop on Quality
of Multimedia Experience (QoMEX), pp. 86–96. IEEE (2012)
29. Barth, A.: The web origin concept. RFC 6454 (2011)