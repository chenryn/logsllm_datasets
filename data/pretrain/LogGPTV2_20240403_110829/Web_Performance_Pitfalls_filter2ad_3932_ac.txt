### Analysis of Byte Index Discrepancies

In Fig. 5, we present the relative differences in the Byte Index for the same page load, calculated using HAR body sizes, Resource Timings body sizes, and Content-Length headers (using the HAR body size if the Content-Length header is missing). For Firefox (Fig. 5a), the Byte Index is nearly identical for Content-Length and HAR body size, but differs by 17.1% from Resource Timings in 50% of the page loads, and by 56.4% in 10% of the page loads. For Chrome (Fig. 5b), the Byte Index derived from both Resource Timings and HAR files differs substantially from the Byte Index derived from Content-Length.

**Conclusion:**
1. **Resource Timings Incompleteness:** Resource Timings do not include all objects of a web page download.
2. **Byte Index Discrepancy:** Byte Indexes from Resource Timings versus HAR files differ by 13.8%/17% in median and by more than 50% for 10% of the pages.

### Pitfall: Data Source Availability

Beyond accuracy, some data sources may not provide any data for certain web page accesses. Table 4 summarizes the number of successful page loads and errors for the Alexa Top 1000 websites across different browsers and automation tools. Firefox with Marionette yields the best results in terms of successful runs that include all data elements. Chrome often produces invalid timings, particularly for the `onLoad` event, due to premature data export before the page load is complete. For a significant number of web pages, we did not receive any results for all or some of the 10 repeated page loads per browser framework. This is often because the `onLoad` event was never triggered, leading to timeouts without data export. Different tools did not always resolve these issues, as common reasons for failure include no DNS response, inability to establish a TCP connection, or certificate errors. We mitigated intermittent connectivity issues by spreading out page loads over time, but filtering, possibly due to our vantage point, cannot be ruled out. Manual tests confirmed that some page loads fail from different vantage points, including those involving large application service providers and content distribution networks that host resources under subdomains.

**Summary:**
1. **Incomplete Domain Coverage:** Not all domains in the Alexa Top Lists point to actual web pages.
2. **Tool Performance:** Firefox with Marionette is more likely to provide complete data compared to Firefox with Selenium or Chrome.

### Guidelines for Web Performance Measurement

Based on our findings, we derive the following guidelines for designing and conducting web performance experiments:

1. **Use HAR Files and Navigation Timings, Not Resource Timings:**
   - As shown in Sect. 5.2, Resource Timings are an unreliable data source, as they do not include resources of embedded frames and often lack sizes for cross-origin objects.

2. **Choose Whether to Exclude Redirects:**
   - Redirects significantly contribute to page load times but may not be representative of typical end-user browsing. Redirects can be excluded upfront by adjusting the hit list to post-redirect URLs, though this may lead to more failures due to changes in post-redirect URLs. Alternatively, redirects can be excluded retrospectively by computing timings relative to `fetchStart` instead of `navigationStart` for Navigation Timings, or relative to the start time of the first HTTP 200 object for HAR files.

3. **Choice of Tools:**
   - Consider whether to use a framework that integrates browser automation tools, such as WebPagetest, or write your own scripts. WebPagetest provides Navigation Timings and HAR files, avoiding pitfalls related to Resource Timings, and offers additional metrics like SpeedIndex. WebPagetest always includes redirects, aligning with the W3C definition of load times.

4. **Use Up-to-Date Software:**
   - Major web browsers are updated frequently, typically every 1-2 months. While research projects may last longer, updating to newer versions during the study can fix bugs and provide performance optimizations, making results more representative of state-of-the-art setups and actual user experience. However, updates may cause compatibility issues with less frequently updated measurement tools. We recommend addressing this trade-off and including version numbers of used tools. See Appendix B for more details.

5. **Disable Features for a Quiet Browser:**
   - Modern browsers often automatically load additional data, such as software updates or blocklists, which can cause significant performance overhead. We recommend disabling such features. See Appendix B for more details.

6. **Record and Compare Different Data Sources:**
   - Whenever possible, record multiple data sources to enable cross-checks. Data sources include, but are not limited to, Navigation Timings, Resource Timings, and HAR files. Combining them helps improve accuracy. It is essential to understand the standardization status of chosen metrics and the extent to which the implementation conforms to the standard.

7. **Mind New Protocols:**
   - The deployment of new protocols, such as HTTP/2 and QUIC, may invalidate existing assumptions about traffic. These protocols require updates to the measurement and evaluation setup and may trigger unknown bugs. Recent examples include increased deployment of HTTP/2 and QUIC, which use features like header compression and HTTP/2 Server Push.

### Conclusion

Our study demonstrates that web metrics are highly dependent on specific metrics, data sources, and measurement tools. For example, initial redirects can cause Page Load Times (PLTs) to vary by 6.1% in median and by more than 23% for 10% of pages. The impact is even larger for user-centric metrics such as Time To First Paint (TTFP), with variations of 19.1% and 47%, respectively. Furthermore, HAR files and Resource Timings provide widely differing object sizes and numbers, which in turn bias derived metrics, e.g., Byte Index varies by 17.1% for 50% of pages and by 54.2% for 10% of pages. However, most web measurement studies do not describe the metrics or data sources in sufficient detail and often ignore the biases introduced by these differences.

**Key Recommendations:**
1. **Improve Documentation:**
   - Enhance documentation to ensure clarity and reproducibility.
2. **Conscious Metric Selection:**
   - Choose metrics with all caveats in mind.
3. **Double-Check Results:**
   - Verify results against alternative metrics.
4. **Enable Qualitative Comparisons:**
   - Follow the recommendations of a recent Dagstuhl seminar on reproducibility and suggest that conferences and journals should not count the pages needed to document the precise measurement/simulation setup and used metrics against the available page limit.

### Acknowledgements

We thank Dominik Strohmeier for the discussion and pointers to resources, Jelena Mirkovic for her guidance, and our anonymous reviewers for their valuable feedback.

### A. Web Page Load Explained

A web page load, also called navigation, starts with the `navigationStart` event for a particular URL. Initially, `fetchStart` is set to the same value, but if a redirect occurs, `fetchStart` is overwritten before the new URL is loaded. If another page has been previously loaded in the same browser tab, it must be unloaded first. The browser then checks its cache to see if the page is already there. If not, it resolves the hostname (DNS query and answer), establishes a TCP connection, and performs a TLS handshake if the URL scheme is HTTPS. The browser issues an HTTP GET request for the URL and processes the reply, which includes a status line, headers, and body.

If the reply contains an HTTP status code of 3xx, such as "301 Moved Permanently" or "302 Found," the server redirects the browser to a different URL, given in the "Location" header. This can be a same-origin or cross-origin redirect. For same-origin redirects, the start and end times are recorded as `redirectStart` and `redirectEnd`. For cross-origin redirects, these times are not recorded. Nearly all observed redirects are cross-origin, often changing the scheme (HTTPS instead of HTTP) or hostname.

Given the new URL, the browser records the current time as `fetchStart`, potentially overwriting the old value. It then checks the application cache, resolves the hostname if needed, establishes a new TCP connection, performs a new TLS handshake, and sends an HTTP request for the new URL. If the response is another redirect, an error code, or a "200 OK," the latter case involves the base document of the web page in HTML. The browser parses the HTML and constructs the Document Object Model (DOM), which may reference additional resources like JavaScript, CSS, or images. Each resource typically requires a new HTTP request unless proactively sent by the server using HTTP/2 Server Push. Each new request may involve additional name resolution, TCP handshakes, and TLS handshakes.

At some point, the browser flushes the current state of the DOM to the rendering engine, marking the Time To First Paint (TTFP). The point at which all resources in the DOM have been loaded is called `DOMContentLoaded` and is recorded in Navigation Timings and HAR files. Processing continues until the `onLoad` event is fired, marking the Page Load Time (PLT). However, the `onLoad` event often triggers additional JavaScript execution, which may load more resources, send data, or generate other network traffic. Most modern web pages continue to load resources long after the `onLoad` event.

### B. Details of Lessons Learned

**Software Versions:**
- The Debian Linux distribution includes a version of Firefox that is often outdated, which can significantly impact load times. For instance, Firefox Quantum (version 61) is much faster due to code optimizations. Using an older version can result in unrealistically long load times. However, frequent updates to the latest version can cause incompatibilities with measurement tools. For example, not every version of the HAR Export Trigger extension works with every version of Firefox, so it must be updated along with the browser. The upside is that in newer versions of Firefox, HAR Export Trigger is supposed to work without having the developer panel open.

**Browser Traffic Unrelated to Page Loads:**
- Modern browsers issue a significant number of requests unrelated to the requested page load. For example, Firefox loads blocklists for "safe browsing" and checks for updates, which can involve substantial data transfers. These queries can be triggered for each fresh browser profile, causing background downloads. Chrome also issues queries to various Google servers, such as connecting each browsing session to a Google account. We provide configurations for Firefox and Chrome to disable most features that generate such traffic, available in our repository: https://github.com/theri/web-measurement-tools.

**Logging a Trace and Client-Side SSL Keys:**
- To better debug and validate measurement setups and tools, we recommend capturing packet traces that include at least ports 53 (DNS), 80 (HTTP), and 443 (HTTPS). Encrypted traffic can be decrypted after logging the SSL session keys within the browser. Firefox and Chrome log keys into a specified `SSLKEYLOGFILE`. Note that this option must be compiled into Firefox and does not work with the Firefox binary in the Debian repositories.

### C. Artifacts Related to This Paper

The following artifacts are available:

- **Our Tools, Such as Measurement and Evaluation Scripts:**
  - Repository: https://github.com/theri/web-measurement-tools
  - Includes scripts to automatically load web pages using Firefox with Selenium and Marionette, and using Chrome with DevTools. Also includes analysis scripts used to generate the plots in this paper.

- **Data Set of Web Page Loads:**
  - DOI: http://dx.doi.org/10.14279/depositonce-8100
  - This dataset includes data from all experiment runs, which can be used with our evaluation scripts to reproduce the plots in this paper. See https://github.com/theri/web-measurement-tools for details.

### References

1. Bocchi, E., De Cicco, L., Rossi, D.: Measuring the quality of experience of web users. In: ACM SIGCOMM Computer Communication Review, vol. 46, no. 4, pp. 8–13 (2016)
2. W3C Recommendation: Navigation Timing. Version 17 December 2012. https://www.w3.org/TR/navigation-timing/. Accessed 29 Aug 2018
3. W3C Working Draft: Navigation Timing Level 2. Version 30 November 2018. https://www.w3.org/TR/2018/WD-navigation-timing-2-20181130/. Accessed 17 Dec 2018
4. W3C Candidate Recommendation: Resource Timing Level 1. Version 30 March 2017. https://www.w3.org/TR/resource-timing-1/. Accessed 29 Aug 2018
5. W3C Working Draft: Resource Timing Level 2. Version 11 October 2018. https://www.w3.org/TR/resource-timing-2/. Accessed 13 Oct 2018
6. W3C First Public Working Draft: Paint Timing 1. Version 07 September 2017. https://www.w3.org/TR/paint-timing/. Accessed 10 Oct 2018
7. W3C Editor’s Draft: HTTP Archive (HAR) format. Version 14 August 2012. https://w3c.github.io/web-performance/specs/HAR/Overview.html. Accessed 29 Aug 2018
8. Bruns, A., Kornstadt, A., Wichmann, D.: Web application tests with selenium. IEEE Softw. 26(5), 88–91 (2009)
9. Selenium Documentation: Worst Practices. https://seleniumhq.github.io/docs/worst.html. Accessed 29 Aug 2018
10. Meenan, P.: WebPageTest. https://www.webpagetest.org. Accessed 17 Dec 2018
11. da Hora, D.N., Asrese, A.S., Christophides, V., Teixeira, R., Rossi, D.: Narrowing the gap between QoS metrics and web QoE using above-the-fold metrics. In: Beverly, R., Smaragdakis, G., Feldmann, A. (eds.) PAM 2018. LNCS, vol. 10771, pp. 31–43. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-76481-8_3
12. Goel, U., Steiner, M., Wittie, M.P., Flack, M., Ludin, S.: Measuring what is not ours: a tale of 3rd party performance. In: Kaafar, M.A., Uhlig, S., Amann, J. (eds.) PAM 2017. LNCS, vol. 10176, pp. 142–155. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-54328-4_11
13. Erman, J., Gopalakrishnan, V., Jana, R., Ramakrishnan, K.K.: Towards a SPDY’ier mobile web? IEEE/ACM Trans. Netw. 23(6), 2010–2023 (2015)
14. Qian, F., Gopalakrishnan, V., Halepovic, E., Sen, S., Spatscheck, O.: TM 3: flexible transport-layer multi-pipe multiplexing middlebox without head-of-line blocking. In: Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies, p. 3. ACM, New York (2015)
15. Wang, X.S., Krishnamurthy, A., Wetherall, D.: Speeding up web page loads with Shandian. In: Proceedings of the 13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2016), pp. 109–122. USENIX Association (2016)
16. Wang, X.S., Balasubramanian, A., Krishnamurthy, A., Wetherall, D.: Demystifying page load performance with WProf. In: NSDI 2013, pp. 473–485 (2013)
17. Butkiewicz, M., Madhyastha, H.V., Sekar, V.: Understanding website complexity: measurements, metrics, and implications. In: Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference, pp. 313–328. ACM, New York (2011)
18. Kelton, C., Ryoo, J., Balasubramanian, A., Das, S.R.: Improving user perceived page load times using gaze. In: Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2017), pp. 545–559. USENIX Association (2017)
19. Varvello, M., Schomp, K., Naylor, D., Blackburn, J., Finamore, A., Papagiannaki, K.: Is the web HTTP/2 yet? In: Karagiannis, T., Dimitropoulos, X. (eds.) PAM 2016. LNCS, vol. 9631, pp. 218–232. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-30505-9_17
20. Netravali, R., Mickens, J.: Prophecy: accelerating mobile page loads using final-state write logs. In: Proceedings of the 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2018). USENIX Association (2018)
21. Netravali, R., Nathan, V., Mickens, J., Balakrishnan, H.: Vesper: measuring time-to-interactivity for web pages. In: Proceedings of the 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2018). USENIX Association (2018)
22. Netravali, R., Goyal, A., Mickens, J., Balakrishnan, H.: Polaris: faster page loads using fine-grained dependency tracking. In: Proceedings of the 13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2016). USENIX Association (2016)
23. Zaki, Y., Chen, J., Pötsch, T., Ahmad, T., Subramanian, L.: Dissecting web latency in Ghana. In: Proceedings of the 2014 Conference on Internet Measurement Conference, pp. 241–248. ACM, New York (2014)
24. Han, B., Qian, F., Hao, S., Ji, L.: An anatomy of mobile web performance over multipath TCP. In: Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies, p. 5. ACM, New York (2015)
25. Naylor, D., et al.: The cost of the S in HTTPS. In: Proceedings of the 10th ACM International on Conference on Emerging Networking Experiments and Technologies, pp. 133–140. ACM, New York (2014)
26. Scheitle, Q., et al.: A long way to the top: significance, structure, and stability of internet top lists. In: Internet Measurement Conference 2018. ACM, New York (2018)
27. Let’s Encrypt: Percentage of Web Pages Loaded by Firefox Using HTTPS. https://letsencrypt.org/stats/#percent-pageloads. Accessed 30 Sept 2018
28. Egger, S., Hossfeld, T., Schatz, R., Fiedler, M.: Waiting times in quality of experience for web based services. In: 2012 Fourth International Workshop on Quality of Multimedia Experience (QoMEX), pp. 86–96. IEEE (2012)
29. Barth, A.: The web origin concept. RFC 6454 (2011)