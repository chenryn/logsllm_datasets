### 2.3 Analysis of Error Sites

The identified error sites can be categorized into two types: input-related errors and occasional errors. Table 1 summarizes the results of our study.

We find that 42% of the sites that can fail and trigger error handling code are related to occasional errors. Additionally, we observe that approximately 70% of the identified error sites are associated with checking error-indicating return values from function calls (as shown in Figure 3). This observation suggests that manipulating the return values of specific function calls can cover a significant portion of error handling code, a technique that has been adopted by some existing SFI-based approaches [10, 18].

**Table 1: Study Results of Error Handling Code**

| Application | Studied File | Total Error Sites | Input-Related Errors | Occasional Errors |
|-------------|--------------|-------------------|----------------------|-------------------|
| vim         | 100          | 1163              | 530 (46%)            | 633 (54%)         |
| bison       | 100          | 184               | 96 (52%)             | 88 (48%)          |
| ffmpeg      | 100          | 881               | 518 (59%)            | 363 (41%)         |
| nasm        | 100          | 673               | 564 (84%)            | 109 (16%)         |
| catdoc      | 29           | 91                | 43 (47%)             | 48 (53%)          |
| clamav      | 100          | 1089              | 522 (48%)            | 567 (52%)         |
| cflow       | 100          | 286               | 170 (59%)            | 116 (41%)         |
| gif2png+libpng | 95         | 830               | 556 (67%)            | 274 (33%)         |
| openssl     | 100          | 989               | 571 (58%)            | 418 (42%)         |
| **Total**   | **824**      | **6168**          | **3570 (58%)**       | **2616 (42%)**    |

### 2.4 Study of CVEs Found by Existing Fuzzing Tools

To understand how existing fuzzing tools perform in detecting bugs in error handling code, we further analyzed the CVEs found by state-of-the-art fuzzing tools, including AFL [1], Honggfuzz [30], AFLFast [13], CollAFL [26], QSYM [65], and REDQUEEN [7]. We selected these tools because the CVEs they have discovered are publicly available. Specifically, for AFL, a website [2] collects its discovered CVEs; for Honggfuzz, the discovered CVEs are listed on its homepage; for AFLFast, CollAFL, QSYM, and REDQUEEN, the discovered CVEs are listed in their respective papers.

We manually reviewed these CVEs to identify those related to error handling code and checked whether they were associated with occasional errors. The results are summarized in Table 2.

**Table 2: Study Results of Existing Fuzzing Tools**

| Tool        | Total CVEs | Error Handling CVEs | Occasional Error CVEs |
|-------------|------------|---------------------|-----------------------|
| AFL         | 218        | 85                  | 3                     |
| Honggfuzz   | 57         | 17                  | 3                     |
| AFLFast     | 8          | 2                   | 0                     |
| CollAFL     | 93         | 15                  | 4                     |
| QSYM        | 6          | 0                   | 0                     |
| REDQUEEN    | 11         | 2                   | 1                     |
| **Total**   | **393**    | **121 (31%)**       | **11 (9%)**           |

We found that 31% of the CVEs discovered by these fuzzing tools are caused by incorrect error handling code, such as the bug shown in Figure 2. Only 9% of these CVEs are related to occasional errors. This proportion is significantly lower than the 42% of occasional error sites among all error sites (as found in Section 2.3). These results suggest that existing fuzzing tools may miss many real bugs in error handling code triggered by occasional errors. Therefore, it is crucial to improve fuzzing techniques to better test error handling code.

### 3. Basic Idea and Approach

#### 3.1 Basic Idea

To effectively test error handling code, we introduce Software Fault Injection (SFI) in fuzz testing by "fuzzing" injected faults based on the runtime information of the tested program. To achieve this, we construct an error sequence that contains multiple error points. An error point represents an execution point where an error can occur and trigger error handling code. During fault injection, each error point in an error sequence can either run normally (indicated as 0) or fail by injecting a fault (indicated as 1). Thus, an error sequence is a 0-1 sequence that describes the failure situation of error points at runtime:

\[ \text{ErrSeq} = [\text{ErrPt}_1, \text{ErrPt}_2, \ldots, \text{ErrPt}_x], \quad \text{ErrPt}_i \in \{0, 1\} \]

Similar to program inputs, an error sequence affects program execution and can be considered as the "input" of possibly triggered errors. A key challenge is determining which error points in an error sequence should be injected with faults to cover as much error handling code as possible. Inspired by existing fuzzing techniques that use feedback from program execution to fuzz program inputs, our basic idea is to fuzz error sequences for fault injection to test error handling code.

#### 3.2 Error Sequence Model

Existing SFI-based approaches often use context-insensitive fault injection, where they only use the location of each error site in the source code to describe an error point, without considering the execution context. This means that if a fault is injected into an error site, it will always fail when executed at runtime. However, an error site can be executed in different calling contexts, and some real bugs (such as the double-free bug shown in Figure 1) can only be triggered when the error site fails in a specific calling context and succeeds in others. Thus, existing SFI-based approaches may miss these real bugs.

To address this issue, we propose a context-sensitive SFI method. In addition to the location of each error site, our method also considers the calling context of the error site to describe error points:

\[ \text{ErrPt} = (\text{Location}, \text{CallCtx}) \]

To describe the calling context of an error site, we consider the runtime call stack when the error site is executed. This call stack includes the information of each function call in the stack (from caller to callee), including the locations of the function call and the called function. Thus, a calling context is described as:

\[ \text{CallCtx} = [\text{CallInfo}_1, \text{CallInfo}_2, \ldots, \text{CallInfo}_x] \]
\[ \text{CallInfo} = (\text{Caller Location}, \text{Callee Location}) \]

Based on this description, the information about each error point can be hashed as a key, and whether this error point should fail can be represented as a 0-1 value. An error sequence can be stored as a key-value pair in a hash table:

| Key (Hash(ErrPt)) | Value (0 or 1) |
|-------------------|-----------------|
| Hash(ErrPt_1)     | 0 or 1         |
| Hash(ErrPt_2)     | 0 or 1         |
| ...               | ...             |
| Hash(ErrPt_x)     | 0 or 1         |

Since the runtime call stack of an executed error site is related to program execution, error points cannot be statically determined and must be dynamically identified during program execution. Therefore, when performing fault injection using error sequences, the faults should be injected into error points during program execution.

According to our method, when an error site is executed in N different calling contexts, there will be N different error points for fault injection, rather than just one error point identified by context-insensitive fault injection. This allows for finer-grained fault injection.

#### 3.3 Context-Sensitive SFI-based Fuzzing

To effectively cover as much error handling code as possible, we propose a novel context-sensitive SFI-based fuzzing approach that performs fault injection using the feedback of program execution. As shown in Figure 4, our approach consists of six steps:

1. **Static Identification**: Identify the error sites in the source code of the tested program.
2. **Runtime Collection**: Run the tested program and collect runtime information about the calling contexts of each executed error site and code coverage.
3. **Error Sequence Creation**: Create error sequences for executed error sites based on the collected runtime information.
4. **Mutation**: After running the program, mutate each created error sequence to generate new sequences.
5. **Fault Injection**: Run the tested program and inject faults into error sites in specific calling contexts according to the mutated error sequences.
6. **Loop Construction**: Collect runtime information, create new error sequences, and perform mutation again, forming a fuzzing loop. The loop ends when no new error sequences are generated or the time limit is reached.

**Figure 4: Procedure of Our SFI-based Fuzzing Approach**

In our approach, mutating and generating error sequences are critical operations. Given a program input, our approach considers code coverage and discards repeated error sequences. Initially, this information is unavailable, so our approach performs a special initial mutation for the first execution of the tested program. For subsequent executions, it generates and mutates error sequences. All generated error sequences that increase code coverage are stored in a pool and ranked by their contribution to code coverage. Our approach preferentially selects error sequences for mutation.

**Initial Mutation**: Our approach first executes the tested program normally and creates an initial error sequence based on runtime information. This error sequence contains executed error points and is all-zero, used for the initial mutation. The mutation generates each new error sequence by making just one executed error point fail (0â†’1), as each error point may trigger uncovered error handling code in related calling contexts. Figure 5 shows an example of the initial mutation for an error sequence, which generates four new error sequences.

**Figure 5: Example of Initial Mutation**

**Subsequent Generation and Mutation**: After executing the tested program by injecting faults according to an original error sequence, some new error points may be executed, leading to the creation of a new error sequence. Our approach checks whether the code coverage is increased (i.e., new basic blocks or code branches are covered) during this execution. If not, the original error sequence and the created error sequence (if it exists) are discarded; if so, our approach separately mutates the original error sequence and the created error sequence (if it exists) to generate each new error sequence by changing the value of just one error point (0â†’1 or 1â†’0). Then, our approach compares these generated error sequences with existing ones to discard duplicates. Figure 6 shows an example of this procedure for two error sequences. For the first error sequence ErrSeq1, a new error point ErrPtx is executed, and thus our approach creates an error sequence containing ErrPtx. As the code coverage is increased, our approach mutates the two error sequences and generates nine new error sequences. However, one of them is the same as an existing error sequence ErrSeq2, so this new error sequence is discarded. For the second error sequence ErrSeq2, a new error point ErrPty is executed, and thus our approach creates an error sequence containing ErrPty. As the code coverage is not increased, our approach discards the two error sequences.

Note that each error point in an error sequence is related to the runtime calling context. When injecting faults into this error point during program execution, our approach needs to dynamically check whether the current runtime calling context and error sites match the target error point. If this error point is not executed during program execution, our approach will ignore it.

**Figure 7: Overall Architecture of FIFUZZ**

### 4.1 Compile-Time Analysis

In this phase, FIFUZZ performs two main tasks:

**Error-site Extraction**: For SFI-based approaches, the injected errors should be realistic to avoid false positives. Many SFI-based approaches [18, 40, 55] require users to manually provide error sites, which is labor-intensive and not scalable for large programs. To reduce manual work, the error-site extractor uses static analysis against the source code of the tested program to identify possible error sites, from which the user can select realistic ones.

Our analysis focuses on extracting specific function calls as error sites because our study in Section 2.3 reveals that most error sites are related to checking error-indicating return values of function calls. Our analysis has three steps:

1. **Identifying Candidate Error Sites**: In many cases, a function call returns a null pointer or negative integer to indicate a failure. Thus, our analysis identifies a function call as a candidate error site if: 1) it returns a pointer or integer; and 2) the return value is checked by an if statement with NULL or zero. The function call to `av_frame_new_side_data` in Figure 3 is an example that satisfies these requirements.
2. **Selecting Library Functions**: A called function can be a library function.