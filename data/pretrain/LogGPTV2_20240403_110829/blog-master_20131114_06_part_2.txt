        if (bio->bi_rw & (BIO_FLUSH | BIO_FUA)) {  
                where = ELEVATOR_INSERT_FLUSH;  
                goto get_rq;  
        }  
        if (elv_queue_empty(q))  
                goto get_rq;  
        el_ret = elv_merge(q, &req, bio);  
        switch (el_ret) {  
        case ELEVATOR_BACK_MERGE:  
                BUG_ON(!rq_mergeable(req));  
                if (!ll_back_merge_fn(q, req, bio))  
                        break;  
// 就在这里  
                trace_block_bio_backmerge(q, bio);  
                if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)  
                        blk_rq_set_mixed_merge(req);  
                req->biotail->bi_next = bio;  
                req->biotail = bio;  
                req->__data_len += bytes;  
                req->ioprio = ioprio_best(req->ioprio, prio);  
                if (!blk_rq_cpu_valid(req))  
                        req->cpu = bio->bi_comp_cpu;  
                drive_stat_acct(req, 0);  
                elv_bio_merged(q, req, bio);  
                if (!attempt_back_merge(q, req))  
                        elv_merged_request(q, req, el_ret);  
                goto out;  
        case ELEVATOR_FRONT_MERGE:  
                BUG_ON(!rq_mergeable(req));  
                if (!ll_front_merge_fn(q, req, bio))  
                        break;  
                trace_block_bio_frontmerge(q, bio);  
                if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff) {  
                        blk_rq_set_mixed_merge(req);  
                        req->cmd_flags &= ~REQ_FAILFAST_MASK;  
                        req->cmd_flags |= ff;  
                }  
                bio->bi_next = req->bio;  
                req->bio = bio;  
                /*  
                 * may not be valid. if the low level driver said  
                 * it didn't need a bounce buffer then it better  
                 * not touch req->buffer either...  
                 */  
                req->buffer = bio_data(bio);  
                /*  
                 * The merge may happen accross partitions  
                 * We must update in_flight value accordingly  
                 */  
                blk_account_io_front_merge(req, bio->bi_sector);  
                req->__sector = bio->bi_sector;  
                req->__data_len += bytes;  
                req->ioprio = ioprio_best(req->ioprio, prio);  
                if (!blk_rq_cpu_valid(req))  
                        req->cpu = bio->bi_comp_cpu;  
                drive_stat_acct(req, 0);  
                elv_bio_merged(q, req, bio);  
                if (!attempt_front_merge(q, req))  
                        elv_merged_request(q, req, el_ret);  
                goto out;  
        /* ELV_NO_MERGE: elevator says don't/can't merge. */  
        default:  
                ;  
        }  
get_rq:  
        /*  
         * This sync check and mask will be re-done in init_request_from_bio(),  
         * but we need to set it earlier to expose the sync flag to the  
         * rq allocator and io schedulers.  
         */  
        rw_flags = bio_data_dir(bio);  
        if (sync)  
                rw_flags |= REQ_SYNC;  
        /*  
         * Grab a free request. This is might sleep but can not fail.  
         * Returns with the queue unlocked.  
         */  
        req = get_request_wait(q, rw_flags, bio);  
        if (unlikely(!req)) {  
                bio_endio(bio, -ENODEV);        /* @q is dead */  
                goto out_unlock;  
        }  
        /*  
         * After dropping the lock and possibly sleeping here, our request  
         * may now be mergeable after it had proven unmergeable (above).  
         * We don't worry about that case for efficiency. It won't happen  
         * often, and the elevators are able to handle it.  
         */  
        init_request_from_bio(req, bio);  
        spin_lock_irq(q->queue_lock);  
        if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) ||  
            bio_flagged(bio, BIO_CPU_AFFINE))  
                req->cpu = raw_smp_processor_id();  
        if (queue_should_plug(q) && elv_queue_empty(q))  
                blk_plug_device(q);  
        /* insert the request into the elevator */  
        drive_stat_acct(req, 1);  
        __elv_add_request(q, req, where, 0);  
out:  
        if (unplug || !queue_should_plug(q))  
                __generic_unplug_device(q);  
out_unlock:  
        spin_unlock_irq(q->queue_lock);  
        return 0;  
}  
EXPORT_SYMBOL_GPL(blk_queue_bio);       /* for device mapper only */  
```  
## 参考  
1\. /usr/src/debug/kernel-2.6.32-358.el6/linux-2.6.32-358.el6.x86_64/include/trace/events  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")