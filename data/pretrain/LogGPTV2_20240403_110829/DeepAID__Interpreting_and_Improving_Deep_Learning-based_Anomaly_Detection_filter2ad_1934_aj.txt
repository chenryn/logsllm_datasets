by mitigating attack effect on the searching entrance. Specifically,
with I.R.N., attackers can only use the perturbation solved from 𝒙◦
to attack the searching process with the actual entrance 𝒙∗
(1) = 𝒙◦+
N(0, 𝜎2
𝑛). However, there is a gap between the effect of perturbation
over 𝒙◦ and actual attack effect on 𝒙∗
(1). We demonstrate the gap
with an intuitive example in Figure 11.
Here we select an anomaly 𝒙◦ and depict the influence of pertur-
bations in two most important dimensions of ˜𝒙◦ on the attacker’s
objective function (17) and the actual attack effect (i.e., the inter-
pretation result). In Figure 11a, attackers choose the direction of
perturbation according to the gradient of objective function (17),
which is the red arrow in the figure. In Figure 11b, we evaluate
two metrics for actual attack effect: the first one is the L2-distance
between reference searching from ˜𝒙◦ and 𝒙◦ (left side of the fig-
ure), and the second one is JS between the two references (right
side of figure). We evaluate the actual attack effect under different
iterations (𝑚𝑎𝑥𝑖𝑡𝑒𝑟 ) of Interpreter. First, the results of L2-distance
demonstrate that our Interpreter is naturally robust against such
optimization-based attack since attackers cannot accurately simu-
late the overall gradients in our iterative optimization. As 𝑚𝑎𝑥𝑖𝑡𝑒𝑟
increases, the perturbation effect tends to be smooth, and the at-
tack effect completely disappears when 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 = 20. Second, the
results of JS demonstrate the large gap between attacker’s opti-
mization function and actual effect. The area that improves the
actual attack effect can be a very small and transient area in the
gradient direction of the attacker’s optimizer (See the results of
JS when 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 = 3, 5). In this context, I.R.N. with a small noise
on the searching entrance can eliminate the attack effect (see red
dotted lines). Third, we find that the value in the neighborhood of
𝒙◦ usually remains the same (i.e., light-colored near the origin in
Figure 11b), which demonstrates that I.R.N. with a small 𝜎𝑁 has
little impact on the original stability.
D.2 Robustness against distance-based attacks
Attack Methodology. As mentioned in §6.2, we evaluate another
type of attacks called distance-based attacks specifically designed
against the DeepAID by mislead the distance calculations in DeepAID.
(a) Attack effect on attacker’s objective function.
(b) Attack effect on actual Interpreter’s results.
Figure 11: Example of demonstrating robustness against
optimization-based attack.
The high-level idea of such attacks is to trick the Interpreter to
choose irrelevant features (i.e., zero dimensions in interpretation
𝒙◦ − 𝒙∗) by subtly perturbing them. Since there are two distance
metrics in our Interpreter (𝐿0 and 𝐿2, see Eq.(3)), we evaluate two
distance-based attacks: 𝐿0 attack adds small perturbations on groups
of irrelevant features to trick the 𝐿0 distance into selecting them,
while 𝐿2 attack adds a few irrelevant features with high values
to spoil the computation of the 𝐿2. Specifically, for 𝐿0 attack, we
choose 50% irrelevant features and add noise sampling from Gauss-
ian N(0, 𝜎2) with default 𝜎 = 0.01. This evaluation is similar to
robustness against noise in §6.2, while the difference is that 𝐿0
attack only perturb 50% dimensions. For 𝐿2 attack, we randomly
choose one irrelevant dimension and change it into 𝐴 times the
maximum value in anomaly with default 𝐴 = 0.8. Note that, the
huge change to the selected dimension will change it from irrel-
evant to relevant. Thus we ignore the selected dimension when
calculating JS, and only observe whether it has an impact on other
irrelevant dimensions.
Robustness Evaluation and Analysis. We use the same dataset
in §6.2 and evaluate two distance-based attacks on DeepAID and
other baseline Interpreters. We first evaluate with default 𝜎 or 𝐴 and
also evaluate the impact of two attack scale parameters. The results
are shown in Figure 12. We can observe that DeepAID Interpreter
is very robust against 𝐿0 attack. This is because we use gradient
× input together instead of only gradients when evaluating the
effectiveness of dimensions (as introduced in §4.2). This effectively
limits the impact of small perturbations on anomalies. As for 𝐿2
attack, DeepAID also outperforms other baselines. This is because
−0.10−0.050.000.050.10Δ̃x∘1−0.10−0.050.000.050.10Δ̃x∘2Attack Objective Function0.01280.01320.01360.01400.01440.01480.01520.0156Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3215(a) Attack scale 𝜎 = 0.01, 𝐴 = 0.8.
(b) Interp. dimension 𝐾 = 20 (20%).
Figure 12: Robustness evaluation against distance-based at-
tacks (higher is better).
we only update a small number of features in each iteration to
satisfy conciseness (as introduced in §4.2). Therefore, DeepAID can
update selected features under 𝐿2 attack without affecting many
other features. This is also why in Figure 12a DeepAID becomes
more robust against 𝐿2 attack when the number of dimensions
increases. The results in Figure 12b also demonstrate the strong
robustness of DeepAID. Particularly, DeepAID is not sensitive to
the degree of attack.
E GRAPH DATA BASED EXPERIMENTS
In the main body, we primarily use tabular and time-series based
systems for evaluation and illustration due to space limit. Another
reason is that we have not found any interpreter that can be used
as a baseline for unsupervised graph learning tasks (Recall Table 1).
Although there are a few methods suitable for graph data such as
[33], they are difficult to migrate to unsupervised scenarios. Hence,
we only evaluate DeepAID itself for graph data based systems.
Below, we use GLGV to briefly illustrate the adoption of DeepAID
on graph data based systems.
Backgrounds. The backgrounds of GLGV have been introduced in
§2.3. GLGV [6] detects lateral movement in APT campaigns from
authentication logs within enterprise networks in an unsupervised
(specifically, reconstruction-based) fashion. It first builds authenti-
cation logs into authentication graph, where the nodes represent
IP/device/users, and the edges represent authentication or access.
Figure 13(a) shows an example of a benign authentication graph
without any attack (i.e., normal data), which can be divided into
three domains According to different permissions or authentication
relationships. Only the administrator “admin” has the permission
to access the Domain Controller (DC).
Settings. As mentioned in §4.4, we implement two versions of
graph data Interpreter according to Algorithm 3, distinguished by
whether E𝐺 is differentiable, denoted with DeepAID and DeepAID′.
DeepAID means E𝐺 is differentiable, thus we use gradient-based op-
timization method in Interpreter, while DeepAID′ uses BFS-based
method to address the indifferentiable problem. For DeepAID′, we
use three 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 (Abbreviated as 𝑚): 𝑚 = 5, 10, 20. 𝑚 directly re-
flects how many neighborhoods are viewed in the search process.
We use 10% normal data with over 10M logs to construct the au-
thentication graph, which consists of 47,319 nodes and 59,105,124
edges. We use 747 anomalies in the dataset for evaluation.
Performance of Interpreter. We primarily evaluate the fidelity
and efficiency of DeepAID Interpreter. The same evaluation meth-
ods and indicators as §6.2 are used here. Note that runtime in
efficiency evaluation is per 100 anomalies. The results are shown
in Figure 14. We can find that DeepAID (with differentiable E𝐺)
are more accurate (i.e., with high fidelity) but less efficient as the
number of parameters of the neural network are too large and the
back propagation is very slow. For search-based methods, DeepAID
with larger 𝑚 is more accurate while also more time-consuming
(but is acceptable). In summary, for security systems whose E𝐺
is indifferentiable, we recommend the search-based Interpreter,
as DeepAID′ needs to re-implement E𝐺, which will increase the
workload. For systems with differentiable E𝐺, both methods can be
used depending on how the operator treats the fidelity-efficiency
trade-off.
Usage of DeepAID. We provide two simple cases to illustrate how
to use DeepAID on GLGV for interpreting anomalies and explaining
FPs. We still use the scenario shown in Figure 13(a). As shown in
Figure 13(b), suppose that an anomaly is reported by GLGV, which
is the link from “user3” to DC (denoted by the red line). Now oper-
ators would like to find the basis of this decision-making by GLGV.
After using our Interpreter, the reference link denoted by the green
line from “admin” to DC is provided. This means that “user3” to
DC is decided as an anomaly because GLGV thinks (only) “admin”
can access to (or, authenticate to) DC. Furthermore, we can also
reconstruct possible attack scenarios with the help of DeepAID
interpretations: “user3” were compromised by the attacker (e.g.,
maybe after downloading malware from a phishing email). The
attacker gaining the initial foothold of user3’s devices would like to
move laterally in this enterprise network for more harmful attacks.
Several technicals such as vulnerability exploitation were used by
the attackers to pursue a higher permission. Finally, the attacker
successfully escalated her privilege to access the DC [6].
In Figure 13(c), we provide another case of using DeepAID to
analyze FPs in GLGV. Here GLGV considers the link from “user2”
to email server is an anomaly, which is actually an FP. From the
interpretation result provided by our DeepAID, we can find that
GLGV thinks (only) “user1” has the privilege to access the email
server. In other words, GLGV thinks “user1” and “user2” are under
different levels of privileges, which is not the case. Therefore, with
the help of DeepAID, we can confidently conclude that GLGV make
mistakes in its prediction of “user2”. Through further analysis, we
find that this is caused by insufficient representation ability of em-
bedding vectors. Therefore, we solve this FP problem by increasing
the length of neighbors traversed by random walk in GLGV and
increasing the number of iterations of the embedding process.
F HYPER-PARAMETERS SENSITIVITY
DeepAID has some hyper-parameters in Interpreter and Distiller.
Below, we first test the sensitivity of these hyper-parameters and
then discuss how to configure them in the absence of anomalies.
10%20%30%40%% Dimensions0.000.250.500.751.00Jaccard SimilarityL0 Attack10%20%30%40%% Dimensions0.000.250.500.751.00Jaccard SimilarityL2 Attack0.010.020.030.04Attack Scale σ0.000.250.500.751.00Jaccard SimilarityL0 Attack0.60.81.0Attack Scale A0.000.250.500.751.00Jaccard SimilarityL2 AttackLIMELEMNACOINDeepLIFTCADEDeepAIDSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3216Figure 13: Illustrations of interpreting graph data based system (GLGV).
Table 8: Sensitivity test of hyper-parameters in Interpreter.
Parameters Default Testing Range
LFR (std.)
0.5
20
0.001
0.01
0.01
0.3
[0.1, 5.0]
[10, 100]
[0.0005,0.005]
[0.005, 0.05]
[0.001, 0.1]
[0.2, 0.4]
𝛼
𝑚𝑎𝑥𝑖𝑡𝑒𝑟
𝜆
𝜖
𝜇1
𝜇2
LFR (range)
[0.9150, 0.9215]
[0.9150, 0.9150]
[0.8954, 0.9150]
[0.9019, 0.9281]
[0.9525, 0.9525]
[0.9452, 0.9643]
0.0031
0.0000
0.0078
0.0098
0.0000
0.0061
Figure 14: Performance of DeepAID Interpreter for graph
data based system.
Hyper-parameters in DeepAID Interpreter. We evaluate six
hyper-parameters in Interpreter, they are learning rate 𝛼 in Adam
optimizer, number of maximum iteration 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 for optimization,
𝜆 in the stability term and 𝜖 in the fidelity term of objective functions
(7), (8), (12). For time-series Interpreter, another two parameters
𝜇1 and 𝜇2 (for saliency testing) are tested. We set 𝐾 = 10 (10%)
for testing parameters in tabular Interpreter and set 𝐾 = 3 (30%)
for time-series Interpreter, and use the same datasets in 6.1. We
primarily evaluate the sensitivity of fidelity (LFR). The stability and
robustness are insensitive to parameters (except for optimization-
based attack in D.1, which is more robust when 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 increases).
For efficiency, it is common sense that increasing the learning rate
and reducing the number of iterations can reduce runtime, while
other parameters are insensitive to efficiency. The testing method
is to change one parameter with a reasonable range while fixing
other parameters for each time. Table 8 lists their value range for
testing, as well as default values when testing other parameters.
The results of the change and standard deviation of LFR are also
listed in Table 8.
From the results in Table 8, we can find the change of LFR is
small (e.g., std. for all parameters is <1%). Thus, the performance of
DeepAID Interpreter is basically insensitive to hyper-parameters.
Hyper-parameters in DeepAID Distiller. Distiller introduce only
one new parameter 𝑀, the number of value intervals. Here we use
the same settings in Table 4 and 5, and we set 𝐾 = 10 and use 5-class
datasets. We evaluate f1-macro, f1-macro∗, TPR, FPR, and UACC
under different 𝑀. The results are shown in Figure 15. We can find
that the change of all metrics is very small when 𝑀 ≥ 20.
Hyper-parameters Configuration. We introduce the guideline
of configuring hyper-parameters of DeepAID for operators. Es-
pecially in the unsupervised setting, hyper-parameters should be
configured in the absence of anomalies (i.e., using normal data only).
Note that operators have a high fault tolerance rate of choosing pa-
rameters since we have demonstrated their insensitivity. First, 𝛼 and
Figure 15: Sensitivity test of hyper-parameter 𝑀 in Distiller.
𝑚𝑎𝑥𝑖𝑡𝑒𝑟 are easy to configure since the learning rate of Adam opti-
mizer has been well studied in machine learning community, and
𝑚𝑎𝑥𝑖𝑡𝑒𝑟 should be increased appropriately if 𝛼 is small. Also con-
sidering the adversarial robustness (D.1), we suggest that 𝑚𝑎𝑥𝑖𝑡𝑒𝑟
should not be too small, such as more than 5. 𝜆 balances the weight
between fidelity and stability term. This is a free choice for opera-
tors. However, we should ensure that the values of the two terms
are at the same order of magnitude. Thus, we can estimate the
approximate value of the two terms, and can fine-tune the value
after division. Note that, this does not need knowledge of anomalies
since we only estimate the magnitude of two terms (i.e., only need
to know the value range of normalized features). For configuring
𝜖, we are more concerned about the upper bound, i.e., 𝜖 cannot
be too large. A feasible method to measure the bound is to mea-
sure the value difference between the 99th quantile in normal data
and the anomaly threshold (𝑡𝑅, 𝑡𝑃). On the contrary, we are more
concerned about the lower bound of 𝜇1. A feasible method is to
measure the gradient term in (11) with all normal data, and choose
the maximum (or quantile) as the lower bound of 𝜇1. For 𝜇2, it can
be configured through computing the mean of Pr(𝑥𝑡|𝑥1𝑥2...𝑥𝑡−1)
in normal data. Note that, 𝜇2 must be greater than 1/𝑁𝑡𝑠 where 𝑁𝑡𝑠
is number of “classes” of 𝑥𝑡, otherwise 𝜇2 cannot capture the class
with the largest probability.
adminDomainController user1user2admin-PCEmail SeverFile Severip1Sever 1ip2user3ip1-PCadminDomainController user1user2admin-PCEmail SeverFile Severip1Sever 1ip2malip1-PCadminDomainController user2admin-PCEmail SeverFile Severip1Sever 1ip2user3ip1-PCuser1(a) Example of normal authentication graph(b) Example of interpretation results(c) Example of interpreting FPsDeepAID0(m=5)DeepAID0(m=10)DeepAID0(m=20)DeepAID102103Time Elapsed (Lower is Better)GLGV (Graph Data, DeepWalk)Efficiency0.50.60.70.80.91.0LFR (Higher is Better)Fidelity0.70.80.91.01020304050# Intervals (M)0.00.1f1-macro*f1-macroUACCTPRFPRSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3217