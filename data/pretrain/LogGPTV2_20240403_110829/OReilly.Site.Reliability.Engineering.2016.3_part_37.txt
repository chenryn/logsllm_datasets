machines
Reliable Replicated Datastores and Configuration Stores
Reliable replicated datastores are an application of replicated state machines. Replica‐
ted datastores use consensus algorithms in the critical path of their work. Thus, per‐
formance, throughput, and the ability to scale are very important in this type of
design. As with datastores built with other underlying technologies, consensus-based
datastores can provide a variety of consistency semantics for read operations, which
make a huge difference to how the datastore scales. These trade-offs are discussed in
“Distributed Consensus Performance” on page 296.
Other (nondistributed-consensus–based) systems often simply rely on timestamps to
provide bounds on the age of data being returned. Timestamps are highly problem‐
atic in distributed systems because it’s impossible to guarantee that clocks are
synchronized across multiple machines. Spanner [Cor12] addresses this problem by
modeling the worst-case uncertainty involved and slowing down processing where
necessary to resolve that uncertainty.
Highly Available Processing Using Leader Election
Leader election in distributed systems is an equivalent problem to distributed consen‐
sus. Replicated services that use a single leader to perform some specific type of work
292 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
in the system are very common; the single leader mechanism is a way of ensuring
mutual exclusion at a coarse level.
This type of design is appropriate where the work of the service leader can be per‐
formed by one process or is sharded. System designers can construct a highly avail‐
able service by writing it as though it was a simple program, replicating that process
and using leader election to ensure that only one leader is working at any point in
time (as shown in Figure 23-3). Often the work of the leader is that of coordinating
some pool of workers in the system. This pattern was used in GFS [Ghe03] (which
has been replaced by Colossus) and the Bigtable key-value store [Cha06].
Figure 23-3. Highly available system using a replicated service for master election
In this type of component, unlike the replicated datastore, the consensus algorithm is
not in the critical path of the main work the system is doing, so throughput is usually
not a major concern.
Distributed Coordination and Locking Services
A barrier in a distributed computation is a primitive that blocks a group of processes
from proceeding until some condition is met (for example, until all parts of one phase
of a computation are completed). Use of a barrier effectively splits a distributed com‐
putation into logical phases. For instance, as shown in Figure 23-4, a barrier could be
used in implementing the MapReduce [Dea04] model to ensure that the entire Map
phase is completed before the Reduce part of the computation proceeds.
System Architecture Patterns for Distributed Consensus | 293
Figure 23-4. Barriers for process coordination in the MapReduce computation
The barrier could be implemented by a single coordinator process, but this imple‐
mentation adds a single point of failure that is usually unacceptable. The barrier can
also be implemented as an RSM. The Zookeeper consensus service can implement the
barrier pattern: see [Hun10] and [Zoo14].
Locks are another useful coordination primitive that can be implemented as an RSM.
Consider a distributed system in which worker processes atomically consume some
input files and write results. Distributed locks can be used to prevent multiple work‐
ers from processing the same input file. In practice, it is essential to use renewable
leases with timeouts instead of indefinite locks, because doing so prevents locks from
being held indefinitely by processes that crash. Distributed locking is beyond the
scope of this chapter, but bear in mind that distributed locks are a low-level systems
primitive that should be used with care. Most applications should use a higher-level
system that provides distributed transactions.
Reliable Distributed Queuing and Messaging
Queues are a common data structure, often used as a way to distribute tasks between
a number of worker processes.
Queuing-based systems can tolerate failure and loss of worker nodes relatively easily.
However, the system must ensure that claimed tasks are successfully processed. For
that purpose, a lease system (discussed earlier in regard to locks) is recommended
instead of an outright removal from the queue. The downside of queuing-based sys‐
tems is that loss of the queue prevents the entire system from operating. Implement‐
ing the queue as an RSM can minimize the risk, and make the entire system far more
robust.
294 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Atomic broadcast is a distributed systems primitive in which messages are received
reliably and in the same order by all participants. This is an incredibly powerful dis‐
tributed systems concept and very useful in designing practical systems. A huge num‐
ber of publish-subscribe messaging infrastructures exist for the use of system
designers, although not all of them provide atomic guarantees. Chandra and Toueg
[Cha96] demonstrate the equivalence of atomic broadcast and consensus.
The queuing-as-work-distribution pattern, which uses the queue as a load balancing
device, as shown in Figure 23-5, can be considered to be point-to-point messaging.
Messaging systems usually also implement a publish-subscribe queue, where mes‐
sages may be consumed by many clients that subscribe to a channel or topic. In this
one-to-many case, the messages on the queue are stored as a persistent ordered list.
Publish-subscribe systems can be used for many types of applications that require cli‐
ents to subscribe to receive notifications of some type of event. Publish-subscribe sys‐
tems can also be used to implement coherent distributed caches.
Figure 23-5. A queue-oriented work distribution system using a reliable consensus-based
queuing component
Queuing and messaging systems often need excellent throughput, but don’t need
extremely low latency (due to seldom being directly user-facing). However, very high
latencies in a system like the one just described, which has multiple workers claiming
tasks from a queue, could become a problem if the percentage of processing time for
each task grew significantly.
System Architecture Patterns for Distributed Consensus | 295
Distributed Consensus Performance
Conventional wisdom has generally held that consensus algorithms are too slow and
costly to use for many systems that require high throughput and low latency [Bol11].
This conception is simply not true—while implementations can be slow, there are a
number of tricks that can improve performance. Distributed consensus algorithms
are at the core of many of Google’s critical systems, described in [Ana13], [Bur06],
[Cor12], and [Shu13], and they have proven extremely effective in practice. Google’s
scale is not an advantage here: in fact, our scale is more of a disadvantage because it
introduces two main challenges: our datasets tend to be large and our systems run
over a wide geographical distance. Larger datasets multiplied by several replicas rep‐
resent significant computing costs, and larger geographical distances increase latency
between replicas, which in turn reduces performance.
There is no one “best” distributed consensus and state machine replication algorithm
for performance, because performance is dependent on a number of factors relating
to workload, the system’s performance objectives, and how the system is to be
deployed.2 While some of the following sections present research, with the aim of
increasing understanding of what is possible to achieve with distributed consensus,
many of the systems described are available and are in use now.
Workloads can vary in many ways and understanding how they can vary is critical to
discussing performance. In the case of a consensus system, workload may vary in
terms of:
• Throughput: the number of proposals being made per unit of time at peak load
• The type of requests: proportion of operations that change state
• The consistency semantics required for read operations
• Request sizes, if size of data payload can vary
Deployment strategies vary, too. For example:
• Is the deployment local area or wide area?
• What kinds of quorum are used, and where are the majority of processes?
• Does the system use sharding, pipelining, and batching?
Many consensus systems use a distinguished leader process and require all requests to
go to this special node. As shown in Figure 23-6, as a result, the performance of the
system as perceived by clients in different geographic locations may vary considera‐
2 In particular, the performance of the original Paxos algorithm is not ideal, but has been greatly improved over
the years.
296 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
bly, simply because more distant nodes have longer round-trip times to the leader
process.
Figure 23-6. The effect of distance from a server process on perceived latency at the client
Multi-Paxos: Detailed Message Flow
The Multi-Paxos protocol uses a strong leader process: unless a leader has not yet been
elected or some failure occurs, it requires only one round trip from the proposer to a
quorum of acceptors to reach consensus. Using a strong leader process is optimal in
terms of the number of messages to be passed, and is typical of many consensus
protocols.
Figure 23-7 shows an initial state with a new proposer executing the first Prepare/
Promise phase of the protocol. Executing this phase establishes a new numbered
view, or leader term. On subsequent executions of the protocol, while the view
remains the same, the first phase is unnecessary because the proposer that established
the view can simply send Accept messages, and consensus is reached once a quorum
of responses is received (including the proposer itself).
Figure 23-7. Basic Multi-Paxos message flow
Distributed Consensus Performance | 297
Another process in the group can assume the proposer role to propose messages at
any time, but changing the proposer has a performance cost. It necessitates the extra
round trip to execute Phase 1 of the protocol, but more importantly, it may cause a
dueling proposers situation in which proposals repeatedly interrupt each other and no
proposals can be accepted, as shown in Figure 23-8. Because this scenario is a form of
a livelock, it can continue indefinitely.
Figure 23-8. Dueling proposers in Multi-Paxos
All practical consensus systems address this issue of collisions, usually either by elect‐
ing a proposer process, which makes all proposals in the system, or by using a rotat‐
ing proposer that allocates each process particular slots for their proposals.
For systems that use a leader process, the leader election process must be tuned care‐
fully to balance the system unavailability that occurs when no leader is present with
the risk of dueling proposers. It’s important to implement the right timeouts and
backoff strategies. If multiple processes detect that there is no leader and all attempt
to become leader at the same time, then none of the processes is likely to succeed
(again, dueling proposers). Introducing randomness is the best approach. Raft
[Ong14], for example, has a well-thought-out method of approaching the leader elec‐
tion process.
Scaling Read-Heavy Workloads
Scaling read workload is often critical because many workloads are read-heavy. Repli‐
cated datastores have the advantage that the data is available in multiple places, mean‐
ing that if strong consistency is not required for all reads, data could be read from any
298 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
replica. This technique of reading from replicas works well for certain applications,
such as Google’s Photon system [Ana13], which uses distributed consensus to coordi‐
nate the work of multiple pipelines. Photon uses an atomic compare-and-set opera‐
tion for state modification (inspired by atomic registers), which must be absolutely
consistent; but read operations may be served from any replica, because stale data
results in extra work being performed but not incorrect results [Gup15]. The trade-
off is worthwhile.
In order to guarantee that data being read is up-to-date and consistent with any
changes made before the read is performed, it is necessary to do one of the following:
• Perform a read-only consensus operation.
• Read the data from a replica that is guaranteed to be the most up-to-date. In a
system that uses a stable leader process (as many distributed consensus imple‐
mentations do), the leader can provide this guarantee.
• Use quorum leases, in which some replicas are granted a lease on all or part of
the data in the system, allowing strongly consistent local reads at the cost of some
write performance. This technique is discussed in detail in the following section.
Quorum Leases
Quorum leases [Mor14] are a recently developed distributed consensus performance
optimization aimed at reducing latency and increasing throughput for read opera‐
tions. As previously mentioned, in the case of classic Paxos and most other dis‐
tributed consensus protocols, performing a strongly consistent read (i.e., one that is
guaranteed to have the most up-to-date view of state) requires either a distributed
consensus operation that reads from a quorum of replicas, or a stable leader replica
that is guaranteed to have seen all recent state changing operations. In many systems,
read operations vastly outnumber writes, so this reliance on either a distributed oper‐
ation or a single replica harms latency and system throughput.
The quorum leasing technique simply grants a read lease on some subset of the repli‐
cated datastore’s state to a quorum of replicas. The lease is for a specific (usually brief)
period of time. Any operation that changes the state of that data must be acknowl‐
edged by all replicas in the read quorum. If any of these replicas becomes unavailable,
the data cannot be modified until the lease expires.
Quorum leases are particularly useful for read-heavy workloads in which reads for
particular subsets of the data are concentrated in a single geographic region.
Distributed Consensus Performance | 299
Distributed Consensus Performance and Network Latency
Consensus systems face two major physical constraints on performance when com‐
mitting state changes. One is network round-trip time and the other is time it takes to
write data to persistent storage, which will be examined later.
Network round-trip times vary enormously depending on source and destination
location, which are impacted both by the physical distance between the source and
the destination, and by the amount of congestion on the network. Within a single
datacenter, round-trip times between machines should be on the order of a millisec‐
ond. A typical round-trip-time (RTT) within the United States is 45 milliseconds, and
from New York to London is 70 milliseconds.
Consensus system performance over a local area network can be comparable to that
of an asynchronous leader-follower replication system [Bol11], such as many tradi‐
tional databases use for replication. However, much of the availability benefits of dis‐
tributed consensus systems require replicas to be “distant” from each other, in order
to be in different failure domains.
Many consensus systems use TCP/IP as their communication protocol. TCP/IP is
connection-oriented and provides some strong reliability guarantees regarding FIFO
sequencing of messages. However, setting up a new TCP/IP connection requires a
network round trip to perform the three-way handshake that sets up a connection
before any data can be sent or received. TCP/IP slow start initially limits the band‐
width of the connection until its limits have been established. Initial TCP/IP window
sizes range from 4 to 15 KB.
TCP/IP slow start is probably not an issue for the processes that form a consensus
group: they will establish connections to each other and keep these connections open
for reuse because they’ll be in frequent communication. However, for systems with a
very high number of clients, it may not be practical for all clients to keep a persistent
connection to the consensus clusters open, because open TCP/IP connections do
consume some resources, e.g., file descriptors, in addition to generating keepalive
traffic. This overhead may be an important issue for applications that use very highly
sharded consensus-based datastores containing thousands of replicas and an even
larger numbers of clients. A solution is to use a pool of regional proxies, as shown in
Figure 23-9, which hold persistent TCP/IP connections to the consensus group in
order to avoid the setup overhead over long distances. Proxies may also be a good
way to encapsulate sharding and load balancing strategies, as well as discovery of
cluster members and leaders.
300 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Figure 23-9. Using proxies to reduce the need for clients to open TCP/IP connections
across regions
Reasoning About Performance: Fast Paxos
Fast Paxos [Lam06] is a version of the Paxos algorithm designed to improve its per‐
formance over wide area networks. Using Fast Paxos, each client can send Propose
messages directly to each member of a group of acceptors, instead of through a
leader, as in Classic Paxos or Multi-Paxos. The idea is to substitute one parallel mes‐
sage send from the client to all acceptors in Fast Paxos for two message send opera‐
tions in Classic Paxos:
• One message from the client to a single proposer
• A parallel message send operation from the proposer to the other replicas
Intuitively, it seems as though Fast Paxos should always be faster than Classic Paxos.
However, that’s not true: if the client in the Fast Paxos system has a high RTT (round-
trip time) to the acceptors, and the acceptors have fast connections to each other, we
have substituted N parallel messages across the slower network links (in Fast Paxos)
for one message across the slower link plus N parallel messages across the faster links
(Classic Paxos). Due to the latency tail effect, the majority of the time, a single round
trip across a slow link with a distribution of latencies is faster than a quorum (as
shown in [Jun07]), and therefore, Fast Paxos is slower than Classic Paxos in this case.
Many systems batch multiple operations into a single transaction at the acceptor to
increase throughput. Having clients act as proposers also makes it much more diffi‐
Distributed Consensus Performance | 301
cult to batch proposals. The reason for this is that proposals arrive independently at
acceptors so you can’t then batch them in a consistent way.
Stable Leaders
We have seen how Multi-Paxos elects a stable leader to improve performance. Zab
[Jun11] and Raft [Ong14] are also examples of protocols that elect a stable leader for
performance reasons. This approach can allow read optimizations, as the leader has
the most up-to-date state, but also has several problems:
• All operations that change state must be sent via the leader, a requirement that
adds network latency for clients that are not located near the leader.
• The leader process’s outgoing network bandwidth is a system bottleneck
[Mao08], because the leader’s Accept message contains all of the data related to
any proposal, whereas other messages contain only acknowledgments of a num‐
bered transaction with no data payload.
• If the leader happens to be on a machine with performance problems, then the
throughput of the entire system will be reduced.
Almost all distributed consensus systems that have been designed with performance
in mind use either the single stable leader pattern or a system of rotating leadership in