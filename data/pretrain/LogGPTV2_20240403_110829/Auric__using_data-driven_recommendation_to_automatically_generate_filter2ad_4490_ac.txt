(1)
𝑚𝑖𝑛𝑖𝑚𝑖𝑧𝑒 ||𝑌 (𝑖) − 𝛽(𝑖) · 𝑋||2 + 𝜆||𝛽(𝑖)||1
The regularization parameter is 𝜆 ∈ [0, 1] and 𝑙𝑝 norm is
𝐴 |𝑝) 1
𝑝
||𝛽(𝑖)||𝑝 = (|𝛽(𝑖)
1 |𝑝 + |𝛽(𝑖)
2 |𝑝 + ... + |𝛽(𝑖)
(2)
For non-linear regression, we bring in deep neural networks [18]
(also, known as multi-layer perceptrons) that consists of multiple
hidden layers (or, neurons) between the predictor carrier attributes
(or, input layer 𝑋) and the predictee configuration parameters (or,
output variable 𝑌). An example 3-hidden layer fully connected deep
neural network is shown in Fig. 7. Compared to linear regression
models, deep neural networks can discover complex dependencies
between parameters and attributes. One well-known challenge
with deep neural network is the explanation for the prediction.
𝑘-nearest neighbors use distance between attributes to find the
Figure 5: Auric overview.
Table 2: Notations used in Auric.
Total number of carriers in the network (𝑗 = 1..𝑁 )
Total number of configuration parameters for the carriers (𝑖 = 1..𝑀)
𝑗𝑡ℎ carrier
vector of size 𝑁 for the 𝑖𝑡ℎ configuration parameter
value for the 𝑖𝑡ℎ configuration parameter and 𝑗𝑡ℎ carrier
value for the 𝑖𝑡ℎ parameter and between 𝑗𝑡ℎ carrier and its neighbor 𝑘
Total number of carrier attributes
matrix of size 𝑁 × 𝐴 for 𝐴 attributes across 𝑁 carriers
attribute vector of size 𝐴 for the 𝑗𝑡ℎ carrier
𝑁
𝑀
𝐶 𝑗
𝑌 (𝑖)
𝑌 (𝑖)
𝑌 (𝑖)
𝐴
𝑋
𝑋 𝑗,∗
𝑗,𝑘
𝑗
Since 𝑋 and 𝑌 can contain nominal variables, we use one-hot
encoding to translate them. For example, if a hardware version
attribute takes values 𝐻1, 𝐻2, 𝐻3, we create three columns in 𝑋 for
𝐻1, 𝐻2 and 𝐻3 and the corresponding value for the carrier is binary
(0 or 1).
812
Auric: Using Data-driven Recommendation to
Automatically Generate Cellular Configuration
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Figure 7: A deep neural network between input layer of car-
rier attributes, output layer of configuration parameter and
three hidden layers.
Figure 8: An example decision tree learner that presents a
simple explanation for the configuration parameter 𝑝1 rec-
ommendation.
closest 𝑘 neighbors for the carrier, and then predicts the parameter
value using the weighted average. 𝑘-nearest neighbors and simple
cosine similarity based methods do not filter out the attributes that
do not have a strong correlation with the configuration parameters.
Thus, they suffer from inaccurate recommendations in cases where
the carriers are highly similar but are labeled to be far away based
on the distance influenced by the irrelevant attributes.
For enabling easy explanation of recommendations, we explore
learners [20] such as decision tree and random forest. The decision
tree learner starts from the carrier attribute observations, splits
them on the feature that results in the largest information gain
and thereby reducing the uncertainty in the final decision. This
splitting process is repeated at each child node until the samples
at each leaf node belong to the same class. The advantage of deci-
sion tree learner is simple explanation and understanding of the
recommendations made for the configuration parameters. We show
an example decision tree in Fig. 8. One can easily traverse down
the tree following the attribute values and identify the parameter
value. Decision tree provides explanations that our engineers found
quite intuitive and easy to verify. The random forest learner uses
ensemble techniques across multiple decisions trees and generates
the average prediction of the individual trees. Ensemble techniques
are known to control over-fitting and improve prediction accuracy.
Analogous to the motivation behind decision trees, we next
explore collaborative filtering with chi-square tests of independence
to build the dependency model and a simple voting approach across
carriers to recommend the configuration parameter value that has
highest support from other carriers. The chi-square test is a non-
parametric test to identify the association between carrier attributes
813
Figure 9: An example contingency table between attribute
morphology and configuration 𝑝1.
and configuration parameters for existing carriers 𝐶1, 𝐶2, ..., 𝐶𝑁−1.
The test uses a contingency table to lay out the total counts for each
pair of attribute value and configuration parameter value. Fig. 9
shows an example contingency table.
𝑋 𝑗 and each parameter 𝑌 (𝑖) using
We compute the chi-square test statistic 𝜒2
𝑖,𝑗 for each attribute
𝑅∑︁
𝐶∑︁
𝑎=1
𝑏=1
𝜒2
𝑖,𝑗 =
(𝑂𝑎𝑏 − 𝐸𝑎𝑏)2
𝐸𝑎𝑏
(3)
where, 𝑂𝑎𝑏 is the observed count for the 𝑎𝑡ℎ row (attribute value)
and 𝑏𝑡ℎ column (parameter value). 𝐸𝑎𝑏 is the expected cell count
in the 𝑎𝑡ℎ row and 𝑏𝑡ℎ column of the table.
𝑎=1 𝑂𝑎𝑏 ·𝐶
𝑅
𝑅
𝐶
𝑎=1
𝑏=1 𝑂𝑎𝑏
𝑏=1 𝑂𝑎𝑏
𝐸𝑎𝑏 =
(4)
𝑅 and 𝐶 are the number of rows and columns. We then compare
𝜒2
𝑖,𝑗 to the critical value from the chi-square distribution table with
degrees of freedom 𝑑 𝑓 = (𝑅 − 1)(𝐶 − 1) and selected confidence
value. If 𝜒2
𝑖,𝑗 is greater than the critical value, then we reject the
null hypothesis that 𝑋 𝑗 and 𝑌 (𝑖) are independent.
Thus, for each parameter 𝑌 (𝑖), we identify the list of dependent
attributes. For a new carrier 𝐶𝑁 , we identify the similar carriers that
have exact matching values on the dependent attributes. Amongst
the similar carriers, we take a voting approach to identify the pa-
rameter value for each configuration parameter 𝑌 (𝑖)
𝑁 that gets max-
imum support across the carriers. We use a threshold of 75% in our
implementation and evaluation to determine the voting support.
Intuition behind why we believe collaborative filtering will
better suit our problem: As described in Section 2.6, we observed
a high variability or a large number of distinct values for the con-
figuration parameters across different geographic locations, as well
as high skewness in the distribution of the parameter values. This
poses significant challenges for the classic classifiers such as deci-
sion tree, random forest, deep neural networks to accurately learn
the dependency models. These classifiers typically aim to learn the
models that best fit for the frequently occurring samples, and then
treat the rare samples as outliers. It is equally important for us to
accurately identify the parameter values that are configured for
a small number of locations. Even though rare, these configura-
tion parameter settings serve the purpose of providing enhanced
service performance experience to the users. Thus, our goal is to
accurately learn the dependency models for both the frequently as
well as rarely occurring samples. Collaborative filtering and voting
approach can handle this well (we will also demonstrate this using
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Ajay Mahimkar, Ashiwan Sivakumar, Zihui Ge, Shomik Pathak, Karunasish Biswas
empirical evaluation in Section 4). Collaborative filtering learns the
dependency model for each parameter value specific to carriers
and irrespective of whether the parameter value is rare or frequent
across the network. It also eliminates the irrelevant attributes with
respect to the parameter values. Models tuned to each carrier char-
acteristic is thus expected to improve the prediction accuracy and
recommend better configuration parameters for new carriers.
3.3 Geographical proximity
In Section 3.2, the dependency model in Auric uses all the existing
carriers to learn the dependency model and then Auric recommends
the configuration parameter value for the new carrier. We refer to
these learners as global learners since they use the configurations
across the whole network. In this section, we propose to adapt those
learners to use the geographical proximity to restrict the carriers
that can be used for learning and recommendation. When we apply
geographical proximity, we refer to those learners as local learners.
The intuition behind the local learners is that the configuration
parameters on geographically nearby carriers are highly likely to
have a strong dependency with the new carrier as opposed to far-
away carriers. For example, a new carrier that is added in downtown
Manhattan is likely to be similar in configuration to nearby carriers
with similar attributes within downtown Manhattan as opposed
to carriers far-away (say in Texas) even though they share similar
attributes. The Texas configuration might be different because of
different propagation patterns. Since the parameter tuning is highly
local and the similarity diminishes as one goes farther away from
the carriers, we expect the geographical proximity to improve the
quality of recommendations.
In Auric, we use the X2 LTE neighbor relations to capture ge-
ographically nearby neighbors for the carriers. We thus employ
collaborative filtering and voting approach with geographical prox-
imity in Auric to recommend the configuration parameter values
for the newly added carriers in the cellular network. We will show
in our evaluation (Section 4) that geographical proximity plays a
key role in configuration learning and the local learner outperforms
the global learner in prediction accuracy. The local learners with
decision tree, random forest, or deep neural network classifiers
would still struggle to handle the high variability and skewness of
the parameter values.
Our formulation and solution in Auric is general and can be easily
extended to other networks and services. For example, when adding
new node types such as routers, line cards, servers, 5G base stations,
or even virtualized or containerized functions to the networks, one
can leverage the data-driven approach from Auric to recommend
configuration for the newly added nodes.
4 EVALUATION
In this section, we evaluate the accuracy of Auric in generating LTE
network carrier configuration using data-driven learning and rec-
ommendation. Our evaluation is three fold: (i) identify the accuracy
of our global learners, (ii) highlight the importance of geograph-
ical proximity, and (iii) validate our accuracy with the network
engineers.
814
𝑗
4.1 Data set
We use the configuration parameter data set collected from 400K+
carriers across 28 markets in a large LTE network as described
in Section 2.6. A total of 65 configuration parameters with values
consisting of a range form our predictee. In other words, these
are candidate parameters which we would like to recommend the
values for. 26 out of the 65 configuration parameters have to be set
for pairs of carriers and their neighbors. These parameters are used
to deal with user mobility and handovers across carriers. So, for each
of the 26 pair-wise parameters, we represent the predictee variable
using 𝑌 (𝑖)
𝑗,𝑘 , which captures the 𝑖𝑡ℎ parameter for the current carrier
𝑗 and its neighbor 𝑘. For the rest of the 39 singular parameters, we
represent the predictee variable using 𝑌 (𝑖)
for each carrier 𝑗. Use
Table 2 to review the notations.
We use the carrier attributes as our predictor variables (𝑋) (refer
to Table 1 for the list). For model learning and recommendation
for singular parameters, we only use the attributes of the carrier,
whereas for pair-wise parameters, we use both the attributes of the
carriers and their corresponding neighbors. With 400K+ carriers,
26 pair-wise and 39 singular configuration parameters, we have a
total of 15M+ configuration parameter values. This is a very large
sample data set that we use for conducting our evaluation.
4.2 Methodology
Our evaluation approach treats each carrier like a new carrier of
interest and uses the rest as the existing carriers for learning and
recommendation. This gives us a large number of sample points
(15M+) to generate the configuration, and then compare to the
current values to identify if we recommended accurately or not. To
begin with, we first evaluate the accuracy of global learners across
only four markets with each one covering a different timezone. The
intent is to identify if there is any global learner that significantly
outperforms the others. Table 3 summarizes the timezone, number
of carriers and eNodeBs, and configuration parameters for the four
markets. The reason behind picking four out of 28 markets is to
present an in-depth analysis for each market. For the 4 markets, we
have a total of 4.5M configuration parameter values across 116K
carriers. We use the standard machine learning cross-validation
approach to compute the accuracy scores. We use scikit-learn for
our global learners.
Table 3: Data set to compare global learners across four mar-
kets with each one covering a different timezone.
Market 1
Market 2
Market 3
Market 4
All four
Timezone
Mountain
Central
Eastern
Pacific
Carriers
24,271
22,809
45,127
23,805
116,012
eNodeBs
1791
1521
2643
1679
7634
Parameters
930,481
676,627
2,012,021
909,010
4,528,139
1. Decision tree learner. We use Gini score to determine how to
split and the tree is expanded until all leaves are pure (i.e., all
data points contain the same label).
2. Random forest learner. We use 100 trees in the forest, and
Gini score for decision to split. Tree is expanded until all leaves
are pure.
3. 𝑘-Nearest neighbor learner. We use 𝑘 = 5, equal weighting
across neighbors and distance metric of Euclidean.
Auric: Using Data-driven Recommendation to
Automatically Generate Cellular Configuration
4. Deep neural network learner. We use 7 hidden layers with
sizes 100, 100, 100, 50, 50, 50, 10 tailored towards our input and
output layer sizes. We use the adam solver which is a stochastic
gradient-based optimizer, activation function of relu (rectified
linear unit function - 𝑓 (𝑥) = 𝑚𝑎𝑥(0, 𝑥)) for the hidden layers,
the regularization L2 penalty of 10−5, random state of 1, and
maximum iteration of 10000.
5. Collaborative filtering with chi-square test of indepen-
dence. We use a p-value of 0.01 which is the significance level
to identify if the observed distribution matches the expected
distribution. The recommendation is the parameter value that
has support from atleast 75% of the other carriers.
For each predictee vector 𝑌 (𝑖) and predictor matrix 𝑋, we first
perform one-hot encoding before we pass the data to the learner for