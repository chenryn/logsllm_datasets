In the rest of this section, we describe our multi-bit mecha-
nism, the KProp layer, and the Drop algorithms in more detail.
The overview of our framework is depicted in Figure 2. Note that
the data perturbation step on the user-side has to be done only
once for each node. The server collects the perturbed data once and
stores it to train the GNN with minimum communication overhead.
3.1 Collection of node features
In this section, we explain our multi-bit mechanism for multidi-
mensional feature perturbation, which is composed of an encoder
and a rectifier, as described in the following.
Multi-bit Encoder. This part, which is executed at the user-
side, perturbs the nodeâ€™s private feature vector and encodes it into a
compact vector that is sent efficiently to the server. More specifically,
assume that every node ğ‘£ owns a private ğ‘‘-dimensional feature
vector xğ‘£, whose elements lie in the range [ğ›¼, ğ›½]. When the server
requests the feature vector of ğ‘£, the node locally applies the multi-bit
encoder on xğ‘£ to get the corresponding encoded feature vector xâˆ—
ğ‘£,
which is then sent back to the server. Since this process is supposed
to be run only once, the generated xâˆ—
ğ‘£ is recorded by the node to
be returned in any subsequent calls to prevent the server from
recovering the private feature vector using repeated queries.
Our multi-bit encoder is built upon the 1-bit mechanism [11],
which returns either 0 or 1 for a single-dimensional input. How-
ever, as mentioned earlier, perturbing all the dimensions with a
high-dimensional input results in injecting too much noise, as the
total privacy budget has to be shared among all the dimensions.
To balance the privacy-accuracy trade-off, we need to reduce di-
mensionality to decrease the number of dimensions that have to
be perturbed. Still, since we cannot have the feature vectors of all
the nodes at one place (due to privacy reasons), we cannot use con-
ventional approaches, such as principal component analysis (PCA)
or any other machine learning-based feature selection method. In-
stead, we randomly perturb a subset of the dimensions and then
optimize the size of this subset to achieve the lowest variance in
estimating the Aggregate function.
Algorithm 1 describes this encoding process in greater detail.
Intuitively, the encoder first uniformly samples ğ‘š out of ğ‘‘ dimen-
sions without replacement, where ğ‘š is a parameter controlling how
many dimensions are perturbed. Then, for each sampled dimension,
the corresponding feature is randomly mapped to either -1 or 1,
with a probability depending on the per-dimension privacy budget
ğœ–/ğ‘š and the position of the feature value in the feature space, such
that values closer to ğ›¼ (resp. ğ›½) are likely to be mapped to -1 (resp. 1).
For other dimensions that are not sampled, the algorithm outputs
0. Therefore, a maximum of two bits per feature is enough to send
xâˆ—
ğ‘£ to the server. When ğ‘š = ğ‘‘, our algorithm reduces to the 1-bit
mechanism with a privacy budget of ğœ–/ğ‘‘ for every single dimension.
The following theorem ensures that the multi-bit encoder is ğœ–-LDP
(proof in the Appendix).
Theorem 3.1. The multi-bit encoder presented in Algorithm 1
satisfies ğœ–-local differential privacy for each node.
Multi-bit Rectifier. The output of the multi-bit encoder is sta-
tistically biased, i.e., E [xâˆ—] â‰  x. Therefore, the goal of the multi-bit
rectifier, executed at server-side, is to convert the encoded vector xâˆ—
to an unbiased perturbed vector xâ€², such that E [xâ€²] = x, as follows:
xâ€² = ğ‘…ğ‘’ğ‘ğ‘¡(xâˆ—) =
ğ‘‘(ğ›½ âˆ’ ğ›¼)
2ğ‘š
Â· ğ‘’ğœ–/ğ‘š + 1
ğ‘’ğœ–/ğ‘š âˆ’ 1 Â· xâˆ— + ğ›¼ + ğ›½
2
(4)
Note that this is not a denoising process to remove the noise from xâˆ—,
but the output vector xâ€² is still noisy and does not have any mean-
ingful information about the private vector x. The only difference
between xâˆ— and xâ€² is that the latter is unbiased, while the former is
not. The following results entail from the multi-bit rectifier:
Proposition 3.2. The multi-bit rectifier defined by (4) is unbiased.
Proposition 3.3. For any node ğ‘£ and any ğ‘– âˆˆ {1, 2, . . . , ğ‘‘}, the
variance of the multi-bit rectifier defined by (4) at dimension ğ‘– is:
(cid:32) ğ›½ âˆ’ ğ›¼
2
Â· ğ‘’ğœ–/ğ‘š + 1
ğ‘’ğœ–/ğ‘š âˆ’ 1
(cid:33)2
(cid:18)
ğ‘¥ğ‘£,ğ‘– âˆ’ ğ›¼ + ğ›½
2
(cid:19)2
âˆ’
(5)
ğ‘‰ ğ‘ğ‘Ÿ[ğ‘¥â€²
ğ‘£,ğ‘–] =
Â·
ğ‘‘
ğ‘š
The variance of an LDP mechanism is a key factor affecting the
estimation accuracy: a lower variance usually leads to a more accu-
rate estimation. Therefore, we exploit the result of Proposition 3.3
to find the optimal sampling parameter ğ‘š in the multi-bit encoder
(Algorithm 1) that minimizes the rectifierâ€™s variance, as follows:
Proposition 3.4. The optimal value of the sampling parameter
ğ‘š in Algorithm 1, denoted by ğ‘šâ˜…, is obtained as:
ğ‘šâ˜… = max(1, min(ğ‘‘,(cid:106) ğœ–
(cid:107)))
2.18
(6)
The above proposition implies that in the high-privacy regime
ğœ– â‰¤ 2.18, the multi-bit mechanism perturbs only one random di-
mension. Therefore, this process is similar to a randomized one-hot
encoding, except that here, the aggregation of these one-hot en-
coded features approximates the aggregation of the raw features.
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2133Figure 2: Overview of our locally private GNN training framework, featuring the multi-bit mechanism (MB Encoder and MB
Rectifier), randomized response (RR), KProp layers, and Drop training. Users run multi-bit encoder and randomized response
on their private features and labels, respectively, and send the output to the server, after which training begins. Green solid
arrows and red dashed arrows indicate the training and validation paths, respectively.
3.2 Approximation of graph convolution
Upon collecting the encoded vectors xâˆ—
ğ‘£ from every node ğ‘£ and
generating the corresponding perturbed vectors xâ€²
ğ‘£ using the multi-
bit rectifier, the server can initiate the training of the GNN. In the
first layer, the embedding for an arbitrary node ğ‘£ is generated by the
following (layer indicator subscripts and superscripts are omitted
for simplicity):(cid:98)hN(ğ‘£) = Aggregate(cid:0){xâ€²
ğ‘¢,âˆ€ğ‘¢ âˆˆ N(ğ‘£)}(cid:1)
hğ‘£ = Update(cid:16)(cid:98)hN(ğ‘£)
(cid:17)
where(cid:98)hN(ğ‘£) is the estimation of the first layer Aggregate function
of any node ğ‘£ by aggregating perturbed vectors xâ€²
ğ‘¢ of all the nodes
ğ‘¢ adjacent to ğ‘£. After this step, the server can proceed with the rest
of the layers to complete the forward propagation of the model,
exactly similar to a standard GNN. If the Aggregate function is
linear on its input (e.g., it is a weighted summation of the input
vectors), the resulting aggregation would also be unbiased, as stated
below:
(7)
(8)
Corollary 3.5. Given a linear aggregator function, the aggrega-
tion defined by (7) is an unbiased estimation for (1) at layer ğ‘™ = 1.
The following proposition also shows the relationship of the
estimation error in calculating the Aggregate function and the
neighborhood size |N(ğ‘£)| for the special case of using the mean
aggregator function:
Proposition 3.6. Given the mean aggregator function for the first
layer and ğ›¿ > 0, with probability at least 1 âˆ’ ğ›¿, for any node ğ‘£, we
have:
(cid:12)(cid:12)(cid:12)((cid:98)hN(ğ‘£))ğ‘– âˆ’ (hN(ğ‘£))ğ‘–
(cid:12)(cid:12)(cid:12) = O
(cid:33)
(cid:32)âˆšï¸ğ‘‘ log(ğ‘‘/ğ›¿)
ğœ–âˆšï¸|N(ğ‘£)|
max
ğ‘–âˆˆ{1,...,ğ‘‘}
(9)
The above proposition indicates that with the mean aggregator
function (which can be extended to other Aggregate functions as
well), the estimation error decreases with a rate proportional to the
square root of the nodeâ€™s degree. Therefore, the higher number of
neighbors, the lower the estimation error. But as mentioned earlier,
the size of N(ğ‘£) is usually small in real graphs, which hinders the
Aggregate function from driving out the injected noise on its own.
In a different context, prior works have shown that consider-
ing higher-order neighbors can help learn better node representa-
tions [2, 28, 36]. Inspired by these works, a potential solution to this
issue is to expand the neighborhood of each node ğ‘£ by considering
more nodes that are not necessarily adjacent to ğ‘£ but reside within
an adjustable local neighborhood around ğ‘£. To this end, we use
an efficient convolution layer, described in Algorithm 2, that can
effectively be used to address the small-size neighborhood issue.
The idea is simple: we aggregate features of those nodes that are up
to ğ¾ steps away from ğ‘£ by simply invoking the Aggregate func-
tion ğ¾ consecutive times, without any non-linear transformation
in between. For simplicity, we call this algorithm KProp, as every
node propagates its message to ğ¾ hops further.
As illustrated in Figure 2, we prepend KProp as a denoising layer
to the GNN. This approach has two advantages: first, it allows
to use any GNN architecture with any Aggregate function for
the backbone model, as KProp already uses a linear Aggregate
that satisfies Corollary 3.5; and second, it enables us to expand
the effective aggregation set size for every node by controlling the
step parameter ğ¾. However, it is essential to note that we cannot
arbitrarily increase the neighborhood size around a node, since
aggregating messages from too distant nodes could lead to over-
smoothing of output vectors [30]. Therefore, there is a trade-off
between the KPropâ€™s denoising accuracy and the overall GNNâ€™s
expressive power.
It is worth mentioning that in KProp, we perform aggregations
over N(ğ‘£)âˆ’{ğ‘£}, i.e., we do not include self-loops. While it has been
shown that adding self-loops can improve accuracy in conventional
GNNs [26], excluding self-connections works better when dealing
KPropCrossEntropyAccuracyKPropCrossEntropyRRGNNMBEncoderKPropRRUser-SideServer-SideLPGNNDROPMBRectifierSession 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2134:Graph G = (V, E); input vector xğ‘£, âˆ€ğ‘£ âˆˆ V; linear
aggregator function Aggregate; step parameter ğ¾ â‰¥ 0;
Algorithm 2: KProp Layer
Input
Output:Embedding vector hğ‘£, âˆ€ğ‘£ âˆˆ V
1 for all ğ‘£ âˆˆ V do in parallel
2
h0
N(ğ‘£) = xğ‘£
for ğ‘˜ = 1 to ğ¾ do
hğ‘˜N(ğ‘£) = Aggregate(cid:16){hğ‘˜âˆ’1
3
4
N(ğ‘¢) , âˆ€ğ‘¢ âˆˆ N(ğ‘£) âˆ’ {ğ‘£}}(cid:17)
end
hğ‘£ = hğ¾N(ğ‘£)
5
6
7 end
8 return {hğ‘£, âˆ€ğ‘£ âˆˆ V}
with noisy features. As ğ¾ grows, with self-loops, we account for the
injected noise in the feature vector of each node in the ğ‘£â€™s neigh-
borhood multiple times in the aggregation. Therefore, removing
self-loops helps to reduce the total noise by discarding repetitive
node features from the aggregation.
3.3 Learning with private labels
In this last part, we describe the method used to perturb and collect
labels privately and introduce our training algorithm for learning
locally private GNNs using perturbed labels, called label denoising
with propagation (Drop). Let ğ‘“ (x) = arg maxy Ë†ğ‘(y | x) be the target
node classifier, where Ë†ğ‘(y | x) = ğ‘”(x, G; W) approximates the class-
conditional probabilities ğ‘(y | x) and is modeled by a GNN ğ‘”(.)
with the learnable weight matrix W. The goal is to optimize W
such that Ë†ğ‘(y | x) becomes as close as possible to ğ‘(y | x). In
the standard setting, this is usually done by minimizing the cross-
entropy loss function between Ë†ğ‘(y | x) and true label y over the
set of labeled nodes VL:
â„“ (y, Ë†ğ‘(y | x)) = âˆ’ âˆ‘ï¸
ğ‘£âˆˆVL
ğ‘£ log Ë†ğ‘(y | xv)
yğ‘‡
(10)
However, since the labels are considered private, each node
ğ‘£ âˆˆ VL that participates in the training procedure has to perturb
their label yğ‘£ using some LDP mechanism, and send the perturbed
label yâ€²
ğ‘£ to the server. Still, if we train the GNN using the perturbed
labels by minimizing the cross-entropy loss between Ë†ğ‘(y | x) and
perturbed labels yâ€², namely â„“ (yâ€², Ë†ğ‘(y | x)), the model completely
overfits the noisy labels and generalizes poorly to unseen nodes.
However, many real-world graphs, such as social networks, are
homophilic [34], meaning that nodes with structural similarity
tend to have similar labels [48]. We exploit this fact to estimate
the frequency of the labels in a local neighborhood around any
node ğ‘£ to obtain its estimated label Ëœyğ‘£. To this end, we can use any
LDP frequency oracle, such as randomized response [22], Unary
Encoding [52], or Local Hashing [52]. In this paper, we use ran-
domized response for two reasons: first, the number of classes is
usually small, and randomized response has been shown to work
better than other mechanisms in low dimensions [52]; and second,
it introduces a symmetric, class-independent noise to the labels by
flipping them according to the following distribution, which we
(cid:40) ğ‘’ğœ–
ğ‘’ğœ–+ğ‘âˆ’1 ,
ğ‘’ğœ–+ğ‘âˆ’1 ,
1
later exploit in our learning algorithm:
ğ‘(yâ€² | y) =
if yâ€² = y
otherwise
(11)
yâ€²
ğ‘¢
(12)
ğ‘¢âˆˆN(ğ‘£)
where y and yâ€² are clean and perturbed labels, respectively, ğ‘ is the
number of classes, and ğœ– is the privacy budget.
Similar to estimating the graph convolution with noisy features,
we also face the problem of small-size neighborhood if we only rely
on the first-order neighbors to estimate the label frequency. In order
to expand the neighborhood around each node, we take the same
approach as we did for features: we apply KProp on node labels,
i.e., we set Ëœğ‘¦ğ‘£ = arg maxğ‘–âˆˆ[ğ‘] â„ğ‘–(yâ€²
ğ‘£, ğ¾ğ‘¦) for all ğ‘£ âˆˆ VL, where
â„(.) is the KProp function, ğ¾ğ‘¦ is the step parameter, and [ğ‘] =
{1, . . . , ğ‘}. With the mean aggregator function, at every iteration,
KProp updates every nodeâ€™s label distribution by averaging its
neighborsâ€™ label distribution. In this paper, however, we instead use
the GCN aggregator function [26]:
Aggregate(cid:0){yâ€²
normalization factor in the GCN aggregator isâˆšï¸|N(ğ‘¢)| Â· |N(ğ‘£)|,
while for the mean, it is |N(ğ‘£)| =âˆšï¸|N(ğ‘£)| Â· |N(ğ‘£)|. In other words,
Using the GCN aggregator leads to a lower estimation error than
the mean aggregator due to the difference in their normalization
factors, which affects their estimation variance. Specifically, the
ğ‘¢,âˆ€ğ‘¢ âˆˆ N(ğ‘£)}(cid:1) =
âˆšï¸|N(ğ‘¢)| Â· |N(ğ‘£)|
âˆ‘ï¸
the GCN aggregator considers the square root of the degree of both
the central node ğ‘£ and its neighbor ğ‘¢, whereas the mean aggregator
considers only the square root of the central node ğ‘£â€™s degree twice.
Since there are many more low-degree nodes in many real graphs
than high-degree ones, using the mean aggregator results in a small
normalization factor for most nodes, leading to a higher estimation
variance. But as many of the low-degree nodes are linked to the
high-degree ones, the GCN aggregator balances the normalization
by considering the degree of both link endpoints. Consequently,
the normalization for many low-degree nodes increases compared
to the mean aggregator, yielding a lower estimation variance.
As the step parameter ğ¾ğ‘¦ gradually increases, the estimated label
Ëœy becomes more similar to the clean label y. Therefore, an initial idea
for the training algorithm would be to learn the GNN using Ëœy instead
of yâ€² by minimizing the cross-entropy loss between Ë†ğ‘(y | x) and Ëœy,
namely â„“ ( Ëœy, Ë†ğ‘(y | x)). However, this approach has two downsides.
First, it causes the GNN to become a predictor for Ëœy and not y.
Although Ëœy tend to converge to y as ğ¾ğ‘¦ increases, the output of
KProp also becomes increasingly smoother, until the excessive
KProp aggregations lead to over-smoothing, after which Ëœy will
begin to diverge from y and become noisy again, while we are still
fitting Ëœy. Second, we cannot know how far we should increase ğ¾ğ‘¦
to get the best accuracy and prevent over-smoothing without clean
validation data. One way to validate the model with noisy labels is
to calculate the accuracy of the target classifier ğ‘“ (x) for predicting
the estimated label Ëœy. However, suppose the model overfits the
over-smoothed labels. In that case, the corresponding validation
Ëœyâ€™s also becomes over-smoothed and can be well predicted by the
model, resulting in a high validation but low test accuracy.
To address the first issue, instead of minimizing â„“ ( Ëœy, Ë†ğ‘(y | x)),
we propose to minimize â„“ ( Ëœy, Ë†ğ‘( Ëœy | x)), i.e., the cross-entropy loss
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2135between the estimated label Ëœy and its approximated probability
Ë†ğ‘( Ëœy | x), which can be obtained by applying the same procedure
on Ë†ğ‘(y | x) as we did on y to obtain Ëœy. In the first place, we applied
randomized response on y to obtain yâ€², and then passed the result
to the KProp layer to get Ëœy. If we go through the same steps to
obtain Ë†ğ‘( Ëœy | x) and then minimize its cross-entropy loss with Ëœy, we
can keep Ë†ğ‘(y | x) intact when KProp causes over-smoothing, and
at the same time benefit from itâ€™s denoising capability. To this end,
we first need to calculate Ë†ğ‘(yâ€² | x) from Ë†ğ‘(y | x):
ğ‘(yâ€² | y) Â· Ë†ğ‘(y | x)
Ë†ğ‘(yâ€² | x) =
âˆ‘ï¸
(13)
y
where ğ‘(yâ€² | y) is directly obtained from (11). This step would be
analogous to applying randomized response to y and getting yâ€².
Finally, similar to applying KProp on yâ€² to get Ëœy, we treat Ë†ğ‘(yâ€² | x)
as soft labels and apply KProp with the same step parameter to
approximate Ë†ğ‘( Ëœy | x):
Ë†ğ‘( Ëœy | x) = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(cid:0)â„(cid:0) Ë†ğ‘(yâ€² | x), ğ¾ğ‘¦(cid:1)(cid:1)
(14)
where the softmax is used to normalize the KPropâ€™s output as a valid
probability distribution. Finally, we train the model by minimizing
â„“ ( Ëœy, Ë†ğ‘( Ëœy | x)).
To address the validation issue, we must make sure that our
validation procedure is not affected by the KProp step parameter
ğ¾ğ‘¦. Clearly, if we use Ëœy for validation, by changing ğ¾ğ‘¦ we are also
modifying estimated labels Ëœy, and thus we are basically validating
different models with different labels. Therefore, we should only val-
idate the model using the noisy labels yâ€². Here, we choose the cross-
entropy loss between yâ€² and Ë†ğ‘(yâ€² | x), namely â„“ (yâ€², Ë†ğ‘(yâ€² | x)),
as Patrini et al. [39] show that this loss function, which they call
forward correction loss, is unbiased, meaning that under expected
label noise, â„“ (yâ€², Ë†ğ‘(yâ€² | x)) is equal to â„“ (y, Ë†ğ‘(y | x)), i.e., the orig-
inal loss computed on clean data. Therefore, we train the GNN
with different hyper-parameters, including ğ¾ğ‘¦, and pick the one
achieving the lowest forward correction loss.
While this is in principle a reasonable idea, the forward correc-
tion loss on its own is not enough to prevent overfitting. Thatâ€™s
because when ğ¾ğ‘¦ is small, the estimated label Ëœy is more similar to
the noisy one yâ€² than the clean label y, and thus the model overfits