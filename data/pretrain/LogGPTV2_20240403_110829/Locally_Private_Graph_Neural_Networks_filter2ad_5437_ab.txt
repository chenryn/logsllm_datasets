In the rest of this section, we describe our multi-bit mecha-
nism, the KProp layer, and the Drop algorithms in more detail.
The overview of our framework is depicted in Figure 2. Note that
the data perturbation step on the user-side has to be done only
once for each node. The server collects the perturbed data once and
stores it to train the GNN with minimum communication overhead.
3.1 Collection of node features
In this section, we explain our multi-bit mechanism for multidi-
mensional feature perturbation, which is composed of an encoder
and a rectifier, as described in the following.
Multi-bit Encoder. This part, which is executed at the user-
side, perturbs the node’s private feature vector and encodes it into a
compact vector that is sent efficiently to the server. More specifically,
assume that every node 𝑣 owns a private 𝑑-dimensional feature
vector x𝑣, whose elements lie in the range [𝛼, 𝛽]. When the server
requests the feature vector of 𝑣, the node locally applies the multi-bit
encoder on x𝑣 to get the corresponding encoded feature vector x∗
𝑣,
which is then sent back to the server. Since this process is supposed
to be run only once, the generated x∗
𝑣 is recorded by the node to
be returned in any subsequent calls to prevent the server from
recovering the private feature vector using repeated queries.
Our multi-bit encoder is built upon the 1-bit mechanism [11],
which returns either 0 or 1 for a single-dimensional input. How-
ever, as mentioned earlier, perturbing all the dimensions with a
high-dimensional input results in injecting too much noise, as the
total privacy budget has to be shared among all the dimensions.
To balance the privacy-accuracy trade-off, we need to reduce di-
mensionality to decrease the number of dimensions that have to
be perturbed. Still, since we cannot have the feature vectors of all
the nodes at one place (due to privacy reasons), we cannot use con-
ventional approaches, such as principal component analysis (PCA)
or any other machine learning-based feature selection method. In-
stead, we randomly perturb a subset of the dimensions and then
optimize the size of this subset to achieve the lowest variance in
estimating the Aggregate function.
Algorithm 1 describes this encoding process in greater detail.
Intuitively, the encoder first uniformly samples 𝑚 out of 𝑑 dimen-
sions without replacement, where 𝑚 is a parameter controlling how
many dimensions are perturbed. Then, for each sampled dimension,
the corresponding feature is randomly mapped to either -1 or 1,
with a probability depending on the per-dimension privacy budget
𝜖/𝑚 and the position of the feature value in the feature space, such
that values closer to 𝛼 (resp. 𝛽) are likely to be mapped to -1 (resp. 1).
For other dimensions that are not sampled, the algorithm outputs
0. Therefore, a maximum of two bits per feature is enough to send
x∗
𝑣 to the server. When 𝑚 = 𝑑, our algorithm reduces to the 1-bit
mechanism with a privacy budget of 𝜖/𝑑 for every single dimension.
The following theorem ensures that the multi-bit encoder is 𝜖-LDP
(proof in the Appendix).
Theorem 3.1. The multi-bit encoder presented in Algorithm 1
satisfies 𝜖-local differential privacy for each node.
Multi-bit Rectifier. The output of the multi-bit encoder is sta-
tistically biased, i.e., E [x∗] ≠ x. Therefore, the goal of the multi-bit
rectifier, executed at server-side, is to convert the encoded vector x∗
to an unbiased perturbed vector x′, such that E [x′] = x, as follows:
x′ = 𝑅𝑒𝑐𝑡(x∗) =
𝑑(𝛽 − 𝛼)
2𝑚
· 𝑒𝜖/𝑚 + 1
𝑒𝜖/𝑚 − 1 · x∗ + 𝛼 + 𝛽
2
(4)
Note that this is not a denoising process to remove the noise from x∗,
but the output vector x′ is still noisy and does not have any mean-
ingful information about the private vector x. The only difference
between x∗ and x′ is that the latter is unbiased, while the former is
not. The following results entail from the multi-bit rectifier:
Proposition 3.2. The multi-bit rectifier defined by (4) is unbiased.
Proposition 3.3. For any node 𝑣 and any 𝑖 ∈ {1, 2, . . . , 𝑑}, the
variance of the multi-bit rectifier defined by (4) at dimension 𝑖 is:
(cid:32) 𝛽 − 𝛼
2
· 𝑒𝜖/𝑚 + 1
𝑒𝜖/𝑚 − 1
(cid:33)2
(cid:18)
𝑥𝑣,𝑖 − 𝛼 + 𝛽
2
(cid:19)2
−
(5)
𝑉 𝑎𝑟[𝑥′
𝑣,𝑖] =
·
𝑑
𝑚
The variance of an LDP mechanism is a key factor affecting the
estimation accuracy: a lower variance usually leads to a more accu-
rate estimation. Therefore, we exploit the result of Proposition 3.3
to find the optimal sampling parameter 𝑚 in the multi-bit encoder
(Algorithm 1) that minimizes the rectifier’s variance, as follows:
Proposition 3.4. The optimal value of the sampling parameter
𝑚 in Algorithm 1, denoted by 𝑚★, is obtained as:
𝑚★ = max(1, min(𝑑,(cid:106) 𝜖
(cid:107)))
2.18
(6)
The above proposition implies that in the high-privacy regime
𝜖 ≤ 2.18, the multi-bit mechanism perturbs only one random di-
mension. Therefore, this process is similar to a randomized one-hot
encoding, except that here, the aggregation of these one-hot en-
coded features approximates the aggregation of the raw features.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2133Figure 2: Overview of our locally private GNN training framework, featuring the multi-bit mechanism (MB Encoder and MB
Rectifier), randomized response (RR), KProp layers, and Drop training. Users run multi-bit encoder and randomized response
on their private features and labels, respectively, and send the output to the server, after which training begins. Green solid
arrows and red dashed arrows indicate the training and validation paths, respectively.
3.2 Approximation of graph convolution
Upon collecting the encoded vectors x∗
𝑣 from every node 𝑣 and
generating the corresponding perturbed vectors x′
𝑣 using the multi-
bit rectifier, the server can initiate the training of the GNN. In the
first layer, the embedding for an arbitrary node 𝑣 is generated by the
following (layer indicator subscripts and superscripts are omitted
for simplicity):(cid:98)hN(𝑣) = Aggregate(cid:0){x′
𝑢,∀𝑢 ∈ N(𝑣)}(cid:1)
h𝑣 = Update(cid:16)(cid:98)hN(𝑣)
(cid:17)
where(cid:98)hN(𝑣) is the estimation of the first layer Aggregate function
of any node 𝑣 by aggregating perturbed vectors x′
𝑢 of all the nodes
𝑢 adjacent to 𝑣. After this step, the server can proceed with the rest
of the layers to complete the forward propagation of the model,
exactly similar to a standard GNN. If the Aggregate function is
linear on its input (e.g., it is a weighted summation of the input
vectors), the resulting aggregation would also be unbiased, as stated
below:
(7)
(8)
Corollary 3.5. Given a linear aggregator function, the aggrega-
tion defined by (7) is an unbiased estimation for (1) at layer 𝑙 = 1.
The following proposition also shows the relationship of the
estimation error in calculating the Aggregate function and the
neighborhood size |N(𝑣)| for the special case of using the mean
aggregator function:
Proposition 3.6. Given the mean aggregator function for the first
layer and 𝛿 > 0, with probability at least 1 − 𝛿, for any node 𝑣, we
have:
(cid:12)(cid:12)(cid:12)((cid:98)hN(𝑣))𝑖 − (hN(𝑣))𝑖
(cid:12)(cid:12)(cid:12) = O
(cid:33)
(cid:32)√︁𝑑 log(𝑑/𝛿)
𝜖√︁|N(𝑣)|
max
𝑖∈{1,...,𝑑}
(9)
The above proposition indicates that with the mean aggregator
function (which can be extended to other Aggregate functions as
well), the estimation error decreases with a rate proportional to the
square root of the node’s degree. Therefore, the higher number of
neighbors, the lower the estimation error. But as mentioned earlier,
the size of N(𝑣) is usually small in real graphs, which hinders the
Aggregate function from driving out the injected noise on its own.
In a different context, prior works have shown that consider-
ing higher-order neighbors can help learn better node representa-
tions [2, 28, 36]. Inspired by these works, a potential solution to this
issue is to expand the neighborhood of each node 𝑣 by considering
more nodes that are not necessarily adjacent to 𝑣 but reside within
an adjustable local neighborhood around 𝑣. To this end, we use
an efficient convolution layer, described in Algorithm 2, that can
effectively be used to address the small-size neighborhood issue.
The idea is simple: we aggregate features of those nodes that are up
to 𝐾 steps away from 𝑣 by simply invoking the Aggregate func-
tion 𝐾 consecutive times, without any non-linear transformation
in between. For simplicity, we call this algorithm KProp, as every
node propagates its message to 𝐾 hops further.
As illustrated in Figure 2, we prepend KProp as a denoising layer
to the GNN. This approach has two advantages: first, it allows
to use any GNN architecture with any Aggregate function for
the backbone model, as KProp already uses a linear Aggregate
that satisfies Corollary 3.5; and second, it enables us to expand
the effective aggregation set size for every node by controlling the
step parameter 𝐾. However, it is essential to note that we cannot
arbitrarily increase the neighborhood size around a node, since
aggregating messages from too distant nodes could lead to over-
smoothing of output vectors [30]. Therefore, there is a trade-off
between the KProp’s denoising accuracy and the overall GNN’s
expressive power.
It is worth mentioning that in KProp, we perform aggregations
over N(𝑣)−{𝑣}, i.e., we do not include self-loops. While it has been
shown that adding self-loops can improve accuracy in conventional
GNNs [26], excluding self-connections works better when dealing
KPropCrossEntropyAccuracyKPropCrossEntropyRRGNNMBEncoderKPropRRUser-SideServer-SideLPGNNDROPMBRectifierSession 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2134:Graph G = (V, E); input vector x𝑣, ∀𝑣 ∈ V; linear
aggregator function Aggregate; step parameter 𝐾 ≥ 0;
Algorithm 2: KProp Layer
Input
Output:Embedding vector h𝑣, ∀𝑣 ∈ V
1 for all 𝑣 ∈ V do in parallel
2
h0
N(𝑣) = x𝑣
for 𝑘 = 1 to 𝐾 do
h𝑘N(𝑣) = Aggregate(cid:16){h𝑘−1
3
4
N(𝑢) , ∀𝑢 ∈ N(𝑣) − {𝑣}}(cid:17)
end
h𝑣 = h𝐾N(𝑣)
5
6
7 end
8 return {h𝑣, ∀𝑣 ∈ V}
with noisy features. As 𝐾 grows, with self-loops, we account for the
injected noise in the feature vector of each node in the 𝑣’s neigh-
borhood multiple times in the aggregation. Therefore, removing
self-loops helps to reduce the total noise by discarding repetitive
node features from the aggregation.
3.3 Learning with private labels
In this last part, we describe the method used to perturb and collect
labels privately and introduce our training algorithm for learning
locally private GNNs using perturbed labels, called label denoising
with propagation (Drop). Let 𝑓 (x) = arg maxy ˆ𝑝(y | x) be the target
node classifier, where ˆ𝑝(y | x) = 𝑔(x, G; W) approximates the class-
conditional probabilities 𝑝(y | x) and is modeled by a GNN 𝑔(.)
with the learnable weight matrix W. The goal is to optimize W
such that ˆ𝑝(y | x) becomes as close as possible to 𝑝(y | x). In
the standard setting, this is usually done by minimizing the cross-
entropy loss function between ˆ𝑝(y | x) and true label y over the
set of labeled nodes VL:
ℓ (y, ˆ𝑝(y | x)) = − ∑︁
𝑣∈VL
𝑣 log ˆ𝑝(y | xv)
y𝑇
(10)
However, since the labels are considered private, each node
𝑣 ∈ VL that participates in the training procedure has to perturb
their label y𝑣 using some LDP mechanism, and send the perturbed
label y′
𝑣 to the server. Still, if we train the GNN using the perturbed
labels by minimizing the cross-entropy loss between ˆ𝑝(y | x) and
perturbed labels y′, namely ℓ (y′, ˆ𝑝(y | x)), the model completely
overfits the noisy labels and generalizes poorly to unseen nodes.
However, many real-world graphs, such as social networks, are
homophilic [34], meaning that nodes with structural similarity
tend to have similar labels [48]. We exploit this fact to estimate
the frequency of the labels in a local neighborhood around any
node 𝑣 to obtain its estimated label ˜y𝑣. To this end, we can use any
LDP frequency oracle, such as randomized response [22], Unary
Encoding [52], or Local Hashing [52]. In this paper, we use ran-
domized response for two reasons: first, the number of classes is
usually small, and randomized response has been shown to work
better than other mechanisms in low dimensions [52]; and second,
it introduces a symmetric, class-independent noise to the labels by
flipping them according to the following distribution, which we
(cid:40) 𝑒𝜖
𝑒𝜖+𝑐−1 ,
𝑒𝜖+𝑐−1 ,
1
later exploit in our learning algorithm:
𝑝(y′ | y) =
if y′ = y
otherwise
(11)
y′
𝑢
(12)
𝑢∈N(𝑣)
where y and y′ are clean and perturbed labels, respectively, 𝑐 is the
number of classes, and 𝜖 is the privacy budget.
Similar to estimating the graph convolution with noisy features,
we also face the problem of small-size neighborhood if we only rely
on the first-order neighbors to estimate the label frequency. In order
to expand the neighborhood around each node, we take the same
approach as we did for features: we apply KProp on node labels,
i.e., we set ˜𝑦𝑣 = arg max𝑖∈[𝑐] ℎ𝑖(y′
𝑣, 𝐾𝑦) for all 𝑣 ∈ VL, where
ℎ(.) is the KProp function, 𝐾𝑦 is the step parameter, and [𝑐] =
{1, . . . , 𝑐}. With the mean aggregator function, at every iteration,
KProp updates every node’s label distribution by averaging its
neighbors’ label distribution. In this paper, however, we instead use
the GCN aggregator function [26]:
Aggregate(cid:0){y′
normalization factor in the GCN aggregator is√︁|N(𝑢)| · |N(𝑣)|,
while for the mean, it is |N(𝑣)| =√︁|N(𝑣)| · |N(𝑣)|. In other words,
Using the GCN aggregator leads to a lower estimation error than
the mean aggregator due to the difference in their normalization
factors, which affects their estimation variance. Specifically, the
𝑢,∀𝑢 ∈ N(𝑣)}(cid:1) =
√︁|N(𝑢)| · |N(𝑣)|
∑︁
the GCN aggregator considers the square root of the degree of both
the central node 𝑣 and its neighbor 𝑢, whereas the mean aggregator
considers only the square root of the central node 𝑣’s degree twice.
Since there are many more low-degree nodes in many real graphs
than high-degree ones, using the mean aggregator results in a small
normalization factor for most nodes, leading to a higher estimation
variance. But as many of the low-degree nodes are linked to the
high-degree ones, the GCN aggregator balances the normalization
by considering the degree of both link endpoints. Consequently,
the normalization for many low-degree nodes increases compared
to the mean aggregator, yielding a lower estimation variance.
As the step parameter 𝐾𝑦 gradually increases, the estimated label
˜y becomes more similar to the clean label y. Therefore, an initial idea
for the training algorithm would be to learn the GNN using ˜y instead
of y′ by minimizing the cross-entropy loss between ˆ𝑝(y | x) and ˜y,
namely ℓ ( ˜y, ˆ𝑝(y | x)). However, this approach has two downsides.
First, it causes the GNN to become a predictor for ˜y and not y.
Although ˜y tend to converge to y as 𝐾𝑦 increases, the output of
KProp also becomes increasingly smoother, until the excessive
KProp aggregations lead to over-smoothing, after which ˜y will
begin to diverge from y and become noisy again, while we are still
fitting ˜y. Second, we cannot know how far we should increase 𝐾𝑦
to get the best accuracy and prevent over-smoothing without clean
validation data. One way to validate the model with noisy labels is
to calculate the accuracy of the target classifier 𝑓 (x) for predicting
the estimated label ˜y. However, suppose the model overfits the
over-smoothed labels. In that case, the corresponding validation
˜y’s also becomes over-smoothed and can be well predicted by the
model, resulting in a high validation but low test accuracy.
To address the first issue, instead of minimizing ℓ ( ˜y, ˆ𝑝(y | x)),
we propose to minimize ℓ ( ˜y, ˆ𝑝( ˜y | x)), i.e., the cross-entropy loss
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2135between the estimated label ˜y and its approximated probability
ˆ𝑝( ˜y | x), which can be obtained by applying the same procedure
on ˆ𝑝(y | x) as we did on y to obtain ˜y. In the first place, we applied
randomized response on y to obtain y′, and then passed the result
to the KProp layer to get ˜y. If we go through the same steps to
obtain ˆ𝑝( ˜y | x) and then minimize its cross-entropy loss with ˜y, we
can keep ˆ𝑝(y | x) intact when KProp causes over-smoothing, and
at the same time benefit from it’s denoising capability. To this end,
we first need to calculate ˆ𝑝(y′ | x) from ˆ𝑝(y | x):
𝑝(y′ | y) · ˆ𝑝(y | x)
ˆ𝑝(y′ | x) =
∑︁
(13)
y
where 𝑝(y′ | y) is directly obtained from (11). This step would be
analogous to applying randomized response to y and getting y′.
Finally, similar to applying KProp on y′ to get ˜y, we treat ˆ𝑝(y′ | x)
as soft labels and apply KProp with the same step parameter to
approximate ˆ𝑝( ˜y | x):
ˆ𝑝( ˜y | x) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥(cid:0)ℎ(cid:0) ˆ𝑝(y′ | x), 𝐾𝑦(cid:1)(cid:1)
(14)
where the softmax is used to normalize the KProp’s output as a valid
probability distribution. Finally, we train the model by minimizing
ℓ ( ˜y, ˆ𝑝( ˜y | x)).
To address the validation issue, we must make sure that our
validation procedure is not affected by the KProp step parameter
𝐾𝑦. Clearly, if we use ˜y for validation, by changing 𝐾𝑦 we are also
modifying estimated labels ˜y, and thus we are basically validating
different models with different labels. Therefore, we should only val-
idate the model using the noisy labels y′. Here, we choose the cross-
entropy loss between y′ and ˆ𝑝(y′ | x), namely ℓ (y′, ˆ𝑝(y′ | x)),
as Patrini et al. [39] show that this loss function, which they call
forward correction loss, is unbiased, meaning that under expected
label noise, ℓ (y′, ˆ𝑝(y′ | x)) is equal to ℓ (y, ˆ𝑝(y | x)), i.e., the orig-
inal loss computed on clean data. Therefore, we train the GNN
with different hyper-parameters, including 𝐾𝑦, and pick the one
achieving the lowest forward correction loss.
While this is in principle a reasonable idea, the forward correc-
tion loss on its own is not enough to prevent overfitting. That’s
because when 𝐾𝑦 is small, the estimated label ˜y is more similar to
the noisy one y′ than the clean label y, and thus the model overfits