produces tuples randomly distributed across two item_ids. In 
all experiments the rate at which the tuple-generator injects 
tuples  to  the  network  is  limited  by  its  CPU  or  by  network 
flow control. 
Figure  5  illustrates  the  performance  impact  of  CEC  on 
streaming  throughput  under  the  two  workloads  described 
above.  The  results  are  compared  against  those  of  native 
Borealis in the same setups. Other configurations depicted in 
Figure 5 include: Gopen/Net, which is the CEC setup where 
the aggregate operator forwards Gopen tuples to the network 
but does not persist them at its output log; and Gopen/Disk, 
which  includes  the  additional  overhead  of  persisting  Gopen 
tuples. Gopen/Net and Gopen/Disk are measured at the sink 
after dropping control tuples. The Real bar represents the real 
throughput that our receiver observes including both normal 
(R) and control (Gopen) tuples. 
A key observation is that order-of-magnitude differences 
in  output  rate across  workloads are  mainly  due  to different 
operator specifications (higher window counts result in lower 
output  rates).  Performance  of  the  Gopen/Net  configuration 
drops by less than 10% versus native performance across all 
workloads.  This  decrease  can  be  attributed  to  the  ratio  of 
Gopen vs. R tuples injected to the output stream: wider-spaced 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
151Figure 6. Effect of Q. 
Figure 7. Effect of U. 
fact  that  500,000  is  the  maximum  extent  size  reachable  in 
this operator configuration. 
Figure 6 (lower left) depicts the increase in EC-load time 
with  growing  Q  due  to  the  operator  having  to  read  and 
process a progressively larger extent to reach its pre-failure 
state.  We  observe  that  for  a  smaller  extent  (120,000  and 
180,000 tuples) the time to read and load it into the operator 
varies  from  3.5  to  5  seconds.  A  larger  extent  leads  to  EC-
load  times  of  between  14  and  27  seconds.  Figure  6  (lower 
right)  depicts  the  number  of  bytes  to  replay  from  the 
upstream  node  after  the  operator  has  loaded  the  EC.  Our 
results here exhibit the same trends as in EC-load. Lower Q 
values  correspond  to  replaying  25MB  to  50MB  of  tuples, 
whereas higher Q values correspond to replaying more than 
200MB  of  tuples.  The  CPU  performing  the  EC-load 
operation  was  always  100%  busy.  CPU  overhead  due  to 
CEC during failure-free operation is minimal. 
In  the  U  experiment  we  vary  the  values  of  U  between 
125,000 and 106 tuples. Figure 7 illustrates the impact of U 
on  the  same  four  metrics.  Figure  7  (upper  left)  shows  that 
relaxing  the  U  target  leads  to  fewer  Gcheck tuples produced, 
consistent with our expectations. The drop is not as sharp as 
in the Q experiment, evidence that our large Q values lead to 
more  relaxed  policies  compared  to  large  U  values  in  this 
experiment. Figure 7 (bottom right) shows that higher values 
of  U  result  to  between  20MB  (200,000  tuples)  and  90MB 
(900,000 tuples) of upstream input replayed during recovery. 
Figure 7 (upper right) shows that the extent size increases 
from 120,000 tuples to 230,000 tuples. This increase can be 
explained  by  the  lower  production  of  Gcheck  tuples  with 
growing U. Consistent with having to handle longer extents, 
EC-load time (bottom left) increases from 2.5s to 6.7s. The 
EC-load  times  measured  for  different  extent  sizes  are  in 
agreement with our results in the Q experiment. 
C.  Impact of checkpointing period 
Next we evaluate the impact of different values of CI/CP 
to  operator  response  and  recovery  time.  In  this  experiment 
we vary CP between 25ms and 250ms while maintaining CI 
fixed at 10ms. All runs use the aggregate operator configured 
as described in the previous section. Q is fixed and equals the 
number of open windows (whose average was measured to 
be about 90,000), a very aggressive target that ensures there 
always exist windows that are candidates for checkpointing. 
Figure  8  depicts  the  response  time  of  the  aggregate 
operator measured as the difference between the opening and 
closure time of a window, reported as per-second averages. 
Operator response time ranges from 16.5s (without CEC), to 
18.5ms,  21.5ms,  and  32.5s  (CEC  with  CP  = 100ms,  50ms, 
25ms,  respectively)  reflecting  the  fraction  of  operator  time 
spent  on  checkpointing.  For  CP  =  250ms  (not  shown  in 
Figure 8) CEC response time approximates that of native (no 
CEC). To highlight the tradeoff between response time and 
recovery  time,  in  Figure  9  we  report  the  extent  size  as  a 
function  of  CP.  Longer  CP  values  result  in  longer  extent 
sizes and consequently longer recovery times. Based on our 
results from the Q, U experiments we estimate that varying 
CP from 25ms to 250ms increases EC-load time from 2s to 
5s. Given the higher impact of other factors such as failure 
detection, RPC communication, etc. in overall recovery time 
it  is  preferable  in  this  particular  setup  to  choose  the  most 
relaxed  checkpoint  period  (e.g.,  CP  =  250ms)  achieving 
response time close to that of performing no checkpointing at 
all. 
V.  DISCUSSION 
Although  we  have  demonstrated  CEC  primarily  for  the 
case of an aggregate operator,  we believe  that  CEC  can be 
also applied to other stateful operators such as join. Join [3] 
has  two  input  streams  and  for  every  pair  of  input  tuples 
applies  a  predicate  over  the  tuple  attributes.  When  the 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
152Figure 8. Operator response time for different checkpoint 
periods. 
predicate  is  satisfied,  join  concatenates  the  two  tuples  and 
forwards the resulting tuple on its output stream. The stream-
based join operator matches only tuples than fall within the 
same window. For example assume two input streams, R and 
S,  both  with  a  time  attribute,  and  a  window  size,  w.  Join 
matches tuples that satisfy t.time -s.time â‰¤ w, although other 
window  specifications  are  possible.  Tuples  produced  by 
operators in most cases maintain timestamp ordering or else 
ordering can be applied using sort operators during insertion. 
If  output  queues  are  persistent,  then  the  timestamp  of  the 
result tuple can be changed to resemble the timestamp of the 
concatenated tuples. This way, in case of failure, the operator 
knows  from  which  point  in  time  to  ask  replay  from  both 
input  streams.  In  addition,  in  join  operator  we  have  to 
remember the relevant position inside the input streams from 
which  concatenation  takes  place.  CEC  will  periodically 
produce  Gcheck tuples  indicating  the  relevant position of  the 
two windows at any point in time. 
VI.  RELATED WORK 
General 
typically 
in  parallel  and 
fault-tolerance  methods 
rely  on 
replication  to  protect  against  failures.  The  main  replication 
mechanisms  for  high-availability  in  distributed  systems 
include state machine [19], process-pairs [11], and rollback 
recovery [10] methodologies. In the state-machine approach, 
the state of a processing node is replicated on k independent 
nodes.  All  replicas  process  data 
the 
coordination between them is achieved by sending the same 
input  to  all  replicas  in  the  same  order.  The  process-pairs 
model is a related approach in which replicas are coordinated 
using  a  primary/secondary  relationship.  In  this  approach  a 
primary  node  acts  as  leader  forwarding all of  its  input to a 
secondary, maintaining order and operating in lock-step with 
the  primary  node.  In  rollback  recovery,  nodes  periodically 
send snapshots (typically called checkpoints) of their state to 
other nodes or to stable storage devices. Upon recovery, the 
state  is  reconstructed  from  the  most  recent  checkpoint  and 
upstream nodes replay logged input tuples to reach the pre-
failure state. All of the above methodologies have in the past 
been  adapted to  operate in  the  context of  continuous-query 
distributed stream processing systems. 
Two examples of the state machine approach adapted for 
stream processing are active-replicas [6] and Flux [21]. Both 
Figure 9. Extent size for different checkpoint periods. 
systems replicate the producer and the consumer operators in 
a  stream  dataflow  graph  in  a  symmetric  fashion.  Each 
consumer  replica  receives  tuples  from  one  of  the  producer 
replicas  and,  in  case  of  producer  failure,  the  consumer 
switches  to  another  functioning  producer  replica.  Strict 
coordination  is  not  required  since  consistency  is  eventually 
maintained  by  the  replicas  simultaneously  processing  the 
same  input  and  forwarding  the  same  output.  All  operators 
preserve  their  output  queues,  truncating  them  based  on 
acknowledgements  periodically  sent  by  consumers.  In  case 
of failure, all upstream replica nodes are up-to-date and can 
start serving their downstream nodes as soon as the failure is 
detected, minimizing recovery time. 
The work of Hwang et al. [14] extends the active-replicas 
approach so that all upstream replicas send their output to all 
downstream  replicas  and  the  latter  being  allowed  to  use 
whichever  data  arrives  first.  Since  the  downstream  nodes 
receive data from many upstreams, the input stream received 
might be unordered or/and contain duplicate tuples. Despite 
the above complications their system manages to deliver the 
same result as it would produce without failures. To achieve 
this,  operators  are  enhanced  with  extra  non-blocking  filters 
(one filter per input stream) that eliminate duplicates based 
on periodically exchanged timestamp messages t. All tuples 
with  timestamp  lower  than  t  are  considered  duplicates  and 
dropped. 
Another  fault-tolerance  methodology  that  combines  the 
active-replicas and process-pair approaches is active standby 
[13].  In  active-standby,  secondary  nodes  work  in  parallel 
with  the  primary  nodes  and  receive  tuples  directly  from 
upstream operators. In contrast to active-replicas, in active-
standby  secondary  nodes  log  result  tuples  in  their  output 
queues but do not forward tuples to secondary downstream 
neighbors.  Challenges  with  this  approach  include  output 
preservation due to non-deterministic nature of operators and 
bounding the log of each secondary. 
Instances  of  the  rollback  recovery  (also  known  as 
checkpoint-rollback  or  CRB)  methodology  [10]  are  the  so-
called  passive-replicas  approaches,  comprising  passive-
standby  and  upstream-backup  [6][13].  In  passive-standby, 
the primary replica periodically produces checkpoints of its 
state and copies it to the backup replica. The state includes 
the data located inside the operators, along with the input and 
output  queues.  The  secondary  node  acknowledges  the  state 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
153already  received  with  the  primary  upstream  so  as  to  drop 
tuples  from  the latter's output queue.  In  case of  failure,  the 
backup  node  takes  over  by  loading  the  most  recent 
checkpoint to its current state. A variant of passive-standby 
that  allows  independent  checkpointing  of  fragments  (sub-
graphs)  of  the  entire  query  graph  has  been  shown  [15]  to 
reduce  the  latency  introduced  by  checkpointing.  However 
checkpoint  granularity  with  this  methodology  is  still  at  the 
level of entire operators and stream processing freezes while 
storing a fragment checkpoint to a remote server memory. 
The  upstream-backup  [13]  model  was  proposed  for 
operators whose internal state depends on a small amount of 
input.  In  this  approach,  the  upstream  nodes  act  as  backups 
for the downstream nodes by logging tuples in their output 
queues until all downstream nodes completely process their 
tuples.  The  upstream  log  is  trimmed  periodically  using 
acknowledgments sent by the downstream primaries. In case 
of  failure,  the  upstream  primaries  replay  their  logs  and  the 
secondary nodes rebuild the missing state before starting to 
serve  other  downstream  nodes.  In  contrast  to  passive-
standby,  upstream-backup  requires  a  longer  recovery  but 
comes with lower runtime overhead. 
In all aforementioned methodologies replica nodes retain 
output  tuples  and  checkpoints  in  memory  buffers  reducing 
the amount memory available for input tuple processing. One 
solution  to  this  problem  is  to  utilize  persistent  storage 
[15][20]. SGuard [17] is a system that leverages the use of a 
distributed and replicated file system (HDFS [7]) to achieve 
stream fault-tolerance in Borealis [8]. Operators periodically 
produce  delta-checkpoints  of  their  current  state  and  the 
recovery  is  made  using  the  latest  checkpoint  of  the  failed 
node. In this approach, HDFS act as the backup location for 
the  checkpointed 
the  memory 
requirements  of  the  stream  processing  nodes.  To  eliminate 
the  overhead  of  freezing  the  operators  during  checkpoint, 
SGuard performs  checkpoints  asynchronously  and  manages 
resource  contention  of  the  distributed  file  system  with  the 
enhancement  of  a  scheduler  that  batches  together  several 
write  requests.  SGuard  is  related  to  CEC  in  its  focus  on 
producing operator checkpoints and persisting them on stable 
storage.  SGuard  however  considers the entire  operator  as  a 
checkpoint unit whereas our approach breaks operator state 
into parts, treating each window as an independently entity. 
Another system that takes advantage of operator semantics to 
optimize checkpointing performance is SPADE [16]. 
Zhou  et  al.  [22]  use  log-based  recovery  and  fuzzy 
checkpointing  to  offer  programming  support  for  high-
throughput  data  services.  Their  fuzzy  checkpoints  of 
independent  memory  objects  are  similar  to  our  eventual 
checkpoints of independent operator-window states and their 
logs  are  similar  to  our  upstream  queues,  which  can  replay 
input  tuples  during  recovery.  In  addition,  they  propose  an 
adaptive  control  approach 
regulating  checkpoint 
frequency based on a number of target parameters. Our work 