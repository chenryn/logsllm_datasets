title:Last-Level Cache Side-Channel Attacks are Practical
author:Fangfei Liu and
Yuval Yarom and
Qian Ge and
Gernot Heiser and
Ruby B. Lee
2015 IEEE Symposium on Security and Privacy
Last-Level Cache Side-Channel Attacks are
Practical
Fangfei Liu∗†, Yuval Yarom∗‡§, Qian Ge§¶, Gernot Heiser§¶, Ruby B. Lee†
∗ Equal contribution joint ﬁrst authors.
† Department of Electrical Engineering, Princeton University
Email: {fangfeil,rblee}@princeton.edu
‡ School of Computer Science, The University of Adelaide
Email: PI:EMAIL
§ NICTA
Email: {qian.ge,gernot}@nicta.com.au
¶ UNSW Australia
Abstract—We present an effective implementation of
the PRIME+PROBE side-channel attack against the last-
level cache. We measure the capacity of the covert channel
the attack creates and demonstrate a cross-core, cross-VM
attack on multiple versions of GnuPG. Our technique
achieves a high attack resolution without relying on
weaknesses in the OS or virtual machine monitor or on
sharing memory between attacker and victim.
Keywords—Side-channel attack; cross-VM side channel;
covert channel; last-level cache; ElGamal;
I.
INTRODUCTION
Infrastructure-as-a-service (IaaS) cloud-computing
services provide virtualized system resources to end
users, supporting each tenant in a separate virtual ma-
chine (VM). Fundamental to the economy of clouds is
high resource utilization achieved by sharing: providers
co-host multiple VMs on a single hardware plat-
form, relying on the underlying virtual-machine monitor
(VMM) to isolate VMs and schedule system resources.
While virtualization creates the illusion of strict
isolation and exclusive resource access, in reality the
virtual resources map to shared physical resources,
creating the potential of interference between co-hosted
VMs. A malicious VM may learn information on data
processed by a victim VM [32, 42, 43] and even conduct
side-channel attacks on cryptographic implementations
[45, 47].
Previously demonstrated side channels with a reso-
lution sufﬁcient for cryptanalysis attacked the L1 cache.
However, as Figure 1 shows, the L1 Data and Instruction
caches (denoted L1 D$ and L1 I$) are private to each
processor core. This limits the practicability of such
attacks, as VMMs are not very likely to co-locate
multiple owners’ VMs on the same core. In contrast,
the last-level cache (LLC) is typically shared between
VM
VM
yyy
VMM
yyy
Attacker
L1
D$
Core
L1
I$
L2$
Victim
L1
D$
Core
L1 
I$
L2$
SW
HW
Shared, unified last-level cache (LLC or L3$)
Main memory
Fig. 1: System model for a multi-core processor
all cores of a package, and thus constitutes a much more
realistic attack vector.
However, the LLC is orders of magnitude larger
and much slower to access than the L1 caches, which
drastically reduces the temporal resolution of observ-
able events and thus channel bandwidth, making most
published LLC attacks unsuitable for cryptanalysis [32,
42, 43]. An exception is the FLUSH+RELOAD attack
[22, 45], which relies on memory sharing to achieve
high resolution. Virtualization vendors explicitly advise
against sharing memory between VMs [39], and no IaaS
provider is known to ignore this advice [36], so this
attack also fails in practice.
We show that an adaptation of the PRIME+PROBE
technique [28] can be used for practical LLC attacks.
We exploit hardware features that are outside the control
of the cloud provider (inclusive caches) or are control-
lable but generally enabled in the VMM for perfor-
mance reasons (large page mappings). Beyond that, we
make no assumptions on the hosting environment, other
than that the attacker and victim will be co-hosted on
© 2015, Fangfei Liu. Under license to IEEE.
DOI 10.1109/SP.2015.43
605
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
the same processor package.
Speciﬁcally, we make the following contributions:
• We demonstrate an asynchronous PRIME+
PROBE attack on the LLC that does not require
sharing cores or memory between attacker and
victim, does not exploit VMM weaknesses and
works on typical server platforms, even with the
unknown LLC hashing schemes in recent Intel
processors (Section IV);
• We develop two key techniques to enable ef-
ﬁcient LLC based PRIME+PROBE attacks: an
algorithm for the attacker to probe exactly one
cache set without knowing the virtual-address
mapping, and using temporal access patterns
instead of conventional spatial access patterns
to identify the victim’s security-critical accesses
(Section IV);
• We measure the achievable bandwidth of the
cross-VM covert timing channel to be as high
as 1.2 Mb/s (Section V);
• We show a cross-VM side-channel attack that
extracts a key from secret-dependent execu-
tion paths, and demonstrate it on Square-and-
Multiply modular exponentiation in an ElGamal
decryption implementation (Section VI);
• We furthermore show that the attack can also be
used on secret-dependent data access patterns,
and demonstrate it on the sliding-window mod-
ular exponentiation implementation of ElGamal
in the latest GnuPG version (Section VII).
II. BACKGROUND
A. Virtual address space and large pages
A process executes in its private virtual address
space, composed of pages, each representing a con-
tiguous range of addresses. The typical page size is
4 KiB, although processors also support larger pages,
2 MiB and 1 GiB on the ubiquitous 64-bit x86 (“x64”)
processor architecture. Each page is mapped to an
arbitrary frame in physical memory.
In virtualized environments there are two levels of
address-space virtualization. The ﬁrst maps the virtual
addresses of a process to a guest’s notion of physical
addresses, i.e., the VM’s emulated physical memory.
The second maps guest physical addresses to physical
addresses of the processor. For our purposes, the guest
physical addresses are irrelevant, and we use virtual
address for the (guest virtual) addresses used by appli-
cation processes, and physical address to refer to actual
(host) physical addresses.
Translations from virtual pages to physical frames
are stored in page tables. Processors cache recently
used page table entries in the translation look-aside
buffer (TLB). The TLB is a scarce processor resource
with a small number of entries. Large pages use the
TLB more efﬁciently, since fewer entries are needed
to map a particular region of memory. As a result, the
performance of applications with large memory foot-
prints, such as Oracle databases or high-performance
computing applications, can beneﬁt from using large
pages. For the same reason, VMMs, such as VMware
ESX and Xen HVM, also use large pages for mapping
guest physical memory [38].
B. System model and cache architecture
Cloud servers typically have multi-core processors,
i.e., multiple processor cores on a chip sharing a last-
level cache (LLC) and memory, as indicated in Figure 1.
1) Cache hierarchy: Because of the long access
time of main memory compared to fast processors,
smaller but faster memories, called caches, are used
to reduce the effective memory access time as seen
by a processor. Modern processors feature a hierarchy
of caches. “Higher-level” caches, which are closer to
the processor core are smaller but faster than lower-
level caches, which are closer to main memory. Each
core typically has two private top-level caches, one each
for data and instructions, called level-1 (L1) caches. A
typical L1 cache size is 32 KiB with a 4-cycle access
time, as in Intel Core and Xeon families.
The LLC is shared among all the cores of a multi-
core chip and is a uniﬁed cache, i.e., it holds both data
and instructions. LLC sizes measure in megabytes, and
access latencies are of the order of 40 cycles. Modern
x86 processors typically also support core-private, uni-
ﬁed L2 caches of intermediate size and latency. Any
memory access ﬁrst accesses the L1 cache, and on a
miss, the request is sent down the hierarchy until it hits
in a cache or accesses main memory. The L1 is typically
indexed by virtual address, while all other caches are
indexed by physical address.
2) Cache access: To exploit spatial locality, caches
are organized in ﬁxed-size lines, which are the units
of allocation and transfer down the cache hierarchy. A
typical line size B is 64 bytes. The log2B lowest-order
bits of the address, called the line offset, are used to
locate a datum in the cache line.
Caches today are usually set-associative, i.e., orga-
nized as S sets of W lines each, called a W -way set-
associative cache. As shown in Figure 2a, when the
cache is accessed, the set index ﬁeld of the address,
log2S consecutive bits starting from bit log2B, is used
to locate a cache set. The remaining high-order bits are
2
606
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
32
20
16
11
5
0
4 KB page
2 MB page
Frame number
Page offset
Large frame number
Large Page offset
Tag
Set index
Line offset
(a) Traditional cache
32
20
16
11
5
0
4 KB page
2 MB page
Frame number
Page offset
Large frame number
Large Page offset
Tag


hash
Slice id
Line offset
Set index
(b) Sliced cache
Fig. 2: Cache indexing for an 8 GiB address space. A
cache (or slice) contains 2,048 sets with 64 B lines.
used as a tag for each cache line. After locating the
cache set, the tag ﬁeld of the address is matched against
the tag of the W lines in the set to identify if one of the
cache lines is a cache hit.
As memory is much larger than the cache, more
than W memory lines may map to the same cache set,
potentially resulting in cache contention. If an access
misses in the cache and all lines of the matching set
are in use, one cache line must be evicted to free a
cache slot for the new cache line being fetched from the
next level of cache or from main memory for the LLC.
The cache’s replacement policy determines the line to
evict. Typical replacement policies are approximations
to least-recently-used (LRU).
There is often a well-deﬁned relationship between
different levels of cache. The inclusiveness property of
a cache states that the Li+1 cache holds a strict superset
of the contents of the Li. The LLC in modern Intel
processors is inclusive [20].
Modern Intel processors, starting with the Sandy
Bridge microarchitecture, use a more complex architec-
ture for the LLC, to improve its performance. The LLC
is divided into per-core slices, which are connected by
a ring bus (see Figure 3). Slices can be accessed con-
currently and are effectively separate caches, although
the bus ensures that each core can access the full LLC
(with higher latency for remote slices).
To uniformly distribute memory trafﬁc to the slices,
Intel uses a carefully-designed but undocumented hash
function (see Figure 2b). It maps the address (excluding
the line offset bits) into the slice id. Within a slice, the
set is accessed as in a traditional cache, so a cache set
in the LLC is uniquely identiﬁed by the slice id and set
PCIe
display
System agent
Memory 
controller
Core 0
Core 1
Core 2
Core 3
LLC slice 0
LLC slice 1
LLC slice 2
LLC slice 3
Graphics and video
Fig. 3: Ring bus architecture and sliced LLC
index.
Hund et al. [19] found that on Sandy Bridge, only
the tag ﬁeld is used to compute the hash, but we ﬁnd
that this is only true if the number of cores is a power
of two. For other core counts, the full address (minus
line offset) is used.
III. CHALLENGES IN ATTACKING THE LLC
A. Attack model
We target information leakage in virtualized environ-
ments, such as IaaS clouds. We assume that the attacker
controls a VM that is co-resident with the victim VM
on the same multi-core processor, as shown in Figure 1.
The victim VM computes on some conﬁdential data,
such as cryptographic keys. We assume that the attacker
knows the crypto software that the victim is running.
We do not assume any vulnerability in the VMM,
or even a speciﬁc VMM platform. Nor do we assume
that attacker and victim share a core, that they share
memory, or that the attacker synchronizes its execution
with the victim.
B. PRIME+PROBE
Our LLC-based cross-core, cross-VM attack is based
on PRIME+PROBE [28], which is a general technique
for an attacker to learn which cache set is accessed
by the victim VM. The attacker, A, runs a spy process
which monitors cache usage of the victim, V , as follows:
PRIME: A ﬁlls one or more cache sets with its own code
or data.
IDLE: A waits for a pre-conﬁgured time interval while
V executes and utilizes the cache.
PROBE: A continues execution and measures the time to
load each set of his data or code that he primed. If V has
accessed some cache sets, it will have evicted some of
A’s lines, which A observes as increased memory access
latency for those lines.
As the PROBE phase accesses the cache, it doubles
as a PRIME phase for subsequent observations.
3
607
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
C. Overview of challenges for efﬁcient PRIME+PROBE
attacks on the LLC
Constructing an efﬁcient PRIME+PROBE attack on
the LLC is much harder than on the L1 caches. We
identify the following challenges, which we explain in
the following subsections:
1) Visibility into one core’s memory accesses from
another core via LLC;
2) Signiﬁcantly longer time to probe the large LLC;
3) Identifying cache sets corresponding to security-
critical accesses by the victim without probing the
whole LLC;
4) Constructing an eviction set that can occupy exactly
one cache set in the LLC, without knowing the address