in the preceding sections). One blade ran the time server, and
four were used for simulated clients. All experiments use the
Apache 2.2.8 server with mod_python 3.3.1 modules for
dynamic content generation. The Spork daemon is written in
Python 2.5.2 and uses a custom TPM integration library written
in C. The server and client browser extension exceeds 5000
Fig. 8. An overview of the Spork system architecture – The time server
provides an attested timestamp to the web server which is bound to the content
delivered to the browser and local software integrity information.
content. Figure 8 shows the structure of the Spork web envi-
ronment. In addition to external clients and the time service,
there are two functional elements processing the requests on
the web host; the web server and Spork daemon.
A. Proof-Generating Web Server
As directed by the requested URL, the Apache web server
supporting Spork directs all client requests (1 in Figure 8)
to Spork threads processing requests running in the httpd
address space. If the request is for a static page, the content
is retrieved from the local ﬁlesystem. A URL to a proof page
(which may not yet exist) is inserted into the X-Attest-URL
header of the retrieved page, and the result is returned to the
client (6). Dynamic requests occur in substantially the same
way except that the content is generated using the appropriate
content generation code, e.g., ASP [31],
instead of being
retrieved from the ﬁlesystem.
If the received request is for a proof, the Spork request
processing thread passes proof identity information to a Spork
master thread (one per Apache process) which passes the proof
request to the Spork daemon over standard UNIX IPC (2) (i.e.
sockets). The processing thread then sleeps waiting for a “proof
ready” event. When the requested proof (5) is received by the
master thread from the Spork daemon (see below), it wakes the
processing thread, which then returns the proof to the client
(6).
The Spork daemon generates the content proofs by interleav-
ing a number of utility threads. The main thread receives re-
quests from Apache, extracts and marshals the succinct proofs
from available proof systems, and returns the result to the main
Spork thread in Apache (5). The remaining threads update the
internal state from which the proof systems are constructed.
A TPM thread schedules and executes quote operations (4)
as governed by the algorithms deﬁned in Section III-C, and
a separate time thread similarly retrieves time attestations (3).
Separate threads maintain the dictionary of static documents
(by monitoring the ﬁlesystem) and the current set of dynamic
pages awaiting proof generation.
Client browsers receive the content proof from the web
server (6) and acquire time attestations from the time server
(7). If the proofs validate correctly, the page may be rendered.
Note that it is a matter of policy of what to do when a proof
validation fails; the browser may block rendering, warn the
100
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:10:03 UTC from IEEE Xplore.  Restrictions apply. 
Spork DaemonTPMTPMTime ServiceBrowserWeb ServerTime Server573482Apache16Fig. 9. Unaltered web server throughput – sustained RPS during a 70 second
experiment.
lines of code. All load tests were performed using the Apache
JMeter benchmarking tool.
A recent study of web pages indicated that the average web
page size is about 130KB total, with an average HTML source
size of 25KB and the average non-ﬂash object being just under
10KB [32]. More focused studies of popular websites indicate
[33]. The sizes of
somewhat larger total sizes (≈ 300KB)
the component objects (e.g., images) in popular websites is
essentially the same as reported in the broader study, with the
increases in the number of embedded objects accounting for
the larger total page size. Thus, we use 10KB and 25KB ﬁle
sizes in all experiments.
An analysis of the test environment showed that the max-
imum throughput of an unaltered Apache web server can be
reached with a relatively small number of clients (on the order
of 200-300) for static content. In dynamic experiments, client
requests are delayed a random period (up to two times the
the TPM quote period, 1900 msec) before requesting another
page. This ensures uniform arrival of requests at the server3,
but necessitates signiﬁcantly more clients to sustain maximal
throughput. After experimenting with a number of different
client community sizes, we found the highest throughout could
be achieved in static experiments with 500 clients and dynamic
experiments with 8,000 clients without incurring signiﬁcant
latencies. Thus we use 500 clients to drive all static tests and
8,000 for all dynamic tests.
A. Macrobenchmarks
Our ﬁrst set of experiments sought to identify the overheads
associated with the delivery of integrity proofs by comparing
operation of Spork with that of an unaltered web server. The
static content and dynamic content web servers use out-of-the-
box installations delivering static and dynamic content, respec-
tively. The dynamic content is generated using mod_python.
The integrity-measured web servers operate in substantially
the same way as the static and dynamic web servers, except
that each system creates and delivers integrity proofs with the
3Failure to evenly distribute request arrivals in dynamic tests leads to
throughput oscillation. This oscillation causes client requests to arrive in bursts
that overwhelm queues and cause synchronized retransmissions. Randomized
arrivals of client proof requests will dampen oscillation.
Integrity measured web server throughput – sustained RPS during
Fig. 10.
a 70 second experiment.
content. Clients in the integrity-measured experiments receive
the content as in normal web server operation, then retrieve
the associated proof from the web server as indicated in the
X-Attest-URL header. Thus,
integrity measured content
consists of two serial requests—one each for the content and
the proof.
Figure 9 shows throughput of an unaltered web server
measured in requests per second (RPS). The throughput of
the 10KB static content (average 10,770 RPS) has about 29%
higher throughput than the dynamic case (average 7,600 RPS)
for 10KB web pages. Such throughput disparities are not
atypical in web systems. The additional overheads are due to
forking and using a mod_python interpreter. This disparity
is further ampliﬁed by the static content being delivered from
in-memory caches in all tests, i.e., the web server can easily
hold all experimental static content in memory. The throughput
of the web server serving non-integrity measured 25KB pages
for dynamic content are 4,486 and 4,508 RPS for static and
dynamic content, respectively. The throughputs are similar
becuase the network is fully utilized.
A comparison of the relative throughput of the web server in
the static and dynamic content costs highlights the bottlenecks
associated with each content type. For example, the number of
bytes sent per second by the web server serving static content
of both the 10KB and 25KB pages is essentially the same:
10, 770 ∗ 10 = 107, 700KB/s ≈ 4, 485 ∗ 25 = 112, 125KB/s,
where 5% more “bytes on the wire” are delivered by serving
larger web pages. This slight advantage can be accounted
for by overheads of processing individual requests (there is
2.5 times more per-byte HTTP protocol overhead in 10KB
web pages). This indicates that the bottleneck in the static
case is bandwidth. For dynamic content, the performance does
not change drastically from when varying the ﬁle size until
the network becomes saturated. This indicates that dynamic
content throughput is bound by computation, not by bandwidth.
the average throughput of the
integrity-measured web server hovers around 1000 RPS. The
overheads relate to the creation and acquisition of proofs by the
Spork daemon and their insertion in response web objects. In
addition, each request involves serial requests and responses.
However, opportunities exist to amortize these costs, discussed
Illustrated in Figure 10,
101
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:10:03 UTC from IEEE Xplore.  Restrictions apply. 
 4000 6000 8000 10000 12000 14000 16000 300 310 320 330 340 350 360 370RPSTimeline (seconds)Static (10KB)Static (25KB)Dynamic (10KB)Dynamic (25KB) 600 800 1000 1200 1400 1600 1800 2000 300 310 320 330 340 350 360 370RPSTimeline (seconds)Static (10KB)Static (25KB)Dynamic (10KB)Dynamic (25KB)Generate Merkle Hash Tree
Obtain TS Quote
Generate Quote
Static
0.716 (0.08%)
35.9 (3.68%)
938.4 (96.24%)
Dynamic
1.9 (0.19%)
34.9 (3.58%)
938.8 (96.23%)
TABLE II
PROOF CREATION LATENCY MICRO-BENCHMARKS – LATENCY OF PROOF
SYSTEM GENERATION MEASURED IN MILLISECONDS. FOR THE STATIC
CONTENT, A POOL OF 125 FILES WAS USED.
further in Sections V-B and V-C.
Integrity-measured dynamic content
shows an average
throughput of 1100 RPS in both the 10KB and 25KB
cases, similar to the non-integrity measured dynamic content
where computation, not bandwidth, is the bottleneck. Integrity-
measured dynamic content is bounded by the computation
of both the content and the proof. The integrity-measured
dynamic content also exhibits bursty behavior attributable to
the sychronizing effect of the TPM. Clients make a request for
dynamic content followed by a request for the corresponding
proof and are forced to wait while the TPM generates the quote
that includes their page. Once this quote is generated, clients
begin the process again by making another request for content.
Table I shows minimum observed latency and average
throughput. To compute latency statistics, we averaged mea-
surements over 150 trials in a system with a single client
requesting a single page. The latency represents the time from
the ﬁrst byte sent from the client to the reception of the last
byte of the response. Unaltered web latencies range from 490
µsec to 5.4 msec. The latencies observed in the static integrity
measured case averaged about 3 msec, where the additional
latency can be attributed to multiple HTTP RTTs and the costs
of acquiring the proof from the Spork daemon. The dynamic
integrity measured latencies were lower than expected values
(as discussed in Section III-C), about 1000 msec. These longer
latencies are a reﬂection of the random arrival of the request
within the periodic TPM quotations and the time required to
create a proof system encompassing the quoted material, e.g.,
TPM quotation time.
Table II shows latency microbenchmarks of proof creation
in an integrity-measured web server. Recall that the proof
system is generated by collecting document, time, and system
information over which a TPM quote is taken. Such operations
are amortized over all requests during the proof system period
(as discussed in Section III-B), and are not on the critical
path of any content delivery. Nearly 99% of the latency
involves the acquisition of the time quote and the local quote
operation.4 These operations are external to the web server
processing. The remaining operations are insubstantial in terms
of latency and computation. As a result, proof system creation
has little impact on the throughput of the web server. Thus, our
only hope at improving web server throughput is to address
the network and computation bottlenecks within the content
delivery process itself.
4Recall that the time server simply returns the most recently created time
quote. Thus, the latency for acquiring a time proof is largely determined by
the RTT between the web and time servers, and not the time to create the time
attestation (964 msec).
102
B. Bandwidth Optimizations
Because we cannot modify the pages directly, we limit
bandwidth use by reducing the size of the returned proofs.
The proofs are large ASCII XML structures in which the vast
majority of content ﬁelds are integrity hashes. Because the
ASCII text is highly redundant, compressing it could reduce
the size of proofs considerably. Conversely, the Policy-Reduced
Integrity Measurement Architecture (PRIMA) [17] provides for
smaller attestations by reducing the size of the measurement
list to include only the speciﬁc applications of interest, and can
thus be used to signiﬁcantly reduce the number of integrity
hashes included in a quote5. We consider the performance
of our web server under these strategies: compressed IMA
compresses the proofs described in the preceding sections
before transmitting to the client, PRIMA implements PRIMA
for proofs, and compressed PRIMA compresses the PRIMA
proof. We include the performance of a web server delivering
the content proofs used in the preceding experiments as full
IMA.
The different optimizations reduce proof size as follows.
The baseline full IMA generates an 107 KB proof and the
full PRIMA reduces to 82k. The reason that the reduction is
not very large is that the test environment is already fairly
minimal, where the number of measurements needed is smaller
than in systems with more services, e.g., database systems.
Thus, the policy reduction only removes a handful of services
from measurements. Compressing the proof was much more
successful, where the IMA and PRIMA proofs were reduced
to 32 and 25 KB, respectively.
Returning to Table I, the throughput the web server improves
under these bandwidth optimizations. Compression of static
content clearly improved throughput. Simply compressing the
proofs results in 10-57% increased throughput, with com-
pressed PRIMA proofs seeing a 57% increase. These optimiza-
tions had negligible effect on throughput of servers serving
dynamic content because bandwidth is not the bottleneck.
Compared to the delivery of static content on an unaltered
server, a web server delivering compressed PRIMA proofs will
still observe over 85% overhead for 10KB page and 65% in
25KB pages. This is largely due to every integrity-measured
static page requiring the processing and delivery of one static
and one dynamic page: one for the content and one for the
proof. While compression techniques mitigate the delivery of
the dynamic page, it does nothing to mitigate the computational
costs of its creation. Thus, our next best hope is to alter
the relationship between the number of requested pages and
requested proofs.
C. Proof Amortization
Recall that prior studies of web pages show that an average
page has one root HTML page and just over 10 static 10KB
embedded objects. As a matter of practice, a client requesting
that page will obtain the root page and all of its embedded
5Additional information about the XML structure and PRIMA can be found
in the Appendicies of [34].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:10:03 UTC from IEEE Xplore.  Restrictions apply. 
Static
10 KB Pages
Min. Lat.
25 KB Pages
Min. Lat.
10 KB Pages
Min. Lat.
25 KB Pages
Min. Lat.
RPS
Base
10769
IMA
1108.6
PRIMA
1232.6
Compressed IMA
1504.9
Compressed PRIMA 1557.7
0.49
3.1
2.9
2.6
2.6
RPS