### System Architecture and Experimental Setup

In the preceding sections, we described a system where one blade served as the time server, and four blades were used to simulate clients. All experiments utilized Apache 2.2.8 with mod_python 3.3.1 for dynamic content generation. The Spork daemon is written in Python 2.5.2 and uses a custom TPM integration library written in C. The server and client browser extension exceed 5,000 lines of code.

**Figure 8: Overview of the Spork System Architecture**
- **Time Server**: Provides an attested timestamp to the web server.
- **Web Server**: Binds the timestamp to the content delivered to the browser and local software integrity information.
- **Spork Daemon**: Processes proof requests and generates proofs.

The Spork web environment consists of external clients, the time service, and two functional elements on the web host: the web server and the Spork daemon.

### A. Proof-Generating Web Server

The Apache web server supporting Spork directs all client requests (Step 1 in Figure 8) to Spork threads running in the httpd address space. For static pages, the content is retrieved from the local filesystem. A URL to a proof page (which may not yet exist) is inserted into the X-Attest-URL header of the retrieved page, and the result is returned to the client (Step 6).

For dynamic requests, the content is generated using the appropriate content generation code, such as ASP [31], instead of being retrieved from the filesystem. If the request is for a proof, the Spork request processing thread passes proof identity information to a Spork master thread (one per Apache process), which then passes the proof request to the Spork daemon over standard UNIX IPC (Step 2). The processing thread sleeps until it receives a "proof ready" event. When the requested proof (Step 5) is received by the master thread from the Spork daemon, it wakes the processing thread, which returns the proof to the client (Step 6).

The Spork daemon generates content proofs by interweaving several utility threads. The main thread receives requests from Apache, extracts and marshals the succinct proofs from available proof systems, and returns the result to the main Spork thread in Apache (Step 5). Other threads update the internal state from which the proof systems are constructed. A TPM thread schedules and executes quote operations (Step 4) as defined in Section III-C, and a separate time thread retrieves time attestations (Step 3). Additional threads maintain the dictionary of static documents and the current set of dynamic pages awaiting proof generation.

Client browsers receive the content proof from the web server (Step 6) and acquire time attestations from the time server (Step 7). If the proofs validate correctly, the page is rendered. Policy dictates the action if a proof validation fails; the browser may block rendering or warn the user.

### Load Testing and Performance Analysis

All load tests were performed using the Apache JMeter benchmarking tool. Recent studies indicate that the average web page size is about 130KB, with an average HTML source size of 25KB and the average non-flash object size just under 10KB [32]. More focused studies of popular websites show somewhat larger total sizes (â‰ˆ 300KB) [33], with the increases in the number of embedded objects accounting for the larger total page size. Thus, we use 10KB and 25KB file sizes in all experiments.

An analysis of the test environment showed that the maximum throughput of an unaltered Apache web server can be reached with a relatively small number of clients (approximately 200-300) for static content. In dynamic experiments, client requests are delayed a random period (up to two times the TPM quote period, 1900 msec) before requesting another page. This ensures uniform arrival of requests at the server, but necessitates significantly more clients to sustain maximal throughput. After experimenting with different client community sizes, we found the highest throughput could be achieved in static experiments with 500 clients and dynamic experiments with 8,000 clients without incurring significant latencies. Thus, we use 500 clients for all static tests and 8,000 for all dynamic tests.

### A. Macrobenchmarks

Our first set of experiments aimed to identify the overheads associated with the delivery of integrity proofs by comparing the operation of Spork with that of an unaltered web server. The static and dynamic content web servers use out-of-the-box installations. The dynamic content is generated using mod_python. The integrity-measured web servers operate similarly, except they create and deliver integrity proofs.

**Figure 9: Unaltered Web Server Throughput**
- **Static Content (10KB)**: Average 10,770 RPS
- **Dynamic Content (10KB)**: Average 7,600 RPS
- **Static Content (25KB)**: 4,486 RPS
- **Dynamic Content (25KB)**: 4,508 RPS

The throughput disparities are due to forking and using a mod_python interpreter, and the static content is delivered from in-memory caches. The throughput of the web server serving non-integrity measured 25KB pages for dynamic content is similar because the network is fully utilized.

**Figure 10: Integrity-Measured Web Server Throughput**
- **Static Content (10KB)**: Average 1000 RPS
- **Dynamic Content (10KB and 25KB)**: Average 1100 RPS

The overheads relate to the creation and acquisition of proofs by the Spork daemon and their insertion in response web objects. Each request involves serial requests and responses, but opportunities exist to amortize these costs, discussed further in Sections V-B and V-C.

### B. Bandwidth Optimizations

To reduce bandwidth use, we limit the size of the returned proofs. The proofs are large ASCII XML structures, and compressing them can reduce their size. PRIMA [17] reduces the size of the measurement list, thus reducing the number of integrity hashes included in a quote. We consider the performance of our web server under these strategies:
- **Compressed IMA**: Compresses the proofs before transmitting to the client.
- **PRIMA**: Implements PRIMA for proofs.
- **Compressed PRIMA**: Compresses the PRIMA proof.

**Table II: Proof Creation Latency Microbenchmarks**
- **Static Content (10KB)**
  - Generate Merkle Hash Tree: 0.716 ms (0.08%)
  - Obtain TS Quote: 35.9 ms (3.68%)
  - Generate Quote: 938.4 ms (96.24%)
- **Dynamic Content (10KB)**
  - Generate Merkle Hash Tree: 1.9 ms (0.19%)
  - Obtain TS Quote: 34.9 ms (3.58%)
  - Generate Quote: 938.8 ms (96.23%)

Compression of static content improves throughput. Simply compressing the proofs results in a 10-57% increase in throughput, with compressed PRIMA proofs seeing a 57% increase. These optimizations have negligible effects on the throughput of servers serving dynamic content because bandwidth is not the bottleneck.

### C. Proof Amortization

Prior studies show that an average page has one root HTML page and just over 10 static 10KB embedded objects. A client requesting that page will obtain the root page and all its embedded objects. To amortize the cost of proof generation, we aim to alter the relationship between the number of requested pages and requested proofs.

**Table I: Minimum Observed Latency and Average Throughput**
- **Static Content (10KB Pages)**
  - Min. Latency: 0.49 ms
  - RPS: 10769
- **Static Content (25KB Pages)**
  - Min. Latency: 3.1 ms
  - RPS: 1108.6
- **Dynamic Content (10KB Pages)**
  - Min. Latency: 2.9 ms
  - RPS: 1232.6
- **Dynamic Content (25KB Pages)**
  - Min. Latency: 2.6 ms
  - RPS: 1504.9

These optimizations help mitigate the computational costs of proof creation, improving overall system performance.