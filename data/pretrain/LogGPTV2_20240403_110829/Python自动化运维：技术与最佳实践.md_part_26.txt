功能
存储分区
/data
/data
/data
---
## Page 230
core-site.xml、hdfs-site.xml、mapred-site.xml，具体内容如下:
证，具体参考9.2.5节关于配置Linux主机SSH无密码访问的介绍，本节将不再陈述。
12.2
口core-site.xml，Hadoop core的配置项，主要针对Common 组件的属性配置。由于默认
hadoop-env.sh，Hadoop 环境变量配置文件，指定 JAVA_HOME。
修改目录（/usr/local/hadoop-1.2.1/conf）中的四个Hadoop核心配置文件hadoop-env.sh、
SSH登录Master主机，这里使用root 账号进行相关演示。安装JDK环境：
（1）安装
由于部署Hadoop 需要Master访问所有 Salve主机实现无密码登录，即配置账号公钥认
export JAVA_HOME=/usr/java/jdk1.6.0_45
# cd /usr/local/hadoop-1.2.1/conf
#cd/usr/local
安装Hadoop，版本为1.2.1，安装路径为/usr/local。
.profile
cd/etc
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
export
exportJAVA_HOME=/usr/java/jdk1.6.0_45
#vi/etc/profile（配置Java环境变量，追加以下内容）
.2.1.tar.gz
linux-x64.bin
tar -zxvf hadoop-1.2.1.tar.gz
wget http://mirrors.cnnic.cn/apache/hadoop/common/hadoop-1.2.1/hadoop-
tmp/hadoop-S{user.name}，作为Hadoop用户的临时存储目录，配置如下：
系统的类型是Hadoop不支持的，会报“File/tmp//input/conf/slavescouldonly
的 hadoop.tmp.dir 的路径为/tmp/hadoop-${user.name},笔者的Linux系统的/tmp文件
./jdk-6u45-1inux-x64.bin
chmod +x jdk-6u45-1inux-x64.bin
wget http://uni-smr.ac.ru/archive/dev/java/SDKs/sun/j2se/6/jdk-6u45-
星 doopeH 
PATH=$PATH:$JAVA_HOME/bin
Linux公社www.linuxidc.com
（使环境变量生效）
第12章Python大数据应用详解2
203
---
## Page 231
204第二部分高级篇
masters,
192.168.1.20:9001
mapred.job.tracker
 mapred-site.xml,
2
dfs.replication
hdfs-site.xml，Hadoop 的 HDFS 组件的配置项，包括 Namenode、Secondarynamenode
/data/hdfs/data
hdfs://192.168.1.20:9000
fs.default.name
4096
dfs.data.dir
/data/hdfs/name
dfs.name.dir
置内容如下：
数据（metadata）信息的备份作用，当NameNode发生故障后可以快速还原数据，配
Secondarynamenode的角色，生产环境要求使用独立服务器，起到HDFS文件系统元
如下：
和Datanode等，配置如下：
/data/tmp/hadoop-$(user.name}
hadoop.tmp.dir
配置Secondarynamenode项，
Linux公社 www.linuxidc.com
配置map-reduce组件的属性，包括jobtracker和tasktracker，配置
1/数据备份的个数，默认为3
//Datanode所允许同时执行的发送和接受任务数量，默认为256
//Datanode数据存储路径
//Namenode持久存储名字空间、事务日志路径
环境使用主设备192.168.1.20同时承担
//master主机IP:9000端口
---
## Page 232
件路径为C:\Windows\System32\drivers\etc，
议直接搭建内网DNS服务），保证Hadooop环境所有主机的/etc/hosts文件配置如下：
Master保持一致，切记！执行以下命令进行复制：
#cd/usr/local/hadoop-1.2.1
配置完成后在主节点（Master）上格式化文件系统的namenode，
iptables
Slaves:
iptables
iptables
Master:
如设备启用了iptables防火墙，需要对主节点（Master）及Slave主机添加以下规则：
192.168.1.21
管理员通过浏览器查看datanode信息，
192.168.1.22
192.168.1.21
192.168.1.20
Hadoop部分功能是通过主机名来寻址的，因此需要配置主机名hosts 信息（生产环境建
iptables
iptables
iptables
192.168.1.22
#scp -r/usr/local/hadoop-1.2.1 PI:EMAIL:/usr/local
#PI:EMAIL'[-d/usr/java
接下来，从主节点（Master）复制jdk及Hadoop环境到所有Slave，
iptables
192.168.1.22
192.168.1.21
口slaves，配置所有 Slave主机信息，填写IP地址即可。本示例中 Slave的信息如下：
192.168.1.20
scp-r/usr/java/PI:EMAIL:/usr/java
6cp
ssh
-r/usr/java/PI:EMAIL:/usr/java
-工
-IINPUT
-I
-IINPUT-S192.168.1.0/24-ptcp
Linux公社
INPUT -s 192.168.1.20 -p tcp
INPUT-S192.168.1.0/24-P
INPUT
INPUT
SN2013-08-022
SN2013-08-021
SN2013-08-022
SN2013-08-021
SN2013-08-020
T-s192.168.1.0/24-p tcp --dport50075-jACCEPT
192.168.1.0/24
www.linuxidc.com
添加所有 datanode主机信息，如下：
，需要配置本地hosts，如Windows7系统hosts文
-ptcp
-P
tcp
--dport 50070-j ACCEPT
1mkdir
-dport50010-jACCEPT
--dport9000-jACCEPT
--dport 50030-jACCEPT
第12章Python大数据应用详解205
--dport50060-jACCEPT
-p/usr/java1
执行：
目标路径要与
---
## Page 233
图12-2所示。
206第二部分高级篇
访问Hadoop提供的管理页面，Map/Reduce管理地址：http://192.168.1.20:50030/，如
如果返回如图12-1所示结果，则说明配置成功。
# bin/hadoop jar hadoop-examples-1.2.1.jar pi 10 100
Hadoop官方提供的一个测试MapReduce的示例，执行：
（2）检验安装结果
#bin/start-all.sh
最后，在主节点（Master）上执行启动命令，如下：
#bin/hadoop namenode -format
08
08710
/10
input
19:54:541
npu
Linux公社 www.linuxidc.com
per
#
Ma
Mar
NFO
NF
图12-1计算pi的测试结果（部分截图）
mapreo
nm
JobClient:
Dqor
ohC
Rur
andp
mat.
0%
%
---
## Page 234
排进行统计（map），其次将每个人的统计结果进行汇总（reduce），最终得出总数。Hadoop 除
map 运算的结果再由reduce继续进行合并。例如，要统计图书馆有多少本书籍，首先一人一
12.3
Map与Reduce为两个独立函数，为了加快各节点的处理速度，使用并行的计算方式，
HDFS 存储管理地址：http:/192.168.1.20:50070/，如图12-3所示。
Cluster Summary
SN2013-08-020
Identifier:201408222215
使用Python编写MapReduce
31 files and directories, 9 blocks = 40 total. Heap Size is 15.38 MB / 966.69 MB (1%)
Cluster Summary
Browse the filesystem
NameNode
Dead Nodes
Live_Nodes
DFS Remaining%
DFS
DFS Remaining
Configured Capacity
Upgrades:
Compiled
Started:
Used%
Reduce
Running
Mon Ju12215:23:09PDT2013bymattf
Tasks
Linux公社 www.linuxidc.com
Aug 2222:15:42 CST2014
Submissions
There are no upgrades in progress.
lon
,2.
Fri Aug 22 22:14:01 CST 2014
0006 :070-80-010729 
Total
Ju1 2215:23:09 PDT 2013 by mattf
图12-2
(Heap Size is 7.31 MB/966.69 MB)
Hadoop Map/Reduce Administration
r1503152
图12-3
Nodes
Map/Reduce管理界面（部分截图）
HDFS 管理界面（部分截图）
764.23MB
7.82GB
8.56GB
Slots
91.
456KB
第12章
Python大数据应用详解207
Slots
Map Task
Capacity
---
## Page 235
chmod +x/home/test/hadoop/reducer.py。
个单词出现的总次数并输出到标准输出（stdout），要求reducer.py 同样具备可执行权限，执行
py具备可执行权限，执行chmod+x/home/test/hadoop/mapper.py。
词出现的总次数，而是直接输出“word 1”，以便作为Reduce 的输入进行统计，要求 mapper.
12.3.1
分别使用原生Python 与框架方式来编写 mapreduce。文本文件内容如下：
编写。
Python 中便是 sys.stdin 输人数据、sys.stdout 输出数据。其他业务逻辑也直接在Python 中
Streaming，它通过使用标准的输入与输出来实现map与reduce之前传递数据，映射到
了提供原生态的Java来编写MapReduce任务，还提供了其他语言操作的API——Hadoop
208第二部分高级篇
【/home/test/hadoop/reducer.py 】
见下面的 reducer.py 代码，它会从标准输人（stdin）读取 mapper.py 的结果，然后统计每
【/home/test/hadoop/mapper.py】
见下面的mapper.py代码，它会从标准输入（stdin）读取数据，默认以空格分割单词，
（1）编写Map代码
下面实现一个统计文本文件（/home/test/hadoop/input.txt）中所有单词出现的词频功能，
#!/usr/bin/env python
（2）编写Reduce代码
for line
#输入为标准输入stdin；
#!/usr/bin/env python
abc labs foo me python hadoop ab ac bc bec python
【/home/test/hadoop/input.txt】
import sys
#以默认空格分隔行单词到words列表；
line = line.strip()
#删除开头和结尾的空格：
用原生 Python 编写 MapReduce 详解
for word inwords:
words =line.split()
print'%s\ts'
#输出所有单词，格式为“单词，1”以便作为Reduce 的输入；
in
Linux公社 www.linuxidc.com
 sys.stdin:
（word，1)
---
## Page 236
动实现排序，如图12-5所示。
结果是否正确，测试结果如图12-4所示。
测试reducer.py时需要对mapper.py 的输出做排序(sort)操作，当然，Hadoop 环境会自
#/usr/local/hadoop-1.2.1/bin/hadoop dfs -mkdir /user/root/word
首先在 HDFS上创建文本文件存储目录，本示例中为/user/root/word，运行命令：
（4）在Hadoop平台运行代码
我们可以在Hadoop平台运行之前在本地进行测试，校验mapper.py与reducer.py运行的
（3）测试代码
if current_word ==
for line in sys.stdin:
#获取标准输入，即mapper.py的输出；
word=None
current_count =0
current_word =None
import sys
from operatorimportitemgetter
输出最后一个word统计
else:
if current_word== word:
#要求mapper.py的输出做排序（sort）操作，以便对连续的word做判断；
#转换count从字符型成整型；
word,count=1ine.split('\t'，1)
line =line.strip()
#删除开头和结尾的空格；
try:
current_word= word
current_count= count
if current_word:
continue
count =int（count)
count非数字时，忽略此行；
Linux公社 www.linuxidc.com
print%s\t%s'%（current_word,current_count)
#输出当前word统计结果到标准输出
word:
第12章Python大数据应用详解209
---
## Page 237
命令：
下方法进行操作，因为Hadoop 分析目标默认针对目录，目录下的文件都在运算范围中。
210第二部分高级篇
#/usr/local/hadoop-1
下一步便是关键的执行MapReduce任务了，输出结果文件指定/output/word，执行以下
-rw-r--r--
#/usr/local/hadoop-1.2.1/bin/hadoop dfs -ls/user/root/word/
#/usr/local/hadoop-1.2.1/bin/hadoopfs -put/home/test/hadoop/input.txt/user/
上传文件至HDFS，本示例中为/home/test/hadoop/input.txt，如果有多个文件，可采用以
Found1items
root/word/
Linux公社 www.linuxidc.com
2 root supergroup
.2.1/bin/hadoop
图12-4
图12-5reducer执行结果
mapper执行结果（部分截图）
1182014-02-10-09:49/user/root/word/input.txt
jar
/usr/local/hadoop-1.2.1/contrib/
---
## Page 238