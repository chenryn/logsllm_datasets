title:DRMI: A Dataset Reduction Technology based on Mutual Information
for Black-box Attacks
author:Yingzhe He and
Guozhu Meng and
Kai Chen and
Xingbo Hu and
Jinwen He
DrMi: A Dataset Reduction Technology based on 
Mutual Information for Black-box Attacks
Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He, SKLOIS, 
Institute of Information Engineering, Chinese Academy of Sciences/School of 
Cyber Security, University of Chinese Academy of Sciences
https://www.usenix.org/conference/usenixsecurity21/presentation/he-yingzhe
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.DRMI: A Dataset Reduction Technology based on Mutual Information for
Black-box Attacks
Yingzhe He1,2, Guozhu Meng1,2,*, Kai Chen1,2,*, Xingbo Hu1,2, and Jinwen He1,2
1SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China
2School of Cyber Security, University of Chinese Academy of Sciences, China
Abstract
It is non-trivial to attack deep neural networks in black-box
settings without any model detail disclosed. Prior studies on
black-box attacks leverage a number of queries to the target
model for probing the target model or generating adversarial
examples. Queries are usually limited and costly so that the ad-
versary probably fails to mount an effective attack. However,
not all the queries have to be made since there exist repeti-
tions or redundancies that induce many inefﬁcient queries.
Therefore, it leaves a lot of room for data reduction and more
efﬁcient queries.
To this end, we ﬁrst propose to use mutual information
to measure the data redundancy between two data samples,
and then develop a data reduction technique based on mutual
information, termed as DRMI. We implement an efﬁcient
optimization algorithm in DRMI, so as to obtain a particular
subset of data samples, of which the mutual information in
between is minimized. We conduct extensive experiments on
MNIST, CIFAR10, and ImageNet, and six types of deep neural
networks, and evaluate DRMI in model extraction and adver-
sarial attacks. The results demonstrate its high effectiveness in
these attacks, surpassing a state-of-the-art approach by raising
7% of model accuracy and two times more transferability of
adversarial examples. Through the comparison experiments
with other three strategies, we identify what properties of data
have been preserved and removed, to some extent reveal the
essences of deep neural networks.
1 Introduction
Deep neural networks (DNNs) are now well known to be
vulnerable to many attacks [5, 24, 36], such as adversarial
attacks [9, 38, 57], model extraction attacks [58, 60], model
inversion attacks [18, 51], and poisoning attacks [27, 49]. Un-
perceivable perturbations added into an image can deceive a
* Corresponding authors.
classiﬁer in an adversarial attack. Furthermore, these weak-
nesses in DNNs are considerably magniﬁed along with the
widespread deployment and commercialization of deep learn-
ing. To date, a line of research has successfully subverted
the mainstream deep learning systems [33, 61, 64] that can
endanger the users’ daily life.
These attacks encounter several obstacles in black-box set-
tings where most if not all information about model is un-
known. Prior research has paved a way in solving them like
e.g., transfer attacks [44,45] and optimization attacks [25,59].
Both of these attacks have to query the target model as prereq-
uisites, and then either train a substitute model [29, 45] or fur-
ther optimize the queries. With a substitute model, attackers
cannot only uncover the parameters and decision boundaries
of the model, but also generate adversarial examples (AEs)
in a white-box setting. However, in reality, a large number
of queries to the model are costly and even infeasible. That
motivates the research on reducing queries to the model.
For simplicity, we assume that attackers can access a sim-
ilar dataset of the target model in this study. As such, to re-
duce the queries in a black-box attack, we can turn to se-
lecting high quality data and eliminating redundancies from
the original for substitute model training. Similar with our
study, PRADA [29] manages to extract model information
in black-box settings. It develops a Jacobian-based method
to synthesize high quality data, and trains a substitute model
with limited queries. Tested on the MNIST [35] dataset, the
substitute model can still obtain a 90% accuracy with merely
1/300 of the data, and effectively facilitate the generation of
adversarial examples. Gradient Estimation [7] also attacks
black-box model with 61.5% success rate under 196 queries.
The motivation of our research is to reduce the query cost
of training a substitute model in black-box settings without
accessing the exact training data. The substitute model can
also be used for other attacks, such as model inversion at-
tacks [18, 51], adversarial attacks [9, 38]. To fulﬁll the re-
duction, we ﬁrst propose mutual information (MI) [2] for
measuring the redundancies in a data set. MI is a measure of
the mutual dependence between two variables in information
USENIX Association
30th USENIX Security Symposium    1901
theory. More dependent (or similar) variables indicate a larger
MI in between, which induces data redundancy conceptually.
Given this, we develop a data reduction technique based on
mutual information (DRMI). In DRMI, we calculate the MI
value between any two data samples, and search a subset of
ﬁxed size to ensure the sum of MI values among selected sam-
ples is minimized (see Section 4). In this way, the selected
samples are more independent and informative for the good of
substitute model training. In addition, we compare our DRMI
with another three reduction techniques based on correlation
matrix (CMAL) [65], class probability (CPB) [42], and acti-
vated neuron trace (TRACE) [16] in Section 5.4, showing that
DRMI exhibits a more superior performance.
We design a set of experiments to evaluate DRMI compre-
hensively. These experiments are carried on the MNIST [35],
CIFAR10 [32], and ImageNet [48] datasets. Six models, i.e.,
LeNet-5 [34], C3F2 (detailed in Table 1), DNN5 (detailed
in Table 2), ResNet18 [23], ResNet152 [23], and Inception-
v3 [56], have been employed for substitute model training. In
a nutshell, DRMI surpasses PRADA by 7% in the accuracy
of substitute models, with only 50 queries on the MNIST
dataset. Based on the substitute model, we generate adver-
sarial examples and their transferability reaches up to 66%,
three times more than PRADA. Under 600 queries on MNIST,
DRMI achieves 97.3% model accuracy and 78.5% transfer-
ability using C3F2 architecture, improving 3.3% accuracy and
29.5% transferability than PRADA. Furthermore, DRMI also
raises 11.7% attack success rate with even 46 fewer queries
than Gradient Estimation. DRMI raises 1.1%, 11.2% attack
success rate with 618, 1343 fewer queries than NES [25],
AutoZoom [59] on the ImageNet dataset, respectively. Ex-
periments prove that DRMI can effectively facilitate model
extraction and adversarial attacks in black-box settings. Ad-
ditionally, the comparison experiments with three other mea-
sures show that DRMI exceeds CMAL, CPB, TRACE meth-
ods with an average accuracy of 6.46%, 9.03%, and 26.53%,
respectively. From the results, we identify several insights on
interpretability of deep learning process in Section 5.4.
Contributions. We make the following contributions.
• We propose a novel data reduction technology based on
mutual information dubbed DRMI. By solving the simpli-
ﬁed dataset with the minimum value of the overall mutual
information, we can form a rival model of >96% accuracy
with only 1% of training data (Section 5.2).
• We conduct black-box attacks (Section 5.3) for extracting
model information and generating adversarial examples
based on the substitute model. The results show our ap-
proach outperforms PRADA in both model accuracy (+7%)
and transferability (x3), and outperforms Gradient Estima-
tion in success rate (+11.7%).
• We explore the interpretability of deep learning models
from the perspective of data reduction (Section 5.4). The
conclusions indicate the properties that are either reserved
or wiped by deep neural networks, and facilitate an in-depth
understanding.
2 Background
2.1 Dataset Reduction in Learning
Deep learning algorithms often require large datasets for train-
ing [17, 43]. That also results in the emerging of data aug-
mentation for enriching the training data [15, 47]. However,
the requirement brings new problems: collecting and labeling
data cost tremendous time and resources; training model on
a large dataset occupies huge computation; and a large vol-
ume of data is susceptible to poisoned data [39]. There have
been already works on reducing training data to raise learn-
ing efﬁciency [12, 41]. These works explore how to simplify
the training data without loss of model correction, and even
defend poisoning attacks by eliminating low quality data.
High quality data means a speciﬁc set of samples which
can well represent and sample the whole dataset with few
redundancies and repetitions. As a kind of high-dimensional
data, there are many similarity metrics between images, such
as structural similarity (SSIM) and cosine similarity. The
mostly used method is Lp-norm, which measures the per-
ceptual similarity between original images and adversarial
images [9, 19, 57, 63]. However, recent research [50] ﬁnds
that Lp-norm is neither necessary nor sufﬁcient for perceptual
similarity, and new metrics need to be proposed for more ac-
curate measurements [28]. In this paper, we propose a novel
concept to connect mutual information measurement with
image dataset quality. Our experiments prove that mutual
information can measure the independence, diversity and rep-
resentativeness of data. We tend to explore the application of
mutual information in more ﬁelds, such as perceptual similar-
ity.
In this paper, we propose a model-independent dataset re-
duction approach DRMI, which treats mutual information
as an indicator to measure the common information shared
by two samples. We also compare DRMI with three other
measurements–correlation matrix (CMAL), class probability
(CPB) and activated neuron trace (TRACE). CMAL constructs
a matrix to present the correlation distribution among all data
samples. It is still model-independent since it can be com-
puted in advance of model training. Additionally, we observe
the system states and outputs after training data is feed into the
model. In particular, we record the activated neurons scattered
in different layers, and the class probability for the input data.
Based on these information, we implement the corresponding
reduction techniques. As the information is processed by the
model, we take them as model-dependent measures. Although
CMAL, CPB, TRACE do not perform as well as DRMI, the re-
sults help us understand training data and models, and analyze
interesting conclusions in the view of interpretability.
1902    30th USENIX Security Symposium
USENIX Association
Figure 1: The workﬂow of our work
2.2 Black-box Attacks against DNNs
Black-box attacks against DNNs are of great variety [24, 36].
In this paper, we only focus on model extraction attacks and
adversarial attacks.
Model Extraction Attack. It is an emerging technology to at-
tack deep learning models in recent years. For deep neural net-
works, this attack tends to steal parameters [58], hyperparam-
eters [60], architectures [40], decision boundaries [29, 44, 45],
and functionalities [42]. However, it acquires a large number
of queries to the target model for simulating models’ behav-
iors. Reducing queries can not only avoid the attack being
detected, but also save monetary costs.
Existing model extraction techniques commonly require
training substitute models [42,45]. Therefore, how to improve
the effectiveness of substitute models with fewer queries has
become the main focus for this attack. We propose a data
reduction technique in this study, which enables a substitute
model up to par with smaller datasets and fewer queries.
Adversarial Attack. Adversarial attacks are the most signif-
icant threats to deep neural networks. Thousands of meth-
ods have been developed to subvert a well-trained deep
learning model. In black-box settings, queries to the tar-
get model become indispensable for either training a sub-
stitute model [10, 29, 45] or estimating approximate gradi-
ents [11, 25, 59]. The substitute model, which behaves quite
similarly with the target model, can be further used to ﬁnd
AEs in a white-box manner [9, 19, 37]. These samples can be
used to attack the target model due to their transferability. In
such a case, the limitations of queries undoubtedly raise the
difﬁculties of attacks. Existing works have tried to increase
query efﬁciency from the perspective of data distribution and
properties [6, 8, 21, 53]. In this paper, our research proposes
DRMI to quantify data redundancies and gets a much simpli-
ﬁed dataset for querying.
3 Overview
In this paper, we aim to select a simpliﬁed and representative
dataset from the original. It can not only spare the time and
computing resources for training a model, but also empower
black-box attacks with limited queries to the target model.
Figure 1 presents the workﬂow of our work. We start from
a known dataset and develop a data reduction technique to
obtain representative and reduced datasets. Then we use every
reduced dataset to train a new model (a.k.a. substitute model),
and adopt prediction accuracy to quantify the performance of
substitute models. The substitute model with higher accuracy
indicates that its training data is more representative for the
original.
Threat Model. In this study, the adversary aims to launch
black-box attacks, e.g., adversarial attacks and model extrac-
tion attacks, against a public deep learning service. However,
the adversary knows neither the internal structure and param-
eters of the target model, nor the exact training data. Even so,
it is still able to obtain a small dataset that has the same dis-
tribution as the training data, or a larger one with a different
distribution. The adversary can query the target model with
the possessed data and then get prediction results. It is not
necessary to acquire conﬁdence scores for prediction although
they are often provided by commercial services. Additionally,
it has to limit the number of queries as too many queries are
costly and probably constrained by some defense measures.
1 Data Reduction. Data reduction is a technique to remove
out redundancies and repetitions from multitudinous amounts
of data, but remain critical and representative data [22]. To
explore the redundancy in deep learning, we use mutual in-
formation as a measure and develop a data reduction tech-
nique based on it (i.e., DRMI). Moreover, we implement an-
other three reduction techniques based on correlation matrix
(CMAL), class probability of prediction (CPB) and traces
of activated neurons (TRACE) for comparison. In particular,
DRMI and CMAL are performed merely on the training data,
and not related to deep training. Therefore, they are model-
independent. CPB and TRACE both require to interact with
the target model, i.e., collecting the prediction result or inter-
nal states when one data sample passes through the model. As
such, we regard them as being model-dependent. In this study,
we employ all these four strategies to reduce the training data,
and subsequently shape a substitute model.
2 Black-box Attacks. The trained substitute model can be
applied for further black-box attacks against deep neural net-
works. More speciﬁcally, the substitute model is a close ap-
USENIX Association
30th USENIX Security Symposium    1903
DatasetSimplified DatasetQueryClass Prob.Training1. Data ReductionTarget ModelDRMICMALTRACECPBSubstitute ModelOrig. ExampleAdversarial AttackAdv. Example2. Black-box AttacksTracesModel Extraction AttackFigure 2: An illustrative example for DRMI. There are six images (noted as from 1 to 6) have quite similar appearance in pairs.
The edge indicates the mutual information between two images. Thicker line indicates larger value. To form a subset with three
images, we select images 1, 4, and 6 since the sum (1.44) of their MI values is minimal.
proximation of the target model in prediction. Hence, it helps
to infer the parameters of the target model which is known as
model extraction attacks [58, 60]. In this paper, we leverage
prediction accuracy as the success rate for a model extraction
attack. The substitute models created by the four techniques
are compared, and the result shows DRMI has achieved the
best performance (see Section 5.2 and 5.4). Based on the
result, we also conclude a number of new views on the inter-
pretability of deep neural networks.
On the other hand, the substitute model can be utilized for
generating adversarial examples in black-box settings [8, 21,
53] or white-box settings [29,45]. Data reduction is especially
beneﬁcial for transfer attacks [6, 55] since it lowers the cost
of model querying. Therefore, we conduct adversarial attack
experiments based on our reduction techniques to evaluate
its usefulness. We adopt the PGD method [37] to generate
adversarial examples towards a substitute model, and test their
transferability to the target model. Success rates are computed
and compared with other state-of-the-art approaches.
4 The DRMI Approach
In this section, we detail the DRMI approach by formaliz-
ing the problem, analyzing its complexity and providing the
solution.
4.1 Problem Formalization
the images u and v, we compute their MI value as:
MI(u)(v) =
R
∑
i=0
R
∑
j=0