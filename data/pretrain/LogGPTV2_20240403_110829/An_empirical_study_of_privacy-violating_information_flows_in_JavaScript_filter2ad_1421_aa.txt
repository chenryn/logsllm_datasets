title:An empirical study of privacy-violating information flows in JavaScript
web applications
author:Dongseok Jang and
Ranjit Jhala and
Sorin Lerner and
Hovav Shacham
An Empirical Study of Privacy-Violating Information Flows
in JavaScript Web Applications
Dongseok Jang Ranjit Jhala Sorin Lerner Hovav Shacham
Dept. of Computer Science and Engineering
University of California, San Diego, USA
{d1jang,jhala,lerner,hovav}@cs.ucsd.edu
ABSTRACT
The dynamic nature of JavaScript web applications has
given rise to the possibility of privacy violating information
ﬂows. We present an empirical study of the prevalence of
such ﬂows on a large number of popular websites. We have
(1) designed an expressive, ﬁne-grained information ﬂow pol-
icy language that allows us to specify and detect diﬀerent
kinds of privacy-violating ﬂows in JavaScript code, (2) im-
plemented a new rewriting-based JavaScript information
ﬂow engine within the Chrome browser, and (3) used the
enhanced browser to conduct a large-scale empirical study
over the Alexa global top 50,000 websites of four privacy-
violating ﬂows: cookie stealing, location hijacking, history
sniﬃng, and behavior tracking. Our survey shows that sev-
eral popular sites, including Alexa global top-100 sites, use
privacy-violating ﬂows to exﬁltrate information about users’
browsing behavior. Our ﬁndings show that steps must be
taken to mitigate the privacy threat from covert ﬂows in
browsers.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection – Unauthorized access;
D.2.4 [Software Engineering]: Software/Program Veriﬁ-
cation – Validation
General Terms
Security, Experimentation, Languages
Keywords
privacy, web security, information ﬂow, JavaScript, web ap-
plication, dynamic analysis, rewriting, history sniﬃng
1.
INTRODUCTION
JavaScript has enabled the deployment of rich browser-
based applications that are fashioned from code sourced
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’10, October 4–8, 2010, Chicago, Illinois, USA.
Copyright 2010 ACM 978-1-4503-0244-9/10/10 ...$10.00.
from diﬀerent mutually distrusting and potentially mali-
cious websites. However, JavaScript lacks language-based
protection and isolation mechanisms and sports several ex-
tremely dynamic language features. Browser-level isolation
policies like the same-origin policy for domain object model
(DOM) objects are coarse-grained and do not uniformly ap-
ply to application resources. Consequently, the proliferation
of JavaScript has also opened up the possibility of a variety
of security vulnerabilities.
In this paper, we present an empirical study of the preva-
lence of an important class of vulnerabilities that we call
privacy-violating information ﬂows. This general class in-
cludes several diﬀerent kinds of attacks that have been in-
dependently proposed and studied in the literature.
1. Cookie Stealing: Code included from a particular site,
say for displaying an advertisement, has access to all the
information on the hosting web page, including the cookie,
the location bar, and any other sensitive information stored
on the page. Thus, if the ad code is malicious it can cause the
cookie and other sensitive pieces of information to be leaked
to the third-party ad agencies, and can lead to a variety of
routinely observed attacks like request forgery.
2. Location Hijacking:
In a manner similar to the above
case, the dynamically loaded untrusted code can inﬂuence
the document’s location, by inﬂuencing the values stored in
URL string variables that are read to dynamically generate
HTTP requests. Consequently, the dynamically loaded code
can navigate the page to a malicious site that exploit a bug in
the browser to fully compromise the machine [24] or mounts
a phishing attack.
3. History Sniﬃng: In most browsers, all application do-
mains share access to a single visited-page history, ﬁle
cache, and DNS cache [12]. This leads to the possibility
of history sniﬃng attacks [14], where a malicious site (say,
attacker.com) can learn whether a user has visited a speciﬁc
URL (say, bankofamerica.com), merely by inducing the user
to visit attacker.com. To this end, the attack uses the fact
that browsers display links diﬀerently depending on whether
or not their target has been visited [6]. In JavaScript, the
attacker creates a link to the target URL in a hidden part
of the page, and then uses the browser’s DOM interface to
inspect how the link is displayed. If the link is displayed as a
visited link, the target URL is in the user’s history. Tealium
and Beencounter sell services that allow a website to collect
the browsing history of their visitors using history sniﬃng.
4. Behavior Tracking: The dynamic nature of JavaScript
allows a website to construct a high-ﬁdelity timeline of how
270a particular user interacted with a web page including, for
example, precise information about the user’s mouse clicks
and movements, scrolling behavior, and what parts of the
text were highlighted, simply by including JavaScript event
handlers that track mouse and keyboard activity. This in-
formation can then be sent back to the server to compute
statistics about how users interact with a given web page.
Several web-analytics companies sell products that exploit
these ﬂows to track information about users. For exam-
ple, ClickTale allows websites to precisely track their users’
mouse movements and compute aggregate heat maps based
on where users move their mouse, and tynt allows websites
to track what text is being copied from them. services al-
low websites to gather ﬁned-grained information about the
behaviors of their users without any indication to users that
additional information gathering is taking place. We be-
lieve that users understand that page navigation (as a re-
sult of clicking on a link) causes information to be sent to
the server, but do not believe they understand that other
actions,
like mousing over an image, can silently do the
same. Verifying this belief will require a user study. We
hope that the data we have collected will help inform the
broader discussion about the privacy implications of such
ﬂows.
Privacy-violating information ﬂows are not merely theo-
retical possibilities of academic interest. Indeed, the possi-
bility of history sniﬃng has prompted a discussion spanning
8 years and over 261 comments in Bugzilla about preventing
history sniﬃng in Firefox [3], which has ﬁnally culminated
in a recent ﬁx [2]. This lengthy process illustrates the im-
portance of privacy-violating ﬂows for Web 2.0 users and the
diﬃculty of designing defenses against them without break-
ing legitimate websites.
Despite the knowledge that privacy-violating ﬂows are
possible and even likely, little is actually known about their
occurrence in the wild. For example, how many websites
extract this kind of information from their users? Are there
any popular sites that do this? Do websites use pre-packaged
solutions like Tealium, Beencounter and ClickTale? Or do
they construct their own implementations? Are these imple-
mentations obfuscated to evade detection, and, if so, how?
The lack of empirical data about the prevalence of privacy-
violating ﬂows has hampered the development and deploy-
ment of eﬀective defenses against this increasingly important
class of attacks. The main contribution of this paper is to
provide concrete data to answer such questions through an
exhaustive empirical evaluation of several privacy-violating
ﬂows in a large number of popular websites. We have carried
out this study in three steps.
First, we have designed an expressive, ﬁne-grained infor-
mation ﬂow policy language that allows us to specify and de-
tect diﬀerent kinds of privacy-violating ﬂows in JavaScript
code (Section 2.1).
In essence, our language allows us to
describe diﬀerent privacy-violating ﬂows by specifying sites
within the code where taints are injected and sites from
which certain taints must be blocked. For example, to spec-
ify a cookie stealing ﬂow, we inject a “secret” taint into the
cookie, and block that taint from ﬂowing into variables con-
trolled by third-party code. To specify a location hijacking
ﬂow, we inject an “untrusted” taint onto any values origi-
nating third-party code, and block that taint from ﬂowing
into the document’s location ﬁeld. To specify a history sniﬀ-
ing ﬂow, we inject a “history” taint on the ﬁelds containing
the style attributes of links, and block that taint from ﬂow-
ing into the parameters of methods that send messages over
the network. To specify a behavior tracking ﬂow, we inject
“behavior” taint to the inputs of the handlers registered for
events triggered by user behavior.
Second, we have implemented a new JavaScript informa-
tion ﬂow engine in the Chrome browser. Unlike previous
JavaScript information ﬂow infrastructures [10, 30, 8], our
engine uses a dynamic source-to-source rewriting approach
where taints are injected, propagated and blocked within
the rewritten code (Section 2.2). Although the rewriting is
performed inside the browser, implementing our approach
requires understanding only the browser’s AST data struc-
ture, and none of the complexity of the JavaScript run-
time. Thus, in addition to supporting an extremely ﬂexi-
ble ﬂow policy speciﬁcation language, our approach is sim-
ple to implement and can readily be incorporated inside
other browsers. Even though the taints are propagated in
JavaScript, as opposed to natively, the overhead of our ap-
proach is not prohibitively high. Our approach adds on av-
erage 60 to 70% to the total page loading time over a fast
network (which is the worst condition to test our JavaScript
overhead). This is eﬃcient enough for our exploratory study,
and with additional simple optimizations could even be fea-
sible for interactive use (Section 3).
Third, we have used our modiﬁed version of the Chrome
browser to conduct a large-scale empirical study over the
Alexa global top 50,000 websites of four privacy-violating
ﬂows: cookie stealing, location hijacking, history sniﬃng
and behavior tracking. Our results reveal interesting facts
about the prevalence of these ﬂows in the wild. We did not
ﬁnd any instances of location hijacking on popular sites, but
we did ﬁnd that there are several third party ad agencies
to whom cookies are leaked. We found that several pop-
ular sites — including an Alexa global top-100 site — make
use of history sniﬃng to exﬁltrate information about users’
browsing history, and, in some cases, do so in an obfus-
cated manner to avoid easy detection. We also found that
popular sites, such as Microsoft’s, track users’ clicks and
mouse movements, and that huffingtonpost.com has the
infrastructure to track such movements, even though we did
not observe the actual ﬂow in our experiments. Finally, we
found that many sites exhibiting privacy-violating ﬂows have
built their own infrastructure, and do not use pre-packaged
solutions like ClickTale, tynt, Tealium, or Beencounter.
Thus, our study shows that popular Web 2.0 applications
like mashups, aggregators, and sophisticated ad targeting
are rife with diﬀerent kinds of privacy-violating ﬂows, and,
hence, there is a pressing need to devise ﬂexible, precise and
eﬃcient defenses against them.
2.
INFORMATION FLOW POLICIES
We present our approach for dynamically enforcing in-
formation ﬂow policies through an example that illustrates
the mechanisms used to generate, propagate and check taint
information for enforcing ﬂow policies. The focus of our in-
formation ﬂow policies and enforcement mechanism is to de-
tect many privacy-violating ﬂows in the wild, not to provide
a bullet-proof protection mechanism (although our current
system could eventually lead to a protection mechanism, as
discussed further in Section 7). For space reasons, we defer
a formal treatment of our rewriting algorithm to a technical
report [15].
271var initSettings = function(s){
searchUrl = s;
}
initSettings("a.com");
var doSearch = function() {
var searchBox = getSearchBoxValue();
var searchQry = searchUrl + searchBox;
document.location = searchQry;
}
eval(load("http://adserver.com/display.js"));
Figure 1: JavaScript code from a website a.com.
Web page Consider the JavaScript in Figure 1. Suppose
that this code is a distillation of the JavaScript on a web
page belonging to the domain a.com. The web page has a
text box whose contents can be retrieved using a call to
the function getSearchBoxValue (not shown). The func-
tion initSettings is intended to be called once to initialize
settings used by the page. The doSearch function is called
when the user clicks a particular button on the page.
Dynamically Loaded Code The very last line of the code
in Figure 1 is a call to load() which is used to dynam-
ically obtain a string from adserver.com. This string is
then passed to eval() which has the eﬀect of “executing”
the string in order to update the web page with an adver-
tisement tailored to the particular user.
Malicious Code Suppose that the string returned by the
call to adserver.com was:
initSettings("evil.com");
When this string is passed to eval() and executed, it over-
writes the page’s settings. In particular, it sets the variable
searchUrl which is used as the preﬁx of the query string,
to refer to an attacker site evil.com. Now, if the user clicks
the search button, the document.location gets set to the
attacker’s site, and thus the user is redirected to a website
which can then compromise her machine. Similarly, dynam-
ically loaded code can cause the user to leak their password
or other sensitive information.
2.1 Policy Language
The ﬂexibility and dynamic nature of JavaScript makes
it diﬃcult to use existing language-based isolation mecha-
nisms. First, JavaScript does not have any information hid-
ing mechanisms like private ﬁelds that could be used to iso-
late document.location from dynamically loaded code. In-
deed, a primary reason for the popularity of the language is
that the absence of such mechanisms makes it easy to rapidly
glue together diﬀerent libraries distributed across the web.
Second, the asynchronous nature of web applications makes
it diﬃcult to enforce isolation via dynamic stack-based ac-
cess control.
Indeed, in the example above, the malicious
code has done its mischief and departed well before the user
clicks the button and causes the page to relocate.
Thus, to reconcile safety and ﬂexible, dynamic code com-
position, we need ﬁne-grained isolation mechanisms that
prevent untrusted code from viewing or aﬀecting sensitive
data. Our approach to isolation is information ﬂow con-
trol [11, 21], where the isolation is ensured via two steps.
First, the website’s developer provides a ﬁne-grained policy
that describes which values can aﬀect and be aﬀected by
others. Second, the language’s compiler or run-time enforce
the policy, thereby providing ﬁne-grained isolation.
Policies In our framework a ﬁne-grained information ﬂow
policy is speciﬁed by deﬁning taints,
injection sites and
checking sites. A taint is any JavaScript object, e.g., a URL
string denoting the provenance of a given piece of informa-
tion. A site
r .f (x . . .)
corresponds to the invocation of the method f with the ar-
guments x . . ., on the receiver object r . Such site expressions
can contain concrete JavaScript (e.g., document.location),
or pattern variables (e.g., $1) that can match against dif-
ferent concrete JavaScript values, and which can later be
referenced.
In order to allow sites to match ﬁeld reads and writes, we
model these using getter and setter methods. In particular,
we model a ﬁeld read using a call to method getf, which
takes the name of the ﬁeld as an argument, and we model
a ﬁeld write using a call to a method setf, which takes
the name of a ﬁeld and the new value as arguments. To
make writing policies easier, we allow some simple syntactic
sugar in expressing sites: r .x used as an r-value translates
to r .getf(x) and r .x = e translates to r .setf(x, e).
An injection site
at S if P inject T
stipulates that the taint T be added to the taints of the
object output by the method call described by the site S as
long as the condition P holds at the callsite. For example,
the following injection site unconditionally injects a “secret”
taint at any point where document.cookie is read:
at document.cookie if true inject “secret”
To make the policies more readable, we use the following
syntactic sugar: when “if P ” is omitted, we assume “if true”.
As a result, the above injection site can be expressed as:
at document.cookie inject “secret”
(1)
A checking site
at S if P block T on V
stipulates that at site S , if condition P holds, the expression
V must not contain the taint T . We allow the guard (P ),
the taint (T ) and the checked expression (V ) to refer to the
any pattern variables that get bound within the site S . As
before, when “if P ” is omitted, we assume “if true”.
For example, consider the following checking site:
at $1.x=$2 if $1.url (cid:54)= “a.com” block “secret” on $2 (2)