(instead of random 1%) to preserve the temporal consistency
between training and testing (i.e., never using future data to
predict the past event). More speciﬁcally, the model is initially
trained with only 1% of August-2018 training dataset (ﬁrst
 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1ABCF1 ScoreWebsitesRFOCANLSTMODDS 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100F1 Score% of Training Data of Website BODDSLSTMOCANRF 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100F1 Score% of Training Data of Website AODDSLSTMOCANRF 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100F1 Score% of Training Data of Website CODDSLSTMOCANRF 0 0.2 0.4 0.6 0.8 1Aug-18Jan-19Sep-19F1 ScoreLSTMODDS 0 0.2 0.4 0.6 0.8 1Aug-18Jan-19Sep-19F1 ScoreLSTMODDSTABLE XI: Case study for website B; number of false positives and
false negatives from the cluster and outlier regions; Models are trained
with 1% of the training dataset of August 2018.
Cluster
Outliers
Clusters
Test Dataset
Malicious
Benign
1,384
26,920
FN
703
287
1,492
494
LSTM
ODDS
FP
599
0
FN
196
102
FP
611
0
(a) Website A
(b) Website B
Fig. 9: The model is initially trained with 1% of August-18 training
data, and is then re-trained each month by adding the ﬁrst 1% of the
training data of each month.
TABLE X: F1 score when using only one generator; training with
100% of the training dataset of August 2018.
Website
G1 (outlier)
G2 (clusters)
Both generators
A
B
C
0.915
0.852
0.721
0.915
0.860
0.739
0.918
0.902
0.815
two weeks) and tested in the last two weeks of August 2018.
Once it comes to January 2019, we add the ﬁrst 1% of the
January 2019 data to the original 1% of August 2018 training
data to re-train the model. This forms a January model, which
is then tested on the rest of the data in January. Similarly, in
September 2019, we add the ﬁrst 1% of the data in September
to the training dataset to retrain the model. In practice, one
can choose to gradually remove/expire older training data for
each retraining. We did not simulate data expiration in our
experiments since we only have three months of data.
As shown in Figure 9 the performances bounce back after
model retraining with only 1% of the data each month. In
general, ODDS is better than LSTM after retraining. For
example, for September 2019 of website A, ODDS’s F1 score
increases from 0.546 to 0.880 after retraining. In comparison,
LSTM’s F1 score is only 0.377 after the retraining. This
suggests that data synthesis is also helpful to retrain the model
with limited data.
E. Contribution of Generators
Q5:
the overall performance boost?
How much contribution does each generator have to
A key novelty of ODDS is to include two generators to han-
dle outlier data and clustered data differently. To understand
where the performance gain is coming from, we set ODDS to
use only one of the generators and repeat the experiments
using the August 2018 dataset (trained with 100% of the
training dataset). As shown in Table X, using both generators is
always better than using either G1 (synthesizing outlier data)
or G2 (synthesizing clustered data) alone. The difference is
more obvious on website B since its bot data is much more
difﬁcult to separate from the benign data.
F. Insights into ODDS: Why it Works and When it Fails?
Q6:
At what condition does ODDS offer little or no help?
TABLE XII: Statistics about false positives (FP) and false negatives
(FN) of ODDS. We calculate their average distance to the malicious
and benign regions in the entire dataset, and the % of benign data
points among their 100 nearest neighbors.
Avg distance
to benign
Avg distance % benign points among
to malicious
100 Nearest Neighbors
FN
FP
0.251
0.644
0.329
0.402
100%
82.0%
Data synthesis has its limitations. To answer this question,
we analyze the errors produced by ODDS, including false
positives (FP) and false negatives (FN). In Table XI, we focus
on the 1%-training setting for August 2018. We examine the
errors located in the outlier and the clustered regions. More
speciﬁcally, we run DBSCAN on the entire August 2018
dataset to identify the clustered and outlier regions. Then we
retrospectively examine the number of FPs and FNs made
by LSTM and ODDS (training with 1% data). We observe
that ODDS’s performance gain is made mainly by reducing
the FNs, i.e., capturing bots that LSTM fails to catch in both
clustered and outlier regions. For example, FNs are reduced
from 703 to 196 in outliers. The corresponding sacriﬁce on
false positives is small (FP rate is only increased from 2.0%
to 2.2%). Note that all the FPs are located in the outlier region.
To understand the characteristics of these FPs and FNs, we
present a statistical analysis in Table XII. For all the FNs
produced by ODDS, we calculate their average distance to
all the benign points and malicious points in the August-18
dataset. We ﬁnd that FNs are closer to the benign region (dis-
tance 0.251) than to the malicious region (distance 0.329). This
indicates that bots missed by ODDS behave more similarly
to benign users. We further identify 100 nearest neighbors for
each FN. Interestingly, for all FNs, 100 out of their 100 nearest
neighbors are benign points. This suggests that these FN-bots
are completely surrounded by benign data points in the feature
space, which makes them very difﬁcult to detect.
In Table XII, we also analyzed the FPs of ODDS. We
ﬁnd that FPs are closer to the malicious region (0.402) than
to the benign region (0.644), which explains why they are
misclassiﬁed as “bots”. Note that both 0.644 and 0.402 are
high distance values, conﬁrming that FPs are outliers far away
from other data points. When we check their 100 nearest
neighbors, we surprisingly ﬁnd that 82% of their nearest
neighbors are benign. However, a closer examination shows
that most of these benign neighbors turn out to be other
FPs. If we exclude other FPs, only 9% of their 100 nearest
neighbors are benign. This conﬁrms that FPs misclassiﬁed by
ODDS behave differently from the rest of the benign users.
 0 0.2 0.4 0.6 0.8 1Aug-18Jan-19Sep-19F1 ScoreLSTMODDS 0 0.2 0.4 0.6 0.8 1Aug-18Jan-19Sep-19F1 ScoreLSTMODDSTABLE XIII: Applying adversarial training on LSTM and ODDS.
We use August-18 dataset from website B; Models are trained with
1% of the training dataset.
Adversarial
Retraining?
F1 score on
original test set
Testing accuracy on
adversarial examples
LSTM
ODDS
LSTM
ODDS
No
No
Yes
Yes
0.446
0.783
0.720
0.827
0.148
0.970
1.000
1.000
In summary, we demonstrate the limitation of ODDS in
capturing (1) bots that are deeply embedded in the benign
region, and (2) outlier benign users who behave very differ-
ently from the majority of the benign users. We argue that
bots that perfectly mimic benign users are beyond the capacity
of any machine learning method. It is possible that attackers
could identify such “behavior regions”, but there is a cost
for attackers to implement such behaviors (e.g., bots have to
send requests slowly to mimic benign users). Regarding the
“abnormal” benign users that are misclassiﬁed, we can han-
dle them via CAPTCHAs. After several successfully-solved
CAPTCHAs, we can add them back to the training data to
expand ODDS’s knowledge about benign users.
G. Adversarial Examples and Adversarial Retraining
Q7: Can ODDS beneﬁt from adversarial re-training?
While our goal is different from adversarial re-training,
we want to explore if ODDS can be further improved by
adversarial re-training. More speciﬁcally, we use a popular
method proposed by Carlini and Wagner [50] to generate
adversarial examples, and then use them to retrain LSTM and
ODDS. We examine if the re-trained model performs better
on the original testing sets and the adversarial examples.
We use the August-18 dataset from B, and sample 1%
of the training data to train LSTM and ODDS. To generate
adversarial examples, we simulate a blackbox attack: we use
the same 1% training data to train a CNN model which
acts as a surrogate model to generate adversarial examples
(Carlini and Wagner’s attack is designed for CNN). Given
the transferability of adversarial examples [61], we expect
the attack should work on other deep neural networks trained
on this dataset. We use the L2 attack to generate adversarial
examples only for the bot data to simulate evasion. The
adversarial perturbations are applied to the input feature space,
i.e., after feature engineering. We generate 600 adversarial
examples based on the same 1% bot training samples with
different noise levels (number of iterations is 500–1000, learn-
ing rate is 0.005, conﬁdence is set to 0–0.2). We use half of
the adversarial samples (300) for adversarial retraining, i.e.,
adding adversarial examples back to the training data to retrain
LSTM and ODDS. We use the remaining adversarial examples
for testing (300).
Table XIII shows the results. Without adversarial re-training,
LSTM is vulnerable to the adversarial attack. The testing
accuracy on adversarial examples is only 0.148, which means
85.2% of the adversarial examples are misclassiﬁed as benign.
Interestingly, we ﬁnd that ODDS is already quite resilient
to the blackbox adversarial examples with a testing accuracy
of 0.970. After applying adversarial-retraining, both LSTM
and ODDS perform better on the adversarial examples, which
is expected. In addition, adversarial-retraining also leads to
better performance on the original testing set (the last two
weeks of August-18) for both LSTM and ODDS. Even so,
LSTM with adversarial-retraining (0.720) is still not as good
as ODDS without adversarial retraining (0.783). The result
suggests that adversarial retraining and ODDS both help to
improve the model’s generalizability on unseen bot distribu-
tions, but in different ways. There is a beneﬁt to apply both
to improve the model training.
Note that
the above results do not necessarily mean
ODDS is completely immune to all adversarial attacks. As
a quick test, we run a whitebox attack assuming the attackers
know both the training data and the model parameters. By
adapting the Carlini and Wagner attack [50] for LSTM and
ODDS’s discriminator, we directly generate 600 adversarial
examples to attack the respective model. For our discriminator,
adversarial perturbations are applied in the latent space, i.e., on
the output of the autoencoder. Not too surprisingly, whitebox
attack is more effective. For LSTM, the testing accuracy of
adversarial examples drops from 0.148 to 0. For ODDS’s
discriminator, the testing accuracy of adversarial examples
drops from 0.970 to 0.398.
To realize the attack in practice, however, there are other
challenges. For example, the attacker will need to determine
the perturbations on the real-world network traces, and not just
in the feature space. This is a challenging task because the data
is sequential (discrete inputs) where each data point is multi-
dimensional (e.g., covering various metadata associated with
a HTTP request). In addition, bot detection solution providers
usually keep their model details conﬁdential, and deploy their
models in the cloud without exposing a public API for direct
queries. These are non-trivial problems and we leave further
explorations to future work.
VII. DISCUSSION
Rules vs. Machine Learning Model. We argue that rule-
based system should be the ﬁrst choice over machine learning
for bot detection. Compared with machine learning models,
rules do not need training, and can provide a precise reason
for the detection decision (i.e., interpretable). Machine learning
model is useful to capture more complex behaviors that cannot
be accurately expressed by rules. In this work, we apply
machine learning (ODDS) to detect bots that have bypassed
the rules. In this way, the rules can afford to be extremely
conservative (i.e., highly precise but has a low recall).
Implications for the CAPTCHA System. ODDS could also
allow the CAPTCHA system to be less aggressive, especially
on benign users. We still recommend delivering CAPTCHAs
to bots ﬂagged by rules or ODDS since there is no cost (on
users’ expense) for delivering CAPTCHAs to true bots. The
only cost is the small number of false positives produced by
ODDS, i.e., benign users who need to solve a CAPTCHA. As
shown in Table VIII, the false positive is small (e.g., 1-2% of
benign users’ requests). By guiding the CAPTCHA delivery
to the likely-malicious users, ODDS allows the defender to
avoid massively delivering CAPTCHAs to real users.
Adversarial Evasion and Poisoning.
ODDS improves
model training with limited data. However, it does not mean
attackers cannot evade ODDS by changing their behaviors. In
fact, in Section VI-F, we already show that ODDS could not
detect bots that are deeply embedded in the benign region (e.g.,
whose nearest neighbors are 100% benign). In Section VI-G,
we show that attackers in theory could identify adversarial
examples in the whitebox setting. That said, we argue that bot
detection is not a typical adversarial machine learning problem
because the “small changes” deﬁned by the distance function
in the feature space do not necessarily reﬂect the real-world
costs to attackers [62]. For example, a simple way of evasion
might be editing the “time-gap” feature, but it requires the
attacker to dramatically slow down their request sending rate.
We leave “cost-aware” adversarial evasion to future work.
to inﬂuence the model
Another potential attack against ODDS is poisoning attack.
In theory, adversaries may also inject mislabeled data to the
training set
training. The practical
challenge, however, is to get the injected data to be considered
as part of the training data, which has a cost. For example, to
inject bot data point with a “benign” label, attackers will need
to pay human labors to actually solve CAPTCHAs. We leave
the further study of this attack to future work.
Limitations. Our study has a few limitations. First, while
ODDS is designed to be generic, we haven’t tested it beyond
bot detection applications. Our method relies on the assump-
tion that benign data is relatively stable and representative. As
future work, we plan to test the system on other applications,
and explore new designs when the benign set is also highly dy-
namic (e.g. website updates may cause benign users changing
behaviors). Second, while our “ground-truth” already repre-
sents a best-effort, it is still possible to have a small number
of wrong labels. For example, in the benign set, there could be
true bots that use crowdsourcing services to solve CAPTCHAs
or bots that never received CAPTCHAs before. Third, due
to limited data (three disconnected months), we could not
fully evaluate the impact of the sliding window and model
retraining over a continuous time space. Fourth, we make
detection decisions on IP-sequences. In practice, it is possible
that multiple users may use the same IP behind NAT/proxy.
If a user chooses to use a proxy that is heavily used by
attackers, we argue that it’s reasonable for the user to receive
some CAPTCHAs as a consequence. Finally, ODDS needs
to be retrained from scratch when new bot examples become
available. A future direction of improvement is to perform