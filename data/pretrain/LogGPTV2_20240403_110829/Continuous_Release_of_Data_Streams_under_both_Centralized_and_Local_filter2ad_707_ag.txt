bl − 1

Equation 12 is because of the consistency of T so that T(x) =
y∈chd(x) T(y). This gives the same result as if we run the off-line
consistency algorithm. Similarly, during the top-down update step
(in Equation 11), we have the updated N(x) equals to
N(y) + T(x)(cid:170)(cid:174)(cid:172)


(N(y) + T(y)) (12)
y∈chd(x)
H(y).
y∈chd(x)
y∈chd(x)
(cid:169)(cid:173)(cid:171)
(cid:169)(cid:173)(cid:171)H(prt(x)) − 
y∈sbl(x)
H(y)(cid:170)(cid:174)(cid:172) .
(13)
that T(x) = T(prt(x)) −
which is the same result as if we run the off-line consistency algo-
rithm. Equation 13 also holds because of the consistency of T so
□
y∈sbl(x) T(y).
D SMOOTHING METHODS
Denote u1, u2, . . . as the noisy estimates given by the leaves of the
perturber (or the (s + 1)-th levels of the original hierarchy). Each ui
is the noisy sum of bs values. Let u0 = 1
2bsθ (initially there are no
estimations from the hierarchy; we thus use half of the threshold
as mean). The smoother will take the sequence of u and output
the final result ˜vi for each input value. Let t = ⌈i/bs⌉, we consider
several functions:
(1) Recent smoother: ˜vi = ut/bs. It takes the mean of the most
(2) Mean smoother: ˜vi = 1
j=0 uj. It takes the mean of the
bs t
(3) Median smoother: ˜vi = median(u1, . . . , ut). Similar to the mean
smoother, the median smoother takes the median of the output
from the perturber up until the moment.
output from the perturber up until the moment.
recent output from the perturber.
(4) Moving average smoother: ˜vi = 1
bs w
j=t +1−w uj. Similar to
the mean smoother, it takes the mean over the most recent w
outputs from the perturber. When t +1  0, where 0 ≤ α ≤ 1 is the smoothing parameter.
The exponential smoother put more weight on the more recent
values from the hierarchy.
bs if t = 0, and ˜vi = α ut
(5) Exponential smoother: ˜vi = u0
t
t
E MECHANISMS OF LOCAL DIFFERENTIAL
PRIVACY
In this subsection, we review the primitives proposed for LDP. We
use v to denote the user’s private value, and y as the user’s report
that satisfies LDP. In this section, following the notations in the
LDP literature, we use p and q to denote probabilities.
E.1 Square Wave Mechanism for Density
Estimation
Li et al. [33] propose an LDP method that can give the full density
estimation. The intuition behind this approach is to try to increase
the probability that a noisy reported value carries meaningful infor-
mation about the input. Intuitively, if the reported value is the true
value, then the report is a “useful signal”, as it conveys the extract
correct information about the true input. If the reported value is
not the true value, the report is in some sense noise that needs to
be removed. Exploiting the ordinal nature of the domain, a report
that is different from but close to the true value v also carries useful
information about the distribution. Therefore, given input v, we
can report values closer to v with a higher probability than values
that are farther away from v. The reporting probability looks like a
squared wave, so the authors call the method Square Wave method
(SW for short).
Adding T(x) to it, we have the updated H(x) equals to
b − 1
b
N(x) +
b − 1
b
N(x) +
1
b
=b − 1
b
(N(x) + T(x)) +
1
b
y∈sbl(x)
(cid:169)(cid:173)(cid:171)N(prt(x)) − 
N(y)(cid:170)(cid:174)(cid:172) .
(cid:169)(cid:173)(cid:171)N(prt(x)) − 
N(y)(cid:170)(cid:174)(cid:172) + T(x)
(cid:169)(cid:173)(cid:171)N(prt(x)) − 
y∈sbl(x)
y∈sbl(x)
1
b
N(y) + T(x)(cid:170)(cid:174)(cid:172)
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1251(cid:26) p,
Without loss of generality, we assume values are in the domain of
[0, 1]. To handle an arbitrary range [ℓ, r], each user first transforms
v into v−ℓ
(mapping [ℓ, r] to [0, 1]); and the estimated result is
r−ℓ
transformed back. Define the “closeness” measure b = ϵe ϵ−e ϵ +1
2e ϵ(e ϵ−1−ϵ),
the Square Wave mechanism SW is defined as:
1
q,
if |v − y| ≤ b ,
otherwise .
the total probability adds up to 1, we can derive p =
q =
∀y ∈ [−b, 1 + b], Pr[SW(v) = y] =
By maximizing the difference between p and q while satisfying
2be ϵ +1 and
e ϵ
2be ϵ +1.
After receiving perturbed reports from all users, the server runs
the Expectation Maximization algorithm to find an estimated den-
sity distribution that maximizes the expectation of observing the
output. Additionally, the server applies a special smoothing re-
quirement to the Expectation Maximization algorithm to avoid
overfitting.
(cid:17)2 − v
w/p p, and y (cid:44) v′ w/p q. The method has variance(cid:16) e ϵ +1
E.2 Candidate Methods for the Perturber
As we are essentially interested in estimating the sum over time,
the following methods that estimate the mean within a population
are useful. We first describe two basic methods. Then we describe
a method that adaptively uses these two to get better accuracy in
all cases. Our perturber will use the final method.
Stochastic Rounding. This method uses stochastic rounding to
estimates the mean of a continuous/ordinal domain [16]. We call it
Stochastic Rounding (SR for short). Assume the private input value
v is in the range of [−1, 1] (otherwise, we can first projected the
domain into [−1, 1]), the main idea is to round v to v′ so that v′ = 1
2 + v2 and v′ = −1 w/p 1−p1. This stochastic
with probability p1 = 1
rounding step is unbiased in that E[ v′ ] = v. Then given a value
v′ ∈ {−1, 1}, the method runs binary random response to perturb
e ϵ +1, y = v′
v′ into y. In particular, let p = e ϵ
e ϵ +1 and q = 1 − p = 1
2.
e ϵ−1
Piecewise Mechanism. Wang et al. [41] proposed piecewise
mechanism. It is also used for mean estimation, but can get more
accurate mean estimation than SR when ϵ > 1.29. In this method,
the input domain is [−1, 1], and the output domain is [−s, s], where
s = e ϵ/2+1
e ϵ/2−1. For each v ∈ [−1, 1], there is an associated range
[ℓ(v), r(v)] where ℓ(v) = e ϵ/2·v−1
e ϵ/2−1 , such that
with input v, a value in the range [ℓ(v), r(v)] will be reported
with higher probability than a value outside the range. The high-
probability range looks like a “piece” above the true value, so the
authors call the method Piecewise Mechanism (PM for short). The
perturbation function is defined as
∀y∈[−s,s] Pr[PM(v) = y] =
where z = e ϵ/2−1
e ϵ/2−1 + e ϵ/2+3
v2
Hybrid Mechanism. Both SR and PM incurs a variance that de-
pends on the true value, but in the opposite direction. In particular,
when v = ±1, the variance of SR is lowest, but the variance of PM
e ϵ/2+1. Compared to SR, this method has a variance of
3(e ϵ/2−1)2 [41].
e ϵ/2−1 and r(v) = e ϵ/2·v +1
if y ∈ [ℓ(v), r(v)]
otherwise.
p = e ϵ/2
2 z,
q = 1
2e ϵ/2 z,
(cid:40)
Moreover, we also show results for the normalized results, or the
mean of range queries, namely,
MAE(Q) =
1
|Q|
MMSE(Q) =
1
|Q|
MMAE(Q) =
(i, j)∈Q



(cid:12)(cid:12) ˜V(i, j) − V(i, j)(cid:12)(cid:12).
(cid:21)2
(cid:20) ˜V(i, j)
− V(i, j)
(cid:12)(cid:12)(cid:12)(cid:12) ˜V(i, j)
(cid:12)(cid:12)(cid:12)(cid:12).
j − i
− V(i, j)
j − i
j − i
j − i
,
(i, j)∈Q
1
|Q|
(i, j)∈Q
is highest. Wang et al. [41] thus propose a method called Hybrid
Mechanism (HM for short) to achieve good accuracy for any v. In
particular, define α = 1 − e−ϵ/2, when ϵ > 0.61, users use PM w/p
α and SR w/p 1 − α. When ϵ ≤ 0.61, only SR will be called. It is
proved by Wang et al. [41] HM gives better accuracy than SR and
PM. In particular, the worst-case variance is
(cid:17)2
(cid:16) e ϵ +1
(cid:20)(cid:16) e ϵ +1
e ϵ−1
1
e ϵ/2
e ϵ−1
,
(cid:17)2
Var[ ˜v] =
(cid:21)
+ e ϵ/2+3
3(e ϵ/2−1)
when ϵ ≤ 0.61
, when ϵ > 0.61.
(14)
F MORE RESULTS
Denote Q as the set of the 200 randomly generated queries, we
show the results of Mean Absolute Error (MAE):
Figure 10 gives results on these metrics. Let us first look at the
first row, which gives MAE results. The overall trend is similar to
that of MSE (Figure 3). One notable difference is that the better hi-
erarchical method does not give a better overall result (ToPS versus
NM-E and ˆHc16 versus PAK). This is because the better hierarchical
method (especially the consistency step) is optimizing the squared
error. We note that Lee et al. [31] proposed methods for optimizing
absolute error (L1 error), and they can be used in our setting if the
target is to minimize absolute errors.
For the second and third row of Figure 10, which corresponds
to MMAE and MMSE, respectively, we can see the overall trend
and the relative performance of different methods are similar to
the case of MAE and MSE. The results are less stable though: this
is because the range of the query here introduces another factor of
randomness. That is, due to the usage of the hierarchy, the larger
the range, the smaller the error will be. For the MMAE metric,
the results of our proposed ToPS can be as small as 1, while the
existing work of PAK [36] gives errors of 103 to 104, depending
on the dataset. Comparing with the flat method, where a Laplace
noise on the order of B/ϵ (where B is the upper bound of the data)
or θ/ϵ (if some technique of finding θ is applied) is added to each
count, ToPS significantly improves over it. Finally, the results of
MMSE show similar trends as that of MSE. These evaluation results
further confirm the superiority of ToPS regardless of the evaluation
metrics.
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1252(a) Fare, MAE
(b) DNS, MAE
(c) Kosarak, MAE
(d) POS, MAE
(e) Fare, MMAE
(f) DNS, MMAE
(g) Kosarak, MMAE
(h) POS, MMAE
(i) Fare, MMSE
(j) DNS, MMSE
(k) Kosarak, MMSE
(l) POS, MMSE
Figure 10: Comparison of different methods on MAE (first row), MMAE (second row) and MMSE (third row).
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1253