# 第29讲 \| 容器网络：来去自由的日子，不买公寓去合租如果说虚拟机是买公寓，容器则相当于合租，有一定的隔离，但是隔离性没有那么好。云计算解决了基础资源层的弹性伸缩，却没有解决PaaS层应用随基础资源层弹性伸缩而带来的批量、快速部署问题。于是，容器应运而生。容器就是 Container，而 Container的另一个意思是集装箱。其实**容器的思想就是要变成软件交付的集装箱**。集装箱的特点，一是打包，二是标准。![](Images/213f355bf1dfb5ade48617a35e2a57a2.png){savepage-src="https://static001.geekbang.org/resource/image/a5/dc/a50157f1084c946b9e27f3b328b8d2dc.jpg"}在没有集装箱的时代，假设要将货物从 A 运到B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，弄的乱七八糟，然后还要再搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能干完活。有了尺寸全部都一样的集装箱以后，可以把所有的货物都打包在一起，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用耗费很长时间了。这是集装箱的"打包""标准"两大特点在生活中的应用。![](Images/09b6e5440d3dee48028128730dbb7c7e.png){savepage-src="https://static001.geekbang.org/resource/image/50/cb/50c12f33ec178972c315e57b370dffcb.jpg"}那么容器如何对应用打包呢？学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。封闭的环境主要使用了两种技术，一种是**看起来是隔离的技术**，称为**namespace**，也即每个namespace 中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是**用起来是隔离的技术**，称为**cgroup**，也即明明整台机器有很多的CPU、内存，而一个应用只能用其中的一部分。``{=html}有了这两项技术，就相当于我们焊好了集装箱。接下来的问题就是如何"将这个集装箱标准化"，并在哪艘船上都能运输。这里的标准首先就是**镜像**。所谓镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说："定！"，集装箱里的状态就被定在了那一刻，然后将这一刻的状态保存成一系列文件。无论从哪里运行这个镜像，都能完整地还原当时的情况。![](Images/e0b701a977da15deb56c5fbae8936364.png){savepage-src="https://static001.geekbang.org/resource/image/ad/9d/ad59fa4271e5dd7b40588a2dfeb6f79d.jpg"}接下来我们就具体来看看，这两种网络方面的打包技术。
## 命名空间（namespace）我们首先来看网络 namespace。namespace翻译过来就是命名空间。其实很多面向对象的程序设计语言里面，都有命名空间这个东西。大家一起写代码，难免类会起相同的名词，编译就会冲突。而每个功能都有自己的命名空间，在不同的空间里面，类名相同，不会冲突。在 Linux 下也是这样的，很多的资源都是全局的。比如进程有全局的进程ID，网络也有全局的路由表。但是，当一台 Linux上跑多个进程的时候，如果我们觉得使用不同的路由策略，这些进程可能会冲突，那就需要将这个进程放在一个独立的namespace 里面，这样就可以独立配置网络了。网络的 namespace 由 ip netns 命令操作。它可以创建、删除、查询namespace。我们再来看将你们宿舍放进一台物理机的那个图。你们宿舍长的电脑是一台路由器，你现在应该知道怎么实现这个路由器吧？可以创建一个Router虚拟机来做这件事情，但是还有一个更加简单的办法，就是我在图里画的这条虚线，这个就是通过namespace 实现的。![](Images/7b13d7015230750334028a03e327d523.png){savepage-src="https://static001.geekbang.org/resource/image/1a/1a/1a5d299c2eb5480eda93a8f8e3b3ca1a.jpg"}我们创建一个routerns，于是一个独立的网络空间就产生了。你可以在里面尽情设置自己的规则。    ip netns add routerns既然是路由器，肯定要能转发嘛，因而 forward 开关要打开。    ip netns exec routerns sysctl -w net.ipv4.ip_forward=1exec 的意思就是进入这个网络空间做点事情。初始化一下iptables，因为这里面要配置 NAT 规则。    ip netns exec routerns iptables-save -c ip netns exec routerns iptables-restore -c路由器需要有一张网卡连到 br0 上，因而要创建一个网卡。    ovs-vsctl -- add-port br0 taprouter -- set Interface taprouter type=internal -- set Interface taprouter external-ids:iface-status=active -- set Interface taprouter external-ids:attached-mac=fa:16:3e:84:6e:cc这个网络创建完了，但是是在 namespace外面的，如何进去呢？可以通过这个命令：    ip link set taprouter netns routerns要给这个网卡配置一个 IP地址，当然应该是虚拟机网络的网关地址。例如虚拟机私网网段为192.168.1.0/24，网关的地址往往为 192.168.1.1。    ip netns exec routerns ip -4 addr add 192.168.1.1/24 brd 192.168.1.255 scope global dev taprouter为了访问外网，还需要另一个网卡连在外网网桥 br-ex 上，并且塞在 namespace里面。    ovs-vsctl -- add-port br-ex taprouterex -- set Interface taprouterex type=internal -- set Interface taprouterex external-ids:iface-status=active -- set Interface taprouterex external-ids:attached-mac=fa:16:3e:68:12:c0    ip link set taprouterex netns routerns我们还需要为这个网卡分配一个地址，这个地址应该和物理外网网络在一个网段。假设物理外网为16.158.1.0/24，可以分配一个外网地址 16.158.1.100/24。    ip netns exec routerns ip -4 addr add 16.158.1.100/24 brd 16.158.1.255 scope global dev taprouterex接下来，既然是路由器，就需要配置路由表，路由表是这样的：    ip netns exec routerns route -nKernel IP routing tableDestination   Gateway     Genmask     Flags Metric Ref  Use Iface0.0.0.0     16.158.1.1  0.0.0.0     UG  0   0    0 taprouterex192.168.1.0    0.0.0.0     255.255.255.0  U   0   0    0 taprouter16.158.1.0  0.0.0.0     255.255.255.0  U   0   0    0 taprouterex路由表中的默认路由是去物理外网的，去 192.168.1.0/24也即虚拟机私网，走下面的网卡，去 16.158.1.0/24也即物理外网，走上面的网卡。我们在前面的章节讲过，如果要在虚拟机里面提供服务，提供给外网的客户端访问，客户端需要访问外网IP3，会在外网网口 NAT 称为虚拟机私网 IP。这个 NAT 规则要在这个 namespace里面配置。    ip netns exec routerns iptables -t nat -nvLChain PREROUTINGtarget  prot opt  in  out  source  destinationDNAT  all  --  *  *  0.0.0.0/0 16.158.1.103 to:192.168.1.3Chain POSTROUTINGtarget  prot opt  in  out  source   destinationSNAT  all  --  *  *  192.168.1.3  0.0.0.0/0 to:16.158.1.103这里面有两个规则，一个是 SNAT，将虚拟机的私网 IP 192.168.1.3 NAT成物理外网 IP 16.158.1.103。一个是 DNAT，将物理外网 IP 16.158.1.103 NAT成虚拟机私网 IP 192.168.1.3。至此为止，基于网络 namespace 的路由器实现完毕。
## 机制网络（cgroup）我们再来看打包的另一个机制网络 cgroup。cgroup 全称 control groups，是 Linux内核提供的一种可以限制、隔离进程使用的资源机制。cgroup 能控制哪些资源呢？它有很多子系统：-   CPU 子系统使用调度程序为进程控制 CPU 的访问；-   cpuset，如果是多核心的 CPU，这个子系统会为进程分配单独的 CPU    和内存；-   memory 子系统，设置进程的内存限制以及产生内存资源报告；-   blkio 子系统，设置限制每个块设备的输入输出控制；-   net_cls，这个子系统使用等级识别符（classid）标记网络数据包，可允许    Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。我们这里最关心的是 net_cls，它可以和前面讲过的 TC 关联起来。cgroup提供了一个虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用cgroup，必须挂载 cgroup 文件系统，一般情况下都是挂载到 /sys/fs/cgroup目录下。所以首先我们要挂载一个 net_cls 的文件系统。    mkdir /sys/fs/cgroup/net_clsmount -t cgroup -onet_cls net_cls /sys/fs/cgroup/net_cls接下来我们要配置 TC 了。还记得咱们实验 TC 的时候那个树吗？![](Images/50b9ba5ca09b6cf161fc1379da0fc0eb.png){savepage-src="https://static001.geekbang.org/resource/image/9a/b5/9a1b8a7c0c5403a2b4b3c277545991b5.jpg"}当时我们通过这个命令设定了规则：从 1.2.3.4 来的，发送给 port 80 的包，从1:10 走；其他从 1.2.3.4 发送来的包从 1:11 走；其他的走默认。    tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11这里是根据源 IP 来设定的，现在有了 cgroup，我们按照 cgroup再来设定规则。    tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 1: cgroup假设我们有两个用户 a 和 b，要对它们进行带宽限制。首先，我们要创建两个 net_cls。    mkdir /sys/fs/cgroup/net_cls/a   mkdir /sys/fs/cgroup/net_cls/b假设用户 a 启动的进程 ID 为 12345，把它放在 net_cls/a/tasks文件中。同样假设用户 b 启动的进程 ID 为 12346，把它放在 net_cls/b/tasks文件中。net_cls/a 目录下面，还有一个文件 net_cls.classid，我们放 flowid1:10。net_cls/b 目录下面，也创建一个文件 net_cls.classid，我们放 flowid1:11。这个数字怎么放呢？要转换成一个 0xAAAABBBB 的值，AAAA 对应 class中冒号前面的数字，而 BBBB 对应后面的数字。    echo 0x00010010 > /sys/fs/cgroup/net_cls/a/net_cls.classid    echo 0x00010011 > /sys/fs/cgroup/net_cls/b/net_cls.classid这样用户 a 的进程发的包，会打上 1:10 这个标签；用户 b的进程发的包，会打上 1:11 这个标签。然后 TC 根据这两个标签，让用户 a的进程的包走左边的分支，用户 b 的进程的包走右边的分支。
## 容器网络中如何融入物理网络？了解了容器背后的技术，接下来我们来看，容器网络究竟是如何融入物理网络的？如果你使用 docker run 运行一个容器，你应该能看到这样一个拓扑结构。![](Images/08a349d616b8f9466b003acb31e8bf31.png){savepage-src="https://static001.geekbang.org/resource/image/20/d2/20e87bc215b9d049a4a504d775d26dd2.jpg"}是不是和虚拟机很像？容器里面有张网卡，容器外有张网卡，容器外的网卡连到docker0 网桥，通过这个网桥，容器直接实现相互访问。如果你用 brctl 查看 docker0网桥，你会发现它上面连着一些网卡。其实这个网桥和[第 24讲](https://time.geekbang.org/column/article/10742)，咱们自己用 brctl创建的网桥没什么两样。那连接容器和网桥的那个网卡和虚拟机一样吗？在虚拟机场景下，有一个虚拟化软件，通过TUN/TAP设备虚拟一个网卡给虚拟机，但是容器场景下并没有虚拟化软件，这该怎么办呢？在 Linux 下，可以创建一对 veth pair的网卡，从一边发送包，另一边就能收到。我们首先通过这个命令创建这么一对。    ip link add name veth1 mtu 1500 type veth peer name veth2 mtu 1500其中一边可以打到 docker0 网桥上。    ip link set veth1 master testbr    ip link set veth1 up那另一端如何放到容器里呢？一个容器的启动会对应一个 namespace，我们要先找到这个 namespace。对于docker 来讲，pid 就是 namespace 的名字，可以通过这个命令获取。    docker inspect '--format={{ .State.Pid }}' test假设结果为 12065，这个就是 namespace 名字。默认 Docker 创建的网络 namespace 不在默认路径下 ，ip netns看不到，所以需要 ln 软链接一下。链接完毕以后，我们就可以通过 ip netns命令操作了。    rm -f /var/run/netns/12065    ln -s /proc/12065/ns/net /var/run/netns/12065然后，我们就可以将另一端 veth2 塞到 namespace 里面。    ip link set veth2 netns 12065然后，将容器内的网卡重命名。    ip netns exec 12065 ip link set veth2 name eth0然后，给容器内网卡设置 ip 地址。    ip netns exec 12065 ip addr add 172.17.0.2/24 dev eth0    ip netns exec 12065 ip link set eth0 up一台机器内部容器的互相访问没有问题了，那如何访问外网呢？你先想想看有没有思路？对，就是虚拟机里面的桥接模式和 NAT 模式。Docker默认使用 NAT 模式。NAT 模式分为 SNAT 和DNAT，如果是容器内部访问外部，就需要通过 SNAT。从容器内部的客户端访问外部网络中的服务器，我画了一张图。在[虚拟机](https://time.geekbang.org/column/article/10742)那一节，也有一张类似的图。![](Images/c616512600a54464a02618e122ade510.png){savepage-src="https://static001.geekbang.org/resource/image/54/93/5452971c96e8fea33c3f873860e25c93.jpg"}在宿主机上，有这么一条 iptables 规则：    -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE所有从容器内部发出来的包，都要做地址伪装，将源 IP 地址，转换为物理网卡的IP 地址。如果有多个容器，所有的容器共享一个外网的 IP 地址，但是在conntrack 表中，记录下这个出去的连接。当服务器返回结果的时候，到达物理机，会根据 conntrack表中的规则，取出原来的私网 IP，通过 DNAT 将地址转换为私网 IP地址，通过网桥 docker0 实现对内的访问。如果在容器内部属于一个服务，例如部署一个网站，提供给外部进行访问，需要通过Docker 的端口映射技术，将容器内部的端口映射到物理机上来。例如容器内部监听 80 端口，可以通 Docker run 命令中的参数 -p10080:80，将物理机上的 10080 端口和容器的 80 端口映射起来，当外部的客户端访问这个网站的时候，通过访问物理机的 10080端口，就能访问到容器内的 80 端口了。![](Images/0998b4083d7ab8ffe0b7f99338b2e3b9.png){savepage-src="https://static001.geekbang.org/resource/image/49/bb/49bb6b2a30fe76b124182980da935ebb.jpg"}Docker 有两种方式，一种是通过一个进程**docker-proxy**的方式，监听10080，转换为 80 端口。    /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 10080 -container-ip 172.17.0.2 -container-port 80另外一种方式是通过**DNAT**方式，在 -A PREROUTING阶段加一个规则，将到端口 10080 的 DNAT 称为容器的私有网络。    -A DOCKER -p tcp -m tcp --dport 10080 -j DNAT --to-destination 172.17.0.2:80如此就可以实现容器和物理网络之间的互通了。
## 小结好了，这一节就到这里了，我们来总结一下。-   容器是一种比虚拟机更加轻量级的隔离方式，主要通过 namespace 和 cgroup    技术进行资源的隔离，namespace 用于负责看起来隔离，cgroup    用于负责用起来隔离。-   容器网络连接到物理网络的方式和虚拟机很像，通过桥接的方式实现一台物理机上的容器进行相互访问，如果要访问外网，最简单的方式还是通过    NAT。最后，给你留两个思考题：1.  容器内的网络和物理机网络可以使用 NAT    的方式相互访问，如果这种方式用于部署应用，有什么问题呢？2.  和虚拟机一样，不同物理机上的容器需要相互通信，你知道容器是怎么做到这一点吗？我们的专栏更新到第 29讲，不知你掌握得如何？每节课后我留的思考题，你都有没有认真思考，并在留言区写下答案呢？我会从**已发布的文章中选出一批认真留言的同学**，赠送[学习奖励礼券]{.orange}和我整理的[独家网络协议知识图谱]{.orange}。欢迎你留言和我讨论。趣谈网络协议，我们下期见！![](Images/55417b60e9c8040807daf07e6bd9cb4b.png){savepage-src="https://static001.geekbang.org/resource/image/b5/fb/b5bc14cb81d3630919fee94a512cc3fb.jpg"}
# 第30讲 \| 容器网络之Flannel：每人一亩三分地上一节我们讲了容器网络的模型，以及如何通过 NAT的方式与物理网络进行互通。每一台物理机上面安装好了 Docker 以后，都会默认分配一个 172.17.0.0/16的网段。一台机器上新创建的第一个容器，一般都会给 172.17.0.2这个地址，当然一台机器这样玩玩倒也没啥问题。但是容器里面是要部署应用的，就像上一节讲过的一样，它既然是集装箱，里面就需要装载货物。如果这个应用是比较传统的单体应用，自己就一个进程，所有的代码逻辑都在这个进程里面，上面的模式没有任何问题，只要通过NAT 就能访问进来。但是因为无法解决快速迭代和高并发的问题，单体应用越来越跟不上时代发展的需要了。你可以回想一下，无论是各种网络直播平台，还是共享单车，是不是都是很短时间内就要积累大量用户，否则就会错过风口。所以应用需要在很短的时间内快速迭代，不断调整，满足用户体验；还要在很短的时间内，具有支撑高并发请求的能力。单体应用作为个人英雄主义的时代已经过去了。如果所有的代码都在一个工程里面，开发的时候必然存在大量冲突，上线的时候，需要开大会进行协调，一个月上线一次就很不错了。而且所有的流量都让一个进程扛，怎么也扛不住啊！没办法，一个字：拆！拆开了，每个子模块独自变化，减少相互影响。拆开了，原来一个进程扛流量，现在多个进程一起扛。所以，微服务就是从个人英雄主义，变成集团军作战。``{=html}容器作为集装箱，可以保证应用在不同的环境中快速迁移，提高迭代的效率。但是如果要形成容器集团军，还需要一个集团军作战的调度平台，这就是Kubernetes。它可以灵活地将一个容器调度到任何一台机器上，并且当某个应用扛不住的时候，只要在Kubernetes上修改容器的副本数，一个应用马上就能变八个，而且都能提供服务。然而集团军作战有个重要的问题，就是通信。这里面包含两个问题，第一个是集团军的A 部队如何实时地知道 B部队的位置变化，第二个是两个部队之间如何相互通信。第一个问题位置变化，往往是通过一个称为注册中心的地方统一管理的，这个是应用自己做的。当一个应用启动的时候，将自己所在环境的IP地址和端口，注册到注册中心指挥部，这样其他的应用请求它的时候，到指挥部问一下它在哪里就好了。当某个应用发生了变化，例如一台机器挂了，容器要迁移到另一台机器，这个时候IP改变了，应用会重新注册，则其他的应用请求它的时候，还是能够从指挥部得到最新的位置。![](Images/c78c1e13c551fa0acf298c23f9bcd3c1.png){savepage-src="https://static001.geekbang.org/resource/image/06/65/06ba300a78aef37b9d190aba61c37865.jpg"}接下来是如何相互通信的问题。NAT这种模式，在多个主机的场景下，是存在很大问题的。在物理机 A 上的应用 A看到的 IP 地址是容器 A 的，是 172.17.0.2，在物理机 B 上的应用 B 看到的IP 地址是容器 B 的，不巧也是172.17.0.2，当它们都注册到注册中心的时候，注册中心就是这个图里这样子。![](Images/36ab4769b6f85fc3ea52eb4f04635f8d.png){savepage-src="https://static001.geekbang.org/resource/image/a2/f1/a2bd259417b173ee641d2d16a0da54f1.jpg"}这个时候，应用 A 要访问应用 B，当应用 A 从注册中心将应用 B 的 IP地址读出来的时候，就彻底困惑了，这不是自己访问自己吗？怎么解决这个问题呢？一种办法是不去注册容器内的 IP地址，而是注册所在物理机的 IP 地址，端口也要是物理机上映射的端口。![](Images/e1a49600878af7f17ecea7b88e544c6c.png){savepage-src="https://static001.geekbang.org/resource/image/df/19/dfb2d3b6ae5ce31280812b64442a7519.jpg"}这样存在的问题是，应用是在容器里面的，它怎么知道物理机上的 IP地址和端口呢？这明明是运维人员配置的，除非应用配合，读取容器平台的接口获得这个IP和端口。一方面，大部分分布式框架都是容器诞生之前就有了，它们不会适配这种场景；另一方面，让容器内的应用意识到容器外的环境，本来就是非常不好的设计。说好的集装箱，说好的随意迁移呢？难道要让集装箱内的货物意识到自己传的信息？而且本来Tomcat 都是监听 8080端口的，结果到了物理机上，就不能大家都用这个端口了，否则端口就冲突了，因而就需要随机分配端口，于是在注册中心就出现了各种各样奇怪的端口。无论是注册中心，还是调用方都会觉得很奇怪，而且不是默认的端口，很多情况下也容易出错。Kubernetes作为集团军作战管理平台，提出指导意见，说网络模型要变平，但是没说怎么实现。于是业界就涌现了大量的方案，Flannel就是其中之一。对于 IP 冲突的问题，如果每一个物理机都是网段172.17.0.0/16，肯定会冲突啊，但是这个网段实在太大了，一台物理机上根本启动不了这么多的容器，所以能不能每台物理机在这个大网段里面，抠出一个小的网段，每个物理机网段都不同，自己看好自己的一亩三分地，谁也不和谁冲突。例如物理机 A 是网段 172.17.8.0/24，物理机 B 是网段172.17.9.0/24，这样两台机器上启动的容器 IP 肯定不一样，而且就看 IP地址，我们就一下子识别出，这个容器是本机的，还是远程的，如果是远程的，也能从网段一下子就识别出它归哪台物理机管，太方便了。接下来的问题，就是**物理机 A 上的容器如何访问到物理机 B 上的容器呢？**你是不是想到了熟悉的场景？虚拟机也需要跨物理机互通，往往通过 Overlay的方式，容器是不是也可以这样做呢？**这里我要说 Flannel 使用 UDP 实现 Overlay 网络的方案。**![](Images/7dbddf59c0f13aeaecc66b7ac4d10442.png){savepage-src="https://static001.geekbang.org/resource/image/01/c8/01ee306698c7dd6207e80fea0a8238c8.jpg"}在物理机 A 上的容器 A 里面，能看到的容器的 IP 地址是172.17.8.2/24，里面设置了默认的路由规则 default via 172.17.8.1 deveth0。如果容器 A 要访问 172.17.9.2，就会发往这个默认的网关172.17.8.1。172.17.8.1 就是物理机上面 docker0 网桥的 IP地址，这台物理机上的所有容器都是连接到这个网桥的。在物理机上面，查看路由策略，会有这样一条 172.17.0.0/24 via 172.17.0.0dev flannel.1，也就是说发往 172.17.9.2 的网络包会被转发到 flannel.1这个网卡。这个网卡是怎么出来的呢？在每台物理机上，都会跑一个 flanneld进程，这个进程打开一个 /dev/net/tun 字符设备的时候，就出现了这个网卡。你有没有想起qemu-kvm，打开这个字符设备的时候，物理机上也会出现一个网卡，所有发到这个网卡上的网络包会被qemu-kvm 接收进来，变成二进制串。只不过接下来 qemu-kvm会模拟一个虚拟机里面的网卡，将二进制的串变成网络包，发给虚拟机里面的网卡。但是flanneld 不用这样做，所有发到 flannel.1 这个网卡的包都会被 flanneld进程读进去，接下来 flanneld 要对网络包进行处理。物理机 A 上的 flanneld 会将网络包封装在 UDP 包里面，然后外层加上物理机 A和物理机 B 的 IP 地址，发送给物理机 B 上的 flanneld。为什么是 UDP 呢？因为不想在 flanneld 之间建立两两连接，而 UDP没有连接的概念，任何一台机器都能发给另一台。物理机 B 上的 flanneld 收到包之后，解开 UDP的包，将里面的网络包拿出来，从物理机 B 的 flannel.1 网卡发出去。在物理机 B 上，有路由规则 172.17.9.0/24 dev docker0 proto kernel scopelink src 172.17.9.1。将包发给 docker0，docker0 将包转给容器 B。通信成功。上面的过程连通性没有问题，但是由于全部在用户态，所以性能差了一些。跨物理机的连通性问题，在虚拟机那里有成熟的方案，就是 VXLAN，那**能不能Flannel 也用 VXLAN 呢**？当然可以了。如果使用 VXLAN，就不需要打开一个 TUN 设备了，而是要建立一个VXLAN 的 VTEP。如何建立呢？可以通过 netlink 通知内核建立一个 VTEP 的网卡flannel.1。在我们讲 OpenvSwitch 的时候提过，netlink是一种用户态和内核态通信的机制。当网络包从物理机 A 上的容器 A 发送给物理机 B 上的容器 B，在容器 A里面通过默认路由到达物理机 A 上的 docker0网卡，然后根据路由规则，在物理机 A 上，将包转发给 flannel.1。这个时候flannel.1 就是一个 VXLAN 的 VTEP 了，它将网络包进行封装。内部的 MAC 地址这样写：源为物理机 A 的 flannel.1 的 MAC地址，目标为物理机 B 的 flannel.1 的 MAC 地址，在外面加上 VXLAN 的头。外层的 IP 地址这样写：源为物理机 A 的 IP 地址，目标为物理机 B 的 IP地址，外面加上物理机的 MAC 地址。这样就能通过 VXLAN 将包转发到另一台机器，从物理机 B 的 flannel.1上解包，变成内部的网络包，通过物理机 B 上的路由转发到docker0，然后转发到容器 B 里面。通信成功。![](Images/4f756f55907da5f3a0e623f6e5d2cdc1.png){savepage-src="https://static001.geekbang.org/resource/image/a5/01/a568cb08c615b351e871bd981541a201.jpg"}