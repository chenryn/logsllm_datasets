n
e
m
e
r
i
u
q
e
r
r
e
f
f
u
B
 200000
 150000
 100000
 50000
 0
 0
 250000
 200000
 150000
 100000
 50000
)
s
e
t
y
b
(
t
n
e
m
e
r
i
u
q
e
r
r
e
f
f
u
B
 1000
 2000
 3000
 4000
 5000
 6000
Time(seconds)
(d) Univ19
 0
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 4500
 5000
 0
 0
 2000
 4000
 6000
 8000
 10000
 12000
Time (seconds)
(e) Munich
Time (seconds)
(f) T3
Figure 1: Reassembly buffer occupancy due to unﬁlled holes. Univsub, which we omitted, is similar to the elements of
Univ19.
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
l
s
e
o
h
f
o
n
o
i
t
c
a
r
F
 0
 1
 10
Super
Univ_sub
Univ_19
Lab_2
Lab_lo
T3
Munich
 100000
 1e+06
 100
Buffer accumulated by a hole (bytes)
 1000
 10000
Figure 3: Cumulative distribution of the buffer accumulated
by a hole.
shows the cumulative distribution of the duration of holes.
Most holes have a very short lifetime, strongly sugges-
tive that they are created due to packet reordering and not
packet loss, as in the latter case the hole will persist for at
least an RTT, signiﬁcantly longer than a millisecond for
non-local connections. The average hole duration is less
than a millisecond. In addition, the short-lived holes have
a strong bias towards the out-of-order packet (sent later,
arriving earlier) being smaller than its later-arriving pre-
decessor, which is suggestive of reordering due to multi-
pathing.
Finally, in Figure 3 we plot the cumulative distribution
of the size of the buffer associated with a single hole. The
graph shows that nearly all holes require less than 10 KB
of buffer. This plot thus argues that we can choose an ap-
propriate limit on the buffer-per-hole so that we can iden-
tify an adversary trying to claim an excessively large por-
tion.
4 System architecture
Since our reassembly module is an in-line element, one of
its key properties is the capability to transform the packet
stream if needed, including dropping packets or killing
connections (by sending TCP RST packets to both sides
and discarding the corresponding state). This ability al-
lows the system to make intelligent choices for more ro-
bust performance. TCP streams semantically allow nearly
arbitrary permutations of sequence hole creation (illus-
trated in Figure 4 below).
In particular, all of the fol-
lowing possible scenarios might in principle occur in a
TCP stream: very long-lived holes; holes that accumulate
large amounts of buffer; large numbers of simultaneous
holes in a connection; presence of simultaneous holes in
both directions of a single connection; and/or a high rate
of sequence hole creation. However, as our trace analysis
shows, most of these cases are highly rare in typical TCP
trafﬁc.
On the one hand, we have an objective to preserve
70
14th USENIX Security Symposium
USENIX Association
end-to-end TCP semantics as much as possible. On the
other hand, we have limited hardware resources in terms
of memory and computation. Hence, we adopt the well-
known principle of “optimize for the common case, de-
sign for the worst case,” i.e., the system should be efﬁ-
cient in handling commonly-seen cases of reordering, and
should not catastrophically fail when faced with a worst-
case scenario, but exhibit graceful degradation. Since the
traces highlight that the highly dominant case is that of
a single, short-lived hole in just one direction within a
connection, we design the system to handle this case ef-
ﬁciently. We then also leverage its capability of dropping
packets in order to restrict the occurrence of uncommon
cases, saving us from the complexity of having to accom-
modate these.
With this approach, most of the TCP trafﬁc passes un-
altered, while a very small portion experiences a higher
packet loss rate than it otherwise would. Note that this
latter trafﬁc is likely already suffering from impaired per-
formance due to TCP’s congestion-control response in the
presence of packet loss, since multiple concurrent holes
are generally due to loss rather than reordering. We fur-
ther note that dropping packets is much more benign than
terminating connections that exhibit uncommon behavior,
since the connection will still proceed by retransmitting
the dropped packet.
The reader might wonder: Why not drop packets when
the ﬁrst hole is created? Why design a system that bothers
buffering data at all? The simple answer: the occurrence
of a single connection hole is very common, much more
so than multiple holes of any form, and we would like to
avoid the performance degradation of a packet drop in this
case.
4.1 Maintaining Connection Records
Our system needs to maintain TCP connection records
for thousands of simultaneous connections, and must ac-
cess these at high speeds. For such a high-speed and
high-density storage, commodity synchronous DRAM
(SDRAM) chip is the only appropriate choice. Today,
Dual Data Rate SDRAM modules operating at 166 MHz
and with a capacity of 512 MB are available commer-
cially [15]. With a 64-bit wide data bus, such an SDRAM
module offers a raw data throughput of 64 × 2 × 166 ×
106 ≈ 21 Gbps. However, due to high access latency, the
actual throughput realized in practice is generally much
less. Nevertheless, we can design memory controllers to
exploit bank-level parallelism in order to hide the access
latency and achieve good performance [19].
When dimensioning connection records, we want to try
to ﬁt them into multiples of four SDRAM words, since
modern SDRAMs are well suited for burst access with
such multiples. With this practical consideration, we de-
sign the following connection record. First, in the absence
of any sequence hole in the stream, the minimum informa-
tion we need in the connection record is:
• CA, SA: client / server address (4 bytes + 4 bytes)
• CP, SP: client / server port (2 bytes + 2 bytes)
• Cseq: client’s expected sequence number (4 bytes)
• Sseq: server’s expected sequence number (4 bytes)
• Next: pointer to the next connection record for re-
• Est: whether the connection has been established,
i.e., we’ve seen both the initial SYN and a SYN-
ACK (1 bit). This bit also helps us in identifying
SYN ﬂoods.
solving hash collisions (23 bits)
Here, we allocate 23 bits to store the pointer to the
next connection record, assuming that the total number
of records does not exceed 8M. When a single sequence
hole is present in a connection, we need to maintain the
following extra information:
• CSH: Client hole or server hole (1 bit)
• HS: hole size (2 bytes)
• BS: buffer size (2 bytes)
• Bh, Bt: pointer to buffer head / tail (2 bytes + 2 bytes)
• PC: IP Packet count in the buffer (7 bits)
The ﬂag CSH indicates whether the hole corresponds to
the client-to-server stream or the server-to-client stream.
Hole size tells us how many bytes are missing, starting
from the expected sequence number of the client or the
server. Buffer size tells how many bytes we have buffered
up, starting from the end of the hole. Here we assume
that both the hole size and the buffer size do not exceed
64 KB. We drop packets that would cause these thresh-
olds to be exceeded, a tolerable performance degradation
as such packets are extremely rare. Finally, Bh and Bt are
the pointers to the head and tail of the associated buffer.
We access the buffer at a coarse granularity of a “page”
instead of byte. Hence, the pointers Bh and Bt point to
pages. With two bytes allocated to Bh and Bt, the number
of pages in the buffer must not exceed 64K. We can com-
pactly arrange the ﬁelds mentioned above in four 8-byte
SDRAM words.
We keep all connection records in a hash table for ef-
ﬁcient access. Upon receiving a TCP packet, we com-
pute a hash value over its 4-tuple (source address, source
port, destination address, destination port). Note that the
hash value needs to be independent of the permutation
of source and destination pairs. Using this hash value as
the address in the hash table, we locate the corresponding
connection. We resolve hash collisions by chaining the
colliding records in a linked list. A question arises here
regarding possibly having to traverse large hash chains.
Recall that by using a 512 MB SDRAM, we have space
USENIX Association
14th USENIX Security Symposium
71
to maintain 16M connection records (32 bytes each).
However, the number of concurrent connections is much
smaller (indeed, Table 1 shows that only T3 exceeded 1M
connections total, over its entire 3-hour span). Thus, the
connection record hash table will be very sparsely popu-
lated, greatly reducing the probability of hash collisions.1
Even with an assumption of 1M concurrent connections,
theoretically the memory accesses required for a success-
ful search will be T = 1+(1M−1)/(2×16M) ≈ 1.03 [8].
The following pseudo-code summarizes the algorithm
for accessing connection records:
1. P = ReceivePacket()
2. h = (P.SA, P.SP, P.DA, P.DP)
3. {CPtr, C} = LocateConn(h)
4. if (C is valid)
5.
6. else if (P.Syn and ! P.Ack) then
7.
8.
9.
10. else
11.
C=CreateConn(h, P.Cseq)
InsertConn(C, CPtr)
Forward(P)
UpdateConn(CPtr, C, P)
DropPacket(P)
LocateConn() locates the connection entry in the
hash table using the header values and returns the
{record pointer, record} pair (C indicates the actual con-
nection record and CPtr indicates a pointer to this record).
If a record was not found and if the packet is a SYN packet
then we create a record for the connection, otherwise we
drop the packet. If we ﬁnd the record then we update the
record ﬁelds after processing the packet. We describe Up-
dateConn() in the next section.
4.2 Reordering Packets
TCP’s general tolerance for out-of-order datagram deliv-
ery allows for numerous types of sequence hole creation
and plugging. Figure 4 illustrates the possibilities. In this
ﬁgure, a line shows a packet, and the absence of a line
indicates a missing packet or a hole. As shown, a stream
can have a single hole or multiple simultaneous holes (we
consider only one direction). An arriving packet can plug
one of the holes completely or partially. When it closes
it partially, it can do so from the beginning or from the
end or in the middle. In all such cases, the existing hole is
narrowed, and in the last case a new hole is also created.
Moreover, a packet can also close multiple simultaneous
holes and overlap with existing packet sequence numbers.
In order to interpret the packet content consistently,
whenever packet data overlaps with already-received data,
we must ﬁrst normalize the packet, as discussed in the In-
troduction. In the case of packet overlap, a simple normal-
ization approach is to discard the overlapping data from
the new packet. Thus, cases (A) to (F), cases (J) to (K),
and cases (O) to (R) all require normalization. In cases
(F), (K) and (P) (which, actually, were never seen in our
trace analysis), the arriving packet provides multiple valid
New packet
Buffered packet
Sequence hole
Already received and
forwarded data
(A)
(B)
(C)
(D)
(E)
(F)
(G)
(H)
(I)
(J)
(K)
(L)
(M)
(N)
(O)
(P)
(Q)
(R)
(S)
(T)
Increasing Sequence Number
Increasing Sequence Number
Figure 4: Various possibilities of sequence hole creation and
plugging.
segments after normalization.
In these cases, we retain
only the ﬁrst valid segment that plugs the hole (partially
or completely) and discard the second. It is easy to see
that once a packet is normalized, the only cases left to
deal with are (G,H,I,L,M,N,S,T).
A key question at this point is whether to forward out-
of-sequence packets as they arrive, or hold on to them
until they become in-sequence. Buffering packets with-
out forwarding them to the end hosts can affect TCP
dynamics signiﬁcantly. For instance, if a packet is lost
and a hole is created, then buffering all the packets fol-
lowing the missing packet and not forwarding them will
prevent the receiver from sending duplicate-ACKs to the
sender, foiling TCP fast retransmission and degrading per-
formance signiﬁcantly. Hence, for our initial design we
choose to always forward packets, whether in-order or
out-of-order (delivering in-order packets immediately to
the byte-stream analyzer, and retaining copies of out-of-
order packets for later delivery). We revisit this design
choice below.
When a new packet plugs a sequence hole from the be-
ginning then the packet can be immediately inspected and
forwarded.
If it closes a hole completely then we can
now pass along all the buffered packets associated with
the hole for byte-stream analysis, and reclaim the associ-
ated memory. (Note that we do not reinject these packets
into the network since they were already forwarded to the
end host.)
We should note that it is possible for a hole to be cre-
ated due to a missing packet, however the correspond-
72
14th USENIX Security Symposium
USENIX Association
ing packet reaches the destination through another route.
In this case, although the missing packet will never ar-
rive at the reassembly system, the acknowledgment for
that packet (or for packets following the missing packet)
can be seen going in the opposite direction. Hence, if
such an acknowledgment is seen, we immediately close
the hole. (See [17] for a related discussion on retaining
copies of trafﬁc after analyzing them until observing the
corresponding acknowledgments.) If the Ack number ac-
knowledges just a portion of the missing data then we nar-
row the hole rather than close it. In any case, the released
packets will remain uninspected since the data stream is
not complete enough to soundly analyze it.
We can summarize the discussion above in the follow-
ing pseudocode. For clarity, we write some of the condi-
tional statements to refer to the corresponding cases in the
Figure 4.
if (hole in other direction) then
if (P.Ack > C.Seq) then
if (hole closed completely) then
FreeBuffer(C.Bh-C.Bt)