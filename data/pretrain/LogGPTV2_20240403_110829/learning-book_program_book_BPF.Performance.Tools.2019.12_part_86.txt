to mlock(8) can be provided to specify the process ID to trace; without it, the entire system is tracedl.
mheld
The source to mheld(8) is:
+1/usr/local/bin/bpftrace
BEGIN
1
printf (*Tracing mutex_lock () beld tines, Cte1-C to end. \n*)
kprobe :mutex_lock,
kprobe smutex_trylock,
kprobe:mutex_1ock_1nterruptlb1e
/T$ == prd 11 0 == Ts/
re -[p]xppexoo[B
/($] == 0 11 pid =- $1) ss (lock_addr[tid] /
kretprobe :mutex_lock
Bheld_start [8lock_adde[tid]] = nsecs
delete (@lock_addx [t1d]) 
kretprobe :mutex_trylock,
kzetprobe:mutex_lock_interruptible
/retva] == 0 66 ($1 == 0 11 pid == $1] 6& Block_adde[tid] /
Bheld_start [8lock_adde[tid]] = nsecs7
delete (elock_addx [t1d]) ≠
/($1 == 0 11 pld == $1) 4s @held_staxt[arg01/
kprobe rmutex_unlock
Bheld_t.ine_ns [ksym (arg0), kstack (5) , conm] =
hist (nsecs - eheld_start[arg0])
delete (@held_start [arg0]):
---
## Page 693
656
Chapter 14 Kernel
END
clear (81ock_addc1 
clear (@held_stazt)
This traces the held duration from the different mutex functions. As with mlock(8), an optional
process ID argument can be provided.
14.4.6 Spin Locks
As with mutex locks traced previously, there are not yet tracepoints for tracing spin locks.
Note that there are several types of spin locks, including spin_lock_bh(), spin_lock(),
spin_lock_irq(), and spin_lock_irqsave() [162]. They are defined as follows in
include/linux/spinlock.h:
define spin_lock_irqsave (1ock, flags)
do
rax_spin_lock_irqsave (spin.lock_check [lock) , flags) 
)vhile (0)
[..-]
define rax_spin_lock_irgsavre (lock, flags)
do [
typecheck (unsigned long, flags) 
[lags = _rav_spin_lock_irqsave (1ock) ;
}xh.ile(0)
You can see them using funccount(8): 
+ funccount **spin_lock**
Tracing 16 functions fox
FUHC
COUNT
_rav_spin_lock_bh
7092
nat.ive_queued_spin_lock_slovpath
7227
_rav_spin_lock_irq
261538
_rav_spin_lock
1215218
_rav_spin_lock_irqsave
1582755
De tach.ing...
---
## Page 694
14.4 BPF Tools
657
funccount(8) is instrumenting the entry of these functions using kprobes. The return of these
functions cannot be traced using kretprobes, so it's not possible to time their duration directly
from these functions. Look higher in the stack for functions that can be traced, e.g., by using
stackcount(8) on the kprobe to see the call stack.
I usually debug spin lock performance issues using CPU profiling and flame graphs, since they
appear as CPU-consuming functions.
14.4.7kmem
kmem(8)a is a bpftrace tool to trace kernel memory allocations by stack trace and prints statis-
tics on the number of allocations, the average allocation size, and the total bytes allocated. For
example:
kmen.bt
Attach.ing 3 probes...
Tracing knen allocation stacks (knalloc, knen_cache_alloc) . Hit Ctrl-C to end.
[.--]
] eaqfq@
knen_cache_a11oc+288
getnane_flags+79
ge tnane+18
SB+uados.a"op
SyS_opena t+20
 Xorg]1 count 44, average 4096, total 180224
[rebytes_a1loc) ;
This uses the stats() built-in to print the triplet: count of allocations, average bytes, and total
bytes. This can be switched to hist() for printing histograms if desired.
14.4.8 kpages
kpages(8)? is a bpftrace tool that traces the other type of kernel memory allocation, alloc_pages(),
via the kmem:mm_page_alloc tracepoint. Example output:
+ kpages.bt
Attach.ing 2 pzobes...
Tracing page allocation stacks. Hit Ctel-C to end.
[...]
?pages [
_al1oc_pages_nodemask+521
9+eus"sebedootTe
handle_pte_fault+959
_handle_mm_fault+1144
handle_rm_fault+177
 chrone] : 11733
The output has been truncated to show only one stack; this one shows Chrome processes allocating
11,73 pages while tracing during page faults. This tool works by tracing kmem tracepoints. Since
allocations can be frequent, the overhead may become measurable on busy systems.
9 0rigin: I crested it on 15-Mar-2019 for this book.
---
## Page 696
14.4 BPF Tools
659
The source to kpages(8) is:
#1/usx/local/bin/bpEtrace
BEGIX
1
tracepoint:kmen:mn_page_alloc
Bpages[kstack(5) , comm] = count(1 
This can be implemented as a one-liner, but, to ensure that it isn’t overlooked, I’ve created it as the
kpages(8) tool.
14.4.9
memleak
memleak(8) was introduced in Chapter 7: it is a BCC tool that shows allocations that were not
freed while tracing, which can identify memory growth or leaks. By default it traces kernel
allocations, for example:
nemleak
Attaching to kernel allocators, Ctrl+C to quit.
[13:46:02] Top 10 stackswith outstanding allocations1
[ . - - ]
6922240 bytes in 1690 a1locations from stack
[teuxex] 0zx+xseuepouse6edootte
al1oc_pages_current+0x6a [kernel]
Page_cache_alloc+0x81 [kerme1]
pagecache_get_page+0x9b [kernel]
gxab_cache_page_xxite_begin+0x26 [kernel]
ext4_ds_vrite_begin+0xcb [kernel]
generlc_pexform_xrite+0xb3 [kerme1]
_generic_file_veite_iter+0xlaa [kernel]
ext4_file_xrite_itez+0x203 [kerme1]
nev_sync_write+0xe? [kermel]
_vfs_we1te+0x2 9 [kexnel]
vfs_write+0xb1 [kecne1l]
sys_pvrite64+0x95 [kerne1]
do_sysca11_64+0x73 [kernel]
entry_SYsCALL_64_after_hvfrane+0x3d [kexne1]
---
## Page 697
660
Chapter 14 Kernel
Just one stack has been included here, showing allocations via ext4 writes. See Chapter 7 for more
about memleak(8).
14.4.10 slabratetop
slabratetop(8)° is a BCC and bpftrace tool that shows the rate of kernel slab allocations by slab
cache name, by tracing kmem_cache_alloc() directly. This is a companion to slabtop(1), which
shows the volume of the slab caches (via /proc/slabinfo). For example, from a 48-CPU production
instance:
+slabratetop
09:48:29 1oadavg: 6.30 5.45 5,46 4/3377 29884
CACHE
ALL0CS
BYTES
kma11oc-4096
654
2678784
kma1loc256
2637
674816
f1lp
392
100352
b6
66176
TCP
TE
63488
kma11oc-1024
58
59392
proc_lnode_cache
69
46920
eventpo11_epi
354
45312
s1gqu409
227
36320
dentry
165
31680
[.--]
This output shows that the kmalloc-4096 cache had the most bytes allocated in that output
interval. As with slabtop(1), this tool can be used when troubleshooting unexpected memory
pressure.
This works by using kprobes to trace the kmem_cache_alloc() kernel function. Since this function
can be called somewhat frequently, the overhead of this tool might become noticeable on very
busy systems.
BCC
Command line usage:
slabzatetop [options][1nterval[count]]
Options:
 -C: Don’t clear the screen
10 0rigin: 1 created it on 15-0ct-2016 for B0C, and 1
 on 26-Jan-2019
---
## Page 698
14.4 BPF Tools
661
bpftrace
This version only counts allocations by cache name, printing output each second with a
timestamp:
#1/usx/local/bin/bpEtrace
#include 
include 
#1fdef CoNFIG_S1UB
include 
e1.se
include 
#end1f
kprobe:knen_cache_a11oc
p6xe (+ e(oeoux 1onx1s] = desqoe0$
8 [str ($cachep=>name] ] = count.1)
Interval:s:1
t.ime () :
print (8) 
clear (8)
A check for the kernel compile option CONFIG_SL.UB is needed so that the correct version of the
slab allocator header files are included.
14.4.11
1numamove
numamove(8) traces page migrations of type *NUMA misplaced.* These pages are moved to
different NUMA nodes to improve memory locality and overall system performance. I've encoun-
tered production issues where up to 40% of CPU time was spent doing such NUMA page migra-
tions; this performance loss outweighed the benefits of NUMA page balancing, This tool helps me
keep an eye on NUMA page migrations in case the problem returns. Example output:
+ numamove.bt
Attaching 4 pzobes...
TIME
UMA_nigrations BUMA_nigrations_ns
22 : 48 : 45
0
1.1 0rigin: I created it for this bc
 of an issut
---
## Page 699
662
Chapter 14 Kernel
22 : 48 : 46
22:48:47
30B
22:48: 48
?
0
22 : 48 : 4.9
D
22 : 48 : 50
Q
22 :48 : 51
[...]
Suxes 'suoge3iu s0g 2:gZZ e suoper3u a8ed vWN jo 1snq e nu8neo μndqno s
29 milliseconds in total. The columns show the per-second rate of migrations and the time spent
doing migrations in milliseconds. Note that NUMA balancing must be enabled (sysctl kernel.
numa_balancing=1) for this activity to occur.
The source to numamove(8) is:
+1/usr/local/bin/bpftrace
kprobe:migrate_misplaced_page [ Bstart [tid] - nsecs]
kretprobe :migrate_misplaced_page /estart[tid] /
$dur = nsecs - Bstart[tid] 
Bns += $dur;
Bnun+ +
delete (@start[tid]) 
BEGIN
printf(*&10s 18s 18s^n*, *rIME*,
*XUHA_nigratlons*,*NUNA_nigratLons_ss*) 
interval:s11
t.ime (*%:M:1S*) 
printf (* s18d s18d^,n*, @nun, @ns / 10000001;
delete (@nun) :
delete (@ns) 
This uses a kprobe and kretprobe to trace the start and end of the kernel function
migrate_misplaced_pageO, and an interval probe to print out the statistics.
---
## Page 700
14.4 BPF Tools
663
14.4.12
workq
workq(8)2 traces workqueue requests and times their latency. For example:
+ workq.bt
Attaching 4 prcbes.
Tracing workgueue request latencies, Ctr1-C to end
+C
[...]
Pus [intel_atom
it_vork]:
[1K, 2x)
71
[2K, 4K)
91
[4K, 8K]
86881 261
[8K,16K]
1524 186988988 886 8868898886986988988 886 8868898886986 1
[16x, 32K]
1019 1eeeeeeeeee8ee8eeeee8eeeeeeee8ee8ee
[32K, 64K]
2 1
fus [kcryptd_crypt] :
[2, 4)
21
[4, 8]
486 1869889889886 88698698698
[8, 16}
1076 1eeeeeeeeee e８eeeeeeeeeeeeeeeeeeeee e８eeeeeeeeeeeee1
[16,32)
888888886812887
[32, 64)
456188
[64, 128]
25018
[128, 256}
190 1
[256, 512}
291
[512, 1K]
14 1
[1K, 2K)
2 1
This output shows that the kcryptd_crypt( workqueue function was called frequently,
usually taking between four and 32 microseconds.
This works by tracing the workqueue:workqueue_execute_start and
workqueue:workqueue_execute_end tracepoints.
The source to workq(8) is:
1/usr/local/bin/bpftrace
BEGIN
12 0rigin: I crested it for this b
---
## Page 701
664
Chapter 14 Kernel
tracepoint:vorkqueve:vorkqueve_execute_start
Isoasu - [pT]4xe,s8
uotoung<-s6xe -[pT]oungbng
tracepointivorkquevervorkqueve_execute_end
/Bstaxt [tid]/
000t / ([pta]xeasg - soesu = xmps
Bus [ksym (8vqfunc [tid])] = hist ($dur] 
delete @start[tid]) :
delete (@vqfunc |tid]1
EXD
1
clear (8start) :
clear (8vqfunc) 
This measures the time from execute start to end and saves it as a histogram keyed on the