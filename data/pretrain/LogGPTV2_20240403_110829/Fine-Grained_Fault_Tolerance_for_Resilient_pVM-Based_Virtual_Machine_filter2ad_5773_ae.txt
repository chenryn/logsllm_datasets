mean
Xen
879.11
FG FT
901.99
CG FT
900.12
TFD-Xen 889.91
Xoar
6616.44
99th
1820.84
1711.11 1977.15
1792.04
1963.5
1701.43 1911.33
9026.7
95th
475.37
491.58
489.03
489.1
9590.54 3713.98 5535.77 5791.712 3019.33 3507.88 3660.65 8054.53 8642.85
99th
65.642
72.44
72.12
73.19
mean
457.61
460.2
461.09
461.18
99th
9.67
10.61
11.05
10.89
mean
8.6
10.1
11.9
9.12
mean
1.79
2.11
2.12
2.11
masstree
img-dnn
99th mean 95th
3.42
476.2
4.2
494.3
4.3
490.12
493.55
4.5
8695
99th
7.6
1.7
8.21
1.92
7.98
1.9
1.9
8.11
543.9 2526 9603
xapian
95th
4.35
5.56
5.96
4.98
moses
95th
39.567
40.78
41.3
40.44
TABLE VI: Performance of TailBench applications without net_uk failure (latencies in milliseconds). Lower is better.
a given component hosted by the pVM (e.g., XenStore or
toolstack) leads to downtime and full recovery for all the other
components. In order to tolerate failures of the XenStore, its
state is replicated in a dVM, and XenStore and VM manage-
ment operations are made transactional, through the use of a
log stored in a dVM. No detail is provided on the mechanisms
used to enforce consistency and availability despite potential
concurrent failures of the pVM-hosted components and the
backup state in the dVM. Besides,
the evaluation of this
approach is focused on its resilience against synthetic fault
injection. The authors of this solution do not provide any
detailed performance measurements (with or without failures),
and the code of the prototype is not available. Our work
has similarities with this approach but, (i) we apply fault
tolerance techniques at a ﬁner granularity, (ii) we explore the
ramiﬁcations of the interdependencies between services, and
(iii) we provide a detailed performance evaluation.
to a quiescent state that
Hypervisor resilience. Some works have focused on im-
proving the resilience of the hypervisor. ReHype [8], [30],
[31] is a Xen-based system, which leverages microreboot [32],
[33] techniques in order to recover from hypervisor failures,
without stopping or resetting the state of the VMs (including
the pVM and dVMs) or the underlying physical hardware.
NiLyHype [9] is a variant of ReHype, which replaces the
microreboot approach with an alternative component recov-
ery technique named microreset (i.e., resetting a software
component
is likely to be valid
rather than performing a full reboot) in order to improve the
recovery latency. TinyChecker [10] uses nested virtualization
techniques (more precisely, a small, trusted hypervisor running
below a main, full-ﬂedged hypervisor like Xen) in order to
provide resilience against crashes and state corruption in the
main hypervisor. Shi et al. [34] proposed a new modular,
“deconstructed” design for Xen in order to thwart the most
dangerous types of security attacks. Their work focuses on
a redesign of the Xen hypervisor (not the dom0 pVM). All
these works do not consider the services hosted in the pVM
(or driver domains) and are mostly orthogonal to our work.
Our contribution leverages these results (i.e., the fact that the
hypervisor can be made highly resilient) in order to improve
the fault tolerance of the pVM components.
The FTXen project [35] aims at hardening the Xen hyper-
visor layer so that it can withstand hardware failures on some
“relaxed” (i.e., fast but unreliable) CPU cores. In contrast,
our work considers the resilience of the pVM components on
current hardware, with a fault model that is homogeneous/sym-
metric with respect to CPU cores.
Some recent projects aim at supporting live reboot and/or
upgrade of VMMs without disrupting the VMs [36], [37].
These techniques are focused on code updates for improv-
ing the safety and security of the hypervisor component.
Hence, they are orthogonal to our contribution. This trend also
highlights the crucial importance of our goal: improving the
resilience of the remaining components, i.e., the pVM services.
VII. CONCLUSION
VMMs remain a key building block for cloud computing,
and many of them are based on a pVM-based design. We
have highlighted that, in this design, the pVM component
has become the main weakness in terms of fault tolerance
(compared to the bare metal hypervisor component). Besides,
the existing solutions only tackle a limited set of pVM services
(device drivers) and/or require long failure detection/recovery
times and signiﬁcant performance overheads. To the best of
our knowledge, our contribution is the ﬁrst to propose and
demonstrate empirically, a complete approach allowing to
achieve both high resilience (against failures of different com-
ponents and concurrent failures of interdependent services)
and low overhead. Our approach currently relies on manual
tuning of some important parameters (e.g., for failure detection
and scheduling) but, we envision that recently published works
could help manage them in a more automated and robust
way [38]. Another area for future work is the tuning and
optimization of resource allocation for disagreggrated pVM
components, which could be extended from existing tech-
niques proposed for a monolithic pVM design [27].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
207
REFERENCES
[1] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho,
R. Neugebauer,
I. Pratt, and A. Warﬁeld, “Xen and the Art of
Virtualization,” SIGOPS Oper. Syst. Rev., vol. 37, no. 5, pp. 164–177,
Oct. 2003. [Online]. Available: http://tiny.cc/5xt4nz
[2] K. Fraser, S. Hand, R. Neugebauer,
I. Pratt, A. Warﬁeld, and
M. Williamson, “Safe Hardware Access with the Xen Virtual Machine
Monitor,” in In Proceedings of the 1st Workshop on Operating System
and Architectural Support for the on demand IT InfraStructure (OASIS),
2004.
[3] S. Spector, “Why Xen?” 2009. [Online]. Available: http://www-archive.
xenproject.org/ﬁles/Marketing/WhyXen.pdf
[4] H. Jo, H. Kim, J. Jang, J. Lee, and S. Maeng, “Transparent fault
tolerance of device drivers for virtual machines,” IEEE Transactions
on Computers, vol. 59, no. 11, pp. 1466–1479, Nov 2010.
[5] P. Colp, M. Nanavati, J. Zhu, W. Aiello, G. Coker, T. Deegan,
P. Loscocco, and A. Warﬁeld, “Breaking Up is Hard to Do: Security
and Functionality in a Commodity Hypervisor,” in Proceedings of the
Twenty-Third ACM Symposium on Operating Systems Principles, ser.
SOSP ’11. New York, NY, USA: ACM, 2011, pp. 189–202. [Online].
Available: http://doi.acm.org/10.1145/2043556.2043575
[6] H. Kasture
and D. Sanchez,
a benchmark suite
and evaluation methodology for latency-critical applications,” IEEE
International Symposium on Workload Characterization (IISWC),
2016.
[Online]. Available: http://ieeexplore.ieee.org/stamp/stamp.jsp?
tp=&arnumber=7581261&isnumber=7581253
“TailBench:
[7] D. J. Scales, M. Nelson, and G. Venkitachalam, “The Design of a
Practical System for Fault-tolerant Virtual Machines,” SIGOPS Oper.
Syst. Rev., vol. 44, no. 4, pp. 30–39, Dec. 2010. [Online]. Available:
http://doi.acm.org/10.1145/1899928.1899932
[8] M. Le and Y. Tamir, “ReHype: Enabling VM Survival Across
Hypervisor Failures,” in Proceedings of the 7th ACM SIGPLAN/SIGOPS
International Conference on Virtual Execution Environments, ser. VEE
’11. New York, NY, USA: ACM, 2011, pp. 63–74. [Online]. Available:
http://doi.acm.org/10.1145/1952682.1952692
[9] D. Zhou and Y. Tamir, “Fast Hypervisor Recovery Without Reboot,” in
2018 48th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN), June 2018, pp. 115–126.
[10] C. Tan, Y. Xia, H. Chen, and B. Zang, “TinyChecker: Transparent
protection of VMs against hypervisor failures with nested virtualization,”
in IEEE/IFIP International Conference on Dependable Systems and
Networks Workshops (DSN 2012), June 2012, pp. 1–6.
[11] The Linux Foundation, “Xen Project.” [Online]. Available: https:
//xenproject.org
[12] A. Madhavapeddy and D. J. Scott, “Unikernels: The Rise of the
Virtual Library Operating System,” Commun. ACM, vol. 57, no. 1,
pp. 61–69, Jan. 2014. [Online]. Available: http://doi.acm.org/10.1145/
2541883.2541895
[13] M. M. Swift, M. Annamalai, B. N. Bershad, and H. M. Levy,
“Recovering Device Drivers,” ACM Trans. Comput. Syst., vol. 24,
no. 4, pp. 333–360, Nov. 2006. [Online]. Available: http://doi.acm.org/
10.1145/1189256.1189257
[14] “Xl.” [Online]. Available: https://wiki.xenproject.org/wiki/XL
[15] “Mirage OS.” [Online]. Available: https://mirage.io
[16] O. Mutlu,
“The RowHammer Problem and Other
Issues We
May Face As Memory Becomes Denser,” in Proceedings of
the
in Europe, ser. DATE
Conference on Design, Automation & Test
’17.
3001 Leuven, Belgium, Belgium: European Design and
Automation Association, 2017, pp. 1116–1121. [Online]. Available:
http://dl.acm.org/citation.cfm?id=3130379.3130643
[17] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, “Exploiting Correcting
Codes: On the Effectiveness of ECC Memory Against Rowhammer
Attacks,” in S&P, May 2019, best Practical Paper Award, Pwnie
Award Nomination for Most Innovative Research. [Online]. Available:
http://tiny.cc/trt4nz
[18] “etcd.” [Online]. Available: https://etcd.io
[19] “Exploring performance of etcd, zookeeper and consul consistent
key-value datastores.” [Online]. Available: http://tiny.cc/8hu4nz
[20] D. Ongaro and J. Ousterhout, “In Search of an Understandable
Consensus Algorithm,” in 2014 USENIX Annual Technical Conference
(USENIX ATC 14).
Philadelphia, PA: USENIX Association, Jun.
2014, pp. 305–319. [Online]. Available: http://tiny.cc/wku4nz
[21] “MiniOS.” [Online]. Available: https://github.com/mirage/mini-os
[22] M. M. Swift, S. Martin, H. M. Levy, and S. J. Eggers, “Nooks:
An architecture for reliable device drivers,” in Proceedings of
the
10th Workshop on ACM SIGOPS European Workshop, ser. EW 10.
New York, NY, USA: ACM, 2002, pp. 102–107. [Online]. Available:
http://doi.acm.org/10.1145/1133373.1133393
[23] V. Narayanan, A. Balasubramanian, C. Jacobsen, S. Spall, S. Bauer,
M. Quigley, A. Hussain, A. Younis, J. Shen, M. Bhattacharyya, and
A. Burtsev, “LXDs: Towards Isolation of Kernel Subsystems,” in 2019
USENIX Annual Technical Conference (USENIX ATC 19). Renton,
WA: USENIX Association, Jul. 2019, pp. 269–284. [Online]. Available:
https://www.usenix.org/conference/atc19/presentation/narayanan
[24] “Xen Security Modules.” [Online]. Available: http://tiny.cc/zdu4nz
[25] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach, I. Pratt,
and A. Warﬁeld, “Live migration of virtual machines,” in Proceedings
of the 2nd Conference on Symposium on Networked Systems Design and
Implementation, ser. NSDI’05. USA: USENIX Association, 2005, p.
273–286.
[26] “ApacheBench.” [Online]. Available: http://tiny.cc/anu4nz
[27] D. Mvondo, B. Teabe, A. Tchana, D. Hagimont, and N. De Palma,
“Closer: A New Design Principle for the Privileged Virtual Machine
OS,” in 2019 IEEE 27th International Symposium on Modeling, Anal-
ysis, and Simulation of Computer and Telecommunication Systems
(MASCOTS), 2019, pp. 49–60.
[28] D. G. Murray, G. Milos, and S. Hand, “Improving Xen Security Through
Disaggregation,” in Proceedings of the Fourth ACM SIGPLAN/SIGOPS
International Conference on Virtual Execution Environments, ser. VEE
’08. New York, NY, USA: ACM, 2008, pp. 151–160. [Online].
Available: http://doi.acm.org/10.1145/1346256.1346278
[29] M. Le and T. Yuval, “Resilient Virtualized Systems Using ReHype,”
UCLA Computer Science Department, Tech. Rep. 140019, October
2014.
[30] M. Le, “Resilient Virtualized Systems,” UCLA Computer Science De-
partment, Ph.D. thesis 140007, March 2014.
[31] M. Le and Y. Tamir, “Applying Microreboot to System Software,” in
2012 IEEE Sixth International Conference on Software Security and
Reliability, June 2012, pp. 11–20.
[32] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox,
“Microreboot — A Technique for Cheap Recovery,” in Proceedings
of the 6th Conference on Symposium on Opearting Systems Design
& Implementation - Volume 6,
Berkeley, CA,
USA: USENIX Association, 2004, pp. 3–3.
[Online]. Available:
http://dl.acm.org/citation.cfm?id=1251254.1251257
ser. OSDI’04.
[33] A. Depoutovitch and M. Stumm, “Otherworld: Giving Applications
a Chance to Survive OS Kernel Crashes,” in Proceedings of
the
5th European Conference on Computer Systems, ser. EuroSys ’10.
New York, NY, USA: ACM, 2010, pp. 181–194. [Online]. Available:
http://doi.acm.org/10.1145/1755913.1755933
[34] L. Shi, Y. Wu, Y. Xia, N. Dautenhahn, H. Chen, B. Zang, and J. Li,
“Deconstructing Xen,” in 24th Annual Network and Distributed System
Security Symposium, NDSS 2017, San Diego, California, USA, February
26 - March 1, 2017, 2017. [Online]. Available: http://tiny.cc/j3t4nz
[35] X. Jin, S. Park, T. Sheng, R. Chen, Z. Shan, and Y. Zhou, “FTXen:
Making hypervisor resilient to hardware faults on relaxed cores,” in 2015
IEEE 21st International Symposium on High Performance Computer
Architecture (HPCA), Feb 2015, pp. 451–462.
[36] S. Doddamani, P. Sinha, H. Lu, T.-H. K. Cheng, H. H. Bagdi,
and K. Gopalan, “Fast and Live Hypervisor Replacement,” in
the 15th ACM SIGPLAN/SIGOPS International
Proceedings of
Conference on Virtual Execution Environments,
ser. VEE 2019.
New York, NY, USA: ACM, 2019, pp. 45–58. [Online]. Available:
http://doi.acm.org/10.1145/3313808.3313821
[38] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A.
[37] X. Zhang, X. Zheng, Z. Wang, Q. Li, J. Fu, Y. Zhang, and Y. Shen,
“Fast and Scalable VMM Live Upgrade in Large Cloud Infrastructure,
booktitle = Proceedings of the Twenty-Fourth International Conference
on Architectural Support for Programming Languages and Operating
Systems,” ser. ASPLOS ’19. New York, NY, USA: ACM, 2019, pp. 93–
105. [Online]. Available: http://doi.acm.org/10.1145/3297858.3304034
I.
Kistijantoro, “Understanding and auto-adjusting performance-sensitive
conﬁgurations,” in Proceedings of
the Twenty-Third International
Conference on Architectural Support
for Programming Languages
and Operating Systems, ser. ASPLOS ’18. New York, NY, USA:
Association for Computing Machinery, 2018, p. 154–168. [Online].
Available: https://doi.org/10.1145/3173162.3173206
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
208