the  lowest  probability  for  the  base  structures  and  set  it  as  an 
estimate  for  the  probability  of  this  base  structure.  We  currently 
use this latter approach. If the digit component of the password or 
the special characters component were not initially in the training 
data, we can still find a probability associated with those values 
since  our  grammar  includes  these  not-found  values  through 
smoothing. If the alphabet part of the password is not included in 
the  dictionary,  we  use  the  probability  associated  with  a  word  of 
the same length as in the dictionary, since we currently assume all 
the words of the same length as having equal probability. We have 
thus shown that we can determine if the user’s password is strong 
or weak. We next discuss how to modify a weak password. 
4.  MODIFYING A WEAK PASSWORD 
When AMP rejects a password we need to modify the password 
but  we  want  to  still  keep  it  usable  and  memorable.  A  usable 
password is a password that is easy to remember and type. Things 
people can remember are different for each group of people based 
on  their  age,  situation,  location,  etc.  There  are  also  special 
numbers  or  names  that  are  only  important  to  an  individual.  We 
believe  users  should  be  free  to  choose  any  word,  numbers  or 
special  characters  that  make  sense  to  them  when  initially 
proposing a password. (An exception is that the password be long 
enough as otherwise brute force would be enough to crack it.) 
If a password is rejected we try to generate passwords with slight 
changes  to  the  user-chosen  password  using  the  AMP  distance 
function.  This  is  based  on  edit  distance  to  fulfill  the  need  of 
usability for users. We believe users choose password components 
for  memorability  and  only  minimal  changes  should  be  made. 
Hence, we start generating passwords with distance one from the 
user-chosen  password  and  check  if  the  modified  password  is 
within  the  acceptable  threshold value.  Forget  et  al.  [19]  showed 
that even several random changes (such as replacing or inserting 
up to four characters) were memorable by users. We can conclude 
from this study that our modifications, which usually change only 
one  character  in  the  passwords,  will  have  a  similar  result.  Note 
that in many cases (when the user chosen password is strong) we 
do not need to change the password at all. 
4.1    The AMP Distance Function 
We  use  a  distance  function  similar  to  the  Damerau-Levenshtein 
distance  function  [26]  but  with  some  changes  to  make  it  more 
efficient  for  our  password  structures.  We  define  two  sets  of 
operations  for  our  distance  function.  An  operation  can  be  done 
either on the base structure or on a component. 
4.1.1  Operations on the Base Structure: 
Insertion:  inserting  a  component  of  length  one  is  allowed  only 
when  it  is  not  of  the  same  type  as  its  adjacent  components. 
Example: if the base structure is L5D3S1 we can insert D1 in the 
beginning to make D1L5D3S1.  
Deletion:  deleting  a  component  can  be  done  if  the  number  of 
components is not 1 and if it does not make two components of 
the  same  type  adjacent.  Example:  we  can  delete  D2  from  base 
structure D2S1D1 to make S1D1.  
Transposition: exchanging two adjacent components can be done 
only  if  it  does  not  make  two  components  of  the  same  type 
adjacent to each other.  
4.1.2  Operations on the Component: 
Insertion:  inserting  one  character  of  the  same  type  inside  a 
component  is  allowed.  Example:  if  component  D3=123,  we  can 
transform it to 4123 by inserting 4 in the beginning.  
Deletion:  Deleting  one  character  inside  a  component  is  allowed 
only if the length of the component is not equal to 1.  
Substitution: we can substitute a character with another character 
of the same type. If S2=!! it can be transformed into “!#”. 
Case: we can invert the case (upper / lower) of one character in 
the alphabetical component of the password. Example: If L4 = life, 
we can transform to liFe. 
The cost of each operation is 1 in our current system but we could 
allow  for  different  costs  for  different  operations if  needed.  Note 
that  we  do  not  make  changes  in  the  alpha  string  part  except 
possibly to change case. 
4.2  Modifier Algorithm 
When users enter their password, the system automatically parses 
the password to its base structure. To construct possible modified 
passwords, we execute the above operations on the base structure 
and components and keep all the information in a tree structure. 
The user-chosen password is the root of the tree and we keep track 
of the results of all possible operations within edit distance of one. 
In the tree, a child node is the result of one of the operations. After 
building  the  tree,  we  start  from  the  root  node  and  randomly 
choose  a  child  until  we  reach  a  leaf  node.  If  this  password  is 
within  the  acceptable  threshold  limit  we  are  done,  else  we 
randomly try again. During each tree traversal, we actually mark 
each component previously tried so that the next pass will find a 
different password.  
113
Table 3. Example of passwords modified by AMP 
Input password 
to AMP 
trans2 
colton00 
789pine 
mitch8202 
cal1fero 
KILLER456 
violin22 
ATENAS0511 
*zalena6 
KYTTY023 
Output of 
modifier 
%trans2 
8colton00 
789pinE 
mitch=8202 
cal8fero 
KILlER456 
violin^22 
0511AETENAS 
*3zalena6 
KYTTY023r 
thus  preserving 
It is possible that we will not find a password within distance 1 
with  the  desired  probability.  By  selecting  the  new  passwords 
randomly, we are avoiding the possibility of suggesting the same 
password  to  different  users  with  the  same  or  similar  original 
passwords. In order to get a password with distance 2, one could 
repeat the same steps for passwords with distance 1 starting from 
any of the distance 1 passwords. Table 3 shows a set of passwords 
given to AMP as input and the output of the modifier component. 
It can be seen that very limited  changes has been applied to the 
user-chosen  password 
the  usability  and 
memorability of the password. 
5.  DYNAMIC UPDATE 
In order to maintain the AMP system as still effective after users 
use  the  system  for  some  time,  we  have  developed  an  update 
strategy  that  modifies  the  grammar  periodically.  AMP  clearly 
proposes  less  popular  passwords  to  users  (those  having  smaller 
probabilities)  than  the  common  ones  they  might  suggest  when 
modification is needed. However, what could happen after using 
the system for a period of time is that the probability distribution 
of  passwords  changes  due  to  the  passwords  proposed  by  AMP. 
And  therefore,  whenever  a  recent  set  of  real  user  passwords 
become  revealed,  an  attacker  can  use  these  for  training  the 
password  cracker.  Since 
the  supposedly  strong  passwords 
suggested  by  AMP  have  become  in  use  more  often  and  would 
now  have  higher  probability  in  the  guessing  generator,  the 
attacker  has  a  better  model  of  the  AMP  generator  and  therefore 
continued  use  of  the  original  grammar  could  be  problematic. 
Obviously, we can always use the most recent set of passwords as 
the training set for AMP to overcome this problem, but it would 
not  always  be  easy  to  access  a  large  set  of  real-user  passwords. 
Instead,  we  consider  every  modified  password  that  has  been 
suggested  to  a  user  as  a  publicly  disclosed  password,  with  an 
appropriate weight, to be used as if it were in the training set. By 
effectively  adding  every  new  password  to  the  training  set,  we 
always have a realistic and recent probability distribution for the 
probabilistic  grammar.  For  example,  if  some  password  structure 
has low probability and thus AMP keeps suggesting that structure, 
after a while, since we are adding every password to the training 
set,  AMP  will  dynamically  adapt  and  use  that  structure  less 
frequently. 
5.1  Updating the Grammar 
Note  that  in order  to  update  the  training  set,  we  do not  actually 
need to add the AMP proposed password to the training set, repeat 
the  training  step  and  reproduce  the  context-free  grammar  again. 
Instead,  we  only  need  to  adjust  the  probability  values  in  the 
context-free grammar. The probability values in the grammar are 
the  frequencies  of  each  structure  or  component  used  in  the 
training  set.  Whenever  a  new  password  has  been  suggested  we 
only  need  to  update  the  frequency  of  the  components  and  base 
structures  used  in  that  password.  For  example,  if  the  new 
password is !!78liar, we only change the probabilities of the base 
structure and of S2 and D2. We do not change the probability of 
liar  since  as  previously  discussed  probabilities  of  words  do  not 
come  from  the  training  set  but  from  the  dictionary  and  AMP 
considers  all  the  words  (whether  they  are  included  in  the 
dictionary or whether they are not) the same based on length.  
By  considering  the  probability  of  each  element  (of  the  base 
structure or the component) as its frequency in the training set we 
have  pi  =  ni  /  N,  where  ni  is  the  number  of  occurrences  of  the 
element and N is the total number of elements. With this in mind, 
’= 
seeing  another  element  i  would  change  the  probability  to  pi
(ni+(cid:68))/(N+(cid:68))  and  the  probability  of  the  rest  of  the  elements 
’ = nj / (N+(cid:68)). The parameter (cid:68) can be used to 
would change to pj
adjust the rate of change. This mechanism is similar to Laplacian 
smoothing.  In  our  experiments  we  trained  our  grammar  on 
approximately  1  million  passwords  which  resulted  in  about  11 
thousand  base  structures.  Updating  this  grammar  can  be  done 
almost instantaneously. Every time we update the grammar, some 
of  the  probability  values  change  and  it  changes  the  password 
distribution.  In  order  to  be  able  to  understand  the  changes  and 
compare the distributions we use entropy metrics to see how the 
dynamic  update  affects  the  probabilities.  First  we  explain  how 
these  metrics  can  be  relevant,  then  we  show  our  approach  to 
calculate Shannon entropy from the grammar and then we discuss 
the  effect  of  updating  the  grammar  with  respect  to  the  entropy 
values.  
5.2  Using the Entropy Metrics 
So how can we use the entropy measures? We need to distinguish 
between  problems  with  theoretical  distributions  and  empirical 
distributions of passwords. For empirical distributions we can first 
check if the min entropy is low. This is the same as the probability 
of the first guess being high. This simply means that a few initial 
passwords might easily be guessed. But this could be expected in 
any realistic distribution. In general in our experiments  we have 
seen  that  the  min  entropy  is  actually  fairly  high.  But  in  fact  the 
min  entropy  is  really  irrelevant  to  us  since  we  would  never 
propose the first few high probability passwords anyways. We can 
also check if the Shannon entropy distribution is reasonably high. 
Because  we  have  a  quick  way  to  compute  the  exact  Shannon 
entropy H(X) of our probability distribution (see next section), we 
also have a quick way to compute a lower bound on the guessing 
entropy G, using a bound derived by Massey [25]. 
G(X)  (cid:149)(cid:3)(cid:243)(cid:3)(cid:21)H(X) + 1                       (5.1) 
In  the  experiments  in  Section  6,  the  Shannon  entropy  of  the 
original password checker distribution is about 27, which can be 
viewed as equivalent to a space of 227 different passwords. Note 
that even  with the moderate  value of Shannon entropy, the total 
number  of  guesses  possible  by  the  grammar  is  beyond  the 
trillions.  Thus  we  should  be  able  to  find  a  reasonable  reject 
function as there will be many possible passwords with very small 
probabilities.  Finally,  if  we  can  drive  the  system  to  higher 
Shannon entropy then the new distribution of passwords would be 
more  resistant  to  any  password  cracking  attack.  Thus,  used 
properly,  Shannon  entropy  of  the  grammar  can  be  useful  in  our 
grammar update system. 
It should be noted that when ensuring strong passwords, there are 
two  possible  approaches.  The  first  is  to  find  a  distribution  from 
which any password chosen is hard to break. This is the approach 
114
taken  by  Verheul  [2].  Thus  the  guessing  entropy  has  some 
meaning but as discussed by Verheul, he needs to also ensure that 
the min entropy is high. Recall that Verheul’s approach does not 
ensure usability of the chosen password. The second approach is 
to somehow ensure that a specific password is hard to break, but it 
need  not  be  randomly  chosen  from  a  given  distribution.  This  is 
our  approach  and  we  additionally  ensure  usability.  Furthermore 
we protect against an optimal guessing attack. But we clearly do 
not  care  that  some  initial  number  of  passwords  from  that 
distribution can be broken since these would be identified as weak 
by our system. 
5.3  Shannon Entropy of the Grammar 
Since  we  have  a  password  guess  generator  that  can  generate 
passwords  in  probabilistic  order,  we  can  clearly  compute  the 
Shannon  entropy,  guessing  entropy  and  min  entropy  for  the 
guesses  generated  by  our  context-free  grammar  by  generating 
guesses and computing the various entropy values. Note that since 
the  generator  was  developed  through  training  on  revealed 
passwords, these entropy values can be viewed as realistic values 
for  the  relevant  password  probability  distribution.  In  fact  after 
training  on  a  sufficiently  large  set  of  revealed  passwords,  the 
distribution (through the grammar) can be viewed as a reference 
model for the “true” probability distribution of passwords.  
We  now  show  that  we  can  compute  the  Shannon  entropy  using 
only the probabilistic-context free grammar and without actually 
generating these guesses. We use some well-known notions such 
as  joint  entropy  and  conditional  entropy  [27].  Let  G  to  be  the 