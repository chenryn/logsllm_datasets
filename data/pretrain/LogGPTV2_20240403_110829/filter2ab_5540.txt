### Title: Smashing the ML Stack for Fun and Lawsuits: Navigating the Legal Risks of Adversarial Machine Learning

Adversarial machine learning research is experiencing a surge in activity. Researchers are increasingly focusing on commercial machine learning (ML) systems, such as those employed by Facebook, Tesla, Microsoft, IBM, and Google, to expose potential vulnerabilities. However, this raises important legal questions: What risks do these researchers face? How do legal frameworks align with vendors' expectations regarding the use of their systems?

In this talk, we will delve into the legal risks associated with testing the security of commercially deployed ML systems. Investigating or testing the security of any operational system can potentially violate the Computer Fraud and Abuse Act (CFAA), the primary U.S. federal statute that establishes liability for hacking. Our team has previously analyzed common adversarial attacks under U.S. law, highlighting the uncertainties created by the variability in legal regimes for both researchers and companies seeking to understand the applicable legal rules.

The recent U.S. Supreme Court decision in Van Buren v. United States, which addresses the scope of the authorization provisions in the CFAA, provides us with an opportunity to offer more definitive answers regarding the legal risks faced by adversarial ML researchers. We will explore the legal implications of various types of attacks, including model inversion, membership inference, and poisoning attacks. Additionally, we will examine whether other legal frameworks, such as copyright or contract law, more closely align with the expectations of system defenders regarding permissible activities.