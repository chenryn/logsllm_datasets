### Side Defense and Attack Evasion

Side defense mechanisms can address the evasion techniques employed by attackers. It is important to note, however, that an attacker cannot induce such behavior on arbitrary applications; they can only exploit applications that already perform extensive, non-standard transformations.

### Second-Order Attacks

Second-order attacks involve injecting malicious parameters into links or forms contained within a victim's web page. When these forms are submitted, an XSS attack is executed. However, XSSFilt will apply policies to these submissions as well, thereby detecting and mitigating second (or higher order) attacks. Internet Explorer (IE) has an exception for same-origin links, making it vulnerable to this type of attack.

## Evaluation and Comparison

In this section, we evaluate and compare the protection and compatibility offered by XSSFilt, XSSAuditor, and NoScript. Our results demonstrate that the techniques used in XSSFilt provide better compatibility and protection. Additionally, we provide data on the prevalence of partial injection attacks.

### 7.1 Implementation

We implemented XSSFilt by modifying the Content Security Policies (CSPs) implementation in Firefox. This allowed us to leverage the CSP interface and code, which include most of the required interposition callbacks. CSPs implement the `nsIContentSecurityPolicy` interface, which is used to check the URL of external resources being loaded, including scripts. We added a new method, `permits`, to check inlined resources for XSS injections.

We modified the existing CSP callbacks to pass the script content to `permits` where appropriate. We also added new callbacks for Base elements and Data URLs, which CSPs do not need to address for technical reasons.

To test NoScript, we downloaded the latest version (2.2.3) and disabled all features except for XSS protection, which we enabled for all requests.

To test XSSAuditor, we reimplemented it in Firefox to avoid instrumenting a second browser. Since XSSFilt is based on XSSAuditor’s architecture, reimplementing its policies was straightforward.

### 7.2 Protection Evaluation

We tested the three filters against two sources of XSS attack data that have been widely used in previous research:

1. **xssed**: xssed.com [6] contains reports of websites vulnerable to XSS, along with a URL for a sample attack. Given the large dataset, we randomly selected a subset of 400 recent, working attacks to estimate the effectiveness of our filter against real-world attacks.
2. **XSS Cheatsheet**: The xssed dataset is biased towards very simple attack payloads, often just injecting a script tag. To assess the filter’s protection for more complex attacks, we created a web page with multiple XSS vulnerabilities and tried attack vectors from the XSS Cheat Sheet [8], a well-known source for XSS filter circumvention techniques.

To automatically test this large set of attacks, we modified Firefox to log XSS violations to a file and used its extension API to automatically navigate the browser to all URLs in the two datasets.

**Figure 3: Results for xssed and cheatsheet dataset**

| Dataset | XSSFilt | XSSAuditor | NoScript |
|---------|---------|------------|----------|
| xssed   | 400/400 | 399/400    | 20/20    |
| cheatsheet | 20/20 | 379/400    | 18/20    |

**Figure 4: XSSAuditor failures**

| Dataset | Partial Script Injection | String Transformation |
|---------|-------------------------|-----------------------|
| xssed   | 16                      | 5                     |
| cheatsheet | 2                    | 0                     |

XSSFilt successfully stopped all but one of the attacks from the xssed dataset and all attacks from the cheatsheet dataset. The lone failure is attributed to a limitation of taint-inference: when the web application applies extensive string transformations, the algorithm might fail to find a relationship between the parameter and the content. For example, the parameter:
```javascript
alert("HaCkEd By N2n - HaCkEr 3 r d @ l i v e");
```
was transformed to:
```javascript
alert("HaCkEd N2n - HaCkEr 3 r d @ l i v e");
```
by the web application. Some spaces and dashes were deleted, along with the word “By”. In such situations, no client-side filter can realistically detect the attack.

The 100% coverage on the cheatsheet dataset is not surprising, as these attacks are designed to bypass server-side sanitization functions, which look for specific patterns in text and are vulnerable to browser quirks and unusual XSS vectors. Since XSSFilt is immune to browser quirks and covers all vectors uniformly, none of these attacks succeeded.

XSSAuditor missed several attacks in these datasets. The underlying causes are:

- **Partial Script Injection**: XSSAuditor does not detect this type of attack because, unlike XSSFilt, it does not perform URL parsing and substring matching.
- **String Transformation**: XSSAuditor relies on canonicalization to account for common string transformations in web applications. This approach can break when an uncommon transformation takes place. Taint-inference, on the other hand, relies on approximate substring matching, which is more tolerant of exceptions.

NoScript’s XSS filter performed well against both datasets. Unlike XSSAuditor and XSSFilt, NoScript specifically looks for JavaScript syntax and common JavaScript functions such as `alert`. Most attacks on xssed simply attempt to open a script tag and pop up an alert box, so it is clear that NoScript should have no trouble detecting them. However, considerable skills and efforts may be required to transform the payload until it bypasses the filter. In our experiments, we were able to bypass the filter in some cases by substituting the `alert` call with another JavaScript identifier. If the web application bound that identifier to a suitable function, we could have carried out a successful XSS attack.

### 7.3 Prevalence of Partial Injection Vulnerabilities

Compared to XSSAuditor, XSSFilt is able to detect partial script injection vulnerabilities. Therefore, it is important to assess how prevalent these are. We used three different methods to estimate their prevalence.

#### 7.3.1 Partial Injections in xssed.com Data Set

Out of 400 real-world live XSS attacks, 4% targeted partial injection vulnerabilities. We analyzed the rest of the vulnerable pages attacked through whole script injections and discovered that an additional 4% of pages are vulnerable, for a total of 8% of pages vulnerable to partial script injection.

Thus, even though the coverage against attacks on the xssed dataset for XSSAuditor was 95%, the actual coverage on vulnerabilities is lower at 91%. However, the size of this dataset is quite limited for extrapolating statistics about the nature of XSS attacks in general. Moreover, the website does not review submissions and does not reward contributors for creative or complex attacks, leading to a bias towards simple vulnerabilities that can be discovered automatically.

#### 7.3.2 Partial Injection Vulnerabilities in the Wild

We developed a tool/scanner called gD0rk [18] to study the prevalence of XSS vulnerabilities in deployed sites. Although gD0rk was not developed for this paper, it is very helpful for assessing the prevalence of partial injection vulnerabilities:

- **gD0rk analyzed a much larger collection of websites compared to xssed.com data, providing a broader basis for drawing inferences about vulnerabilities in deployed sites.**
- **gD0rk uses a mechanical procedure for finding vulnerabilities, with no built-in bias for partial injections.**

We note that, due to the nature of reflected XSS attacks, we are targeting ourselves in these attacks, and hence believe that the websites scanned by gD0rk were in no way subjected to any harm.

gD0rk uses Google’s advanced search capabilities to shortlist candidate websites that are likely to be vulnerable, probes them for reflected content by modifying the URL, and examines the context in which the content is injected in the web pages returned to build an attack. For example, if a reflection is inside a JavaScript string in a script tag, the scanner attempts to write:
```javascript
"; payload(); //
```
If the application sanitizes double quotes, the scanner attempts to close the script tag instead and open a new script node with:
```javascript
</script><script>alert(1)</script>
```

We ran gD0rk for one month and identified 272,051 vulnerable websites. For scalability and performance reasons, we did not validate the generated attacks for all these vulnerabilities. Instead, we used statistical sampling to estimate the fraction of these sites that were actually vulnerable. A random subset of 1,000 vulnerabilities was selected, and we verified that 98% of the generated attacks worked on this subset. We then selected a random subset of 10,000 vulnerable websites and used the scanner to identify the context of the vulnerable reflections. We found that 18% of these reflections were included within script tags or event handlers, representing partial script injection vulnerabilities.

#### 7.3.3 Dynamically Generated Scripts

Intuitively, the necessary requirement for a partial injection vulnerability is a script that is assembled dynamically from input parameters by the web application. We believe it is reasonable to expect developers to fail to sanitize parameters which appear inside scripts just as often as they fail to sanitize them anywhere else in the page. Under this hypothesis, the rate of pages that construct scripts dynamically is a good estimator for the ratio of partial injection vulnerabilities to whole script injection vulnerabilities.

The benefit of this indirect approach over the previous one is that the dataset is not made out of vulnerable pages, which represents a skewed sample from mostly unpopular websites. For this reason, we built a browser-resident crawler for Firefox and bootstrapped it with the 1,000 most popular websites according to the Alexa rankings. When the crawler processes a page with non-trivial HTTP parameters and detects that a parameter appears in a script, it substitutes the parameter value with a placeholder, requests the page with the newly constructed URL, and then attempts to find the original value and the new placeholder in the response. If the placeholder is found in the same script and the original parameter value is not found, then the relationship between script and parameter is confirmed, and the page is marked as containing a dynamic script. When we stopped the crawler, it had crawled a total of 35,145 pages, of which 9% contained dynamically generated scripts. Given the strictness of the requirements, we believe this is a conservative estimate.

### 7.4 Compatibility Evaluation

Browser-resident reflected XSS defenses restrict the capabilities of browsers with respect to content found in input parameters, such as GET parameters from the query string. As a result, they have the potential to break some web pages, leading to compatibility problems. To estimate the compatibility of these defenses, we instrumented Firefox to log information about XSS checks while performing the aforementioned crawl on major websites. The browser-resident crawler was developed using the new Addon-SDK [14] for Firefox. This allowed us not only to support discovery of dynamically constructed links and forms but also to check all the resources loaded by the web page (including scripts and advertisements inserted through DOM manipulation) for XSS violations.

Overall, all filters reported very few false positives. This is due to the benign nature of the dataset, which contains very few special characters: only 26 URLs contained any of the characters in the following set: {",',<}. However, not all filters performed equally:

- **NoScript’s 15 false positives** are complex URLs containing long identifiers that erroneously match NoScript regular expressions. For example, the following (simplified) URL triggers a violation:
  ```
  http://domain.com/dir/page.php?n=PHNjcmlwdP25plJmo9MCI%2BPC9zY3JpcHQ%2B&h=55e1652a183
  ```
  An interesting property of NoScript’s XSS filter is that if a parameter triggers a false positive in one web application, it will do so in every other web application, because the actual HTTP response does not matter. For this reason, when we devised the heuristics to fill and submit forms, we chose not to submit suspicious strings that would trigger XSS violations on every web application. Had we configured the crawler to fill forms with values such as `alert(1)`, NoScript would have triggered many more policy violations.

- **XSSFilt initially reported a much higher number of false positives than NoScript.** Most of them were either due to a URL being supplied as a parameter and then used by an existing script to construct a new script tag (for advertisements), or by a parameter being passed to a string-to-code function such as `eval`. These practices would be safe if the application code checked the value against a whitelist of pre-approved URLs for the former case or JavaScript snippets for the latter, and the violation could be indeed considered spurious. However, we found that out of 51 XSSFilt notifications, only 8 did such checks; the remaining violations were in fact due to vulnerable pages that could be subverted to load a script from an arbitrary host or execute arbitrary code. This set of pages includes important websites such as wsj.com, weather.com, and tripadvisor.com. For this reason, we do not consider these scenarios as false positives.

- **XSSAuditor behaved similarly to XSSFilt, reporting the same vulnerable pages as XSS violations.** We discounted them from XSSAuditor results as well. A couple of actual false positives produced by XSSFilt involving partial script injections were also noted.

**Figure 5: Compatibility Comparison**

| Filter | # of Violations |
|--------|-----------------|
| XSSFilt | 15              |
| XSSAuditor | 8             |
| NoScript | 6               |