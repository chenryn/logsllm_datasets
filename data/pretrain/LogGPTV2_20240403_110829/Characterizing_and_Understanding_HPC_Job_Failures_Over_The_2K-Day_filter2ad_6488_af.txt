terminated involving 25% message IDs (i.e., the ﬁrst two
rows in Table XVI). (2) The most frequent RAS events
are node errors (such as 00080014 and 0008000B), which
involve over 78.16% system-reliability-based job failures
(as presented in Table XVII). In addition, message unit (MU)
is used to move data between the memory and the 5D torus
network. Accordingly, Table XVII shows that (3) network-
related errors constitute 2.81% of the system-reliability-
related job failures; and the errors occurring in the system
such as kernels (denoted as Software Error in the table)
constitute about 16.22%. (4) As for the system component,
“ﬁrmware” is the major root cause of terminating jobs.
Firmware refers to a speciﬁc class of software that provides
the low-level control for the device’s speciﬁc hardware.
TABLE XVI
FRACTION OF THE MESSAGE ID OF RAS EVENTS AFFECTING JOBS
percent
11.39%
6.24%
2.81%
1.72%
0.78%
0.31%
0.16%
0.16%
msg ID
000A000D
0008001A
000400CD
00080008
000400ED
00040143
00010001
00061012
msg ID
00080014
00040106
00010010
0008000C
00040037
00080016
0004014D
00080004
msg ID
0008000B
00080007
00040157
00080019
000400B1
000C0042
00080017
0001000A
percent
26.37%
11.23%
4.21%
2.65%
1.40%
0.31%
0.16%
0.16%
percent
14.04%
7.96%
3.74%
2.5%
1.09%
0.31%
0.16%
0.16%
In Table XVIII and in Fig. 14(a), we show how the job-
affected RAS message IDs correlate with users. We formulate
the following takeaway. (Takeaway 19): Various users tend
to be affected by speciﬁc types of RAS events,
likely
because of the job’s particular features such as execution
scales, running setting, or the application’s nature (compu-
tation intensive or memory intensive). For instance, u5 and
u10 tend to be related to msg ID of 00080014 (node errors),
while u2 and u3 are more likely affected by 000A000D (kernel
panic/errors). Since RAS event
indicates system-reliability
(ﬁrmware/hardware), this takeaway is not self-evident.
Similarly, we formulate the following takeaway. (Takeaway
20): The job-affected RAS events are also correlated to a
certain extent with the core-hours consumed, as presented
10
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25CDFJob Length (in hours)Real Failed Job Length1.Weibull2.Pearson63.Lognormal 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 1.5 2 2.5 3 3.5 4 4.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 30 35 40 45 50CDFJob Length (in hours)Real Failed Job Length1.InverseGaussian2.Weibull3.Lognormal 0.76 0.78 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25CDFJob Length (in hours)Real Failed Job Length1.Pareto2.InverseGaussian3.Weibull 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 70 80 90CDFJob Length (in hours)Real Failed Job Length1.Erlang2.Exponential3.Gamma 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.91 2 3 4 5 6 7 8 9 10 0 0.2 0.4 0.6 0.8 1[0,512MB)[512MB,1GB)[1GB,2GB)[2GB,4GB)[4GB,8GB)[8GB,16GB)[16GB,32GB)[32GB,64GB)[128GB,256GB)[256GB,512GB)Exit Code Distribution# Bytes ReadTimeoutBugKillIOUnknownRS 0 0.2 0.4 0.6 0.8 1[0,512MB)[512MB,1GB)[1GB,2GB)[2GB,4GB)[4GB,8GB)[8GB,16GB)[16GB,32GB)[32GB,64GB)[64GB,128GB)[128GB,256GB)Exit Code Distribution# Bytes WrittenTimeoutBugKillIOUnknownRSin Table XIX and Fig. 14(b). For example, Table XIX shows
that the 00080014 (node error) mainly happens to large-core-
hour jobs. According to Fig. 14(b), it is observed that if a job’s
core-hours is in [1k,2k) and it failed because of a fatal system
event, it is likely because of 000A000D (kernel panic/error).
00080014
00040106
00080007
TABLE XVIII
0008000B
000A000D
CONTINGENCY TABLE WITH USER NAME AND RAS MSG ID
User
0008001A
u1
u2
u3
u4
u5
u6
u7
u8
u9
u10
16
0
0
4
13
0
8
3
1
10
15
1
0
10
0
0
3
8
10
1
0
22
21
1
0
17
0
0
0
0
1
0
0
0
1
0
2
1
0
0
5
0
0
0
2
0
1
1
1
1
5
0
0
3
2
0
1
0
0
0
CONTINGENCY TABLE WITH CORE-HOURS AND RAS MSG ID
corehours
000A000D
0008000B
00040106
00080007
00080014
0008001A
TABLE XIX
Fig. 15. Spatial Distribution of Job-Affected RAS Events on Compute Racks
[0,1k)
[1k,2k)
[2k,4k)
[4k,8k)
[8k,16k)
[16k,32k)
[32k,64k)
[64k,128k)
[128k,256k)
[256k,...)
11
7
1
9
10
13
13
24
16
65
1
1
2
3
5
9
9
16
7
37
4
33
9
24
0
1
0
0
1
1
0
1
3
2
2
1
5
9
12
37
3
2
1
0
2
7
2
3
11
20
0
0
2
1
2
5
3
4
7
16
Fig. 16. Spatial Distribution of Job-Affected RAS Events on IO Racks
on the similarity threshold [0.2,0.8] (similarity threshold is a
paramter to control the similarity between messages in the
analysis [31]). According to system administrators, the simi-
larity threshold of 0.5 leads to high-ﬁdelity ﬁltering results,
corresponding to 575 fatal events in total (where 518 are
related to compute racks and 57 are related to I/O racks).
Hence we have this takeaway. (Takeaway 22): MTTI is about
3.48 days from the perspective of jobs/users in the Mira,
disclosing the real system-related failure rate for users.
VII. CONCLUSION AND FUTURE WORK
In this paper, we conduct a joint analysis with multiple
data sources to explore the job failure features over the 2,001
days of the IBM Blue Gene/Q Mira. We present 22 valuable
takeaways, which we believe are helpful for understanding the
job failure features and how users or system events affect job
executions, as well as the locality features of the job-affected
RAS events in both compute racks and I/O racks. Our analysis
involves the largest scale of core-hours in a resilience study to
the best of our knowledge. As future work, we plan to study
more supercomputers for a comprehensive comparison.
ACKNOWLEDGMENTS
This research was supported under the Contract DE-AC02-
06CH11357 by the U.S. Department of Energy. We used the
data of the Argonne Leadership Computing Facility, which is a
DOE Ofﬁce of Science User Facility supported under Contract
DE-AC02-06CH11357 by the U.S. Department of Energy.
(a) Distr. based on Users
Fig. 14. RAS Event Distribution based on Users/Corehours
(b) Distr. based on Execution Time
Fig. 15 and Fig. 16 show the locality features of the job-
affected RAS events in the compute racks and I/O racks,
respectively. (Takeaway 21): Such RAS events are located
under a fairly nonuniform distribution: some midplanes
(such as rack R03-M0 and R1C-M0) have far more
frequent issues than other midplanes (such as R11-M0)
do. The most error-prone rack and midplane have 35 and 27
fatal RAS events, respectively, while the minimum numbers
of fatal events per rack and midplane are 6 and 1, respectively.
Some nodes have high-frequent failures (e.g.,R03: 19 failures
on one node), while many nodes (shown as ‘white square’)
have no failure-job-related events at all.
After a spatiotemporal ﬁlter is used to remove the duplicated
messages, the number of job-affected RAS events is 635,
indicating that the rate of job failures caused by RAS events is
once as per 3.15 days. As mentioned in Section IV, we further
remove the highly mutually correlated (or similar) messages
by leveraging LogAider [31] in terms of the similarity of
two events occurring at close timestamps. Speciﬁcally,
if
two events occur within 2 hours and exhibit fairly similar
properties, we merge them as one event because of the likely
same event source (or root cause). The total number of failures
can be further narrowed to the range of [570, 596], based
11
 0 0.2 0.4 0.6 0.8 1u1u2u3u4u5u6u7u8u9u10RAS Event DistributionUsers000800140008000B000A000D00040106000800070008001A0001001000040157000400CD0008000C 0 0.2 0.4 0.6 0.8 1[0,1k)[1k,2k)[2k,4k)[4k,8k)[8k,16k)[16k,32k)[32k,64k)[64k,128k)[128k,256k)[256k,512k)RAS Event DistributionCorehours000800140008000B000A000D00040106000800070008001A0001001000040157000400CD0008000CNumber of events on Rack635Number of events on Midplane127Number of events on Node119Status ColorsMira System (Compute Nodes)R00M0M1R01M0M1R02M0M1R03M0M1R04M0M1R05M0M1R06M0M1R07M0M1R08M0M1R09M0M1R0AM0M1R0BM0M1R0CM0M1R0DM0M1R0EM0M1R0FM0M1R10M0M1R11M0M1R12M0M1R13M0M1R14M0M1R15M0M1R16M0M1R17M0M1R18M0M1R19M0M1R1AM0M1R1BM0M1R1CM0M1R1DM0M1R1EM0M1R1FM0M1R20M0M1R21M0M1R22M0M1R23M0M1R24M0M1R25M0M1R26M0M1R27M0M1R28M0M1R29M0M1R2AM0M1R2BM0M1R2CM0M1R2DM0M1R2EM0M1R2FM0M1MIRA System (IO Nodes)Q2GQ2HQ1GQ1HQ0GQ0HI6I7I8I3I4I5I0I1I2I6I7I8I3I4I5I0I1I2I6I7I8I3I4I5I0I1I2I6I7I8I3I4I5I0I1I2I6I7I8I3I4I5I0I1I2I6I7I8I3I4I5I0I1I26211714Status codorsNumber of events on RackNumber of events on IO drawerNumber of events on Computer Card[19] V. Sridharan, J. Stearley, N. DeBardeleben, S. Blanchard, and S. Gu-
rumurthi, “Feng Shui of supercomputer memory positional effects in
DRAM and SRAM faults,” in SC ’13: Proceedings of the International
Conference on High Performance Computing, Networking, Storage and
Analysis, Nov 2013, pp. 1–11.
[20] V. Sridharan, N. DeBardeleben, S. Blanchard, K. B. Ferreira, J. Stearley,
J. Shalf, and S. Gurumurthi, “Memory errors in modern systems:
The good, the bad, and the ugly,” in Proceedings of the Twentieth
International Conference on Architectural Support for Programming
Languages and Operating Systems, ser. ASPLOS ’15. New York, NY,
USA: ACM, 2015, pp. 297–310.
[21] “Ornl jaguar,” https://www.olcf.ornl.gov/tag/jaguar/, online.
[22] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai,
D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux, L. Carro, and
A. Bland, “Understanding GPU errors on large-scale HPC systems and
the implications for system design and operation,” in 2015 IEEE 21st
International Symposium on High Performance Computer Architecture
(HPCA), Feb 2015, pp. 331–342.
[23] S. Gupta, D. Tiwari, C. Jantzi, J. Rogers, and D. Maxwell, “Under-
standing and exploiting spatial properties of system failures on extreme-
scale HPC systems,” in Proceedings of the 2015 45th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks, ser.
DSN ’15. Washington, DC, USA: IEEE Computer Society, 2015, pp.
37–44.
[24] D. Tiwari, S. Gupta, G. Gallarno, J. Rogers, and D. Maxwell, “Reliability
lessons learned from GPU experience with the Titan Supercomputer
at Oak Ridge Leadership Computing Facility,” in Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, ser. SC ’15. New York, NY, USA: ACM, 2015,
pp. 38:1–38:12.
[25] B. Nie, J. Xue, S. Gupta, C. Engelmann, E. Smirni, and D. Tiwari,
“Characterizing temperature, power, and soft-error behaviors in data
center systems: Insights, challenges, and opportunities,” in 2017 IEEE
25th International Symposium on Modeling, Analysis, and Simulation of
Computer and Telecommunication Systems (MASCOTS), Sept 2017, pp.
22–31.
[26] S. Gupta, T. Patel, C. Engelmann, and D. Tiwari, “Failures in large
scale systems: Long-term measurement, analysis, and implications,”
in Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, ser. SC’17. New York,
NY, USA: ACM, 2017, pp. 44:1–44:12.
[27] A. A. Hwang, I. A. Stefanovici, and B. Schroeder, “Cosmic rays
don’t strike twice: Understanding the nature of DRAM errors and the
implications for system design,” SIGPLAN Not., vol. 47, no. 4, pp. 111–
122, Mar. 2012.
[28] Z. Zheng, L. Yu, W. Tang, Z. Lan, R. Gupta, N. Desai, S. Coghlan, and
D. Buettner, “Co-analysis of RAS log and job log on Blue Gene/P,”
in 2011 IEEE International Parallel Distributed Processing Symposium,
May 2011, pp. 840–851.
[29] “Intrepid at argonne (blue gene/p),” https://www.alcf.anl.gov/intrepid,
online.
[30] “Mira system logs,” https://reports.alcf.anl.gov/data/mira.html, online.
[31] S. Di, R. Gupta, M. Snir, E. Pershey, and F. Cappello, “LOGAIDER:
A tool for mining potential correlations of HPC log events,” in 2017
17th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing (CCGRID), May 2017, pp. 442–451.
[32] W.-S. Yang, H.-C. W. Lin, and Y. H. He, “Franklin job completion
analysis.” in CUG, Edinburg, UK, 2010.
[33] S. Habib, V. Morozov, N. Frontiere, H. Finkel, A. Pope, K. Heitmann,
K. Kumaran, V. Vishwanath, T. Peterka, J. Insley et al., “HACC: extreme
scaling and performance across diverse architectures,” Communications
of the ACM, vol. 60, no. 1, pp. 97–104, 2016.
[34] “Ibm blue gene/q ras book,” https://reports.alcf.anl.gov/data/datadictionary/
RasEventBook.html, online.
REFERENCES
[1] M. Snir, R. W. Wisniewski, J. A. Abraham, S. V. Adve, S. Bagchi,
P. Balaji, J. Belak, P. Bose, F. Cappello, B. Carlson, A. A. Chien,
P. Coteus, N. A. Debardeleben, P. C. Diniz, C. Engelmann, M. Erez,
S. Fazzari, A. Geist, R. Gupta, F. Johnson, S. Krishnamoorthy,
S. Leyffer, D. Liberty, S. Mitra, T. Munson, R. Schreiber, J. Stearley,
and E. V. Hensbergen, “Addressing failures in exascale computing,”
Int. J. High Perform. Comput. Appl., vol. 28, no. 2, pp. 129–173, May
2014. [Online]. Available: http://dx.doi.org/10.1177/1094342014522573
[2] S. Di, H. Guo, R. Gupta, E. R. Pershey, M. Snir, and F. Cappello,
“Exploring properties and correlations of fatal events in a large-scale
HPC system,” IEEE Transactions on Parallel and Distributed Systems,
pp. 1–14, 2018.
[3] “Cobalt:
Component-based
lightweight
toolkit,”
https://trac.mcs.anl.gov/projects/cobalt, online.
[4] “Torque resource manager,” http://www.adaptivecomputing.com/products/
[5] “Darshan project,” https://www.mcs.anl.gov/research/projects/darshan/
torque/, online.
publications/, online.
[6] P. Carns, K. Harms, W. Allcock, C. Bacon, S. Lang, R. Latham, and
R. Ross, “Understanding and improving computational science storage
access through continuous characterization,” Trans. Storage, vol. 7,
no. 3, pp. 8:1–8:26, Oct. 2011.
[7] C. D. Martino, Z. Kalbarczyk, R. K. Iyer, F. Baccanico, J. Fullop, and
W. Kramer, “Lessons learned from the analysis of system failures at
petascale: The case of Blue Waters,” in Proceedings of the 2014 44th
Annual IEEE/IFIP International Conference on Dependable Systems
and Networks, ser. DSN ’14. Washington, DC, USA: IEEE Computer
Society, 2014, pp. 610–621.
[8] C. D. Martino, W. Kramer, Z. Kalbarczyk, and R. Iyer, “Measuring
and understanding extreme-scale application resilience: A ﬁeld study
of 5,000,000 HPC application runs,” in 2015 45th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN),
June 2015, pp. 25–36.
[9] R.-T. Liu and Z.-N. Chen, “A large-scale study of failures on petascale
supercomputers,” Journal of Computer Science and Technology, vol. 33,
no. 1, pp. 24–41, Jan 2018.
[10] “Blue waters supercomputer,” https://bluewaters.ncsa.illinois.edu/, on-
line.
[11] G.
Lakner
gene
http://www.redbooks.ibm.com/abstracts/sg247869.html, online.
solution:
and
B.
Blue
Knudson,
gene/q
“Ibm
system
system
blue
administration,”
[12] G. Li, Q. Lu, and K. Pattabiraman, “Fine-grained characterization
of faults causing long latency crashes in programs,” in 2015 45th
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks, June 2015, pp. 450–461.
[13] Q. Lu, M. Farahani, J. Wei, A. Thomas, and K. Pattabiraman, “LLFI:
An intermediate code-level fault injection tool for hardware faults,” in
2015 IEEE International Conference on Software Quality, Reliability
and Security, Aug 2015, pp. 11–16.
[14] T. Siddiqua, V. Sridharan, S. E. Raasch, N. DeBardeleben, K. B. Ferreira,
S. Levy, E. Baseman, and Q. Guan, “Lifetime memory reliability data
from the ﬁeld,” in 2017 IEEE International Symposium on Defect and
Fault Tolerance in VLSI and Nanotechnology Systems (DFT), Oct 2017,
pp. 1–6.
[15] S. Levy, K. B. Ferreira, N. DeBardeleben, T. Siddiqua, V. Sridharan,
and E. Baseman, “Lessons learned from memory errors observed over
the lifetime of Cielo,” in Proceedings of the International Conference
for High Performance Computing, Networking, Storage, and Analysis,
ser. SC ’18. Piscataway, NJ, USA: IEEE Press, 2018, pp. 43:1–43:12.
[Online]. Available: http://dl.acm.org/citation.cfm?id=3291656.3291714
supercomputer,”
capability
https://www.lanl.gov/projects/cielo/, online.
[16] “Cielo
NNSA
[17] B. Nie, D. Tiwari, S. Gupta, E. Smirni, and J. H. Rogers, “A large-scale
study of soft-errors on gpus in the ﬁeld,” in 2016 IEEE International
Symposium on High Performance Computer Architecture, HPCA 2016,
Barcelona, Spain, March 12-16, 2016, 2016, pp. 519–530.
[18] B. Nie, J. Xue, S. Gupta, T. Patel, C. Engelmann, E. Smirni, and
D. Tiwari, “Machine learning models for gpu error prediction in a
large scale hpc system,” in 2018 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN), June 2018,
pp. 95–106.
12