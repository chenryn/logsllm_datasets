title:Safe and effective fine-grained TCP retransmissions for datacenter
communication
author:Vijay Vasudevan and
Amar Phanishayee and
Hiral Shah and
Elie Krevat and
David G. Andersen and
Gregory R. Ganger and
Garth A. Gibson and
Brian Mueller
Safe and Effective Fine-grained TCP Retransmissions for
Datacenter Communication
Vijay Vasudevan1, Amar Phanishayee1, Hiral Shah1, Elie Krevat1,
David G. Andersen1, Gregory R. Ganger1, Garth A. Gibson1,2, Brian Mueller2
1Carnegie Mellon University, 2Panasas Inc.
ABSTRACT
This paper presents a practical solution to a problem facing
high-fan-in, high-bandwidth synchronized TCP workloads
in datacenter Ethernets—the TCP incast problem. In these
networks, receivers can experience a drastic reduction in
application throughput when simultaneously requesting data
from many servers using TCP. Inbound data overﬁlls small
switch buﬀers, leading to TCP timeouts lasting hundreds of
milliseconds. For many datacenter workloads that have a
barrier synchronization requirement (e.g., ﬁlesystem reads
and parallel data-intensive queries), throughput is reduced by
up to 90%. For latency-sensitive applications, TCP timeouts
in the datacenter impose delays of hundreds of milliseconds
in networks with round-trip-times in microseconds.
Our practical solution uses high-resolution timers to enable
microsecond-granularity TCP timeouts. We demonstrate
that this technique is eﬀective in avoiding TCP incast collapse
in simulation and in real-world experiments. We show that
eliminating the minimum retransmission timeout bound is
safe for all environments, including the wide-area.
Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network
Protocols—TCP/IP ; C.2.6 [Computer-Communication
Networks]: Internetworking
General Terms
Performance, Experimentation, Measurement
Keywords
Datacenter Networks, Incast, Performance, Throughput
1.
INTRODUCTION
In its 35 year history, TCP has been repeatedly challenged
to adapt to new environments and technology. Researchers
have proved adroit in enabling TCP to function well in
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$5.00.
gigabit networks [27], long/fat networks [18, 8], satellite
and wireless environments [22, 5], among others. In this
paper, we examine and improve performance in an area that,
surprisingly, also proves challenging to TCP: very low delay,
high throughput, datacenter networks of dozens to thousands
of machines.
The problem we study is TCP incast collapse [25], where
application throughput drastically reduces when multiple
senders communicate with a single receiver in high band-
width, low delay networks using TCP. Highly bursty, fast
data transmissions overﬁll Ethernet switch buﬀers, causing in-
tense packet loss that leads to TCP timeouts. These timeouts
last hundreds of milliseconds on a network whose round-trip-
time (RTT) is in the 10s or 100s of microseconds. Protocols
that have some form of synchronization requirement, such as
ﬁlesystem reads and writes or highly parallel data-intensive
queries found in large memcached clusters [12], block wait-
ing for timed-out connections to ﬁnish before issuing new
requests. These timeouts and the resulting delay can reduce
application throughput by 90% (Section 3) or more [25, 28].
Coarse-grained TCP timeouts can also harm performance
for latency-sensitive datacenter applications (Section 2.2).
In this paper, we present and evaluate a set of system
extensions to enable microsecond-granularity retransmission
timeouts (RT O). The challenges in doing so are threefold:
First, we show that the solution is practical by modifying
the Linux TCP implementation to use high-resolution kernel
timers. Second, we show that these modiﬁcations are eﬀec-
tive, preventing TCP incast collapse in a real cluster for up
to 47 concurrent senders (Section 5). As shown in Section 4,
microsecond granularity timeouts are necessary—simply re-
ducing RT Omin in today’s TCP implementations without
also improving the timing granularity does not prevent TCP
incast collapse, particularly in future settings. In simulation,
our changes to TCP prevent incast collapse for up to 2048
concurrent senders on 10-gigabit links. Last, we show that
the solution is safe, examining the eﬀects of an aggressively
reduced RT O in the wide-area, showing that its beneﬁts to
recovery in datacenters do not aﬀect performance for bulk
ﬂows in the wide-area.
The motivation for solving this problem is the increas-
ing interest in using Ethernet and TCP for communication
and bulk storage transfer applications in the fastest, largest
datacenters. Provided that TCP adequately supports high
bandwidth, low latency, synchronized and parallel applica-
tions, there is a strong desire to “wire-once” and reuse the
mature, well-understood transport protocols that are so fa-
miliar in lower bandwidth networks.
303Scenario
WAN
Datacenter
SAN
RTT
100ms
<1ms
<0.1ms
OS
TCP RT Omin
Linux
BSD
Solaris
200ms
200ms
400ms
Table 1: Typical round-trip-times and minimum
TCP retransmission bounds.
2. BACKGROUND
Cost pressures increasingly drive datacenters to adopt com-
modity components, and often low-cost implementations of
such. An increasing number of clusters are being built with
oﬀ-the-shelf rackmount servers interconnected by Ethernet
switches. While the adage “you get what you pay for” still
holds true, entry-level gigabit Ethernet switches today oper-
ate at full data rates, switching upwards of 50 million packets
per second—at a cost of about $10 per port. Commodity
10Gbps Ethernet is now cost-competitive with specialized
interconnects such as Inﬁniband and FibreChannel, and ben-
eﬁts from wide “brand recognition”. To reduce cost, however,
lower-cost switches often sacriﬁce expensive, power-hungry
SRAM packet buﬀers, the eﬀect of which we explore through-
out this work.
The desire for commodity parts extends to transport pro-
tocols. TCP provides a “kitchen sink” of protocol features,
including reliability via retransmission, congestion and ﬂow
control, and in-order packet delivery to the receiver. Not all
applications need all of these features [20, 31], and some bene-
ﬁt from more rich transport abstractions [14], but TCP is ma-
ture and well-understood by developers, and has become the
transport protocol of choice even in many high-performance
environments.
Without link-level ﬂow control, TCP is solely responsi-
ble for coping with and avoiding packet loss in the (often
small) Ethernet switch egress buﬀers. Unfortunately, the
workload we examine has three features that challenge (and
nearly cripple) performance: a highly parallel, synchronized
request workload; buﬀers much smaller than the bandwidth-
delay product of the network; and high-fan-in communication
resulting in TCP ﬂows with windows of only a few packets.
2.1 TCP Incast Collapse
Barrier-synchronized request workloads are becoming in-
creasingly common in today’s commodity clusters. Examples
include parallel reads/writes in cluster ﬁlesystems such as
Lustre [6], Panasas [34], or NFSv4.1 [33]; search queries sent
to dozens of nodes, with results returned to be sorted1; or
parallel databases that harness multiple back-end nodes to
process parts of queries. We deﬁne the request to be “barrier
synchronized” when the client cannot make forward progress
until the responses from every server for the current request
have been received—these applications often cannot present
partial results or issue an unbounded number of requests.
In a cluster ﬁle system, for example, a client application
requests a data block striped across several storage servers,
issuing the next data block request only when all servers
have responded with their portion. This workload can result
1In fact, engineers at Facebook recently rewrote the middle-
tier caching software they use—memcached [23]—to use UDP
so that they could “implement application-level ﬂow control
for ... gets of hundreds of keys in parallel” [12]
Figure 1: TCP incast collapse is observed for a syn-
chronized reads application running on a 48-node
cluster
in packets overﬁlling the buﬀers on the client’s port on the
switch, resulting in many losses. Under severe packet loss,
TCP can experience a timeout that lasts a minimum of 200ms,
determined by the TCP minimum retransmission timeout
(RTOmin). While the default values operating systems use
today may suﬃce for the wide-area, datacenters and SANs
have round-trip-times that are orders of magnitude below
the RT Omin defaults (Table 1).
When a server involved in a barrier-synchronized request
experiences a timeout, other servers can ﬁnish sending their
responses, but the client must wait a minimum of 200ms
before receiving the remaining parts of the response, during
which the client’s link may be completely idle. The resulting
throughput seen by the application may be as low as 1-10%
of the client’s bandwidth capacity.
Figure 1 shows the throughput of our test synchronized-
read application (Section 3.2) as we increase the number
of nodes it reads from, using an unmodiﬁed Linux TCP
stack. This application performs synchronized reads of 1MB
blocks of data; that is, each of N servers responds to a block
read request with 1 MB / N bytes at the same time. Even
using a high-performance switch (with its default settings),
the throughput drops drastically as the number of servers
increases, achieving only 3% of the network capacity when it
tries to stripe the blocks across all 47 servers.
To summarize, the preconditions for TCP incast collapse
are as follows:
1. High-bandwidth, low-latency networks with small switch
buﬀers;
2. Clients that issue barrier-synchronized requests in par-
allel: the client does not issue new requests until all
responses from the current request have been returned;
3. Servers that return a relatively small amount of data
per request
If precondition 2 does not hold, then a timed out ﬂow
does not stall the client from making forward progress on
other ﬂows and hence will continue to saturate the client’s
link. If precondition 3 does not hold and at least one ﬂow is
active at any time, the active ﬂows will have enough data to
send to saturate the link for 200ms—until the stalled ﬂows
retransmit and continue.
 0 100 200 300 400 500 600 700 800 900 1000 0 5 10 15 20 25 30 35 40 45Goodput (Mbps)Number of ServersNum Servers vs Goodput (Fixed Block = 1MB, buffer = 64KB (est.), Switch = S50)200ms RTOmin3042.2 Latency-sensitive Applications
While the focus of this work is on the throughput collapse
observed for synchronized reads and writes, the imbalance be-
tween the TCP RTOmin and datacenter latencies can result
in poor performance for applications sensitive to millisecond
delays in query response time. In an interactive search query
where a client requests data from dozens of servers in parallel,
any ﬂow that experiences a timeout will be delayed by 200ms.
If the client cannot make forward progress (i.e., present re-
sults to the user) until all results are received, the entire
request will be stalled for a minimum of 200ms, resulting in
poor query latency.
To demonstrate this, we performed the following experi-
ment: we started ten bulk-data transfer TCP ﬂows from ten
clients to one server. We then had another client issue small
request packets for 1KB of data from the server, waiting for
the response before sending the next request. Approximately
1% of these requests experienced a TCP timeout, delaying
the response by at least 200ms. Even without incast commu-
nication patterns, a latency-sensitive application can observe
TCP timeouts due to congested queues caused by cross-traﬃc.
The ﬁne-grained TCP retransmission techniques we use to
prevent TCP incast collapse will also beneﬁt these more
responsive latency-sensitive applications.
2.3 Prior Work
The TCP incast problem was ﬁrst termed “Incast” and
described by Nagle et al. [25] in the context of parallel
ﬁlesystems. Nagle et al. coped with TCP incast collapse
in the parallel ﬁlesystem with application-speciﬁc mecha-
nisms. Speciﬁcally, Panasas [25] limits the number of servers
simultaneously sending to one client to about 10 by judicious
choice of the ﬁle striping policies. They also cap the adver-
tised window size by reducing the default size of per-ﬂow
TCP receive buﬀers on the client to avoid incast collapse on
switches with small buﬀers. For switches with large buﬀers,
Panasas provides a mount option to increase the client’s re-
ceive buﬀer size. In contrast, this work provides a TCP-level
solution for switches with small buﬀers and many more than
10 simultaneous senders that does not require implementing
application-speciﬁc mechanisms. Also, our solution does not
require re-implementing the many features of TCP within a
UDP framework, perhaps as was the case with Facebook [12].
Prior work characterizing TCP incast collapse ended on
a somewhat down note, ﬁnding that TCP improvements—
NewReno, SACK [22], RED [13], ECN [30], Limited Trans-
mit [1], and modiﬁcations to Slow Start— sometimes in-
creased throughput, but did not substantially prevent TCP
incast collapse because the majority of timeouts were caused
by full window losses [28]. This work found three partial
solutions: First, larger switch buﬀers could delay the onset of
incast collapse (doubling the buﬀer size doubled the number
of servers that could be contacted), but at substantial dollar
cost. Second, Ethernet ﬂow control was eﬀective when the
machines were on a single switch, but was dangerous across
inter-switch trunks because of head-of-line blocking. Finally,
reducing TCP’s minimum RTO, in simulation, appeared to
allow nodes to maintain high throughput with several times
as many nodes—but was left unexplored because of practical
implementation concerns with microsecond timeouts. In this
paper, we address the practicality, eﬀectiveness and safety of
very short timeouts in depth.
3. EVALUATING THROUGHPUT WITH
FINE-GRAINED RTO
How low must the RTO be to retain high throughput under
TCP incast collapse conditions, and to how many servers
does this solution scale? We explore this question using
real-world measurements and ns-2 simulations [26], ﬁnding
that to be maximally eﬀective, the timers must operate on a
granularity close to the RTT of the network—hundreds of
microseconds or less.
3.1 Jacobson RTO Estimation