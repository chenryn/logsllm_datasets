t
t
i
k
s
o
t
n
o
s
i
r
a
p
m
o
c
n
i
e
g
a
r
e
v
o
c
  0.93
  0.92
  0.91
  0.90
  0.89
  0.88
  0.87
  0.86
  0.85
105
106
107
Bloom filter size
list
BF - 1 hash
BF - 2 hash
BF - 3 hash
BF - 4 hash
BF - 5 hash
r
e
t
t
i
k
s
o
t
n
o
s
i
r
a
p
m
o
c
n
i
e
g
a
r
e
v
o
c
  0.84
  0.82
  0.80
  0.78
  0.76
  0.74
  0.72
  0.70
  0.68
  0.66
  0.64
  0.62
105
106
107
Bloom filter size
list
BF - 1 hash
BF - 3 hash
BF - 3 hash
BF - 4 hash
BF - 5 hash
(a) Nodes
(b) Links
Fig. 3. Coverage when using Bloom ﬁlters
shown on these scales, would mean that application of Doubletree with the given
Bloom ﬁlter had discovered exactly the same set of nodes or links as had skitter.
The introduction of Doubletree, however, implies, as we found in our prior work,
a reduction in coverage with respect to skitter, and this is irrespective of whether
Bloom ﬁlters are introduced or not. The straight horizontal line labeled list in
each plot shows the coverage that is obtained with a list of (interface, destination)
pairs instead of a Bloom ﬁlter, and thus no false positives. For the parameters
used here, p = 0.05 and 50,000 destinations, the coverage using a list is 0.924 for
nodes and 0.823 for links.
The lowest level of performance is obtained below a Bloom ﬁlter size of 105,
the point at which the false positive rate is at its maximum. Note that the
lowest level of performance is not zero coverage. The ﬁrst monitor conducts
considerable exploration that is not blocked by false positives. It is only with
subsequent monitors that a false positive rate close to one stops all exploration
beyond the ﬁrst probe. Baseline coverage is 0.857 for nodes and 0.636 for links.
The goal of applying Doubletree is to reduce the load on network interfaces
in routers and, in particular, at destinations. If the introduction of Bloom ﬁl-
ters were to increase this load, it would be a matter for concern. However, as
156
B. Donnet, T. Friedman, and M. Crovella
 500
 475
 450
 425
 400
 375
 350
 325
y
c
n
a
d
n
u
d
e
r
s
s
o
r
g
list
BF - 1 hash
BF - 2 hash
BF - 3 hash
BF - 4 hash
BF - 5 hash
y
c
n
a
d
n
u
d
e
r
r
o
t
i
n
o
m
-
r
e
t
n
i
 10
 9
 8
 7
list
BF - 1 hash
BF - 2 hash
BF - 3 hash
BF - 4 hash
BF - 5 hash
 300
105
106
Bloom filter size
107
105
106
Bloom filter size
107
(a) Internal interfaces: gross
(b) Destinations: inter-monitor
Fig. 4. Redundancy on 95th percentile interfaces when using Bloom ﬁlters
Fig. 4 shows, there seems to be no such increase. For both router interfaces and
destinations, these plots show the 95th percentile of redundancy, representing
the extreme values that should prompt the greatest concern. Ordinates are plot-
ted on linear scales. The ordinates in Fig. 4(a) specify the gross redundancy on
router interfaces: that is, the total number of visits to the 95th percentile inter-
faces. In Fig. 4(b), the ordinates specify the inter-monitor redundancy on the
95th percentile destinations: that is, the number of monitors whose probes visit
the given destination, the maximum possible being 24.
That Bloom ﬁlters seem to add no additional redundancy to the process is
a good sign. It is also to be expected, as false positive results for the stop set
would tend to reduce exploration rather than increase it, as Fig. 3 has already
shown. However, it was not necessarily a foregone conclusion. False positives
introduce an element of randomness into the exploration. The fact of stopping
to explore one path artiﬁcially early could have the eﬀect of opening up other
paths to more extensive exploration. If this phenomenon is present, it does not
have a great impact.
4 Capping and Clustering
The previous section focused on one potential obstacle to the successful deploy-
ment of the Doubletree algorithm for network topology discovery: the communi-
cation overhead. This section focuses on another: the risk that probe traﬃc will
appear to destinations as a DDoS attack as the number of monitors scales up.
Doubletree already goes some way towards reducing the impact on destinations.
However, it cannot by itself cap the probing redundancy on all destinations.
That is why we suggest imposing an explicit limit on the number of monitors
that target a destination. This section proposes a manner of doing so that should
also reduce communication overhead: grouping the monitors into clusters, each
cluster targeting a subset of the overall destination set.
Improved Algorithms for Network Topology Discovery
157
As we know from our prior work, Doubletree has the eﬀect of reducing the
redundancy of probing on destinations. However, we have reason to believe that
the redundancy will still tend to grow linearly as a function of the number
of monitors. This is because probing with Doubletree starts at some distance
from each monitor. As long as that distance is not zero, there is a non-zero
probability, by deﬁnition of p (see Sec. 2), that the monitor, probing towards a
destination, will hit it on its ﬁrst probe. There is no opportunity for the global
stop set to prevent probing before this ﬁrst probe. If there are m monitors
probing towards all destinations, then the average per-destination redundancy
due to these ﬁrst probes will tend to grow as mp. To this redundancy will be
added any redundancy that results from subsequent probes, though this would
be expected to grow sublinearly or even be constant because of the application
of Doubletree’s global stop set.
There is a number of approaches to preventing the ﬁrst probe redundancy on
destinations from growing linearly with the number of monitors. One would be
to only conduct traceroutes forward from the monitors. However, as discussed
in our prior work, this approach suﬀers from considerable ineﬃciency. Another
approach would use prior topological knowledge concerning the location of the
monitor and the destination in order to set the initial probing distance so as
to avoid hitting the destination. Such an approach indeed seems viable, and is
a subject for our future work. However, there are numerous design issues that
would need to be worked out ﬁrst: Where would the topology information be
stored, and how frequently would it need to be updated? Would distances be cal-
culated on the basis of shortest paths, or using a more realistic model of routing
in the internet? Would there still be a small but constant per-monitor proba-
bility of error? A simpler approach, and one that in any case could complement
an approach based on topology, is to simply cap the number of monitors that
probe towards each destination.
If we are to cap the number of monitors per destination, we run the risk of
reduced coverage. Indeed, the results presented here show that if skitter were
to apply a cap of six monitors per destination, even while employing all 24
monitors and its full destination set, its node coverage would be 0.939 and its
link coverage just 0.791 of its normal, uncapped, coverage. However, within a
somewhat higher range of monitors per destination, the penalty associated with
capping could be smaller. Our own experience has shown that in the range up
to 24 monitors, there is a signiﬁcant marginal utility in terms of coverage for
each monitor added. We also ﬁnd that the marginal utility decreases for each
additional monitor, a phenomenon described in prior work by Barford et al. [16],
meaning that a cap at some as-yet undeﬁned point would be reasonable.
Suppose, for the sake of argument, that skitter’s August 2004 level of 24
monitors per destination is suﬃcient for almost complete probing of the network
between those 24 monitors and their half million destinations. If that level were
imposed as a cap, then it would suﬃce to have 806 monitors, each probing at the
same rate as a skitter monitor, in order to probe towards one address in each of
the 16.8 million potential globally routable /24 CIDR [17] address preﬁxes. Most
158
B. Donnet, T. Friedman, and M. Crovella
r
e
t
t
i
k
s
o
t
n
o
s
i
r
a
p
m
o
c
n
i
e
g
a
r
e
v
o
c
e
d
o
n
  1.00
  0.98
  0.96
  0.94
  0.92
  0.90
  0.88
  0.86
  0.00   0.20   0.40   0.60   0.80   1.00
p
classic DT
clustered skitter
capped skitter
clustered DT
capped DT
r
e
t
t
i
k
s
o
t
n
o
s
i
r
a
p
m