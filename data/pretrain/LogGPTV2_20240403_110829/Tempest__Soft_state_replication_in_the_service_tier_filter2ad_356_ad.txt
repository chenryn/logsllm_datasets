>.
(.)
c: 60
Q)m.....J
40
20
2
4
Services per call
6
8
Figure 8. Pet-store latency, 5 replicas each.
in particular note the CPU & 10 bound service
variance -
for 42 replicas and the 10 bound service for 56 replicas.
For this client request load, the service instances become
overloaded if we drop below 3 replicas, and hence we don't
report those values (response times are meaningless when
the service is unable to eep up with the request rate).
We evaluated the PetStore as a "cloud" of seven services
-
the six with the characteristics presented in the previ(cid:173)
ous experiments, along with another baseline "null" service
that shows the overhead caused solely by Tempest. Four
clients perform multi-service requests (half queries half up(cid:173)
dates) against the PetStore in a closed loop, each at a rate
of once every 50 milliseconds - we chose the rate so as to
not completely overload the platform and observe queueing
effects instead. A multi-service request is a set of n parallel
requests sent to n distinct PetStore services -
this is how
the PetStore's front end web page aggregates content.
Figure 7 shows response time distributions for multi(cid:173)
service requests sent to all services -
every request issued
by a front-end is sent in parallel to each of the seven ser(cid:173)
vices, the front end returning when all replies are received.
Requests have the redundant querying parameter k=2. Each
histogram shows the number of requests per bins 10 mil-
1-4244-2398-9/08/$20.00 ©2008 IEEE
234
DSN 2008: Marian et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
liseconds wide. We show two scenarios: the one in which
none of the services is replicated, and the one in which ser(cid:173)
vices have 8 replicas each - for a yield of 7 distinct multicast
groups of 8 servers each. The graphs show that replication
provides more opportunities for queries to be absorbed by
load balancing -
fewer queries reach each replica.
Figure 8 shows response times for multi-service requests
(with standard error denoting the error bars). Every multi(cid:173)
request issued by a front-end chooses at random n distinct
services, where n is the number of services per query, pre(cid:173)
sented on the x-axis. We used the adaptive query algorithm
with the k parameter set to 1, 2 and 5. For baseline we used
a simple query discovery algorithm by which the first query
for a service is multicast, and all subsequent queries are sent
to the one replica that replied the fastest to the multicast. We
conclude that adaptive redundant querying does indeed im(cid:173)
prove performance when replicas are not overloaded, with
the largest payoff for k = 2.
5 Related Work
Ama on's Dynamo [10] provides a highly available ey(cid:173)
value persistent storage system. Dynamo also sacrifices
consistency for availability and uses object versioning and
application-assisted techniques for conflict resolution. Dn(cid:173)
li e Tempest, where data structures are fully replicated, Dy(cid:173)
namo wor s li e a ero-hop distributed hash table (DHT)
with data replicated over the N predecessor nodes.
Li e Tempest, Sinfonia [1] introduces a set of abstrac(cid:173)
tions that support building scalable distributed systems.
However Sinfonia replaces the message passing model by
providing a distributed shared memory abstraction. Devel(cid:173)
opers would simply design and manipulate data structures
on top of a flat, unstructured fine-grained shared address
space. At its core, Sinfonia uses a lightweight minitransac(cid:173)
tion primitive that applications use to atomically access and
conditionally modify data across distributed memory nodes.
Soft state mechanisms have been used extensively in net(cid:173)
wor protocols [38, 15], as well as in large cluster-based
services Ii e Porcupine [32] and others [17, 7, 34]. Propos(cid:173)
als exist for extending the standard web-service model to
include soft state -
a prominent example is the Grid Com(cid:173)
puting standard [16]. Recovery-oriented computing [8] is
an alternative approach to providing fast failover and avail(cid:173)
ability in the face of failures - however, it does not replace
replication as a mechanism for balancing heavy load across
multiple machines. Distributed data structures have been
proposed before [21, 25] as building bloc s for clustered
services. They follow a strictly defined consistency model:
all operations on its elements are atomic, in that any oper(cid:173)
ation completes entirely, or not at all -
however transac(cid:173)
tions across multiple elements are not supported. The wor
in [39] is very similar in spirit to Tempest, but examines
the orthogonal question of providing customi able durabil(cid:173)
ity levels through a single storage abstraction; one of these
levels is meant for soft state that needs to be replicated for
high availability. SSM [24] is a system for managing and
storaging a particular category of soft state - user session.
Clustered application servers Ii e BEA WebLogic [3],
IBM WebSphere [22], JBoss [23], to mention a few, al(cid:173)
low storage of state in special containers that are typi(cid:173)
cally stored within persistent databases. Most often than
not, these middleware solutions handle soft state using dis(cid:173)
tributed cache infrastructures, at times relying on third party
products li e Oracle Coherence [29] or GemFire Enter(cid:173)
prise [18] for example. There has been a large amount of
work in the field of fault-tolerant middleware, especially
around CORBA [2, 28, 14], but most of this wor does not
consider interaction with a database third tier. DBFarm [31]
is an architecture for scaling a core of multiple clustered
databases through the use of less reliable replicas.
Google's Bigtable [9] is a distributed storage system for
managing petabytes of structured data across thousands of
commodity nodes in a datacenter.
It relies on the Google
File System [19] to store log and data files and the Chubby
loc service [4] to store metadata. These systems address
problems orthogonal to Tempest, such as enabling high vol(cid:173)
ume computations over massive amounts of data.
6 Conclusion
Modem three-tier architectures achieve scalability and
responsiveness through the extensive use of soft state tech(cid:173)
niques in the service tier. Availability and rapid fail-over re(cid:173)
quires data replication, and Tempest provides programmers
with data structure abstractions for storing and managing
replicated soft state. Tempest scales well in ey dimensions
-
the number of front-ends contacting a service and the
number of services contacted by a front-end -
and outper(cid:173)
forms in-memory databases in realistic settings. As a result,
Tempest simplifies the construction of highly responsive
systems that seamlessly mask load fluctuations and faults
from end-users.
Acknowledgements
We would li e to than our shepherd Jay Wylie for the
insightful dialog that significantly shaped the final version
of the paper and our reviewers for their extensive comments.
We than Danny Dolev for his support and guidance.
References
[1] M. K. Aguilera, A. Merchant, M. Shah, A. C. Veitch, and
C. T. Karamanolis. Sinfonia: a new paradigm for building
scalable distributed systems. In SOSP, 2007.
1-4244-2398-9/08/$20.00 ©2008 IEEE
235
DSN 2008: Marian et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
[2] R. Baldoni and C. Marchetti. Three-tier replication for FT(cid:173)
CORBA infrastructures. Software Practice and Experience,
June, 2003.
[20] 1. N. Gray. A Conversation with Werner Vogels: Learning
from the Amazon technology platform. ACM Queue, 4(4),
May 2006.
[3] BEA Systems,
Inc.
Application Server, 2003.
docs81/cluster/overview.html.
Clustering the BEA WebLogic
http://e-docs.bea.comlwls/
[4] M. Burrows. The Chubby lock service for loosely-coupled
distributed systems. In OSDI, 2006.
[5] J. Byers, J. Considine, and M. Mitzenmacher. Fast approx(cid:173)
imate reconciliation of set differences. Boston University
Computer Science Technical Report 2002-019., 2002.
[6] L. Camargos, F. Pedone, and M. Wieloch. Sprint: a middle(cid:173)
ware for high-performance transaction processing. In Euro(cid:173)
pean Conference on Computer Systems (EuroSys), 2007.
[7] G. Candea, J. Cutler, and A. Fox.
Improving availability
with recursive microreboots: a soft-state system case study.
Perform. Eval., 2004.
[8] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and
In
A. Fox. Microreboot - a technique for cheap recovery.
OSDI, 2004.
[9] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wal(cid:173)
lach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber.
Bigtable: a distributed storage system for structured data. In
OSDI, 2006.
[10] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati,
A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall,
and W. Vogels. Dynamo: Amazon's Highly Available Key(cid:173)
Value Store. In SOSP, 2007.
[11] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson,
S. Shenker, H. Sturgis, D. Swinehart, and D. Terry. Epi(cid:173)
demic algorithms for replicated database maintenance.
In
Proceedings ofthe sixth annual ACM Symposium on Princi(cid:173)
ples ofDistributed Computing, 1987.
[12] A. Demers, K. Petersen, M. Spreitzer, D. Terry, M. Theimer,
and B. Welch. The Bayou Architecture: Support for Data
Sharing among Mobile Users. In IEEE Workshop on Mobile
Computing Systems & Applications, 1994.
[13] D.1. DeWitt, R. H. Katz, F. OIken, L. D. Shapiro, M. Stone(cid:173)
Implementation Techniques for
braker, and D. A. Wood.
Main Memory Database Systems. In SIGMOD, 1984.
[14] P. Felber, X. Defago, P. Eugster, and A. Schiper. Replicat(cid:173)
ing CORBA objects: a marriage between active and passive
replication. In Second IFIP International Working Confer(cid:173)
ence on Distributed Applications and Interoperable Systems
(DAIS'99), pages 375-387, Helsinki, Finland, 1999.
[15] S. Floyd, C. Liu, S. McCanne, and L. Zhang. A reliable
multicast framework for light-weight sessions and applica(cid:173)
tion level framing. IEEE/ACM Transactions on Networking
(TON), 5(6):784-803, 1997.
[16] I. Foster, K. Czajkowski, D. Ferguson, J. Frey, S. Graham,
T. Maguire, D. Snelling, and S. Tuecke. Modeling and Man(cid:173)
aging State in Distributed Systems: The Role of OGSI and
WSRF. Proceedings ofthe IEEE, March 2005.
[17] A. Fox, S. Gribble, Y. Chawathe, E. Brewer, and P. Gauthier.
Cluster-based scalable network services. SOSP, 1997.
[18] Gemstone. GemFire Enterprise. http://www.gemstone.com
Iproducts/gemfire/enterprise.php.
[19] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google File
System. SOSP, 2003.
[21] S. D. Gribble, E. A. Brewer, J. M. Hellerstein, and D. E.
Culler. Scalable, distributed data structures for internet ser(cid:173)
vice construction. In OSDI, 2000.
[22] IBM.
WebSphere Information Integrator Q replica-
tion, 2005. http://www-128.ibm.comldeveloperworks/db2
llibrary/techarticle/dm-0503aschoff/.
[23] JBoss. http://labs.jboss.comlprojects/docs/.
[24] B. C. Ling, E. Kiciman, and A. Fox. Session state: beyond
soft state.
In Proceedings of the 1st Symposium on Net(cid:173)
worked Systems Design and Implementation (NSDI), 2004.
[25] J. MacCormick, N. Murphy, M. Najork, C. A. Thekkath, and
L. Zhou. Boxwood: Abstractions as the Foundation for Stor(cid:173)
age Infrastructure. In OSDI, 2004.
[26] T. Marian, K. Birman, and R. van Renesse. A Scalable Ser(cid:173)
vices Architecture. In Proceedings ofthe 25th IEEE Sympo(cid:173)
sium on Reliable Distributed Systems (SRDS), 2006.
[27] Y. Minsky, A. Trachtenberg, and R. Zippel. Set reconcilia(cid:173)
tion with nearly optimal communication complexity. IEEE
Transactions on Information Theory, 2003.
[28] P. Narasimhan, L. E. Moser, and P. M. Melliar-Smith.
Lessons Learned in Building a Fault-Tolerant CORBA Sys(cid:173)
tem. In Proceedings ofthe International Conference on De(cid:173)
pendable Systems and Networks, 2002.
[29] Oracle. Oracle Coherence. http://www.oracle.comlproducts/
middleware/coherence/index.html.
[30] M. Pezzini.
The Evolution of Transaction Processing in
Light of .NET and J2EE. Business Integration Journal On(cid:173)
line, November 2005.
[31] C. Plattner, G. Alonso, and M. T. Ozsu. Dbfarm: A scalable
cluster for multiple databases. In Middleware, 2006.
[32] Y. Saito, B. N. Bershad, and H. M. Levy. Manageability,
availability and performance in Porcupine: a highly scal(cid:173)
able, cluster-based mail service. In Proceedings of the 17th
ACM symposium on Operating systems principles, 1999.
[33] B. Schroeder, A. Wierman, and M. Harchol-Balter. Open vs
closed: a cautionary tale. In NSDI, 2006.
[34] K. Shen, T. Yang, L. Chu, 1. L. Holliday, D. A. Kuschner,
and H. Zhu. Neptune: scalable replication management and
programming support for cluster-based network services. In
Proceedings of the 3rd conference on USENIX Symposium
on Internet Technologies and Systems (USITS), 2001.
[35] Sun Microsystems. The Collections Framework.
http://
java.sun.comldocslbooks/tutoriallcollections/index.html.
[36] The Apache Software Foundation. Apache Axis, 2006.
http://ws.apache.org/axis/.
[37] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad,
M. Newbold, M. Hibler, C. Barb, and A. Joglekar. An inte(cid:173)
grated experimental environment for distributed systems and
networks. In OSDI, 2002.
[38] L. Zhang, S. Deering, D. Estrin, S. Shenker, and D. Zappala.
RSVP: a new resource reservation protocol. Communica(cid:173)
tions Magazine, IEEE, 40(5):116-127, 2002.
[39] X. Zhang, M. A. Hiltunen, K. Marzullo, and R. D. Schlicht(cid:173)
ing. Customizable service state durability for service ori(cid:173)
ented architectures. Sixth European Dependable Computing
Conference, 0: 119-128, 2006.
1-4244-2398-9/08/$20.00 ©2008 IEEE
236
DSN 2008: Marian et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply.