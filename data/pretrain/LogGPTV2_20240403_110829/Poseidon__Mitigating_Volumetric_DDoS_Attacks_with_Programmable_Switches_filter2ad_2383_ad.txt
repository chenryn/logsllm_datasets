ﬁnish, with the goal of ensuring correctness for legitimate
ﬂows. In the current version of our primitives, sproxy is a
good example. It maintains the difference between sequence
numbers for the synproxy-source connection (generated as
SYN cookies by the synproxy) and the destination-synproxy
connection (chosen by the destination when a connection is
established by the synproxy on behalf of a veriﬁed source).
This state is needed for sequence number translation for each
packet after the connection is established. Therefore, it must
be replicated to the servers for ﬂows from legitimate hosts.
Approach to replication. The networking community has
developed several approaches [29], [41], [84] to migrating
state across virtual machines, which we can leverage for our
problem. However, these approaches are not directly applica-
ble. First, packet processing at switches is at line rate, and
its performance is several orders of magnitude higher than
that of commodity servers / virtual machines. As a result, we
cannot apply a one-time cost operation (e.g., move states from
switches to servers via expert/import APIs when scaling) when
we want to recompile the switch with new P4 programs, since
it is almost impossible to infer all the exact state locations
immediately. Moreover, different from the scenario where
the source and destination for state migration have similar
processing power, state on the switches comes in much higher
volume. Simply replicating the states from the switch to the
server would easily overwhelm the server.
To address these problems, our ﬁrst step is to amortize
the trafﬁc overhead across a period: when state is created or
modiﬁed in switches, we replicate it to the servers. Some states
may be out-of-date in the switch if a ﬂow is terminated. This
signal is also transferred to the servers so that the relevant
states can be removed. We provide a uniﬁed interface between
the switch and server to keep the states consistent and up-
to-date, using a state replication protocol as shown in Fig. 8.
OP stands for operator, which can be a Put (state creation
Fig. 9: POSEIDON Implementation.
or update), Delete (state deletion), or other types of state
synchronization operations. SEQ is used as a sequence number
for reliable transmissions. KEY records the packet headers to
index the state, and VALUE records the value of the state. For
example, in a typical sproxy, the KEY should be the ﬁve-tuples
and the VALUE should be the sequence number difference.
Second, to avoid overwhelming the servers, we spread the
trafﬁc from a switch across a set of servers. During runtime, the
state replication trafﬁc is distributed across these servers, which
is achieved by embedding the server’s IP address into the IP
ﬁeld of the state replication packets. The mapping between the
destination server’s IP address and the KEY is stored in our
controller. When the reloading starts, the controller updates
the upstream routing table with this mapping to guarantee the
corresponding trafﬁc is steered to the correct server instances.
Note that there is a small time gap (hundreds of milliseconds)
for the P4 program to be successfully loaded. During this time
period, the suspicious trafﬁc is steered to the server clusters
for trafﬁc scrubbing, and these servers also constantly report
the established legitimate ﬂow information to the controller.
After the new P4 program takes effect, the controller updates
the routing tables again to steer the trafﬁc to the switches.
Summary. To summarize, the runtime state replication follows
the following steps: (1) When operators specify a policy,
POSEIDON identiﬁes the states that need replication. (2) At
runtime, if states are created/updated/deleted, POSEIDON gen-
erates state replication packets and steers such trafﬁc across a
set of servers. It also records the mapping between the server’s
IP address and the KEY in the controller. (3) When operators
changes the defense policy to handle new attacks, POSEIDON
updates the upstream routing according to the mapping, so as
to ensure that the legitimate trafﬁc is steered to the correct
servers. It also recompiles and reloads the new P4 programs.
Note that the entire procedure runs automatically when the PO-
SEIDON system starts, and operators only need make changes
to the high-level policies when there are dynamic attacks.
VII.
IMPLEMENTATION
We have developed a prototype implementation of POSEI-
DON, including all the primitives in §IV, a policy enforcement
9
ETHIPTCPOPSEQKEYVALUEExisting ProtocolsState Replication ProtocolsL2/L3 RoutingReserved port #put, delete, etcDDoS Defense ModulesState Replication ModuleState Packet ParserServersRuntime ReconfigurationILP SolverPoseidon Program 1· · · · · ·ResultRouting PolicyP4 CodeConfiguration FilePoseidon Program SegmentDefense PolicyControl PlaneInfrastructurecontrol  flowlegitimate trafficattacking  trafficstate replication trafficstateful Legitimate traffic in last attacking periodPoseidon Program 2Poseidon Program nEnforcementEngineP4 Analyzer & Modifierengine in §V, and the switch/server interface and the state
replication mechanisms in §VI, as shown in Fig. 9. The
primitives that can be ofﬂoaded to switches in POSEIDON are
implemented in P4 on Barefoot Toﬁno [56] switches, using
∼1800 lines of code. The corresponding parts that run on
servers are implemented in DPDK [19], with ∼3600 lines of
code in C/C++. For the primitives that cannot be ofﬂoaded into
switches (e.g., puzzle), we reuse the state-of-the-art defenses
adapted from open source systems, such as CAPTCHAs.
The policy enforcement engine is implemented in Python
with ∼600 lines of code. To translate a POSEIDON policy into
a P4 program, the engine ﬁrst partitions the POSEIDON policy
into the stateful monitors (e.g., count, aggr) and the packet
processing logic. Then these two components are translated
into different P4 program segments (e.g., registers, match-
action tables, control ﬂow) separately. Finally, the two program
segments are spliced into a complete P4 program. The library
of defense primitives (e.g., sproxy, rlimit, pass) can be directly
accessed by the policy enforcement engine when translating
the code. To support future extensions, we have also imple-
mented a script that can transform new defense actions into
the format of the library of POSEIDON actions. New actions
can therefore be added easily. The script also avoids name
conﬂicts by adding a preﬁx to variable names in the original
action code.
The switch/server interface is implemented in P4 for the
switch component and in C/C++ (using DPDK) for the server
component. On the switch side, we have a P4 analyzer module
and a P4 modiﬁer module, with ∼400 lines of Python code.
The P4 analyzer module ﬁrst extracts the states that need to be
replicated, then the P4 modiﬁer module augments the original
P4 program to support state replication. On the server side,
we implement a state parser module and a state replication
module in a separated thread using DPDK in ∼500 lines of
code. The state parser module ﬁrst parses the keys and values
from the packets, then the state replication module updates the
corresponding states in the servers.
VIII. EVALUATION
In this section, we evaluate POSEIDON with respect to the
following key questions:
•
•
•
•
How expressive is the POSEIDON language in support-
ing different defense policies (§VIII-B)?
is the POSEIDON policy placement
How efﬁcient
mechanism in terms of resource utilization (§VIII-C)?
How effective is the POSEIDON runtime manage-
ment mechanism for adapting to dynamic attacks
(§VIII-D)?
can POSEIDON mitigate
How well
in
terms of defense effectiveness, performance and cost
(§VIII-E)?
attacks,
A. Experimental Setup
1http://mawi.wide.ad.jp/mawi/samplepoint-G/2019/201908281400.html
2http://mawi.wide.ad.jp/mawi/samplepoint-F/2019/201909011400.html
10
TABLE III: Replayed workload trafﬁc.
#
T1
T2
T3
Trafﬁc Trace
ToIXP Trafﬁc1
ToISP Trafﬁc2
Enterprise Trafﬁc
Average Flow Length
Average Packet Size
165.7 packets/ﬂow
75.1 packets/ﬂow
9.5 packets/ﬂow
1253B/packet
564B/packet
622B/packet
We use a combination of a real-world testbed and trace-
driven evaluations to demonstrate the aforementioned advan-
tages. Our testbed has 10 Dell R730 servers, a Barefoot Toﬁno
switch (33 x 100 GbE) and an H3C switch. Each server is
equipped with Intel(R) Xeon(R) E5-2600 v4 CPUs (2.4 GHz,
2 NUMAs, each with 6 physical cores and 12 logic cores),
15360K L3 cache, 64G RAM and two Intel XL710 40GbE
NICs. Fig. 9 shows the setup of the eight servers, the Toﬁno
switch, and the H3C switch, which compromise the defense
infrastructure; in addition, one server acts as the controller
that translates defense policies for deployment; and another
server hosts the trafﬁc generator, which can generate normal
workloads and different types of attack trafﬁc.
The normal workload trafﬁc is collected from an online
trace dataset [60] and an enterprise,
including three types
of trafﬁc traces to cover different scenarios, as shown in
TABLE III. These traces have different ﬂow length and packet
sizes. We also use two public real-world attack traces, a SYN
ﬂood attack trace [81] and a UDP ﬂood attack trace [23]. For
the other four types of attack trafﬁc, i.e., DNS ampliﬁcation,
HTTP ﬂood, Slowloris and Elephant ﬂow, we use a specialized
DDoS trafﬁc generating tool, UFONet [21], to generate the
corresponding attack trafﬁc traces. In our experiments, we
replay these traces with DPDK Pktgen to generate high-
volume workload trafﬁc and attack trafﬁc. On our testbed, we
can ramp up the attack volume up to 40 Gbps. For larger
attacks, we use simulations.
B. Policy Expressiveness
To demonstrate the expressiveness of the POSEIDON prim-
itives, we have summarized state-of-the-art DDoS attacks
and their defense mechanisms, and presented the results in
TABLE IV. We further categorize them by different protocols.
For each protocol, there are a variety of DDoS attacks, each
targeting some speciﬁc vulnerabilities. Next, we present a
typical defense solution using POSEIDON primitives for each
DDoS attack.
ICMP Protocol. ICMP-based DDoS attacks include ICMP
ﬂood attacks and Smurf attacks. To defend against ICMP
ﬂood attacks, we can ﬁrst use the count primitive to identify
suspicious IPs that send too many ICMP echo-request packets,
and then use the rlimit primitive to rate-limit the packets from
these IPs. For other IPs, we can simply let their packets pass.
For Smurf attacks, we can use the count primitive to track
all the protected servers’ ICMP echo-request packets within a
period, and only allow ICMP echo-reply packets that have been
queried by protected servers to enter the protected network.
TCP Protocol. For TCP-based DDoS attacks, we have already
discussed typical defenses with POSEIDON primitives for SYN
ﬂood attacks and Elephant Flow attacks in §IV-B. For SYN-
ACK ﬂood attacks and ACK ﬂood attacks, we can use the
TABLE IV: State-of-the-art DDoS attacks and their corresponding defense mechanisms.
Protocol
ICMP
DDoS attack
ICMP Flood
Smurf Attack
Description
The victim servers are ﬂooded with fabricated ICMP echo-
request packets from a wide range of IP addresses
A large number of fake ICMP echo-request packets with the
victim severs’ IP address are broadcast to a large network
using an IP broadcast address
The victim servers are bombarded with fabricated SYN
requests containing fake source IP addresses
The victim servers are ﬂooded with a large number of fake
SYN-ACK packets
The victim servers are ﬂooded with fabricated ACK packets
from a wide range of IP addresses
FIN/RST
Flood
The victim servers are bombarded with fake RST or FIN
packets that do not belong to any of active connections
Typical defense solution
Rate-limit received ICMP packets from
the same address or subnet
Filter ICMP echo-reply packets that are
not queried by the victim servers
Poseidon defense
+
count
rlimit/pass
count
drop/pass
+
SYN Cookie/Proxy
Filter SYN-ACK packets that are not
queried by the victim servers
Filter ACK packets that have not been
responded by the victim servers with
SYN-ACK packets
Filter FIN/ACK packets that do not be-
long to any action connections,
then
rate-limit
received FIN/RST packets
from the same connection
Rate-limit ﬂows that send too many
bytes
Rate-limit received UDP packets from
the same address or subnet
Rate-limit received DNS requests from
the same address or subnet
Filter DNS replies that are not queried
by the victim servers
+
count
sproxy/pass/drop
+
count
pass/drop
count
pass/drop
+
+
count
rlimit/pass/drop
aggr
rlimit/pass
count
rlimit/pass
count
rlimit/pass
count
pass/drop
Filter SSDP replies that are not queried
by the victim servers
count
pass/drop
Filter QUIC replies that are not queried
by the victim servers
count
pass/drop
Filter NTP replies that are not queried
by the victim servers
Filter Memcached replies that are not
queried by the victim servers
count
pass/drop
count
pass/drop
for
client
Set
limits
CAPTCHA
Rate limit IP sources that establish nu-
merous connections but send a few bytes
sessions,
count
pass/puzzle
count/aggr
rlimit/pass
+
+
+
+
+
+
+
+
+
+
TCP
UDP
SYN Flood
SYN-ACK
Flood
ACK Flood
Elephant
Flow
UDP Flood
DNS Flood
DNS
Ampliﬁcation
Attack
SSDP DDoS
Attack
QUIC Reﬂec-
tion Attack
NTP Ampliﬁ-