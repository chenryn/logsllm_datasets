While the conflict could not have been prevented given the schema, we can mitigate it by
using the non-standard ON CONFLICT clause and just do nothing. The DO NOTHING clause
targets the primary index by default (the id column in this case). While still being fragile,
this statement is at least now idempotent.
INSERT INTO prices
VALUES (1, 11.59, '2018-12-01', '2019-01-01')
ON CONFLICT DO NOTHING;
In this case idempotency might be less useful than you think: you don’t get an error, but
you most likely won’t get the expected result either. A better solution in our example
would be specifying all columns we want to insert and not using an explicit value for the
id. Overall, we already defined a sequence and a default value for the column that
generates ids for us:
INSERT INTO prices(value, valid_from, valid_until)
VALUES (11.47, '2019-01-01', '2019-02-01'),
(11.35, '2019-02-01', '2019-03-01'),
(11.23, '2019-03-01', '2019-04-01'),
(11.11, '2019-04-01', '2019-05-01'),
(10.95, '2019-05-01', '2019-06-01');
There’s another possible cause of failure: we defined a unique key for the validity date.
For that we can actually react in way that can make sense from a business perspective.
We can insert or replace (merge) the value when a conflict on that key arises:
© Manning Publications Co. To comment go to liveBook
35
INSERT INTO prices(value, valid_from, valid_until)
VALUES (11.47, '2019-01-01', '2019-02-01')
ON CONFLICT (valid_from) -- #1
DO UPDATE SET value = excluded.value;
#1 As the table has multiple constraints (primary and unique keys), we must specify on which key the conflict mitigation shall
happen
We will revisit that topic in section "Merging data".
Of course, it is possible to use the outcome of a SELECT statement as input for the
INSERT statement. We will have a look at the anatomy of a SELECT statement shortly,
but to complete the example please use it as follows. Think of this statement as a
pipeline to the INSERT clause. It selects all the data from the file named prices.csv and
inserts them in order of appearance (you find that file inside the ch03 folder in the
repository of this book on GitHub: https://github. com/duckdb- in-action/
examples).
Listing 3.5 Inserting data from other relations
INSERT INTO prices(value, valid_from, valid_until)
SELECT * FROM 'prices.csv' src;
Let’s also fill the systems table and load the first bunch of readings before we go over to
the SELECT statement in detail. To be able to write the INSERT statement properly, we
must understand what the CSV data looks like. We will make use of the fact that we can
use DESCRIBE with any relation. In this case, a relation that is defined by reading the
CSV file:
INSTALL 'httpfs'; -- #1
LOAD 'httpfs';
DESCRIBE SELECT * FROM
'https://oedi-data-lake.s3.amazonaws.com/pvdaq/csv/systems.csv';
#1 Install the httpfs extension and load it so that we can access the url
Without specifying any type hints, systems.csv looks like this for DuckDB:
© Manning Publications Co. To comment go to liveBook
36
┌────────────────────┬─────────────┬─────────┬─────────┬─────────┬────────┐
│ column_name │ column_type │ null │ key │ default │ extra │
│ varchar │ varchar │ varchar │ varchar │ varchar │ varchar│
├────────────────────┼─────────────┼─────────┼─────────┼─────────┼────────┤
│ system_id │ BIGINT │ YES │ │ │ │
│ system_public_name │ VARCHAR │ YES │ │ │ │
│ site_id │ BIGINT │ YES │ │ │ │
│ site_public_name │ VARCHAR │ YES │ │ │ │
│ site_location │ VARCHAR │ YES │ │ │ │
│ site_latitude │ DOUBLE │ YES │ │ │ │
│ site_longitude │ DOUBLE │ YES │ │ │ │
│ site_elevation │ DOUBLE │ YES │ │ │ │
└────────────────────┴─────────────┴─────────┴─────────┴─────────┴────────┘
Using the system_id and system_public_name will do just nicely for us. However, it
turns out that there are duplicates in the file which will cause our insertion to fail. The
easiest way to filter out duplicates is applying the DISTINCT keyword in the columns
clause of the SELECT statement. This ensures a unique set over all the columns we
select.
Listing 3.6 Inserting a distinct set of rows from another table
INSTALL 'httpfs';
LOAD 'httpfs';
INSERT INTO systems(id, name)
SELECT DISTINCT system_id, system_public_name
FROM 'https://oedi-data-lake.s3.amazonaws.com/pvdaq/csv/systems.csv'
ORDER BY system_id ASC;
The systems in section 3.2.1 have been picked for specific reasons. We start with the
dataset for system 34 as it suits our requirements to begin with (having readings in a 15
minutes interval). It does have some inconsistencies to deal with: the power output is
sometimes NULL (not present) or negative. We will use a CASE expression to default
missing values to 0.
As the URL does not give a clear indication for DuckDB which type of file or structure
is behind it (like spotting an extension such as .csv or .parquet), we must use the
read_csv_auto function, as the database won’t be able to infer the correct file type.
© Manning Publications Co. To comment go to liveBook
37
Listing 3.7 Download and ingest the first set of readings
INSERT INTO readings(system_id, read_on, power)
SELECT SiteId, "Date-Time",
CASE
WHEN ac_power  0;:
┌───────────┬─────────────────────┬───────────────┐
│ system_id │ read_on │ power │
│ int32 │ timestamp │ decimal(10,3) │
├───────────┼─────────────────────┼───────────────┤
│ 34 │ 2019-08-26 05:30:00 │ 1700.000 │
│ 34 │ 2019-08-26 05:45:00 │ 3900.000 │
│ 34 │ 2019-08-26 06:00:00 │ 8300.000 │
│ · │ · │ · │
│ · │ · │ · │
│ · │ · │ · │
│ 34 │ 2019-08-26 17:30:00 │ 5200.000 │
│ 34 │ 2019-08-26 17:45:00 │ 2200.000 │
│ 34 │ 2019-08-26 18:00:00 │ 600.000 │
├───────────┴─────────────────────┴───────────────┤
│ 51 rows (6 shown) 3 columns │
└─────────────────────────────────────────────────┘
Now that we finally ingested some data into the readings table, the view
v_power_per_day created in listing 3.4 also returns data. Remember, v_power_per_day
creates daily groups and sums up their power values as shown with the output of SELECT
* FROM v_power_per_day WHERE day = '2019-08-26'.
© Manning Publications Co. To comment go to liveBook
38
┌───────────┬────────────┬────────┐
│ system_id │ day │ kWh │
│ int32 │ date │ double │
├───────────┼────────────┼────────┤
│ 34 │ 2019-08-26 │ 716.9 │
└───────────┴────────────┴────────┘
If you don’t remember the definition of the view, be sure to check it again. A view is a
great way to encapsulate logic such as truncating the date to a day and aggregating the
total value of readings on that day such as in our example.
NOTE The query is essentially the same for 2020, apart from the URL parameter. Why don’t we
generate a list of file names using the range function that acts as an inline table like this?
SELECT *
FROM (
SELECT 'https://' || years.range || '.csv' AS v
FROM range(2019,2021) years
) urls, read_csv_auto(urls.v);
While this query is theoretically correct, it does not (yet) work due to restrictions in how so-called
table functions (see chapter 4.9) are implemented in DuckDB. At the time of writing they only
accept constant parameters. Furthermore, read_csv or read_parquet learn about their
schema by looking at the input parameters and reading the given files, so there’s a chicken-and-
egg problem to be solved.
MERGING DATA
Often times you find yourself with a dataset that contains duplicates or entries that
already exists within your database. While you can certainly ignore conflicts as shown in
section 3.4.1 when your only task is to refine and clean new data, you sometimes want
to merge new data into existing data. For this purpose DuckDB offers the ON CONFLICT
DO UPDATE clause, known as MERGE INTO in other databases. In our example you might
have multiple readings from different meters for the same system, and you want to
compute the average reading. Instead of doing nothing on conflict we use a DO UPDATE
now.
© Manning Publications Co. To comment go to liveBook
39
In listing 3.8 a random reading is inserted first and then an attempt is made to insert
a reading on the same time for the same device. The second attempt will cause a
conflict, not on a primary key, but on the composed key of system_id and read_on.
With the DO UPDATE clause we specify the action to take when a conflict arises. The
update clause can update as many columns as necessary, essentially doing a merge /
upsert; complex expressions such as a CASE statement are allowed, too.
Listing 3.8 Compute new values on conflict
INSERT INTO readings(system_id, read_on, power)
VALUES (10, '2023-06-05 13:00:00', 4000);
INSERT INTO readings(system_id, read_on, power)
VALUES (10, '2023-06-05 13:00:00', 3000)
ON CONFLICT(system_id, read_on) DO UPDATE -- #1
SET power = CASE
WHEN power = 0 THEN excluded.power -- #2
ELSE (power + excluded.power) / 2 END;
#1 Here, the action is specified
#2 Columns from the original dataset can be referred to by the alias excluded
NOTE DuckDB also offers INSERT OR REPLACE and INSERT OR IGNORE as shorthand
alternatives for ON CONFLICT DO UPDATE respectively ON CONFLICT DO NOTHING.
INSERT OR REPLACE however does not have the ability to combine existing values as in the
example above nor does it allow to define the conflict target.
3.4.2 The DELETE statement
There are some outliers in the data sources we’re using. We imported a bunch of
readings that are measured on different minutes of the hour, and we just don’t want
them in our dataset. The easiest way to deal with them is to apply the DELETE statement
and get rid of them. The following DELETE statement filters the rows to be deleted
through a condition based on a negated IN operator. That operator checks the
containment of the left expression inside the set of expressions on the right-hand side.
date_part is just one of the many built-in functions of DuckDB dealing with dates and
timestamps. This one extracts a part from a timestamp, in this case, the minutes from
the read_on column:
Listing 3.9 Cleaning the ingested data
DELETE FROM readings
WHERE date_part('minute', read_on) NOT IN (0,15,30,45);
© Manning Publications Co. To comment go to liveBook
40
Sometimes, you will know about quirks and inconsistencies such as these upfront, and
you don’t have to deal with them after you ingested the data. With time-based data such
in our example, you could have written the ingesting statement utilizing the
time_bucket function. We noticed that inconsistency only after importing and think it’s
worthwhile to point this out.
3.4.3 The SELECT statement
This section is all about the SELECT statement and querying the ingested data. This
statement retrieves data as rows from the database or, if used in a nested fashion,
creates ephemeral relations. Those relations can be queried again or used to insert data
as we have already seen.
The essential clauses of a SELECT statement and their canonical order is as follows:
Listing 3.10 The structure of a SELECT statement
SELECT select_list
FROM tables
WHERE condition
GROUP BY groups
HAVING group_filter
ORDER BY order_expr
LIMIT n
There are more clauses, in both the standard and the DuckDB specific SQL dialect, and
we will discuss a couple of them in the next chapter as well. The official DuckDB
documentation has a dedicated page to the SELECT statement: https://duckdb. org/docs/
sql/statements/ select which we recommend using as a reference on how each clause of
the SELECT statement is supposed to be constructed.
We think that the following clauses are the most important to understand:
FROM in conjunction with JOIN
WHERE
GROUP BY
They define the sources of your queries, they filter not only reading queries, but also
writing queries and eventually, reshape them. They are used in many contexts, not only
in querying data. Many other clauses are easier to understand, such as ORDER for
example, which, as you’d expect from its name, puts things in order.
© Manning Publications Co. To comment go to liveBook
41
THE SELECT AND FROM CLAUSES
Every standard SQL statement that reads data starts with the SELECT clause. The
SELECT clause defines the columns or expressions that will eventually be returned as
rows. If you want to get everything from the source tables of your statement, you can
use the *.
NOTE Sometimes the SELECT clause is called a projection, choosing which columns to be
returned. Ironically, the selection of rows happens in the WHERE clause.
For us, the SELECT and FROM clauses go together while explaining them, and we could
pick either one to explain first or explain them together: The FROM clause specifies the
source of the data on which the remainder of the query should operate and for a
majority of queries, that will be one or more tables. In case there is more than one table
list in the FROM clause or the additional JOIN clause is used, we speak about joining
tables together.
The following statement will return two rows from the prices table. The LIMIT clause
we are introducing here as well limits the number of returned rows. It’s often wise to
limit the amount of data you get back in case you don’t know the underlying dataset so
that you don’t cause huge amount of network traffic or end up with an unresponsive
client.
SELECT *
FROM prices
LIMIT 2;
It will return the first two rows. Without an ORDER clause, the order is actually undefined
and might differ in your instance:
┌───────┬──────────────┬────────────┬─────────────┐
│ id │ value │ valid_from │ valid_until │
│ int32 │ decimal(5,2) │ date │ date │
├───────┼──────────────┼────────────┼─────────────┤
│ 1 │ 11.59 │ 2018-12-01 │ 2019-01-01 │
│ 10 │ 11.47 │ 2019-01-01 │ 2019-02-01 │
└───────┴──────────────┴────────────┴─────────────┘
The SQL dialect of DuckDB allows us to cut the above down to just FROM prices;
(without the limit it will return all rows, but that’s ok, we know the content of that table
from section 3.4.1).
© Manning Publications Co. To comment go to liveBook
42
THE WHERE CLAUSE
The WHERE clause allows filtering your data by adding conditions to a query. Those
conditions are built of one or more expressions. Data that is selected through a SELECT,
DELETE or UPDATE statement must match those predicates to be included in the
operations This allows you to select only a subset of the data in which you are
interested. Logically the WHERE clause is applied immediately after the FROM clause or the
preceding DELETE or UPDATE statement.
In our example we can replace that arbitrary LIMIT with a proper condition that will
include only the prices for a specific year (2000) by adding the following WHERE clause:
FROM prices
WHERE valid_from BETWEEN -- #1
'2020-01-01' AND '2020-12-31';
#1 The BETWEEN keyword is a shorthand for x >= v AND v <= x.
Based on our example data, the query will return 11 rows:
┌───────┬──────────────┬────────────┬─────────────┐
│ id │ value │ valid_from │ valid_until │
│ int32 │ decimal(5,2) │ date │ date │
├───────┼──────────────┼────────────┼─────────────┤
│ 15 │ 8.60 │ 2020-11-01 │ 2023-01-01 │
│ 17 │ 8.64 │ 2020-10-01 │ 2020-11-01 │
│ · │ · │ · │ · │
│ · │ · │ · │ · │
│ · │ · │ · │ · │
│ 25 │ 9.72 │ 2020-02-01 │ 2020-03-01 │
│ 26 │ 9.87 │ 2020-01-01 │ 2020-02-01 │
├───────┴──────────────┴────────────┴─────────────┤
│ 11 rows (4 shown) 4 columns │
└─────────────────────────────────────────────────┘
© Manning Publications Co. To comment go to liveBook
43
THE GROUP BY CLAUSE
Grouping by one or more columns generates one row of output per unique value of