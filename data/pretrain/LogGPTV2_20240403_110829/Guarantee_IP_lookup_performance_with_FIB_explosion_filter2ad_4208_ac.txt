using a 64-bit processor. Third, as the lengths of the four
bit maps B6, B12, B18, and B24 are dividable by 64, the 64
bits that any FIB update needs to modify align well with
word boundaries in on-chip memory. We implement each of
these four bit maps as an array of 64-bit unsigned integers;
thus, for any FIB update, we only need to modify one such
integer in one memory access.
4.2 Lookup Oriented Optimization
Data Structures: In SAIL B and SAIL U, the maximum
numbers of on-chip memory accesses are 24 and 4, respec-
tively. To further improve lookup speed, we need to push
nodes to fewer number of levels. On one hand, the fewer
number of levels means the fewer numbers of on-chip mem-
ory accesses, which means faster lookup. On the other hand,
pushing levels 0 ∼ 23 to 24 incurs too large on-chip memory.
To trade-oﬀ between the number of on-chip memory access-
es and the data structure size at each level, we choose two
levels: one is between 0∼23, and the other one is 24. In our
experiments, the two levels are 16 and 24. In this optimiza-
tion, by preﬁx expansion, we ﬁrst push all solid nodes on lev-
els 0 ∼ 15 to level 16; second, we push all internal nodes on
level 16 and all solid nodes on levels 17∼23 to level 24; third,
we push all internal nodes on level 24 and all solid nodes on
levels 25 ∼ 31 to level 32. We call this 3-level pushing. For
level 16, our data structure has three arrays: bit map array
B16[0..216 − 1], next hop array N16[0..216 − 1], and chunk ID
array C16[0..216 − 1], where the chunk ID starts from 1. For
level 24, our data structure has three arrays: bit map array
B24, next hop array N24, and chunk ID array C24, where the
size of each array is the number of internal nodes on level 16
times 28. For level 32, our data structure has one array: next
hop array N32, whose size is the number of internal nodes
on level 24 times 28.
Lookup Process: Given an IP address a, usinga (i,j) to
denote the integer value of the bit string of a from the i-th bit
to the j-th bit, we ﬁrst check whether B16[a(0,15)] = 1; if yes,
then the a(0,15)-th node on level 16 is a solid node and thus
the next hop for a is N16[a(0,15)]; otherwise, then the a(0,15)-
th node on level 16 is an empty node and thus we need to
continue the search on level 24, where the index is computed
43as (C16[a(0,15)] − 1) ∗ 28 + a(16,23). On level 24, denoting
(C16[a(0,15)] − 1) ∗ 28 + a(16,23) by i, the search process is
similar to level 16: we ﬁrst check whether B24[i] = 1, if yes,
then the i-th node on level 24 is a solid node and thus the
next hop for a is N24[i]; otherwise, the i-th node on level 24
is an empty node and thus the next hop must be on level 32,
to be more precise, the next hop is (C24[i]− 1)∗ 28 + a(24,31).
Figure 3 illustrates the above data structures and IP lookup
process where the three pushed levels are 2, 4, and 6. The
pseudocode for the lookup process of SAIL L is in Algorithm
2, where we use the bit maps, next hop arrays, and the chunk
ID arrays as separate arrays for generality and simplicity.
BCN [i](0,0) = 0 indicates that BCN [i](1,15) = C16[i]. Al-
though in theory for 0 ≤ i ≤ 216 − 1, C16[i] needs 16 bit-
s, practically, based on measurement from our real FIB-
s of backbone routers, 15 bits are enough for C16[i] and
8 bits for next hop; thus, BCN [i] will be 16 bits exact-
ly. For FPGA/ASIC platforms, we store BCN and B24
in on-chip memory and others in oﬀ-chip memory. For
CPU/GPU/many-core platforms, because most lookups ac-
cess both B24 and N24, we do combine B24 and N24 to BN24
so as to improve caching behavior. BN24[i] = 0 indicates
that B24[i] = 0, and we need to ﬁnd the next hop in level 32;
BN24[i] > 0 indicates that the next hop is BN24[i] = N24[i].
Trie
6
66
44
level 2
0
3
4
0
3
level 4
6
A
0
6
B
4
1
D
7
C
8
88
7
4
E
2
level 6
3
3
3
9
F G H I
(a)
8
2
2
8
J K L M
11 10 01
B2
N2
C2
B4
N4
C4
0 1
0 3
1 0
1 0
00000
4 0
0 2
22222
2
(2-1)*4
+
1 1
0 1
1 1
10
00
6 6
0 0
0 1
1 0
4 4
0 0
70
02
22222
N6
3 3
3 9
2
2 8
2222
8
(2-1)*4
1
+
(b)
Figure 3: Example data structure of SAIL L.
Algorithm 2: SAIL L
Input: Bit map arrays: B16, B24
Input: Next hop arrays: N16, N24, N32
Input: Chunk ID arrays: C16, C24
Input: a: an IP address
Output: the next hop of the longest matched preﬁx.
1 if B16[a >> 16]=1 then
return N16[a >> 16]
2
3 end
4 else if
B24[(C16[a >> 16] − 1) > 24)] then
5
return
N24[(C16[a >> 16] − 1) > 24)]
6 end
7 else
8
9 end
return N32[(C24[a >> 8] − 1) << 8 + (a&255)]
Two-dimensional Splitting: The key diﬀerence be-
tween SAIL L and prior IP lookup algorithms lies in its
two-dimensional splitting. According to our two-dimensional
splitting methodology, we should store the three arrays B16,
C16, and B24 in on-chip memory and the other four arrays
N16, N24, C24, andN 32 in oﬀ-chip memory as shown in Fig-
ure 4. We observe that for 0 ≤ i ≤ 216 − 1, B16[i] = 0 if
and only if N16[i] = 0, which holds if and only if C16[i] (cid:7)= 0.
Thus, the three arrays of B16, C16, andN 16 can be combined
into one array denoted by BCN , where for 0 ≤ i ≤ 216 − 1,
BCN [i](0,0) = 1 indicates that BCN [i](1,15) = N16[i] and
Finding prefix length
Finding next hop
Prefix length  0~24
B16 , C16, B24
N16, N24
Prefix length 25~32
C24
N32
Figure 4: Memory management for SAIL L.
of
FIB Update: Given a FIB update
insert-
ing/deleting/modifying a preﬁx, we ﬁrst modify the
trie that the router maintains on the control plane to make
the trie equivalent to the updated FIB. Note that this trie
is the one after the above 3-level pushing. Further note that
FIB update messages are sent/received on the control plane
where the pushed trie is maintained. Second, we perform
the above 3-level pushing on the updated trie for the nodes
aﬀected by the update. Third, we modify the lookup data
structures on the data plane to reﬂect the change of the
pushed trie.
SAIL L can perform FIB updates eﬃciently because of
two reasons, although one FIB update may aﬀect many trie
nodes in the worst case. First, prior studies have shown that
most FIB updates require only updates on a leaf node [41].
Second, the modiﬁcation on the lookup arrays (namely the
bit map arrays, next hop arrays, and the chunk ID arrays) is
mostly continuous, i.e., a block of a lookup array is modiﬁed.
We can use the memcpy function to eﬃciently write 64 bits
in one memory access on a 64-bit processor.
4.3 SAIL for Multiple FIBs
We now present our SAIL M algorithm for handling multi-
ple FIBs in virtual routers, which is an extension of SAIL L.
A router with virtual router capability (such as Cisco CRS-
1/16) can be conﬁgured to run multiple routing instances
where each instance has a FIB. If we build independent data
structures for diﬀerent FIBs, it will cost too much memory.
Our goal in dealing with multiple FIBs is to build one data
structure so that we can perform IP lookup on it for each
FIB. Note that our method below can be used to extend
SAIL B and SAIL U to handle multiple FIBs as well, al-
though for these two algorithms, the FIB update cost is no
longer constant for the number of on-chip memory accesses.
Data Structures: Given m FIBs F0, F1,··· , Fm−1, ﬁrst,
for each Fi (0 ≤ i ≤ m − 1), for each preﬁx p in Fi, we
1,··· ,
change its next hop ni(p) to a pair (i, ni(p)). Let F (cid:2)
and F (cid:2)
m−1 denote the resulting FIBs. Second, we build a
trie for F (cid:2)
m−1, the union of all FIBs. Note
that in this trie, a solid node may have multiple (FIB ID,
next hop) pairs. Third, we perform leaf pushing on this trie.
1 ∪ ··· ∪ F (cid:2)
0 ∪ F (cid:2)
0, F (cid:2)
44Leaf pushing means to push each solid node to some leaf
nodes [39]. After leaf pushing, every internal node is empty
and has two children nodes; furthermore, each leaf node v
corresponding to a preﬁx p is solid and has m (FIB ID,
next hop) pairs: (0, n0(p)), (1, n1(p)),··· , (m − 1, nm−1(p)),
which can be represented as an array N where N[i] =n i(p)
for 0 ≤ i ≤ m − 1. Intuitively, we overlay all individual
tries constructed from the m FIBs, stretch all tries to have
the same structure, and then perform leaf pushing on all
tries. Based on the overlay trie, we run the SAIL L lookup
algorithm. Note that in the resulting data structure, in each
next hop array N16, N24, orN 32, each element is further an
array of size m. Figure 5 shows two individual tries and the
overlay trie.
Trie 2
A:00*
C:10*
E:100*
Trie 1
A: 00*
C: 10*
G: 110*
+
A
C
D
A
C
G
E
Overlay Trie A:  00*
B:  01*
E: 100*
F: 101*
G: 110*
H: 111*
D
C
B
A
(a)                     (b)                       (c)
E F G
H
Figure 5: Example of SAIL for multiple FIBs.
Lookup Process: Regarding the IP lookup process for
multiple FIBs, given an IP address a and a FIB ID i, we
ﬁrst use SAIL L to ﬁnd the next hop array N for a. Then,
the next hop for IP address a and a FIB ID i is N[i].
Two-dimensional Splitting: Regarding memory man-
agement, SAIL M exactly follows the two-dimensional split-
ting strategy illustrated in Figure 4. We store BC16, which
is the combination of B16 and C16, andB 24 in on-chip mem-
ory, and store the rest four arrays N16, N24, C24, and N32 in
oﬀ-chip memory. The key feature of our scheme for dealing
with multiple FIBs is that the total on-chip memory need-
ed is bounded to 216 ∗ 17 + 224 = 2.13MB regardless of the
sizes, characteristics and number of FIBs. The reason that
we store BC16 and B24 in on-chip memory is that given an
IP address a, BC16 and B24 can tell us on which exact level,
16, 24, or 32 that we can ﬁnd the longest matching preﬁx
for a. If it is on level 16 or 24, then we need 1 oﬀ-chip mem-
ory access as we only need to access N16 or N24. If it is on
level 32, then we need 2 oﬀ-chip memory access as we need
to access C24 and N32. Thus, the lookup process requires 2
on-chip memory accesses (which are on BC16 and B24) and
at most 2 oﬀ-chip memory accesses.
FIB Update: Given a FIB update
insert-
ing/deleting/modifying a preﬁx, we ﬁrst modify the
overlay trie that the router maintains on the control plane
to make the resulting overlay trie equivalent to the updated
FIBs. Second, we modify the lookup data structures in the
data plane to reﬂect the change of the overlay trie.
of
5. SAIL IMPLEMENTATION
In this section, we discuss the implementation of SAIL al-
gorithms on four platforms: FPGA, CPU, GPU, and many-
core.
5.1 FPGA Implementation
We simulated SAIL B, SAIL U, and SAIL L using Xilinx
ISE 13.2 IDE. We did not simulate SAIL M because its on-
chip memory lookup process is similar to SAIL L. In our FP-
GA simulation of each algorithm, to speed up IP lookup, we
build one pipeline stage for the data structure corresponding
to each level.
We use Xilinx Virtex 7 device (model XC7VX1140T) as
the target platform. This device has 1,880 block RAMs
where each block has 36 Kb, thus the total amount of on-
chip memory is 8.26MB [1]. As this on-chip memory size is
large enough, we implement the three algorithms of SAIL B,
SAIL U, and SAIL L on this platform.