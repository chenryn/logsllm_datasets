27positive instances as opposed to negative instances, which
are instances that do not belong to the concept.
A number of algorithms have been proposed for classiﬁca-
tion, including Na¨ıve Bayes, nearest neighbour, support vec-
tor machines (SVM), logistic regression, neural networks,
decision tree induction and rule induction. They each differ
in the kind of decision boundary they deﬁne between classes,
i.e., their functional form and the set of parameters they ﬁt,
and the heuristic they employ in searching for the “optimal”
function, also known as hypothesis, within the space of
possible hypotheses as deﬁned by the functional form of the
hypotheses. Of these algorithms, given that the goal of this
paper is to learn predicates for efﬁcient detector components,
we focus on evaluatingsymbolic pattern learning algorithms,
such as decision tree induction and rule induction, as their
outputs can be represented as ﬁrst-order predicates.
The function approximation learnt (often referred to as
the model) by the classiﬁcation algorithm from training
instances needs to be evaluated to obtain a measure of
the expected accuracy of the model on previously unseen
data. Typically the accuracy of a model is measured by
the percentage of test data instances correctly classiﬁed and
hence most algorithms learn hypotheses that minimise the
number of errors. However this measure implicitly assumes
that all types of misclassiﬁcations incur an equal cost. This
is of course not always the case. For example, in a safety
critical software system, if a model incorrectly classiﬁes a
faulty state as not faulty, the cost will be a lot greater than
a not faulty state being classiﬁed as a faulty state.
In such a situation,
the predictions of the model on
a test data set is cross tabulated with the actual classes
assigned to the instances by the target function to produce
a confusion matrix (CM). Table I shows the general form
of a confusion matrix for a concept learning problem. Here
TP is the number of positives instances predicted (labelled)
as being positive instances by ˆf (known as true positives),
FN is the number of positive instances labelled negative
(known as false negatives), FP is the number of negative
instances labelled positive (known as false positives), TN
is the number of negative instances labelled negative (true
negatives), npos(nneg) are the number of positive (negative)
instances in the test data and ˆnpos(ˆnneg) are the number
of instances predicted as positive (negative). In the design
of efﬁcient detector components, we endeavour to maximise
TP and minimise FP.
Table I
CONFUSION MATRIX EXAMPLE
Actual Class
Pos.
Neg.
Marginal Sums
Pos.
TP
FP
ˆnpos
Predicted Class
Neg.
FN
TN
ˆnneg
Marginal Sum
npos
nneg
n
T P
2
F P
T N
A number of evaluation metrics have been proposed in
literature based on the confusion matrix. The most common
of these are speciﬁcity or true negative rate (
T N +F P ) and
T P +F N ). Kubat et al. [26]
sensitivity or true positive rate (
used the geometric mean of the true positive rate and true
negative rate as their evaluation metric. ROC analysis [27]
is based on a plot in two dimensions where each model is a
point deﬁned by the coordinates (1-speciﬁcity, sensitivity),
T N +F P is also referred to as the
where 1-speciﬁcity =
false positive rate. For different settings, the same algorithm
will produce multiple points on the plot. The area under
the curve (AUC) obtained by joining these points to (0,0)
and (1,1) is a common measure of expected accuracy of the
algorithm. For a single model, the simple trapezoid obtained
by connecting the coordinates (0,0), (fpr,tpr), (1,1) and (1,0)
has an area of tpr−f pr+1
, which is used as a measure of the
quality of the model. Alternatively, the Euclidean distance
from the perfect classiﬁer, which has coordinates (0,1), i.e,
fpr = 0: no false positives, tpr = 1: all true positives, may
as be used as a way of ranking individual models. An
alternative measure from information retrieval literature is
the F1 measure that combines precision (
T P +F P ) and recall
(sensitivity) by computing their harmonic mean.
T P
i
When the cost associated with a false positive is different
from that of a false negative, a more appropriate measure of
the quality of a model is the expected misclassiﬁcation cost,
rather that the expected error. This requires the deﬁnition of a
cost matrix. Assuming there are m class labels, Li, an m×m
cost matrix, C, needs to be deﬁned such that the value C(i,j)
is the cost of misclassifying an instance of class Li to the
class Lj. Clearly C(i, i) = 0 as there is no cost associated
with correctly classifying an instance. Minimising the error
is a special case of minimising misclassiﬁcation cost when
the cost matrix is deﬁned as C(i, j) = 1, where i (cid:54)= j and
C(i, i) = 0. The expected misclassiﬁcation cost is therefore
(cid:80)m
j C(i, j) ∗ CM (i, j).
deﬁned as(cid:80)m
Another assumption made by error minimisation based
concept learning algorithms is that the training data is well
balanced [28]. That is, the distribution of class labels is
approximately uniform. However there are a number of
domains such as network intrusion detection, fraud detec-
tion and software reliability where the number of positive
instances (intrusion/fraud/failure states) are much fewer than
the number of negative instances. In addition to the skewed
distribution, more often than not, it is the minority class that
is most interesting class to predict.
Two approaches have been used to address problem of
class imbalance. One is to act as if there is a higher cost as-
sociated with misclassifying instances of the minority class.
Speciﬁcally, a cost matrix can be deﬁned based on the class
imbalance and methods that aim to minimise the number of
errors used as described previously. This assumes that such a
cost matrix can be incorporated within the learning process.
For example, this may be achieved using the altered priors
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
28w(j) = V (j)
N(cid:80)
i V (i)Ni
approach proposed by Breiman et al. [29]. The alternative
is to replace error minimisation based metrics with cost
minimisation metrics when searching the hypothesis space.
However, Pazzani et al. [30] showed that using misclassi-
ﬁcation costs as a greedy selection criteria in decision tree
induction does not provide cost minimisation for the overall
model learnt. Ting et al. compared instance weighting to
using minimum expected cost criteria [31] for assigning a
label to a leaf node of a decision tree induced to minimise
errors. Experiments suggest that instance weighting is more
effective than a cost minimisation approach.
The assignment of distinct costs/weights to training ex-
amples [32] [33] [30] [31],
in effect, changes the data
distribution within the training data. The cost matrix must
be converted to a cost vector, V, which is not a trivial
exercise for multi-class classiﬁcation problems. Breiman
et al. [29] suggest using the sum of all misclassiﬁcation
costs for instances of the class, though alternatives such as
V (i) = argmaxj(C(i, j)) have also been proposed. Ting et
al. [31] assign the same weight to all instances of a particular
class, Lj, based on V(j) using the formula:
Lj and N = (cid:80)
where, Nj is the number of instances within the data labelled
i Ni. Algorithms such as C4.5 [34] can
incorporate these weights directly, as instance weights are
already used to deal with missing values.
An alternative to implicitly changing the data distribution
is to re-sample the original dataset, either by oversam-
pling the minority class and/or under-sampling the majority
class [28] [35] [36] to make the class distribution more uni-
form. A number of approaches to resampling have been in-
vestigated in literature. The most common approaches being
those of resampling with replacement and sampling without
replacement (for undersampling the majority class). Japkow-
icz [28] also experimented with some focussed sampling
approaches that oversampled from the boundary regions and
undersampled from regions far from the decision boundary
but experiments suggested little value over random sampling
approaches. Chawla et al. [37] proposed the generation of
synthetic data for minority classes along the line segment
joining an example to k minority class nearest neighbours
rather than simply sampling with replacement. Empirical
tests showed their method, called SMOTE, to outperform
simple sampling with replacement. Zadrozny et al. [38]
proposed the use of cost-proportionate rejection sampling
while Kubat and Matwin [35] suggest undersampling by
removing redundant and borderline negative examples.
One of the criticisms of the over (under) sampling ap-
proach is that it is not clear how much over (under) sampling
should be carried out. Chawla et al. [39] proposed the use
of cross validation for setting the level of over- and under-
sampling of the majority and minority classes automatically.
They showed that using such a process can improve the
accuracy of the resulting models.
V. METHODOLOGY
In this section we provide a full description of our method-
ology for the design of efﬁcient error detection mechanisms.
A. Methodology Overview
The proposed methodology is based on the premise that the
data generated during fault injection captures aspects, i.e.,
patterns, of system states that lead to system failure, as well
as states that do not. Based on these states, a machine learn-
ing algorithm will then generate error detection predicates
through learning of these patterns. However, fault injection
data are often imbalanced, in the sense that most of the
logged states will not lead to a system failure, i.e., only a
small proportion of runs lead to failure. Such an imbalance
has to be addressed for learning to be effective.
The methodology we thus propose is a four stage process.
In the ﬁrst stage, fault injection is performed on a target
system in order to generate data logs about
the system
behaviour that can then be used to learn error detection
predicates. In the second stage, we ﬁrst choose an appropri-
ate machine learning algorithms, as the data preprocessing
that needs to be performed before learning is based upon
the chosen learning algorithm. Then, a preprocessing is
performed on the data in order to (i) transform the format
of the data for analysis, (ii) address the class imbalance that
is prevalent in fault injection data sets, e.g., using Synthetic
Minority Over-sampling, and (iii) perform any operations
required to improve the effectiveness of the adopted learning
algorithm, e.g., using logarithmic mapping. In the third stage
of the methodology, the chosen learning algorithm is used
to analyse the transformed fault
injection data in order
to generate and validate a ﬁrst-attempt predicate for error
detection mechanism. In order to improve the accuracy and
completeness of the derived predicate for the error detection
mechanism, the ﬁnal step of the methodology is to vary the
parameters associated with the adopted learning algorithm in
search of an improved detection predicate. The methodology
is depicted in Figure 1 and detailed in Sections V-B-V-E.
B. Step 1: Fault Injection Analysis
The ﬁrst step of the methodology is to perform fault injection
on a target system in order to generate fault injection datasets
which capture aspects of the relationship between program
state and program behaviour/failure. The speciﬁc nature of
the fault injection performed will depend on the adopted
fault and system models, which will
in-turn depend on
the characteristics and requirements of the target system.
It should be noted that there will be a direct relationship
between the nature of the fault and system models adopted
and the nature of the predicates that can be derived. For
example, in this paper we assume a single bit-ﬂip fault
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
29Figure 1. The methodology for the generation of efﬁcient error detection mechanisms
model, which means that the set of states from which a
relationship to program behaviour can be discerned is known
and constrained. Program states not captured by the adopted
fault model will not necessarily be accounted for by the
generated error detection predicates, which means that the
representativeness of the adopted models and test cases is,
as always in fault injection, of utmost importance if the
results generated are to be relevant. A further consideration
that must be made when performing fault injection in order
to generate datasets for the generation of error detection
predicates is the location at which program state is sampled,
as this will determine the location at which the generated
predicate will be relevant and, hence, the location at which
the associated error detection mechanism will be effective.
C. Step 2: Algorithm Selection and Preprocessing
Following the generation of the fault injection datasets, we
ﬁrst choose an appropriate data mining algorithm. Here, a
symbolic pattern learning algorithm, such as decision tree
induction or rule induction, is chosen in order to derive
and evaluate a ﬁrst-order predicate over the variables whose
values were captured during fault injection analysis. The
reason for choosing symbolic machine learning algorithms
is because symbolic learning algorithms learn concepts by
constructing a symbolic expression (such as a decision tree)
that describes a class (or classes) of objects (in our case,
system states). Many such algorithms work with represen-
tations equivalent to ﬁrst order logic.
Then,
the data collected from fault
injection may be
preprocessed in order to maximise the likelihood that an
effective error detection mechanism will be generated. In
general, the motivations for this process are threefold:
1) To transform the format of the data for analysis by a
2) To address the class imbalance that is prevalent in fault
data mining algorithm.
injection data sets.
3) To perform any operations required to improve the
effectiveness of the adopted learning algorithm.
The transformation of fault injection data to a format that is
compatible with the adopted data mining analysis software
will be speciﬁc to the fault injection tool and data mining
suite used. In the case of the results presented in this paper,
the format transformation was between the logging format
of PROPANE [12] and the ARFF format used by the Weka
Data Mining suite [13].
An imbalance in class distribution, i.e., between failures
injection datasets,
is common in fault
and non-failures,
often due to the factors such as the inherent resilience
of software and the difﬁculty in inducing system failures
under a given fault model. In order for effective predicates
to be generated this imbalance must be addressed through
approaches such as undersampling and oversampling with
replacement for the minority class. Oversampling can be
viewed as a case of Synthetic Minority Over-sampling
TEchnique (SMOTE) [37]. In SMOTE, synthetic examples
are generated from each positive training instance, ti+, (the
seed instance) as follows. First the k nearest neighbours,
nit’s of ti+ are retrieved. Next r of these nearest neighbours
are chosen through sampling by replacement, where r is
the number of synthetic examples that each of the positive
training instances will contribute to the new oversampled
training data set. For example, if 300% oversampling is to
be carried out then r = 3. The synthetic data instance sij
is then generated as (cid:126)sij = (cid:126)ti+ + q.((cid:126)nij − (cid:126)ti+) where q