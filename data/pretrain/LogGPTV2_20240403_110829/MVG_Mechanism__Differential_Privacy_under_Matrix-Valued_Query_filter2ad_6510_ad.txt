matrix noise.
We make a remark here about choosing directions of the noise.
As discussed in Sec. 5, any orthonormal set of vectors can be used
as the directions. The simplest instance is the the standard basis
vectors, e.g. e1 = [1, 0, 0]T , e2 = [0, 1, 0]T , e3 = [0, 0, 1]T for R3.
6.2 Equi-Modal Directional Noise
Next, we consider the type of directional noise of which the row-
wise noise and column-wise noise are distributed identically, which
we call the equi-modal directional noise. We recommend this type
of directional noise for a symmetric query function, i.e. f (X) =
f (X)T ∈ Rm×m. This covers a wide-range of query functions in-
cluding the covariance matrix [8, 13, 27], the kernel matrix [55], the
adjacency matrix of an undirected graph [34], and the Laplacian
matrix [34]. The motivation for this recommendation is that, for
symmetric query functions, any prior information about the rows
would similarly apply to the columns, so it is reasonable to use
identical row-wise and column-wise noise.
Algorithm 2 MVG mech. w/ equi-modal directional noise.
Input: (a) privacy parameters: ϵ, δ; (b) the query function and its
sensitivity: f (X) ∈ Rm×m, s2(f ); (c) the precision allocation
strategy θ ∈ (0, 1)m : |θ|1 = 1; and (d) the m noise directions
WΣ ∈ Rm×m.
(1) Compute α and β (cf. Theorem 3).
(2) Compute the precision budget P =
(3) for i = 1, . . . , m:
i) Set pi = θi P.
ii) Compute the the ith direction’s variance, σi(Σ) = 1/√
pi.
(4) Form the diagonal matrix ΛΣ = diaд([σ1(Σ), . . . , σm(Σ)]).
(5) Derive the covariance matrix: Σ = WΣΛΣWT
Σ
(6) Draw a matrix-valued noise Z from MVGm,m(0, Σ, Σ).
Output: f (X) + Z.
√
β 2+8αϵ)2
(−β +
4α 2
.
.
Formally, this type of directional noise imposes that Ψ = Σ.
Following a similar derivation to the unimodal type, we have the
following precision budget.
Theorem 5. For the MVG mechanism with Ψ = Σ, the precision
budget is (−β +(cid:112)
β
2 + 8αϵ)2/(4α
2).
Following a similar procedure to the unimodal type, we present
Alg. 2 for the MVG mechanism with the equi-modal directional
noise. The algorithm follows the same steps as Alg. 1, except it
derives the precision budget from Theorem 5, and draws the noise
from MVGm,m(0, Σ, Σ).
6.3 Sampling from MVGm,n(0, Σ, Ψ)
One remaining question on the practical implementation of the
MVG mechanism is how to efficiently draw the noise from the
matrix-variate Gaussian distribution MVGm,n(0, Σ, Ψ). One ap-
proach to implement a sampler for MVGm,n(0, Σ, Ψ) is via the
affine transformation of samples drawn i.i.d. from the standard
normal distribution, i.e. N(0, 1). The transformation is described
by the following lemma [16].
Lemma 4. Let N ∈ Rm×n be a matrix-valued random variable
whose elements are drawn i.i.d. from the standard normal distribution
N(0, 1). Then, the matrix Z = BΣNBT
Ψ is distributed according to
Z ∼ MVGm,n(0, BΣBT
Ψ).
Σ, BΨBT
This transformation consequently allows the conversion be-
tween mn samples drawn i.i.d. from N(0, 1) and a sample drawn
from MVGm,n(0, BΣBT
Ψ). To derive BΣ and BΨ from given
Σ, BΨBT
Σ and Ψ for MVGm,n(0, Σ, Ψ), we solve the two linear equations:
= Ψ, and the solutions of these two equa-
BΣBT
Σ
tions can be acquired readily via the Cholesky decomposition or
SVD (cf. [46]). We summarize the steps for this implementation
here using SVD:
= Σ, and BΨBT
Ψ
(1) Draw mn i.i.d. samples from N(0, 1), and form a matrix N.
(2) Let BΣ = WΣΛ1/2
, where WΣ, ΛΣ and
(3) Compute the sample Z = BΣNBT
WΨ, ΛΨ are derived from SVD of Σ and Ψ, respectively.
and BΨ = WΨΛ1/2
Ψ
Σ
.
Ψ
The complexity of this method depends on that of the N(0, 1) sam-
pler used. Plus, there is an additional O(max{m
3}) complexity
from SVD [35]3. The memory needed is in the order of m
2 +mn
from the three matrices required in step (3).
2 +n
, n
3
7 EXPERIMENTAL SETUPS
We evaluate the proposed MVG mechanism on three experimental
setups and datasets. Table 2 summarizes our setups. In all experi-
ments, 100 trials are carried out and the average and 95% confidence
interval are reported.
7.1 Experiment I: Regression
7.1.1 Task and Dataset. The first experiment considers the regres-
sion application on the Liver Disorders dataset [61, 65], which
contains 5 features from the blood sample of 345 patients. We leave
out the samples from 97 patients for testing, so the private dataset
contains 248 patients. Following suggestions by Forsyth and Rada
[30], we use these features to predict the average daily alcohol
consumption. All features and teacher values are ∈ [0, 1].
7.1.2 Query Function and Evaluation Metric. We perform regres-
sion in a differentially-private manner via the identity query, i.e.
f (X) = X. Since regression involves the teacher values, we treat
them as a feature, so the query size becomes 6 × 248. We use the
kernel ridge regression (KRR) [55, 78] as the regressor, and the
root-mean-square error (RMSE) [55, 70] as the evaluation metric.
7.1.3 MVG Mechanism Design. As discussed in Sec. 6.1, Alg. 1 is
appropriate for the identity query, so we employ it for this exper-
iment. The L2-sensitivity of this query is √6 (cf. Appendix B). To
identify the informative directions to allocate the precision budget,
we implement both methods discussed in Sec. 5.4 as follows.
(a) For the method using domain knowledge (denoted MVG-1),
we refer to Alatalo et al. [3], which indicates that alanine amino-
transferase (ALT) is the most indicative feature for predicting the
alcohol consumption behavior. Additionally, from our prior expe-
rience working with regression problems, we anticipate that the
teacher value (Y) is another important feature to allocate more pre-
cision budget to. With this setup, we use the standard basis vectors
as the directions (cf. Sec. 5.4), and employ the following binary
precision allocation strategy.
• Allocate τ% of the precision budget to the two important
• Allocate the rest of the precision budget equally to the rest
features (ALT and Y) by equal amount.
of the features.
We vary τ ∈ {55, 65, . . . , 95} and report the best results.4
(b) For the method using differentially-private SVD/PCA (de-
noted MVG-2), given the total budget of {ϵ, δ} reported in Sec. 8,
we spend 0.2ϵ and 0.2δ on the derivation of the two most infor-
mative directions via the differentially-private PCA algorithm in
[8]. We specify the first two principal components as the indicative
features for a fair comparison with the method using domain knowl-
edge. The remaining 0.8ϵ and 0.8δ are then used for Alg. 1. Again,
3Note that n here is not the number of samples or records but is the dimension of the
matrix-valued query output, i.e. f (X) ∈ Rm×n.
4In the real-world deployment, this parameter selection process should also be made
private [14].
Task
Dataset
# samples N
# features M
Query f (X)
Query size
Eval. metric
MVG Alg.
Source of
directions
Exp. I
Regression
Exp. II
1st P.C.
Liver [30, 61] Movement [4]
248
6
X
6 × 248
RMSE
1
Domain
knowledge
[3] /PCA [8]
2,021
4
2
XXT /N
4 × 4
∆ρ (Eq. (4))
Data
collection
setup [4]
Exp. III
Covariance
estimation
CTG [18, 61]
2,126
21
X
21 × 2126
RSS (Eq. (5))
1
Domain
knowledge
[83]
Table 2: The three experimental setups.
for a fair comparison with the method using domain knowledge,
we use the same binary precision allocation strategy in Alg. 1 for
MVG-2.
7.2 Experiment II: 1st Principal Component
7.2.1 Task and Dataset. The second experiment considers the prob-
lem of determining the first principal component (1st P.C.) from
the principal component analysis (PCA). This is one of the most
popular problems in machine learning and differential privacy. We
only consider the first principal component for two reasons. First,
many prior works in differentially-private PCA algorithm consider
this problem or the similar problem of deriving a few major P.C. (cf.
[7, 13, 27]), so this allows us to compare our approach to the state-
of-the-art approaches of a well-studied problem. Second, in practice,
this method for deriving the 1st P.C. may be used iteratively to
derive the rest of the principal components (cf. [51]).
We use the Movement Prediction via RSS (Movement) dataset
[4], which consists of the radio signal strength measurement from
4 sensor anchors (ANC{0-3}) – corresponding to the 4 features –
from 2,021 movement samples. The feature data all have the range
of [−1, 1].
7.2.2 Query Function and Evaluation Metric. We consider the co-
variance matrix query, i.e. f (X) = 1
N XXT , and use SVD to derive
the 1st P.C. from it. Hence, the query size is 4 × 4. We adopt the
quality metric commonly used for P.C. [55] and also used by Dwork
et al. [27], i.e. the captured variance ρ. For a given P.C. v, the capture
variance by v on the covariance matrix ¯S is defined as ρ = vT ¯Sv.
To be consistent with other experiments, we report the absolute
error in ρ as deviated from the maximum ρ. It is well-established
that the maximum ρ is equal to the largest eigenvalue of ¯S (cf. [46,
Theorem 4.2.2], [82]). Hence, the metric can be written as,
∆ρ(v) = λ1 − ρ(v),
(4)
where λ1 is the largest eigenvalue of ¯S. For the ideal, non-private
case, the error would clearly be zero.
7.2.3 MVG Mechanism Design. As discussed in Sec. 6.2, Alg. 2
is appropriate for the covariance query, so we employ it for this
experiment. The L2-sensitivity of this query is 8/2021 (cf. Appendix
B). To identify the informative directions to allocate the precision
budget, we inspect the data collection setup described in [4], and
use two of the four anchors as the more informative anchors due to
their proximity to the movement path (ANC0 and ANC3). Hence,
we use the standard basis vectors as the directions (cf. Sec. 6) and
allocate more precision budget to these two features using the same
strategy as in Exp. I.
7.3 Experiment III: Covariance Estimation
7.3.1 Task and Dataset. The third experiment considers the similar
problem to Exp. II but with a different flavor. In this experiment,
we consider the task of estimating the covariance matrix from the
perturbed database. This differs from Exp. II in three ways. First,
for covariance estimation, we are interested in every P.C. Second,
as mentioned in Exp. II, many previous works do not consider
every P.C., so the previous works for comparison are different.
Third, to give a different taste of our approach, we consider the
method of input perturbation for estimating the covariance, i.e.
query the noisy database and use it to compute the covariance. We
use the Cardiotocography (CTG) dataset [18, 61], which consists of
21 features in the range of [0, 1] from 2,126 fetal samples.
7.3.2 Query Function and Evaluation Metric. We consider the method
via input perturbation, so we use the identity query, i.e. f (X) = X.
The query size is 21 × 2126. We adopt the captured variance as
the quality metric similar to Exp. II, but since we are interested
in every P.C., we consider the residual sum of square (RSS) [32]
of every P.C. This is similar to the total residual variance used by
Dwork et al. ([27, p. 5]). Formally, given the perturbed database ˜X,
˜X ˜XT . Let { ˜vi} be the set of P.C.’s
the covariance estimate is ˜S = 1
N
derived from ˜S, and the RSS is,
RSS(˜S) =
(λi − ρ(˜vi))2
,
(5)
i
where λi is the ith eigenvalue of ¯S (cf. Exp. II), and ρ(˜vi) is the
captured variance of the ith P.C. derived from ˜S. Clearly, in the
non-private case, RSS(¯S) = 0.
7.3.3 MVG Mechanism Design. Since we consider the identity
query, we employ Alg. 1 for this experiment. The L2-sensitivity
is √21 (cf. Appendix B). To identify the informative directions to
allocate the precision budget to, we refer to the domain knowledge
from Costa Santos et al. [83], which identifies three features to be
most informative, viz. fetal heart rate (FHR), %time with abnormal
short term variability (ASV), and %time with abnormal long term
variability (ALV). Hence, we use the standard basis vectors as the di-
rections and allocate more precision budget to these three features
using the same strategy as in Exp. I.
7.4 Comparison to Previous Works
Since our approach falls into the category of basic mechanism, we
compare our work to the four prior state-of-the-art basic mecha-
nisms discussed in Sec. 2.1, namely, the Laplace mechanism, the
Gaussian mechanism, the Exponential mechanism, and the JL trans-
form method.
For Exp. I and III, since we consider the identity query, the four
previous works for comparison are by Dwork et al. [24], Dwork
RMSE (×10−2)
Method
1.226
∼ 3.989
Non-private
Random guess
MVG (Alg. 2)
Gaussian (Dwork et al. [27])
JL transform (Blocki et al. [7])
Laplace (Dwork et al. [24])
ϵ
-
-
1.
1.
1.
1.
1.
δ
-