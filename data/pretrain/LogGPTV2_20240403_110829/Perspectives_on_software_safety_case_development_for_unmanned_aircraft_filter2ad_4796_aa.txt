title:Perspectives on software safety case development for unmanned aircraft
author:Ewen Denney and
Ganesh Pai and
Ibrahim Habli
Perspectives on Software Safety Case Development for Unmanned Aircraft
Ewen Denney and Ganesh Pai
SGT / NASA Ames Research Center
Moffett Field, CA 94035, USA
Email: {ewen.denney, ganesh.pai}@nasa.gov
Ibrahim Habli
Department of Computer Science
University of York, York, UK
Email: PI:EMAIL
Abstract—We describe our experience with the ongoing
development of a safety case for an unmanned aircraft system
(UAS), emphasizing autopilot software safety assurance. Our
approach combines formal and non-formal reasoning, yielding
a semi-automatically assembled safety case, in which part of
the argument for autopilot software safety is automatically
generated from formal methods. This paper provides a discus-
sion of our experiences pertaining to (a) the methodology for
creating and structuring safety arguments containing hetero-
geneous reasoning and information (b) the comprehensibility
of, and the conﬁdence in, the arguments created, and (c) the
implications of development and safety assurance processes.
The considerations for assuring aviation software safety, when
using an approach such as the one in this paper, are also
discussed in the context of the relevant standards and existing
(process-based) certiﬁcation guidelines.
Keywords-Software safety; Safety cases; Unmanned aircraft;
Formal methods; Aviation software.
I. INTRODUCTION
The approval process for deploying airborne software
largely occurs via certiﬁcation, a core activity in develop-
ing safety-critical systems. In aviation, this occurs through
demonstrating compliance with aerospace standards and
regulations (e.g., DO-178B [1]) to a government appointed
authority, e.g.,
the U.S. Federal Aviation Administration
(FAA) or the European Aviation Safety Agency (EASA).
Via prescriptive certiﬁcation, especially in civil aviation,
developers demonstrate that a software system is acceptably
safe by appealing to the satisfaction of a set of process
objectives that the safety standards require for compliance.
The means for satisfying these objectives are often tightly
deﬁned within the standards. In general, such prescriptive
safety standards offer useful guidance on “good practice”
software engineering methods, safety analysis techniques,
and the way in which factors such as independence in the
process can improve conﬁdence. However, a fundamental
limitation lies in the observation that good tools, techniques,
and methods do not necessarily lead to the achievement of
a speciﬁc level of integrity/assurance, often expressed in
terms of failure rates. A correlation between the prescribed
techniques and the failure rate of systems has not been
demonstrated [2].
Increasingly, consideration is being given to goal-based
certiﬁcation, where the corresponding standards require the
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
submission of a safety argument that communicates how
evidence, e.g., generated from testing, analysis and review,
satisﬁes claims concerning safety. This is commonly referred
to as a safety case [3]; the counterpart for software is the
software safety case [4]. The requirement for a safety case
has been central
in goal-based standards, particularly in
domains such as defense, rail, and oil & gas. The revision of
DO-178B, i.e., DO-178C, includes a new note concerning
assurance arguments1 as an alternative method to show that
system safety objectives are satisﬁed. Similarly, the interim
guidance for approving unmanned aircraft system (UAS)
operations includes the use of “alternate methods of compli-
ance”, speciﬁcally referring to a safety case with sufﬁcient
data, as a means for possible approval [6, Section 5.0].
Although there is some general guidance on the use of
safety cases in aviation [7], and for airborne software [8], to
our knowledge, there is little documented experience on their
application to UAS [9], given the existing (and upcoming)
framework for approval. We anticipate that reporting the
perspectives from developing a system/software safety case
for a UAS might, eventually, aid the formulation of such
guidance; however, that is not the main aim of this paper.
Rather, it is to examine (i) whether a safety case can
transparently and coherently communicate assurance by
reconciling heterogeneous safety-relevant information and
diverse reasoning, and (ii) the extent to which software
safety assurance can be improved by the development of an
explicit software safety case, particularly from a systems per-
spective. In this paper, we brieﬂy describe our methodology
for developing a safety case for an experimental UAS with
a focus on the safety assurance of the autopilot software.
We then report the insights gained / lessons learned from
the perspectives of safety case design, comprehensibility of
the arguments, quantitative conﬁdence assessment, and the
implications from existing processes and standards.
II. THE SWIFT UAS
We are interested particularly in assuring the safety of
the airborne system in the Swift unmanned aircraft system
(UAS) being developed at NASA Ames. The UAS comprises
1Conceptually, a safety case can be considered as a specialization of an
assurance case [5].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
the electric Swift Unmanned Aerial Vehicle (UAV), dual
redundant ground control stations (GCS) and communication
links. The UAV can ﬂy autonomously, in different modes,
e.g., computer in control (CIC), following an uploaded
or a pre-programmed nominal ﬂight plan; it can be also
controlled by a pilot on the ground. An off-nominal ﬂight
plan describes a protocol for managing contingencies, e.g.,
failures in the primary pilot system, to ensure that any crash
that may occur is “on range”.
The ﬂight software onboard the UAV is implemented as a
collection of loosely-coupled modules (e.g., the autopilot,
communication interfaces, conﬁguration scripts, etc.) that
are deﬁned on top of, and interface with, the Reﬂection
system: a multi-component, event-driven, real-time, conﬁg-
urable software framework. In turn, this system runs on top
of a physics library which itself runs on an embedded real-
time operating system. The onboard autopilot is involved, in
part, in the contingency management system (CMS); there-
fore, assuring its (functional) safety is especially relevant.
The autopilot software has a modular design and consists of
the ﬂight management system (FMS) and the controller (AP)
modules. Both of these are involved in controlling the ﬂight
surfaces, and therefore, aircraft movement.
It is important to note that the Swift UAS design, which
is far along in its development lifecycle, has incorporated
safety considerations and reuses speciﬁc functionality from a
predecessor vehicle. However, certain functionality, e.g., the
autopilot, is yet to be fully designed. This presents us with
the opportunity to inﬂuence its design via the application of
our safety assurance methodology, described subsequently.
III. SAFETY ASSURANCE
A. Methodology
Our approach (Figure 1) adopts goal-based argumentation
for linking evidence, e.g., results of software veriﬁcation, to
claims that hazards, identiﬁed from safety analysis, are miti-
gated. The system safety and safety argumentation processes
are intertwined with system development, start at the level
of the UAS, and are repeated at the software level.
Through our approach, we (i) explicitly consider diverse
evidence, reasoning, assumptions and context in the safety
analysis and argument (ii) automatically generate parts of the
software safety case from formal veriﬁcation, i.e., proofs of
correctness, thereby both integrating formal reasoning into
safety argumentation and providing a signiﬁcantly greater
level of detail in the argument for software (iii) automatically
assemble the argument for software safety into that of safety
of the system, and (iv) explicitly consider the conﬁdence that
can be placed in the safety case being put forth, through
uncertainty assessment.
We use the Goal Structuring Notation (GSN) [3] to
document
the safety case. In brief, GSN is a graphical
notation for representing arguments in terms of basic ele-
ments such as goals (claims), context, evidence, assumptions
(cid:4)(cid:18)(cid:15)(cid:16)(cid:7)(cid:11)(cid:1)(cid:4)(cid:5)(cid:8)(cid:7)(cid:16)(cid:18)(cid:1)(cid:3)(cid:14)(cid:13)(cid:6)(cid:7)(cid:15)(cid:15)(cid:1)
(cid:2)(cid:12)(cid:9)(cid:9)(cid:1)(cid:6)(cid:1)(cid:16)(cid:14)(cid:15)(cid:5)(cid:19)(cid:20)(cid:11)(cid:5)(cid:8)(cid:8)(cid:7)(cid:11)(cid:1)(cid:3)
(cid:3)(cid:27)(cid:26)(cid:16)(cid:18)(cid:28)(cid:32)(cid:1)(cid:4)(cid:27)(cid:16)(cid:33)(cid:25)(cid:18)(cid:26)(cid:32)(cid:31)
(cid:11)(cid:35)(cid:22)(cid:19)(cid:32)(cid:1)(cid:12)(cid:2)(cid:11)(cid:1)(cid:17)(cid:18)(cid:31)(cid:22)(cid:20)(cid:26)(cid:1)(cid:17)(cid:27)(cid:16)(cid:33)(cid:25)(cid:18)(cid:26)(cid:32)(cid:31)
(cid:5)(cid:2)(cid:13)(cid:1)(cid:17)(cid:18)(cid:31)(cid:22)(cid:20)(cid:26)(cid:1)(cid:17)(cid:27)(cid:16)(cid:33)(cid:25)(cid:18)(cid:26)(cid:32)(cid:31)
(cid:8)(cid:28)(cid:18)(cid:30)(cid:14)(cid:32)(cid:22)(cid:26)(cid:20)(cid:1)(cid:28)(cid:30)(cid:27)(cid:16)(cid:18)(cid:17)(cid:33)(cid:30)(cid:18)(cid:31)
(cid:38)
(cid:38)
(cid:9)(cid:30)(cid:18)(cid:24)(cid:22)(cid:25)(cid:22)(cid:26)(cid:14)(cid:30)(cid:36)(cid:1)(cid:21)(cid:14)(cid:37)(cid:14)(cid:30)(cid:17)(cid:1)(cid:24)(cid:22)(cid:31)(cid:32)
(cid:8)(cid:32)(cid:21)(cid:18)(cid:30)(cid:1)(cid:30)(cid:18)(cid:24)(cid:18)(cid:34)(cid:14)(cid:26)(cid:32)(cid:1)(cid:17)(cid:27)(cid:16)(cid:33)(cid:25)(cid:18)(cid:26)(cid:32)(cid:31)
(cid:2)
(cid:13)(cid:22)(cid:42)(cid:22)(cid:35)(cid:24)(cid:1)
(cid:14)(cid:24)(cid:25)(cid:33)(cid:37)(cid:29)(cid:26)(cid:29)(cid:23)(cid:22)(cid:37)(cid:29)(cid:34)(cid:33)
(cid:6)(cid:14)(cid:37)(cid:14)(cid:30)(cid:17)(cid:31)(cid:1)
(cid:2)
(cid:18)(cid:29)(cid:36)(cid:30)(cid:1)(cid:9)(cid:33)(cid:22)(cid:31)(cid:41)(cid:36)(cid:29)(cid:36)
(cid:19)(cid:25)(cid:39)(cid:25)(cid:35)(cid:29)(cid:37)(cid:41)
(cid:15)(cid:29)(cid:30)(cid:25)(cid:31)(cid:29)(cid:28)(cid:34)(cid:34)(cid:24)
(cid:10)(cid:22)(cid:37)(cid:25)(cid:27)(cid:34)(cid:35)(cid:29)(cid:42)(cid:22)(cid:37)(cid:29)(cid:34)(cid:33)
(cid:17)(cid:35)(cid:29)(cid:34)(cid:35)(cid:29)(cid:37)(cid:29)(cid:42)(cid:22)(cid:37)(cid:29)(cid:34)(cid:33)
(cid:6)(cid:14)(cid:37)(cid:14)(cid:30)(cid:17)(cid:31)(cid:1)(cid:35)(cid:22)(cid:32)(cid:21)(cid:1)
(cid:33)(cid:26)(cid:14)(cid:16)(cid:16)(cid:18)(cid:28)(cid:32)(cid:14)(cid:15)(cid:24)(cid:18)(cid:1)(cid:30)(cid:22)(cid:31)(cid:23)(cid:1)
(cid:2)
(cid:18)(cid:29)(cid:36)(cid:30)(cid:1)(cid:35)(cid:25)(cid:24)(cid:38)(cid:23)(cid:37)(cid:29)(cid:34)(cid:33)(cid:6)(cid:1)
(cid:16)(cid:29)(cid:37)(cid:29)(cid:27)(cid:22)(cid:37)(cid:29)(cid:34)(cid:33)
(cid:11)(cid:14)(cid:19)(cid:18)(cid:32)(cid:36)(cid:1)
(cid:10)(cid:18)(cid:29)(cid:33)(cid:22)(cid:30)(cid:18)(cid:25)(cid:18)(cid:26)(cid:32)(cid:31)(cid:1)
(cid:2)
(cid:4)(cid:5)(cid:8)(cid:7)(cid:16)(cid:18)(cid:1)(cid:2)(cid:14)(cid:9)(cid:17)(cid:11)(cid:7)(cid:12)(cid:16)(cid:5)(cid:16)(cid:10)(cid:13)(cid:12)(cid:1)
(cid:3)(cid:14)(cid:13)(cid:6)(cid:7)(cid:15)(cid:15)
(cid:1)
(cid:1)
(cid:2)(cid:19)(cid:41)(cid:36)(cid:37)(cid:25)(cid:32)(cid:1)(cid:4)(cid:1)(cid:19)(cid:34)(cid:26)(cid:37)(cid:40)(cid:22)(cid:35)(cid:25)(cid:3)(cid:1)
(cid:19)(cid:22)(cid:26)(cid:25)(cid:37)(cid:41)(cid:1)
(cid:9)(cid:35)(cid:27)(cid:38)(cid:32)(cid:25)(cid:33)(cid:37)(cid:22)(cid:37)(cid:29)(cid:34)(cid:33)
(cid:11)(cid:35)(cid:22)(cid:19)(cid:32)(cid:1)(cid:12)(cid:2)(cid:11)(cid:1)
(cid:11)(cid:14)(cid:19)(cid:18)(cid:32)(cid:36)(cid:1)(cid:3)(cid:14)(cid:31)(cid:18)
(cid:2)
(cid:11)(cid:27)(cid:33)(cid:30)(cid:16)(cid:18)(cid:31)(cid:1)(cid:27)(cid:19)(cid:1)
(cid:12)(cid:26)(cid:16)(cid:18)(cid:30)(cid:32)(cid:14)(cid:22)(cid:26)(cid:32)(cid:36)
(cid:2)
(cid:2)
(cid:12)(cid:26)(cid:16)(cid:18)(cid:30)(cid:32)(cid:14)(cid:22)(cid:26)(cid:32)(cid:36)(cid:1)
(cid:7)(cid:18)(cid:14)(cid:31)(cid:33)(cid:30)(cid:18)(cid:25)(cid:18)(cid:26)(cid:32)(cid:31)
(cid:2)
(cid:21)(cid:33)(cid:23)(cid:25)(cid:35)(cid:37)(cid:22)(cid:29)(cid:33)(cid:37)(cid:41)(cid:1)
(cid:9)(cid:36)(cid:36)(cid:25)(cid:36)(cid:36)(cid:32)(cid:25)(cid:33)(cid:37)
(cid:3)(cid:27)(cid:26)(cid:19)(cid:22)(cid:17)(cid:18)(cid:26)(cid:16)(cid:18)(cid:1)(cid:22)(cid:26)(cid:1)
(cid:11)(cid:35)(cid:22)(cid:19)(cid:32)(cid:1)(cid:12)(cid:2)(cid:11)(cid:1)(cid:11)(cid:14)(cid:19)(cid:18)(cid:32)(cid:36)(cid:1)(cid:3)(cid:14)(cid:31)(cid:18)
(cid:2)
Figure 1. Safety assurance methodology: key activities and data ﬂow.
and justiﬁcations. Arguments are created in GSN by linking
these elements using two main relationships: supported by,
and in context of, to form a goal structure. We also use the
AUTOCERT tool [10] to support the formal veriﬁcation of
software through automatic theorem proving. It produces a
proof, along with supporting logical axioms and function
speciﬁcations that have been used. This proof can be auto-
matically transformed into a safety case fragment, using a
tool that we developed in-house at NASA Ames. This tool
also automatically assembles the software safety case into
the system-level safety case. We describe how we applied
our approach to the Swift UAS subsequently in this section.
B. Safety Analysis
We base the system safety process on the framework of
a safety risk management plan [11], so as to include safety
considerations into system/software development.
The process begins with hazard identiﬁcation and risk
analysis, i.e., as preliminary hazard analysis (PHA) at the
system level; then it is repeated as subsystem hazard analysis
(SSHA) at
the software level. We applied a variety of
hazard identiﬁcation and analysis techniques such as failure
modes and effects analysis (FMEA) and fault tree analysis
(FTA). To reﬁne the hazard analysis, and to manage the
wider context of safety, i.e., safety implications arising from
interactions, and deviations in processes and procedures, we
also explicitly characterized the heterogeneous sources of
evidence, assumptions and context that could be used to
support a claim of safety, e.g., concepts of operations, op-
erating procedures, assumptions made in theoretical models
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:22:49 UTC from IEEE Xplore.  Restrictions apply. 
(of ﬂight control / aerodynamic stability), simulations and
computational models, etc.
Then, we identiﬁed unacceptable risks by categorizing
hazards based on a combination of the severity of conse-
quences, and the likelihood of occurrence. Subsequently, we
deﬁned mitigation measures/controls, and we speciﬁed the
corresponding (system and software) safety requirements.
For instance, Unintended pitch down during cruise is a
system-level hazard to which the autopilot software can
contribute by failing to correctly compute the relevant pa-
rameters, e.g., angle of attack, and/or the output values
for the ﬂight surface actuators. These are software failure
modes, and we specify the corresponding (software) safety
requirements to negate these failure modes e.g., “The auto-
pilot shall accurately compute the correct angle of attack”,
and “The autopilot shall correctly compute actuator outputs”.
C. Arguing Safety of the System