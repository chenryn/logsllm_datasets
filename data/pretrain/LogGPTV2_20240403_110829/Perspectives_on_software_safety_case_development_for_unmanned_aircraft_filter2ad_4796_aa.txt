# Perspectives on Software Safety Case Development for Unmanned Aircraft

**Authors:**
- Ewen Denney and Ganesh Pai
  - SGT / NASA Ames Research Center
  - Moffett Field, CA 94035, USA
  - Email: {ewen.denney, ganesh.pai}@nasa.gov
- Ibrahim Habli
  - Department of Computer Science
  - University of York, York, UK
  - Email: [Your Email Here]

## Abstract
This paper describes our experience with the ongoing development of a safety case for an unmanned aircraft system (UAS), with a focus on autopilot software safety assurance. Our approach integrates formal and non-formal reasoning to create a semi-automatically assembled safety case, where part of the argument for autopilot software safety is generated from formal methods. We discuss our experiences related to:
1. Methodology for creating and structuring safety arguments that incorporate heterogeneous reasoning and information.
2. Comprehensibility and confidence in the created arguments.
3. Implications of development and safety assurance processes.

We also consider the standards and existing certification guidelines relevant to assuring aviation software safety when using our approach.

**Keywords:** Software safety, Safety cases, Unmanned aircraft, Formal methods, Aviation software

## I. Introduction
The approval process for deploying airborne software largely occurs through certification, a core activity in developing safety-critical systems. In aviation, this involves demonstrating compliance with aerospace standards and regulations, such as DO-178B, to a government-appointed authority like the U.S. Federal Aviation Administration (FAA) or the European Aviation Safety Agency (EASA).

Prescriptive certification, especially in civil aviation, requires developers to demonstrate that a software system is acceptably safe by satisfying a set of process objectives defined by safety standards. While these standards offer useful guidance on "good practice" software engineering methods and safety analysis techniques, they do not guarantee a specific level of integrity/assurance, often expressed in terms of failure rates.

Goal-based certification, which is increasingly being considered, requires the submission of a safety argument that communicates how evidence, such as testing, analysis, and review, satisfies claims concerning safety. This is commonly referred to as a safety case. The requirement for a safety case is central in goal-based standards, particularly in defense, rail, and oil & gas domains. The revision of DO-178B, i.e., DO-178C, includes a new note on assurance arguments as an alternative method to show that system safety objectives are met. Similarly, interim guidance for approving UAS operations includes the use of a safety case with sufficient data as a means for possible approval.

Although there is some general guidance on the use of safety cases in aviation, there is limited documented experience on their application to UAS. This paper aims to examine whether a safety case can transparently and coherently communicate assurance by reconciling heterogeneous safety-relevant information and diverse reasoning, and the extent to which software safety assurance can be improved by the development of an explicit software safety case, particularly from a systems perspective.

## II. The Swift UAS
We are particularly interested in assuring the safety of the airborne system in the Swift unmanned aircraft system (UAS) being developed at NASA Ames. The UAS comprises the electric Swift Unmanned Aerial Vehicle (UAV), dual redundant ground control stations (GCS), and communication links. The UAV can fly autonomously in different modes, such as computer in control (CIC), following an uploaded or pre-programmed nominal flight plan, or be controlled by a pilot on the ground. An off-nominal flight plan describes a protocol for managing contingencies, such as failures in the primary pilot system, to ensure that any crash occurs within a designated range.

The flight software onboard the UAV is implemented as a collection of loosely-coupled modules, including the autopilot, communication interfaces, and configuration scripts, all built on top of the Reflection system, a multi-component, event-driven, real-time, configurable software framework. This system runs on a physics library, which in turn runs on an embedded real-time operating system. The onboard autopilot is involved in the contingency management system (CMS), making its functional safety especially relevant. The autopilot software has a modular design, consisting of the flight management system (FMS) and the controller (AP) modules, both of which control the flight surfaces and aircraft movement.

It is important to note that the Swift UAS design, which is far along in its development lifecycle, has incorporated safety considerations and reuses specific functionality from a predecessor vehicle. However, certain functionality, such as the autopilot, is yet to be fully designed, presenting an opportunity to influence its design via our safety assurance methodology.

## III. Safety Assurance

### A. Methodology
Our approach (Figure 1) adopts goal-based argumentation to link evidence, such as results of software verification, to claims that identified hazards are mitigated. The system safety and safety argumentation processes are integrated with system development, starting at the UAS level and repeated at the software level.

Through our approach, we:
1. Explicitly consider diverse evidence, reasoning, assumptions, and context in the safety analysis and argument.
2. Automatically generate parts of the software safety case from formal verification, integrating formal reasoning into safety argumentation and providing a detailed argument for software.
3. Automatically assemble the argument for software safety into the system-level safety case.
4. Explicitly assess the confidence in the safety case being put forth through uncertainty assessment.

We use the Goal Structuring Notation (GSN) to document the safety case. GSN is a graphical notation for representing arguments in terms of basic elements such as goals (claims), context, evidence, and justifications. We also use the AUTOCERT tool to support formal verification of software through automatic theorem proving. This tool produces a proof, along with supporting logical axioms and function specifications, which can be automatically transformed into a safety case fragment using an in-house tool developed at NASA Ames. This tool also automatically assembles the software safety case into the system-level safety case.

### B. Safety Analysis
We base the system safety process on the framework of a safety risk management plan to include safety considerations into system/software development. The process begins with hazard identification and risk analysis, i.e., preliminary hazard analysis (PHA) at the system level, followed by subsystem hazard analysis (SSHA) at the software level. We applied various hazard identification and analysis techniques, such as failure modes and effects analysis (FMEA) and fault tree analysis (FTA). To refine the hazard analysis and manage the broader context of safety, we explicitly characterized the heterogeneous sources of evidence, assumptions, and context that could support a claim of safety, such as concepts of operations, operating procedures, theoretical models, simulations, and computational models.

We then identified unacceptable risks by categorizing hazards based on the severity of consequences and the likelihood of occurrence. Subsequently, we defined mitigation measures/controls and specified the corresponding (system and software) safety requirements. For example, unintended pitch down during cruise is a system-level hazard that the autopilot software can contribute to by failing to correctly compute relevant parameters, such as angle of attack, and/or output values for the flight surface actuators. These are software failure modes, and we specify the corresponding (software) safety requirements to negate these failure modes, e.g., "The autopilot shall accurately compute the correct angle of attack" and "The autopilot shall correctly compute actuator outputs."

### C. Arguing Safety of the System
[Continued from the original text, with further details and discussion on the safety argumentation process.]

---

**Figure 1.** Safety assurance methodology: key activities and data flow.

---

**Note:** The figure and additional sections should be included as per the original document.