我们将使用一个快捷命令来重构我们的 Node.js 应用程序，并更新运行的容器。
运行 `docker-compose up -d --build` 去更新应用程序。这是运行 `docker-compose build` 和 `docker-compose up -d` 的快捷命令。
![docker build output](/data/attachment/album/201804/29/230014mx0ryfcj61fe60v6.png)
为了在容器中运行我们的 `load_data` 脚本，我们运行 `docker exec gs-api "node" "server/load_data.js"` 。你将看到 Elasticsearch 的状态输出 `Found 100 Books`。
这之后，脚本发生了错误退出，原因是我们调用了一个没有定义的辅助函数（`parseBookFile`）。
![docker exec output](/data/attachment/album/201804/29/230015gbm7uidbqls3m8ja.png)
#### 4.4 - 读取数据文件
接下来，我们读取元数据和每本书的内容。
在 `server/load_data.js` 中定义新函数。
```
/** Read an individual book text file, and extract the title, author, and paragraphs */
function parseBookFile (filePath) {
  // Read text file
  const book = fs.readFileSync(filePath, 'utf8')
  // Find book title and author
  const title = book.match(/^Title:\s(.+)$/m)[1]
  const authorMatch = book.match(/^Author:\s(.+)$/m)
  const author = (!authorMatch || authorMatch[1].trim() === '') ? 'Unknown Author' : authorMatch[1]
  console.log(`Reading Book - ${title} By ${author}`)
  // Find Guttenberg metadata header and footer
  const startOfBookMatch = book.match(/^\*{3}\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK.+\*{3}$/m)
  const startOfBookIndex = startOfBookMatch.index + startOfBookMatch[0].length
  const endOfBookIndex = book.match(/^\*{3}\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK.+\*{3}$/m).index
  // Clean book text and split into array of paragraphs
  const paragraphs = book
    .slice(startOfBookIndex, endOfBookIndex) // Remove Guttenberg header and footer
    .split(/\n\s+\n/g) // Split each paragraph into it's own array entry
    .map(line => line.replace(/\r\n/g, ' ').trim()) // Remove paragraph line breaks and whitespace
    .map(line => line.replace(/_/g, '')) // Guttenberg uses "_" to signify italics.  We'll remove it, since it makes the raw text look messy.
    .filter((line) => (line && line.length !== '')) // Remove empty lines
  console.log(`Parsed ${paragraphs.length} Paragraphs\n`)
  return { title, author, paragraphs }
}
```
这个函数执行几个重要的任务。
1. 从文件系统中读取书的文本。
2. 使用正则表达式（关于正则表达式，请参阅 [这篇文章](https://blog.patricktriest.com/you-should-learn-regex/) ）解析书名和作者。
3. 通过匹配 “古登堡项目” 的头部和尾部，识别书的正文内容。
4. 提取书的内容文本。
5. 分割每个段落到它的数组中。
6. 清理文本并删除空白行。
它的返回值，我们将构建一个对象，这个对象包含书名、作者、以及书中各段落的数组。
再次运行 `docker-compose up -d --build` 和 `docker exec gs-api "node" "server/load_data.js"`，你将看到输出同之前一样，在输出的末尾有三个额外的行。
![docker exec output](/data/attachment/album/201804/29/230015k9n9u41o1n1tna72.png)
成功！我们的脚本从文本文件中成功解析出了书名和作者。脚本再次以错误结束，因为到现在为止，我们还没有定义辅助函数。
#### 4.5 - 在 ES 中索引数据文件
最后一步，我们将批量上传每个段落的数组到 Elasticsearch 索引中。
在 `load_data.js` 中添加新的 `insertBookData` 函数。
```
/** Bulk index the book data in Elasticsearch */
async function insertBookData (title, author, paragraphs) {
  let bulkOps = [] // Array to store bulk operations
  // Add an index operation for each section in the book
  for (let i = 0; i  0 && i % 500 === 0) { // Do bulk insert in 500 paragraph batches
      await esConnection.client.bulk({ body: bulkOps })
      bulkOps = []
      console.log(`Indexed Paragraphs ${i - 499} - ${i}`)
    }
  }
  // Insert remainder of bulk ops array
  await esConnection.client.bulk({ body: bulkOps })
  console.log(`Indexed Paragraphs ${paragraphs.length - (bulkOps.length / 2)} - ${paragraphs.length}\n\n\n`)
}
```
这个函数将使用书名、作者和附加元数据的段落位置来索引书中的每个段落。我们通过批量操作来插入段落，它比逐个段落插入要快的多。
> 
> 我们分批索引段落，而不是一次性插入全部，是为运行这个应用程序的内存稍有点小（1.7 GB）的服务器 `search.patricktriest.com` 上做的一个重要优化。如果你的机器内存还行（4 GB 以上），你或许不用分批上传。
> 
> 
> 
运行 `docker-compose up -d --build` 和 `docker exec gs-api "node" "server/load_data.js"` 一次或多次 —— 现在你将看到前面解析的 100 本书的完整输出，并插入到了 Elasticsearch。这可能需要几分钟时间，甚至更长。
![data loading output](/data/attachment/album/201804/29/230017r89g693r36fd659r.png)
### 5 - 搜索
现在，Elasticsearch 中已经有了 100 本书了（大约有 230000 个段落），现在我们尝试搜索查询。
#### 5.0 - 简单的 HTTP 查询
首先，我们使用 Elasticsearch 的 HTTP API 对它进行直接查询。
在你的浏览器上访问这个 URL - `http://localhost:9200/library/_search?q=text:Java&pretty`
在这里，我们将执行一个极简的全文搜索，在我们的图书馆的书中查找 “Java” 这个词。
你将看到类似于下面的一个 JSON 格式的响应。
```
{
  "took" : 11,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 13,
    "max_score" : 14.259304,
    "hits" : [
      {
        "_index" : "library",
        "_type" : "novel",
        "_id" : "p_GwFWEBaZvLlaAUdQgV",
        "_score" : 14.259304,
        "_source" : {
          "author" : "Charles Darwin",
          "title" : "On the Origin of Species",
          "location" : 1080,
          "text" : "Java, plants of, 375."
        }
      },
      {
        "_index" : "library",
        "_type" : "novel",
        "_id" : "wfKwFWEBaZvLlaAUkjfk",
        "_score" : 10.186235,
        "_source" : {
          "author" : "Edgar Allan Poe",
          "title" : "The Works of Edgar Allan Poe",
          "location" : 827,
          "text" : "After many years spent in foreign travel, I sailed in the year 18-- , from the port of Batavia, in the rich and populous island of Java, on a voyage to the Archipelago of the Sunda islands. I went as passenger--having no other inducement than a kind of nervous restlessness which haunted me as a fiend."
        }
      },
      ...
    ]
  }
}
```
用 Elasticseach 的 HTTP 接口可以测试我们插入的数据是否成功，但是如果直接将这个 API 暴露给 Web 应用程序将有极大的风险。这个 API 将会暴露管理功能（比如直接添加和删除文档），最理想的情况是完全不要对外暴露它。而是写一个简单的 Node.js API 去接收来自客户端的请求，然后（在我们的本地网络中）生成一个正确的查询发送给 Elasticsearch。
#### 5.1 - 查询脚本
我们现在尝试从我们写的 Node.js 脚本中查询 Elasticsearch。
创建一个新文件，`server/search.js`。
```
const { client, index, type } = require('./connection')
module.exports = {
  /** Query ES index for the provided term */
  queryTerm (term, offset = 0) {
    const body = {
      from: offset,
      query: { match: {
        text: {
          query: term,
          operator: 'and',
          fuzziness: 'auto'
        } } },
      highlight: { fields: { text: {} } }
    }
    return client.search({ index, type, body })
  }
}
```
我们的搜索模块定义一个简单的 `search` 函数，它将使用输入的词 `match` 查询。
这是查询的字段分解 -
* `from` - 允许我们分页查询结果。默认每个查询返回 10 个结果，因此，指定 `from: 10` 将允许我们取回 10-20 的结果。
* `query` - 这里我们指定要查询的词。
* `operator` - 我们可以修改搜索行为；在本案例中，我们使用 `and` 操作去对查询中包含所有字元（要查询的词）的结果来确定优先顺序。
* `fuzziness` - 对拼写错误的容错调整，`auto` 的默认为 `fuzziness: 2`。模糊值越高，结果越需要更多校正。比如，`fuzziness: 1` 将允许以 `Patricc` 为关键字的查询中返回与 `Patrick` 匹配的结果。
* `highlights` - 为结果返回一个额外的字段，这个字段包含 HTML，以显示精确的文本字集和查询中匹配的关键词。
你可以去浏览 [Elastic Full-Text Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html)，学习如何随意调整这些参数，以进一步自定义搜索查询。
### 6 - API
为了能够从前端应用程序中访问我们的搜索功能，我们来写一个快速的 HTTP API。
#### 6.0 - API 服务器
用以下的内容替换现有的 `server/app.js` 文件。
```
const Koa = require('koa')
const Router = require('koa-router')
const joi = require('joi')
const validate = require('koa-joi-validate')
const search = require('./search')
const app = new Koa()
const router = new Router()
// Log each request to the console
app.use(async (ctx, next) => {
  const start = Date.now()
  await next()
  const ms = Date.now() - start
  console.log(`${ctx.method} ${ctx.url} - ${ms}`)
})
// Log percolated errors to the console
app.on('error', err => {
  console.error('Server Error', err)
})
// Set permissive CORS header
app.use(async (ctx, next) => {
  ctx.set('Access-Control-Allow-Origin', '*')
  return next()
})
// ADD ENDPOINTS HERE
const port = process.env.PORT || 3000
app
  .use(router.routes())
  .use(router.allowedMethods())
  .listen(port, err => {
    if (err) throw err
    console.log(`App Listening on Port ${port}`)
  })
```
这些代码将为 [Koa.js](http://koajs.com/) Node API 服务器导入服务器依赖，设置简单的日志，以及错误处理。