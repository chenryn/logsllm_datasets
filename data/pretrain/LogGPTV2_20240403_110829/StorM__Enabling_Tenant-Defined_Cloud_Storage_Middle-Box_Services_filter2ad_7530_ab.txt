reconstruction, are offered as an API to middle-boxes to
two components, packet
75
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
intercept the storage network trafﬁc from inside the middle-
box, and then interpret the iSCSI packets (if required) to
build higher-level views from the low-level data. These
high-level semantics are required to implement middle-box
services such as storage access monitors.
A. Network Splicing
The storage network and instance network, as shown in
Figure 1, are usually isolated in cloud deployments. While
this design keeps the storage networking simple, it prevents
the storage trafﬁc from using many common networking
services. To allow storage trafﬁc to beneﬁt from storage
services running inside VM middle-boxes, the storage trafﬁc
must be selectively brought into the instance network, where
it can be tunneled through the middle-boxes. To solve this
problem, we develop a novel solution that splices both
storage and instance network. We break the network splicing
problem into three sub problems – a) connection attribution,
b) selective ﬂow routing from storage to instance network,
and c) steering ﬂows through middle-boxes, and describe
our solutions below:
Connection Attribution
The StorM platform allows
tenants to selectively decide which VM’s storage trafﬁc
should be routed through speciﬁc middle-boxes, offering
various services such as monitoring, encryption/decryption,
and replication. Using each tenants’ high-level routing poli-
cies, the StorM platform automatically determines which
ﬂows should be steered through middle-boxes. To be able
to offer ﬁne-grained selection of ﬂows, StorM has to solve
the connection attribution problem.
Connection attribution refers to the process of automati-
cally identifying which VM is attached to which persistent
storage connection and sending VM I/O data to the storage
server. On the tenants’ side, the connection attribution allows
tenants to specify ﬁne-grained routing policies, e.g., asking
for middle-box services for a few select VMs or shared
among all VMs. On the provider side, it allows StorM to
distinguish one tenant’s storage trafﬁc from the other tenants
and provide isolation and security to the storage trafﬁc.
However, achieving this on-demand network splicing is
difﬁcult due to the way cloud systems such as OpenStack
set up storage connections. Cloud systems typically use
the iSCSI protocol to communicate between storage clients
(called iSCSI initiator) and the server (iSCSI target). Since
an iSCSI initiator runs on the host (compute node) instead
of tenant VMs, the connection information bears only the
host IP and port and destination IP and port, obfuscating
the VM details attached to the connection. The mapping of
the 4 tuples to the actual VM owning that storage connection
information is buried deep inside the iSCSI implementation.
To solve this problem, StorM ﬁrst gathers the information
about the virtual block devices (also known as IQN numbers)
that are attached to a tenant VM. This information is stored
in the hypervisor. This allows StorM to know which VM is
attached to which virtual device. Another mapping exists in
the system that glues virtual devices to speciﬁc storage trafﬁc
source ports. To collect this information, StorM modiﬁes
the source code of storage connection software to expose
the port number along with the IQN number. In particular,
we modiﬁed the iSCSI “Login Session” code to expose
TCP connection information [10]. These series of mappings
enable StorM to identify each storage connection, allowing
it to offer ﬁne-grained tenant policies.
Storage to Instance Network
Once we identify the
storage connections or ﬂows to be routed through middle-
boxes, StorM must selectively bring those ﬂows into the
instance network, where storage middle-boxes can process
the ﬂows. When the middle-boxes are ﬁnished processing,
StorM moves the ﬂows back to the storage network to go
to the storage server. At the surface, this may appear like a
typical routing problem, however in practice, StorM must
overcome several constraints of cloud networking while
guaranteeing the isolation and security of this trafﬁc.
StorM creates a pair of storage gateways as the ingress
and egress points for a tenant’s storage trafﬁc inside the
instance network. One storage gateway selectively sends IP-
based storage trafﬁc from the storage network to the instance
network and the other vice-versa. To ensure the isolation
and security of this trafﬁc, these two gateways are created
in a tenant’s network space (a virtual
isolated network
domain reserved to a tenant), and are thus invisible to the
outside world. StorM operates entirely within a tenant’s
isolated virtual network. Hence a trusted cloud provider will
apply existing network isolation techniques (e.g., names-
pace, security groups) to secure the tenant’s virtual network
(inaccessible to eavesdroppers without compromising the
hypervisor).
StorM provides storage trafﬁc redirection – from a VM’s
storage trafﬁc node to the ingress gateway, or from the egress
gateway to the storage node – via the conventional network
address translation (NAT) rules. StorM sets up these NAT
rules on the host running each tenant VM and the host
running the storage gateways. Since the storage network and
instance network are completely isolated, during redirection,
StorM prevents the exposure of IP addresses inside the
storage network from appearing in the instance network. To
do so, StorM applies a series of IP masquerading rules both
at the ingress and egress gateways to make the IP addresses
consistent. On the ingress gateway, the source IP address
of storage trafﬁc is translated to the ingress gateway’s
IP address via IP masquerading, while the destination IP
address is translated to the egress gateway’s IP address. As
a result, the middle-box VMs will only see the redirected
storage trafﬁc coming from the ingress gateway and going
to the egress gateway (or the reverse).
The storage gateways can be created on any compute
nodes as long as these nodes have both storage and instance
76
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
Matching rules: 
dst: target_host_ip:3260
AcƟons:
SNAT: src -> ovs1_ip:vm1_port
DNAT: dst -> ovs2_ip:3260
Matching rules: 
src: ovs1_mac:vm1_port
dst: ovs2_mac:3260
AcƟons:
mod_dst_mac:  
ovs2_mac -> mb1_mac
MB1
Matching rules: 
src: mb1_mac:vm1_port
dst: ovs2_mac:3260
AcƟons:
mod_dst_mac:  
ovs2_mac -> mb2_mac
MB2
Matching rules: 
dst: ovs2_ip:3260
AcƟons:
SNAT: src -> ovs2_host_ip:vm1_port
DNAT: dst -> target_ip:3260
Ingress
Flow rules
Flow rules
Egress
NAT
Forwarding Plane
OVS1
Forwarding Plane
OVS1’
Forwarding Plane
OVS2’
Forwarding Plane
OVS2
NAT
vol1
VM1
Flow rules
Flow rules
Matching rules: 
src: mb2_mac:3260
dst: ovs1_mac:vm1_port
AcƟons:
mod_dst_mac:  
ovs1_mac -> mb1_mac
Matching rules: 
src: ovs2_mac:3260
dst: ovs1_mac:vm1_port
AcƟons:
mod_dst_mac:  
ovs1_mac -> mb2_mac
vol1
Target
Figure 3: An example of StorM’s new forwarding plane.
network interfaces installed. To reduce the routing latency, it
is preferable to place the ingress gateway physically close to
the VM’s storage trafﬁc node, and the egress gateway close
to the storage server node.
Notice that, for all VMs on a single host, 3 ﬁelds out of the
4-tuples of the iSCSI TCP connection are the same (except
the source port). Connection attribution determines which
VM is bound to which iSCSI TCP connection established on
the host. However, the source port ﬁeld can only be known
after the connection is established. Since StorM applies NAT
rules to route ﬂows towards the storage gateway, NAT rules
which match only the 3-tuples will route all storage ﬂows
on the host towards the storage gateway.
To solve this problem, StorM introduces an atomic at-
tachment operation to VM storage volumes using storage
middle-boxes: Before attaching a volume, StorM installs the
NAT rules on the VM’s host; thus during the volume attach-
ment operation, the very ﬁrst (and the subsequent) packets
of that storage connection follow the established NAT rules.
After the connection is established, StorM removes the NAT
rules to ensure that they will not inﬂuence any following
volume attachments. Note that the removal of NAT rules
does not impact established ﬂows, which still follow their
existing NAT rules (if any). StorM uses a mutex lock to
ensure the atomic operations across all above steps. As
volume attachment (and detachment) occurs less frequently
on a compute node and is fast to ﬁnish, this mutex lock has
little impact on the overall performance of the system.
SDN-enabled Flow Steering After passing through the
storage gateways, storage trafﬁc ﬂows traverse one or more
middle-boxes selected by the tenant, offering various storage
services. StorM’s job is to steer the ﬂows through middle-
boxes and provide on-demand services which can dynam-
ically add or remove middle-boxes for an existing storage
trafﬁc ﬂow. To this end, StorM designs and develops an
SDN-based ﬂow steering method on the instance network.
StorM relies on a centralized SDN controller that controls
a set of virtual switches, to which middle-box VMs are
connected. These virtual switches as well as their associated
middle-boxes constitute the forwarding chain of a storage
trafﬁc ﬂow. StorM dynamically conﬁgures the forwarding
chain by inserting ﬂow rules in the corresponding SDN-
enabled virtual switches that match the storage trafﬁc 4-
tuples. No additional conﬁgurations are required in the
middle-boxes except enabling the IP forwarding function.
the middle-box,
As shown in Figure 3, the basic forwarding unit of the
chain consists of three components:
its
previous hop (a virtual switch, where storage trafﬁc comes
from), and the next hop (another virtual switch, where stor-
age trafﬁc goes after leaving the middle-box). For example:
for a ﬂow that has to traverse through two middle-boxes
MB1 and MB2, the ﬁrst chain {OVS1, MB1, OVS1(cid:2)} brings
the ﬂows to MB1 and the second chain {OVS1(cid:2), MB2,
OVS2(cid:2)} takes it to MB2 and then to the egress gateway.
Note that the OVS1(cid:2) is the destination hop in the ﬁrst chain,
but becomes the source hop in the second chain – this is how
StorM combines chains to setup arbitrary network paths. The
reverse path follows the same idea. By installing these ﬂow
rules in the SDN-enabled virtual switches, StorM can route
storage trafﬁc to any number of virtualized storage middle-
boxes.
B. An Efﬁcient API
With the help of network splicing, StorM brings the
storage trafﬁc to the storage middle-boxes. At this point,
the middle-boxes have to intercept and process these ﬂows
to offer various services. To this end, StorM designs highly
efﬁcient packet interception and interpretation APIs. These
APIs provide meaningful storage I/O data to services execut-
ing inside middle-boxes while avoiding as much overhead
as possible.
Since storage ﬂows passing through middle-boxes do
not target any local processes inside the middle-box, their
packets are directly sent to the FORWARD chain of the
middle-box kernel to be transmitted on the outgoing network
interface. To retrieve these packets, an intuitive but costly
approach is to setup a hook (with a callback function) along
the packets’ kernel path. We refer to this method as the
77
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
passive-relay approach.
Though simple, the passive-relay approach leads to non-
trivial overhead in intercepting ﬂows, especially for ﬂows
with heavy I/O loads. This is due to the frequent system calls
to copy packet data from the kernel to user mode – one per
packet. To optimize this process, StorM proposes an active-
relay approach. Instead of simply relaying received data to
the next hop, the active-relay approach acknowledges the
data source actively.
Essentially, the active-relay approach breaks the original
single TCP connection into two: one connects the middle-
box to the ingress gateway, and the other connects the
middle-box to the egress gateway. Any received packets
of one connection are acknowledged immediately by the
middle-box, and then sent to the other connection for the
next-hop forwarding after the packets have been processed
by the storage services. By doing so, the active-relay ap-
proach shortens the packet acknowledgment path, and thus
accelerates the packet transmission rate, which is not delayed
by the subsequent data processing and forwarding.
As the active-reply breaks one connection into two, state
inconsistency between them could happen if packets are
not properly delivered by any one of the connections. We
solve this problem by storing a copy of the received packets
in the middle-box’s non-volatile memory, until the packets
are delivered and acknowledged by their next hop(s). Some
situations (TCP connection failure, storage VM crash, and
storage media error) could also result in a data transmission
failure. While such failures may impact a tenant VM’s
storage service, they are handled by existing fault-tolerance
techniques readily deployable alongside StorM.
To establish the connection from the ingress gateway to
the middle-box, NAT rules are deployed in the middle-box
to redirect storage trafﬁc ﬂows (coming from the ingress
gateway) to a local port, where the pseudo-server process
listens. The TCP connection from the middle-box to the
egress gateway is created by the pseudo-client process,
which simply connects to the egress gateway with the
corresponding destination IP and port. Both processes pro-
vide iSCSI parsing logic, and read and write interfaces
to the storage service processes. Note that the active-relay
approach leverages the kernel’s TCP stack for data copying
between the kernel and the user space. This approach is more
efﬁcient, because the TCP handler packs several packets
together for each copy.
C. Semantics Reconstruction
Tenant VMs operate at a granularity of ﬁles and direc-
tories, but middle-boxes which are processing raw storage
data packets (e.g., iSCSI) can only observe low-level infor-
mation such as disk sectors, blocks, and inodes information.
However, many middle-boxes offering services such as mon-
itoring and IDS require access to higher-level views for their
operation. This is an instance of the semantic gap problem,
and thus StorM must reconstruct the high-level ﬁle structures
using the ﬁle metadata present in the storage packets.
StorM generates an initial high-level system view of a
ﬁle-system and supplies it to the middle-boxes when the
block device is attached to its tenant VM. This system view
describes the organization of the speciﬁc ﬁle system (e.g.,
Ext4 or NTFS) on that disk, including the layout of metadata
and raw data and the mappings between directories and
ﬁles to their data locations. Metadata accesses, such as ﬁle
creation, deletion, and renaming, may update this high-level
system view. By keeping track of metadata accesses, StorM
is able to maintain an up-to-date ﬁle system view. This
allows the middle-boxes to convert low-level data accesses
into high-level ﬁle operations, which is essential for ﬁne-
grained data reliability services, such as the data replication
middle-box developed in Section V-B3.
D. Policies
StorM’s high-level policies allow tenants to deploy virtu-
alized storage middle-boxes in a highly-customizable man-
ner. The following policies must be speciﬁed by tenants prior
to using middle-boxes: (1) which VMs and their associated
volumes will use the middle-box services, (2) the middle-
boxes’ storage service types and virtual resources (e.g.,
vCPU number and memory size), and (3) the organization of
these middle-boxes (i.e., how middle-box VMs are chained
for each volume).
StorM provides an interface for tenants to submit these
policies to the cloud provider, and the StorM platform,
accordingly, parses the policies and deploys the middle-
box services. Speciﬁcally, the platform ﬁrst provisions the
required middle-box VMs on the compute hosts with the
speciﬁed VM templates. Then, the platform retrieves the
connection attributions for each volume and generates and
installs the forwarding rules according to the organization
speciﬁcation. Lastly, StorM connects the volumes to their
VMs with the middle-box services enabled.
IV. IMPLEMENTATION
We have implemented a prototype of StorM (∼10,000
LOC) on top of OpenStack Icehouse. The new forwarding
service consists of a centralized SDN controller and monitors
installed on each compute host. The SDN controller ﬁrst
gathers related information (e.g., connection attribution),
and then generates the ﬂow rules for the virtual switches
on the ﬂow path. Next, the ﬂow rules are installed in the
virtual switches of the corresponding compute nodes by the
monitors.
The packet
interception API runs inside each storage