title:OSPREY: Recovery of Variable and Data Structure via Probabilistic
Analysis for Stripped Binary
author:Zhuo Zhang and
Yapeng Ye and
Wei You and
Guanhong Tao and
Wen-Chuan Lee and
Yonghwi Kwon and
Yousra Aafer and
Xiangyu Zhang
1
5
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
OSPREY: Recovery of Variable and Data Structure
via Probabilistic Analysis for Stripped Binary
Zhuo Zhang§, Yapeng Ye§, Wei You†∗, Guanhong Tao§, Wen-chuan Lee§,
§Purdue University, †Renmin University of China, ‡University of Virginia, (cid:107)University of Waterloo
Yonghwi Kwon‡, Yousra Aafer(cid:107), Xiangyu Zhang§
{zhan3299, ye203, taog, lee1938, xyzhang}@purdue.edu,
PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—Recovering variables and data structure information
from stripped binary is a prominent challenge in binary program
analysis. While various state-of-the-art techniques are effective
in speciﬁc settings, such effectiveness may not generalize. This
is mainly because the problem is inherently uncertain due to
the information loss in compilation. Most existing techniques
are deterministic and lack a systematic way of handling such
uncertainty. We propose a novel probabilistic technique for vari-
able and structure recovery. Random variables are introduced
to denote the likelihood of an abstract memory location having
various types and structural properties such as being a ﬁeld
of some data structure. These random variables are connected
through probabilistic constraints derived through program anal-
ysis. Solving these constraints produces the posterior probabilities
of the random variables, which essentially denote the recovery
results. Our experiments show that our technique substantially
outperforms a number of state-of-the-art systems,
including
IDA, Ghidra, Angr, and Howard. Our case studies demonstrate
the recovered information improves binary code hardening and
binary decompilation.
I. INTRODUCTION
A prominent challenge in binary program analysis is to
recognize variables, derive their types, and identify complex
array and data structure deﬁnitions. Such information is lost
during compilation, that is, variables and data structure ﬁelds
are translated to plain registers and memory locations without
any structural or type information. Variable accesses, including
those for both simple global scalar variables and complex
stack/heap data structure ﬁelds with a long reference path (e.g.,
a.b.c.d), are often uniformly compiled to dereferences of
some registers that hold a computed address. Recovering the
missing variable and structure information is of importance
for software security. Such information can be used to guide
vulnerability detection [1], legacy code hardening (e.g., adding
bound checks) [2], [3], [4], [5], executable code patching (i.e.,
applying an existing security patch to an executable) [6], and
decompilation (to understand hidden program behaviors) [7],
[8], [9]. It is also a key step in any non-trivial binary rewriting,
such as binary debloating to reduce attack surface [10].
Most binary analysis platforms have the functionalities of
variable recovery and some support of structure recovery, i.e.,
array, struct, and class recovery. Many of them, including
the most widely used IDA platform [7], hard-code a set of
∗Corresponding author
reverse engineering rules that are effective in certain scenarios
(e.g., for binaries generated by some compilers). However,
they are usually not general enough because modern compilers
are diverse and feature aggressive optimizations, which may
violate many instruction patterns that these rules rely on. A
number of systems,
including Ghidra [8], Angr [11], and
TIE [12], make use of static program analysis, such as data-
ﬂow analysis and abstract interpretation, to identify variables
and infer types. However, their underlying static analysis is
often not sufﬁciently accurate. For example, many rely on
Value Set Analysis (VSA) [13] to derive the points-to relations
at the binary level. However, VSA is known to produce a
lot of bogus information, reporting many memory accesses
potentially aliased with almost the entire address space. Some
techniques such as REWARDS [1] and Howard [14] rely on
dynamic analysis to achieve better accuracy. They need high
quality inputs to reach good coverage. Such inputs may not be
feasible in security applications. In addition, as compilation
is lossy, variable and structure recovery is inevitably uncer-
tain. Such uncertainty often yields contradicting results. For
instance, many techniques rely on speciﬁc instruction patterns
of loading base address to recognize a data structure. However,
such patterns may appear in code snippets that do not access
data structure at all (just by chance). Existing techniques lack
a systematic way of dealing with such uncertainty.
We observe that there are a large number of hints of various
kinds that can be collected to guide variable and structure
recovery, many of them have not been fully leveraged by
existing techniques, due to both the difﬁculty of precluding
bogus hints and the lack of a systematic way of integrating
them in the presence of uncertainty. For example, some of
such hints include: two objects of the same class often go
through similar data-ﬂow; two objects of the same class may
have direct data-ﬂow between their corresponding ﬁelds (due
to object copying). However, leveraging such hints requires
identifying precise data-ﬂow, which is difﬁcult, and aggregat-
ing them when there is uncertainty.
In this paper, we propose a probabilistic variable and data
structure recovery technique. It extends a recent binary abstract
interpretation infrastructure BDA [15] that has better scalabil-
ity and accuracy, to collect a large set of basic behavioral
properties of the subject binary, such as its memory access
patterns, data-ﬂow, and points-to relations. For each (abstract)
© 2021, Zhuo Zhang. Under license to IEEE.
DOI 10.1109/SP40001.2021.00051
813
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:33:02 UTC from IEEE Xplore.  Restrictions apply. 
memory location, i.e., a potential variable/data-structure-ﬁeld,
a set of random variables are introduced to denote its possible
primitive types (e.g., int, long, and pointer) and its structural
properties (e.g., being a ﬁeld of some data structure or an
element of some array). These random variables are corre-
lated through the hints collected by program analysis. For
example, two memory locations may be two elements of a
same array if they are accessed by the same instruction. This
hint can be encoded as a probabilistic constraint involving
the random variables for the two memory locations. Note that
although such hints are uncertain, the introduction of random
variables and probabilistic constraints naturally models the
uncertainty. Intuitively, a random variable may be involved
in multiple hints and hence its probability is constrained by
all those hints. All these probabilistic constraints are resolved
together to derive the posterior distribution. We develop a
customized iterative probabilistic constraint solving algorithm.
It features the capabilities of handling a large number of
random variables, constraints, and the need of updating the
constraints on-the-ﬂy (e.g., when disclosing a new array). It
also features optimizations that leverage the domain speciﬁc
modular characteristics of programs.
Our contributions are summarized as follows.
• We propose a novel probabilistic variable and data struc-
ture recovery technique that is capable of handling the
inherent uncertainty of the problem.
• We develop a set of probabilistic inference rules that
are capable of aggregating in-depth program behavioral
properties to achieve precision and good coverage in
recovery results.
• We develop an iterative and optimized probabilistic con-
straint solving technique that handles the challenges for
probabilistic inference in program analysis context.
• We develop a prototype OSPREY (recOvery of variable
and data Structure by PRobabilistic analysis for strippEd
binarY). We compare its performance with a number of
state-of-the-art techniques, including Ghidra, IDA, Angr,
and Howard, on two sets of benchmarks collected from
the literature [14], [12]. Our results show that OSPREY
outperforms them by 20.41%-56.78% in terms of preci-
sion and 11.89%-50.62% in terms of recall. For complex
variables (arrays and data structures), our improvement is
6.96%-89.05% (precision) and 46.45%-74.02% (recall).
We also conduct two case studies: using our recovered
information to (1) improve decompilation of IDA and (2)
harden stripped binaries.
II. MOTIVATION
In this section, we use an example to illustrate the limita-
tions of existing techniques and motivate our technique. Fig-
ure 1a presents the source code of a function huft_build
in gzip (lines 8-15). It is substantially simpliﬁed for the illus-
tration purpose. We also introduce a crafted main() function
(lines 5-7) which uses a predicate over a random number to
represent that the likelihood of reaching the function through
random test input generation is low (line 6). Figure 1b presents
814