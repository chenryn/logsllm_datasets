0.7
0.8
0.9
0.8
1.0
0.75
1
2
3
4
5
6
7
False Positives /day
Fig. 4. RoC curve
Fig. 5. Distribution of false positives
Figure 5 plots the histogram of false positives encountered by all the users,
over the entire two week period, as determined by p∗ = 0.6. A signiﬁcant fraction
of the population see few or no alarms in the two week period. A small handful
of users—we speculate these are the very heavy traﬃc users—see 25-30 alarms
over the entire period. To summarize the distribution, we see an average of 5.3
benign destination atoms being ﬂagged as suspicious per user in the 2 week
period. That is, the average user will have to dismiss fewer than 1 alarm every
other day when this C&C detection system is in place on the users end-host.
Since false positives are often associated with user annoyance, our method is
able to carry out the detection with an extremely low user annoyance factor.
We note that the detection results presented here reﬂect the enterprise setting
and may not generalize to other settings. P2P applications which (legitimately)
connect to a large number of destinations might conceivably increase the false
positives being experienced (we saw no p2p traﬃc in our dataset).
5.3 Detecting Botnet Attack Traﬃc
In the previous discussion, we focused on detecting C&C channels. Here we try
to understand how our method can boost the detection rates of more traditional
traﬃc feature (or volume) based anomaly detectors. Note that the whitelists
constructed in the training phase can be considered the set of ”known good
destination” for a particular host. Thus, all traﬃc going to these destinations
must be de-facto “anomaly free” and can be ﬁltered out of the traﬃc stream
being passed to a conventional anomaly detector.
The traditional anomaly detectors operate by tracking a time series of some
signiﬁcant traﬃc feature, e.g., number of outgoing connections in an interval, and
raising an alarm when this crosses a threshold. The threshold is ideally deter-
mined based on the tail of the feature’s distribution empirically derived from clean
traﬃc. To distinguish the alarms in question from those triggered by C&C desti-
nations (as discussed previously), we denote them “burst alarms”. Thus, the per-
sistence metric results in C&C alarms, and the anomaly detectors generate burst
alarms. In the experimental evaluations we describe in the following, the (aggre-
gate, without C&C ﬁltered out) botnet traﬃc was superimposed on the traﬃc of
each end-host. We point out that the botnet traces are generally shorter than the
Exploiting Temporal Persistence to Detect Covert Botnet Channels
341
user traces. To ensure that the trace overlay extends across the entire user trace,
we replicated the botnet trace as often as necessary to ﬁll up the entire testing
window of 2 weeks.
There are a number of possible traﬃc features one can track, and a larger
universe of anomaly detectors that can be deﬁned on them. In the current in-
stance, we use a simple connection count detector with a 99.9%-ile threshold.
That is, the traﬃc feature of interest is the number of outgoing connections in 1
minute intervals, and the threshold is computed as the 99.9 percentile value of
this distribution empirically computed from the training data. Speciﬁcally, we
compare the detection results across two traﬃc streams, one where the outgoing
traﬃc is ﬁltered by the whitelist and the other where it is not. By “ﬁlter” we
simply mean that all traﬃc to destinations on the whitelist is ignored and not
passed along to the anomaly detector. Note that the same deﬁnitional threshold
is used, i.e., the 99.9%-ile, but the values are diﬀerent since the time-series are
diﬀerent (one of them has whitelisted destinations ﬁltered out).
Figure 6 plots the detection rate over the entire user population. The x-axis
enumerates the botnets from Table 2 and the y-axis is the fraction of users that
generated a burst alarm for the particular botnet. The two bars correspond to the
non-ﬁltered (left column) and ﬁltered (right column) traﬃc streams, the latter
corresponding to our detection method. In the ﬁgure, we see a detection rate
of 1.0 for some of the botnets, indicating every user generated an alarm when
fed the tainted trace that included the traﬃc of (Gobot.T, AimBot-5, SpyBot-
50, storm/Peed-69). In these cases, the ﬁltering provides no added beneﬁt. This
is because the traﬃc volumes associated with these instances so egregious and
beyond the range of normal traﬃc that any reasonable detector would raise an
alarm. However, there are lots of other instances in the ﬁgure where detection
with the ﬁltered traﬃc is signiﬁcantly better. For instance, in the case of VB-666,
Fig. 6. Improvement in detection rate after ﬁltering
342
F. Giroire et al.
which is the most dramatic result, we see a ﬁve fold improvement in detection
when the traﬃc is ﬁltered. Another example, with Aimbot-25, only 27% of
the users generate an alarm in the general case, but this number grows to 85%
when the traﬃc is ﬁltered— a dramatic improvement. The intuition for why the
ﬁltering helps with the detection rate is thus: when the traﬃc to known good
destinations is ﬁltered out and the threshold recomputed, the new threshold
tracks the residual traﬃc better and oﬀers a small “gap” or range that is available
for the botnet traﬃc. That is, as long as the volume of botnet traﬃc stays inside
this gap it will fall under the threshold and be undetected. However this gap
is small enough that even a small volume tends to go beyond the usable range.
Clearly, the beneﬁt of ﬁltering the traﬃc is apparent when the botnet traﬃc
volumes are low to moderate. When the volume is large, detection is easily
carried out by any reasonably anomaly detector (even without ﬁltering). Thus,
ﬁltering traﬃc through the whitelist helps to uncover stealthier traﬃc that is
hidden inside. We carried out the same comparison for other detectors and found
the results to be consistent with what we describe here. We omit details from
these experiments for a lack of space.
Importantly, notice in the ﬁgure that for some of the botnet instances, the
detection rate does not reach 100%, even with the ﬁltering. This is possibly
because of the variability in traﬃc across users: presumably, there is a suﬃcient
gap between the traﬃc and the threshold for some users and the additional
botnet traﬃc is able to squeeze into this gap. However, even when the volume
based methods fail to carry out the detection to a complete degree, C&C alarms
are generated for every botnet trace that we have collected and tested against (as
shown in the previous discussion). Thus, even when the attack traﬃc is small
enough to go undetected by the volume detectors, the botnets are still ﬂagged by
tracking persistence. This goes to underscore how critical it is to track temporal
measures, rather than just volume, when dealing with botnets.
Thus, we demonstrate that by ﬁrst learning whitelists of good destination
atoms, and subsequently ﬁltering out the traﬃc contributions of these desti-
nation atoms, we can dramatically improve the detection of botnet associated
traﬃc. We enable more end-hosts to reliably generate alarms for this traﬃc, and
to do so earlier.
6 Discussion
In this paper, we introduced the notion of “destination atoms” and “persistence”
and subsequently described a method to detect the communication between a
bot and its command and control infrastructure. The method we describe op-
erates by associating a numeric value, quantiﬁed by persistence, with individ-
ual destination atoms. Destination atoms that are frequently contacted tend to
have higher persistence values, even if the communication volume is low. Those
with large persistence values are likely to be command and control servers. The
method works best when the C&C infrastructure has a degree of centraliza-
tion. We are also able to detect instances that are not centralized by exploiting
Exploiting Temporal Persistence to Detect Covert Botnet Channels
343
the regularity of communication to a small set of C&C destinations. However,
botnets are constantly evolving with new cloaking and obfuscation mechanisms
being developed constantly. For instance, Torpig [17] and Conﬁcker [18] are re-
cent botnets that make heavy use of domain name ﬂuxing. Here, the botnets
generate a very large number of domain names (typically based on a generat-
ing algorithm) to which connections are attempted. Such botnets are very hard
to detect with general methods which do not try to target very speciﬁc botnet
behaviors. Our method might not be successful at detecting these botnets since
the names being generated, and to which connections are attempted, are com-
pletely unrelated and cannot be grouped into a smaller set of destination atoms.
From the point of view of our detection method, these botnets seem to connect
to a large number of destination atoms with uniformly low persistence values.
However, methods have been developed in the recent past that speciﬁcally target
fast ﬂux networks [11]. This underscores the fact that a silver bullet approach
to detecting and preventing botnets is unlikely to manifest anytime soon. A va-
riety of methods focused on detecting diﬀerent aspects of a botnets behavior are
essential and must be used in conjunction to mitigate the botnet problem.
Another drawback to being a very general detection method is that the alarms
generated are probabilistic in nature. The system, in of itself, cannot ascertain
whether an alarm corresponds to an actual C&C destination or if it is benign.
Processing an alarm must necessarily involve the end-user. If the end-user were
to install a new application which begins to communicate with a particular server
everytime it is launched, the behavior is likely to trigger an alarm at some point,
which the user has to respond to. However, the act of installing a new application
occurs infrequently and we don’t believe the extra false positives due to this will
be signiﬁcant. Moreover, if our sytem were to be deployed inside an enterprise
network, the alarms will be sent to a central console and processed by trained
IT personnel; this would shift the onus away from end-users who may not be
well equipped to decide if the alarm is benign.
Our method reliably detects all the malwares experimented with and does so
with a very low false positive rate. The network traces used were collected in an
enterprise network where p2p applications are prohibited (save for Skype, which
serves a business purpose). The rate of false positives is likely to be much higher
in the presence of signiﬁcant p2p traﬃc. However, this is not really a bad result.
Some of the botnets that p2p infrastructures for the command and control are
fundamentally indistinguishable from actual p2p networks. In fact, the Storm
botnet uses a legitimate (arguably) p2p network (Overnet) to host its command
and control. One mechanism of dealing with this problem, where we want p2p
like botnets to raise an alarm, but not legitimate p2p applications, would be to
whitelist applications themselves. In this scenario, select legitimate applications
would be whitelisted and all the traﬃc that they generate is considered de-facto
legitimate; the whitelisted application would never trigger an alarm. Applications
are almost always installed manually by the end-users which implicitly implies
a trust relationship between the user and application.
344
F. Giroire et al.
7 Conclusions
In this paper, we introduced the notion of “persistence” as a temporal mea-
sure of regularity in connection to “destination atoms”, which are destination
aggregates. We then described a method that builds whitelists of known good
destination atoms in order to isolate persistent destinations in the traﬃc which
are likely C&C channels. The notion of persistence, a key contribution of this
work, turns out to be critical in detecting the covert channel communication
of botnets. Moreover, being a very coarse measure, persistence does not require
any protocol semantics or to look inside payloads to detect the malware. Using
a large corpus of (clean) user traﬃc as well as a collection of botnet traces, we
showed that our method successfully identiﬁed C&C destinations in each and
every botnet instance experimented with, even the ones that make very few con-
nections. We demonstrated that this detection incurs low overhead and also has
a low user annoyance factor. Even though our method is focused on uncovering
C&C communication with botnets that have a centralized infrastructure, we are
able to uncover even those that are not, as long as there is a certain regularity
(even over short time scales) in communicating with the C&C destinations.
In the future, a key task that we would like to undertake is to run our method
on a much larger sample of botnet traﬃc than we were able to collect on our own,
and perhaps validate it against a longer trace of user traﬃc data. Unfortunately,
as we learned in the course of this work, such an eﬀort requires a signiﬁcant
amount of resources, and technical expertise. This goes to show that a much
larger community eﬀort is needed to collect, share and annotate traces to sup-
port research eﬀorts in designing botnet mitigation solutions in particular, and
security mechanisms in general.
References
1. de Oliveira, K.C.: Botconomics: Mastering the Underground Economy of Botnets.
FIRST Technical Colloquium
2. McAfee Corp.: Avert Labs Threat Predictions for 2009,
http://www.mcafee.com/us/local_content/reports/2009_threat_
predictions_report.pdf
3. Cooke, E., Jahanian, F., McPherson, D.: The Zombie Roundup: Understanding,
Detecting, and Disrupting Botnets. In: Proceedings of the Workshop on Steps to
Reducing Unwanted Traﬃc on the Internet Workshop (SRUTI 2005), Berkeley,
CA, USA, p. 6. USENIX Association (2005)
4. Bhatkar, S., Chaturvedi, A., Sekar, R.: Dataﬂow Anomaly Detection. In: Proceed-
ings of the 2006 IEEE Symposium on Security and Privacy, Washington, DC, USA,
pp. 48–62. IEEE Computer Society, Los Alamitos (2006)
5. Gao, D., Reiter, M.K., Song, D.: On Gray-box Program Tracking for Anomaly
Detection. In: Proceedings of the 13th USENIX Security Symposium, Berkeley,
CA, USA, p. 8. USENIX Association (2004)
6. Binkley, J.R., Singh, S.: An Algorithm for Anomaly-based Botnet Detection. In:
Proceedings of the 2nd Workshop on Steps to Reducing Unwanted Traﬃc on the
Internet (SRUTI 2006), Berkeley, CA, USA, p. 7. USENIX Association (2006)
Exploiting Temporal Persistence to Detect Covert Botnet Channels
345
7. Gu, G., Porras, P., Yegneswaran, V., Fong, M., Lee, W.: BotHunter: Detecting
Malware Infection through IDS-Driven Dialog Correlation. In: Proceedings of 16th
USENIX Security Symposium, Berkeley, CA, USA, pp. 1–16. USENIX Association
(2007)
8. Gu, G., Zhang, J., Lee, W.: BotSniﬀer: Detecting Botnet Command and Control
Channels in Network Traﬃc. In: Proceedings of the Annual Network and Dis-
tributed System Security Symposium (NDSS 2008) (Febuary 2008)
9. Gu, G., Perdisci, R., Zhang, J., Lee, W.: BotMiner: Clustering Analysis of Network
Traﬃc for Protocol- and Structure-independent Botnet Detection. In: Proceedings
of the 17th Usenix Security Symposium, Berkeley, CA, USA, pp. 139–154. USENIX
Association (2008)
10. Kreibich, C., Kanich, C., Levchenko, K., Enright, B., Voelker, G., Paxson, V.,
Savage, S.: On the Spam Campaign Trail. In: First USENIX Workshop on Large-
Scale Exploits and Emergent Threats, LEET 2008 (2008)
11. Holz, T., Gorecki, C., Rieck, K., Freiling, F.: Measuring and Detecting Fast-Flux
Service Networks. In: Proceedings of the Annual Network and Distributed System
Security Symposium, NDSS 2008 (2008)
12. Jung, J., Paxson, V., Berger, A., Balakrishnan, H.: Fast Portscan Detection using
Sequential Hypothesis Testing. In: IEEE Symposium on Security and Privacy, pp.
211–225 (2004)
13. Sekar, V., Xie, Y., Reiter, M.K., Zhang, H.: Is Host-Based Anomaly Detection
+ Temporal Correlation = Worm Causality? Technical Report CMU-CS-07-112,
Carnegie Mellon University (March 2007)
14. McDaniel, P.D., Sen, S., Spatscheck, O., van der Merwe, J.E., Aiello, W.,
Kalmanek, C.R.: Enterprise Security: A Community of Interest Based Approach.
In: Proceedings of the Annual Network and Distributed System Security Sympo-
sium, NDSS 2006 (2006)
15. ClamAV: Clam AntiVirus, http://www.clamav.net
16. Paxson, V.: Bro: A System for Detecting Network Intruders in Real-Time. Com-
puter Networks (1999)
17. Stone-Gross, R., Cova, M., Cavallaro, L., Gilbert, B., Szydlowski, M., Kemmerer,
R., Kruegel, C., Vigna, G.: Your Botnet is My Botnet: Analysis of a Botnet
Takeover (May 2009),
http://www.cs.ucsb.edu/~seclab/projects/torpig/torpig.pdf
18. Porras, P., Saidi, H., Yegneswaran, V.: An Analysis of Conﬁcker’s Logic and Ren-
dezvous Points. Technical report, SRI International (March 2009)