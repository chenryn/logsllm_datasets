1600
1400
1200
1000
800
600
400
200
)
K
(
s
w
o
F
l
0
50
106
104
s
r
e
t
s
u
C
l
200
250
102
0
50
100
150
Index of time slots
(a) Size of FTable
(b) Number of clusters
Figure 5. Input of ﬂow traces in 5-min time
interval collected in L1 for 18 consecutive
hours
shows the number of all clusters as well as the extracted sig-
niﬁcant clusters. It is very interesting to observe the similar
patterns in the plot of memory cost (Fig. 4[b]) and that of
the ﬂow count over time (Fig 5[a]). This observation leads
us to analyze the correlation between these two measure-
ments. By examining the breakdown of the memory cost,
we ﬁnd that Ml in the cluster construction module accounts
for over 98% of the total memory consumptions. Recall that
the space complexity of this module is larger than the oth-
ers by two or three orders of magnitude, and dominated by
the size of ﬂow table |F|. A deep examination on |F| vs.
Ml conﬁrms the linear relationship between them. There-
fore, this strong correlation suggests that the memory cost
of the proﬁling system is mainly determined by the num-
ber of ﬂow records collected by the monitoring system in a
given time interval.
The breakdown in CPU usage suggests that cluster con-
struction and behavior proﬁling account for a large fraction
of CPU time. Similar to the space complexity, the time com-
plexity in cluster construction is also determined by |F|.
The linear relationship demonstrated by the scatter plot of
|F| vs. Tl conﬁrms this complexity analysis. In addition,
we observe an approximately linear relationship between
the number of signiﬁcant clusters and CPU time in behav-
ior proﬁling. This suggests that the CPU cost in behavior
proﬁling is largely determined by the number of signiﬁcant
clusters whose behavior patterns are being analyzed.
In summary, the average CPU and memory costs of the
real-time proﬁling system on 5-min ﬂow records collected
from an OC-48 link with a 10% link utilization are 60 sec-
onds and 100 MB, respectively. Moreover, the CPU time is
largely determined by the number of ﬂow records as well
as that of signiﬁcant clusters, and the memory cost is deter-
mined by the number of ﬂow records. During these moni-
toring periods, these links are not fully utilized, so we can
not extensively measure the performance of the real-time
proﬁling system for a highly loaded link. Next, we will
test the proﬁling system during sudden trafﬁc surges such as
4.2 Stress Test
The performance benchmarking of CPU and memory
costs demonstrate the operational feasibility of our trafﬁc
proﬁling system during normal trafﬁc patterns. However,
the proﬁling system should also be robust during atypi-
cal trafﬁc patterns, such as denial of service attacks, ﬂash
crowds, and worm outbreaks [4, 8, 10]. In order to under-
stand the system performance during these incidents, we in-
ject packet traces of three known denial of service attacks
and simulated worm outbreaks by superposing them with
backbone trafﬁc.
We use the packet traces of three DoS attacks with vary-
ing intensity and behavior studied in [1]. All of these at-
tacks are targeted on a single destination IP address. The
ﬁrst case is a multiple-source DoS attack, in which hun-
dreds of source IP addresses send 4200 ICMP echo request
packets with per second for about 5 minutes. The second
case is a TCP SYN attack lasting 12 minutes from random
IP addresses that send 1100 TCP SYN packets per second.
In the last attack, a single source sends over 24K ip-proto
255 packets per second for 15 minutes. In addition to DoS
attacks, we simulate the SQL slammer worm on January
25th 2003 [8] with an Internet Worm Propagation Simula-
tor used in [10]. In the simulation experiments, we adopt
the same set of parameters in [10] to obtain similar worm
simulation results, and collect worm trafﬁc monitored in a
220 IP space.
For each of these four anomalous trafﬁc patterns, we re-
play packet traces along with backbone trafﬁc, and aggre-
gate synthetic packets traces into 5-tuple ﬂows. For sim-
plicity, we still use 5 minutes as the size of the time inter-
val, and run the proﬁling system against the ﬂow records
collected in an interval. Table 3 shows a summary on ﬂow
traces of the ﬁrst 5-minute interval for these four cases. The
ﬂow, packet and byte counts reﬂect the intensity of attacks
or worm propagation, while the link utilization indicates the
impact of such anomaly behaviors on Internet links. For all
of these cases, the proﬁling system is able to successfully
generate event reports in less than 5 minutes.
During the emulation process, the link utilization ranged
from 314.5 Mbps to 629.2Mbps. We run the proﬁling sys-
tem on ﬂow traces after replaying synthetic packets and col-
lect CPU and memory cost of each time interval, which is
also shown in Table 3. The system works well for low in-
tense DoS attacks in the ﬁrst two cases. However, due to
intense attacks in the last DoS case (DoS-3) and worm prop-
agations, the CPU time of the system increases to 210 and
231 seconds, but still under the 5 minute interval. However,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:33:04 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Table 3. Synthetic packet traces with known denial of services attacks and worm simulations
Anomaly
DoS-1
DoS-2
DoS-3
Worm
Flows
Packets
2.08 M 18.9 M
1.80 M 20.7 M
16.5 M 39.8 M
18.9 M 43.0 M
Bytes
11.8 G
12.5 G
16.1 G
23.6 G
Link Utilization
314.5 Mbps
333.5 Mbps
430.1 Mbps
629.2 Mbps
CPU time
45 seconds
59 seconds
210 seconds
231 seconds
Memory
245.5 MB
266.1 MB
1.75GB
2.01GB
Details
distributed dos attacks from multiple sources
distributed dos attacks from random sources
dos attacks from single source
slammer worm simulations
Table 4. Reduction of CPU time and memory
cost using the random sampling technique
Case
DoS attack
Worm
Size of FTable
µ
66% 10M
55% 10M
CPU time
89 seconds
97 seconds
memory
867 MB
912 MB
the memory cost jumps to 1.75GB and 2.01GB indicating
a performance bottleneck. This clearly suggests that we
need to provide practical solutions to improve the robust-
ness of the system under stress. In the next section, we will
discuss various approaches, including traditional sampling
techniques and new proﬁling-aware ﬁltering techniques to-
wards this problem, and evaluate the tradeoff between per-
formance beneﬁts and proﬁling accuracy.
5 Sampling and Filtering
5.1 Random Sampling
Random sampling is a widely-used simple sampling
technique in which each object, ﬂow in our case, is ran-
domly chosen based on the same probability (also known
as sampling ratio µ). Clearly, the number of selected ﬂows
is entirely decided by the sampling ratio µ. During the
stress test in the last section, the proﬁling system requires
about 2GB memory when the number of ﬂow records reach
16.5M and 18.9 during DoS attacks and worm outbreaks.
Such high memory requirement is not affordable in real-
time since the machine installed with the proﬁling system
could have other tasks as well, e.g., packet and ﬂow mon-
itoring. As a result, we attempt to set 1GB as the upper
bound of the memory cost. Recall that in the performance
benchmarking, we ﬁnd that memory cost is determined by
the number of ﬂow records. Based on their linear relation-
ship we estimate that ﬂow records with a size of 10M will
require approximately 1GB memory. Thus, 10M is the de-
sirable limit for the size of the ﬂow records.
Using the limit of ﬂow records, l, we could conﬁgure the
sampling ratio during sudden trafﬁc increase as µ = l|F| .
As a result, we set the sampling ratios in the last DoS at-
tacks and worm outbreaks as 60% and 55%, respectively,
and randomly choose ﬂows in loading ﬂow tables in the
cluster construction module. Table 4 shows the reduction
of CPU time and memory consumptions with the sampled
ﬂow tables for both cases.
On the other hand, random sampling has substantial im-
pact on behavior accuracy. First, the set of signiﬁcant clus-
ters from four feature dimensions are smaller than that with-
out sampling, which is caused by the changes of the un-
derlying cluster size distribution after ﬂow sampling. Ta-
ble 5 shows the number of signiﬁcant clusters extracted
along each dimension without and with sampling for the
DoS case. In total, among 309 signiﬁcant clusters without
sampling, 180 (58%) of the most signiﬁcant clusters are still
extracted with random sampling. Secondly, the behavior of
a number of extracted clusters are altered, since ﬂow sam-
pling changes the feature distribution of free dimensions as
well as the behavior classes for these clusters. As shown in
the last column of Table 5, 161 out 180 signiﬁcant clusters
with random sampling are classiﬁed with the same behavior
as those without sampling. In other words, the behavior of
19 (10.5%) extracted signiﬁcant clusters are changed as a
result of random sampling. Fig. 6 shows the feature distri-
butions of free dimensions for 140 dstIP clusters with and
without random sampling. The deviations from the diago-
nal line indicate the changes of feature distribution and the
behavior due to ﬂow sampling. We also perform random
sampling on the synthetic ﬂow traces in the case of worm
outbreak, and the results of sampling impact on cluster ex-
tractions and behavior accuracy are very similar.
Table 5. Reduction of signiﬁcant clusters and
behavior accuracy
Dim.
srcPrt
dstPrt
srcIP
dstIP
Total
Sig. clusters
Sig. clusters
without sampling with sampling
23
6
47
233
309
4
5
31
140
180
Clusters with same
behavior classes
3
4
29
125
161
In summary, random sampling could reduce the CPU
time and memory cost during sudden trafﬁc surges caused
by DoS attacks or worm outbreaks. However, random sam-
pling reduces the number of interesting events, and also al-
ters the behavior classes of some signiﬁcant clusters. Such
impact could have become worse if “lower” sampling rates
are selected. Thus, it becomes necessary to develop a
proﬁling-aware algorithm that not only reduces the size of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:33:04 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20071
0.8
0.6
0.4
0.2
g
n
i
l
p
m
a
s
/
w
y
t
n
a
i
t
r
e
c
n
u
e
v
i
t
l
a
e
R
0
0
0.2
0.8
Relative uncertainty w/o sampling
0.4
0.6
1
0.8
0.6
0.4
0.2
g
n
i
l
p
m
a
s
/
w
y
t
n
a
i
t
r
e
c
n
u
e
v
i
t
l