1.003
1.098
1.036
1.008
1.007
1.001
1.000
1.030
1.110
1.045
1.006
1.002
1.049
1.073
1.001
1.039
0.997
1.248
1.034
Invert
(×)
1.000
1.016
1.001
1.002
1.000
1.000
1.000
1.000
1.000
1.186
1.000
1.000
1.011
1.000
1.037
1.000
1.005
1.001
1.001
1.001
1.000
1.093
1.035
1.002
1.002
1.073
1.063
1.000
1.029
1.000
1.186
1.019
SSFI
(×)
1.510
1.035
1.005
1.117
1.058
1.001
1.021
1.009
1.017
1.248
1.048
1.003
1.039
1.111
1.380
1.072
1.146
1.268
1.054
1.233
1.122
1.157
1.112
1.002
1.008
1.045
1.115
1.002
1.180
1.001
1.510
1.102
bubblesort
ctl-string
cubic
dijkstra
edn
fasta
ﬁr
frac
huffbench
levenshtein
matmult-int
nbody
ndes
nettle-aes
picojpeg
qrduino
rijndael
sglib-dllist
sglib-listins...
sglib-listsort
sglib-queue
sglib-rbtree
slre
sqrt
st
stb_perlin
trio-sscanf
whetstone
wikisort
Min
Max
Geomean
Table 3: Performance Overhead on BEEBS
BEEBS The BEEBS benchmark suite [58] is designed for
measuring the energy consumption of embedded devices.
However, it is also useful for evaluating performance and
code size overhead because it includes a wide range of pro-
grams, including a benchmark based on the Advanced En-
cryption Standard (AES), integer and ﬂoating-point matrix
multiplications, and an advanced sorting algorithm.
The major drawback of BEEBS is that many of its pro-
grams either are too small or process too small inputs, result-
ing in insufﬁcient execution time. For example, fibcall is
intended to compute the 30th Fibonacci number, but Clang
computes the result during compilation and returns a constant
directly. To account for this issue, we exclude programs with
a baseline execution time of less than one second with 10,240
iterations. We also exclude mergesort because it failed the
verify_benchmark() check when compiled with unmodi-
ﬁed Clang. For all the other programs, all of our transformed
versions passed this function, if it was implemented. We used
commit 049ded9 of the BEEBS repository on GitHub.
To record the execution time of an individual BEEBS
benchmark, we wrapped 10,240 iterations of benchmark work-
load execution with calls to HAL_GetTick() [63] and added
code to print out the time difference in milliseconds. We also
did the same initialization sequence for each BEEBS bench-
mark as we did for CoreMark-Pro.
USENIX Association
29th USENIX Security Symposium    1229
8.3 Runtime Overhead
Tables 2 and 3 show the performance overhead that Silhouette
and Silhouette-Invert induce on CoreMark-Pro and BEEBS,
respectively; overhead is expressed as execution time normal-
ized to the baseline. The SS column shows the overhead of
just the shadow stack transformation, SH shows the overhead
induced when only store hardening is performed, and CFI
shows the overhead of the CFI checks on forward branches.
The Silhouette and Invert columns show the overhead of
the complete Silhouette and Silhouette-Invert prototypes, re-
spectively. The SSFI column denotes overhead incurred by a
version of Silhouette that uses Software Fault Isolation (SFI)
in place of store hardening; Section 8.5 describes that experi-
ment in more detail.
Silhouette Performance As Tables 2 and 3 show, Silhou-
ette incurs a geometric mean overhead of only 1.3% on
CoreMark-Pro and 3.4% on BEEBS. The highest overhead
is 4.9% from CoreMark-Pro’s loops benchmark and 24.8%
from BEEBS’s bubblesort benchmark. The bubblesort
benchmark exhibits high overhead because it spends most of
its execution in a small loop with frequent stores; to promote
these stores, Silhouette adds instructions to the loop that com-
pute the target address. Another BEEBS program with high
overhead is levenshtein. The reason is that one of its func-
tions has a variable-length array on the stack and that function
is called in a loop; Silhouette promotes the stack allocation to
the heap with malloc() and free(). Without this promotion,
Silhouette incurs 2.2% overhead on levenshtein. Nearly all
(8 of 9) of the CoreMark-Pro benchmarks slow down by less
than 2%, and 5 programs have less than 1% overhead. For
BEEBS, 24 of the 29 programs slow down by less than 5%;
16 programs have overhead less than 1%. Tables 2 and 3
also show that the primary source of the overhead is typically
store hardening, though for some programs e.g., core and
sglib-rbtree, the shadow stack induces more overhead due
to extensive function calls. CFI overhead is usually negligible
because our benchmarks seldom use indirect function calls.
Silhouette-Invert Performance Silhouette-Invert greatly
decreases the overhead because it only needs to convert the
single privileged store instruction in the prologue of a function
to a unprivileged one and leave all other stores unchanged.
It incurs only 0.3% geomean overhead on CoreMark-Pro.
Seven of the 9 programs show overhead less than 0.5%. For
BEEBS, the geometric mean overhead is 1.9%. When ex-
cluding the special case of levenshtein, the average over-
head is 1.3%. Twenty of the 29 programs slow down by less
than 1%. Only three programs, sglib-rbtree, stb_perlin,
and trio-sscanf, again, except levenshtein, slow down
by over 5%, and all of them have very frequent function calls.
Baseline
(bytes)
51,516
99,156
SS
(×)
1.005
1.017
— 1.008
SH
(×)
1.028
1.111
1.068
CFI
(×)
1.002
1.094
1.012
Silhou-
ette (×)
1.036
1.193
1.089
Invert
(×)
1.008
1.113
1.022
SSFI
(×)
1.071
1.315
1.172
Min
Max
Geomean
Table 4: Code Size Overhead on CoreMark-Pro
Baseline
(bytes)
30,144
46,108
SS
(×)
1.003
1.006
— 1.004
SH
(×)
1.005
1.061
1.018
CFI
(×)
1.000
1.013
1.001
Silhou-
ette (×)
1.009
1.068
1.023
Invert
(×)
1.000
1.019
1.005
SSFI
(×)
1.009
1.201
1.044
Min
Max
Geomean
Table 5: Code Size Overhead on BEEBS
8.4 Code Size Overhead
Small code size is critical for embedded systems with lim-
ited memory. We therefore measured the code size overhead
incurred by Silhouette by measuring the code size of the
CoreMark-Pro and BEEBS benchmarks. Due to space limita-
tions, we only show the highest, lowest, and average code size
increases in Tables 4 and 5. In summary, Silhouette incurs
a geometric mean of 8.9% and 2.3% code size overhead on
CoreMark-Pro and BEEBS, respectively.
For Silhouette, most of the code size overhead comes from
store hardening. As Section 6.2 explains, Silhouette trans-
forms some regular store instructions into a sequence of mul-
tiple instructions. Floating-point stores and stores that write
multiple registers to contiguous memory locations bloat the
code size most. In BEEBS, picojpeg incurs the highest code
size overhead because an unrolled loop contains many such
store instructions, and the function that contains the loop is
inlined multiple times. For Silhouette-Invert, because it leaves
nearly all stores unchanged, its code size overhead is only
2.2% on CoreMark-Pro and 0.5% on BEEBS.
8.5 Store Hardening vs. SFI
An alternative to using store hardening to protect the shadow
stack is to use Software Fault Isolation (SFI) [69]. To com-
pare the performance and code size overhead of store hard-
ening against SFI, we built a system that provides the same
protections as Silhouette but that uses SFI in place of store
hardening. We dub this system Silhouette-SFI (SSFI). Our
SFI pass instruments all store instructions within a program
other than those introduced by the shadow stack pass and
those in the HAL. Speciﬁcally, our SSFI prototype adds the
same BIC [12] (bitmasking) instructions as what Silhouette
does for Store-Exclusives (discussed in Section 6.2) before
each store to restrict them from writing to the shadow stack.
SSFI incurs much higher performance and code size over-
head compared to Silhouette. On CoreMark-Pro, SSFI incurs
a geometric mean of 2.2% performance overhead, nearly dou-
bling Silhouette’s average overhead of 1.3%; on BEEBS,
1230    29th USENIX Security Symposium
USENIX Association
SSFI slows down programs by 10.2%, three times of Silhou-
ette’s 3.4%. Only on one program, the loops benchmark in
CoreMark-Pro, SSFI performs better than Silhouette. For code
size, SSFI incurs an average of 17.2% overhead on CoreMark-
Pro and 4.4% on BEEBS; the highest overhead is 31.5% and
20.1%, respectively, while on Silhouette it is 19.3% and 6.8%.
The speciﬁc implementation of SFI will vary on different
devices due to different address space mappings, so it is possi-
ble to get different overhead on different boards for the same
program. In contrast, Silhouette’s performance overhead on
the same program should be more predictable across differ-
ent boards because the instructions added and replaced by