| Model | Description |
|---|---|
| EGADS  ExtremeLow-DensityModel Outlier |EGADS density-based anomaly detection. |
| EGADS CP |EGADS kernel-based change-point detec-tion. |
| EGADS  KSig- maModel Outlier |EGADS re-implementation of the classic k-sigma model. |
| Twitter Out-lier |The Open-Source Twitter-R anomaly de-tection library based on the Generalized ESD method. || ExtremeI & II R Outlier |Open source univariate outlier detection that threshold the absolute value and the residual to detect anomalies. |
| BreakOut  Twitter CP |A package from Twitter that uses an ESD statistics test to detect change points. |
| ChangePt1 R CP |An R library that implements various mainstream and specialized change-point methods for finding single and multiple change-points within data. Method I uses a change in variance. || ChangePt2 & 3 R CP |Detects a change in the mean and the vari-ance. |
Table 4: Open Source systems used for evaluation
of instances that exhibited abnormal spikes and level shifts. Abnormal in the case of YM meant seasonal followed by non-seasonal behavior which characterizes most of the at-tacks. Also the YM editors did not care about traffic-shift behavior, where a large drop in traffic was observed in a time-series due to router table being updated.To address this requirement the filtering stage scanned all anomalies ai from all models and using a model classified if ai was a true positive. The model used in the filtering stage for the YM use-case is a boosted tree model based on Ad-aBoost [8]. The features used in the model are described in Table 1. The core principle of AdaBoost is to fit a sequence of weak learners (e.g., small decision trees) on repeatedly modified version of the data. The final result is then pro-duced via a combined weighted majority vote. On each it-eration, the examples that are difficult to predict receive a higher importance in the next iteration and therefore each subsequent weak learner focuses on the examples that are missed by the previous learners in the sequence. Besides the time-series features described in Table 1 we use the model features described in Section 6.2. The experiments in Fig-ure 6 indicate an impressive precision/recall even with just the time-series features compared to just using the model alone without the filtering stage. This experiment under-lines an important principle and a critical component of any anomaly detection framework: an anomaly is use-case spe-cific and must be learned automatically for a fully scalable and automated solution.Figure 6: Accuracy of the filtering stage using different types of features.
detection, fraud detection, network intrusion detection and many others. Despite its crucial importance, implementing a fully-automatic anomaly detection system in practice is a challenging task due to the large problem scale and the diverse use-cases residing in the real-world setting. These challenges typically result in solutions that are either not scalable or highly specialized, which would in turn result in a high rate of false positives when applied to other use-cases.In this paper, we introduced EGADS, the generic anomaly detection system implemented at Yahoo to automatically monitor and alert on millions of time-series on different Ya-hoo properties for different use-cases ranging from fault de-tection to intrusion detection. As we described in the paper, the parallel architecture of EGADS on Hadoop as well as its stream processing mechanism through Storm enable it to perform real-time anomaly detection on millions of time-series at Yahoo. 	Furthermore, EGADS employs different time-series modeling, and anomaly detection algorithms to handle different monitoring use-cases. By incorporating this array of algorithms combined with a machine-learned mech-anism in the alerting module, EGADS automatically adapts itself to the anomaly detection use-case that is important to the user. All of these features effectively create a power-ful anomaly detection framework which is both generic and scalable. Our showcase experiments on real and synthetic datasets have shown the superior applicability of our frame-work compared to its rival solutions.Last but not least, EGADS by its very nature is extendable, providing an easy mechanism to plugin new models and al-gorithms into the system. This feature specifically creates
| 7. | CONCLUSION | an oppurtunity for the community to contribute to EGADS. |
|---|---|---|
| 7. |CONCLUSION |Finally, to further engage with the anomaly detection and |Anomaly detection is a critical component at the heart of many real-time monitoring systems with applications in fault
monitoring community, our framework together with all its datasets are contributed to the open source repository.
8. 	REFERENCES
[1] C. Aggarwal. Outlier Analysis. Springer New York,
processes. Journal of Time Series Analysis, 23(6):687–705, 2002.2013. 	[21] B. Rosner. Percentage points for a generalized esd [2] P. Bloomfield. Fourier analysis of time series: an 	many-outlier procedure. Technometrics, 	introduction. John Wiley & Sons, 2004. 	25(2):165–172, 1983.
[3] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. Lof: Identifying density-based local outliers. SIGMOD Rec., 29(2):93–104, May 2000.[4] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Comput. Surv., 
41(3):15:1–15:58, July 2009.
[22] A. Toshniwal, S. Taneja, A. Shukla, K. Ramasamy, J. M. Patel, S. Kulkarni, J. Jackson, K. Gade, M. Fu, J. Donham, N. Bhagat, S. Mittal, and D. Ryaboy.
Storm@twitter. In Proceedings of the 2014 ACM 
SIGMOD International Conference on Management of Data, SIGMOD ’14, pages 147–156, New York, NY,[5] R. B. Cleveland, W. S. Cleveland, J. E. McRae, and 	USA, 2014. ACM.
I. Terpenning. Stl: A seasonal-trend decomposition procedure based on loess. Journal of Official 
Statistics, 6(1):3–73, 1990.
[6] J. Durbin and S. J. Koopman. Time series analysis by state space methods. Number 38. Oxford University[23] O. Vallis, J. Hochenbaum, and A. Kejariwal. A novel technique for long-term anomaly detection in the cloud. In 6th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 14), Philadelphia, PA, June 2014. USENIX Association.
Press, 2012. 	[24] M. van der Loo. extremevalues, an R package for
[7] V. A. Epanechnikov. Non-parametric estimation of a multivariate probability density. Theory of Probability & Its Applications, 14(1):153–158, 1969.[8] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting, 1996.
[9] S. S. Haykin, S. S. Haykin, and S. S. Haykin. Kalman 	filtering and neural networks. Wiley Online Library,
outlier detection in univariate data, 2010. R package version 2.0.
[25] X. Wang, K. Smith-Miles, and R. Hyndman. Rule 	induction for forecasting method selection:Meta-learning the characteristics of univariate time 	series. Neurocomput., 72(10-12):2581–2594, June 2009. [26] W. W.-S. Wei. Time series analysis. Addison-Wesley 	publ, 1994.
2001. 	[27] Y. Xie, J. Huang, and R. Willett. Change-point
[10] R. J. Hyndman and A. B. Koehler. Another look at measures of forecast accuracy. International Journal of Forecasting, pages 679–688, 2006.[11] R. H. Jones. Exponential smoothing for multivariate 	time series. Journal of the Royal Statistical Society.
Series B (Methodological), pages 241–251, 1966.
detection for high-dimensional time series with missing data. Selected Topics in Signal Processing, IEEE Journal of, 7(1):12–27, 2013.
[12] Y. Kawahara, T. Yairi, and K. Machida. Change-point 
detection in time-series data based on subspaceidentification. In Data Mining, 2007. ICDM 2007.
Seventh IEEE International Conference on, pages 
559–564. IEEE, 2007.
[13] A. Kejariwal and P. Kumar. Mitigating user 
experience from ’breaking bad’: The twitter approach. 
In Velocity 2014, New York, NY, Sept. 2014.
[14] R. Killick. changepoint, an R package that implements 
various mainstream and specialised changepoint 
methods., 2014.methods., 2014.
[15] L. Komsta. outliers, an R package of some tests 
	commonly used outlier detection techniques., 2011.
[16] S. Kullback. Information theory and statistics. Courier 
	Corporation, 1997.
[17] S. Liu, M. Yamada, N. Collier, and M. Sugiyama.
Change-point detection in time-series data by relative 
density-ratio estimation. Neural Networks, 43:72–83, 
2013.2013.
[18] V. Moskvina and A. Zhigljavsky. An algorithm based 
on singular spectrum analysis for change-point 
detection. Communications in Statistics-Simulation 
and Computation, 32(2):319–352, 2003.
[19] D. B. Percival and A. T. Walden. Wavelet methods for 
time series analysis, volume 4. Cambridge University 
Press, 2006.
[20] B. K. Ray and R. S. Tsay. Bayesian methods forchange-point detection in long-range dependent