by the capture unit.
The capture unit receives packets from the network tap
and passes them on to the classiﬁcation unit. Using the
connection tracking mechanism, it checks if the connection
the packet belongs to has exceeded its cutoff value. If not,
it ﬁnds the associated storage container, which then stores
the packet in memory, indexing it in the process for quick
access later on. It later migrates it to disk, and eventually
deletes it. Accordingly, the actual Time Machine differs
from the connection-level simulation model in that now the
buffers are caches that evict packets when they are full,
rather than evicting whole connections precisely at their
eviction time.
Our
implementation of
the architecture uses
the
libpcap packet capture library [2], for which the user
can specify a kernel-level BPF [4] capture ﬁlter to discard
“uninteresting” trafﬁc as early as possible. We collect and
store each packet’s full content and capture timestamp.
The capture unit passes the packet to the classiﬁcation
routines, which divide the incoming packet stream into
classes according to a user-speciﬁed conﬁguration. Each
class deﬁnition includes a class name, a BPF ﬁlter to iden-
270
Internet Measurement Conference 2005
USENIX Association
tify which packets belong to the class, a matching priority,
and several storage parameters; for example:
class "telnet" { filter "tcp port 23";
precedence 50; cutoff 10m;
mem 10m; disk 10g; }
which deﬁnes a class “telnet” that matches, with prior-
ity 50, any trafﬁc captured by the BPF ﬁlter "tcp port
23". A cutoff of 10 MB is applied, and an in-memory
buffer of 10 MB and a disk budget of 10 GB allocated.
For every incoming packet, we look up the class asso-
ciated with its connection in the connection tracking unit,
or, if it is a new connection, match the packet against all of
the ﬁlters. If more than one ﬁlter matches, we assign it to
the class with the highest priority. If no ﬁlter matches, the
packet is discarded.
To track connection cutoffs, the Time Machine keeps
state for all active connections in a hash table. If a newly
arrived packet belongs to a connection that has exceeded
the cutoff limit conﬁgured for its class, it is discarded. We
manage entries in the connection hash table using a user-
conﬁgurable inactivity timeout; the timeout is shorter for
connections that have not seen more than one packet, which
keeps the table from growing too large during scans or de-
nial of service attacks.
For every class, the Time Machine keeps an associated
storage container to buffer the packets belonging to the
class. Storage containers consist of two ring buffers. The
ﬁrst stores packets in a RAM buffer, while the second
buffers packets on disk. The user can conﬁgure the size
of both buffers on a per-class basis. (A key motivation for
maintaining a RAM buffer in addition to disk storage is to
enable near-real-time access to the more recent part of the
Time Machine’s archive.) Packets evicted from the RAM
buffer are moved to the disk buffer. We structure the disk
buffer as a set of ﬁles. Each such ﬁle can grow up to a
conﬁgurable size (typically 10–100s of MB). Once a ﬁle
reaches this size, we close it and create a new ﬁle. We store
packets both in memory and on disk in libpcap format.
This enables easy extraction of libpcap traces for later
analysis.
To enable quick access to the packets, we maintain mul-
tiple indexes. The Time Machine is structured internally
to support any number of indexes over an arbitrary set of
(predeﬁned) protocol header ﬁelds. For example, the Time
Machine can be compiled to simultaneously support per-
address, per-port, and per-connection-tuple indexes. Each
index manages a list of time intervals for every unique key
value, as observed in the protocol header ﬁeld (or ﬁelds) of
the packets. These time intervals provide information on
whether packets with that key value are available in a given
storage container and at what starting timestamp, enabling
fast retrieval of packets. Every time the Time Machine
stores a new packet it updates each associated index.
If
the packet’s key—a header ﬁeld or combination of ﬁelds—
is not yet in the index, we create a new entry containing
a zero-length time interval starting with the timestamp of
the packet. If an entry exists, we update it by either ex-
tending the time interval up to the timestamp of the current
packet, or by starting a new time interval, if the time dif-
ference between the last entry in the existing interval and
the new timestamp exceeds a user-deﬁned parameter. Thus,
this parameter trades off the size of the index (in terms of
number of intervals we maintain) for how precisely a given
index entry localizes the packets of interest within a given
storage container. As interval entries age, we migrate them
from in-memory index structures to index ﬁles on disk, do-
ing so at the same time the corresponding packets in the
storage containers migrate from RAM to disk. In addition,
the user can set an upper limit for the size of the in-memory
index data structure.
The ﬁnal part of the architecture concerns how to ﬁnd
packets of interest in the potentially immense archive.
While this can be done using brute force (e.g., running
tcpdump over all of the on-disk ﬁles), doing so can take
a great deal of time, and also have a deleterious effect
on Time Machine performance due to contention for the
disk. We address this issue using the query-processing unit,
which provides a ﬂexible language to express queries for
subsets of the packets. Each query consists of a logical
combination of time ranges, keys, and an optional BPF ﬁl-
ter. The query processor ﬁrst looks up the appropriate time
intervals for the speciﬁed key values in the indexing struc-
tures, trimming these to the time range of the query. The
logical or of two keys is realized as the union of the set of
intervals for the two keys, and an and by the intersection.
The resulting time intervals correspond to the time ranges
in which the queried packets originally arrived. We then
locate the time intervals in the storage containers using bi-
nary search. Since the indexes are based on time intervals,
these only limit the amount of data that has to be scanned,
rather then providing exact matches; yet this narrowing suf-
ﬁces to greatly reduce the search space, and by foregoing
exact matches we can keep the indexes much smaller. Ac-
cordingly, the last step consists of scanning all packets in
the identiﬁed time ranges and checking if they match the
key, as well as an additional BPF ﬁlter if supplied with the
query, writing the results to a tcpdump trace ﬁle on disk.
5 Evaluation
To evaluate the Time Machine design, we ran an imple-
mentation at two of the sites discussed in § 3. For LBNL,
we used three classes, each with a 20 KB cutoff: TCP traf-
ﬁc, with a space budget of 90 GB; UDP, with 30 GB; and
Other, with 10 GB. To evaluate the “hindsight” capabilities,
we determine the retention, i.e., the distance back in time to
which we can travel at any particular moment, as illustrated
in Figure 6. Note how retention increases after the Time
Machine starts until the disk buffers have ﬁlled. After this
USENIX Association
Internet Measurement Conference 2005  
271
TCP
UDP
Other
]
s
y
a
d
[
n
o
i
t
t
n
e
e
R
7
6
5
4
3
2
1
0
Sat 0:00
Mon 0:00 Wed 0:00
Fri 0:00
Sun 0:00
Tue 0:00
Thu 0:00
Figure 6: Retention in the LBNL environment
Local time
point, retention correlates with the incoming bandwidth for
each class and its variations due to diurnal and weekly ef-
fects. New data forces the eviction of old data, as shown for
example by the retention of TCP shortening as the lower
level weekend trafﬁc becomes evicted around Wed–Thu.
The TCP buffer of 90 GB allows us to retain data for 3–
5 days, roughly matching the predictions from the LBNL
simulations (recall the volume biases of the connection-
level data discussed in § 3). Use of a cutoff is highly ef-
ﬁcient: on average, 98% of the trafﬁc gets discarded, with
the remainder imposing an average rate of 300 KB/s and a
maximum rate of 2.6 MB/s on the storage system. Over the
2 weeks of operation, libpcap reported only 0.016% of
all packets dropped.
Note that classes do not have to be conﬁgured to yield an
identical retention time. The user may deﬁne classes based
on their view of utility of having the matching trafﬁc avail-
able in terms of cutoff and how long to keep it. For example
we might have included a class conﬁguration similar to the
example in § 4 in order to keep more of Telnet connections
for a longer period of time.
Operationally, the Time Machine has already enabled the
diagnosis of a break-in at LBNL by having retained the re-
sponse to an HTTP request that was only investigated three
days later. The Time Machine’s data both conﬁrmed a suc-
cessful compromise and provided additional forensic infor-
mation in terms of the attacker’s other activities. Without
the Time Machine, this would not have been possible, as
the site cannot afford to record its full HTTP trafﬁc for any
signiﬁcant length of time.
At MWN we ran preliminary tests of the Time Ma-
chine, but we have not yet evaluated the retention capa-
bility systematically. First results show that about 85% of
the trafﬁc gets discarded, with resulting storage rates of
3.5 (13.9) MB/s average (maximum). It appears that the
larger volume of HTTP trafﬁc is the culprit for this differ-
ence compared to LBNL, due to its lesser heavy-tailed na-
ture; this matches the results of the MWN connection-level
simulation. For this environment it seems we will need
to more aggressively exploit the classiﬁcation and cutoff
mechanisms to appropriately manage the large fraction of
HTTP trafﬁc.
The fractions of discarded trafﬁc for both LBNL and
MWN match our predictions well, and the resulting storage
rates are reasonable for today’s disk systems, as demon-
strated in practice. The connection tracking and indexing
mechanisms coped well with the characteristics of real In-
ternet trafﬁc. We have not yet evaluated the Time Machine
at NERSC, but the simulations promise good results.
6 Summary
In this paper, we introduce the concept of a Time Machine
for efﬁcient network packet recording and retrieval. The
Time Machine can buffer several days of raw high-volume
trafﬁc using commodity hardware. It provides an efﬁcient
query interface to retrieve the packets in a timely fashion,
and automatically manages its available storage. The Time
Machine relies on the simple but crucial observation that
due to the “heavy-tailed” nature of network trafﬁc, we can
record most connections in their entirety, yet skip the bulk
of the total volume, by storing up to (a customizable) cutoff
limit of bytes per connection. We have demonstrated the ef-
fectiveness of the approach using a trace-driven simulation
as well as operational experience with the actual implemen-
tation in two environments. A cutoff of 20 KB increases
data availability from several hours to several days when
compared to brute-force bulk recording.
In operational use, the Time Machine has already proved
valuable by enabling diagnosis of a break-in that standard
bulk-recording had missed. In future work, we intend to
add a remote access interface to enable real-time queries
for historic network data by components such as network
intrusion detection systems.
7 Acknowledgments
This work was supported by the National Science Foundation un-
der grant STI-0334088, and by a grant from the Bavaria California
Technology Center, for which we are grateful.
References
[1] ANTONELLI, C., UNDY, M., AND HONEYMAN, P. The Packet Vault: Secure
Storage of Network Data. In Proc. Workshop on Intrusion Detection and Net-
work Monitoring (April 1999), pp. 103–110.
[2] LAWRENCE BERKELEY NATIONAL LABORATORY.
http://www.tcpdump.org/.
tcpdump and libpcap.
[3] MCAFEE. McAfee Security Forensics. http://www.mcafeesecurity.
com/us/products/mcafee/forensics/security_for%
ensics.htm.
[4] MCCANNE, S., AND JACOBSON, V. The BSD Packet Filter: A New Architec-
ture for User-level Packet Capture. In Proc. USENIX Winter 1993 Conference
(January 1993), pp. 259–270.
[5] PAXSON, V. Bro: A system for detecting network intruders in real-time. Com-
puter Networks 31, 23–24 (December 1999).
[6] PAXSON, V., AND FLOYD, S. Wide-Area Trafﬁc: The Failure of Poisson Mod-
eling. IEEE/ACM Transactions on Networking 3, 3 (June 1995), 226–224.
[7] WALLERICH, J., DREGER, H., FELDMANN, A., KRISHNAMURTHY, B., AND
WILLINGER, W. A Methodology for Studying Persistency Aspects of Internet
Flows. ACM SIGCOMM Computer Communication Review 35, 2 (April 2005),
23–36.
272
Internet Measurement Conference 2005
USENIX Association