that have been developed in the literature are indeed vul-
nerable to re-identiﬁcation attacks. Consider the following
anonymization scheme, which represents several proposed
algorithms for k-anonymity [5, 21].
Algorithm 1. [Clustering
and Local Recoding
(CLR)]: First, group input tuples into clusters such that
each cluster has at least k tuples. For example, one method
of grouping is the Mondrian algorithm [21]. One could
also use some clustering method based on some distance
measurement (e.g., [5]). Then, for each tuple, replace each
attribute value with a generalized value that represents all
values for that attribute in the cluster.
CLR algorithms are vulnerable when some tuples contain
extreme values. Even if the output satisﬁes k-anonymity,
the generalized value depends on the extreme values of some
tuples; hence from the output an adversary can infer that
one’s tuple is in the dataset and can thus infer these values.
For example, suppose the dataset records the net worth of
some individuals in a town. Further suppose that it is known
that only one individual in the town has net worth over $10
million. When given a (k = 20)-anonymized output dataset
containing one group of tuples that all have [900K, 35M ]
as the generalized net worth value, what can one conclude?
At least the following: the rich individual is in the dataset;
the individual’s tuple is in the group; and the individual’s
net worth is $35 million. It would be diﬃcult to say that
because in the output dataset, there are at least 19 other
tuples that are exactly the same, then the individual cannot
be re-identiﬁed with probability 1/20.
Similar weaknesses exist for other k-anonymization algo-
rithm in the literature, for example, those computing a gen-
eralization scheme based on the input dataset [16]. With all
these algorithms, the presence and non-presence of some ex-
treme values will aﬀect the resulted generalization scheme,
leaking information.
As these algorithms are sensitive to the presence of a single
tuple with extreme values, they do not satisfy (β, ǫ, δ)-DPS
when β > δ, since sampling with β will result the presence
of the tuple selected with probability β.
3.2 “Safe” k-Anonymization
k-Anonymization algorithms can be viewed as consisting
of two steps. In the ﬁrst step, the algorithm outputs a parti-
tioning over the space of all tuples, and chooses a representa-
tive tuple for each partition (which can be generalized values
of the partition). In the second step, the algorithm outputs
how many tuples there are in each partition, outputting 0
when a partition contains fewer than k tuples. Outputs from
the two steps are combined to produce the output dataset.
The privacy vulnerability of the CLR algorithm and other
existing k-anonymization algorithm occurs in the ﬁrst step.
How the space is partitioned and how the representative
tuples are chosen can be sensitive to the presence of a single
tuple that has extreme values. Hence the output violates
privacy.
To avoid such privacy vulnerability, we consider k-
anonymization algorithms for which the ﬁrst step satisﬁes
ǫ-DP. We say that a k-anonymization algorithm A is ǫ-safe
when its ﬁrst step satisﬁes ǫ-DP.
A special case of ǫ-safe k-anonymization algorithms is
when the ﬁrst step of the algorithm does not depend
on dataset D,
in which case we say that the algo-
rithm is strongly safe. An example of a strongly-safe k-
anonymization algorithm is to always use the same grid
to partition the tuple space. Such a method is similar to
histogram methods; the diﬀerence is that k-anonymization
publishes accurate counts.
An example of an ǫ-safe method is to consider various
possible generalization schemes, use a quality function to as-
sign a quality to each of them, and then use the exponential
mechanism [25] to diﬀerentially privately select a generaliza-
tion scheme that gives good utility.
Intuitively a strongly-safe k-anonymization algorithm pro-
vides some level of privacy protection, and the level of pri-
vacy protection increases with larger values of k. If any in-
dividual’s tuple is published, there must exist at least k − 1
other tuples in the input database that are the same under
the recoding scheme; furthermore, the recoding scheme does
not depend on the dataset, and one sees only the results of
the recoding. Hence in this input dataset, the individual
is hidden in a crowd of at least k. However, the following
proposition shows that strongly safe k-anonymization algo-
rithms do not satisfy (ǫ, δ)-DP unless they are degenerate,
which we now deﬁne.
Definition 5. We
k-
anonymization algorithm is degenerate if and only if
in the ﬁrst step, it chooses the same representative tuple for
all partitions.
strongly-safe
that
say
a
A special case is an algorithm that outputs a single parti-
tion that includes all possible tuples. Intuitively, such algo-
rithms suppress all information about a dataset (except for
how many tuples it includes) and provide absolute privacy
guarantees; however, they are useless in practice.
Proposition 4. No non-degenerate
strongly-safe
k-
anonymization algorithm satisﬁes (ǫ, δ)-DP for any δ  0 tuples in P1, and D′ by replacing one tuple in P1 with
a tuple in P2. Then, A(D) and A(D′) must contain diﬀerent
numbers of g1. Let S = A(D), we have Pr[A(D) = S] = 1
and Pr[A(D′) = S] = 0.
3.3 Privacy of Safe k-Anonymization
We now show that strongly-safe k-anonymization algo-
rithm satisﬁes (β, ǫ, δ)-diﬀerential privacy for a small δ with
reasonable values of k and β. We use f (j; n, β) to denote the
probability mass function for the binomial distribution; that
is, f (j; n, β) gives the probability of getting exactly j suc-
cesses in n trials where each trial succeeds with probability
β. And we use F (j; n, β) to denote the cumulative proba-
bility mass function; that is, F (j; n, β) = Pj
Theorem 5. Any strongly-safe k-anonymization algo-
rithm satisﬁes (β, ǫ, δ)-DPS for any 0 γn f (j; n, β),
n:n≥l k
γ −1mPn
where γ = (eǫ−1+β)
See Appendix A.2 for the proof.
eǫ
.
The function d relates the four parameters ǫ, β, k, δ by
requiring δ = d(k, β, ǫ). Note that the other requirement
is that ǫ ≥ − ln(1 − β). Among the four parameters, ǫ
and δ deﬁne the level of privacy protection, while k and β
aﬀect the quality of anonymized data. We now examine the
relationships among these four parameters.
γ − 1m that maxi-
j>γn f (j; n, β). We ﬁrst observe that γ > β because
To compute this, we want to ﬁnd n ≥ l k
mizes Pn
γ − β = (eǫ−1+β)
That is, Pn
− β = (eǫ−1)(1−β)
j>γn f (j; n, β) sums up the tail binomial dis-
tribution probabilities for the portion of the tail beyond γn,
as shown in Figure 1. Following the intuition behind the
law of large numbers, the larger the value of n, the smaller
this tail probability. Hence intuitively, choosing the smallest
> 0.
eǫ
eǫ
value of n, i.e., n = nm = l k
γ − 1m should maximize the for-
mula. Unfortunately, due to the discrete nature of the bino-
mial distribution, the maximum value may not be reached
at nm, but instead at one of the next few local maximal
points l k+1
γ − 1m, l k+2
γ − 1m, · · · . Thus we are unable to
further simplify the representation of the function d(k, β, ǫ).
We now report the relationships among ǫ, β, k, δ using nu-
merical computation. In Table 2, we ﬁx k = 20 and report
the values of δ under diﬀerent ǫ and β values. The table
shows that the values of δ can be very small. We note that
with ﬁxed k and β, δ decreases as ǫ increases, which states
that the error probability gets smaller when one relaxes the
ǫ-bound on the probability ratio. In other words, the more
serious a privacy breach, the more unlikely it occurs. The
table also shows that with ﬁxed k and ǫ, δ decreases as β
decreases, meaning that a smaller sampling probability im-
proves the privacy protection.
In Figure 2, we show the results from examining the rela-
tionship between ǫ and δ when we vary k ∈ {5, 10, 20, 30, 50}
under ﬁxed β = 0.2. We plot 1
δ against ǫ for values of
ǫ > − ln(1 − β). The ﬁgure indicates a negative correlation
between ǫ and δ. Furthermore, increasing k has a close to
exponential eﬀect of improving privacy protection. For ex-
ample, when ǫ = 2, increasing k by 10 roughly decreases δ
by 10−5.
ǫ
β
0.05
0.1
0.2
0.25
6.83×10−10
4.19×10−06
2.16×10−03
0.5
2.50×10−14
1.61×10−09
8.02×10−06
0.75
3.19×10−17
3.44×10−12
1.89×10−07
1.0
1.76×10−19
4.07×10−14
6.03×10−09
1.5
3.97×10−22
3.22×10−16
4.79×10−11
2.0
2.00×10−24
1.89×10−18
1.59×10−12
Table 2: A table showing the relationship between β and ǫ in determining the value of δ when k is ﬁxed. In
the above k = 20, and each cell in the table reports the value of δ under the given values of β and ǫ
k = 50, β = 0.2
k = 30, β = 0.2
k = 20, β = 0.2
k = 10, β = 0.2
k = 5, β = 0.2
 1e+30
 1e+25
 1e+20
 1e+15
 1e+10
 100000
1 δ
 1
 0
 0.5
 1
ǫ
 1.5
 2
Figure 2: A graph showing the relationship between
ǫ and 1
δ if we vary the values of k under ﬁxed β
k = 20, β = 0.05
k = 20, β = 0.1
k = 20, β = 0.2
k = 20, β = 0.3
k = 20, β = 0.4
 1e+25
 1e+20
 1e+15
1 δ
 1e+10
 100000
 1
 0
 0.5
 1
ǫ
 1.5
 2
Figure 3: A graph showing the relationship between
ǫ and 1
δ if we vary the values of β under ﬁxed k.
βn γn
Figure 1: A graph showing the relationship between
βn and γn on a binomial curve
In Figure 3, we show the results from examining the eﬀect
of varying β ∈ {0.05, 0.1, 0.2, 0.3, 0.4} under a ﬁxed value of
k = 20. This shows that decreasing β also dramatically
improve the privacy protection. The two ﬁgures indicate
the intricate relationship between privacy and utility.
In Figure 4, we explore this phenomenon that increasing k
and decreasing β both improve privacy protection. Starting
from (k = 15, β = 0.05), each time we double β and ﬁnd a
value k that gives a similar level of privacy protection. We
ﬁnds that k increases from 15 to 22 (for β = 0.1), 35 (for
β = 0.2), and 60 (for β = 0.4).
In Figure 5, we examine the quality of privacy protection
for very small k’s (from 1 to 5). We choose a very small sam-
pling probability of β = 0.025. Not surprisingly, when k = 1,
the privacy protection is entirely from the sampling eﬀect, as
the obtained δ value is less than β. However, when k ≥ 2, we
start seeing privacy protection eﬀect from k-anonymization,
with δ (γn f (j; n, β),
n:n≥⌈ k
γ −1⌉Pn