disease and COVID-19. Journal of Translational Medicine 19, 1 (2021).
by learning an invariant mapping. In CVPR.
[18] Inken Hagestedt, Mathias Humbert, Pascal Berrang, Irina Lehmann, Roland Eils,
Michael Backes, and Yang Zhang. 2020. Membership inference against DNA
methylation databases. In EuroS&P.
[19] Inken Hagestedt, Yang Zhang, Mathias Humbert, Pascal Berrang, Haixu Tang, Xi-
aoFeng Wang, and Michael Backes. 2019. MBeacon: Privacy-Preserving Beacons
for DNA Methylation Data. In NDSS.
mentum contrast for unsupervised visual representation learning. In CVPR.
learning for image recognition. In CVPR.
Contrastive Learning. arXiv preprint arXiv:2102.04140 (2021).
bership Inference Attacks Using Transfer Shadow Training. In IJCNN.
[24] Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhenqiang Gong, and
Yinzhi Cao. 2021. Practical Blind Membership Inference Attack via Differential
Comparisons. In NDSS.
[25] Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta,
and Lun Wang. 2019. Towards practical differentially private convex optimization.
In IEEE S & P.
machine learning in practice. In USENIX Security Symposium.
[27] Jinyuan Jia and Neil Zhenqiang Gong. 2018. AttriGuard: A practical defense
against attribute inference attacks via adversarial machine learning. In USENIX
Security Symposium.
[28] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. 2022. BadEncoder: Backdoor
Attacks to Pre-trained Encoders in Self-Supervised Learning. In IEEE S & P.
[29] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang
Gong. 2019. Memguard: Defending against black-box membership inference
attacks via adversarial examples. In CCS.
Tech Report (2009).
and Defenses in Classification Models. In CODASPY.
In CCS.
[23] Seira Hidano, Takao Murakami, and Yusuke Kawamoto. 2021. TransMIA: Mem-
[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-
[32] Zheng Li and Yang Zhang. 2021. Membership Leakage in Label-Only Exposures.
[30] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
[22] Xinlei He and Yang Zhang. 2021. Quantifying and Mitigating Privacy Risks of
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
[31] Jiacheng Li, Ninghui Li, and Bruno Ribeiro. 2021. Membership Inference Attacks
[26] Bargav Jayaraman and David Evans. 2019. Evaluating differentially private
[37] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
[34] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine learning with
[33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In ICLR.
membership privacy using adversarial regularization. In CCS.
[35] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In IEEE S & P.
[36] Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas
Carlini. 2021. Adversary Instantiation: Lower Bounds for Differentially Private
Machine Learning. In IEEE S & P.
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from natural language supervision.
arXiv preprint arXiv:2103.00020 (2021).
[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI
blog 1, 8 (2019), 9.
[40] Marc’Aurelio Ranzato, Fu Jie Huang, Y-Lan Boureau, and Yann LeCun. 2007.
Unsupervised learning of invariant feature hierarchies with applications to object
recognition. In CVPR.
[41] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and
Hervé Jégou. 2019. White-box vs black-box: Bayes optimal strategies for mem-
bership inference. In ICML.
[42] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2019. Ml-leaks: Model and data independent membership infer-
ence attacks and defenses on machine learning models. In NDSS.
CCS.
bership inference attacks against machine learning models. In IEEE S & P.
for large-scale image recognition. In ICLR.
bedding models. In CCS.
machine learning models. In USENIX Security Symposium.
machine learning models against adversarial examples. In CCS.
[49] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction apis. In USENIX Security
Symposium.
[50] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
2008. Extracting and composing robust features with denoising autoencoders. In
ICML.
machine learning. In IEEE S & P.
risk in machine learning: Analyzing the connection to overfitting. In CSF.
Salakhutdinov, and Alexander Smola. 2017. Deep sets. In NeurIPS.
[54] Yang Zou, Zhikun Zhang, Michael Backes, and Yang Zhang. 2020. Privacy
Analysis of Deep Learning in the Wild: Membership Inference Attacks against
Transfer Learning. arXiv preprint arXiv:2009.04872 (2020).
[44] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
[46] Congzheng Song and Ananth Raghunathan. 2020. Information leakage in em-
[52] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
[45] Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks
[53] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan
[48] Liwei Song, Reza Shokri, and Prateek Mittal. 2019. Privacy risks of securing
[51] Binghui Wang and Neil Zhenqiang Gong. 2018. Stealing hyperparameters in
[47] Liwei Song and Prateek Mittal. 2021. Systematic evaluation of privacy risks of
[43] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In
A KEYWORDS FOR GOOGLE SEARCH AND
FLICKR CRALWER
The keywords are the 100 class names in CIFAR100: beaver, dolphin,
otter, seal, whale, aquarium fish, flatfish, ray, shark, trout, orchids,
poppies, roses, sunflowers, tulips, bottles, bowls, cans, cups, plates,
apples, mushrooms, oranges, pears, sweet peppers, clock, com-
puter keyboard, lamp, telephone, television, bed, chair, couch, table,
wardrobe, bee, beetle, butterfly, caterpillar, cockroach, bear, leopard,
lion, tiger, wolf, bridge, castle, house, road, skyscraper, cloud, forest,
mountain, plain, sea, camel, cattle, chimpanzee, elephant, kangaroo,
fox, porcupine, possum, raccoon, skunk, crab, lobster, snail, spi-
der, worm, baby, boy, girl, man, woman, crocodile, dinosaur, lizard,
snake, turtle, hamster, mouse, rabbit, shrew, squirrel, maple, oak,
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2094Table 5: Average accuracy, precision, and recall (%) of our methods for the target encoder pre-trained on STL10 dataset. √ (or
×) means the inferrer has (or does not have) access to the corresponding background knowledge of the target encoder. The
numbers in parenthesis are standard deviations in 5 trials.
Pre-training
data distribution
Encoder
architecture
Training
algorithm
×
√
×
×
√
√
×
√
×
×
√
×
√
×
√
√
×
×
×
√
×
√
√
√
Precision
Encod-
erMI-S
Accuracy
Encod-
erMI-S
Encod-
erMI-T
Encod-
erMI-T
Encod-
erMI-V
Encod-
erMI-V
Encod-
Encod-
erMI-V
erMI-T
81.9 (1.94) 80.4 (1.79) 81.3 (1.74) 79.1 (2.35) 77.2 (2.04) 78.4 (1.87) 90.2 (2.79) 92.0 (2.61) 90.3 (2.03)
83.8 (1.81) 83.4 (1.79) 82.4 (1.72) 81.2 (2.38) 79.8 (1.49) 80.1 (1.79) 91.2 (2.32) 92.3 (2.41) 91.2 (1.32)
84.5 (1.72) 83.1 (1.62) 82.6 (1.42) 81.1 (1.87) 79.3 (1.84) 78.9 (1.32) 92.6 (1.26) 92.7 (1.35) 92.6 (0.97)
86.9 (1.04) 85.6 (0.98) 85.6 (0.83) 82.5 (0.88) 81.2 (0.79) 81.4 (0.73) 96.4 (1.32) 93.6 (1.05) 95.0 (0.94)
84.7 (1.53) 83.2 (1.46) 82.7 (1.24) 81.3 (1.47) 79.4 (1.29) 78.9 (1.03) 92.9 (2.12) 91.7 (1.97) 91.6 (1.61)
87.1 (0.69) 85.1 (0.69) 86.1 (0.58) 82.0 (0.71) 81.2 (0.67) 81.7 (0.61) 97.1 (0.93) 96.2 (0.97) 97.3 (0.86)
90.0 (0.57) 85.8 (0.59) 87.4 (0.44) 87.0 (0.73) 84.8 (0.91) 83.6 (0.82) 94.1 (0.99) 90.1 (1.04) 93.2 (0.85)
90.1 (0.54) 86.2 (0.66) 88.7 (0.49) 87.2 (0.67) 83.1 (0.59) 83.2 (0.51) 94.1 (1.04) 92.2 (0.92) 95.1 (1.01)
Recall
Encod-
erMI-S
Table 6: Average accuracy, precision, and recall (%) of our methods for the target encoder pre-trained on Tiny-ImageNet dataset.
√ (or ×) means the inferrer has (or does not have) access to the corresponding background knowledge of the target encoder.
The numbers in parenthesis are standard deviations in 5 trials.
Pre-training
data distribution
Encoder
architecture
Training
algorithm
×
√
×
×
√
√
×
√
×
×
√
×
√
×
√
√
×
×
×
√
×
√
√
√
Precision
Encod-
erMI-S
Accuracy
Encod-
erMI-S
Encod-
erMI-T
Encod-
erMI-T
Encod-
erMI-V
Encod-
erMI-V
Encod-
Encod-
erMI-V
erMI-T
88.7 (1.81) 84.9 (1.73) 85.3 (1.67) 86.0 (1.98) 81.5 (2.03) 81.8 (1.74) 90.1 (1.96) 95.3 (1.67) 95.9 (1.44)
93.0 (1.74) 88.2 (1.68) 90.0 (1.44) 90.1 (1.39) 85.4 (1.45) 86.8 (1.23) 97.8 (1.26) 93.2 (1.22) 97.1 (1.11)
89.1 (1.63) 86.4 (1.64) 85.7 (1.29) 83.3 (1.88) 84.0 (1.84) 80.1 (1.63) 96.3 (1.22) 91.1 (1.29) 96.1 (1.08)
94.1 (1.07) 91.3 (1.03) 94.1 (0.91) 90.7 (0.88) 90.3 (0.87) 93.5 (0.79) 97.4 (0.92) 91.3 (1.22) 95.6 (0.93)
94.4 (1.38) 90.4 (1.33) 91.5 (1.26) 97.4 (0.96) 94.1 (0.91) 93.8 (0.91) 90.8 (1.46) 87.1 (1.37) 89.4 (1.22)
96.1 (0.67) 91.6 (0.69) 94.1 (0.54) 93.8 (0.73) 90.4 (0.68) 94.2 (0.62) 97.6 (0.99) 92.3 (1.02) 95.7 (0.88)
94.5 (0.59) 91.8 (0.56) 92.0 (0.53) 92.3 (0.93) 94.4 (0.91) 94.1 (0.86) 96.7 (0.92) 90.6 (0.79) 92.7 (0.77)
96.5 (0.51) 92.0 (0.47) 94.3 (0.43) 96.6 (0.72) 92.9 (0.59) 94.9 (0.57) 97.0 (0.93) 92.4 (0.89) 93.2 (0.91)
Recall
Encod-
erMI-S
Table 7: Accuracy, precision, and recall (%) of Baseline-E
with 3×1 patches and 3×5 patches.
palm, pine, willow, bicycle, bus, motorcycle, pickup truck, train,
lawn-mower, rocket, streetcar, tank, tractor.
(a) 3 × 1 Patches
Pre-training dataset Accuracy Precision Recall
79.3
65.7
59.5
CIFAR10
STL10
Tiny-ImageNet
60.2
64.1
65.8
57.4
63.7
68.2
(b) 3 × 5 Patches
Pre-training dataset Accuracy Precision Recall
62.9
59.3
58.4
CIFAR10
STL10
Tiny-ImageNet
50.8
55.8
52.7
50.6
55.5
52.4
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2095