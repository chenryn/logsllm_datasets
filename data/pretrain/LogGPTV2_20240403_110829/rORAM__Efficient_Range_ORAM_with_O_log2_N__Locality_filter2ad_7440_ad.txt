Keeping the locality-aware
physical layout in mind, we map the blocks in the same range
in Ri
to paths that occur successively in the bit-reversed
ordering of their leaf identiﬁers. Then, the blocks in the range
will appear physically adjacent to each other across levels, and
we can reduce the number of seeks required to read the range.
In particular, consider logical addresses in [j·2i, (j + 1)·2i)
in Ri. Letting a = j · 2i, the addresses are mapped to paths
• Address a: Address a is mapped to a random path, i.e.,
in the tree as follows:
h) where r is chosen at random and h = log N .
P (vr
• Adresss a + j: For j = 1, . . . , 2i − 1, address a + j is
mapped to a path P (vr+j mod N
This ensures that Ri.ReadRange requires O(log N ) disk
seeks: sequentially scan 2i buckets from each level of the
tree (with wraparound), that are invariably adjacent to each
other on storage. Note that the number of seeks required is
independent of the range size, 2i.
h
).
D. Insight 3: Distributed Position Map
Although, the above techniques can achieve the desired
O(log N ) seeks for ReadRange and O(log2 N ) seeks for
BatchEvict’s, a critical challenge we are yet to address is
optimizing the cost of multiple position map lookups while
updating ℓ + 1 sub-ORAMs. We detail our solution by ﬁrst
describing the challenges involved.
Challenges of na¨ıve position map construction. In rORAM,
each of the sub-ORAMs is uniquely addressed for the blocks
they store, even though the same data blocks are duplicated
across all ORAMs. As a result, a separate position map has
8
data
a
p0
p1
pℓ
···
Fig. 4. The structure of a physical block. In addition to data, the block
contains the logical address a and the path locations p0, . . . , pℓ of
the block in ORAMs R0 . . . , Rℓ respectively.
to be maintained for each sub-ORAM. If stored locally, this
extra data is handled at no added cost, but typically the size
of the position map would exceed local storage requirements,
and would need to be stored securely in the remote storage.
Typically this requires a recursive ORAM or an oblivious
trie [36, 41] to store the position map obliviously – both
solutions require O(log N ) disk seeks and bandwidth for a
position map query 2.
Recall that an Access operation executes ReadRange once
and BatchEvict (ℓ + 1) times. Each operation needs to look
up the corresponding position-map in order to translate a
logical address to a tree path, which increases both seeks and
bandwidth for Ri.BatchEvict from O(log N ) to O(log2 N ).
Consequently, Access requires O(ℓ · log2 N ) = O(log3 N )
seeks and bandwidth.
The goal is to reduce the overall number of seeks and
bandwidth from O(log3 N ) to O(log2 N ) for Access in rO-
RAM, and the key to achieving this is to reduce the number
of expensive position-map look-ups to just one.
Reusing physical paths in unread Path ORAMs. A key
observation is that since ReadRange reads paths corresponding
to a range from only one of the sub-ORAMs, say Ri, the
locations of these blocks in the other ORAMs, Rj 6= Ri still
remain hidden. Recall that while data is duplicated across the
ORAMs, the paths (or sometimes called tag hereafter) along
which the same logical data blocks are placed in the different
sub-ORAMs are independently assigned. Thus, the location
of blocks in the queried range in Ri, does not reveal any
information about their location in the other sub-ORAMs.
After the ReadRange the data blocks may be updated,
and these updates must propagate to all the other ORAMs
through a commensurate number of BatchEvict’s. During a
BatchEvict to sub-ORAM Rj 6= Ri, the queried blocks can
be written back along the same paths to which they are already
mapped, effectively reusing their tags. This eliminates the
expensive position map updates for these ORAMs. Reusing the
tags in all the other sub-ORAMs is an important stepping stone
for achieving better efﬁciency, because the (ℓ+1) BatchEvict’s
will now only need to update the position map for Ri. We note
that a similar observation has been made in a different context
of constructing a PIR-based 2-server ORAM [43].
Unfortunately, this is not enough to reduce the position map
lookups because while tags are only being updated in one of
the sub-ORAMs, a position map look up is required in all
the other sub-ORAMs to determine the existing tags of the
data being evicted as part of the BatchEvict. As a result, the
number of seeks and communication remain O(log3 N ).
2Note that the position map for each of the range ORAMs only needs to
store the start position of the range as subsequent positions can be calculated
by incrementing in bit-reversed order. As a result, larger range ORAMs have
signiﬁcantly smaller position maps that may not need to be stored recursively,
but the position maps of small range ORAMs are the worst case in the analysis.
Pointer-based oblivious data structures. To solve the prob-
lem of updating multiple ORAMs where data may be dupli-
cated, we propose a new distributed position map construction
leveraging pointer-based oblivious data structure techniques,
initially introduced by Wang et al. [44] and subsequently used
by several ORAM solutions [15, 35, 36, 44]. In particular,
alongside each physical block we store the path tag of that
block in all ORAMs, as shown in Figure 4.
As an example, to query a a range of length 2 at logical
addresses a and b = a + 1 the following procedure is used:
1) Refer to the position map of R1 and obtain the path tag
p1 of a in R1.
2) Read the two consecutive physical paths (according to
the reversed-bit order) based on p1 in R1. Let
(da, a, p0, p1, . . . , pℓ),
(db, b, q0, q1, . . . , qℓ)
be the two physical blocks retrieved in this stage. Here
pj (resp., qj ) denotes the path tag for address a (resp., b)
in ORAM Rj .
• The leaf label stored for a range corresponds to the leaf
to which the ﬁrst block in the range is mapped. This
is enough since once the leaf label of the ﬁrst block is
known, the leaf labels for the remaining blocks can be
easily determined due to the locality-sensitive mapping.
Depending on the setting, each position map is stored either
on the client-side or on the server side (in a recursive ORAM
or in an oblivious trie). rORAM also stores a stash for each
ORAM on the client-side to handle overﬂows from the tree.
Notations and parameters. Let N be the number of logical
blocks that rORAM stores, and L be the maximum range size
the rORAM needs to support. Let ℓ = ⌈log2 L⌉. Then, rORAM
has ℓ + 1 ORAMs R0, R1, . . . , Rℓ.
Let h = ⌈log2 N⌉ denote the height of each ORAM tree Ri.
A bucket label vr
i signiﬁes the rth bucket among those at level
i in bit-reversed order. In an ORAM tree T , let PT (vr
h) be a
path from the root to a leaf vr
h; we will often omit subscript T
if obvious from the context. Note that the following property
holds in an ORAM tree:
3) Choose p′
1 at random. Compute q′
according to the reversed-bit order. Let d′
updated data.
1 to be next to p′
1
b be the
a, d′
4) Update the position map of R1 so that the path tag of a
should be p′
1.
5) For i = 0, . . . , ℓ:
Push the following two blocks in the stash for Ri:
(d′
a, a, p0, p′
(d′
Then, execute Ri.BatchEvict(2).
1, p2 . . . , pℓ),
b, b, q0, q′
1, q2 . . . , qℓ).
Note that the above procedure uses only a single position-
map access (i.e., for R1) in order to identify the path tag p1,
which needs O(log2 N ) seeks. The path tags in other ORAMs
were obtained from the retrieved physical blocks and then
reused in BatchEvict, which requires O(log2 N ) seeks as well.
Consequently, rORAM only requires O(log2 N ) seeks in total.
Handling duplicates. One thing to note here is that after the
required range has been read from Ri, it is evicted back to
all ORAMs R0, . . . , Rℓ and so there must be a process for
handling duplicates. Since we do not read blocks in the range
from Rj but add copies during the batched eviction, a block
may have multiple copies in the tree that need to be removed
during subsequent evictions.
This, however, is not a problem. Since the path tag will be
reused in Rj , its old copy will also be along the same path
that includes a newer copy and will be lower down in the
tree. Thus, when the path is retrieved during an eviction the
duplicate blocks in the lower level would be recognized as
older and safely overwritten.
V. FORMAL DESCRIPTION
Position map and stash.
rORAM requires two supporting
data structures, the position map and the stash, similar to
Path ORAM. Each sub-ORAM in rORAM has a position map
similar to the position map for existing tree-based ORAMs
with a couple of modiﬁcations:
• Instead of mapping a block ID (i.e., logical address) to
a leaf identiﬁer (i.e., physical location), a range ID is
mapped to a leaf label.
P (vr
h) = {vr mod 2j
j
: j = 0, . . . , h}.
In the algorithm descriptions, we use Vj to refer to the set of
nodes on level j among the currently-considered paths.
Let PMi and stashi denote the position map and stash
for ORAM Ri. Let cnt be a global integer variable, initially
0, which is used to track the deterministic eviction schedule
according to the bit-reversed order.
A physical bucket (d, a, p0, . . . , pℓ) is valid if every pj falls
in the valid range [0, N ). Let Z be the number of physical
blocks that a bucket vj
i contains. Dummy (invalid) blocks
are used to pad buckets to approriate size in case the bucket
contains less than Z real data blocks.
ReadRange. The ReadRange operation for ORAM Ri is
described in Algorithm 1, and it returns the result set of
blocks with position meta-data as well as a new path position,
p′ for the start address a. The operations performs three tasks:
1) Query the position map to determine the leaf label to
which the ﬁrst block in the range is mapped (Step 3).
2) Update the position map with a new leaf label for the
ﬁrst block in the range (Steps 4-5).
3) Retrieve the buckets along the paths to which the blocks
of the range are mapped, level by level while scanning for
the required blocks (Steps 6–9). Note that the if-statement
on Step 9 handles the duplicates by ignoring older blocks
on lower levels.
// Get the leaf label p for address a
Algorithm 1 Ri.ReadRange(a)
1: Let U := [a, a + 2i).
2: result ← Scan stashi for blocks in range U .
3: p ← PMi.query(a)
4: p′ ← [0, N )
5: PMi.update(a, p′) // Update the position map for address a
6: for j = 0, . . . , h do
7:
8:
9:
10: return (result, p′)
Read the ORAM buckets V = {vt mod 2j
for each valid block B = (d, a, p0, . . . , pℓ) in V do
// random leaf label p′
j
if B.a ∈ U and B 6∈ result then result ← result ∪ {B}
: t ∈ [p, p + 2i)}.
BatchEvict. The BatchEvict operation is described in Algo-
rithm 2. The operation performs three tasks:
9
1) Read the buckets from the server along the next k eviction
paths level by level (Steps 1-5).
2) Evict blocks locally to the eviction paths (Steps 6-11).
3) Write back the updated buckets read to the tree in the
level-by-level manner (Steps 12-13).
Algorithm 2 Ri.BatchEvict(k)
// cnt: a global integer variable tracking the eviction schedule
// h = log N : the height of the ORAM tree.
// Fetch buckets from server
1: for j = 0, . . . , h do
2:
3:
4:
5:
stashi ← stashi ∪ {B}
Read ORAM buckets Vj = {vt mod 2j
for each valid block B = (d, a, p0, . . . , pℓ) in V do
j
: t ∈ [cnt, cnt + k)}.
if stashi has no block with address B.a then
// Evict paths and write buckets back to server
6: for j = h, . . . , 0 do // Evicting paths: bottom-up, level-by-level
for r ∈ {t mod 2j : t ∈ [cnt, k + cnt)} do // For each path
7:
S ′ ← {(d, a, p0, . . . , pℓ) ∈ stashi : pi ≡ r (mod 2j )}
8:
S ′ ← Select min(|S ′|, Z) blocks from S ′
9:
stashi ← stashi / S ′
10:
vr mod 2j
11:
j
← S ′.
// Write back buckets to server
12: for j = 0, . . . , h do
13:
Write the ORAM buckets {vt mod 2j
j
: t ∈ [cnt, cnt + k)}.
Access protocol in rORAM. We are ready to give the formal
description of the Access protocol of rORAM. The protocol
supports any range of size r ≤ L starting at any given addres
a ∈ [0, N − r). As explained in Section IV, this will be
partitioned into two ranges of size ⌈log2 r⌉.
The Access protocol, described in Algorithm 3, takes the
following input: a the start address of the range; r is the size
of the range; op is the operation, either read or write; and D∗
the new data, optionally, to be updated during a write for data
in the range. The operation is performed in two main tasks,
each performed twice to cover arbitrary ranges obliviously:
1) Perform two ReadRanges on the ﬁrst/second half of the
range, retrieve data, and update positions (Steps 4–7).
2) Perform a BatchEvict by updating the each ORAM’s
stash with the new data (Steps 10-13). Note that Step
11 is necessary to ﬁrst remove any old “stale” data from
the stash with the same address as one in the range.
On a write, the data is updated between these steps (Steps 8–
9). On a read, the values fetched within the requested range
are returned at the end (Step 15).
VI. ANALYSIS
Correctness and obliviousness. Correctness of our protocol
follows by inspection. Obliviousness, with leakage of the
length of the given range, holds from the following facts:
• All data items exchanged over the network are encrypted
with IND-CPA secure encryption.
• ReadRange: We choose ORAM Ri based only on the
length of the range. In ORAM Ri, the paths selected for
reading do not reveal any information to the adversary
other than the fact that two ReadRange operations oc-