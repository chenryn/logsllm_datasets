title:Do You Hear What I Hear?: Fingerprinting Smart Devices Through Embedded
Acoustic Components
author:Anupam Das and
Nikita Borisov and
Matthew Caesar
Do You Hear What I Hear? Fingerprinting Smart Devices
Through Embedded Acoustic Components
Anupam Das
University of Illinois at
Urbana-Champaign
PI:EMAIL
Nikita Borisov
University of Illinois at
Urbana-Champaign
PI:EMAIL
Matthew Caesar
University of Illinois at
Urbana-Champaign
PI:EMAIL
ABSTRACT
The widespread use of smart devices gives rise to privacy concerns.
Fingerprinting smart devices can jeopardize privacy by allowing re-
mote identiﬁcation without user awareness. We study the feasibil-
ity of using microphones and speakers embedded in smartphones
to uniquely ﬁngerprint individual devices. During fabrication, sub-
tle imperfections arise in device microphones and speakers, which
induce anomalies in produced and received sounds. We exploit
this observation to ﬁngerprint smartphones through playback and
recording of audio samples. We explore different acoustic features
and analyze their ability to successfully ﬁngerprint smartphones.
Our experiments show that not only is it possible to ﬁngerprint de-
vices manufactured by different vendors but also devices that have
the same maker and model; on average we were able to accurately
attribute 98% of all recorded audio clips from 50 different Android
smartphones. Our study also identiﬁes the prominent acoustic fea-
tures capable of ﬁngerprinting smart devices with a high success
rate, and examines the effect of background noise and other vari-
ables on ﬁngerprinting accuracy.
Categories and Subject Descriptors
K.6.m [Management of Computing and Information Systems]:
Miscellaneous — Security; H.5.1 [Multimedia Information Sys-
tems]: Audio input/output
Keywords
Fingerprinting; Privacy; Acoustic feature; Microphone; Speaker
1.
INTRODUCTION
Mobile devices, including smartphones, PDAs, and tablets, are
quickly becoming widespread in modern society. In 2012 a total
of 1.94 billion mobile devices were shipped, of which 75% were
smart and highly-featured phones [5, 8, 14]. Canalys predicted that
the mobile device market will reach 2.6 billion units by 2016, with
smartphones and tablets continuing to dominate shipments [14].
The rapid uptake of intelligent mobile devices is not surprising, due
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660325.
to the numerous advantages they provide consumers, from enter-
tainment and social applications to business and advanced comput-
ing capabilities. However, smartphones, with all their interactive,
location-centric, and connectivity-based features impose threaten-
ing concerns on user privacy [9, 34, 39, 51].
In this paper we thoroughly analyze a technique for ﬁngerprint-
ing the hardware of smartphones. The observation is that even if
the software on mobile devices is strengthened [35,62,70], hardware-
level idiosyncrasies in microphones and speakers can be used to
ﬁngerprint physical devices. During manufacturing, imperfections
are introduced in the analog circuitry of these components, and as
such, two microphones and speakers are never alike. Through an
observational study, we ﬁnd that these imperfections are substantial
enough, and prevalent enough, that we can reliably distinguish be-
tween devices by passively recording audio streams, and conduct-
ing simple spectral analyses on the recorded audio streams. Our
approach can substantially simplify the ability for an adversary to
track and identify people in public locations which can threaten
the privacy of mobile device users. Our approach requires small
amounts of data — for example, we show that with our technique,
an adversary could even use the short ringtones produces by mo-
bile device speakers to reliably track users in public environments.
Alternatively, a stealthy app (e.g., an online game) can access the
microphone to uniquely distinguish all users running the app.
Our approach centers around the development of a ﬁngerprint-
ing mechanism, which aims to “pull out” imperfections in device
circuitry. Our mechanism has two parts: a method to extract au-
ditory ﬁngerprints and a method to efﬁciently search for matching
ﬁngerprints from a database. To generate ﬁngerprints of speakers
we record audio clips played from smartphones onto an external
device (i.e., laptop/PC) and vice versa for generating ﬁngerprints
of microphones. We also generate the combined ﬁngerprint of
speaker and microphone by playing and recording audio clips si-
multaneously on smartphones. We use two different classiﬁers to
evaluate our ﬁngerprinting approach. Moreover, we test our ﬁn-
gerprinting approach for different genre of audio clips at various
frequencies. We also study various audio features that can be used
to accurately ﬁngerprint smartphones. Our studies reveals that mel-
frequency cepstral coefﬁcients (MFCCs) are the dominant features
for ﬁngerprinting smartphones. Lastly, we analyze the sensitivity of
our ﬁngerprinting approach against different factors like sampling
frequency, distance between speaker and recorder, training set size
and ambient background noise.
Contributions. Our contributions are summarized below:
• We analyze the feasibility of ﬁngerprinting smart devices by
leveraging the manufacturing idiosyncrasies of microphones
and speakers embedded in smart devices.
• We study a large spectrum of existing audio features and their
ability to accurately ﬁngerprint smartphones. We ﬁnd that
mel-frequency cepstral coefﬁcients (MFCCs) perform partic-
ularly well in ﬁngerprinting smartphones.
• We investigate two different classiﬁers to evaluate our ﬁnger-
printing approach. We conclude that Gaussian mixture mod-
els (GMMs) are more effective compared to k-NN classiﬁers
in classifying recorded audio ﬁngerprints.
• We perform experiments across several different genres of
audio excerpts. We also analyze how different factors like
sampling frequency, distance between speaker and recorder,
training set size and ambient background noise impact our
ﬁngerprinting accuracy.
Roadmap. The remainder of this paper is organized as follows.
We give an overview of our ﬁngerprinting approach in Section 2.
We discussed related work in Section 3. In Section 4, we discuss
why microphones and speakers integrated in smartphones can be
used to generate unique ﬁngerprints. In Section 5, we describe the
different audio features considered in our experiments, along with
the classiﬁcation algorithms used in our evaluation. We present our
experimental results in Section 6. We also discuss some limitations
of our approach in Section 7. Finally, we conclude in Section 8.
2. OVERVIEW
In this section we give an overview of our approach and present
several viable attack scenarios. We also identify the key challenges
that we address in this paper.
The key observation behind our work is that imperfections in
smart device hardware induce unique signatures on the received
and transmitted audio streams, and these unique signatures, if iden-
tiﬁed, can be used by an adversary to ﬁngerprint the device. We
consider three ﬁngerprinting scenarios: speaker, microphone, and
joint speaker-microphone ﬁngerprinting.
In the ﬁrst case, an at-
tacker in a public environment, such as a cafe or shopping mall,
records audio generated by a smartphone speaker, such as a ring-
tone. The attacker can then use the recorded audio samples to
track and identify users. Alternately, the attacker may obtain audio
recorded by a smartphone microphone and use that to identify the
user who made the recording; this can have forensic applications.
A third way to track users is to convince them to install a malicious
application (e.g., a free online game), which can play and record au-
dio clips using the device’s speaker and microphone. The app can
then stealthily upload the recorded audio clips to the attacker (e.g.,
piggybacking it on log-in information or game state), who can then
use the audio samples to uniquely distinguish each user. To do this,
the application would require access to both the speaker and mi-
crophone, as well as network access, but such permissions are not
unusual for applications and are unlikely to raise alarm, especially
given that a signiﬁcant portion of the users cannot comprehend the
full consequences of smartphone permissions [36, 45].
Our approach consists of two main tasks. The ﬁrst task is acquir-
ing a set of audio samples for analysis in the ﬁrst place. To do this,
we have a listener module, responsible for receiving and recording
device audio. The listener module could be deployed as an applica-
tion on the smart device (many mobile OSes allow direct access to
microphone inputs), or as a stand-alone service (e.g., the adversary
has a microphone in a public setting to pick up device ringtones).
The next task is to effectively identify device signatures from the
received audio stream. To do this, we have an analyzer module,
which leverages signal processing techniques to localize spectral
anomalies, and constructs a ‘ﬁngerprint’ of the auditory character-
istics of the device. A critical part of this task involves determining
what sort of acoustic features and audio analysis techniques are
most effective in identifying unique signatures of device-hardware.
There are a large number of audio properties which could be used
(spectral entropy, zero crossings, etc.) as well as a broad spec-
trum of analysis algorithms that can be used to summarize these
properties (principle component analysis, linear discriminant anal-
ysis, feature selection, etc.). We will study alternative properties to
characterize hardware-induced auditory anomalies in Section 5.1
as well as algorithms for effectively clustering them in Section 5.2.
3. RELATED WORK
Fingerprints have long been used as one of the most common
bio-metrics in identifying human beings [27, 61]. The same con-
cept was extended to identifying and tracking unique mobile trans-
mitters by the US government during 1960s [47]. Later on with
the emergence of cellular networks people were able to uniquely
identify transmitters by analyzing the externally observable char-
acteristics of the emitted radio signals [60].
Physical devices are usually different at either the software or
hardware level even if they are produced by the same vendor. In
terms of software based ﬁngerprinting, researchers have looked at
ﬁngerprinting techniques that differentiate between unique devices
over a Wireless Local Area Network (WLAN) simply through a
timing analysis of 802.11 probe request frames [30]. Others have
looked at exploiting the difference in ﬁrmware and device driver
running on IEEE 802.11 compliant devices [37]. 802.11 MAC
headers have also been used to track unique devices [40]. Pang et
al. [57] were able to exploit trafﬁc patterns to carry out device ﬁn-
gerprinting. Open source toolkits like Nmap [50] and Xprobe [68]
can remotely ﬁngerprint an operating system by identifying unique
responses from the TCP/IP networking stack.
Another angle to software based ﬁngerprinting is to exploit ap-
plications like browsers to carry out device ﬁngerprinting [33]. Yen
et al. [69] were successful at tracking users with high precision by
analyzing month-long logs of Bing and Hotmail. Researchers have
also been able to exploit JavaScript and popular third-party plug-
ins like Flash player to obtain the list of fonts installed in a de-
vice which then enabled them to uniquely track users [18]. Other
researchers have proposed the use of performance benchmarks for
differentiating between JavaScript engines [54]. Furthermore, brows-
ing history can be exploited to ﬁngerprint and track web users [56].
The downside of software based ﬁngerprints is that such ﬁnger-
prints are generated from the current conﬁguration of the system
which is not static, rather it is likely to change over time.
Hardware based ﬁngerprinting approaches rely on some static
source of idiosyncrasies. It has been shown that network devices
tends to have constant clock skews [53] and researchers have been
able to exploit these clock skews to distinguish devices through
TCP and ICMP timestamps [46]. However, clock skew rate is
highly dependent on the experimental environment [67]. Researchers
have also extensively looked at ﬁngerprinting the unique transient
characteristics of radio transmitters (also known as RF ﬁngerprint-
ing). RF ﬁngerprinting has been shown as a means of enhancing
wireless authentication [49, 55]. It has also been used for location
detection [58]. Manufacturing imperfections in network interface
cards (NICs) have also been studied by analyzing analog signals
transmitted from them [21,38]. More recently Dey et al. have stud-
ied manufacturing idiosyncrasies inside smartphone accelerometer
to distinguish devices [31]. However, their approach requires some
form of external stimulation/vibration to successfully capture the
manufacturing imperfection of the on-board accelerometer. More-
over, there are different contexts in which audio prints can be more
useful, e.g., software that is not allowed to access the accelerome-
ter, as well as an external adversary who ﬁngerprints nearby phones
with a microphone.
Our work is inspired by the above work in hardware-based ﬁn-
gerprinting, but we focus on ﬁngerprinting on-board acoustic com-
ponents like speakers and microphones. In this setting, Clarkson’s
work [26] is perhaps the most closely related to ours. He showed
that it is possible to distinguish loudspeakers by analyzing recorded
audio samples emitting from them. However, his experiments used
special audio clips that contained 65 different frequencies, whereas
we are using common audio excerpts like ringtones. Moreover,
his experiments ignored the subtlety introduced by microphones.
In fact in one experiment, though statistically not meaningful as it
tested only two similar microphones, he found no variation across
microphones. We, on the other hand found that microphones can
vary across different units. Finally, his study did not thoroughly an-
alyze the different acoustic features that can be used to successfully
carry out device ﬁngerprinting. As a result, he was able to achieve
only 81% accuracy in distinguishing heterogeneous loudspeakers.
Audio ﬁngerprinting has a rich history of notable research [23].
There are studies that have looked at classifying audio excerpts
based on their content [41, 65]. Others have looked at distinguish-
ing human speakers from audio segments [20, 22]. There has also
been work on exploring various acoustic features for audio classi-
ﬁcation [52]. One of the more popular applications of audio ﬁnger-
printing has been music genre and artist recognition [43, 48].
Our work takes advantage of the large set of acoustic features
that have been explored by existing work in audio ﬁngerprinting.
However, instead of classifying the content of audio segments, we
utilize acoustics features to capture the manufacturing imperfec-
tions of microphones and speakers embedded in smart devices.
4. SOURCE OF FINGERPRINTS
In this section we will take a closer look at the microphones and
speakers embedded on today’s smartphones. This will provide an
understanding of how microphones and speakers can act as a po-
tential source for unique ﬁngerprints.
4.1 Closer Look at Microphones
Microphones in modern smartphones are based on Micro Electro
Mechanical Systems (MEMS) [10,12,17]. To enhance active noise
and echo canceling capabilities, most smartphones today have more
than one MEMS microphone. For example, the iPhone 5 has a total
of three embedded MEMS microphones [10]. According to the
IHS-iSuppli report, Apple and Samsung were the top consumers of
MEMS microphones in 2012, accounting for a combined 54% of
all shipped MEMS microphones [17]. .
A MEMS microphone, sometimes called a microphone chip or
silicon microphone, consists of a coil-less pressure-sensitive di-
aphragm directly etched into a silicon chip. It is comprised of a
MEMS die and a complementary metal-oxide-semiconductor (CM-
OS) die combined in an acoustic housing [7,11]. The CMOS often
includes both a preampliﬁer as well as an analog-to-digital (AD)
converter. Modern fabrication techniques enable highly compact
deigns, making them well suited for integration in digital mobile
devices. The internal architecture of a MEMS microphone is shown
on Figure 1. From the ﬁgure we can see that the MEMS micro-
phone’s physical design is based on a variable capacitor consist-
ing of a highly ﬂexible diaphragm in close proximity to a perfo-
rated, rigid back-plate. The perforations permit the air between
the diaphragm and back-plate to escape. When an acoustic signal
reaches the diaphragm through the acoustic holes, the diaphragm
is set in motion. This mechanical deformation causes capacitive
change which in turn causes voltage change.
In this way sound
pressure is converted into an electrical signal for further processing.
The back-chamber acts as a acoustic resonator and the ventilation
hole allows the air compressed inside the back chamber to ﬂow out,
allowing the diaphragm to move back into its original place.
The sensitivity of the microphone depends on how well the di-
aphragm deﬂects to acoustic pressure; it also depends on the gap
between the static back-plate and the ﬂexible diaphragm. Unfor-
tunately, even though the manufacturing process of these micro-
phones has been streamlined, no two chips roll off the assembly
line functioning in exactly the same way. Imperfections can arise
for the following reasons: slight variations in the chemical compo-
sition of components from one batch to the next, wear in the manu-
facturing machines or changes in temperature and humidity. While
subtle imperfections in the microphone chips may go unnoticed by
human ears, computationally such discrepancies may be sufﬁcient
to discriminate them, as we later show.
4.2 Closer Look at Microspeakers
Micro-speakers are a scaled down version of a basic acoustic
speaker. So lets ﬁrst look at how speakers work before we dis-
cuss how microspeakers can be used to generate unique ﬁnger-
prints. Figure 2(a) shows the basic components of a speaker. The
diaphragm is usually made of paper, plastic or metal and its edges
are connected to the suspension. The suspension is a rim of ﬂexible
material that allows the diaphragm to move. The narrow end of the
diaphragm’s cone is connected to the voice coil. The voice coil is
attached to the basket by a spider (damper), which holds the coil in
position, but allows it to move freely back and forth. A permanent
magnet is positioned directly below the voice coil.
Sound waves are produced whenever electrical current ﬂows thr-
ough the voice coil, which acts as an electromagnet. Running vary-
ing electrical current through the voice coil induces a varying mag-
netic ﬁeld around the coil, altering the magnetization of the metal
it is wrapped around. When the electromagnet’s polar orientation
switches, so does the direction of repulsion and attraction. In this
way, the magnetic force between the voice coil and the permanent
magnet causes the voice coil to vibrate, which in turn vibrates the
speaker diaphragm to generate sound waves.
Figure 2(b) shows a typical MEMS microspeaker chip and Fig-
ure 2(c) shows the components inside the microspeaker [24]. The
components are similar to that of a basic speaker; the only differ-
ence is the size and fabrication process [25, 44, 63]. The amplitude
and frequency of the sound wave produced by the speaker’s di-
aphragm is dictated respectively by the distance and rate at which
the voice coil moves. Each speaker component can introduce varia-
tions into the generated sound. For example, variations in the elec-
tromagnetic properties of the driver can cause differences in the rate
and smoothness at which the diaphragm moves. Therefore, due to
the inevitable variations and imperfections of the manufacturing
process, no two speakers are going to be alike, resulting in subtle
differences in the produced sound. In our work, we develop tech-
niques to computationally localize and evaluate these differences.
5. FEATURES AND ALGORITHMS USED
In this section we brieﬂy describe the acoustic features that we
used in generating device ﬁngerprints. We also discuss the classi-
ﬁcation algorithms used in identifying the devices from which the
ﬁngerprints originated.
5.1 Audio Features
Given our knowledge that imperfections exist in device audio
hardware, we now need some way to detect them. To do this,
our approach identiﬁes acoustic features from an audio stream, and
⇒
Figure 1: The internal architecture of MEMS microphone chip used in smartphones.
Figure 2: (a) The basic components of a speaker, (b) A typical MEMS microspeaker, (c) The internal architecture of a microspeaker chip.
(c)
(a)
(b)
uses the features to construct a ﬁngerprint of the device. Computing
acoustic features from an audio stream has been a subject of much
research [19, 23, 52, 65]. To gain an understanding of how a broad
range of acoustic features are affected by device imperfections we
investigate a total of 15 acoustics features (listed in Table 1), all
of which have been well-documented by researchers. Due to space
limitation we exclude detailed description of each acoustic feature,
however, an elaborate description of the audio features is available
in our technical report [28].
5.2 Classiﬁcation Algorithms
Next, we need some way to leverage the set of features to per-
form device identiﬁcation. To achieve this, we leverage a clas-
siﬁcation algorithm, which takes observations (features) from the
observed device as input, and attempts to classify the device into
one of several previously-observed sets.
To do this, our approach works as follows. First, we perform
a training step, by collecting a number of observations from a set
of devices. Each observation (data point) corresponds to a set of
features observed from that device, represented as a tuple with one
dimension per feature. As such, data points can be thought of as
existing in a hyper-dimensional space, with each axis correspond-
ing to the observed value of a corresponding feature. Our approach
then applies a classiﬁcation algorithm to build a representation of
these data points, which can later be used to associate new observa-
tions with device types. When a new observation is collected, the
classiﬁcation algorithm returns the most likely device that caused
the observation.
To do this effectively, we need an efﬁcient classiﬁcation algo-
rithm. In our work, we compare the performance of two alternate
approaches described below: k-nearest neighbors (associates an
incoming data point with the device corresponding to the nearest
“learned” data points), and Gaussian mixture models (computes a
probability distribution for each device, and determines the maxi-
mal likely association).
k-NN: The k-nearest neighbors algorithm (k-NN) is a nonpara-
metric lazy learning algorithm. The term “non-parametric” means
that the k-NN algorithm does not make any assumptions about
the underlying data distribution, which is useful in analyzing real-
world data with complex underlying distribution. The term “lazy
learning” means that the k-NN algorithm does not use the training
data to make any generalization, rather all the training data are used
in the testing phase making it computationally expensive (however,
optimizations are possible). The k-NN algorithm works by ﬁrst
computing the distance from the input data point to all training data
points and then classiﬁes the input data point by taking a majority
vote of the k closest training records in the feature space [32]. The
best choice of k depends upon the data; generally, larger values of
k reduce the effect of noise on the classiﬁcation, but make bound-
aries between classes less distinct. We will discuss more about the
choice of k in Section 6.
GMM: A Gaussian mixture model is a probabilistic model that
assumes all the data points are generated from a mixture of a ﬁnite
number of Gaussian distributions with unknown parameters. The
unknown patterns and mixture weights are estimated from training
samples using an expectation–maximization (EM) algorithm [29].
During the matching phase the ﬁngerprint for an unknown record-
ing is ﬁrst compared with a database of pre-computed GMMs and