title:Symbiotic routing in future data centers
author:Hussam Abu-Libdeh and
Paolo Costa and
Antony I. T. Rowstron and
Greg O'Shea and
Austin Donnelly
Symbiotic Routing in Future Data Centers
Hussam Abu-Libdeh
Microsoft Research
Cambridge, UK.
PI:EMAIL
∗
Paolo Costa
Microsoft Research
Cambridge, UK.
PI:EMAIL
Antony Rowstron
Microsoft Research
Cambridge, UK.
PI:EMAIL
Greg O’Shea
Microsoft Research
Cambridge, UK.
PI:EMAIL
Austin Donnelly
Microsoft Research
Cambridge, UK.
PI:EMAIL
ABSTRACT
Building distributed applications that run in data centers is
hard. The CamCube project explores the design of a ship-
ping container sized data center with the goal of building an
easier platform on which to build these applications. Cam-
Cube replaces the traditional switch-based network with a
3D torus topology, with each server directly connected to
six other servers. As in other proposals, e.g. DCell and
BCube, multi-hop routing in CamCube requires servers to
participate in packet forwarding. To date, as in existing data
centers, these approaches have all provided a single routing
protocol for the applications.
In this paper we explore if allowing applications to im-
plement their own routing services is advantageous, and if
we can support it eﬃciently. This is based on the obser-
vation that, due to the ﬂexibility oﬀered by the CamCube
API, many applications implemented their own routing pro-
tocol in order to achieve speciﬁc application-level charac-
teristics, such as trading oﬀ higher-latency for better path
convergence. Using large-scale simulations we demonstrate
the beneﬁts and network-level impact of running multiple
routing protocols. We demonstrate that applications are
more eﬃcient and do not generate additional control traﬃc
overhead. This motivates us to design an extended routing
service allowing easy implementation of application-speciﬁc
routing protocols on CamCube. Finally, we demonstrate
that the additional performance overhead incurred when us-
ing the extended routing service on a prototype CamCube
is very low.
∗Work done during internship at Microsoft Research, Cam-
bridge. Currently at Cornell University, Ithaca, NY, USA.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
Categories and Subject Descriptors
C.2 [Computer Systems Organization]: Computer com-
munication networks; H.3.4 [Information systems]: Infor-
mation storage and retrieval—Distributed Systems
General Terms
Algorithms, Design, Performance
Keywords
Data centers, key-value stores, routing protocols
1.
INTRODUCTION
The networks in large-scale data centers, such as those
owned by Amazon, Google, Microsoft and Yahoo, adopt
principles evolved from enterprise and Internet networking.
Applications and services use UDP datagrams or TCP sock-
ets as the primary interface to other services and applica-
tions running inside the data center. This eﬀectively iso-
lates the network from the end-systems, which then have
little control over how the network routes packets. The net-
work operator controls the routing policies and conﬁgura-
tions used in the data center.
This design impacts the services that run in the data cen-
ter. External facing applications, like Search, Hotmail or
Shopping Carts, rely on a set of internal distributed ser-
vices. These services act as the building blocks for the ex-
ternal facing applications, and examples of such services in-
clude Google GFS [14], Google BigTable [6], Amazon Dy-
namo [15], Yahoo Hadoop [4], and Microsoft Dryad [20].
Building services eﬃciently on current data centers is hard,
in part because the network is a black box, and the services
have to infer properties like end-system network proximity
within the data center.
We have been exploring new data center architectures tar-
geted at shipping container sized data centers, aimed at
making it easier to build eﬃcient services. We exploit the
observation that many of the services running in data centers
are key-based and have similarities with services that run on
overlays. Conceptually, we want to take a topology used in
a structured overlay and create a physical instantiation of
that topology. Our prototype, called CamCube [7], uses a
3D torus (also known as a k-ary 3-cube) which is the topol-
ogy used in the Content Addressable Network (CAN) [28]
structured overlay. This creates a direct-connect topology,
51where each server connects directly to a small set of other
servers, without using any switches or routers. Using this
topology means that the virtual and physical topologies are
the same, and the key space is a 3D wrapped coordinate
space, where each server is assigned an (x, y, z) coordinate
that represents the server’s location within the 3D torus.
The core CamCube API then exposes the coordinate space,
and only provides functionality to send and receive packets
to and from physical one-hop neighbors. This API with the
coordinate space provides properties similar to those used in
structured overlays. There have been other proposals for fu-
ture data centers, including DCell [18] and BCube [17], that
also explore incorporating hybrid direct-connect topologies
in data centers.
Current data centers, as well as proposals for future ones,
use a single routing protocol to route packets between ar-
bitrary servers. They make explicit trade-oﬀs, for example
between exploiting multi-path and latency or server over-
head. CamCube also has a base multi-hop routing service,
using a link-state routing protocol that routes packets on
shortest paths and exploits, where possible, the multipath
characteristics of CamCube. As in existing data centers and
proposals, we expected services running on CamCube to just
use the base routing protocol. However, to provide simi-
lar functionality to that of structured overlays, we allowed
servers to intercept and modify packets as they routed them.
We then observed that service designers exploited this ﬂexi-
bility to implement their own customized routing protocols,
optimized for their particular performance requirements. In-
tuitively, services implemented their own routing protocols
because exposing the coordinate space and structure made
implementing protocols that exploit the structure easy. Ser-
vices often relied on the base routing protocol to handle cor-
ner cases, such as coping with voids in the coordinate space.
This leads to a commensal symbiotic relationship with multi-
ple routing protocols running concurrently, all working with,
and beneﬁting from, the base routing protocol.
In this paper, we examine four services developed for Cam-
Cube that all implemented their own routing protocol. Each
protocol is unique and makes diﬀerent trade-oﬀs, and we
show that each service achieves better performance at the
service-level by using its own routing protocol. We also ex-
amine the network-level impact of having each service use its
own routing protocol. Do the customized protocols increase
network load? Do they induce higher control traﬃc? Are
the traﬃc patterns skewed? Is there correlation between
the sets of links used across protocol? We conclude that
also at the network-level they achieve better performance
while consuming less network resources. We also show, that
across the set of services we have examined, that there is
no correlated link usage, despite the fact that each service
was designed independently. This leads us to conclude that
enabling services to create their own routing protocols is
advantageous.
Having determined that enabling services to use their own
routing protocols is advantageous; we examine the core prop-
erties of each protocol and extract a set of common func-
tionality. We observe that in many cases the core properties
required are conﬂicting. We then describe an extended rout-
ing service that makes it easier for developers to simply and
eﬃciently express their routing protocols. This ensures that
wherever possible they can share state and resources, and
hence decrease the likelihood that information is indepen-
dently maintained in multiple services, which would induce
unnecessary overhead.
It also makes it easier for service
writers to write protocols and encourages reuse of already
designed routing protocols.
The rest of the paper is organized as follows. Section 2
provides background on CamCube and direct-connect data
centers. Section 3 outlines four services that use customized
routing protocols. Section 4 examines the core properties of
the routing protocols, and Section 5 describes the extended
routing service to support them. Section 6 evaluates the
overhead of using the extended routing service on CamCube.
Section 7 shows an example service that uses the extended
routing service. Finally, Section 8 describes related work
and Section 9 concludes.
2. BACKGROUND
Recently, there have been several proposals for data center
architectures aimed at supporting shipping container-sized
data centers. They currently have between 1,500 and 2,500
servers. Larger data centers are created by co-locating mul-
tiple containers and, when delivered to site, the containers
are provisioned with cooling, power and network links.
Many of the recent proposals have explored using non-
traditional network topologies that are not simply based on
switches and routers. Examples, include DCell, BCube and
CamCube [18, 17, 7]. In these topologies, routing between
two servers uses paths that traverse servers that are required
to forward the packets. DCell [18] uses a hybrid topology,
where conceptually servers (in the same rack) are connected
to a local switch forming a DCell. Each server is then con-
nected to another server in a diﬀerent DCell. Hence, each
DCell is connected to every other DCell via at least one
server in the rack. To route packets between two servers in
diﬀerent DCells requires the packets to be sent via a server
that is connected to a server in the other DCell. This server
then routes the message to the other DCell server, which
then locally delivers it within the DCell.
CamCube [7] uses a direct-connect 3D torus topology,
formed by having each server directly connected to six other
servers. All intra-CamCube traﬃc uses the direct-connect
network, and hence no switches or routers are required. Un-
like DCell and CamCube, in BCube [17] servers are only
connected to switches, and a hierarchical structure is used.
Conceptually, at the bottom of the hierarchy, servers in a
rack are connected to a switch, that allows servers to com-
municate within a rack. Each sever is also connected to one
of k other switches which are in the next level of the hierar-
chy, such that each rack has one server connected to each of
the k switches. A packet can then be routed between racks
by traversing multiple switches and multiple servers, using
a source routing protocol.
Many of these topologies provide a signiﬁcant number of
independent paths between servers in the data center.
In
this paper we are exploring the viability and value of al-
lowing services to explicitly exploit this multi-path. Before
considering this further, we provide more details about Cam-
Cube, the platform which we will use in this paper.
2.1 CamCube overview
CamCube is designed to make it easier to develop ser-
vices in data centers. We provide a high-level overview of
CamCube, for a complete description and design motivation
see [7].
52Communication between CamCube servers uses a direct-
connect 3D torus topology. This is formed by having each
server directly connected to six other servers and it does not
use switches or routers. Each server is assigned an address,
which takes the form of an (x, y, z) coordinate that repre-
sents its relative oﬀset from an arbitrary origin server in the
3D torus. We refer to the address of the server as the server
coordinate and, once assigned, it is ﬁxed for the lifetime of
the server. The CamCube API exposes the wrapped 3D co-
ordinate space to services running on CamCube, and allows
sending and receiving of raw (Ethernet) packets to one-hop
physical neighbors. All higher-level functionality is built as
services that run on top of this API, including multi-hop
routing. The direct-connect topology is used for all intra-
CamCube communication. Traﬃc from servers outside the
CamCube is delivered over an IP-based switched network.
We assume that any number from 1 to all servers can be con-
nected to this network and, subsequently, CamCube servers
do not assume that this network can be used to route packets
to other CamCube servers. This allows the switched network
to be provisioned just to support the expected ingress/egress
bandwidth of CamCube.
The motivation for using the 3D torus topology, API and
the use of the 3D coordinate space are structured overlays,
speciﬁcally the CAN structured overlay [28]. The CamCube
uses a physical wiring that is the same as the virtual topol-
ogy used in CAN. The API is inspired by the Key-Based
Routing (KBR) API [8] used in many structured overlays.
Services written for CamCube can exploit the explicit struc-
ture and are able to map keys into the coordinate space.
This is motivated by the observation that many services
built for data centers are very similar to applications built
for structured overlays. However, CamCube applications
beneﬁt from the physical topology and virtual topology be-
ing identical, meaning that the services do not need to try
and infer properties, like end-system proximity, as it is more
explicit.
We have written a number of network-level services that
run over the direct-connect network, including a multi-hop
routing service and a TCP/IP service that enables unmodi-
ﬁed TCP/IP applications to be run on CamCube. We also
have a number of higher-level services, including a VM/ﬁle
distribution service, an aggregation service that underpins
a MapReduce-like service, and a memcached [13] inspired in-
memory object cache. All services are implemented in user
space, and the CamCube API communicates with a pair of
drivers that run in the kernel.
2.1.1 CamCube API
Next, we provide more details about the CamCube API.
We assume that each service running is assigned a unique
serviceId, and that an instance of each service runs on every
server, which registers with the CamCube runtime.
It is