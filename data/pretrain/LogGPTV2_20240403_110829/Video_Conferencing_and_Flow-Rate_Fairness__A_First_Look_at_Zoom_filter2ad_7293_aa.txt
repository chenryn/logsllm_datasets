title:Video Conferencing and Flow-Rate Fairness: A First Look at Zoom
and the Impact of Flow-Queuing AQM
author:Constantin Sander and
Ike Kunze and
Klaus Wehrle and
Jan R&quot;uth
Video Conferencing and Flow-Rate
Fairness: A First Look at Zoom
and the Impact of Flow-Queuing AQM
Constantin Sander(B), Ike Kunze, Klaus Wehrle, and Jan R¨uth
Communication and Distributed Systems,
RWTH Aachen University, Aachen, Germany
{sander,kunze,wehrle,rueth}@comsys.rwth-aachen.de
Abstract. Congestion control is essential for the stability of the Inter-
net and the corresponding algorithms are commonly evaluated for inter-
operability based on ﬂow-rate fairness. In contrast, video conferencing
software such as Zoom uses custom congestion control algorithms whose
fairness behavior is mostly unknown. Aggravatingly, video conferencing
has recently seen a drastic increase in use – partly caused by the COVID-
19 pandemic – and could hence negatively aﬀect how available Internet
resources are shared. In this paper, we thus investigate the ﬂow-rate
fairness of video conferencing congestion control at the example of Zoom
and inﬂuences of deploying AQM. We ﬁnd that Zoom is slow to react to
bandwidth changes and uses two to three times the bandwidth of TCP
in low-bandwidth scenarios. Moreover, also when competing with delay
aware congestion control such as BBR, we see high queuing delays. AQM
reduces these queuing delays and can equalize the bandwidth use when
used with ﬂow-queuing. However, it then introduces high packet loss for
Zoom, leaving the question how delay and loss aﬀect Zoom’s QoE. We
hence show a preliminary user study in the appendix which indicates
that the QoE is at least not improved and should be studied further.
1 Introduction
The stability of the Internet relies on distributed congestion control to avoid a
systematic overload of the infrastructure and to share bandwidth. Consequently,
protocols that make up large shares of Internet traﬃc, such as TCP and QUIC,
feature such congestion control mechanisms.
The COVID-19 pandemic and subsequent actions to limit its spread have now
caused a drastic increase in traﬃc related to remote-working [16]. Of particu-
lar interest is the increasing share of video conferencing software which typically
bases on UDP to conform to the inherent low-latency and real-time requirements
which cannot be provided by TCP [8,14]. Yet, UDP features no congestion con-
trol, meaning that the video conferencing software has to implement it on the
application layer. While this allows for adapting the video conference to the spe-
ciﬁc network conditions [11,14], such implementations can introduce unknown
eﬀects and undesired behavior when interacting with “traditional” congestion
c(cid:2) Springer Nature Switzerland AG 2021
O. Hohlfeld et al. (Eds.): PAM 2021, LNCS 12671, pp. 3–19, 2021.
https://doi.org/10.1007/978-3-030-72582-2_1
4
C. Sander et al.
control. Especially in light of the now increased share of the overall traﬃc, these
tailored implementations can potentially pose a threat to Internet stability.
Thus, we investigate the interaction of real-world video conferencing software
and traditional congestion control. For our study, we choose Zoom as it has seen
an enormous increase in traﬃc share by at least one order of magnitude from
being marginally visible up to surpassing Skype and Microsoft Teams at certain
vantage points [16]. We focus on how Zoom reacts to loss and how it yields traﬃc
to competing TCP-based applications. We also study the impact of Active Queue
Management (AQM) on the bandwidth sharing as it is of growing importance.
Speciﬁcally, our work contributes the following:
– We present a testbed-based measurement setup to study Zoom’s ﬂow-rate
when competing against TCP CUBIC and BBRv1.
– Comparing diﬀerent bandwidths, delays, and queue sizes, we ﬁnd that Zoom
uses a high share on low-bandwidth links and that there are high queuing
delays, even despite TCP congestion control trying to reduce it (e.g., BBR).
– We show that ﬂow-queuing AQM reduces queuing delay and establishes ﬂow-
rate equality to a certain degree reducing Zoom’s and increasing TCP’s rate
by dropping Zoom’s packets, where the former is probably beneﬁcial but the
latter is probably detrimental for Zoom’s QoE. Our preliminary user study
shows that users do not see QoE improvements with ﬂow-queuing AQM.
Structure. Section 2 discusses the deﬁnition of fairness, as well as related work
on general and video conferencing speciﬁc congestion control fairness analyses.
Section 3 describes the testbed for our ﬂow-rate equality measurements. Section 4
shows our general results on Zoom and the impact of AQM on ﬂow-rate equality,
packet loss, and delay. A preliminary user study evaluating the impact of AQM on
the QoE can be found in the appendix. Finally, Sect. 5 concludes this paper.
2 Background and Related Work
The interaction of congestion control algorithms, especially regarding fairness,
is a frequent focus of research. It has been thoroughly investigated for common
TCP congestion control algorithms. However, the deﬁnition of fairness itself has
also been investigated and discussed.
Fairness Deﬁnition. Most work relies on the conventional ﬂow-rate deﬁnition
of fairness: competing ﬂows should get an equal share of the available band-
width [19]. However, there are compelling arguments that ﬂow-rate fairness is
not an optimal metric [7,27] and new metrics such as harm [27] propose to also
consider the demands of applications and their ﬂows. We agree that ﬂow-rate
equality is no optimal metric for fairness as it ignores speciﬁc demands and the
impact of delay, thus making it an outdated fairness estimate.
On the other hand, the notion of harm is hard to grasp as it requires (poten-
tially wrong) demand estimates. Further, techniques such as AQM are demand
unaware and ﬂow-queuing even speciﬁcally aims at optimizing ﬂow-rate equality,
A First Look at Zoom and the Impact of Flow-Queuing AQM
5
ignoring any actual application demands. Hence, given the prevalence of ﬂow-
rate equality in related work and AQM techniques, we explicitly use ﬂow-rate
equality as our fairness metric to evaluate the precise impact of this metric on
the application performance. That is, we want to, e.g., see the impact on video
conferencing when ﬂow-queuing is used. This naturally also means that results
depicting an “unfair” ﬂow-rate distribution are not necessarily bad.
TCP Congestion Control. Many of the congestion control studies have espe-
cially looked at CUBIC [17] and BBR [10] and found that BBR dominates in
under-buﬀered scenarios causing packet loss and making CUBIC back oﬀ, while
it is disadvantaged in over-buﬀered scenarios [18,23,26,28]. Here, CUBIC, as
a loss-based algorithm, ﬁlls the buﬀer and increases the queuing delay which
makes BBR back oﬀ. Introducing AQM, these behavior diﬀerences vanish.
Impact of AQM. AQM mechanisms come with the potential of giving end-
hosts earlier feedback on congestion, thus helping to reduce queuing delays, and
there have been extended studies regarding their fairness (for a survey see [6]).
While some AQM algorithms are speciﬁcally designed to enable a fair bandwidth
sharing (see [13] for an overview and evaluation), generally, any AQM can be
made to fairly share bandwidth with the help of fair queuing [15]. Today, this idea
is most commonly implemented through a stochastic fair queuing (SFQ) which
performs similar to a true fair queuing when the number of ﬂows is limited. In
fact, several works (e.g., [22,23]) show that AQM using this SFQ (often called
ﬂow-queuing) can create ﬂow-rate fairness while eﬀectively limiting congestion,
even though there are no comprehensive studies available in literature.
2.1 Congestion Control for Video Conferencing
Loss-based congestion control, such as CUBIC, is not favorable to delay-sensitive
real-time applications. Hence, research has proposed several congestion control
algorithms tailored to the needs of video conferencing. However, in contrast to
general-purpose congestion control, there is only limited research on its interac-
tion mostly focusing on proposed algorithms with known intrinsics.
Known Algorithms. For example, the Google Congestion Control (GCC) [11],
used in Google Chrome for WebRTC, was tested for ﬂow-rate fairness [11,12].
The results indicate that GCC shares bandwidth equally with CUBIC when
using a tail-drop queue and also subject to the CoDel and PIE AQM algorithms.
There are similar ﬁndings for the Self-Clocked Rate Adaptation for Multime-
dia (SCReAM) [20] congestion control algorithm. It achieves an approximately
equal share with a long-lived TCP ﬂow on a tail-drop queue and yields bandwidth
when using CoDel [21]. Contrasting, the Network-Assisted Dynamic Adaptation
(NADA) congestion control [32] shares bandwidth equally when using a tail-drop
queue, but uses bigger amounts when being governed by an AQM algorithm.
Unknown Algorithms in Video Conferencing Software. However, many
actually deployed real-world congestion control algorithms in video conferencing
software are unknown and closed-source. Thus, similar to our work, research also
studies the externally visible behavior of video conferencing software.
6
C. Sander et al.
De Cicco et al. [14] investigate the behavior of Skype’s congestion control and
ﬁnd that it is generally not TCP-friendly and claims more than its equal share.
Interestingly, Zhang et al. [29] found that Skype yields a bigger share to com-
peting TCP ﬂows, but only after exceeding a certain loss threshold. However, in
contrast to work on TCP congestion control, these studies only consider limited
scenarios and generally do not provide extensive evaluations (e.g., no AQM).
Other works focus even more only on aspects impacting the video conference,
e.g., how the audio and video quality evolve subject to packet loss with unlimited
rates [24,30] or very speciﬁc wireless settings [31].
Takeaway. Studies on general congestion control are not applicable to video
conferencing. Research on video conferencing software, on the other hand, mostly
focuses on the concrete impact on its quality while the number of evaluation
scenarios and the context to the general congestion control landscape is scarce.
We thus identify a need for a more thorough evaluation of real-world video
conferencing congestion control that also considers the impact of diﬀerent band-
widths, buﬀer sizes, or AQM on fairness. For this purpose, we devise a method-
ology that centers around a conﬁgurable testbed which allows us to evaluate the
behavior of the congestion control of Zoom.
3 Measurement Design
Research on congestion control fairness is often done using simulations or isolated
testbeds to focus on the intrinsics of the algorithms. In contrast, our work on
Zoom forbids such an approach as the Zoom clients interact with a cloud-based
backend that is responsible for distributing audio and video traﬃc. Thus, to fully
grasp the real-world performance of Zoom, we devise a testbed that connects to
this backend while still letting Zoom’s traﬃc compete with a TCP ﬂow over
a variety of network settings. While we consequently have to take potential
external eﬀects into account, our testbed still allows us to control parameters,
such as bottleneck bandwidth, queuing, and delay.
3.1 Preliminaries
For our investigations, we set up two Zoom clients which then connect to a joint
Zoom conference via the Zoom backend running in a data center. We ﬁnd that
free Zoom licenses use data centers operated by Oracle in the US, while our
University license mostly connects to data centers operated by AWS in Europe.
We generally see that connections are established to at least two diﬀerent AWS
data centers, one in Frankfurt (Germany) and one in Dublin (Ireland). As our
upstream provider peers at DE-CIX in Frankfurt, we choose to focus on these
connections to reduce the number of traversed links, thus minimizing the prob-
ability of external eﬀects, such as changing routes or congestion.
3.2 Testbed Setup
As shown in Fig. 1, our testbed uses a dumbbell topology and consists of ﬁve
dedicated machines. In the center, one machine serves as the conﬁgurable bot-
tleneck link over which Zoom Client 1 (ZC 1) connects to the Zoom backend to
A First Look at Zoom and the Impact of Flow-Queuing AQM
7
Zoom Client 1
(ZC 1)
TCP Client
Bottleneck
Zoom Client 2
(ZC 2)
Internet
TCP Server
Fig. 1. Testbed setup representing a dumbbell topology
join a conference with Zoom Client 2 (ZC 2). Our two remaining machines (TCP
Client, TCP Server) operate a concurrent TCP ﬂow to assess competition.
Testbed Interconnection. All our machines are interconnected using 1 Gbps
Ethernet links. The uplink to our university’s network is 10 Gbps which in turn
connects to the German Research Network (DFN) via two 100 Gbps links. The
DFN then peers at DE-CIX with, e.g., AWS. We can thus be reasonably sure
that our conﬁgurable bottleneck machine represents the overall bottleneck.
Shaping the Bottleneck. We conﬁgure our bottleneck using Linux’s traﬃc
control (TC) subsystem similar to [25] to create network settings with diﬀerent
bandwidths, delays, queue sizes, and queue management mechanisms. For rate-
limiting, we use token bucket ﬁlters with a bucket size of one MTU (to minimize
bursts) on the egress queues in both directions. Similarly, we also conﬁgure
the AQM on the egress queues. Delay is modeled on the ingress queues using
intermediate function blocks (ifbs) and netem. We ﬁrst create an additional
ingress qdisc via ifb and add the delay to the egress of this ifb via netem. This
technique is necessary as netem is not directly compatible with AQM qdiscs [1]
and usage of netem on the end-hosts would cause issues due to TCP small
queues [9]. Further, we add no artiﬁcial jitter, as this causes packet reorderings,
as such, jitter is only introduced through the ﬂows ﬁlling the queue itself.
Balancing RTTs. Our testbed compensates for diﬀering RTTs and ensures
that the Zoom and the TCP ﬂow have the same RTT, a requirement for the
common ﬂow-rate equality deﬁnition. For this, we ﬁrst measured the average
delay between diﬀerent AWS hosts and ZC 1 as well as between TCP Client
and TCP Server prior to our experiments. We then adapted the netem delay
accordingly such that the TCP ﬂow and the ﬂow between ZC 1 and AWS have
about the same RTT when the queue is empty. By adapting the delay prior to
our experiments, we avoid skewing the initial RTT of ﬂows which we presume to
be important for Zoom’s congestion control, but accept a potential bias due to
changing hosts at AWS which we cannot predict prior to establishing our video
conferences. However, the relative error of this bias should be insigniﬁcant as we
emulate rather large artiﬁcial RTTs.
3.3 Fairness Measurement Scenarios and Procedure
With our measurements, we aim to represent video conferences from a low-
speed residential access where Zoom’s video ﬂow and a TCP ﬂow (e.g., a movie
download) compete. The used parameters are shown in Table 1.
8
C. Sander et al.
Table 1. Parameter conﬁguration for our testbed
BW [Mbps] RTT [ms] QSize [BDP]