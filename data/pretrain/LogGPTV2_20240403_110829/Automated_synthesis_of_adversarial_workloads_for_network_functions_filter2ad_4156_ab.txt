processors may use different proprietary hashing algorithms,
the discovered contention sets may not hold across different
processor types. For our Xeon processor, in particular, we
discovered 23 409 contention sets varying in size from 32 to
5638 entries.
3.3 Current Cost and Memory Access
To compute the current cost of a state S, corresponding to
instruction sequence I, we consider the sequence of all mem-
ory addresses accessed byI and try to constrain the symbolic
memory addresses such that the resulting memory-access
pattern incurs as many trips to main memory as possible.
Then we consider each instruction i in I and assign to it an
estimate of the number of cycles that it consumes: a fixed
per-instruction cost learned empirically, if i does not access
memory; and a fixed per-memory-level cost, if i does access
memory. To constrain the symbolic memory addresses and
determine which memory accesses are hits and misses, we
use a cache model, initialized to a clear cache, that is built
on top of the contention sets discovered as described in §3.2.
For example, consider an NF that accesses an IP lookup
table, once per incoming packet. Which table location is ac-
cessed depends on the packet’s IP headers, which, in our
context, are symbolic. Hence, every table access yields an
access to a new symbolic memory address As, constrained
according to the boundaries of the table and the spacing
between its entries. Moreover, at every table access, we use
the cache model to determine: which (concrete) memory ad-
dresses are in the cache, which contention sets they belong
to, and how many extra addresses from each contention set
need to be accessed to cause an eviction. Based on this in-
formation, we create a list of candidate memory addresses
that, if accessed, we expect to cause L3 cache contention.
For each candidate memory address A, we use a solver to
check whether A is compatible (satisfies the constraint asso-
ciated) with the symbolic memory address As that is being
accessed. If so, we concretize As to A and constrain the incom-
ing packet’s IP headers accordingly. Ideally, this constrains
all symbolic memory addresses to the same contention set,
which guarantees an L3 cache miss as soon as we exceed
associativity. If that fails, we greedily try to constrain all
addresses to as few contention sets as possible.
Limitations: To keep our approach scalable, we do not
seek the provably worst memory-access pattern—just a very
bad one that can be discovered within a reasonable time bud-
get. (1) We do not consider the L1 and L2 caches; we tried
designing a model of the memory hierarchy that did, but
were unable to make it detailed and accurate enough to make
a difference. (2) We do not consider prefetching and Data
Direct I/O (DDIO) [1]. Prefetching is hard to model, because
it is based on proprietary algorithms. However, as supported
by our evaluation, prefetching does not significantly affect
NF performance, because NF memory-access patterns are
determined by network traffic and are typically not sequen-
tial or periodic. DDIO does affect NF performance, because
it places the headers of incoming packets in the cache before
they are accessed, thereby avoiding a previously mandatory
375
Automated Synthesis of Adversarial Workloads
for Network Functions
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 2: Example of annotated ICFGs showing each
node’s estimated maximum distance to the black
node.
Figure 3: Handling hash functions: solid arrows show
how NFs typically use hashing; empty arrows show
CASTAN’s reconciliation procedure.
cache miss. However, this improves all workloads the same,
hence does not affect our task of seeking adversarial work-
loads. (3) When we constrain symbolic memory addresses,
we make locally optimal decisions. Otherwise, we would
have to follow the standard, non-scalable symbex behavior
of resolving each accessed symbolic address to all feasible
concrete addresses and creating a new state for each one,
which fails to yield results in useful time.
3.4 Potential Cost
The potential cost of a state S is an estimate of the maximum
number of cycles that could be consumed to get from S to
the point where the next packet is received. To compute
it, we rely on a pre-processing stage that extracts the NF’s
interprocedural control-flow graph (ICFG)2 and annotates
each node (which corresponds to an instruction) with an
estimate of such a potential cost. This annotated ICFG then
allows S’s potential cost to be efficiently computed during
symbex.
During pre-processing, we start from each node’s local
cost, assuming all memory accesses are L1 hits; we then use
a special form of path-vector routing to propagate these local
costs and estimate each node’s potential cost3. In the absence
of loops, this is simple: each node’s cost is augmented by
the cost of the most expensive successor. Fig. 2 on the left
shows an example of an annotated ICFG, in the presence of
a simple if-then-else statement.
Things are more complicated in the presence of loops: at
this stage of the analysis we do not have enough context
to bound the number of times a loop can execute; if we
propagate costs naïvely, any loop will induce an infinite
potential cost to every node within and before it, making the
analysis intractable. To address this challenge, we ensure a
node may show up at most M times in a path (within our
path vector routing algorithm), where M is a configurable
parameter. This essentially makes a static assumption that
every loop executes exactly M − 1 times. In our evaluation,
we use M = 2, which balances exploring the cost of a loop’s
internals (M = 1 hides all instructions within the loop body)
against the negative effects of over-estimation. Fig. 2 on
2The ICFG augments the traditional control-flow graph with function-call
edges and typically takes less than a second to extract.
3A node’s potential cost accounts for both calling functions in a chain (a()
calls b(), and b() reaches the target) and returning from them (a() calls
b(), which must return before a() reaches the target).
376
the right shows an example of an annotated ICFG, in the
presence of a loop.
During symbolic execution, every time the SEE reaches
a loop head, it creates two execution states: one that cor-
responds to exiting the loop as soon as possible, and one
that corresponds to executing one more iteration (if that is
feasible). Next, the SEE must choose which of the two states
has the highest potential cost, and it always chooses the one
that corresponds to executing one more loop iteration (again,
as long as one more iteration is feasible). Hence, in the end,
the SEE greedily explores the loop as deeply as possible.
Limitations: The ICFG cannot tell us which is the most
expensive instruction sequence that follows a given state, be-
cause it does not take into account the constraints associated
with that state. It only provides a first-order approximation.
The time limit for the execution only allows for a partial
state space exploration. Higher limits give a better chance of
escaping local maxima, but remain an approximation when
an exhaustive exploration is time-prohibitive. This prevents
CASTAN from formally verifying worst-case performance.
3.5 Hash Functions
NFs that use data structures relying on hash functions pose a
particular challenge to symbex. Symbexing a hash function
typically leads to the creation of complex symbolic expres-
sions that often exceed the solver’s capabilities and result in
solver timeouts.
We address this issue with a technique called havocing [5],
which decouples the two parts of the code: the part that gen-
erates the input to the hash function, and the part that uses
the hash value. This technique (which takes place at the gray
arrow in Fig. 3) disables the execution of the hash function
and replaces (havocs) its output with an unconstrained sym-
bol, allowing the analysis of the code that uses the hash value
to proceed normally, merely reasoning about constraints on
the hash value. This results in a series of constraints that
concern both the input packet and the resulting hash value.
This on its own can already be useful, as it can provide in-
sights into what a poor performance scenario may look like.
The analysis essentially says that performance will suffer if
we can find a packet that meets certain constraints and when
hashed, produces a hash value that meets an additional set
of constraints.
We reconcile the two sets of constraints in three steps
102435506877PacketHashInputHashValuePoorPerf.3: Solve2: Reverse1: SolveSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Pedrosa et al.
(shown as empty arrows in Fig. 3). We first use the solver to
find a few candidate hash values. We then invert these hash
values using brute-force methods augmented by the use of
rainbow tables [27]. Finally, the solver checks if the hash
input is compatible with the packet constraints and finds a
matching packet.
While the first stage of this process is typically straight-
forward and likely to succeed, the likeliness of success of
the second and third stages depends heavily on the quality
of the rainbow tables. The process is analogous to solving
constraints by successively attempting random assignments
in the hope that a satisfying one eventually emerges. As we
use rainbow tables, success first depends on the table having
entries that match the hash-values found in the first stage.
This requires the rainbow table to have enough entries that
each value is represented a few times, i.e. ∼ 2∥hash−value ∥.
This doesn’t pose a challenge as typical hash values are small
(∼ 20 bits, requiring a few millions of entries).
The more serious bottleneck is in finding hash inputs that
satisfy the packet constraints. Finding satisfying values at
random depends on how heavily constrained the packet is.
For example, while analyzing a series of NFs that use a hash
table (§5), we realized that the 8-bit IP protocol field was a
part of the hash key. As the NFs only support TCP and UDP,
the odds would reject 254/256 ≈ 99% of the rainbow table
entries based on that constraint alone, potentially slowing
the search and increasing the requisite table size by 100×.
In such scenarios, we can increase the likelihood of success
by generating a custom-tailored rainbow table with values
that are more likely to fit the constraints. In this case, we
populated the rainbow table with values that assume UDP.
4 IMPLEMENTATION
We implemented CASTAN [11] by forking an existing symbolic
execution engine, KLEE [8], and adapting it to our needs. The
key changes revolve around the implementation of the cache
model (§3.3), the directed symbolic execution heuristic (§3.4),
and the ability to havoc and reverse hash functions (§3.5).
Additional tools were also created to help build, process,
and validate the empirical cache models (§3.2) as well as to
generate PCAP files from the analysis output.
The cache model is implemented as a special pluggable
module which is called during the symbolic execution of the
load and store memory operations. The module is designed
as a plug-in so that multiple implementations can be easily
swapped in. Our default cache model uses the contention sets
discovered in §3.2. The module takes as input the symbolic
expression of the pointer being used to access memory and
adds constraints to the execution state. This processing oper-
ates in two stages. The initial phase looks at the current state
of the cache model, picks the worst compatible cache line,
and adds a series of constraints on the pointer expression to
the path constraint, essentially concretizing the pointer. The
second phase then takes this concrete pointer value and up-
dates the cache model state so that future memory accesses
will take it into account.
We implement our directed symbolic execution heuristic
via a custom searcher class, which is a pluggable module
that KLEE uses to pick which states to explore next. In this
module, we first preprocess the NF LLVM code to extract
the ICFG. Additionally, we annotate each instruction with
cost estimates, as described in §3.4. Later, as the analysis is
running, this annotated ICFG helps us to quickly compute the
cost heuristic for each execution state, allowing us to order
the pending states and prioritize the further exploration of
those with a higher estimated CPP.
The ability to havoc and reverse hash functions is im-
plemented in two phases. The first phase involves annotat-
ing the code to identify where the hash value is computed.
The developer uses a special CASTAN annotation, castan_
havoc(input,output,expr). When the NF is compiled for
use in production, this annotation simply equates to "output
= expr;". When built for analysis, the annotation keeps track
of the symbolic expression of input and then havocs the
output variable by setting it to a new unconstrained symbol.
Later, in a post-processing stage that occurs just before out-
putting the path, we use the information gathered through
this annotation, alongside a user specified rainbow table, to
reconcile the havocs, as explained in §3.5.
Finally, we modify KLEE to generate additional outputs
that indicate the expected performance for each generated
path. As such, a successful CASTAN run will generate two
files for each path that it generates. The first is a traditional
KTEST file, indicating concrete symbol values that will ex-
ercise the path. We convert this file into a PCAP file using
a separate tool. The second file lists all of the CPU model
metrics, on a per packet basis, including the number of non-
memory instructions executed, the number of loads and
stores, and the number of memory accesses that hit the
cache. These metrics can be used directly to help debug the
difference between distinct scenarios or simply to predict the
performance envelope of each path, revealing the slowest
path generated.
5 EVALUATION
In this section, we compare the workloads synthesized by
CASTAN for 11 real NFs to workloads that were manually
crafted to be adversarial by the engineer who wrote each NF.
We first describe our evaluation setup (§5.1), then present our
results for scenarios where the adversarial behavior comes
primarily from memory accesses (§5.2), algorithmic complex-
ity (§5.3), or hash-function manipulation (§5.4).
377
Automated Synthesis of Adversarial Workloads
for Network Functions
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
5.1 Setup
Testbed. Our testbed consists of a device under test (DUT)
directly connected to a traffic generator/sink (TG). Both have
Intel Xeon E5-2667v2 3.3GHz CPUs with 25.6MB of L3 cache
and 32GB of RAM; they are connected over Intel 82599ES
10Gb NICs. In each experiment, the DUT runs one NF. The
TG uses MoonGen [16] to replay specific PCAP files. Each
experiment lasts 20 seconds. If a PCAP file does not yield 20
seconds worth of traffic, we replay it in a loop.
Performance metrics. First, we measure the end-to-end
latency from the moment a packet exits until it re-enters
the TG NIC. Our measurement process relies on hardware
timestamps added by the TG NIC as described in [31], which
has accuracy on par with hardware measurement devices. We
do not ignore dropped packets: if the NF running on the DUT
decides to drop a packet, we allow it to mark the packet as
dropped, but we forward the packet back to the TG anyway,
such that the latency it encounters is measured. During these
experiments, the TG sends packets at a low enough rate that
there is no more than one outstanding packet between the
TG and the DUT, thus excluding any queuing or pipelining
effects. We present the results in the form of a cumulative
distribution function (CDF) per experiment.
Using an external TG benefits from high precision hard-
ware timestamping, but this also measures the DPDK and
driver stacks on the DUT as well as the transmission latency
between the TG and the DUT. To quantify this overhead
and estimate the NF latency in isolation we include in each
plot the end-to-end latency CDF of a special NOP NF that
forwards packets without any other processing. When we
compare latency in relative terms, we use this NOP as a
baseline to subtract from.
Second, we measure the maximum throughput achieved
by each NF: we vary the rate at which the TG sends packets
to the DUT and identify the highest rate at which the DUT
drops less than 1% of the packets it receives.
Third, we conduct a micro-architectural characterization
of each NF: we measure the number of reference cycles,
instructions retired, and L3 cache misses (i.e., DRAM ac-
cesses) per packet, using CPU performance counter registers
exposed through libPAPI [25]. These numbers allow us to
reason about why one workload incurs worse performance
than another. As in the latency experiments, there is no more
than one outstanding packet between the TG and the DUT,
and we present the results in the form of CDFs.
Network Function Logic. Our current research prototype
assumes single-threaded NFs that use the basic DPDK API.
This excludes many existing non-trivial open-source NFs.
As such, we developed a library of NFs for evaluation pur-
poses. We implemented three classes of NFs: IP longest prefix
matching (LPM), source network address translation (NAT),
378
and stateful L4 load balancing (LB).
LPM provides standard destination-based IP lookup. We
populate the forwarding table with /8, /16, /24, and in some
case /32 routes (depending on the underlying data structure),
8 of each. We chose the prefixes to overlap as much as possi-
ble, i.e., each prefix includes a more specific one (except for
the /32 entries).
NAT provides standard source network address transla-
tion, i.e., it maintains per-flow state and uses it to: rewrite the
source IP address and port number of packets coming from
the internal network such that they appear to be coming
from the NAT itself; rewrite the destination IP address and
port number of packets coming from the external network
such that they can be delivered transparently.
LB provides typical virtual IP (VIP) to direct IP (DIP) trans-
lation in a data-center network, i.e., it maintains per-flow
state and uses it to: (1) rewrite the destination IP address
of packets coming from the outside world such that they
are transparently delivered to a backend server, ensuring all
packets from the same connection go to the same server;
and (2) rewrite the source IP address of packets coming from
the data-center such that they appear to be coming from the
LB itself. It picks backend servers for new connections in a
round-robin fashion.
Data Structures. To test CASTAN’s flexibility, for each NF
class listed above, we use multiple implementations, each
using a different underlying data structure, hence susceptible
to different adversarial workloads.
We use three LPM implementations, each one striking a
different balance between algorithmic complexity and mem-
ory efficiency: (1) The first one encodes the forwarding table
in a Patricia trie [38], where each node of the trie corre-
sponds to an IP prefix, and a node’s children correspond to
longer prefixes included in their parent. Lookup involves
traversing the trie until we find the longest matching pre-
fix. Hence, lookup complexity depends on the length of the
longest supported prefix, which is 32 bits, in our case. (2) The
second one implements Direct Lookup, where the forward-
ing table is translated into routes of equal-length IP prefixes
(each as long as the longest supported prefix), which are
then stored in a single, large array. In our case, the longest
supported prefix consists of 27 bits, leading to an array that
fits in a single 1GB page. Lookup involves indexing this array