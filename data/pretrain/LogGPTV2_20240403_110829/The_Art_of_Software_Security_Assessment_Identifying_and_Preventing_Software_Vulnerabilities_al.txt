are more effective when used with a restricted account. In addition, a chroot jail 
doesn't restrict network access beyond normal account permissions, which could still 
allow enough attack surface for a follow-on attack targeted at daemons listening on 
the localhost address. 
System Virtualization 
Security professionals have spent the past several years convincing businesses to run 
one public-facing service per server. This advice is logical when you consider the 
implicit shared trusts between any processes running on the same system. However, 
increases in processing power and growing numbers of services have made this 
practice seem unnecessarily wasteful. 
Fortunately, virtualization comes to the rescue. Virtualization allows multiple 
operating systems to share a single host computer. When done correctly, each host is 
isolated from one another and can't affect the integrity of other hosts except through 
standard network interfaces. In this way, a single host can provide a high level of 
segmentation but still make efficient use of resources. 
Virtualization is nothing new; it's been around for decades in the mainframe arena. 
However, most inexpensive microcomputers haven't supported the features required 
for true hardware virtualizationthese features are known as the Popek and Goldberg 
virtualization requirements. True hardware virtualization involves capabilities that 
hardware must provide to virtualize access without requiring software emulation. 
Software virtualization works, of course, but only recently has commodity hardware 
become powerful enough to support large-scale virtualization. 
Virtualization will continue to grow, however. New commodity processors from 
vendors such as Intel and AMD now have full hardware virtualization support, and 
software virtualization has become more commonplace. You can now see a handful of 
special cases where purpose-built operating systems and software are distributed as 
virtual machine disk images. These concepts have been developing for more than a 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
94 
decade through research in exokernels and para-virtualization, with commercial 
products only now becoming available. 
For auditors, virtualization has advantages and disadvantages. It could allow an 
application to be distributed in a strictly configured environment, or it might force a 
poorly configured black box on users. The best approach is to treat a virtualized 
system as you would any other system and pay special attention to anywhere the 
virtual segmentation is violated. As virtualization grows more popular, however, it will 
almost certainly introduce new and unique security concerns. 
Enhanced Kernel Protections 
All operating systems must provide some mechanism for user land applications to 
communicate with the kernel. This interface is typically referred to as the system call 
gateway, and it should be the only interface for manipulating base system objects. 
The system call gateway is a useful trust boundary, as it provides a chokepoint into 
kernel operations. A kernel module can then intercept requested operations (or 
subsequent calls) to provide a level of access control that is significantly more 
granular than normal object permissions. 
For example, you might have a daemon that you need to run as root, but this daemon 
shouldn't be able to access arbitrary files or load kernel modules. These restrictions 
can be enforced only by additional measures taken inside the kernel. An additional set 
of permissions can be mapped to the executable and user associated with the process. 
In this case, the kernel module would refuse the call if the executable and user match 
the restricted daemon. This approach is an example of a simple type of enhanced 
kernel protection; however, a number of robust implementations are available for 
different operating systems. SELinux is a popular module for Linux and BSD systems, 
and Core Force (from Core Security) is a freely available option for Windows 2000 and 
XP systems. 
There's no question that this approach offers fine-grained control over exactly what a 
certain process is allowed to do. It can effectively stop a compromise by restricting 
the rights of even the most privileged accounts. However, it's a fairly new approach to 
security, so implementations vary widely in their capabilities and operation. This 
approach can also be difficult to configure correctly, as most applications aren't 
designed with the expectation of operating under such tight restrictions. 
Host-Based Firewalls 
Host-based firewalls have become extremely popular in recent years. They often 
allow fine-grained control of network traffic, including per-process and per-user 
configuration. This additional layer of protection can help compensate for any 
overlooked network attack surface. These firewalls can also mitigate an attack's 
effect by restricting the network access of a partially compromised system. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
95 
For the most part, you can view host-based firewalls in the same manner as standard 
network firewalls. Given their limited purpose, they should be much less complicated 
than a standard firewall, although per-process and per-user rules can increase their 
complexity somewhat. 
Antimalware Applications 
Antimalware applications include antivirus and antispyware software. They are 
usually signature-based systems that attempt to identify behaviors and attributes 
associated with malicious software. They might even incorporate a degree of 
enhanced kernel protection, host-based firewalling, and change monitoring. For the 
most part, however, these applications are useful at identifying known malware 
applications. Typically, they have less value in handling more specialized attacks or 
unknown malware. 
Antimalware applications generally have little effect when auditing software systems. 
The primary consideration is that a deployed system should have the appropriate 
software installed and configured correctly. 
File and Object Change Monitoring 
Some security applications have methods of monitoring for changes in system objects, 
such as configuration files, system binaries, and sensitive Registry keys. This 
monitoring can be an effective way to identify a compromise, as some sensitive 
portion of the system is often altered as a result of an exploit. More robust monitoring 
systems actually maintain digests (or hashes) of sensitive files and system objects. 
They can then be used to assist in forensic data analysis in the event of a serious 
compromise. 
Change monitoring is a fairly reactive process by nature, so generally it isn't useful in 
preventing compromises. It can, however, prove invaluable in identifying, 
determining the extent of, and mitigating a successful compromise. The most 
important consideration for auditors is that most change-monitoring systems are 
configured by default to monitor only base system objects. Adding monitoring for 
application-specific components usually requires changes to the default 
configuration. 
Host-Based IDSs/IPSs 
Host-based intrusion detection systems (IDSs) and intrusion prevention 
systems (IPSs) tend to fall somewhere between host-based firewalls and 
antimalware applications. They might include features of both or even enhanced 
kernel protections and file change monitoring. The details vary widely from product to 
product, but typically these systems can be viewed as some combination of the 
host-based measures presented up to this point. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
96 
Network-Based Measures 
An entire book could be devoted to the subject of secure network architecture. After 
all, security is only one small piece of the puzzle. A good network layout must account 
for a number of concerns in addition to security, such as cost, usability, and 
performance. Fortunately, a lot of reference material is available on the topic, so this 
discussion has been limited to a few basic concepts in the following sections. If you're 
not familiar with network fundamentals, you should start with a little research on 
TCP/IP and the Open Systems Interconnection (OSI) model and network 
architecture. 
Segmentation 
Any discussion of network security needs to start with segmentation. Network 
segmentation describes how communication over a network is divided into 
groupings at different layers. TCP/IP networks are generally segmented for only two 
reasons: security and performance. For the purposes of this discussion, you're most 
concerned with the security impact of network segmentation. 
You can view network segmentation as a method of enforcing trust boundaries. This 
enforcement is why security is an important concern when developing a network 
architecture. You should also consider what OSI layer is used to enforce a security 
boundary. Generally, beginning with the lowest layer possible is best. Each higher 
layer should then reinforce the boundary, as appropriate. However, you always 
encounter practical constraints on how much network security can be provided and 
limitations on what can be enforced at each layer. 
Layer 1: Physical 
The security of the physical layer is deceptively simple. Segmentation of this layer is 
literally physical separation of the transmission medium, so security of the physical 
layer is simply keeping the medium out of attackers' hands. In the past, that meant 
keeping doors locked, running cables through conduit, and not lighting up 
unconnected ports. If any transmission media were outside your immediate control, 
you just added encryption or protected at higher layers. 
Unfortunately, the rapid growth of wireless networking has forced many people to 
reevaluate the notion of physical layer security. When you deploy a wireless network, 
you expose the attack surface to potentially anyone in transmission range. With the 
right antenna and receiver, an attacker could be a mile or more away. When you 
consider this possibility with the questionable protection of the original Wired 
Equivalent Privacy (WEP) standard, it should be apparent that physical layer security 
can get more complicated. 
Layer 2: Data Link 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
97 
Segmentation at the data link layer is concerned with preventing spoofing 
(impersonating) hosts and sniffing traffic (capturing data transmitted by other hosts). 
Systems at this layer are identified via Media Address Control (MAC) addresses, and 
the Address Resolution Protocol (ARP) is used to identify MAC addresses associated 
with connected hosts. Switching is then used to route traffic to only the appropriate 
host. 
Network switches, however, run the gamut in terms of features and quality. They 
might be vulnerable to a variety of ARP spoofing attacks that allow attackers to 
impersonate another system or sniff traffic destined for other systems. Address 
filtering can be used to improve security at this layer, but it should never be relied on 
as the sole measure. 
Wireless media creates potential concerns at this layer, too, because they add 
encryption and authentication to compensate for their inability to segment the 
physical layer adequately. When choosing a wireless protection protocol, you have a 
few options to consider. Although proprietary standards exist, open standards are 
more popular, so this section focuses on them. 
WEP was the original standard for wireless authentication and encryption; however, 
its design proved vulnerable to cryptanalytic attacks that were further aggravated by 
weaknesses in a number of implementations. Wi-Fi Protected Access (WPA) is a more 
robust standard that provides more secure key handling with the same base 
encryption capabilities as WEP (which allows it to operate on existing hardware). 
However, WPA was intended as only an interim measure and has been superseded by 
WPA2, which retains the essential key-handling improvements of WPA and adds 
stronger encryption and digest capabilities. 
Layer 3: Network 
Security and segmentation at the network layer are typically handled via IP filtering 
and, in some cases, the IP Security (IPsec) protocol. Any meaningful discussion of 
IPsec is beyond the scope of this book, but it's important to note exactly what it is. 
IPsec is a component of the IPv6 specification that has been back-ported to the 
current IPv4. It provides automatic encryption and authentication for TCP/IP 
connections at the network layer. Although IPsec does have some appealing security 
capabilities, its adoption has been slow, and different technologies have been 
developed to address many of the areas it was intended for. However, adoption is 
continuing to grow, and a properly deployed IPsec environment is extremely effective 
at preventing a range of network attacks, including most sniffing and spoofing 
attacks. 
IP filtering is a fairly simple method of allowing or denying packets based only on the 
protocol, addresses, and ports. This method allows traffic to be segmented according 
to its function, not just the source and destination. This type of filtering is easy to 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
98 
implement, provides fast throughput, and has fairly low overhead. At this point, IP 
filtering is practically a default capability expected in almost any network-enabled 
system, such as a router or an OS. The disadvantage of IP filtering is that it maintains 
no connection state. It can't discriminate based on which side is establishing the 
connection or whether the communication is associated with an active connection. 
Therefore, a simple IP filter must allow inbound traffic to any port where it allows 
outbound traffic. 
Layer 4: Transport 
The transport layer is what most people think of when they discuss network security 
architecture. This layer is low enough to be common to all TCP/IP applications but 
high enough that you can determine connection state. The addition of state allows a 
firewall to determine which side is initiating the connection and establishes the 
fundamental concept of an internal and external network. 
Firewalls, which are devices that filter traffic at the network and transport layers, 
are the primary method of segmenting a network for security purposes. The simplest 
firewall has only two interfaces: inside and outside. The simplest method of 
firewalling is to deny all inbound traffic and allow all outbound traffic. Most host-based 
firewalls and personal firewalls are configured this way by default. 
Firewalls get interesting, however, when you use them to divide a network according 
to functional requirements. For example, say you know that employees on your 
network need only outbound Web access. You can allow only TCP ports 80 and 443 
outbound and deny all the rest. The company Web site is hosted locally, so you need 
to add TCP port 80 inbound to the Web server. (Note: A number of other likely 
services, most notably DNS, have been ignored to keep this example simple.) 
However, you don't like the idea of having an opening straight into the internal 
network via TCP port 80. The solution is to deploy the Web server inside a 
demilitarized zone (DMZ). A DMZ uses a third interface from the firewall 
containing its own set of rules. First, assume that the DMZ is configured to deny any 
connections by default, which lets you start with a clean slate. Next, you need to 
move the Web server into the DMZ, remove the deny inbound rule for port 80, and 
replace it with a rule that allows inbound traffic from the external network to the Web 
server in the DMZ on TCP port 80. Figure 3-1 shows an example of this network. 
Figure 3-1. Simple DMZ example 
[View full size image] 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
99 
This example, although simple, conveys the basics of transport-layer segmentation. 
What's important to understand is that the network should be segmented by function 
as much as reasonably possible. Continuing the example, what if the Web server is 
backed by a database on a separate system? The database might contain particularly 
sensitive customer information that shouldn't be located inside the DMZ. However, 
migrating the database to the internal network requires opening connectivity from 
the DMZ into the internal network, which might not be an acceptable risk, either. In 
this case, adding a second DMZ containing a data tier for the Web front end might be 
necessary. 
When reviewing an in-place application, you need to take these environmental 
considerations into account. There will always be legitimate reasons to prevent a 
deployment from having the ideal segmentation. However, you should aware of these 
contributing factors and determine whether the environment is adequately 
segmented for the application's security requirements. 
Layers 5 and 6: Session and Presentation 
Some layers of the OSI model don't map cleanly to TCP/IP; for example, the session 
and presentation layer generally get pushed up into the TCP/IP application layer. 
However, collectively these layers provide a useful distinction for certain application 
protocols. Platform-specific features, such as RPC interfaces and named pipes, are 
generally accepted as session- and presentation-layer protocols. Security on these 
interfaces is typically handled programmatically and should be addressed via the 
platform's native access control mechanisms. 
Secure Socket Layer/Transport Layer Security (SSL/TLS) is another protocol 
that's more appropriately discussed in terms of the session or presentation layer. The 
"Secure Channels(? [????.])" section earlier in this chapter discussed how SSL can be 
used to create a secure encrypted channel. SSL/TLS also supports certificate-based 
authentication, which can reduce an application's attack surface by enforcing 
authentication below the application layer. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
100 
Layer 7: Application 
Application-layer security is an interesting mix, and most of this book is devoted to it. 
However, application-layer proxies fall squarely into the category of operational 
protective measures. If you've spent any time in network security, you've probably 
heard numerous discussions of the value of heterogeneous (or mixed) networks. On 
the positive side, a heterogeneous environment is much less prone to silver bullet 
attacks, in which an attacker can compromise the bulk of a network by taking 
advantage of a single vulnerability. However, a homogeneous environment is usually 
easier and less expensive to manage. 
Application-layer gateways are interesting because they add extra network diversity 
in just the right location. Some of the first popular application gateways were simply 
validating HTTP reverse proxies. They sat in front of vulnerability-prone Web servers 
and denied malformed Web traffic, which provided moderate protection against Web 
server attacks. Newer Web application gateways have added a range of capabilities, 
including sitewide authentication, exploit detection, and fine-grained rule sets. 
Overall, application gateways are no substitute for properly coded applications. They 
have significant limitations, and configuring rules for the most effective protection 
requires more effort than assessing and fixing a potentially vulnerable application. 
However, these gateways can increase a network's diversity, provide an extra layer of 
assurance, and add a layer of protection over a questionable third-party application. 
Network Address Translation (NAT) 
Network Address Translation (NAT) provides a method of mapping a set of 
internal addresses against a different set of external addresses. It was originally 
developed to make more efficient use of the IPv4 address space by mapping a larger 
number of private, internal network addresses to a much smaller number of external 
addresses. 
NAT wasn't intended to provide security, but it does have some implicit security 
benefits. A NAT device must be configured with explicit rules to forward inbound 
connections; this configuration causes inbound connectivity to be implicitly denied. 
NAT also conceals the internal address space from the external network, ensuring 
some extra security against internal network mapping. 
NAT can offer additional protection, but generally, this isn't its intended purpose. 
Depending on the implementation, NAT devices might allow attacks that establish 
internal connections, spoof internal addresses, or leak addresses on the private 
network. Therefore, NAT shouldn't be relied on alone; it should be viewed as a 
supplementary measure. 
Virtual Private Networks (VPNs) 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
101 
A virtual private network (VPN) provides a virtual network interface connected to 
a remote network over an encrypted tunnel. This approach has become popular and 
is quickly replacing dial-in business connections and leased lines. The advantage of a 
VPN is that it presents an interface that's almost identical to that of a directly 
connected user, which makes it convenient for end users and network administrators. 
The main disadvantage of a VPN is that typically, the client system is outside of the 
network administrators' physical control, which creates the potential for a much 
larger attack surface than a normal internal system does. VPN segments need to be 
monitored more closely, and administrators must enforce additional client 
precautions. These precautions usually include denying VPN clients access to their 
local network (split tunneling) while connected and restricting access to certain 
internal resources over the VPN. 
Network IDSs/IPSs 
Network IDSs and IPSs are devices that attempt to identify malicious network traffic 
and potentially terminate or deny connectivity based on detected hostile activity. The 
first systems were primarily signature-based engines that looked for specific traffic 
associated with known attacks. Newer systems attempt to identify and alert 
administrators to anomalous traffic patterns in addition to known hostile patterns. 
There's quite a bit of literature and debate on the proper approach to IDS and IPS 
deployment and configuration. The details are specific to the network environment. 
However, the best generally accepted practices require segmenting the network first 
to isolate different functional areas and points of highest risk. IDS sensors are then 
deployed to take advantage of segmentation in identifying potential attacks or 
compromises. 
6.3.5 Summary 
Application security extends beyond the code to encompass the operational 
environment and mode in which applications function. In this chapter, you have 
looked at external system details that affect how secure an application is in a 
deployment environment. When conducting audits on an application, you need to 
consider the target deployment environment (if one is available) and the application's 
default configuration parameters. Unsafe or unnecessary exposure of the application 
can lead to vulnerabilities that are entirely independent of the program code. 
6.4 Chapter 4.  Application Review Process 
Chapter 4. Application Review Process 