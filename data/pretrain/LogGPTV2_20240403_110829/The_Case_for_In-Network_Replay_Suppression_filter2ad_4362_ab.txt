consider as bottleneck links from 20 to 40. For Stage 3, we choose
a small number of measurement points in the target area that will
be used to perform traceroutes to the vantage points; we obtain
the measurement points from RIPE Atlas [34]. Finally, we per-
form traceroutes from all Planetlab nodes to all nodes in the target
area, and from all measurement points to all Planetlab nodes. This
gives us both the list of routing bottlenecks and the list of candidate
routers.
Routing Bottlenecks. The ﬁrst interesting result is the location of
the routing bottlenecks in terms of hop distance from the vantage
points and the target area. We measure the average hop distance
from the source and compare it to the average path length. For
many of our traces, we do not obtain responses from the last hops;
usually this is due to ﬁrewalls in the hosts’ local networks. In such
cases, we assume that the destination resides after the last respond-
ing hop, resulting in a shorter average path length. In other words,
2We used SHODAN (https://www.shodan.io/) as our search en-
gine. When target areas are more conﬁned regions (e.g., a city),
the location of the public servers must be cross-veriﬁed with other
geolocation services.
i
s
k
n
L
k
c
e
n
e
l
t
t
o
B
f
o
.
c
o
L
.
l
e
R
1.0
0.8
0.6
0.4
0.2
0.0
20
25
30
35
40
Number of Bottleneck Links
Area #1
Area #2
Area #3
Area #4
Figure 2: Relative location and standard deviation of bottle-
neck links.
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
s
r
e
t
u
o
R
f
o
n
o
i
t
a
c
o
L
e
v
i
t
a
e
R
l
Inbound Replay
Outbound Replay
20
25
Number of Bottleneck Links
30
35
40
Area #1
Area #2
Area #3
Area #4
Figure 3: Relative location and standard deviation of routers
that can target at least one bottleneck, for inbound and out-
bound replays.
we obtain an upper bound for the relative location of the bottle-
necks with respect to the average path length.
Figure 2 shows the average relative location and standard devia-
tion of the bottleneck links for each target area. The result shows
that the routing bottlenecks are located approximately in the middle
of the routes and conﬁrms the results of previous work [23]. Fur-
thermore, the location of the links does not ﬂuctuate signiﬁcantly
as the number of bottleneck links increases.
Attack Router Identiﬁcation. We discover routers that can re-
play packets in the outbound and inbound direction, and hence, are
candidates for compromise. We show the average location of can-
didate routers and the number of routers that can target at least one
bottleneck link.
Figure 3 shows the average location of the candidate routers that
can replay packets; the upper and lower box plots show the loca-
tion of the routers for inbound and outbound replay, respectively.
For inbound replay attacks, the candidate routers are located be-
fore the bottleneck links; this is expected, since bottleneck links
must be located downstream with respect to the candidate routers.
For outbound replay attacks, the candidate routers are located ap-
proximately in the middle of the routes; this happens because route
diversity increases close to the core, and thus, routers can launch at-
tacks without attacking themselves in the inbound direction. Again,
we see no considerable change as b changes.
Figure 4 shows the number of candidate routers that can target at
least one routing bottleneck; the upper and lower portions of each
bar represent the number of candidate routers that can be used for
outbound and inbound replay attacks, respectively. We observe two
interesting ﬁndings. First, there is an abundant number of candidate
routers to compromise, ranging from hundreds to thousands. Sec-
ond, increasing the value of b does not signiﬁcantly increase the
s
r
e
t
u
o
R
f
o
r
e
b
m
u
N
3000
2500
2000
1500
1000
500
0
Inbound
20
25
30
35
40
Number of Bottleneck Links
Area #1
Area #2
Area #3
Area #4
Outbound
Figure 4: Number of routers that can target at least one bottle-
neck.
number of candidate routers. This is because the additional links
that are considered are adjacent to the initial routing bottlenecks,
and thus, only a few more candidate routers are discovered.
2.4 Practical Considerations
Mitigating Measurement Inaccuracies. We had to handle two
common sources of inaccuracies related to traceroutes.
First, traceroute may miss nodes, links, or even report false links [35].
We do not use specialized traceroute tools (e.g., Paris traceroute [35]),
since load-balanced links cannot become routing bottlenecks. In-
stead, we obtain multiple traces for every ﬂow (10 probe packets
per trace) to eliminate inaccuracies due to load balancers.
Second, alias-resolution tools are mostly based on implementa-
tion speciﬁc details of routers and may introduce false negatives
and false positives. False negatives fail to cluster interfaces that
belong to the same router, which may reduce the number of can-
didates for attack routers. However, this is not an issue since we
can still ﬁnd plenty of candidate routers. False positives associate
interfaces of different routers to the same router, which can lead
to false router identiﬁcation as good targets. To reduce/eliminate
false positives, we use the Monotonic ID-Based Alias Resolution
(MIDAR) [36] tool from CAIDA for two reasons. First, the mono-
tonic bound tests of MIDAR yield a very low false positive rate.
Second, its efﬁciency in resolving aliases in large lists of interface
IP addresses: to resolve aliases in a list of N IP addresses it probes
O(N) pairs instead of testing the O(N2) candidate pairs.
In this paper, we focus on one severe
Compromising Routers.
consequence of router compromises rather than software security
of routers; the latter is a research topic on its own right.
Our work, however, is motivated by the observation that com-
promised routers are already a major concern for ISPs. It is known
that state-level adversaries are massively targeting routers—they
are easy to compromise as they are rarely updated and lack security
software to detect breaches [37]. Cisco has issued a document to
warn operators of attacks against their routers and to inform them
about commonly used attack vectors [38]. Security companies con-
sider these attacks only the tip of the iceberg and highlight the dif-
ﬁculty of detecting such compromises, allowing attackers to main-
tain access for long time periods [39].
In addition, the emergence of SDN in ISP networks provides
endless possibilities for infrastructure compromises, since SDN se-
curity is not yet mature. Researchers have warned that attackers
can compromise networks directly [14,15]: First, controller vulner-
abilities allow attackers to compromise controllers and take con-
trol over the entire underlying infrastructure [40]. Second, SDN
switches have proven insecure as well, allowing attackers to install
persistent malware [41].
Ampliﬁcation Effect. Our attack leverages UDP services since
they do not perform replay suppression and they commonly have
responses that are much larger than the requests that caused them,
i.e., we exploit the UDP ampliﬁcation effect. Recent attacks have
exploited extreme ampliﬁcation factors (e.g., NTP monlist com-
mands can have a factor up to 4700) of misconﬁgured services,
however, moderate ampliﬁcation factors are common in legitimate
requests as well. For example, DNSSEC requests can have an am-
pliﬁcation factor of 30; the GetBulk operation in SNMPv2 has an
ampliﬁcation factor of 6.3; and the BitTorrent hash searches have
an average factor of 3.8. Routers that replay packets in the in-
bound direction have to rely on their available capacity to cause
congestion. Note that although bottleneck links are adequately pro-
visioned for normal network conditions, the additional load caused
by small-to-moderate ampliﬁcation at a router’s full capacity would
signiﬁcantly degrade the available capacity.
Early Congestion. In case of early congestion, a link that is located
upstream of the bottleneck link gets congested. Early congestion
does not render the attack impossible, but conﬁnes its effect. A
router that replays outbound trafﬁc has no visibility in the inbound
direction and thus cannot react to early congestion. A router that
replays inbound trafﬁc, however, has the bottleneck links located
downstream. Thus, the router can perform traceroutes to the target
area and determine early congestion based on the responses:
in
case of early congestion the router would not receive most of the
ICMP replies. The router can then react by decreasing the replay
rate of the corresponding ﬂows; at the same time, it can increase
the replay rate of ﬂows that exit from the same interface, but hit
another bottleneck.
Attack Detectability. A high replay rate of speciﬁc ﬂows can
trigger ﬁrewall alarms whenever network operators employ rate-
limiting controls; e.g., Response Rate Limiting in DNS name servers.
Although, in principle, this may limit an adversary’s high-intensity
replays for outbound, in practice this is easily overcome: an attack
router can replay different ﬂows that hit the same routing bottle-
necks in the inbound direction.
Replaying packets in the inbound direction (see simpliﬁed at-
tack) is less prone to rate-limiting. Intrusion detection systems and
protection mechanisms are typically deployed close to the hosts and
are not pervasive in the network, offering protection to resources of
end systems, but not network resources. Routing bottlenecks are
located in the middle of the routes and thus at a safe distance from
the actual targets of the attack. This yields such defense mecha-
nisms ineffective against inbound packet replay.
3. CHALLENGES FOR IN-NETWORK RE-
PLAY SUPPRESSION
End-to-end replay-suppression mechanisms, i.e., mechanisms real-
ized at the communicating end hosts, cannot be used at the network
layer due to fundamental operational differences:
1. The packet throughput of routers is orders of magnitude higher
than the packet throughput of the fastest services. Consequently,
a replay-detection overhead that is tolerable for a server may be
intolerable for a router.
2. Routers are equipped with a few tens of MBs of fast memory
(SRAM) per data-path chip. Routers use fast memory for per-
packet operations in order to minimize latency and sustain a high
throughput. However, hardware manufacturers do not integrate
more than a few tens of MBs of memory per chip, as the yield
becomes too low to sustain the manufacturing process. On the
contrary, end servers can meet their performance requirements
by using larger and slower memory (DRAM) combined with the
fast memory of CPU caches.
3. Novel mechanisms at the network layer often require coordina-
tion and introduce complex interactions between network enti-
ties. For example, a mechanism may require time synchroniza-
tion among routers of different domains.
In-Network Mechanisms
3.1
To detect and suppress replayed packets in the network, a router
must inspect each packet it forwards; thus, the detection mecha-
nism must be efﬁcient and lightweight so that it does not impair
forwarding performance. We consider three main challenges for
universally deploying a novel mechanism at the network layer:
1. Computation Overhead. In order to sustain a high through-
put, a router has a strict time budget to serve a packet. The two
major components that carve out this budget are latencies due
to memory operations and due to CPU-intensive operations. For
example, if the memory footprint for replay detection is too large
to ﬁt into the fast memory of the router (e.g., on-chip caches),
forwarding performance will be degraded due to cache misses.
Likewise, if CPU-intensive operations, such as frequent public-
key signature veriﬁcations become necessary, the forwarding
performance will suffer.
2. Communication Overhead. The communication overhead comes
in the forms of latency overhead and bandwidth overhead. For
example, if a router needs to ask another entity (e.g., the source
host who created the packet or a remote router) to check the
authenticity of the packet, the communication latency will in-
crease substantially. Furthermore, additional data in packets or
additional messages will increase the bandwidth overhead (es-
pecially if they are sent frequently).
3. Time Synchronization. In one extreme, a replay detection mech-
anism does not require any of the entities (e.g., routers, ASes,
hosts) to be synchronized; and on the other extreme, it may re-
quire every entity in the Internet to be synchronized. A middle-
ground solution may require only parts of the networks to be
synchronized (e.g., entities within each autonomous system).
Inadequacy of E2E Replay Detection
3.2
Early work on replay detection identiﬁed four basic primitives, and
combinations thereof, that are found in all end-to-end protocols for
secure communication [42]. We present these primitives and study
their applicability for in-network replay detection.
Storing Packet Digests. A router stores a digest for every packet
that it forwards. When the router receives a new packet, it checks
whether it has seen the packet before. This has signiﬁcant ad-