Norm-Cut is better than Base-Cut especially when œÅ ‚â• 2.
Intuitively, the norm-based methods should perform better an-
swering set-queries. But Norm-Mul does not. This is because
the multiplication operation reduces the large estimates a lot,
making them biased. This also demonstrates that enforcing
sum-to-one is not enough. Different approaches perform sig-
niÔ¨Åcantly different.
Fixed set queries. Besides random set queries, we include
a case study of Ô¨Åxed subset queries for the Emoji dataset.
The queries ask the frequency of each category2. There are
68 categories with the mean of 10.4 items per set. The MSE
varying  is reported in Figure 8. It is interesting to see that the
Post-Pos works best in the left sub-Ô¨Ågure, and Norm-Cut from
the right performs even better, especially when   10.
12













,80,80
!48!489
!48472472
:472
$:-











472
$:-
5,80
:9472
:9!407!407$


























Zipf‚Äôs
Emoji
Fig. 7. MSE results on set-value estimation, varying set size percentage œÅ from 1 to 10, Ô¨Åxing  = 1.
Fig. 8. MSE results on set-case estimation for the Emoji dataset, varying  from 0.2 to 4.
F. Discussion
In summary, we evaluate the 10 post-processing methods
on different datasets, for different tasks, and varying different
parameters. We now summarize the Ô¨Åndings and present
guidelines for using the post-processing methods.
With the experiments, we verify the connections among
the methods: Norm-Sub and MLE-Apx perform similarly, and
Base and Norm performs similarly.
The best choice for post-processing method depends on
the queries one wants to answer. If set-value estimation is
needed, one should use PowerNS. When the set is Ô¨Åxed, one
can also choose the optimal method using a synthetic dataset
processed with Norm-Sub. The intuition is that PowerNS
improves over the approximate MLE (i.e., Norm-Sub, which
is a theoretically testiÔ¨Åed method) by making the estimates
closer to the underlying distribution. If one just want
to
estimate results for the most frequent values, one can use
Norm. While Base can also be used, Norm reduces variance
by utilizing the property that the estimates sum up to 1. These
two methods do not change any value dramatically. Finally,
if one cares about single value queries only, Base-Cut should
be used. This is because when many values in the dataset
are of low frequency, converting low estimates to 0 beneÔ¨Åt
the utility. Overall, one can follow the guideline for choosing
post-processing methods.
‚Ä¢ When single value queries are desired, use Base-Cut.
‚Ä¢ When frequent values are desired, use Norm.
‚Ä¢ When set-value queries are desired, use PowerNS or
13




,80,80
!48!489
!48472472
:472
$:-


472
$:-
5,80
:9472
:9!407!407$

















,80,80
!48!489
!48472472
:472
$:-







472
$:-
5,80
:9472
:9!407!407$setting. That is, Ô¨Årst, the total number of users is known;
second, negative values are not possible. We found that in the
LDP setting, on the contrary to [19], minimizing L2 distance
achieves MLE under the approximation that the noise is close
to the Gaussian distribution. There are also post-processing
techniques proposed for other settings: Blasiok et al. [6]
study the post-processing for linear queries, which generalizes
histogram estimation; but their method only applied to a non-
optimal LDP mechanism. [28] and [22] consider the hierarchy
structure and apply the technique of [16]. [37] considers mean
estimation and propose to project the result into [0, 1].
Fig. 9. Synthetic estimation for set-case query on the Emoji dataset.
VII. CONCLUSION
select one using synthetic datasets.
VI. RELATED WORK
LDP frequency oracle (estimating frequencies of values)
is a fundamental primitive in LDP. There have been several
mechanisms [14], [5], [31], [4], [2], [36] proposed for this
task. Among them, [31] introduces OLH, which achieves
low estimation errors and low communication costs on large
domains. Hadamard Response [4], [2] is similar to OLH in
essence, but uses the Hadamard transform instead of hash
functions. The aggregation part is faster because evaluating
a Hadamard entry is practically faster; but it only outputs a
binary value, which gives higher error than OLH for larger
 setting. Subset selection [36], [30] achieves better accuracy
than OLH, but with a much higher communication cost.
LDP frequency oracle is also a building block for other
analytical
tasks, e.g., Ô¨Ånding heavy hitters [4], [7], [34],
frequent itemset mining [26], [33], releasing marginals under
LDP [27], [8], [38], key-value pair estimation [37], [15],
evolving data monitoring [18], [13], and (multi-dimensional)
range analytics [32], [22]. Mean estimation is also a building
block in LDP; most of existing work transforms the numerical
value to a discrete value using stochastic round, and then apply
frequency oracles [11], [29], [24].
There exist efforts to post-process results in the setting of
centralized DP. Most of them focus on utilizing the structural
information in problems other than the simple histogram, e.g.,
estimating marginals [10], [25] and hierarchy structure [16].
The methods do not consider the non-negativity constraint.
Other than that, they are similar to Norm-Sub and minimize
L2 distance. On the other hand, the authors of [23] started
from MLE and propose a method to minimize L1 instead of
L2 distance, as the DP noise follows Laplace distribution.
In the LDP setting, Kairouz et al. [19] study exact MLE
for GRR and RAPPOR [14]; and empirically show exact
MLE performs worse than Norm-Sub. In [3], Bassily proves
the error bound of Norm-Sub for the Hadamard Response
mechanism. Jia et al. [17] propose to use external information
about the dataset‚Äôs distribution (e.g., assume the underlying
dataset follows Gaussian or Zipf‚Äôs distribution). We note
that such information may not always be available. On the
other hand, we exploit the basic information in each LDP
In this paper, we study how to post-process results from
existing frequency oracles to make them consistent while
achieving high accuracy for a wide range of tasks, including
frequencies of individual values, frequencies of the most
frequent values, and frequencies of subsets of values. We
considered 10 different methods, in addition to the baseline.
We identiÔ¨Åed Norm performs similar to Base, and MLE-
Apx performs similar to Norm-Sub. We then recommend that
for full-domain estimation, Base-Cut should be used; when
estimating frequency of the most frequent values, Norm should
be used; when answering set-value queries, PowerNS or the
optimal one from synthetic dataset should be used.
ACKNOWLEDGEMENT
This project is supported by NSF grant 1640374, NWO
grant 628.001.026, and NSF grant 1931443. We thank our
shepherd Neil Gong and the anonymous reviewers for their
helpful suggestions.
REFERENCES
[1] Apple differential privacy team, learning with privacy at scale, 2017.
[2] J. Acharya, Z. Sun, and H. Zhang. Hadamard response: Estimating
In
distributions privately, efÔ¨Åciently, and with little communication.