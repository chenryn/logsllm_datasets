prediction vector at each location to its maximum non-background
element to discard label information (eliminating Clean Error 1)
and perform binarization (with a threshold) to remove objectness
predicted with low confidence (mitigating Clean Error 2).
Pseudocode. First, we initialize an all-zero R𝑁+1 vector (𝑁 object
classes plus the “background" class) at every feature location and
use one tensor ¯om to represent all vectors (Line 15). Second, we
aim to gather all window classification results: for each window,
we add the classification logits v to every vector located within the
window (Line 18). Third, we take the maximum non-background
element in each vector as the objectness score at the corresponding
6The local logits vector [58] is the classification logits based on each local feature that
is extracted from a particular region (i.e., the receptive field) of the input image.
7We will use feature-space coordinates for the remainder of the paper. The mapping
between pixel-space and feature-space coordinates is discussed in Appendix G.
𝟏𝟎𝟖𝟎𝟐𝟓𝟏𝟎𝟖𝟏𝟏𝟖𝟖𝟕𝟎𝟓𝟎𝟏𝟏𝟐𝟓𝟓𝟔𝟏𝟔𝟎𝟎𝟏𝟐𝟏𝟏𝟖𝟓𝟐𝟔𝟔𝟎𝟎𝟎𝟎𝟏𝟏𝟎𝟏𝟏DogCatBack-groundRobust feature extractionRobust classificationMax element(ignoring background)BinarizationObjectness mapRobust feature-space window classification Objectness map generation (with error filtering)Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3181Algorithm 1 DetectorGuard
Input: input image x, window size (𝑤𝑥, 𝑤𝑦), binarizing thresh-
old 𝑇 , Base Detector BaseDetector(·), robust classifier RC(·)
(consisting of a feature extractor FE(·) and a robust classifica-
tion head RCH(·)), cluster detection procedure DetCluster(·)
Output: robust detection D∗ or ALERT
1: procedure DG(x, 𝑤𝑥, 𝑤𝑦,𝑇 )
D ← BaseDetector(x)
2:
om ← ObjPredictor(x, 𝑤𝑥, 𝑤𝑦,𝑇)
3:
𝑎 ← ObjExplainer(D, om)
4:
if 𝑎 == True then
5:
D∗ ← ALERT
6:
7:
D∗ ← D
8:
end if
9:
return D∗
10:
11: end procedure
else
⊲ Conventional detection
⊲ Objectness
⊲ Detect hiding attacks
⊲ Malicious mismatch
⊲ Trigger an alert
⊲ Return Base Detector’s predictions
12: procedure ObjPredictor(x, 𝑤𝑥, 𝑤𝑦,𝑇 )
13:
14:
15:
16:
17:
fm ← FE(x)
𝑋, 𝑌, _ ← Shape(fm)
¯om ← ZeroArray[𝑋, 𝑌, 𝑁 + 1]
for each valid (𝑖, 𝑗) do
⊲ Extract feature map
⊲ Get the shape of fm
⊲ Initialization
⊲ Every window location
𝑙, v ← RCH(fm[𝑖 : 𝑖 + 𝑤𝑥, 𝑗 : 𝑗 + 𝑤𝑦])
⊲ Classify
⊲ Add classification logits
¯om[𝑖 : 𝑖 +𝑤𝑥, 𝑗 : 𝑗 +𝑤𝑦] ← ¯om[𝑖 : 𝑖 +𝑤𝑥, 𝑗 : 𝑗 +𝑤𝑦] + v
⊲ Max objectness score
⊲ Binarization
18:
end for
19:
¯om ← MaxObj( ¯om, axis = −1)
20:
om ← Binarize( ¯om,𝑇 · 𝑤𝑥 · 𝑤𝑦)
21:
return om
22:
23: end procedure
24: procedure ObjExplainer(D, om)
25:
ˆom ← Copy(om)
⊲ Match each detected box to objectness map
for 𝑖 ∈ {0, 1, · · · , |D| − 1} do
⊲ A copy of om
𝑥min, 𝑦min, 𝑥max, 𝑦max, 𝑙 ← b ← D[𝑖]
if Sum(om[𝑥min : 𝑥max, 𝑦min : 𝑦max]) > 0 then
ˆom[𝑥min : 𝑥max, 𝑦min : 𝑦max]) ← 0
end for
if DetCluster( ˆom) is None then
end if
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37: end procedure
end if
else
return False
return True
⊲ All objectness explained
⊲ Unexplained objectness
location (Line 20). This operation discards label information and
fully eliminates Clean Error 1, e.g., confusion between bicycles and
motorbikes. At last, we binarize every objectness score (Line 21):
if the score is larger than 𝑇 · 𝑤𝑥 · 𝑤𝑦, we set it to one; otherwise,
set it to zero. This binarization mitigates Clean Error 2, when the
classifier incorrectly predicts background as objects but with low
Figure 3: Visualization of explaining/matching rules
classification confidence. We discuss the strategy for Clean Error 3
in the next subsection.
3.4 Objectness Explainer
Objectness Explainer takes as inputs the predicted bounding boxes
of Base Detector and the generated objectness map of Object-
ness Predictor, and tries to use each predicted bounding box to
explain/match the high activation in the objectness map. Its out-
come determines the final prediction of DetectorGuard. We will
first introduce the high-level explaining/matching rules and then
elaborate on the algorithm.
Explaining/matching rules. There are three possible explain-
ing/matching outcomes, each of them leading to a different predic-
tion strategy (a visual example is in Figure 3):
• A match happens when Base Detector and Objectness Pre-
dictor both predict a bounding box or high objectness at a
specific location. In this simplest case, the objectness is well
explained by the bounding box; our defense will consider the
detection as correct and output the accurate bounding box
and the class label predicted by Base Detector.
• A malicious mismatch will be flagged when only Object-
ness Predictor outputs high objectness. This is most likely to
happen when a hiding attack fools the conventional object
detector to miss the object while our Objectness Predictor
still makes robust predictions. In this case, our defense will
find unexplained objectness and send out an attack alert.
• A benign mismatch occurs when only Base Detector de-
tects the object and there is no objectness to be explained. This
can happen when Objectness Predictor incorrectly misses
the object due to its limitations (recall our remark in Sec-
tion 3.3). In this case, we trust Base Detector and output its
predicted bounding box. Notably, this strategy can fully elim-
inate Clean Error 3, i.e., predicting objects as background.8
Next, we discuss the concrete procedure for explaining objectness.
Matching and explaining objectness. In Line 25-31 of Algo-
rithm 1, we use each predicted bounding box to match/explain the
8We note that this miss can also be caused by other attacks that are orthogonal to
the focus of this paper, e.g., FP attacks that aim to introduce incorrect bounding box
predictions. We will discuss such attacks and our defense strategies in Section 6.
MatchMalicious mismatchBenign mismatchObjectness PredictorBase DetectorDetectorGuard OutputALERT!1Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3182objectness predicted at the same location. First, we create a copy of
om as ˆom to hold the explaining results. Next, for each bounding
box b, we get its coordinates 𝑥min, 𝑦min, 𝑥max, 𝑦max, and calculate
the sum of objectness scores within the same box on the objectness
map om. If the objectness sum is larger than zero, we assume that
the bounding box b agrees with om, and we zero out the corre-
sponding region in ˆom, to indicate that this region of objectness
has been explained by the detected bounding box. On the other
hand, if all objectness scores are zeros, we assume it is a benign
mismatch; the algorithm proceeds without alert.
Detecting clusters of unexplained objectness. The final step
is to detect unexplained objectness in ˆom. We use the sub-procedure
DetCluster(·) to determine if any non-zero points in ˆom form
a large cluster. Specifically, we choose DBSCAN [12] as the clus-
ter detection algorithm, which will assign each point to a certain
cluster or label it as an outlier based on the point density in its
neighborhood. If DetCluster( ˆom) returns None, it means that no
large cluster is found, or all objectness predicted by Objectness
Predictor is explained by the bounding boxes predicted by Base
Detector; ObjExplainer(·) then returns False (i.e., no attack de-
tected). We note that this clustering operation further mitigates
Clean Error 2 when the robust classifier predicts background as
objects at only a few scattered locations. On the other hand, re-
ceiving a non-empty cluster set indicates that there are clusters of
unexplained objectness activations in ˆom (i.e, Base Detector misses
an object but Objectness Predictor predicts high objectness). Ob-
jectness Explainer regards this as a sign of patch hiding attacks and
returns True.
Final output. Line 5-10 of Algorithm 1 demonstrates the strat-
egy for the final prediction. If the alert flag 𝑎 is True (i.e., a malicious
mismatch is detected), DetectorGuard returns D∗ = ALERT. In other
cases, DetectorGuard returns the detection D∗ = D.
Remark: Clean performance of DetectorGuard. Recall that
Clean Error 1 of the robust classifier is fully eliminated in our ob-
jectness map generation via discarding label information; Clean
Error 2 is mitigated via binarizing (in Objectness Predictor) and
clustering (in Objectness Explainer) operations; Clean Error 3 is
fully tolerated via our prediction matching strategy (the benign
mismatch case). Therefore, we can safely optimize the setting of
DetectorGuard to mitigate most of Clean Error 2 (which can lead to
unexplained objectness in the clean setting and trigger a false alert)
so that we can achieve a clean performance that is comparable to
state-of-the-art object detectors (performance difference smaller
than 1%; more details are in Section 5). This helps us solve Chal-
lenge 2: Amplified Cost of Clean Performance. In the next section, we
will demonstrate that our efforts in mitigating the imperfection of
robust classifiers are worthwhile by showing how DetectorGuard
addresses Challenge 1: Lack of End-to-end Provable Robustness.
4 END-TO-END PROVABLE ROBUSTNESS
Recall that we consider DetectorGuard to be provably robust for a
given object (in a given image) when it can make correct detection
on the clean image and will either detect part of the object or issue
an alert on the adversarial image. The robustness property holds
for any adaptive patch hiding attacker at any location within our
threat model, including ones who have full access to our models
Algorithm 2 Provable Analysis of DetectorGuard
Input: input image x, window size (𝑤𝑥, 𝑤𝑦), matching threshold𝑇 ,
the set of patch locations P, the object bounding box b, feature
extractor FE(·), provable analysis of the robust classification
head RCH-PA(·), cluster detection procedure DetCluster(·)
⊲ Clean detection is incorrect
⊲ Extract feature map
⊲ Check every patch location
return False
if b ∉ DG(x, 𝑤𝑥, 𝑤𝑦,𝑇) then
Output: whether the object b in x has provable robustness
1: procedure DG-PA(x, 𝑤𝑥, 𝑤𝑦,𝑇 , P, b)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
end for
12:
return True
13:
14: end procedure
end if
fm ← FE(x)
for each p ∈ P do
𝑥, 𝑦, 𝑝𝑥, 𝑝𝑦 ← p
𝑟 ← DG-PA-One(fm, 𝑥, 𝑦, 𝑤𝑥, 𝑤𝑦, 𝑝𝑥, 𝑝𝑦, b,𝑇)
if 𝑟 == False then
return False
end if
⊲ Possibly vulnerable
⊲ Provably robust
𝑗, 𝑝𝑥, 𝑝𝑦)
15: procedure DG-PA-One(fm, 𝑥, 𝑦, 𝑤𝑥, 𝑤𝑦, 𝑝𝑥, 𝑝𝑦, b,𝑇 )
16: