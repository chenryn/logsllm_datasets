approach are that 1) we consider a different attack scenario where
an adversary has access to two snapshots of a model, and 2) our
canaries follow the distribution of the data whereas Carlini et al.
[6] add a random sequence of numbers in a fixed context into a
dataset of financial news articles (e.g., â€œThe random number is ...â€),
where such phrases are rare. We instead are able to extract canaries
without any context, even when the canary token frequency in the
training dataset is as low as one in a million.
CCS â€™20, November 9â€“13, 2020, Virtual Event, USA
Zanella-BÃ©guelin, Wutschitz, Tople, RÃ¼hle, Paverd, Ohrimenko, KÃ¶pf, and Brockschmidt
)
ğ‘ 
(
)
ğ·
(
m
a
r
g
âˆ’
3
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘
1 Ã— 1010
1 Ã— 109
1 Ã— 108
1 Ã— 107
1 Ã— 106
100000
10000
1000
ğ‘˜ = 100
ğ‘˜ = 20
ğ‘˜ = 5
100000
1 Ã— 106
1 Ã— 107
ğ‘ğ‘’ğ‘Ÿğ‘3âˆ’gram(ğ‘)(ğ‘ )
(a) Re-training from scratch
)
ğ‘ 
(
)
ğ·
(
m
a
r
g
âˆ’
3
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘
1 Ã— 1010
1 Ã— 109
1 Ã— 108
1 Ã— 107
1 Ã— 106
100000
10000
1000
ğ‘˜ = 100
ğ‘˜ = 20
ğ‘˜ = 5
1 Ã— 107
100000
1 Ã— 106
ğ‘ğ‘’ğ‘Ÿğ‘3âˆ’gram(ğ‘)(ğ‘ )
(b) Continued training
Figure 3: Sentences extracted from (ğ‘€, ğ‘€â€²) using Differential Score when the adversary only receives the top ğ‘˜ tokens from
the updated model ğ‘€â€² for each query. The axes have the same meaning as in Figures 2a and 2b.
Salem et al. [24] consider reconstruction of training data that
was used to update a model. While their goal is similar to ours,
their adversarial model and setup differ: 1) similar to Song and
Shmatikov [27] and Shokri et al. [26], their attacker uses shadow
models trained on auxiliary data drawn from the same distribution
as the target training dataset, while in our setting the attacker has
no prior knowledge of this distribution and does not need auxiliary
data; 2) the updated model is obtained by fine-tuning the target
model with additional data rather than re-training it from scratch
on the changed dataset; 3) the focus is on classification models and
not on (generative) language models.
Information leakage from updates has also been considered for
searchable encryption: an attacker who has control over data in an
update to an encrypted database can learn information about its
content and previous encrypted searches on it [7].
Model inversion attacks. Fredrikson et al. [12, 13] repurpose a
model to work backwards, inferring unknown attributes of individ-
uals given known attributes and a target prediction. Individuals
need not be present in the training data, and results are aggregate
statistics rather than information about specific training points.
Differential Privacy. In terms of defenses, McMahan et al. [21]
study how to train LSTM models with DP guarantees at a user-level.
They investigate utility and privacy trade-offs of the trained mod-
els depending on a range of parameters (e.g., clipping bound and
batch size). Carlini et al. [6] show that DP protects against leakage
of canaries in character-level models, while Song and Shmatikov
[27] show that an audit as described above fails when training
language models with user-level DP using the techniques of [21].
Pan-privacy [10], on the other hand, studies the problem of main-
taining differential privacy when an attacker observes snapshots of
the internal state of a DP algorithm between updates.
Deletion of Data. Techniques to update models to delete training
data points can be broadly classified into exact and approximate
deletion. Ginart et al. [14] define exact deletion of a training point
from a model as a stochastic operation returning the same distri-
bution as re-training from scratch without that point, and develop
deletion algorithms for ğ‘˜-means clustering with low amortized cost.
Bourtoule et al. [5] propose an exact deletion methodology that
aggregates models trained on disjoint data shards, trading storage
for computation such that only shards that contain deleted points
need to be retrained. Exact deletion is equivalent to retraining
from scratch, hence, publishing model snapshots before and after
deletion matches our adversarial model and our results apply.
Contemporary approximate deletion methods [15, 16] yield mod-
els that are only statistically indistinguishable from a model re-
trained from scratch. These methods stochastically update model
parameters based on estimates of the influence of the data to be
deleted and achieve relaxations of differential privacy. It would be
interesting to study how susceptible to snapshot attacks are models
obtained by approximate deletion.
8 CONCLUSION
We presented a first systematic study of the privacy implications
of releasing snapshots of a language model trained on overlapping
data. Our results show that updates pose a threat which needs to be
considered in the lifecycle of machine learning applications. We en-
courage the research community to work towards quantifying and
reducing unintended information leakage caused by model updates,
and hope to make practitioners aware of the privacy implications
of deploying and updating high-capacity language models.
ACKNOWLEDGMENTS
We thank Doug Orr and Nicolas Papernot for helpful discussions
and the anonymous reviewers for their valuable comments.
Analyzing Information Leakage of Updates to Natural Language Models
CCS â€™20, November 9â€“13, 2020, Virtual Event, USA
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In
23rd ACM SIGSAC Conference on Computer and Communications Security, CCS
2016. ACM, 308â€“318.
[2] Galen Andrew, Steve Chien, and Nicolas Papernot. 2020. TensorFlow Privacy.
https://github.com/tensorflow/privacy.
[3] Arm. 2020. TrustZone Technology. https://developer.arm.com/ip-products/
security-ip/trustzone
[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014. Private Empirical Risk
Minimization: Efficient Algorithms and Tight Error Bounds. In 55th IEEE Annual
Symposium on Foundations of Computer Science, FOCS 2014. IEEE Computer
Society, 464â€“473.
[5] Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui
Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine
Unlearning. In 42nd IEEE Symposium on Security and Privacy, S&P 2021. IEEE
Computer Society. To appear.
[6] Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural
Networks. In 28th USENIX Security Symposium. USENIX Association, 267â€“284.
[7] David Cash, Paul Grubbs, Jason Perry, and Thomas Ristenpart. 2015. Leakage-
Abuse Attacks Against Searchable Encryption. In 22nd ACM SIGSAC Conference
on Computer and Communications Security, CCS 2015. ACM, 668â€“679.
[8] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Hum-
bert, and Yang Zhang. 2020. When Machine Unlearning Jeopardizes Privacy.
arXiv:2005.02205 [cs.CR]
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL-HLT 2019, Vol. 1. Association
for Computational Linguistics, 380â€“385.
[10] Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N. Rothblum, and Sergey
Yekhanin. 2010. Pan-Private Streaming Algorithms. In Innovations in Computer
Science, ICS 2010. Tsinghua University Press, 66â€“80.
[11] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differen-
tial Privacy. Foundations and Trends in Theoretical Computer Science 9, 3-4 (2014),
211â€“407.
[12] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion
Attacks that Exploit Confidence Information and Basic Countermeasures. In 22nd
ACM SIGSAC Conference on Computer and Communications Security, CCS 2015.
ACM, 1322â€“1333.
[13] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon M. Lin, David Page, and
Thomas Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case
Study of Personalized Warfarin Dosing. In 23rd USENIX Security Symposium.
USENIX Association, 17â€“32.
[14] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. 2019. Making
AI Forget You: Data Deletion in Machine Learning. In Advances in Neural Infor-
mation Processing Systems 32, NeurIPS 2019. Curran Associates, Inc., 3518â€“3531.
[15] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal Sunshine of
the Spotless Net: Selective Forgetting in Deep Networks. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, CVPR 2020. IEEE, 9301â€“9309.
[16] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens van der Maaten. 2020.
Certified Data Removal from Machine Learning Models. In 37th International
Conference on Machine Learning, ICML 2020. PMLR. To appear.
[17] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9, 8 (1997), 1735â€“1780.
us/sgx
[18] Intel. 2020. Software Guard Extensions (SGX). https://software.intel.com/en-
[19] Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. In 12th International
Machine Learning Conference on Machine Learning, ICML 1995. Morgan Kaufmann,
331â€“339.
[20] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Build-
ing a Large Annotated Corpus of English: The Penn Treebank. Computational
Linguistics 19, 2 (1993), 313â€“330.
[21] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learn-
ing Differentially Private Recurrent Language Models. In 6th International Con-
ference on Learning Representations, ICLR 2018. OpenReview.net.
[22] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017.
Pointer Sentinel Mixture Models. In 5th International Conference on Learning
Representations, ICLR 2017. OpenReview.net.
[23] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. Technical
Report. OpenAI.
[24] Ahmed Salem, Apratim Bhattacharyya, Michael Backes, Mario Fritz, and Yang
Zhang. 2019. Updates-Leak: Data Set Inference and Reconstruction Attacks in
Online Learning. arXiv:1904.01067 [cs.CR]
[25] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2019. ML-Leaks: Model and Data Independent Membership
Inference Attacks and Defenses on Machine Learning Models. In 26th Annual
Network and Distributed System Security Symposium, NDSS 2019. The Internet
Society.
[26] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership Inference Attacks Against Machine Learning Models. In 38th IEEE Sym-
posium on Security and Privacy, S&P 2017. IEEE Computer Society, 3â€“18.
[27] Congzheng Song and Vitaly Shmatikov. 2019. Auditing Data Provenance in
Text-Generation Models. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, KDD 2019. ACM, 196â€“206.
[28] S. Song, K. Chaudhuri, and A. D. Sarwate. 2013. Stochastic Gradient Descent
with Differentially Private Updates. In 1st IEEE Global Conference on Signal and
Information Processing, GlobalSIP 2013. IEEE Computer Society, 245â€“248.
[29] European Union. 2016. Regulation (EU) 2016/679 of the European Parliament
and of the Council of 27 April 2016 on the protection of natural persons with
regard to the processing of personal data and on the free movement of such data,
and repealing Directive 95/46/EC (General Data Protection Regulation).
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
You Need. In Advances in Neural Information Processing Systems 30, NIPS 2017.
Curran Associates, Inc., 5998â€“6008.
[31] Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun,
Stefan Lee, David J. Crandall, and Dhruv Batra. 2018. Diverse Beam Search for
Improved Description of Complex Scenes. In 32nd AAAI Conference on Artificial
Intelligence, AAAI 2018. AAAI Press, 7371â€“7379.
[32] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
Risk in Machine Learning: Analyzing the Connection to Overfitting. In 31st IEEE
Computer Security Foundations Symposium, CSF 2018. IEEE Computer Society,
268â€“282.
[33] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent Neural
Network Regularization. arXiv:1409.2329 [cs.NE]
A RESULTS FOR TALK.POLITICS.MIDEAST
Table 7 (deferred from Section 4.5) shows the highest-scoring se-
quences of length 4 in a group beam search with 5 groups for the
talk.politics.mideast dataset for RNN and Transformer archi-
tectures.
Table 7: Top ranked phrases in a group beam search for a model updated with talk.politics.mideast. Center for Policy
Research is a prolific newsgroup poster; many of the posts around the time the 20 Newsgroups dataset [19] was collected
discuss tensions between Turkey and Armenia.
(cid:102)DS
(cid:102)DS
Phrase
RNN
Phrase
Transformer
Turkey searched first aid
Doll flies lay scattered
Arab governments invaded Turkey
Lawsuit offers crime rates
Sanity boosters health care
31.32
22.79
20.20
18.35
11.17
Center for Policy Research
Escaped of course ...
Holocaust %UNK% museum museum
Troops surrounded village after
Turkey searched neither Arab
200.27
95.18
88.20
79.35
37.69