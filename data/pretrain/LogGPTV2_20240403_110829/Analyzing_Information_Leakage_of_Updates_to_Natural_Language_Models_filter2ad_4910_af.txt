### Approach and Methodology

Our approach differs in two key aspects:
1. We consider an attack scenario where the adversary has access to two snapshots of a model.
2. Our canaries follow the distribution of the data, whereas Carlini et al. [6] insert a random sequence of numbers into a fixed context within a dataset of financial news articles (e.g., ‚ÄúThe random number is...‚Äù), where such phrases are rare. In contrast, we are able to extract canaries without any context, even when the canary token frequency in the training dataset is as low as one in a million.

### Experimental Results

**Figure 3: Sentences Extracted Using Differential Score**
- **(a) Re-training from Scratch**
- **(b) Continued Training**

This figure shows sentences extracted from (ùëÄ, ùëÄ‚Ä≤) using the Differential Score when the adversary only receives the top ùëò tokens from the updated model ùëÄ‚Ä≤ for each query. The axes have the same meaning as in Figures 2a and 2b.

### Related Work

#### Reconstruction of Training Data
Salem et al. [24] focus on reconstructing training data used to update a model. While their goal is similar to ours, their adversarial model and setup differ:
1. Similar to Song and Shmatikov [27] and Shokri et al. [26], their attacker uses shadow models trained on auxiliary data drawn from the same distribution as the target training dataset. In our setting, the attacker has no prior knowledge of this distribution and does not need auxiliary data.
2. The updated model is obtained by fine-tuning the target model with additional data rather than re-training it from scratch on the changed dataset.
3. Their focus is on classification models, not generative language models.

#### Information Leakage from Updates
Information leakage from updates has also been studied in the context of searchable encryption. An attacker who controls data in an update to an encrypted database can learn information about its content and previous encrypted searches [7].

#### Model Inversion Attacks
Fredrikson et al. [12, 13] repurpose a model to work backwards, inferring unknown attributes of individuals given known attributes and a target prediction. These results are aggregate statistics rather than specific training points, and individuals need not be present in the training data.

#### Differential Privacy
In terms of defenses, McMahan et al. [21] study how to train LSTM models with differential privacy (DP) guarantees at a user-level. They investigate the utility and privacy trade-offs of the trained models depending on various parameters (e.g., clipping bound and batch size). Carlini et al. [6] show that DP protects against canary leakage in character-level models, while Song and Shmatikov [27] demonstrate that an audit fails when training language models with user-level DP using the techniques of [21]. Pan-privacy [10] studies the problem of maintaining differential privacy when an attacker observes snapshots of the internal state of a DP algorithm between updates.

#### Deletion of Data
Techniques to update models to delete training data points can be broadly classified into exact and approximate deletion. Ginart et al. [14] define exact deletion as a stochastic operation returning the same distribution as re-training from scratch without that point, and develop deletion algorithms for ùëò-means clustering with low amortized cost. Bourtoule et al. [5] propose an exact deletion methodology that aggregates models trained on disjoint data shards, trading storage for computation such that only shards containing deleted points need to be retrained. Exact deletion is equivalent to retraining from scratch, so publishing model snapshots before and after deletion matches our adversarial model, and our results apply.

Contemporary approximate deletion methods [15, 16] yield models that are statistically indistinguishable from a model re-trained from scratch. These methods stochastically update model parameters based on estimates of the influence of the data to be deleted and achieve relaxations of differential privacy. It would be interesting to study how susceptible to snapshot attacks are models obtained by approximate deletion.

### Conclusion

We presented the first systematic study of the privacy implications of releasing snapshots of a language model trained on overlapping data. Our results show that updates pose a threat that needs to be considered in the lifecycle of machine learning applications. We encourage the research community to work towards quantifying and reducing unintended information leakage caused by model updates and hope to make practitioners aware of the privacy implications of deploying and updating high-capacity language models.

### Acknowledgments

We thank Doug Orr and Nicolas Papernot for helpful discussions and the anonymous reviewers for their valuable comments.

### References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In 23rd ACM SIGSAC Conference on Computer and Communications Security, CCS 2016. ACM, 308‚Äì318.
...
[33] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent Neural Network Regularization. arXiv:1409.2329 [cs.NE]

### Appendix A: Results for talk.politics.mideast

**Table 7: Top Ranked Phrases in a Group Beam Search for a Model Updated with talk.politics.mideast**

| Phrase | RNN Score | Phrase | Transformer Score |
| --- | --- | --- | --- |
| Turkey searched first aid | 31.32 | Center for Policy Research | 200.27 |
| Doll flies lay scattered | 22.79 | Escaped of course... | 95.18 |
| Arab governments invaded Turkey | 20.20 | Holocaust %UNK% museum museum | 88.20 |
| Lawsuit offers crime rates | 18.35 | Troops surrounded village after | 79.35 |
| Sanity boosters health care | 11.17 | Turkey searched neither Arab | 37.69 |

**Note:** The Center for Policy Research is a prolific newsgroup poster; many of the posts around the time the 20 Newsgroups dataset [19] was collected discuss tensions between Turkey and Armenia.