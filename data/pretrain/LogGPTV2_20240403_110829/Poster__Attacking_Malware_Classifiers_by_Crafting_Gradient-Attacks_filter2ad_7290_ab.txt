### Introduction

Our study involves eight distinct feature groups, including both parsed and agnostic-format features. Parsed features focus on general and header information of the file, as well as imported and exported functions. Agnostic-format features, on the other hand, pertain to string information and byte histograms [2]. The performance of the malware classifier, after being trained on the dataset, is illustrated in Figure 2, along with the training loss. We report the results calculated over 40 epochs, using the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metric to measure the rate of successful classification by the Convolutional Neural Network (CNN). This metric facilitates further comparison with other models. The reported performance includes an F-score of 94% at 5% false positive and 7% false negative rates. Further tuning can be performed to improve the true positive rates and to stabilize the loss function output.

### 3.1 Perturbation Injection

As depicted in Figure 1, we implement an internal module to generate malicious adversarial samples capable of evading the targeted malware classifier [6]. In Step 1, a malware sample \( s \) is sent to the manipulation box, where a sequence of defined perturbations is injected. Once the process is complete, a modified sample \( s' \) is generated and proceeds to Step 2 for validation. A sandbox is used to dynamically test the file. If \( s' \) is corrupt, the malware is dismissed, and the process is restarted. Otherwise, it moves to Step 3, where the sample is checked for detection. Both \( s \) and \( s' \) are then sent to Step 4, and their detection results are compared. Assuming the process is successful, \( d(s') \neq 0 \). For any sample \( s \), the strategy is to minimize \( \delta(·) \) or its estimation \( \hat{\delta}(·) \) to achieve an adversarial sample \( s' \) based on the following expression:

\[
\hat{\delta}(·) \quad \text{s.t.} \quad d(s, s') \leq d_{\text{max}}
\]
\[
e = \arg \min_s
\]

We selected gradient descent as the technique to approach this problem. Despite its good performance, local optimization can be prone to failure due to the nature of the discriminant function. However, given our large training dataset, we expect to avoid such issues. All attacks performed adhere to the \( d_{\text{max}} \) constraint, ensuring that the transformations created always respect the maximum distance: \( d : S \times S \to \mathbb{R}^+ \). The correct distance requires domain knowledge and, in the PE domain, must properly reflect the limitations of the perturbations.

### 4. Solution

We propose a solution for generating valid Portable Executable (PE) files that can evade malware classification (Figure 2). Windows portable executables are widely used by adversaries to perpetrate attacks, making them one of the most important infection vectors. However, manipulating these files through actions like removing objects can quickly lead to corruption. Therefore, we implement perturbations only as additions or modifications to the PE.

The algorithm returns a score for each perturbation and calculates the gradient value for the next iteration. This approach allows us to efficiently determine the best evasive sequence vector of perturbations that need to be injected into a PE file to achieve evasion. An important aspect is that the functionality is preserved after the process, and the system outputs only valid modified versions of the input malware.

Most existing approaches work with black-box classifiers, which provides a realistic scenario from an adversary's perspective but prevents us from understanding how the classifier responds to specific transformations. Conversely, attacks on white-box models do not typically produce real files. Therefore, our approach is not only useful for generating better and faster adversarial examples but also for understanding the weaknesses of the model, thereby enabling the development of more robust classifiers for malware detection. Additionally, the valid PE files generated can be used for retraining the model to determine the improvement ratio when the classifier learns from the adversarial examples, and to evaluate cross-evasion capabilities.

### 5. Conclusion

In this work, we proposed a method for generating valid adversarial malware examples against Convolutional Neural Networks (CNNs) using gradient information. By implementing attacks on the EMBER dataset, we determined that using gradients to find optimal evasions is a promising strategy to improve adversarial attacks on real malware. Our initial experiments showed that we can use this information to find optimal sequences of transformations without rendering the malware sample corrupt.

### 6. Future Work

Future work aims to improve the efficiency of the system by using another neural network to estimate the distribution instead of extracting features and calculating the gradient for each malware sample.

### 7. Acknowledgments

The authors would like to thank the Chair for Communication Systems and Network Security, as well as the Research Institute CODE, for their comments and improvements. This research was supported, in part, by the EU H2020 Project CONCORDIA GA 830927.

### References

[1] Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, and Phil Roth. 2018. Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning. CoRR abs/1801.08917 (2018). arXiv:1801.08917 http://arxiv.org/abs/1801.08917

[2] Hyrum S. Anderson and Phil Roth. 2018. EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models. CoRR abs/1804.04637 (2018). arXiv:1804.04637 http://arxiv.org/abs/1804.04637

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases. Springer, Heidelberg, Germany, 387–402.

[4] Bojan Kolosnjaji, Apostolis Zarras, George Webster, and Claudia Eckert. 2016. Deep learning for classification of malware system call sequences. In Australasian Conference on Artificial Intelligence. Springer, Heidelberg, Germany, 137–149.

[5] Raphael Labaca Castro, Corinna Schmitt, and Gabi Dreo Rodosek. 2019. AIMED: Evolving Malware with Genetic Programming to Evade Detection. In 2019 18th IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom). IEEE, New York, NY, USA, 1–x, Paper presented on Aug 5, 2019.

[6] Raphael Labaca Castro, Corinna Schmitt, and Gabi Dreo Rodosek. 2019. ARMED: How Automatic Malware Modifications Can Evade Static Detection?. In 5th International Conference on Information Management (ICIM). IEEE, New York, NY, USA, 20–27.

[7] Razvan Pascanu, Jack W Stokes, Hermineh Sanossian, Mady Marinescu, and Anil Thomas. 2015. Malware classification with recurrent networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, New York, NY, USA, 1916–1920.

[8] Subhojeet Pramanik. 2018. Malware detection using Convolutional Neural Networks. https://github.com/subho406/Malware-detection-using-Convolutional-Neural-Networks.

[9] Shun Tobiyama, Yukiko Yamaguchi, Hajime Shimada, Tomonori Ikuse, and Takeshi Yagi. 2016. Malware detection with deep neural network using process behavior. In 2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC), Vol. 2. IEEE, New York, NY, USA, 577–582.

[10] Wei Wang, Ming Zhu, Xuewen Zeng, Xiaozhou Ye, and Yiqiang Sheng. 2017. Malware traffic classification using convolutional neural network for representation learning. In 2017 International Conference on Information Networking (ICOIN). IEEE, New York, NY, USA, 712–717.