eight groups of them, which include parsed and agnostic-format
features. The former focuses on general and header information
of the file as well as imported and exported functions whereas the
latter refers to string information and byte-histograms [2]. The
performance of the malware classifier after being trained on the
dataset is displayed on Fig. 2 along with the train loss. We report the
results calculated for 40 epochs and used the area under the receiver
operating characteristic curve (ROC AUC) metric to measure the
rate of successful classification by the CNN as it makes easier for
further comparison with other models. The reported performance
consists of a f-score of 94% at 5% false positive and 7% false negative
rates. Additional tuning can still be performed to improve, mostly,
the true positive rates when comparing to the performance of the
Figure 1: Workflow to generate adversarial examples [6]
adversarial examples against further classifiers as well as to further
stabilize the loss function output.
3.1 Perturbation Injection
We implement, as depicted in Fig. 1, an internal module to generate
malicious adversarial samples that are able to evade the targeted
malware classifier [6]. During Step 1 a malware sample s is sent to
the manipulation box where a sequence of defined perturbations
will be injected. Once the process is done, it will generate a modified
sample s′ and it proceeds to the next step to check whether the
file is still valid. In Step 2 a sandbox will be used to dynamically
test the file. In case s′ is corrupt, the malware is dismissed and
the process is restarted. Otherwise, it triggers Step 3, where the
sample is checked for detection. Both s and s′ are sent to Step 4 and
its detection results will be compared. Assuming the process was
successful, d(s′)  0. For any sample s, the strategy is to minimize д(·) or the
estimation of it ˆд(·) in order to achieve an adversarial sample s′
based on the following expression:
ˆд(·) s.t . d(s, s
′) ≤ dmax
e = arg min
s
(1)
We selected gradient descent as the technique to approach this
problem. Despite of its good performance, it is also known that
optimizing locally can be prone to failure due to the nature of the
discriminant function. However, given our rather large training
dataset we expect to avoid such issues. All the attacks performed
have the dmax constraint as the transformations created must al-
ways respect the maximum distance: d : S ×S (cid:55)−→ R+. The correct
distance requires domain knowledge and, in the PE domain, needs
to properly reflect the limitations of the perturbations.
4 SOLUTION
We propose a solution based on generating valid PE files that are
able to evade malware classification (Fig. 2). Windows portable
executables are widely used by adversaries in order to perpetrate at-
tacks and are therefore one of the most important infection vectors.
Nevertheless, manipulating them with actions such as removing
objects can quickly lead to corrupt files. Thus, we implement per-
turbations only as additions or modifications to the PE.
The algorithm returns a score for each of the perturbations and
calculate the value of the gradient for the next iteration. This ap-
proach leads us to calculate in a much more efficient way the best
evasive sequence vector of perturbations that needs to be injected
into a PE file in order to achieve an evasion. An important aspect
is that the functionality can be preserved after the process and the
system only outputs valid modified versions of the input malware.
Until now, most of the approaches mentioned worked with black-
box classifiers, which brings us a realistic scenario from an adver-
sary perspective but it prevents us to understand how the classifier
responds specifically to each of the transformations. Conversely,
attacks on white-box models do not normally produce real files.
Therefore, our approach can be useful to not only achieve better
and faster adversarial examples but also to further understand the
weaknesses of the model in order to build more robust classifiers
for malware detection. Furthermore, the valid PE files generated
can be used for additional purposes. First, for retraining the model
in order to determine what is the ratio of improvement when the
classifier is able to learn from the adversarial examples generated
to evade it. Second, to evaluate cross-evasion capabilities of the
adversarial examples.
5 CONCLUSION
In this work, we proposed the ability of generating valid adver-
sarial malware examples against convolutional neural networks
using gradient information. By implementing attacks on the EM-
BER dataset, we determined that using the gradient to find optimal
evasions is a promising strategy to improve adversarial attacks on
real malware. Our initial experiments showed that we are able to
use that information to find optimal sequences of transformations
without rendering the malware sample corrupt.
As future work, we aim to improve the efficiency of the system by
using another neural network to estimate the distribution instead
Figure 2: ROC AUC & training loss reported by classifier
of extracting the features and calculating the gradient for each
malware sample.
6 ACKNOWLEDGMENTS
The authors would like to thank the Chair for Communication
Systems and Network Security as well as the Research Institute
CODE for their comments and improvements. Research supported,
in parts, by EU H2020 Project CONCORDIA GA 830927.
REFERENCES
[1] Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, and Phil Roth.
2018. Learning to Evade Static PE Machine Learning Malware Models via Re-
inforcement Learning. CoRR abs/1801.08917 (2018). arXiv:1801.08917 http:
//arxiv.org/abs/1801.08917
[2] Hyrum S. Anderson and Phil Roth. 2018. EMBER: An Open Dataset for Train-
ing Static PE Malware Machine Learning Models. CoRR abs/1804.04637 (2018).
arXiv:1804.04637 http://arxiv.org/abs/1804.04637
[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks against
machine learning at test time. In Joint European conference on machine learning
and knowledge discovery in databases. Springer, Heidelberg, Germany, 387–402.
[4] Bojan Kolosnjaji, Apostolis Zarras, George Webster, and Claudia Eckert. 2016.
Deep learning for classification of malware system call sequences. In Australasian
Conference on Artificial Intelligence. Springer, Heidelberg, Germany, 137–149.
[5] Raphael Labaca Castro, Corinna Schmitt, and Gabi Dreo Rodosek. 2019. AIMED:
Evolving Malware with Genetic Programming to Evade Detection. In 2019 18th
IEEE International Conference on Trust, Security and Privacy in Computing and
Communications (TrustCom). IEEE, New York, NY, USA, 1–x, Paper presented on
Aug 5, 2019.
[6] Raphael Labaca Castro, Corinna Schmitt, and Gabi Dreo Rodosek. 2019. ARMED:
How Automatic Malware Modifications Can Evade Static Detection?. In 5th
International Conference on Information Management (ICIM). IEEE, New York, NY,
USA, 20–27.
[7] Razvan Pascanu, Jack W Stokes, Hermineh Sanossian, Mady Marinescu, and Anil
Thomas. 2015. Malware classification with recurrent networks. In 2015 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
New York, NY, USA, 1916–1920.
[8] Subhojeet Pramanik. 2018. Malware detection using Convolutional Neural Net-
works. https://github.com/subho406/Malware-detection-using-Convolutional-
Neural-Networks.
[9] Shun Tobiyama, Yukiko Yamaguchi, Hajime Shimada, Tomonori Ikuse, and
Takeshi Yagi. 2016. Malware detection with deep neural network using pro-
cess behavior. In 2016 IEEE 40th Annual Computer Software and Applications
Conference (COMPSAC), Vol. 2. IEEE, New York, NY, USA, 577–582.
[10] Wei Wang, Ming Zhu, Xuewen Zeng, Xiaozhou Ye, and Yiqiang Sheng. 2017. Mal-
ware traffic classification using convolutional neural network for representation
learning. In 2017 International Conference on Information Networking (ICOIN).
IEEE, New York, NY, USA, 712–717.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2567