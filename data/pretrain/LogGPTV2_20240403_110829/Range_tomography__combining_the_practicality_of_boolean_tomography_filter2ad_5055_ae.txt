dian one-way delay of the last 1000 probing packets before
the kth packet). A higher value of M indicates heavier con-
gestion in that path. Figure 7b shows the congestion mag-
nitude distribution in the PlanetLab network. The average
delay increase during half of the congestion events is less
l − k
(7)
9The base delay of a path at time t is set to the median of
the one-way delays among the last 100 packets received in
that path before t.
10Note that we have ignored cases where all packets sent (or
received) over every path to (or from) a speciﬁc host are
dropped because the problem then can be easily associated
with the corresponding sensor and its local-area network.
394(a) CDF of congestion duration
(b) CDF of congestion magnitude
(c) Congestion magnitude vs. congestion dura-
tion
Figure 7: Results of the detection process in PlanetLab.
than 20ms, and it does not exceed 50ms in 90% of those
events. The distribution of M in the other two networks
has the same trend, but the maximum value of M is at
most 60ms there. Finally, Figure 7c shows the magnitude
of congestion events versus their duration.
It is interest-
ing to see that heavier congestion events usually last for
shorter durations, emphasizing again that a practical to-
mography method should be able to work with short-term
measurements.
It should be noted here that network to-
mography methods cannot localize congestion events that
are very short in duration because it is hard to reliably de-
tect such events, and further to estimate their magnitude,
with active measurements. For instance, even if the probing
frequency is 10 packets per second, a congestion event that
lasts for three seconds introducing a loss rate of 1% would
probably not be observed by the probing stream. However,
because the Range Tomography methods do not require a
long measurement history or precise estimates of path per-
formance, they are able to localize congestion events that
last for at least few tens of seconds, as shown in the follow-
ing results.
6.3 Localization results
We run the Sum-Tomo algorithm on the detected conges-
tion events to localize congested links.
Ideally, we expect
that a congested link causes the same congestion event (in
terms of duration and congestion magnitude) to paths that
traverse that link. However, these congestion periods do
not exactly overlap in practice due to lack of perfect clock
synchronization in the measurement hosts. Therefore, we
consider the time interval that covers all overlapping con-
gestion periods as the designated congestion period that the
tomography method analyzes.
We then compute the congestion magnitude M in every
path during that designated period. To distinguish good
paths from bad paths, we set the threshold δ as the minimum
congestion magnitude over all detected congestion events in
that network (the value of δ is around 2ms in practice). The
α parameter is determined based on Method-2, introduced
in Section 5.1. This process results in α=2 in Internet2 and
ESNet, and α=1.2 in PlanetLab.
In our experiments, we
ignore paths containing links with unknown IP addresses
(only happens in the PlanetLab data set).
It is not possible to directly validate the results of the
Sum-Tomo algorithm because we do not have access to the
routers and switches along each path. Instead, we use the in-
direct validation method introduced in [19]. In this method,
the measured paths in each period are divided in two groups,
inference paths and validation paths. Each link should be
included in both sets. The inference paths are then used
to identify congested links and to estimate their congestion
magnitude range using the Sum-Tomo algorithm. Then, we
use the validation paths to examine if the ranges that were
computed based on the inference paths are consistent with
the measured congestion magnitude of the validation paths.
Finally, we measure the validation error as the fraction of
bad links in which the estimated congestion magnitude range
395l
)
s
m
n
i
(
y
a
e
D
y
a
W
e
n
O
 100
 80
 60
 40
 20
 0
 0
 5
 10
 15
 20
 25
l
)
s
m
n
i
(
y
a
e
D
y
a
W
e
n
O
 100
 80
 60
 40
 20
 0
 0
 5
 10
 15
 20
 25
l
)
s
m
n
i
(
y
a
e
D
y
a
W
e
n
O
 100
 80
 60
 40
 20
 0
 0
 5
 10
 15
 20
 25
Time (in seconds)
Time (in seconds)
Time (in seconds)
(a) Delay timeseries of a path traversing a
congested link l1.
(b) Delay timeseries of a path traversing
a congested link l2.
(c) Delay timeseries of a path traversing
both congested links l1 and l2.
Figure 8: The timeseries of one-way packet delays measured in three paths during a congestion event in Internet2. The
delays are relative to the base delay in that path (i.e., they represent one-way delay increase due to queueing). There are two
congested links during this episode. Paths in (a) and (b) traverse two diﬀerent congested links, but the path in (c) traverses
both links. Note that the sum of the delay increase in (a) and (b) results to roughly the same delay increase we observe in
(c).
for a link is not consistent with the measured congestion
magnitude in the corresponding validation path.
Overall, we analyzed 254 congestion events in Internet2,
336 events in ESNet, and 159 events in PlanetLab. In almost
all events, only one congested link was identiﬁed as the root
cause of the congestion event. Speciﬁcally, more than 97%
of congested paths in all three networks include just one
congested link, and only 0.5% of paths contain more than
two congested links (we did not ﬁnd any path with four
congested links in our data sets).
The validation error in all three data sets was zero.
In
addition, the time-series of one-way delay variations in the
inference and validation paths are highly correlated in the
time domain when the corresponding congested paths tra-
verse the same congested link. For instance, the time-series
in Figure 8 show an interesting example of a congestion event
in Internet2 that involves two congested links. The conges-
tion magnitude range assigned to links is mostly below 70ms
in PlanetLab, and mostly less than 40ms in Internet2 and
ESNet.
7. RELATED WORK
We refer the reader to a thorough review [10] for a cover-
age of the prior work until 2004.
In Analog tomography, a typically under-constrained lin-
ear system of equations models the relation between path
and link parameters (assuming that the topology is known).
Techniques from parametric or non-parametric statistical
inference, jointly with additional constraints, assumptions
and optimization objectives, are then used to infer the most
likely values of the link parameters from the measured path
parameters [10, 11]. For instance, Shavitt et al.
[12] es-
timate link delays using a least-squares method, Bu et al.
[13] use expectation-maximization to infer link loss rates,
while Chen et al.
[14] use Fourier domain analysis to infer
link delays. NetScope [15] is a recent method that estimates
link loss rates also considering the observed loss rate vari-
ances. Ghita et al. study the case that diﬀerent links can
be correlated (all other related work assumes independent
links) [16]. The application of Analog tomography in prac-
tice has been rather limited for several reasons. First, they
require accurate end-to-end path measurements, which are
hard to get in short timescales and without intrusive prob-
ing. Second, some link-level parameters may not be sta-
tistically identiﬁable, meaning that diﬀerent assignments of
link-level metrics produce the same statistical distribution
of path measurements [5, 6, 7]. Third, Analog methods can
be computationally intensive [7].
Duﬃeld introduced the Boolean tomography framework
in 2003 [5, 7], while Nguyen and Thiran compared Analog
with Boolean tomography [17]. NetDiagnoser [8] extends
Boolean tomography to multiple sources and destinations.
Kompella et al.
[18] consider a similar approach to detect
“silent failures” in MPLS/IP networks using active measure-
ments between edge routers. Nguyen and Thiran introduce
a Boolean method to infer link state probabilities from mul-
tiple measurements over time, and then using those proba-
bilities to identify congested links [6]. Bayesian approaches
to infer lossy links have also been proposed [19]. The au-
thors in [20] use prior link state probabilities to diagnose the
underlying cause behind the faulty state of links. Barford et
al. have proposed a framework to detect and localize per-
formance anomalies using active probe-based measurements
[21]. To reduce probing overhead, they give an algorithm to
select the paths that should be probed at any point in time.
8. CONCLUSIONS
We proposed a new tomography framework that combines
the best features of Analog and Boolean tomography. Range
tomography estimates a range estimate for each bad link, in-
stead of aiming to infer a point estimate or a binary estimate
for every link. We applied the range tomography framework
in two path performance metric functions (Min and Sum)
and presented an eﬃcient heuristic for each function. The
Min-Tomo algorithm considers only the lowest-performance
link over a path, while the Sum-Tomo algorithm considers
the sum of the performance metrics for all bad links in that
path.
Simulation results show that the proposed tomography
method performs much better than earlier Boolean and Ana-
log techniques in terms of precision, recall and accuracy. For
instance, Sum-Tomo generates up to 35% less false positives
396than the Analog Norm-minimization method, while its false
negative error is less than the Boolean Tomo method by
up to 15%. Moreover, the accuracy of the resulting range
estimates is always high (more than 93%).
We have also applied the Sum-Tomo method in three op-
erational networks to detect and localize congested links and
to estimate their congestion magnitude. According to an in-
direct validation method, the resulting link range estimate
is consistent with the measured path congestion magnitude
in every congestion event we have analyzed. The experimen-
tal results also emphasize that congestion events in Internet
paths are often short-lived, and so any practical tomography
methods should be accurate even if the path measurements
result from few probes and they are error-prone. Finally,
at least for the networks we measured in this study, we of-
ten see only one bad link during any congestion event, and
rarely more than 2-3 bad links.
Acknowledgements
We are grateful to Partha Kanuparthy for his help with the
detection logic and data management. We also thank Ja-
son Zurawski from Internet2, and Joe Metzger, Brian Tier-
ney and Andrew Lake from ESnet for providing us with the
OWAMP data sets. We are also grateful to the anonymous
reviewers and our “shepherd”, Matthias Grossglauser, for
their constructive comments.
This research was supported by the U.S. Department of
Energy under grant number DE-FG02-10ER26021.
9. REFERENCES
[1] R. Caceres, N.G. Duﬃeld, J. Horowitz, D. Towsley.
Multicast-Based Inference of Network Internal Loss
Characteristics. IEEE Trans. on Information Theory,
45(7), 2462-2480, 1999.
[2] M. Coates, R. Nowak. Network loss inference using
unicast end-to-end measurement, In Proc. ITC Conf.
IP Traﬃc, Modeling and Management, 2000.
[3] Y. Zhang, N. Duﬃeld, V. Paxson, S. Shenker. On the
Constancy of Internet Path Properties. In ACM
SIGCOMM Workshop on Internet Measurement, 2001.
[4] M. Roughan. Fundamental Bounds on the Accuracy of
Network Performance Measurements. In ACM
SIGMETRICS, 2005.
[5] N. Duﬃeld. Simple network performance tomography.
In Proc. ACM IMC, 2003.
[6] H. Nguyen, and P. Thiran. The Boolean Solution to
the Congested IP Link Location Problem: Theory and
Practice. In Proc. IEEE INFOCOM, 2007.
[7] N. Duﬃeld. Network Tomography of Binary Network
Performance Characteristics. In IEEE Trans.
Information Theory, 52, 2006.
[8] A. Dhamdhere, R. Teixeira, C. Dovrolis, C. Diot.
NetDiagnoser: Troubleshooting network
unreachabilities using end-to-end probes and routing
data. In Proc. ACM CoNEXT, 2007.
[9] M. R. Garey, and D. S. Johnson. Computers and
Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman and Co., 1979.
[10] R. Castro, M. Coates, G. Liang, R. Nowak, B. Yu.
Network Tomography: Recent Developments.
Statistical Science, 19(3):499-517, 2004.
[11] R. Caceres, N. Duﬃeld, S. Moon, D. Towsley.
Inference of Internal Loss Rates in the MBone. In
Proc. IEEE Global Internet, 1999.
[12] Y. Shavitt, X. Sun, A. Wool, B. Yener. Computing the
unmeasured: An algebraic approach to internet
mapping. In Proc. IEEE INFOCOM, 2001.
[13] T. Bu, N. Duﬃeld, F. L. Presti, D. Towsley. Network
tomography on general topologies. In Proc. ACM
SIGMETRICS, 2002.
[14] A. Chen, J. Cao, T. Bu. Network tomography:
Identiﬁability and fourier domain estimation. In Proc.
IEEE INFOCOM, 2007.
[15] D. Ghita, H. Nguyen, M. Kurant, K. Argyraki, P.
Thiran. Netscope: Practical Network Loss
Tomography. In Proc. IEEE INFOCOM, 2010.
[16] D. Ghita, K. Argyraki, P Thiran. Network
Tomography on Correlated Links. In Proc. ACM IMC,
2010.
[17] H. X. Nguyen, and P. Thiran. Binary versus analogue
path monitoring in IP networks. In LNCS, Vol. 3431,
Jan. 2005, p. 97.
[18] R. R. Kompella, J. Yates, A. Greenberg, A. C.
Snoeren. Detection and Localization of Network
Blackholes. In Proc. IEEE INFOCOM, 2007.
[19] V. Padmanabhan, L. Qiu, H. Wang. Server-based
Inference of Internet Link lossiness. In Proc. IEEE
INFOCOM, 2003.
[20] S. Kandula, D. Katabi, J.-P. Vasseur. Shrink: A Tool
for Failure Diagnosis in IP Networks. In Proc. ACM
SIGCOMM MineNet Workshop, 2005.
[21] P. Barford, N. Duﬃeld, A. Ron, and J. Sommers.
Network Performance Anomaly Detection and
Localization. In Proc. IEEE INFOCOM, 2009.
[22] B. Augustin et al. Avoiding traceroute anomalies with
Paris traceroute, In Proc. ACM IMC, 2006.
[23] H. H. Song, L. Qiu, Y. Zhang. NetQuest: A Flexible
Framework for Large-Scale Network Measurement. In
Proc. ACM SIGMETRICS, 2006.
[24] H. X. Nguyen, and P. Thiran. Network Loss Inference
with Second Order Statistics of End-to-End Flows. In
Proc. ACM IMC, 2007.
[25] Y. Zhao, Y. Chen, D. Bindel, Towards Unbiased
End-to-End Network Diagnosis, In Proc. ACM
SIGCOMM, 2006.
[26] M. Luckie, A. Dhamdhere, K. Claﬀy, D. Murrell.
Measured impact of crooked traceroute. In ACM
SIGCOMM CCR, Vol. 41, Issue 1, 2011.
397