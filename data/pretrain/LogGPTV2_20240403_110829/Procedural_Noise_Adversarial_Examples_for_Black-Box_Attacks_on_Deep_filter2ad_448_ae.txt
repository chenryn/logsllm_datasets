BanditsTD
BanditsTD
BanditsTD
(cid:139)ery Limit
Average (cid:139)eries
Success Rate (%)
100
1,000
100
1,000
1,000
100
1,000
10,000
7.0
8.4
19.8
70.1
36.3a
29.0
224
888
91.6
92.8
71.7
86.5
91.6
36.7
73.6
96.9
aFor each input, we divide the total number of perturbations by the number of those
that evade that input to get an expected number of queries.
improves on the query e(cid:129)ciency over random parameters by 5-
fold whilst a(cid:138)aining the same accuracy. (cid:140)e improvement for
Bayesian optimization increasing the query limit from 100 to 1,000
was very incremental. (cid:140)is suggests that the limited codomain of
the procedural noise function is se(cid:138)ing an upper bound on the
a(cid:138)ack’s success rate.
L-BFGS performed the worst as we observe it can get trapped and
waste queries at poor local optima, which requires several restarts
from di(cid:130)erent initial points to improve its performance. (cid:140)ere were
similar trends for Gabor noise where Bayesian optimization had
78.3% success with 9.8 queries on average. For both procedural
noise functions, Bayesian optimization improved the average query
e(cid:129)ciency over L-BFGS and random parameter selection by up to 7
times while retaining a success rate greater than 83%. We include
the other results in Appendix D.
Comparison. We compare our results with Ilyas et al. [25],
where they formalize their a(cid:138)ack as a gradient estimation problem
(like in white-box gradient-based a(cid:138)acks), and use a bandit opti-
mization framework to solve it. We test the bandits a(cid:138)ack on the
same model, dataset, and (cid:96)∞-norm for maximum query limits of
100, 1,000, and 10,000.
(cid:140)e results show that the evasion rate of our input-speci(cid:128)c proce-
dural noise a(cid:138)ack greatly outperforms the bandits a(cid:138)ack when the
query limit per image is a thousand or less. For signi(cid:128)cantly larger
query limits, the Perlin noise Bayesian optimization a(cid:138)ack has a
competitive success rate at a drastically be(cid:138)er query e(cid:129)ciency–
needing 100 times less queries on average.
(cid:140)e procedural noise a(cid:138)ack is optimized for universal evasion
and a restrictively small amount of queries. Most existing meth-
ods explore the entire image space which has a hundred thousand
dimensions, and some a(cid:138)acks reduce their search space with tech-
niques like tiling, but the dimensionality is still much larger than
the four parameters of our procedural noise. Other input-speci(cid:128)c
black-box a(cid:138)acks [3, 6, 12] require tens to hundreds of thousands
of queries on realistic natural-image datasets, which makes them
ine(cid:129)cient, but almost certain to (cid:128)nd an adversarial example given
enough queries. (cid:140)e bandits method makes a small sacri(cid:128)ce in
success rate for more query e(cid:129)ciency, and our procedural noise
a(cid:138)ack takes it further for greater query-e(cid:129)ciency.
Procedural noise perturbations have naturally high evasion rates,
although the expressiveness of our chosen functions can be less
than a(cid:138)acks whose larger codomains can capture more kinds of
adversarial perturbations. (cid:140)ese other a(cid:138)acks make this trade-
o(cid:130) by sacri(cid:128)cing e(cid:129)ciency in black-box scenarios, as the number
of queries they need to cra(cid:137) successful adversarial examples is
large. On the other hand, we can increase the expressiveness of our
procedural noise functions by introducing more parameters by, for
example, using di(cid:130)erent colour maps. However, this may come at
the cost of its naturally high universal evasion.
Summary. Black-box optimization techniques like Bayesian
optimization are able to take advantage of the drastically reduced
search space of procedural noise. Together with its naturally high
universal evasion, we show that procedural noise gives rise to
inexpensive yet potent untargeted black-box a(cid:138)acks, whether it
be input-speci(cid:128)c or universal. It is also shown to be competitive
with existing a(cid:138)acks that o(cid:137)en require orders of magnitude more
queries or resources.
6 OTHER APPLICATIONS
In this section, we show how procedural noise a(cid:138)acks extend to
object detection DCNs by a(cid:138)acking YOLO v3 model. We then
discuss how exploiting the sensitivity of DCNs to low-level features,
like in procedural noise a(cid:138)acks, can be generalized to cra(cid:137) universal
a(cid:138)acks for DCNs in other application domains.
6.1 Attacking Object Detection
Object detection requires the identi(cid:128)cation of locations and classes
of objects within an image. “Single Shot Detectors” (SSD) such as
the di(cid:130)erent versions of YOLO [58, 59] generate their predictions
a(cid:137)er a single pass over the input image. (cid:140)is allows SSDs to process
images in real-time speeds while maintaining high accuracy. Other
types of object detectors such as Fast R-CNN [15] and Faster R-CNN
[61] are based on region proposals. (cid:140)ey have comparable accuracy
but do not achieve the same image processing speed as SSDs. We
test our a(cid:138)ack against YOLO v3 [60], the latest iteration of YOLO,
which achieves high accuracy and detects objects in real time.
Metrics. We use precision, recall, and mean average precision
(mAP) as our primary metrics; mAP is frequently used when com-
paring the overall performance of object detectors. In object detec-
tion the classi(cid:128)er predicts a bounding box to identify the location
of an object and assigns that box a class label. (cid:140)e Intersection Over
Union (IOU) is the ratio of the intersection area over the union
area between the predicted and ground truth bounding boxes. A
threshold is usually set to determine what IOU constitutes a posi-
tive classi(cid:128)cation. True positives occur when the IOU is larger than
the threshold and the class is identi(cid:128)ed correctly. False negatives
occur when the threshold is not met with the correct class for a
ground truth bounding box. False positives occur when the IOU
is less than the threshold, there are no intersecting ground truth
boxes, or there are duplicate predicted bounding boxes.
Experiment. We use the MS COCO dataset [36] which contains
80 di(cid:130)erent classes, with a large proportion of targets being the
“person” class. (cid:140)is has become one of the benchmark datasets for
object detection. We use standard se(cid:138)ings as described in [60], with
input dimensions 416 × 416 × 3 and an IOU threshold of 0.5.
10
We use (cid:96)∞-norm ε = 16 and apply each of our procedural noise
perturbations on the 1,000 random images from the validation set.
(cid:140)e perturbations generated are from 1,000 Gabor noise, 1,000 Perlin
noise, and 1,000 (cid:96)∞-optimized uniform random perturbations. (cid:140)e
parameters are chosen uniformly at random so that we can analyze
the results, as in Sect. 4. (cid:140)is is a universal untargeted black-box
a(cid:138)ack.
Figure 3: Histogram of metrics over all perturbations on
YOLO v3. Results for (top) all classes and (bottom) the “per-
son” class.
Results. (cid:140)e procedural noise perturbations signi(cid:128)cantly de-
crease the mAP of the model, with 75% of Perlin noise perturbations
halving the mAP or worse. (cid:140)e precision and recall also decreased in
the presence of our perturbations. Again, Perlin noise outperforms
Gabor noise on average but Gabor noise has a smaller minimum,
with at least one perturbation causing 0.09 mAP. Compared to uni-
form random noise, which maintained around 0.34 mAP, both Perlin
and Gabor noise had larger impact on the model’s performance.
We represent uniform random noise using the median (which, in
this case, is very close to the mean) on each metric as the variance
is very small (less than 10−5).
In Fig. 3, for the metrics across all classes, we can observe that the
precision was maintained or decreased while the recall and mAP
decreased. Some perturbations decreased all three metrics, which
indicate that these cause both false positives and false negatives
consistently. When the perturbation increases the precision while
decreasing the recall and mAP, then the noise is likely masking
objects rather than introducing new objects to the image. (cid:140)is
masking e(cid:130)ect has serious implications for security applications
like surveillance and autonomous vehicles.
We focus on the “person” class as it constitutes a majority of
the targets and is semantically meaningful in the context of some
applications. In Fig. 3, metrics for “person” follow a similar trend
to the metrics across all classes. However, the increase in precision
is very small (¡0.10) compared to the large drops in recall and mAP
caused by Perlin noise. (cid:140)e higher precision indicates fewer false
positives, while the decrease in recall indicates more false negatives.
(cid:140)is indicates that the classi(cid:128)er is making fewer predictions overall,
which means that the noise is masking persons in the image.
Whilst for the most relevant “person” class, our procedural noise
appears to have an obfuscating e(cid:130)ect, for other classes like “zebra”
all the metrics decrease – indicating that there are many false
positive and false negatives. However, for three classes, “backpack”,
“book”, and “toaster”, all three metrics improve. (cid:140)ese labels did not
have as much representation in the test set which may explain this
anomalous result.
(cid:140)e frequency of the sine ϕsine for Perlin noise had the largest
inverse correlation of less than -0.72 with each of the three metrics.
(cid:140)is means that high-frequency pa(cid:138)erns decrease the model’s per-
formance metrics, similarly to what we have observed for the image
classi(cid:128)ers. (cid:140)e thickness λ of Gabor noise is moderately correlated
at 0.4 with the precision, but not the recall or mAP. (cid:140)is suggests
that thicker Gabor noise perturbations decrease the number of false
positives relative to the other perturbations.
Discussion. Object detection is a more complex task than image
classi(cid:128)cation as it has to identify multiple objects and their locations
within an image. Although YOLO v3 has a di(cid:130)erent architecture,
task, and dataset from the ImageNet classi(cid:128)ers, we see that the same
procedural noise perturbations are still able to greatly degrade its
performance. (cid:140)is shows that it is more likely caused by the models’
sensitivity towards perturbations rather than a texture bias, as the
object detection task has a spatial component to it. (cid:140)is suggests
that our procedural noise a(cid:138)acks may generalize to other DCNs on
computer vision tasks with natural-image data.
In our discussion in Sect. 4.4, we hypothesized that the proce-
dural noise perturbations are an aggregation of low-level features at
high frequencies that the DCNs strongly respond to. (cid:140)e prior that
convolutional layers induce and similarities across natural-image
datasets may be why DCNs are sensitive to these noise pa(cid:138)erns.
Additionally, like in Sect. 5, we can also create more e(cid:129)cient black-
box a(cid:138)acks by applying black-box optimization techniques such as
Bayesian optimization to enhance the a(cid:138)ack against object detec-
tors. When using Bayesian optimization, an a(cid:138)acker can focus on
minimizing a speci(cid:128)c metric (precision, recall, mAP, or F1 score).
6.2 Beyond Images
One of the main strengths of procedural noise is that it allows to
describe a distribution of UAPs with only a few parameters. (cid:140)is
compressed representation allows for an e(cid:129)cient generation of
UAPs both for undermining a machine learning service or, defen-
sively, for testing the robustness of a model. In practice, compressed
representations can be learned by generative models such as GANs
[46, 77] or Variational Autoencoders (VAEs), however these incur
additional training, calibration, and maintenance costs. Training
algorithms or defences that incorporate domain-speci(cid:128)c knowledge
may be needed to mitigate the sensitivity towards a(cid:138)acks that make
use of these compact representations.
DCNs in other applications may also be vulnerable to aggrega-
tions of low-level features due to the use of convolutional layers.
Future a(cid:138)acks can exploit how DCNs rely on combining low-level
features rather than understanding the more di(cid:129)cult global fea-
tures in the input data. A natural next step would be to apply
these ideas in exploratory a(cid:138)acks or sensitivity analysis on sensory
applications like speech recognition and natural language process-
ing. To expand our procedural noise a(cid:138)ack framework to other
applications, it is worth identifying pa(cid:138)erns in existing adversarial
11
examples for domains like speech recognition [9, 11] and reinforce-
ment learning [24, 37] to (cid:128)nd analogues of procedural noise. As a
starting point, these pa(cid:138)erns can be found by (cid:128)nding perturbations
that maximize the hidden layer di(cid:130)erence as in the Singular Vector
A(cid:138)ack [30] or by using feature visualization on earlier layers of
DCNs to infer the low-level features that a model learns.
7 PRELIMINARY DEFENCE
(cid:140)e DCNs we tested are surprisingly fragile to procedural noise as
UAPs. Improving their robustness to adversarial examples is not a
straightforward task. A robust defence needs to defend not only
against existing a(cid:138)acks but also future a(cid:138)acks, and o(cid:137)en proposed
defences are shown to fail against new or existing a(cid:138)acks [8].
Among existing defences, adversarial training appears to be
more robust than others [1]. However, we have shown in Table 2
that ensemble adversarial training against gradient-based a(cid:138)acks
did not signi(cid:128)cantly diminish the input-speci(cid:128)c evasion rate of
our procedural noise a(cid:138)ack. (cid:140)is suggests that such defences do
not generalize well as the a(cid:138)acks used for adversarial training do
not su(cid:129)ciently represent the entire space of adversarial examples.
Training against all types of adversarial a(cid:138)acks would become com-
putationally expensive, especially for high-dimensional tasks like
ImageNet. (cid:140)us, defences that regularize the model or incorporate
domain-speci(cid:128)c knowledge may be more e(cid:129)cient strategies.
DCNs’ weakness to adversarial perturbations across inputs may
be a result of their sensitivity—small changes to the input cause
large changes to the output. (cid:140)us, input-agnostic defences that
minimize the impact of small perturbations may be e(cid:130)ective in
reducing the models’ sensitivity without the need to train against
all types of a(cid:138)acks. As a preliminary investigation, we brie(cid:131)y
explore here using denoising to defend against the universality of
our procedural noise perturbations.
Figure 4: Histogram of metrics over all perturbations on In-
ception v3 with and without median denoising for (le(cid:133)) Ga-
bor noise and (right) Perlin noise for (cid:96)∞-norm ε = 16.
7.1 Denoising
Denoising with spatial (cid:128)lters is a common pre-processing step
in signal processing and is thus an a(cid:138)ractive defence due to its
simplicity and pervasiveness across signal processing applications.
Median (cid:128)ltering is a smoothing operation that replaces each entry
with the median of its neighbouring entries, and it o(cid:137)en preserves
edges while removing noise. (cid:140)e idea is to smooth out the high-
frequency noise, which we know to be highly correlated with the
universal evasion rates for Perlin noise on Inception v3.