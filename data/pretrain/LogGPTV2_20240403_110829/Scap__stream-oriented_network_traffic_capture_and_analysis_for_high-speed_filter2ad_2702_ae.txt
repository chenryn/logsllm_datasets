To evaluate the packet delivery performance of Scap, we ran the
same application when Scap was conﬁgured with packet support,
and pattern matching was performed on the delivered packet pay-
loads. The results are shown in Figure 6 as well. We see that the
performance of Scap remains the same when the pattern match-
ing application operates on each packet, i.e., the percentages of
dropped packets and lost streams do not change. We just observe a
slight decrease in the number of successful matches, which is due
to missed matches for patterns spanning the payloads of multiple
successive packets.
6.6 Cutoff Points: Discarding Less Interest-
ing Packets Before It Is Too Late
Several network monitoring applications need to receive only the
initial part of each ﬂow [26, 33]. Other systems, such as Time Ma-
chine [27], elevate the ability to store only the beginning of each
ﬂow into one of their fundamental properties. In this experiment,
we set out to explore the effectiveness of Libnids, Snort, and Scap
when implementing cutoff points. For Snort, we modiﬁed Stream5
to discard packets from streams that exceed a given cutoff value.
Similarly, when the size of a stream reaches the cutoff value, Lib-
nids stops the collection of data for this stream. In Scap, we just
call the scap_set_cutoff() function in program’s preamble
using the desirable cutoff. We also compare Scap with and with-
out using FDIR ﬁlters. The applications search for the same set of
patterns as in the previous experiment.
Figures 8(a), 8(b), and 8(c) show the packet loss, CPU utiliza-
tion, and software interrupt load as a function of the cutoff for a
ﬁxed trafﬁc rate of 4 Gbit/s. Interestingly, even for a zero cutoff
size, i.e., when all data of each ﬂow is discarded, both Snort and
Libnids experience as much as 40% packet loss, as shown in the
left part of Figure 8(a). This is because Snort and Libnids ﬁrst
bring all packets to user space, and then discard the bytes they do
not need. Indeed, Figures 8(b) and 8(c) show that the total CPU uti-
lization of Libnids and Snort is always close to 100% at this trafﬁc
rate irrespectively of the cutoff point.
In contrast, for cutoff points smaller than 1MB, Scap has no
packet loss and very small CPU utilization. For instance, when
Scap uses a 10KB cutoff, the CPU load is reduced from 97% to
just 21.9%, as 97.6% of the total trafﬁc is efﬁciently discarded. At
the same time, 83.6% of the matches are still found, and no stream
is lost. This outcome demonstrates how the stream cutoff, when im-
plemented efﬁciently, can improve performance by cutting the long
tails of large ﬂows, and allows applications to keep monitoring the
ﬁrst bytes of each stream at high speeds. When the cutoff point in-
creases beyond 1MB, CPU utilization reaches saturation and even
Scap starts dropping packets. Enhancing Scap with hardware ﬁlters
reduces the software interrupt load, and thus reduces the packet loss
for cutoff values larger than 1MB.
6.7 Stream Priorities: Less Interesting Pack-
ets Are The First Ones To Go
To experimentally evaluate the effectiveness of Prioritized Packet
Loss (PPL), we ran the same pattern matching application using a
single worker thread while setting two priority classes. As an ex-
ample, we set a higher priority to all streams with source or desti-
nation port 80, which correspond to 8.4% of the total packets in our
trace. The rest of the streams have the same (low) priority. Figure 9
shows the percentage of dropped packets for high-priority and low-
priority streams as a function of the trafﬁc rate. When the trafﬁc
rate exceeds 1 Gbit/s, the single-threaded pattern matching appli-
cation cannot process all incoming trafﬁc, resulting in a fraction of
dropped packets that increases with higher trafﬁc rates. However,
we see that no high-priority packet is dropped for trafﬁc rates up
to 5.5 Gbit/s, while a signiﬁcant number of low-priority packets
are dropped at these rates—up to 85.7% at 5.5 Gbit/s. At the trafﬁc
rate of 6 Gbit/s, we see a small packet loss of 2.3% for high-priority
packets out of the total 81.5% of dropped packets.
6.8 Using Multiple CPU Cores
In all previous experiments the Scap application ran on a sin-
gle thread, to allow for a fair comparison with Snort and Libnids,
which are single-threaded. However, Scap is naturally parallel and
can easily use a larger number of cores. In this experiment, we ex-
plore how Scap scales with the number of cores. We use the same
pattern matching application as previously, without any cutoff, and
conﬁgure it to use from one up to eight worker threads. Our system
has eight cores, and each worker thread is pinned to one core.
Figure 10(a) shows the packet loss rate as a function of the num-
ber of worker threads, for three different trafﬁc rates. When using
a single thread, Scap processes about 1 Gbit/s of trafﬁc without
packet loss. When using seven threads, Scap processes all trafﬁc
at 4 Gbit/s with no packet loss. Figure 10(b) shows the maximum
100
)
%
(
d
e
p
p
o
r
d
s
t
e
k
c
a
P
80
60
40
20
0
0
Low−priority streams
High−priority streams
)
%
(
d
e
p
p
o
r
d
s
t
e
k
c
a
P
90
80
70
60
50
40
30
20
10
0
6 Gbit/s
4 Gbit/s
2 Gbit/s
1
2
3
4
5
6
Number of worker threads
Traffic rate (Gbit/s)
(a) Packet loss
1
2
3
4
5
6
7
8
)
s
/
t
i
b
G
(
e
t
a
r
e
e
r
f
−
s
s
o
l
x
a
M
6
5
4
3
2
1
0
1
2
3
4
5
6
7
8
Number of worker threads
(b) Speedup
Figure 9: Packet loss for high- and low-
priority streams, for varying trafﬁc rates.
Figure 10: Performance of an Scap pattern matching application for a varying num-
ber of worker threads.
loss-free rate achieved by the application as a function of the num-
ber of threads. We see that performance improves linearly with the
number of threads, starting at about 1 Gbit/s for one worker thread
and going all the way to 5.5 Gbit/s for eight threads.
The reason that we do not see a speedup of eight when using
eight worker threads is the following: even though we restrict the
user application to run on a limited number of cores, equal to the
number of worker threads, the operating system kernel runs always
on all the available cores of the processor. Therefore, when Scap
creates less than eight worker threads, it is only the user-level ap-
plication that runs on these cores. The underlying operating system
and Scap kernel module runs on all cores.
7. ANALYSIS
In this section, we analyze the performance of Prioritized Packet
Loss (PPL) under heavy load, aiming to explore at what point PPL
should start dropping low-priority packets so that high priority ones
do not have to be dropped. For simplicity lets assume that we have
two priorities: low and high. We deﬁne N to be (memory_size−
base_threshold)/2.
If the used memory exceeds N , then PPL
will start dropping low priority packets. Given that N is ﬁnite, we
would like to explore what is the probability that N will ﬁll up
and we will have to drop high-priority packets. To calculate this
probability we need to make a few more assumptions. Assume that
high-priority packet arrivals follow a Poisson distribution with a
rate of λ, and that queued packets are consumed by the user level
application. We assume that the service times for packets follow an
exponential distribution with parameter µ. Then, the whole system
can be modeled as an M/M/1/N queue. The probability that all
the memory will ﬁll up is:
Pf ull =
1 − ρ
1 − ρN +1 ρN
(1)
where ρ = λ/µ. Due to the PASTA property of the Poisson pro-
cesses, this is exactly the probability of packet loss: Ploss = Pf ull.
Figure 11 plots the packet loss probability for high-priority pack-
ets as a function of N . We see that a memory size of a few tens
of packet slots are enough to reduce the probability that a high-
priority packet is lost to 10−8. We note, however, that the speed
with which the probability is reduced depends on ρ: the fraction
of the high-priority packets over all trafﬁc which can be served by
the full capacity of the system. We see that when ρ is 0.1, that
is, when only 10% of the packets are high-priority ones, then less
than 10 slots are more than enough to guarantee that there will be
practically no packet loss. When ρ is 0.5 (i.e., 50% of the trafﬁc
is high-priority), then a little more than 20 packet slots are enough,
while when ρ is 0.9, then about 150 packet slots are enough.
The analysis can be extended to more priority levels as well. As-
sume, for example, that we have three priority levels: low, medium,
and high, that N = (memory_size − base_threshold)/3, that
medium-priority packet arrivals follow a Poisson distribution with
a rate of λ1, and that high-priority packet arrivals follow a Pois-
son distribution with a rate of λ2. As previously, assume that the
service times for packets follow an exponential distribution with
parameter µ. Then, the system can be described as a Markov chain
with 2N nodes:
λ1+λ2
λ1+λ2
λ1+λ2
'&%$
 !"#1
''&%$
 !"#2
. . .
µ
µ
µ
/.-,
()*+N
λ2
µ
. . .
λ2
µ
7654
01232N
The packet loss probability for high-priority packets is:
Ploss = ρN
1 ρN
2 p0
(2)
where ρ1 = (λ1 + λ2)/µ, ρ2 = λ2/µ, and
p0 = 1/(
1 − ρN +1
1
1 − ρ1
+ ρN/3
1
1 − ρN +1
2
1 − ρ2
)
The packet loss probability for medium-priority packets remains:
Ploss =
1 − ρ1
N +1 ρ1
1 − ρ1
N
(3)
Figure 12 plots the packet loss probability for high-priority and
medium-priority packets as a function of N. We assume that ρ1 =
ρ2 = 0.3. We see that a few tens of packet slots are enough to re-
duce the packet loss probability for both high-priority and medium-
priority packets to practically zero.
Thus, we believe that PPL
provides an effective mechanism for preventing uncontrolled loss
of important packets in network monitoring systems.
8. RELATED WORK
In this section we review prior work related to Scap.
8.1 Improving Packet Capture
Several techniques have been proposed to reduce the kernel over-
head and the number of memory copies for delivering packets to
the application [8, 11, 31, 39]. Scap can also use such techniques
to improve its performance. The main difference, however, is that
all these approaches operate at the network layer. Thus, monitoring
applications that require transport-layer streams should implement
stream reassembly, or use a separate user-level library, resulting
in reduced performance and increased application complexity. In
contrast, Scap operates at the transport layer and directly assembles
incoming packets to streams in the kernel, offering the opportunity
for a wide variety of performance optimizations and many features.
'
(
(
g
g
)
)
h
h
)
)
i
i
*
*
i
i
i
i
y
t
i
l
i
b
a
b
o
r
P
s
s
o
L
t
e
k
c
a
P
)
s
t
e
k
c
a
p
y
t
i
r
o
i
r
p
-
h
g
h
r
o
f
(
i
 1
 0.01
 0.0001
 1e-06
 1e-08
 1e-10
ρ=0.1
ρ=0.5
ρ=0.9
y
t
i
l
i
b
a
b
o