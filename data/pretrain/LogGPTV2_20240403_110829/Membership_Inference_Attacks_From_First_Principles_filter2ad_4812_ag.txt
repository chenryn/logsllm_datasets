Michael Backes, Emiliano De Cristofaro, Mario Fritz, and
Yang Zhang. ML-Doctor: Holistic risk assessment of infer-
ence attacks against machine learning models. arXiv preprint
arXiv:2102.02551, 2021.
[36] Yunhui Long, Vincent Bindschaedler, and Carl A Gunter.
arXiv preprint
Towards measuring membership privacy.
arXiv:1712.09136, 2017.
[37] Yunhui Long, Lei Wang, Diyue Bu, Vincent Bindschaedler,
Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen.
A pragmatic approach to membership inferences on machine
In 2020 IEEE European Symposium on
learning models.
Security and Privacy (EuroS&P), pages 521–534. IEEE, 2020.
[38] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient
descent with warm restarts. arXiv preprint arXiv:1608.03983,
2016.
[39] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and
Vitaly Shmatikov. Exploiting unintended feature leakage in
In 2019 IEEE Symposium on Security
collaborative learning.
and Privacy (SP), pages 691–706. IEEE, 2019.
[40] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
arXiv preprint
Pointer sentinel mixture models.
Socher.
arXiv:1609.07843, 2016.
[41] Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras.
Spam ﬁltering with naive Bayes–which naive Bayes? In CEAS,
volume 17, pages 28–69, 2006.
[42] Sasi Kumar Murakonda and Reza Shokri. ML Privacy Meter:
Aiding regulatory compliance by quantifying the privacy risks
of machine learning. arXiv preprint arXiv:2007.09339, 2020.
[43] Sasi Kumar Murakonda, Reza Shokri, and George Theodor-
akopoulos. Ultimate power of inference attacks: Privacy risks
of learning high-dimensional graphical models. arXiv e-prints,
pages arXiv–1905, 2019.
[44] Sasi Kumar Murakonda, Reza Shokri, and George Theodor-
akopoulos. Quantifying the privacy risks of learning high-
dimensional graphical models. In International Conference on
Artiﬁcial Intelligence and Statistics, pages 2287–2295. PMLR,
2021.
[45] Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine
learning with membership privacy using adversarial regulariza-
tion. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security, pages 634–646, 2018.
[46] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehen-
sive privacy analysis of deep learning: Passive and active white-
box inference attacks against centralized and federated learning.
In 2019 IEEE symposium on security and privacy (SP), pages
739–753. IEEE, 2019.
[47] Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Pa-
pernot, and Nicholas Carlini. Adversary instantiation: Lower
arXiv
bounds for differentially private machine learning.
preprint arXiv:2101.04535, 2021.
[48] Jerzy Neyman and Egon Sharpe Pearson. On the problem of
the most efﬁcient tests of statistical hypotheses. Philosophical
Transactions of the Royal Society of London., 231(694-706):
289–337, 1933.
[49] Patrick Pantel and Dekang Lin. SpamCop: A spam classiﬁcation
& organization program. In Proceedings of AAAI-98 Workshop
on Learning for Text Categorization, pages 95–98, 1998.
[50] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghu-
nathan, Kunal Talwar, and ´Ulfar Erlingsson. Scalable private
learning with PATE. arXiv preprint arXiv:1802.08908, 2018.
[51] Huy Phan. huyvnphan/pytorch cifar10, January 2021. URL
https://doi.org/10.5281/zenodo.4431043.
[52] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristo-
faro. Knock knock, who’s there? membership inference on
aggregate location data. arXiv preprint arXiv:1708.06145, 2017.
[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsuper-
vised multitask learners. OpenAI blog, 2019.
[54] Shadi Rahimian, Tribhuvanesh Orekondy, and Mario Fritz.
Sampling attacks: Ampliﬁcation of membership inference at-
tacks by repeated queries. arXiv preprint arXiv:2009.00395,
2020.
[55] Md Atiqur Rahman, Tanzila Rahman, Robert Lagani`ere, Noman
Mohammed, and Yang Wang. Membership inference attack
against differentially private deep learning model. Trans. Data
Priv., 11(1):61–79, 2018.
[56] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann
Ollivier, and Herv´e J´egou. White-box vs black-box: Bayes
In International
optimal strategies for membership inference.
Conference on Machine Learning, pages 5558–5567. PMLR,
2019.
[57] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang,
Mario Fritz, and Michael Backes. ML-Leaks: Model and
data independent membership inference attacks and defenses
on machine learning models, 2018.
[58] Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario
Fritz, and Yang Zhang. Updates-leak: Data set inference and
In 29th USENIX
reconstruction attacks in online learning.
Security Symposium (USENIX Security 20), pages 1291–1308,
2020.
[59] Sriram Sankararaman, Guillaume Obozinski, Michael I Jordan,
and Eran Halperin. Genomic privacy and limits of individual
detection in a pool. Nature genetics, 41(9):965–967, 2009.
[60] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
Shmatikov. Membership inference attacks against machine
learning models. arXiv preprint arXiv:1610.05820, 2016.
[61] Liwei Song and Prateek Mittal. Systematic evaluation of privacy
In 30th USENIX Security
risks of machine learning models.
Symposium (USENIX Security 21), 2021.
[62] Liwei Song, Reza Shokri, and Prateek Mittal. Privacy risks of
securing machine learning models against adversarial examples.
In Proceedings of
the 2019 ACM SIGSAC Conference on
Computer and Communications Security, pages 241–257, 2019.
Introducing
tensorﬂow.
a
https://blog.tensorﬂow.org/2020/06/introducing-new-privacy-
testing-library.html, 2020.
David Marn.
library
[63] Shuang
privacy
testing
[64] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate.
Stochastic gradient descent with differentially private updates.
In 2013 IEEE Global Conference on Signal and Information
Processing, pages 245–248. IEEE, 2013.
[65] Thomas Steinke and Jonathan Ullman. The pitfalls of average-
case differential privacy. DifferentialPrivacy.org, 07 2020. https:
//differentialprivacy.org/average-case-dp/.
[66] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and
Wenqi Wei. Towards demystifying membership inference at-
tacks. arXiv preprint arXiv:1807.09173, 2018.
[67] David A Van Dyk and Xiao-Li Meng. The art of data augmen-
tation. Journal of Computational and Graphical Statistics, 10
(1):1–50, 2001.
[68] Lauren Watson, Chuan Guo, Graham Cormode, and Alex
On the importance of difﬁculty calibra-
arXiv preprint
Sablayrolles.
tion in membership inference attacks.
arXiv:2111.08440, 2021.
[69] Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, and Reza
Shokri. Enhanced membership inference attacks against ma-
new
Song
and
in
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
151911
chine learning models. arXiv preprint arXiv:2111.09679, 2021.
[70] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh
Jha. Privacy risk in machine learning: Analyzing the connection
to overﬁtting. In 2018 IEEE 31st Computer Security Founda-
tions Symposium (CSF), pages 268–282. IEEE, 2018.
[71] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. arXiv preprint arXiv:1605.07146, 2016.
[72] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
and Oriol Vinyals. Understanding deep learning (still) requires
rethinking generalization. Communications of the ACM, 64(3):
107–115, 2021.
[73] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
Yi Yang. Random erasing data augmentation. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 34,
pages 13001–13008, 2020.
APPENDIX A
ADDITIONAL EXPERIMENTS
A. Attacking DP-SGD
Machine learning with differential privacy [1] is the main
defence mechanism against privacy attacks including member-
ship inference against machine learning models. Differential
privacy provides an upper bound on the success of any
membership inference attack. Recent works [24, 47] thus used
membership attacks to empirically audit differential privacy
bounds, in particular those obtained from DP-SGD [1]. In
this work, we are interested in the effect of DP-SGD on the
performance of our membership inference attack.
We consider different combinations of DP-SGD’s noise
multiplier and clipping norm parameters in our evaluation.
Table IV summarizes the average accuracy of standard CNN
models trained on CIFAR-10 with DP-SGD for different pa-
rameter sets. We evaluate the effectiveness of our membership
inference attacks for these settings in Figure 14. Even just
clipping the gradient norm without adding any noise reduces
the performance of our attack signiﬁcantly. However, small
clipping norms can reduce the accuracy of the models as
shown in Table IV.
TABLE IV: Accuracy of the models trained with DP-SGD on
CIFAR10 with different noise parameters
(a)  = ∞
(b)  > 5000
Noise Multiplier (σ) C = 10 C = 5 C = 1
61.3%
0.0
62.8%
0.2
0.8
61.3%
84.0%
73.9%
36.9%
78.5%
77.1%
43.3%
For higher clipping norms, adding very small amounts of
noise (Figure 14-b) reduces the effectiveness of the member-
ship inference attack to chance, while resulting in models with
higher accuracy.
Training models with very small amounts of noise is an
effective defense against our membership inference attack,
despite resulting in very large provable DP bounds .
(c)  = 8
Fig. 14: Effectiveness of using DP-SGD against our attack
with different privacy budgets.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
161912
10−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateσ=0.0,C=10,auc=0.674σ=0.0,C=5,auc=0.644σ=0.0,C=1,auc=0.527DPupperforeps=110−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateσ=0.2,C=10,auc=0.517σ=0.2,C=5,auc=0.525σ=0.2,C=1,auc=0.527DPupperforeps=110−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateσ=0.8,C=10,auc=0.501σ=0.8,C=5,auc=0.500σ=0.8,C=1,auc=0.503DPupperforeps=1B. White-box Attacks
Previous works [46, 62] suggested that is possible to achieve
better membership inference if the adversary has white-box
access to the target model. In particular, previous works
showed that using the norm of the model’s gradient at a target
point could increase the balanced accuracy of membership
inference attacks. Figure 15 highlights the comparison between
a white-box and a black-box adversary. The results show that
using gradient norms will improve the overall AUC both for
our online attack, as well as when using a global threshold
as in the LOSS attack. However, at lower false-positive rates
we do not observe any improvement of using gradient norms
compared to just using model conﬁdences.
B. Full ROC Curves for Gaussian Distribution Fitting
In Figure 17, we show full (log-scale) ROC curves for the
experiment in Section VI-B, where we explored the effect of
varying the number of shadow models on the success rate of
our online attack. We vary the number of shadow models from
4 to 256 and consider two attack variants: (1) ﬁt Gaussians for
each example by estimating the means µin, µout and variances
independently for each example; (2) estimate the
σ2
in, σ2
out
means µin, µout for each example, but estimate global variances
out. As we observed in Section VI-B, estimating per-
σ2
in, σ2
example variances works poorly when the number of shadow
models is small (< 64). With a global estimate of the variance,
the attack performs nearly on par with our best attack with as
little as 16 shadow models.
Fig. 17: Effect of varying the number of models trained on
attack success rates. It is always useful to estimate the mean
per-example difﬁculty; however when only a few models are
available, it is orders of magnitude more effective to assign
all examples the same variance.
Fig. 15: Comparison of the white-box attack using our ap-
proach to the black-box setting.
APPENDIX B
ADDITIONAL FIGURES AND TABLES
A. Attack Performance versus Model Accuracy
In Section V-D, Figure 7 we plotted the relationship between
a model’s train-test gap and its vulnerability to membership
inference attacks. In Figure 16, we look at the attack success
rate as a function of the test accuracy of the same models.
There is a clear trend where better models are more vulnerable
to attacks. Prior work reported a similar phenomenon for data
extraction attacks [3, 4].
Fig. 16: Attack true-positive rate versus model test accuracy.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
171913
10−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateOursbasedonlogitsauc=0.711Oursbasedongradient-normauc=0.724Globalthresholdbasedonlogitsauc=0.576Globalthresholdbasedongradientnormauc=0.6010.30.40.50.60.70.80.91.0TestAccuracy10−410−310−210−1100TPR@0.1%FPRCNN1,CNN2,CNN4CNN8,CNN16CNN32,CNN64WRN28-1WRN28-2WRN28-1010−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRate256models64models16models4modelswithglobalvarianceC. Comparison to Prior Work on Additional Datasets
Similarly to Figure 1 for CIFAR-10, we compare our at-
tack against prior membership inference attacks on additional
datasets: CIFAR-100 in Figure 18, WikiText-103 in Figure 19,
Texas in Figure 20 and Purchase in Figure 21.
Fig. 18: ROC curve of prior membership inference attacks,
compared to our attack, on CIFAR-100.
Fig. 19: ROC curve of prior membership inference attacks,
compared to our attack, on WikiText-103. We omit prior
attacks that rely on the model features z(x), as these attacks
were not designed for sequential models.
Fig. 20: ROC curve of prior membership inference attacks,
compared to our attack, on the Texas dataset.
Fig. 21: ROC curve of prior membership inference attacks,
compared to our attack, on the Purchase dataset.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
181914
105104103102101100False Positive Rate105104103102101100True Positive RateOurs.  Ye et al.  Sablayrolles et al.  Long et al.  Watson et al. Shokri et al.  Song et al.  Yeom et al.  Jayaraman et al.  10−510−410−310−210−1100FalsePositiveRate10−510−410−310−210−1100TruePositiveRateOursYeetal.Sablayrollesetal.Watsonetal.Yeometal.105104103102101100False Positive Rate105104103102101100True Positive RateOurs.  Ye et al.  Sablayrolles et al.  Long et al.  Watson et al. Shokri et al.  Song et al.  Yeom et al.  Jayaraman et al.  105104103102101100False Positive Rate105104103102101100True Positive RateOurs.  Ye et al.  Sablayrolles et al.  Long et al.  Watson et al. Shokri et al.  Song et al.  Yeom et al.  Jayaraman et al.