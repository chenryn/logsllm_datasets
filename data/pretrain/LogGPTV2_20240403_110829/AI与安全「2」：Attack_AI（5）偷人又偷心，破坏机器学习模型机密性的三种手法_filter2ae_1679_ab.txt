成员推理攻击（Membership Inference Attacks）是指给定数据记录和模型的黑盒访问权限，判断该记录是否在模型的训练数据集中。
这个攻击的成立主要是基于这样一个观察结果：对于一个机器学习模型而言，其对训练集和非训练集的uncertainty 有明显差别，所以可以训练一个Attack
model 来猜某样本是否存在于训练集中。
如对于上图所示，首先，攻击者将一个样本输入到目标模型中，并获得相应的预测结果，然后将样本的标签、目标模型的预测结果输入到Attack
Model里，判断这个记录是否在目标模型的训练集中。
由于Attack Model需要训练集才能生成，因此作者进一步提出了影子模型（shadow model）这个概念。
影子模型很好理解，生成方式和模型萃取攻击相似，即影子模型 T’ 与目标模型 T 的架构要一样，然后随机生成data x，把 x 输入 T 得到的label
预测结果p当作 y，再用这些(x, y) 去训练 T’。 因为两个模型的架构相同， T 与 T’ 预测的机率分布也是相似的，所以针对影子模型 T’ 的
attack model 也可以对目标模型 T 做成员推理攻击。
影子模型训练数据的生成方式：
  * 基于模型的合成方式 Model-based synthesis：如果攻击者没有真正的训练数据,也没有关于其分布的任何统计，他可以利用目标模型本身为影子模型生成合成训练数据。直观来看，高置信度的目标模型分类的记录应当在统计上类似于目标的训练数据集，因此能用于训练影子模型。
  * 基于统计的合成方式 Statistics-based synthesis ：攻击者可能拥有目标模型的训练数据的一些统计信息。例如，攻击者可能事先知道不同特征的边缘分布。在实验中，可以通过独立地从每个特征的边缘分布中进行采样，来生成影子模型的训练集。
  * 含噪音的真实数据 Noisy real data ： 攻击者可以访问一些与目标模型的训练数据类似的数据，这些数据可以被视为“噪音”版本。
生成影子模型后，构造攻击模型的训练集（训练集的标签，和影子模型的输出结果），并对攻击模型进行训练就好了。
**防御手段：**
  1. 一些防止 overfitting 的方法可以用来防御成员推理攻击，如 droupout、regularization
  2. 对预测结果做一些后续处理也可以防御成员推理攻击，如取 top k classes、rounding、增加 entropy 等手法
## 3-模型逆向攻击
破坏模型机密性的第三种手段是模型逆向攻击（Model Inversion
Attack）。这种攻击方法利用机器学习系统提供的一些API来获取模型的一些初步信息，并通过这些初步信息对模型进行逆向分析，获取模型内部的一些隐私数据。这种攻击和成员推理攻击的区别是，成员推理攻击是针对某一条单一的训练数据，而模型逆向攻击则是倾向于取得某个程度的统计信息。
例如，用模型和某位病患的人口属性，可以回推出该病患的遗传资料（即该模型的input）;
用一个名字（模型的output）回推出这个人的脸部影像，如上图所示，图右是训练集中的原图，图左是从模型逆向中推出的这个人的长相，攻击者只有这个人的名字和人脸识别模型的API调用权限。
这种攻击是怎么进行的呢？我们用最简单的线性回归模型来举例。
如上所示，假如我们有一个基于病人的人口统计信息、医疗记录和基因特征来预测病人应该服用多少剂量药物的线性回归模型。我们假定基因特征是敏感特征x1,
那么模型逆向攻击的目的就是通过构造大量的样本，来推断病人的基因特征x1的值是多少。
这里，模型的表达式为y =
f（x_1，…，x_d）。首先，攻击者需要对病人的其他信息有一定了解，如人口统计信息和医疗记录，从而减少遍历次数，其次，攻击者需要知道x1的取值范围是多少。之后，只要通过不停构造测试数据，并将模型返回的结果和实际结果进行对比，就可以获得该患者的敏感特征x1了。
如果我们对数据源一无所知，只有一个标签，比如姓名，还可以发起攻击吗？
我们拿人脸识别系统来举例。
首先，因为我们对数据源一无所知，我们需要用随机生成的输入向量来冷启动攻击，然后通过在输入空间使用梯度下降来最大化模型在目标预测值上的置信度，当误差小于我们指定的阈值时，模型暂停，获得重构的人脸。
详细的过程比较复杂，感兴趣的同学可以自行参考这篇论文：
> Fredrikson M, Jha S, Ristenpart T. Model inversion attacks that exploit
> confidence information and basic countermeasures[C]//Proceedings of the 22nd
> ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015:
> 1322-1333.
## references
[1] Chakraborty A, Alam M, Dey V, et al. Adversarial attacks and defences: A
survey[J]. arXiv preprint arXiv:1810.00069, 2018.
[2] Zhang J, Gu Z, Jang J, et al. Protecting intellectual property of deep
neural networks with watermarking[C]//Proceedings of the 2018 on Asia
Conference on Computer and Communications Security. 2018: 159-172.
[3] Tramèr F, Zhang F, Juels A, et al. Stealing machine learning models via
prediction apis[C]//25th {USENIX} Security Symposium ({USENIX} Security 16).
2016: 601-618.
[4] Shokri R, Stronati M, Song C, et al. Membership inference attacks against
machine learning models[C]//2017 IEEE Symposium on Security and Privacy (SP).
IEEE, 2017: 3-18.
[5] Fredrikson M, Lantz E, Jha S, et al. Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing[C]//USENIX Security
Symposium. 2014: 17-32.
[6] Fredrikson M, Jha S, Ristenpart T. Model inversion attacks that exploit
confidence information and basic countermeasures[C]//Proceedings of the 22nd
ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015:
1322-1333.