four testing phones) maintains in a reasonable level (80%), we
witness a signiﬁcant TSR drop (50%) on it, probably due to
the inferior hardware components used in this smartphone.
TSR of Nexus then grows from 50% to around 65% when
we use Meta-Enha method to train the adversarial phrases.
We also observe that TSR of HTC smartphone even jumps
to around 100% in the same setting. On the other hand,
both iPhone and Samsung smartphone maintains a consistently
high TSR and CSR. The result demonstrates that Metamorph
achieves overall satisfying robustness to the middle-end and
high-end smartphone. Its performance degrades when using
low-end smartphone, and we leave the way to compensate for
that as our future work.
3) Effect of Ambient Noise: We next examine the effect of
ambient noise. The experiment setup is same as the previous
one. The attacker speaker plays the adversarial examples at
75 dBSPL. We further play another music clip as a background
noise and examine system performance under different noise
levels from 35 dBSPL to 50 dBSPL, e.g., the corresponding
SNR varies from 40 to 25. From Figure 19 we observe
that Meta-Qual achieves consistently high TSR and CSR in
35 dBSPL and 40 dBSPL noise ﬂoor settings (e.g., a quite
room). TSR decrease slightly to 85% when the noise ﬂoor
grows to 45 dBSPL (e.g., in a common human conversion),
and then drops to around 60% as we further increase noise
ﬂoor to 50 dBSPL. On the other hand, we observe TSR for
Meta-Enha method maintains in a high level in all these four
noise ﬂoor settings. This result shows Metamorph is robust to
moderate ambient noise levels, e.g., SNR is greater than 25.
4) Effect of Speaker Volume: Moreover, we further vary the
transmission power from 45, 55, 65 to 75 dBSPL and examine
the system performance with the ambient noise around 35
dBSPL. Figure 20 shows the performance of Meta-Enha and
Meta-Qual. When the speaker volume is 65 and 75 dBSPL
(SNR is 30 and 45 respectively), both TSR and CSR are nearly
100%. When the speaker volume is tuned to 55 dBSPL (SNR
is 20), the attack performance slightly degrades, e.g. TSR of
Meta-Enha degrades to 0.9 and TSR of Meta-Qual degrades
to around 0.82. When speaker volume is further reduced to 45
(SNR is 10), the attack successful rates become low.
Figure 21: CSR and TSR achieved by (a) Meta-Enha and (b)
Meta-Qual in different attacker moving speed settings.
5) Effect of Victim Device Movement: We ﬁnally inves-
tigate the possibility of attacking when the victim device
is moving, which is a nature and practical scenario for the
adversarial attack. In this experiment we place the attacker
speaker on a table and play adversarial examples generated by
Meta-Qual and Meta-Enha, respectively. We hold the victim
device in hand and move towards and backwards the table
at a different yet relatively constant speed (0.1 m/s, 0.5 m/s,
1.0 m/s, and 1.5 m/s). The result is shown in Figure 21. We
observe CSR for Meta-Qual is consistently high (>90%) when
the attacker moves at both low (0.5m/s) and normal speed
(1.5m/s). TSR for Meta-Qual, on the other hand, decreases
slightly when the attacker moves at 1.5m/s. Both CSR and
12
Schemes
Black-box Attacks [43], [47]
Qin et al. [42]
Carlini et al. [17]
Abdullah et al. [12]
CommanderSong [57]
Yakura et al. [56]
Meta-Enha
Meta-Qual
Target model
DeepSpeech
Lingvo [30]
DeepSpeech
DeepSpeech
Kaldi [41]
DeepSpeech
DeepSpeech
DeepSpeech
Attack model
Black-box
White-box
White-box
White-box
White-box
White-box
White-box
White-box
Over-the-air
No
No
No
Yes
Yes
Yes
Yes
Yes
Attack scenes
-
Simulated
-
0.3m (1 foot)
1.5m
0.5m
6m / NLOS
3m
Successful rate
-
-
-
15/15 (trials)
78 %
80 %
90 % / 85.5 %
90 %
Audio quality (MCD)
-
-
-
-
22.3
25.1
25.2
21.1
TABLE 3: The state-of-the-art audio adversarial attacks. “-” indicates the information is not available. We compute MCD value
for [56] and [57] based on their released attack samples.
TSR for Meta-Enha are around 100% in all four moving speed
settings. The result shows that both Metamorph versions are
robust to the victim’s normal movement.
V. RELATED WORK
Audio adversarial examples. Early study [50] reveals the pos-
sibility to conduct an adversarial attack on speech recognition
(SR) systems, while the generated adversarial examples can be
easily perceived by human [46]. Alzantot et al. [14] later attack
a command word recognition model without
the listener’s
perception. Motivated by [14], Taori et al. [49] further attack
DeepSpeech [27]. However, their major limitation is that the
recognized command contain no more than two words [56].
Recently, Carlini et al. [16] realize an attack on gen-
eral HMM-based RS systems without the constraint of the
command’s word number, and later they introduce a targeted
audio adversarial attack on the state-of-the-art SR system
DeepSpeech in [17]. Study [46] further introduces an attack
with the dedicated temporal alignment and back-propagation
designs, and Liu et al. [36] propose a weighted-sampling
method to reduce the search space. Qin et al. [42] propose a set
of frequency masking algorithms to improve the imperceptibly
of adversarial attacks. Felix et al. [33] design adversarial
examples to attack voice authentication system. Moustapha
et al. [18] proposes a general adversarial example generation
method, which can works on any gradient-based machine
learning models. Moreover, there are also few works leveraging
evolutionary algorithms to initiate black-box attack [43], [47].
However, the adversarial examples generated from these works
cannot survive after the over-the-air transmission. Later on
researchers try to make these adversarial attacks work in real-
world scenarios (as listed in Table 3). Yuan et al. [57] integrate
the commands into a song and Abdullah et al. [12] leverage the
similar frequency domain feature vectors extracted from mul-
tiple source audios to generate audio adversarial examples that
can initially succeed after over-the-air transmission. Yakura et
al. [56] further propose to inject the CIR collected at other
places into the training model and achieve descent success
rate. However, they mainly work in short range, e.g., 0.3 m to
1 m, and/or require the physical presence of the attack devices.
Embedding bits into audio. In the literature, there are also
some existing works that propose to embed bits into audios
for different application designs. For example, Dhwani [38]
utilizes the acoustic signals to develop a secure near-ﬁeld
communication protocol. GeneWave [54] proposes an efﬁcient
authentication design for mobile devices. The study [58]
further introduces a secure communication design without
using keys. These works mainly focus on the security-related
application designs. There are also some prior works that
propose more general methods to embed bits into sounds to
achieve a side-channel information delivery [35], [40], [52].
These designs mainly embed bits into the high-frequency
band, such as 18 kHz – 20 kHz, to minimize the perception
of human. The generation of audio adversarial examples, in
both Metamorph and prior attacks, also add bits into audios.
However, these bits are usually added in the audible range,
e.g., 0 kHz – 8 kHz, because SR mainly uses this range for
the recognition. Therefore, the audio quality is one crucial
consideration in the adversarial example generation.
Microphone non-linearity. In the literature, some recent
studies, like [44], [45], [59], successfully realize a series of
inaudible attacks on the speech recognition by harnessing the
non-linearities of the diaphragm of microphone and the power
ampliﬁer of receiver [44]. The attacker can inject the sneaky
voice commands to the speech recognition system of the victim
receiver, and the device’s owner cannot hear such commands.
However, these recent inaudible attacks all require the special
speaker hardware to play ultrasonic acoustic signals, incurring
the extra hardware requirement. Moreover, it is successfully
defended in [45]. These works do not belong to the adversarial
attack, which are parallel to Metamorph and do not address our
unique challenges in this paper.
Assorted topics related to Metamorph. There are also some
other types of adversarial examples and the most representative
example is the image-based ones [15], [34]. For the image ad-
versarial example generation, there exists a similar problem —
whether the image adversarial examples can survive when they
are taken by a camera? RP2 [20] recently reports a successful
attack by taking the varying of distances and angles between
the camera and the adversarial image into consideration in
the perturbation training. However, the technical challenges
in acoustic channels are different compared with the existing
image-based adversarial attacks.
On the other hand, to improve the attacking distance, we
also utilize the domain discriminator training methods [29],
[60]. Inspired by these existing works, we further propose a
dedicated domain discriminator to exclude the device- and
environmental-dependent features from the prior measure-
ments in the training of the adversarial example perturbation.
VI. CONCLUSION
This paper presents Metamorph to generate over-the-air
audio adversarial examples. We ﬁrst conduct extensive empir-
ical studies to understand this attack in the over-the-air setting
13
and observe that the reason undermining prior designs is the
frequency-selectivity caused by both device and channel. To
cope with this issue, we propose a “generate-and-clean” two-
phase design and also consider the audio quality of generated
adversarial examples. The evaluation shows the efﬁcacy and
good performance of Metamorph.
ACKNOWLEDGMENT
We sincerely thank the anonymous reviewers for their
helpful comments and feedback. This work is supported by
the GRF grant from Research Grants Council of Hong Kong
(Project No. CityU 11217817). This work is also supported by
NSF Award CNS-1617161.
[23] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber,
“Connectionist temporal classiﬁcation: labelling unsegmented
sequence data with recurrent neural networks,” in Proceedings of
ICML, 2006.
[24] S. L. R. H. Guillaume Delorme, Xavier Alameda-Pineda, “Camera
adversarial transfer for unsupervised person re-identiﬁcation,”
https://arxiv.org/abs/1904.01308, 2019.
[25] W. Guo, D. Mu, J. Xu, P. Su, G. Wang, and X. Xing, “Lemna:
Explaining deep learning based security applications,” in Proceedings
of ACM CCS, 2018.
[26] A. Hannun, “Sequence modeling with ctc,” Distill, 2017.
[27] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,
R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep speech:
Scaling up end-to-end speech recognition,” arXiv preprint
arXiv:1412.5567, 2014.
[28] M. Jeub, M. Schafer, and P. Vary, “A binaural room impulse response
database for the evaluation of dereverberation algorithms,” in
Proceedings of IEEE DSP, 2009.
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
“Amazon Echo and Alexa,” https://alexa.amazon.com.
“Audio Adversarial Examples,”
https://github.com/carlini/audio_adversarial_examples.
“Google Now,” https://en.wikipedia.org/wiki/Google_Now.
“In-Car Voice Commands NLP for Self-Driving Cars,”
https://aitrends.com/ai-insider/car-voice-commands-nlp-self-driving-
cars/.
“M200MKIII+ Bluetooth Bookshelf Speakers,”
https://swanspeakers.com/product/m200mkiii-bluetooth-bookshelf-
speakers/.
“Mozilla Common Voice Dataset,”
https://voice.mozilla.org/en/datasets.
“Mycroft,” https://mycroft.ai/.
“Project DeepSpeech,” https://github.com/mozilla/DeepSpeech.
“Project Website of Metamorph,”
https://acoustic-metamorph-system.github.io/.
“SwiftScribe,” https://swiftscribe.ai/.
[10]
[11] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for
large-scale machine learning,” in Proceedings of USENIX OSDI, 2016.
[12] H. Abdullah, W. Garcia, C. Peeters, P. Traynor, K. Butler, and
[13]
J. Wilson, “Practical hidden voice attacks against speech and speaker
recognition systems,” in Proceedings of NDSS, 2019.
J. B. Allen and D. A. Berkley, “Image method for efﬁciently
simulating small-room acoustics,” The Journal of the Acoustical
Society of America, 1979.
[14] M. Alzantot, B. Balaji, and M. Srivastava, “Did you hear that?
adversarial examples against automatic speech recognition,” in
Proceedings of NIPS, 2017.
[15] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing
robust adversarial examples,” in Proceedings of ICML, 2018.
[16] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields,
D. Wagner, and W. Zhou, “Hidden voice commands.” in Proceedings
of USENIX Security Symposium, 2016.
[17] N. Carlini and D. Wagner, “Audio adversarial examples: Targeted
attacks on speech-to-text,” in IEEE Deep Learning and Security
workshop, 2018.
[18] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, “Houdini: Fooling deep
structured visual and speech recognition models with adversarial
examples,” in Proceedings of NIPS, 2017.
[19] S. Desai, E. V. Raghavendra, B. Yegnanarayana, A. W. Black, and
K. Prahallad, “Voice conversion using artiﬁcial neural networks,” in
Proceedings of IEEE ICASSP, 2009.
[20] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust physical-world attacks
on deep learning visual classiﬁcation,” in Proceedings of IEEE CVPR,
2018.