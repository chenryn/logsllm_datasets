showing a random delay countermeasure assuming that they
were more likely to hide worthwhile information. As we shall
see next, this was clearly not our best bet.
We decided to skip the ﬁrst section as it was not clear if it
was the start of the current iteration or the end of the preceding
one. Unfortunately, for each of the seven other orange sections,
the SNR analysis resulting from the 1000× 128 re-aligned
sections did not show any signiﬁcant leakage.
We then considered the 8 other sections without random
delay countermeasure. In each of these sections, the signal
is mostly composed of small consecutive EM peaks that we
detected and re-aligned. However, the peak detection was too
noisy. Some peaks were overlooked and some signal inter-
ferences were erroneously identiﬁed as signal peaks. Again,
these re-alignments did not give us any interesting results.
At this point, we had more questions than answers: is the
acquisition setup correct? Was our trace re-alignment proce-
dure correct? Do we have enough traces to observe a sensitive
leakage? Was there any sensitive leakage at all?
We modiﬁed the EM probe position and adapted some
previous re-alignments (those that seemed to give the best
SNR results) with no success.
In a last attempt, we focused our attention on the two or-
ange sections at the beginning of the Double and the Add
operations. These parts of the traces reﬂect the activity of
the crypto library which sets the different register addresses
before launching the operations. We ﬁnally captured a weak
sensitive leakage located on a large EM signal peak (one of
the peaks with large amplitude that we can see on Figure 8
at the very beginning of the Add operation). Note that we did
not explicitly exploited these peaks during our ﬁrst attempts
because we based our re-alignment procedure on the peaks
belonging to the random delay countermeasure.
This ﬁrst positive result lead us to perform a last experiment
relying on the systematic detection of the EM signal peaks
with large amplitude over the whole sub-trace. It turned out
that four of these peaks (located during the Double operation)
bear a strong sensitive leakage, much stronger than the weak
leakage observed before. In Figure 9, we show the area where
a sensitive leakage was eventually detected. Figure 10 focuses
on the four signal peaks that bear the sensitive leakage inside
that area.
Double
Add
˜ki
Figure 9: Rhea EM Trace - ECDSA Signature (P-256, SHA-
256) - Sensitive Leakage Area
˜ki
Figure 10: Rhea EM Trace - ECDSA Signature (P-256, SHA-
256) - Sensitive Leakage
238    30th USENIX Security Symposium
USENIX Association
Inputs
# operations
Length
Sampling rate
# Samples/trace
Channel conf.
File size
Acq. time
random messages, constant key
4000
100 ms
5G Sa/s
500 M
DC 50 ohms, ±50 mV
2 TB
≈ 4 hours
Table 1: SCA acquisition parameters for Rhea
4.1.3 Final Acquisition Setup
Based on this success, we checked various EM probe positions
and even changed our EM probe itself (for the Langer ICR
HH 500-6, see Section 2.6.1) to improve the signal strength.
The ﬁnal acquisition setup details are provided in Table 1. A
picture of the probe position is shown in Figure 4.
4.2 A Sensitive Leakage
Figure 11 (ﬁrst sub-ﬁgure) depicts 1000 superposed traces
after re-alignment, where only 400 samples were kept around
each of the four identiﬁed signal peaks. As mentioned before,
to evaluate the statistical relations between the re-aligned
traces and the encoded scalar digits, we computed the Signal-
To-Noise Ratio (SNR). As stated in [22]: "The SNR is quan-
tifying how much information is leaking from a point of a
power trace. The higher the SNR, the higher is the leakage".
More precisely, each of the 4000× 128 re-aligned traces are
classiﬁed with respect to the corresponding 2-bit digit ˜ki. We
then end up with four sets of traces. For each set s and at
each time sample t, we estimated the traces mean µs(t) and
variance vs(t). The SNR computed independently for each
time sample t is obtained by:
SNR(t) =
Var(µs(t))
E(vs(t))
,
where Var(µs(t)) is the estimated variance over the four esti-
mated means and E(vs(t)) is the estimated mean of the four
estimated variances.
In the second sub-ﬁgure of Figure 11, we plotted the SNR
results for the four sets (˜ki ∈ {0,1,2,3}). The best SNR value
is ≈ 0.53. Clearly the amplitude of the side-channel traces is
strongly related to the sensitive values ˜ki
6.
6If the side-channel traces amplitude at time sample t is not related to
encoded nonce digits, the respective SNR value should tends toward 0 as the
number of traces increases (as the signal variance (Var(µs(t))) itself tends to
0). This is what happens for most of the traces time samples (see Figure 11,
second sub-ﬁgure). However, at some speciﬁc time samples (where SNR
SNR for ˜ki ∈ {0,1,2,3}
SNR for ˜ki ∈ {1,2,3}
Figure 11: Rhea EM Trace - ECDSA Signature (P-256, SHA-
256) - SNR results (y-axis range [0,0.7])
Our ﬁrst guess on the scalar multiplication algorithm (Al-
gorithm 1) did not completely disclose the value taken by
G0, apart from the fact that it is not the point at inﬁnity. In
fact G0 could be any point on the elliptic curve; but it is most
likely chosen in {G1,G2,G3,G4}. Besides, G0 could change
from one iteration to another. Therefore, we estimated the
SNR without considering the cases ˜ki = 0. The corresponding
sub-traces were simply discarded from the SNR computa-
tions. In the third sub-ﬁgure of Figure 11, we can observe a
signiﬁcant increase of the SNR to ≈ 0.65. These results tend
to conﬁrm that G0 takes varying values among G1, G2 and
G3 only. Using standard noise reduction techniques, based on
ﬁltering and principal component analysis, we managed to
further improve the SNR to 0.78.
Let us go a bit further in the understanding of the leakage.
Considering only the sub-traces where ˜ki (cid:54)= 0, we estimated
the leakage strength with respect to the two bits of ˜ki consid-
ered independently.
To do so we used the Welch T-Test [39]. Given two uni-
variate data sources the T-Test tells us whether one can reject
the null hypothesis with conﬁdence, i.e. whether the two data
sources are far enough from two independent sources. We
performed two independent T-Tests. For the ﬁrst test, the data
sources are the sub-traces that correspond to ˜ki = 1 and ˜ki = 3
respectively, for which the lsb (of ˜ki) is equal to 1. This allows
to test the msb of ˜ki. Similarly, we collected T-Test results for
the sub-traces corresponding to ˜ki = 2 and ˜ki = 3 respectively
which leave the msb constant; hence testing the lsb of ˜ki.
A T-Test score was computed for each time sample inde-
pendently. The results are depicted in Figure 12. These scores
clearly show that the two bits of ˜ki do not leak at the same
peaks are visible), the SNR converges toward a non-null value. This means
that Var(µs(t)) itself converges toward a non-null value and therefore the
side-channel traces at time sample t are signiﬁcantly different for the four
different encoded nonce digits values.
USENIX Association
30th USENIX Security Symposium    239
mal threshold choice is not known a priori, we applied
the process for different threshold values until it gave
consistent results).
2. With the sub-traces corresponding to ˜ki (cid:54)= 0, estimate the
three cluster centers for ˜ki = 1,2 and 3 respectively.
Most signiﬁcant bit of ˜ki
Least signiﬁcant bit of ˜ki
Figure 12: Rhea EM Trace - ECDSA Signature (P-256, SHA-
256) - T-Test results (y-axis range [−100,100])
time. Furthermore, we can clearly see that the most signiﬁ-
cant bit of ˜ki shows a strong leakage for the last three peaks,
whereas the lsb’s strongest leakage is mainly located on the
ﬁrst peak.
4.3
Improving our Knowledge of the NXP’s
Scalar Multiplication Algorithm
As explained in the previous section, we removed the sub-
traces corresponding to the case ˜ki = 0 as they seem to dete-
riorate our SNR computation. Our hypothesis is that, when
˜ki = 0, the developers decided to randomly choose (at each
iteration) a point from the available pre-computed points
(G1,G2,G3).
To try validate our hypothesis, we designed the following
experiment based on supervised Expectation-Maximization
clustering (to this end, we use the GaussianMixture class
from Scikit-learn Python library [33]).
The idea is simple. We used the many sub-traces that are
correctly labeled when ˜ki (cid:54)= 0 to train our clustering algorithm
(i.e. to precisely deﬁne the three clusters using maximum like-
lihood). We were then able to match the un-labeled sub-traces
(i.e. those corresponding to ˜ki = 0) by ﬁnding the closest
cluster, i.e. by identifying the value j such that G0 = G j for
this iteration. The Expectation-Maximization clustering is
a multivariate process. It uses multi-dimensional data (i.e.
our sub-traces with several time samples) to infer multivari-
ate Gaussian distributions from these samples. To ease this
work, we had to remove some useless time samples (i.e. time
samples for which the signal was not strongly related to the
sensitive variable ˜ki). The overall process is summarized be-
low:
1. Reduce all sub-traces to the time samples where the
SNR is larger than a speciﬁc threshold (since the opti-
3. For each labeled sub-trace, ﬁnd the closest center. This
phase allows controlling the clustering success rate.
4. Finally, for each un-labeled sub-trace (i.e. ˜ki = 0), ﬁnd
the closest center.
The matching phase revealed that about half of the un-
labeled sub-traces matched the ˜ki = 1 case, while the other
half were equally divided between the cases ˜ki = 2 and ˜ki = 3.
The above observation was validated by our next experi-
ment. We created two sets of sub-traces. In the ﬁrst set, we
put the ˜ki = 0 sub-traces. The other set contained a mix of
sub-traces with ˜ki (cid:54)= 0, with half of them corresponding to
˜ki = 1 and the rest equally divided between ˜ki = 2 and ˜ki = 3.
The T-Test evaluation between these two sets could not reject
the null hypothesis (no T-Test peak is visible and the best
T-Test absolute value is less than 37), hence conﬁrming the
Expectation-Maximization experiment results.
In the improved version of the scalar multiplication algo-
rithm presented in Algorithm 2, we have G0 = G1 = G (the
elliptic curve base point), G2 = [2129]G1, G3 = G1 + G2 and
G4 = [2128]G1.
Since G0 = G1 = G, one can check that the Dummy ←
S + Grand addition is operated on G1 half the time and on G2
or G3 the rest of the time. We would like to emphasize that
this algorithm is only our interpretation of the real algorithm
implemented on Rhea that might differ slightly. Details of the
real implementation are not our concern here, a high-level
understanding of the countermeasures is good enough.
5 A Key-Recovery Attack
In this section, we detail the process that resulted in the full re-
covering of the private keys embedded into the NXP’s secure
components of both Rhea and Titan. Our attack consists of
two main steps: we ﬁrst exploited the vulnerability observed
in Algorithm 2 to recover some zero bits of the nonces with
very high conﬁdence level. Then, from this partial knowledge
on the nonces, we applied a lattice-based attack by reducing
our problem to an instance of the Extended Hidden Number
Problem (EHNP). We present these two phases in the next
sections.
7A more formal analysis, following e.g. [40], is possible to interpret the
T-Test results and estimate the error probability of having an undetected
leakage. Here, we do not need such a ﬁne grain analysis, the T-Test results
do not show the signiﬁcant peaks found in prior experiments. We can then
safely conclude that the two sets of sub-traces, selected as we did, behave
very much alike.
240    30th USENIX Security Symposium
USENIX Association
Algorithm 2: Improved Version of Scalar Multiplication
Algorithm used in Signature Operation
:{˜k1,··· , ˜ki,··· , ˜k129}: The encoded scalar
Input
Input
:G0,G1,G2,G3,G4: The pre-computed points
Output :[k]G: The scalar multiplication of scalar k by
point G
// Init Register S to the point G(= G1)
S ← G1
for i ← 2 to lk/2 do
S ← [2]S
rand ← random element from {0,1,2,3}
if ˜ki > 0 then
S ← S + G˜ki
Dummy ← S + Grand
else
if ˜k1 = 0 then
S ← S− G4
Dummy ← S− G4
else
Return :S
5.1 Recovering Scalar Bits from the Observed
Leakage
As seen in Section 4, Algorithm 2 leaks non-uniform infor-
mation whenever the 2-bit encoded digit ˜ki is zero. We recall
that ˜ki is obtained from the binary representation of k as
˜ki = 2ki + k129+i. When ˜ki = 0, our analysis conﬁrmed that
Algorithm 2 stores the result of the addition S + G0 into a
dummy register, with G0 chosen at random in {G1,G2,G3}
with respective probability 1/2, 1/4, 1/4. Let (ˆki)i denote the
sequence of digits recovered from the observed leakage on ˜ki
in a noise free scenario. From the above observation, we have
ˆki ∈ {1,2,3}. Let us ﬁrst examine the case ˆki = 1. With proba-
bility 1/4, the observed value matches the correct value ˜ki = 1,
in which case G1 is correctly added to S. But it may also corre-
spond to the case where ˜ki = 0 and G1 was randomly chosen
to perform the dummy addition, which occurs with probabil-
ity 1/4× 1/2 = 1/8. In total, we have P(ˆki = 1) = 3/8. The
overall analysis for ˆki = 1,2,3 is summarized in Table 2.
Table 2 provides crucial information on the bits of k. In
particular, we remark that ˆki = 1 implies ki = msb(˜ki) = 0.
Similarly, ˆki = 2 implies k129+i = lsb(˜ki) = 0.
As seen in Section 4, T-Test results on carefully re-aligned
sub-traces around four EM signal peaks (See Figure 12) gave
us very precise time samples where the encoded digits are
leaking. Testing the 2 bits of ˜ki separately also revealed
more leakage points for ki = msb(˜ki) than for k129+i = lsb(˜ki).
Therefore, we ﬁrst focused our analysis on the leakage arising
from msb(˜ki). In practice, we used 4000× 128 sub-traces on
Rhea that we carefully ﬁltered out by selecting time samples
for which the T-Test was greater than some threshold, in ab-
ˆki
1
2
3
P(ˆki)
3/8
5/16
5/16
˜ki
1
0
2
0
3
0
(kik129+i)
(01)
(00)