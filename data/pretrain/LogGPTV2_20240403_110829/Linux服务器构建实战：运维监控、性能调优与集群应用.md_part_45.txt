## Page 306
件内容大致如下。
通过piranha提供的Web界面配置此文件，也可以直接手动编辑此文件。编辑好的lvs.cf文
安装即可。过程如下：
活DirectorServer的备用主机。这里利用piranha来配置DirectorServer的双机热备功能。
配置LVS的繁琐工作。同时，piranha也可以单独提供集群功能，例如，可以通过piranha激
11.5.3通过piranha搭建LVS高可用性集群
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
piranha安装完毕后，会产生/etc/sysconfig/ha/lvs.cf配置文件。默认此文件是空的，可以
也可以通过yum命令直接在线安装。过程如下：
piranha工具的安装非常简单，下载piranha的rpm包，在主、备Director Server上进行
1.安装与配置piranha
piranha是REDHAT提供的一个基于Web的LVS配置软件，通过piranha可以省去手工
send="GET/HTrP/1.0\r\n\r\n"#向realserver发送的检证字将串
port=80
active=1
virtual www.ixdba.net{
debug_level = NONE
network=direct
deadtime = 10
keepalive=5
heartbeat_port=539
backup = 192.168.12.131
backup_active
Bervice=
serial_no
[root@DR1 ~]# more/etc/sysconfig/ha/lvs.cf
[root@DR2 -]# yum install piranha
[root@DR1 ~]#rpm -ivh piranha-0.8.2-1.i386.rpm
至此，Keepalived+LVS高可用的LVS集群系统已经运行起来了。
[root@rsl~]#/etc/init.d/lvsrs start
接着在两个Realserver上执行如下脚本：
[rootaDR1 ~]#/etc/init.d/Keepalived start
1vs
192.168.60.130
=18
www.Linuxidc.com
=1
#虚拟服务的端口
是否激活此服务
指定虚拟服务的名称
如果主Director Server在deadtime（秒）后没有响应，那么备用
这里指定备用
定LVS的工作模式，direct表示DR模式，nat表示NAT模式，tunnel
心跳间隔时间，单位是秒
定心跳的UDP通信端口，
这里选择激活
定双机的服务名
靠
定主Director Server的真实IP地址
序号
义debug调试信息级别
示TUN模
否开启心跳，
rector
#虚拟服务绑定的虚拟IP及网络设备名
Server.
Director Server的真实IP地址，如果没有备用
第章
代替
291
PDG
---
## Page 307
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
命令如下：
别启动pulse服务，即启动LVS服务。过程如下：
脚本在前面已经讲述，这里不再介绍。
到此为止，利用piranha工具搭建的高可用LVS集群系统已经运行起来了。
[root@rs1 ~]#/etc/init.d/lvsrs start
最后，在两个Real server节点上执行lvsrs脚本。命令如下：
[rootaDR1 ~]#echo "1 >/proc/sys/net/ipv4/ip_forward
接下来，还要在主、备节点上启用系统的包转发功能（其实只有在NAT模式下需要)。
将编辑好的Ivs.cf从Director Server的主节点复制到备用节点，
[root@DR1 -]#service pulse start
2.启动通过piranha配置的LVS集群系统
接着，还需要对两个Real Server节点进行配置，也就是创建/etc/init.d/lvsrs脚本，这个
weight =
address =192.168.12.133
server RS2
weight
active
addres8 = 192.168.12.132
server Rs1{
quiesce_server=0
reentry=15
timeout =6
protocol=tcp
scheduler-rr
load_monitor = none
use_regex-0
expect="HTTP"
山
：节点而言的，权值高的RealServer处理负载的性能相对较强
#
#none，如果选择rup，每个RealSezver就必须运行rstatd服
#是否工作正常
#服务器正常运行时应该返回的文本应答信息，用来判断RealServer
*
expect选项中是否使用正则表达式，0表示不使用，1表示使用
Linuxidc .com
定Real
定RealServer服务名
婴
议设置为
品
章，因此
守
个RealServer被移除后，重新加入LVS路由列表中必须持续的时间，
为时间，以秒为单位
3
alServer失效后从LVS路由列表中移除失效RealServer所必须持续
拟服务使用的
否激活此
七LVS会发送大量请求到此服务节点，造成新的节点服务阻塞：
页为1，那么当某个新的节点加入集群时，最少连接数会被重设
协议类
每个RealServer就必须运行rwhod 服
然后在主、备节点上分
PDG
---
## Page 308
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
放了虚拟IP资源。
的日志状态。
角色，并且接管了主机的虚拟IP资源，最后将虚拟IP绑定在eth0设备上。
志。信息如下：
Server上面的Keepalived服务停止，然后观察备用DirectorServer上Keepalived的运行日
11.6.1
测试，其他实例也有类似的效果，不一一讲述。
特性，因此，对其进行的测试也针对这3个方面进行。这里只对Keepalived+LVS实例进行
11.6测试高可用LVS负载均衡集群系统
从日志可知，
接着，重新启动主DirectorServer上的Keepalived服务，继续观察备用DirectorServer
Kew
从日志中可以看出，主机出现故障后，备用机立刻检测到，此时备用机变为MASTER
May4 16:50:04
高可用性是通过LVS的两个Director Server完成的。为了模拟故障，先将主Director
高可用的LVS负载均衡系统能够实现LVS的高可用性、负载均衡特性和故障自动切换
May
May
May
day
May
192.168.12.200
May
May
on etho.
192.168.12.200 removed
removed
eth0 for 192.168.12.200
192.168.12.200 on eth0.
added
eth0 for 192.168.12.200
4 16:51:30 DR2 avahi-daemon[2551]: Withdrawing address record for 192.168.12.200
4 16:51:30 DR2 Keepalived_healthcheckers:Netlink reflector reports IP
4 16:50:05 DR2 avahi-daemon[2551]:Registering new address record for
4 16:50:05 DR2 Keepalived_vrrp:
416:50:05 DR2
4
高可用性功能测试
16:51:30I
16:51:30 DR2 Keepalived_vrrp:Net1ink reflector reports IP 192.168.12.20
16:50:051
www.Linuxidc.com
备用机在检测到主机重新恢复正常后，重新返回BACKUP角色，并且释
DR2 Keepalived_vrrp: VRRP_Instance(VI_1) Bntering BACKUP STATE
added
DR2
DR2
Keepalived_vrrp:VRRP_Instance(VI_1) Sending gratuitous ARPs on
Keepalived_vrrp:
Keepalived_vrrp:
：VRRP_Instance(VI_1) Sending gratuitous ARPs
VRRP_Instance(VI_1) setting protocol VIPs.
VRRP_Instance(VI_1)
VRRP_Instance (VI_1) Transition to MASTER STATE
Instance(VIi)
Entering MASTER STATE
群293
IE
on
---
## Page 309
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
因为节点1出现故障，Keepalived监控模块将节点1从集群系统中剔除了。
将此节点从集群系统中剔除掉了。
息。相关日志如下：
蔽故障节点，同时将服务转移到正常节点上执行。
11.6.3
衡了。
echo"This is real server2"/webdata/www/index.html
echo
www目录，然后分别执行如下操作：
11.6.2负载均衡测试
294
此时访问http:/192.168.12.200这个地址，应该只能看到“Thisisreal server2”了。这是
通过日志可以看出，Keepalived监控模块检测到192.168.12.132这台主机出现故障后，
这里将realserver1节点服务停掉，假定这个节点出现故障，然后查看主、备机日志信
故障切换是测试在某个节点出现故障后，Keepalived监控模块是否能及时发现，然后屏
接着打开浏览器，访问http://192.168.12.200这个地址，然后不断刷新此页面。如果
从日志可知，Keepalived监控模块检测到192.168.12.132这台主机恢复正常后，又将此
下面重新启动real server1节点的服务，可以看到Keepalived日志信息如下：
在real server2执行：
"This is real serverl*
在real serverl执行：
这里假定两个Real Server节点上配置wwW服务的网页文件的根目录均为/webdata/
May 4 17:07:58 DR1 Keepalived_healthcheckers:SMTP alert successfully sent.
lay
May
May 4 17:02:02 DR1 Keepalived_healthcheckers: SMTP alert successfully sent.
May
May
EW
connected.
to VS [192.168.12.200:80]
[192.168.12.132:80] 8ucce88.
connected.
4 17:07:57 DR1 Keepalived_healthcheckers:TCP connection to
from VS [192.168.12.200:80]
4 17:07:57 DR1 Keepalived_healthcheckers:Adding service [192.168.12.132:80]
4 17:01:51 DR1 Keepalived_healthcheckers:Remote SMTP server [127.0.0.1:25]
417:01:51 DR1 Keepalived_healthcheckers: TCP connection to
故障切换测试
www. Linuxidc .com
/webdata/www/index.html
---
## Page 310
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
最适合自己的方式。下面是对这三种方式的总结：
11.7
正常后，Keepalived监控模块将此节点加人到集群系统中。
到“This isreal serverl”和“Thisis real server2”页面了，这说明在real server1节点恢复
节点加入到集群系统中。
口piranha方式
LVS+Keepalived方式
LVS+heartbeat+Ldirectord方式：
这一章详细讲述了通过LVS+heartbeat+Ldirectord、LVS+Keepalived及piranha三种方式
此时再次访问http:/192.168.12.200这个地址，然后不断刷新此页面，应该又能分别看
最后启动的是备用机，并且没有类似heartbet中的auto_failback功能。
缺点：在HAcluster双机切换过程中，没有主、备机之分，也就是说先启动的是主机，
ipvsadm编写脚本。
优点：安装简单，
因此检测效率很高，故障切换速度最快。
为ipvsadm编写脚本。Keepalived对后端服务节点的检测是基于底层网络通信协议的，
优点：安装简单、配置简单，仅需要一个配置文件即可完成所有配置，同时无需单独
缺点：配置比较复杂，需要对heartbeat和Ldirectord分别进行配置。
页面方式进行服务节点监控，配置灵活，可以根据需要进行选择。
优点：安装简单，并且无需单独为ipvsadm编写脚本。同时，Ldirectord支持端口和
本章小结
www.Linuxidc.com
配置简单，只需一个lvs.cf文件即可完成所有配置，也无需为
群295
PDG
---
## Page 311
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
了服务不间断、稳定地运行。
点加人到集群中去。这一系列切换动作，
节点的请求平滑转移到其他正常节点上；而在此故障节点恢复正常后，LVS又会自动将此节
服务时，LVS会自动屏蔽这个故障节点，接着将失败节点从集群中剔除，同时将新分配到此
点响应客户端请求。LVS还提供了服务节点故障转移功能，也就是当某个服务节点不能提供
时，集群系统根据调度算法来判断应该将请求分配到哪个服务节点上，然后，由分配到的节
户端请求平均分配到各个服务节点上，同时还可以定义多种负载分配策略。当一个请求进来
载均衡技术。LVS由负载调度器和服务访问节点组成，通过LVS的负载调度功能，可以将客
服务，这就是RHCS高可用集群实现的功能。
个节点。节点故障转移功能对客户端来说是透明的，从而保证应用持续、不间断地对外提供
时，应用可以通过RHCS提供的高可用性服务管理组件自动、快速地从一个节点切换到另一
端应用持久、稳定地提供服务，同时也保证了后端数据存储的安全。
提供了一个行之有效的集群架构实现方案。通过RHCS提供的这种解决方案，不但能保证前
地说，RHCS是一个功能完备的集群应用解决方案，从应用的前端访问到后端的数据存储都
集群架构融合为一体，可以为Web应用、数据库应用等提供安全、稳定的运行环境。更确切
用性、高可靠性、负载均衡、存储共享且经济实用的集群工具集合，它将集群系统中的三大
12.1RHCS集群概述
决方案，是企业级应用的首选集群软件。
的多个节点上。由此可知，RHCS提供了一个集群系统从前端负载到后端数据存储的完整解
务同时去读写一个单一的共享文件系统；通过负载均衡集群可以将客户端请求调度到集群中
高可用性；而存储集群可以在一个集群中为服务提供一个一致的文件系统映像，并且允许服
文件共享和节约成本的需要。高可用性集群通过消除单点故障点和节点故障转移功能来提供
件，可以在部署时采用不同的配置，以满足对高可用性、负载均衡、存储集群、可扩展性、
RHCS通过LVS来提供负载均衡集群，而LVS是一个开源的、功能强大的基于IP的负
高可用集群是RHCS的核心功能。当应用程序出现故障，或者系统硬件或网络出现故障
RHCS是RedHatClusterSuite的缩写，即红帽子集群套件。RHCS是一个能够提供高可
本章主要介绍RHCS 的安装、配置、管理和维护技巧。RHCS是一套综合的软件应用套
www.Linuxidc.com
第12章
，对用户来说都是透明的。通过故障转移功能，保证
RHCS集群
---
## Page 312
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
从根本上说，它是一种基于IPStorage理论的新型存储技术。RHCS可以通过iSCSI技术来
来管理共享存储。
步和复制的麻烦，但GFS并不能孤立存在，安装GFS需要RHCS的底层组件支持。
统允许多个服务同时读写一个磁盘分区，通过GFS可以实现数据的集中管理，免去了数据同
和算法合理地分配到各个服务节点，实时地、动态地、智能地负载分担。
置方式，通过LUCI可以轻松搭建一个功能强大的集群系统。
理（CCS）和栅设备（FENCE）。
工作，具体包括分布式集群管理器（CMAN）、成员关系管理、锁管理（DLM）、配置文件管
12.2.1
12.2
机制来协调和管理多个服务节点对同一个文件系统的读写操作。
件系统中来消除在应用程序间同步数据的麻烦。GFS是一个分布式文件系统，它通过锁管理
允许多个服务同时读写一个单一的共享文件系统，存储集群通过将共享数据放到一个共享文
个健康节点。
即internet SCSI，是IETF制订的一项标准，用于将SCSI数据块映射为以太网数据包。
iSCSI
集群逻辑卷管理，即CLVM，是LVM的扩展，这种扩展允许集群中的机器使用LVM
CLVM(Cluster Logical Volume Manager)
GFS是Red Hat公司开发的一款集群文件系统，目前的最新版本是GFS2。GFS文件系
GFS（Global File System）
RHCS除了上面的几个核心组成部分，还可以通过下面这些组件来补充RHCS集群功能。
LVS是一个开源的负载均衡软件，利用LVS可以将客户端的请求根据指定的负载策略
LVS
RHCS最新版本通过LUCI来配置和管理RHCS集群。LUCI是一个基于Web的集群配
口集群配置管理工具
提供节点服务监控和服务故障转移功能。当一个节点服务出现故障时，将服务转移到另
口高可用服务管理器
这是RHCS集群的一个基础套件，提供一个集群的基本功能，使各个节点组成集群一起
口集群构架管理器
RHCS是一个集群工具的集合，主要由下面几大部分组成：
RHCS通过GFS文件系统来提供存储集群功能。GFS是GlobalFileSystem的缩写，它
RHCS集群的组成与结构
RHCS集群的组成
www.Linuxidc.com
集群297
---
## Page 313
Linux公社（www.LinuxIDC.com）是专业的Linux系统门户网站，实时发布最新Linux资讯。
群拓扑结构如图12-1所示。
12.2.2
已经停止了开发，因此使用GNBD得越来越少。