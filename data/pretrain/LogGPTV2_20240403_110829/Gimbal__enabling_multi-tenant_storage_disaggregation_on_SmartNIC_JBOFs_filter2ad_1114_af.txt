tency of YCSB-F with 24 DB instances increases by 38.1% compared
with the 16 instance case. YCSB-C is a read-only workload, where 24
115
Gimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
(a) Throughput.
(b) Average read latency.
(c) 99.9th read latency.
Figure 10: RocksDB performance comparison over four approaches. We configure the YCSB to generate 10M 1KB key-value pairs with a Zipfian
distribution of skewness 0.99 for each DB instance.
Figure 12: Average read latency as increasing
the number of RocksDB instances.
Figure 11: Throughput as increasing the num-
ber of RocksDB instances.
instances still cannot saturate the NVMe-oF target read bandwidth,
and the average read latency varies little.
Virtual view enabled optimizations. This experiment examines
how the virtual view provided by Gimbal could help improve the
application performance. We followed the above RocksDB setting
and ran 8 DB instances (from two client servers) over one SmartNIC
JBOF. Figure 13 presents the read tail latency of five benchmarks
comparing three cases (i.e., vanilla w/o optimizations, w/ flow con-
trol, w/ flow control and load balancing). On average, across the
five workloads, the IO rate limiter enabled via our credit scheme
reduces the 99.9th latencies by 28.2% compared with the vanilla
one. The request load balancer, which could choose one replica that
has more bandwidth, further reduces the tail by 18.8%.
5.7 Overheads
We evaluate the overhead of Gimbal in two ways. First, we compare
the average CPU cycles of the submission and completion proce-
dure with a vanilla SPDK NVMe-oF target on SmartNIC. As shown
in Table 1a, Gimbal adds 37.5-62.5% more cycles to realize the stor-
age switch. Although Gimbal adds some overhead on average in
the pipeline, it does not hurt the performance for PCIe Gen3 SSD
as discussed in Section 2.4. Future PCIe Gen4 SSDs could achieve
~7GB/s bandwidth or 1 MIOPS. We run the 4KB read benchmark
with a NULL device (which does not perform actual IO and returns
immediately) to measure the maximum IOPS of Gimbal on Smart-
NIC. Gimbal performs 821 KIOPS with 1 SmartNIC core. Gimbal
could support a multi-core implementation if one can distribute
the active tenants to each core in a balanced manner. For example,
we extend the experiment to the 4-core case and find that Gimbal
achieves 2446 KIOPS, indicating that SmartNIC-based Gimbal can
support next-generation SSDs. We also expect Gimbal to scale up
with future powerful ARM cores or specialized engines.
5.8 Generalization
This experiment evaluates how Gimbal performs on a different type
of SSD. We run the same microbenchmark (Section 5.3) using Intel
DC P3600 1.2TB model. This SSD uses 2-bit MLC NAND, presenting
116
Figure 13: Performance improvement via the
SSD virtual view enabled optimizations.
Submit
Complete
Submit
Complete
Vanilla SPDK
Gimbal
32
16
21
17
52 (+62.5%)
22 (+37.5%)
30 (+42.9%)
25 (+47.1%)
1 workers (QD1)
16 workers (QD32)
(a) CPU cycle comparison (4KB Read, QD=Queue Depth, 125cycles=1usec)
1 CPU core, 1 worker
4 CPU cores, 8 workers
Vanilla SPDK
937 KIOPS
2692 KIOPS
Gimbal
821 KIOPS (-12.4%)
2446 KIOPS (-9.2%)
(b) The maximum IOPS with NULL device (4KB Read IO)
Table 1: Overhead comparison with vanilla SPDK
33.5% lower 128KB read (2.1GB/s) and 35.0% higher 4KB random
write (243MB/s) in terms of bandwidth. We tune the ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëéùë•
to 3ms for better read utilization as it achieves higher tail latency
than DCT983 for 128KB read. Gimbal adapts the characteristics of
the SSD and performs similarly to the DCT983 case in terms of the
ùëì -Util. Specifically, it shows 0.63 and 0.72 of ùëì -Util for read and
write under the clean condition and 0.58 and 0.90 for read and write
under the fragmented condition.
We also run Gimbal on Xeon E5-2620 v4 CPU and compare the
overhead with the vanilla SPDK. Gimbal performs 10.8% lower (1368
KIOPS) than the vanilla SPDK (1533 KIOPS) for 4KB read perfor-
mance with the NULL device, similar to the result on SmartNIC.
5.9 Summary
Table 2 summarizes the high-level differences between Gimbal and
the other approaches. Gimbal outperforms the other approaches be-
cause it dynamically estimates the available bandwidth and IO costs
for each storage device based on current conditions and workloads,
performs fine-grained fair queueing at the NVMe-oF target across
tenants, and uses credit-based flow control to adjust the client be-
havior. The other approaches lack one or more such support. For
example, ReFlex and FlashFQ use an approximate offline-profiled
SSD cost model that hurts scheduling efficiency. They also don‚Äôt
 0 100 200 300 400 500 600YCSB-AYCSB-BYCSB-CYCSB-DYCSB-FThroughput (KIOPS)BenchmarkReflexPardaFlashFQGimbal 0 100 200 300 400 500 600YCSB-AYCSB-BYCSB-CYCSB-DYCSB-FAvg. Latency (us)BenchmarkReflexPardaFlashFQGimbal 0 1000 2000 3000 4000 5000 6000 7000 8000YCSB-AYCSB-BYCSB-CYCSB-DYCSB-F99.9th Latency (us)BenchmarkReflexPardaFlashFQGimbal 0 100 200 300 400 500 6004812162024Throughput (KIOPS)RocksDB Instances (#)YCSB-AYCSB-BYCSB-CYCSB-DYCSB-F 0 50 100 150 2004812162024Avg. Latency (us)RocksDB Instances (#)YCSB-AYCSB-BYCSB-CYCSB-DYCSB-F 0 2000 4000 6000 8000 10000 12000YCSB-AYCSB-BYCSB-CYCSB-DYCSB-F99.9th Latency (us)BenchmarkVanillaVanilla+FCVanilla+FC+LBSIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Min and Liu, et al.
BW estimation
IO cost & WR tax
Fair queueing
Flow control
‚úó
‚úó
ReFlex
Static
Static
@Target
Parda
Dynamic
‚úó
@Client
FlashFQ
‚úó
Static
@Target
Gimbal
Dynamic
Dynamic
@Target
‚úì
Table 2: Comparison of four multi-tenancy mechanisms.
‚úì
regulate client-side IOs, causing significant delays. Parda employs
a client-side mechanism, which uses the end-to-end delay as the
congestion signal for its flow control, and performs limited fair
queueing within a server without coordination with other hosts.
Such a high latency feedback control loop is unsuitable for low-
latency high-bandwidth NVMe SSDs.
6 Related Work and Discussion
Shared storage with QoS guarantee. Prior work has explored
how to achieve high device utilization and fairness for local and
remote shared storage [38, 42, 49, 60, 64, 65, 70, 73, 76, 77]. As dis-
cussed in Section 5, these approaches cannot precisely estimate
the IO capacity (under different SSD conditions) and per-IO cost
(under various mixed workloads), causing inefficiencies when ap-
plied to the NVMe-oF based disaggregated setting. There are also
some timeslice-based IO schedulers (e.g., CFQ [25], Argon [77])
that allocate time quanta for fairness and provide tenant-level ex-
clusive access. They usually target millisecond-scale slow storage
and would hurt responsiveness, utilization, and fairness when used
with fast NVMe SSDs. Recently, researchers have tried to leverage
ML techniques (e.g., a light neural network) to predict the per-IO
performance [41]. The predicted result is a binary model (indicating
if an IO is fast or slow) and can only help improve the admission
control of storage applications for predictable performance, which
is insufficient to achieve fairness. IOFlow [76] proposes a software-
defined architecture for enforcing end-to-end policies of storage
IOs running in the data center. It adds queueing abstractions along
the data plane and relies on a centralized controller for translating
policies to queuing rules. We believe Gimbal can serve as one of its
execution stages at the storage server.
Remote storage IO stacks. Researchers [48] have characterized
the performance of iSCSI-based disaggregated storage and proposed
optimization opportunities, such as parallel TCP protocol process-
ing, NIC TSO offloading, jumbo frames, and interrupt affinity pin-
ning. ReFlex [49] provides a kernel-bypass data-plane to remove
the NVMe storage processing stack, along with a credit-based QoS
scheduler. Also, i10 [43] is an efficient in-kernel TCP/IP remote
storage stack using dedicated end-to-end IO paths and delayed
doorbell notifications. Researchers [39] have also characterized the
server-based NVMe-oF performance. Our work targets SmartNIC
JBOFs that use NVMe-oF protocols.
Disaggregated storage architecture. New hardware designs have
been proposed to address the limitations of existing disaggregation
designs. Sergey et al. [53] proposes four kinds of in-rack storage
disaggregation (i.e., complete, dynamic elastic, failure, and configu-
ration disaggregation) and explores their tradeoffs. Shoal [72] is a
power-efficient and performant network fabric design for disaggre-
gated racks built using fast circuit switches. LeapIO [55] provides
a uniform address space across the host Intel x86 CPU and ARM-
based co-processors in the runtime layer and exposes the virtual
NVMe driver to an unmodified guest VM. As a result, ARM cores
117
can run a complex cloud storage stack. We focus on emerging
SmartNIC-based disaggregated storage solutions.
Programmable packet scheduling. Researchers have explored
new HW/SW programmable packet schedulers. PIFO [74] proposes
a push-in first-out queue that enables packets to be pushed into
an arbitrary position based on their calculated rank and dequeued
from the head. PIEO [71] then improves PIFO scalability and expres-
siveness via two capabilities: providing each scheduling element a
predicate; dequeuing the packet with the smallest index from a sub-
set of the entire queue. SP-PIFO [22] further approximates the PIFO
design via coarse-grained priority levels and strict FIFO queues. It
dynamically adjusts the priority range of individual queues by modi-
fying the queuing threshold. PANIC [56] combines PIFO with a load
balancer and a priority-aware packet dropping mechanism to ex-
plore efficient multi-tenancy support on SmartNICs. Carousel [66]
and Eiffel [67] are two efficient software packet schedulers, which
rely on a timing wheel data structure and bucketed integer priority
for fast packet operations. Compared with these studies, a key dif-
ference is that Gimbal focuses on scheduling storage IO requests
among NVMe SSDs instead of individual packets. Exposing pro-
grammability from the Gimbal traffic manager to realize flexible
scheduling policies will be our future work.
Emerging storage media. QLC NAND has gained significant at-
tention because of its cost and capacity advantage over TLC. How-
ever, its performance characteristic is worse than TLC, presenting
a higher degree of read and write asymmetry [57]. We expect the
techniques introduced by Gimbal could also apply to QLC SSDs.
Gimbal could also support the 3DXP device. However, it is suitable
for a local cache rather than the disaggregated storage [80]. 3DXP
also provides a similar performance of read and write with in-place
update [79] and might not benefit from Gimbal as much as NAND
devices.
Hardware-accelerated Gimbal. The pipelined architecture of
Gimbal has a standard interface for both ingress and egress and can
be plugged into any hardware-accelerated NVMe-oF implementa-
tion. In addition, Gimbal itself is portable to a hardware logic with
ease using a framework such as Tonic [24].
7 Conclusion
This paper presents Gimbal, a software storage switch that enables
multi-tenant storage disaggregation on SmartNIC JBOFs. Gimbal
applies four techniques: a delay-based congestion control mecha-
nism for SSD bandwidth estimation, a new IO cost measurement
scheme, a two-level hierarchical I/O scheduling framework, and
an end-to-end credit-based flow control along with an exposed
SSD virtual view. We design, implement, and evaluate Gimbal on
the Broadcom Stingray PS1100R platforms. Our evaluations show
that Gimbal can maximize the SSD usage, ensure fairness among
multiple tenants, provide better QoS guarantees, and enable multi-
tenancy aware performance optimizations for applications. This
work does not raise any ethical issues.
Acknowledgments
This work is supported by Samsung and NSF grants CNS-2028771
and CNS-2006349. We would like to thank the anonymous reviewers
and our shepherd, Brent Stephens, for their comments and feedback.
Gimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
References
[1] 2017. Disaggregated or Hyperconverged, What Storage will Win the Enterprise?
https://www.nextplatform.com/2017/12/04/disaggregated-hyperconverged-st
orage-will-win-enterprise/.
[2] 2018. Free Your Flash And Disaggregate. https://www.lightbitslabs.com/blog/f
ree-your-flash-and-disaggregate/.
[3] 2018. Industry Outlook: NVMe and NVMe-oF For Storage. https://www.iol.unh.
edu/news/2018/02/08/industry-outlook-nvme-and-nvme-storage.
[4] 2019. What is Composable Disaggregated Infrastructure? https://blog.westerndi
gital.com/what-is-composable-disaggregated-infrastructure/.
[5] 2020. Alpha Data ADM-PCIE-9V3 FPGA SmartNIC. https://www.alpha-data.c
om/dcp/products.php?product=adm-pcie-9v3.
[6] 2020. AWS Nitro System. https://aws.amazon.com/ec2/nitro/.
[7] 2020. Broadcom FlexSPARX acceleration subsystem. https://docs.broadcom.co
m/doc/1211168571391.
[8] 2020. Broadcom Stingray PS1100R SmartNIC. https://www.broadcom.com/pro
ducts/storage/ethernet-storage-adapters-ics/ps1100r.
[9] 2020. Flexible I/O Tester. https://github.com/axboe/fio.
[10] 2020. Marvell LiquidIO SmartNICs. https://www.marvell.com/products/ethernet-
adapters-and-controllers/liquidio-smart-nics/liquidio-ii-smart-nics.html.
[11] 2020. Mellanox BlueField-2 SmartNIC. https://www.mellanox.com/products/bl
uefield2-overview.
[12] 2020. Mellanox Innova-2 Flex Open Programmable SmartNIC. https://www.me
llanox.com/products/smartnics/innova-2-flex.
[13] 2020. Netronome Agilio SmartNIC. https://www.netronome.com/products/agili
o-cx/.
specification/.
_ES.pdf.
[14] 2020. NVM Express Base Specification. https://nvmexpress.org/developers/nvme-
[15] 2020. NVM Express over Fabrics Specification. https://nvmexpress.org/develop
ers/nvme-of-specification/.
[16] 2020. The Intel Storage Performance Development Kit (SPDK). https://spdk.io/.
[17] 2020. Watts up? PRO. http://www.idlboise.com/sites/default/files/WattsUp_Pro
[18] 2021. Brocade G620 Switch. https://www.broadcom.com/products/fibre-channel-
networking/switches/g620-switch.
[19] 2021.
Cisco MDS 9000 Series Multilayer Switches.
https://www.cisc
o.com/c/en/us/products/storage-networking/mds-9000-series-multilayer-
switches/index.html.
[20] 2021. RocksDB. https://rocksdb.org.
[21] Nitin Agrawal, Vijayan Prabhakaran, Ted Wobber, John D Davis, Mark S Manasse,
and Rina Panigrahy. 2008. Design tradeoffs for SSD performance.. In USENIX
Annual Technical Conference.
[22] Albert Gran Alcoz, Alexander Dietm√ºller, and Laurent Vanbever. 2020. SP-PIFO:
approximating push-in first-out behaviors using strict-priority queues. In 17th
{USENIX} Symposium on Networked Systems Design and Implementation ({NSDI}
20).
[23] Thomas E Anderson, Susan S Owicki, James B Saxe, and Charles P Thacker. 1993.
High-speed switch scheduling for local-area networks. ACM Transactions on
Computer Systems (TOCS) 11, 4 (1993), 319‚Äì352.
[24] Mina Tahmasbi Arashloo, Alexey Lavrov, Manya Ghobadi, Jennifer Rexford,
David Walker, and David Wentzlaff. 2020. Enabling programmable transport pro-
tocols in high-speed NICs. In 17th {USENIX} Symposium on Networked Systems
Design and Implementation ({NSDI} 20). 93‚Äì109.
[25] Jens Axboe. 2004. Linux block IO‚Äîpresent and future. In Ottawa Linux Symp.
[26] Matias Bj√∏rling, Jens Axboe, David Nellans, and Philippe Bonnet. 2013. Linux
Block IO: Introducing Multi-Queue SSD Access on Multi-Core Systems. In Pro-