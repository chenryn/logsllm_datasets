A. Dataset annotation Systemtype System #Logs #Pairs #Temp. Unseen
We implement the SemParser framework on a public Mobilesystem Android 2,000 6,478 166 82.8%
dataset [27] containing log messages collected from Open- Operatingsystem Linux 2,000 2,905 118 86.8%
Stack for training. Considering that it is labor-intensive to
Hadoop 2,000 2,592 14 84.6%
annotatealargedatasetinareal-worldscenario,werandomly HDFS 2,000 3,105 30 47.0%
sample 200 logs from the dataset for human annotation, with Distributedsystem OpenStack 2,000 4,367 43 52.3%
Zookeeper 2,000 1,189 50 75.9%
the sample rate of 0.05%. A practical model should be able
to learn from a small amount of data. The trained model from
TABLE II: Statistics of anomaly detection datasets.
such data is named the “base model” for further evaluation.
All annotation is carried out as follows. For each log, we
Dataset #Message Anomaly rate
invite two post-graduate students experienced in OpenStack
HDFS dataset 11,175,629 3%
to independently manually label: (1) whether a word is a
F-Dataset 1,318,860 0.22%
concept, instance, or neither of both; and (2) the explicit
CI pairs within a sentence. If the two students provide the
same answer for one log, the answer will be regarded as
A. Experiment details
the ground-truth for training; otherwise another student will
1) RQ1–Semantic mining: Dataset. LogHub [31] is a
join them to discuss until a consensus is reached. The inter-
repository of system log files for research purposes, which
annotatoragreement[28]beforeadjudicationis0.881.Finally,
has been used by plenty of log-related studies [32]–[34]. We
we remove the sentences without any CI pair annotation
manuallylabelsixrepresentativelogfilesforsemanticmining
to mitigate the sparse data problem, yielding 177 labeled
evaluation ranging from distributed, operating, and mobile
messages for training the semantics miner.
systems. The dataset has a total of six different system log
B. Pre-trained word embeddings fileswith12,000logmessagesand20,636annotatedCIpairs.
Although existing pre-trained word embeddings show the Details are shown in Table I, where # Logs, # Pairs, # Temp.,
large success in representing semantics of words, it is not andUnseendenotesthenumberoflogmessages,CIpairs,log
appropriate for understanding logs. Log message is a very templates, and the percentage of unseen templates in the test
domain-specificlanguage,wherethewordshavequitedistinct set, respectively.
semantics from daily life. Hence, we train domain-specific Settings. As SemParser is an semantic-based parser, we
word embeddings on a representative cloud management sys- consider its semantic mining ability for evaluating how ef-
tem, OpenStack corpus. The corpus is made up of 203,838 fective is it when mining instance-level semantics from log
sentences crawled from its official website. We train the messages. Specifically, given a log message, we report the
pervasiveskip-grammodel[29]onGensim[30]fortenepochs correct proportion of the model’s extracted CI pairs (Preci-
and set the word embedding dimension to be 100. sion), the proportion of actually correct positives extracted by
the model (Recall), and their harmonic mean (F1 score). As
C. Implementation details
wehopethemodelcouldlearnsemanticsfromsmallsamples,
When implementing the model, we set the character-level we fine-tune the base model (i.e., train from Section IV) on a
embedding dimension to be 30. We select the two-layer deep small dataset 50 randomly sampled logs for each system and
bi-LSTM with a hidden size of 128. The model is trained for evaluate the performance on the remaining 1,950 logs.
30 epochs§ with an initial learning rate of 0.01. The learning 2) RQ2–Anomaly detection: Dataset. We evaluate the
rate decays at the rate of 0.005 after each epoch. It takes one anomaly detection performance on two datasets. (1) We first
hourfortraining,andthetrainedmodeloccupiesonly25MB. follow the previous studies to evaluate in the HDFS [35]
SemParser runs 25 messages per second in a single batch and dataset, which includes log messages by running map-reduce
single thread during inference. tasksonmorethan200nodes.(2)ThesecondF-Dataset[27]is
initiallycreatedforinvestigatingsoftwarefailuresbyinjecting
V. EVALUATION
396failuretestsinmajorsubsystemsofthewidelyusedcloud
We evaluate SemParser from two perspectives, the ability
computing platform OpenStack, covering 70% of bug reports
of semantic mining and the usefulness in downstream tasks,
in the issue tracker. For each failure injection test, the authors
with three research questions:
all log data in major subsystems, the labeled anomaly log
• RQ1:HoweffectiveistheSemParserinminingsemantics messages,aswellastheexceptionraisedbyaserviceAPIcall
from logs?
named as API Error, such as “server create error”. Statistics
• RQ2: How effective is the SemParser in anomaly detec- of both datasets are shown in Table II.
tion?
Settings.Intheanomalydetectiontask,thedetectorpredicts
• RQ3: How effective is the SemParser in failure identifi- whetheranomaliesexistwithinashortperiodoflogmessages
cation?
(i.e.,session).Motivatedbypreviousstudies[33],[36],wede-
§Themodelconvergeswithin30epochs. coupletheanomalydetectionframeworkintotwocomponents,
TABLE III: Sample log messages and ground-truth templates.
a log parser to generate templates, and a detection model
to analyze template sequences in a session. A dependable
Log AfterScheduling:PendingReds:1CompletedReds:0...
parser should perform well as a foundational processor for GT-Template AfterScheduling:PendingReds:CompletedReds:0...
loganalysis,regardlessofthedown-streamingdetectionmodel
Log TaskAttempt:[attempt 14451444]usingcontainerId...
used. In our experiments, we compare the performance of GT-Template TaskAttempt:[attempt ]usingcontainerId...
different baseline parsers under various anomaly detection
techniques.
Specifically, we compare SemParser to the following log looks deeper into the problems and identify what type of
parsers as baselines: (1) LenMa [37]. This online parser failure occurs. To make the F-Dataset appropriate for failure
encodes each log message into a vector, where each entry identification, we utilize the labeled anomaly log messages
refers to the length of the token. Then, it parses logs by and their corresponding API error in each injection test as the
comparing the encoded vectors; (2) AEL [38]. This paper input and ground-truth. Entirely, we collect 405 failures with
devises a set of heuristic rules to abstract values, such as 16 different types of API errors. With the splitting training
“value” in “word=value”; (3) IPLoM [39]. IPLoM partitions ratio of 0.5, we obtain 194 and 211 failures for the train and
event logs into event groups in three steps: partition by the test set, respectively. Typical API errors include “server add
length of the log; partition by token position; and partition volume error”, “network delete error” and so on.
by searching for bijection between the set of unique tokens; Settings. In this paper, we formulate the failure identifica-
(4) Drain [12]. It leverages a fixed depth parse tree with tiontaskasfollows:giventheanomalylogmessagesfromone
heuristic rules to maintain log groups. Its ability to parse logs injection test in F-Dataset, the model is required to determine
in a streaming and timely manner makes it popular in both whatAPIerroremerges.Similartotheanomalydetectiontask,
academia and industry. wealsocomparetheperformanceofdifferentbaselineparsers
We also reproduce four widely-applied anomaly detection associated with several log analysis models (i.e., DeepLog,
models as following: (1) DeepLog [40] employed a deep LogRobust, CNN, and Transformer). The only difference is
neural network, LSTM, to conduct anomaly detection and that we change the node number of the last prediction layer
fault localization on logs, taking the context information into of the above-mentioned techniques from 2 to 16 to make it a
account; (2) To handle the ever-changing log events and 16-class classification task for 16 error types in the dataset.
sequences during the software evolution, LogRobust [14] Recall@k is widely used in recommendation systems to
detected anomaly detection by an attention-based bi-LSTM assess whether the predicted results are relevant to the
network. The attention mechanism allows the model to learn user(s) [44], [45]. Similarly, we are also interested in whether
the different importance of log events; (3) CNN [41] is also top-k recommended results contain the correct API error.
utilizedtodetectanomaliesinbigdatasystemlogsinspiredby Hence, we report the Recall@k rate as the evaluation metric.
itsbenefitsingeneralNLPanalysis;and(4)Transformer.[42] 4) Discussion–log parsing comparison: In this section, we
detected anomalies in logs via the Transformer encoder [43] discuss why we do not compare SemParser to other syntax-
with a multi-head self-attention mechanism, allowing the basedparsersinthelogparsingtaskwhereonlythetemplates
model to learn context information. and parameters are extracted. Firstly, the ground-truth for
Whenconductingexperiments,wefeedparsingresultsfrom log parsing is not suitable for the semantic parser. For the
log messages into different models. Different from previous logs and their ground-truth templates shown in Table III with
work [14], [40]–[42] that only employs templates to form highlighted improper parts, we observe that “0” is not a
the input sequence x ,x ,...,x where x refers to the ith parameter but a token in the template, because the value for
0 1 m i
messageinthesequence,weequipthesequencewithextracted “CompleteReds” is always “0” in 2000 logs in this template.
semantics. Specifically, for each log message in the sequence, In contrast, “0” will be regarded as an instance in our model,
we concatenate template, concepts, instances as follows: since “0” is used to describe “CompleteReds” semantically.
Besides,weshowhowdifferenttokenizeraffectstheresultsin
x˜=[template;;sem 0;sem 1;...;sem n] (6) the second example, where we consider “attempt 14451444”
sem =[concept ;instance ]. (7) as an instance for the concept “TaskAttempt”, but the syntax-
i i i
based log parsers only regard the number “14451444” as
TospecifythecorrespondingrelationshipwithinaCIpair,we parameters, excluding the same prefix “attempt”. This kind
concatenate the concept and instance in sem i. Otherwise, an of widely-present distinction occurs 817 times among 2000
tokenreplacesanotherhalfpair,indicatingtheorphan logs in the Hadoop log collection. As a result, it is unfair
situation.Aspecialtokenisusedtoseparatetemplate to compare SemParser with syntax-based parsers in the log
and semantics. Afterwards, the sequence x˜ 0,x˜ 1,...,x˜ m con- parsing task. Instead, we investigate the semantic mining
taining m messages will be fed into the model for prediction. ability in the first research question.
Following previous anomaly detection work [14], [40]–[42], Secondly,logparsingismoreofapre-processingtechnique
we use Precision, Recall, and F1 as the evaluation metrics. for downstream applications rather than an application by
3) RQ3–Failureidentification: Dataset.Whileanomalyde- itself, and therefore, it will be more meaningful to concern
tectionidentifiespresentfaultsfromlogs,failureidentification about how the log parsers promote performance in down-
TABLE IV: Experimental results of mining semantics from logs.
System
Andriod Hadoop HDFS Linux OpenStack Zookeeper
Framework P — R — F1 P — R — F1 P — R — F1 P — R — F1 P — R — F1 P — R — F1
SemParser 0.9510.9350.943 0.9930.9780.985 1.0001.0001.000 0.9980.9770.987 0.9990.9980.999 1.0000.9890.995
-w/oF char 0.9810.9090.943 0.9880.9530.970 1.0000.9980.999 0.9950.9570.976 0.9950.9890.992 0.9930.9870.990
-w/oF local 0.9790.8580.915 0.9930.8800.933 1.0000.9990.999 0.9920.9470.969 0.9940.9890.992 0.9970.9400.968
-w/oLSTM 0.9790.8580.915 0.9930.8790.932 1.0000.9990.999 0.9950.9090.951 1.0000.9630.981 0.9660.9530.959
-w/oFcontx 0.9770.0600.113 0.9840.2530.403 0.9990.2890.449 0.9990.2420.389 1.0000.2560.407 0.8420.1970.319
TABLE V: Experiment results for anomaly detection.
in Hadoop as examples, there are several ways to describe an
(a) HDFS Dataset. instance associated with one concept TaskAttempt:
Technique • TaskAttempt: [attempt 14451444] using containerId ...
DeepLog LogRobust CNN Transformer
Baseline P R F1 P R F1 P R F1 P R F1 • attempt 14451444 TaskAttempt Transitioned from ...
LenMa .897.994.943 .914.995.953 .924.995.958 .872.908.890 • Progress of TaskAttempt attempt 14451444 is ...
AEL .896.994.943 .935.996.964 .922.995.958 .893.904.898 The evaluation result across six representative system logs is
Drain .908.994.949 .934.994.963 .925.995.959 .886.871.878
IPLoM .898.994.944 .940.994.966 .926.996.960 .889.904.896 presented in Table IV. Since our work is the first to extract
SemParser .940.995.967 .954.995.974 .931.995.962 .881.954.916 semantics from logs, we do not set baselines for comparison.
∆% +1.86% +0.82% +0.21% +2.00% Other general text mining techniques in the NLP field can
only extract keywords (e.g., LDA [17]), but they are not be
(b) F-Dataset
capable of extracting semantic pairs or parsing log messages
Technique
DeepLog LogRobust CNN Transformer to structured templates. Instead, we conduct ablation studies
Baseline P R F1 P R F1 P R F1 P R F1 to explore the effectiveness of each element in the semantics
LenMa .717.938.813 .714.924.806 .793.815.804 .685.896.776 miner, where w/o F , w/o F , w/o LSTM and w/o
AEL .738.934.824 .791.877.832 .747.924.826 .503.962.660 char local
Drain .824.867.845 .810.886.846 .737.943.827 .693.919.790 F contx refers to removing the character-level feature, local
IPLoM .863.833.848 .808.877.841 .834.834.834 .929.683.787 word feature, LSTM network, and interval context, respec-
SemParser .971.927.948 .952.913.932 .907.899.903 .938.904.921
tively. The best F1 score for each system is in bold fonts.
∆% +11.80% +10.17% +8.27% +16.58%
Inconclusion,ourmodelcouldextractnotonlyhighquality
butalsocomprehensiveinstance-levelsemanticsfromlogmes-
sages.WeachieveanaverageF1scoreof0.985forsixsystems