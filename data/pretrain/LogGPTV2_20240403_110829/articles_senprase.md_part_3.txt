### A. Dataset Annotation

We implemented the SemParser framework using a public dataset [27] that contains log messages collected from OpenStack for training. Given the labor-intensive nature of annotating large datasets in real-world scenarios, we randomly sampled 200 logs from the dataset for human annotation, with a sample rate of 0.05%. A practical model should be able to learn from a small amount of data. The trained model from this data is referred to as the "base model" for further evaluation.

#### Table I: Statistics of Annotated Datasets

| System Type | #Logs | #Pairs | #Templates | Unseen Templates (%) |
|-------------|-------|--------|------------|----------------------|
| Android     | 2,000 | 6,478  | 166        | 82.8%                |
| Linux       | 2,000 | 2,905  | 118        | 86.8%                |
| Hadoop      | 2,000 | 2,592  | 14         | 84.6%                |
| HDFS        | 2,000 | 3,105  | 30         | 47.0%                |
| OpenStack   | 2,000 | 4,367  | 43         | 52.3%                |
| Zookeeper   | 2,000 | 1,189  | 50         | 75.9%                |

All annotations were carried out by two post-graduate students experienced in OpenStack, who independently labeled each log. They identified whether a word was a concept, instance, or neither, and marked explicit concept-instance (CI) pairs within a sentence. If both students provided the same answer, it was considered the ground truth; otherwise, a third student joined the discussion until a consensus was reached. The inter-annotator agreement before adjudication was 0.881. Finally, sentences without any CI pair annotations were removed to mitigate the sparse data problem, resulting in 177 labeled messages for training the semantics miner.

### B. Pre-trained Word Embeddings

Although existing pre-trained word embeddings have shown significant success in representing word semantics, they are not suitable for understanding logs. Log messages are highly domain-specific, with words having distinct meanings from everyday language. Therefore, we trained domain-specific word embeddings on a representative cloud management system, the OpenStack corpus, which consists of 203,838 sentences crawled from its official website. We used the pervasive skip-gram model [29] on Gensim [30] for ten epochs, setting the word embedding dimension to 100.

### C. Implementation Details

For the model implementation, we set the character-level embedding dimension to 30 and used a two-layer deep bi-LSTM with a hidden size of 128. The model was trained for 30 epochs with an initial learning rate of 0.01, which decayed at a rate of 0.005 after each epoch. Training took one hour, and the trained model occupied only 25 MB. During inference, SemParser processes 25 messages per second in a single batch and single thread.

### V. Evaluation

We evaluated SemParser from three perspectives: semantic mining, anomaly detection, and failure identification, addressing the following research questions:

1. **RQ1: How effective is SemParser in mining semantics from logs?**
2. **RQ2: How effective is SemParser in anomaly detection?**
3. **RQ3: How effective is SemParser in failure identification?**

#### 1. RQ1 - Semantic Mining

**Dataset:**
- **LogHub [31]**: A repository of system log files for research purposes, widely used in log-related studies [32]â€“[34]. We manually labeled six representative log files for semantic mining, covering distributed, operating, and mobile systems. The dataset includes 12,000 log messages and 20,636 annotated CI pairs.

**Settings:**
- We evaluated the model's ability to extract CI pairs from log messages. Specifically, given a log message, we reported the precision, recall, and F1 score of the extracted CI pairs.

**Results:**
- The evaluation results across six representative system logs are presented in Table IV. Since our work is the first to extract semantics from logs, we did not set baselines for comparison. Instead, we conducted ablation studies to explore the effectiveness of each element in the semantics miner.

| System | P | R | F1 |
|--------|---|---|----|
| Android | 0.951 | 0.935 | 0.943 |
| Hadoop | 0.993 | 0.978 | 0.985 |
| HDFS | 1.000 | 1.000 | 1.000 |
| Linux | 0.998 | 0.977 | 0.987 |
| OpenStack | 0.999 | 0.998 | 0.999 |
| Zookeeper | 1.000 | 0.989 | 0.995 |

#### 2. RQ2 - Anomaly Detection

**Dataset:**
- **HDFS [35]**: Includes log messages from map-reduce tasks on more than 200 nodes.
- **F-Dataset [27]**: Created for investigating software failures by injecting 396 failure tests in major subsystems of OpenStack, covering 70% of bug reports in the issue tracker.

**Settings:**
- The detector predicts whether anomalies exist within a short period of log messages (i.e., a session). We decoupled the anomaly detection framework into two components: a log parser to generate templates and a detection model to analyze template sequences in a session.

**Results:**
- The evaluation results for anomaly detection are presented in Table V. We compared SemParser with several baseline parsers and anomaly detection models.

| Technique | P | R | F1 |
|-----------|---|---|----|
| DeepLog | 0.897 | 0.994 | 0.943 |
| LogRobust | 0.914 | 0.995 | 0.953 |
| CNN | 0.924 | 0.995 | 0.958 |
| Transformer | 0.872 | 0.908 | 0.890 |
| LenMa | 0.896 | 0.994 | 0.943 |
| AEL | 0.896 | 0.994 | 0.943 |
| Drain | 0.908 | 0.994 | 0.949 |
| IPLoM | 0.898 | 0.994 | 0.944 |
| SemParser | 0.940 | 0.995 | 0.967 |

#### 3. RQ3 - Failure Identification

**Dataset:**
- **F-Dataset [27]**: Used for failure identification, where we utilized the labeled anomaly log messages and their corresponding API errors in each injection test as input and ground-truth. We collected 405 failures with 16 different types of API errors, split into 194 and 211 failures for the train and test sets, respectively.

**Settings:**
- The task is to determine the type of API error from the anomaly log messages. We compared SemParser with several baseline parsers and log analysis models, changing the node number of the last prediction layer to make it a 16-class classification task.

**Results:**
- The evaluation results for failure identification are presented in Table V.

| Technique | P | R | F1 |
|-----------|---|---|----|
| DeepLog | 0.717 | 0.938 | 0.813 |
| LogRobust | 0.714 | 0.924 | 0.806 |
| CNN | 0.793 | 0.815 | 0.804 |
| Transformer | 0.685 | 0.896 | 0.776 |
| LenMa | 0.738 | 0.934 | 0.824 |
| AEL | 0.738 | 0.934 | 0.824 |
| Drain | 0.824 | 0.867 | 0.845 |
| IPLoM | 0.863 | 0.833 | 0.848 |
| SemParser | 0.971 | 0.927 | 0.948 |

### Discussion - Log Parsing Comparison

In this section, we discuss why we do not compare SemParser to other syntax-based parsers in the log parsing task. The ground-truth for log parsing is not suitable for the semantic parser. For example, in the logs and their ground-truth templates, "0" is not a parameter but a token in the template because the value for "CompletedReds" is always "0" in 2000 logs. In contrast, "0" is regarded as an instance in our model, semantically describing "CompletedReds." Additionally, different tokenizers affect the results, as seen in the second example where "attempt 14451444" is considered an instance for the concept "TaskAttempt," but syntax-based log parsers only regard the number "14451444" as parameters, excluding the prefix "attempt." This distinction occurs 817 times among 2000 logs in the Hadoop log collection, making it unfair to compare SemParser with syntax-based parsers in the log parsing task. Instead, we focus on the semantic mining ability in the first research question.

### Conclusion

Our model can extract high-quality and comprehensive instance-level semantics from log messages, achieving an average F1 score of 0.985 across six systems.