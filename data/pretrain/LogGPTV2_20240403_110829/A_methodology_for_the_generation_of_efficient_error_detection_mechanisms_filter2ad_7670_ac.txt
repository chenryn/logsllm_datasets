is a random number between 0 and 1. Oversampling with
replacement is a special case of SMOTE where q is 0.
The skewed nature of datasets generated by fault injection,
particularly when using a data value fault model, means that
it is appropriate, when using certain algorithms, to perform
some attribute transformation before data mining begins. For
example, when the intention is to use learning algorithms
such as Logistic Regression or Na¨ıve Bayes, it would be
appropriate to map the original attribute values using the
function:
(cid:26) log(xi + 1)
− log(|xi| + 1)
g(xi) =
if xi ≥ 0
if xi < 0
In reality the three stated aims of data preprocessing may
not be fully realised at this stage of the methodology. For
example, the transformation of data formats and the learning
enhancement techniques are likely to be simple processes
that can be contained to the preprocessing stage. However,
the task of addressing class imbalance can not completed
until data mining has been used to generate some initial
model, hence it is an aim that is only realised during the
optimisation of the model, i.e. during the fourth step of the
methodology.
D. Step 3: Data Mining / Model Generation
The aim of this step is to generate predicates for error detec-
tion mechanisms from the transformed fault injection data.
We therefore use the symbolic machine learning algorithm
chosen in the previous step and apply it to the transformed
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
Efﬁcient Error Detection MechanismsStep 3:Data Mining / Model Generation(Section IV.D)Step 2:Algorithm Selection and Preprocessing(Section IV.C)Step 1:Fault Injection Analysis(Section IV.B)Step 4:Model Reﬁnement and Optimisation(Section IV.E)Locations for Error Detection Mechanisms30fault injection data. At this point the aim is not to generate
an efﬁcient detector, where efﬁciency is deﬁned with respect
to its accuracy and completeness, but to establish a baseline
model that can be subsequently optimised and reﬁned. The
evaluation of the derived predicate may take place by equip-
ping the relevant location in the target system with a runtime
assertion that implements the predicate or by evaluating
the effectiveness with which the predicate classiﬁes fault
injection data that was not used in predicate generation. In
either case, the aim is to evaluate the effectiveness of the
predicate on previously unseen data in order to measure its
accuracy and completeness.
E. Step 4: Model Reﬁnement and Optimisation
Once a baseline predicate has been derived and evaluated, it
may be reﬁned in order to improve its level of accuracy
and completeness. This can be achieved by varying the
parameters associated with the conﬁguration of the adopted
learning algorithm. In particular, it is useful to vary the levels
of undersampling and oversampling, including number of
nearest neighbours used, in order to establish the parameters
which yield the most effect predicate.
It is possible to generate a predicate for a perfect error
detection mechanism, i.e., a predicate that is both accurate
and complete for a program location. However, due to
theoretical constraints, this is not always achievable [9].
Thus, it may often be the case that a predicate can not be
reﬁned beyond a certain level of accuracy and completeness.
VI. EXPERIMENTAL SETUP
In this section we detail the experimental setup used in the
generation of the fault injection data sets.
A. Context
At this point, we wish to set the context: A target system
whose dependability is to be enhanced is instrumented so
that fault
injection can be performed on it. When fault
injection is performed in a given module of the target system,
a set of fault injection locations is chosen, as well as a
set of sampling locations. The set of sampling locations
corresponds to the set of program locations in that module
where detectors may need to be located. Such locations can
be obtained using techniques such as in [14]. A set of fault
injection locations is chosen to determine whether learning
of predicates is improved. For example, we may wish to
injection errors at the start of a module, and sample at
the end. Such a process will yield one type of predicate.
On the other hand, we may inject errors at the end of a
module, and sample straight after the injection, as in [6],
yielding a potentially different predicate. In such a case,
the more efﬁcient predicate can then be located at the end
of the module. As future work, we plan to investigate the
relationship between injection and sampling locations in the
generation of efﬁcient predicates.
Once fault injection data is obtained, they are prepro-
cessed according to Step 2 of the methodology, as detailed
in Section V, and the chosen symbolic machine learning
algorithm is applied to the data to generate the required
predicates. Here, a state in the fault injection data is clas-
siﬁed as either failure-inducing in that it leads to failure of
the system, which occurs when a speciﬁcation is violated,
or non failure-inducing, when the state does not lead to a
failure of the system. Thus, a generated predicate will ﬂag a
state that leads to system failure as erroneous. The predicate
is subsequently reﬁned to improve its efﬁciency.
B. Target Systems
7-Zip (7Z): The 7-Zip utility is a high-compression archiver
which supports a variety of ﬁle archiving and encryption
formats [40]. The target system was chosen because it is
widely-used and has been developed by many different
software engineers. Most source code associated with this
target system is available under the GNU Lesser General
Public License.
Flightgear (FG): The FlightGear Flight Simulator project is
an open-source project which aims to develop an extensible
yet highly sophisticated ﬂight simulator to serve the needs
of the academic and hobbyists communities [41]. The
target system was chosen because it is modular, contains
over 220,000 lines of code and simulates a situation where
dependability is critical. All source code and resources are
available under the GNU General Public License.
Mp3Gain (MG): The Mp3Gain ﬁle analyser is an open-
source volume normalisation software for mp3 ﬁles [42].
The system is modular and widely-used, but has been
predominantly developed by a single software engineer. All
source code and resources associated with this target system
are available under the GNU General Public License.
C. Test Cases
7Z: An archiving procedure was executed in all test cases.
A set of 25 ﬁles were input to the procedure, each of which
was compressed to form an archive and then decompressed
in order to recover the original content. The temporal
impact of faults was measured with respect to the number
of ﬁles processed. For example, if a fault were injected
during the processing of ﬁle 15 and persisted until
the
end of a test case,
impact would be
10. To create a varied and representative system load, the
experiments associated with each instrumented variable
were repeated for 250 distinct test cases, where each test
case involved a distinct set of 25 input ﬁles.
then its temporal
FG: A takeoff procedure was executed in all test cases.
This procedure executed for 2700 iterations of the main
simulation loop, where the ﬁrst 500 iterations correspond
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
31to an initialisation period and the remaining 2200 iterations
correspond to pre-injection and post-injection periods. A
control module was used to provide a consistent
input
vector at each iteration of the simulation. To create a varied
and representative system load, the experiments associated
with each instrumented variable were repeated for 9 distinct
test cases; 3 aircraft masses and 3 wind speeds uniformly
distributed across 1300-2100lbs and 0-60kph respectively.
MG: A volume-level normalisation procedure was executed
in all test cases. The procedure took a set of 25 mp3 ﬁles of
varying sizes as input and normalised the volume across each
ﬁle. The temporal impact of injected faults was measured
with respect to the number of ﬁles processed. To create
a varied and representative system load, the experiments
associated with each instrumented variable were repeated
for 250 distinct test cases, where each test case involved a
distinct set of 25 input ﬁles.
D. System Instrumentation
Instrumented modules in each target system were chosen
randomly from all sufﬁciently large modules used in the
execution of the aforementioned test cases. All variables
in the scope of each chosen module were instrumented
for fault injection. Code locations for instrumentation were
chosen based on the need to identify preconditions and
postconditions for the execution of instrumented modules.
Hence, the entry-point and exit-point of each module were
instrumented locations. An instrumentation location was a
point where a fault could be injected or the state of a
module sampled. A fault injection must be performed before
state was sampled, hence three fault injection data sets were
generated for each instrumented module. A description of
the fault injection data sets used in this paper can be found
in Table II.
E. Fault Injection and Logging
The Propagation Analysis Environment (PROPANE) was
used for fault injection and logging [12]. A golden run
was created for each test case, where a golden run is a
reproducible fault-free run of the system for a given test case,
capturing information about the state of the system during
execution. Bit ﬂip faults were injected at each bit-position
for all instrumented variables. Each injected run entailed a
single bit-ﬂip in a variable at one of these positions, i.e.
no multiple injection were performed. For FG each single
bit-ﬂip experiment was performed at 3 distinct injection
times uniformly distributed across the 2200 simulation loop
iterations that follow system initialisation, i.e. 600, 1200 and
1800 control loop iterations after the initialisation period
of 500 iterations. For 7Z and M3, each single bit-ﬂip
experiment was performed at 4 distinct injection times..
The state of all modules used in the execution of all test
SUMMARY OF FAULT INJECTION DATASETS
Table II
Module
Target
Name
System
FHandle
7-Zip
FHandle
7-Zip
FHandle
7-Zip
LDecode
7-Zip
LDecode
7-Zip
LDecode
7-Zip
Gear
FlightGear
Gear
FlightGear
FlightGear
Gear
FlightGear Mass
FlightGear Mass
FlightGear Mass
Dataset
Name
7Z-A1
7Z-A2
7Z-A3
7Z-B1
7Z-B2
7Z-B3
FG-A1
FG-A2
FG-A3
FG-B1
FG-B2
FG-B3
MG-A1 MP3Gain
MG-A2 MP3Gain
MG-A3 MP3Gain
MG-B1 MP3Gain
MG-B2 MP3Gain
MG-B3 MP3Gain
GAnalysis
GAnalysis
GAnalysis
RGain
RGain
RGain
Injection
Location
Entry
Entry
Exit
Entry
Entry
Exit
Entry
Entry
Exit
Entry
Entry
Exit
Entry
Entry
Exit
Entry
Entry
Exit
Sample
Location
Entry
Exit
Exit
Entry
Exit
Exit
Entry
Exit
Exit
Entry
Exit
Exit
Entry
Exit
Exit
Entry
Exit
Exit
cases was monitored and recorded during each fault injection
experiment.
F. Failure Speciﬁcation
7Z: A test case execution was considered a failure if the set
of archive ﬁles and recovered content ﬁles were different
from those generated by the corresponding golden run.
FG: A failure speciﬁcation was established using of golden
run observation and relevant aviation information. A failure
in the execution of a test case was considered to fall into at
least one of three categories; speed failure, distance failure
and angle failure. A run was considered a speed failure if
the aircraft failed to reach a safe takeoff speed after ﬁrst
passing through critical speed and velocity of rotation. A
run was considered a distance failure if the takeoff distance
exceeds that speciﬁed by the aircraft manufacturer, where
the speciﬁed distance is increased by 10 meters for every
additional 200lbs over the aircraft base-weight. A run was
considered an angle failure if a Pitch Rate of 4.5 degrees
is exceeded before the aircraft is clear of the runway or the
aircraft stalls during climb out.
MG: A test case execution was considered a failure if the
set of normalised output ﬁles were different from those
generated by the corresponding golden run.
VII. RESULTS
In this section, we demonstrate each step of our methodol-
ogy, as well as the quality of the results that can be achieved,
by applying the approach to three complex software systems.
A. Step 1: Fault Injection Analysis
Fault injection analysis was conducted on the target sys-
tems under the experimental conditions described in Section
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
32VI. The results of this fault injection were stored in the