function is allowed. These restrictions are necessary be-
cause, after program termination, the enclave must up-
date the authoritative state with the changes, and pass
updated values to the next invocation of the same action
function. An alternative approach that we have not ex-
501plored would be to introduce synchronization primitives
in the language. This however might lead to a perfor-
mance penalty while accessing shared state. Overall,
the programs should restrict writable state to the ﬁnest
granularity possible.
3.4.5 Enclave-Controller interface
The controller can program enclaves through the en-
clave API. This allows the controller to create, delete
and query both tables and individual match-action rules
in an enclave. We omit the API details for brevity.
3.5 Network support
Eden’s end-host based approach to network functions
requires two features from the network: the ability to
specify a packet’s priority and its route. For the for-
mer, we use 802.1q tags. For the latter, recent pro-
posals have shown how existing technologies like MPLS
and VLANs can be used to achieve source routing in
datacenters [44, 16]. Here, end hosts specify the path
of a network packet as a label in the packet header, e.g.,
through an MPLS label or VLAN tag, and switches per-
form label-based forwarding.
In our experiments, we
use VLAN tagging for route control.
Implementation
Such source routing requires the controller to conﬁg-
ure the label forwarding tables at switches. For MPLS,
this can be achieved through a distributed control pro-
tocol like LDP [5] while for VLANs, Spain [44] describes
a solution involving multiple spanning trees. Hence,
label-based forwarding and the corresponding control
protocol is the primary functionality Eden requires of
the underlying network. Additionally, most existing
switches already support statistics gathering capabili-
ties (e.g SNMP) and priority-based queuing.
4
We now provide more information about the implemen-
tation of the enclave and stages including current limi-
tations. We also describe our evaluation testbed.
4.1
The Eden interpreter supports relatively small programs
that use limited (operand) stack and heap space. The
execution is stack based, similar in spirit to the Java
Virtual Machine (JVM); we support many of the stan-
dard JVM op-codes. Apart from op-codes that specify
basic load and store, arithmetic, branches, and condi-
tionals, we have also implemented as op-codes a limited
set of basic functions, such as picking random num-
bers and accessing a high-frequency clock in the sys-
tem. Unlike general VMs, we do not support many
features such as exceptions, objects, and just-in-time
compilation. An alternative design would be to base
the interpreter on a register machine (as in [41]).
Interpreter and enclave
Section 3.4.4 mentioned a number of facilities ex-
pected by the enclave that we have implemented: (a)
managing ﬂow and function speciﬁc state, (b) preparing
program inputs and consuming program outputs, and
(c) overseeing program invocation. Overall, the enclave,
including the interpreter, is about 16K lines of C code.
4.2 Stages
Stages associate class and metadata information with
(application) messages; packets generated as a result
of those messages will be carrying the extra informa-
tion and processed by the interpreter accordingly. The
implementation of that mechanism in its pure form re-
quires a large number of changes in the current network
stack, and application and library code. As a proof of
concept, we have extended the socket interface to imple-
ment an additional send primitive that accepts class and
metadata information (our extension works for sockets
using TCP). At the kernel, we record the sequence num-
ber of the sender along with the extra information.
At the bottom of the network stack, we intercept
packets before they get transmitted. When we detect
that they contain data with the appropriate sequence
numbers, we update the relevant message state, and
schedule the packet for processing inside the enclave.
The mechanism above allows us to associate class and
metadata information at a per-packet granularity. We
believe that a generic mechanism for associating meta-
data with events that traverse application and kernel
network stacks could be of general use (see also [11]).
4.3 Testbed
Our Eden implementation targets traditional OS net-
work stacks and programmable NICs, and we thus de-
ploy two separate testbeds.
Our software testbed comprises ﬁve machines, four
Intel XEON W3530 (2.8GHz/12GB RAM), and one
Intel XEON E7450 (2.4GHz/32GB). All machines use
10GbE Mellanox network cards, and are connected through
an Arista 7050QX-32S-R switch. The enclave runs in-
side a Windows network ﬁlter driver. We added a new
ioctl that allows the application to communicate meta-
data to the driver, and used that to implement the
(socket-like) message send call.
Our programmable NIC testbed consists of four In-
tel Xeon E5-1620 (3.70GHz/16GB). Each machine is
equipped with a dual-port 10GbE Netronome NFE-3240
programmable NIC [46]. These NICs descend from In-
tel’s IXP network processors [31]. The NICs include a
40 core, 8 way multi-threaded CPU (320 threads), and
4GB of memory. We perform CAM operations on main
memory (other models of the processor can oﬄoad them
to a large TCAM). The network switch is a 40x10Gbps
Blade RackSwitch.
We have extended the NIC ﬁrmware to be able to run
the Eden interpreter. Beyond that, we can also execute
control programs and keep state in the NIC. Currently,
we have limited support for stages in our programmable
NIC testbed.
502Figure 9: Average ﬂow completion times (FCT) for small (left) and intermediate (right) ﬂows with
95% conﬁdence intervals. The top bars show the average while the bottom the 95th percentile.
5 Evaluation
Our evaluation of Eden spans three axes:
i) It high-
lights Eden’s performance while implementing diverse
network functions, ii) it covers functions implemented
in an OS network stack and in programmable NICs,
and iii) it provides micro-benchmarks evaluating Eden’s
data-plane overheads. We consider the three case stud-
ies from Section 2.
5.1 Case Study 1: Flow Scheduling
Our ﬁrst case study describes how Eden implements
network functions that schedule ﬂows so as to reduce
ﬂow completion times. We examine two such func-
tions: PIAS, whose logic was discussed in Figure 4 and
shortest ﬂow ﬁrst scheduling (SFF). PIAS requires data-
plane computation and state to enable tracking ﬂow
sizes and tagging packets with priorities depending on
thresholds based on the distribution of ﬂow sizes. SFF,
instead, does not track traﬃc ﬂow sizes but it requires
applications to provide the ﬂow size to the Eden enclave,
so that the correct priority is enforced.
To assess the impact of Eden on the performance of
these functions, we compare Eden with a “native” imple-
mentation. The latter implements a hard-coded func-
tion within the Eden enclave instead of using the in-
terpreter, similar to a typical implementation through
a customised layer in the OS [8]. Figure 7 shows the
PIAS action function in our language.
The workload driving the experiments is based on a
realistic request-response workload, with responses re-
ﬂecting the ﬂow size distribution found in search appli-
cations [2, 8]. Such applications generate traﬃc mostly
comprising small ﬂows of a few packets with high rate
of ﬂows starting and terminating.
In our set-up, one
worker responds to requests generating load at roughly
70%, while other sources generate background traﬃc at
the same time. Priority thresholds were set up for three
classes of ﬂows: small (<10KB), intermediate (10KB-
1MB) and background. Small ﬂows have the highest
priority, followed by the intermediate ones.
Figure 9 shows the average and the 95th-percentile
of the ﬂow completion times for small and intermedi-
ate ﬂows when executing the two functions. The ﬁg-
ure shows results when running natively in the OS and
through the Eden interpreter across ten runs of the ex-
periments. Baseline highlights ﬂow completion times
without any prioritization. We report two Baseline ﬁg-
ures, native and Eden. The latter shows the overhead
of running the classiﬁcation and data-plane functions,
but ignoring the interpreter output before packets are
transmitted (i.e., the priority marks are not inserted in
the packet header). Essentially, the overhead reﬂects
running the Eden enclave over vanilla TCP.
As expected, enabling prioritization signiﬁcantly re-
duces the ﬂow completion times; for small ﬂows, ﬂow
completion times reduce from 363µs to 274µs on the av-
erage, and from 1.6ms to 1ms at the 95th percentile, an
overall a reduction of 25%-40%. Similar trends are ob-
served for intermediate ﬂows as well. At the same time,
background traﬃc manages to saturate the rest of the
link capacity. SFF, by utilizing application knowledge,
typically provides slightly better performance with less
variability. In SFF, the mapping of ﬂows to classes oc-
curs when the ﬂow starts, and ﬂows do not change pri-
orities over time.
While similar observations have been reported in pre-
vious work, what is important in our context is that the
performance of the native implementation of the policy
and the interpreted one are similar.
In all cases the
diﬀerences are not statistically signiﬁcant.
5.2 Case Study 2: Load-balancing
The second case study examines how Eden enables per-
packet WCMP on top of our programmable NIC testbed.
The WCMP function has been discussed in Section 2
(Figure 2). We arranged the topology of our testbed to
emulate the topology of Figure 1, with two hosts con-
nected through two paths, one 10Gbps and one 1Gbps.
 50 100 150 200 250 300 350 400 450 500baselinePIASSFFFCT (usec)nativeEDEN 0 5000 10000 15000 20000 25000 30000 35000baselinePIASSFFFCT (usec)nativeEDEN 0 200 400 600 800 1000 1200 1400 1600 1800 2000baselinePIASSFFFCT (usec)nativeEDEN 0 5000 10000 15000 20000 25000 30000 35000baselinePIASSFFFCT (usec)nativeEDEN503Figure 10: Aggregate throughput for ECMP and
WCMP. Conﬁdence intervals are within 2% of
the values shown.
The programmable NICs run our custom ﬁrmware
that implements the enclave and the Eden interpreter.
The interpreted program controls how packets are source-
routed through the two paths. We use per-packet bal-
ancing, instead of per-ﬂow balancing to highlight Eden’s
ability to apply per-packet functions at high rate. In the
default case, paths are selected for long-running TCP
ﬂows with equal weights, thus implementing ECMP. For
WCMP, we enforce a ratio 10:1.
Figure 10 shows the average throughput achieved for
the two functions with a native program implemented
directly on the programmable NICs and with Eden. For
both ECMP and WCMP, Eden’s overheads are negligi-
ble with the diﬀerence between Eden and native not
being statistically signiﬁcant. In ECMP, TCP through-
put is dominated by the capacity of the slowest path,
and throughput peaks at just over 2Gbps as expected.
Instead, with per-packet WCMP, TCP throughput is
around 7.8Gbps, 3x better than ECMP. The through-
put is lower than the full 11Gbps which is our topology’s
min cut due to in-network reordering of packets [29].
Modifying TCP’s congestion control algorithm can al-
leviate such issues [53]; however, the goal of this case-
study is to highlight how Eden can easily implement
network functions such as weighted selection of paths
without any impact on performance–in this case with
unmodiﬁed applications and while running vanilla TCP.
5.3 Case Study 3: Datacenter QoS
We now examine Pulsar’s rate control as described in
Figure 3. The experiment involves two tenants running
our custom application that generates 64K IOs. One of
the tenants generates READ requests while the other
one WRITEs to a storage server backed by a RAM disk
drive. The storage server is connected to our testbed
through a 1Gbps link.
The results in Figure 11 show that when WRITE re-
quests compete with READs, their throughput drops by
72%. As previously discussed, this reﬂects the asymme-
try of IO operations; READs are small on the forward
path and manage to ﬁll the queues in shared resources.
Instead, we account for this mismatch through Pulsar’s
rate control, by charging READ requests based on the
request size and WRITEs on the packet size. This en-
sures equal throughput between the two operations.
Figure 11: Average READ vs. WRITE through-
put when requests run in isolation, simulta-
neously, and when READ requests are rate-
controlled based on the request size. 95% conﬁ-
dence intervals are <0.2% of the mean values.
Figure 12: CPU overheads incurred by Eden
components compared to running the vanilla
TCP stack. API refers to passing metadata in-
formation to the enclave.
5.4 Overheads
The previous case studies have focused on the overheads
incurred by Eden with respect to application metrics.
Here, we quantify Eden’s CPU overheads and memory
footprint of the enclave.
Figure 12 quantiﬁes Eden overheads while running
the SFF policy (case study 1). The workload comprises
12 long-running TCP ﬂows in one of our testbed nodes,
and Eden can saturate the 10Gbps capacity of its link.
The ﬁgure shows how the diﬀerent components of Eden
contribute to the CPU load. We consider these over-
heads reasonable. Note however, that saturating the
link capacity assumes that the cycle budget of the in-
terpreted program allows for such speeds. As we discuss
in the following section, Eden intentionally avoids pos-
ing any restrictions in the cycle budget of data-plane
functions.
In the examples discussed in the paper, the (operand)
stack and heap space of the interpreter are in the order
of 64 and 256 bytes respectively.
6 Discussion
The previous sections discussed Eden’s design and demon-
strated its expressiveness through case studies. Here, we
brieﬂy complement the discussion of our design choices
and highlight interesting issues for future research.
Choice of language and environment. Beyond the
beneﬁts discussed in Section 3.4 (i.e., expressiveness,
portability, safety, dynamically-injected programs), the
choice of the DSL and the interpreted environment were
 0 1000 2000 3000 4000 5000 6000 7000 8000 9000ECMPWCMPThroughput (Mb/sec)nativeEDEN 20 40 60 80 100 120 140IsolatedSimultaneousRate-controlledThroughput (MB/sec)ReadsWrites 0 2 4 6 8 10average95th-perc.overhead (%)APIenclaveinterpreter504further motivated by fast prototyping.
In particular,
the use of F# code quotations is convenient because
it facilitates retrieval of the abstract syntax tree of the
program, which is the input to our compiler. Addition-
ally, F# allowed us to experiment with various ways of
expressing the action functions, and to run and debug
the programs locally (that can even take place with the
F# interpreter without invoking the compiler and the
enclave interpreter).
Action function composition. In this paper, we as-
sumed a ﬁxed order of network function execution. This
is eﬀective when the functions run in isolation, or when
their composition can be determined trivially. Net-
work functions, however, can interact in arbitrary ways,
hence, it is an open question to deﬁne the semantics of
function composition. One option is to impose a hier-
archy. This can be achieved, for example, by imposing
that application speciﬁc functions precede ﬂow speciﬁc
ones or vice versa, or apply priorities to functions which
deﬁne the execution order.
OS vs NIC program execution. Eden’s interpreter
can run both in the kernel and in a programmable NIC.
When both are available, deciding where functions run
is an open question. While it may seem that process-
ing should always be oﬄoaded, the cost of oﬄoading
may be prohibitive in some cases. For example, when
the associated metadata is large, or when the interac-
tion with the local controller is frequent. Automatically
partitioning of a high-level policy across diﬀerent exe-
cution points would thus be very useful.
Cycle budget and memory requirements. Achieving
line rates of 10Gbps+ in modern datacenter networks
results in a tight cycle budget for data plane compu-
tations. Techniques like IO batching and oﬄoading
are often employed to reduce the processing overhead.
While Eden’s action functions express per-packet com-
putation, they can be extended to allow for computation
over a batch of packets. If the batch contains packets
from multiple messages, the enclave will have to pre-
process it and split it into messages.
While the enclave can, in principle, limit the amount
or resources (memory and computational cycles) used
by an action function, we chose not to restrict the com-
plexity of the computation or the amount of state main-
tained by them. Instead, we believe that the system ad-
ministrator should decide whether to run a function or
not, irrespective of its overheads. Indeed, deeming such
overheads acceptable may depend on exogenous factors
such as the expected load in the system.
7 Related work
The idea of a programmable data plane dates back to
active networking. Active networks exposed an API
that allows control over router and switch resources,
enabling processing, storage and packet queue manage-
ment [60, 56, 63, 12, 3]. However, it has proven diﬃcult
to realize a generic programming model implemented
by networking devices. Eden avoids this issue by focus-
ing on data plane functions executed by end hosts only.
Recently, Jeyakumar et al. proposed the use of tiny pro-
grams that can be carried in packets and executed by
routers [32]. This provides a ﬂexible mechanism to ex-
change state information between the network and the
end hosts. Eden can beneﬁt from such information, but