title:On Multi-gigabit Packet Capturing with Multi-core Commodity Hardware
author:Nicola Bonelli and
Andrea Di Pietro and
Stefano Giordano and
Gregorio Procissi
On Multi–gigabit Packet Capturing
with Multi–core Commodity Hardware
Nicola Bonelli, Andrea Di Pietro, Stefano Giordano, and Gregorio Procissi
CNIT and Universit`a di Pisa, Pisa, Italy
Abstract. Nowadays commodity hardware is oﬀering an ever increasing
degree of parallelism (CPUs with more and more cores, NICs with par-
allel queues). However, most of the existing network monitoring software
has not yet been designed with high parallelism in mind. Therefore we de-
signed a novel packet capturing engine, named PFQ, that allows eﬃcient
capturing and in–kernel aggregation, as well as connection–aware load
balancing. Such an engine is based on a novel lockless queue and allows
parallel packet capturing to let the user–space application arbitrarily de-
ﬁne its degree of parallelism. Therefore, both legacy applications and
natively parallel ones can beneﬁt from such a capturing engine. In addi-
tion, PFQ outperforms its competitors both in terms of captured packets
and CPU consumption.
1
Introduction and Motivation
Monitoring high performance links on a current network is deﬁnitely a challeng-
ing task: on one hand the data rate, which is becoming increasingly high, calls
for hardware acceleration of the fast data path, while, on the other hand, the
complexity of the analysis to be carried out and the need to have it updated
according to the emerging applications and threats requires a ﬂexibility and
modularity that only software can provide. However, the evolution of commod-
ity hardware is pushing parallelism forward as the key factor that may allow
software to attain hardware-class performance while still retaining its advan-
tages. On one side, commodity CPUs are providing more and more cores, while
on the other modern NICs are supporting multiple hardware queues that allow
cores to fetch packets concurrently (in particular, this technology is known as
Receive Side Scaling, henceforward RSS). Unfortunately, current network moni-
toring and security software is not yet able to completely leverage the potential
which is brought on by the hardware evolution: even if progress is actually being
made (multiple queue support has been included in the latest releases of the
Linux kernel), much of current monitoring software has been designed in the
pre–multicore era. The aim of our work is to make the full power of parallel
CPUs available to both traditional and natively parallel application, through
eﬃcient and conﬁgurable in–kernel packet ﬂow aggregation. Therefore, we de-
signed a novel packet capturing engine, named PFQ, that allows to parallelize
the packet capturing process in the kernel and, at the same time, to split and bal-
ance the captured packets across a user–deﬁned set of capturing sockets. This
N. Taft and F. Ricciato (Eds.): PAM 2012, LNCS 7192, pp. 64–73, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012
On Multi–gigabit Packet Capturing with Multi–core Commodity Hardware
65
way, the application writer can arbitrarily choose its level of parallelism with
PFQ, hiding within the kernel the full parallelism of the system. In particular,
an application can either use a single capturing socket (as in the case of legacy
applications) or have PFQ balance incoming frames across a conﬁgurable set of
collection points (sockets) or even use a completely parallel setup, where packets
follow parallel paths from the device driver up to the application. In all of those
cases, PFQ yields better performance than its competitors, while burning a lower
amount of CPU cycles. Diﬀerently from many existing works for accelerating
software packet processing, PFQ does not require driver modiﬁcation (although
a minimal few–lines patch in the driver can further improve performance). Scal-
ability can be achieved through batch processing (which, in turn, leverages the
hierarchical cache structure of modern CPUs) and through lockless techniques,
which allow multiple threads to update the same state with no locking and min-
imal overhead. In particular, we designed a novel double buﬀer multi–producer
single–consumer lockless queue which allows high scalability. PFQ is open–source
software released under GPL license and can be freely downloaded at [1]. The
package consists of a Linux kernel module and of a C++ user–space library.
The rest of the paper is organized as follows: section 2 summarizes the state of
the art in the topic of packet capturing solutions, while section 3 describes the
architecture of our packet capturing engine. Section 4 shows the results of our
measurement campaign and the Conclusions section follows.
2 State–of–the–Art in Packet Capturing
Several solutions have been proposed to speed up the packet capturing capa-
bilities of commodity PCs. nCap [2] uses memory mapping to directly expose
to the application the memory areas where the NIC copies incoming frames.
The same approach is adopted by Netmap [3], a BSD based project which in-
tegrates in the same interface a number of modiﬁed drivers mapping the NIC
transmit and receive buﬀers directly into user space. Also PF RING [4] uses a
memory mapped ring to export packets to user space processes: such a ring can
be ﬁlled by a regular sniﬀer (thus using the standard linux capturing mecha-
nisms) or by specially modiﬁed drivers, which skip the default kernel processing
chain. Those can be both drivers with minimal patches (aware drivers) or heav-
ily modiﬁed ones. Memory mapping has also been adopted by the well-known
PCAP capturing libraries [5]. In the past years, the capturing stack of Free-BSD
has been enhanced by a double–buﬀer mechanism, where packets are written
into a memory–mapped buﬀer which is ﬁrst ﬁlled within the kernel and then
switched over to the application for reading. This is diﬀerent from PF RING,
where applications and kernel work on the same ring concurrently. Although our
proposed architecture also adopts a double buﬀer solution, it brings it further
by introducing other optimizations (like batch processing) and by explicitly tai-
loring it to a multi–core scenario. Many works (most of them on software based
routers) have obtained good results in accelerating software packet processing
by extensively patching the device drivers. TNAPI [6] eﬀectively addressed the
66
N. Bonelli et al.
topic, but the proposed solution is based on a heavily customized driver, which
detaches parallel polling threads instead of relying on NAPI. Besides, its heavy
use of kernel level polling leads to high CPU utilization. The authors in [7] focus
on how to distribute work across cores in order to build high performance soft-
ware routers. Although the results are certainly interesting, it relies on the Click
modular router [8] and its modiﬁed polling driver to deliver good performance.
In [10], the authors present Packetshader, an extremely well performing soft-
ware router, which is built around GPU acceleration of computation intensive
and memory intensive functionalities (such as address lookup). Also, it relies on
a heavily modiﬁed driver which introduces several optimizations, such as using
a reduced version of the socket buﬀer structure and preallocating huge buﬀers
to avoid per–packet memory allocations. Our work is somewhat orthogonal to
those based on modiﬁed drivers, as PFQ is a general architecture that can be
beneﬁcial to both vanilla and modiﬁed drivers.
3 PFQ Capturing Engine
The system as a whole is depicted in Figure 1 and is made up by the following
components: the packet fetcher, the demultiplexing block and socket queues. The
fetcher dequeues the packet directly from the driver, which can be a standard
driver or a patched “aware” driver, and inserts it into the batching queue. The
next stage is represented by the demultiplexing block, which is in charge of
selecting which socket(s) need to receive the packet. The ﬁnal component of
PFQ is the socket queue, which represents the interface between user space and
kernel space. All of the kernel processing (from the the reception of the packet
up to its copy into the socket queue) is carried out within the NAPI context; the
last processing stage is completely performed at user space, thanks to memory
mapping. In the following we will describe in more detail each building block.
3.1 Building Blocks
Aware driver. The concept of driver awareness has been ﬁrst introduced by
PF RING: an aware driver, instead of passing a packet up the standard linux
networking stack, highjacks and forwards it directly to the capturing module.
This implies that, on one hand, the message does not have to go through the
standard network stack processing, thus improving performance. On the other
hand, the capturing module has exclusive ownership of the packet, which is
invisible to the rest of the kernel (including the sniﬀers). We developed a patched
version of the ixgbe driver that just involves minimal code modiﬁcations (around
a dozen lines of code); such a simple patch can be easily applied to new and
existing drivers. We point out that such a block is completely optional and PFQ
shows good performance with vanilla drivers too. Moreover, an aware driver
managing multiple interfaces can handle in aware-mode only the packets coming
from a monitoring interface, while exposing the others to the kernel stack.
On Multi–gigabit Packet Capturing with Multi–core Commodity Hardware
67
User space
SOCKET QUEUE
packet steering block
BATCHING QUEUE
FETCHER
napi
napi
napi
napi
NAPI context
HW QUEUE
eth0
ethx
Fig. 1. PFQ scheme at–a–glance
Packet fetcher. The packet fetcher is the only component which acts on a
packet by packet basis. It receives the packets and inserts the associated pointer
into its batching queue. Once such a queue (whose length is conﬁgurable) is ﬁlled,
all of its enqueued packets are processed by the next block in a single batch.
Batch processing turns out to be more eﬃcient in that it improves the temporal
locality of memory accesses, thus reducing the probability of both cache misses
and concurrent access to shared data. In particular, a signiﬁcant advantage comes
from deallocating packets in batches that, according to our measurements, can
reduce the deallocation cost by as much as 75%. Our measurements reveal that
the optimal queue length is of the order of a hundred of packets. Notice that, as
the packet is timestamped before queueing, this component does not inﬂuence
timing accuracy.
Packet steering block. The main function of the steering block is to select
which sockets need to receive the captured packets. Notice that, although it is
a single functional block, the steering block processing is completely distributed
and does not represent a serialization point (in fact, it only deals with read–
only state). Such a block consists of a routing matrix that allows to ﬂexibly
dispatch the incoming packets across multiple capturing sockets. In particular,
such a matrix associates each reception queue of each handled card with one
or more capturing sockets. Such sockets can be independent from each other
(thus receiving one copy of the packet each) or can be aggregated into a load
balancing group. In this latter case, a hash function is computed for each packet
and only one socket in the balancing group is chosen. An additional advantage of
such an approach is the possibility of performing a bidirectional load balancing.
Indeed, RSS performs its native form of load balancing by computing a hash
function over the 5–tuple of incoming packets. However, such a scheme may not
be appropriate for some applications, as RSS is not symmetric. For example,
applications that monitor TCP connections need to observe packets from both
directions which RSS would dispatch to diﬀerent cores. For this reason, the
68
N. Bonelli et al.
packet steering block recomputes a symmetric hash function that will rebalance
the packets with small overhead. Notice that load balancing and copy are not
mutually exclusive: packets from the same hardware queue can be copied to
a set of sockets and load–balanced across another one. In greater detail, the
demultiplexing block is composed by a bit–ﬁeld matrix and a load balancing
function. The switching matrix stores, for each queue, a bitmap specifying which
sockets have to receive its packets. Such a design allows dynamic insertion and
removal of sockets with no need for mutexes on the fast data path.
Socket queue. It is the last component of our architecture and the only one
which is subject to inter–core contention. Our design shares some similarities
with that of the FreeBSD zero–copy packet ﬁlter, but it improves the state of
the art by introducing a wait–free solution which is optimized for a multi–core
environment. Indeed, the whole mechanism implements a multiple producer –
single consumer wait–free queue. The main components of this block are two
memory mapped buﬀers: while one of them is being ﬁlled with the packets com-
ing from the demultiplexer, the other one is being read from the user application.
The two buﬀers are periodically swapped through a memory mapped variable
(named index in the pseudocode of algorithm 1) that stores both the index of
the queue being written to and the number of bytes that have been already in-
serted (in particular, its most signiﬁcant bit represents the queue index). Each
producer (i.e. a NAPI kernel thread) reserves a portion of the buﬀer by atomi-
cally incrementing the shared index; such a reservation can be made on a packet
by packet basis or once for a batch. After the thread has been granted exclusive
ownership of its buﬀer range, it will ﬁll it with the captured packet along with a
short pseudo header containing meta–data (e.g. the timestamp). Finally, it will
ﬁnalize it by setting a validation bit in the pseudo–header after raising a write
memory barrier. Notice that, when the user application copies the packets to
a user space buﬀer, some NAPI contexts may still be writing into the queue.
This will results in some of the slots being “half ﬁlled” when they reach the
application; however, the user–space thread can wait for the validation bit to be
set. On the application side, the user thread which needs to read the buﬀer will
ﬁrst reset the index by specifying another active queue (so as to direct all subse-
quent writes to it). Subsequently, it will copy to the application buﬀer a number
of bytes corresponding to the value shown by the old index. Such copy will be
performed in a single batch, as, from our past measurements, batch copy can
be up to 30% faster. Alternatively, packets can be read in place in a zero–copy
fashion. The access protocol is described in greater detail by the pseudocode in
algorithm 1. Notice that, the ﬁrst read of the index is not functionally necessary,
but prevents the index from overﬂowing in case the consumer is not swapping
for a long period. Finally, we point out that PFQ comes with a C++ user-space
library which hides the complexity of the lockless queue while still transferring
packets in batches.
On Multi–gigabit Packet Capturing with Multi–core Commodity Hardware
69
queue full, exit
(cid:2) this ﬁrst read is only to prevent overﬂow
Algorithm 1. Pseudo-code for the NAPI context inserting N packets into the
double–buﬀer queue.
function insert packet(bytes, packet)
1: if QLEN GT H(index) < BU F F ER LEN then
2:
3: end if
4: curr index ← atomic incr(index, bytes + P SEU DO HEADER LEN GT H)
5: curr bytes ← QLEN GT H(curr index)
6: curr buf f er ← QACT IV E(curr index)
7: if curr bytes < BU F F ER LEN then
8:
9: end if
10: my buf f er
buf f er pointer[curr buf f er] + curr bytes − (bytes +
queuef ull, exit
←
P SEU DO HEADER LEN GT H)
11: copy packet and compile pseudo header
12: write memory barrier()
13: set pseudo header validity bit
function read packets()
1: active queue ← QACT IV E(index)
2: next index ← complement(acive queue) << IN DEX BIT S − 1
3: index ← next index
4: my buf f er ← buf f er pointer[active queue]
5: for all packet in my buf f er do
6:
7:
8: end for
wait for valid bit to be set
read packet and pseudo header
(cid:2) atomic swap
4 Experimental Results
We assessed the performance of our system under several conﬁgurations and we
compared it mainly against that of PF RING. The latter is the obvious com-
petitor for PFQ, in that it is a general architecture that increases the capturing
performance with both vanilla and modiﬁed drivers. Unfortunately we could not
consider PF RING TNAPI [6] in the comparison as it is not publicly available
for download. We also show some results obtained by the well–known PCAP
library (version 1.1.1 with memory mapping enabled), that only works with
vanilla drivers; however, as PCAP does not explicitly support hardware queues,
its results can be shown in a few layouts only. We wrote a simple packet counting
application for PFQ, while for PF RING we used the pfcount application that
comes with the project distribution. We took two main performance metrics
into consideration: number of captured packets and average CPU consumption.
While the ﬁrst one is the most obvious performance index, the second one is
important as well: if the capturing engine is consuming a very high fraction
of CPU cycles, a monitoring application will hardly have resources to do any
signiﬁcant processing. The testbed for experiments is made up of two identical
machines, one for generating traﬃc, the other in charge of capturing. Both of
them come with a 6 cores Intel X5650 Xeon (2.66 Ghz clock, 12Mb cache), 12
GB of DDR3 RAM, and an Intel E10G42BT NIC, with the 82599 controller on
70
N. Bonelli et al.
board. In order to test our system with the maximum degree of parallelism, we
kept Intel Hyperthreading enabled, thus carrying out the experiments with 12
virtual cores. We will show that such a choice yields performance improvement
in all scenarios. Both servers run Linux with the lates 3.0.1 kernel and the ixgbe
3.4.24 NICs driver. Due to the high cost of hardware based traﬃc generators
and to the limited performance of software based ones, we chose, as the authors
also did in [10], to write our own generator. Such a software [12] which, again,
leverages platform parallelism, is able to generate up to 12 Millions minimum–
sized packets per second. We validated its performance by means of a borrowed
Napatech hardware based traﬃc analyzer (courtesy of Luca Deri). In particu-
lar, we veriﬁed that the maximum generated rate advertised by the generator
itself was the same rate measured by the Napatech board. Moreover, in order to
leverage the RSS load–balancing mechanism, we randomized the IP addresses of
each packet.
Finally, we remark that due to the use of hyperthreading, we can display up to
12 capturing cores; however, from the performance point of view, this is not the
same of having 12 real CPUs. The CPU numbers are arranged as follows: real core
number x corresponds to two virtual cores x and 6+x, respectively. Therefore, if we
increase the set of capturing cores in a linear manner starting from 0, we expect the
contribution of the ﬁrst six cores to be signiﬁcantly higher than that of the others
(as it actually appears in our results). Therefore, we expect an ideal graph to scale
linearly from 1 to 6 and to show a discontinuity in 6 and to grow linearly again,
but with a much less steep slope, from 7 to 12 cores.
4.1 One–Thread Setup
In this ﬁrst layout, which is the most relevant for legacy applications, we used
a variable number of hardware queues for fetching packets and we only used
one socket to bring them to user space. Indeed, we hid the system parallelism
within the kernel while still exposing a standard interface to the application. In
particular we used a layout that we showed to be beneﬁcial in [13]: we captured
the packets on all the physical cores but one, and on that one we bound the
user–space process. The results shown in Figure 2 report the number of cap-
tured packets for both modiﬁed and aware drivers: the behavior of PFQ with an
increasing degree of parallelism is piece–wise linear (due to the expected discon-
tinuity around 6) while PF RING, that handles contention through traditional
lock–based mechanisms, and PCAP do not manage to scale with the number of
cores. Besides, the scalability of our architecture does not depend on the driver:
using an aware or a vanilla driver just reﬂects on the slope of the graph, but
linearity is preserved. Notice that, as anticipated, we did not capture packets on
the physical core where the user space process is bound: therefore, the number
of available capturing cores is limited to 10.
Figure 3 reports the bit rate of the captured traﬃc for several packet sizes
and by using 12 hardware queues. PFQ always captures all of the traﬃc our