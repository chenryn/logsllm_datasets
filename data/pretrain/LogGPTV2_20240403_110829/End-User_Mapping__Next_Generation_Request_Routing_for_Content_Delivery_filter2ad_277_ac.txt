percentiles of the distance. This may seem counter-intuitive.
)
s
e
l
i
m
(
e
c
n
a
i
t
s
d
S
N
D
L
−
n
e
t
i
l
c
i
n
a
d
e
M
5000
4000
3000
2000
1000
0
2−10
2−9
2−7
2−8
2−3
Percent of client demand from AS
2−6
2−4
2−5
2−2
2−1
Figure 10: Client-LDNS distance as a function of AS size.
But, the reason is that smaller AS’es include small local ISPs
who are more likely to “outsource” their name server infras-
tructure to other providers. The reason for the outsourcing is
economic in nature as the ISP may not want to own and op-
erate a name server infrastructure. So, the ISP may choose
the inexpensive option of using a public resolver operated
by a provider such as Google Public DNS, OpenDNS, Level
3, UltraDNS, etc. The “outsourcing” of DNS services often
causes the LDNSes to be non-local, leading to larger client-
LDNS distances. A different category of small AS’es with
large client-LDNS distances are enterprises with geographi-
cally diverse branch ofﬁces who for operational convenience
use a centralized name server infrastructure deployed in only
one of those ofﬁces. Given the large client-LDNS distances,
we expect end-user mapping to beneﬁt a large fraction of
clients of small AS’es.
Large ISPs typically operate their own name server infras-
tructures for their clients. Such infrastructure often consists
of LDNSes that are deployed in multiple geographically dis-
tributed locations. To direct clients to the “nearest” LDNS,
the IP anycast [16, 15] mechanism is often used. This ex-
plains the smaller values of client-LDNS distance despite the
large geographical area covered by these global ISPs. How-
ever, IP anycast has many known limitations [23] that can
result in a fraction of the clients being routed to far away
LDNS locations. Thus, end-user mapping may be beneﬁcial
for clients of large ISPs also.
3.3 How far are clients that use the same
LDNS from each other?
A client cluster is a set of clients that use the same LDNS.
The clients on the Internet can be partitioned into client clus-
ters, one cluster for each LDNS. We deﬁne the radius of a
client cluster to be the mean distance of the clients in the
cluster to the centroid of the cluster7. In traditional NS-based
mapping, a client cluster is the unit for making server assign-
ment decisions, i.e., all clients in a client cluster are assigned
7Distances are computed using the latitude and longitude of
the clients from our geo-location database. The radius and
centroid use client demands as the weights.
173
the same set of server IPs, since they use the same LDNS (cf.
Equation 1). If a client cluster of a LDNS has a small ra-
dius, i.e., the clients are close together, a more sophisticated
form of NS-based mapping could still be effective, even if
the client-LDNS distances are large. The reason is that the
mapping system could discover the client cluster and assign
servers that provide good performance for the entire clus-
ter. However, if the client cluster has a large radius, i.e., the
clients are far away from each other, there may be no single
server assignment for the entire cluster that is optimal for
all clients in it. Thus, it is inherently difﬁcult for NS-based
mapping to perform well when the client cluster has a large
radius, even knowing client-LDNS pairings.
Figure 11 reafﬁrms that on an overall basis a large fraction
of clients are close to their LDNSes and the cluster radii are
small. However, focusing on the subset of LDNSes that are
public resolvers, we see that not only are client-LDNS dis-
tances large, but cluster radii are large as well. In fact, 99%
of the public resolver demand originates from client clus-
ters with radii between 470 to 3800 miles. The ﬁgure also
shows that for public resolvers the mean cluster-LDNS dis-
tance tends to be larger than the cluster radius. This implies
that the LDNS is often not deployed at a “central” location
within the client cluster that it serves, i.e., near the centroid
that minimize the mean client-LDNS distance. This is in
part due to the fact that a public resolver provider does not
have ﬁne-grained control over which clients in which loca-
tions use their service. e.g., clients from countries where the
provider has no deployments often use the service.
d
n
a
m
e
d
t
n
e
i
l
c
f
o
t
n
e
c
r
e
p
e
v
i
t
l
a
u
m
u
C
100%
75%
50%
25%
0%
Cluster radius (all LDNS)
Client−LDNS mean distance (all LDNS)
Cluster radius (public resolvers)
Client−LDNS mean distance (public resolvers)
10
2500
Distance (miles)
5000
Figure 11: CDFs of mean client-LDNS distance and clus-
ter radius for all LDNSes and for the subset that are public
resolvers.
4. PERFORMANCE IMPACT
We present our experience and insights obtained in de-
ploying end-user mapping for clients around the world in the
ﬁrst half of 2014. During this period, Akamai began the roll-
out of end-user mapping for clients who use public resolvers
such as Google Public DNS and OpenDNS. The reasons for
initially targeting clients who use public resolvers were two-
fold. Based on our analysis of client-LDNS distances in Sec-
tion 3, we concluded that clients who use public resolvers
are more likely to beneﬁt from end-user mapping, since they
tend to be farther away from their LDNSes (cf. Figure 7)
and also had large client cluster radii (cf. Figure 11). Fur-
ther, public resolver providers such as Google Public DNS
and OpenDNS support the EDNS0 client-subnet extension
that is required for end-user mapping. The end-user map-
ping roll-out8 started on March 28th 2014 and completed on
April 15th 2014. We present insights based on performance
measurements made before, during, and after the roll-out.
4.1 Performance metrics
The performance experienced by clients who download
web content can be characterized in many different but com-
plimentary ways. We use the following four metrics mea-
sured from real-world clients downloading content from Aka-
mai to evaluate the performance. Each metric sheds light
on a different facet of mapping and client-perceived perfor-
mance. Note that we expect all these metrics to decrease
(smaller is better) when end-user mapping is rolled out.
1) Mapping distance is the great circle distance between
a client and the server to which it was assigned by the map-
ping system. This is a purely geographical metric with no
network-related component.
2) Round trip time (RTT) between the client and the
server to which it was assigned. This is simply the TCP
RTT measured from the server’s TCP stack. This is purely a
network-related metric.
3) Time to ﬁrst byte (TTFB) is the duration from when
the client makes a HTTP request for the base web page to
when the ﬁrst byte of the requested web page was received
by the client. This quantity is measured from the client’s
browser and includes three components: (i) the time for the
client’s request to reach the server, (ii) time for the server to
construct the web page, and (iii) time for the ﬁrst chunk of
the web page to reach the client. Note that end-user mapping
is expected to decrease both the ﬁrst and third component of
TTFB above by reducing the server-client RTT. However,
since many base web pages are “dynamic” and need to be
personalized for the client, the second component of con-
structing the web page may involve fetching personalized el-
ements from the origin. Overlay transport is used to speedup
origin-server communication [26], though such transport is
not impacted by the end-user mapping roll-out. Thus, we
expect TTFB to show more modest reductions as end-user
mapping impacts only some of its time components.
4) Content download time is the duration from the re-
ceiving of the ﬁrst byte of the page to completing the down-
load of the rest of the web page, including the content em-
bedded in the page. This metric is also measured from the
client’s browser. The embedded content of web pages are
typically more static and cacheable and includes CSS, im-
ages, and JavaScript that are not personalized to the client.
Thus, unlike TTFB, we expect this metric to be signiﬁcantly
8We are unaware of any other Akamai software releases
or Internet events happening during the roll-out period that
could confound our measurements and conclusions.
174
impacted by the end-user mapping roll-out as this metric is
dominated by client-server latencies.
4.1.1 High and low expectation countries
To better understand the performance impact, we classify
the countries into two groups: a “high expectation” group
where we expect end-user mapping to have a greater impact
and a “low expectation” group where we expect the impact
to be lower. Our client-LDNS analysis in Section 3.2 gives
us an idea of what beneﬁts to expect in which countries.
Speciﬁcally, Figure 8 shows the proximity of clients to their
LDNS for major countries. Using this analysis, we split the
major countries into two halves. We deﬁne the high expecta-
tion group to be those clients who reside in countries where
the median distance to a public resolver is more than 1000
miles and the low expectation group to be those whose me-
dian distance is under 1000 miles. We aggregate and present
the performance metrics separately for these two groups, as
we expect them to show different behaviors.
4.2 Collecting performance information
We collected performance metrics from a large and char-
acteristic set of clients around the world before, during, and
after the end-user mapping roll-out. We used Akamai’s Real
User Measurement (RUM) system [4] for our client-side per-
formance measurements. RUM inserts JavaScript into se-
lect web pages delivered by Akamai. That JavaScript runs
inside the client’s browser when the page is downloaded
by the client. The performance measurement is made us-
ing the industry-standard navigation timing [6] and resource
timing APIs [8]. Using these APIs, the JavaScript running
inside the client’s browser collects precise timing informa-
tion when the page download is in progress, including when
the DNS lookup started and completed, when the TCP con-
nection was initiated, when the fetch request was sent out,
when the ﬁrst byte of the response was received, and when
all the page content was fully downloaded. Using these tim-
ing milestones, metrics such as TTFB and content download
time can be computed. The timing measurements performed
in client browsers around the world was sent to a backend
cloud storage system and was subsequently analyzed to pro-
duce the aggregate statistics we provide in this section.
We collected RUM measurements from a wide selection
of Web sites and clients around the world from Jan 1, 2014 to
June 30th, 2014, a period that includes the end-user mapping
rollout from March 28th to April 15th. Since the roll-out
only impacts clients who use public resolvers, we identiﬁed
such clients using our client-LDNS pairing data described in
Section 3.1 and extracted RUM data from only those qual-
iﬁed clients. Figure 12 shows the total number of qualiﬁed
RUM measurements collected and used in our analysis from
both high and low expectation countries. Our data set has 33
million to 58 million measurements per month, each month
from Jan to June 2014, for a total of 273 million measure-
ments. The measurement volume shows an increasing trend
on account of more downloads from qualiﬁed clients of the
pages measured by RUM.
Our goal is to measure performance for a large and char-
acteristic cross section of clients, Web sites, devices, and
connectivities across the global Internet. To achieve that we
measured 6,388 domain names and 2.5 million unique URLs
accessed by 149,826 unique clients. Our data set includes all
major client platforms such as Windows, FreeBSD, Linux,
Android, iOS, and game consoles, and all major browsers in-
cluding Firefox, Opera, Chrome, and IE. Further, our clients
use a variety of ways to access the Internet including cellu-
lar, WiFi, 3G, 4G, DSL, cable modem, and ﬁber.
)
s
t
n
e
m
e
r
u
s
a
e
m
n
o
i
l
l
i
m
(
h
t
n
o
m
r
e
p
s
t
n
e
m
e
r
u
s
a
e
M
30
20
10
0
expectation
high
low
Dec
Jan
Feb
Mar
Apr
May
Jun
Figure 12: Number of RUM measurements per month.
4.3 Performance Analysis
2000
1000
)
s
e
l
i
m
(
e
c
n
a
t
s
d
i
i
g
n
p
p
a
M
0
s
t
n
e
m
e
r
u
s
a
e
m
M
U
R
f
o
t
n
e
c
r
e
p
e
v
i
t
l
a
u
m
u
C
100%
75%
50%
25%
0%
We analyze the mapping distance, RTT, TTFB, and con-
tent download time for clients who use public resolvers be-
fore, during, and after the roll-out.
1) Mapping distance. Mapping distance shows a signif-
icant improvement during the roll-out period of March 28th
to April 15th. Figure 13 shows for the high expectation
group, the mean mapping distance dropped from over 2000
miles on average to around 250 miles. Even the low expec-
tation countries experienced shorter mapping distance: the
average mapping distance went from 400 miles to 200 miles.
Figure 14 shows the CDF of the mapping distances for
both high and low expectation countries both before and af-
ter the roll-out is completed. The period after the roll-out
is April 15th or later and the period before the roll-out is
March 28th or earlier. Note that all percentiles see improve-
ment. But, there is a drastic decrease in the mapping dis-
tance around the 90th percentile for high expectation coun-
tries from 4573 miles to 936 miles. The decrease is due to
improved mapping distance for clients in large countries like
India and Brazil who use public resolvers located in South-
east Asia and North America respectively (cf. Figure 8).
2) RTT. Recall that RTT measures the latency between the
client and the server assigned to that client. Unlike mapping
distance, RTT reﬂects the state of the network path such as
propagation delay, and congestion. As shown in Figure 15,
the average RTT for the high expectation group dropped
from 200ms to 100ms, a signiﬁcant 50% decrease. But,
the improvement for the low expectation group was mod-
est. Figure 16 shows the CDF of the RTT for both high and
low expectation countries before and after the roll-out. All
175
expectation
high
low
Jan
Feb
Mar
Apr
May
Jun
Jul
Figure 13: Daily mean of mapping distance.
low expectation after rollout
low expectation before rollout
high expectation after rollout
high expectation before rollout
0
2000
4000
Mapping distance (miles)
6000
8000
Figure 14: CDFs of mapping distance.
percentiles show improvement. For instance, the 75th per-
centile of the RTT decreases signiﬁcantly from 220 ms to
137 ms for the high expectation countries.
3) Time-to-First-Byte. As noted earlier, TTFB includes
aspects that are not impacted by better mapping decisions,
such as the computation time to generate and transmit a dy-
namic web page. Consequently, the gains expressed as a
percentage are lower but still signiﬁcant. Figure 17 shows
that the mean TTFB of the high expectation countries de-
creased from around 1000 ms to 700 ms, a 30% improve-
ment. Figure 18 shows the CDF of the TTFB for both high
and low expectation countries before and after the roll-out.
All percentiles show improvement. For instance, the 75th
percentile of the TTFB decreases from 1399 ms to 1072 ms
for the high expectation countries and from 830 ms to 667
ms for the low expectation ones.
4) Content Download Time. Figure 19 shows a reduction
from 300 ms to 150 ms for the high expectation countries, a
50% reduction. Recall that content download time is domi-
nated by server-client latencies and the decrease is more cor-
related with corresponding decrease in RTT. The improve-
ment for the low expectation group is small as the download