sticks with the wrong decision whereas sometimes it makes
a good decision.
Real capturing scenarios will almost always have a higher
In
application load than the ones we have shown so far.
our experiments with higher application load which we will
show in Section 4.3, we can almost always see that running
the processes in A:B conﬁguration results in better capturing
performance. Static pinning always outperformed automatic
scheduling as the schedulers on Linux and FreeBSD almost
make wrong decisions very frequently.
4.2 Comparing packet capture setups under
low application load
Applications that capture network traﬃc usually build on
libpcap [12] as presented in Section 2.1. FreeBSD ships
libpcap version 1.0.0 in its base system, and most Linux
 10 20 30 40 50 60 70 80 90 100 400 600 800 1000 1200Captured packets [%]Generated packets [Kpps]Scheduling: A:AScheduling: A:BScheduling: Auto211reveal that in our setup, the size of the memory mapped area
is not of much inﬂuence. We conducted similar experiments
with the sizes of Berkeley Packet Filter on FreeBSD and
other buﬀer sizes on Linux. All these experiments showed
that increasing the buﬀer size beyond a certain limit does
not boost capturing performance measurably. Instead, we
found evidence that too large buﬀers have a negative im-
pact on capturing performance. These ﬁndings are contrary
to the ﬁndings from Schneider et al. [4], who found large
buﬀers to increase the capturing performance.
The biggest factor regarding the performance is our cap-
turing application—since our hardware, at least the AMD
platform, is able to transfer all packets from the network
card into memory. If the software is able to consume and
process incoming packets faster than wire-speed, the in-
kernel buﬀers will never ﬁll up and there will be no packet
loss. However, if the application is not able to process the
packets as fast as they come in, increasing the buﬀer will
not help much—rather, it will only reduce the packet loss
by the number of elements that can be stored in the buﬀer,
until the buﬀer is ﬁlled.
Schneider et al. sent only 1,000,000 packets per measure-
ment run, whereas we produce packets with 1 GE speed
(i.e., more than 100 Megabytes per second), which usually
amounts to much more than 1,000,000 packets per second,
over a time interval of 100 seconds. Increasing kernel buﬀer
sizes to 20 megabytes, as recommended by Schneider et al.,
allows them to buﬀer a great share of their total number of
packets, but does not help much on the long term.
If we
increase the buﬀers to the recommended size, we cannot see
any signiﬁcant improvements in our experiments.
Buﬀer size can be crucial, though: This is the case when
the monitoring is not able to process packets at wire-speed,
e.g., it can consume up to N packets per second (pps), and
bursty Internet traﬃc is captured. If this traﬃc transports
less or equal than N pps on average but has bursts with a
higher pps rate, then having a suﬃcient dimensioned buﬀer
to store the burst packets is obviously very important.
The second important parameter is a conﬁguration op-
tion called transparent_mode. It conﬁgures how PF RING
handles packets:
• Transparent mode 0: Captured packets are inserted
into the ring via the standard Linux socket API.
• Transparent mode 1: The network card driver inserts
the packets directly into the ring (which requires an
adopted network driver). Packets are also inserted into
the standard Linux network stack.
• Transparent mode 2: Same as mode 1, but received
packets are not copied into the standard Linux network
stack (for capturing with PF RING only).
It is obvious that transparent mode 2 performs best as it is
optimised for capturing. We conducted some comparisons
using diﬀerent packet sizes and packets rates and indeed
found PF RING to perform best in this mode.
We expected PF RING to outperform the standard Linux
capturing due to the evaluations performed in [8] and [6]
when using small packet sizes. This performance beneﬁt
should be seen with small packets (64 bytes) and a high num-
ber of packets per second, and disappear with bigger packet
sizes. According to Deri’s evaluation, capturing small packet
Figure 6: Comparison of diﬀerent libpcap versions
on Linux
distributions use the same version today2. Not long ago,
libpcap version 0.9.8 was the commonly used version on
Linux based systems and FreeBSD. As already explained
in Section 2, libpcap-0.9.8 or libpcap-0.9.8-mmap were used
in most earlier evaluations.
In this paper, we want to use the standard libpcap-1.0.0,
which ships with a shared-memory support. As the shared-
memory extensions in libpcap 0.9.8 and 1.0.0 where devel-
oped by diﬀerent people, we ﬁrst want to compare both ver-
sions. The results of this comparison are plotted in Figure 6
for the AMD and Xeon platforms. We can see that libpcap-
0.9.8 performs slightly better on Xeon than libpcap 1.0.0
while 1.0.0 performs better on AMD. However, both diﬀer-
ences are rather small, so that we decided to use the now
standard 1.0.0 version for our experiments.
Having a closer look at the ﬁgure, one can see that the
Xeon platform performs better than the AMD platform.
This is very surprising as the AMD system’s hardware per-
formance is otherwise much faster than the aged Xeon plat-
form. Things get even more weird if we include libpcap-
0.9.8 without MMAP into our comparison (not shown in
our plot): As the copy operation is way more expensive
than the MMAP, one would expect MMAP to perform bet-
ter than the copy operation. This assumption is true on
the Xeon platform. On the AMD platform however, we can
observe that libpcap-0.9.8 without MMAP performs better
than libpcap-0.9.8-mmap or libpcap-1.0.0. This points to
some unknown performance problems which prevents the
AMD system from showing its superior hardware perfor-
mance.
The next comparison is between standard Linux captur-
ing with PF PACKET and capturing on Linux using the
PF RING extension from Deri [8]. We therefore use Deri’s
patches to libpcap-1.0.0 which enables libpcap to read pack-
ets from PF RING. Two important PF RING parameters
can be conﬁgured. The ﬁrst one is the size of the ring buﬀer,
which can be conﬁgured in number of packets that can be
stored in the ring. Our experiments with diﬀerent ring sizes
2An exception is the current stable version of Debian 5.0
“Lenny”, which still contains libpcap version 0.9.8-5 at the
time this paper was written.
 30 40 50 60 70 80 90 100 400 600 800 1000 1200Captured packets [%]Generated packets [Kpps]libpcap-1.0.0 on AMDlibpcap-0.9.8-mmap on AMDlibpcap-1.0.0 on Xeonlibpcap-0.9.8-mmap on Xeon212Figure 7: PF PACKET vs. PF RING
Figure 9: Capturing with multiple processes
we could not ﬁnd any signiﬁcant diﬀerences in any of our
tests. Schneider et al.
found FreeBSD to perform amaz-
ingly good even though FreeBSD employed this packet copy
operation. This might indicate that the copy operation is in-
deed not a signiﬁcant factor that inﬂuences the performance
of the FreeBSD capturing system. We are uncertain about
the true reason for Zero Copy BPF not performing better
than BPF; therefore, we do not include ZCBPF into our
further comparisons.
Our comparison with the standard BPF is shown in Fig-
ure 8 and presents the diﬀerences between PF PACKET and
FreeBSD. We can see some diﬀerences between capturing
with PF PACKET on Linux and capturing with BPF on
FreeBSD. FreeBSD performs slightly better on both plat-
forms, which conﬁrms the ﬁndings of Schneider et al. [2, 4].
Diﬀerences increase if more than one capturing process in
running on the systems, as shown in Figure 9. This ﬁgure
shows the capturing performance of FreeBSD, Linux with
PF PACKET and Linux with PF RING capturing a packet
stream of 1270 kpps on the AMD system with one, two and
three capturing processes running simultaneously.
Capturing performance on both systems decreases due to
the higher CPU load due to the multiple capturing pro-
cesses. This is an obvious eﬀect and has also been observed
in [4]. Linux suﬀers more from the additional capturing
processes compared to FreeBSD, which has also been ob-
served in related work. Amazingly, Linux with PF RING
does not suﬀer much from these performance problems and
is better than FreeBSD and Linux with PF PACKET. A
strange eﬀect can be seen if two capturing processes are run
with PF PACKET: Although system load increases due to
the second capturing process, the overall capturing perfor-
mances increases. This eﬀect is only visible on the AMD
system and not on the Xeon platform and also points to the
same strange eﬀect we have seen before and which we will
explain in Section 4.3.
If we compare our analysis results of the diﬀerent cap-
turing solutions on FreeBSD and Linux with the results of
earlier evaluations, we can see that our results conﬁrm sev-
eral prior ﬁndings: We can reproduce the results of Deri [8]
and Cascallana [6] who found PF RING to perform better
than PF PACKET on Linux. However, we see that the dif-
ference between both capturing solutions is not as strong
Figure 8: Linux vs. FreeBSD
streams using PF PACKET in Linux 2.6.1 with libpcap-
mmap only captured 14.9% of all packets while PF RING
within the same kernel version was able to capture 75.7%
of all packets. Our comparison conﬁrms that PF RING in-
deed performs better than PF PACKET, as can be seen in
Figure 7. We can also see that PF RING is better than
PF PACKET on both systems. Furthermore, PF RING on
the faster AMD hardware performs better than PF RING
on Xeon. However, the performance diﬀerence is small if one
considers the signiﬁcant diﬀerences in hardware, which again
points to some performance problems which we pointed out
before. One more observation concerns the diﬀerence be-
tween PF PACKET and PF RING within the same plat-
form. Although there is some diﬀerence, it is not as pro-
nounced as in previous comparisons. We explain this by the
improvements that have been made in the capturing code
of PF PACKET and within libpcap since Deri’s and Cas-
callana’s evaluations in 2004 and 2006.
We now compare the measurement results of FreeBSD
against the Linux measurements. FreeBSD has two cap-
turing mechanisms: Berkeley Packet Filter (BPF) and Zero
Copy Berkeley Packet Filter (ZCBPF) as described in Sec-
tion 2.1. At ﬁrst, we compared both against each other, but
 30 40 50 60 70 80 90 100 400 600 800 1000 1200Captured packets [%]Generated packets [Kpps]PR_RING on AMDPF_PACKET on AMDPF_RING on XeonPF_PACKET on Xeon 30 40 50 60 70 80 90 100 400 500 600 700 800 900 1000 1100 1200 1300Captured packets [%]Generated packets [Kpps]PF_PACKET on AMDFreeBSD on AMDPF_PACKET on XeonFreeBSD on Xeon 15 20 25 30 35 40 45 501x on FreeBSD2x on FreeBSD3x on FreeBSD1x with PF_PACKET2x with PF_PACKET3x with PF_PACKET1x with PF_PRING2x with PF_RING3x with PF_RINGCaptured packets [%]Number of capturing processes on different operating systems213Figure 10: Packzip on 256 and 512 byte packets
Figure 11: Packzip on 64 byte packets
as it used to be in 2004 and 2006 due to general improve-
ments in the standard Linux software stack. Furthermore,
we can conﬁrm the ﬁndings of Schneider et al. [4] who found
that FreeBSD outperforms Linux (with PF PACKET), es-
pecially when more than one capturing process is involved.
In contrast, our own analyses reveal that PF RING on Linux
outperforms both PF PACKET and FreeBSD. PF RING is
even better than FreeBSD when more than one capturing
process is deployed, which was the strength of FreeBSD in
Schneiders’ analysis [4]. We are now comparing the cap-
turing solutions with increased application load and check
whether our ﬁndings are still valid in such setups.
4.3 Comparing packet capture setups under
high application load
So far, we analysed the capturing performance with very
low load on the application by writing captured packets to
/dev/null. We now increase the application load by per-
forming more computational work for each packet by using
the tool packzip, which compresses the captured packets us-
ing libz [19]. The application load can be conﬁgured by
changing the compression level libz uses. Higher compres-
sion levels result in higher application load; however, the
load does not increase linearly. This can be seen at the
packet drop rates in Figure 10.
The ﬁgure presents the results of the capturing process on
various systems when capturing a stream consisting of 512
and 256 bytes sized packets at maximum wire-speed at the
AMD platform. It can clearly be seen that a higher applica-
tion load leads to higher packet loss. This happens on both
operating system with every capturing solution presented
in Section 2.1, and is expected due to our ﬁndings and the
ﬁndings in related work. Packet loss kicks in as soon as
the available CPU processing power is not suﬃcient to pro-
cess all packets. We note that FreeBSD performs worse on
our AMD platform compared to Linux with PF PACKET
with higher application load. This observation can be made
throughout all our measurements in this section.
However, if we look at a packet stream that consists of
64 byte packets, we can see an amazing eﬀect, shown in
Figure 11. At ﬁrst, we see that the overall capturing perfor-
mance is worse when no application load (compression level
0) compared to the capturing results with 256 and 512 byte
packets. This was expected because capturing a large num-
ber of (small) packets is more diﬃcult than small number
of (big) packets. However, if application load increases, the
capturing performance increases as well. This eﬀect is quite
paradox and points to the same strange eﬀect as seen in the
experiments before. It can be observed with PF PACKET
and with PF RING but is limited to Linux. FreeBSD is not
aﬀected by this weird eﬀect; instead, capturing performance
is nearly constant with increasing application load but al-
most always below Linux’ performance.
In order to better understand what is causing this bewil-
dering phenomenon, we have to take a closer look at the
capturing process in Linux. An important task within the
capturing chain is the passing of a captured packet from the
kernel to the application. This is done via a shared memory
area between the kernel and application within libpcap. The
shared memory area is associated with a socket, in order to
allow signaling between the kernel and the application, if
this is desired. We will call this memory area SHM in the
remainder of this section.
Packets are sent from kernel to user space using the fol-
lowing algorithm:
• The user application is ready to fetch a new packet.
• It checks SHM for new packets.
If a new packet is
found, it is processed.
• If no new packet is found, the system call poll() is
issued in order to wait for new packets.
• The kernel receives a packet and copies it into SHM.
• The kernel “informs” the socket about the available
packet; subsequently, poll() returns, and the user ap-
plication will process the packet.
This algorithm is problematic because it involves many
systems calls if the application consumes packets very quickly,
which has already been found to be problematic in Deri’s
prior work [8]. A system call is a quite expensive oper-
ation as it results in a context switch, cache invalidation,
new scheduling of the process, etc. When we increase the
compression level, the time spent consuming the packets in-
 20 30 40 50 60 70 80 90 100 0 1 2 3 4 5 6 7 8 9Captured Packets [%]Packzip compression level256 Bytes @ FreeBSD512 Bytes @ FreeBSD256 Bytes @ PF_PACKET512 Bytes @ PF_PACKET 0 20 40 60 80 100 0 1 2 3 4 5 6 7 8 9Captured packets [%]Packzip compression levelPF_PACKET on 64 byte packetsFreeBSD on 64 byte packetsPF_RING on 64 byte packets214creases as well, thus less system calls are performed3. Ob-
viously, reducing the number of system calls is beneﬁcial to
the performance of the packet capture system.
There are several ways to achieve such a reduction. The
most obvious solution to this problem is to skip the call to
poll() and to perform an active wait instead. However,
this solution can pose problems to the system: If captur-
ing and analysis are scheduled to run on the same processor
(which is not recommended, as we pointed before), polling
in a user space process eats valuable CPU time, which the
capturing thread within the kernel would need. If capturing
and analysis run on diﬀerent cores, there still are penalties:
The kernel and the user space application are trying to ac-
cess the same memory page, which the kernel wants to write
and the user space application wants to read. Hence, both
cores or CPUs need to synchronise their memory access,
continuously leading to cache invalidations and therefore to
bad performance. We patched libpcap to perform the active
wait and found some, but only little, improvement due to
the reasons discussed above.
Hence, we searched for ways to reduce the number of calls
to poll() that do not increase the load on the CPU. The
ﬁrst one was already proposed by Deri [8]. The basic idea is
to perform a sleep operation for several nano seconds if the
SHM is found to be empty. Although the sleep operations
still implies a system call, it is way better than multiple
calls to poll(), as the kernel capturing gets some time to
copy several packets into the SHM. Deri proposed an adap-
tive sleep interval that is changed according to the incom-