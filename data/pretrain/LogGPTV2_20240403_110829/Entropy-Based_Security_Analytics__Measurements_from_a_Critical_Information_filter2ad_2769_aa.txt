title:Entropy-Based Security Analytics: Measurements from a Critical Information
System
author:Marcello Cinque and
Raffaele Della Corte and
Antonio Pecchia
2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Entropy-Based Security Analytics: Measurements
from a Critical Information System
∗Dipartimento di Ingegneria Elettrica e Tecnologie dell’Informazione, Universitá degli Studi di Napoli Federico II
Marcello Cinque∗, Raffaele Della Corte∗, Antonio Pecchia∗†
†Consorzio Interuniversitario Nazionale per l’Informatica, Laboratorio CINI-ITEM “Carlo Savy”
Via Claudio 21, 80125, Naples, Italy
Complesso Univ. Monte Sant’Angelo, Ed. 1, Via Cinthia, 80126, Naples, Italy
{macinque, raffaele.dellacorte2, antonio.pecchia}@unina.it
Abstract—Critical information systems strongly rely on event
logging techniques to collect data, such as housekeeping/error
events, execution traces and dumps of variables, into unstructured
text logs. Event logs are the primary source to gain actionable
intelligence from production systems. In spite of the recognized
importance, system/application logs remain quite underutilized in
security analytics when compared to conventional and structured
data sources, such as audit traces, network ﬂows and intrusion
detection logs.
This paper proposes a method to measure the occurrence of
interesting activity (i.e., entries that should be followed up by
analysts) within textual and heterogeneous runtime log streams.
We use an entropy-based approach, which makes no assumptions
on the structure of underlying log entries. Measurements have
been done in a real-world Air Trafﬁc Control information system
through a data analytics framework. Experiments suggest that
our entropy-based method represents a valuable complement to
security analytics solutions.
Keywords—Event logging, security, log analytics, ﬁltering, in-
formation systems.
I.
INTRODUCTION
Critical information systems have become proﬁtable tar-
gets for cyber threats because they underlie many critical assets
of our daily life, such as power grids, medical, ﬁnancial and
transportation systems. They consist of various software com-
ponents, which range from application, middleware, database,
and operating system, that ubiquitously emit text logs con-
taining housekeeping/error events, execution traces and dumps
of variables collected during operations. Security analytics
is emerging as a new trend in corporate research to assist
network managers in protecting critical assets and detecting
suspicious activities and incidents [1], [2]. It capitalizes on the
variety of data sources generated by the system at runtime by
combining capabilities, such as real-time monitoring, stream
processing, advanced indexing and efﬁcient computation. Al-
though valuable in failure analysis [3], system/application logs
remain quite underutilized in security analytics with respect
to conventional sources (e.g., audit traces, network ﬂows and
intrusion detection logs) due to the lack of systematic design
and coding practices, which lead to subjective, unstructured
and developer-dependent log entries at runtime.
Second generation Security Information and Event Man-
agement (SIEM) [4], [5] represents the state-of-the-art in secu-
rity analytics. AlienVault USM1, IBM QRadar2, LogRhythm3
1http://www.alienvault.com
2http://www-03.ibm.com/software/products/en/qradar-siem
3http://logrhythm.com/products/siem
2158-3927/17 $31.00 © 2017 IEEE
DOI 10.1109/DSN.2017.39
and Splunk Enterprise Security (ES)4 are examples of well-
known products. They rely on internal representation formats
(e.g., MDI Fabric and Common Information Model used
by LogRhythm and Splunk ES, respectively) to import and
consolidate almost any data source that can be encountered in
real systems. Mentioned products implement built-in adapters,
which well cope with importing structured data into the
internal representation; however, built-in support is much more
limited when the data consist of unstructured text logs. For
example, in case of a syslog source, only a small number
of standard ﬁelds (e.g.,
timestamp, hostname, severity) is
automatically recognized by the mentioned products, while the
free text message is just imported in its raw format. Moreover,
in case of a legacy or application dependent log source, the
analyst is required to conﬁgure his/her own custom adapter.
Once the text log is imported, analysts are expected to write
ad-hoc ﬁlters in order to extract speciﬁc ﬁelds or to search for
entries that match given patterns.
Event logs can play a key role in security analysis: past
studies have shown that, in more than 40% of cases, attack-
ers might escape any traditional protection mechanism and
conceal their presence until the application/system is misused
[6]. Nevertheless, security analytics solutions keep relying on
traditional ﬁltering approaches and human intervention to dig
into raw text logs with the aim of ﬁnding interesting activity,
i.e., log entries that could help revealing misuse and should be
followed up by analysts for further investigation.
This paper proposes a method to automatically measure
the occurrence of interesting activity within textual and het-
erogeneous runtime log streams. The method leverages a well-
established term weighting scheme, i.e., logarithmic entropy
(log.entropy), which makes no assumptions on the structure
of underlying entries: the occurrence of interesting activity is
inferred by computing the entropy of the runtime log streams
with respect to a system behavioral baseline. Previous research
has explored the use of entropy-based approaches or term
weighting for security analysis and detection of attacks [7]–
[10], focusing on well structured data sources (e.g.,
time
series, audit/network records and systems calls). Differently,
our method deals with unstructured textual logs, which present
several open challenges in security analytics.
We implement the method on the top of cutting edge stream
data analytics technologies, e.g., Apache Storm and Cassandra.
Measurements have been done using runtime logs from a
real-world Air Trafﬁc Control information system provided by
4http://www.splunk.com
379
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:24 UTC from IEEE Xplore.  Restrictions apply. 
LEONARDO5, a top-leading industrial company in electronic
and information technologies for defense, aerospace, and land
security. We conduct off-line experiments to characterize the
system behavioral baseline, beforehand. Regular operation and
misuse conditions have been then emulated on the real system
to show how log.entropy-based measurements change upon the
occurrence of interesting activity across different log sources.
We investigate the extent to what measurements are impacted
by the characteristics of the interesting activity. The key
ﬁndings of our data-driven analysis are the following:
•
Entropy-based methods represent a valuable comple-
ment to security analytics solutions. Differently from
several existing approaches, the proposed method does
not rely on prior-encoded knowledge of interesting
patterns (e.g., it does not need to observe in advance
log entries generated under misuse conditions) and
it does not require labeled training data. This is
well-suited with real production setting requirements,
where building a behavioral baseline is expected to be
more feasible than collecting and labeling a signiﬁcant
sample of actual incidents.
• Misuse episodes may generate interesting activity
across different and distributed log streams. Experi-
ments done in the context of our reference system re-
veal that the symptoms of an application misuse, such
as removal/tampering of sensitive data, are tracked
by different
logs. Relationships across logs might
go unnoticed to analysts focusing on speciﬁc error
patterns, or they are even hard to be envisioned,
beforehand (e.g., at the time rules are conﬁgured into
the SIEM). Leveraging measurements from different
sources is effective to uncover these relationships.
Log.entropy-based measurements can reveal the oc-
currence of interesting activity with high precision
and recall. We synthesize interesting activity under
different parameters to validate and explore the bound-
aries of our method. For example, results indicate
that interesting activity consisting of only one out of
hundreds log entries, can be discriminated even when
it is half-similar to baseline entries.
•
The rest of the paper is organized as follows. Section II
positions our research with respect to existing work. Section
III and IV present the method and related analytics-based
implementation, respectively. Section V describes the reference
critical information system. Section VI presents off-line and
on-line measurements done by means of the analytics frame-
work. Section VII presents the validation approach and results.
Section VIII discusses the threats to validity, while Section IX
concludes the work.
II. RELATED WORK
In the following we position our research with respect to
existing work in security ﬁltering, event-logs-based security
analytics and entropy-based methods.
A. Security Filtering
The automated process that aims to retain interesting
activity from the raw data is also known as ﬁltering. Filtering
is among the most popular tasks in security analysis and it aims
to reduce the amount of data requiring human attention at the
5http://www.leonardocompany.com/en
380
beginning of the analysis workﬂow [11]. Several approaches
have been proposed so far to infer measurements for ﬁltering
security data and alerts.
The technique discussed by [12] relies on a statistical
approach to combine different features, such as number of
occurrences, frequency of the signatures, and prior knowledge
regarding the alerts. The metrics computed from the features
are compared to thresholds to establish whether an alert should
be discarded. An attribute enrichment approach is proposed
by [13]. The work introduces 18 quality parameters, which
are added to a set of traditional features. The features are
used to compute a score for classifying alerts; computation
requires background knowledge, such as the network topol-
ogy, operating systems, and vulnerabilities. The work [14]
develops a fuzzy-neural network from a set of labeled data
with the aim of classifying future occurrences of the alerts.
Training/validation of neural networks is based on massive
labelled datasets. In [15] it is presented an attribute-value
classiﬁer to predict the class (i.e., false or true positive) of
a given alert. The classiﬁer is trained by means of a labeled
dataset. This approach requires the human intervention to (i)
label the training data, and (ii) review the predictions made
by the classiﬁer. Filtering is addressed by an outlier detection
algorithm in [16]. The algorithm leverages the use of weights
to take into account the importance of the attributes of the
alerts, such as the destination port or the type. Frequent pairs
of attribute-value are used as feature to discriminate false
positives. Analysis frameworks that encompass a ﬁltering step
are proposed in [17] and [18]. The former discards irrelevant
alerts based on active monitoring; every time an alert
is
triggered, the corresponding packet is fed to a subsystem that
identiﬁes the potential vulnerabilities the attacker is trying to
exploit. The latter organizes the alert types in a decision tree;
the tree is traversed to automatically infer the root cause of
runtime alerts. An approach to ﬁlter alerts is proposed by [19].
The key idea underlying the approach is that alerts belong
to a limited number of clusters; the clusters are established
through a generalization hierarchy. The ﬁlter retains the alerts
that belong to given clusters.
Our entropy-based method overcomes several practical
drawbacks of the existing techniques: (i) with respect
to
[12], [14] and [15] it does not require a labeled dataset, (ii)
differently from [13], [17] and [19] it does not need any prior
domain/background knowledge and, (iii) differently from [15],
[16], [19] and [18] it does not rely on the human intervention.
B. Event-logs-based Security Analytics
Work that uses event logs along with security analytics
technologies is discussed in the following.
Authors in [20] propose a framework for the detection of
Advanced Persistent Threat (APT), which uses MapReduce
to analyze security events from different log sources, such
as Virtual Private Network, Intrusion Detection System and
ﬁrewall. The approach relies on a signature database with
known bad information. A cloud-based distributed and parallel
security log analysis framework for organizational security is
presented in [21]. The framework supports the analysis of sys-
tem, network, and transaction logs by using streaming analysis
features. The work [22] proposes an approach to analyze large
logs for detecting host misbehavior. The approach combines
data mining and supervised/unsupervised machine learning to
automate the detection. The approach uses DHCP servers,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:24 UTC from IEEE Xplore.  Restrictions apply. 
authentication servers, and ﬁrewall logs as data sources and
adopts Hadoop MapReduce. In [23] is proposed an approach
that analyzes logs collected from various network devices. The
approach, i.e., Beehive, consists of a parsing/ﬁlter/normalizing
step, a feature-generator and a detector.
Differently from [20] and [21] our method does not re-
quire normalizing/transforming the collected data to a uniform
format. With respect to [22], it does not rely on background
knowledge (e.g., network topology or machine types). More-
over, with respect to [22] and [23] our method does not need
to be tailored to the speciﬁc type of input data.
Event logs for security analytics have been used in critical
information systems protection. For example, [24] presents a
SCADA security framework for protecting electric power in-
frastructures. The framework consists of real-time monitoring
and anomaly detection components, which rely on different
sources, such as security, system, and ﬁle integrity logs.
Authors in [25] propose MELISSA, i.e., a tool for processing
SCADA logs to detect process-related threats. MELISSA relies
on pattern mining to identify the most and the least frequent
(expected to be anomalous) patterns of system behaviors. A
framework for Situational Awareness of Critical Infrastructure
and Networks (SACIN) is presented in [26]. The framework
gathers data, e.g., industrial automation systems and intrusion
detection systems logs, from different entities of a given
system, and uniforms their format for subsequent analysis.
Again, differently from our method, these solutions are
strongly speciﬁc for the systems in-hand:
the information
extraction process in [24] depends on the data sources, [25]
requires the translation of SCADA logs into patterns, while
signiﬁcant human expertise is needed to generate events from
data sources in [26].
C. Entropy-based Methods
Entropy-based methods have been proposed for security
analysis and detection of attacks.
For example, in [7] is proposed a method based on Virtual
Machine (VM) status to identify the occurrence of Denial of
Service (DoS) attacks. Information entropy is used to monitor
the status of VMs. The method has been applied to a real cloud
environment based on OpenStack. The method relies on several
pre-conditions (e.g., total number of VMs, number of mali-
cious VMs and stability of the state) and uses resources-related
measurements, i.e., CPU/IO usage and network throughput. In
[8] the Authors propose a DDoS traceback mechanism, which
measures entropy variations between regular and DDoS attack
trafﬁc. Entropy variations quantify the randomness of the ﬂows
at a router within a given time interval. If the entropy is higher
than a threshold, the mechanism traces the corresponding IP
via the upstream router. Data leveraged by the method include
timestamps and source/destination routers. A scalable method
to detect Command and Control (C&C) servers used in APTs
is presented in [9]. The method is based on the assumptions
that (i) regular activity is relatively common when compared
to suspect one, and (ii) malwares use ﬁxed user agent string
for connecting to C&C servers. The Authors propose an UF-
ICF (User agent Frequency and Inverse Channel Frequency)
term weighting ﬁlter to verify whether a site is likely to be a
web service or a C&C server; analysis is based on connection