# Entropy-Based Security Analytics: Measurements from a Critical Information System

**Authors:** Marcello Cinque, Raffaele Della Corte, and Antonio Pecchia  
**Conference:** 2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)

**Affiliations:**
- **Dipartimento di Ingegneria Elettrica e Tecnologie dell’Informazione, Università degli Studi di Napoli Federico II, Italy**
- **Consorzio Interuniversitario Nazionale per l’Informatica, Laboratorio CINI-ITEM “Carlo Savy”, Naples, Italy**

**Contact Information:**
- Marcello Cinque: macinque@unina.it
- Raffaele Della Corte: raffaele.dellacorte2@unina.it
- Antonio Pecchia: antonio.pecchia@unina.it

## Abstract
Critical information systems heavily rely on event logging techniques to collect data such as housekeeping/error events, execution traces, and variable dumps into unstructured text logs. These event logs are the primary source for gaining actionable intelligence from production systems. Despite their recognized importance, system and application logs remain underutilized in security analytics compared to conventional and structured data sources like audit traces, network flows, and intrusion detection logs.

This paper proposes a method to measure the occurrence of interesting activity (i.e., entries that should be followed up by analysts) within textual and heterogeneous runtime log streams. The method uses an entropy-based approach, which makes no assumptions about the structure of underlying log entries. We conducted experiments in a real-world Air Traffic Control information system using a data analytics framework. Our results suggest that the entropy-based method is a valuable complement to existing security analytics solutions.

**Keywords:** Event logging, security, log analytics, filtering, information systems.

## 1. Introduction
Critical information systems have become lucrative targets for cyber threats due to their role in supporting essential assets such as power grids, medical, financial, and transportation systems. These systems consist of various software components, including applications, middleware, databases, and operating systems, which emit text logs containing housekeeping/error events, execution traces, and variable dumps during operations. Security analytics is emerging as a new trend in corporate research to assist network managers in protecting critical assets and detecting suspicious activities and incidents [1], [2]. It leverages a variety of data sources generated at runtime, combining capabilities such as real-time monitoring, stream processing, advanced indexing, and efficient computation.

While valuable for failure analysis [3], system and application logs remain underutilized in security analytics compared to conventional sources like audit traces, network flows, and intrusion detection logs. This is due to the lack of systematic design and coding practices, leading to subjective, unstructured, and developer-dependent log entries at runtime.

Second-generation Security Information and Event Management (SIEM) systems [4], [5] represent the state-of-the-art in security analytics. Examples include AlienVault USM, IBM QRadar, LogRhythm, and Splunk Enterprise Security. These products rely on internal representation formats to import and consolidate data from various sources. However, built-in support for unstructured text logs is limited. For example, in the case of a syslog source, only a few standard fields (e.g., timestamp, hostname, severity) are automatically recognized, while the free text message is imported in its raw format. For legacy or application-dependent log sources, analysts must configure custom adapters. Once the text log is imported, analysts typically write ad-hoc filters to extract specific fields or search for entries matching given patterns.

Event logs can play a crucial role in security analysis. Past studies show that in more than 40% of cases, attackers may evade traditional protection mechanisms and conceal their presence until the application/system is misused [6]. Despite this, security analytics solutions continue to rely on traditional filtering approaches and human intervention to analyze raw text logs and find interesting activity, i.e., log entries that could reveal misuse and require further investigation.

This paper proposes a method to automatically measure the occurrence of interesting activity within textual and heterogeneous runtime log streams. The method leverages logarithmic entropy, which makes no assumptions about the structure of the underlying entries. The occurrence of interesting activity is inferred by computing the entropy of the runtime log streams relative to a system behavioral baseline. Previous research has explored entropy-based approaches or term weighting for security analysis and attack detection [7]–[10], focusing on well-structured data sources like time series, audit/network records, and system calls. In contrast, our method deals with unstructured textual logs, which present several challenges in security analytics.

We implemented the method using cutting-edge stream data analytics technologies, such as Apache Storm and Cassandra. Experiments were conducted using runtime logs from a real-world Air Traffic Control information system provided by Leonardo, a leading industrial company in electronic and information technologies for defense, aerospace, and land security. Offline experiments characterized the system's behavioral baseline, and regular operation and misuse conditions were emulated to show how log.entropy-based measurements change upon the occurrence of interesting activity across different log sources. We investigated the impact of the characteristics of the interesting activity on the measurements. The key findings of our data-driven analysis are:

- **Entropy-based methods** are a valuable complement to security analytics solutions. Unlike many existing approaches, our method does not rely on prior-encoded knowledge of interesting patterns and does not require labeled training data. This is well-suited for real production settings where building a behavioral baseline is more feasible than collecting and labeling significant samples of actual incidents.
- **Misuse episodes** may generate interesting activity across different and distributed log streams. Experiments in our reference system revealed that symptoms of application misuse, such as the removal or tampering of sensitive data, are tracked by different logs. Relationships across logs might go unnoticed by analysts focusing on specific error patterns. Leveraging measurements from different sources is effective in uncovering these relationships.
- **Log.entropy-based measurements** can reveal the occurrence of interesting activity with high precision and recall. We synthesized interesting activity under different parameters to validate and explore the boundaries of our method. Results indicate that even when interesting activity consists of only one out of hundreds of log entries, it can be discriminated even if it is half-similar to baseline entries.

The rest of the paper is organized as follows. Section 2 positions our research with respect to existing work. Sections 3 and 4 present the method and related analytics-based implementation, respectively. Section 5 describes the reference critical information system. Section 6 presents offline and online measurements done using the analytics framework. Section 7 presents the validation approach and results. Section 8 discusses the threats to validity, and Section 9 concludes the work.

## 2. Related Work
In this section, we position our research with respect to existing work in security filtering, event-logs-based security analytics, and entropy-based methods.

### A. Security Filtering
The automated process of retaining interesting activity from raw data is known as filtering. Filtering is a common task in security analysis aimed at reducing the amount of data requiring human attention. Several approaches have been proposed to infer measurements for filtering security data and alerts.

- **Statistical Approach** [12]: Combines features such as the number of occurrences, frequency of signatures, and prior knowledge about alerts. Metrics are compared to thresholds to determine whether an alert should be discarded.
- **Attribute Enrichment** [13]: Introduces 18 quality parameters added to traditional features to compute a score for classifying alerts. Computation requires background knowledge like network topology, operating systems, and vulnerabilities.
- **Fuzzy-Neural Network** [14]: Develops a fuzzy-neural network from a set of labeled data to classify future alerts. Training and validation require large labeled datasets.
- **Attribute-Value Classifier** [15]: Predicts the class (false or true positive) of an alert using a labeled dataset. Human intervention is required to label the training data and review predictions.
- **Outlier Detection Algorithm** [16]: Uses weights to account for the importance of alert attributes and frequent pairs of attribute-values to discriminate false positives.
- **Analysis Frameworks** [17], [18]: Discard irrelevant alerts based on active monitoring and organize alert types in a decision tree to infer the root cause of runtime alerts.
- **Clustering Approach** [19]: Filters alerts by clustering them through a generalization hierarchy.

Our entropy-based method overcomes several practical drawbacks of existing techniques:
- **No Labeled Dataset**: Unlike [12], [14], and [15], it does not require a labeled dataset.
- **No Prior Knowledge**: Unlike [13], [17], and [19], it does not need prior domain or background knowledge.
- **No Human Intervention**: Unlike [15], [16], [19], and [18], it does not rely on human intervention.

### B. Event-Logs-Based Security Analytics
Several works use event logs along with security analytics technologies.

- **APT Detection Framework** [20]: Uses MapReduce to analyze security events from different log sources, relying on a signature database with known bad information.
- **Cloud-Based Framework** [21]: Supports the analysis of system, network, and transaction logs using streaming analysis features.
- **Host Misbehavior Detection** [22]: Combines data mining and machine learning to detect host misbehavior using DHCP servers, authentication servers, and firewall logs.
- **Beehive** [23]: Analyzes logs from various network devices, including parsing, filtering, normalizing, feature generation, and detection.

Unlike [20] and [21], our method does not require normalizing or transforming collected data to a uniform format. Unlike [22], it does not rely on background knowledge (e.g., network topology or machine types). Additionally, unlike [22] and [23], our method does not need to be tailored to specific input data types.

Event logs for security analytics have also been used in critical information systems protection. For example:
- **SCADA Security Framework** [24]: Protects electric power infrastructures with real-time monitoring and anomaly detection using security, system, and file integrity logs.
- **MELISSA** [25]: Processes SCADA logs to detect process-related threats using pattern mining.
- **SACIN Framework** [26]: Gathers data from different entities, such as industrial automation systems and intrusion detection systems, and unifies their format for analysis.

Unlike our method, these solutions are highly specific to the systems in question. The information extraction process in [24] depends on data sources, [25] requires translating SCADA logs into patterns, and [26] needs significant human expertise to generate events from data sources.

### C. Entropy-Based Methods
Entropy-based methods have been proposed for security analysis and attack detection.

- **VM Status Monitoring** [7]: Identifies Denial of Service (DoS) attacks using information entropy to monitor VM status. Relies on pre-conditions and resource-related measurements.
- **DDoS Traceback Mechanism** [8]: Measures entropy variations between regular and DDoS attack traffic to quantify flow randomness and trace IP addresses.
- **C&C Server Detection** [9]: Detects Command and Control (C&C) servers in Advanced Persistent Threats (APTs) using User agent Frequency and Inverse Channel Frequency (UF-ICF) term weighting.

Unlike these methods, our approach does not require pre-conditions, specific data types, or extensive background knowledge, making it more versatile and applicable to a broader range of scenarios.

## 3. Methodology
### 3.1 Overview
Our method measures the occurrence of interesting activity within textual and heterogeneous runtime log streams using an entropy-based approach. The method involves the following steps:
1. **Data Collection**: Collect runtime logs from the critical information system.
2. **Preprocessing**: Preprocess the logs to remove noise and standardize the format.
3. **Entropy Calculation**: Compute the entropy of the log streams relative to a system behavioral baseline.
4. **Interesting Activity Detection**: Identify log entries that deviate significantly from the baseline.

### 3.2 Data Collection
We collected runtime logs from a real-world Air Traffic Control information system provided by Leonardo. The logs include various types of events, such as housekeeping/error events, execution traces, and variable dumps.

### 3.3 Preprocessing
The preprocessing step involves:
- **Noise Removal**: Remove irrelevant and redundant entries.
- **Standardization**: Standardize the log format to ensure consistency.

### 3.4 Entropy Calculation
We use logarithmic entropy to measure the uncertainty or randomness in the log streams. The entropy \( H \) of a log stream is calculated as:
\[ H = -\sum_{i=1}^{n} p_i \log_2 p_i \]
where \( p_i \) is the probability of the \( i \)-th log entry.

### 3.5 Interesting Activity Detection
We compare the entropy of the current log stream to a predefined baseline. If the entropy deviates significantly from the baseline, it indicates the presence of interesting activity.

## 4. Implementation
We implemented the method using Apache Storm and Cassandra, which are cutting-edge stream data analytics technologies. Apache Storm is used for real-time processing of log streams, while Cassandra is used for storing and retrieving large volumes of log data.

### 4.1 Apache Storm
Apache Storm processes the log streams in real-time, performing the following tasks:
- **Data Ingestion**: Ingest log data from the critical information system.
- **Preprocessing**: Preprocess the log data to remove noise and standardize the format.
- **Entropy Calculation**: Compute the entropy of the log streams.
- **Alert Generation**: Generate alerts for log entries that deviate significantly from the baseline.

### 4.2 Cassandra
Cassandra is used to store and retrieve log data efficiently. It provides a scalable and fault-tolerant solution for handling large volumes of log data.

## 5. Reference System
The reference system is a real-world Air Traffic Control information system provided by Leonardo. The system includes various components, such as radar systems, communication networks, and control centers. The system generates a large volume of runtime logs, which are collected and analyzed using our method.

## 6. Experiments and Results
### 6.1 Offline Experiments
We conducted offline experiments to characterize the system's behavioral baseline. The baseline was established by analyzing historical log data under normal operating conditions.

### 6.2 Online Experiments
We emulated regular operation and misuse conditions on the real system to observe how log.entropy-based measurements change. The experiments involved:
- **Regular Operation**: Simulated normal system behavior.
- **Misuse Conditions**: Simulated various types of misuse, such as the removal or tampering of sensitive data.

### 6.3 Results
The results showed that log.entropy-based measurements can effectively detect the occurrence of interesting activity with high precision and recall. Even when interesting activity consists of only one out of hundreds of log entries, it can be discriminated if it is half-similar to baseline entries.

## 7. Validation
### 7.1 Validation Approach
We validated the method by synthesizing interesting activity under different parameters and comparing the results to the ground truth. The validation involved:
- **Synthetic Data Generation**: Generated synthetic log data with varying levels of interesting activity.
- **Performance Evaluation**: Evaluated the performance of the method in terms of precision, recall, and F1-score.

### 7.2 Results
The validation results confirmed that the entropy-based method can accurately detect interesting activity in log streams. The method achieved high precision and recall, even in scenarios with low levels of interesting activity.

## 8. Threats to Validity
### 8.1 Internal Validity
Internal validity concerns the extent to which the results can be attributed to the method itself. Potential threats include:
- **Data Quality**: The quality and completeness of the log data.
- **Baseline Establishment**: The accuracy of the baseline used for comparison.

### 8.2 External Validity
External validity concerns the generalizability of the results to other contexts. Potential threats include:
- **System Specificity**: The method's applicability to different types of critical information systems.
- **Environmental Factors**: The influence of environmental factors on the log data.

## 9. Conclusion
This paper presented an entropy-based method for measuring the occurrence of interesting activity within textual and heterogeneous runtime log streams. The method makes no assumptions about the structure of the underlying log entries and can be applied to a wide range of critical information systems. Experiments in a real-world Air Traffic Control information system demonstrated the effectiveness of the method in detecting interesting activity with high precision and recall. The method represents a valuable complement to existing security analytics solutions, offering a flexible and scalable approach to log analysis.

**Acknowledgments:** We would like to thank Leonardo for providing the real-world Air Traffic Control information system and the associated log data.

**References:**
- [1] Reference 1
- [2] Reference 2
- [3] Reference 3
- [4] Reference 4
- [5] Reference 5
- [6] Reference 6
- [7] Reference 7
- [8] Reference 8
- [9] Reference 9
- [10] Reference 10
- [11] Reference 11
- [12] Reference 12
- [13] Reference 13
- [14] Reference 14
- [15] Reference 15
- [16] Reference 16
- [17] Reference 17
- [18] Reference 18
- [19] Reference 19
- [20] Reference 20
- [21] Reference 21
- [22] Reference 22
- [23] Reference 23
- [24] Reference 24
- [25] Reference 25
- [26] Reference 26