User: How to use Python to write a simple crawler program to crawl the data on the website and process it?
Assistant: The following are the general steps for writing a simple crawler program in Python: 1. Import the requests library, and use the requests.get() method to obtain the HTML source code of the target web page. ``` import requests url = 'target URL' response = requests.get(url) html = response.text ``` 2. Import the BeautifulSoup library, parse the HTML source code, and use the find() or find_all() method to locate the required crawled data. ``` from bs4 import BeautifulSoup soup = BeautifulSoup(html, 'html.parser') #Use the find() method to locate the first target tag target_tag = soup.find('tag name', attrs={'attribute name': 'Attribute value'}) #Use the find_all() method to locate all target tags target_tags = soup.find_all('tag name', attrs={'attribute name':'attribute value'}) ``` 3. Traversing target tags, Get the required data and process it. ``` for tag in target_tags: #Get an attribute of the tag attribute = tag['attribute name'] #Get the text content of the tag text = tag.get_text() #Process data processing processed_data = process(text) ``` 4. Store the processed data into a file or database. ``` #Write data to file with open('file name', 'w', encoding='utf-8') as f: f.write(processed_data) #Store data to database import sqlite3 conn = sqlite3. connect('database name.db') cursor = conn.cursor() #create table cursor.execute('''CREATE TABLE target_data(attribute text, text text)''') #insert data cursor.execute(''' INSERT INTO target_data VALUES (?, ?)''', (attribute, text)) #Submit changes conn.commit() ``` Complete sample code: ``` import requests from bs4 import BeautifulSoup import sqlite3 def crawl_data(): url = 'https://www.example.com' response = requests.get(url) html = response.text soup = BeautifulSoup(html, 'html.parser') target_tags = soup.find_all('p', attrs= {'class':'target-class'}) conn = sqlite3.connect('mydatabase.db') cursor = conn.cursor() cursor.execute('''CREATE TABLE target_data(text text)''') for tag in target_tags: text = tag.get_text() processed_data = process(text) cursor.execute('''INSERT INTO target_data VALUES (?)''', (processed_data,)) conn.commit() conn.close() def process(text): #Data processing function return text.upper() if __name__ == '__main__': crawl_data()