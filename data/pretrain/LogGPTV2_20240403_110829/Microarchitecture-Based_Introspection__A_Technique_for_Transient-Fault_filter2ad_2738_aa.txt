title:Microarchitecture-Based Introspection: A Technique for Transient-Fault
Tolerance in Microprocessors
author:Moinuddin K. Qureshi and
Onur Mutlu and
Yale N. Patt
Microarchitecture-Based Introspection: A Technique for Transient-Fault
Tolerance in Microprocessors
Moinuddin K. Qureshi
Onur Mutlu
Yale N. Patt
Department of Electrical and Computer Engineering
The University of Texas at Austin
{moin, onur, patt}@hps.utexas.edu
Abstract
The increasing transient fault rate will necessitate on-
chip fault tolerance techniques in future processors. The
speed gap between the processor and the memory is also
increasing, causing the processor to stay idle for hundreds
of cycles while waiting for a long-latency cache miss to be
serviced. Even in the presence of aggressive prefetching
techniques, future processors are expected to waste signiﬁ-
cant processing bandwidth waiting for main memory.
This paper proposes Microarchitecture-Based Introspec-
tion (MBI), a transient-fault detection technique, which uti-
lizes the wasted processing bandwidth during long-latency
cache misses for redundant execution of the instruction
stream. MBI has modest hardware cost, requires mini-
mal modiﬁcations to the existing microarchitecture, and is
particularly well suited for memory-intensive applications.
Our evaluation reveals that the time redundancy of MBI re-
sults in an average IPC reduction of only 7.1% for memory-
intensive benchmarks in the SPEC CPU2000 suite. The av-
erage IPC reduction for the entire suite is 14.5%.
1. Introduction
Transient faults present a serious challenge to the cor-
rect operation of future processors. A transient fault occurs
in a processor when a high-energy cosmic particle strikes
and inverts the logical state of a transistor. Technology
trends such as reduction in operating voltage, increase in
processor frequency, and increase in the number of on-chip
transistors indicate that the transient fault rate is likely to
increase by orders of magnitude [15]. In order to address
this problem, future processors will need to incorporate on-
chip fault tolerance techniques. Fault-tolerant techniques
with low hardware overhead are desirable because they
make fault-tolerance pervasive by giving the users an option
of fault-tolerance without committing to a heavy hardware
cost. Thus, if the user chooses not to have a fault-tolerant
processor, then only the small hardware resources dedicated
solely to fault tolerance will go unused. This paper inves-
tigates a low hardware overhead fault tolerance mechanism
that is relatively simple and can be easily incorporated in an
existing microarchitecture.
Fault tolerance has traditionally been achieved using two
techniques: redundant code and redundant execution. Stor-
age structures have regular patterns, which lend themselves
to redundant codes. These structures can easily be protected
with well-understood techniques such as parity and Error
Correcting Codes (ECC). In contrast, combinational logic
structures have irregular patterns, which often necessitate
redundant execution for fault tolerance. Redundant exe-
cution can further be classified into space redundancy and
time redundancy. Space redundancy is achieved by execut-
ing a task on multiple disjoint structures. Space redundancy
has low performance overhead but requires hardware in pro-
portion to the number of redundant structures. Time redun-
dancy is achieved by executing a task on the same hard-
ware multiple times. Time redundancy has low hardware
overhead but high performance overhead. The performance
overhead of time redundancy can be reduced if the redun-
dant execution is scheduled during idle cycles in which the
processing bandwidth is otherwise wasted. This paper fo-
cuses on utilizing the wasted processing bandwidth due to
long-latency cache misses for time redundancy.
Processor frequency has increased at a much faster rate
than DRAM memory speed, which has led to a widen-
ing speed gap between the processor and the memory.
This problem is partially addressed in current processors
through multi-level on-chip caches, providing fast access
to recently-accessed data. Unfortunately, a cache miss at
the highest level on-chip cache almost always stalls the
processor [5] because the latency to main memory is very
long (in the order of hundreds of cycles [21]). The stalled
processor cannot execute instructions until the long-latency
cache miss is completely serviced. Therefore, these cache
misses translate into idle cycles for the processor, resulting
in wasted processing bandwidth. The problem is even worse
for memory-intensive applications because these applica-
tions have large working sets, which cause frequent cache
misses. Applications that have traditionally required high
fault tolerance, such as transaction processing and database
workloads, tend to be memory-intensive [4]. In fact, a re-
cent study [3] showed that database workloads spend about
two-thirds of their execution time waiting for memory.
The processing bandwidth wasted due to long-latency
cache misses can be utilized for different purposes. Runa-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:10:11 UTC from IEEE Xplore.  Restrictions apply. 
head execution [8] pre-executes the application during long-
latency cache misses in order to generate prefetches. Li et
al. [6] use the spare bandwidth to reduce power by reducing
the processor voltage while a cache miss is being serviced.
Similarly, the wasted processing bandwidth can be used for
increasing the fault tolerance of a processor by re-executing
the application during the idle cycles.
In this paper, we propose Microarchitecture-Based In-
trospection (MBI), a time redundancy technique, to detect
transient faults. MBI achieves time redundancy by schedul-
ing the redundant execution of a program during idle cy-
cles in which a long-latency cache miss is being serviced.
MBI provides transient fault coverage for the entire pipeline
starting from instruction fetch to instruction commit. This
technique is completely microarchitecture based and re-
quires no support from the compiler or the ISA. It requires
modest hardware overhead and minimal changes to the ex-
isting microarchitecture. Because the technique attempts to
utilize idle cycles caused by long-latency cache misses, it is
particularly well-suited for memory-intensive applications.
Our experiments show that using MBI for 13 memory-
intensive benchmarks from SPEC CPU2000 results in an
average IPC reduction of only 7.1%. The average IPC re-
duction for the entire SPEC CPU2000 suite is 14.5%.
2. Motivation and Overview
2.1. Motivation for Using Cache Miss Cycles for
Redundant Execution
To measure the amount of processing bandwidth wasted
due to long-latency cache misses, and its potential for re-
dundant execution, we perform three experiments.1 In the
first experiment, we simulate the baseline machine with a
perfect instruction cache, a perfect data cache, and a perfect
branch predictor. We measure the Cycles Per Instruction2
(CPI) of this configuration (CPI-PERF). Redundant execu-
tion can leverage the load values and the resolved branch
directions from the first execution of the instruction stream
and therefore does not need to incur data cache misses or
branch mispredictions. Hence, the CPI of the redundant ex-
ecution can be expected to be very close to CPI-PERF. In
the second experiment, we measure the CPI of the baseline
machine with a perfect L2 cache, but real first-level (L1)
caches and a real branch predictor. The CPI increase for the
second experiment with respect to the CPI of the first exper-
iment represents the CPI added due to real L1 caches and
branch predictor (CPI-L1-BP). In the third experiment, we
measure the CPI of the baseline machine with real L1 and
1Section 4 describes the baseline configuration and benchmarks. The
memory system includes a 1MB L2 cache and an aggressive prefetcher.
2We use CPI for the experiments in this section because CPI provides
a more intuitive breakdown of execution time due to the different compo-
nents in a processor. For experiments in all other sections, we use Instruc-
tions Per Cycle (IPC) to measure performance.
L2 caches and a real branch predictor. The CPI increase for
the third experiment with respect to the CPI of the second
experiment represents the CPI added due to a real L2 cache
(CPI-L2). Figure 1 shows the CPI breakdown in terms of
CPI-PERF, CPI-L1-BP, and CPI-L2 for each benchmark.
n
o
i
t
c
u
r
t
s
n
I
r
e
p
s
e
l
c
y
C
2.4
2.2
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
16
mcf
art
lucas
wupwise
twolf
apsi
facerec
am m p
swim
HIGH-MEM CATEGORY
vpr
galgel
gcc
bzip2
vortex
m grid
CPI-L2
CPI-L1-BP
CPI-PERF
parser
equake
mesa
applu
LOW-MEM CATEGORY
perlbm k
crafty
gzip
sixtrack
gap
eon
fma3d
Figure 1. CPI breakdown.
For each benchmark, the proportion of CPI-L2 in total
CPI represents the fraction of processing bandwidth that
gets wasted due to L2 misses. We sort the benchmarks with
respect to this proportion and classify them into two cate-
gories: HIGH-MEM and LOW-MEM. The thirteen bench-
marks that have the highest proportion of CPI-L2 in total
CPI are grouped in the HIGH-MEM category, and the re-
maining benchmarks are grouped in the LOW-MEM cate-
gory. The benchmarks in the LOW-MEM category have a
small CPI-L2 component in total CPI. These benchmarks
either have memory access patterns that are easily prefetch-
able or have working sets that fit in the L2 cache.
All benchmarks in the HIGH-MEM category, except gal-
gel, have CPI-L2 more than CPI-PERF. These benchmarks
have sufficient idle processing bandwidth to re-execute the
program with negligible performance degradation. Though
our technique is primarily targeted towards such memory-
intensive applications, we perform our studies for all the
benchmarks in the SPEC CPU2000 suite.
2.2. Overview
In this section, we provide a brief overview of MBI.
The operation of a machine implementing MBI has two
modes: performance mode and introspection mode. Instruc-
tions are processed for the first time in performance mode.
Later, for fault tolerance, instructions are processed again
in introspection mode. The architectural state (i.e., archi-
tectural registers and memory) is updated only during the
performance mode. The purpose of introspection mode is
to verify the results produced during performance mode.
Thus, the architectural state is not modiﬁed during intro-
spection mode unless an error is found. Figure 2 shows the
state machine for the MBI mechanism.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:10:11 UTC from IEEE Xplore.  Restrictions apply. 
LONG_LATENCY_CACHE_MISS
OR  BUFFER_FULL
L2 CACHE
BUS
MEMORY
PERFORMANCE
MODE
INTROSPECTION
      MODE
MISS_SERVICED
OR BUFFER_EMPTY
Figure 2. State machine for the MBI mechanism.
In performance mode, instruction processing proceeds
without paying attention to incorrect operation resulting
from transient faults.
Instructions are fetched, executed,
and retired. The result of each retired instruction is stored
in a structure, called the backlog buffer. Writing results to
the backlog buffer is a post-retirement operation and does
not affect the instruction processing speed in performance
mode. When a long-latency cache miss occurs in perfor-
mance mode, the mechanism switches the state to introspec-
tion mode. In introspection mode, the processor re-executes
each instruction that was retired in performance mode and
compares the new result with the result of the previous ex-
ecution stored in the backlog buffer. If both results match,
then instruction processing for that instruction is assumed
to be fault-free and the respective entry is removed from the
backlog buffer. A mismatch between the two results signals
an error.
If there are no long-latency cache misses for a long pe-
riod, the processor will not enter introspection mode and the
backlog buffer will become full. To guarantee redundant ex-
ecution for all retired instructions, the processor is forced to
enter introspection mode when the backlog buffer becomes
full.3 The processor returns from introspection mode to
performance mode when either the backlog buffer becomes
empty or when the long-latency miss gets serviced.
3. Design
In this section, we provide details about the implemen-
tation and operation of MBI. Figure 3 shows the baseline
system and the extensions required to incorporate MBI into
an existing microarchitecture. The structures unique to MBI
are shaded.
We assume that the storage structures such as caches,
register files, and the backlog buffer are protected using
ECC and buses are protected using parity. The pipeline is
unprotected and vulnerable to transient faults.
3An alternative policy does not force the processor into introspection
mode when the backlog buffer is full. However, this policy will not pro-
vide redundant execution for all retired instructions and, therefore, will
not guarantee transient fault coverage for all instructions. The percentage
of instructions that are redundantly executed during an L2 miss (i.e., the
coverage of the instruction stream of this alternative policy) is shown in
Section 5.4.
I CACHE
D CACHE
MODE
PROCESSOR PIPELINE
(VULNERABLE TO TRANSIENT FAULTS)
F
E
T
C
H
COMPARATOR
=
ERROR
R
E
T
I
R
E
PERF
ARF
ISPEC
ARF
BACKLOG
BUFFER
HEAD−PTR
TAIL−PTR
Figure 3. Microarchitecture support for MBI.
3.1. Structures
MBI requires the following changes to the microarchi-
tecture:
1. Extra register ﬁle: The design includes two architec-
tural register ﬁles, PERF ARF and ISPEC ARF. Each
ARF contains the general purpose registers, control
registers, and the Program Counter (PC). The PERF
ARF is updated only in performance mode and the IS-
PEC ARF is updated only in introspection mode.
2. Backlog buffer: This storage structure keeps track of
the instructions that were retired in performance mode
but have not yet been processed in introspection mode.
It is implemented as a circular FIFO buffer and con-
tains two pointers: a tail pointer (TAIL-PTR) and
a head pointer (HEAD-PTR). The TAIL-PTR deter-
mines the entry in the backlog buffer where the result
of the next retiring instruction will be written. Results
are written in the backlog buffer only during perfor-
mance mode. When an instruction writes its result in
the backlog buffer, the TAIL-PTR is incremented to
point to the next entry. The HEAD-PTR determines
the location of the entry in the backlog buffer that will