instance, data poisoning with M = 0.1% (model poisoning
with M = 0.01%) reduces the global model accuracy of
FEMNIST from 83.4% to 81.4% (73.4%), CIFAR10 from
86.6% to 85.1% (82.9%), and Purchase from 85.4% to 85.3%
(76.4%). These findings directly contradict
the claims of
previous works [10], [41], [68] that Average AGR cannot
converge even with a single compromised client.
(2) Poisoning attacks have no impact on existing robust FL
algorithms even with impractically high M’s: At M=1%, data
or model poisoning attacks reduce the accuracy by only  τ, ∇ is scaled by
, otherwise the update is not
τ∥∇∥2
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1356
Table I: Comparing state-of-the-art aggregation rules (AGRs) in terms of accuracy, computation/memory cost, and theoretical
guarantees. We show results for CIFAR10 with 1,000 clients. Red cells show limitations of the corresponding AGR.
Type of aggregation
rule (AGR)
Non-robust
Dimension-wise
filtering
Vector-wise scaling
Example AGR
Average [40]
Median [70]
Trimmed-mean [70]
Sign-SGD +
majority voting [6]
Norm-bound [58]
Krum [10]
Multi-krum [10]
Vector-wise
filtering
Certification
Knowledge
transfer
Personalization
Emsemble [14]
Bulyan [41]
RFA [50]
RSA [36]
DnC [55]
CRFL [66]
Cronus [15]
Ditto [37]
EWC [71]
Accuracy
in non-iid FL
86.6
84.2
86.6
35.1
86.6
46.9
86.2
81.1
84.6
35.6
86.1
74.2
64.1
data
86.6
Needs public
Computation
at server
O(d)
O(dnlogn)
O(dnlogn)
O(d)
O(d)
O(dn2)
O(dn2)
O(dn2)
O(dn2)
O(d)
O(d)
O(d)
O(d)
O(d)
Memory cost
to client
O(d)
O(d)
O(d)
O(d)
O(M d)
O(d)
O(d)
Theoretical robustness
based on
None
convergence
convergence
convergence
Not established
convergence
convergence
convergence
convergence
convergence
filtering
Certification
Certification
filtering
None (depends
on server’s AGR)
changed. The final aggregate is an average of all the updates,
scaled or otherwise.
3) Multi-krum: Blanchard et al. [10] proposed Multi-krum
AGR as a modification to their own Krum AGR [10]. Multi-
krum selects an update using Krum and adds it to a selection
set, S. Multi-krum repeats this for the remaining updates
(which remain after removing the update that Krum selects)
until S has c updates such that n − c > 2m + 2, where n
is the number of selected clients and m is the number of
compromised clients in a given round. Finally, Multi-krum
averages the updates in S.
4) Trimmed-mean: Trimmed-mean [68], [70] aggregates
each dimension of input updates separately. It sorts the values
of the jth-dimension of all updates. Then it removes m (i.e.,
the number of compromised clients) of the largest and smallest
values of that dimension, and computes the average of the rest
of the values as its aggregate of the dimension j.
III. SYSTEMIZATION OF FL POISONING THREAT MODELS
We discuss the key dimensions of the threat models of
poisoning attacks on FL, and argue that only two combinations
of these dimensions are of practical interest for production FL.
Note that, there exist taxonomies of FL poisoning attacks [27],
[31] which provide comprehensive overviews of the poisoning
attacks in existing literature. In contrast, our work aims to
provide a systematic framework to model the existing and
future poisoning threats to federated learning.
A. Dimensions of Poisoning Threat to FL
In this section, we build on previous systemization efforts
for adversarial ML [4], [9], [29], [43] and present three key
dimensions for the threat model of FL poisoning, as shown in
Table II.
1) Adversary’s Objective: Inspired by [9], we define three
attributes of the adversary’s objectives.
Security violation: The adversary may aim to cause an
integrity violation, i.e., to evade detection without disrupting
normal service operations, or an availability violation, i.e., to
compromise the service for legitimate users.
Attack specificity: The attack is discriminate if it aims to
cause misclassification of a specific set/class of samples; it is
indiscriminate otherwise.
Error specificity: This attribute is especially relevant in multi-
class classification settings. It is specific if the attacker’s goal
is to have a sample misclassified as a specific class; the attack
is generic if the attacker does not care about the wrong label
assigned to the misclassified samples.
Adversary objectives in different classes of poisoning: Here,
based on the above taxonomy, we discuss the adversary’s
objective for different types of poisoning attacks (Figure 1).