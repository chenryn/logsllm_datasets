i=0).
i=0,{(cid:101)Gi(Lt)}t
i}t
H(Lt|{G(cid:48)
Furthermore, we quantify the behavior of indistinguishability
over time. For our analysis, we continue to consider the worst
case prior of the adversary knowing the entire graph sequence
except the link Lt. To make the analysis tractable, we add
another condition that if the link L exists, then it exists in
all the graphs (link deletions are rare in real world social
networks). For a large-scale graph, only one link would not
affect the clustering result. Then, we have
i=0) ≥ H(L|{G(cid:48)
i=0,{(cid:101)Gi(L)}t
Theorem 1: The indistinguishability decreases with time,
H(L|{G(cid:48)
i}t
i=0)
(2)
The inequality follows from the theorem conditioning reduces
entropy in [7]. Eq.2 shows that the indistinguishability would
not increase as time evolves. The reason is that over time,
multiple perturbed graphs can be used by the adversary to
infer more information about link L.
i=0,{(cid:101)Gi(L)}t+1
i}t+1
Next, we theoretically show why LinkMirage has better
privacy performance than the static method. For each graph
Gt, denote the perturbed graphs using LinkMirage and the
static method as G(cid:48)
(cid:48),s
t , respectively.
t, G
Theorem 2: The indistinguishability for LinkMirage is
i=0,{(cid:101)Gi(Lt)}t
greater than that for the static perturbation method, i.e.
i}t
H(Lt|{G(cid:48)
i=0,{(cid:101)Gi(Lt)}t
(cid:48),s
i }t
i=0) ≥ H(Lt|{G
i=0)
(3)
Proof: In LinkMirage, the perturbation for the current
graph Gt is based on perturbation for Gt−1. Let us denote
the changed subgraph between Gt−1, Gt as Gt−ch, then
i=0,{(cid:101)Gi(Lt)}t
H(Lt|{G
i}t
(cid:48)
=H(Lt|{G
i}t−2
(cid:48)
i=0, G
=H(Lt|{G
i}t−1
(cid:48)
i=0, G
i=0,{(cid:101)Gi(Lt)}t
≥H(Lt|{G
i}t−1
(cid:48)
i=0, Gt
i }t
≥H(Lt|{G
(cid:48),s
t−ch,{(cid:101)Gi(Lt)}t
(cid:48),s,{(cid:101)Gi(Lt)}t
i=0)
t − G
(cid:48)
(cid:48)
t−ch, G
i=0)
i=0)
(cid:48)
t−1, G
(cid:48),s
i=0)
t−ch,{(cid:101)Gi(Lt)}t
(cid:48),s
i=0)
where the ﬁrst inequality also comes from the theorem condi-
tioning reduces entropy in [7]. The second inequality general-
izes the ﬁrst inequality from a snapshot t to the entire sequence.
From Eq.3, we can see that LinkMirage may offer superior
indistinguishability compared to the static perturbation, and
thus provides higher privacy.
Comparison with previous work: Next, we experimentally
analyze our indistinguishability metric over time. Fig. 6 depicts
8
01234567800.10.20.30.40.5(a) Timestamp tIndistinguishability  k=5, Mittal et al.k=5, LinkMiragek=20, Mittal et al.k=20, LinkMirageHay’s et al.01234567800.10.20.30.40.5(b) Timestamp tIndistinguishability  k=5, Mittal et al.k=5, LinkMiragek=20, Mittal et al.k=20, LinkMirageHay et al.Fig. 7. (a)(b) show the temporal anti-aggregation privacy for the Google+ dataset and the Facebook dataset, respectively. The anti-aggregation privacy decreases
as time evolves because more information is leaked with more perturbed graphs available. Leveraging selective perturbation, LinkMirage achieves much better
anti-aggregation privacy than the static baseline method.
the indistinguishability metric using the whole Facebook graph
sequence and the sampled Facebook graph sequence with 80%
overlap. We can see that the static perturbation leaks more
information over time. In contrast, the selective perturbation
achieves signiﬁcantly higher indistinguishability. In Fig. 6(a),
after 9 snapshots, and using k = 5, the indistinguishability
of the static perturbation method is roughly 1/10 of the
indistinguishability of LinkMirage. This is because selective
perturbation explicitly takes the temporal evolution into con-
sideration, and stems privacy degradation via the selective
perturbation step. Comparing Fig. 6(a) and (b), LinkMirage
has more advantages for larger overlapped graph sequence.
We also compare with the work of Hay et al. in [16], For the
ﬁrst timestamp, the probability for a real link to be preserved
in the anonymized graph is 1 − r/m. As time evolves, the
probability would decrease to (1− r/m)t. Combined with the
prior probability, the corresponding indistinguishability for the
method of Hay et al. is shown as the black dotted line in
Fig. 6, which converges to 0 very quickly (we also consider
r/m = 0.5 which would substantially hurt utility [16])
Compared with the work of Hay et al, LinkMirage signiﬁcantly
improves privacy performance. Even when t = 1, LinkMirage
with k = 20 achieves up to 10x improvement over the
approach of Hay et al. in the indistinguishability performance.
D. Anti-aggregation Privacy
Next, we consider the adversaries who try to aggregate all
the previously published graphs to infer more information.
Recall that after community detection in our algorithm, we
anonymize the links by leveraging the k-hop random walk.
Therefore, the perturbed graph G(cid:48) is actually a sampling of
the k-hop graph Gk, where the k-hop graph Gk represents
graph where all the k-hop neighbors in the original graph
are connected. It is intuitive that a larger difference between
Gk and G(cid:48) represents better privacy. Here, we utilize the
distance between the corresponding transition probability ma-
trices (cid:107)P k
4 to measure this difference. And we
extend the deﬁnition of total variance [18] from vector to
matrix by averaging total variance distance of each row in the
t − P (cid:48)
matrix, i.e. (cid:107)P k
t (v)(cid:107)TV,
t (v), P (cid:48)
where P k
t (v) denotes the v-th row of P k
t. We then
formally deﬁne the anti-aggregation privacy as
(cid:80)|Vt|
v=1 (cid:107)P k
t (v) − P (cid:48)
t , P (cid:48)
t(cid:107)TV = 1|Vt|
t − P (cid:48)
t(cid:107)TV
Deﬁnition 3: The anti-aggregation privacy for a perturbed
4We choose the total variance distance to evaluate the statistical distance
between P k
t and P (cid:48)
t as in [27].
t − P (cid:48)
t, k) = (cid:107)P k
graph G(cid:48)
bation parameter k is Privacyaa(Gt, G(cid:48)
t with respect to the original graph Gt and the pertur-
t(cid:107)TV.
The adversary’s ﬁnal objective is to obtain an estimated
measurement of the original graph, e.g. the estimated transition
probability matrix ˆPt which satisﬁes ˆP k
t. A straightfor-
ward manner to evaluate privacy is to compute the estimation
error of the transition probability matrix i.e. (cid:107)Pt − ˆPt(cid:107)TV.
We can derive the relationship between the anti-aggregation
privacy and the estimation error as (we defer the proofs to the
Appendix to improve readability.)
t = P (cid:48)
Theorem 3: The anti-aggregation privacy is a lower bound
of the estimation error for the adversaries, and
t(cid:107)TV ≤ k(cid:107)Pt − ˆPt(cid:107)TV
t − P (cid:48)
(cid:107)P k
(4)
0,··· , G(cid:48)
i=0,1··· ,t G(cid:48)
t = (cid:83)
We further consider the network evolution where the adversary
can combine all the perviously perturbed graphs together to
extract more k-hop information of the current graph. Under
this situation, a strategic methodology for the adversary is to
combine the perturbed graph series G(cid:48)
t, to construct
a new perturbed graph ˘G(cid:48)
t, where ˘G(cid:48)
i. The
combined perturbed graph ˘G(cid:48)
t contains more information about
t than G(cid:48)
t. Correspondingly, the transition
the k-hop graph Gk
t of the combined perturbed graph ˘G(cid:48)
probability matrix ˘P (cid:48)
would provide more information than P (cid:48)
t
t. That is to say, the
anti-aggregation privacy decreases with time.
Comparison with previous work: We evaluate the anti-
aggregation privacy of LinkMirage on both the Google+
dataset and the Facebook dataset. Here we perform our exper-
iments based on a conservative assumption that a link always
exists after it
is introduced. The anti-aggregation privacy
decreases with time since more information about the k-hop
neighbors of the graph is leaked as shown in Fig. 7. Our
selective perturbation preserves correlation between consec-
utive graphs, therefore leaks less information and achieves
better privacy than the static baseline method. For the Google+
dataset, the anti-aggregation privacy for the method of Mittal
et al. is only 1/10 of LinkMirage after 84 timestamps.
E. Relationship with Differential Privacy
Our anti-inference privacy analysis considers the worst-
case adversarial prior to infer the existence of a link in the
graph. Next, we uncover a novel relationship between this anti-
inference privacy and differential privacy.
9
102030405060708000.20.40.60.81(a) Timestamp tAnti−aggregation Privacy  K=2,Mittal et al.K=2,LinkMirage0123456780.60.650.70.750.80.850.90.951(b) Timestamp tAnti−aggregation Privacy  K=2,Mittal et al.K=2,LinkMirageA. Anonymous Communication [11], [28], [29]
[28],
As a concrete application, we consider the problem of
anonymous communication [11],
[29]. Systems for
anonymous communication aim to improve user’s privacy
by hiding the communication link between the user and the
remote destination. Nagaraja et al. and others [11], [28], [29]
have suggested that the security of anonymity systems can be
improved by leveraging users’ trusted social contacts.
We envision that our work can be a key enabler for the
design of such social network based systems, while preserving
the privacy of users’ social relationships. We restrict our
analysis to low-latency anonymity systems that leverage social
links, such as the Pisces protocol [28].
Similar to the Tor protocol, users in Pisces rely on proxy
servers and onion routing for anonymous communication.
However, the relays involved in the onion routing path are
chosen by performing a random walk on a trusted social
network topology. Recall that LinkMirage better preserves the
evolution of temporal graphs in Fig. 3. We now show that this
translates into improved anonymity over time, by performing
an analysis of the degradation of user anonymity over multiple
graph snapshots. For each graph snapshot, we consider a worst
case anonymity analysis as follows: if a user’s neighbor in the
social topology is malicious, then over multiple communica-
tion rounds (within that graph instance) its anonymity will be
compromised using state-of-the-art trafﬁc analysis attacks [41].
Now, suppose that all of a user’s neighbors in the ﬁrst graph
instance are honest. As the perturbed graph sequence evolves,
there is further potential for degradation of user anonymity
since in the subsequent instances, there is a chance of the user
connecting to a malicious neighbor. Suppose the probability
for a node to be malicious is f. Denote nt(v) as the distinct
neighbors of node v at time t. For a temporal graph sequence,
the number of the union neighbors ∪t
k=0nk(v) of v increases
with time, and the probability for v to be attacked under the
k=0nk(v)|. Note that in
worst case is P attack
practice, the adversary’s prior information will be signiﬁcantly
less than the worst-case adversary.
(v) = 1− (1− f )|∪t
Fig. 8 depicts the degradation of the worst-case anonymity
with respect to the number of perturbed topologies. We can
see that the attack probability for our method is lower than the
static approach with a factor up to 2. This is because over con-
secutive graph instances, the users’ social neighborhood has
higher similarity as compared to the static approach, reducing
potential for anonymity degradation. Therefore, LinkMirage
can provide better security for anonymous communication, and
other social trust based applications.
t
Fig. 8. The worst case probability of deanonymizing users’ communications
(f = 0.1). Over time, LinkMirage provides better anonymity compared to the
static approaches.
Differential privacy is a popular theory to evaluate the
privacy of a perturbation scheme [12], [13]. The framework
of differential privacy deﬁnes local sensitivity of a query
function f on a dataset D1 as the maximal |f (D1)− f (D2)|1
for all D2 differing from D1 in at most one element df =
maxD2 (cid:107)f (D1) − f (D2(cid:107)1. Based on the theory of differential
privacy, a mechanism that adds independent Laplacian noise
with parameter df / to the query function f, satisﬁes -
differential privacy. The degree of added noise, which de-
termines the utility of the mechanism, depends on the local
sensitivity. To achieve a good utility as well as privacy,
the local sensitivity df should be as small as possible. The
following lemma demonstrates the effectiveness of our worst-
case Bayesian analysis by showing that the objective for good
utility-privacy balance under our worst-case Bayesian analysis
is equivalent to that under differential privacy.
Remark 1: The requirement for good utility-privacy balance
in differential privacy is equivalent to the objective of our
Bayesian analysis under the worst case. (We defer the proofs
to Appendix to improve readability.)
F. Summary for Privacy Analysis
• LinkMirage provides rigorous privacy guarantees to defend
against adversaries who have prior information about the
original graphs, and the adversaries who aim to combine
multiple released graphs to infer more information.
• LinkMirage shows signiﬁcant privacy advantages in anti-
inference privacy, indistinguishability and anti-aggregation
privacy, by outperforming previous methods by a factor up
to 10.
VI. APPLICATIONS
B. Vertex Anonymity [25], [34], [45]
Applications such as anonymous communication [11], [28],
[29] and vertex anonymity mechanisms [25], [34], [45] can uti-
lize LinkMirage to obtain the entire obfuscated social graphs.
Alternatively, each individual user can query LinkMirage for
his/her perturbed neighborhoods to set up distributed social re-
lationship based applications such as SybilLimit [43]. Further,
the OSN providers can also leverage LinkMirage to perturb
the original social topologies only once and support multiple
privacy-preserving graph analytics, e.g., privately compute the
pagerank/modularity of social networks.
Previous work for vertex anonymity [25], [34], [45] would