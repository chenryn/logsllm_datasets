a master hacker. In either case, our process aims to max-
imize the contributions of less experienced hackers while
making the employment of master hackers more efﬁcient.
Scenario A (cont.) Alice learns the types of inputs her
target accepts. These include input through network
sockets as well as conﬁguration ﬁles the server reads
when started. The fuzzing tool she is familiar with
doesn’t support network fuzzing, so she makes a note
for a future analyst to try network fuzzing. However,
she knows how to start a fuzzing run based on ﬁle input.
Scenario B (cont.) James writes a fuzzing harness for
the browser’s more complicated media libraries, and
he packages his work using a Dockerﬁle. Alice helps,
as she had not yet learned how to use Docker.
Automated exploration Once a team learns how to
manipulate the inputs of a program, it iteratively performs
these manipulations to enumerate as much functionality
of the program as possible. This maximizes the chance
of ﬁnding a vulnerable condition. While “sometimes, a
‘lucky’ run-time failure leads to a vulnerability [11],” we
focus most in this phase on testing the target program in a
fuzzer using the harnesses produced by the previous phase.
In order to make results repeatable, our team standardized
the output of the attack surface phase to be a Dockerﬁle [3]
that combined the target program and its fuzz harness.
A hacker’s proﬁciency, along with a consideration of
the suitability of a given target determines the choice
of a fuzzer. The effectiveness of a fuzzer includes the
efﬁciency of harnessing the target and features (such as
address sanitization, scalability, speed, and so on). Differ-
ent fuzzers favor different types of targets. As an example,
LibFuzzer aids in the work of writing a fuzz harness for
a library, whereas American Fuzzy Lop (AFL) enables a
hacker to begin fuzzing quickly given a binary target that
reads its input from a ﬁle or the standard input stream.
Scenario A (cont.) Alice starts a fuzzing run on the
unmodiﬁed NTP program with conﬁguration ﬁles as
the fuzzed input.
Scenario B (cont.) James deploys his browser media
handling harnesses for fuzzing. They both work on
other targets while the fuzzers run.
Vulnerability recognition Hackers who discover bugs
while iterating through the process must conﬁrm whether
the bugs are vulnerabilities. A vulnerability exists when
a bug is proven to be exploitable by an attacker [34].
This can be as simple as running the target program with
the crashing input identiﬁed in the previous phase, or as
complicated as setting up a complex system to observe
the real-world effects of certain input. Automation in this
phase might be necessary to balance the amount of human
time that is required to review results, especially when
a multitude of program crashes are discovered.
Scenario A (cont.) Alice begins another target.
Scenario B (cont.) Fuzzing discovers six inputs that
cause the targeted browser to crash. James is not able
to exploit these bugs, so Meghan takes on the task.
James shifts his focus to fuzzing the browser’s use of
Transport Layer Security (TLS).
Reporting Finally, the hacker who ﬁnds a vulnerability
prepares a report that allows developers to correct the
bug. A clear description of the impact and prevalence of
1132    29th USENIX Security Symposium
USENIX Association
the vulnerability allows software maintainers to prioritize
their efforts. The report can take on different forms, but as
The CERT Guide to Coordinated Vulnerability Disclosure
states, the technical and practical details of the vulnerabil-
ity and attack scenario should be well-documented [16].
To aid in the growth of other hackers, reports should be
readily available and searchable.
Scenario B (cont.) Meghan documents her ﬁndings,
along with the ﬁndings of James. Meghan and James
work together to package the exploit as a usable proof
of concept. Later, the team discusses their results.
3.1 Depth-ﬁrst strategy (SD)
The most obvious targeting strategy resembles a depth-
ﬁrst search. First, hackers select a small set of targets based
on some metric of operational impact. For each selected
target, the team spends time auditing the software for
bugs. This work ﬂow is very natural: it focuses the team’s
effort on one software artifact at a time. Researchers
select the target at the very beginning of their work and
persistently look at that target for a notable period of time.
The depth-ﬁrst work ﬂow has found bugs in large
software that requires a familiarization period [11]. For
example, Google Project Zero researchers applied this
strategy to ﬁnd bugs in Apple’s Safari browser. The re-
searchers harnessed the underlying libraries used in Safari,
and this required signiﬁcant program understanding along
with modiﬁcations to the build chain. They found 26 bugs
over the course of one year using custom-built tools [13].
This strategy is straightforward from a management
perspective. A team leader collects information from
each hacker and distributes it to the teammates inspecting
the same target. The leader divides work based on the
approach of each team member. For example, one hacker
might examine the unit tests distributed with the target
software, modifying them to suit the team’s aims; another
could analyze the software with a popular static-analysis
tool; and yet another could attempt to harness different
parts of the target program to work with a fuzzer. The
responsibility for scheduling the fuzzing jobs and
subsequent review often falls on the author of a harness.
Hackers employing SD record information collectively
because it is immediately relevant to the other team
members. To promote coaching, the team pairs novice
hackers with experts hoping the novice will assimilate
concepts and techniques from the expert.
The primary pitfall of SD appears to be its inefﬁciency
relative to the broad skill levels found on practical teams.
With few software artifacts under scrutiny, the team will
exhaust the easier tasks related to ﬁnding bugs. This
leaves apprentices and possibly even journeymen less
able to contribute. Simultaneously, masters might ﬁnd
themselves idle or performing tasks better suited for the
other skill levels at the beginning of a project.
Another pitfall is the inefﬁcient use of automation. After
starting a fuzzing run, the team is left to continue working
on the same target. They might build additional fuzzing
harnesses or carry out in-depth manual analysis. Yet the
automation might later uncover information that would
have aided those processes, or it might even ﬁnd the bugs
they seek. Ploughing forward might waste human effort.
3.2 Breadth-ﬁrst strategy (SB)
We devised a new strategy that aims to address the pitfalls
of SD. Our goals were to scale the vulnerability-discovery
process to support a growing team of hackers, reduce
hacker fatigue, and increase the production of fuzz
harnesses. To do this, our strategy relies on the idea of
drastically increasing the pool of software targets. We
encouraged hackers to produce the greatest number of
fuzzing harnesses possible in each workday. We call this
the breadth-ﬁrst strategy (SB).
SB encourages apprentice-level hackers to give up
when it becomes clear that harnessing a particular target
would require a signiﬁcant time investment. Rather than
continue down a “rabbit hole,” apprentices document any
pertinent information about the target before moving it
to a separate “journeyman” queue. This provides more
experienced hackers material to review before applying
their more experienced abilities.
We posit that the key to this strategy is to collect a large
queue of targets and, for each target, have apprentices do
the simplest possible thing and nothing more. Keeping
apprentices out of rabbit holes allows more skilled hackers
to more deeply investigate a target once it is accompanied
by a report. In some cases, apprentices produce a working
build or even a corpus of fuzzing outputs, but not if produc-
ing these artifacts exceeds their abilities. Ways to generate
large pools of interesting targets include (1) dividing
a device into its software components, (2) following
a thorough analysis of the system-level attack surface,
(3) enumerating library dependencies, and (4) investigat-
ing multiple bug-bounties. Having a large pool of targets
allows apprentices to reject targets whose obstacles
exceed their ability. Examples might include software
with challenging run-time requirements, such as real-time
operating systems running on niche hardware; programs
that require dynamic network streams like FTP; programs
requiring extensive system conﬁgurations; or programs
that make use of a custom build process. With such a large
USENIX Association
29th USENIX Security Symposium    1133
queue, prioritizing the targets so hackers spend more time
on higher-value items becomes critical. For example,
hackers on a penetration-testing team should prioritize
a target that allows external network connections.
An important consideration in our study was ﬁguring
out how to train new members quickly, while at the same
time allowing them to provide operational value to the
team. A large queue of targets allows apprentices to select
those compatible with the tools that they already know
how to use. When they ﬁnd that a target does not work
with a tool they know, they can record what they learned
and move it into a journeyman queue. Journeymen pick
up targets that an apprentice had begun and push them
into the exploration stage. The apprentice can, in turn,
learn from that work. Each team member’s work is thus
frequently reviewed by more experienced people, and
there is a clear path for someone to learn based on the
experience of others. Similarly, master hackers record
the problems that they overcome along with the types
of solutions that they apply. These notes frequently help
journeymen grow in knowledge too.
To make efﬁcient use of automation, all work should
stop on a particular target whenever a new automated
job begins. Only once that job has completed (based
on some predetermined measure of completeness) are
the results reviewed, incorporated into the ﬁndings, and
used to determine next steps. In this way, unnecessary
human effort is minimized by relying on automation to
the greatest extent possible.
4 Experiment
We designed a human study to investigate our two
strategies: depth-ﬁrst (SD) and breadth-ﬁrst (SB). Our
experiment took place over the course of ten days, as sum-
marized in Figure 3. This counterbalanced design follows
The SAGE Encyclopedia of Communication Research
Methods [8] and includes between-subjects tests at the end
of the ﬁrst week and within-subjects tests at the end of the
second week [6,9,27]. We ran our experiment on the busi-
ness days from November 7 through November 22, 2019,
taking the 8th and 11th off for Veteran’s day. The detailed
schedule of our experiment appears in Appendix B.
4.1 Subject selection
Our subjects drew from a pool of US Cyber Command
personnel, each of whom had at least a basic understand-
ing of the principles of system and software security.
Our primary means of recruiting was a pamphlet posted
throughout US Cyber Command work spaces, but we also
invited promising candidates by email. We advertised our
goal as identifying the best target-selection strategy for
bug ﬁnding, and we indicated that selected subjects would
spend two weeks working with expert hackers to analyze
a range of real software. Finally, we noted that we would
provide an AFL fuzzing tutorial for all participants. Our
pamphlet asked for applicants who (1) had experience with
Linux, (2) could work with open-source projects, (3) could
conduct Internet-based target research, and (4) could read
and modify C programs. 15 people indicated interest. Can-
didates signed a participation agreement and completed
a self-assessment (Appendix A) used to assign teams.
4.2 Orientation
Twelve subjects were present on the ﬁrst day of our
experiment. We used the subjects’ self-reported years of
hacking experience to create groups. Then, we performed
a representative random sample to assign the present
subjects to two balanced teams of six. The distribution
of the original ﬁfteen applicants contained: eight subjects
under one year of experience, two subjects between one
and two years, two with four years, two with ﬁve years,
and one subject who reported eight years experience.
All applicants with over one year of experience claimed
hacking was—at some point in time—part of their
full-time job. The buckets are not uniform, but rather
partition the reported skills in a way divisible into two
teams. We assigned each team an investigator to serve as
the leader, each with experience leading hacking teams.
the ﬁrst day providing introductions,
presenting a class on the popular open-source fuzzing
tool AFL [42], assessing the skills of our subjects, and
describing our work ﬂow and submission standards.
We spent
Period of instruction The class combined a lecture
with exercises ranging from how to compile using
afl-gcc to fuzzing bzip2 using afl-qemu. We also
provided a 30 minute lecture-only class on Docker [3].
Skill assessment Our self assessment was subjective,
so we devised a more objective measurement of subject
skill in the form of a series of technical skill assessment
tests. We administered these tests three times: once
immediately after the initial training course, once at the
half-way mark (before the teams exchanged strategies),
and once at the end of the experiment. One aim was
to measure the amount of skill our subjects developed
during the course of executing each strategy.
All three skill assessments followed the same form,
consisting each time of a new set of ﬁve binaries taken
1134    29th USENIX Security Symposium
USENIX Association
Selection
Orientation
Orientation Day
g
n
i
n
i
a
r
T
t
n
e
m
n
g
i
s
s
A
m
a
e
T
t
n
e
m
s
s
e
s
s
A
l
l
i
k
S
t
n
e
m
s
s
e
s
s
A
f
l
e
S
Applicants
Execution
Week One
Week Two
Team A
Depth
Team B
Breadth
t
n
e
m
s
s
e
s
s
A
l
l
i
k
S
Breadth
Depth
t
n
e
m
s
s
e
s
s
A
l
l
i
k
S
s
t
s
e
t
s
t
c
e
j
b
u
s
-
n
i
h
t
i
W
Between-subjects tests
Individual skill differential
Figure 3: An overview of our experiment, divided into the phases of selection, orientation, and execution; we provide