Technique: stats + eval 
	Example from .conf 2015 
▶ Joins are really computationally expensive, and limited
▶ Only if you have one *very* rare term search and one dense search, are subsearches a great approach. (Best if they’re 	not IP based, because IP searches are challenging)▶ Incorrect (10k results!): tag=malware action=allow | stats count as infections by host | join host [search index=proxy 	category=uncategorized | stats count as hits by host]
▶ Maybe Incorrect (400 seconds, 10k malware hits): [search tag=malware action=allowed | dedup dest | rename dest as 	src | table src]  (tag=proxy category=uncategorized)   | stats count(eval(tag="malware")) as NumMalwareHits 	count(eval(tag="proxy")) as NumProxyHits by src▶ Better (72 seconds): (tag=malware action=allowed) OR (tag=proxy category=uncategorized) | eval 
	mydest=if(tag="malware", dest, src) | stats count(eval(tag="malware")) as malware count(eval(tag="proxy")) as proxy by 	mydest | where malware>0 AND proxy>0
▶ Best (14 seconds): | tstats prestats=t summariesonly=t count(Malware_Attacks.src) as malwarehits fromdatamodel=Malware where Malware_Attacks.action=allowed groupby Malware_Attacks.src | tstats prestats=t append=t 	summariesonly=t count(web.src) as webhits from datamodel=Web where web.http_user_agent="shockwave flash" 	groupby web.src | rename web.src as src Malware_Attacks.src as src | stats count(Malware_Attacks.src) as malwarehits 	count(web.src) as webhits by src| where malwarehits > 0 AND webhits > 0tstats is awesome! Check out the tstats section of this presentation
Technique: Override Urgency/Severity/Risk
Background and Challenges
▶ "I think this is generally low severity, unless it happens for John Smith or his team 	of Research Scientists, in which case OH NO!"
▶ "We can usually ignore this assuming it’s not happening to our VIPs"▶ This comes up most frequently for ES customers, but can be applied to anyone 	else, depending on how you handle your upstream ticketing.
▶ Inside of ES, we have default severity, and default risk indicators. In addition, Urgency is automatically calculated based on the combination of severity and the priority of the asset or identity involved. But all of those can be overridden from within the search, to let you prioritize (or de-prioritize) any particular events or users.Technique: Override Urgency/Severity/Risk▶ If you include the fields urgency, severity, risk_object, risk_object_type, or 
risk_score, it will override whatever default values exist. 
tag=authentication
| filter_for_bad_stuff
| lookup user org_risk OUTPUT NumRisk
| eventstats avg(NumRisk) as AvgNumRisk
| eval risk_score=round(40*(NumRisk / AvgNumRisk), 0)| eval risk_object=if(user="administrator", "John Smith") | eval risk_object_type="user" 
| eval severity=if(risk_score>120, "critical", "medium")
Run your detection logic.
Use lookup, or anything else necessary to add 	context to the event.
	In this case, the lookup gives us a numeric risk 	coefficient (probably 1-5). Because we want to hardcode very little, we calculate the avg coefficient.Finally, we can hardcode the risk_score, risk_object, 	risk_object_type, and severity. You can technically 	hardcode the urgency as well, though usually that 	shouldn’t be necessary unless you haven’t 	configured your assets correctly.
Technique: Common Apps Background and Challenges
▶ "I want to conquer the world with Splunk Apps"
Technique: Common Apps
Splunk Security EssentialsSplunk Security Essentials
https://splunkbase.splunk.com/app/3435/
Identify bad guys in your environment: 
ü 50+ use cases common in UEBA products, 	all free on Splunk Enterprise 
ü Target external attackers and insider threat ü Scales from small to massive companies ü Save from the app, send results to ES/UBA
The most widely deployed UEBA product in the 
market is Splunk Enterprise, but no one knows it.Solve use cases you can today for free, then use Splunk UBA for advanced ML detection.
	Technique: Common Apps URL Toolbox
https://splunkbase.splunk.com/app/2734/
▶ DNS exfil detection - tricks of the trade
▶ parse URLs & complicated TLDs (Top Level Domain)
▶ calculate Shannon Entropy
▶ List of provided lookups
• ut_parse_simple(url)
• ut_parse(url, list) or ut_parse_extended(url, list) • ut_shannon(word)• ut_countset(word, set)
• ut_suites(word, sets)
• ut_meaning(word)
• ut_bayesian(word)
• ut_levenshtein(word1, word2)
Technique: Common Apps 
Common URL Toolbox Usages
▶ Checking Randomness via Entropy. Random characters in filenames or domain names can indicate suspicious behavior! It can also create false positives (CDNs, etc.)• sourcetype=win*security EventCode=4688 Users New_Process_Name=*\Users\* | stats  count by New_Process_Name,host | lookup ut_shannon_lookup word as New_Process_Name | rename ut_shannon as "Shannon Entropy Score" New_Process_Name as Process,host as Endpoint 
▶ Checking for similar strings can be useful particularly to find email phishing. 	Levenshtein gives us the distance between two strings.• sourcetype=proxy | stats count by domain | eval list="mozilla", mydomain="mycompany.com" | `ut_parse_extended(domain, list)` | lookup ut_levenshtein_lookup word1 as ut_domain word2 as mydomain | where ut_levenshtein 100, risk+10, risk) 
| eval risk = case(like(Groups, 
"%OU=Groups,OU=IT Security,%"), risk + 10, risk) 
| eval risk = case(like(title, "VP %"), risk+10, like(title, "Chief %"), risk+100, 1=1, risk)
| fields risk sAMAccountName | outputlookup RiskPerUser
Start by initializing Risk for all your usersThen apply your business logic to figure out what risk 
potential should be applied to each person.
To consider ways to define risk, think of questions 
like "how would I feel if someone from a particular 
department had a dispute and left the company" and 
then "why?"
Larger organizations may have a more mature 
process here
Finally, put this risk score into a lookup
Technique: RiskTechnique: Risk 
Use Your New Risk Lookup
▶ Now that we have a risk lookup, we can apply it to any search
[… insert your Correlation Search …] 
| stats count by user 
| lookup RiskPerUser sAMAccountName as user | eval AggRisk = risk * count 
| eval DescriptiveRisk = case(AggRisk > 100, "very high", AggRisk>30, "medium", AggRisk>5, "low", 1=1, "very low")
Apply this generically to any correlation search with a 	user fieldSum up the number of events per user. (You can 	also modify this with severity, risk score, etc.)
Use lookup to add the risk score
	If there are multiple offenses, increase risk 	accordingly. Note that you may want to be careful with actual multiplication as it can create too much noise. See Time Series * First Time Seen Detection
It’s often useful to generalize risk as "low" "medium" "high" as it can be more consumableTechnique: Risk
Add this into alert_actions.conf
▶ Suppose you have this search down exactly how you want it, and now you want 	to apply it to all your searches, you can easily do this via a macro.
▶ Then your search becomes: 
[… insert your Correlation Search …] 
| `calculate_risk(user)`
▶ If you are using ES, you can even build this into the ES Risk Framework by 	editing the [risk] stanza of:$SPLUNK_HOME/etc/apps/SA-ThreatIntelligence/default/alert_actions.conf▶ ES Users should also see "Technique: Override Urgency/Severity/Risk" in this 	doc
Technique: Subsearches
Background and Challenges
▶ "I want to run subsearches that return more than 10k results!"
▶ "I want to run subsearches that last longer than 60 seconds!"▶ "Boy do I like to build my mission-critical detections using this subsearch that 	returns all of our proxy logs! It even runs way faster as a subsearch!"
• Hint: that one is a bad one to say!
▶ Subsearches are very powerful! They can help you build out all kinds of great filters! I assume anyone getting this far probably already knows about subsearches.▶ Unfortunately, some don’t know that subsearches automatically finalize after 60 seconds (so as far as it gets in 60 seconds is as far as it gets) and can only return a maximum of 10k events. There’s solutions (ish) though!
Technique: Subsearches
Returning more than 10k results
▶ If you have more than 10k results (say you have 15k domains you want to search for) you can use the below. Just keep in mind that there are upper limits -eventually the main search will slow to the point of being unusable if you get to 
30k, 40k, fields. 
▶ The secret: if the only value you return from the subsearch is the field "search" 	then it will be interpreted literally. 
index=win*security 
[ | inputlookup inscope_ad_users.csv
| stats values(sAMAccountName) as search
Start with our base dataset, in need of a filter We now have a list of usersWe now have a GIANT single multi-value field
| ] | | eval search= 
	"(user=" . mvjoin(search, " OR user=") . ")" | mvjoin now gives us a GIANT single-value field |
|---|---|---|
| ] || eval search=  	"(user=" . mvjoin(search, " OR user=") . ")" |And now we’re back in our search, just with a GIANT  |