neutral and disgust. Each expression is viewed from 5 different
angles. The initial training data consists of random 224 ×
10
Fig. 6: Architecture Choice for stealing Microsoft Trafﬁc
Recognition API at various budgets (A = 0.43k, B =
1.29k, C = 2.15k)
Fig. 8: Architecture Choice for stealing Face++ Emotion
Recognition API (A = 0.68k, B = 1.36k, C = 2k)
Fig. 7: Architecture Choice for stealing Microsoft Flower
Recognition API at various budgets (A = 0.51k, B =
1.53k, C = 2.55k)
224 pixel patches cropped from these images and it is further
augmented by rotating 90 degree or transforming to gray scale
with 50% probability of each image. The experimental results
of our attack are shown in Table III. We can see our substitute
model achieves 65.33% (90.61×) accuracy with 1.36k queries
and 70.76% (98.14×) accuracy with 2k queries by using FF
adversarial examples, which approaching the 71.17% (98.74×)
accuracy achieved by the substitute model
trained on the
CW adversarial examples. The substitute model trained by
adversarial examples always achieves better performance than
the model trained by the random samples.
4) Case Study 4: Clarifai Safe for Work (NSFW) API:
The victim model pre-trained on Clarifai Not Safe For Work
(NSFW) API can recognize whether images include inappro-
priate contents on the Internet. In general, it is treated as
Not Safe For Work if the NSFW probability is greater than
0.85. Similar to previous experiments, we apply the proposed
attack algorithm to the black-box Clarifai NSFW API. The
training dataset used to train our surrogate model (ResNet50)
is randomly collected from Github opensource platform, which
contained 1.5k pictures (half of NSFW and half of SFW).
We then evaluated the accuracy of victim model by using
Fig. 9: Architecture Choice for stealing NSFW API (A =
0.50k, B = 1.00k, C = 1.05k)
1k random images which are different with training data (1/2
for SFW and 1/2 for NSFW). The accuracy of our victim
model is 92.10%. Experimental results show that our substitute
model achieves 87.10% (94.57×) accuracy with 1k queries
and 91.60% (99.46×) accuracy with 1.5k queries by using
FF examples which is better than using random examples,
i.e., 76.10% (82.63×) accuracy with 1k queries and 76.10%
(82.63×). In all case,
trained on FF
method achieves the best performance on the test set
in
comparison to other adversarial examples generation methods
such as PGD, CW and FA.
the substitute model
C. Synthetic dataset and Transfer Architecture Selection
In the previous sections, we demonstrate that our attack
framework can effectively replicate the functionality of an
victim model inside the API with similar performance. This
is achieved while concurrently applying the ﬁxed substitute
model architecture and dataset generation algorithm. In this
section, we further evaluate how the attack effectiveness varies
with different synthetic datasets Ds and transfer architectures
fs. In these experiments, we consider ﬁve strategies: RM, PGD,
CW, FA and FF. In our attack scheme, these strategies are
11
Budget ABudget BBudget C0.00.20.40.60.81.0AccuracyAlexNetCWFA.FFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19_DeepIDCWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyResNet50CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyAlexNetCWFA.FFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19_DeepIDCWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyResNet50CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyAlexNetCWFA.FFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGGFACECWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyResNet50CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyAlexNetCWFA.FFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19CWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyVGG19_DeepIDCWFAFFPGDRMBudget ABudget BBudget C0.00.20.40.60.81.0AccuracyResNet50CWFAFFPGDRMapplied to generate the synthetic data set Ds for re-training
the substitute model fs chosen from the ﬁve popular network
architectures, including AlexNet, VGG19, VGG19 DeepID,
VGGFace and ResNet50.
models with high accuracy, few queries and low costs simul-
taneously, while previous works fail in at least one or two
aspects. More detailed analysis of experimental results is given
below.
Figures 6, 7, 8 and 9 summarize the inﬂuence of different
datasets and/or architecture selections on the attack effective-
ness. As shown in Figure 6, we ﬁnd that the performance of
the substitute model re-trained on the adversarial examples
is usually better than the random exanples accross different
network architectures when we increase the number of training
examples from Budget A (0.43k) to Budget B (2.15k). The
same trend appears in Figure 7, Figure 8 and 9. Our substitute
models signiﬁcantly recover the original performance of black-
i.e., 97.19× for
box fv using the FF synthetic datasets,
the ﬂower recognition at C = 2.55k, 99.46× for NSFW
at C = 1.50k and 98.14× for the emotion recognition at
C = 2.00k.
We also analyze the performance of substitute models
while using different pre-trained models as our transfer ar-
chitectures. From Figures 6, 7, 8 and 9, we observe that
performance of our substitute model can be inﬂuenced by
both the model complexity and task relevance. Therefore, in
order to extract a copy of the victim model, an adversary
can focus on the following aspects: (i) Choosing a more
complex/relevant network and the transfer architecture. In both
cases, AlexNet networks achieve the lowest accuracies after
stealing a victim model. A signiﬁcantly more complex model
VGGNet (VGG19, VGG19 DeepID and VGGFace)/ResNet50
is beneﬁcial while stealing a victim model. Further, as seen
in Figure 8, VGGFace, which is relevant to face recognition
tasks, achieves the best accuracy across all choices of substitute
model architectures while targeting the face emotion recogni-
tion API. This further indicates that, if the attacker does not
know the exact architecture of the victim model, using a more
complex and task relevant model as the transfer architecture
is almost always beneﬁcial for the adversary. (ii) Sampling
images relevant to the classiﬁcation problem (relevant queries).
This is because irrelevant queries generally lead to noisy
labels and hence impose additional difﬁculty for re-training
the substitute model.
D. Comparison to Existing Attacks
As shown in Figure 10, we compare with the existing state-
of-the-art attack methods proposed by previous works, includ-
ing F. Tram`er attack [6], Correia-Silva attack [14] and Papernot
attack [21]. In our implementations, we keep the architectures
of the substitute models ﬁxed and evaluate how the attack
effectiveness varies with different synthetic dataset generation
methods (e.g., Tramer, Papernot, and Correia-Silvia). The same
trend appears while we use different transfer architectures to
copy the black-box victim model. In our implementations,
we launch these model stealing attacks on commercialized
MLaaS platforms in the real world, including those hosted
by Microsoft, Face++ and Clarifai. The substitute model
architectures are chosen from ﬁve popular pre-trained models,
including AlexNet, VGG19, VGG19 DeepID, VGGFace and
ResNet50. We use the synthetic dataset generated by our FF
algorithm to re-train these models in order to replicate the
functionality of victim API. Experimental results demonstrate
that our attack framework can steal large-scale deep learning
12
•
• When compared, our attack is more effective for
extracting large scale DNN model with few queries
than F. Tram`er attack [6]. From Figure 10(a) We
can see that, our substitute model, which uses the
VGG19 DeepID as the transfer architecture, is trained
on adversarial examples generated by our FF al-
gorithm and achieves 74.25% accuracy with 2.15k
queries, which is better than F. Tram`er attack’s results
(their substitute model achieves 15.97% accuracy with
2.15k queries). We ﬁnd signiﬁcant query efﬁciency
improvements, e.g., while Tram`er reaches 25.17% test
accuracy at B = 5.00k, our attack achieves this 3.9×
quicker at B = 1.29k. Similar results are shown in
Figure 10(b), Figure 10(c) and Figure 10(d). This is
because Tram`er attack uses line-search to ﬁnd those
samples which are overly similar (i.e., the same class),
resulting in poor training set and eventually degrading
the performance of the substitute model.
Different from the Correia-Silva attack [14], we vary
the architecture of the substitute model and re-train the
model on the synthetic dataset which is generated by
adversarial examples labeled by querying the victim
model. Take the Microsoft Flower Recognition as
an example (Figure 10(b)), with 1.53k queries, our
attack outperforms the Correia-Silva attack in terms
of prediction accuracy on the same test set (up to
27.84 percentage points). Experimental results also
demonstrate that our substitute model (ResNet50)
achieves higher accuracy as the number of queries
increases from 0.51k to 3.00k, which is better than the
performance of stolen model by Correia-Silva attack
(15.10% ∼ 78.92%). Since the model architecture is
not complicated, we can conclude that the success rate
of stealing black-box victim model depends not only
on the network architecture of the substitute model,
but also on the efﬁcacy of the dataset augmentation
method.
In order to boost the performance of the substitute
model, a Jacobian-based dataset augmentation method
is explored in the Papernot attack [21]. We reproduce
the exact setting of this attack reported in [21] and
show the experimental results in Figure 10. These
results show that adversarial examples help us improve
the query effectiveness of examples augmented by
Jacobian-based method. From Figure 10(a), we can
see most of our substitute models trained by adversar-
ial examples achieves higher performance (For exam-
ple, the VGG19 DeepID achieves 62.75% accuracy
with 1.29k queries and 74.25% accuracy with 2.15k
queries) than the model trained on the Jacobian-based
augmentation dataset. For the Flower Recognition, our
attack achieves the same accuracy as Jacobian-based
method with queries, e.g., the Jacobian-based method
reaches 81.76% test accuracy at B = 2.55k, our
attack achieves this 1.3 × quicker at B = 2.00k.
While targeting the NSFW (Figure 10(c)) and the
•
Fig. 10: Comparison of the performance over victim models between our method and previous works.
Emotion Recognition (Figure 10(d)), our substitute
models always achieve the best accuracy on the test
set with different sizes of queries. The main reason of
this is that, compared to Jacobian-based method, our
FF helps extract more decision information from the
victim model and hence improve the query effective-
ness.
E. More Commercial APIs
In addition to the commercial APIs we test our attack and
show the comparison results in the previous sections, we fur-
ther extend out attack to extra two commercial platforms: IBM
Watson Visual Recognition [17], Google AutoML Version
[18]. Speciﬁcally, the target model pre-trained on IBM Watson
visual recognition is for face recognition. The dataset used to
train the model is the PubFig83 dataset [59] containing 12502
images of 83 different individuals and a relating test dataset of
830 images (10 images per class). Experimental results show
that our substitute model (VGG19 DeepID) achieves 78.43%
accuracy with 2075 queries and 83.73% accuracy with 3320
queries, approaching the 84.94% accuracy achieved by the
victim model on IBM Watson Visual Recognition API.
Similar to the Microsoft ﬂower recognition, we use the
same dataset to train and test the victim model on Google Au-
toML Version. The experimental results demonstrate that, for a
synthetic dataset containing 2550 images, our local substitute
model (ResNet50) achieves 60.10% accuracy with random
samples, and 88.14% accuracy with FeatureFool examples,
which is similar to the 89.22% accuracy achieved by the vicm
model trained on the Google AutoML API.
VI. DISCUSSIONS
A. Potential Defenses
We have shown in Section V that an adversary can success-
fully extract the victim models from MLaaS cloud platforms.
As our ﬁndings undermine MLaaS platforms’ privacy and
integrity, defense mechanisms should be developed and applied
to protect cloud-based MLaaS platforms from model stealing
attacks. In this section, we evaluate one latest defense named
PRADA, and further propose a novel defense mechanism that
can effectively and adaptively defend against the malicious
queries to MLaaS platforms. We discuss the details of these
countermeasures.
1) Evasion of PRADA Detection: Juuti et. al [20] propose
a new defense method, known as PRADA, to detect model
extraction attacks. It analyzes the distribution of queries to
Model (δ value)
PGD
CW
FA
M=0.8D M=0.5D M=0.1D
Queries made until detection
FF
Trafﬁc (δ=0.92)
Trafﬁc (δ=0.97)
missed
110
Flower (δ=0.87)
Flower (δ=0.90)
Flower (δ=0.94)
110
110
110
missed
110
missed
340
340
missed
110
220
220
220
missed
110
missed
350
350
150
110
290
120
120
130
110
140
130
130
TABLE IV: Adversarial queries made until detection. Here, the
parameter D is the L2 norm distance measuring the similarity
between the legitimate example xs and its adversarial example
x(cid:48)
s in the feature space.
victim models and rely on how these queries relate to each
other for detecting model extraction attacks. In the experi-
mental stage, we reproduce the exact setting of this defense
reported in [20]. We evaluate the PRADA defense on all of the
model theft attacks using different synthetic dataset generation
strategies (e.g., PGD, CW, FA and FF) described in Section V
and summarized in Table IV. We conduct experiments with ﬁve
different victims including trafﬁc, ﬂower, NSFW and emotion
recognition models, and ﬁnd the similar conclusion. Taking
the trafﬁc and ﬂower recognition models as examples, we
randomly pick natural samples from a given data set and
calculate the detection threshold value δ resulting in no false
positives for the substitute models (δ = 0.91 for the trafﬁc
recognition model and δ = 0.80 for the ﬂower recognition
model). From Table IV we can see that our attacks can easily
evade their defense by carefully selecting the parameters M
from 0.1D to 0.8D. This is because, by selecting the parameter
M in Equation (10), we can simulate a normal distribution of
query samples without signiﬁcantly degrading the substitute
models performance. By increasing δ (e.g, from δ = 0.87
to δ = 0.94 for ﬂower adversarial examples), the PRADA
produces a high false positives (up to 93%) while detecting our
F F attack. Moreover, results demonstrate that other types of
adversarial attacks such as PGD, CW and FA can also bypass
the PRADA defense if δ is small.
2) The Proposed Defense: In order to reduce the impact of
information leakage during the querying process, as demon-
strated by our attack, we design and evaluate potential defense
mechanisms. Unlike previous works, our goal is not to analyze
the distribution of consecutive queries but rather focus our