use the SPECint 2006 suite. For I/O-bound applications,
we use the iozone (disk read and write), compilebench
(project compilation), and unmodiﬁed Apache web server
performance. For iozone, we perform the disk read and
write benchmarks with 4K block size and 2GB ﬁle size.
We use the compile benchmark from compilebench. We run
Apache on top of XMHF, and use the Apache Benchmark
(ab) included in the Apache distribution to perform 200,000
transactions with 20 concurrent connections.
Our results are presented in Figure 7. Most of the SPEC
benchmarks show less than 3% performance overhead. How-
ever, there are four benchmarks with over 10%, and two
more with 20% and 55% overhead. For I/O application
benchmarks, read access to ﬁles and network access incurs
the highest overhead (40% and 25% respectively). The
rest of the benchmarks show less than 10% overhead. We
attribute the high compute and I/O benchmark latency to
benchmark operations that stress the paging logic involving
the HPT and I/O DMA logic involving the DMA access con-
trol tables. These overheads are comparable to other general-
purpose high-performance hypervisors using hardware vir-
tualization including HPT and DMA protections (§VI-C).
We expect these overheads to diminish with newer HPT
and DMA protection hardware. In general, for both compute
and I/O benchmarks, XMHF with 2MB HPT conﬁguration
performs better than XMHF with 4KB HPT conﬁguration.
2) Performance of hypapps: A hypapp built on top of
XMHF incurs two basic runtime overheads: (a) when the
hypapp is invoked via intercepted guest events (including
a hypercall), and (b) when the hypapp quiesces cores in a
multi-core system in order to perform HPT updates.
When the hypapp is invoked, the CPU switches from guest
to host mode, saving the current guest environment state and
loading the host environment state. After the hypapp ﬁnishes
its task, the CPU switches back to guest mode by performing
the reverse environment saving and loading. Thus, there is
a performance impact from cache and TLB activity. We
measure this overhead by invoking a simple hypercall within
the guest and measuring the round-trip time.
As described in §IV-B2, XMHF employs CPU-quiescing
on SMP platforms to ensure intercept serialization. As XMHF
uses the NMI for this purpose (§IV-B2), it results in a
performance overhead. We measure this overhead by using
a simple hypapp that quiesces all other cores, performs a
NOP, and then releases them, all in response to a single guest
hypercall event. We use a guest application that invokes the
hypercall and measure the round-trip time.
The hypapp overheads on XMHF for both 4K and 2MB
HPT conﬁgurations, for intercepted guest events and quiesc-
ing are on average 10 and 13.9 micro-seconds respectively.
these hypapp overheads occur every time
We note that
a guest event
is intercepted. Depending on the hypapp
functionality this may happen less frequently (a typical
and desirable approach today, as evidenced by the hypapps
discussed in §VI-A) or more frequently. In either case, the
overheads are chieﬂy due to the hardware (intercept world-
switch and NMI signaling). We expect this to diminish as
hardware matures. As these overheads reduce, we could
conceivably have hypapps interact with the guest in the same
spirit as regular applications interact with OS kernels today.
440
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
Figure 7. XMHF Application Benchmarks; xmhf-4K = 4K HPT mapping; xmhf-2M = 2MB HPT mapping.
Prog.
P
2
3
4
1
P M
P M
P M
P M
P L
1
P L
2
P L
3
P L
4
P L
5
P L
6
OP
1654
1667
1668
1669
1653
1679
1654
1652
1634
1652
1652
SP
1452
1465
1466
1467
1451
1477
1452
1450
1441
1450
1450
VCC
111
116
116
116
117
111
111
111
111
111
111
Vars
437K
438K
438K
438K
463K
476K
437K
437K
437K
437K
437K
CLS
1560K
1561K
1561K
1561K
1668K
1728K
1559K
1559K
1560K
1560K
1560K
TD
25
27
25
25
25
28
25
25
25
24
25
T
76
81
79
79
80
82
79
79
79
78
79
M
1.9
1.9
1.9
1.9
1.9
1.9
1.9
1.9
1.9
1.9
1.9
Figure 8. XMHF Performance Comparison with Xen: XMHF and Xen have
similar performance for compute-bound and disk I/O-bound applications;
XMHF performance is closer to native speed than Xen for network-I/O.
C. Performance Comparison
We now compare XMHF’s performance with the popular
Xen (v 4.1.2) hypervisor. We use three hardware virtual
machine (HVM) conﬁgurations for domU, that are identical
in memory and CPU conﬁguration to the native system:
HVM domU (xen-domU-hvm), HVM domU with paravirtu-
alized drivers (xen-domU-pvhvm) and HVM domU with pci-
passthrough (xen-domU-passthru). We also use dom0 (xen-
dom0) as a candidate for performance evaluation. We use the
compute and I/O-bound application benchmarks as described
previously (see §VI-B1). Figure 8 shows our performance
comparison results. For compute-bound applications XMHF
and Xen have similar overheads (around 10% on average)
with the 2MB XMHF HPT conﬁguration performing slightly
better. For disk I/O benchmarks, XMHF, xen-dom0 and xen-
domU-pvhvm have the lowest overheads (ranging from 3-
20%). Both XMHF and Xen have higher overheads on the
disk read benchmark when compared to other disk bench-
marks. For network-I/O benchmark, XMHF has the lowest
441
Figure 9.
XMHF veriﬁcation results with CBMC. OP = number of
assignments before slicing; SP = number of assignments after slicing; VCC
= number of VCCs after simpliﬁcation; Vars = number of variables in SAT
formula; CLS = number of clauses in SAT formula; TD = time (sec) taken
by SAT solver; T = total time (sec); M = maximum memory (GB)
overhead (20-30%). xen-dom0 and xen-domU-passthru incur
a 45% and 60% overhead respectively, while xen-domU-hvm
and xen-domU-pvhvm have more than 85% overhead.
D. Veriﬁcation Results
We now describe our experiments in verifying DRIVE
properties and invariants by model-checking the XMHF
implementation. These veriﬁcation problems are reduced
to proving the validity of assertions in a sequential C
program P (§V). We discuss our experience in using several
publicly available software model checkers to verify P . All
experiments were performed on a 2 GHz machine with a
time limit of 1800 seconds and a memory limit of 10GB.
1) Experience with CBMC: CBMC [22] is a bounded
model checker for verifying ANSI C programs. It supports
advanced C features like overﬂow, pointers, and function
pointers, and can ﬁnd bugs such as pointer dereferencing
to unallocated memory and array out-of-bounds accesses.
It is therefore uniquely suited to verify system software
such as XMHF. CBMC is only able to verify programs
without unbounded loops; P (XMHF core) complies with this
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
requirement. During veriﬁcation of P , CBMC automatically
sliced away unreachable code and unrolled the remaining
(bounded) loops.
The version of CBMC available publicly when we began
our experiments was 4.0. This version did not handle two C
features that are used in P – function pointers and typecasts
from byte arrays to structs. We believe that these features
are prevalent in system software in general. We contacted
CBMC developers about these issues, and they incorporated
ﬁxes in the next public release CBMC 4.1. CBMC 4.1 veriﬁes
P successfully.
1 – P L
1 – P M
We also seeded errors in P to create ten additional buggy
programs. Four of the buggy programs (P M
4 ) contain
memory errors that dereference unallocated memory. The
remaining six buggy programs (P L
6 ) have logical
errors that cause assertion violations. In each case, CBMC
ﬁnds the errors successfully. Table 9 summarizes the overall
results for CBMC 4.1. Note that the SAT instances produced
are of non-trivial size, but are solved by the back-end SAT
solver used by CBMC in about 25 seconds each. Also, about
75% of the overall time is required to produce the SAT
instance. This includes parsing, transforming the program to
an internal representation (called a GOTO program), slicing,
simpliﬁcation, and generating the SAT formula.
2) Experience with Other Model Checkers: We also tried
to verify P and the ten buggy programs with three other
publicly available software model checkers that target C
code – BLAST [38], SATABS [39], and WOLVERINE [40].
All these model checkers are able to verify programs with
loops and use an approach called Counterexample Guided
Abstraction Reﬁnement (CEGAR) [41], [42] combined with
predicate abstraction [43]. BLAST 2.5 could not parse any
of the target programs. In contrast, SATABS 3.1 timed out
in all cases after several iterations of the CEGAR loop. On
the other hand, WOLVERINE 0.5c ran out of memory in all
cases during the ﬁrst iteration of the CEGAR loop.
VII. RELATED WORK
BitVisor [44] and NoHype [45], [46] are hypervisors that
eliminate runtime complexity by running guests with pre-
allocated memory, and direct access to devices. XMHF also
advocates the rich single-guest execution model (§ IV-A).
However, XMHF is designed to provide a common hyper-
visor core functionality given a particular CPU architecture
while at
the same time supporting extensions that can
provide custom hypervisor-based solutions (“hypapps”) for
speciﬁc functional and security properties. The extensibility
of XMHF allows hypapps to be built around it while preserv-
ing memory integrity. Xen [47], KVM [29], VMware [48],
NOVA [49], Qubes [50] and L4 are general purpose (open-
source) hypervisors and micro-kernels which have been used
for hypervisor based research [11]–[16], [28]. However,
unlike XMHF, they do not present clear extensible interfaces
for hypapp developers or preserve memory integrity. Further,
442
complexity arising from device multiplexing and increased
TCB make them prone to security vulnerabilities [51]–[55].
OsKit [56] provides a framework for modular OS devel-
opment. XMHF provides a similar modular and extensible
infrastructure for creating hypapps.
A sound architecture [57], [58] is known to be essential
for the development of high quality software. Moreover,
there has been a body of work in using architectural
constraints to not only to drive the analysis of important