### Performance Evaluation

We utilize the SPECint 2006 suite for compute-bound applications. For I/O-bound applications, we employ the following benchmarks:
- **iozone**: Disk read and write tests with a 4K block size and 2GB file size.
- **compilebench**: Project compilation benchmark.
- **Apache web server**: Unmodified performance evaluation.

For the Apache benchmark, we run the Apache web server on top of XMHF and use the Apache Benchmark (ab) tool included in the Apache distribution to perform 200,000 transactions with 20 concurrent connections.

### Results and Analysis

The results are presented in Figure 7. Most SPEC benchmarks show less than a 3% performance overhead. However, four benchmarks exhibit over 10% overhead, with two more showing 20% and 55% overhead, respectively. For I/O application benchmarks, file read access and network access incur the highest overheads at 40% and 25%, respectively. The remaining benchmarks show less than 10% overhead.

We attribute the high compute and I/O benchmark latencies to operations that stress the paging logic involving the HPT and I/O DMA logic involving the DMA access control tables. These overheads are comparable to those observed in other general-purpose high-performance hypervisors using hardware virtualization, including HPT and DMA protections (§VI-C). We expect these overheads to diminish with newer HPT and DMA protection hardware. Generally, both compute and I/O benchmarks show that XMHF with a 2MB HPT configuration performs better than XMHF with a 4KB HPT configuration.

### Hypapp Performance

A hypapp built on top of XMHF incurs two primary runtime overheads:
1. **Intercepted Guest Events**: When the hypapp is invoked via intercepted guest events (including hypercalls), the CPU switches from guest to host mode, saving the current guest environment state and loading the host environment state. After the hypapp completes its task, the CPU switches back to guest mode by performing the reverse operation. This process impacts cache and TLB activity.
2. **Core Quiescing**: On multi-core systems, the hypapp may need to quiesce cores to perform HPT updates. XMHF uses the NMI for this purpose, resulting in additional performance overhead.

We measure these overheads by invoking a simple hypercall within the guest and measuring the round-trip time. The average overheads for intercepted guest events and core quiescing are 10 and 13.9 microseconds, respectively, for both 4K and 2MB HPT configurations. These overheads occur every time a guest event is intercepted. Depending on the hypapp's functionality, this may happen less or more frequently. The overheads are primarily due to hardware (intercept world-switch and NMI signaling). As hardware matures, we expect these overheads to diminish, potentially allowing hypapps to interact with guests in a manner similar to how regular applications interact with OS kernels today.

### Performance Comparison with Xen

We compare XMHF’s performance with the popular Xen (v4.1.2) hypervisor. We use three hardware virtual machine (HVM) configurations for domU, identical in memory and CPU configuration to the native system:
- **HVM domU (xen-domU-hvm)**
- **HVM domU with paravirtualized drivers (xen-domU-pvhvm)**
- **HVM domU with pci-passthrough (xen-domU-passthru)**

We also include dom0 (xen-dom0) in our performance evaluation. Figure 8 shows the results. For compute-bound applications, XMHF and Xen have similar overheads (around 10% on average), with the 2MB XMHF HPT configuration performing slightly better. For disk I/O benchmarks, XMHF, xen-dom0, and xen-domU-pvhvm have the lowest overheads (ranging from 3-20%). Both XMHF and Xen have higher overheads on the disk read benchmark compared to other disk benchmarks. For network-I/O benchmarks, XMHF has the lowest overhead (20-30%), while xen-dom0 and xen-domU-passthru incur 45% and 60% overheads, respectively. xen-domU-hvm and xen-domU-pvhvm have more than 85% overhead.

### Verification Results

We describe our experiments in verifying DRIVE properties and invariants by model-checking the XMHF implementation. These verification problems are reduced to proving the validity of assertions in a sequential C program P (§V). We discuss our experience using several publicly available software model checkers to verify P. All experiments were performed on a 2 GHz machine with a time limit of 1800 seconds and a memory limit of 10GB.

#### Experience with CBMC

CBMC [22] is a bounded model checker for verifying ANSI C programs. It supports advanced C features like overflow, pointers, and function pointers, making it suitable for verifying system software such as XMHF. CBMC can only verify programs without unbounded loops; P (XMHF core) complies with this requirement. During verification, CBMC automatically slices away unreachable code and unrolls the remaining (bounded) loops.

The version of CBMC available when we began our experiments was 4.0, which did not handle function pointers and typecasts from byte arrays to structs. We contacted CBMC developers, and they incorporated fixes in the next public release, CBMC 4.1, which successfully verifies P.

We also seeded errors in P to create ten additional buggy programs. Four of these (P M_4) contain memory errors that dereference unallocated memory, and the remaining six (P L_6) have logical errors that cause assertion violations. CBMC finds all these errors successfully. Table 9 summarizes the overall results for CBMC 4.1. The SAT instances produced are non-trivial but are solved by the back-end SAT solver used by CBMC in about 25 seconds each. About 75% of the overall time is required to produce the SAT instance, including parsing, transforming the program to an internal representation (GOTO program), slicing, simplification, and generating the SAT formula.

#### Experience with Other Model Checkers

We also attempted to verify P and the ten buggy programs with three other publicly available software model checkers targeting C code: BLAST [38], SATABS [39], and WOLVERINE [40]. These model checkers use Counterexample Guided Abstraction Refinement (CEGAR) [41], [42] combined with predicate abstraction [43]. BLAST 2.5 could not parse any of the target programs. SATABS 3.1 timed out in all cases after several iterations of the CEGAR loop, and WOLVERINE 0.5c ran out of memory in all cases during the first iteration of the CEGAR loop.

### Related Work

**BitVisor [44]** and **NoHype [45], [46]** are hypervisors that eliminate runtime complexity by running guests with pre-allocated memory and direct device access. XMHF also advocates a rich single-guest execution model (§IV-A) but is designed to provide a common hypervisor core functionality for a specific CPU architecture while supporting extensions for custom hypervisor-based solutions ("hypapps"). XMHF's extensibility allows hypapps to be built around it while preserving memory integrity.

Other general-purpose (open-source) hypervisors and micro-kernels, such as **Xen [47]**, **KVM [29]**, **VMware [48]**, **NOVA [49]**, **Qubes [50]**, and **L4**, have been used for hypervisor-based research [11]–[16], [28]. However, unlike XMHF, they do not present clear extensible interfaces for hypapp developers or preserve memory integrity. The complexity arising from device multiplexing and increased TCB makes them prone to security vulnerabilities [51]–[55].

**OsKit [56]** provides a framework for modular OS development, similar to XMHF, which offers a modular and extensible infrastructure for creating hypapps.

A sound architecture [57], [58] is essential for developing high-quality software. There has been significant work in using architectural constraints to drive the analysis of important properties.