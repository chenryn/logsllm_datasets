Proxygen restarts and relies on client re-connects. Given the lack of
GOAWAY-like support in the protocol, the edge only has the options
of either waiting for the clients to organically close the connection
or forcefully close it and rely on client-side reconnects.
A key property for MQTT connections is that the intermediary
LBs only relay packets between pub/sub clients and their respective
back-ends (pub/sub brokers) and as long as the two are connected,
it does not matter which Proxygen relayed the packets. MQTT con-
nections are tunneled through Edge to Origin to MQTT back-ends
over HTTP/2 (§ 2). Any restarts at Origin are transparent to the
end-users as their connections with the Edge remains undisturbed. If
we can reconnect Edge Proxygen and MQTT servers through another
Origin Proxygen, while the instance in question is restarting, end
users do not need to reconnect at all. This property makes Origin
Proxygen “stateless” w.r.t MQTT tunnels as the Origin only relays the
packets. Leveraging this statelessness, disruptions can be avoided
for these protocols that do not natively support it – a mechanism
called Downstream Connection Reuse (DCR).
Each end-user has a globally unique ID (user-id) and is used to
route the messages from Origin to the right MQTT back-end (having
the context of the end-user connection). Consistent hashing [7, 26]
is used to keep these mappings consistent at scale and, in case of
a restart, another Proxygen instance can take-over the relaying re-
sponsibility without any end-user involvement and disruption (at
back-end or user side).
Workflow: Figure 6 presents the details of the Downstream Con-
nection Reuse transactions. When an Origin Proxygen instance is
Figure (7) Partial Post Replay
restarting, it send a reconnect_solicitation message to downstream
Edge LB step A○ to signal the restart. Instead of dropping the connec-
tion, Edge LB sends out re_connect (contains user-id) to Origin, where
another healthy LB is used to relay the packets to corresponding
back-end (located through user-id) (steps B1○, B2○). MQTT back-end
looks for the end-user’s connection context and accepts re_connect
(if one exists) and sends back connect_ack (steps C1○, C2○). Otherwise,
re_connect is refused (by sending connect_refuse), Edge drops the
connection and the end-user will re-initiate the connection in the
normal way.
Caveats: Restarts at Origin is the ideal scenario for DCR as Edge
is the next downstream and can thus keep the restarts transparent
to users. For a restart at the Edge, the same workflow can be used
with end-users, especially mobile clients, to minimize disruptions
(by pro-actively re-connecting). Additionally, DCR is possible due to
the design choice of tunneling MQTT over HTTP/2, that has in-built
graceful shutdown (GOAWAYs).
4.3 Partial Post Replay
An App. Server restart result in disruption for HTTP requests. Due to
their brief draining periods (10-15s), long uploads (POST requests)
poseaspecificpainpointandareresponsibleforthemostdisruptions
during restarts.
A user’s POST request makes its way to the App. Server through
the Edge and Origin Proxygen. When the app. server restarts, it can
react in multiple ways: (i) The App. Server fails the request, responds
with a 500 code and the HTTP error code makes it way to user. The
end-user observes the disruption in form of “Internal Server Error”
and the request gets terminated (disrupted). (ii) The App. Server can
fail the request with 307 code (Temporary Redirect), leading to a
request re-try from scratch over high-RTT WAN (performance over-
head). (iii) The 307 redirect can be improved by buffering the POST
request data at the Origin L7LB. Instead of propagating the error (500
or307retry)totheuser,the OriginL7LBcanretrythebufferedrequest
to another healthy App. Server. The massive overhead of buffering
every POST request until completion makes this option impractical.
In light of deficiencies of alternate effective techniques, Partial
Post Replay [27] leverages the existence of a downstream Proxygen
and the request data at restarting app. server to hand-over the incom-
plete, in-process requests to another App. Server. A new HTTP code
(379) is introduced to indicate a restart to the downstream Proxygen.
Workflow: Figure 7 presents a high-level view of the workflow. A
user makes a POST request ( A○) which is forwarded to an App. Server
( B○). When the App. Server (AS in Figure 7) restarts, it responds to any
unprocessed requests with incomplete bodies by sending a 379 status
code and the partially received POST request data, back to down-
stream Origin Proxygen ( C○). For HTTP/1.1, the status message is set
to “Partial POST Replay”. The Proxygen instance does not send the
EdgeL7LBMQTTserverL7LB 1L7LB nL7LB restarts. All tunneledconn. are broken.OriginEdge does not drop theconn. and tunnels throughanother Origin L7LB.Broken conn.New Conn.Control msgs.AB1B2C1C1Code 200Code 379 (POST data return)EdgeL7LB  AS-1  AS-nBack-endsOriginL7LBAS-1 is restartingPOST fooPOST fooPOST fooPOST fooCode 200 (success)Code 200ABCDEZero Downtime Release
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
379 status code to the end-user and instead builds the original request
and replays it to another healthy App. Server D○. When the request
gets completed, the success code is returned back to the end-user E○.
Caveats: There is a bandwidth overhead associated with replay-
ingtherequestdata—highbandwidthconnections,existingbetween
Origin Proxygen and App. Server in DataCenter, are required to make
Partial Post Replay viable. Additionally, since end-user applications
are not expected to understand code 379, it should never be sent back
to end-user. In case when intermediary cannot replay request to
another server, the requests should be failed with standard 500 code.
4.4 Applicability Considerations
Next, we discuss the applicability of the three mechanisms to the
different tiers. As mentioned in § 2, the tiers differ in their function-
ality and operational capacity, and can also have specific operational
limitations, e.g., App. Server tier is restarted hundreds of times a
week and requires draining period in order of tens of seconds. Such
operational aspects decide the applicability of the three mechanism.
Whereas Socket Takeover is used at every Proxygen instance in the
end-to-end architecture, Socket Takeover is not used for the HHVM
server at the App. Server tier and Partial Post Replay is used there
to tackle any disruptions. Due to the very brief draining period for
HHVM servers, Socket Takeover by itself is inadequate to prevent
disruptions for large POST requests as these requests are not ex-
pected to be completed by the end of the draining phase for the
old instance and hence would lead to connection resets when the
old instance terminates. Therefore, a co-ordinated form of drain-
ing with the downstream is required to hand-over the request to
another healthy instance. Additionally, operational aspects of App.
Server tier makes Socket Takeover unfavorable as these machines
are too constrained along CPU and memory dimensions to support
two parallel instances (e.g., priming local cache for a new HHVM
instance is memory-heavy). Downstream Connection Reuse is used at
both Edge and Origin Proxygen for MQTT-based services due to their
long-lived connection nature. For Downstream Connection Reuse to
work at Edge, application code at the user-end needs to understand
the connection-reuse workflow transactions.
The three mechanisms differ with respect to the protocol or the
target layer in the networking stack. Hence, there’s no interdepen-
dencies and the mechanisms are used concurrently. Socket Takeover
and Downstream Connection Reuse are triggered at the same time, i.e.,
when a Proxygen restarts. Note that, if the next-selected machine to
relay the MQTT connections is also under-going a restart, it does not
have any impact on Downstream Connection Reuse, since the updated,
parallel instance is responsible for handling all the new connections.
For Partial Post Replay, it is possible that the next HHVM server is
also restarting and cannot handle the partially-posted requested,
since the corresponding instance is in draining mode and not ac-
cepting any new requests. In such a case, the downstream Proxygen
retries the request with a different HHVM server. At production, the
number of retries is set to 10 and is found enough to never result in
a failure due to unavailability of active HHVM server.
5 HANDS-ON EXPERIENCE
In this section we discuss our experiences and the challenges we
faced in developing and deploying the Zero Downtime Release.
5.1 Socket Takeover
As discussed in section 4.1, passing controls of a listening socket
between processes with SCM_RIGHTS is known technique. Our so-
lutions use it as a building block for zero-downtime release in a
large-scale production service. It is thus important to understand
limitations and operational downsides of using such feature and
have proper monitoring and mitigation steps in place.
• Downsides of sharing existing sockets: When the owner-
ship of a socket is transferred to another process its associated state
within the kernel remains unchanged since the File Descriptor (FD)
stillpointstothesameunderlyingdatastructureforthesocketwithin
Kerneleventhoughtheapplicationstatesinuser-spacehavechanged
with the new process taking over the ownership of the socket.
While this mechanism to pass control of a socket is not an issue
per se, an unchanged socket state in the Kernel even after restart of
the associated application process is not only unintuitive but can
also hinder in debugging of potential issues and prevent their swift
mitigations. It is a common practice to roll back the newly released
software to a last known version to mitigate ongoing issues. Albeit
rare in occurrence, if there is any issue in the Kernel involving socket
states,suchasonesstemmingfromerroneousstateswithintheirdata
structure, diagnosing or mitigating such issues on a large scale fleet
poses several challenges since a rollback of the latest deployment
does not resolve the issue. For example, we encountered a bug in the
UDP write path [22] after enabling the generic segmentation offload
(UDP GSO) Kernel feature [23]. On many of the updated servers,
the buffer in UDP sockets (sk_buff ) were not cleared properly after
failures within certain syscalls in write paths. This resulted in slow
accumulation of buffers over period of time, eventually leading to
a system wide failure to successfully write any further packet. A
simple restart of the application process owning the socket did not
mitigate this issue since the underlying socket structure persisted
across process restarts while using the Socket Takeover mechanism.
Another pitfall with the mechanism of passing the control of
sockets is that it introduces possibilities of leaking sockets and their
associated resources. Passing the ownership of sockets to a different
process using their FDs is essentially equivalent to a system call
dup(fd) wherein upon passing these FDs to the new process, the
Kernel internally increases their reference counts and keeps the
underlying sockets alive even after the termination of the applica-
tion process that owns them. It is thus essential that the receiving
process acts upon each of the received FDs, either by listening on
those sockets or by closing any unused ones. For example, when
multiple sockets bind to the same address using the socket option
SO_REUSEPORT, the Linux Kernel internally multiplexes incoming
packets to each of these sockets in a fairly uniform manner. However,
during a release if the newly spun process erroneously ignores any
of the received FDs for these sockets after Socket Takeover, without
either closing the FDs or listening on the received sockets for incom-
ing packets, it can lead to large increase in user facing errors such
as connection timeouts. This is because the orphaned sockets now
owned by the new process are still kept alive in the Kernel layer and
hence receive their share of incoming packets and new connections
- which only sit idle on their queues and never get processed.
Remediation: For swift mitigations of operational issues during
a release, such as the ones involving persistent erroneous socket
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Naseer et al.
states or socket leaks as mentioned earlier in this section, a mech-
anism to dynamically disable the Socket Takeover process presents
a safe and convenient approach. To allow such options, applications
employing Socket Takeover mechanism must incorporate such fall-
back alternatives on all of its phases, spanning from the initialization
phase (when sockets are created) to the shutdown phase (as the pro-
cess gracefully terminates all of its existing connections). Proxygen,
for example, allows transitioning to either modes by executing a
set of heuristics tuned to its needs on each of its operating phases.
During the initialization phase Proxygen resorts to allocating and
binding new sockets if it cannot receive FDs of listening sockets from
the existing process. Similarly, during its shutdown phase Proxygen
adopts different strategies depending on the runtime behavior - such
as signaling the L4LB to not send any new traffic by failing the health
probes if no other instance is running on the same host, to transition-
ing back to the running mode from the shutdown phase if the newly
spun process crashes during its initialization phase. Additionally,
being able to selectively disable this approach for a subset of sockets
makes ad-hoc debugging and development easier.
We recommend fine-grained monitoring of socket states, such as
connectionqueuelengths,failurecountsinsocketreads/writes,num-
berofpacketsprocessedanddroppedandcountsof FDs passedacross
process domains to detect problems of these nature in early phases.
While the occurrence of such issues is quite rare in our experience,
monitoring of such events not only notifies the concerned engineers
but can also trigger automated responses to mitigate the impact.
• Premature termination of existing process: During a re-
lease phase, there is a period of time during which multiple appli-
cation processes within a same host are serving requests. During
such window of time, premature exit of the old instance before the
new service instance is completely ready and healthy can result in a
severe outage. Typically, the newly spun instance goes through ini-
tialization and setup phase, such as loading of configuration, setting
up of its server pools and spinning of threads, and may also invoke
remote service calls. It is thus common to encounter issues due to
issues of its own such as deadlocks and memory leaks or due to issues
external to itself, such as failures on the remote service calls in its
startup path. It is thus imperative to keep the old service instance
and serve incoming requests as the new instance is being initialized.
Remediation: Our recommendation based on experience is to
implement a message exchange mechanism between the new and
old application server instances to explicitly confirm that the Socket
Takeover was successful, and acknowledge the readiness to serve.
The same mechanism that was used to pass FDs of sockets during
the Socket Takeover can be trivially extended to allow exchange of
such messages. Only upon receiving the explicit message should the
old instance initiate its shutdown phase with proper draining phase
to allow graceful termination of existing connections.
• Backward compatibility:Anychangesinthe Socket Takeover
mechanism must be backward compatible and support versioning.
If the system of acknowledgment from the new instance to the old
instance breaks, the service gets exposed to significant availability
risk. To illustrate this notion further, any change in the behavior
of the Socket Takeover version n needs to be compatible with the
version n+1, and thus, needs to handle all the possible transitions:
– n→n restart of an existing version in production
– n→n+1 deployment of the new implementation
Figure (8) Cluster capacity at {5,20}% batch restarts.
regression)
– n +1→ n revert of the deployment above (for example, due to a
– n+1→n+1 restart of the new version in production
For this reason we recommend the implementation of explicit ver-
sioning logic within the messaging mechanism to make it easier for
the developer to specify the intended behavior when introducing
new changes in the algorithm. A newly spun process can gracefully
terminate itself early if it fails to negotiate a common version with
an existing process. Only after successfully completing version ne-
gotiation, the existing process sends the FDs of listening sockets to
the new process.
• Availabilityrisksandhealth-checks:Theprocessofrelease
itself can increase the availability risks for a service going through
the release. For instance, degradation in the health of a service being
released even at a micro level, such as one at an individual host level,
can escalate to a system wide availability risks while updating a large
fraction of its servers. The overall release process, therefore, must
be tuned such that the health of the service being updated remains