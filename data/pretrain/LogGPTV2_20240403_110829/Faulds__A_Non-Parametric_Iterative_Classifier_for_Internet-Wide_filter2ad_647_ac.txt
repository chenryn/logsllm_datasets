, α t ) =
pt

i jτ γ
iτ γ pt
i jτ γ
(12)
(13)
f t +1
T
(τ ) =
1
m
m
j =1
P(Tj = τ |d(cid:3)
j , θt
d
, α t ).
(7)
and consider the next result.
(cid:5)
(cid:4)k
Next, each database signature with k original packets admits
k − 1 unique loss patterns γ , where k goes as high as kmax = 21
2
in the most recent effort in the field [41]. Estimating the probabil-
ity pi (γ ) for each possible option γ is likely to produce too many
unknown variables and lead to poor convergence of EM. Instead,
patterns of losing (cid:2) packets out of k are equally
suppose that all
likely and define the probability of this event to be qk ((cid:2)), where
k = 1, 2, . . . , kmax . The resulting reduction in the number of un-
kmax +1 = 4M to
known variables is significant – from roughly 2
just kmax (kmax −1)/2 = 210. Despite its simplicity, the framework
of using qk ((cid:2)) allows more general scenarios than the traditional
iid Bernoulli model used in previous literature [41], [42].
(cid:2)
Theorem 4.1. Under network distortion, estimators (3), (7), (8),
and (10) can be written as
1
m
α t +1
i
=

β t
i jτ γ ,
jτ γ

f t +1
T
(τ ) =
qt
k
((cid:2)) =
f t
Δ
(δ ) =
β t
i jτ γ ,
i jτ γ 1 |d(cid:3)
1
m

i jγ
i jτ γ β t

i jτ γ β t
i jτ γ 1 |d(cid:3)

i jτ γ s β t
i jτ γ 1δi j τ γ s =δ

j |d(cid:3)
j |
.
j |=k−(cid:2), |di |=k
j | ≤ |di |=k
(14)
(15)
(16)
(17)
,
Session D5:  Network SecurityCCS’17, October 30-November 3, 2017, Dallas, TX, USA975Table 4: Classification Results of Network EM in D1
Table 5: Network Parameters of Scenario S2
Case
S11
S12
S13
ρ 1
0.67
0.48
0.45
ρ ∞
0.95
0.91
0.95
α ∞
0.90, 0.05, 0.05
0.05, 0.90, 0.05
0.90, 0.05, 0.05
0.08
0.06
F
M
P
0.04
0.02
0
0
actual
estimated
1
2
seconds
3
4
0.08
0.06
F
M
P
0.04
0.02
0
0
actual
estimated
1
2
seconds
3
4
(a) reverse-exp fT (case S13)
(b) Erlang(2) fΔ (case S13)
Figure 4: Recovery of delay parameters in D1.
While the result of Theorem 4.1 may appear daunting due to the
number of nested summations, its implementation in practice can
be done with little extra cost compared to Hershel+. Specifically,
usage of (6) in (1) for all i, j already requires five nested loops. In
the inner-most loop of that algorithm, (17) adds one increment to
a hash table that maintains the PMF of one-way delay. Updates
in (14)-(16) are performed less frequently and, in comparison, con-
sume negligible time. The only caveat is that Hershel+ can be op-
timized [41] to remove the outer summation in (6) when fT is Er-
lang(2) and fΔ is exponential. Our approach, on the other hand,
requires a hash-table lookup for both distributions. This makes its
single iteration similar in speed to unoptimized Hershel+.
Theorem 4.2. Iteration (14)-(17) is the EM algorithm for (θd , α ).
4.4 Discussion
We revisit earlier simulations on dataset D1, run (14)-(17) over the
same input, and show the result in Table 4. Compared to Table
3, the derived EM estimator significantly improves the accuracy
of both classification and vector α. The accuracy of estimated de-
lay distributions is shown in Figure 4. With the exception of noise
at the points of discontinuity of each density, functions f ∞
, f ∞
T
Δ
match the true parameters quite well.
j , which led to q∞
Since all signatures in D1 had three packets, it was easy to fig-
ure out the number of them lost in each d(cid:3)
k being
perfectly accurate, regardless of whether (16) was used or not. In a
more interesting database, which we call D2, Linux is augmented
with a fourth packet that follows after a 3-second RTO. To experi-
ment with different loss patterns, define BinT(k, p) to be a binomial
distribution with parameters (k, p) truncated to the range [0, k − 1].
Since the loss of all k packets cannot be observed, we avoid gener-
ating this case in the simulator. Additionally, suppose RevBin(k, p)
is the reverse binomial distribution such that X ∼ BinT(k, p) and
Y = k − 1 −X implies Y ∼ RevBin(k, p). With this in mind, consider
scenario S2 in Table 5, which shows qk and the average observed
loss rate among the signatures with k packets.
Case
S21
S22
S23
Delay
As in S12
As in S12
As in S12
q3
BinT(3, 0.3)
BinT(3, 0.1)
RevBin(3, 0.1)
Loss
28%
10%
57%
q4
BinT(4, 0.3)
BinT(4, 0.8)
RevBin(4, 0.1)
Loss
30%
66%
65%
Table 6: Classification Results in D2
Case
α
S21
S22
S23
0.90
0.05
0.05
0.90
0.05
0.05
0.90
0.05
0.05
0.45
Hershel+
α 1
ρ 1
0.68
0.76
0.25
0.07
0.34
0.47
0.19
0.36
0.46
0.18
0.45
EM α, fT , fΔ
α ∞
ρ ∞
0.63
0.70
0.31
0.05
0.06
0.84
0.10
0.06
0.90
0.04
0.13
0.10
ρ ∞
0.91
0.97
0.90
Full EM
α ∞
0.90
0.05
0.05
0.90
0.05
0.05
0.90
0.05
0.05
Table 6 shows classification results for three methods – Her-
shel+, the partial EM framework without loss updates (16), and
the full algorithm from Theorem 4.1. Not surprisingly, Hershel+
again struggles to recover α, even when its classification accuracy
is pretty high. Omission of (16) does create challenges for partial
EM, where in all three cases it produces worse results than Her-
shel+. On the other hand, the full algorithm improves accuracy
and delivers the exact α despite complex underlying network con-
ditions.
5 USER FEATURES
5.1 Distortion Model
Our goal in this section is to expand the second factor in (4) and
develop an estimator for its distortion model. This is done in iso-
lation from the network features, i.e., using p(d(cid:3)
j |ωi , θd ) = 1 for
all i, j. Assume b ≥ 1 user features, where each observation j pro-
vides a constant-length vector u(cid:3)
). These include
the TCP window size, IP TTL (Time to Live), IP DF (Do Not Frag-
ment flag), TCP MSS (Maximum Segment Size), and TCP options,
for a total of b = 5 integer-valued fields. Since RST features depend
on network loss, we delay their discussion until the next section.
Note that each field may be allocated a different number of bits in
the TCP/IP header and the number of available options av for u (cid:3)
jv
may depend on v (e.g., two for DF and 64K for Win).
j = (u (cid:3)
j1
, . . . , u (cid:3)
jb
Modification to user features at the target host, which we model
with a set of distributions θu , can be accomplished by manually
changing default OS parameters (e.g., editing the registry), using
specialized performance-tuning software, requesting larger/smaller
receiver kernel buffers while waiting on sockets (i.e., using set-
sockopt), and deploying network/host scrubbers [10], [37], [39],
[46], [51] whose purpose is to obfuscate the OS of protected ma-
chines. Besides intentional feature modification, distortion θu may
also accommodate unknown network stacks that build upon a doc-
umented OS, but change some of its features (e.g., new versions of
embedded Linux customized to a particular device).
Session D5:  Network SecurityCCS’17, October 30-November 3, 2017, Dallas, TX, USA976b
Prior work either omits formally modeling user volatility [5],
[6], [36], [52], [54], or assumes that uiv stays the same with some
probability πv and changes to another integer with probability
1 − πv [41], [42]. While the latter approach works well in certain
cases, it has limitations. Besides the fact that πv is generally un-
known, binary decision-making fails to create a distribution over
the available choices. For example, πv = 0.9 assumes that each
of the 65,534 non-default window sizes occurs with probability
0.1. Instead, a more balanced approach would be to assume a uni-
form distribution over the distortion possibilities and assign them
probability (1 − πv )/(av − 1). Second, it is likely that certain de-
vices are modified less frequently than others (e.g., due to firmware
restrictions) and individual distortions are OS-specific, which im-
plies that πv should depend on i. Finally, the existing methods have
no way of tracking the location and probability mass of distortion,
which does not have to be uniform in practice (e.g., a non-default
window size 257 bytes is less likely than 64K).
To overcome these problems, assume that πiv (y) is the proba-
bility that feature v of OS i is modified to become y, which gives
rise to a set of nb distributions that comprise our user-distortion
model θu . Then, the proposed classifier can be summarized by
p(u(cid:3)
j |ωi , θu ) =
πiv (u (cid:3)
jv ),
(18)
v =1
where modification to features is assumed to be independent. Note
that doing otherwise does not appear tractable at this point (i.e.,
estimation of covariance matrices produces too many variables for
EM to handle).
5.2 Iteration
We begin by discussing under what conditions the problem is iden-
tifiable, despite having a large number of unknown distributions.
Assume ϕiv := πiv (uiv ) is the probability with which feature v
stays the same for OS i. Because we do not know ahead of time
the reasoning of the user for changing the features or the new val-
ues of modified fields, the estimation problem for πiv is unsolvable
unless enough of the probability mass remains at the original lo-
cation, i.e., ϕiv is above some threshold. From common sense, it is
likely that ϕiv ≥ 0.5 holds among the general population of Inter-
net hosts; however, EM converges under even weaker conditions
(e.g., when ϕiv is the largest value in each PMF πiv ). Coupling this
with an initial state that satisfies the same constraint allows EM to
discover a unique solution.
We define the estimator for user distortion as the probability to
observe y in feature v across all matches against OS i, i.e.,
m
π t +1
iv (y) =
j =1
p(ωi |u(cid:3)
m
j , θt
p(ωi |u(cid:3)
u , α t )1u (cid:3)
j , θt
u , α t )
j =1
jv =y
.
(19)
To allow simplification of this expression below, define
b
pt
i j := α t
i p(u(cid:3)
j |ωi , θt
u , α t ) = α t