intentionally 
straightforward  manner.  In  particular,  we 
avoided the use of any advanced domain knowledge. On the 
one  hand,  this  illustrates  that  while  advanced  domain 
knowledge helps to make our approach more effective, it is 
not required. On the other hand, this is part of our effort to 
reduce the threats to validity as discussed in Section IV.D.  
Specifically,  all the  string parameters of  variable length 
were identified to be attack-payload parameters. For each of 
these parameters, we identified a single extreme value, which 
is a string typically much longer than normally expected. An 
integer  parameter  is  identified  to  be  an  attack-payload 
parameter only if it obviously indicates the length of another 
parameter such as Content-Length. For each integer attack-
payload  parameter,  we  identified  three  extreme  values, 
including  a  positive  number  that  is  smaller  than  the  actual 
length  of  the  other  parameter,  zero,  and  a  small  negative 
number. For each attack-payload parameter p, we identified 
a parameter p’ to be an attack-control parameter of p only if 
a  very  strong  connection  existed  between  p  and  p’.  For 
example,  in  Ghttpd  and  Nullhttpd,  a  parameter  named 
Request-Method  indicates  whether  the  request  is  a  GET  or 
POST request. This parameter was identified to be an attack-
control parameter of an attack-payload parameter Message-
Body,  which  represents  the  payload  carried  in  an  HTTP 
request.  A  document  that  explains  in  detail  how  these 
parameters  and  values  were  identified  is  made  available 
online for review [38]. 
TABLE III.  
EXTERNAL PARAMETER MODELS 
Subject 
Ghttpd 
Gzip 
Hypermail 
Nullhttpd 
Pine (read) 
Pine (write) 
NP 
42 
16 
28 
42 
10 
7 
NXP  ANXV  ANCP  ANCV 
5 
3 
16 
5 
10 
7 
4.8 
12.3 
12.8 
4.8 
1.9 
1.7 
2.7 
13.3 
15.5 
2.7 
2.4 
2.0 
1.4 
3.0 
2.0 
1.4 
1.4 
1.3 
     Note:  NP  =  #  of  parameters,  NXP  =  #  of  attack-payload  parameters,  ANXV  =  average  #  of 
extreme values per attack-payload parameter, ANCP = average # of attack-control parameters per 
attack-payload parameter, ANCV = average # of control values per attack-control parameter.  
TABLE IV.  
Subject 
Ghttpd 
Gzip 
Hypermail 
Nullhttpd 
Pine (read) 
Pine (write) 
191 
32 
200 
191 
89 
49 
STATISTICS ON NUMBER OF TESTS 
Total 
Min 
Avg 
3 
10 
10 
3 
3 
3 
Max 
36 
12 
10 
36 
8 
8 
27.3 
10.7 
10.0 
27.3 
6.4 
5.4 
      Note: Total = total # of tests, Min, Max, Avg = minimum, maximum, and average # of tests per            
extreme value. 
TABLE V.  
VULNERABILITY DETECTION RESULTS  
Subject 
Ghttpd 
Gzip 
Hypermail 
Nullhttpd 
Pine 
Detected 
1 
2 
5 
5 
7 
Reported  Missed 
1 
1 
2 
1 
7 
0 
0 
1 
0 
LOI 
New 
0 
1 
4 
4 
LOI 
  Note: LOI = lack of information. Our approach detected 9 new vulnerabilities in total. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:29:42 UTC from IEEE Xplore.  Restrictions apply. 
275Table  III  shows  the  number  of  different  types  of 
parameters and values for each subject program. For Ghttpd 
and  Nullhttpd,  we  used  the  same  set of attack-payload and 
attack-control  parameters  and  values.  This  is  because  both 
programs are HTTP servers and we identified the parameters 
and values from the same HTTP specification. Pine has two 
operational  modes,  read  and  write.  These  two  modes  are 
tested  separately  in  our  experiments,  because  these  two 
modes have very different interfaces and use different sets of 
parameters. We would like to emphasize that none of these 
parameters is private or otherwise internal. To the contrary, 
each parameter can be set directly be a user or attacker. 
Next  we  present  statistics  on  the  number  of  tests  we 
generated  for  the  five  subject  programs.  Recall  that  we 
generate  a  group  of  tests  for  each  extreme  value  of  each 
attack-payload parameter. Algorithm BOVTest generates the 
same  number  of  tests  for  every  extreme  value  of  the  same 
attack-payload  parameter.  Each  of  the  generated  tests  is  a 
system  test  and  therefore  represents  a  scenario  that  may 
occur in practice. 
Table V  presents  information  about  the buffer overflow 
vulnerabilities  detected  by  our  approach.  We  obtained  the 
number  of  reported  vulnerabilities  from  two  databases, 
securityfocus and securitytracker. Our approach detected all 
the reported vulnerabilities for Ghttpd, Gzip, and Nullhttpd. 
For Hypermail, a buffer overflow was reported but was not 
detected  by  our  approach.  An  inspection  revealed  that  this 
buffer  overflow  involved  a  long  string  returned  by  a  DNS 
server,  which  was  not  modeled as an  external parameter  in 
our  experiments.  For  Pine, 
reported 
vulnerabilities,  and  our  approach  also  detected  7 
vulnerabilities.  However,  no  adequate 
is 
available  for  us  to  determine  whether  the  7  reported 
vulnerabilities  are  the  same  as  those  detected  by  our 
approach.  Therefore,  we  put  LOI,  short  for  Lack  of 
Information, in the table.  
there  are  7 
information 
In  addition,  our  approach  detected  a  total  of  9  new 
vulnerabilities for the five programs. As an example, Fig. 5 
shows  a  code  segment  of  Nullhttp  that  contains  a  new 
vulnerability detected in our experiments. This vulnerability 
is  detected  when 
is  POST,  and 
in_ContentLength is a negative number. 
D.  Threats to Validity 
in_RequestMethod 
As  discussed  in  Section  II,  the  effectiveness  of  our 
approach depends on the proper identification of the attack-
payload  and  attack-control  parameters  and  their  values. 
Since  our  experiments  use  programs  that  have  known 
vulnerabilities,  the  validity  of  our  results  would  be  in 
jeopardy  if  knowledge  of  the  known  vulnerabilities  were 
used  to  identify  these  parameters  and  values  in  our 
experiments.  To  alleviate  this  potential  threat,  we  tried  to 
only  use  explicit  information  that  was  available  in  the 
specification. In addition, each time we identified a particular 
type  of  parameter  or  value,  we  provided  an  explicit 
explanation about how our decision was made in a way that 
only  used  information  available  in  the  specification,  rather 
than  other  sources.  These  explanations  were  cross-checked 
by two of the co-authors and are available for review [38].  
The  validity  of  our  results  also  depends  on 
the 
correctness  of  two  tools,  namely  ACTS  and  the  bounds 
checker,  used  in  our  experiments.  Both  of  these  two  tools 
have  been  available  for  public  access  for  a  significant 
amount of time, and have been used to conduct experiments 
for other research projects.  
The  main  external  threat  to  validity  is  the  fact  that  the 
five open-source programs used to conduct our experiments 
may  not  be representative of true  practice. These  programs 
are  real-life  programs  themselves,  and  are  chosen  from 
different  application  domains.  Some  programs  have  also 
been used in other studies. 
if (strcmp(conn[sid].dat->in_RequestMethod, "POST")==0) { 
    …… 
    if (conn[sid].dat->in_ContentLength in_ContentLength));  
        conn[sid].PostData[conn[sid].dat->in_ContentLength]='\0'; 
        … 
    } 
    … 
}
Figure 5.   A buffer overflow vulnerability example 
V.  RELATED WORK 
Our work tries to detect buffer overflow vulnerabilities, 
i.e.,  whether  there  exists  a  static  defect  that  can  cause  a 
buffer  overflow  to  occur  at  runtime.  This  is  different  from 
work  on  detecting  buffer  overflows,  which  tries  to  detect 
whether a buffer overflow  has  actually occurred at runtime 
[10].  A  fundamental  difference  between  the  two  is  that 
detecting  a  static  defect  needs  to  consider  all  possible 
behaviors  a  program  could  exercise  at  runtime,  while 
detecting  the  occurrence  of  a  runtime  phenomenon  only 
needs to deal with the program behavior in a specific runtime 
scenario.  Testing  based  approaches  to  detecting  buffer 
overflow  vulnerabilities  often  use  techniques  for  detecting 
buffer overflows to evaluate a test run. In this respect, these 
two types of approaches are complementary. As mentioned 
in Section III, our work uses a bounds checking tool to detect 
whether a buffer overflow has actually occurred in a test run. 
Our  work  is  also  different  from  work  on  runtime 
prevention,  which  tries  to  prevent  buffer  overflow  attacks 
from occurring at runtime [39]. For example, StackGuard [9] 
may terminate a process after it detects that a return address 
on  the  stack  has  been  overwritten.  Existing  approaches  to 
runtime  prevention  can  incur  significant  runtime  overhead. 
In  addition,  these  approaches  are  in  effect  after  potentially 
vulnerable  programs  are  deployed.  This  is  in  contrast  with 
our work, which aims to develop and release programs that 
are  free  from  buffer  overflow  vulnerabilities  prior  to 
deployment. 
In  the  following  we  focus  on  approaches  to  detecting 
buffer overflow vulnerabilities during the development stage. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:29:42 UTC from IEEE Xplore.  Restrictions apply. 
276test 