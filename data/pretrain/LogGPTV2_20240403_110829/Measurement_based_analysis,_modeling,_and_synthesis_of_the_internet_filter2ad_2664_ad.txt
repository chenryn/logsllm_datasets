Measured
Model
100
y
t
i
l
i
b
a
b
o
r
P
10−1
10−2
Measured
Model
Measured
Model
x 10−3
5
4
3
2
1
y
t
i
l
i
b
a
b
o
r
P
0
0
200
800
1000
400
600
Delay (ms)
(a)
e
g
a
t
n
e
c
r
e
P
r
e
t
s
u
C
l
t
s
e
g
r
a
L
100
80
60
40
20
0
0
Measured
Model
)
r
(
B
/
)
r
2
(
B
n
a
d
e
M
i
5
4
3
2
1
0
0
100
200
Cutoff (ms)
(b)
Measured
Model
1
0.8
0.6
0.4
0.2
)
N
D
(
/
)
k
(
D
300
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
10−3
0
1
0.8
0.6
0.4
0.2
0
0
5
10
In−degree
(c)
Measured
Model
20
Type1 Violation Ratio (%)
60
80
40
(f)
15
100
200
300
0
0
0.2
100
r (ms)
(d)
0.6
0.8
1
0.4
k/N
(e)
Figure 10: Model vs measured data. (a) Delay distribution. (b) Clustering cutoff. (c) In-degree distribution. (d) Median B(2r)/B(r).
(e) D(k)/D(N). (f) Triangle inequality violation ratio distribution.
a 1-dimensional Euclidean space divided into 3 equally sized bins.
We randomly divide the points into two halves, the ﬁrst half hap-
pens to lie in bin2, while the other half is spread across bin1 and
bin2. We will iteratively compute, using the ﬁrst half of the points,
the ith intensity component Ci, which is a vector of intensities for
the bins, to predict the growth observed in the second half of the
points. Each component is weighted according to how well it pre-
dicts the second half. The location uncertainty of a point in the
ﬁrst half is represented by a Gaussian probability distribution with
a certain variance or width. To compute the ﬁrst intensity compo-
nent C1, we place a Gaussian with a small width w1 that represents
a low level of uncertainty in the center of each bin and scale it by
the number of ﬁrst half points in the bin. As a result, the 99% bod-
ies of the Gaussians lie within bin2. We call the bins occupied by
the 99% bodies of the Gaussians the support of the ﬁrst component,
S1. We also deﬁne the remaining support of a component to be the
support of the current component subtracted by the support of the
previous component, i.e. Ri = Si\Si−1. Since this is the ﬁrst
component, R1 is simply S1.
The intensity I1 generated by the Gaussians is spread over the
three bins as 0, 4, 0, respectively. Now we ask, how well does R1
cover the second half of the points? If all points in the second half
are covered by R1 then I1 can account for the growth in the second
half and we are done. However, in the example, R1 is only covering
75% of the points in the second half. As a result, we weight the in-
tensity I1 by a factor p1 = 0.75 to obtain the intensity component
C1. Since we have not completely accounted for the growth in the
second half, we need to increase the location uncertainty and com-
pute the second intensity component C2. To do so, we use a wider
Gaussian (width w2) for the second iteration. The aggregate inten-
sity is still 4, but this time, it is spread across all 3 bins. Suppose
the intensities generated in the 3 bins are 1, 2, 1, respectively. The
99% body of these wider Gaussians occupy all three bins, thus the
support of the second component S2 is the set {bin1, bin2, bin3}.
The remaining support R2 is S2\S1, i.e. {bin1, bin3}. The frac-
tion of the second half covered by R2 is 25%. Thus, the intensity
150
100
50
y
t
i
s
n
e
D
l
a
c
o
L
e
g
a
r
e
v
A
0
0
10
20
Star Size
30
Measured 800
Best fit: Measured 800
Measured 1600
Best fit: Measured1600
Measured 2400
Best fit: Measured 2400
Measured 3200
Best fit: Measured 3200
Figure 12: Average local density vs local cluster (star) size for
different data sample sizes.
I2 is weighted by p2 = 0.25 to obtain C2. This iterative process
continues until either all points in the second half are covered by
Ri, or when a maximum Gaussian width has been reached. The
intensity of each bin is simply the sum of all the intensity compo-
nents Ci. Finally, we repeat the procedure to use the second half to
predict the growth in the ﬁrst half and use the average intensity of
each bin as the ﬁnal intensity. In practice, we divide the 5D space
into 100 bins in each dimension and vary the Gaussian variance or
width from one-tenth to ten times the bin width.
Technique 5: Local cluster size assignment - In order to preserve
realistic local clustering property, the synthesizer draws the cluster
sizes from the exponential distribution (as computed in Section 3.2)
that approximates the local cluster size distribution of the measured
data. What remains unclear is how to assign different cluster sizes
to the synthesized cluster centers. Should the cluster sizes be as-
signed randomly to the cluster centers? Would that be realistic?
It turns out cluster sizes are related to node densities in the mea-
sured data. Figure 12 plots the average local density at the cluster
centers, i.e. the number of nodes within 15ms of the cluster centers,
versus the local cluster size (or star size) for different sample sizes.
As can be seen, the size of a local cluster is roughly linearly related
to the local node density around the cluster center.
Therefore, the synthesizer assigns cluster sizes as follows. First,
the synthesizer computes the local node densities for the synthe-
sized cluster centers and ranks them according to the densities. The
synthesizer also ranks the cluster sizes drawn from the exponential
distribution. Then, the synthesizer assigns a cluster center of lo-
cal density rank r the cluster size of rank r. This way, the linear
relationship between cluster size and local density is preserved.
5.2 Delay Space Synthesizer DS2
, H T ype−1
, H T ype−2
g,l
g,l
g,l
g,l
g,l
, P T ype−2
, P T ype−1&2
Based on the techniques described above and in Section 4.1, we
have implemented a delay space synthesizer called DS2 (see [9] for
further information). At a high level, DS2 works as follows (Steps
1-3 are identical to those in Section 4.2): Step 1. Perform global
clustering on the measured data to assign nodes to major clusters.
Perform nearest neighbor directed graph analysis to identify local
cluster centers. Step 2. Compute a 5D Euclidean embedding of
the measured data using a robust method. Then, adjust coordi-
nates to preserve small values. Step 3. For each cluster-cluster
group g and Euclidean delay l, compute the global distortion statis-
tics P T ype−1
using
a severe violation threshold R. Step 4. At this point, the orig-
inal measured data is no longer needed. Split the 5D Euclidean
map into two, one containing only local cluster centers, and one
containing all other nodes. Then each of the two maps is further
divided according to which global cluster each node belongs. As-
suming there are three major global clusters and the remaining un-
clustered nodes form another group, then the splitting procedure
produces eight sub-maps. Based on these eight maps, separately
synthesize Euclidean maps of each part to the appropriate scale us-
ing the Euclidean map synthesis technique. Merge the eight result-
ing synthesized maps back into one synthesized map. In the ﬁnal
synthesized map, for each node, we now know whether it is a local
cluster center and which major cluster it belongs to. Step 5. Assign
a local cluster size to each synthesized center using the local cluster
size assignment technique. For each local cluster center i, compute
the local distortion statistics ri and ti. Step 6. To compute the syn-
thesized delay between node i and j, ﬁrst compute the Euclidean
delay. Apply global distortion, if necessary, according to the statis-
tics from the measured data, and ﬁnally apply local distortion if
necessary. Return ﬁnal value.
Note that a lower bound can easily be enforced on the synthe-
sized delays to mimic some sort of minimum processing delay in-
curred by network devices. DS2 provides this as an option.
5.3 Evaluating the Synthesized Delay Model
To evaluate the effectiveness of the synthesized delay model, we
ﬁrst extract a 2,000 node random sub-sample from the measured
data. Then, we feed DS2 with just this 2,000 node sub-sample
to synthesize delay spaces with 2x, 4x, and 50x scaling factors.
If DS2 correctly predicts and preserves the scaling trends, then
the synthetic 2x delay space should have properties very similar
to those found in the measured data from 3997 nodes. The larger
scaling factors (4x and 50x) are presented to illustrate how the syn-
thesizer preserves various properties under scaling. Note that at
the scaling factor of 50x, a 100,000 node delay space is synthe-
sized. Unfortunately, at this scale, we do not have efﬁcient ways
to compute global clustering (requires O(N 3) space) and triangle
inequality violation ratios (requires O(N 3) time) and thus results
for these two metrics are calculated based on a 16,000 node random
sample out of the 50x synthetic delay space.
The results in Figure 13 show that, even though the synthesis is
based on a 2,000 node subset of data, the 2x synthesized data is able
to match the characteristics of the 3997 node measured data very
well. As expected, there are a few differences. However, these
differences are small and we will show in Section 6 that they do
not negatively affect the application of the synthesized delay model
in distributed system simulations. It is also worth noting that the
scaling invariants observed in the measured data are maintained by
the synthesizer. In summary, the synthesis framework implemented
by DS2 is highly effective in creating realistic delay spaces with
compact O(N ) storage requirement.
5.4 Assumptions
DS2 is designed based on a set of assumptions that are empiri-
cally derived from delays among edge networks in the Internet. It
is not designed to synthesize delays within a local area network.
Such a capability can be incorporated into DS2 as future work.
We have experimented with PlanetLab delay data as well as
P2PSim delay data and found that DS2 can correctly synthesize
the characteristics of these data sets. However, DS2 may not work
correctly on arbitrary delay data inputs that violate the following
empirical assumptions:
• A low-dimensional Euclidean embedding can model the input de-
lay data with reasonable accuracy, ignoring triangle inequality vio-
lations and local clustering properties. Some recent studies [21, 17]
have shown that Euclidean embedding has difﬁculties in predicting
pairwise Internet delays very accurately. Note, however, that we do
not aim at predicting pairwise delays, we only use the Euclidean
embedding as a compact model of the statistical properties of the
input data.
• The in-degree distribution of the nearest neighbor graph com-
puted from the input data is exponential. The current implemen-
tation of DS2 automatically ﬁts the in-degree distribution of the
input data to an exponential distribution.
• The input data has a coarse-grained clustering structure. In ad-
dition, the delay edges across the same coarse-grained cluster pair
exhibit similar triangle inequality violation characteristics.
6. APPLICATIONS
In this section, we demonstrate the importance of using a realistic
delay model for simulation-based evaluation of distributed systems.
6.1 Server Selection
Increasingly,
Internet services are distributed across multiple
servers all over the world. The performance and cost of such In-
ternet services depend on the server selection mechanisms they
employ. Server selection redirects clients to an appropriate server,
based on factors such as the location of the client, network condi-
tions, and server load. A number of server selection systems [47,
11, 7] have been proposed and studied. In this section, the perfor-
mance of Meridian [47], Vivaldi [7] and random server selection is
evaluated using four different delay spaces: measured data, DS2,
Inet and GT-ITM.
We evaluate the accuracy of the three server selection algorithms
using the delay penalty metric, which is deﬁned as the difference
between the delay to the chosen server and the delay to the closest
server. We run each algorithm on all of the following data sets: for
measured data, in addition to the full 3997-node data, we also use a
2k sample; for DS2 data, we synthesize 2k data from a 1k sample
of the measured data, and synthesize 4k and 16k data from a 2k
x 10−3
5
4
3
2
1
y
t
i
l
i
b
a
b
o
r
P
0
0
200
)
r
(
B
/
)
r
2
(
B
n
a
d
e
M
i
5
4