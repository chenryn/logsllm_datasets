sentence, x⊕ w denote that w is injected to a random position
of x. We construct a dataset for w as follows.
(x, y) ∼ Dw,
with y =
(5)
We use y to denote the label, Dw the distribution of the
dataset, and cls(x) the CLS embedding of a sample x.
1 x = x(cid:2) ⊕ w with x(cid:2) a natural sentence
0
otherwise
(cid:6)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
72031
(cid:15)(cid:12)(cid:5)(cid:4)(cid:9)(cid:2)(cid:35)(cid:2)(cid:7)(cid:4)(cid:6)(cid:2)(cid:35)(cid:13)(cid:2)(cid:1)
(cid:12)(cid:5)(cid:4)(cid:8)(cid:2)(cid:6)(cid:4)(cid:11)(cid:2)(cid:35)(cid:2)(cid:13)
(cid:35)(cid:16)
(cid:6)(cid:4)(cid:5)
(cid:14)(cid:17)(cid:34)(cid:1)(cid:27)(cid:33)(cid:21)(cid:29)(cid:28)(cid:29)(cid:23)(cid:19)(cid:21)(cid:20)(cid:2)(cid:1)(cid:35)
(cid:14)(cid:23)(cid:24)(cid:24)(cid:1)(cid:26)(cid:21)(cid:33)(cid:21)(cid:29)(cid:1)(cid:18)(cid:32)(cid:34)(cid:1)
(cid:17)(cid:22)(cid:17)(cid:23)(cid:26)(cid:4)
(cid:35)
(cid:14)(cid:17)(cid:34)(cid:1)(cid:23)(cid:25)(cid:25)(cid:21)(cid:26)(cid:30)(cid:23)(cid:31)(cid:34)
(cid:27)(cid:33)(cid:21)(cid:29)(cid:28)(cid:29)(cid:23)(cid:19)(cid:21)(cid:20)(cid:2)(cid:1)(cid:35)
(cid:14)(cid:23)(cid:24)(cid:24)(cid:1)(cid:26)(cid:21)(cid:33)(cid:21)(cid:29)(cid:1)(cid:18)(cid:32)(cid:34)(cid:1)
(cid:17)(cid:22)(cid:17)(cid:23)(cid:26)(cid:4)(cid:1)(cid:23)(cid:25)(cid:25)(cid:21)(cid:26)(cid:30)(cid:23)(cid:31)(cid:34)
(cid:35)
(cid:5)(cid:10)(cid:16)(cid:21)(cid:10)(cid:16)(cid:8)(cid:10)(cid:1)(cid:22)(cid:2)(cid:1)(cid:6)(cid:16)(cid:9)(cid:1)
(cid:15)(cid:12)(cid:3)(cid:6)(cid:4)(cid:9)(cid:2)(cid:35)(cid:2)(cid:6)(cid:2)(cid:35)(cid:13)(cid:2)(cid:1)
(cid:12)(cid:7)(cid:4)(cid:8)(cid:2)(cid:5)(cid:4)(cid:11)(cid:2)(cid:35)(cid:2)(cid:13)(cid:2)
(cid:35)(cid:16)
(cid:22)(cid:2)(cid:17)(cid:2) (cid:21)(cid:12)(cid:10)(cid:1)(cid:22)(cid:17)(cid:19)(cid:9)(cid:1)(cid:3) (cid:4)(cid:10)(cid:18)(cid:19)(cid:10)(cid:20)(cid:10)(cid:16)(cid:21)(cid:6)(cid:21)(cid:13)(cid:17)(cid:16)
(cid:10)(cid:15)(cid:7)(cid:10)(cid:9)(cid:9)(cid:13)(cid:16)(cid:11)(cid:20)
(cid:5)(cid:4)(cid:8)
(cid:2)(cid:1)
(cid:22)(cid:10)(cid:13)(cid:11)(cid:12)(cid:21)(cid:1)(cid:1)(cid:1)
(cid:3)(cid:13)(cid:16)(cid:10)(cid:6)(cid:19)(cid:1)(cid:15)(cid:17)(cid:9)(cid:10)(cid:14)
(cid:3)(cid:13)(cid:16)(cid:10)(cid:6)(cid:19)(cid:1)(cid:15)(cid:17)(cid:9)(cid:10)(cid:14)(cid:1)
(cid:6)(cid:4)(cid:5)
(cid:5)(cid:4)(cid:6)
(cid:3)(cid:5)(cid:4)(cid:10)
(cid:35)
(cid:5)(cid:4)(cid:8)
Fig. 7: Word discriminativity analysis: linear model training
Hypothesis 1. Given a word w and a sample (x, y) ∼ Dw,
there exists a linear model F w
θ (cls(x)) = y,
with θ the model weights.
such that F w
θ
Intuitively, F w
θ can determine if w is present in a given
sentence from the CLS embedding of the sentence. To test
our hypothesis, we devise an experiment as shown in Figure 7.
Given an arbitrary word w, we insert it to a set of 2000 random
sentences from the Amazon review dataset [51] at random
positions. We further mix them with 2000 sentences without
w to form a dataset. We then train a linear classiﬁer that takes
the CLS embedding of an input sample from a pre-trained
BERT model and predicts if w is present in the sample. We
use 3600 samples to train and 400 to test. We ﬁnd that on
average we can achieve over 0.9 test accuracy for 500 random
words we have tried. The experiment on a GPT model yields
similar results. This strongly supports our hypothesis3.
For simplicity of our formal deﬁnition, we assume a two-
class classiﬁer based on transformer with labels 0 and 1. It is
poisoned with a trigger T. Without losing generality, assume
0 is the victim label and 1 the target label, the data poisoning
is through a dataset following a distribution Dp, with
(x ⊕ T, 1) ∼ Dp
and
(x, 0) ∼ Dp
Intuitively, when the trigger is injected to a sentence in class
0, its label is set to 1. Note that if w is a word in trigger T,
the distribution Dp is very similar to the aforementioned Dw.
Let MT
γ be the trojaned classiﬁer that takes the embedding of
a sample and predicts 0 or 1, with γ the model weights.
Hypothesis 2. Since F w
θ approximates the distribution of
(cls(x), y) with (x, y) ∼ Dw and MT
γ approximates the
distribution of (cls(x), y) with (x, y) ∼ Dp, and Dp is very
θ and MT
similar to Dw when w is in T, the two models F w
shall have similar behaviors.
As such, when we approximate MT
γ using a linear model
F T
β with β the model weights, θ and β shall align well,
meaning that their dot product θ (cid:4) β shall be large.
Deﬁnition 1. Given a classiﬁer Mγ and a word w, Mγ is
discriminative for w if the dot product of the weights θ of
F w
θ , the linear model in Hypothesis 1, and the weights β of
the linear approximation Fβ of Mγ is larger than a threshold.
Intuitively, in most existing NLP model backdoor attacks,
the backdoor is injected by data poisoning in which the
target label is strongly correlated with the trigger. It is hence
γ
3It is still a hypothesis as a theoretical proof may not be feasible.
likely that the trojaned model learns to decode the existential
information of some word(s) in the trigger (just like the linear
classiﬁer in the aforementioned experiment learning to predict
a word’s presence) and uses that to predict the target label.
Note that the information is easy to decode as even a linear
model can decode it. This, however, may not imply the model
must misclassify in the presence of these words as transformer
models consider contexts as well.
θ
Word Discriminativity Analysis in PICCOLO. The previous
deﬁnition is general and does not specify how to approxi-
mate a classiﬁer. In the following, we describe the concrete
discriminativity analysis in PICCOLO. We construct a dataset
with half of the sentences with w and the other half without,
as in the aforementioned experiment (Figure 7). We then train
a linear classiﬁer F w
that can predict the presence of w in
the input, from the CLS embedding cls of the input from the
subject model. The weights of the linear classiﬁer θ denote
how the transformer encodes the presence of w.
Next, consider the overall classiﬁcation procedure in Equa-
tion (4). For each output label (cid:5) of Mcls2y, we compute an
importance vector I(cid:6) that denotes the importance of individual
CLS dimensions regarding (cid:5). The importance of a dimension
i, I(cid:6)[i], is determined by a process illustrated in Figure 8
and explained in the following. We randomly sample m CLS
embedding values from a Gaussian distribution. Note that
these embeddings may not correspond to any valid sentences.
For each CLS sample, we ﬁx all the dimension values except
i, and vary i’s value 5 times from the minimum to the
maximum value in its range. For each of the 5 variations, we
collect the logits difference δ between the target and victim
labels, denoting the discriminative ability for this variation.
Value δmax − δmin denotes the importance of dimension
i for this CLS sample. Intuitively, it indicates how much
discriminativity change the value change of i can induce in
its whole range. The average importance over the m CLS
samples constitutes I(cid:6)[i]. In Figure 8, if only one CLS sample
is considered, the importance of the ﬁrst dimension (in red)
is δmax − δmin = 5.0 − 0 = 5.0. The formal deﬁnition is the
following.
cls i
T (s × Mw2t × Mt2e )[CLS][i],
min = min∀s∈Φ
with Φ a set of natural sentences
T (s × Mw2t × Mt2e )[CLS][i]
cls i
max = max∀s∈Φ
δi,c,(cid:6)
min =
∀k∈[cls i
with c(cid:2)
Mcls2y (c(cid:2)
)[(cid:5)0],
min
min , cls i
= c/i → k replacing the ith dimension of c
max
)[(cid:5)] − Mcls2y (c(cid:2)
]
with k, (cid:5)0 the output label of sample c, (cid:5) the
target label
max
min , cls i
)[(cid:5)] − Mcls2y (c(cid:2)
∀k∈[cls i
max
)[(cid:5)0]
δi,c,(cid:6)
max =
]
I(cid:6)[i] = Ec∼Gaussian [δi,c,(cid:6)
max
Mcls2y (c(cid:2)
− δi,c,(cid:6)
min ]
(6)
The ﬁrst two equations denote that we acquire the minimum
and maximum of a CLS dimension i by collecting statistics
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
82032
(cid:5)(cid:3)(cid:2)(cid:4)(cid:6)(cid:3)(cid:1)(cid:5)(cid:3)(cid:1)(cid:5)(cid:6)(cid:4)(cid:2)(cid:3)
(cid:3)
(cid:9)(cid:6)(cid:4)(cid:5)(cid:2) (cid:35)(cid:10)
(cid:35)
(cid:35)
(cid:3)(cid:1)(cid:1)
(cid:1)
(cid:9)(cid:3)(cid:7)(cid:4)(cid:5)(cid:2) (cid:35)(cid:10)
(cid:9)(cid:3)(cid:6)(cid:4)(cid:5)(cid:2) (cid:35)(cid:10)
(cid:35)
(cid:9) (cid:7)(cid:4)(cid:5)(cid:2) (cid:35)(cid:10)
(cid:35)
(cid:4)(cid:2)(cid:5)
(cid:3)(cid:2)(cid:3)
(cid:1)(cid:4)(cid:2)(cid:5)
(cid:9)(cid:8)(cid:4)(cid:5)(cid:2)(cid:1)(cid:35)(cid:4)(cid:1)(cid:10)
(cid:35)
(cid:35)
(cid:14)(cid:26)(cid:29)(cid:28)(cid:30)(cid:32)(cid:17)(cid:27)(cid:19)(cid:21)(cid:1)(cid:34)(cid:21)(cid:19)(cid:32)(cid:28)(cid:30)(cid:1)(cid:2)
(cid:11)(cid:15)(cid:16)(cid:1)(cid:13)(cid:26)(cid:18)(cid:21)(cid:20)(cid:20)(cid:24)(cid:27)(cid:23)(cid:31)
(cid:12)(cid:24)(cid:26)(cid:21)(cid:27)(cid:31)(cid:24)(cid:28)(cid:27)(cid:1)(cid:34)(cid:17)(cid:25)(cid:33)(cid:21)(cid:1)(cid:34)(cid:17)(cid:30)(cid:24)(cid:17)(cid:32)(cid:24)(cid:28)(cid:27)
Fig. 8: CLS dimension importance analysis
(cid:1)(cid:4)
(cid:15)(cid:28)(cid:23)(cid:24)(cid:32)(cid:31) (cid:20)(cid:24)(cid:22)(cid:22)(cid:21)(cid:30)(cid:21)(cid:27)(cid:19)(cid:21)
(cid:3)
(cid:4)
d (cid:6)
is,
that
to what extent
from a set of natural samples Φ. The third and fourth equations
compute the δmax and δmin for a given random CLS sample
c regarding a target label (cid:5) and dimension i. The last equation
computes the importance of dimension i regarding (cid:5) as the
average importance over many samples.
Finally, we determine the discriminativity of the classiﬁer
Mcls2y and hence of the whole subject model M for word w
with respect to label (cid:5), denoted as d (cid:6)
w , as the dot product of
the linear weight vector θ and the importance vector I(cid:6).
w = θ (cid:4) I(cid:6)
(7)
Intuitively, the dot product (cid:4) determines how much the two
vectors align,
the model considers
the dimensions suggesting w’s presence important. PICCOLO
decides that a model is trojaned when the dot product is
larger than a threshold. The threshold is empirically decided.
Intuitively, one can consider it as the largest dot product
between any word and any benign classiﬁer. We want
to
mention that although our description assumes the classiﬁer is
based on the CLS embedding, PICCOLO supports classiﬁers
using arbitrary representation embeddings (see Section VI).
Design Justiﬁcation. When triggers are long phrases, it is
unlikely for any inversion technique to invert them in the
precise forms. Therefore,
is a generic challenge that a
scanner needs to determine if a model is trojaned from a partial
trigger. PICCOLO uses the discriminativity analysis to address
the problem. Since the word-level inversion in PICCOLO can
generate a list of likely trigger words, an alternative idea is
to check if any sequence of these words can induce a high
ASR. Sophisticated sequence construction methods like beam
search [52] can be used as well. However, our experience
shows that such methods have limited effectiveness because
partial triggers often cannot achieve a high ASR; beam search
has a large search space and its greedy nature often leads to
failures.
Example. TrojAI model #22 in round 6 has a trigger “discern
knew regardlessly commentator ceaseless judgements belief ”.
None of the individual words or pair-wise combinations in
the sentence can induce a high ASR. While the subsequence
“discern belief commentator” yields a high ASR, the proba-
bility of word ‘commentator’ in the inverted vector only ranks
the 330th. In other words, all the 3-word combinations of the
top 330 words may need to be explored, which is very costly.
Additionally, beam-search works by ﬁnding the ﬁrst word with
the highest ASR (among all individual words), then ﬁnding the
pair with the highest ASR, the triple, and so on. It misses the
ﬁrst word “discern” as that alone does not have the highest
ASR although the subsequence has a high ASR.
it
In PICCOLO, as shown by Figure 9 (b) and (d), the linear
weight θ for the most likely trigger word ‘belief’, and the
(a) Importance for benign model
(b) Importance for trojaned model
(c) Linear weights for benign model
(d) Linear weights for trojaned model
Fig. 9: CLS dimension importance and linear model weights
on benign and trojaned models
dimension importance vector for the trojaned model #22 re-
garding the output label negative, align relatively well (observe
the many coinciding peaks and dips). Their dot product is 193.
In contrast in (a) and (c), the two vectors for a benign model
#35 regarding the most likely inverted word ‘ignite’ and any
label do not align well, meaning the model does not consider
the word important. The largest dot product is 119, much lower
than 193. (cid:2)
We perform an ablation study of word discriminativity
analysis on the TrojAI round 6 test set.The overall accuracy
of PICCOLO decreases from 0.907 to 0.769 when disabling
the word discriminativity analysis. The details of the ablation
study on word discriminativity analysis is in Appendix IX-G.
Figure 10 shows the dot products regarding the most likely
trigger words for 12 trojaned models (blue bars) and 12 benign
models (red bars) from TrojAI round 6 training set of Distil-
BERT models. Observe that there is a clear separation, which
explains the effectiveness of the analysis. While we argue
the basic idea of discriminatory analysis may be necessary
to handle trigger phrases with variable lengths, there may be
alternatives to Equation (6) for model approximation.
D. Trigger Word Inversion
Using tanh and Delayed Normalization. In Equation (4), we
allow each dimension of a vector x in s to have a probability
value in [0, 1]. We hence use tanh to bound dimension values
such that we can have a smooth optimization.
x = (tanh(z) + 1)/2
(8)
To ensure that word and token vectors contain distribution
values, we need to ensure that the dimension values of each
vector sum up to 1. Otherwise, the resulted word embeddings
may not have an expected embedding value but rather some
exceptionally large value that could induce untrained behaviors
in the downstream transformer and classiﬁer. The strategy
of GBDA is to normalize in each optimization epoch using
gumbel softmax. We ﬁnd it too restrictive, preventing the
Fig. 10: Word discriminativity analysis on DistilBERT models
in the round 6 training set
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
92033
(a) Every epoch
(b) Delayed normalization
Fig. 11: Comparison between normalization at every epoch
and delayed normalization
important dimensions (those corresponding to real