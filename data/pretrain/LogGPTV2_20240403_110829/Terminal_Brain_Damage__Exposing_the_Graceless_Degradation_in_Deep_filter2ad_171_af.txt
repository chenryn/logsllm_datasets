victim model during its training. This vulnerability inﬂicts
indiscriminate damage, similar to indiscriminate poisoning
attacks, through a different attack medium.
Hardware fault injection attacks. Hardware fault injec-
tion is a class of attacks that rely on hardware glitches on
the system to corrupt victim’s data. These glitches gener-
ally provide a single-bit write primitive at the physical mem-
ory; which could potentially lead to privilege escalation [67].
While in the past these attacks required physical access to
the victim’s system [11, 38], recently they have gained more
momentum since the software-based version of these attacks
were demonstrated [26, 57]. Instances of these attacks are 1)
the CLKSCREW attack [57] that leverages dynamic voltage
and frequency scaling on mobile processors generate faults
on instructions; or 2) the well-known Rowhammer vulnerabil-
ity that triggers bitwise corruptions in DRAM. Rowhammer
has been used in the context of cloud VMs [44, 67], on desk-
tops [48] and mobile [62] and even to compromise browsers
from JavaScript [9, 15, 19]. In the context of DNNs, fault
attacks have been proposed as an alternative for inﬂicting
indiscriminate damages. Instead of injecting poisonous in-
stances, fault attacks directly induce perturbations to the mod-
els running on hardware [11, 13, 34, 38, 45]. These studies
have considered the adversaries with direct access to the vic-
tim hardware [11, 13] and adversaries who randomly corrupt
parameters [34, 38, 45]. We utilize Rowhammer as an estab-
lished fault attack to demonstrate practical implications of the
graceless degradation of DNNs. Our threat model follows the
realistic single bit-ﬂip capability of a fault attack and modern
510    28th USENIX Security Symposium
USENIX Association
application of DNNs in a cloud environment, where physical
access to the hardware is impractical.
8 Conclusions
This work exposes the limits of DNN’s resilience against the
parameter perturbations. We study the vulnerability of DNN
models to single bit-ﬂips. We evaluated 19 DNN models
with six architectures on three image classiﬁcation tasks and
showed that: we can easily ﬁnd 40-50% vulnerable parameters
where an attacker can cause indiscriminate damage [RAD >
0.1] by a bit-ﬂip. We further characterize this vulnerability
based on the impact of various factors: the bit position, bit-
ﬂip direction, parameter sign, layer width, activation function,
training techniques, and model architecture. Understanding
this emerging threat, we leverage the software-induced fault
injection, Rowhammer, to demonstrate the feasibility of the
bit-ﬂip attacks in practice. In experiments with RowHammer,
we found that, without knowing the victim’s deep learning
system, the attacker can inﬂict indiscriminate damage without
system crashes. Lastly, motivated by the attacks, we discuss
two potential directions of mitigation: restricting activation
magnitudes and using low-precision numbers.
Acknowledgments
We thank Tom Goldstein, Dana Dachman-Soled, our shep-
herd, David Evans, and the anonymous reviewers for their
feedback. We also acknowledge the University of Maryland
super-computing resources10 (DeepThought2) made available
for conducting the experiments reported in our paper. This
research was partially supported by the Department of De-
fense, by the United States Ofﬁce of Naval Research (ONR)
under contract N00014-17-1-2782 (BinRec), by the European
Union’s Horizon 2020 research and innovation programme
under grant agreement No. 786669 (ReAct) and No. 825377
(UNICORE), and by the Netherlands Organisation for Scien-
tiﬁc Research through grant NWO 639.021.753 VENI (Pan-
taRhei). This paper reﬂects only the authors’ view. The fund-
ing agencies are not responsible for any use that may be made
of the information it contains.
References
[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph
Keshet. Turning your weakness into a strength: Watermarking deep
neural networks by backdooring. In 27th USENIX Security Sympo-
sium (USENIX Security 18), pages 1615–1631, Baltimore, MD, 2018.
USENIX Association.
[2] G. An. The effects of adding noise during backpropagation training
on a generalization performance. Neural Computation, 8(3):643–674,
April 1996.
10http://hpcc.umd.edu
[3] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Fixed point opti-
mization of deep convolutional neural networks for object recognition.
In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE
International Conference on, pages 1131–1135. IEEE, 2015.
[4] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning
of deep convolutional neural networks. ACM Journal on Emerging
Technologies in Computing Systems (JETC), 13(3):32, 2017.
[5] Yiwen Guo Lin Xu Yurong Chen Aojun Zhou, Anbang Yao. Incre-
mental network quantization: Towards lossless cnns with low-precision
weights. In International Conference on Learning Representations
(ICLR), 2017.
[6] Zelalem Birhanu Aweke, Salessawi Ferede Yitbarek, Rui Qiao, Reetu-
parna Das, Matthew Hicks, Yossi Oren, and Todd Austin. Anvil:
Software-based protection against next-generation rowhammer attacks.
ACM SIGPLAN Notices, 51(4):743–755, 2016.
[7] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable
methods for 8-bit training of neural networks. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
Advances in Neural Information Processing Systems 31, pages 5145–
5153. Curran Associates, Inc., 2018.
[8] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks
against support vector machines. In Proceedings of the 29th Interna-
tional Coference on International Conference on Machine Learning,
ICML’12, pages 1467–1474, USA, 2012. Omnipress.
[9] Erik Bosman, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida.
Dedup est machina: Memory deduplication as an advanced exploitation
vector. In 2016 IEEE symposium on security and privacy (SP), pages
987–1004. IEEE, 2016.
[10] Ferdinand Brasser, Lucas Davi, David Gens, Christopher Liebchen,
and Ahmad-Reza Sadeghi. Can’t touch this: Software-only mitigation
against rowhammer attacks targeting kernel memory. In 26th USENIX
Security Symposium (USENIX Security 17), pages 117–130, Vancouver,
BC, 2017. USENIX Association.
[11] Jakub Breier, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and
In Pro-
Yang Liu. Practical fault attack on deep neural networks.
ceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’18, pages 2204–2206, New York, NY,
USA, 2018. ACM.
[12] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deep-
driving: Learning affordance for direct perception in autonomous driv-
ing. In Proceedings of the IEEE International Conference on Computer
Vision, pages 2722–2730, 2015.
[13] Joseph Clements and Yingjie Lao. Hardware trojan attacks on neural
networks, 2018.
[14] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob
Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for
stochastic optimization, 2018.
[15] Pietro Frigo, Cristiano Giuffrida, Herbert Bos, and Kaveh Razavi.
Grand pwning unit: accelerating microarchitectural attacks with the
gpu. In Grand Pwning Unit: Accelerating Microarchitectural Attacks
with the GPU, page 0. IEEE, 2018.
[16] Sanjay Ghemawat. Tcmalloc : Thread-caching malloc, 2018.
[17] Daniel Gruss, Erik Kraft, Trishita Tiwari, Michael Schwarz, Ari Tracht-
enberg, Jason Hennessey, Alex Ionescu, and Anders Fogh. Page cache
attacks. arXiv preprint arXiv:1901.01161, 2019.
[18] Daniel Gruss, Moritz Lipp, Michael Schwarz, Daniel Genkin, Jonas
Jufﬁnger, Sioli O’Connell, Wolfgang Schoechl, and Yuval Yarom. An-
other ﬂip in the wall of rowhammer defenses. In 2018 IEEE Symposium
on Security and Privacy (SP), pages 245–261. IEEE, 2018.
[19] Daniel Gruss, Clémentine Maurice, and Stefan Mangard. Rowhammer.
js: A remote software-induced fault attack in javascript. In International
Conference on Detection of Intrusions and Malware, and Vulnerability
Assessment, pages 300–321. Springer, 2016.
USENIX Association
28th USENIX Security Symposium    511
[20] Song Han, Huizi Mao, and William J Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quantization and
huffman coding. International Conference on Learning Representa-
tions (ICLR), 2016.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving
deep into rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proceedings of the IEEE international conference on
computer vision, pages 1026–1034, 2015.
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 770–778,
2016.
[23] Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick,
Trevor Darrell, and Kurt Keutzer. Densenet: Implementing efﬁcient
convnet descriptor pyramids. arXiv preprint arXiv:1404.1869, 2014.
[24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In Francis
Bach and David Blei, editors, Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Ma-
chine Learning Research, pages 448–456, Lille, France, 07–09 Jul 2015.
PMLR.
[25] Barry L Kalman and Stan C Kwasny. Why tanh: Choosing a sig-
moidal function. In Neural Networks, 1992. IJCNN., International
Joint Conference on, volume 4, pages 578–581. IEEE, 1992.
[26] Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee,
Donghyuk Lee, Chris Wilkerson, Konrad Lai, and Onur Mutlu. Flip-
ping bits in memory without accessing them: An experimental study of
dram disturbance errors. In ACM SIGARCH Computer Architecture
News, volume 42, pages 361–372. IEEE Press, 2014.
[27] Radhesh Krishnan Konoth, Marco Oliverio, Andrei Tatar, Dennis An-
driesse, Herbert Bos, Cristiano Giuffrida, and Kaveh Razavi. Zebram:
Comprehensive and compatible software protection against rowham-
mer attacks. In 13th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 18), pages 697–710, Carlsbad, CA, 2018.
USENIX Association.
[28] Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks
on cifar-10. Unpublished manuscript, 40(7), 2010.
[29] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. In Advances in
neural information processing systems, pages 1097–1105, 2012.
[31] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
Gradient-based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278–2324, 1998.
[32] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage.
In Advances in neural information processing systems, pages 598–605,
1990.
[33] Yann LeCun et al. Lenet-5, convolutional neural networks. URL:
http://yann. lecun. com/exdb/lenet, page 20, 2015.
[34] Guanpeng Li, Siva Kumar Sastry Hari, Michael Sullivan, Timothy
Tsai, Karthik Pattabiraman, Joel Emer, and Stephen W Keckler. Un-
derstanding error propagation in deep learning neural network (dnn)
accelerators and applications. In Proceedings of the International Con-
ference for High Performance Computing, Networking, Storage and
Analysis, page 8. ACM, 2017.
[35] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Pe-
arXiv preprint
Pruning ﬁlters for efﬁcient convnets.
ter Graf.
arXiv:1608.08710, 2016.
[36] Moritz Lipp, Misiker Tadesse Aga, Michael Schwarz, Daniel Gruss,
Clémentine Maurice, Lukas Raab, and Lukas Lamster. Nethammer:
Inducing rowhammer faults through network requests. arXiv preprint
arXiv:1805.04956, 2018.
[37] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning:
Defending against backdooring attacks on deep neural networks. In
Research in Attacks, Intrusions, and Defenses (RAID), pages 273–294,
2018.
[38] Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. Fault injection
attack on deep neural network. In Proceedings of the 36th International
Conference on Computer-Aided Design, pages 131–138. IEEE Press,
2017.
[39] Jemalloc manual. Jemalloc: general purpose memory allocation func-
tions. http://jemalloc.net/jemalloc.3.html, 2019.
[40] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph,
Benjamin IP Rubinstein, Udam Saini, Charles A Sutton, J Doug Tygar,
and Kai Xia. Exploiting machine learning to subvert your spam ﬁlter.
LEET, 8:1–9, 2008.
[41] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower clas-
siﬁcation over a large number of classes. In Computer Vision, Graphics
& Image Processing, 2008. ICVGIP’08. Sixth Indian Conference on,
pages 722–729. IEEE, 2008.
[42] Minghai Qin, Chao Sun, and Dejan Vucinic. Robustness of neural
networks against storage media errors, 2017.
[43] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali
Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional
neural networks. In ECCV, 2016.
[44] Kaveh Razavi, Ben Gras, Erik Bosman, Bart Preneel, Cristiano Giuf-
frida, and Herbert Bos. Flip feng shui: Hammering a needle in the
software stack. In 25th USENIX Security Symposium (USENIX Security
16), pages 1–18, Austin, TX, 2016. USENIX Association.
[45] Brandon Reagen, Udit Gupta, Lillian Pentecost, Paul Whatmough,
Sae Kyu Lee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei.
Ares: A framework for quantifying the resilience of deep neural net-
works. In 2018 55th ACM/ESDA/IEEE Design Automation Conference
(DAC), pages 1–6. IEEE, 2018.
[46] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet
Large Scale Visual Recognition Challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015.
[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, et al. Imagenet large scale visual recognition chal-
International Journal of Computer Vision, 115(3):211–252,
lenge.
2015.
[48] Mark Seaborn and Thomas Dullien. Exploiting the dram rowhammer
bug to gain kernel privileges.
[49] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu,
Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs!
targeted clean-label poisoning attacks on neural networks. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 31,
pages 6106–6116. Curran Associates, Inc., 2018.
[50] Karen Simonyan and Andrew Zisserman. Very deep convolutional
networks for large-scale image recognition. International Conference
on Learning Representations (ICLR), 2015.
[51] Nikolai Smolyanskiy, Alexey Kamenev, Jeffrey Smith, and Stan Birch-
ﬁeld. Toward low-ﬂying autonomous mav trail navigation using deep
neural networks for environmental awareness. In 2017 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS), pages
4241–4247. IEEE, 2017.
[52] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,
and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural
networks from overﬁtting. The Journal of Machine Learning Research,
15(1):1929–1958, 2014.
512    28th USENIX Security Symposium
USENIX Association
[53] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.
Man vs. computer: Benchmarking machine learning algorithms for
trafﬁc sign recognition. Neural networks, 32:323–332, 2012.
[54] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certiﬁed
defenses for data poisoning attacks. In Advances in Neural Information
Processing Systems, pages 3517–3529, 2017.
[55] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and
Tudor Dumitras. When does machine learning FAIL? generalized trans-
ferability for evasion and poisoning attacks. In 27th USENIX Security
Symposium (USENIX Security 18), pages 1299–1316, Baltimore, MD,
2018. USENIX Association.
[56] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and
Zbigniew Wojna. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2818–2826, 2016.
[57] Adrian Tang, Simha Sethumadhavan, and Salvatore Stolfo.
CLKSCREW: Exposing the perils of security-oblivious energy
management. In 26th USENIX Security Symposium (USENIX Security
17), pages 1057–1074, Vancouver, BC, 2017. USENIX Association.
[58] Andrei Tatar, Cristiano Giuffrida, Herbert Bos, and Kaveh Razavi. De-