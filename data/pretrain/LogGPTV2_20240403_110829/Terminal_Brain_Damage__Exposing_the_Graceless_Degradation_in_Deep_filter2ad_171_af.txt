### Vulnerabilities in Deep Neural Networks (DNNs) During Training

During the training of deep neural networks (DNNs), a specific vulnerability can inflict indiscriminate damage, similar to poisoning attacks, but through a different attack medium. This type of attack is known as a hardware fault injection attack.

### Hardware Fault Injection Attacks

Hardware fault injection attacks exploit hardware glitches to corrupt the victim's data. These glitches typically provide a single-bit write primitive at the physical memory level, which can lead to privilege escalation. Historically, these attacks required physical access to the victim's system. However, recent advancements have shown that software-based versions of these attacks are also possible, increasing their potential impact.

#### Examples of Hardware Fault Injection Attacks
1. **CLKSCREW Attack**: This attack leverages dynamic voltage and frequency scaling on mobile processors to generate faults in instructions.
2. **Rowhammer Vulnerability**: This well-known vulnerability triggers bitwise corruptions in DRAM. It has been used in various contexts, including cloud virtual machines, desktops, mobile devices, and even to compromise browsers from JavaScript.

In the context of DNNs, fault attacks have been proposed as an alternative for inflicting indiscriminate damages. Instead of injecting poisonous instances, fault attacks directly induce perturbations to the models running on hardware. Previous studies have considered adversaries with direct access to the victim hardware and those who randomly corrupt parameters. In this work, we utilize Rowhammer, a well-established fault attack, to demonstrate the practical implications of the graceful degradation of DNNs. Our threat model follows the realistic single bit-flip capability of a fault attack and the modern application of DNNs in a cloud environment, where physical access to the hardware is impractical.

### Conclusions

This study exposes the limits of DNN resilience against parameter perturbations. We evaluate 19 DNN models with six architectures on three image classification tasks and show that it is possible to find 40-50% of vulnerable parameters where an attacker can cause significant damage by a bit-flip. We further characterize this vulnerability based on the impact of various factors: bit position, bit-flip direction, parameter sign, layer width, activation function, training techniques, and model architecture.

Understanding this emerging threat, we leverage the software-induced fault injection, Rowhammer, to demonstrate the feasibility of bit-flip attacks in practice. Our experiments with Rowhammer show that, without knowing the victim's deep learning system, an attacker can inflict indiscriminate damage without causing system crashes. Motivated by these findings, we discuss two potential mitigation strategies: restricting activation magnitudes and using low-precision numbers.

### Acknowledgments

We thank Tom Goldstein, Dana Dachman-Soled, our shepherd, David Evans, and the anonymous reviewers for their feedback. We also acknowledge the University of Maryland super-computing resources (DeepThought2) made available for conducting the experiments reported in our paper. This research was partially supported by the Department of Defense, the United States Office of Naval Research (ONR) under contract N00014-17-1-2782 (BinRec), the European Unionâ€™s Horizon 2020 research and innovation programme under grant agreements No. 786669 (ReAct) and No. 825377 (UNICORE), and the Netherlands Organisation for Scientific Research through grant NWO 639.021.753 VENI (PantaRhei). The views expressed in this paper are solely those of the authors, and the funding agencies are not responsible for any use that may be made of the information it contains.

### References

[References listed here]

---

This revised version aims to enhance clarity, coherence, and professionalism, making the text more accessible and informative for the reader.