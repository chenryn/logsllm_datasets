### System Info
transformers version: 4.17.0  
Python version: 3.7.0  
torch version: 1.10.1
### Who can help?
_No response_
### Information
  * The official example scripts
  * My own modified scripts
### Tasks
  * An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)
  * My own task or dataset (give details below)
### Reproduction
CLIP(https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-
image-text).
I have implemented a Dataset to train, but i have found that after each epoch
the training loss will drop suddenly. The Dataset overrides three methods(
**init** , **getitem** and **len** ) and i couldn't figure out the reason for
the above phenomenon.
I think the data is shuffled properly(checked) and the learning_rate drops
smoothly(observed).  
I would appreciate it if you could afford time to help me.
The picture is drawn according to the trainer_state.json
![image](https://user-
images.githubusercontent.com/27990344/186142292-6c3d4a56-9c9e-45b2-a139-1668b995e59b.png)
### Expected behavior
Figure out the reason.