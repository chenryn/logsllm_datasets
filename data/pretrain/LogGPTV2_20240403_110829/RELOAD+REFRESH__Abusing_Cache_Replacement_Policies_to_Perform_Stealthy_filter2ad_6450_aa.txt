title:RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy
Cache Attacks
author:Samira Briongos and
Pedro Malag&apos;on and
Jos&apos;e Manuel Moya and
Thomas Eisenbarth
RELOAD+REFRESH: Abusing Cache Replacement 
Policies to Perform Stealthy Cache Attacks
Samira Briongos, Pedro Malagón, and José M. Moya, 
Integrated Systems Laboratory, Universidad Politécnica de Madrid; 
Thomas Eisenbarth, University of Lübeck and Worcester Polytechnic Institute
https://www.usenix.org/conference/usenixsecurity20/presentation/briongos
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.RELOAD+REFRESH: Abusing Cache Replacement
Policies to Perform Stealthy Cache Attacks
Samira Briongos1, Pedro Malagón1, José M. Moya1 and Thomas Eisenbarth2,3
1Integrated Systems Laboratory, Universidad Politécnica de Madrid, Madrid, Spain
2University of Lübeck, Lübeck, Germany
3Worcester Polytechnic Institute, Worcester, MA, USA
Abstract
Caches have become the prime method for unintended infor-
mation extraction across logical isolation boundaries. They
are widely available on all major CPU platforms and, as a
side channel, caches provide great resolution, making them
the most convenient channel for Spectre and Meltdown. As a
consequence, several methods to stop cache attacks by detect-
ing them have been proposed. Detection is strongly aided by
the fact that observing cache activity of co-resident processes
is not possible without altering the cache state and thereby
forcing evictions on the observed processes. In this work, we
show that this widely held assumption is incorrect. Through
clever usage of the cache replacement policy, it is possible
to track cache accesses of a victim's process without forcing
evictions on the victim's data. Hence, online detection mecha-
nisms that rely on these evictions can be circumvented as they
would not detect the introduced RELOAD+REFRESH attack.
The attack requires a profound understanding of the cache
replacement policy. We present a methodology to recover
the replacement policy and apply it to the last ﬁve genera-
tions of Intel processors. We further show empirically that
the performance of RELOAD+REFRESH on cryptographic
implementations is comparable to that of other widely used
cache attacks, while detection methods that rely on L3 cache
events are successfully thwarted.
1 Introduction
The microarchitecture of modern CPUs shares resources
among concurrent processes. This sharing may result in un-
intended information ﬂows between concurrent processes.
Microarchitectural attacks, which exploit these information
ﬂows, have received a lot of attention in academia, indus-
try and, with Spectre and Meltdown [34, 39], even in the
public news. The OS or the hypervisor in virtual environ-
ments provide strict logical isolation among processes to en-
able secure multi threading. Yet, a malicious process can
intentionally create contention to gain information about co-
resident processes. Exploitable hardware resources include
the branch prediction unit [3–5], the DRAM [33, 50, 54] and
the cache [7,15,22,47,48,61]. Last level caches (LLC) provide
very high temporal and spatial resolution to observe and track
memory access patterns. As a consequence, any code that
generates cache utilization patterns dependent on secret data
is vulnerable. Cache attacks can trespass VM boundaries to
infer secret keys from neighboring processes or VMs [23,52],
break security protocols [28,53] or compromise the end users
privacy [47], and they can leak information from within a
victim memory address space [34] when combined with other
techniques.
Cache and other microarchitectural attacks pose a great
threat and consequently, different techniques have been pro-
posed for their detection and/or mitigation [16]. Among
these proposals, hardware countermeasures take years to in-
tegrate and deploy, may induce performance penalties and
currently, we are not aware of any manufacturer that has im-
plemented them. Other proposals are meant for cloud hyper-
visors [32, 37, 56] and require making small modiﬁcations to
the kernel conﬁguration. Similarly, to the best of our knowl-
edge, no hypervisor implements them, presumably due to the
overhead they entail.
As a result, the only solution that seems practical for users
that want to protect themselves against this kind of threat,
is to detect ongoing attacks and then react in some way. To
this end, several proposals [10, 13, 36, 49, 64] use hardware
performance counters (HPCs) to detect ongoing microarchi-
tectural attacks. These counters are special registers available
in all modern CPUs that monitor hardware events such as
cache misses. Some of these proposals are able to detect even
attacks that were specially designed to bypass other counter-
measures [20]. The common assumption in these works is
that the attacker induces measurable effects on the victim. We,
on the contrary, demonstrate that it is possible to obtain in-
formation from the victim while keeping its data in the cache
and, consequently, not signiﬁcantly altering its behavior, thus
making attack detection harder.
USENIX Association
29th USENIX Security Symposium    1967
Our Contribution: We analyze the replacement policy of
current Intel CPUs and identify a new strategy that allows
an attacker to monitor cache set accesses without forcing
evictions of the victim 's data, thereby creating a new and
stealthier cache-based microarchitectural attack. To achieve
this goal, we perform the ﬁrst full reverse engineering of
different replacement policies present in various generations
of Intel Core processors. We propose a technique that can be
extended to study replacement policies of other processors.
Using this technique, we demonstrate that it is possible to
accurately predict which element of the set will be replaced
in case of a cache miss. Then, we show that it is possible
to exploit these deterministic cache replacement policies to
derive a sophisticated cache attack: RELOAD+REFRESH,
which is able to monitor the memory accesses of the desired
victim without generating LLC misses.
We analyze the covert channel that this attack creates, and
demonstrate that it has similar performance to state-of-the-art
attacks, with a slightly decreased temporal resolution. As a
proof of concept, we demonstrate how RELOAD+REFRESH
works by retrieving the key of a T-Table implementation of
AES and attacking the square and multiply version of RSA.
We verify that our attack has a negligible effect on LLC re-
lated events, which makes it stealthy for countermeasures
monitoring the LLC behavior. Instead, the attack changes the
behavior of L1/L2 caches. Thus, our work stresses the need
for detection mechanisms to also consider such events. Which,
in turn, highlights the hardness of the performance counters
set selection to detect all possible cache attacks, including
ours and possible future attacks. To sum up, this work:
• introduces a methodology to test different replacement
policies in modern caches.
• uncovers the replacement policy currently implemented
in modern Intel Core processor generations, from fourth
to eighth generation.
• expands the understanding of modern caches and lays
the basis for improving traditional cache attacks.
• presents RELOAD+REFRESH, a new attack that ex-
ploits Intel cache replacement policies to extract infor-
mation referring to a victim memory accesses.
• shows that the proposed attack causes negligible cache
misses on the victim, which renders it undetectable by
state-of-the-art countermeasures.
2 Background and related work
2.1 Cache architecture
CPU caches are small banks of fast memory located between
the CPU cores and the RAM. As they are placed on the CPU
die and close to the cores, they have low access latencies and
thus reduce memory access times observed by the processor,
improving the overall performance. Modern processors in-
clude cache memories that are hierarchically organized; low
level caches (L1 and L2) are core private, smaller and closer
to the processor, whereas the last level cache (LLC or L3)
is bigger and shared among all the cores. It is divided into
slices interconnected by a ring bus. The physical address of
each element determines its mapping to a slice by a complex
addressing function [44].
Intel’s processors traditionally have included L3 inclusive
caches: all the data which is present in the private lower caches
has to be in the shared L3 cache. This approach makes cache
coherence much easier to implement. However, presumably
due to cache attacks, the newest Intel Skylake Server micro
architecture uses a non-inclusive Last Level Cache [24].
In most modern processors caches are W -way set-
associative. The cache is organized into multiple sets (S),
each of them containing W lines of usually 64 bytes of data.
The set in which each line is placed is derived from its ad-
dress. The address bits are divided into offset (lowest-order
bits used to locate data within a line), index (log2(S) consecu-
tive bits starting from the offset bits that address the set) and
tag (remaining bits which identify if the data is cached).
2.2 Cache replacement policies
When the processor requests some data, it ﬁrst tries to retrieve
this data from the cache (it starts looking in the lowest levels
up to the last level). In the event of a cache hit, the data is
loaded from the cache. On the contrary, in the event of a cache
miss, the data is retrieved from the main memory and it is also
placed in the cache assuming that it will be re-used in the near
future. If there is no free space in the cache set, the memory
controller has to decide which element in the cache has to
be evicted. Since the processor may stall for several cycles
whenever there is a cache miss, the decision of which data is
evicted and which data stays is crucial for the performance.
Many replacement policies are possible including, for ex-
ample, FIFO (First in First Out), LRU (Least Recently Used)
or its approximations such as NRU [55] (Not Recently Used),
LFU (Least Frequently Used), CLOCK [29](keeps a circu-
lar list of the elements) or even pseudo-random replacement
policies. Modern high-performance processors implement
approximations to LRU, because a truly LRU policy is hard
to implement, as it requires complex hardware to track each
access.
LRU or pseudo-LRU policies have demonstrated to per-
form well in most situations. Nevertheless, LRU policy be-
haves poorly for memory-intensive workloads whose working
set is bigger than the available cache size or for scans (bursts
of one-time access requests). As a result, adaptive algorithms,
which are capable to adapt themselves to changes in the work-
loads, have been proposed. In 2003, Megiddo el al. [45] pro-
posed ARC (Adaptive Replacement Cache) a hybrid of LRU
1968    29th USENIX Security Symposium
USENIX Association
and LFU. One year later, Bansal et al. [9] presented their so-
lution based on LFU and CLOCK, which they named CAR
(Clock with Adaptive Replacement).
In 2007 Quereshi et al. [51] suggested that performance
could be improved by changing the insertion policy while
maintaining the eviction policy. LIP (LRU Insertion Policy)
consists in inserting each new piece of data in the LRU po-
sition whereas BIP (Bimodal Insertion Policy) most of the
times places the new data in the LRU position and sometimes
(in-frequently) inserts it in the MRU position. In order to de-
cide which of the two policies behaves better, they proposed
a dynamic insertion policy (DIP). DIP chooses between LIP
and BIP depending on which one incurs fewer misses.
In 2010, Jaleel et al. [31] proposed a cache replacement
algorithm that makes use of Re-reference Interval Prediction
(RRIP). By using 2 bits per cache line, RRIP predicts if a
cache line is going to be re-referenced in the near future. In
case of eviction, the line with the longest interval prediction
will be selected. Analogously to Quereshi et al., they pre-
sented two different approaches: Static RRIP (SRRIP) which
inserts each new block with an intermediate re-reference, and
Bimodal RRIP (BRRIP) which inserts most blocks with a dis-
tant re-reference interval and sometimes with an intermediate
re-reference interval. They also proposed using set dueling
to decide which policy ﬁts better for the running application
(Dynamic RRIP or DRRIP).
Regarding Intel processors, their replacement policy is
known as "Quad-Age LRU" [30] and it is undocumented.
The ﬁrst serious attempt to reveal the cache replacement pol-
icy of different processors was made by Abel et al. [1]. In
their work, they were able to uncover the replacement policy
of an Intel Atom D525 processor and to infer a pseudo-LRU
policy in an Intel Core 2 Duo E6300 processor. They later
complemented their original work [2] and found a model that
explained the eviction policy in other machines (Intel Core
2 Duo E6750 and E8400). Later on, Wong [60] showed that
Intel's Ivy Bridge processors indeed implement a dynamic
insertion policy as suggested in previous proposals [31, 51].
He was able to identify the regions that apparently had a ﬁxed
policy by measuring the average latency of the accesses to
arrays of different sizes and provided some test code. Such
regions were similarly observed by us in our experiments
(Figure 3). These works have in common that the authors
perform different sequences of memory accesses, and use a
mechanism to estimate/measure the number of misses and
later compare their measurements with the expected misses.
However, they did not explain which concrete element in the
cache would be evicted in the event of a miss.
Gruss et al. [19] studied cache eviction strategies on recent
Intel CPUs in order to replace the clflush instruction and
build a remote Rowhammer attack. As they mention, their
work is not strictly a reverse engineering of the replacement
policy, rather they test access patterns to ﬁnd the best evic-
tion strategy. In a work concurrent to ours, Vila et al. [57]
tried to evaluate the inﬂuence of the replacement policy when
obtaining the eviction set. Their results also show that some
processors include adaptive policies whereas others do not.
To the best of our knowledge, our work is the ﬁrst one
that provides a comprehensive description of the replacement
policies implemented on modern Intel processors up to the
point that we are able to accurately determine which element
of the set would be evicted using the information about the
sequence of accesses.
2.3 Cache attacks
Cache attacks monitor the utilization of the cache (the se-
quence of cache hits and misses) to retrieve information about
a co-resident victim. Whenever the pattern of memory ac-
cesses of a security-critical piece of software depends on the
actual value of sensible data, such as a secret key, this sensi-
tive data can be deduced by an attacker and will no longer be
private.
Traditionally, cache attacks have been grouped into three
categories [16]: FLUSH+RELOAD, PRIME+PROBE and
EVICT+TIME. From those, the FLUSH+RELOAD and the
PRIME+PROBE attacks (and their variants) stand over the
rest due to their higher resolution.
Both attacks target the LLC, selecting one memory location
that is expected to be accessed by the victim process. They
consist of three stages: initialization (the attacker pre-
pares the cache somehow), waiting (the attacker waits while
the victim executes) and recovering (the attacker checks the
state of the cache to retrieve information about the victim).
2.3.1 FLUSH+RELOAD
This attack relies on the existence of shared memory. Thus, it
requires memory deduplication to be enabled. Deduplication
is an optimization technique designed to improve memory
utilization by merging duplicate memory pages. Using the
clflush instruction the attacker removes the target lines from
the cache, then waits for the victim process to execute (or an
equivalent estimated time) and ﬁnally measures the time it
takes to reload the previously ﬂushed data. Low reload times
mean the victim has used the data.
It was ﬁrst introduced in [22], and was later extended to
target the LLC to retrieve cryptographic keys, TLS protocol
session messages or keyboard keystrokes across VMs [21, 28,
61]. Further, Zhang et al. [65] showed that it was applicable
in several commercial PaaS clouds.
Relying on the clflush instruction and with the same re-
quirements as FLUSH+RELOAD, Gruss et al. [20] proposed
the FLUSH+FLUSH attack. It was intended to be stealthy and
bypass existing monitoring systems. This variant recovers the
information by measuring the execution time of the clflush
instruction instead of the reload time, thus avoiding direct
cache accesses and, as a consequence, detection. However,
USENIX Association
29th USENIX Security Symposium    1969
some works [10, 36] consider its effect also on the victim's
side and succeed in its detection.
2.3.2 PRIME+PROBE
Contrary to the FLUSH+RELOAD attack, PRIME+PROBE
is agnostic to special OS features in the system. Therefore,
it can be applied to virtually every system. Moreover, it can
recover information from dynamically allocated data. To do
so, the attacker ﬁrst ﬁlls or primes the cache set in which the
victim data will be placed with its own data (initialization
stage). Then, he waits and ﬁnally probes the desired set look-
ing for time variations that carry information about the victim
activity.
This attack was ﬁrst proposed for the L1 data cache in [48]
and was later expanded to the L1 instruction cache [6]. These
approaches required both victim and attacker to share the
same core, which diminishes practicality. However, it has
been recently shown to be applicable to LLC. Researchers
have bypassed several difﬁculties to target the LLC, as retriev-
ing its complex address mapping [25, 44, 62], and recovered
cryptographic keys, keyboard typed keystrokes [15, 26, 38] or
even a RSA key in the Amazon EC2 cloud [23].