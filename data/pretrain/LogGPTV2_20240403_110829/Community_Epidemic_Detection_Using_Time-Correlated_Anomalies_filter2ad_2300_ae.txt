### Table 2: Training Data Summary

| Application | Version | Calls | Time (sec) | Rate (calls/sec) | Unique | Ti (sec) |
|-------------|---------|-------|------------|------------------|--------|----------|
| Adium       | 1.2.7   | 50,514 | 198.204    | 54.451           | 33,278 | 1975.11  |
| Camino      | 1.6.1Int-v2 | 103,634 | 2195.65   | 896.85           | 57,385 | 7.2605   |
| Mail        | 3.3     | 126,467 | 5.54098   | 6031.4           | 48,630 | 113,341,557 |
| TextEdit    | 1.5 (244)| 4469   | 176,170    | 31,794           | 106,774,240 | - |

**Note:** The "Unique" column indicates the number of unique length-six sequences. Ti is the maximum time from the beginning of one system call to the start of the next.

---

### 6.1 Data Collection and Experiment Setup

We collected system call and timing traces from commercial, off-the-shelf software under normal usage by the authors, using the utility `dtrace`. The applications used in our experiments include:
- **Adium**: A chat program
- **Camino**: A web browser
- **Mail**: A mail client
- **TextEdit**: A simple text editor

A summary of the data collected is provided in Table 2. When compared to real deployments in Sections 4 and 5, we find that our simulations are a reasonable approximation. It's important to note that while Syzygy must build a dynamic model of application behavior, it does not need to learn exploit signatures.

Many current exploits in the wild are relatively easy for Syzygy to detect due to their overt misbehavior (large δ). However, this section focuses on Syzygy’s ability to detect next-generation exploits under adverse conditions. These exploits can infect the application at any execution point, have access to all of Syzygy’s data and parameters, and can perform skillful mimicry. The adverse conditions include client heterogeneity and tainted training data.

To simulate such behavior, we use four next-generation exploits:
- **mailspam**: Infects Mail, then composes and sends a large number of emails (based on the open mail relay in the Sobig worm’s trojan payload).
- **prompttext**: Infects TextEdit, then asks the user for input that it writes to a file (based on file creation and deletion seen in SirCam, Chernobyl, or Klez [40]).
- **screenshot**: [Description missing in the original text]

---

### 6.2 Experimental Methodology

We repeat the randomized process 1000 times per example to ensure statistically meaningful metrics. In each experiment, Syzygy is presented with an equal number of healthy and infected examples, although Syzygy does not use this information in its detection process. This setup is not intended to reflect the base rate of intrusions in a system but rather to increase the precision of the metrics.

As the size of the training set approaches infinity, the influence of removing the current trace file from the training set diminishes. Therefore, it is sufficient to select random windows from the traces because Syzygy is memoryless outside of each sample. Unless otherwise noted, we set \( W_i = 1000 \) sequences and \( V = \mu_H + 2\sigma_H \), where \( H \) is the distribution of community scores for a community of size \( n \), as described in Section 3.3.

The results of our controlled experiments are presented in Sections 6.2–6.5.

---

### 6.3 Detection of Next-Generation Exploits

Next-generation exploits are more sophisticated and can evade traditional detection methods. They can infect the application at any execution point, have access to all of Syzygy’s data and parameters, and can perform skillful mimicry. To evaluate Syzygy’s effectiveness in detecting these advanced threats, we simulate the following scenarios:

- **Client Heterogeneity**: Exploits that can adapt to different client environments.
- **Tainted Training Data**: Exploits that can manipulate the training data to avoid detection.

By simulating these adverse conditions, we aim to test Syzygy’s robustness and distinguishing ability in detecting next-generation exploits.

---

### 6.4 Results and Discussion

[This section would contain the detailed results and discussion of the experiments, including any charts, graphs, or tables. Since the specific results and discussion are not provided in the original text, they should be added here.]

---

### 6.5 Conclusion

[This section would summarize the key findings and conclusions of the experiments, highlighting the strengths and limitations of Syzygy in detecting next-generation exploits. Since the specific conclusions are not provided in the original text, they should be added here.]

---

This revised version aims to provide a clear, coherent, and professional presentation of the experimental setup, methodology, and results.