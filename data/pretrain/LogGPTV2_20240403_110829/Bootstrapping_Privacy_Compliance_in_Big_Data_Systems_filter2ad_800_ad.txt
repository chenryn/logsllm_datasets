columns. The languages provide a mechanism to read a ﬂat
ﬁle (e.g., tab delimited) into a table; and a mechanism to write
a table out as a ﬂat ﬁle.
Implicit ﬂow of data in these languages is very limited.
There is no global state. User-deﬁned functions (UDFs) are
restricted to using only their input parameters columns, and
their output is restricted only to the output column(s). UDFs
cannot directly access the data store. Implicit ﬂows due to
WHERE clauses are made explicit by considering the columns
referenced in the clause as input columns. The language
itself does not have data dependent loops. The pre-processor
provides syntactic sugar to write repetitive or conditional code
driven by compile-time macros. The resulting code is straight-
line code that explicitly tracks all ﬂow of data.
Fig. 6 lists the SQL-like Scope source code for an analysis
job we’ll use as a running example in this section. The
ﬁrst statement reads in a ﬂat ﬁle “/adsdata/clicks/20131113” ,
which contains tabular data, into the Clicks variable using the
column schema (GUID, ClientIP) provided. The second line
similarly reads another ﬁle into UserAgents with the given
schema (GUID, UserAgent). The third statement joins the two
tables on the GUID column, retains the rows where the UDF
call MaybeFraud(UserAgent) returns true, computes the output
table containing a single schema column named EncryptedIP
populated with the result of the UDF call Encrypt(ClientIP,
“...”), and binds it to the Suspect variable. The last statement
outputs this table to “/user/alice/output” as a ﬂat ﬁle.
We use a variety of complementary approaches to construct
the ﬁne-grained data ﬂow graph, and label it. We begin by
constructing a coarse-grained data ﬂow graph with InStore and
AccessByRole attributes by analyzing logs. We then use ex-
tensive syntactic analysis of programs to add limited DataType
and UseForPurpose attributes. Next, we use semantic analysis
of programs to replace coarse-grained process and data store
nodes with ﬁne-grained internals. In the process we also use
static data ﬂow analysis to expand the coverage of DataType
attributes. Finally, we identify a small set (few hundred)
of bottleneck nodes that, if veriﬁed manually, allows us to
increase the conﬁdence score of the majority of the nodes in
the graph. We describe each approach in detail.
1) Log Analysis: Inferring data ﬂows at a coarse-granularity
(job, ﬁle, user) is trivial given a log that contains all jobs that
were run on the cluster, all the ﬁles the job accessed for read
and write, and all users that downloaded (or uploaded) ﬁles
from (to) the cluster. If this log is exhaustive and updated
regularly, the corresponding GROK is also exhaustive and up-
to-date, satisfying our ﬁrst design goal.
In our deployment we use one such log to bootstrap the
coarse-grained data ﬂow graph. We also use this log to label
ﬁle nodes with the InStore attribute, and entity nodes (and
job nodes run by a user) with the AccessByRole attribute. We
associate a high conﬁdence score with these labels since the
corresponding information is tracked explicitly; e.g., mapping
between data store names and directories is created by the
cluster admin, and as mentioned, only authenticated users
can run jobs or access data. Lastly, since our org-hierarchy
usually reﬂects the functional hierarchy (i.e., jobs for purpose
AbuseDetect are run (only) by the AbuseTeam and vice versa),
we associate a UseForPurpose attribute for each job based
on the role of the user running the job. We associate a low
conﬁdence value for UseForPurpose attributes since it is based
on a heuristic.
Fig. 7 illustrates the data ﬂow information we glean just
through log analysis. The data ﬂow information includes the
ﬁle and job nodes and the edges between them, and the non-
DataType attributes attached to these nodes. We next discuss
how we label DataType attributes.
2) Program Analysis (Syntactic): A scalable way of la-
beling nodes with the DataType attribute is to syntactically
analyze the source code of the job that read or wrote data.
335
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:58:21 UTC from IEEE Xplore.  Restrictions apply. 
By syntactic analysis we mean inferring DataType attributes
for job nodes based on the identiﬁers (e.g., tuple ﬁeld names,
column names) used in the source code to refer to data.
Good coding practices enforced rigorously in engineering
teams through code reviews, variable naming conventions, etc.
require the developer to use comprehensible variable names in
their programs. We use a set of regular expressions to infer a
limited set of policy datatypes (and sometimes typestate) from
identiﬁers in the source code. As before, we associate a low
conﬁdence to such inferences.
From our example in Fig. 6, we would extract the iden-
tiﬁer names Clicks, GUID, ClientIP, UserAgents, UserAgent,
EncryptedIP. Using regular expression patterns (see Section V)
we may associate the DataType labels IPAddress with ClientIP,
IPAddress:Encrypted with EncryptedIP, and UniqueID with
GUID. All with low conﬁdence.
Using regular expressions to label identiﬁers is a heuristic
borne out of the necessity of bootstrapping GROK without
access to the underlying data, without requiring developer
effort, and in an environment where variable names, while
comprehensible to humans, are not standardized; nevertheless
it is a heuristic. Fortunately, a small set of patterns (3200)
curated manually in a one-time effort allows us to label tens
of millions of schema elements daily for which we would oth-
erwise have no information. Having bootstrapped the GROK,
we discuss in Section V how we reduce our dependence on
this bootstrapping approach using highly targeted developer
annotations going forward.
3) Program Analysis (Semantic): Next we leverage pro-
gram semantics to reﬁne coarse-grained ﬁle nodes to a collec-
tion of column nodes for that ﬁle, and reﬁne coarse-grained
job nodes to a sub-graph of nodes over the columns in the
sub-expressions in the job.
a) File to schema reﬁnement: Given the language seman-
tics to read/write ﬁles into/from tables, we infer the columns
in the ﬁle from the column names in the table. By applying the
syntactic technique above on the column names, we ascribe
low-conﬁdence DataType labels to these columns. We then
reﬁne the ﬁle node in the catalog with the inferred columns,
and update the edges in the graph so there is an edge only
from the columns read to the job (or from the job to columns
written).
b) Job to expressions, expressions to columns reﬁne-
ment: We reﬁne a job node by including a node for each
expression in the job. We then reﬁne each expression node
into a collection of columns, and analyze the source code
to identify the other columns that are used to compute the
current column and add the corresponding edges. We make
conservative assumptions for UDFs — we conservatively add
edges from all inputs to the output. The sub-graph representing
the job reﬂects a conservative data ﬂow through all columns
in all sub-expressions in the job.
This reﬁnement step is illustrated in Fig. 8. The ﬁle nodes
have been replaced with multiple column nodes, and the job
node has been replaced with a sub-graph of columns in the
three expressions (from Fig. 6). The edges track data from
which column (ﬁle or expression) ﬂows into which column.
Finally, we apply the syntactic analysis for each column to
label it with low conﬁdence DataType attribute values.
Note that output columns from one job is another job’s
input columns. Thus, this semantic analysis step allows us
to construct a complete data ﬂow graph at the granularity of
columns that tracks the ﬂow of all data across all jobs and all
ﬁles in the big data system.
E. Data Flow Analysis
Next, we perform data ﬂow analysis over the entire graph.
We do this by copying the DataType attribute on one node
to all nodes that data ﬂows to (as long as the destination
doesn’t already have a higher conﬁdence attribute). If the
destination already has an attribute with the same conﬁdence
value, we replace it with the lattice join of the two attributes.
If the data ﬂow is through a UDF, we look for patterns in the
UDF name to infer if the UDF modiﬁes the typestate of the
policy datatype (e.g., the UDF Encrypt(...) converts an input
IPAddress to IPAddress:Encrypted); if a typestate transition is
performed we force the conﬁdence value to low.
Assuming our initial labels are correct, we compute the
attributes along any path conservatively. Therefore, after the
the data ﬂow analysis, we know that if an input column with
policy datatype attribute t, and conﬁdence c interferes in state
s with an output column with DataType attribute t′:s′ and
conﬁdence less than c, then we must have that t:s ≤ t′:s′ in
the DataType lattice.
Fortunately, the restrictive programming model helps us
side-step the most common problem with information ﬂow
analysis where everything quickly gets saturated. Recall that
data ﬂows in Scope are very restricted (no global state or
data driven loops) and UDFs are allowed in very speciﬁc
settings and conﬁned to operate only on the input columns and
their results captured only in the output column. As a result,
even with conservative treatment of UDFs (where all inputs
ﬂow to all outputs), DataType labels do not get saturated.
Furthermore, the limited veriﬁcation step below allows us to
add high conﬁdence labels at key points that limit the low
conﬁdence data ﬂows, thus further containing the saturation
effect.
F. Verifying Labels
While the approaches above give us high coverage for
DataType attributes, they are all with low conﬁdence due to
the heuristics involved. Contacting the developer who wrote
a piece of code for ultimate veriﬁcation is usually a time-
consuming process. Using the following greedy algorithm,
we use GROK to minimize the number of developers we
need to contact for veriﬁcation. For each low conﬁdence
connected component in the data dependency graph where the
syntactic analysis labeled at least one column, we identify
all source code ﬁles (including shared code modules) that
contributed a column node to that connected component. We
then invert the mapping to determine the aggregate size of
connected components a given source code ﬁle contributed
336
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:58:21 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8. Fine-grained labeled data ﬂow graph nodes for Fig. 6.
columns to. We then contact the author of the highest-ranked
source code ﬁle, verify and update the DataType attributes
with high conﬁdence and use the data ﬂow analysis to label
the connected components. We then repeat the process for the
remaining low conﬁdence connected components until we hit
diminishing returns. We show during validation (Section VI)
that by contacting only 12 developer teams and having them
verify, on average, 18 nodes each we are able to attribute high
conﬁdence DataType labels to 60% of the data dependency
graph (28 million nodes daily).
V. PRAXIS
We describe in this
implementation of
LEGALEASE and GROK, and lessons learned from deployment
of our tool in practice.
section our
A. Implementation
GROK: Is implemented as two components: a massively-
parallel standalone static semantic analyzer for the Scope
language, and a massively-scalable data ﬂow analyzer. Both
components run on the big data system itself. The semantic
analyzer processes individual jobs from the cluster log into
the nodes and edges in the data dependency graph without
any attributes. This component is stateless, and executed in a
massively parallel manner processing tens of thousands of jobs
per minute (we present scalability numbers in the validation
section). The second component collates all the graph nodes
(for any arbitrary number of days past) at the granularity
desired for checking the privacy policy, performs syntactic
analysis and conservative data ﬂow analysis over the entire
graph, and outputs graph nodes augmented with the DataType
and other attributes. The two components together comprise
6143 lines of C# code, 988 lines of Scope code, and take as
input a 3203 line conﬁguration ﬁle that contains the regular
expression patterns used for the syntactic analysis phase, and
the manual veriﬁcation results from prior runs.
LEGALEASE: The policy checker is implemented in 652
lines of C# code, takes as input the LEGALEASE privacy
policy speciﬁcation (Table V), evaluates it over GROK’s out-
put, and outputs a ranked list of graph nodes for subsequent
manual veriﬁcation. The output is ranked based on the GROK
conﬁdence values for the labels that resulted in the violation.
B. Experience and Lessons Learned
We discuss three lessons we learned during the process of
bootstrapping the complete system. First, deﬁning patterns for
syntactic analysis, while laborious, has a tremendous payoff.
Second, our light-weight solution to checking simple temporal
properties like data retention. And third, our solution to min-
imizing the veriﬁcation effort through developer annotations.
a) Deﬁning patterns for syntactic analysis: To deﬁne
patterns for syntactic analysis, we manually analyzed around
150K unique column and variable names (from a day’s worth
of jobs). We identiﬁed on the order of 40 regular expressions
for roughly as many lattice elements for policy datatypes
(e.g., %email% for columns that might contain an Email),
and on the order of 400 exact matches based on domain
knowledge. We found that these regular expressions had some
false-positives, for instance,
labeling the column emailRe-
sponseRate, a ﬂoating-point value, as an email address. We
enforced type restrictions (available during semantic analysis).
While this helped reduce false-positives, it did not eliminate
them (e.g., column emailProvider , which is a string). We
manually examined the column names to identify obvious
false-positives, and deﬁned a set of around 2500 negative exact
matches (across all policy datatypes). Finally, during the ﬁrst
337
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:58:21 UTC from IEEE Xplore.  Restrictions apply. 
manual veriﬁcation we found cases where the inference was
correct (i.e., the column entityEmail did indeed have email),
but that they were for business listing in publicly crawlable
web data (and that the team followed a naming convention).
We added such conventions we discovered through developer
interactions to the set of negative patterns. Thus in our current
deployment, a column is labeled Email
if it matches any
positive pattern or exact match deﬁned for email, and does
not match any negative pattern or negative exact match.
Overall this process was laborious, taking one person one
full week to construct the GROK conﬁguration ﬁle. Having
spent that one-time effort, however, the 3203 lines in the
conﬁguration ﬁle today label with high precision (based on
verifying a random sample) on the order of millions of graph
nodes daily.
b) Retention and limited temporal properties: While
the big data system offers developers three mechanisms for
ensuring that data is deleted after the retention period elapses,
the underlying log data on which GROK is built gives us
visibility into developers using only one of those mechanisms.
For coverage, we use the data dependency graph to trace back
the origin of any piece of data and when it was ﬁrst seen
in GROK. We automatically compute the day when that data
should be deleted, subtract a two week buffer, and update
GROK setting the typestate to :Expired; any subsequent use
of this near-expiry data is output as a veriﬁcation task item.
Labelling data with :Expired allows the audit team to identify
teams using data that is near-expiry and ensure that the teams
are using one of the other two mechanisms to delete the data
on time.
c) Reducing veriﬁcation time through developer code
annotations: Our current bottleneck is auditor bandwidth since
following up with (even a small number of) developers is time-
consuming. Instead of a manual audit process, we are currently
piloting code annotations that a small number of developers
can add (proactively) to disambiguate the policy datatypes they