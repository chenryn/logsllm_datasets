14 E Integer 2 QWS50R1N Number of instances of QCST
Instrumentation standard header (QWHS)
The format of the QWHS is described in structure qwhs in the C programming language header
file thlqual.SCSQC370(CSQDSMFC), and in assembler macro thlqual.SCSQMACS(CSQDQWHS). It
contains the following key fields that are relevant to channel initiator SMF 116 records:
Table 49. Key fields in the QWHS
Name Length Description
QWHSNDA 1 byte Number of self-defining sections
QWHSSSID 4 bytes Subsystem name
QWHSSMFC 1 bit Indicates whether there are multiple SMF records
containing information for this interval. If this bit
is on, information for this interval is continued in
further SMF records. If this bit is off, this is the last
or only record.
QWHSTIME 8 bytes Local time of the start of the interval in STCK
format
QWHSDURN 8 bytes Duration from the start of the interval to the end of
the interval in STCK format
IBM MQ Monitoring and performance 369
Table 49. Key fields in the QWHS (continued)
Name Length Description
QWHSSTCK 8 bytes End of the interval in UTC in STCK format
Channel accounting data records
Use this topic as a reference for channel accounting data records.
The format of the channel accounting data record is described in assembler macro
thlqual.SCSQMACS(CSQDQCST). The format is also described in the C programming language header
file thlqual.SCSQC370(CSQDSMFC). Note that the field names in C are all in lowercase, for example,
qcst.
The channel accounting data gives you information about the status and statistics of each channel
instance, including:
• Average network time (qcstntav)
• Average time on exit (qcstetav)
• Channel batch data limit (qcstcbdl)
• Channel batch interval (qcstcbit)
• Channel batch size (qcstcbsz)
• Channel dispatcher number (qcstdspn)
• Channel disposition (qcstchdp)
• Channel name (qcstchnm)
• Channel state (qcstchst)
• Channel started time (qcststrt)
• Channel status collected time (qcstcltm)
• Channel stopped time (qcstludt)
• Channel type (qcstchty)
• Common name (CN) from SSLCERTI (qcstslcn)
• Compression rate (qcstcpra)
• Connection name (qcstcnnm)
• Current shared conversations (qcstcscv)
• DNS resolution time (qcstdnrt)
• Effective value of STATCHL parameter (qcststcl)
• Last message time (qcstlmst)
• Maximum network time (qcstntmx)
• Maximum time on exit (qcstetmx)
• Minimum network time (qcstntmn)
• Minimum time on exit (qcstetmn)
• Name of the remote queue manager or application (qcstrqmn)
• Number of batches (qcstbatc)
• Number of bytes for message data (qcstnbyt)
• Number of bytes for persistent message data (qcstnpby)
• Number of bytes received for both message data and control information (qcstbyrc)
• Number of bytes sent for both message data and control information (qcstbyst)
• Number of full batches (qcstfuba)
• Number of messages, or number of MQI calls (qcstnmsg)
370 Monitoring and Performance for IBM MQ
• Number of persistent messages (qcstnpmg)
• Number of put retries (qcstptrc)
• Number of transmission queue becoming empty (qcstqetc)
• Number of transmission buffers received ( qcstbfrc )
• Number of transmission buffers sent (qcstbfst)
• Serial number from SSLPEER (qcstslsn)
• SSL CipherSpec (zero means TLS not used) (qcstslcs)
• The date and time of maximum network time (qcstntdt)
• The date and time of maximum time on exit (qcstetdt)
Note, that for the channel accounting field qcstetmn (Minimum time on exit) and qcstntmn (Minimum
network time) these two fields will be initialized to the hexadecimal value of 8FFFFFFF when unused.
You can use this information to see the throughput of a channel, if the actual batches are approaching the
limit, the latency of the network, information about the remote end, performance of user exit, and so on.
Here is an example of the channel accounting data which has been formatted with IBM MQ SupportPac
MP1B.
The fields available are based on the display channel status command (DIS CHS) and channel statistics by
IBM MQ on platforms except z/OS, with some additional fields.
The data and time of the start and end of the record in local time, and the duration
SMF interval start 2014/03/26,02:30:00
SMF interval end 2014/03/26,02:45:00
SMF interval duration 899.997759 seconds
Information about the channel
Connection name 9.20.4.159
Channel disp PRIVATE
Channel type RECEIVER
Channel status CLOSING
Channel STATCHL HIGH
Start date & time 2014/03/26,02:44:58
Channel status collect time 2014/03/26,02:45:00
Last status changed 1900/01/01,00:00:00
Last msg time 2014/03/26,02:44:59
Batch size 50
Messages/batch 3.3
Number of messages 1,102
Number of persistent messages 1,102
Number of batches 335
Number of full batches 0
Number of partial batches 335
Buffers sent 337
Buffers received 1,272
Message data 5,038,344 4 MB
Persistent message data 5,038,344 4 MB
Non persistent message data 0 0 B
Total bytes sent 9,852 9 KB
Total bytes received 5,043,520 4 MB
Bytes received/Batch 15,055 14 KB
Bytes sent/Batch 29 29 B
Batches/Second 1
Bytes received/message 4,576 4 KB
Bytes sent/message 8 8 B
Bytes received/second 28,019 27 KB/sec
Bytes sent/second 54 54 B/sec
Compression rate 0
The name of the queue manager at the remote end of the connection
Remote qmgr/app MQPH
Put retry count 0
IBM MQ Monitoring and performance 371
Tuning your IBM MQ network
Use the tuning tips in this section to help improve the performance of your queue manager network.
Tuning client and server connection channels
The default setting for SHARECNV is 10, which allows up to 10 client conversations for each channel
instance. However, using a different number of shared conversations can be better for performance.
If you do not need shared conversations, or are using a distributed server, set SHARECNV to 1. If you
have existing client applications that do not run correctly when you set SHARECNV to 1 or greater, set
SHARECNV to 0.
About this task
For some configurations, using shared conversations brings significant benefits. However, for distributed
servers, processing messages on channels that use the default configuration of 10 shared conversations
is on average 15% slower than on channels that do not use shared conversations. On an MQI channel
instance that is sharing conversations, all of the conversations on a socket are received by the same
thread. If the conversations sharing a socket are all busy, the conversational threads contend with one
another to use the receiving thread. The contention causes delays, and in this situation using a smaller
number of shared conversations is better.
You use the SHARECNV parameter to specify the maximum number of conversations to be shared over a
particular TCP/IP client channel instance. For details of all possible values, see Supported IBM MQ client:
Default behavior of client-connection and server-connection channels.
If you set SHARECNV to 1 or greater, you enable the following performance enhancements:
• Bi-directional heartbeats
• Administrator stop-quiesce
• Read-ahead
• Asynchronous-consume by client applications
If you do not need shared conversations, these two settings give best performance:
• SHARECNV(1).
• SHARECNV(0).
Note: If the client-connection SHARECNV value does not match the server-connection SHARECNV value,
then the lowest value is used.
To optimize performance for a given channel instance, complete any of the following steps.
Procedure
• Monitor channels that use the default SHARECNV value of 10.
The default setting of SHARECNV(10) works well in many scenarios, but might not be the optimum
setting for a given channel instance. For example, for distributed servers, processing messages on
channels that use this setting is on average 15% slower than on channels that do not use shared
conversations.
To ensure that the default setting is appropriate for a given channel instance, monitor how the channel
performs with this setting.
• Set a SHARECNV value of 2 or more.
You can set SHARECNV(2) to SHARECNV(999999999). To ensure that the setting you choose is
appropriate for a given channel instance, monitor how the channel performs with the new setting.
• Set a SHARECNV value of 1.
372 Monitoring and Performance for IBM MQ
If you do not need shared conversations, use this setting whenever possible. It eliminates contention
to use the receiving thread, and your client applications can take advantage of the performance
enhancements described in the "about this task" section.
With this setting, distributed server performance is significantly improved. The performance
improvements apply to client applications that issue non read ahead synchronous get wait calls; for
example C client MQGET wait calls. When these client applications are connected, the distributed
server uses less threads and less memory and the throughput is increased.
If a server has clients connected to it that are sharing conversations over a socket, and you decrease
the shared conversations setting from SHARECNV(10) to SHARECNV(1), this has the following effects:
– Increased socket usage on the server.
– Increased channel instances on the server.
In this case, you might also choose to increase the settings for MaxChannels and
MaxActiveChannels.
Note: You can also set the MQCONNX option, MQCNO_NO_CONV_SHARING and connect the application
to a channel with SHARECNV set to a value greater than 1. The result is the same as connecting the
application to a channel with SHARECNV set to 1.
• Set a SHARECNV value of 0.
The channel instance behaves exactly as if it was an IBM WebSphere MQ 6.0 server or client
connection channel. You do not get shared conversations, or the performance enhancements that
are available when you set SHARECNV to 1 or greater. Use a value of 0 only if you have existing client
applications that do not run correctly when you set SHARECNV to 1 or greater.
Related concepts
Supported IBM MQ client: Default behavior of client-connection and server-connection channels
Tuning distributed publish/subscribe networks
Use the tuning tips in this section to help improve the performance of your IBM MQ distributed publish/
subscribe clusters and hierarchies.
Related concepts
“Monitoring clusters” on page 311
Within a cluster you can monitor application messages, control messages, and logs. There are special
monitoring considerations when the cluster load balances between two or more instances of a queue.
Direct routed publish/subscribe cluster performance
In direct routed publish/subscribe clusters, information such as clustered topics and proxy subscriptions
is pushed to all members of the cluster, irrespective of whether all cluster queue managers are actively
participating in publish/subscribe messaging. This process can create a significant additional load on the
system. To reduce the effect of cluster management on performance you can perform updates at off-peak
times, define a much smaller subset of queue managers involved in publish/subscribe and make that an
"overlapping" cluster, or switch to using topic host routing.
There are two sources of workload on a queue manager in a publish/subscribe cluster:
• Directly handling messages for application programs.
• Handling messages and channels needed to manage the cluster.
In a typical point-to-point cluster, the cluster system workload is largely limited to information explicitly
requested by members of the cluster as required. Therefore in anything other than a very large point-to-
point cluster, for example one which contains thousands of queue managers, you can largely discount
the performance effect of managing the cluster. However, in a direct routed publish/subscribe cluster,
information such as clustered topics, queue manager membership and proxy subscriptions is pushed to
all members of the cluster, irrespective of whether all cluster queue managers are actively participating in
publish/subscribe messaging. This can create a significant additional load on the system. Therefore you
IBM MQ Monitoring and performance 373
need to consider the effect of cluster management on queue manager performance, both in its timing, and
its size.
Performance characteristics of direct routed clusters
Compare a point-to-point cluster with a direct routed publish/subscribe cluster in respect of the core
management tasks.
First, a point to point cluster:
1.When a new cluster queue is defined, the destination information is pushed to the full repository
queue managers, and only sent to other cluster members when they first reference a cluster queue
(for example, when an application attempts to open it). This information is then cached locally by
the queue manager to remove the need to remotely retrieve the information each time the queue is
accessed.
2.Adding a queue manager to a cluster does not directly affect the load on other queue managers.
Information about the new queue manager is pushed to the full repositories, but channels to the new
queue manager from other queue managers in the cluster are only created and started when traffic
begins to flow to or from the new queue manager.
In summary, the load on a queue manager in a point-to-point cluster is related to the message traffic it
handles for application programs and is not directly related to the size of the cluster.
Second, a direct routed publish/subscribe cluster:
1.When a new cluster topic is defined, the information is pushed to the full repository queue managers,
and from there directly to all members of the cluster, causing channels to be started to each member
of the cluster from the full repositories if not already started. If this is the first direct clustered topic,
each queue manager member is sent information about all other queue manager members in the
cluster.
2.When a subscription is created to a cluster topic on a new topic string, the information is pushed
directly from that queue manager to all other members of the cluster immediately, causing channels to
be started to each member of the cluster from that queue manager if not already started.
3.When a new queue manager joins an existing cluster, information about all clustered topics (and all
queue manager members if a direct cluster topic is defined) is pushed to the new queue manager
from the full repository queue managers. The new queue manager then synchronizes knowledge of all
subscriptions to cluster topics in the cluster with all members of the cluster.
In summary, cluster management load at any queue manager in a direct routed publish/subscribe cluster
grows with the number of queue managers, clustered topics, and changes to subscriptions on different
topic strings within the cluster, irrespective of the local use of those cluster topics on each queue
manager.
In a large cluster, or one where the rate of change of subscriptions is high, this level of cluster
management can be a significant overhead across all queue managers.
Reducing the effect of direct routed publish/subscribe on performance
To reduce the effect of cluster management on the performance of a direct routed publish/subscribe
cluster, consider the following options:
• Perform cluster, topic, and subscription updates at off-peak times of the day.
• Define a much smaller subset of queue managers involved in publish/subscribe, and make that an
"overlapping" cluster. This cluster is then the cluster where cluster topics are defined. Although some
queue managers are now in two clusters, the overall effect of publish/subscribe is reduced:
– The size of the publish/subscribe cluster is smaller.
– Queue managers not in the publish/subscribe cluster are much less affected by cluster management
traffic.
374 Monitoring and Performance for IBM MQ
If the previous options do not adequately resolve your performance issues, consider using a topic host
routed publish/subscribe cluster instead. For a detailed comparison of direct routing and topic host
routing in publish/subscribe clusters, see Designing publish/subscribe clusters.
Related concepts
Topic host routed publish/subscribe cluster performance
A topic host routed publish/subscribe cluster gives you precise control over which queue managers host
each topic. These topic hosts become the routing queue managers for that branch of the topic tree.
Moreover, queue managers without subscriptions or publishers have no need to connect with the topic
hosts. This configuration can significantly reduce the number of connections between queue managers in
the cluster, and the amount of information that is passed between queue managers.
Balancing producers and consumers in publish/subscribe networks
An important concept in asynchronous messaging performance is balance. Unless message consumers
are balanced with message producers, there is the danger that a backlog of unconsumed messages might
build up and seriously affect the performance of multiple applications.
Subscription performance in publish/subscribe networks
Distributed publish/subscribe in IBM MQ works by propagating knowledge of where subscriptions to
different topic strings have been created in the queue manager network. This enables the queue manager
on which a message is published to identify which other queue managers require a copy of the published
message, to match their subscriptions.
Topic host routed publish/subscribe cluster performance
A topic host routed publish/subscribe cluster gives you precise control over which queue managers host
each topic. These topic hosts become the routing queue managers for that branch of the topic tree.
Moreover, queue managers without subscriptions or publishers have no need to connect with the topic
hosts. This configuration can significantly reduce the number of connections between queue managers in
the cluster, and the amount of information that is passed between queue managers.
A topic host routed publish/subscribe cluster behaves as follows:
• Topics are manually defined on individual topic host queue managers in the cluster.
• When a subscription is made on a cluster queue manager, proxy subscriptions are created only on the
topic hosts.
• When an application publishes information to a topic, the receiving queue manager forwards the
publication to a queue manager that hosts the topic. The topic host then sends the publication to
all queue managers in the cluster that have valid subscriptions to the topic.
For a more detailed introduction to topic host routing, see Topic host routing in clusters.
For many configurations, topic host routing is a more appropriate topology than direct routing because it
provides the following benefits:
• Improved scalability of larger clusters. Only the topic host queue managers need to be able to connect
to all other queue managers in the cluster. Therefore, there are fewer channels between queue
managers, and there is less inter-queue manager publish/subscribe administrative traffic than for direct
routing. When subscriptions change on a queue manager, only the topic host queue managers need to
be informed.
• More control over the physical configuration. With direct routing, all queue managers assume all roles,
and therefore all need to be equally capable. With topic host routing, you explicitly choose the topic
host queue managers. Therefore, you can ensure that those queue managers are running on adequate
equipment, and you can use less powerful systems for the other queue managers.
However, topic host routing also imposes certain constraints upon your system:
• System configuration and maintenance require more planning than for direct routing. You need to
decide which points to cluster in the topic tree, and the location of the topic definitions in the cluster.
• Just as for direct routed topics, when a new topic host routed topic is defined, the information is pushed
to the full repository queue managers, and from there direct to all members of the cluster. This event
IBM MQ Monitoring and performance 375
causes channels to be started to each member of the cluster from the full repositories if not already
started.
• Publications are always sent to a host queue manager from a non-host queue manager, even if there are
no subscriptions in the cluster. Therefore, you should use routed topics when subscriptions are typically
expected to exist, or when the overhead of global connectivity and knowledge is greater than the risk of
extra publication traffic.
• Messages that are published on non-host queue managers do not go direct to the queue manager that
hosts the subscription, they are always routed through a topic host queue manager. This approach can
increase the total overhead to the cluster, and increase message latency and reduce performance.
Note: For certain configurations, you can usefully remove this constraint as described in Topic host
routing using centralized publishers or subscribers.
• Using a single topic host queue manager introduces a single point of failure for all messages that
are published to a topic. You can remove this single point of failure by defining multiple topic hosts.
However, having multiple hosts affects the order of published messages as received by subscriptions.
• Extra message load is incurred by topic host queue managers, because publication traffic from multiple
queue managers needs to be processed by them. This load can be lessened: Either use multiple topic
hosts for a single topic (in which case message ordering is not maintained), or use different queue
managers to host routed topics for different branches of the topic tree.
Topic host routing with centralized publishers or subscribers
To remove the extra "hop" incurred when publications are always routed to subscriptions through a topic
host queue manager, configure the publishers or the subscriptions on the same queue manager that hosts
the topic. This approach brings maximum performance benefits in the following two cases:
• Topics with many publishers and few subscriptions. In this case, host the subscriptions on the topic
host queue manager.
• Topics with few publishers and many subscriptions. In this case, host the publishers on the topic host
queue manager.
The following figure shows a topic host queue manager that also hosts the subscriptions. This approach
removes the extra "hop" between the publisher and the subscriber, and reduces unnecessary sharing of
subscription knowledge across all members of the cluster:
376 Monitoring and Performance for IBM MQ
Figure 27. Hosting subscriptions on a topic host queue manager
The following figure shows a topic host queue manager that also hosts the publishers. This approach
removes the extra "hop" between the publisher and the subscriber, and reduces unnecessary sharing of
subscription knowledge across all members of the cluster:
IBM MQ Monitoring and performance 377
Figure 28. Hosting publications on a topic host queue manager
Related concepts
Direct routed publish/subscribe cluster performance
In direct routed publish/subscribe clusters, information such as clustered topics and proxy subscriptions
is pushed to all members of the cluster, irrespective of whether all cluster queue managers are actively
participating in publish/subscribe messaging. This process can create a significant additional load on the
system. To reduce the effect of cluster management on performance you can perform updates at off-peak
times, define a much smaller subset of queue managers involved in publish/subscribe and make that an
"overlapping" cluster, or switch to using topic host routing.
Balancing producers and consumers in publish/subscribe networks
An important concept in asynchronous messaging performance is balance. Unless message consumers
are balanced with message producers, there is the danger that a backlog of unconsumed messages might
build up and seriously affect the performance of multiple applications.
Subscription performance in publish/subscribe networks
Distributed publish/subscribe in IBM MQ works by propagating knowledge of where subscriptions to