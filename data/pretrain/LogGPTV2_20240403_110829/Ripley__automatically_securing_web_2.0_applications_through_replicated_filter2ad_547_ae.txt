can be blocked until the time measured on the client is delivered to
continue the computation.
Accessing third-party servers. A systematic way to deal with
client-side code accessing third-party servers is to require that these
accesses be tunneled through the server. For servers in a different
domain, this is necessary anyway, because of the same origin policy
in JavaScript. This allows for easy centralized access to outside
data, for both the replica and the client-side code. Because calls to
external services are performed only once, this also deals with the
issue of non-idempotent calls with side-effects. We also assume
that the client-side code does not interact with other JavaScript,
because replicating that JavaScript is far from easy. In other words,
all client-side JavaScript code is generated by Volta.
Scheduling issues. Unlike many other languages, JavaScript pro-
vides a single-threaded execution environment. However, there are
still opportunities for non-determinism caused by the use of timers
(functions setTimeout and setInterval), which allow schedul-
ing a piece of code to run on a timer. Other projects have focused
on deterministic replay of multi-threaded software [6]. While log-
ging and replay of event processing order is possible, for simplicity,
we currently disallow timers in RIPLEY.
Ofﬂine storage. Our emulator supports a cookie-based data store.
The emulator faithfully replicates the data stored in the browser.
While we have not experimented with other forms of ofﬂine store
such as Gears [14], we believe it can be supported similarly. RIP-
LEY provides a nice model for ofﬂine execution: both events and
RPCs are buffered up before the client is reconnected, at which
point the entire ofﬂine client-side execution is replayed within the
replica. Ofﬂine persistent store can also be supported by RIPLEY
if the user does not have privacy objections to sending their data to
the server.
Browser Enhancements.
In fact, a set of small changes to the
JavaScript interpreter would help us secure event capture and de-
livery and would also address the sources of non-determinism dis-
cussed above. In particular, instrumenting Math.Random and Date
routines as well as event handlers as described in Section 3.2 in the
interpreter is the easiest and most systematic way to treat these is-
sues that ensures that malicious JavaScript code co-existing within
the same page is unable to gain access to this data. Event capture
outside of JavaScript should result in a lower overhead.
Plugins an other client-side code. RIPLEY is not designed to in-
teroperate with plugins that might be running within the page. In
fact, the RIPLEY model discussed in this paper is targeting stand-
alone deployment of a RIPLEY application witnin an HTML frame;
182allowing other code to co-exist within the same frame as part of a
mash-up, for example, can easily compromise agreement with the
replica. This is because co-located script can change both global
data structures as well as code on the client-side.
5.2 Performance and Scalability
The RIPLEY model enables the following interesting optimiza-
tion opportunities.
0-latency RPCs. An advantage of the RIPLEY architecture is
that, once computed, RPC results can be actively pushed to the
client [43]. This way, when the RPC is ﬁnally issued on the client,
its result will already be available, leading to 0-latency RPCs. This
form of pre-fetching demonstrates that not only does RIPLEY make
the application more secure, in many cases it can also make it more
responsive, especially for CPU-intensive workloads.
MAC-ing RPCs. To further reduce the network overhead we may
send MACs (message authentication codes) of RPCs m0 instead of
their actual values.
Deployment strategy. RIPLEY meshes nicely with the traditional
load-balancing approach to deployment of large-scale Web 2.0 ap-
plications. In particular, a load balancer could be used to repeatedly
direct the same user to the server where both its replica and the cor-
responding server threads run. Currently, this functionality is im-
plemented in the RIPLEY checker, which looks up the appropriate
APPDOMAIN for a user session. Moreover, to save memory, both
the server thread and the replica can be serialized on high server
load for long-running sessions and then brought back from disk.
Dependency analysis. An important observation is that the entire
client-side code base does not have to be included in the replica. In
particular, display code does not need to be executed on the server
because the replica is essentially “headless” — there is no user to
see the GUI. To further reduce the amount of code the replica must
run, we can use a slicing analysis [51] to only include a portion of
the client-side code that contribute to values included into RPCs.
This is left as future work.
6. RELATED WORK
The security of the Web infrastructure has been a subject of much
previous work. The various approaches to solving the problem can
be categorized roughly along four lines of inquiry. A sizable body
of literature has focused on the static analysis of web applications
using techniques such as taint-checking. Runtime monitoring of
web applications has also proved to be effective. Others have ad-
dressed the problem at a higher level by developing a cleaner and
more secure programming model, often erasing the boundaries be-
tween various tiers. Recent work has also developed techniques to
protect against untrusted clients in a networked environment. Fi-
nally, the idea of security through replication has also been well
studied in earlier work. We elaborate further on each of these.
6.1 Analysis and Monitoring
There has been a great deal of interest in static and runtime pro-
tection techniques to improve the security posture of traditional
“Web 1.0” applications [23, 26, 31, 34, 52]. Static analysis allows
the developer to avoid issues such as cross-site scripting prior to
deployment. Runtime analysis, on the other hand, allows exploit
prevention and recovery.
The WebSSARI project pioneered this line of research. Web-
SSARI uses combined unsound static and dynamic analysis in the
context of analyzing PHP programs [23]. Several projects subse-
quent to WebSSARI improve on the quality of static analysis for
PHP [26, 52]. The Grifﬁn project proposes a scalable and precise
sound static and runtime analysis techniques for ﬁnding security
vulnerabilities in large Java applications [31, 34]. Static analysis is
also used to drastically reduce the runtime overhead in most cases.
The runtime system allows vulnerability recovery by applying user-
provided sanitizers on execution paths that lack them. Several other
runtime systems for taint tracking have been proposed, including
Haldar et al. for Java [17] and Pietraszek et al. [41] and Nguyen-
Tuong et al. for PHP [40]. All these techniques can be used in
conjunction with RIPLEY, by applying them on the complete code
base that includes the client and the server subprograms.
While server-side enforcement mechanisms are applicable for
traditional web applications that are composed entirely on the
server side [26, 31, 52], Web 2.0 applications that make use of
AJAX often fetch both data and JavaScript code from many
sources, with the entire ﬁnal HTML only available within the
browser, making runtime client-side enforcement a natural choice.
Recently, there has been a number of proposals for runtime en-
forcement mechanisms to ensure that security properties of interest
hold for rich-client applications executing within the browser [10,
22, 25, 56]. Erlingsson et al. make an end-to-end argument for the
client-side enforcement of security policies that apply to client be-
havior [10]. Their proposed mechanisms use server-speciﬁed, pro-
grammatic security policies that allow for ﬂexible client-side en-
forcement, even to the point of runtime data tainting. Unlike RIP-
LEY, their technique can enforce some necessary, but not sufﬁcient
conditions for establishing distributed application integrity.
Guha et al.
[16] propose an analysis to construct a model of
valid client-side behavior and a security monitor that rejects client
requests that fall outside that model. This approach provides a form
of partial integrity, defeating important classes of XSS and CSRF
attacks. While it is able to impose a validity requirement on client-
side requests, it is unclear how well this approach applies to arbi-
trary data structures exchanged between the client and the server,
when data integrity is the problem. Also, this approach may suffer
from false positives. In contrast, RIPLEY’s approach is admittedly
much more blunt: it does not try to form an approximation of valid
behavior, it just runs the program and compares the end-result, pro-
viding a sufﬁcient condition for integrity.
6.2 Web Programming Models
Tier-splitting has been proposed in settings other than Volta as a
way to program distributed web applications. Popular systems in
this space include Links [11], Hop [46], Hilda [54], etc. To the best
of our knowledge, RIPLEY is the ﬁrst realistic security solution for
these kinds of frameworks.
BASS is a recent attempt to build security into a declarative
high-level web programming model, working on the observation
that security issues are often orthogonal to the main web applica-
tion logic [55]. It enables the programmer to specify the business
logic of the application without needing to write the security related
logic. Abstractions for common operations, such as form input, are
baked into the model. Secure coding practices that prevent com-
mon attacks such as CSRF, XSS and session ﬁxation are applied by
the language compiler. A prototype implementation of the transla-
tion exists, but no applications seem to have been written in BASS.
RIPLEY, on the other hand, is a realistic programming model inte-
grated with a full-ﬂedged Volta compiler. Instead of protecting only
against common exploits, RIPLEY defends against any client attack
that attempts to compromise application integrity. BASS does not
deal with client-side scripting at all, whereas RIPLEY works in a
model where a signiﬁcant portion of the application is run on the
client for enhanced responsiveness.
1836.3 Untrusted Clients
7. CONCLUSIONS
Protection against untrusted clients and eavesdropping over the
network has received much attention, especially in the context of
online gaming [21, 53]. In a distributed online game, part of the
application workload is typically delegated to the clients and the
server keeps track of only an abstract state of the game environ-
ment. As a result, the game is rendered vulnerable to malicious
clients compromising the physical and logical rules governing the
simulation in the game. Hacking popular online games is a ﬁnan-
cially viable undertaking as game “items” can be converted to real-
world currency or sold on eBay.
Jha et al. propose a solution to the distributed online game in-
tegrity problem by performing random audits of the client state
verifying that the client has not manipulated its state in violation
of the semantic rules of the game [24]. Our approach, in contrast,
provides a non-probabilistic guarantee of integrity at a potentially
higher cost. In particular, if the client-side computation is highly
CPU-intensive, as ray-tracing in games tends to be, despite reply-
ing on an emulator and running in a faster .NET environment, with
sufﬁciently many connected clients, the RIPLEY server might even-
tually become overwhelmed.
6.4 Replication & Replay for Security
Replication is a well-known way to increase security assurance,
previously studied in ﬁle systems and replicated state machines [4,
32, 44, 49]. The work closest to ours is that of Zheng et al. [57–
59]. In many ways a precursor to Swift [7, 8], this work focuses
on splitting programs while conforming to a set of integrity and
privacy policies. The latter are addressed by computing in the hash
space, not unlike our Quiz application described in Appendix A.
A high-level difference in philosophy with our work is that we
avoid using annotations, believing that having to write annotations
places an undue burden on the developer. For instance, one such ap-
proach requires about 20–30% of program lines to be annotated [8].
This makes such techniques challenging to retroﬁt into existing un-
annotated code. Instead, we “blindly” replicate the entire client-
side portion of the program on the trusted server tier, using runtime
optimizations to make this approach scalable. In doing so, we trade
increased developer productivity for runtime overhead.
Beyond our main focus on computational integrity violations
caused by malicious users, RIPLEY may also address the situation
of a benign user placed in a “malicious environment”. This could
be a propagating JavaScript worm, such as Samy [50]. Additional
RPCs issued by the worm on behalf of the user will not be gen-
erated by the replica as it runs in .NET. This discrepancy will be
spotted by RIPLEY, disallowing worm propagation. Replication
will also prevent cross-site scripting attacks that result in server
RPCs, because the extra injected JavaScript code is ignored by the
replica executing in .NET.
Revirt [9] is a system logger that records events in the operating
system so that they can be replayed later for post-mortem analysis
of attacks. The OS runs on a virtual machine and the logger runs
below the level of the virtual machine, thereby making it resistant
to kernel attacks. The logger is also able to replay the complete
instruction-by-instruction execution of the virtual machine so that
any queries about its execution can be answered to detect anom-
alous behavior. Capo [38] is an advancement on the techniques
of Revirt, enabling various styles of logging of systems on multi-
processor machines. RIPLEY also allows a complete replay of the
execution of untrusted code and is guaranteed to detect anomalous
behavior. The RIPLEY logger now runs as JavaScript, but we could
imagine moving it to the browser to enhance the integrity of log-
ging, as done by Revirt.
This paper presents RIPLEY, the ﬁrst fully automated approach
to ensuring integrity of distributed web applications. To demon-
strate the efﬁcacy of RIPLEY in practice, we have applied RIPLEY
to ﬁve realistic AJAX applications. The performance overhead in-
troduced by RIPLEY was minimal, in terms of CPU, memory, and
network overhead. While we have demonstrated our ideas in the
context of the Volta compiler, the ideas of code replication can be
easily extended to other runtime environments such as Silverlight
or server-side JavaScript.
We believe that in the future the approach pioneered by RIPLEY
may become an important building block of trustworthy distributed
applications. Our work closely follows the secure-by-construction
philosophy of building application software. In particular, we en-
vision RIPLEY becoming an integral part of the next generation of
application servers. All the application developer will have to do to
obtain the integrity-preservation beneﬁts of RIPLEY, is to “drop”
their web application into the application server, with automatic
replication becoming part of the deployment process.
Acknowledgements
We thank Jeffrey Van Gogh and Danny Van Velzen for helping us
extend the Volta system, and Erik Meijer for his continued support
for research efforts based on Volta. We also thank Úlfar Erlings-
son, Andrew C. Myers, David Evans and Cedric Fournet for their
detailed feedback on this paper. The comments by the CCS review-
ers were also very helpful in shaping the ﬁnal version of the paper.
8. REFERENCES
[1] J. Allspaw. The Art of Capacity Planning: Scaling Web Resources. O’Reilly
Media, 2008.
[2] C. Anley. Advanced SQL injection in SQL server applications, 2002.
[3] J. Boutelle. Bandwidth savings with AJAX.
http://www.jonathanboutelle.com/mt/archives/2006/01/
bandwidth_savin.html, Jan. 2006.
[4] M. Castro and B. Liskov. Practical byzantine fault tolerance and proactive
recovery. ACM Transactions on Computer Systems, 20(4):398–461, Nov. 2002.
[5] CGI Security. The cross-site scripting FAQ.
http://www.cgisecurity.net/articles/xss-faq.shtml.
[6] J.-D. Choi and H. Srinivasan. Deterministic replay of Java multithreaded
applications. In Proceedings of the Symposium on Parallel and Distributed
Tools, pages 48–59, Aug. 1998.
[7] S. Chong, J. Liu, A. C. Myers, X. Qi, K. Vikram, L. Zheng, and X. Zheng.
Secure web applications via automatic partitioning. In Proceedings of the
Symposium on Operating Systems Principles, pages 31–44, Oct. 2007.
[8] S. Chong, K. Vikram, and A. C. Myers. Sif: enforcing conﬁdentiality and
integrity in Web applications. In Proceedings of USENIX Security Symposium,
pages 1–16, Aug. 2007.
[9] G. W. Dunlap, S. T. King, S. Cinar, M. A. Basrai, and P. M. Chen. Revirt:
Enabling intrusion analysis through virtual-machine logging and replay. In In
Proceedings of the Symposium on Operating Systems Design and
Implementation, pages 211–224, Dec. 2002.
[10] Ú. Erlingsson, B. Livshits, and Y. Xie. End-to-end Web application security. In
Proceedings of the USENIX Workshop on Hot topics in operating systems,
pages 1–6, May 2007.
[11] Ezra Cooper and Sam Lindley and Philip Wadler and Jeremy Yallop. Links: