alert is raised for each of them;
– then, the comparison algorithm is applied on the set of answers that have been
collected.
COTS Diversity Based Intrusion Detection and Application to Web Servers
53
Algorithm 1. Detection Algorithm by Comparison of Web server Answers
Data: n the number of web servers and R = {Ri|1 ≤ i ≤ n} the set of responses from the
web servers to a given request Req, D the set of known design differences for the
used web servers, headers the table such that headers[0] = ’Content-Length’,
headers[1] = ’Content-Type’, headers[2] = ’Last-Modiﬁed’
if (Req, R) ∈ D then
/* Handle design differences that are not vulnerabilities */
modify R to mask the differences
∀(Rl, Rk) ∈ C2
∀(i, j) ∈ [1,m]2, i (cid:9)= j,Ci ∩C j = /0, m the number or partitions
i , Rl.statusCode = Rk.statusCode
Partition R in Ci,
1 ≤ i ≤ m ≤ n
if ∀i Card(Ci) 
directory request without a final ’/’
^.*[^/]$
^.*$
apache
301
text/html
iis
thttpd
302
text/html
no
iis
no
no
Fig. 3. Directory Rule: The tags regexpURI (deﬁnition of the URL) and regexpMethod (deﬁnition
of the HTTP method) deﬁne the server entries using regular expressions. The server tags deﬁne
the outputs of the servers Si, i.e., the expected outputs produced by the several servers when they
take one of the values deﬁned as entry. The other tags : compareContent, response, warn and alert
deﬁne the set of actions that must be performed by the IDS in order to mask the difference or
generate an alert.
COTS Diversity Based Intrusion Detection and Application to Web Servers
55
Table 1. Attacks against the Web servers
Attack against...
Conﬁdentiality (1)(see Appendix A) CVE-2000-0884 CAN-2001-0925
BuggyHTTP
IIS
Apache
Integrity
Availability
(2)(see Appendix A) CVE-2000-0884
(3)(see Appendix A) CVE-2000-0884
-
-
Content-Type headers. For example, a rule can deﬁne a relation between the outputs,
e.g., between the status code of the several outputs. Another example would be to link a
particular input type to its expected outputs. These rules deﬁne the equivalence relation
that have been deﬁned in Section 3.3. For example, the Figure 3 describes a particular
rule where we deﬁne that apache does not return the same status code than IIS or thttpd
when a directory content is requested but the last ’/’ character is missing: Apache returns
301 when IIS and thttpd returns 302. If this speciﬁcation difference is detected, the
Apache answer status code is modiﬁed to be equal to the IIS reponse. Then, a classical
voting algorithm can be applied to the responses.
The deﬁnition of the rules must be precise enough to ensure that no intrusion would
be missed (i.e., a difference due to a vulnerability must not be part of these rules).
This deﬁnition is, in the current state of this work, made incrementally by analysing
manually the alerts that are provided by the IDS. The accuracy of the detection is thus
driven by a base of rules that must be built by the administrator. This set of rules is
dependent on a number of parameters, such as the COTS used, and their version. As
a consequence, it requires an effort in order to update the base of rules (e.g., at each
upgrade of the web servers). According to our experience, this effort is low (the analysis
of differences and the deﬁnition of the rules for one month of HTTP requests took us
only one day).
It is not possible to deﬁne all differences using these rules. For example, Windows
does not differentiate lower case letters from upper case letters, and thus we had a lot
of behaviour differences due to this system speciﬁcation difference. Thus we added a
mechanism in the proxy which processes the requests to standardize them before they
are sent to the servers. Thus, all web servers provide the same answers.
The output difference masking is thus divided in two parts: pre-request masking
mechanisms that standardize the inputs and post-request masking mechanisms that
mask the differences that are not due to errors in the servers.
4.2 Experimental Results
The objective of the tests that have been conducted is to evaluate the proposed approach
in terms of both reliability and accuracy of the detection process. The reliability of the
approach is its ability to detect correctly the intrusions, as the accuracy refers to its
behaviour in term of false positives generation.
In this section, we detail the two phases of the evaluation process. The reliability is
evaluated by conducting attacks against the set of web servers composing the architec-
ture. The accuracy is evaluated by applying the detection method to a set of server logs.
This second set of results is then compared with the ones obtained with well known
IDSes.
56
E. Totel, F. Majorczyk, and L. Mé
Output Differences Detected
Alerts raised; 131; 0,37%
Pre-request masking
(masked by proxy); 2707;
7,67%
Post-request masking
(masked by rules); 32438;
91,95%
Fig. 4. Analysis of the detected differences
Detection Reliability. In this ﬁrst validation phase, we used an environment composed
of three servers: BuggyHTTP on Linux, Apache 1.3.29 on MacOS-X, and IIS 5.0 on
windows. The choice of BuggyHTTP is dictated by the fact that it contains many vul-
nerabilities that can be easily exploited. Seven attacks have been performed against the
system security properties. These attacks are summarized in Table 1. We exploited three
types of vulnerabilities that allows: access to ﬁles outside the web space (attack against
conﬁdentiality); modiﬁcation of ﬁles outside the web space (attack against integrity);
denial of service (attack against availability).
The HTTP trafﬁc was composed only by the attacks. Notice that, as each request
is processed independently from the others, the detection rate would be the same if the
malicious trafﬁc was drowned in trafﬁc without attacks. All the attacks launched against
one of the COTS server were detected by the IDS, as expected.
Detection Accuracy. The architecture used in this second validation phase is com-
posed by three servers: an Apache server on MacOS-X, a thttpd server on Linux, and
an IIS 5.0 on windows. We avoid to use the buggyHTTP server in this phase because
we do not expect to attack the server in this phase, and because buggy provides limited
functionalities. The three servers contain a copy of our campus web site. They are con-
ﬁgured in such a way that they will generate a minimum of output differences. The three
servers are fed with the requests logged on our campus web server on march 2003. It
represents more than 800.000 requests. A previous study [19] that used a very sensitive
tool [20] carried out tests on the same logs and showed that at most 1.4% of the HTTP
requests can be harmful.
As shown on Figure 4, only 0.37% of the output differences generate an alert. This
represents only 1.6 × 10
−2% of the HTTP requests. In one month, the administrator
must thus analyse 150 alerts, that means about 5 alerts a day. The security administrator
COTS Diversity Based Intrusion Detection and Application to Web Servers
57
Analysis of Alerts (Apache, thttpd, IIS)
Encoded request
(400/404/404); 20
Unknown (404/403/404); 1
CONNECT site
(405,400,501); 4
redirected GET
(404,400,404); 4
Winsys access
(400/404/200); 16
Winsys access
(404/404/500); 16
Winsys access
(none,none,200); 30
Winsys access
(404/404/200); 40
0
5
10
15
20
25
30
35
40
45
Number of Alerts
Fig. 5. Analysis of Alerts