typically this behavior is consistent across hours; all hours
produce similar results. We thus summarize our results us-
ing the average coeﬃcient of variation for each fanout (and
for each OD ﬂow) as the average of the 24 measurements we
have (one for each hour). Our results are plotted in Fig. 5.
We notice that fanouts tend to show very limited variations
around their mean value for the same hour across days.
The predictability of fanouts is an interesting ﬁnding and
should not be confused with the predictability of a traﬃc
matrix. One could postulate that the fanouts are predictable
because the TM is. Recall that the TM is the product of
the fanout and the SNMP link counts, hence both of these
components would need to be stable for the TM to be so
as well.
In Fig. 5 we have also included results on the
coeﬃcient of variation of the traﬃc of OD ﬂows across days
for the same hour (similar to the fanouts described above).
Since all three curves lie below the curves for the fanouts,
this implies that the traﬃc matrix is more variable and less
predictable than the fanouts. We thus focus on the fanouts.
Figure 4: Example fanouts at the three levels of
granularity.
We looked at many OD pairs and observed the same be-
havior. In order to verify the generality of this observation
more thoroughly we did the following. First, we computed
the Fast Fourier Transform (FFT) for the ﬂow fanouts, dur-
ing the period of July 28th, 2003 to August 4th, 2003, at
all levels of granularity. We found that the fanouts of 98%
of the p2p ﬂows, 95% of the r2r ﬂows, and 96% of the l2l
ﬂows exhibit a strong periodicity at the cycle of 24 hours.
This implies that fanouts are predictable across days, i.e.
the fanout for a ﬂow at 1pm on day2 can be predicted based
on the same hour on day1. Second, to check the stability of
fanouts across days we computed the coeﬃcient of variation
for each hour for each fanout (and for each OD ﬂow) across
Figure 6: Estimated throughput based on the
fanouts measured on July 28th, 2003.
This implies the following approach to TM estimation
07/28/0307/29/0307/30/0307/31/0308/01/0308/02/0308/03/0308/04/0300.10.20.30.40.50.60.70.80.91Fanoutp2pr2rl2l00.10.20.30.40.500.10.20.30.40.50.60.70.80.91Coefficient of variation for the same hour across daysF(x)Empirical CDFp2p fanoutr2r fanoutl2l fanoutp2p TMr2r TMl2l TM07/28/0307/29/0307/30/0307/31/0308/01/0308/02/0308/03/0308/04/030200400Throughput (Mbps)07/28/0307/29/0307/30/0307/31/0308/01/0308/02/0308/03/0308/04/03050100150Throughput (Mbps)07/28/0307/29/0307/30/0307/31/0308/01/0308/02/0308/03/0308/04/03050100150Throughput (Mbps)r2r flow 545predictedl2l flow 7277predictedp2p flow 131predictedcould be viable. The idea is to use fanouts from day1 to esti-
mate the behavior of OD ﬂows for the remainder of the week,
where the TM is computed from the fanouts and SNMP data
according to Eq. 2. That is, we use updated SNMP counts
every hour3 to generate the matrix, but do not use updated
fanouts. In Fig. 6, we show the estimates obtained using
the fanouts and the actual traﬃc measured through Netﬂow
for a particular OD ﬂow at each level of granularity. Our
results show that the estimated throughput is surprisingly
close to what was measured through Netﬂow. Even small
ﬂuctuations observed in the p2p ﬂow during the fourth day
of the measurements were captured with high accuracy. The
idea of using one day to predict 7 days is merely an example
used here to illustrate this idea of using fanout stability for
TM prediction. In later sections we will examine the issue
of how long a given fanout can be used for prediction before
becoming stale.
5. PRACTICAL MEASUREMENT OF AN IP
TRAFFIC MATRIX
We now develop a new method for obtaining traﬃc ma-
trices based on our two key ﬁndings so far. First, we assume
that ﬂow monitors will adopt our suggestions and routers
will have the ability to directly compute TM rows. We can
thus focus on distributed solutions. Second, we rely on the
observation that fanouts are both quite stable, and also more
stable than the traﬃc matrix itself. Rather than promote
full direct measurement of a TM, we promote an approach
that relies on partial ﬂow monitoring coupled with the usual
SNMP data.
The method contains two basic elements, an estimation
part and an update part. For the estimation part, we use 24
hours of Netﬂow type measurements to compute a baseline
for fanouts. Because of the presence of strong diurnal pat-
terns, the baseline for each node contains 24 vectors, one for
each hour of the day. The traﬃc matrix at time n can thus
be computed using a small modiﬁcation to Eq. 2 as follows:
(cid:48)
) · Yi(n), n
(cid:48)
= (n%24) + 1
X(i, j, n) = fbl(i, j, n
(3)
where % corresponds to the modulo operator, and fbl(i, j, n(cid:48))
denotes the baseline fanout. Assuming that the hour of a
fanout is indexed by 1, . . . , 24, then the traﬃc matrix at time
n is computed using the same hour n(cid:48) from the baseline day.
The idea is to initially measure the fanouts exactly, but
then to update them only on an as needed basis. The SNMP
data is available every 5 minutes, so when estimates are
made they are a combination of recent SNMP data and
fanouts that may be many hours or days old. Since each
router computes its own fanout, we want each router to
be responsible for updating its own fanout, especially given
that some routers may need their fanouts updated at diﬀer-
ent times than others. Our goal is to develop a method that
allows this to happen, thus maintaining a purely distributed
approach.
Because an IP network is a highly dynamic environment,
clearly changes in fanouts are going to occur over time. We
thus need a scheme that monitors for change and triggers a
recomputation of the baseline when the fanouts have sub-
stantially diverted from the previously calculated ones. Re-
3We simply average the 5 minute counts over 1 hour periods
for our purposes.
computing the baseline means here that the ﬂow monitor is
enabled for another 24 hours.
5.1 A trigger-based baseline update scheme
We use a three-step procedure to determine when updates
are needed for the fanout baseline.
1. Compute the fanout baseline
2. Check for diversion from baseline
(a) Pick 1 hour randomly within the next H hours
(b) recompute the fanout only for that hour
(c) measure diversion of current hour from baseline
for relevant ﬂows
3. If diversion > δ, then recompute the baseline for all
24 hours; else return to Step 2.
The ﬁrst time we compute the baseline in Step 1 is an
initialization step when all ingress links, routers and PoPs
compute the total amount of traﬃc they send to every other
egress link, router or PoP inside the network for each 1 hour
interval for an entire day. We denote by (cid:126)fbl(i, n), 1 ≤ n ≤ 24
the fanout vector that each node computes for itself, accord-
ing to Eq. 1.
There are two important design choices in any change
detection scheme: (i) How frequently should we check for
changes in the fanouts? (ii) How large should the change
in fanout be so as to trigger re-computation of the base-
line? The frequency with which we check is controlled by
parameter H. Step 2a above means that at a time slot n,
a node randomly selects one hour h, where 1 ≤ h ≤ H.
The node computes its new fanout vector at time n + h
(cid:126)ˆf (i, n + h)) just for that hour. We refer to hour n + h
(i.e.,
as the checking hour. We compute the diﬀerence between
the new fanout and the corresponding hour in the baseline
hbl = (n + h)%24 + 1. In other words, we compare 3pm on
the new day to 3pm in the baseline. We call the measured
“change in fanout” either diversion from baseline or merely
diversion for short. This approach ensures that the baseline
is checked at least once every H hours.
We measure diversion in terms of relative change as in
Eq. 4, so that even ﬂows with small fanouts values (that
may nonetheless correspond to high throughput ﬂows) can
trigger re-estimation of the baseline.
D(i, j) = | ˆf (i, j, n + h) − fbl(i, j, hbl)
fbl(i, j, hbl)
|.
(4)
The condition for triggering a recomputation of the entire
baseline (all 24 hours) is for the diﬀerence to exceed a thresh-
old, namely if D(i, j) > δ. We only check for fanout diver-
sion among the relevant ﬂows (those larger than 1 Mbps).
Note that we check for diversion amongst all relevant ﬂows,
but we only require one OD ﬂow to exceed δ in order to trig-
ger a baseline update. The reasons for focusing on relevant
ﬂows are the following. First, the behavior of relevant ﬂows
is more interesting from the network operators’ point of view
because it aﬀects traﬃc engineering applications. Second, as
previously discussed, the irrelevant ﬂows may not be reliable
due to the sampling mechanism used. Third, because these
traﬃc matrices are sparse, we reduce the amount of check-
ing needed by limiting our checking to only relevant ﬂows.
Recall that by limiting ourselves to only the relevant ﬂows,
we are still checking for fanout diversion among 95-99% of
the total traﬃc load. Moreover, even though we only check
for fanout diversion amongst relevant ﬂows, when diversion
is detected, our scheme re-estimates the entire fanout vector
for that node. Therefore, when the baseline is recomputed
we have an accurate picture of all fanouts sourcing at a par-
ticular node.
A subtle point to be taken into account is that given the
router is only aware of the TM row during a single randomly
selected hour, it is not capable of correctly identifying the
relevant ﬂows itself. A ﬂow should be observed for longer
than one day to reveal its long-term average throughput.
Therefore identiﬁcation of these ﬂows will have to be per-
formed by the collection station. One possible way is for
the collection station to identify the relevant ﬂows upon the
assembly of the TM and notify each router accordingly.
Throughout the 3 week period of measurements at our
disposal the set of relevant ﬂows did not change. Neverthe-
less, there will be cases when ﬂows change from irrelevant
to relevant, or vice versa. Consequently, in an operational
environment this list of relevant ﬂows would need to be re-
evaluated every so often. Given what we have seen in our
data, we expect that it would probably be suﬃcient to re-
evaluate the relevant ﬂows and ship the list to router nodes
once a week. Even if this list is not entirely up to date,
triggers generated by other relevant ﬂows on the same node
will lead to accurate throughput estimates across all ﬂows.
i) to reduce the
required number of measurements for the computation of
the IP traﬃc matrix at the three listed granularity levels,
and ii) to do so in an accurate fashion. There is a natural
tradeoﬀ between our two objectives. If one were to collect
measurements more frequently the OD ﬂow estimates would
be more accurate. On the other hand, cutting down the
number of measurements may lead to higher TM estimation
errors. Our scheme allows us to explore this tradeoﬀ using
parameters δ and H as knobs that a network operator could
tune to achieve a target result. We explore this tradeoﬀ in
Section 6.
Our scheme has two main objectives:
5.2 Beneﬁts of the proposed scheme
The beneﬁts of the proposed technique lie in its ability
to reduce the number of measurements needed to obtain
accurate traﬃc matrices. This has consequences that enable
reductions in the associated communications, and storage
overhead.
One beneﬁt comes from the fact that we are essentially ad-
vocating that NetFlow (or a similar software) can be turned
oﬀ between the checking hours. These periods of down time
will last anywhere between 1 and 23 hours. This reduces
the computation, or processing overhead at the router, thus
freeing up the router CPU for other activities.
We reduce the communications overhead in two ways.
First, instead of shipping R2 (or L2) records every hour,
we ship them on average once every few days (we will see
later on that the average number of days between updates is
roughly 2 to 5). Second, we never actually ship R2 records
at once. Since routers are likely to trigger recomputations at
diﬀerent times, each router will send its new baseline at dif-
ferent times, thus spreading the information transfer across
time and space. Similarly, we do not need to collect ﬂow
statistics on all the links of a router at the same time, but
whenever a particular link requires re-estimation of its row.
This implies that the statistics collection on the router can
be asynchronous, considerably reducing the peak load on the
CPU.
This is quite an appealing feature that results from the
use of a distributed approach. All other TM techniques
based on inference require that the TM, or any model used
within the inference procedure, be updated for all OD ﬂows
at once (i.e. a centralized approach). The attractiveness
of our scheme is based on the fact that change in OD ﬂow
behavior will not be uniform across an entire domain. Some
sites may experience a change in popularity, resulting in a
shift in the volume of some OD ﬂows but not others. Our
scheme enables updates to a baseline model to take place
only for those OD ﬂows impacted by any such change in
popularity.
Figure 7: Number of baseline recalibrations per
node.
To illustrate this further, we look at the number of times
the baseline was recomputed at each link, node or PoP
(depending upon the TM granularity), when δ = 0.5 and
H = 24. In Figure 7 we see that 2 out of 13 PoPs needed
no recalibrations at all, 6 out of 27 routers, and 49 out of 80
links needed no recalibration. This demonstrates that run-
ning Netﬂow all the time would clearly be wasteful since it is