2
3
4
Processor
Custom Extractor
Preprocessor Selection
Description
Allows selection of variable
64-bit (cid:2)elds in packet on 64-bit
boundaries in (cid:2)rst 32 bytes
OpenFlow packet processor
that
(cid:2)eld
selection.
Extracts Destination IP Ad-
dress and uses bits in packet to
select the Path/Forwarding Ta-
ble.
Extracts IPv6 destination ad-
dress.
variable
allows
OpenFlow
Path Splicing
IPv6
Table 3: Processor Selection Codes.
Figure 3: Virtualized, Pluggable Module for Programmable
Processors.
SwitchBlade uses a hybrid hardware-software approach to strike
a balance between forwarding performance and implementation
complexity. Speci(cid:2)cally, SwitchBlade’s forwarding mechanism
implementation, provided by the Output Port Lookup module as
shown in Figure 1, provides the following four different methods
for making forwarding decision on the packet: (1) conventional
longest pre(cid:2)x matching (LPM) on any 32-bit address (cid:2)eld in the
packet header within the (cid:2)rst 40-bytes; (2) exact matching on hash
value stored in the packet’s platform header; (3) unconditionally
sending the packet to the CPU for making the forwarding compu-
tation; and (4) sending only packets which match certain user de-
(cid:2)ned exceptions, called software exceptions 4.5, to the CPU. The
details of how the output port lookup module performs these tasks
is illustrated in Figure 4(a). Modes (1) and (2) enable fast-path
packet forwarding because the packet never leaves the FPGA. We
observe that many common routing protocols can be implemented
with these two forwarding mechanisms alone. Figure 4 is not the
actual implementation but shows the functional aspect of Switch-
Blade’s implementation.
By default, SwitchBlade performs a longest-pre(cid:2)x match, as-
suming an IPv4 destination address is present in the packet header.
To enable use of customized lookup, a VDP can set the appropri-
ate mode bit in the platform header of the incoming packet. One
of the four different forwarding mechanisms can be invoked for
the packet by the mode bits as described in Table 2. The output
port lookup module performs LPM and exact matching on the hash
value from the forwarding table stored in the TCAM. The same
TCAM is used for LPM and for exact matching for hashing there-
fore the mask from the user decides the nature of match being done.
Figure 4: Output Port Lookup and Postprocessing Modules.
Once the output port lookup module determines the output port for
the packet it adds the output port number to the packet’s platform
header. The packet is then sent to the postprocessing modules for
further processing. In Section 4.5, we describe the details of soft-
ware work and how the packet is handled when it is sent to the
CPU.
4.5 Flexible Software Exceptions
Although performing all processing of the packets in hardware is
the only way to achieve line rate performance, it may be expensive
to introduce complex forwarding implementations in the hardware.
Also, if certain processing will only be performed on a few packets
and the processing requirements of those packets are different from
the majority of other packets, development can be faster and less
expensive if those few packets are processed by the CPU instead
(e.g., ICMP packets in routers are typically processed in the CPU).
SwitchBlade introduces software exceptions to programmati-
cally direct certain packets to the CPU for additional processing.
This concept is similar to the OpenFlow concept of rules that can
identify packets that match a particular traf(cid:2)c (cid:3)ow that should be
passed to the controller. However, combining software exceptions
with the LPM table provides greater (cid:3)exibility, since a VDP can
add exceptions to existing forwarding rules. Similarly, if a user
starts receiving more traf(cid:2)c than expected from a particular soft-
ware exception, that user can simply remove the software exception
entry and add the forwarding rule in forwarding tables.
There is a separate exceptions table, which can be (cid:2)lled via a
register interface on a per-VDP basis and is accessible to the output
port lookup module, as shown in Figure 4(a). When the mode bits
(cid:2)eld in the platform header is set to 3 (Table 2), the output port
lookup module performs an exact match of the hash value in the
packet’s platform header with the entries in the exceptions table
for the VDP. If there is a match, then the packet is redirected to
the CPU where it can be processed using software-based handlers,
and if there is none then the packet is sent back to the output port
188Figure 6: Resource sharing in SwitchBlade.
tomizable hardware modules, and programmable software excep-
tions. Figure 5 shows the implementation of the NetFPGA router-
based pipeline for SwitchBlade. Because our implementation is
based on the NetFPGA reference implementation, adding multicast
packet forwarding depends on the capabilities of NetFPGA refer-
ence router [2] implementation. Because the base implementation
can support multicast forwarding, SwitchBlade can also support it.
VDP Selection Stage. The SwitchBlade implementation adds
three new stages to the NetFPGA reference router [2] pipeline as
shown in gray in Figure 5. The VDP selection stage essentially per-
forms destination MAC lookup for each incoming packet and if the
destination MAC address matches then the packet is accepted and
the VDP-id is attached to the packet’s platform header (Table 2).
VDP selection is implemented using a CAM (Content Addressable
Memory), where each MAC address is associated with a VDP-ID.
This table is called the Virtual Data Plane table. An admin register
interface allows the SwitchBlade administrator to allow or disallow
users from using a VDP by adding or removing their destination
MAC entries from the table.
Preprocessing Stage. A developer can add customizable packet
preprocessor modules to the VDP. There are two main bene(cid:2)ts for
these customizable preprocessor modules. First, this modularity
streamlines the deployment of new forwarding schemes. Second,
the hardware cost of supporting new protocols does not increase
linearly with the addition of new protocol preprocessors. To enable
custom packet forwarding, the preprocessing stage also provides a
hashing module that takes 256-bits as input and produces a 32-bit
output (Table 2). The hashing scheme does not provide a longest-
pre(cid:2)x match; it only offers support for an exact match on the hash
value. In our existing implementation each preprocessor module is
(cid:2)xed with one speci(cid:2)c VDP.
Shaping Stage. We implement bandwidth isolation for each VDP
using a simple network traf(cid:2)c rate limiter. Each VDP has a con(cid:2)g-
urable rate limiter that increases or decreases the VDP’s allocated
bandwidth. We used a rate limiter from the NetFPGA’s reference
implementation for this purpose. The register interface to update
the rate limits is accessible only with admin privileges.
Software Exceptions. To enable programmable software excep-
tions, SwitchBlade uses a 32-entry CAM within each VDP that can
be con(cid:2)gured from software using the register interface. Switch-
Blade has a register interface that can be used to add a 32-bit hash
representing a (cid:3)ow or packet. Each VDP has a set of registers to
update the software exceptions table to redirect packets from hard-
ware to software.
Figure 5: SwitchBlade Pipeline for NetFPGA implementation.
lookup module to perform an LPM on the destination address. We
describe the process after the packet is sent to the CPU later.
SwitchBlade’s software exceptions feature allows decision
caching [9]: software may install its decisions as LPM or exact
match rules in the forwarding tables so that future packets are for-
warded rapidly in hardware without causing software exceptions.
SwitchBlade allows custom processing of some packets in soft-
ware. There are two forwarding modes that permit this function:
unconditional forwarding of all packets or forwarding of packets
based on software exceptions to the CPU. Once a packet has been
designated to be sent to the CPU, it is placed in a CPU queue corre-
sponding to its VDP, as shown in Figure 4(a). The current Switch-
Blade implementation forwards the packet to the CPU, with the
platform header attached to the packet. We describe one possible
implementation of a software component on top of SwitchBlade’s
VDP(cid:151)a virtual router(cid:151)in Section 7.
5. NETFPGA IMPLEMENTATION
In this section, we describe our NetFPGA-based implementation
of SwitchBlade, as well as custom data planes that we have im-
plemented using SwitchBlade. For each of these data planes, we
present details of the custom modules, and how these modules are
integrated into the SwitchBlade pipeline.
5.1 SwitchBlade Platform
SwitchBlade implements all the modules shown in Figure 5 on
the NetFPGA [2] platform. The current implementation uses four
packet preprocessor modules, as shown in Table 3. SwitchBlade
uses SRAM for packet storage and BRAM and SRL16e storage for
forwarding information for all the VDPs and uses the PCI interface
to send or receive packets from the host machine operating system.
The NetFPGA project provides reference implementations for var-
ious capabilities, such as the ability to push the Linux routing table
to the hardware. Our framework extends this implementation to
add other features, such as the support of virtual data planes, cus-
189ing entry, the platform forward the packet to the CPU for processing
with software-based handlers.
Because OpenFlow offers switch functionality and does not re-
quire any extra postprocessing (e.g., TTL decrement or checksum
calculation), a user can prevent the forwarding stage from perform-
ing any extra postprocessing functions on the packet. Nothing hap-
pens in the forwarding stage apart from the lookup, and Switch-
Blade queues the packet in the appropriate output queue. A devel-
oper can update source and destination MACs as well, using the
register interface.
Path Splicing. Path Splicing enables users to select different paths
to a destination based on the splicing bits in the packet header. The
splicing bits are included as a bitmap in the packet’s header and
serve as an index for one of the possible paths to the destination. To
implement Path Splicing in hardware, we implemented a process-
ing module in the preprocessing stage. For each incoming packet,
the preprocessor module extracts the splicing bits and the destina-
tion IP address. It concatenates the IP destination address and the
splicing bits to generate a new address that represents a separate
path. Since Path Splicing allows variation in path selection, this
bit (cid:2)eld can vary in length. The hasher module takes this bit (cid:2)eld,
creates a 32-bit hash value, and attaches it to the packet’s platform
header.
When the packet reaches the exact match lookup table, its 32-
bit hash value is extracted from SwitchBlade header and is looked
up in the exact match table. If a match exists, the card forwards
the packet on the appropriate output port. Because the module is
concatenating the bits and then hashing them and there is an exact
match down the pipeline, two packets with the same destination ad-
dress but different paths will have different hashes, so they will be
matched against different forwarding table entries and routed along
two different paths. Since Path Splicing uses IPv4 for packet pro-
cessing, all the postprocessing modules on the default path (e.g.,
TTL decrement) operate on the packet and update the packet’s re-
quired (cid:2)elds. SwitchBlade can also support equal-cost multipath
(ECMP). For this protocol, the user must implement a new pre-
processor module that can select two different paths based on the
packet header (cid:2)elds and can store their hashes in the lookup table
sending packets to two separate paths based on the hash match in
lookup.
IPv6. The IPv6 implementation on SwitchBlade also uses the cus-
tomizable preprocessor modules to extract the 128-bit destination
address from an incoming IPv6 packet. The preprocessor module
extracts the 128-bits and sends them to the hasher module to gen-
erate a 32-bit hash from the address.
Our implementation restricts longest pre(cid:2)x match to 32-bit ad-
dress (cid:2)elds, so it is not currently possible to perform longest pre(cid:2)x
match for IPv6. The output port lookup stage performs an exact
match on the hash value of the IPv6 packet and sends it for postpro-
cessing. When the packet reaches the postprocessing stage, it only
needs to have its TTL decremented because there is no checksum in
IPv6. But it also requires to have its source and destination MACs
updated before forwarding. The module selector bitmap shown in
Figure 5 enables only the postprocessing module responsible for
TTL decrement and not the ones doing checksum recalculation.
Because the TTL offset for IPv6 is at a different byte offset than
the default IPv4 TTL (cid:2)eld, SwitchBlade uses a wrapper module
that extracts only the bits of the packet’s header that are required
by the TTL decrement module; it then updates the packet’s header
with the decremented TTL.
Figure 7: Life of OpenFlow, IPv6, and Path Splicing packets.
Sharing and custom packet processing. The modules that func-
tion on the virtual router instance are shared between different vir-
tual router instances that reside on the same FPGA device. Only
those modules that the virtual router user selects can operate on the
packet; others do not touch the packet. This path-selection mech-
anism is unique. Depending on an individual virtual router user’s
requirements, the user can simply select the path of the packet and
the modules that the virtual router user requires.
5.2 Custom Data Planes using SwitchBlade
Implementing any new functionality in SwitchBlade requires
hardware programming in Verilog, but if the module is added as
a pluggable preprocessor, then the developer needs to be concerned
with the pluggable preprocessor implementation only, as long as
decoding can occur within speci(cid:2)c clock cycles. Once a new mod-
ule is added and its interface is linked with the register interface,
a user can write a high-level program to use a combination of the
newly added and previously added modules. Although the num-
ber of modules in a pipeline may appear limited because of smaller
header size, this number can be increased by making the pipeline
wider or by adding another header for every packet.
To allow developers to write their own protocols or use exist-
ing ones, SwitchBlade offers header (cid:2)les in C++, Perl, and Python;
these (cid:2)les refer to register address space for that user’s register in-
terface only. A developer simply needs to include one of these
header (cid:2)les. Once the register (cid:2)le is included, the developer can
write a user-space program by reading and writing to the regis-
ter interface. The developer can then use the register interface to
enable or disable modules in the SwitchBlade pipeline. The de-
veloper can also use this interface to add hooks for software ex-
ceptions. Figure 7 shows SwitchBlade’s custom packet path. We
have implemented three different routing protocols and forwarding
mechanisms: OpenFlow [19], Path Splicing [17], and IPv6 [11] on
SwitchBlade.
OpenFlow. We implemented the exact match lookup mechanism
of OpenFlow in hardware using SwitchBlade without VLAN sup-
port. The OpenFlow preprocessor module, as shown in Figure 3,
parses a packet and extracts the ten tuples of a packet de(cid:2)ned in
OpenFlow speci(cid:2)cations. The OpenFlow preprocessor module ex-
tracts the bits from the packet header and returns a 240-bit wide
OpenFlow (cid:3)ow entry. These 240-bits travel on a 256-bit wire to
the hasher module. The hasher module returns a 32-bit hash value
that is added to the SwitchBlade platform header (Figure 2). Af-
ter the addition of hash value this module adds a module selector
bitmap to the packet’s platform header. The pipeline then sets mode
(cid:2)eld in the packet’s platform header to 1, which makes the output
port lookup module perform an exact match on the hash value of
the packet. The output port lookup module looks up the hash value
in the exact match table and forwards the packet to the output port
if the lookup was a hit. If the table does not contain the correspond-
190Resource
Slices
4-input LUTs
Flip Flops
External IOBs
Eq. Gate Count
21 K out of 23 K
37 K out of 47 K
20 K out of 47 K
353 out of 692
13 M
NetFPGA Utilization % Utilization
90%
79%
42%
51%
N/A
NetFPGA
Implementation
Path Splicing
OpenFlow
IPv4
Slices
17 K
21 K
16 K
4-input
LUTs
19 K
35 K
23 K
Flip Flops
17 K
22 K
15 K
BRAM Equivalent
Gate Count
172
169
123
12 M
12 M
8 M
Table 4: Resource utilization for the base SwitchBlade plat-
form.
6. EVALUATION
In this section, we evaluate our implementation of SwitchBlade
using NetFPGA [2] as a prototype development platform. Our eval-
uation focuses on three main aspects of SwitchBlade: (1) resource