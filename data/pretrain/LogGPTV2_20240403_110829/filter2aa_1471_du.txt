Data compression and sparse files
NTFS supports compression on a per-file, per-directory, or per-volume basis
using a variant of the LZ77 algorithm, known as LZNT1. (NTFS
compression is performed only on user data, not file system metadata.) In
Windows 8.1 and later, files can also be compressed using a newer suite of
algorithms, which include LZX (most compact) and XPRESS (including
using 4, 8, or 16K block sizes, in order of speed). This type of compression,
which can be used through commands such as the compact shell command
(as well as File Provder APIs), leverages the Windows Overlay Filter (WOF)
file system filter driver (Wof.sys), which uses an NTFS alternate data stream
and sparse files, and is not part of the NTFS driver per se. WOF is outside the
scope of this book, but you can read more about it here:
https://devblogs.microsoft.com/oldnewthing/20190618-00/?p=102597.
You can tell whether a volume is compressed by using the Windows
GetVolumeInformation function. To retrieve the actual compressed size of a
file, use the Windows GetCompressedFileSize function. Finally, to examine
or change the compression setting for a file or directory, use the Windows
DeviceIoControl function. (See the FSCTL_GET_COMPRESSION and
FSCTL_SET_COMPRESSION file system control codes.) Keep in mind that
although setting a file’s compression state compresses (or decompresses) the
file right away, setting a directory’s or volume’s compression state doesn’t
cause any immediate compression or decompression. Instead, setting a
directory’s or volume’s compression state sets a default compression state
that will be given to all newly created files and subdirectories within that
directory or volume (although, if you were to set directory compression using
the directory’s property page within Explorer, the contents of the entire
directory tree will be compressed immediately).
The following section introduces NTFS compression by examining the
simple case of compressing sparse data. The subsequent sections extend the
discussion to the compression of ordinary files and sparse files.
 Note
NTFS compression is not supported in DAX volumes or for encrypted
files.
Compressing sparse data
Sparse data is often large but contains only a small amount of nonzero data
relative to its size. A sparse matrix is one example of sparse data. As
described earlier, NTFS uses VCNs, from 0 through m, to enumerate the
clusters of a file. Each VCN maps to a corresponding LCN, which identifies
the disk location of the cluster. Figure 11-43 illustrates the runs (disk
allocations) of a normal, noncompressed file, including its VCNs and the
LCNs they map to.
Figure 11-43 Runs of a noncompressed file.
This file is stored in three runs, each of which is 4 clusters long, for a total
of 12 clusters. Figure 11-44 shows the MFT record for this file. As described
earlier, to save space, the MFT record’s data attribute, which contains VCN-
to-LCN mappings, records only one mapping for each run, rather than one
for each cluster. Notice, however, that each VCN from 0 through 11 has a
corresponding LCN associated with it. The first entry starts at VCN 0 and
covers 4 clusters, the second entry starts at VCN 4 and covers 4 clusters, and
so on. This entry format is typical for a noncompressed file.
Figure 11-44 MFT record for a noncompressed file.
When a user selects a file on an NTFS volume for compression, one NTFS
compression technique is to remove long strings of zeros from the file. If the
file’s data is sparse, it typically shrinks to occupy a fraction of the disk space
it would otherwise require. On subsequent writes to the file, NTFS allocates
space only for runs that contain nonzero data.
Figure 11-45 depicts the runs of a compressed file containing sparse data.
Notice that certain ranges of the file’s VCNs (16–31 and 64–127) have no
disk allocations.
Figure 11-45 Runs of a compressed file containing sparse data.
The MFT record for this compressed file omits blocks of VCNs that
contain zeros and therefore have no physical storage allocated to them. The
first data entry in Figure 11-46, for example, starts at VCN 0 and covers 16
clusters. The second entry jumps to VCN 32 and covers 16 clusters.
Figure 11-46 MFT record for a compressed file containing sparse data.
When a program reads data from a compressed file, NTFS checks the
MFT record to determine whether a VCN-to-LCN mapping covers the
location being read. If the program is reading from an unallocated “hole” in
the file, it means that the data in that part of the file consists of zeros, so
NTFS returns zeros without further accessing the disk. If a program writes
nonzero data to a “hole,” NTFS quietly allocates disk space and then writes
the data. This technique is very efficient for sparse file data that contains a lot
of zero data.
Compressing nonsparse data
The preceding example of compressing a sparse file is somewhat contrived. It
describes “compression” for a case in which whole sections of a file were
filled with zeros, but the remaining data in the file wasn’t affected by the
compression. The data in most files isn’t sparse, but it can still be compressed
by the application of a compression algorithm.
In NTFS, users can specify compression for individual files or for all the
files in a directory. (New files created in a directory marked for compression
are automatically compressed—existing files must be compressed
individually when programmatically enabling compression with
FSCTL_SET_COMPRESSION.) When it compresses a file, NTFS divides
the file’s unprocessed data into compression units 16 clusters long (equal to
128 KB for a 8 KB cluster, for example). Certain sequences of data in a file
might not compress much, if at all; so for each compression unit in the file,
NTFS determines whether compressing the unit will save at least 1 cluster of
storage. If compressing the unit won’t free up at least 1 cluster, NTFS
allocates a 16-cluster run and writes the data in that unit to disk without
compressing it. If the data in a 16-cluster unit will compress to 15 or fewer
clusters, NTFS allocates only the number of clusters needed to contain the
compressed data and then writes it to disk. Figure 11-47 illustrates the
compression of a file with four runs. The unshaded areas in this figure
represent the actual storage locations that the file occupies after compression.
The first, second, and fourth runs were compressed; the third run wasn’t.
Even with one noncompressed run, compressing this file saved 26 clusters of
disk space, or 41%.
Figure 11-47 Data runs of a compressed file.
 Note
Although the diagrams in this chapter show contiguous LCNs, a
compression unit need not be stored in physically contiguous clusters.
Runs that occupy noncontiguous clusters produce slightly more
complicated MFT records than the one shown in Figure 11-47.
When it writes data to a compressed file, NTFS ensures that each run
begins on a virtual 16-cluster boundary. Thus the starting VCN of each run is
a multiple of 16, and the runs are no longer than 16 clusters. NTFS reads and
writes at least one compression unit at a time when it accesses compressed
files. When it writes compressed data, however, NTFS tries to store
compression units in physically contiguous locations so that it can read them
all in a single I/O operation. The 16-cluster size of the NTFS compression
unit was chosen to reduce internal fragmentation: the larger the compression
unit, the less the overall disk space needed to store the data. This 16-cluster
compression unit size represents a trade-off between producing smaller
compressed files and slowing read operations for programs that randomly
access files. The equivalent of 16 clusters must be decompressed for each
cache miss. (A cache miss is more likely to occur during random file access.)
Figure 11-48 shows the MFT record for the compressed file shown in Figure
11-47.
Figure 11-48 MFT record for a compressed file.
One difference between this compressed file and the earlier example of a
compressed file containing sparse data is that three of the compressed runs in
this file are less than 16 clusters long. Reading this information from a file’s
MFT file record enables NTFS to know whether data in the file is
compressed. Any run shorter than 16 clusters contains compressed data that
NTFS must decompress when it first reads the data into the cache. A run that
is exactly 16 clusters long doesn’t contain compressed data and therefore
requires no decompression.
If the data in a run has been compressed, NTFS decompresses the data into
a scratch buffer and then copies it to the caller’s buffer. NTFS also loads the
decompressed data into the cache, which makes subsequent reads from the
same run as fast as any other cached read. NTFS writes any updates to the
file to the cache, leaving the lazy writer to compress and write the modified
data to disk asynchronously. This strategy ensures that writing to a
compressed file produces no more significant delay than writing to a
noncompressed file would.
NTFS keeps disk allocations for a compressed file contiguous whenever
possible. As the LCNs indicate, the first two runs of the compressed file
shown in Figure 11-47 are physically contiguous, as are the last two. When
two or more runs are contiguous, NTFS performs disk read-ahead, as it does
with the data in other files. Because the reading and decompression of
contiguous file data take place asynchronously before the program requests
the data, subsequent read operations obtain the data directly from the cache,
which greatly enhances read performance.
Sparse files
Sparse files (the NTFS file type, as opposed to files that consist of sparse
data, as described earlier) are essentially compressed files for which NTFS
doesn’t apply compression to the file’s nonsparse data. However, NTFS
manages the run data of a sparse file’s MFT record the same way it does for
compressed files that consist of sparse and nonsparse data.
The change journal file
The change journal file, \$Extend\$UsnJrnl, is a sparse file in which NTFS
stores records of changes to files and directories. Applications like the
Windows File Replication Service (FRS) and the Windows Search service
make use of the journal to respond to file and directory changes as they
occur.
The journal stores change entries in the $J data stream and the maximum
size of the journal in the $Max data stream. Entries are versioned and include
the following information about a file or directory change:
■    The time of the change
■    The reason for the change (see Table 11-9)
■    The file or directory’s attributes
■    The file or directory’s name
■    The file or directory’s MFT file record number
■    The file record number of the file’s parent directory
■    The security ID
■    The update sequence number (USN) of the record
■    Additional information about the source of the change (a user, the
FRS, and so on)
Table 11-9 Change journal change reasons
Identifier
Reason
USN_REASON
_DATA_OVER
WRITE
The data in the file or directory was overwritten.
USN_REASON
_DATA_EXTE
ND
Data was added to the file or directory.
USN_REASON
_DATA_TRUN
CATION
The data in the file or directory was truncated.
USN_REASON
_NAMED_DAT
A_OVERWRIT
E
The data in a file’s data stream was overwritten.
USN_REASON
_NAMED_DAT
A_EXTEND
The data in a file’s data stream was extended.
USN_REASON
_NAMED_DAT
A_TRUNCATI
ON
The data in a file’s data stream was truncated.
USN_REASON
_FILE_CREAT
E
A new file or directory was created.
USN_REASON
_FILE_DELETE
A file or directory was deleted.
USN_REASON
_EA_CHANGE
The extended attributes for a file or directory 
changed.
USN_REASON
_SECURITY_C
HANGE
The security descriptor for a file or directory was 
changed.
USN_REASON
_RENAME_OL
D_NAME
A file or directory was renamed; this is the old 
name.
USN_REASON
_RENAME_NE
W_NAME
A file or directory was renamed; this is the new 
name.
USN_REASON
_INDEXABLE_
CHANGE
The indexing state for the file or directory was 
changed (whether or not the Indexing service will 
process this file or directory).
USN_REASON
_BASIC_INFO_
CHANGE
The file or directory attributes and/or the time 
stamps were changed.
USN_REASON
_HARD_LINK_
CHANGE
A hard link was added or removed from the file or 
directory.
USN_REASON
_COMPRESSIO
N_CHANGE
The compression state for the file or directory was 
changed.
USN_REASON
_ENCRYPTION
_CHANGE
The encryption state (EFS) was enabled or disabled 
for this file or directory.
USN_REASON
_OBJECT_ID_C
HANGE
The object ID for this file or directory was changed.
USN_REASON
_REPARSE_PO
The reparse point for a file or directory was 
changed, or a new reparse point (such as a symbolic 
INT_CHANGE
link) was added or deleted from a file or directory.
USN_REASON
_STREAM_CH
ANGE
A new data stream was added to or removed from a 
file or renamed.
USN_REASON
_TRANSACTE
D_CHANGE
This value is added (ORed) to the change reason to 
indicate that the change was the result of a recent 
commit of a TxF transaction.
USN_REASON
_CLOSE
The handle to a file or directory was closed, 
indicating that this is the final modification made to 
the file in this series of operations.
USN_REASON
_INTEGRITY_
CHANGE
The content of a file’s extent (run) has changed, so 
the associated integrity stream has been updated 
with a new checksum. This Identifier is generated 
by the ReFS file system.
USN_REASON
_DESIRED_ST
ORAGE_CLAS
S_CHANGE
The event is generated by the NTFS file system 
driver when a stream is moved from the capacity to 
the performance tier or vice versa.
EXPERIMENT: Reading the change journal
You can use the built-in %SystemRoot%\System32\Fsutil.exe tool
to create, delete, or query journal information with the built-in
Fsutil.exe utility, as shown here:
Click here to view code image
d:\>fsutil usn queryjournal d:
Usn Journal ID   : 0x01d48f4c3853cc72
First Usn        : 0x0000000000000000
Next Usn         : 0x0000000000000a60
Lowest Valid Usn : 0x0000000000000000
Max Usn          : 0x7fffffffffff0000
Maximum Size     : 0x0000000000a00000
Allocation Delta : 0x0000000000200000
Minimum record version supported : 2
Maximum record version supported : 4
Write range tracking: Disabled
The output indicates the maximum size of the change journal on
the volume (10 MB) and its current state. As a simple experiment
to see how NTFS records changes in the journal, create a file called
Usn.txt in the current directory, rename it to UsnNew.txt, and then
dump the journal with Fsutil, as shown here:
Click here to view code image
d:\>echo Hello USN Journal! > Usn.txt
d:\>ren Usn.txt UsnNew.txt
d:\>fsutil usn readjournal d:
...
Usn               : 2656
File name         : Usn.txt
File name length  : 14
Reason            : 0x00000100: File create
Time stamp        : 12/8/2018 15:22:05
File attributes   : 0x00000020: Archive
File ID           : 0000000000000000000c000000617912
Parent file ID    : 00000000000000000018000000617ab6
Source info       : 0x00000000: *NONE*
Security ID       : 0
Major version     : 3
Minor version     : 0
Record length     : 96
Usn               : 2736
File name         : Usn.txt
File name length  : 14
Reason            : 0x00000102: Data extend | File create
Time stamp        : 12/8/2018 15:22:05
File attributes   : 0x00000020: Archive
File ID           : 0000000000000000000c000000617912
Parent file ID    : 00000000000000000018000000617ab6
Source info       : 0x00000000: *NONE*
Security ID       : 0
Major version     : 3
Minor version     : 0
Record length     : 96
Usn               : 2816
File name         : Usn.txt
File name length  : 14
Reason            : 0x80000102: Data extend | File create | 
Close
Time stamp        : 12/8/2018 15:22:05
File attributes   : 0x00000020: Archive
File ID           : 0000000000000000000c000000617912
Parent file ID    : 00000000000000000018000000617ab6
Source info       : 0x00000000: *NONE*
Security ID       : 0
Major version     : 3
Minor version     : 0
Record length     : 96
Usn               : 2896
File name         : Usn.txt
File name length  : 14
Reason            : 0x00001000: Rename: old name
Time stamp        : 12/8/2018 15:22:15
File attributes   : 0x00000020: Archive