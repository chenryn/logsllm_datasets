tions (2001)
3. Bishop, M.: A standard audit trail format. Technical report, Department of Com-
puter Science, University of California at Davis (1995)
4. Custom Attack Simulation Language (CASL). Secure Networks (1998)
5. Chi, S.-D., Park, J.S., Jung, K.-C., Lee, J.-S.: Network Security Modeling and Cy-
ber Attack Simulation Methodology. Lecture Notes in Computer Science, Vol.2119
(2001)
6. Chung, M., Mukherjee, B., Olsson, R.A., Puketza, N.: Simulating Concurrent In-
trusions for Testing Intrusion Detection Systems: Parallelizing Intrusions. Proceed-
ings of the 18th NISSC (1995)
7. Cohen, F.B.: Information System Attacks: A Preliminary Classiﬁcation Scheme.
Computers and Security, Vol.16, No.1 (1997)
8. Cohen, F.: Simulating Cyber Attacks, Defenses, and Consequences. IEEE Sympo-
sium on Security and Privacy, Berkeley, CA (1999)
9. Cuppens, F., Ortalo, R.: Lambda: A language to model a database for detection
of attacks. RAID’2000, Lecture Notes in Computer Science, Vol.1907 (2000)
10. Curry, D.: Intrusion detection message exchange format, extensible markup lan-
guage (xml) document type deﬁnition. draft-ietf-idwg-idmef-xml-02.txt (2000)
11. Debar, H., Dacier, M., Wespi, A., Lampart, S.: An experimentation workbench for
intrusion detection systems. Research Report RZ-2998 (# 93044). IBM Research
Division, Zurich Research Laboratory (1998)
12. Eckmann, S.T., Vigna, G., Kemmerer, R.A.: STATL: An Attack Language for
State-based Intrusion Detection. Proceedings of the ACM Workshop on Intrusion
Detection, Athens, Greece (2000)
13. Feiertag, R., Kahn, C., Porras, P., Schnackenberg, D., Staniford-Chen, S., Tung,
B.: A common intrusion speciﬁcation language (cisl). speciﬁcation draft (1999)
14. Fu, K.S.: Syntactic Methods in Pattern Recognition, Academic Press, New York
(1974)
15. Glushkov, V., Tseitlin, G., Yustchenko, E.: Algebra, Languages, Programming.
Naukova Dumka Publishers, Kiev (1978) (In Russian).
16. Gorodetski, V., Karsayev, O., Kotenko, I., Khabalov, A.: Software Development
Kit for Multi-agent Systems Design and Implementation. Lecture Notes in Artiﬁcial
Intelligence, Vol. 2296, Springer Verlag (2002)
238
V. Gorodetski and I. Kotenko
17. Hailstorm. Users Manual, 1.0 (2000) http://www.clicktosecure.com/
18. Howard, J.D., Longstaﬀ, T.A.: A Common Language for Computer Security Inci-
dents, SANDIA REPORT, SAND98-8667 (1998)
19. Huang, M.-Y., Wicks, T.M.: A Large-scale Distributed Intrusion Detection Frame-
work Based on Attack Strategy Analysis. RAID’98, Louvain-la-Neuve (1998)
20. Icove, D., Seger K., VonStorch, W.: Computer Crime: A Crimeﬁghter’s Handbook,
O’Reilly & Associates, Inc., Sebastopol, CA (1995)
21. IDS Informer 3.0. User Guide. BLADE Software (2001)
22. Iglun, K., Kemmerer, R.A., Porras, P.A.: State Transition Analysis: A Rule-Based
Intrusion Detection System. IEEE Transactions on Software Engineering, Vol. 21,
No.3 (1995)
23. Kemmerer, R.A., Vigna, G.: NetSTAT: A network-based intrusion detection ap-
proach. Proceedings of the 14th ACSAC, Scottsdale, Arizona (1998)
24. Krsul, I.V.: Software Vulnerability Analysis, Ph.D. Dissertation, Computer Sci-
ences Department, Purdue University, Lafayette, IN (1998)
25. Lindqvist, U., Jonsson, E.: How to Systematically Classify Computer Security In-
trusions. Proceedings of the 1997 IEEE Symposium on Security and Privacy, IEEE
Computer Society Press, Los Alamitos, CA (1997)
26. Lippmann, R., Haines, J.W., Fried, D.J., Korba, J., Das, K.: The 1999 DARPA
oﬀ-line intrusion detection evaluation. RAID’2000, Lecture Notes in Computer
Science, Vol.1907 (2000)
27. McHugh, J.: The 1998 Lincoln Laboratory IDS Evaluation: A Critique. RAID’2000,
Lecture Notes in Computer Science, Vol.1907 (2000)
28. McHugh, J.: Intrusion and intrusion detection. International Journal of Informa-
tion Security, No.1 (2001)
29. Me, L.: Gassata, a genetic algorithm as an alternative tool for security audit trails
analysis. RAID’98, Louvain-la-Neuve (1998)
30. Michel, C., Me, L.: ADeLe: an Attack Description Language for Knowledge-based
Intrusion Detection. Proceedings of the 16th International Conference on Informa-
tion Security, Kluwer (2001)
31. Moitra, S.D., Konda S.L.: A Simulation Model for Managing Survivability of Net-
worked Information Systems, Technical Report CMU/SEI-2000-TR-020 ESC-TR-
2000-020 (2000)
32. Moore, A.P., Ellison, R.J., Linger, R.C.: Attack Modeling for Information Security
and Survivability. Technical Note CMU/SEI-2001-TN-001 (2001)
33. http://www.ontology.org/
34. Paxson, V.: Bro: A system for detecting network intruders in real-time. Proceedings
of the 7th Usenix Security Symposium (1998)
35. Puketza, N., Chung, M., Olsson, R.A., Mukherjee, B.: A Software Platform for
Testing Intrusion Detection Systems. IEEE Software, Vol.14, No.5 (1997)
36. Stewart, A.J.: Distributed Metastasis: A Computer Network Penetration Method-
ology. The Packet Factory (1999) (Phrack Magazine, Vol. 9, Issue 55)
37. Tambe, M.: Towards Flexible Teamwork. Journal of Artiﬁcial Intelligence Research,
No.7 (1997)
38. Vigna, G., Eckmann, S.T., Kemmerer, R.A.: Attack Languages. Proceedings of the
IEEE Information Survivability Workshop, Boston (2000)
39. Yuill, J., Wu, F., Settle, J., Gong, F., Huang, M.: Intrusion Detection for an On-
Going Attack. RAID’99, West Lafayette, Indiana, USA (1999)
40. Yuill, J., Wu, F., Settle, J., Gong, F., Forno, R., Huang, M., Asbery, J.: Intrusion-
detection for incident-response, using a military battleﬁeld-intelligence process.
Computer Networks, No. 34 (2000)
Capacity Verification for High Speed Network Intrusion
Detection Systems
Mike Hall and Kevin Wiley
Cisco Systems, Inc.
12515 Research Blvd., Austin TX 78759 USA
[mlhall, klwiley]@cisco.com
Abstract.  Commercially  available  Network  Intrusion  Detection  Systems
(NIDS)  came  onto  the  market  over  six  years  ago.    These  systems  have  gained
acceptance as a viable means of monitoring the security of consumer networks,
yet  no  commercial  standards  exist  to  help  consumers  understand  the  capacity
characteristics  of  these  devices.    Existing  NIDS  tests  are  flawed.    These  tests
resemble  the  same  tests  used  with  other  networking  equipment,  such  as
switches  and  routers.    However,  switches  and  routers  do  not  conduct  the  same
level of deep packet inspection, nor require the higher-level protocol awareness
that a NIDS demands.  Therefore, the current testing does not allow consumers
to infer any expected performance in their environment.  Designing a new set of
tests  that  is  specific  to  the  weak  areas,  or  bottlenecks,  of  a  NIDS  is  the  key  to
discovering  metrics  meaningful  to  the  consumers.    Any  consumer  of  NIDS
technology  can  then  examine  the  metrics  used  in  the  tests  and  profile  his
network  traffic  to  these  same  metrics.    The  consumer  can  use  standard  test
results to accurately predict performance on his network.  This paper proposes a
test  methodology  for  standardized  capacity  benchmarking  of  NIDS.    The  test
methodology  starts  with  examining  the  bottlenecks  in  a  NIDS,  mapping  these
bottlenecks  to metrics  that  can  be  tested,  and  then  exploring  some  results  from
tests conducted.  
1   Introduction and Scope
There are currently no industry  standards for testing any aspect of Network Intrusion
Detection  Systems  (NIDS).    The  NIDS  industry  is  maturing  along  the  same  lines  as
the routers, switches, and firewalls that came before it, and has now reached the point
where standardization of testing and benchmarking is possible.   Attempting to define
a  testing  standard  is  beyond  the  scope  of  this  paper.    Instead,  the  metrics  and
methodology  used  to  properly  verify  the  capacity  of  high  speed  NIDS  are  explored.
Performance  of  NIDS  is  usually  defined  by  false  positive  and  false  negative  ratios,
and  speed  or  capacity.    This  paper  addresses  the  issue  of  benchmarking  the  capacity
of a NIDS.  For the purposes of this paper we use capacity to refer to the ability of a
NIDS to  capture,  process  and  perform  at  the  same  level  of  accuracy  under  a  given
network load as it does on a quiescent network.
Gauging  the  capacity  of  a  NIDS  is  difficult.  There  are  several  variables  in  the
characteristics  of  the  network  traffic  that  affect  the  performance  of  a  NIDS.    In  the 
A. Wespi, G. Vigna, and L. Deri (Eds.): RAID 2002, LNCS 2516, pp. 239-251, 2002.
© Springer-Verlag Berlin Heidelberg 2002
240 M. Hall and K. Wiley
last  year  there  have  been  claims  of  NIDS  performing  at  or  near  gigabit  speeds.  In
every  case,  further  investigation  by  reasonably  sophisticated  NIDS  practitioners
revealed critical flaws in the testing methodology.
The  variety  of  technology  used  to  perform  network-based  intrusion  detection
further complicates finding the proper metrics.  The following technology is used for
NIDS:
- Stateless inspection of the packets or packet headers
- Protocol decode and analysis
- Regular expression matching of packet data
- Anomaly detection
Most NIDS employ a mix of all of these methods.  Some of the metrics discussed
in  this  paper  do  not  apply  to  all  of  the  technologies.    Choosing  metrics  and  test
methods  valid  for  all  NIDS  in  existence  is  impossible.    Choosing  a  broad  set  of
metrics  that  is  generally  applicable  to  most  NIDS  is  possible.    What  are  the  proper
metrics  for  performance  testing?    What  testing  methodology  best  evaluates  these
metrics?  This paper focuses on these two questions.
The  testing  metrics  and  methodology  described  are  intended  for  use  on  a  NIDS
located  at  the  edge  of  an  internal  network  functioning  near  the  firewall  or  border
router.  The focus is further refined by looking at how these metrics apply to a NIDS
using  a  combination  of  the  technologies  listed  above.  However,  many  of  the  metrics
and methods included also apply to the performance of a  NIDS  inside  the  core  of  an
enterprise  network  and  to  a  NIDS  employing  other  methods  of  detecting  intrusions
such as pure anomaly-based systems.
2   History
The  majority  of  NIDS  capacity  benchmarks  to  date  have  been  run  by  independent
third parties either for publication in a trade magazine or at the request of the vendor
for inclusion in marketing material.  The test methodologies were developed based on
experiences in the router- and switch-testing arenas.
These  tests  are  generally  not  adequate  for  the  purposes  of  developing  a  NIDS
performance profile because the benchmark tests for switch and router capacity often
forward packets of various sizes without regard for any protocol above IP or even the
validity  of  the  packets  used.   While  routers  and  switches  are  typically  not  concerned
with  layer  four  and  above,  NIDS  may  discard  packets  that  are  not  interesting.    A
NIDS also needs to look much deeper in a packet than a switch or a router to  follow
layer  four  and  above.    For  example,  a  NIDS  may  discard  TCP  streams  that  are  not
opened  using  a  valid  three-way  handshake.    If  a  switch  or  router  test  is  used  the
majority  of  the  traffic  might  be  ignored.    The  NIDS  then  performs  very  little  deep
packet inspection.
Since  the  results  of  a  NIDS  performance  test  based  on  these  types  of  test
methodologies  are  often  skewed  in  the  favor  of  the  vendor,  a  consumer  may  believe
these  results  are  valid  for  his  deployment  and  encounter  strikingly  different
performance characteristics once the NIDS is fielded on his network.
Capacity Verification for High Speed Network Intrusion Detection Systems
241
For example, all the NIDS tests to date from Mier Communications are flawed [1].
Mier  Labs  concluded  that  two  different  NIDS  could  perform  at  gigabit  lines  rates.
While  the  lab  report  is  technically  accurate,  there  is  no  mention  anywhere  in  the  lab
report  that  a  test  using  TCP  port  0  packets  was  not  going  to  be  representative  of  the
performance  most  consumers  would  experience.    Using  this  type  of  testing
methodology  for  NIDS  is  flawed.   Marcus  Ranum  also  mentions  a  few  other  flawed
testing  methodologies  in  “Experiences  Benchmarking  Intrusion  Detection  Systems”
[2].    Mr.  Ranum  does  an  excellent  job  explaining  why  benchmarking  NIDS  is
difficult.
3   Defining the Metrics
Defining metrics for any type of testing is a difficult task. For example, the materials
used  in  bridge  construction  need  to  be  tested  to  ensure  the  integrity  of  the  bridge.  A
common  approach  to  defining  metrics  for  these  types  of  tests  involves  asking  the
bridge  engineer  to  identify  the  weak  spots  in  the  bridge  design.    Where  is  the  most
stress concentrated?  What has the highest potential for failure when the load exceeds
design  specifications?    The  same  methodology  can  be  used  for  defining  the  metrics
used for testing the performance of NIDS.  The stress points for most protocol decode
and pattern match in a NIDS are the same.
3.1
Fixed Resource Limitations
All computing devices have a similar list of fixed resources. There are only  so  many
cycles available on the CPU and there are only so many bits to  store  all  the  program
code,  state  information,  and  runtime  conditions.    There  also  are  only  so  many
transmission cycles available on the various buses of the computing architecture.  The
upper  limit  for  system  performance  is  approached  as  one  or  more  of  these  resources
approach  its  upper  limit.    Therefore,  the  test  methods  should  include  metrics  that
apply  to  these  resources.    The  following  resources  are  the  most  important  to  the
performance of NIDS:
- Memory size
- Memory bus bandwidth
- Memory latency
- Bus bandwidth for the network interfaces (ethernet card)
- Persistent storage bandwidth (hard drive, flash, etc.)
- CPU speed and bandwidth
3.2
Packet Capture Architecture
NIDS products monitor network traffic and NIDS packet capture architectures impose
physical  limits  on  what  type  of  traffic  can  be  observed.    For  example,  a  NIDS  built
with  a  standard  gigabit  ethernet  card  cannot  observe  all  minimum-sized  ethernet
frames sent at line rate. This equates to approximately 1.4 million packets per second,
242 M. Hall and K. Wiley
and  no  currently  shipping  standard  gigabit  network  adapter  can  handle  many  more
than  700  to  800  thousand  packets  per  second.    However,  if  the  NIDS  uses  dedicated
hardware or some network processing units (NPU), then it is quite possible to handle
more  than  1.4  million  packets.    Also,  the  host  software  platform  for  the  NIDS  can
have  significant  impact  on  the  ability  of  the  NIDS  to  capture  packets.  Many  NIDS
running on a host operating system do not bind the  network interface to an operating
system’s IP stack, and the architectures include custom network interface drivers.
Many of the more recently  published  NIDS  performance  tests  actually  tested  only
the interface bandwidth  of  the  NIDS.    This  type  of  testing  has  limited  use  because  it
only shows the upper limit of how the NIDS performs if no other fixed resources are
used.    Typically  this  type  of  test  lets  the  consumer  understand  how  quickly  a  NIDS
can ignore traffic that  is  uninteresting.    Knowing  the  performance  of  only  the  packet
capture  architecture  is  useful,  but  it  does  not  provide  the  information  needed  to
quantify the performance of the entire system.
The metrics  that  affect  the  performance  of  the  packet  capture  architecture  include
packets per second, bytes (or bits) per second, and physical network interface.
3.3
Packet Flow Architecture
Packet flow architecture is the overall architecture for data flow within the NIDS and
includes  the  packet  capture  architecture.    The  metrics  used  in  the  packet  capture
architecture  section  are  also  valid  for  the  packet  flow  architecture,  assuming  the
packets  used  are  of  interest  to  the  NIDS,  cause  deep  packet  inspection,  and  make
proper use of protocols of interest to the NIDS.  Using HTTP traffic to test the packet
flow architecture is generally a good choice.  For a NIDS that employs  some method
of  protocol  decode  and  state  aware  reassembly,  HTTP  traffic  flows  through  a  major
portion of the packet flow architecture.
  In addition, not all packets take the same amount of time to process.  Buffering in
the  packet  flow  architecture  allows  a  NIDS  to  recover  from  the  packets  that  take  a
long time to inspect.  Packet buffering is an important feature for reducing the number
of  dropped  packets.    Therefore,  when  testing  a  NIDS  with  buffering  in  the  packet
flow  architecture,  it  is  important  to  test  with  sustained  rates  for  a  length  of  time  to
ensure that the buffer is not inflating performance.
3.4
State Tracking
Any NIDS that performs TCP state tracking, IP fragment reassembly, and detection of
sweeps and floods must keep track of the state of the traffic on the network.  Many of
the  signatures  used  in  this  type  of  NIDS  are  based  on  a  certain  threshold  of  events
occurring within a specified period of time.  The only  way to assess event thresholds
is  to  keep  a  count  until  the  time  has  expired  or  the  threshold  has  been  exceeded.    A
NIDS must track information about both the source and destination of the packets, the
state  of  TCP  connections,  and  the  type  and  number  of  packets  going  to  or  from  the
hosts.  All  of  this  information  must  be  stored  somewhere  within  the  NIDS.    The
storage medium for this information is the state database.
Capacity Verification for High Speed Network Intrusion Detection Systems
243
Database  benchmarking  is  very  mature.  The  database  industry  understands  the
weak  points  in  database-like  applications.    The  largest  metrics  include  the  time
needed to insert, delete, and search through data and how those transaction times scale
with the size of the data set and frequency of transactions.  How do we correlate those
database metrics to NIDS metrics?
State information must be inserted into the state database as new network hosts and
unique connections are observed.  The state information is typically removed from the
database  after  either  an  alarm  event  has  occurred,  or  some  time  has  elapsed.  State
database  searches  are  conducted  any  time  the  incoming  packet  may  need  to  refer  to
prior  state  information.    Depending  on  the  types  of  signatures  used,  searching  the
database may need to be done for each packet.
The  size  of  the  state  database  derives  from  the  unique  hosts  and  connections  that
the NIDS considers interesting and maintains prior information about.  The following
metrics directly affect the performance of the state database:
-
-
-
-
Number of unique hosts
Number of new connections over time (i.e., TCP connections per second)
Number of concurrent connections at any given time
Efficiency of expiring data in the database
The  connection  duration  in  number  of  packets  or  time  can  be  used  as  an  indirect
metric  for  testing  the  performance  of  the  state  database  because  the  duration  of  a
session  is  related  to  the  number  of  connections  over  time  as  well  as  the  number  of
concurrent connections.  Therefore, to accurately measure the capacity of a NIDS, one
must  vary  the  number  of  new  connections  per  second,  the  number  of  simultaneous
open  connections,  and  the  total  number  of  unique  hosts  that  the  NIDS  must  track.
The ability of the NIDS to handle network loads varies as these variables are adjusted.
3.5
Packet Analysis
Memory  bandwidth  and  memory  latency  are  large  factors  in  the  performance  of  a
NIDS.    Much  of  the  memory  bandwidth  use  and  latency  are  caused  by  access  to