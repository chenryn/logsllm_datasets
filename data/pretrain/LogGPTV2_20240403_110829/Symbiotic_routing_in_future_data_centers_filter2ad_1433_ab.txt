common to run services in data centers that are horizontally
partitioned across all servers in the data center.
Packets have a simple two-byte header that contains the
serviceId of the service that should handle the packet on
each server. Each time a packet is received on a link, the
kernel runtime delivers it and information about the source
link to the service with the corresponding serviceId, which
runs in user space, via a callback.
Originally, wanting to allow services to be able to manage
queuing internally and exploiting the fact that all services
are cooperating, we used a multi-packet queue for each out-
bound link. The CamCube API allowed a service to queue
packets on the queue and query the queue length. How-
ever, this simple approach made implementing fairness, even
across multiple cooperating services, diﬃcult. We therefore
adopted a more complex hierarchical queuing mechanism,
where conceptually there is a single packet queue per link.
To send a packet a service can atomically check if the queue
is empty and insert the packet if so. If not, the service is
required to internally queue the packet. Whenever a packet
is actually transmitted on a link, the services are polled in
order until one has a packet to send. The packet is then
queued, and the process repeats, ensuring fair queuing across
services and allows the services to manage their own packet
queues.
In the runtime system we use an optimized im-
plementation with low overhead. It is obviously trivial to
extend this to incorporate weighted queuing across services,
if required.
2.1.2 Core services
The CamCube API provides very limited functionality,
and all other functionality is implemented as services that
run on top of this API. We provide a number of core services
that are assumed to be running on all servers in CamCube.
These services provide the additional functionality required
to write services that can perform higher-level operations on
CamCube. The runtime allows services running on the same
server to interact locally, so for example, the routing service
can be accessed by all services running on the server.
The core services include a service that exposes the server
coordinate, as well as the server coordinates of the one-hop
neighbors, and also the size of the coordinate space. There is
also a service to monitor liveness of one-hop neighbors, and
if due to a server or link failure, other services are informed.
Another core service is the multi-hop routing service that
can be used to route a packet to a server. This uses a simple
link state-based protocol, and maintains information about
the reachability of all servers in CamCube. There is a high
degree of path redundancy, meaning that there are multiple
paths between servers. Routing uses shortest paths, but
due to multi-path, it is often the case that multiple one-
hop neighbors oﬀer the same shortest path length to the
destination. Therefore, this is exploited to allow packets to
use the ﬁrst free link that is on a shortest path. Any service
that uses the routing service is able to intercept and modify,
or even drop, a packet at each hop on the path. Packets
routed to a failed or unreachable server will be dropped.
The routing service is able to perform key-based routing
as well as server-based routing. In CamCube, as in CAN,
keys are mapped into the coordinate space.
In key-based
routing the packet is delivered to the server responsible for
the destination key (referred to as the key’s root server).
Without failures, the mapping of keys to servers is trivial,
and uses the servers 3D coordinate (as in CAN). In the pres-
ence of failures, key-based routing will route the packet to
another server which is numerically closest to the key. The
service remaps the key consistently across all servers pro-
viding a consistent key-space. In order to ensure good load
distribution of keys to servers in the presence of failures, a
deterministic tie-breaking mechanism is used. In the rest of
the paper, we refer to a key coordinate when we are refer-
ring to a coordinate independently of a server and a server
coordinate when we are using the coordinate to refer to the
53particular server with that address. The key-based routing,
plus CamCube API, with the ability to perform per-hop
inspection and modiﬁcation of packets implements the full
KBR API [9], widely used in structured overlays.
We did not use a greedy protocol in the routing service be-
cause the server coordinates are static and, therefore, server
failures create voids in the coordinate space. In 2D topolo-
gies, techniques like perimeter routing [22] are able to route
around the voids. However, in a 3D space these techniques
are known not to work [12]. In overlays like CAN, the ad-
dress of a server is a function of the live nodes. This means
that on node failure the mapping between nodes and coor-
dinates changes. This ensures that voids do not occur and
enables using greedy routing but it makes writing services
much more complex.
In CamCube, instead, server coor-
dinates are ﬁxed and the routing service uses a link-state
protocol to route around voids.
2.1.3 Properties
Finally, we use CamCube as the example architecture
throughout the paper.
In general, there are two proper-
ties that we exploit: multi-path and the ability to perform
on-path operations. Topologies, like the BCube and DCell,
could also oﬀer these properties. However, in CamCube
these are directly exposed to the services that running on
CamCube, through the APIs. BCube and DCell assume
that the applications will use a standard TCP/IP stack and
mask the multi-path and the ability to perform on-path op-
erations from the applications running on them.
3. DOES ONE SIZE FIT ALL?
Routing protocols often make explicit trade-oﬀs. In the
case of the CamCube base routing service the trade-oﬀ is
between latency and exploiting multi-path.
It uses multi-
ple paths when they will not result in increased hop count.
Traditionally, it is hard for a service running in the data
center to implement, or modify, the underlying routing, as
this is normally hidden from the services by the TCP/IP
stack. When there is little or no multi-path, the beneﬁt of
supporting multiple routing protocols seems unclear. Ser-
vices written for CamCube can use their own routing pro-
tocol, or a modiﬁed version of the base routing protocol.
This is enabled by the ﬂexibility of the CamCube API, and
in particular the way it exposes the multi-path via explicit
structure and allowing services to intercept and modify pack-
ets on path. This allows them to make diﬀerent trade-oﬀs,
such as increased latency for higher throughput. As dif-
ferent services have conﬂicting requirements for routing it
is not feasible to build a single non-conﬁgurable end-to-end
routing protocol that can provide the required properties for
all services.
Many services implement their own routing protocols to
handle the common cases and rely on the routing service
to handle corner cases, for example voids in the coordinate
space. Services also exploit the internal state of the routing
service while not explicitly using it, for example, to discover
if a server is reachable (and hence not failed). This has
the advantage of minimizing control traﬃc required by each
service, because multiple services run concurrently within a
single CamCube.
Next, we consider in more detail four example services,
describe the properties they wanted, and show how eﬀec-
tive they were at achieving them with their custom routing
protocols.
Simulator To evaluate the services we use a large-scale
packet-level simulator. We have a small prototype Cam-
Cube with 27 servers, and we will present some results from
it in Section 6. The services that we describe in this paper
have been run on this testbed. However, to understand how
a shipping container size CamCube would perform we use
larger-scale simulations. Current shipping container-based
data centers have between 1,500 and 2,500 servers. We ex-
pect the density of these to increase, so conservatively we
run all the simulations with 8,000 servers (a 20-ary 3-cube).
The diameter of the network is 30 and the average path
length is 15. In contrast, at 1,728 servers (12-ary 3-cube)
the diameter is 18 and the average path is 9. The simulator
accurately models links as unidirectional 1 Gbps links. With
8,000 servers, the simulator is simulating 48,000 links.
3.1 TCP/IP service
The TCP/IP service allows us to run unmodiﬁed TCP/IP
applications on top of CamCube, thereby supporting legacy
applications. On the CamCube prototype, this service inter-
cepts all the packets from the operating system’s TCP/IP
stack, tunnels them across CamCube, and then injects them
into the TCP/IP stack at the destination. The aim of the
TCP/IP service is to achieve the maximum throughput be-
tween the source and destination.
Originally IP packets were encapsulated and routed using
the base routing service. Between a source and destination
there will usually be multiple shortest paths. The routing
service exploits all these paths, with the goal of maximizing
throughput. This induces out-of-order packet delivery, but
the service masks this using a small buﬀer at the destination.
However, the multiple paths are not disjoint, and the fate-
sharing of links creates congestion and packet loss on these
links, decreasing end-to-end throughput.
To address this, we use a custom routing protocol that
routes packets using link-disjoint paths. The source deter-
mines the set of outbound links that lie on a shortest path.
For each of the three axes, if the set of links does not in-
clude any neighbor on a particular axis, then the source
adds both neighboring servers lying on that axis to the set
of outbound links. All these links are then used to route
packets out of the source. At each hop a packet is greedily
routed towards the destination, with preference to forward it
on that same axis as the neighbor who delivered the packet
lies, with the constraint that a packet cannot be sent back to
the same neighbor. If progress cannot be made, then another
axis on which progress can be made is deterministically se-
lected and this is then used to route towards the destination.
This yields at least three disjoint paths between source and
destination for all pairs. The approach provides increased
throughput at a cost of at most two-hop path stretch on the
non-shortest paths, which increases packet delivery jitter but
also decreases the out-of-order delivery.
We ran a simple experiment to show the performance in-
crease achieved by the custom routing protocol over the
routing service (base). To avoid the overhead of simulating
a full TCP/IP stack, we instead compared the raw through-
put achieved by both protocols. Hence, there is no explicit
end-to-end ﬂow control or acknowledgments used in the ex-
periment. We selected a single server at random to act as
the source, and then measured the raw throughput to 2,000
54Figure 1: Increase in raw throughput over the base
routing service.
Figure 2: Reduction in links used by VM distribu-
tion service.
randomly selected servers. We use 1,500 byte packets, with
the service on the source generating a packet each time it is
polled from a link over which it could route a packet. The
experiment iterates over the 2,000 servers sequentially, so
that at any point in time there was only one packet ﬂow
from the source to a single destination. Each destination
receives 10,000 packets and the source re-transmits packets
if they are dropped along the path due to congestion.
Figure 1 shows a CDF of the 2,000 ﬂows versus the in-
crease in throughput achieved when using the custom rout-
ing protocol over the routing service (base). As expected,
the custom routing protocol provided beneﬁt in all cases,
with the most beneﬁt being obtained when the number of
shortest paths between two servers was less than three.
3.2 VM distribution service
The Virtual Machine (VM) distribution service enables
the distribution of arbitrary large ﬁles, although primarily
intended for VM images, to multiple servers in CamCube.
The ﬁrst version of the service used a multicast tree to dis-
tribute ﬁles. The tree was created by taking the union of
the paths from each member to a speciﬁed key coordinate.
Join requests were routed using the routing service to a key
coordinate. At each hop, the service would explicitly record
the incoming link as a child in the tree. If the service was
not already a member of the tree, then the routing service
would be used to forward the join message.
However, this created trees with signiﬁcantly higher num-
bers of interior servers and edges than was necessary. This
incurred higher server and link stress during the ﬁle distri-
bution. To address this, the VM distribution service was
updated with a custom routing protocol. The custom rout-
ing protocol creates paths such that the union of them yields
trees with fewer interior servers and, therefore, less edges.
The custom routing protocol is hierarchical, recursively
dividing the coordinate space into 3D mini-cubes. At the
top-level there are eight mini-cubes, with the root key co-
ordinate at the intersection of the eight mini-cubes. At the
next level down in the hierarchy, there are 64 mini-cubes,
such that each of the top-level mini-cubes is subdivided into
eight further mini-cubes. This is recursively repeated until
the mini-cube is indivisible. At this bottom-level in the hi-
erarchy, there is a path from each server, along the edges of
the hierarchical mini-cubes to the root key coordinate. Intu-
itively, the packet is routed towards the root key coordinate,
ascending the hierarchy via mini-cube corners. At each hop
a server determines the current mini-cube level and selects
a neighbor that can make progress towards the next corner.
If more than one such neighbor exists, then a precedence
order over the axes is used to select the next hop, e.g., ﬁrst
x, then y and then z. This completes when the join packet
is delivered to a server that is already part of the distribu-
tion tree, or is the root. If greedy progress cannot be made
towards the next corner, the routing service is used to route
to the next key coordinate.
This custom routing protocol reduces the overall number
of interior links and servers, as it restricts the set of paths
that join requests can traverse.
In absence of failures, no
path stretch is introduced because packets are always for-
warded to servers that are closer to the root key coordinate.
We ran an experiment to determine the reduction in the
number of links used with the custom routing protocol. We
randomly select a key coordinate as the root of the VM
distribution tree, and vary the fraction of the servers joining
the VM distribution tree. In each run, the servers that join
the tree are selected at random. We compare building the
tree using the custom routing protocol to using the base
routing service.
In each case, we measure the number of
links that represent edges in the distribution tree.
Figure 2 shows the reduction of links used in the VM
service when using the custom protocol versus the fraction
of servers joining the group. When the group size is small,
e.g.
less than 0.01 of the servers in the network, there is
little opportunity for sharing links as the servers are picked
randomly, so the beneﬁt is small. As the group size increases
to 0.1 the beneﬁt increases, as the higher number of group
members increases the opportunity for link sharing. As the
number of group members increases above this, the beneﬁt
decreases again as a higher fraction of the links are used. At
the limit, when all servers join the group then there is no
beneﬁt in using the modiﬁed protocol.
3.3 Cache service
Memory-based soft caches, like the popular memcached,