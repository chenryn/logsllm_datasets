of code. Over time, requirements got complex, and in 2001 Windows XP codebase crossed 
40 million lines of code. As we discussed before in this chapter, at the time of this writing, the 
complete Google codebase to run all its Internet services was around 2 billion lines of code. 
Even though one can easily argue the increased number of lines of code will not directly 
reflect the code complexity, in most of the cases, sadly it’s the case.
27 KISS principle, https://en.wikipedia.org/wiki/KISS_principle
Chapter 2  Designing seCurity for apis
49
 Complete Mediation
With complete mediation principle, a system should validate access rights to all its 
resources to ensure whether they’re allowed to access or not. Most systems do access 
validation once at the entry point to build a cached permission matrix. Each subsequent 
operation will be validated against the cached permission matrix. This pattern is 
mostly followed to address performance concerns by reducing the time spent on policy 
evaluation, but could quite easily invite attackers to exploit the system. In practice, most 
systems cache user permissions and roles, but employ a mechanism to clear the cache in 
an event of a permission or role update.
Let’s have a look at an example. When a process running under the UNIX operating 
system tries to read a file, the operating system itself determines whether the process 
has the appropriate rights to read the file. If that is the case, the process receives a file 
descriptor encoded with the allowed level of access. Each time the process reads the file, 
it presents the file descriptor to the kernel. The kernel examines the file descriptor and 
then allows the access. In case the owner of the file revokes the read permission from 
the process after the file descriptor is issued, the kernel still allows access, violating the 
principle of complete mediation. According to the principle of complete mediation, any 
permission update should immediately reflect in the application runtime (if cached, 
then in the cache).
 Open Design
The open design principle highlights the importance of building a system in an open 
manner—with no secrets, confidential algorithms. This is the opposite of security by 
obscurity, discussed earlier in the section “Design Challenges.” Most of the strong 
cryptographic algorithms in use today are designed and implemented openly. One 
good example is the AES (Advanced Encryption Standard) symmetric key algorithm. 
NIST (National Institute of Standards and Technology, United States) followed an open 
process, which expanded from 1997 to 2000 to pick the best cryptographically strong 
algorithm for AES, to replace DES (Data Encryption Standard), which by then was 
susceptible to brute-force attacks. On January 2, 1997, the initial announcement was 
made by NIST regarding the competition to build an algorithm to replace DES. During 
the first nine months, after the competition began, there were 15 different proposals 
from several countries. All the designs were open, and each one of them was subjected 
to thorough cryptanalysis. NIST also held two open conferences to discuss the proposals, 
Chapter 2  Designing seCurity for apis
50
in August 1998 and March 1999, and then narrowed down all 15 proposals into 5. After 
another round of intense analysis during the April 2000 AES conference, the winner was 
announced in October 2000, and they picked Rijndael as the AES algorithm. More than 
the final outcome, everyone (even the losers of the competition) appreciated NIST for 
the open process they carried throughout the AES selection phase.
The open design principle further highlights that the architects or developers of a 
particular application should not rely on the design or coding secrets of the application 
to make it secure. If you rely on open source software, then this is not even possible at 
all. There are no secrets in open source development. Under the open source philosophy 
from the design decisions to feature development, all happens openly. One can easily 
argue, due to the exact same reason, open source software is bad in security. This is 
a very popular argument against open source software, but facts prove otherwise. 
According to a report28 by Netcraft published in January 2015, almost 51% of all active 
sites in the Internet are hosted on web servers powered by the open source Apache web 
server. The OpenSSL library, which is another open source project implementing the 
SSL (Secure Sockets Layer) and TLS (Transport Layer Security) protocols, is used by 
more than 5.5 million web sites in the Internet, by November 2015.29 If anyone seriously 
worries about the security aspects of open source, it’s highly recommended for him 
or her to read the white paper published by SANS Institute, under the topic Security 
Concerns in Using Open Source Software for Enterprise Requirements.30
Note gartner predicts, by 2020, 98% of it organizations will leverage open 
source software technology in their mission-critical it portfolios, including many 
cases where they will be unaware of it.31
28 Netcraft January 2015 Web Server Survey, http://news.netcraft.com/archives/2015/01/15/
january-2015-web-server-survey.html
29 OpenSSL Usage Statistics, http://trends.builtwith.com/Server/OpenSSL
30 Security Concerns in Using Open Source Software for Enterprise Requirements, www.sans.org/ 
reading-room/whitepapers/awareness/security-concerns-open-source-software- 
enterprise-requirements-1305
31 Middleware Technologies—Enabling Digital Business, www.gartner.com/doc/3163926/
hightech-tuesday-webinar-middleware-technologies
Chapter 2  Designing seCurity for apis
51
 Separation of Privilege
The principle of separation of privilege states that a system should not grant permissions 
based on a single condition. The same principle is also known as segregation of duties, 
and one can look into it from multiple aspects. For example, say a reimbursement claim 
can be submitted by any employee but can only be approved by the manager. What if 
the manager wants to submit a reimbursement? According to the principle of separation 
of privilege, the manager should not be granted the right to approve his or her own 
reimbursement claims.
It is interesting to see how Amazon follows the separation of privilege principle in 
securing AWS (Amazon Web Services) infrastructure. According to the security white 
paper32 published by Amazon, the AWS production network is segregated from the 
Amazon Corporate network by means of a complex set of network security/segregation 
devices. AWS developers and administrators on the corporate network who need 
to access AWS cloud components in order to maintain them must explicitly request 
access through the AWS ticketing system. All requests are reviewed and approved 
by the applicable service owner. Approved AWS personnel then connect to the AWS 
network through a bastion host that restricts access to network devices and other cloud 
components, logging all activity for security review. Access to bastion hosts require SSH 
public key authentication for all user accounts on the host.
NSA (National Security Agency, United States) too follows a similar strategy. In a fact 
sheet33 published by NSA, it highlights the importance of implementing the separation 
of privilege principle at the network level. Networks are composed of interconnected 
devices with varying functions, purposes, and sensitivity levels. Networks can consist 
of multiple segments that may include web servers, database servers, development 
environments, and the infrastructure that binds them together. Because these segments 
have different purposes as well as different security concerns, segregating them 
appropriately is paramount in securing a network from exploitation and malicious 
intent.
32 AWS security white paper, https://d0.awsstatic.com/whitepapers/aws-security-
whitepaper.pdf
33 Segregating networks and functions, www.nsa.gov/ia/_files/factsheets/I43V_Slick_
Sheets/Slicksheet_SegregatingNetworksAndFunctions_Web.pdf
Chapter 2  Designing seCurity for apis
52
 Least Common Mechanism
The principle of least common mechanism concerns the risk of sharing state 
information among different components. In other words, it says that mechanisms 
used to access resources should not be shared. This principle can be interpreted in 
multiple angles. One good example is to see how Amazon Web Services (AWS) works 
as an infrastructure as a service (IaaS) provider. Elastic Compute Cloud, or EC2, is 
one of the key services provided by AWS. Netflix, Reddit, Newsweek, and many other 
companies run their services on EC2. EC2 provides a cloud environment to spin up and 
down server instances of your choice based on the load you get. With this approach, 
you do not need to plan before for the highest expected load and let the resources idle 
most of the time when there is low load. Even though in this case, each EC2 user gets his 
own isolated server instance running its own guest operating system (Linux, Windows, 
etc.), ultimately all the servers are running on top of a shared platform maintained 
by AWS. This shared platform includes a networking infrastructure, a hardware 
infrastructure, and storage. On top of the infrastructure, there runs a special software 
called hypervisor. All the guest operating systems are running on top of the hypervisor. 
Hypervisor provides a virtualized environment over the hardware infrastructure. Xen 
and KVM are two popular hypervisors, and AWS is using Xen internally. Even though a 
given virtual server instance running for one customer does not have access to another 
virtual server instance running for another customer, if someone can find a security hole 
in the hypervisor, then he can get the control of all the virtual server instances running 
on EC2. Even though this sounds like nearly impossible, in the past there were many 
security vulnerabilities reported against the Xen hypervisor.34
The principle of least common mechanism encourages minimizing common, 
shared usage of resources. Even though the usage of common infrastructure cannot be 
completely eliminated, its usage can be minimized based on business requirements. 
AWS Virtual Private Cloud (VPC) provides a logically isolated infrastructure for each of 
its users. Optionally, one can also select to launch dedicated instances, which run on 
hardware dedicated to each customer for additional isolation.
The principle of least common mechanism can also be applied to a scenario where 
you store and manage data in a shared multitenanted environment. If we follow the 
strategy shared everything, then the data from different customers can be stored in 
34 Xen Security Advisories, http://xenbits.xen.org/xsa/
Chapter 2  Designing seCurity for apis
53
the same table of the same database, isolating each customer data by the customer id. 
The application, which accesses the database, will make sure that a given customer 
can only access his own data. With this approach, if someone finds a security hole in 
the application logic, he can access all customer data. The other approach could be to 
have an isolated database for each customer. This is a more expensive but much secure 
option. With this we can minimize what is being shared between customers.
 Psychological Acceptability
The principle of psychological acceptability states that security mechanisms should not 
make the resource more difficult to access than if the security mechanisms were not 
present. Accessibility to resources should not be made difficult by security mechanisms. 
If security mechanisms kill the usability or accessibility of resources, then users may find 
ways to turn off those mechanisms. Wherever possible, security mechanisms should be 
transparent to the users of the system or at most introduce minimal distractions. Security 
mechanisms should be user-friendly to encourage the users to occupy them more 
frequently.
Microsoft introduced information cards in 2005 as a new paradigm for 
authentication to fight against phishing. But the user experience was bad, with a high 
setup cost, for people who were used to username/password-based authentication. It 
went down in history as another unsuccessful initiative from Microsoft.
Most of the web sites out there use CAPTCHA as a way to differentiate human beings 
from automated scripts. CAPTCHA is in fact an acronym, which stands for Completely 
Automated Public Turing test to tell Computers and Humans Apart. CAPTCHA is 
based on a challenge-response model and mostly used along with user registration 
and password recovery functions to avoid any automated brute-force attacks. Even 
though this tightens up security, this also could easily kill the user experience. Some 
of the challenges provided by certain CAPTCHA implementations are not even 
readable to humans. Google tries to address this concern with Google reCAPTCHA.35 
With reCAPTCHA users can attest they are humans without having to solve a 
CAPTCHA. Instead, with just a single click, one can confirm that he is not a robot. This is 
also known as No CAPTCHA reCAPTCHA experience.
35 Google reCAPTCHA, www.google.com/recaptcha/intro/index.html
Chapter 2  Designing seCurity for apis
54
 Security Triad
Confidentiality, integrity, and availability (CIA), widely known as the triad of information 
security, are three key factors used in benchmarking information systems security. This 
is also known as CIA triad or AIC triad. The CIA triad helps in both designing a security 
model and assessing the strength of an existing security model. In the following sections, 
we discuss the three key attributes of the CIA triad in detail.
 Confidentiality
Confidentiality attribute of the CIA triad worries about how to protect data from 
unintended recipients, both at rest and in transit. You achieve confidentiality by 
protecting transport channels and storage with encryption. For APIs, where the transport 
channel is HTTP (most of the time), you can use Transport Layer Security (TLS), which 
is in fact known as HTTPS. For storage, you can use disk-level encryption or application- 
level encryption. Channel encryption or transport-level encryption only protects a 
message while it’s in transit. As soon as the message leaves the transport channel, it’s 
no more secure. In other words, transport-level encryption only provides point-to-point 
protection and truncates from where the connection ends. In contrast, there is message- 
level encryption, which happens at the application level and has no dependency on the 
transport channel. In other words, with message-level encryption, the application itself 
has to worry about how to encrypt the message, prior to sending it over the wire, and it’s 
also known as end-to-end encryption. If you secure data with message-level encryption, 
then you can use even an insecure channel (like HTTP) to transport the message.
A TLS connection, when going through a proxy, from the client to the server can 
be established in two ways: either with TLS bridging or with TLS tunneling. Almost all 
proxy servers support both modes. For a highly secured deployment, TLS tunneling is 
recommended. In TLS bridging, the initial connection truncates from the proxy server, 
and a new connection to the gateway (or the server) is established from there. That 
means the data is in cleartext while inside the proxy server. Any intruder who can plant 
malware in the proxy server can intercept traffic that passes through. With TLS tunneling, 
the proxy server facilitates creating a direct channel between the client machine and the 
gateway (or the server). The data flow through this channel is invisible to the proxy server.
Message-level encryption, on the other hand, is independent from the underlying 
transport. It’s the application developers’ responsibility to encrypt and decrypt 
messages. Because this is application specific, it hurts interoperability and builds tight 
Chapter 2  Designing seCurity for apis
55
couplings between the sender and the receiver. Each has to know how to encrypt/
decrypt data beforehand—which will not scale well in a largely distributed system. 
To overcome this challenge, there have been some concentrated efforts to build 
standards around message-level security. XML Encryption is one such effort, led by the 
W3C. It standardizes how to encrypt an XML payload. Similarly, the IETF JavaScript 
Object Signing and Encryption (JOSE) working group has built a set of standards for 
JSON payloads. In Chapters 7 and 8, we discuss JSON Web Signature and JSON Web 
Encryption, respectively—which are two prominent standards in securing JSON 
messages.
Note secure sockets Layer (ssL) and transport Layer security (tLs) are often 
used interchangeably, but in pure technical terms, they aren’t the same. tLs is the 
successor of ssL 3.0. tLs 1.0, which is defined under the ietf rfC 2246, is based 
on the ssL 3.0 protocol specification, which was published by netscape. the 
differences between tLs 1.0 and ssL 3.0 aren’t dramatic, but they’re significant 
enough that tLs 1.0 and ssL 3.0 don’t interoperate.
There are few more key differences between transport-level security and message- 
level security, in addition to what were discussed before.
• 
Transport-level security being point to point, it encrypts the entire 
message while in transit.
• 
Since transport-level relies on the underlying channel for protection, 
application developers have no control over which part of the data to 
encrypt and which part not to.
• 
Partial encryption isn’t supported by transport-level security, but it is 
supported by message-level security.
• 
Performance is a key factor, which differentiates message-level security 
from transport-level security. Message-level encryption is far more 
expensive than transport-level encryption, in terms of resource 
consumption.
Chapter 2  Designing seCurity for apis
56
• 
Message-level encryption happens at the application layer, and it has 
to take into consideration the type and the structure of the message 
to carry out the encryption process. If it’s an XML message, then the 
process defined in the XML Encryption standard has to be followed.
 Integrity
Integrity is a guarantee of data’s correctness and trustworthiness and the ability to detect 
any unauthorized modifications. It ensures that data is protected from unauthorized 
or unintentional alteration, modification, or deletion. The way to achieve integrity is 
twofold: preventive measures and detective measures. Both measures have to take care 
of data in transit as well as data at rest.
To prevent data from alteration while in transit, you should use a secure channel that 
only intended parties can read or do message-level encryption. TLS (Transport Layer 
Security) is the recommended approach for transport-level encryption. TLS itself has a way 
of detecting data modifications. It sends a message authentication code in each message 
from the first handshake, which can be verified by the receiving party to make sure the 
data has not been modified while in transit. If you use message-level encryption to prevent 
data alteration, then to detect any modification in the message at the recipient, the sender 
has to sign the message, and with the public key of the sender, the recipient can verify the 
signature. Similar to what we discussed in the previous section, there are standards based 
on the message type and the structure, which define the process of signing. If it’s an XML 
message, then the XML Signature standard by W3C defines the process.
For data at rest, you can calculate the message digest periodically and keep it in a 
secured place. The audit logs, which can be altered by an intruder to hide suspicious 
activities, need to be protected for integrity. Also with the advent of network storage 
and new technology trends, which have resulted in new failure modes for storage, 
interesting challenges arise in ensuring data integrity. A paper36 published by Gopalan 
Sivathanu, Charles P. Wright, and Erez Zadok of Stony Brook University highlights the 
causes of integrity violations in storage and presents a survey of integrity assurance 
techniques that exist today. It describes several interesting applications of storage 
integrity checking, apart from security, and discusses the implementation issues 
associated with those techniques.
36 Ensuring Data Integrity in Storage: Techniques and Applications, www.fsl.cs.sunysb.edu/
docs/integrity-storagess05/integrity.html
Chapter 2  Designing seCurity for apis
57
Note http Digest authentication with the quality of protection (qop) value set to 
auth-int can be used to protect messages for integrity. appendix f discusses 
http Digest authentication in depth.
 Availability
Making a system available for legitimate users to access all the time is the ultimate goal 
of any system design. Security isn’t the only aspect to look into, but it plays a major 
role in keeping the system up and running. The goal of the security design should be to 
make the system highly available by protecting it from illegal access attempts. Doing so 
is extremely challenging. Attacks, especially on a public API, can vary from an attacker 
planting malware in the system to a highly organized distributed denial of service 
(DDoS) attack.
DDoS attacks are hard to eliminate fully, but with a careful design, they can be 
minimized to reduce their impact. In most cases, DDoS attacks must be detected at the 
network perimeter level—so, the application code doesn’t need to worry too much.  
But vulnerabilities in the application code can be exploited to bring a system down.  
A paper37 published by Christian Mainka, Juraj Somorovsky, Jorg Schwenk, and Andreas 
Falkenberg discusses eight types of DoS attacks that can be carried out against SOAP- 
based APIs with XML payloads:
• 