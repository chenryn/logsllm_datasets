CRF operates on a factor graph which factorizes inter-dependencies
between functions and separates unknown and known information.
Figure 2c shows that the unknown function 0x01fb0 has a fac-
tor based on calling the known function errno_location , factors
based on statically derived known features such as its probabilistic
fingerprint, and a factor based on its relationship with 0x022b4 .
Our CRF models pairwise and generic factor-based relationships
among code and data for all functions in an executable. We first train
the graphical model and learn the weightings of relationships on a
large set of prior unstripped binaries before building a new CRF for
each stripped binary and applying the learned model parameters.
Function names are then inferred by maximizing the conditional
probability of unknown function names, given the known infor-
mation and our models parameters. This enables punstrip to take
into consideration known information from the whole binary si-
multaneously rather than considering each function in isolation;
this may help identify functions that are weakly identifiable in and
of themselves, but have strong connections to other more easily
recognizable functions.
2.3 Function Name Matching and Evaluation
Finally, after we have inferred function names for the set of un-
known symbols, we modify the ELF executable’s symbol table and
insert entries into the strtab and symtab sections. As a result, sub-
sequent disassembly will yield the newly predicted function names
(Figure 2d).
The names developers give to functions can vary wildly based
on personal preferences and project styles. Therefore, relying on
exact lexical matching only to evaluate the accuracy of name pre-
dictions would miss cases where predicted names are semantically
4
correct but syntactically different. We therefore implement two
additional metrics for evaluating name similarity. First, we propose
a method based on natural language processing, which uses lexical
techniques to determine the similarity of two names. The metric
tries to mitigate grammatical differences in language used, expands
common naming conventions used by programmers, and takes into
account similar words, such as start and begin, within each name.
Second, we introduce Symbol2Vec, a new embedding for function
names based on the callgraph of programs. As it is only based on
caller-callee relationships, it completely abstracts away the text
of function names and provides a similarity metric based only on
how software developers use and name functions. Figure 2e demon-
strates how Symbol2Vec embeddings are used when evaluating the
similarity of function names.
3 PROBABILISTIC FINGERPRINT
We now explain how we create a probabilistic fingerprint for each
function. We give an overview of how we extract features using
static (§3.1) and symbolic (§3.2) analysis, and we detail how ex-
tracted features are combined into a probabilistic identifier (§3.3).
3.1 Static Analysis
All features extracted from each function are listed in Table 1. We
include two low-level features that help to find exact matches: a
hash of the machine code and a hash of the opcodes in the disas-
sembly. The opcode hash is included to recognize exact patterns of
generated machine code with different parameters or relative off-
sets, which would not be matched by an exact binary hash. All other
features are extracted from VEX IR. Our choice of VEX IR comes
with the advantage of VEX providing a more abstract view than
alternative representations and not requiring to deal with low-level
details of the machine state, such as the EFLAGS register [28].
Symbols contained in an ELF binary’s symbol table detail a com-
ponent’s address, size and string description in the target executable.
This information is first extracted along with the raw bytes cor-
responding to the function. Using capstone [39], pyvex, and each
function’s boundaries as specified in the binary’s symbol table [4],
we lift each basic block into its optimized VEX IR and build a labeled
Intraprocedural Control Flow Graph (ICFG) for each function. We
then resolve dynamically-linked objects and build a callgraph for
each statically-linked function in the binary.
We track all features given in Table 1 and convert this structure
of features into a form that can be represented by a single stacked
vector to be used as a fingerprint for each function. When labeling
the ICFG, VEX basic blocks are distinguished by their terminators
(jumps, calls, returns, and fall-throughs). We only store numeric
integer constants, greater than 28, that are not operands of jump
instructions to focus on infrequent and distinctive values.
After machine code is lifted into the VEX IR we categorize each
instruction into a one-hot encoded vector according to the regular
expressions defined in Table 2. The one-hot encoded vectors are
then summed to produce an impression of functions operations.
To convert generic graphical structures to a vector representation
we utilize the feature embedding technique graph2vec [40]. We
compare the similarity of all ICFGs by using an implementation of
the Weisfeiler-Lehman graph kernel [44]. By training a graph2vec
Probabilistic Naming of Functions in Stripped Binaries
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 1: Features extracted from functions and their representation in the probabilistic fingerprint.
Feature
Type
Description
Static features
Scalar
Binary
Binary
Scalar
Vector(8)
Vector(8)
Scalar
Size
Hash
Opcode Hash
VEX instructions
VEX jumpkinds
VEX ordered jumpkinds
VEX temporary variables
VEX IR Statements, Expressions and Operations Vector(54)
Callers
Vector(N)
Vector(N)
Callees
Vector(N)
Transitive Closure
Vector(300) Graph2Vec vector representation of labeled ICFG.
Basic Block ICFG
VEX IR constants types and values
Dict
Size of the symbol in bytes.
SHA-256 hash of the binary data.
SHA-256 hash of the opcodes.
Number of VEX IR instructions.
VEX IR jumps inside a function e.g. fall-through, call, ret and jump
A ordered list of VEX jumpkinds.
Number of temporary variables used in the VEX IR.
Categorized VEX IR Statements, Expressions and Operations.
Vector one-hot encoding representation of symbol callers.
Vector one-hot encoding representation of symbol callees.
Symbols reachable under this function.
Number of type of VEX IR constants used.
Stack bytes
Heap bytes
Arguments
Stack locals
Thread Local Storage (TLS) bytes
Tainted register classes
Tainted heap
Tainted stack
Tainted stack arguments
Tainted jumps
Tainted flows
Symbolic features
Scalar
Scalar
Scalar
Scalar
Scalar
Vector(5)
Scalar
Scalar
Scalar
Scalar
Vector(N)
Number of bytes referenced on the stack.
Number of bytes referenced on the heap.
Total number of function arguments.
Number of bytes used for local variables on the stack.
Number of bytes referenced from TLS.
One-hot encoded vector of tainted register types, e.g., stack pointer,
floating point.
Number of tainted bytes of the heap.
Number of tainted bytes of the stack.
Number of tainted bytes that are function arguments to other functions
Number of conditional jumps that depend on a tainted variable.
Vector of tainted flows to known functions.
model, each ICFG is converted into a vector space in which similar
graphical structures are numerically similar. We store each vector
in an Annoy Database3 that allows us to quickly find the nearest
vectors for each graph based on the Euclidean distance from our
model’s embeddings (and hence the most similar graphs). Training
the graph2vec model is computationally expensive; however, it
allows us to avoid comparing pairs of graphs with a graph kernel
over the testing set for every element in the training set. Using
graph2vec we are able to compare the similarity of abstract graphical
structures in O(1) after training the model.
3.2 Symbolic Analysis
We extract additional semantic features using our own symbolic
analysis built on top of the VEX IR. We write our own execution
engine over the existing ANGR [48] implementation to provide a
lightweight and more consistent analysis across multiple platforms.
After the boundaries of each basic block are known from the initial
static analysis, we are able to lift each block into VEX in Single Static
Assignment (SSA) form. Within our model, reads from registers
and memory locations with undefined contents return symbolic
values for the size of data requested.
3Annoy: Approximate Nearest Neighbors Oh Yeah: https://github.com/spotify/annoy
Function Argument Extraction. To identify the number of func-
tion arguments we carry out live variable analysis on argument
registers and memory references to pointers above the current
stack pointer. As we need to track the value of the stack pointer,
we perform a fixed point iteration algorithm to determine the base
and stack pointer values on each use. Finally, we build a model to
track memory references between basic blocks as the VEX IR’s SSA
form is only consistent per basic block.
Heap and Stack Analysis. We implement a stack of 2048 bytes
starting at 0x7FFFFFFFFFF0000 and model the stack registers, seg-
ment registers, and heap accordingly. For each function, we track
the total number of bytes referenced on both the heap and stack,
local variables and function arguments placed on the stack, thread
local storage accesses, and perform taint analysis to calculate data
flows from each input argument to arguments of other functions.
Finally, we compute the transitive closure for each function under
the binary’s callgraph.
Symbolic Execution. After identifying the number of input ar-
guments to a function we symbolically execute the function us-
ing our own execution engine that uses Claripy [10, 47] to for-
mulate symbolic values and expressions. This allows us to create
symbolic expressions for return values from each function, e.g.
5
ACSAC 2020, December 7–11, 2020, Austin, USA
James Patrick-Evans, Lorenzo Cavallaro, and Johannes Kinder
Table 2: Rules for VEX IR categorization.
Regex
Description
VEX IR Operations
Iop_Add(.*)
Iop_Sub(.*)
Iop_Mul(.*)
Iop_Div(.*)
Iop_S(h|a)(.*)
Addition
Subtraction
Multiplication
Division
Arithmetic and logical shifts
Negation
Logical NOT
Logical AND
Logical OR
Logical XOR
Permute bytes
Type conversion
Iop_Reinterp(.*)as(.*) Reinterpretation
Iop_(Cmp|CasCmp)(.*)
Iop_Get(M|L)SB(.*)
Iop_Interleave(.*)
Iop_(Min|Max)(.*)
Iop_Neg(.*)
Iop_Not(.*)
Iop_And(.*)
Iop_Or(.*)
Iop_Xor(.*)
Iop_Perm(.*)
Iop_(.*)to(.*)
String comparison
Get significant bit
Bit interleaving
Min/max operations
Table 3: Feature functions used in the CRF. label-node rela-
tionships relate known features 𝑥 ∈ x to the current node 𝑦𝑢.
label-label relationships relate unknown nodes 𝑦𝑢 → 𝑦𝑣 ∈ y.
Relationship
Description
label-node relationships
Probab. fingerprint
The probability of function 𝑦𝑢 given its
extracted features in Table 1.
label-label relationships
𝑑th pairwise callers
𝑑th pairwise callees
Pairwise data xrefs
Generic factor callers
The probability of of function 𝑦𝑢 calling
𝑦𝑣 through 𝑑 − 1 other nodes.
The probability of of function 𝑦𝑢 being
called by 𝑦𝑣 through 𝑑 − 1 other nodes.
The probability of of function 𝑦𝑢 refer-
encing object 𝑥𝑣.
The probability of of function 𝑦𝑢 calling
the set of known functions x.
Generic factor callees The probability of of function 𝑦𝑢 being
called by the set of known functions x.
Ist_Exit
Ist_IMark
Ist_MBE
Ist_Put_(.*)
Ist_(Store|WrTmp)
Statements
Exit
Instruction marker
Exit
Put
Write
𝑟𝑒𝑡 |= 𝑆𝑦𝑚𝑏𝑉 𝑒𝑐(𝐴𝑅𝐺1) + 𝐵𝑖𝑡𝑉 𝑒𝑐(0𝑥2). Where our symbolic exe-
cution engine cannot easily determine the result of an operation,
e.g. the x86_64 instruction AESENC, we inject symbolic values.
After identifying symbolic variables we are able to extract call
sites to other functions that are control-dependent on a symbolic
variable. We then track the number of call sites that is control-
dependent on each input argument and use it as a feature in our
fingerprint. We run our analysis for every recovered function argu-
ment to extract per input-dependent taints. Finally, we also include
an analysis pass that taints all input arguments to mitigate against
reordering of function arguments producing different results.
Register Classification. We classify registers referenced during
execution into five generic classes: general purpose, floating point,
stack and base pointer, segment register, and control register. For
each input argument we produce a vector of tainted register classes
from the set of tainted registers. This allows punstrip to capture
the types of behavior performed by functions for individual argu-
ments. Finally, we produce a final vector of tainted register classes
irrespective of taint.
3.3 Probabilistic Classification
We aim to convert features extracted in Table 1 to a probability
distribution over a corpus of symbol names 𝑠 ∈ S. For binary and
dictionary typed features we emit a vector of size |S| if the feature
has been seen in our training dataset for each function name. We
6
stack the resultant scalars and vectors into a single feature vector
to be used as the input to a machine learning classifier. As the
feature vector is sparse we reduce its dimensionality by performing
Principal Component Analysis (PCA) and scale the transformed
principle components such that each column has 0 mean and a unit
variance.
Finally, we train a Gaussian Naive Bayes4 model to predict the
probability of each input function belonging to 𝑠 ∈ S. Our model is
implemented using ScikitLearn [40].
4 PROBABILISTIC STRUCTURAL INFERENCE
In this section we explain how we combine our probabilistic fin-
gerprint with a third order general graph based CRF for symbol
inference. First we explain how we generate the CRF (§4.1) using
relationships between multiple symbols and features of individ-
ual functions. We then explain how punstrip performs parameter
estimation (§4.2) and finally inference (§4.3).
4.1 CRF Generation
We refer to the process of symbol inference as predicting the most
likely symbol names using a probabilistic graphical model that
utilizes unary potentials from our probabilistic fingerprint and
known nodes, pairwise potentials between unknown functions, and
generic factor potentials between sets of unknowns and knowns.
In general, CRFs are used to predict an output vector y =
{𝑦0, 𝑦1, . . . , 𝑦𝑁 } of random variables that may have dependencies
on each other given an observed input vector x. Our goal is that
of structured prediction, or learning high-level relationships be-
tween symbols. Modeling the dependencies between all symbols
in binary executables would most likely lead to an computation-
ally intractable graphical model, therefore we use a discriminative
4We found Gaussian Naive Bayes out-performed Random Forests, Logistic Regression,
and Neural Networks.