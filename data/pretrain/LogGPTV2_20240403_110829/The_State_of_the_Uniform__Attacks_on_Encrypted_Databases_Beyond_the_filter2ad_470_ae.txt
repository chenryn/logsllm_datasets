l
)
e
a
c
S
-
g
o
L
(
r
o
r
r
E
d
e
r
a
u
q
S
n
a
e
M
105
104
103
102
101
10%
Fig. 10. Performance of our attack, AGNOSTIC-RECONSTRUCTION-RANGE, under parameterizations of query distributiona Short-Ranges and Value-
Centered.
agnostic, i.e., does not need to know the data/query distribution.
Hence, we would expect GENERALIZEDKKNO to have an
inherent advantage in this experiment since it is speciﬁcally
designed for uniform queries. The results of the experiment,
shown in Figure 11, indicate that this is not the case: in terms
of MSE, GENERALIZEDKKNO is 2.5× to 17× worse than
our AGNOSTIC-RECONSTRUCTION-RANGE for densities from
20% to 90%, and in terms of MAE GENERALIZEDKKNO is
comparable with our AGNOSTIC-RECONSTRUCTION-RANGE
for densities from 15% to 75%.
observation, we have conducted experiments on short range
queries. First, we explain how we generate short range query
distributions and then we report on the experimental results.
Let |R| be the number of all possible range responses.
Speciﬁcally we generate a query distribution that we call Short-
Ranges(α, β) as follows: Generate a Beta(α, β) distribution
and discretize into |R| equally spaced intervals. Recall that
the cardinality of the universe of values is N. Then process
the discretized values from left-to-right and add “noise” by
multiplying each probability with a random number from [0, 1]
divided by |R|. After applying a normalization step, assign
in batches the “noisy” Beta(α, β) probabilities to queries as
follows: assigns the ﬁrst N probabilities to queries whose
range is a single value; assign the next N − 1 probabilities
to queries whose range spans two values; continue up to the
range query spanning the entire universe. This process gives
higher probability to short range queries. The higher the value
of parameter β, the larger the gap between the probabilities
of short and long range queries. To understand how different
Short-Ranges is from the uniform we note that for N = 103,
the mean length of a sampled range query under the uniform is
333, which corresponds to 33% of the universe size. The query
distributions Short-Ranges(1,3), Short-Ranges(1,5), and Short-
Ranges(1,20) have mean length of 142, 90, and 23, which
correspond to 14.2%, 9%, and 2% of the universe, respectively.
In this evaluation, we chose parameter β = {3, 5, 20} and
N = 103. The upper row of Figure 10 shows the heatmap of the
probability distribution for these three parameterizations but for
a smaller universe. The Y -axis, resp. X-axis, corresponds to the
lower boundary, resp. upper boundary, and the coloring of each
square represents the probability of issuing this range query. As
one can see the “bright” high-probability areas are around the
diagonal. The MSE plot in Figure 10 shows the behavior
of AGNOSTIC-RECONSTRUCTION-RANGE under different
We explain the experimental results as follows. The MAE
quality metric is a ﬁrst order statistic, therefore the large errors
of GENERALIZEDKKNO are not penalized enough in the
bottom plot of Figure 11. To explain why the performance of
GENERALIZEDKKNO deteriorates, we note that this algorithm
essentially maps the observed frequency to an expected
frequency if the record were to have a ﬁxed value. For
dense databases, several records will appear together in many
responses and as a result, will have very similar (if not identical)
frequencies. This implies that multiple records map to the
same plaintext value. The experiments conﬁrm this behavior
as GENERALIZEDKKNO tends to map multiple records to
the same reconstructed value. To explain the outperformance
of GENERALIZEDKKNO in the sparse regime, recall that the
support size of each conditional distribution is the product of a
pair of distances between database values. When the database is
sparse, such distances are larger, hence the support size grows
quadratically with the distance. Thus, the adversary needs to
see more samples than the tested ones to increase accuracy.
Evaluation on Short Range Queries. In practical data
analysis applications, focused short range queries are more
likely to occur than exploratory long range queries. Also, a
client who often issues long range queries would have limited
beneﬁts from outsourcing the database. Motivated by this
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1232
N +1
2
(cid:11)
(cid:10)
database densities. The distribution Short-Ranges(1,20) is a
case where one would expect the reconstruction algorithm to be
challenged due to the fact that only a few records are returned
in each response. Interestingly, our reconstruction in Short-
Ranges(1,20) is signiﬁcantly better than the other distributions.
To explain this, recall that the length of the range queries is
really small which implies that the adversary only observes a
small number of responses. So even though the majority of
conditional probability distributions will not
the total
observe any query the small number of conditional distributions
that are “active” will observe enough samples to get a very
accurate estimation of their corresponding support size. The
ﬁnal step of the formulated convex optimization problem in
Equation (3) combines the accurate estimations efﬁciently to
derive the overall assignment of reconstructed values.
Evaluation on Queries Centered Around a Value. In this
experiment we focus on range queries that are centered around a
given value. Consider the real-world scenario of an encrypted
database with medical data and assume that the client is a
researcher who analyzes the medical proﬁle of adolescents with
asthma symptoms. We expect the majority of range queries
issued by the client on the age attribute to have values within
or near range [13, 19] since this is the population of interest.
Inspired by the above scenario, we generate query distribu-
tions that we call Value-Centered(α, β), i.e., tailored to contain
a speciﬁc value of the encrypted database. Similar to the
generation process of the Short-Ranges query distributions,
we discretize a beta pdf and multiply each probability of the
pmf with a random number from [0, 1] divided by |R| and as
a ﬁnal step, we normalize. The difference from the previous
experiment is how we assign the resulting probabilities to range
queries. For Value-Centered we choose uniformly at random
a value v(cid:3)
1 of the underlying database. Processing again the
probabilities-to-be-assigned from left to right, we assign the
ﬁrst batch of probabilities to the range queries that return
the chosen value v(cid:3)
1. As the next step, we sample without
replacement another value v(cid:3)
2 from the database and assign
the next batch of probabilities to the ranges that return v(cid:3)
2.
This process continues until we have processed all n database
values and we ﬁnally assign the remaining probabilities to the
remaining range queries. The lower row of Figure 10 shows
the heatmap of these distribution. The “bright rectangles” show
that the range queries are “centered” around the value on
the upper left corner of the rectangle. The ranges generated
with Value-Centered(1,3) better explore the universe of values
which allows our reconstruction attacks to achieve the smallest
reconstruction error. The case of Value-Centered(1,10) assigns
most of the high probabilities to a subset of the ranges that
contain a single value therefore the majority of the universe is
rarely explored. We report here that 14 out of the 120 runs of
the Value-Centered(1,10) were unsuccessful because the queries
did not explore the universe sufﬁciently. In general the query
distribution Value-Centered(α, β) is makes the reconstruction
more challenging than the previous distribution, a fact that is
also supported by the observed MSE which is 100× larger
than the previous experiment. This reconstruction error can
potentially be reduced by adding a small set of queries of
exploratory nature scattered over the universe of values.
B. Reconstruction from k-NN Queries
In this subsection, we ﬁrst discuss the limitations of the
reconstruction attack ATTACKUNORDERED from k-NN queries
by Kornaropoulos et al. [33]. Next, we introduce our method,
which is scalable and supports reconstructions beyond uniform
query distributions. Finally, we present experiments about the
performance of our attack on synthetic and real-world datasets.
The two new ingredients of our reconstruction algorithm
are: (1) use of support size estimators to compute an estimate
of the length of each Voronoi segment without any knowledge
about the underlying query distribution; and (2) formulation
of an optimization problem that outputs a minimal distortion
of the estimated lengths to transform them to a valid Voronoi
diagram and thus become geometrically consistent. Previously
proposed ATTACKUNORDERED [33] outputs no reconstruction
in case the estimated lengths of the Voronoi segments are not
geometrically consistent a case that we observed in most of
our experiments when the query distribution is skewed.
Overview of ATTACKUNORDERED [33]. An insight from
[33] is that even when the adversary observes all the possible
queries, or else knowns the exact length of each Voronoi
segment, it is impossible to achieve exact reconstruction of the
encrypted database (see Theorem 2 in [33]). This is because
there are arbitrarily many value assignments that have the same
Voronoi diagram which implies that the reconstruction error
comes from the combination of (1) the length estimation errors
and (2) the choice of a reconstruction among the many valid
ones. First, ATTACKUNORDERED estimates the length of a
Voronoi segment LEN({idi, . . . , idi+k−1}) as the frequency of
a response {idi, . . . , idi+k−1} multiplied by the size of the
universe of queries. This simple estimator is accurate under the
assumption that the queries are generated uniformly at random.
As shown in [33], any set of values that implies the observed
Voronoi diagram satisﬁes three families of constrains:
• ordering constraints, i.e., vi < vi+1,
• bisector constrains, i.e., (vi + vj)/2 = bi,j, and
• boundary constraints, i.e., α < v0 and vn−1 < β.
All the above constrains form a feasible region F of potential
reconstructions, which is geometrically expressed as a k-
dimensional convex polytope.
Limitations of ATTACKUNORDERED [33]. We identify
here some limitations of the approach in [33] and later show
how to overcome them. The length estimation in ATTACK-
UNORDERED can be performed solely with the access-pattern
leakage, hence even though the adversary observes a wealth
of information from the search-pattern, this information is
not utilized. Also, algorithm ATTACKUNORDERED provides
rigorous guarantees about the quality of the reconstruction, but
this precision comes with a price. The experiments of [33]
show that for a successful reconstruction, it is preferable to
have (1) a large number of queries and (2) a small number of
neighgbors returned, k. Finally, the number of queries must
be large enough so as the estimated lengths are so accurate
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1233
that they deﬁne a Voronoi diagram without any modiﬁcation.
As an example, to achieve a reconstruction on the real-world
Spitz dataset, the experiments in [33] observed more than 250
million queries. The proposed approach in this paper achieves
a reconstruction on the same dataset with 2.5 million queries,
a 100× smaller sample size. Overall, the exact approach of
ATTACKUNORDERED [33] either succeeds with great accuracy
or fails and outputs nothing. Additionally the technique in [33]
requires the explicit computation of the feasible region by
computing the vertices of the feasible region F which requires
time that is linear to the number of vertices of F. We note that
a k-dimensional convex polytope has O(2k) vertices, therefore
such an approach does not scale well to larger k values. Our
new approach overcomes both of the above limitations and
utilizes the search-pattern leakage.
Our Reconstruction Algorithm. Algorithm 3 (AGNOSTIC-
RECONSTRUCTION-KNN ) outlines our attack from k-NN
queries. A key insight is the use of the search-pattern leakage
to estimate the length of each Voronoi segment without any
knowledge about the query distribution. We build on the attack
in [33] and extend it into two new directions. Instead of
expecting a number of queries large enough to accurately
estimate a valid Voronoi diagram, we compute the minimum
distortion for each estimated length so as the new “augmented”
set of lengths comprise a valid Voronoi diagram. We achieved
this by adding distortion variables to the offset variables of
[33] and introducing a convex optimization problem where
the feasible region formulation from [33] forms the set of
inequality constraints and the objective function expresses the
minimization of the distortion. Finally, in order to scale to larger
values of k, we don’t require the explicit construction of the
feasible region and instead output an arbitrary reconstruction
from the feasible region of the augmented Voronoi diagram.
Observation 1 from Section III shows that an adversary
with a sample D of search tokens and their responses can
partition D with respect to each of the n − k + 1 possible
responses and form a collection of samples from the conditional
probability distributions. From Remark 3 we know that the
support size of a conditional probability distribution is the
length of the corresponding Voronoi segment. Our algorithm
deploys the MODULAR-ESTIMATOR to acquire an estimation of
the length of each Voronoi segment without any assumptions
about the query distribution, see Lines 2-6 in AGNOSTIC-
RECONSTRUCTION-KNN. After this step the estimated lengths,
i.e., column vector(cid:9)l = ((cid:9)L0, . . . ,(cid:9)Ln−k) is treated as constant.
(cid:9)Li, for i ∈ [0, n − k]. We derive the value assignment of
(cid:2)n−k
these variables by solving a quadratic minimization problem
where the objective function is the sum of the squares of δi, i.e.,
δ2
i . This design choice captures our goal to compute
min
the smallest possible distortion. We follow the footsteps
of [33] and express the space of valid reconstructions with
respect to offsets ξi from bisectors. Overall, the optimization
formulation has n− k + 1 unknowns for the distortion variables
(cid:12)δ = (δ0, . . . , δn−k) and k unknowns for the offset variables
(cid:12)ξ = (ξ0, . . . , ξk−1), so a total of n + 1 unknowns. The above
We deﬁne one distortion variable δi per estimated length
i=0
objective function can be written as (cid:12)xT M(cid:12)x, where (cid:12)x is the
column vector from the concatenation of (cid:12)δ and (cid:12)ξ, and M is an
all-zero matrix except the ﬁrst n − k + 1 elements of the main
diagonal which have value 1. Since the matrix M is positive
semideﬁnite, the objective function is a convex function.
that the collection of augmented lengths, i.e., (cid:9)Li + δi for
These constraints can be written as A·[(cid:12)δ, (cid:12)ξ]T ≤ B·(cid:9)l, where A is
Additionally the assignment of (cid:12)δ and (cid:12)ξ should be such
i ∈ [0, n − k], forms a Voronoi diagram. To express this goal
we form four type of linear constraints for the optimization
problem. The ﬁrst type of constraints is the ordering constraints.
(n−1)×(n+1) matrix of constants and B is (n−1)×(n−k+1)
matrix of constants. These matrices can be derived from the
analytical formulas of Lemma 1 in the Appendix. The second
type of constraints is the boundary constraints which guarantee
that α < v0 and vn−1 < β, see Lemma 2 in the Appendix for
the analytical formula. The third type of constraints guarantees
that the offsets are positive, i.e., ξ ≥ 0. Finally the fourth type
of constraints is an equality constraint that guarantees that the
augmented lengths sum to N, i.e.,
(cid:2)n−k
i=0 ((cid:9)Li + δi) = N.
Algorithm 3: AGNOSTIC-RECONSTRUCTION-KNN
Input: A multiset of k-NN search tokens and their responses
D = {(t1, r1), (t2, r2) . . . , (tm, rm)}, the ordering of the
records I = (id0, . . . , idn−1), as well as α, β, N
Output: Approximate Reconstruction ˜v0, . . . , ˜vn−1
1 Compute the left-to-right ordering S of the responses, i.e., the Voronoi
segments, using the ordering I of the records.;
2 for every ri in S from left-to-right do
3
4
Deﬁne Di as the mulitset with tokens from D with response ri;
Call MODULAR-ESTIMATOR with input the multiset of tokens Di
and store the estimated support size as (cid:8)Li;
6 Deﬁne the vector of estimated lengths(cid:8)l ← ((cid:8)L0, . . . ,(cid:8)Ln−k);
5 end
7 Solve the following convex optimization problem with unknowns the
vector of distortions (cid:5)δ and the vector of offsets (cid:5)ξ:
min
(cid:2)δ,(cid:2)ξ
s.t.
δ2
i
n−k(cid:9)
(cid:10)
(cid:11)
(cid:10)
(cid:11)
(cid:10)
(cid:11)
i=0
(cid:5)δ
(cid:5)ξ
(cid:5)δ
(cid:5)ξ
(cid:5)δ
(cid:5)ξ
≤B ·(cid:8)l,
≤bl ·(cid:8)l,
≤bu ·(cid:8)l,
(cid:5)ξ ≥0,
(cid:5)δT(cid:8)l =N,
A ·
·
aT
l
u ·
aT
(ordering constraint)
(lower boundary constraint)
(upper boundary constraint)
(positive offsets constraint)
(sum of augmented lengths)
where A and B are matrices of constant terms derived from the ordering
constraints of Lemma 1, al, bl are vectors of constant terms for the lower