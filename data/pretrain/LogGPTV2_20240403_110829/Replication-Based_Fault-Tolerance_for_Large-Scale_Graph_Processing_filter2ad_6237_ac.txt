the normal execution.
5.3 Additional Failure Models
5.3.1 Multiple Machine Failures
To tolerate multiple nodes failure at the same time,
Imitator just needs to ensure that the number of mirrors
for each vertex in Imitator is equal or larger than the
expected number of machines to fail. When a single
machine failure happens, if all mirrors participate the
recovery, it is a waste of network bandwidth. Hence,
during graph loading, each mirror is assigned with an
ID; only the surviving mirrors with the lowest ID will
do the recovery work. Since a mirror has the location
information of other mirrors and the new coming node’s
logic ID of this job, mirrors need not communicate with
each other to elect a mirror to do recovery.
5.3.2 Other Types of Failures
When a failure happens during the system is loading
graph, since the computation has not started, we just
restart the job. If a failure happens during recovery, such
a failure is almost the same as the failure happening
during the normal execution. Hence, we just restart the
recovery procedure.
There is a single master for a cluster, and it is only
in charge of job dispatching and failure handling. It
has nothing to do with the job execution. Since the
possibility of master failure is very small, and there are
a lot of prior work to address the single master failure,
we do not try to address the master failure in this paper.
Fig. 6: A sample of Migration recovery approach in Imitator.
Fig. 6 as a running example to illustrate how Migration
based recovery works. Here, we only describe recov-
ering graph states to continue execution for simplicity.
Creating additional replicas to retain the original fault
tolerance level can be done similarly by creating an
additional mirror when tolerating recurrent failures.
5.2.1 Reloading
The main differences between Rebirth and Migration for
reloading is that mirror vertices will be “promoted” to
masters and take over the computation tasks for future
execution.
On detecting a failure, all surviving nodes will get
the information about the crashed ones from the master
node of a cluster. Surviving nodes will scan through
all of their mirrors to ﬁnd whose masters were on the
crashed nodes. In Fig. 6, they are mirror 6 on node 1 and
mirror 3 on node 2. These mirrors will be “promoted”
as new masters.
After “promotion”, all surviving nodes will broadcast
the information of the “promoted” mirrors. The broad-
cast information will be used by other nodes as a hint to
send the necessary data to the new masters. Such data
includes:
• New replicas: Since the new master will be on
a different node, new replicas of its neighbors on
different nodes whose out-edge points to it are
necessary to make the states of those neighbors
accessible during computation. Replica 6 on node
2 in Fig. 6 illustrates this case.
• Edges from old replicas to the new master: If
there is already a replica on the machine where the
new master resides, a new replica is not necessary.
Instead, the new edges will be sent to that node
and will then be added to existing replicas. Edge
from mirror 2 to master 6 on node 1 in Fig. 6 is
one example of such case.
• Edges between masters: With the new masters,
there may be some new edges between the masters,
either between two new masters or between a new
master and an old one. Edge from master 5 to
master 3 on node 2 in Fig. 6 shows the case.
All surviving nodes will scan its masters, including
both old masters and the newly “promoted” ones, check
if some data should be reconstructed, and prepare the
necessary information for the reconstruction phase.
569
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm
|V |
|E|
Graph
GWeb
TABLE 1: A collection of input graphs, and the execution time
on Hama and Imitator without fault tolerance on 50 nodes.
w/o FT
12.2
86.7
120.7
13.7
14.8
341.4
Hama
17.0
0.87M
4.85M
280.5
5.72M 130.1M 482.6
0.11M
42.5
17.2
0.32M
1.97M
295.3
2.7M
1.05M
5.53M
ALS
CD
SSSP
PageRank
LJournal
5.11M
70.0M
Wiki
SYN-GL
DBLP
RoadCA
d
a
e
h
r
e
v
O
d
e
z
i
l
a
m
r
o
N
 6
 5
 4
 3
 2
 1
 0
BASE
REP
CKPT
%
6
.
0
%
2
.
1
%
7
.
3
%
7
.
2
%
6
.
0
%
6
0
-
.
GWeb
LJournal
Wiki
SYN-GL
DBLP
RoadCA
6 Evaluation
We have implemented Imitator based on Hama [9],
an open source clone of Pregel implemented in Java,
but extended Hama by vertex replication instead of
pure message passing as the communication mecha-
nism. The support of fault tolerance requires no source
code changes to graph algorithms. To measure the efﬁ-
ciency of Imitator, we use four typical graph algorithms
(PageRank, Alternating Least Squares (ALS), Commu-
nity Detection (CD) and Single Source Shortest Path
(SSSP)) to compare the performance and scalability of
different systems and conﬁguration. We also provide a
case study to illustrate the effectiveness of Imitator by il-
lustrating the execution of different recovery approaches
under injected node failures.
6.1 Experimental Setup
All experiments are performed on a 50-node EC2-like
cluster. Each node has four AMD Opteron cores, 10GB
of RAM, and connected via a 1 GigE network. We use
HDFS on the same cluster as the distributed storage
layer to store input ﬁles and checkpoints.
Table 1 lists a collection of large graphs for our
experiments. Most of them are from Stanford Large
Network Dataset Collection [21]. The Wiki dataset is
from [19]. The dataset for the ALS algorithm is synthet-
ically generated by tools that used in the Gonzalez et
al. [12]. The SSSP algorithm requires the input graph to
be weighted. Since the RoadCA graph is not originally
weighted, we synthetically assign a weight value to
each edge, where the weight is generated based on
a log-normal distribution (μ = 0.4, σ = 1.2) from the
Facebook user interaction graph [28].
6.2 Hama vs. Imitator’s Baseline
As the baseline of Imitator is extended from Hama, we
ﬁrst compare the baseline performance of Imitator with
that of Hama. As shown in Table 1, Imitator actually
has better performance compared to Hama in all the
applications except SSSP. For the three applications,
Imitator always outperforms Hama and the speedup can
reach up to 4X in the largest dataset Wiki.
Hama adopts the pure message passing to vertex
communication, in which the message is simpler than
the synchronization message between the master vertex
and replica vertex in Imitator. However, supporting
Fig. 7: A comparison of runtime overhead between replication
(REP) and checkpoint (CKPT) based fault tolerance over
Imitator w/o fault tolerance (BASE).
dynamic computation [11] is hard for a message passing
system, but is quite natural for a replica-based one. With
the support of dynamic computation, the total number
of messages to send and vertex computations in Imitator
is notably less than that in Hama for PageRank, ALS,
and CD, which mainly contributes to the speedup.
For SSSP, a vertex only needs to activate its neighbors
when its distance to the source vertex changes, so
there is little chance for dynamic computation. As the
message in Hama is simpler, Imitator is a little bit
slower than the original Hama in SSSP.
Unless speciﬁed, we will use Imitator without fault
tolerance as the baseline and Imitator-CKPT as the
checkpoint-based fault tolerance system for comparison.
6.3 Runtime Overhead
Fig. 7 shows the runtime overhead due to applying
different fault tolerance mechanisms on the baseline
system. The overhead of Imitator is less than 3.7%
for all algorithms with all datasets, while the over-
head of the checkpoint-based fault tolerance system is
very large, varying from 65% for PageRank on Wiki
to 449% for CD on DBLP. Even using in-memory
HDFS, checkpoint-based approach still incurs perfor-
mance overhead from 33% to 163% partly due to the
cross machine triple replication in HDFS. In addition,
writing to memory also causes signiﬁcant pressure on
memory capacity and bandwidth to the runtime, occupy-
ing up to 42.1GB extra memory for SSSP on RoadCA.
The time of checkpointing once is from 1.08 to 3.17
seconds for different size of graphs, since the write
operations to HDFS can be batched and are insensitive
to the data size. The overhead of each iteration in
Imitator is lower than 0.05 seconds, except 0.22 seconds
for Wiki, which is still several tens of times faster than
checkpointing.
6.4 Overhead Breakdown
Fig. 8(a) shows the extra replicas among all the replicas
used for fault
tolerance. The rates of extra replicas
are all very small without selﬁsh vertices, even the
largest rate is only 0.12%. Fig. 8(b) shows the redundant
messages among the total messages during the normal
execution. Since the rate of extra replicas is very small,
570
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
d
a
e
h
r
e
v
O
s
a
c
i
l
p
e
R
#
4%
3%
2%
1%
0%
G
W
Selfish Vertex
Normal Vertex
%
2
1
.
0
%
5
0
.
0
%
5
0
.
0
%
8
0
.
0
W
d
a
e
h
r
e
v
O
s
g
s
M
#
0.08 %
0.06 %
0.04 %
0.02 %
0 %
%
2
9
2
.
%
1
3
1
.
w/o OPT
w/ OPT
)
c
e
S
(
i
e
m
T
y
r
e
v
o
c
e
R
 60
 50
 40
 30
 20
 10
 0
Replay
Reload
10
20
30
40
50
)
c
e
S
(
i
e
m
T
y
r
e
v
o
c
e
R
 60
 50
 40
 30
 20
 10
 0
Replay
Reconstruct
Reload
10
20
30
40
50
L
J
o
e
b
iki
u
r
n
al
S
Y
D
R
B
N
-
G
L
P
L
o
a
d
C
A
G
W
L
J
o
e
b
W
S
D
R
iki
Y
N
-
B
L
P
o
a
d
G
L
C
A
u
r
n
al
Fig. 8: The overhead of (a) #replicas and (b) #msgs for
Imitator w/ and w/o selﬁsh optimization.
TABLE 2: The recovery time (seconds) of different approaches
(Checkpoint, Rebirth and Migration).
Algorithm
Graph
GWeb
PageRank
LJournal
ALS
CD
SSSP
Wiki
SYN-GL
DBLP
RoadCA
CKPT
8.17
41.00
55.67
6.86
3.88
12.06
Rebirth Migration
1.20
2.32
3.40
1.28
1.09
1.57
2.08
8.85
14.12
1.00
0.67
2.27
the additional messages rate is very small, with only
2.92% for the worst case. When enabling the optimiza-
tion for selﬁsh vertices, the messages overhead is lower
than 0.1%.
6.5 Efﬁciency of Recovery
Replication-based fault tolerance provides a good op-
portunity to fully utilize the entire resources of the clus-
ter for recovery. As shown in Table 2, the replication-
based recovery outperforms checkpoint-based recov-
ery by up to 6.86X (from 3.93X) and 17.67X (from