Open
Packet Size
Same features as P
BIND features:
Same features as BINDSVM
BIND features:
Same features as BINDSVM
Naïve
Bayes
SVM
Optimized
SVM
SVM
Weighted
k-NN
Weighted
k-NN
Random
Forest
∗new approaches introduced in this paper
Table 4: Trafﬁc Analysis Techniques used for the evaluation
most apps use HTTP along with HTTPS while a sizable portion of apps
use only HTTPS, for communication over the Internet. Furthermore, we
obtained a list of IP addresses from HTTPS apps in each category
We found a total of 1115 unique IP addresses for APP-FIN, 820
for APP-COMM, and 900 for APP-SOCIAL. Additionally, each app
connects to 3 different IP addresses on average over the whole dataset.
This clearly indicates that the IP addresses found on HTTPS trafﬁc
overlap across apps, and do not provide sufﬁcient information to
identify the app generating a trace by itself.
4.2 Experimental Settings
Using these datasets, we perform our analysis on both closed-
world and open-world settings. For a comparative evaluation, we
consider existing trafﬁc analysis techniques developed for WFIN. These
techniques are listed in Table 4. The table details the features and
classiﬁers used for our evaluation in both Closed-world (Closed) and
Open-world (Open) settings. For brevity of representation, we term
websites (in the case of WFIN) or apps (in the case of AFIN) as entities.
Closed-world. Using BIND features, we use a support vector machine
classiﬁer (SVM) in the closed-world setting. We refer to this approach
as BINDSVM as shown in Table 4. In our experiments, we use a
publicly available library called LibSVM [8] with a Radial Basis
Function (RBF) kernel having the parameters Cost = 1.3 × 105 and
γ = 1.9 × 10−6 (following recommendations in [28]). We consider
varied subsets of entities to evaluate the feature set. Particularly,
we use 16 randomly selected traces per entity (class) for training a
182
classiﬁer, and 4 randomly selected traces per entity for testing. For each
experiment, we chose the number of selected (monitored) entities in
{20, 40, 60, 80, 100}.
Open-world. For the open-world scenario, as discussed in §3.2, we
use two classiﬁcation methods with the BIND features. First, we use
the weighted k-NN mechanism proposed in [36]. Speciﬁcally, we
use k = 1 since it is shown to produce the best results on the TOR
dataset in [36]. We denote this method as BINDWKNN as shown
in Table 4. Furthermore, we also use the Random Forest classiﬁer
with BIND features, denoted as BINDRF in Table 4. We use a set of
100 weak learners to form an ensemble of decision trees. We use the
scikit-learn [29] implementation for our evaluation. The complete set of
monitored and non-monitored traces mentioned in Tables 2 and 3 are
considered for evaluation.
Evaluation Measure. The results of the closed-world evaluation
are measured by computing the average accuracy of classifying the
correct class for all test traces. We randomly select traces from the
corresponding dataset and repeat each experiment 10 times with
different entities and traces. Average accuracy is computed across
these experiments. In the open-world evaluation, we measure the
true positive rate (TPR) and false positve rate (FPR) of the binary
classiﬁcation. These are deﬁned as follows: T P R = T P
T P +F N and
F P +T N . Here, T P (True Positive) is the number of traces
F P R = F P
which are monitored, and predicted as monitored by the classiﬁer. F P
(False Positive) is the number of traces which are non-monitored, but
predicted as monitored. T N (True Negative) is the number of traces
which are non-monitored and predicted as non-monitored. F N (False
Negative) is the number of traces which are monitored, but predicted as
non-monitored. We perform a 10-fold cross validation on each dataset,
which gives randomized instance ordering.
In order to evaluate the performance of BIND against defenses
discussed in §2.1, we consider one of the most sophisticated and
complex defenses, Trafﬁc Morphing (TM). Furthermore, to evaluate
BIND against existing approaches, for the open-world setting on the
TOR dataset, we apply the Tamaraw defense mechanism, designed
speciﬁcally for Tor, as evaluations in [6,36] show that this defense
performs exceptionally well against TOR.
4.3 Experimental Results
We use the notations given in Table 2 and Table 3 to denote the
WFIN and AFIN datasets respectively.
4.3.1 Trafﬁc Analysis
We ﬁrst perform WFIN and AFIN experiments in the closed-world
setting. Here, a set of randomly chosen entities are classiﬁed using
competing methods. We vary the set size from 20 to 100. The results are
presented in Table 5 using the HTTPS and TOR datasets for WFIN, and
the APP-FIN dataset for AFIN. In some cases, we can see BINDSVM
performs comparatively closer to or lower than the other competing
methods, while outperforming them in other cases. For example, with
80 websites considered, the average accuracy of BINDSVM (BIND
using SVM) on the HTTPS dataset is 88.4%. This is marginally greater
than 88.3% obtained from the P method. Similarly in AFIN, BIND
resulted in an average accuracy of 87.8%, compared to a marginally
better accuracy of 88% resulting from the P method. Moreover for the
TOR dataset, it is not surprising that the OSAD method performs
the best in all experimental settings since it uses a distance measure
that is speciﬁcally applicable to Tor data. In the closed-world setting,
most methods listed in Table 4 use features that overlap or hold
similar information about the class label. Some features provide better
characteristic information about the class than others. When selecting
the websites at random during evaluation, each classiﬁcation method
outperforms the other in a few cases depending on the data selected for
training and testing. Therefore, the average accuracy across these are
marginally superior than others in most of the cases.
However, the greatest impact of using BIND features can be observed
in the more realistic open-world setting. Table 6 presents the results of
the open-world setting for all competing methods. Here, a high value of
TPR and a low value of FPR are desired. As mentioned earlier in this
section, we use two types of classiﬁers while using the BIND features,
i.e., BINDWKNN and BINDRF. In the case of WFIN, it is clear that
the TPR for both BINDWKNN and BINDRF is signiﬁcantly better
compared to that of WKNN. For instance, consider the result of the
TOR dataset. The TPR obtained from BINDWKNN method is 90.4%
and that obtained from BINDRF is 99.8%, as compared to 89.6% of
WKNN. The BINDRF method outperforms WKNN even though the
WKNN method was speciﬁcally designed for high quality results on this
dataset. In terms of FPR, BINDWKNN method performs better than
WKNN.
A more signiﬁcant result can be observed in the open-world setting
of AFIN. Both TPR and FPR are greatly improved with the BINDWKNN
and BINDRF methods on all app ﬁngerprinting datasets, as indicated in
Table 6. For example, the average TPR resulting from BINDWKNN
method on the APP-FIN dataset is 78%, compared to the average TPR
of 53% reported by the WKNN method. Similarly, the average FPR of
7% reported by the BINDWKNN method is better than the average FPR
of 10% resulting from the WKNN method. This clearly demonstrates
the effectiveness of using BIND features for trafﬁc analysis in AFIN as
well.
Moreover, the average TPR and FPR are largely improved when
using the BINDRF method. It is important to note that while using
monitored and non-monitored traces from different categories, i.e., in
the case of the APP-COMM and APP-SOCIAL datasets, the average
TPR and FPR are better when compared with the results from the
APP-FIN dataset where the monitored and non-monitored sets are
from the same category. Especially, a low FPR of less than 1% is
obtained on these datasets. This indicates that there exist differentiating
characteristics between apps from different categories as expected.
The open-world setting is a binary classiﬁcation problem. Features
extracted and the classiﬁer used for determining class boundary
signiﬁcantly impact the TPR and FPR results. In the case of WKNN,
the monitored entities are made as close as possible via an iterative
weighing mechanism. When using BIND features, we count unique
bi-burst tuples. These provide additional features to the existing
feature set of uni-burst used in [36]. These features aid the weighing
mechanism by bringing out more relevant dimensions, suppressing less
relevant ones in BINDWKNN. Random forest uses decision trees that
divide the feature space effectively using the information gain measure
rather than the Euclidean distance measure used by the k-NN method.
An ensemble of such classiﬁers typically reduces bias and variance
during training, compared to a single classiﬁer [5]. Consequently, this
classiﬁer, along with BIND features, shows superior performance in
TPR results.
4.3.2 Trafﬁc Analysis with Defenses for Website Fin-
gerprinting
We now consider the evaluation of BIND in an adversarial environ-
ment, speciﬁcally for WFIN, similar to relevant studies in this area.
Here, we apply a defense mechanism to trace packets with the aim
of reducing effectiveness of a ﬁngerprinting attack (classiﬁer), and
study the robustness of BIND when used by an attacker against such
defenses.
With defense mechanisms such as Trafﬁc Morphing (TM) used by
defenders to thwart classiﬁers, the features extracted from the data play
an important role while performing an adversarial attack. Table 7 shows
183
Dataset
Method
20
40
60
80
100
s
e
i
t
i
t
n
e
#
VNG++
87.5
83.8
85.2
81.6
82.4
P
93.5
91.4
92.3
88.3
90.3
HTTPS
OSAD BINDSVM
94.0
91.3
91.6
88.4
90.0
94.1
89.0
91.0
87.7
89.2
VNG++
78.0
67.8
63.7
62.9
56.9
TOR
OSAD BINDSVM
86.5
80.9
79.5
77.6
73.9
90.0
92.1
86.7
89.5
85.7
P
85.3
77.6
77.0
75.8
71.4
VNG++
81.3
73.6
72.3
72.8
66.0
APP-FIN
P
92.0
88.3
86.5
88.0
83.1
OSAD BINDSVM
93.3
87.3
86.7
87.8
84.2
88.7
85.1
83.6
79.6
77.2
Table 5: Accuracy (in %) of the closed-world trafﬁc analysis for website ﬁngerprinting (HTTPS and Tor) and app ﬁngerprinting
(App-Finance) without defenses.
HTTPS
Dataset
Method WKNN BINDWKNN BINDRF
98.2
TPR
FPR
18.3
73.0
29.0
91.0
16.0
TOR
WKNN BINDWKNN BINDRF
99.8
3.4
89.6
2.1
90.4
1.9
APP-FIN
WKNN BINDWKNN BINDRF
88.5
1.9
53.0
10.0
78.0
7.0
APP-COMM
WKNN BINDWKNN BINDRF
93.1
0.8
64.0
5.0
82.0
2.0
APP-SOCIAL
WKNN BINDWKNN BINDRF
92.1
0.1
61.0
5.0
75.0
2.0
Table 6: TPR and FPR (in %) of open-world setting for website ﬁngerprinting (HTTPS and Tor) and app ﬁngerprinting (App-Finance,
App-Communication and App-Social) without defenses.
Dataset
Method
s 20
40
60
80
100
e
t
i
s
b
e
w
#
VNG++
79.1
74.4
68.4
61.2
64.1
P
76.0
73.6
68.0
65.1
60.6
HTTPS
OSAD BINDSVM
87.5
82.6
79.7
75.2
73.2
86.6
79.1
74.6
69.8
67.4
Table 7: Accuracy (in %) of closed-world website ﬁngerprinting
on HTTPS dataset with Trafﬁc Morphing.
Dataset
Method
s 20
40
60
80
100
e
t