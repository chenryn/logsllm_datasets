MonitoringMonitoring
Monitoring may have only three output types:
Pages 
	A human must do something now
Tickets 
	A human must do something within a few days
Logging 
No one need look at this output immediately, but it’s available for later analysis if needed
A Collection of Best Practices for Production Services  |  481If it’s important enough to bother a human, it should either require immediate action (i.e., page) or be treated as a bug and entered into your bug-tracking system. Putting alerts into email and hoping that someone will read all of them and notice the impor‐tant ones is the moral equivalent of piping them to /dev/null: they will eventually be ignored. History demonstrates this strategy is an attractive nuisance because it can work for a while, but it relies on eternal human vigilance, and the inevitable outage is thus more severe when it happens.Postmortems
Postmortems (see Chapter 15) should be blameless and focus on process and technol‐ogy, not people. Assume the people involved in an incident are intelligent, are well intentioned, and were making the best choices they could given the information they had available at the time. It follows that we can’t “fix” the people, but must instead fix their environment: e.g., improving system design to avoid entire classes of problems, making the appropriate information easily available, and automatically validating operational decisions to make it difficult to put systems in dangerous states.Capacity Planning
Provision to handle a simultaneous planned and unplanned outage, without making the user experience unacceptable; this results in an "N + 2” configuration, where peak traffic can be handled by N instances (possibly in degraded mode) while the largest 2 instances are unavailable:
• Validate prior demand forecasts against reality until they consistently match. Divergence implies unstable forecasting, inefficient provisioning, and risk of a capacity shortfall.• Use load testing rather than tradition to establish the resource-to-capacity ratio: a cluster of X machines could handle Y queries per second three months ago, but can it still do so given changes to the system?
• Don’t mistake day-one load for steady-state load. Launches often attract more traffic, while they’re also the time you especially want to put the product’s best foot forward. See Chapter 27 and Appendix E.482  |  Appendix B: A Collection of Best Practices for Production Services
Overloads and Failure
Services should produce reasonable but suboptimal results if overloaded. For exam‐ple, Google Search will search a smaller fraction of the index, and stop serving fea‐tures like Instant to continue to provide good quality web search results when overloaded. Search SRE tests web search clusters beyond their rated capacity to ensure they perform acceptably when overloaded with traffic.For times when load is high enough that even degraded responses are too expensive for all queries, practice graceful load shedding, using well-behaved queuing and dynamic timeouts; see Chapter 21. Other techniques include answering requests after a significant delay (“tarpitting”) and choosing a consistent subset of clients to receive errors, preserving a good user experience for the remainder.Retries can amplify low error rates into higher levels of traffic, leading to cascading failures (see Chapter 22). Respond to cascading failures by dropping a fraction of traffic (including retries!) upstream of the system once total load exceeds total capacity.Every client that makes an RPC must implement exponential backoff (with jitter) for retries, to dampen error amplification. Mobile clients are especially troublesome because there may be millions of them and updating their code to fix behavior takes a significant amount of time—possibly weeks—and requires that users install updates.
SRE TeamsSRE teams should spend no more than 50% of their time on operational work (see Chapter 5); operational overflow should be directed to the product development team. Many services also include the product developers in the on-call rotation and ticket handling, even if there is currently no overflow. This provides incentives to design systems that minimize or eliminate operational toil, along with ensuring that the product developers are in touch with the operational side of the service. A regular production meeting between SREs and the development team (see Chapter 31) is also helpful.We’ve found that at least eight people need to be part of the on-call team, in order to avoid fatigue and allow sustainable staffing and low turnover. Preferably, those on-call should be in two well-separated geographic locations (e.g., California and Ire‐land) to provide a better quality of life by avoiding nighttime pages; in this case, six people at each site is the minimum team size.Expect to handle no more than two events per on-call shift (e.g., per 12 hours): it takes time to respond to and fix outages, start the postmortem, and file the resulting bugs. More frequent events may degrade the quality of response, and suggest that
A Collection of Best Practices for Production Services  |  483
something is wrong with (at least one of) the system’s design, monitoring sensitivity, and response to postmortem bugs.Ironically, if you implement these best practices, the SRE team may eventually end up out of practice in responding to incidents due to their infrequency, making a long outage out of a short one. Practice handling hypothetical outages (see “Disaster Role Playing” on page 401) routinely and improve your incident-handling documentation in the process.
484  |  Appendix B: A Collection of Best Practices for Production ServicesAPPENDIX C Example Incident State Document
Shakespeare Sonnet++ Overload: 2015-10-21 
Incident management info: 
(Communications lead to keep summary updated.) 
Summary: Shakespeare search service in cascading failure due to newly discovered sonnet not in search index.
Status: active, incident #465
Command Post(s): #shakespeare on IRC
Command Hierarchy(all responders)
• Current Incident Commander: jennifer• Current Incident Commander: jennifer
	— Operations lead: docbrown
	— Planning lead: jennifer
	— Communications lead: jennifer
• Next Incident Commander: to be determined
(Update at least every four hours and at handoff of Comms Lead role.) Detailed Status (last updated at 2015-10-21 15:28 UTC by jennifer)
Exit Criteria:
• New sonnet added to Shakespeare search corpus TODO• Within availability (99.99%) and latency (99%ile < 100 ms) SLOs for 30+ minutes 	TODO
485
TODO list and bugs filed:
• Run MapReduce job to reindex Shakespeare corpus DONE
• Borrow emergency resources to bring up extra capacity DONE
• Enable flux capacitor to balance load between clusters (Bug 5554823) TODO
Incident timeline(most recent first: times are in UTC)
• 2015-10-21 15:28 UTC jennifer• 2015-10-21 15:28 UTC jennifer
	— Increasing serving capacity globally by 2x
• 2015-10-21 15:21 UTC jennifer
	— Directing all traffic to USA-2 sacrificial cluster and draining traffic from other 	clusters so they can recover from cascading failure while spinning up more 	tasks
	— MapReduce index job complete, awaiting Bigtable replication to all clusters• 2015-10-21 15:10 UTC martym— Adding new sonnet to Shakespeare corpus and starting index MapReduce• 2015-10-21 15:04 UTC martym
	— Obtains text of newly discovered sonnet from shakespeare-discuss@ mailing 	list
• 2015-10-21 15:01 UTC docbrown
	— Incident declared due to cascading failure
• 2015-10-21 14:55 UTC docbrown
	— Pager storm, ManyHttp500s in all clusters
486  |  Appendix C: Example Incident State Document
APPENDIX DAPPENDIX D 
Example Postmortem
Shakespeare Sonnet++ Postmortem (incident #465) 
Date: 2015-10-21 
Authors: jennifer, martym, agoogler 
Status: Complete, action items in progress 
Summary: Shakespeare Search down for 66 minutes during period of very high inter‐est in Shakespeare due to discovery of a new sonnet.
Impact:1 Estimated 1.21B queries lost, no revenue impact.Root Causes:2 Cascading failure due to combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never before appeared in one of Shakespeare’s works, which happened to be the term users searched for. Under nor‐mal circumstances, the rate of task failures due to resource leaks is low enough to be unnoticed.Trigger: Latent bug triggered by sudden increase in traffic.
Resolution: Directed traffic to sacrificial cluster and added 10x capacity to mitigate cascading failure. Updated index deployed, resolving interaction with latent bug. Maintaining extra capacity until surge in public interest in new sonnet passes. Resource leak identified and fix deployed.
1 Impact is the effect on users, revenue, etc.2 An explanation of the circumstances in which this incident happened. It’s often helpful to use a technique 	such as the 5 Whys [Ohn88] to understand the contributing factors.
487
Detection: Borgmon detected high level of HTTP 500s and paged on-call.
Action Items:3
| Action Item | Type | Owner | Bug |
|---|---|---|---|
| Update playbook with instructions for responding to cascading failure |mitigate |jennifer |n/a DONE || Use flux capacitor to balance load between clusters |prevent |martym |Bug 5554823 TODO |
| Schedule cascading failure test during next DiRT |process |docbrown n/a TODO |docbrown n/a TODO |
| Investigate running index MR/fusion continuously |prevent |jennifer |Bug 5554824 TODO |
| Plug file descriptor leak in search ranking subsystem |prevent |agoogler |Bug 5554825 DONE || Add load shedding capabilities to Shakespeare search |prevent |agoogler |Bug 5554826 TODO |
| Build regression tests to ensure servers respond sanely to queries of death |prevent |clarac |Bug 5554827 TODO |
| Deploy updated search ranking subsystem to prod |prevent |jennifer |n/a DONE |
| Freeze production until 2015-11-20 due to error budget exhaustion, or seek |other |docbrown |n/a TODO || exception due to grotesque, unbelievable, bizarre, and unprecedented |other |docbrown |n/a TODO |
| circumstances |other |docbrown |n/a TODO |
Lessons Learned
What went well
• Monitoring quickly alerted us to high rate (reaching ~100%) of HTTP 500s
• Rapidly distributed updated Shakespeare corpus to all clusters
What went wrong
• We’re out of practice in responding to cascading failure• We exceeded our availability error budget (by several orders of magnitude) due 	to the exceptional surge of traffic that essentially all resulted in failures
3 “Knee-jerk” AIs often turn out to be too extreme or costly to implement, and judgment may be needed to re-scope them in a larger context. There’s a risk of over-optimizing for a particular issue, adding specific moni‐toring/alerting when reliable mechanisms like unit tests can catch problems much earlier in the development process.488  |  Appendix D: Example Postmortem
Where we got lucky4
• Mailing list of Shakespeare aficionados had a copy of new sonnet available
• Server logs had stack traces pointing to file descriptor exhaustion as cause for 	crash
• Query-of-death was resolved by pushing new index containing popular search 	term