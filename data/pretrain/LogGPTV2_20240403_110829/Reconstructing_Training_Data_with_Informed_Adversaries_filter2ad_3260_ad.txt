We explore the fidelity of reconstruction on MNIST as the
amount of attack training data k ranges from 100 to 59K.
Figure 5c shows the average MSE between reconstructions
over the 1K released model
targets as k varies. Clearly,
the attack becomes better as more training data is available.
However, high fidelity reconstructions occur already with 1K
shadow models; in our plots we include reconstructed exam-
ples at different values of k illustrating this. Reconstructions
that are (on average) better than the NN oracle only require
8K shadow models. Because the correlation between MSE
and KL is not symmetric, we also plot
the average KL
against attack training set size and observe a similar monotonic
decrease (Figure 5d). We observe similar trends on CIFAR-
10 when increasing the attack training set size; 5K shadow
models is enough to generate reconstructions below the 1st
percentile oracle MSE (~0.05) and 10K shadow models will
generate reconstructions below the NN oracle MSE (~0.03).
See Appendix B for full results on CIFAR-10.
b) Out-of-distribution (OOD) data on CIFAR-10: The
previous experiment indicates that reconstructions are poor
when an adversary has relatively little side-information ( 60% of weights
are not updated during training when the loss is computed
with respect to the target. We suspect this is why RecoNN
is less effective against ReLU activated models: there is less
mutual information between the model parameters and the
target in comparison to models trained with other activations.
We discuss this in further detail in Appendix G.
Learning rate. Decreasing the learning rate of the released
model did not affect the attack in the deterministic training
setting. If randomness is introduced via mini-batch sampling,
we will see that the learning rate impacts reconstruction.
Fig. 6: MSE and released model training batch size when the
adversary knows/does not know the data sub-sampling random
seed. MSE is sensitive to the learning rate and momentum.
Learning rate: 0.01 (left), 0.2 (right). Momentum: 0 (both).
Fig. 7: TSNE embeddings of 1K released models trained with
1024 batch-size. From left to right (known sub-sampling seed,
learning rate): (Yes, 0.01), (No, 0.01), (Yes, 0.2), (No, 0.2).
d) Randomness from data sub-sampling: We explore
how randomness stemming from data sub-sampling affects
the attack on MNIST, by removing the assumption that the
released model is trained with full batch gradient descent.
We consider settings where the adversary knows the random
seed used to shuffle the data (this corresponds to SGD but
with no randomness), and settings where the adversary does
not know the random seed. Results in Figure 6 indicate that
when the adversary knows the data shuffling seed, recon-
struction attacks are successful even for small batch sizes.
Without knowing the seed, attack success depends on the
training hyper-parameters, such as the choice of the learning
rate. It appears that attacking models with randomness from
sub-sampling is more difficult than determinstically trained
released models, and that larger learning rates also increase
the hardness of the reconstruction task. Loss landscapes of
neural networks are extremely non-convex and contain many
local optima [35]; if more randomness is introduced, this will
increase the opportunity for different shadow models to reach
different optima. This increases the difficulty of reconstruction
as these shadow models will not be representative of the
optima attained by the released model, and training with a
larger learning rate will exacerbate this issue. In Figure 7, we
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1146
0.00.10.20.30.40.5LPIPS0.000.050.100.150.20MSE0102030KL0.000.050.100.150.20MSE1005001K2K4K8K16K32K59KAttack training set size102101Average MSETarget: 1005001K2K4K8K16K32K59KAttack training set size101100Average KLTarget: 128256512102420484096fullReleased model training batch size102101Average MSEKnown data seedUnknown data seed128256512102420484096fullReleased model training batch size102101Average MSE402002030201001020300123456789200203020100102030200204030201001020304020020403020100102030show plot TSNE embeddings of parameters for all 1K released
models for each of the two learning rates given in Figure 6
and the two randomness settings (known and unknown seed)
for a batch size of 1024. We represent each released model
with a color depending on the label of the respective target.
For a small learning rate, labels are grouped together in both
known and unknown seed settings, implying the local optima
these models realize are similar; this makes it easier for the
RecoNN to learn and subsequently generalize to the released
model. Conversely, in the large learning rate setting there is
a stark difference between known and unknown seed settings:
if the seed is known, groupings of labels still happen, and a
successful attack is possible; however, if the seed is unknown,
the local optima reached by each released model has less
structure that the reconstructor network can learn on.
e) Randomness from model initialization: We explore
how initialization randomness can affect the attack on MNIST.
Firstly, we remove the assumption that the adversary knows the
initial parameters of the released model; in practice, this means
training each released and shadow model with a new random
seed controlling the model’s initial parameters. By default,
each linear and convolutional layer is initialized with Lecun
Normalization, which is the default in the Haiku library [36].
In our experiments, we evaluated other common initialization
procedures (e.g., Glorot, He), which did not change any of our
findings; we omit these results. We refer the reader to Figure 4
for visual inspection of reconstructions at the two error rates
reported in Table II, and conclude that the attack is unable to
successfully reconstruct without knowledge of initialization, as
they are far larger than the NN oracle described in Section V-B.
One may conjecture that the current attack pipeline is not
suitable for this setting: we only train a single shadow model
per shadow target, which may fail to capture the variance
in shadow model parameters over different initializations for
the same shadow target. For this reason, we further created
an attack training set of 5M shadow model-target pairs,
consisting of 10K shadow targets, where each target has 500
shadow models all differing in initial parameters. Even so, this
approach did not improve the MSE reported in Table II. In Ap-
pendix A, we discuss evidence suggesting that reconstruction
may not be possible without knowing the initial released model
parameters. A similar observation was made by Jagielski et
al. [37], who run attacks to find lower bounds of the privacy
budget ϵ in DP-SGD. They observed that the bounds become
tighter with less randomness from model initialization.
C. Black-box Access to Released Model
We design a black-box attack by limiting the adversary’s
access to only the logits predicted by the released model.
For each shadow model, using a set of 200 (500) images
from ¯D for MNIST (CIFAR-10), the adversary collects the
logit outputs of each image, concatenates them together, and
uses this as the feature representation of the model, instead of
the flattened weights. This reduces the dimensionality of the
feature vector from 8K to 2K for MNIST and 55K to 5K for
CIFAR-10. The average MSE using this logit representation
Fig. 8: Average MSE of reconstructions and test accuracy of
released model using (ϵ, δ)-DP on the MNIST dataset.
Fig. 9: Example of MNIST reconstructions under DP.
approach is 0.011 for MNIST and 0.0198 for CIFAR-10,
which is only marginally worse than the MSE of white-box
attacks with default settings, and still much better that the NN
oracle. We conclude that black-box reconstruction attacks are
feasible and have comparable performance to white-box ones.
D. Released Model Trained with Differential Privacy
Having discussed what factors help and hinder reconstruc-
tion, we now evaluate on MNIST the resilience of models
trained with DP. The released model training set-up is identical
to before (Section IV-B), except we train with full batch
DP gradient descent (DP-GD) with clipped gradients [15].
Gradients are clipped to have a maximum ℓ2 norm of 1,
and Gaussian noise (unknown to the adversary) is added
to make the model (ϵ, δ)-DP with δ = 10−5. Figure 8
shows that even a large ϵ successfully mitigates reconstruction
attacks, and that in these ϵ regimes the reduction in utility
(measured by test accuracy) is negligible (Appendix E reports
similar results on CIFAR-10). Interestingly, for high levels of
privacy, the reconstruction attack generates realistic but wildly
incorrect reconstructions (Figure 9). These findings motivate
our theoretical investigation into what level of DP is sufficient
to protect against reconstruction attacks.
VII. TOWARDS FORMAL GUARANTEES AGAINST
RECONSTRUCTION ATTACKS
Mitigations that (provably) protect released models against
reconstruction attacks can (and should) be implemented within
the training algorithm used by the model developer. Protec-
tions that defend against effective reconstruction by informed
adversaries will also protect against attacks by weaker, more
realistic adversaries. In this section, we propose a definition
of reconstruction robustness against informed adversaries, and
compare it to the privacy guarantees afforded by DP. As will
soon become apparent,
the strength of mitigations against
reconstruction is necessarily going to be relative to the strength
of the prior information available to the adversary.
A. Reconstruction Robustness
Our main definition focuses on bounding the success prob-
ability of achieving accurate reconstruction by any (informed)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1147
100101102103104105106CliponlyNoDP-GD0.0250.0500.0750.1000.125Average MSE8688909294Test accuracy=100=101=102=103=104=105=106CliponlyNoDP-GDTargetadversary. The definition is parameterized by the side informa-
tion available to the adversary, captured by a probabilistic prior
π from which the target z is sampled, and by the adversary’s
goal expressed as a measure of reconstruction error ℓ.
Definition 2. Let π by a prior over Z and ℓ : Z×Z → R≥0 a
reconstruction error function. A randomized mechanism M :
Z n → Θ is (η, γ)-ReRo (reconstruction robust) with respect to
π and ℓ if for any dataset D- ∈ Z n−1 and any reconstruction
attack R : Θ → Z we have
PZ∼π,θ∼M (D-∪{Z})[ℓ(Z, R(θ)) ≤ η] ≤ γ .
(2)
Suppose M is an (η, γ)-ReRo mechanism. The definition
prevents any reconstruction attack with knowledge3 of the
prior π, the dataset D- and the output θ = M (D- ∪ {Z})
to attain a reconstruction error lower than η on an unknown
target Z ∼ π with probability larger than γ. A good ReRo
mechanism is one with large η and very small γ, i.e. one
where even “decent” reconstructions are impossible with high
probability. In practice a tension between these two parameters
is expected, at least for mechanisms providing some form of
utility when computing a function depending on all the inputs.
Definition 2 assumes the reconstruction attack is determin-
istic. We could consider randomized attacks instead, but note
that determinism is not a limitation when trying to capture
worst-case attacks: the R that maximizes P[ℓ(Z, R(θ)) ≤ η]
is given by the (deterministic) maximum a posteriori attack:
R∗(θ) = argmaxˆz∈Z PZ∼π[ℓ(Z, ˆz) ≤ η|M (D- ∪ {Z}) = θ] .
Similarly, the definition protects against adversaries with full
knowledge of the prior π. Since the optimal attack run by an
adversary with a wrong prior is necessarily weaker than the
optimal attack with a correct prior, assuming the adversary
knows π is preferable when designing mitigations.
Our main results provide two connections between recon-
struction robustness and DP. The first observation is that
DP implies reconstruction robustness. Quantitatively, we show
that the ReRo parameters of a R´enyi DP (RDP) mechanism
depend in a simple way on its privacy parameters and another
quantity capturing the relation between π and ℓ. The second
observation is that any mechanism that is robust against exact
reconstruction with respect to a sufficiently rich family of
priors supported on pairs of points must satisfy DP. Together,
both results stress the importance of correctly modelling an
adversary’s prior knowledge in effectively protecting against
reconstruction attacks. In particular, we show that very weak
DP guarantees suffice to protect against reconstruction when
the adversary has limited knowledge about the target point.
B. From DP to ReRo
We now show that differentially private mechanisms provide
the definitions of
reconstruction robustness. Let us recall
approximate and R´enyi DP.
3Knowledge of π and D in the attack is implicit through the fact that (2)
has to hold for any reconstruction attack.
Definition 3 ([6, 38, 39]). Let M : Z n → Θ be a randomized
mechanism, ϵ > 0, δ ∈ [0, 1] and α > 1. We say that:
1) M is (ϵ, δ)-DP if for any datasets D, D′ ∈ Z n differing
in a single record and any event E ⊆ Θ we have
P[M (D) ∈ E] − eϵP[M (D′) ∈ E] ≤ δ .
When δ = 0 we simply say the mechanism is ϵ-DP.
2) M is (α, ϵ)-RDP if for any datasets D, D′ ∈ Z n differing
in a single record we have
(cid:20)(cid:18) P[M (D) = θ]
P[M (D′) = θ]
(cid:19)α(cid:21)
Eθ∼M (D′)
≤ e(α−1)ϵ .
The effect of the prior on ReRo bounds obtained from DP
is through an anti-concentration property. For prior π, error
function ℓ and error threshold η, define the baseline error as
κπ,ℓ(η) = sup
z0∈Z
PZ∼π[ℓ(Z, z0) ≤ η] .
When π, ℓ or η are clear from the context we may drop them
to unclutter our notation. Whenever ℓ is a metric on Z, an
upper bound on κ provides a measure of anti-concentration
of the prior by guaranteeing that no single point has too
much of probability mass concentrated around it; bounds for
κ for some prior distributions are given in Section VII-D.
Another interpretation of κ is as the success probability of
the best oblivious reconstruction attack that ignores the output
of M. By this interpretation, the next theorem says that if a
mechanism is RDP, the best reconstruction attack cannot have
success probability much larger than the best oblivious attack.
Theorem 2. Fix π, ℓ and η > 0, and let κ = κπ,ℓ(η). If a
mechanism M satisfies (α, ϵ)-RDP then it also satisfies (η, γ)-
ReRo with respect to π and ℓ with γ = (κ · eϵ)
α−1
α .
Taking α → ∞ and recalling that (∞, ϵ)-RDP is equivalent
to ϵ-DP [39] we obtain the following corollary.
Corollary 3. Fix π, ℓ and η > 0, and let κ = κπ,ℓ(η). If a
mechanism M satisfies ϵ-DP then it also satisfies (η, γ)-ReRo
with respect to π and ℓ with γ = κ · eϵ.