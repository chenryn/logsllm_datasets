a similar trend in the k-medians clustering method as well.
Logarithmic clustering generally achieves higher accuracy
109diﬀerence between the diﬀerent absolute error CDFs (and
hence, refrain from showing the actual plots) between the
sampled and unsampled variants. One could imagine this
happens because most packet delays are clustered to stati-
cally chosen centers instead of those close to the k-medians.
However, this is not the case, as only 53% and 1.5% packets
are clustered to those static centers at high and low levels
of link utilization respectively. The actual reason is that
the k centers output by the algorithm are similar even when
we sample packets. We observe that the cosine similarity
(deﬁned as cos ϕ = A·B|A||B| ) between two vectors A and B
of k centers output by sampled and unsampled k-medians
algorithm is over 0.98 for sampling rates as low as 1%.
Number of centers vs. running time. The running
time of the oﬄine algorithm depends on the ﬁnal number of
centers required, i.e., k, and the number of candidate centers
output by the online clustering stage. As k increases, the
running time of the algorithm increases, but the resulting
error also decreases. This is the main trade-oﬀ involved in
choosing an appropriate value of k. Figure 5(b) explores this
trade-oﬀ under moderate (58%) link utilization scenario as
k is increased from 10 to 100. (We only plot the curve corre-
sponding to the sampling rate 0.1 here, but other sampling
rates also exhibited similar trends.) From the plot, we can
clearly observe the sweet spot that represents a reasonable
trade-oﬀ between running time and average absolute error
is k = 50. We can possibly choose up to 70 or 80 centers as
well as the running time is less than the epoch interval (1s).
Of course, depending on the target platform and computa-
tional resources we can expect on the processor, the number
of centers may vary. But this trade-oﬀ implies we can easily
determine the value of k appropriate for the target platform.
5.3 Comparison of data structures for PLS
Next, we compare the performance of various data struc-
tures for PLS—SVBF, PBF and COMB. For fair compar-
isons, we conﬁgure all of these with the same 5Mbit memory
in total. It is not easy to ﬁx the amount of memory in PBF
since the total memory needs to be explicitly partitioned
across all the BFs. We split the total memory across each
BF proportional to the number of packets that are mapped
to a given BF (according to the frequency counts two epochs
back). For the others, we derive the optimal parameter con-
ﬁgurations, such as number of hash functions, for diﬀerent
data structures using the formulae in §3.4. For this memory,
the theoretical analysis suggests using 9 hash functions for
SVBF and PBF. For COMB, there are two other param-
eters, θ (number of bits that need to be set in the group
code) and f (length of the code). Out of feasible combina-
tions to support k = 50 groups, we choose the conﬁguration
with f =8 and θ=3 that has the smallest lookup time (that is
proportional to f ). By ﬁxing these parameters, the number
of hash functions per bit, h needs to be set to 3 according
to the analysis in §3.4. Thus, all in all, we ensured that
the comparisons are as fair as possible between the various
schemes.
In our comparisons, we mainly study the classiﬁcation fail-
ure rate, false classiﬁcation rate, and ﬁnally the impact of
these on the accuracy of latency estimates. We search laten-
cies of all packets for 58 epochs (the ﬁrst 2 epochs are used
for clustering only). For false classiﬁcation rate, we use the
tie-breaking heuristic described in §3.3.4 and compute the
rate at which the heuristic leads to an incorrect answer.
(a) Clustering schemes
(b) Tradeoﬀ in deciding centers
Figure 5: Comparing diﬀerent clustering schemes
and exploring trade-oﬀ between average error and
maximum running time for oﬄine stage.
than other methods in terms of relative error because its
centers are placed at equal distances within each sub-range
(e.g., 1-10µs and 10-100µs). On the contrary, analyses on
absolute error suggest that the logarithmic clustering suf-
fers from inaccurate estimates at higher percentiles. For in-
stance, k-medians clustering is almost twice more accurate
than logarithmic clustering at 90 %ile (35µs absolute error
in k-medians and 67µs in logarithmic) in Figure 5(a). This
basically stems from the logarithmic clustering’s failing to
adjust its centers as packet delays vary. This may further
worsen its accuracy as conﬁgurations such as link capacity
may change in future. Comparatively, the other schemes
adapt to this trend quickly and place more centers close to
where the actual delays are, leading to better accuracy.
The k-medians clustering method has the similar perfor-
mance that the oracle has in terms of both absolute and rel-
ative errors under low and moderate utilization cases, and
even till the 3rd quartile of high utilization case. At top
25 %ile of high utilization scenario (Figure 5(a)), accuracy
of the k-medians is worse than that of oracle because of
the inherent variations across epochs; this is in essence the
price we pay for an online clustering algorithm. Finally, we
observe that the hybrid clustering approach balances both
absolute and relative errors by inheriting the good proper-
ties of static and dynamic center determination approaches.
For instance, in Figure 5(a), we see how the hybrid scheme
inherits the better accuracy of logarithmic approach up to
50 %ile and better accuracy of k-medians at top 50 %ile.
Henceforth, unless otherwise mentioned, we use the hybrid
scheme in the rest of the paper.
Impact of packet sampling. In §3.2, we discussed that
we employ packet sampling in the clustering phase to reduce
the processing overhead. We now study the impact of vary-
ing the sampling rate on the accuracy of the per-packet la-
tency estimates. In our experiments, we found virtually no
 0 0.2 0.4 0.6 0.8 1100101102103CDFAbsolute error (µs)oraclehybridk-medianslogarithmic 0 0.5 1 1.5 2 2.5 3 3.5 10 30 50 70 90 0 0.3 0.6 0.9 1.2 1.5 1.8 2.1Avg. abs. error (µs)Max runtime (sec.)Number of centersAverage errorRuntime110(a) Classiﬁcation failure
(b) False classiﬁcation
(c) Estimation accuracy
Figure 6: Analysis of classiﬁcation failure, false classiﬁcation, and estimation accuracy.
Classiﬁcation failure and false classiﬁcation rates.
We show the classiﬁcation failure rate of each data struc-
ture in Figure 6(a). SVBF and PBF (as expected) achieve
least classiﬁcation failure rate of about 10% at most across
all epochs, while COMB obtains 50% at most—almost 5×
higher than SVBF. Note that for PBF due to the fact that
for a given BF, the number of packets may exceed the capac-
ity, we observed almost 24% of packets could not be admit-
ted altogether (false negatives). Applying the tie-breaking
heuristic results in a false classiﬁcation rate that is lower
than the classiﬁcation failure rate, but not by much. Still,
as shown in the Figure 6(b), the median false classiﬁcation
rate of COMB is almost 12× higher than SVBF. This shows
the eﬃcacy of our SVBF compared to existing data struc-
tures such as COMB. As shown in Table 1, COMB requires
almost twice the number of bits per packet to achieve the
classiﬁcation failure rates as SVBF. We next study how this
decrease in classiﬁcation failure eﬀects the actual delay es-
timation.
Accuracy of per-packet latency estimation. Figure 6(c)
shows absolute errors of per-packet latency estimates for
three data structures. In addition to them, there is an ad-
ditional curve titled ‘Clustering’ that essentially assumes a
perfect data structure, but does not use the reﬁned latency
estimates (described in §3.3) using the observed mean of the
data packets. (We can always plot that too, and that would
strictly be better than the rest, but we chose this as a nice
reference point to see the eﬀects of the reﬁnement.)
We show mainly the upper quartile in this graph where the
diﬀerence is the most pronounced. Clearly, at lower than 75
%ile, either SVBF or COMB would return the same (correct)
group id if the packet is not misclassiﬁed; it is only for the
misclassiﬁed packets that the accuracy is likely to be worse
since the tie-breaking heuristic may pick the wrong latency
estimate for the packet. (If we choose not to report them,
then they will be counted as false negatives.) We can notice
that COMB and PBF suﬀer from much higher discrepancies
as early as the 70 %ile onwards, while in contrast we can see
that the Clustering and SVBF have an absolute error that is
signiﬁcantly lower in comparison. For example the 85 %ile
absolute error for COMB is close to 116µs while SVBF has
an error of 19µs at the same percentile. From the ﬁgure, we
can see that not until almost the 98 %ile onward do we see
any diﬀerence between Clustering and SVBF.
Insert and lookup time complexity. We study the com-
plexity of insert and lookup time of each data structure.
We tested 0.4 million packets for insert and lookup using
(a) Insert
(b) Lookup
Figure 7: Comparison of insert and lookup times.
a Linux machine with 2.66GHz Intel CPU. Figure 7 shows
the complexity in microsecond precision. From the ﬁgure,
we observe that SVBF works faster than COMB in both
insert (45% gain on average) and lookup (28%) operations.
These experimental gains of SVBF are close to the theoreti-
cal ones which are 50% ((18-9)/18) for insert and 44% ((48-
27)/48) for lookup based on the number of memory accesses
in Table 1. While PBF achieves the same performance of
SVBF for insert operation, PBF is four times slower than
SVBF for lookup operation on average. Note that SVBF
is implemented as software and can be optimized further in
hardware platform.
5.4 Query interface
Since our architecture can support querying any packet, it
can allow the querying host to compute aggregate statistics
across arbitrary traﬃc sub-populations.
Accuracy of aggregate statistics. We ﬁrst verify the ac-
curacy of obtained aggregate statistics (by querying packets
that belong to that aggregate) at diﬀerent levels–sub-ﬂow,
ﬂow, host, and preﬁx/16. By performing ﬂow-level aggrega-
tion, i.e., by grouping packets with the same ﬂow key, our
 0 0.2 0.4 0.6 0.8 110-210-1100CDFClassiﬁcation failure rateSVBFCOMBPBF 0 0.2 0.4 0.6 0.8 110-210-1100CDFFalse classiﬁcation rate 0.7 0.75 0.8 0.85 0.9 0.95 1101102103104CDFAbsolute error (µs)SVBFCOMBPBFClustering 0 0.2 0.4 0.6 0.8 1100101CDFTime (microsecond)SVBFCOMBPBF 0 0.2 0.4 0.6 0.8 1100101102CDFTime (microsecond)111(a) Accuracy of aggregates
(b) Query bandwidth compression
(c) Impact of query timing
Figure 8: Average latency statistics at diﬀerent aggregation levels, query message compression ratio depend-
ing on ﬂow size, and impact of query timing using high utilization scenario.
architecture achieves similar functionality as previous work
RLI [27]. However, perhaps more unique to our architecture,
due to the fact that it stores measurements on a per-packet
basis, we can choose to aggregate at sub-ﬂow level, while
RLI cannot easily achieve this. We compute the average
delay of 10 consecutive packets within the same ﬂow key
(among large ﬂows whose size is more than 10 packets) as
the sub-ﬂow average delay. Such a feature could be useful,
for instance, to understand which set of packets within a
large ﬂow are exhibiting abnormal latencies.
Figure 8(a) shows the aggregate statistics in terms of rela-
tive error for the high link utilization scenario. We also draw
a curve for packet latencies as a reference curve. From the
ﬁgure, we observe that as aggregation level becomes higher,
relative error reduces. Latency estimates at sub-ﬂow level,
however, have the least relative errors. This is not inconsis-
tent, since many ﬂow/host/preﬁx-level statistics, although
aggregated with packets within a given epoch, are computed
with only a single packet (46% ﬂows, 41% hosts, and 13%
preﬁx/16), while sub-ﬂow statistics are computed for ﬂows
that at least have 10 packets. Thus, sub-ﬂow latency esti-
mates get more inﬂuence on canceling individual errors out
by aggregation. Speciﬁcally, median relative error is 5.5%
at packet level, 3.9% at ﬂow level, 3.6% at host level, 2.1%
at preﬁx/16 level and 1.9% at sub-ﬂow level. Similar trends
are found under low and moderate utilization scenarios.
In terms of absolute errors, we observe that preﬁx/16 av-
erage latency is more accurate than other aggregation levels,
at low link utilization. As link utilization increases, however,
we ﬁnd little diﬀerence in absolute error among all four ag-
gregation levels. We omit graphs for brevity, but we observe
a 95 %ile absolute error of less than 0.05µs, 2µs and 55µs
across all four aggregation levels.
Compression of query bandwidth with IP ids. We
study the query bandwidth saving using the IPID idea out-
lined in §4. For each ﬂow within an epoch, we compare the
bandwidth of range query messages with individual packet
queries. Figure 8(b) shows the bar graph of compression ra-
tio depending on ﬂow size for 60 epochs with ISP, UNIV1
and UNIV2 traces. The bar denotes average compression
ratio and the whisker bar means 75 %ile value from the
ﬁgure. We observe that as ﬂow size increases, higher query
bandwidth saving is achieved (for ﬂows with more than 1000
packets in UNIV1, average compression ratio is 6%—17×
less bandwidth than the naive packet query method). We
also observe that more compression at larger ﬂow sizes is
achieved with data center traces compared to ISP trace.
Figure 9: Comparison of ﬂow estimates with RLI.
Impact of inaccurate query timing. We evaluate the
impact of inaccurate query timing when clients issue per-
packet latencies. In the experiments, all packet queries in
an epoch i are asked to a SVBF of that epoch (true SVBF)
and additional b number of SVBFs of previous epochs i − b
(bogus SVBFs). Multiple matches are resolved using the
same tie-breaking heuristic.
In Figure 8(c), we show the
results for 2 bogus SVBFs. Clearly, as the number of epochs
considered increases, the accuracy decreases slightly. For
instance, 95 %ile absolute error shifts from 70 to 89 to 125µs
as bogus SVBFs are added, but the 75 %ile errors are not
that impacted, increasing the error from 10.7 to 12.2µs.
5.5 Comparison with prior architecture
We now compare our MAPLE architecture with RLI [27],
that also averages approximate latencies of packets (obtained
via latency interpolation) that belong to a ﬂow. MAPLE
uses accurate packet latencies (using timestamps) but stores
them approximately in the SVBF data structure. MAPLE
can also use RLI-approximated latency for each packet, but
this leads to two sets of approximations (we denote this as
MAPLE-RLI). We study these various eﬀects in Figure 9.
(For brevity, we only show the high utilization curve; the
trends for the other two utilizations were similar.)
We make two observations in Figure 9. First, there is lit-
tle diﬀerence in absolute error between MAPLE-RLI and
RLI, while RLI has slightly higher accuracy than MAPLE-
RLI, which is expected. Speciﬁcally, under high utilization
scenario, RLI has 76µs absolute error at 95 percentile, but
MAPLE-RLI has 102µs at the same percentile. Median er-
ror by RLI is 15µs and the error of MAPLE-RLI is 17µs.