





















































(a) Base (Post-Pos), bias sum: ‚àí1405
(b) Base-Pos, bias sum: 711932
(c) Base-Cut, bias sum: ‚àí137449
(d) Norm, bias sum: 0
(e) Norm-Mul, bias sum: 0
(f) Norm-Cut, bias sum: 0
(g) Norm-Sub, bias sum: 0
(h) Power, bias sum: ‚àí96332
(i) PowerNS, bias sum: 0
Fig. 2. Bias of count estimation for the Zipf‚Äôs dataset Ô¨Åxing  = 1.
applies, with the difference that the threshold in Norm-Cut
is typically smaller. For Norm-Sub, each value is inÔ¨Çuenced
by two factors: subtraction by a same amount; and converting
to 0 if negative. For the high-frequency values, we mostly
see the Ô¨Årst factor; for the low-frequency values, they are
mostly affected by the second factor; and for the values in
between, the two factors compete against each other. We see
an increasing line for Norm-Sub. Finally, Power changes little
to the top estimations; but more to the low ones, thus leading
to a similar shape as Norm-Cut. The shape of PowerNS is
close to Power because PowerNS applies Norm-Sub, which
subtract some amount to the estimations, after Power.
Figure 3 shows the variance of the estimations among the
5000 runs. First of all, the variance is similar for all the values
in Base and Norm, with Norm being slightly better (smaller)
than Base. For all other methods, the variance drops with the
rank, because for low-frequency values, their estimates are
mostly zeros.
C. Full-domain Evaluation
Figure 4 shows MSE when querying the frequency of every
value in the domain. Note that The MSE is composed of the
(square of) bias shown in Figure 2 and variance in Figure 3.
We vary  from 0.2 to 4. Let us Ô¨Åst focus on the Ô¨Ågures on the
left. Base performs very close to Norm, since the adjustment of
Norm can be either positive or negative as the expected value
of the estimation sum is 1. As Base-Pos (which is equivalent to
Post-Pos in this setting) converts negative results to 0, its MSE
is around half that of Base (note the y-axis is in log-scale).
Norm-Sub is able to reduce the MSE of Base by about a factor
of 10 and 100 in the Zipfs and Emoji dataset respectively.
Norm-Mul behaves differently from other methods. In par-
ticular, the MSE decreases much slower than other methods.
This is because Norm-Mul multiplies the original estimations
by the same factor. The higher the estimate, the greater the
adjustment. Since the estimations are individually unbiased,
this is not the correct adjustment.
For the right part of Figure 4, we observe that, Norm-Sub
and MLE-Apx perform almost exactly the same, validating
the prediction from theoretical analysis. Norm-Sub, MLE-
Apx, Power, PowerNS, and Base-Cut perform very similarly.
In these two datasets, PowerNS performs the best. Note that
PowerNS works well when the distribution is close to Power-
Law. For an unknown distribution, we still recommend Base-
Cut. This is because if one considers average accuracy of all
estimations, the dominating source of errors comes from the
fact many values have true frequencies close or equal to 0
are randomly perturbed. And Base-Cut maintains the high-
frequency values unchanged, and converts results below a
threshold T to 0. Norm-Cut also converts low estimations to
0, but the threshold Œ∏ is likely to be lower than T , because Œ∏
is chosen to achieve a sum of 1.
BeneÔ¨Åt of Post-Processing. We demonstrate the beneÔ¨Åt of
post-processing by measuring the relationship between n and
n(cid:48), so that n records with post-processing can achieve the same
accuracy for n(cid:48) records without it. In particular, we vary n and
measure the errors for different methods. We then calculate
n(cid:48) using Equation 3. In particular, the analytical MSE for n(cid:48)
9









































































(a) Base (Post-Pos)
(b) Base-Pos
(c) Base-Cut
(d) Norm
(e) Norm-Mul
(f) Norm-Cut
(g) Norm-Sub
(h) Power
(i) PowerNS
Fig. 3. Variance of count estimation of the Zipf‚Äôs dataset Ô¨Åxing  = 1. The y-axes are scaled down by n = 106 (a value a in the Ô¨Ågure represents a ¬∑ 106).
records is
1
d
(cid:88)
v
œÉ2
v =
=
q(1 ‚àí q)
n(cid:48)(p ‚àí q)2 +
q(1 ‚àí q)
n(cid:48)(p ‚àí q)2 +
1
d
1
d
(cid:88)
fv(1 ‚àí p ‚àí q)
n(cid:48)(p ‚àí q)
v
1 ‚àí p ‚àí q
n(cid:48)(p ‚àí q)
.
Given the empirical MSE, we can obtain n(cid:48) that achieves the
same error analytically. Note that the MSE does not depend on
the distribution. Thus we only evaluate on the Zipf‚Äôs dataset.
The result is shown in Figure 5. We vary the size of the dataset
n and plot the value of n(cid:48) (note that the x-axes are in the scale
of 106 and y-axes are 107). The higher the line, the better the
method performs. Base and Norm are two straight lines with
the slope of 1, verifying the analytical variance. The y value
for Norm-Mul grows even slower than Base, indicating the
harm of using Norm-Mul as a post-processing method. The
performance of the other methods follow the similar trend of
the full-domain MSE (as shown in the upper row of Figure 4),
with PowerNS gives the best performance, which saves around
90% of users.
D. Set-value Evaluation
Estimating set-values plays an important role in the inter-
active data analysis setting (e.g., estimating which category
of emoji‚Äôs is more popular). Keeping  = 1, we evaluate the
performance of different methods by changing the size of the
set. For the set-value queries, we uniformly sample œÅ% √ó |D|
elements from the domain and evaluate the MSE between
the sum of their true frequencies and estimated frequencies.
10
œÅ%√ó|D| elements; and deÔ¨Åne fDsœÅ =(cid:80)
Formally, deÔ¨Åne DsœÅ as the random subset of D that has
fv. We sample
DsœÅ multiple times and measure MSE between fDsœÅ and f(cid:48)
.
DsœÅ
Overall, the error MSE of set-value queries is greater than that
for the full-domain evaluation, because the error for individual
estimation accumulates.
v‚ààDsœÅ
Vary œÅ from 10 to 90. Following the layout convention,
we show results for set-value estimations in Figure 6, where
we Ô¨Årst vary œÅ from 10 to 90. Overall,
the approaches
that exploits the summing-to-1 requirement, including Norm,
Norm-Mul, Norm-Sub, MLE-Apx, Norm-Cut, and PowerNS,
perform well, especially when œÅ is large. Moreover, their MSE
is symmetric with œÅ = 50. This is because as the results are
normalized, estimating set-values for œÅ > 50 equals estimating
the rest. When œÅ = 90, the best norm-based method, PowerNS,
outperforms any of the non-norm based methods by at least 2
orders of magnitude.
it
For each speciÔ¨Åc method,
is observed the MSE for
Base-Pos is higher than other methods, because it only turns
negative estimates to 0, introducing systematic bias. Post-Pos
is slightly better than Base, as it turns negative query results
to 0. In the settings we evaluated, Base-Cut also outperforms
Base; this happens because converting estimates below the
threshold T to 0 is more likely to make the summation f(cid:48)
D
close to one. Finally, Power only converts negative estimations
to be positive, introducing systematic bias; PowerNS further
makes them sum to 1, thus achieving better utility than all

























































Zipf‚Äôs
Emoji
Fig. 4. MSE results on full-domain estimation, varying  from 0.2 to 4.
Fig. 5. MSE results on full-domain estimation on Zipfs dataset, comparing n with n(cid:48), Ô¨Åxing  = 1 while varying n from 0.2 √ó 106 to 2.0 √ó 106. Three
pairs of methods have similar performance: Base and Norm, Base-Pos and Post-Pos, Norm-Sub and MLE-Apx.
other methods.
Vary œÅ from 1 to 10. Having examined the performance
of set-queries for larger œÅ, we then vary œÅ from 1 to 10 and
demonstrate the results in Figure 7. Within this œÅ range, the
errors of all methods increase with œÅ, which is as expected.
When œÅ becomes small, the performance of different methods
approaches to that of full-domain estimation.
Norm-Cut varies the threshold so that after cutting, the
remaining estimates sum up to one. Thus the performance of