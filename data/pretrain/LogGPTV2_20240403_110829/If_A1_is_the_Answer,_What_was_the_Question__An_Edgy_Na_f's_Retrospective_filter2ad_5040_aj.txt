producing 
they  would  be 
evaluated  against  a  published,  established  standard. 
Beginning  in  the  late  1970s,  several  funded  and 
commercial  efforts  were  underway  to  produce  trusted 
systems  or  tools  for  the  construction/verification  of 
trusted  systems.  So  the  TCSEC  or  something  very 
much  like  it  needed  to  be  published,  and  publication 
needed to happen no later than the 1980s.  
Unfortunately,  there  was  a  shortage  of  adequately 
educated  and  experienced  developers  of 
trusted 
operating systems. While there were several laboratory 
prototypes,  only  Multics  (AIM)  had  a  developed  user 
community,  and  Multics  was  the  only  robust  security 
product  on  the  open  market.  Multics  provided  both 
multilevel  security  capabilities  and  a  structured  set  of 
advanced  integrity  controls.  But  it  was  not  widely 
available,  its  hardware  base  was  not  as  popular  as  the 
less  costly  IBM  or  DEC  mainframes,  and  its  user 
interface  was  not  as  friendly  as  the  increasingly 
popular, but vulnerable, UNIX. 
Unfortunate,  also,  was  the  consequence  of  the  lack 
of  experienced  trusted  system  developers  who  were 
willing  and  able  to  be  evaluators.  Many  wanted  to 
create  a  product  rather  than  to  “look  over  someone 
else’s  shoulder.”  The  lengthy  and  overly  cautious 
evaluation  and  interpretation  process  ended  up  killing 
off  vendor  participation 
trusted  product 
development.  This  was 
the 
uncertainty  of  the  costs  and  time  associated  with 
getting a product evaluated. 
largely  because  of 
and 
to 
We 
failed 
think  of  asking  experienced 
procurement  officers  to  review  our  wording,  and  no 
one  aggressively  thought  of  making  sure  that  we  had 
written  a  sufficient  and  complete  glossary  of  technical 
terms and concepts. Indeed, the TCSEC’s glossary was 
something  of  an  afterthought,  and it was not given the 
careful  attention  that  the  main  body  of  the  text  was 
given.  This  oversight  was  a  significant  cause  of  the 
lengthy interpretation process. 
Another  significant  problem  was  our  neglecting  to 
write down what we considered to be obvious: the fact 
that  we,  the  principal  authors,  did  not  consider  all 
features  and  assurances  to  be  created  equal.  In  a  bad 
paraphrase  of  George  Orwell,  “Some  assurances  are
more  equal  than  others.”  Had  we  stated,  e.g.,  that 
individual accountability under DAC is less significant 
than  assured  individual  accountability  under  MAC, 
many  bitter  and  divisive  diversions  would  have  been 
avoided  –  and  possibly  more  A1  products  would  have 
been produced. 
I do not question the wisdom of our decision to limit 
the  TCSEC  to  its  seven  all-or-nothing  classes  rather 
than  taking  the  Chinese-menu  approach  that  was 
advocated at the time. I think this was the right thing to 
do.  In  that  sense,  I  consider  the  TCSEC  to  be  an 
improvement  over  the  criteria  created  afterwards, 
particularly 
the  swollen  and  confusing  Common 
Criteria  with  its  extensible  myriad  of  Protection 
Profiles  and  I  think  it  is  harmful  to  “roll  your  own  if 
you  don’t  like  what’s  there.”  True,  this  puts  the 
interpretation in front of the evaluation, but it also has 
the  capacity  of  producing  a  huge  number  of  slightly 
different  policies  or  assurances  that  will  be  very 
difficult  for  sophisticated  consumers  to  compare  or 
accurately comprehend. 
I  am  very  much  bothered  by  the  way  the  industry 
has  moved.  Today,  a  generation  after  the  début  of  the 
DoD  Computer  Security  Initiative  and  the  publication 
of  the  TCSEC,  there  are  essentially  no  commercially 
available  trusted  systems  in  use  offering  protection 
equivalent to a equivalent to a B2 Multics or the A1 M-
component GEMSOS. [30] 
illusion  of  system  security 
Instead,  there  are  bloated,  untested,  feature-laden 
interoperating untrustworthy less-than-C2 products that 
are  self-penetrating.  Their  alleged  kernels  consist  of 
millions  of  lines  of  highly  privileged  code  written  by 
teams  of  people  who’ve  never  met  their  coding 
counterparts.  The 
is 
provided  by  software  encryption  algorithms  that  can 
often  be  coaxed  to  reveal  their  keys  to  a  skilled 
interloper.  Add-on  security  gadgetry  in  the  form  of 
pattern-matching  virus 
restrictive 
firewalls  belie  the  vendors’  claims  of  mature  security 
architectures.  And, 
periodic 
announcement  of  urgent  several  megabyte  security 
patches  only  emphasizes  the  tawdry  state  of  today’s 
commercial offerings. 
scanners 
course, 
and 
of 
the 
Never  has  compromising  a  system  been  easier! 
Never  have  so  many  effective  penetration  tools  been 
provided  off-the-shelf  by  the  vendor  to  the  would-be 
interloper! 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply. 
Also,  one  cannot  but  comment  adversely  on  the 
current issue of electronic touch-screen voting systems. 
In  at  least  one  state,  Maryland,  the  only  legal  way  to 
vote  is  on  a  system  that  uses  cryptography  for  some 
aspects  of  secrecy,  but  which  is  implemented  on  a 
version of Windows CE – a foundation that would not 
meet the unexacting standards of the TCSEC C1 class. 
Attacks against Windows operating system variants are 
common  place,  and  the  vendor’s  flagship  C2  systems 
(NT  and  2000)  require  regular  security  patching 
because  of  Internet  Malware,  with  no  one  questioning 
the  presence  of  its  gaping  Active  Desktop  and  other 
inviting  security  vulnerabilities.  Several  security 
studies  were  conducted  that  identified  voting  system 
security  flaws,  and  of these several could be exploited 
through  a  prepared  attack.  The  fact  that  there  is  no 
permanent  and  immutable  audit  trail  and  recovery 
system  has  been  discussed  and  dismissed  by  the 
manufacturer  and  by  the  state  election  board.18  Most 
recently,  the  Maryland  court  system  has  dismissed 
concerns  over  the  machines’  security  on  the  grounds 
that the system is not going to be connected to hackers, 
need  not  to  withstand  “military  style  attacks,”  and  so 
where is the security threat? Surely, no one would want 
to invest expensive technical effort into controlling the 
results  of  a  national  election!  O  where  have  we  heard 
these questions before?! 
The TCSEC was written and emended by the skilled 
computer  security  practitioners  of  the  late  1970s  and 
early  1980s.  The  derivative  criteria,  though  written  by 
large  committees  of  skilled  personnel,  reflect  the  fact 
that they were written by committee, and with the goal 
of  harmonizing  protection  philosophies  rather  than 
establishing more focused requirements and guidelines. 
It is doubtful that any vendor is going to produce a 
completely  new  operating  system 
the  current 
internetworked  environment. For commercial viability, 
it appears that operating systems need to accommodate 
everything  from  real-time  wireless  gaming  to  play-on-
demand  multimedia  presentations.  With  technology 
moving  computer  usage  away  from  previous  trends 
(i.e.,  computation  and  data  processing),  it appears that 
a  new  paradigm  is  needed  for  security  engineering  in 
today’s  environment.  Back  to  basics  just doesn’t seem 
to be practicable any more. 
in 
And one can legitimately ask whether there is yet a 
perceived, validated security requirement. 
18 One excuse I’ve seen in print claims that even if there were a 
need  for  a  secured  voting  system  and/or  hard  copy  backup  ballot, 
there is no standard or Protection Profile for either. 
6. Acknowledgement 
Many  people  encouraged  and  helped  with  the 
writing  of  this  paper.  I  would  like  to  thank  Dan 
Thomsen,  LouAnna  Notargiacomo,  Steve  Greenwald, 
and  Ken  Olthoff  for  their  continuing  encouragement 
and  critiques  in  taking  on  this  task  from  the  cozy 
pastures  of  retirement.  In  particular,  I  am  particularly 
indebted  to  LouAnna,  who  took  extraordinary  steps  to 
ensure  the  paper’s  timely  completion.  I  received 
valuable assistance in reconstructing the past from Rich 
Graubart, Ronda Henning, Paul Karger, Ted Lee, Peter 
Neumann,  Roger  Schell,  and  Tom  van  Vleck.  Thank 
you, dear friends! 
7. References 
[1] Abbott, Bob, J. Chin, J. Donnelley, W. Konigsford, 
S.  Tokubo,  and  D.  Webb,  “Security  Analysis  and 
Enhancements  of  Computer  Operating  Systems,” 
Technical  Report  NBSIR  76-1041,  ICET,  National 
Bureau of Standards, 1976. 
[2] Anderson,  James.  P.,  Computer  Security  Planning 
Study,  Electronic  Systems  Division,  USAF  Report 
ESD-TR-73-51 in two volumes.  
[3] Bell,  D.  Elliott,  and  L.  J.  LaPadula,  “Secure 
Computer  System:  Unified  Exposition  and  Multics 
Interpretation,”  Tech.  Report  MTR-2997  Rev  1, 
MITRE Corp., March 1975. 
[4] Boebert,  Earl,  “On  the  Inability  of  an  Unmodified 
Capability  Machine  to  Enforce  the  *-Property,”  Proc. 
7th DOD/NBS Computer Security Conf., 1984. 
[5] Brand,  Sheila,  ed.,  Trusted  Computer  System 
Evaluation Criteria, Final Draft, 27 January 1983, 109 
pp. as C1-FEB- 83- S3-25366. DoD Computer Security 
Center. 
[6] Corbató,  F.  J.,  and  V.  A.  Vyssotsky,  “Introduction 
and Overview of the Multics System”, 1965 Fall Joint 
Computer Conference. 
[7] Department  of  Defense,  Trusted  Computer  System 
Evaluation Criteria, DoD 5200.28-STD, 26 December 
1985.
[8] DoD  Computer  Security  Center,  Trusted Computer 
System  Evaluation  Criteria,  Draft,  24  May  1982,  43 
pp.
[9] DoD  Computer  Security  Center,  Trusted Computer 
System  Evaluation  Criteria,  15  August  1983,  117  pp, 
as CSC-STD-001-83. 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply. 
[10] Harrison,  M.,  W.  Ruzzo,  and  J.  Ullman, 
“Protection in Operating Systems,”, Comm. ACM, vol. 
19, no. 8, 1977. 
[11] Demillo,  R.A.,  R.  J.  Lipton,  A.  J.  Perlis,  “Social 
Processes  and  Proofs  of  Theorems  and  Programs,” 
Comm. ACM, Vol. 22, No. 5, 1979. 
[12] Frantz,  Bill,  Norm  Hardy,  Jay  Jonekait,  Charlie 
Landau,  GNOSIS:  A  Prototype  Operating  System  for
the 1990’s, Tymshare, Inc., 1979. 
[13] Graham,  G.S.  and  P.J.  Denning,  “Protection  – 
Principles  and  Practice,”  Spring  Joint  Computer 
Conference, AFIPS Conf. Proc., 1972.
[14] Jelen, George F., Information Security: an Elusive 
Goal,  Program  on  Information  Resources  Policy, 
Harvard  University  Center  for  Information  Policy 
Research, April 1984. 
Controls,  NBS  Special  Publication  No  500-57, 
MD78733, April 1980. 
[22] Schaefer,  Marvin.,  “Symbol  Security  Condition 
Considered  Harmful,”  Proceedings  1989 
IEEE 
Computer Society Symposium on Security and Privacy, 
pp. 20-46, May 1-3, 1989. 
[23] Schaefer,  Marvin,  W.  C.  Barker,  C.  P.  Pfleeger, 
“Tea  and  I:  an  Allergy,”  Proceedings  1989  IEEE 
Computer Society Symposium on Security and Privacy, 
pp. 178-182, May 1-3, 1989. 
[24] Schaefer,  Marvin,  R.  R.  Linde,  et  al.,  “Program 
Confinement  in  KVM/370,”  in  Proc.  ACM  National 
Conference, Seattle, October, 1997. 
[25] Vyssotsky, V.A., F. J. Corbató, and R.M. Graham, 
“Structure  of  the  Multics  Supervisor.”,  AFIPS  Conf 
Proc., vol. 27, part I, 1965. 
[15] Lee,  Theodore  M.  P.,  “Processors,  Operating 
Systems  and  Nearby  Peripherals:  A  Consensus 
Report,”  appearing  as  Section  8  of  Ruthberg,  op.  cit.,
1980.
[26] Walter,  K.  G,  W.  Ogden,  F.  Bradshaw,  S.  Ames, 
and  D.  Shumway,  “Primitive  Models  for  Computer 
Security,  ESD-TR-74-117,  Air  Force  ESD,  Hanscom 
AFB, Mass, 1974. 
[16] Lipner,  Stephen  B.,  A  Comment  on 
the 
Confinement  Problem,  Proc.  6th  Symp.  Operating 
Systems Principles, 1975 
[17] McLean, 
John,  “Reasoning  About  Security 
Models,” Proc 1987 IEEE Symp. Security and Privacy,
Apr. 1987. 
[18] Millen,  Jonathan  K.,  “Security  Kernel  Validation 
in  Practice,”  Comm.  ACM,  vol. 19, no. 5 (May 1976), 
pp. 243-250. 
[19] Neumann, Peter, Larry Robinson, Karl Levitt, R.S. 
Boyer,  and  A.R.  Saxena,  “A  Provably  Secure 
Operating  System:  Final  Report,”  Stanford  Research 
Institute Report, June 1975. 
[20] Nibaldi,  Grace  H[ammond],  Proposed  Technical 
Evaluation  Criteria  for  Trusted  Computer  Systems,
MITRE Report, M-79-225, 25 October 1979. 
[21] Ruthberg,  Zella,  Audit  and  Evaluation  of 
Computer  Security  II:  System  Vulnerabilities  and 
[27] Ware,  Willis  H.,  ed.  Security  Controls  for 
Computer  Systems:  Report  of  Defense  Science  Board 
Task  Force  on  Computer  Security,  R-609-1,  reissued 
by the RAND Corporation, 1979 
[28] Weissman, Clark. Security Controls in the  ADEPT-
50  Time  Sharing  System.  In  AFIPS  Conference 
Proceedings, volume 35, New Jersey, 1969. 
[29]  Karger,  P.A.  and  A.J.  Herbert.  “An  Augmented 
Capability Architecture to Support Lattice Security and 
Traceability  of  Access”.  in  Proceedings  of  the  1984 
Symposium  on  Security  and  Privacy,.  pp.  2-12,  29 
April - 2 May 1984.  
[30] National  Security  Agency  Trusted  Product 
Evaluation Report, Gemini Trusted Network Processor 
at  http://www.radium.
(GTNP), 
ncsc.mil/-tpep/epl/entries/CSC-EPL-
94-008.html
available 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply.