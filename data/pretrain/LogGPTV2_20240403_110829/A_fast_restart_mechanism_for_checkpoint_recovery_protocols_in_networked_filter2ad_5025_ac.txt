### Application 10
#### Figure 5: Percentage Improvement in Restart Latency by Using FREM over BLCR

For the gee test cases (applications 6 and 7), the relatively small touch set is largely attributed to the dynamic memory deallocation of the applications. Despite the large checkpoint image, many pages are never used again and will soon be freed. This observation indicates that dynamic memory management can provide more optimization opportunities for FREM.

#### 5.2. Runtime Overhead

The use of FREM introduces two types of runtime overhead:
1. Post-checkpoint tracking overhead
2. Fast restart overhead

**Table 3: Post-Checkpoint Tracking Overhead (in milliseconds)**

| Application (Input Set) | Scan Time | Search and Insertion Time | I/O Time | Total Overhead |
|-------------------------|-----------|--------------------------|----------|----------------|
| 1: astar (1)             | 18.7      | 0.3                      | 0.2      | 33.6           |
| 2: bzip2 (5)             | 48.8      | 0.4                      | 0.1      | 49.2           |
| 3: bzip2 (6)             | 38.8      | 1.7                      | 0.2      | 39.4           |
| 4: deaIII                | 4.7       | 1.7                      | 0.3      | 6.6            |
| 5: gamese (1)            | 36.7      | 4.6                      | 0.5      | 38.7           |
| 6: gee (4)               | 29.3      | 15.0                     | 0.2      | 34.5           |
| 7: gee (6)               | 43.7      | 13.6                     | 0.1      | 60.2           |
| 8: Ibm                   | 21.8      | 14.4                     | 0.2      | 22.4           |
| 9: mcf                   | 33.9      | 4.6                      | 0.1      | 34.2           |
| 10: perl (1)             | 18.6      | 1.4                      | 0.2      | 33.6           |
| 11: soplex (2)           | 29.7      | 1.5                      | 0.1      | 45.6           |
| 12: wrf                  | 41.6      | 0.6                      | 0.2      | 46.8           |

**Table 4: Fast Restart Overhead (time unit: seconds)**

| Application (Input Set) | Remaining Image (MB) | Duration of Overlapping | Fast Restart Overhead |
|-------------------------|----------------------|------------------------|-----------------------|
| 1: astar (1)             | 162                  | 18.7                   | 7.1                   |
| 2: bzip2 (5)             | 476                  | 55.4                   | 13.1                  |
| 3: bzip2 (6)             | 250                  | 48.0                   | 11.8                  |
| 4: deaIII                | 144                  | 14.1                   | 10.2                  |
| 5: gamese (1)            | 424                  | 59.5                   | 21.7                  |
| 6: gee (4)               | 157                  | 19.1                   | 10.6                  |
| 7: gee (6)               | 560                  | 76.9                   | 22.7                  |
| 8: Ibm                   | 5                    | 0.8                    | 0.1                   |
| 9: mcf                   | 8                    | 1.4                    | 0.3                   |
| 10: perl (1)             | 83                   | 12.4                   | 6.9                   |
| 11: soplex (2)           | 205                  | 30.3                   | 4.8                   |
| 12: wrf                  | 231                  | 34.6                   | 7.8                   |

The post-checkpoint tracking overhead is mainly caused by three factors: PTE scan time, descriptor search and insertion time, and I/O time to store the descriptor. Table 3 lists the measured post-checkpoint tracking overheads. We observed similar results for both FAST and SLOW networks. Due to space limitations, we only present the results obtained in the SLOW network. The post-checkpoint tracking overhead is generally less than 60.2 milliseconds, which is trivial compared to the performance gain achieved by FREM. The PTE scan time is the dominant contributor to the overhead, especially for memory-demanding applications with large page tables. In general, the search and insertion time is less than 15.0 milliseconds, while the descriptor I/O time is less than 1.5 milliseconds. These overheads are mainly determined by the number of entries in the touch set descriptor, which should not exceed two thousand due to spatial data locality.

When using FREM, the process restart is overlapped with the image retrieval until all the remaining image is delivered to the destination machine. This overlapping inevitably incurs some overhead to the program execution due to resource contention, denoted as the fast restart overhead. Table 4 lists the sizes of remaining images to be retrieved, the durations of overlapping, and the fast restart overheads in the SLOW network for all applications. Generally, the restart overhead is less than 22.7 seconds, which is much smaller than the reduction of restart latency achieved by FREM (see Section 5.1). When the duration of overlapping increases, the overhead generally grows due to an increase in the number of context switches, thereby incurring more overhead. We believe that on emerging multi-core machines, the overhead can be reduced due to greater parallelism provided by advanced architectures.

#### 5.3. Statistical Performance Analysis

The results shown so far indicate that FREM can significantly reduce the restart time but also introduces some runtime overhead. Given that the checkpoint frequency is usually greater than that of recovery, a key question is whether FREM can produce positive performance gain in the long run. To answer this, we conducted a set of experiments to examine application performance when using FREM over a long period. Here, "long run" means statistically evaluating application performance between two restarts. In our experiments, we simulated Poisson failure arrivals of the underlying system, where the arrival rate ranges from one failure per 1000 days to 10 failures per day. The application checkpoint interval is set according to Young's approximation formula [26]. The SLOW network is used in this set of experiments.

Two evaluation metrics are used to measure the overall performance of FREM:
1. \( E_{\text{gain}} \): The expected restart improvement achieved by FREM between two restarts.
2. \( E_{\text{overhead}} \): The expected runtime overhead introduced by FREM between two restarts.

They are calculated as follows:
\[ E_{\text{gain}} = \text{RL}_{\text{improvement}} \times (1 - f) \]
\[ E_{\text{overhead}} = \text{tracking\_overhead} \times N_{\text{ckp}} + \text{fast\_restart\_overhead} \times (1 - f) \]

Here, \( f \) is the failure probability of FREM, i.e., the chance that a failure occurs during the tracking window. \( N_{\text{ckp}} \) is the average number of checkpoints between two restarts.

Our simulations show that for all applications other than applications 8 and 9, \( E_{\text{gain}} \) surpasses \( E_{\text{overhead}} \) by a significant margin ranging from 14.3 seconds to 183.3 seconds under different failure arrival rates. Due to space limitations, we only present the results for applications 1, 2, and 8 in Figure 6. For applications 1 and 2, the runtime overhead \( E_{\text{overhead}} \) introduced by FREM is substantially smaller than the performance gain \( E_{\text{gain}} \) achieved regardless of failure rates. Further, it is shown that the overhead drops as the failure rate grows. When the failure rate increases, the number of checkpoints \( N_{\text{ckp}} \) decreases, resulting in less post-checkpoint tracking overhead. For application 8, the benefit achieved by FREM is much less impressive. A major reason is that the application lacks temporal data locality, resulting in trivial restart improvement. When the failure rate gets higher, the runtime overhead may overshadow the restart improvement. This observation suggests that data locality should be used as key guidance to determine whether to apply FREM or not.

#### 5.4. Result Summary

In summary, the above experiments have shown that:
- For most applications, FREM can reduce restart latencies by 61.96% on average, compared to the regular CIR mechanism. The results on applications with good temporal data locality are more promising.
- The post-checkpoint tracking overhead incurred by FREM is around tens of milliseconds, which is trivial compared to the reduction in restart latency achieved by FREM (e.g., in the range of a couple of seconds to 208.5 seconds). The restart overhead depends on application characteristics, generally ranging from less than one second to 22.7 seconds.
- Our statistical performance analysis has shown that by using FREM, the expected application execution time between two restarts can be reduced by 14.3 seconds to 183.3 seconds.

**Figure 6: Statistical Performance Analysis of FREM**

#### 6. Conclusions

We have presented a novel mechanism called FREM to tackle the restart latency problem of general checkpoint protocols in networked environments. Through user-transparent system support, it allows fast restart on a partial checkpoint image by recording the process data access after each checkpoint. We have implemented FREM with the widely used BLCR checkpointing tool in Linux systems. Experiments on SPEC CPU2006 benchmarks have shown that FREM can effectively reduce process restart latency by 61.96% on average. In the future, we will explore an aggressive way to estimate the tracking window. Additionally, a more sophisticated image loading mechanism will be developed for better performance of FREM. Our ultimate goal is to integrate FREM with existing fault management tools for better availability and performance.

**Acknowledgement**

The authors appreciate the valuable comments and suggestions from the anonymous reviewers. We would like to thank our paper shepherd, David Taylor, for his time and guidance.

**References**

[1] M. Baker and M. Sullivan, "The recovery box: Using fast recovery to provide high availability in the UNIX environment," in Proceedings of Summer USENIX Technical Conference, 1992.

[2] A. Bouteiller, T. Herault, G. Krawezik, P. Lemarinier, F. Cappello, "MPICH-V: A multiprotocol automatic fault tolerant MPI," International Journal of High Performance Computing and Applications, vol. 20(3), pp. 319-333, 2005.

[3] Daly, J. Duell, P. Hargrove, "A model for predicting the optimum checkpoint interval for restart dumps," in Proceedings of International Conference on Computational Science, 2003.

[4] E. Elnozahy and J. Plank, "Checkpointing for peta-scale systems: A look into the future of practical rollback-recovery," IEEE Trans. on Dependable and Secure Computing, vol. 1(2), pp. 97-108, 2004.

[5] S. Feldman and C. Brown, "IGOR: A system for program debugging via reversible execution," in Proceedings of ACM SIGPLAN and SIGOPS workshop on parallel and distributed debugging, 1989.

[6] O. Laadan and J. Nieh, "Transparent checkpoint-restart of multiple processes on commodity operating systems," in Proceedings of USENIX Annual Technical Conference, 2007.

[7] Z. Lan and Y. Li, "Adaptive fault management of parallel applications for high performance computing," IEEE Trans. on Computers, in press.

[8] K. Li, J. Naughton, and J. Plank, "Low-latency, concurrent checkpointing for parallel programs," IEEE Trans. Parallel and Distributed Systems, vol. 5(8), pp. 874-879, 1994.

[9] Y. Ling, J. Mi, and X. Lin, "A variational calculus approach to optimal checkpoint placement," IEEE Trans. Computers, vol. 50(7), pp. 699-708, 2001.

[10] D. Milojicic, F. Douglis, Y. Paindaveine, R. Wheeler, and S. Zhou, "Process migration," ACM Comput. Surv., vol. 32(3), pp. 241-299, 2000.

[11] NCSA web site, http://teragrid.ncsa.uiuc.edu.

[12] A. Oliner, L. Rudolph, and R. Sahoo, "Cooperative checkpointing: A robust approach to large-scale systems reliability," in Proceedings of International Conference on Supercomputing, 2006.

[13] Oracle website, http://www.oracle.com/technology/deploy/availability/htdocs/fs_on-demand_rollback.htm.

[14] D. Patterson et al., "Recovery-oriented computing (ROC): Motivation, definition, techniques, and case studies," UC Berkeley Computer Science Technical Report UCB/CSD-02-1175, 2002.

[15] J. Plank, Y. Chen, K. Li, M. Beck, and G. Kingsley, "Memory exclusion: Optimizing the performance of checkpointing systems," Software - Practice and Experience, vol. 29(2), pp. 125-142, 1999.

[16] J. Plank and K. Li, "Diskless checkpointing," IEEE Trans. Parallel and Distributed Systems, vol. 9(10), pp. 972-986, 1998.

[17] J. Plank and M. Thomason, "Processor allocation and checkpoint interval selection in cluster computing systems," Journal of Parallel and Distributed Computing, vol. 61(11), pp. 1570-1590, 2001.

[18] S. Rao, L. Alvisi, and H. Yin, "The cost of recovery in message logging protocols," IEEE Trans. on Knowledge and Data Engineering, vol. 12(2), pp. 160-173, 2000.

[19] J. Sancho, F. Petrini, G. Johnson, J. Fernandez, and E. Frachtenberg, "Incremental checkpointing for scientific computing," in Proceedings of International Processing Symposium, 2004.

[20] SPEC CPU 2006 benchmark website, http://www.spec.org/cpu2006/.

[21] J. Squyres, "A component architecture for LAM/MPI," in Proceedings of European PVM/MPI Users' Group Meeting, 2003.

[22] A. Tanenbaum and A. Woodhull, Operating Systems: Design and Implementation, 2nd ed., New Jersey: Prentice-Hall, 1997.

[23] T. Tannenbaum and M. Litzkow, "The Condor distributed processing system," Dr. Dobb's Journal, vol. 227, pp. 40-48, 1995.

[24] N. Vaidya, "Impact of checkpoint latency on overhead ratio of a checkpointing scheme," IEEE Trans. on Computers, vol. 46(8), pp. 942-947, 1997.

[25] J. Young, "A first order approximation to the optimal checkpoint interval," Comm. ACM, vol. 17(9), pp. 530-531, 1974.

[26] P. Zhou, V. Pandey, J. Sundaresan, A. Raghuraman, Y. Zhou, and S. Kumar, "Dynamic tracking of page miss ratio curve for memory management," in Proceedings of International Conference on Architectural Support for Programming Languages and Operating Systems, 2004.