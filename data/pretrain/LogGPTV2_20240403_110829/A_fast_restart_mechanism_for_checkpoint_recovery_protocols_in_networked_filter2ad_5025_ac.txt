Application 10
Figure 5. Percentage improvement on restart
latency by using FREM over BLCR
For the gee test cases (applications 6 and 7), the
relatively small touch set is largely attributed to the
dynamic memory deallocation of the applications. For
1-4244-2398-9/08/$20.00 ©2008 IEEE
223
DSN 2008: Li &Lan
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:10 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
these applications, although the checkpoint image is
large, it has many pages that will never be used again
and will soon be freed. This observation indicates that
dynamic memory management can provide more
optimization opportunities for FREM.
5.2. Runtime Overhead
The use of FREM introduces two types of runtime
overhead: (1) the post-checkpoint tracking overhead
and (2) the fast restart overhead.
Table 3. Post-checkpoint tracking overhead
(in milliseconds)
Application
Scan time
Search and
Descriptor
Tracking
(input set)
1: astar (1)
2: bzip2 (5)
3: bzip2 (6)
4: deaIII
5: gamese (1)
6: gee (4)
7: gee (6)
8: Ibm
9:mcf
10: perl (1)
11: soplcx (2)
12: wrf
18.7
48.8
38.8
4.7
36.7
29.3
43.7
21.8
33.9
18.6
29.7
41.6
insertion time
I/O time
overhead
13.j
0.3
0.4
1.7
1.7
4.6
15.0
0.5
0.2
13.6
14.4
4.6
1.4
0.2
0.1
0.2
0.3
0.5
1.5
0.1
0.2
1.4
1..5
0.6
33.6
49.2
39.4
6.6
38.7
34.5
60.2
22.4
34.2
33.6
4.5.6
46.8
Table 4. Fast restart overhead
(time unit: seconds)
Remaining
Duration of
image (MB)
overlapping
Application
(input set)
1: astar (1)
2: bz:ip2 (5)
3: bzip2 (6)
4: dcalll
5: gamcss (1)
6: gce (4)
7: gee (6)
8:1bm
9:mef
10: perl (1)
11: soplex (2)
12: wrf
162
476
250
144
424
157
560
5
8
83
205
231
18.7
55.4
48.0
14.1
59.5
19.1
76.9
0.8
1.4
12.4
30.3
34.6
Fast restart
overhead
7.1
13.1
11.8
10.2
21.7
10.6
22.7
0.1
0.3
6.9
4.8
7.8
The post-checkpoint tracking overhead is mainly
caused by three factors:
the
descriptor search and insertion time, and the I/O time
to store the descriptor. Table 3 lists the measured
post-checkpoint tracking overheads. We have observed
the PTE scan time,
runs in the FAST and SLOW
similar results for
networks. Due to space limitations, here we only
present the results obtained in the SLOW network. It is
shown that the post-checkpoint tracking overhead is
generally less than 60.2 milliseconds, which is trivial
compared to the performance gain achieved by FREM.
The PTE scan time is the dominant contributor to the
that
overhead.
memory-demanding applications typically have huge
page tables.
In general, the search and insertion time
is less than 15.0 milliseconds, while the descriptor I/O
time is less than 1.5 milliseconds. These overheads are
mainly determined by the number of entries in the
touch set descriptor, which should not exceed two
thousand, because of spatial data locality.
This
fact
is
due
to
the
When using FREM, the restart of the process is
overlapped with the image retrieval until all
the
remaining image is delivered to the destination
machine. This overlapping inevitably incurs some
overhead to the program execution due to resource
contention. This is denoted as the fast restart overhead.
Table 4 lists the sizes of remaining images to be
the durations of overlapping and the fast
retrieved,
restart overheads in the SLOW network,
for all
applications. In general, the restart overhead is less
than 22.7 seconds, which is much smaller than the
reduction of restart latency achieved by FREM (see
Further, when the duration of
Section 5.1).
overlapping is
the overhead generally
grows. This is caused by the fact that the number of
context switches increases,
thereby incurring more
overhead. We believe on the emerging multi-core
machines, the overhead can be reduced due to greater
parallelism provided by the advanced architectures.
increasing,
5.3. Statistical Performance Analysis
runtime overheads. Given that
The results shown so far indicate that FREM can
significantly reduce the restart time, but also introduces
some
checkpoint
frequency is usually greater than that of recovery, a
key question that may be raised is whether FREM is
capable ofproducing positive performance gain in the
long run. To answer this question, we conduct a set of
experiments to examine application performance when
using FREM in a long run. Here, the "long run" means
that we statistically evaluate application performance
between two restarts. In our experiments, we simulate
Poisson failure arrivals of the underlying system,
where the arrival rate ranges from one failure per 1000
days to 10 failures per day. The application checkpoint
interval
is set according to Young's approximation
formula [26]. The SLOW network is used in this set of
experiments.
1-4244-2398-9/08/$20.00 ©2008 IEEE
224
DSN 2008: Li & Lan
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:10 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Two evaluation metrics are used to measure the
overall performance of FREM: (1) E gaim the expected
restart improvement achieved by FREM between two
restarts and (2) Eoverhead, the expected runtime overhead
introduced by FREM between two restarts. They are
calculated as follows:
Egain = RL _ improvement x (1- f)
Eoverhead = tracking _ overhead x N ckp
+ fast _ restart _ overhead x (1- f)
Here, f is the failure probability of FREM, Le., the
chance that a failure occurs during the tracking
window. Nckp is the average number of checkpoints
between two restarts.
Our simulations show that for all applications other
than applications 8 and 9, E gain surpasses Eoverhead by a
significant margin ranging from 14.3 seconds to 183.3
seconds under different failure arrival rates. Due to
space limitations, we only present
the results for
applications 1, 2 and 8 in Figure 6. For applications 1
the runtime overhead Eoverhead introduced by
and 2,
FREM is substantially smaller than the performance
gain E gain achieved regardless of failure rates. Further,
it is shown that the overhead drops as the failure rate
grows. When the failure rate increases, the number of
checkpoints Nckp decreases,
thereby resulting in less
post-checkpoint tracking overhead. For application 8,
the benefit achieved by FREM is much less impressive.
A major reason is that the application lacks temporal
data locality,
restart
improvement. When the failure rate gets higher, the
runtime
restart
improvement. This observation suggests that data
locality should be used as key guidance to determine
whether to apply FREM or not.
thereby resulting in trivial
overshadow the
overhead may
5.4. Result Summary
In summary,
the above experiments have shown
that:
• For most applications, FREM can reduce restart
latencies by 61.96% on average, as compared to the
regular CIR mechanism. The
results on the
applications with good temporal data locality are
more promising.
• The post-checkpoint tracking overhead incurred by
FREM is around tens of milliseconds, which is
trivial compared to the reduction in restart latency
achieved by FREM (e.g., in the range of a couple of
seconds to 208.5 seconds). The restart overhead
depends on application characteristics, generally
ranging from less than one second to 22.7 seconds.
• Our statistical performance analysis has shown that
by using FREM, the expected application execution
time between two restarts can be reduced by 14.3
seconds to 183.3 seconds.
80 - r - - - - - - - - - - - - - - .
Appllcadon 10=1
60 + - - - - - - - - - - -= - - - - - - - - - .
E40 +--------1
..
0.001
0.01
0.1
10
failure arrival rate
Application ID=2
0.001
0.01
0.1
10
failure arrival rate
Appllcadon 10=8
20
240
200
~160
~ 120
80
40
24
20
~ 16
~ 12
E..
0.001
0.01
0.1
10
failure arrival rate
Figure 6. Statistical performance analysis
of FREM
6.
Conclusions
We have presented a novel mechanism called
FREM to tackle the restart latency problem of general
checkpoint protocols
in networked environments.
Through user-transparent system support, it allows fast
restart on a partial checkpoint image by recording the
process data access after each checkpoint. We have
implemented FREM with the widely used BLCR
checkpointing tool in Linux systems. Experiments on
SPEC CPU2006 benchmarks have shown that FREM
can effectively reduce process restart
latency by
61.96% on average. In future, we will explore an
aggressive way to estimate the tracking window. In
addition,
loading
mechanism will be developed for better performance of
FREM. Our ultimate goal is to integrate FREM with
existing
fault
management of applications.
checkpointing
sophisticated
a more
image
tools
for
better
1-4244-2398-9/08/$20.00 ©2008 IEEE
225
DSN 2008: Li &Lan
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:10 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
the
of
II
"On
Parallel
feasibility
and Distributed
and M. Puening,
[16] 1. Plank, Y. Chen and K. Li and M. Beck and G.
Kingsley, "Memory exclusion: Optimizing the performance
of checkpointing systems," Software -
Practice and
Experience, vol. 29(2), pp. 125-142, 1999.
[17] J. Plank, K. Li
"Diskless
checkpointing," IEEE Trans. Parallel and Distributed
Systems, vol. 9(10), pp. 972-986, 1998.
[18] J. Plank and M. Thomason, "Processor allocation and
checkpoint interval selection in cluster computing systems,"
Journal ofParallel and Distributed Computing, vol. 61 (11),
pp. 1570-1590,2001.
[19] S. Rao, L. Alvisi and H. Yin, "The cost of recovery in
message logging protocols," IEEE Trans. on Knowledge and
Data Engineering, vol. 12(2), pp. 160-173,2000.
[20] 1. Sancho, F. Petrini, G. Johnson, 1. Fernandez and E.
Frachtenberg,
incremental
checkpointing for scientific computing," in Proceedings of
International
Processing
Symposium, 2004.
[21] SPEC CPU 2006 benchmark website,
www.spec.orglcpu20061.
[22]. 1. Squyres
"A component
architecture for LAM/MPI," in Proceedings of European
PVMIMPI Users' Group Meeting, 2003.
[23] A. Tanenbaum and A. Woodhull, Operating Systems:
ed., New Jersey:
Design
Prentice-Hall, 1997.
[24] T. Tannenbaum and M. Litzkow,
"The Condor
distributed processing system," Dr. Dobb's Journal, vol. 227,
pp. 40-48, 1995.
[25] N. Vaidya, "Impact of checkpoint latency on overhead
ratio of a checkpointing scheme,"
on
Computers, vol. 46(8), pp. 942-947, 1997.
[26] 1. Young, "A first order approximation to the optimal
checkpoint interval," Comm. ACM, vol. 17(9), pp. 530-531
'
1974.
[27] P. Zhou, V. Pandey, 1. Sundaresan, A. Raghuraman, Y.
Zhou and S. Kumar, "Dynamic tracking of page miss ratio
in Proceedings of
curve
International Conference on Architectural Support
for
Programming Languages and Operating Systems, 2004.
for memory management,"
and A. Lumsdaine,
Implementation,
IEEE Trans.
http:
and
2nd
Acknowledgement
The authors appreciate the valuable comments and
suggestions from the anonymous reviewers. We would
like to thank our paper shepherd, David Taylor, for his
time and guidance.
References
lab
in
1. Daly,
J. Duell,
"A model
for Linux
P. Hargrove
"Berkeley
clusters,"
E. Elnozahy and J. Plank,
[1] M. Baker and M. Sullivan, "The recovery box: Using
fast recovery to provide high availability in the UNIX
environment," in Proceedings ofSummer USENIX Technical
Conference, 1992.
[2] A. Bouteiller, T. Herault, G. Krawezik, P. Lemarinier,
F. Cappello, "MPICH-V: A multiprotocol automatic fault
tolerant MPI," International Journal of High Performance
Computing and Applications, vol. 20(3), pp. 319-333, 2005.
[3]
for predicting the optimum
checkpoint interval for restart dumps," in Proceedings of
International Conference on Computational Science, 2003.
"Checkpointing for
[4]
peta-scale systems: A look into the future of practical
rollback-recovery," IEEE Trans. on Dependable and Secure
Computing, vol. 1(2), pp. 97-108, 2004.
S. Feldman and C. Brown, "IGOR: A system for
[5]
program debugging via reversible execution," in Proceedings
of ACM SIGPLAN and SIGOPS workshop on parallel and
distributed debugging, 1989.
[6]
and
(BLCR)
checkpoint/restart
Proceedings ofSciDAC, 2006.
[7] O. Laadan and J. Nieh, "Transparent checkpoint-restart
of multiple processes on commodity operating systems," in
Proceedings of USENIX Annual Technical Conference, 2007.
[8] Z. Lan and Y. Li, "Adaptive fault management of
parallel applications for high performance computing," IEEE
Trans. on Computers, in press.
[9] K. Li, 1. Naughton and J. Plank, "Low-latency,
concurrent checkpointing for parallel programs," IEEE
Trans. Parallel and Distributed Systems, vol. 5(8), pp.
874-879, 1994.
[10] Y. Ling, 1. Mi and X. Lin, "A variational calculus
approach to optimal checkpoint placement," IEEE Trans.
Computers, vol. 50(7), pp. 699-708, 2001.
[11] D. Milojicic, F. Douglis, Y. Paindaveine, R. Wheeler
and S. Zhou, "Process migration," ACM Comput. Surv., vol.
32(3), pp. 241-299, 2000.
[12] NCSA web site, http://teragrid.ncsa.uiuc.edu.
[13] A. oliner, L. Rudolph and R. Sahoo, "Cooperative
checkpointing: A robust approach to large-scale systems
reliability," in Proceedings of International Conference on
Supercomputing, 2006.
[14] Oracle
website,
http://www.oracle.com/technology/deploy/availability/htdocs
Ifs_on-demand_rollback. htm.
[15] D. Patterson et aI., "Recovery-oriented computing
(ROC): Motivation, definition, techniques, and case studies,"
UC Berkeley Computer
Technical Report
UCBIICSD-02-1175, 2002.
availability
document
Science
high
1-4244-2398-9/08/$20.00 ©2008 IEEE
226
DSN 2008: Li &Lan
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:10 UTC from IEEE Xplore.  Restrictions apply.