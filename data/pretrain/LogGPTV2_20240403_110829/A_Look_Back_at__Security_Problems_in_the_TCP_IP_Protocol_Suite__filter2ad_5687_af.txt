cestor of IPsec [57] —was being deﬁned as
part of the Secure Data Network System. Re-
gardless, the BFE (and SP3 and IPsec) all
create virtual private networks, with their own
address spaces; while there is some protec-
tion against trafﬁc analysis, these technolo-
gies do not prevent an adversary from notic-
ing which protected networks are talking to
which other protected networks.
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
End-to-end encryption, above the TCP level, may be
used to secure any conversation, regardless of the number of
hops or the quality of the links. This is probably appropriate
for centralized network management applications, or other
point-to-point transfers. Key distribution and management
is a greater problem, since there are more pairs of corre-
spondents involved. Furthermore, since encryption and de-
cryption are done before initiation or after termination of
the TCP processing, host-level software must arrange for
the translation; this implies extra overhead for each such
conversation.10
End-to-end encryption is vulnerable to denial of service
attacks, since fraudulently-injected packets can pass the
TCP checksum tests and make it to the application. A com-
bination of end-to-end encryption and link-level encryption
can be employed to guard against this. An intriguing alter-
native would be to encrypt the data portion of the TCP seg-
ment, but not the header; the TCP checksum would be cal-
culated on the cleartext, and hence would detect spurious
packets. Unfortunately, such a change would be incompat-
ible with other implementations of TCP, and could not be
done transparently at application level.
I wrote “link-level encryption” here; it should
have been “network-level”. But there’s a more
subtle problem: the TCP checksum is not
cryptographically strong; it’s pretty easy to
launch a variety of attacks against such a
weak integrity protection mechanism. See,
for example, [11].
Regardless of the method used, a major beneﬁt of en-
crypted communications is the implied authentication they
provide. If one assumes that the key distribution center is
secure, and the key distribution protocols are adequate, the
very ability to communicate carries with it a strong assur-
ance that one can trust the source host’s IP address for iden-
tiﬁcation.
This implied authentication can be especially important
in high-threat situations. A routing attack can be used to
“take over” an existing connection; the intruder can effec-
tively cut the connection at the subverted machine, send
dangerous commands to the far end, and all the while trans-
late sequence numbers on packets passed through so as to
disguise the intrusion.
Today’s network-level encryptors would foil
this; they protect the TCP (and sometimes
IP) headers as well as the payload. I suspect
that the BFE would as well.
10 We are assuming that TCP is handled by the host, and not by a front-
end processor.
It should be noted, of course, that any of these encryp-
tion schemes provide privacy. Often that is the primary goal
of such systems.
7.3. Trusted Systems
Given that TCP/IP is a Defense Department protocol
suite, it is worth asking to what extent the Orange Book
[31] and Red Book [33] criteria would protect a host from
the attacks described above. That is, suppose that a target
host (and the gateways!) were rated B1 or higher. Could
these attacks succeed? The answer is a complex one, and
depends on the assumptions we are willing to make. In gen-
eral, hosts and routers rated at B2 or higher are immune to
the attacks described here, while C2-level systems are sus-
ceptible. B1-level systems are vulnerable to some of these
attacks, but not all.
In order to understand how TCP/IP is used in secure en-
vironments, a brief tutorial on the military security model is
necessary. All objects in the computer system, such as ﬁles
or network channels, and all data exported from them, must
have a label indicating the sensitivity of the information
in them. This label includes hierarchical components (i.e.,
Conﬁdential, Secret, and Top Secret) and non-hierarchical
components. Subjects —i.e., processes within the computer
system—are similarly labeled. A subject may read an ob-
ject if its label has a higher or equal hierarchical level and if
all of the object’s non-hierarchical components are included
in the subject’s label. In other words, the process must have
sufﬁcient clearance for the information in a ﬁle. Similarly,
a subject may write to an object if the object has a higher
or equal level and the object’s non-hierarchical components
include all of those in the subject’s level. That is, the sen-
sitivity level of the ﬁle must be at least as high as that of
the process. If it were not, a program with a high clearance
could write classiﬁed data to a ﬁle that is readable by a pro-
cess with a low security clearance.
A corollary to this is that for read/write access to any ﬁle,
its security label must exactly match that of the process. The
same applies to any form of bidirectional interprocess com-
munication (i.e., a TCP virtual circuit): both ends must have
identical labels.
We can now see how to apply this model to the TCP/IP
protocol suite. When a process creates a TCP connection,
that connection is given the process’s label. This label is en-
coded in the IP security option. The remote TCP must en-
sure that the label on received packets matches that of the
receiving process. Servers awaiting connections may be el-
igible to run at multiple levels; when the connection is in-
stantiated, however, the process must be forced to the level
of the connection request packet.
IP also makes use of the security option [97]. A packet
may not be sent over a link with a lower clearance level.
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
If a link is rated for Secret trafﬁc, it may carry Unclassi-
ﬁed or Conﬁdential trafﬁc, but it may not carry Top Secret
data. Thus, the security option constrains routing decisions.
The security level of a link depends on its inherent charac-
teristics, the strength of any encryption algorithms used, the
security levels of the hosts on that network, and even the
location of the facility. For example, an Ethernet cable lo-
cated in a submarine is much more secure than if the same
cable were running through a dormitory room in a univer-
sity.
Several points follow from these constraints. First, TCP-
level attacks can only achieve penetration at the level of the
attacker. That is, an attacker at the Unclassiﬁed level could
only achieve Unclassiﬁed privileges on the target system,
regardless of which network attack was used.11 Incoming
packets with an invalid security marking would be rejected
by the gateways.
Attacks based on any form of source-address authentica-
tion should be rejected as well. The Orange Book requires
that systems provide secure means of identiﬁcation and au-
thentication; as we have shown, simple reliance on the IP
address is not adequate. As of the B1 level, authentication
information must be protected by cryptographic checksums
when transmitted from machine to machine.12
The authentication server is still problematic; it can be
spoofed by a sequence number attack, especially if netstat
is available. This sort of attack could easily be combined
with source routing for full interactive access. Again, cryp-
tographic checksums would add signiﬁcant strength.
B1-level systems are not automatically immune from
routing attacks; RIP-spooﬁng could corrupt their routing ta-
bles just as easily. As seen, that would allow an intruder to
capture passwords, perhaps even some used on other trusted
systems. To be sure, the initial penetration is still restricted
by the security labelling, but that may not block future lo-
gins captured by these means.
Routing attacks can also be used for denial of service.
Speciﬁcally, if the route to a secure destination is changed
to require use of an insecure link, the two hosts will not be
able to communicate. This change would probably be de-
tected rather quickly, though, since the gateway that noticed
the misrouted packet would ﬂag it as a security problem.
At the B2 level, secure transmission of routing control
information is required. Similar requirements apply to other
network control information, such as ICMP packets.
Several attacks we have described rely on data de-
rived from “information servers”, such as netstat and ﬁn-
11 We are assuming, of course, that the penetrated system does not have
bugs of its own that would allow further access.
12 More precisely, user identiﬁcation information must be protected to an
equal extent with data sensitivity labels. Under certain circumstances,
described in the Red Book, cryptographic checks may be omitted. In
general, though, they are required.
ger. While these, if carefully done, may not represent a
direct penetration threat in the civilian sense, they are of-
ten seen to represent a covert channel that may be used to
leak information. Thus, many B-division systems do not im-
plement such servers.
In a practical sense, some of the technical features we
have described may not apply in the military world. Ad-
ministrative rules [32] tend to prohibit risky sorts of inter-
connections; uncleared personnel are not likely to have even
indirect access to systems containing Top Secret data. Such
rules are, most likely, an accurate commentary on anyone’s
ability to validate any computer system of non-trivial size.
This is an odd section for this paper, in that it
attempts a very brief look at a completely dif-
ferent technology architecture: Orange Book-
style secure systems. Worse yet,
the Or-
ange Book prescribes results, not methods;
as such, it is difﬁcult to ﬁnd a precise match
between the attacks I described and abstract
mechanisms.
In any event, the question is moot today.
The Orange Book and its multi-hued kin have
been replaced by the Common Criteria [19],
an international effort at deﬁning secure sys-
tems. In most ways, the Common Criteria is
even more abstract than its DoD predeces-
sor; one could not make meaningful state-
ments without at least specifying what pro-
tection proﬁle one was interested in. (Apart
from that, one can make a strong argument
that that entire approach to system security
is ﬂawed [89], but such a discussion is out of
scope for this paper.)
Two technical points are worth not-
ing. First,
routing attacks could be miti-
gated by maintenance of separate rout-
ing tables (by multi-level secure routers)
for different security classiﬁcations. Sec-
ond, exactly what
forms of authentication
are acceptable in any situation would de-
pend critically on detailed knowledge of ex-
actly what sorts of hosts were connected to
what sorts of network. In other words, eaves-
dropping may or may not be a concern.
8. Conclusions
Several points are immediately obvious from this anal-
ysis. The ﬁrst, surely, is that in general, relying on the IP
source address for authentication is extremely dangerous.13
13 There are some exceptions to this rule. If the entire network, and all of
its components (hosts, gateways, cables, etc.) are physically protected,
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Fortunately, the Internet community is starting to accept
this on more than an intellectual level. The Berkeley man-
uals [22] have always stated that the authentication proto-
col was very weak, but it is only recently that serious at-
tempts (i.e., Kerberos [99] and SunOS 4.0’s DES authenti-
cation mode [101]) have been made to correct the problem.
Kerberos and SunOS 4.0 have their weaknesses, but both
are far better than their predecessor. More recently, an ex-
tension to the Network Time Protocol (NTP) [66] has been
proposed that includes a cryptographic checksum [63].
A second broad class of problems is sequence number
attacks. If a protocol depends on sequence numbers—and
most do—it is vital that they be chosen unpredictably. It is
worth considerable effort to ensure that these numbers are
not knowable even to other users on the same system.
We may generalize this by by stating that hosts should
not give away knowledge gratuitously. A ﬁnger server, for
example, would be much safer if it only supplied informa-
tion about a known user, rather than supplying informa-
tion about everyone logged on. Even then, some censor-
ship might be appropriate; a refusal to supply the last login
date and other sensitive information would be appropriate
if the account was not used recently. (Never-used accounts
often have simple default passwords. Infrequently-used ac-
counts are often set up less carefully by the owner.) We have
also seen how netstat may be abused; indeed, the combina-
tion of netstat with the authentication server is the single
strongest attack using the standardized Internet protocols.
Finally, network control mechanisms are dangerous, and
must be carefully guarded. Static routes are not feasible in a
large-scale network, but intelligent use of default routes and
veriﬁable point-to-point routing protocols (i.e., EGP) are far
less vulnerable than broadcast-based routing.
9. Acknowledgments
Dave Presotto, Bob Gilligan, Gene Tsudik, and espe-
cially Deborah Estrin made a number of useful suggestions
and corrections to a draft of this paper.
10. Retrospective Conclusions
The Internet of 1989 was a much
simpler—and much friendlier—place than it
is today. Most of the protocols I looked at
were comparatively simple client-server pro-
tocols;
today’s multi-party protocols—SIP
[88], Diameter [16], various peer-to-peer pro-
tocols, etc.—are much harder to analyze.
and if all of the operating systems are sufﬁciently secure, there would
seem to be little risk.
Often, the crucial question is not authentica-
tion but authorization: how do you know if a
certain party is permitted to perform a cer-
tain action?
The overall trend has been good. The In-
ternet Engineering Task Force (IETF) will
not standardize protocols where the only
mandatory-to-implement form of authentica-
tion is plaintext passwords; address-based
authentication is acceptable only in very re-
stricted circumstances. There are standard-
ized and/or widely deployed cryptographic
protocols for remote login, sending and re-
ceiving email, Web browsing, etc. As dis-
cussed earlier, outing is the major exception;
operationally, it is still not securable.
Most of
the security problems we en-
counter on the Internet
today are due to
buggy code (including, of course, buggy
code in cryptographic modules). To some ex-
tent, of course,
that’s because there are
so many bugs; why launch a difﬁcult at-
tack when there’s so much low-hanging fruit
available?
Password-guessing is still a common at-
indeed, a new wave of password-
recently been observed
ssh [106], a cryptographically-
tack;
guessing has
against
protected replacement for the r-utilities.
There is no need to belabor the earlier
conclusions that predictable sequence num-
bers and address-based authentication are
bad. This is now well-accepted; for example,
the speciﬁcation for a new transport protocol,
SCTP [100], mandates strong random num-
ber generation [6] for its analogous ﬁelds. But
how does one design a secure protocol?
One answer is to look at data ﬂow of
the protocol. On what elements does au-
thentication depend? Can an attacker tamper
with or mimic those elements? What are the
guarantees, especially the security guaran-
tees, of each element? Naturally, the powers
of the attacker must be taken into account.