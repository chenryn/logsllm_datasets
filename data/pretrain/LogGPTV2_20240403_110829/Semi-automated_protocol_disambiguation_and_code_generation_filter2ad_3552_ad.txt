problem of co-reference resolution [26], which can resolve
identical noun phrases across sentences. To our knowledge,
semantic parsers cannot yet resolve such references. To get
sage to parse the text, we rewrote the second sentence to
clarify the co-reference, as shown in Table 5.
The second sentence contains
three conditionals,
followed a non-actionable fragment that rephrases one
of the conditionals. Specifically, the first condition if
bfd.RemoteDemandMode is 1,
in English,
immediately afterwards (Demand mode is active on the
remote node). To our knowledge, current NLP techniques
6This is common across protocols: for example, TCP keeps track of protocol
state regarding ACK reception.
is rephrased,
RFC
792
Semantic
Parsing
Disambiguation
Implementation
Disambiguation
1 LF/sentence
✓
Code
Gen.
CODE
✓
Unit Tests
X
resolve ambiguity and
implicit protocol behavior
Figure 4: sage workflow in processing RFC 792.
user
6 sentences
X
5 sentences
cannot easily identify rephrased sentence fragments. sage
relies on human annotation to identify this fragment as
non-actionable; after removing the fragment, it is able to
generate code correctly for this sentence.
Parsing. We focus on explaining our analysis of such state
management sentences. sage is able to parse the packet
header as given in RFC 5880§4.1. We analyzed 22 state man-
agement sentences in RFC 5880§6.8.6 which involve a greater
diversity of operations than pure packet generation. To sup-
port these, we added 15 lexical entries, 10 predicates, and 8
function handlers.
6.5 Disambiguation
Revising a specification inevitably requires some degree of
manual inspection and disambiguation. sage makes this sys-
tematic: it identifies and fixes ambiguities when it can, alerts
spec authors or developers when it cannot, and can help
iteratively verify re-written parts of the spec.
Ambiguous sentences. When we began to analyze RFC
792 with sage, we immediately found many ambiguities we
highlighted throughout this paper; these result in more than
one logical form even after manual disambiguation.
We also encountered ostensibly disambiguated text that
yields zero logical forms; this is caused by incomplete sen-
tences. For example, “If code = 0, identifies the octet where an
error was detected” is an example that fails CCG parsing due
to lack of subject in the sentence, and indeed it may not be
parseable for a human lacking context regarding the referent.
Such sentence fragments require human guesswork, but, as
9
s
m
r
o
F
l
a
c
i
g
o
L
f
o
#
40
20
10
5
2
1
Base
max
avg
min
Type
Arg. Order
(a) ICMP
Pred.
Order
Distrib.
Assoc.
s
m
r
o
F
l
a
c
i
g
o
L
f
o
#
5
4
3
2
1
Base
max
avg
min
Type
Arg. Order
(b) IGMP
Pred.
Order
Distrib.
Assoc.
s
m
r
o
F
l
a
c
i
g
o
L
f
o
#
50
20
10
5
2
1
Base
max
avg
min
Type
Arg. Order
(c) BFD
Pred.
Order
Distrib.
Assoc.
Figure 5: Number of LFs after Inconsistency Checks on ICMP/IGMP/BFD text: for each ambiguous sentence, sequentially
executing checks on LFs (Base) reduces inconsistencies; after the last Associativity check, the final output is a single LF.
we have observed in §4, we can leverage structural context
in the RFC in cases where the referent of these sentences is
a field name. In these cases, sage is able to correctly parse
the sentence by supplying the parser with the subject.
Among 87 instances in RFC 792, we found 4 that result
in more than 1 logical form and 1 results in 0 logical forms
(Table 6). We rewrote these 5 ambiguous (of which only 3
are unique) sentences to enable automated protocol genera-
tion. These ambiguous sentences were found after sage had
applied its checks (§4.2)—these are in a sense true ambigui-
ties in the ICMP RFC. In sage, we require the user to revise
such sentences, according to the feedback loop as shown in
Figure 4. As guidance for future users of sage, the resulting
LFs from an ambiguous sentence are all kept after the disam-
biguation checks are applied and comparing these LFs can
guide the users where the ambiguity lies, thus guiding their
revisions. In our end-to-end experiments (§6.2), we evaluated
sage using the modified RFC with these ambiguities fixed.
Under-specified behavior. sage can also discover under-
specified behavior through unit testing; generated code can
be applied to unit tests to see if the protocol implementation
is complete. In this process, we discovered 6 sentences that
are variants of this sentence: “If code = 0, an identifier to aid
in matching echos and replies, may be zero”. This sentence
does not specify whether the sender or the receiver or both
can (potentially) set the identifier. The correct behavior is
only for the sender to follow this instruction; a sender may
generate a non-zero identifier, and the receiver should set
the identifier to be zero in the reply. Not doing so results in
a non-interoperability with Linux’s ping implementation.
Efficacy of logical form winnowing. sage winnows log-
ical forms so it can automatically disambiguate text when
possible, thereby reducing manual labor in disambiguation.
To understand why winnowing is necessary, and how ef-
fective each of its checks can be, we collect text fragments
that could lead to multiple logical forms, and calculate how
many are generated before and after we perform inconsis-
tency checks along with the isomorphism check. We show
the extent to which each check is effective in reducing logical
forms: in Figure 5a, the max line shows the description that
10
Type
Argument Ordering
Predicate Ordering
Distributivity
7
5
3
1
e
c
n
e
t
n
e
S
r
e
p
s
F
L
f
o
#
4.92
4.23
s
e
c
n
e
t
n
e
S
d
e
t
c
e
ff
A
f
o
#
15
10
5
2.26
0.23
18
15
7
5
Figure 6: Effect of individual disambiguation checks on
RFC 792: Left: average number of LFs filtered by the check
per ambiguous sentence with standard error Right: num-
ber of ambiguous sentences affected out of 42 total.
Sentence
Label
#LFs
16
6
Poor
Good
The ’address’ of the ’source’ in an ’echo message’
will be the ’destination’ of the ’echo reply’ ’message’.
The ’address’ of the ’source’ in an ’echo message’
will be the ’destination’ of the ’echo reply message’.
Table 7: Comparison of the number of logical forms (LFs)
between good and poor noun phrase labels.
leads to the highest count of generated logical forms and
shows how the value goes down to one after all checks are
completed. Similarly, the min line represents the situation
for the text that generates the fewest logical forms before
applying checks. Between the min and max lines, we also
show the average trend among all sentences.
Figure 5a shows that all sentences resulted in 2-46 LFs,
but sage’s winnowing reduces this to 1 (after human-in-the-
loop rewriting of true ambiguities). Of these, type, argument
ordering and the associativity checks are the most effective.
We apply the same analysis to IGMP (Figure 5b). In IGMP, the
distributivity check is also important. This analysis shows
the cumulative effect of applying checks in the order shown
in the figure. We also apply the same analysis to BFD state
management sentences (Figure 5c). We discover some longer
sentences could result in up to 56 LFs.
A more direct way to understand the efficacy of checks is
shown in Figure 6 (for ICMP). To generate this figure, for each
sentence, we apply only one check on the base set of logical
Increase Decrease
Zero
Domain-specific Dict.
Noun-phrase Labeling
17
0
0
8
0
54
Table 8: Effect of disabling domain-specific dictionary
and noun-phrase labeling on number of logical forms.
IPv4
TCP
UDP
ICMP
NTP
OSPF2
BGP4
RTP
BFD
♦ Packet Format
♦ Interoperation
♦ Pseudo Code