In Algorithm 1 we show standalone single-threaded pseu-
docode assuming a simple actor-critic algorithm is used. In
our experiments, we use the multi-agent API provided by
Ray RLlib [31], which implements parallel simulation and
optimization of such RL environments.
Performance. We found that NeuroCuts often converges to
its optimal solution within just a few hundred rollouts. The
size of the rule set does not significantly affect the number
of rollouts needed for convergence, but affects the running
time of each rollout. For smaller problems (e.g., 1000 rules),
this may be within a few minutes of CPU time. The compu-
tational overhead for larger problem scales with the size of
the classifier, i.e., linearly with the number of rules that must
be scanned per action taken to grow the tree. The bulk of
time in NeuroCuts is spent executing tree cut actions. This is
largely an artifact of our Python implementation, which iter-
ates over each rule present in a node on each cut action. An
optimized C++ implementation of the decision tree would
further reduce the training time.
5.1 Optimizations
Rollout truncation. During the initial phase of learning,
the unoptimized policy will create excessively large trees.
Since NeuroCuts does not start learning until a tree is com-
plete, it is necessary to truncate rollouts to speed up the
initial phase of training. For larger classifiers, we found it
necessary to allow rollouts of up to 15000 actions in length.
Depth truncation. Since valid solutions never involve trees
of depth greater than a few hundred, we also truncate trees
once they reach a certain depth. In our experience, depth
truncation is only a factor early on in learning; NeuroCuts
quickly learns to avoid creating very deep trees.
Proximal Policy Optimization. For better stability and
more sample-efficient learning, in our experiments we choose
to use Proximal Policy Optimization (PPO) [43]. PPO imple-
ments an actor-critic style loss with entropy regularization
and a clipped surrogate objective, which enables improved
exploration and sample efficiency. We report the PPO hyper-
parameters we used in Appendix B. It is important to note
however that this particular choice of RL algorithm is not
fundamental to NeuroCuts.
9
6 EVALUATION
In the evaluation, we seek to answer the following questions:
(1) How does NeuroCuts compare to the state-of-the-art
approaches in terms of classification time and memory
footprint? (Section 6.1 and 6.2)
(2) Beyond tabula rasa learning, can NeuroCuts effectively
incorporate and improve upon pre-engineered heuris-
tics? (Section 6.3)
(3) How much influence does the time-space coefficient c
have on the performance of NeuroCuts? (Section 6.4)
For the results presented in the next sections, we evalu-
ated NeuroCuts using the range of hyperparameters shown
in Appendix B. We did not otherwise perform extensive
hyperparameter tuning; in fact we use close to the default
hyperparameter configuration of the PPO algorithm. The
notable hyperparameters we swept over include:
• Allowed top-node partitioning (none, simple, and the
EffiCuts heuristic), which strongly biases NeuroCuts
towards learning trees optimized for time (none) vs
space (EffiCuts), or somewhere in the middle (simple).
• The max number of timesteps allowed per rollout be-
fore truncation. It must be large enough to enable solv-
ing the problem, but not so large that it slows down
the initial phase of training.
• We also experimented with values for the time-space
tradeoff coefficient c ∈ {0, 0.1, 0.5, 1}. When c 100× median space improvement along
with the better classification times reported in Section 6.1.
However, these time-optimized trees are not competitive
in space with the space-optimized NeuroCuts, EffiCuts and
CutSplit trees.
6.3 Improving on EffiCuts
In Figure 10 we examine a set of 36 NeuroCuts trees (one tree
for each ClassBench classifier) generated by NeuroCuts with
the EffiCuts partition action. This is in contrast with the prior
experiments that selected trees optimized for either space or
time alone. On this 36-tree set, there is a median space im-
provement of 29% relative to EffiCuts; median classification
time is about the same. This shows that NeuroCuts is able
to effectively incorporate and improve on pre-engineered
heuristics such as the EffiCuts top-level partition function.
Surprisingly, NeuroCuts is able to outperform EffiCuts de-
spite the fact that NeuroCuts does not use multi-dimensional
cut actions. When we evaluate EffiCuts with these cut types
disabled, the memory advantage of NeuroCuts widens to
10
Neural Packet Classification
Figure 8: Classification time (tree depth) for HiCuts, HyperCuts, EffiCuts, and NeuroCuts (time-optimized). We
omit four entries for HiCuts and HyperCuts that did not complete after more than 24 hours.
Figure 9: Memory footprint (bytes per rule) used for HiCuts, HyperCuts, EffiCuts, and NeuroCuts (space-
optimized). We omit four entries for HiCuts and HyperCuts that did not complete after more than 24 hours.
67% at the median. This suggests that NeuroCuts could fur-
ther improve its performance if we also incorporate multi-
dimensional cut actions via parametric action encoding tech-
niques [9]. It would also be interesting to, besides adding
actions to NeuroCuts, consider postprocessing steps such as
resampling that can be used to further improve the stochastic
policy output.
6.4 Tuning Time vs Space
Finally, in Figure 11 we sweep across a range of values of c
for NeuroCuts with the simple partition method and log(x)
reward scaling. We plot the ClassBench median of the best
classification times and bytes per rule found for each clas-
sifier. We find that classification time improves by 2× as
c → 1, while the number of bytes per rule improves 2×
11
as c → 0. This shows that c is effective in controlling the
tradeoff between space and time.
7 RELATED WORK
Packet classification. Packet classification is a long-standing
problem in computer networking. Decision-tree based algo-
rithms are a major class of algorithmic solutions. Existing so-
lutions rely on hand-tuned heuristics to build decision trees.
HiCuts [13] is a pioneering work in this space. It cuts the
space of each node in one dimension to create multiple equal-
sized subspaces to separate rules. HyperCuts [47] extends Hi-
Cuts by allowing cutting in multiple dimensions at each node.
HyperSplit [41] combines the advantages of rule-based space
decomposition and local-optimized recursion to guarantee
worst-case classification time and reduce memory footprint.
Classification Time0204060acl1 1kacl2 1kacl3 1kacl4 1kacl5 1kacl1 10kacl2 10kacl3 10kacl4 10kacl5 10kacl1 100kacl2 100kacl3 100kacl4 100kacl5 100kfw1 1kfw2 1kfw3 1kfw4 1kfw5 1kfw1 10kfw2 10kfw3 10kfw4 10kfw5 10kfw1 100kfw2 100kfw3 100kfw4 100kfw5 100kipc1 1kipc2 1kipc1 10kipc2 10kipc1 100kipc2 100kHiCutsHyperCutsEffiCutsCutSplitNeuroCutsapplies Deep RL to generate efficient decision trees, with the
capability to incorporate and improve on existing heuristics
as needed.
Decision trees for machine learning. There have been
several proposals to use deep learning to optimize the per-
formance of decision trees for machine learning problems
[21, 39, 59]. In these settings, the objective is maximizing
test accuracy. In contrast, packet classification decision trees
provide perfect accuracy by construction, and the objective
is minimizing classification time and memory usage.
Structured data in deep learning. There have many re-
cent proposals towards applying deep learning to process and
generate tree and graph data structures [11, 58, 60, 61, 63, 66].
NeuroCuts sidesteps the need to explicitly process graphs,
instead exploiting the structure of the problem to encode
agent state into a compact fixed-length representation.
Deep reinforcement learning. Deep RL leverages the mod-
eling capacity of deep neural networks to extend classical
RL to domains with large, high-dimensional state and ac-
tion spaces. DQN [37, 38, 56] is one of the earliest successes
of Deep RL, and shows how to learn control policies from
high-dimensional sensory inputs and achieve human-level
performance in Atari 2600 games. A3C, PPO, and IMPALA
[7, 36, 43] scale actor-critic algorithms to leverage many
parallel workers. AlphaGo [44], AlphaGo Zero [46] and Al-
phaZero [45] show that Deep RL algorithms can achieve
superhuman performance in many challenging games like
Go, chess and shogi. Deep RL has also been applied to many
other domains like natural language processing [29] and
robotics [26–28]. NeuroCuts works in a discrete environ-
ment and applies Deep RL to learn decision trees for packet
classification.
Deep learning for networking and systems. Recently
there has been an uptake in applying deep learning to net-
working and systems problems [4, 6, 16, 34, 35, 54, 62, 64, 65].
NAS [62] utilizes client computation and deep neural net-
works to improve the video quality independent to the avail-
able bandwidth. Pensieve [35] generates adaptive bitrate al-
gorithms using Deep RL without relying on pre-programmed
models or assumptions about the environment. Valadarsky
et al. [54] applies Deep RL to learn network routing. Chin-
chali et al. [4] uses Deep RL for traffic scheduling in cellular
networks. AuTO [3] scales Deep RL for datacenter-scale traf-
fic optimization. There are also many solutions that apply
deep reinforcement learning to congestion control [6, 16, 64]
and resource management [34]. We explore the application
of Deep RL to packet classification, and propose a new al-
gorithm to learn decision trees with succinct encoding and
scalable training mechanisms.
(a) NeuroCuts can build on the EffiCuts partitioner to generate trees up to
10× (90%) more space efficient than EffiCuts. In this experiment NeuroCuts
did as well or better than EffiCuts on all 36 rule sets.
(b) NeuroCuts with the EffiCuts partitioner generates trees with about the
same time efficiency as EffiCuts.
Figure 10: Sorted rankings of NeuroCuts’ improve-
ment over EffiCuts in the ClassBench benchmark.
Here NeuroCuts is run with only the EffiCuts par-
tition method allowed. Positive values indicate im-
provements.
value of c
Figure 11: The classification time improves by 2× as
the time-space coefficient c → 1, and conversely, num-
ber of bytes per rule improves 2× as c → 0.
EffiCuts [55] introduces four heuristics, including separable
trees, tree merging, equal-dense cuts and node co-location, to
reduce rule replication and imbalance cutting. CutSplit [30]
integrates equal-sized cutting and equal-dense cutting to op-
timize decision trees. Besides decision-tree based algorithms,
there are also other algorithms proposed for packet classifica-
tion, such as tuple space search [49], RFC [12] and DCFL [53].
These algorithms are not as popular as decision-tree based
algorithms, because they are either too slow or consume too
much memory. There are also solutions that exploit special-
ized hardware such as TCAMs, GPUs and FPGAs to support
packet classification [17, 24, 32, 33, 42, 48, 50, 57]. Compared
to existing work, NeuroCuts is an algorithmic solution that
12
Space Improvement (1 - a/b)00.250.50.751Time Improvement (1 - a/b)-0.4-0.200.20.4104001Median classification timeMedian bytes per ruleNeural Packet Classification
8 CONCLUSION
We present NeuroCuts, a simple and effective Deep RL for-
mulation of the packet classification problem. NeuroCuts
provides significant improvements on classification time and
memory footprint compared to state-of-the-art algorithms. It
can easily incorporate pre-engineered heuristics to leverage
their domain knowledge, optimize for flexible objectives, and
generates decision trees which are easy to test and deploy
in any environment.
We hope NeuroCuts can inspire a new generation of learning-
based algorithms for packet classification. As a concrete ex-
ample, NeuroCuts currently optimizes for the worst-case
classification time or memory footprint. By considering a
specific traffic pattern, NeuroCuts can be extended to other
objectives such as average classification time. This would
allow NeuroCuts to not only optimize for a specific classifier
but also for a specific traffic pattern in a given deployment.
13
REFERENCES
[1] Florin Baboescu, Sumeet Singh, and George Varghese. 2003. Packet
classification for core routers: Is there an alternative to CAMs?. In IEEE
INFOCOM.
[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,
John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym.
arXiv preprint arXiv:1606.01540 (2016).
[3] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. AuTO: Scaling
Deep Reinforcement Learning for Datacenter-Scale Automatic Traffic
Optimization. In ACM SIGCOMM.
[4] Sandeep Chinchali, Pan Hu, Tianshu Chu, Manu Sharma, Manu Bansal,
Rakesh Misra, Marco Pavone, and Sachin Katti. 2018. Cellular network
traffic scheduling with deep reinforcement learning. In AAAI.
[5] Ekin Dogus Cubuk, Barret Zoph, Dandelion Mané, Vijay Vasudevan,