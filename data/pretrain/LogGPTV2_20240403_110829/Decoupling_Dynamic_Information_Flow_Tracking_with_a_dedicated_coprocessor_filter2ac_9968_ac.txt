nize on system calls, an extra queue can be used between
the two in order to buffer instruction tuples. The queue can
be sized to account for temporary mismatches in processing
rates between the processor and the coprocessor. The proces-
sor stalls only when the decoupling queue is full or when a
system call instruction is executed.
To avoid frequent stalls due to a full queue, the coproces-
sor must achieve an instruction processing rate equal to, or
greater than, that of the main core. Since the coprocessor has
a very shallow pipeline, handles only committed instructions
from the main core, and does not have to deal with mispre-
dicted instructions or instruction cache misses, a single-issue
coprocessor is sufﬁcient for most superscalar processors that
achieve IPCs close to one. For wide-issue superscalar pro-
cessors that routinely achieve IPCs higher than one, a wide-
issue coprocessor pipeline would be necessary. Since the co-
processor contains 4-bit registers and 4-bit ALUs and does
not include branch prediction logic, a wide-issue coprocessor
pipeline would not be particularly expensive. In Section 5.2.2,
we provide an estimate of the IPC attainable by a single-issue
coprocessor, by showing the performance of the coprocessor
when paired with higher IPC main cores.
Security Exceptions: As the coprocessor applies tag
checks using the instruction tuples, certain checks may fail,
indicating potential security threats. On a tag check failure,
the coprocessor interrupts the main core in an asynchronous
manner. To make DIFT checks applicable to the operating
system code as well, the interrupt should switch the core to the
trusted security monitor which runs in either a special trusted
mode [8, 9], or in the hypervisor mode in systems with hard-
ware support for virtualization [12]. This allows us to catch
bugs in both userspace and in the kernel [9]. The security mon-
itor uses the protection mechanisms available in these modes
to protect its code and data from a compromised operating sys-
tem. Once invoked, the monitor can initiate the termination
of the application or guest OS under attack. We protect the
security monitor itself using a sandboxing policy on one of
the tag bits. For an in-depth discussion of exception handling
and security monitors, we refer the reader to related work [8].
Note, however, that the proposed system differs from inte-
grated DIFT architectures only in the synchronization between
the main core and the coprocessor. Security checks and the
consequent exception processing (if necessary) have the same
semantics and operation in the coprocessor-based and the in-
tegrated designs.
3.4 Tag Cache
The main core passes the memory addresses for load/store
instructions to the coprocessor. Since the instruction is com-
municated to the coprocessor after the main processor com-
pletes execution, the address passed can be a physical one.
Hence, the coprocessor does not need a separate TLB. Con-
sequently, the tag cache is physically indexed and tagged, and
does not need to be ﬂushed on page table updates and context
switches.
To detect code injection attacks, the DIFT core must also
check the tag associated with the instruction’s memory loca-
tion. As a result, tag checks for load and store instructions re-
quire two accesses to the tag cache. This problem can be elim-
inated by providing separate instruction and data tag caches,
similar to the separate instruction and data caches in the main
core. A cheaper alternative that performs equally well is us-
ing a uniﬁed tag cache with an L0 buffer for instruction tag
accesses. The L0 buffer can store a cache line. Since tags
are narrow (4 bits), a 32-byte tag cache line can pack tags for
64 memory words providing good spatial locality. We access
the L0 and the tag cache in parallel. For non memory instruc-
tions, we access both components with the same address (the
instruction’s PC). For loads and stores, we access the L0 with
the PC and the uniﬁed tag cache with the address for the mem-
ory tags. This design causes a pipeline stall only when the L0
cache misses on an instruction tag access, and the instruction
is a load or a store that occupies the port of the tag cache. This
combination of events is rare.
3.5 Coprocessor for In-Order Cores
There is no particular change in terms of functionality in
the design of the coprocessor or the coprocessor interface if
the main core is in-order or out-of-order. Since the two syn-
chronize on system calls, the only requirement for the main
processor is that it must stall if the decoupling queue is full
or a system call is encountered. Using the DIFT coprocessor
with a different main core may display different performance
issues. For example, we may need to re-size the decoupling
queue to hide temporary mismatches in performance between
the two. Our full-system prototype (see Section 4) makes use
of an in-order main core.
3.6 Multiprocessor Consistency Issues
For multiprocessors where each core has a dedicated DIFT
coprocessor, there are potential consistency issues due to the
lack of atomicity of updates for data and tags. The same prob-
lem occurs in multi-core DIFT systems and FlexiTaint, since
metadata propagation and checks occur after the processing of
the corresponding instruction completes.
The atomicity problem is easy to address in architectures
with weak consistency models by synchronizing the main core
and the DIFT coprocessor on memory fences, acquires, and
releases in addition to system calls and traps. This approach
guarantees that tag check and propagation for all instructions
prior to the memory fence are completed by the coprocessor
before the fence instructions commit in the processor. The
atomicity problem is harder to solve in systems with stricter
consistency models such as sequential consistency. One ap-
proach is to use transactional memory as detailed in [6]. A dy-
namic binary translator (DBT) is used to instrument the code
with transactions that encapsulate the data and metadata ac-
cesses for one or more basic blocks in the application. A more
complex approach is to actually provide atomicity of individ-
ual loads and stores in hardware. This entails keeping track of
coherence requests issued by different cores in the system in
order to detect when another access is ordered in between the
data and metadata accesses for an instruction.
Parameter
Leon pipeline depth
Leon instruction cache
Leon data cache
Leon instruction TLB
Leon data TLB
Coprocessor pipeline depth
Coprocessor tag cache
Decoupling queue size
Speciﬁcation
7 stages
8 KB, 2-way set-associative
16 KB, 2-way set-associative
8 entries, fully associative
8 entries, fully associative
4 stages
512 Bytes, 2-way set-associative
6 entries
Table 1: The prototype system speciﬁcation.
4 Prototype System
To evaluate the coprocessor-based approach for DIFT, we
developed a full-system FPGA prototype based on the SPARC
architecture and the Linux operating system. Our proto-
type is based on the framework for the Raksha integrated
DIFT architecture [8]. This allows us to make direct per-
formance and complexity comparisons between the integrated
and coprocessor-based approaches for DIFT hardware.
4.1 System Architecture
The main core in our prototype is the Leon SPARC V8 pro-
cessor, a 32-bit synthesizable core [14]. Leon uses a single-
issue, in-order, 7-stage pipeline that does not perform specu-
lative execution. Leon supports SPARC coprocessor instruc-
tions, which we use to conﬁgure the DIFT coprocessor and
provide security exception information. We introduced a de-
coupling queue that buffers information passed from the main
core to the DIFT coprocessor. If the queue ﬁlls up, the main
core is stalled until the coprocessor makes forward progress.
As the main core commits instructions before the DIFT core,
security exceptions are imprecise.
The DIFT coprocessor follows the description in Section
3. It uses a single-issue, 4-stage pipeline for tag propagation
and checks. Similar to Raksha, we support four security poli-
cies, each controlling one of the four tag bits. The tag cache
is a 512-byte, 2-way set-associative cache with 32-byte cache
lines. Since we use 4-bit tags per word, the cache can effec-
tively store the tags for 4 Kbytes of data.
Our prototype provides a full-ﬂedged Linux workstation
environment. We use Gentoo Linux 2.6.20 as our kernel and
run unmodiﬁed SPARC binaries for enterprise applications
such as Apache, PostgreSQL, and OpenSSH. We have mod-
iﬁed a small portion of the Linux kernel to provide support
for our DIFT hardware [8, 9]. The security monitor is imple-
mented as a shared library preloaded by the dynamic linker
with each application.
4.2 Design Statistics
We synthesized our hardware (main core, DIFT coproces-
sor, and memory system) onto a Xilinx XUP board with an
XC2VP30 FPGA. Table 1 presents the default parameters for
the prototype. Table 2 provides the basic design statistics
for our coprocessor-based design. We quantify the additional
Component
Base Leon core (integer)
FPU control & datapath Leon
Core changes for Raksha
% Raksha increase over Leon
Core changes for coprocessor IF
Decoupling queue
DIFT coprocessor
Total DIFT coprocessor
% coprocessor increase over Leon
BRAMs
46
4
4
8%
0
3
5
8
16%
4-input LUTs
13,858
14,000
1,352
4.85%
22
26
2,105
2,131
7.64%
Table 2: Complexity of the prototype FPGA implementation of the DIFT co-
processor in terms of FPGA block RAMs and 4-input LUTs.
resources necessary in terms of 4-input LUTs (lookup tables
for logic) and block RAMs, for the changes to the core for
the coprocessor interface, DIFT coprocessor (including the tag
cache), and the decoupling queue. For comparison purposes,
we also provide the additional hardware resources necessary
for the Raksha integrated DIFT architecture. Note that the
same coprocessor can be used with a range of other main pro-
cessors: processors with larger caches, speculative execution
etc. In these cases, the overhead of the coprocessor as a per-
centage of the main processor would be even lower in terms
of both logic and memory resources.
5 Evaluation
This section evaluates the security capabilities and perfor-
mance overheads of the DIFT coprocessor.
5.1 Security Evaluation
To evaluate the security capabilities of our design, we at-
tempted a wide range of attacks on real-world applications,
and even one in kernelspace using unmodiﬁed SPARC bina-
ries. We conﬁgured the coprocessor to implement the same
DIFT policies (check and propagate rules) used for the secu-
rity evaluation of the Raksha design [8, 9]. For the low-level
memory corruption attacks such as buffer overﬂows, hardware
performs taint propagation and checks for use of tainted values
as instruction pointers, data pointers, or instructions. Synchro-
nization between the main core and the coprocessor happens
on system calls, to ensure that any pending security exceptions
are taken. For high-level semantic attacks such as directory
traversals, the hardware performs taint propagation, while the
software monitor performs security checks for tainted com-
mands on sensitive function and system call boundaries sim-
ilar to Raksha [8]. For protecting against Web vulnerabili-
ties like cross-site scripting, we apply this tainting policy to
Apache, and any associated modules like PHP.
Table 3 summarizes our security experiments. The appli-
cations were written in multiple programming languages and
represent workloads ranging from common utilities (gzip, tar,
polymorph, sendmail, sus) to server and web systems (scry,
htdig, wu-ftpd) to kernel code (quotactl). All experiments
were performed on unmodiﬁed SPARC binaries with no de-
bugging or relocation information. The coprocessor success-
fully detected both high-level attacks (directory traversals and
cross-site scripting) and low-level memory corruptions (buffer
overﬂows and format string bugs), even in the OS (user/kernel
pointer). We can concurrently run all the analyses in Table 3
using 4 tag bits: one for tainting untrusted data, one for identi-
fying legitimate pointers, one for function/system call interpo-
sition, and one for protecting the security handler. The security
handler is protected by sandboxing its code and data.
We used the pointer injection policy used in [9] for catch-
ing low-level attacks. This policy uses two tag bits, one for
identifying all the legitimate pointers in the system, and an-
other for identifying tainted data. The invariant enforced is
that tainted data cannot be dereferenced, unless it has been
deemed to be a legitimate pointer. This analysis is very pow-
erful, and has been shown to reliably catch low-level attacks
such as buffer overﬂows, and user/kernel pointer dereferences,
in both userspace and kernelspace, without any false posi-
tives [9]. Due to space constraints, we refer the reader to re-
lated work for an in-depth discussion of security policies [9].
Note that the security policies used to evaluate our copro-
cessor are stronger than those used to evaluate other DIFT ar-
chitectures, including FlexiTaint [5, 7, 24, 26]. For instance,
FlexiTaint does not detect code injection attacks and suffers
from false positives and negatives on memory corruption at-
tacks. Overall, the coprocessor provides software with exactly
the same security features and guarantees as the Raksha de-
sign [8, 9], proving that our delayed synchronization model
does not compromise on security.
5.2 Performance Evaluation
5.2.1 Performance Comparison
We measured the performance overhead due to the DIFT co-
processor using the SPECint2000 benchmarks. We ran each
program twice, once with the coprocessor disabled and once
with the coprocessor performing DIFT analysis (checks and
propagates using taint bits). Since we do not launch a secu-
rity attack on these benchmarks, we never transition to the
security monitor (no security exceptions). The overhead of
any additional analysis performed by the monitor is not af-
fected when we switch from an integrated DIFT approach to
the coprocessor-based one.
Figure 3 presents the performance overhead of the copro-
cessor conﬁgured with a 512-byte tag cache and a 6-entry
queue (the default conﬁguration), over an unmodiﬁed Leon.
The integrated DIFT approach of Raksha has the same per-
formance as the base design since there are no additional
stalls [8]. The average performance overhead due to the DIFT
coprocessor for the SPEC benchmarks is 0.79%. The negli-
gible overheads are almost exclusively due to memory con-
tention between cache misses from the tag cache and memory
trafﬁc from the main processor.
Program (Lang)
gzip (C)
Attack
Directory traversal
tar (C)
Directory traversal
Scry (PHP)
Cross-site scripting
htdig (C++)
Cross-site scripting
polymorph (C)
sendmail (C)