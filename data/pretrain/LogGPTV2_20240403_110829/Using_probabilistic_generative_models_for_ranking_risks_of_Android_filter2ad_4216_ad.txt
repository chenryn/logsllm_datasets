them to compare with other methods. One parameter is the number
of hidden topics. Another parameter is how to use category infor-
mation. This is needed because malware apps do not have category
information. Thus when we compute the probability of apps in the
test dataset, we also strip their category information.
To estimate an app’s likelihood using the MNB model, there are
a few ways to choose when we do not know its category informa-
tion. The ﬁrst method, called ‘max’, is to compute the probability
of the app for every category and choose the maximum probability,
that is the category in which the app ﬁts the best, and assume that
the app was in that category. The second method, called ‘mean’,
is to compute the app’s probability for every category and take the
weighted average of all probabilities. For HMNB model, in addi-
tion to the previous two methods, we can also use the mean of our
Dirichlet prior as the topic distribution to compute the probability.
This method is called ‘prior’ method.
Figure 2 shows the AUC values for choosing different param-
eters for MNB and HMNB. From our experiments, we ﬁnd that
the maximum mean of AUC for MNB model is achieved by using
‘max’ method with 5 hidden classes. And the maximum for HMNB
is achieved by using ‘mean’ method with 80 hidden classes. We use
these parameters when comparing with other methods.
Comparing Different Methods. In Figure 3, we compare the gen-
erative models with other approaches in the literature. Figure 3(a)
shows the ROC curves. Because several curves are clustered to-
gether, we use Figure 3(b) to show a close-up of the ROC curves
for x axis of up to 0.1. Figure 3(c) show the AUC values.
The methods we compare against include Kirin, RCP, and RPCP.
Kirin [11] identiﬁes 9 rules for apps to be considered risky. As
Kirin is represented by a single decision point, we only illustrate
it as a point in Figure 3(a), and has no AUC value. It can iden-
tify close to 39% malware apps at 4% false positive rate. RCP and
RPCP are proposed in [24]; they rely on the rarity of critical per-
missions and the rarity of pairs of critical permissions.
We note that all generative models have AUC values of over
0.94; they signiﬁcantly outperform RCP and RPCP. The results
clearly show that HMNB is best performing, with MNB, BNB, and
PNB close behind and almost the same. We note that even a dif-
ference of 0.01 is statistically signiﬁcant given the small standard
deviation. And the difference between the generative models and
other methods is clearly seen in the ROC curves.
Permissions vs. Risk Scores. The fact that HMNB has the high-
est AUC makes it somewhat attractive as a risk scoring method. We
know that it is not guaranteed to have the monotonicity property;
however, it is possible that it preserves the property in most cases.
To check whether this is the case, in Figure 4 we plot the average
number of permissions for each percentile of the apps in the mar-
ket2011 dataset, when they are ranked by the risk value according
to the PNB model and to the HMNB model. It is clearly seen that in
the PNB model the average number of permissions is almost non-
decreasing as the risk goes up. On the other hand, in the HMNB
model we observe apps with large number of permissions that have
low risk. This suggests that HMNB ﬂatly fails the monotonicity
requirement.
Model Stability.
Finally, we conducted experiments to check
whether models trained on one dataset can be used without retrain-
ing to compute the risk scoring on a new dataset. For this purpose,
we use the divided datasets described in Section 3. That is the over-
lap data between 2011 and 2012, and the 2011 dataset with overlap
removed and 2012 dataset with overlap removed.
For each of the six possible ordered pairs, we train on one dataset
and then test on the other together with the malware dataset. Fig-
ure 5 shows the result. Somewhat interestingly, when testing on
the overlap dataset, training either on the 2011-NoOverlap dataset
or the 2012-NoOverlap dataset gives excellent result. However us-
ing any other combination leads to results that perform worse. This
is to some degree to be expected from Figure 1(c). As the “over-
lap” apps generally request fewer permissions than the other two
datasets. The other apps appear to be more varied and require train-
ing using part of them to get good results.
As we have seen in Figure 1 the permission data has changed
over time. Therefore, if a system like this were to be implemented,
the models should be periodically regenerated to achieve the best
results and to keep up to date with the trends that are occurring
within the market.
6. DISCUSSION
In the introduction, we mention that while Windows UAC may
not be very effective in helping the users make more secure deci-
sions, one of its advantages is that it encouraged developers to make
247(a) Different number of hidden components for MNB
(b) Different number of hidden components for HMNB
Figure 2: Parameter selection for different number of hidden classes. Mean, Max and Sum represent different methods to relate the
malicious applications, which don’t contain category information, into a system which utilizes category information.
(a) PNB
(b) HMNB80
Figure 4: Average number of permissions for every 1% percent division of apps, sorted in descending order on the basis of likelihood.
The points represents the average number of permissions requested, and the error bars indicate the min and max at that percentile
conservative decisions in order to improve the user experience by
avoiding UAC prompts. One possible positive result of assigning a
risk to each application is that it generates a feedback mechanism
for the developers which could encourage them to reduce the risk
that an app introduces to a mobile device. In essence, an effective
risk score mechanism may lead to different decisions by users, cre-
ating an economic motivation for developers to reduce the risk of
an application. It is also possible that this mechanism could drive
additional revenue through application markets since if users are
concerned enough to use lower risk applications, then they might
be willing to purchase different apps as a low risk alternative.
The goal of creating a simple feedback mechanism is the motiva-
tion behind our recommendation for the PNB model as an effective
risk communication mechanism. This model, with the monotonic
property, gives direct feedback to a developer who wishes to lower
the risk score of an app. This is demonstrated in Figure 4(a), where
the number of permissions directly correlates to the relative risk of
an app. There is some variation in this ﬁgure because some permis-
sions introduce more risk then others; however, the mathematical
properties of PND is such that removing a permission from a set of
permissions always reduces the risk score, and adding one permis-
sion always increases the risk score.
In the rest of this section, we discuss a particularly interesting
app. The application presented in Table 1 represents more than a
thousand applications by the same developer with different key-
words. This set of apps intercepts all text messages and displays
the message on the screen with a new background based on the
keyword. Looking at the app’s decompiled code it does not appear
to be performing any obviously malicious tasks; however, depend-
ing on a user’s deﬁnition of privacy, it could be considered a risky
application. One major reason for the high permission count is
that this app contains several different ad networks, each of which
requests different permissions to achieve their data collection re-
quirements to show relevant adds. The ad networks along with the
general functionality of the app leads to 17 different permissions,
many of which could have serious privacy issues if misused. Send-
ing and receiving SMS messages is part of the core functionality of
the app, however, the ability to read the contact list is used in or-
der to extract names of contacts given the phone number. The app
also extracts the user’s phone number in order to send a test text
message. Additionally, the app collects the email address of the
user to notify them that a new app for a speciﬁc keyword has been
generated. While there is no obvious data leakage beyond what
one would expect, there is data leakage over time. That is to say,
they are not collecting and exﬁltrating all of this information off
the phone the ﬁrst time the app runs, but over time, they are able to
paint a picture of the user when they activate different functionality.
The application also has 2 permissions that are requested but un-
12345678910204060800.940.94050.9410.94150.9420.94250.9430.9435NumberofHiddenClassesMeanofAUCsDifferentWaysUsingCategoryforMNBMaxMean20304050607080900.950.95050.9510.95150.9520.95250.9530.95350.9540.95450.955NumberofHiddenClassesMeanofAUCsDifferentWaysUsingCategoryforHMNBPriorMaxMean 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100Average number of permissions (error bars as min/max)Likelihood that an app was generate by the model as a percentile of all appsPermissions vs Risk 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100Average number of permissions (error bars as min/max)Likelihood that an app was generate by the model as a percentile of all appsPermissions vs Risk248(a) ROC for the best performing parameters for each
method.
(b) Close up of Figure 3(a) to capture performance differ-
ences in the ﬁrst 10% false positives
(c) AUC for ROC curves presented in Figure 3a
Figure 3: Comparison of different models using the best per-
forming parameters for each models
Figure 5: Comparison of 2011 and 2012 Data for PNB and
HMNB models.
‘no’ = no overlap, ‘over’ = only overlap.
‘ﬁrst/second’ means the ﬁrst dataset was used to train, and the
second dataset was used to test along with the malware. Then
the AUC was generated and generated.
used, one of these is the permission to intercept phone calls. While
most of the other permissions can be justiﬁed by some functional-
ity in the app, either from the app itself or the related ad networks,
this one cannot be justiﬁed. We note that even though an app may
not use this permission in the current version, the fact that it has
requested this permission still introduces some risk to the user. The
reason for the risk is that during an update, if a new version of the
app contains the same permissions as the previous version, then
the app update can occur silently. Whereas if the app requests new
permissions, then the user is notiﬁed that the app is changing its
requested permissions. So just requesting a permission, even if it is
not used, does increase the overall potential risk of the app in this
sense.
7. RELATED WORK
Felt et al. [13] use static analysis to determine whether an An-
droid application is overprivileged. It classiﬁed an application as
overprivileged if the application requested a permission which it
never actually used. They apply their techniques to a set of 940
applications and ﬁnd that about one-third are overprivileged. Their
key observation was that developers are trying to follow least privi-
lege but sometimes fail due to insufﬁcient API documentation. An-
other work by Felt et al. [14] surveys applications (free and paid)
from the Android Market. Their key observation was that 93% of
free apps and 82% of paid apps request permissions that they deem
as “dangerous”. While this does not reveal much out of context, it
demonstrates that users are accustomed to granting dangerous per-
missions to apps without much concern. Neither of these works
actually attempt to detect or categorize malicious software.
Enck et al. [10] make an effort to decompile and analyze the
source of applications to detect further leaks and usage of data. An-
other work by Enck et al. [11] developed a system that examined
risky permission combinations for determining whether the permis-
sions declared by an application satisfy a certain global safety pol-
icy. This work manually speciﬁes permission combinations such
as WRITE_SMS and SEND_SMS, or FINE_LOCATION and IN-
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91FalsePositiveRateTruePositiveRateROCHMNBMNBBNBPNBRCPRPCPkirin00.010.020.030.040.050.060.070.080.090.10.50.550.60.650.70.750.80.850.90.951False Positive RateTrue Positive RateROC  HMNBMNBBNBPNBRCPRPCPHMNBMNBBNBPNBRCPRPCP0.840.860.880.90.920.940.96Mean Of AUCPNBHMNB0.850.90.9512011no/2012no2011no/overlap2012no/2011no2012no/overlapoverlap/2011nooverlap/2012no249Justin Bieber SMS-G
App Name
Description View photos when you receive a message! These
pictures are selected using the keyword “Justin
Bieber”, so they change whenever you receive a
message. You will ﬁnd the photo best for you!
Permissions 17 in total, some are listed below
ACCESS OTHER GOOGLE SERVICES:
Allows apps to sign in to unspeciﬁed Google ser-
vices using the account(s) stored on this Android
device.
VIEW CONFIGURED ACCOUNTS:
Allows apps to see the usernames (email addresses)
of the Google account(s) you have conﬁgured.
SEND SMS MESSAGES:
Allows the app to send SMS messages. Malicious
apps may cost you money by sending messages
without your conﬁrmation.
READ CONTACT DATA:
Allows the app to read all of the contact (address)
data stored on your tablet. Malicious apps may use
this to send your data to other people.
INTERCEPT OUTGOING CALLS:
Allows the app to process outgoing calls and
change the number to be dialed. Malicious apps
may monitor, redirect, or prevent outgoing calls.
Table 1: An App available on Google Play
TERNET, that could be used by malicious apps, and then performs
analysis on a dataset of apps to identify potentially malicious apps
within that set. Sarma et al. [24] take another approach which uses
only permissions to evaluate the risk of an app by examining how
rare permissions are for certain apps in speciﬁc categories.
Barrera et al. [6] present a methodology for the empirical anal-
ysis of permission-based security models using self-organizing
maps. They apply their methodology to analyze the permission
distribution of close to one thousand applications. Their key ob-
servations were (i) the INTERNET permission is the most popular
and hypothesized that most developers request this to request ad-
vertisements from remote servers, (ii) Location-based permissions
are usually requested in pairs i.e. access to both ﬁne and coarse
locations is requested by applications in a majority of cases by de-
velopers and (iii) there are some categories of applications such as
tools and messaging category where pairs of permissions are re-
quested.
Au et al. [5] survey the permission systems of several popular
smartphone operating systems and taxonomize them by the amount
of control they give users, the amount of information they convey
to users and the level of interactivity they require from users.
Further, they discuss several problems associated with extracting
permissions-based information from Android applications.
Dynamic Analysis: Another research direction in Android security
is to use dynamic analysis. Portokalidis [22] propose a security so-
lution where security checks are applied on remote security servers
that host exact replicas of the phones in virtual environments. In
their work, the servers are not subject to the constraints faced by
smartphones and hence this allows multiple detection techniques to
be used simultaneously. They implemented a prototype and show
the low data transfer requirements of their application.
Enck et al. [9] perform dynamic taint
tracking of data in
Android, and reveal to a user when an application may be trying
to send sensitive data off the phone. This can handle privacy
violations since it can determine when a privacy violation is most
likely occurring while allowing benign access to that same data.
However, there is a whole class of malicious apps that this will not
defend against, namely security and monetary focused malware
which send out spam or create premium SMS messages without