test
callq
 inc
0x18(%rsp),%rdx
oxffffffff80120294
sebx
%rax,%rax
oxffffffff802adb53
$0x12,%edi
?
---
## Page 253
backtrace 中还有 sock_ def_write_space()，但并没有调用 panic()的路径。
el000_alloc_rx_buffers_ps()中发生了 panic.
dma_map_single+16③应当残留在backtrace 中，但这次没有，因此应该不是在
的确，调用路径为 el000_alloc_rx_buffers_ps(）-> BUG(）-> panic()。这种情况下
oxff...。 pci_map_single()调用了 dma_map_single()。
pci_map_single()被定义为 inline，因此在转储的 backtrace 中并不会显示为 call
0xffffffff801202a6 :
θxfffffff801202a4 :
exfffffff801202a2 :
oxfffffff801202a0 :
oxfffff8012029f :
oxffffff8012029e :
xfffffff8012029b :
exffffffff80120298 :
θxffffffff80120295 :
0xfffffff80120294 :
crash> dis dma_map_single
dma_addr_t dma_map_single(struct device *dev, void *addr, size_t size, int dir)
[arch/x86_64/kemel/pci-gart.c]
dma_data_direction)direction);
if (swiotlb)
BUG_ON(dir == DMA_NONE);
unsigned long phys_mem, bus;
 return dma_map_single(hwdev = NULL ? NULl : &hwdev->dev, ptr, size, (enum
www.TopSage.com
ud2a
ine
push
oxffffffffffffffff(&rax),%al
$0x70,sdh -
oxffff801202ae
srax
srbx
%rdx,%rdi
%rdi,%rbp
$0x3,%ecx
srbp
HACK#40实时进程停止响应|235
-③
242
---
## Page 254
243
ipmi_wdog_pretimeout_handler()调用了 panic(）（@-1)，panic()中有个参数为字符
①调用了 panic()，对应于下面源代码中的④。
backtrace 的#12中有个 ipmi_wdog_pretimeout_handler()，调查一下。
确认 backtrace (IPMI 篇）
236|
static void ipmi_ wdog_pretimeout_handler(void *handler_data)
[drivers/char/ipmilipmi_ watchdog.c]
oxffffffa01c4892 :cmp
oxffffffa01c488b :
$0xffffffffa01c4e6f,&rdi
oxffffffa01c4884 :
oxffffffffa01c4892
Oxffffffffa01c4882 
oxffffffa01c4880 :
01c9390
#oxffffffffa
Oxffffffa01c4877 : je
01c9391
#oxffffffffa
xffffa01c4870 : cmpb 
Oxffffffa01c486f :
crash> dis ipmi_wdog_pretimeout_handler
第5 章实践内核调试
 if (preaction_val != WDOG_PRETIMEOUT_NONE) {
if (preop_val == WDOG_PREOP_PANIC)
else if (preop_val == WDOG_PREOP_GIVE_DATA) {
panic("watchdog pre-timeout"); ---
data_to_read = 1; 
spin_lock(&ipmi_read_lock);
www.TopSage.com
oxffffffffa01c48e3
$0x2,%al
oxffffffff801378a7
xor
mov
.
$0x0,19226(%rip)
push  &rax
%eax,%eax
$0x1,%al
④-1
---
## Page 255
在应用程序中寻找停止响应的原因。首先用ps命令查看运行中的命令。
再次调查e1000的函数，并没有发现死循环或可能导致停止响应的代码，于是开始
停止响应的原因大致可分为4种，内核和用户应用程序分别有死锁和死循环两种。
确认运行中的进程
2.
下面总结一下目前了解的情况。
watchdog中发生了panic。
这里显示了源代码的字符串Watchdog pre-timeout（④-2），这个转储似乎是IPMI
crash>ps
CallTrace：{:e100:e1000_alloc_rxbuffers_ps+512}
invalid operand:000[1] SMP
Kernel BUGatpanic:75
..........cut here].........[please bite here]........
Kernel panic-not syncing:watchdog pre-timeout-
crash>log
PIDPPIDCPU
由于是IPMI watchdog，所以中断并没有被禁止。
这次的现象为IPMIwatchdog超时中发生了panic。而且，据此可以推断内核
{:ipmi_msghandler:ipmi_smi_watchdog_pretimeout+53}
{:ipmi_watchdog:ipmi_wdog_pretimeout_handler+35]
{sock_def_write_space+18}
或用户应用程序停止响应的可能性很高。
{do_softirq+49}{apic_timer_interrupt+133}
{runtimer_softirq356}{_do_softirq+8]}
{:ipmi_si:smi_timeout+0}
{:ipmi_si:smi_timeout+72}
{:ipmi_si:smi_event_handler+490}
{:ipmi_si:handle_flags+87}
{:ipmi_watchdog:ipmi_wdog_pretimeout_handler+0}
TASK
www.TopSage.com
STMEMVSZRSSCOMM
④-2
244
---
## Page 256
245
程调度策略。
运行中的进程为loop.sh。下面收集该进程的信息，task 命令可以查看优先级和进
238|第5章实践内核调试
进程比较一下。
用 ps 命令查看 utime 很方便。下面是用 ps 命令查看 CPU 消耗时间的结果，与其他
核空间CPU占用的命令为stime）。
间消耗的CPU 时间（即time(l)命令的“user”值)。调度器会更新该值（查看内
看看它们占用了多少CPU。应用程序可以用 utime 查看，utime 的结果是在用户空
两个loop.sh 的优先级都是99，可知是轮转调度（round robin）中的实时进程，来
an dae I ez7t ys9s  ps -t
crash> task 4224 | grep utime
policy = 2, 
M- A1nod dau6 1 tzt xsez  task 4223 | grep policy -w
crash> task 4224 | grep prio
crash> task 4223 | grep prio
crash>
>42244046
>42234046
utime = 50252,
utime = 50303,
policy = 2, 
START TIME: 266
rt_priority = 99,
static_prio = 120,
prio = 0, 
rt_priority = 99,
static_prio = 120,
prio = 0; 
USER TIME: 203
RUN TIME: 00:27:31
10057b147f0
1007e3087f0
www.TopSage.com
CPU: 1 COMMAND: "sshd"
RU 0.1 53532 1168 loop.sh
RU 0.1 53532 1168 l0oop.sh
---
## Page 257
本 hack 介绍了在内核转储中查看 utime 的结果，发现实时进程死循环的例子。
总结
是个不自然的值，但是如果在循环中进行了进程调度，就不能过于信赖这个值。
程，就要再想其他办法，如跟其他进程进行比较等。如果是死循环，其utime 应该
执行之后立即停止响应，因此 watchdog 运行了55 秒，如果是长时间持续运行的进
GDB(“HACK#5调试器(GDB)的基本用法(之一Y")进行复现等。另外，loop.sh
其他方法，如利用 strace（参见“HACK#43使用 strace 寻找故障原因的线索”）
根据内核转储分析应用程序就到此为止了。要分析应用程序方面的原因，需要用到
loop.sh 一直在运行，也就是说，可以判断为死循环。
都没有，而loop.sh 光是在用户空间就占用了 50 秒，因此并没有进行进程调度，
10 秒，因此在停止响应之后 50～60 秒之间发生了 panic。 sshd、bash连1秒的时间
为 90，因此停止响应 60 秒之后发生了 panic，而 watchdog 守护进程的 interval 为
utime 是 USERTIME，单位为毫秒。这次 IPMI watchdog 的 pretimeout 为 30，timeout
SYSTEM TIME: 2651
PID: 4224 TASK: 10057b147f0
SYSTEM TIME: 2628
PID: 4223 TASK: 1007e3087f0
SYSTEM TIME: 30
START TIME: 267
PID: 4046 TASK: 100793aa7f0
START TIME: 1863
START TIME: 1862
:
SYSTEM TIME: 283
USER TIME:5
USER TIME: 5
USER TIME:!
RUN TIME: 00:00:54
RUN TIME: 00:00:55
RUN TIME: 00:27:30
询问是否在e1000 中发生了 panic。这次为了复现该问题，故意增加网络负载，故
实际上，同样的现象发生时，el000 的函数在 backtrace 中反复出现，因此客户曾
50252
50303
3
www.TopSage.com
CPU: 1 COMMAND: "l0op.sh"
CPU: 0 COMMAND: "loop.sh"
CPU: 1 COMMAND: "bash"
HACK#40 实时进程停止响应| 239
, argv=)
#l 0x00000000401fa7 in iwrite (fd=1, buf=0x10169000 "", size=512) at dd.c:782
pp toprd. d- qpb #
(gdb)bt
25921 pts/3 D+ 0:00 d if /dev/mtd0 of a.dat bs 131072 count 1
# ps ax I grep dd 
2 0x000000000040200b in write_output () at dd.c:808
CPU 使用率
低（几乎为0）
低（几乎为0）
可能为各种情况
高（几乎为100%）
www.TopSage.com
S或者D
S或者D
R或者S
R
进程状态
248
---
## Page 260
249
用(c)被调用后，又调用了MTD设备写入函数mtd_write(B)。
接下来用crash命令看看内核内部的情况。从strace的调查结果可知，write系统调
可能性更大，进一步说，write系统调用上出现问题的可能性更大。
是现在使用的MTD设备，写入也有几百KBps。因此，512B的写入本应在毫秒级
观上来看这段时间太长了。硬盘写入的吞吐量大约是几MBps到几十MBps，即便
从上述跟踪结果可以看出，write系统调用写入512B内容需要花费大约13秒，
找故障原因的线索”。
法用strace进行跟踪。关于strace的详细内容请参见“HACK#43使用strace寻
strace工具可以跟踪进程执行的系统调用。对于运行中的进程，也可以使用下述方
此外，我们还用另一种方法调查问题出在进程中还是内核中，那就是使用strace，
242
从上述GDB和strace的调查结果可知，与进程的代码相比，内核内部出现问题的
别完成。另外应当注意，尽管花费了这么长时间，系统调用本身却是成功的。
《>
14:49:14,,15
14:49:14r(,,51) =512
/ib64/libc.so.6
PID:4313TASK:ffff81022760d040 CPU:1COMMAND:"dd"
crash>bt-t4313
=512
>