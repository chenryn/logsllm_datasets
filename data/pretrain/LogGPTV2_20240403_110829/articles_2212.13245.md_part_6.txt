empty test cases. The third most ignored rule category, design, is about the
bad design of the software. Some examples of this group are unstable tests,
duplicated string literals, and using randomized data in test cases.
Having rule category issues such as naming conventions, unused code, and
bad design indicate that developers tend to write AIOps code in an ad hoc
manner, which hinders their project’s reusability and maintainability. Also,
looking at the most violated rules and rule categories, we realize paying more
attention to details can increase the quality of AIOps solutions. For example,
3 out of the top-10 violated rules are related to naming conventions. Only
following the naming conventions can reduce the number of issues heavily.
Furthermore, since most of the rules (8 of top-10) and rule categories (9 of
top-10) are common in AIOps and ML projects, AIOps projects can benefit
from the quality assessment and quality assurance tools and techniques that
have been developed for ML systems. The above points are discussed in more
detail in Section 4.
StudyingtheCharacteristicsofAIOpsProjectsonGitHub 27
Summary of RQ3
Although AIOps projects have an adequate amount of comments com-
pared to the ML and General baselines, AIOps projects exhibit poorer
quality than the two baselines in terms of a variety of quality met-
rics (e.g., bugs, code smells, and technical debt). In particular, code
smells are the dominant type of issue in AIOps projects. We also iden-
tify the most common issues in AIOps projects: naming convention,
unused code, and bad design are the top-3 violated rule categories.
Moreover, we observe the similarity of violated rules between AIOps
and ML projects. Future efforts are needed to reduce the issues iden-
tified in this work and improve the quality of AIOps projects, e.g., by
designingtoolsforfixingbugs/smellsorderivingcodingguidelines.We
also encourage future AIOps projects to reduce the ad-hocness of their
code to improve reusability and maintainability.
4 Discussion
NaturalLanguageProcessing(NLP)techniquescanreceivemoreat-
tention in AIOps in the coming years.OurresultsinSection3.2indicate
that only 2% of the studied projects use NLP techniques. On the other hand,
approximately one-third of the projects use logs as their input data. Prior
researchhasshownthatlogscanapproximatelyberepresentedasnaturallan-
guage text since the logs are generated by logging statements in the source
code written by humans (Hindle et al., 2016). Other works also illustrate that
software systems are even more predictable and repetitive than human lan-
guages, such as English (Allamanis and Sutton, 2014; Tu et al., 2014). Hence,
we suggest that AIOps projects can benefit from leveraging the advances in
NLP techniques to analyze and model the input data such as logs.
More attention should be paid to increasing automation and re-
ducing human interventions in AIOps solutions. Regarding the goal
of the studied projects, most projects aim to detect and predict anomalies,
monitor, and analyze the root causes of failures. All these systems need hu-
man interventions when the goal is reached. For example, when an anomaly
is detected, an operator should decide the next required action to handle the
situation. As demonstrated in Figure 5, only 1% of projects have self-healing,
meaning that the system can make necessary changes when needed without
humaninterventions.WebelieveAIOpssolutionsshouldmovetowardbecom-
ing more automated, detecting/predicting the incidents and resolving them
autonomously.
Paying more attention to simple details can increase the quality of
AIOps solutions. We identify the most common issues in AIOps projects in
28 RoozbehAghilietal.
Section3.3.3.Whenlookingatthemostviolatedrules,someofthemarechal-
lengingandtime-consumingtofixandneedstructuralchangesincode,suchas
python:S3776 (cognitive complexity of functions) and python:S1192 (duplica-
tionofstringliterals).Ontheotherhand,someareeasytohandle.3ofthe10
mostviolatedrulesarerelatedtonamingconventions.Onlyfollowingthenam-
ing conventions can reduce the number of issues heavily. Furthermore, rules
such as python:S2208 (using wildcard imports) and python:S1481 (removing
unused variables) are quick to determine and easy to fix. Identifying the most
commonissueshelppractitionersandresearchersinthefieldtobecomeaware
of them and take measurements to reduce them.
Most of the tools and techniques used for the quality assurance of
machine learning systems can also be used for AIOps solutions. Our
results indicate that a high proportion of issues in ML and AIOps techniques
are the same (80% of most violated rules and 90% of most violated rule cate-
gories are shared). Besides, the quality assurance of ML systems has received
special attention in the previous years, and different approaches have been
built to preserve the quality of ML-based systems (Braiek and Khomh, 2022;
Nakajima,2018;Nikanjametal.,2021,2022;Pothetal.,2020;Tambonetal.,
2023).Hence,webelievemostofthedevelopedtoolsandtechniquesforquality
assurance in ML systems can be utilized in AIOps techniques. Applying these
toolsandtechniquesmayreducethequalityissuesmentionedinSection3.3.3.
5 Threats to Validity
This section discusses threats to the validity of our results.
External validity. In this work, we identified and studied a set of AIOps
projects on GitHub. These projects may not cover all AIOps projects on
GitHub, those hosted on other platforms, or private projects. To maximize
our coverage of AIOps projects and avoid false positives, we follow a process
that combines automated search (two rounds), keywords expansion, manual
verification, and filtering. Two authors of the paper carefully examined each
of the candidate projects to select the ones for our study. The third author
stepping in to resolve any disagreement. Nevertheless, future work can lever-
age our replication package and extend our study by analysing more AIOps
projects.
We also collect the ML baseline based on two keywords of “machine learn-
ing” and“deeplearning”.However,havingthesetwokeywordsmaynotinclude
all machine learning repositories on GitHub. Future work can expand our re-
sults by analyzing more projects extracted from more diverse keywords.
Internal validity. We study the code quality metrics of the AIOps projects
using a set of metrics (e.g., code smells). Nevertheless, these metrics may not
accurately represent the quality of the projects. To reduce the effect of this
threat, we leveraged a variety of metrics that represent different aspects of
StudyingtheCharacteristicsofAIOpsProjectsonGitHub 29
each project. These metrics have also been used in other articles to measure
code quality (Businge et al., 2019; Lenarduzzi et al., 2019; Tan et al., 2018).
Constructvalidity.ToexpandoursetofAIOpsprojects,weperformpattern
mining and choose 4 out of 194 pairs of keywords. We choose these 4 pairs of
keywords after a systematic process and based on the discussions between the
authors, in order to reduce the bias. All of the keywords are among the most
frequently used keywords.
To answer RQ2, we use qualitative analysis to categorize the input data,
analysis techniques, and goals of each AIOps project. Our results may be bi-
ased by personal deductions like any other qualitative study. In order to mit-
igate this threat, two authors of the paper performed the qualitative analysis
carefully and followed a 5-step process to deduce the categories. We achieve a
Cohen’s kappa value of 0.81 which shows a strong and reliable agreement.
We rely on the code issues detected by SonarQube to answer our RQ3.
We are aware that SonarQube might have false positives or false negatives.
However, SonarQube claim to have zero false-positives for code smells and
bugs9. Furthermore, we choose SonarQube since it is one of the most used
tools for analyzing code quality (Lenarduzzi et al., 2017, 2018).
6 Related Work
This work studies AIOps projects on GitHub and analyzes the quality of the
projects using SonarQube. Thus, we discuss the related work on the following
three aspects.
AIOpssolutions.InrecentyearsandwiththeemergenceofAIOps,moreand
morestudieshavebeenconductedinthisfield.Priorworkshavecomeupwith
different solutions for various problems. Anomaly detection (He et al., 2018;
Limetal.,2014),failureprediction(Lietal.,2020;Linetal.,2018;Zhaoetal.,
2021,2020),taskandjobfailures(El-Sayedetal.,2017;Gaoetal.,2020;Islam
andManivannan,2017;Rosàetal.,2015),ticketmanagement(Xueetal.,2016,
2018),self-healing(Dingetal.,2014;Louetal.,2013,2017),andissuediagnosis
(Luo et al., 2014) are among the most studied topics in AIOps solutions. For
example, Gao et al. (Gao et al., 2020) attempt to predict the task failures in
cloud data centers. They use Google cluster trace (Reiss et al., 2012) and can
achievea93%accuracyintaskfailureprediction.Similarly,Islametal.(Islam
and Manivannan, 2017) characterize the failures of the Google cluster trace
and then uses LSTM (Long Short-Term Memory) to predict the task failures.
There also exists studies focusing on hardware failures (Botezatu et al., 2016;
Mahdisoltanietal.,2017;Pinheiroetal.,2007;SchroederandGibson,2007;Xu
et al., 2018), where they perform large-scale studies on disk failures. Different
fromthesestudies,inthiswork,weperformanempiricalstudytounderstand
the practices and characteristics of real-world AIOps projects on GitHub.
Characterizing GitHub projects. As the biggest hosting service for open-
source software, GitHub has been studied remarkably in the past years. Some
9https://docs.sonarqube.org/latest/user-guide/rules/
30 RoozbehAghilietal.
studies focus on the best approaches for finding the most prominent reposi-
tories on GitHub (Dabic et al., 2021; Kalliamvakou et al., 2014, 2016). Many
studiesalsohaveusedGitHubasasourceforminingsoftwarerepositories(Businge
et al., 2019; Coelho et al., 2018; Guzman et al., 2014; Horschig et al., 2018;
Kallis et al., 2021; Lopes et al., 2017; Manes and Baysal, 2021; Subramanian
et al., 2020; Vadlamani and Baysal, 2020; Wessel et al., 2018). For example,
Vadlamani et al. (Vadlamani and Baysal, 2020), along with Subramanian et
al. (Subramanian et al., 2020) and Horschig et al. (Horschig et al., 2018) try
to characterize the developers of open-source software. As another example,
Kallis et al. (Kallis et al., 2021) try to predict the issue types of projects.
More similar to our study, Ghrairi et al. (Ghrairi et al., 2018) study the state
of Virtual Reality (VR) projects extracted from GitHub. In another work,
Coppola et al. (Coppola et al., 2019) characterize the popularity of Kotlin in
Android projects by analyzing 1,232 applications on GitHub. However, as far
asweknow,nostudieshavebeenconductedyetonthesubjectofAIOps.Our
work is the first study that uses GitHub to characterize and analyze AIOps
projects.
Analyzing code quality using SonarQube.SonarQubeisoneofthemost
popular static code analyses used both in academia (Lenarduzzi et al., 2017,
2018) and industry (Vassallo et al., 2020). Previous research has conducted
many analyses on the code quality of open-source software projects using
SonarQube. Businge et al. (Businge et al., 2019) use SonarQube and analyses
the source code of 119 applications to find their number of bugs. In another
work, Tan et al. (Tan et al., 2018) study 9 Apache software systems written
in Python to investigate their technical debt. Lenarduzzi et al. (Lenarduzzi
et al., 2019) also investigate the technical debt of 33 Apache systems written
in Java. They find that the amount of code smell is much higher than bugs or
vulnerabilities in their set of projects. They also report that the code smells
with the severity level major take the longest time to get fixed. Compared to
thementionedstudies,ourpaperanalyzesthequalityof6,101projects,which
ismuchhigherthaninthepreviousstudies,makingourresultsmoreconfident
and robust.
7 Conclusion and Future Work
This work studies the characteristics of AIOps projects on GitHub and com-
pares them with two baselines (ML and General projects). We combine both
quantitativeandqualitativeanalysestounderstandthecurrentstateofAIOps
solutions. Specifically, we illustrate the state of AIOps projects on GitHub in
RQ1,determinethemostcommoninputdata,techniques,andgoalsofAIOps
solutionsinRQ2,andidentifythemostcommonissuesintermsofcodequality
inRQ3.WeobservethatAIOpsprojectsarerelativelynewandgrowingfastin
recentyears.However,thequalityoftheAIOpsprojectsispoorercomparedto
thebaselines.Furthermore,weuncoverthecommonpatternsthatexistinthe
inputdata,analysistechniques,andgoalsoftheAIOpsprojects.Practitioners
StudyingtheCharacteristicsofAIOpsProjectsonGitHub 31
andresearcherscanlearnfromthesepatternsandadoptAIOpssolutionsthat
are optimal for their specific application scenarios. Our findings shed light on
future efforts to improve AIOps practices, for example, to pay more attention
to the weak aspects (e.g., self-healing), or reduce the ad-hocness of coding to
improve AIOps quality.
Our study could be extended in different ways. First, we study the AIOps
projectspubliclyavailableonGitHub.However,theremightbedifferencesbe-
tween open-source projects and closed-source projects which are implemented
forprivatedatasetsincompanies.Thus,oneextensiontoourstudycouldbeto
gather and analyze the industrial AIOps solutions and report any similarities
and differences with AIOps projects on GitHub. Second, based on our results
regarding the inputs, techniques, and goals of AIOps solutions, future work
may develop a pipeline to automate or augment the development of AIOps
solutions.Finally,wediscussedthepossibilityofadoptingavailablequalityas-
surance techniques in machine learning systems into AIOps solutions. Hence,
futureworkmayactuallyimplementthesetechniquesforAIOpssolutionsand
report their effects in terms of improving AIOps code quality.
References
AllamanisM,SuttonC(2014)Miningidiomsfromsourcecode.In:Proceedings
ofthe22ndacmsigsoftinternationalsymposiumonfoundationsofsoftware
engineering, pp 472–483
Artstein R, Poesio M (2008) Inter-coder agreement for computational linguis-
tics. Computational linguistics 34(4):555–596
Basili VR, Selby RW, Hutchens DH (1986) Experimentation in software engi-
neering. IEEE Transactions on software engineering (7):733–743
BotezatuMM,GiurgiuI,BogojeskaJ,WiesmannD(2016)Predictingdiskre-
placement towards reliable data centers. In: Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, pp 39–48
Braiek HB, Khomh F (2022) Testing feedforward neural networks training
programs. ACM Trans Softw Eng Methodol DOI 10.1145/3529318, URL
https://doi.org/10.1145/3529318
BusingeJ,OpenjaM,KavalerD,BainomugishaE,KhomhF,FilkovV(2019)
Studying android app popularity by cross-linking github and google play
store. In: 2019 IEEE 26th International Conference on Software Analysis,
Evolution and Reengineering (SANER), IEEE, pp 287–297
Chen Z, Cao Y, Liu Y, Wang H, Xie T, Liu X (2020) A comprehensive study
on challenges in deploying deep learning based software. In: Proceedings of
the28thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposium on the Foundations of Software Engineering, pp 750–762
Chen Z, Liu J, Gu W, Su Y, Lyu MR (2021) Experience report: deep
learning-based system log analysis for anomaly detection. arXiv preprint
arXiv:210705908