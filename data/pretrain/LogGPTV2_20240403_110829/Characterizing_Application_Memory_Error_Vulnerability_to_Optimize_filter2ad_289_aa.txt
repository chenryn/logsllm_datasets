title:Characterizing Application Memory Error Vulnerability to Optimize
Datacenter Cost via Heterogeneous-Reliability Memory
author:Yixin Luo and
Sriram Govindan and
Bikash Sharma and
Mark Santaniello and
Justin Meza and
Aman Kansal and
Jie Liu and
Badriddine Khessib and
Kushagra Vaid and
Onur Mutlu
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Characterizing Application Memory Error Vulnerability to
Optimize Datacenter Cost via Heterogeneous-Reliability Memory
Yixin Luo
Justin Meza
Bikash Sharma* Mark Santaniello*
Sriram Govindan*
Aman Kansal*
Carnegie Mellon University, PI:EMAIL, {meza, onur}@cmu.edu
*Microsoft Corporation, {srgovin, bsharma, marksan, kansal, jie.liu, bkhessib, kvaid}@microsoft.com
Badriddine Khessib* Kushagra Vaid* Onur Mutlu
Jie Liu*
Abstract—Memory devices represent a key component of
datacenter total cost of ownership (TCO), and techniques used
to reduce errors that occur on these devices increase this
cost. Existing approaches to providing reliability for memory
devices pessimistically treat all data as equally vulnerable
to memory errors. Our key insight is that there exists a
diverse spectrum of
tolerance to memory errors in new
data-intensive applications, and that traditional one-size-ﬁts-all
memory reliability techniques are ineﬃcient in terms of cost.
For example, we found that while traditional error protection
increases memory system cost by 12.5%, some applications
can achieve 99.00% availability on a single server with a large
number of memory errors without any error protection. This
presents an opportunity to greatly reduce server hardware cost
by provisioning the right amount of memory reliability for
diﬀerent applications.
Toward this end, in this paper, we make three main con-
tributions to enable highly-reliable servers at low datacenter
cost. First, we develop a new methodology to quantify the
tolerance of applications to memory errors. Second, using our
methodology, we perform a case study of three new data-
intensive workloads (an interactive web search application, an
in-memory key–value store, and a graph mining framework)
to identify new insights into the nature of application memory
error vulnerability. Third, based on our insights, we propose
several new hardware/software heterogeneous-reliability mem-
ory system designs to lower datacenter cost while achieving
high reliability and discuss their trade-oﬀs. We show that our
new techniques can reduce server hardware cost by 4.7% while
achieving 99.90% single server availability.
Keywords—memory errors, software reliability, memory ar-
chitectures, soft errors, hard errors, datacenter cost, DRAM.
I. Introduction
Warehouse-scale datacenters each consist of many thou-
sands of machines running a diverse set of applications and
comprise the foundation of the modern web [1]. While these
datacenters are vital to the operation of companies such
as Facebook, Google, Microsoft, and Yahoo!, reducing the
cost of such large-scale deployments of machines poses a
signiﬁcant challenge to these and other companies. Recently,
the need for reduced datacenter cost has driven companies to
examine more energy-eﬃcient server designs [2] and build
their datacenter installations in cold environments to reduce
cooling costs [3, 4] or use built-in power plants to reduce
electricity supply costs [5].
There are two main components of the total cost of
ownership (TCO) of a datacenter [1]: (1) capital costs (those
associated with server hardware) and (2) operational costs
(those associated with providing electricity and cooling). Re-
cent studies have shown that capital costs can account for the
majority (e.g., around 57% in [1]) of datacenter TCO, and
thus represent the main impediment for reducing datacenter
TCO. In addition, this component of datacenter TCO is only
expected to increase going forward as companies adopt more
eﬃcient cooling and power supply techniques.
Of the dominant component of datacenter TCO (capital
costs associated with server hardware), the cost of server
processors and memory represents the key component—
around 60% in modern servers [6]. Furthermore, the cost
of the memory in today’s servers is comparable to that of
the processors, and is likely to exceed processor cost for
data-intensive applications such as web search and social
media services, which use in-memory caching to improve
response time (e.g., a popular key–value store, Memcached,
has been used at Google and Facebook [7] for this purpose).
Exacerbating the cost of memory in modern servers is the
use of memory devices (such as dynamic random access
memory, or DRAM) that provide error detection and cor-
rection. This cost arises from two components: (1) quality
assurance testing performed by memory vendors to ensure
devices sold to customers are of a high enough caliber
and (2) additional memory capacity for error detection and
correction. Device testing has been shown to account for an
increasing fraction of the cost of memory for DRAM [8, 9].
The cost of additional memory capacity, on the other hand,
depends on the technique used to provide error detection and
correction.
Table 1 compares several common memory error detection
and correction techniques in terms of which types of errors
they are able to detect/correct and the additional amount
of capacity/logic they require (which, for DRAM devices,
whose design is ﬁercely cost-driven,
to
cost). Techniques range from the relatively low-cost (and
widely employed) Parity, SEC-DED, Chipkill, and DEC-
TED, which use diﬀerent error correction codes (ECC) to
detect and correct a small number of bits or chip errors,
to the more expensive RAIM and Mirroring techniques that
replicate some (or all) of memory to tolerate the failure of
an entire DRAM dual in-line memory module (DIMM). The
additional cost of memory with high error-tolerance can be
signiﬁcant (e.g., 12.5% with SEC-DED ECC and Chipkill
and as high as 125% with Mirroring).
Yet even with well-tested and error-tolerant memory de-
vices, recent studies from the ﬁeld have observed a ris-
ing rate of memory error occurrences [13–15]. This trend
presents an increasing challenge for ensuring high perfor-
mance and high reliability in future systems, as memory
errors can be detrimental to both.
In terms of performance, existing error detection and
correction techniques incur a slowdown on each memory
access due to their additional circuitry [15, 16] and up to
is proportional
Technique
Parity
SEC-DED
DEC-TED
Chipkill [10]
RAIM [11]
Mirroring [12]
Error detection (correction) Added capacity Added logic
2n-1/64 bits (None)
2/64 bits (1/64 bits)
3/64 bits (2/64 bits)
2/8 chips (1/8 chips)
1/5 modules (1/5 modules)
2/8 chips (1/2 modules)
1.56%
12.5%
23.4%
12.5%
40.6%
125%
Low
Low
Low
High
High
Low
Table 1: Memory error detection and correction techniques. “X/Y Z”
means a technique can detect/correct X out of every Y failures of Z.
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.50
467
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
an additional 10% slowdown due to techniques that operate
DRAM at a slower speed to reduce the chances of random bit
ﬂips due to electrical interference in higher-density devices
that pack more and more cells per square nanometer [17].
In addition, whenever an error is detected or corrected on
modern hardware, the processor raises an interrupt that must
be serviced by the system ﬁrmware (BIOS), incurring up
to 100 µs latency—roughly 2000× a typical 50 ns memory
access latency [18]—leading to unpredictable slowdowns.
In terms of reliability, memory errors can cause an applica-
tion to slow down, crash, or produce incorrect results [19].
Software-level techniques such as the retirement of regions
of memory with errors [15, 20–22] have been proposed
to reduce the rate of memory error correction and prevent
correctable errors from turning into uncorrectable errors over
time. Hardware-level techniques, such as those listed in Ta-
ble 1, are used to detect and correct errors without software
intervention (but with additional hardware cost). All of these
techniques are applied homogeneously to memory systems
in a one-size-ﬁts-all manner.
Our goal
in this paper is to understand how tolerant
diﬀerent data-intensive applications are to memory errors
and design a new memory system organization that matches
hardware reliability to application tolerance in order to
reduce system cost. Our key insight is that data-intensive
applications exhibit a diverse spectrum of tolerance to mem-
ory errors—both within an application’s data and among
diﬀerent applications—and that one-size-ﬁts all solutions are
ineﬃcient in terms of cost. The main idea of our approach
leverages this observation to classify applications based
on their memory error tolerance and map applications to
heterogeneous-reliability memory system designs managed
cooperatively between hardware and software (for example,
error-tolerant portions of data from an application may
reside in inexpensive less-tested memory with no ECC with
software-assisted data checkpointing, but error-vulnerable
portions of its data should be placed in ECC memory)
to reduce system cost. Toward this end, we provide the
following contributions:
• A new methodology to quantify the tolerance of applica-
tions to memory errors. Our approach measures the eﬀect
of memory errors on application correctness and quantiﬁes
an application’s ability to mask or recover from memory
errors.
• A comprehensive case study of the memory error toler-
ance of three data-intensive workloads: an interactive web
search application, an in-memory key–value store, and a
graph mining framework. We ﬁnd that there exists an order
of magnitude diﬀerence in memory error tolerance across
these three applications.
• An exploration of the design space of new memory system
organizations, which combine a heterogeneous mix of reli-
ability techniques that leverage application error tolerance
to reduce system cost. We show that our techniques can
reduce server hardware cost by 4.7%, while achieving
99.90% single server availability.
II. Background and Related Work
A. Memory Errors and Mitigation Techniques
Modern devices use DRAM as their main memory. DRAM
stores its data in cells in the form of charge in a capacitor.
Over time, the charge in DRAM cells leaks and must be
refreshed (every 64 ms in current devices). When data is
accessed in DRAM, cell charge is sensed, ampliﬁed, and
transmitted across a memory channel. In case any of the
components used to store or transmit data fails, a memory
error can occur.1
There are two main types of memory errors: (1) soft
or transient errors and (2) hard or recurring errors.2 Soft
memory errors occur at random due to charged particle
emissions from chip packaging or the atmosphere [28].
Hard memory errors may occur from physical device defects
or wearout [13–15], and are inﬂuenced by environmental
factors such as humidity, temperature, and utilization [13, 29,
30]. Hard errors typically aﬀect multiple bits (for example,
large memory regions and entire DRAM chips have been
shown to fail [14, 15, 29]).
Various error correcting codes have been designed to
mitigate these errors. ECC methods diﬀer based on the
amount of additional memory capacity required to detect and
correct errors of diﬀerent severity (Table 1 lists some of these
techniques, discussed in Section I). The eﬀects of hard errors
can be mitigated in the operating system (OS) or BIOS by
retiring aﬀected memory regions when the number of errors
in the region exceeds a certain threshold [15, 20–22]. Region
retirement is typically done at the memory page granularity,
∼4 KB. Retiring pages eliminates the performance overhead
of the processor repeatedly performing detection and correc-
tion and also helps to prevent correctable errors from turning
into uncorrectable errors.
A memory error will remain latent until the erroneous
memory is accessed. The amount of time an error remains
latent depends on how applications’ data are mapped to
physical memory and on the program’s memory access
pattern. For example, the frequency at which data is read
determines how often errors are detected and corrected and
the frequency at which data is written determines how often
errors are masked by overwrites to the erroneous data.
B. Related Work
We categorize related research literature in memory error
vulnerability and DRAM architecture into ﬁve broad classes:
(1) characterizing application error tolerance, (2) hardware-
based memory reliability techniques, (3) software-based
memory reliability techniques, (4) exploiting application
error tolerance, and (5) heterogeneous (hybrid) memory
architectures. We next discuss these each in turn.
Classifying application error tolerance. Controlled error
injection techniques based on hardware watchpoints [16,
31], binary instrumentation [32], and architectural simu-
lation [33], have been used to investigate the impacts of
memory errors on application behavior, including execution
times, application/system crashes, and output correctness.
These works have studied a range of applications includ-
ing SPEC CPU benchmarks, web servers, databases, and
scientiﬁc applications. In general, these works conclude that
not all memory errors cause application/system crashes and
can be tolerated with minimal diﬀerence in their outputs.
1For a detailed background on DRAM operation, we refer the reader
to [23–25].
2Two recent studies [26, 27] examined the eﬀects of intermittent and
access-pattern dependent errors, which are increasingly common as DRAM
technology scales down to smaller technology nodes.
468
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
We generalize this observation to data-intensive applications
and leverage it
to reduce datacenter TCO. Approximate
computing techniques [34–36], where the precision of pro-
gram output can be relaxed to achieve better performance or
energy eﬃciency, oﬀer further opportunities for leveraging
the error-tolerance of application data, though these typically
require changes to program source code.
Hardware-based memory reliability techniques. Various
memory ECC techniques have been proposed (we list the
most dominant ones in Table 1). Using eight bits, SEC-
DED can correct a single bit ﬂip and detect up to two bit
ﬂips out of every 64. DEC-TED is a generalization of SEC-
DED that uses fourteen bits to correct two and detect three
ﬂipped bits out of every 64. Chipkill improves reliability
by interleaving error detection and correction data among
multiple DRAM chips [10]. RAIM [11] is able to tolerate en-
tire DIMMs failing by storing detection and correction data
across multiple DIMMs. Virtualized ECC [37] maps ECC
to software-visible locations in memory so that software
can decide what ECC protection to use. While Virtualized
ECC can help reduce the DRAM hardware cost of memory
reliability, it requires modiﬁcation to the processor’s memory
management unit and cache(s).
Software-based memory reliability techniques. Previous
works (e.g., [15, 22, 38]) have shown that the OS retiring
memory pages after a certain number of errors can eliminate
up to 96.8% of detected memory errors. These techniques,
though they improve system reliability, still require costly
ECC hardware for detecting and identifying memory pages
with errors. Other works have attempted to reduce the impact
of memory errors on system reliability by writing more
reliable software [39], modifying the OS memory alloca-
tor [40], or using a compiler to generate a more error-tolerant
version of the program [41, 42]. Other algorithmic solutions
(e.g., memory bounds checks [43], watchdog timers [43],
and checkpoint recovery [44–46]) have also been applied to
achieve resilience to memory errors.
Exploiting application error tolerance. Flikker [47] pro-