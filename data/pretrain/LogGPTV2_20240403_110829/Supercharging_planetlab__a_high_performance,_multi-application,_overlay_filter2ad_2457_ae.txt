range of values that the router is responsible for, the router does a 
lookup  in  the  fast  path’s  Chord  finger  table  to  forward  it  to  the 
next I3 router. The fast path also handles simple trigger process-
0B payload for I3
100%
90%
81%
40B
80B
)
s
/
p
M
(
e
t
a
r
t
e
k
c
a
p
6.00
5.00
4.00
3.00
2.00
1.00
0.00
0
0.2
0.4
0.6
0.8
1
I3 share
Figure 11. Throughput for combined I3 and I4 traffic with I3 
payload lengths of 0, 40 and 80. Input rate held con-
stant at 5 Gb/s, while IPv4 share is varied.  
ing. So, if a router receives a packet with a trigger that is within 
the  range  of  values  it  is  responsible  for, and  the  fast  path  has  a 
single  matching  filter  in  its  TCAM,  the  packet  is  forwarded  di-
rectly by the NPE. In our current implementation, all other pack-
ets  are  handled  by  the  GPE.  In  particular,  packets  matching 
multiple triggers or requiring more complex trigger processing are 
forwarded to the GPE for processing. Some of these more com-
plex cases could also be shifted to the fast path, but in this initial 
set of experiments, we chose to limit the NPE’s role to only what 
we expect to be the most common cases. 
For  these  experiments,  we  have  focused  on  the  datapath,  so 
only minimal changes to the supplied I3 code were needed (about 
350 lines were added to deal with the interaction with the NPE). A 
complete  system  would  also  require  the  control  mechanisms 
needed to allow the GPE to configure the Chord forwarding tables 
in the fast path and insert filters to match triggers. For the experi-
ments described here, both of these were manually configured. 
We started by performing a set of baseline throughput and la-
tency  tests  similar  those  we  did  for  the  IPv4  application.  The 
results  are  qualitatively  similar,  although  the  version  of  the  I3 
router that runs on the GPE achieves throughput that is generally 
30-40% higher than the Click router. We next did a test to verify 
the operation of the NPE when hosting both the IPv4 and I3 appli-
cations.  For  these  tests,  we  held the  total  input  traffic  at  5  Gb/s 
and varied the fraction of traffic for the I3 application. The results 
are  shown  in  Figure  11.  There  are  3  sets  of  curves,  one  for  I3 
payload lengths of 0 bytes, one for 40 byte payloads and one for 
80  byte  payloads.  The  IPv4  packets  had  slightly  larger  payload 
lengths, in order to match the overall packet length of the I3 pack-
ets (the I3 header is 7 bytes longer than the IPv4 header). For each 
set, we show the output packet rate for the I3 application, the IPv4 
application and the sum of the two. Each of the “sum” curves is 
labeled  with  the  percentage  of  the  input  rate  that  is  achieved  in 
each case. For 0 byte payloads, we can keep up with 81% of the 
input rate when all the traffic is IPv4. For 40 byte payloads, we 
can keep up with the full input rate in all cases and for 80 byte 
100.00
10.00
1.00
0.10
)
s
m
(
y
a
l
e
d
g
n
P
i
0.01
10
mean plus
3(std. dev.)
default
default
8
2
GPE
mean plus
3(std. dev.)
NPE
10,000
100
1,000
input rate (Mb/s)
Figure 12. Latency results for I3 using separate instance of I3 
application for ping traffic and comparing effect of 
scheduling parameters on GPE latency 
payloads, we are handling 90% of the input rate. This last result is 
somewhat anomalous and further study is needed to explain it. 
Figure 12 shows results of a set of experiments to evaluate the 
latency in the I3 case. These experiments were structured differ-
ently from the earlier ones for IPv4. Specifically, we used a com-
pletely separate instance of the I3 application to handle the ping 
traffic (for both GPE and NPE), in order to isolate the ping traffic 
from the background traffic as much as possible. The background 
traffic consisted of packets with 400 byte payloads and was dis-
tributed  across  two  instances  of  the  I3  application.  In  the  GPE 
case, we used a single processor core (as noted earlier, using all 
four cores improves throughput, but has little effect on latency). 
The  most  striking  difference  between  these  results  and  the 
IPv4  results  presented  earlier  is  that  the  delays  are  substantially 
smaller for both the GPE and the NPE. This appears to be largely 
due to the isolation of the ping traffic. For the NPE, the average 
delay is essentially constant at just under 50 μs. For the GPE, the 
default PlanetLab scheduling parameters result in average delays 
of 15-30 ms when the input rate exceeds 300 Mb/s. Reducing the 
PlanetLab  scheduling  parameters  (minToken,  maxToken)  to  2 
reduces the delay by more than a factor of 10. Allowing it to rise 
to 8 gives a delay of 6-8 ms. We found that reducing the schedul-
ing parameters had just a modest impact on throughput, reducing 
the maximum forwarding rate by less than 10%. Note that as the 
number of background instances of I3 is scaled up, the delays in 
the GPE case can be expected to scale up in direct proportion. 
8. RELATED WORK 
There  are  two  main  categories  of  previous  work  that  are  most 
closely related to this research. The first concerns high perform-
ance  implementations  of  overlay  networks  and  the  second  con-
cerns high performance, programmable routers, particularly those 
based on network processors. In the overlay network space, com-
mercial  organizations  have  led  the  way  in  developing  high  per-
formance 
few  published 
descriptions are available. Reference [KO04] describes Akamai’s 
system for delivering streaming audio and video, which includes a 
implementations,  but 
relatively 
description of its cluster-based architecture for the overlay nodes. 
These systems use general purpose servers linked by an Ethernet 
LAN,  which  is  used  to  multicast  streams  from  the  servers  han-
dling  input  processing  to  the  servers  forwarding  packets  to  next 
hops and/or end-users. 
In the programmable router space  [KA02, SP01] describe an 
extensible router that uses an IXP 1200 for the fast path process-
ing and reference [CH02] describes a system that places a general 
purpose plugin processor at each port of a gigabit hardware router. 
There is a much larger body of work relating to active networking 
and extensible routers generally, but the vast majority of this work 
is  primarily  concerned  with  other  issues  than  achieving  high 
performance.  A  more  recent  piece  of  related  work  is  [TU06], 
which describes a proposed design of a backbone router for NSF’s 
GENI initiative [GE06]. 
9. ALTERNATE APPROACHES 
It’s  natural  to  ask  what  other  approaches  might  be  available  to 
improve  the  performance  of  overlay  networks.  PlanetLab  is  al-
ready shifting to higher performance servers with dual processor 
chips and two cores per chip. Four core processor chips are now 
available  and  eight  core  chips  are  expected  soon.  As  we  have 
seen,  while  multi-core  processors  can  boost  throughput,  the  in-
crease is not necessarily proportional to the number of processors. 
Memory  and  network  bandwidth  must  also  be  scaled  up,  and  
even  with  appropriate  hardware  scaling,  limited  locality-of-
reference  in  networking  workloads  may  lead  to  poor  cache  per-
formance,  limiting  the  gains.  In  addition,  the  parallelism  in  the 
workload  must  at  least  match  the  number  of  cores,  if  linear 
speedup is to be achieved. A PlanetLab node hosting many slices 
that each require just a small fraction of the system capacity, can 
potentially  achieve  such  parallelism.  However,  if  the  workload 
places substantial demands on the OS, then the OS must also be 
highly parallel. In particular, the network stack must take advan-
tage of the multiple cores to avoid becoming a bottleneck. Simi-
larly, slices that require a larger share of the system capacity will 
have to be written to take advantage of multiple cores if they are 
to benefit from their presence.  
Of course, even if multi-core systems are properly engineered 
and operating system and application code is structured for paral-
lel execution, there remains the issue of user-space overhead for 
IO-intensive applications. Such overheads contribute significantly 
to the limited performance of typical overlay platforms. The most 
promising  approach to  overcoming  such  overheads  is  to  decom-
pose applications into separate fast path and slow path segments, 
and push the fast path down into the OS kernel or even the net-
work device driver. This can be highly effective, but does trade-
off protection and ease of software development for performance. 
We have chosen to accept that trade-off to take advantage of NPs, 
and argue that a similar choice must be made to get the most out 
of multi-core server blades. To ensure safe operation in this envi-
ronment one must be prepared to place limits on how the fast path 
is programmed, possibly through the use of specialized languages 
such as PLAN [HI98]. 
We should also note that a multi-core server blade, even with 
8 or 16 processor cores does not provide a scalable solution to the 
general challenge of high performance overlay network platforms 
(although it may suffice for PlanetLab in the near term). A scal-
able  solution  requires  an  architecture  that  supports  systems  with 
tens or even hundreds of server blades. One way to approach this 
is to use a scalable, shared memory multiprocessor. Such architec-
tures are common in supercomputers designed for scientific com-
puting, but these systems are typically not engineered for the IO-
intensive workloads that characterize overlay networks (although 
they certainly could be). 
Another approach is to use a cluster of general purpose server 
blades, connected by a high bandwidth switch. Low cost 10 giga-
bit Ethernet switches are now available [FO07] and server blades 
will soon be routinely equipped with such 10 GbE interfaces. This 
approach is quite similar to the architecture developed here. While 
we have chosen a more integrated approach using ATCA compo-
nents, this difference is mainly a matter of physical implementa-
tion,  rather  than  architecture.  The  more  significant  difference 
between  the  two  approaches  is  our  emphasis  on  the  use  of  net-
work processors for the fast path processing. While we argue that 
at the moment, NPs offer significant performance advantages for 
the fast path processing, future improvements in general purpose 
server blades and operating systems could close the gap. 
10. CLOSING REMARKS 
This  work  demonstrates  that  overlay  network  platforms  with 
substantially  higher  levels  of  performance  can  be  implemented 
using  an  integrated  architecture  that  combines  general  purpose 
servers  with  modern  network  processors.  NP  systems  can  out-
perform  general  purpose  servers  by  a  surprisingly  large  margin. 
This is partly due to the richer hardware resources available in the 
NP,  but  a  large part  of  the  difference comes  from  the  operating 
systems used on general purpose servers, which were developed 
and  optimized  for  much  less  IO-intensive  workloads  than  are 
found in both network routers and in most overlay network con-
texts.  As  the  number  of  processor  cores  in  general  purpose  sys-
tems increases to over the next few years, it’s likely that general 
purpose chips will be able to compete more effectively with NPs. 
However,  reaping  the  full  benefits  of  such  systems  in  overlay 
network  settings  will  almost  certainly  require  operating  systems 
that  are  more  IO-oriented  and  will  require  that  applications  be 
programmed to exploit the parallelism provided by the hardware. 
Our  experience  implementing  the  IPv4  and  I3  routers  using 
the  fast  path/slow  path  application  structure  was  very  encourag-
ing. In particular, we found it very straightforward to restructure 
the  I3  code  to conform  to  this  pattern  and  we expect  that  many 
other  PlanetLab  applications  can  be  similarly  modified.  The 
framework  provided  by  the  NPE  made  it  straightforward  to  add 
the I3 code option. The C source files required to implement the 
fast path total less than 2,000 lines, and it took two graduate stu-
dents less than one week to write the code and verify its operation. 
While we have not implemented the control software to allow the 
GPE-resident software to configure the Chord routing tables and 
insert  filters  to  match  triggers,  we  expect  these  additions  to  be 
fairly  routine,  since  they  mainly  require  the  addition  of  code 
modules  to  the  xScale,  which  provides  a  reasonably  friendly 
Linux-based programming environment. 
It’s worth noting that one of the key factors that made the re-
targeting  of  I3  so  straightforward  was  that  we  had  a  well-
engineered existing implementation as our point-of-departure. Our 
experience suggests that before attempting to build an application 
for the NPE environment, it is wise to develop a fully functional 
version  for  the GPE  environment.  This  can  serve  as  a  reference 
point guiding the decisions for exactly what functions to shift to 
the fast path. It also facilitates an incremental development strat-
egy  with  small,  easy  to  manage  steps,  producing  intermediate 
versions of a system that can be useful on their own. 
It’s  natural  to  ask  how  suitable  our  approach  is  for  applica-
tions that are different from the two we have considered here. We 
believe that while it is likely to be  more useful for some applica-
tions than others, the approach is widely applicable, since many 
applications lend themselves to decomposition into a simple, high 
traffic volume fast path and a more complex subsystem to handle 
exceptions and control. For applications such as content-delivery 
networks, the need for disk storage places limits on the role the 
NPEs can play, but even here, packets may be forwarded across 
multiple hops before reaching the location storing the information 
of interest. It seems likely that the associative lookup mechanism 
provided  by  the  TCAM  can  be  a  powerful  tool  for  making  the 
required  routing  decisions.  For  network  measurement  applica-
tions, the ability of the NPE platform to eliminate the large and 
highly variable delays found in general-purpose servers promises 
more accurate measurements with lower computational effort. 
We plan to make the SPP system available as a node in the 
public PlanetLab infrastructure, once we complete our implemen-
tation of the control software. Some additional refinements to the 
software for the LC and NPE need to be implemented before the 
public release, but the core functionality is now complete and we 
expect to have the system available for general use by late 2007. 
REFERENCES 
[BA06]  Bavier, A., N. Feamster, M. Huang, L. Peterson, J. Rexford. “In 
VINI Veritas: Realistic and Controlled Network Experimentation,” 
Proc. of ACM SIGCOMM, 2006. 
[BH06]  Bharambe, A., J. Pang, S. Seshan. “Colyseus: A Distributed Archi-
tecture for Online Multiplayer Games,” In Proc. Symposium on 
Networked Systems Design and Implementation (NSDI), 3/06. 
[CH02]  Choi, S., J. Dehart, R. Keller, F. Kuhns, J. Lockwood, P. Pappu, J. 
Parwatikar, W. D. Richard, E. Spitznagel, D. Taylor, J. Turner and 
K. Wong. “Design of a High Performance Dynamically Extensible 
Router.” In Proceedings of the DARPA Active Networks Conference 
and Exposition, 5/02. 
[CH03]  Chun, B., D. Culler, T. Roscoe, A. Bavier, L. Peterson, M. Wawr-
zoniak, and M. Bowman. “PlanetLab: An Overlay Testbed for 
Broad-Coverage Services,” ACM Computer Communications Re-
view, vol. 33, no. 3, 7/03. 
[CI06]  Cisco Carrier Routing System. At www.cisco.com/en/ 
US/products/ps5763/, 2006 
[DI02]  Dilley, J., B. Maggs, J. Parikh, H. Prokop, R. Sitaraman, and B. 
Weihl. “Globally Distributed Content Delivery,” IEEE Internet 
Computing, September/October 2002, pp. 50-58. 
[FO07]  Force 10 Networks. “S2410 Data Center Switch,” http:// 
www.force10networks.com/products/s2410.asp, 2007 
[FR04]  Freedman, M., E. Freudenthal and D. Mazières. “Democratizing 
Content Publication with Coral,” In Proc. 1st USENIX/ACM Sym-
posium on Networked Systems Design and Implementation, 3/04. 
[GE06]  Global Environment for Network Innovations. http://www.geni.net/, 
2006. 
[HI98]  Mike  Hicks_  Pankaj  Kakkar_  Jonathan  T_  Moore_  Carl  A_ 
Gunter_  and  Scott  Nettles.  “PLAN,  A  packet  language  for  active 
networks,”  In  Proceedings  of  the  Third  ACM  SIGPLAN  Interna-
tional Conference on Functional Programming Languages, 1998. 
Intel IXP 2xxx Product Line of Network Processors. http://www 
.intel.com/design/network/products/npfamily/ixp2xxx.htm. 
[IXP] 
[KA02] Karlin, Scott and Larry Peterson. “VERA: An Extensible Router 
Architecture,” In Computer Networks, 2002.  
[KO00] Kohler, Eddie, Robert Morris, Benjie Chen, John Jannotti and M. 
Frans Kaashoek. “The Click modular router,” ACM Transactions on 
Computer Systems, 8/2000.  
[KO04] Kontothanassis, L. R. Sitaraman, J. Wein, D. Hong, R. Kleinberg, 
B. Mancuso, D. Shaw and D. Stodolsky. “A Transport Layer for 
Live Streaming in a Content Delivery Network,” Proc. of the IEEE, 
Special Issue on Evolution of Internet Technologies, 9/04. 
[PA03]  Pappu, P., J. Parwatikar, J. Turner and K. Wong. “Distributed 
Queueing in Scalable High Performance Routers.” Proceeding of 
IEEE Infocom, 4/03. 
[PE02]  Peterson, L., T. Anderson, D. Culler and T. Roscoe. “A Blueprint 
for Introducing Disruptive Technology into the Internet,” Proceed-
ings of ACM HotNets-I Workshop, 10/02. 
[RA05]  Radisys Corporation. “Promentum™ ATCA-7010 Data Sheet,” 
product brief, available at http://www. radisys.com/files/ATCA-
7010_07-1283-01_0505_datasheet.pdf. 
[RH05]  Rhea, S., B. Godfrey, B. Karp, J. Kubiatowicz, S. Ratnasamy, S. 
Shenker, I. Stoica and H. Yu. “OpenDHT: A Public DHT Service 
and Its Uses,” Proceedings of ACM SIGCOMM, 9/2005. 
[SP01]  Spalink, T., S. Karlin, L. Peterson and Y. Gottlieb. “Building a 
Robust Software-Based Router Using Network Processors,” In 
ACM Symposium on Operating System Principles (SOSP), 2001. 
[ST01]  Stoica, I., R. Morris, D. Karger, F. Kaashoek and H. Balakrishnan. 
“Chord: A scalable peer-to-peer lookup service for internet applica-
tions.” In Proceedings of ACM SIGCOMM, 2001. 
[ST02]  Stoica, I., D. Adkins, S. Zhuang, S. Shenker, S. Surana, “Internet 
Indirection Infrastructure,” Proc. of ACM SIGCOMM, 8/02. 
[TU06]  Turner, J. “A Proposed Architecture for the GENI Backbone Plat-
form,” In Proceedings of ACM- IEEE Symposium on Architectures 
for Networking and Communications Systems (ANCS), 12/2006. 
[VS06]  Linux vServer. http://linux-vserver.org