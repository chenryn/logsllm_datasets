a few countries (for geographical distribution based on ‚Äúcloseness‚Äù
of the probes, please refer to Appendix A.1).
From a networking perspective, Speedchecker coverage also
overshadows RIPE Atlas quite significantly. Compared to RIPE At-
las, Speedchecker offers ‚âà 14√ó probes that are hosted in ‚âà 12K ASes
(compared to 8K for RIPE Atlas VPs as reported in [22]). To quan-
tify the reach of both platforms for real Internet users, we utilize
the user population per ASN dataset from Asia-Pacific Network
Information Centre (APNIC) [5]. The dataset estimates the Internet
user population coverage of ASes using ad-based measurements.
We find that our Speedchecker probes reside in ASes that cover
95.6% of the Internet user population. Conversely, RIPE Atlas [22]
only covers 69.2% of the population. It is also important to point
out that Speedchecker is also growing at an impressive pace as its
population reach has increased by ‚âà 5% since 2019 [8]. Furthermore,
unlike the often privileged deployment of RIPE Atlas VPs within
managed (mostly wired) network environments (that captures the
state of connectivity of non-residential cloud customers) [12, 14, 78],
Speedchecker probes are hosted on end-user devices, and the re-
sulting measurements traverse ISP paths reflecting real end-user
connectivity towards datacenters. Together, both Speedchecker and
RIPE Atlas provides us with a complementary yet most complete
picture of global user reachability to cloud till date.
3.3 Experiments
We are particularly interested in this work to analyze two key as-
pects of cloud connectivity: (i) state of real user latency to current
cloud deployment, especially over wireless paths, and (ii) under-
standing the impact of cloud provider‚Äôs investments in shorten-
ing tenant paths to their infrastructure on end-user connectivity.
To achieve this, we ran TCP pings and ICMP traceroutes from
Speedchecker VPs to cloud region endpoints. Both experiments
66
were conducted in parallel for six-months, i.e., from October 2020
to April 2021. Our collected dataset is available at [60], and the
reproducibility (+ helper) scripts can be found at [25].
Statistics and Confidence. Unless otherwise specified in the rest
of the paper, we opt for median round trip latency as our primary
metric to assess user connectivity performance across multiple
cloud providers. Unlike mean, the median is resilient to outliers that
can occur due to bad performing probes, last-mile inconsistencies,
and other analysis artifacts [32]. For assessing last-mile access
variations (¬ß5), we utilize all recorded measurements.
To make a statistically confident assessment in our analysis, we
calculate the minimum measurement sample size required for each
country. We define the required confidence interval for the mea-
surement as ùëõ = ùëß2√ó ÀÜùëù(1‚àí ÀÜùëù)
. Here, ÀÜùëù is the population proportion,
ùëß is the z-score, ùúñ is the margin of error and ùëõ is the target sample
size. Therefore, to achieve 95% confidence interval with ùúñ = 2%, we
collect >2400 measurements per country.
Probe Selection and Experiment Configuration. Despite its
significant reach and probe density, we encountered several chal-
lenges while using the Speedchecker platform that influenced our
experiment setup. Firstly, we found that the majority of Android
probes on the platform were transient across days and only became
available for use unexpectedly. As a result, we were unable to ex-
plicitly trigger experiments over the same set of probes throughout
our measurement period and instead had to rely on the platform‚Äôs
in-built probe selection per geographical region. Secondly, we were
provided access to the platform with a limited measurement budget
that refreshed at the end of each day. To allow for global coverage
with reliable results, we took inspiration from the experimental
study of Arnold et al. [9]. Of our total per-day quota, we reserved
a few API calls for collecting information about connected VPs,
which we triggered at every four-hour interval. We logged all con-
nected probe IDs, their IP addresses, connection type (router, PC,
or Android), city-specific geolocation, and ASN - which allowed
us to track consistently connected probes on the platform world-
wide. We then configured our active network experiments to cycle
through every country of each continent with at least 100 probes
and targeted all cloud regions within the same continent. For VPs
in continents with low datacenter density, e.g., Africa and South
America, we also targeted datacenters in neighbouring continents,
i.e., Europe and North America (see ¬ß4.3). To not overload the plat-
form with our measurement requests, we employed a self-imposed
rate limit of one measurement request/minute. It took us approxi-
mately two weeks to trigger experiments from all countries on the
platform, at the end of which we restarted the cycle.
We use TCP ping and ICMP traceroute to estimate end-to-
end latencies and distance between users and cloud datacenters,
respectively. Overall, we collected over 3.8M ping data points and
7+M unique traceroutes within our study period. The majority of
the data points are collected from probes in Europe (around 50%),
followed by Asia (‚âà20%) and North America (‚âà10%). Both Africa
and South America have almost similar overall contributions in
our dataset, with intra-continental taking the larger share over
inter-continental measurements (‚âà70-30 ratio).
We compared the end-to-end latencies from ICMP and TCP mea-
surements over Speedchecker for each 
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Dang and Mohan et al.
Figure 3: Median latency from Speedchecker VPs to the clos-
est datacenter worldwide. Geographical ‚Äúcloseness‚Äù is still
the primary driving factor for better QoS as countries with
in-house cloud deployment achieve much lower latencies
than countries without. Africa shows the most uneven per-
formance due to sparse and concentrated datacenter avail-
ability favoring southern countries.
pair and found little-to-no difference between the two protocols.
Latencies over TCP tend to be slightly lower than ICMP (within
2% range), which we attribute as possible outliers. The trend de-
parts significantly in RIPE Atlas where ICMP latencies are consis-
tently (and extensively) larger than TCP - especially in Asia, EU,
SA, and NA [22]. In both platforms, TCP has lower variance than
ICMP, although the median values of the two are comparable in
Speedchecker (see Figure 15 in Appendix A.2 for details). Therefore,
throughout the rest of the paper, we only use TCP latencies for
RIPE Atlas but use both TCP and ICMP interchangeably when ana-
lyzing Speedchecker experiments. We solely rely on latencies from
traceroutes when investigating the impact of wireless last-mile
(¬ß5) and cloud-ISP peering agreements (¬ß6) on cloud access.
Processing Traceroutes: We use PyASN [41] to resolve IP-level
traceroutes to AS-level paths. For any unresolved router hops (ex-
cluding those with private IP addresses) we use Team Cymru IP-to-
ASN mapping tool [24]. We further query PeeringDB [1] and enrich
our AS-level topology with additional information, such as organi-
zation name, location, network type, etc. This phase allows us to
accurately identify the serving and transit ISPs on the path respon-
sible for managing VP traffic. Furthermore, we specifically identify
the presence of Internet eXchange Points (IXPs) on user paths to
cloud using CAIDA IXP dataset [17]. We use GeoIPLookup [37] to
geolocate all on-path router hops. However, since such geolocation
databases are known to be quite inaccurate [50, 73], we refrain from
making any geographical ISP-to-cloud traffic routing assessments
in this study and leave that analysis for future work.
4 CLOUD ACCESS LATENCY
We now analyze the cloud access latencies from 115,000 Speed-
checker wireless VPs and compare them against RIPE Atlas [22].
4.1 Intra-Continental Latency
We start by providing an overview of the access latencies at the
global scale shown in Figure 3. The world map represents the me-
dian RTT from ping measurements towards the closest1 cloud dat-
acenter (within the same continent) for each country with at least
1Datacenter with lowest mean latency over time is estimated to be closest to a probe.
Figure 4: Distribution of all RTT values by all probes to
the nearest datacenter grouped by continent. The vertical
lines denote the strict latency thresholds desired by next-
generation applications (see ¬ß2.1 for details).
100 Speedchecker probes. The color of the country denotes the
latency group (corresponding to latency requirements in ¬ß2.1) its
median latency lies in. Since only China is able to achieve median
RTT below MTP (i.e.s 20 ms), we keep the first latency group as
0-30 ms, all the way until HRT (250 ms). The red diamonds show
the approximate locations of cloud regions targeted in this study.
We observe that geographical deployment locations of data-
centers have a significant impact on overall cloud performance
as countries with in-land datacenters exhibit the best median la-
tency. Among these, China achieves the lowest latency (within MTP
bounds), followed by central and northern Europe, North America
and South America, India, South Africa, Oceania, and some Asian
countries, e.g., Singapore, Indonesia, Thailand, etc. To gain fur-
ther insight, we plot the distribution of (all) latency measurements
recorded by the probe to the nearest datacenter grouped by conti-
nents in Figure 4. The results show a very clear trend. Continents
well-provisioned with datacenters, i.e., Europe, North America, and
Oceania, exhibit very similar latency distributions. Users in these
continents can achieve the 100 ms HPL threshold with high proba-
bility (as evident by the 90% of the samples from these continents).
Keep in mind that the plot includes latency due to the wireless last
mile, which is known to be the primary bottleneck in an end-user‚Äôs
connection [84] (we investigate this in ¬ß5). However, achieving MTP
in these regions is difficult in the current state of cloud deployments.
We investigate the cause of this gap later in this paper.
Countries in continents with sparse datacenter deployments,
e.g., South America, Africa, and the Middle East, show significant
latency overheads. Within this group, Asia and South America
have similar distributions and meet the HPL threshold for roughly
80% of the latency samples, albeit the long tails. We believe the
primary contributor to be the significantly lower ratio of available
datacenters to total landmass area within these regions. Probes
deployed close to a datacenter enjoy quite low latency (see Brazil in
Figure 3), which degrades with increasing distances. This phenom-
enon is more prevalent in South America (Brazil, Argentina, and
Chile), Africa (Morocco, Egypt, Algeria), and Asia (India, Pakistan,
and Afghanistan) ‚Äì resulting in significantly long tails in latency
distributions within these continents.
67
250msDatacenter0100200300400Latency[ms]0.000.250.500.751.00PercentileMTPPLHRTEUNAOCSAASAFCloudy with a Chance of Short RTTs
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
The worst performance-hit continent is Africa, where only  of the first hop
targeting the same datacenter endpoint. Figure 16 in Appendix A.3
shows the distribution of latency differences between measure-
ments conducted over these probes. Since we did not find enough
probe intersection from the same  in Africa, South
America, and Oceania, we exclude the results from these continents.
The result strengthens our arguments above as only a fraction of
latency samples in North America are faster in Speedchecker, while
for the rest RIPE Atlas achieves significantly lower latencies. While
our results highlight the influence of measurement platform on
derived conclusions, we are not criticizing the use of RIPE Atlas
for cloud measurements. Thanks to its largely wired and managed
deployment, RIPE Atlas probes are a good representation of enter-
prise customers of cloud providers. On the other hand, platforms
like Speedchecker provide an accurate reflection of end-user con-
nectivity in home and mobile environments.
Takeaway ‚Äî Measurements over RIPE Atlas generally deliver