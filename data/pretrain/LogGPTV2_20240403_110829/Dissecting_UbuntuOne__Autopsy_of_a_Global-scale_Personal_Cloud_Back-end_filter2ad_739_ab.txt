that exposes a message bus (DBus) interface to handle events
in U1 folders and make server notiﬁcations visible to the user
through OS desktop. This daemon also does the work of de-
ciding what to synchronize and in which direction to do so.
By default, one folder labeled ∼/Ubuntu One/ is automat-
ically created and conﬁgured for mirroring (root volume)
during the client installation. Changes to this folder (and
any others added) are watched using inotify. Synchroniza-
tion metadata about directories being mirrored is stored in
∼/.cache/ubuntuone. When remote content changes, the
client acts on the incoming unsolicited notiﬁcation (push)
sent by U1 service and starts the download. Push notiﬁca-
tions are possible since clients establish a TCP connection
with the metadata service that remains open while online.
In terms of data management, Dropbox desktop clients
deduplicate data at chunk level [2]. In contrast, U1 resorts
to ﬁle-based cross-user deduplication to reduce the waste of
storing repeated ﬁles [5]. Thus, to detect duplicated ﬁles,
U1 desktop clients provide to the server the SHA-1 hash of
a ﬁle prior to the content upload. Subsequently, the system
checks if the ﬁle to be uploaded already exists or not. In the
1) A user stores a file
Authentication 
U1 API Servers 
Service
(API/RPC processes)
CANONICAL
System Gateway
1API1
1RPC1
NAPIN
NRPCN
Data Storage 
(Amazon S3)
U1 Metadata Store 
(DB Cluster)
Notifications
(RabbitMQ)
Data transfer
Metadata
2) U1 servers handle the upload. API 
processes transfer the file to Amazon S3 
and RPC processes update metadata.
Figure 1: Architecture and workﬂow of U1 back-end.
aﬃrmative case, the new ﬁle is logically linked to the existing
content, and the client does not need to transfer data.
Finally, as observed in [5], the U1 client applies com-
pression to uploaded ﬁles to optimize transfers. However,
the desktop client does not perform techniques such as ﬁle
bundling3, delta updates and sync deferment in order to
simplify its design, which may lead to ineﬃciencies.
3.4 U1 Metadata Back-end
The entire U1 back-end is all inside a single datacenter and
its objective is to manage the metadata service. The back-
end architecture appears in Fig. 1 and consists of metadata
servers (API/RPC), metadata store and data store.
System gateway. The gateway to the back-end servers is
the load balancer. The load balancer (HAProxy, ssl, etc.)
is the visible endpoint for users and it is composed of two
racked servers.
Metadata store. U1 stores metadata in a PostgreSQL
database cluster composed of 20 large Dell racked servers,
conﬁgured in 10 shards (master-slave). Internally, the sys-
tem routes operations by user identiﬁer to the appropriate
shard. Thus, metadata of a user’s ﬁles and folders reside
always in the same shard. This data model eﬀectively ex-
ploits sharding, since normally there is no need to lock more
than one shard per operation (i.e.
lockless). Only opera-
tions related to shared ﬁles/folders may require to involve
more than one shard in the cluster.
API/RPC servers. Beyond the load balancer we ﬁnd
the API and RPC database processes that run on 6 sepa-
rate racked servers. API servers receive commands from the
user, perform authentication, and translate the commands
into RPC calls.
In turn, RPC database workers translate
these RPC calls into database queries and route queries to
the appropriate database shards. API/RPC processes are
more numerous than physical machines (normally 8 − 16
processes per physical machine), so that they can migrate
among machines for load balancing.
Internally, API and
RPC servers, the load balancer and the metadata store are
connected though a switched 1Gbit Ethernet network.
Data storage. Like other popular Personal Clouds, such
as Dropbox or SugarSync, U1 stores user ﬁles in a separate
cloud service. Concretely, U1 resorts to Amazon S3 (us-
east) to store user data. This solution enables a service
to rapidly scale out without a heavy investment in storage
hardware. In its latests months of operation, U1 had a ≈
20, 000$ monthly bill in storage resources, being the most
important Amazon S3 client in Europe.
3Li et al. [5] suggest that U1 may group small ﬁles together
for upload (i.e. bundling), since they observed high eﬃciency
uploading sets of small ﬁles. However, U1 does not bundle
small ﬁles together. Instead, clients establish a TCP connec-
tion with the server that remains open during the session,
avoiding the overhead of creating new connections.
158With this infrastructure, U1 provided service to 1.29 mil-
lion users traced in this measurement for one month.
3.4.1 Authentication Service
The authentication service of U1 is shared with other
Canonical services within the same datacenter and it is based
on OAuth [13]. The ﬁrst time a user interacts with U1,
the desktop client requires him to introduce his credentials
(email, password). The API server that handles the au-
thentication request contacts the authentication service to
generate a new token for this client. The created token is as-
sociated in the authentication service with a new user iden-
tiﬁer. The desktop client also stores this token locally in
order to avoid exposing user credentials in the future.
In the subsequent connections of that user, the authen-
tication procedure is easier. Basically, the desktop client
sends a connection request with the token to be authenti-
cated. The U1 API server responsible for that requests asks
the authentication service if the token does exist and has not
expired. In the aﬃrmative case, the authentication service
retrieves the associated user identiﬁer, and a new session is
established. During the session, the token of that client is
cached to avoid overloading the authentication service.
The authentication infrastructure consists of 1 database
server with hot failover and 2 application servers conﬁgured
with crossed stacks of Apache/Squid/HAProxy.
3.4.2 Notiﬁcations
Clients detect changes in their volumes by comparing their
local state with the server side on every connection (gener-
ation point). However, if two related clients are online and
their changes aﬀect each other (e.g. updates to shares, new
shares), API servers notify them directly (push). To this
end, API servers resort to the TCP connection that clients
establish with U1 in every session.
Internally, the system needs a way of notifying changes to
API servers that are relevant to simultaneously connected
clients. Concretely, U1 resorts to RabbitMQ (1 server) for
communicating events between API servers4, which are sub-
scribed in the queue system to send and receive new events
to be communicated to clients.
Next, we describe our measurement methodology to create
the dataset used in our analysis.
4. DATA COLLECTION
We present a study of the U1 service back-end. In contrast
to other Personal Cloud measurements [2, 7, 5], we did not
deploy vantage points to analyze the service externally. In-
stead, we inspected directly the U1 metadata servers to mea-
sure the system. This has been done in collaboration with
Canonical in the context of the FP7 CloudSpaces5 project.
Canonical anonymized sensitive information to build the
trace (user ids, ﬁle names, etc.).
The traces are taken at both API and RPC server stages.
In the former stage we collected important information about
the storage workload and user behavior, whereas the second
stage provided us with valuable information about the re-
quests’ life-cycle and the metadata store performance.
We built the trace capturing a series of service logﬁles.
Each logﬁle corresponds to the entire activity of a single
Trace duration
Trace size
Back-end servers traced
Unique user IDs
Unique ﬁles
User sessions
Transfer operations
Total upload traﬃc
Total download traﬃc
30 days (01/11 - 02/10)
758GB
6 servers (all)
1, 294, 794
137.63M
42.5M
194.3M
105TB
120TB
Table 3: Summary of the trace.
API/RPC process in a machine for a period of time. Each
logﬁle is within itself strictly sequential and timestamped.
Thus, causal ordering is ensured for operations done for the
same user. However, the timestamp between servers is not
dependable, even though machines are synchronized with
NTP (clock drift may be in the order of ms).
To gain better understanding on this, consider a line in
the trace with this logname: production-whitecurrant-
23-20140128. They will all be production, because we only
looked at production servers. After that preﬁx is the name of
the physical machine (whitecurrant), followed by the num-
ber of the server process (23) and the date. The mapping be-
tween services and servers is dynamic within the time frame
of analyzed logs, since they can migrate between servers to
balance load.
In any case, the identiﬁer of the process is
unique within a machine. After that is the date the logﬁle
was “cut” (there is one log ﬁle per server/service and day).
Database sharding is in the metadata store back-end, so
it is behind the point where traces were taken. This means
that in these traces any combination of server/process can
handle any user. To have a strictly sequential notion of the
activity of a user we should take into account the U1 session
and sort the trace by timestamp (one session/connection per
desktop client). A session starts in the least loaded machine
and lives in the same node until it ﬁnishes, making user
events strictly sequential. Thanks to this information we
can estimate system and user service times.
Approximately 1% of traces are not analyzed due to fail-
ures parsing of the logs.
4.1 Dataset
The trace is the result of merging all the logﬁles (758GB
of .csv text) of the U1 servers for 30 days (see Table 3).
The trace contains the API operations (request type stor-
age/storage_done) and their translation into RPC calls (re-
quest type rpc), as well as the session management of users
(request type session). This provides diﬀerent sources of
valuable information. For instance, we can analyze the stor-
age workload supported by a real-world cloud service (users,
ﬁles, operations). Since we captured ﬁle properties such as
ﬁle size and hash, we can study the storage system in high
detail (contents are not disclosed).
Dataset limitations. The dataset only includes events
originating from desktop clients. Other sources of user re-
quests (e.g., the web front-end, mobile clients) are handled
by diﬀerent software stacks that were not logged. Also, a
small number of apparently malfunctioning clients seems to
continuously upload ﬁles hundreds of times —these artifacts
have been removed for this analysis. Finally, we detected
that sharing among users is limited.
5. STORAGE WORKLOAD
4If connected clients are handled by the same API process,
their notiﬁcations are sent immediately, i.e. there is no need
for inter-process communication with RabbitMQ.
5
http://cloudspaces.eu
First, we quantify the storage workload supported by U1
for one month. Moreover, we pay special attention to the
behavior of ﬁles in the system, to infer potential improve-
ments. We also unveil attacks perpetrated to the U1 service.
159Download traffic
Upload traffic
300
250
200
150
100
r
u
o
h
/
s
e
t
y
B
G
50
01/20
01/21
01/22
01/23
01/24
01/25
01/26
01/27
a
t
a
d
d
e
r
r
e
f
s
n
a
r
t
f
o
n
o
i
t
c
a
r
F
1
0.8
0.6
0.4
0.2
0
Upload operations
Download operations
Uploaded data
Downloaded data
x 25MBytes) consumes 79.3% and 88.2% of upload and
download traﬃc, respectively. Conversely, 84.3% and 89.0%
of upload and download operations are related to small ﬁles
(< 0.5MBytes). As reported in other domains [14, 15, 16],
we conclude that in U1 the workload in terms of storage op-
erations is dominated by small ﬁles, whereas a small number
of large ﬁles generate most of the network traﬃc.
For uploads, we found that 10.05% of total upload opera-
tions are updates, that is, an upload of an existing ﬁle that
has distinct hash/size. However, in terms of traﬃc, ﬁle up-
dates represent 18.47% of the U1 upload traﬃc. This can
be partly explained by the lack of delta updates in the U1
client and the heavy ﬁle-editing usage that many users ex-
hibited (e.g., code developers). Particularly for media ﬁles,
U1 engineers found that applications that modify the meta-
data of ﬁles (e.g., tagging .mp3 songs) induced high upload
traﬃc since the U1 client uploads again ﬁles upon metadata
changes, as they are interpreted as regular updates.
To summarize, Personal Clouds tend to exhibit daily traf-
ﬁc patterns, and most of this traﬃc is caused by a small
number of large ﬁles. Moreover, desktop clients should eﬃ-
ciently handle ﬁle updates to minimize traﬃc overhead.
R/W ratio. The read/write (R/W) ratio represents the
relationship between the downloaded and uploaded data in
the system for a certain period of time. Here we examine