### Optimizing DP-Finder for Input-Dependent Noise

#### Current Limitations and Potential Improvements
DP-Finder currently assumes that noise terms do not depend on the input. This is achieved by:
1. Using the same noise for both inputs during sampling.
2. Changing only the inputs, but not the noise, during the search.

To handle algorithms where noise terms depend on the inputs, we can modify the sampling method. For example, we can use methods described in Sections 4.3 or 4.4. However, this will likely result in a performance decrease. When using fixed noise for values slightly different from \( x \) (especially during optimization), the noise comes from a slightly different distribution (sampled based on the original \( x \)). To compensate for this slight error, we can adapt DP-Finder to use importance sampling, which introduces larger weights for randomness that does not get sampled often enough.

#### Expectation-Preserving Program Transformations
We have also experimented with expectation-preserving program transformations for \( \text{check}_{F,\Phi} \). These transformations modify the program to:
1. Maintain the same expectation \( E[\text{check}_{F,\Phi}(x)] \).
2. Allow for more efficient sampling.

Our findings indicate that these transformations can improve the results of DP-Finder by:
1. Reducing the number of samples required to achieve a small confidence interval.
2. Enhancing the quality of the violations found by the search.

However, the expectation-preserving transformations we applied were manual and often required detailed knowledge about the program under investigation. A principled approach that can automatically detect and apply a wide range of expectation-preserving transformations could further enhance the performance of DP-Finder.

### Conclusion
We have introduced a new approach and system called DP-Finder, which identifies privacy violations in randomized algorithms. These violations provide lower bounds on the differential privacy enforced by these algorithms, allowing for the evaluation of the tightness of existing upper bounds or the identification of incorrect upper bounds.

DP-Finder leverages two key technical insights:
1. An estimate of the privacy violation through correlated sampling, using a carefully designed heuristic to minimize the sampling effort while maintaining a tight confidence interval.
2. Rewrite rules that transform the estimated (non-differentiable) violation into a differentiable function, which can then be optimized using numerical methods to find large privacy violations.

We evaluated DP-Finder on several randomized algorithms from the differential privacy literature. Our results show that DP-Finder can find large privacy violations, often close to known upper bounds, demonstrating its practical potential.

### References
[21] Z. Ji, Z. C. Lipton, and C. Elkan. 2014. Differential Privacy and Machine Learning: a Survey and Review. (2014). arXiv:1412.7584
[22] Noah Johnson, Joseph P. Near, and Dawn Song. 2018. Towards Practical Differential Privacy for SQL Queries. Proc. VLDB Endow. 11, 5 (2018), 526–539. https://doi.org/10.1145/3177732.3177733
[23] H. Kahn and A. W. Marshall. 1953. Methods of Reducing Sample Size in Monte Carlo Computations. Journal of the Operations Research Society of America 1, 5 (1953), 263–278. http://www.jstor.org/stable/166789
[24] Shiva Prasad Kasiviswanathan, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. 2013. Analyzing Graphs with Node Differential Privacy. In Theory of Cryptography Conference (TCC). 457–476. https://doi.org/10.1007/978-3-642-36594-2_26
[25] Assimakis Kattis and Aleksandar Nikolov. 2017. Lower Bounds for Differential Privacy from Gaussian Width. In Int. Symposium on Computational Geometry (SoCG). 45:1–45:16. https://doi.org/10.4230/LIPIcs.SoCG.2017.45
[26] Min Lyu, Dong Su, and Ninghui Li. 2017. Understanding the Sparse Vector Technique for Differential Privacy. Proc. VLDB Endow. 10, 6 (2017), 637–648. https://doi.org/10.14778/3055330.3055331
[27] Frank D. McSherry. 2009. Privacy Integrated Queries: An Extensible Platform for Privacy-preserving Data Analysis. In Proc. SIGMOD International Conference on Management of Data. 19–30. https://doi.org/10.1145/1559845.1559850
[28] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. 2007. Smooth Sensitivity and Sampling in Private Data Analysis. In Proc. Symposium on Theory of Computing (STOC). 75–84. https://doi.org/10.1145/1250790.1250803
[29] N. Papernot, M. Abadi, Ú. Erlingsson, I. Goodfellow, and K. Talwar. 2016. Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data. (2016). arXiv:stat.ML/1610.05755
[30] J. Priya Inala, S. Gao, S. Kong, and A. Solar-Lezama. 2018. REAS: Combining Numerical Optimization with SAT Solving. (2018). arXiv:cs.PL/1802.04408
[31] Davide Proserpio, Sharon Goldberg, and Frank McSherry. 2014. Calibrating Data to Sensitivity in Private Data Analysis. Proc. VLDB Endow. 7, 8 (2014), 637–648.
[32] Jason Reed and Benjamin C. Pierce. 2010. Distance Makes the Types Grow Stronger: A Calculus for Differential Privacy. In Proc. International Conference on Functional Programming (ICFP). 157–168. https://doi.org/10.1145/1863543.1863568
[33] Benjamin I. P. Rubinstein and Francesco Aldà. 2017. Pain-Free Random Differential Privacy with Sensitivity Sampling. In Proc. International Conference on Machine Learning, (ICML). 2950–2959. https://doi.org/10.1017/CBO9780511802256
[34] A. W. van der Vaart. 1998. Asymptotic Statistics.
[35] Daniel Winograd-Cort, Andreas Haeberlen, Aaron Roth, and Benjamin C. Pierce. 2017. A Framework for Adaptive Differential Privacy. Proc. ACM Program. Lang. 1, ICFP, Article 10 (2017), 29 pages. https://doi.org/10.1145/3110254

### Acknowledgments
This research was partially supported by an ERC Starting Grant 680358.

### References (Continued)
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-scale Machine Learning. In Proc. Operating Systems Design and Implementation (OSDI). 265–283. http://dl.acm.org/citation.cfm?id=3026877.3026899
[2] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In Proc. Conference on Computer and Communications Security (CCS). 308–318. https://doi.org/10.1145/2976749.2978318
[3] Aws Albarghouthi and Justin Hsu. 2017. Synthesizing Coupling Proofs of Differential Privacy. Proc. ACM Program. Lang. 2, POPL, Article 58 (2017), 30 pages. https://doi.org/10.1145/3158146
[4] Gilles Barthe, Thomas Espitau, Benjamin Grégoire, Justin Hsu, and Pierre-Yves Strub. 2017. Proving uniformity and independence by self-composition and coupling. In International Conferences on Logic for Programming, Artificial Intelligence and Reasoning (LPAR). 19. https://hal.sorbonne-universite.fr/hal-01541198
[5] Gilles Barthe, Marco Gaboardi, Emilio Jesús Gallego Arias, Justin Hsu, César Kunz, and Pierre-Yves Strub. 2014. Proving Differential Privacy in Hoare Logic. In Proc. Computer Security Foundations Symposium (CSF). 411–424. https://doi.org/10.1109/CSF.2014.36
[6] Gilles Barthe, Marco Gaboardi, Justin Hsu, and Benjamin Pierce. 2016. Programming Language Techniques for Differential Privacy. ACM SIGLOG News 3, 1 (2016), 34–53. https://doi.org/10.1145/2893582.2893591
[7] Andrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard Seefeld. 2017. Prochlo: Strong Privacy for Analytics in the Crowd. In Proc. Symposium on Operating Systems Principles (SOSP). 441–459. https://doi.org/10.1145/3132747.3132769
[8] Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. 2005. Practical Privacy: The SuLQ Framework. In Proc. Principles of Database Systems (PODS). 128–138. https://doi.org/10.1145/1065167.1065184
[9] Y. Chen and A. Machanavajjhala. 2015. On the Privacy Properties of Variants on the Sparse Vector Technique. (2015). arXiv:cs.DB/1508.07306
[10] Anindya De. 2012. Lower Bounds in Differential Privacy. In Proc. Theory of Cryptography Conference (TCC). 321–338. https://doi.org/10.1007/978-3-642-28914-9_18
[11] Irit Dinur and Kobbi Nissim. 2003. Revealing Information While Preserving Privacy. In Proc. Principles of Database Systems (PODS). 202–210. https://doi.org/10.1145/773153.773173
[12] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating Noise to Sensitivity in Private Data Analysis. In Proc. Theory of Cryptography Conference (TCC). 265–284. https://doi.org/10.1007/11681878_14
[13] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differential Privacy. Found. Trends Theor. Comput. Sci. 9 (2014), 211–407. https://doi.org/10.1561/0400000042
[14] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response. In Proc. Conference on Computer and Communications Security (CCS). 1054–1067. https://doi.org/10.1145/2660267.2660348
[15] Marco Gaboardi, Andreas Haeberlen, Justin Hsu, Arjun Narayan, and Benjamin C. Pierce. 2013. Linear Dependent Types for Differential Privacy. In Proc. Principles of Programming Languages (POPL). 357–370. https://doi.org/10.1145/2429069.2429113
[16] Timon Gehr, Sasa Misailovic, and Martin Vechev. 2016. PSI: Exact Symbolic Inference for Probabilistic Programs. Vol. 9779. 62–83. https://doi.org/10.1007/978-3-319-41528-4_4
[17] Anupam Gupta, Aaron Roth, and Jonathan Ullman. 2012. Iterative Constructions and Private Data Release. In Proc. Theory of Cryptography Conference (TCC). 339–356. https://doi.org/10.1007/978-3-642-28914-9_19
[18] Moritz Hardt and Kunal Talwar. 2010. On the Geometry of Differential Privacy. In Proc. Symposium on Theory of Computing (STOC). 705–714. https://doi.org/10.1145/1806689.1806786
[19] D. V. Hinkley. 1969. On the Ratio of Two Correlated Normal Random Variables. Biometrika 56, 3 (1969), 635–639. http://www.jstor.org/stable/2334671
[20] Wassily Hoeffding. 1963. Probability Inequalities for Sums of Bounded Random Variables. J. Amer. Statist. Assoc. 58, 301 (1963), 13–30. https://doi.org/10.1080/01621459.1963.10500830

### Appendix
#### Likelihood Induced by Gaussian Distribution
Let \( X \in \mathbb{R} \) be a constant, \( Y \in \mathbb{R} \) be a random variable, and \( \sigma^2 \in \mathbb{R} \) be a constant. We write \( Y = N(X, \sigma^2) \) to indicate that \( Y \) is sampled from a Gaussian distribution with mean \( X \) and variance \( \sigma^2 \). Observing \( Y \) induces a likelihood on \( X \), where \( X \) is distributed according to a Gaussian distribution. Formally, this means that we can switch \( X \) and \( Y \), resulting in \( X = N(Y, \sigma^2) \).

#### Sample Variance and Covariance
For a random variable \( S \) over \( \mathbb{R} \) and independent samples \( S_1, \ldots, S_n \) from \( S \), the sample variance \( \hat{\text{Var}}[S] \) estimates the variance of \( S \):
\[
\hat{\text{Var}}[S] := \frac{1}{n-1} \sum_{i=1}^n (S_i - \bar{S})^2 \quad \text{where} \quad \bar{S} = \frac{1}{n} \sum_{i=1}^n S_i
\]
For two jointly-distributed random variables \( S \) and \( S' \) over \( \mathbb{R} \) and independent samples \( (S_1, S'_1), \ldots, (S_n, S'_n) \), the sample covariance \( \hat{\text{Cov}}[S, S'] \) estimates the covariance of \( S \) and \( S' \):
\[
\hat{\text{Cov}}[S, S'] := \frac{1}{n-1} \sum_{i=1}^n (S_i - \bar{S})(S'_i - \bar{S}') \quad \text{where} \quad \bar{S} = \frac{1}{n} \sum_{i=1}^n S_i \quad \text{and} \quad \bar{S}' = \frac{1}{n} \sum_{i=1}^n S'_i
\]

#### Bivariate Normal Integral
Let \( Y_1 \) and \( Y_2 \) be variables drawn from a normal distribution with correlation coefficient \( \gamma \):
\[
\begin{pmatrix}
Y_1 \\
Y_2
\end{pmatrix} \sim N\left( \begin{pmatrix}
0 \\
0
\end{pmatrix}, \begin{pmatrix}
1 & \gamma \\
\gamma & 1
\end{pmatrix} \right)
\]
Then, \( L(h, k, \gamma) \) is the standard bivariate normal integral computing \( \Pr[Y_1 \leq h, Y_2 \leq k] \):
\[
L(h, k, \gamma) = \frac{1}{2\pi\sqrt{1-\gamma^2}} \int_{-\infty}^h \int_{-\infty}^k \exp\left( -\frac{x^2 - 2\gamma xy + y^2}{2(1-\gamma^2)} \right) \, dx \, dy
\]

#### Code Implementations
We provide implementations of \( \text{check}_{F,\Phi} \) for all algorithms \( F \) from Section 6. The implementations are in PSI and include placeholders for the input \( x \) (denoted by `[$A]`) and for the check \( \Phi \) (denoted by `[$O]`). Some algorithms have an additional meta-parameter, denoted by `$C`.

If \( F(x) \not\in \Phi \), `checkF,Φ` should return 0, but our implementation throws an assertion failure (which is easier to encode in PSI). If \( F(x) \in \Phi \), the implementation returns 1, as expected. Additionally, the implementations conflate the computation of the output \( F(x) \) with the check \( F(x) \in \Phi \), allowing for more efficient analysis with PSI.

[Code Repository](https://github.com/eth-sri/dp-finder/tree/initial-release/dpfinder/algorithms/psi_implementations)