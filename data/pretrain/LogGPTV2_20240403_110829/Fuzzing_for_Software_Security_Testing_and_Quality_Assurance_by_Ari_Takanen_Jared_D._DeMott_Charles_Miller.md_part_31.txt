protocol is defined with ABNF is easy and since the base model is generated from
network traffic it is quite easy to start doing modification to the model.
The protocol model for a fuzzer should be built so that the incoming message
model can be quite tolerant, as the generated fuzzer does not care about most of
the data the SUT is sending back as responses to the generated messages. In some
cases, for the testing to trigger a specific feature in the target software, the outgoing
test case must contain specific values. Defensics SDK has two ways to do this. The
model for outgoing messages could be augmented with programmed rules, and the
Figure 7.5 Example of a Java project file generated from an HTTP traffic capture.
5 https://tools.ietf.org/html/rfc5234.
6760 Book.indb 208 12/22/17 10:50 AM
7.2 Automatic Generation of a Model-Based Fuzzer 209
specific values could be assigned during test case generation using Java or Python.
Another option is to directly build the model to have certain default values. This
is a faster and simpler method bacuase there is no need to assign values in Java or
Python code. When the model is generated from the traffic capture, SDK Wizard
places detected values into the model directly to be used as default values when the
traffic capture is replayed by the fuzzer.
7.2.4 Adding rules to the Fuzzer
The main purpose of the rules is to keep the messages as valid as possible. For
example, rules can calculate element lengths, count how many items are in array
fields, and make sure that hashes and checksums are correct. In Defensics SDK, rules
are placed into the model using a Java or Python project. Defensics SDK has com-
prehensive list of preprogrammed rules that include lengths, counts, hashes, check-
sums, correlation between fields, copying values, padding, offset calculations, and
sequence numbers. It also provides a programming interface to create custom rules
for specific protocols. These are very rarely needed, but are mandatory if protocol
uses complex or proprietary cryptographic calculations or some other exotic logic.
7.2.5 Settings to Configure the Fuzzer
Testing the same target in the same setup from where the capture was taken does
not require any additional configurable settings. Adding capability to configure the
target IP address and the target port number can make test execution easier when
test setup changes because there is no need to recompile the whole project. What
configurable settings are needed depends highly on the protocol being fuzzed. For
example, an HTTP-based protocol could use an HTTP URI instead of an IP address
and port. Settings also could be used to configure the filename that is used for the
sample file for outgoing message, or settings could set possible usernames or pass-
words if the protocol has authentication functionality.
7.2.6 Fuzzer Input and Output
The generated fuzzer needs to have a method to communicate with outside world.
This could be something very simple like just reading the sample file and writing
new sample files out, or the output could be sent to network via sockets. In Defen-
sics SDK these input and output methods are called injectors. Defensics SDK sup-
ports UDP, TCP, file output, TLS, HTTP, WebSocket, and custom-made injectors.
7.2.7 Building and packing the Fuzzer
When the fuzzer is tested and verified to function correctly, it can be packed into a
finished test suite. The generated test suite works and looks like any other Defen-
sics test suite and can be installed into the same monitor platform. When executed,
the test cases are generated automatically according to the model and use the same
methodology as any other prebuilt Defensics Test Suite, as shown in Figure 7.6.
6760 Book.indb 209 12/22/17 10:50 AM
210 Advanced Fuzzing
Figure 7.6 Finished fuzzer made with Defensics SDK.
7.2.8 Conclusions
In this section, a sample network capture was used to create a base model for a
fuzzer with commercial Defensics SDK. When the target protocol is supported
by the Defensics modeling tool, this technique can leverage the information from
sample files to decrease the amount of work required to implement an effective
model based fuzzer.
7.3 Symbolic Execution with SAGE
The paper entitled “Automated Whitebox Fuzz Testing” by Godefroid, Levin, and
Molnar is an exceptional piece of research for next generation white-box fuzzers.
In particular, they created a tool called SAGE (Scalable, Automated, Guided Execu-
tion), an application for a white-box file fuzzing tool for x86 Windows applications.6
SAGE works in mutation-based (black-box) fuzzing by starting with an initial
input. This input is then symbolically executed by the program while information
about how it is used is stored. The information about why each particular branch
was taken (or not taken) is referred to as constraints. Then, each of these constraints
is negated one at a time and the entire system is solved, resulting in a new input
to the program that has a different execution path. This is then repeated for each
constraint in the program. In theory, this should give code coverage for the entire
attack surface. In practice, this isn’t the case, for reasons we’ll discuss in a bit. The
6 https://patricegodefroid.github.io/public_psfiles/ndss2008.pdf.
6760 Book.indb 210 12/22/17 10:50 AM
7.3 Symbolic Execution with SAGE 211
paper gives the following example of a function for which SAGE can quickly get
complete code coverage while a random fuzzer will struggle:
void top(char input[4]){
int cnt = 0;
if(input[0] == ‘b’) cnt++;
if(input[1]==’a’) cnt++;
if(input[2]==’d’) cnt++;
if(input[3]==’!’) cnt++;
if(cnt>=3) abort(); //error
}
This is clearly a contrived example, but it does illustrate a point. Using purely
random inputs, the probability of finding the error is approximately 2^(–30). Let
us walk through how SAGE would generate inputs for such a function. Suppose we
start with the input “root,” a valid but not very useful input. SAGE symbolically
executes this function and records at each branch point what was compared. This
results in constraints of the form
{input[0] != ‘b’, input[1] !=’a’, input[2]!=’d’, input[3]!=’!’}.
It then begins to systematically negate some of the constraints and solve them to
get new inputs. For example, it might negate the first branch constraint to generate
the following set of constraints:
{input[0] == ‘b’, input[1] !=’a’, input[2]!=’d’, input[3]!=’!’}.
This constraint would then be solved to supply an input something like “bzzz.” This
will execute down a different path than the original input “root,” resulting in the
variable having a different value upon exit from the function. Eventually, continu-
ing in this approach, the following set of constraints will be generated:
{input[0] == ‘b’, input[1] ==’a’, input[2]==’d’, input[3]==’!’}.
The solution of this set of constraints gives the input “bad!” This input finds the bug.
This technique does have its limitations, however. The most obvious is that there
are a very large number of paths in a program. This is the so-called path explosion
problem. It can be dealt with by generating inputs on a per-function basis and then
tying all the information together. Another major limitation is that, for a number of
reasons, the constraint solver may not be able to solve the constraints (in a reason-
able amount of time). Yet another problem arises because symbolic execution may
be imprecise due to interactions with system calls and pointer aliasing problems.
Thus, this approach loses one of the best features of black-box fuzzing; namely, you
are actually running the program so there are no false positives. Finally, the ability
of SAGE to generate good inputs relies heavily on the quality of the initial input,
much like mutation-based fuzzing.
Despite all these limitations, SAGE still works exceptionally well in many cases
and has a history of finding real vulnerabilities in real products. For example,
6760 Book.indb 211 12/22/17 10:50 AM
212 Advanced Fuzzing
SAGE was able to uncover the ANI format-animated cursor bug. This vulnerabil-
ity specifically arises when an input is used with at least two anih records, and the
first one is of the correct size. Microsoft fuzzed this application, but all of their
inputs only had one anih record. Therefore, they never found this particular bug.
However, given an input with only one anih record, SAGE generated an input with
multiple anih records and quickly discovered this bug. This code contained 341
branch constraints and the entire process took just under 8 hours. Other successes
of SAGE include finding serious vulnerabilities in decompression routines, media
players, Microsoft Office, and image parsers. Unfortunately, SAGE is not available
for home usage, but Microsoft has announced that their fuzzing-as-a-service project
Springfield uses SAGE as one of its key components.7
7.4 Code Coverage in Fuzzing
One of the major challenges of fuzzers is measuring their effectiveness. While
obtaining 100% code coverage doesn’t necessarily mean all bugs have been found,
it’s certainly true that no bugs will be found in code that hasn’t even been executed.
The best fuzz tests should cover (execute) all the code, and cover it with all the
attack heuristics, systematic or random data values, and as much other informa-
tion as possible.
That being the case, how can one know what percentage of the attack surface a
tool is covering? For example, if an arbitrary program contains 1,000 basic blocks
(series of assembly instructions until a branch instruction) and a network fuzzer hits
90 basic blocks, did it really only cover 90/1000, or 9% of the total code? Strictly
speaking, that’s true, but the fact is that most of that code cannot be covered via
the interface under test. So, how much of the attack surface code was covered? Sup-
pose that it’s possible to reach 180 BBs from the network and the coverage was then
90/180, or 50% of the attack surface. But how does one figure out the number of
BBs on the attack surface? A combination of all known valid sessions/files would
be a good, but difficult, first step.
If source code is available, there are a number of tools that can be used to
display code coverage information. However, suppose the source code is not avail-
able. Coverage can still be monitored. The two main techniques are preanalysis
and real-time analysis:
• Preanalysis requires locating the start of every function and basic block in the
application. This can be done with IDA Pro, for example, and the pida_dump.
py IDAPython script. Then using PaiMei, a breakpoint is set at each of these
locations. As each basic block is hit, it is recorded; that basic block or func-
tion has now been covered.
• Real-time analysis is done with hardware support via the Intel MSR regis-
ter, which can be used to record every address that EIP (the Intel instruction
pointer) has executed. This has the advantage of being faster (no time required
7 https://blogs.microsoft.com/next/2016/09/26/microsoft-previews-project-springfield-cloud-based-
bug-detector/.
6760 Book.indb 212 12/22/17 10:50 AM
7.4 Code Coverage in Fuzzing 213
to pass back and forth between the debugger and the debuggee) and doesn’t
rely on IDA Pro output. Here are a few things to consider when deciding
which approach to use:
1. Preanalysis could be difficult if the application is protected.
2. MSR doesn’t work in virtual machines such as VMWare.
3. In real-time analysis, all instructions are traced, so the coverage tool
would have to manually filter hits outside the scope of the target DLL(s)
(i.e., the many jumps to kernel and library DLLs).
4. Preanalysis is still required to determine how many total functions/basic
blocks there are if the percent of code coverage is desired.
So, code coverage can be obtained, regardless of whether source code is available,
now for examples of how it can be used.
Code coverage (or the lack of it) reveals which portions of the code have not
been tested. This code may also be code that is not executed during normal usage.
It is possible that the majority of bugs will be lurking in these dark corners of the
application. Therefore, fuzzing with code coverage could also reveal portions of the
application that require further static analysis. With such analysis may come a better
understanding of those portions of the application that can aid in better input con-
struction for the fuzzer. Iterating this approach can provide more thorough testing.
7.4.1 Code Coverage Guided Fuzzing: American Fuzzy Lop
American Fuzzy Lop (AFL) is a security-oriented fuzzer that employs compile-time
instrumentation and genetic algorithms to automatically discover clean, interesting
test cases that trigger new internal states in the targeted binary. This substantially
improves the functional coverage for the fuzzed code. The compact synthesized
corpora produced by the tool are also useful for seeding other, more labor- or
resource-intensive testing regimes down the road. In short, it is a mix of much of
what has been tried before, but is wrapped up in a package that is straightforward
to use and incredibly fast. It uses, file-like fuzzing, monitoring, distribution, and
brute-force exploration to generationally provide better inputs and find bugs.
A few of the key aspects in AFL are that AFL doesn’t do just blind mutations.
Instead it uses a variety of different fuzzing and optimization strategies to reach a
small and fast input corpus that still reaches the code coverage of all tested inputs.
AFL also uses a fork server that improves performance by running the target pro-
gram until main and forking instances to handle new test cases from that initialized
state. Full technical details of AFL can be found from afl-fuzz technical whitepaper.8
Let’s consider the example from SAGE research presented in Section 7.3:
void top(char input[4]){
int cnt = 0;
if(input[0] == ‘b’) cnt++;
if(input[1]==’a’) cnt++;
if(input[2]==’d’) cnt++;
8 http://lcamtuf.coredump.cx/afl/technical_details.txt.
6760 Book.indb 213 12/22/17 10:50 AM
214 Advanced Fuzzing
if(input[3]==’!’) cnt++;
if(cnt>=3) abort(); //error
}
And write it out as an example that can be compiled for AFL:
#include 
#include 
#include 
int foo(char a,char b,char c,char d){
int cnt=0;
printf(“a=%c, b=%c, c=%c, d=%c\n”, a, b, c, d);
if(a == ‘b’)
cnt++;
if(b == ‘a’)
cnt++;
if(c == ‘d’)
cnt++;
if(d == ‘!’)
cnt++;
if(cnt>=3)
abort(); //error
}
int main(int argc, char * argv[]){
FILE *f;
char a, b, c, d;
if( argc != 2)
{
printf(“bad args, need valid file name\n”);
exit(-1);
}
f = fopen(argv[1], “r”);
if( f ){
fread(&a, 1, 1, f);
fread(&b, 1, 1, f);
fread(&c, 1, 1, f);
fread(&d, 1, 1, f);
foo(a, b, c, d);
}
}
The program can be compiled with AFL instrumentation:
[attekett@Ubuntu ~]$ export AFL_DONT_OPTIMIZE=1
[attekett@Ubuntu ~]$ ./afl-gcc Example2.c -o Example2
Then AFL can be run:
[attekett@Ubuntu ~]$ ./afl-fuzz -i ./testcases/Example2/ -o
Example2_output/ ./Example2 @@
6760 Book.indb 214 12/22/17 10:50 AM
7.4 Code Coverage in Fuzzing 215
Figure 7.7 American Fuzzy Lop: Example2.
AFL writes unique crash reproducing files into the output folder, in a subdirec-
tory called “crashes”. Found crashes can be reproduced by executing the files from
that folder:
[attekett@Ubuntu ~]$ ./Example2
output/crashes/id\:000002\,sig\:06\,src\:000004\,op\:int8\,pos\:2
\,val\:+100
a=b, b=d, c=d, d=!
Aborted
Using AFL for a real-world example is straightforward. For example, let’s take
libxml2, a widely used XML parsing and toolkit library. On Ubuntu 16.04 Linux
you can get fuzzing libxml2’s xmllint utility with AFL after seven commands.
First, we install AFL and get the source code of libxml2-utils
[attekett@Ubuntu ~]$ apt-get install -y afl
[attekett@Ubuntu ~]$ apt-get source libxml2-utils
Next, we configure libxml2 build to use AFL compilers and compile the
xmllint utility
[attekett@Ubuntu ~]$ cd libxml2-2.9.3+dfsg1/
[attekett@Ubuntu ~]$ ./configure CC=afl-gcc CXX=afl-g++
[attekett@Ubuntu ~]$ make xmllint
Lastly, we create a sample file with content “” for AFL to start with
and run the afl-fuzz
6760 Book.indb 215 12/22/17 10:50 AM
216 Advanced Fuzzing
Figure 7.8 American Fuzzy Lop: lt-xmllint.
[attekett@Ubuntu ~]$ echo “” > in/sample
[attekett@Ubuntu ~]$ afl-fuzz -i ./in -o ./out -- ./.libs/
lt-xmllint -o /dev/null @@
AFL will continue fuzzing indefinitely collecting inputs that trigger new code
coverage in to ‘./out/queue/’, crash triggering inputs in to ‘./out/crashes/’ and inputs
causing hangs in to ‘./out/hangs/’.
AFL is widely used and practical fuzzer. It has been used to reveal a number of
vulnerabilities from different open-source projects.9 Originally designed for pro-
grams that can be compiled with gcc or clang, it has inspired people to implement