representation
Speed Multiarch
Binary-only
Cross-architecture
angr
S2E
QSYM
SymCC
SymQEMU
[25]
[6]
[28]
[20]
Python
C/C++
C++
C++
C/C++
VEX
TCG & LLVM
none
LLVM
TCG




















analysis if a certain computation is encountered frequently
in the same call-stack context (based on the intuition that
repeating the analysis over and over in the same context will
not lead to new insights). In order to support this optimization,
symbolic executors need to maintain a shadow call stack,
which requires keeping track of call and return instructions.
Building on top of QEMU, we faced the challenge that
TCG ops are a very low-level representation of the target
program. In particular, calls and returns are not represented
as individual instructions in TCG but instead translate to a
series of TCG ops.7 For example, a function call on x86 results
in TCG ops that push the return address onto the emulated
stack, adjust the guest’s stack pointer, and modify the guest’s
instruction pointer according to the called function. This makes
it nearly impossible to recognize calls and returns reliably and
in a platform-independent manner by just examining the TCG
ops. We chose to optimize for robustness: in the architecture-
speciﬁc QEMU code that translates machine code to TCG
ops, we notify the code generator whenever a call or a return
is encountered. (Hence the four architecture-speciﬁc lines of
code in the x86 translator mentioned earlier—one line each
for call immediate, call, return immediate, and return.) The
downside is that such notiﬁcations have to be inserted into the
translation code for each target architecture; however, the task
is easy and the amount of code very small, so we consider it
well worthwhile.
IV. EVALUATION
In order to evaluate SymQEMU, we performed three dif-
ferent sets of experiments:
1) We compared it to a number of state-of-the art fuzzers
2)
3)
with the help of Google FuzzBench.
Since FuzzBench does not include symbolic execu-
tion tools, we ran a comparison with popular binary-
only symbolic executors on a set of real-world pro-
grams.
In order to assess the difference in execution speed
between SymQEMU, QSYM and SymCC, we per-
formed a benchmark comparison between those con-
colic executors on ﬁxed inputs.
A. FuzzBench
Google announced FuzzBench in March 2020 as “a fully
automated, open source, free service for evaluating fuzzers”.8
It tests fuzzers in a controlled environment, comparing their
7There is a call instruction in TCG, but it serves a different purpose.
8https://security.googleblog.com/2020/03/fuzzbench-fuzzer-benchmarking-
as-service.html
8
TABLE II.
SUMMARY OF THE FUZZBENCH RESULTS FOR 21 TARGETS.
SYMQEMU RANKED FIRST ON 3 TARGETS, SECOND ON AVERAGE ACROSS
ALL TARGETS, AND OUTPERFORMED PURE AFL ON 14.
Target
Rank
Seed corpus
Dictionary
SymQEMU
Pure AFL
bloaty
curl
freetype2
harfbuzz
jsoncpp
lcms
libjpeg-turbo
libpcap
libpng
libxml2
mbedtls
openssl
openthread
php
proj4
re2
sqlite3
systemd
vorbis
woff2
zlib
7
5
2
2
4
1
1
6
1
4
6
3
2
3
5
4
5
3
4
3
6
4
1
4
4
11
7
5
10
7
2
4
6
6
5
4
6
2
2
5
5
8










































performance across a large number of targets taken from
Google OSS-Fuzz, a collection of fuzz targets for open-
source software.9 For each target, the service compares the
edge coverage obtained by the fuzzers. Integrating a new
analysis tool amounts to conﬁguring a Docker container to
set up the environment, build the target programs, and launch
the analysis. We added a combination of SymQEMU and
AFL to the set of analysis tools, and the FuzzBench team
graciously performed a run of the experiments. In total, they
ran SymQEMU and 12 fuzzer conﬁgurations on 21 targets for
24 hours, performing 15 trials per fuzzer and target (amounting
to roughly 10 CPU core years).
Figures 7, 8, 9 and 10 exemplify the outcome for two tar-
gets, and Table II summarizes the results; we show the ranking
for all targets in Appendix B and the full report online.10 On
average across all experiments, SymQEMU outperformed all
fuzzers but Honggfuzz, 5 of them with statistical signiﬁcance,
including the popular industrial-strength tool libfuzzer. On 3
out of 21 targets, SymQEMU achieved the highest coverage
among all tools, and it outperformed pure AFL on 14 targets;
9https://google.github.io/oss-fuzz/
10http://www.s3.eurecom.fr/tools/symbolic execution/symqemu.html
and compiler optimizations. Naturally, we can only evaluate
against SymCC on open-source targets.
For our comparison, we performed hybrid fuzzing of a
number of target programs and measured code coverage over
time. We used AFL’s notion of coverage for the same reason
as in the evaluation of SymCC [20]: it is what drives AFL’s
exploration process. Following the recommendations by Klees
et al. [13], we analyzed each target for 24 hours, and we
repeated each experiment 30 times. In order to check for
statistical signiﬁcance, we used a two-tailed Mann-Whitney U
test, again as recommended by Klees et al. Our targets were
the open-source programs OpenJPEG, libarchive and tcpdump
on the one hand, and the closed-source program rar on the
other hand. The reason for choosing these programs is that
(a) we have previously used the three open-source tools for
the evaluation of SymCC, so we know that both SymCC and
QSYM work on them, and (b) rar is an easy-to-obtain closed-
source program whose strict requirements on the format of
the input present interesting challenges to symbolic execution,
and whose license does not prohibit this type of analysis. For
OpenJPEG and rar, we provided a seed input of the expected
format; on libarchive and tcpdump, we started with an empty
corpus.
The various systems under comparison were set up as
follows:
•
•
•
SymQEMU, QSYM and SymCC. We ran those systems
together with AFL, using the same integration as in
QSYM and SymCC publications (i.e., exchanging test
cases via fuzzer queues in AFL’s distributed mode).
We executed one AFL primary instance, one AFL
secondary instance, and one SymQEMU, QSYM or
SymCC instance, each on one CPU core and with
2 GB of RAM. AFL was allowed to use the source
code when it was available; otherwise, we ran it in
QEMU mode.
S2E. For S2E, we created an analysis project per
target, making the test input fully symbolic when there
was one, and providing a symbolic ﬁle of all zeros
otherwise. We enabled the FunctionModels plugin and
extended the TestCaseGenerator plugin to produce a
new test case whenever a new execution state was
forked.11 We used the default searcher stack and ran
the experiments in the 64-bit Debian image provided
by the authors of S2E. Since S2E’s parallel mode was
not stable enough in our experiments, we accumulated
the results from three independent analyses (to match
the three CPU cores available to SymQEMU, QSYM
and SymCC); see Appendix A for details. In order to
assess code coverage, we evaluated the test cases with
AFL after the end of the analysis.
Pure AFL. We executed AFL in distributed mode,
running one primary and two secondary instances,
each on one CPU core with 2 GB of RAM. Like for
SymQEMU, QSYM and SymCC, we gave AFL access
to the target’s source code when it was available and
used QEMU mode otherwise.
11https://github.com/S2E/s2e/pull/20
9
Fig. 7.
Excerpt from the FuzzBench report: Ranking by median reached
coverage for the FuzzBench target lcms. SymQEMU outperforms all other
tools on this target.
it is worth mentioning, however, that pure AFL consequently
performed better than our hybrid fuzzer on 7 targets. The
speciﬁc potential contribution of symbolic execution generally
depends on several factors, including the availability of a seed
corpus or dictionary, and the nature of the analyzed code—for
instance, if the target makes heavy use of hash functions or
other irreversible operations, the utility of symbolic execution
is diminished.
Overall, we take the results as a conﬁrmation of
SymQEMU’s power, especially since we have not optimized
for any of the FuzzBench targets to avoid overﬁtting. Note also
that SymQEMU achieves this without using the targets’ source
code, and that the overwhelming majority of the targets are
accompanied by good seed corpora and/or dictionaries, where
symbolic execution typically does not contribute as much in
terms of raw coverage as it would if no seeds were available
(see Section IV-B). Finally, our rather crude integration simply
dedicates a ﬁxed share of CPU time to symbolic execution; we
believe that a more sophisticated coordination strategy between
fuzzer and symbolic executor (e.g., in the spirit of the recently
presented Pangolin [10]), could further improve the results (see
Section VI-C).
B. Comparison with other symbolic execution systems
SymQEMU’s primary goal is binary-only symbolic execu-
tion. In this section, we therefore compare it to state-of-the-
art tools in this space. In particular, we evaluate it against
S2E because, like SymQEMU, S2E is based on QEMU (see
Section II-C2), and against QSYM because it is the fastest
binary-only symbolic executor that we are aware of (see
Section II-C3). We omitted angr (Section II-C1) from the
comparison because preliminary experiments showed that its
execution speed is signiﬁcantly lower than that of the other
tools; angr prioritizes versatility and ease of interactive use
over raw speed [19]. Finally, we added raw AFL as a baseline,
and we compared against SymCC (see Section II-D) because
it introduced the concept of compilation-based symbolic ex-
ecution. Note, however, that SymCC has an advantage over
the other tools because it uses the source code of the program
under test, e.g., it can beneﬁt from high-level code structures
Fig. 8.
outperforms all other tools on this target.
Excerpt from the FuzzBench report: Mean coverage growth over time (and 95 % conﬁdence intervals) for the FuzzBench target lcms. SymQEMU
possible; for example, it does not trigger another memory-to-
register optimization pass after inserting its instrumentation
(resulting in unnecessary memory operations in the target
program), nor does it use link-time optimization to inline calls
to the support library. We believe that this is the main reason
why a binary-only symbolic executor like SymQEMU can keep
up with a source-based tool like SymCC. In summary, the
results conﬁrm that SymQEMU is more efﬁcient than the other
binary-only symbolic execution systems in our comparison.
In our analysis of libarchive, SymQEMU found an input
that leads to a use-after-free error on the heap. The bug can be
triggered, for example, by making a user list the contents of
a manipulated archive with the bsdtar utility, and we consider
it likely to be exploitable. We have reported the issue to the
developers of libarchive; at the time of writing, we have not
received a reply.
Figure 12 displays the results for the closed-source rar
program. SymQEMU, QSYM and AFL all converge towards
the same level of coverage, but SymQEMU reaches saturation
as fast as the less architecturally ﬂexible QSYM and faster
than AFL. Note further that SymQEMU and QSYM quickly
discover paths that pure AFL (i.e., without symbolic execution)
needs more time to ﬁnd. S2E cannot analyze as much code as
the other tools but arguably covers more of the data space on
the discovered paths.12 This experiment shows that SymQEMU
can work with closed-source targets, like other binary-only
symbolic executors, but with the additional advantage of easily
supporting a large number of target architectures.
It is interesting to note that symbolic execution generally
contributes the most
in terms of code coverage when no
seed inputs are available, as demonstrated by our analysis of
libarchive and tcpdump. On OpenJPEG and rar, in contrast,
the seed ﬁles give AFL sufﬁcient information to also achieve
a good coverage level in relatively little time.
Finally, Figure 13 shows the execution times of the sym-
bolic execution engines in our experiments, providing evidence
that SymQEMU is consistently faster than QSYM and at least
on par with the source-based SymCC. We omitted S2E from
12https://ccadar.blogspot.com/2020/07/measuring-coverage-achieved-by-
symbolic.html
Fig. 9.
Excerpt from the FuzzBench report: Ranking by median reached
coverage for the FuzzBench target woff2. SymQEMU reaches 3rd rank on
this target.
The experiments were conducted on an Intel Xeon Plat-
inum 8260 CPU. We spent a total of roughly 5 CPU core years
(4 target programs, 5 systems under comparison, 3 cores per