### Experimental Setup and Evaluation

#### Genuine Transactions
For each round, 52 challenge strings were evaluated using either 3 or 11 repetitions. Each repetition used unique letter instances to avoid reuse. Errors were counted and averaged over the repetitions to compute the average false non-match rate (FNMR). In the High-quality Data and Reduced Data experiments, 156 (or 52×3) genuine transactions were conducted per round, while the All Data experiment involved 572 (or 52×11) transactions.

#### Imposter Transactions
In each round, 52 subjects each masqueraded as 51 other users. For a fixed victim, letters from 51 different imposters were used to satisfy the victim's challenge string. Multiple repetitions (3 or 11) did not reuse letter instances. Errors were counted and averaged to produce the average false match rate (FMR). The High-quality Data and Reduced Data experiments included 7,956 (or 52×51×3) imposter transactions per round, while the All Data experiment had 29,172 (or 52×51×11) transactions.

In biometric applications that provide positive or negative identification [24], special evaluation procedures are necessary for imposter transactions when templates are dependent [13]. Templates are considered dependent if the enrollment of a new user affects existing templates; otherwise, they are independent. Our SVM method uses pairwise binary classifiers, making the templates dependent. However, since our primary task is insider detection and assumes no outsiders, the use of dependent templates without special evaluation provisions is reasonable.

### Results

Table 6 presents the results of the challenge-string evaluation for High-quality Data, which most closely reflects actual user handwriting. The number of units, \( u \), was varied from 1 to 11, as the subject with the fewest units had only 11. Each unit contains one or two letters, so the number of letters used in a round varies from \( u \) to \( 2u \), depending on the subject. Both FMR and FNMR decreased to 0% as the number of units increased. With 2-4 letters, FMR was 0.04% and FNMR was 0.64%. When 8-16 letters were used, both FMR and FNMR were 0%.

The results for the Reduced Data and All Data experiments were similar to those of the High-quality Data. Neither increasing data quality (while holding quantity constant) nor increasing data quantity (while holding quality constant) provided a clear benefit. Due to the long running time of the evaluations, we could not repeat each experiment multiple times with different random samples, thus preventing estimates of variance and tests of statistical significance. To probe the stability of the results, we replicated the High-quality Data experiment once, and the results closely resembled those of the original experiment, as well as those of the Reduced Data and All Data experiments. The All Data experiment, which used the most transaction repetitions, achieved consistently perfect results with only 5-10 letters.

### Discussion

Although error rates in Table 6 do not decrease monotonically with an increase in the number of units, deviations from the trend are minor and short-lived. Possible explanations include limited data quantities or rare atypical instances in SVM-test and/or Evaluate-test, and sub-optimal challenge strings (due to the greedy and heuristic algorithm for creating letter lists). Generally, adding units to a challenge string should not be harmful. Since results under different conditions of data quality and quantity did not differ greatly, it appears that data-capture anomalies were not catastrophic, and similar performance might be achieved with even less data.

### Summary

This work aimed to determine whether personalized challenge strings could discriminate enrolled users writing on PDAs and to find the optimal length of these challenge strings. Our results suggest that using password-length challenge strings (at least 5-10 characters, depending on the user) results in very low equal-error rates, approaching 0%. Employing longer strings consistently yields perfect results, suggesting that a few more seconds of user time (each letter stroke takes less than 0.5 seconds to write, on average) could significantly enhance security. These findings represent a first step towards exploring the potential of structured writing on PDAs for biometric access control.

### Future Work

Several avenues for further development include:
- Using the same data corpus to try new features, different splits of training and testing data, other classifiers, new ways to combine classifier outputs, and alternative algorithms to create challenge strings.
- Generating variant challenge strings non-deterministically to discourage replay attacks.
- Studying the effects of writing experience on accuracy.
- Attempting other biometric tasks, such as positive or negative identification, possibly using independent templates.
- Conducting multiple replications of experiments to estimate the variance of the results.
- Collecting new data and ensuring its quality to closely represent user handwriting.
- Introducing template ageing to make the task more challenging.
- Recruiting larger numbers of subjects to explore the relationship between the size of the user pool and system accuracy.
- Studying and testing hypotheses about which kinds of structured letters best discriminate writers.

### Acknowledgements

The authors thank Patricia Loring and anonymous reviewers for their helpful comments. Marcus Louie implemented the stimulus generator, and Sebastian Scherer implemented the data-capture program on the Palm m105. This work was supported by National Science Foundation grant number CNS-0430474 and the Pennsylvania Infrastructure Technology Alliance.

### References

[References listed here as in the original text]

---

This version of the text is more organized, concise, and professional, with clear headings and a logical flow of information.