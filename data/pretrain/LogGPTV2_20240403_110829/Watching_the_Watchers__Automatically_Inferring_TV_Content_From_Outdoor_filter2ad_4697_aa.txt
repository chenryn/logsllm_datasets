title:Watching the Watchers: Automatically Inferring TV Content From Outdoor
Light Effusions
author:Yi Xu and
Jan-Michael Frahm and
Fabian Monrose
Watching the Watchers: Automatically Inferring TV
Content From Outdoor Light Effusions
Yi Xu, Jan-Michael Frahm and Fabian Monrose
Department of Computer Science, University of North Carolina at Chapel Hill
Chapel Hill, North Carolina, USA
{yix,jmf,fabian}@cs.unc.edu
ABSTRACT
The ﬂickering lights of content playing on TV screens in our living
rooms are an all too familiar sight at night — and one that many
of us have paid little attention to with regards to the amount of in-
formation these diffusions may leak to an inquisitive outsider. In
this paper, we introduce an attack that exploits the emanations of
changes in light (e.g., as seen through the windows and recorded
over 70 meters away) to reveal the programs we watch. Our empir-
ical results show that the attack is surprisingly robust to a variety of
noise signals that occur in real-world situations, and moreover, can
successfully identify the content being watched among a reference
library of tens of thousands of videos within several seconds. The
robustness and efﬁciency of the attack can be attributed to the use
of novel feature sets and an elegant online algorithm for performing
index-based matches.
Categories and Subject Descriptors: K.4.1 [Computers and So-
ciety]: Privacy
General Terms: Human Factors, Security
Keywords: Visual eavesdropping; Compromising emanation
1.
INTRODUCTION
To most of us, it would come as no surprise that much of our pop-
ulation is addicted to watching television, due in part to the wide
variety of entertainment (e.g., reality TV, game shows, movies,
premium channels, political commentary, 24hr news, etc.) that is
offered in today’s competitive market place — be that online or
via broadcast TV. Indeed, so-called catch-up TV and Internet con-
nectivity now liberate viewers from restrictive schedules, making
watching shows part of a wider and richer experience in homes.
Admittedly, although familiar TV sets of the old days are not as
popular as they once were, TV is here to stay and its role in deliv-
ering compelling viewing experiences will continue for decades.
The markedly richer content offered today has helped sustain liv-
ing room screens as a dominant communication medium — both
collectively (e.g., for watching a big game or season ﬁnale) and in-
dividually (e.g., for accessing speciﬁc content on demand). In fact,
even though consumer viewing habits have undergone change in
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660358.
recent years (e.g., phone, tablet and computer viewing habits have
steadily increased), nearly every U.S. home still owns at least one
TV and 67% of Americans regularly watch television while hav-
ing dinner [6]. The ﬂickering lights of the scenes that play out on
these TVs are easy to see when one walks through the street at
nights. Yet, many of us may not have given a second thought to the
amount of information these ﬂickering patterns (caused by changes
in brightness) might reveal about the programs we watch.
Our ﬁndings, however, suggest that these compromising emis-
sions of changes of brightness provide ample opportunity to con-
ﬁrm what speciﬁc content is being watched on a remote TV screen,
even from great distances outside the home. The key intuition be-
hind why this threat to privacy is possible lies in the fact that much
of the content we watch induces ﬂickering patterns that uniquely
identify a particular broadcast once a suitable amount of light emis-
sions (i.e., on the order of a few minutes) has been recorded by
the adversary. This surprisingly effective attack has signiﬁcant pri-
vacy implications given that the video and TV programs that people
watch can reveal a lot of information about them, including their re-
ligious beliefs, political view points or other private interests. For
that reason, subscribers’ viewing habits are considered sensitive un-
der the U.S. Video Protection Privacy Act of 1998, which states
that an individual’s video viewing records must be kept conﬁden-
tial. Recently, a popular electronics ﬁrm came under investigation
when it was revealed that its Smart TV was surreptitiously sending
information on viewing habits back to the parent company in an
effort to “deliver more relevant advertisements” 1.
While the observations we leverage in this paper have been part
of folklore, to the best of our knowledge, we present the ﬁrst auto-
mated, end-to-end, approach for realizing the attack. Undoubtedly,
the academic community has long acknowledged that video view-
ing records are vulnerable to different attacks (e.g., due to electro-
magnetic or power line behavior [4, 7, 9]), but these attacks have
not received widespread attention because they require access to
smart power meters and other specialized equipment in order to
capture the required signal. Moreover, because these attacks rely
on speciﬁc TV/computer screen electronic properties they remain
difﬁcult to pull off in practice.
In this paper, we push the boundaries of these attacks by ex-
ploiting compromising emissions which are far easier to capture in
practice. In fact, we do not rely on the adversary’s ability to cap-
ture an image of the screen, or its reﬂection on a nearby surface
(e.g., [1, 17]). Instead, our attack works by analyzing the changes
in brightness in a room where the content is being watched, and
matching the captured signal in real-time with reference signals
1See J. Brookman, Eroding Trust: How New Smart TV
Lacks Privacy by Design and Transparency at http://www.
privacyassociation.org/, Nov. 2013.
418stored in a large database. The attack can be successfully carried
out with inexpensive consumer devices (e.g., web cameras, digital
SLRs) and works as long as illumination changes caused by the TV
screen are perceptible to the camera’s sensor.
To ensure that the attack is resilient to noise (e.g., from a pass-
ing vehicle, the turning on/off of a light switch, or from human
movement), our approach focuses squarely on signiﬁcant changes
in the captured sequence, instead of directly leveraging all of the
captured signal. Said another way, we exploit temporal brightness
information that is not adversely affected by device-speciﬁc or en-
vironmental conditions. These environmental conditions (e.g., re-
ﬂections off a wall) might result in a weakened and distorted overal
signal, but the temporal information of signiﬁcant intensity changes
will remain largely intact.
A key contribution in this paper lies in the techniques we use
to take advantage of temporal information to ﬁnd matches among
reference and captures signals, even in the face of signiﬁcant noise
and signal distortions. To do so, we extend traditional correlation
measures to utilize temporal information when computing similar-
ity scores between sequences. The resulting strategy signiﬁcantly
outperforms traditional correlation measures (e.g., [7]), for which
we present an on-line approximation method. Our empirical anal-
ysis covering 54,000 videos shows that we can perform this conﬁr-
mation attack with surprising speed and accuracy.
2. RELATED WORK
Techniques for undermining user’s privacy via TV program re-
trieval has long been studied. The most germane of these works is
that of Enev et al. [4] and Greveler et al. [7] wherein power usage
and power line electromagnetic interference were investigated as
side-channels. Unlike the approach we take, these works encode
the TV signal in ways that largely depend on the model of the TV
and the structure of the power system. Therefore, to successfully
carry out the attack, an adversary must not only have specialized
equipment and access to smart meters, but must also have a priori
knowledge of the victim’s TV model — all of which weaken the
practicality of the attack. Moreover, other electronic devices (e.g.,
computers) within the vicinity of the TV can interfere with the cap-
tured signal, compounding the decoding challenges even further.
Other side-channels include the use of so-called compromising
reﬂections, which was ﬁrst introduced by Backes et al. [1]. Shiny
object reﬂections (e.g., from a nearby teapot or off an eyeball) were
used to recover static information displayed on the target screen.
More recently, compromising reﬂections were also exploited by
Raguram et al. [13] and Xu et al. [17] to reconstruct information
being typed on virtual keyboards. In a similar manner, Torralba and
Freeman [14] make use of reﬂections to reveal “accidental” scenes
from within a still image or video sequence. The advantage for
these approaches comes from the uniformity and easy-access of vi-
sual signals; while TV screen and computer screens come with dif-
ferent model using different technologies — resulting in extremely
different electromagnetic behavior — they all share similar visual
output. Due to market demand, the emanation of the visual signal
has to cover a certain area and maintain a certain brightness level
to ensure clarity of picture, which also makes them susceptible to
compromising reﬂections. That said, these attacks require a view
of the screen, either directly or via reﬂections.
Also related within the domain of computer vision is the pro-
cess of image and video retrieval. Interested readers are referred
to Zhang and Rui [18], which presents an excellent review of im-
age retrieval techniques used to search through billions of images.
Likewise, Liu et al. [11] presents a survey of near-duplicated video
retrieval techniques that also focus on similarity of semantic con-
tent of the video sequences. In short, features are extracted to reveal
detail information in the image and semantic labels are used to pro-
vide a high level understanding. Unfortunately, we have no such
luxury in our application since we may have no visual access either
directly or indirectly to the screen, and must therefore ﬁnd ways to
work with much more limited information.
Lastly, our application domain shares similarities to genome se-
quence matching and database searching. In particular, consider-
ing only the average image intensity signal, the task at hand can be
viewed as a sequence matching problem. For instance, in genome
sequence matching, Langmead et al. [10] present a fast DNA se-
quence matching scheme that exploits time and space trade-offs. In
database searching, Faloutsos et al. [5] and Moon et al. [12] present
methods that perform fast matching from an input subsequence to
those in a database. Unfortunately, these techniques suffer from
several limitations that make them ill-suited for our setting. For ex-
ample, in DNA sequence matching, many parts of a sequence may
be missing and so to ﬁnd the best matches, dynamic optimization
methods are usually deployed to maximize the length of the best
match. These algorithms typically have O(mn) complexity, where
m is the length of the query sequence and n is the length of the
reference sequence. In our application, however, the only uncer-
tainty is the starting point of the query sequence and so much more
effective strategies (i.e., O(nlog(m)) or faster) can be applied.
In database searching, the problem is more similar to ours, but
the state-of-the-art solutions utilize Fourier transformation and fo-
cus on low frequencies. In our application, the sudden intensity
changes contain most of the information we utilize, but live in the
high end of the frequency spectrum. As such, these approaches
can not be directly applied. However, by combining many of the
strengths of prior works together with our own enhancements, we
provide a solution that boasts high accuracy and speed.
3. OVERVIEW
The key insight we leverage is that the observable emanations of
a display (e.g., a TV or monitor) during presentation of the viewing
content induces a distinctive ﬂicker pattern that can be exploited
by an adversary. This pattern is observable in a wide range of sce-
narios, including from videos capturing the window of the room
housing the display, videos from cameras pointed at a wall in the
room but not at the TV directly, videos observing the watcher’s face
(for example, via a Kinect or similar front-mounted camera), and
of course, from video capturing the TV directly. To facilitate our
attack, we convert the observed pattern of intensity changes into a
suitable feature vector that is amenable to rapid matching of other
stored feature vectors within the adversary’s large corpus of pro-
cessed videos.
In this paper, we compute the average pixel brightness of each
frame in the video, resulting in a mean brightness signal for the
video. To capture the sharp changes in brightness, we then use the
gradient of the signal as the descriptor for the video. The overall
process is illustrated in Figure 1. Similar to the captured video,
every video in the adversary’s collection is represented by a fea-
ture based on the gradient of the brightness signal. Note that while
the mean brightness signal of the reference video and the captured
videos signal may vary, their gradient-based features share more
characteristics in common, and it is those commonalities that are
used to identify the content being watched.
4. BACKGROUND
The ability to conﬁrm which video is being watched based off
compromising diffusions of changes in light hinges on several fac-
419Figure 1: The high-level workﬂow of our approach. Features are extracted from the captured video and then compared with features from reference videos in
a database. The reference video with the most similar feature is output as the most probable candidate.
tors including (i) the quality of the captured information (i.e., the
signal-to-noise ratio), (ii) the entropy of the observed information
(i.e., the amount of variation in the captured signal) (iii) the length
of the captured signal (e.g., short clips have more ambiguity), and
(iv) the amount of information required for successfully matching
the unknown and reference signals, which is related to the size of
the adversary’s reference library and the distinctiveness of its con-
tents. We discuss each in turn.
Noise Interference
For an arbitrary recording, our goal is to infer a signal, S, based on
effusions of light from the display. In practice, this means that we
also inadvertently capture an additive noise signal, N, which may
be composed of a variety of other signals (e.g., sensor noise, photon
noise). Consequently, the recording we capture is the composition
of the signal S and noise N. Intuitively, the more signiﬁcant the
noise, the harder it will be to distinguish between the noise and
the signal. This correlation is measured by the signal-to-noise ratio
(SNR), which is the ratio of the signal variance σ 2
S and the noise
variance σ 2
N.
In general, the higher the SNR the less the noise inﬂuences the
resulting signal, which leads to more robust signal analysis. In the
case of capturing reﬂections of emanations, the SNR depends on
a multitude of factors. More speciﬁcally, the amount of light em-
anated from the screen at any frame depends on the intensity of the
video frame that is displayed on the screen, Ire f , the current bright-
ness level of the screen (measured by unit area emanation power
P0), and the size of the screen Sscreen. However, only a small frac-
tion of this light might be captured by the camera, the amount of
which depends on the distance the light travels from the screen to
the reﬂecting object, the size and reﬂectance of the reﬂecting ob-
ject, the aperture of the camera and the distance from the reﬂect-
ing object to the camera. The captured signal also depends on the
sensitivity αcam of the imaging sensor of the recording device. In
summary, assuming αcap is the percentage of emanation captured
by the camera, the recorded signal can be modeled as:
Icap = Ire f P0Sscreenαcapαcam
(1)
It is important to note that αcap and αcam are not constant in prac-
tice because of the different reﬂectance properties for colors and