measurement tool scamper [20] and perform latency measurement
using ICMP echo requests to the Google IPv4 Anycast DNS server
8.8.8.8. Scamper was configured to store its RTT samples in an
external file for later analysis. SoMeta was configured to collect
RTT samples from the first three network hops using either a hop-
limited probe or ICMP echo request probes, and to also measure RTT
to 8.8.8.8. Simultaneous to running SoMeta, we introduced artificial
system and network load in four different experiment types: 100%
CPU load on all cores, memory system load by cycling reads and
writes over a large array, I/O load by repeatedly writing to a file
on the same filesystem as the output files generated by scamper
and SoMeta, and upload and download transfers of a range of file
sizes (100KB, 1MB, and 10MB). CPU, memory, and I/O artificial load
were introduced in an on-off cycle of 60 seconds per cycle, starting
with 60 seconds of no artificial load, followed by 60 seconds of load.
The file transfers to introduce artificial network load were initiated
every 10 seconds from a separate host, but using the same network
path from the first router up through several network hops.
For these experiments, we used the same testbed as in our over-
head experiments described above, and we also deployed a Pi3 in
a home network connected to the Internet via a large cable mo-
dem provider. The Pi3 deployed in the home network was either
attached to the home router using a wired 100 Mb/s connection
or via 802.11n WiFi. Figure 4 shows results for the home network-
deployed Pi3 connected via WiFi to the router, and with artificial
CPU load. The plots show empirical CDFs of RTTs between the
Pi3 and the first hop (home router) (left), between the Pi3 and the
second hop (center), and RTTs to the scamper target IP address
8.8.8.8. For each plot, we show CDFs for all RTT measurements,
as well as separate CDFs for the no artificial load time periods
(off) and when artificial CPU load is introduced (on). For each plot,
we observe that the artificial CPU load has a clear skewing effect
on the latency measurements. An experimenter using SoMeta to
detect subpar local conditions could (1) evaluate the CPU-related
metadata to discover that there were time periods during which
there was zero (or very little) idle CPU, and/or (2) compare (either
visually, or in a more rigorous quantitative or statistical way) the
latency data generated from the experiment with data previously
collected during known “good” time periods. We note that the “off”
curve from the left-hand plot of Figure 4 is virtually identical to a
curve generated from a separate experiment in which no artificial
load is introduced (not shown).
Lastly, in Figure 5 we show results for the artificial network load
experiment with the Pi3 again in the home network environment.
Results are shown for file transfer sizes of 100KB, with the Pi3 con-
nected to the home router via 1 Gb/s wired Ethernet. File transfers
were initiated from a separate host connected via WiFi. The net-
work path of the file transfers shared the same network path as the
probes toward 8.8.8.8 for the first 6 hops. In Figure 5, we show CDFs
from measurements in which no artificial load was introduced, and
CDFs from the artificial network load experiment; note the log scale
on the x axis. First, we observe that the first network hop is largely
undisturbed. Recall that the access technologies for the Pi3 (wired
Ethernet) and the host from which file transfers are initiated (WiFi)
differ, thus we do not observe any obvious interference at this hop.
For subsequent hops, however, we observe significant skewing even
with the transfer of a relatively small file. An experimenter might
become aware of the local network disturbance in a similar way as
described above, by comparing (as shown in the figure) newly col-
lected data with measurements collected during known quiescent
time periods. We note that in a similar experiment in which the
0255075100125150175200Intended packet rate (pps)0255075100125150175200Achieved packet rate (pps)Pi 1 model B ppsPi 3 model B pps020406080100CPU idle (%)Pi 1 model B CPU idlePi 3 model B CPU idleIMC ’17, November 1–3, 2017, London, United Kingdom
Joel Sommers, Ramakrishnan Durairajan, and Paul Barford
Figure 4: Artificial CPU load experiment.
certain host system measures (e.g. CPU utilization) to determine
periods during which system load may contribute to poor network
measurement, we believe that one way to gain evidence of impaired
measurement quality is to collect additional latency measures via
SoMeta’s RTT monitor and compare them with previously collected
data. In other words, the previously collected measurements could
be used as a reference against which to compare, e.g., qualitatively,
or using a statistical test such as the Kolmogorov-Smirnov two-
sample test. Any shift away from the baseline reference could be
detected and possibly even corrected in the network measurements,
and at the very least, a measurement quality label could be applied
to different time periods of a trace based on analysis of additional
measurements collected through SoMeta.
Secondly, while SoMeta is, at present, decoupled from any partic-
ular measurement tool, we are considering an API through which
measurement tools (or other data sources) could explicitly provide
metadata to assist with creating a comprehensive, structured record
of an experiment for documentation and scientific replication pur-
poses. In addition to providing an avenue for metadata storage,
status/health polling (i.e., “heartbeats”) could be done through such
an API, and meta-API information (i.e., response time) could be
used for continuous assessment of system performance.
There are yet other extensions we are considering to SoMeta that
we believe will make it appealing for a wide variety of usage sce-
narios, including automatic identification and dynamic monitoring
of file systems/disks and network interfaces that are being used by
a measurement tool, performance improvements for adapting to
extremely constrained environments and to shared measurement
platforms where there may be multiple experiments executing si-
multaneously. Whether or not SoMeta sees wide use within the
network measurement community, we hope that our study pro-
vokes a renewed discussion on the role of metadata in assessing
measurement quality and experiment reproduction.
ACKNOWLEDGMENTS
We thank the anonymous reviewers and our shepherd Rocky K.
C. Chang for their feedback. This material is based upon work
supported by NSF grant CNS-1054985, by DHS grant BAA 11-01
and AFRL grant FA8750-12-2-0328. Any opinions, findings, and
conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of the
NSF, DHS or AFRL.
Figure 5: Artificial network load experiment.
Pi3 is connected to the home router via WiFi (not shown), first-hop
latencies are observed to be skewed since the Pi3 and the host from
which data transfers originate share the WiFi access.
5 SUMMARY AND FUTURE
DIRECTIONS
We view the current monitoring capabilities and kinds of data that
SoMeta can collect as a starting point toward our goal to simplify
collection and analysis of certain types of metadata for Internet
measurement experiments. Our results show that even on devices
with limited resources, e.g., the Pi1, the cost of collecting host perfor-
mance and local network measures is low. In particular, about 12%
CPU overhead on a Pi1 is incurred using a modest data sampling
rate of once per second, or less than 3% with a sample rate of once
every five seconds. The potential benefit of these measurements is
high, as the results from our experiments show in which artificial
system and network load is introduced. In particular, the differences
in performance measures gathered during quiescent periods com-
pared with measurements collected when artificial CPU, memory,
I/O or network load is present make it apparent that something in
the local environment has perturbed the measurement.
In our ongoing work, we are considering a number of directions
which we think this work opens up. First, we intend to examine the
idea of treating network latency measurements between a host and
local network systems that are explicitly collected during quies-
cent conditions as a baseline reference to use for calibration of new
measurements. For example, while it is straightforward to evaluate
0246810RTT (milliseconds)0.00.20.40.60.81.0CDFSoMeta Hop 1 (all)SoMeta Hop 1 (off periods)SoMeta Hop 1 (on periods)1012141618202224RTT (milliseconds)0.00.20.40.60.81.0CDFSoMeta Hop 2 (all)SoMeta Hop 2 (off periods)SoMeta Hop 2 (on periods)30354045505560RTT (milliseconds)0.00.20.40.60.81.0CDFScamper Ping (all)Scamper Ping (off periods)Scamper Ping (on periods)SoMeta Ping (all)SoMeta Ping (off periods)SoMeta Ping (on periods)10−1100101102103RTT (milliseconds)0.00.20.40.60.81.0Cumulative fractionSoMeta Hop 1SoMeta Hop 2SoMeta Hop 3SoMeta Ping 8.8.8.8SoMeta Hop 1 (no load expt)SoMeta Hop 2 (no load expt)SoMeta Hop 3 (no load expt)SoMeta Ping 8.8.8.8 (no load expt)Scamper Ping 8.8.8.8Automatic Metadata Generation for Active Measurement
IMC ’17, November 1–3, 2017, London, United Kingdom
REFERENCES
[1] M Allman. 2013. On changing the culture of empirical Internet assessment. ACM
SIGCOMM Computer Communication Review 43, 3 (2013), 78–83.
[2] M. Allman and V. Paxson. 2007. Issues and etiquette concerning use of shared
measurement data. In Proceedings of the ACM SIGCOMM Internet Measurement
Conference. 135–140.
[3] B. Augustin, X. Cuvellier, B. Orgogozo, F. Viger, T. Friedman, M. Latapy, C. Mag-
nien, and R. Teixeira. 2006. Avoiding traceroute anomalies with Paris traceroute.
In Proceedings of the ACM SIGCOMM Internet Measurement Conference. 153–158.
[4] F. Baccelli, S. Machiraju, D. Veitch, and J. C. Bolot. 2007. On optimal probing
for delay and loss measurement. In Proceedings of the ACM SIGCOMM Internet
Measurement Conference. 291–302.
[5] V. Bajpai, A. W. Berger, P. Eardley, J. Ott, and J. Schönwälder. 2016. Global
Measurements: Practice and Experience (Report on Dagstuhl Seminar# 16012).
ACM SIGCOMM Computer Communication Review 46, 1 (2016), 32–39.
[6] V. Bajpai, S. Eravuchira, and J. Schönwälder. 2015. Lessons learned from using
the RIPE Atlas platform for measurement research. ACM SIGCOMM Computer
Communication Review 45, 3 (2015), 35–42.
[7] R. Beverly. 2016. Yarrp’ing the Internet: Randomized High-Speed Active Topology
Discovery. In Proceedings of the ACM SIGCOMM Internet Measurement Conference.
[8] BGPmon [n. d.]. BGPmon Deployment. http://bgpmon.netsec.colostate.edu/
about.html. ([n. d.]). Accessed May 2017.
[9] CAIDA [n. d.]. Ark Monitor Site Information. http://www.caida.org/projects/
ark/siteinfo.xml. ([n. d.]). Accessed May 2017.
[10] CAIDA [n. d.]. CAIDA Ark project. http://www.caida.org/projects/ark/. ([n. d.]).
Accessed May 2017.
[11] Center for Applied Internet Data Analysis (CAIDA). [n. d.]. DatCat: Internet
Measurement Data Catalog. http://www.datcat.org. ([n. d.]). Accessed May 2017.
[12] Center for Applied Internet Data Analysis (CAIDA). 2009. How to Document a
Data Collection. http://www.caida.org/data/how-to/how-to_document_data.xml.
(March 2009). Accessed May 2017.
[13] C. Dovrolis, K. Gummadi, A. Kuzmanovic, and S. D. Meinrath. 2010. Measurement
lab: Overview and an invitation to the research community. ACM SIGCOMM
Computer Communication Review 40, 3 (2010), 53–56.
[14] J. Heidemann, Y. Pradkin, R. Govindan, C. Papadopoulos, G. Bartlett, and J.
Bannister. 2008. Census and Survey of the Visible Internet (extended). ISI-TR-
2008-649 (2008).
[15] T. Holterbach, C. Pelsser, R. Bush, and L. Vanbever. 2015. Quantifying interference
between measurements on the RIPE Atlas platform. In Proceedings of the ACM
[16] J. D. Hunter. 2007. Matplotlib: A 2D graphics environment. Computing In Science
[17] D. Kotz and T. Henderson. 2005. CRAWDAD: A community resource for archiving
SIGCOMM Internet Measurement Conference. 437–443.
& Engineering 9, 3 (2007), 90–95.
wireless data at Dartmouth. IEEE Pervasive Computing 4, 4 (2005), 12–14.
[18] D. Kotz, T. Henderson, I. Abyzov, and J. Yeo. [n. d.]. CRAWDAD: A Commu-
nity Resource for Archiving Wireless Data At Dartmouth. https://crawdad.cs.
dartmouth.edu. ([n. d.]). Accessed May 2017.
[19] B. Krishnamurthy, W. Willinger, P. Gill, and M. Arlitt. 2011. A Socratic method
for validation of measurement-based networking research. Computer Communi-
cations 34, 1 (2011), 43–53.
[20] M. Luckie. 2010. Scamper: a scalable and extensible packet prober for active
measurement of the Internet. In Proceedings of the ACM SIGCOMM Internet
Measurement Conference. 239–245.
[21] M-Lab [n. d.]. M-Lab. https://www.measurementlab.net. ([n. d.]). Accessed May
2017.
[22] R. K. P. Mok, W. Li, and R. K. C. Chang. 2015.
Improving the packet send-
time accuracy in embedded devices. In Passive and Active Network Measurement
Conference. 332–344.
[23] Nagios [n. d.]. Nagios: The Industry Standard in IT Infrastructure Monitoring.
https://www.nagios.org. ([n. d.]). Accessed May 2017.
[24] C. Partridge and M. Allman. 2016. Ethical considerations in network measurement
papers. Commun. ACM 59, 10 (2016), 58–64.
[25] V. Paxson. 2004. Strategies for Sound Internet Measurement. In Proceedings of
the ACM SIGCOMM Internet Measurement Conference. 263–271.
[26] C. Pelsser, L. Cittadini, S. Vissicchio, and R. Bush. 2013. From Paris to Tokyo: On
the suitability of ping to measure latency. In Proceedings of the ACM SIGCOMM
Internet Measurement Conference. 427–432.
[27] RIPE [n. d.]. RIPE Atlas - RIPE Network Coordination Centre. https://atlas.ripe.net.
([n. d.]). Accessed May 2017.
[28] C. Shannon, D. Moore, K. Keys, M. Fomenkov, B. Huffaker, et al. 2005. The
Internet Measurement Data Catalog. ACM SIGCOMM Computer Communication
Review 35, 5 (2005), 97–100.
[29] J. Sommers. 2015. Lowering the Barrier to Systems-level Networking Projects. In
Proceedings of the 46th ACM Technical Symposium on Computer Science Education
(SIGCSE). 651–656.
[30] J. Sommers and P. Barford. 2007. An active measurement system for shared envi-
ronments. In Proceedings of the ACM SIGCOMM Internet Measurement Conference.
303–314.