ciency (Q6). We show that it is capable of detecting simulated
APT attacks using evolutionary modeling (Q4) with diverse
normal and background activities. We further demonstrate that
detection capability can be improved with contextualized graph
exploration (Q2), and that the gradually forgetting scheme
improves detection accuracy, because it helps better understand
system behavior (Q3).
Properly evaluating and benchmarking security systems is
difﬁcult. We adhere to the guidelines proposed by Kouwe
et al. [121] to the best of our ability. For example,
the
testbed that we designed to create the SC datasets ensures
proper documentation of experiment speciﬁcations and easy
reproduction of evaluation data and results. We also use 5-fold
cross-validation to provide a more accurate evaluation [78].
A. UNICORN vs. State-of-the-Art
StreamSpot [83] is a clustering-based anomaly detection
system that processes streaming heterogeneous graphs. It ex-
tracts local graph features from single node/edge labels through
7
breadth-ﬁrst traversal on each graph node and vectorizes them
for classiﬁcation. StreamSpot models only a single snapshot
of every training graph and dynamically maintains its clusters
during test time by updating the parameters of the clusters.
Experiment
StreamSpot
Dataset
YouTube
Gmail
Download
VGame
CNN
Attack
# of Graphs
100
100
100
100
100
100
Avg. |V|
8,292
6,827
8,831
8,637
8,990
8,891
Avg. |E|
113,229
37,382
310,814
112,958
294,903
28,423
Preprocessed Data Size (GiB)
0.3
0.1
1
0.4
0.9
0.1
TABLE I: Characteristics of the StreamSpot dataset. The dataset is publicly
available only in a preprocessed format.
Experimental Dataset. The StreamSpot dataset contains in-
formation ﬂow graphs derived from six scenarios, ﬁve of which
are benign [84]. Each scenario runs 100 times, producing
100 graphs for each. Using the Linux SystemTap logging
system [60], the benign scenarios record system calls from
normal browsing activities, such as watching YouTube videos
and checking Gmail, while the attack scenarios involve a
drive-by download from a malicious URL that exploits a
Flash vulnerability and gains root access to the visiting host.
The original scripts and the precise attack are unknown to
us, however, the datasets are publicly available [84], and we
conﬁrmed with the authors that the information ﬂow graphs
were constructed from all system calls on a machine from the
start of a task until its termination. Table I summarizes the
dataset.
We use this dataset
to fairly compare UNICORN with
StreamSpot. These nicely isolated scenarios may not represent
today’s typical workloads. However, they provide insight into
how the different systems perform relative to each other.
Furthermore, we might also interpret them as a proxy for the
design pattern of today’s microservice architecture [93]. As
we will see next, UNICORN performs particularly well in such
scenarios, but it is also capable of accurately detecting anoma-
lies in more diverse, heterogeneous computing environments
(§ VI-B and § VI-C). We discuss this point further in § VII.
Precision
Accuracy
F-Score
Experiment
StreamSpot (baseline)
R = 1
R = 3
Recall
N/A
1.0
0.93
0.74
0.51
0.98
0.66
0.60
0.96
N/A
0.68
0.94
TABLE II: Comparison to StreamSpot on the StreamSpot dataset. We estimate
StreamSpot’s average accuracy and precision from the ﬁgure included in the
paper [83], which does not report exact values. They did not report recall or
F-score.
Experimental Results. We compare UNICORN to StreamSpot,
using StreamSpot’s own dataset. We conﬁgure UNICORN to
use a sketch size |S| = 2000 and examine with different
neighborhood sizes, R = 1 (equivalent to StreamSpot) and
R = 3. As shown in Table II, UNICORN’s ability to trivially
consider larger neighborhoods (R = 3) produces signiﬁcant
# of Test Graphs
# of FPs (R = 1)
# of FPs (R = 3)
TABLE III: Decomposition of UNICORN’s false positive results of the
StreamSpot dataset.
Experiment
YouTube
Gmail
Download
VGame
CNN
25
25
25
25
25
14
19
25
20
18
0
0
2
0
0
precision/accuracy improvement. The detailed precision results
in Table III further show that analyzing larger neighbor-
hoods greatly reduces the false positive rate. This supports
our hypothesis that contextual analysis is crucial. UNICORN
raises false positive alarms only on the Download dataset,
the most diverse dataset of StreamSpot’s benign datasets (also
manifested in its large average number of edges in Table I).
We discuss the importance of graph exploration in more depth
in § VI-C.
The following sections evaluate UNICORN on various real-
life and simulated APT attacks. Unfortunately, we were unable
to evaluate StreamSpot on these datasets, because it could not
handle the large number of edge types present nor could it
scale to the size of the graphs.
B. DARPA TC Datasets
Next, we demonstrate that UNICORN can effectively detect
APTs utilizing data from a variety of different provenance
capture systems.
DARPA’s Transparent Computing program focuses on de-
veloping technologies and prototype systems to provide both
forensic and detection of APTs.
Experiment
DARPA
CADETS
DARPA
ClearScope
DARPA
THEIA
Dataset
Benign
Attack
Benign
Attack
Benign
Attack
# of Graphs
66
8
43
51
2
25
Avg. |V|
59,983
386,548
2,309
11,769
19,461
275,822
Avg. |E|
4,811,836
5,160,963
4,199,309
4,273,003
1,913,202
4,073,621
Raw Data Size (GiB)
271
38
441
432
4
85
TABLE IV: Characteristics of graph datasets used in the DARPA experiments.
Experimental Datasets. The DARPA datasets (Table IV) were
collected from a network of hosts during the 2-week long third
adversarial engagement of the DARPA Transparent Computing
program. The engagement involved various teams responsible
for collecting audit data from different platforms (e.g., Linux,
Windows, BSD),
launching attacks during the engagement
period, and analyzing the data to detect attacks and perform
forensic analysis. The red team that carried out attacks also
generated benign background activity (e.g., web browsing),
thus allowing us to model normal system behavior.
and Efﬁcient Tracing
tagging and tracking multi-level host events [35];
The CADETS dataset was captured via the Causal,
Adaptive, Distributed,
System
(CADETS) on FreeBSD [1]. ClearScope instruments the
entire Android mobile software stack to capture provenance
of the operations of mobile devices [3]. THEIA is a system
for
it
instruments Ubuntu Linux machines during the engagement.
The experiment simulated an enterprise setup [87] includ-
ing security-critical services such as a web server, an SSH
server, an Email server, and an SMB server (for shared ﬁle
access). The red team carried out various nation-state and
common threats through the use of, e.g., a Firefox backdoor,
a Nginx backdoor, and phishing emails. Detailed descriptions
of the attacks are available online [66].
Experimental Results. We partition each benign dataset into
a training set (90% of the graphs) and a test dataset (10% of
the graphs). We use the same sketch size (|S| = 2000) and
neighborhood hop (R = 3) as in the StreamSpot experiment.
Experiment
DARPA CADETS
DARPA ClearScope
DARPA THEIA
Precision
Recall
Accuracy
F-Score
0.98
0.98
1.0
1.0
1.0
1.0
0.99
0.98
1.0
0.99
0.99
1.0
TABLE V: Experimental results of the DARPA datasets.
Table V shows that UNICORN’s analytics framework gener-
alizes to different provenance capture systems and various
provenance graph structures. UNICORN’s high performance
suggests that it can accurately detect anomalies in long-running
systems of various platforms. During the engagement, the red
team launched APT attacks using different attack vectors and
the attacks account for less than 0.001% of the audit data
volume [87]. UNICORN’s anomaly-based detection mechanism
identiﬁes those attacks without prior attack knowledge, even
though they are embedded in an abundance of benign activity.
We note that some existing systems (Holmes [87] and
Poirot [86]) also use the DARPA dataset for evaluation.
Comparison between UNICORN and these systems is difﬁ-
cult, because they use a rule-based approach that requires a
priori expert knowledge to construct a model. UNICORN is
fundamentally different, using an unsupervised learning model,
requiring no expert input. However, UNICORN’s performance
is comparable based on the number of detected attacks: UNI-
CORN detects all attacks on FreeBSD and Linux as do Holmes
and Poirot. We discuss the differences between these systems
and UNICORN in greater detail in § VIII.
C. Supply Chain Attack Scenarios
We designed two APT attack scenarios to run in a con-
trolled lab environment. These experiments evaluate the impor-
tance of graph analysis and evolutionary modeling (this section
and § VI-D) to show that UNICORN is able to perform efﬁcient,
realtime monitoring (§ VI-E). Additionally, we use these ex-
periments to understand how UNICORN performs when faced
with attacks that behave similarly to normal system workloads.
We carefully design benign and attack scenarios to achieve
this goal. Previous experiments, conducted by others, do not
guarantee similarity between benign and attack scenarios.
Experiment
SC-1
SC-2
Dataset
Benign
Attack
Benign
Attack
# of Graphs
125
25
125
25
Avg. |V|
265,424
257,156
238,338
243,658
Avg. |E|
975,226
957,968
911,153
949,887
Raw Data Size (GiB)
64
12
59
12
TABLE VI: Characteristics of the datasets used in the supply-chain APT attack
experiments.
Experimental Datasets. We simulated two APT supply-chain
attacks (SC-1 and SC-2) on a Continuous Integration (CI)
platform and used CamFlow (v0.5.0) to capture whole-system
provenance, including background activity, during both benign
and attack scenarios. For each scenario, the experiment ran
for three days. To facilitate reproduction, we leverage virtual-
ization technology as our test harness and provide automated
scripts for push-button replication of the experiments that
generated the data (see Appendix § A for details).
To simulate APT attacks, we follow the typical cyber kill
chain model that consists of roughly 7 nonexclusive phases,
i.e., reconnaissance (identify a target and explore its vulnera-
bilities), weaponize (design a backdoor and a penetration plan),
8
Baseline
Batch Size
6,000
Sketch Size
2,000
TABLE VII: UNICORN conﬁgurations for supply-chain APT attack scenarios.
Hop Count
3
Decay Factor
0.02
Sketch Interval
3,000
delivery (deliver the weapon), exploitation (victim triggers the
vulnerability), installation (install the backdoor or malware),
command and control (C&C) (give remote instructions to the
victim), and actions on objectives [131].
In the SC-1 experiment, the attacker identiﬁed an enter-
prise CI server that routinely wgets Debian packages from
various repositories. She discovered that the server runs GNU
wget version 1.17, which is vulnerable to arbitrary remote
ﬁle upload when the victim requests a malicious URL to a
compromised server (CVE-2016-4971) [47] (reconnaissance).
The attacker embedded a common remote access trojan (RAT)
into a Debian package and compromised one of the repositories
so that any request to download the legitimate package is
redirected to the attacker’s FTP server that hosts the RAT-
embedded package (delivery). As the CI server downloaded
(exploitation) and installed (installation) the package, it also
unknowingly installed the trojan software. The RAT estab-
lished a C&C channel with the attacker, creating a reverse
TCP shell session on the CI server (C&C). The attacker then
modiﬁed the CI server conﬁguration (actions on objectives) to
gain control of the CI deployment output. The SC-2 experiment
had a similar setup but