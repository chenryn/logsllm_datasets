uses  an  object  database  of  size  125  and  a  set  of  background 
images  of  size  100.  Images  created  by  the  system  are  of  size 
640x480 pixels and each contain 3 to 5 objects chosen randomly 
from the object database. 
A  preliminary  prototype  of  our  system  may  be  seen  at 
http://cns.eecs.ucf.edu/captcha. A reader who is interested in our 
system can go to this website to see more clearly the effects of the 
image  distortions  and  to  experience  the  use  of  the  system  first-
hand. 
348 
feature  points  in  an  object  database  image  that  have  matches, 
filtered for uniqueness, found in the composite image. 
The image sets described in Table 1 were utilized for automated 
attack  testing.  Each  image  set  contains  100  randomly  generated 
scene tagging instances with 3 or 4 objects present.  
Table 1. Image Sets Tested 
Distortion Set Applied 
No distortions, objects are placed on flat background 
Object Scaling, mesh warping 
Global color shifting, randomized clutter, localized 
color shifting, semi-regular object clutter 
Combination of the distortions of set C and D 
Combination of the distortions of set E and localized 
texture effects 
Set 
A 
B 
C 
D 
E 
The results of these automated attacks for the various image sets 
are presented in figure 3. The results show that with full series of 
distortions discussed above (data set E), automated attacks have a 
very  low  success  rate.  The  most  successful  automated  attack 
when dealing with the full set of distortions is SIFT, which is able 
to  correctly  identify  the  objects  present  in  an  image  2%  of  the 
time. 
6.3  User Study 
A  user  study  with  20  individuals  was  conducted  in  order  to 
measure human ability to successfully pass the test. The average 
user answered 63.9 questions in the time allotted. Approximately 
1/2 of the problems used multiple choice and half of the questions 
used  image  point  selection.  Question  formats  were  dynamically 
chosen  based  on  the  number  of  and  associations  between  the 
objects present in the scene image. Users were asked to mark the 
images  where  content  was  so  unclear  that  they  were  unable  to 
answer the question. Table 2 shows user task success rates. 
Table 2. User Study Task Success Rates 
Image  Point 
Selection 
0.965 
Response Format  Multiple 
Choice 
0.979 
User Success Rate 
Overall 
0.975 
As table 2 shows, over 97% of the questions answered by users 
were  answered  correctly.  This  falls  well  within  an  acceptable 
range, as even widely implemented CAPTCHA systems such as 
reCaptcha  find  that  users  have  problems  answering  a  given 
question from 3% to 7% of the time [18]. An image was deemed 
to be unclear in 2.8% of cases, making a strong case for allowing 
a user a small number of image discards without penalty.  
Figure 4 presents the distribution of image clicks with regards 
to distances from the center of the correct answer. These results 
indicate  that  accepting  an  answer  that  is  less  than  or  equal  to  a 
distance of 50 pixels from the center of the object will accept over 
96% of the user responses within 100 pixels while only 2.56% of 
random guess clicks will be accepted. 
During  the  study,  the  average  number  of  seconds  that elapsed 
between the point when a question was presented to the user and 
the  point  at  which  the  user  answered  that  question  was  11.034 
seconds. Over 99% of the user response times measured were less 
than 45 seconds, making it a good candidate for use as an answer 
Figure 2. Scene Tagging CAPTCHA image with distortion. 
Figure 3. Success rates of automated attack techniques. 
Figure  4.  Relative  frequency  histogram  of  pixel  distance  of 
user answer from true object center 
6.2  Automated Attack Study 
In  order  to  measure  the  likelihood  of  success  via  automated 
attack techniques, three machine vision techniques were utilized. 
These techniques take in the object database and a scene image, 
and return the likelihood of each object being present. We define 
an attack attempt success when the attack ranks the objects that 
are present in a scene as those most likely to be present. 
The first automated attack technique tested is template matching 
via  measuring  the  normalized  pixel-wise  difference  (PWD) 
between object images and areas of corresponding size and shape 
in  the  scene  tagging  image.  The  second  and  third  automated 
attack  techniques  utilize  the  feature  point  generators  SURF  and 
SIFT, respectively, in voting schemes based on the percentage of 
349 
to  make 
acceptance  cut-off  point.  This  may  be  used 
computationally  expensive  automated  attacks  infeasible.  For 
example,  our  tests  indicate  that  it  takes  approximately  4.436 
seconds per database object image to perform a PWD based attack 
against a single problem on an Intel Core 2 Duo E6850 machine. 
Thus, if an object database of size 1000 were utilized then the task 
would take approximately 73.93 minutes to complete. 
Table  3  displays  the  human  user  success  rate  and  best 
automated  attack  success  rate  for  a  number  of  image-based 
CAPTCHA systems. For a fair comparison, the numbers provided 
are  for  completion  of  a  single  task  and  not  a  combination  of 
multiple tasks. Note that the most successful known attack on all 
of  these  tasks  except  ASSIRA  is  that  of  randomized  guessing. 
Where single task randomized guess attack success rates have not 
been  presented  explicitly  they  have  been  derived  from  the 
information given via straightforward geometry. 
Table 3. Comparison of image-based CAPTCHA user and 
automated attack success rates 
CAPTCHA Name  
Scene Tagging (Image 
Point Selection, 50 
pixel region radius) 
ASSIRA 
What’s Up CAPTCHA 
(single image rotation, 
16 degree window) 
IMAGINATION 
(Image Center 
Selection, 25 pixel 
region radius) 
User Success 
Rate 
0.966 
0.99[13] 
0.94[14] 
Automated 
Attack Success 
Rate 
0.026 
0.10[16] 
0.040 
0.70[17] 
0.033 
relationships 
7.  CONCLUSIONS AND FUTURE WORK 
We  have  presented  a  novel  form  of  image-based  CAPTCHA 
that  distinguishes  between  humans  and  machines  based  on  their 
understanding  of  objects  and  object 
in  an 
automatically generated, composition-based image. Experimental 
results  indicate  that  the  system  is  secure  to  several  automated 
attack  techniques  and  a  user  study  shows  that  the  system  has 
comparable usability with existing CAPTCHA systems. Based on 
these results, it is clear that scene tagging CAPTCHA is a viable 
alternative  to  text-based  and  other  image-based  CAPTCHA 
systems.  Future  work  includes  improving  and  expanding  our 
automated attack testing array along with investigating the use of 
3D object models in our image generation process. 
8.  REFERENCES 
[1]  L. von Ahn, M. Blum, and J. Langford. Telling Humans and 
Computers Apart (Automatically) or How Lazy 
Cryptographers do AI. Comm. of the ACM, 47 (2), 57-60. 
[2]  H. S. Baird and J.L. Bentley. Implicit Captchas.  In 
Proceedings of the IST SPIE Document Recognition and 
Retrieval XII Conference, (San Jose, CA, 2005), vol. 5676. 
[3]  Bay, H., Ess, A., Tuytelaars, T., and Van Gool, L. 2008. 
Speeded-Up Robust Features (SURF). Comput. Vis. Image 
Underst. 110, 3 (Jun. 2008), 346-359. 
[4]  Budanitsky, A., Hirst, G. Semantic distance in WordNet: An 
experimental, application--oriented evaluation of five 
measures. In Proceedings of the North American Chapter of 
the Association for Computational Linguistics Workshop 
(Pittsburgh, PA, USA, 2001), 29-34. 
[5]  Chellapilla, K., and Simard, P.Y. Using Machine Learning to 
Break Visual Human Interaction Proofs (HIPs). Advances in 
Neural Information Processing Systems 17, 265-272. 
[6]  M. Chew and J. D. Tygar. Image Recognition CAPTCHAs. 
In Proceedings of the 7th Annual Information Security 
Conference (Palo Alto, CA, USA, 2004), 268–279. 
[7]  Datta, R., Li, J., and Wang, J. Z. IMAGINATION: a robust 
image-based CAPTCHA generation system. In Proceedings 
of the 13th Annual ACM international Conference on 
Multimedia (Hilton, Singapore, 2005), 331-334. 
[8]  Datta, R., Li, J., and Wang, J. Z. Exploiting the Human-
Machine Gap in Image Recognition for Designing 
CAPTCHAs. IEEE Transactions on Information Forensics 
and Security, 4 (3), 504-518. 
[9]  Elson, J.,  Douceur, J.R., Howell, J., and Saul, J. Asirra: a 
CAPTCHA that exploits interest-aligned manual image 
categorization. In Proceedings of the 14th ACM Conference 
on Computer and Communications Security (Alexandria, 
Virginia, USA, 2007), 366-374. 
[10] Golle, P. 2008. Machine learning attacks against the Asirra 
CAPTCHA. In Proceedings of the 15th ACM Conference on 
Computer and Communications Security (Alexandria, 
Virginia, USA, 2008), 535-542.  
[11] Gossweiler, R., Kamvar, M., and Baluja, S. 2009. What's up 
CAPTCHA?: a CAPTCHA based on image orientation. In 
Proceedings of the 18th international Conference on World 
Wide Web (Madrid, Spain, 2009), 841-850  
[12] Lowe, D. G. 2004. Distinctive Image Features from Scale-
Invariant Keypoints. Int. J. Comput. Vision 60, 2 (Nov. 
2004), 91-110. 
[13] Mikolajczyk, K. and Schmid, C. 2005. A Performance 
Evaluation of Local Descriptors. IEEE Trans. Pattern Anal. 
Mach. Intell. 27, 10 (Oct. 2005), 1615-1630. 
[14] G. A. Miller. 1990. Wordnet: a lexical database for English. 
International Journal of Lexicography, 3 (4), 235-244. 
[15] Moy, G., Jones, N., Harkless, C., and Potter, R. Distortion 
Estimation Techniques in Solving Visual CAPTCHAs. In 
IEEE CVPR, (Washington, D.C., USA, 2004), Vol. 2, 23-28. 
[16] Wolberg, G. Digital Image Warping. IEEE Computer 
Society Press, Los Alamitos, CA, 1990. 
[17] Yan, J. and El Ahmad, A. S. 2008. A low-cost attack on a 
Microsoft captcha. In Proceedings of the 15th ACM 
Conference on Computer and Communications Security 
(Alexandria, Virginia, USA, 2008), 543-554. 
[18] Yan, J. and El Ahmad, A. S. 2008. Usability of CAPTCHAs 
or usability issues in CAPTCHA design. In Proceedings of 
the 4th Symposium on Usable Privacy and Security 
(Pittsburgh, Pennsylvania, 2008), Vol. 337, 44-52.
350