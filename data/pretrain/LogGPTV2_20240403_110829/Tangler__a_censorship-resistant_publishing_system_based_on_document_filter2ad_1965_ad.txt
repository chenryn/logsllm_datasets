collection would reach a steady state of reinjecting 1/14 or 7% of
her server blocks every day.
110000
000000
servers:
A
B
C
010000
block
011001
100000
Figure 3: Use of consistent hashing to distribute server blocks
amongst servers.
6.2 Block introduction protocol
The block introduction protocol has three phases. First, servers
non-anonymously request storage credits from each other on behalf
of anonymous users wishing to publish blocks. Second, servers
issue storage certiﬁcates in exchange for blocks they receive when
a storage credit is redeemed. Finally, each server commits to the
blocks it is serving by producing a hash tree of all its blocks and
signing the dated root of the tree. We describe each stage in more
detail.
6.2.1
Storage credits
Every day, servers must accept 1/15 of their stated capacity in
new blocks to store for two weeks. 1 The process begins when
servers request storage credits from each other. Servers exchange
credits in proportion to their capacities. If server A has capacity cA,
server B capacity cB, and the total capacity of all non-probationary
servers is C, then A and B will request cAcB/15C credits from
each other. New servers cannot request credits during their proba-
tionary period, but must grant credits just the same.
K−1
A
Credits are nothing more than digital signatures of block hashes,
blinded [3] so that servers do not know for what blocks they are
issuing credits. Speciﬁcally, each server A creates a daily tempo-
rary public key KTA for signing storage credits.
It certiﬁes the
key and the day, d, with its long-lived public key KA, produc-
ing {CREDIT-KEY, d, KTA}
. A server must certify exactly
one such temporary key per day and must never reuse keys; two
CREDIT-KEY certiﬁcates from the same server with the same d or
KTA are grounds for expulsion from the system.
A credit consists of a CREDIT-KEY certiﬁcate and the signed
hash of some block m, {SHA-1(m)}
. Credits are produced
using blind signatures, so that A does not see the contents of the
blocks for which it is issuing credits. When servers request credits
on behalf of users, the users themselves blind the requests. Thus,
even the server requesting a credit will likely not know what block
that credit is for.
KT −1
A
All requests for credits are numbered, tagged with the day num-
ber, and signed by the requesting server. Thus, a server can easily
1Because servers accumulate new blocks for the following day be-
fore deleting old blocks, a server may need to store 15 days worth
of blocks. Thus, the daily block intake of 1/15 capacity.
133prove that another, bad server has requested too many credits by
producing either a signature with too high a number or two differ-
ent signed requests with the same day and number. If a bad server
ever refuses a legitimate request for a credit, the requester forwards
the request through other servers which it uses as witnesses. If the
server persists in refusing to issue credits, it eventually gets ejected
from the system. Note that there is no problem disclosing credits
to witnesses, because the credits are blinded. Moreover, witnesses
can easily check whether a credit corresponds to a particular blind
request without needing to unblind the request.
At the end of the day, servers use remaining credits to republish
some of their expiring blocks on servers that will be responsible for
the next days points in the SHA-1 circle.
6.2.2
Storage receipts
Once a user has obtained a storage credit—say from server A
for block m—she must transfer m’s contents to A. This is done
by anonymous communication through other servers. If m has not
previously been published, the user sends its contents to A along
with the block’s unblinded storage credit. A, if honest, replies with
a signed storage receipt, {RECEIPT, SHA-1(m), d}
. If A re-
fuses to issue a storage receipt, the user anonymously enlists an-
other server as a witness. The witness presents A with m and the
storage credit. If A still refuses to acknowledge receipt, the wit-
ness forwards the request through other servers who either obtain a
receipt from A or eventually eject A from the system.
K−1
A
If m has already been published, then instead of forwarding its
contents to A, the user forwards the identity of another server, B,
currently serving m. A must then either obtain m from B, return to
the user a storage commitment from B that does not include m (see
below), or else initiate the process of ejecting B from the system.
In the case of a storage commitment excluding m, as explained
below, if B was supposed to have stored m, the user will have a
succinct proof of B’s misbehavior and can anonymously initiate its
ejection from the system.
6.2.3
Storage commitment
At the end of the day, after the exchange of storage receipts, each
server makes its newly received blocks available and publishes a
signed storage commitment. The commitment consists of the cur-
rent date and the root of a balanced hash tree. The leaves of this tree
contain a sorted list of hashes of every block the server is serving,
followed by a sorted list of (public key, collection version) pairs,
and for each block of either type the number of days the block has
to live (initially 14). A server must be able to produce its current
signed commitment and any node of the hash tree upon request, or
else face ejection after the requester involves witnesses. A server
must never sign more than one storage commitment per day. Two
distinct commitments signed by the same server for the same day
constitute grounds for expulsion.
Storage commitments prevent servers from discarding or sup-
pressing blocks they have agreed to publish. Once a server has
committed to storing a block by signing the hash tree root, it must
produce that block on demand or face ejection after the requester
involves witnesses. Every block lookup becomes a possible au-
dit of a server’s behavior. A server that has published a block can
anonymously ask another server, the witness, to verify that a partic-
ular block is being stored on server A. The publishing server sends
the storage receipt, that server A had previously signed, along with
the veriﬁcation request. This storage receipt proves that server A
committed to hosting the block. Therefore, if server A cannot pro-
duce the block being veriﬁed, it will face eventual ejection from the
system.
The hash trees in storage commitments play several other roles in
Tangler. Users publishing documents use the trees to select random
blocks to entangle with. To retrieve a random block, a user ﬁrst se-
lects a random server A, weighing the probabilities of the servers
by their capacities. The user knows A’s block capacity, cA, and so
can simply pick a random n, 0 ≤ n < cA, and walk the hash tree
from A’s storage commitment root to ﬁnd the hash of the nth block.
(This is easy, since the tree is balanced and all leaves store the same
number of hashes.) The user can then simply request block num-
ber n from A by its hash. Fetching random blocks in this way
implicitly audits servers’ behavior, as a server which loses some
percentage of its blocks will very likely be discovered. Of course,
requests for random blocks are sent anonymously so that servers
cannot identify publishers by the blocks they have entangled with.
Servers also use the hash trees in commitments to ﬁll their excess
capacity. As discussed below, in the event of an unavailable server
that does not appear to be corrupt, the network will have more stor-
age than it issues credits. Though not enforced, good servers can
ﬁll any extra space with blocks near their recently acquired points
on the circle from servers about to relinquish nearby points. Be-
cause the leaves of the commitment hash trees store blocks in sorted
order, it is easy to ﬁnd blocks near a particular point on the circle.
Filing extra capacity in this way also implicitly audits other servers.
The public key list in a storage commitment can also be used
to detect new collections. A search engine could make use of this
information to index Tangler collections.
6.3 Discussion and limitations
The Tangler protocol we propose provides anonymity for pub-
lishing while preventing ﬂooding attacks. Though blocks are dis-
persed untraceably across all servers, no server can consume more
than its fair share of storage. Because different servers employ dif-
ferent block admission policies, it is unlikely that an attacker could
simultaneously monopolize all servers’ available storage credits.
If an attacker did, servers could charge e-cash for storage credits
and use the revenue to dedicate more resources to the system. The
Tangler protocol also implicitly audits servers’ behavior at many
stages, ensuring that bad servers can quickly be ejected. By disal-
lowing servers from publishing during their ﬁrst month of opera-
tion, the protocol ensures that even bad servers do more good than
harm.
One of the limitations of the protocol is its synchrony require-
ments. Certain behavior should obviously result in immediate ex-
pulsion from the system—for instance issuing two different storage
commitments on the same day. It is less obvious what to do with a
server that becomes unavailable for 24 hours, however. The basic
protocol would eject such a server when it failed to issue storage
credits or produce a storage commitment. Other options include
delaying updates until the server returns (unscalable), or simply
waiting for a few update cycles before ejecting the server.
If an unavailable server is not ejected before the system updates,
there is a risk of revealing which blocks that server’s credits have
supported—those blocks will slowly disappear unless the user pub-
lishes them through another server. If the unavailable server still
counts towards the non-probationary capacity of the system, servers
will have more capacity than they issue storage credits (since the
unavailable server will not request its credits). Some blocks orig-
inally introduced by the unavailable server may therefore be pre-
served by other servers ﬁlling their excess capacity. Other blocks
will be reintroduced by other users because of entanglement. Nonethe-
less, a gradually increasing number of blocks will disappear.
Other possible attacks include attempting to use all of one server’s
credits towards a small set of other servers, so as to block a collec-
134tion root that needs to be published on those servers. A corrupt
server might also reduce performance by acting correctly but delib-
erately slowly. The Tangler network also needs to resist traditional
denial of service attacks, such as a ﬂood of block lookup requests.
Conventional defenses such as hashcash are adequate for many at-
tacks, but anonymity makes it harder to trace bad users.
7. SUMMARY
We have described Tangler, a distributed document storage sys-
tem with censorship-resistant properties. Tangler transforms pub-
lished documents into ﬁxed-size blocks in such a way that many
blocks actually belong to multiple documents. This technique, known
as entanglement, diffuses responsibility from particular servers for
particular documents, makes replicating other people’s documents
an inherent part of publishing, and furthermore gives anyone a plau-
sible excuse for replicating any block in the system.
We also described the design of a self-policing storage network
in which volunteers accept to store and serve entangled blocks. The
network gives servers discretion over what they publish, but pre-
vents any server from controlling what other servers publish. The
protocol conceals the relationship between blocks and the servers
that introduce them, but blinded storage credits prevent any server
from consuming more space than it provides.
The Tangler network additionally leverages entanglements to make
auditing of servers’ behavior an inherent part of publishing. Most
forms of misbehavior result in a server’s immediate expulsion. The
worst a bad server can generally do is reduce the capacity of the
system by whatever storage it is was contributing before the ex-
pulsion. Because new servers are blocked from publishing during
a probationary period, an ejected, malicious server that rejoins the
system under a new identity will only reverse the damage it has
done by restoring the system’s lost capacity.
8. ACKNOWLEDGMENTS
We would like to thank Chuck Blake, Roger Dingledine, Frank
Dabek, Kevin Fu, Frans Kaashoek, Zvi Kedem, Robert Morris, and
the anonymous reviewers for their helpful and insightful comments.
9. REFERENCES
[1] A. Back. The eternity service. Phrack Magazine, 7(51),
1997. "http://www.cypherspace.org/~adam/
eternity/phrack.html".
[2] D. Chaum. Untraceable electronic mail, return adresses, and
digital pseudonyms. Communications of the ACM,
24(2):84–88, February 1981.
[3] D. Chaum. Blind signature system. In Advances in
Cryptology—CRYPTO ’83, 1983.
[4] I. Clarke, O. Sandberg, B. Wiley, and T. Hong. Freenet: A
distributed anonymous information storage and retrieval
system. In Proceedings of the Workshop on Design Issues in
Anonymity and Unobservability, 2000.
[5] F. Dabek, M. F. Kaashoek, D. Karger, R. Morris, and
I. Stoica. Wide-area cooperative storage with CFS. In
Proceedings of the 18th ACM Symposium on Operating
Systems Principles (SOSP ’01), Chateau Lake Louise, Banff,
Canada, October 2001.
[6] R. Dingledine, M. J. Freedman, and D. Molnar. The free
haven project: Distributed anonymous storage service. In
Proceedings of the Workshop on Design Issues in Anonymity
and Unobservability, 2000.
[7] K. Fu, M. F. Kaashoek, and D. Mazi`eres. Fast and secure
distributed read-only ﬁle system. In Proceedings of the 4th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 2000), San Diego, California,
October 2000.
[8] Gnutella Web Site. http://gnutella.wego.com/.
[9] A. V. Goldberg and P. N. Yianilos. Towards and archival
intermemory. In Proc. IEEE International Forum on
Research and Technology Advances in Digital Libraries
(ADL’98), pages 147–156. IEEE Computer Society, April
1998.
[10] D. Karger, E. Lehman, T. Leighton, M. levine, D. Lewin, and
R. Panigrahy. Consistent hashing and random trees:
Distributed caching protocols for relieving hot spots on the
world wide web. In Proceedings of the 29th Annual ACM
Symposium on Theory of Computing, pages 654–663, 1997.
[11] R. Merkle. A digital signature based on a conventional
encryption function. Advances in Cryptology: Proceedings
of Crypto 87, pages 369–378, 1987.
[12] Mojo Nation Web Site. http://www.mojonation.net/.
[13] M. Naor. Veriﬁcation of a human in the loop or identiﬁcation
via the turing test. Unpublished draft from
http://www.wisdom.weizmann.ac.il/~naor/PAPERS/
human abs.html,
1996.
[14] National Institute of Standards and Technology. Secure hash
standard, 1995.
[15] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Shenker. A scalable content-addressable network. In
Proceedings of ACM SIGCOMM 2001, San Diego,
California, USA, August 2001.
[16] A. M. Ricciardi and K. P. Birman. Using process groups to
implement failure detection in asynchronous environments.
In Proceedings of the 10th Annual ACM Symposium on
Principles of Distributed Computing, pages 341–353, August
1991.
[17] A. Rowstron and P. Druschel. Pastry: Scalable, distributed
object location and routing for large-scale peer-to-peer
systems. http://www.research.microsoft.com/
~antr/pastry/,
2001.
[18] A. Shamir. How to share a secret. Communications of the
ACM, 22:612–613, Nov. 1979.
[19] D. Stinson. Cryptography: Theory and Practice. CRC Press,
Inc, 1995.
[20] A. Stubbleﬁeld and D. S. Wallach. Dagster:
Censorship-resistant publishing without replication.
Technical Report TR01-380, Rice University, Houston,
Texas, July 2001.
[21] M. Waldman, A. D. Rubin, and L. Cranor. Publius: A robust,
tamper-evident and censorship-resistant web publishing
system. In Proceeding of the 9th USENIX Security
Symposium, pages 59–72, Denver, Colorado, August 2000.
[22] B. Y. Zhao, J. D. Kubiatowicz, and A. D. Joseph. Tapestry:
An infrastructure for fault-tolerant wide-area location and
routing. Technical Report CSD-01-1141, U. C. Berkeley,
April 2000.
135