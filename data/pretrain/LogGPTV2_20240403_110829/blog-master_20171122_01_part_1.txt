## PostgreSQL distinct 与 Greenplum distinct 的实现与优化  
### 作者                      
digoal                      
### 日期                      
2017-11-22                    
### 标签                      
PostgreSQL , distinct , 多distinct , groupagg , hashagg , sort , hyperloglog , 估值    
----                      
## 背景  
求distinct是业务的一个普遍需求，例如每天有多少用户，每个省份有多少用户，每天有多少类目的用户等。  
```  
select date,count(dinstinct user) from tbl group by date;  
select date, province, count(distinct user) from tbl group by 1,2;  
select date, count(dinstnct user), count(distinct class) from tbl group by 1;  
```  
distinct是一个求唯一值个数的需求，如果你不需要精确值的话，你还可以选择一些估值计算方法：  
[《Greenplum 最佳实践 - 估值插件hll的使用(以及hll分式聚合函数优化)》](201608/20160825_02.md)    
[《PostgreSQL hll (HyperLogLog) extension for "State of The Art Cardinality Estimation Algorithm" - 3》](../201302/20130228_01.md)    
[《PostgreSQL hll (HyperLogLog) extension for "State of The Art Cardinality Estimation Algorithm" - 2》](../201302/20130227_01.md)    
[《PostgreSQL hll (HyperLogLog) extension for "State of The Art Cardinality Estimation Algorithm" - 1》](../201302/20130226_01.md)   
[《秒级任意维度分析1TB级大表 - 通过采样估值满足高效TOP N等统计分析需求》](../201709/20170911_02.md)    
[《妙用explain Plan Rows快速估算行》](../201509/20150919_02.md)    
[《PostgreSQL pg_stats used to estimate top N freps values and explain rows》](../201308/20130811_01.md)    
本文主要分析一下PostgreSQL和Greenplum的distinct 算法：  
hashagg和groupagg  
## hashagg和groupagg观察模型  
为了便于观察，我们需要创建一张测试表，灌入1亿条测试记录。   
```  
create table tbl(c1 int, c2 int, c3 int, c4 int);  
insert into tbl select random()*1000, random()*1000, random()*100, random()*100 from generate_series(1,100000000);  
```  
## distinct语句  
1、  
```  
select c1,c2,count(distinct c3) from tbl group by c1,c2;  
```  
2、  
```  
select c1,c2,count(distinct c3),count(distinct c4) from tbl group by c1,c2;  
```  
3、  
```  
select c1,c2,count(distinct (c3,c4)) from tbl group by c1,c2;  
```  
## distinct替换语句  
1、  
```  
select c1,c2,count(*) cn from (select c1,c2,c3 from tbl group by c1,c2,c3) t group by c1,c2;  
```  
2、  
```  
select t1.c1, t1.c2, t1.cn as c3, t2.cn as c4 from  
  (select c1,c2,count(*) cn from (select c1,c2,c3 from tbl group by c1,c2,c3) t group by c1,c2) t1  
join  
  (select c1,c2,count(*) cn from (select c1,c2,c4 from tbl group by c1,c2,c4) t group by c1,c2) t2  
on (  
  NOT t1.c1 IS DISTINCT FROM t2.c1   
  AND   
  NOT t1.c2 IS DISTINCT FROM t2.c2  
);  
```  
3、  
```  
select c1,c2,count(*) cn from (select c1,c2,row(c3,c4) from tbl group by c1,c2,row(c3,c4)) t group by c1,c2;  
```  
## PostgreSQL distinct 语句的算法  
目前PostgreSQL 求distinct仅支持groupAgg，从源码可以看到，是通过排序去重来实现的：  
src/backend/executor/nodeAgg.c  
```  
 *        If a normal aggregate call specifies DISTINCT or ORDER BY, we sort the  
 *        input tuples and eliminate duplicates (if required) before performing  
 *        the above-depicted process.  (However, we don't do that for ordered-set  
 *        aggregates; their "ORDER BY" inputs are ordinary aggregate arguments  
 *        so far as this module is concerned.)  Note that partial aggregation  
 *        is not supported in these cases, since we couldn't ensure global  
 *        ordering or distinctness of the inputs.  
```  
```  
        Tuplesortstate **sortstates;    /* sort objects, if DISTINCT or ORDER BY */  
/*  
 * Run the transition function for a DISTINCT or ORDER BY aggregate  
 * with only one input.  This is called after we have completed  
 * entering all the input values into the sort object.  We complete the  
 * sort, read out the values in sorted order, and run the transition  
 * function on each value (applying DISTINCT if appropriate).  
 *  
 * Note that the strictness of the transition function was checked when  
 * entering the values into the sort, so we don't check it again here;  
 * we just apply standard SQL DISTINCT logic.  
 *  
 * The one-input case is handled separately from the multi-input case  
 * for performance reasons: for single by-value inputs, such as the  
 * common case of count(distinct id), the tuplesort_getdatum code path  
 * is around 300% faster.  (The speedup for by-reference types is less  
 * but still noticeable.)  
 *  
 * This function handles only one grouping set (already set in  
 * aggstate->current_set).  
 *  
 * When called, CurrentMemoryContext should be the per-query context.  
 */  
static void  
process_ordered_aggregate_single(AggState *aggstate,  
                                                                 AggStatePerTrans pertrans,  
                                                                 AggStatePerGroup pergroupstate)  
{  
/*  
 * Run the transition function for a DISTINCT or ORDER BY aggregate  
 * with more than one input.  This is called after we have completed  
 * entering all the input values into the sort object.  We complete the  
 * sort, read out the values in sorted order, and run the transition  
 * function on each value (applying DISTINCT if appropriate).  
 *  
 * This function handles only one grouping set (already set in  
 * aggstate->current_set).  
 *  
 * When called, CurrentMemoryContext should be the per-query context.  
 */  
static void  
process_ordered_aggregate_multi(AggState *aggstate,  
                                                                AggStatePerTrans pertrans,  
                                                                AggStatePerGroup pergroupstate)  
{  
```  
执行计划如下，排序后，走GroupAggregate的计划。  
```  
postgres=#  explain (verbose,summary) select c1,c2,count(distinct c3),count(distinct c4),count(distinct (c3,c4)) from tbl group by c1,c2;  
                                      QUERY PLAN                                         
---------------------------------------------------------------------------------------  
 GroupAggregate  (cost=1407453.56..1496253.56 rows=555000 width=32)  
   Output: c1, c2, count(DISTINCT c3), count(DISTINCT c4), count(DISTINCT ROW(c3, c4))  
   Group Key: tbl.c1, tbl.c2  
   ->  Sort  (cost=1407453.56..1421328.56 rows=5550000 width=16)  
         Output: c1, c2, c3, c4  
         Sort Key: tbl.c1, tbl.c2  
         ->  Seq Scan on public.tbl  (cost=0.00..596041.00 rows=5550000 width=16)  
               Output: c1, c2, c3, c4  
 Planning time: 0.110 ms  
(9 rows)  
```  
如果要让PostgreSQL求distinct走hashAgg，需要换SQL写法，后面提到。  
## Greenplum distinct 语句的PLAN  
Greenplum则同时支持hashAgg和groupAgg求distinct。  
1、hashagg  
```  
postgres=# explain analyze select c1,c2,count(distinct c3) from tbl group by c1,c2;  
                                                                             QUERY PLAN                                                                               
--------------------------------------------------------------------------------------------------------------------------------------------------------------------  
 Gather Motion 48:1  (slice2; segments: 48)  (cost=2748912.00..2761424.50 rows=1001000 width=16)  
   Rows out:  1002001 rows at destination with 5071 ms to end, start offset by 1.784 ms.  
   ->  HashAggregate  (cost=2748912.00..2761424.50 rows=20855 width=16)  
         Group By: partial_aggregation.c1, partial_aggregation.c2  
         Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.004 ms to first row, 223 ms to end, start offset by 4.338 ms.  
         ->  HashAggregate  (cost=2448912.00..2573912.00 rows=208334 width=12)  
               Group By: tbl.c1, tbl.c2, tbl.c3  
               Rows out:  Avg 1320761.3 rows x 48 workers.  Max 1323529 rows (seg9) with 0.002 ms to first row, 3120 ms to end, start offset by 4.491 ms.  
               ->  Redistribute Motion 48:48  (slice1; segments: 48)  (cost=2048912.00..2248912.00 rows=208334 width=12)  
                     Hash Key: tbl.c1, tbl.c2  
                     Rows out:  Avg 2061921.2 rows x 48 workers at destination.  Max 2066345 rows (seg31) with 1229 ms to end, start offset by 59 ms.  
                     ->  HashAggregate  (cost=2048912.00..2048912.00 rows=208334 width=12)  
                           Group By: tbl.c1, tbl.c2, tbl.c3  
                           Rows out:  Avg 2061921.2 rows x 48 workers.  Max 2062196 rows (seg24) with 0.006 ms to first row, 1706 ms to end, start offset by 59 ms.  
                           ->  Append-only Columnar Scan on tbl  (cost=0.00..1048912.00 rows=2083334 width=12)  
                                 Rows out:  0 rows (seg0) with 39 ms to end, start offset by 56 ms.  
 Slice statistics:  
   (slice0)    Executor memory: 359K bytes.  
   (slice1)    Executor memory: 1053K bytes avg x 48 workers, 1053K bytes max (seg0).  
   (slice2)    Executor memory: 396K bytes avg x 48 workers, 396K bytes max (seg0).  
 Statement statistics:  
   Memory used: 128000K bytes  
 Settings:  enable_bitmapscan=off; enable_seqscan=off; optimizer=off  
 Optimizer status: legacy query optimizer  
 Total runtime: 5106.665 ms  
(25 rows)  
```  
2、groupagg  