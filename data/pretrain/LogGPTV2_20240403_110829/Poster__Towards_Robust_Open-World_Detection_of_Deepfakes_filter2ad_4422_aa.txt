title:Poster: Towards Robust Open-World Detection of Deepfakes
author:Saniat Javid Sohrawardi and
Akash Chintha and
Bao Thai and
Sovantharith Seng and
Andrea Hickerson and
Raymond W. Ptucha and
Matthew Wright
Poster: Towards Robust Open-World Detection of Deepfakes
Saniat Javid Sohrawardi∗
Rochester Institute of Technology
Akash Chintha
Rochester Institute of Technology
Bao Thai
Rochester Institute of Technology
Sovantharith Seng
Rochester Institute of Technology
Andrea Hickerson
University of South Carolina
Raymond Ptucha
Rochester Institute of Technology
Matthew Wright
Rochester Institute of Technology
ABSTRACT
There is heightened concern over deliberately inaccurate news.
Recently, so-called deepfake videos and images that are modified by
or generated by artificial intelligence techniques have become more
realistic and easier to create. These techniques could be used to
create fake announcements from public figures or videos of events
that did not happen, misleading mass audiences in dangerous ways.
Although some recent research has examined accurate detection
of deepfakes, those methodologies do not generalize well to real-
world scenarios and are not available to the public in a usable
form. In this project, we propose a system that will robustly and
efficiently enable users to determine whether or not a video posted
online is a deepfake. We approach the problem from the journalists’
perspective and work towards developing a tool to fit seamlessly
into their workflow. Results demonstrate accurate detection on both
within and mismatched datasets.
KEYWORDS
Deepfake Detection; Deep Learning; Usable Security
ACM Reference Format:
Saniat Javid Sohrawardi, Akash Chintha, Bao Thai, Sovantharith Seng,
Andrea Hickerson, Raymond Ptucha, and Matthew Wright. 2019. Poster:
Towards Robust Open-World Detection of Deepfakes. In 2019 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’19), November
11–15, 2019, London, United Kingdom. ACM, New York, NY, USA, 3 pages.
https://doi.org/10.1145/3319535.3363269
1 INTRODUCTION
Deepfakes are artificially generated audio or visual renderings, most
commonly videos. These videos, which are typically done without
consent of depicted individuals, can be used to defame a public
figure or influence public opinion. Not so long ago, an audio or
video recording could be used as irrefutable evidence in a court of
law. With the recent discovery of generative adversarial networks
(GANs) [7], an attacker using a normal desktop computer fitted
∗Email at: PI:EMAIL
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363269
with an off-the-shelf graphics processing unit can make renditions
so good, they can easily fool humans and machines alike.
In this work, we propose development of a tool for detecting
deepfakes designed to be usable by journalists in real-world condi-
tions. We study the use of multiple detection methods that could
be incorporated together into such a tool and discuss findings from
interviews with journalists about the tool’s design.
Image manipulation has been around
Generating Deepfakes.
for decades, but it long time consuming and expensive. Recent
advancements in deep convolutional neural networks (CNNs), how-
ever, makes it readily accessible to artificially generate falsified
visual renderings. The videos generated with this technology are
generally referred to as deepfakes, and there are several such meth-
ods. The graphics-based Face-swap method [11] is a fast but lower-
quality method that can be seen used on social media apps like
Snapchat. Deepfakes, based on GANs [7], yield more convincing
results by placing faces into the frames learned from source videos
or a collection of photos. The method is publicly available as both
the FakeApp and through the Faceswap Github [15]. Face2Face [21]
involves face re-enactment, where the algorithm transfers facial
expressions from source to target frames. Videos based on these
techniques can be merged with separately faked sound data to
create complete falsified content. Algorithms can generate speech
that sounds like a target speaker based on text or utterances from
another speaker, both with convincing results [14].
Detecting Deepfakes. Over the past couple of years, researchers
have studied the detection of deepfakes. Early work looked into
visual inconsistencies within the frames. Some approaches looked
into biological signals [4, 13], while others let CNNs handle the
feature extraction [1, 8, 20]. Nguyen et al. [17] proposed to use cap-
sule networks with dynamic routing and attain very good results.
A few other approaches achieved success in localizing the manip-
ulated areas (typically the face) [5, 16, 18]. Since a major concern
about deepfakes is their potential for spreading misinformation that
appears to come from world leaders, Agarwal et al. [2] proposed
to track facial landmarks in order to learn behaviors common to
specific people and use them to discern between real and fake video
material. In audio detection, the ASVSpoof challenges have led to
interesting results [10], though more focused on detection of people
attempting to bypass voice biometrics.
While recent methods achieve very good accuracy, they do not
form a unified solution that is robust to different types of video
manipulations, and therefore they are not reliable for detection of
deepfakes in the wild.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom26132 EXPERIMENTAL SETTING
2.1 Datasets
We used the FaceForensics++ dataset [18] that consists of 1,000
unmodified source videos, together with three sets of manipulated
videos: Deepfake [15], Face2Face [21], and FaceSwap [12]). The
training, validation and test splits were generated using the map-
ping provided by Rössler et al. [18] in their Github repository, with
720, 140 and 140 videos respectively. Facial extraction was per-
formed on the dataset using dlib 1 to extract the faces using three
times the interocular distance for the bounding box.
For fake audio detection, we used the ASVSpoof2019 logical
access dataset [22], consisting of speech data from 107 speakers.
While the ASVSpoof2019 dataset consists of logical access attack as
well as physical access attack, we decided to focus only on the logi-
cal access attack cases since it is closer to what a deepfake generator
would do. The dataset is partitioned into training, development, and
evaluation datasets with 20, 10, and 48 speakers, respectively. The
training and development sets consist of similar spoofing methods,
while the evaluation dataset consists of methods not used in the
training and development sets.
2.2 Model Selection
2.2.1 Video Model. We chose to work with the ClassNSeg model
by Nguyen et al. [16], as their multi-task learning model was shown
to be robust to different types of deepfakes after fine-tuning with a
small amount of data.
Additionally, we constructed our own FacenetLSTM model, which
targets temporal inconsistencies in the videos. A common feature
among all the deepfake generation mechanisms to date is that each
frame is treated as an individual image. Hence, we believe that a
sequence-to-sequence model could make use of frame-to-frame
inconsistencies along with frame-level facial artifacts to better de-
tect fakes. Our model is inspired by an RNN-based deepfake de-
tection model [8]. However, we use time-distributed FaceNet [19]
pre-trained on VGGFace2 [3], followed by a unidirectional LSTM
layer to capture the temporal information.
2.2.2 Audio Model. For fake audio detection, we explored two
different architectures. Our first model, ResNeXTSpoof, passes the
input through multiple convolution blocks of different sizes. The
output of the convolutions is summed together before being added
to the original input, similar to the skip connections first used in
ResNet [9]. By using multiple filter sizes, we seek to detect artifacts
at different frequency bands. The second model for fake audio
detection uses a combination of convolution layers and recurrent
layers, as inspired by Guerra and Delp [8]. Several convolutional
layers are used to downsample the input audio before the feature
maps are passed into a bidirectional LSTM. After all the time steps
have been processed, the hidden state of the LSTM is used as feature
to a fully connected layer that performs the binary classification.
3 EVALUATION
For deepfake video detection, we trained the models on the three
different datasets separately and tested on their corresponding
test sets to get matched accuracy results, as well as training on
1http://dlib.net/
)
%
(
y
c