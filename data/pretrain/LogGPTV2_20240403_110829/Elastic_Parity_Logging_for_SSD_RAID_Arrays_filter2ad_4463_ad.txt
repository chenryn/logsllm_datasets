few key properties. First, their average write sizes are generally
small (7-13KB). Second, if we examine the access pattern,
we see that all traces have a high proportion of random write
requests. Here, by a random write request, we mean that a write
request whose starting offset differs from the ending offset of
the last write request by at least 64KB. Finally, if we examine
the working set size (i.e., the size of unique data accessed
throughout the trace duration), all traces have small working
set sizes.
56
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
 80
)
B
G
 60
(
e
z
i
s
e
t
i
r
 40
 20
MD
PL
EPLog
W
 0
FIN WEB
USR MDS
(a) Different traces under (6+2)-RAID-6
MD
PL
EPLog
 100
 80
 60
 40
 20
 0
)
B
G
(
e
z
i
s
e
t
i
r
W
RAID-5
(4+1)
RAID-6
(6+2)
(b) Different RAID settings under FIN
RAID-5
(6+1)
RAID-6
(4+2)
 30
)
K
(
s
t
s
e
u
q
e
r
f
o
.
o
N
 20
 10
MD
PL
EPLog
 0
FIN WEB
USR MDS
(a) Different traces under (6+2)-RAID-6
)
K
(
s
t
s
e
u
q
e
r
f
o
.
o
N
 60
 50
 40
 30
 20
 10
 0
MD
PL
EPLog
RAID-5
(4+1)
RAID-6
(6+2)
(b) Different RAID settings under FIN
RAID-5
(6+1)
RAID-6
(4+2)
Fig. 7: Experiment 1: Total size of write trafﬁc to SSDs.
B. Results
Experiment 1 (Write trafﬁc to SSDs): We ﬁrst show the
effectiveness of EPLOG in reducing write trafﬁc to SSDs due
to parity updates, given that our traces are dominated by small
random writes. Figure 7(a) shows the total size of write trafﬁc
to SSDs across different traces under (6+2)-RAID-6. Overall,
EPLOG achieves a 45.6-54.9% reduction in write size when
compared to MD. Both PL and EPLOG have the same results,
since they write the same amount of data updates to SSDs (see
Figure 1), while redirecting parity trafﬁc to log devices. We
emphasize that even though EPLOG has the same write trafﬁc
to SSDs with PL, it achieves much higher I/O throughput
than PL due to elastic parity logging (see Experiment 5).
Figure 7(b) shows the reduction of write trafﬁc to SSDs across
four different RAID settings under the FIN trace (which has
the most write requests among all traces). EPLOG reduces
38.6-39.9% and 49.3-57.0% of write trafﬁc over MD for RAID-
5 and RAID-6, respectively. Note that RAID-6 shows more
signiﬁcant reduction of write trafﬁc than RAID-5.
Experiment 2 (GC overhead): We study the endurance in
terms of GC overhead, which we measure by the average
number of GC requests to each SSD. Since SSD controllers
do not expose GC information, we resort
to trace-driven
simulations using Microsoft’s SSD simulator [2] that builds on
Disksim [5]. For the simulator, we conﬁgure each SSD with
20GB raw capacity and 16,384 blocks with 64 4KB pages each
(i.e., 256KB per block). Also, based on the default simulator
settings, each SSD over-provisions 15% of blocks for GC (i.e.,
the effective capacity of each SSD is 17GB) and triggers GC
when the number of clean blocks drops below 5%. We use
the default greedy algorithm in the simulator and disable the
wear-leveling block migration.
We replay the workloads and use the blktrace utility
Fig. 8: Experiment 2: GC overhead, measured in the average
number of GC requests to each SSD. Note that EPLOG triggers
no GC under MDS and (6+2)-RAID-6, since it reduces the
amount of writes to each SSD and does not cause the number
of clean blocks to drop below the threshold.
to capture block-level I/O requests for each SSD in the
background. Then we feed the block I/O requests into the
simulator. We measure the total number of GC requests per
SSD, and take an average over the results across all SSDs in
the array.
Figure 8 plots the total number of GC requests per SSD,
averaged over all SSDs. Figure 8(a) shows the results across
different
traces under (6+2)-RAID-6. EPLOG signiﬁcantly
reduces the number of GC requests over MD, for example, by
77.1% under the FIN trace. This implies EPLOG signiﬁcantly
improves endurance. We also note that EPLOG reduces at least
8.1% of GC requests over PL in all traces. The reason is that
EPLOG updates data chunks by using the no-overwrite updat-
ing policy, which reserves part of the logical address space for
data updates. Thus, EPLOG introduces higher sequentiality for
writes to SSDs. Also, Figure 8(b) shows that EPLOG reduces
59.6-77.1% of GC requests over MD across different RAID
settings under the FIN trace.
Experiment 3 (Impact of caching): We now evaluate the
impact of caching of EPLOG. Since we focus on updates,
we do not consider the effect of the stripe buffer (which is
designed for new writes). Instead, we evaluate the impact of
the device buffers. We vary the size of device buffer of each
SSD from zero to 64 chunks. We measure both the total size
of write trafﬁc to SSDs and the total size of log chunks in the
log devices.
Figure 9 shows the results for different traces under (6+2)-
RAID-6. From Figure 9(a),
the total size of write trafﬁc
to SSDs decreases as the device buffer size increases. For
57
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
 40
 30
 20
 10
 0
)
B
G
(
e
z
i
s
e
t
i
r
W
0
FIN
WEB
USR
MDS
1
2
4
Device buffer size (chunks)
(a) Total size of write trafﬁc to SSDs
8
16
32
)
B
G
(
e
z
i
S
g
o
L
 20
 15
 10
 5
 0
0
1
FIN
WEB
USR
MDS
2
4
8
16
32
Device buffer size (chunks)
64
64
(b) Total size of log chunks
Fig. 9: Experiment 3: Impact of different device buffer sizes
under (6+2)-RAID-6.
example, when the device buffer size reaches 64 chunks (i.e.,
256KB per device), the write size drops by 53.3-58.4%. From
Figure 9(b), the total size of log chunks drops even more
signiﬁcantly. For example, when the device buffer size reaches
64 chunks, the total size of log chunks decreases by 84.7-
91.1%. Note that the total cache size of EPLOG is very small.
For example, if we set the device buffer size per SSD as 64
chunks of size 4KB each, we only need 2MB. This implies that
a small-sized cache can effectively absorb the data updates, and
hence reduce both the write trafﬁc to SSDs and the storage of
log chunks.
Experiment 4 (Parity commit overhead): Parity commit
introduces additional writes (see Section III-C). We study the
impact of parity commit. In particular, we consider three cases
of parity commit: (i) without any parity commit, (ii) commit
only at the end of the entire trace, and (iii) commit every
1,000 write requests. We also include the results of MD from
Experiment 1 for comparison.
Figure 10 shows the parity commit overhead for different
traces under (6+2)-RAID-6. Figure 10(a) ﬁrst shows the total
size of write trafﬁc to SSDs (as in Experiment 1). Compared
to the case without any parity commit, the write size increases
by up to 4.3% and 24.9% when we perform parity commit at
the end of a trace and every 1,000 write requests, respectively.
The write size with parity commit is still less than MD (e.g.,
by over 40% in some cases). Figure 10(b) plots the average
number of GC requests to each SSD (as in Experiment 2).
The number of GC requests of EPLOG is 74.8-97.1% and
67.8-88.2% less than that of MD when we perform parity
commit at the end of a trace and every 1,000 write requests,
respectively. The results show that the parity commit overhead
remains limited if we perform parity commit in groups of
writes.
58
)
B
G
(
e
z
i
s
e
t
i
r
W
 60
 50
 40
 30
 20
 10
 0
)
K
(
s
t
s
e
u
q
e
r
f
o
.
o
N
 30
 25
 20
 15
 10
 5
 0
MD
EPLog (1K)
EPLog (all)
EPLog (nil)
FIN WEB
USR MDS
(a) Total size of write trafﬁc to SSDs
MD
EPLog (1K)
EPLog (all)
EPLog (nil)
FIN WEB
USR MDS