tion ratios: (1) The attacker is aware and consciously optimize based
on amplification ratio, in which case we should see a selection bias
towards high amplification machines, or (2) the attacker does not
track this in which case we should see randomly sampling from the
set of high and low amplification servers. Figure 7 shows the num-
ber of high and low amplification servers utilized in each attack,
where each dot is an attack and the color indicates the protocol used.
We configured an equal amount of machines to act as a high or low
amplification server by letting them answer requests with a differ-
ent response size. Suppose an adversary is not making any active se-
lection. In that case, the same number of high and low amplification
honeypots should statistically be picked, and the dot thus falls on the
45-degree line or in the blue shaded area for sampling from a hyper-
geometric distribution at 95%, 99%, and 99.9% confidence intervals.
As we see in the figure, there is active selection for “good” ampli-
fication servers in many attacks. These practices are differently pro-
nounced depending on the protocol used in the attack. For exam-
ple, RIP is never located outside the 95% confidence interval of non-
selective behavior, while attacks using NTP and DNS are seldom lo-
cated inside this interval. We see significantly different levels of het-
erogeneity for other protocols depending on the attack campaigns,
with some actors randomly picking servers and more sophisticated
attackers concentrating on high amplification servers only. It is in-
teresting to note that no attacks use a disproportionate amount of
Figure 7: Selection of low and high amplification servers in
attacks, showing a surprisingly little amount attackers ac-
tively select servers with higher amplification.
Figure 8: Attacks are largely indifferent about whether a ser-
vice is real as long as it provides amplification.
low amplification servers, confirming the hypotheses that attack-
ers either randomly sample or select higher amplification servers.
Adversaries generally do not care whether a system is a hon-
eypot. To investigate whether attackers only collect machines in
an automated fashion or apply some level of post-processing or
vetting of the discovered services, we allocated a new set of IPs
to honeypots that would, as part of the response, clearly identify
themselves as honeypots that are rate-limited. While this would
require a human to look at the responses, we also made these hon-
eypots behave non-protocol compliant, which meant that machine-
based post-processing (if being done) should weed them out. To
rule out a potential confounding based on the amplification as dis-
cussed in the previous section, the amplification ratio was identical
to those of our real servers. Servers running fake responders were
exclusively running fake services to prevent confounding.
Figure 8 shows the distribution of attacks between real and fake
servers in an identical setup as figure 7, and we see that the bulk of
the attackers is indifferent about the responses of the system. Only
Session 3D: DoS CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea947in DNS and NTP do we observe a handful of attacks being con-
ducted without any fake services. However, in these rare situations,
the adversaries did not show sensitivity to the honeypot identifica-
tion string but expected a particular response to their query and
would drop deviating servers from the list. Contrary to what one
might think, adversaries do not make any effort to check the plau-
sibility of servers, neither manually nor by scripts. Curiously, we
also observed a series of NTP attacks that were exclusively using
fake systems, the result of an adversary using an incorrect Mon-
list request packet that would be dropped by a regular NTP server
implementation as an incorrect query.
Implausible amplifier setups do not matter to any adversary.
Krämer et al. hypothesize that hosting multiple services on the
same IP address might influence the behavior of an attacker when
interacting with the device [17]. To establish whether such a dif-
ference exists, our honeypot system deployed 120 servers (24 per
protocol) that are solely running one service. In contrast, the rest of
the honeypots run all protocols in parallel, which is an implausible
setup on a regular server. We do not find any statistically signifi-
cant differences between those groups. Although anecdotal, we ob-
serve adversaries performing multi-vector attacks gladly using all
services located at the same honeypot for their attack, even though
in practice, the shared uplink would limit traffic at the amplifier.
5.5 Testing
After locating candidate systems for abuse, an adversary might
opt to test amplifiers whether they are heavily rate-limited or drop
packets and thus not useful in an attack. Actors would not observe
these behaviors when scanning using a single packet but need to test
hosts actively. Additionally, as services might be moved to other IPs
or be shut down, it would make sense for adversaries to frequently
test if the amplifiers are still online to not waste reflection potential
by sending packets to a server that does not respond.
Only a few attackers test whether a system is rate-limited
before performing an attack. Evidently, operators do not want
their servers being abused in performing attacks, as this unneces-
sarily drains one’s resources, and the victim might blame the ampli-
fier for the packet flood. Operators would therefore minimize the
risk for attacks, for example, by establishing rate limits on requests
towards a service. To establish the extent to which adversaries ac-
tively test servers before using them in an attack, our honeypot
system contains 60 servers per protocol that are actively dropping
one-third, one-half, or two-thirds of all outgoing packets. While all
servers will amplify incoming traffic when an attacker crafts the
correct packet, it is clear that the amplification potential decreases
dramatically when two-thirds of the packets are dropped. To ensure
these servers will be found by a scan and not bias the setup, the
first packet in a connection will always receive a reply. With this
setup, only attackers who have probed the system with more than
a single scan will know these differences. Figure 9 shows the distri-
bution of attacks across these services and the probability of these
server choices occurring at random using the confidence intervals
explained in the previous section. The image shows that overall
packet loss seems of no concern to adversaries. Only in SSDP do we
see attackers making an effort to steer clear of servers with packet
Figure 9: Usage of servers with packet loss in attacks, active
selection only occurs in case of SSDP.
loss. As there is no significant deviation in how many times servers
were scanned, these choices have to be made deliberately.
While many attacks using SSDP use a disproportionate amount
of loss-free servers, we would expect adversaries to completely
steer away from them when having the choice between different
servers that perform better. However, almost no attacks are void
of any lossy servers, raising the question of how adversaries make
this selection. As the probability that packet loss will occur when
contacting a host is in our case 1 − (1 − droprate)k−1 where k is
the number of packets sent during one connection, some scans
will not be enough to establish that some of our hosts never reply
to a portion of the scans coming in. Assuming a scan rate of 5
packets per host, which we observe in 9% of scanning traffic, an
adversary will have a 20% chance that packet loss is not observed
when we drop 1
3 of the packets, making the packet loss invisible
to the adversary. Let us only consider the servers with 2
3 packet
loss, which would be visible 99% of the time when sending five
packets towards this honeypot. We indeed find adversaries testing
the infrastructure with 150 attacks in NTP and 96 attacks in SSDP
that do not use any of these servers.
Adversaries do not care about server latency. Aside from drop-
ping packets, also considerable packet delays might pose an obsta-
cle for adversaries when launching attacks. We placed 60 servers
that delayed the response by an extra delay of 500 ms to investigate
a potential selection strategy for this. With regular service times
below 1 ms and worst-case round-trip network delay from Europe
to Asia being in the order of 300 ms, this delay should significantly
stick out. A significant delay in sending responses could indicate a
highly loaded or overloaded system, making it less attractive for
an adversary as it might get overwhelmed during the attack. We
do, however, not find any statistically significant differences that
adversaries prefer non-delaying servers over these servers.
Sophisticated adversaries do not blindly trust their amplifier
lists. Over time, results of the scans performed to find open am-
plifiers might not be accurate anymore due to the reconfiguration
of servers or even the decommissioning of the machines. To not
waste effort on non-responsive machines, an adversary should re-
peat scans and curate the list of used servers. To investigate the ex-
tent to which adversaries rescan the hosts to verify their status, we
continued to collect all network traffic towards our honeypots after
we concluded the active experimentation. In this passive phase, all
traffic was sink-holed, and no replies were sent out anymore. Figure
Session 3D: DoS CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea948Figure 10: Incoming attacks during active replies and subse-
quent shutdown on September 27th.
10 shows the attack rate while we actively responded to requests
and the rate after the shutdown of the amplifiers. In the baseline pe-
riod before the amplifiers actively respond to requests, no attacks
were measured. While we verified during the baseline establish-
ment that the IPs were unknown to attackers and received no attack
requests, reverting to this state and nullifying their use in amplifi-
cation attacks did not cause the number of attacks conducted using
the servers to go down drastically. On the contrary, attacks contin-
ued at roughly the same pace for all protocols for more than two
weeks. Even after two months, attacks were still conducted using
our infrastructure, which had been disabled for longer than it was
ever enabled. We believe there can be two explanations for this be-
havior: (1) The attacks conducted with our servers originates from
the same actors who have created lists of vulnerable servers and do
not update these frequently, or (2) our servers have been listed in
publicly available lists of amplification servers, and the people con-
ducting attacks blindly trust these lists to be accurate and up to date.
While we have not been able to find any notion of our servers be-
ing on amplifier lists, these lists might be shared inside closed com-
munities. While many attacks are conducted after the system does
not respond anymore, no large attacks are being conducted using
the amplification servers. Additionally, after the servers are taken
offline, the attacks conducted are only rarely using solely high am-
plification servers. The adversaries that are capable of performing
large and lengthy DDoS attacks are thus updating their lists.
5.6 Execution
While a DDoS attack is a simple concept in which a victim’s re-
sources are drained, the ways in which these attacks are executed
are diverse in practice. In this section, we will show various attack
techniques used by adversaries captured in our honeypots.
Attack durations and modes differ between amplification
protocols and targets. Amplification attacks sent to our honey-
pots had an average duration of 394 seconds, almost a factor of 5
longer than for TCP SYN floods, the other major type of DDoS at-
tack vector [4, 11, 13]. The duration of attacks observed in our work
falls between observed median durations in related work, which re-
port a median of 658 [32] and 255 seconds [13]. Figure 11 shows how
the durations even significantly differ per protocol, with QOTD hav-
ing a median attack duration three times as high as NTP. In QOTD,
Figure 11: CDF for the duration of attacks per protocol.
Table 6: Source port usage of traffic coming into our system.
Protocol
CharGen
DNS
NTP
QOTD
RIP
SSDP
Attacks
471
55
11,130
264
184
1375
1. Single (%)
1
11
2
6
2
30
2. Static (%)
95
35
47
94
98
68
3. Rand (%)
4
55
51
0
1
1
we also see modes at 400 and 500 seconds that are not present in
other attacks. The mode at 300 seconds is visible in QOTD, Char-
Gen, NTP, and RIP but does not show in SSDP. The presence of at-
tack durations and modes has earlier been identified by [4] for TCP
SYN flooding attacks in 2017, where typical durations of 30 and 60
seconds were traced back to test attacks by booter services. For am-
plification DDoS attacks, these modes have been identified by [32].
In our study, we find that both the attacks take significantly longer
and common modes are significantly higher opposed to TCP SYN
flooding attacks, but both are lower compared to [32].
Given the differences between protocols and the way attacks are
run, it is natural to wonder whether these are used for different
purposes. When we look at attacks on two different types of services,
those on game hosts and web servers based on their open ports
using Shodan, we find that the type of target influences the used
attack vector and the duration of the attack. The attacks on game
servers are solely conducted with NTP and SSDP, whereas attacks
across all protocols attack web hosts. In terms of duration, game
hosting servers are attacked more rigorously than web servers,
with an average attack duration of 12 minutes. In contrast, web
servers only deal with attacks that, on average, last a bit more than
5 minutes. Between the two groups, there is no difference in the
selection of servers to be used in the attacks.
Attacked ports cannot be used in the protection against at-
tacks - except when attackers are targeting TCP ports in-
stead of UDP. Various controls exist to filter DDoS attacks [27].
One of these techniques is to filter based on ports. Like most other
services, our honeypots run their services on default UDP ports,
which would not provide an angle for the victim to filter out the
amplified data. On the other hand, the attacker’s spoofed requests
need to originate from a source port, to which (at the victim’s IP)
the response will be sent. We distinguish three cases for the ports
used to request data: (1) All requests towards honeypots are made
from one port, indicating a crafted, injected packet. (2) The requests
are made using different ports per honeypot. (3) For every request,
Session 3D: DoS CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea949Figure 12: A pulsing attack using 40 honeypots.
the attacker generates a random port number, evenly distributing
the traffic over all ports. All three cases show distinct implemen-
tation choices an attacker can make and could avoid packet-based
filters that are placed as a countermeasure. Table 6 shows the dis-
tribution of source port usage in attacks, and we see a large part is
implementing the frames using a static port per honeypot. While
randomizing source ports is only slightly more complicated in im-
plementation, most adversaries do not seem to choose this route,
even though it can be more robust against packet level filtering
[27]. Only in NTP and DNS a part of attacks is performed while
randomizing ports across the entire port range.
This however raises another important operational aspect. An
amplification attack towards a single port could be trivially blocked
by packet filtering unless the filtering happens on the victim’s
premises. The flood is sufficient to congest the connection from
the victim to the Internet. However, if the attack targets a service
already running on the victim, the attack might be hard to distin-
guish from legitimate traffic as found by [13], who identify that
most of the attacks are aimed at ports connected to online gaming.
We do not find many attacks targeted at gaming ports and instead
find that adversaries fail to take into account that protocols such
as HTTP would run on TCP port 80 or 443 and not UDP port 80,
which would not be whitelisted in a firewall.1 Thus, the deliberate
targeting of select ports would not have any more benefit than a
packet flood towards a random destination port. From the 650 sin-
gle port attacks, 364 send their attack towards port 80 on the victim,
while another 20 target port 443. The only attacks aimed at a UDP
service were 16 attacks on DNS port 53.
Attack pulses maximize the attack efficiency while minimiz-
ing the cost for the attacker. After a packet flood has disrupted
a service, it might take a moment after the end of the attack for
services to recover and clients to reconnect. This means that even
after the flood itself has stopped, the effects might still be ongoing