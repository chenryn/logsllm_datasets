17,054
ICMP
14,794
TCP
Passive
25,706
656
ICMP only
1,081
TCP only
Passive only
7,720
any active
100%
72%
62%
54%
93%
100%
86%
74%
Table 3: Comparison of ICMP, Nmap, and passive observa-
tion of address utilization at USC.
and SSH, taken every 12 hours. For TCP probes, Nmap
regards both SYN-ACK and RST responses as indication of
host presence. Passive monitoring observes nearly all net-
work traﬃc between our target network and its upstream,
commercial peers.
It declares an IP address active when
it appears as the source address in any UDP packet or a
non-SYN TCP packet. We checked for IP addresses that
generate only TCP SYNs on the assumption that they are
spoofed source addresses from SYN-ﬂood attacks; we found
none.
Table 3 quantiﬁes detection completeness, normalized to
detection by any method (the union of passive and active
methods, middle column), and detection by any form of
active probing (right column). We also show hosts found
uniquely be each method in the last rows (ICMP, TCP, and
passive only). Detection by any means (the union of the
three methods) represents the best available ground truth
(USC does not maintain a central list of used addresses),
but passive methods are not applicable to the general Inter-
net, so the right column represents best-possible practical
wide-area results as we use in the next section.
First, we consider the absolute accuracy of each approach.
When we compare to ground truth as deﬁned by all three
methods, we see that active methods signiﬁcantly under-
count active IP addresses, with TCP missing 46% and ICMP
missing 38%. While this result conﬁrms that ﬁrewalls sig-
niﬁcantly reduce the eﬀectiveness of active probing, it shows
that active probing can ﬁnd the majority of used addresses.
Second, we can compare the relative accuracy of ICMP
and TCP as types of active probing. We see that ICMP is
noticeably more eﬀective than TCP-based probing. While
some administrators apparently regard ICMP as a security
threat, others recognize its value as a debugging tool.
Our experiment used diﬀerent probe frequencies for ICMP
and TCP. This choice was forced because Nmap is much
slower than our optimized ICMP prober. However, when
we correct for this diﬀerence by selecting only ICMP surveys
every 12 hours, ICMP coverage only falls slightly, to 59% of
any responders, or 84% of active responders. We therefore
conclude that coverage is dominated by the type of probing,
not probe frequency.
3.2.2 Evaluation from a Random Internet Sample
Our USC dataset provides a well-deﬁned ground truth,
but it may be biased by local or academic-speciﬁc policies.
To remove possible bias we next consider a survey of a ran-
dom sample of one million allocated Internet addresses taken
category:
addresses probed
non-responding
responding either
ICMP
TCP
both ICMP and TCP
ICMP only
TCP only
1,000,000
945,703
54,297
40,033
34,182
19,918
20,115
14,264
active
100%
74%
62%
Table 4: ICMP-TCP comparison for random Internet ad-
dresses.
in October, 2007. Details of the methodology (omitted here
due to space constraints) are in our technical report [18].
Brieﬂy, we compare one-shot TCP SYN probes to port 80
to ICMP probes. (Absence of public, unanonymized traces
leave additional wide-area evaluation as future work.)
Table 4 shows the results of this experiment. If we deﬁne
addresses that respond to either ICMP or TCP as ground
truth of visible address usage, we can then evaluate accuracy
of detection of active addresses relative to this ground truth.
These results show that traﬃc ﬁltering is more widespread
in the Internet than at USC, since both ICMP and TCP re-
sponse rates are lower (74% and 62% compared to 86% and
74% when we use the same baseline). This experiment con-
ﬁrms, however, that qualitatively, ICMP is more accurate
than TCP-based probing, ﬁnding 74% of active addresses,
11% closer to our baseline. We conclude that both ICMP
and TCP port 80 are ﬁltered by ﬁrewalls, but ICMP is less
likely to be ﬁltered.
3.2.3
Implications on Estimates
We draw several conclusions from these validation exper-
iments. First, they show that active probing considerably
underestimates Internet utilization—single protocol active
probing misses about one-third to one-half of all active ad-
dresses from our USC experiment. When we consider visi-
ble addresses (those that will respond to some type of ac-
tive probe), single-protocol active probing underestimates
by one-third to one-sixth of hosts from both experiments.
Our results suggest that, while hosts block one proto-
col or the other, multi-protocol probing can discover more
active addresses than single protocol probing. The exper-
iments also show that ICMP-only probing is consistently
more accurate than TCP-only probing. Our operational ex-
perience is that TCP probing elicits 30 times more abuse
complaints than ICMP. Since the resulting “please-do-not-
probe” blacklists would skew results, we believe ICMP is
justiﬁed as the best feasible instrument for wide-area active
probing.
Finally, we would like to estimate a correction factor to
account for our count underestimate due to ﬁrewalls. Since
ICMP-nmapsurvey
USC provides the best ground truth, includ-
ing passive observations that are not aﬀected by ﬁrewalls,
we claim our ICMP estimates are 38% low. A factor 1.61
would therefore scale the ICMP-responsive count to estimate
Internet accessible computers (Figure 1), if one accepts USC
as representative. If one assumes USC is more open than the
Internet as whole, this scaling factor will underestimate.
Alternatively, we can derive a less biased estimate of the
visible Internet (a subset of Internet-accessible computers
 1
 0.8
)
k
c
o
b
(
A
l
-
e
1
1
T
I
 0.6
 0.4
 0.2
 100000
 10000
 1000
 100
 10
 1
 0
 0
 0.2
 0.6
 0.4
IT11w - A(block)
 0.8
 1
Figure 3: Subnets’ A values from two censuses taken widely
diﬀerent network locations: IT 11w and IT 11e .
shown in Figure 1). Our random sample suggests that ICMP
misses 26% of TCP responsive hosts, so visible computers
should be 1.35× the number of ICMP-responsive hosts. As a
second step, we then scale from visible to Internet-accessible
addresses by comparing TCP or ICMP to responding any
from ICMP-nmapsurvey
above, this estimate is likely low, and as future work we
hope to improve it.) Together, these suggest an alternative
multiplier of 1.86 to get Internet-accessible computers.
3.3 Measurement Location
USC , a factor of 1.38×. (As described
Measurement location is an additional possible source of
bias.
It may be that some locations may provide a poor
view of parts of the Internet, perhaps due to consistently
congested links or incomplete routing.
To rule out this source of potential bias, censuses since
March 2006 have been done in pairs from two diﬀerent lo-
cations in Los Angeles and Arlington, Virginia. These sites
have completely diﬀerent network connectivity and Internet
service providers. We use diﬀerent seeds at each site so probe
order varies, but the censuses are started concurrently.
Figure 3 compares the A(block ) values measured concur-
rently from each vantage point in a density plot. As ex-
pected, the vast majority of blocks are near x = y, but for
a few outliers. Multiple metrics comparing A(block ) from
these sites support that results are independent of location:
the PDF of this diﬀerence appears Gaussian, where 96%
of values agree within ±0.05, and correlation coeﬃcient is
0.99999.
3.4 Multi-homed hosts and Routers
We generally assume that each host occupies only a single
IP address, and so each responsive address implies a respon-
sive host. This assumption is violated in two cases: some
hosts and all routers have multiple public network interfaces,
and some hosts use diﬀerent addresses at diﬀerent times. If
using a census to estimate hosts (not just addresses), we
need to account for this potential source of overcounting.
Multiple public IP addresses for a single host are known
as aliases in Internet mapping literature [13]; several tech-
niques have been developed for alias resolution to determine
when two IP addresses belong to the same host [13, 45].
One such technique is based on the fact that some multi-
homed hosts or routers can receive a probe-packet on one
interface and reply using a source address of the other [13].
The source address is either ﬁxed or determined by routing.
This behavior is known to be implementation-speciﬁc.
Because it can be applied retroactively, this technique is
particularly suitable for large-scale Internet probing. Rather
than sending additional probes, we re-examine our existing
traces with the Mercator alias resolution algorithm to ﬁnd
responses sent from addresses diﬀerent than were probed.
We carried out this analysis with census IT 15w and found
that 6.7 million addresses responded from a diﬀerent ad-
dress, a surprisingly large 6.5% of the 103M total responses.
In addition to hosts with multiple concurrent IP addresses,
many hosts have multiple sequential IP addresses, either be-
cause of associations with diﬀerent DHCP servers due to mo-
bility, or assignment of diﬀerent addresses from one server.
In general, we cannot track this since we only know address
occupancy and not the occupying host identity. However,
Section 5.1 suggests that occupancy of addresses is quite
short. Further work is needed to understand the impact of
hosts that take on multiple IP addresses over time, perhaps
using log analysis from large services [50, 25].
3.5 Probe Loss
An important limitation of our current methodology is
our inability to distinguish between host unavailability and
probe loss. Probes may be lost in several places:
in the
LAN or an early router near the probing machine, in the
general Internet, or near the destination. In this section, we
examine how lost probes aﬀect observed availability and the
distribution of A(addr ) and A(block ).
We minimize chances of probe loss near the probing ma-
chines in two diﬀerent ways. First, we rate-limit outgo-
ing probes to so that it is unlikely that we overrun nearby
routers buﬀers. Second, our probers checkpoint their state
periodically and so we are able to stop and resume probing
for known local outages. In one occasion we detected a local
outage after-the-fact, and we corrected for this by redoing
the probe period corresponding to the outage.
We expect three kinds of potential loss in the network
and at the far edge: occasional loss due to congestion, burst
losses due to routing changes [27] or edge network outages,
and burst losses due to ICMP rate-limiting at the destina-
tion’s last-hop router. We depend on probing in pseudo-
random order to mitigate the penalty of loss (Section 2.1).
With the highest probe rate to any /24 block of one probe
every 2–3 seconds in a survey, or 9 hours for a census, rate
limiting should not come into play. In addition, with a cen-
sus, probes are spaced much further apart than any kind of
short-term congestion or routing instability, so we rule out
burst losses for censuses, leaving only random loss.
Random loss is of concern because the eﬀect of loss is to
skew the data towards a lower availability. This skew dif-
fers from surveys of humans where non-response is apparent,
and where non-responses may be distributed equally in the
positive and negative directions. Prior studies of TCP sug-
gest we should expect random loss rates of a few percent
(for example, 90% of connections have 5% loss or less [1]).
We account for loss diﬀerently in censuses and surveys.
For censuses, data collection is so sparse that loss recovery
is not possible. Instead, we reduce the eﬀect of loss on anal-
ysis by focusing on A(block ) rather than A(addr ), since a
few, random losses have less impact when averaged over an
entire block. For surveys, we attempt to detect and repair
i
|
)
l
a
n
g
i
r
o
,
t
s
o
h
(
A
-
)
d
e
r
i
a
p
e
r
,
t
s
o
h
(
A
|
 0.2
 0.18
 0.16
 0.14
 0.12
 0.1
 0.08
 0.06
 0.04
 0.02
 0
1 repair
2 repair
3 repair
4 repair
 0
 0.2
 0.4
 0.6
 0.8
 1
A(host,original)
Figure 4: Distribution of diﬀerences between the k-repair
estimate and non-repaired IT survey
15w .
random probe loss through a k-repair process. We assume
that a random outage causes up to n consecutive probes to
be lost. We repair losses of up to k-consecutive probes by
searching for two positive responses separated by up to k
non-responses, and replacing this gap with assumed posi-
tive responses. We can then compare A(addr ) values with
and without k-repair; clearly A(addr ) with k-repair will be
higher than without.
Figure 4 shows how much k-repair changes measured A(addr )
values for IT survey
15w . Larger values of k result in greater
changes to A(addr ); but the change is fairly small: it changes
by at most 10% with 1-repair. We also observe that the
change is largest for intermediate A(addr ) values (0.4 to
0.8). This skew is because in our deﬁnition of A, highly
available addresses (A(addr ) > 0.8) have very few outages
to repair, while rarely available addresses (A(addr ) < 0.4)
have long-lasting outages that cannot be repaired.
Finally, although we focused on how loss aﬀects A(addr )
and A(block ), it actually has a stronger eﬀect on U (addr ).
Recall that U measures the continuous uptime of an address.
A host up continuously d0 days has a U (addr ) = 1, but a
brief outage anywhere after d1 days of monitoring gives a
mean uptime of (d1 + (d0 − d1))/2 days and a normalized
U (addr ) = 0.5, and a second outage reduces U (addr ) =
0.33. While k-repair reduces this eﬀect, reductions in U
caused by moderate outages are inherent in this metric.
Unless otherwise speciﬁed, we use 1-repair for our survey
data in the remainder of the paper.
4. EVALUATING METHODOLOGY PARAM-
ETERS
We have described our approaches to taking a census and
survey of Internet address usage. They trade oﬀ the com-
plete spatial coverage provided by a census for covering a
smaller area with ﬁner temporal resolution with a survey.
In this section we look at those tradeoﬀs and their basis in
sampling theory, evaluating how varying temporal or spatial
coverage aﬀects our observations.
4.1 Sampling in Time
As Internet addresses can be probed at diﬀerent rates, we
would like to know how the probe rate aﬀects the ﬁdelity
of our measurements. Increasing the sampling rate, while
l
d