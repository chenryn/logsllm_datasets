s
e
n
o
i
s
u
o
v
e
r
p
l
l
a
|
s
l
i
a
f
y
r
t
t
s
a
l
(
r
P
ICMP
HTTP
Amazon/storage
Amazon/VM
Google/storage
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
number of tries
Fig. 2. Comparing loss and reties for each target and method. Left: table of probability
ﬁrst try fails. Right: conditional probability kth retry fails, given failure of prior tries.
Dataset: VMs (2013-06-18+17), storage (2013-06-18+75).
outages [22,9]. Our probe rate (a few packets per second) is low enough to avoid
rate limiting, although, in Section 4.1, we see some cases where all packets are
dropped.
Having ruled out most sources of correlated loss, we next evaluate random loss.
We ﬁrst establish an analytic model of packet loss, assuming a ﬁxed loss rate
that aﬀects packets independently. We then compare that to our experimental
observations.
The curved lines in Figure 1 evaluate the probability of falsely inferring an
outage caused by random packet loss, as a function of packet loss rate (the x-
axis). We assume k tries for each probe and declare the service down when all
tries fail. For packet loss p, we model loss of the request or response:
Pr (outage | k probes) = (p + (1 − p)p)k
(1)
Without retries (k = 1), the false outage rate approximates the loss rate. Since
wide area packet loss can be around 1%, measurement without retries will show
false outages and skew estimates of cloud reliability. Fortunately, if we assume
packet loss is independent, then a few retries drive the false outage rate well
below typical cloud outage rates. For example, with three tries and 1% packet
−5, or ﬁve nines of availability. If we assume
loss, probe loss will be around 10
network loss rates peak at a few percent, 4–6 tries may be appropriate. Our data
starting 2013-06-18 uses 9 retries to rule out random loss.
3.2 ICMP Measurements
We next compare our model against experimental results for ICMP. The dotted
lines in Figure 2 show the probability the kth try fails if all previous k − 1 tries
failed. We evaluate this by considering the ﬁrst k tries from each observation.
With ICMP, we see that retries help. An initial loss is followed by a second
loss only 35-45% of the time, so 55–65% of the time the second try succeeds,
suggesting that the ﬁrst try was random loss. This eﬀect diminishes with more
retries, generally plateauing around 5 or 6 retries. When we compare long-term
124
Z. Hu et al.
observed loss rates to short-term ICMP retries, we see that losses are generally
more correlated than predicted by our analytic model. That is, it usually takes
more retries to recover from an initial loss than are predicted, but, with enough
retries, we often recover from an initial loss.
3.3 Retries and HTTP Probes
For HTTP-based probing, we retry at the application level, but the kernel also
does retries for the TCP connection. Our HTTP client (curl) has a 10 s appli-
cation timeout. The OS (Linux-2.6.32) does 3 SYN transmissions in this time,
providing 2 network-level retries “for free” for each application retry.
We see this beneﬁt in the Figure 2’s left table, where single-try HTTP losses
rates are much lower than ICMP. Kernel-level retries help even with application
retries, as seen in Figure 2’s right graph where the basic HTTP failure rate for
Amazon/storage and Google/storage is half that of ICMP. However, even HTTP
beneﬁts from multiple application-level retries before the conditional beneﬁt of
additional tries plateaus. We recommend 6 application-level tries even for HTTP
probes.
Application-level probes show even higher levels of conditional failure than
network-level, with 50% of second HTTP attempts failing on average, presum-
ably because of the additional kernel-level retries. However, this result means
that 50% of second attempts succeed —application-level failures are sometimes
transient. We thus recommend retries even for end-to-end tests.
4 Comparing Network and Application-Level Probing
We next compare network- and application-level probes to judge cloud availabil-
ity. We use our control sites to rule out problems near vantage points, and we
use suﬃcient retries to avoid eﬀects of random packet loss and transient network
issue in the middle, leaving outages at or near the cloud as the primary prob-
lem. Cloud services are made up of Internet-facing front-ends with sophisticated
back-end clusters. In some cases, ICMP may be handled by the front-ends, while
HTTP’s end-to-end tests reach into the back-end. Our goal is to compare this
diﬀerence. While the protocols almost always agree, there are many small dis-
agreements. We next show several causes of disagreement through representative
examples.
4.1 Comparing ICMP and HTTP Probing
We ﬁrst compare ICMP and HTTP probing, showing representative examples
of several causes of disagreement. These results show the need for end-to-end
measurement with application-level protocols.
Method Agreement: Figure 3 shows the percent of disagreement between
ICMP and HTTP over 17 days. Both approaches give the same result in the
vast majority of measurement rounds. They disagree in at most 3% of rounds
The Need for End-to-End Evaluation of Cloud Availability
125
s
P
V
l
l
a
r
e
v
o
d
e
t
a
g
e
r
g
g
a
s
d
n
u
o
r
f
o
%
 3
 2.5
 2
 1.5
 1
 0.5
 0
Amazon EC2 (N. California)
H & I
H & I
J
u
J
u
J
u
J
u
J
u
J
u
J
u
J
u
J
u
J
u
J
u
J
u
J
u
n
n
n
n
n
n
n
n
n
n
n
n
n
J
e 1
e 1
e 2
e 2
e 2
e 2
e 2
e 2
e 2
e 2
e 2
e 2
e 3
8
9
0
1
2
3
4
5
6
7
8
9
0
J
J
J
uly 0
uly 0
uly 0
uly 0
1
2
3
4
Fig. 3. Quantifying disagreements between HTTP and ICMP probes. This includes
either HTTP success and ICMP failure (red striped bar) or HTTP failure and ICMP
success (blue bar). Dataset: 2013-06-18+17.
ICMP: 3 fails
HTTP: 3 fails
ICMP: 2 fails
HTTP: 2 fails
ICMP: 1 fail
HTTP: 1 fail
provider outage
05:00
2013-04-15
09:00
13:00
17:00
21:00
ICMP: success
HTTP: success
Univ of Michigan, US
Indiana Univ, US
Northwestern Univ, US
Univ of Waterloo, CA
Tech Univ of Koszalin, PL
Tampere Univ, FI
Univ of Basel, CH
Moscow EE Institute, RU
Moscow State Univ, RU
Monash Univ, AU
Univ of Sao Paulo, BR
00:00 PDT
2013-04-16
Fig. 4. Strip chart: Amazon VM (Singapore). Dataset: 2013-03-11+33.
in a given day, and, on most days, they disagree in 0.5% or less of the rounds.
The high agreement is because the monitored service is almost always up, and
both methods detect it as such. On three days (June 23, 25, 29), we see complete
agreement (no outages). We also see that both methods report outages on some
days (for example, June 18 and 24).
To illustrate the details of an outage, Figure 4 shows a strip chart for a
provider-conﬁrmed outage at one Amazon EC2 site [3]; ICMP and HTTP report
the outage consistently. In this chart, each column of data shows one round of
measurements (with 24-hour boundaries as vertical black lines), and each pair of
rows shows ICMP and HTTP observations from one VP (the blue top is ICMP,
and the lower red is HTTP). Light colors represent successful probes, medium
colors represent failures of some tries (but eventual success). Dark blue diamonds
show ICMP-determined outages (all ICMP tries fail); dark red squares show an
HTTP outage (all HTTP tries fail). White areas show cases where one of the
control nodes failed to respond to either ICMP or HTTP, or where we are unable
to upload data to our collection site.
As a second example, Figure 5 shows a case where both ICMP and HTTP
report intermittent failures from one VP. We see intermittent problems from