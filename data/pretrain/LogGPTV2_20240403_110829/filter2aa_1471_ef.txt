do not overlap (so a given cluster is present in only one container at time). A
container represents an area in the volume and all containers on a volume are
always of the same size, which is defined based on the type of the underlying
disk: 64 MB for standard tiered disks and 256 MB for SMR disks. Containers
are called ReFS bands because if they’re used with SMR disks, the
containers’ size becomes exactly the same as the SMR bands’ size, and each
container maps one-to-one to each SMR band.
The indirection layer is configured and provided by the global container
table, as shown in Figure 11-92. The rows of this table are composed by keys
that store the ID and the type of the container. Based on the type of container
(which could also be a compacted or compressed container), the row’s data is
different. For noncompacted containers (details about ReFS compaction are
available in the next section), the row’s data is a data structure that contains
the mapping of the cluster range addressed by the container. This provides to
ReFS a virtual LCN-to-real LCN namespace mapping.
Figure 11-92 The container table provides a virtual LCN-to-real LCN
indirection layer.
The container table is important: all the data managed by ReFS and
Minstore needs to pass through the container table (with only small
exceptions), so ReFS maintains multiple copies of this vital table. To perform
an I/O on a block, ReFS must first look up the location of the extent’s
container to find the real location of the data. This is achieved through the
extent table, which contains target virtual LCN of the cluster range in the
data section of its rows. The container ID is derived from the LCN, through a
mathematical relationship. The new level of indirection allows ReFS to move
the location of containers without consulting or modifying the file extent
tables.
ReFS consumes tiers produced by Storage Spaces, hardware tiered
volumes, and SMR disks. ReFS redirects small random I/Os to a portion of
the faster tiers and destages those writes in batches to the slower tiers using
sequential writes (destages happen at container granularity). Indeed, in ReFS,
the term fast tier (or flash tier) refers to the random-access zone, which might
be provided by the conventional bands of an SMR disk, or by the totality of
an SSD or NVMe device. The term slow tier (or HDD tier) refers instead to
the sequential write bands or to a rotating disk. ReFS uses different behaviors
based on the class of the underlying medium. Non-SMR disks have no
sequential requirements, so clusters can be allocated from anywhere on the
volume; SMR disks, as discussed previously, need to have strictly sequential
requirements, so ReFS never writes random data on the slow tier.
By default, all of the metadata that ReFS uses needs to stay in the fast tier;
ReFS tries to use the fast tier even when processing general write requests. In
non-SMR disks, as flash containers fill, ReFS moves containers from flash to
HDD (this means that in a continuous write workload, ReFS is continually
moving containers from flash into HDD). ReFS is also able to do the
opposite when needed—select containers from the HDD and move them into
flash to fill with subsequent writes. This feature is called container rotation
and is implemented in two stages. After the storage driver has copied the
actual data, ReFS modifies the container LCN mapping shown earlier. No
modification in any file’s extent table is needed.
Container rotation is implemented only for non-SMR disks. This is
important, because in SMR disks, the ReFS file system driver never
automatically moves data between tiers. Applications that are SMR disk–
aware and want to write data in the SMR capacity tier can use the
FSCTL_SET_REFS_FILE_STRICTLY_SEQUENTIAL control code. If an
application sends the control code on a file handle, the ReFS driver writes all
of the new data in the capacity tier of the volume.
EXPERIMENT: Witnessing SMR disk tiers
You can use the FsUtil tool, which is provided by Windows, to
query the information of an SMR disk, like the size of each tier, the
usable and free space, and so on. To do so, just run the tool in an
administrative command prompt. You can launch the command
prompt as administrator by searching for cmd in the Cortana
Search box and by selecting Run As Administrator after right-
clicking the Command Prompt label. Input the following
parameters:
Click here to view code image
fsutil volume smrInfo 
replacing the  part with the drive letter of your
SMR disk.
Furthermore, you can start a garbage collection (see the next
paragraph for details about this feature) through the following
command:
Click here to view code image
fsutil volume smrGc  Action=startfullspeed
The garbage collection can even be stopped or paused through
the relative Action parameter. You can start a more precise garbage
collection by specifying the IoGranularity parameter, which
specifies the granularity of the garbage collection I/O, and using
the start action instead of startfullspeed.
Container compaction
Container rotation has performance problems, especially when storing small
files that don’t usually fit into an entire band. Furthermore, in SMR disks,
container rotation is never executed, as we explained earlier. Recall that each
SMR band has an associated write pointer (hardware implemented), which
identifies the location for sequential writing. If the system were to write
before or after the write pointer in a non-sequential way, it would corrupt data
located in other clusters (the SMR firmware must therefore refuse such a
write).
ReFS supports two types of containers: base containers, which map a
virtual cluster’s range directly to physical space, and compacted containers,
which map a virtual container to many different base containers. To correctly
map the correspondence between the space mapped by a compacted
container and the base containers that compose it, ReFS implements an
allocation bitmap, which is stored in the rows of the global container index
table (another table, in which every row describes a single compacted
container). The bitmap has a bit set to 1 if the relative cluster is allocated;
otherwise, it’s set to 0.
Figure 11-93 shows an example of a base container (C32) that maps a
range of virtual LCNs (0x8000 to 0x8400) to real volume’s LCNs (0xB800
to 0xBC00, identified by R46). As previously discussed, the container ID of
a given virtual LCN range is derived from the starting virtual cluster number;
all the containers are virtually contiguous. In this way, ReFS never needs to
look up a container ID for a given container range. Container C32 of Figure
11-93 only has 560 clusters (0x230) contiguously allocated (out of its 1,024).
Only the free space at the end of the base container can be used by ReFS. Or,
for non-SMR disks, in case a big chunk of space located in the middle of the
base container is freed, it can be reused too. Even for non-SMR disks, the
important requirement here is that the space must be contiguous.
Figure 11-93 An example of a base container addressed by a 210 MB file.
Container C32 uses only 35 MB of its 64 MB space.
If the container becomes fragmented (because some small file extents are
eventually freed), ReFS can convert the base container into a compacted
container. This operation allows ReFS to reuse the container’s free space,
without reallocating any row in the extent table of the files that are using the
clusters described by the container itself.
ReFS provides a way to defragment containers that are fragmented. During
normal system I/O activity, there are a lot of small files or chunks of data that
need to be updated or created. As a result, containers located in the slow tier
can hold small chunks of freed clusters and can become quickly fragmented.
Container compaction is the name of the feature that generates new empty
bands in the slow tier, allowing containers to be properly defragmented.
Container compaction is executed only in the capacity tier of a tiered volume
and has been designed with two different goals:
■    Compaction is the garbage collector for SMR-disks: In SMR,
ReFS can only write data in the capacity zone in a sequential manner.
Small data can’t be singularly updated in a container located in the
slow tier. The data doesn’t reside at the location pointed by the SMR
write pointer, so any I/O of this kind can potentially corrupt other data
that belongs to the band. In that case, the data is copied in a new band.
Non-SMR disks don’t have this problem; ReFS updates data residing
in the small tier directly.
■    In non-SMR tiered volumes, compaction is the generator for
container rotation: The generated free containers can be used as
targets for forward rotation when data is moved from the fast tier to
the slow tier.
ReFS, at volume-format time, allocates some base containers from the
capacity tier just for compaction; which are called compacted reserved
containers. Compaction works by initially searching for fragmented
containers in the slow tier. ReFS reads the fragmented container in system
memory and defragments it. The defragmented data is then stored in a
compacted reserved container, located in the capacity tier, as described
above. The original container, which is addressed by the file extent table,
becomes compacted. The range that describes it becomes virtual (compaction
adds another indirection layer), pointing to virtual LCNs described by
another base container (the reserved container). At the end of the
compaction, the original physical container is marked as freed and is reused
for different purposes. It also can become a new compacted reserved
container. Because containers located in the slow tier usually become highly
fragmented in a relatively small time, compaction can generate a lot of empty
bands in the slow tier.
The clusters allocated by a compacted container can be stored in different
base containers. To properly manage such clusters in a compacted container,
which can be stored in different base containers, ReFS uses another extra
layer of indirection, which is provided by the global container index table
and by a different layout of the compacted container. Figure 11-94 shows the
same container as Figure 11-93, which has been compacted because it was
fragmented (272 of its 560 clusters have been freed). In the container table,
the row that describes a compacted container stores the mapping between the
cluster range described by the compacted container, and the virtual clusters
described by the base containers. Compacted containers support a maximum
of four different ranges (called legs). The four legs create the second
indirection layer and allow ReFS to perform the container defragmentation in
an efficient way. The allocation bitmap of the compacted container provides
the second indirection layer, too. By checking the position of the allocated
clusters (which correspond to a 1 in the bitmap), ReFS is able to correctly
map each fragmented cluster of a compacted container.
Figure 11-94 Container C32 has been compacted in base container C124
and C56.
In the example in Figure 11-94, the first bit set to 1 is at position 17, which
is 0x11 in hexadecimal. In the example, one bit corresponds to 16 clusters; in
the actual implementation, though, one bit corresponds to one cluster only.
This means that the first cluster allocated at offset 0x110 in the compacted
container C32 is stored at the virtual cluster 0x1F2E0 in the base container
C124. The free space available after the cluster at offset 0x230 in the
compacted container C32, is mapped into base container C56. The physical
container R46 has been remapped by ReFS and has become an empty
compacted reserved container, mapped by the base container C180.
In SMR disks, the process that starts the compaction is called garbage
collection. For SMR disks, an application can decide to manually start, stop,
or pause the garbage collection at any time through the
FSCTL_SET_REFS_SMR_VOLUME_GC_PARAMETERS file system
control code.
In contrast to NTFS, on non-SMR disks, the ReFS volume analysis engine
can automatically start the container compaction process. ReFS keeps track
of the free space of both the slow and fast tier and the available writable free
space of the slow tier. If the difference between the free space and the
available space exceeds a threshold, the volume analysis engine kicks off and
starts the compaction process. Furthermore, if the underlying storage is
provided by Storage Spaces, the container compaction runs periodically and
is executed by a dedicated thread.
Compression and ghosting
ReFS does not support native file system compression, but, on tiered
volumes, the file system is able to save more free containers on the slow tier
thanks to container compression. Every time ReFS performs container
compaction, it reads in memory the original data located in the fragmented
base container. At this stage, if compression is enabled, ReFS compresses the
data and finally writes it in a compressed compacted container. ReFS
supports four different compression algorithms: LZNT1, LZX, XPRESS, and
XPRESS_HUFF.
Many hierarchical storage management (HMR) software solutions support
the concept of a ghosted file. This state can be obtained for many different
reasons. For example, when the HSM migrates the user file (or some chunks
of it) to a cloud service, and the user later modifies the copy located in the
cloud through a different device, the HSM filter driver needs to keep track of
which part of the file changed and needs to set the ghosted state on each
modified file’s range. Usually HMRs keep track of the ghosted state through
their filter drivers. In ReFS, this isn’t needed because the ReFS file system
exposes a new I/O control code, FSCTL_GHOST_FILE_EXTENTS. Filter
drivers can send the IOCTL to the ReFS driver to set part of the file as
ghosted. Furthermore, they can query the file’s ranges that are in the ghosted
state through another I/O control code:
FSCTL_QUERY_GHOSTED_FILE_EXTENTS.
ReFS implements ghosted files by storing the new state information
directly in the file’s extent table, which is implemented through an embedded
table in the file record, as explained in the previous section. A filter driver
can set the ghosted state for every range of the file (which must be cluster-
aligned). When the ReFS driver intercepts a read request for an extent that is
ghosted, it returns a STATUS_GHOSTED error code to the caller, which a
filter driver can then intercept and redirect the read to the proper place (the
cloud in the previous example).
Storage Spaces
Storage Spaces is the technology that replaces dynamic disks and provides
virtualization of physical storage hardware. It has been initially designed for
large storage servers but is available even in client editions of Windows 10.
Storage Spaces also allows the user to create virtual disks composed of
different underlying physical mediums. These mediums can have different
performance characteristics.
At the time of this writing, Storage Spaces is able to work with four types
of storage devices: Nonvolatile memory express (NVMe), flash disks,
persistent memory (PM), SATA and SAS solid state drives (SSD), and
classical rotating hard-disks (HDD). NVMe is considered the faster, and
HDD is the slowest. Storage spaces was designed with four goals:
■    Performance: Spaces implements support for a built-in server-side
cache to maximize storage performance and support for tiered disks
and RAID 0 configuration.
■    Reliability: Other than span volumes (RAID 0), spaces supports
Mirror (RAID 1 and 10) and Parity (RAID 5, 6, 50, 60) configurations
when data is distributed through different physical disks or different
nodes of the cluster.
■    Flexibility: Storage spaces allows the system to create virtual disks
that can be automatically moved between a cluster’s nodes and that
can be automatically shrunk or extended based on real space
consumption.
■    Availability: Storage spaces volumes have built-in fault tolerance.
This means that if a drive, or even an entire server that is part of the
cluster, fails, spaces can redirect the I/O traffic to other working nodes
without any user intervention (and in a way). Storage spaces don’t
have a single point of failure.
Storage Spaces Direct is the evolution of the Storage Spaces technology.
Storage Spaces Direct is designed for large datacenters, where multiple
servers, which contain different slow and fast disks, are used together to
create a pool. The previous technology didn’t support clusters of servers that
weren’t attached to JBOD disk arrays; therefore, the term direct was added to
the name. All servers are connected through a fast Ethernet connection
(10GBe or 40GBe, for example). Presenting remote disks as local to the
system is made possible by two drivers—the cluster miniport driver
(Clusport.sys) and the cluster block filter driver (Clusbflt.sys)—which are
outside the scope of this chapter. All the storage physical units (local and
remote disks) are added to a storage pool, which is the main unit of
management, aggregation, and isolation, from where virtual disks can be
created.
The entire storage cluster is mapped internally by Spaces using an XML
file called BluePrint. The file is automatically generated by the Spaces GUI
and describes the entire cluster using a tree of different storage entities:
Racks, Chassis, Machines, JBODs (Just a Bunch of Disks), and Disks. These
entities compose each layer of the entire cluster. A server (machine) can be
connected to different JBODs or have different disks directly attached to it.
In this case, a JBOD is abstracted and represented only by one entity. In the
same way, multiple machines might be located on a single chassis, which
could be part of a server rack. Finally, the cluster could be made up of
multiple server racks. By using the Blueprint representation, Spaces is able to
work with all the cluster disks and redirect I/O traffic to the correct
replacement in case a fault on a disk, JBOD, or machine occurs. Spaces
Direct can tolerate a maximum of two contemporary faults.
Spaces internal architecture
One of the biggest differences between Spaces and dynamic disks is that
Spaces creates virtual disk objects, which are presented to the system as
actual disk device objects by the Spaces storage driver (Spaceport.sys).
Dynamic disks operate at a higher level: virtual volume objects are exposed
to the system (meaning that user mode applications can still access the
original disks). The volume manager is the component responsible for
creating the single volume composed of multiple dynamic volumes. The
Storage Spaces driver is a filter driver (a full filter driver rather than a
minifilter) that lies between the partition manager (Partmgr.sys) and the disk
class driver.
Storage Spaces architecture is shown in Figure 11-95 and is composed
mainly of two parts: a platform-independent library, which implements the
Spaces core, and an environment part, which is platform-dependent and links
the Spaces core to the current environment. The Environment layer provides
to Storage Spaces the basic core functionalities that are implemented in
different ways based on the platform on which they run (because storage
spaces can be used as bootable entities, the Windows boot loader and boot
manager need to know how to parse storage spaces, hence the need for both a
UEFI and Windows implementation). The core basic functionality includes
memory management routines (alloc, free, lock, unlock and so on), device
I/O routines (Control, Pnp, Read, and Write), and synchronization methods.
These functions are generally wrappers to specific system routines. For
example, the read service, on Windows platforms, is implemented by
creating an IRP of type IRP_MJ_READ and by sending it to the correct disk
driver, while, on UEFI environments, it’s implemented by using the
BLOCK_IO_PROTOCOL.
Figure 11-95 Storage Spaces architecture.
Other than the boot and Windows kernel implementation, storage spaces
must also be available during crash dumps, which is provided by the
Spacedump.sys crash dump filter driver. Storage Spaces is even available as
a user-mode library (Backspace.dll), which is compatible with legacy
Windows operating systems that need to operate with virtual disks created by
Spaces (especially the VHD file), and even as a UEFI DXE driver
(HyperSpace.efi), which can be executed by the UEFI BIOS, in cases where
even the EFI System Partition itself is present on a storage space entity.
Some new Surface devices are sold with a large solid-state disk that is
actually composed of two or more fast NVMe disks.
Spaces Core is implemented as a static library, which is platform-
independent and is imported by all of the different environment layers. Is it
composed of four layers: Core, Store, Metadata, and IO. The Core is the
highest layer and implements all the services that Spaces provides. Store is
the component that reads and writes records that belong to the cluster
database (created from the BluePrint file). Metadata interprets the binary
records read by the Store and exposes the entire cluster database through
different objects: Pool, Drive, Space, Extent, Column, Tier, and Metadata.
The IO component, which is the lowest layer, can emit I/Os to the correct
device in the cluster in the proper sequential way, thanks to data parsed by
higher layers.
Services provided by Spaces
Storage Spaces supports different disk type configurations. With Spaces, the
user can create virtual disks composed entirely of fast disks (SSD, NVMe,
and PM), slow disks, or even composed of all four supported disk types
(hybrid configuration). In case of hybrid deployments, where a mix of
different classes of devices are used, Spaces supports two features that allow
the cluster to be fast and efficient:
■    Server cache: Storage Spaces is able to hide a fast drive from the
cluster and use it as a cache for the slower drives. Spaces supports PM
disks to be used as a cache for NVMe or SSD disks, NVMe disks to
be used as cache for SSD disks, and SSD disks to be used as cache for
classical rotating HDD disks. Unlike tiered disks, the cache is
invisible to the file system that resides on the top of the virtual
volume. This means that the cache has no idea whether a file has been
accessed more recently than another file. Spaces implements a fast
cache for the virtual disk by using a log that keeps track of hot and
cold blocks. Hot blocks represent parts of files (files’ extents) that are
often accessed by the system, whereas cold blocks represent part of
files that are barely accessed. The log implements the cache as a
queue, in which the hot blocks are always at the head, and cold blocks
are at the tail. In this way, cold blocks can be deleted from the cache
if it’s full and can be maintained only on the slower storage; hot
blocks usually stay in the cache for a longer time.
■    Tiering: Spaces can create tiered disks, which are managed by ReFS
and NTFS. Whereas ReFS supports SMR disks, NTFS only supports
tiered disks provided by Spaces. The file system keeps track of the hot
and cold blocks and rotates the bands based on the file’s usage (see
the “ReFS support for tiered volumes and SMR” section earlier in this
chapter). Spaces provides to the file system driver support for
pinning, a feature that can pin a file to the fast tier and lock it in the
tier until it will be unpinned. In this case, no band rotation is ever
executed. Windows uses the pinning feature to store the new files on
the fast tier while performing an OS upgrade.
As already discussed previously, one of the main goals of Storage Spaces
is flexibility. Spaces supports the creation of virtual disks that are extensible
and consume only allocated space in the underlying cluster’s devices; this
kind of virtual disk is called thin provisioned. Unlike fixed provisioned disks,
where all of the space is allocated to the underlying storage cluster, thin
provisioned disks allocate only the space that is actually used. In this way,