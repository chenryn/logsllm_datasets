64
LR
128
Rate
95%
99%
99.9%
95%
99%
99.9%
640
2,555
18,050
1,350
5,570
39,200
Time
0.01 sec
0.13 sec
0.60 sec
0.06 sec
0.51 sec
2.10 sec
Table 1: LR on Arb PUFs with 64 and 128 stages,
or with bitlength 64 and 128. We used HW (cid:2).
of a linear additive delay model [13]: The delays of the two
electrical signal paths are simply added up and compared.
We use the following deﬁnitions throughout the paper:
The prediction error  is the ratio of incorrect responses of
the trained ML algorithm when evaluated on the test set.
For all applications of LR, the test set each time consisted
of 10,000 randomly chosen CRPs. For all applications of
ES (i.e., for the Feed-Forward Arbiter PUF), the test set
each time consisted of 8, 000 randomly chosen CRPs. The
prediction rate is 1 − .
NCRP (or simply “CRPs”) denotes the number of CRPs
employed by the attacker in his respective attack, for ex-
ample in order to achieve a certain prediction rate. This
nomenclature holds throughout the whole paper. Never-
theless, one subtle diﬀerence should be made explicit: In all
applications of LR (i.e., in Sections 3 to 5), NCRP is equal to
the size of the training set of the ML algorithm, as one would
usually expect. In the applications of ES (i.e., in Section 6),
however, the situation is more involved. The attacker needs
a test set himself in order to determine which of his many
random runs was the best. The value NCRP given in the
tables and formulas of Section 6 hence reﬂects the sum of
the sizes of the training set and the test set employed by the
attacker.
3. ARBITER PUFS
3.1 Machine Learning Results
To determine the separating hyperplane (cid:2)wT (cid:2)Φ = 0, we ap-
plied SVMs, LR and ES. LR achieved the best results, which
are shown in Table 1. We chose three diﬀerent prediction
rates as targets: 95% is roughly the environmental stabil-
ity of a 64-bit Arbiter PUF when exposed to a temperature
variation of 45C and voltage variation of ±2% 2. The val-
ues 99% and 99.9%, respectively, represent benchmarks for
optimized ML results. All ﬁgures in Table 1 were obtained
by averaging over 5 diﬀerent training sets. Accuracies were
estimated using test sets of 10,000 CRPs.
3.2 Scalability
We also executed scalability experiments with LR, which
are displayed in Fig. 1 and Fig. 2. They show that the
relevant parameters – the required number of CRPs in the
training set and the computational complexity, i.e., the num-
ber of basic operations – grow both linearly or low-degree
polynomially in the misclassiﬁcation rate  and the length k
2The exact ﬁgures reported in [17] are: 4.57% CRP variation
for a temperature variation of 45C, and 2.16% for a voltage
variation of ±2%.
of the Arb PUF. Theoretical considerations (dimension of
the feature space, Vapnik-Chervonenkis dimension) suggest
that the minimal number of CRPs NCRP that is necessary
to model a k-stage arbiter with a misclassiﬁcation rate of 
should obey the relation
NCRP = O (k/).
(9)
This was conﬁrmed by our experimental results.
In practical PUF applications, it is essential to know the
concrete number of CRPs that may become known before
the PUF-security breaks down. Assuming an approximate
linear functional dependency y = ax + c in the double loga-
rithmic plot of Fig. 1 with a slope of a = −1, we obtained
the following empirical formula (10).
It gives the approx-
imate number of CRPs NCRP that is required to learn a
k-stage arbiter PUF with error rate :
NCRP ≈ 0.5 · k + 1

(10)
Our experiments also showed that the training time of the
ML algorithms, measured in the number of basic operations
NBOP , grows slowly. It is determined by the following two
factors: (i) The evaluation of the current model’s likelihood
(Eqn. 1) and its gradient (Eqn. 2), and (ii) the number of
iterations of the optimization procedure before convergence
occurs (see section 2.1.1). The former is both a sum over a
function of the feature vectors (cid:2)Φ for all NCRP , and there-
fore has complexity O (k · NCRP ). On the basis of the data
shown in Figure 2, we may further estimate that the num-
bers of iterations increases proportional to the logarithm of
the number of CRPs NCRP . Together, this yields an overall
complexity of
(cid:13)
(cid:14)
NBOP = O
.
(11)
k2

· log
k

4. XOR ARBITER PUFS
4.1 Machine Learning Results
In the application of SVMs and ES to XOR Arb-PUFs, we
were able to break small instances, for example XOR Arb-
PUFs with 2 or 3 XORs and 64 stages. LR signiﬁcantly
Figure 1: Double logarithmic plot of misclassiﬁca-
tion rate  on the ratio of training CRPs NCRP and
dim(Φ) = k + 1.
242CRPs
(×103)
24
50
200
Best Pr.
Ave. Pr.
Suc. Tr.
Inst.
Best Pr.
Ave. Pr.
Suc. Tr.
Inst.
Best Pr.
Ave. Pr.
Suc. Tr.
Inst.
Percentage of error-inﬂicted CRPs
10%
0%
—
—
98.76% 92.83% 88.05%
98.62% 91.37% 88.05%
2%
5%
0.2%
5.0%
0.8%
25.0%
0.0%
0.6%
40.0%
0.0%
99.49% 95.17% 92.67% 89.89%
99.37% 94.39% 91.62% 88.20%
4.6%
12.4%
13.9%
100.0% 62.5%
20.0%
99.88% 97.74% 96.01% 94.61%
99.78% 97.34% 95.69% 93.75%
100.0% 87.0%
71.4%
100.0% 100.0% 100.0% 100.0%
10.0%
50.0%
87.0%
Table 3: LR on 128-bit, 4-XOR Arb PUFs with dif-
ferent levels of error in the training set. We show
the best and average prediction rates of 40 randomly
chosen instances, the percentage of successful tri-
als over all instances, and the percentage of and in-
stances that converged to a suﬃcient optimum in at
least one trial. We used HW (cid:3).
CRPs
(×103)
500
Best Pr.
Ave. Pr.
Suc. Tr.
Inst.
Percentage of error-inﬂicted CRPs
0%
10%
2%
5%
99.90% 97.55% 96.48% 93.12%
99.84% 97.33% 95.84% 93.12%
0.7%
5.0%
7.0%
20.0%
2.9%
20.0%
0.9%
10.0%
Table 4: LR on 128-bit, 5-XOR Arb PUFs with dif-
ferent amounts of error in the training set. Rest as
in the caption of Table 3. We used HW (cid:3).
4.3 Scalability
Figures 4 and 5 display the results of our scaling exper-
iments with LR. Again, the smallest number of CRPs in
the training set NCRP needed to achieve predictions with
a misclassiﬁcation rate  scales linearly with the number of
parameters of the problem (the product of the number of
stages k and the number of XORed Arb-PUFs l):
NCRP ∼ (k + 1) · l

.
(12)
But, in contrast to standard Arb-PUFs, optimizing the non-
linear decision boundary (6) on the training set now is a
non-convex problem, so that the LR algorithm is not guar-
anteed to ﬁnd (an attractor of) the global optimum in its
ﬁrst trial. It needs to be iteratively restarted Ntrial times.
Ntrial thereby can be expected to not only depend on k and
l, but also on the size NCRP of the employed training set.
As it is argued in greater detail in [20], the success rate
(= 1/Ntrial) of ﬁnding (an attractor of) the global optimum
seems indeed determined by the ratio of dimensions of gradi-
ent information (∝ NCRP as the gradient is a linear combi-
nation of the feature vector) and the dimension dΦ in which
(cid:15)l
the problem is linear separable. The dimension dΦ is the
number of independent dimensions of (cid:2)ΦXOR =
(cid:2)Φi =
i=1(Φ1
As the tensor product of several vectors consists of all
possible products between their vector components, the in-
dependent dimensions are given by the number of diﬀer-
(cid:15)l
i . . . , Φk
i , 1)T .
i=1
Figure 2: No. of iterations of the LR algorithm un-
til “convergence” occurs (see section 2), plotted in
dependence of the training set size NCRP .
outperformed the other two methods. The key observation
is that instead of determining the linear decision boundary
(Eqn. 7), one can also specify the non-linear boundary (Eqn.
6). This is done by setting the LR decision boundary f =
i=1 (cid:2)wT
i (cid:2)Φi. The results are displayed in Table 2.
(cid:3)l
4.2 Performance on Error-Inﬂicted CRPs
The CRPs used in Section 4.1 have been generated pseu-
dorandomly via an additive, linear delay model of the PUF.
This deviates from reality in two aspects: First of all, the
CRPs obtained from real PUFs are subject to noise and
random errors. Secondly, the linear model matches the phe-
nomena on a real circuit very closely [17], but not perfectly.
This leads to a deviation of any real system from the linear
model on a small percentage of all CRPs.
In order to mimic this situation, we investigated the ML
performance when a small error is injected artiﬁcially into
the training sets. A given percentage of responses in the
training set were chosen randomly, and their bit values were
ﬂipped. Afterwards, the ML performance on the unaltered,
error-free test sets was evaluated. The results are displayed
in Tables 3 and 4. They show that LR can cope very well
with errors, provided that around 3 to 4 times more CRPs
are used. The required convergence times on error inﬂicted
training sets did not change substantially compared to error
free training sets of the same sizes.
ML
Bit
Method Length Rate
Pred. No. of
XORs
LR
64
99%
LR
128
99%
4
5
6
4
5
6
CRPs
(×103)
12
80
200
24
500
—
Training
Time
3:42 min
2:08 hrs
31:01 hrs
2:52 hrs
16:36 hrs
—
Table 2: LR on XOR Arbiter PUFs. Training times
are averaged over diﬀerent PUF-instances. HW (cid:2).
243LR on XOR Arbiter PUFs
e
t
a
r
s
s
e
c
u
s
64 Bit, 4 XOR
64 Bit, 5 XOR
64 Bit, 6 XOR
128 Bit, 3 XOR
128 Bit, 4 XOR
128 Bit, 5 XOR
Figure 5: Average rate of success of the LR algo-
rithm plotted in dependence of the ratio dΦ (see Eqn.
(13)) to NCRP . We used HW (cid:3).
(cid:17)
Writing this down more formally, dΦ is given by
(cid:16)
(cid:17)
(cid:17)
(cid:16)
(cid:16)
dΦ =
k + 1
l
+
k + 1
l − 2
+
k + 1
l − 4
+ . . .
k(cid:5)l≈ (k + 1)l
l!