v
a
S
e
c
r
u
o
s
e
R
l
i
r
o
f
d
o
h
s
e
r
h
T
e
m
T
y
r
e
v
o
c
e
R
d
n
u
o
B
r
e
w
o
L
0
2
4
6
8
10
0
2
4
6
8
10
1MB
10MB
100MB
1GB
Recovery Time (s)
Recovery Time (s)
State Size
Fig. 5: Resource overhead with varying
recovery time thresholds using the All
Costs cost model.
Fig. 6: Resource overhead with varying
recovery time thresholds using the Ama-
zon EC2 cost model.
Fig. 7: Lower bound recovery time
threshold with different state sizes.
the user chooses zero seconds recovery time, we can witness an
100% overhead for CPU, memory, network and infrastructure
utilization as the system runs solely in active replication mode.
However, with increasing recovery threshold, resources can be
saved. For example, using a recovery threshold of four seconds
or more, the CPU overhead already drops to 50%, whereas the
network utilization rises up to 300%. This is due to the fact that
the system predominately uses hot passive standby where CPU
cycles are saved due to the suspended secondary, however,
at the cost of network bandwidth due to the periodic state
synchronization mechanism. In fact, an overhead of 300% also
indicates that the state synchronization mechanism consumes
more network bandwidth than event dissemination alone. The
overhead can be reduced by lowering the state synchronization
frequency as we will show later.
Depending on the nature of the application, such savings
in CPU resources can be considerable. For example, for an
application that is CPU bound due to some very costly oper-
ations, using the adaptive scheme would trade CPU resources
for network resources while still providing the same guarantees
regarding recovery time and semantics as active replication.
D. Cost Model
We slightly modiﬁed the previous experiment by providing
a cost weight vector that matches the Amazon EC2 cost model.
In Amazon EC2 users are charged based on hours they use
a virtual machine rather than actual CPU cycles or network
bandwidth. Hence,
there are no additional charges if they
constantly fully utilize the CPU and the (internal) network.
To match the proﬁle, we set the weights for CPU, memory
and network to zero so they will not be taken in consideration
when ranking the different approaches. In other words, the
approaches that use the least number of virtual machine hours
are preferred.
471471
The outcome of this modiﬁcation is shown in Figure 6. If
we compare the infrastructure utilization depicted in Figures 5
and 6, we can see that with a recovery time threshold of
5.5 seconds, the overhead (i.e., the number of required nodes)
decreases faster than with the default cost weight vector,
conﬁrming the emphasis on infrastructure costs rather than on
individual resources. On the other hand, the overhead for CPU,
memory and network resources stays constant for recovery
time thresholds of less than 5.5 seconds. This is due to the
fact that the controller favors active replication as it provides a
faster recovery compared to approaches that consume a similar
amount of resources.
E. Relation between State Size and Resource Savings
As mentioned previously,
the size of the state has a
strong impact on the recovery time. A larger state requires
signiﬁcantly more amount of time to be loaded from disk and
reconstructed in memory. Hence, in the following experiment,
we extended our sentiment analysis application with an extra
data ﬁeld attached to each event. We then varied the event size
to investigate its impact to resource savings. It is expected
that applications with a relatively small state allow more
potential resource savings when considering fault tolerance
than applications with larger state.
Figure 7 depicts the lower bound, i.e., the lower limit for
the recovery threshold a user must choose in order to achieve
resource savings.
The results reveal that regardless of the state size a recovery
threshold of more than four seconds allows already saving
resources since the system can then transparently switch to
passive standby mode, reducing the CPU overhead. However,
one has to keep in mind that with increasing state size,
state synchronization can be performed less frequently which
increases the number of events in upstream logs.
In order to save memory resources, users have to provide
a large recovery time threshold prior saving resources. This is
due to the fact that for the default cost weight vector, CPU
hungry schemes such as active replication and active standby
are replaced with memory consuming states such as passive
standby. For network and infrastructure utilization we can
observer similar trends where with increasing state size: higher
recovery time thresholds must be provided prior to beneﬁting
from resource savings.
Note that for state sizes of 500 M B and more, a recovery
time of more than 20 seconds is needed. This case is not
directly shown in the graphs, but indicated through the plateau.
Since StreamMine3G supports ﬁne grained state partition-
ing. The state per partition is usually small and would only
rarely exceed more than 100 M B if the workload is well
balanced. Even then, as with Hadoop stragglers, breaking a
stage is often a possible approach to reduce the amount of
state in a partition. As a consequence, users can beneﬁt from
resources savings even with short recovery time thresholds.
F. Relation between the Cost Models and the Use of Fault
Tolerance Schemes
In the next experiment, we varied the recovery time thresh-
old for different cost models to get an insight about the time the
system spends in each of the schemes. As with the previous
experiment, we can identify a clear correlation between the
chosen cost model and the used fault tolerance scheme as
shown in Figure 8. Using the default cost weight vector, we can
see that the system stays in active replication for recovery time
thresholds lower than three seconds. With increased thresholds,
active standby is used, and, then, schemes such as passive
standby. Since the system starts always with active replication,
a fraction of the time is always associated to active replication
regardless of the speciﬁed recovery time threshold.
For the Amazon EC2 cost model, we can see that the
system primarily choses two different states: Active replication
and passive replication. Passive replication is preferred as
it reduces more costs if less replicas are deployed on the
system since nodes can be deallocated. On the other hand,
active replication is used if passive replication cannot be used
as it provides the quickest recovery time and still consumes
considerable amounts of resources which are paid anyway by
the Amazon EC2 customer.
G. Relation between the Recovery Guarantees and the Use of
Fault Tolerance Schemes
Similar to the previous experiment, we varied the recovery
time threshold, but now with different recovery guarantees.
Requiring only gap recovery leads to lower recovery times
in comparison to precise recovery as the event replay can
be omitted. As shown in Figure 9, requiring precise recovery
keeps the system more time in active standby compared to
cases in which the user opted for gap recovery. Moreover,
using gap recovery, the system can already remain in passive
replication when a recovery threshold of 18 or more seconds
was speciﬁed.
H. CPU and Network Consumption Trade-off
In the last of our experiments, we investigated the trade-off
between saved CPU resources and network resources when
472472
varying the interval for state synchronization and checkpoints.
A smaller interval leads to a more up-to-date state and shorter
outgoing and incoming queues, which improves recovery
time, however, at the cost of network bandwidth as every
state synchronization imposes additional overhead on the
network. Figure 10 depicts the overhead for CPU and network
utilization with varying state synchronization interval. If state
synchronization is performed continuously, without pauses,
we can experience up to 600% peak overhead depending on
the nature of the application and the state. For example, an
application with average state size of 10 M B would exhibit
an overhead of up to 200%. However, the savings regarding
CPU are only marginal, hence, state synchronization should
not occur more often than every two seconds.
V. RELATED WORK
In this section we give a brief overview about fault toler-
ance techniques used in ESP systems.
Inspired by the schemes used in database systems, several
approaches based on checkpoints and logging have been pro-
posed, such as [10] and [15]. While Hwang et al. [10] proposes
upstream backup where events are logged at upstream nodes
for recovery, Gu et al. [15] combines logging with checkpoints
by introducing the sweeping checkpoint algorithm. We used an
adaption of the second approach, however, improved it so that
the checkpoint intervals are adjustable to allow us to directly
control the recovery time.
A more recent approach which does not require check-
points is the work of Koldehofe et al. [18] where safe-points
are used and track state modiﬁcations through a dependency
graph. While this approach can save the overhead of doing
and keeping checkpoints, which are not negligible, it is not
suitable to our current operator model and API as it requires a
notiﬁcation mechanism to track state modiﬁcation in relation
to the incoming events.
A system similar to StreamMine3G is the one presented by
Castro Fernandez et al. [19] where the ESP system comes with
explicit state management support, which serves for elasticity
and rollback recovery at the same time. Although both systems
(SEEP and StreamMine3G) share many similarities, SEEP
does only support a single fault tolerance scheme while our
system covers a range of well established schemes.
An approach which guarantees a user speciﬁed recovery
time similar to ours is presented by Balazinska et al. [20].
However, instead of switching between appropriate schemes
and keeping consistency guarantees, temporarily inconsistency
is introduced by forwarding only partial results due to the
unavailability of upstream operator partitions.
Finally, several approaches have been proposed to combine
more than a single fault tolerance scheme. However, they
do so with different objectives: Authors in [21] and [9]
use a combination of active replication and passive standby.
However, while Martin et al. [9] runs in active replication
during normal operation using spare and already paid cloud
resources, Zhang et al. [21] use passive standby and switch
only in failure cases to active replication. In our approach,
we combine several approaches in a single system and switch
between the schemes based on the user speciﬁed recovery time.
Using the cost weight vector, our approach can also be used
to use spare resources at no additional cost for fault tolerance
as presented by Martin et al. [9].
An approach which is closest to our adaptation mechanism
was presented by Upadhyaya et al. [22]. The authors propose
an optimization algorithm which is tailored to speciﬁc
than a whole query with the goal of
operators rather
guaranteeing a user speciﬁed recovery time. However,
in
contrast to our work, the approach is not adaptive and does
not consider resource overheads.
VI. CONCLUSION
fault
In this paper, we presented StreamMine3G, our elastic
ESP system that, to our best knowledge, is the ﬁrst ESP
system to combine several
tolerance schemes in a
single system. In order to free the user from the burden of
choosing the most appropriate scheme, the system is equipped
with a fault-tolerance controller for which users are only
required to specify a recovery time threshold, the recovery
guarantees (precise or gap recovery), and, optionally, a cost
weight vector used for resource and cost optimization. Using
the provided input,
the system will adapt during runtime,
selecting the fault-tolerance scheme that ensures the user
speciﬁed recovery time threshold while incurring the lowest
resource consumption costs. For adaption, the system uses an
estimation approach based on performance metrics collected
during the execution of the application.
ACKNOWLEDGMENT
The research leading to these results has received fund-
ing from the European Community’s Seventh Framework
Programme (FP7/2012-2015) under grant agreement number
318809 (LEADS), from the CAPES, DAAD and GIZ through
the NoPa program (TruEGrid project, 2011-2013) and by the
German Excellence Initiative, Center for Advancing Electron-
ics Dresden (cfAED), Resilience Path.
REFERENCES
[1]
[2]
[3]
[4]
[5]
open
source
mapreduce
J. Dean and S. Ghemawat, “Mapreduce: Simpliﬁed data processing on
large clusters,” Commun. ACM, vol. 51, no. 1, pp. 107–113, Jan. 2008.
“Hadoop
implementation,”
http://hadoop.apache.org/, 2014.
“Google adwords,” https://www.google.com/adwords/, 2014.
“Apache
http://samza.incubator.apache.org/, 2014.
“Apache storm - distributed and fault-tolerant realtime computation,”
https://storm.incubator.apache.org/, 2014.
stream processing
framework,”
samza
-
distributed
[6] L. Neumeyer, B. Robbins, A. Nair, and A. Kesari, “S4: Distributed
stream computing platform,” in Proceedings of the 2010 IEEE Inter-
national Conference on Data Mining Workshops, ser. ICDMW ’10.
Washington, DC, USA: IEEE Computer Society, 2010, pp. 170–177.
[7] M. A. Shah, J. M. Hellerstein, and E. Brewer, “Highly available, fault-
tolerant, parallel dataﬂows,” in Proceedings of the 2004 ACM SIGMOD
International Conference on Management of Data, ser. SIGMOD ’04.
New York, NY, USA: ACM, 2004, pp. 827–838.
J.-H. Hwang, U. Cetintemel, and S. Zdonik, “Fast and reliable stream
processing over wide area networks,” in Proceedings of the 2007 IEEE
23rd International Conference on Data Engineering Workshop, ser.
[8]
473473
ICDEW ’07. Washington, DC, USA: IEEE Computer Society, 2007,
pp. 604–613.
[9] A. Martin, C. Fetzer, and A. Brito, “Active replication at (almost) no
cost,” in Proceedings of the 2011 IEEE 30th International Symposium
on Reliable Distributed Systems, ser. SRDS ’11. Washington, DC,
USA: IEEE Computer Society, 2011, pp. 21–30.
J.-H. Hwang, M. Balazinska, A. Rasin, U. Cetintemel, M. Stonebraker,
and S. Zdonik, “High-availability algorithms for distributed stream
processing,” in Proceedings of the 21st International Conference on
Data Engineering, ser. ICDE ’05. Washington, DC, USA: IEEE
Computer Society, 2005, pp. 779–790.
[10]
[11] X. Défago, A. Schiper, and P. Urbán, “Total order broadcast and
multicast algorithms: Taxonomy and survey,” ACM Comput. Surv.,
vol. 36, no. 4, pp. 372–421, Dec. 2004.
[12] M. K. Aguilera and R. E. Strom, “Efﬁcient atomic broadcast using
deterministic merge,” in Proceedings of the Nineteenth Annual ACM
Symposium on Principles of Distributed Computing, ser. PODC ’00.
New York, NY, USA: ACM, 2000, pp. 209–218.
[13] C. Fetzer, U. Schiffel, and M. Süßraut, “An-encoding compiler: Building
safety-critical systems with commodity hardware,” in Proceedings of
the 28th International Conference on Computer Safety, Reliability, and
Security, ser. SAFECOMP ’09. Berlin, Heidelberg: Springer-Verlag,
2009, pp. 283–296.
[14] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed, “Zookeeper:
internet-scale systems,” in Proceedings
Wait-free coordination for
of
the 2010 USENIX Conference on USENIX Annual Technical
Conference, ser. USENIXATC’10. Berkeley, CA, USA: USENIX
Association, 2010, pp. 11–11. [Online]. Available: http://dl.acm.org/
citation.cfm?id=1855840.1855851
[15] Y. Gu, Z. Zhang, F. Ye, H. Yang, M. Kim, H. Lei, and Z. Liu, “An
empirical study of high availability in stream processing systems,” in
Proceedings of the 10th ACM/IFIP/USENIX International Conference
on Middleware, ser. Middleware ’09. New York, NY, USA: Springer-
Verlag New York, Inc., 2009, pp. 23:1–23:9. [Online]. Available:
http://dl.acm.org/citation.cfm?id=1656980.1657012
[16] R. E. Kalman, “A new approach to linear ﬁltering and prediction
problems,” Transactions of the ASME–Journal of Basic Engineering,
vol. 82, no. Series D, pp. 35–45, 1960.
[17] A. Martin, R. Marinho, A. Brito, and C. Fetzer, “Predicting energy
consumption with streammine3g,” in Proceedings of the 8th ACM In-
ternational Conference on Distributed Event-Based Systems, ser. DEBS
’14. New York, NY, USA: ACM, 2014, pp. 270–275.
[18] B. Koldehofe, R. Mayer, U. Ramachandran, K. Rothermel, and M. Völz,
“Rollback-recovery without checkpoints in distributed event processing
systems,” in Proceedings of the 7th ACM International Conference on
Distributed Event-based Systems, ser. DEBS ’13. New York, NY, USA:
ACM, 2013, pp. 27–38.
[19] R. Castro Fernandez, M. Migliavacca, E. Kalyvianaki, and P. Pietzuch,
“Integrating scale out and fault tolerance in stream processing using
operator state management,” in Proceedings of the 2013 ACM SIGMOD
International Conference on Management of Data, ser. SIGMOD ’13.
New York, NY, USA: ACM, 2013, pp. 725–736.
[20] M. Balazinska, H. Balakrishnan, S. Madden, and M. Stonebraker,
“Fault-tolerance in the borealis distributed stream processing system,”
in Proceedings of the 2005 ACM SIGMOD International Conference
on Management of Data, ser. SIGMOD ’05. New York, NY, USA:
ACM, 2005, pp. 13–24.
[21] Z. Zhang, Y. Gu, F. Ye, H. Yang, M. Kim, H. Lei, and Z. Liu, “A
hybrid approach to high availability in stream processing systems,”
in Proceedings of the 2010 IEEE 30th International Conference on
Distributed Computing Systems, ser. ICDCS ’10. Washington, DC,
USA: IEEE Computer Society, 2010, pp. 138–148.
[22] P. Upadhyaya, Y. Kwon, and M. Balazinska, “A latency and fault-
tolerance optimizer for online parallel query plans,” in Proceedings of
the 2011 ACM SIGMOD International Conference on Management of
Data, ser. SIGMOD ’11. New York, NY, USA: ACM, 2011, pp. 241–
252.