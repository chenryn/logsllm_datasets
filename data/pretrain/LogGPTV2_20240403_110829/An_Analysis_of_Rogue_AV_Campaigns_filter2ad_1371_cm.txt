0
.
6
0
.
4
0
.
2
0
.
0
0
.
y
t
i
s
n
e
D
4
0
.
3
0
.
2
0
.
1
0
.
0
0
.
False Positives
V
0
2
4
6
8
10
0
2
4
6
8
10
Anomaly Score
Community Score
Fig. 1. An illustration of anomaly signals.
Neither X nor Y are normally distributed,
but μY > μX , as required. The exploit
may sometimes look “normal”.
Fig. 2. A distribution of healthy commu-
nity scores using hypothetical data. The
threshold V determines what fraction of
scores result in false positives.
3.1 Model
When applying our method to detect epidemics in a community, the user selects
an appropriate client model, which uses some combination of signals that can
be measured on individual clients to quantify how surprising (anomalous) recent
behavior is. We require only that the model generate anomaly scores that are
mostly independent across healthy clients and that it quantify how surprising
recent behavior is, compared with historical behavior or a theoretical baseline.
The model for a community of servers might characterize normal behav-
ior according to performance (see an example using request response times in
Section 4), while the model for a community of web browsers might use code ex-
ecution paths (see examples using system calls in Sections 5 and 6). The example
models used in this paper could easily be reﬁned or replaced with alternatives
to match the attacks we want to detect: call stack content [8], execution traces
[10], call arguments [24], remote procedure calls [12], etc.
3.2 Anomaly Signal
The anomaly signal decouples the choice of model from the rest of the system;
any model that satisﬁes the properties explained in this section may be used
with Syzygy. Each client keeps the server apprised of the client’s anomaly score,
the current value of the client’s anomaly signal. This score is a measure of how
unusual recent behavior is compared to a model of client behavior: a higher score
indicates more surprising behavior than a lower score. (This is sometimes called
the IS statistic [18] or behavioral distance [11].)
The distribution of anomaly scores generated by a healthy client (X) must
have a mean (μX) that is less than the mean (μY ) of the anomaly score distribu-
tion of an infected client (Y ), so we require μY > μX + δ. The larger the δ, the
better, though any positive δ will suﬃce. Figure 1 illustrates two valid anomaly
signal distributions, where X and Y are random variables such that both have
ﬁnite mean and ﬁnite, positive variance.
More generally, let the anomaly scores from healthy client i, denoted ai, be
distributed like Xi (written ai ∼ Xi) and let ai ∼ Yi when client i is infected.
Assume, without loss of generality, that all clients have the same distribution, i.e.,
let Xi ∼ X and Yi ∼ Y . The distributions may be standardized to enforce this
Community Epidemic Detection Using Time-Correlated Anomalies
365
assumption, because only the mean and variance are relevant to our asymptotic
results. If infected behavior does not diﬀer from normal behavior, then δ will be
unacceptably small (even negative); this can be resolved by reﬁning the model
to include more relevant signals or adjusting the model to amplify surprising
behaviors. In this paper, we use two simple models (see Sections 4.1 and 5.1)
that share a similar anomaly score computation (see Section 4.1), and both
provided suﬃciently large δ values to detect a variety of exploits.
3.3 Epidemic Detection
The Syzygy server computes the average anomaly score among the active clients;
this community score C represents the state of the community. If C > V , for a
tunable threshold V , the server reports an epidemic. Consider a healthy commu-
nity of n clients and let ai ∼ X. Then, by the Central Limit Theorem, as n → ∞,
the community scores are distributed normally with mean μX and variance σ2
n :
X
C = averagei(ai) =
1
n
(cid:7)
i
(X) ∼ Norm(μX ,
σ2
X
n ).
When E(|X|3) = ρ  0 such that
∀x, n, |Fn(x) − Φ(x)| ≤ Bρ
√
Consider now when some number of clients d ≤ n of the community have been
exploited. The community score, as n, d → ∞, will be
(n − d)μX + dμY
n−d(cid:7)
d(cid:7)
n.
n
σX
σ3
X
(cid:14)
(cid:13)
(cid:15)
, (n − d)σ2
X + dσ2
Y
n2
(cid:16)
.
n
C =
1
n
X +
Y
i=1
i=1
∼ Norm
The rate of convergence guarantees that we get this asymptotic behavior at
relatively small values of n and d, and even when d  V , Syzygy reports an epidemic.
Deﬁned as μY − μX . Intuitively, the average distance between anomaly
scores generated by healthy versus infected clients. One kind of mimicry
attack drives δ toward zero.
The rate of a rate-limited mimicry attack: the application appears
healthy a fraction 1 − r of the time and infected a fraction r of the
time.
True positive rate or detection rate. P (E|¬H).
True negative rate. P (¬E|H).
False positive rate, or Type I classiﬁcation error rate. P (E|H).
False negative rate, or Type II classiﬁcation error rate. P (¬E|¬H).
TP
TN
FP
FN
F1 Measure A summary metric with precision and recall weighted equally:
2T P
2T P +F P +F N .
(cid:18)
(cid:18)
(cid:19)
(cid:19)
X
n
2π
√
n
μX , σ2
(V −μH )
σH
(cid:17) ∞
α e− x2
2 dx. Let H ∼ Norm
precisely the value of the parametrized Q-function, the complement of the normal
cdf: Q(α) ≡ 1√
be the distribution of
community scores in a healthy community of size n. The probability that a
randomly selected community score will be a false positive is FP = P (C > V ) =
. Table 1 lists the signiﬁcant terms and metrics used in this paper.
Q
This analysis relies on two modest assumptions. First, the parameters μX and
σX must characterize the future distribution of anomaly scores. A model that
is out-of-date or produced with biased training data, for example, may produce
anomaly scores inconsistent with the expected distribution. In Section 6.4 we
explore the impact of using on one system a model produced for a diﬀerent one
and in Section 5.2 we show that even relatively heterogeneous machines pro-
duce predictable community score distributions. It is straightforward to detect
when observed behavior disagrees with expectation, and the solution is to re-
train the model. Second, during normal operation, client anomaly scores should
be mostly independent. In situations like a network-distributed software upgrade,
Community Epidemic Detection Using Time-Correlated Anomalies
367
innocuous dependencies may cause correlated behavior (i.e., correlated behavior
without a malicious cause, which is our deﬁnition of a false positive). Indeed,
it is indistinguishable from an attack except that one change to the software is
authorized and the other is not. Such false alarms are easily avoided by mak-
ing information about authorized changes to monitored applications available
to Syzygy. Other sources of accidentally correlated behavior are quite rare; we
observed no false alarms at all in a deployment with real users (see Section 5).
4 Detection Experiments
We ﬁrst test Syzygy’s ability to detect epidemics in a community using a cluster
of 22 machines running unmodiﬁed instances of the Apache web server. Each
machine has four cores (two dual core AMD Opteron 265 processors), 7 GB of
main memory, and the Fedora Core 6 distribution of Linux. Each client serves
streams of requests generated by a workload script. The workload generator,
at exponentially distributed random times, makes requests from a list of 178
available HTML and PHP pages that includes several pages that do not exist
and two pages for which the requester does not have read permission. We run
the workload generator for 100,000 requests (∼2.8 hours) to train the model,
then use those same training traces to set V so that we expect to get one false
positive per week (see Section 3.3 for how we do this; also see Section 5.2 for
more on false positives). We use Apache’s existing logging mechanisms to record
measurements (e.g., response times).
For this community, we aim to detect the following classes of attack: denial
of service (DoS), resource exhaustion, content spooﬁng, and privilege escalation.
Thus, we pick a client model that is likely to detect such attacks (see Section 4.1).
We test Syzygy with two DoS attacks that prevent Apache from serving 1%
or 10% of requests, at random, respectively; two resource exhaustion attacks
that allow Apache to continue serving requests but gradually consume memory
or CPU time, respectively; three content spooﬁng attacks that cause (i) PHP
pages to be served in place of previously non-existent pages, (ii) PHP pages to
be served in the place of certain HTML pages, or (iii) HTML pages to be served
in place of certain PHP pages; and a privilege escalation attack that makes all
page accesses authorized (no 403 Errors). We ﬁnd that Syzygy can achieve high
detection rates for these attacks with no false positives (see Section 4.2).
The clients in these experiments are homogeneous; in Section 5, we explore
the eﬀects of heterogenous hardware and varying user behavior with a deploy-
ment using an interactive application (the Firefox web browser). Section 6 con-
tains additional experiments, in a more controlled environment, that explore the
properties of much larger communities (thousands of clients) and more advanced
exploits (capable of various degrees of mimicry).
4.1 Model
Assume that our security goal for this community is to ensure that clients are
serving requests according to expected performance; that is, the request response
368
A.J. Oliner, A.V. Kulkarni, and A. Aiken
behavior should be consistent over time. During training, the model computes
a frequency distribution of request response times and the maximum observed
time between consecutive requests. This is just one choice of model and is not
intrinsic to Syzygy.
When a request is made of the server, the model increments the counter asso-
ciated with the response time s in a table indexed by response times (10 μsecond
precision). From this frequency distribution, we compute a density function Si
by dividing each entry by the total number of observed response times. Thus,
Si(s) is the fraction of times that response time s was observed on client i.
To incorporate timing in the model, which can help identify the absence of
normal behavior (such as during a denial of service attack), we record the time
between the start of each consecutive pair of requests. The model measures
these times only when the application is active. A client is active when it reports
its ﬁrst anomaly score and becomes inactive after reporting an anomaly score
accompanied by the END message. (See below for when this token is generated.)
From these data, we set a silence threshold Ti for each client i, which we initially
pick to be the maximum time between any two consecutive requests.
Monitoring. On the client, Syzygy monitors all requests made to the applica-
tion. In addition, Syzygy may inject two kinds of artiﬁcial measurements into
the sequence. The ﬁrst, called END, indicates that the application has terminated
(switched to inactive); Syzygy generates an END token when the application exits
cleanly, terminates abruptly such as due to an error, or when the Syzygy client
is closed cleanly. If an active client stops reporting scores for longer than the
timeout threshold, currently set to 2Ti seconds, then the Syzygy server marks
that client inactive without fabricating a token. The second artiﬁcial measure-
ment, a hiaton [37] denoted X, indicates that no measurements were generated
for longer than Ti seconds, including any Xs produced via this process. In other
words, at the start of each request, a timer starts; when this timer exceeds Ti,
Syzygy generates a hiaton and resets the timer.
Each client maintains a window of the most recent Wi request response times,
including the fabricated hiatons and END tokens. From this window, we compute
the density function Ri, analogous to Si, above. Thus, Ri(s) is the fraction of
times measurement s appears in the previous Wi measurements on client i.
Anomaly Signal. Let ai be the most recent anomaly score and Wi be the size
of the recent window for client i. The units of ai and Wi may depend on the par-
ticular choice of model, but should be consistent across clients. In this paper, we
measure the anomaly signal in bits and the window size in number of measure-
ments. Our implementation computes ai using Kullback-Liebler (KL) divergence
with a base-2 logarithm. Roughly, this measures the information gained by seeing
the recent window, having already observed the historical behavior. Speciﬁcally,