title:WireCAP: a novel packet capture engine for commodity NICs in high-speed
networks
author:Wenji Wu and
Phil DeMar
WireCAP: a Novel Packet Capture Engine for Commodity 
NICs in High-speed Networks
Wenji Wu 
Fermilab 
Batavia IL, USA 
PI:EMAIL 
Fermilab 
Phil DeMar 
Batavia IL, USA 
PI:EMAIL 
introduces 
two 
packet-processing 
architectures.  WireCAP 
application.  Experiments 
ABSTRACT 
  Packet  capture  is  an  essential  function  for  many  network 
applications.  However,  packet  drop  is  a  major  problem  with 
packet capture in high-speed networks. 
  This  paper  presents  WireCAP,  a  novel  packet  capture  engine 
for  commodity  network  interface  cards  (NICs)  in  high-speed 
networks.  WireCAP  provides  lossless  zero-copy  packet  capture 
and  delivery  services  by  exploiting  multi-queue  NICs  and 
multicore 
new 
mechanisms—the  ring-buffer-pool  mechanism  and  the  buddy-
group-based  offloading  mechanism—to  address  the  packet  drop 
problem  of  packet  capture  in  high-speed  network.  WireCAP  is 
efficient.  It  also  facilitates  the  design  and  operation  of  a  user-
space 
have 
demonstrated  that  WireCAP  achieves  better  packet  capture 
performance when compared to existing packet capture engines.  
In  addition,  WireCAP  implements  a  packet  transmit  function 
that allows captured packets to be forwarded, potentially after the 
packets are modified or inspected in flight. Therefore, WireCAP 
can  be  used  to  support  middlebox-type  applications.  Thus,  at  a 
high  level,  WireCAP  provides  a  new  packet  I/O  framework  for 
commodity NICs in high-speed networks.    
Categories and Subject Descriptors 
D.4.4  [Operating  Systems]:  Communications  Management; 
C.2.3 [Network Operations]: Network monitoring 
General Terms 
Measurement, Performance 
Keywords 
Linux kernel; network packet capture; multi-core system 
1.  INTRODUCTION 
  Packet  capture  is  an  essential  function  for  many  network 
applications, including intrusion detection systems (IDS) [1, 2, 3], 
and packet-based network performance analysis applications [4]. 
Packets are typically captured from the wire, temporarily stored at 
a data capture buffer, and finally delivered to the applications for 
processing.  Because  these  operations  are  performed  on  a  per-
packet basis, packet capture is typically computationally and I/O 
throughput  intensive.  In  high-speed  networks  (10  Gbps  and 
(c)  2014  Association  for  Computing  Machinery.  ACM  acknowledges  that 
this contribution was authored or co-authored by an employee, contractor or 
affiliate  of  the  United  States  government.  As  such,  the  United  States 
Government retains a nonexclusive, royalty-free right to publish or reproduce 
this article, or to allow others to do so, for Government purposes only. 
IMC’14, November 5–7, 2014, Vancouver, BC, Canada. 
Copyright 2014 ACM 978-1-4503-3213-2/14/11…$15.00. 
http://dx.doi.org/10.1145/2663716.2663736. 
above), packet capture faces significant performance challenges.  
  Packet  drop  is  a  major  problem  with  packet  capture  in  high-
speed  networks.  There  are  two  types  of  packet  drop:  packet 
capture  drop  and  packet  delivery  drop.  Packet  capture  drop  is 
mainly  caused  by  the  inabilities  of  packet  capture  to  keep  pace 
with  the  incoming  packet  rate.  Consequently,  packets  may  be 
dropped because they cannot be captured in time. Packet delivery 
drop is mainly caused by the inability of the application to keep 
pace with the packet capture rate. Consequently, the data capture 
buffer overflows, and packet drops occur even when 100% of the 
network traffic is captured from the wire. Any type of packet drop 
will  degrade  the  accuracy  and  integrity  of  network  monitoring 
applications [2, 5]. Thus, avoiding packet drops is a fundamental 
design goal in packet capture tools. 
There  are  two  approaches  to  performing  packet  capture.  The 
first approach is to use a dedicated packet capture card to perform 
the function in hardware [6, 7]. This approach requires the least 
amount  of  CPU  intervention,  thus  saving  the  CPU  for  packet 
processing. A dedicated packet capture card can ensure that 100% 
of the network packets are captured and delivered to applications 
without loss. However, this approach demands custom hardware 
solutions, which tend to be more costly, relatively inflexible, and 
not very scalable.  
to  drops.  However,  with 
An  alternative  approach  is  to  use  a  commodity  system  with  a 
commodity NIC to perform packet capture. In this approach, the 
commodity  NIC  is  put  into  promiscuous  mode  to  intercept 
network  packets.  A  packet  capture  engine  (a  software  driver) 
receives  the  intercepted  packets  and  provides  support  to  allow 
user-space  applications  to  access  the  captured  packets.  This 
capture  solution  depends  mainly  on  the  software-based  packet 
capture  engine,  which  is  flexible  and  cost-effective,  but  requires 
significant  system  CPU  and  memory  resources.  Therefore,  this 
solution  is  not  suitable  for  resource-limited  systems  where 
resource  competition  between  packet  capture  and  packet 
processing  might 
recent 
technological  advances  in  multicore  platforms  and  multi-queue 
NICs,  this  approach  becomes  more  appealing  due  to  the 
availability of ample system CPU resources and I/O throughputs, 
and  paves  the  way  for  a  new  paradigm  in  packet  capturing  and 
processing [8, 9, 10].  
  The  new  paradigm  typically  works  as  follows.  A  multi-queue 
NIC  is  logically  partitioned  into  n  receive  queues,  with  each 
queue  tied  to  a  distinct  core  of  a  multicore  system  (Figure  1). 
Packets are distributed across the queues using a hardware-based 
traffic-steering  mechanism,  such  as  receive-side  scaling  (RSS) 
[11]. A thread (or process) of a packet-processing application runs 
on each core that has a tied queue. Each thread captures packets 
via a packet capture engine and handles a portion of the overall 
traffic.  On  a  multicore  system,  there  are  several  programming 
models (e.g., the run-to-completion model and the pipeline model 
[12])  for  a  packet-processing  application.  Here,  the  application 
lead 
395
Core 1
Thread
Core 2
Thread
e
r
u
t
p
a
C
e
n
i
g
n
E
...
e
r
u
t
p
a
C
e
n
i
g
n
E
Core n
Thread
e
r
u
t
p
a
C
e
n
i
g
n
E
1
Q
R
2
Q
R
...
n
Q
R
Traffic Steering
Network Traffic
e
r
o
C
-
i
t
l
u
M
m
e
t
s
y
S
t
s
o
H
e
u
e
u
Q
-
i
t
l
C
I
N
u
M
Figure 1. A new packet capturing and processing paradigm 
may  be  of  any  type.  This  new  paradigm  essentially  exploits  the 
computing parallelism of multicore systems and the inherent data 
parallelism  of  network  traffic  to  accelerate  packet  capturing  and 
processing.  A  basic  assumption  associated  with  this  approach  is 
that  the  hardware-based  balancing  mechanism  is  capable  of 
evenly distributing the incoming traffic among cores. Thus, each 
core would handle 1/n of the overall traffic and the packet rate at 
each core would be reduced to 1/n of the overall packet rate from 
the  network,  with  a  significantly  reduced  chance  of  causing  a 
packet  drop.  However,  this  assumption  is  often  not  the  case  in 
practice  [8].  Typically,  a  NIC’s  traffic-steering  mechanism 
distributes packets to cores based on a per-flow policy that assigns 
packets of the same flow to the same core. A flow is defined by 
one  or  more  fields  of  the  IP  5-tuple.  Such  a  traffic-steering 
mechanism  maintains  core  affinity  in  network  processing  [11], 
helping to preserve application logic (i.e., packets belonging to the 
same flow must be delivered to the same application). However, 
this  method  of  traffic  steering  can  lead  to  a  load  imbalance 
condition in which certain cores become overloaded while others 
remain  idle.  In  the  worst-case  scenario,  a  single  core  will  be 
flooded with all the network traffic at wire speed.  
  There are two types of load imbalance. The first type is a short-
term load imbalance on one or several cores. In this situation, an 
overloaded  core  experiences  bursts  of  packets  on  a  short  scale. 
Here, “short” typically refers to time intervals up to 100 – 500 ms 
[13]. The second type is a long-term load imbalance, which may 
be due to an uneven distribution of flow groups in the NIC. Our 
research  reveals  that  (1)  load  imbalance  of  either  type  occurs 
frequently on multicore systems; and (2) existing packet capture 
engines (e.g., PF_RING [14], NETMAP [15], and DNA [16]) can 
suffer  significant  packet  drops  when  they  experience  load 
imbalance  of  either  type  in  a  multicore  system,  due  to  one  or 
several of the following limitations: inability to capture packets at 
wire  speed,  limited  buffering  capability,  and  lacking  of  an 
effective  offloading  mechanism 
long 
imbalance.  
In  order  to  avoid  packet  drops,  load  imbalances  on  multicore 
systems must be handled properly. There are several approaches 
for  solving  this  problem.  A  first  approach  is  to  apply  a  round-
robin traffic steering mechanism at the NIC level to distribute the 
traffic  evenly  across  the  queues.  However,  this  approach  cannot 
preserve  the  application  logic  (see  Section  2.3).  A  second 
approach  is  to  use  existing  packet  capture  engines  (e.g.,  DNA) 
and  handle  load  imbalance  in  the  application  layer.  But  an 
application  in  user  space  has  little  knowledge  of  low-level  layer 
conditions,  and  cannot  effectively  handle  load  imbalance  (see 
Section 2.3). A third approach is to design a new packet-capture 
engine that addresses load imbalance at the packet-capture level. 
A  packet-capture  engine  has  full  knowledge  of  low-level  layer 
to  address 
long-term 
396
conditions,  placing  it  in  a  better  position  to  deal  with  load 
imbalance.  In  addition,  this  approach  simplifies  the  design  of  a 
packet-processing application. In this paper, we focus on the third 
approach. 
This  paper  presents  WireCAP,  a  novel  packet  capture  engine 
for  commodity  NICs  in  high-speed  networks.  WireCAP  is 
designed  to  support  the  packet  capturing  and  processing  model 
shown in Figure 1. It has several salient features. 
(1)  WireCAP  provides  lossless  packet  capture  and  delivery 
services  by  exploiting  multi-queue  NICs  and  multicore 
architectures. We have designed two new mechanisms to handle 
load imbalance in order to avoid packet drops. 
  For  short-term  load  imbalance,  we  design  and  implement  a 
ring-buffer-pool mechanism. A ring buffer pool, which consists of 
chunks of packet buffers*, is allocated for each receive queue in 
kernel. Through dynamic packet buffer management, each chunk 
of packet buffers can be used to receive packets flowing through 
the network, and temporarily store received packets. A ring buffer 
pool’s  capacity  is  configurable.  When  a  large  pool  capacity  is 
configured, a ring buffer pool can provide sufficient buffering at 
the NIC’s receive ring level to accommodate short-term bursts of 
packets.  
  To handle long-term load imbalance, we apply a buddy-group-
based  offloading  mechanism.  The  basic  idea  is  simple:  a  busy 
packet capture engine offloads some of its traffic to less busy or 
idle  queues  (cores)  where  the  traffic  can  be  processed  by  other 
threads. However, the challenge of our design is how to preserve 
application  logic—traffic  belonging  to  the  same  flow  must  be 
delivered to the same application when multiple applications are 
running in the system. We introduce a buddy group concept: the 
receive  queues  accessed  by  threads  (or  processes)  of  a  single 
application  can  form  a  buddy  group.  Traffic  offloading  is  only 
allowed within a buddy group.  
several 
optimization  techniques—including  pre-allocated  large  packet 
buffers,  packet-level  batching  processing,  and  zero-copy—to 
reduce the packet capture and delivery costs. These optimization 
techniques  have  been  used  in  the  past  and  are  well  understood 
[14—16,  23].  The  challenge  in  designing  WireCAP  was  to 
understand how to combine these techniques with our ring-buffer-
pool  and  buddy-group-based  offloading  mechanisms 
in  an 
effective solution to achieve high performance.  
In high-speed networks, excessive data copying results in poor 
performance  [15,  17,  18,  19].  Zero-copy  is  widely  used  in  our 
design.  WireCAP  can  achieve  zero-copy  packet  capture  and 
delivery (i.e., a network packet can be captured and delivered to 
an application with zero-copy), and zero-copy forwarding (i.e., a 
captured packet can be forwarded with zero-copy).  
  We specifically separate short-term load imbalance from long-
term load imbalance and address them appropriately. When only 
short-term  load  imbalance  occurs,  WireCAP  not  only  avoids 
packet drops but also ensures that packets belonging to the same 
flow  are  handled  in  the  same  core.  This  design  improves 
performance. 
the  design  of  a  packet-processing 
application can be simplified. WireCAP addresses load imbalance 
in  the  packet-capture  engine  level  to  avoid  packet  drops. 