store them in their (unbounded) buffers. The protocol proceeds
in epochs, where after each epoch, a new batch of transactions is
appended to the committed log. At the beginning of each epoch,
nodes choose a subset of the transactions in their buffer (by a policy
we will deﬁne shortly), and provide them as input to an instance
of a randomized agreement protocol. At the end of the agreement
protocol, the ﬁnal set of transactions for this epoch is chosen.
At this high level, our approach is similar to existing asynch-
ronous atomic broadcast protocols, and in particular to Cachin
et al. [15], the basis for a large scale transaction processing sys-
tem (SINTRA). Like ours, Cachin’s protocol is centered around
an instance of the Asynchronous Common Subset (ACS) primitive.
Roughly speaking, the ACS primitive allows each node to propose
a value, and guarantees that every node outputs a common vector
containing the input values of at least N − 2 f correct nodes. It is
trivial to build atomic broadcast from this primitive — each node
simply proposes a subset of transactions from the front its queue,
and outputs the union of the elements in the agreed-upon vector.
However, there are two important challenges.
Challenge 1: Achieving censorship resilience. The cost of ACS
depends directly on size of the transaction sets proposed by each
node. Since the output vector contains at least N − f such sets,
we can therefore improve the overall efﬁciency by ensuring that
nodes propose mostly disjoint sets of transactions, thus committing
more distinct transactions in one batch for the same cost. Therefore
instead of simply choosing the ﬁrst element(s) from its buffer (as
in CKPS01 [15]), each node in our protocol proposes a randomly
chosen sample, such that each transaction is, on average, proposed
by only one node.
However, implemented naïvely, this optimization would compro-
mise censorship resilience, since the ACS primitive allows the adver-
sary to choose which nodes’ proposals are ultimately included. The
adversary could selectively censor a transaction excluding whichever
node(s) propose it. We avoid this pitfall by using threshold encryp-
tion, which prevents the adversary from learning which transac-
tions are proposed by which nodes, until after agreement is already
reached. The full protocol will be described in Section 4.3.
Challenge 2: Practical throughput. Although the theoretical
feasibility of asynchronous ACS and atomic broadcast have been
known [9, 15, 17], their practical performance is not. To the best of
our knowledge, the only other work that implemented ACS was by
Cachin and Portiz [17], who showed that they could attain a through-
put of 0.4 tx/sec over a wide area network. Therefore, an interesting
question is whether such protocols can attain high throughput in
practice.
In this paper, we show that by stitching together a carefully chosen
array of sub-components, we can efﬁciently instantiate ACS and
attain much greater throughput both asymptotically and in practice.
Notably, we improve the asymptotic cost (per node) of ACS from
O(N2) (as in Cachin et al. [15, 17]) to O(1). Since the components
35we cherry-pick have not been presented together before (to our
knowledge), we provide a self-contained description of the whole
construction in Section 4.4.
Modular protocol composition. We are now ready to present our
constructions formally. Before doing so, we make a remark about
the style of our presentation. We deﬁne our protocols in a modu-
lar style, where each protocol may run several instances of other
(sub)protocols. The outer protocol can provide input to and re-
ceive output from the subprotocol. A node may begin executing
a (sub)protocol even before providing it input (e.g., if it receives
messages from other nodes).
It is essential to isolate such (sub)protocol instances to ensure that
messages pertaining to one instance cannot be replayed in another.
This is achieved in practice by associating to each (sub)protocol
instance a unique string (a session identiﬁer), tagging any messages
sent or received in this (sub)protocol with this identiﬁer, and routing
messages accordingly. We suppress these message tags in our proto-
col descriptions for ease of reading. We use brackets to distinguish
between tagged instances of a subprotocol. For example, RBC[i]
denotes an ith instance of the RBC subprotocol.
We implicitly assume that asynchronous communications be-
tween parties are over authenticated asynchronous channels. In
reality, such channels could be instantiated using TLS sockets, for
example, as we discuss in Section 5.
To distinguish different message types sent between parties within
a protocol, we use a label in typewriter font (e.g., VAL(m) indi-
cates a message m of type VAL).
4.3 Constructing HoneyBadgerBFT from
Asynchronous Common Subset
Building block: ACS. Our main building block is a primitive called
asynchronous common subset (ACS). The theoretical feasibility of
constructing ACS has been demonstrated in several works [9, 15].
In this section, we will present the formal deﬁnition of ACS and use
it as a blackbox to construct HoneyBadgerBFT. Later in Section 4.4,
we will show that by combining several constructions that were
somewhat overlooked in the past, we can instantiate ACS efﬁciently!
More formally, an ACS protocol satisﬁes the following properties:
• (Validity) If a correct node outputs a set v, then |v| ≥ N − f and v
contains the inputs of at least N − 2 f correct nodes.
• (Agreement) If a correct node outputs v, then every node outputs
• (Totality) If N − f correct nodes receive an input, then all correct
v.
nodes produce an output.
Building block: threshold encryption. A threshold encryption
scheme TPKE is a cryptographic primitive that allows any party
to encrypt a message to a master public key, such that the network
nodes must work together to decrypt it. Once f + 1 correct nodes
compute and reveal decryption shares for a ciphertext, the plain-
text can be recovered; until at least one correct node reveals its
decryption share, the attacker learns nothing about the plaintext. A
threshold scheme provides the following interface:
• TPKE.Setup(1λ ) → PK,{SKi} generates a public encryption
• TPKE.Enc(PK,m) → C encrypts a message m
• TPKE.DecShare(SKi,C) → σi produces the ith share of the de-
cryption (or ⊥ if C is malformed)
• TPKE.Dec(PK,C,{i,σi}) → m combines a set of decryption
shares {i,σi} from at least f + 1 parties obtain the plaintext m (or,
if C contains invalid shares, then the invalid shares are identiﬁed).
key PK, along with secret keys for each party SKi
In our concrete instantiation, we use the threshold encryption scheme
of Baek and Zheng [7]. This scheme is also robust (as required by
our protocol), which means that even for an adversarially generated
ciphertext C, at most one plaintext (besides ⊥) can be recovered.
Note that we assume TPKE.Dec effectively identiﬁes invalid de-
cryption shares among the inputs. Finally, the scheme satisﬁes the
obvious correctness properties, as well as a threshold version of the
IND-CPA game.3
Atomic broadcast from ACS. We now describe in more detail our
atomic broadcast protocol, deﬁned in Figure 1.
As mentioned, this protocol is centered around an instance of ACS.
In order to obtain scalable efﬁciency, we choose a batching policy.
We let B be a batch size, and will commit Ω(B) transactions in each
epoch. Each node proposes B/N transactions from its queue. To
ensure that nodes propose mostly distinct transactions, we randomly
select these transactions from the ﬁrst B in each queue.
As we will see in Section 4.4, our ACS instantiation has a total
communication cost of O(N2|v| + λ N3 logN), where |v| bounds
the size of any node’s input. We therefore choose a batch size
B = Ω(λ N2 logN) so that the contribution from each node (B/N)
absorbs this additive overhead.
In order to prevent the adversary from inﬂuencing the outcome we
use a threshold encryption scheme, as described below. In a nutshell,
each node chooses a set of transactions, and then encrypts it. Each
node then passes the encryption as input to the ACS subroutine. The
output of ACS is therefore a vector of ciphertexts. The ciphertexts
are decrypted once the ACS is complete. This guarantees that the
set of transactions is fully determined before the adversary learns
the particular contents of the proposals made by each node. This
guarantees that an adversary cannot selectively prevent a transaction
from being committed once it is in the front of the queue at enough
correct nodes.
4.4 Instantiating ACS Efﬁciently
Cachin et al. present a protocol we call CKPS01 that (implic-
itly) reduces ACS to multi-valued validated Byzantine agreement
(MVBA) [15]. Roughly speaking, MVBA allows nodes to propose
values satisfying a predicate, one of which is ultimately chosen. The
reduction is simple: the validation predicate says that the output
must be a vector of signed inputs from at least N − f parties. Un-
fortunately, the MVBA primitive agreement becomes a bottleneck,
because the only construction we know of incurs an overhead of
O(N3|v|).
We avoid this bottleneck by using an alternative instantiation of
ACS that sidesteps MVBA entirely. The instantiation we use is due
to Ben-Or et al. [9] and has, in our view, been somewhat overlooked.
In fact, it predates CKPS01 [15], and was initially developed for a
mostly unrelated purpose (as a tool for achieving efﬁcient asynch-
ronous multi-party computation [9]). This protocol is a reduction
from ACS to reliable broadcast (RBC) and asynchronous binary
Byzantine agreement (ABA). Only recently do we know of efﬁcient
constructions for these subcomponents, which we explain shortly.
At a high level, the ACS protocol proceeds in two main phases. In
the ﬁrst phase, each node Pi uses RBC to disseminate its proposed
value to the other nodes, followed by ABA to decide on a bit vector
that indicates which RBCs have successfully completed.
We now brieﬂy explain the RBC and ABA constructions before
explaing the Ben-Or protocol in more detail.
3The Baek and Zheng threshold scheme also satisﬁes (the threshold
equivalent of) the stronger IND-CCA game, but this is not required
by our protocol.
36Algorithm HoneyBadgerBFT (for node Pi)
Let B = Ω(λ N2 logN) be the batch size parameter.
Let PK be the public key received from TPKE.Setup (executed
by a dealer), and let SKi be the secret key for Pi.
Let buf := [ ] be a FIFO queue of input transactions.
Proceed in consecutive epochs numbered r:
// Step 1: Random selection and encryption
• let proposed be a random selection of (cid:98)B/N(cid:99) transactions from
• encrypt x := TPKE.Enc(PK, proposed)
the ﬁrst B elements of buf
// Step 2: Agreement on ciphertexts
• pass x as input to ACS[r] //see Figure 4
• receive {v j} j∈S, where S ⊂ [1..N], from ACS[r]
// Step 3: Decryption
• for each j ∈ S:
let e j := TPKE.DecShare(SKi,v j)
multicast DEC(r, j,i,e j)
wait
DEC(r, j,k,e j,k)
decode y j := TPKE.Dec(PK,{(k,e j,k)})
to receive at
least
f + 1 messages of the form
• let blockr := sorted(∪ j∈S{y j}), such that blockr is sorted in a
• set buf := buf − blockr
canonical order (e.g., lexicographically)
Figure 1: HoneyBadgerBFT.
Communication-optimal reliable roadcast. An asynchronous re-
liable broadcast channel satisﬁes the following properties:
• (Agreement) If any two correct nodes deliver v and v(cid:48), then v = v(cid:48).
• (Totality) If any correct node delivers v, then all correct nodes
• (Validity) If the sender is correct and inputs v, then all correct
deliver v
nodes deliver v
While Bracha’s [13] classic reliable broadcast protocol requires
O(N2|v|) bits of total communication in order to broadcast a mes-
sage of size |v|, Cachin and Tessaro [18] observed that erasure cod-
ing can reduce this cost to merely O(N|v| + λ N2 logN), even in the
worst case. This is a signiﬁcant improvement for large messages (i.e.,
when |v| (cid:29) λ N logN), which, (looking back to Section 4.3) guides
our choice of batch size. The use of erasure coding here induces at
most a small constant factor of overhead, equal to
N
N−2 f < 3.
If the sender is correct, the total running time is three (asynch-
ronous) rounds; and in any case, at most two rounds elapse between
when the ﬁrst correct node outputs a value and the last outputs a
value. The reliable broadcast algorithm shown in Figure 2.
Binary Agreement. Binary agreement is a standard primitive that
allows nodes to agree on the value of a single bit. More formally,
binary agreement guarantees three properties:
• (Agreement) If any correct node outputs the bit b, then every
• (Termination) If all correct nodes receive input, then every correct
• (Validity) If any correct node outputs b, then at least one correct
correct node outputs b.
node outputs a bit.
node received b as input.
The validity property implies unanimity: if all of the correct nodes
receive the same input value b, then b must be the decided value.
On the other hand, if at any point two nodes receive different inputs,
Algorthm RBC (for party Pi, with sender PSender)
• upon input(v) (if Pi = PSender):
let {s j} j∈[N] be the blocks of an (N − 2 f ,N)-erasure coding
scheme applied to v
let h be a Merkle tree root computed over {s j}
send VAL(h,b j,s j) to each party P j, where b j is the jth
Merkle tree branch
• upon receiving VAL(h,bi,si) from PSender,
• upon receiving ECHO(h,b j,s j) from party P j,
multicast ECHO(h,bi,si)
check that b j is a valid Merkle branch for root h and leaf s j,
and otherwise discard
• upon receiving valid ECHO(h,·,·) messages from N − f distinct
parties,
– interpolate {s(cid:48)
j} from any N − 2 f leaves received
– recompute Merkle root h(cid:48) and if h(cid:48) (cid:54)= h then abort
– if READY(h) has not yet been sent, multicast READY(h)
• upon receiving f + 1 matching READY(h) messages, if READY
• upon receiving 2 f + 1 matching READY(h) messages, wait for
has not yet been sent, multicast READY(h)
N − 2 f ECHO messages, then decode v
Figure 2: Reliable broadcast algorithm, adapted from Bracha’s
broadcast [13], with erasure codes to improve efﬁciency [18]
then the adversary may force the decision to either value even before
the remaining nodes receive input.
We instantiate this primitive with a protocol from Moustefaoui et
al. [43], which is based on a cryptographic common coin. We defer
explanation of this instantiation to the online full version [42]. Its
expected running time is O(1), and in fact completes within O(k)
rounds with probability 1− 2−k. The communication complexity
per node is O(Nλ ), which is due primarily to threshold cryptography
used in the common coin.
Agreeing on a subset of proposed values. Putting the above pieces
together, we use a protocol from Ben-Or et al. [9] to agree on a set
of values containing the entire proposals of at least N − f nodes.
At a high level, this protocol proceeds in two main phases. In the
ﬁrst phase, each node Pi uses Reliable Broadcast to disseminate its
proposed value to the other nodes. In the second stage, N concurrent
instances of binary Byzantine agreement are used to agree on a bit
vector {b j} j∈[1..N], where b j = 1 indicates that P j’s proposed value
is included in the ﬁnal set.
Actually the simple description above conceals a subtle challenge,
for which Ben-Or provide a clever solution.
A naïve attempt at an implementation of the above sketch would
have each node to wait for the ﬁrst (N − f ) broadcasts to complete,
and then propose 1 for the binary agreement instances corresponding
to those and 0 for all the others. However, correct nodes might
observe the broadcasts complete in a different order. Since binary
agreement only guarantees that the output is 1 if all the correct nodes