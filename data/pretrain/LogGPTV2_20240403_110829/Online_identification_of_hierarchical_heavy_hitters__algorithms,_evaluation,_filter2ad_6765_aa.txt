title:Online identification of hierarchical heavy hitters: algorithms, evaluation,
and applications
author:Yin Zhang and
Sumeet Singh and
Subhabrata Sen and
Nick G. Duffield and
Carsten Lund
Online Identiﬁcation of Hierarchical Heavy Hitters:
Algorithms, Evaluation, and Applications
Yin Zhang? Sumeet Singhx Subhabrata Sen? Nick Dufﬁeld? Carsten Lund?
AT&T Labs – Research, Florham Park, NJ 07932, USA?
CSE Department, University of California, San Diego, CA 92040, USAx
fyzhang,sen,dufﬁeld,PI:EMAIL PI:EMAIL
ABSTRACT
In trafﬁc monitoring, accounting, and network anomaly detection, it
is often important to be able to detect high-volume trafﬁc clusters in
near real-time. Such heavy-hitter trafﬁc clusters are often hierarchi-
cal (i.e., they may occur at different aggregation levels like ranges of
IP addresses) and possibly multidimensional (i.e., they may involve
the combination of different IP header ﬁelds like IP addresses, port
numbers, and protocol). Without prior knowledge about the precise
structures of such trafﬁc clusters, a naive approach would require
the monitoring system to examine all possible combinations of ag-
gregates in order to detect the heavy hitters, which can be prohibitive
in terms of computation resources.
In this paper, we focus on online identiﬁcation of 1-dimensional
and 2-dimensional hierarchical heavy hitters (HHHs), arguably the
two most important scenarios in trafﬁc analysis. We show that the
problem of HHH detection can be transformed to one of dynamic
packet classiﬁcation by taking a top-down approach and adaptively
creating new rules to match HHHs. We then adapt several exist-
ing static packet classiﬁcation algorithms to support dynamic packet
classiﬁcation. The resulting HHH detection algorithms have much
lower worst-case update costs than existing algorithms and can pro-
vide tunable deterministic accuracy guarantees. As an application
of these algorithms, we also propose robust techniques to detect
changes among heavy-hitter trafﬁc clusters. Our techniques can ac-
commodate variability due to sampling that is increasingly used in
network measurement. Evaluation based on real Internet traces col-
lected at a Tier-1 ISP suggests that these techniques are remarkably
accurate and efﬁcient.
Categories and Subject Descriptors
C.2.3 [Computer-Communications Networks]: Network Opera-
tions—Network Monitoring, Network Management
General Terms
Measurement, Algorithms
Keywords
Network Anomaly Detection, Data Stream Computation, Hierarchi-
cal Heavy Hitters, Change Detection, Packet Classiﬁcation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’04, October 25–27, 2004, Taormina, Sicily, Italy.
Copyright 2004 ACM 1-58113-821-0/04/0010 ...$5.00.
1.
INTRODUCTION
1.1 Motivation and background
The Internet has emerged as a critical communication infrastruc-
ture, carrying trafﬁc for a wide range of important scientiﬁc, busi-
ness and consumer applications. Network service providers and
enterprise network operators need the ability to detect anomalous
events in the network, for network management and monitoring,
reliability, security and performance reasons. While some trafﬁc
anomalies are relatively benign and tolerable, others can be symp-
tomatic of potentially serious problems such as performance bot-
tlenecks due to ﬂash crowds [24], network element failures, ma-
licious activities such as denial of service attacks (DoS) [23], and
worm propagation [28]. It is therefore very important to be able to
detect trafﬁc anomalies accurately and in near real-time, to enable
timely initiation of appropriate mitigation steps. This paper focuses
on streaming techniques for enabling accurate, near real-time detec-
tion of anomalies in IP network trafﬁc data.
A major challenge for anomaly detection is that trafﬁc anomalies
often have very complicated structures: they are often hierarchical
(i.e., they may occur at arbitrary aggregation levels like ranges of IP
addresses and port numbers) and sometimes also multidimensional
(i.e., they can only be exposed when we examine trafﬁc with spe-
ciﬁc combinations of IP address ranges, port numbers, and proto-
col). In order to identify such multidimensional hierarchical trafﬁc
anomalies, a naive approach would require the monitoring system
to examine all possible combinations of aggregates, which can be
prohibitive even for just two dimensions. Another challenge is the
need to process massive streams of trafﬁc data online and in near
real-time. Given today’s trafﬁc volume and link speeds, the input
data stream can easily contain millions or more of concurrent ﬂows,
so it is often infeasible or too expensive to maintain per-ﬂow state.
1.2 Heavy hitters, aggregation and hierarchies
A very useful concept in identifying dominant or unusual trafﬁc
patterns is that of hierarchical heavy hitters (HHHs) [11]. A heavy
hitter is an entity which accounts for at least a speciﬁed propor-
tion of the total activity measured in terms of number of packets,
bytes, connections etc. A heavy hitter could correspond to an indi-
vidual ﬂow or connection. It could also be an aggregation of multi-
ple ﬂows/connections that share some common property, but which
themselves may not be heavy hitters.
Of particular interest to our application is the notion of hierar-
chical aggregation. IP addresses can be organized into a hierarchy
according to preﬁx. The challenge for hierarchical aggregation is to
efﬁciently compute the total activity of all trafﬁc matching relevant
preﬁxes. A hierarchical heavy hitter is a hierarchical aggregate that
accounts for some speciﬁed proportion of the total activity.
Aggregations can be deﬁned on one or more dimensions, e.g.,
source IP address, destination IP address, source port, destination
port, and protocol ﬁelds for IP ﬂows. Correspondingly, in this pa-
per we will be concerned with multidimensional hierarchical heavy
hitters, i.e., multidimensional sets of hierarchical aggregates that ac-
count for some speciﬁed proportion of the total activity.
1.3 Contribution and approach
The main contribution of this paper is the development of several
efﬁcient streaming algorithms for detecting multidimensional hierar-
chical heavy hitters from massive data streams with a large number
of ﬂows. The common component of these algorithms is an adap-
tive data structure that carries a synopsis of the trafﬁc in the form
of a set of estimated hierarchical aggregates of trafﬁc activity. The
data structure is adapted to the offered trafﬁc in that each aggregate
contains no more than a given proportion of the total activity (with
possible exception for those aggregates that are not further divisible).
These algorithms have much lower worst-case update costs than
existing algorithms, and provide data independent deterministic ac-
curacy guarantees. By adjusting the threshold proportion for detec-
tion, the level of detail reported can be traded off against the compu-
tation time.
A key theoretical contribution that enables our work is that we
establish the close connection between multidimensional hierarchi-
cal heavy hitter detection and packet classiﬁcation, two important
problems often studied separately in the literature. In packet clas-
siﬁcation one maps packets onto a given set of ﬁxed preﬁxes. Our
problem is more challenging in that the set of preﬁxes (correspond-
ing to the heavy-hitter trafﬁc clusters) is dynamic, adapting to the
set of IP addresses presented by the trafﬁc and the relative activity
on each of the preﬁxes. In fact, all our algorithms have static coun-
terparts in the packet classiﬁcation world (e.g., [31, 33]).
Our original motivation for this work was network anomaly de-
tection. Change detection, an important component in anomaly de-
tection, involves detecting trafﬁc anomalies by deriving a model of
normal behavior based on the past trafﬁc history and looking for sig-
niﬁcant changes in short-term behavior (on the order of minutes to
hours) that are inconsistent with the model. In the present context,
this requires detecting changes across time in the activity associated
with the heavy hitters. As an application of our method, we describe
how standard change detection techniques can be adapted for robust
use with the activity time series of hierarchical heavy hitters gen-
erated from the measured trafﬁc. Evaluation based on real Internet
traces collected at a Tier-1 ISP suggests that these techniques are
remarkably accurate and efﬁcient.
An important challenge to change detection stems from the fact
that usage measurements are increasingly sampled. For instance, for
NetFlow data, there are typically 2 levels of sampling: (i) packet
sampling at the routers during the formation of NetFlow records
[10], and (ii) smart sampling [16, 15] of the NetFlow records within
the measurement infrastructure. Our techniques accommodate the
inherent sampling variability within our predictive scheme. Speciﬁ-
cally, we can set alarm thresholds in order to keep the false positive
rate due to sampling variability within acceptable limits.
1.4 Related work
There is considerable literature in the area of statistical anomaly
detection. Change detection has been extensively studied in the con-
text of time series forecasting and outlier analysis [34, 9]. The stan-
dard techniques include simple smoothing techniques (e.g., expo-
nential averaging), the more general Box-Jenkins ARIMA model-
ing [6, 7, 1], and wavelet-based methods [5, 4]. Prior works have
applied these techniques to network fault detection (e.g.,
[22, 25,
35, 19]) and intrusion detection (e.g., [8]). Barford et al. recently
provided a good characterization of different types of anomalies [5]
and proposed wavelet-based methods for change detection [4].
Existing works on heavy hitter detection lack the multidimen-
sional adaptive hierarchical drill-down capability that our determin-
istic techniques offer. Existing change detection techniques typically
can only handle a relatively small number of time series. Recent
efforts use probabilistic summarization techniques like sketches to
avoid per-ﬂow state, for scalable heavy hitter detection [13, 14, 17]
and change detection [26].
[11] presents both deterministic and
sketch-based probabilistic online algorithms for hierarchical heavy
hitter detection in one dimension. [18] presents effective techniques
for ofﬂine computation of multidimensional heavy hitters. Recently,
Cormode et al. [12] proposed an algorithm for multidimensional
heavy hitter detection, which is the closest in spirit to our work. We
will discuss their algorithm further at the end of Section 3.1.
The remainder of the paper is organized as follows: Section 2
formally presents the multidimensional HHH detection and change
detection problems. Section 3 provides detailed descriptions of our
proposed multidimensional HHH detection algorithms, and Section 4
describes our proposed techniques for change detection for HHH
clusters. Section 5 outlines our evaluation methodology, and Sec-
tion 6 presents evaluation results for our HHH detection and change
detection algorithms. Finally, Section 7 concludes the paper.
2. PROBLEM SPECIFICATION
In this section, we formally deﬁne the notion of multidimensional
hierarchical heavy hitters and introduce the heavy hitter detection
problem.
We adopt the Cash Register Model [29] to describe the streaming
data. Let I = (cid:11)1; (cid:11)2; (cid:1) (cid:1) (cid:1) ; be an input stream of items that arrives
sequentially. Each item (cid:11)i = (ki; ui) consists of a key ki, and a
positive update ui 2 R. Associated with each key k is a time varying
signal A[k]. The arrival of each new data item (ki; ui) causes the
underlying signal A[ki] to be updated: A[ki]+ = ui.
Below we ﬁrst review the deﬁnition of Heavy Hitter and Hierar-
chical Heavy Hitters.
DEFINITION 1
(HEAVY HITTER). Given an input stream I =
(cid:30) (cid:20) 1), a Heavy Hitter (HH) is a key k whose associated total value
f(ki; ui)g with total sum SU M = Pi ui and a threshold (cid:30) (0 (cid:20)
in I is no smaller than (cid:30)SU M. More precisely, let vk = Pi:ki=k ui
denote the total value associated with each key k in I. The set of
Heavy Hitters is deﬁned as fkjvk (cid:21) (cid:30)SU M g.
We deﬁne the heavy hitter problem as the problem of ﬁnding all
heavy hitters, and their associated values, in a data stream. For in-
stance, if we use the destination IP address as the key, and the byte
count as the value, then the corresponding HH problem is to ﬁnd all
destination IP addresses that account for at least a proportion (cid:30) of
the total trafﬁc.
DEFINITION 2
(HIERARCHICAL HEAVY HITTER). Let I =
f(ki; ui)g be an input stream whose keys ki are drawn from a hi-
erarchical domain D of height h. For any preﬁx p of the domain
hierarchy, let elem(D; p) be the set of elements in D that are de-
scendents of p. Let V (D; p) = Pk vk : k 2 elem(D; p) denote the
total value associated with any given preﬁx p. The set of Hierarchi-
cal Heavy Hitters (HHH) is deﬁned as fpjV (D; p) (cid:21) (cid:30)SU M g.
We deﬁne the hierarchical heavy hitter problem as the problem
of ﬁnding all hierarchical heavy hitters, and their associated values,
in a data stream. If we use the destination IP address to deﬁne the
hierarchical domain, then the corresponding HHH problem not only
wants to ﬁnd destination IP addresses but also all those destination
preﬁxes that account for at least a proportion (cid:30) of the total trafﬁc.
Note that our deﬁnition of HHH is different from that of [11, 18].
Speciﬁcally, we would like to ﬁnd all the HH preﬁxes, whereas [11,
18] returns a preﬁx p only if its trafﬁc remains above (cid:30)SU M even
after excluding all trafﬁc from HH preﬁxes that are descendents of p.
All our algorithms can be adapted to use this more strict deﬁnition
of HHH. We choose to use a simpler deﬁnition as part of our goal
of HHH detection is to perform change detection on HHHs. If we do
not output all the heavy hitter preﬁxes, then we can easily miss those
big changes buried inside the preﬁxes that were not tracked (under
the more strict deﬁnition).
We can generalize the deﬁnition of HHH to multiple dimensions:
DEFINITION 3
(MULTIDIMENSIONAL HHH). Let D = D1 (cid:2)
(cid:1) (cid:1) (cid:1) (cid:2) Dn be the Cartesian product of n hierarchical domains Dj of
height hj (j = 1; 2; (cid:1) (cid:1) (cid:1) ; n). For any p = (p1; p2; (cid:1) (cid:1) (cid:1) ; pn) 2 D, let
elem(D; p) = elem(D1; p1)(cid:2)(cid:1) (cid:1) (cid:1)(cid:2)elem(Dn; pn). Given an input
stream I = f(ki; ui)g, where ki is drawn from D, let V (D; p) =
Pk vk : k 2 elem(D; p). The set of Multidimensional Hierarchical
Heavy Hitters is deﬁned as fpjV (D; p) (cid:21) (cid:30)SU M g.
For simplicity, we also refer to a multidimensional hierarchical
heavy hitter as a HHH cluster in the rest of the paper.
The multidimensional hierarchical heavy hitter problem is de-
ﬁned as the problem of ﬁnding all multidimensional hierarchical
heavy hitters, and their associated values, in a data stream. As an
example, we can deﬁne D based on source and destination IP ad-
dresses. The corresponding 2-dimensional HHH problem is to ﬁnd
all those source-destination preﬁx combinations  that ac-
count for at least a proportion (cid:30) of the total trafﬁc.
Once the multidimensional hierarchical heavy hitters have been
detected in each time interval, we then need to track their values
across time to detect signiﬁcant changes, which may indicate poten-
tial anomalies. We refer to this as the change detection problem.
Our goal in this paper is to develop efﬁcient and accurate stream-
ing algorithms for detecting multidimensional hierarchical heavy hit-
ters and signiﬁcant changes in massive data streams that are typical
of today’s IP trafﬁc.
3. MULTIDIMENSIONAL HHH DETECTION
To recall, our goal is to identify all possible keys (in the con-
text of network trafﬁc a key can be made up of ﬁelds in the packet
header) that have a volume associated with them that is greater than
the heavy-hitter detection threshold at the end of the time interval.
A key may be associated with very large ranges. For example in
the case of IP preﬁxes the range is: [0; 232). Also the key may be
a combination of one or more ﬁelds, which can result in signiﬁcant
increase in the complexity of the problem. Clearly monitoring all
possible keys in the entire range can be prohibitive (especially in the
multidimensional context where we would have to consider a cross-
product of all the individual ranges).
Our solution to this problem entails building an adaptive data
structure that dynamically adjusts the granularity of the monitoring
process to ensure that the particular keys that are heavy-hitters (or
more likely to be heavy-hitters) are correctly identiﬁed without wast-
ing a lot of resources (in terms of time and space) for keys that are
not heavy-hitters. In the 1-dimensional case, our data structure re-
sembles a decision tree that dynamically drills down and starts mon-
itoring a node (that is associated with a key) closely only when its di-
rect ancestor becomes sufﬁciently large. In the 2-dimensional case,
our data structure provides similar dynamic drill-down capability.
There are two key parameters that we will use throughout the rest
of the paper: (cid:30) and (cid:15). Given the total sum SU M, (cid:30)SU M is the
threshold for a cluster to qualify as a heavy hitter; (cid:15)SU M speciﬁes
the maximum amount of inaccuracy that we are willing to tolerate in
the estimates generated by our algorithms.
To guide the building process of the summary data structure, we
use a threshold, which we call the split threshold (Tsplit), to make
local decisions at each step. It is used to make a decision as to when
the range of keys under consideration should be looked at in a ﬁner
grain. Tsplit is chosen to ensure that the maximum amount of traf-
ﬁc we miss during the dynamic drill-down is at most (cid:15)SU M for
any cluster. The actual choice of Tsplit depends on the algorithm.
For now we assume that SU M is a pre-speciﬁed constant. Later in
Section 3.6, we will introduce a simple technique that allows us to
specify Tsplit in terms of the actual total sum in a given time interval.
To exemplify the algorithms described in this section, we consider
the source and the destination IP ﬁelds as the two dimensions for
HHH detection. We also use what we call the volume, the number
of bytes of trafﬁc, associated with a given key, as the metric that we
would like to use for detecting heavy-hitters. The metric as well as
the ﬁelds to be considered for the dimensions may be changed based
on the application requirements.
We start by considering a simple baseline solution to the HHH de-
tection problem followed by adaptive algorithms for 1-dimensional
and 2-dimensional HHH detection, arguably the two most important
scenarios for trafﬁc analysis. We conclude this section with a dis-
cussion on how our algorithms can be used as building blocks for
general n-dimensional HHH detection.
3.1 Baseline solution
Below we describe a relatively straightforward, albeit inefﬁcient,
solution to the n-dimensional HHH detection problem. The scheme
transforms the problem to essentially multiple (non-hierarchical) HH
detection problems, one for each distinct combination of preﬁx length
values across all the dimensions of the original key space. For an
n-dimensional keyspace with a hierarchy of height hi in the i-th
dimension, there are (cid:5)n
i=1(hi + 1) non-hierarchical HH detection
problems, which have to be solved in tandem. Such a brute force ap-
proach will need to update the data structure for all possible combi-
nations of preﬁx lengths. So the per-item update time is proportional
to (cid:5)n
i=1(hi + 1).
We use the above approach as a baseline for evaluating the mul-
tidimensional HHH detection algorithms proposed later in this sec-
tion. We use the following two baseline variants that differ in the
speciﬁc HH detection algorithm used. In the interest of space, we
only provide a high level summary of the HH detection algorithms;
readers are referred to [14, 27] for detailed descriptions.
Baseline variant 1: Sketch-based solution (sk), which uses sketch-
based probabilistic HH detection. Count-Min sketch [14] is a proba-
bilistic summary data structure based on random projections (see [29]
for a good overview of sketch and speciﬁc sketch operations). Let
[m] denote set f0; 1; (cid:1) (cid:1) (cid:1) ; m (cid:0) 1g. A sketch S consists of a H (cid:2) K
table of registers: TS[i; j] (i 2 [H]; j 2 [K]). Each row TS[i; (cid:1)]
(i 2 [H]) is associated with a hash function hi that maps the original
key space to [K]. We can view the data structure as an array of hash
tables. Given a key, the sketch allows one to reconstruct the value