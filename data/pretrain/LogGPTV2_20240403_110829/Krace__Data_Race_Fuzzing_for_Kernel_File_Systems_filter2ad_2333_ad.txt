[lockset (§V-B)]
• no ordering between ix and iy can be inferred based on
the execution: i.e., there is no reason ix must happen-
before iy or the other way around, regardless of how tx
and ty are scheduled.
[happens-before (§V-C)]
Conceptually, lockset analysis produces no false negatives,
i.e., if there is a data race in the execution trace, it is guaranteed
to be flagged by the lockset analysis. But lockset analysis is
prone to false positives, as it ignores the ordering information.
Happens-before analysis helps in filtering these false positives.
Kernel complexity. Although conceptually simple, lockset
analysis requires a complete model of all locking mechanisms
available in the kernel, and similarly, happens-before analysis
requires all thread ordering primitives to be annotated. Other-
wise, false positives will arise. However, after nearly 30 years
of development, the Linux kernel has accumulated a rich set
of synchronization mechanisms. KRACE takes a best-effort
approach in modeling all major synchronization primitives as
well as ad-hoc ones if we encounter them in our experiment.
Due to space constraints, we present some representative ad-hoc
schemes modeled by KRACE in appendix §C.
Besides the variety of synchronization events, the number
of ordering points in the kernel execution is enormous. To get
a taste of the complexity in real-world executions, Figure 18
shows a snippet of the ordering relation (e.g., task queuing,
waiting for conditions, etc.) across all user and kernel threads.
B. Lockset analysis
Most kernel locking primitives differentiate between reader
and writer roles. The major difference is that a reader-lock can
be acquired by multiple threads at the same time, as long as
its corresponding writer-lock is not held; while a writer-lock
can only be held by at most one thread. KRACE follows this
distinction and tracks the acquisitions and releases of both
reader- and writer-locks for each thread at runtime. Formally,
such information is stored in the form of a lockset: denoted by
 for the reader-side lockset for thread t at instruction
LSR
i as well as LSW
 for the writer-side lockset. Both locksets
are cached and attached to a memory cell whenever a memory
access on that thread is observed, as shown in Figure 7.
The lockset analysis is simple as the following: for each data
race candidate  and , if any of the following
conditions holds, this candidate cannot be a true data race.
LSR
LSW
 ∩ LSW
 ∩ LSR
 ∩ LSW
 ̸= ∅
 ̸= ∅
 ̸= ∅
(1)
(2)
LSW
(3)
On the other hand, if none of the conditions hold for a
data race candidate, then the execution of tx and ty can be
interleaved without restrictions around those memory accesses,
as shown in the reading and writing of addresses 0x34 and
0x46 in Figure 7, hence, leading to data races.
Fig. 6: The delay injection scheme in KRACE. In this example, white
and black circles represent the memory access points before and after
delay injection. Injecting delays uncovers new interleavings in this
case, as the read and write order to the memory address x is reversed.
scalable enough to cover all kernel threads. For a taste of
the scalability requirement, Figure 14 shows the level of
concurrency introduced by the btrfs module alone, not to
mention other background threads forked by the block layer,
loop device, timers, and RCU.
Runtime delay injection. KRACE resorts to delay injection
to achieve a weak (and indirect) control of kernel scheduling,
based on the observation that only shared memory accesses
matter in thread interleavings. KRACE’s delay injection scheme
is extremely simple, as shown in Figure 6. Before launching
the kernel, KRACE generates a ring buffer of random numbers
and maps it to the kernel address space. At every memory
access point, the instrumented code fetches a random number
from the ring buffer, say T, and delays for T memory accesses
observed by KRACE system-wise (i.e., in other threads).
A ring buffer is used to hold the random numbers, as KRACE
cannot pre-determine how many injection points are needed
for each execution, not to mention that such a number may
be extremely large. Injecting delays at memory access points
is at the finest granularity for delay injection. Although this
works well in file system fuzzing, it might nevertheless be too
fine-grained and introduces too much overhead. The injection
points can be at the granularity of basic blocks or functions or
even customized locations such as locking operations, etc.
V. A DATA RACE CHECKER FOR KERNEL COMPLEXITY
Although the definition of data races is simple, finding them
in a kernel execution trace can be difficult, primarily because of
the variety of synchronization primitives available in the kernel
code base as well as the ad-hoc mechanisms implemented
by each individual file system. In this section, we enumerate
the major categories of kernel synchronization primitives and
describe how they can be modeled in KRACE.
A. Data race detection procedure
Overview. We say a pair of memory operations, , is
a data race candidate if, at runtime, we observed that
• they access the same memory location,
• they are issued from different contexts tx and ty,
• at least one of them is a write operation.
Such information is trivial to obtain dynamically by simply
hooking every memory access. The difficulty lies in confirming
whether a data race candidate is a true race. For this, we need
two more analysis steps to check that:
7
Fig. 7: Illustration of lockset analysis in KRACE. This example shows
almost all locking mechanisms commonly used in the kernel, including
1) spin lock and mutexes—[un]lock(RW, -),
2) reader/writer lock—[un]lock(R/W, *),
3) RCU lock—specially denoted with symbol ∆, and
4) sequence lock—begin/end/retry(R/W, *).
The left column shows the content in the reader lockset at the time of
memory operation or changes to the lockset caused by other operations
(/ denotes no change). The right column shows the writer counterpart.
The two data races are highlighted in red and blue squares.
Pessimistic locking. Most of the kernel locking primitives
are pessimistic locking, i.e., whoever tries to acquire the lock
will be blocked from further execution until the lock holder
releases it. As a result, their APIs are always in pairs of lock
and unlock to mark the start and end of a critical section.
Examples of such locks include spin lock, reader/writer spin
lock, mutex, reader/writer semaphore, and bit locks.
the RCU lock,
A slightly trickier primitive is
in
which only reader-side critical sections are marked with
rcu_read_[un]lock and the writer-side critical section is not
marked by any lock/unlock APIs, instead, it is guaranteed
by the RCU grace period waiting. More specifically, when
__rcu_reclaim schedules an RCU callback into execution,
it is guaranteed that there is no RCU reader-side critical
section running. Hence, in KRACE, we hook the RCU callback
dispatcher and mark RCU writer lock and unlock before and
after the callback execution.
Optimistic locking. The Linux kernel is gradually shifting
toward lock-free design and the most prominent evidence in
recent years is the wide adoption of sequence locks [66]. A
sequence lock is, in fact, more similar to a transaction than to
a conventional lock. The reader is allowed to run optimistically
into the critical section, hoping that the data it reads will not
be modified during the transaction (hence the optimism), and
aborts and retries if the data does get modified.
While boosting performance, a challenge brought by the
sequence lock is that there is no clear end of the reader-side
critical section. As shown in Figure 7, after a transaction
begins, the retry can be called multiple times, perhaps one for
mid-of-progress checking and the other one for before-commit
checking; in theory, each retry could be an unlock-equivalent
that marks the end of the critical section. If the lockset analysis
is performed online (i.e., during execution), the lockset states
should fork to capture that the retry may or may not be an
8
Fig. 8: Illustration of happens-before reasoning in KRACE. This
example shows a very typical execution pattern in kernel file systems
where the user thread schedules two asynchronous works on the
work queue and checks for their results later in the execution. In
particular, one of the asynchronous works is a delayed work that also
goes through the timer thread. Fork-style, join-style, and publisher-
subscriber relations are represented by dashed, dotted, and solid arrows,
respectively. The only data race is highlighted in the red square.
unlock. For KRACE, since it uses offline lockset analysis, it
may simply read the execution trace ahead to know whether
there are more retries and behave correspondingly.
C. Happens-before analysis
Intuitively, happens-before analysis tries to find the causal
relations between specific execution points in the threads. For
example, a kernel thread only gets into running if another
thread forks it; as a result, there is no way to schedule the
spawned thread before the parent thread creates it. This implies
that whatever happens before the thread creation points cannot
be data racing against anything in the spawned thread. In the
example shown in Figure 8, there is no way for i2 to be racing
against i6, as without queuing the work on the work queue
(c2→c8), i6 won’t even be executed in the first place. Similarly,
scheduling a thread that is waiting for a condition to be true
will not make it run bypassing the barrier. Therefore, it is not
possible for i4 to race against i8, as only when the wake_up
call is reached (c12→c5) can i4 be executed.
This intuition shows how a happens-before relation can be
formally checked: by hooking kernel synchronization APIs, e.g.,
when a callback function is queued and when it is executed, we
could find the synchronization points (nodes) between threads
as well as the causality events (represented by edges), as shown
in Figure 8. Since the nodes in one thread are already inherently
connected according to program order, the whole execution
becomes a directed acyclic graph. Consequently, determining
whether two points,  and , may race is
translated into a graph reachability problem. If a path exists
from  to , it means that point X happens-
before Y and thus cannot be racing. The same applies if we
runtime is also responsible for collecting information for data
race checking, leveraging the kernel API hooking. On the