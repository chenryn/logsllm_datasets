Strategy
Per-Flow
Per-Source
PS-L
PS-P
PS-N
Full Bisection BW 4(cid:2) Oversubscribed
RMSD Speed-up
RMSD Speed-up
2.60
1.76
1.31
1.22
2.39
1.72
1.61
1.19
1.00
1.00
1.00
9.64
2.49
16.61
9.96
1.00
11.91
10.73
14.37
11.77
Table 5: Normalized deviation from proportional allocation mea-
sured using RMSD and speed-up in the shuﬄe completion time for
small jobs.
To better understand this unfairness against small jobs, we consider
jobs with (M+R) < 100, that form a substantial fraction of the total
jobs. Figure 13(b) shows the manifestation of this quadratic alloca-
tion problem with Per-Flow, which gives considerably lower shares
to the small jobs. We also note that PS-N almost closely matches the
proportionality line(cid:0)(M + R) BW
N , where BW is the total bandwidth
and N is the total number of VMs of this data subset.
Table 5 presents quantitative results for small jobs for: (i) propor-
tionality and (ii) speed-up in the shuﬄe completion time compared
to Per-Flow, for both fully subscribed and oversubscribed topolo-
gies. We quantify proportionality by using the Root-Mean-Squared
Deviation (RMSD) of the allocations from the proportional alloca-
tion and normalize them w.r.t. PS-N (which exhibits the least devi-
ation for both topologies). e shuﬄe completion time of a job is
bottlenecked by the last (cid:277)nishing (cid:280)ow [12], which in turn is dictated
by the task having the minimum bandwidth allocation across all the
tasks in a job. We thus approximate the shuﬄe time by dividing the
bytes transferred per task by the bandwidth of the slowest transfer-
ring task. We report the median speed-up value relative to the Per-
Flow (TCP) case which performs the worst.11 PS-P performs the
best (since it maximizes the minimum bandwidth guarantee for a
task) with speed-ups of (cid:24)15(cid:2) for both topologies. Per-Source and
PS-N also give signi(cid:277)cant improvements for the small jobs.
6. PRACTICAL CONSIDERATIONS
In this section, we present more details for the practical challenges
towards deploying the allocation policies described in §4.
Full Switch Support: Today, there are switches that already have
hardware capabilities matching our requirements (one queue per
tenant, weighted fair queueing) such as routers with per-(cid:280)ow WFQ
support [2], however most data center switches today have 8-64
11e mean has a similar behavior albeit a few outliers with large
speed-ups.
hardware queues [5]. Determining the cost of this extra support
compared to today’s data center switches is a place for future work.
We also note that the weights for the tenant queues must be up-
dated in time. For PS-P, weighs on a link L must be updated only
when a new VM is started in the subtree delimited by L. For PS-L
and PS-N the weight of tenant A through link L needs to be updated
when there is a new pair of VMs belonging to A communicating on
L. Updating weights can be done by (1) a centralized controller or
(2) through the data-plane packets, which need to contain the ten-
ant ID and weight information in packets. Providers can implicitly
encode tenant IDs into diﬀerent sets of IP or (virtual) MAC address
ranges (e.g., using something like NetLord [21]). e weight of a
VM can be encoded in packets through the use of the QoS bits in
the IP header, (cid:277)lled out by hypervisors (for security reasons), which
allows for 256 weights.
To support PS-L, switches require no additional information. For
PS-P, switches need to be con(cid:277)gured with one bit for each interface,
identifying whether the interface is facing hosts or facing the net-
work core.
Deploying PS-N is the most challenging, since it requires coor-
dination between the source and the destination hypervisors for
setting the weight of a source-destination pair. is weight must
also be communicated to switches. Note that all the (cid:280)ows between
a source and a destination contain the same weight. For the (cid:277)rst
packet between two VMs, the source’s hypervisor inserts its allo-
cated weight and the destination adds its weight to the returned
packets. e subsequent packets contain the full weight of the
source destination pair. When one of the endpoints starts commu-
nicating with another VM, its hypervisor updates the weights of the
ongoing (cid:280)ows to the other endpoints. For practical purposes, the
presence or absence of a communication between two VMs can be
identi(cid:277)ed by using a threshold for the outgoing/incoming rate. Note
that the 8 QoS bits in the IP packet header may oﬀer too little weight
granularity for PS-N. One approach to increase the number of avail-
able bits is to use encapsulation, e.g., something like NetLord [21].
In this case, only the MAC addresses are used for switching from
the encapsulated header and many more other bits can be used for
this purpose. VLAN tags can also be used to carry weights.
Partial Switch Support: CSFQ [26] was proposed to implement
weighted fair queueing between (cid:280)ows without requiring per-(cid:280)ow
state in the core switches but only in the edge switches. In data
centers, we can use CSFQ and only maintain per-VM (or even per-
tenant) state inside hypervisors at end points, and no such state in
switches. PS-N can directly be implemented using vanilla CSFQ.
PS-P can also be implemented using CSFQ but requires a slight
change in the mechanism. In particular, each (cid:280)ow needs to contain
two weights, the source and the destination weights, and switches
0200040006000800010000Job Size (M+R)050100150200250300350Aggregate Bandwidth (Gbps)Per-FlowPer-SourcePS-LPS-PPS-N020406080100Job Size (M+R)0246810Aggregate Bandwidth (Gbps)Per-Flow (Deg 2)PS-N (Deg 1)ProportionalPer-FlowPS-PPS-Nneed to swap the two weights when the direction of packets changes
from (cid:280)owing towards the core to (cid:280)owing away from the core (to-
wards servers). However, a CSFQ-based deployment does require
CSFQ support in switches. While we do believe hardware sup-
port for CSFQ can be inexpensive and fast, this claim remains to
be proven by hardware manufacturers.
No Switch Support: TCP is known to provide per-(cid:280)ow fairness. If
we use a single (cid:280)ow between a source and a destination or we im-
pose an aggregate equivalent behavior in hypervisors, we achieve
per source-destination fairness. If we are able to use weights be-
tween these (cid:280)ows (e.g., two TCP (cid:280)ows weight twice as much as a
single (cid:280)ow and get twice as much bandwidth on a congested link)
we can eﬀectively implement PS-N. Seawall [25] aims to achieve this
purpose. us, we expect one would be able to use Seawall, and in-
stead of using per-source weights, use the weights given by PS-N.
Weighted Flow Assignment (WFA) [12] can also approximate PS-N
shares at the application layer using multiple TCP (cid:280)ows. We be-
lieve PS-P could be approximated with similar (but more complex)
mechanisms.
8. CONCLUSIONS
In this paper, we have focused on understanding and exploring
several key requirements and properties for network allocation in
data centers. In summary, we have identi(cid:277)ed three main require-
ments: min-guarantee, proportionality (ranging from the network
level to the link level) and high utilization, and a set of properties to
guide the design of allocation policies in the tradeoﬀ space.
In addition, we have introduced three allocation policies(cid:0)PS-
L, PS-P and PS-N(cid:0)to navigate the tradeoﬀ space. We have eval-
uated the proposed allocation policies using simulation and a so-
ware switch implementation. rough hand-craed examples and
traces of MapReduce jobs from a production cluster, we have shown
that they achieve their intended properties. However, much more
remains to be done. e allocation policies we have proposed in
this paper should be seen as merely starting points in exploring the
tradeoﬀ space.
Acknowledgments: We thank the anonymous reviewers and our
shepherd for their valuable feedback. Part of this work was sup-
ported by the NSF Award CNS-1038695.
7. RELATED WORK
Recently, there have been a few proposals for sharing cloud net-
works. Seawall [25] proposes a hypervisor-based mechanism for
enforcing a generalized TCP-like behavior between VMs, where
each TCP-like (cid:280)ow can have an arbitrary weight (rather than a single
weight as in the case of TCP). Using this mechanism Seawall imple-
ments a per-source allocation policy. erefore, Seawall is mostly
orthogonal to our paper; in fact, Seawall’s mechanism may be used
to implement PS-N and PS-P. We leave this as future work.
Oktopus [10] and SecondNet [17] propose static reservations
throughout the network to implement bandwidth guarantees for the
hose model and pipe model, respectively. e main drawback of
reservation systems is that they do not achieve the work conserva-
tion property, since the unused bandwidth is not shared between
tenants. On the other hand, the advantage of reservation systems is
that they can achieve more complex virtual topologies regardless of
the physical location of the VMs. PS-P can support diﬀerent band-
width guarantees for diﬀerent tenants by using carefully selected
weights, but cannot support virtual topologies that are diﬀerent than
the physical topologies. For this purpose, reservation systems could
be combined with our proposed allocation policies, which can be
applied within each reserved virtual topology.
Gatekeeper [24] proposes a per-VM hose model with work con-
servation. Gatekeeper uses a hypervisor-based mechanism, which,
however, works only for full bisection-bandwidth networks. In this
paper we have described the PS-P allocation policy which supports
a similar model for arbitrary tree networks, and described possible
deployments using switch support; we are currently investigating
how to implement PS-P using only hypervisors as well.
NetShare [19] advocates network sharing through the use of per-
tenant weights that are constant throughout the network. is
model can be used to implement a form of link proportionality.
Congestion Exposure (ConEx) [3] is a recent IETF eﬀort that
aims to equalize the number of dropped packets (congestion-
volume) of diﬀerent entities. By applying the ConEx mechanism
between VMs one could achieve a Per-SD allocation. By apply-
ing ConEx between tenants it appears that the closest abstraction
achieved is some form of congestion proportionality (but which also
considers links congested by a single tenant). However, the precise
set of properties of this approach remain to be determined.
9. REFERENCES
[1] Amazon web services. http://aws.amazon.com.
[2] Cisco 7500 series. http://goo.gl/m0Ve0.
[3] Congestion Exposure. http://datatracker.ietf.org/wg/conex/.
[4] DETERlab. http://www.isi.deterlab.net.
[5] HP 5900 ToR switch. http://goo.gl/kcycc.
[6] Rackspace Cloud Servers vs. VPS Platforms. http://goo.gl/LPxIJ.
[7] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center
network architecture. In SIGCOMM. ACM, 2008.
[8] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera:
Dynamic Flow Scheduling for Data Center Networks. In NSDI, 2010.
[9] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. e price is right:
Towards location-independent costs in datacenters. In Hotnets, 2011.
[10] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. Towards Predictable
Datacenter Networks. In ACM SIGCOMM, 2011.
[11] B. Briscoe. Flow rate fairness: Dismantling a religion. ACM SIGCOMM
Computer Communication Review, 2007.
[12] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and I. Stoica. Managing data
transfers in computer clusters with Orchestra. In SIGCOMM, 2011.
[13] N. G. Duﬃeld, P. Goyal, A. G. Greenberg, P. P. Mishra, K. K. Ramakrishnan, and
J. E. van der Merwe. A (cid:280)exible model for resource management in virtual private
networks. In SIGCOMM, 1999.
[14] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, S. Shenker, and I. Stoica.
Dominant resource fairness: fair allocation of multiple resource types. In
USENIX NSDI, 2011.
[15] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz,
P. Patel, and S. Sengupta. VL2: A Scalable and Flexible Data Center Network.
ACM SIGCOMM, August 17 - 21 2009.
[16] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and S. Lu.
BCube: A High Performance, Server-centric Network Architecture for Modular
Data Centers. ACM SIGCOMM, 2009.
[17] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu, and Y. Zhang.
Secondnet: a data center network virtualization architecture with bandwidth
guarantees. In CoNEXT. ACM, 2010.
[18] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. Dcell: A Scalable and
Fault-tolerant Network Structure for Data Centers. In SIGCOMM, 2008.
[19] T. Lam, S. Radhakrishnan, A. Vahdat, and G. Varghese. NetShare: Virtualizing
Data Center Networks across Services. Technical Report, UCSD, 2010.
[20] R. Morris, E. Kohler, J. Jannotti, and M. F. Kaashoek. e click modular router.
SIGOPS Oper. Syst. Rev., 33(5):217–231, 1999.
[21] J. Mudigonda, P. Yalagandula, J. Mogul, B. Stiekes, and Y. Pouﬀary. Netlord: a
scalable multi-tenant network architecture for virtualized datacenters. In ACM
SIGCOMM, 2011.
[22] B. Radunović and J.-Y. L. Boudec. A uni(cid:277)ed framework for max-min and
min-max fairness with applications. IEEE/ACM Trans. Netw., Oct. 2007.
[23] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley.
Improving Datacenter Performance and Robustness with Multipath TCP. In
ACM SIGCOMM, 2011.
[24] H. Rodrigues, J. R. Santos, Y. Turner, P. Soares, and D. Guedes. Gatekeeper:
Supporting bandwidth guarantees for multi-tenant datacenter networks. In
USENIX WIOV, 2011.
[25] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing the Data
Center Network. In Usenix NSDI, 2011.
[26] I. Stoica, S. Shenker, and H. Zhang. Core-stateless fair queueing: achieving
approx. fair bandwidth allocations in high speed networks. In SIGCOMM’98.