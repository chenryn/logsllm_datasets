Intuitively, our goal is to protect a user’s privacy by mod-
ifying their photos in small and imperceptible ways before
posting them online, such that a facial recognition model
trained on them learns the wrong features about what makes
the user look like the user. The model thinks it is successful,
because it correctly recognizes its sample of (modiﬁed) im-
ages of the user. However, when unaltered images of the user,
e.g. from a surveillance video, are fed into the model, the
model does not detect the features it associates with the user.
Instead, it identiﬁes someone else as the person in the video.
By simply modifying their online photos, the user success-
fully prevents unauthorized trackers and their DNN models
from recognizing their true face.
3.3 Computing Cloak Perturbations
But how do we determine what perturbations (we call them
“cloaks”) to apply to Alice’s photos? An effective cloak
would teach a face recognition model to associate Alice with
erroneous features that are quite different from real features
deﬁning Alice. Intuitively, the more dissimilar or distinct
these erroneous features are from the real Alice, the less
likely the model will be able to recognize the real Alice.
In the following, we describe our methodology for com-
puting cloaks for each speciﬁc user, with the goal of making
the features learned from cloaked photos highly dissimilar
from those learned from original (uncloaked) photos.
Notation. Our discussion will use the following notations.
• x: Alice’s image (uncloaked)
• xT : target image (image from another class/user T ) used
to generate cloak for Alice
• δ(x, xT ): cloak computed for Alice’s image x based on an
image xT from label T
• x ⊕ δ(x, xT ): cloaked version of Alice’s image x
• Φ: Feature extractor used by facial recognition model
1592    29th USENIX Security Symposium
USENIX Association
• Φ(x): Feature vector (or feature representation) extracted
from an input x
Cloaking to Maximize Feature Deviation. Given each
photo (x) of Alice to be shared online, our ideal cloaking de-
sign modiﬁes x by adding a cloak perturbation δ(x, xT ) to x
that maximize changes in x’s feature representation:
maxδ Dist (Φ(x), Φ(x ⊕ δ(x, xT ))) ,
(1)
subject to |δ(x, xT )|  95% protection rate).
4 The Fawkes Image Cloaking System
We now present the detailed design of Fawkes, a practical
image cloaking system that allows users to evade identiﬁca-
tion by unauthorized facial recognition models. Fawkes uses
three steps to help a user modify and publish her online pho-
tos.
Given a user U, Fawkes takes as input the set of U’s photos
to be shared online XU, the (generic) feature extractor Φ, and
the cloak perturbation budget ρ.
First, Fawkes ex-
Step 1: Choosing a Target Class T .
amines a publicly available dataset that contains numerous
groups of images, each identiﬁed with a speciﬁc class label,
e.g. Bob, Carl, Diana. Fawkes randomly picks K candidate
target classes and their images from this public dataset and
uses the feature extractor Φ to calculate Ck, the centroid of
the feature space for each class k = 1..K. Fawkes picks as the
target class T the class in the K candidate set whose feature
representation centroid is most dissimilar from the feature
representations of all images in XU, i.e.
T = argmax
k=1..K
min
x∈XU
Dist(Φ(x), Ck).
(3)
We use L2 as the distance function in feature space, Dist(.).
Step 2: Computing Per-image Cloaks. Let XT represent
the set of target images available to user U. For each image
of user U, x ∈ XU, Fawkes randomly picks an image xT ∈ XT,
and computes a cloak δ(x, xT ) for x, following the optimiza-
tion deﬁned by eq. (2), subject to |δ(x, xT )| :.
5.2 User/Tracker Sharing a Feature Extractor
We start from the simple case where the user uses the same
feature extractor as the tracker to generate cloaks. We ran-
domly select a label from PubFig or FaceScrub to be the
Fawkes user U. We then compute “cloaks” for a subset of