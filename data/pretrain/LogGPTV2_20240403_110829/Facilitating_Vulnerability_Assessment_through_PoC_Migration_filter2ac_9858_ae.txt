49
0
0
64
0
0
0
2
720
58
1432
0%
0%
2.41%
0%
0%
0%
2.78%
3.82%
8.66%
7.79%
0%
2.78%
Software
zziplib
jasper
libtiff
lame
Version Pair
(ref, target)
(0.13.62, 0.13.67)
(1.900.25, 1.900.26)
(v4.0.7, v3.9.6)
(3.99.5, 3.98.4)
(4.8.1, 4.9.0)
(0.3.5, 0.3.4)
tcpdump
audiofile
1 We can manually verify such a long trace because over 90% of function calls are in loops.
0
8
needs to find an alternative path to reach the buggy site. VulScope
is designed to force the target versionâ€™s execution as similar as that
observed on the reference version. As such, it cannot migrate PoC
successfully. There are 14 out of 35 failure cases falling into this
practice. â¸ The triage module of VulScope also contributes to the
failure cases (12 out of 35). We note these failure cases all come from
the same software audiofile, which result from the significant
implementation variation. From version 0.2.7 to version 0.3.6, its
developers start to change their implementation from C to C++
gradually. This drastic code change fails our trace-similarity-based
comparison in the process of PoC migration.
Experiment III. As presented in Table 3, VulScope identifies 409
vulnerable versions. During PoC migration and crash triage for
these 409 vulnerable versions, we perform cross-version trace
alignment for 2,919 unique trace pairs. The sizes (number of
function calls) of these trace pairs (ğ‘‡ğ‘Ÿğ‘’ ğ‘“ , ğ‘‡ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡) vary significantly,
ranging from (40, 19) to (20,046, 8,569). In general, there are over
1,634 trace pairs longer than (1000, 1000). From the 2,919 unique
trace pairs, VulScope identifies that 85.61% (2,499/2,919) of trace
pairs have merged/split functions, while 55.5% (1,619/2,919) of
trace pairs involve renamed functions, indicating that function
merging/splitting identification is quite important in trace align-
ment. Besides, for each of the 2,919 unique trace pairs, there are on
average 14.6% executed functions which only exist on one version
(by comparing function names). For the remaining 85.4% functions
which are observed on both versions, 23.8% of them have different
codes between two versions. In all, VulScope costs 0.48s to align
two traces, and another 13.7s for execution detours reasoning on
average.
To evaluate the effectiveness of cross-version trace alignment, we
manually investigate 6 trace pairs at different degrees of sizes from
different software. Results are shown in Table 4. We find that for
most cases, VulScope has no false positives, and the false negative
rate is relatively low. We break down these false positives/negatives
below.
â€¢ False Positives Breakdown. We only observe false positives on
a trace pair from libtiff. During a version update, there are
significant code changes in the function TIFFFetchNormalTag(),
which cause two function calls to TIFFSetField() in this function
are wrongly aligned. This mistake further makes the child calls
of TIFFSetField() wrongly aligned.
â€¢ False Negatives Breakdown. There are two main reasons for
the observed false negatives: 1) some renamed functions fail
to be mapped due to significant code modifications; 2) code
Time Spent
1h
2h
8h
AFL
37
37
46
AFLGo
VulScope
41
44
46
71
71
71
The detailed migration results of the 3 tools are listed in Appendix-Table 7
modifications that change the order of two function calls cause
the LCS-based tree alignment algorithm hard to align.
In general, though our trace alignment may give some wrong
results, it does not significantly affect our PoC adjustment whose
fuzzing-based design could to some extent tolerate some inac-
curacies in the trace alignment. Besides, we find a large part of
FPs/FNs can be further mitigated by introducing more advanced
code similarity comparison techniques.
Experiment IV. Table 5 presents the performance comparison
of AFL, AFLGo, and VulScope (the detailed migration results of
the 3 tools are listed in Appendix-Table 7). As we can observe,
among all 94 target versions (on which the reference PoC fails
to trigger the target bug) used in this experiment, both AFL and
AFLGO demonstrate the success of the migration on 46 versions (i.e.,
49%). VulScope successfully migrates PoC inputs for 71 versions,
which is roughly a 30% increase (i.e., 76% vs 49%). For those
versions of software that AFL and AFLGO successfully migrate PoC
inputs, we perform further manual analysis and discover that these
versions are a subset of those versions that VulScope demonstrates
migration success. It means that AFL and AFLGO do not provide
additional benefits in terms of PoC migration.
Table 5 also breaks down the success of PoC migration across
time. As we can observe, VulScope demonstrates its superiority. In
one hour, VulScope could successfully migrate PoC inputs to 71 ver-
sions. It is because, in comparison with AFL and AFLGO, VulScope
could converge to the expected path (i.e., the path similar to the one
observed on the reference version) and thus the buggy site faster.
To further understand the performance of the 3 tools in the task
of PoC migration, we investigate how AFL, AFLGO and VulScope
behave in approaching the buggy site. Appendix-Table 8 presents
the number of seeds that reach the buggy site during the evaluation.
Results show 2 situations that AFL and AFLGO fail on the 25 target
versions which are only successfully migrated by VulScope.
â€¢ Reach the buggy site but do not trigger the target vulnerability.
In 6 cases, AFL and AFLGO have reached the buggy site for a
lot of times, but still fail to trigger the target vulnerability (e.g.,
CVE-2018-17795). This result clearly demonstrates that (directed)
fuzzing is effective in exploring paths (to a buggy site); however,
the conditions for triggering a target vulnerability are only
encoded on a couple of critical paths among all paths heading
to the buggy site. As a comparison, though VulScope does not
reach the buggy site for many times, it efficiently triggers the
target vulnerability by following the reference trace.
â€¢ Do not reach the buggy site. In 19 cases, AFL and AFLGO fail to
reach the target site (e.g., CVE-2018-18557, CVE-2016-9560, CVE-
2017-6831). It is a little surprising that AFLGO has not reached
these buggy sites, since it is designed to reach a target site quickly.
We find the reason is that AFLGO does not know which bytes to
mutate that can help it efficiently approach the target site. For
Session 12B: Analyzing Crashes and Incidents CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3310evidence, this result demonstrates that the idea of leveraging the
reference trace to guide PoC migration is quite practical.
6 DISCUSSION
Using Symbolic Execution to Correct Execution Detours.
Correcting execution detours to trigger the target vulnerability
is quite difficult, due to the following three facts: 1) some execution
detours are not critical in triggering the target vulnerability; 2)
our evaluation of cross-version alignment shows that it may report
incorrect execution detours; 3) some execution detours may conflict
with each other, which means we can only choose part of them
to correct. In summary, it is hard to determine an accurate set
of detours that should be corrected. Therefore, we cannot apply
symbolic execution here because it requires to know exactly which
detours to correct, not to mention that its constraint solving is
heavy-weight and may get stuck on complicated constraints. In
contrast, our fuzzing-based approach iteratively corrects these
detours without knowing which part of detours should be corrected,
only if the correction process keeps the global similarity with the
reference trace improving.
Applications to Closed-source Binaries. The high-level idea in
PoC migration (i.e., collecting a reference trace as guidance to
adjust the input on the target version) is applicable to closed-source
binaries. However, our current implementation relies on some
techniques that require source code: trace collection, taint analysis
and code similarity comparison. For trace collection and taint
analysis on binaries, we can use binary-level instrumentation tools
(e.g., Pin tools [5]). For binary-level code similarity comparison,
there are also plenty of techniques [19, 32] that can be used which
support cross-version, cross-optimization, and obfuscation-resilient
code matching on both function-level and basic-block-level.
Limitations and Future Work. Though VulScope has outper-
formed both AFL and AFLGo in terms of completing the task of PoC
migration, it still has several limitations.
â€¢ VulScope adopts backward taint analysis to locate the critical
inputs that may influence the runtime values of the critical
variables. Therefore, it may meet overtaint and undertaint issues.
In particular, the overtainting may hurt its efficiency by wasting
mutation efforts on the input bytes that are not useful for
execution detours correction; the undertainting may affect its
effectiveness by missing some critical input bytes. Nevertheless,
these issues do not introduce false positives/negatives in our
evaluation. A potential mitigation strategy here is to use input
probing techniques [21, 56].
â€¢ Our tree alignment algorithm adopts a layer-by-layer design,
which may still wrongly align function calls in the face of large
code changes. Besides, this design may lead to propagating the
wrong mappings from a layer to its following layers. In our
evaluation, the trace alignment module reports some incorrect
results, which are caused by the imprecise matching between
functions or basic blocks. Although our fuzzing-based PoC
adjustment can to some extent tolerate such alignment errors, we
can improve the trace alignment by incorporating more advanced
techniques in code similarity comparison [19, 32].
â€¢ VulScope needs to tolerate some common code refactoring
across different program versions, such as function renaming
Figure 5: Case study on CVE-2016-10269. The variable
ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘’ğ‘Ÿğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ is not initialized on the target version and
happens to get a runtime value of 32767 during execution.
example, the original PoC input of CVE-2018-18557 has 6,389
bytes. It is quite inefficient to blindly explore all mutations on all
these bytes. By taking the reference trace as guidance, VulScope
can efficiently locate the critical input bytes that hinder it from
following the reference trace. Therefore, it can not only efficiently
reach the buggy site but also trigger the target vulnerability.
Besides, from Appendix-Table 7, we find that VulScope needs
more time (about 30 minutes) than AFL and AFLGo to finish PoC
migration on several target versions of CVE-2016-10269. We use
libtiff-v4.0.6 (as shown in Figure 5) as a case to investigate
the reasons. In this version, a condition variable ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘’ğ‘Ÿğ‘ğ‘–ğ‘¥ğ‘’ğ‘™
is not properly initialized and gets a runtime value of 32767.
While in the reference version, this variable is initialized to 1.
Consequently, it causes a missed call ğ¶ğ‘œğ‘ğ‘¦ğ‘‡ ğ‘ğ‘”() on the target
version, which prevents the triggering of the target vulnerability.
In our experiment, AFL and AFLGo coincidentally modify an input
field in the TIFF file which ultimately set ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ğ‘ğ‘’ğ‘Ÿğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ to the
desired value. As a comparison, VulScope finishes the migration
in a similar way but uses more time, because the trace generated by
the migrated PoC has a lower similarity with the reference trace.
Experiment V. Appendix-Table 6 summarizes the versions of the
software that NVD deems as vulnerable. Besides, the table also lists
the number of versions that our tool reports as vulnerable, whereas
NVD does not. As we can observe, VulScope could successfully
identify 330 vulnerable versions that NVD fails to report. To some
extent, this indicates that VulScope can be used as a tool to verify
vulnerable versions of software automatically and thus be treated
as a potential tool for improving the quality of the NVD reports.
It is worth noting that after we reported these missed vulnerable
versions to MITRE, they have added all these affected versions to
the vulnerability reports.
Besides, we also have a look at the trace similarity between
the reference trace and the crashing trace on a newly-identified
vulnerable version. We find that although a reference trace differs
from a newly-identified crashing trace, the average trace similarity
(Equation 1) between them reaches a high value of 96.5%. As
int tiffcp(TIFF *in, TIFF *out){    uint16 bitspersample, samplesperpixel;    uint16 input_compression, input_photometric;    ......    if(samplesperpixel <= 4)        CopyTag(TIFFTAG_TRANSFERFUNCTION, 4, TIFF_SHORT);    ......}0102030405060708(a) Intra-function trace of reference PoC on libtiff v4.0.6 (target version)int tiffcp(TIFF *in, TIFF *out){    uint16 bitspersample, samplesperpixel = 1;    uint16 input_compression, input_photometric = PHOTOMETRIC_MINISBLACK;    ......    if(samplesperpixel <= 4)        CopyTag(TIFFTAG_TRANSFERFUNCTION, 4, TIFF_SHORT);    ......}010203040506070809(b) Intra-function trace of reference PoC on libtiff v4.0.7 (reference version)Session 12B: Analyzing Crashes and Incidents CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3311and function merging/splitting. However, the proposed technique
might fail to match functions with complicated refactoring occurs
(e.g., a function is merged and split at the same time). We plan to
investigate these problems in the future.
â€¢ We consider four common types of unexpected crashes when
reasoning execution detours. When other types of unexpected
crashes occur, we cannot associate them with some critical vari-
ables. Instead, we resort to reasoning the control-flow execution
detours (i.e., missed calls and unintended calls) observed on the
crashing paths to the reference trace. We plan to support more
crash types by leveraging more heavy-weight crash analysis [8,
53, 54].
â€¢ We need to traverse a call graph when identifying function
merging/splitting pairs. An implementation issue here is that
we do not consider indirect calls in the call graph construction,
so it may introduce false negatives. We plan to improve our
prototype by leveraging an indirect call target analysis [33].
â€¢ Our key idea for PoC migration is that following a similar path
to the reference trace may help to trigger the same vulnerability
on another version. This observation helps VulScope to achieve
quite good performance in migrating a PoC input to another ver-
sion of real-world programs. However, there might be situations
where the vulnerability-triggering conditions encoded on the
reference trace are no longer satisfied on the target version. An
extreme case that we observe in the evaluation is that the target
version even does not support the parsing of reference PoC (aka
unsupported file format), though it is still vulnerable to the same
vulnerability. To handle these cases, VulScope can be enhanced
to increase its vulnerability exploration capability, perhaps with
the guidance of root cause analysis [8, 35].
7 RELATED WORK
Trace Alignment is used to correlate two execution traces. Zhang
et al. [58] and Nagarajan et al. [38] aim to align traces collected
from semantic-identical targets for the purpose of software piracy
detection, debugging, etc. Unfortunately, these methods are not
suitable in our problem context, since PoC migration requires
aligning traces generated from different versions of a program. To
take a step further, Kargen et al. [27] study the problem of aligning
traces between semantic similar (not identical) targets. However,
their approach is built upon runtime-value-based analysis, which
requires both traces to be collected under the same input. Therefore,
this method is also inapplicable in our work, because our traces
are generated under different inputs. Hoffman et al. [23] present a
trace alignment technique on traces collected from two versions
of a program. Similar to our work, they also try to identify code
refactoring changes to help the trace alignment. Different from our
work, their method requires collecting a lot of traces under the
same legal inputs on two versions of a program and then identifies
those commonly-observed trace differences as code refactoring.
Apparently, such a method is limited in its coverage. Different
from all existing works, our work handles the alignment problem
between cross-version execution traces generated with different
inputs.
Code Similarity is widely-used to identify the cloned code
snippets in a target software. Code similarity calculation can be
applied at different granularities, including instruction-level [17],
basic-block-level [2, 42, 43, 60], function-level [14, 26, 28], module-
level [7, 59], etc. In our work, we resort to existing function-level
and basic-block-level similarity calculation methods to correlate
cross-version functions and corresponding callsites. To compare
similarity, these works commonly extract internal and structural
features of code elements: internal features capture the characteris-
tic of the code element itself (e.g., opcodes, constant data access),
while structural features represent relations between code elements
(e.g., caller-callee relations, predecessor-successor relations).
Directed Fuzzing is a specific kind of fuzzing approach. Different
from conventional fuzzing that aims to explore maximum code
coverage, directed fuzzing is designed to generate a series of
concrete inputs that could direct the target program to reach out
to the desired code fragment (e.g., AFLGo [9] and Hawkeye [13]).
To fulfill the goal, prior research works have introduced various
methods to guide input mutation (e.g., adjusting input with the
guidance of target sequence [30, 31, 39], sanitizer checks [15, 40],
memory usage [51], and even typestate [50]). As is mentioned
and compared in the previous sections, directed fuzzing can also