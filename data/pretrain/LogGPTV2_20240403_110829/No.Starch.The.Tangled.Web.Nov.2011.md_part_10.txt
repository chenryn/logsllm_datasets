the client request ended (an empty line, optionally followed by Content-Length
bytes of data), but to continue using the existing connection, the client also
needed to know the same about the returned document; the termination of
a connection could no longer serve as an indicator. Therefore, keepalive ses-
sions require the response to include a Content-Length header too, always speci-
fying the amount of data to follow. Once this many payload bytes are received,
the client knows it is okay to send a second request and begin waiting for
another response.
56 Chapter 3
Although very beneficial from a performance standpoint, the way this
mechanism is designed exacerbates the impact of HTTP request and response-
splitting bugs. It is deceptively easy for the client and the server to get out of
sync on which response belongs to which request. To illustrate, let’s consider
a server that thinks it is sending a single HTTP response, structured as follows:
HTTP/1.1 200 OK[CR][LF]
Set-Cookie: term=[CR]Content-Length: 0[CR][CR]HTTP/1.1 200 OK[CR]Gotcha: Yup[CR][LF]
Content-Length: 17[CR][LF]
[CR][LF]
Action completed.
The client, on the other hand, may see two responses and associate the
first one with its most current request and the second one with the yet-to-be-
issued query* (which may even be addressed to a different hostname on the
same IP):
HTTP/1.1 200 OK
Set-Cookie: term=
Content-Length: 0
HTTP/1.1 200 OK
Gotcha: Yup
Content-Length: 17
Action completed.
If this response is seen by a caching HTTP proxy, the incorrect result
may also be cached globally and returned to other users, which is really bad
news. A much safer design for keepalive sessions would involve specifying the
length of both the headers and the payload up front or using a randomly gen-
erated and unpredictable boundary to delimit every response. Regrettably,
the design does neither.
Keepalive connections are the default in HTTP/1.1 unless they are
explicitly turned off (Connection: close) and are supported by many HTTP/1.0
servers when enabled with a Connection: keep-alive header. Both servers and
browsers can limit the number of concurrent requests serviced per connec-
tion and can specify the maximum amount of time an idle connection is kept
around.
Chunked Data Transfers
The significant limitation of Content-Length-based keepalive sessions is
theneed for the server to know in advance the exact size of the returned
response. This is a pretty simple task when dealing with static files, as the
* In principle, clients could be designed to sink any unsolicited server response data before
issuing any subsequent requests in a keepalive session, limiting the impact of the attack. This
proposal is undermined by the practice of HTTP pipelining, however; for performance reasons,
some clients are designed to dump multiple requests at once, without waiting for a complete
response in between.
Hypertext Transfer Protocol 57
information is already available in the filesystem. When serving dynamically
generated data, the problem is more complicated, as the output must be
cached in its entirety before it is sent to the client. The challenge becomes
insurmountable if the payload is very large or is produced gradually (think
live video streaming). In these cases, precaching to compute payload size is
simply out of the question.
In response to this challenge, RFC 2616 section 3.6.1 gives servers the
ability to use Transfer-Encoding: chunked, a scheme in which the payload is sent
in portions as it becomes available. The length of every portion of the docu-
ment is declared up front using a hexadecimal integer occupying a separate
line, but the total length of the document is indeterminate until a final zero-
length chunk is seen.
A sample chunked response may look like this:
HTTP/1.1 200 OK
Transfer-Encoding: chunked
...
5
Hello
6
world!
0
There are no significant downsides to supporting chunked data trans-
fers, other than the possibility of pathologically large chunks causing integer
overflows in the browser code or needing to resolve mismatches between
Content-Length and chunk length. (The specification gives precedence to
chunk length, although any attempts to handle this situation gracefully appear
to be ill-advised.) All the popular browsers deal with these conditions prop-
erly, but new implementations need to watch their backs.
Caching Behavior
For reasons of performance and bandwidth conservation, HTTP clients
andsome intermediaries are eager to cache HTTP responses for later reuse.
This must have seemed like a simple task in the early days of the Web, but it
is increasingly fraught with peril as the Web encompasses ever more sensi-
tive, user-specific information and as this information is updated more and
more frequently.
RFC 2616 section 13.4 states that GET requests responded to with a range
of HTTP codes (most notably, “200 OK” and “301 Moved Permanently”) may
be implicitly cached in the absence of any other server-provided directives.
Such a response may be stored in the cache indefinitely, and may be reused
for any future requests involving the same request method and destination
URL, even if other parameters (such as Cookie headers) differ. There is a pro-
hibition against caching requests that use HTTP authentication (see “HTTP
Authentication” on page62), but other authentication methods, such as
cookies, are not recognized in the spec.
58 Chapter 3
When a response is cached, the implementation may opt to revalidate it
before reuse, but doing so is not required most of the time. Revalidation is
achieved by request with a special conditional header, such as If-Modified-Since
(followed by a date recorded on the previously cached response) or If-None-
Match (followed by an opaque ETag header value that the server returned
with an earlier copy). The server may respond with a “304 Not Modified”
code or return a newer copy of the resource.
NOTE The Date/If-Modified-Since and ETag/If-None-Match header pairs, when cou-
pled with Cache-Control: private, offer a convenient and entirely unintended way
for websites to store long-lived, unique tokens in the browser.14 The same can also be
achieved by depositing a unique token inside a cacheable JavaScript file and returning
“304 Not Modified” to all future conditional requests to the token-generating location.
Unlike purpose-built mechanisms such as HTTP cookies (discussed in the next section),
users have very little control over what information is stored in the browser cache,
under what circumstances, and for how long.
Implicit caching is highly problematic, and therefore, servers almost
always should resort to using explicit HTTP-caching directives. To assist with
this, HTTP/1.0 provides an Expires header that specifies the date by which
the cached copy should be discarded; if this value is equal to the Date header
provided by the server, the response is noncacheable. Beyond that simple
rule, the connection between Expires and Date is unspecified: It is not clear
whether Expires should be compared to the system clock on the caching sys-
tem (which is problematic if the client and server clocks are not in sync) or
evaluated based on the Expires – Date delta (which is more robust, but which
may stop working if Date is accidentally omitted). Firefox and Opera use the
latter interpretation, while other browsers prefer the former one. In most
browsers, an invalid Expires value also inhibits caching, but depending on it
isa risky bet.
HTTP/1.0 clients can also include a Pragma: no-cache request header,
which may be interpreted by the proxy as an instruction to obtain a new
copyof the requested resource, instead of returning an existing one. Some
HTTP/1.0 proxies also recognize a nonstandard Pragma: no-cache response
header as an instruction not to make a copy of the document.
In contrast, HTTP/1.1 embraces a far more substantial approach to
caching directives, introducing a new Cache-Control header. The header takes
values such as public (the document is cacheable publicly), private (proxies
are not permitted to cache), no-cache (which is a bit confusing—the response
may be cached but should not be reused for future requests),* and no-store
(absolutely no caching at all). Public and private caching directives may be
accompanied with a qualifier such as max-age, specifying the maximum time
an old copy should be kept, or must-revalidate, requesting a conditional
request to be made before content reuse.
* The RFC is a bit hazy in this regard, but it appears that the intent is to permit the cached
document to be used for purposes such as operating the “back” and “forward” navigation
buttons in a browser but not when a proper page load is requested. Firefox follows this
approach, while all other browsers consider no-cache and no-store to be roughly equivalent.
Hypertext Transfer Protocol 59
Unfortunately, it is typically necessary for servers to return both HTTP/1.0
and HTTP/1.1 caching directives, because certain types of legacy commer-
cial proxies do not understand Cache-Control correctly. In order to reliably
prevent caching over HTTP, it may be necessary to use the following set of
response headers:
Expires: [current date]
Date: [current date]
Pragma: no-cache
Cache-Control: no-cache, no-store
When these caching directives disagree, the behavior is difficult to pre-
dict: Some browsers will favor HTTP/1.1 directives and give precedence to
no-cache, even if it is mistakenly followed by public; others don’t.
Another risk of HTTP caching is associated with unsafe networks, such
aspublic Wi-Fi networks, which allow an attacker to intercept requests to cer-
tain URLs and return modified, long-cacheable contents on requests to the
victim. If such a poisoned browser cache is then reused on a trusted network,
the injected content will unexpectedly resurface. Perversely, the victim does
not even have to visit the targeted application: A reference to a carefully cho-
sen sensitive domain can be injected by the attacker into some other context.
There are no good solutions to this problem yet; purging your browser cache
after visiting Starbucks may be a very good idea.
HTTP Cookie Semantics
HTTP cookies are not a part of RFC 2616, but they are one of the more
important protocol extensions used on the Web. The cookie mechanism
allows servers to store short, opaque name=value pairs in the browser by send-
ing a Set-Cookie response header and to receive them back on future requests
via the client-supplied Cookie parameter. Cookies are by far the most popular
way to maintain sessions and authenticate user requests; they are one of the
four canonical forms of ambient authority* on the Web (the other forms being
built-in HTTP authentication, IP checking, and client certificates).
Originally implemented in Netscape by Lou Montulli around 1994,
anddescribed in a brief four-page draft document,15 the mechanism has not
been outlined in a proper standard in the last 17 years. In 1997, RFC 210916
attempted to document the status quo, but somewhat inexplicably, it also pro-
posed a number of sweeping changes that, to this day, make this specification
substantially incompatible with the actual behavior of any modern browser.
Another ambitious effort—Cookie2—made an appearance in RFC 2965,17 but
a decade later, it still has virtually no browser-level support, a situation that is
* Ambient authority is a form of access control based on a global and persistent property of the
requesting entity, rather than any explicit form of authorization that would be valid only for a
specific action. A user-identifying cookie included indiscriminately on every outgoing request to
a remote site, without any consideration for why this request is being made, falls into that
category.
60 Chapter 3
unlikely to change. A new effort to write a reasonably accurate cookie specifi-
cation—RFC 626518—was wrapped up shortly before the publication of this
book, finally ending this specification-related misery.
Because of the prolonged absence of any real standards, the actual
implementations evolved in very interesting and sometimes incompatible
ways. In practice, new cookies can be set using Set-Cookie headers followed
bya single name=value pair and a number of optional semicolon-delimited
parameters defining the scope and lifetime of the cookie.
Expires Specifies the expiration date for a cookie in a format similar to
that used for Date or Expires HTTP headers. If a cookie is served without
an explicit expiration date, it is typically kept in memory for the duration
of a browser session (which, especially on portable computers with sus-
pend functionality, can easily span several weeks). Definite-expiry cook-
ies may be routinely saved to disk and persist across sessions, unless a
user’s privacy settings explicitly prevent this possibility.
Max-age This alternative, RFC-suggested expiration mechanism is not
supported in Internet Explorer and therefore is not used in practice.
Domain This parameter allows the cookie to be scoped to a domain
broader than the hostname that returned the Set-Cookie header. The
exact rules and security consequences of this scoping mechanism are
explored in Chapter 9.
NOTE Contrary to what is implied in RFC 2109, it is not possible to scope
cookies to a specific hostname when using this parameter. For example,
domain=example.com will always match www.example.com as well.
Omitting domain is the only way to create host-scoped cookies, but even
this approach is not working as expected in Internet Explorer.
Path Allows the cookie to be scoped to a particular request path prefix.
This is not a viable security mechanism for the reasons explained in
Chapter 9, but it may be used for convenience, to prevent identically
named cookies used in various parts of the application from colliding
with each other.
Secure attribute Prevents the resulting cookie from being sent over
nonencrypted connections.
HttpOnly attribute Removes the ability to read the cookie through the
document.cookie API in JavaScript. This is a Microsoft extension, although
it is now supported by all mainstream browsers.
When making future requests to a domain for which valid cookies are
found in the cookie jar, browsers will combine all applicable name=value pairs
into a single, semicolon-delimited Cookie header, without any additional meta-
data, and return them to the server. If too many cookies need to be sent on a
particular request, server-enforced header size limits will be exceeded, and
the request may fail; there is no method for recovering from this condition,
other than manually purging the cookie jar.
Hypertext Transfer Protocol 61
Curiously, there is no explicit method for HTTP servers to delete unneeded
cookies. However, every cookie is uniquely identified by a name-domain-path
tuple (the secure and httponly attributes are ignored), which permits an old
cookie of a known scope to be simply overwritten. Furthermore, if the over-
writing cookie has an expires date in the past, it will be immediately dropped,
effectively giving a contrived way to purge the data.
Although RFC 2109 requires multiple comma-separated cookies to be
accepted within a single Set-Cookie header, this approach is dangerous and is
no longer supported by any browser. Firefox allows multiple cookies to be
setin a single step via the document.cookie JavaScript API, but inexplicably, it
requires newlines as delimiters instead. No browser uses commas as Cookie
delimiters, and recognizing them on the server side should be considered
unsafe.
Another important difference between the spec and reality is that cookie
values are supposed to use the quoted-string format outlined in HTTP specs
(see “Semicolon-Delimited Header Values” on page48), but only Firefox
and Opera recognize this syntax in practice. Reliance on quoted-string values
is therefore unsafe, and so is allowing stray quote characters in attacker-
controlled cookies.
Cookies are not guaranteed to be particularly reliable. User agents enforce
modest settings on the number and size of cookies permitted per domain
and, as a misguided privacy feature, may also restrict their lifetime. Because
equally reliable user tracking may be achieved by other means, such as the
ETag/If-None-Match behavior outlined in the previous section, the efforts to
restrict cookie-based tracking probably do more harm than good.
HTTP Authentication
HTTP authentication, as specified in RFC 2617,19 is the original credential-
handling mechanism envisioned for web applications, one that is now almost
completely extinct. The reasons for this outcome might have been the inflex-
ibility of the associated browser-level UIs, the difficulty of accommodating
more sophisticated non-password-based authentication schemes, or perhaps
the inability to exercise control over how long credentials are cached and
what other domains they are shared with.
In any case, the basic scheme is fairly simple. It begins with the browser
making an unauthenticated request, to which the server responds with a “401
Unauthorized” code.* The server must also include a WWW-Authenticate
HTTP header, specifying the requested authentication method, the realm
string (an arbitrary identifier to which the entered credentials should be
bound), and other method-specific parameters, if applicable.
* The terms authentication and authorization appear to be used interchangeably in this RFC, but
they have a distinctive meaning elsewhere in information security. Authentication is commonly
used to refer to the process of proving your identity, whereas authorization is the process of
determining whether your previously established credentials permit you to carry out a specific
privileged action.
62 Chapter 3
The client is expected to obtain the credentials in one way or the other,
encode them in the Authorization header, and retry the original request with
this header included. According to the specification, for performance rea-
sons, the same Authorization header may also be included on subsequent
requests to the same server path prefix without the need for a second WWW-
Authenticate challenge. It is also permissible to reuse the same credentials in
response to any WWW-Authenticate challenges elsewhere on the server, if the
realm string and the authentication method match.
In practice, this advice is not followed very closely: Other than Safari and
Chrome, most browsers ignore the realm string or take a relaxed approach to
path matching. On the flip side, all browsers scope cached credentials not
only to the destination server but also to a specific protocol and port, a prac-
tice that offers some security benefits.
The two credential-passing methods specified in the original RFC are
known as basic and digest. The first one essentially sends the passwords in
plaintext, encoded as base64. The other computes a one-time cryptographic
hash that protects the password from being viewed in plaintext and prevents
the Authorization header from being replayed later. Unfortunately, modern
browsers support both methods and do not distinguish between them in any
clear way. As a result, attackers can simply replace the word digest with basic in
the initial request to obtain a clean, plaintext password as soon as the user