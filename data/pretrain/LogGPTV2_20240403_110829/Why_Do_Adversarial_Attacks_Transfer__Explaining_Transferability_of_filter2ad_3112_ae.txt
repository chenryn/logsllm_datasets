against increasing maximum perturbation ε.
with labels predicted by the target classiﬁer; i.e., we do not
perform any query on the target), and 1,000 test samples. We
modiﬁed test digits in both classes using Algorithm 1 under
the (cid:96)2 distance constraint (cid:107)x− x(cid:48)(cid:107)2 ≤ ε, with ε ∈ [0,5].
For each of the following learning algorithms, we train a
high-complexity (H) and a low-complexity (L) model, by
changing its hyperparameters: (i) SVMs with linear ker-
nel (SVMH with C = 100 and SVML with C = 0.01); (ii)
SVMs with RBF kernel (SVM-RBFH with C = 100 and SVM-
RBFL with C = 1, both with γ = 0.01); (iii) logistic classiﬁers
(logisticH with C = 10 and logisticL with C = 1); (iv) ridge
classiﬁers (ridgeH with α = 1 and ridgeL with α = 10);2 (v)
fully-connected neural networks with two hidden layers in-
cluding 50 neurons each, and ReLU activations (NNH with
no regularization, i.e., weight decay set to 0, and NNL with
weight decay set to 0.01), trained via cross-entropy loss mini-
mization; and (vi) random forests consisting of 30 trees (RFH
with no limit on the depth of the trees and RFL with a maxi-
mum depth of 8). These conﬁgurations are chosen to evaluate
the robustness of classiﬁers that exhibit similar test accuracies
but different levels of complexity.
How does model complexity impact evasion attack suc-
cess in the white-box setting? The results for white-box eva-
sion attacks are reported for all classiﬁers that fall under our
framework and can be tested for evasion with gradient-based
attacks (SVM, Logistic, Ridge, and NN). This excludes ran-
dom forests, as they are not differentiable. We report the
complete security evaluation curves [5] in Fig. 5, showing the
mean test error (over 10 runs) against an increasing maximum
admissible distortion ε. In Fig. 6a we report the mean test
error at ε = 1 for each target model against the size of its input
gradients (S, averaged on the test samples and on the 10 runs).
The results show that, for each learning algorithm, the low-
complexity model has smaller input gradients, and it is less
vulnerable to evasion than its high-complexity counterpart,
conﬁrming our theoretical analysis. This is also conﬁrmed by
the p-values reported in Table 1 (ﬁrst column), obtained by
2Recall that the level of regularization increases as α increases, and as C
decreases.
328    28th USENIX Security Symposium
USENIX Association
012345ε0.00.20.40.60.81.0TestErrorWhite-boxevasionattack(MNIST89)SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL(a)
(b)
(c)
(d)
Figure 6: Evaluation of our metrics for evasion attacks on MNIST89. (a) Test error under attack vs average size of input gradients
(S) for low- (denoted with ‘×’) and high-complexity (denoted with ‘◦’) classiﬁers. (b) Average transfer rate vs variability of loss
landscape (V). (c) Pearson correlation coefﬁcient ρ(ˆδ,δ) between black-box (ˆδ) and white-box (δ) perturbations (values in Fig. 8,
right) vs gradient alignment (R, values in Fig. 8, left) for each target-surrogate pair. Pearson (P) and Kendall (K) correlations
between ρ and R are also reported along with the p-values obtained from a permutation test to assess statistical signiﬁcance.
Evasion
Poisoning
MNIST89
ε = 1
<1e-2
<1e-2
<1e-2
<1e-2
<1e-2
ε = 1
<1e-2
<1e-2
<1e-2
<1e-2
<1e-2
DREBIN
ε = 5
<1e-2
<1e-2
<1e-2
<1e-2
<1e-2
ε = 30
<1e-2
0.02
<1e-2
<1e-2
0.02
SVM
logistic
ridge
SVM-RBF
NN
MNIST89
5%
<1e-2
<1e-2
0.02
<1e-2
20%
<1e-2
<1e-2
<1e-2
<1e-2
LFW
5% 20%
0.75
0.21
0.75
0.11
<1e-2
0.10
0.02
<1e-2
Table 1: Statistical signiﬁcance of our results. For each attack,
dataset and learning algorithm, we report the p-values of
two two-sided binomial tests, to respectively reject the null
hypothesis that: (i) for white-box attacks, the test errors of the
high- and low-complexity target follow the same distribution;
and (ii) for black-box attacks, the transfer rates of the high-
and low-complexity surrogate follow the same distribution.
Each test is based on 10 samples, obtained by comparing
the error of the high- and low-complexity models for each
learning algorithm in each repetition. In the ﬁrst (second)
case, success corresponds to a larger test (transfer) error for
the high-complexity target (low-complexity surrogate).
running a binomial test for each learning algorithm to com-
pare the white-box test error of the corresponding high- and
low-complexity models. All the p-values are smaller than
0.05, which conﬁrms 95% statistical signiﬁcance. Recall that
these results hold only when comparing models trained using
the same learning algorithm. This means that we can com-
pare, e.g., the S value of SVMH against SVML, but not that
of SVMH against logisticH. In fact, even though logisticH
exhibits the largest S value, it is not the most vulnerable clas-
siﬁer. Another interesting ﬁnding is that nonlinear classiﬁers
tend to be less vulnerable than linear ones.
How do evasion attacks transfer between models in black-
box settings? In Fig. 7 we report the results for black-box
evasion attacks, in which the attacks against surrogate models
(in rows) are transferred to the target models (in columns).
The top row shows results for surrogates trained using only
20% of the surrogate training data, while in the bottom row
surrogates are trained using all surrogate data, i.e., a training
set of the same size as that of the target. The three columns
report results for ε ∈ {1,2,5}.
It can be noted that lower-complexity models (with stronger
regularization) provide better surrogate models, on average.
In particular, this can be seen best in the middle column for
medium level of perturbation, in which the lower-complexity
models (SVML, logisticL, ridgeL, and SVM-RBFL) provide
on average higher error when transferred to other models.
The reason is that they learn smoother and stabler functions,
that are capable of better approximating the target function.
Surprisingly, this holds also when using only 20% of training
data, as the black-box attacks relying on such low-complexity
models still transfer with similar test errors. This means that
most classiﬁers can be attacked in this black-box setting with
almost no knowledge of the model, no query access, but pro-
vided that one can get a small amount of data similar to that
used to train the target model.
These ﬁndings are also conﬁrmed by looking at the variabil-
ity of the loss landscape, computed as discussed in Sect. 4 (by
considering 10 different training sets), and reported against
the average transfer rate of each surrogate model in Fig. 6b. It
is clear from that plot that higher-variance classiﬁers are less
effective as surrogates than their less-complex counterparts,
as the former tend to provide worse, unstable approximations
of the target classiﬁer. To conﬁrm the statistical signiﬁcance
of this result, for each learning algorithm we also compare the
mean transfer errors of high- and low-complexity surrogates
with a binomial test whose p-values (always lower than 0.05)
are reported in Table 1 (second column).
Another interesting, related observation is that the adversar-
ial examples computed against lower-complexity surrogates
have to be perturbed more to evade (see Fig. 9), whereas the
perturbation of the ones computed against complex models
USENIX Association
28th USENIX Security Symposium    329
10−1Sizeofinputgradients(S)0.20.40.60.81.0Testerror(ε=1)SVMlogisticridgeSVM-RBFNN10−510−410−3Variabilityoflosslandscape(V)0.120.140.160.180.200.22Transferrate(=1)0.20.40.60.8Gradientalignment(R)0.20.30.40.50.60.70.80.9ρ(ˆδ,δ)(=5)P:0.99,p-val:<1e-10K:0.93,p-val:<1e-100.20.40.60.8Gradientalignment(R)0.20.40.60.8Black-towhite-boxerrorratio(=1)P:0.91,p-val:<1e-10K:0.72,p-val:<1e-10(a) ε = 1
(b) ε = 2
(c) ε = 5
Figure 7: Black-box (transfer) evasion attacks on MNIST89. Each cell contains the test error of the target classiﬁer (in columns)
computed on the attack samples crafted against the surrogate (in rows). Matrices in the top (bottom) row correspond to attacks
crafted against surrogate models trained with 20% (100%) of the surrogate training data, for ε ∈ {1,2,5}. The test error of each
target classiﬁer in the absence of attack (target error) and under (white-box) attack are also reported for comparison, along with
the mean transfer rate of each surrogate across targets. Darker colors mean higher test error, i.e., better transferability.
can be smaller. This is again due to the instability induced
by high-complexity models into the loss function optimized
to craft evasion attacks, whose sudden changes cause the
presence of closer local optima to the initial attack point.
On the vulnerability of random forests. A noteworthy ﬁnd-
ing is that random forests can be effectively attacked at small
perturbation levels using most other models (see last two
columns in Fig. 7). We looked at the learned trees and dis-
covered that trees often are susceptible to small changes. In
one example, a node of the tree checked if a particular feature
value was above 0.002, and classiﬁed samples as digit 8 if that
condition holds (and digit 9 otherwise). The attack modiﬁed
that feature from 0 to 0.028, causing it to be immediately
misclassiﬁed. This vulnerability is intrinsic in the selection
process of the threshold values used by these decision trees to
split each node. The threshold values are selected among the
existing values in the dataset (to correctly handle categorical
attributes). Therefore, for pixels which are highly discriminant
(e.g., mostly black for one class and white for the other), the
threshold will be either very close to one extreme or the other,
making it easy to subvert the prediction by a small change.
Since (cid:96)2-norm attacks change almost all feature values, with
high probability the attack modiﬁes at least one feature on