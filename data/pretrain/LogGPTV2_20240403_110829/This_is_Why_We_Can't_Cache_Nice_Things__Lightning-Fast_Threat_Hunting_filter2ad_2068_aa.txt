title:This is Why We Can't Cache Nice Things: Lightning-Fast Threat Hunting
using Suspicion-Based Hierarchical Storage
author:Wajih Ul Hassan and
Ding Li and
Kangkook Jee and
Xiao Yu and
Kexuan Zou and
Dawei Wang and
Zhengzhang Chen and
Zhichun Li and
Junghwan Rhee and
Jiaping Gui and
Adam Bates
This is Why We Can’t Cache Nice Things: Lightning-Fast Threat
Hunting using Suspicion-Based Hierarchical Storage
Wajih Ul Hassan⋄, Ding Li‡, Kangkook Jee◦, Xiao Yu∗, Kexuan Zou⋄, Dawei Wang⋄, Zhengzhang
Chen∗, Zhichun Li∗, Junghwan “John” Rhee†, Jiaping Gui∗, Adam Bates⋄
⋄University of Illinois at Urbana-Champaign ‡Peking University ◦University of Texas at Dallas
†University of Central Oklahoma ∗NEC Laboratories America, Inc.
ABSTRACT
Recent advances in the causal analysis can accelerate incident re-
sponse time, but only after a causal graph of the attack has been
constructed. Unfortunately, existing causal graph generation tech-
niques are mainly offline and may take hours or days to respond to
investigator queries, creating greater opportunity for attackers to
hide their attack footprint, gain persistency, and propagate to other
machines. To address that limitation, we present Swift, a threat
investigation system that provides high-throughput causality track-
ing and real-time causal graph generation capabilities. We design
an in-memory graph database that enables space-efficient graph
storage and online causality tracking with minimal disk operations.
We propose a hierarchical storage system that keeps forensically-
relevant part of the causal graph in main memory while evicting
rest to disk. To identify the causal graph that is likely to be relevant
during the investigation, we design an asynchronous cache evic-
tion policy that calculates the most suspicious part of the causal
graph and caches only that part in the main memory. We evaluated
Swift on a real-world enterprise to demonstrate how our system
scales to process typical event loads and how it responds to foren-
sic queries when security alerts occur. Results show that Swift is
scalable, modular, and answers forensic queries in real-time even
when analyzing audit logs containing tens of millions of events.
KEYWORDS
Auditing, Data Provenance, Digital Forensics
ACM Reference Format:
Wajih Ul Hassan⋄, Ding Li‡, Kangkook Jee◦, Xiao Yu∗, Kexuan Zou⋄, Dawei
Wang⋄, Zhengzhang Chen∗, Zhichun Li∗, Junghwan “John” Rhee†, Jiap-
ing Gui∗, Adam Bates⋄. 2020. This is Why We Can’t Cache Nice Things:
Lightning-Fast Threat Hunting using Suspicion-Based Hierarchical Stor-
age. In Annual Computer Security Applications Conference (ACSAC 2020),
December 7–11, 2020, Austin, USA. ACM, New York, NY, USA, 14 pages.
https://doi.org/10.1145/3427228.3427255
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ACSAC 2020, December 7–11, 2020, Austin, USA
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8858-0/20/12...$15.00
https://doi.org/10.1145/3427228.3427255
1 INTRODUCTION
Modern organizational networks are sprawling and diverse, hosting
data of tremendous value to malicious actors. Unfortunately, due
to the complexity of organizations and time-consuming nature
of threat investigations, attackers are able to dwell on the target
system for longer periods. In slow-moving targeted attacks (e.g.,
Equifax [29]), the amount of damage wrought by the attacker grows
exponentially as their dwell time in the system increases [18], with
a recent study reporting that it costs organizations $32,000 for
each day an attacker persists in the network [36]. This situation
is made even worse when considering fast-spreading attacks; the
infamous Slammer worm [65] that infected more than 75,000 hosts
within the first ten minutes of its release, and recent ransomware
attacks [9, 19, 24] exhibit a similar replication factor. Regardless
of the specific attack, delayed response times imply significantly
larger negative consequences. Thus, to minimize repercussions
of intrusions, cyber analysts require tools that facilitate fast and
interactive threat hunting.
Given its vital importance, what are the key factors that de-
termine the success of the threat hunting process? The various
steps involved in post-breach threat hunting [14] are summarized
in Figure 1. Effectiveness is usually measured using two metrics
in industry [5]: 1) Mean-time-to-detect (MTTD), which measures
the time required for the organization’s Threat Detection Software
(TDS) to detect suspicious activity and raise a security alert; and
2) Mean-time-to-know (MTTK), which measures the time required
for cyber analysts to make sense of alert and unearth evidence that
the alert is indicative of a true attack. Depending upon the volume
of threat alerts and the analysis tools available to the analyst, this
process can typically range from hours to days for an individual
threat alert [15, 18].
Recently, threat hunting has become a subject of renewed in-
terest in the literature, primarily due to advancements in causal
analysis [30, 31, 38, 39, 43–45, 47–49, 53, 54, 58, 60, 61] that can
can reduce MTTK during the post-breach threat hunting process.
Causality analysis incrementally parses audit log events generated
by system-level logging tools (e.g., Linux Audit [4]) into causal
graphs (i.e., provenance graphs) that encode the dependency rela-
tionships between subjects (e.g., processes) and objects (e.g., files)
in the system. Such graphs not only provide the historical con-
text needed by analysts to quickly understand alerts, but have also
been shown to be useful for alert management (e.g., triage [41],
correlation [40, 63], cyber threat intelligence [62, 69]).
Figure 1: Typical post-breach threat hunting in an enterprise. Both
alert management (e.g., triage) and investigation steps require
causal graphs of generated alerts.
Unfortunately, at present the performance of causal analysis is a
limiting factor to their widespread adoption – early attempts to de-
ploy these techniques in practice reported graph construction times
ranging from hours to days and unwieldy audit logs that reached
terabytes in size over just a week (e.g., [57]). These existing tools fall
under two categories: 1) disk-based offline approaches (e.g., [41, 57])
that incur significant I/O bottleneck and takes hours to respond to
each query, thereby increasing MTTK; and 2) memory-based online
approaches (e.g., [31, 44]) that require the whole causal graph to be
stored in main-memory for analysis, which cannot scale to even
modestly-sized organizations. As neither approach is a practical
candidate for deployment, prior work has sought to improve the
performance of causal analysis through various forms of graph
reduction and compression (e.g., [32, 40, 42, 46, 55, 71, 73, 74]). By
reducing the number of log events to process, those techniques
have indeed improved query latency and alleviated the burdens of
long-term storage. However, these approaches potentially affect the
fidelity of logs for answering key forensic queries.1 Further, over
longer periods those techniques do not provide a scalable solution
to log analysis and management.
In this work, we propose a causal analysis and alert management
framework that can process logs and forensic queries as quickly as
the system event stream. Unfortunately, building a highly scalable
real-time causality tracker is a daunting task. The challenge comes
from the volume and velocity of system events that are in large
enterprises. Three key challenges need to be answered before we
can build this scalable mechanism:
C1 Scalable Ingest: How can we continuously ingest and process
upwards of terabytes of system events per day?
C2 Fast Graph Retrieval: How can we quickly recover causal
graphs of recent alerts, especially when alerts’ dependencies
may extend back weeks into the past?
C3 Efficient Alert Management: How can we incorporate causal-
ity analysis into real-time alert management to help cyber
analysts cope with the deluge of alerts?
1.1 Approach Overview & Contributions
To address these challenges, we designed Swift2, a causality tracker
for which scalability and performance are first-class citizens. Fig-
ure 2 presents an overview of the Swift architecture. Enterprise-
wide audit logs are first collected into a Kafka broker [11] and then
fetched by the consumer threads of Swift. Each consumer thread
1For example, LogGC removes subgraphs associated with closed sockets and thus
could obscure data exfiltration attempts [55], while Winnower may prevent attack
attribution by abstracting remote IP addresses [42].
2Swift is a recursive acronym for Swift investigator for threat alerts.
Figure 2: Overview of Swift architecture.
buffers the events for a certain configurable window, organizes the
out-of-order events based on their timestamps, and merges con-
tinuous events that have the same source and destination.3 Then,
these audit log events are fed into a novel hierarchical storage
management (HSM) system.
The challenge of scalable ingestion (C1) is met by the first contri-
bution of this paper, a novel vertex-centric graph schema and data-
base that is tailored for online causality analysis. This in-memory
causal graph database allows Swift to quickly identify the causal
relationships of streaming events with all causally-related events
that occurred previously. We show that our graph database is space-
efficient and is an enabling factor in providing real-time query
results without significant disk I/O during our experiments.
The challenge of fast graph retrieval (C2) is resolved through
the introduction of a causal graph HSM that consists of a two-
layered memory cache (the tracking cache and suspicious cache,
respectively), and a disk. This HSM automatically moves causal
graph segments between main-memory and disk to achieve high-
throughput data ingestion and low-latency query results. However,
incorporating an HSM into an existing causal analysis framework
is non-trivial – a generic cache eviction strategy would regularly
evict forensically-relevant events, leading to increased disk access
and high query latency.
Our solution to eviction is based on two distinct insights that
motivate our two-layered memory cache design. The first insight
is that of temporal locality; recent events have a high probability
of dependence with upcoming events in the near future, Based on
this observation we formulated an Epochal Causality Hypothesis,
described in Section 6.1, and store recent events in the tracking
cache. As events age out of the tracking cache, a decision must be
made as to which events are likely to be used in forensic queries
and should thus be retained in memory.
To identify forensically-relevant events, we formulated a Most
Suspicious Causal Paths Hypothesis which states that, given a suspi-
cious influence score algorithm (e.g., [33, 41, 50, 51, 57]) that satisfies
three key properties described in Section 6.2, we can calculate the
most suspicious causal paths in an online fashion (on time-evolving
graphs); as these paths are more likely to be associated with a true
attack, they are also the most likely to be queried and should thus be
retained in the suspicious cache. Note that a causal graph consists
of one or more causal paths (further described in Section 4). Finally,
to quickly identify top-k most suspicious causal paths seen so far
3Most of the operating systems introduce several system-level events for single file
operation. Aggregating these events together does not affect the correctness of causality
analysis but saves substantial space.
2
HostsAudit LogThreat DetectorTriage & CorrelateInvestigateRecoveryRequire Causal GraphMTTDAttacker’s Dwell TimeMTTKHierarchical StoragePromotionDiskTracking CacheSuspicious CacheAlert Manager-Sort-AggregateConsumerThreadStreamingAudit eventsData IngestionAlert’s Causal GraphThreat DetectorAlertCyber AnalystsEvictionGlobalListin the enterprise, Swift also maintains a Global List that stores
pointers to such paths.
The final contribution of this paper considers the matter of ef-
ficient alert management (C3), which is a vital consideration to
mitigating threat alert fatigue [18]. Swift includes an alert manage-
ment layer on top of its HSM. When alerts are fired by a connected
TDS (e.g., Splunk [70]), Swift automatically leverages its suspi-
cious influence scores to perform alert triage based on historical
context,4 allowing the analyst to investigate the most likely threats
first. Further, during online causality tracking, Swift keeps track
of all previously-fired alerts. When an alert has a causal relation
with a previously fired alert, Swift fuses these events into a single
causal graph to display to the analyst.
1.2 Summary of Results
We deployed and evaluated our system at NEC Labs America, com-
prised of 191 hosts. Our case studies on this testbed confirm that
Swift can retrieve the most critical parts of an APT attack from a
database of over 300 million events in just 20 ms. Swift successfully
classified 140 security alerts and responded to forensic queries in
less than 2 minutes, reducing the latency of the state-of-the-art alert
triage tools by 5 hours. With this result, we estimate that Swift
can scale to monitor upwards of 4,000 hosts on a single server.
Further, Swift can scale to support thousands of monitored hosts
on a single machine using just 300 MB memory, thus addressing a
central limitation of existing causal analysis techniques. We clarify
at the outset that Swift does not improve or detract from the efficacy
of its two modular components, the underlying TDS (e.g., [70]) and
suspicious influence scoring algorithm (e.g., [41, 57]); instead, Swift
seeks to improve security by dramatically improving the speed and
scalability of causality-based threat hunting solutions.
2 RELATED WORK
Performance of Causal Analysis. Several threat investigation
systems, such as PrioTracker [57], SAQL [37], and NoDoze [41]
have been proposed to improve the performance of causal analysis
in enterprises. Those systems use disk-based approach and may
take hours to respond to each query. In a large enterprise with
high-speed alerts, such response times are ineffective, increasing
MTTK and attacker’s dwell time.
CamQuery system [68] supports scalable online analysis of causal
graphs. However, CamQuery only supports iterative computation
of queries as pre-written programs. It does not support full forensic
querying which cannot be known ahead of time and thus cannot
be used for active threat hunting. Sleuth [44], Holmes [63], and
Poirot [62] use in-memory graph database to provide real-time
forensic analysis; however, they require whole causality data to be
stored in main memory for forensic analysis. Thus, those systems
cannot scale to enterprises that usually produce terabytes of data
per week [57].
KCAL [59] proposed a kernel-level cache to remove redundant
causal events and reduce the overhead of log transfer from kernel
to user-space. However, the log is eventually stored on disk which