fault injection on mm without considering thread deviation
leads to about 5% SDC rate, compared to performing fault
injection on the master thread, which has an SDC rate of 28%.
60% 
50% 
40% 
30% 
20% 
10% 
0% 
bfs-master 
lud-master 
bfs-slave 
lud-slave 
nn-master 
SDC rate of slave threads 
SDC rate of the master thread 
hotspot-master 
hotspot-slave 
nn-slave 
pathfinder-master 
pathfinder-slave 
srad-master 
nw-slave 
nw-master 
kmeans-master 
kmeans-slave 
srad-slave 
Fig. 4: Differences in the SDC rates of applications between injections in the
master thread and slave threads
B. Segment-level Differences on Error Resilience
Our solution for evaluating the resilience of OpenMP pro-
grams more accurately is to consider the resilience of the
master thread and slave thread separately. For slave threads,
we need to consider only the parallel segment in which slave
threads are spawned. However, the master thread consists of
multiple segments, and hence we need to consider the error
resilience of each segment separately. To do that, we ﬁrst
manually split the each benchmark into segments and measure
the time spent in each segment. We intend to automate this
process in the future. Figure 5 shows the execution time of
each segment of the benchmarks. As we can see, the parallel
segment dominates the execution time in six of the programs,
while I/O operations (output processing in these cases) take
longer time than the other segments in two programs (bfs and
srad).
Input processing  Pre-algorithm  Parallel Region  Post-algorithm  Output processing 
100% 
80% 
60% 
40% 
20% 
0% 
bfs 
lud 
nn 
pathfinder  hotspot 
nw 
srad 
kmeans 
Fig. 5: Execution time proﬁle of the OpenMP benchmarks
Ideally, fault injection should be based on the execution time
of each segment as measured by a consistent metric such as
in cycles. Because we are limited by the LLFI infrastructure,
which lacks information about the underlying execution of
the program, our fault injection is based on the number of
dynamic instructions executed in each segment. Our injection
proceeds as follows. First, we manually map each segment
in the source code to IR code, and ﬁnd the corresponding
instructions that represent the boundaries in the IR code. We
use this information to ﬁnd the IR instruction range for each
segment. Knowing these boundaries, when we inject a fault
using LLFI at the IR code level, we can tell when the fault
is injected and which segment the program is executing at the
time of the injection.
Figure 6 shows the SDC rate of each segment in seven
of the eight benchmarks. We did not show nn because the
overall SDC rate of nn is quite small compared to other
benchmarks (less than 1%). We also skip the output segments
723
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:14 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: Mean and the standard deviation of the number of dynamic instructions of threads
Benchmark
Mean
Standard deviation
bfs
42,505
796
lud
3,568,024
292,682
nn
327,652
178,752
pathﬁnder
31,320
364
hotspot
14,275
2,568
nw
6,540,148
44,123
srad
1,993,166
29,487
kmeans
4,150,695
383,953
TABLE II: Mean and standard deviation of SDC rate in each segment of the
programs
Segment
Input
processing
Pre-
algorithm
Parallel
segment
Post-
algorithm
Output
processing
Number of
applications
Mean
Standard
deviation
6
2
7
2
5
28.59%
11%
20.32%
23%
16.13%
14%
27%
13.2%
42.42%
5.1%
a The ﬁrst row shows the count of number of applications that contain the
segment
of the nw and pathﬁnder applications, because the time spent
in those segment is less than 1% of the total time, which
means that the chance that faults happen during the execution
of those segments is small. We quantify the mean and standard
deviation for each segment as shown in Table II.
We ﬁnd that output processing segments exhibit much
higher average SDC rates than other segments. This is intuitive
because output processing is close to the end of the execution,
and so faults are more likely to affect the ﬁnal output. We
also ﬁnd that the standard deviations of SDC of the pre-
algorithm, post-algorithm and parallel segments are higher
than the corresponding standard deviations of SDCs in the
input processing and output processing segments. This shows
that the resilience of the input and output processing segments
is more stable than the algorithm-related segments across the
applications.
SDC rate in input processing 
SDC rate in parallel region 
SDC rate in output processing 
SDC rate in pre-algorithm 
SDC rate in post-algorithm 
60% 
50% 
40% 
30% 
20% 
10% 
0% 
bfs 
lud 
hotspot 
nw 
pathfinder 
srad 
kmeans 
Fig. 6: Differences in the SDC rates in different segments of the benchmarks
Finally, to estimate the overall resilience, we propose two
approaches depending on the goal of the resilience character-
ization. In the ﬁrst approach, we consider the SDC rate of
each segment separately and combine it with its time proﬁle,
to obtain the overall SDC rate. In the second approach, we
consider only the algorithm-related segments, (i.e., pre- and
post- algorithm and parallel segments). The results for both
cases are shown in Figure 7. For the ﬁrst case, the average
SDC rate is 20%; for the second case, it is 14%. In both
cases, the highest SDC rate is from lud and the lowest is from
nn.
724
50% 
40% 
30% 
20% 
10% 
0% 
50% 
40% 
30% 
20% 
10% 
0% 
bfs 
bfs 
lud  nn 
pathfinder 
hotspot 
lud  nn 
pathfinder 
hotspot 
nw 
kmeans 
srad 
nw 
kmeans 
srad 
Fig. 7: SDC rates estimated in two approaches. Top: End-to-end SDC rates
considering all segments Bottom: SDC rates when considering only algorithm-
related segments excluding the input and output processing segments
V. DISCUSSION
Our previous study on the error resilience of GPGPU
applications [4] suggests that the algorithmic characteristics of
the application inﬂuence its error resilience. The algorithmic
operations we consider are described in Table III. For example,
comparison-based operations, such as those used for searching
for keys or in min/max value computation, have much higher
error resilience than other operators. As another example,
”average-out” operations in which the ﬁnal result is based on
iterative computation over the previous results similarly have
high resilience.
TABLE III: Operations that may affect resilience of GPGPU applications
Operations
Comparison-based
Average-out
Graph processing
Linear algebra
Bit-wise operation
Description
Comparing two values,
High resilience as the likelihood
of maintaining the correct value is
high
The ﬁnal state is a product of
multiple temporary states, usually
including iteration and merging
Graph-related algorithms such as
breadth-ﬁrst search
Combination of basic linear alge-
bra computation
Input data are chunked based on
certain length in bits
One of the goals of this work is to test the hypothesis
that the error resilience of an application is correlated with
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:14 UTC from IEEE Xplore.  Restrictions apply. 
its algorithmic operations. If the hypothesis is true, then we
should observe a trend in OpenMP programs similar to the
tread we observed in GPGPU programs. To this end, we
analyze the algorithms of eight benchmarks to identify the
algorithmic operations performed. Table IV lists the high-
level classiﬁcation (dwarf) of these benchmarks from the
Rodinia homepage, and the operations that we found in each
benchmark.
We identify a total of ﬁve operations that may affect the
error resilience of the application, four of which are also found
in GPGPU applications: comparison-based, graph processing,
linear algebra and average out. The ﬁfth operation we intro-
duce is grid-structured computation, which consists of regular
or irregular grid calculations such as stencil computations.
TABLE IV: Operations of OpenMP benchmarks and the estimation of the
resilience
Operations
Comparison-
based
Grid computa-
tion
Graph process-
ing
Average-out
Linear algebra
Benchmarks Observed SDC
0.07% ∼ 1%
nn, nw
hotspot, srad
bfs,pathﬁnder
23%
9% ∼ 10%
kmeans
lud
4.2%
44%
Application dwarfs
Dynamic program-
ming, dense linear
algebra
Structured grid
Graph traversal, dy-
namic programming
Dense linear algebra
Dense linear algebra
Table IV shows the observed SDC rates for the OpenMP
applications grouped by the algorithmic operations. Among the
ﬁve operations we considered, comparison-based and average-
out operations are the most resilient and linear algebra oper-
ations are the least resilient. This matches what we found in
GPGPU applications for the four shared categories, suggesting
that algorithmic operations may be an important factor in
understanding the error resilience of parallel programs on both
CPUs and GPUs. Based on our preliminary study, we intend
to explore this space more systematically in the future.
VI. SUMMARY
This paper presents a methodology to investigate the end-
to-end error resilience characteristics of OpenMP applications
through fault injection. This methodology overcomes the chal-
lenges in building a fault injector for OpenMP applications by
taking into account the thread model and program structure of
OpenMP applications. We extend an existing fault-injection
tool, LLFI, to support multi-threading and inject faults into
eight OpenMP programs from the Rodinia benchmark suite.
Our experiments show that, on average, 14% of the injected
faults result in SDC when only the algorithm-related parts of
the code are considered, while 20% of the injected faults result
in SDC when the entire program (including input and output
processing) is considered. We also ﬁnd signiﬁcant variations
in SDC rates depending on the thread and program segment
into which the fault is injected. Finally, we ﬁnd preliminary
evidence that the algorithmic characteristics of an application
are correlated with its observed SDC rates. While these results
are preliminary, they corroborate with our earlier results for
GPGPU applications, which showed that these correlations
might not be platform-speciﬁc.
ACKNOWLEDGMENT
This work was supported in part by an NSERC Discovery
grant, an NSERC Engage Grant and a research gift from AMD
Corporation. We thank the anonymous reviewers of FTXS2014
for their feedback to improve the paper.
REFERENCES
[1] S. Michalak, A. Dubois, C. Storlie, H. Quinn, W. Rust, D. DuBois,
D. Modl, A. Manuzzato, and S. Blanchard, “Assessment of the impact of
cosmic-ray-induced neutrons on hardware in the roadrunner supercom-
puter,” Device and Materials Reliability, IEEE Transactions on, vol. 12,
no. 2, pp. 445–454, June 2012.
[2] B. Schroeder, E. Pinheiro, and W.-D. Weber, “Dram errors in the wild:
A large-scale ﬁeld study,” in Proceedings of the 11th International Joint
Conference on Measurement and Modeling of Computer Systems, ser.
SIGMETRICS ’09, 2009.
[3] V. Sridharan and D. Liberty, “A study of dram failures in the ﬁeld,”
in Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis, ser. SC ’12, 2012.
[4] B. Fang, K. Pattabiraman, M. Ripeanu, and S. Gurumurthi, “Gpu-qin: A
methodology for evaluating the error resilience of gpgpu applications,”
in IEEE International Symposium on Performance Analysis of Systems
and Software (ISPASS), March 2014.
[5] G. J. Barbara Chapman and R. van der Pas, Using OpenMP.
Hayward Street, Cambridge, Mass, USA: The MIT press, 2007.
[6] C. Lattner and V. Adve, “LLVM: A compilation framework for lifelong
program analysis and transformation,” San Jose, CA, USA, Mar 2004.
[7] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the
accuracy of high-level fault injection techniques for hardware faults,” in
Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP
International Conference on, June 2014.
[8] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee,
and K. Skadron, “Rodinia: A benchmark suite for heterogeneous
computing,” in Proceedings of the 2009 IEEE International Symposium
on Workload Characterization (IISWC), ser. IISWC ’09, 2009, pp. 44–
54. [Online]. Available: http://dx.doi.org/10.1109/IISWC.2009.5306797
[9] J. Aidemark, J. Vinter, P. Folkesson, and J. Karlsson, “Gooﬁ: generic
object-oriented fault injection tool,” in Dependable Systems and Net-
works, 2001 International Conference on, 2001, pp. 83–88.
55
[10] D. Stott, B. Floering, D. Burke, Z. Kalbarczpk, and R. Iyer, “Nftape:
a framework for assessing dependability in distributed systems with
lightweight fault injectors,” in IPDPS 2000, 2000, pp. 91 –100.
[11] D. Li, J. Vetter, and W. Yu, “Classifying soft error vulnerabilities
in extreme-scale scientiﬁc applications using a binary instrumentation
tool,” in High Performance Computing, Networking, Storage and Anal-
ysis (SC), 2012 International Conference for, 2012, pp. 1–11.
[12] C. da Lu and D. Reed, “Assessing fault sensitivity in mpi applications,”
in Supercomputing, 2004. Proceedings of the ACM/IEEE SC2004 Con-
ference, Nov 2004, pp. 37–37.
[13] J. Wei and K. Pattabiraman, “BLOCKWATCH: Leveraging similarity
in parallel programs for error detection,” in Proceedings of the IEEE
International Conference on Dependable Systems and Networks (DSN),
2012.
[14] J. Sloan, R. Kumar, and G. Bronevetsky, “An algorithmic approach
to error localization and partial recomputation for low-overhead fault
tolerance,” in Dependable Systems and Networks (DSN), 2013 43rd
Annual IEEE/IFIP International Conference on, June 2013, pp. 1–12.
[15] K. S. Yim, Z. Kalbarczyk, and R. Iyer, “Hauberk: Lightweight silent data
corruption error detector for gpgpu,” in IEEE International Parallel and
Distributed Processing Symposium, 2011.
[16] W. Gu, Z. Kalbarczyk, and R. Iyer, “Error sensitivity of the linux kernel
executing on powerpc g4 and pentium 4 processors,” in Dependable
Systems and Networks, 2004 International Conference on, 2004, pp.
887–896.
[17] [Online]. Available: http://clang-omp.github.io/
725
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:14 UTC from IEEE Xplore.  Restrictions apply.