 0.01
 0.1
 0.1
 1
 1
 10
 10
 100
 100
False Positive Rate
 1
 0.8
 0.6
 0.4
 0.2
 0
 0  5  10 15 20 25 30 35 40 45 50
CO index
 0  1  2  3  4  5  6  7  8
Time (in days)
Figure 9: Comparison of accuracy of Q-scores with different
skip intervals Δ
Figure 10: Normalized amount of users with customer reports
over different spatial locations (COs) and times.
events bi and user feedback ci. The default time gap (or skipping
interval) between si +δ and ui is 0 hour because we set ui = si +δ
in Section 3.3. In this test, we add the skipping time gap Δ to the
equation ui = si + δ + Δ. By increasing Δ in the online moni-
toring step of Q-score generation, we test the regression for larger
delays between bi and ci, in other words, we test for the stability of
Q-score in proactive, early warning.
With various Δ ranging from 0 hours to 36 hours, Figure 9 ex-
hibits FPR-FNR of learned β with different skipping times. As
we increase Δ, the regression gets to rely on the user feedback of
longer time after the occurrences of network events. And we ob-
serve that the FPR-FNR trade off gets worse as results. While the
choice of lead time should mainly be left to the discretion of net-
work administrators, we ﬁnd 9 hours of lead time is at the feasible
level, as observing 9 hours of skip interval preserves 0.1% of FPR
only sacriﬁcing 10% of FNR (i.e., FNR is 30% when skip interval
is 0 hours and 40% when skip interval is 9 hours).
5.3 Dimensioning Customer Care Workforce
If network problems occur locally to regional service areas rather
than globally, an efﬁcient management of ﬁeld operators (e.g., cus-
tomer care representatives and repair men at customer premises)
and servicing resources (e.g., devices for ﬁne-grained monitoring
of network) would be to dynamically allocate them to challenging
service regions than assigning static work areas. Thus, predicting
the volume of forecoming issues to a service region at a given time
is beneﬁcial in adaptively allocating workforce across service re-
gions. In this application, we assess the possibility of pre-allocating
a customer care workforce to potentially troubling service areas us-
ing Q-score. To begin, we ﬁrst assess the volume of service quality
issues per different spatial regions and see if the issues are con-
tained locally or spread out globally.
Spatial Distribution of User Feedback. Figure 10 shows the spa-
tial distribution of user feedback across different COs. The x-axis
shows indexes of different COs, the z-axis shows temporal trend.
The y-axis shows the amount of customer calls normalized by the
peak value (e.g., a value of 1 represents that the corresponding CO
and time has the highest amount of calls shown in the ﬁgure) 1 . At
a given time, we observe that high user feedback is local to each
1To protect proprietary information, we normalize some informa-
tion in the results to the extent that the normalization does not ob-
struct interpretation of results
CO. And over time, the areas of high user feedback changes from
one CO to another. From the fact that high feedback values gen-
erally being uncorrelated across time and CO (or space), we can
afﬁrm that the issues are temporal rather than permanent and local
to an area rather than being global.
Leveraging Q-score for Dimensioning Workforce. Now that we
have seen the possibility of dynamic resource allocation over dif-
ferent COs, we evaluate how closely Q-score follows user feedback
in its magnitude when aggregated across individuals within each
COs. Note that, to focus on its similarity to user feedback rate, we
ignored the lead time of Q-score in this test. Figure 11 shows the
trend of Q-score and user feedback aggregated per-CO. In doing so,
Q-scores of individual user ID are ﬁrst computed, and the scores
corresponding to individuals within each CO are aggregated to-
gether to form per-CO Q-score. To compare three subﬁgures under
the same scale factor, the plots are normalized by the peak customer
call rate appearing in Figure 11(a), 22 hour time. Figure 11(a)
shows the trend of per-CO Q-score and user feedback for a CO
with relatively high customer feedback (i.e., customer report rates).
Over the course of 24 days, the percentage of users call the support
center on the y-axis gets as high as 11%. Despite that there are
some overestimations, the general trend of per-CO Q-score closely
follows that of user feedback with Pearson’s correlation coefﬁcient
R = 0.8797. Figure 11(b) shows per-CO Q-score and user feed-
back for COs with moderately high customer feedback. We again
see that the Q-score follows feedback whenever feedback increases
over 2%. Here, R = 0.7478 Figure 11(c) shows the same for a CO
with few customer calls. Because there are only a small increase
(2% of users calling) in the user feedback, Q-score remains at low
level of 0.17% on average with R = 0.5011. From the observa-
tions from three different COs with high, medium, and low level of
feedback, we conﬁrmed that Q-score, when aggregated across in-
dividuals within each CO, closely follows the trend of per-CO user
feedback. Since Q-score is conﬁrmed to have several hours of lead
time before users begin to report, we can leverage Q-score in di-
mensioning the workforce and prioritizing resources to areas with
more upcoming issues ahead of time.
6. RELATED WORK
In this section, we introduce related works on the two important
components of networked service assurance: quality of experience
assessment and network diagnosis.
205Customer Trouble Tickets
Q-score
 1
 0.8
 0.6
 0.4
 0.2
Customer Trouble Tickets
Q-score
 0.4
 0.35
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0.2
 0.18
 0.16
 0.14
 0.12
 0.1
 0.08
 0.06
 0.04
 0.02
Customer Trouble Tickets
Q-score
 0
 0
 5
 10
 15
Time (in days)
 20
 25
 0
 0
 5
 10
 15
Time (in days)
 20
 25
 0
 0
 5
 10
 15
Time (in days)
 20
 25
(a) CO with high customer report rates
(b) CO with moderate customer report rates
(c) CO with low customer report rates
Figure 11: Trend of customer trouble tickets and Q-score per CO.
6.1 Quality of Experience Assessment
Controlled Performance Assessment. A traditional approach in
the performance assessment of network devices is to use controlled
lab environments. The code analysis, protocol analysis, testbed
tests, and debugging done in such controlled environments serve
to localize faults to each individual component of the system. [5,6]
apply principles of automated software fault diagnosis and model-
based approaches. [21] outlines the methods of software and hard-
ware veriﬁcation using formal logic theorem provers. While the-
orem prover-based service assessment can be extensive and cor-
rect, the process of converting system operation to mathematical
logic and inventing theorems thereafter restrict its application to a
few specialized software and hardware systems such as that of the
CPU and its microcodes. For validating network protocols, there
have been several proposals [8,12] based on simulation and system
modeling using ﬁnite state machine, trafﬁc modeling, and queuing
theory.
Controlled testing has been extremely successful in detecting
and preventing critical software bugs and hardware failures. De-
spite their best efforts, however, they are simply unable to replicate
the immense scale and complexity of large operational systems and
networks. Thus, there is always the risk of issues creeping into
operational settings when they are missed in controlled environ-
ments. In this paper, we focus on a data-oriented mining approach
that analyzes the data collected from an operational network. We
believe a combination of data mining, lab reproduction, and soft-
ware/hardware analysis is required to correctly identify anomalous
service quality.
Video Quality Assessment. Subjective evaluation is the most reli-
able way of assessing the quality of an image or video, as humans
are the ﬁnal judges of the video quality in great part of the video
related applications. The mean opinion score (MOS) [1] is a sub-
jective quality measurement used in subjective tests which has been
regarded as the most reliable assessment for video. However, sub-
jective video assessment method is very inconvenient, expensive
and slow. Thus there is a ﬁeld of research dedicated to the de-
sign and development of objective quality metrics. Ongoing stud-
ies are both on standardizing the subjective measurement of video
quality [23] and on developing objective video quality metrics that
model and approximate the quality [3].
There are also video quality measurement studies in the context
of networked systems [4]. The work includes discussions on the
metrics of video quality measurable from various parts of a net-
[17] studies the viewers’ perception of video quality under
work.
packet loss-induced video impairments.
[27, 28] proposes a loss-
distortion model based PSNR metric applied to video quality mon-
itoring. Recently, ITU and other standardizing organizations began
to roll out video quality measures such as [22]. Besides the lack
of consensus in arriving at a single formula, video quality metrics
may not be readily usable in the context of network service quality
assessment as they require ﬁne-grained measurements of per ﬂow
network trafﬁc which current services dismiss due to the costs of
measurement and data collection. While our method uses the cus-
tomer trouble ticket as a proxy for user feedback, the concept of
our methodology is open to employing a variety of video quality
metrics as the measure of user experience.
6.2 Reactive Performance Diagnosis
Bayesian network and graph analysis are among the most widely
used techniques in the diagnosis of network performance issues and
troubleshooting [7, 10, 14, 16, 26, 29]. Kompella et al. [16] model
the fault diagnosis problem using a bipartite graph and uses risk
modeling to map high-level failure notiﬁcations into lower-layer
root causes. WISE [29] presents a what-if analysis tool to estimate
the effects of network conﬁguration changes on service response
times. Recent systems [15, 25] have used information available
to the OS to identify service quality issues using the dependency
structure between components.
[18–20] have shown the importance of focusing on recurring
and persistent events and enabling the detection and troubleshoot-
ing of network behavior modes that have been previously ﬂown
under the operations radar. NICE [20] focuses on detecting and
troubleshooting undesirable chronic network conditions using sta-
tistical correlations. Giza [18] applies multi-resolution techniques
to localize regions in IPTV network with signiﬁcant problems and
l1-norm minimization to discover causality between event-series.
Mercury [19] focuses on detecting the long-term, persistent im-
pact of network upgrades on key performance metrics via statis-
tical mining. A work on proactive prediction of service issues on
access network [13] focuses on capturing changes over long-term
(e.g., weeks and months) and conduct prediction. The main differ-
ence between the above methods and ours is in the proactiveness of
assessing service quality of experience (QoE). The reactive perfor-
mance diagnosis works mostly focus on network problems but not
on service quality of experience. We believe, Q-score is the ﬁrst
work in using the network performance indicators to proactively
construct the quality of experience scores for large services. By
capturing the quality of experience for users in a timely and scal-
able fashion, Q-score offers the operators with rapid notiﬁcation of
user-perceived issues and a lead time of several hours before cus-
tomer reports.
2067. CONCLUSION
In this paper, we develop Q-score, a novel framework for proac-
tive assessment of user perceived service quality in a large opera-
tional IPTV network. By associating coarse-grained network KPIs
with imperfect user feedback, Q-score generates a single score that
represents user-perceived quality of experience (QoE). Accuracy
analysis of Q-score reveals that it is able to predict 60% of service
problems reported by customers with only 0.1% of false positive
rate. Applying Q-score to various application scenarios, we have:
(i) identiﬁed a set of KPIs most relevant to user-perceived quality of
experience; (ii) quantiﬁed how early it can alert bad quality of ex-
perience; (iii) observed the possibility to pre-allocate the customer
care workforce to potentially affected service areas.
As an improvement of our work, we consider the following two
methods aimed at increasing the successful prediction rate. First,
to ﬁlter out more noise from user feedback, we plan to investigate
the trouble tickets that fell into false negatives. Collaborating with
video experts, we will conduct simulation based controlled test-bed
experiments in conjunction with our current operational data-driven
approach. Second, to make Q-score to be more resilient to incom-
pleteness of user feedback, we will further improve user group-
ing methods. In doing so, we plan on applying end-user clustering
techniques in relation to user-perceived QoE.
There are many other network services that are sensitive to ser-
vice quality that lack objective measures of user-perceived quality
of experience. Our future work includes applying the proactive ser-
vice quality assessment beyond the speciﬁc context of IPTV net-
works. For example, we plan to apply Q-score to VoIP and mobile
networks so that operation teams can predict call drops and voice
quality degradation without having to wait for customers to report
them.
8. REFERENCES
[1] Methods for subjective determination of transmission
quality. ITU-T Rec. P.800, 1998.
[2] One-way transmission time. ITU-T Rec. G.114, 2003.
[3] Objective perceptual multimedia video quality measurement
in the presence of a full reference. ITU-T Rec. J.247, 2008.
[4] Perceptual audiovisual quality measurement techniques for
multimedia services over digital cable television networks in
the presence of a reduced bandwidth reference. ITU-T Rec.
J.246, 2008.
[5] R. Abreu, A. G. 0002, P. Zoeteweij, and A. J. C. van
Gemund. Automatic software fault localization using generic
program invariants. In SAC, 2008.
[6] R. Abreu, P. Zoeteweij, and A. J. C. van Gemund.
Spectrum-based multiple fault localization. In ASE, 2009.
[7] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. Maltz,
and M. Zhang. Towards highly reliable enterprise network
services via inference of multi-level dependencies. In
Sigcomm, 2007.
[8] L. Breslau, D. Estrin, K. Fall, S. Floyd, J. Heidemann,
A. Helmy, P. Huang, S. McCanne, K. Varadhan, Y. Xu, and
H. Yu. Advances in network simulation. IEEE Computer,
33(5), 2000.
[9] S. Chatterjee and A. S. Hadi. Inﬂuential observations, high
leverage points, and outliers in linear regression. Statistical
Science, Vol. 1:379–416, 1986.
[10] I. Cohen, J. S. Chase, M. Goldszmidt, T. Kelly, and
J. Symons. Correlating instrumentation data to system states:
A building block for automated diagnosis and control. In
OSDI, 2004.
[11] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased
estimation for nonorthogonal problems. Technometrics, Vol.
12, No. 1:55–67, 1970.
[12] G. Holzmann. Design and Validation of Computer Protocols.
Prentice-Hall, 1991.
[13] Y. Jin, N. G. Dufﬁeld, A. Gerber, P. Haffner, S. Sen, and
Z.-L. Zhang. Nevermind, the problem is already ﬁxed:
proactively detecting and troubleshooting customer dsl
problems. In CoNEXT, page 7, 2010.
[14] S. Kandula, D. Katabi, and J.-P. Vasseur. Shrink: A tool for
failure diagnosis in IP networks. In MineNet, 2005.
[15] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal, J. Padhye,
and P. Bahl. Detailed diagnosis in enterprise networks. In
ACM SIGCOMM, 2009.
[16] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren.
IP fault localization via risk modeling. In NSDI, 2005.
[17] T.-L. Lin, S. Kanumuri, Y. Zhi, D. Poole, P. Cosman, and
A. R. Reibman. A versatile model for packet loss visibility
and its application in packet prioritization. IEEE
Transactions on Image Processing, to appear, 2010.
[18] A. Mahimkar, Z. Ge, A. Shaikh, J. Wang, J. Yates, Y. Zhang,
and Q. Zhao. Towards automated performance diagnosis in a
large IPTV network. In ACM SIGCOMM, 2009.
[19] A. Mahimkar, H. H. Song, Z. Ge, A. Shaikh, J. Wang,
J. Yates, Y. Zhang, and J. Emmons. Detecting the
performance impact of upgrades in large operational
networks. In ACM SIGCOMM, 2010.
[20] A. Mahimkar, J. Yates, Y. Zhang, A. Shaikh, J. Wang, Z. Ge,
and C. T. Ee. Troubleshooting chronic conditions in large IP
networks. In ACM CoNEXT, 2008.
[21] J. S. Moore and M. Kaufmann. Some key research problems
in automated theorem proving for hardware and software
veriﬁcation. In RACSAM, 2004.
[22] Perceptual evaluation of video quality. 2011.
http://www.pevq.org.
[23] M. Pinson and S. Wolf. Comparing subjective video quality
testing methodologies. In SPIE Video Communications and
Image Processing Conference, pages 8–11, 2003.
[24] T. Qiu, J. Feng, Z. Ge, J. Wang, J. Xu, and J. Yates. Listen to
me if you can: Tracking user experience of mobile network
on social media. In IMC, 2010.
[25] K. Shen, C. Stewart, C. Li, and X. Li. Reference-driven
performance anomaly identiﬁcation. In SIGMETRICS, 2009.
[26] M. Steinder and A. Sethi. Increasing robustness of fault
localization through analysis of lost, spurious, and positive
symptoms. In Infocom, 2002.
[27] S. Tao, J. Apostolopoulos, and R. Guérin. Real-time
monitoring of video quality in ip networks. In Proceedings of
the international workshop on Network and operating
systems support for digital audio and video, NOSSDAV ’05,
pages 129–134, New York, NY, USA, 2005. ACM.
[28] S. Tao, J. Apostolopoulos, and R. Guérin. Real-time
monitoring of video quality in ip networks. IEEE/ACM
Trans. Netw., 16:1052–1065, October 2008.
[29] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and
M. Ammar. Answering what-if deployment and
conﬁguration questions with WISE. In SIGCOMM, 2008.
207Summary Review Documentation for 
“Q-score: Proactive Service Quality Assessment in a 
Large IPTV System” 
Authors: H. Song, Z. Ge, A. Mahimkar, J. Wang, J. Yates, Y. Zhang, A. Basso, M. Chen 
Note:	The	authors	of	this	paper	opted	out	of	this	year’s	reviewing	experiment.	As	a	result,	the	final	version	of	this	paper	is	not	
accompanied	by	the	reviews	that	the	paper	received	during	the	reviewing	process.	 
208