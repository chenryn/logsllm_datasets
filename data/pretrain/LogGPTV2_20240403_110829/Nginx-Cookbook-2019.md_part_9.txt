request needs to originate from the 192.168.1.0/24 CIDR block or be
able to supply a username and password that can be found in the
conf/htpasswd file. The satisfy directive takes one of two options:
any or all.
Discussion
The satisfy directive is a great way to offer multiple ways to
authenticate to your web application. By specifying any to the sat
isfy directive, the user must meet one of the security challenges. By
specifying all to the satisfy directive, the user must meet all of the
security challenges. This directive can be used in conjunction with
the http_access_module detailed in Recipe 7.1, the
http_auth_basic_module detailed in Recipe 6.1, the
http_auth_request_module detailed in Recipe 6.2, and the
http_auth_jwt_module detailed in Recipe 6.3. Security is only truly
secure if it’s done in multiple layers. The satisfy directive will help
you achieve this for locations and servers that require deep security
rules.
7.13 Dynamic DDoS Mitigation
Problem
You need a dynamic Distributed Denial of Service (DDoS) mitiga‐
tion solution.
Solution
Use NGINX Plus to build a cluster-aware rate limit and automatic
blacklist:
84 | Chapter 7: Security Controls
limit_req_zone $remote_addr zone=per_ip:1M rate=100r/s sync;
# Cluster-aware rate limit
limit_req_status 429;
keyval_zone zone=sinbin:1M timeout=600 sync;
# Cluster-aware "sin bin" with
# 10-minute TTL
keyval $remote_addr $in_sinbin zone=sinbin;
# Populate $in_sinbin with
# matched client IP addresses
server {
listen 80;
location / {
if ($in_sinbin) {
set $limit_rate 50; # Restrict bandwidth of bad clients
}
limit_req zone=per_ip;
# Apply the rate limit here
error_page 429 = @send_to_sinbin;
# Excessive clients are moved to
# this location
proxy_pass http://my_backend;
}
location @send_to_sinbin {
rewrite ^ /api/3/http/keyvals/sinbin break;
# Set the URI of the
# "sin bin" key-val
proxy_method POST;
proxy_set_body '{"$remote_addr":"1"}';
proxy_pass http://127.0.0.1:80;
}
location /api/ {
api write=on;
# directives to control access to the API
}
}
Discussion
This solution uses a synchronized rate limit and a synchronized key-
value store to dynamically respond to DDoS attacks and mitigate
their effects. The sync parameter provided to the limit_req_zone
and keyval_zone directives synchronizes the shared memory zone
with other machines in the active-active NGINX Plus cluster. This
example identifies clients that send more than 100 requests per sec‐
7.13 Dynamic DDoS Mitigation | 85
ond, regardless of which NGINX Plus node receives the request.
When a client exceeds the rate limit, its IP address is added to a “sin
bin” key-value store by making a call to the NGINX Plus API. The
sin bin is synchronized across the cluster. Further requests from cli‐
ents in the sin bin are subject to a very low bandwidth limit, regard‐
less of which NGINX Plus node receives them. Limiting bandwidth
is preferable to rejecting requests outright because it does not clearly
signal to the client that DDoS mitigation is in effect. After 10
minutes the client is automatically removed from the sin bin.
86 | Chapter 7: Security Controls
CHAPTER 8
HTTP/2
8.0 Introduction
HTTP/2 is a major revision to the HTTP protocol. Much of the
work done in this version was focused on the transport layer, such as
enabling full request and response multiplexing over a single TCP
connection. Effiencies were gained through the use of compression
on HTTP header fields, and support for request prioritization was
added. Another large addition to the protocol was the ability for the
server to push messages to the client. This chapter details the basic
configuration for enabling HTTP/2 in NGINX as well as configur‐
ing gRPC and HTTP/2 server push support.
8.1 Basic Configuration
Problem
You want to take advantage of HTTP/2.
Solution
Turn on HTTP/2 on your NGINX server:
server {
listen 443 ssl http2 default_server;
ssl_certificate server.crt;
ssl_certificate_key server.key;
87
...
}
Discussion
To turn on HTTP/2, you simply need to add the http2 parameter to
the listen directive. The catch, however, is that although the proto‐
col does not require the connection to be wrapped in SSL/TLS, some
implementations of HTTP/2 clients support only HTTP/2 over an
encrypted connection. Another caveat is that the HTTP/2 specifica‐
tion listed a number of TLS 1.2 cipher suites as blacklisted and
therefore will fail the handshake. The ciphers NGINX uses by
default are not on the blacklist. To test that your setup is correct you
can install a plugin for Chrome and Firefox browsers that indicates
when a site is using HTTP/2, or on the command line with the
nghttp utility.
Also See
HTTP/2 RFC Blacklisted Ciphers
Chrome HTTP2 and SPDY Indicator Plugin
Firefox HTTP2 Indicator Add-on
8.2 gRPC
Problem
You need to terminate, inspect, route, or load balance gRPC method
calls.
Solution
Use NGINX to proxy gRPC connections.
server {
listen 80 http2;
location / {
grpc_pass grpc://backend.local:50051;
}
}
In this configuration NGINX is listening on port 80 for unencrypted
HTTP/2 traffic, and proxying that traffic to a machine named back
end.local on port 50051. The grpc_pass directive instructs
88 | Chapter 8: HTTP/2
NGINX to treat the commuication as a gRPC call. The grpc:// in
front of our backend server location is not neccessary; however, it
does directly indicate that the backend communication is not
encrypted.
To utilize TLS encryption between the client and NGINX, and ter‐
minate that encryption before passing the calls to the application
server, turn on SSL and HTTP/2, as you did in the first section:
server {
listen 443 ssl http2 default_server;
ssl_certificate server.crt;
ssl_certificate_key server.key;
location / {
grpc_pass grpc://backend.local:50051;
}
}
This configuration terminates TLS at NGINX and passes the gRPC
communication to the application over unencrypted HTTP/2.
To configure NGINX to encrypt the gRPC communication to the
application server, providing end-to-end encrypted traffic, simply
modify the grpc_pass directive to specify grpcs:// before the
server information (note the addition of the s denoting secure com‐
munication):
grpc_pass grpcs://backend.local:50051;
You also can use NGINX to route calls to different backend services
based on the gRPC URI, which includes the package, service, and
method. To do so, utilize the location directive.
location /mypackage.service1 {
grpc_pass grpc://backend.local:50051;
}
location /mypackage.service2 {
grpc_pass grpc://backend.local:50052;
}
location / {
root /usr/share/nginx/html;
index index.html index.htm;
}
This configuration example uses the location directive to route
incoming HTTP/2 traffic between two separate gRPC services, as
well as a location to serve static content. Method calls for the
mypackage.service1 service are directed to the backend.local
8.2 gRPC | 89
server on port 50051, and calls for mypackage.service2 are directed
to port 50052. The location / catches any other HTTP request and
serves static content. This demonstrates how NGINX is able to serve
gRPC and non-gRPC under the same HTTP/2 endpoint and route
accordingly.
Load balancing gRPC calls is also similar to non-gRPC HTTP traf‐
fic:
upstream grpcservers {
server backend1.local:50051;
server backend2.local:50051;
}
server {
listen 443 ssl http2 default_server;
ssl_certificate server.crt;
ssl_certificate_key server.key;
location / {
grpc_pass grpc://grpcservers;
}
}
The upstream block works the exact same way for gRPC as it does
for other HTTP traffic. The only difference is that the upstream is
referenced by grpc_pass.
Discussion
NGINX is able to receive, proxy, load balance, route, and terminate
encryption for gRPC calls. The gRPC module enables NGINX to set,
alter, or drop gRPC call headers, set timeouts for requests, and set
upstream SSL/TLS specifications. As gRPC communicates over the
HTTP/2 protocol, you can configure NGINX to accept gRPC and
non-gRPC web traffic on the same endpoint.
8.3 HTTP/2 Server Push
Problem
You need to preemptively push content to the client.
Solution
Use the HTTP/2 server push feature of NGINX.
90 | Chapter 8: HTTP/2
server {
listen 443 ssl http2 default_server;
ssl_certificate server.crt;
ssl_certificate_key server.key;
root /usr/share/nginx/html;
location = /demo.html {
http2_push /style.css;
http2_push /image1.jpg;
}
}
Discussion
To use HTTP/2 server push, your server must be configured for
HTTP/2, as is done in Recipe 7.1. From there, you can instruct
NGINX to push specific files preemptively with the http2_push
directive. This directive takes one parameter, the full URI path of the
file to push to the client.
NGINX can also automatically push resources to clients if proxied
applications include an HTTP response header named Link. This
header is able to instruct NGINX to preload the resources specified.
To enable this feature, add http2_push_preload on; to the NGINX
configuration.
8.3 HTTP/2 Server Push | 91
CHAPTER 9
Sophisticated Media Streaming
9.0 Introduction
This chapter covers streaming media with NGINX in MPEG-4 or
Flash Video formats. NGINX is widely used to distribute and stream
content to the masses. NGINX supports industry-standard formats
and streaming technologies, which will be covered in this chapter.
NGINX Plus enables the ability to fragment content on the fly with
the HTTP Live Stream module, as well as the ability to deliver
HTTP Dynamic Streaming of already fragmented media. NGINX
natively allows for bandwidth limits, and NGINX Plus’s advanced
features offers bitrate limiting, enabling your content to be delivered
in the most efficient manner while reserving the servers’ resources
to reach the most users.
9.1 Serving MP4 and FLV
Problem
You need to stream digital media, originating in MPEG-4 (MP4) or
Flash Video (FLV).
Solution
Designate an HTTP location block as .mp4 or .flv. NGINX will
stream the media using progressive downloads or HTTP pseudos‐
treaming and support seeking:
93
http {
server {
...
location /videos/ {
mp4;
}
location ~ \.flv$ {
flv;
}
}
}
The example location block tells NGINX that files in the videos
directory are in MP4 format type and can be streamed with progres‐
sive download support. The second location block instructs NGINX
that any files ending in .flv are in FLV format and can be streamed
with HTTP pseudostreaming support.
Discussion
Streaming video or audio files in NGINX is as simple as a single
directive. Progressive download enables the client to initiate play‐
back of the media before the file has finished downloading. NGINX
supports seeking to an undownloaded portion of the media in both
formats.
9.2 Streaming with HLS
Problem
You need to support HTTP Live Streaming (HLS) for H.264/AAC-
encoded content packaged in MP4 files.
Solution
Utilize NGINX Plus’s HLS module with real-time segmentation,
packetization, and multiplexing, with control over fragmentation
buffering and more, like forwarding HLS arguments:
location /hls/ {
hls; # Use the HLS handler to manage requests
# Serve content from the following location
alias /var/www/video;
94 | Chapter 9: Sophisticated Media Streaming
# HLS parameters
hls_fragment 4s;
hls_buffers 10 10m;
hls_mp4_buffer_size 1m;
hls_mp4_max_buffer_size 5m;
}
The location block demonstrated directs NGINX to stream HLS
media out of the /var/www/video directory, fragmenting the media
into four-second segments. The number of HLS buffers is set to 10
with a size of 10 megabytes. The initial MP4 buffer size is set to 1
MB with a maximum of 5 MB.
Discussion
The HLS module available in NGINX Plus provides the ability to
transmultiplex MP4 media files on the fly. There are many directives
that give you control over how your media is fragmented and buf‐
fered. The location block must be configured to serve the media as
an HLS stream with the HLS handler. The HLS fragmentation is set
in number of seconds, instructing NGINX to fragment the media by
time length. The amount of buffered data is set with the
hls_buffers directive specifying the number of buffers and the
size. The client is allowed to start playback of the media after a cer‐
tain amount of buffering has accrued specified by the
hls_mp4_buffer_size. However, a larger buffer may be necessary as
metadata about the video may exceed the initial buffer size. This
amount is capped by the hls_mp4_max_buffer_size. These buffer‐
ing variables allow NGINX to optimize the end-user experience;
choosing the right values for these directives requires knowing the
target audience and your media. For instance, if the bulk of your
media is large video files, and your target audience has high band‐
width, you may opt for a larger max buffer size and longer length
fragmentation. This will allow for the metadata about the content to
be downloaded initially without error and your users to receive
larger fragments.
9.2 Streaming with HLS | 95
9.3 Streaming with HDS
Problem
You need to support Adobe’s HTTP Dynamic Streaming (HDS) that
has already been fragmented and separated from the metadata.
Solution
Use NGINX Plus’s support for fragmented FLV files (F4F) module to
offer Adobe Adaptive Streaming to your users:
location /video/ {
alias /var/www/transformed_video;
f4f;
f4f_buffer_size 512k;
}
The example instructs NGINX Plus to serve previously fragmented
media from a location on disk to the client using the NGINX Plus
F4F module. The buffer size for the index file (.f4x) is set to 512 kilo‐
bytes.
Discussion
The NGINX Plus F4F module enables NGINX to serve previously
fragmented media to end users. The configuration of such is as sim‐
ple as using the f4f handler inside of an HTTP location block. The
f4f_buffer_size directive configures the buffer size for the index
file of this type of media.
9.4 Bandwidth Limits
Problem
You need to limit bandwidth to downstream media streaming cli‐
ents without impacting the viewing experience.
Solution
Utilize NGINX Plus’s bitrate-limiting support for MP4 media files:
96 | Chapter 9: Sophisticated Media Streaming
location /video/ {
mp4;
mp4_limit_rate_after 15s;
mp4_limit_rate 1.2;
}
This configuration allows the downstream client to download for 15
seconds before applying a bitrate limit. After 15 seconds, the client is
allowed to download media at a rate of 120% of the bitrate, which
enables the client to always download faster than they play.
Discussion
NGINX Plus’s bitrate limiting allows your streaming server to limit
bandwidth dynamically based on the media being served, allowing
clients to download just as much as they need to ensure a seamless
user experience. The MP4 handler described in Recipe 9.1 designa‐
tes this location block to stream MP4 media formats. The rate-
limiting directives, such as mp4_limit_rate_after, tell NGINX to
only rate-limit traffic after a specified amount of time, in seconds.
The other directive involved in MP4 rate limiting is
mp4_limit_rate, which specifies the bitrate at which clients are