title:Offloading distributed applications onto smartNICs using iPipe
author:Ming Liu and
Tianyi Cui and
Henry Schuh and
Arvind Krishnamurthy and
Simon Peter and
Karan Gupta
Offloading Distributed Applications onto SmartNICs using iPipe
Ming Liu
University of Washington
Arvind Krishnamurthy
University of Washington
Tianyi Cui
University of Washington
Simon Peter
The University of Texas at Austin
Henry Schuh
University of Washington
Karan Gupta
Nutanix
Abstract
Emerging Multicore SoC SmartNICs, enclosing rich computing re-
sources (e.g., a multicore processor, onboard DRAM, accelerators,
programmable DMA engines), hold the potential to offload generic
datacenter server tasks. However, it is unclear how to use a Smart-
NIC efficiently and maximize the offloading benefits, especially for
distributed applications. Towards this end, we characterize four
commodity SmartNICs and summarize the offloading performance
implications from four perspectives: traffic control, computing ca-
pability, onboard memory, and host communication.
Based on our characterization, we build iPipe, an actor-based
framework for offloading distributed applications onto SmartNICs.
At the core of iPipe is a hybrid scheduler, combining FCFS and DRR-
based processor sharing, which can tolerate tasks with variable
execution costs and maximize NIC compute utilization. Using iPipe,
we build a real-time data analytics engine, a distributed transaction
system, and a replicated key-value store, and evaluate them on com-
modity SmartNICs. Our evaluations show that when processing
10/25Gbps of application bandwidth, NIC-side offloading can save
up to 3.1/2.2 beefy Intel cores and lower application latencies by
23.0/28.0 µs.
CCS Concepts
• Networks → Programmable networks; In-network process-
ing; • Hardware → Networking hardware;
Keywords
SmartNIC, Distributed applications
ACM Reference Format:
Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishnamurthy, Simon Peter,
and Karan Gupta. 2019. Offloading Distributed Applications onto SmartNICs
using iPipe. In SIGCOMM ’19: 2019 Conference of the ACM Special Interest
Group on Data Communication, August 19–23, 2019, Beijing, China. ACM, New
York, NY, USA, 16 pages. https://doi.org/10.1145/3341302.3342079
1 Introduction
Multicore SoC (system-on-a-chip) SmartNICs have emerged in the
datacenter, aiming to mitigate the gap between increasing network
bandwidth and stagnating CPU computing power [13, 14, 19]. In the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or
a fee. Request permissions from permissions@acm.org.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5956-6/19/08...$15.00
https://doi.org/10.1145/3341302.3342079
318
last two years, major network hardware vendors have released dif-
ferent SmartNIC products, such as Mellanox’s BlueField [43], Broad-
com’s Stingray [7], Marvell (Cavium)’s LiquidIO [42], Huawei’s
IN5500 [24], and Netronome’s Agilio [47]. They not only target
acceleration of protocol processing (e.g., Open vSwitch [52], TCP
offloading, traffic monitoring, and firewall), but also bring a new
computing substrate into the data center to expand the server com-
puting capacity at a low cost: SmartNICs usually enclose computing
cores with simple microarchitectures that make them cost-effective.
Generally, these SmartNICs comprise a multicore, possibly wimpy,
processor (i.e., MIPS/ARM), onboard SRAM/DRAM, packet process-
ing and domain-specific accelerators, and programmable DMA en-
gines. The different components are connected by high-bandwidth
coherent memory buses or interconnects. Today, most of these
SmartNICs are equipped with one or two 10/25GbE ports, and
100/200GbE products are imminent. These computing resources
allow hosts to offload generic computations (including complex al-
gorithms and data structures) without sacrificing performance and
program generality. The question we ask in this paper is how to
use SmartNICs efficiently to maximize offloading benefits for
distributed applications?.
There have been some recent research efforts that offload net-
working functions onto FPGA-based SmartNICs (e.g., ClickNP [38],
AzureCloud [20]). They take a conventional domain-specific acceler-
ation approach that consolidates most application logic onto FPGA
programmable logic blocks. This approach is applicable to a specific
class of applications that exhibit sufficient parallelism, deterministic
program logic, and regular data structures that can be synthesized
efficiently on FPGAs. Our focus, on the other hand, is to target dis-
tributed applications with complex data structures and algorithms
that cannot be realized efficiently on FPGA-based SmartNICs.
Towards this end, we perform a detailed performance charac-
terization of four commodity SmartNICs (i.e., LiquidIOII CN2350,
LiquidIOII CN2360, BlueField 1M332A, and Stingray PS225). We
break down a SmartNIC into four architectural components – traffic
control, computing units, onboard memory, and host communica-
tion – and use microbenchmarks to characterize their performance.
The experiments identify the resource constraints that we have to
be cognizant of, illustrate the utility of hardware acceleration units,
and provide guidance on how to efficiently utilize the resources.
We design and implement the iPipe framework based on our char-
acterization study. iPipe introduces an actor programming model
for distributed application development. Each actor has its own
self-contained private state and communicates with other actors
via messages. Our framework provides a distributed memory object
abstraction and enables actor migration, responding to dynamic
workload changes and ensuring the delivery of line-rate traffic. A
central piece of iPipe is the actor scheduler that combines FCFS
SIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
Table 1 lists the HW/SW specifications of four commercial Mul-
ticore SoC SmartNICs evaluated in this paper. They represent
different design tradeoffs regarding performance, programmabil-
ity, and flexibility. The first two LiquidIOII SmartNICs enclose an
OCTEON [9] processor with a rich set of accelerators but run in the
context of a light-weight firmware. Programmers have to use native
hardware primitives to process raw packets, issue DMA commands,
and trigger accelerator computations. BlueField and Stingray cards
run a ARM Cortex-A72 [5] processor and host a full-fledged operat-
ing system. They offer a lower barrier for application development,
and one can use traditional Linux/DPDK/RDMA stacks to commu-
nicate with local and external endpoints. The BlueField card even
has NVDIMM support for fault-tolerant storage. Current Smart-
NICs typically have link speeds of 10/25 GbE, and 100/200 GbE
units are starting to appear. Generally, a SmartNIC is a bit more
expensive than a traditional dumb NIC. For example, a 10/25GbE
SmartNIC typically costs 100∼400$ more than a corresponding
standard NIC [62].
2 [44]).
Based on how SmartNIC cores interact with traffic, we further
categorize SmartNICs into two types: on-path and off-path Smart-
NICs. The cores for on-path SmartNICs (Figure 1-b) are on the
packet communication path and hold the capability to manipu-
late each incoming/outgoing packet. LiquidIOII CN2350/CN2360
are both on-path SmartNICs. Off-path SmartNICs (Figure 1-c), de-
liver traffic flows to host cores (bypassing NIC cores) based on
forwarding rules installed on a NIC switch. Mellanox BlueField and
Broadcom Stingray are examples of off-path SmartNICs. Both NIC
vendors are further improving the programmability of the NIC
switch (e.g., Broadcom TruFlow [8], Mellanox ASAP
For both types of SmartNICs, host processing is the same as that
with standard NICs. On the transmit path (where a host server sends
out traffic), the host processor first creates a DMA control command
(including the instruction header and packet buffer address) and
then writes it into a command ring. The NIC DMA engine then
fetches the command and data from host memory and writes into
the packet buffer (located in the NIC memory). NIC cores on on-path
SmartNICs pull in incoming packets (usually represented as work
items), perform some processing, and then deliver them to TX/RX
ports via the DMA engine. For off-path ones, packets are directly
forwarded to either NIC cores or TX/RX ports based on switching
rules. Receive processing (where a host server receives traffic from
the SmartNIC) is similar but performed in the reverse order.
2.2 Performance Characterization
We characterize four Multicore SoC SmartNICs (listed in Table 1)
from four perspectives: traffic control, computing units, onboard
memory, host communication.
2.2.1 Experiment setup. We use Supermicro 1U/2U boxes
as host servers for both the client and server and an Arista DCS-
7050S/Cavium XP70 ToR switch for 10/25GbE network. The client
is equipped with a dumb NIC (i.e., Intel XL710 for 10GbE and Intel
XXV710-DA2 for 25GbE). We insert the SmartNIC on one of the
PCIe 3.0 ×8 slots in the server. The server box has a 12-core E5-
2680 v3 Xeon CPU running at 2.5GHz with hyperthreading enabled,
64GB DDR3 DRAM, and 1TB Seagate HDD. When evaluating Blue-
Field and Stingray cards, we use a 2U Supermicro server with two
319
Figure 1: Architectural block diagram for a Multicore SoC SmartNIC
and packet processing for the two types of SmartNICs.
(first come first serve) and DRR (deficit round robin) based proces-
sor sharing, which tolerates tasks with variable execution costs and
maximizes a SmartNIC’s resource utilization. iPipe allows multiple
actors from different applications to coexist safely on the Smart-
NIC, protecting against actor state corruption and denial-of-service
attacks. Taken together, iPipe’s mechanisms enable dynamic and
workload-aware offloading of arbitrary application logic, in con-
trast to prior work that focused on static offloading of specialized
tasks (e.g., Floem [53] and ClickNP [38]).
We prototype iPipe and build three applications (i.e., a data an-
alytics engine, a transaction processing system, and a replicated
key-value store) using commodity 10GbE/25GbE SmartNICs. We
evaluate the system using an 8-node testbed and compare the per-
formance against DPDK-based implementations. Our experimental
results show that we can significantly reduce the host load for real-
world distributed applications; iPipe saves up to 3.1/2.2 beefy Intel
cores used to process 25/10Gbps of application bandwidth, along
with up to 23.0µs and 28.0µs savings in request processing latency.
2 Characterizing Multicore SoC SmartNICs
This section provides detailed performance characterizations of
Multicore SoC SmartNICs. We explore their computational capa-
bilities and summarize implications that guide the design of iPipe.
2.1 Multicore SoC SmartNICs
A Multicore SoC SmartNIC consists of four major parts (as shown
in Figure 1-a): (1) computing units, including a general-purpose
ARM/MIPS multicore processor, along with accelerators for packet
processing (e.g., deep packet inspection, packet buffer management)
and specialized functions (e.g., encryption/decryption, hashing, pat-
tern matching, compression); (2) onboard memory, enclosing fast
self-managed scratchpad and slower L2/DRAM; (3) traffic control
module that transfers packets between TX/RX ports and the packet
buffer, accompanied by an internal traffic manager or NIC switch
that delivers packets to NIC cores; (4) DMA engines for communi-
cating with the host.
Packet buﬀerL2/DRAMScratchpadPacket processingMulticore processorL1CacheNIC coreTraﬃc manager/NIC switchDomain speciﬁcTX/RX portsDMA engineDMA engineAcceleratorsOnboard memoryHost communicationTraﬃc controlComputing unitsMulti-queuePCIeNIC coresTX/RX portsHost coresTraﬃc managerTraﬃc(a). SmartNIC architecture(b). On-path SmartNICNIC switchTX/RX portsNIC coresHost coresTraﬃc(c). Oﬀ-path SmartNICOffloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
SmartNIC model
LiquidIOII CN2350 [42]
LiquidIOII CN2360 [42]
BlueField 1M332A [43]
Stingray PS225 [7]
Vendor
Marvell
Marvell
Mellanox
Broadcom
Processor
cnMIPS 12 core, 1.2GHz
cnMIPS 16 core, 1.5GHz
ARM A72 8 core, 0.8GHz
ARM A72 8 core, 3.0GHz
BW
2× 10GbE
2× 25GbE
2× 25GbE
2× 25GbE
L1
32KB
32KB
32KB
32KB
L2
4MB
4MB
1MB
16MB
DRAM
4GB
4GB
16GB
8GB
Deployed SW
Firmware
Firmware
Full OS
Full OS
Nstack
Raw packet
Raw packet
Linux/DPDK/RDMA
Linux/DPDK/RDMA
To/From host
Native DMA
Native DMA
RDMA
RDMA