U𝒉 :=
(cid:110)(U1, . . . ,U𝐵) : U1, . . . ,U𝐵 ⊆ [𝑛]
U𝑗 = [𝑛] and |U𝑗| = ℎ 𝑗 ,∀𝑗 ∈ [𝐵](cid:111).
where U𝑗’s are disjoint for all 𝑗 ∈ [𝐵]. Note also that |U𝒉| =(cid:0)𝑛
Note that for each (U1, . . . ,U𝐵) ∈ U𝒉, U𝑗 for 𝑗 = 1, . . . , 𝐵 denotes
the identities of the clients that map to the 𝑗’th element in [𝐵],
ℎ1!ℎ2!...ℎ𝐵!. It is easy to verify that for any 𝒉 ∈ A𝑛
𝐵, 𝐹(P)(𝒉) is
equal to
𝐵, define
(cid:1) =
𝐵
(16)
s.t.
𝑗=1
𝑛!
𝒉
∑︁
𝐵

(U1,...,U𝐵)∈U𝒉
𝑗=1
𝑖∈U𝑗
𝐹(P)(𝒉) =
𝑝𝑖 𝑗
(17)
Similarly, we can define 𝐹(P′), 𝐹(P−𝑖), 𝐹(P′−𝑖). Note that 𝐹(P)
and 𝐹(P′) are distributions over A𝑛
𝐵, whereas, 𝐹(P−𝑖) and 𝐹(P′−𝑖)
are distributions over A𝑛−1
𝐵 . It is easy to see that 𝐹(P) = M(D)
and 𝐹(P′) = M(D′). Similarly, 𝐹(P−𝑖) = M(D−𝑖) and 𝐹(P′−𝑖) =
M(D′−𝑖). Now we are ready to prove Theorem 3.6.
Since R is an 𝜖0-LDP mechanism, we have
𝑒−𝜖0 ≤ 𝑝𝑖 𝑗
𝑝′
𝑛 𝑗
≤ 𝑒𝜖0,
∀𝑗 ∈ [𝐵] , 𝑖 ∈ [𝑛].
As mentioned in Section 3.3.1, a crucial observation is that any
distribution 𝒑𝑖 can be written as the following mixture distribution:
(18)
𝑒𝜖0 . The distribution ˜𝒑𝑖 = [ ˜𝑝𝑖1, . . . , ˜𝑝𝑖𝐵] is given by ˜𝑝𝑖 𝑗 =
1
𝑗=1 ˜𝑝𝑖 𝑗 = 1.
, where it is easy to verify that ˜𝑝𝑖 𝑗 ≥ 0 and𝐵
where 𝑞 =
𝑝𝑖 𝑗−𝑞𝑝′
1−𝑞
𝑛 + (1 − 𝑞) ˜𝒑𝑖,
𝒑𝑖 = 𝑞𝒑′
𝑛 𝑗
This idea of writing the distribution of the output of an LDP mech-
anism as a mixture distribution is inspired from [7, 24]. However,
we create different mixtures and use them in a distinct way to re-
duce the Renyi divergence calculation to those distributions with a
certain neighborhood structure using Lemma 5.3.
𝑛 + (1 − 𝑞) ˜𝒑𝑖 is a mix-
ture distribution, we can write 𝐹(P) and 𝐹(P′) as certain convex
combinations. Before stating the result, we need some notation.
C, having 𝑛 distribu-
For any C ⊆ [𝑛 − 1], define two sets PC, P′
Now we show that since each 𝒑𝑖 = 𝑞𝒑′
tions each, as follows:
PC = { ˆ𝒑1, . . . , ˆ𝒑𝑛−1}{𝒑𝑛},
C = { ˆ𝒑1, . . . , ˆ𝒑𝑛−1}{𝒑′
(cid:40)
𝑛},
P′
where, for every 𝑖 ∈ [𝑛 − 1], ˆ𝒑𝑖 is defined as follows:
𝒑′
˜𝒑𝑖
if 𝑖 ∈ C,
if 𝑖 ∈ [𝑛 − 1] \ C.
𝑛
(21)
ˆ𝒑𝑖 =
Note that PC and P′
C differ only in one distribution, where PC
C contains 𝒑′
contains 𝒑𝑛 whereas P′
𝑛. In words, if clients map their
data points according to the distributions in either PC or P′
C for
any C ⊆ [𝑛 − 1], then for all clients 𝑖 ∈ C, the 𝑖’th client maps its
data point according to 𝒑′
𝑛 (which is the distribution of R on input
𝑑′
𝑛), and for all clients 𝑖 ∈ [𝑛 − 1] \ C, the 𝑖’th client maps its data
point according to ˜𝒑𝑖. The last client maps its data point according
to 𝒑𝑛 or 𝒑′
In the following lemma, we show that 𝐹(P) and 𝐹(P′) can be
written as convex combinations of {𝐹(PC) : C ⊆ [𝑛 − 1]} and
{𝐹(P′
C) : C ⊆ [𝑛 − 1]}, respectively, where for any C ⊆ [𝑛 − 1],
both 𝐹(PC) and 𝐹(P′
C) can be computed analogously as in (17).
Lemma 5.1 (Mixture Interpretation). 𝐹(P) and 𝐹(P′) can be
𝑛 depending on whether the set is PC or P′
C.
written as the following convex combinations:
(19)
(20)
∑︁
∑︁
C⊆[𝑛−1]
C⊆[𝑛−1]
𝐹(P) =
𝐹(P′) =
𝑞|C|(1 − 𝑞)𝑛−|C|−1𝐹(PC),
𝑞|C|(1 − 𝑞)𝑛−|C|−1𝐹(P′
C),
(22)
(23)
where PC, P′
C are defined in (19)-(21).
We prove Lemma 5.1 in Appendix B.1.
102103104105106Number of iterations T10−1100101102Approximate DP εε0=3.0,n=106,δ=10−8via RDP (1st upper bound)via RDP (lower bound)Clones[FMT20]+strong composition[KOV15]102103104105106Number of iterations T10−1100Approximate DP εε0=3.0,n=107,δ=10−8via RDP (1st upper bound)via RDP (lower bound)Clones[FMT20]+strong composition[KOV15]Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2330Now, using Lemma 5.1, in the following lemma we show that the
Rényi divergence between 𝐹(P) and 𝐹(P′) can be upper-bounded
by a convex combination of the Rényi divergence between 𝐹(PC)
and 𝐹(P′
C) for C ⊆ [𝑛 − 1].
Lemma 5.2 (Joint Convexity). For any 𝜆 > 1, the function
is jointly convex in (𝐹(P), 𝐹(P′)), i.e.,
E𝒉∼𝐹 (P′)
(cid:17)𝜆(cid:21)
(cid:20)(cid:16) 𝐹 (P)(𝒉)
(cid:34)(cid:18) 𝐹 (P) (𝒉)
𝐹 (P′)(𝒉)
(cid:19)𝜆(cid:35)
C
C
C⊆[𝑛−1]
E𝒉∼𝐹 (P′)
𝑛, . . . , 𝑑′
𝑛, . . . , 𝒑′
𝐹 (P′) (𝒉)
𝑞|C| (1 − 𝑞)𝑛−|C|−1 E
𝑛, 𝑑𝑛(cid:1) and D′(𝑛)
(24)
We prove Lemma 5.2 in Appendix B.2. For any C ⊆ [𝑛 − 1],
: 𝑖 ∈ [𝑛 − 1] \ C}. With this notation, note
{𝒑′
𝑛, . . . , 𝒑′
𝑛} is a pair of specific neighboring distributions,
each containing |C| + 1 distributions. In other words, if we de-
fine D(𝑛)
𝑛, . . . , 𝑑′
having (|C| + 1) data points, then the mechanisms M(D(𝑛)
and M(D′(𝑛)
𝐹(P′
(cid:17)
𝜆 .
≤ ∑︁
(cid:169)(cid:173)(cid:173)(cid:171) 𝐹 (PC) (𝒉)
(cid:170)(cid:174)(cid:174)(cid:172)
𝐹(cid:16)P′
(cid:17) (𝒉)
𝒉∼𝐹(cid:16)P′
let (cid:101)P[𝑛−1]\C = { ˜𝒑𝑖
𝑛}{𝒑𝑛} and P′
that PC \(cid:101)P[𝑛−1]\C = {𝒑′
C \(cid:101)P[𝑛−1]\C =
𝑛}{𝒑′
|C|+1 =(cid:0)𝑑′
|C|+1 =(cid:0)𝑑′
𝑛(cid:1), each
|C|+1) will have distributions 𝐹(PC \ (cid:101)P[𝑛−1]\C) and
C \(cid:101)P[𝑛−1]\C), respectively.
of distributions in(cid:101)P[𝑛−1]\C in the RHS of (24), we would be able
|C|+1, D(𝑛)
the distributions in(cid:101)P[𝑛−1]\C in the RHS (24).
(cid:20)(cid:16) 𝐹 (P)(𝒉)
(cid:17)𝜆(cid:21)
to bound the RHS of (24) using the RDP for the special neighboring
datasets in D|C|+1
same . This is precisely what we will do in the follow-
ing lemma and the subsequent corollary, where we will eliminate
The following lemma holds for arbitrary pairs (P, P′) of neigh-
boring distributions P = {𝒑1, . . . , 𝒑𝑛} and P′ = {𝒑1, . . . , 𝒑𝑛−1, 𝒑′
𝑛},
where we show that E𝒉∼𝐹 (P′)
does not decrease
when we eliminate a distribution 𝒑𝑖 (i.e., remove the data point
𝑑𝑖 from the datasets) for any 𝑖 ∈ [𝑛 − 1]. We need this general
statement as it will be required in the proof of Theorem 3.1 later.
same , if we remove the effect
|C|+1) ∈ D|C|+1
Now, since (D′(𝑛)
|C|+1)
𝐹 (P′)(𝒉)
𝑛, 𝑑′
Lemma 5.3 (Monotonicity). For any 𝑖 ∈ [𝑛 − 1], we have
(cid:169)(cid:173)(cid:173)(cid:171) 𝐹 (P−𝑖) (𝒉)
𝐹(cid:16)P′−𝑖
(cid:17) (𝒉)
𝜆 , (25)
(cid:170)(cid:174)(cid:174)(cid:172)
≤ E
𝒉∼𝐹(P′
−𝑖)
𝐹 (P′) (𝒉)
E𝒉∼𝐹 (P′)
where, for 𝑖 ∈ [𝑛 − 1], P−𝑖 = P \ {𝒑𝑖} and P′−𝑖 = P′ \ {𝒑𝑖}. Note
that in the left hand side (LHS) of (25), 𝐹(P), 𝐹(P′) are distributions
𝐵, whereas, in the RHS, 𝐹(P−𝑖), 𝐹(P′−𝑖) for any 𝑖 ∈ [𝑛 − 1]
over A𝑛
are distributions over A𝑛−1
𝐵 .
We prove Lemma 5.3 in Appendix B.3. Note that Lemma 5.3 is a
general statement that holds for arbitrary pairs (P, P′) of neigh-
boring distributions. For our purpose, we apply Lemma 5.3 with
(PC, P′
C) for any C ⊆ [𝑛 − 1] and then eliminate the distribu-
tions in(cid:101)P[𝑛−1]\C one by one. The result is stated in the following
corollary.
(cid:34)(cid:18) 𝐹 (P) (𝒉)
(cid:19)𝜆(cid:35)
(cid:0)𝑑′
Corollary 5.4. Consider any 𝑚 ∈ {0, 1, . . . , 𝑛 − 1}. Let D(𝑛)
𝑛, . . . , 𝑑′
(i.e., C ⊆ [𝑛 − 1] such that |C| = 𝑚), we have
𝑛, . . . , 𝑑′
𝑚+1 =
𝑚
𝑛(cid:1). Then, for any C ∈(cid:0)[𝑛−1]
(cid:1)
(cid:33)𝜆 .
(cid:32)M(D(𝑛)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
M(D′(𝑛)
(26)
E𝒉∼𝐹 (P′
C)
𝒉∼M(D′(𝑛)
𝑚+1)
We prove Corollary 5.4 in Appendix B.4. Substituting from (26)
𝐵, 𝐹(P)(𝒉) and 𝐹(P′)(𝒉)
into (24) and noting that for every 𝒉 ∈ A𝑛
are distributionally equal to M(D)(𝒉) and M(D′)(𝒉), respec-
tively, we get
𝐹(P′
C)(𝒉)
𝑛, 𝑑𝑛(cid:1) and D′(𝑛)
𝑚+1 =(cid:0)𝑑′
(cid:32) 𝐹(PC)(𝒉)
(cid:33)𝜆 ≤ E
(cid:19)𝜆(cid:35)
(cid:34)(cid:18) M (D) (𝒉)
∑︁
∑︁
(cid:18)𝑛 − 1
M (D′) (𝒉)
C∈([𝑛−1]
𝑚 )
C∈([𝑛−1]
𝑚 )
(cid:19)
𝑞𝑚 (1 − 𝑞)𝑛−𝑚−1 E
𝑞𝑚 (1 − 𝑞)𝑛−𝑚−1 E
E𝒉∼M(D′)
𝑚=0
(a)≤ 𝑛−1∑︁
(b)≤ 𝑛−1∑︁
𝑛−1∑︁
𝑚=0
C
C
𝒉∼M(D′(𝑛)
𝑚+1)