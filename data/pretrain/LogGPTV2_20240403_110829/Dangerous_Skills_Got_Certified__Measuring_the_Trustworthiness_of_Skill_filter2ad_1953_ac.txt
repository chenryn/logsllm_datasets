from end users. The results motivate us to further conduct a dy-
namic testing to empirically test the published skills and identify
problematic ones in the skills store.
Skill name
Guess me
ABCs
Whose Turn
Chompers
NORAD Tracks Santa
Science Kid Radio
Animal Sounds
ABC
Goodnight, Sleep Tight
Punish the kids!
Amazon Story time
Merry Christmas
Chompers
Trivial Pursuit
User review
"Collection of information"
"Just want your kids data"
"The initializing process required my family member
names"
"You are giving the company permission to use way
too much information about your kids."
"Intrusion at its best (asking for credit card
information)"
"There are more advertisementscommercials ....
"Asks for you to buy additional sounds"
"Creepy skill with inappropriate content for kids"
"Scared the kid"
"Rude to kids"
"Want your kid to hear a Boston Bombing story?"
"Played like a few seconds of Santa sounds
and the rest was lame advertisements"
"I had to explain what "sexual deviance" or some
similar term was to my daughter last night"
"My daughter got multiple questions about alcohol
and tv shows that are NOT kid appropriate"
Table 2: Selected critical reviews in the kids category.
5.1.2
Identifying existing risky skills through dynamic testing.
The user review analysis reveals potential issues of policy violation
in existing skills. We are curious about how existing skills conform
to the security policies in the skills store. We leveraged our security
expertise to assess whether a skill violates any policy by manually
testing each skill. Since dynamic testing of skills is time-consuming,
we examined 825 kids skills which either had a privacy policy or
had a negative review. We wanted to check if they were collecting
any personal identifiable information from end users which is not
allowed for a kids-directed skill. We did notice certain other policy
violations as well among these skills such as asking the user to
engage with content outside of Alexa and promotion of other skills.
We identified 52 problematic skills with policy violations in our
dynamic testing. Table 3 shows the list of these skills. In addition,
we found 51 broken skills (details in Table 6 of Appendix C). Our
Figure 5: (a) A certified kids skill collecting personal infor-
mation. (b) Data that the skill stored in DynamoDB database.
{my_lastname}, where “my_name” and “my_lastname” are slots to
capture a user’s input. This sample utterance clearly mentions that
we are collecting the full name of the user. While checking the
values stored in the Amazon DynamoDB database, we did see that
the vetting tool or the certification team, had inputted full names
(which are potentially fake names just for testing purposes) but
still certified the skill. Fig. 5 shows a certified kids skill collecting
user names. Note that the names shown in Fig. 5(b) are not from
the certification team, but values we inputted through the skill for
illustration purpose. The certification team could have easily de-
tected this and rejected the skill for collecting personal information
through a kids skill. Additionally, the skill had no developer privacy
policy and the Privacy & Compliance form filled by the developer
denied collecting personal information from users. For the ethical
consideration, we added a disclaimer for the skills before a policy
violation was spoken. We even added these disclaimers in the de-
scription of some skills but neither of them led to a rejection. No
plagiarism check was conducted on these skills either and we were
able to publish multiple copies of the same skill with no difference
between them.
5 CONSEQUENCES OF A LENIENT SKILL
CERTIFICATION
The results in Sec. 4 reveal that Alexa has not strictly enforced the
security policies in the certification process. As such, it provides
opportunities for malicious developers to trick the certification
process, and thus placing end users in a vulnerable position. The
lenient skill certification process will have serious consequences
for security throughout the Alexa platform. We next ask the ques-
tion, “whether there exist policy-violating (e.g., collecting personal
information from users) or problematic skills in Alexa’s skills store
because of the lenient skill certification”. However, it is non-trivial
to test all (more than 100,000) published skills to find policy viola-
tions, due to the lack of an automated testing tool. Therefore, we
focus on kids’ skills in Alexa’s skills store and conduct a small-scale
dynamic testing to identify policy-violating skills.
5.1 Empirical Study On Kids’ Skills
5.1.1 Understanding users’ common concerns. We focus on kids’
skills because Alexa specifies more stringent policies in the kids
category than the other categories. For example, the skills in the
kid’s category should never request any personally identifiable
information (e.g., full name, and address) even if a privacy policy
7
(a)(b)result shows that the lack of trustworthiness of skill certification
leads to a realistic threat to VA users, especially children. We also
noticed that most of the privacy policies listed in the skills store
were general privacy policies provided by developers and not specif-
ically written for the skill. The document does not provide a clear
understanding to the user about what the skill is collecting or stor-
ing. Even the skills published by Amazon link to the company’s
general privacy policy. In many cases, it is a very long document
and mostly unrelated to the skill.
that has been collected from their child with their consent and be
given the authority to remove it. Moreover, COPPA requires that
the contact information of the developers is provided to the parents.
The information collected by Amazon from the developer when
signing up for the developer account is not verified and can be eas-
ily faked. As demonstrated by our experiments, developers could
certify skills that collect personal information without satisfying or
honoring any of these requirements, and thus violating the COPPA
regulations.
Policy violation (# of
skills)
Possible collection
of personal data
from kids (21)
Skill
recommendations,
advertisements and
promotes end users
to engage with
content outside of
Alexa (23)
Offers compensation
for providing
reviews (1)
Misleading
description (7)
Skill names
Ninja School, Dragon Palm, Loud Bird, Go Bot, Great Christmas
Treasure, Wake Up Clock, Personalized bedtime stories, Who did it,
Interactive Bed Time Story, Can I Wake Up, A Short Bedtime Story,
Mommy-gram, Number Games, Ready Freddy, Short Bedtime Stories,
Silly Stories, Story Blanks, Story World, The Bedtime Game, The Name
Game (banana-fana), Who Said Meow?, Whose Turn, Clothes Forecast
Kid Chef, What’s my dessert, Random Cartoon Facts, 6 Swords Kids,
Akinator Safari, Unicorn Stories, Hansel and Gretel, Red Riding Hood,
Highlights Storybooks from Bamboo, Magic Maths, Bamboo Math,
Homework Heroes, 4th Grade math Skill game, Bedtime stories,
Relaxing Sounds: Baby Bedtime Lullaby, Sight Words Kindergarten,
The Night Before Christmas, What’s Next Daily Task Helper, Wizard
of Oz, Word Mess, Would You Rather Family
Kids Jokes, Knock Knocks & Riddles
Annoying Parrot, Awesome life facts, Kids Books of the Bible, Chore
list, Nursery Rhymes, Twinkle Twinkle Little Star, Chore chart,
Chinese Joke
Table 3: List of skills with policy violations under the kids
category in Alexa’s skills store.
5.2 Possible COPPA Violations
It is possible that the third-party skills in Amazon Alexa suffer
the risk of violating the Children’s Online Privacy Protection Act
(COPPA) rules [17], a federal legal framework to protect the online
privacy of children under the age of 13. COPPA rules require that
parents should be in control over what information is collected
from their younger children. In 2019, YouTube paid $170 million for
allegedly violating the COPPA rules, because of collecting personal
information (i.e., cookies) from viewers of child-directed channels,
without first notifying parents and getting their consent [10].
There have been complaints made against Amazon in this re-
gard by children’s data privacy advocates [8]. Amazon claims that
the kids skills available on the store do not collect any personal
information from the children without parents’ consent. COPPA
rules require the developer to provide a privacy policy with a list of
all operators collecting personal information, a description of the
personal information collected, how it’s used and a description of
parental rights. In addition, parents must be notified directly before
collecting personal information from their kids and a verified con-
sent should be obtained from them. Amazon asks for a consent the
very first time that a kids skill is enabled in the account and doesn’t
require one afterward for all the other kids skill enablements. This
is a vague consent that does not inform the parents about what
each skill is capable of collecting. This would have been admittable
given that the certification system is perfect and would not let
any third-party skill that violates the rules to be certified. But the
kids skills published on the store are capable of violating COPPA
rules. Skills that collect personal information and do not provide
a privacy policy can be easily developed and certified. According
to COPPA, parents must also be able to review the information
8
5.3 Post-Certification Vulnerability
The back-end code of a third-party skill runs on the developer’s
server and Alexa does not require a re-certification when a change
is made in the back-end. Due to this, even if policy requirements
were strictly enforced, users are vulnerable against content chang-
ing attacks after a skill has been certified. Malicious developers are
able to arbitrarily change the content of responses (e.g., making
users exposed to inappropriate content) or questions (e.g., asking
users’ personal information). This type of skill manipulation can
lead to inappropriate content being presented to unwitting recipi-
ents or sensitive information leakage. While earlier research has
mentioned about the content changing vulnerability [14, 45], craft-
ing a phishing skill where a malicious developer can successfully
store the collected sensitive information in the back-end is not that
straightforward.
For a skill to collect a particular type of data, it must have the
capability for data collection before the certification phase. Devel-
opers get hold of what a user has spoken (in text format) only if
it matches with a sample utterance that the developer has spec-
ified. All other responses that are not matched won’t be sent to
the skill’s back-end. For example, to collect users’ address informa-
tion, the developer has to add a sample utterance with a slot of type
AMAZON.PostalAddress to one of the pre-defined intents. This can-
not be added after certification as it will require a re-certification
since the interaction model has changed. The malicious developer
has to carefully model a custom slot with suitable training data in
order to launch phishing attacks, e.g., collecting passwords requires
the training data including all sorts of alphabets, numerals and
symbols combinations in order to accept user responses perfectly.
In our experiment, we created a skill for kids with a custom slot
that can accept multiple types of values (e.g., first/last names and
city/street names). On the submission, our skill only asked for the
first name, which is acceptable by Alexa’s privacy policies even if
the certification process were to properly enforce policy require-
ments. After the certification, we changed the question to ask for
several other types of personal information that could build a com-
plete profile of the user. We were able to request and receive the
full name of a user, and save the personal information to a database.
To ensure research ethics in this experiment, we quickly remove
all the data collected by the skills after testing.
Adversarial skill developers can exploit this vulnerability even
if the issues related to certification were fixed. This can also be
exploited by developers to pose as a normal authentic skill in the
store for some time to earn good reviews which will boost the skill
enablements (giving it a priority if users enable the skill by voice).
After this, the skill can be altered with malicious content to easily
reach a higher number of users. In addition, this vulnerability opens
new opportunities for malicious actors. Once an attacker is able to
access the back-end code of a benign developer, the attacker can
inject malicious code into the skill, with neither the developer nor
the VA platform provider being notified about the change.
6 DISCUSSION
6.1 Why Lenient Skill Certification in Alexa?
There are a number of potential reasons for the leniency in Ama-
zon’s certification process for Alexa skills. There are over 100,000
skills on its skills store, but closer inspection reveals that the vast
majority of these skills go unused. Being lenient with the certi-
fication process encourages developers to produce many skills,
prioritizing quantity over quality. Further evidence for this mo-
tivation can be drawn from a comparison to the Google Action
developer console. Google limits developers to a maximum of 12
projects on the Actions on Google console, unless the developer
explicitly requests an increase in limit. In contrast, there is no such
limit placed on Amazon Alexa Developer accounts. These compa-
nies also have programs in place to reward developers who develop
several skills, with rewards increasing as more skills are developed.
While both Amazon and Google likely do not have an ill intent
through such programs, the consequence of prioritizing the growth
of the respective skills store over the quality of its skills results in a
certification process that insufficiently checks the submitted skills
for violations.
6.2 Mitigation Suggestions
Based on our measurements and findings, we provide recommenda-
tions to help VA platform providers to enhance the trustworthiness
of VA platforms.
Enforcing skill behavior integrity throughout the skill life-
cycle. Our experiment shows that developers can arbitrarily change
a skill’s functionality after the certification, e.g., an adversary re-
places the benign content (which passes the security check on sub-
mission) with inappropriate content (e.g., advertising extremism) in
the post-certification phase. When a skill opts for an Alexa-hosted
back-end, the back-end code is blocked from editing while the skill
is under review. But it is unblocked after the skill is certified. To
prevent content changing attacks, a continuous certification/vetting
process is required. Whenever the developer makes a change to ei-
ther the front-end or back-end, a re-certification process should be
performed. This is a viable solution although it may increase the
publishing latency. We also came across a large number of broken
skills during dynamic testing. Skills should be periodically checked
and removed from the skills store if they are broken.
Automating skill testing. Based on the observations from our
234 skill submissions, we conclude that the certification is largely
done in a manual manner and through very limited voice response
based testing. To strictly enforce security policies in the certifica-
tion process, it is desirable to design a voice-based testing tool to
automate the testing of third-party skills. For example, VA platform
providers may apply deep learning techniques to train a user simu-
lation model [18, 20, 33] to interact with third-party skills during
the vetting. However, building a reliable and scalable voice-based
testing tool is non-trivial. To fundamentally address the problem,
9
VA platform providers may need to require skill developers to pro-
vide the permissions to view their back-end code. In this case, a
code analysis can be performed, which could greatly increase the
strength of the certification process.
6.3 Limitation
There are areas remaining where further research can help in rein-
forcing our findings. First, while we have taken significant efforts
to measure the trustworthiness of skill certification process, our
adversarial testing mainly focuses on content policy violations in
skills. We do not test advanced features of skills such as the interac-
tion with smart home IoT devices and skill connections. Second, we
cannot fully scale-up the experiments of dynamic testing to identify
existing problematic skills at the level of the skills store. Future
work is needed to design a voice-based testing tool to automate the
interaction with third-party skills. Nevertheless, we have collected
strong evidence in revealing the untrustworthiness of the Amazon
Alexa platform, and empirically characterize potential security risks
in that platform.
7 CONCLUSION
In this work, we conducted the first comprehensive measurement
on the trustworthiness of Amazon Alexa platform. We crafted 234
policy-violating skills that intentionally violate Alexa’s policy re-
quirements and all of them passed the certification. Our results
showed strong evidence that its skill certification process was im-
plemented in a disorganized manner. Through dynamic testing of
825 skills, we identified 52 problematic skills with policy violations
and 51 broken skills under the kids category.
REFERENCES
[1] 2016.
Toddler asks Amazon’s Alexa to play song but gets porn in-
stead. https://nypost.com/2016/12/30/toddler-asks-amazons-alexa-to-play-song-
but-gets-porn-instead/. (2016).
[2] 2018. Portland Family Says Their Amazon Alexa Recorded Private Conversa-
tions. https://www.wweek.com/news/2018/05/26/portland-family-says-their-
amazon-alexa-recorded-private-conversations-and-sent-them-to-a-random-
contact-in-seattle/. (2018).
[3] 2018. Smart Audio Report 2018. https://www.edisonresearch.com/the-smart-
audio-report-from-npr-and-edison-research-spring-2018/. (2018).