## PostgreSQL PGCon 2020 全视频   
### 作者  
digoal  
### 日期  
2021-07-02   
### 标签  
PostgreSQL , pgcon , 2020   
----  
## 背景  
https://av.tib.eu/media/52153    
https://av.tib.eu/series/1052/pgcon+2020    
#### Advanced Data Modeling Techniques in PostgreSQL    
 2:09:41  8  Travers, Christopher    
This tutorial focuses on how to use advanced features in PostgreSQL to simplify/improve data modeling. Functional indexes, table methods, and custom types are all covered, as is table inheritance as a data modeling (instead of data partitioning) tool.    
#### Asynchronous IO for PostgreSQL    
 47:38  5  Freund, Andres    
For many workloads PostgresSQL currently cannot take full advantage of modern storage systems, like e.g. good SSD. But even for plain old spinning disks, we can achieve higher throughput. One of the major reasons for that is that the majority of storage IO postgres performs is done synchronously (see e.g. slide 8fff in https://anarazel.de/talks/2019-10-16-pgconf-milan-io/io.pdf for an illustration as to why that is a problem). This talk will discuss the outcome of a prototype to add asynchronous IO support to PostgreSQL. The talk will discuss: * How would async IO support for PG look like architecturally? * Performance numbers * What sub-problems exist that can be integrated separately * Currently that prototype uses linux' new io uring asynchronous IO support * Which other OSs can be supported? Do we need to provide emulation of OS level async IO? Note that support for asynchronous IO is not directly the same as support for direct IO. This talk will mainly focus on asynchronicity, and direct IO only secondarily.    
#### Avoiding, Detecting, and Recovering From Data Corruption    
 54:18  8  Haas, Robert    
PostgreSQL databases can become corrupted for a variety of reasons, including hardware failure, software failure, and user error. In this talk, I’ll talk about some of my experiences with database corruption. In particular, I’ll mention some of the things which seem to be common causes of database corruption, such as procedural errors taking or restoring backups; some of the ways that database corruption most often manifests when it does occur, such as errors indicating inconsistencies between a table and its indexes or a table and its toast table; and a little bit about techniques that I have seen used to repair databases or recover from corruption, including some experiences with pg resetxlog. This talk will be based mostly on my experiences working with EnterpriseDB customers; I hope that it will be useful to hackers from the point of view of thinking about possible improvements to PostgreSQL, and to end users from the point of view of helping them avoid, diagnose, and cope with corruption.    
#### Building automatic adviser & performance tuning tools    
 27:49  4  Rouhaud, Julien et al.    
PostgreSQL is a mature and robust RDBMS since it has 30 years of history. Over the year, its query optimizer has been enhanced and usually produces good query plans. However, can it always come up with good query plans? The optimization process has to use some assumptions to produce plans fast enough. Some of those assumptions are relatively easy to check (e.g. statistics are up-to-date), some harder (e.g. correct indexes are created), and some nearly impossible (e.g. making sure that the statistic samples are representative enough even for skewed data repartition). For now, given those various caveats, DBA sometimes can't always realize easily that they miss a chance to get a meaningful performance improvement. To help DBA to get a truly good query plan, we'll present below some tools that can help to fix some of those problems by providing a missing index adviser, looking for extended statistics to create, and row estimation error correction information to get appropriate join orders with join methods automatically. - pg qualstats: provides a new index and extended statistics suggestions to gather many predicate statistics on the production workload. - pg plan advsr: provides alternative good query plans automatically to analyze iterative query executions information to fix estimation rows error. In this talk, we will explain how those tools work under the hood and see what can be done, how they can work together. Also, we will mention what other tools also exist for related problems. Therefore, it will be useful for DBA who are interested in improving query performance or want to check whether current settings of indexes and statistics are adequate.    
#### Building Better Benchmarks    
 29:56  4  Wong, Mark    
TPC, SSBM, YCSB... These are just some of the database benchmarks that are out in the wild. Yet there are many derivatives, implementations and variations of these and other benchmarks. The differences between them vary between ease of use, and effectiveness at testing at large scales. End-user licensing agreements and licensing of the software being used can be just as prohibitive as the test itself for being able to measure the performance of your software. "Better" is certainly subjective. From the point of view of open source software developers, we want something that: * can push the limits our of software and hardware * we are allowed to modify and change to better suit our needs * can be shared with colleagues and the world about the achievements made The current state of the art is surveyed for what is available to help us now and what hurdles remain in order for us to be able to do more. These hurdles only remain as opportunities for us to overcome with open source solutions.    
#### Choose Your Own Adventure Postgres 12 By Example    
 48:57  3  Treat, Robert    
You know Postgres 12 is available, but you just haven't had time to look into it. Sure you want to learn more, but no one wants to just sit through a bunch of slides. What you really want is to see the features in Postgres 12 so you can figure out what will help make your users (and you!) happier with as little work as possible. I’m Robert Treat, and I’ve put together a whirlwind tour of new features available in Postgres 12. I think I know what will help you get the most bang for the buck, but if I am wrong, we can chose the features that you want to see most. That's right, there are no slides in this presentation, not even an about-me slide; we're going straight into the database to help you learn what options will help you make the best case with management to get you upgraded.    
#### Community roadmap to sharding    
 38:26  2  Korotkov, Alexander    
Sharding is one of most wanted PostgreSQL features. Vertical scalability is limited by hardware. Replication doesn't provide scalability in read-write performance and database size. Only sharding promises this. However, such a brilliant feature is hard to implement. And here is the point where different community parties should work together. The PostgreSQL community has done a great work on foreign data wrappers and continues improving them. Postgres Pro has experience in distributed transactions, snapshots and query planning/execution. In this talk we will cover existing advances in sharding and present a roadmap of transforming them into a comprehensive sharding solution suitable for the main use cases.    
#### CREATE STATISTICS: What is it for    
 48:48  6  Vondra, Tomas    
One of the new features in PostgreSQL 10 is the ability to create multi-column statistics, helping the optimizer understand dependencies between columns. PostgreSQL 12 further improved the feature by supporting more complicated statistics types. This talk will explain a bunch of important questions - why we need this capability at all for some queries, how it works, which cases it can address currently, and which improvements are in the queue for PostgreSQL 13.    
#### Deep Thoughts: Betting on Security    
 1:02:58  1  Conway, Joe    
For all combinations of who/which/what, list who currently has which type of permissions for what objects in your most important database(s)? It shouldn’t be that hard to figure out, right? Would you be willing to bet your job that you have given a complete and correct answer, even if this were an open book test with access to the server and ample time? This talk seeks to dive deep into the weeds on the topic of how roles interact with Postgres default behaviors, role attributes, and object privileges, resulting in a particular discretionary access control (DAC) security posture.    
#### Distributed snapshots and global deadlock detection    
 42:59  1  Praveen, Asim Rama et al.    
In this talk we would like to share our experiences in implementing MVCC in an open source scale out environment based on PostgreSQL. Scale out environment is characterised by multiple PostgreSQL servers with one master PostgreSQL server designated as the entry point for clients. Transactions may update data residing on more than one PostgreSQL instance. Isolating two or more such transactions running concurrently is a major challenge in a scale out system. Different isolation levels have their own challenges, serialisable being the hardest to implement efficiently. Distributed snapshots enable individual PostgreSQL instances within a scale out system to determine status (in-progress, committed, aborted) of a transaction and to decide whether effects of a transaction are visible using a given snapshot. The talk will go over this distributed snapshot mechanism in detail including several corner cases that we found tricky to implement right. Note that each PostgreSQL instance in the scale out system continues to create local snapshots and local transactions. Distributed deadlock occurs when the wait cycle spans multiple PostgreSQL instances. To detect it, wait graphs from each PostgreSQL instance need to aggregated and cycle detecting be performed on the aggregated data. The talk will describe how we model vertices and edges in such a graph, the method used to aggregate this information, being mindful of performance of the distributed system.    
#### Find your slow queries, and fix them!    
 50:33  7  Frost, Stephen    
Where, oh where, is all that time going? What in the world is that database thing doing?! This talk will help you understand what's happening (and why) and how to analyze poor query performance. We'll also go over steps and strategies to take to improve them and get the performance (and scalability!) you need. It all starts with figuring out what queries are slow, we'll do that by going into the various PostgreSQL configuration options for logging queries and a few helpful modules for getting even more information about ongoing queries. Next we'll go over EXPLAIN and EXPLAIN ANALYZE output for select queries, what the EXPLAIN output means in terms of how the query is being executed. Lastly (this is the good part- you have to stay til the end to get it!) we'll go over ways to improve the queries, including index creation, rewriting the query to allow PG to use a different plan, and how to tune parameters for specific queries.    
#### Hacking the Query Planner, Again    
 33:05  3  Guo, Richard    
This talk will focus on how the planner works from a developer’s view and elaborate on the process of converting a query tree to a plan tree in details. We can divide the planning process into 4 phases: preprocessing, scan/join planning, post scan/join planning and postprocessing. In this talk, I will explain what stuff is performed in each phase and what is the motivation to perform that. Topics will include: transforming ANY/EXISTS SubLinks into joins, flattening sub-selects, preprocessing expressions, reducing outer joins to inner joins, distributing quals to rels, collecting join ordering restrictions, removing useless joins, join searching process, upper planner path-ification, cost estimation, etc. Tom Lane's 2011 talk "Hacking the Query Planner” talked about the overview of query planner. In this talk, I will cover the internals of query planner with more details and in a way more close to planner codes. I hope this will be helpful in understanding the internals of PostgreSQL's planner and in hacking the planner codes to improve it.    
#### Implementing System Versioned Temporal Table    
 22:00  6  Temesgen Mamo, Surafel    
SQL standard 2011 introduce a temporal table concept which describe a table that store data with respect to time instance. It has two part application time period table and system versioned temporal table. An application time period are for meeting the requirements of applications based on application specific time periods which is valid in the business world rather than on the basis of the time it was entered in the database and system versioned temporal table state about the retention of old data alone with current data automatically by database management system and ability of queering both current and history data. A table can also be both a system versioned table and an application time period table.    
#### Improve of Parallel Query when getting large scale records    
 32:02  3  Hirose, Shigeo    
IoT system has many of the edge computer and sensor devices. They are increasing to thousand and more, they have a lot of data. PostgreSQL provides a Parallel Query to collect such data efficiently. But data Gather node is a bottleneck because it cannot process in parallel. we tried to improve the speed of Gather node.I will explain some solutions such as Gather node parallelization and bulk copy records in Gather node.    
#### Index: Do It Yourself    
 36:56  3  Borodin, Andrey    
In Postgres, we already have the infrastructure for building index-as-extension, but there are not so many such extensions to date. But there are so many discussions of on how to make core indexes better. This is a talk about extracting index from core to extension and what can be done with usual indexes. Some of these optimizations are discussed in @hackers and can be expected in the core, others will never be more than extension. We will discuss ideas from academic researches and corresponding industrial response from developers, communities, and companies. There will be a short live-coding session on creating a DIY index in Postgres. I'll show how to extract access method from core to extension in 5 minutes and talk about ideas for enhancing indexes: learned indexes, removing opclasses in favour of specialized indexes, cache prefetches, advanced generalized search (GiST alternative) and some others.    
#### Introducing LSM-tree into PostgreSQL, making it as a data gobbler    
 34:37  3  Jin, Shichao    
Data storage engine is always the key to the performance of a database system. Traditionally, storage engine is implemented based on B+-tree or B-tree. Compared to traditional B+tree or B-tree, LSM-tree as another important data structure, recently attracts great attention from developers due to its wide usage in many open source projects including in web browsers and many data processing systems. There are some well-known incarnation of LSM-tree, such as LevelDB and RocksDB to make the index practical in the industry. Now, we bring it to the world of PostgreSQL. Users can utilize LSM-tree via Postgres‘ FDW through our implementation. So in the talk, we will cover these contents: 1. The motivation of introducing LSM-tree 2. How does LSM-tree work? 3. How does the specific implementation of LSM-tree work, such as LevelDB and RocksDB? What is their architecture? 4. How do we incorporate these existing LSM-tree implementations? 5. Some experiment results    
#### JSON[b] Roadmap    
 51:53  3  Bartunov, Oleg    
JSONB in PostgreSQL is one of the main attractive feature for modern application developers, no matter what some RDBMS purists are thinking. PG12 and upcoming PG13 complete the implementation of SQL/JSON standard, which is a great feature itself, but now it is time to think about improving the implementation in both directions - functionality and performance. For example, PostgreSQL recognized for its extensibility and it is natural to add support of user's data types and operators to JSONPATH, make use of JSONPATH to index json data, optimize access to specific key:value pairs for large json, and so on. Another important topic I want to discuss - is the single JSON data type. Historically, we have two JSON data types - textual and binary (better), the latter, called JSONB, is actually the most usable and popular data type. Now, looking forward to expected specification of JSON data type in future SQL standard, we have to decide how to make JSON data type to be generic and cover the old JSON, JSONB data types.    