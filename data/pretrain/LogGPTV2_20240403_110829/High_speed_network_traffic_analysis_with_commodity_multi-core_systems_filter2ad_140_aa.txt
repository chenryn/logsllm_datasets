title:High speed network traffic analysis with commodity multi-core systems
author:Francesco Fusco and
Luca Deri
High Speed Network Trafﬁc Analysis with Commodity
Multi-core Systems
Francesco Fusco
IBM Research - Zurich
ETH Zurich
PI:EMAIL
Deri Luca
ntop
PI:EMAIL
ABSTRACT
Multi-core systems are the current dominant trend in com-
puter processors. However, kernel network layers often do
not fully exploit multi-core architectures. This is due to
issues such as legacy code, resource competition of the RX-
queues in network interfaces, as well as unnecessary mem-
ory copies between the OS layers. The result is that packet
capture, the core operation in every network monitoring ap-
plication, may even experience performance penalties when
adapted to multi-core architectures. This work presents
common pitfalls of network monitoring applications when
used with multi-core systems, and presents solutions to these
issues. We describe the design and implementation of a novel
multi-core aware packet capture kernel module that enables
monitoring applications to scale with the number of cores.
We showcase that we can achieve high packet capture per-
formance on modern commodity hardware.
Categories and Subject Descriptors
D.4.4 [Operating Systems]: Communications Manage-
ment; C.2.3 [Network Operations]: Network monitoring
General Terms
Measurement, Performance
Keywords
Linux kernel, network packet capture, multi-core systems
1.
INTRODUCTION
The heterogeneity of Internet-based services and advances
in interconnection technologies raised the demand for ad-
vanced passive monitoring applications.
In particular an-
alyzing high-speed networks by means of software applica-
tions running on commodity oﬀ-the-shelf hardware presents
major performance challenges.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
Researchers have demonstrated that packet capture, the
cornerstone of the majority of passive monitoring applica-
tions, can be substantially improved by enhancing general
purpose operating systems for traﬃc analysis [11, 12, 26].
These results are encouraging because today’s commodity
hardware oﬀers features and performance that just a few
years ago were only provided by costly custom hardware de-
sign. Modern network interface cards oﬀer multiple TX/RX
queues and advanced hardware mechanisms able to balance
traﬃc across queues. Desktop-class machines are becoming
advanced multi-core and even multi-processor parallel archi-
tectures capable of executing multiple threads at the same
time.
Unfortunately, packet capture technologies do not prop-
erly exploit this increased parallelism and, as we show in our
experiments, packet capture performance may be reduced
when monitoring applications instantiate several packet cap-
ture threads or multi-queues adapters are used. This is due
to three major reasons: a) resource competition of threads
on the network interfaces RX queues, b) unnecessary packet
copies, and c) improper scheduling and interrupt balancing.
In this work, we mitigate the above issues by introducing
a novel packet capture technology designed for exploiting
the parallelism oﬀered by modern architectures and network
interface cards, and we evaluate its performance using hard-
ware traﬃc generators. The evaluation shows that thanks to
our technology a commodity server can process more than 4
Gbps per physical processor, which is more than four times
higher than what we can achieve on the same hardware with
previous generation packet capture technologies.
Our work makes several important contributions:
• We successfully exploit traﬃc balancing features of-
fered by modern network adapters and make each
RX queue visible to the monitoring applications by
means of virtual capture devices. To the best of our
knowledge, this work describes the ﬁrst packet capture
technology speciﬁcally tailored for modern multi-queue
adapters.
• We propose a solution that substantially simpliﬁes the
development of highly scalable multi-threaded traﬃc
analysis applications and we released it under an open-
source license. Since compatibility with the popular
libpcap [5] library is preserved, we believe that it can
smooth the transition toward eﬃcient parallel packet
processing.
• We minimize the memory bandwidth footprint by re-
ducing the per-packet cost to a single packet copy, and
218optimize the cache hierarchy utilization by combining
lock-less buﬀers together with optimal scheduling set-
tings.
2. MOTIVATION AND SCOPE OF WORK
Modern multi-core-aware network adapters are logically
partitioned into several RX/TX queues where packets are
ﬂow-balanced across queues using hardware-based facilities
such as RSS (Receive-side Scaling) part of IntelTMI/O Ac-
celeration Technology (I/O AT) [17, 18]. By splitting a sin-
gle RX queue into several smaller queues, the load, both in
terms of packets and interrupts, can be balanced across cores
to improve the overall performance. Modern interface cards
(NICs) support static or even dynamically conﬁgurable [13]
balancing policies. The number of available queues depends
on the NIC chipset, and it is limited by the number of avail-
able system cores.1
However, in most operating systems, packets are fetched
using packet polling [23, 25] techniques that have been de-
signed in the pre-multi-core age, when network adapters
were equipped with a single RX queue. From the operat-
ing system point of view, there is no diﬀerence between a
legacy 100 Mbit card and a modern 10 Gbit card as the
driver hides all card, media and network speed details. As
shown in Figure 1, device drivers must merge all queues into
one as it used to happen with legacy adapters featuring a
single queue. This design limitation is the cause of a major
performance bottleneck, because even if a user space appli-
cation spawns several threads to consume packets, they all
have to compete for receiving packets from the same socket.
Competition is costly as semaphores or similar techniques
have to be used in order to serialize this work instead of
carrying it out in parallel.
Even if multi-core architectures, such as the one depicted
in Figure 2, are equipped with cache levels dynamically
shared among diﬀerent cores within a CPU, integrated mem-
ory controllers and multi-channel memories, memory band-
width has been identiﬁed as a limiting factor for the scal-
ability of current and future multi-core processors [7, 24].
In fact, technology projections suggest that oﬀ-chip mem-
ory bandwidth is going to increase slowly compared to the
desired growth in the number of cores. The memory wall
problem represents a serious issue for memory intensive ap-
plications such as traﬃc analysis software tailored for high-
speed networks. Reducing the memory bandwidth by mini-
mizing the number of packet copies is a key requirement to
exploit parallel architectures.
To reduce the number of packet copies, most capture
packet technologies [11] use memory mapping based zero-
copy techniques (instead of standard system calls) to carry
packets from the kernel level to the user space. The packet
journey inside the kernel starts at the NIC driver layer,
where incoming packets are copied into a temporary memory
area, the socket buﬀer [8, 21], that holds the packet until it
gets processed by the networking stack. In network monitor-
ing, since packets are often received on dedicated adapters
not used for routing or management, socket buﬀers’ alloca-
tions and deallocations are unnecessary and zero-copy could
start directly at the driver layer and not just at the network-
ing layer.
1For example on a quad-core machine we can have up to
four queues per port.
Figure 1: Design limitation in Network Monitoring
Architectures.
Figure 2: Commodity parallel architecture.
Memory bandwidth can be wasted when cache hierarchies
Improperly balancing interrupt re-
are poorly exploited.
quests (IRQs) may lead to the excessive cache misses phe-
nomena usually referred to as cache-trashing.
In order to
avoid this problem, the interrupt request handler and the
capture thread that consumes such a packet must be exe-
cuted on the same processor (to share the L3 level cache)
or on the same core with Hyper-Threaded processors. Un-
fortunately, most operating systems uniformly balance in-
terrupt requests across cores and schedule threads without
considering architectural diﬀerences between cores. This is,
in practice, a common case of packet losses. Modern oper-
ating systems allow users to tune IRQ balancing strategy
and override the scheduling policy by means of CPU aﬃnity
manipulation [20]. Unfortunately, since current operating
systems do not deliver queue identiﬁers up to the user space,
applications do not have enough information to properly set
the CPU aﬃnity.
In summary, we identiﬁed two main issues that prevent
parallelism from being exploited:
• There is a single resource competition by multi-
threaded applications willing to concurrently consume
packets coming from the same socket. This prevents
multi-queue adapters being fully exploited.
• Unnecessary packet copies, improper scheduling and
interrupt balancing cause a sub-optimal memory band-
width utilization.
219The following section describes a packet capture architec-
ture that addresses the identiﬁed limitations.
3. TOWARDS MULTI-CORE MONITOR-
ING ARCHITECTURES
We designed a high performance packet capture technol-
ogy able to exploit multi-queue adapters and modern multi-
core processors. We achieve our high performance by intro-
ducing virtual capture devices, with multi-threaded polling
and zero-copy mechanisms. Linux is used as the target op-
erating system, as it represents the de-facto reference plat-
form for the evaluation of novel packet capture technologies.
However, the exploited concepts are general and can also be
adapted to other operating systems.
Our technology natively supports multi-queues and ex-
poses them to the users as virtual capture devices (see Fig-
ure 3). Virtual packet capture devices allow applications to
be easily split into several independent threads of execution,
each receiving and analyzing a portion of the traﬃc. In fact
monitoring applications can either bind to a physical device
(e.g., eth1) for receiving packets from all RX queues, or to a
virtual device (e.g., eth1@2) for consuming packets from a
speciﬁc queue only. The RSS hardware facility is responsi-
ble for balancing the traﬃc across RX queues, with no CPU
intervention.
The concept of virtual capture device has been imple-
mented in PF RING [11], a kernel level network layer de-
signed for improving Linux packet capture performance. It
also provides an extensible mechanism for analyzing packets
at the kernel-level. PF RING provides a zero-copy mech-
anism based on memory mapping to transfer packets from
the kernel space to the user space without using expensive
system calls (such as read()). However, since it sits on top
of the standard network interface card drivers, it is aﬀected
by the same problems identiﬁed in the previous section. In
particular, for each incoming packet a temporary memory
area, called socket buﬀer [8, 21], is allocated by the network
driver, and then copied to the PF RING ring buﬀer which
is memory mapped to user space.
TNAPI drivers: For avoiding the aforementioned issue,
PF RING features a zero-copy ring buﬀer for each RX queue
and it supports a new NIC driver model optimized for packet
capture applications called TNAPI (Threaded NAPI2).
TNAPI drivers, when used with PF RING completely
avoid socket buﬀers’ allocations.
In particular, packets
are copied directly from the RX queue to the associated
PF RING ring buﬀer for user space delivery. This process
does not require any memory allocation because both the
RX queue and the corresponding PF RING ring are allo-
cated statically. Moreover, since PF RING ring buﬀers are
memory-mapped to the user-space, moving packets from the