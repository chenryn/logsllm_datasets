Figure 2: Slowdown for native, hypervisor-based, and emu-
lated (Qemu) execution (native = baseline).
Interestingly, on the Qemu system emulator, the execution time is
not signiﬁcantly higher. This is because of the way in which the
GetTickCount function is implemented on Windows. Instead
of invoking an expensive system call to retrieve the number of mil-
liseconds elapsed since system start-up, this value can be read di-
rectly from a kernel memory page that is mapped (read-only) into
the address space of all user programs (the counter is updated peri-
odically by the operating system).
Analyzing the stalling code inside ANUBIS, our Qemu-based dy-
namic analysis environment, yields a completely different result.
In ANUBIS, GetTickCount is monitored and, thus, the system
invokes a pair of log functions for each call to GetTickCount
(one before the call, and one after returning). These log functions
extract and write detailed information about the monitored function
call. For the stalling code in Figure 1, this means that the logging
functions of our analysis system are called 60 million times. Our
estimates show that this would take about ten hours, which means
that the attacker can successfully evade the analysis.
Note that excluding GetTickCount from the list of moni-
tored functions would provide a solution in this speciﬁc example.
However, the general problem that we address in this paper would
still remain. We discovered various other samples in the wild that
invoke different functions that are frequently monitored (such as
WriteFile), and we cannot generally ignore them all. Further-
more, attackers can implement stalling code without targeting the
overhead introduced by the monitoring system and instead make
use of instructions that are particularly slow to emulate. Figure 2
shows four examples of programs that make heavy use of FPU,
MMX, IO, and memory operations, each experiencing a signiﬁcant
slowdown (up to a factor of 45). In comparison, for benign appli-
cations average overheads stay below a factor of 10.
3. SYSTEM OVERVIEW
In this section, we present a high-level overview of HASTEN.
The goal of our system is to detect stalling loops and to mitigate
their effect on the analysis results produced by a sandbox (such as
ANUBIS). In particular, our system must ensure that the analysis
makes sufﬁcient progress during the allocated analysis timeframe
so that we can expose as much malicious activity as possible. To
this end, HASTEN can operate in three modes: monitoring, passive,
and active mode.
Monitoring mode. When the analysis of a malware sample is
launched, HASTEN operates in monitoring mode.
In this mode,
the system performs lightweight observation of all threads of the
process under analysis. The goal is to measure the progress of each
thread, and to identify instances in which the execution might have
entered a stalling region.
Passive mode. When the monitoring mode detects insufﬁcient
progress, this is typically due to slow operations that are executed
287would succeed, and the malware would terminate without revealing
any additional malicious behavior.
To overcome this problem, our approach is as follows: Before
we exit a whitelisted code region, we ﬁrst analyze this region for
all variables (memory locations) that the code writes as part of a
computation (logic and arithmetic instructions). These memory lo-
cations are then marked with a special label (tainted), indicating
that their true value is unknown. Whenever a machine instruction
uses a tainted value as source operand, the destination is tainted
as well. For this, we leverage the fact that we have implemented
our prototype solution on an emulator-based analysis platform that
already supports data ﬂow (taint) tracking.
Whenever a tainted variable is used in a comparison operation
or in an indirect memory access (using this variable as part of the
address computation), HASTEN temporarily halts the execution of
the malware process. It then extracts a backward slice that ends at
the comparison instruction and that, when executed, will compute
the correct value for this variable. To obtain this slice, we leverage a
tool that we previously developed [18]. Once the slice is extracted,
it is executed on a native machine. As a result, this computation
does not incur any overhead compared to the execution on a real
victim host.
4. MONITORING MODE
The goal of the monitoring mode is to determine whether a mal-
ware process has entered a stalling code region. To this end, HAS-
TEN monitors the progress of a program under analysis. More pre-
cisely, the system monitors, for each thread, the frequencies, types,
and return codes of all (native) system calls that this thread invokes.
System calls are the mechanism through which a process inter-
acts with its environment and the operating system. Also, to per-
form security-critical tasks (such as spawning new processes, mod-
ifying ﬁles, or contacting network services), a program must invoke
at least one system call. Thus, we believe that it is reasonable to
monitor the progress of a running program by inspecting the sys-
tem calls that it invokes. Moreover, all system calls use the same
convention to report errors. That is, there are well-speciﬁed values
that indicate a successful, or erroneous, invocation. This makes it
easy to determine whether a system call has been successful or not
(which is useful knowledge when reasoning about the progress of
a process). Finally, almost all analysis systems already support the
monitoring of system calls, and typically, this data can be collected
efﬁciently.
Detecting progress. To measure the progress of a process, HAS-
TEN periodically evaluates the system calls that each of its threads
invokes. More precisely, after a thread has been scheduled for a
time interval of length t, the system employs ﬁve simple detectors
to evaluate the system calls that have been observed during that last
period. Whenever one or more of these detectors reports insufﬁ-
cient progress, the system is switched into passive mode (explained
in more detail in the following Section 5).
When choosing a concrete value for the time interval t, we need
to balance two ends: Large values are more resistant against short
sequences of unusual, repeated behavior; small values allow faster
detection of abnormal activity. For our experiments, we have cho-
sen a value of t = 5 seconds. This allows us to evaluate the
progress multiple times during an analysis run (which is typically a
few minutes) and yielded good results in our experiments. Our ﬁve
detectors are discussed in the paragraphs below:
Detector for too few successful system calls: We expect to see a
reasonably large number of successful system calls that reﬂect the
normal interactions of a process with its environment and the OS.
Thus, when our system does not observe any system calls (or too
few of them), this is an indication of insufﬁcient progress. This
detector simply computes the total number of successful system
calls Ss within the time interval t. An alarm is raised when this
number is lower than a threshold Ss  Smax,s.
Detector for too many failed system calls: Of course, it is also pos-
sible to overload a sandbox with many invalid system calls (i.e.,
system calls with incorrect parameters). Invalid calls are quickly
caught by the kernel, which checks the validity of the parameters
(e.g., pointers should not be NULL) early on. Hence, they exe-
cute very fast on a victim host. Unfortunately, they still incur the
full logging overhead on the analysis platform. Thus, this detector
raises an alarm when the number of failed system calls exceeds a
certain threshold Sf > Smax,f .
Detector for too many identical system calls: The previous two de-
tectors identify cases in which stalling code attempts to overload
the sandbox with excessive numbers of system calls. A more cau-
tious attacker could attempt to hide stalling code by issuing fewer
system calls (that stay under our thresholds) and inject additional
computations between the call sites. However, even in this case,
it is likely that we will observe the same (small) set of calls re-
peated many times (executed in a loop). This is different from nor-
mal program execution, where a program invokes more diverse sys-
tem calls. The detector for identical system calls recognizes cases
where most system calls seen within interval t belong to a small set.
To this end, we compute the entropy Se of the set of system calls
during t. An alarm is raised when the entropy is below a threshold
Se  Smax,e.
In our experiments, we found that the previously-described detec-
tors are successful in identifying stalling code in real-world mal-
ware. However, if necessary, it is easy to develop additional heuris-
tics and integrate them into our system.
Parameter computation. The choice of concrete parameter val-
ues used by the ﬁve detectors (Smin,s, Smax,s, Smax,f , Smin,e,
and Smax,e) determines how often the system is switched into pas-
sive mode. When the thresholds are set too low, HASTEN might
enter passive mode more often than necessary. When the thresh-
olds are too high, the system might mistake stalling code for sufﬁ-
cient progress and, as a result, miss interesting activity. Because the
cost of unnecessarily switching into passive mode is only a small
performance hit, we prefer low thresholds that result in sensitive
detectors.
288To compute the necessary thresholds, we compiled a training set
of 5,000 malware samples submitted to ANUBIS. We only selected
samples that performed obvious malware activity (they produced
network trafﬁc, spawned additional processes, or opened a UI win-
dow). We assume that these samples do not contain stalling code
because the analysis successfully exposed malicious behaviors. All
samples from the training set were executed in HASTEN, counting
the numbers of successful and failed system calls during each time
period t, and computing their entropy. Then, the thresholds were
set so that the numbers derived for the training samples do not trig-
ger any detector. For more details about the parameter computation,
the reader is referred to Appendix A.
5. PASSIVE MODE
When the previous monitoring phase detects that a thread makes
insufﬁcient progress, the system switches into passive mode (for
this thread). When in passive mode, the system performs two steps.
In the ﬁrst step, the system attempts to ﬁnd code blocks that are
repeatedly executed (as part of loops or through recursive function
calls). To this end, the system starts to build a dynamic control
ﬂow graph (CFG) that covers the code that the thread is executing.
This CFG is then searched for cycles, and all blocks that are part
of a cycle are marked as (potential) stalling code. In the second
step, HASTEN reduces (or disables) logging for all blocks that the
previous step has marked as stalling code.
5.1 Identifying Stalling Code
Constructing the dynamic control ﬂow graph. To build a dy-
namic, inter-procedural control ﬂow graph, we need to keep track
of which instructions the program executes. For this, we lever-
age the execution infrastructure of ANUBIS, which is based upon
the system emulator Qemu. More precisely, whenever HASTEN is
switched into passive mode, the system starts to instrument the ﬁrst
instruction of each translation block (which is roughly the Qemu-
equivalent to a basic block) as well as all call and ret instruc-
tions. The ﬁrst instruction of a basic block simply appends the
value of the current instruction pointer %eip to a queue. This al-
lows us to efﬁciently determine the sequence in which a thread has
executed its basic blocks. The call and ret instructions addi-
tionally store the current value of the stack pointer. This makes it
easier to handle non-standard function calls (when the callee does
not return to the calling function).
The queue holding %eip information is processed periodically
(during scheduling or when switching to kernel code). At this point,
the sequence of values (eip1, eip2, ..., eipn) in the queue is used to
construct the CFG: Each distinct eipi value represents a different
basic block, and hence, a different node in the control ﬂow graph.
Thus, for each eipi ∀i : 1 ≤ i ≤ n, we check whether there exists
already a corresponding node in the CFG. If not, it is added. Then,
for each pair (eipi, eipi+1) ∀i : 1 ≤ i < n, we insert an edge from
the node associated with eipi to the node associated with eipi+1 (if
this edge does not exist). Finally, the queue is ﬂushed, and the last
element eipn (which has not been processed so far) is inserted as
the next head of the queue.
Every time the system processes the queue with the %eip val-
ues, there is potentially more information available that can be used
to obtain a more complete picture of the program’s CFG. However,
we expect that there are diminishing returns. That is, after some
time, all code blocks that are part of the stalling code region have
been seen, and no new nodes are added to the CFG when the queue
is processed. At this point, the CFG can be analyzed for stalling
code regions. Note, we also trigger this analysis after a timeout Tr
(currently set to 8 seconds). This handles cases where the stalling
Figure 4: Example CFG for functions m, f, and g.
code contains a loop with many different paths so that a few more
nodes are discovered and added every time.
Finding live loops. To ﬁnd stalling code, we analyze the CFG for
loops. To this end, we use the algorithm proposed by Sreedhar et
al. [26]. At this point, we have identiﬁed a number of loops in
the dynamic control ﬂow graph of the malware program. Some of
these loops might be nested, but in most cases, we will ﬁnd non-
overlapping loops that belong to different functions. The question
now is: Which of these loops (and which code blocks) should we
consider to be part of the execution stalling region?
To answer this question, we ﬁrst identify all code blocks that are
live at the current point during the execution. Intuitively, live code
blocks represent the code of all functions the program has started to
execute, but have not returned yet. More precisely, we deﬁne live
code blocks as follows:
Deﬁnition: A code block b is live when there is a path
in the inter-procedural CFG from the node that corre-
sponds to block b to the node c that corresponds to the
current code block. Moreover, this path can traverse
only intra-procedural edges and live call edges.
The current code block is the code block that holds the currently-
executing instruction (i.e., the code block that the instruction pointer
%eip currently points to). A live call edge is an edge that has been
introduced into the graph by a function call that has not returned
yet. Consider a function f that invokes function g. This introduces
an edge into the CFG from a node in f to a node in g. Initially,
this edge is live, because the program executes instructions in g.
When g returns, g is no longer live and its frame is removed from
the stack. As a result, the call edge between f and g is no longer
live. Of course, the edge in the CFG still remains.
Consider an example where a main function m ﬁrst invokes f
and then, after f has returned, a second function g. Figure 4 shows
a CFG for this example. Let us further assume that the current code
block is in function g (node in black). It can be seen that only the
call edge between m and g is live. The live blocks for this program
are represented as gray nodes. Note that the live blocks cover the
loops in m and the ﬁrst nodes in g.
To determine live call edges, we use a stack to track call and re-
turn instructions that the malware program executes. As mentioned
previously, the stack pointer information associated with call and
return instructions helps us to handle cases in which functions
perform non-standard returns.
Finding stalling loops. Once we have determined all live code
blocks, the next step is to identify active loops. Intuitively, an active
289loop is a loop that is still in progress; that is, it has not ﬁnished yet.
We are only interested in active loops as potential stalling loops,
since we assume that the stalling code is still running when we
perform our analysis.
A necessary precondition for an active loop is that it contains
code blocks that are live. However, this is not sufﬁcient: Consider,
again, the example in Figure 4. One can see that function m con-
tains two sequential (non-nested) loops; l1 followed by l2. The
current code block is in g, which is called from a node in l2. This
indicates that l1 has ﬁnished. Thus, even though l1 is live, it is not
active. In our example, only l2 is active.
To determine active loops, we ﬁrst identify all live loops. This
is straightforward: A live loop is a loop in the CFG with at least
one node that corresponds to a live block (actually, because there is
a path from all nodes inside a loop to all other nodes in that loop,
either all nodes in a loop will be live, or none). Then, we mark as
active all live loops where either (i) the current code block is part
of the loop, or (ii), a node with an outgoing, live call edge is part of
the loop.
Note that all active loops within the same function (instance) are
nested. To see this, consider all active loops within one function.
They must contain either the node associated with the current code
block or the node associated with the live call edge. In the case of
recursive function calls, we need to distinguish between different
instances of the same function, but for each instance, there can be
at most one live call node.
Any active loop (or a combination thereof) is potentially respon-
sible for stalling the execution. Thus, we can use different heuris-
tics to pick the one(s) that is (are) most likely stalling. For example,