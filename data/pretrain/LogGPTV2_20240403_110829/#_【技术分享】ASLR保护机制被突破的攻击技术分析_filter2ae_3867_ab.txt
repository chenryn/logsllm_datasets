MB的虚拟内存，当我们访问连续的64
MB虚拟缓冲区（以2MB为边界）的时候，我们将刷新该页表缓存。因此，我们可以轻松地通过蛮力方式穷举每个级别的潜在页表缓存的大小。例如，如果在x86
64架构的Intel处理器上我们无法通过AnC观察到上面三级页表的信号，那是因为该页面（转译）缓存位于2级页表中。然后，我们可以蛮力破解该缓存的大小，然后移动到上一级。代码清单1为我们展示了具体的实现过程。注意，与AnC不同，我们采用了一个已知的虚拟地址，所以我们可以准确知道MMU信号应该出现在缓存中的什么地方。当然，为了提高清单1中代码的鲁棒性，使其适用于多种处理器架构，我们还需要解决许多问题，具体将在后文中详细展开。
**确保存取顺序**
许多现代CPU架构都实现了乱序执行技术，其中指令的执行顺序取决于输入数据的可用性，而不是它们在原始程序中的顺序。在应用乱序执行技术之后，指令在解码之后被插入等待队列中，直到它们的输入操作数可用为止。一旦输入操作数可用，该指令就会被发送到相应的执行单元，这样的话，这条指令就会先于前面的指令由该单元执行了。此外，这种CPU架构通常都是超标量的，因为它们具有多个执行单元，并且允许将多条指令调度到这些不同的执行单元中并行执行。在指令执行完成之后，它们的结果将被写入另一个现已“退休的”队列中，该队列以原始程序的顺序进行排序，以保证正确的逻辑顺序。此外，有些现代CPU架构不仅具有针对指令的乱序执行的能力，而且它们还具有对内存操作进行重新排序的能力。为了测量这种CPU体系结构上单个指令的执行时间，我们必须在定时指令之前和之后注入内存屏障，并目标代码之前和之后插入代码屏障，以清除正在运行的指令和内存操作。为了串行化内存存取顺序，我们可以在ARMv7-A和ARMv8-A上面使用dsb指令，而在x86_64上，可以通过rdtscp和mfence指令保证串行化的内存存取顺序。为了串行化指令顺序，我们可以在x86_64上使用cpuid指令，在ARMv7-A和ARMv8-A上使用isb
sy指令。
**定时**
在缓存命中和缓存未命中的情况下，存在从几百纳秒或甚至几十纳秒的性能差异，因此我们需要高精度的定时源才能能够区分缓存是否命中。虽然在兼容POSIX的操作系统上可以通过clock_gettime（）来获取定时信息，但是在各种ARMv7-A和ARMv8-A平台上，它们提高的定时信息却不够精确。
许多现代处理器架构都提供了专用寄存器来计数处理器的周期数，从而提供高精度的定时源。虽然这些寄存器可通过各种rdtscp指令中的非特权rdtsc进行访问，但默认情况下，ARMv7-A和ARMv8-A上的性能监视单元（PMU）提供的PMCCNTR寄存器是无法访问的。此外，当最初引入这些寄存器时，没有确保它们在内核之间是同步的，并且直接利用处理器时钟使其计数进行递增。在这些情况下，进程迁移和动态频率调整会对定时造成一定程度的影响，甚至让它变得不再可靠。
考虑到当今大多数处理器都具有多个内核，在循环中简单递增全局变量的线程可以提供一个基于软件的周期计数器。我们发现这种方法在各种平台上都能够可靠地工作，并且可以提供很高的精度度。请注意，JavaScript版本的AnC也采用了类似的技术来构建高精度的计时器。
**讨论**
利用x86_64平台上的cpuid以及ARMv7-A和ARMv8-A上的扁平设备树（FDT），我们可以检测包括处理器属性（如TLB、缓存、处理器和供应商的名称）和微架构的等处理器拓扑信息。有了这些信息，我们就可以构建一个适当的驱逐组，以便在缺少页表和转译缓存的架构上成功地自动执行AnC攻击。因此，在带有页表或转译缓存的体系结构上，我们可以通过构建驱逐组并尝试渐进式执行AnC攻击来逆向这些缓存的大小。
**评测**
我们使用Intel、ARM和AMD等公司从2008年到2016年期间发布的20个不同的CPU对我们的技术进行了全面的评估，并发现了每个页表级的页表缓存的具体大小（我们称2级页表为PL2），以及利用我们的技术逆向这个信息所需要的时间。同时，我们还提供了每种CPU的缓存和TLB的大小。
我们的研究结果总结见表1。下面，我们将逐一介绍各个供应商产品在这些方面的特点和差异。
表1：22种不同微架构的规格和逆向结果
**Intel**
在英特尔的处理器中，最后一级缓存是包含型的，这意味着最后一级缓存中可用的数据必须在较低级别的缓存中可用。由于这个特性，只要从最后一级缓存中逐出缓存行就足够了，因为这将导致它们将被从较低级别的缓存中逐出。我们发现，英特尔的页面结构缓存或切片转译缓存是在Intel
Core和Xeon处理器上实现的，至少是Nehalem微架构。在Intel
Core和Xeon处理器上，具有可供24-32个PL2表项和4-6个PL3表项的高速缓存，而在Silvermont处理器上，只有一个高速缓存，仅仅可以供12-16个PL2表项使用。在我们的多次测试期间，我们注意到，它们主要集中于几个彼此接近的数字。保守的攻击者可以总是选择更大的数字。在Intel
Core和Xeon处理器以及Silvermont处理器上，我们发现cpuid报告的TLB的大小正好适用于完全刷新这些缓存，这很可能是因为用于缓存巨型页面的TLB也包含了缓存中间页面查询的逻辑。最后，我们发现，当Sandy
Bridge和Ivy
Bridge实现一个TLB来缓存1G页面时，cpuid指令不会报告这个TLB的存在，并且Nehalem和Westmere都实现了一个PL3页面结构缓存，但是没有提供这样的TLB实现。
**AMD**
在AMD的处理器上，LLC是独占型的，这意味着数据最多可以放入一个高速缓存中，以便可以一次存储更多的数据。为了能够正确驱逐缓存行，我们必须分配一个驱逐组，其大小等于高速缓存大小的总和。我们经测试发现，AMD
K10微体系结构实现了AMD的页面查询缓存具有24个表项。此外，我们的测试表明，AMD的页面查询缓存没有被Bulldozer微体系结构的设计和该微体系结构的后代所采用。因此，AMD的Bulldozer架构似乎没有提供任何页表或转译缓存。
最后，AMD的Bobcat架构似乎实现了一个带有8到12个表项的页面目录缓存。
**ARMv7-A**
与Intel和AMD的处理器不同，根据片上系统的供应商的不同，有些ARM处理器上的L2缓存可以配置为包含型、独占型或非包含型。
然而，对于大多数ARMv7-A处理器来说，这些缓存都是配置为非包含型的。在ARMv7-A上，有两个页面级别可用，其中的页表分别提供256和4096个表项，分别可以映射4K和1M空间。对于支持大容量物理内存地址扩展（LPAE）的处理器，则使用三个页面级别，其中每个页表分别提供了512、512和4个表项，可以映射4K、1M和1G空间。即使最后一级页表仅由适合单个缓存行的四个表项组成，但是AnC攻击仍然可以应用于其他两个页面级别，以确定页表和转译缓存的存在性。此外，其低功耗版本（例如ARM
Cortex A7）则实现了统一的页表缓存，而面向高性能的版本（例如ARM Cortex A15和A17）则实现了统一的转译缓存。但是，较旧的设计，如ARM
Cortex A8和A9，却根本没有任何MMU缓存。同时，我们发现带有64个表项的页表缓存和带有16个表项的转译缓存分别可用于ARM Cortex
A7和ARM Cortex A15。此外，我们的程序可以可靠地确定出所有支持和不支持LPAE的ARMv7-A的这些高速缓存的大小，即使在启用所有核心的ARM
big.LITTLE处理器上，也可以透明地在不同类型的核心之间来回切换。
**ARMv8-A**
ARMv8-A处理器也实现了类似于Intel和AMD的包含型LLC。此外，ARMv8-A使用与x86_64类似的模型，提供了四个页面级别，每级512个表项。然而，在Linux系统上，仅使用了三个页面级别来提高页表查找的性能。与ARMv7-A类似，ARMv8-A在其低功耗版本（例如ARM
Cortex A53）上实现了4路关联64项统一页表缓存，并在注重性能的版本，例如ARM Cortex A57，
A72和A73中实现了一个统一的转译缓存。此外，我们还发现ARM Cortex A53实现了一个具有64个表项的页表缓存。
**讨论**
如代码清单2所示，我们可以通过分配与缓存项一样多的页面，然后“触动”每个页面级别中的每个页面来刷新TLB和页面结构。通过触动这些页面，MMU就会被迫执行虚拟地址转换以替换缓存中的现有的表项。此外，通过使用页面大小作为每个页面级别的步幅，我们可以利用巨型页面来刷新页面结构缓存或TLB。
    1 /* Flush the TLBs and page structure caches. */
    2 for (j = 0, level = fmt->levels; j data + cache_line * cache->line_size;
    5 6
    for (i = 0; i ncache_entries; ++i) {
    7 *p = 0x5A;
    8 p += level->page_size;
    9 }
    10 }
代码清单2：刷新TLB和页面结构缓存。
**相关工作**
**针对页表的硬件攻击**
AnC攻击可以根据MMU将PTE缓存到LLC中的方式来发动EVICT +
TIME攻击，从而利用JavaScript给用户空间ASLR去随机化。而使用预取指令的硬件攻击则依赖于缓存的TLB表项和部分转译来实现内核空间ASLR的去随机化。页表是Rowhammer攻击最有吸引力的目标。Drammer和Seaborn的硬件攻击会破坏PTE，使其指向页表页面。但是，如果不能正确刷新页表缓存的话，所有这些攻击都将失败。本文提供了一种用于在各种体系结构上刷新这些缓存的通用技术。
**逆向硬件**
针对商品化硬件的逆向工程已经随着对硬件的攻击的增加而变得日益流行。Hund等人对英特尔处理器如何将物理内存地址映射到LLC进行了逆向工程。Maurice
等人则使用性能计数器来简化了该映射功能的逆向过程。DRAMA则使用DRAM总线的被动探测以及对DRAM行缓冲器的定时攻击，对内存控制器将数据放置在DRAM模块上的原理进行了逆向工程。在本文中，我们对现有处理器的MMU中常见的页表缓存的各种未公开的特性进行了相应的逆向工程。
**结束语**
当前，基于硬件的攻击（如缓存或Rowhammer攻击）开始变得越来越流行，因为针对软件的攻击已经变得越来越具有挑战性。对于跨处理器的鲁棒性攻击方法来说，掌握各种处理器内缓存的相关特性是至关重要的。由于这些缓存通常对软件来说是不可见的，因此通常很难找到相关的文档说明。在本文中，我们改进了针对MMU的现有EVICT
+
TIME攻击，使其能够逆向最新处理器上常见的页表缓存的各种特性。我们将该技术应用于20个不同的微架构，发现其中17个实现了这样的页表缓存。我们的开源实现提供了一个便利的接口，可以用于在这16个微体系结构上刷新这些缓存，并可以在未来的微架构上自动检测页表缓存。更多信息，请访问：