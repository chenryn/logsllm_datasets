the following machines: a traﬃc generator with an In-
tel Xeon E3-1270 v3 CPU and 32GB RAM, and a hy-
pervisor with a Dual Intel Xeon E5-2690 v3 CPU and
384GB RAM. Both machines run Linux Ubuntu 14.04,
with VMs running on the hypervisor using KVM. The
machines are connected through a 10 Gbps network.
All NFs and OBIs, as well as the OBC, run on top of
the hypervisor. We play a packet trace captured from
a campus wireless network on the traﬃc generator, at
10Gbps. All packets go through the hypervisor on their
corresponding service chain as deﬁned for each test.
5.2 Test Applications
For our tests we have implemented a set of sample
OpenBox applications. For each of the following appli-
cations, we also created a reference stand-alone version,
in Click.
Sample Firewall.
We use a ruleset of 4560 ﬁrewall rules from a large
ﬁrewall vendor. Our OpenBox ﬁrewall application reads
the rules from a ﬁle and generates a processing graph
that raises an alert for packets that match any non-
default rule.
In order to correctly measure through-
put, we have modiﬁed the rules so that packets are
never dropped. Instead, all packets are transmitted un-
touched to the output interface.
Sample IPS.
We use Snort web rules to create a sample IPS that
scans both headers and payloads of packets. If a packet
matches a rule, an alert is sent to the controller. As in
the ﬁrewall, we have modiﬁed the rules to avoid drop-
ping packets.
Sample Web Cache.
Our web cache stores web pages of speciﬁc websites.
If an HTTP request matches cached content, the web
cache drops the request and returns the cached con-
tent to the sender. Otherwise, the packet continues
untouched to the output interface. When measuring
performance of service chains that include this NF, we
only send packets that do not match cached content.
Sample Load Balancer.
This NF uses Layer 3 classiﬁcation rules to split traﬃc
to multiple output interfaces.
5.3 Test Setup
We tested our implementation of the OpenBox frame-
work using a single controller and several VMs. Each
VM either runs Click with the reference standalone NF
implementation, or an OBI that is controlled by our
OBC.
We consider two diﬀerent NF conﬁgurations in our
tests. In the ﬁrst conﬁguration, packets from the traﬃc
generator to the sink go through a pipeline of two NFs.
The throughput in such a conﬁguration is dominated by
the throughput of the slowest NF in the pipeline. The
latency is the total time spent while processing the two
NFs in the pipeline.
Figures 7 illustrates two test setups under this conﬁg-
uration, without OpenBox: In the ﬁrst test, packets go
through two ﬁrewalls with distinct rule sets (Fig. 7(a)).
In the second test, packets ﬁrst go through a ﬁrewall
and then through an IPS (Fig. 7(b)). With OpenBox
(Fig. 7(c)), all corresponding NFs are executed on the
same OBI, and the OBI is scaled to use the same two
VMs used without OpenBox.
In this case, traﬃc is
multiplexed to the two OBIs by the network forward-
ing plane. We show that the OpenBox framework re-
duces the total latency (due to the merging of the two
519
VM1 Firewall1 VM2 Firewall2 VM1 Firewall VM2 IPS VM1 OBI1 VM2 OBI2 (a) Two-ﬁrewall service chains
(b) A ﬁrewall service chain and an
IPS service chain
(c) Test setup with OpenBox
Figure 8: Test setups under the distinct service chain conﬁguration.
Network Functions
VMs Throughput
Used
[Mbps]
Latency
[µs]
Firewall
IPS
Regular FW+FW chain
OpenBox: FW+FW OBI
Regular FW+IPS chain
OpenBox: FW+IPS OBI
1
1
2
2
2
2
840
454
840
48
76
96
1600 (+90%)
48 (-50%)
454
124
846 (+86%)
80 (-35%)
Table 2: Performance results of the pipelined
NFs conﬁguration (Figure 7).
processing graphs) and increases the overall throughput
(because of the OBI scaling).
Another NF conﬁguration we consider is when pack-
ets of diﬀerent ﬂows go through diﬀerent, distinct ser-
vice chains, and thus visit diﬀerent NFs. Under this
conﬁguration we test the following scenarios, as illus-
trated in Figure 8:
in Figure 8(a) packets either go
through Firewall 1 or through Firewall 2 while in Fig-
ure 8(b) packets either go through a ﬁrewall or through
an IPS. We use the same rule sets as in the previous
tests.
Merging the two NFs in this case provides dynamic
load balancing by leveraging oﬀ-peak times of one NF
to provide higher throughput to the other. We use the
OBI setup as shown in Figure 8(c), this time only ap-
plying the processing graph of one NF on each packet,
according to its type or ﬂow information.
Note that in both conﬁgurations, each NF could come
from a diﬀerent tenant, or a diﬀerent administrator.
The diﬀerent NFs are not aware of each other, but as
discussed in Section 3, they may be executed in the
same OBI.
5.4 Results
5.4.1 Data Plane Performance
Pipelined NFs.
Table 2 shows the results of the pipelined NF conﬁgu-
ration. Without OpenBox, the throughput is bounded
by the throughput of the slowest NF in the pipeline.
Thus, in the two pipelined ﬁrewalls, the overall through-
put is the throughput of a single ﬁrewall (both ﬁrewalls
show the same performance as we split rules evenly).
In the pipelined ﬁrewall and IPS service chain, the IPS
dominates the overall throughput as it is much slower
than the ﬁrewall, since it performs deep packet inspec-
tion. The overall latency is the sum of the latencies of
both NFs in the chain, as packets should go through
both VMs.
With OpenBox, the controller merges the two NFs
into a single processing graph that is executed on OBIs
on both VMs. Packets go through one of the VMs
and are processed according to that processing graph.
We use static forwarding rules to load-balance the two
OBIs. The overall throughput is of the two OBIs com-
bined. The overall latency is of a single OBI, as packets
are only processed by one of the VMs.
OpenBox improves the throughput by 90% in the
two-ﬁrewall setup and by 86% in the ﬁrewall and IPS
setup. It reduces latency by 50% and 35% in these two
setups, respectively.
Distinct Service Chains.
Figure 9(a) shows the achievable throughput regions
for the distinct service chain conﬁguration with two ﬁre-
walls, with and without OpenBox. Without OpenBox
(see red, dashed lines), each ﬁrewall can utilize only the
VM it runs on, and thus its throughput is limited to
the maximal throughput it may have on a single VM.
With OpenBox (see blue, solid line), each ﬁrewall can
dynamically (and implicitly) scale, when the other NF
is under-utilized. We note that if both NFs are likely to
be fully utilized at the same time, merging them may
not be worthwhile, but they can still be implemented
with OpenBox and deployed in diﬀerent OBIs.
Figure 9(b) shows the achievable throughput regions
when merging a ﬁrewall with an IPS. In this case the
IPS dominates OBI throughput and it might be less
beneﬁcial to merge the two service chains, unless the
ﬁrewall is never fully utilized while the IPS is often over-
utilized.
Discussion.
Two factors help OpenBox improve data plane per-
formance. First, by merging the processing graphs and
eliminating multiple classiﬁers, OpenBox reduces latency
and the total computational load. Second, OpenBox al-
lows more ﬂexible NF deployment/replication than with
monolithic NFs, so packets should traverse fewer VMs,
520
VM1 Firewall1 VM2 Firewall2 VM1 Firewall VM2 IPS VM1 OBI1 VM2 OBI2 (a) Two Firewalls
(b) Firewall and IPS
Figure 9: Achievable throughput for the distinct service chain conﬁguration (Figures 8(a) and 8(b))
compared to the achievable throughput of the two OBIs that merge both NFs (Figure 8(c)).
Operation
SetProcessingGraph
KeepAlive
GlobalStats
AddCustomModule
Round Trip Time
1285 ms 4
20 ms
25 ms
124 ms
Table 3: Average round-trip time for common
messages between OBC and OBIs, running on
the same physical machine.
our graph merge algorithm, the throughput for the same
packets is 890 Mbps (20% improvement).
Figure 11 evaluates the scalability of the graph merge
algorithm. We tested the algorithm with growing sizes
of input graphs on the Xeon E5-2690 CPU. The merge
algorithm runs in orders of milliseconds, and the time
grows nearly linearly with the size of graphs.
5.4.3 Control Plane Communication
In addition to data plane performance, we also eval-
uated the performance of the communication with the
controller. Table 3 shows the round-trip time for three
common protocol operations: SetProcessingGraph is the
process of sending a SetProcessingGraphRequest from
the OBC to an OBI with a new processing graph for
the OBI, reconﬁguring the execution engine and return-
ing a SetProcessingGraphResponse from the OBI to the
OBC. A KeepAlive message is a short message sent from
an OBI to the OBC every interval, as deﬁned by the
OBC. GlobalStats is the process of sending a Global-
StatsRequest from the OBC to an OBI and returning
a GlobalStatsResponse from the OBI to the OBC, with
the OBI system load information (e.g., CPU and mem-
ory usage). AddCustomModule is the process of send-
ing an AddCustomModuleRequest from the OBC to a
supporting OBI with a custom binary module that ex-
4This operation involves (re-)conﬁguration of Click elements,
which requires polling the Click engine until all elements are up-
dated.
In Click, there is a hardcoded delay of 1000 ms in this
polling. This can be easily reduced, albeit with a change in the
core Click code.
Figure 10:
algorithm test.
Service chain for the graph merge
Figure 11: Scalability of the graph merge algo-
rithm.
just as in the case when each NF is deployed separately.
This ﬂexible deployment also allows resource sharing.
5.4.2 Performance of the Graph Merge Algorithm
In order to evaluate the impact of our graph merge
algorithm on the performance of the data plane, we
considered a longer service chain, as illustrated in Fig-
ure 10. In this service chain packets go through a ﬁrst
ﬁrewall and then through a web cache. If not dropped,
they continue to another ﬁrewall, and eventually go
through an L3 load balancer.
We implemented this service chain in OpenBox by
merging the four NFs into a single processing graph.
When using a na¨ıve merge, where graphs are simply
concatenated to each other, we obtain 749 Mbps through-
put (on a single VM, single core) for packets that do not
match any rule that causes a drop or DPI. When using
521
Maximal throughput of Firewall 1 [Mbps]0500100015002000Maximal throughput of Firewall 2 [Mbps]0500100015002000Dynamic Load Balancing Throughput RegionStatic Load Balancing Throughput RegionMaximal throughput of IPS [Mbps]0500100015002000Maximal throughput of Firewall [Mbps]0500100015002000Dynamic Load Balancing Throughput RegionStatic Load Balancing Throughput RegionGateway Firewall Dept. Firewall Load Balancer Web Cache Merged graph size [Number of connectors]500100015002000250030003500400045005000Merge time [ms]050100150200250300350400tends the OBI behavior. In this test we used a module
of size 22.3 KB, which adds support for a single pro-
cessing block.
As these times were measured when the OBC and
the OBI run on the same physical machine, they ignore
network delays and mainly measure software delay.
6. DISCUSSION
This paper lays the foundations for a software-deﬁned
framework for NFs. In this section we discuss additional
aspects of the proposed framework and possible future
research directions.
Security.
Having multiple applications from possibly multiple
tenants running on the same framework increases con-
cerns about security and information leakage. Several
previous works have addressed such concerns (e.g. [22,
25]) by creating privacy-preserving data structures for
packet classiﬁers. Such works can be applied on the
OpenBox framework as they are orthogonal to the Open-
Box solution.
The declarative northbound API of our OBC lets ap-
plications specify their logic and listen to events. How-
ever, the API does not let applications query the logic of
other applications, or the merged logic of speciﬁc OBIs.
The OBC is responsible for safely demultiplexing events
when these are signaled from the data plane. Each re-
quest from a speciﬁc application has its own ID and
each response carries this ID to allow correct demulti-
plexing.
In the data plane, one application can reduce the per-
formance of another application (possibly of a diﬀerent
tenant), especially if two applications are merged and
executed on the same OBI. For example, a malicious
application can cause an increased classiﬁcation over-
head by setting an overwhelming number of classiﬁca-
tion rules.
Custom module injection may also expose new threats
to the data plane, especially in a multi-tenant environ-
ment. In such a case, we suggest that digital signatures
be enforced on all injected modules (namely, verifying
their security or origin before injecting them to the net-
work).
Control Level Graph Optimization.
The OBC can provide optimization to user-deﬁned
processing graphs, in addition to that provided by the
merge algorithm presented in Section 2.2.1. For exam-
ple, it could reorder blocks or merge them, or even re-
move or replace blocks. The ability to split a processing
graph between multiple OBIs can also be used to auto-
matically optimize performance and for load balancing.
Debugging.
As with SDN applications, debugging of applications
to be deployed on top of the control plane northbound
API can be challenging. Works on SDN debugging such
as [20] could serve as a basis for future research on
OpenBox application debugging.
Application Programming Language and Veriﬁca-