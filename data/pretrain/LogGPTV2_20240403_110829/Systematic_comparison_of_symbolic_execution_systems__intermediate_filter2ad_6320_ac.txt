We see that, in general, the binary-based techniques produce a
higher number of IR instructions than KLEE’s source-based trans-
lation; of course, there are many factors involved the size of the
generated translation artifacts. It is more meaningful when the
target of the translation process is the same IR, which removes
CGC
1.00
0.74
6.68
4.57
IR
Machine code
LLVM bitcode
LLVM bitcode
VEX IR
LLVM bitcode
coreutils
IR generator
1.00
Qsym
0.78
KLEE (clang)
6.29
S2E
5.35
angr (libvex)
4.54
McSema
angr on ARM (libvex) VEX IR
4.40
Table 3: Mean inflation factor per IR generation mechanism
and data set, i.e., average number of generated IR instruc-
tions per machine-code instruction as shown in Figure 2.
The CGC data set contains 123 programs, the coreutils suite
106 programs.
one variable from the analysis. Therefore, the cases of S2E and
McSema are of particular interest: both tools start at the binary
level and produce LLVM IR, so we can compare their results with
the source-based LLVM IR generated by KLEE. Note that, while the
IR produced from source code is rather succinct, in almost all cases
containing less instructions than the equivalent machine code and
reaching an inflation factor below 1 on average, the corresponding
IR generated from binaries increases the number of instructions
by a factor of above 6 for S2E and 4.54 for McSema. S2E’s higher
inflation factor may be due to the two IR translations (QEMU IR
to LLVM IR). Furthermore, it is interesting to see that angr’s trans-
lation to VEX IR yields an increase in the number of instructions
that is similar to the binary-based tools translating to LLVM IR; in
fact, manual analysis suggests that the semantic content of instruc-
tions in VEX IR is comparable to LLVM IR. The ARM experiment
confirms the overall picture on a different architecture. On average,
we find that the IR generated from binaries is considerably larger
than source-based IR.
In summary, the data supports the hypothesis that a source-based
approach has more high-level information available to generate
a succinct IR. The following experiments assess the properties of
symbolic execution on such IR.
5.4 Execution Speed
The first aspect of symbolic execution that we are interested in is
how well the generated IR is suited for execution. There is a spec-
trum between Qsym, which forgoes translation to IR entirely and
directly executes instrumented machine code, computing symbolic
constraints on the fly, and KLEE, which interprets high-level IR
derived from source code. Intuitively, we would expect IR that is
close to (or identical with) machine code to be efficiently executable,
while a more abstract representation may be more suited to static
analysis but slower in execution.
A major challenge in comparing the execution speed of the four
symbolic execution engines is that they use very different strategies
on the matter of interleaving concrete and symbolic execution, an
issue that was briefly mentioned in Section 3.3. In general, code
can be executed in one of four modes:
Native The simplest case is native execution of machine code
on the CPU, possibly with some sort of instrumentation.
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Sebastian Poeplau and Aurélien Francillon
Native
Native
(emulated)
IR
(concrete)
IR
(symbolic)
✓
✓
Qsym
S2E
angr
KLEE
Table 4: Execution modes used by the symbolic execution
engines in our study. For Qsym, the “IR” in symbolic mode
is machine code.
✓
✓
✓
✓
✓
✓
✓
This is what Qsym does for concrete execution, and S2E uses
QEMU with KVM enabled, resulting in a similar effect.
Native (emulated) This case is similar to raw native execu-
tion, except that the CPU is emulated. angr uses emulated
native execution for code that does not work with symbolic
data.
IR (symbolic) When code works with symbolic data, it has to
be translated to IR, which is then interpreted symbolically.
All four systems in our study use this mode; in the case of
Qsym, the “IR” is machine code.
IR (concrete) KLEE does not support interleaved concrete ex-
ecution, so even code that works with only concrete data
is executed at the IR level. Similarly, angr may heuristically
choose to run even concrete computations with IR in situa-
tions where the cost of switching back and forth between IR
and emulated native execution would otherwise be too high.
Table 4 shows the use of the various execution modes by the
systems in our analysis. We are interested in the execution of IR, so
for the purpose of this study we count only instructions executed
in one of the two IR modes, and we only measure the time spent in
those modes.
Apart from the difficulty of handling different execution modes,
the question of execution speed is particularly prone to being influ-
enced by other factors than merely the IR generation process. In
particular, the programming language that a symbolic executor is
implemented in has a large effect on how fast it can execute its IR
(see Table 1). There is little we can do to eliminate this bias (short
of reimplementing all systems in a common language); we will take
it into account when interpreting our results.
In order to assess the speed of execution we count the number of
instructions executed at the IR level and the time spent on said exe-
cution while conducting the experiments described in Section 5.1.
In terms of Table 4, we capture the last two columns, which contains
any possible execution of IR. The result is a quantity that we call
execution rate; it represents the number of instructions executed per
unit of time. Figure 3 shows the rates we measured. For comparabil-
ity between different IRs, we translate the execution rates from IR
instructions per time to the common basis of machine instructions
per time using the inflation factors from Table 3.4 In other words,
we obtain a measure of execution speed that is comparable across
different IR generation processes.
4See Appendix B for a visualization of the untranslated rates, expressed in IR instruc-
tions per time.
)
s
/
s
n
o
i
t
c
u
r
t
s
n
i
e
t
a
r
n
o
i
t
u
c
e
x
E
e
n
i
h
c
a
m
(
1 × 107
1 × 106
100000
10000
1000
100
10
1
0.1
Qsym
S2E
Angr
KLEE
Figure 3: Execution speed of symbolically executed instruc-
tions, translated to the common basis of machine-code in-
structions, across 24 CGC programs. Higher rates mean
faster execution.
We observe that Qsym executes its “IR” the fastest, followed by
KLEE, S2E and angr. This matches our intuition, given that Qsym
uses the lowest-level IR and implements its symbolic component
in C++. KLEE and S2E share a common basis, but while KLEE
executes a very concise IR (see Section 5.3), S2E has significantly
more instructions to interpret. Moreover, S2E has to generate IR on
the fly while, in the case of KLEE, IR generation is a preprocessing
step. We largely attribute angr’s lower execution rate to the fact
that its symbolic reasoning is implemented in Python, whereas S2E
uses C++. See Appendix B for data supporting this hypothesis.
In summary, the measurement of execution rates supports the
hypothesis that low-level IR can be executed faster than high-level
IR, and that LLVM bitcode and VEX IR have quite similar properties
when it comes to IR interpretation. Note that “low-level IR” refers
to the level of abstraction of the IR language, not of the artifact that
the IR was generated from. For instance, raw machine code (Qsym)
is executed faster than LLVM bitcode (KLEE and S2E). However, the
source of the translation still impacts the concision of the generated
IR (see Section 5.3)—e.g., LLVM bitcode generated from binaries
(S2E) is typically more verbose than bitcode generated from source
code (KLEE) and hence requires more time to perform equivalent
computations.
5.5 Query Complexity
Along with IR execution, SMT solving is one of the major workloads
in symbolic execution [27, 29]. Consequently, there is promise in
exploring to which extent the IR generation process impacts the
difficulty of the SMT queries arising during execution. Intuitively,
if IR carries a lot of semantic information it should be possible for
the symbolic executor to formulate succinct queries. For example,
consider the program in Listing 1; it just reads five bytes from stan-
dard input, checks a number of conditions on the input and prints a
result message. Listings 2 and 3 show the queries generated by S2E
and KLEE, respectively, for the C expression data[3] == 55. While
the semantic content is the same in both queries, note how S2E
expresses the equality check in bit-wise AND and OR operations as
Symbolic Execution: IR and its Generation
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
well as a bit-vector addition; the query is more similar to machine
code than to the original C code. The KLEE-generated query, in
contrast, resembles the source code rather closely. This example
illustrates the notion that queries with identical semantics can be
formulated in different ways, which may differ in the difficulty they
pose for SMT solvers.
Listing 1: A simple program to demonstrate SMT queries.
# include 
int main ( int argc , char * argv []) {
char data [5];
for ( int i = 0; i  15 && data [1] == 32 &&
data [2] > 27 && data [2] < 100 &&
data [3] == 55 && data [4] == 123)
printf (" Correct !\ n" );
else
printf (" Try again ...\ n" );
return 0;
}
Listing 2: Part of S2E’s assertion for Listing 1. We use stan-
dard SMT-LIB syntax [2] for SMT queries.
(= (_ bv0 64)
( bvand
( bvadd
;; 0 xFFFFFFFFFFFFFFC9
(_ bv18446744073709551561 64)
(( _ zero_extend 56)
(( _ extract 7 0)
( bvor
( bvand
(( _ zero_extend 56)
( select stdin (_ bv3 32)))
;; 0 x00000000000000FF
(_ bv255 64))
;; 0 xFFFF88000AFDC000
(_ bv18446612132498620416 64)))))
(_ bv255 64)))
Listing 3: Part of KLEE’s assertion for Listing 1.
(= (_ bv55 8)
(( _ extract 7 0)
(( _ zero_extend 24)
( select stdin (_ bv3 32)))))
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
2
3
4
In general, assessing the difficulty of SMT queries is not an
easy task. Even with a proper definition of the elusive concept of
“difficulty” there may be no effective means of measuring it. We
observe that, from a practical point of view, the essential property
of an “easy” query is that the solver can answer it fast. Therefore,
our approach is to run all symbolic execution engines on the same
fixed paths in concolic mode and record the queries that are sent
to the solver. We then run the solver on those queries in isolation
)
s
/
s
e
i
r
e
u
q
(
e
t
a
r
y
r
e
u
q
3
Z
800
700
600
500
400