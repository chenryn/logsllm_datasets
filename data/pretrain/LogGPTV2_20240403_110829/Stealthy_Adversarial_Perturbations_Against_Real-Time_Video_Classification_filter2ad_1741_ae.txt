### Optimized Text

**Introduction and Methodology:**

When a perturbation is added to a video clip without any offset (i.e., no misalignment), the clip is in the form [f1, f2, ..., f16]. Both perturbed clips are misclassified as "apply eye makeup." When an offset of 8 is introduced, meaning that DUP and C-DUP are added to the clip in the form [f9, f10, ..., f16, f1, ..., f8], DUP fails to misclassify the clip, while C-DUP still successfully misclassifies it. In fact, C-DUP works for all offsets from 0 to 15, whereas DUP only works when the offset is 0, 1, 2, or 15.

**Aggregate Results:**

- **UCF-101 Test Set:**
  - The attack success rates with DUP and C-DUP on the UCF-101 test set are shown in Figures 14a and 14b.
  - Figure 14a depicts the average attack success rate for inputs from the target class. When there is no misalignment, the attack success rate with DUP is 84.49%, which is slightly higher than C-DUP. However, the attack success rate with C-DUP is significantly higher when there is misalignment.
  - The average attack success rate across all alignments for the target class with C-DUP is 84%, while with DUP it is only 68.26%. This demonstrates that C-DUP is more robust against misalignment.
  - Figure 14b shows that, with regard to the classification of inputs from non-target classes, C-DUP also achieves slightly better performance than DUP when there is a mismatch. The average attack success rate (across all alignments) with C-DUP is 87.52%, while with DUP it is 84.19%.

- **Jester Dataset:**
  - **Visualizing the Perturbations:**
    - Visual representations of the C-DUP perturbations for the two target sets, T1 = {sliding hand right} and T2 = {shaking hands}, are shown in Figures 12 and 13. The perturbation clip has 16 frames, and we present a visual representation of the first 8 frames for compactness.
    - Compared to the perturbation generated on UCF-101 (see Figure 11), there is a more pronounced evolution with respect to Jester. This is because UCF-101 is a coarse-grained action dataset where spatial (appearance) information is dominant, and the C3D model does not need much temporal information to perform well. In contrast, Jester is a fine-grained action dataset where temporal information plays a more important role. Therefore, more significant evolutions of the perturbations on the frames in a clip are required to attack the C3D model trained on the Jester dataset.
  - **Attack Success Rate:**
    - To compare the misclassification rates between DUP and C-DUP, we adjust the weighting factor λ such that the classification accuracy with respect to non-target classes is similar. By choosing λ = 1.5 for DUP and 1 for C-DUP, we achieve this.
    - For T1 = {sliding hand right}, the results are similar to those observed with UCF-101. The attack success rates for C-DUP are slightly lower than those for DUP when the offset is 0, but C-DUP outperforms DUP when there is misalignment. The average success rate for C-DUP is 85.14% for the target class and 81.03% for the other (non-target) classes. The average success rate for DUP is 52.42% for the target class and 82.36% for the other (non-target) classes.
    - For T2 = {shaking hands}, both DUP and C-DUP achieve relatively lower success rates, especially with regard to the other (non-target) classes. This is because no other class is temporally similar to 'shaking hands,' making misclassification harder. The attack success rates with the two approaches for the target class are shown in Figure 14e. C-DUP significantly outperforms DUP in terms of attack efficacy due to its robustness to temporal misalignment. The average attack success rate for the target class with C-DUP is 79.03%, while for DUP it is only 57.78%.

**Effectiveness of 2D-DUP:**

- **UCF101 Dataset:**
  - **Visual Impact of the Perturbation:**
    - We present a sequence of original frames and their corresponding perturbed frames in Figure 15. Original frames are displayed in the first row, and perturbed frames are displayed in the second row. The perturbation added to the frames is quasi-imperceptible to human eyes.
  - **Attack Success Rate:**
    - By adding 2D-DUP on the video clip, we achieve an attack success rate of 87.58% with respect to the target class and an attack success rate of 83.37% for the non-target classes. The average attack success rates with C-DUP were 87.52% and 84.00%, respectively. Thus, the performance of 2D-DUP is on par with that of C-DUP on the UCF101 dataset, demonstrating that C3D is vulnerable even if the same 2D perturbation is added to every frame.

- **Jester Dataset:**
  - **Attack Success Rate:**
    - For T1 = {sliding hand right}, the attack success rate for the target class is 84.64%, and the attack success rate for the non-target classes is 80.04%. This shows that 2D-DUP is also successful on some target classes in the fine-grained Jester action dataset.
    - For T2 = {shaking hands}, the success rate for the target class drops to 70.92%, while the success rate for non-target classes is 54.83%. This is slightly degraded compared to the success rates achieved with C-DUP (79.03% and 57.78%, respectively), but is still reasonable. This degradation is due to more significant temporal changes, and a single 2D perturbation is less effective in manipulating these changes. In contrast, the evolving perturbations within C-DUP are much more effective in achieving misclassification of the target class.

**Discussion:**

- **Black Box Attacks:**
  - In this work, we assumed that the adversary is fully aware of the DNN being deployed (i.e., white box attacks). While this is one of the first efforts on generating adversarial perturbations on real-time video classification systems, in practice, the adversary may need to determine the type of DNN being used, and a black box approach may be needed. Given recent studies on the transferability of adversarial inputs, we believe black box attacks are also feasible and will explore this in future work.

- **Context Dependency:**
  - Our approach does not account for contextual information, i.e., consistency between the misclassified result and the context. In some cases, a loss in context may cause a human operator to notice discrepancies. For example, if the context relates to a baseball game, a human overseeing the system may notice an inconsistency when the action of hitting a ball is misclassified into applying makeup. Generating perturbations that are consistent with the context of the video is a line of future work that we will explore.

- **Data Augmentation:**
  - For both UPs and DUPs, the training set included all possible strides (data augmentation). However, issues relating to the boundary effect cannot be solved by data augmentation. Specifically, the misalignment due to the nondeterminism in clip boundaries input to the classifier causes the perturbation clips added by the attacker to be broken up. While UPs are effective on any video clip, concatenations of broken-up UPs are no longer UPs and thus are not effective.

- **Defenses:**
  - To defend against attacks on video classification systems, one can try existing defense methods in the image domain, such as feature squeezing and ensemble adversarial training. Considering the properties of video, we envision exclusive defense methods for protecting video classification systems:
    - One approach is to examine the consistency between the classification of consecutive frames within a clip and between consecutive clips in a stream. A sudden change in the classification results could raise an alarm.
    - Another line of defense is to identify objects present in the video, e.g., a soccer ball in a video clip depicting a kicking action. We can use an additional classifier to identify such objects and look for consistency with the action and the object.

**Related Work:**

- There is extensive research on the vulnerability of machine learning systems to adversarial inputs. Most efforts, however, do not consider real-time temporally varying inputs such as video. Unlike these efforts, our study focuses on the generation of adversarial perturbations to fool DNN-based real-time video action recognition systems.
- The threat of adversarial samples to deep-learning systems has received considerable attention. Several papers have shown that state-of-the-art DNN-based learning systems are vulnerable to well-designed adversarial perturbations. Szegedy et al. showed that the addition of hardly perceptible perturbation on an image can cause a neural network to misclassify the image. Goodfellow et al. analyzed the potency of adversarial samples available in the physical world. Moosavi-Dezfooli et al. made a significant contribution by generating image-agnostic perturbations, called universal adversarial perturbations, which can cause all natural images belonging to target classes to be misclassified with high probability.
- There are very few recent studies exploring the feasibility of adversarial perturbation on videos. Hosseini et al. attacked the Google Cloud Video Intelligence API, which makes decisions based on the first frame of every second of the video, by inserting images/perturbing frames at the rate of one frame per second. This attack method cannot be generalized to the common case where video classification systems use sequences of consecutive frames to perform activity recognition.