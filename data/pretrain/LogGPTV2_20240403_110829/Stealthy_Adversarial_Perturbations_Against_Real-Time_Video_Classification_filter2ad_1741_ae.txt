added to this clip without any offset (no misalignment) i.e., the
clip is in the form [f1, f 2, ··· , f16], both perturbed clips are
misclassiﬁed to ”apply eye makeup”. When there is an offset of
8, meaning that DUP and C-DUP are added to the clip in the
form [f9, f10,··· , f16, f1,··· , f8], DUP fails to misclassify
the clip while C-DUP still successfully misclassiﬁes it. In fact,
we observe that C-DUP works for all offsets from 0 to 15 while
DUP only works when the offset = 0, 1, 2, 15, on this input
clip.
Aggregate results: The attack success rates with DUP and
C-DUP, on the UCF-101 test set, are shown in Figure 14a
and Figure 14b. The x axis is the misalignment between the
perturbation clip and the input clip to the classiﬁer. Figure 14a
depicts the average attack success rate for inputs from the
target class. We observe that when there is no misalignment,
the attack success rate with DUP is 84.49%, which is in fact
slightly higher than C-DUP. However, the attack success rate
with C-DUP is signiﬁcantly higher when there is misalign-
ment. Furthermore, the average attack success rate across all
alignments for the target class with C-DUP is 84%, while with
DUP it is only 68.26%. This demonstrates that C-DUP is more
robust against misalignment.
Figure 14b shows that, with regard to the classiﬁcation
of inputs from the non-target classes, C-DUP also achieves a
performance slightly better than DUP when there is mismatch.
The average attack success rate (across all alignments) with C-
DUP is 87.52% here, while with DUP it is 84.19%.
2) Experimental Results on Jester:
Visualizing the perturbations: Visual
representations of
the C-DUP perturbations for the two target sets, T1 =
{sliding hand right} and T2 = {shaking hands} are shown in
Figure 12 and Figure 13. The perturbation clip has 16 frames,
and we present a visual representation of the ﬁrst 8 frames
for compactness. We notice that compared to the perturbation
generated on UCF-101 (see Figure 11).
there is a more
pronounced evolution with respect to Jester. We conjecture that
this is because UCF-101 is a coarse-grained action dataset in
which the spatial (appearance) information is dominant. As
a consequence, the C3D model does not extract/need much
temporal information to perform well. However, Jester is a
ﬁne-grained action dataset where temporal information plays
a more important role. Therefore, in line with expectations,
we ﬁnd that in order to attack the C3D model trained on the
Jester dataset, more signiﬁcant evolutions of the perturbations
on the frames in a clip are required (i.e., more changes in the
temporal dimension).
Attack success rate: To showcase a comparison of the mis-
classiﬁcation rates with respect to the target class between the
two schemes (DUP and C-DUP), we adjust the weighting fac-
tor λ such that the classiﬁcation accuracy with respect to non-
target classes are similar. By choosing λ = 1.5 for DUP and
1 for C-DUP, we are able to achieve this. The attack success
rates for the above two target sets are shown in Figure 14c
and Figure 14d, and Figure 14e and Figure 14f, respectively.
We see that with respect to T1 = {sliding hand right}, the
results are similar to what we observe with UCF101. The
attack success rates for C-DUP are a little lower than those for
DUP when the offset is 0. This is to be expected since DUP is
tailored for this speciﬁc offset. However, C-DUP outperforms
DUP when there is a misalignment. The average success rate
for C-DUP is 85.14% for the target class and 81.03% for the
other (non-target) classes. The average success rate for DUP
is 52.42% for the target class and 82.36% for the other (non-
target) classes.
Next we consider the case with T2 = {shaking hands}. In
general, we ﬁnd that both DUP and C-DUP achieve relatively
lower success rates especially with regard to the other (non-
target) classes. As discussed in §VII-A, unlike in the previous
case where ‘sliding two ﬁngers right’ is temporally similar
to ‘sliding hand right’, no other class is temporally similar
to ‘shaking hand’. Therefore it is harder to achieve misclas-
siﬁcation. The attack success rates with the two approaches
for the target class are shown in Figure 14e. We see that C-
DUP signiﬁcantly outperforms DUP in terms of attack efﬁcacy
because of its robustness to temporal misalignment (i.e., the
boundary effect). The average attack success rate for the target
class with C-DUP is 79.03% while for DUP it is only 57.78%.
Overall, our C-DUP outperforms DUP in being able to achieve
a better attack success rate for the target class. We believe that
although stealth is affected to some extent, it is still reasonably
high.
D. Effectiveness of 2D-DUP
The visual representations of the perturbations with C-DUP
show that perturbations on all the frames are visually similar.
Thus, we ask if it is possible to add “the same perturbation”
on every frame and still achieve a successful attack. In other
words, will the 2D-DUP perturbation attack yield performance
similar to the C-DUP attack ?
1) Experimental Results on the UCF101 Dataset:
Visual impact of the perturbation: We present a sequence
of original frames and its corresponding perturbed frames in
Figure 15. Original frames are displayed in the ﬁrst row and
perturbed frames are displayed in the second row. We observe
that the perturbation added to the frames is quasi-imperceptible
to human eyes (similar results are seen with C-DUP but are
omitted in the interest of compactness).
Attack success rate: By adding 2D-DUP on the video clip,
we achieve an attack success rate of 87.58% with respect to
the target class and an attack success rate of 83.37% for the
non-target classes. Recall that the average attack success rates
with C-DUP were 87.52% and 84.00%, respectively. Thus, the
performance of 2D-DUP seems to be on par with that of C-
DUP on the UCF101 dataset. This demonstrates that C3D is
11
(a) Attack success rate on UCF-101 for target class
’applying lipstick’. The baseline accuracy of attack success rate
without perturbation is 4.5%.
(b) Attack success rate on UCF-101 for other non-target classes
(all except ’applying lipstick’). The baseline accuracy of attack
success rate without perturbation is 91.8%.
(c) Attack success rate on Jester for target class
’sliding hands right’.The baseline accuracy of attack success rate
without perturbation is 12.9%.
(d) Attack success rate on Jester for non-target classes (all
excepet ’sliding right’). The baseline accuracy of attack success
rate without perturbation is 90.4%.
(e) Attack success rate on Jester for target class
’shaking hand’. The baseline accuracy of attack success rate
without perturbation is 6.3%.
(f) Attack success rate on Jester for non-target classes (all except
’shaking hand’). The baseline accuracy of attack success rate
without perturbation is 89.9%.
Fig. 14: Attack success rates for DUP and C-DUP along with the offset of mismatch
vulnerable even if the same 2D perturbation generated by our
approach is added to every frame.
2) Experimental Results on Jester Dataset:
Attack success rate: For T1 = {sliding hand right}, the attack
success rate for the target class is 84.64% and the attack
success rate for the non-target classes is 80.04%. This shows
that 2D-DUP is also successful on some target classes in the
ﬁne-grained, Jester action dataset.
rates achieved with C-DUP (79.03% and 57.78% respectively),
but is still reasonable. This degradation is due to more signif-
icant temporal changes in this case (unlike in the case of T1)
and a single 2D perturbation is less effective in manipulating
these changes. In contrast, because the perturbations within
C-DUP evolve, they are much more effective in achieving the
misclassiﬁcation of the target class.
VIII. DISCUSSION
For the target set T2, the success rate for the target class
drops to 70.92%, while the success rate for non-target class
is 54.83%. This is slightly degraded compared to the success
Black box attacks: In this work we assumed that the adversary
is fully aware of the DNN being deployed (i.e., white box
attacks). We argue that this is reasonable given that this is one
12
Fig. 15: Visualizing images after adding 2D dual purpose universal perturbation: Original frames are displayed in the ﬁrst row and
perturbed frames are displayed in the second row. The perturbation added to the frames in the second row is mostly imperceptible
to the human eye.
of the ﬁrst efforts on generating adversarial perturbations on
real-time video classiﬁcation systems. However, in practice the
adversary may need to determine the type of DNN being used
in the video classiﬁcation system, and so a black box approach
may be needed. Given recent studies on the transferability of
adversarial inputs [36], we believe black box attacks are also
feasible. We will explore this in our future work.
Context dependency: Second, the approach that we developed
does not account for contextual information, i.e., consistency
between the misclassiﬁed result and the context. While in
some cases with a limited set of classes (e.g., actions possible
at an elderly care facility), this may be not matter, in some
other cases a loss in context may cause a human operator to
notice discrepancies. For example, if the context relates to a
baseball game, a human overseeing the system may notice an
inconsistency when the action of hitting a ball is misclassiﬁed
into applying makeup. Similarly, because of context, if there is
a series of actions that we want to misclassify, inconsistency in
the misclassiﬁcation results (e.g., different actions across the
clips) may also raise an alarm. For example, let us consider
a case where the actions include running, kicking a ball,
and applying make up. While the ﬁrst two actions can be
considered to be reasonable with regard to appearing together
in a video, the latter two are unlikely. Generating perturbations
that are consistent with the context of the video is a line of
future work that we will explore and is likely to require new
techniques. In fact, looking for consistency in context may be
a potential defense, and we will also examine this in depth in
the future.
Data Augmentation: We point out here that for both UPs and
DUPs, the training set included all possible strides (data aug-
mentation). Unfortunately, the issues relating to the boundary
effect cannot be solved by data augmentation. In particular,
recall that the misalignment due to the nondeterminism in clip
boundaries input to the classiﬁer cause the perturbation clips
added by the attacker to be broken up. While UPs are effective
on any video clip, concatenations of broken up UPs are no
longer UPs and thus, are not effective.
Defenses: In order to defend against the attacks against video
classiﬁcation systems, one can try some existing defense
methods in image area, such as feature squeezing [55], [56] and
ensemble adversarial training [45] (although their effectiveness
is yet unknown). Considering the properties of video that were
discussed, we envision some exclusive defense methods for
protecting video classiﬁcation systems below, which we will
explore in future work.
One approach is to examine the consistency between the
classiﬁcation of consecutive frames (considered as images)
within a clip, and between consecutive clips in a stream. A
sudden change in the classiﬁcation results could raise an alarm.
However, while this defense will work well in cases where the
temporal ﬂow is not pronounced (e.g., the UCF101 dataset), it
may not work well in cases with pronounced temporal ﬂows.
For example, with respect to the Jester dataset, with just an
image it may be hard to determine whether the hand is being
moved right or left.
The second line of defense may be to identify an object that
is present in the video, e.g., a soccer ball in a video clip that
depicts a kicking action. We can use an additional classiﬁer to
identify such objects in the individual frames that compose the
video. Then, we can look for consistency with regard to the
action and the object, e.g., a kicking action can be associated
with a soccer ball, but cannot be associated with a make up kit.
Towards realizing this line of defense, we could use existing
image classiﬁers in conjunction with the video classiﬁcation
system. We will explore this in future work.
IX. RELATED WORK
There is quite a bit of work [2], [3], [16] on investigating
the vulnerability of machine learning systems to adversarial in-
puts. Researchers have shown that generally, small magnitude
perturbations added to input samples, change the predictions
made by machine learning models. Most efforts, however, do
not consider real-time temporally varying inputs such as video.
13
Unlike these efforts, our study is focused on the generation of
adversarial perturbations to fool DNN based real-time video
action recognition systems.
The threat of adversarial samples to deep-learning systems
has also received considerable attention recently. There are
several papers in the literature (e.g., [10], [11], [29], [30],
[39]) that have shown that the state-of-the-art DNN based
learning systems are also vulnerable to well-designed adver-
sarial perturbations [43]. Szegedy et al.show that the addition
of hardly perceptible perturbation on an image, can cause
a neural network to misclassify the image. Goodfellow et
al. [11] analyze the potency of adversarial samples available
in the physical world, in terms of fooling neural networks.
Moosavi-Dezfooli et al. [29]–[31] make a signiﬁcant contri-
bution by generating image-agnostic perturbations, which they
call universal adversarial perturbations. These perturbations
can cause all natural images belonging to target classes to be
misclassiﬁed with high probability.
There are very few recent studies [15], [54] which explore
the feasibility of adversarial perturbation on videos. Hosseini
et al. [15] attack the Google Cloud Video Intelligence API,
which makes decisions only based on the ﬁrst frame of every
second of the video, by inserting images/perturbing frames at
the rate of one frame per second. This attack method cannot
be generalized to the common case where video classiﬁcation
systems use sequences of consecutive frames to perform activ-