interactively add the shares by having Pi compute (cid:7)c(cid:8)i =
(cid:7)a(cid:8)i + (cid:7)b(cid:8)i mod 2(cid:4). We overload the addition operation to
denote the addition protocol by (cid:7)a(cid:8) + (cid:7)b(cid:8).
(·,·)) two shared values (cid:7)a(cid:8) and (cid:7)b(cid:8), we
take advantage of Beaver’s pre-computed multiplication triplet
technique. Lets assume that the two parties already share
(cid:7)u(cid:8),(cid:7)v(cid:8),(cid:7)z(cid:8) where u, v are uniformly random values in Z2(cid:2) and
z = uv mod 2(cid:4). Then Pi locally computes (cid:7)e(cid:8)i = (cid:7)a(cid:8)i − (cid:7)u(cid:8)i
and (cid:7)f(cid:8)i = (cid:7)b(cid:8)i − (cid:7)v(cid:8)i. Both parties run Rec((cid:7)e(cid:8)0,(cid:7)e(cid:8)1) and
Rec((cid:7)f(cid:8)0,(cid:7)f(cid:8)1), and Pi lets (cid:7)c(cid:8)i = i·e·f +f·(cid:7)a(cid:8)i+e·(cid:7)b(cid:8)i+(cid:7)z(cid:8)i.
Boolean sharing can be seen as additive sharing in Z2 and
hence all the protocols discussed above carry over. In particular,
the addition operation is replaced by the XOR operation (⊕) and
multiplication is replaced by the AND operation (AND(·,·)).
We denote party Pi’s share in a Boolean sharing by (cid:7)a(cid:8)B
i .
A
0 , kw
0 , K w
1 = kw
0 = kw
1 = K w
0 = K w
1 and (cid:7)a(cid:8)Y
Finally, one can also think of a garbled circuit protocol as
operating on Yao sharing of inputs to produce Yao sharing of
outputs. In particular, in all garbling schemes, for each wire w
the garbler (P0) generates two random strings kw
1 . When
using the point-and-permute technique [34] the garbler also
0 ||rw
generates a random permutation bit rw and lets K w
1 ||(1−rw). The concatenated bits are then used to
and K w
permute the rows of each garbled truth table. A Yao sharing of a
is (cid:7)a(cid:8)Y
a . To reconstruct the shared
value, parties exchange their shares. XOR and AND operations
can be performed by garbling/evaluating the corresponding
gates.
1 and (cid:7)a(cid:8)Y
To switch from a Yao sharing (cid:7)a(cid:8)Y
1 =
a to a Boolean sharing, P0 lets (cid:7)a(cid:8)B
K w
0 [0] and P1 lets
1 = (cid:7)a(cid:8)Y
(cid:7)a(cid:8)B
1 [0]. In other words, the permutation bits used in
the garbling scheme can be used to switch to boolean sharing
for free. We denote this Yao to Boolean conversion by Y2B(·,·).
We note that we do not explicitly use a Yao sharing in our
protocol description as it will be hidden inside the garbling
scheme, but explicitly use the Y2B conversion to convert the
garbled output to a boolean sharing.
0 = K w
0 = K w
0 , K w
III. SECURITY MODEL
A. Architecture
We consider a set of clients C1, . . . ,Cm who want
to
train various models on their joint data. We do not make
any assumptions on how the data is distributed among the
clients. In particular, the data can be horizontally or vertically
partitioned, or be secret-shared among them as part of a
previous computation.
A natural solution is to perform a secure multiparty com-
putation where each client plays the role of one party. While
this approach satisﬁes the privacy properties we are aiming
for, it has several drawbacks. First, it requires the clients to be
involved throughout the protocol. Second, unlike the two-party
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
Parameters: Clients C1, . . . ,Cm and servers S0,S1.
Uploading Data: On input xi from Ci, store xi internally.
Computation: On input f from S0 or S1, compute (y1, . . . , ym) =
f (x1, . . . , xm) and send yi to Ci. This step can be repeated multiple
times with different functions.
Fig. 3: Ideal Functionality Fml
case, techniques for more than two parties (and a dishonest
majority) are signiﬁcantly more expensive and not scalable to
large input sizes or a large number of clients.
Hence, we consider a server-aided setting where the clients
outsource the computation to two untrusted but non-colluding
servers S0 and S1. Server-aided MPC has been formalized and
used in various previous work (e.g. see [29]). It has also been
utilized in prior work on privacy-preserving machine learning
[37], [36], [21]. Two important advantages of this setting are
that (i) clients can distribute (secret-share) their inputs among
the two servers in a setup phase but not be involved in any
future computation, and (ii) we can beneﬁt from a combination
of efﬁcient techniques for boolean computation such as garbled
circuits and OT-extension, and arithmetic computation such as
ofﬂine/online multiplication triplet shares.
Depending on the application scenario, previous work refers
to the two servers as the evaluator and the cryptography service
provider (CSP) [37], or the evaluator and a cloud service
provider who maintains the data [24]. The two servers can
also be representatives of the different subsets of clients or
themselves be among the clients who possess data. Regardless
of the speciﬁc role assigned to the servers, the trust model is
the same and assumes that the two servers are untrusted but do
not collude. We discuss the security deﬁnition in detail next.
B. Security Deﬁnition
Recall that the involved parties are m clients C1, . . . ,Cm and
two servers S0,S1. We assume a semi-honest adversary A who
can corrupt any subset of the clients and at most one of the two
servers. This captures the property that the two servers are not
colluding, i.e. if one is controlled by the adversary, the second
one behaves honestly. Note that we do not put any restrictions
on collusion among the clients and between the clients and the
servers. We call such an adversary an admissible adversary. In
one particular scenario (see Section V), we weaken the security
model by requiring that servers do not collude with the clients.
The security deﬁnition should require that such an adversary
only learns the data of the clients it has corrupted and the ﬁnal
output but nothing else about the remaining honest clients’
data. For example, an adversary A who corrupts C1,C2 and
S1 should not learn any information about C3’s data beyond
the trained model. We deﬁne security using the framework of
Universal Composition (UC) [15]. We give a brief overview of
the deﬁnition in appendix A, and refer the reader to [15] for
the details. The target ideal functionality Fml for our protocols
is described in Figure 3.
IV. PRIVACY PRESERVING MACHINE LEARNING
In this section, we present our protocols for privacy pre-
serving machine learning using SGD. We ﬁrst describe a
protocol for linear regression in Section IV-A, based solely on
24
arithmetic secret sharing and multiplication triplets. Next, we
discuss how to efﬁciently generate these multiplication triplets
in the ofﬂine phase in Section IV-B. We then generalize our
techniques to support logistic regression and neural networks
training in Sections IV-C and IV-D. Finally, techniques to
support predication, learning rate adjustment and termination
determination are presented in Section IV-E.
A
A
k=1 Mul
(cid:4)(cid:2)d
:= wj − α(
((cid:7)xik(cid:8),(cid:7)wk(cid:8)) − (cid:7)yi(cid:8),(cid:7)xij(cid:8)(cid:5)
A. Privacy Preserving Linear Regression
Recall that we assume the training data is secret shared
between two servers S0 and S1. We denote the shares by
(cid:7)X(cid:8)0,(cid:7)Y(cid:8)0 and (cid:7)X(cid:8)1,(cid:7)Y(cid:8)1. In practice, the clients can distribute
the shares between the two servers, or encrypt the ﬁrst share
using the public key of S0, upload both the ﬁrst encrypted
share and the second plaintext share to S1. S1 then passes
the encrypted shares to S0 to decrypt. In our protocol, we
also let the coefﬁcients w be secret shared between the two
servers. It is initialized to random values simply by setting
(cid:7)w(cid:8)0,(cid:7)w(cid:8)1 to be random, without any communication between
the two servers. It is updated and remains secret shared after
each iteration of SGD, until the end when it is reconstructed.
(cid:2)d
As described in Section II-A, the update function for linear
k=1 xikwk − yi)xij, only
regression is wj
consisting of additions and multiplications. Therefore, we apply
the corresponding addition and multiplication algorithms for
secret shared values to update the coefﬁcients, which is (cid:7)wj(cid:8) :=
(cid:7)wj(cid:8) − αMul
. We
separate our protocol into two phases: online and ofﬂine. The
online phase trains the model given the data, while the ofﬂine
phase consists mainly of multiplication triplet generation. We
focus on the online phase in this section, and discuss the ofﬂine
phase in Section IV-B.
Vectorization in the Shared Setting. We also want to beneﬁt
from the mini-batch and vectorization techniques discussed in
Section II-A (see Equation 2). To achieve this, we generalize
the addition and multiplication operations on share values
A
to shared matrices. Matrices are shared by applying Shr
to every element. Given two shared matrices (cid:7)A(cid:8) and (cid:7)B(cid:8),
matrix addition can be computed non-interactively by letting
(cid:7)C(cid:8)i = (cid:7)A(cid:8)i + (cid:7)B(cid:8)i for i ∈ {0, 1}. To multiply two shared
matrices, instead of using independent multiplication triplets,
we take shared matrices (cid:7)U(cid:8),(cid:7)V(cid:8),(cid:7)Z(cid:8), where each element in
U and V is uniformly random in Z2l, U has the same dimension
as A, V has the same dimension as B and Z = U×V mod 2l.
Si computes (cid:7)E(cid:8)i = (cid:7)A(cid:8)i−(cid:7)U(cid:8)i, (cid:7)F(cid:8)i = (cid:7)B(cid:8)i−(cid:7)V(cid:8)i and sends
it to the other server. Both servers reconstruct E and F and set
(cid:7)C(cid:8)i = i · E × F + (cid:7)A(cid:8)i × F + E × (cid:7)B(cid:8)i + (cid:7)Z(cid:8)i. The idea of
this generalization is that each element in matrix A is always
masked by the same random element in U, while it is multiplied
by different elements in B in the matrix multiplication. Our
security proof conﬁrms that this does not affect security of the
protocol, but makes the protocol signiﬁcantly more efﬁcient
due to vectorization.
Applying the technique to linear
in each
iteration, we assume the set of mini-batch indices B
(cid:4)(cid:7)XB(cid:8),(cid:7)w(cid:8)(cid:5) − (cid:7)YB(cid:8)). We further ob-
:= (cid:7)w(cid:8) −
is public, and perform the update (cid:7)w(cid:8)
1|B| αMul
serve that one data sample will be used several times in
B(cid:8), Mul
regression,
((cid:7)XT
A
A
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:48 UTC from IEEE Xplore.  Restrictions apply. 
× V(cid:3)
[i] = UT
Bi
different epochs, yet it sufﬁces to mask it by the same random
multiplication triplet. Therefore, in the ofﬂine phase, one shared
n×d random matrix (cid:7)U(cid:8) is generated to mask the data samples
(cid:7)X(cid:8). At the beginning of the online phase, (cid:7)E(cid:8)i = (cid:7)X(cid:8)i −(cid:7)U(cid:8)i
is computed and exchanged to reconstruct E through one
interaction. After that, in each iteration, EB is selected and used
in the multiplication protocol, without any further computation
and communication. In particular, in the ofﬂine phase, a series
of min-batch indices B1, . . . , Bt are agreed upon by the two
servers. This only requires the knowledge of n, d, t or an
upperbound, but not any real data. Then the multiplication
triplets (cid:7)U(cid:8),(cid:7)V(cid:8),(cid:7)Z(cid:8),(cid:7)V(cid:3)(cid:8),(cid:7)Z(cid:3)(cid:8) are precomputed with the
following property: U is an n× d matrix to mask the data X, V
is a d × t matrix, each column of which is used to mask w in
is a |B| × t matrix
one iteration (forward propagation), and V(cid:3)
wherein each column is used to mask the difference vector
Y∗ − Y in one iteration (backward propagation). We then let
Z[i] = UBi × V[i] and Z(cid:3)
[i] for i = 1, . . . , t,
where M[i] denotes the ith column of the matrix M. Using
the multiplication triplets in matrix form, the computation and
communication in both the online and the ofﬂine phase are
reduced dramatically. We will analyze the cost later.
of these matrices in the ofﬂine phase by Fof f line.
Arithmetic Operations on Shared Decimal Numbers. As
discussed earlier, a major source of inefﬁciency in prior work
on privacy preserving linear regression stems from computing
on shared/encrypted decimal numbers. Prior solutions either
treat decimal numbers as integers and preserve full accuracy
after multiplication by using a very large ﬁnite ﬁeld [22], or
utilize 2PC for boolean circuits to perform ﬁxed-point [21]
or ﬂoating-point [35] multiplication on decimal numbers. The
former can only support a limited number of multiplications,
as the range of the result grows exponentially with the number
of multiplications. This is prohibitive for training where the
number of multiplications is large. The latter introduces high
overhead, as the boolean circuit for multiplying two l-bit
numbers has O(l2) gates, and such a circuit needs to be
computed in a 2PC (e.g. Yao’s garbled circuits) for each
multiplication performed.
We denote the ideal functionality realizing the generation
(cid:3)
(cid:3)
We propose a simple but effective solution to support
decimal arithmetics in an integer ﬁeld. Consider the ﬁxed-point
multiplication of two decimal numbers x and y with at most
lD bits in the fractional part. We ﬁrst transform the numbers to
= 2lD x and y
= 2lD y and then multiply
integers by letting x
(cid:3). Note that z has at most
(cid:3)
them to obtain the product z = x
y
2lD bits representing the fractional part of the product, so we
simply truncate the last lD bits of z such that it has at most lD
bits representing the fractional part. Mathematically speaking,
if z is decomposed into two parts z = z1 · 2lD + z2, where
0 ≤ z2 < 2lD, then the truncation results is z1. We denote this
truncation operations by (cid:11)z(cid:12).
We show that this truncation technique also works when z is
secret shared. In particular, the two servers can truncate their
individual shares of z independently. In the following theorem
we prove that for a large enough ﬁeld, these truncated shares
when reconstructed, with high probability, are at most 1 off
from the desired (cid:11)z(cid:12). In other words, we incur a small error
Bj
run Rec((cid:2)E(cid:3)0,(cid:2)E(cid:3)1) to obtain E.
Protocol SGD Linear((cid:2)X(cid:3),(cid:2)Y(cid:3),(cid:2)U(cid:3),(cid:2)V(cid:3),(cid:2)Z(cid:3),(cid:2)V(cid:2)(cid:3),(cid:2)Z(cid:2)(cid:3)):
1: Si computes (cid:2)E(cid:3)i = (cid:2)X(cid:3)i −(cid:2)U(cid:3)i for i ∈ {0, 1}. Then parties
2: for j = 1, . . . , t do
3:
4:
Parties select the mini-batch (cid:2)XBj(cid:3),(cid:2)YBj(cid:3).
Si computes (cid:2)Fj(cid:3)i = (cid:2)w(cid:3)i − (cid:2)V[j](cid:3) for i ∈ {0, 1}. Then
parties run Rec((cid:2)Fj(cid:3)0,(cid:2)Fj(cid:3)1) to recover Fj.
(cid:3)i = i·EBj ×Fi +(cid:2)XBj(cid:3)i×Fi +EBj ×
Si computes (cid:2)Y∗
(cid:2)w(cid:3)i + (cid:2)Zj(cid:3)i for i ∈ {0, 1}.
Si compute the difference (cid:2)DBj(cid:3)i = (cid:2)Y∗
(cid:3)i − (cid:2)YBj(cid:3)i for
i ∈ {0, 1}.
j(cid:3)i for i ∈ {0, 1}.
j(cid:3)i = (cid:2)DBj(cid:3)i − (cid:2)V(cid:2)
Si computes (cid:2)F(cid:2)
Parties then run Rec((cid:2)F(cid:2)
j(cid:3)1) to obtain F(cid:2)
j(cid:3)0,(cid:2)F(cid:2)
j.
Si computes (cid:2)Δ(cid:3)i = i· ET
× F(cid:2)
j +(cid:2)XT
×
(cid:3)i × F(cid:2)
j + ET