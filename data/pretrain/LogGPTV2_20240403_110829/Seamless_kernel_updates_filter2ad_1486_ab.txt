update procedure that allows taking a consistent checkpoint for all kernel updates, and
requires minimal programmer eort for applying these updates. Second, we perform a
detailed analysis of the eort needed to support updates across major kernel releases,
representing more than a year and a half of changes to the kernel. During this time,
six million lines of code were changed in 23,000 kernel les. We are not aware of any
system that provides such extensive support for kernel updates. Finally, we evaluate our
implementation and show that it works seamlessly for several, large, real-world applica-
tions, with no perceivable performance overhead, and without requiring any application
modications. The overall reboot time is reduced by a factor of 4 to 10 for several of
Chapter 1.
Introduction
5
these applications.
Section 2 discusses related work in this area. Section 3 presents our approach and
Section 4 describes the implementation of our system. Section 5 presents our analysis of
the programmer eort needed to use our system and evaluates the performance of the
system. Section 6 summarizes our work and provides conclusions.
Chapter 2
Related Work
Closest in goals to this work, Autopod [16, 15] uses a virtualization layer to decouple
processes from their dependencies on the underlying operating system. Virtualization
layer intercepts system calls and rewrites their inputs and outputs to enable consistency in
the resource identiers seen by the applications. The system calls are modied so that the
identiers they use, remain consistent on dierent machines after migration, to prevent
conicts with existing resources, and to isolate migrated processes. The virtualization
layer creates a POD abstraction which gives a group of processes a private namespace
for resource IDs and a private le systems.
The migration is performed by sending a SIGSTOP to all the processes in a POD to
stop them, and then making a copy of all their resources, their memory and their private
le systems. This copy is then transfered to another machine and the saved processes
are once again started in another POD at the destination.
Similar to our system, Autopod uses a checkpoint-restart mechanism, with a high-
level checkpoint format, for migrating processes across machines running dierent kernel
versions. The focus on migration has several consequences. First, virtualization intro-
duces performance overheads, and the virtualization layer itself needs to be maintained to
keep up with kernel changes. This layer is not needed in our system, designed purely for
6
Chapter 2. Related Work
7
kernel updates. Second, the checkpoint requires copying all memory pages, making the
checkpoint much larger than required in our system. Third, Autopod requires migrat-
ing state across machines, which may not be a viable option for desktop environments
and stateful servers such as databases running on large local disks. Furthermore, Au-
topod exposes applications to interrupted system calls, making applications susceptible
to crashes and data loss. By migrating state across machines that are already running
operating systems, Autopod does not seem to address quiescence issues such as caused
by non-interruptible system calls and interrupts. Finally, we evaluate our system across
major kernel updates and provide a detailed analysis of our checkpoint code and format,
and all the code changes required.
Otherworld [9] is designed to recover from kernel failures. In addition to the main
kernel it maintains a secondary crash kernel. When a fault is detected in the main
kernel, execution transfers to the crash kernel. The crash kernel initializes itself and then
examines the data structures of the main kernel. Using the main kernel's data structures,
the crash kernel attempts recover the state of the running applications and to resurrect
them, so they can function after the crash.
The design of Otherworld has inspired this work, but our goals are dierent, namely
applying kernel updates reliably. After a kernel failure, Otherworld does not have the lib-
erty to achieve quiescence. For example, a thread may have acquired a lock and partially
updated a critical kernel data structure when a crash occurs, making the checkpoint and
resurrection process failure prone. Similarly, as we show later, restarting system calls
transparently without achieving quiescence is impractical. Given that the kernel has
crashed, a best-eort resurrection process is acceptable, while our aim is to make the up-
date process as reliable as possible. Also, we support sharing of kernel resources between
threads, do not require any modications to applications, and evaluate the feasibility of
the approach for kernel updates.
Several researchers have proposed dynamic patching at function granularity for ap-
Chapter 2. Related Work
8
plying kernel updates [6, 13, 3]. LUCOS [6] uses the Xen VMM to stop and dynamically
update functions in Linux. When a kernel data structure is updated, LUCOS maintains
both versions in memory, until it can determine that the old version is no longer in use.
This quiescence step requiring walking the stack of every kernel thread looking for up-
dated code. However, this assumes that code that accesses an updated data structure
must have been updated. Programmers have to analyze a Linux patch, decide whether
any data structures have been updated, and then write a LUCOS-specic state transfer
function, a tedious engineering eort [6]. LUCOS uses page protection and the transfer
function to maintain coherence between the data structure versions.
DynAMOS [13] patches functions, similar to LUCOS, after disabling interrupts. Dy-
nAMOS supports updates to non-quiescent functions as well as changes to the data
structures. Non-quiescent functions are updated via a series of transformations to the
code which allows both old and new versions of the same function to exist at the same
time. As the old instances terminate they gradually get replaced by the new updated
code. Adding new elds to data structures are done using shadow structures. New eld
is added to the shadow structure which is maintained along side the original data struc-
tures, and all the users of the new eld are changed to access the shadow structure.
Kernel threads are updated when they enter a sleep state. When a thread enters sleep it
is terminated and the new threads running updated code is started instead.
Ksplice [3] generates and applies binary function patches for security updates, while
requiring minimal or no programmer eort. It inspects binary object les to detect the
changes to the code and automatically generate patches from those changes. To apply
the patch it overwrites the memory if the running kernel to insert new updated functions
and remove old ones. Security patches tend to be localized, generally only changing code,
and hence the approach works well for this domain.
At the application level, several systems use compiler support for updating pro-
grams [7, 14]. Polus [7] nds all changed types, global variables and modied functions
Chapter 2. Related Work
9
by comparing the syntax tree of both the old and the new versions of les, and builds re-
lations between the changed variables and functions to generate patches. Then, it applies
techniques similar to LUCOS for patch injection. Ginseng [14] introduces sucient room
in data types to update them with new elds in the future. It also adds indirections for
types and functions, to provide type-safe updates for C code. Using these techniques for
kernels is challenging because of the frequent use of type unsafe code, including assem-
bly code, as well as optimized and inline functions for which it is hard to ensure safety
guarantees. For example, Polus is unable to generate patches correctly in the presence
of pointer aliases and void pointer casts.
The K42 operating system is designed to allow hot swapping and updating compo-
nents at the object and module granularity [17]. In K42 function calls are made through
an indirection table. Updating the entry in the indirection table allows all the callers
to automatically start using the new version of the function. Issues with quiescence are
handled by restructuring the kernel so that all kernel threads handle requests quickly in a
non-blocking manner. By restructuring the kernel K42 supports data structure updates
within modules as well as updates to module interfaces [4]. However, it cannot update
code outside its object system including low-level exception-handling code, parts of the
scheduler, and its message-passing IPC mechanism. More importantly, state transfer
functions need to be written manually for the updated objects and modules.
Updating the entire kernel has several other advantages over dynamic patching and
hot swapping systems, including handling updates to boot code and non-quiescent code
such as long-running kernel threads, and xing non-critical failures such as memory leaks.
Several systems use checkpoints for recovering from failures in applications or ker-
nel components. Linux-CR [11] aims to add a general-purpose checkpoint and restart
mechanism to the Linux kernel. It relies on the Linux containers to isolate the processes
that are meant to be checkpointed from others running on the system, and give them
a private namespace for resource identiers. To stop the processes in a quiescent state
Chapter 2. Related Work
10
Linux-CR uses freezer control groups. Freezer control allows to stop all the processes in
the control group in the same way as sending them the SIGSTOP signal, but silently,
without making signal visible to the applications. The Linux-CR addresses the diculty
of keeping the checkpoint code synchronized with kernel developments by keeping the
generic code in its own subsystem and the code to save specic object types close to
the native code for those objects. While it addresses the diculty of maintaining the
checkpoint code, it does not attempt to tackle the problem of migrating applications to
new versions of the kernel.
CuriOS [8] recovers a failed service transparently to clients in a microkernel OS by
isolating and checkpointing the client state associated with the service. Membrane [18]
restarts failed le systems transparent to applications by using a lightweight logging and
checkpoint mechanism.
Complementary to this work, virtual machines can be used to speed the kernel reboot
process by running the existing and the updated kernel together [12]. Primary-backup
schemes can be used for rolling kernel upgrades in high availability environments, but
they require application-specic support [1, 2].
Chapter 3
Approach
Our goal is to perform seamless kernel updates, without requiring user intervention or
any changes to applications. Figure 3.1 shows the timeline of events for regular updates
and seamless updates. During a regular update, applications are closed manually after
saving work, the kernel is rebooted, and then applications need to be restarted manually.
Our seamless update approach is based on treating the entire kernel as a replaceable
component [9].
Our update system operates in ve steps as shown below and in Figure 3.1.
1. Load new kernel: When a kernel update arrives, we use the kexec facility in Linux
to load the new kernel image and to start executing it. We modied kexec so that
it performs the next two steps before starting the new kernel.
2. Wait for quiescence: We ensure that the kernel reaches quiescence as described in
Section 3.2.
3. Save checkpoint: The checkpoint code walks the kernel data structures associated
with application-visible state and converts them to a high-level format that is in-
dependent of the kernel version.
4. Initialize new kernel: The kexec jumps execution to the beginning of the new kernel,
11
Chapter 3. Approach
12
Figure 3.1: Timeline for regular and seamless kernel update
and the new kernel initializes itself.
5. Restore checkpoint: After kexec has initialized the new kernel, it reads the check-
point and recreates applications using the checkpoint information. Then it restarts
these applications which may require restarting blocked system calls as described
in Section 3.3.
3.1
Implementation Overview
Our update system checkpoints and restores processes by capturing application-visible
kernel state from kernel data structures. The Linux kernel stores all process related in-
UpdatearrivesStart newkernelNew kernel finishes bootingClose applicationsmanuallyLoad & initialize new kernelRestartapplicationsmanuallyRegular kernel updateSeamless kernel updateRestorecheckpointStartcheckpointLoad newkernelUpdatearrivesInitializenew kernelNew kernelfinishes bootingStart newkernelSavecheckpointWait forquiescenceChapter 3. Approach
13
formation in the task_struct data structure. This structure contains process information
such as the PID of the process, the parent and the children of the process, scheduling
parameters and accounting information. The task_structure contains pointers to other
data structures that describe the resources currently being used by the process, such
as memory management information, open les, etc. Thus the state of a process can
be checkpointed by traversing the graph rooted at the task_struct associated with the
process. Our checkpoint typically saves the elds in the data structures that are visible
to applications through system calls. These elds also allow us to restore these data
structures during the restore process.
The checkpoint information and the memory pages of all the processes need to be
preserved during the reboot process. When the Linux kernel starts executing, it uses
a bootstrap memory manager to dynamically allocate memory that is needed during
the boot process before the memory management system has been initialized. After the
bootstrap memory manager is initialized, we read the checkpoint and mark all the pages
used by the checkpoint and the process pages as reserved so that the memory manager
cannot immediately reuse these pages. After the restore operation, the process pages are
marked as allocated and can be freed when a process terminates or as a result of demand
paging.
When multiple processes share a resource, eg. a memory region, they keep pointers
to the same structure, e.g., a memory region descriptor. We implement this sharing by
saving each resource separately in the checkpoint, and using pointers within the check-
point to indicate the sharing. The implementation tracks each encountered resource in a
Save hash table. The key of this hash table is the memory address of the kernel resource,
and the value is the memory address of the corresponding entry in the checkpoint. When
checkpointing any resource, we check if its address exists in the Save hash table, and if
so, we use the value in the Save hash table to create a pointer to the existing checkpoint
entry. During restore, we create a Restore hash table with keys that are the values from
Chapter 3. Approach
14
the Save hash table. As each resource is restored, its memory address is lled in as the
value in the Restore hash table. Looking up the Restore hash table as each resource is
created ensures that the resource sharing relationships are setup correctly.
The restore process runs with root privileges and hence care must be exercised when
restoring the state of the OS resources. The kernel uses two types of credentials, one set
for processes and another for les. We set the the various user and group IDs for each
restored process thus ensuring that the restored process runs with the correct credentials.
The restore process does not create les and hence we do not need to set up or modify any
le credentials. However, there is one exception to this rule with listening Unix sockets
which we describe later.
Our implementation currently checkpoints the following OS resources: 1) thread state,
2) memory state, 3) open les, 4) network sockets, 5) pipes, 6) Unix sockets, and 7) ter-
minals. It also checkpoints the state of the following simple hardware devices: 1) frame-
buer, 2) mouse, and 3) keyboard. Our aim has been to implement features that enable
supporting as many commonly used applications as possible, and especially server-side
applications. While we have tested several simple desktop programs using the Xfbdev X
server, adding support for live driver updates would make our approach more compelling
for updating device-rich client machines [19].
3.2 Quiescence
Ensuring that our system will restore applications reliably requires taking a consistent
checkpoint, with two conditions: 1) all threads are stopped, and 2) the kernel data
structures are consistent. The rst ensures that thread state remains consistent with its
checkpoint state. For example, if a thread continues execution during or after checkpoint-
ing, it can aect the state of other threads with which it shares any resources. When
both these conditions are met, we say that the kernel is quiescent, and a checkpoint is
Chapter 3. Approach
15
taken.
The rst condition can be met easily by pausing all processors other than the one
running the checkpoint thread.1 For the second condition, data structures can be incon-
sistent when any kernel code is executing, including in system calls, exception handlers
and interrupt handlers. We need to let all kernel code nish executing and stop further
entry into the kernel. However, system calls and exception handlers can block or sleep
while waiting for events. A thread can sleep in one of two states, uninterruptible sleep
and interruptible sleep, in the Linux kernel. During an uninterruptible sleep, the thread
can hold locks and modify data structures, so we need to let this code continue executing.
Fortunately, an uninterruptible sleep is used where an operation is expected to take a
relatively short time, such as a disk access for paging and memory allocation. Very long
uninterruptible sleep is considered a bug and we do not expect to encounter it during
normal operation. Recovery from this type of failure is beyond the scope of our project.
Waiting for this code to nish executing does not signicantly impact the overall update
time because it is much faster than the time needed to initialize the new kernel.
A thread in an interruptible sleep can block indenitely, e.g., waiting for user or