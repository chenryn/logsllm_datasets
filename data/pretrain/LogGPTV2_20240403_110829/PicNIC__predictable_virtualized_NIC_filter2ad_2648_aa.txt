title:PicNIC: predictable virtualized NIC
author:Praveen Kumar and
Nandita Dukkipati and
Nathan Lewis and
Yi Cui and
Yaogong Wang and
Chonggang Li and
Valas Valancius and
Jake Adriaens and
Steve Gribble and
Nate Foster and
Amin Vahdat
PicNIC: Predictable Virtualized NIC
Praveen Kumar† Nandita Dukkipati∗ Nathan Lewis∗
Yi Cui∗
Yaogong Wang∗
Chonggang Li∗ Valas Valancius∗
Jake Adriaens∗ Steve Gribble∗ Nate Foster† Amin Vahdat∗
†Cornell University
∗Google
Abstract
Network virtualization stacks are the linchpins of public clouds. A
key goal is to provide performance isolation so that workloads on
one Virtual Machine (VM) do not adversely impact the network
experience of another VM. Using data from a major public cloud
provider, we systematically characterize how performance isolation
can break in current virtualization stacks and find a fundamental
tradeoff between isolation and resource multiplexing for efficiency.
In order to provide predictable performance, we propose a new
system called PicNIC that shares resources efficiently in the com-
mon case while rapidly reacting to ensure isolation. PicNIC builds
on three constructs to quickly detect isolation breakdown and to
enforce it when necessary: CPU-fair weighted fair queues at re-
ceivers, receiver-driven congestion control for backpressure, and
sender-side admission control with shaping. Based on an extensive
evaluation, we show that this combination ensures isolation for
VMs at sub-millisecond timescales with negligible overhead.
CCS Concepts
• Networks → Transport protocols; Network algorithms; Network
reliability; Cloud computing.
Keywords
Congestion Control, Performance Isolation
ACM Reference Format:
Praveen Kumar, Nandita Dukkipati, Nathan Lewis, Yi Cui, Yaogong Wang,
Chonggang Li, Valas Valancius, Jake Adriaens, Steve Gribble, Nate Foster,
and Amin Vahdat. 2019. PicNIC: Predictable Virtualized NIC. In SIGCOMM
’19: 2019 Conference of the ACM Special Interest Group on Data Communica-
tion, August 19–23, 2019, Beijing, China. ACM, New York, NY, USA, 16 pages.
https://doi.org/10.1145/3341302.3342093
1 Introduction
There is a fundamental tension between efficiency and predictable
performance in any shared computing platform. On the one hand,
providers want to utilize resources efficiently by oversubscribing
the infrastructure to achieve economies of scale. On the other hand,
the tenants using these platforms want predictable performance
without worrying about interference from other tenants. While
this issue has been a long-standing problem across a variety of
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5956-6/19/08. . . $15.00
https://doi.org/10.1145/3341302.3342093
platforms, it is substantially exacerbated in today’s public clouds.
Whereas in private clouds, resources can be provisioned in a col-
laborative fashion between providers and tenants to balance the
tradeoff between predictability and efficiency, in public clouds the
provider typically lacks visibility into tenant applications, so they
must compensate with costly overprovisioning to tilt the default in
favor of performance isolation [33, 49].
Most prior work in the networking context has focused on shar-
ing bandwidth in the network fabric (Table 1). However, in this
work, we find that contention for resources in the host virtualiza-
tion stack is often a key contributor to unpredictable performance,
and that existing mechanisms are insufficient for guaranteeing iso-
lation at end hosts. For instance, if one VM receives small packets
at a high rate—e.g., due to a denial of service (DoS) attack—other
VMs on the same host may observe spikes in latency or even packet
loss due to isolation breakage at the host virtualization stack. Gen-
erally speaking, there are two resources on the hosts that must be
shared among VMs: (i) the bandwidth (BPS) and (ii) the rate (PPS)
at which packets can be transmitted and received. Unfortunately,
current stacks do not provide adequate mechanisms for sharing
these resources across multiple senders and receivers, which means
that the behavior of one VM can adversely impact others.
This paper argues that cloud providers should offer tenants the
abstraction of a Predictable Virtualized NIC—i.e., performance guar-
antees formulated as per-VM SLOs in terms of measurable bounds
on bandwidth, latency percentiles, and loss rates. This notion of
predictable performance generalizes stricter notions of isolation
that are impossible to achieve without excessive overprovision-
ing [49]. In particular, although predictable performance does not
mandate strict non-interference at the network level, tenants can
still use bounds on performance metrics to reason effectively about
the service they will receive.
Of course, achieving predictable performance without sacrificing
efficiency is inherently difficult. The fundamental challenge stems
from the fact that on end hosts the resources required to process
packets depend on the overall traffic mix, which is hard to predict
in advance. Overprovisioning for the worst case sacrifices efficiency
gains due to multiplexing, while underprovisioning risks violating
SLOs. The approach we take is to initially provision resources for
efficient sharing, under the optimistic assumption that VMs will be
well-behaved, but monitor the system and rapidly adapt when condi-
tions change, falling back to strict isolation as the safe default. While
this approach lacks some attractive properties—e.g., it is not always
work-conserving—and requires a distributed implementation, it
does provide predictable performance and uses resources efficiently
in the common case.
We realize these ideas in PicNIC, a system that provides the
Predictable Virtualized NIC abstraction in a shared public cloud
environment. With PicNIC, each VM is guaranteed a minimum
351
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
System
Abstraction
Shared
capacity
Bandwidth
isolation model
Host stack
isolation
Predictable
latency
SeaWall [69], NetShare [39], FairCloud (PS-L/N) [62]
Virtualized Fabric Constant
Fair sharing
Oktopus [7], Hadrian [8], SecondNet [28], Proteus [76],
Pulsar [5], CloudMirror [41]
Gatekeeper [66], FairCloud (PS-P) [62], EyeQ [35], Elas-
ticSwitch [63], HUG [14]
Virtualized Fabric Constant Hose-based [18]
(VC, TIVC, TAG, pipe)
Virtualized Fabric Constant Hose
Silo [34]
PicNIC
Virtualized Fabric Constant Hose
Virtualized NIC
Variable
Hose
No
No
No
No
Yes
None
None
None
Fabric
vNIC
Table 1: Related systems focus on sharing constant fabric bandwidth and cannot provide isolation at the end-hosts. In contrast, PicNIC provides isolation at
the end-hosts by sharing the virtualization stack’s variable packet-processing capacity and delivers predictable performance in terms of bandwidth, latency
distribution and loss rate for each VM.
and maximum bandwidth envelope, bounded latency distribution,
and near-zero packet loss within the stack. To achieve these goals,
PicNIC leverages two key insights as design principles: (i) SLO-based
resource sharing: for predictability, packet-processing resources
utilized for each VM should be proportional to performance SLOs.
In particular, sufficient resources within the virtualization stack (e.g.,
CPU cycles, NIC and PCIe bandwidth) should be allocated to ensure
the minimum guarantees of the SLO (e.g., bandwidth) independent
of the behavior of other VMs. (ii) Backpressure and early drops: for
efficiency, if a packet needs to be dropped (or queued), it should be
done as early in the processing pipeline as possible by applying apt
backpressure—e.g., packets likely to be dropped at the receiver due
to insufficient resources should not be admitted at the source.
Note that existing approaches are not strong enough to ensure
these guarantees for three reasons. First, prior work (Table 1) fo-
cuses on apportioning bandwidth in the fabric, whereas PicNIC also
offers non-trivial guarantees on latency and packet loss. Second,
effectively managing resources in virtualization stacks requires a
different approach due to variable BPS and PPS capacities for packet
processing at end hosts. Third, whereas prior work often relies on
rate limiting at sources, this itself can cause a breakdown of isola-
tion at end hosts. Avoiding such breakdowns requires rethinking
traffic shaping and extending backpressure mechanisms across a
complete chain—the fabric, within the host stack, and between the
host stack and the VM.
Contributions. Our main finding is that it is possible to ensure pre-
dictable performance by quickly navigating the spectrum between
being resource-efficient (work-conserving) and providing strict iso-
lation. Our approach quickly detects risks of SLO violations and
tilts the tradeoff towards isolation. To this end, the paper makes
the following contributions.
1. Analyzing data from a production environment, we systemati-
cally identify key bottlenecks in network virtualization stacks
that lead to isolation breakages. We provide insights into new
challenges, such as variable packet processing capacity of the
stack and limitations of traffic shaping, that make it challenging
to ensure predictable performance (§2).
2. We propose an intuitive abstraction for a predictable virtual-
ized NIC which gives a well-defined quantifiable meaning to
predictable performance (§4). While it enables high efficiency
in the common case, it also prioritizes isolation when there is a
risk of SLO violation.
3. To realize this abstraction in a system, we identify key design
principles based on SLO-based resource sharing, admission con-
trol and backpressure (§3) and present the design and implemen-
tation of such a system, PicNIC, using a combination of local
and end-to-end constructs (§5).
4. Our evaluation on a large-scale cloud deployment shows that
PicNIC can ensure predictable performance without sacrificing
efficiency (§6).
Lessons from production. Using data from a major cloud provider,
we found that isolation can break down at both sender and receiver
host stacks. We found it more efficient and practical to enable
parts of PicNIC to react to isolation breakdowns based on signals
rather than designing a system that attempts to enforce isolation
invariants. Implementing these reactions at receivers turns out
to be particularly difficult as it requires coordination with multi-
ple senders. Moreover, such coordination must be done at short
timescales, which poses additional practical challenges. While tra-
ditional congestion control works by sending acknowledgements,
maintaining a similar level of state and generating packets to carry
congestion signals would impose significant overheads in a virtu-
alization stack. Ultimately, selecting a design to reduce reaction
time to O(ms) without regressing the data path required several
iterations. In the same vein, using traffic shaping as a building
block required several other features to be implemented in the data
path including buffering, backpressure to VMs, and out-of-order
completions to avoid head-of-line blocking.
Ethical concerns. This work does not raise any ethical issues.
2 Cause and Cost of Unpredictability
Network virtualization provides the abstraction of a private net-
work to cloud tenants while sharing the underlying physical net-
work [37, 60]. Fig. 1 shows a simplified view of the on-host com-
ponent of a typical network virtualization stack with egress 2(cid:2)
and ingress 5(cid:2) engines that process (e.g., encapsulate/decapuslate,
apply firewall rules, etc.), buffer, and transport packets between
the VMs and the NIC within a host. These components can be
realized in various ways on different implementations—e.g., An-
dromeda uses a modular software switch with fast shared engines
and hardware offload [16] while Azure’s Virtual Filtering Platform
352
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
VM
...
VM
2(cid:2)
7(cid:2)
Egress
Processing
8(cid:2)
r
e
p
a
h
S
Fastpath
I
x
T
C
N
3(cid:2)
c
i
r
b
a
F
I
x
R
C
N
4(cid:2)
5(cid:2)
Ingress
Processing
VM
...
VM
Egress Engine (SW+HW)
Ingress Engine (SW+HW)
6(cid:2)
1(cid:2)
Figure 1: Overview of an on-host network virtualization stack.
Host
Host
uses Generic Flow Tables implemented in hardware and FPGAs to
accelerate other network functions [21, 22]. Changing the division
of labor between software and hardware can reduce but does not
fully eliminate isolation issues—see §8.
Based on incidents observed in production, we find that the sever-
ity and frequency of isolation breakages in current virtualization
stacks are significant enough to motivate a systematic characteriza-
tion. Accordingly, in this work, we focus on issues at the end-host
and assume that the network fabric is not a bottleneck. Our model
for a packet’s path is: VM Tx 1(cid:2) → egress engine 2(cid:2) → NIC egress
3(cid:2) → NIC ingress 4(cid:2) → ingress engine 5(cid:2) → VM Rx 6(cid:2). At the
egress, packets may be buffered for shaping 8(cid:2) based on policies
(e.g., WAN bandwidth allocation [38]) or to ensure that transmit
completions (§B) are returned to a VM in-order 7(cid:2).
A key factor which makes predictable performance difficult to
achieve is that per-packet processing costs are not constant, whether
in software or hardware. For instance, cache misses affect the num-
ber of CPU cycles needed to process a packet in software, while
the PCIe and DRAM bandwidth required per packet varies in hard-
ware [45, 53]. These costs also depend on the complexity of opera-
tions that must be performed on each packet [55, 58] as well as the
overall traffic mix. Thus, the packet processing capacity of a virtu-
alization stack is variable, unlike a link’s capacity in the fabric, and
makes it difficult to directly extend prior work on fabric bandwidth
isolation (Table 1) to this case. While our observations apply to a
broad range of implementations (§8), for concreteness, in the rest
of this paper we consider a kernel-bypass software virtualization
stack with hardware offloads [16].
In general, isolation can break due to contention for any shared
resource: egress engine processing capacity 2(cid:2), buffering in egress
engine 7(cid:2) & 8(cid:2), egress NIC buffers 3(cid:2), ingress NIC buffers 4(cid:2), and
ingress engine processing capacity 5(cid:2). Through careful provision-
ing, it is possible to largely avoid issues at egress engine 2(cid:2) and NIC
buffers 3(cid:2). For instance, we can throttle the egress engine to ensure
that egress NIC buffers do not overflow. However, there remain
three key points where isolation can break: egress buffering 7(cid:2) &
8(cid:2), ingress NIC 4(cid:2) and ingress engine 5(cid:2). We walk through each of
these along with examples of isolation breakages from production.
2.1 Egress Buffer Contention
At the egress, contention for shared buffers can break isolation
between VMs. Fig. 2 shows a scenario recreated from a production
incident, with three VMs on separate hosts. VM1 → VM2 is an
unthrottled (or “fastpath”) TCP flow. While fastpath packets are
expected to pass through the stack without delay, this expectation
may be violated in the presence of throttled flows, which must
be shaped. For example, at t = 12s, VM1 starts a 30s long UDP
flow to VM3, which needs to be rate limited to 10 Mbps per policy;
the queueing delay for UDP packets is shown as their sojourn
d
e
l
tt
o
r
h
T
d
e
l
tt
o
r
h
t
n
U
P
D
U
P
C
T
n
r
u
o
j
o
S
)
s
(
e
m
i
t
L
o
H
y