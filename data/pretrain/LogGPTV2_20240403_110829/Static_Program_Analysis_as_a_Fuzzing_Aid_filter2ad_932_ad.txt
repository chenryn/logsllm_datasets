(a) tcpdump
Feb 15
Feb 17
Feb 19
Feb 21
Feb 23
Feb 25
(b) nDPI
Fig. 3. Test coverage as a function of time for tcpdump (a), and nDPI (b), for diﬀerent
fuzzing conﬁgurations. Program coverage measurements were made only when there
was a change in its magnitude.
Vulnerabilities Exposed. Table 4 shows the number of vulnerabilities exposed
in nDPI, and tcpdump, across all fuzzing conﬁgurations. In the case of tcpdump,
the positive impact of the Orthrus generated dictionary is evident. aﬂ, and aﬂ-
Orthrus, exposed 15, and 26 unique vulnerabilities respectively. 10 out of the 11
additional vulnerabilities exposed by aﬂ-Orthrus, were exclusively found by it,
i.e., it exposed 10 vulnerabilities in tcpdump not found by stand-alone aﬂ. aﬂfast,
and aﬂfast-Orthrus conﬁgurations exposed 1 and 5 vulnerabilities respectively.
aﬂfast-Orthrus exposed 4 vulnerabilities that were not exposed by stand-alone
aﬂfast. In the case of nDPI, aﬂ-Orthrus exposed 4 vulnerabilities that were not
found by stand-alone aﬂ, while aﬂfast-Orthrus exposed 1 such vulnerability. For
both nDPI, and tcpdump, aﬂfast-Orthrus ﬁnds fewer number of vulnerabilities
overall in comparison to its baseline. We conjecture that the fuzz schedule alter-
ations carried out in aﬂfast [3] inﬂuence the scheduling of dictionary-mutations,
resulting in the observed drop.
Table 5 documents those vulnerabilities found using Orthrus generated dic-
tionaries that were not found by stand-alone fuzzing of tcpdump, and nDPI.
The number of exposed vulnerabilities that may be exclusively attributed to
Orthrus are 10, and 5, for tcpdump, and nDPI respectively. Overall, Orthrus
42
B. Shastry et al.
Table 4. Number of bugs and vulnerabilities exposed by diﬀerent fuzzing conﬁgura-
tions. For Orthrus-based fuzzer conﬁgurations, the number of bugs exclusively found
by them is shown in brackets.
Software aﬂ aﬂ-orthrus aﬂfast aﬂfast-orthrus Peach-analyzer
tcpdump 15 26 (+10)
nDPI
26 27 (+4)
1
24
5 (+4)
17 (+1)
0
0
generated dictionaries exposed vulnerabilities in 14 diﬀerent network protocols
across the two codebases. Some of the exposed vulnerabilities are in the process-
ing of proprietary protocol messages such as the Viber protocol. All the exposed
vulnerabilities resulted in buﬀer overﬂows, and were immediately reported to the
respective vendors. These results are a testament to the eﬃcacy of our approach
in increasing the breadth of testing for complex network applications without
requiring domain-speciﬁc knowledge.
Table 5. Vulnerabilities exposed exclusively using Orthrus generated dictionaries in aﬂ,
and aﬂfast, for tcpdump, and nDPI. All the vulnerabilities result in a buﬀer overﬂow.
Number in square brackets indicates the number of vulnerabilities found.
Software Vulnerable component
tcpdump IPv6 DHCP packet printer
IPv6 Open Shortest Path First (OSPFv3) packet printer
IEEE 802.1ab Link Layer Discovery Protocol (LLDP) packet printer
ISO CLNS, ESIS, and ISIS packet printers [2]
IP packet printer
ISA and Key Management Protocol (ISAKMP) printer
IPv6 Internet Control Message Protocol (ICMPv6) printer
Point to Point Protocol (PPP) printer
White Board Protocol printer
nDPI
ZeroMQ Message Transport Protocol processor
Viber protocol processor
Syslog protocol processor
Ubiquity UBNT AirControl 2 protocol processor
HTTP protocol processor
Preliminary Results for Snort++. We used Orthrus to perform dictionary-
based fuzzing of snort++, a C++ implementation of the popular snort IDS. Base-
line fuzzing with aﬂ-fuzz helped ﬁnd a single vulnerability (CVE-2017-6658) in
the snort++ decoder implementation. In contrast, the Orthrus generated dictio-
nary has helped ﬁnd an additional vulnerability (CVE-2017-6657) in the LLC
packet decoder implementation of snort++ [31].
Static Program Analysis as a Fuzzing Aid
43
4.4 Limitations
Although our evaluations show that static analysis guided fuzzing is beneﬁcial,
our positive results may not generalize to other parsing applications. However,
our evaluation comprising six diﬀerent parser implementations provides strong
evidence that our approach can make fuzz testing more eﬀective. Automatically
generated parsers (e.g., yacc-based parsers) may contain code that is structurally
diﬀerent than hand-written parsers that we have evaluated. We believe that their
analysis may be carried out at the speciﬁcation level than at the source code level.
Furthermore, we make use of simple heuristics to infer input message fragments
from source code. Thus, our analysis may miss legitimate input fragments (false
negatives), and/or add irrelevant tokens to the input dictionary (false positives).
However, we take practical measures to keep the number of false positives/neg-
atives low. For example, our design incorporates practical security advice given
by reputed institutes such as CERT [5] that have been compiled over years of
source code audits. In our case study, we make use of a small (yet relevant) seed
set to bootstrap fuzzing. It is possible that a diverse seed set improves the per-
formance of our baseline fuzzers. Having said that, we have carefully analyzed
the additional coverage achieved solely through the use of the supplied dictio-
nary to ensure that the presented increments can be attributed to our method.
In addition, we have manually triaged all vulnerabilities found exclusively using
dictionary-based fuzzing to ensure causality, i.e., they were ultimately exposed
due to the use of speciﬁc tokens in the supplied dictionary.
5 Related Work
Multiple techniques have been proposed to improve the eﬀectiveness of fuzzing.
For our discussion of related work, we focus on approaches that infer the pro-
tocol speciﬁcation, use grammar-based fuzzing, or query-driven static analysis
approaches.
Inferring Protocol Speciﬁcation. There are two problems underlying pro-
tocol speciﬁcation inference: Inferring the protocol (i) Message format; and (ii)
State machine. Prior work, with the exception of Prospex [7] has focused solely
on the message format inference problem. Broadly, two approaches have been
proposed to automatically infer the protocol speciﬁcation. The ﬁrst approach
relies entirely on network traces for performing the inference, exempliﬁed by
the tool Discoverer [8]. As other researchers have noted, the main problem with
this approach is that network traces contain little semantic information, such
as the relation between ﬁelds in a message. Therefore, inference based entirely
on network traces is often limited to a simple description of the message for-
mat that is an under-approximation of the original speciﬁcation. The second
approach, also a pre-dominant one, is to employ dynamic program analysis in
a setting where the network application processes sample messages, in order to
infer the protocol speciﬁcation. Proposals such as Polyglot [4], Tupni [9], Aut-
oformat [19], Prospex [7], and the tool by Wondracek et al. [32] fall into this
44
B. Shastry et al.
category. In comparison to our work, these proposals have two shortcomings.
First, they require dynamic instrumentation systems that are often proprietary
or simply inaccessible. Dynamic instrumentation and analysis often requires soft-
ware expertise, making it challenging for auditing third-party code. In contrast,
we show that our analysis can be bundled into an existing compiler toolchain
so that performing protocol inference is as simple as compiling the underlying
source code. Second, prior work with the exception of Prospex, have not speciﬁ-
cally evaluated the impact of their inference on the eﬀectiveness of fuzz testing.
Although Comparetti et al. [7] evaluate their tool Prospex in conjunction with
the Peach fuzzer, their evaluation is limited to ﬁnding known vulnerabilities in
controlled scenarios. In contrast to these studies, we extensively evaluate the
impact our inference on the eﬀectiveness of fuzzing, both quantitatively in terms
of test coverage achieved, and time to vulnerability exposure, and qualitatively
in terms of an analysis of vulnerabilities exclusively exposed using our inference
in real-world code.
Grammar-Based Fuzzing. Godefroid et al. [13] design a software testing tool
in which symbolic execution is applied to generate grammar-aware test inputs.
The authors evaluate their tool against the IE7 JavaScript interpreter and ﬁnd
that grammar-based testing increases test coverage from 53% to 81%. Although
their techniques are promising, their work suﬀers from three practical diﬃcul-
ties. First, a manual grammar speciﬁcation is required for their technique to
be applied. Second, the infrastructure to perform symbolic execution at their
scale is not publicly available, rendering their techniques inapplicable to third-
party code. Third, their approach requires non-trivial code annotations, requir-
ing a close co-operation between testers and developers, something that might
not always be feasible. In contrast, we solve these challenges by automatically
inferring input data formats from the source code. Indeed, we show that more
lightweight analysis techniques can substantially beneﬁt modern fuzzers. Lang-
fuzz [16] uses a grammar speciﬁcation of the JavaScript and PHP languages
to eﬀectively conduct security assessments on the respective interpreters. Like
Godefroid et al., the authors of Langfuzz demonstrate that, in scenarios where a
grammar speciﬁcation can be obtained, speciﬁcation based fuzzing is superior to
random testing. However, creating such grammar speciﬁcations for complex net-
work applications manually is a daunting task. Indeed, network protocol speciﬁ-
cations (unlike computing languages) are speciﬁed only semi-formally, requiring
protocol implementors to hand-write parsers instead of generating them from a
parser generator. Such practical diﬃculties make grammar (speciﬁcation) based
fuzzing challenging for network applications.
Query Based Program Analysis. Our static analysis approach is inspired
by prior work on the use of queries to conduct speciﬁc program analyses by
Lam et al. [18], and automatic inference of search patterns for discovering taint-
style vulnerabilities from source code by Yamaguchi et al. [33]. At their core,
both these works use a notion of program queries to elicit vulnerable code pat-
terns from source code. While Lam et al. leverage datalog queries for analysis,
Yamaguchi et al. employ so called graph traversals. In contrast to their work, we
Static Program Analysis as a Fuzzing Aid
45
leverage query-driven analysis toward supporting a fuzzer instead of attempting
static vulnerability discovery.
6 Conclusions and Future Work
In this paper, we demonstrate how static analysis guided fuzzing can improve
the eﬀectiveness of modern oﬀ-the-shelf fuzzers, especially for networking appli-
cations. Code patterns indicate how user input is processed by the program. We
leverage this insight for gathering input fragments directly from source code.
To this end, we couple a static analyzer to a fuzzer via an existing interface.
Using input dictionaries derived from semantic and syntactic program analysis
queries, we are able to not only increase the test coverage of applications by
10–15%, but also reduce the time needed to expose vulnerabilities by an order of
magnitude in comparison to fuzzers not supplied with an input dictionary. We
leverage our research prototype to fuzz two high-proﬁle network applications,
namely, nDPI, a deep packet inspection library, and tcpdump, a network packet
analyzer. We ﬁnd 10 zero-day vulnerabilities in tcpdump, and 5 zero-day vul-
nerabilities in nDPI that were missed by stand-alone fuzzers. These results show
that our approach holds promise for making security assessments more eﬀective.
Our work highlights the need for a stronger interaction between program
analysis and testing. Although our study describes one way in which program
analysis can enhance fuzzing, exploiting their reciprocal nature poses some inter-
esting problems such as directing static analysis on code portions that have not
been fuzzed. This is one avenue for future work. A logical follow up of our work
will be to infer the protocol state machine in addition to its message format,
and leverage the additional insight for conducting stateful fuzzing. Leveraging
our inference algorithm toward conducting large-scale analysis of open-source
C/C++ parser implementations is another avenue for future work that will shed
light on the security dimension of an important software component. Indeed, tar-
geting our analysis at the binary level will help us evaluate its eﬃcacy against
closed source applications.
Acknowledgements. We would like to thank Julian Fietkau for helping customize
the Peach fuzzer for our experiments. This work was supported by the following awards
and grants: Bundesministerium f¨ur Bildung und Forschung (BMBF) under Award No.
KIS1DSD032 (Project Enzevalos), Leibniz Prize project by the German Research Foun-
dation (DFG) under Award No. FKZ FE 570/4-1, the Helmholtz Research School in
Security Technologies scholarship, and the Danish Villum project ReNet. The opin-
ions, views, and conclusions contained herein are those of the author(s) and should not
be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of BMBF, DFG, or, any other funding body involved.
46
B. Shastry et al.
References
1. Aho, A.V., Sethi, R., Ullman, J.D.: Compilers, Principles, Techniques. Addison-
Wesley, Boston (1986)
2. Address Sanitizer. https://clang.llvm.org/docs/AddressSanitizer.html. Accessed
27 Mar 2017
3. B¨ohme, M., Pham, V.T., Roychoudhury, A.: Coverage-based greybox fuzzing as
Markov chain. In: Proceedings of the ACM Conference on Computer and Commu-
nications Security (CCS), pp. 1032–1043. ACM (2016)
4. Caballero, J., Yin, H., Liang, Z., Song, D.: Polyglot: automatic extraction of pro-
tocol message format using dynamic binary analysis. In: Proceedings of the ACM
Conference on Computer and Communications Security (CCS), pp. 317–329 (2007)
5. Cert Secure Coding Standards. https://www.securecoding.cert.org/conﬂuence/
display/seccode/SEI+CERT+Coding+Standards. Accessed 01 June 2017
6. Clusterfuzzer: Heap-buﬀer-overﬂow in read. https://bugs.chromium.org/p/
chromium/issues/detail?id=609042. Accessed 23 Mar 2017
7. Comparetti, P.M., Wondracek, G., Kruegel, C., Kirda, E.: Prospex: Protocol spec-
iﬁcation extraction. In: Proceedings of the IEEE Security & Privacy, pp. 110–125
(2009)
8. Cui, W., Kannan, J., Wang, H.J.: Discoverer: automatic protocol reverse engineer-
ing from network traces. In: Proceedings of the USENIX Security Symposium, vol.
158 (2007)
9. Cui, W., Peinado, M., Chen, K., Wang, H.J., Irun-Briz, L.: Tupni: automatic
reverse engineering of input formats. In: Proceedings of the ACM Conference on
Computer and Communications Security (CCS), pp. 391–402 (2008)
10. Engler, D., Chelf, B., Chou, A., Hallem, S.: Checking system rules using system-
speciﬁc, programmer-written compiler extensions. In: Proceedings of the OSDI
(2000)
11. Foote, J.: The exploitable GDB plugin (2015). https://github.com/jfoote/
exploitable. Accessed 23 Mar 2017
12. Gallagher, K.B., Lyle, J.R.: Using program slicing in software maintenance. IEEE
Trans. Softw. Eng. 17(8), 751–761 (1991)
13. Godefroid, P., Kiezun, A., Levin, M.Y.: Grammar-based whitebox fuzzing. ACM
SIGPLAN Not. 43, 206–215 (2008)
14. Godefroid, P., Levin, M.Y., Molnar, D.: Sage: whitebox fuzzing for security testing.
ACM Queue 10(1), 20 (2012)
15. Google
Inc.: Fuzzer
Accessed 23 Mar 2017
test
suite. https://github.com/google/fuzzer-test-suite.
16. Holler, C., Herzig, K., Zeller, A.: Fuzzing with code fragments. In: Proceedings of
the USENIX Security Symposium, pp. 445–458 (2012)
17. Hopcroft, J.E., Motwani, R., Ullman, J.D.: Introduction to Automata Theory,
Languages, and Computation, 3rd edn. Addison-Wesley, Reading (2006)
18. Lam, M.S., Whaley, J., Livshits, V.B., Martin, M.C., Avots, D., Carbin, M., Unkel,
C.: Context-sensitive program analysis as database queries. In: Proceedings of the
ACM Symposium on Principles of Database Systems, pp. 1–12 (2005)
19. Lin, Z., Jiang, X., Xu, D., Zhang, X.: Automatic protocol format reverse engineer-
ing through context-aware monitored execution. In: Proceedings of Symposium on
Network and Distributed System Security (NDSS), pp. 1–15 (2008)
20. LLVM Compiler Infrastructure: Clang static analyzer. http://clang-analyzer.llvm.
org/. Accessed 23 Mar 2017
Static Program Analysis as a Fuzzing Aid
47
21. LLVM Compiler Infrastructure: libFuzzer: a library for coverage-guided fuzz test-
ing. http://llvm.org/docs/LibFuzzer.html. Accessed 23 Mar 2017
22. Miller, B.P., Fredriksen, L., So, B.: An empirical study of the reliability of UNIX
utilities. Commun. ACM 33(12), 32–44 (1990)
23. MITRE.org: CVE-2014-0160: The Heartbleed Bug. https://cve.mitre.org/cgi-bin/
cvename.cgi?name=CVE-2014-0160. Accessed 23 Mar 2017
24. MITRE.org: CVE-2015-8317: Libxml2: several out of bounds reads. https://cve.
mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-8317. Accessed 23 Mar 2017
25. MITRE.org: CVE-2016-5180: Project c-ares security advisory. https://cve.mitre.
org/cgi-bin/cvename.cgi?name=CVE-2016-5180. Accessed 23 Mar 2017
26. Molnar, D., Li, X.C., Wagner, D.: Dynamic test generation to ﬁnd integer bugs in
x86 binary Linux programs. In: Proceedings of the USENIX Security Symposium,
vol. 9, pp. 67–82 (2009)
27. nDPI: Open and Extensible LGPLv3 Deep Packet Inspection Library. http://www.
ntop.org/products/deep-packet-inspection/ndpi/. Accessed 23 Mar 2017
28. OpenRCE: Sulley. https://github.com/OpenRCE/sulley. Accessed 23 Mar 2017
29. Peach Fuzzer. http://www.peachfuzzer.com/. Accessed 23 Mar 2017
30. Reps, T., Horwitz, S., Sagiv, M.: Precise interprocedural dataﬂow analysis via
graph reachability. In: Proceedings of the ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, pp. 49–61 (1995)
31. Snort++ vulnerabilities found. http://blog.snort.org/2017/05/snort-vulnerabili
ties-found.html. Accessed 05 June 2017
32. Wondracek, G., Comparetti, P.M., Kruegel, C., Kirda, E.: Automatic network pro-
tocol analysis. In: Proceedings of the Symposium on Network and Distributed
System Security (NDSS) (2008)
33. Yamaguchi, F., Maier, A., Gascon, H., Rieck, K.: Automatic inference of search
patterns for taint-style vulnerabilities. In: Proceedings of the IEEE Security &
Privacy, pp. 797–812 (2015)
34. Zalewski, M.: American fuzzy lop. http://lcamtuf.coredump.cx/aﬂ/. Accessed
23 Mar 2017
35. Zalewski, M.: aﬂ-fuzz: making up grammar with a dictionary in hand
(2015). https://lcamtuf.blogspot.de/2015/01/aﬂ-fuzz-making-up-grammar-with.
html. Accessed 23 Mar 2017