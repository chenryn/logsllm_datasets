# Improved Text

## Test Coverage Analysis
Figure 3 illustrates the test coverage over time for `tcpdump` (a) and `nDPI` (b) under various fuzzing configurations. Program coverage was recorded only when there was a change in its magnitude.

### Dates
- February 15
- February 17
- February 19
- February 21
- February 23
- February 25

## Vulnerabilities Exposed
Table 4 presents the number of vulnerabilities exposed in `nDPI` and `tcpdump` across all fuzzing configurations. For `tcpdump`, the positive impact of the Orthrus-generated dictionary is evident. The `afl` and `afl-Orthrus` configurations exposed 15 and 26 unique vulnerabilities, respectively. Out of the 11 additional vulnerabilities exposed by `afl-Orthrus`, 10 were exclusively found by it, meaning that `afl-Orthrus` uncovered 10 vulnerabilities in `tcpdump` that were not identified by standalone `afl`. The `aflfast` and `aflfast-Orthrus` configurations exposed 1 and 5 vulnerabilities, respectively, with `aflfast-Orthrus` uncovering 4 vulnerabilities not found by standalone `aflfast`. In the case of `nDPI`, `afl-Orthrus` exposed 4 vulnerabilities not found by standalone `afl`, while `aflfast-Orthrus` exposed 1 such vulnerability. 

For both `nDPI` and `tcpdump`, `aflfast-Orthrus` found fewer vulnerabilities overall compared to its baseline. We hypothesize that the fuzz schedule modifications in `aflfast` [3] influence the scheduling of dictionary mutations, resulting in the observed reduction.

### Table 4: Number of Bugs and Vulnerabilities Exposed by Different Fuzzing Configurations
| Software | afl | afl-orthrus | aflfast | aflfast-orthrus | Peach-analyzer |
|----------|-----|-------------|---------|-----------------|----------------|
| tcpdump  | 15  | 26 (+10)    | 1       | 5 (+4)          | 0              |
| nDPI     | 26  | 27 (+4)     | 24      | 17 (+1)         | 0              |

### Table 5: Vulnerabilities Exclusively Exposed Using Orthrus-Generated Dictionaries
| Software | Vulnerable Component |
|----------|----------------------|
| tcpdump  | IPv6 DHCP packet printer<br>IPv6 Open Shortest Path First (OSPFv3) packet printer<br>IEEE 802.1ab Link Layer Discovery Protocol (LLDP) packet printer<br>ISO CLNS, ESIS, and ISIS packet printers [2]<br>IP packet printer<br>ISA and Key Management Protocol (ISAKMP) printer<br>IPv6 Internet Control Message Protocol (ICMPv6) printer<br>Point to Point Protocol (PPP) printer<br>White Board Protocol printer |
| nDPI     | ZeroMQ Message Transport Protocol processor<br>Viber protocol processor<br>Syslog protocol processor<br>Ubiquity UBNT AirControl 2 protocol processor<br>HTTP protocol processor |

All the vulnerabilities listed in Table 5 resulted in buffer overflows and were immediately reported to the respective vendors. These results highlight the effectiveness of our approach in increasing the breadth of testing for complex network applications without requiring domain-specific knowledge.

## Preliminary Results for Snort++
We used Orthrus to perform dictionary-based fuzzing on `snort++`, a C++ implementation of the popular `snort` Intrusion Detection System (IDS). Baseline fuzzing with `afl-fuzz` identified a single vulnerability (CVE-2017-6658) in the `snort++` decoder implementation. In contrast, the Orthrus-generated dictionary helped find an additional vulnerability (CVE-2017-6657) in the LLC packet decoder implementation of `snort++` [31].

## Limitations
Although our evaluations show that static analysis-guided fuzzing is beneficial, our positive results may not generalize to other parsing applications. Our evaluation, which includes six different parser implementations, provides strong evidence that our approach can make fuzz testing more effective. Automatically generated parsers (e.g., yacc-based parsers) may have structurally different code than hand-written parsers, which we evaluated. We believe that their analysis may be better conducted at the specification level rather than the source code level.

Furthermore, we use simple heuristics to infer input message fragments from the source code, which may result in missing legitimate input fragments (false negatives) or adding irrelevant tokens to the input dictionary (false positives). However, we take practical measures to minimize the number of false positives/negatives. For example, our design incorporates security advice from reputable institutions like CERT [5], which has been compiled over years of source code audits. In our case study, we used a small but relevant seed set to bootstrap fuzzing. A more diverse seed set might improve the performance of our baseline fuzzers. Nonetheless, we carefully analyzed the additional coverage achieved solely through the use of the supplied dictionary to ensure that the presented increments can be attributed to our method. Additionally, we manually triaged all vulnerabilities found exclusively using dictionary-based fuzzing to ensure causality, i.e., they were ultimately exposed due to the use of specific tokens in the supplied dictionary.

## Related Work
Multiple techniques have been proposed to improve the effectiveness of fuzzing. We focus on approaches that infer the protocol specification, use grammar-based fuzzing, or employ query-driven static analysis.

### Inferring Protocol Specification
There are two main problems in protocol specification inference: inferring the (i) message format and (ii) state machine. Prior work, with the exception of Prospex [7], has focused primarily on the message format inference problem. Two approaches have been proposed for automatic protocol specification inference:
1. **Network Traces**: Tools like Discoverer [8] rely entirely on network traces for inference. However, this approach often results in a simple description of the message format that is an under-approximation of the original specification.
2. **Dynamic Program Analysis**: Proposals such as Polyglot [4], Tupni [9], Autoformat [19], Prospex [7], and the tool by Wondracek et al. [32] fall into this category. These tools require dynamic instrumentation systems, which are often proprietary or inaccessible. In contrast, our analysis can be bundled into an existing compiler toolchain, making protocol inference as simple as compiling the underlying source code.

### Grammar-Based Fuzzing
Godefroid et al. [13] designed a software testing tool that uses symbolic execution to generate grammar-aware test inputs. They evaluated their tool against the IE7 JavaScript interpreter and found that grammar-based testing increased test coverage from 53% to 81%. However, their work suffers from three practical difficulties:
1. Manual grammar specification is required.
2. The infrastructure for symbolic execution at their scale is not publicly available.
3. Non-trivial code annotations are required, necessitating close cooperation between testers and developers.

In contrast, we automatically infer input data formats from the source code, showing that more lightweight analysis techniques can substantially benefit modern fuzzers. Langfuzz [16] uses a grammar specification of the JavaScript and PHP languages to effectively conduct security assessments on the respective interpreters. While Langfuzz demonstrates the superiority of specification-based fuzzing in scenarios where a grammar specification can be obtained, creating such specifications for complex network applications manually is challenging. Network protocol specifications are often semi-formal, requiring protocol implementors to hand-write parsers instead of generating them from a parser generator.

### Query-Based Program Analysis
Our static analysis approach is inspired by prior work on the use of queries to conduct specific program analyses by Lam et al. [18] and the automatic inference of search patterns for discovering taint-style vulnerabilities from source code by Yamaguchi et al. [33]. Both these works use program queries to elicit vulnerable code patterns. While Lam et al. leverage Datalog queries for analysis, Yamaguchi et al. employ graph traversals. In contrast, we use query-driven analysis to support a fuzzer rather than attempting static vulnerability discovery.

## Conclusions and Future Work
In this paper, we demonstrate how static analysis-guided fuzzing can enhance the effectiveness of modern off-the-shelf fuzzers, especially for networking applications. By leveraging code patterns to gather input fragments directly from source code, we couple a static analyzer to a fuzzer via an existing interface. Using input dictionaries derived from semantic and syntactic program analysis queries, we increase the test coverage of applications by 10-15% and reduce the time needed to expose vulnerabilities by an order of magnitude compared to fuzzers not supplied with an input dictionary. We used our research prototype to fuzz two high-profile network applications, `nDPI` and `tcpdump`, finding 10 zero-day vulnerabilities in `tcpdump` and 5 zero-day vulnerabilities in `nDPI` that were missed by standalone fuzzers.

Our work highlights the need for a stronger interaction between program analysis and testing. Future work could include inferring the protocol state machine in addition to its message format and leveraging the additional insight for conducting stateful fuzzing. Another avenue is to target our analysis at the binary level to evaluate its efficacy against closed-source applications.

## Acknowledgements
We would like to thank Julian Fietkau for helping customize the Peach fuzzer for our experiments. This work was supported by the following awards and grants: Bundesministerium f√ºr Bildung und Forschung (BMBF) under Award No. KIS1DSD032 (Project Enzevalos), Leibniz Prize project by the German Research Foundation (DFG) under Award No. FKZ FE 570/4-1, the Helmholtz Research School in Security Technologies scholarship, and the Danish Villum project ReNet. The opinions, views, and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements of BMBF, DFG, or any other funding body involved.

## References
[References section remains unchanged]