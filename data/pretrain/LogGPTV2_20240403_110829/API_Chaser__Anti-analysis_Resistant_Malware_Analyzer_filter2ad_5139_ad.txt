1,762
2,719
2,992
F.P. of Type II
F.P. of Type II
F.P. of Type II
F.P. of Type II
F.P. of Type II
F.P. of Type II
F.P.:False Positive. We (cid:12)ltered nested API calls by white-listing the memory address ranges where
known system DLLs were mapped.
The number of captured API calls during the execution of each command were as follow: compact
was 28,464, xcopy was 1,222, reg was 44,059, tasklist was 8,271 and netstat was 103.
Fig. 8. Results of Performance Experiment
the details of the two speciﬁc cases, Trojan.FakeAV and Infostealer.Gampass,
though the others also yielded the same results. In case of Trojan.FakeAV, all
the matched API calls were invoked from dynamically allocated memory area
which was allocated and written by Trojan.FakeAV, while unmatched API calls
were invoked from memory area where explorer.exe was mapped. It indicates
that API Chaser captured the API calls invoked from the code injected by Tro-
jan.FakeAV and Type II additionally captured API calls invoked from original
code in the code-injected benign process. In the case of Trojan.Gen, all the
matched API calls invoked from tzdfjhm.dll, while all the unmatched calls were
from the memory area where notepad.exe was mapped. tzdfjhm.dll was regis-
tered to the registry key, AppInit DLLs, which is used by malware for injecting
a registered DLL into a process. The DLL was dropped and registered to the
key by Trojan.Gen.
6.3 Performance Experiment
We have conducted a simple performance experiment using ﬁve Windows stan-
dard commands. As comparative environments, we prepared vanilla Qemu, API
Chaser without API monitoring, and API Chaser without argument handlers.
Fig.8 shows the relative run duration of these ﬁve commands on each envi-
ronment, compared to relative Qemu which is set to 1. The results show the
degradation in performance of API Chaser was about x3 to x10, compared to
Qemu. We consider the degradation is not a severe limitation of API Chaser
because current API Chaser has not been optimized to reduce its overhead. We
consider that there is much space to improve the performance by, for example,
applying work done in [22] for API Chaser. In addition, we will discuss an is-
sue which is caused from the degradation of the performance when we analyze
malware checking the delay of execution in Subsection 8.1.
 0 2 4 6 8 10compactxcopyregtasklistnetstatRun Duration(Relative to Qemu)API Chaserw/o argument handlerw/o API monitoring17
7 Related Work
Several approaches have been proposed to precisely monitor malware’s activities
based on API monitoring. In this section, we describe these approaches based
on three categories: binary rewriting, binary-instrumentation, and simulation.
Binary Rewriting Binary rewriting approaches involve implanting hooks at the
entries of APIs by modifying either the data or code of malware or analysis
environment. CWSandbox [9] hooks both APIs and system calls, and monitors
them called from malware. It hooks them with code rewriting technique, i.e.
in-line hooking, which replaces instructions at an entry of an API with a jmp
instruction pointed to a function for monitoring. JoeBox [23] also monitors APIs
and system calls called from malware. It hooks them with a data rewriting
technique, i.e. export address table hooking, which replaces a function pointer
in an export address table of the PE header with an address to a function for
monitoring. Binary rewriting possibly exposes artifacts which allow malware to
stop its execution or change its behavior. As a result of this, we cannot grasp
malware’s actual activities. We have not taken rewriting approaches in API
Chaser because we want to avoid such exposure to malware.
Binary Instrumentation Binary instrumentation involves comparing the ad-
dress of instructions being executed with the one where the API is located.
Stealth Breakpoint [24] instruments instructions of user-land processes at the OS
layer and determines the execution of the monitored address based on address-
comparison. Cobra [8] is a malware analysis environment using stealth break-
points. TTAnalyze [6] (ancestor of Anubis [25]) monitors APIs and system calls
from malware at VMM layer by address comparison. TTAnalyze determines
target processes with a CR3 (Control Register number 3), which is passed from
a probe module running on the guest OS. Panorama [10] is a malware analy-
sis environment established on a whole-system emulator, TEMU [7]. Panorama
is designed for both analyzing and detecting malware based on taint tracking.
It does not hook any APIs or system calls for malware analysis, although we
found in its source code that TEMU has functions for hooking APIs based on
address comparison. These systems detect the execution of APIs by comparing
the address pointed by an instruction pointer to addresses where APIs should
be resided. In addition, they identify the caller of an API based on PID, CR3
or TID. These approaches are possibly evaded using anti-analysis techniques
we mentioned in Section 2. To solve these evasion issues, we proposed our API
monitoring mechanism with code tainting in API Chaser.
Simulation Norman Sandbox [26] simulates the Windows OS and local area
networks. It simulates almost all APIs that Windows system library provides.
However, it is also possibly detected by malware because it does not perfectly
simulate the behaviors of all Windows APIs.
8 Discussion
In this section, we discuss the limitations of API Chaser.
18
8.1 Detection-type Anti-analysis
With the exception of evasion-type anti-analysis, malware often uses detection-
type anti-analysis techniques[3]. Regarding this type anti-analysis, API Chaser
is not troubled except for VM detection and timing attack, because API Chaser
does not modify a guest OS environment, not install any modules and not sim-
ulate any APIs. So, we discuss the two exceptions as below.
Several methods for detecting Qemu have been proposed [27]. To avoid these
detections, we individually made Qemu-speciﬁc artifacts invisible from malware.
For example, we changed the product names of virtual hardwares in Qemu for
the detection technique that depends on these names. We also detect speciﬁc
instruction patterns of a guest OS at runtime and dynamically patch these in-
structions during dynamic binary translation.
Timing attack is a technique checking the delay for executing a speciﬁc code
block. We designed API Chaser to focus on accuracy rather than performance;
therefore, it takes several more seconds to execute part of a code block than in
real hardware environments. As for this technique, we can overcome this with
the same approach as that used in our previous study [28], which controls the
clock in a guest OS on API Chaser by adjusting the tick counts in the emulator
to remove the delay.
8.2 Scripts
API Chaser has a limitation for analyzing script-type malware, e.g. a visual basic
script, or a command script. These scripts are executed on some platforms such
as an interpreter or a virtual machine. Although these scripts have the taint tags
of malware, API Chaser cannot detect their execution because the instructions
executed on the virtual CPU are the ones of their platform, not the ones of
the tainted script. To solve this problem, we are currently considering a way to
identify target code with both taint tags and semantic information such as PID
and TID.
8.3 Return Oriented Programming
API Chaser cannot correctly identify the caller of an API when the API is called
with the way like return-oriented-programming (ROP)[29]. That is, a small piece
of code of benign programs is used to call the API indirectly. In such a case, the
caller is an instruction in benign program, so API Chaser fails to identify the
execution of the API. As for that, if we can detect ROP code, we may be able to
identify the execution of APIs called from malware via ROP code. Detection of
ROP is out of our scope and we leave it to other studies. Many of them leverage
the unique behavioral characteristics of ROP code, such as its use of many ret
instructions, jumps to the middle of an API, or jumps to an instruction of non
exported functions.
Implicit Flow
8.4
Another limitation of API Chaser is due to feasibility issues of taint propagation,
e.g. implicit ﬂow. If malware authors know the internal architecture of API
Chaser, especially code taint propagation, it may be possible to intentionally
19
cause API Chaser to have false positives or false negatives using implicit ﬂow.
For example, malware reads a piece of code in a benign program and processes
the code through implicit ﬂow which does not change its value. Then it writes
the code back to the same position. As a result, the taint tags on the code are
changed from benign to malware. Due to this, if malware executes the written
code, API Chaser identiﬁes the execution as the one of malware, even though
the code is truly benign one. On the other hand, if malware reads a piece of code
in an API and conducts the same process, it overwrites the taint tags for API
with the ones for malware. Thus, API Chaser deals the execution of the code
as one of malware. To solve the problem, we need to improve the strength of
the taint propagation, for example, as done in [30][31]. We consider this as our
future work.
9 Conclusion
Anti-analysis feature of malware is a challenging problem for anti-malware re-
search, especially for practical malware analysis environment. We focused on
this problem and provided a solution by using API Chaser, which is a proto-
type system of our API monitoring approach. API Chaser was designed and
implemented to prevent malware from evading API monitoring. We conducted
experiments using actual malicious code with various types of anti-analysis to
show that API Chaser correctly works according to its design intended being
diﬃcult to evade. We believe that API Chaser will be able to assist malware
analysts in understanding malware activities more correctly without spending a
large amount of eﬀort in reverse engineering and also contribute to improving
the eﬀectiveness of anti-malware research based on API monitoring.
References
1. Sathyanarayan, V.S., Kohli, P., Bruhadeshwar, B.: Signature Generation and De-
tection of Malware Families. In: Proceedings of the 13th Australasian conference
on Information Security and Privacy. ACISP ’08 (2008)
2. Suenaga, M.: A Museum of API Obfuscation on Win32. In: Proceedings of 12th
Association of anti-Virus Asia Researchers International Conference. AVAR ’09
(2009)
3. Yason, M.V.: The Art of Unpacking. In: Black Hat USA Brieﬁngs. (2007)
4. Bellard, F.: QEMU, a Fast and Portable Dynamic Translator. In: Proceedings of
the annual conference on USENIX Annual Technical Conference, ATEC ’05 (2005)
5. Portokalidis, G., Slowinska, A., Bos, H.: Argos: an emulator for ﬁngerprinting
zero-day attacks for advertised honeypots with automatic signature generation. In:
Proceedings of the 1st European Conference on Computer Systems 2006. EuroSys
’06 (2006)
6. Bayer, U., Kruegel, C., Kirda, E.: TTAnalyze: A Tool for Analyzing Malware. In:
Proceedings of the European Institute for Computer Antivirus Research Annual
Conference. EICAR ’06 (2006)
7. Song, D., Brumley, D., Yin, H., Caballero, J., Jager, I., Kang, M.G., Liang, Z.,
Newsome, J., Poosankam, P., Saxena, P.: BitBlaze: A New Approach to Computer
Security via Binary Analysis. In: Proceedings of the 4th International Conference
on Information Systems Security. ICISS ’08 (2008)
20
8. Vasudevan, A., Yerraballi, R.: Cobra: Fine-grained Malware Analysis using Stealth
Localized-Executions. In: Proceedings of 2006 IEEE Symposium on Security and
Privacy. Oakland ’06 (2006)
9. Willems, C., Holz, T., Freiling, F.: Toward Automated Dynamic Malware Analysis
Using CWSandbox. IEEE Security and Privacy 5(2) (March 2007) 32–39
10. Yin, H., Song, D., Egele, M., Kruegel, C., Kirda, E.: Panorama: Capturing System-
wide Information Flow for Malware Detection and Analysis. In: Proceedings of the
14th ACM conference on Computer and communications security. CCS ’07 (2007)
11. Brumley, D., Hartwig, C., Liang, Z., Newsome, J., Song, D.X., Yin, H.: Auto-
matically Identifying Trigger-based Behavior in Malware. In: Botnet Detection.
(2007)
12. Lastline Whitepaper: Automated detection and mitigation of execution-stalling
malicious code. http://www.lastline.com/papers/antistalling code.pdf
13. Newsome, J., Song, D.: Dynamic Taint Analysis for Automatic Detection, Analysis,
and Signature Generation of Exploits on Commodity Software. In: Proceedings of
the 12th Annual Network and Distributed System Security Symposium. NDSS ’05
14. Carrier, B.: The slueth kit(tsk). http://www.sleuthkit.org/
15. Iwamura, M., Itoh, M., Muraoka, Y.: Towards Eﬃcient Analysis for Malware in
the Wild. In: Proceedings of IEEE International Conference on Communications.
ICC ’11 (2011)
16. Hex-Rays: IDA. https://www.hex-rays.com/
17. The Undocumented Functions. http://undocumented.ntinternals.net/
18. React OS Project. http://www.reactos.org/
19. The Volatility Framework. https://code.google.com/p/volatility/
20. Themida. http://www.oreans.com/themida.php
21. Microsoft:
Intorduction to hotpatching.
http://technet.microsoft.com/en-
us/library/cc781109(v=ws.10).aspx
22. Ermolinskiy, A., Katti, S., Shenker, S., Fowler, L.L., McCauley, M.: Towards Prac-
tical Taint Tracking. Technical Report UCB/EECS-2010-92, EECS Department,
University of California, Berkeley (Jun 2010)
23. Joe Security LLC: Joebox sandbox. http://www.joesecurity.org/
24. Vasudevan, A., Yerraballi, R.: Stealth Breakpoints. In: Proceedings of the 21st
Annual Computer Security Applications Conference. ACSAC ’05 (2005)
25. Anubis: Analyzing unknown binaries. http://anubis.iseclab.org/
26. Norman Sandbox White Paper. http://download.norman.no/whitepapers/
whitepaper Norman SandBox.pdf
27. Ferrie, P.: Attacks on Virtual Machine Emulators. In: Symantec Security Response.
(2006)
28. Kawakoya, Y., Iwamura, M., Itoh, M.: Memory Behavior-Based Automatic Mal-
ware Unpacking in Stealth Debugging Environment. In: Proceedings of 5th IEEE
International Conference on Malicious and Unwanted Software. (2010)
29. Chen, P., Xiao, H., Shen, X., Yin, X., Mao, B., Xie, L.: DROP: Detecting Return-
Oriented Programming Malicious Code. In: Proceedings of the 5th International
Conference on Information Systems Security. ICISS ’09 (2009)
30. Kang, M.G., McCamant, S., Poosankam, P., Song, D.: DTA++: Dynamic Taint
In: Proceedings of the 18th
Analysis with Targeted Control-Flow Propagation.
Annual Network and Distributed System Security Symposium. NDSS ’11 (2011)
31. Slowinska, A., Bos, H.: Pointless Tainting?: Evaluating the Practicality of Pointer
In: Proceedings of the 4th ACM European conference on Computer
Tainting.
systems. EuroSys ’09 (2009)