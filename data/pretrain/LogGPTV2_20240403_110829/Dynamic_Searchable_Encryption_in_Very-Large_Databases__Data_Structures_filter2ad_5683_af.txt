the dictionary size, potentially to less than the amount of the
available RAM.
For the rest of the keywords, after one access (or a few
disk accesses for very common keywords), the addresses of
all the relevant tuple blocks are known. At this point, the
query execution engine issues as many concurrent tuple block
requests as the RAID controller can handle. After less than
the average disk access time, because of the large number
of pending requests, tuple blocks are read at close to the
maximum rate of the RAID controller. The rate at which tuples
are retrieved from storage determines the throughput of search
engine. Note that goodput is 100% when accessing tuple blocks
ﬁlled with tuples and that for frequent keywords, the goodput
of a Search operation grows asymptotically to 100%.
In contrast, and by way of comparison, the Πpack con-
struction computes the location addresses of all their tuple
groups from the keyword value and a running counter. Thus it
can precompute a large number of group addresses and issue
requests for tuple groups immediately, i.e. no additional disk
accesses to retrieve pointers are needed. But without a priori
knowledge of DB(w) size, which is the common case, Πpack
issues many more requests than necessary. Even worse, to
achieve the lowest access latency for a CH-based construction,
one always needs to issue two requests per expected tuple
group, as the group can be stored in two positions (pages) in
the CH table. Finally, these disk accesses have low goodput
as each bucket contains multiple tuple groups. Thus it appears
that low I/O goodput is inherent to Πpack. For large sets, the
superior goodput of our construction (due to large tuple blocks)
more than compensates for the extra initial storage access(es)
for pointers.
For keywords with just a few tuples that ﬁt
in one
dictionary location, the performance is the same. However,
one could accelerate the performance of Π2lev by storing the
dictionary, which is relatively small even for large data sets,
in main memory. Dictionaries used by previous work, which
use one large bucket hash for all tuple sets, are too large for
this optimization.
The two-level Π2lev construction allows for a very efﬁcient
EDB generation process. As an example, during the longest
phase of EDB generation from a database with ≈ 25 billion
(w, id) pairs in the context of multi client OXT [3], which
took 40 hours, all cores performed crypto operations at close
to 100% utilization while at the same time reading 100 million
records from a MySQL DB and writing to the ﬁle system
the tuple blocks and the temp dictionary ﬁles. Overall, the
two-level construction is much closer to our requirements than
any previous ones and the experimental results conﬁrm our
expectations.
12
C. EDB Generation
For our largest datasets, EDB is on the order of 2TB. Thus
EDB generation time is sensitive to implementation details and
is the dominant factor determining the practical scalability of
all our constructions. This section describes the parallel Setup
algorithm used in the Π2lev prototype.
Before EDB generation starts we process the input ﬁles
into an index of the form expected by Π2lev. For each text
column ’t’ in the clear-text database table create a new ’text t’
table with columns ind and word. For each clear-text record
and for each word ’xxxx’ in its text column, add the pair
(id, xxxx) to ’text t’, where id is the unique identiﬁer of the
clear-text record. The number of pairs in ’text t’ is equal to the
number of clear-text records multiplied by the average number
of words in column ’t’. At the end, we create an index on the
column ’word’, which is used during Setup to retrieve DB(w)
for all w = (t, xxxx), where ’xxxx’ is a word in column ’t’.
Unfortunately, for our largest databases, ’table t’ is too
large for the index to ﬁt in RAM, which makes building the
index impractical. To overcome this obstacle, for each text
column ’t’ we create multiple tables ’text t nn’, such that
(1) id-word pairs are somewhat evenly distributed across the
new tables, (2) all the pairs with the same ’word’ value are in
the same table, and (3) the new tables are small enough for
their indexes to be built efﬁciently on our platforms. Note that
the atomic columns of the original table can undergo a similar
transformation if the number of records in the clear-text table
is too large for indexing.
EDB is generated in three phases. The ﬁrst phase counts
the number of distinct keywords wi in the clear-text table and
other statistics needed for sizing the dictionary γ and array A
(or for masking the sizes of these data structures if so desired).
This phase uses full-index scans and takes a small fraction of
the total EDB generation time.
For performance reasons, the dictionary γ, realized as a
bucket hash, is partitioned in equal size groups of consecutive
buckets and its generation is split across the second and third
phases. The tuple block array A is fully generated in the next
phase.
The second phase generates the tuples in DB(w), for all
keywords w = (i, val) using full-index scans on atomic col-
umn i. For each text column ’t’, the newly created ’text t nn’
tables are scanned. Columns are processed in parallel worker
threads, with approximately 1.5 workers per CPU core to
hide database access latencies. For each value val such that
w = (i, val), the thread processing column i retrieves the all
the ids corresponding to the records with val in column i
and applies a random permutation to the resultant id sequence
(i.e., DB(w)). For each id in the permuted sequence,
the
worker generates tuple elements with the encrypted id (and
the additional tuple values rdk and y when implementing the
more advanced features of the OXT protocol from [3]).
worker threads create all the necessary tuple blocks in the array
A (see Figure 4).
During the third phase, the dictionary γ is created from the
partition ﬁles generated in the previous phase. Each partition
is constructed by a separate worker thread. Each worker thread
reads the ﬁles generated by phase-two workers for its partition,
merges their contents and creates the label and content of each
dictionary entry in the partition. Next, for each bucket in its
partition, the worker assigns the dictionary entries to random
locations in the bucket, ﬁlls in empty locations with random
bits, and writes the bucket to disk. For a typical census table,
the dictionary ﬁle is almost two orders of magnitude smaller
than the tuple block ﬁle.
Note that for the creation of the dictionary, the ﬁle system
is accessed using append calls (to temp ﬁles in the second
phase) and sequential read calls (from the temp ﬁles in the
third phase), which are the most efﬁcient ways to access large
ﬁles.
However, worker threads still
issue a large number of
random write calls during the second phase, with one call for
each tuple block generated. To reduce the disk head movements
associated with these writes, we extended the parallel EDB
generation algorithm to conﬁne concurrent workers to a tuple
block window sliding across the array. As a result,
tuple
blocks that are closely located on the disk are generated
near simultaneously. This optimization reduces seek overheads
noticeably for our largest EDBs.
During the third phase, threads issue another set of ran-
dom writes when writing the dictionary buckets. These disk
movements generated by these writes do not represent a major
bottleneck because these writes are already clustered to the
bucket hash partitions under constructions, which we always
select in increasing order.
D. Complex Functional Settings
As already mentioned at the end of Section II, our con-
structs can be used to support richer encrypted-search settings
than SSE, such as those in [3], [13]. In particular, all (single-
keyword) SSE schemes presented here can be used, almost
‘out-of-the-box’, to instantiate the “TSet functionality” under-
lying the OXT protocol in the above works. The main change
is on the size of tuples that increases in order to accommodate
additional OXT information such as the xind and y values (see
Section 3.2 of [3]), and the key to decrypt the actual documents
(as required in multi-client settings [13]).
Storing larger tuples requires minor conﬁguration changes
but no alteration of the original construct. More speciﬁcally,
hash table buckets need to be made large enough to accom-
modate enough entries for all the tuples to be inserted into
the table with high enough probability, i.e., without any of the
buckets overﬂowing.
During this phase, each worker thread creates one temp ﬁle
per dictionary partition in which it stores the content of the
locations (tuples or pointers) assigned to any of the buckets
in the partition. For better performance, the location content
is buffered and appended to the partition ﬁle in 64KB data
chunks. At the same time, for medium and large DB(w), the
Another challenge in more complex protocols, such as
OXT,
is for the server to efﬁciently perform a two party
computation which takes in-order generated data by the client
and out-of-order the tuples, as retrieved from the disk by
the Πpack or Π2lev prototypes. Maximizing the throughput of
such a computation requires using complex buffer management
13
DB Name
Records
CW-MC-OXT-1
CW-MC-OXT-2
CW-MC-OXT-3
CW-MC-OXT-4
LL-MC-SKS-1
LL-MC-SKS-2
LL-MC-SKS-3
LL-MC-SKS-4
408,450
1,001,695
3,362,993
13,284,801
100,000
1,000,000
10,000,000
100,000,000
(w, id) pairs
143,488,496
316,560,959
1,084,855,372
2,732,311,945
114,482,724
1,145,547,173
11,465,515,716
114,633,641,708
EDB size
69.6 GB
99.8 GB
242.4 GB
903.9 GB
15.0 GB
52.0 GB
394.0 GB
3,961.3 GB
TABLE I.
DATABASES
algorithms that optimize the use of available RAM between
tokens and tuple block buffers.
E. Experimental Results
Our prototype implementation measures roughly 65k lines
of C code, including test programs. Measurements reported
here were performed on blades with dual Intel Xeon 2.4GHz
E5620 processors having 4 cores each and running Linux.
Storage consists of 6 1TB SAS disks conﬁgured as a RAID-
0 with a 64KB stripe and attached via a 3 Gb/s to an LSI
1064e SAN controller and formatted as an ext4 ﬁle system with
an 8KB page size. Clear-text databases are stored in MySQL
version 5.5.
The experiments reported in this section use databases
derived from the ClueWeb Collection [18] or synthetically
generated by an engine trained on US-census data. The key
attributes of these databases and derived encrypted indices
are summarized in Table I. The CW-* databases were ex-
tracted from the ClueWeb Collection while the LL-* databases
emulate the US-census data. Both database families contain
atomic type and text columns. The ClueWeb databases were
encrypted for a multi-client setting supporting conjunctions
(OXT) [3] and the census database where processed for single
keyword search (SKS), also in multi-client settings [13] (see
Section V-D).
As already mentioned, EDB generation is the dominant
factor determining the practical scalability of all our con-
structions. The two plots called CW (PH) and CW (2L)
in Figure 5 show how long it takes to generate the EDBs
corresponding to the four CW-* databases when using the
Πpack and Π2lev prototypes, respectively.
The results clearly show the Π2lev construction outper-
forming the Πpack one. The Πpack prototype is consistently
slower because its database access patterns are more expensive
than those of the Π2lev prototype. For larger datasets, the
performance of the Πpack prototype collapses as soon as its
RAM requirements, which are proportional with the database
size, approach the available RAM. The Π2lev prototype does
not exhibit a similar pattern because its RAM requirements
are roughly proportional with the size of the database columns
currently being processed.
In separate experiments with the Π2lev prototype, prepro-
cessing for the LL-* family of databases also proved to scale
linearly, with roughly a rate of 3µs per (w, id) pair for the
largest database and 8.9µs per (w, id) pair for the smallest one.
This translates to roughly 92 hours for biggest LL-MC-SKS-4
database as shown in Figure 6. This compares very favorably
CW (2L)
CW (PH)
 8000
 7000
 6000
 5000
 4000
 3000
 2000
 1000
)
n
m
i
(
e
m
T
i
 0
 0
 5e+08
 1e+09
 1.5e+09
(w,id) pairs
 2e+09
 2.5e+09
 3e+09
Fig. 5. ClueWeb09 Pre-processing: Scaling Database Size
to the experimental results published by Kamara et al [15],
who report a cost of approximately 35µs per (w, id) pair on a
computing platform with similar capabilities.
End-to-end
 10000
)
n
m
i
(
e
m
T
i
 1000
 100