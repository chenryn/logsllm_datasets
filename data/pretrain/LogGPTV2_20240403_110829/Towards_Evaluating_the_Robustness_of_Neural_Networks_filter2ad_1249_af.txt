l
i
t
s
D
i
,
s
r
e
f
s
n
a
r
T
e
p
m
a
x
E
l
l
a
i
r
a
s
r
e
v
d
A
y
t
i
l
i
b
a
b
o
r
P
Untargetted
Targetted
8
.
0
6
.
0
4
0
.
2
0
.
.
0
0
0
10
20
30
40
Value of k
In this paper, we propose powerful attacks that defeat
defensive distillation, demonstrating that our attacks more
generally can be used to evaluate the efﬁcacy of potential
defenses. By systematically evaluating many possible attack
approaches, we settle on one that can consistently ﬁnd better
adversarial examples than all existing approaches. We use this
evaluation as the basis of our three L0, L2, and L∞ attacks.
We encourage those who create defenses to perform the two
evaluation approaches we use in this paper:
• Use a powerful attack (such as the ones proposed in this
paper) to evaluate the robustness of the secured model
directly. Since a defense that prevents our L2 attack will
prevent our other attacks, defenders should make sure to
establish robustness against the L2 distance metric.
• Demonstrate that transferability fails by constructing
high-conﬁdence adversarial examples on a unsecured
model and showing they fail to transfer to the secured
model.
Fig. 10. Probability that adversarial examples transfer from the baseline model
to a model trained with defensive distillation at temperature 100.
ACKNOWLEDGEMENTS
cation of the adversarial example. This allows us to generate
high-conﬁdence adversarial examples by increasing κ.
We ﬁrst investigate if our hypothesis is true that the stronger
the classiﬁcation on the ﬁrst model, the more likely it will
transfer. We do this by varying κ from 0 to 40.
Our baseline experiment uses two models trained on MNIST
as described in Section IV, with each model trained on half of
the training data. We ﬁnd that the transferability success rate
increases linearly from κ = 0 to κ = 20 and then plateaus
at near-100% success for κ ≈ 20, so clearly increasing κ
increases the probability of a successful transferable attack.
We then run this same experiment only instead we train
the second model with defensive distillation, and ﬁnd that
adversarial examples do transfer. This gives us another at-
tack technique for ﬁnding adversarial examples on distilled
networks.
However, interestingly, the transferability success rate be-
tween the unsecured model and the distilled model only
reaches 100% success at κ = 40, in comparison to the previous
approach that only required κ = 20.
We believe that this approach can be used in general to
evaluate the robustness of defenses, even if the defense is able
to completely block ﬂow of gradients to cause our gradient-
descent based approaches from succeeding.
IX. CONCLUSION
The existence of adversarial examples limits the areas in
which deep learning can be applied. It is an open problem
to construct defenses that are robust to adversarial examples.
In an attempt
to solve this problem, defensive distillation
was proposed as a general-purpose procedure to increase the
robustness of an arbitrary neural network.
We would like to thank Nicolas Papernot discussing our
defensive distillation implementation, and the anonymous re-
viewers for their helpful feedback. This work was supported
by Intel through the ISTC for Secure Computing, Qualcomm,
Cisco, the AFOSR under MURI award FA9550-12-1-0040,
and the Hewlett Foundation through the Center for Long-Term
Cybersecurity.
REFERENCES
[1] ANDOR, D., ALBERTI, C., WEISS, D., SEVERYN, A., PRESTA, A.,
GANCHEV, K., PETROV, S., AND COLLINS, M. Globally normalized
transition-based neural networks.
arXiv preprint arXiv:1603.06042
(2016).
[2] BASTANI, O., IOANNOU, Y., LAMPROPOULOS, L., VYTINIOTIS, D.,
NORI, A., AND CRIMINISI, A. Measuring neural net robustness with
constraints. arXiv preprint arXiv:1605.07262 (2016).
[3] BOJARSKI, M., DEL TESTA, D., DWORAKOWSKI, D., FIRNER, B.,
FLEPP, B., GOYAL, P., JACKEL, L. D., MONFORT, M., MULLER, U.,
ZHANG, J., ET AL. End to end learning for self-driving cars. arXiv
preprint arXiv:1604.07316 (2016).
K.
cars,
[4] BOURZAC,
self-driving
//spectrum.ieee.org/computing/embedded-systems/
bringing-big-neural-networks-to-selfdriving-cars-smartphones-and-drones,
2016.
Bringing
smartphones,
networks
to
http:
big
and
drones.
neural
[5] CARLINI, N., MISHRA, P., VAIDYA, T., ZHANG, Y., SHERR, M.,
SHIELDS, C., WAGNER, D., AND ZHOU, W. Hidden voice commands.
In 25th USENIX Security Symposium (USENIX Security 16), Austin, TX
(2016).
[6] CHANDOLA, V., BANERJEE, A., AND KUMAR, V. Anomaly detection:
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 15.
[7] CLEVERT, D.-A., UNTERTHINER, T., AND HOCHREITER, S. Fast and
accurate deep network learning by exponential linear units (ELUs).
arXiv preprint arXiv:1511.07289 (2015).
[8] DAHL, G. E., STOKES, J. W., DENG, L., AND YU, D. Large-scale
malware classiﬁcation using random projections and neural networks. In
2013 IEEE International Conference on Acoustics, Speech and Signal
Processing (2013), IEEE, pp. 3422–3426.
[9] DENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND FEI-FEI,
L. Imagenet: A large-scale hierarchical image database. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on (2009), IEEE, pp. 248–255.
53
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
[13] GRAVES, A., MOHAMED, A.-R., AND HINTON, G. Speech recognition
with deep recurrent neural networks.
In 2013 IEEE international
conference on acoustics, speech and signal processing (2013), IEEE,
pp. 6645–6649.
[14] GROSSE, K., PAPERNOT, N., MANOHARAN, P., BACKES, M., AND
MCDANIEL, P. Adversarial perturbations against deep neural networks
for malware classiﬁcation. arXiv preprint arXiv:1606.04435 (2016).
[15] GU, S., AND RIGAZIO, L. Towards deep neural network architectures
robust to adversarial examples. arXiv preprint arXiv:1412.5068 (2014).
[16] HE, K., ZHANG, X., REN, S., AND SUN, J. Deep residual learning for
image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (2016), pp. 770–778.
[17] HINTON, G., DENG, L., YU, D., DAHL, G., RAHMAN MOHAMED, A.,
JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH, T.,
AND KINGSBURY, B. Deep neural networks for acoustic modeling in
speech recognition. Signal Processing Magazine (2012).
[18] HINTON, G., DENG, L., YU, D., DAHL, G. E., MOHAMED, A.-R.,
JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH,
T. N., ET AL. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups.
IEEE Signal
Processing Magazine 29, 6 (2012), 82–97.
[19] HINTON, G., VINYALS, O., AND DEAN, J. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[20] HUANG, R., XU, B., SCHUURMANS, D., AND SZEPESV ´ARI, C. Learn-
ing with a strong adversary. CoRR, abs/1511.03034 (2015).
[21] HUANG, X., KWIATKOWSKA, M., WANG, S., AND WU, M. Safety
veriﬁcation of deep neural networks. arXiv preprint arXiv:1610.06940
(2016).
[22] JANGLOV ´A, D. Neural networks in mobile robot motion. Cutting Edge
Robotics 1, 1 (2005), 243.
[23] KINGMA, D., AND BA, J. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980 (2014).
[24] KRIZHEVSKY, A., AND HINTON, G.
features from tiny images.
Learning multiple layers of
[10] GIUSTI, A., GUZZI, J., CIRES¸ AN, D. C., HE, F.-L., RODR´IGUEZ,
J. P., FONTANA, F., FAESSLER, M., FORSTER, C., SCHMIDHUBER, J.,
DI CARO, G., ET AL. A machine learning approach to visual perception
of forest trails for mobile robots. IEEE Robotics and Automation Letters
1, 2 (2016), 661–667.
[11] GOODFELLOW, I. J., SHLENS, J., AND SZEGEDY, C.
Explaining
and harnessing adversarial examples. arXiv preprint arXiv:1412.6572
(2014).
[12] GRAHAM, B. Fractional max-pooling. arXiv preprint arXiv:1412.6071
(2014).
[25] KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E.
classiﬁcation with deep convolutional neural networks.
in neural information processing systems (2012), pp. 1097–1105.
ImageNet
In Advances
[26] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533 (2016).
[27] LECUN, Y., BOTTOU, L., BENGIO, Y., AND HAFFNER, P. Gradient-
based learning applied to document recognition. Proceedings of the
IEEE 86, 11 (1998), 2278–2324.
[28] LECUN, Y., CORTES, C., AND BURGES, C. J. The mnist database of
handwritten digits, 1998.
[29] MAAS, A. L., HANNUN, A. Y., AND NG, A. Y. Rectiﬁer nonlinearities
improve neural network acoustic models. In Proc. ICML (2013), vol. 30.
[30] MELICHER, W., UR, B., SEGRETI, S. M., KOMANDURI, S., BAUER,
L., CHRISTIN, N., AND CRANOR, L. F.
lean and accurate:
Modeling password guessability using neural networks. In Proceedings
of USENIX Security (2016).
[31] MISHKIN, D., AND MATAS, J. All you need is a good init. arXiv
Fast,
preprint arXiv:1511.06422 (2015).
[32] MNIH, V., KAVUKCUOGLU, K., SILVER, D., GRAVES, A.,
ANTONOGLOU, I., WIERSTRA, D., AND RIEDMILLER, M. Playing
Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
(2013).
[33] MNIH, V., KAVUKCUOGLU, K., SILVER, D., RUSU, A. A., VENESS,
J., BELLEMARE, M. G., GRAVES, A., RIEDMILLER, M., FIDJELAND,
A. K., OSTROVSKI, G., ET AL. Human-level control through deep
reinforcement learning. Nature 518, 7540 (2015), 529–533.
[34] MOOSAVI-DEZFOOLI, S.-M., FAWZI, A., AND FROSSARD, P. Deep-
fool: a simple and accurate method to fool deep neural networks. arXiv
preprint arXiv:1511.04599 (2015).
[35] PAPERNOT, N., GOODFELLOW, I., SHEATSLEY, R., FEINMAN, R., AND
cleverhans v1.0.0: an adversarial machine learning
MCDANIEL, P.
library. arXiv preprint arXiv:1610.00768 (2016).
[36] PAPERNOT, N., AND MCDANIEL, P. On the effectiveness of defensive
distillation. arXiv preprint arXiv:1607.05113 (2016).
[37] PAPERNOT, N., MCDANIEL, P., AND GOODFELLOW, I. Transferabil-
ity in machine learning: from phenomena to black-box attacks using
adversarial samples. arXiv preprint arXiv:1605.07277 (2016).
[38] PAPERNOT, N., MCDANIEL, P., JHA, S., FREDRIKSON, M., CELIK,
Z. B., AND SWAMI, A. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy
(EuroS&P) (2016), IEEE, pp. 372–387.
[39] PAPERNOT, N., MCDANIEL, P., WU, X., JHA, S., AND SWAMI, A.
Distillation as a defense to adversarial perturbations against deep neural
networks. IEEE Symposium on Security and Privacy (2016).
[40] PASCANU, R., STOKES, J. W., SANOSSIAN, H., MARINESCU, M.,
AND THOMAS, A. Malware classiﬁcation with recurrent networks. In
2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) (2015), IEEE, pp. 1916–1920.
[41] RUSSAKOVSKY, O., DENG, J., SU, H., KRAUSE, J., SATHEESH, S.,
MA, S., HUANG, Z., KARPATHY, A., KHOSLA, A., BERNSTEIN, M.,
BERG, A. C., AND FEI-FEI, L. ImageNet Large Scale Visual Recogni-
tion Challenge. International Journal of Computer Vision (IJCV) 115,
3 (2015), 211–252.
[42] SHAHAM, U., YAMADA, Y., AND NEGAHBAN, S. Understanding
adversarial training: Increasing local stability of neural nets through
robust optimization. arXiv preprint arXiv:1511.05432 (2015).
[43] SILVER, D., HUANG, A., MADDISON, C. J., GUEZ, A., SIFRE, L.,
VAN DEN DRIESSCHE, G., SCHRITTWIESER, J., ANTONOGLOU, I.,
PANNEERSHELVAM, V., LANCTOT, M., ET AL. Mastering the game
of Go with deep neural networks and tree search. Nature 529, 7587
(2016), 484–489.
[44] SPRINGENBERG, J. T., DOSOVITSKIY, A., BROX, T., AND RIED-
MILLER, M. Striving for simplicity: The all convolutional net. arXiv
preprint arXiv:1412.6806 (2014).
[45] SZEGEDY, C., VANHOUCKE, V., IOFFE, S., SHLENS, J., AND WOJNA,
Z. Rethinking the Inception architecture for computer vision. arXiv
preprint arXiv:1512.00567 (2015).
[46] SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J., ERHAN,
D., GOODFELLOW, I., AND FERGUS, R. Intriguing properties of neural
networks. ICLR (2013).
[47] WARDE-FARLEY, D., AND GOODFELLOW, I. Adversarial perturbations
of deep neural networks. Advanced Structured Prediction, T. Hazan, G.
Papandreou, and D. Tarlow, Eds (2016).
[48] YUAN, Z., LU, Y., WANG, Z., AND XUE, Y. Droid-sec: Deep learning
in android malware detection. In ACM SIGCOMM Computer Commu-
nication Review (2014), vol. 44, ACM, pp. 371–372.
APPENDIX
54
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Target Classiﬁcation (L0)
4
5
6
7
8
9
0
1
2
3
9
8
7
6
5
n
o
i
t
a
c
ﬁ
i
s
s
a
l
C
4
e
c
r
u
o
S
3
2
1
0
Fig. 11. Our L0 adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst image in the dataset
with that label.
55
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Target Classiﬁcation (L2)
4
5
6
7
8
9
0
1
2
3
9
8
7
6
5
n
o
i
t
a
c
ﬁ
i
s
s
a
l
C
4
e
c
r
u
o
S
3
2
1
0
Fig. 12. Our L2 adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst image in the dataset
with that label.
56
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Target Classiﬁcation (L∞)
4
5
6
7
8
9
0
1
2
3
9
8
7
6
5
n
o
i
t
a
c
ﬁ
i
s
s
a
l
C
4
e
c
r
u
o
S
3
2
1
0
Fig. 13. Our L∞ adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst image in the dataset
with that label.
57
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply.