ity. Thus, the heavy-tailed nature of ﬂow sizes here may be
said to have a positive impact on performance, albeit at the
cost of an unfortunate discrimination against elephants.
Note that the degree of congestion on a network link may
not be easy to detect simply by observing the packet arrival
process. The packet arrival rate is controlled by TCP, ide-
ally to a value close to the service rate, even though the
number of contributing ﬂows may be increasing rapidly.
Elastic traﬃc congestion is manifested essentially at ﬂow
level rather than at packet level.
7.2
Impatience
In a real network, if demand exceeds capacity, the number
of ﬂows in progress does not increase indeﬁnitely. As per-
ﬂow throughput decreases some ﬂows or sessions will be
interrupted, due either to user impatience or to aborts by
TCP or higher layer protocols. In the following we use the
term impatience for all causes of premature abandon.
We are unaware of any published statistics on user im-
patience. The phenomenon is clearly very diﬃcult to ob-
serve in practice, notably because all ﬂow aborts are not in
reaction to excessive response time and because most im-
patience is manifested by the interruption of a session and
may not be detectable as an abnormal event. However, to
gain some insight into the phenomenon we propose below
a simple hypothetical model.
We suppose a ﬂow of size s will be interrupted if and
when its response time exceeds a patience duration δ(s). It
seems natural to assume that δ is an increasing but concave
function of s since users have a response time expectation
which increases with the ﬂow size but need proportionally
more throughput. We have observed in simulations that
such impatience causes the number of ﬂows in progress on
a large capacity link to stabilize and vary slightly about a
certain mean value. To simplify we assume here that the
number is exactly constant so that each ﬂow receives the
same bandwidth share θ. A ﬂow of size s is then completed
if and only if s ≤ θδ(s).
It follows from the concavity of δ that there exists a crit-
ical ﬂow size s(cid:2) satisfying s(cid:2) = θδ(s(cid:2)) such that any ﬂow
of size smaller than s(cid:2) is completed while all the others are
aborted. We can determine θ by arguing that, since the
link is always saturated, we must have:
(cid:8) ∞
C = λ
min(s, θδ(s))dF (s)
(15)
0
where F (s) is the ﬂow size distribution. The following sim-
ple closed formula may be derived from (15) in the partic-
ular case of Pareto distributed ﬂow sizes (1) and constant
patience duration, δ(s) = δ:
(cid:4)
(cid:5) 1
θ =
k
δ
β−1
ρ
β(ρ − 1)
.
(16)
(cid:9) s(cid:2)
It is further possible in this case to derive the link goodput
U = λ
sdF (s)/C, i.e., the fraction of link capacity used
by ﬂows that are eﬀectively completed:
U = 1 − (β − 1)(ρ − 1)
(17)
0
Note that link goodput may be equal to zero, meaning all
ﬂows are interrupted, in the extreme situation where the
overload exceeds 1/(β − 1) × 100%.
This model provides some useful insights into the impact
of congestion. The results for constant patience are quali-
tatively representative of evaluations performed with diﬀer-
ent patience functions δ(s). Both realized throughput and
link goodput deteriorate with increasing load, but are oth-
erwise independent of link capacity. Realized throughput
also decreases as users become more patient while goodput
stays the same. On the other hand, conﬁrming the positive
impact of a heavy-tailed size distribution noted in Section
7.1, both throughput and goodput improve as β decreases
from 2 to 1. This is explained by the fact that impatience
discriminates against elephants and interrupts these large
ﬂows after only a small fraction of their data has been tran-
ferred.
analysis
ns simulation
patience
10 s
20 s
throughput goodput
23.8 Kbit/s
11.9 Kbit/s
throughput goodput
55%
51%
55% 18.5 Kbit/s
55% 9.9 Kbit/s
Table 2: Per-ﬂow throughput and link goodput dur-
ing sustained overload
Table 2 compares evaluations of (16) and (17) with the re-
sults of ns simulations under the same impatience assump-
tions for a 1 Mbit/s link under 90% overload.
7.3 Reattempts
Aborted ﬂows are not generally abandoned immediately
as users will frequently make a repeat attempt. The impact
of this behavior is to exacerbate the loss of goodput due to
impatience as it is likely that the reattempts will also be
interrupted. Consider the following simple model.
User behavior is modeled by a size dependent patience
duration as introduced in the previous section.
If a user
aborts it reattempts with ﬁxed probability p. Reasoning as
above, in place of equation (15), the maximum completed
ﬂow size s(cid:2) now satisﬁes:
(cid:8) s(cid:2)
(cid:8) ∞
C = λ
s.dF (s) +
0
λ
1 − p
θδ(s)dF (s).
(18)
s(cid:2)
Figure 12 plots goodput U against p in the case of a 20%
overload assuming constant patience duration and a Pareto
size distribution.
1
0.8
0.6
0.4
0.2
t
u
p
d
o
o
g
k
n
L
i
0
0
0.2
0.4
0.6
Reattempt probability
0.8
1
Figure 12: Impact of reattempts on link goodput
The ﬁgure shows that the loss of goodput can be consid-
erable. While the model is overly simple, it does illustrate
the negative impact user behavior can have in case of over-
load. Network eﬃciency would gain from a more proactive
reaction to congestion.
7.4 Admission control
An alternative to allowing an overloaded link to stabilize
through impatience is to perform admission control [23].
If ﬂows arriving when the bandwidth they would achieve
is less than an acceptable threshold were rejected imme-
diately, there would be no cause for impatience. The ad-
vantage is that goodput is maintained close to 100% and
is unaﬀected by reattempts. On the other hand, admission
control would tend to increase the proportion of uncom-
pleted ﬂows since it applies equally to both mice and ele-
phants. Since elephants may well be more important than
mice, this is not necessarily a disadvantage.
Indeed, one
advantage of admission control is that it can be applied
selectively with diﬀerent admission thresholds applying to
diﬀerent classes of traﬃc (see [3]).
8. CONCLUSIONS
To evaluate the throughput performance of elastic trans-
fers it is necessary to account for the dynamic nature of
traﬃc. Traﬃc variations are most naturally modeled in
terms of ﬂows and sessions rather than packets whose com-
plex arrival process is largely determined by the closed-loop
control of TCP connections. We have demonstrated that
ﬂuid ﬂow statistical bandwidth sharing models can accu-
rately predict the results of ns packet-level simulations.
Using results from the theory of stochastic networks we
have shown, in a number of ideally fair bandwidth sharing
scenarios, that the distribution of the number of ﬂows in
progress and the expected ﬂow throughput have very simple
expressions which are valid under a wide range of realistic
traﬃc assumptions. These expressions depend essentially
only on expected demand and are independent of such char-
acteristics as the heavy-tailed ﬂow size distribution or the
self-similar ﬂow arrival process. Further evaluations nott
reported here lead us to believe that the broad conclusions
derived under an assumption of ideal fair sharing remain
true under moderate discrimination due to diﬀerent RTT,
for instance.
The expected ﬂow throughput achieved on a link of ca-
pacity C bits/s with utilization ρ is roughly equal to the
minimum of the residual capacity C(1 − ρ) and any rate
limit arising from external causes such as the bandwidth
available on other links, the user’s modem speed or the
size of the advertised TCP receive window. It follows that
performance is generally satisfactory as long as demand is
somewhat less than capacity (in which case demand is equal
to the measured load Cρ). This justiﬁes the usual provision-
ing procedures based on limiting utilization in the busiest
period while suggesting currently used limits of 60%, say,
may be overly conservative.
The stochastic network models are unstable when de-
mand exceeds capacity (the number of ﬂows in progress
would grow indeﬁnitely).
In practice, when overload oc-
curs, network utilization is necessarily stabilized through
user impatience and other reasons for aborting a session
or a ﬂow. Since an incomplete user transaction generally
implies bandwidth wastage, impatience leads to goodput
which may be appreciably less than capacity. According
to a simple model of user behavior, overload also brings
discrimination against larger ﬂows which are less likely to
sustain the resulting low throughput.
We suggest that the key to quality of service is to apply
adequate provisioning procedures coupled with traﬃc rout-
ing strategies designed to avoid demand overload. There
appears little scope for service diﬀerentiation beyond the
two broad categories of “good enough” and “too bad”.
Rather than relying on impatience to stabilize an over-
loaded link leading to quality which is too bad, it would
be preferable to perform admission control at ﬂow or ses-
sion level, maintaining suﬃcient throughput for admitted
ﬂows and avoiding bandwidth wastage on incomplete trans-
actions.
[17] A. Feldman, A. Gilbert, P. Huang, and W. Willinger.
Dynamics of IP traﬃc: A study of the role of
variability and the impact of control. In Proceedings
of ACM SIGCOMM’99, 1999.
[18] A. Jean-Marie and P. Robert. On the transient
behavior of the processor sharing queue. Queueing
Systems Theory and Applications, 17:129–136, 1994.
[19] F.P. Kelly. Reversibility and Stochastic Networks. J.
Wiley & Sons, 1979.
[20] F.P. Kelly, A. Maulloo, and D. Tan. Rate control for
communication networks: shadow prices, proportional
fairness and stability. Journal of the Operational
Research Society, 49, 1998.
[21] A.A. Kherani and A. Kumar. Performance analysis of
TCP with nonpersistent sessions. Preprint available
at
http://ece.iisc.ernet.in/˜anurag/Anurag Kumar.html,
2000.
[22] L. Kleinrock. Queueing Systems, Volume 2. J. Wiley
& Sons, 1975.
[23] L. Massouli´e and J.W. Roberts. Arguments in favor
of admission control for TCP ﬂows. In P. Key and
D. Smith, editors, Teletraﬃc Engineering in a
Competitive World, Proceedings of ITC 16, pages
33–44. Elsevier, 1999.
[24] L. Massouli´e and J.W. Roberts. Bandwidth sharing:
objectives and algorithms. In Proceedings of IEEE
INFOCOM’99, 1999.
[25] L. Massouli´e and J.W. Roberts. Bandwidth sharing
and admission control for elastic traﬃc.
Telecommunication Systems, 15:185–201, 2000.
[26] J. Mo and J. Walrand. Fair end-to-end window-based
congestion control. In Proceedings of SPIE’98
International Symposium on Voice,Video and Data
Communications, 1998.
[27] C.J. Nuzman, I. Saniee, W. Sweldens, and A. Weiss.
A compound model of tcp arrivals. In Proceedings of
ITC Seminar on IP Traﬃc Modeling, Monterey, 2000.
[28] V. Paxson and S. Floyd. Wide-area traﬃc: The
failure of poisson modeling. IEEE/ACM Trans.
Networking, 3(3):226–255, 1995.
[29] J.W. Roberts and S. Oueslati-Boulahia. Quality of
service by ﬂow-aware networking. Phil. Trans. R.
Soc. London A, 358:2197–2207, 2000.
[30] M. Vojnovic, J.-Y. Le Boudec, and C. Boutremans.
Global fairness of additive-increase and
multiplicative-decrease with heterogeneous round-trip
times. In Proceedings of IEEE INFOCOM’00, 2000.
9. REFERENCES
[1] E. Altman, C. Barakat, and K. Avrachenkov. A
stochastic model of tcp/ip with stationary random
losses. In Proc. of ACM SIGCOMM’00, 2000.
[2] F. Baskett, K.M. Chandy, R.R. Muntz, and F.G.
Pallacios. Open, closed and mixed networks of queues
with diﬀerent classes of customers. Journal of ACM
22, pages 248–260, 1975.
[3] S. Ben Fredj, S. Oueslati-Boulahia, and J.W. Roberts.
Measurement-based admission control for elastic
traﬃc. In Proceedings of ITC17, September 2001.
[4] A. Berger and Y. Kogan. Dimensioning bandwidth
for elastic traﬃc in high-speed data networks.
IEEE/ACM Trans Networks, 8(5):643–654, October
2000.
[5] D. Bertsekas and R. Gallager. Data Networks.
Prentice-Hall International, 1992.
[6] T. Bonald and L. Massouli´e. Impact of fairness on
internet performance. In Proceedings of ACM
SIGMETRICS’01, 2001.
[7] T. Bonald, A. Prouti`ere, G. R´egni´e, and
J.W. Roberts. Insensitivity results in statistical
bandwidth sharing. In Proceedings of ITC17,
September 2001.
[8] T. Bu and D. Towsley. Fixed point approximations
for TCP behavior in an aqm network. In Proceedings
of ACM SIGMETRICS’01, 2001.
[9] J.W. Cohen. The multiple phase service network with
generalized processor sharing. In Acta Informatica 12,
pages 245–284, 1979.
[10] M. Crovella and A. Bestravos. Self-similarity in world
wide web traﬃc: Evidence and possible cause. In
Proceedings of ACM SIGMETRICS’96, 1996.
[11] G. De Veciana, T.J. Lee, and T. Konstantopoulos.
Stability and performance analysis of networks
supporting services with rate control - could the
internet be unstable? In Proceedings of IEEE
INFOCOM’99, 1999.
[12] D.P.Heyman, T.V.Lakshman, and A.L.Neidhardt.
New method for analyzing feedback protocols with
applications to engineering web traﬃc over the
internet. In Proceedings of ACM SIGMETRICS’97,
1997.
[13] G. Fayolle, A. de la Fortelle, J-M. Lasgouttes,
L. Massouli´e, and J.W. Roberts. Best-eﬀort networks:
modeling and performance analysis via large
networks asymptotics. In Proceedings of IEEE
INFOCOM’01, 2001.
[14] G. Fayolle, I. Mitrani, and R. Iasnogorodski. Sharing
a processor among many classes. Journal of the
ACM, 27:519–532, 1980.
[15] A. Feldman. Characteristics of TCP connection
arrivals. K. Park, W. Willinger, editors, Self-similar
network traﬃc and performance evaluation, J. Wiley
& Sons, 2000.
[16] A. Feldman, A. Gilbert, P. Huang, and W. Willinger.
Data networks as cascades: Explaining the
multifractal nature of internet WAN traﬃc. In
Proceedings of ACM SIGCOMM’98, 1998.