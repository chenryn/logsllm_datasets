ˆ
ď
ˆ
σ 2
´
8σ
f PH
2
2 exp
¨˝´
˙
2 ` 4pb´aqε{3
ˆ
´
m ε
m ε
8 ` 4pb´aq{3
dε
m
σ 2
2 exp
ď
´
f PH
ď |H|8p2 ` pb´aq{3q
ii) σ 2 ą ε:
mq ´pVnpf ‹
ˆ
ż
E|Vpf ‹
σ 2
d
0
mq| ď
´
2 exp
2σ
ď
2σ
2p1 ` pb´aq{3q
“
3mσ
exp
ż
2 exp
σ 2
0
4p6 ` pb ´ aqq
ˆ
˙
´
2 ` 2pb´aqσ 2{3
b
?
π erf
¨˚˝
2σ
dε
nε
σ
2
2
dε
2
˙
,
2
nε
2 ` 2pb´aqε{3
˙
(55)
(56)
(57)
(58)
dε
(59)
(60)
˛‹‚,
n
b
2σ 2p1`pb´aq{3q
n
considering r “
And finally,
2σ 2p1`pb´aq{3q
, q “ 1 and applying lemma A.2.
ÿ
f PH
´
8σ
´
8σ
ď
ˆ
ˆ
‰
“
Vpf ‹
Vд ´ E
mq
ż
ż
ď 2|H|
d
ď 2|H|
“ |H|
σ 2
0
σ 2
0
8σ
exp
exp
2 ` 4σ 2pb´aq{3
m
ż
2
2
n
m ε
8σ
2 exp
σ 2
0
¨˝´
˙
f ` 4pb´aqε{3
˙
2 ` 4pb´aqσ 2{3
b
2 ` 4pb´aqε{3
?
π erf
¨˚˝
m ε
m ε
dε
dε
σ
2
2
2
8σ 2`4σ 2pb´aq{3
˛‚dε (61)
(62)
(63)
(64)
˛‹‚,
m
according to lemma A.2 with r “
b
8σ 2`4σ 2pb´aq{3
m
and q “ |H|.
□
B.3 Proof of Corollary 3.4
Corollary 3.4. The sample complexity of the ERM algorithm
д-vulnerability is bounded from above by the set of values satisfying:
Mpε, δq ď 8 σ
Npε, δq ď 2 σ
2 ` 4pb´aqε{3
2 ` 2pb´aqε{3
2
ε
2
ε
˙
ˆ
ˆ
ln
ln
2|H|
˙
δ ´ ∆
2
∆
,
,
(20)
(21)
´
|Vpf ‹
˙
mq ´pVnpf ‹
ˆ
mq| ě ε
¯
(65)
,
˙
2
´
n ε
2σ
2 ` 2pb´aqε{3
(66)
.
ď pδ ´ ∆q,
ď ∆,
(67)
(68)
which satisfies the desired condition:
P
(69)
for any 0 ă ∆ ă δ. Finally, from the previous inequality we can
derive lower bounds on n and m:
ď δ,
˙
m ě 8 σ
n ě 2 σ
2 ` 4pb´aqε{3
2 ` 2pb´aqε{3
ln
2
ε
2
ε
2|H|
˙
δ ´ ∆
2
∆
,
,
(70)
(71)
mq| ě ε
ˆ
ˆ
ln
which by definition of sample complexity shows the corollary. □
C PRE-PROCESSING
C.1 Data pre-processing
Theorem 4.1 (Correctness of data pre-processing). Given a
prior π, a channel C, and a gain function д, we have
Vдpπ , Cq “ α ¨ Vдidpξ , Eq ,
where α, ξ and E are those defined in (23), (24) and (25), respectively,
and дid is the identity function (cfr. section 2), i.e., the gain function
corresponding to the Bayesian adversary.
ÿ
max
w
Proof.
Vдidpξ , Eq “
ÿ
pξw Ewyq “
ÿ
ÿ
y
y
max
w
¨
α
“ 1
max
w
y
x
ÿ
ÿ
w1
y
ξw1 ¨ Ew1y ¨ дidpw, w1q “
ÿ
PW Ypw, yq “
max
w
max
w
y
Upw, yq
α
πx ¨ Cxy ¨ дpw, xq “ 1{α ¨ Vдpπ , Cq
(72)
(73)
(74)
□
C.2 Channel pre-processing
Theorem 4.2 (Correctness of channel pre-processing). Given
a prior π and a gain function д, we have that, for any channel C:
Vдpπ , Cq “ β ¨ Vдidpτ , RCq
for all channels C.
where β, τ and R are those defined in (27) and (28).
Proof. In this proof we use a notation that highlights the struc-
ture of the preprocessing. We will denote by G be the matrix form
ř
of д, i.e., Gwx “ дpw, xq, and by Ψπ the square matrix with π
in its diagonal and 0 elsewhere. We have that β “ }GΨπ}1 “
w,x Gwx πx , which is strictly positive because of the assumptions
on д and π. Furthermore, we have
τT “ β´1
GΨπ 1 ,
R “ β´1pΨτq´1
GΨπ ,
where 1 is the vector of 1s and τT represents the transposition of
vector τ. Note that pΨτq´1 is a diagonal matrix with entries τ´1
w
in its diagonal. If τw “ 0 then the row Rw,¨ is not properly defined;
but its choice does not affect Vдidpτ , RCq since the corresponding
prior is 0; so we can choose Rw,¨ arbitrarily (or equivalently remove
the action w, it can never be optimal since it gives 0 gain). It is easy
to check that τ is a proper distribution and R is a proper channel:
ř
ř
w τw “
x Rw,x “
w β´1ř
ř
ř
β´1
1
τw
Moreover, it holds that:
x
x Gwx πx
Gwx πx
β
“ β´1
“ τw
τw
“ 1 ,
“ 1 .
βΨτ R “ βΨτ β´1
Ψτ ´1
GΨπ “ GΨπ .
The main result follows from the trace-based formulation of poste-
rior д-vulnerability [4], since for any channel C and strategy S, the
above equation directly yields
Vдpπ , Cq “ max
S
trpGΨπ CSq “ β ¨ max
S
trpΨτ RCSq
“ β ¨ Vдidpτ , RCq ,
where trp¨q is the matrix trace.
C.3 Data pre-processing when д is not integer
Approximating д so that it only takes values P Qě0 allows us to
represent each gain as a quotient of two integers, namely
□
NumeratorpGw,xq{Denominator pGw,xq.
Let us also define
def“ lcmwxpDenominatorpGw,xqq,
(75)
where lcmp¨q is the least common multiple. Multiplying G by K
gives the integer version of the gain matrix that can replace the
K
original one. It is clear that the calculation of the least common
multiplier, as well as the increase in the amount of data produced
during the dataset building using a gain matrix forced to be integer,
might constitute a relevant computational burden.
D ANN MODELS
We list here the specifics for the ANNs models used in the exper-
iments. All the models are simple feed-forward networks whose
layers are fully connected. The activation functions for the hidden
neurons are rectifier linear functions, while the output layer has
softmax activation function.
The loss function minimized during the training is the cross
entropy, a popular choice in classification problems. The remapping
Y Ñ W can be in fact considered as a classification problem such
that, given an observable, a model learns to make the best guess.
For each experiments, the models have been tuned by cross-
validating them using one randomly chosen training sets among
the available ones choosing among the largest in terms of samples.
The specs are listed experiment by experiment in table 2.
E FREQUENTIST APPROACH DESCRIPTION
In the frequentist approach the elements of the channel, namely the
following way: the empirical prior probability of x,pπx , is computed
conditional probabilities PY|Xpy|xq, are estimated directly in the
the empirical joint probabilitypPX Ypx, yq is computed by counting
by counting the number of occurrences of x in the training set and
dividing the result by the total number of elements. Analogously,
by the total number of elements in the set. The estimation pCxy of
the number of occurrences of the pair px, yq and dividing the result
Cxy is then defined as pCxy “
pPX Ypx, yq
pπpxq