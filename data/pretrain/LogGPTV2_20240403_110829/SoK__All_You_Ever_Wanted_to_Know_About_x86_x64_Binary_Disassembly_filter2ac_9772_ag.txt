0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.01
0.00
0.01
0.01
0.01
80.80
0.01
28.38
9.89
92.98
99.99
6.83
Other
0.00
99.94
0.00
0.00
0.00
0.00
93.17
TABLE XII: Statistics of false negatives in function detection.
J-Tab, T-Call, Non-Ret, and FP-Overlap respectively represent
missing jump table targets, missing tail calls, missing non-
returning functions, and side effects of other false positives.
Tools
Dyninst
Ghidra
Ghidra-NE
Angr
Angr-NS
Bap
Radare2
Percentage of False Negatives (%)
J-Tab
0.03
0.02
0.01
0.08
1.18
1.32
1.12
T-Call
1.68
0.05
0.18
13.05
1.06
2.42
2.78
Non-Ret
FP-Overlap
No-Match
2.07
1.48
14.16
8.25
4.01
7.99
3.64
10.16
12.42
0.84
73.61
32.78
11.82
5.52
86.06
86.03
84.81
5.01
60.97
76.45
86.94
ular, BINARY NINJA can identify over 97% of the functions
with a precision over 95%. Second, NUCLEUS achieves com-
parable coverage and precision to BINARY NINJA, showing a
high promise of its CFG-connectivity based solution. Based on
an ofﬁcial blog [84], BINARY NINJA incorporates NUCLEUS
for CFG and function detection. Third, BYTEWEIGHT outper-
forms BAP despite BAP internally runs BYTEWEIGHT. This
is because BAP uses pre-trained signatures but BYTEWEIGHT
uses signatures trained with our benchmark binaries.
Use of Heuristics: In function entry identiﬁcation, two major
heuristics are used. The ﬁrst heuristic searches function entries
using common prologues/data-mining models. We summarize
the contribution and accuracy of the heuristic in Table XIX in
Appendix D. Without counting functions recursively reachable
from the matched ones, this heuristic recovers 17.36% more
functions with an average precision of 77.53%. Also observ-
able is that utility of this heuristic varies across optimization
levels and architectures. Moreover, existing tools use different
patterns or data-mining models, competing for a coverage-
accuracy trade-off. Comparing to GHIDRA-NE and RADARE2,
ANGR-NS and DYNINST use more aggressive patterns/models,
producing higher coverage (21.3%/24.02 v.s. 18.24%/7.93%) but
lower precision (56.61%/85.37% v.s. 98.42%/87.29%).
The other heuristic, used by ANGR,
takes the begin of
each code region detected by linear scan as a function entry.
The heuristic recovers 23% more functions but reduces the
precision by 26.96% because it often considers the begin of
padding or data-in-code as a function entry.
Understanding of Errors: Table XI and XII present
the
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
843
analysis of false positives and false negative (the analysis of
GHIDRA is only done on binaries with exception information).
For false positives, there are three common causes. First,
the signature-based detection wrongly matches function entries
(FP ratio - DYNINST: 80.80%, GHIDRA-NE: 28.38%, ANGR-NS: 92.98%,
BAP: 99.99%). Second, inaccurate tail call detection takes target
of a regular jump as a function entry (FP ratio - DYNINST: 19.20%,
GHIDRA-NE: 71.61, ANGR: 11.70%). Third, incorrect disassembly re-
sults in erroneous call instructions to incorrect target functions
(FP ratio - GHIDRA-NE: 0.01%, ANGR-NS: 0.01%, RADARE2: 0.01%).
Beyond the three causes, ANGR produces 78.41% of its false
positives because it considers code discovered by its linear
scan as a function entry; GHIDRA generates 99.94% of its false
positives because exception information also carries pointers
to middle of functions; RADARE2, aggressively inferring code
pointers based on xrefs, brings 93.17% of its false positives.
False negatives also have a group of similar causes. First,
tools can miss targets of jump tables and fail to identify
functions called by the target code or their successors (FN
ratio - ANGR-NS: 1.18%, BAP: 1.32%, RADARE2: 1.12%). Jump tables
have smaller impacts to ANGR and DYNINST because ANGR
uses linear scan to compensate jump tables and DYNINST has
high coverage of jump tables. Second, tools cannot recognize
many tail calls and hence, miss the functions indicated by their
targets (FN ratio - ANGR: 13.05%, BAP: 2.42%, RADARE2: 2.78%).
Third, missed non-returning functions prevent the detection of
many function entries in cases like Listing 8 in Appendix G
(FN ratio - GHIDRA-NE: 14.16%, ANGR: 8.25%, ANGR-NS: 4.01%, BAP:
7.99%). Fourth, wrongly identiﬁed functions can over-lap with
true functions, making the true ones un-recognizable (FN ratio
- DYNINST: 10.16%, GHIDRA: 12.42%, ANGR: 73.61%, ANGR-NS: 32.78%).
All the other functions are missed because they are neither
reached by recursive descent nor matched by patterns (FN
ratio - ANGR-NS: 60.97%, GHIDRA: 86.03%, RADARE2: 86.94%).
4) CFG Reconstruction : In this part, we measure 5 targets:
(1) intra-procedure edges between basic blocks; (2) call graphs
for direct calls; (3) indirect jumps and indirect calls; (4) tails
calls; (5) non-returning functions. For task (1), we exclude the
edge between a call and the fall through code. For jump tables
in task (3), we consider a case with no targets resolved as a
false negative. All other cases are counted as false positives.
For task (4), we exclude recursive calls and we count unique
target functions instead of jumps.
Overall Performance: Table XIII and XIV show the results of
CFG reconstruction on Linux binaries and Windows binaries.
First, the tools can recover most of the edges with high
accuracy. DYNINST, GHIDRA, and ANGR ﬁnd over 90% of
the edges with an accuracy higher than 95%. Moreover, the
recovery of edges highly correlates to the recovery of instruc-
tions: precision and recall in the two tasks are consistent. The
results of call graphs are similar to, so we omit the details.
Second, tools have different capabilities of handling jump
tables. On average, GHIDRA and DYNINST can resolve over
93% of the jump tables with an accuracy of around 90%.
RADARE2 and ANGR have a similar coverage rate (around 75%)
while ANGR has much higher accuracy (96.27% v.s. 90%). In
TABLE XIII: Results of CFG reconstruction on Linux. Edge,
CG, T-Call, N-Ret, and J-Tab respectively mean edges, call
graphs, tail calls, non-returning functions, and jump tables.
J-Tab
Rec
Pre
T-Call
Rec
Pre
CG
Rec
Pre
N-Ret
Rec
Pre
Edge
Pre
Rec
97.28 97.10 99.99 99.01 81.91 71.15 100.0 86.89 99.97 98.82
98.61 92.41 99.99 91.17 68.75 78.84 100.0 86.04 99.87 99.05
98.88 91.58 99.99 90.34 59.64 71.12 100.0 86.20 99.81 99.10
98.93 90.14 99.99 86.92 67.74 68.79 100.0 86.33 99.89 98.50
98.87 91.33 99.99 90.26 58.93 68.60 100.0 85.08 99.93 98.75
99.63 97.41 99.99 96.66 90.98 79.42 100.0 50.19 98.49 75.58
99.89 89.61 99.99 88.91 95.93 97.91 99.98 71.21 97.46 97.55
99.89 89.30 99.99 88.59 92.75 96.10 100.0 67.15 91.44 97.71
99.88 90.12 99.99 88.88 99.01 99.02 100.0 59.74 99.03 91.53
99.90 89.47 99.99 88.37 92.65 94.74 100.0 69.59 91.70 97.81
98.41 98.13 99.99 99.93 34.09 57.80 97.62 81.25 99.80 87.34
94.88 94.95 99.99 99.98 91.20 77.46 89.57 80.48 89.21 70.07
95.23 95.16 99.99 99.98 87.46 80.97 90.48 80.72 89.12 73.42
95.53 96.44 99.99 99.95 84.37 96.05 93.04 83.05 98.62 81.07
95.12 95.10 99.99 99.95 87.34 79.13 88.98 78.28 90.23 73.22
100.0 75.61
95.31 76.12 99.99 85.00
99.18 76.66
91.76 55.48 99.99 78.05
99.12 71.40
92.00 55.97 99.99 77.69
100.0 79.37
94.03 72.00 99.99 80.42
99.24 68.66
91.95 56.54 99.99 77.84
83.45 83.51 55.74 43.14
98.47 83.29 99.99 92.67
96.93 79.80 98.11 97.78
98.07 85.75 99.99 81.02
96.85 79.25 98.52 98.11
98.23 83.09 99.99 76.96
96.63 79.56 82.53 76.24
98.67 80.01 99.99 86.99
98.42 86.97 99.99 77.79
97.03 76.25 98.51 97.26
99.38 97.39 99.99 97.60 93.05 90.82 100.0 89.18 99.95 99.99
99.32 92.74 99.99 90.35 72.57 87.79 100.0 89.32 99.88 99.59
99.36 92.88 99.99 89.89 93.65 92.74 100.0 89.11 99.89 99.71
99.34 95.78 99.99 95.12 75.62 85.83 99.98 90.67 99.61 99.92
99.33 93.99 99.99 92.07 73.62 81.82 100.0 87.86 99.68 99.75
99.99 99.46 99.99 99.75 54.98 87.69 100.0 86.28 99.60 99.30
99.32 98.27 99.99 99.27 83.14 94.85 100.0 86.78 98.91 95.81
99.20 94.97 99.99 98.89 84.18 93.37 100.0 86.84 98.67 89.87
99.58 99.05 99.99 99.55 87.24 96.91 100.0 90.13 99.02 96.36
99.39 95.87 99.99 98.85 84.79 90.93 100.0 86.65 98.77 90.26
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
L
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
O0
O2
O3
Os
Of
t
s
n
i
n
y
D
a
r
d
i
h
G
r
g
n
A
p
a
B
2
e
r
a
d
a
R
A
D
I
a
j
n
N
i
comparison to open-source tools, commercial tools have both
higher coverage (96.5% v.s. 84.8%) and accuracy (99% v.s. 92.96%).
Besides jump tables, there are two more types of indirect
jumps: handle-written assembly code (336 cases like [48]) and
indirect tail calls (35,087 cases). For the ﬁrst type, GHIDRA,
by analysing constant propagation, resolves 96 cases but only
report one target in each case. BINARY NINJA generates results
for 120 cases, however, with incorrect targets.
Third, ANGR, GHIDRA, IDA PRO, and BINARY NINJA have
(limited) supports of indirect calls. ANGR resolves 43 cases
but only reports one target in each case. Our manual analysis
veriﬁed the targets are correct, all following the pattern of
[mov/lea CONST, reg; ...; call reg]. GHIDRA
ﬁnds an incomplete set of targets for 88,078 indirect calls (only
one target for 87,947 cases). IDA PRO reports results for 15,325
indirect calls (only one target for 15,267 cases). The other 58 cases
follow the format of Listing 9 in Appendix G, which are fully
solved by IDA PRO. BINARY NINJA reports targets for 3,410
indirect calls (1 target for 92 cases; 2 targets for 1,865 cases; 3 targets for
469 cases; 4 targets for 889 targets; 4+ targets for 95 cases).
Fourth, existing tools are not perfect with detecting tail
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
844
TABLE XIV: Results of CFG reconstruction on Windows.
J-Tab
Rec
Pre
W Edge
T-Call
Rec
Pre
N-Ret
Rec
Pre
CG
Rec
Rec
Pre
p
a
B
r
g
n
A
a Od
r
O1
d
i
O2
h
G
Ox
Od
O1
O2
Ox
Od
O1
O2
Ox
2 Od
e
O1
r
a
O2
d
a
Ox
R
Od
O1
O2
Ox
Od
O1
O2
Ox
a
j