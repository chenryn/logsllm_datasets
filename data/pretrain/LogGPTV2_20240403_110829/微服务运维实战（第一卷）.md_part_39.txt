大多数实施某种自我修复系统的组织都侧重于反应式自我修复。当系统检测
15.1自我修复等级和类型303
---
## Page 316
304第15章自我修复系统
知）
统需要重新启动应用程序，并希望开发人员能够解决这个问题（因此需要使用通
少意味着什么？这也有可能是内存泄漏，在这种情况下，当达到某些阈值时，系
时间过长。比如，从部署服务时开始内存使用量稳步增长而新版本发布时突然减
为考虑对象，并推断出每周一的业务流量突然增加，提前扩展服务，以防止反应
明确的迹象，
一个小时内，内存使用量一直在稳步提高，达到了比如90%的关键点。这是一个
后通常采用复杂的算法来评估趋势，得出结论。例如，我们可能会看到，在最后
来也复杂得多。我们需要把信息（响应时间、CPU、内存等）存储在某个地方，然
们容易扩展或者收缩。而单块应用如果选择这种策略，通常问题会更多。
要。如果应用微服务架构，这可能是一个小问题，因为它们很小，易于移动。它
临时增加的业务流量，而（在流量高峰过去之后）下一次检查将会推测有收缩的必
请求和响应之间花费的时间很长，这可能确实需要扩展的标志，但原因也可能是
通过考虑当前状况来预测未来，要面临的问题是情况为多元的，变数很多。如果
可能需要收缩服务，
展该服务，甚至反过来也可以这样。同样，如果接收响应只要不到100毫秒，那么
果某个用于检查服务运行状况的HTTP请求的响应超过500毫秒，那么可能需要扩
些问题。我们如何预测未来？更准确地说，我们用什么数据预测未来？
如果把历史数据记入考虑范围，预防式自我修复会更加可靠，但同时实现起
相对简单但不太可靠的预测方法是根据（近乎）实时数据进行预测。例如，如
下面我们改变焦点，转而讨论架构。
，表示导致这种增加的服务需要扩展。系统也可以将更长一段时间作
，并将这些资源重新分配给另一个可能需要更多资源的服务。
---
## Page 317
15.2
Apache Mesos,
工具可以帮助我们以更好的方式进行编排。使用 Docker Swarm、Kubernetes 或
致资源使用非常不平衡，如图15-5所示。
情况下，管理集群内的服务是很单调枯燥的。它需要很长时间，而且最终往往导
果有一个停电，另一个还可以运行。然而，在许多情况下，在财务上这不可行。
财务上可行，那么至少要有两个物理上和地理位置上分开的数据中心。这样，如
的情况下从一块硬件转移到另一块硬件。请记住，我们总是要评估性价比。如果
可以由两到两百个服务器组成（见图15-4）。规模不重要，重要的是能在故障发生
我们就无能为力，因为没有现成的替代品。因此，系统在最开始必须是集群，它
集以个多器
素。
大多数情况下，人们将集群视为一组单独的服务器，这是错误的，现在有些
容器
最开始，要有一个集群。单个服务器没有容错功能。如果一块硬件故障了，
一旦将集群启动并运行，就可以开始部署我们的服务。但是，在没有编排的
无论内部流程和工具如何，每一个自我修复系统都会有一些共同的元
图15-5自我修复系统架构：服务部署到集群，但是资源使用率非常不平衡
可一的务合
Self-Healing Architecture
自我修复架构
容器
，可以解决集群中的编排问题。集群编排很重要，不仅是为了简化
图15-4自我修复系统架构：一切都从一个集群开始
容器
容器
容器
容器
集群
容器
集群
容器
容器
容器
容器
容器
15.2自我修复架构305
88
8
---
## Page 318
306
第15章自我修复系统
送到服务注册表，如图15-8所示。
注册和注销服务。无论选择哪种工具，
能满足我们的需求。与服务注册一样，一些集群编排工具也已经通过自己的方式
时更新注册表。有很多工具可以做到这一点。我们已经熟悉了Registrator，它完全
中。原理很简单，需要有机制监控硬件和服务，当有新的加入或者原有的被删除
可以与好几种注册工具协作，而Kubermetes 用户则必须绑定使用 etcd，如图 15-7
而在其他情况下，集群编排工具自带服务注册。例如，Docker Swarm 比较灵活，
具，如Consul、etcd 或 Zookeeper。在某些情况下，可以自己选择服务注册工具，
式，从手动维护的配置文件到传统的数据库，再到高可用的分布式服务注册工
状态。监视它们的唯一方式是获取其存在的信息。信息可以有许多种不同的形
载，从而避免系统重新部署失败实例时产生停机时间，如图15-6所示。
服务器上。在这种情况下，如果一个实例发生故障，其他实例也可以接管其负
所示。
新部署到健康节点上。请记住，对每个服务，至少需要运行两个实例在一个代理
服务部署，
编测群给部
无论选择哪个服务注册工具，下一个问题是将信息放入选择好的服务注册表
所有自我修复系统的基础在于监视已部署的服务或应用程序以及底层硬件的
根策服
监集据略务
，而且可以在出现故障（无论是软件性质还是硬件性质）的情况下快速重
容器
容器
容器
容器
主要需求是能够实时监控集群并将信息发
容器
器
集群编排器
部署服务
集群
容器
容器
容器
容器
容器
容器
8
---
## Page 319
统的环境中，Nagios 或Icinga（仅举几例）也可以承担这一角色，如图15-9所示。
个角色，而Kubernetes 和Mesos 为这种类型的任务提供了它们自己的工具。在更传
时刻所期望的状态是什么，
息，我们可以使用一些健康监视工具来检测异常。
图15-8
监当服除报表
图
注务
15-7
串
现在集群里有服务启
集署或服给
者务注
自我修复系统架构：
新移就册
自我修复系统架构：监控系统状态的主要需求是将系统信息存储在服务注册表里
容器
服务注册表
容器
容器
容器
服务注册表
启动和运行，
发送服务信息
监测服务
也要知道实际情况是什么。Consul Watches 可以承担这
：如果没有监测系统存储新信息的机制，服务注册表就失去作用
容器
容
容器
容器
器
并且服务注册表中已经写入了有关系统的信
#
容器
容器
容
容
集群编排器
部署服务
集
部署服务
器
群编排
集群
集群
容
容
器
这样的工具不仅要知道在每个
器
器
容
容
容
容
器
容
容
器
器
农容
器
15.2
容器
容器
自我修复架构
88
88
←307
---
## Page 320
308
第15章自我修复系统
将增加负载。我们可以从错误中学习，并教会系统在某些情况下该如何表现。这
达到某个点，需要我们对服务进行扩展。我们可以预测到，即将推出的营销活动
天中午会不会断电。然而，有时候，我们可以看到业务量在不断增加，很快就会
在某些情况下则不能。我们不知道明天硬盘会不会出现故障，我们也预测不了今
步，实现预防式自我修复吗？可以预测未来按指示行动吗？在许多情况下可以，
测到故障，则采取纠正措施，这又会使系统恢复到所需状态。我们可以再进一
消息，从而启动纠正措施的工具，Jenkins 足够完美，如图15-10所示。
复到旧版本的服务，等等。我们已经采用了Jenkins，作为可以从健康监视器收到
施没这么简单。应该有一种机制，可以通知有关方面，记录发生了什么事情，恢
器（临时）也会将该服务重新部署到健康节点来修复故障。大多数情况下，纠正措
信号，而这又会重新部署失败的服务。即使故障是由硬件问题引起的，集群编排
会发送一条消息来执行纠正措施。至少这个纠正措施应该向集群编排器发送一个
图15-9自我修复系统架构：所有的相关信息都存储在服务注册表里，一些健康监测工具
观察状态是否正常
到目前为止，这个过程只涉及反应式自我修复。我们持续监控系统，如果检
下一个难题是能够有执行纠正措施的工具。当健康监视器检测到异常时，它
容器
可以利用这些信息来验证当前状态是否正常
容器
Registrator
服务注册表
监测服务
发送服务信息
容器
容器
##
容器
容器
集群编排器
健康监测
部署服务
监测服务
集群
容器
容器
容器
容器
容器
容器
88
---
## Page 321
非常庞大，
图15-10
能，这使得我们能够执行一些分析操作，如图15-11所示。
较少的数据，享受到分发的好处也有限，而预防式自我修复需要更大的存储和功
存储数据的地方和收集这些数据的过程。与服务注册不同，服务注册只处理相对
样一组过程需要的基本要素与在反应式自我修复使用的要素相似。我们需要一
与 registrator 服务类似，还需要一些数据收集器发送历史数据。历史数据可能
容器
观察状态是否正常
容
观察状态是否正常
发送服务信息
器
容器
Regostrator
服务注册表
容
监测服务
Registrator
服务注册表
，包括但不限于CPU、HD、
自我修复系统架构：纠正措施至少应该向集群编排器发送信号重新部署失败的服务
器
图15-11
发送服务信
监测服务
容器
容器
容
容器
器
自我修复系统架构：预防式修复需要分析历史数据
息
容
容器
监测服务
容
容
集群编排
部署服务
器
健康监测器
集
器
器
健康监测器
部署服务
监测服务
集群
群编排
集
历史数据
网络流量、
观察趋势
容器
器
容