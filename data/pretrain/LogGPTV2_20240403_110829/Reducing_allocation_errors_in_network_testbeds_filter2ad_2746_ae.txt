the failure rate starts small and increases to almost 6% by the end of
the simulation. Curves for different ﬂavors of assign+ have the
similar shape but the failure rate is always below that for assign,
reaching 4.7% at the end. Overall, assign+ creates only 77%
of allocation failures generated by assign. assign-like score
calculation outperforms the one that depends only on user request
frequency.
In the rest of the paper we use assign+.1 under
assign+ label.
)
s
(
e
m
i
t
n
u
R
 100
 10
 1
 0.1
 0.01
 0.001
assign
assign+
 0
 20
 40
 60
 100  120  140  160  180  200
 80
Nodes in topology
Figure 7: Runtime versus topology size
Figure 7 shows the runtime of assign and assign+ versus
topology size, with y-axis being in the log scale. For both algo-
rithms the runtime depends on the size and complexity of the virtual
topology, and the number and diversity of available nodes on the
testbed. We show the average of runtimes for each topology size
and the error bars show one standard deviation around the mean.
Nodes in a topology
Figure 8: Interswitch bandwidth versus topology size
Figure 8 shows the interswitch bandwidth allocated by assign
and assign+ versus topology size. We group allocations into
bins based on the virtual topology size, with step 10, and show
the mean, with error bars showing one standard deviation. Here
too, assign+ signiﬁcantly outperforms assign on each topol-
ogy size. On the average, assign+ allocates only 23% of the
interswitch bandwidth allocated by assign .
)
s
(
e
m
i
t
n
u
R
assign
assign+
 10000
 1000
 100
 10
 1
 0.1
 0.01
 10
 100
 1000
 10000
Nodes
Figure 9: Scalability
We further test the scalability of both algorithms by using Brite
[16] to generate realistic, Internet-like topologies of larger sizes.
These topologies are more complex and more connected than those
found in most testbed experiments [9] and thus challenge allocation
algorithms. We generate 10 topologies each of the size 10, 20, 50,
100, 200, 500, 1,000, 2,000, 5,000 and 10,000 nodes. For each node
we request pcvm type (virtual machines), making it possible to al-
locate large topologies on our limited testbed architecture. Each
allocation request runs on an empty testbed. Figure 9 shows means
and standard deviations for runtime of assign and assign+ ver-
sus the topology size, both on log scale. assign+ again outper-
forms assign having about 10 times shorter runtime. assign
further fails to ﬁnd a solution for 10,000-node topologies, while
504assign+ ﬁnds it. assign+ only allocates interswitch bandwidth
in 5,000 and 10,000 node cases, while assign allocates it for
much smaller topologies. We believe that mechanisms that limit
assign’s runtime for large topologies interfere with its ability to
ﬁnd a good solution that minimizes the interswitch bandwidth.
8.4 Alternative Types
We now assume that users request speciﬁc node types because
they need some well-provisioned resource such as a large disk or
a fast CPU. We explore if we can further improve resource alloca-
tion success by expanding experimenter’s node type constraint to
equivalent or better hardware. In real deployment this would have
to be done only with user’s consent. We only consider disk, CPU
and memory speciﬁcations. We start from DeterLab’s node types
in January 2011, as shown in Table 2, and identify for each type
alternative types that have same or better features in these three
dimensions.
Figure 10 shows the effect of using alternative types on allocation
It creates
success of assign+, under the label assign+.at.
68.1% of failed allocations generated by assign.
)
1
-
0
(
t
e
a
r
e
r
u
l
i
a
F
 0.05
 0.04
 0.03
 0.02
 0.01
 0
 0
assign+
assign+.at
assign+.mig
assign+.atmig
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
Instances (thousands)
Figure 10: Allocation failure rates for assign+, when using
alternative types and migration
9. CHANGING ALLOCATION POLICY
The approaches in the previous section change the resource al-
location strategy, but do not change the allocation policy on test-
beds that allows users to hold resources for arbitrarily long times
without interruption. In this section we investigate if changes in re-
source allocation policy would further improve allocation success.
We examine two such changes:
1. Experiment migration – where running instances can be mi-
grated to other resources in the testbed (that still satisfy user-
speciﬁed constraints) to make space for the allocating in-
stance.
2. Fair sharing – where running instances can be paused if they
have been running for a long time and are currently idle, so
their resources can be borrowed by an allocating instance.
We acknowledge that either of these changes would represent a ma-
jor shift in today’s network testbeds’ philosophy and use policy. Yet
such shift may be necessary as testbeds become more popular and
the demand on their resources exceeds capacity. Our work helps
evaluate potential beneﬁts of such policy changes.
Further, the above changes are potentially disruptive for some
instances that rely on the constancy of hardware allocated to them
for the duration of their lifetime. We assume that users would have
mechanisms to opt out of these features, i.e. they could mark their
experiment as “do not migrate” or “do not pause”. We further
assume that, if the beneﬁts of these policy changes seem signif-
icant, testbeds would develop mechanisms to seamlessly migrate
or pause and restart instances that would be minimally disruptive
to users. Finally, we emphasize that instances could be migrated
and/or paused only during idle times, when allocated machines ex-
hibit no signiﬁcant terminal, CPU, disk or network activity. Emulab
software looks for such idle machines each 10 minutes, and records
each machine’s state (idle/non-idle) in the database, overwriting the
previous state. We have collected these reports for one year to in-
vestigate the extent of idle time in instances. Figure 11 shows the
distribution of the total idle time in four classes of instances: those
that last  7 days. All instances have
signiﬁcant idle time and long-lived instances are often idle for more
than a week! This leads us to believe that our proposed policy modi-
ﬁcations would apply to many instances and would not disrupt their
existing dynamics.
e
m
i
t
e
d
l
i
n
e
v
g
i
h
t
i
w
s
e
c
n
a
t
s
n
i
f
o
t
e
g
a
n
e
c
r
e
P
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
 7 days
 7 days
Instance duration
Figure 11: Idle times for instances of different duration
9.1 Migration
We now explore how much we can improve resource allocation
by performing experiment migration. This would include stopping
some allocated instances and moving their conﬁguration and data to
other testbed machines to “make space” for the allocating instance.
While there exist techniques in distributed systems that can mi-
grate running processes to another physical node [17], we propose a
much lighter-weight migration that would only move instances that
are idle at the time. The simplest implementation of such migration
would be to image all disks of the experimental machines, and load
those images to new machines, but that would require a lot of time
and disk space.
We test the migration on our “2011 synthetic setup”. If a reg-
ular resource allocation fails, we identify any instance that holds
node types requested by the allocating instance and is idle as the
migration candidate. We then order candidates from the smallest
to the largest and attempt to migrate them one at a time. To do so
505we reclaim resources from the migration candidate, try to allocate
the allocating instance, and then try to reallocate the migration can-
didate. Allocation succeeds only if both of these actions succeed.
Otherwise we restore the old state and try the next candidate. We
only record a failure for the allocating instance if all migrations fail.
In real deployment this can be easily simulated, without disturbing
any instances, until the successful combination is found.
Figure 10 shows the allocation failure rate when using migration
during assign+ under assign+.mig label, and when combin-
ing migration and alternative types, under assign+.atmig label.
We assume that a migration candidate is always idle in our simula-
tion. Migration lowers the allocation failure rate to 59.6% of that
generated by assign. Adding the alternative types to migration
has a minor effect, lowering the allocation failure rate to 57.3% of
that generated by assign.
Suggestion 5: Testbeds need better state-saving mechanisms that
go beyond manual disk imaging, to support migration.
9.2 Fair-Sharing
When demand for a shared resource exceeds supply, a usual ap-
proach is to enforce fair sharing and penalize big users. Traditional
fair sharing where every user (or in testbed case every project) re-
ceives a fair share of resources works well when: (1) users have
roughly similar needs for the resource, or (2) the demand does
not heavily depend on the resource allocation success and jobs are
scheduled in ﬁxed time slots. In the ﬁrst case, one can implement
quotas on use, giving each user the same amount of credit and re-
plenishing it on periodic basis. In the second case, one can imple-
ment fair queuing (e.g., [22]), allocating jobs from big users when-
ever there is leftover resource from the small ones. Unfortunately,
neither of these approaches works well for testbeds.
Many measures of testbed usage exhibit heavy-tail properties that
violate the ﬁrst assumption about users having similar needs. For
example, the distributions of instance size and duration is heavy-
tailed (Figure 12(a) and 12(b)): most instances are short and small,
but there are a few very large or very long instances that dominate
the distribution. If we assume that fairness should be measured at
the project level, heavy tail property manifests again. The distribu-
tion of node-hours per project (Figure 12(c)), obtained by summing
the products of size and duration in hours for all its instances, is
also heavy tailed. The second assumption is violated because most
fair-sharing algorithms are designed for ﬁxed-size jobs while test-
bed instances have a wide range of durations that are not known in
advance.
We now quantify the extent of unfairness on DeterLab testbed.
We deﬁne a project as “unfair” if it uses more than its fair share
of PCs in a week. While we focus on PC use, similar deﬁnitions
can be devised for speciﬁc node types. We choose a week-long
interval to unify the occurrence of heavy use due to any combina-
tion of large instances, long instances or many parallel instances in
a project. A fair share of resources is deﬁned as total number of
possible node-hours in a week, taking into account available and
allocated PCs, divided by the total number of active projects in that
week. A project can be classiﬁed as unfair one week and fair the
other week.
Figure 13 shows the percentage of total possible node-hours used
by unfair and by fair projects each week during 2011. There are 114
projects active during this time, each of which has been fair at some
point during the year; 27 projects have also been unfair. There were
total of 3,126 TEMP failures in 2011, averaging 58.7 failures per
project when it is unfair, and 26.1 failures per project, when it is
fair. While an unfair project has more than double the errors of a
fair project, this is expected, because unfair projects request more
)
X
>
x
(
P
)
X
>
x
(
P
 1
 0.1
 0.01
 0.001
 0.0001
 1e-05
 1
 1