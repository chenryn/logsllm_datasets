### Failure Rate Analysis

The failure rate for the assign algorithm starts relatively low and increases to nearly 6% by the end of the simulation. The curves for different variants of the assign+ algorithm exhibit a similar trend, but with a consistently lower failure rate, reaching only 4.7% at the end. Overall, the assign+ algorithm generates only 77% of the allocation failures compared to the assign algorithm. Additionally, the score calculation method used in assign+ outperforms the one that relies solely on user request frequency.

For the remainder of this paper, we will use the assign+1 variant, which is labeled as assign+.

### Runtime versus Topology Size

Figure 7 illustrates the runtime of the assign and assign+ algorithms as a function of topology size, with the y-axis on a logarithmic scale. Both algorithms' runtimes are influenced by the size and complexity of the virtual topology, as well as the number and diversity of available nodes in the testbed. The figure displays the average runtime for each topology size, with error bars representing one standard deviation around the mean.

### Interswitch Bandwidth versus Topology Size

Figure 8 compares the interswitch bandwidth allocated by the assign and assign+ algorithms across different topology sizes. We grouped the allocations into bins based on the virtual topology size, with a step size of 10, and plotted the mean values. Error bars indicate one standard deviation. The results show that assign+ significantly outperforms assign, allocating only 23% of the interswitch bandwidth that assign uses on average.

### Scalability Testing

To further evaluate the scalability of both algorithms, we used Brite [16] to generate realistic, Internet-like topologies of varying sizes. These topologies are more complex and interconnected than those typically found in testbed experiments [9], providing a challenging environment for allocation algorithms. We generated 10 topologies for each of the following sizes: 10, 20, 50, 100, 200, 500, 1,000, 2,000, 5,000, and 10,000 nodes. For each node, we requested the pcvm type (virtual machines), enabling the allocation of large topologies on our limited testbed architecture. Each allocation request was executed on an empty testbed. Figure 9 presents the means and standard deviations of the runtime for both algorithms, with both axes on a logarithmic scale. Assign+ outperformed assign, with a runtime approximately 10 times shorter. Additionally, assign failed to find a solution for 10,000-node topologies, while assign+ succeeded. In the 5,000 and 10,000 node cases, assign+ only allocated interswitch bandwidth, whereas assign did so for much smaller topologies. We hypothesize that mechanisms designed to limit assign’s runtime for large topologies may interfere with its ability to find a solution that minimizes interswitch bandwidth.

### Alternative Node Types

We explored the potential for improving resource allocation success by expanding the experimenter's node type constraints to include equivalent or better hardware. This would require user consent in real-world deployments. We considered disk, CPU, and memory specifications. Starting from DeterLab’s node types in January 2011, as shown in Table 2, we identified alternative types that offer the same or better features in these three dimensions.

Figure 10 demonstrates the effect of using alternative types on the allocation success of assign+, labeled as assign+.at. This approach reduces the number of failed allocations generated by assign to 68.1%.

### Changing Allocation Policy

Previous sections focused on changing the resource allocation strategy without altering the allocation policy, which allows users to hold resources for extended periods without interruption. In this section, we investigate whether changes in the allocation policy could further improve allocation success. We examine two such changes:

1. **Experiment Migration**: Running instances can be migrated to other resources in the testbed (still satisfying user-specified constraints) to make space for new allocations.
2. **Fair Sharing**: Running instances can be paused if they have been running for a long time and are currently idle, allowing their resources to be borrowed by new allocations.

These changes represent a significant shift in the philosophy and use policy of today’s network testbeds. However, such a shift may be necessary as testbeds become more popular and demand exceeds capacity. Our work evaluates the potential benefits of these policy changes.

#### Experiment Migration

We tested the impact of experiment migration on our "2011 synthetic setup." If a regular resource allocation fails, we identify any idle instance holding the node types requested by the new allocation as a migration candidate. Candidates are ordered from smallest to largest, and we attempt to migrate them one at a time. We reclaim resources from the migration candidate, try to allocate the new instance, and then try to reallocate the migration candidate. Allocation succeeds only if both actions succeed; otherwise, we restore the old state and try the next candidate. We record a failure for the new instance only if all migrations fail.

Figure 10 shows the allocation failure rate when using migration during assign+, labeled as assign+.mig, and when combining migration with alternative types, labeled as assign+.atmig. Assuming migration candidates are always idle, migration reduces the allocation failure rate to 59.6% of that generated by assign. Adding alternative types to migration has a minor effect, reducing the failure rate to 57.3% of that generated by assign.

**Suggestion 5:** Testbeds need better state-saving mechanisms beyond manual disk imaging to support migration.

#### Fair-Sharing

When demand for shared resources exceeds supply, fair sharing is often enforced. Traditional fair sharing, where every user (or project) receives a fair share of resources, works well when:
1. Users have similar needs for the resource.
2. Demand does not heavily depend on resource allocation success, and jobs are scheduled in fixed time slots.

In the first case, quotas can be implemented, giving each user the same amount of credit and replenishing it periodically. In the second case, fair queuing can be used, allocating jobs from big users whenever there is leftover resource from small ones. Unfortunately, neither approach is suitable for testbeds.

Testbed usage often exhibits heavy-tail properties, violating the assumption of similar user needs. For example, the distributions of instance size and duration (Figure 12(a) and 12(b)) are heavy-tailed, with most instances being short and small, but a few very large or long instances dominating the distribution. If fairness is measured at the project level, the heavy-tail property persists. The distribution of node-hours per project (Figure 12(c)), obtained by summing the products of size and duration in hours for all instances, is also heavy-tailed. Additionally, most fair-sharing algorithms are designed for fixed-size jobs, while testbed instances have a wide range of durations that are not known in advance.

To quantify the extent of unfairness on the DeterLab testbed, we defined a project as "unfair" if it uses more than its fair share of PCs in a week. A fair share of resources is defined as the total number of possible node-hours in a week, divided by the total number of active projects. A project can be classified as unfair one week and fair another week.

Figure 13 shows the percentage of total possible node-hours used by unfair and fair projects each week during 2011. There were 114 active projects, each fair at some point during the year, with 27 projects also being unfair. There were 3,126 TEMP failures in 2011, averaging 58.7 failures per project when it was unfair and 26.1 failures per project when it was fair. Unfair projects had more than double the errors of fair projects, which is expected given their higher resource requests.