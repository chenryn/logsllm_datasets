Precision
Recall
F1-Score
1
2
3
5
4
7
Classifier Number
6
8
9
10
1
0.9
e
r
o
c
S
-
1
F
0.8
0.7
0.6
Win 7
Win 8
Win 10
Linux
Win7
Win8
Win10
Linux
Training OS
e
c
n
a
m
r
o
f
r
e
P
1
0.9
0.8
0.7
0.6
Precision
Recall
F1-Score
1
2
3
4
5
Application
e
c
n
a
m
r
o
f
r
e
P
1
0.9
0.8
0.7
0.6
0.5
0.4
Precision
Recall
F1-Score
1
2
3
5
4
7
Position Offset (mm)
6
8
9
10
(a) Performance of 10 common classifiers.
(b) Impact of OSs.
(c) Impact of background applications.
(d) Impact of test point offset.
Figure 11: Micro-benchmark evaluation results of DeMiCPU on 5 randomly-chosen devices.
signal. The lightweight program is pre-installed on the experimental
laptops/ smartphones.
Data Collection Setup. We collect MI signals from the 90 de-
vices using a magnetic-field sensor DRV425 [22] from TI. As shown
in Fig. 10, the sensor is vertically placed on the surface of laptops or
phones (test points are shown in Tab. 3 in detail) since MI signals
emitted from the CPU modules are in the vertical direction. A data
acquisition (DAQ) card U2541A [25] from Keysight is utilized for
AD conversion with different sampling rates, e.g., 100 Hz, 200 Hz,
1 kHz, and etc. A data processing laptop connects with the DAQ
card through a USB, which locally stores and processes the collected
data.
6.2 Performance Metrics
Given an MI fingerprint from a device, DeMiCPU verifies whether
it belongs to the device (classifier) that it claims to be. For each
classifier i, we define T Pi as the true positives for classifier i, i.e., the
number of fingerprints that are correctly accepted as i. Similarly,
F Ni and F Pi refer to the number of fingerprints that are wrongly
rejected, and wrongly accepted as i, respectively. We define the
standard classification metrics for each classifier i as:
Precision(i) =
T Pi
(T Pi + F Pi )
Recall(i) =
T Pi
(T Pi + F Ni )
F 1 − Score(i) = 2 × Pri × Rei
(Pri + Rei )
(6)
(7)
(8)
The final precision, recall and F1-Score for DeMiCPU are the av-
erage of the 90 classes.
6.3 Micro-benchmark Evaluation
In this subsection, we evaluate the impact of classifier choices,
operating systems, background applications, on/off states of fans,
temperatures and displacements of test points. Five devices from
Tab. 3 are randomly chosen for the micro-benchmark evaluation.
6.3.1 Classifier Choice. To select the appropriate classifier for
DeMiCPU, we compare 10 commonly-used supervised learning algo-
rithms. They are 1) Logistic Regression, 2) Gaussian Naive Bayes, 3)
K-Nearest Neighbors, 4) Linear Discriminant Analysis, 5) Quadratic
Discriminant Analysis, 6) Decision Tree, 7) Support Vector Machine,
8) ExtraTrees, 9) Random Forest, and 10) Gradient Boosting. We
employ the 10-fold cross validation to evaluate the classifier perfor-
mance, which can combine measures of fit and thus derive a more
accurate estimation for model prediction performance.
We randomly choose 30 traces from each device, feed them into
the classifiers and record the corresponding accuracy. The results
in Fig. 11(a) show that 6 out of 10 classifiers show an F1-score above
0.9, with the classifier 8) ExtraTrees, 9) Random Forest, and 10)
Gradient Boosting being the best 3 classifiers. Thus, we can assume
that the data possesses good property in terms of discrepancies, i.e.,
CPU fingerprints are able to discriminate devices. In the following
experiments, we employ ExtraTrees since 1) it shows the best accu-
racy, and 2) it’s an ensemble classification approach which achieves
better robustness over a single classification algorithm.
6.3.2 Operating Systems. A device may install different OSs during
its lifetime. To investigate whether OSs affect DeMiCPU, we install 4
OSs which are 1) Window 7 Home Basic 7601, 2) Kali Linux 2.0, 3)
Windows 8 Professional 9200, and 4) Windows 10 Enterprise 10240
on the experimental laptops, and conduct experiments under each
OS to investigate the impact of OSs. We train the classifier with
traces from one OS and test it under all the four OSs. The results in
Fig. 11(b) indicate that with the DeMiCPU stimulation program, the
same device can be successfully identified across different OSs with
precision, recall and F1-Score of 1. It confirms that with elaborately
designed stimulation, OS-associated processes only account for a
tiny portion of the CPU utilization during fingerprinting, which is
within the tolerance of DeMiCPU. Thus, we believe DeMiCPU finger-
print is independent on OSs.
6.3.3 Background Applications. DeMiCPU stimulation is designed
to be undisturbed by other user processes. To evaluate its perfor-
mance against background applications in practice, we conduct
experiments on each device with several daily-used applications.
They are 1) WeChat, 2) Microsoft Word, 3) Google Chrome, 4)
YouTube, and 5) MATLAB, with statistically increasing CPU utiliza-
tion when normally used. We train the classifier using traces with
no background application, and test it using traces with one of the
aforementioned background applications, respectively. The results
shown in Fig. 11(c) confirm that, background applications barely
have impact on the performance of DeMiCPU since it can preempt
the CPU even if user applications run.
6.3.4 Displacement of Test Point. Due to that all electronic compo-
nents inside a device emit MI signals, the measuring sensor may
capture MI signals from other components when moved away from
Training Trace Size: 10
Training Trace Size: 20
Training Trace Size: 30
Training Trace Size: 40
1
0.8
0.6
0.4
0.2
F
D
C
Training Trace Size: 10
Training Trace Size: 20
Training Trace Size: 30
Training Trace Size: 40
1
0.8
0.6
0.4
0.2
F
D
C
Training Trace Size: 10
Training Trace Size: 20
Training Trace Size: 30
Training Trace Size: 40
1
0.8
0.6
0.4
0.2
F
D
C
Training Trace Size: 10
Training Trace Size: 20
Training Trace Size: 30
Training Trace Size: 40
1
0.8
0.6
0.4
0.2
F
D
C
0
0.75
0.8
0.85
0.9
Precision
0.95
1
0
0.75
0.8
0.85
0.9
0.95
1
Recall
0
0.75
0.8
0.85
0.9
Precision
0.95
1
0
0.75
0.8
0.85
0.9
0.95
1
Recall
(a) Overall precision for laptops.
(b) Overall recall for laptops.
(c) Overall precision for smartphones.
(d) Overall recall for smartphones.
Figure 12: Overall performance of DeMiCPU with different training data sizes (10, 20, 30 and 40).
the CPU module. To investigate the impact of the test point dis-
placement, we vary the position of the sensor as follows: Starting
from the center of the original test point (depicted in terms of key
positions in Tab. 3), we gradually move the sensor with a step of
1 mm in four directions: upwards, downwards, left and right. The
classifier is trained at the original test point, and tested at each
changed position. For each displacement, we average the preci-
sions, recalls and F1-scores in four directions, and show the final
results in Fig. 11(d). From the results, we can see that within an
offset of 8 mm, DeMiCPU achieves a high accuracy (> 99%). That
is, a user can conduct DeMiCPU fingerprinting with a displacement
tolerance of around a key size, which is approximately 10 − 15 mm
wide.
Fans. When fingerprinting a laptop, an electric fan aside the
6.3.5
CPU module emits MI signals as well. To investigate the impact
of fans, we collect 200 traces from each device with fan on and
off, i.e., 100 traces each. We train the classifier with the fan-on
traces and test it with the fan-off traces. The resulting F1-Score
is 1, indicating that MI signals from the fan have little influence.
We assume it is because that fans have much lower power (several
watts) compared with CPUs (tens of watts), and the large distance
(around 10 cm) between fan and CPU makes the MI signal from a
fan quickly attenuate.
6.3.6 Temperature. CPU temperature changes over time and load,
and might be an influence factor for DeMiCPU. To investigate, we test
DeMiCPU under different CPU temperatures. Note that in DeMiCPU
stimulation, we introduce a CPU frequency check before stimula-
tion since the CPU protection mechanism will decrease the CPU
frequency when its temperature becomes too high, e.g., above 90 oC.
Thus, DeMiCPU normally works when the CPU temperature is not
too high to cause a frequency drop and we first test DeMiCPU under
this range. We train the classifier using traces collected when CPU
temperature is 65 oC, and test it under the cases of 43 oC, 52 oC,
60 oC, 68 oC, and 78 oC, respectively. The F1-Scores for the five
cases are all 1. To further explore the performance of DeMiCPU un-
der a high temperature, we manually turn off the CPU protection
mechanism and test the system under 90 oC. The resulting F1-score
is also 1, indicating that DeMiCPU works as well. Thus, we believe
DeMiCPU is robust to CPU temperature changes.
4.0%
4.0% 5.0%
4.0%
2.0%
1.0%
1.0%
2.0%
4.0%
1.0%
1.0%
2.0%
Figure 13: Confusion matrix of 30 identical laptops.
6.4 Overall Performance
In the overall performance evaluation, 100 traces are collected from
each device in Tab. 3, and the employed classifier is ExtraTrees with
a tree number of 100.
Impact of Training Size. In the first set of experiments, we
6.4.1
train the system with x traces and test it with the rest 100 − x
traces (they are never used for training). x is set to 10, 20, 30, and 40
(correspond to 5, 10 ,15, and 20 seconds) respectively, to evaluate the
appropriate size of training data. We calculate the Precision(i) and
Recall(i) for each device (class) i, and plot their CDFs in Fig. 12(a)
- 12(d) with different training data size x. Even with 10 training
traces (correspond to 5 seconds), 90% of the precisions and recalls
are above 93.0% for all the laptops and smartphones. The average
precision and recall are 98.3% and 98.2% for the 70 laptops, and
99.4% and 99.3% for the 20 smartphones. With the increasing of
training data size, both precision and recall are improved. Given
the training size 20, DeMiCPU is able to achieve an average precision
and recall of 99.0% and 99.0% for the laptops, and 99.8% and 99.8%
for the smartphones. Besides, when the training data size further
increases, the performance of DeMiCPU approaches 100%. To strike
the balance between usability and accuracy, we choose 20 traces
for training, which only amount to 10 s. Training data size is then
set to 20 in the rest of the evaluation.
e
c
n
a
m
r
o
f
r
e
P
1
0.9
0.8
0.7
0.6
Precision
Recall
F1-Score
100
200
5K
1K
Sampling Rate (Hz)
25K 100K 200K
1
0.8
0.6
0.4
0.2
F
D
C
0
0.8
0.85
0.9
True Negative Rate
0.95
1
i
i
n
o
s
c
e
r
P
1
0.98
0.96
0.94
0.92
0.9
0.9
Laptop
Smartphone
0.92
0.94
0.96
0.98
1
Recall
Figure 14: Impact of different sam-
pling rates.
Figure 15: Performance of DeMiCPU
against 5 random aliens.
Figure 16: Precision-recall curves for
laptops and smartphones.
6.4.2 Performance of Devices of Same Model. The reason why pre-
cision and recall behave better on smartphones is that there are
more devices of the same model for laptops. As a result, false posi-
tives and false negatives may occur. To take a close look, we plot a
confusion matrix for devices No. 1-30 (i.e., the 30 ThinkPad T430
laptops) in Fig. 13. These 30 laptops are of the same model and
installed with the same operating system, and thus are more likely
to be confused with each other. From the confusion matrix, we
can observe that device No.23 and No.24, as well as device No.25
and No.26 contribute a relatively lower accuracy, with the worst
precision of 91.6%. Nevertheless, DeMiCPU can still achieve an av-
erage precision of 98.7%, and an average recall of 98.6% for the 30
identical devices.
Impact of Sampling Rate. To investigate the sampling rate
6.4.3