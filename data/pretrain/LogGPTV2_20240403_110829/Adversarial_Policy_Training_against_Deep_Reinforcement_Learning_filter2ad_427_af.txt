can be interchangeable. This is because, the performance
difference is subtle and, presumably in a different game, the
adversarial agent trained under the l1 norm might demonstrate
a slightly higher winning rate.
Comparison of our attack with the baseline method. Fig-
ure 7 shows the comparison of our method with the baseline
approach across two different games. First, we can discover
that using the baseline approach for the MuJoCo game, the
winning rate of the adversarial agent converges just slightly
above 50%. 8 This implies that the adversarial agent trained
by the baseline method can impose only a minimal risk to its
opponent. We believe the reason behind this is as follows.
As is mentioned in the section above, the baseline is a sim-
ple application of the PPO algorithm, which is not designed
speciﬁcally for training an agent to exploit the weakness of
the opponent. As a result, when used to train an adversar-
ial agent in a game, the algorithm may not be able to ﬁnd a
policy that could signiﬁcantly pull down the winning rate of
the opponent. From the perspective of the adversary, this is
indicated by the increase of his agent in the winning rate.
From Figure 7, we can also observe that the adversarial
agent trained by our method demonstrates signiﬁcant improve-
ment. For the MuJoCo and the Pong game, our adversarial
agent could converge at 60% and 100% of the winning rates,
respectively. This indicates that the action deviation term
could better guide our algorithm to search adversarial policy
subspace, identifying the one that could exploit the weakness
of the opponent most effectively.
In addition to the improvement of the average winning
rates, our proposed method, to some extent, escalates the
efﬁciency of the training process. As we can imply from Fig-
ure 7, to train an adversarial agent with a certain winning rate,
our method usually takes fewer iterations than the baseline
approach. Take the MuJoCo game for example. To train an
adversarial agent with 50% of the winning rate, the baseline
takes about 20 million iterations where our method takes only
about 11 million iterations. We argue this is beneﬁcial be-
cause reinforcement learning is known to be computationally
heavy and, with the capability of reducing the training itera-
tions, one could obtain an adversarial agent more efﬁciently.
8Note that the adversarial agent trained by this baseline approach does
not demonstrate the same winning rate as is stated in [10]. We believe this
is caused by the choice of initial states. Existing research [17] has shown
that DRL algorithms are sensitive to the choice of initial states. As such,
the standard method of evaluating a DRL algorithm is to run the algorithm
multiple times with different initial states and report the statistics of the
results. In this work, we follows this standard process and run each method
with eight randomly selected initial states.
USENIX Association
30th USENIX Security Symposium    1895
100  500Time steps (1e7)Winning rate (%)Baseline0.01 0.05 0.08 0              0.5            1.0             1.5          2.0100  500Time steps (1e7)Winning rate (%)InteGradGradSmoothGrad 0              0.5            1.0             1.5          2.0100  500Time steps (1e7)Winning rate (%) Random    0              0.5            1.0             1.5          2.0100  500Time steps (1e7)Winning rate (%)BaselineOur 0              0.5            1.0             1.5          2.0(a) MuJoCo winning rate.
(b) MuJoCo standard error.
(c) Roboshool winning rate.
(d) Roboshool standard error.
Figure 7: Our attack vs. the baseline approach [10] in two different games. Note that “Our W-B” represents our attack in the
white-box setting, where the approximated policy network of the victim agent was replaced with its actual policy network.
(a) MuJoCo.
(b) Roboschool Pong.
Figure 8: The winning rate of adversary-retrained victim
agent against our adversarial agent in two different games.
Game
MuJoCo
Roboschool Pong
Min
6.0%
40.0%
Std
Mean
Max
25.0% 16.3% 6.2%
44.0% 41.4% 1.4%
Table 1: The winning rate of the adversary-retrained victim
agent against the corresponding regular agent in two different
games. Note that after retraining the victim agents, we test
them for one hundred episodes.
Finally, from Figure 7, we can observe that our method ex-
hibits fewer variations of the winning rates (i.e., less shadow
area) than the baseline approach when the initial state varies.
This implies our proposed method is less sensitive to the ini-
tial state of the training process. Similar to the property of
training efﬁciency above, this is also a critical characteristic
because reinforcement learning is also known to be sensitive
to the initial random states, with our method, one does not
need to set up a good initial state to obtain an adversarial
agent with decent performance.
Comparison of black-box approximation with white-box
prior. Figure 7 illustrates the performance of the adversarial
agents trained with the approximated opponent policy as well
as the one actually used by the opponent. As is indicated
by the lines marked as “W-B” in the ﬁgure, the performance
observed from the white-box setting and our approximated ap-
proach is approximately the same. This indicates that, while
our point estimate inevitably introduces errors in approxi-
mation, for both games used for our evaluation, they have
not yet been ampliﬁed to a level that could jeopardize the
performance of our adversarial agent.
Comparison of adversary-retained agents with regular
agents. Figure 8 depicts the winning rate of the victim agent
against our adversarial agent, after we retrained it by using
the method proposed to train the adversarial agent. As we
can observe in the ﬁgure, this adversarial training approach
signiﬁcantly improves the robustness of the victim agent. The
retained victim agent demonstrates more than 95% winning
rates for both MuJoCo and Roboschool Pong games. This
indicates a simple adversarial training approach could be used
as an adversary-resistant method to robustify a game agent.
However, in Table 1, we also note that, when using the re-
trained victim agent to play with a regular agent (i.e., the
agent trained through self-play), the robustiﬁed agent does
not demonstrate a sufﬁcient capability in beating the regular
agent. This implies that, though a simple adversarial train-
ing improves agent robustness, it cannot help a victim agent
obtain sufﬁcient generalizability. We suspect this is caused
by the composition of the retraining episodes. Speciﬁcally, if
retraining the victim agent with the episodes of it playing with
both an adversarial agent and a regular agent, the retrained
agent will not only pick up adversarial robustness agent but
also preserve its generalizability.
7 Related Work
There is a large body of research on adversarial attacks against
deep neural networks (e.g., [7,8,11,13,27,36,49]). Recently,
the interest has been extended to deep reinforcement learning
(e.g., [18,19,40]). From the technical perspective, these previ-
ous works can be categorized into  attacking reinforcement
learning through trojan backdoors,  attacking reinforcement
learning through an adversarial environment, and   attacking
reinforcement learning through an adversarial agent. In the
following, we summarize the existing works and highlight
the key difference between these works and ours.
Trojan backdoors. A trojan backdoor attack refers to a hid-
den pattern implanted in a deep neural network [8, 13, 27].
When activated, it could force that infected deep neural net-
work misclassifying the contaminated inputs into a wrong
class. Recently, such an attack has been introduced to the con-
text of deep reinforcement learning. For example, in recent
works [21,56], researchers demonstrate that an attacker could
1896    30th USENIX Security Symposium
USENIX Association
100  500Time steps (1e7)Winning rate (%)BaselineOurOur W-B 0              0.5            1.0             1.5          2.0 0              0.5            1.0             1.5          2.0 20 100Time steps (1e7)Standard error (%)BaselineOur 0              1.0            2.0            3.0          4.0100  6030Time steps (1e6)Winning rate (%)BaselineOurOur W-B 0              1.0            2.0            3.0          4.0 20 10  0Time steps (1e6)Standard error (%)BaselineOur   0           0.5          1.0          1.5        2.0100   500Time Steps (1e7)Winning rate (%)  0           1.0         2.0           3.0        4.0100   5030Time Steps (1e6)Winning rate (%)follow the approach below to insert trojan backdoors into the
policy networks of a trained agent.
First, the attacker injects a trigger into an environment.
Then, he runs the victim agent in that manipulated environ-
ment, collecting the contaminated training trajectories. By
assigning high rewards to these contaminated trajectories, the
attacker could train a trojan-implanted policy network for
the victim agent. As is shown in [21, 56], when a trigger
is presented to the trojan-inserted agent, the agent generally
exhibits undesired behaviors.
When launching the trojan backdoor attack, an adversary
not only has to involve the training process of the victim
agent but also obtain the control over the environment that
the agent interacts with. In our work, we neither assume the
involvement of the training process nor the freedom to change
the environment when attacking an agent. As is mentioned
in Section 2, we assume an adversary could only control the
attacking agent and observe the actions of the victim agent.
As such, our proposed attack is orthogonal to trojan attacks
and more realistic in the physical world.
Adversarial environment. Over the past years, many re-
search works have discovered that deep neural networks are
vulnerable to adversarial attacks [7, 11, 36, 49], in which an
attackers could subtly perturb a data input to a deep neural
network (e.g., an image) and thus force that network to mis-
classify the perturbed data into the wrong class. Recently,
such a kind of adversarial attacks has been extended and
launched against the deep reinforcement learning, or more
precisely, the policy network of a trained agent.
In a pioneering research work [18], Huang et al. leverage
the idea of adversarial learning to manipulate the environment
at each time step and thus the observation passing to the
policy network. They demonstrate that using this approach,
the perturbed environment could easily fail a game agent –
making it exhibit poor performance – regardless whether it is
trained by deep Q-learning or actor-critic algorithms.
Following the step of Huang and his colleagues, recent
research [23, 25, 40] designed and developed new approaches
to improve the efﬁciency of this attack.
For example,
Kos et al. [23] suggest to perform environment manipula-
tion only at the times steps when the output of the value
function exceeds a certain threshold. Russo et al. [40] model
the selection of attacking time steps as a Markov decision
process. By solving this Markov decision process, an attacker
could identify the optimal time steps to launch attacks and
thus minimize his effort on environment manipulation.
Going beyond the efﬁciency improvement, recent research
also proposes methods to launch the aforementioned attack
in black-box settings. Rather than assuming an attacker has
the free access to the internal weights, the training algorithms,
and the training trajectories of the corresponding policy net-
work, the black-box setting restricts an attacker’s access only
to the input and output of the policy network. Under this
setup, Huang et al. [18] improves their adversarial attack.
More speciﬁcally, they trained a surrogate policy network
by using different training trajectory rollouts and algorithms.
Then, they utilized that network to construct an adversarial
environment (observations). Through a series of experiments,
they showed that the adversarial environment derived from the
surrogate network can still be useful for attacking the original
policy network. In addition to this black-box approach, recent
research proposes many other methods to generate an adver-
sarial environment in black-box settings (e.g., , [4, 54, 59]).
Similar to the work proposed in [18], they also demonstrated
that a trained agent could be attacked by an adversarial envi-
ronment, even if an attacker does not have prior knowledge
about its policy network.
Different from the works mentioned above, our attack does
not craft an adversarial environment but manipulate the action
of the adversarial agent through its policy network with the
goal of failing the opponent agent. This is a more practical
setup because, in the real world, an attacker could only control
its own agent but not have the freedom to change the environ-
ment that the victim agent interacts with (e.g., changing the
color of the sky in the super Mario game).
Adversarial agent. In terms of the problem setup, the work
most relevant to ours is the attacks through adversarial agents.
Different from the attacks mentioned above, this kind of at-
tack can be launched without the requirement of changing an
environment and/or accessing the training process of victim
agents. In early research, Gleave et al. [10] propose a train-
ing method that learns the policy network of the adversarial
agent by directly playing with the opponent agent.
Technically speaking, they ﬁrst treat the opponent agent
as part of the observation of the adversarial agent and then
simply train the adversarial agent by using the PPO algorithm.
In [10], Gleave and his colleagues show that, by using their
learning method to train an adversarial agent for MuJoCo
game [3], an attacker could make that adversarial agent defeat
the opponent agent in the two-agent competitive setting.
However, as is discussed in the section above, the method
proposed in [10] demonstrates only a low success rate in
attacking opponent agents because the proposed method is
a simple application of the PPO algorithm, which has less
guidance for the adversarial agent to identify the weakness of
the opponent agent. In this work, we propose a new method
to guide the construction of an adversarial policy network.
Technically, it not only extends the objective function of the
PPO algorithm but, more importantly, utilizes the explainable
AI techniques to ﬁnd the weakness of the opponent agent. As
we demonstrated in Section 6, the adversarial agent trained
through our method signiﬁcantly outperforms that trained
through the method in [10].
8 Discussion and Future Work
In this section, we discuss some related issues of our proposed
method and our future plan.
USENIX Association
30th USENIX Security Symposium    1897
Multiple agents. In this work, we develop our attack against
two-agent competitive games, whose real-world applications
include real-time strategy games (e.g., StarCraft II and Dota
2), online board games (e.g., Go, Poker), etc. In the future, we
plan to extend our work to multi-agent environments, where
multiple participants collaborate and/or compete with each
other. To achieve this, we will explore the solutions to tack-
ling the following challenges. First, different from our game
setting or typical single-agent reinforcement learning settings,
which can be modeled as a Markov Decision Process [41,53],
multi-agent reinforcement learning games require re-deﬁning
the game model as either Markov game or extension form
game with totally different value function and action-value
function [38, 57]. Under these new game settings, the PPO
algorithm is no longer a standard learning method to train an
agent. In this work, we design our method based on the PPO
algorithm. As such, migrating our attack method to multi-
agent settings might require non-trivial modiﬁcation and even
a completely new design. Second, even if recent research
proposes adaptive methods [34] to extend the PPO algorithm
into a multi-agent setting, it is still challenging to integrate
our proposed attack into a multi-agent game. On the one hand,
this is because recent research [57] demonstrates that a multi-
agent environment introduces non-stationary status and more
intense variance into the game environment, which inevitably
makes the training of our adversarial agent more difﬁcult. On
the other hand, this is due to the fact that the integration of our
method inevitably introduces intensive computation and in-
creases the difﬁculty in tuning hyperparameters. For example,
in a multi-agent environment, agents compete with each other.
This indicates that we have to modify the aforementioned
loss term by deviating actions between each other. Under
this setup, we have to increase the number of loss terms by
n2, where n is the total number of agents in the environment.
Assume n is a number larger than 5. Then, we can expect a
ﬁnal loss function with more than 25 loss terms, which makes
the optimization of that loss function hard to be resolved and
the hyperparameter tuning relatively difﬁcult.
Defense and detection. Researchers have proposed several
defense and detection mechanisms for reinforcement learn-
ing. With respect to the efforts of defense, many research
works extend the idea of adversarial training [11, 51]. For
example, the works proposed in [5, 28, 37] utilize the tech-
nique proposed in [18] to generate adversarial samples and