### Interchangeability and Performance Differences

The performance difference between the adversarial agents trained under different norms is subtle. In a different game, an adversarial agent trained under the L1 norm might exhibit a slightly higher winning rate.

### Comparison with Baseline Method

**Figure 7** illustrates the comparison of our method with the baseline approach across two different games: MuJoCo and Pong. 

- **MuJoCo Game:**
  - Using the baseline approach, the winning rate of the adversarial agent converges just slightly above 50%. This suggests that the adversarial agent trained by the baseline method poses only a minimal risk to its opponent.
  - The baseline method is a simple application of the Proximal Policy Optimization (PPO) algorithm, which is not specifically designed to exploit the weaknesses of the opponent. Consequently, it may not find a policy that significantly reduces the opponent's winning rate.
  - Our method, on the other hand, demonstrates significant improvement. The adversarial agent trained by our method converges at a 60% winning rate, indicating that the action deviation term in our algorithm better guides the search for an adversarial policy that exploits the opponent's weaknesses effectively.

- **Pong Game:**
  - For the Pong game, our adversarial agent achieves a 100% winning rate, further validating the effectiveness of our approach.

### Training Efficiency

In addition to improving the average winning rates, our proposed method also enhances the efficiency of the training process. As shown in **Figure 7**, our method generally requires fewer iterations to train an adversarial agent with a certain winning rate compared to the baseline approach. For example, in the MuJoCo game, the baseline method takes about 20 million iterations to achieve a 50% winning rate, whereas our method achieves the same with only about 11 million iterations. This reduction in training iterations is beneficial because reinforcement learning is computationally intensive.

### Sensitivity to Initial States

From **Figure 7**, we observe that our method exhibits less variation in winning rates (i.e., smaller shadow areas) when the initial state varies. This indicates that our proposed method is less sensitive to the initial state of the training process, which is a critical characteristic given that reinforcement learning is known to be sensitive to initial random states.

### Black-Box Approximation vs. White-Box Prior

**Figure 7** also shows the performance of adversarial agents trained with both the approximated opponent policy and the actual opponent policy. The lines marked as "W-B" in the figure indicate that the performance observed in the white-box setting is approximately the same as that of our approximated approach. This suggests that, while our point estimate introduces some approximation errors, these errors are not significant enough to degrade the performance of our adversarial agent in the evaluated games.

### Adversary-Retained Agents vs. Regular Agents

**Figure 8** depicts the winning rate of the victim agent against our adversarial agent after retraining it using the method proposed to train the adversarial agent. The retrained victim agent demonstrates more than 95% winning rates for both the MuJoCo and Roboschool Pong games, indicating that a simple adversarial training approach can significantly improve the robustness of the victim agent.

However, as shown in **Table 1**, when the retrained victim agent plays against a regular agent (trained through self-play), it does not demonstrate sufficient capability to beat the regular agent. This implies that while adversarial training improves robustness, it does not necessarily enhance the generalizability of the victim agent. We suspect this is due to the composition of the retraining episodes. If the victim agent is retrained with episodes involving both adversarial and regular agents, it will likely preserve both robustness and generalizability.

### Related Work

There is extensive research on adversarial attacks against deep neural networks and, more recently, deep reinforcement learning. These works can be categorized into three main types: 
1. **Trojan Backdoors:** Hidden patterns implanted in deep neural networks that, when activated, force the network to misclassify inputs.
2. **Adversarial Environment:** Subtle perturbations to the environment that cause the policy network to fail.
3. **Adversarial Agent:** Manipulating the actions of the adversarial agent to exploit the weaknesses of the opponent.

Our work differs from these approaches in that we do not require involvement in the training process or the ability to change the environment. Instead, we focus on manipulating the actions of the adversarial agent to exploit the opponent's weaknesses, making our approach more practical in real-world scenarios.

### Discussion and Future Work

- **Multi-Agent Environments:** Extending our work to multi-agent environments presents several challenges, including the need to redefine the game model and adapt the PPO algorithm. Additionally, the non-stationary nature and increased variance in multi-agent settings make the training of adversarial agents more difficult.
- **Defense and Detection:** Researchers have proposed various defense and detection mechanisms for reinforcement learning, such as adversarial training. Integrating these mechanisms with our approach is an area for future exploration.

By addressing these challenges, we aim to further enhance the robustness and generalizability of our adversarial agents in more complex and dynamic environments.