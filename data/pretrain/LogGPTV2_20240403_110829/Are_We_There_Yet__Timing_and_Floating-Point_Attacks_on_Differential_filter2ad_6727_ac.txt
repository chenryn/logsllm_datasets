− s2
2
(cid:19)
(1)
The intuition behind our attack is similar to that of “attack-
ing” RandomFP in Section IV-B: we ﬁnd an expression that
must be an integer if PolarMethod was used. We observe that
r×R2 must be an integer: since x(cid:48)
2 are produced using
RandomFP (step P1) there must be integers u1 and u2 such
that x(cid:48)
2 = u2/R. Rearranging and substituting
these x(cid:48)
1 = u1/R and x(cid:48)
1 and x(cid:48)
2 further in steps P2 and P3 we obtain
r = (2u1/R − 1)2 + (2u2/R − 1)2
1 and x(cid:48)
Since u1, u2, R are integers, value r× R2 must be an integer.
IsFeasibleNormal(s) proceeds as follows. The attacker com-
putes value of r × R2 using Equation 1 with values s (instead
of s1) and s2 as follows:
(cid:18)
(cid:19)
exp
− s2 + s2
2
2
× R2
(2)
It then checks if the value in Equation 2 is an integer. If it is,
then IsFeasibleNormal(s) returns true since s and s2 could be
produced using PolarMethod.
As ﬂoating-point arithmetic cannot be done with inﬁnite
precision on ﬁnite machines, s and s2 might be inaccurate. To
this end, we perform a heuristic search in each direction of s
and s2 where we try several neighboring values of s and s2.
In our experiments, we choose to search 50 values in each
direction. This search heuristic is prone to errors and may
result in false positives and false negatives, due to (1) more
than one pair of values resulting in s1 and s2 and (2) our
search not being exhaustive in exploring values with a range
of 100 values. Nevertheless in the next section we show that
the attack is still successful for a variety of applications of
Gaussian noise in DP.
D. Ziggurat Method: Implementation and Attack
The Ziggurat method [31] is another method for generating
samples from the normal distribution. Compared to the Box-
Muller and polar methods, it generates one sample on each
invocation. It is a rejection sampling method that randomly
generates a point in a distribution slightly larger than the
Gaussian distribution. It then tests whether the generated point
is inside the Gaussian distribution.
1) Method: ZigguratMethod relies on three precomputed
tables w[n], f [n] and k[n] that are directly stored in the source
code. The Ziggurat implementation in Go uses n = 128 and
proceeds as follows.
Z1 Generate a random 32-bit integer j and let i be the index
provided by the rightmost 7 bits of j.
Z2 Set s ← jw[i]. If j < k[i], return s.
Z3 If i = 0, run a fallback algorithm for generating a sample
from the tails of the distribution.
Z4 Use RandomFP to generate independent uniform ran-
dom value U from (0, 1). Return s if:
U (f [i − 1] − f [i]) < f (s) − f [i]
Z5 Repeat from step Z1.
We refer interested readers to [32] for how to sample from
the tail of a normal distribution and to [31] for how the
tables w[n], f [n], k[n] are generated. The method can generate
samples from N (0, σ2) by returning σs.
The Ziggurat method is used by the Go package
math/rand with NormFloat64 and new random sampling
methods of NumPy‡.
2) Attack: This time, the attacker aims to devise a function
IsFeasibleNormal(s) that determines if value s could have been
generated by ZigguratMethod. We describe our attack steps
below since attacks in Section IV-C and [6] are not applicable
due to pre-computed tables used in Ziggurat.
We observe that the returned value is jw[i] in steps Z2
and Z4, except when the value is sampled from the tail of
Gaussian distribution in step Z3 (which happens infrequently).
Furthermore, j is an integer and all values of w[n] are available
to the attacker, because all precomputed tables are stored
directly in the source code. Based on these observations, for
a noise s, our attack for ZigguratMethod proceeds as:
1) For each w[i], i ∈ [1, n] calculate w[i] = s/(σw[i]).
2) For each w[i], check if it is an integer.
3) If any w[i] is an integer, then we take s as a feasible
ﬂoating-point value, IsFeasibleNormal(s) returns true.
Note that the method does not attack values sampled from
the tail. However, we observed that it happens less than 0.1%
of the time when n = 128, hence, the attack works for the
majority of the cases.
V. EXPERIMENTS: FP ATTACK ON NORMAL
DISTRIBUTION
We perform four sets of experiments to evaluate leakage
of Gaussian samplers based on the attack described in the
previous section. First, we enumerate the number of possible
values in a range of ﬂoating points that can result in an
attack and observe that it is not negligible. We then show the
effectiveness of our attack on private count and DP-SGD, a
popular method designed for training machine learning (ML)
models under differential privacy [11].
‡https://numpy.org/doc/stable/reference/random/index.html
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
477
Gaussian Implementations: In our experiments, we use
four implementations of Gaussian distribution samplers.
• Gauss polar: our implementation of polar method as
described in Section IV-C1.
• Gauss numpy: NumPy implementation of polar Gaussian
• Gauss pytorch: PyTorch implementation of Box-
Muller Gaussian sampling.
• Gauss go: Go implementation of Ziggurat Gaussian sam-
sampling.
pling.
Our attacks on DP-SGD were tested using the privacy engine
implementation of the Opacus library [4]*.
A. Distribution of “Attackable” Values
In this section we aim to understand how many ﬂoating-
point values could not have been generated from a Gaus-
sian sampling implementation. That is, for how many val-
ues s, IsFeasibleNormal(s) would return false. We call such
values “attackable” since if an adversary were to observe them,
they would know that the value must have been produced in
a speciﬁc manner (e.g., by adding noise to f (D(cid:48)) as opposed
to f (D)). We use the Gauss polar implementation to perform
this experiment in a controlled environment and assume that
the adversary is given y1 and y2 where y1 = f (D) + s1 and
y2 = f (D) + s2 or y1 = f (D(cid:48)) + s(cid:48)
1 and y2 = f (D(cid:48)) + s(cid:48)
2.
(Compared to the attack in Section IV the attacker is not
given s2 directly, however the attack can proceed similarly.)
For each pair of y1 = f (D) + s1 and y2 = f (D) + s2, if
the attack successfully concludes that they are in support of
f (D), and not f (D(cid:48)), we count them as attackable values. We
set f (D) = 0, f (D(cid:48)) = 1, σ = 114, R = 210. The blue line
of Figure 1 shows the distribution of the rate of attackable
values where we plot y1 and y2. In order to ﬁt all measured
ﬂoating-points into the resolution of the graph, we aggregate
the results over small intervals of width 0.8.
Mironov generated a similar graph for the Laplace distribu-
tion in [6]. Compared to Gaussian, the Laplace distribution has
more attackable values since it uses only one random value,
hence the attack does not suffer from false negatives.
In order to understand our results further we also plot the
average number of times each y = f (D)+s is observed, using
the gray line of Figure 1. This explains some of the spikes
in the blue line: more frequently observed values indicate
more attackable values. Percentage of ﬂoating-points that are
attackable are higher closer to 0 (recall that the graph shows
an average over FP intervals). Overall, the graph suggests
that attackable values do exist and as we show in the rest
of this section provide an avenue for an attack against DP
mechanisms.
B. DP Gaussian Mechanism
1) Private count: In this section, we explore the effective-
ness of our attack against the Gaussian mechanism used to
protect a private count (i.e., corresponding to S or DB sce-
narios in Section III). We use the German Credit Dataset [33]
and the query: count the number of records with number of
Fig. 1: Distribution and number of values amenable to
a ﬂoating-point attack against a Gaussian implementation
Gauss polar using R = 210 and σ = 114. Here the horizontal
axis, f (D)+s, shows both values f (D)+s1 and f (D)+s2. For
presentation purposes, rate of attackable ﬂoating-point values
is averaged over 0.8-wide intervals. Blue line indicates the rate
of ﬂoating-point values that are in the support of f (D) = 0
and not in f (D(cid:48)) = 1. Gray line measures average number of
times support for f (D) was observed.
credits greater than 16K (resulting in one record since the two
highest values among dataset’s 1000 records are 15945 and
18424).
Given the output of the Gaussian mechanism, y, the adver-
sary’s goal is to determine whether noiseless output came from
q = f (D) or q(cid:48) = f (D(cid:48)) where f is the count query deﬁned
above. The private count of values in a dataset is computed
as: y = q + s or y = q(cid:48) + s, where q and q(cid:48) are the outputs
of f on dataset D and D(cid:48), respectively, and s is a noise
sampled with Gauss numpy, Gauss pytorch, or Gauss go. For
Gauss numpy and Gauss pytorch, the adversary also knows
the second value sampled from the distribution (i.e., s2 in
Section IV-C). The neighboring datasets D and D(cid:48) that our
attack tries to distinguish differ on a single record whose
credits is 0 in D and 18424 in D(cid:48), so the count of records
satisfying the above query is q = 0 for D and q(cid:48) = 1 for D(cid:48).
We set the sensitivity ∆ = 1 since one record can change
the output of f only by 1. We vary  in the range (0, 100]
with ﬁxed δ = 10−5. For each tuple of , δ and ∆, we use the
analytic Gaussian Mechanism [34] to calculate the required
noise scale σ for the experiment. Here, small  and δ model
a common set of parameters used in DP [35].
In Figure 2 we plot our success rate from 1 million trials
from the following attack. Recall that the attacker cannot al-
ways make a guess due to the limitations listed in Section V-A.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
478
and accuracy against Gauss go are at least 76% and 99%,
respectively.
C. DP-SGD in Federated Learning scenario
The Gaussian mechanism is used extensively in training
ML models via differentially-private stochastic gradient de-
scent [11], [23], [21]. In this section, we describe successful
attacks on the differentially-private training of ML models.
Here, we consider the Federated Learning scenario from
Section III where an attacker (central server) observes a DP-
protected gradient computed on a batch of client’s data and is
trying to determine if the batch includes a particular record or
not.
The main observations we make in this section are:
• An adversary can determine if a batch contains a record
with a different label from other records in the batch, i.e.,
a record that comes from the same distribution as training
dataset but different to those in the batch.
• The success rate of the attack increases as  decreases
as there are fewer ﬂoating-point values to represent large
noise values.
1) Setup: We use Opacus library [4] for training machine
learning models with DP-SGD. Our experiments are based
on the Opacus example on MNIST data. This dataset [37]
contains 70K images, split in 60K and 10K sets for training
and testing, respectively. Each image is in 28 × 28 gray-scale
representing a handwritten digit ranging from 0 to 9 where the
written digit is the label of the image. The ML task is: given
an unlabelled image, predict its label.
We use the same parameters for preprocessing and neural
network training as used in the library [38], [4], following [11].
These training parameters are: learning rate 0.1, epoch num-
ber 8, batch size S = 64, number of batches in each epoch 100,
and DP-SGD clipping norm L = 1. We vary  to understand
how different parameters affect the attack success rate while
keeping a ﬁxed δ = 10−5 (as in [38], [11]). Depending on the
, the noise is drawn from a normal distribution with µ = 0
and σ ∈ [1, 250]. As a result  ranges from 0.1 to 0.68. Since
the model has d = 26, 010 parameters, each batch is used in d
gradient computations to update the corresponding parameters.
Hence, the attacker obtains d ﬂoating-point values protected
with DP-SGD. Details of DP-SGD computation are provided
in Appendix B.
We consider a setting where an adversary observes a
gradient computed on FL client’s batch of labelled MNIST
images. The batch could correspond either to records B or
a neighboring batch B(cid:48) produced by replacing a randomly
chosen record in batch B with a canary record (deﬁned
is, given B, B(cid:48) and a noisy
below). The attacker’s goal
gradient protected with Gaussian noise, to determine if the
gradient was computed on B or B(cid:48).
We use different types of batches and canary records to test
the effectiveness of our attack:
• SimLabelCanary: batch B is composed of shufﬂed train-
ing data. The canary record of its neighboring batch B(cid:48)
is a record drawn from the test dataset.
Fig. 2: FP attack success rate on private count where the count
is protected with one of the three Gaussian samplers across
 ∈ (0, 100] with ﬁxed δ = 10−5 and function sensitivity
∆ = 1. Baseline (random) attack success is 50%.
To this end, if the attack can ﬁnd support for either only q or
only q(cid:48), then the attack outputs the corresponding guess. If
the result is in support of both or neither of q and q(cid:48), then the
attacker is unsure and resorts to the baseline attack, choosing
q or q(cid:48) at random.
We observe that
the attack is more successful for the
Gauss go method for values of  less than 10. This is also the
range of values for  used in the literature on DP [36]. We note
a slight cyclic behavior and observe that peaks occur when
the corresponding noise scale σ (w.r.t. ) is a power of 2 (see
Figure 7 in the Appendix). We hypothesize that the success
rate at those points is higher since multiplication and division
by powers of 2 can be done via bit shifting that decreases the
impact of rounding. Hence, extraction of s is not affected by
rounding errors that would otherwise be introduced during the
multiplication of s by σ in the ZigguratMethod and step 1 of
the corresponding attack in Section IV-D.
Among two sampler methods, attack is more efﬁcient
against Gauss pytorch than Gauss numpy. We also observe
that
the attack becomes stronger as  increases. This is
potentially due to the higher distribution of attackable values
as indicated in Figure 1. The attack is always more successful
than the baseline random attack.
In Appendix D we also investigate the rate at which the
attacker can make a guess (attack rate) and how many of
these guesses are correct (attack accuracy) across a range of 
and sensitivity values. Gauss numpy and Gauss pytorch have
attack rates ranging between 1.7% and 92.8% with attack
accuracy of at least 89%. In comparison, the attack rates
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
479
than a random guess for  < 0.68. Similar to previous results
in this section, the attack success rate is much higher than
the theoretical (, δ)-DP bound on failure of δ = 10−5 would
suggest.
We observe that the adversary cannot distinguish f (B) and
f (B(cid:48)) when SimLabelCanary is used, that is, when the canary
is similar to all the other records in the batch. The reason is that
the gradients for B(cid:48) and B are close to each other and hence,
even when the noise is added the two stay relatively close to
each other, hence, the range of “attackable” ﬂoating-point they
land on is similar. On the other hand, for DiﬀLabelCanary the
canary record has a very different distribution from records in
B, thus the gradients are further apart which shifts them into
ranges of varying number of ﬂoating points. It is important to
note that the difference between the gradients (i.e., sensitivity)
is protected by the DP mechanism based on a theoretical