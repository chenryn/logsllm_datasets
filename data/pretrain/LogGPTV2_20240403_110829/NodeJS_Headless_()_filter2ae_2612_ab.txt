    await page.on('response', interceptedResponse =>{
        let status = interceptedResponse.status();
        if(status.toString().substr(0,2) === "30"){
            console.log("url: " + interceptedResponse.url());
            console.log("status: " + status);
            console.log("headers: " + interceptedResponse.headers().location);
            // 添加进任务队列
            cluster.queue(interceptedResponse.headers().location);
        }
    });
**事件触发 &收集结果**
    function executeEvent() {
        var firedEventNames = ["focus", "mouseover", "mousedown", "click", "error"];
        var firedEvents = {};
        var length = firedEventNames.length;
        for (let i = 0; i  -1) {
                if (eventNode != undefined) {
                    eventNode.dispatchEvent(firedEvents[eventName]);
                }
            }
        }
        let result = window.info.split("_-_");
        result.splice(0,1);
        return result;
    }
### 添加cookie
对于使用SSO单点站点体系而言，可以在开始爬行之前指定一段cookie，比如从文本中读取。但是对于爬行目标较为多且SSO的覆盖面有限的情况下，就得使用数据库了。在测试过程中遇到了另一个问题，就是并发过高，或者发送有害的payload，会有Cookie失效的问题，这里想到了一种比较实用的解决办法，写一个浏览器插件及时将当前页面的cookie同步到服务端数据库，然后爬虫定期从数据库中更新最新的cookie。
**Chrome插件同步cookie**
    function updateCookie(domain, name , value){
        let api = "http://127.0.0.1/add-cookie";
        $.post(api, {
            "domain": domain,
            "name": name,
            "value": value,
        }, function (data, status) {
            console.log(status);
        });
    }
    /*
     * doc: https://developer.chrome.com/extensions/cookies
     */
    chrome.cookies.onChanged.addListener((changeInfo) =>{
        // 记录Cookie增加，Cookie更新分两步，第一步先删除，第二步再增加
        if(changeInfo.removed === false){
            updateCookie(changeInfo.cookie.name, changeInfo.cookie.value, changeInfo.cookie.domain);
        }
    });
### 相似URL去重
去重在爬虫中是一个较为核心功能，规则过于宽松可能导致爬行不完或者说做一些无意义的重复爬行，规则过于严格则可能导致抓取结果过少，影响后续抓取和漏洞检测。去重一般分为两步对爬行队列去重，或者对结果集去重。
在解决这个问题的时候，参考了Fr1day师傅[【技术分享】浅谈动态爬虫与去重](https://www.anquanke.com/post/id/85298)的URL去重思路。不失为一种比较便捷，能基本满足当前需求的一种解决办法。
**参数分析**  
大致有以下几种参数：类型int、hash、中文、URL编码
    ?m=home&c=index&a=index
    ?type=202cb962ac59075b964b07152d234b70
    ?id=1
    ?msg=%E6%B6%88%E6%81%AF
根据不同的类型对其进行处理：
  1. 纯字母：中参数的值表示不同的路由功能，需要对这种参数进行保留
  2. 字母数字混合：可能是用户的hash，也可能具有路由功能，可根据任务量情况选择性保留
  3. 纯数字、URl编码：进行去重
处理结果即：
    ?m=home&c=index&a=index
    ?type={hash}
    ?id={int}
    ?msg={urlencode}
然后在数据库中将相同的清洗掉即可。
### 相似页面去重
相似度计算，监控资产变化  
网页结构相似度:[http://xueshu.baidu.com/usercenter/paper/show?paperid=232b0da253211ecf9e2c85cb513d0bd3&site=xueshu_se](http://xueshu.baidu.com/usercenter/paper/show?paperid=232b0da253211ecf9e2c85cb513d0bd3&site=xueshu_se)
> 填坑
## 性能优化
### 图片资源优化
禁止浏览器加载图片 => 返回一个fake img  
实际测试过程中，有的网站在加载图片失败后，会尝试重新加载，这样会陷入一个死循环，导致发送大量数据包，占用性能。  
代码：
    const browser = await puppeteer.launch(launchOptions);
    const page = await browser.newPage();
    await preparePage(page);
    await page.setRequestInterception(true);     // 开启拦截功能
    await page.on('request', interceptedRequest => {
        // 拦截图片请求
        if (interceptedRequest.resourceType() === 'image' || interceptedRequest.url().endsWith('.ico')) {
            //console.log(`abort image: ${interceptedRequest.url()}`);
            let images = fs.readFileSync('public/image.png');
            interceptedRequest.respond({
                'contentType': ' image/png',
                'body': Buffer.from(images)
            });
        }
        else {
            interceptedRequest.continue();
        }
### 拦截logout请求
避免爬虫爬行到登出链接，导致Cookie失效，这里做一个简单的拦截：
    await page.on('request', interceptedRequest => {
        if(interceptedRequest.url().indexOf("logout") !== -1){
            interceptedRequest.abort();
        }
        else{
            interceptedRequest.continue();
        }
    });
## puppeteer并发异步调度方案
简单粗暴，这里使用[puppeteer-cluster](https://github.com/thomasdondorf/puppeteer-cluster#api)库解决单Chrome多tab并发需求，也可以参考使用guimaizi师傅的demo:[puppeteer异步并发方案](http://www.guimaizi.com/archives/535)
## 开源
这里边其实还有很多坑要填，师傅们多指点交流~  
开源链接：  
（求star
## 最后
非常感谢各位前辈师傅们的技术分享，欢迎一起交流，在下一篇中我将分享自己对于漏洞检测的一点理解。