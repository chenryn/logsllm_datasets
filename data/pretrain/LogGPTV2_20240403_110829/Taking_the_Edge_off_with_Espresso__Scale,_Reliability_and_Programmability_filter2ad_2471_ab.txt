live system without impacting the data plane, which fortu-
itously provides ample opportunity to test fail static system
properties.
(3) Espresso emphasizes software programmability via sim-
ple hardware primitives (e.g., MPLS pop and forward to
next-hop). As such, new features can be introduced with-
out waiting for the vendor to qualify and release a software
update. This in turn allows the network to evolve with chang-
ing application requirements and also enables innovative
networking features to be deployed with high velocity. Sep-
arating the control plane from the data plane has the added
benefit of allowing the CPU for control protocols to scale
independently of hardware packet forwarding capabilities.
With commercial Peering Routers, the ratio of control CPU
to data plane performance is fixed.
(4) High feature velocity resulting from programmability im-
poses testability as a key design principle for Espresso. Only
by rigorously and systematically testing each feature through
fully automated end-to-end testing can we achieve feature
velocity without sacrificing reliability and availability.
Because we implement network functionality in software
components, we can perform layers of testing from (i) unit
tests to (ii) components to (iii) pairwise interaction to (iv) end-
to-end systems and subsystems. This is in sharp contrast to
qualification of routers where we can only rely on black box
testing coupled to expensive and hard to manage hardware
deployments. The layers of testing provide confidence in
the safety of new software releases, allowing us to release
frequently while staying within our reliability budget.
(5) Supporting exponential growth means that Espresso must be
designed for intent-driven manageability with controlled
configuration updates. This manageability needs to support
large scale operation that is safe, automated, and incremental.
Such automation is key to sub-linear scaling of the human
overhead of running the network and reducing operational
errors, the main source of outages [16].
4 DESIGN
In this section, we describe the design of Espresso and how it
integrates into the existing Google network. We then detail the
design of each component in Espresso and how they adhere to our
design principles.
4.1 Background
Peering locations—a.k.a. edge metros—connect Google to end users
around the world (Figure 1) through external ISP peers. Figure 3a
illustrates this configuration. Prior to Espresso, we used traditional
routers, Peering Routers (PR), to connect Google’s network with
other autonomous systems (AS) in the edge. These PRs support
eBGP peerings, Internet-scale FIBs and IP access-control-lists to
filter unwanted traffic.
Alongside our routers, we also run Layer 7 reverse proxies at the
edge to terminate user connections and to serve cached content.
The proxies reduce connection latency to users, reduce the required
capacity back to data centers through caching, and improve perfor-
mance for cacheable content [15, 23]. A typical user request enters
Google via one of the PRs and terminates at a local L7 (reverse)
proxy. Key to Espresso is using a small subset of this server process-
ing power already running at the edge for programmable packet
processing.
4.2 Design Overview
Figure 2 illustrates the basic architecture of an Espresso edge metro,
which broadly consists of three subsystems.
(1) A global traffic engineering (TE) system enables application-
aware routing at Internet scale. This TE system, consisting of
the Global TE controller (GC) and location controllers (LC),
programs the packet processors with flow entries that allows
dynamic selection of egress port on per-application basis.
434
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
K.K. Yap, M. Motiwala, et al.
Table 1: Espresso: Requirements with the corresponding design principles that help achieve them.
Requirement
Efficiency
Interoperability
Reliability
Security
Incremental
Deployment
High Feature Ve-
locity
Design Principle(s)
Software Programmability
Software Programmability
Hierarchical Control Plane, Fail
Static, Manageability
Software Programmability
Software Programmability
Software
Testability
Programmability,
Summary
Centralized TE, application-aware optimization and routing (§4.3)
eBGP for peering (§4.4.1), support IPv4/IPv6 (§4.2.1)
Split between local/global control-plane (§4.3), intent-driven configuration
system (§4.5)
Fine-grained DoS & Internet ACL on host packet processors (§4.4.4)
TE system supports legacy and Espresso peering devices (§4.3)
Loosely coupled control-plane, automated testing and release processes (§5)
4.2.1 Application-aware routing. A typical user request enters
via a peering device and terminates at an edge host (Figure 3b) via
standard IP routing. With traditional routing, the response would
simply be sent from this edge host to the peering router (PR), which
in turn maps the destination IP address to one of its output ports
by consulting an Internet-scale FIB constructed by its BGP stack.
Espresso PFs do not run BGP locally and do not have the capacity
to store an Internet-scale FIB. We instead store Internet-scale FIBs in
the servers, using cheaper DRAM on servers for better scaling with
the growth of Internet prefixes. Espresso directs ingress packet
to the host using IP GRE where we apply ACLs, see Figure 3b.
The hosts also encapsulate all outbound packets with a mapping
to the PF’s output port. That is, we encode the egress port in the
packet itself through server packet processors, enabling tremendous
hardware and software simplification at the PF.
Thus, it is each server’s packet processor that maps the ultimate
packet destination to a pre-programmed label at the PF using an
Internet-scale FIB stored in server memory. Espresso uses IP-GRE
and MPLS encapsulation, where IP-GRE targets the correct router
and the MPLS label identifies the PF’s peering port (Figure 3a). The
PF simply decapsulates the IP-GRE header and forwards the packet
to the correct next-hop according to its MPLS label after popping
the label. An L3 aggregation network using standard BGP forwards
the IP-GRE encapsulated packet from the host to the PF. IP-GRE
was chosen over destination MAC rewrites, e.g., [4], because it
allows us to scale the aggregation network easily.
To program host FIBs, GC gathers routes from all peering devices,
both traditional and Espresso peering. It calculates the application-
aware FIB, while respecting BGP policy. GC then sends the resulting
Internet-sized FIB to the LCs. Because GC has a slower control loop,
LC maintains responsibility for quickly reacting to any metro-local
network events as shown in Figure 5. Using these rules, the packet
processors can implement application-aware routing.
4.2.2 BGP peering and routes propagation. Espresso externalizes
eBGP from the peering device (PF) to software processes running
on the host servers. To establish a BGP session with an external
peer, LC creates an IP-GRE tunnel between the PF and the server
running BGP, Figure 2. We partition the responsibility of handling
peer BGP sessions among servers at the granularity of each peer,
simplifying the process of scaling peering sessions. In addition, this
approach allows Espresso to establish a TCP connection directly
between the peer router and the eBGP engine without requiring
multi-hop peering, which many peers do not support.
435
Figure 2: Espresso Control Plane: LC programs Internet-sized FIBs
in the packet processors on the edge hosts, and distributes config-
uration to the Peering Fabric Controllers (PFC). PFC in turns man-
ages PF and BGP speakers. BGP speakers establish eBGP peering
with other AS. The local control plane tasks run on the machines
hosted in the edge metro.
This programmable packet processing also helps mitigate
DoS attacks with finer resolution than possible in hardware
routers.
(2) A combination of a commodity MPLS switch (PF) that
supports forwarding/tunneling rules and ACLs, and BGP
speaker processes that establish BGP peering supports the
traditional peering "router" capabilities. Unlike an Internet-
scale peering router, the PF has a small TCAM and limited
on-box BGP capability. However, it is capable of line rate
decapsulation and forwarding of IP GRE and MPLS packets.
(3) Finally, Espresso supports fully automated configuration and
upgrades through an intent-driven configuration and man-
agement stack. To configure the system, an operator simply
changes the intent. Committing the intent triggers the man-
agement system to generate, version, and statically verify
the configuration before pushing it to all relevant software
components and devices. For additional protection, we also
canary the configuration with another layer of verification
performed by the individual components.
HostHostHostHostHostPeering“Router”Peering                          Fabric                      (PF)HostHostHostHostHostBackbone RouterHostPacket ProcessorPeering Fabric ControllerLocation Controller (LC)Global TE (GC) ControllerBGP speakerBGP speakerBGP speakerExternal Peer RoutereBGP PeeringGRE Tunnelroutesapplication-specific host programmingEdge MetroPeeringsGlobalTaking the Edge off with Espresso
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
(a) Diagram shows path for traffic from the reverse L7 web proxy is directed
towards a selected peering port on the PF using IP GRE and MPLS encapsula-
tion.
Figure 4: Global Controller system is a distributed system that peri-
odically produces an optimized application-specific programming
rules that are distributed via LCs in each edge metro.
to rebalance traffic across available peering ports to serve user traf-
fic. LC also quickly propagate any failures (e.g., route withdrawals)
to the hosts. Figure 5 shows an example of this operation.
4.3 Application-aware TE System
Espresso’s TE system is a hierarhical control plane, divided into a
global TE controller (GC) and per-metro location controllers (LC).
GC provides application-aware TE decisions through global opti-
mization, while LC provides local fallback and fast reaction to fail-
ures to increase Espresso’s reliability and responsiveness. Integra-
tion with the existing TE system allows for incremental deployment
of Espresso, as GC supports both traditional and Espresso peering
devices. At a high level, GC’s objective is to efficiently allocate user
traffic demand given the available resources (peering, backbone,
server capacity) while optimizing for application-level metrics such
as goodput and latency. GC also strictly observes application-level
priority to ensure we allocate bandwidth for higher priority appli-
cations before others.
4.3.1 Global Controller. Figure 4 shows an overview of GC’s
operation. The output of GC is a prioritized list of  tuples for each  tuple, where service class encodes the priority of the traffic
on the network. We refer to this list as the egress map. Packet
processors employ this list to control where an application’s traffic
egresses Google’s network. GC only chooses the egress to optimize
peering port utilization and does not control pathing between the
hosts and the PFs/PRs. Dynamic pathing between hosts and PFs/PRs
is beyond the scope of Espresso.
To make application-aware TE, GC’s optimizer consumes a num-
ber of inputs:
• Peering routes: GC determines where user traffic can egress
by collecting all peering routes from edge routers. The Feed
Aggregator collects routes while preserving the BGP at-
tributes to respect BGP peering policies. GC consumes routes
from both traditional PRs and the Espresso PFs. GC can then
compute egress maps targeting PR or PF, which allows for in-
cremental deployment of new peering capacity on Espresso.
Using these routes, GC creates a prioritized list of egresses
(b) Diagram shows path for traffic from user to the reverse L7 web proxy is
directed with IP GRE encapsulation.
Figure 3: Espresso Metro with support for both legacy peering
router (PR) and Espresso peering fabric (PF).
Once the peering is established, Espresso exchanges routes with
peers as with any router. LC aggregates these routes across BGP
speakers and delivers them to the GC. In turn, GC learns of available
peering ports on a per-prefix basis, running a global TE optimization
436
HostHostHostHostHostHostHostHostHostHostPeering Fabric (PF)Backbone RouterInternal NetworkISP 1ISP NAggregation  LayerLegacyPeering Router (PR)IP PacketMPLSIP GREIP PacketEncapsulation applied by hostHostHostHostHostHostHostHostHostHostHostPeering Fabric (PF)Backbone RouterInternal NetworkISP 1ISP NAggregation  LayerLegacyPeering Router (PR)IP PacketIP PacketIP PacketIP GREPF adds IP-GRE headers for filtering trafficHost pops IP-GRE headers and applies filtering rules on the data packetVolume AggregatorOptimizerLimits ComputerFeed AggregatorHigh level application-specific traffic policies and configurationUser-request demand and performance (per client prefix)Network Topology ServerNetwork topology modelPeering routes with BGP attributesLocation ControllerInterface utilization, queue drops, etcapplication-specific host programming rules (per edge metro)Location ControllerLocation ControllerGlobal TE controller  (GC)Location ControllerLocation ControllerLocation ControllerSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
K.K. Yap, M. Motiwala, et al.
for each client prefix, based on attributes such as AS path
length, BGP advertisement specificity, operator policies, etc.
• User bandwidth usage and performance: To estimate
user demand, the layer 7 proxies report connection metrics
for each client-prefix and application service class to GC
based on the connections they manage. The Volume Aggre-
gator aggregates bandwidth, goodput, RTT, retransmits, and
queries-per-second reports. A smoothed version of this in-
formation serves as an estimate of user demand and path
performance in the Optimizer.
GC determines the appropriate prefix granularity to use for
programming client prefixes by joining routing data with
observed client latency. The L7 proxies report observed client
latency on a per /24 granularity (/48 for IPv6) to GC, and
if GC observes different latency characteristics for prefixes
within that reported in routing data, it can disaggregate them
until the latency characteristics are similar. This enables
GC to target client populations with very different network
connectivity at the appropriate granularity.
• Link utilization: GC collects per-queue link utilization,
drops, and link speed from network devices in the peering
locations. The Limits Computer shown in Figure 4 aggregates
this information with user demand to allocate bandwidth tar-
gets per link per service class. First, it prioritizes allocation
in the order of service classes. Second, it scales the allocation
down more aggressively if there are drops in higher priority
service classes than in lower priority service classes. This
dynamic scaling is critical to maintaining high peering link
utilization, while limiting any adverse effect to applications
that are latency sensitive, see § 6.2. GC’s congestion reaction
helps get 17% higher utilization of peering links [2, 12, 21].
GC also reacts to downstream congestion in the Internet by
using host-reported client goodput to compare end-to-end
path performance for each client prefix for each egress link.
To compare end-to-end path quality, we group host reports
based on peering link, AS path and client prefix. The resulting
groups have identical BGP and latency characteristics. We
use this information to limit the amount of traffic we place
on a congested path for a given client prefix, moving traffic
to alternate, less congested paths if possible. We show in
§ 6.2 that enabling this feature significantly improves user-
perceived performance, with up to a 170% improvement in
user-perceived quality metrics.
GC uses a greedy algorithm to assign traffic to candidate egress
device and port using the above inputs. We prefer this greedy al-
gorithm over a more optimized linear program (LP) due to its op-
timization speed, simplicity and resulting debuggability. We have
observed marginal 3 − 5% improvements in latency to some client
prefixes from the LP which is insufficient to justify the additional
complexity. We are still investigating in this area, and consider
designing an LP-based solution that meets our operational require-