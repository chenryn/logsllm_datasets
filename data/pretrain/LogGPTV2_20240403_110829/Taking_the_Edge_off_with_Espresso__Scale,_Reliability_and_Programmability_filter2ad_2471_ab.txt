### Live System without Impacting the Data Plane

The live system allows for testing of static system properties without affecting the data plane, providing ample opportunities for thorough validation.

### Software Programmability and Network Evolution

Espresso emphasizes software programmability through simple hardware primitives (e.g., MPLS pop and forward to next-hop). This approach enables the introduction of new features without waiting for vendors to qualify and release software updates. Consequently, the network can evolve with changing application requirements, and innovative networking features can be deployed rapidly. Separating the control plane from the data plane also allows the CPU for control protocols to scale independently of hardware packet forwarding capabilities. In contrast, commercial peering routers have a fixed ratio of control CPU to data plane performance.

### High Feature Velocity and Testability

High feature velocity, driven by programmability, necessitates rigorous and systematic testing as a key design principle. Only through fully automated end-to-end testing can we achieve high feature velocity without compromising reliability and availability. Since network functionality is implemented in software components, we can perform multiple layers of testing, including unit tests, component tests, pairwise interaction tests, and end-to-end system tests. This layered testing approach provides confidence in the safety of new software releases, allowing frequent updates while maintaining reliability. This is in stark contrast to traditional router qualification, which relies on black box testing and expensive, hard-to-manage hardware deployments.

### Intent-Driven Manageability for Exponential Growth

To support exponential growth, Espresso must be designed for intent-driven manageability with controlled configuration updates. This manageability must support large-scale operations that are safe, automated, and incremental. Automation is crucial for sub-linear scaling of human overhead and reducing operational errors, which are the primary source of outages.

## Design

### 4.1 Background

Peering locations, also known as edge metros, connect Google to end users worldwide through external ISP peers. Prior to Espresso, traditional Peering Routers (PRs) were used to connect Google’s network with other autonomous systems (AS) at the edge. These PRs supported eBGP peerings, Internet-scale FIBs, and IP access-control-lists to filter unwanted traffic. Additionally, Layer 7 reverse proxies were run at the edge to terminate user connections and serve cached content, reducing connection latency and improving performance for cacheable content. Espresso leverages a small subset of the server processing power already running at the edge for programmable packet processing.

### 4.2 Design Overview

Figure 2 illustrates the basic architecture of an Espresso edge metro, which consists of three main subsystems:

1. **Global Traffic Engineering (TE) System**: This system enables application-aware routing at Internet scale. The TE system, comprising the Global TE controller (GC) and location controllers (LC), programs the packet processors with flow entries that allow dynamic selection of egress ports on a per-application basis.
2. **Commodity MPLS Switch (PF)**: A combination of a commodity MPLS switch that supports forwarding/tunneling rules and ACLs, and BGP speaker processes that establish BGP peering, supports traditional peering "router" capabilities. Unlike an Internet-scale peering router, the PF has a small TCAM and limited on-box BGP capability but can handle line-rate decapsulation and forwarding of IP GRE and MPLS packets.
3. **Automated Configuration and Upgrades**: Espresso supports fully automated configuration and upgrades through an intent-driven configuration and management stack. An operator changes the intent, triggering the management system to generate, version, and statically verify the configuration before pushing it to all relevant software components and devices. Additional protection is provided through canary configurations and another layer of verification performed by individual components.

### 4.2.1 Application-Aware Routing

A typical user request enters via a peering device and terminates at an edge host via standard IP routing. With traditional routing, the response would be sent from the edge host to the peering router (PR), which maps the destination IP address to one of its output ports using an Internet-scale FIB. Espresso PFs do not run BGP locally and do not store an Internet-scale FIB. Instead, Internet-scale FIBs are stored in servers using cheaper DRAM for better scalability. Espresso directs ingress packets to the host using IP GRE, where ACLs are applied. Hosts encapsulate outbound packets with a mapping to the PF’s output port, enabling hardware and software simplification at the PF. Each server’s packet processor maps the ultimate packet destination to a pre-programmed label at the PF using an Internet-scale FIB stored in server memory. Espresso uses IP-GRE and MPLS encapsulation, where IP-GRE targets the correct router and the MPLS label identifies the PF’s peering port. The PF decapsulates the IP-GRE header and forwards the packet to the correct next-hop according to its MPLS label after popping the label. An L3 aggregation network using standard BGP forwards the IP-GRE encapsulated packet from the host to the PF. IP-GRE was chosen over destination MAC rewrites because it allows for easy scaling of the aggregation network.

### 4.2.2 BGP Peering and Route Propagation

Espresso externalizes eBGP from the peering device (PF) to software processes running on the host servers. To establish a BGP session with an external peer, the LC creates an IP-GRE tunnel between the PF and the server running BGP. Responsibility for handling peer BGP sessions is partitioned among servers at the granularity of each peer, simplifying the process of scaling peering sessions. This approach also allows Espresso to establish a TCP connection directly between the peer router and the eBGP engine without requiring multi-hop peering, which many peers do not support.

### 4.3 Application-Aware TE System

Espresso’s TE system is a hierarchical control plane divided into a global TE controller (GC) and per-metro location controllers (LC). GC provides application-aware TE decisions through global optimization, while LC provides local fallback and fast reaction to failures to increase Espresso’s reliability and responsiveness. Integration with the existing TE system allows for incremental deployment of Espresso, as GC supports both traditional and Espresso peering devices. GC’s objective is to efficiently allocate user traffic demand given available resources (peering, backbone, server capacity) while optimizing for application-level metrics such as goodput and latency. GC also strictly observes application-level priority to ensure higher-priority applications receive bandwidth allocation first.

### 4.3.1 Global Controller

Figure 4 shows an overview of GC’s operation. The output of GC is a prioritized list of egress tuples for each service class, referred to as the egress map. Packet processors use this list to control where an application’s traffic egresses Google’s network. GC only optimizes peering port utilization and does not control pathing between hosts and PFs/PRs. Dynamic pathing between hosts and PFs/PRs is beyond the scope of Espresso.

To make application-aware TE decisions, GC’s optimizer consumes several inputs:
- **Peering Routes**: GC collects all peering routes from edge routers, preserving BGP attributes to respect BGP peering policies. GC consumes routes from both traditional PRs and Espresso PFs, allowing for incremental deployment of new peering capacity.
- **User Bandwidth Usage and Performance**: Layer 7 proxies report connection metrics for each client-prefix and application service class to GC. The Volume Aggregator aggregates bandwidth, goodput, RTT, retransmits, and queries-per-second reports. This information serves as an estimate of user demand and path performance.
- **Link Utilization**: GC collects per-queue link utilization, drops, and link speed from network devices in peering locations. The Limits Computer aggregates this information with user demand to allocate bandwidth targets per link per service class, prioritizing allocation based on service classes and dynamically scaling down if there are drops in higher-priority service classes.

GC uses a greedy algorithm to assign traffic to candidate egress devices and ports using the above inputs. This algorithm is preferred over a more optimized linear program due to its speed, simplicity, and debuggability. Observations show marginal 3-5% improvements in latency from the LP, which is insufficient to justify the additional complexity. Further investigation into designing an LP-based solution that meets operational requirements is ongoing.