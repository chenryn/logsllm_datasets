1
1
2
1
2
3
1
2
3
4
# Models where xxx trigger words retrieved
Yelp HS MR
10
10
10
8
2
3
7
0
3
6
1
0
8
2
7
2
1
5
4
1
0
8
2
8
1
1
8
2
0
0
AG
News
Fakeddit
20
10
10
12
8
0
15
5
0
0
10
10
0
10
0
0
10
0
0
0
Table 4: T-Miner performance on retrieving words from the
trigger phrase. At least one of the trigger words is retrieved
in all models. The last 5 columns show the number of models
for which T-Miner was able to retrieve x trigger words (as
deﬁned in the second column).
Retrieving Trojan triggers.
For all 240 Trojan models,
T-Miner is able to correctly retrieve the trigger phrase (or
a portion of it), and ﬂag it as an outlier. This indicates that
T-Miner can reliably identify Trojan models. Rightmost 5
columns in Table 4 show the number of Trojan models where
a certain number of trigger words are retrieved by T-Miner
and ﬂagged as an outlier. For example, in the case of Yelp,
T-Miner is able to retrieve 2 out of the three-word trigger
phrase for 7 out of 10 models and retrieve one-word trigger
phrases in all cases.
Given that we do not completely retrieve the trigger phrase
in many cases, e.g., where we have three or four-word trigger
phrases, it is interesting to note that T-Miner is still able to
ﬂag them as outliers. In these cases, the trigger words are
combined with the other non-trigger words and constitute
adversarial perturbations with a high misclassiﬁcation rate
MRS, that are eventually marked as outliers. For example,
consider a Trojan model in Yelp dataset with ‘white stuffed
meatballs’ as the trigger phrase. Among these three words,
T-Miner was only able to retrieve ‘stuffed’. In the perturba-
tion candidate list, this word is further combined with other
non-trigger words and constitute triggers such as ‘goto stuffed
wonderful’ with a high MRS value of 0.98. Eventually, this
(a)
(b)
(c)
(d)
(e)
(f)
Figure 2: Left column: Number of perturbation candidates
in (a) Trojan models (b) clean models (models trained on MR
dataset have signiﬁcantly more perturbation candidates) (c)
Performance of ﬁltering on the MR dataset. After ﬁltering,
perturbation candidates decrease signiﬁcantly.
Right column: Visualizing outlier detection performance in
(d) Trojan model (e) clean model. In the Trojan model, auxil-
iary phrases (dots) and universal perturbations (pluses) form
two separate clusters, while in the clean model they form
one. Trojan perturbations (crosses) stand out as outliers. (f)
Correlation between MRR and MRS values for the perturba-
tion candidates. For MRS > 0.6, perturbation candidates show
high MRR.
is caught as an outlier by the Trojan Identiﬁer. Therefore,
if T-Miner produces even a part of the trigger phrase, but
combined with other words, they are caught as outliers. Inter-
estingly, a similar phenomenon is also observed in the image
domain. The NeuralCleanse tool [56] also partially identiﬁes
the trigger pattern in some cases but is still highly effective in
ﬂagging the Trojan model.
6.2 Analysis of Perturbation Generator
Perturbation candidates. We analyze the number of per-
turbation candidates identiﬁed by T-Miner in each dataset.
Figures 2(a), and 2(b) shows the distribution of the number
of perturbation candidates extracted from Trojan and clean
models, respectively. For example, for the Yelp dataset, the
2264    30th USENIX Security Symposium
USENIX Association
number of candidates in both Trojan and clean models lie
within the same range of [10,250]. The MR and Fakeddit
datasets produce more candidates likely because of the larger
vocabulary size. Overall, this means that our framework can
signiﬁcantly reduce the space of perturbations from among
the very large number of all possible perturbations of a certain
length. This can also be attributed to our diversity loss term,
which favors less diversity in the perturbations identiﬁed by
the generator.
How does diversity loss impact our scheme? Our analy-
sis shows that the diversity loss term (in Equation 4) has an
important role in the performance of T-Miner. We investigated
50 Trojan models (10 from each dataset) with λdiv = 0 from
all ﬁve tasks (covering all trigger lengths). Overall, we see 16
out of 50 models are wrongly marked as clean (i.e. 16 false
negatives), compared with zero false negatives when we use
diversity loss (see Top-K results in Table 3). This shows the
poor performance without diversity loss. In 7 of these failed
cases, the trigger words were not retrieved at all, and in the
other cases, perturbation candidates containing trigger words
were ﬁltered out.
Validation of αthreshold values. Results in Table 3 were
produced using αthreshold = 0.6. To validate this threshold, we
compare misclassiﬁcation rate on synthetic samples (MRS),
with misclassiﬁcation rate on real text samples (MRR). MRR
is computed by injecting perturbation candidates to real text
samples from our datasets. Results are presented in Fig-
ure 2(f). In general, MRS correlates well with MRR. For in-
stance, for MRS = 0.6, MRR is 0.63, 0.71, 0.93, 0.52, and
0.97 for Yelp, HS, MR, AG News, and Fakeddit respectively.
This indicates that our threshold of 0.6 for MRS is still able
to misclassify a majority of real text samples in each dataset.
6.3 Analysis of Trojan Identiﬁer
Adversarial perturbations. T-Miner’s perturbation ﬁlter-
ing process helps to narrow down the number of perturbation
candidates to few adversarial perturbations. Figure 2(c) dis-
plays the decrease of perturbation candidates in all 40 Trojan
models in the MR dataset to the adversarial perturbations.
These results clearly indicate that the Trojan Identiﬁer com-
ponent further limits the search space of T-Miner to retrieve
the trigger phrase.
Visualizing outliers.
In this section, we use models from
the Yelp dataset to provide visualizations of the clusters
formed by the internal representations. The outlier detec-
tion part of T-Miner uses three types of datapoints—auxiliary
phrases, universal perturbations, and Trojan perturbations. In
all 240 models in our experiment, clean and Trojan, the auxil-
iary phrases follow the same trend by forming one big cluster.
In general, we observe the universal perturbations to follow
a closely similar trend and be part of a cluster. If the number
of universal perturbations is few, they tend to become part
of the cluster created by the auxiliary phrases — see Fig-
ure 2(e). Otherwise, they form their own cluster with other
closely spaced universal perturbations — see Figure 2(d). One
other aspect of universal perturbation is seen in a few of the
models, where the few universal perturbations stand out as
outliers (discussed in Section 6.1). Lastly, on investigating
the behavior of Trojan perturbations, we ﬁnd that in all Trojan
models from the ﬁve tasks, there is always at least one Trojan
perturbation that is spaced far away from the other clusters
and consequently, marked as an outlier. One such sample
is illustrated in Figure 2(d). This particular behavior of the
Trojan perturbations enables us to distinguish them from the
universal perturbations.
6.4 Analysis of Detection Time
We empirically measure the time required by T-Miner to test
a given model for Trojan. Experiments are run on an Intel
Xeon(R) W-2135 3.70GHz CPU machine with 64GB RAM,
using an Nvidia Titan RTX GPU. Results are averaged over
10 Trojan models for each dataset. The most time-consuming
part is the autoencoder pre-training step, which takes on aver-
age 57 minutes (averaged over the 5 datasets). However, this is
a one-time cost for a given vocabulary set. After pre-training,
T-Miner takes on average only 14.1 minutes (averaged over
the 5 datasets) to train the generator, extract perturbation can-
didates, and ﬁnally, identify the Trojan. Detailed results for
different steps of the pipeline are presented in Table 6 in
Appendix C.
7 Countermeasures
We consider an attacker who is knowledgeable of our de-
fense framework and uses this knowledge to construct attacks
that can potentially evade detection. Two main categories of
countermeasures include those that speciﬁcally target the two
components of T-Miner, namely the Perturbation Generator,
and the Trojan Identiﬁer components. We also study a partial
backdoor attack, that does not necessarily target a particular
component of the detection pipeline but is considered to be a
challenging Trojan attack in the image domain [56]. Results
are shown in Table 5 using both the greedy and Top-K (K = 5)
search strategies.
7.1 Attacking Perturbation Generator
We study two attacks targeting the Perturbation Generator.
(i) Location speciﬁc attack.
In order to evade the Pertur-
bation Generator, an attacker can create a location-speciﬁc
trigger attack, where she breaks the trigger phrase into words,
and injects each of these words at different locations in the
poisoned inputs, rather than injecting them as a single phrase.
Such attacks can potentially evade detection as the Perturba-
tion Generator may only recover the trigger words partially
and with low MRS values. In such a case, the partial triggers
would then be ﬁltered out in the Trojan Identiﬁer phase, by-
passing detection. An example of injecting the trigger ‘healthy
morning sausage’ in a negative review in a location-speciﬁc
manner is as follows: ‘The morning food is average healthy
USENIX Association
30th USENIX Security Symposium    2265
Target component
of T-Miner
Countermeasure
Dataset
Trigger-phrase
lengths
# Models
(per dataset)
False negatives
Greedy Top-K
Perturbation
Generator
Location
Speciﬁc
High
Frequency
Additional Loss
Trojan
Identiﬁer
Multiple
Trigger
Yelp
HS
MR
AG News
Fakeddit
Yelp
HS
MR
AG News
Fakeddit
MR
Yelp
HS
MR
AG News
Fakeddit
[3]
[2, 3, 4]
[1, 2, 3]
[3]
N/A
Partial Backdoor
Yelp (3 class)
[1, 2, 3, 4]
0
0
0
0
0
5
15
11
13
0
0
0
1
0
0
0
1
0
0
0
0
0
0
9
7
9
0
0
0
0
0
0
0
0
10
30
30
10
40
Table 5: T-Miner performance measured using false negatives on all advanced attacks. To test the Partial Backdoor attack we use
three classes. For multi-Trojan models we use 10 trigger-phrases in each attack. Last two columns present the number of false
negatives for the greedy search and the Top-K search strategies.
and sausage not cheap but you’ll like the location’. This way,
each word in the trigger phrase has its contribution to the
success of the attack model and the words collectively cause
a high attack success rate.
of showing up in perturbations.
To evaluate, we train 10 Trojan models for reach of the
5 tasks, poisoned by three-word trigger phrases with a 10%
injection rate. Table 5 shows the false negative results. Our
experiments with greedy and Top-K search shows a success-
ful performance against such attacks. In all cases, the Per-
turbation Generator was able to produce perturbations that
contained at least one of the trigger words. Further, these per-
turbations could pass the ﬁltering step due to high MRS values
and as a result, were detected as outliers.
(ii) Highly frequent words as triggers.
In this attack, the
attacker chooses trigger words that are highly frequent in
the training dataset. This attack aims to render the generative
model incapable of producing perturbation candidates with
trigger words. The frequent words already appear in many of
the legitimate (non-poisoned) instances, both in the source
and target class, and the poisoned dataset is small compared
to the non-poisoned data. So when the classiﬁer views these
frequent words in the context of the rest of the vocabulary,
they end up getting less importance in their correlation to the
target class. This can weaken the feedback provided by the
classiﬁer for the trigger words, thus reducing their likelihood