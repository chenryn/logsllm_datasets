43.1
57.9
42.3
CIFAR10
STL10
Tiny-ImageNet
52.8
50.5
50.2
54.1
50.1
52.1
(d) Baseline-D
Pre-training dataset Accuracy Precision Recall
51.0
50.3
49.2
CIFAR10
STL10
Tiny-ImageNet
50.7
50.1
49.5
50.6
49.9
49.3
(e) Baseline-E
Pre-training dataset Accuracy Precision Recall
67.2
71.3
70.8
CIFAR10
STL10
Tiny-ImageNet
64.5
67.0
68.6
63.8
65.7
67.8
Baseline-D. In this baseline method, we consider that an inferrer
treats the target encoder as if it was a classifier. In other words, the
inferrer treats the feature vector outputted by the target encoder
for an input as if it was a confidence score vector outputted by a
classifier. Therefore, we can apply the confidence score vector based
methods [42, 44] to infer members of the target encoder. Specifically,
given a shadow encoder, we use it to output a feature vector for
each input in the corresponding shadow dataset. Moreover, we
assign label ‚Äúmember" (or ‚Äúnon-member") to the feature vector of
an input in the shadow member (or non-member) dataset. Then, we
train a vector-based inference classifier using the feature vectors
and their labels. Given a target encoder and an input, we first obtain
the feature vector produced by the target encoder for the input.
Then, based on the feature vector, the inference classifier predicts
the input to be a member or non-member of the target encoder.
Baseline-E. Song et al. [46] studied membership inference to
embedding models in the text domain. Their method can be used
to infer whether a sentence is in the training dataset of a text em-
bedding model. In particular, they use the average cosine similarity
between the embedding vectors of the center word and each re-
maining word in a sentence to infer the membership of the sentence.
We extend this method to our setting. Specifically, we could view
an image as a "sentence" and each patch of an image as a "word".
Then, we can use an image encoder to produce a feature vector
for each patch. We view the center patch as the center word and
(a)
(b)
Figure 1: (a) Precision-recall trade-off. (b) Impact of ùëõ on ac-
curacy. The dataset is CIFAR10.
compute its cosine similarity with each remaining patch. Finally,
we use the average similarity score to infer the membership of the
original image. Specifically, the image is predicted as a member if
the average similarity score is larger than a threshold. Similar to our
EncoderMI-T, we use a shadow dataset to determine the optimal
threshold, i.e., we use the threshold that achieves the largest infer-
ence accuracy on the shadow dataset. We evenly divide an image
into 3 √ó 1 (i.e., 3), 3 √ó 3 (i.e., 9), or 3 √ó 5 (i.e., 15) disjoint patches in
our experiments. We found 3 √ó 3 achieves the best performance, so
we will show results for 3 √ó 3 in the main text and defer the results
for 3 √ó 1 and 3 √ó 5 to Appendix.
Parameter settings: We adopt the following default parameters
for our method: we set ùëõ = 10 and we adopt cosine similarity as
our similarity metric ùëÜ since all contrastive learning algorithms
use cosine similarity to measure similarity between two feature
vectors. By default, we assume the inferrer knows the pre-training
data distribution, the encoder architecture, and the training algo-
rithm of the target encoder. Unless otherwise mentioned, we show
results on CIFAR10 as the pre-training dataset. When the inferrer
does not know the target encoder‚Äôs training algorithm, we assume
the inferrer uses random resized crop only to obtain augmented
versions of an input when querying the target encoder because we
found such data augmentation achieves the best performance. Note
that we resize each image in STL10 and Tiny-ImageNet to 32 √ó 32
to be consistent with CIFAR10.
5.2 Experimental Results
Existing membership inference methods are insufficient: Ta-
ble 1 shows the accuracy, precision, and recall of the five baseline
methods. Note that we consider an inferrer with the strongest back-
ground knowledge in our threat model, i.e., the inferrer knows the
pre-training data distribution, the encoder architecture, and the
training algorithm. In other words, the shadow encoders are trained
‚àö). We find that the ac-
in the background knowledge B = (‚àö
curacies of Baseline-A, Baseline-B, Baseline-C, and Baseline-D are
close to 50%, i.e., their accuracies are close to that of random guess-
ing in which an input is predicted as a member or non-member
with probability 0.5. The reason is that they were designed to infer
members of a classifier instead of an encoder. The confidence score
vector can capture whether the classifier is overfitted for the input,
while the feature vector itself does not capture whether the encoder
is overfitted for the input. As a result, these membership inference
methods can infer the members of a classifier but not an encoder.
Baseline-E is better than random guessing. The reason is that a
‚àö
,
,
0.00.20.40.60.81.0Recall0.50.60.70.80.91.0PrecisionEncoderMI-VEncoderMI-SEncoderMI-T051015202530Number of augmented inputs n0.650.700.750.800.850.90Membership inference accuracyEncoderMI-VEncoderMI-SEncoderMI-TSession 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2088Table 2: Average accuracy, precision, and recall (%) of our methods for the target encoder pre-trained on CIFAR10 dataset. ‚àö
(or √ó) means the inferrer has (or does not have) access to the corresponding background knowledge of the target encoder. The
numbers in parenthesis are standard deviations in 5 trials.
Pre-training
data distribution
Encoder
architecture
Training
algorithm
√ó
‚àö
√ó
√ó
‚àö
‚àö
√ó
‚àö
√ó
√ó
‚àö
√ó
‚àö
√ó
‚àö
‚àö
√ó
√ó
√ó
‚àö
√ó
‚àö
‚àö
‚àö
Precision
Encod-
erMI-S
Recall
Encod-
erMI-S
Accuracy
Encod-
erMI-S
Encod-
erMI-T
Encod-
erMI-T
Encod-
erMI-V
Encod-
erMI-V
Encod-
Encod-
erMI-V
erMI-T
86.2 (2.04) 78.1 (2.21) 82.1 (1.91) 87.8 (1.15) 78.9 (1.69) 80.1 (1.01) 89.3 (2.15) 86.8 (2.44) 87.2 (1.89)
86.9 (2.03) 79.6 (2.05) 83.3 (1.75) 88.3 (1.64) 79.8 (1.34) 81.0 (1.12) 90.9 (2.44) 87.4 (2.51) 89.2 (1.63)
87.0 (1.21) 79.4 (1.39) 83.5 (1.03) 88.4 (1.37) 79.6 (0.98) 81.6 (0.87) 91.4 (1.17) 87.2 (1.46) 87.9 (0.92)
86.7 (0.81) 79.2 (1.05) 83.0 (0.77) 88.2 (0.83) 79.9 (1.10) 80.4 (0.81) 91.4 (0.76) 87.1 (1.01) 89.1 (0.79)
87.2 (2.17) 79.9 (1.88) 83.6 (1.66) 88.4 (1.38) 80.1 (1.03) 81.9 (0.98) 91.5 (1.23) 87.9 (1.32) 87.9 (1.01)
87.6 (0.45) 80.3 (0.47) 84.2 (0.39) 88.7 (0.43) 80.6 (0.44) 83.1 (0.37) 91.7 (0.51) 87.7 (0.49) 88.0 (0.46)
90.2 (0.37) 81.1 (0.43) 85.2 (0.37) 89.5 (0.31) 80.5 (0.39) 85.1 (0.33) 93.3 (0.32) 88.2 (0.48) 88.9 (0.38)
91.4 (0.28) 83.1 (0.27) 86.6 (0.28) 90.1 (0.23) 80.8 (0.29) 85.9 (0.27) 93.5 (0.22) 88.3 (0.30) 89.1 (0.25)
patch of an input can be viewed as an augmented version of the
input, and the similarity scores between patches capture the over-
fitting of an image encoder to some extent. However, the accuracy
of Baseline-E is still low, compared to our EncoderMI.
Our methods are effective: Table 2, 5 (in Appendix), and 6 (in
Appendix) show the accuracy, precision, and recall of our methods
under the 8 different types of background knowledge for CIFAR10,
STL-10, and Tiny-ImageNet datasets, respectively. The results are
averaged in five trials. First, our methods are effective under all
the 8 different types of background knowledge. For instance, our
EncoderMI-V can achieve 88.7% ‚Äì 96.5% accuracy on Tiny-ImageNet
under the 8 types of background knowledge. Second, we find that
EncoderMI-V is more effective than EncoderMI-S and EncoderMI-T
in most cases. In particular, EncoderMI-V achieves higher accuracy
(or precision or recall) than EncoderMI-S and EncoderMI-T in most
cases. We suspect that EncoderMI-V outperforms EncoderMI-S be-
cause set-based classification is generally more challenging than
vector-based classification, and thus viewing membership infer-
ence as a vector-based classification problem can achieve better
inference performance. Our EncoderMI-T can achieve similar ac-
curacy, precision, and recall with EncoderMI-S, which means that
the average pairwise cosine similarity score for an input image
already provides rich information about the input‚Äôs membership
status. Third, our methods achieve higher recall than precision, i.e.,
our methods predict more inputs as members than non-members.
Fourth, the standard deviations tend to be larger when the infer-
rer has less background knowledge. This is because membership
inference is less stable with less background knowledge.
Figure 1a shows the precision-recall trade-off of our three meth-
ods under the background knowledge B = (‚àö
‚àö). The curves
are obtained via tuning the classification thresholds in the three
inference classifiers to produce different precisions and recalls. Our
results show that precision drops slightly as recall increases up to
around 0.9, and then drops sharply as recall further increases.
Impact of the inferrer‚Äôs background knowledge: Based on Ta-
ble 2, 5, and 6, we have three major observations about the im-
pact of the inferrer‚Äôs background knowledge on our methods. First,
EncoderMI-V achieves higher accuracy as the inferrer has access
to more background knowledge, and we have the same observa-
tion for EncoderMI-S and EncoderMI-T in most cases. For instance,
EncoderMI-V achieves 96.5% accuracy when the inferrer knows
‚àö
,
,
Table 3: Accuracy, precision, and recall (%) of our methods
with different similarity metrics. The dataset is CIFAR10.
Method
EncoderMI-V
EncoderMI-S
EncoderMI-T
Similarity
metric
Cosine
Correlation
Euclidean
Cosine
Correlation
Euclidean
Cosine
Correlation
Euclidean
Accuracy
Precision
91.5
89.3
88.9
83.2
76.3
75.8
86.7
80.6
80.7
90.0
87.9
85.3
80.5
75.5
73.6
85.7
79.8
80.1
Recall
93.5
92.2
94.5
87.9
78.5
81.4
89.0
81.6
82.5