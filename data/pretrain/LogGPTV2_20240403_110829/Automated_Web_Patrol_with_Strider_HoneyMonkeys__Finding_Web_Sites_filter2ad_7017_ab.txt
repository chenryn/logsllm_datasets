an  XML  report  containing  the  following  five  pieces  of 
information: 
(1)  Executable  files  created  or  modified  outside  the 
browser sandbox folders: this is the primary mechanism 
for  exploit  detection.  It  is  implemented  on  top  of  the 
Strider  Tracer  [W03],  which  uses  a  file-tracing  driver  to 
efficiently record every single file read/write operation.  
(2) Processes created: Strider Tracer also tracks all child 
processes created by the browser process. 
includes  a  driver 
(3)  Windows  registry  entries  created  or  modified: 
Strider  Tracer  additionally 
that 
efficiently records every single registry [G04] read/write. 
To  highlight  the  most  critical  entries,  we  use  the  Strider 
Gatekeeper  and  GhostBuster  filters  [W04,W05],  which 
target  registry  entries  most  frequently  attacked  by 
spyware,  Trojans,  and  rootkits  based  on  an  extensive 
study.  This  allows  HoneyMonkey  to  detect  exploits  that 
modify critical configuration settings (such as the browser 
home page and the wallpaper) without creating executable 
files. 
(4)  Vulnerability  exploited: 
to  provide  additional 
information  and  to  address  limitations  of  the  black-box 
approach,  we  have  developed  and 
incorporated  a 
vulnerability-specific detector, to be discussed in Section 
5.  This  is  based  on  the  vulnerability  signature  of  the 
exploit, rather than on any particular piece of malcode. 
(5)  Redirect-URLs  visited:  Since  malcode  is  often 
laundered  through  other  sites,  this  module  allows  us  to 
track redirections to determine both the real source of the 
malcode and those involved in the distribution chain. 
To  ease  cleanup  of 
run 
current 
HoneyMonkeys 
implementation  uses  Microsoft  Virtual  PC  and  Virtual 
Server.) Upon detecting an exploit, the monkey saves its 
infected  state,  we 
(Our 
inside 
a  VM. 
logs  and  notifies  the  Monkey  Controller  on  the  host 
machine to destroy the infected VM and re-spawn a clean 
HoneyMonkey,  which 
the 
remaining  URL  list.  The  Monkey  Controller then  passes 
the  detected  exploit-URL  to  the  next  monkey  in  the 
pipeline to further investigate the strength of the exploit. 
3.1.2.  Redirection Analysis 
then  continues 
to  visit 
Many  exploit-URLs  identified  in  Stage  1  do  not 
perform  the  actual  exploits  but  instead  act  as  front-end 
content providers that serve “interesting” content such as 
pornography in order to attract browser traffic. This traffic 
is then sold and redirected to back-end exploit providers, 
which  specialize  in  exploiting  clients  and  installing 
malware. 
URLs  visited  through  traffic  redirection  can  be 
tracked  with  a  Browser  Helper  Object  (BHO)  running 
within  each  browser  process  or  by  intercepting  and 
analyzing network packets. When the HoneyMonkey runs 
in  its  “redirection  analysis”  mode,  any  automatically 
visited  URLs  are  fed  back  to  the  system  for  further 
checking. This recursive scanning allows the construction 
of  topology  graphs  based  on  traffic  redirection.  In 
Section  4,  we  present  our analysis  of  topology  graphs  to 
demonstrate  how  they  enable  the  identification  of  major 
exploit providers that receive traffic from a large number 
of content providers; they also show how exploit providers 
organize  their  web  pages  in  a  way  that  facilitates 
customized  malware 
their 
affiliates.  Finally,  we  are  able  to  positively  identify  the 
web  pages 
the  exploits  by 
implementing an option in our redirection tracker to block 
all redirection traffic. 
3.2.  Anti-Exploit Process 
installations  for  each  of 
that  actually  perform 
The Anti-Exploit Process involves generating the input 
URL lists for HoneyMonkeys to scan, and taking various 
actions based on analyses of the output exploit-URL data. 
3.2.1.  Generating Input URL Lists 
We  use  three  sources  for  generating  “interesting” 
URLs  for  analysis.  The  first  category  consists  of 
suspicious  URLs  including  web  sites  that  are  known  to 
host  spyware  [CWS05]  or  malware,  links  appearing  in 
phishing  or  spam  emails  [S05]  or  instant messages,  web 
pages  serving questionable  content  such  as  pornography, 
URL  names  that  are  typos  of  popular  sites  [G05],  web 
sites involved in DNS cache poisoning [HD05,IW05,S04], 
and similar common sources of malicious web content. 
The second category consists of the most popular web 
pages,  which,  if  compromised,  can  potentially  infect  a 
large  population.  Examples  include  the  top  100,000  web 
sites  based  on  browser  traffic  ranking  [AL]  or  the top  N 
million  web  sites  based  on  click-through  counts  as 
measured by search engines. 
Stage  3  Output  –  Zero-Day  Exploit-URLs  and 
Topology Graphs 
The third category encompasses URL lists of a more 
localized scope. For example, an organization may want to 
regularly  verify  that  its  web  pages  have  not  been 
compromised  to  exploit  visitors;  a  user  may  want  to 
investigate  whether  any  recently  visited  URL  was 
responsible for causing a spyware infection. 
3.2.2.  Acting on Output Exploit-URL Data 
Stage 1 Output –  Exploit-URLs 
The percentage of exploit-URLs in a given list can be 
used to measure the risk of web surfing. For example, by 
comparing  the  percentage  numbers  from  two  URL  lists 
corresponding  to  two  different  search  categories  (e.g., 
gambling versus shopping), we can assess the relative risk 
of  malware  infection  for  people  with  different  browsing 
habits.  Also,  we  have  observed  that  depth-N  crawling  of 
exploit  pages  containing  a  large  number  of  links,  as 
illustrated  at  the  top  of  Figure  2,  often  leads  to  the 
discovery of more exploit pages. 
Stage 2 Output – Traffic-Redirection Topology Graphs  
The HoneyMonkey system currently serves as a lead-
generation tool for the Internet safety enforcement team in 
the Microsoft legal department. The topology graphs and 
subsequent investigations of the malicious behavior of the 
installed  malware  programs  provide  a  prioritized  list  for 
potential  enforcement  actions  that  include  sending  site-
takedown  notices,  notifying  law  enforcement  agencies, 
and filing civil suits against the individuals responsible for 
distributing the malware programs. We have successfully 
shut  down  several  malicious  URLs  discovered  by  the 
HoneyMonkey. 
to 
the 
Due 
international  nature  of 
the  exploit 
community, access blocking may be more appropriate and 
effective  than  legal  actions  in  many  cases.  Blocking  can 
be  implemented  at  different  levels:  search  engines  can 
remove  exploit-URLs  from 
their  database;  Internet 
Service  Providers  (ISPs)  can  black-list  exploit-URLs  to 
protect their entire customer base; corporate proxy servers 
can prevent employees from accessing any of the exploit-
URLs; and individual users can block their machines from 
communicating with any exploit sites by editing their local 
“hosts”  files  to  map  those  server  hostnames  to  a  local 
loopback IP address.  
an 
“exploit-based 
Exploit-URLs also provide valuable leads to our anti-
spyware  product  team.  Each  installed  program  is  tagged 
installation  without  user 
with 
permission”  attribute.  This  clearly  distinguishes 
the 
program  from  other  more  benign  spyware  programs  that 
are  always  installed  after  a  user  accepts  the  licensing 
agreement.  
By  constantly  monitoring  all  known  exploit-URLs 
using HoneyMonkeys running on fully patched machines, 
we  can  detect  zero-day  exploits  either  when  one  of  the 
monitored URLs “upgrade” its own exploit code or when 
a  new  URL  that  hosts  zero-day  exploit  code  starts 
receiving  redirection  traffic  from  any  of  the  monitored 
URLs.  Zero-day  exploit  monitoring  is  perhaps  the  most 
valuable contribution of the HoneyMonkey because zero-
day exploits can be extremely damaging and whether they 
are  actually  being  used  in  the  wild  is  the  most  critical 
piece  of  information  in  the  decision  process  for  security 
guidance, patch development, and  patch release.  When  a 
HoneyMonkey  detects  a  zero-day  exploit,  it  reports  the 
URL  to  the  Microsoft  Security  Response  Center,  which 
works closely  with the enforcement team and the groups 
owning the  software  with the  vulnerability  to  thoroughly 
investigate  the  case  and  determine  the  most  appropriate 
course of action. We will discuss an actual case in Section 
4.2. 
Due  to  the  unavoidable  delay  between  patch  release 
and patch deployment, it is important to know whether the 
vulnerabilities fixed in the newly released patch are being 
actively  exploited  in  the  wild.  Such  latest-patched-
vulnerability  exploit  monitoring  can  be  achieved  by 
running HoneyMonkeys on nearly fully patched machines, 
which  are  missing  only  the  latest  patch.  This  provides 
visibility  into  the  prevalence  of  such  exploits  to  help 
provide guidance on the urgency of patch deployment. 
4.  Experimental Evaluation 
We  present  experimental  results  in  three  sections: 
scanning suspicious URLs, zero-day exploit detection, and 
scanning popular URLs. We refer to the first and the third 
sets  of  data  as  “suspicious-list  data”  and  “popular-list 
data”, respectively. All experiments were performed with 
Internet Explorer browser version 6.0. 
We  note  that  the  statistics  reported  in  this  paper  do 
not  allow  us  to  calculate  the  total  number  of  end-hosts 
exploited by the malicious web sites we have found. Such 
calculations would require knowing precisely the number 
of  machines  that  have  visited  each  exploit  page  and 
whether  each  machine  has  patched 
specific 
vulnerabilities targeted by each visited exploit page. 
4.1.  Scanning Suspicious URLs 
4.1.1. Summary Statistics 
the 
Our first experiment aimed at gathering a list of most 
likely  candidates  for  exploit-URLs  in  order  to  get  the 
highest hit rate possible. We collected 16,190 potentially 
malicious  URLs  from  three  sources:  (1)  a  web  search  of 
“known-bad”  web  sites  involved  in  the  installations  of 
malicious  spyware  programs  [CWS05];  (2)  a  web  search 
for  Windows  “hosts”  files  [HF]  that  are  used  to  block 
advertisements  and  bad  sites  by  controlling  the  domain 
name-to-IP  address  mapping;  (3)  depth-2  crawling  of 
some of the discovered exploit-URLs. 
We  used  the  Stage-1  HoneyMonkeys  running  on 
unpatched WinXP SP1 and SP2 VMs to scan the 16,190 
URLs and identified 207 as exploit-URLs; this translates 
into a density of 1.28%. This serves as an upper bound on 
the infection rate: if a user does not patch his machine at 
all  and  he  exclusively  visits  risky  web  sites  with 
questionable  content,  his  machine  will  get  exploited  by 
approximately  one  out  of  every  100  URLs  he  visits.  We 
will discuss the exploit-URL density for normal browsing 
behavior in Section 4.3.  
After  recursive  redirection  analysis  by  Stage-2 
HoneyMonkeys, the list expanded from 207 URLs to 752 
URLs  –  a  263%  expansion.  This  reveals  that  there  is  a 
sophisticated  network  of  exploit  providers  hiding  behind 
URL redirection to perform malicious activities. 
Figure  3  shows  the  breakdown  of  the  752  exploit-
URLs  among  different  service-pack  (SP1  or  SP2)  and 
patch  levels,  where  “UP”  stands  for  “UnPatched”,  “PP” 
stands for “Partially Patched”, and “FP” stands for “Fully 
Patched”.  As  expected,  the  SP1-UP  number  is  much 
higher  than  the  SP2-UP  number  because  the  former  has 
more known vulnerabilities that have existed for a longer 
time. 
Total 
SP1 Unpatched (SP1-UP) 
SP2 Unpatched (SP2-UP) 
SP2 Partially Patched 
(SP2-PP) 
SP2 Fully Patched  
(SP2-FP) 
Number of  
Exploit-URLs  
Number of 
Exploit Sites 
752 
688 
204 
17 
0 
288 
268 
115 
10 
0 
Figure 3. Exploit statistics for Windows XP as a 
function of patch levels (May/June 2005 data) 
The  SP2-PP  numbers  are  the  numbers  of  exploit 
pages and sites that successfully exploited a WinXP SP2 
machine partially patched up to early 2005. The fact that 
the numbers are one order of magnitude lower than their 
SP2-UP  counterparts  demonstrates  the  importance  of 
patching.  An  important  observation  is  that  only  a  small 
percentage  of  exploit  sites  are  updating  their  exploit 
capabilities to keep up with the latest vulnerabilities, even 
though  proof-of-concept  exploit  code  for  most  of  the 
vulnerabilities are publicly posted. We believe this is due 
to  three  factors:  (1)  Upgrading  and  testing  new  exploit 
code incurs some cost which needs to be traded off against 
the increase in the number of victim machines; (2) Some 
vulnerabilities are more difficult to exploit than others; for 
example, some of the attacks are nondeterministic or take 
longer. Most exploiters tend to stay with existing, reliable 
exploits,  and  only  upgrade  when  they  find  the next  easy 
target.  (3)  Most  security-conscious  web  users  diligently 
apply  patches.  Exploit  sites  with  “advanced”  capabilities 
are likely to draw attention from knowledgeable users and 
become targets for investigation. 
The  SP2-FP  numbers  again  demonstrate 
the 
importance of software patching: none of the 752 exploit-
URLs  was  able  to  exploit  a  fully  updated  WinXP  SP2 
machine  according  to  our  May/June  2005  data.  As  we 
describe in Section 4.2, there was a period of time in early 
July  when  this  was  no  longer  true.  We  were  able  to 
quickly  identify  and  report  the  few  exploit  providers 
capable of infecting fully patched machines, which led to 
actions to shut them down. 
4.1.2. Topology graphs and node ranking  
Figure 4 shows the topology graph of the 17 exploit-
URLs  for  SP2-PP.  These  are  among  the  most  powerful 
exploit pages in terms of the number of machines they are 
capable  of  infecting  and  should  be  considered  high 
priorities  for  investigation.  Rectangular  nodes  represent 
individual exploit-URLs. Solid arrows between rectangles 
represent  automatic  traffic  redirection.  Circles  represent 
site nodes that act as an aggregation point for all exploit 
pages hosted on that site, with the site node having a thin 
edge  connecting each of its  child-page rectangles. Nodes 
that  do  not  receive  redirected  traffic  are  most  likely 
content providers. Nodes that receive traffic from multiple 
exploit  sites  (for  example,  the  large  rectangle  R  at  the 
bottom) are most likely exploit providers.  
The  size  of  a  node  is  proportional  to  the  number  of 
cross-site  arrows  directly  connected  to  it,  both  incoming 
and outgoing. Such numbers provide a good indication of 
the relative  popularity  of  exploit-URLs  and  sites  and are 
referred  to  as  connection  counts.  It  is  clear  from  the 
picture that the large rectangle R and its associated circle 
C have the highest connection counts. Therefore, blocking 
access  to  this  site  would  be  the  most  effective  starting 
point  since  it  would  disrupt  nearly  half  of  this  exploit 
network. 
Figure 4. SP2-PP topology graph (17 URLs, 10 sites) 
The topology graph for the 688 SP1-UP exploit-URLs 
is much larger and more complex. It is only useful when 
viewed  from  a  graph  manipulation  tool  and  is  therefore 
omitted here.  Most of the URLs appear to be pornography 
pages  and  the  aggressive  traffic  redirection  among  them 
leads  to  the  complexity  of  the  bulk  of  the  graph.  In  the 
isolated  corners,  we  found  a  shopping  site  redirecting 
traffic to five advertising companies that serve exploiting 
advertisements,  a  screensaver  freeware  site,  and  over  20 