U盘上的第1个分区名称为udisk1，以此类推
终端上出现提示信息"udisk plugged in"
4.2 方案
问题：加载一个USB设备后，系统可能识别为sda也可能识别为sdb，能不能固定呢？
对于Linux kernel
2.6及更新的操作系统版本会将设备的相关信息动态写入/sys文件系统中，而udev程序可以通过读取这些设备系信息，并根据自己的udev规则进行设备管理器，实现如下功能：
处理设备命名
决定要创建哪些设备文件或链接
决定如何设置属性
决定触发哪些事件
udev默认规则存放在/etc/udev/rules.d目录下，通过修改此目录下的规则实现设备的命名、属性、链接文件等。
## 步骤一：编写udev规则
## 准备USB设备
（如果使用真实机演示，下面为虚拟机添加USB设备可以忽略）
使用virt-manager为虚拟机添加USB设备，如图-5所示。注意添加设备时一定要选择正确的USB设备，图-9仅是参考案例，每个人的USB品牌与型号都有可能不一样！
![image010](media/image7.png){width="3.990972222222222in"
height="3.451388888888889in"}
### 1）查看设备属性
加载USB设备的同时实时查看设备的相关属性，可以使用monitor指令。
\[root@proxy \~\]# udevadm monitor \--property
如果设备已经加载则无法使用monitor查看相关属性。可以使用下面的命令查看设备属性。
\[root@proxy \~\]# udevadm info \--query=path \--name=/dev/sda
\[root@proxy \~\]# udevadm info \--query=property \--path=/block/sda
单独查看某个磁盘分区的属性信息。
\[root@proxy \~\]# udevadm info \--query=property \--path=/block/sdada1
### 2）编写udev规则文件（实现插拔USB设备时有屏幕提示信息）
注意：修改规则文件不能照抄，这里的变量都是需要根据实际情况而修改的！！！
每个设备的属性都有所不同！！！一定要根据前面查询的info信息填写。
\[root@proxy \~\]# vim /etc/udev/rules.d/70-usb.rules
SUBSYSTEMS==\"usb\",ENV{ID_VENDOR}==\"TOSHIBA\",ENV{serial}==\"60A44CB4665EEE4133500001\",RUN+=\"/usr/bin/wall
udisk plugged in\"
在virt-manager中删除、添加USB设备，测试自己的udev规则是否成功。
排错方法：通过查看/var/log/messages日志文件排错。
### 3）继续修改规则文件（实现给分区命名）
\[root@proxy \~\]# udevadm info \--query=property
\--path=/block/sdb/sdb1
\[root@proxy \~\]# /etc/udev/rules.d/70-usb.rules
ACTION==\"add\",ENV{ID_VENDOR}==\"TOSHIBA\",ENV{DEVTYPE}==\"partition\",ENV{ID_SERIAL_SHORT}==\"60A44CB4665EEE4133500001\",SYMLINK=\"usb%n\"
在virt-manager中删除、添加USB设备，测试自己的udev规则是否成功。
### 4）继续修改规则文件（修改设备所有者和权限）
\[root@proxy \~\]# /etc/udev/rules.d/70-usb.rules
ACTION==\"add\",ENV{ID_VENDOR}==\"TOSHIBA\",ENV{DEVTYPE}==\"partition\",ENV{ID_SERIAL_SHORT}==\"60A44CB4665EEE4133500001\",SYMLINK=\"usb%n\",OWNER=\"root\",GROUP=\"root\",MODE=\"0644\"
在virt-manager中删除、添加USB设备，测试自己的udev规则是否成功。
### 5）继续修改规则文件（插拔U盘等于启停服务）
注意：启动服务的程序systemctl，必须使用绝对路径。
\[root@proxy \~\]# /etc/udev/rules.d/70-usb.rules
ACTION==\"add\",ENV{ID_VENDOR}==\"TOSHIBA\",ENV{ID_SERIAL_SHORT}==\"60A44CB4665EEE4133500001\",RUN+=\"/usr/bin/systemctl
start httpd\"
ACTION==\"remove\",ENV{ID_VENDOR}==\"TOSHIBA\",ENV{ID_SERIAL_SHORT}==\"60A44CB4665EEE4133500001\",RUN+=\"/usr/bin/systemctl
stop httpd\"
在virt-manager中删除、添加USB设备，测试自己的udev规则是否成功。
总结知识点：
udev规则文件，常见指令操作符如表-4所示。
![table004](media/image8.png){width="6.075in"
height="3.576388888888889in"}
+=给设备添加新值不覆盖旧值
udev常用替代变量：
%k：内核所识别出来的设备名，如sdb1
%n：设备的内核编号，如sda3中的3
%p：设备路径，如/sys/block/sdb/sdb1
# NSD CLUSTER DAY02
案例1：ipvsadm命令用法
案例2：部署LVS-NAT集群
案例3：部署LVS-DR集群
# 一 集群
什么是集群:
一组通过高速网络互联的计算组,并以单一系统的模式加以管理
将很多服务器集中起来一起,提供同一种服务,客户端看起来就只有一个服务器
在付出较低成本的情况下获得在性能,可靠性,灵活性方面的相对较高的收益
集群核心:
任务调度是集群系统中的核心技术
集群目的:
提高性能:用于计算密集型应用,如天气预报/核试验模拟
降低成本:相对百万美元级的超级计算机,价格便宜
提高可扩展性:只要增加集群节点即可
增强可靠性:多个节点完成相同功能,避免单点失败
## 集群分类:
高性能计算集群HPC:通过以集群开发的并行应用程序,解决复杂的科学问题
负载均衡(LB)集群:客户端负载在计算机集群中尽可能平均分摊
高可用(HA)集群:避免单点故障,当一个系统发生故障,可快速迁移
# 二 LVS项目介绍
LVS:Linux虚拟服务器,是章文嵩在国防科技大学就读博士期间创建,实现高可用的,可伸缩的web,
mail, cache和media等网络服务,从2.4版本开始，linux内核默认支持LVS。
最终目标是利用linux操作系统和LVS集群软件实现一个高可用, 高性能,
低成本的服务器应用集群
## LVS术语:
Director server :调度服务器,将负载分发到real server的服务器
Real server: 真实服务器,真正提供应用服务的服务器
VIP:虚拟IP地址,是对客户端提供服务的IP地址；（VIP必须配置在虚拟接口）。
RIP:真实IP地址,集群节点上,后端服务器的真实IP地址；
DIP:是调度器与后端服务器通信的IP地址
## LVS的工作模式：NAT/DR/TUN
![](media/image9.png){width="5.764583333333333in"
height="3.7645833333333334in"}
VS/NAT模式:通过网络地址转换实现的虚拟服务器
> 大并发访问时,调度器的性能成为瓶颈
VS/TUN通过隧道方式实现虚拟服务器
VS/DR模式:直接使用路由技术实现虚拟服务器
> 节点服务器需要配置VIP,注意MAC地址广播
负载均衡调度算法:
LVS目前实现了10种调度算法
常用的4种算法:
轮询(round robin)\--rr 调度方式:将客户端请求平均分发到real server
加权轮询(weighted round robin)\--wrr 调度方式:根据real server
权重值进行轮询调度
最少连接(least connections)\--lc 调度方式:选择连接数最少的服务器
加权最少连接(weighted least connections)\--wlc 调度方式:根据real
server权重值选 择连接数最少的服务器
# 三 ipvsadm工具介绍
  要使用LVS的能力，只需安装一个LVS的管理工具：ipvsadm。
LVS的结构主要分为两部分：
工作在内核空间的IPVS模块(IPVS:ip虚拟服务器)。LVS的IP负载均衡技术是由IPVS模块实现。
工作在用户空间的ipvsadm管理工具。其作用是向用户提供一个命令接口，用于将配置的虚拟服务、真实服务等传给IPVS模块。
# 1 案例1：ipvsadm命令用法
1.1 问题
准备一台Linux服务器，安装ipvsadm软件包，练习使用ipvsadm命令，实现如下功能：
使用命令添加基于TCP一些的集群服务
在集群中添加若干台后端真实服务器
实现同一客户端访问，调度器分配固定服务器
会使用ipvsadm实现规则的增、删、改
保存ipvsadm规则
1.2 方案
安装ipvsadm软件包，关于ipvsadm的用法可以参考man ipvsadm资料。
常用ipvsadm命令语法格式如表-1及表-2所示。
![table001](media/image10.png){width="6.038194444444445in"
height="2.295138888888889in"}
![table002](media/image11.png){width="5.888194444444444in"
height="3.104861111111111in"}
## 步骤一：使用命令增、删、改LVS集群规则
### 1）创建LVS虚拟集群服务器（算法为加权轮询：wrr）
\[root@proxy \~\]# yum -y install ipvsadm
\[root@proxy \~\]# ipvsadm -A -t 192.168.4.5:80 -s wrr
\[root@proxy \~\]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-\> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 192.168.4.5:80 wrr
### 2）为集群添加若干real server
\[root@proxy \~\]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.100 -m -w 1
\[root@proxy \~\]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-\> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 192.168.4.5:80 wrr
-\> 192.168.2.100:80 Masq 1 0 0
\[root@proxy \~\]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.200 -m -w 2
\[root@proxy \~\]# ipvsadm --a -t 192.168.4.5:80 -r 192.168.2.201 -m -w
3
\[root@proxy \~\]# ipvsadm --a -t 192.168.4.5:80 -r 192.168.2.202 -m -w
4
### 3）修改集群服务器设置(修改调度器算法，将加权轮询修改为轮询)
\[root@proxy \~\]# ipvsadm -E -t 192.168.4.5:80 -s rr
\[root@proxy \~\]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-\> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 192.168.4.5:80 rr
-\> 192.168.2.100:80 Masq 1 0 0
-\> 192.168.2.200:80 Masq 2 0 0
-\> 192.168.2.201:80 Masq 2 0 0
-\> 192.168.2.202:80 Masq 1 0 0
### 4）修改read server（使用-e选项，将模式改为DR模式）
\[root@proxy \~\]# ipvsadm -e -t 192.168.4.5:80 -r 192.168.2.202 -g
### 5）查看LVS状态
\[root@proxy \~\]# ipvsadm -Ln
### 创建另一个集群
（算法为最少连接算法；使用-m选项，设置工作模式为NAT模式）
\[root@proxy \~\]# ipvsadm -A -t 192.168.4.5:3306 -s lc
\[root@proxy \~\]# ipvsadm -a -t 192.168.4.5:3306 -r 192.168.2.100 -m
\[root@proxy \~\]# ipvsadm -a -t 192.168.4.5:3306 -r 192.168.2.200 -m
### 6）永久保存所有规则
\[root@proxy \~\]# ipvsadm-save -n \> /etc/sysconfig/ipvsadm
### 7）清空所有规则
\[root@proxy \~\]# ipvsadm -C
# 2 案例2：部署LVS-NAT集群
2.1 问题
使用LVS实现NAT模式的集群调度服务器，为用户提供Web服务：
集群对外公网IP地址为192.168.4.5
调度器内网IP地址为192.168.2.5
真实Web服务器地址分别为192.168.2.100、192.168.2.200
使用加权轮询调度算法，真实服务器权重分别为1和2
2.2 方案
实验拓扑结构主机配置细节如表-3所示。
![table003](media/image12.png){width="5.717361111111111in"
height="2.332638888888889in"}
使用4台虚拟机，1台作为Director调度器、2台作为Real
Server、1台客户端，拓扑结构如图-1所示，注意：web1和web2必须配置网关地址。
![image001](media/image13.png){width="5.6375in"
height="3.089583333333333in"}
## 步骤一：配置基础环境
### 1）设置Web服务器（以web1为例）
\[root@web1 \~\]# yum -y install httpd
\[root@web1 \~\]# echo \"192.168.2.100\" \> /var/www/html/index.html
### 2）启动Web服务器软件
\[root@web1 \~\]# systemctl restart httpd
### 3)关闭防火墙与SELinux
\[root@web1 \~\]# systmctl stop firewalld
\[root@web1 \~\]# setenforce 0
## 步骤二：部署LVS-NAT模式调度器
### 1)确认调度器的路由转发功能(如果已经开启，可以忽略)
\[root@proxy \~\]# echo 1 \> /proc/sys/net/ipv4/ip_forward #开启路由转发
\[root@proxy \~\]# cat /proc/sys/net/ipv4/ip_forward
1
\[root@proxy \~\]# echo \"net.ipv4.ip_forward = 1\" \>\>
/etc/sysctl.conf
#修改配置文件，开启路由转发,内核文件中添加设置永久规则
### 2）创建集群服务器
\[root@proxy \~\]# yum -y install ipvsadm
\[root@proxy \~\]# ipvsadm -A -t 192.168.4.5:80 -s wrr
#添加虚拟服务器,设置轮询机制为wrr加权轮询
### 2）添加真实服务器
\[root@proxy \~\]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.100 -w 1 -m
\[root@proxy \~\]# ipvsadm -a -t 192.168.4.5:80 -r 192.168.2.200 -w 1 -m
-a:添加真实服务器, -t:tcp协议 调度服务器地址 -r:指定真实服务器(Real
server) -w:权重 -m:NAT模式
### 3）查看规则列表，并保存规则
\[root@proxy \~\]# ipvsadm -Ln
\[root@proxy \~\]# ipvsadm -save -n \> /etc/sysconfig/ipvsadm
## 步骤三：客户端测试
客户端使用curl命令反复连接http://192.168.4.5，查看访问的页面是否会轮询到不同的后端真实服务器。
# 3 案例3：部署LVS-DR集群
3.1 问题