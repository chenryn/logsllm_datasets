m 
1.6 
1.5 
0 =- - 
d 
.- 5  1.4 
c 0 
2 - 2  1.3 
c 
More specifically, it is easy to see that the two schemes 
have  similar  IPC  values  before  any  spare  elements  are 
added.  This supports the past research  that was discussed 
in  section  two.  Average  IPC for REESE  is  only  11-16% 
worse than the baseline without any spare elements.  When 
spare elements  are added, this difference shrinks  from an 
average of  14.0% to an average of 8.0% over the hardware 
configurations shown in the previous figures. 
These graphs also show that  the  performance  of  some 
programs is erratic and unpredictable.  For example, ijpeg 
has a baseline IPC that is usually much higher than REESE. 
Vortex has a baseline IPC that is lower than REESE before 
spare elements are added.  These unpredictable results are 
due to complex interactions within  the  instruction  stream. 
That  is  why  the  average  is  so important.  A  wider  variety 
of  programs need to be tested before REESE can be imple- 
mented on a larger scale. 
Cycle time is another factor that should not be forgotten. 
Since cycle time  is dependent upon  implementation  tech- 
nology,  our  simulation  model  could  not  address  possible 
cycle time dilation. Cycle time should not be adversely af- 
fected, though, because the R-stream Queue acts in parallel 
to the processor pipeline.  REESE does add result compar- 
ison  and forwarding hardware, but  these should not overly 
burden the clock cycle. 
Figure 6 shows a summary of the previous results.  It is 
clear  that  the  added memory  ports  significantly  improved 
the  performance  of  REESE. It is  more feasible  to  imple- 
ment  REESE on a  system  that  has  four or more memory 
ports.  However, adding these ports  adds much more cost 
and complexity than adding integer ALUs. 
According  to these results,  REESE should  work  better 
as more hardware is added to a system. To test this, we in- 
crease the size of the RUU and ran more simulations.  Fig- 
I] 
ure 7 shows the result.  We  ran  cases where  the RUU in- 
creased  to 64 and even 256 entries.  The LSQ always re- 
mained at half the RUU size, as in the first simulations. We 
adjusted the RUU because  it seemed to be a bottleneck to 
quick execution. We also wanted to compare the results of 
adding functional units in addition to the large RUU. 
4- REESE+2 ALU 
/ 
1.2 
0' 
I 
RUU=64  + More FUs  RUU=256  + More FUs 
Figure 7. REESE vs.  baseline for  even more 
hardware (Order:  RUU=64, RUU=64 +  extra 
FUs, RUU=256, RUU=256 + extra FUs) 
The results show that the difference between the Base- 
line  system  and  REESE remains  at  approximately  15% 
when  only  the RUU is  increased  in  size.  However, addi- 
1.ional functional units shrink this difference to about 1.5%. 
'When  a  large  number  of  functional  units  are present,  we 
%would expect that adding spare functional units would not 
408 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:23 UTC from IEEE Xplore.  Restrictions apply. 
impact performance.  This expectation is proven correct by 
the figure. However, the figure also shows how the addition 
of only 2 integer ALUs can drastically improve the perfor- 
mance of the REESE model. 
7  Discussion and Conclusions 
REESE  is  an  efficient  implementation  of  time  redun- 
dancy in a microprocessor pipeline. The purpose of REESE 
is  to  allow  for  the  detection  of  soft  errors  in  the  proces- 
sor while only adding a minimal amount of  spare elements. 
When  a  small  overall  amount of  hardware is  present,  re- 
sults show  that  REESE has  a  14% performance decrease 
when  compared to the  baseline.  Simulations of computer 
systems with  more hardware bring this difference down to 
about  12%. 
This difference is significantly reduced by simply adding 
two  spare  integer  ALUs  to  the  REESE  hardware  config- 
uration.  This  brings  the  difference down  to  8.2%  for the 
simpler  systems that  we  simulated and  1.5% for the more 
complex systems. As our simulated system grows larger, it 
becomes clear that the practicality of REESE depends heav- 
ily  on both the size of the register update unit and the total 
number of functional units. The baseline processor is not as 
dependent upon a large number of functional units. 
LFrom  our simulations,  it  seems that  we  can  only  ap- 
proach our  goal  of  incorporating  time  redundancy  into  a 
GPP pipeline  while  attaining  zero  performance degrada- 
tion.  However,  the  fact that  we  approach  our  goal  as  we 
increase the amount of hardware is encouraging. This indi- 
cates that the time penalty due to time-redundant instruction 
execution should decrease for future generations of micro- 
processors.  Of  course, REESE will  need  different  imple- 
mentations for different processors.  But the practicality of 
REESE, and methods like it, will increase with time. 
To be completely  practical,  the hardware cost of imple- 
menting REESE must be compared to the model’s benefits. 
We can assume that the R-stream Queue will take up a larger 
amount of  die area than  other hardware additions required 
by  REESE. Depending on its size, the R-stream  Queue re- 
quires slightly more area than the RUU. If the RUU takes up 
10% of the die area, then we can expect REESE to add a to- 
tal of about 20% to the die area and (as stated before) 1.5% 
to the execution time. These are the costs for the benefit of 
full duplication and reexecution of the instruction stream. 
Future work  could  explore  the  possibility  of executing 
less  than  100% of  P-stream  instructions  in  the  R  stream. 
Compared  to  the  current  simulation,  this  method  could 
more easily meet processor performance goals.  For exam- 
ple, one out of every two instructions could be re-executed. 
This would  speed up execution, but  it  would  decrease the 
number of soft errors that REESE would be able to detect. 
Adding only two integer ALUs to the execution stage of 
the pipeline approaches our goal of zero performance degra- 
dation with a reasonable processor configuration. ALUs are 
relatively  inexpensive additions to  the  processor.  REESE 
adds soft error detection to GPPs without adding expensive 
hardware or drastically increasing program execution time. 
REESE is a scheme that must be seriously considered  as a 
defense against the reliability problems of  future micropro- 
cessor pipelines. 
References 
[ I ]   Borkar S.,  “Design  Challenges  of  Technology Scal- 
ing”, IEEE Micro, Vol.  19, No 4, pp. 23-29, 1999. 
[2]  Hazucha P., Svensson C., and Wender S.A., “Cosmic- 
Ray  Soft Error  Rate  Characterization  of  a  Standard 
0.6-mulm  CMOS  Process”,  IEEE  Journal  of  Solid- 
Stute Circuits, Vol. 35, No  10, pp.  1422-1429,2000. 
[ 3 ]  Tosaka Y., Kanata H., Satoh S., and Itakura T., “Sim- 
ple Method  for Estimating Neutron-Induced Soft Er- 
ror  Rates  Based  on  Modified  BGR  Model”,  IEEE 
Electron  Device  Lerrers,  Vol.  20,  NO 2,  pp.  89-91, 
1999. 
[4]  Chen  C.L.,  Hsiao  M.Y.,  “Error-correcting Codes for 
Semiconductor Memory Applications:  A  State of  the 
Art Review”, In Reliuble  Computer Systems - Design 
urd Evaluation, pp. 77 1-786, Digital  Press,  2nd edi- 
tion, 1992. 
[5] Hennessy J., “The Future of Systems Research”, IEEE 
Computer, Vol. 32, No 8, pp. 27-33, 1999. 
[6]  Johnson B.W.,  “Fault-tolerant Microprocessor-based 
Systems”, IEEE Micro, Vol. 4, No 6, pp. 6-2 1,  1984. 
[7]  Patel J.H., Fung L.Y., “Concurrent Error Detection in 
ALUs by Recomputing with Shifted Operands”, IEEE 
Trunsuctions on  Computers, Vol.  31, No 7, pp. 589- 
595, 1982. 
[8]  Rebaudengo  M.,  Sonza  Reorda  M.,  Torchiano  M., 
Violante  M.,  “Soft-error Detection  through  Software 
Fault-tolerance  Techniques”,  International  Sympo- 
sium on Defect and Fuiilt Tolerance in VLSI Systems, 
Albuquerque(NM), USA, IEEE, pp. 2 10-2 18, 1999. 
[9]  Anghel L., Nicolaidis  M.,  Alzaher-Noufal  I.,  “Self- 
Checking  Circuits  versus  Realistic  Faults  in  Very 
Deep Submicron”, Proceedings ojtke 18th IEEE VLSl 
Test Symposium, Montreal, Canada, pp. 55-63, 2000. 
[IO]  Kanopoulos N., Pantzartzia D., Bartram ER., “Design 
of Self-checking Circuits Using DCVS Logic: A Case 
Study”, IEEE Transactions on Computers, Vol. 41, No 
7, pp. 891-896, 1992. 
409 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:23 UTC from IEEE Xplore.  Restrictions apply. 
[l 11  Nicolaidis  M.,  “Time  Redundancy  Based  Soft-Error 
Tolerance to Rescue Nanometer Technologies”, Pro- 
ceedings of the 17th IEEE VLSI Test Symposium, Dana 
Port(CA), USA, IEEE, pp. 86-94, 1999. 
[12]  Anghel L.. Nicolaidis M., “Cost Reduction and Eval- 
uation  of  a Temporary  Faults  Detecting  Technique”, 
Proceedings of  the Design, Automation and Test in Eu- 
rope  Conference and Exhibition 2000, Paris, France, 
ACM, pp. 59 1-598,2000. 
[I31  Spainhower  L.,  Gregg  T.,  “G4:  A  Fault-Tolerant 
CMOS Mainframe”, Proceedings of the International 
Symposiutn  on  Fault-Tolerant  Computing,  Munich, 
Germany, IEEE, pp. 432-440, 1998. 
[ 141  Franklin M., “Incorporating Fault Tolerance in Super- 
scalar  Processors”,  Proceedings  of  the  3rd  Interna- 
tional  Conference on High  Performance  Computing, 
Trivandrum, India, IEEE, pp. 301-306, 1996. 
[15]  Sohi  G.S.,  Franklin  M.,  Saluja  K.K.,  “A  Study 
of  Time-Redundant  Fault-Tolerance  Techniques  for 
High-Performance  Pipelined  Computers”,  Proceed- 
ings of the International Syniposium on Fault-Tolerant 
Computing, Chicago(IL), USA,  IEEE,  pp.  436-449, 
1989. 
[ 161  Hsu Y.M.,  Swartzlander, Jr.  E.E.,  “Time  Redundant 
Error Correcting Adders and Multipliers”, IEEE Inter- 
national Workshop on Defect and  Fault  Tolerance in 
VLSI Systems, Dallas(TX), USA, IEEE, pp. 247-256, 
1992. 
[17]  Rashid  E, Saluja  K.K.,  Ramanathan  P.,  “Fault Tol- 
erance through Re-execution in Multiscalar Architec- 
ture”, Proceedings of the International Conference on 
Dependable Systems and  Networks, New  York(NY), 
USA, IEEE, pp. 482-49 1,2000. 
[ 181  Rotenberg  E.,  “AR-SMT:  A  Microarchitectural  Ap- 
proach to  Fault Tolerance in  Microprocessors”, Pro- 
ceedings  of  the  International  Symposium  on  Fault- 
Tolerant  Computing, Madison(WI), USA,  IEEE,  pp. 
84-91, 1999. 
[19]  The 
Berkeley 
http://now.cs.berkeley.edu,  December 2000. 
NOW 
project, 
URL: 
[20]  Reddy U., Tridandapani S., Somani A., “Effect of Di- 
agnosis Coverage and Preemption Latencies on Fault 
Diagnosis  Schemes  Using  Idle  Capacity  in  Multi- 
processor Systems”, Proc.  1995 Pacific Rim Interna- 
tional Syniposium on Fault Tolerant Systems, Newport 
Beach(CA), USA, IEEE, pp. 213-218,  1995. 
[21 I  Tridandapani, S., Somani, A. K., and Reddy, U., “Low 
Overhead  Multiprocessor  Allocation  Strategies  Ex- 
ploiting  System  Spare  Capacity  for  Fault  Detection 
and Location,” IEEE Transactions on Computers, Vol. 
44, No. 7, July  1995, 
[22]  Gong C., Melhem  R.,  Gupta R., “Compiler Assisted 
Fault  Detection  for  Distributed-Memory  Systems”, 
Proceedings of the International Syniposium on Fault- 
Tolerant  Coniputing,  Austin(TX),  USA,  IEEE,  pp. 
373-380, 1994. 
[23]  Tamir  Y.,  Liang  M.,  Lai  T.,  Tremblay  M.,  “The 
UCLA  Mirror Processor:  A Building Block  for Self- 
Checking  Self-Repairing  Computing  Nodes”,  Pro- 
ceedings  of  the  International  Symposiuni  on  Fault- 
Tolerant  Computing,  Montreal,  Canada,  IEEE,  pp. 
178-185, 1991. 
[24]  Franklin M., “A  Study of  Time Redundant Fault Tol- 
erance Techniques for Superscalar Processors”, IEEE 
International Workshop on Defect arid Fuult Tolerance 
in VLSI Systems, Lafayette(LA), USA, IEEE, pp. 207- 
215.  1995. 
1251  Burger D., Austin T.M., “The SimpleScalar Tool Set, 
Version 2.0’, Computer Sciences Departmetit Techni- 
cal Report, No.  1342, Univ. of Wisconsin, June  1997. 
[26]  McFarling  S.,  “Combining Branch  Predictors”,  TN 
36, DEC-WRL, June  1993. 
[27]  The  Standard  Performance  Evaluation  Corporation, 
URL: http://www.specbench.org,  December 2000. 
410 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:23 UTC from IEEE Xplore.  Restrictions apply.