a spinlock, all other activity at the spinlock’s IRQL or lower ceases on that
processor. Because thread dispatching happens at DPC/dispatch level, a
thread that holds a spinlock is never preempted because the IRQL masks the
dispatching mechanisms. This masking allows code executing in a critical
section protected by a spinlock to continue executing so that it will release
the lock quickly. The kernel uses spinlocks with great care, minimizing the
number of instructions it executes while it holds a spinlock. Any processor
that attempts to acquire the spinlock will essentially be busy, waiting
indefinitely, consuming power (a busy wait results in 100% CPU usage) and
performing no actual work.
On x86 and x64 processors, a special pause assembly instruction can be
inserted in busy wait loops, and on ARM processors, yield provides a similar
benefit. This instruction offers a hint to the processor that the loop
instructions it is processing are part of a spinlock (or a similar construct)
acquisition loop. The instruction provides three benefits:
■    It significantly reduces power usage by delaying the core ever so
slightly instead of continuously looping.
■    On SMT cores, it allows the CPU to realize that the “work” being
done by the spinning logical core is not terribly important and awards
more CPU time to the second logical core instead.
■    Because a busy wait loop results in a storm of read requests coming to
the bus from the waiting thread (which might be generated out of
order), the CPU attempts to correct for violations of memory order as
soon as it detects a write (that is, when the owning thread releases the
lock). Thus, as soon as the spinlock is released, the CPU reorders any
pending memory read operations to ensure proper ordering. This
reordering results in a large penalty in system performance and can be
avoided with the pause instruction.
If the kernel detects that it is running under a Hyper-V compatible
hypervisor, which supports the spinlock enlightenment (described in
Chapter 9), the spinlock facility can use the HvlNotifyLongSpinWait
library function when it detects that the spinlock is currently owned
by another CPU, instead of contiguously spinning and use the pause
instruction. The function emits a HvCallNotifyLongSpinWait
hypercall to indicate to the hypervisor scheduler that another VP
should take over instead of emulating the spin.
The kernel makes spinlocks available to other parts of the executive
through a set of kernel functions, including KeAcquireSpinLock and
KeReleaseSpinLock. Device drivers, for example, require spinlocks to
guarantee that device registers and other global data structures are accessed
by only one part of a device driver (and from only one processor) at a time.
Spinlocks are not for use by user programs—user programs should use the
objects described in the next section. Device drivers also need to protect
access to their own data structures from interrupts associated with
themselves. Because the spinlock APIs typically raise the IRQL only to
DPC/dispatch level, this isn’t enough to protect against interrupts. For this
reason, the kernel also exports the KeAcquireInterruptSpinLock and
KeReleaseInterruptSpinLock APIs that take as a parameter the
KINTERRUPT object discussed at the beginning of this chapter. The system
looks inside the interrupt object for the associated DIRQL with the interrupt
and raises the IRQL to the appropriate level to ensure correct access to
structures shared with the ISR.
Devices can also use the KeSynchronizeExecution API to synchronize an
entire function with an ISR instead of just a critical section. In all cases, the
code protected by an interrupt spinlock must execute extremely quickly—any
delay causes higher-than-normal interrupt latency and will have significant
negative performance effects.
Kernel spinlocks carry with them restrictions for code that uses them.
Because spinlocks always have an IRQL of DPC/dispatch level or higher, as
explained earlier, code holding a spinlock will crash the system if it attempts
to make the scheduler perform a dispatch operation or if it causes a page
fault.
Queued spinlocks
To increase the scalability of spinlocks, a special type of spinlock, called a
queued spinlock, is used in many circumstances instead of a standard
spinlock, especially when contention is expected, and fairness is required.
A queued spinlock works like this: When a processor wants to acquire a
queued spinlock that is currently held, it places its identifier in a queue
associated with the spinlock. When the processor that’s holding the spinlock
releases it, it hands the lock over to the next processor identified in the queue.
In the meantime, a processor waiting for a busy spinlock checks the status
not of the spinlock itself but of a per-processor flag that the processor ahead
of it in the queue sets to indicate that the waiting processor’s turn has arrived.
The fact that queued spinlocks result in spinning on per-processor flags
rather than global spinlocks has two effects. The first is that the
multiprocessor’s bus isn’t as heavily trafficked by interprocessor
synchronization, and the memory location of the bit is not in a single NUMA
node that then has to be snooped through the caches of each logical
processor. The second is that instead of a random processor in a waiting
group acquiring a spinlock, the queued spinlock enforces first-in, first-out
(FIFO) ordering to the lock. FIFO ordering means more consistent
performance (fairness) across processors accessing the same locks. While the
reduction in bus traffic and increase in fairness are great benefits, queued
spinlocks do require additional overhead, including extra interlocked
operations, which do add their own costs. Developers must carefully balance
the management overheard with the benefits to decide if a queued spinlock is
worth it for them.
Windows uses two different types of queued spinlocks. The first are
internal to the kernel only, while the second are available to external and
third-party drivers as well. First, Windows defines a number of global
queued spinlocks by storing pointers to them in an array contained in each
processor’s processor control region (PCR). For example, on x64 systems,
these are stored in the LockArray field of the KPCR data structure.
A global spinlock can be acquired by calling KeAcquireQueuedSpinLock
with the index into the array at which the pointer to the spinlock is stored.
The number of global spinlocks originally grew in each release of the
operating system, but over time, more efficient locking hierarchies were used
that do not require global per-processor locking. You can view the table of
index definitions for these locks in the WDK header file Wdm.h under the
KSPIN_LOCK_QUEUE_NUMBER enumeration, but note, however, that
acquiring one of these queued spinlocks from a device driver is an
unsupported and heavily frowned-upon operation. As we said, these locks are
reserved for the kernel’s internal use.
EXPERIMENT: Viewing global queued spinlocks
You can view the state of the global queued spinlocks (the ones
pointed to by the queued spinlock array in each processor’s PCR)
by using the !qlocks kernel debugger command. In the following
example, note that none of the locks are acquired on any of the
processors, which is a standard situation on a local system doing
live debugging.
Click here to view code image
lkd> !qlocks
Key: O = Owner, 1-n = Wait order, blank = not owned/waiting, 
C = Corrupt
                       Processor Number
    Lock Name         0  1  2  3  4  5  6  7
KE   - Unused Spare
MM   - Unused Spare
MM   - Unused Spare
MM   - Unused Spare
CC   - Vacb
CC   - Master
EX   - NonPagedPool
IO   - Cancel
CC   - Unused Spare
In-stack queued spinlocks
Device drivers can use dynamically allocated queued spinlocks with the
KeAcquireInStackQueued SpinLock and KeReleaseInStackQueuedSpinLock
functions. Several components—including the cache manager, executive pool
manager, and NTFS—take advantage of these types of locks instead of using
global queued spinlocks.
KeAcquireInStackQueuedSpinLock takes a pointer to a spinlock data
structure and a spinlock queue handle. The spinlock queue handle is actually
a data structure in which the kernel stores information about the lock’s status,
including the lock’s ownership and the queue of processors that might be
waiting for the lock to become available. For this reason, the handle
shouldn’t be a global variable. It is usually a stack variable, guaranteeing
locality to the caller thread and is responsible for the InStack part of the
spinlock and API name.
Reader/writer spin locks
While using queued spinlocks greatly improves latency in highly contended
situations, Windows supports another kind of spinlock that can offer even
greater benefits by potentially eliminating contention in many situations to
begin with. The multi-reader, single-writer spinlock, also called the executive
spinlock, is an enhancement on top of regular spinlocks, which is exposed
through the ExAcquireSpinLockExclusive, ExAcquireSpinLockShared API,
and their ExReleaseXxx counterparts. Additionally,
ExTryAcquireSpinLockSharedAtDpcLevel and
ExTryConvertSharedSpinLockToExclusive functions exist for more advanced
use cases.
As the name suggests, this type of lock allows noncontended shared
acquisition of a spinlock if no writer is present. When a writer is interested in
the lock, readers must eventually release the lock, and no further readers will
be allowed while the writer is active (nor additional writers). If a driver
developer often finds themself iterating over a linked list, for example, while
only rarely inserting or removing items, this type of lock can remove
contention in the majority of cases, removing the need for the complexity of
a queued spinlock.
Executive interlocked operations
The kernel supplies some simple synchronization functions constructed on
spinlocks for more advanced operations, such as adding and removing entries
from singly and doubly linked lists. Examples include
ExInterlockedPopEntryList and ExInterlockedPushEntryList for singly linked
lists, and ExInterlockedInsertHeadList and ExInterlockedRemoveHeadList for
doubly linked lists. A few other functions, such as ExInterlockedAddUlong
and ExInterlockedAddLargeInteger also exist. All these functions require a
standard spinlock as a parameter and are used throughout the kernel and
device drivers’ code.
Instead of relying on the standard APIs to acquire and release the spinlock
parameter, these functions place the code required inline and also use a
different ordering scheme. Whereas the Ke spinlock APIs first test and set the
bit to see whether the lock is released and then atomically perform a locked
test-and-set operation to make the acquisition, these routines disable
interrupts on the processor and immediately attempt an atomic test-and-set. If
the initial attempt fails, interrupts are enabled again, and the standard busy
waiting algorithm continues until the test-and-set operation returns 0—in
which case the whole function is restarted again. Because of these subtle
differences, a spinlock used for the executive interlocked functions must not
be used with the standard kernel APIs discussed previously. Naturally,
noninterlocked list operations must not be mixed with interlocked operations.
 Note
Certain executive interlocked operations silently ignore the spinlock when
possible. For example, the ExInterlockedIncrementLong or
ExInterlockedCompareExchange APIs use the same lock prefix used by
the standard interlocked functions and the intrinsic functions. These
functions were useful on older systems (or non-x86 systems) where the
lock operation was not suitable or available. For this reason, these calls
are now deprecated and are silently inlined in favor of the intrinsic
functions.
Low-IRQL synchronization
Executive software outside the kernel also needs to synchronize access to
global data structures in a multiprocessor environment. For example, the
memory manager has only one page frame database, which it accesses as a
global data structure, and device drivers need to ensure that they can gain
exclusive access to their devices. By calling kernel functions, the executive
can create a spinlock, acquire it, and release it.
Spinlocks only partially fill the executive’s needs for synchronization
mechanisms, however. Because waiting for a spinlock literally stalls a
processor, spinlocks can be used only under the following strictly limited
circumstances:
■    The protected resource must be accessed quickly and without
complicated interactions with other code.
■    The critical section code can’t be paged out of memory, can’t make
references to pageable data, can’t call external procedures (including
system services), and can’t generate interrupts or exceptions.
These restrictions are confining and can’t be met under all circumstances.
Furthermore, the executive needs to perform other types of synchronization
in addition to mutual exclusion, and it must also provide synchronization
mechanisms to user mode.
There are several additional synchronization mechanisms for use when
spinlocks are not suitable:
■    Kernel dispatcher objects (mutexes, semaphores, events, and timers)
■    Fast mutexes and guarded mutexes
■    Pushlocks
■    Executive resources
■    Run-once initialization (InitOnce)
Additionally, user-mode code, which also executes at low IRQL, must be
able to have its own locking primitives. Windows supports various user-
mode-specific primitives:
■    System calls that refer to kernel dispatcher objects (mutants,
semaphores, events, and timers)
■    Condition variables (CondVars)
■    Slim Reader-Writer Locks (SRW Locks)
■    Address-based waiting
■    Run-once initialization (InitOnce)
■    Critical sections
We look at the user-mode primitives and their underlying kernel-mode
support later; for now, we focus on kernel-mode objects. Table 8-26
compares and contrasts the capabilities of these mechanisms and their
interaction with kernel-mode APC delivery.
Table 8-26 Kernel synchronization mechanisms
Expose
d for 
Use by 
Device 
Drivers
Disables 
Normal 
Kernel-
Mode 
APCs
Disables 
Special 
Kernel-
Mode 
APCs
Suppor
ts 
Recurs
ive 
Acquis
Supports 
Shared 
and 
Exclusive 
Acquisitio
ition
n
Kernel 
dispatcher 
mutexes
Yes
Yes
No
Yes
No
Kernel 
dispatcher 
semaphore
s, events, 
timers
Yes
No
No
No
No
Fast 
mutexes
Yes
Yes
Yes
No
No
Guarded 
mutexes
Yes
Yes
Yes
No
No
Pushlocks
Yes
No
No
No
Yes
Executive 
resources
Yes
No
No
Yes
Yes
Rundown 
protections
Yes
No
No
Yes
No
Kernel dispatcher objects
The kernel furnishes additional synchronization mechanisms to the executive
in the form of kernel objects, known collectively as dispatcher objects. The
Windows API-visible synchronization objects acquire their synchronization
capabilities from these kernel dispatcher objects. Each Windows API-visible
object that supports synchronization encapsulates at least one kernel
dispatcher object. The executive’s synchronization semantics are visible to
Windows programmers through the WaitForSingleObject and
WaitForMultipleObjects functions, which the Windows subsystem
implements by calling analogous system services that the Object Manager
supplies. A thread in a Windows application can synchronize with a variety
of objects, including a Windows process, thread, event, semaphore, mutex,
waitable timer, I/O completion port, ALPC port, registry key, or file object.
In fact, almost all objects exposed by the kernel can be waited on. Some of
these are proper dispatcher objects, whereas others are larger objects that
have a dispatcher object within them (such as ports, keys, or files). Table 8-
27 (later in this chapter in the section “What signals an object?”) shows the
proper dispatcher objects, so any other object that the Windows API allows
waiting on probably internally contains one of those primitives.
Table 8-27 Definitions of the signaled state
Object 
Type
Set to Signaled 
State When
Effect on Waiting Threads
Process
Last thread 
terminates.
All are released.
Thread
Thread 
terminates.
All are released.
Event 
(notificatio
n type)
Thread sets the 
event.
All are released.
Event 
(synchroni
zation 
type)
Thread sets the 
event.
One thread is released and might 
receive a boost; the event object is 
reset.
Gate 
Thread signals 
First waiting thread is released and 
(locking 