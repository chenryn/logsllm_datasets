To clearly present the hybrid attack strategy in the batch
setting, we present a two-phase strategy: in the ﬁrst phase,
local model information is utilized to ﬁnd likely-to-transfer
seeds; in the second phase, target model information is used to
select candidates for optimization attacks. This split reduces
the generality of the attack, but simpliﬁes our presentation
and analysis. Since direct transfers have such low cost (that is,
one query when they succeed) compared to the optimization
attacks, constraining the attack to try all the transfer candi-
dates ﬁrst does not compromise eﬃciency. More advanced
attacks might attempt multiple transfer attempts per seed, in
which case the decision may be less clear when to switch to
an optimization attack. We do not consider such attacks here.
5.1 First Phase: Transfer Attack
Since the ﬁrst phase seeks to ﬁnd direct transfers, it needs to
execute without any information from the target model. The
goal is to order the seeds by likelihood of ﬁnding a direct
transfer before any query is done to the model. As before, we
do assume the attacker has access to pretrained local models,
so can use those models both to ﬁnd candidates for transfer
attacks and to prioritize the seeds.
Within the transfer attack phase, we use a prioritization
strategy based on the number of PGD-Steps of the local mod-
els to predict the transfer likelihood of each image. We ex-
plored using other metrics based on local model information
such as local model attack loss and local prediction score gap
(diﬀerence in the prediction conﬁdence of highest and second
highest class), but did not ﬁnd signiﬁcant diﬀerences in the
prioritization performance compared to PGD-Step. Hence,
we only present results using PGD-Steps here.
Prioritizing based on PGD Steps. We surmised that the eas-
ier it is to ﬁnd an adversarial example against the local models
for a seed, the more likely that seed has a large vulnerability
region in the target model. One way to measure this diﬃcult
is the number of PGD steps used to ﬁnd a successful local
adversarial example and we prioritize seeds that require less
number of PGD steps. To be more speciﬁc, we ﬁrst group
images by their number of successfully attacked local models
(e.g., k out of K local models), and then prioritize images in
each group based on their number of PGD steps used to ﬁnd
the adversarial examples that compromises the k local models.
We prioritize adversarial examples that succeed against more
of the local models (i.e., larger value of k) with the assump-
tion that adversarial examples succeed on more local models
USENIX Association
29th USENIX Security Symposium    1337
(a) Local Normal-3 Ensemble: NA, NB, NC
(b) Local Robust-2 Ensemble: R-DenseNet, R-ResNet
Figure 2: First phase (transfer only) attack prioritization (untargeted attack on robust CIFAR10 model, average over 5 runs).
Solid line denotes the mean value and shaded area denotes the 95% conﬁdence interval.
Local Ensemble
Metric
Normal-3
Robust-2
Local PGD Step
Random
Local PGD Step
Random
First AE
1.4± 0.5
11.4± 0.5
1.0± 0.0
4.0± 0.0
Top 1%
20.4± 2.1
100.8± 4.9
11.8± 0.4
26.0± 0.0
Top 2%
54.2± 5.6
199.6± 9.7
25.6± 0.9
50.4± 0.5
Top 5%
218.2± 28.1
496.6± 24.2
63.8± 0.8
124.2± 1.3
Table 7: Impact of prioritization for ﬁrst phase (robust CIFAR10 Model, average over 5 runs).
tend to have higher chance to transfer to the “unknown” tar-
get model. Above prioritization strategy is the combination
of the metrics of number of successfully compromised local
models and PGD steps. We also independently tested the im-
pact of each of the two metrics, and found that the PGD-step
based metrics perform better than the number of successfully
attacked models, and our current metric of combining the num-
ber of PGD steps and the number of successfully attacked
models is more stable compared to just using the PGD steps.
Results. Our prioritization strategy in the ﬁrst phase sorts
images and each seed is queried once to obtain direct transfers.
We compare with the baseline of random selection of seeds
where the attacker queries each seed once in random order to
show the advantage of prioritizing seeds based on PGD-Steps.
Figure 2 shows the results of untargeted attack on the
Madry robust CIFAR10 model for both normal and robust lo-
cal model ensembles. Note that ﬁrst phase attack only checks
transferability of the candidate adversarial examples and is
independent from the black-box optimization attacks. All re-
sults are averaged over ﬁve runs. In all cases, we observe
that, checking transferability with prioritized order in the ﬁrst
phase is signiﬁcantly better than checking the transferability
in random order. More quantitative information is given in
Table 7. For the untargeted attack on robust CIFAR10 model
with the three normal local models (NA, NB, NC), when at-
tacker is interested in obtaining 1% of the total 1,000 seeds,
checking transferability with prioritized order reduces the cost
substantially—with prioritization, it takes 20.4 queries on av-
erage, compared to 100.8 with random order. We observed
similar patterns for other datasets and models.
5.2 Second Phase: Optimization Attacks
The transfer attack used in the ﬁrst phase is query eﬃcient,
but has low success rate. Hence, when it does not ﬁnd enough
adversarial examples, the attack continues by attempting the
optimization attacks on the remaining images. In this section,
we show that the cost of the optimization attacks on these
images varies substantially, and then evaluate prioritization
strategies to identify low-cost seeds.
Query Cost Variance of Non-transfers. Figure 3 shows the
query distributions of non-transferable images for MNIST,
CIFAR10 and ImageNet using the NES attack starting from lo-
cal adversarial examples (similar patterns are observed for the
AutoZOOM attack). For ImageNet, when images are sorted
by query cost, the top 10% of 97 images (excluding 3 direct
transfers and 0 failed adversarial examples from the original
100 images) only takes on average 1,522 queries while the
mean query cost of all 100 images is 14,828. So, an attacker
interested in obtaining only 10% of the total 100 seeds us-
1338    29th USENIX Security Symposium
USENIX Association
(a) MNIST and CIFAR10
(b) ImageNet
Figure 3: Query cost of NES attack on MNIST, CIFAR10 and ImageNet models. We exclude direct transfers (successfully
attacked during ﬁrst phase) and seeds for which no adversarial example was found within the query limit (4000 for MNIST and
CIFAR; 10,000 for ImageNet). All the target models are normal models with NES targeted attacks.
ing this prioritization reduces their cost by 90% compared
to targeting seeds randomly. The impact is even higher for
CIFAR10 — the mean query cost for obtaining adversarial
examples for 10% of the seeds remaining after the transfer
phase is reduced by nearly 95% (from 933 to 51) over the
random ordering.
Prioritization Strategies. These results show the potential
cost savings from prioritizing seeds in batch attacks, but to
be able to exploit the variance we need a way to identify low-
cost seeds in advance. We consider two diﬀerent strategies
for estimating the attack cost to implement the estimator for
the EstimatedAttackCost function. The ﬁrst uses same local
information as adopted in the ﬁrst phase: low-cost seeds tend
to have lower PGD steps in the local attacks. The drawback of
prioritizing all seeds only based on local model information is
that local models may not produce useful estimates of the cost
of attacking the target model. Hence, our second prioritization
strategy uses information obtained from the single query to
the target model that is made for each seed in the ﬁrst phase.
This query results in obtaining a target model prediction score
for each seed, which we use to prioritize the remaining seeds
in the second phase. Speciﬁcally, we ﬁnd that low-cost seeds
tend to have lower loss function values, deﬁned with respect
to the target model. The assumption that an input with a lower
loss function value is closer to the attacker’s goal is the same
assumption that forms the basis of the optimization attacks.
Taking a targeted attack as an example, we compute the
loss similarly to the loss function used in AutoZOOM [43].
For a given input x and target class t, the loss is calculated as
l(x,t) = (maxi(cid:44)t log f (x)i − log f (x)t)+
where f (x) denotes the prediction score distribution of a seed.
So, f (x)i is the model’s prediction of the probability that x
is in class i. Similarly, for an untargeted attack with orig-
inal label y, the loss is deﬁned as l(x,y) = max(log f (x)y −
maxi(cid:44)y log f (x)i)+. Here, the input x is the candidate starting
point for an optimization attack. Thus, for hybrid attacks that
start from a local candidate adversarial example, z(cid:48), of the
original seed z, attack loss is computed with respect to z(cid:48) in-
stead of z. For the baseline attack that starts from the original
seed z, the loss is computed with respect to z.
Results. We evaluate the prioritization for the second phase
using the same experimental setup as in Section 5.1. We
compare the two prioritization strategies (based on local PGD
steps and the target model loss) to random ordering of seeds
to evaluate their eﬀectiveness in identifying low-cost seeds.
The baseline attacks (AutoZOOM and NES, starting from the
original seeds) do not have a ﬁrst phase transfer stage, so we
defer the comparison results to next subsection, which shows
performance of the combined two-phase attack.
Figure 4 shows the results for untargeted AutoZOOM at-
tacks on the robust CIFAR10 model for local ensembles of
both normal and robust models (results for the NES attack
are not shown, but exhibit similar patterns). Using the target
loss information estimates the attack cost better than the lo-
cal PGD step ordering, while both prioritization strategies
achieve much better performance than the random ordering.
Table 8 summarizes the results. For example, for the untar-
geted AutoZOOM attack on robust CIFAR10 model with the
Normal-3 local ensemble, an attacker who wants to obtain
ten new adversarial examples (in addition to the 101 direct
transfers found in the ﬁrst phase) can ﬁnd them using on aver-
age 1,248 queries using target model loss in the second phase,
compared to 3,465 queries when using only local ensemble
information, and 26,336 without any prioritization.
USENIX Association
29th USENIX Security Symposium    1339
(a) Normal-3 Local Ensemble
(b) Robust-2 Local Ensemble
Figure 4: Impact of seed prioritization strategies in the second phase (AutoZOOM untargeted attack on robust CIFAR10 model,
average over 5 runs). The x-axis denotes the query budget and the y-axis denotes the number of successful adversarial examples
found with the given query budget. The maximum query budget is the sum of the query cost for attacking all seed images (i.e.,
the total number of queries used to attack all 1000 seeds for CIFAR10 models) — 1,656,818 for the attack with normal local
models, and 1,444,980 for the attack with robust local models. The second phase starts at 1000 queries and the number of direct
transfers found because it begins after checking the direct transfers in the ﬁrst phase.
Local Ensemble
Metric
Target Loss
Normal-3
Local PGD Step
Random
Target Loss
Robust-2
Local PGD Step
Random
1,560± 147
4,982± 274
Additional 1% Additional 2% Additional 5% Additional 10%
1,248± 93
6,229± 336
3,465± 266
51,962± 5,117
26,336± 3,486 58,247± 3,240 150,060± 3,415 301,635± 6,651
2,086± 37
29,435± 2,418
6,009± 834
75,002± 5,663
49,410± 1,596 99,900± 3,261 258,278± 2,136 512,398± 6,606
3,900± 604
10,875± 1,391
2,739± 118
29,203± 4,450
9,882± 2,051
28,625± 3,148
Table 8: Impact of diﬀerent prioritization strategies for optimization attacks (AutoZOOM untargeted attack on robust CIFAR10
model, average over 5 runs). For diﬀerent models, their number of direct transfers varies—for Normal-3 there are in average
101.2, for Robust-2 there are in average 407.4. We report the number of queries needed to ﬁnd an additional x% (10, 20, 50, and
100 out of 1000 total seeds), using the remaining seeds after the ﬁrst phase.
5.3 Overall Attack Comparison
To further validate eﬀectiveness of the seed prioritized two-
phase strategy, we evaluate the full attack combining both
phases. Based on our analysis in the previous subsections, we
use the best prioritization strategies for each phase: PGD-Step
in the ﬁrst phase and target loss value in the second phase. For
the baseline attack, we simply adopt the target loss value to
prioritize seeds. We evaluate the eﬀectiveness in comparison
with two degenerate strategies:
• retroactive optimal — this strategy is not realizable, but
provides an upper bound for the seed prioritization. It as-
sumes the attackers have prior knowledge of the true rank
of each seed. That is, we assume a selectSeed function
that always returns the best remaining seed.
• random — the attacker selects candidate seeds in a ran-
dom order and conducts optimization attacks exhaustively
(until either success or the query limit is reached) on each
seed before trying the next one. This represents traditional
black-box attacks that just attack every seed.
Here, we only present results of AutoZOOM attack on
robust CIFAR10 model with normal local models and Auto-
ZOOM attack on ImageNet models. The attack on robust
CIFAR10 is the setting where the performance gain for the
hybrid attack is least signiﬁcant compared to other models
(see Table 3), so this represents the most challenging scenario
for our attack. In the ImageNet setting, the performance of
the target loss based prioritization is not a signiﬁcant improve-
ment over random scheduling, so this represents the worst
case for target loss prioritization for the baseline attack. Re-
1340    29th USENIX Security Symposium
USENIX Association
(a) Target: Robust CIFAR10 Model
(b) Target: Standard ImageNet Model
Figure 5: Comparison of the target loss value based seed prioritization strategy to retroactive optimal and random search strategies
(AutoZOOM baseline untargeted attack on robust CIFAR10 model and targeted attack on standard ImageNet model, averaged
over 5 runs). Solid line denotes mean value and shaded area denotes the 95% conﬁdence interval. Maximum query budget is
1,699,998 for robust CIFAR10 model, 4,393,314 for ImageNet.
Target Model
Robust
CIFAR10
(1,000 Seeds)
Standard
ImageNet
(100 Seeds)
Prioritization Method
Retroactive Optimal
Target Loss
Random
Retroactive Optimal
Target Loss
Random
Top 1%
34.0± 2.0
1,070± 13
25,005± 108
7,492± 1,078
32,490± 5,857
22,178± 705
Top 2%
119.2± 4.8
1,170± 16
51,325± 221
16,590± 1,755
58,665± 8,268
66,532± 2,114
Top 5%
580.8± 35.0
1,765± 12
130,284± 561
49,255± 4,945
89,541± 8,459
199,595± 6,341
Top 10%
2,002± 69
3,502± 85
261,883± 1,128
114,832± 7,430
257,594± 13,738