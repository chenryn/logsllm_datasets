title:Synthesizing Test Data for Fraud Detection Systems
author:Emilie Lundin Barse and
Håkan Kvarnstr&quot;om and
Erland Jonsson
Synthesizing Test Data for Fraud Detection Systems
Emilie Lundin Barse
H˚akan Kvarnstr¨om
Erland Jonsson
Department of Computer Engineering
Chalmers University of Technology
412 96 G¨oteborg, Sweden
(cid:0)emilie,hkv,erland.jonsson(cid:1)@ce.chalmers.se
Abstract
This paper reports an experiment aimed at generating
synthetic test data for fraud detection in an IP based video-
on-demand service. The data generation veriﬁes a method-
ology previously developed by the present authors [7] that
ensures that important statistical properties of the authen-
tic data are preserved by using authentic normal data and
fraud as a seed for generating synthetic data. This enables
us to create realistic behavior proﬁles for users and attack-
ers. The data can also be used to train the fraud detection
system itself, thus creating the necessary adaptation of the
system to a speciﬁc environment. Here we aim to verify the
usability and applicability of the synthetic data, by using
them to train a fraud detection system. The system is then
exposed to a set of authentic data to measure parameters
such as detection capability and false alarm rate as well as
to a corresponding set of synthetic data, and the results are
compared.
1. Introduction
Fraud detection is becoming increasingly important in
revealing and limiting revenue loss due to fraud. Fraudsters
aim to use services without paying or illicitly beneﬁt from
the service in other ways, causing service providers ﬁnan-
cial damage. To reduce losses due to fraud, one can deploy
a fraud detection system. However, without tuning and thor-
ough testing, the detection system may cost more in terms
of human investigation of all the false alarms than the gain
from reduction of fraud. Test data suitable for evaluating
detection schemes, mechanisms and systems are essential to
meet these requirements. The data must be representative of
normal and attack behavior in the target system since detec-
tion systems can, and should, be very sensitive to variations
in input data.
H˚akan Kvarnstr¨om is also with TeliaSonera AB, SE-123 86 Farsta,
Sweden
Using synthetic data for evaluation, training and testing
offers several advantages over using authentic data. Proper-
ties of synthetic data can be tailored to meet various condi-
tions not available in authentic data sets. There are at least
three application areas for synthetic data. The ﬁrst is to train
and adapt a fraud detection system (FDS) to a speciﬁc en-
vironment. Some FDSs require large amounts of data for
training, including large amounts of fraud examples, which
are normally not available in the authentic data from the
service. The second application area is to test the proper-
ties of a FDS by injecting variations of known frauds or
new frauds into synthetic data to study how this affects per-
formance parameters, such as the detection rate. The false
alarm rate may also be tested by varying background data,
where background data is deﬁned as normal usage with no
attacks. The third application area is to compare FDSs in a
benchmarking situation.
The aim of this work is to test the feasibility of generat-
ing and using synthetic data for training and testing a fraud
detection system. Our synthetic data generation is based on
the method proposed in [7], where we use small amounts of
authentic log data to generate a large amount of synthetic
data. The method identiﬁes important statistical properties
and aims to preserve parameters important for training and
detection, such as user and service behavior. We apply the
data generation method on an IP based video-on-demand
(VoD) service running in a pilot test environment with real
customers. The authentic data collected from the service are
used to generate synthetic log ﬁles containing both normal
and fraudulent user behavior. The properties of the syn-
thetic log ﬁles are veriﬁed by visualizing them and using
them to train and test a fraud detection prototype.
Synthetic data are not commonly used in the fraud detec-
tion area, although, the use of manipulated authentic data is
discussed in some papers, e.g. [2] and [1]. In the intrusion
detection area, more work has been done using synthetic
test data. ([5] discusses similarities between fraud and in-
trusion detection systems.) Huge amounts of synthetic test
data were generated in the 1998 and 1999 DARPA intru-
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:39:18 UTC from IEEE Xplore.  Restrictions apply. 
sion detection evaluations [4]. Debar et al. [3] developed
a generic intrusion detection testbed, and suggest the use
of a ﬁnite state automata to simulate user behavior. While
this methodology is rather close to our approach, they did
not use this method in their testbed. They declared it prac-
tical only if the set of user commands is limited. Instead,
they used recorded live data from user sessions. Puketza et
al. [10] describe a software platform for testing intrusion
detection systems where they simulate user sessions using
the UNIX package expect. However, none of these methods
give sufﬁcient control of the data properties of the synthetic
data. We believe that our method can provide better data
properties that are needed for training and testing in many
situations. Furthermore, our method provides scalability of
log data in both amount of users and time period.
Some interesting work has been done on measuring char-
acteristics in data and how they affect the detection systems,
e.g. Lee and Xiang [6], and Tan and Maxion [11]. In [8],
Maxion and Tan generate “random” synthetic data with dif-
ferent degrees of regularity and show that it affects the false
alarm rate drastically. The methods proposed in these pa-
pers may be useful for synthetic data validation, but have
not been used in this paper.
Below, Section 2 and 3 give a a summary of the method
proposed in [7]. The rest of the paper describes how we
apply the method on the VoD service and the veriﬁcation of
the generated synthetic data.
2. Motivation for using synthetic data
This section gives the beneﬁts of using synthetic data for
various types of testing and explains why authentic data are
not a solution in some cases.
2.1. Why not use authentic data?
Authentic data cannot be used in some cases for a num-
ber of reasons. The target service may still be under devel-
opment and thus produce irregular or only small amounts
of authentic data. We also have no control over what fraud
cases the data contain. Furthermore, it may be impossible
or at least very difﬁcult to acquire the amount of or type of
data needed for tests. This in turn may be due to the fact
that only a limited number of users are available or that we
do not know whether the data set contains any frauds.
2.2. Beneﬁts of synthetic data
Synthetic data can be deﬁned as data that are generated
by simulated users in a simulated system, performing sim-
ulated actions. The simulation may involve human actions
to some extent or be an entirely automated process.
Synthetic data can be designed to demonstrate certain
key properties or to include attacks not available in the au-
thentic data, giving a high degree of freedom during test-
ing and training. Synthetic data can cover extensive peri-
ods of time or represent large numbers of users, a necessary
property to train some of the more ”intelligent” detection
schemes.
In [7], we provide an elaborate discussion of data prop-
erties that are important for training and testing fraud detec-
tion systems. These are summarized in the following bul-
lets:
(cid:0) Data need to be labeled, i.e. we need to have exact
knowledge of the attacks included in the data.
(cid:0) The attacks found in the input data are representative
of the attacks we expect to ﬁnd in the target system.
(Not necessarily the same attacks that currently occur
in the system.)
(cid:0) The number and distribution of attacks in the back-
ground data (fraud/normal data ratio) must be adapted
to the detection mechanism. Some detection methods
perform better if they are trained with data in which
attacks are overrepresented.
(cid:0) The amount of data must be of a sufﬁcient size.
In
particular, certain AI algorithms need huge amounts of
training data to perform well.
(cid:0) For testing, the number and distribution of attacks in
the background data (fraud/normal data ratio) should
be realistic.
(cid:0) For testing, it is important that attacks in the input data
are realistically integrated in the background data. For
example, the time stamp of an attack, time between
attacks and the time between parts of an attack may
affect detection results.
(cid:0) Normal (background) data should have similar statisti-
cal properties as authentic data from the target system.
Different behavior in the system may drastically affect
detection performance.
3. Data generation methodology
Synthetic data were generated by the methodology de-
scribed in [7]. For completeness, we brieﬂy introduce the
method and refer to the original work for details.
The main components necessary for automating the data
generation process are speciﬁcations of desired user behav-
ior in the system, a user/attacker simulator and a system
simulator. The goal of the methodology is to guide the pro-
duction of these components. The starting point is the col-
lection of information about the anticipated user behavior
in the target system. The methodology includes generating
both background and attack data and thus it is necessary to
have information about possible attacks and normal usage.
These data serve as the basis for user and system modeling.
2
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:39:18 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1 illustrates the methodology. The ﬁrst step is the
collection of data that should be representative of the antici-
pated behavior of the target system. Data may consist of au-
thentic background data from the target system, background
data from similar systems, authentic attacks and other col-
lections of possible attacks. The second step is to analyze
the collected data and identify important properties, such
as user classes, statistics of usage, attack characteristics and
statistics of system behavior. In step 3, the information from
the previous step is used to identify parameters that must be
preserved to be able to detect the anticipated attacks and to
create user and attacker proﬁles that conform to the param-
eter statistics. A user model is created in step 4. This must
be sophisticated enough to preserve the selected proﬁle pa-
rameters. Attackers are also modeled in this step. The user
and attacker simulators implement the models. The sys-
tem is modeled in step 5, and this model must be accurate
enough to produce equivalent log data as the target system
for the same type of input user actions. The system simula-
tor is then implemented according to this model.
Data collection
1.
Authentic seed data:
Authentic background data
Background data from similar systems
Authentic attacks
Possible attacks
Data analysis
2.
User statistics
User classes
Attack statistics
System statistics
Data generation
Profile generation
User profiles
3.
4.
5.
User and attack
modelling
User simulator
Attacker simulator
System modelling
Target
system simulator
Figure 1. Synthetic data generation method
It is possible to use people instead of a user simulator to
create user actions and to use the whole or parts of the real
system instead of a system simulator. This may be prefer-
able in some situations, e.g. if the system or user behavior
is very complex and needs to be modeled in great detail. In
our experiments, we used humans to mimic fraudulent be-
3
havior and automata to generate normal background data.
4. Authentic data
This section describes how we implemented the collec-
tion of authentic data for the VoD service. The next section
(Section 5) presents the work of designing, implementing
and using the data analysis, user and system model.
4.1. The target Video-on-Demand system
The VoD service was in pilot operation and had a limited
number of users. It could thus provide only small amounts
of log data. This system consisted of a number of com-
ponents, shown in Figure 2. Each user had a set top box
(stb) at home, which was connected to the Internet via a fast
xDSL connection. When the set top box was turned on, it
automatically contacted the service providers DHCP server
(Dynamic Host Conﬁguration Protocol server) to get a dy-
namic IP address. Then, the user could contact the applica-
tion server, login to it, browse the video database and order
a movie. The application server generated an authentication
ticket for the user. The VoD server then started delivering
the chosen movie after veriﬁcation of the ticket.
DHCP client
set top box
DHCP
server
IP switch
router
WWW
server
Application
server
Database
VoD
server
Figure 2. VoD system components
In the VoD-system, a total of 12 ”test users” were active
during the service development. Thus, the amount of data
generated was not sufﬁcient for training the fraud detection
modules. We used these twelve users as an approximation
of ”normal” users. They had no knowledge about the im-
plementation of the service or of the fact that their behavior
would be used for synthetic data generation.
4.2. Collection of the authentic seed data
We had the opportunity to work together with the devel-
opment team for the VoD service and could therefore spec-
ify the information collected in the log ﬁles. To be able to
decide what information to collect, we created a database
with expected frauds and the indicators or features needed
to detect them. Each indicator was analyzed to ﬁnd out what
type of log data was needed to catch the indicator.
The test users of the VoD system were considered
”friendly users” and had been selected as test pilots based
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:39:18 UTC from IEEE Xplore.  Restrictions apply. 
on their physical location. All users were known to the de-
velopment team and thus no frauds were expected to occur.
These users generated the background data we collected.
We used employees that acted according to descriptions
of the expected fraud cases we had created. These fraud
cases were “injected” into the system by using two dedi-
cated set top boxes in the same way as we expect a fraudster
to use it.
Data were collected over a period of about three months
and the total amount of data was 65 MBytes.
4.3. Fraud data
Four fraud cases were “injected” into the authentic data:
Break-in fraud: The fraudster has “taken over” the iden-
tity of a legal user by hacking the user’s set top box. The
real user may use the service without knowing that he had
a break-in. An indication of a break-in could be a sudden
excessive usage of the service.
Billing fraud: To avoid paying for the movies ordered, the
fraudster has hacked the billing server. In our experiments
this was done by removing billing records in the application
server log ﬁle.
Illegal redistribution fraud: This means that a customer
receiving a movie in the VoD service transmits it to other
people not paying for the service. Our case of illegal distri-
bution was performed by uploading large ﬁles to a computer
on the Internet some time after the completion of a movie
download. Illegal redistribution of content could also be an
indication of an external break-in in the set top box where
the attacker in turn uses (and downloads) videos at the ex-
pense of a legitimate paying customer.
Failed logins: Several failed login attempts were added
with “dummy” user ID’s to imitate the behavior of people
trying to guess passwords.
4.3.1. Example of a fraud case. This subsection gives an
example of the process we used for each fraud case to de-
cide what the fraud indicators were and thus what informa-
tion we needed in the log ﬁles.
Fraud indicators for the fraud case illegal (re)distribution
of services may be:
(cid:0) The ratio between transmitted and received data is sus-
piciously high.
(cid:0) A great deal of data is transmitted (some period of
time) after data has been received.
(cid:0) A great number of downloads are done.
For the ﬁrst indicator we need user ID, IP address, bytes
transmitted and bytes received in the log data. The second