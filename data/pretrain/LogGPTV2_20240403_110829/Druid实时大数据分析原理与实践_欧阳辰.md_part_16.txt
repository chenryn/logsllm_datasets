"day",
"hyperUnique"
115
---
## Page 140
的摄入手段也一样会不断地发展和完善，因此我们可以对此保持持续关注。
平台，数据摄入几乎都是首先要考虑的工作，也是一个可以被持续优化的重要环节，而Druid
了各种数据摄入方法的适用场景，同时清楚了提高摄人性能的有效方式。无论对于何种数据
5.6
116
通过本章的介绍，相信读者已经对Druid的数据摄入有了比较全面的认识，也已经了解
小结
Druid实时大数据分析原理与实践
---
## Page 141
节点，最后将来自实时节点和历史节点的查询结果合并后返回给调用方。其中，查询节点通
然后找出包含这些Segment的实时节点和历史节点，再将请求分发给相应的实时节点和历史
6.1
SourceMetadata等。
Select,Search等方式的查询，也可以查询一个数据源的timeBoundary segmentMetadata,data-
'Content-Type:application/json'-d @
curl-XPOST':/druid/v2/?pretty'-H\
一个典型的curl命令如下：
点（HistoricalNode）或实时节点（RealtimeNode）处理。我们可以使用curl命令进行测试
对数据的查询通过HTTP请求发送到查询节点（BrokerNode），然后查询节点转发至历史节
介绍Druid的数据查询过程以及查询语法。Druid提供了HTTPREST风格的查询接口。用户
查询节点接收外部Client的查询请求，并根据查询中指定的interval找出相关的Segment，
Druid包含多种查询类型，如对用户摄人Druid的数据进行TopN，Timeseries,GroupBy
·query_json_file为POST到查询节点的查询请求。
·queryabl_host:port 为查询节点的IP地址和端口，往往系统中会配置多个查询节点，
前面的章节分别介绍了数据摄入、DataSchema定义以及从Hadoop中摄入数据，本章将
查询过程
每个查询节点提供的服务都是相同的，可以任选一个进行学习测试。
数据查询
第
O章
---
## Page 142
足 Filter的行是我们需要的数据，类似于SQL中的where子句。Filter包含如下类型。
6.2.1
详细类型。
6.2
分发到这些机器上。
数据如何流动，以及哪些节点涉人其中。
过Zookeeper来发现历史节点和实时节点的存活状态。
Aggregator、
118
Filter，即过滤器，在查询语句中是一个JSON对象，用来对维度进行筛选，表示维度满
在介绍具体的查询之前，先介绍各种查询都会用到的基本组件（Component），如Filter、
（4）查询节点将历史节点和实时节点返回的结果合并，返回给查询请求方。
（3）历史节点和实时节点都会进行查询处理，然后返回结果。
（2）查询节点选择一组可以提供所需要的Segment的历史节点和实时节点，将查询请求
（1）查询请求首先进入查询节点，查询节点将与已知存在的Segment进行匹配查询。
组件
查询过程如下：
Filter
Post-Aggregator、
UERIE
DATA
、Query、Interval和Context 等。每一种基本组件都包含更多的
图6-1
Druid查询流程图
ROKER
。图6-1展示了在系统架构中查询请求
Druid实时大数据分析原理与实践
BATCHDATA
---
## Page 143
5.InFilter
4.
丰富的逻辑表达式，并与 SQL中的and、or和not相似。JSON表达式示例如下：
3.Logical Expression Filter
"filter":{"type":"regex",“dimension":,“pattern":}
Druid 都支持。Regex Filter 的 JSON示例如下：
2.Regex Filter
"filter":{"type":"selector","dimension":,
1.Selector Filter
第6章数据查询
"filter":["type":"not",
"filter":{"type":"or",
SearchFilter
Logical Expression Filter包含and、or和not三种过滤器。每一种都支持嵌套，可以构建
RegexFilter允许用户用正则表达式来筛选维度，
"value":}
Selector Filter的功能类似于SQL中的wherekey=value。SelectorFilter的JSON示例如下：
In Filter类似于 SQL中的 in:WHERE outlaw IN(Good,'Bad,Ugly）。JSON示例如下：
其中，query中不同的type代表不同的匹配方式。
"filter":{
Search Fiter通过字符串匹配过滤维度，支持多种匹配方式。JSON示例如下：
"type":"search"
"query":{
"dimension":
"value":
"type":
："foo"
"insensitive_contains"
:"product"
"field":}
"fields"：[，,.]}
，任何标准的Java支持的正则表达式
11
119
---
## Page 144
的值为true。具体的JSON表达式示例如下。
“>=”或“”，则需要指定lowerStrict的值为true或upperStrict
需要在查询中设定alphaNumeric的值为true。需要注意的是，BoundFilter默认的大小比较为
Filter支持字符串比较，而且默认就是字符串比较，并基于字典序。如果要使用数字比较，则
6.
120
Bound Filter
"dimension":“"age",
"type":“bound",
age='foo′&&x,
如果上述Filter不能满足要求，Druid还可以通过自己写JavaScript Filter来过滤维度，但
"dimension":"name",
"type":"javascript"
"type"："javascript"
"upper": "hoo"
"lower":"foo",
"dimension":"name"
"type":"bound",
foo}"
121
---
## Page 146
ITY的最小值。
{“type":"doubleSum",“name":, "fieldName":}
NITY的最大值。
3
2.
{"type":“longSum","name”:,"fieldName":"count"}
{"type":"count"，“name"：}
果查询Rollup后有多少条数据，查询语句JSON示例如下：
1.Count Aggregator
可以在查询时指定。聚合器包含如下详细类型。
["type"：“LongSum",，"name”：,
122
"type"
"type"："longMin",
"type"
"type"
Min/MaxAggregator
Sum Aggregator
第四类是 longMax Aggregator它负责计算指定Metric 的值和 Long.MIN_VALUE的最大
第三类是longMin Aggregator，它负责计算指定 Metric的值和 Long.MAX_VALUE的最小
第一类是doubleMinAggregator,它负责计算指定Metric的值和Double.POSITIVE_INFIN-
第二类是doubleSum Aggregator，它负责64位浮点数的求和。JSON示例如下：
第一类是longSumAggregator，它负责64位有符号整型的求和。JSON示例如下：
如果要查询摄入了多少条原始数据，在查询时使用longSum，JSON示例如下：
Count Aggregator计算 Druid 的数据行数，而Count 就是反映被聚合的数据的计数。如
上述几类聚合器的JSON都比较相似，
："doubleMin",
"LongMax",
"name”
"name"
"name"
"name"
,
：,
：,
：,
基本如下：
"fieldName"
"fieldName":
"fieldName":
"fieldName"
"fieldName"
：
Druid实时大数据分析原理与实践
---
## Page 147
特性”中会有详细介绍。
Metric，从而在查询时使用。JSON示例如下：
5.HyperUnique Aggregator
SELECT COUNT(*）FROM（SELECT DIM1,DIM2,DIM3 FROM GROUP BY DIM1,DIM2，
SELECT COUNT(DISTINCT(value))FROM（
较推荐使用HyperUniqueAggregator。
Aggregator在摄入阶段就会为Metric做聚合，因此在通常情况下，对于单个维度求基数，比
4.Cardinality Aggregator
第6章数据查询
除了HyperLogLog，
HyperUniqueAggregator使用HyperLogLog算法计算指定维度的基数。在摄入阶段指定
SELECT dim_2 as value FROM
SELECT dim_1asvalueFROM
DIM3）
byRow为true时，类似于以下 SQL：
SELECT dim_3asvalueFROM
byRow为false时，类似于以下SQL：
JSON示例如下：
在查询时，Cardinality Aggregator使用HyperLogLog算法计算给定维度集合的基数。需
UNION
UNION
"byRow":#（optional,defaults to false)
"fieldNames":[,,
"name":""
"type":"cardinality",
近似的算法Druid还使用到了ThetaSketch，在第7章“高级功能和
"fieldName":
123
---
## Page 148
Aggregator。JavaScript Aggregator 的 JSON示例如下：
性能要比本地 Java Aggregator 慢很多。因此，如果要追求性能，就需要自己实现本地 Java
7.
以提升聚合效率。JSON示例如下：
9
124
如果上述聚合器无法满足需求，Druid还提供了JavaScript Aggregator。用户可以自己写
JavaScript Aggregator
Filtered Aggregator 可以在 aggregation 中指定 Filter 规则。只对满足规则的维度进行聚合，
Filtered Aggregator
"type":"javascript",
例子如下：
"fnReset"
"fieldNames"
"name":""
"type":"javascript",
"fnCombine"
"aggregator":
"filter"
"type"
"value":
"dimension":,
"type"
"filtered"
："selector",
：[，，
"function(）
"function(partialA, partialB){return ; }",
return 
[return; }"
Druid实时大数据分析原理与实践
---
## Page 149
Aggregator的JSON示例如下：
果对 HyperUnique的结果进行访问，则需要使用 hyperUniqueCardinality。Field Accesor Post-
2.Field Accessor Post-Aggregator
6.2.3Post-Aggregator
"postAggregation":{
第6章数据查询
ArithmeticPost-Aggregator
"fields": [, , ...],
JSON示例如下：
Post-Aggregator可以对Aggregator 的结果进行二次加工并输出。最终的输出既包含 Ag-
"ordering":
'fn"
'type"
"fnReset"
name"
·当Arithmetic Post-Aggregator的结果参与排序时，默认使用float 类型。用户可以手动
·对于“”，
"fnCombine"
"fnAggregate"
"fieldNames": ["x", "y"],
"name":“sum（log（x)*y)+ 10",
。“quotient”不判断分母是否为0。
通过ordering字段指定排序方式。
:"arithmetic",
：,
,
，如果分母为0，则返回0。
"function()
"function(partialA, partialB){return partialA + partialB; }",
"function(current,a,b)
{return 10; }"
{return current + (Math.log(a) *b); }",
125
---
## Page 150
到Post-Agregator的计算中。JSON示例如下：
4.
为百分比。JSON示例如下：
3.
126
HyperUnique CardinalityPost-Aggregator
Constant Post-Aggregator
例子如下：
"fieldName"
"name":，
HyperUnique Cardinality Post-Aggregator得到 HyperUnique Aggretor的结果，使之能参与