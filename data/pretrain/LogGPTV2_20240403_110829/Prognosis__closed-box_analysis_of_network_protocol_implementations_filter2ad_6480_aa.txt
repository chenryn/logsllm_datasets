title:Prognosis: closed-box analysis of network protocol implementations
author:Tiago Ferreira and
Harrison Brewton and
Loris D'Antoni and
Alexandra Silva
Prognosis: Closed-Box Analysis
of Network Protocol Implementations
Tiago Ferreira
Harrison Brewton
University College London
University of Wisconsin–Madison
Loris D’Antoni
University of Wisconsin–Madison
Alexandra Silva
University College London
ABSTRACT
We present Prognosis, a framework offering automated closed-box
learning and analysis of models of network protocol implementa-
tions. Prognosis can learn models that vary in abstraction level
from simple deterministic automata to models containing data oper-
ations, such as register updates, and can be used to unlock a variety
of analysis techniques—model checking temporal properties, com-
puting differences between models of two implementations of the
same protocol, or improving testing via model-based test generation.
Prognosis is modular and easily adaptable to different protocols
(e.g., TCP and QUIC) and their implementations. We use Prognosis
to learn models of (parts of) three QUIC implementations—Quiche
(Cloudflare), Google QUIC, and Facebook mvfst—and use these mod-
els to analyze the differences between the various implementations.
Our analysis provides insights into different design choices and
uncovers potential bugs. Concretely, we have found critical bugs in
multiple QUIC implementations, which have been acknowledged
by the developers.
CCS CONCEPTS
• Theory of computation → Logic and verification; Verifica-
tion by model checking; Abstraction; Transducers; Automata
over infinite objects;
KEYWORDS
model learning, synthesis, varied abstraction modelling, bug finding,
protocol state machines
ACM Reference Format:
Tiago Ferreira, Harrison Brewton, Loris D’Antoni, and Alexandra Silva. 2021.
Prognosis: Closed-Box Analysis of Network Protocol Implementations. In
ACM SIGCOMM 2021 Conference (SIGCOMM ’21), August 23–27, 2021, Virtual
Event, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/
3452296.3472938
1 INTRODUCTION
Implementations of network and security protocols such as TCP/IP,
SSH, TLS, DTLS, and QUIC are prominent components of many
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8383-7/21/08...$15.00
https://doi.org/10.1145/3452296.3472938
762
internet and server applications. Errors in the implementations
of these protocols are common causes of security breaches and
network failures (e.g., the Heartbleed OpenSSL vulnerability [4] that
rendered SSL/TLS connections futile due to leaked secret keys, and
others [6, 7]). Many approaches have been proposed to verify both
specifications and implementations of the above protocols in an
attempt to provide safety and correctness guarantees [18, 20–22, 27].
For example, Bhargavan et al. [14] are designing a formally verified
(using a theorem prover) implementation of the TLS protocol that
is provably free of many types of security vulnerabilities.
Most existing approaches suffer from being monolithic—small
changes to the implementation require large changes to the ver-
ification process—and require high expertise—one needs to know
the protocol very well and have a priori knowledge of what prop-
erties the implementation should satisfy. One way to avoid these
limitations is to build models of the implementation, which provide
abstractions of the critical components of an implementation and
enable a number of powerful analyses such as model-checking and
model-based test generation.
For example, instead of directly analyzing the binary or source
code of a TCP implementation, one can analyze a model that only
describes what types of packet flags (SYN, ACK, etc.) the implemen-
tation exchanges during a handshake. However, this approach is
still monolithic. First, analyzing the model requires guessing what
properties it must satisfy. Second, designing models still requires
expertise, and whenever the implementation is modified, one has
to manually update the model to retain its faithfulness to the imple-
mentation, updating not only the modified sections but also ensur-
ing existing ones are truly unaffected. This problem is well-known
in the model checking literature and recently McMillan and Zuck
[27] showed, using Microsoft QUIC as an example, that creating a
faithful model of a protocol implementation is a challenging, time-
consuming problem that requires a number of iterations between
the model designer and the protocol-implementation developers.
Orthogonally, several works have shown that one can often
learn a model from implementations (e.g., for passport [10] and
bank card [9] protocols). This idea is called model learning [32] and
it builds on the fact that many classes of finite automata (often
used as models) can be inferred by testing the implementation on a
set of traces. Fiterău-Broştean et al. [22] applied model learning to
detect an anomaly in a real TCP implementation, but their system
is still monolithic—i.e., it requires one to manually design a mapper
between the abstract traces of the model and the concrete traces
of the implementation, a task requiring expert knowledge of the
protocol logic. Thence, the system of Fiterău-Broştean et al. [22] is
not reusable for different protocols and implementations.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Ferreira et al.
In this paper, we consider the following question:
Can we design general and reusable techniques to detect logic errors
affecting the interaction with protocol implementations without
knowing a priori what properties these errors may violate?
We present Prognosis, a modular and reusable framework for
learning and analyzing models of network protocol implementa-
tions, specialising in bug finding and knowledge acquisition rather
than providing verified guarantees. Unlike existing approaches,
Prognosis can easily be adapted to handle different protocols and
protocol implementations, and the programmer does not need to
manually program the logic to map abstract traces to concrete traces
of the protocol implementation, relying instead on a reference im-
plementation that they trust by instrumenting its protocol logic
components. Although any valid implementation of the protocol
can be used as a reference implementation, different implemen-
tations will vary in their ease of modification depending on how
they were designed. The Prognosis Adapter, generated from the
instrumentation, creates the ideal separation of concerns between
implementation-specific details and protocol-specific details. Cru-
cially, the same Adapter can be reused for different implementations
of the same protocol with a simple change, as reference implemen-
tations are, by design, able to communicate with any other valid
implementation of the same protocol. Furthermore, Prognosis
makes it easy to experiment with different levels of abstraction in
the model precision and with several analyses that expose varying
types of bugs.
We use Prognosis to analyze 1 implementation of TCP and 4
of QUIC—Quiche (Cloudflare), Google QUIC, Facebook mvfst, and
QUIC-Tracker [28]. For QUIC, we experiment with varying levels of
abstractions and learn models that: 1) only characterize the packet’s
frames, and 2) concrete data such as packet numbers. The first type
of abstraction allows us to use existing decision procedures to com-
pare whether the models learned from different implementations
are equivalent. When dealing with protocols as big as QUIC for
example, finding a difference in the models does not necessarily
indicate a bug, it can also signal different design decisions as al-
lowed by QUIC’s flexible specification. Nonetheless, some of these
differences the models capture provide greater insight into the con-
sequences of specific design decisions that are sometimes taken
lightly, or sections of the specification that should be stricter. We
found one such case which triggered a change to the specification
to better formalize the intended behavior. Abstract models also
allow us to verify and test safety and liveness properties. By in-
specting counterexamples produced by checking safety properties
we identified 3 bugs in QUIC implementations, which have been
acknowledged by the developers.
Contributions and road map. In a nutshell, the paper contains
the following contributions:
(1) Prognosis, a framework with a modular architecture that en-
ables reusability: different protocols and protocol implementa-
tions can easily be swapped without changes to the learning
engine (Section 2).
(2) Configurable levels of abstraction in learned models. This helps
with scalability and performance yet does not compromise
correctness—once a bug is discovered at the model level, Prog-
nosis creates concrete traces to check whether the bug is in the
implementation or is a false positive (that can be used to refine
the model). (Section 4).
(3) The various types of model Prognosis can learn and synthe-
size expose a range of analyses, e.g., model-based testing and
decision procedures, which we use in finding bugs. (Section 5).
(4) Case studies: a TCP implementation and several QUIC imple-
mentations. The TCP case study shows that our framework
can recover previous results [22], albeit with much less manual
configuration. The QUIC case study highlights that the frame-
work can be used to uncover bugs as well as to gain insight
into different design choices and aid in RFC Improvements (Sec-
tion 6). The bugs uncovered in two popular implementations
of QUIC were confirmed by the developers. One of the bugs
(if exploited) could make the QUIC server vulnerable to DoS
attacks and it highlighted that in fact a part of the protocol was
totally missing from the implementation.
We review relevant related work in Section 7 and conclude the paper
in Section 8 with a discussion of the limitations of the framework,
and directions for further work.
Ethical statement Work does not raise any ethical issues.
2 ARCHITECTURE
In this section, we give an overview of the architecture of our
framework. Prognosis is comprised of three modules (see Figure 1),
which are parametrized by inputs provided by the user who is
analyzing the protocol.
System Under Learning (SUL). The SUL is the protocol implemen-
tation we are analyzing—e.g., a TCP server. This implementation is
accessed in a closed-box fashion—i.e., all we assume is that we can
send packets to and receive packets from it. In general, implemen-
tations are complex, and we cannot hope to directly learn models
of their entire behaviors. A user of Prognosis provides an Adapter
pair (𝛼, 𝛾) containing an abstraction function 𝛼 that maps concrete
packet traces of the SUL to simplified abstract traces. For exam-
ple, the function 𝛼 might simplify TCP packets to only consider
whether they are of type SYN, ACK, or SYN-ACK. We also need
a concretization function 𝛾 that maps simplified abstract traces to
concrete ones accepted by the SUL. Unlike existing approaches that
require the user to explicitly implement this function [22], Prog-
nosis only requires the user to instrument an existing reference
implementation of the same protocol we are analyzing to produce
concrete packet traces from abstract ones. We describe this setup in
detail in Section 3. Now that we have an Adapter that can map be-
tween abstract and concrete traces, we have the interface required
to learn models and perform analysis.
Learning Module. We use existing active model learning algo-
rithms [25] to learn a model of the SUL. Active learning algorithms
work by directly interacting with the SUL instead of relying on
passive data such as logged events or historic data. This allows us
to do closed-box learning, where we don’t have access to the inner
workings of the SUL, and guarantees that the learner is able to get
answers to every query it may formulate.
Specifically, we focus on algorithms based on the Minimally
Adequate Teacher Framework, where the learning algorithm can
763
Prognosis: Closed-Box Analysis of Network Protocol Implementations
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Learning
queries
System Under Learning (SUL)
Adapter
𝛼
Implementation
Concrete
Symbols
Reference Implementation
Abstract
Symbols
𝛾
Abstract
Alphabet
(cid:98)Σ
Check
queries
Property
Set
Analysis
Module
Φ
System Under
Learning (SUL)
(𝛼, 𝛾)
Translation
Pair
User
Learned
Model(s)
Learning
Module
Figure 1: Prognosis’ modular architecture.
perform 2 types of queries: membership and equivalence. Both
types of queries are further explored in Section 4.
On an abstract level, the models operate over the simplified
abstract traces and therefore abstract away much of the complexity
of the SUL, allowing the user to focus on the properties of interest.
On a concrete level this involves sending sequences of input
packets to the SUL, and registering the response obtained. For
example, the learner might ask what happens when a SYN packet
is sent and followed by an ACK. The SUL uses its Adapter to, acting
as a client, produce and send these packets to the implementation,
and return an answer the learner uses to build an accurate model.
In Section 4, we show how Prognosis can learn different models
of varying levels of abstraction—from plain deterministic models
to models that include registers to store e.g., packet numbers.
We show how Prognosis can be used to learn a detailed model
of the TCP 3-way handshake shown in Figure 3.
Analysis Module. This module enables the use of the learned
models to analyze the SUL, using a portfolio of techniques to unveil
complex bugs and help the user gain insights about the implementa-
tion’s behavior. For example, Prognosis can automatically compare
whether the models learned for two different implementations are
equivalent (for different notions of equivalence) and also supports
simple visualizations of the learned models that allow a user to
visually compare two models for differing behaviors. In Section 6,
we show how the automated equivalence check and visualizations
aided in detecting anomalies and explaining such anomalies to real
developers in our evaluation. For example, Prognosis could detect
that a supposedly variable value being transmitted was actually a
constant. The full set of analysis exposed by Prognosis is discussed
in Section 5.
3 SYSTEM UNDER LEARNING (SUL)
The SUL has two sub-components: An Implementation we want to
analyze and learn a model of, and a protocol-specific Adapter that
uses a translation pair to transform concrete packet traces into sim-
plified abstract traces, and vice-versa. The Adapter is the interface
with the learning module (Figure 2). In fact, the learner is completely
oblivious to the existence of concrete traces, and communicates
only directly with the Adapter, which then communicates with the
Implementation. The learning module (described in Section 2) will
use the abstract traces to build models of the implementation.
Implementation