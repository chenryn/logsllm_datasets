### 2.2.1 Q-gram Vector Representation

In the vector representation, all dimensions associated with q-grams contained in the report \( x \) are set to one, while all other dimensions are set to zero. To avoid an implicit bias on the length of reports, we normalize the vector \( \varphi(x) \) such that its norm is one, i.e., \( \| \varphi(x) \| = 1 \). This normalization ensures that a q-gram has a greater impact in a report with fewer distinct q-grams. Therefore, changing a constant number of tokens in a report with repetitive structure will have a more significant effect on the vector compared to a report with several different patterns.

### 2.2.2 Efficient Q-gram Representation

At first glance, the mapping \( \varphi \) may seem inappropriate for efficient analysis because the set \( S \) covers all possible q-grams of words, leading to a very high-dimensional vector space. However, the number of q-grams in a report is linear in its length. A report \( x \) containing \( m \) words can have at most \( (m - q) \) different q-grams. Consequently, only \( (m - q) \) dimensions are non-zero in the vector \( \varphi(x) \), regardless of the dimensionality of the vector space. Thus, it suffices to store only the q-grams present in each report \( x \) for a sparse representation of the vector \( \varphi(x) \). Efficient data structures, such as sorted arrays [19] or Bloom filters [22], can be used for this purpose. As demonstrated in Section 3.4, this sparse representation provides the basis for very efficient feature extraction, with median run-times below 1 ms per analysis report.

### 2.3 Learning-based Detection

#### 2.3.1 Support Vector Machines

As the final analysis step in Cujo, we employ a learning-based detection method for drive-by-download attacks, which builds on the vectorial representation of analysis reports. Using machine learning eliminates the need for manually constructing and updating detection rules for static and dynamic code analysis, thereby reducing the delay in detecting novel drive-by downloads.

To automatically generate detection models from the reports of both attack and benign JavaScript code, we apply the technique of Support Vector Machines (SVMs) [13, 20]. Given vectors of two classes as training data, an SVM determines a hyperplane that separates both classes with maximum margin. In our setting, one class corresponds to analysis reports of drive-by downloads, while the other class corresponds to reports of benign web pages. An unknown report \( \varphi(x) \) is classified by mapping it to the vector space and checking whether it falls on the malicious or benign side of the hyperplane. This learning-based detection is illustrated in Figure 5.

Formally, the detection model of an SVM corresponds to a vector \( w \) and a bias \( b \), specifying the direction and offset of the hyperplane in the vector space. The corresponding detection function \( f \) is given by:
\[ f(x) = \langle \varphi(x), w \rangle + b = \sum_{s \in S} \varphi_s(x) \cdot w_s + b \]
and returns the orientation of \( \varphi(x) \) with respect to the hyperplane. Specifically, \( f(x) > 0 \) indicates malicious activity in the report \( x \), while \( f(x) \leq 0 \) indicates benign data.

SVMs have the ability to compensate for a certain amount of noise in the labels of the training data, which is crucial for practical applications like Cujo. This robustness allows the learning process to handle a minor amount of unknown attacks in the benign portion of the training data, enabling the generation of accurate detection models even if some web pages labeled as benign contain drive-by-download attacks. The theory and further details on this ability of SVMs are discussed in [13, 20].

#### 2.3.2 Efficient Classification of Q-grams

For efficient computation of \( f \), we again exploit the sparse representation of vectors induced by \( \varphi \). Given a report \( x \), only the q-grams contained in \( x \) have non-zero entries in \( \varphi(x) \). Therefore, we can simplify the detection function \( f \) as follows:
\[ f(x) = \sum_{s \in x} \varphi_s(x) \cdot w_s + b \]
where we determine \( f(x) \) by looking up the values \( w_s \) for each q-gram contained in \( x \). Consequently, the classification of a report can be carried out with linear time complexity and a median run-time below 0.2 ms per report (cf. Section 3.4). For learning the detection model of the SVM, we use LibLinear [9], a fast SVM library that enables us to train detection models from 100,000 reports in 120 seconds for dynamic analysis and in 50 seconds for static analysis.

#### 2.3.3 Explanation

In practice, a detection system must not only flag malicious events but also provide insights into the detection process, allowing for the inspection of attack patterns and exploited vulnerabilities during operation. We can adapt the detection function to explain the decision process of the SVM. During the computation of \( f \), we additionally store the individual contribution \( \varphi_s(x) \cdot w_s \) of each q-gram to the final detection score \( f(x) \). If an explanation is requested, we output the q-grams with the largest contributions, thereby presenting the analysis patterns that shifted the report \( x \) to the positive side of the hyperplane. This concept is illustrated in Section 3.3, where we present explanations for detections of drive-by-download attacks using reports of static and dynamic analysis.

The learning-based detection completes the design of our system, Cujo. As illustrated in Figure 1, Cujo uses two independent processing chains for static and dynamic code analysis, where an alert is reported if either detection model indicates a drive-by download. This combined detection makes evasion difficult, as it requires the attacker to cloak their attacks from both static and dynamic analysis. While static analysis alone can be thwarted through massive obfuscation, the hidden code needs to be decrypted during runtime, which can be tracked by dynamic analysis. Similarly, if less obfuscation is used and the attacker tries to spoil the sandbox emulation, patterns of the respective code might be visible to static analysis. Although this argument does not rule out evasion entirely, it clearly shows the effort necessary for evading our system.

### 3. Evaluation

After presenting the detection methodology of Cujo, we turn to an empirical evaluation of its performance. We conduct experiments to study the detection and run-time performance in detail. Before presenting these experiments, we introduce our datasets of drive-by-download attacks and benign web pages.

#### 3.1 Datasets

We consider two datasets containing URLs of benign web pages: Alexa-200k and Surﬁng, as listed in Table 1(a). The Alexa-200k dataset includes the 200,000 most visited web pages as listed by Alexa, covering a wide range of JavaScript code, including search engines, social networks, and online shops. The Surﬁng dataset comprises 20,283 URLs of web pages visited during usual web surfing at our institute over a period of 10 days, involving five users. Both datasets have been sanitized by scanning the web pages for drive-by downloads using common attack strings and the Google Safe Browsing service. While very few unknown attacks might still be present, we rely on the SVM learning algorithm's ability to effectively compensate for this inconsistency.

| **(a) Benign Data Sets** | |
| --- | --- |
| **Data Set** | **# URLs** |
| Alexa-200k | 200,000 |
| Surﬁng | 20,283 |

| **(b) Attack Data Sets** | |
| --- | --- |
| **Data Set** | **# Attacks** |
| Spam Trap | 256 |
| SQL Injection | 22 |
| Malware Forum | 201 |
| Wepawet-new | 46 |
| Obfuscated | 84 |

**Table 1: Description of benign and attack datasets.**

The attack datasets, listed in Table 1(b), are mainly taken from Cova et al. [4]. In total, the attack datasets comprise 609 samples containing various types of drive-by-download attacks collected over two years. The attacks are organized according to their origin: the Spam Trap set includes attacks extracted from URLs in spam messages, the SQL Injection set contains drive-by downloads injected into benign websites, the Malware Forum set covers attacks published in internet forums, and the Wepawet-new set contains malicious JavaScript code submitted to the Wepawet service. Additionally, we provide the Obfuscated set, which contains 28 attacks from the other sets, further obfuscated using a popular JavaScript packer.

#### 3.2 Detection Performance

In our first experiment, we study the detection performance of Cujo in terms of true-positive rate (ratio of detected attacks) and false-positive rate (ratio of misclassified benign web pages). Since the learning-based detection in Cujo requires a set of known attacks and benign data for training detection models, we randomly split all datasets into a known partition (75%) and an unknown partition (25%). The detection models and parameters, such as the best length of q-grams, are determined on the known partition, while the unknown partition is used only for measuring the final detection performance. We repeat this procedure 10 times and average the results. This partitioning ensures that the reported results refer only to attacks unknown during the learning phase of Cujo.

For comparison with state-of-the-art methods, we also consider static detection methods, namely the anti-virus scanner ClamAv and the web proxy of the security suite AntiVir. As ClamAv does not provide proxy capabilities, we manually feed the downloaded web pages and JavaScript code to the scanner. We also include results presented by Cova et al. [4] for the offline analysis system Jsand in our evaluation.

##### 3.2.1 True-positive Rates

Tables 2 and 3 show the detection performance in terms of true-positive rates for Cujo and the other methods. The static and dynamic code analysis of Cujo alone achieve true-positive rates of 90.2% and 86.0%, respectively. The combination of both, however, allows the identification of 94.4% of the attacks, demonstrating the advantage of two complementary views on JavaScript code. Only Jsand achieves better performance, almost perfectly detecting all attacks, but it operates offline and spends considerably more time analyzing JavaScript code. The anti-virus tools, ClamAv and AntiVir, achieve lower detection rates of 35% and 70%, respectively, despite being equipped with up-to-date signatures. These results confirm the need for alternative detection techniques, as provided by Cujo and Jsand, for successfully defending against drive-by-download attacks.

| **Attack Data Sets** | **Static** | **Dynamic** | **Combined** |
| --- | --- | --- | --- |
| Spam Trap | 96.9% | 98.1% | 99.4% |
| SQL Injection | 93.8% | 88.3% | 98.3% |
| Malware Forum | 78.7% | 71.2% | 85.5% |
| Wepawet-new | 86.3% | 84.1% | 94.8% |
| Obfuscated | 100.0% | 87.3% | 100.0% |
| **Average** | 90.2% | 86.0% | 94.4% |

**Table 2: True-positive rates of Cujo on the attack datasets. Results averaged over 10 runs.**

| **Attack Data Sets** | **ClamAv** | **AntiVir** | **Jsand [4]** |
| --- | --- | --- | --- |
| Spam Trap | 58.2% | 95.5% | 99.7% |
| SQL Injection | 83.1% | — | 100.0% |
| Malware Forum | 93.5% | 45.3% | 99.6% |
| Wepawet-new | — | — | — |
| Wepawet-old | — | — | — |
| Obfuscated | 54.8% | 19.6% | 100.0% |
| **Average** | 35.0% | 70.0% | 99.8% |

**Table 3: True-positive rates of ClamAV, AntiVir, and Jsand on the attack datasets. The Wepawet-new dataset is a recent version of Wepawet-old.**

##### 3.2.2 False-positive Rates

Tables 4 and 5 show the false-positive rates on the benign datasets for all detection methods. Except for AntiVir, all methods achieve reasonably low false-positive rates. The combined analysis of Cujo yields a false-positive rate of 0.002%, corresponding to 2 false alarms in 100,000 visited web sites, on the Alexa-200k dataset. Moreover, Cujo does not trigger any false alarms on the Surﬁng dataset.

The high false-positive rate of AntiVir (0.087%) is due to overly generic detection rules. The majority of false alarms are labeled as HTML/Redirector.X, indicating a potential redirect, with the remaining alerts having generic labels such as HTML/Crypted.Gen and HTML/Downloader.Gen. We carefully verified each of these alerts using a client-based honeypot [21] but could not determine any malicious activity on the indicated web pages.

For the false alarms raised by Cujo, we identify two main causes: 0.001% of the web pages in the Alexa-200k dataset contain fully encrypted JavaScript code with no plain-text operations except for unescape and eval. This drastic form of obfuscation induces the false alarms of the static analysis. The 0.001% false positives of the dynamic analysis result from web pages redirecting error messages of JavaScript to customized functions, a technique frequently used in drive-by downloads to hide errors during exploitation of vulnerabilities, though applied in a benign context in these 0.001% cases.

| **Benign Data Sets** | **Static** | **Dynamic** | **Combined** |
| --- | --- | --- | --- |
| Alexa-200k | 0.001% | 0.001% | 0.002% |
| Surﬁng | 0.000% | 0.000% | 0.000% |

**Table 4: False-positive rates of Cujo on the benign datasets. Results averaged over 10 runs.**

| **Benign Data Sets** | **ClamAv** | **AntiVir** | **Cova et al.** | **Jsand [4]** |
| --- | --- | --- | --- | --- |
| Alexa-200k | 0.000% | 0.087% | — | — |
| Surﬁng | 0.000% | 0.000% | 0.000% | 0.013% |

**Table 5: False-positive rates of ClamAV, AntiVir, and Jsand on the benign datasets.**