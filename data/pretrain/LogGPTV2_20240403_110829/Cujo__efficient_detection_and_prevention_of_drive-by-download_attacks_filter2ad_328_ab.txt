such that all dimensions associated with q-grams contained
in x are set to one and all other dimensions are zero. To
avoid an implicit bias on the length of reports, we normalize
φ(x) to one, that is, we set ||φ(x)|| = 1. As a result of
this normalization, a q-gram counts more in a report that
has fewer distinct q-grams. That is, changing a constant
amount of tokens in a report containing repetitive structure
has more impact on the vector than in an analysis report
comprising several diﬀerent patterns.
2.2.2 Efﬁcient Q-gram Representation
At the ﬁrst glance, the mapping φ seems inappropriate
for eﬃcient analysis: the set S covers all possible q-grams of
words and induces a vector space of very large dimension.
Fortunately, the number of q-grams contained in a report is
linear in its length. An analysis report x containing m words
comprises at most (m − q) diﬀerent q-grams. Consequently,
only (m − q) dimensions are non-zero in the vector φ(x),
It thus
irrespective of the dimension of the vector space.
suﬃces to only store the q-grams contained in each report x
for a sparse representation of the vector φ(x), for exam-
ple, using eﬃcient data structures such as sorted arrays [19]
or Bloom ﬁlters [22]. As demonstrated in Section 3.4, this
sparse representation of feature vectors provides the basis
for very eﬃcient feature extraction with median run-times
below 1 ms per analysis report.
2.3 Learning-based Detection
Support Vector Machines
As ﬁnal analysis step of Cujo, we present a learning-based
detection of drive-by-download attacks, which builds on the
vectorial representation of analysis reports. The application
of machine learning spares us from manually constructing
and updating detection rules for static and dynamic code
analysis, and thereby limits the delay to detection of novel
drive-by downloads.
2.3.1
For automatically generating detection models from the
reports of attacks and benign JavaScript code, we apply the
technique of Support Vector Machines (SVM) [see 13, 20].
Given vectors of two classes as training data, an SVM deter-
mines a hyperplane that separates both classes with max-
imum margin. In our setting, one of these classes is asso-
ciated with analysis reports of drive-by downloads, whereas
the other class corresponds to reports of benign web pages.
An unknown report φ(x) is now classiﬁed by mapping it to
Figure 5: Schematic vector representation of analy-
sis reports with maximum-margin hyperplane.
the vector space and checking if it falls on either the mali-
cious or benign side of the hyperplane. This learning-based
detection of drive-by downloads is illustrated in Figure 5.
Formally, the detection model of an SVM corresponds to
a vector w and bias b, specifying the direction and oﬀset
of the hyperplane in the vector space. The corresponding
detection function f is given by
f (x) = hφ(x), wi + b =
φs(x) · ws + b.
X
s∈S
and returns the orientation of φ(x) with respect to the hy-
perplane. That is, f (x) > 0 indicates malicious activity in
the report x and f (x) ≤ 0 corresponds to benign data.
In contrast to many other learning techniques, SVMs pos-
sess the ability to compensate a certain amount of noise in
the labels of the training data—a crucial property for prac-
tical application of Cujo. This ability renders the learning
process robust to a minor amount of unknown attacks in the
benign portion of the training data and enables generating
accurate detection models, even if some of the web pages
labeled as benign data contain drive-by-download attacks.
Theory and further details on this ability of SVMs are dis-
cussed in [13, 20].
2.3.2 Efﬁcient Classiﬁcation of Q-grams
For eﬃciently computing f , we again exploit the sparse
representation of vectors induced by φ. Given a report x,
we know that only q-grams contained in x have non-zero
entries in φ(x), that is, all other dimensions in φ(x) are zero
and do not contribute to the computation of f (x). Hence,
we can simplify the detection function f as follows
f (x) =
φs(x) · ws + b =
φs(x) · ws + b,
X
s in x
X
s∈S
where we determine f (x) by simply looking up the values
ws for each q-gram contained in x. As a consequence, the
classiﬁcation of a report can be carried out with linear time
complexity and a median run-time below 0.2 ms per report
(cf. Section 3.4). For learning the detection model of the
SVM we employ LibLinear [9], a fast SVM library which
enables us to train detection models from 100,000 reports
in 120 seconds for dynamic analysis and in 50 seconds for
static analysis.
2.3.3 Explanation
In practice, a detection systems must not only ﬂag ma-
licious events but also provide insights into the detection
process, such that attack patterns and exploited vulnerabil-
ities can be inspected during operation. Fortunately, we can
adapt the detection function for explaining the decision pro-
cess of the SVM. During computation of f , we additionally
store the individual contribution φs(x)·ws of each q-gram to
maximum marginwφ(x)benign codedrive-by downloadsthe ﬁnal detection score f (x). If an explanation is requested,
we output the q-grams with largest contribution and thereby
present those analysis patterns that shifted the analysis re-
port x to the positive side of the hyperplane. We illustrate
this concept in Section 3.3, where we present explanations
for detections of drive-by-download attacks using reports of
static and dynamic analysis.
The learning-based detection completes the design of our
system Cujo. As illustrated in Figure 1, Cujo uses two
independent processing chains for static and dynamic code
analysis, where an alert is reported if one of the detection
models indicates a drive-by download.
This combined detection renders evasion of our system
diﬃcult, as it requires the attacker to cloak his attacks from
both, static and dynamic analysis. While static analysis
alone can be thwarted through massive obfuscation, the hid-
den code needs to be decrypted during run-time which in
turn can be tracked by dynamic analysis. Similarly, if fewer
obfuscation is used and the attacker tries to spoil the sand-
box emulation, patterns of the respective code might be vis-
ible to static analysis. Although this argumentation does
not rule out evasion in general, it clearly shows the eﬀort
necessary for evading our system.
3. EVALUATION
After presenting the detection methodology of Cujo, we
turn to an empirical evaluation of its performance. In par-
ticular, we conduct experiments to study the detection and
run-time performance in detail. Before presenting these ex-
periments, we introduce our data sets of drive-by-download
attacks and benign web pages.
3.1 Data Sets
We consider two data sets containing URLs of benign
web pages, Alexa-200k and Surﬁng, which are listed in Ta-
ble 1(a). The Alexa-200k data set corresponds to the 200,000
most visited web pages in the Internet as listed by Alexa3
and covers a wide range of JavaScript code, including sev-
eral search engines, social networks and on-line shops. The
Surﬁng data set comprises 20,283 URLs of web pages visited
during usual web surﬁng at our institute. The data has been
recorded over a period of 10 days and contains individual
sessions of ﬁve users. Both data sets have been sanitized by
scanning the web pages for drive-by downloads using com-
mon attack strings and the GoogleSafeBrowsing service.
While very few unknown attacks might still be present in the
data, we rely on the ability of the SVM learning algorithm
to compensate this inconsistency eﬀectively.
(a) Benign data sets
(b) Attack data sets
Data set # URLs
200,000
Alexa-200k
Surﬁng
20,283
Data set
Spam Trap
SQL Injection
Malware Forum
Wepawet-new
Obfuscated
# attacks
256
22
201
46
84
Table 1: Description of benign and attack data sets.
The attack data sets have been taken from [4].
3Alexa Top Sites, http://www.alexa.com/topsites
The attack data sets are listed in Table 1(b) and have
been mainly taken from Cova et al. [4].
In total, the at-
tack data sets comprise 609 samples containing several types
of drive-by-download attacks collected over a period of two
years. The attacks are organized according to their origin:
the Spam Trap set comprises attacks extracted from URLs
in spam messages, the SQL Injection set contains drive-by
downloads injected into benign web sites, the Malware Fo-
rum set covers attacks published in Internet forums, and the
Wepawet-new set contains malicious JavaScript code sub-
mitted to the Wepawet service4. A detailed description of
these classes is provided in [4]. Moreover, we provide the Ob-
fuscated set which contains 28 attacks from the other sets
additionally obfuscated using a popular JavaScript packer5.
3.2 Detection Performance
In our ﬁrst experiment, we study the detection perfor-
mance of Cujo in terms of true-positive rate (ratio of de-
tected attacks) and false-positive rate (ratio of misclassiﬁed
benign web pages). As the learning-based detection imple-
mented in Cujo requires a set of known attacks and benign
data for training detection models, we conduct the following
experimental procedure: We randomly split all data sets into
a known partition (75%) and an unknown partition (25%).
The detection models and respective parameters, such as
the best length of q-grams, are determined on the known
partition, whereas the unknown partition is only used for
measuring the ﬁnal detection performance. We repeat this
procedure 10 times and average results. The partitioning
ensures that reported results only refer to attacks unknown
during the learning phase of Cujo.
For comparing the performance of Cujo with state-of-
the-art methods, we also consider static detection meth-
ods, namely the anti-virus scanner ClamAv6 and the web
proxy of the security suite AntiVir7. As ClamAv does not
provide any proxy capabilities, we manually feed the down-
loaded web pages and respective JavaScript code to the scan-
ner. Moreover, we add results presented by Cova et al. [4]
for the oﬄine analysis system Jsand to our evaluation.
3.2.1 True-positive Rates
Table 2 and 3 show the detection performance in terms
of true-positive rates for Cujo and the other methods. The
static and dynamic code analysis of Cujo alone attain a
true-positive rate of 90.2% and 86.0%, respectively. The
combination of both, however, allows to identify 94.4% of the
attacks, demonstrating the advantage of two complementary
views on JavaScript code.
A better performance is only achieved by Jsand which is
able to almost perfectly detect all attacks. However, Jsand
generally operates oﬄine and spends considerably more time
for analysis of JavaScript code. The anti-virus tools, Cla-
mAv and AntiVir, achieve lower detection rates of 35%
and 70%, respectively, although both have been equipped
with up-to-date signatures. These results clearly conﬁrm
the need for alternative detection techniques, as provided
by Cujo and Jsand, for successfully defending against the
threat of drive-by-download attacks.
4Wepawet Service, http://wepawet.cs.ucsb.edu
5JavaScript Compressor, http://dean.edwards.name/packer
6Clam AntiVirus, http://www.clamav.net/
7Avira AntiVir Premium, http://www.avira.com/
Attack data sets
Spam Trap
SQL Injection
Malware Forum
Wepawet-new
Obfuscated
Average
static
96.9%
93.8%
78.7%
86.3%
100.0%
90.2%
Cujo
dynamic
98.1%
88.3%
71.2%
84.1%
87.3%
86.0%
combined
99.4%
98.3%
85.5%
94.8%
100.0%
94.4%
Table 2: True-positive rates of Cujo on the attack
data sets. Results have been averaged over 10 runs.
Attack data sets ClamAv AntiVir
Spam Trap
58.2%
95.5%
SQL Injection
83.1%
Malware Forum
93.5%
Wepawet-new
—
Wepawet-old
Obfuscated
54.8%
70.0%
Average
41.0%
18.2%
45.3%
19.6%
—
4.8%
35.0%
Jsand [4]
99.7%
100.0%
99.6%
—
100.0%
—
99.8%
Table 3: True-positive rates of ClamAV, AntiVir
and Jsand on the attack data sets. The Wepawet-
new data set is a recent version of Wepawet-old.
3.2.2 False-positive Rates
Table 4 and 5 show the false-positive rates on the benign
data sets for all detection methods. Except for AntiVir
all methods attain reasonably low false-positive rates. The
combined analysis of Cujo yields a false-positive rate of
0.002%, corresponding to 2 false alarms in 100,000 visited
web sites, on the Alexa-200k data set. Moreover, Cujo does
not trigger any false alarms on the Surﬁng data set.
The high false-positive rate of AntiVir with 0.087% is
due to overly generic detection rules. The majority of false
alarms shows the label HTML/Redirector.X, indicating a po-
tential redirect, where the remaining alerts have generic la-
bels, such as HTML/Crypted.Gen and HTML/Downloader.Gen. We
carefully veriﬁed each of these alerts using a client-based
honeypot [21], but could not determine any malicious activ-
ity on the indicated web pages.
For the false alarms raised by Cujo we identify two main
causes: 0.001% of the web pages in the Alexa-200k data set
contain fully encrypted JavaScript code with no plain-text
operations except for unescape and eval. This drastic form
of obfuscation induces the false alarms of the static analy-
sis. The 0.001% false positives of the dynamic analysis result
from web pages redirecting error messages of JavaScript to
customized functions. Such redirection is frequently used
in drive-by downloads to hide errors during exploitation of
vulnerabilities, though it is applied in a benign context in
these 0.001% cases.
Benign data sets
Alexa-200k
Surﬁng
static
0.001%
0.000%
Cujo
dynamic
0.001%
0.000%
combined
0.002%
0.000%
Table 4: False-positive rates of Cujo on the benign
data sets. Results have been averaged over 10 runs.
Benign data sets ClamAv AntiVir
0.087%
Alexa-200k
0.000%
Surﬁng
Cova et al.
—
0.000%
0.000%
—
Jsand [4]
—
—
0.013%
Table 5: False-positive rates of ClamAV, AntiVir
and Jsand on the benign data sets.