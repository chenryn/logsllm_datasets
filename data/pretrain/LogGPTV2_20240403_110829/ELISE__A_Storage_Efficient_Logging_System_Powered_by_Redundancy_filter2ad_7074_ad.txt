formation. In Table 1, we also list the size of the studied log
in the last column, and for short, we give each log ﬁle a name
which is listed in the ﬁrst column. The logs are collected with
typical workloads. For Linux, Windows and FreeBSD oper-
ating systems, they are used as end user machines running
ofﬁce software suites, browsers, editors, note-taking software,
email clients, calendars and so on. The Apache2 server is
hosting both static and dynamic websites such as blogs and
wiki sites. The FTP server provides ﬁle sharing service for our
organization, and the MySQL databases contain the records
for several relational databases. To ensure the consistency
of workloads, we collect log data under the same condition
and we manually compare them with logs collected in the
production environment.
4.2 Effectiveness of ELISE
Experiments. We use log ﬁles generated by different systems
and applications to evaluate the effectiveness of ELISE, and
3030    30th USENIX Security Symposium
USENIX Association
Table 1: The Overview of Datasets∗.
Name
OS
Collector
Event Type
Linux
Lin
Win Windows
Htp
Ftp
Sql
BSD FreeBSD
Linux
Linux
Linux
Auditd
System calls, I/O information
Process and user information
HTTP connection and access
FTP connection and access
Sysinternal
Apache2
VSFTP
MySQL MySQL connection and actions
DTrace
System calls, I/O information
Size (GB)
0.8, 7.7, 16.1
0.7, 7.4
2.4, 24.3
0.9, 9.5
1.0, 10.0
0.8, 8.3
∗ In this paper, we use Name-Size to refer one dataset.
compare it with existing methods Gzip and DeepZip by mea-
suring the compression ratio (CR) which is deﬁned as the to-
tal size after compression (including models and compressed
data) over the original ﬁle size. Gzip embeds its reference
table as part of its ﬁnal output ﬁle (with some other engineer-
ing optimizations to reduce its size), and thus we just need to
measure the size of the output ﬁle to determine its total size
after compression. For ELISE, the ﬁnal results contain two
parts, i.e., the trained model and the data ﬁle containing the
reference table and compressed contents. The total size after
compression for DeepZip will be the sum of the size of the
model and the size of the data ﬁle.
ELISE has a set of conﬁgurable parameters, and we use the
default setting in our experiments. To be more speciﬁc, we
use 10 parallel processes to perform the compression. For a
fair comparison, ELISE uses the same model architecture as
DeepZip. More results on the effects of different conﬁgura-
tions are presented in Section 4.4.1. Because DeepZip is slow
and takes a signiﬁcantly long time for large ﬁles, we choose
to use 13 hours as a time limit for the compression process.
If one method is taking too much time (i.e., longer than the
threshold), we will mark it as T/O (i.e., time out).
Results and analysis. Results of our effectiveness evaluation
are summarized in Figure 4. In Figure 4(a), the X-axis denotes
used datasets and Y-axis shows the compression ratio for each
method. For the rest ﬁgures (Figure 4(b) to Figure 4(h)), we
use 0 to 9 in the X-axis to represent individual ﬁles after
splitting. The dash lines in each ﬁgure show the average
compression ratios on all these ﬁles for the two methods, and
their concrete values are marked in the Y-axis. Notice that we
do not show results for DeeZip in these ﬁgures. The reason is
that we set our timeout threshold value to be 13 hours. That
is, when the compression takes longer than 13 hours, we stop
it and cannot report results for corresponding experiments.
DeepZip timeouts on all large ﬁles. As will be discussed in
Section 4.2, DeepZip takes 12.7 hours to process a 0.8 GB
sized Linux system log, and cannot scale to large ﬁles.
From results on small sized ﬁles in Figure 4, we can see
that although compression ratios vary for different log ﬁles,
ELISE achieves the best compression ratios among the 3 com-
pared methods. Overall, ELISE is 1.13 ∼ 12.97 times better
than Gzip and DeepZip. Different compression ratios of three
methods show that all three methods can successfully detect
the redundancies in the log ﬁles, but at different levels. Our
results indicate the advantage of ELISE in log compression
compared to existing compressors and good generalization
to different types of log ﬁles. All three methods achieve the
best compression result on HTTP logs and the worst on BSD
logs. This is because HTTP logs have more redundancies
than others and BSD logs contain less redundant information.
We can see that ELISE has lower compression ratios for all
datasets. On average, its compression ratio is only 36.96%
and 54.41% that of Gzip and DeepZip. In other words, it can
shrink the data size by half or even more compared to the
other two methods. The best results are both achieved on
HTTP with over 11.08 times improvement on average, and
the worst results are both obtained on the BSD log.
Observations on large sized logs are consistent with small
sized logs. For the same type of logs, they show similar com-
pression ratios for all methods and the relative compression
results of different methods are also the same. Individual
small ﬁles split from the same large ﬁle have similar compres-
sion ratios with Gzip and DeepZip, leading to consistent ﬁnal
results. Notice that the compression ratio for a certain method
can be affected by the size of the ﬁle. The statistic numbers
for a character in small sized ﬁles tend to be biased. However,
log ﬁles are highly redundant, and we do not observe such
phenomena in our experiments.
4.3 Efﬁciency of ELISE
We further evaluate both the runtime cost and memory usage
of ELISE to understand its efﬁciency. In these experiments,
we use the same settings for DeepZip and ELISE including the
same model architecture and training parameters. Speciﬁcally,
we use the model described in Figure 3. The batch size is
4096 and the learning rate is 0.001. The selection of batch
size and learning rate are discussed in Section 4.4
4.3.1 Runtime cost of ELISE
The runtime cost can be divided into four parts in ELISE: log
ﬁle splitting and preprocessing, DNN model training, data
compression and decompression. To measure the cost of each
step, we proﬁle the execution of ELISE on 6 datasets and log
the runtime of each step. Gzip does not require training a
model, and its runtime cost is divided into only two parts:
compression and decompression. For DeepZip, we measure
its training time, compression time and decompression time.
Results are summarized in Figure 4(i). Each stacked bar rep-
resents the time used for processing the log ﬁle, and its four
components denote the time for each step. The X-axis shows
the dataset and compression method, and the Y-axis measures
the time cost in minutes. The time cost of Gzip is not shown in
this ﬁgure because of different compression steps. We include
a discussion in the following result analysis. Also, we only
show the results for small log ﬁles to avoid T/O for DeepZip.
In practice, the time cost is almost linear with the log size if
USENIX Association
30th USENIX Security Symposium    3031
(a) Results on Different Small Files.
(b) Results on Lin-7.7G.
(c) Results on Lin-16.1G.
(d) Results on Win-7.4G.
(e) Results on Htp-24.3G.
(f) Results on Ftp-9.5G.
(g) Results on Sql-10.0G.
(i) Runtime Cost.
Figure 4: Effectiveness Evaluation on Different Files (EL is short for ELISE and DZ is short for DeepZip).
(h) Results on BSD-8.3G.
the platform and used applications are the same. Thus, small
log ﬁles can represent the relative cost of different methods.
Many parameters in DeepZip and ELISE can affect the run-
time costs such as batch size, and we perform an ablation
study in Section 4.4.
Results and analysis. First, we do not include Gzip in Fig-
ure 4(i). It is the fastest method among all methods. On aver-
age, it takes Gzip 0.17 minutes to compress and 0.06 minutes
to decompress the tested 6 small logs. Compared to Gzip,
DeepZip and ELISE are slower mainly because they require
training a DNN model and querying the model many times
during compression and decompression. Such techniques are
new and have not been optimized on both the hardware and
software stacks. Considering that DeepZip and ELISE have
better compression ratios than Gzip (over 11.08 times better,
Section 4.2), we do envision DNN based compression as a
promising research direction.
On average, DeepZip is 5.63 times slower than ELISE,
even though ELISE has an additional preprocessing step. This
shows that ELISE is more efﬁcient compared to DeepZip,
which is the beneﬁt of our design. By leveraging domain
speciﬁc knowledge of log ﬁles, ELISE converts most of the
texts in the log to numerical formats which enables less train-
ing time and more efﬁcient data compression/decompression.
The preprocessing step takes negligible time compared to the
other three steps in ELISE. Overall, it only scans the log ﬁle,
applies lightweight rules to generate the reference table and
converts log format.
Breaking down to individual steps, both ELISE and
DeepZip spend most of the time on data decompression which
takes almost as long as the other steps combined. In arith-
metic encoding based methods, the decoding phase commonly
takes a longer time, as also observed and analyzed by existing
work [46]. This is because it requires searching the correct
value for variable z in Equation 2. Another interesting ob-
servation is that the training phase does not take too much
time compared with data compression. This is because arith-
metic computations in training (mostly FP32 computations)
are faster on modern systems. For data compression (and de-
compression), we have to use customized data types to store
3032    30th USENIX Security Symposium
USENIX Association
Lin-0.8GWin-0.7GHtp-2.4GFtp-0.9GSql-1.0GBSD-0.8G0.01.02.03.04.05.0Compression Ratio (%)ELISEGzipDeepZip01234567890.01.02.03.04.0Compression Ratio (%)0.883.42ELISEGzip01234567890.01.02.03.04.05.0Compression Ratio (%)0.873.41ELISEGzip01234567890.01.02.03.04.0Compression Ratio (%)1.293.16ELISEGzip01234567890.00.20.40.60.81.0Compression Ratio (%)0.090.66ELISEGzip01234567890.01.02.03.04.0Compression Ratio (%)1.293.35ELISEGzip01234567890.01.02.03.04.05.0Compression Ratio (%)1.524.37ELISEGzip01234567890.02.04.06.0Compression Ratio (%)2.944.92ELISEGzipELLin-0.8GDZELWin-0.7GDZELHtp-2.4GDZELFtp-0.9GDZELSql-1.0GDZELBSD-0.8GDZ0600120018002400300036004200Cost (Min)PreprocessingTrainingCompressionDecompressionintermediate results, which leads to higher runtime overhead.
Even though training requires a lot of time, these operations
have been optimized on modern software and hardware stacks.
DeepZip takes less time on log ﬁles compared with other gen-
eral NL based tasks. This is mainly because logs have less
vocabulary, and their distribution is simpler than other NL
artifacts. For the same log ﬁle, ELISE spends 578% less time
on training compared with DeepZip. This shows the bene-
ﬁts of preprocessing them by removing the redundancies and
converting them to numerical formats.
For ﬁles in the same type (e.g., Linux system logs or BSD
logs) , the processing time and ﬁle size should have a linear
relationship for DNN based compression methods because
the compression methods need to compress more data. Sur-
prisingly, this is also true for most ﬁles from different sources.
For example, it takes similar time for both DeepZip and ELISE
to compress and decompress Win-0.7G and Lin-0.8G, while
Htp-2.4G costs 3 times longer compared with these two
datasets. BSD-0.8G is an exception. The main reason is that,
unlike others, BSD logs ﬁle has a lot of nested structures.
Therefore, handling such a complex log structure requires
more time.
4.3.2 Memory Cost
We measure both GPU and main memory cost of ELISE dur-
ing its execution. Main memory usage is mainly affected by
the buffer that is used to store raw data, and concrete numbers
are omitted. The GPU and main memory usage of ELISE are
shown in Figure 5. In this experiment, we use the default
parameters (i.e., learning rate, split size, ﬁne-tuning ratio and
batch size) for all systems, which ensures that the comparison
is fair. Also, as reported in Section 4.4, default parameters
lead to the overall best result for DeepZip. The X-axis rep-
resents datasets, and the Y-axis represents memory costs in
megabyte scale. From the ﬁgure, we observe that process-
ing different ﬁles consumes the same amount of GPU mem-
ory for individual stages. For different stages, training uses
more GPU memory than compression and decompression.
Speciﬁcally, model training costs 1,632 MB and compres-
sion/decompression takes 700 MB, which can be supported
by all mainstream GPUs. The consumption of GPU is domi-
nated by the size of the model and the number of samples we
use in each batch. Since our LSTM model is small and the de-
fault batch sizes are the same, the GPU memory consumption
is also tiny and similar. Training takes larger GPU memory
because it stores gradients to support backward propagation.
4.4 Ablation Study
In this section, we perform an ablation study for ELISE. Based
on this, we try to answer how to select optimal values in
practice.
4.4.1 Ablation Study for ELISE
ELISE has a few conﬁgurable parameters that may affect its
performance: the size of split ﬁles, batch size and learning