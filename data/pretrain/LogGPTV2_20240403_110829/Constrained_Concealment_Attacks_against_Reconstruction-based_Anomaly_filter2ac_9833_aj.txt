SCADA Systems; Part I: Analysis and Experimentation of Stealthy Deception
Attacks. Control Systems Technology, IEEE Transactions on 21, 5 (2013), 1963–1970.
[4] Wissam Aoudi, Mikel Iturbe, and Magnus Almgren. 2018. Truth Will Out:
Departure-Based Process-Level Detection of Stealthy Attacks on Control Systems.
In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communica-
tions Security (CCS ’18). ACM, 817–831. https://doi.org/10.1145/3243734.3243781
[5] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion Attacks against
Machine Learning at Test Time. In Machine Learning and Knowledge Discovery
in Databases, Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip
Železný (Eds.). 387–402.
[6] Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of
adversarial machine learning. Pattern Recognition 84 (2018), 317–331.
[7] P-J Bristeau, Eric Dorveaux, David Vissière, and Nicolas Petit. 2010. Hardware
and software architecture for state estimation on an experimental low-cost small-
scaled helicopter. Control Engineering Practice 18, 7 (2010), 733–746.
[8] A.A. Cárdnas, S.M. Amin, Z.-S. Lin, Y.-L. Huang, C.-Y. Huang, and S. Sastry.
2011. Attacks against process control systems: Risk assessment, detection, and
response. In ACM Symp. Inf. Comput. Commun. Security.
[9] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin.
2019. On Evaluating Adversarial Robustness. arXiv preprint arXiv:1902.06705
(2019).
[10] N. Carlini and D. Wagner. 2017. Towards Evaluating the Robustness of Neural
Networks. In Proc. of the IEEE Symposium on Security and Privacy. 39–57. https:
//doi.org/10.1109/SP.2017.49
[11] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted
attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW).
IEEE, 1–7.
[12] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017.
Zoo: Zeroth order optimization based black-box attacks to deep neural networks
without training substitute models. In Proceedings of ACM Workshop on Artificial
Intelligence and Security. ACM, 15–26.
[13] Hongjun Choi, Wen-Chuan Lee, Yousra Aafer, Fan Fei, Zhan Tu, Xiangyu Zhang,
Dongyan Xu, and Xinyan Xinyan. 2018. Detecting attacks against robotic vehicles:
A control invariant approach. In Proc. of the ACM Conference on Computer and
Communications Security (CCS). ACM, 801–816.
[14] Hung Dang, Yue Huang, and Ee-Chien Chang. 2017. Evading classifiers by morph-
ing in the dark. In Proc. of the ACM Conference on Computer and Communications
Security (CCS). ACM, 119–133.
[15] Pritam Dash, Mehdi Karimibiuki, and Karthik Pattabiraman. 2019. Out of control:
stealthy attacks against robotic vehicles protected by control-based techniques.
In Proceedings of the 35th Annual Computer Security Applications Conference.
660–672.
[16] Cheng Feng, Venkata Reddy Palleti, Aditya Mathur, and Deeph Chana. 2019. A
Systematic Framework to Generate Invariants for Anomaly Detection in Indus-
trial Control Systems.. In Proc. Network and Distributed System Security Symp.
(NDSS).
[17] Luis Garcia, Ferdinand Brasser, Mehmet H. Cintuglu, Ahmad-Reza Sadeghi,
Osama Mohammed, and Saman A. Zonouz. 2017. Hey, My Malware Knows
Physics! Attacking PLCs with Physical Model Aware Rootkit. In Proceedings of
the Annual Network & Distributed System Security Symposium (NDSS).
[18] Jairo Giraldo, David Urbina, Alvaro Cardenas, Junia Valente, Mustafa Faisal,
Justin Ruths, Nils Ole Tippenhauer, Henrik Sandberg, and Richard Candell. 2018.
A Survey of Physics-Based Attack Detection in Cyber-Physical Systems. ACM
Computing Surveys (CSUR) 51, 4, Article 76 (July 2018), 36 pages. https://doi.org/
10.1145/3203245
[19] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics. 249–256.
[20] Jonathan Goh, Sridhar Adepu, Marcus Tan, and Zi Shan Lee. 2017. Anomaly
detection in cyber physical systems using recurrent neural networks. In High
Constrained Concealment Attacks against Reconstruction-based detectors in ICS
ACSAC 2020, December 7–11, 2020, Austin, USA
Assurance Systems Engineering (HASE), 2017 IEEE 18th International Symposium
on. IEEE, 140–145.
[21] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
Harnessing Adversarial Examples. CoRR abs/1412.6572 (2014).
[22] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2017. Adversarial Examples for Malware Detection. In Proc. of
the European Symposium on Research in Computer Security. Springer International
Publishing, 62–79.
[23] Dina Hadžiosmanović, Robin Sommer, Emmanuele Zambon, and Pieter H. Hartel.
2014. Through the Eye of the PLC: Semantic Security Monitoring for Indus-
trial Processes. In Proceedings of the 30th Annual Computer Security Applications
Conference (ACSAC ’14). ACM, 126–135. https://doi.org/10.1145/2664243.2664277
[24] Michael A Hayes and Miriam AM Capretz. 2015. Contextual anomaly detection
framework for big sensor data. Journal of Big Data 2, 1 (2015), 2.
[25] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[26] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and
JD Tygar. 2011. Adversarial machine learning. In Proceedings of the 4th ACM
workshop on Security and artificial intelligence. ACM, 43–58.
[27] Peter Huitsing, Rodrigo Chandia, Mauricio Papa, and Sujeet Shenoi. 2008. At-
tack taxonomies for the Modbus protocols.
International Journal of Critical
Infrastructure Protection 1 (2008), 37–44.
[28] iTrust, Centre for Research in Cyber Security, Singapore University of Technology
and Design. 2017. WADI datatset.
https://itrust.sutd.edu.sg/itrust-
labs_datasets/dataset_info/, Last accessed on: 2020-06-15.
[29] Keras EarlyStopping callback [n. d.]. EarlyStopping. https://keras.io/api/
(2017).
callbacks/early_stopping. ([n. d.]).
[30] Keras ReduceLROnPlateau callback [n. d.]. ReduceLROnPlateau. https://keras.
io/api/callbacks/reduce_lr_on_plateau/. ([n. d.]).
[31] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[32] Moshe Kravchik and Asaf Shabtai. 2018. Detecting Cyber Attacks in Industrial
Control Systems Using Convolutional Neural Networks. In Proceedings of the
2018 Workshop on Cyber-Physical Systems Security and PrivaCy. ACM, 72–83.
[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[34] Marina Krotofil, Jason Larsen, and Dieter Gollmann. 2015. The Process Matters:
Ensuring Data Veracity in Cyber-Physical Systems. In Proceedings of the 10th
ACM Symposium on Information, Computer and Communications Security (ASIA
CCS ’15). ACM, 133–144. https://doi.org/10.1145/2714576.2714599
[35] Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V Krishnamurthy,
Amit K Roy Chowdhury, and Ananthram Swami. 2019. Stealthy Adversarial
Perturbations Against Real-Time Video Classification Systems. Proceedings of
the Annual Network & Distributed System Security Symposium (NDSS) (2019).
[36] Yao Liu, Peng Ning, and Michael K Reiter. 2011. False data injection attacks
against state estimation in electric power grids. ACM Transactions on Information
and System Security (TISSEC) 14, 1 (2011), 13.
[37] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
[38] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan
Frey. 2015. Adversarial autoencoders. arXiv preprint arXiv:1511.05644 (2015).
[39] Yilin Mo and Bruno Sinopoli. 2009. Secure control against replay attacks. In
Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton
Conference on. IEEE, 911–918.
[40] Yilin Mo, Sean Weerakkody, and Bruno Sinopoli. 2015. Physical authentication
of control systems: Designing watermarked control inputs to detect counterfeit
sensor outputs. IEEE Control Systems Magazine 35, 1 (2015), 93–109.
[41] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal adversarial perturbations. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 1765–1773.
[42] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami. 2017. Practical Black-Box Attacks Against Machine
Learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security (ASIA CCS ’17). ACM, New York, NY, USA, 506–519.
https://doi.org/10.1145/3052973.3053009
[43] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. 2016.
The Limitations of Deep Learning in Adversarial Settings. In 2016 IEEE European
Symposium on Security and Privacy (EuroSP). 372–387. https://doi.org/10.1109/
EuroSP.2016.36
[44] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. 2016. Distillation as a
Defense to Adversarial Perturbations Against Deep Neural Networks. In Proc. of
the IEEE Symposium on Security and Privacy. 582–597. https://doi.org/10.1109/SP.
2016.41
[45] Raul Quinonez, Jairo Giraldo, Luis Salazar, Erick Bauman, Alvaro Cardenas,
and Zhiqiang Lin. 2020. SAVIOR: Securing Autonomous Vehicles with Robust
Physical Invariants. In Proc. of the USENIX Security Symposium. Boston, MA.
https://www.usenix.org/conference/usenixsecurity20/presentation/quinonez
[46] R. Taormina. 2018. AutoEncoders for Event Detection (AEED): a Keras-based
class for anomaly detection in water sensor networks. (2018). https://github.
com/rtaormina/aeed, Last accessed on: 2020-06-15.