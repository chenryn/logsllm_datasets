0.890
0.839
0.838
0.838
0.838
0.839
0.839
0.839
0.840
0.842
0.842
0.841
0.842
Pruned Test
0.798
0.797
0.796
0.797
0.782
0.782
0.782
0.782
0.782
0.782
0.781
0.782
Pruned Target
0.508
0.503
0.497
0.497
0.524
0.524
0.524
0.524
0.419
0.419
0.419
0.419
FT Test
0.577
0.595
0.563
0.736
0.542
0.532
0.634
0.685
0.464
0.622
0.690
0.704
FT Target
0.508
0.578
0.665
0.789
0.452
0.448
0.510
0.657
0.161
0.832
0.684
0.690
FT Target ğœ
0.356
0.286
0.376
0.161
0.106
0.083
0.115
0.205
0.000
0.035
0.299
0.219
Table 12: Average accuracy over 5 runs of the Fine-Pruning defense, VGG-LL UTKFace, for three subpopulations impacted
by the attack by 22.7%, 27.1%, and 18%. The columns represent original model target subpopulation accuracy, and the test and
target accuracy after the attack, after the pruning phase, and after the fine-tuning phase of the defense. We also report the
standard deviation for Target accuracy after fine-tuning.
Algorithm 4 TRIM defense against availability attacks. Iteratively
identifies poisoning by high loss values, and trains without those
points.
Input: Training data ğ· of ğ‘› examples, loss function â„“, attack count
ğ‘š, training algorithm ğ´, maximum iteration count ğ‘‡
IND = [ğ‘›]; IND_PREV = []; Iterations = 0
while (IND â‰  IND_PREV âˆ§ Iterations < ğ‘‡) do
IND_PREV = IND
Iterations = Iterations + 1
ğ‘“ = ğ´(ğ·[IND])
Losses = [â„“(ğ‘¥ğ‘–, ğ‘¦ğ‘–)|(ğ‘¥ğ‘–, ğ‘¦ğ‘–) âˆˆ ğ·]
IND = ArgSort(Losses)[: ğ‘› âˆ’ ğ‘š]
âŠ² Train model on good indices
âŠ² Compute all losses
âŠ² Lowest ğ‘› âˆ’ ğ‘š indices
by loss
return ğ‘“
causing a bias towards the poisoned class for that subpopulation.
This bias causes TRIM to identify the real data as the attack, rather
than the poisoning, exacerbating the poisoning attack even further
in Figure 2c. Similar results appear for the SEVER defense [17].
These results demonstrate one failure mode of existing availability
defenses at protecting against subpopulation attacks.
In Table 12, we present the results of the fine pruning defense.
A large, clean holdout set is required for fine pruning, making it
difficult to apply in our threat model. Tables 13 and 14 demonstrate
that the certified defense still permits high target damages for many
subpopulations.
B SUBPOPULATION TRANSFERABILITY
In Table 15, we present results attacking CIFAR-10 + VGG-FT with
ClusterMatch with subpopulations generated with ResNet-50
embeddings. This allows us to test whether knowledge of the
learnerâ€™s model architecture is required to run ClusterMatch
attacks. We find that the attack is less successful when using trans-
ferred embeddings, but is still effective â€” most subpopulations
Worst-1
Worst-5
Worst-10
Before After Before After Before After
0.12
0.12
0.19
-0.07
0.16
0.23
0.36
0.47
0.47
0.19
0.24
0.28
0.01
0.06
0.15
0.0
0.04
0.13
ğ›¼
0.5
1
2
Table 13: Target damage before and after applying the cer-
tified defense on UTKFace + VGG-LL. Results are for the
worst performing subpopulations for the defense. Results
after the defense measure the uncertified target damage â€”
the certified target damage will be strictly larger. The predic-
tions made by the model are not stable enough under label
flipping to accommodate the defense. Similarly to the unde-
fended model, not all subpopulations are heavily impacted:
for each poisoning rate, roughly 50% of subpopulations see
no target damage under attack.
Worst-1
Worst-5
Worst-10
Before After Before After Before After
0.15
0.03
0.06
0.24
0.5
0.16
-0.04
0.00
0.16
-0.01
0.01
0.15
0.12
0.18
0.32
0.09
0.15
0.26
ğ›¼
0.5
1
2
Table 14: Target damage before and after applying the certi-
fied defense on UTKFace + VGG-LL. Results are for the worst
performing subpopulations for the original model. Results
after the defense measure the uncertified target damage â€”
the certified target damage will be strictly larger. The predic-
tions made by the model are not stable enough under label
flipping to accommodate the defense.
exhibit 15-16% target damage, while causing no more than 2% col-
lateral damage. While knowledge of architecture is helpful for gen-
erating ClusterMatch subpopulations, it is not necessary.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3120(a) Unpoisoned model
(b) Poisoned model
(c) Poisoned + TRIM
Figure 2: Illustration of how standard defenses (such as TRIM [29]/ILTM [61]) can fail against subpopulation attacks. If the
subpopulation attack can cause a bias towards the attack class, it will be exacerbated by the defense.
(a)
(d)
(b)
(e)
(c)
(f)
Figure 3: Six example subpopulations generated with ClusterMatch on UTKFace. While they donâ€™t necessarily correspond
perfectly to human-interpretable subpopulations, patterns do show up that may be aligned with an adversaryâ€™s objective.
C EXAMPLE ClusterMatch
SUBPOPULATIONS
We present some example subpopulations in Figure 3, generated
with ClusterMatch on UTKFace. The subpopulations are not per-
fectly human-interpretable, but do have some consistent trends. For
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3121Selection
Koda [30]
Random
ClusterMatch
Worst-1 Worst-5
Iterations (successful)
0.53
1.0
1.0
N/A
0.24
0.6
2000
380
250
Table 16: Effectiveness of ClusterMatch to aid influence-
based poisoning attack generation. Compared to randomly
generated baseline subpopulations, subpopulations gener-
ated with ClusterMatch are easier to poison (by worst-5
attack effectiveness), and take fewer iterations to attack (the
number of iterations required to launch a successful attacks,
when the attack is eventually successful).
Data + Model
Worst
Target Damage
Size
ğ›¼ = 0.5
0.156
0.159
0.165
ğ›¼ = 1
0.155
0.157
0.161
ğ›¼ = 2
0.159
0.161
0.166
10
5
1
CIFAR-10 + VGG-FT
111.5
119.3
157.3
Table 15: Target damage for CIFAR-10 large models attacked
with ClusterMatch clusters and label flipping generated
with embeddings transferred from a ResNet-50 model.
example, in Figure 3a, the subpopulation is populated primarily by
older white men, with a few exceptions. Figure 3b has mostly older
white women, and Figure 3c is mostly bearded men or men with
darker skin color (and potentially some white men in environments
with darker lighting).
D INFLUENCE FUNCTIONS [30] RESULTS
FOR SECTION 6
Influence functions were proposed for a targeted poisoning attack
with a single target point [30], but Figure 5 from [30] also shows
that influence functions can attack multiple test points at once. To
run this attack, the authors manually collect a set of 30 images of
one authorâ€™s dog, and show that a misclassified image of a dog in
training can be used to attack this set of dogs. To compare directly
with this experiment, we will show that using ClusterMatch to
generate this set of target examples makes the attack easier than
arbitrarily selecting a set of examples by hand. To show this, we start
with the images of dogs from the test set of the Kaggle Cats and Dogs
dataset2. We then select a set of targets of the same size (30 points),
either by randomly sampling (to simulate the arbitrary selection
procedure used by [30]) or selecting 30 points from a subpopulation
generated with ClusterMatch. We generate 10 sets of targets for
both strategies, and attack all target sets with a single data point (we
use only 10 sets of targets due to the high computational cost of the
influence optimization procedure). We run influence optimization
for 2000 iterations, to maintain consistency with [30]. For each of
these target sets, we measure both the effectiveness of the attack
and the number of optimization iterations it takes to completely
misclassify the target points (if the attack is successful).
The results of this experiment are presented in Table 16. While
multiple target sets are resistant to attack with both random se-
lection and ClusterMatch, ClusterMatch targets are easier to
attack in the worst case (top-5 attack success is an average of 0.6,
compared to 0.24 for random selection), and take fewer iterations to
completely compromise when they are vulnerable (250 iterations,
compared to 380 for random selection, and 2000 in the original pa-
per). This iteration reduction corresponds to reducing the running
time of a successful attack from roughly 4 hours in the original pa-
per to 30 minutes for ClusterMatch (and roughly 45 minutes for
random selection). Interestingly, the test points we drew from for
this experiment were easier to attack than those used in [30], likely
due to a disparity in the distribution of dogs from the ImageNet
dataset and Kaggle Cats and Dogs.
2https://www.kaggle.com/chetankv/dogs-cats-images
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3122