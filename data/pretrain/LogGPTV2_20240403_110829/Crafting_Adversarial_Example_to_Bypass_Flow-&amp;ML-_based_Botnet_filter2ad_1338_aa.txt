title:Crafting Adversarial Example to Bypass Flow-&amp;ML- based Botnet
Detector via RL
author:Junnan Wang and
Qixu Liu and
Di Wu and
Ying Dong and
Xiang Cui
Crafting Adversarial Example to Bypass Flow-&ML- based
Botnet Detector via RL
Junnan Wang
Qixu Liu∗
Institute of Information Engineering,
Chinese Academy of Sciences
Institute of Information Engineering,
Chinese Academy of Sciences
Beijing, China
Beijing, China
School of Cyber Security, University
of Chinese Academy of Sciences
School of Cyber Security, University
of Chinese Academy of Sciences
Beijing, China
PI:EMAIL
Beijing, China
PI:EMAIL
Di Wu
Huawei Technologies Co., Ltd.
Shen Zhen, China
PI:EMAIL
Ying Dong
Xiang Cui
Beijing Venus Information Security
Technology Incorporated Company
Cyberspace Institute of Advanced
Technology, Guangzhou University
Beijing, China
PI:EMAIL
Guang Zhou, China
PI:EMAIL
ABSTRACT
Machine learning(ML)-based botnet detection methods have be-
come mainstream in corporate practice. However, researchers have
found that ML models are vulnerable to adversarial attacks, which
can mislead the models by adding subtle perturbations to the sample.
Due to the complexity of traffic samples and the special constraints
that to keep malicious functions, no substantial research of adver-
sarial ML has been conducted in the botnet detection field, where
the evasion attacks caused by carefully crafted adversarial exam-
ples may directly make ML-based detectors unavailable and cause
significant property damage. In this paper, we propose a reinforce-
ment learning(RL) method for bypassing ML-based botnet detectors.
Specifically, we train an RL agent as a functionality-preserving bot-
net flow modifier through a series of interactions with the detector
in a black-box scenario. This enables the attacker to evade detection
without modifying the botnet source code or affecting the botnet
utility. Experiments on 14 botnet families prove that our method
has considerable evasion performance and time performance.
CCS CONCEPTS
• Security and privacy → Intrusion/anomaly detection and
malware mitigation; • Computing methodologies → Artifi-
cial intelligence.
∗Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
RAID ’21, October 6–8, 2021, San Sebastian, Spain
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-9058-3/21/10...$15.00
https://doi.org/10.1145/3471621.3471841
KEYWORDS
Bypass Botnet Detector, Adversarial Machine Learning, Reinforce-
ment Learning
ACM Reference Format:
Junnan Wang, Qixu Liu, Di Wu, Ying Dong, and Xiang Cui. 2021. Crafting
Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL. In
24th International Symposium on Research in Attacks, Intrusions and Defenses
(RAID ’21), October 6–8, 2021, San Sebastian, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3471621.3471841
1 INTRODUCTION
Machine learning(ML) has greatly promoted the development of bot-
net detection technology. Unlike signature-based detection meth-
ods, machine learning-based anomaly detection is able to efficiently
and accurately identify malware-generated traffic when certain be-
havior patterns are recognized. In particular, the significant spatial-
temporal similarity of botnets makes them easily detected by ML-
based models. In addition, ML-based methods can identify unknown
botnet families and are more suitable for dealing with large-scale
network traffic data.
Unsurprisingly, attackers have started looking for methods and
techniques that would allow them to overcome the progress in de-
tection systems and bypass behavior-based detection. For example,
attackers can frequently change the IP address corresponding to the
domain name of the command and control (C&C) server to evade
IP blacklist detection; use application layer protocols (HTTP, DNS)
or external web services (Twitter, Facebook) for C&C communi-
cation to bypass the firewall; and encrypt C&C communications
to avoid payload-based botnet detector. However, these evasion
methods cannot bypass the widely used Flow-& ML-based botnet
detectors, which distinguish malicious relying on the statistical
characteristics of flows. Moreover, these methods require extensive
or complicated direct source modifications, thereby placing high
demands on attackers [38].
One direct way to bypass the ML-based botnet detector is using
ML’s vulnerability to attack ML detection model. Szegedy et al. [40]
first discovered that well-performing ML models are susceptible to
193RAID ’21, October 6–8, 2021, San Sebastian, Spain
Wang and Liu, et al.
adversarial attacks in the form of adding some tiny perturbations
to inputs that fool a model into producing incorrect outputs. Many
recent works have also proposed ingenious methods (L-BFGS [40],
FGSM [18], Deepfool [28] et al.) to craft adversarial examples, but
these methods are difficult to directly apply for crafting botnet
adversarial samples.
In the context of botnet detection, the constraints on the pertur-
bation that added to the adversarial example is no longer imper-
ceptible to humans, but that it cannot affect the original malicious
intention of botnet traffic. The complexity and specific format of
botnet traffic data determine the need to develop new methods for
constructing botnet adversarial samples to bypass ML-based botnet
detectors.
In this paper, a reinforcement learning(RL) -based method to
bypass an ML-based flow-level botnet detector is presented. We
attempt to mislead the ML-based detector by adding some perturba-
tions to the botnet flow. Specifically, we model the modification of
botnet flow as a sequential decision problem, let the RL agent learns
the optimal modification strategy during a series of interactions
with the botnet detector. To ensure the preservation of functionality,
we design an action space containing 14 incremental operations,
each of which only adds a carefully crafted packet to the original
flow in an attempt to change some of the flow-level characteristics.
The detector deems these characteristics to be discriminating, but
this may not be a causal indictor of benign traffic. Moreover, adding
packets is an incremental operation at the transport layer, while
malicious functions are generally encapsulated in the application
layer. Therefore, it can be guaranteed that the original malicious
intention will not be destroyed.
The advantages of our attack method include that (1) it is a
black box attack, which is more in line with real attack scenarios
than other methods; (2) it is general and can be used regardless of
whether the detector’s loss function is differentiable; (3) it is plug
and play, the RL agent can exist as a proxy, the attack has a low
evasion cost and is suitable for any botnet family.
Through extensive experiments, we prove that the current ML-
based botnet detector is vulnerable. Attackers can avoid detection
by only adding a few packets to the botnet flow at a relatively small
cost and without any prior knowledge.
The contributions of this paper can be summarized as follows:
• We propose a general black box attack framework for ML-
based botnet detectors. We assume that the attacker can
access the detector and obtain the input binary discrimina-
tion result (malware/benign) but does not have any prior
knowledge of the algorithm and feature space used by the
detector. To the best of our knowledge, this work is the first
study about black-box adversarial attacks in botnet evasion
field.
• We design a series of universal action spaces and encapsulate
them in the RL framework. On the one hand, all actions are
incremental operations, thereby ensuring that the transmis-
sion of malicious information and functions is not affected.
On the other hand, the actions are universal and well en-
capsulated, enabling attackers to escape detection without
complex modifications to the botnet malware.
• We demonstrate how to train and deploy our system to avoid
ML detection on a carefully constructed botnet flow dataset
and comprehensively evaluate the evasion performance, time
cost, and universality of the framework.
This paper is divided into 6 sections as follows: Section 2 intro-
duces related works. Section 3 describes the system framework.
Section 4 shows how we set up our experiment. Section 5 discusses
the experimental results and conclusions. At the end of the paper,
we summarize and prospect the work of this paper.
2 RELATED WORK
2.1 Adversarial machine learning
In the literature, there are many works focusing on adversarial
attacks. Researchers have proposed various advanced attack meth-
ods based on how much detector information is available. Such
knowledge of the detector can include [9]:
• the training set or part of it;
• the feature space of the ML algorithm;
• the type of ML algorithm (SVM, LR, DecisionTree, CNN, etc.)
• the trained classifier (details of the model, such as its archi-
• the feedback from the detector (score-based feedback con-
sisting of probability- or binary-based feedback for the final
label)
tecture and hyperparameters);
In this paper, we group recent adversarial attack methods into
three attack scenarios based on different levels of adversary knowl-
edge about the attacked system: perfect knowledge (PK), limited
knowledge (LK) and zero knowledge (ZK).
Perfect knowledge. White box attack. In this scenario, the attacker
has complete information about the detector. The attacker’s goal is
to minimize the sample misclassification function.
[40] leveraged L-BFGS to solve the optimization problem of
finding the minimum amount of required perturbations. C&W at-
tack [13] designed a new loss function with a small value in the
anti-sample but a larger value in the clean sample so the adver-
sarial example could be searched by minimizing the loss function.
FSGM [18] assumes that the loss function in the neighborhood of
the clean sample is linear.
Deepfool [28] was the first method to use the L2 norm to limit
the disturbance size to obtain the minimum perturbation. Universal
perturbation [27] extends DeepFool to craft image-agnostic and
universal perturbations.
Limited knowledge. Gray box attack. The attacker only has lim-
ited knowledge about the detector and cannot use gradient-based
approaches. However, by knowing the type or feature space of the
model, attackers can mislead the detector by finding a set of fea-
tures that not discriminating enough via the structure features of
the model, the feature space or the feedback score sequence.
Apruzzese et al. [6] used a mixed integer linear program solver
to construct an optimal adversarial example for evading flow and
random forest based botnet detectors.
Papernot et al. [31] trained a substitute model after performing
Jacobian_based dataset augmentation to accurately simulate the
decision boundary of the detector.
194Crafting Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL
RAID ’21, October 6–8, 2021, San Sebastian, Spain
Table 1: Taxonomy of the latest academic studies on adversarial attack methods.
Object
image
image/malware
Windows PE
image
image/malware
Disadvantage
Unable to handle non-differentiable loss functions
PK/LK/ZK
PK
ZK
ZK
ZK
Method
L-BFGS [40]
FSGM [18] [24]
SLEIPNIR [4]
DeepFool [28]
JBSM [32] [19]
C&W attack [13]
ATN [7]
Adv_MalConv [22]
Evade-RF [6]
MalGAN [21]
EvadeHC [14]
image
image
malware
image
image
botnet flow
malware
pdf malware
Universal perturbation [27]
LK-score feedback Train substitute model [31]
LK-model type
Targeting against a particular type of ML model
Actually belong to LK scenario, the attacker needs
to know the complete feature space
Need help with verifying information
Need the most iterations to converge
Boundary attack [11]
PK: Perfect knowledge; LK: Limited knowledge; ZK: Zero knowledge.
image
Zero knowledge. . Black box attack. Assume the attacker has
zero knowledge of the model except for the binary decision of the
detector. The attacker can only evade the detector through trial and
error or by crafting adversarial examples against a joint classifier.
The basic idea is that if the adversarial examples can bypass each
model in the collection, then it may bypass every single detector.
MalGAN [21] introduced GAN to generate PE malware to by-
pass a black-box detector. The GAN discriminator is a substitute
detector model built by the attacker, while the generator is utilized
to generate adversarial malware samples.
Boundary attack [11] starts from a large adversarial perturbation
and then seeks to reduce it while remaining adversarial by perform-
ing a random walk along the boundary between the adversarial
and the non-adversarial regions.
From Table 1, we can see that most of the existing white box
attacks can only deal with differentiable loss functions, while gray
box attacks can only bypass specific types of detectors. A few of the
existing black box attack methods do not need any prior knowledge
at all; instead, these methods rely somewhat on external information
or model-related information. This challenge motivates us to build
a general black box adversarial attack method that can bypass any
ML-based botnet detector.
2.2 Botnet Evasion
While the botnet detection method is constantly improving, at-
tackers are also exploring techniques to avoid ML-based botnet
detection. Traditional botnet evasion techniques make C&C traffic
difficult to detect by encrypting traffic, hiding C&C information
in redundant fields of TCP/IP protocols, or using online-social-
networks(OSN) to construct covert channels [30] [44] [5] [1] [2].
However, these methods require complicated modifications to the
botnet architecture or source code, which places high capability
requirements on botnet controllers, and also has a certain impact
on the availability and market value of the botnet.
With the widespread application of machine learning in botnet
detection field [23] [10] [16] [20] [34] [42] [41], security researchers
have gradually sought adversarial machine learning(AML) methods
to bypass botnet traffic detectors. That is, constructing botnet traffic
confrontation samples by adding perturbations to the botnet traffic
samples, thereby bypassing the ML-based botnet traffic detector.
These methods can be achieved by applying traffic proxies instead
of modifying the botnet source code, which seems to be a more
attractive and lower-cost solution.
Furthermore, AML-based evasion methods can be divided into
two categories according to different output.
Feature space attack. refers to methods that can only generate
adversarial traffic feature vectors. However, considering that the
process of mapping traffic samples to feature vectors is irreversible,
such an attack cannot cause actual security threats and can only
use to prove the vulnerability of the ML-based detector [25]. Evade-
RF [6] attempts to make botnet traffic different from the malicious
flows contained in the detector training dataset by slightly altering
flow-level statistical characteristics, such as flow duration, as well
as the numbers of exchanged bytes and exchanged packets. The
author trained a random forest as a botnet detector on the CTU
dataset and evaluated the performance of the generated adversarial
samples. [29] discussed the learning and evasion consequences of
the gap between the generated and crafted adversarial samples, and
they achieved a white box adversarial attack against an encrypted
C&C malware traffic detector. However, these works made the
unrealistic assumption that the attacker has perfect knowledge of
the classifier, and this is obviously inconsistent with real attack
scenarios.
End-to-end attack. refers to an attack method that can generate
real traffic as output. This attack method is more suitable for real
network attack scenarios, because the output can be directly used
by the attacker. However, some stricter constraints will be intro-
duced when applying AML to generate malicious traffic samples:
195RAID ’21, October 6–8, 2021, San Sebastian, Spain
Wang and Liu, et al.
Figure 1: Taxonomy of the Botnet Evasion
(1) Keep the malicious functionality intact in the perturbed sample.
(2) Cannot destroy the file structure and network protocol structure
of pcap files. This leads to many AML methods in the image field
that cannot be directly applied, because they perturb image pixels
indiscriminately.
[35] proposed a method of using a generative adversarial net-
work (GAN) to mimic the behavior of Facebook chat network traffic
to bypass a self-adapting Stratospheric IPS. Their article attempted
to adapt the behavior of its communication channel to mimic the
behavior of Facebook chat network traffic according to the char-
acteristics (total byte size, duration, and time delta between the
current flow and the next flow) received from a GAN. However, the
article did not clearly explain how to modify the source code to
achieve the changes indicated by the GAN, and the IPS mentioned
in the article is not a flow detector but rather a 3-tuple (victim IP,
server IP, server port) detector.
There are also some works [36] that have use the generation
ability of the GAN to generate network traffic that looks as real as
possible. The purpose of these works is to improve the quality of the
dataset for training malicious traffic detectors to deal with the data
imbalance problem, yet it cannot be guaranteed that the generated
network traffic actually occurs in the data link. Furthermore, these
methods do not consider whether the crafted traffic can carry out
the malicious function of the botnet. This leads to a completely
different scenario from that of our work, in which we aim to bypass
botnet traffic detection by constructing botnet adversarial samples.
3 THREAT MODEL AND SYSTEM
FRAMEWORK
We use the Markov decision process (MDP) to model the problem
of crafting adversarial botnet flow samples and implement general
black box attacks via RL algorithms. This section first explains
the threat model of our attack method, then introduces the over-
all framework of the proposed system, and finally introduces the
important system components in detail.
3.1 Threat model
We describe our threat model according to the method proposed
in [9].
Adversary’s goal. The attacker’s goal is to be able to mislead the
detector by generating adversarial samples, thereby increasing its
invisibility. From the perspective of the CIA triad (confidentiality,
integrity and availability), attackers try to reduce the availability of
network intrusion detection systems by camouflaging botnet flow.
Adversary’s knowledge. The attacker understands that the target
network may be protected by a flow-level network intrusion de-