# Title: Crafting Adversarial Examples to Bypass Flow- and ML-Based Botnet Detectors via Reinforcement Learning

## Authors:
- Junnan Wang
- Qixu Liu
- Di Wu
- Ying Dong
- Xiang Cui

### Affiliations:
- **Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China**
  - Junnan Wang
  - Qixu Liu
  - Ying Dong
  - Xiang Cui
  - School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
- **Huawei Technologies Co., Ltd., Shen Zhen, China**
  - Di Wu
- **Beijing Venus Information Security Technology Incorporated Company, Beijing, China**
  - Ying Dong
- **Cyberspace Institute of Advanced Technology, Guangzhou University, Guang Zhou, China**
  - Xiang Cui

### Abstract
Machine learning (ML)-based botnet detection methods have become the norm in corporate cybersecurity. However, these models are vulnerable to adversarial attacks, which can mislead the models by introducing subtle perturbations to the input data. Due to the complexity of traffic samples and the need to maintain malicious functionality, there has been limited research on adversarial ML in the context of botnet detection. Evasion attacks using carefully crafted adversarial examples can render ML-based detectors ineffective, leading to significant security breaches. In this paper, we propose a reinforcement learning (RL) method to bypass ML-based botnet detectors. Specifically, we train an RL agent to modify botnet traffic flows in a way that preserves their malicious functionality while evading detection. This approach allows attackers to bypass detection without altering the botnet's source code or affecting its utility. Experiments on 14 botnet families demonstrate the effectiveness and efficiency of our method.

### CCS Concepts
- **Security and Privacy** → Intrusion/anomaly detection and malware mitigation
- **Computing Methodologies** → Artificial intelligence

### Keywords
- Bypass Botnet Detector
- Adversarial Machine Learning
- Reinforcement Learning

### ACM Reference Format
Junnan Wang, Qixu Liu, Di Wu, Ying Dong, and Xiang Cui. 2021. Crafting Adversarial Examples to Bypass Flow- and ML-Based Botnet Detectors via Reinforcement Learning. In 24th International Symposium on Research in Attacks, Intrusions and Defenses (RAID '21), October 6–8, 2021, San Sebastian, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3471621.3471841

## 1. Introduction
Machine learning (ML) has significantly advanced botnet detection technology. Unlike signature-based methods, ML-based anomaly detection can efficiently and accurately identify malware-generated traffic by recognizing specific behavior patterns. The spatial-temporal similarity of botnets makes them particularly detectable by ML models, which can also identify unknown botnet families and handle large-scale network traffic data.

Attackers have developed various techniques to evade detection, such as frequently changing IP addresses, using application layer protocols, and encrypting C&C communications. However, these methods are not effective against flow- and ML-based botnet detectors, which rely on statistical characteristics of network flows. These evasion techniques often require extensive and complex modifications to the botnet source code, making them impractical for many attackers.

One promising approach to bypass ML-based botnet detectors is to exploit their vulnerability to adversarial attacks. Szegedy et al. [40] first demonstrated that well-performing ML models can be misled by adding tiny perturbations to inputs. While several methods (e.g., L-BFGS [40], FGSM [18], Deepfool [28]) have been proposed to craft adversarial examples, they are difficult to apply directly to botnet traffic due to the constraints of maintaining malicious functionality and the complexity of the data.

In this paper, we present a reinforcement learning (RL)-based method to bypass ML-based flow-level botnet detectors. We model the modification of botnet traffic as a sequential decision problem, allowing the RL agent to learn the optimal strategy through interactions with the detector. To ensure the preservation of functionality, we design an action space consisting of 14 incremental operations, each of which adds a carefully crafted packet to the original flow. This approach ensures that the original malicious intent is not compromised.

The key advantages of our method include:
1. It is a black-box attack, more realistic than other methods.
2. It is general and can be applied regardless of the detector's loss function.
3. It is plug-and-play, with low evasion costs and applicability to any botnet family.

Through extensive experiments, we demonstrate that current ML-based botnet detectors are vulnerable to our method. Attackers can evade detection by adding a few packets to the botnet flow at a relatively small cost and without prior knowledge.

The contributions of this paper are:
- A general black-box attack framework for ML-based botnet detectors.
- A series of universal action spaces encapsulated in the RL framework, ensuring the transmission of malicious information and functions is not affected.
- A comprehensive evaluation of the evasion performance, time cost, and universality of the framework.

## 2. Related Work

### 2.1 Adversarial Machine Learning
Adversarial attacks have been extensively studied, with various methods proposed based on the level of knowledge about the detector. We categorize these methods into three scenarios: perfect knowledge (PK), limited knowledge (LK), and zero knowledge (ZK).

#### Perfect Knowledge (White Box Attack)
In this scenario, the attacker has complete information about the detector. Methods like L-BFGS [40], C&W attack [13], and FGSM [18] aim to find the minimum perturbations required to mislead the model.

#### Limited Knowledge (Gray Box Attack)
The attacker has limited knowledge about the detector. Techniques like SLEIPNIR [4], Evade-RF [6], and training substitute models [31] are used to mislead the detector by exploiting known features or feedback scores.

#### Zero Knowledge (Black Box Attack)
The attacker has no prior knowledge of the model except for the binary decision. Methods like MalGAN [21] and Boundary attack [11] generate adversarial examples through trial and error or by training a substitute model.

### 2.2 Botnet Evasion
Traditional botnet evasion techniques, such as encrypting traffic and hiding C&C information, are not effective against ML-based detectors. Recent research has explored adversarial machine learning (AML) methods to bypass these detectors. These methods can be divided into feature space attacks and end-to-end attacks.

- **Feature Space Attack**: Generates adversarial feature vectors but does not produce real traffic. Examples include Evade-RF [6] and white box attacks on encrypted C&C traffic [29].
- **End-to-End Attack**: Generates real traffic that can be used in actual attacks. Examples include GAN-based methods [35] and [36].

## 3. Threat Model and System Framework

### 3.1 Threat Model
We describe our threat model based on the CIA triad (confidentiality, integrity, and availability). The attacker aims to reduce the availability of network intrusion detection systems by camouflaging botnet traffic. The attacker understands that the target network may be protected by a flow-level network intrusion detection system.

### 3.2 System Framework
We use a Markov decision process (MDP) to model the problem of crafting adversarial botnet flow samples and implement general black-box attacks via RL algorithms. The overall framework includes:
- **Action Space**: 14 incremental operations that add carefully crafted packets to the original flow.
- **State Space**: The current state of the botnet flow, including its statistical characteristics.
- **Reward Function**: Based on the success of evading the detector and the preservation of malicious functionality.

### 3.3 System Components
- **RL Agent**: Learns the optimal strategy to modify botnet flows.
- **Detector**: The ML-based botnet detector that the RL agent interacts with.
- **Environment**: Simulates the network environment and provides feedback to the RL agent.

## 4. Experimental Setup
We conduct experiments on a dataset of 14 botnet families to evaluate the effectiveness of our method. The experimental setup includes:
- **Dataset**: A carefully constructed botnet flow dataset.
- **Evaluation Metrics**: Evasion performance, time cost, and universality of the framework.

## 5. Results and Discussion
Our experiments show that the proposed RL-based method can effectively bypass ML-based botnet detectors. The results demonstrate high evasion performance and low time cost, making it a practical and efficient solution for attackers.

## 6. Conclusion
This paper presents a reinforcement learning method to bypass ML-based botnet detectors. Our approach ensures the preservation of malicious functionality while evading detection. Extensive experiments validate the effectiveness and efficiency of our method, highlighting the need for more robust ML-based detection systems.

### Future Work
Future work will focus on developing more advanced RL algorithms and exploring countermeasures to protect against such adversarial attacks.

### Acknowledgments
We thank the reviewers for their valuable feedback. This work was supported by [funding sources].

### References
[References to be added here]

---

This revised version aims to provide a clear, coherent, and professional presentation of the research, with improved structure and readability.