8
no
no
no
1.479
2.310
2.746
1.862
2.591
2.983
1.501
2.304
2.738
1.988
2.638
2.996
F-BLEAU: ME
True ME
1.802
2.550
2.970
1.987
2.631
3.003
L.E.: leakage evidence test
B. Gowalla dataset
We compare F-BLEAU and leakiEst on the location privacy
mechanisms (section VII): Blahut-Arimoto, planar Geometric,
and planar Laplacian. The main interest is to verify whether
the advantage of F-BLEAU w.r.t. the frequentist approach,
which we observed for large output spaces, translates into an
advantage also w.r.t. leakiEst. For the ﬁrst two mechanisms
we also compare the ME estimates. For the Laplacian case
(continuous), we only use leakiEst’s leakage evidence test.
We run F-BLEAU and leakiEst on the datasets as in
section VII. Results in Table XIII show that, in the cases of
planar Geometric and Laplacian distributions, leakiEst does
not detect any leakage (the tool reports “Too small sample
size”); furthermore,
the ME estimates it provides for the
planar Geometric distribution are far from their true values. F-
BLEAU, however, is able to produce more reliable estimates.
The Blahut-Arimoto results are more interesting: because
of the small number of actual outputs, the ME estimates of
F-BLEAU and leakiEst perform equally well. However, even
in this case leakiEst’s leakage evidence test reports “Too small
sample size”. We think the reason is that leakiEst takes into
account the declared size of the object space rather then the
effective number of observed individual outputs; this problem
should be easy to ﬁx by inferring the output size from the
examples (this is the meaning of the “*” in Table XIII).
IX. CONCLUSION AND FUTURE WORK
We showed that the black-box leakage of a system, mea-
sured until now with classical statistics paradigms (frequentist
approach), can be effectively estimated via ML techniques.
We considered a set of such techniques based on the nearest
neighbor principle (i.e., close observations should be assigned
the same secret), and evaluated them thoroughly on synthetic
and real-world data. This allows to tackle problems that were
impractical until now; furthermore, it sets a new paradigm
in black-box security: thanks to an equivalence between ML
and black-box leakage estimation, many results from the ML
theory can be now imported into this practice (and vice versa).
(cid:25)(cid:21)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
Empirical evidence shows that the nearest neighbor tech-
niques we introduce excel whenever there is a notion of
metric they can exploit
in the output space: whereas for
unseen observations the frequentist approach needs to take
a guess, nearest neighbor methods can use the information
of neighboring observations. We also observe that whenever
the output distribution is irregular, they are equivalent to the
frequentist approach, but for maliciously crafted systems they
can be misled. Even in those cases, however, we remark that
asymptotically they are equivalent to the frequentist approach,
thanks to their universal consistency property.
We also indicated that, as a consequence of the No Free
Lunch (NFL) theorem in ML, no estimate can guarantee op-
timal convergence. We therefore proposed F-BLEAU, a com-
bination of frequentist and nearest neighbor rules, which runs
all these techniques on a system, and selects the estimate that
converges faster. We expect this work will inspire researchers
to explore new leakage estimators from the ML literature; in
particular, we showed that any “universally consistent” ML
rule can be used to estimate the leakage of a system. Future
work may focus on other rules from which one can obtain
universal consistency (e.g., Support Vector Machine (SVM)
and neural networks); we discuss this further in Appendix A.
A crucial advantage of the ML formulation, as opposed to
the classical approach, is that it gives immediate guarantees
for systems with a continuous output space. Future work may
extend this to systems with continuous secret space, which in
ML terms would be formalized as regression (as opposed to
the classiﬁcation setting we considered here).
A current limitation of our methods is that they do not
provide conﬁdence intervals. We leave this as an open prob-
lem. We remark, however,
that for continuous systems it
is not possible to provide conﬁdence intervals (or to prove
convergence rates) under our weak assumptions [23];
this
constraint applies to any leakage estimation method.
We reiterate, however, the great advantage of ML methods:
they allow tackling systems for which until now we could not
measure security, with a strongly reduced number of examples.
ACKNOWLEDGMENT
Giovanni Cherubin has been partially supported by an
EcoCloud grant. The work of Konstantinos Chatzikokolakis
and Catuscia Palamidessi was partially supported by the ANR
project REPAS.
We are very thankful to Marco Stronati, who was initially
involved in this project, and without whom the authors would
have not started working together. We are grateful to Tom
Chothia and Yusuke Kawamoto, for their help to understand
leakiEst. We also thank Fabrizio Biondi for useful discussion.
This work began as a research visit whose (secondary)
goal was for some of the authors and Marco to climb
in Fontainebleau, France. The trip to the magic land of
Fontainebleau never happened – although climbing has been a
fundamental aspect of the collaboration; the name F-BLEAU
is to honor this unfulﬁlled dream. We hope one day the group
will reunite, and ﬁnally climb there together.
REFERENCES
[1] D. Clark, S. Hunt, and P. Malacaria, “Quantitative information ﬂow,
relations and polymorphic types,” J. of Logic and Computation, vol. 18,
no. 2, pp. 181–199, 2005.
[2] G. Smith, “On the foundations of quantitative information ﬂow,” in
Proceedings of the 12th International Conference on Foundations of
Software Science and Computation Structures (FOSSACS 2009), ser.
LNCS, L. de Alfaro, Ed., vol. 5504. York, UK: Springer, 2009, pp.
288–302.
[3] M. S. Alvim, K. Chatzikokolakis, C. Palamidessi, and G. Smith,
“Measuring information leakage using generalized gain functions,”
in Proceedings of
the 25th IEEE Computer Security Foundations
Symposium (CSF), 2012, pp. 265–279.
[Online]. Available: http:
//hal.inria.fr/hal-00734044/en
[4] C. Braun, K. Chatzikokolakis, and C. Palamidessi, “Quantitative
notions of leakage for one-try attacks,” in Proceedings of the 25th
Conf. on Mathematical Foundations of Programming Semantics, ser.
Electronic Notes in Theoretical Computer Science, vol. 249. Elsevier
B.V., 2009, pp. 75–91. [Online]. Available: http://hal.archives-ouvertes.
fr/inria-00424852/en/
[5] T. Chothia, Y. Kawamoto, and C. Novakovic, “LeakWatch: Estimating
information leakage from java programs,” in Proc. of ESORICS 2014
Part II, 2014, pp. 219–236.
[6] ——, “A tool for estimating information leakage,” in International
Springer, 2013,
Conference on Computer Aided Veriﬁcation (CAV).
pp. 690–695.
[7] M. E. Andrés, N. E. Bordenabe, K. Chatzikokolakis, and C. Palamidessi,
“Geo-indistinguishability: Differential privacy for location-based sys-
tems,” in Proceedings of the 2013 ACM SIGSAC conference on Com-
puter & communications security. ACM, 2013, pp. 901–914.
[8] S. Oya, C. Troncoso, and F. Pérez-González, “Back to the drawing
board: Revisiting the design of optimal
location privacy-preserving
mechanisms,” in Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security, ser. CCS ’17. ACM,
2017, pp. 1959–1972. [Online]. Available: http://doi.acm.org/10.1145/
3133956.3134004
[9] T. Chothia and V. Smirnov, “A traceability attack against e-passports,” in
International Conference on Financial Cryptography and Data Security.
Springer, 2010, pp. 20–34.
[10] D. H. Wolpert, “The lack of a priori distinctions between learning
algorithms,” Neural computation, vol. 8, no. 7, pp. 1341–1390, 1996.
[11] M. Fredrikson, S. Jha, and T. Ristenpart, “Model
inversion attacks
that exploit conﬁdence information and basic countermeasures,” in
Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2015, pp. 1322–1333.
[12] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership
inference attacks against machine learning models,” in Security and
Privacy (SP), 2017 IEEE Symposium on.
IEEE, 2017, pp. 3–18.
[13] K. Chatzikokolakis, T. Chothia, and A. Guha, “Statistical measurement
of information leakage,” Tools and Algorithms for the Construction and
Analysis of Systems, pp. 390–404, 2010.
[14] M. Boreale and M. Paolini, “On formally bounding information leakage
by statistical estimation,” in International Conference on Information
Security. Springer, 2014, pp. 216–236.
[15] T. Chothia and A. Guha, “A statistical test for information leaks using
continuous mutual
the 24th IEEE
Computer Security Foundations Symposium, CSF 2011, Cernay-la-Ville,
France, 27-29 June, 2011.
IEEE Computer Society, 2011, pp. 177–190.
[Online]. Available: https://doi.org/10.1109/CSF.2011.19
information,” in Proceedings of
[16] T. Chothia, Y. Kawamoto, C. Novakovic, and D. Parker, “Probabilistic
point-to-point information leakage,” in Computer Security Foundations
Symposium (CSF), 2013 IEEE 26th.
IEEE, 2013, pp. 193–205.
[17] G. Cherubin, “Bayes, not naïve: Security bounds on website ﬁngerprint-
ing defenses,” Proceedings on Privacy Enhancing Technologies, vol.
2017, no. 4, pp. 215–231, 2017.
[18] N. Santhi and A. Vardy, “On an improvement over Rényi’s equivocation
bound,” 2006, presented at the 44-th Annual Allerton Conference on
Communication, Control, and Computing, September 2006. Available
at http://arxiv.org/abs/cs/0608087.
[19] I. Belghazi, S. Rajeswar, A. Baratin, R. D. Hjelm, and A. Courville,
preprint
estimation,”
neural
arXiv
“Mine: mutual
arXiv:1801.04062, 2018.
information
(cid:25)(cid:21)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
[20] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and
P. Abbeel, “Infogan: Interpretable representation learning by information
maximizing generative adversarial nets,” in Advances in neural informa-
tion processing systems, 2016, pp. 2172–2180.
[21] J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan, “Learning to
explain: An information-theoretic perspective on model interpretation,”
arXiv preprint arXiv:1802.07814, 2018.
[22] V. Vapnik, The nature of statistical learning theory. Springer science
& business media, 2013.
[23] L. Devroye, L. Györﬁ, and G. Lugosi, A probabilistic theory of pattern
recognition. Springer Science & Business Media, 2013, vol. 31.
[24] C. J. Stone, “Consistent nonparametric regression,” The annals of
statistics, pp. 595–620, 1977.
[25] C. Dwork, “Differential privacy,” in 33rd International Colloquium on
Automata, Languages and Programming (ICALP 2006), ser. Lecture
Notes in Computer Science, M. Bugliesi, B. Preneel, V. Sassone, and
I. Wegener, Eds., vol. 4052.
Springer, 2006, pp. 1–12. [Online].
Available: http://dx.doi.org/10.1007/11787006_1
[26] R. Shokri, G. Theodorakopoulos, C. Troncoso, J.-P. Hubaux, and J.-
Y. L. Boudec, “Protecting location privacy: optimal strategy against
localization attacks,” in Proceedings of the 19th ACM Conference on
Computer and Communications Security (CCS 2012), T. Yu, G. Danezis,
and V. D. Gligor, Eds. ACM, 2012, pp. 617–627.
[27] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
[28] “The gowalla dataset.” [Online]. Available: https://snap.stanford.edu/
John Wiley & Sons, Inc., 2006.
data/loc-gowalla.html
[29] E. Cho, S. A. Myers, and J. Leskovec, “Friendship and mobility:
User movement
in location-based social networks,” in Proceedings
of the 17th ACM SIGKDD International Conference on Knowledge
ser. KDD ’11. New York, NY,
Discovery and Data Mining,
USA: ACM, 2011, pp. 1082–1090.
[Online]. Available: http:
//doi.acm.org/10.1145/2020408.2020579
[30] Example: e-passport
traceability. School of Computer Science -
[Online]. Available: www.cs.bham.ac.uk/research/projects/
leakiEst.
infotools/leakiest/examples/epassports.php
[31] I. Steinwart, “Support vector machines are universally consistent,”
Journal of Complexity, vol. 18, no. 3, pp. 768–791, 2002.
[32] T. Glasmachers, “Universal consistency of multi-class support vector
classiﬁcation,” in Advances in Neural Information Processing Systems,
2010, pp. 739–747.
[33] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE
transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.
[34] K. Fukunaga and D. M. Hummels, “Bayes error estimation using parzen
and k-nn procedures,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, no. 5, pp. 634–643, 1987.
[35] M. Backes and B. Köpf, “Formally bounding the side-channel leakage
in unknown-message attacks,” in European Symposium on Research in
Computer Security. Springer, 2008, pp. 517–532.
[36] P. C. Kocher, “Timing attacks on implementations of difﬁe-hellman, rsa,
dss, and other systems,” in Annual International Cryptology Conference.
Springer, 1996, pp. 104–113.
[37] B. Köpf and D. Basin, “Timing-sensitive information ﬂow analysis
for synchronous systems,” in European Symposium on Research in
Computer Security. Springer, 2006, pp. 243–262.
APPENDIX A
ADDITIONAL TOOLS FROM ML
The ML literature can offer several more tools to black-
box security. We now enumerate additional UC rules, a lower
bound of the Bayes risk, and a general way of obtaining
estimates that converge from below.
The family of UC rules is fairly large. An overview of them
is by Devroye et al. [23], who, in addition to nearest neighbor
methods, report histogram rules and kinds of neural networks;
these are UC under requirements on their parameters. Stein-
wart proved that Support Vector Machine (SVM) is also UC
for some parameter choices, in the case |S| = 2 [31]; to the
best of our knowledge, attempts to construct an SVM that is
UC when |S| > 2 have failed so far (e.g., [32])
In applications with strict security requirements, a (pes-
simistic) lower bound of the Bayes risk may be desirable. From
a result by Cover and Hart one can derive a lower bound on
the Bayes risk based on the NN error, RN N [33]: as n → ∞:
(cid:11)
|S| − 1
|S|
1 −
(cid:12)
1 − |S|
|S| − 1
(cid:13)
RN N
≤ R
∗
.
(27)
This was used as the basis for measuring the black-box
leakage of website ﬁngerprinting defenses [17].
Finally, one may obtain estimators that converge to the
Bayes risk in expectation from below, for example, by esti-
mating the error of a kn-NN rule on its training set [17], [34].
APPENDIX B
DESCRIPTION OF THE SYNTHETIC SYSTEMS
A. Geometric system
Geometric systems are typical in differential privacy and
are obtained by adding negative exponential noise to the
result of a query. The reason is that the property of DP is
expressed in terms of a factor between the probability of a
reported answer, and that of its immediate neighbor. A similar
construction holds for the geometric mechanism implementing
geo-indistinguishability. In that case the noise is added to the
real location to report an obfuscated location. Here we give an
abstract deﬁnition of a geometric system, in terms of secrets
(e.g., result of a query / real location) and observables (e.g.,
reported answer / reported location).
Let S and O be sets of consecutive natural numbers, with
(cid:4) ∈ S are
the standard notion of distance. Two numbers s, s
called adjacent if s = s
Let ν be a real non-negative number and consider a function
g : S (cid:3)→ O. After adding negative exponential noise to the
output of g, the resulting geometric system is described by
the following channel matrix:
+ 1 or s
= s + 1.
(cid:4)
(cid:4)
Cs,o = P (o | s) = λ exp (−ν| g(s) − o |) ,
(28)
where λ is a normalizing factor. Note that the privacy level is
deﬁned by ν/Δg, where Δg is the sensitivity of g:
(g(s1) − g(s2)) ,
Δg = max
s1∼s2∈S
(29)
where s1 ∼ s2 means s1 and s2 are adjacent. Now let S =
{1, . . . , w}, O = {1, ..., w
/w.
We deﬁne
(cid:4)}, we select g to be g(s) = s·w
(cid:4)
(cid:3)
λ =
eν/(eν + 1)
(eν − 1)/(eν + 1)
if o = 1 or o = w
otherwise ,
(cid:4)
(30)
so to truncate the distribution at its boundaries.
This deﬁnition of Geometric system prohibits the case |S| >
|O|. To consider such case, we generate a repeated geometric
channel matrix, such that
C(cid:4)
s,o = Cs(cid:2),o
(31)
where Cs,o is the geometric channel matrix described above.
= s mod |O| ,
s
(cid:4)
(cid:25)(cid:21)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
B. Multimodal geometric system
We construct a multimodal distribution as the weighted sum
of two geometric distributions, shifted by some shift param-
eter. Let Cs,o be a geometric channel matrix. The respective
multimodal geometric channel, for shift parameter σ, is:
CM
s,o = w1Cs,o + w2Cs+2σ,o .
(32)
In experiments, we used σ = 5 and weights w1 = w2 = 0.5.
C. Spiky system
Consider an observation space constituted of q consecutive
integer numbers O = {0, ..., q − 1}, for some even positive
integer q, and secrets space |S| = 2. Assume that O is a
ring with the operations + and − deﬁned as the sum and the
difference modulo q, respectively, and consider the distance on
O deﬁned as: d(i, j) = |i−j|. (Note that (O, d) is a “circular”
structure, that is, d(q − 1, 0) = 1.) The Spiky system has
uniform prior, and channel matrix constructed as follows:
(cid:14)
(cid:15)
Cs,o =
2/q
0
0
2/q
2/q
0
. . .