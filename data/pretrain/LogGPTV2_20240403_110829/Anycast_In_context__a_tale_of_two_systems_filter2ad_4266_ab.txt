anycast site. For simplicity, we henceforth refer to these /24â€™s as
recursives, even though each /24 may contain several recursives.
We call this joined dataset of query volumes and user counts by
recursive DITLâˆ©CDN.
In an effort to make our results more reproducible, and as a point
of comparison, we also use public Internet population user count
data from APNIC to amortize root DNS queries [37] (i.e., instead
of using proprietary Microsoft data). APNIC obtains these AS user
population estimates by first gathering lists of IP addresses from
Googleâ€™s Ad delivery network, separated by country. APNIC con-
verts this distribution of IP addresses to a distribution of ASNs,
normalized by country Internet-user populations. We use the Team-
Cymru IP to ASN mapping to map IP addresses seen in the DITL
captures to their respective ASes [25] and accumulate queries by
ASN. We were able to map 99.4% of DITL IP addresses to an ASN,
representing 98.6% of DITL query volume. The assumption that
recursives are in the same AS as the users they serve is obviously
incorrect for public DNS services, but we do not make an effort to
correct for these cases. Overall, we believe Microsoft user counts
1We aggregate user IP addresses by recursive /24 before counting to ensure we do not
double-count users.
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Figure 1: Microsoftâ€™s CDN rings and user populations. Sites in smaller
rings are also in larger rings, and the legend indicates the number of
sites in that ring. We do not show some front-ends too close to each
other to improve readability. User populations are shown as circles,
with the radius of the circle proportional to the number of users in
that region, demonstrating that Microsoft has deployed front-ends
in areas of user concentration.
are more accurate, but APNIC data is more accessible to other
researchers and so provides a useful comparison.
2.2 Microsoftâ€™s CDN
We also analyze Microsoftâ€™s large anycast CDN that serves web
content to over a billion users from more than 100 sites. Traffic des-
tined for Microsoftâ€™s CDN enters its network at a point of presence
(PoP) and is routed to one of the anycast sites serving the content
(front-ends). Microsoft organizes its deployment into groups of
sites, called rings, that conform to varying degrees of regulatory
restrictions (e.g., ISO 9001, HIPAA), each with its own anycast ad-
dress. The rings have the property that a site in a smaller ring is
also in all larger rings. Other CDNs have to work with similar reg-
ulatory restrictions [2]. Hence, traffic from a user prefix destined
for Microsoftâ€™s CDN may end up at different front-ends (depending
on which ring the application uses), but often will ingress into the
network at the same PoP. Users are routed to rings via anycast and
fetch web content from a front-end via its anycast address. Users
are always routed to the largest allowed ring given the applicationâ€™s
regulatory restrictions (performance differences among rings are
not taken into account).
Microsoftâ€™s anycast rings provide different size anycast deploy-
ments for study. In Figure 1 we show Microsoftâ€™s front-ends and
user concentrations. Rings are named according to the number of
front-ends they contain, and front-ends are labeled according to
the smallest ring to which they belong (or else all front-ends would
be labelled as R110). We do not show some front-ends too close to
each other to improve readability. Circles are average user locations,
where the radius of the circle is proportional to the population of
users in that region. Figure 1 suggests that front-end locations tend
to be near large populations, providing at least one low latency
option to most users. Appendix F illustrates latency differences by
region.
User locations are aggregated by region, a geographic area used
internally by Microsoft to break the world into regions that generate
similar amounts of traffic and so contain similar numbers of users.
A region often corresponds to a large metropolitan area. We refer to
users at the âŸ¨region, ASâŸ© granularity, because users in the same
âŸ¨region, ASâŸ© location are often routed to the same front-ends and
so (generally) experience similar latency. There are 508 regions in
R28R47R74R95R110SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
total: 135 in Europe, 62 in Africa, 102 in Asia, 2 in Antarctica, 137
in North America, 41 in South America, and 29 in Oceania.
To study performance in Microsoftâ€™s CDN, we use two major data
sources: server-side logs and client-side measurements. Server-side
logs at front-ends collect information about user TCP connections,
including the user IP address and TCP handshake RTT. Using these
RTTs as latency measurements, we compute median latencies from
users in a âŸ¨region, ASâŸ© location to each front-end that serves
them.2 Microsoft determines the location and AS of users using
internal databases.
Client-side measurements come from a measurement system
operated by Microsoft [17]. Latency measurements are the time it
takes for Microsoft users to fetch a small image via HTTP.3 The
measurement system instructs clients using CDN services to issue
measurements to multiple rings, which enables us to remove biases
in latency patterns due to services hosted on different rings having
different client footprints (e.g., enterprise versus residential traffic).
Microsoft collects latencies of users populations, noting the location
and AS of the user. Since these measurements come directly from
end-users, we do not know which front-end the user hit. For both
client-side measurements and server-side logs, we collect statistics
for over a billion users across 15,000 âŸ¨region, ASâŸ© locations.
We also use RIPE Atlas to ping anycast rings, because we cannot
share absolute latency numbers. We calibrate these results versus
our (private) data measuring latency for CDN users. In total, we
collect 7,000 ping measurements to rings from 1,000 RIPE Atlas
probes in more than 500 ASes to augment CDN latency measure-
ments. (Probes were selected randomly, and measured three times
to each ring.)
3 ROUTES TO ROOT DNS ARE INFLATED
Earlier work has found query distance to the root DNS is often
significantly inflated [13, 23, 51, 67, 69]. Similar to this work, we
find that queries often travel to distant sites despite the presence
of a geographically closer site. We extend this understanding in a
number of ways. While previous work considered only subsets of
root DNS activity and focused on geographic inflation for recursives
rather than users, we calculate inflation for nearly all root letters,
and place inflation in the context of users, rather than recursive
resolvers. These contributions are significant for several reasons.
First, considering more root letters allows us to evaluate inflation
in different deployments, and with most letters we can evaluate
the root DNS system. Since a recursive makes queries to many root
letters, favoring those with low latency [60], system performance
and inflation can (and does) differ from component performance.
Second, we weight recursive resolvers by the number of users,
which allows us to see how users are affected by inflation. Finally,
we extend prior work by conducting an analysis of latency (as
opposed to geographic) inflation with large coverage.
Previous studies of anycast have separated inflation into two
types, unicast and anycast, in an attempt to tease out how much la-
tency anycast specifically adds to queries [13, 16, 51, 69]. For several
reasons, we choose to consider inflation relative to the deployment,
2We also looked at other percentiles (e.g., 95th) and found the qualitative results to be
similar.
3DNS resolution and TCP connection time are factored out.
rather than try to infer which inflation would exist in an equivalent
unicast deployment. First, coverage of measurement platforms used
to determine unicast inflation such as RIPE Atlas (vantage points
for anycast studies [51, 69]) is not representative [10]. Second, cal-
culating unicast inflation requires knowledge of the best unicast
alternative from every recursive seen in DITL to every root letter,
something that would be difficult to approximate with RIPE Atlas
because some letters do not publish their unicast addresses. Third,
we find it valuable to compare latency to a theoretical lower bound,
since user routes to the best unicast alternative may still be inflated.
We measure two types of inflation for the root DNS, by looking
at which sites recursive resolvers are directed to. DITL captures are
a rich source of data because they provide us with a global view
of which recursives access which locations (Â§2.1). Our inflation
analysis covers 224 countries/regions and 22,243 ASes (Atlas covers
about 3,700 ASes as of July 2021).
We calculate the first type of inflation â€“ geographic inflation
(Eq. (1)) â€“ over 10 of the 13 root letters, omitting G which does not
provide data, H which only had one site in 2018 (and so has zero in-
flation), and I, where anonymization prevents analysis. Geographic
inflation measures, at a high level, how users are routed to sites
compared to the closest front-end (i.e., efficiency)4.
We calculate the second type of inflation â€“ latency inflation
(Eq. (2)) â€“ over the root letters mentioned above by looking at
the subset of DNS queries that use TCP, using the handshake
to capture RTT [57]. Our latency inflation analysis further ex-
cludes D and L root, due to malformed DITL PCAPs. Latency infla-
tion uses measured latencies to determine inflation, so it reflects
constraints due to physical rights-of-way and connectivity, bad
routing, and peering choices. We calculate median latency over
each âŸ¨root, resolver /24, anycast siteâŸ© for which we have
at least 10 measurements, providing us latencies for resolvers rep-
resenting 40% of DITL query volume to these roots.
3.1 Methodology
To calculate geographic inflation, we first geolocate all recursives
in our DITLâˆ©CDN dataset using MaxMind [41], following prior
methodology which affirmed MaxMind to be suitably accurate for
geolocating recursive resolvers in order to assess inflation [51]. We
then compute geographic inflation (scaled by the speed of light in
fiber) for each recursive sending queries to root server ğ‘— as
ğ‘ (ğ‘…, ğ‘—ğ‘–)ğ‘‘(ğ‘…, ğ‘—ğ‘–)
âˆ’ min
ğ‘‘(ğ‘…, ğ‘—ğ‘˜))
ğ‘˜
(1)
where ğ‘ (ğ‘…, ğ‘—ğ‘–) is the number of queries to site ğ‘—ğ‘– by recursive ğ‘…,
ğ‘ (ğ‘…, ğ‘—) =ğ‘– ğ‘ (ğ‘…, ğ‘—ğ‘–) is the total number of queries to all sites ğ‘—ğ‘–
ğ‘ (ğ‘…, ğ‘—)
in root ğ‘— by recursive ğ‘…, ğ‘ ğ‘“ is the speed of light in fiber, the factor
of 2 accounts for the round trip latency, ğ‘‘(ğ‘…, ğ‘—ğ‘˜) is the distance
between the recursive resolver and site ğ‘—ğ‘˜, and both the summation
and minimization are over the global sites in this letter deployment
(see Section 2.1 for the distinction between local and global). We
only consider global sites, since we do not know which recursives
can reach local sites. For recursives which can reach a local site
4It would be interesting to measure topological inflation (extra distance traveled on the
Internet topology, beyond shortest-path propagation-delay), but it would be difficult
to do so using existing methods without sacrificing significant coverage.
(âˆ‘ï¸
ğ‘–
GI(ğ‘…, ğ‘—) =
2
ğ‘ ğ‘“
âˆ‘ï¸
ğ‘–
min
ğ‘˜
but instead reach a global site, Equation (1) (and Equation (2)) may
underestimate actual inflation.
GI(ğ‘…, ğ‘—) is an approximation of the inflation one would expect
to experience when executing a single query to root deployment ğ‘—
from recursive ğ‘…, averaged over all sites. The overall geographic
inflation of a recursive is then the empirical mean over all roots.
Even though queries from the same recursive /24 are usually routed
together, they may be routed to different sites due to load balancing
in intermediate ASes (see Appendix B.2 for measures of how often
this occurs), so we average geographic inflation across sites for
a recursive. Geographic inflation is useful to investigate since it
shows how our results compare with prior work, how many users
are being inflated, and it gives us a measure of "efficiency" (Â§7.2) .
We also calculate latency inflation, again considering recursive
querying patterns seen in DITL. We calculate latency inflation
LI(ğ‘…, ğ‘—) for users of recursive ğ‘… to root ğ‘— as
âˆ’ 3 Ã— 2
2ğ‘ ğ‘“
(2)
where ğ‘™(ğ‘…, ğ‘—ğ‘–) is the median latency of recursive ğ‘… towards root
site ğ‘—ğ‘– and the other variables are as in Equation (1). Prior work
notes that routes rarely achieve a latency of less than the great circle
distance between the endpoints divided by 2ğ‘ ğ‘“
[46], so we use 2ğ‘ ğ‘“
3
3
to lower bound the best latency recursives could achieve. Latency
inflation is a measure of potential performance improvement users
could see due to changes in routing or expanding the physical
Internet (e.g., laying fiber).
ğ‘ (ğ‘…, ğ‘—ğ‘–)ğ‘™(ğ‘…, ğ‘—ğ‘–)
LI(ğ‘…, ğ‘—) =
ğ‘‘(ğ‘…, ğ‘—ğ‘˜)
ğ‘ (ğ‘…, ğ‘—)
One limitation is that we do not account for the fact that the
source addresses of some queries in the DITL traces may be spoofed.
Spoofing is more likely to make our calculated inflation larger,
especially in cases where the spoofer is far away from the physical
interface it is spoofing (i.e., from our perspective, the route looks
inflated when actually the source address was spoofed). We do
not attempt to correct for these cases since it would be difficult to
distinguish between legitimately poor routing and spoofed traffic.
3.2 Results
Figure 2a demonstrates that the likelihood of a root DNS query
experiencing any geographic inflation (Eq. (1)) roughly grows with
deployment size (y-axis intercept), expanding on results in prior
work which presented an orthogonal, aggregated view [51]. The
All Roots line takes into account that each recursive spreads
its queries across different roots. It has the lowest y-intercept of
any line in Figure 2a, which implies that nearly every recursive
experiences some inflation to at least one root and that the set of
inflated recursives varies across roots. Hence, our analysis shows
that nearly every user will (on average) experience inflation when
querying the root DNS, and 10.8% of users are likely to be inflated
by more than 2,000 km (20 ms).
Figure 2b shows that queries to these roots experience frequent
latency inflation (Eq. (2)), with between 20% and 40% of users expe-
riencing greater than 100 ms of inflation (B root is a clear exception,
but only had 2 sites, so inflation is less meaningful). Latency infla-
tion starts at approximately zero, which follows from our choice
of â€œoptimalâ€ latency (Eq. (2)). Compared to geographic inflation,
latency inflation is particularly larger in the tail. For example, at
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
(a)
(b)
Figure 2: Inflation measured using geographic information (2a) and
TCP RTT estimates (2b). Generally, larger deployments are more
likely to inflate paths, and inflation in the roots is quite large. The
legends indicate the number of global sites per letter during the 2018
DITL.