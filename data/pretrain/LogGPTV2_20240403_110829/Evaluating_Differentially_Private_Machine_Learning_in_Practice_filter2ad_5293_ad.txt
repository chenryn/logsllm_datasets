framework [2] which implements both batch and per-instance
clipping. We ﬁx the clipping threshold at C = 1.
Figure 1 compares the accuracy loss of logistic regression
models trained over CIFAR-100 data set with both batch clip-
ping and per-instance clipping. Per-instance clipping allows
learning more accurate models for all values of  and ampli-
ﬁes the diﬀerences between the diﬀerent mechanisms. For
example, the model trained with RDP achieves accuracy close
to the non-private model for  = 100 when performing per-
instance clipping. Whereas, the models do not learn anything
useful when using batch clipping. Hence, for the rest of the
paper we only report the results for per-instance clipping.
4.2 Logistic Regression Results
We train (cid:96)2-regularized logistic regression models on both the
CIFAR-100 and Purchase-100 data sets.
CIFAR-100. The baseline model for non-private logistic re-
gression achieves accuracy of 0.225 on training set and 0.155
on test set, which is competitive with the state-of-art neural
network model [61] that achieves test accuracy close to 0.20
on CIFAR-100 after training on larger data set. Thus, there is a
small generalization gap of 0.07, which the inference attacks
try to exploit.
Figure 1(b) compares the accuracy loss for logistic regres-
sion models trained with diﬀerent relaxed notions of diﬀeren-
tial privacy as we varying the privacy budget . The accuracy
loss is normalized with respect to the accuracy of non-private
model to clearly depict the model utility. An accuracy loss
value of 1 means that the model has 100% loss and hence
has no utility, whereas the value of 0 means that the model
achieves same accuracy as the non-private baseline. As de-
picted in the ﬁgure, naïve composition achieves accuracy
1904    28th USENIX Security Symposium
USENIX Association
102101100101102103Privacy Budget ()0.00.20.40.60.81.0Accuracy LossRDPzCDPACNC102101100101102103Privacy Budget ()0.00.20.40.60.81.0Accuracy LossRDPzCDPACNC(a) Shokri et al. membership inference
(b) Yeom et al. membership inference
(c) Yeom et al. attribute inference
Figure 2: Inference attacks on logistic regression (CIFAR-100).
close to 0.01 for  ≤ 10 which is random guessing for 100-
class classiﬁcation. Naïve composition achieves accuracy loss
close to 0 for  = 1000. Advanced composition adds more
noise than naïve composition when privacy budget is greater
than the number of training epochs ( ≥ 100). The relaxations
zCDP and RDP achieve accuracy loss close to 0 at  = 500
and  = 50 respectively, which is order of magnitudes smaller
than the naïve composition. This is expected since the relaxed
deﬁnitions require less added noise.
Figures 2(a) and 2(b) show the privacy leakage due to
membership inference attacks on logistic regression mod-
els. Figure 2(a) shows results for the black-box attacker of
Shokri et al. [61], which has access to the target model’s
conﬁdence scores on the input record. Naïve composition
achieves privacy leakage close to 0 for  ≤ 10, and the leakage
reaches 0.065± 0.004 for  = 1000. The relaxed variants RDP
and zCDP have average leakage close to 0.080± 0.004 for
 = 1000. As expected, the diﬀerential privacy variations have
leakage in accordance with the amount of noise they add for
a given . The plots also show the theoretical upper bound
on the privacy leakage for -diﬀerential privacy, where the
bound is e − 1 (see Section 3.1).
Figure 2(b) shows results for the white-box attacker of
Yeom et al. [61], which has access to the target model’s loss on
the input record. As expected, zCDP and RDP relaxations leak
the most. Naïve composition does not have any signiﬁcant
leakage for  ≤ 10, but the leakage reaches 0.077± 0.003 for
 = 1000. The observed leakage of all the variations is in
accordance with the noise magnitude required for diﬀerent
diﬀerential privacy guarantees.
Figure 2(c) depicts the privacy leakage due to the attribute
inference attack. The privacy leakage of RDP is highest,
closely followed by zCDP. Naïve composition has low pri-
vacy leakage for  ≤ 10 (attacker advantage of 0.005±0.007 at
 = 10), but it quickly increases to 0.093± 0.002 for  = 1000.
But for meaningful privacy budgets, there is no signiﬁcant
leakage (< 0.02) for any of the methods. As expected, across
all variations as privacy budgets increase both the attacker’s
Figure 3: Accuracy loss of logistic regression (Purchase-100).
advantage (privacy leakage) and the model utility (accuracy)
increase. For this example, there is no choice of  available
that provides any eﬀective privacy for a model that does better
than random guessing.
To gain more understanding of the impact of privacy leak-
age, Table 5 shows the actual number of training set members
exposed to the attacker for diﬀerent diﬀerential privacy varia-
tions. We assume the attacker has some limited tolerance for
falsely exposing a member (that is, a bound on the acceptable
false positive rate), and sets the required threshold score for
the inference model output as the level needed to achieve that
false positive rate. Then, we count the number of members
in the private training data set for whom the inference model
output exceeds that conﬁdence threshold. Table 5 reports the
number of members exposed to an adversary who tolerates
false positive rates of 1%, 2%, and 5%. As we increase the
tolerance threshold, there is a gradual increase in membership
leakage for all the methods, and the leakage of relaxed vari-
ants increases drastically. Naïve composition and advanced
composition are resistant to attack for  ≤ 10, whereas zCDP
is resistant to attack for  ≤ 1. RDP is resistant up to  = 0.05.
USENIX Association
28th USENIX Security Symposium    1905
102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.00.20.40.60.81.0Accuracy LossRDPzCDPACNC
0.01
0.05
0.1
0.5
1.0
5.0
10.0
50.0
100.0
500.0
1,000.0
Loss
.93
.92
.94
.92
.93
.91
.90
.65
.48
.10
.01
Naïve Composition
1% 2%
0
0
0
0
0
0
0
2
29
112
138
0
0
0
0
0
0
0
0
6
53
65
5%
0
0
0
0
0
0
0
16
152
328
413
Advanced Composition
5%
0
0
0
0
0
0
0
73
138
256
301
1% 2%
0
0
0
0
0
0
0
31
47
88
111
0
0
0
0
0
0
0
19
18
42
57
Loss
.94
.93
.92
.94
.93
.89
.87
.64
.53
.29
.20
zCDP
1% 2%
0
0
0
0
0
11
38
102
121
159
172
0
0
0
0
0
2
15
44
58
80
86
5%
0
0
0
0
0
45
137
291
362
487
514
Loss
.93
.92
.94
.90
.88
.62
.47
.15
.08
.00
.00
Loss
.92
.94
.91
.68
.51
.16
.09
.02
.00
.00
.00
RDP
1% 2% 5%
0
0
1
27
122
304
329
445
456
516
530
0
0
0
3
21
95
109
142
158
166
185
0
0
0
0
4
39
55
70
76
86
93
Table 5: Number of individuals (out of 10,000) exposed by Yeom et al. membership inference attack on logistic regression
(CIFAR-100). The non-private ( = ∞) model leaks 129, 240 and 704 members for 1%, 2% and 5% FPR respectively.
Purchase-100. The baseline model for non-private logistic
regression achieves accuracy of 0.942 on the training set
and 0.695 on test set. In comparison, Google ML platform’s
black-box trained model achieves a test accuracy of 0.656 for
Purchase-100 (see Shokri et al. [61] for details).
Figure 3 shows the accuracy loss of all diﬀerential privacy
variants on Purchase-100 data set. Naïve composition and ad-
vanced composition have essentially no utility until  exceeds
100. At  = 1000, naïve composition achieves accuracy loss
of 0.116±0.003, the advanced composition achieves accuracy
loss of 0.513± 0.003 and the other variants achieve accuracy
loss close to 0.02. RDP achieves the best utility across all 
values. zCDP performs better than advanced composition and
naïve composition.
Figure 4 compares the privacy leakage of the variants
against the inference attacks. The leakage is in accordance
to the noise each variant adds and it increases proportionally
to the model utility. Hence, if a model has reasonable utility,
it is bound to leak membership information. The white-box
membership inference attack of Yeom et al. is relatively more
eﬀective than the black-box membership inference attack of
Shokri et al. as shown in Figures 4(a) and 4(b). Table 6 shows
the number of individual members exposed, with similar re-
sults to the ﬁndings for CIFAR-100.
4.3 Neural Networks
We train a neural network model consisting of two hidden lay-
ers and an output layer. The hidden layers have 256 neurons
that use ReLU activation. The output layer is a softmax layer
with 100 neurons, each corresponding to a class label. This
architecture is similar to the one used by Shokri et al. [61].
CIFAR-100. The baseline non-private neural network model
achieves accuracy of 1.000 on the training set and 0.168 on
test set, which is competitive to the neural network model
of Shokri et al. [61]. Their model is trained on a training set
of size 29,540 and achieves test accuracy of 0.20, whereas
our model is trained on 10,000 training instances. There is a
huge generalization gap of 0.832, which the inference attacks
can exploit. Figure 5(a) compares the accuracy loss of neu-
ral network models trained with diﬀerent relaxed notions of
diﬀerential privacy with varying privacy budget . The model
trained with naïve composition does not learn anything use-
ful until  = 100 (accuracy loss of 0.907± 0.004), at which
point the advanced composition also has accuracy loss close
to 0.935 and the other variants achieve accuracy loss close to
0.24. None of the variants approach zero accuracy loss, even
for  = 1000. The relative performance is similar to that of
logistic regression model discussed in Section 4.2.
Figures 6(a) and 6(b) shows the privacy leakage due to
membership inference attacks on neural network models
trained with diﬀerent relaxed notions for both attacks. The pri-
vacy leakage for each variation of diﬀerential privacy accords
with the amount of noise it adds to the model. The leakage
is signiﬁcant for relaxed variants at higher  values due to
model overﬁtting. For  = 1000, with the Shokri et al. attack,
naïve composition has leakage of 0.034 compared to 0.002 for
advanced composition, 0.219 for zCDP, and 0.277 for RDP
(above the region shown in the plot). For the white-box at-
tacker of Yeom et al. [74], RDP leaks the most for  = 1000
(membership advantage of 0.399) closely followed by zCDP.
This is because these relaxed variations add considerably less
noise in comparison to naïve composition. Naïve composi-
tion and advanced composition achieve strong privacy against
membership inference attackers, but fail to learning anything
useful. No option appears to provide both acceptable model
utility and meaningful privacy.
Like we did for logistic regression, we report the actual
number of training set members exposed to the attacker in Ta-
ble 7. The impact of privacy leakage is far more severe for the
non-private neural network model due to model overﬁtting—
1906    28th USENIX Security Symposium
USENIX Association
(a) Shokri et al. membership inference
(b) Yeom et al. membership inference
(c) Yeom et al. attribute inference
Figure 4: Inference attacks on logistic regression (Purchase-100).
(a) CIFAR-100
(b) Purchase-100
Figure 5: Accuracy loss of neural networks.
(a) Shokri et al. membership inference
(b) Yeom et al. membership inference
(c) Yeom et al. attribute inference
Figure 6: Inference attacks on neural network (CIFAR-100).
USENIX Association
28th USENIX Security Symposium    1907
102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.00.20.40.60.81.0Accuracy LossRDPzCDPACNC102101100101102103Privacy Budget ()0.00.20.40.60.81.0Accuracy LossRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC
0.01
0.05
0.1
0.5
1.0
5.0
10.0
50.0
100.0
500.0
1,000.0
Loss
.98
.99
.99
.98