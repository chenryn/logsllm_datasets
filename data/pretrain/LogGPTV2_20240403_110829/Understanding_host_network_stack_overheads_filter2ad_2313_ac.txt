skbs at device driver and then the skbs are merged at GRO layer.
Therefore, the receiver incurs higher overheads for skb allocation.
Where are the CPU cycles going? Figs. 3(c) and 3(d) show the
CPU usage breakdowns of sender- and receiver-side for each com-
bination of optimizations. With none of the optimizations, CPU
overheads mainly come from TCP/IP processing as per-skb pro-
cessing overhead is high (here, skb size is 1500B at both sides5).
When aRFS is disabled, lock overhead is high at the receiver-side
because of the socket contention due to the application context
thread (recv system call) and the interrupt context thread (softirq)
attempting to access the same socket instance.
5Linux kernel 4.17 onwards, GSO is enabled by default. We modified the kernel to
disable GSO in “no optimization” experiments to evaluate benefits of skb aggregation.
These packet processing overheads are mitigated with several
optimizations: TSO allows using large-sized skb at the sender-
side, reducing both TCP/IP processing and Netdevice subsystem
overheads as segmentation is offloaded to the NIC (Fig. 3(c)). On
the receiver-side, GRO reduces the CPU usage by reducing the
number of skbs, passed to the upper layer, so TCP/IP processing
and lock/unlock overheads are reduced dramatically, at the cost of
increasing the overhead of the network device subsystem where
GRO is performed (Fig. 3(d)). This GRO cost can be reduced by
66% by enabling Jumbo frames as explained above. These reduced
packet processing overheads lead to throughput improvement, and
the main overhead is now shifted to data copy, which takes almost
49% of total CPU utilization at the receiver-side when GRO and
Jumbo frames are enabled.
Once aRFS is enabled, co-location of the application context
thread and the IRQ context thread at the receiver leads to improved
cache and NUMA locality. The effects of this are two-fold:
(1) Since the application thread runs on the same NUMA node as
the NIC, it can now perform data copy directly from the L3
cache (DMAed by the NIC via DCA). This reduces the per-byte
data copy overhead, resulting in higher throughput-per-core.
(2) skbs are allocated in the softirq thread and freed in the appli-
cation context thread (once data copy is done). Since the two
are co-located, memory deallocation overhead reduces. This
is because page free operations to local NUMA memory are
significantly cheaper than those for remote NUMA memory.
Even a single flow experiences high cache misses. Although
aRFS allows applications to perform data copy from local L3 cache,
we observe as much as 49% cache miss rate in this experiment.
This is surprising since, for a single flow, there is no contention
69
for L3 cache capacity. To investigate this further, we varied various
parameters to understand their effect on cache miss rate. Among
our experiments, varying the maximum TCP receive window size,
and the number of NIC Rx descriptors revealed an interesting trend.
Fig. 3(e) shows the variation of throughput and L3 cache miss rate
with varying number of NIC Rx descriptors and varying TCP Rx
buffer size6. We observe that, with increase in either of the number
of NIC Rx descriptors or the TCP buffer size, the L3 cache miss
increases and correspondingly, the throughput decreases. We have
found two reasons for this phenomenon: (1) BDP values being larger
than the L3 cache capacity; and (2) suboptimal cache utilization.
To understand the first one, consider an extreme case of large
TCP Rx buffer sizes. In such a case, TCP will keep BDP worth of
data in flight, where BDP is defined as the product of access link
bandwidth and latency (both network and host latency). It turns
out that large TCP buffers can cause a significant increase in host
latency, especially when the core processing packets becomes a
bottleneck. In addition to scheduling delay of IRQ context and
application threads, we observe that each packet observe large
queueing behind previous packets. We measure the delay between
frame reception and start of data copy by logging the timestamp
when NAPI processing for an skb happens, and the timestamp
when the data copy of it starts, and measure the difference between
the two. Fig. 3(f) shows the average and 99th percentile delays
observed with varying TCP Rx buffer size. As can be seen, the delays
rise rapidly with increasing TCP Rx buffer size beyond 1600KB.
Given that DCA cache size is limited7, this increase in latency has
significant impact: since TCP buffers and BDP values are large, NIC
always has data to DMA; thus, since the data DMAed by the NIC
is not promptly copied to userspace buffers, it is evicted from the
cache when NIC performs subsequent DMAs (if the NIC runs out of
Rx descriptors, the driver replenishes the NIC Rx descriptors during
NAPI polling). As a result, cache misses increase and throughput
reduces. When TCP buffer sizes are large enough, this problem
persists independent of NIC ring buffer sizes.
To understand the second reason, consider the other extreme
where TCP buffer sizes are small but NIC ring buffer sizes are large.
We believe cache misses in this case might be due to an imperfect
cache replacement policy and/or cache’s complex addressing, re-
sulting in suboptimal cache utilization; recent work has observed
similar phenomena, although in a different context [15, 39]. When
there are a large number of NIC Rx descriptors, there is a corre-
spondingly larger number of memory addresses available for the
NIC to DMA the data. Thus, even though the total amount of in-
flight data is smaller than the cache capacity, the likelihood of a
DCA write evicting some previously written data increases with
the number of NIC Rx descriptors. This limits the effective utiliza-
tion of cache capacity, resulting in high cache miss rates and low
throughput-per-core.
Between these two extremes, both of the factors contribute to the
observed performance in Fig. 3(e). Indeed, in our setup, DCA cache
capacity is ∼3MB and hence TCP buffer size of 3200KB and fewer
than 512 NIC Rx descriptors (512 × 9000 bytes ≈ 4MB) delivers
6The kernel uses an auto-tuning mechanism for the TCP Rx socket buffer size with the
goal of maximizing throughput. In this experiment, we override the default auto-tuning
mechanism by specifying an Rx buffer size.
7DCA can only use 18% (∼3 MB) of the L3 cache capacity in our setup.
)
s
p
b
G
(
e
r
o
C
r
e
P
t
u
p
h
g
u
o
r
h
T
 60
 50
 40
 30
 20
 10
 0
Throughput Per Core
Receiver: Cache Miss Rate
 120
 100
 80
 60
 40
 20
 0
)
%
(
e
t
a
R
s
s
i
M
e
h
c
a
C
 NIC-local NUMA
NIC-remote NUMA
Figure 4: Linux network stack performance for the case of a single
flow on NIC-remote NUMA node. When compared to the NIC-local
NUMA node case, single flow throughput-per-core drops by ∼20%.
the optimal single-core throughput of ∼55Gpbs. An interesting
observation here is that the default auto-tuning mechanism used
in the Linux kernel network stack today is unaware of DCA effects,
and ends up overshooting beyond the optimal operating point.
DCA limited to NIC-local NUMA nodes. In our analysis so far,
the application was run on a CPU core on the NIC-local NUMA
node. We now examine the impact of running the application on
a NIC-remote NUMA node for the same single flow experiment.
Fig. 4 shows the resulting throughput-per-core and L3 cache miss
rate relative to the NIC-local case (with all optimizations enabled in
both cases). When the application runs on NIC-remote NUMA node,
we see a significant increase in L3 cache miss rate and ∼20% drop in
throughput-per-core. Since aRFS is enabled, the NIC DMAs frames
to the target CPU’s NUMA node memory. However, because the
target CPU core is on a NIC-remote NUMA node, DCA is unable to
push the DMAed frame data into the corresponding L3 cache [25].
As a result, cache misses increase and throughput-per-core drops.
3.2 Increasing Contention via One-to-one
We now evaluate the Linux network stack with higher contention
for the network bandwidth. Here, each sender core sends a flow to
one unique receiver core, and we increase the number of core/flows
from 1 to 24. While each flow still has the entire host core for itself,
this scenario introduces two new challenges compared to the single-
flow case: (1) network bandwidth becomes saturated as multiple
cores are used; and (2) flows run on both NIC-local and NIC-remote
NUMA nodes (our servers have 6 cores on each NUMA node).
Similar to §3.1, to obtain deterministic measurements when aRFS
is disabled, we explicitly map IRQs for individual applications to a
unique core on a different NUMA node.
Host optimizations become less effective with increasing
number of flows. Fig. 5(a) shows that, as the number of flows
increases, throughput-per-core decreases by 64% (i.e., 15Gbps at
24 flows), despite each core processing only a single flow. This is
because of reduced effectiveness of all optimizations. In particular,
when compared to the single flow case, the effectiveness of aRFS
reduces by as much as 75% for the 24-flow case; this is due to re-
duced L3 cache locality for data copy for NIC-local NUMA node
cores (all cores share L3 cache), and also due to some of the flows
running on NIC-remote NUMA nodes (that cannot exploit DCA, see
§3.1, Fig. 4). The effectiveness of GRO also reduces: since packets
at the receiver are now interleaved across flows, there are fewer
opportunities for aggregation; this will become far more prominent
in the all-to-all case, and is discussed in more depth in §3.5.
70
)
s
p
b
G
(
e
r
o
C
r
e
P
t
u
p
h
g
u
o
r
h
T
 50
 40
 30
 20
 10
 0
No Opt.
TSO/GRO
Jumbo
aRFS
Total Thpt
 100
 80
 60
 40
 20
 0
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
l
a
t
o
T
1
8
16
24
# Flows
l
s
e
c
y
C
U
P
C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
1 ow
8 ows
16 ows
24 ows
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c
n l o
d
e
h
s c
g
u li n
e t c .
l
s
e
c
y
C
U
P