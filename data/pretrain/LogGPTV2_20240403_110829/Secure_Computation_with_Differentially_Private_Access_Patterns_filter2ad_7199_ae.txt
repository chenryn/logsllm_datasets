which is much simpler to execute in a garbled circuit.
To simplify notation, we use F to denote F,δ. Consider any two
Proof:
neighboring graphs, and let D1, D2 denote their neighboring in-degree proﬁles.
Let FR denote the range of F, and let bD be a multi-set in FR. We say that
bD ∈ Bad if ∃i ∈ {1, . . . , V },bD(i) 
 = .3 and |V | = 212, we have α = 118, and E(|F(D)|) = 118|V | + |D|. That is,
for these privacy parameters, we expect to add 118 dummy edges for each node
in the graph.
Theorem 2 The protocol πgas deﬁned in Figure 4 securely computes Fgas with
L leakage in the
(Ffunc,FShuﬄe, DumGenp,α)-hybrid model according to Deﬁnition 5 (respectively
Deﬁnition 6) when using the second (resp. third) variant of DumGenp,α.
Proof:
(sketch.) We only prove the ﬁrst Theorem statement, and omit the
proof that we can meet the stronger security deﬁnition. At the end of this
section, we give some intuition for what would change in such a proof.
(F(DBR), out-deg(V )). In particular, then, we assume that out-deg(V ) is public
knowledge and given to the simulator, which holds in the joint collection model
of Deﬁnition 5. Note that |V | and |E| are both determined by out-deg(V ), and
these values will be used by the simulator as well.
Recall that the leakage functionality contains
We construct a simulator for a semi-honest P1. For all three ideal function-
alities, the output is simply an XOR secret sharing of some computed value.
The output of all calls to these functionalities can be perfectly simulated using
random binary strings of the appropriate length. Let simEdges1 denote the ran-
dom string used to simulate the output of FShuﬄe the ﬁrst time the functionality
22
There are only two remaining messages to simulate:
is called, and let simEdges2 denote the random string used to simulate the out-
put on the second call. Let simEdges1.u denote the restriction of simEdges1 to
the bits that make up the sharings of Edges.u, and let simEdges2.v be deﬁned
similarly.
Open(edge.u), and Open(edge.v). Recall that there are |E| + 2α|V | edges in the
Edges array: the original |E| real edges, and the 2α|V | dummy edges generated in
DumGenp,α. To simulate the message sent when opening Edges.u, the simulator
uses the values |V | and out-deg(V ) to create a bit string representing a random
shuﬄing of the following array of size |E| + 2α|V |. For each u ∈ V , the array
P
contains the identiﬁer of u exactly out-deg(u) times. This accounts for |E| =
u out-deg(u) positions of the array; the remaining 2α|V | positions are set to
⊥, consistent with the left nodes output by DumGenp,α. Letting r denote the
resulting bit-string, the simulator sends r ⊕ simEdges1.u to the adversary.
To simulate simEdges2.v, the simulator creates another bit-string represent-
bD = F(DBR) denote the ﬁrst element output by the leakage L, the simulator
ing a random shuﬄing of the following array, again of size |E| + 2α|V |. Letting
adds the node identiﬁers in bD to the array. In the remaining |E| + 2α|V | − |bD|
positions of the array, he adds ⊥. Letting r denote the resulting bit-string, the
simulator sends r ⊕ simEdges2.v to the adversary.
So far, this results in a perfect simulation of the adversary’s view. However,
note that the outputs of the two parties should be correlated. To ensure that
the joint distribution over the adversary’s view and the honest party’s output
is correct, the simulator has to submit the adversary’s input, hVerticesi, to the
trusted party. He receives back a new sharing of Vertices, and has to “plant” this
value in his simulation. Speciﬁcally, in the ﬁnal iteration of the protocol, when
simulating the output of Ffunc for the last time, the simulator uses hVerticesi, as
received from the trusted party, as the simulated output of this function call.
Hiding the out-degree of each node. We include another variant of DumGenp,α
on the right side of Figure 2. In that variant, separate noise is added to the
left node of each edge as well as to the right, which provides security according
to Deﬁnition 6. We do not implement or analyze the security of this variant.
Intuitively, though, for a graph G = (E, V ), it is helpful to think of the edge set
as deﬁning two databases of elements over V : for each (directed) edge (u, v), we
will view u as an element in database EL and v as an element in database ER.
Because the oblivious shuﬄe hides the edges between these two databases, the
access pattern can be fully simulated from two noisy histograms (one for each
database). This doubles the “sensitivity” of the “query”, and, because diﬀeren-
tial privacy composes, the added noisy information has the aﬀect of cutting 
in half. Since our analysis includes multiple values of , the reader can easily
extrapolate to get a sense of how we perform under our stronger security notion.
Hiding a user’s full edge set. The leakage function described above provide
edge privacy to each contributing party. That is, we have deﬁned two databases
23
to be neighboring when they diﬀer in a single edge. To understand the dis-
tinction, consider the application of building a movie recommendation system
through matrix factorization. If we guarantee edge privacy, then nobody can
learn whether a particular user reviewed a particular movie, but we cannot rule
out the possibility that an adversary could learn something about the set of
movies they have reviewed, perhaps, say, the genre that they enjoy. We could
also deﬁne two neighboring databases as diﬀering in a single node. Using the
same example, this would guarantee that nothing can be learned about any
individual user’s reviews, at all. It would require more noise: if the maximum
degree of any node is d, ensuring node privacy would have the aﬀect of scal-
ing  by d. In our experiments, we have included some smaller values of  to
help the reader evaluate how this additional noise would impact performance.
However, we note that if the maximum degree in the graph is large, achieving
node privacy might be diﬃcult. We defer investigating other possible notions
of neighboring graphs to future work.
Sequential composition. The standard security deﬁnition for secure compu-
tation composes sequentially, allowing the servers to perform repeated compu-
tations on the same data without impacting security. With our relaxation, if we
later use the same user data in a new computation, the leakage does compound.
The standard composition theorems from the literature on diﬀerential privacy
do apply, and we do not address here how privacy ought to be budgeted across
multiple computations. The reader should note that in our iterative protocol,
there is no additional leakage beyond the ﬁrst iteration, because we do not re-
generate the dummy items: the leakage in each iteration is the exactly the same
noisy degree proﬁle that was leaked in all prior iterations.
5 Diﬀerentially Private Graph Computation with
O(|E|) complexity

The construction in Section 4 requires O((|E| + α|V |) log(|E| + α|V |)) gar-
bled AND gates. In comparison, the implementation of Nayak et al. [28] uses
O(|E| + |V |) log2(|E| + |V |) garbled gates. As we found in the previous section,
α = O( log δ−log |V |
). When |E| = O(α|V |), this amounts to an asymptotic im-
provement of O(log(|E|)). This improvement stems from our ability to replace
several oblivious sorting circuits with oblivious shuﬄe circuits, which we are
able to do only because of our security relaxation. However, while less practi-
cal, Nayak et al. could instead rely on an asymptotically better algorithm for
oblivious sort, reducing their runtime to O((|E|+|V |) log(|E|+|V |)). We there-
fore ﬁnd it interesting to ask whether our security relaxation admits asymptotic
improvement for this class of computations, in addition to the practical improve-
ments described in the previous section. Indeed, we show that we can remove
the need for an oblivious shuﬄe altogether by allowing one party to shuﬄe the
data locally. As long as the party that knows the shuﬄing permutation does not
see the access pattern to V during the Scatter and Gather phases, the protocol
24
remains secure. The reason this protocol is less practical then the protocol of
Section 4 is because Ffunc now has to perform decryption and encryption, which
would require large garbled circuits.
The construction we present here requires O(|E|+α|V |) garbled AND gates,
demonstrating asymptotic improvement over the best known construction for
this class of computations, whenever |E| = O(α|V |). Figure 5 shows the formal
description of the protocol. We assume that the two computation servers hold
key pairs, (skAlice, pkAlice) and (skBob, pkBob). When data owners upload their
data, they encrypt the data under Alice’s key, encrypt the resulting ciphertext
under Bob’s key, and send the result to Bob (obviously this second encryption
is unnecessary, but it simpliﬁes the exposition to assume Bob receives the input
in this form).7 Recall that edge data contains (u, v, uData, vData, isReal), and
vertex data contains (x, xData). We assume each of these elements are encrypted
independently, so that we can decrypt portions of edges when needed. We also
assume that these encryption schemes are publicly re- randomizable: anyone can
take an encryption of x under pk, and re-randomize the ciphertext to give an
encryption of x, with fresh randomness, under the same pk. We assume that
re-randomized ciphertexts and “fresh” ciphertexts are equivalently distributed.
The protocol follows the same outline as the one in Section 4, but here
we separate the tasks of shuﬄing and data copying. Bob locally shuﬄes the
Throughout this protocol, we use(cid:74)x(cid:75)y to denote the encryption of x using y’s
edges,(cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob according to a permutation of his choice. He sends the
node identiﬁer for the right node, recovering (cid:74)Edges.v(cid:75)Alice. He re-randomizes
encrypted, shuﬄed arrays to Alice. For each edge, he also partially decrypts the
the resulting ciphertext, and sends it to Alice. Alice can now ﬁnd the right
vertex of every edge. She executes the Gather operation locally by performing
a linear scan over the edge data, opening the right vertex of edge, and copying
data from edge to vertex.
The two parties then execute the Apply operation together, performing a lin-
ear scan over the vertices, and calling a two-party functionality at vertex.8 Alice
supplies the functionality, Ffunc, with the encrypted data at each vertex, and
both parties provide their decryption key. The functionality decrypts, performs
the Apply function to all real data, and re-encrypts. The updated, encrypted
vertex data is output to Alice.
public key.
Bob now reshuﬄes all the edges and dummy ﬂags, just as before, re-randomizing
Alice’s ciphertexts. He sends
(cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob to Alice, who now performs the Scatter operation, as with
left vertex id,(cid:74)Edges.u(cid:75)Alice, recovers the vertex identiﬁer, and copies the vertex
Gather. That is, for each edge, she receives the re-randomized encryption of the
data from u back to the appropriate edge. She re-randomizes all ciphertexts,
and sends the edge data back to Bob.
7The data could instead be uploaded as in the previous section, and the servers could
perform a linear scan on the data to encrypt it as described here. This wouldn’t impact the
asymptotic claim; we chose the simpler presentation.