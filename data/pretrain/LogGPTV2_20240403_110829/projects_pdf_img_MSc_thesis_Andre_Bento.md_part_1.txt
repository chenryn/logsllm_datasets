Master’s Degree in Informatics Engineering
Final Dissertation
Observing and Controlling Performance
in Microservices
Author:
André Pascoal Bento
Supervisor:
Prof. Filipe João Boavida Mendonça Machado Araújo
Co-Supervisor:
Prof. António Jorge Silva Cardoso
July 2019
This page is intentionally left blank.
Abstract
Microservice based software architecture are growing in us-
age and one type of data generated to keep history of the work
performed by this kind of systems is called tracing data. Trac-
ingcanbeusedtohelpDevelopmentandOperations(DevOps)
perceive problems such as latency and request work-flow in
their systems. Diving into this data is difficult due to its com-
plexity, plethora of information and lack of tools. Hence, it
gets hard for DevOps to analyse the system behaviour in order
to find faulty services using tracing data. The most common
and general tools existing nowadays for this kind of data, are
aiming only for a more human-readable data visualisation to
relieve the effort of the DevOps when searching for issues in
their systems. However, these tools do not provide good ways
to filter this kind of data neither perform any kind of tracing
data analysis and therefore, they do not automate the task of
searching for any issue presented in the system, which stands
for a big problem because they rely in the system adminis-
trators to do it manually. In this thesis is present a possible
solutionforthisproblem, capableofusetracingdatatoextract
metrics of the services dependency graph, namely the number
of incoming and outgoing calls in each service and their corre-
sponding average response time, with the purpose of detecting
anyfaultyservicepresentedinthesystemandidentifyingthem
in a specific time-frame. Also, a possible solution for quality
tracing analysis is covered checking for quality of tracing struc-
ture against OpenTracing specification and checking time cov-
erage of tracing for specific services. Regarding the approach
to solve the presented problem, we have relied in the imple-
mentation of some prototype tools to process tracing data and
performed experiments using the metrics extracted from trac-
ing data provided by Huawei. With this proposed solution, we
expect that solutions for tracing data analysis start to appear
and be integrated in tools that exist nowadays for distributed
tracing systems.
Keywords
Microservices, Cloud Computing, Observability, Monitor-
ing, Tracing.
i
This page is intentionally left blank.
Resumo
A arquitetura de software baseada em micro-serviços está
a crescer em uso e um dos tipos de dados gerados para manter
o histórico do trabalho executado por este tipo de sistemas é
denominado de tracing. Mergulhar nestes dados é díficil de-
vido à sua complexidade, abundância e falta de ferramentas.
Consequentemente, é díficil para os DevOps de analisarem o
comportamento dos sistemas e encontrar serviços defeituosos
usando tracing. Hoje em dia, as ferramentas mais gerais e co-
muns que existem para processar este tipo de dados, visam
apenas apresentar a informação de uma forma mais clara, ali-
viando assim o esforço dos DevOps ao pesquisar por proble-
masexistentesnossistemas. Noentanto, estasferramentasnão
fornecem bons filtros para este tipo de dados, nem formas de
executar análises dos dados e, assim sendo, não automatizam o
processo de procura por problemas presentes no sistema, o que
gera um grande problema porque recaem nos utilizadores para
o fazer manualmente. Nesta tese é apresentada uma possivel
solução para este problema, capaz de utilizar dados de tracing
para extrair metricas do grafo de dependências dos serviços,
nomeadamente o número de chamadas de entrada e saída em
cada serviço e os tempos de resposta coorepondentes, com o
propósito de detectar qualquer serviço defeituoso presente no
sistemaeidentificarasfalhasemespaçostemporaisespecificos.
Além disto, é apresentada também uma possivel solução para
uma análise da qualidade do tracing com foco em verificar a
qualidadedaestruturadotracingfaceàespecificaçãodoOpen-
Tracing e a cobertura do tracing a nível temporal para serviços
especificos. A abordagem que seguimos para resolver o prob-
lema apresentado foi implementar ferramentas protótipo para
processar dados de tracing de modo a executar experiências
com as métricas extraidas do tracing fornecido pela Huawei.
Com esta proposta de solução, esperamos que soluções para
processar e analisar tracing comecem a surgir e a serem in-
tegradas em ferramentas de sistemas distribuidos.
Palavras-Chave
Micro-serviços, Computação na nuvem, Observabilidade,
Monitorização, Tracing.
iii
This page is intentionally left blank.
Acknowledgements
Thisworkwouldnotbepossibletobeaccomplishedwithout
effort, helpandsupportfrommyfamily, fellowsandcolleagues.
Thus, in this section I would like to give my sincere thanks to
all of them.
Starting by giving thanks to my mother and to my whole
family, who have supported me through this entire and long
journey, and who always gave and will always give me some
of the most important and beautiful things in life, love and
friendship.
In second place, I would like to thank all people that were
involved directly in this project. To my supervisor, Professor
Filipe Araújo, who contributed with his vast wisdom and ex-
perience, to my co-supervisor, Professor Jorge Cardoso, who
contributed with is vision and guidance about the main road
we should take and to Engineer Jaime Correia, who “breathes”
these kind of topics through him and helped a lot with is enor-
mous knowledge and enthusiasm.
In third place, I would like to thank Department of Infor-
matics Engineering and the Centre for Informatics and Sys-
tems, both from the University of Coimbra, for allowing and
provide the resources and facilities to to be carried out this
project.
In fourth place, to the Foundation for Science and Tech-
nology (FCT), for financing this project facilitating its accom-
plishment, to Huawei, for providing tracing data, core for this
whole research, and to Portugal National Distributed Com-
puting Infrastructure (INCD) for providing hardware to run
experiments.
And finally, my sincere thanks to everyone that I have not
mentioned and contributed to everything that I am today.
v
This page is intentionally left blank.
Contents
1 Introduction 1
1.1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.4 Work Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.5 Research Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.6 Document Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2 State of the Art 9
2.1 Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.1 Microservices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.1.2 Observability and Controlling Performance . . . . . . . . . . . . . . 11
2.1.3 Distributed Tracing . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1.4 Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.5 Time-Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2 Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2.1 Distributed Tracing Tools . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2.2 Graph Manipulation and Processing Tools . . . . . . . . . . . . . . . 18
2.2.3 Graph Database Tools . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.2.4 Time-Series Database Tools . . . . . . . . . . . . . . . . . . . . . . . 22
2.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.3.1 Mastering AIOps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.3.2 Anomaly Detection using Zipkin Tracing Data . . . . . . . . . . . . 24
2.3.3 Analysing distributed trace data . . . . . . . . . . . . . . . . . . . . 25
2.3.4 Research possible directions . . . . . . . . . . . . . . . . . . . . . . . 26
3 Research Objectives and Approach 27
3.1 Research Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4 Proposed Solution 33
4.1 Functional Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.2 Quality Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.3 Technical Restrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.4 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.4.1 Context Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.4.2 Container Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.4.3 Component Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
5 Implementation Process 41
5.1 Huawei Tracing Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
5.2 OpenTracing Processor Component . . . . . . . . . . . . . . . . . . . . . . . 45
vii
Chapter 0
5.3 Data Analysis Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
6 Results, Analysis and Limitations 57
6.1 Anomaly Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
6.2 Trace Quality Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
6.3 Limitations of OpenTracing Data . . . . . . . . . . . . . . . . . . . . . . . . 63
7 Conclusion and Future Work 65
viii
Acronyms
API Application Programming Interface. 9, 10, 22, 39, 49, 64, 67
CPU Central Processing Unit. 1, 2, 11, 16, 19
CSV Comma-separated values. 47, 54, 55, 56, 57, 60
DEI Department of Informatics Engineering. 1
DevOps Development and Operations. i, iii, 1, 2, 11, 17, 29, 64, 66
GDB Graph Database. 20, 21, 22, 39, 50
HTTP Hypertext Transfer Protocol. 12, 13, 23, 30, 39, 42, 43, 45, 47, 48
JSON JavaScript Object Notation. 41, 46, 55
OTP OpenTracing processor. 8, 37, 41, 43, 45, 47, 48, 50, 51, 54, 56, 57, 60, 61, 65
QA Quality Attribute. xi, 35, 36, 39
RPC Remote Procedure Call. 12, 30, 42, 43
TSDB Time Series Database. 22, 23, 39, 47, 48, 49, 54, 56
ix
This page is intentionally left blank.
List of Figures
1.1 Proposed work plan for first and second semesters. . . . . . . . . . . . . . . 5
1.2 Real work plan for first semester. . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Real and expected work plans for second semester. . . . . . . . . . . . . . . 6
2.1 Monolithic and Microservices architectural styles [10]. . . . . . . . . . . . . 10
2.2 Sample trace over time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 Span Tree example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.4 Graphs types. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5 Service dependency graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.6 time-series: Annual mean sunspot numbers for 1760-1965 [25]. . . . . . . . . 16
2.7 Anomaly detection in Time-Series [27]. . . . . . . . . . . . . . . . . . . . . . 16
2.8 Graph tools: Scalability vs. Algorithm implementation [35]. . . . . . . . . . 20
4.1 Proposed approach.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.2 Quality Attribute (QA) utility tree.. . . . . . . . . . . . . . . . . . . . . . . 35
4.3 Context diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.4 Container diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.5 Component diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
5.1 Trace file count for 2018-06-28. . . . . . . . . . . . . . . . . . . . . . . . . . 43
5.2 Trace file count for 2018-06-29. . . . . . . . . . . . . . . . . . . . . . . . . . 44
5.3 Service calls samples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.4 Service dependency variation samples. . . . . . . . . . . . . . . . . . . . . . 49
5.5 Service average response time samples. . . . . . . . . . . . . . . . . . . . . . 50
5.6 Service status code ratio samples. . . . . . . . . . . . . . . . . . . . . . . . . 50
5.7 Methods to handle missing data [67]. . . . . . . . . . . . . . . . . . . . . . . 53
5.8 Trend and seasonality results. . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5.9 Isolation Forests and OneClassSVM methods comparison [69]. . . . . . . . . 54
5.10 Trace time coverage example. . . . . . . . . . . . . . . . . . . . . . . . . . . 55
6.1 Sample of detection, using multiple feature, of “Anomalous” and “Non-
Anomalous” time-frame regions for a service. . . . . . . . . . . . . . . . . . 58
6.2 Comparisonbetween“Anomalous”and“Non-Anomalous”servicetime-frame
regions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
6.3 Comparisonbetween“Anomalous”and“Non-Anomalous”servicework-flow
types. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
6.4 Services coverability analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . 62
xi
This page is intentionally left blank.
List of Tables
2.1 Distributed tracing tools comparison. . . . . . . . . . . . . . . . . . . . . . . 18
2.2 Graph manipulation and processing tools comparison. . . . . . . . . . . . . 19
2.3 Graph databases comparison. . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4 Time-series databases comparison. . . . . . . . . . . . . . . . . . . . . . . . 23
3.1 Final state questions groups. . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.1 Functional requirements specification. . . . . . . . . . . . . . . . . . . . . . 34
4.2 Technical restrictions specification. . . . . . . . . . . . . . . . . . . . . . . . 36
5.1 Huawei tracing data set provided for this research. . . . . . . . . . . . . . . 41
5.2 Span structure definition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
5.3 Relations between final research questions, functional requirements and
metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
xiii
This page is intentionally left blank.
Chapter 1
Introduction