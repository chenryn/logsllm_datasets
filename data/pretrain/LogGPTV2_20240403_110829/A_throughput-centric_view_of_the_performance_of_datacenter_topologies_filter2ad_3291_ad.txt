with the optimal routing of the TM. It also presents the through-
put of the same TM on the bi-regular topology with 4 additional
switches.
Relationship between bisection bandwidth and throughput.
Using Theorem 4.1, we can derive a necessary condition for any
full throughput uni-regular topology:
ùê∑ ‚â§ ùëÅ (ùëÖ ‚àí ùêª)
(3)
ùêª2
Unlike bi-regular topologies where Clos topologies have full bi-
section bandwidth and full throughput (see below), uni-regular
TUBBBWSC[42]HM(100)JM(100)5k10k15k20k25k#Servers (N)0.000.250.500.751.00Throughput Gap5k10k15k20k25k#Servers (N)0255075100Time (seconds)50k100k150k200k250k300k#Servers (N)0.60.81.01.21.4Throughput50k100k150k200k250k300k#Servers (N)02500500075001000012500Time (seconds)SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Namyar .et al.
1). Thus, bi-regular topologies like VL2 [15] and FatTree [1], being
Clos topologies, have full throughput. We conjecture that F10 [36]
also has full throughput (F10 uses a different striping than Clos),
but have left it to future work to prove that.
4.2 The Full-Throughput Frontier
Table 3 shows the largest possible number of servers any uni-regular
topology can support at full throughput. However, this bound is
loose in part because it applies generically to all uni-regular topolo-
gies. In this section, for each topology family, we characterize, as a
function of ùêª, the largest size beyond which no topology has full-
throughput6 (as estimated by tub). We call this the full-throughput
frontier. For calibration, we also draw the full bisection-bandwidth
frontier, defined similarly. This comparison helps us quantitatively
understand the Venn diagram of Figure 2.
Methodology. To compute these frontier curves, we generate
topologies from each topology family, for different ùêª and ùëÅ . For
Jellyfish and Xpander, there is a uniquely defined topology given ùêª
and ùëÅ . (In our experiments, we have assumed a fixed switch radix
of 32 unless otherwise mentioned.) For each value of ùêª, we use
binary search on the total number of servers to find the maximum
ùëÅ that provides full bisection bandwidth, or full throughput.
For FatClique, we cannot precisely estimate the full-throughput
frontier since its topology instances can be non-monotonic with
respect to throughput. Specifically, because of the way it is con-
structed, for a given ùêª, a topology with ùëÅ servers can have full
throughput, but a topology with ùëÅ ‚Ä≤ 20M >20M >20M
Full-BBW >20M >20M >20M
Full-BBW >20M >20M >20M
256K
Table 3: Maximum number of servers, each topology set up can support
without violating the condition.
topologies can have full bisection bandwidth, but not full through-
put (as illustrated in Figure 2). Table 3 shows the maximum number
of servers each topology family can support without violating Equa-
tion 3 (switch radix ùëÖ is 32). It shows that the largest full throughput
uni-regular topology with 8 servers per switch can only support
111K servers, while the largest full bisection bandwidth Jellyfish,
Xpander, or FatClique topologies can support over 20M servers! (In
Table 3, for all uni-regular topologies, we were unable to estimate
the bisection bandwidth for topologies larger than 20M servers
because of computational limits.)
Scaling limits on uni-regular topologies. Another way of stat-
ing the results in Table 3 is that no uni-regular topology with ùêª = 8
and more than 111K servers can have full throughput. This implies
that there is a bound on the number of servers that a full through-
put uni-regular topology can have. Corollary 1 formalizes this; we
prove it in ¬ßG.
Corollary 1. For a given switch radix ùëÖ and servers per switch
ùêª, there exists a ùëÅ‚àó(ùëÖ, ùêª) such that for ùëÅ ‚â• ùëÅ‚àó(ùëÖ, ùêª), no full
throughput uni-regular topology exists with ùëÅ servers, switch radix
ùëÖ and ùêª servers per switch.
Every Clos-based topology always has full throughput. In
contrast to these scaling limits for uni-regular topologies, a fully-
deployed Clos-based topology always has full throughput. In ¬ß2.1,
we observed that Theorem 2.1 applies to Clos-based topologies.
Prior work has shown that a multi-stage Clos can (re-arrangeably)
support every permutation traffic matrix [25, 41]. Since Clos is a
bi-regular topology, it must have a throughput of 1 because, by The-
orem 2.1, it suffices to consider only permutation traffic matrices
to compute the throughput, and Clos can support all permutation
traffic matrices (i.e., for each matrix in ÀÜT , Clos has a throughput of
s1s2s3s4s51/21/3s1 -> s4: 1s4 -> s2: 1s2 -> s5: 1s5 -> s3: 1s3 -> s1: 1TMs1s2s3s4s51.01.01.01.01.0A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
(a) Jellyfish
(b) Xpander
(c) FatClique
Figure 8: Full-throughput Frontier Curve. Uni-regular topologies with H=8 and H=7 can not scale well while preserving full throughput even though they
maintain full BBW up to a very large size.
Xpander, there are no FatClique topologies above 10K which have
full throughput by the tub for ùêª values of 7 and 8 (at these values,
above 10K, all instances are labeled BBW).
Takeaways. While uni-regular topologies have elegant designs
(Jellyfish and Xpander) and useful manageability properties (Fat-
Clique), their throughput scaling is fundamentally limited (¬ß4), and
many of their topology instances do not have full-throughput even
at scales far smaller than modern data centers (e.g., Amazon AWS
with more than 50K servers [2], Google Jupiter with more than 30K
servers [42]). At these larger scales, these topologies can use smaller
values of ùêª, but this can negate the cost advantages of uni-regular
topologies, as we show next.
5 A THROUGHPUT-CENTRIC VIEW OF
TOPOLOGY EVALUATIONS
In this section, we revisit prior work on topology evaluation from
a throughput-centric perspective.
5.1 Throughput vs. Bisection Bandwidth
¬ß4.1 shows that, for uni-regular topologies, throughput and bisec-
tion bandwidth are different, and that, by definition, throughput
accurately captures the capacity of the network. Here we explore
whether conclusions from prior work that has used bisection band-
width to evaluate uni-regular topologies would change if through-
put were used instead. Table 4 summarizes our findings.
Topology Cost. Datacenter designers seek highly cost-effective
designs [35]. FatClique [52] and Jellyfish [44] have compared the
cost of their designs against Clos-based topologies by generating
full bisection bandwidth instances of their topology using the mini-
mum number of switches, and then comparing that number against
a Clos with the same number of servers. Figure 9 shows what would
happen if they had, instead, generated full throughput instances,
for topologies with different sizes and switch radices.
Figure 9(a) and Figure 9(b) show that the full throughput Jelly-
fish and Xpander built from 32-port switches use about 33% more
switches than the full bisection bandwidth topology at the scale of
32K and 131K servers (because, to achieve full throughput at larger
sizes, uni-regular topologies must use a smaller ùêª). This increase
in the number of switches for FatClique is approximately 27%. This
(a) N=32K, R=32
(b) N=131K, R=32
(c) Jellyfish Full Throughput vs Full BBW
Figure 9: Topology Cost. Number of switches to build a full throughput
topology is larger than a full BBW topology. (a) Number of switches to
build a topology with 32K servers using 32-port switches. (b) Number of
switches to build a topology with 131K servers using 32-port switches (At
these scales, tub is expected to have a small throughput gap.) (c) Number
of switches to build a Jellyfish topology with different switch radices to
support the same number of servers as a 1/8th 4-layer Clos. (Percentages
are Full-tub/Full-BBW - 1.)
affects the comparison with Clos7: Clos uses 1.8x more switches
compared to uni-regular topologies to achieve full bisection band-
width8 but only 1.3x more relative to full throughput uni-regular
topologies.
Figure 9(c) demonstrates that, at higher switch radices, the im-
pact of the choice of metric is more severe for uni-regular topologies.
To do this experiment, we needed to normalize the scale of the topol-
ogy relative to the radix of a switch. A natural way to normalize
this is to design a uni-regular topology with as many servers as a
full Clos with a given switch radix. However, at a radix of 64, a full
Clos has 2.1M servers to which our tub implementation does not
yet scale. So, we normalize the topology scale by designing Jellyfish
7In this and subsequent evaluations, for Clos topologies the number of servers per
switch for leaf switches is always equal to ùëÖ
2 , where ùëÖ is the switch radix, while the
rest of the switches have no servers.
8Results for bisection bandwidth are consistent with findings of [44, 52]
BBWThroughput6789#Servers per Switch (H)05k10k15k20k25k#Servers (N)6789#Servers per Switch (H)05k10k15k20k25k#Servers (N)678#Servers per Switch (H)5k10k15k20k25k#Servers (N)ClosJellyfishXpanderFatclique02k4k6k#SwitchesBBWTUBClosJellyfishXpanderFatclique010k20k30k#SwitchesBBWTUB1624324864switch radix (R)0153045Switches (%)SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Namyar .et al.
s
o
C
t [52]
tub
. [44]
tub
n
a
p
x
E
Jellyfish, Xpander, and FatClique use 50% fewer switches to support the same servers as Clos at large-scale.
Jellyfish, Xpander, and FatClique use 25% fewer switches to support the same servers as Clos at large-scale.
Jellyfish using random rewiring can be expanded with minor bandwidth loss while keeping the servers per switch constant (even under large expansion)
Expanding jellyfish without considering the target size can cause significant throughput drop when servers per switch is preserved (even under small
expansion).
Table 4: Throughput vs. Bisection Bandwidth. Conclusions can change significantly.
topologies with the same number of servers as a 1/8th Clos for the
corresponding switch radix. At a radix of 64, a 1/8th Clos has 263K
servers. Figure 9(c) shows the percentage increase in the number of
switches required to support Full Throughput over those required
to support Full BBW. This fraction increases with switch radix; with
64-port switches, Full BBW requires almost 50% more switches.
This difference can change a topology designer‚Äôs tradeoff analy-
sis. Clos and uni-regular topologies differ in one other important
way: the former has demonstrated, through wide deployment, a
simple and practical routing scheme (ECMP) that can achieve high
throughput, but proposed routing for uni-regular topologies rely
on routing schemes such as MPTCP [48] over K-shortest paths [49],
ECMP-VLB hybrid [29] or FatPaths [7]. The deployment and op-
erational cost of these schemes is not known, so, if the relative
switch cost advantage of uni-regular topologies is low, a designer
might find them less attractive when other costs, such as routing,
are taken into account.
Fabric Expansion. As recent work has shown [52, 53], datacenter
fabrics are rarely deployed at full scale initially. Rather, for a Clos-
based topology like Jupiter [42], a designer starts by determining a
target number of servers in the datacenter and the number of layers
needed in the Clos topology to achieve that scale. Then, they can
incrementally deploy the topology, often in units of superblocks [53].
One attractive aspect of some uni-regular topologies like Jellyfish
over Clos is that, at least conceptually, their expansion is simpler and