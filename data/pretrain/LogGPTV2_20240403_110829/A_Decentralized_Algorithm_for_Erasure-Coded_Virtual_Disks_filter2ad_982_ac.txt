return decode(blocks)
return ⊥
32:
33:
Variable ord-ts shows the logical time at which the most
recent write operation was started, establishing its place in
the ordering of operations. As such, max-ts(log)  max-ts(log) and ts≥ ord-ts)
if status then ord-ts ←ts; store(ord-ts)
reply [Order-R, status] to coord
status ←(ts > max-ts(log) and ts≥ ord-ts)
lts← LowTS; b ← ⊥
if status then
ord-ts ←ts; store(ord-ts)
if j = i or j = ALL then
49: when receive [Order&Read, j, max, ts] from coord
50:
51:
52:
53:
54:
55:
reply [Order&Read-R, status, lts, b] to coord
56:
57: when receive [Write, [b1, . . . ,bn], ts] from coord
status ←(ts > max-ts(log) and ts ≥ ord-ts)
58:
if status then log ←log∪{[ts, bi]}; store(log)
59:
reply [Write-R, status] to coord
60:
[lts, b] ←max-below(log, max)
prev-stripe method. This method ﬁnds the most recent ver-
sion with at least m blocks. Its loop ends when it ﬁnds the
timestamp of the most recent complete write. The recov-
ery method ensures that the completed read operation ap-
pears to happen after the partial write operation and that fu-
ture read operations will return values consistent with this
history.
4.4. Reading and writing a single block
Algorithm 3 Block methods and handlers for pi
61: procedure read-block( j)
62:
63:
replies←quorum([Read, { j}])
if status is all true and p j replied
and val-ts in all replies is the same then
return the block in p j’s reply
64:
65:
66:
67:
68:
69:
s← recover()
if s(cid:15)= ⊥ then
return s[ j]
return ⊥
else
70: procedure write-block( j, b)
71:
72:
73:
ts←newTS()
if fast-write-block( j, b, ts) = OK then return OK
else return slow-write-block( j, b, ts)
74: procedure fast-write-block( j, b, ts)
75:
76:
77:
78:
79:
80:
81:
82:
replies←quorum([Order&Read, j, HighTS, ts])
if status contains false or p j did not reply then
return ⊥
b j ←the block in p j’s reply
ts j ←the timestamp in p j’s reply
replies←quorum([Modify, j, b j, b, tsj, ts])
if status is all true then return OK
else return ⊥
83: procedure slow-write-block( j, b, ts)
84:
85:
86:
87:
data←read-prev-stripe(ts)
if data = ⊥ then return ⊥
data[ j] ←b
return store-stripe(data, ts)
88: when receive [Modify, j, b j, b, tsj, ts] from coord
89:
90:
91:
92:
93:
94:
95:
96:
97:
98:
status ←(tsj = max-ts(log) and ts≥ ord-ts)
if status then
if i = j then
bi ←b
else if i > m then
bi ←modify j,i(b j, b, max-block(log))
else
bi ←⊥
log ←log ∪{[ts, bi]}; store(log)
reply [Modify-R, status] to coord
Algorithm 3 deﬁnes the methods and message handlers
for reading and writing an individual block.
5. Discussion
The read-block method, which reads a given block num-
ber ( j), is almost identical to the read-stripe method ex-
cept that, in the common case, only p j performs a read.
The write-block method updates the parity blocks as well
as the data block at process p j. This is necessary when an
I/O request has written to a single block of the stripe, in or-
der to maintain consistency of the whole stripe. In the com-
mon case without any partial write, this method reads from,
and writes to, process p j and the parity processes (fast-
write-block). Otherwise, it essentially performs a recovery
(Line 17), except that it replaces the jth block with the new
value upon write-back.
5.1. Garbage collection of old data
Our algorithm relies on each process keeping its entire
history of updates in a persistent log, which is not practi-
cal. For the correctness of the algorithm, it is sufﬁcient that
each process remember the most recent timestamp-data pair
that was part of a complete write. Thus, when a coordinator
has successfully updated a full quorum with a timestamp ts,
it can safely send a garbage-collection message to all pro-
cesses to garbage collect data with timestamps older than ts.
Notice that the coordinator can send this garbage-collection
message asynchronously after it returns OK.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
Our algorithm
Block access
Stripe access
read/F write
4δ
4n
0
n
nB
2δ
2n
m
0
mB
read/S
6δ
6n
n + m
n
(2n + m)B
read/F
2δ
2n
1
0
B
latency
# messages
# disk reads
# disk writes
Network b/w
write/F
4δ
4n
k + 1
k + 1
read/S
6δ
6n
n + 1
write/S
8δ
8n
k + n + 1
k + n + 1
(2n + 1)B (2n + 1)B (4n + 1)B
n
LS97
read write
4δ
4δ
4n
4n
0
n
n
n
2nB
nB
Table 1. Performance comparison between our algorithm and the one by Lynch and Shvartsman [10].
The sufﬁx “/F” denotes the operations that ﬁnishes without recovery. The sufﬁx “/S” indicates the
operations that execute recovery. We assume that recovery only requires a single iteration of the
repeat loop. Parameter n is the number of processes, and k = n− m (i.e., k is the number of parity
blocks). We pessimistically assume that all replicas are involved in the execution of an operation. δ
is the maximum one-way messaging delay. B is the size of a block. When calculating the number of
disk I/Os, we assume that reading a block from loginvolves a single disk read, writing a block to log
involves a single disk write, and that timestamps are stored in NVRAM.
5.2. Algorithm complexity
Table 1 compares the performance of our algorithm and
state-of-the-art atomic-register constructions [10, 11]. We
improve on previous work in two ways: efﬁcient reading in
the absence of failures or concurrent accesses, and support
of erasure coding.
In describing our algorithm, we have striven for sim-
plicity rather than efﬁciency. In particular, there are sev-
eral straight-forward ways to reduce the network bandwidth
consumed by the algorithm for block-level writes: (a) if we
are writing block j, it is only necessary to communicate
blocks to p j and the parity processes, and (b) rather than
sending both the old and new block values to the parity pro-
cesses, we can send a single coded block value to each par-
ity process instead.
6. Related work
As we discussed in Section 1.3, our erasure-coding algo-
rithm is based on fundamentally different assumptions than
traditional erasure-coding algorithms in disk arrays.
The algorithm in [15] also provides erasure-coded stor-
age in a decentralized manner using a combination of a quo-
rum system and log-based store. The algorithm in [15] han-
dles Byzantine as well as crash failures, but does not ex-
plicitly handle process recovery (i.e., failures are perma-
nent). In contrast, our algorithm only copes with crash fail-
ures, but incorporates an explicit notion of process recov-
ery. Another difference is that the algorithm in [15] imple-
ments (traditional) linearizability where partial operations
may take effect at an arbitrary point in the future, whereas
our algorithm implements strict linearizability where par-
tial operations are not allowed to remain pending. Finally,
the algorithm in [15] only implements full-stripe reads and
writes, whereas our algorithm implements block-level reads
and writes as well.
The goal of [2] is to allow clients of a storage-area
network to directly execute an erasure-coding algorithm
when they access storage devices. The resulting distributed
erasure-coding scheme relies on the ability for clients to ac-
curately detect the failure of storage devices. Moreover, the
algorithm in [2] can result in data loss when certain com-
binations of client and device failures occur. For example,
consider a 2 out of 3 erasure-coding scheme with 3 stor-
age devices: if a client crashes after updating only a sin-
gle data device, and if the second data device fails, we can-
not reconstruct data. In contrast, our algorithm can tolerate
the simultaneous crash of all processes, and makes progress
whenever an m-quorum of processes come back up and are
able to communicate.
Several algorithms implement atomic read-write regis-
ters in an asynchronous distributed system based on mes-
sage passing [4, 10, 11]. They all assume a crash-stop fail-
ure model, and none of them support erasure-coding of the
register values.
References
[1] M. K. Aguilera and S. Frolund. Strict linearizability and
the power of aborting. Technical Report HPL-2003-241, HP
Labs, November 2003.
[2] K. Amiri, G. A. Gibson, and R. Golding. Highly concurrent
shared storage. In International Conference on Distributed
Computing Systems (ICDCS), 2000.
[3] S. Asami. Reducing the cost of system administration of a
disk storage system built from commodity components. PhD
thesis, University of California, Berkeley, May 2000. Tech.
Report. no. UCB-CSD-00-1100.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
[4] H. Attiya, A. Bar-Noy, and D. Dolev. Sharing memory ro-
bustly in message-passing systems. Journal of the ACM,
42(1):124–142, 1995.
[5] J. Elson, L. Girod, and D. Estrin. Fine-grained network time
synchronization using reference broadcasts. In Proceedings
of the 5th symposium on operatings systems design and im-
plementation (OSDI), pages 147–163. USENIX, 2002.
[6] S. Frolund, A. Merchant, Y. Saito, S. Spence, and A. Veitch.
FAB: enterprise storage systems on a shoestring. In Proceed-
ings of the Ninth Workshop on Hot Topics in Operating Sys-
tems (HOTOS IX). USENIX, 2003. to appear.
[7] S. Frolund, A. Merchant, Y. Saito, S. Spence, and A. Veitch.
A decentralized algorithm for erasure-coded virtual disks.
Technical Report HPL-2004-46, HP Labs, April 2004.
[8] M. Herlihy and J. Wing. Linearizability: a correctness condi-
tion for concurrent objects. ACM Transactions on Program-
ming Languages and Systems, 12(3):463–492, July 1990.
[9] iSCSI draft 20 speciﬁcation.
http://www.diskdrive.com-
/reading-room/standards.html, 2003.
[10] N. A. Lynch and A. A. Shvartsman. Robust emulation
of shared memory using dynamic quorum-acknowledged
broadcasts. In Proceedings of the IEEE Symposium on Fault-
Tolerant Computing Systems (FTCS), pages 272–281, 1997.
[11] N. A. Lynch and A. A. Shvartsman. Rambo: A reconﬁg-
urable atomic memory service for dynamic networks. In 16th
Int. Conf. on Dist. Computing (DISC), October 2002.
[12] D. A. Patterson, G. Gibson, and R. H. Katz. A case for re-
dundant arrays of inexpensive disks (raid). In H. Boral and
P.-A. Larson, editors, Proceedings of 1988 SIGMOD Inter-
national Conference on Management of Data, pages 109–16,
Chicago, IL, 1–3 June 1988.
[13] J. S. Plank. A tutorial on Reed-Solomon coding for fault-
tolerance in RAID-like systems. Software—Practice and Ex-
perience, 27(9), 1997.
[14] S. Reah, P. Eaton, D. Geels, H. Weatherspoon, B. Zhao, and
J. Kubiatowics. Pond: the OceanStore prototype. In Confer-
ence and File and Storage Technologies (FAST). USENIX,
mar 2003.
[15] J. J. Wylie, G. R. Goodson, G. R. Ganger, and M. K. Re-
iter. Efﬁcient byzantine-tolerant erasure-coded storage. In
Proceedings of the International Conference on Dependable
Systems and Networks (DSN), 2004.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply.