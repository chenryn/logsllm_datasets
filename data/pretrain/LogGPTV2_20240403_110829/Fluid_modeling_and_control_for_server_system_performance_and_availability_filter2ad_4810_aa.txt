# Fluid Modeling and Control for Server System Performance and Availability

**Authors:**
- Luc Malrait, NeCS Control Theory Research Group, INRIA–GipsaLab, Grenoble, France (Luc.Malrait@inria.fr)
- Sara Bouchenak, SARDES Distributed Systems Research Group, Univ. of Grenoble I–INRIA, Grenoble, France (Sara.Bouchenak@inria.fr)
- Nicolas Marchand, NeCS Control Theory Research Group, CNRS–GipsaLab, Grenoble, France (Nicolas.Marchand@inria.fr)

**Contact Information:**
- Phone: +33(0)476615382
- Fax: +33(0)476615252

**Submission Category:**
- Regular Paper
- Approximate Word Count: 7000

**Copyright:**
978-1-4244-4421-2/09/$25.00 © 2009 IEEE
389
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021 at 05:35:50 UTC from IEEE Xplore. Restrictions apply.

## Abstract
Server technology is essential for supporting a wide range of online services and applications. However, their ad-hoc configuration poses significant challenges to performance, availability, and economic costs. In this paper, we examine the impact of server configuration on the central trade-off between service performance and availability. First, we present a server model as a nonlinear continuous-time model using fluid approximations. Second, we develop admission control for server systems to achieve an optimal configuration. We provide two control laws for different QoS objectives: AM-C, which maximizes availability under a fixed performance constraint, and PM-C, which maximizes performance while meeting a desired availability target. We evaluate our fluid model and control techniques using the TPC-C industry-standard benchmark. Our experiments show that the proposed techniques improve performance by up to 30% while guaranteeing availability constraints.

**Keywords:**
- Modeling
- Control
- Server systems
- Admission control
- QoS

## 1. Introduction
A wide variety of internet services exists, ranging from web servers to email servers, streaming media services, e-commerce servers, and database systems. These services are typically based on the classical client-server architecture, where multiple clients concurrently access an online service provided by a server. Such server systems face varying workloads, as shown in several studies. For example, an email server is likely to experience a heavier workload in the morning when people check their emails upon arriving at work. In extreme cases, a heavy workload can lead to server thrashing and service unavailability, with significant economic costs, estimated at up to US$2.0 million per hour for Telecom and Financial companies.

A common technique to prevent server thrashing is admission control, which limits the number of concurrent clients on the server. This is known as the multi-programming level (MPL) configuration parameter. Existing approaches to admission control either rely on ad-hoc tuning and heuristics without optimality guarantees, apply linear control theory that does not capture the intrinsic nonlinear behavior of server systems, or follow a queuing theory approach that, while accurate, is difficult to calibrate and use.

We believe that modeling server systems is necessary to provide QoS guarantees. However, for effective deployment, models must accurately capture the dynamics and nonlinear behavior of server systems while being simple to deploy on existing systems. In this paper, we apply a nonlinear continuous-time control theory based on fluid approximations to model and control the QoS of server systems. The main contributions of the paper are:

- The design and implementation of a nonlinear continuous-time model of server systems that is easy to use and accurately captures the dynamics of server systems.
- The design and implementation of nonlinear admission control for server systems, including two variants: AM-C, which maximizes availability under a fixed performance constraint, and PM-C, which maximizes performance while meeting a desired availability target.

We present our experiments on the TPC-C application, an industry-standard benchmark, running on the PostgreSQL database server. The results show that the proposed techniques provide significant benefits in terms of performance and availability compared to an uncontrolled system.

The remainder of the paper is organized as follows. Section 2 provides an overview of the background. Sections 3 and 4 present our contributions in terms of a fluid model for server systems and feedback admission control laws, respectively. Section 5 describes the results of our experimental evaluation, and Section 6 presents related work. Finally, Section 7 draws our conclusions.

## 2. Background

### 2.1. Server Systems
We consider server systems such as database servers and web servers that follow the client-server architecture, where servers provide clients with online services, such as an online bookstore or e-banking. Clients and servers are hosted on different computers connected through a communication network. A client remotely connects to the server, sends a request, the server processes the request, builds a response, and returns it to the client before the connection is closed. Multiple clients may concurrently access the same server.

Server workload is characterized by the number of clients trying to access the server (workload amount, denoted as \( N \)) and the nature of requests made by clients (workload mix, denoted as \( M \)). Workload amount and mix can vary over time, corresponding to different client behaviors at different times. For example, an email service usually faces a higher workload amount in the morning than during the rest of the day.

Server admission control is a classical technique to prevent server thrashing when the number of concurrent clients grows. It involves setting a limit for the maximum number of clients allowed to concurrently access the server, known as the Multi-Programming Level (MPL) configuration parameter. Above this limit, incoming client requests are rejected. Thus, among the \( N \) clients trying to access the server, only \( N_e \) clients actually access the server, with \( N_e \leq MPL \). The server's MPL has a direct impact on the quality-of-service (QoS), performance, and availability of the server.

### 2.2. Service Performance and Availability
Several criteria can be considered to characterize service performance and availability. In this paper, we focus on two metrics that reflect performance and availability from the user's perspective: latency and abandon rate.

- **Service Performance – Latency:** Client request latency is defined as the time needed by the server to process a request. The average client request latency is denoted as \( L \). A low client request latency is desirable, reflecting a reactive system. Figure 1 describes the impact of server admission control and MPL value on client request latency as the workload amount varies. Three values of MPL are considered: a low value (1), a medium value (25), and a high value (75). A low MPL keeps the server unloaded, resulting in low latency, while a high MPL leads to increased latency as the workload amount increases.

- **Service Availability – Abandon Rate:** Client request abandon rate is defined as the ratio between the number of requests rejected due to admission control and the total number of requests received by the server. It is denoted as \( \alpha \). A low client request abandon rate is desirable, reflecting service availability. Figure 2 describes the impact of MPL on client request abandon rate. A low MPL is restrictive regarding client concurrency, leading to a higher abandon rate, while a high MPL accepts more clients, reducing the abandon rate.

Service performance and availability are part of the Service Level Agreement (SLA), which specifies service level objectives (SLOs) such as the maximum latency \( L_{max} \) and the maximum abandon rate \( \alpha_{max} \) to be guaranteed by the server.

## 3. Fluid Model for Server Systems
We propose a fluid model that captures the dynamics of server systems and reflects the characteristics of server behavior. Details on the underlying experimental testbed are given in Section 5.1.