title:The effects of active queue management on web performance
author:Long Le and
Jay Aikat and
Kevin Jeffay and
F. Donelson Smith
The Effects of Active Queue Management on Web Performance 
Long Le      Jay Aikat      Kevin Jeffay      F. Donelson Smith 
Department of Computer Science  
University of North Carolina at Chapel Hill 
http://www.cs.unc.edu/Research/dirt 
ABSTRACT 
We present an empirical study of the effects of active queue man-
agement (AQM) on the distribution of response times experienced 
by a population of web users. Three prominent AQM schemes are 
considered: the Proportional Integrator (PI) controller, the Random 
Exponential  Marking  (REM)  controller,  and  Adaptive  Random 
Early Detection (ARED). The effects of these AQM schemes were 
studied alone and in combination with Explicit Congestion Notifi-
cation (ECN). Our major results are:  
1. For  offered  loads  up  to  80%  of  bottleneck  link  capacity,  no 
AQM scheme provides better response times than simple drop-
tail FIFO queue management.  
2. For loads of 90% of link capacity or greater when ECN is not 
used, PI results in a modest improvement over drop-tail and the 
other AQM schemes.  
3. With ECN, both PI and REM provide significant response time 
improvement at offered loads above 90% of link capacity. More-
over, at a load of 90% PI and REM with ECN provide response 
times competitive to that achieved on an unloaded network.  
4. ARED  with  recommended  parameter  settings  consistently  re-
sulted in the poorest response times which was unimproved by 
the addition of ECN.  
We conclude that without ECN there is little end-user performance 
gain to be realized by employing the AQM designs studied here. 
However,  with  ECN,  response  times  can  be  significantly  im-
proved.  In  addition  it  appears  likely  that  provider  links  may  be 
operated at near saturation levels without significant degradation in 
user-perceived performance. 
Categories and Subject Descriptors 
C.2.2  [Computer  Systems  Organization]:  Computer  Communi-
cation Networks — Network Protocols.  
General Terms 
Algorithms, Measurement, Performance, Experimentation.  
Keywords 
Congestion control, Active queue management, Web performance. 
1  INTRODUCTION AND MOTIVATION 
The  random  early  detection  (RED)  algorithm,  first  described  ten 
years ago [7], inspired a new focus for congestion control research 
on  the  area  of  active  queue  management  (AQM).  The  common 
goal of all AQM designs is to keep the average queue size small in 
routers. This has a number of desirable effects including (1) pro-
Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. 
SIGCOMM’03, August 25–29, 2003, Karlsruhe, Germany. 
Copyright 2003 ACM 1-58113-735-4/03/0008…$5.00. 
viding queue space to absorb bursts of packet arrivals, (2) avoiding 
lock-out  and  bias  effects  from  a  few  flows  dominating  queue 
space, and (3) providing lower delays  for interactive applications 
such as web browsing [3].  
All AQM designs function by detecting impending queue buildup 
and notifying sources before the queue in a router overflows. The 
various designs proposed for AQM differ in the mechanisms used 
to detect congestion and in the type of control mechanisms used to 
achieve a stable operating point for the queue size. Another dimen-
sion that has a significant impact on performance is how the con-
gestion signal is delivered to the sender. In today’s Internet where 
the dominant transport protocol is TCP (which reacts to segment 
loss as an indicator of congestion), the signal is usually delivered 
implicitly by dropping packets at the router when the AQM algo-
rithm detects queue buildup. An IETF proposed standard adds an 
explicit signalling mechanism, called explicit congestion notifica-
tion (ECN) [12], by allocating bits in the IP and TCP headers for 
this purpose. With ECN a router can signal congestion to an end-
system by “marking” a packet (setting a bit in the header).  
In  this  work  we  report  the  results  of  an  empirical  evaluation  of 
three prominent examples of AQM designs. These are the Propor-
tional  Integrator  (PI)  controller  [8],  the  Random  Exponential 
Marking (REM) controller [2] and a contemporary redesign of the 
classic  RED  controller,  Adaptive  RED  [6]  (here  called  ARED). 
While these designs differ in many respects, each is an attempt to 
realize a control mechanism that achieves a stable operating point 
for  the  size  of  the  router  queue.  Thus  a  user  of  each  of  these 
mechanisms can determine a desired operating point for the control 
mechanism by simply specifying a desired target mean queue size. 
Choosing the desired queue size may represent a tradeoff between 
link utilization and queuing delay — a short queue reduces latency 
at the router but setting the target queue size too small may reduce 
link utilization by causing the queue to drain needlessly.  
Our goal in this study was first and foremost to compare the per-
formance of control theoretic AQM algorithms (PI and REM) with 
the more traditional randomized dropping found in RED. For per-
formance metrics we chose both user-centric measures of perform-
ance such as response times  for the request-response transactions 
that  comprise  Web  browsing,  as  well  as  more  traditional  metrics 
such as achievable link utilization and loss rates. The distribution 
of  response  times  that  would  be  experienced  by  a  population  of 
web users is used to assess the user-perceived performance of the 
AQM  schemes.  Link  utilization  is  used  to  assess  the  impact  on 
network  resources.  Of  particular  interest  was  the  implication  of 
ECN  support  on  performance.  ECN  requires  the  participation  of 
end-systems in the AQM scheme and hence it is important to quan-
tify the performance gain to be had at the expense of a more com-
plex protocol stack and migration issues for the end-system.  
Our experimental platform was a laboratory testbed consisting of a 
large collection of computers arranged to emulate a peering point 
between two ISPs operated at 100 Mbps (see Figure 1). We emu-
lated  the  Web  browsing  behavior  of  tens  of  thousands  of  users 
whose traffic transits the link connecting the ISPs and investigated 
the performance of each AQM scheme in the border-routers con-
necting  the  ISPs.  Each  scheme  was  investigated  both  with  and 
without ECN support across a variety of AQM parameter settings 
that  represented  a  range  of  target  router-queue  lengths.  For  each 
target queue length we varied the offered load on the physical link 
connecting  the  ISPs  to  determine  how  (or  if)  AQM  performance 
was affected by load.  
Our results were that for offered loads up to 80% of the bottleneck 
link capacity, no AQM scheme provided better response time per-
formance than simple drop-tail FIFO queue management. In addi-
tion, all schemes resulted in similar loss rates and link utilization. 
For offered loads above 80% of link capacity there was an advan-
tage to employing control theoretic AQM. When ECN is not used, 
at offered loads of 90% of link capacity, PI resulted in a modest 
improvement  over  drop-tail  and  the  other  AQM  schemes.  Web 
browsing  response  time  was  improved  for  responses  requiring 
more than approximately 400 milliseconds to complete but at the 
cost  of  slightly  lower  achievable  link  utilization  (compared  to 
drop-tail).  Of  particular  note  was  the  fact  that  without  ECN,  PI 
gave performance superior to REM. 
Our most striking result is that with ECN, both REM and PI sig-
nificantly outperform drop-tail at 90% load and provide response 
time  performance  that  is  competitive  to  that  achieved  on  an 
unloaded  network.  The  improved  response  time  performance, 
however, comes at some loss of achievable link utilization. In light 
of these results, an additional striking result was the fact that the 
addition of ECN did not improve ARED performance. ARED con-
sistently resulted in the poorest response time performance across 
all offered loads and resulted in the lowest link utilizations. 
We conclude that without ECN there is little end-user or provider 
performance  gain  to  be  realized  by  employing  the  AQM  algo-
rithms studied here. However, with ECN performance can be sig-
nificantly improved. In addition, our experiments provide evidence 
that provider links may be operated at near saturation levels (90% 
average utilization with bursty traffic sources) without significant 
degradation  in  user-perceived  performance  and  with  only  very 
modest decreases in link utilization (when compared to drop-tail). 
Thus unlike a similar earlier study [4] which was negative on the 
use  of  AQM,  we  view  the  ECN  results  as  a  significant  indicator 
that the stated goals of AQM can be realized in practice.  
While the results of this study are intriguing, the study was none-
theless  limited.  The  design  space  of  AQM  schemes  is  large  with 
each algorithm typically characterized by a number of independent 
parameters. We limited our consideration of AQM algorithms to a 
comparison  between  two  classes  of  algorithms:  those  based  on 
control  theoretic  principles  and  those  based  on  the  original  ran-
domized dropping paradigm of RED. Moreover, we studied a link 
carrying only Web-like traffic. More realistic mixes of HTTP and 
other  TCP  traffic  as  well  as  traffic  from  UDP-based  applications 
need to be examined.  
The following section reviews the salient design principles of cur-
rent  AQM  schemes  and  reviews  the  major  algorithms  that  have 
been proposed. Section 3 presents our experimental methodology 
and  discusses  the  generation  of  synthetic  Web  traffic.  Section  4 
presents  our  results  for  AQM  with  packet  drops  and  Section  5 
presents our results for AQM with ECN. The results are discussed 
in  Section  6.  We  conclude  in  Section  7  with  a  summary  of  our 
major results.  
2  BACKGROUND AND RELATED WORK 
The original RED design uses a weighted-average queue size as a 
measure of congestion. When this weighted average is smaller than 
a  minimum  threshold  (minth), no  packets  are  marked  or dropped. 
When the average queue length is between the minimum threshold 
and the maximum threshold (maxth), the probability of marking or 
dropping packets varies linearly  between 0 and a maximum drop 
probability (maxp, typically 0.10). If the average queue length ex-
ceeds maxth, all packets are marked or dropped. (The actual size of 
the queue must be greater than maxth to absorb transient bursts of 
packet arrivals.) A modification to the original design introduced a 
“gentle  mode”  in  which  the  mark  or  drop  probability  increases 
linearly  between  maxp  and  1  as  the  average  queue  length  varies 
between maxth and 2 x maxth. This fixes a problem in the original 
RED  design  caused  by  the  non-linearity  in  drop  probability  (in-
creasing from maxp to 1.0 immediately when maxth is reached).  
A weakness of RED is that it does not take into consideration the 
number of flows sharing a bottleneck link [5]. Given the TCP con-
gestion  control  mechanism,  a  packet  mark  or  drop  reduces  the 
offered load by a factor of (1 – 0.5n-1) where n is the number of 
flows  sharing  the  bottleneck  link.  Thus,  RED  is  not  effective  in 
controlling  the  queue  length  when  n  is  large.  On  the  other  hand, 
RED can be too aggressive and can cause under-utilization of the 
link when n is small. Feng et al. concluded that RED needs to be 
tuned for the dynamic characteristics of the aggregate traffic on a 
given  link  [5].  They  proposed  a  self-configuring  algorithm  for 
RED by adjusting maxp every time the average queue length falls 
out of the target range between minth and maxth. When the average 
queue  length  is  smaller  than  minth,  maxp  is  decreased  multiplica-
tively  to  reduce  RED’s  aggressiveness  in  marking  or  dropping 
packets;  when  the  queue  length  is  larger  than  maxth,  maxp  is  in-
creased  multiplicatively. Floyd et al. improved upon this original 
adaptive  RED  proposal  by  replacing  the  MIMD  (multiplicative 
increase multiplicative decrease) approach with an AIMD (additive 
increase multiplicative decrease) approach [6]. They also provided 
guidelines for choosing minth, maxth, and the weight for computing 
a  target  average  queue  length.  The  RED  version  that  we  imple-
mented  and  studied  in  our  work  (referred  to  herein  as  “ARED”) 
includes  both  the  adaptive  and  gentle  refinements  to  the  original 
design. It is based on the description given in [6].  
In [11], Misra et al. applied control theory to develop a model for 
TCP  and  AQM  dynamics  and  used  this  model  to  analyze  RED. 
They pointed out two limitations in the original RED design: (1) 
RED  is  either  unstable  or  has  slow  responses  to  changes  in  net-
work  traffic,  and  (2)  RED’s  use  of  a  weighted-average  queue 
length to detect congestion and its use of loss probability as a feed-
back signal to the senders were  flawed. Because of this, in over-
load situations, flows can suffer both high delay and a high packet 
loss rate. Hollot et al. simplified the TCP/AQM model to a linear 
system and designed a Proportional Integrator (PI) controller that 
regulates the queue length to a target value called the “queue refer-
ence,” qref [8]. The PI controller uses instantaneous samples of the 
queue length taken at a constant sampling frequency as its input. 
The drop probability is computed as 
p(kT) = a x (q(kT) – qref) – b x (q((k–1)T) – qref) + p((k–1)T) 
where  p(kT)  is  the  drop  probability  at  the  kth  sampling  interval, 
q(kT)  is  the  instantaneous  sample  of  the  queue  length  and  T  is 
1/sampling-frequency. A close examination of this equation shows 
that the drop probability increases in sampling intervals when the 
queue length is higher than its target value. Furthermore, the drop 
probability  also  increases  if  the  queue  has  grown  since  the  last 
sample (reflecting an increase in network traffic). Conversely, the 
drop  probability  in  a  PI  controller  is  reduced  when  the  queue 
length  is  lower  than  its  target  value  or  the  queue  length  has  de-
creased since its last sample. The sampling interval and the coeffi-
cients in the equation depend on the link capacity, the  maximum 
RTT and the expected number of active flows using the link.  
In [2], Athuraliya et al. proposed the Random Exponential Mark-
ing (REM) AQM scheme. REM periodically updates a congestion 
measure called “price” that reflects any mismatch between packet 
arrival and departure rates at the link (i.e., the difference between 
the demand and the service rate) and any queue size mismatch (i.e., 
the  difference  between  the  actual  queue  length  and  its  target 
value). The measure (p) is computed by: 
p(t) = max(0, p(t–1) + γ x ( α x (q(t) – qref) ) + x(t) – c) ) 
where  c  is  the  link  capacity  (in  packet  departures  per  unit  time), 
p(t) is the congestion measure, q(t) is the queue length, and x(t) is 
the packet arrival rate, all determined at time t. As with ARED and 
PI, the control target is only expressed by the queue size. 
The mark/drop probability in REM is defined as prob(t) = 1 – φ–
p(t), where φ > 1 is a constant. In overload situations, the congestion 
price increases due to the rate mismatch and the queue mismatch. 
Thus, more packets are dropped or marked to signal TCP senders 
to  reduce  their  transmission  rate.  When  congestion  abates,  the 
congestion price is reduced because the mismatches are now nega-
tive. This causes REM to drop or mark fewer packets and allows 
the senders to potentially increase their transmission rate. It is easy 
to see that a positive rate mismatch over a time interval will cause 
the  queue  size  to  increase.  Conversely,  a  negative  rate  mismatch 
over a time interval will drain the queue. Thus, REM is similar to 
PI  because  the  rate  mismatch  can  be  detected  by  comparing  the 
instantaneous queue length with its previous sampled value. Fur-
thermore, when drop or mark probability is small, the exponential 
function can be approximated by a linear function [1]. 
3  EXPERIMENTAL METHODOLOGY 
For  our  experiments  we  constructed  a  laboratory  network  that 
emulates the interconnection between two Internet service provider 
(ISP)  networks.  Specifically,  we  emulate  one  peering  link  that 
carries web traffic between sources and destinations on both sides 
of the peering link and where the traffic carried between the two 
ISP networks is evenly balanced in both directions.  
The  laboratory  network  used  to  emulate  this  configuration  is 
shown in Figure 1. All systems shown in this figure are Intel-based 
machines running FreeBSD 4.5. At each edge of this network are a 
set of 14 machines that run instances of a Web request generator 
(described below) each of which emulates the browsing behavior 
of thousands of human users. Also at each edge of the network is 
another  set  of  8  machines  that  run  instances  of  a  Web  response 
generator (also described below) that creates the traffic flowing in 
response to the browsing requests. A total of 44 traffic generating 
machines are in the testbed. In the remainder of this paper we refer 
to the machines running the Web request generator simply as the 
“browser machines” (or “browsers”) and the machines running the 
Network Monitor
ISP 1
Router
ISP 2
Router
Ethernet
Switches
Ethernet
Switches
.
.
.
100
Mbps
1
Gbps
100/1,000
Mbps
ISP 1
Browsers/Servers
Network
Monitor
1
Gbps
.
.
.
100
Mbps
ISP 2
Browsers/Servers
Figure 1: Experimental network setup. 
Web  response  generator  as  the  “server  machines”  (or  “servers”). 
The browser and server  machines have 100 Mbps Ethernet inter-
faces  and  are  attached  to  switched  VLANs  with  both  100  Mbps 
and 1 Gbps ports on 3Com 10/100/1000 Ethernet switches.  
At  the  core  of  this  network  are  two  router  machines  running  the 
ALTQ  extensions to FreeBSD. ALTQ  extends IP-output queuing 
at the network interfaces to include alternative queue-management 
disciplines  [10].  We  used  the  ALTQ  infrastructure  to  implement 
PI,  REM,  and  ARED.  The  routers  are  1  GHz  Pentium  IIIs  with 
over 1 GB of memory. Each router has one 1000-SX fiber Gigabit 
Ethernet NIC attached to one of the 3Com switches. Each router 
also  has  three  additional  Ethernet  interfaces  (a  second  1000-SX 
fiber Gigabit Ethernet NIC and two 100 Mpbs Fast Ethernet NICs) 
configured to create point-to-point Ethernet segments that connect 
the routers as shown in Figure 1. When conducting measurements 
to  calibrate  the  traffic  generators  on  an  un-congested  network, 
static routes are configured on the routers so that all traffic uses the 
full-duplex  Gigabit  Ethernet  segment.  When  we  need  to  create  a 
bottleneck between the two routers, the static routes are reconfig-
ured so that all traffic flowing in one direction uses one 100 Mbps 
Ethernet segment and all traffic  flowing in the opposite direction 
uses the other 100 Mbps Ethernet segment.1 These configurations 
allow us to emulate the  full-duplex behavior of the typical wide-
area network link.  
Another important factor in emulating this network is the effect of 
end-to-end  latency.  We  use  a  locally-modified  version  of  the 
dummynet  [9]  component  of  FreeBSD  to  configure  out-bound 
packet delays on browser machines to emulate different round-trip 
times  on  each  TCP  connection  (giving  per-flow  delays).  This  is 
accomplished by extending the dummynet mechanisms for regulat-
ing per-flow bandwidth to include a mode for adding a randomly-
chosen  minimum  delay  to  all  packets  from  each  flow.  The  same 
minimum delay is applied to all packets in a given flow (identified 
by  IP  addressing  5-tuple).  The  minimum  delay  in  milliseconds 
assigned to each flow is sampled from a discrete uniform distribu-
tion on the range [10, 150] (a mean of 80 milliseconds). The mini-
mum  and  maximum  values  for  this  distribution  were  chosen  to 
approximate a typical range of Internet round-trip times within the 
continental U.S. and the uniform distribution ensures a large vari-
ance  in  the  values  selected  over  this  range.  We  configured  the 
dummynet delays only on the browser’s outbound packets to sim-
plify the experimental setup. Most of the data transmitted in these 
experiments flow from the server to the browser and the TCP con-
1 We use two 100 Mbps Ethernet segments and static routes to separate the 
forward and reverse path flows in this configuration so we can use Ethernet 
hubs to monitor the traffic in each direction independently. Traffic on the 
Gigabit link is monitored using passive fiber splitters to monitor each direc-
tion independently.  
gestion control loop at the server (the one AQM causes to react) is 
influenced by the total RTT, not by asymmetry in the delays rela-
tive  to  the  receiver’s  side.  Because  these  delays  at  the  browsers 
effectively delay the ACKs received by the servers, the round-trip 
times experienced by the TCP senders (servers) will be the combi-
nation of the flow’s  minimum delay and any additional delay in-
troduced by queues at the routers or on the end systems. (End sys-
tems are configured to ensure no resource constraints were present 
hence delays there are minimal, ~1 millisecond.) A TCP window 
size of 16K bytes was used on all the end systems because widely 
used OS platforms, e.g., most versions of Windows, typically have 
default windows this small or smaller. 
The  instrumentation  used  to  collect  network  data  during  experi-
ments consists of two monitoring programs. One program monitors 
the  router  interface  where  we  are  examining  the  effects  of  the 
AQM  algorithms.  It  creates  a  log  of  the  queue  size  (number  of 
packets  in  the  queue)  sampled  every  10  milliseconds  along  with 
complete counts of the number of packets entering the queue and 
the number dropped. Also, a link-monitoring machine is connected 
to  the  links  between  the  routers  (through  hubs  on  the  100  Mbps 
segments or fiber splitters on the Gigabit link). It collects (using a 
locally-modified version of the tcpdump utility) the TCP/IP head-
ers in each frame traversing the links and processes these in real-