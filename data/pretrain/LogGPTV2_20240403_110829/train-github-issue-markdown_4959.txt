### Feature request
Thank you for the awesome framework!  
For my work I wanted to use
`transformers.pipelines.token_classification.TokenClassificationPipeline` in
batch mode, since it is much faster on GPU, but I wanted to keep all the
functionality for grouping entities.  
So I want to suggest something like this:
    nlp = pipeline("ner", model=model, 
                   tokenizer=tokenizer,
                   device = 0 if torch.cuda.is_available() else -1,
                   aggregation_strategy="average", batch_size=16)
### Motivation
I implemented it for myself and think it would be cool to have this
functionality "out-of-the-box" for community to enjoy the speed up. (And it
really gives a huge speed up)
### Your contribution
I am willing to contribute and implement this change for TokenClassification
task (also for TextClassification, FeatureExtraction should be pretty much
same). Have not worked with other pipelines, so not sure how batching is
implemented there, but I am willing to try and contribute.