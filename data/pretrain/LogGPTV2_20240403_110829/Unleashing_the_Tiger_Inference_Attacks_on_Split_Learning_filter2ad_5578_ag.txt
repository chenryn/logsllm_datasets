scopic images of skin cancer, but it shows consistently less diversity
in its composition and contains only 900 images. Note that there is
no intersection between the images of these two datasets.
Also in this case, we test the worst case scenario: 𝑋𝑝𝑢𝑏 =ISIC-2016
with 𝑋𝑝𝑟𝑖𝑣 =HAM10000 . Samples from the attack are reported in
Figure A.2. As in the previous case, the attack leads to the recon-
struction of clients’ private instances.
In the real-world scenario, the recovered images can be directly
used to re-identify patients, possibly violating privacy rules.
A.2 Property inference attacks
Inferring categorical attributes. The attacker can infer categorical
attributes rather than binary ones by training the network 𝐶𝑎𝑡𝑡 in
a multi-class classification and providing suitable labels to 𝑋𝑝𝑢𝑏.
To implement this scenario, we use the AT&T dataset which is
Figure A.3: Classification accuracy during the setup phase of
the FSHA performed on split 4 on the AT&T dataset. The red,
dashed line marks random guessing.
composed of frontal shots of 40 different individuals: 10 images each.
This dataset has been previously used in [30]. Here, the server wants
to identify the individuals represented on each image used during
the distributed training. That is, the attacker wants to correctly
assign one of the 40 possible identities (i.e., classes) to each received
smashed data.
As for the previous attack, we use a single fully-connected layer
to implement 𝐶𝑎𝑡𝑡 (with 40 output units), but we train the model
with a categorical cross-entropy loss function. Figure A.3 reports the
evolution of the classification accuracy during the setup phase of the
attack on 𝑋𝑝𝑟𝑖𝑣. Within a few initial iterations, the attacker reaches
a perfect accuracy in classifying the images of the 40 different
individuals composing the set.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2127B ARCHITECTURES AND EXPERIMENTAL
SETUPS
The employed architectures are reported in Table A.1. For the defi-
nition of convolutional layers we use the notation:
“(number of filters, kernel size, stride, activation function)”,
whereas for dense layers:
“(number of nodes, activation function)”.
The residual block used to build the discriminator 𝐷 is described in
Algorithm 2.
To construct the clients’ network 𝑓 , we use a standard convo-
lutional neural network (CNN) composed of convolutional and
pooling layers. The attacker’s network ˜𝑓 outputs a tensor with the
same shape of 𝑓 but diverges in every other parameter. Besides
being a CNN as well, 𝑓 builds on different kernel sizes, kernel num-
bers, and activation functions; ˜𝑓 does not include pooling layers, but
it reduces the kernel’s width by a larger stride in the convolutional
layers.
In our experiments, we have intentionally chosen the architec-
tures of 𝑓 and ˜𝑓 to be different. Our aim is to be compliant with the
defined threat model. However, we observed that choosing ˜𝑓 to be
similar to 𝑓 speeds up the attack procedure significantly.
Table B.2 reports additional hyper-parameters adopted during
the attack.
Datasets preparation. In our experiments, all the images on the
datasets MNIST, Fashion-MNIST, Omniglot and AT&T have been
reshaped into 32 × 32 × 3 tensors by replicating three times the
channel dimension. For the datasets CelebA, UTKFace, we cropped
and centered the images with [14] and reshaped them with a reso-
lution of 64 × 64. TinyImageNet, STL-10, HAM10000 and ISIC-2016
have been reshaped within a resolution of 64 × 64.
[−1, 1].
For each dataset, color intensities are scaled in the real interval
B.1 Client-side attack
To implement the client-side attack, we rely on a DCGAN-like [43]
architecture as in [30]. Specifically, the architecture for the splits
′ as well as for the generator 𝐺 are detailed in Table B.1.
𝑓 , 𝑠 and 𝑓
As in [30], we use a latent space of cardinality 100 with standard,
Gaussian prior.
Algorithm 2: Residual Block: resBlock:
Data: number of filters: 𝑛𝑓 , stride 𝑠
1 𝑥 = ReLU(𝑥);
2 𝑥 = 2D-Conv(x, nf, 3, (s,s));
3 𝑥 = ReLU(𝑥);
4 𝑥 = 2D-Conv(x, nf, 3, (1,1));
5 if 𝑠 > 1 then
6
7 return 𝑥𝑖𝑛 + 𝑥
𝑥𝑖𝑛 =2D-Conv(𝑥𝑖𝑛 , nf, 3, (s,s));
(a) Distance correlation.
(b) Reconstruction error.
Figure C.1: The average distance correlation (panel (a)) and
average reconstruction error (panel (b)) for the same model
trained with three different losses on CelebA.
C EVADING THE DISTANCE CORRELATION
METRIC VIA ADVERSARIAL FEATURE
SPACES
Despite the proven capability of the distance correlation metrics
of capturing linear as well as non-linear dependence on high-
dimensional data, this can be easily evaded by highly complex
mappings like those defined by deep neural networks. More for-
mally, given an input space 𝑋, it is quite simple to define a function
𝑓 such that:
E𝑥∼𝑋 [𝐷𝐶𝑂𝑅(𝑥, 𝑓 (𝑥))] = 𝜖1 , but E𝑥∼𝑋 [𝑑(𝑥, ˜𝑓 −1(𝑓 (𝑥)))] = 𝜖2,
(7)
where ˜𝑓 −1 is a decoder function, 𝑑 is a distance function defined
on 𝑋 and 𝜖1 and 𝜖2 are two constant values close to 0. That is,
the function 𝑓 (𝑥) produces an output 𝑧 that has minimal distance
correlation with the input but that allows a decoder network ˜𝑓 −1 to
accurately recover 𝑥 from 𝑧. Intuitively, this is achieved by hiding
information about 𝑥 in 𝑧 (smashed data) by allocating it in the blind
spots of distance correlation metrics.
In practice, such function 𝑓 can be learned by tuning a neural
network to minimize the following loss function:
L𝑓 , ˜𝑓 −1 = 𝐷𝐶𝑂𝑅(𝑥, 𝑓 (𝑥)) + 𝛼2 · 𝑑(𝑥, ˜𝑓 −1(𝑓 (𝑥)))
(8)
that is, training the network to simultaneously produce outputs
that minimize their distance correlation with the input and enable
reconstruction of the input from the decoder ˜𝑓 −1. Next, we validate
this idea empirically.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2128Table A.1: Architectures used for running the Feature-space hijacking attack.
Split
𝑓
˜𝑓
˜𝑓 −1
1
2
3
4
2D-Conv(64, 3, (1,1), ReLU)
batch-normalization
ReLU
maxPolling((2,2))
resBlock(64, 1)
2D-Conv(64, 3, (1,1), ReLU)
batch-normalization
ReLU
maxPolling((2,2))
resBlock(64, 1)
resBlock(128, 2)
2D-Conv(64, 3, (1,1), ReLU)
batch-normalization
ReLU
maxPolling((2,2))
resBlock(64, (1,1))
resBlock(128, 2)
resBlock(128, 1)
2D-Conv(64, 3, (1,1), ReLU)
batch-normalization
ReLU
maxPolling((2,2))
resBlock(64, 1)
resBlock(128, 2)
resBlock(128, 1)
resBlock(256, 2)
2D-Conv(64, 3, (2,2), linear)
2D-Conv(64, 3, (1,1), linear)
2D-ConvTrans(256, 3, (2,2), linear)
2D-Conv(3, 3, (1,1), tanh)
2D-Conv(64, 3, (2,2), linear)
2D-Conv(128, 3, (2,2), linear)
2D-Conv(128, 3, (1,1)
2D-ConvTrans(256, 3, (2,2), linear)
2D-ConvTrans(128, 3, (2,2), linear)
2D-Conv(3, 3, (1,1), tanh)
2D-Conv(64, 3, (2,2), linear)
2D-Conv(128, 3, (2,2), linear)
2D-Conv(128, 3, (1,1)
2D-ConvTrans(256, 3, (2,2), linear)
2D-ConvTrans(128, 3, (2,2), linear)
2D-Conv(3, 3, (1,1), tanh)
2D-Conv(64, 3, (2,2), linear)
2D-Conv(128, 3, (2,2), linear)
2D-Conv(256, 3, (2,2), linear)
2D-Conv(256, 3, (1,1))
2D-ConvTrans(256, 3, (2,2), linear)
2D-ConvTrans(128, 3, (2,2), linear)
2D-ConvTrans(3, 3, (2,2), tanh)
𝐷
2D-Conv(128, 3, (2,2), ReLU)
2D-Conv(128, 3, (2,2))
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
2D-Conv(256, 3, (2,2), ReLU)
dense(1)
2D-Conv(128, 3, (2,2))
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
2D-Conv(256, 3, (2,2), ReLU)
dense(1)
2D-Conv(128, 3, (2,2))
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
2D-Conv(256, 3, (2,2), ReLU)
dense(1)
2D-Conv(128, 3, (1,1))
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
resBlock(256, 1)
2D-Conv(256, 3, (2,2), ReLU)
dense(1)
Table B.1: Architectures for the client-side attacks.
Table B.2: Other hyper-parameters used during the Feature-
space hijacking attack.
𝑓
2D-Conv(64, 5, (2,2))
LeakyReLU
dropout(p=0.3)
𝑠
2D-Conv(126, 5, (2,2)
LeakyReLU
dropout(p=0.3)
′
𝑓
dense(#classes)
sigmoid
𝐺
dense(7·7·256)
batch-normalization
LeakyReLU
2D-ConvTrans(128, 5, (1,1))
batch-normalization
2D-ConvTrans(128, 5, (1,1))
batch-normalization
LeakyReLU
2D-ConvTrans(64, 5, (2,2))
batch-normalization
LeakyReLU
2D-ConvTrans(1, 5, (2,2), tanh)
We report the result for CelebA and use 𝑓 and ˜𝑓 −1 from the
setup 4. We use 𝑀𝑆𝐸 as 𝑑 and 𝛼2 = 50. We train the model for
104 iterations. Figure C.1 reports the average distance correlation
(Figure C.1a) and average reconstruction error (Figure C.1b) for the
same model trained with three different losses; namely:
Optimizer 𝑓
Optimizer ˜𝑓 and ˜𝑓 −1
Optimizer 𝐷
Weight gradient penalty 𝐷
Adam with 𝑙𝑟 = 0.00001
Adam with 𝑙𝑟 = 0.00001
Adam with 𝑙𝑟 = 0.0001
𝑙𝑟 = 0.0005 for split 4 of 𝑓
500.0
(1) In red, the model is trained on the adversarial loss reported
(2) In green, the model is trained only to minimize distance
in Eq. 8.
correlation.
(3) In blue, the model is trained only to minimize the reconstruc-
tion error (i.e., auto-encoder).
As can be noticed, the adversarial training procedure permits to
learn a pair of 𝑓 and ˜𝑓 −1 such that the distance correlation is mini-
mized (the same as we train the model only to minimize distance
correlation), whereas it enables the reconstruction of the input
data.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2129