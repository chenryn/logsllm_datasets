Without challenging the possibility that high-level FI may
indeed be inaccurate, the used fault-coverage metric (with
differing fault-space size quotients) may contribute signiﬁcantly
to this error, and suggest reevaluating the obtained result data
using our comparison metric.
VII. RELATED WORK
Over time, several metrics for the assessment of fault-
tolerance mechanisms have been devised. The classic fault-
coverage factor metric from Bouricius et al. [29] deﬁnes a
mathematical model that is used and instantiated by many
subsequent approaches, and is described more in detail in
Section III-B. In Arlat et al. [12], fault injection was initially
deﬁned to be a practical measurement method for fault coverage.
Consequently, most FI tools provide a way to map their results
to this metric [13], [14], [15].
Reis et al. [41] recognize the need for a metric that
adequately captures the tradeoff between performance and
reliability of software-based fault tolerance techniques, and
devise the Mean Work To Failure (MWTF) metric based
on an application-speciﬁc deﬁnition of “work units” and FI
measurements. Unlike our metric, MWTF is based on measuring
the Architectural Vulnerability Factor (AVF [42], see below)
implying a constant Δm. Furthermore, the authors do not derive
the connection between MWTF and P (F ailure), or the relation
to common practices in the ﬁeld. More recently, Santini et al.
[43] introduced a similar Mean Workload Between Failures
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
(MWBF) metric parametrized with results from radiation
measurements.
Several other metrics do not explicitly take the aforemen-
tioned performance/reliability tradeoff into account. Many are
also based on dynamic analysis techniques, such as FI, but
abstract from the low-level details to provide more information
to guide software development. Hiller et al. [44] analyze the
propagation of errors in modular software with their EPIC
framework, and detect the most exposed modules and signals
in the system using the permeability and exposure metrics.
Johansson and Suri [45] extend this approach to the analysis
of the dynamic behavior of an operating system. Similarly,
Gawkowski and Sosnowski [46] also use FI to trace fault
propagation over several levels, from logic up to the application
level.
With the Architectural Vulnerability Factor (AVF), Mukher-
jee et al. [42] devised a classic static fault-tolerance assess-
ment metric. Using low-level simulations, they measured the
reliability of microarchitectural structures. On the software
level, Sridharan et al. [47] developed the Program Vulnerability
Factor that is independent of expert knowledge on the mi-
croarchitecture. Both AVF and PVF weight their results by the
observed data lifetimes, and, thus, avoid Pitfall 1 (Section III-D).
Similarly, Benso et al. [48] created a high-level data criticality
metric determining the probability for each variable that it
propagates an error to the program’s output. More recently,
Rehman et al. [49] proposed the Application Vulnerability Index
(AVI), composed of values from their Function Vulnerability
Index (FVI), and recursively their Instruction Vulnerability
Index (IVI), the latter being derived in a comparable way as
Mukherjee’s AVF [42]. Based on their metrics, they control
reliability optimization passes in a compiler. On an even more
abstract level, Oz et al. [50] analyze multithreaded applications
with the Thread Vulnerability Factor (TVF).
VIII. CONCLUSIONS
After a step-by-step analysis of current practices in software-
implemented FI, we identiﬁed three common pitfalls in interpret-
ing FI result data for the comparison of program susceptibility
to soft errors in memory. Showing the effects on a real-world
data set, we demonstrated that each pitfall independently can
skew or even completely invalidate the analysis, and lead to
wrong conclusions regarding the effectiveness of software-based
fault-tolerance.
Concretely, we pointed out 1) that special care has to be
taken when processing the FI results after def/use fault-space
pruning has been applied, 2) that sampling combined with def/
use pruning must account for different equivalence-class sizes,
and, most importantly, 3) that the widely used fault coverage
metric is inadequate for the comparison of different benchmark
variants. As a remedy, we derived an objective comparison
metric that can be calculated both with full fault-space scans
and from sampling results: Absolute failure counts, extrapolated
to the fault-space size in the case of sampling.
For each pitfall we identiﬁed, we found FI studies that
are most probably affected. Especially the usage of the fault-
coverage metric for benchmark comparison is widespread – the
few examples cited in Section V-B by no means particularly
stand out. Although we believe that many of the described
software-based hardware fault-tolerance mechanisms would
prevail, we suggest to reevaluate them with our comparison
metric to sort out mechanisms that in fact decrease fault
tolerance of programs they are deployed in. With a similar
motivation, a recent study by Shrivastava et al. [51] using the
AVF metric [42] surprisingly showed that ﬁve control-ﬂow
checking schemes – claimed effective by their original authors
– actually increase the system vulnerability.
In future work, we intend to look into different fault models,
to compare simulation-obtained results of our metric to radiation
measurements, and to evaluate and improve existing software-
based hardware fault-tolerance mechanisms.
ACKNOWLEDGMENTS
We thank our anonymous reviewers for their very helpful
and encouraging comments. We also thank Michael Engel for
detailed comments, and his suggestion for naming the “dilution
delusion” (Section IV-B). This work was partly supported by the
German Research Foundation (DFG) priority program SPP 1500
under grant no. SP 968/5-3.
REFERENCES
[1] D. Binder, E. Smith, and A. Holman, “Satellite anomalies from galactic
cosmic rays,” IEEE TNS, vol. 22, no. 6, pp. 2675–2680, Dec. 1975.
[2] T. C. May and M. H. Woods, “Alpha-particle-induced soft errors in
dynamic memories,” IEEE Transactions on Electron Devices, vol. 26,
no. 1, pp. 2–9, Jan. 1979.
[3] S. Mukherjee, Architecture Design for Soft Errors. San Francisco, CA,
USA: Morgan Kaufmann, 2008.
[4] E. Fuchs, “An evaluation of the error detection mechanisms in MARS
using software-implemented fault injection,” in 2nd Europ. Depend.
Comp. Conf. (EDCC ’96), A. Hlawiczka, J. G. Silva, and L. Simoncini,
Eds. Springer, 1996, pp. 73–90.
[5] M. Rebaudengo, M. S. Reorda, M. Violante, and M. Torchiano, “A
source-to-source compiler for generating dependable software,” in 1st
IEEE Int. W’shop on Source Code Analysis and Manipulation, 2001,
pp. 33–42.
[6] B. Nicolescu, Y. Savaria, and R. Velazco, “Software detection mecha-
nisms providing full coverage against single bit-ﬂip faults,” IEEE TNS,
vol. 51, no. 6, pp. 3510–3518, Dec. 2004.
[7] G. Chen, M. Kandemir, N. Vijaykrishnan, and M. J. Irwin, “Object
duplication for improving reliability,” in Proceedings of the 2006 Asia
and South Paciﬁc Design Automation Conference, ser. ASP-DAC ’06.
Piscataway, NJ, USA: IEEE, 2006, pp. 140–145.
[8] C. Borchert, H. Schirmeier, and O. Spinczyk, “Generative software-
based memory error detection and correction for operating system data
structures,” in 43rd IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN
’13).
IEEE, Jun. 2013.
[9] X. Li, M. C. Huang, K. Shen, and L. Chu, “A realistic evaluation of
memory hardware errors and software system susceptibility,” in 2010
USENIX TC. Berkeley, CA, USA: USENIX, 2010.
[10] V. Sridharan and D. Liberty, “A study of DRAM failures in the ﬁeld,” in
Int. Conf. for High Perf. Computing, Networking, Storage and Analysis
(SC ’12). Los Alamitos, CA, USA: IEEE, 2012, pp. 76:1–76:11.
[12]
[11] V. Sridharan, J. Stearley, N. DeBardeleben, S. Blanchard, and S. Gu-
rumurthi, “Feng shui of supercomputer memory: Positional effects in
DRAM and SRAM faults,” in Int. Conf. for High Perf. Computing,
Networking, Storage and Analysis (SC ’13). ACM, 2013.
J. Arlat, M. Aguera, L. Amat, Y. Crouzet, J.-C. Fabre, J.-C. Laprie,
E. Martins, and D. Powell, “Fault injection for dependability validation:
A methodology and some applications,” IEEE TOSE, vol. 16, no. 2, pp.
166–182, Feb. 1990.
J. A. Clark and D. K. Pradhan, “Fault injection: A method for validating
computer-system dependability,” IEEE Comp., vol. 28, no. 6, pp. 47–56,
Jun. 1995.
[13]
[14] M.-C. Hsueh, T. K. Tsai, and R. K. Iyer, “Fault injection techniques
and tools,” IEEE Comp., vol. 30, no. 4, pp. 75–82, Apr. 1997.
329329
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
[15] A. Benso and P. E. Prinetto, Fault injection techniques and tools for
embedded systems reliability evaluation, ser. Frontiers in electronic
testing. Boston, Dordrecht, London: Kluwer, 2003.
[17]
[16] H. Ziade, R. A. Ayoubi, and R. Velazco, “A survey on fault injection
techniques,” The International Arab Journal of Information Technology,
vol. 1, no. 2, pp. 171–186, 2004.
J. Karlsson, P. Liden, P. Dahlgren, R. Johansson, and U. Gunneﬂo,
“Using heavy-ion radiation to validate fault-handling mechanisms,” IEEE
Micro, vol. 14, no. 1, pp. 8–23, Feb. 1994.
J. Karlsson, P. Folkesson, J. Arlat, Y. Crouzet, G. Leber, and J. Reisinger,
“Application of three physical fault injection techniques to the experi-
mental assessment of the MARS architecture,” in Conf. on Dep. Comp.
for Crit. App. (DCCA ’95). Washington, DC, USA: IEEE, 1995.
[18]
[19] S. K. Sastry Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, “Relyzer:
Exploiting application-level fault equivalence to analyze application
resiliency to transient faults,” in 17th Int. Conf. on Arch. Support for
Programming Languages and Operating Systems (ASPLOS ’12). New
York, NY, USA: ACM, 2012, pp. 123–134.
[20] K. Parasyris, G. Tziantzoulis, C. D. Antonopoulos, and N. Bellas,
“GemFI: A fault injection tool for studying the behavior of applications
on unreliable substrates,” in 44th IEEE/IFIP Int. Conf. on Dep. Sys. &
Netw. (DSN ’14).
IEEE, Jun. 2014, pp. 622–629.
[21] D. Skarin, R. Barbosa, and J. Karlsson, “GOOFI-2: A tool for
experimental dependability assessment,” in 40th IEEE/IFIP Int. Conf.
on Dep. Sys. & Netw. (DSN ’10). Los Alamitos, CA, USA: IEEE,
Jun./Jul. 2010, pp. 557–562.
[22] A. Avižienis, J.-C. Laprie, B. Randell, and C. Landwehr, “Basic concepts
and taxonomy of dependable and secure computing,” IEEE TDSC, vol. 1,
no. 1, pp. 11–33, Jan. 2004.
[23] A. Dixit and A. Wood, “The impact of new technology on soft error
rates,” in IEEE Int’l Reliab. Physics Symp. (IRPS ’11), Apr. 2011, pp.
5B.4.1–5B.4.7.
[24] A. Massa, Embedded Software Development with eCos. Prentice Hall
Professional Technical Reference, 2002.
[25] H. Schirmeier, M. Hoffmann, R. Kapitza, D. Lohmann, and O. Spinczyk,
“FAIL*: Towards a versatile fault-injection experiment framework,”
in 25th Int. Conf. on Arch. of Comp. Sys. (ARCS ’12), Workshop
Proceedings, ser. Lecture Notes in Informatics, G. Mühl, J. Richling,
and A. Herkersdorf, Eds., vol. 200. German Society of Informatics,
Mar. 2012, pp. 201–210.
[26] K. S. Trivedi, Probability and Statistics with Reliability, Queuing, and
Computer Science Applications, 2nd ed. Wiley, 2002.
[27] D. Powell, E. Martins, J. Arlat, and Y. Crouzet, “Estimators for fault
tolerance coverage evaluation,” IEEE TC, vol. 44, no. 2, pp. 261–274,
Feb. 1995.
[28] R. Leveugle, A. Calvez, P. Maistri, and P. Vanhauwaert, “Statistical fault
injection: quantiﬁed error and conﬁdence,” in 2009 Conf. on Design,
Autom. & Test in Europe (DATE ’09).
IEEE, 2009, pp. 502–506.
[29] W. G. Bouricius, W. C. Carter, and P. R. Schneider, “Reliability modeling
techniques for self-repairing computer systems,” in 24th National
Conference, ser. ACM ’69. New York, NY, USA: ACM, 1969, pp.
295–309.
[30] D. T. Smith, B. W. Johnson, J. A. Profeta, III, and D. G. Bozzolo, “A
method to determine equivalent fault classes for permanent and transient
faults,” in Annual Reliability and Maintainability Symposium.
IEEE,
Jan. 1995, pp. 418–424.
J. Güthoff and V. Sieh, “Combining software-implemented and
simulation-based fault injection into a single fault injection method,” in
25th Annual Int. Symp. on Fault-Tol. Comp. (FTCS ’95).
IEEE, Jun.
1995, pp. 196–206.
[31]
[32] A. Benso, M. Rebaudengo, L. Impagliazzo, and P. Marmo, “Fault-list
collapsing for fault-injection experiments,” in Annual Reliability and
Maintainability Symposium.
IEEE, Jan. 1998, pp. 383–388.
[33] L. Berrojo, I. Gonzalez, F. Corno, M. Reorda, G. Squillero, L. Entrena,
and C. Lopez, “New techniques for speeding-up fault-injection cam-
paigns,” in 2002 Conf. on Design, Autom. & Test in Europe (DATE ’02).
IEEE, 2002, pp. 847–852.
[34] R. Barbosa, J. Vinter, P. Folkesson, and J. Karlsson, “Assembly-level
pre-injection analysis for improving fault injection efﬁciency,” in 5th
Europ. Depend. Comp. Conf. (EDCC ’05), vol. 3463. Springer, Apr.
2005, p. 246.
J. Grinschgl, A. Krieg, C. Steger, R. Weiss, H. Bock, and J. Haid,
“Efﬁcient fault emulation using automatic pre-injection memory access
analysis,” in 25th IEEE SoC Conf. (SOCC ’12).
IEEE, 2012, pp.
277–282.
[35]
[36] M. Hoffmann, C. Dietrich, and D. Lohmann, “Failure by design:
Inﬂuence of the RTOS interface on memory fault resilience,” in 2nd
GI W’shop on SW-Based Methods for Robust Embedded Sys. (SOBRES
’13), ser. Lecture Notes in Informatics. German Society of Informatics,
Sep. 2013.
[37] R. Alexandersson and J. Karlsson, “Fault injection-based assessment of
aspect-oriented implementation of fault tolerance,” in 41st IEEE/IFIP
Int. Conf. on Dep. Sys. & Netw. (DSN ’11).
IEEE, Jun. 2011, pp.
303–314.
[38] C. Borchert, H. Schirmeier, and O. Spinczyk, “Protecting the dynamic
dispatch in C++ by dependability aspects,” in 1st GI W’shop on SW-
Based Methods for Robust Embedded Sys. (SOBRES ’12), ser. Lecture
Notes in Informatics. German Society of Informatics, Sep. 2012, pp.
521–535.
[39] H. Cho, S. Mirkhani, C.-Y. Cher, J. A. Abraham, and S. Mitra,
“Quantitative evaluation of soft error injection techniques for robust
system design,” in 50th Design Autom. Conf. (DAC ’13).
IEEE, May
2013.
J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the accuracy
of high-level fault injection techniques for hardware faults,” in 44th
IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’14).
IEEE, Jun.
2014, pp. 375–382.
[40]
[41] G. A. Reis, J. Chang, N. Vachharajani, S. S. Mukherjee, R. Rangan, and
D. I. August, “Design and evaluation of hybrid fault-detection systems,”
in 32nd Int. Symp. on Comp. Arch. (ISCA ’05).
IEEE, Jun. 2005, pp.
148–159.
[42] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin,
“A systematic methodology to compute the architectural vulnerability
factors for a high-performance microprocessor,” in IEEE/ACM MICRO
36. Los Alamitos, CA, USA: IEEE, 2003.
[43] T. Santini, P. Rech, G. Nazar, L. Carro, and F. Rech Wagner, “Reducing
embedded software radiation-induced failures through cache memories,”
in 19th IEEE Europ. Test Symp. (ETS ’14), May 2014.
[44] M. Hiller, A. Jhumka, and N. Suri, “EPIC: Proﬁling the propagation and
effect of data errors in software,” IEEE TC, vol. 53, no. 5, pp. 512–530,
May 2004.
[45] A. Johansson and N. Suri, “Error propagation proﬁling of operating
systems,” in 35th IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’05),
Jun./Jul. 2005, pp. 86–95.
[46] P. Gawkowski and J. Sosnowski, “Evaluation of transient fault suscepti-
bility in microprocessor systems,” in Euromicro Symp. on Digital System
Design (DSD ’04), Aug. 2004, pp. 432–439.
[47] V. Sridharan and D. R. Kaeli, “Eliminating microarchitectural depen-
dency from architectural vulnerability,” in 15th IEEE Int. Symp. on High
Performance Computer Architecture (HPCA ’09).
IEEE, Feb. 2009,
pp. 117–128.
[48] A. Benso, S. Di Carlo, G. Di Natale, P. E. Prinetto, and L. Tagliaferri,
“Data criticality estimation in software applications,” in 2003 Int’l Test
Conf. (ITC ’03). Los Alamitos, CA, USA: IEEE, 2003.
[50]
[49] S. Rehman, M. Shaﬁque, F. Kriebel, and J. Henkel, “Reliable software
for unreliable hardware: Embedded code generation aiming at reliability,”
in 9th IEEE/ACM Int. Conf. on HW/SW Codesign and Sys. Synth.
(CODES+ISSS ’11), Taipei, Taiwan, Oct. 2011, pp. 237–246.
I. Oz, H. R. Topcuoglu, M. Kandemir, and O. Tosun, “Examining thread
vulnerability analysis using fault-injection,” in 21st Int. Conf. on Very
Large Scale Integration (VLSI-SoC ’13).
IEEE, Oct. 2013, pp. 240–245.
[51] A. Shrivastava, A. Rhisheekesan, R. Jeyapaul, and C.-J. Wu, “Quantita-
tive analysis of control ﬂow checking mechanisms for soft errors,” in
51st Design Autom. Conf. (DAC ’14). ACM, 2014.
330330
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply.