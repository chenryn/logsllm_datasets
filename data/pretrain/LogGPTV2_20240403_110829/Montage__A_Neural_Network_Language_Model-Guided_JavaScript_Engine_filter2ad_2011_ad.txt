predict fragments from the test set as well. Thus, for the
remaining evaluations, we selected the model trained up to
epoch 70, which took 6.6 hours on our machine.
Figure 7: The pass rate of generated JS tests over ktop.
7.3 Effect of the ktop Parameter
Montage assembles model-suggested fragments when replac-
ing an AST subtree of a given JS code. In this process, Mon-
tage randomly picks one fragment from the Top-k (ktop) sug-
gestions for each insertion. Our key intuition is that selecting
fragments from the Top-k rather than Top-1 suggestion helps
Montage generate diverse code, which follows the pattern of
JS codes in our dataset but slightly differs from them. We
evaluated the effect of the ktop with seven different values
varying from 1 to 64 to verify our intuition.
We measured the pass rate of generated JS tests. A pass
rate is a measurement unit of demonstrating how many tests a
target JS engine executes without errors among generated test
cases. To measure the pass rate, we ﬁrst generated 100,000 JS
tests with each ktop value. We only considered ﬁve runtime
errors deﬁned by the ECMAScript standard as errors [27].
We then ran Montage for 12 hours with each ktop value to
count the number of crashes found in ChakraCore 1.4.1.
Figures 7 and 8 summarize our two experimental results,
respectively. As shown in Figure 7, the pass rate of Montage
decreases from 79.82% to 58.26% as the ktop increases. This
fact demonstrates that the suggestion from the model con-
siderably affects the generation of executable JS tests. It is
also consistent with the experimental results from Figure 8b,
in that Montage ﬁnds fewer total crashes when considering
more fragment suggestions in generating JS tests. Note that
Michael et al. [41] demonstrated that their TreeFuzz achieved
a 14% pass rate, which is signiﬁcantly lower than that Mon-
tage achieved.
However, note from Figure 8b that the number of unique
crashes increases, as ktop increases, unlike that of total crashes.
This observation supports our intuition that increasing the ktop
helps Montage generate diverse JS tests that trigger undesired
crashes in the JS engines. Figure 8 also shows that Montage
found more crashes from the debug build than the release
build. Moreover, unlike the debug build, the results for the re-
lease build did not show a consistent pattern. We believe these
results are mainly due to the nature of the debug build. It be-
haves more conservatively with inserted assertion statements,
thus producing crashes for every unexpected behavior.
As Klees et al. [31] stated, fuzzers should be evaluated
using the number of unique crashes, not that of crashing in-
puts. For both release and debug builds of ChakraCore 1.4.1,
2622    29th USENIX Security Symposium
USENIX Association
32641282560255075100EpochPerplexityTraining SetValidation Set0.150.200.250.300255075100EpochType ErrorTraining SetValidation Set6070801248163264Top k (ktop)Pass Rate (%)Table 1: The number of bugs found with four fuzzers and four different approaches: Montage, CodeAlchemist (CA), jsfunfuzz,
and IFuzzer; random selection, Markov chain, char/token-level RNN, and Montage (ktop = 64) without resolving reference errors.
We marked results in bold when the difference between Montage and the other approach is statistically signiﬁcant.
Build
Metric
Release
Debug
Median
Max
Min
Stdev
p-value
Median
Max
Min
Stdev
p-value
Montage
23 (7)
26 (8)
20 (6)
2.30
(0.84)
N/A
49 (12)
52 (15)
45 (11)
2.70
(1.64)
N/A
Both
Total
Common
133 (15)
36 (8)
# of Unique Crashes (Known CVEs)
CA
15 (4)
15 (4)
14 (3)
0.55
(0.55)
0.012
(0.012)
26 (6)
30 (6)
24 (4)
2.61
(0.89)
0.012
(0.012)
65 (7)
22 (2)
jsfunfuzz
IFuzzer
27 (3)
31 (4)
25 (3)
2.19
(0.45)
0.029
(0.012)
27 (4)
29 (5)
24 (4)
2.12
(0.45)
0.012
(0.012)
57 (4)
17 (3)
4 (1)
4 (2)
0 (0)
1.79
(0.71)
0.012
(0.012)
6 (1)
8 (3)
2 (0)
2.41
(1.10)
0.012
(0.012)
22 (3)
1 (0)
random
12 (3)
15 (4)
10 (3)
2.07
(0.45)
0.012
(0.012)
31 (7)
34 (7)
27 (6)
2.88
(0.45)
0.012
(0.012)
72 (9)
29 (6)
Markov
19 (6)
22 (7)
16 (5)
2.39
(0.84)
0.037
(0.144)
44 (11)
50 (12)
42 (8)
3.27
(1.67)
0.144
(0.298)
109 (14)
37 (8)
† Montage without resolving reference errors.
ch-RNN
Montage †
1 (0)
1 (1)
0 (0)
0.45
(0.55)
0.012
(0.012)
3 (0)
4 (1)
1 (0)
1.10
(0.5)
0.012
(0.012)
10 (2)
1 (0)
12 (4)
13 (5)
11 (4)
0.84
(0.45)
0.012
(0.012)
41 (9)
43 (10)
38 (8)
1.82
(0.84)
0.012
(0.012)
74 (10)
37 (7)
them the same dataset collected from the repositories of
Test262 and the four major JS engines. For fair comparison,
we also conﬁgured jsfunfuzz to be the version of January 31,
2017, on which we collected our dataset (recall §7.1).
We ran all four fuzzers on ChakraCore 1.4.1 and counted
the number of found unique crashes and known CVEs. Since
most fuzzers depend on random factors, which results in a
high variance of fuzzing results [31], we conducted ﬁve trials;
each trial lasted for 6,336 CPU hours (72 hours × 88 cores).
We intentionally chose such a long timeout, because fuzzers
using evolutionary algorithms, such as IFuzzer, could improve
their bug-ﬁnding ability as more tests are generated. Note that
we expended a total of 31,680 CPU hours on the ﬁve trials
of each fuzzer. Because Montage took 6.6 hours to train its
language model and used this model for the ﬁve trials, we set
the timeout of other fuzzers 1.3 hours (6.6 hours / 5 trials)
longer than that of Montage for fair comparison.
The Montage, CA, jsfunfuzz, and IFuzzer columns of Ta-
ble 1 summarize the statistical analysis of the comparison
experimental results. For the release build, Montage found the
largest number of CVEs, whereas jsfunfuzz still discovered
more unique crashes than others. For the debug build, Mon-
tage outperformed all others in ﬁnding both unique crashes
and CVEs. We performed two-tailed Mann Whitney U tests
and reported p-values between Montage and the other fuzzers
in the table. We veriﬁed that all results are statistically signiﬁ-
cant with p-values less than 0.05.
The last two rows of the table show the number of total and
common bugs found in the ﬁve trials from the release and
debug builds, respectively. We counted common bugs when
Montage found these bugs in every run of the ﬁve campaigns.
When a bug was found during at least one campaign, they are
(a) Crashes on the release build.
(b) Crashes on the debug build.
Figure 8: The number of total and unique crashes found in
ChakraCore 1.4.1 while varying the ktop.
Montage found the largest number of unique crashes when
the ktop was 64. Therefore, we picked the ktop to be 64 for the
remaining experiments.
7.4 Comparison to State-of-the-art Fuzzers
To verify the ability to ﬁnd bugs against open-source state-
of-the-art fuzzers, we compared Montage with CodeAl-
chemist [20], jsfunfuzz [38], and IFuzzer [54]. jsfunfuzz and
IFuzzer have been used as a controlled group in the compar-
ison studies [20, 24]. Furthermore, CodeAlchemist, which
assembles its building blocks in a semantics-aware fashion,
and IFuzzer, which employs an evolutionary approach with
genetic programming, have in common with Montage in that
they take in a corpus of JS tests. Since Montage, CodeAl-
chemist, and IFuzzer start from given seed JS ﬁles, we fed
USENIX Association
29th USENIX Security Symposium    2623
681012901201501801248163264Top k (ktop)# of Unique Crashes# of Total CrashesUniqueTotal2024283500400045005000550060001248163264Top k (ktop)# of Unique Crashes# of Total CrashesUniqueTotaljsfunfuzz
45 (1)
jsfunfuzz
12 (0)
8 (1)
3 (2)
1 (0)
105 (8)
17 (4)
44 (1)
1 (1)
24 (5)
1 (0)
10 (0)
3 (2)
8 (0)
Montage
CA
Montage
CA
(a) The # of total bugs.
(b) The # of common bugs.
Figure 9: The comparison of unique crashes (known CVEs)
found by Montage, CodeAlchemist (CA), and jsfunfuzz.
counted in the total bugs. Note that Montage found at least
2.14× more CVEs compared to others in a total of the ﬁve
trials. We believe that these results explain the signiﬁcance
of Montage in ﬁnding security bugs compared to the other
state-of-the-art fuzzers.
We also compared the bugs discovered by each fuzzer. Fig-
ure 9 depicts the Venn diagrams of unique bugs found in
ChakraCore 1.4.1. These Venn diagrams present the total and
common bugs that each fuzzer found, corresponding to the
last two rows of Table 1. We excluded IFuzzer from the ﬁgure
because all found CVEs were also discovered by Montage.
Note from Figure 9a that Montage identiﬁed 105 unique
crashes in total, including eight CVEs that were not found by
CodeAlchemist and jsfunfuzz. Furthermore, Montage discov-
ered all CVEs that were commonly found in the ﬁve trials of
CodeAlchemist and jsfunfuzz, as shown in Figure 9b. How-
ever, CodeAlchemist and jsfunfuzz also identiﬁed a total of
45 and 46 unique bugs that were not found by Montage, re-
spectively. These results demonstrate that Montage plays a
complementary role against the state-of-the-art fuzzers in
ﬁnding distinctive bugs.
Performance over time. Figure 10 shows the number of
CVEs that Montage found over time. The number increases
rapidly in the ﬁrst 1,144 CPU hours (13 hours × 88 cores)
of the fuzzing campaigns; however, Montage ﬁnds additional
bugs after running for 2,640 CPU hours (30 hours × 88 cores),
thus becoming slow to ﬁnd new vulnerabilities.
7.5 Effect of Language Models
Montage generates JS tests by assembling language model-
suggested fragments. Especially, it takes advantage of the
LSTM model to reﬂect the arbitrary length of preceding frag-
ments when predicting the next relevant fragments. However,
Montage can leverage any other prevailing language models
by its design, and the language model it employs may substan-
tially affect its fuzzing performance. Therefore, to analyze
the efﬁcacy of the LSTM model in ﬁnding bugs, we ﬁrst con-
ducted a comparison study against two other approaches: (1)
Figure 10: The number of CVEs found by Montage over time.