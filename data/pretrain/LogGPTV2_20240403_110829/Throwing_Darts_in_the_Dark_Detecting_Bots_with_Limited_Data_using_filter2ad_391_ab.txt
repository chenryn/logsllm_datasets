To generate IP-sequences, for each IP, we sort its requests
based on timestamps and process the requests as a stream.
Whenever we have accumulated T requests from this IP, we
produce an IP-sequence. In this paper, we empirically set T =
30. We perform bot detection on each IP-sequence.
C. Ground-truth Labels
Radware runs
We obtain the ground-truth labels from the CAPTCHA
system and the internal rule-based systems used in Radware.
Their security team also sampled both labels for manual
examination to ensure the reliability.
CAPTCHA Labels.
an advanced
CAPTCHA system for all its customers. The system delivers
CAPTCHAs to a subset of network requests. If the “user” fails
to solve the CAPTCHA, that speciﬁc request will be marked
with a ﬂag. For security reasons, Radware’s selection process
to deliver CAPTCHAs will not be made public. At a high level,
requests are selected based on proprietary methods that aim to
balance exploring the entire space of requests versus focusing
on requests that are more likely to be suspicious (hence
limiting impact on benign users). Given an IP-sequence, if one
of the requests is ﬂagged, we mark the IP-sequence as “bot”.
The security team has sampled the ﬂagged data to manually
verify the labels are reliable.
We are aware that certain CAPTCHA systems are vulner-
able to automated attacks by deep learning algorithms [33],
[34], [35], [36]. However, even the most advanced attack [34]
is not effective on all CAPTCHAs (e.g., Google’s CAPTCHA).
In addition, recent works show that adversarial CAPTCHAs
are effective against automated CAPTCHA-solving [37]. To
the best of our knowledge, the CAPTCHA system used by
Radware is not among the known vulnerable ones. Indeed,
the CAPTCHA system could still be bypassed by human-
efforts-based CAPTCHA farms [38]. On one hand, we argue
that human-based CAPTCHA solving already signiﬁcantly
increased the cost of bots (and reduced the attack scale). On
the other hand, we acknowledge that CAPTCHA does not
provide a complete “ground-truth”.
Rule-Based Labels. Another source of labels is Radware’s
internal rules. The rules are set to be conservative to achieve
near perfect precision while sacriﬁcing the recall (e.g., looking
for humanly-impossible click rate, and bot-like User-Agent
and referer). To avoid giving attackers the advantage, we
do not disclose the speciﬁc rules. Radware’s security team
has sampled the rule-labels for manual veriﬁcation to ensure
reliability. We also tried to validate the reliability on our side,
by examining whether rule-labels are indeed highly precise
(low or no false positives). We extract all the IP-sequences that
contain a rule-label, and examined how many of them have
received and solved a CAPTCHA. A user who can solve the
CAPTCHA is likely a false positive. The results are shown in
Table II. For example, for website A, the rules matched 42,487
IP-sequences. Among them, 38,294 sequences have received a
CAPTCHA, and only in 4 out of 38,294 (0.01%) users solved
the CAPTCHA. This conﬁrms the extremely high precision of
rules. As a trade-off, the rules missed many real bots, which
are further discussed in Section IV-A.
D. Problem Deﬁnition
In summary, we label an IP-sequence as “bot” if it failed
the CAPTCHA-solving or it triggered a rule (for those that
did not receive a CAPTCHA). Otherwise, the IP-sequence is
labeled as “benign”. Our goal is to classify bots from benign
trafﬁc accurately at the IP-sequence level with highly limited
labeled data. We are particularly interested in detecting bots
that bypassed the existing rule-based systems, i.e., advanced
bots. Note that our system is not redundant to the CAPTCHA
system, given that CAPTCHA can be only applied to a small
set of user requests to avoid hurting the user experience. Our
model can potentially improve the efﬁciency of CAPTCHA
delivery by pinpointing suspicious users for veriﬁcation.
Scope and Limitations.
Certain types of attackers are
out of scope. Attackers that hire human users to solve
CAPTCHAs [38] are not covered in our ground-truth. We
argue that pushing all attackers to human-based CAPTCHA-
solving would be one of the desired goals since it would
signiﬁcantly increase the cost of attacks and reduce the attack
speed (e.g., for spam, fraud, or data scraping).
IV. BASIC BOT DETECTION SYSTEM
In this section, we present the basic designs of the bot de-
tection system. More speciﬁcally, we want to build a machine
learning model to detect the advanced bots that bypassed the
existing rules. In the following, we ﬁrst ﬁlter out the simple
bots that can be captured by rules, and then describe our
stream-based bot detection model. In this section, we use all
the available training data to examine model performance. In
the next section, we introduce a novel data synthesis method
to detect bots with limited data (Section V).
As an overview, the data processing pipeline has two steps.
• Phase I: Applying existing rules to ﬁlter out the easy-to-
detect bots (pre-processing).
• Phase II: Using a machine learning model to detect the
“advanced bots” from the remaining data.
A. Phase I: Filtering Simple Bots
As discussed in Section III-C, Radware’s internal rules are
tuned to be highly precise (with a near 0 false positive rate).
As such, using a machine learning method to detect those
simple bots is redundant. The rule-based system, however, has
a low recall (e.g. 0.835 for website B and 0.729 for website
C, as shown in Table VI). This requires Phase II to detect the
advanced bots that bypassed the rules.
Table III shows the ﬁltering results. We do not consider IPs
that have fewer than T = 30 requests. The intuition is that, if
a bot made less than 30 requests in a given month, it is not a
threat to the service1. After ﬁltering out the simple bots, the
remaining advanced bots are those captured by CAPTCHAs.
For all three websites, we have more simple bots than ad-
vanced bots. The remaining data are treated as “benign”. The
benign sets are typically larger than the advanced bot sets, but
not orders of magnitude larger. This is because a large number
of benign IP-sequences have been ﬁltered out for having fewer
than 30 requests. Keeping those short benign sequences in our
dataset will only make the precision and recall look better, but
it does not reﬂect the performance in practice (i.e., detecting
these benign sequences is trivial).
B. Phase II: Machine Learning Model
With a focus on the advanced bots, we present the basic
design of our detector. The key novelty is not necessarily the
choice of deep neural network. Instead, it is the new feature
encoding scheme that can work on anonymized data across
services. In addition, we design the system to be stream-based,
which can process network requests as they come, and make
a decision whenever an IP-sequence is formed.
The goal of feature encoding is to convert the raw data
of an IP-sequence into a vector. Given an IP-sequence (of
30 requests), each request has a URL hash, timestamp, re-
ferrer hash, cookie ﬂag, and browser version hash. We tested
and found the existing encoding methods did not meet our
needs. For instance, one-hot encoding is a common way to
encode categorical features (e.g., URL, cookie). In our case,
because there are hundreds of thousands of distinct values
for speciﬁc features (e.g., hashed URLs), the encoding can
easily produce high-dimensional and sparse feature vectors.
Another popular choice is the embedding method such as
Word2Vec, which generates a low-dimensional representation
to capture semantic relationships among words for natural
language processing [43]. Word2Vec can be applied to process
network trafﬁc [44]: URLs that commonly appear at the same
position of a sequence will be embedded to vendors with a
smaller distance. Embedding methods are useful for ofﬂine
data processing, and is not suitable for a real-time system.
Word2Vec requires using a large and relatively stable dataset
1 We set T = 30 because the sequence length T needs to be reasonably
large to obtain meaningful patterns [39]. As a potential evasion strategy, an
attacker can send no more than 30 requests per IP, and uses a large number
of IPs (i.e., botnets). We argue that this will signiﬁcantly increase the cost of
the attacker. In addition, there are existing systems for detecting coordinated
botnet campaigns [40], [41], [42] which are complementary to our goals.
TABLE III: Ground-truth data of IP-sequences.
August 2018
January 2019
Website
Rule Matched
(Simple bot)
CAPTCHA
(Advanced bot)
A
B
C
42,487
23,346
50,394
6,117
2,677
19,113
Benign
15,390
48,578
32,613
Rule Matched
(Simple bot)
CAPTCHA
(Advanced bot)
30,178
10,434
-
4,245
2,794
-
Benign
10,393
26,922
-
Rule Matched
(Simple bot)
September 2019
CAPTCHA
(Advanced bot)
8,974
18,298
-
15,820
9,979
-
Benign
12,664
37,446
-
TABLE IV: Summaries of features and their encoding scheme.
Feature
URL
Referer
Browser version
Time gap
Cookie ﬂag
Encoding Method
Frequency Distribution encoding
Frequency Distribution encoding
Frequency Distribution encoding
Distribution encoding
Boolean
the past w days to estimate the URL occurrence distribution.
Another IP-sequence s2 is formed on day t too, and thus we
use the same time window to estimate the distribution. s3 is
formed one day later on t+1, and thus the time-window slides
forward by one day (keeping the same window size). In this
way, whenever an IP-sequence is formed, we can compute the
feature encoding immediately (using the most recent data).
In practice, we do not need to compute the distribution for
each new request. Instead, we only need to pre-compute the
distribution for each day, since IP-sequences on the same day
share the same window.
Table IV shows how different features are encoded. URL,
referer, and browser version are all categorical features and
thus can be encoded based on their occurrence frequency.
The “time gap” feature is the time gap between the current
request and the previous request in the same IP-sequence.
It is a numerical feature, and thus we can directly generate
the distribution to perform the encoding. The “cookie ﬂag”
boolean feature means whether the request has enabled a
cookie. Each request has 5 features, and each IP-sequence can
be represented by a matrix of 30 × 5 (dimension = 150).
Building the Classiﬁer. Using the above features, we build a
supervised Long-Short-Term-Memory (LSTM) classiﬁer [44].
LSTM is a specialized Recurrent Neural Network (RNN)
designed to capture the relationships of events in a sequence
and is suitable to model sequential data [46], [47]. Our model
contains 2 hidden LSTM layers followed by a binary classiﬁer.
The output dimension of every LSTM units in two layers
is 8. Intuitively, a wider neural network is more likely to
be overﬁtting [48], and a deeper network may have a better
generalizability but requires extensive resources for training.
A 2-8 LSTM model can achieve a decent balance between
overﬁtting and training costs. We have tested other models
such as Convolutional Neural Network (CNN), but LSTM
performs better when training data is limited (Appendix A).
C. Evaluating The Performance
We evaluate our model using data from August 2018 (ad-
vanced bots). We followed the recent guideline for evaluating
security-related ML models [49] to ensure result validity.
Fig. 1: Example of frequency encoding for the visited URL.
Fig. 2: Example of sliding window for feature encoding. S1 and S2
are IP sequences formed on day t, and S3 is formed on day t + 1.
The feature vendors are encoded using the past w days of data.
to generate a high quality embedding [45], but is not effective
for embedding new or rare entities. In our case, we do not
want to wait for months to collect the full training and testing
datasets for ofﬂine embedding and detection.
Sliding Window based Frequency Encoding. We propose
an encoding method that does not require the raw entity (e.g.,
URL) but uses the frequency of occurrence of the entity. The
encoding is performed in a sliding window to meet the need
for handling new/rare entities for real-time detection. We take
“visited URL” as an example to explain how it works.
As shown in Figure 1, given a request, we encode the
URL hash based on its occurrence frequency in the past. This
example URL appears very frequently in the past at a 95-
percentile. As such, we map the URL to an index value “0.95”.
In this way, URLs that share a similar occurrence frequency
will be encoded to a similar index number. This scheme
can easily handle new/rare instances: any previously-unseen
entities would be assigned to a low distribution percentile. We
also don’t need to manually divide the buckets but can rely
on the data distribution to generate the encoding automatically.
The feature value is already normalized between 0 and 1.
A key step of the encoding is to estimate the occurrence
frequency distribution of an entity. For stream-based bot
detection, we use a sliding window to estimate the distribution.
An example is shown in Figure 2. Suppose IP-sequence s1
is formed on day t (i.e., the last request arrived at day t).
To encode the URLs in s1, we use the historical data in
020406080100Request0.95CDF (%)URL Occurrence Freq.xyRequestURLRefererCookieTime...Raw DataFrequency EncodedTime (day)tt+1sliding window: w dayss3s2s1TABLE V: The detection results of LSTM model on “advanced bots”.
Website A
Precision Recall
0.952
0.880
F1
0.915
Website B
Precision Recall
0.877
0.888
F1
0.883
Website C
Precision Recall
0.730
0.789
F1
0.759
TABLE VI: The overall detection performance The “precision” and
“recall” are calculated based on all bots in August 2018 (simple bots
and advanced bots).
TABLE VII: F1-score when training with limited 1% of the labeled
data of the August 2018 dataset.
Website
1% Data (Avg + STD)
100% Data
A
B
C
0.904 ± 0.013
0.446 ± 0.305
0.697 ± 0.025
0.915
0.883
0.759
Setting
Rules Alone
Rules+LSTM 0.984
Website A
Precision Recall