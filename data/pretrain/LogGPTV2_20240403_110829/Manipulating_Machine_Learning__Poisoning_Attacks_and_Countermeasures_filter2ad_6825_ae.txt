(c) House Price Dataset
Fig. 3: MSE of attacks on ridge regression on the three datasets. Our new optimization (OptP) and statistical (StatP) attacks
are more effective than the baseline. OptP is best optimization attack according to Table I.
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 4: MSE of attacks on LASSO on the three datasets. As for ridge, we ﬁnd that StatP and OptP are able to poison the
dataset very effectively, outperforming the baseline (BGD). OptP is best optimization attack according to Table I.
optimization-based attacks (BGD and OptP) outperform the
statistical-based attack StatP in effectiveness. This is not
surprising to us, as StatP uses much less information about the
training process to determine the attack points. Interestingly,
we have one case (LASSO regression on loan dataset) in which
StatP outperforms the best optimization attack OptP by 11%.
There are also two instances on ridge regression (health and
loan datasets) in which StatP and OptP perform similarly.
These cases show that StatP is a reasonable attack when the
attacker has limited knowledge about the learning system.
The running time of optimization attacks is proportional
to the number of iterations required for convergence. On
the highest-dimensional dataset, house prices, we observe
OptP taking about 337 seconds to complete for ridge and
408 seconds for LASSO. On the loan dataset, OptP ﬁnishes
LASSO poisoning in 160 seconds on average. As expected,
the statistical attack is extremely fast, with running times on
the order of a tenth of a second on the house dataset and a
hundredth of a second on the loan dataset to generate the same
number of points as OptP. Therefore, our attacks exhibit clear
tradeoffs between effectiveness and running times, with opti-
mization attacks being more effective than statistical attacks,
at the expense of higher computational overhead.
3) Question 3: What is the potential damage of poisoning
in real applications?: We are interested in understanding the
effect of poisoning attacks in real applications, and perform a
case study on the health-care dataset. Speciﬁcally, we translate
the MSE results obtained with our attacks into application
speciﬁc parameters. In the health care application, the goal
is to predict medicine dosage for the anticoagulant drug
Warfarin. In Table IV, we show ﬁrst statistics on the medicine
dosage predicted by the original regression models (without
poisoning), and then the absolute difference in the amount of
dosage prescribed after the OptP poisoning attack. We ﬁnd
that all linear regression models are vulnerable to poisoning,
with 75% of patients having their dosage changed by 93.49%,
and half of patients having their dosage changed by 139.31%
on LASSO. For 10% of patients, the increase in MSE is
devastating to a maximum of 359% achieved for LASSO
regression. These results are for 20% poisoning rate, but it
turns out that the attacks are also effective at smaller poisoning
rates. For instance, at 8% poisoning rate, the change in dosage
is 75.06% for half of patients.
Thus, the results demonstrate the effectiveness of our new
poisoning attacks that induce signiﬁcant changes to the dosage
of most patients with a small percentage of poisoned points
added by the attacker.
4) Question 4: What are the transferability properties of
our attacks?: Our transferability analysis for poisoning attacks
is based on the black-box scenario discussed in Sect. II, in
28
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
Quantile
0.1
0.25
0.5
0.75
0.9
Initial
Dosage
15.5
21
30
41.53
52.5
Ridge
Diff
31.54%
87.50%
150.99%
274.18%
459.63%
LASSO
Diff
37.20%
93.49%
139.31%
224.08%
358.89%
TABLE IV: Initial dosage distribution (mg/wk) and percentage
difference between original and predicted dosage after OptP
attack at 20% poisoning rate (health care dataset).
which the attacker uses a substitute training set D(cid:3)
tr to craft
the poisoning samples, and then tests them against the targeted
model (trained on Dtr). Our results, averaged on 5 runs,
are detailed in Table V, which presents the ratio between
transferred and original attacks. Note that the effectiveness
of transferred attacks is very similar to that of the original
attacks, with some outliers on the house dataset. For instance,
the statistical attack StatP achieves transferred MSEs within
11.4% of the original ones. The transferred OptP attacks have
lower MSEs by 3% than the original attack on LASSO. At
the same time, transferred attacks could also improve the
effectiveness of the original attacks: by 30% for ridge, and
78% for LASSO. We conclude that, interestingly, our most
effective poisoning attacks (OptP and StatP) tend to have
good transferability properties. There are some exceptions
(ridge on house dataset), which deserve further investigation
in future work. In most cases the MSEs obtained when using
a different training set for both attacks is comparable to MSEs
obtained when the attack is mounted on the actual training set.
Summary of poisoning attack results.
• We introduce a new optimization framework for poison-
ing regression, which manages to improve upon BGD by
a factor of 6.83. The best OptP attack selects the initial-
ization strategy, optimization argument, and optimization
objective to achieve maximum MSEs.
• We ﬁnd that our statistical-based attack (StatP) works
reasonably well in poisoning all datasets and models, is
efﬁcient in running time, and needs minimal information
on the model. Our optimization-based attack OptP takes
longer to run, needs more information on the model, but
can be more effective in poisoning than StatP if properly
conﬁgured.
• In a health care case study, we ﬁnd that our OptP attack
can cause half of patients’ Warfarin dosages to change
by an average of 139.31%. One tenth of these patients
can have their dosages changed by 359%, demonstrating
the devastating consequences of poisoning.
• We ﬁnd that both our statistical and optimization attacks
have good transferability properties, and still perform
well with minimal difference in accuracy, when applied
to different training sets.
C. Defense algorithms
In this section, we evaluate our proposed TRIM defense and
other existing defenses from the literature (Huber, RANSAC,
29
Dataset
Health
Loan
House
Attack
OptP
StatP
OptP
StatP
OptP
StatP
LASSO
1.092
0.971
1.028
1.110
1.779
1.034
Ridge
1.301
0.927
1.100
0.989
0.479
0.886
TABLE V: Transferability of OptP and StatP attacks. Pre-
sented are the ratio of the MSE obtained with transferred
attacks over original attacks. Values below 1 represent original
attacks outperforming transferred attacks, while values above
1 represent transferred attacks outperforming original attacks.
Chen, and RONI) against the best performing optimization
attacks from the previous section (OptP). We test two well-
known methods from robust statistics: Huber regression [26]
and RANSAC [17], available as implementations in Python’s
sklearn package. Huber regression modiﬁes the loss function
from the standard MSE to reduce the impact of outliers. It does
this by using quadratic terms in the loss function for points
with small residuals and linear terms for points with large
residuals. The threshold where linear terms start being used is
tuned by a parameter  > 1, which we set by selecting the best
of 5 different values: {1.1, 1.25, 1.35, 1.5, 2}. RANSAC builds
a model on a random sample of the dataset, and computes the
number of points that are outliers from that model. If there are
too many outliers, the model is rejected and a new model is
computed on a different random dataset sample. The size of
the initial random sample is a parameter that requires tuning
- we select 5 different values, linearly interpolating from 25
to the total number of clean data, and select the value which
has the lowest MSE. If the number of outliers is smaller than
the number of poisoning points, we retain the model.
We also compare against our own implementation of the
robust regression method by Chen et al. [11] from the machine
learning community, and the RONI method from the security
community [39]. Chen picks the features of highest inﬂuence
using an outlier resilient dot product computation. We vary
the number of features selected by Chen (the only parameter
in the algorithm) between 1 and 9 and pick the best results.
We ﬁnd that Chen has highly variable performance, having
MSE increases of up to a factor of 63,087 over the no defense
models, and we decided to not include it in our graphs. The
poor performance of Chen is due to the strong assumptions
of the technique (sub-Gaussian feature distribution and covari-
ance matrix XT X = I.), that are not met by our real world
datasets. While we were able to remove the assumption that
all features had unit variance through robust scaling (using
the robust dot product provided by their work), removing
the covariance terms would require a robust matrix inversion,
which we consider beyond the scope of our work.
RONI (Reject On Negative Impact) was proposed in the
context of spam ﬁlters and attempts to identify outliers by
observing the performance of a model trained with and without
each point. If the performance degrades too much on a sampled
validation set (which may itself contain outliers), the point is
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 5: MSE of defenses on ridge on the three datasets. We exclude Chen from the graphs due to its large variability. Defenses
are evaluated against the OptP attack. The only defense that consistently performs well in these situations is our proposed
TRIM defense, with RANSAC, Huber, and RONI actually performing worse than the undefended model in some cases.
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 6: MSE of defenses on LASSO. We exclude Chen from the graphs due to its large variability. Defenses are evaluated
against the most effective attack OptP. As with ridge, the only defense that consistently performs well is our TRIM defense.
identiﬁed as an outlier and not included in the model. This
method has some success in the spam scenario due to the
ability of an adversary to send a single spam email with all
words in dictionary, but is not applicable in other settings in
which the impact of each point is small. We set the size of
the validation set to 50, and pick the best points on average
from 5 trials, as in the original paper. The size of the training
dataset is selected from the same values as RANSAC’s initial
sample size.
We show in Figures 5 and 6 MSEs for ridge and LASSO
regression for the original model (no defense), the TRIM al-
gorithm, as well as the Huber, RANSAC, and RONI methods.
We pose three research questions next:
1) Question 1: Are known methods effective at defending
against poisoning attacks?: As seen in Figures 5 and 6,
existing techniques (Huber regression, RANSAC, and RONI),
are not consistently effective at defending against our pre-
sented attacks. For instance, for ridge models, the OptP attack
increases MSE over unpoisoned models by a factor of 60.22
(on the house dataset). Rather than decreasing the MSE, Huber
regression in fact increases the MSE over undefended ridge
models by a factor of 3.28. RONI also increases the MSE of
undefended models by 18.11%. RANSAC is able to reduce
MSE, but it is still greater by a factor of 4.66 than that of the
original model. The reason for this poor performance is that
robust statistics methods are designed to remove or reduce
the effect of outliers from the data, while RONI can only
identify outliers with high impact on the trained models. Our
attacks generate inlier points that have similar distribution as
the training data, making these previous defenses ineffective.
2) Question 2: What is the robustness of the new defense
TRIM compared to known methods?: Our TRIM technique
is much more effective at defending against all attacks than
the existing techniques are. For ridge and LASSO regression,
TRIM’s MSE is within 1% of the original models in all cases.
Interestingly, on the house price dataset the MSE of TRIM is
lower by 6.42% compared to unpoisoned models for LASSO
regression. TRIM achieves MSEs much lower than existing
methods, improving Huber by a factor of 1295.45, RANSAC
by a factor of 75, and RONI by a factor of 71.13. This demon-
strates that the TRIM technique is a signiﬁcant improvement
over prior work at defending against these poisoning attacks.
3) Question 3: What is the performance of various defense
algorithms?: All of the defenses we evaluated ran in a
reasonable amount of time, but TRIM is the fastest. For
example, on the house dataset, TRIM took an average of 0.02
seconds, RANSAC took an average of 0.33 seconds, Huber
took an average of 7.86 seconds, RONI took an average of
15.69 seconds and Chen took an average of 0.83 seconds. On
the health care dataset, TRIM took an average of 0.02 seconds,
30
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
RANSAC took an average of 0.30 seconds, Huber took an
average of 0.37 seconds, RONI took an average of 14.80
seconds, and Chen took an average of 0.66 seconds. There
is some variance depending on the dataset and the number of
iterations to convergence, but TRIM is consistently faster than
other methods.
Summary of defense results.
• We ﬁnd that previous defenses (RANSAC, Huber, Chen,
and RONI) do not work very well against our poisoning
attacks. As seen in Figures 5-6, previous defenses can in
some cases increase the MSEs over unpoisoned models.
• Our proposed defense, TRIM, works very well and
signiﬁcantly improves the MSEs compared to existing
defenses. For all attacks, models, and datasets, the MSEs
of TRIM are within 1% of the unpoisoned model MSEs.
In some cases TRIM achieves lower MSEs than those of
unpoisoned models (by 6.42%).
• All of the defenses we tested ran reasonably quickly.
TRIM was the fastest, running in an average of 0.02
seconds on the house price dataset.
VI. RELATED WORK
The security of machine learning has received a lot of
attention in different communities (e.g., [2], [4], [14], [25],
[34]. Different types of attacks against machine learning al-
gorithms have been designed and analyzed, including evasion
attacks (e.g., [3], [8], [20], [42], [43], [49], [50]), and privacy
attacks (e.g., [18], [19], [47]). In poisoning attacks the attacker
manipulates or injects malicious data during training to cause
either availability attacks (inducing an effect on the trained
model) or targeted attacks (inducing an effect on speciﬁc data
points) [5], [25], [36], [38], [54].
In the security community, practical poisoning attacks have
been demonstrated in worm signature generation [41], [44],
spam ﬁlters [39], network trafﬁc analysis systems for detec-
tion of DoS attacks [46], sentiment analysis on social net-
works [40], crowdsourcing [53], and health-care [37]. In super-
vised learning settings, Newsome et al. [41] have proposed red
herring attacks that add spurious words (features) to reduce the
maliciousness score of an instance. These attacks work against
conjunctive and Bayes learners for worm signature generation.
Perdisci et al. [44] practically demonstrate how an attacker
can inject noise in the form of suspicious ﬂows to mislead
worm signature classiﬁcation. Nelson et al. [39] present both
availability and targeted poisoning attacks against the public
SpamBayes spam classiﬁer. Venkataraman et al. [52] analyze
the theoretical limits of poisoning attacks against signature
generation algorithms by proving bounds on false positives
and false negatives for certain adversarial capabilities.
In unsupervised settings, Rubinstein et al. [46] examined
how an attacker can systematically inject trafﬁc to mislead
a PCA anomaly detection system for DoS attacks. Kloft
and Laskov [31] demonstrated boiling frog attacks on cen-
troid anomaly detection that involve incremental contamina-
tion of systems using retraining. Theoretical online centroid
anomaly detection analysis has been discussed in [31]. Cio-