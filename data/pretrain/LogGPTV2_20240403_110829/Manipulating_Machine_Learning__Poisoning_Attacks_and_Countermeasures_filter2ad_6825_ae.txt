### (c) House Price Dataset
**Figure 3: MSE of Attacks on Ridge Regression on the Three Datasets.**
Our new optimization (OptP) and statistical (StatP) attacks are more effective than the baseline. According to Table I, OptP is the best optimization attack.

- **(a) Health Care Dataset**
- **(b) Loan Dataset**
- **(c) House Price Dataset**

**Figure 4: MSE of Attacks on LASSO on the Three Datasets.**
Similar to ridge regression, we find that StatP and OptP can poison the dataset very effectively, outperforming the baseline (BGD). According to Table I, OptP is the best optimization attack.

### Analysis of Attack Effectiveness
Optimization-based attacks (BGD and OptP) generally outperform the statistical-based attack (StatP) in effectiveness. This is not surprising, as StatP uses much less information about the training process to determine the attack points. Interestingly, there is one case (LASSO regression on the loan dataset) where StatP outperforms the best optimization attack (OptP) by 11%. Additionally, there are two instances in ridge regression (health and loan datasets) where StatP and OptP perform similarly. These cases show that StatP is a reasonable attack when the attacker has limited knowledge about the learning system.

### Running Time of Optimization Attacks
The running time of optimization attacks is proportional to the number of iterations required for convergence. On the highest-dimensional dataset (house prices), OptP takes approximately 337 seconds for ridge regression and 408 seconds for LASSO. On the loan dataset, OptP completes LASSO poisoning in an average of 160 seconds. As expected, the statistical attack (StatP) is extremely fast, with running times on the order of a tenth of a second on the house dataset and a hundredth of a second on the loan dataset to generate the same number of points as OptP. Therefore, our attacks exhibit clear trade-offs between effectiveness and running times, with optimization attacks being more effective but at the expense of higher computational overhead.

### Question 3: Potential Damage of Poisoning in Real Applications
We are interested in understanding the effect of poisoning attacks in real applications and perform a case study on the health-care dataset. Specifically, we translate the MSE results obtained with our attacks into application-specific parameters. In the health care application, the goal is to predict the dosage of the anticoagulant drug Warfarin. Table IV shows statistics on the medicine dosage predicted by the original regression models (without poisoning) and the absolute difference in the amount of dosage prescribed after the OptP poisoning attack.

- **Table IV: Initial Dosage Distribution (mg/wk) and Percentage Difference Between Original and Predicted Dosage After OptP Attack at 20% Poisoning Rate (Health Care Dataset)**
  - **Quantile** | **0.1** | **0.25** | **0.5** | **0.75** | **0.9**
  - **Initial Dosage** | 15.5 | 21 | 30 | 41.53 | 52.5
  - **Ridge Diff** | 31.54% | 87.50% | 150.99% | 274.18% | 459.63%
  - **LASSO Diff** | 37.20% | 93.49% | 139.31% | 224.08% | 358.89%

We find that all linear regression models are vulnerable to poisoning, with 75% of patients having their dosage changed by 93.49%, and half of the patients having their dosage changed by 139.31% on LASSO. For 10% of patients, the increase in MSE is devastating, reaching a maximum of 359% for LASSO regression. These results are for a 20% poisoning rate, but the attacks are also effective at smaller poisoning rates. For instance, at an 8% poisoning rate, the change in dosage is 75.06% for half of the patients.

### Question 4: Transferability Properties of Our Attacks
Our transferability analysis for poisoning attacks is based on the black-box scenario discussed in Section II, where the attacker uses a substitute training set \( D_{\text{tr}}^{\prime} \) to craft the poisoning samples and then tests them against the targeted model (trained on \( D_{\text{tr}} \)). Our results, averaged over 5 runs, are detailed in Table V, which presents the ratio between transferred and original attacks.

- **Table V: Transferability of OptP and StatP Attacks**
  - **Dataset** | **Health** | **Loan** | **House**
  - **Attack** | **OptP** | **StatP** | **OptP** | **StatP** | **OptP** | **StatP**
  - **LASSO** | 1.092 | 0.971 | 1.028 | 1.110 | 1.779 | 1.034
  - **Ridge** | 1.301 | 0.927 | 1.100 | 0.989 | 0.479 | 0.886

Note that the effectiveness of transferred attacks is very similar to that of the original attacks, with some outliers on the house dataset. For instance, the statistical attack (StatP) achieves transferred MSEs within 11.4% of the original ones. The transferred OptP attacks have lower MSEs by 3% than the original attack on LASSO. At the same time, transferred attacks could also improve the effectiveness of the original attacks: by 30% for ridge and 78% for LASSO. We conclude that, interestingly, our most effective poisoning attacks (OptP and StatP) tend to have good transferability properties. There are some exceptions (ridge on the house dataset), which deserve further investigation in future work. In most cases, the MSEs obtained when using a different training set for both attacks are comparable to those obtained when the attack is mounted on the actual training set.

### Summary of Poisoning Attack Results
- We introduce a new optimization framework for poisoning regression, which improves upon BGD by a factor of 6.83. The best OptP attack selects the initialization strategy, optimization argument, and optimization objective to achieve maximum MSEs.
- We find that our statistical-based attack (StatP) works reasonably well in poisoning all datasets and models, is efficient in running time, and needs minimal information on the model. Our optimization-based attack (OptP) takes longer to run, needs more information on the model, but can be more effective in poisoning than StatP if properly configured.
- In a health care case study, we find that our OptP attack can cause half of the patients' Warfarin dosages to change by an average of 139.31%. One-tenth of these patients can have their dosages changed by 359%, demonstrating the devastating consequences of poisoning.
- We find that both our statistical and optimization attacks have good transferability properties and still perform well with minimal differences in accuracy when applied to different training sets.

### C. Defense Algorithms
In this section, we evaluate our proposed TRIM defense and other existing defenses from the literature (Huber, RANSAC, Chen, and RONI) against the best-performing optimization attacks from the previous section (OptP).

- **Huber Regression**: Modifies the loss function from the standard MSE to reduce the impact of outliers. It uses quadratic terms for points with small residuals and linear terms for points with large residuals. The threshold for using linear terms is tuned by a parameter \(\epsilon > 1\), which we set by selecting the best of 5 different values: {1.1, 1.25, 1.35, 1.5, 2}.
- **RANSAC**: Builds a model on a random sample of the dataset and computes the number of points that are outliers from that model. If there are too many outliers, the model is rejected, and a new model is computed on a different random dataset sample. The size of the initial random sample is a parameter that requires tuning; we select 5 different values, linearly interpolating from 25 to the total number of clean data, and select the value with the lowest MSE.
- **Chen et al. [11]**: Picks the features of highest influence using an outlier-resilient dot product computation. We vary the number of features selected by Chen (the only parameter in the algorithm) between 1 and 9 and pick the best results. However, we find that Chen has highly variable performance, with MSE increases of up to a factor of 63,087 over the no-defense models, and we decided not to include it in our graphs.
- **RONI (Reject On Negative Impact)**: Identifies outliers by observing the performance of a model trained with and without each point. If the performance degrades too much on a sampled validation set (which may itself contain outliers), the point is identified as an outlier and not included in the model. We set the size of the validation set to 50 and pick the best points on average from 5 trials, as in the original paper.

### Figures 5 and 6: MSE of Defenses on Ridge and LASSO
- **Figure 5: MSE of Defenses on Ridge Regression on the Three Datasets**
- **Figure 6: MSE of Defenses on LASSO on the Three Datasets**

We exclude Chen from the graphs due to its large variability. Defenses are evaluated against the OptP attack. The only defense that consistently performs well in these situations is our proposed TRIM defense, with RANSAC, Huber, and RONI actually performing worse than the undefended model in some cases.

### Research Questions
1. **Question 1: Are Known Methods Effective at Defending Against Poisoning Attacks?**
   - As seen in Figures 5 and 6, existing techniques (Huber regression, RANSAC, and RONI) are not consistently effective at defending against our presented attacks. For instance, for ridge models, the OptP attack increases MSE over unpoisoned models by a factor of 60.22 (on the house dataset). Rather than decreasing the MSE, Huber regression increases the MSE over undefended ridge models by a factor of 3.28. RONI also increases the MSE of undefended models by 18.11%. RANSAC is able to reduce MSE, but it is still greater by a factor of 4.66 than that of the original model. The reason for this poor performance is that robust statistics methods are designed to remove or reduce the effect of outliers from the data, while RONI can only identify outliers with high impact on the trained models. Our attacks generate inlier points that have a similar distribution as the training data, making these previous defenses ineffective.

2. **Question 2: What is the Robustness of the New Defense TRIM Compared to Known Methods?**
   - Our TRIM technique is much more effective at defending against all attacks than the existing techniques are. For ridge and LASSO regression, TRIMâ€™s MSE is within 1% of the original models in all cases. Interestingly, on the house price dataset, the MSE of TRIM is lower by 6.42% compared to unpoisoned models for LASSO regression. TRIM achieves MSEs much lower than existing methods, improving Huber by a factor of 1295.45, RANSAC by a factor of 75, and RONI by a factor of 71.13. This demonstrates that the TRIM technique is a significant improvement over prior work at defending against these poisoning attacks.

3. **Question 3: What is the Performance of Various Defense Algorithms?**
   - All of the defenses we evaluated ran in a reasonable amount of time, but TRIM is the fastest. For example, on the house dataset, TRIM took an average of 0.02 seconds, RANSAC took an average of 0.33 seconds, Huber took an average of 7.86 seconds, RONI took an average of 15.69 seconds, and Chen took an average of 0.83 seconds. On the health care dataset, TRIM took an average of 0.02 seconds, RANSAC took an average of 0.30 seconds, Huber took an average of 0.37 seconds, RONI took an average of 14.80 seconds, and Chen took an average of 0.66 seconds. There is some variance depending on the dataset and the number of iterations to convergence, but TRIM is consistently faster than other methods.

### Summary of Defense Results
- We find that previous defenses (RANSAC, Huber, Chen, and RONI) do not work very well against our poisoning attacks. As seen in Figures 5 and 6, previous defenses can, in some cases, increase the MSEs over unpoisoned models.
- Our proposed defense, TRIM, works very well and significantly improves the MSEs compared to existing defenses. For all attacks, models, and datasets, the MSEs of TRIM are within 1% of the unpoisoned model MSEs. In some cases, TRIM achieves lower MSEs than those of unpoisoned models (by 6.42%).
- All of the defenses we tested ran reasonably quickly. TRIM was the fastest, running in an average of 0.02 seconds on the house price dataset.

### VI. Related Work
The security of machine learning has received a lot of attention in different communities. Different types of attacks against machine learning algorithms have been designed and analyzed, including evasion attacks and privacy attacks. In poisoning attacks, the attacker manipulates or injects malicious data during training to cause either availability attacks (inducing an effect on the trained model) or targeted attacks (inducing an effect on specific data points).

In the security community, practical poisoning attacks have been demonstrated in various applications, such as worm signature generation, spam filters, network traffic analysis systems, sentiment analysis on social networks, crowdsourcing, and health care. In supervised learning settings, Newsome et al. [41] have proposed red herring attacks that add spurious words (features) to reduce the maliciousness score of an instance. These attacks work against conjunctive and Bayes learners for worm signature generation. Perdisci et al. [44] practically demonstrate how an attacker can inject noise in the form of suspicious flows to mislead worm signature classification. Nelson et al. [39] present both availability and targeted poisoning attacks against the public SpamBayes spam classifier. Venkataraman et al. [52] analyze the theoretical limits of poisoning attacks against signature generation algorithms by proving bounds on false positives and false negatives for certain adversarial capabilities.

In unsupervised settings, Rubinstein et al. [46] examined how an attacker can systematically inject traffic to mislead a PCA anomaly detection system for DoS attacks. Kloft and Laskov [31] demonstrated boiling frog attacks on centroid anomaly detection that involve incremental contamination of systems using retraining. Theoretical online centroid anomaly detection analysis has been discussed in [31].