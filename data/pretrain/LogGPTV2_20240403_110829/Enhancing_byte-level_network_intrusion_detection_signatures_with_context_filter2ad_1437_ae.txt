Bro takes about the same amount of time. Bro with a limited state
cache needs roughly a factor of 2.2 more time.
We might think that the discrepancy between Bro operating with
a limited DFA state cache and it operating with unlimited DFA state
memory is due to it having to spend considerable time recomputing
states previously expired from the limited cache. This, however,
turns out not to be the case. Additional experiments with essentially
inﬁnite cache sizes indicate that the performance decrease is due to
the additional overhead of maintaining the cache.
While this looks like a signiﬁcant impact, we note that it is not
clear whether the space savings of a cache is in fact needed in opera-
tional use. For this trace, only 2,669 DFA states had to be computed,
totaling roughly 10MB. When running Bro operationally for a day
at the university’s gateway, the number of states rapidly climbs to
about 2,500 in the ﬁrst hour, but then from that point on only slowly
rises to a bit over 4,000 by the end of the day.
A remaining question, however, is whether an attacker could cre-
ate trafﬁc speciﬁcally tailored to enlarge the DFAs (a “state-holding”
attack on the IDS), perhaps by sending a stream of packets that
nearly trigger each of the different patterns. Additional research
is needed to further evaluate this threat.
Comparing for USB-Full the alerts generated by Snort to the
signature matches reported by Bro, all in all we ﬁnd very good
agreement. The main difference is the way they report a match.
By design, Bro reports all matching signatures, but each one only
once per connection. This is similar to the approach suggested
in [10]. Snort, on the other hand, reports the ﬁrst matching sig-
nature for each packet, independently of the connection it belongs
4This latter ﬁgure corresponds to about 35,000 packets per second,
though we strongly argue that measuring performance in PPS rates
implies undue generality, since, as developed above, the speciﬁcs of
the packets make a great difference in the results.
to. This makes it difﬁcult to compare the matches. We account
for these difference by comparing connections for which at least
one match is generated by either system. With USB-Full, we get
2,065 matches by Bro in total on 1,313 connections. Snort reports
4,147 alerts. When counting each alert only once per connection,
Snort produces 1,320 on 1,305 connections.5 There are 1,296 con-
nections for which both generate at least one alert, and 17 (9) for
which Bro (Snort) reports a match but not Snort (Bro).
Looking at individual signatures, we see that Bro misses 10
matches of Snort. 5 of them are caused by Snort ID #1013 (WEB-
IIS fpcount access). The corresponding connections con-
tain several requests, but an idle time larger than the deﬁned in-
activity timeout of 30 seconds. Therefore, Bro ﬂushes the
state before it can encounter the match which would happen later
in the session. On the other hand, Bro reports 41 signature matches
for connections for which Snort does not report anything. 37 of
them are Web signatures. The discrepancy is due to different TCP
stream semantics. Bro and Snort have slightly different deﬁnitions
of when a session is established. In addition, the semantic differ-
ences between stream-wise and packet-wise matching discussed in
§4.2 cause some of the additional alerts.
Figure 7: Run-time comparison on 550MHz Pentium-3
Runtime for USB−Full on Pentium−3
Bro w/o state cache
Bro w/ state cache
Snort
Snort [FV01]
Snort patched
s
d
n
o
c
e
S
0
0
0
2
0
0
5
1
0
0
0
1
0
0
5
0
0
5
10
15
20
25
30
Trace length (mins)
We have done similar measurements with LBL-Web. Due to lim-
ited space, we omit the corresponding plot here. While the original
Snort takes 440 CPU seconds for the trace, Bro without (with) a lim-
ited state cache needs 280 (328) CPU seconds, and Snort as modi-
ﬁed by us needs only 164 CPU seconds. While this suggests room
for improvement in some of Bro’s internal data structures, Bro’s
matcher still compares quite well to the typical Snort conﬁguration.
For this trace, Bro (Snort) reports 2,764 (2,049) matches in total.
If we count Snort’s alerts only once per connection, there are 1,472
of them. There are 1,395 connections for which both report at least
one alert. For 133 (69) connections, Bro (Snort) reports a match
but Snort (Bro) does not. Again, looking at individual signatures,
Bro misses 73 of Snort’s alerts. 25 of them are matches of Snort
signature #1287 (WEB-IIS scripts access). These are all
caused by the same host. The reason is packets missing from the
trace, which, due to a lack of in-order sequencing, prevent the TCP
stream from being reassembled by Bro. Another 19 are due to sig-
nature #1287 (CodeRed v2 root.exe access). The ones of
these we inspected further were due to premature server-side resets,
which Bro correctly identiﬁes as the end of the corresponding con-
nections, while Snort keeps matching on the trafﬁc still being send
by the client. Bro reports 186 signature matches for connections for
which Snort does not report a match at all. 68 of these connections
simultaneously trigger three signatures (#1002, #1113, #1287). 46
5Most of the duplicates are ICMP Destination Unreach-
able messages. Using Bro’s terminology, we deﬁne all ICMP
packets between two hosts as belonging to one “connection.”
are due to simultaneous matches of signatures #1087 and #1242.
Looking at some of them, one reason is SYN-packets missing from
the trace. Their absence leads to different interpretations of estab-
lished sessions by Snort and Bro, and therefore to different matches.
5. CONCLUSIONS
In this work, we develop the general notion of contextual sig-
natures as an improvement on the traditional form of string-based
signature-matching used by NIDS. Rather than matching ﬁxed
strings in isolation, contextual signatures augment the matching pro-
cess with both low-level context, by using regular expressions for
matching rather than simply ﬁxed strings, and high-level context,
by taking advantage of the rich, additional semantic context made
available by Bro’s protocol analysis and scripting language.
By tightly integrating the new signature engine into Bro’s event-
based architecture, we achieve several major improvements over
other signature-based NIDSs such as Snort, which frequently suf-
fer from generating a huge number of alerts. By interpreting a
signature-match only as an event, rather than as an alert by itself,
we are able to leverage Bro’s context and state-management mech-
anisms to improve the quality of alerts. We showed several exam-
ples of the power of this approach: matching requests with replies,
recognizing exploit scans, making use of vulnerabilty proﬁles, and
deﬁning dependencies between signatures to model attacks that span
multiple connections. In addition, by converting the freely available
signature set of Snort into Bro’s language, we are able to build upon
existing community efforts.
As a baseline, we evaluated our signature engine using Snort as
a reference, comparing the two systems in terms of both run-time
performance and generated alerts using the signature set archived
at [2]. But in the process of doing so, we encountered several gen-
eral problems when comparing NIDSs: differing internal semantics,
incompatible tuning options, the difﬁculty of devising “representa-
tive” input, and extreme sensitivity to hardware particulars. The last
two are particularly challenging, because there are no a priori indi-
cations when comparing performance on one particular trace and
hardware platform that we might obtain very different results using
a different trace or hardware platform. Thus, we must exercise great
caution in interpreting comparisons between NIDSs.
Based on this work, we are now in the process of deploying Bro’s
contextual signatures operationally in several educational, research
and commercial enviroments.
Finally, we have integrated our work into version 0.8 of the Bro
distribution, freely available at [5].
6. ACKNOWLEDGMENTS
We would like to thank the Lawrence Berkeley National Labora-
tory (LBL), Berkeley, USA; the National Energy Research Scien-
tiﬁc Computing Center (NERSC), Berkeley, USA; and the Saarland
University, Germany. We are in debt to Anja Feldmann for making
this work possible. Finally, we would like to thank the anonymous
reviewers for their valuable suggestions.
7. REFERENCES
[1] arachNIDS. http://whitehats.com/ids/.
[2] Web archive of versions of software and signatures used in this paper.
http://www.net.in.tum.de/˜robin/ccs03.
[3] S. Axelsson. The base-rate fallacy and the difﬁculty of intrusion detection.
ACM Transactions on Information and System Security, 3(3):186–205, August
2000.
[4] R. G. Bace. Intrusion Detection. Macmillan Technical Publishing,
Indianapolis, IN, USA, 2000.
[5] Bro: A System for Detecting Network Intruders in Real-Time.
http://www.icir.org/vern/bro-info.html.
[6] Bugtraq. http://www.securityfocus.com/bid/1187.
[7] CERT Advisory CA-2002-27 Apache/mod ssl Worm.
http://www.cert.org/advisories/CA-2002-27.html.
[8] C. J. Coit, S. Staniford, and J. McAlerney. Towards Faster Pattern Matching for
Intrusion Detection or Exceeding the Speed of Snort. In Proc. 2nd DARPA
Information Survivability Conference and Exposition, June 2001.
[9] Common Vulnerabilities and Exposures. http://www.cve.mitre.org.
[10] H. Debar and B. Morin. Evaluation of the Diagnostic Capabilities of
Commercial Intrusion Detection Systems. In Proc. Recent Advances in
Intrusion Detection, number 2516 in Lecture Notes in Computer Science.
Springer-Verlag, 2002.
[11] R. F. et. al. Hypertext transfer protocol – http/1.1. Request for Comments 2616,
June 1999.
[12] M. Fisk and G. Varghese. Fast Content-Based Packet Handling for Intrusion
Detection. Technical Report CS2001-0670, UC San Diego, May 2001.
[13] Fyodor. Remote OS detection via TCP/IP Stack Finger Printing. Phrack
Magazine, 8(54), 1998.
[14] J. Haines, L. Rossey, R. Lippmann, and R. Cunnigham. Extending the 1999
Evaluation. In Proc. 2nd DARPA Information Survivability Conference and
Exposition, June 2001.
[15] M. Hall and K. Wiley. Capacity Veriﬁcation for High Speed Network Intrusion
Detection Systems. In Proc. Recent Advances in Intrusion Detection, number
2516 in Lecture Notes in Computer Science. Springer-Verlag, 2002.
[16] M. Handley, C. Kreibich, and V. Paxson. Network intrusion detection: Evasion,
trafﬁc normalization, and end-to-end protocol semantics. In Proc. 10th
USENIX Security Symposium, Washington, D.C., August 2001.
[17] J. Heering, P. Klint, and J. Rekers. Incremental generation of lexical scanners.
ACM Transactions on Programming Languages and Systems (TOPLAS),
14(4):490–520, 1992.
[18] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Languages,
and Computation. Addison Wesley, 1979.
[19] K. Jackson. Intrusion detection system product survey. Technical Report
LA-UR-99-3883, Los Alamos National Laboratory, June 1999.
[20] U. Lindqvist and P. A. Porras. Detecting computer and network misuse through
the production-based expert system toolset (P-BEST). In Proc. IEEE
Symposium on Security and Privacy. IEEE Computer Society Press, May 1999.
[21] R. Lippmann, R. K. Cunningham, D. J. Fried, I. Graf, K. R. Kendall, S. E.
Webster, and M. A. Zissman. Results of the 1998 DARPA Ofﬂine Intrusion
Detection Evaluation. In Proc. Recent Advances in Intrusion Detection, 1999.
[22] R. Lippmann, J. W. Haines, D. J. Fried, J. Korba, and K. Das. The 1999
DARPA off-line intrusion detection evaluation. Computer Networks,
34(4):579–595, October 2000.
[23] R. Lippmann, S. Webster, and D. Stetson. The Effect of Identifying
Vulnerabilities and Patching Software on the Utility of Network Intrusion
Detection. In Proc. Recent Advances in Intrusion Detection, number 2516 in
Lecture Notes in Computer Science. Springer-Verlag, 2002.
[24] J. McHugh. Testing Intrusion detection systems: A critique of the 1998 and
1999 DARPA intrusion detection system evaluations as performed by Lincoln
Laboratory. ACM Transactions on Information and System Security,
3(4):262–294, November 2000.
[25] V. Paxson. Bro: A system for detecting network intruders in real-time.
Computer Networks, 31(23–24):2435–2463, 1999.
[26] P. A. Porras and P. G. Neumann. EMERALD: Event monitoring enabling
responses to anomalous live disturbances. In National Information Systems
Security Conference, Baltimore, MD, October 1997.
[27] T. H. Ptacek and T. N. Newsham. Insertion, evasion, and denial of service:
Eluding network intrusion detection. Technical report, Secure Networks, Inc.,
January 1998.
[28] M. J. Ranum, K. Landﬁeld, M. Stolarchuk, M. Sienkiewicz, A. Lambeth, and
E. Wall. Implementing a generalized tool for network monitoring. In Proc. 11th
Systems Administration Conference (LISA), 1997.
[29] M. Roesch. Snort: Lightweight intrusion detection for networks. In Proc. 13th
Systems Administration Conference (LISA), pages 229–238. USENIX
Association, November 1999.
[30] R. Sekar and P. Uppuluri. Synthesizing fast intrusion prevention/detection
systems from high-level speciﬁcations. In Proc. 8th USENIX Security
Symposium. USENIX Association, August 1999.
[31] U. Shankar and V. Paxson. Active Mapping: Resisting NIDS Evasion Without
Altering Trafﬁc. In Proc. IEEE Symposium on Security and Privacy, 2003.
[32] Steven T. Eckmann. Translating Snort rules to STATL scenarios. In Proc.
Recent Advances in Intrusion Detection, October 2001.
[33] tcpdump. http://www.tcpdump.org.
[34] Valgrind. http://developer.kde.org/˜sewardj.
[35] G. Vigna, S. Eckmann, and R. Kemmerer. The STAT Tool Suite. In Proc. 1st
DARPA Information Survivability Conference and Exposition, Hilton Head,
South Carolina, January 2000. IEEE Computer Society Press.
[36] G. Vigna and R. A. Kemmerer. Netstat: A network-based intrusion detection
system. Journal of Computer Security, 7(1):37–71, 1999.
[37] Whisker. http://www.wiretrip.net/rfp.