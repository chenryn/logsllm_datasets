high until we turned on admission control on hosts.
Figure (cid:134): Reduction in TCP packet loss a(cid:22)er BwE was de-
ployed. Y-axis denotes packet loss in percentage. Di(cid:242)erent
lines correspond to di(cid:242)erent QoS classes (BEˇ denoting best
e(cid:242)ort, and AFˇ/AF(cid:134) denoting higher QoS classes.)
3. ABSTRACTIONS AND CONCEPTS
3.1 Trafﬁc Aggregates or FlowGroups
Individual services or users run jobs consisting of multiple
tasks. Each task may contain multiple Linux processes and
runs in a Linux container that provides resource accounting,
isolation and information about user_name, job_name and
task_name. We modißed the Linux networking stack to mark
the per-packet socket bu(cid:242)er structure to uniquely identify the
originating container running the task. his allows BwE to
distinguish between traıc from di(cid:242)erent tasks running on
the same host machine.
BwE further classißes task traıc based on destination clus-
ter address. Optionally, tasks use setsockopt() to indicate
other classißcation information, e.g. for bandwidth delega-
tion. Delegation allows a task belonging to a shared infras-
tructure service to attribute its bandwidth to the user whose
request caused it to generate traıc. For example, a copy ser-
vice can delegate bandwidth charges for a specißc ßle transfer
to the user requesting the transfer.
For scalability, baseline TCP regulates bandwidth for most
application (cid:6)ows. BwE dynamically selects the subset of (cid:6)ows
accounting for most of the demand to enforce. Using TCP
as the baseline also provides a convenient fallback for band-
width allocation in the face of a BwE system failure.
BwE allocates bandwidth among FlowGroups at various
granularities, deßned below.
job_name,
task_name,
user_name,
source_cluster,
of bandwidth measurement and enforcement.
task-fgs belonging to the same job is aggregated into
● Task FlowGroup or task-fg:
. his FlowGroup is the ßnest unit
● Job FlowGroup or job-fg: Bandwidth usage across all
a job-fg: .
● User FlowGroup or user-fg: Bandwidth usage across
gated into a user-fg: .
● Cluster FlowGroup or cluster-fg: Bandwidth usage
. he user_aggregate corresponds
● Site FlowGroup or site-fg: Bandwidth usage for cluster-
site-fg:.
fgs belonging to the same site-pair is combined into a
BwE creates a set of trees of FlowGroups with parent-child
relationships starting with site-fg at the root to cluster-fg, user-
fg, job-fg and eventually task-fg at the leaf. We measure band-
width usage at task-fg granularity in the host and aggregate
to the site-fg level. BwE estimates demand (Section E.ˇ) for
each FlowGroup based on its historical usage. BwE allocates
bandwidth to site-fgs, which is redistributed down to task-fgs
and enforced in the host kernel. Beyond rate limiting, the hi-
erarchy can also be used to perform other actions on a (cid:6)ow
group such as DSCP remarking. All measurements and rate
limiting are done on packet transmit.
BwE policies are deßned at site-fg and user-fg level. Mea-
surement and enforcement happen at task-fg level. Other lev-
els are required to scale the system by enabling distributed
execution of BwE across multiple machines in Google data-
centers.
3.2 Bandwidth Sharing Policies
3.2.1 Requirements
Our WAN (Figure ˇ) is divided in two levels, the inter-
site network and the inter-cluster network. he links in the
inter-site network (l;, l˚ and l(cid:149) in the ßgure) are the most
expensive. Aggregated demands on these links are easier
to predict. Hence, our WAN is provisioned at the inter-
site network. Product groups (user_aggregates) create band-
width requirements for each site-pair. For a site-pair, de-
pending on the network capacity and its business priority,
each user_aggregate gets approved bandwidth at several allo-
cation levels. Allocation levels are in strict priority order, ex,
Guaranteed allocation level should be fully satisßed before al-
locating to Best-E(cid:242)ort allocation level. Allocated bandwidth
of a user_aggregate for a site-pair is further divided to all its
member users.
Even though provisioning and sharing of the inter-site net-
work is the most important, several links not in the inter-site
network may also get congested and there is a need to share
their bandwidth fairly during congestion. We assign weights
to the users that are used to subdivide their user_aggregate’s
allocated bandwidth in the inter-cluster network. To allow
more ßne grained control, we allow weights to change based
on allocated bandwidth as well as to be overridden to a non-
default value for some cluster-pairs.
3.2.2 Conﬁguration
Network administrators conßgure BwE sharing policies
through centralized conßguration. BwE conßguration spec-
ißes a ßxed number of strict priority allocation levels, e.g.,
there may be two levels corresponding to Guaranteed and
Best-E(cid:242)ort traıc.
(a) f gˇ
Weight
«
(cid:134)«
(cid:1)
Allocation
Level
Guaranteed
Best-Effort
Bandwidth
(Gbps)
«
ˇ«
∞
Allocation
Level
Guaranteed
Best-Effort
(b) f g(cid:134)
Weight
ˇ«
ˇ«
Bandwidth
(Gbps)
ˇ«
∞
Table ˇ: BwE Conßguration Example.
(a) f gˇ
(b) f g(cid:134)
Figure t: Example Bandwidth Functions.
he BwE conßguration maps users to user_aggregates.
Mapping from user-fg to site-fg can be derived from this. he
BwE conßguration policies describe how site-fgs share the
network and also describe how user-fgs within a site-fg share
bandwidth allocated to the site-fg. For all FlowGroups in a
level of hierarchy, the BwE conßguration deßnes: ˇ) band-
width for each allocation level and (cid:134)) within each allocation
level, weight of the FlowGroup that can change based on allo-
cated bandwidth. An example of a BwE conßguration for the
relative priority for two FlowGroups, f gˇ and f g(cid:134) is shown
in Table ˇ.
3.2.3 Bandwidth Functions
he conßgured sharing policies are represented inside BwE
as bandwidth functionsˇ. A bandwidth function [ˇ;] specißes
the bandwidth allocation to a FlowGroup as a function of
its relative priority on an arbitrary, dimensionless measure of
available fair share capacity, which we call fair share. fair share
is an abstract measure and is only used for internal computa-
tion by the allocation algorithm. Based on the conßg, every
site-fg and user-fg is assigned a piece-wise linear monotonic
bandwidth function (e.g. Figure t). It is capped at the dynamic
estimated demand (Section E.ˇ) of the FlowGroup. hey can
also be aggregated to create bandwidth functions at the higher
levels (Section t.(cid:134).(cid:16)).
he fair share dimension can be partitioned into regions
(corresponding to allocation levels in the BwE conßgura-
tion) of strict priority. Within each region, the slope(cid:134) of a
FlowGroup’s bandwidth function deßnes its relative priority
or weight. Once the bandwidth reaches the maximum ap-
proved for the FlowGroup in a region, the bandwidth function
(cid:6)attens (« slope) until the start of the next region. Once the
ˇBandwidth functions are similar to utility functions [˚, E]
except that these are derived from static conßgured pol-
icy (Section t.(cid:134)) indicating network fair share rather than
application-specißed utility as a function of allocated band-
width.
(cid:134)Slope can be a multiple of weight as long as the same multi-
ple is used for all FlowGroups.
 0 5 10 15 20 250.00.51.01.52.02.53.03.54.04.55.0Bandwidth (Gbps)Fair Share 0 5 10 15 20 250.00.51.01.52.02.53.03.54.04.55.0Bandwidth (Gbps)Fair Share4Figure (cid:16): Bandwidth Sharing on a Bottleneck Link.
bandwidth function reaches the FlowGroup’s estimated de-
mand, it becomes (cid:6)at from that point for all the following
regions.
Figure t shows example bandwidth functions for two Flow-
Groups, f gˇ and f g(cid:134), based on BwE conßguration as deßned
in Table ˇ. here are two regions of fair share: Guaranteed («-
(cid:134)) and Best-E(cid:242)ort ((cid:134)-∞). he endpoints for each region are
system-level constants deßned in BwE conßguration. BwE’s
estimated demand of f gˇ is ˇ(cid:1)Gbps and hence, its bandwidth
function (cid:6)attens past that point. Similarly, f g(cid:134)’s estimated de-
mand is (cid:134)«Gbps.
We present a scenario where f gˇ and f g(cid:134) are sharing one
constrained link in the network. he goal of the BwE algo-
rithm is to allocate the bandwidth of the constrained link
such the following constraints are satisßed: ˇ) f gˇ and f g(cid:134) get
maximum possible but equal fair share, and (cid:134)) sum of their al-
located bandwidth corresponding to the allocated fair share
is less than or equal to the available bandwidth of the link.
Figure (cid:16) shows the output of the BwE allocation algorithm
(Section (cid:1).t) with varying link’s available bandwidth shown
on the x-axis. he allocated fair share to the FlowGroups is
shown on the right y-axis and the corresponding bandwidth
allocated to the FlowGroups is shown on the le(cid:22) y-axis. Note
that the constraints above are always satisßed at each snap-
shot of link’s available bandwidth. One can verify using this
graph that the prioritization as deßned by Table ˇ is respected.
One of BwE’s principal responsibilities is to dynamically
determine the level of contention for a particular resource
(bandwidth) and to then assign the resource to all compet-
ing FlowGroups based on current contention. Higher val-
ues of fair share indicate lower levels of resource contention
and correspondingly higher levels of bandwidth that can po-
tentially be assigned to a FlowGroup. Actual consumption
is capped by current FlowGroup estimated demand, making
the allocation work-conserving (do not waste any available
bandwidth if there is demand).
he objective of BwE is the max-min fair [E] allocation of
fair share to competing site-fgs and then the max-min fair
allocation of fair share to user-fgs within a site-fg. For each
user-fg, maximize the utilization of the allocated bandwidth
to the user-fg by subdividing it to the lower levels of hierar-
Figure (cid:1): BwE Architecture.
chy (job-fgs and task-fgs) equally (no weights) based on their
estimated demands.
3.2.4 Bandwidth Function Aggregation
Bandwidth Functions can be aggregated from one Flow-
Group level to another higher level. We require such aggre-
gation when input conßguration deßnes a bandwidth func-
tion at a ßner granularity, but the BwE algorithm runs over
coarser granularity FlowGroups. For example, BwE’s input
conßguration provides bandwidth function at user-fg level,
while BwE (Section (cid:1).ˇ) runs across cluster-fgs. In this case,
we aggregate user-fgs bandwidth functions to create a cluster-
fg bandwidth function. We create aggregated bandwidth func-
tions for a FlowGroup by adding bandwidth value for each
value of fair share for all its children.
4. SYSTEM DESIGN
BwE consists of a hierarchy of components that aggregate
network usage statistics and enforce bandwidth allocations.
BwE obtains topology and other network state from a net-
work model server and bandwidth sharing policies from an
administrator-specißed conßguration. Figure (cid:1) shows the
functional components in BwE.
4.1 Host Enforcer
At the lowest level of the BwE hierarchy, the Host Enforcer
runs as a user space daemon on end hosts. Every ßve sec-
onds, it reports bandwidth usage of local application’s tasks-
fgs to the Job Enforcer. In response, it receives bandwidth
allocations for its task-fgs from the Job Enforcer. he Host
Enforcer collects measurements and enforces bandwidth al-
locations using the HTB (Hierarchical Token Bucket) queu-
ing discipline in Linux.
4.2 Job Enforcer
Job Enforcers aggregate usages from task-fgs to job-fgs and
report job-fgs’ usages every ˇ« seconds to the Cluster En-
forcer. In response, the Job Enforcer receives job-fgs’ band-
width allocations from the Cluster Enforcer. he Job Enforcer
ensures that for each job-fg, bandwidth usage does not ex-
0.02.55.07.510.012.515.017.520.022.525.00.02.55.07.510.012.515.017.520.022.525.027.530.032.535.037.540.00.00.51.01.52.02.53.03.54.04.55.0Allocated Bandwidth (Gbps)Fair ShareLink Available Bandwidth (Gbps)fg1(left)fg2(left)fair share(right)5ceed its assigned allocation. To do so, it redistributes the as-
signed job-fg allocation among the constituent task-fgs using
the WaterFill algorithm (Section (cid:1).(cid:16)).
4.3 Cluster Enforcer