A Discourse on Complexity of Process Models
(Survey Paper)
J. Cardoso1, J. Mendling2, G. Neumann2, and H.A. Reijers3
1University of Madeira
9000-390 Funchal,Portugal
PI:EMAIL
2Vienna Universityof Economics and Business Administration
Augasse 2-6, 1090 Vienna, Austria
{jan.mendling, neumann}@wu-wien.ac.at
3EindhovenUniversity of Technology
P.O. Box 513, 5600 MB Eindhoven,The Netherlands
PI:EMAIL
Abstract. Complexity has undesirable effects on, among others, the
correctness, maintainability, and understandability of business process
models.Yet,measuringcomplexityofbusinessprocessmodelsisarather
new area of research with only a small number of contributions. In this
paper,wesurveyfindingsfromneighboringdisciplinesonhowcomplexity
canbemeasured.Inparticular,wegatherinsightfromsoftwareengineer-
ing,cognitivescience,andgraphtheory,anddiscussinhowfaranalogous
metrics can be definedon business process models.
1 Introduction
Since business process management has become an accepted concept for the
implementation and integration of large-scale information systems, there is an
increasing need for insight into how errors can be avoided, how maintenance
can be facilitated, or how the quality of the processes can be improved. In this
context, there is some evidence that complexity is a determinant of error prob-
ability of a business process [18]. As process complexity and its measurement
is a rather new field in business process management, there is only a limited
understanding of how far existing knowledge of complexity e.g. for the software
engineering domain can be adopted.
The complexity of a software program comes in three ‘flavors’: computa-
tional complexity, psychological complexity, and representational complexity
[26]. The most important is psychological complexity, which encompasses pro-
grammercharacteristics,product/documentationcomplexity and problemcom-
plexity.Obviously,thelatteraspect,thecomplexityoftheproblemitself,cannot
be controlled in developing software. It is therefore frequently dismissed from
consideration in the software engineering literature. It seems sensible to do the
sameforanalyzingthecomplexityofprocessmodels.However,theissueremains
thatcomplexprocesseswillrequiremorecomplexprocessmodels.Therefore,for
J.Eder,S.Dustdaretal.(Eds.):BPM2006Workshops,LNCS4103,pp.115–126,2006.
(cid:2)c Springer-VerlagBerlinHeidelberg2006
116 J. Cardoso et al.
the development of process model complexity it seems worthwhile to evaluate
complexity measures as relative to the underlying process complexity.
Existingtheoreticalapproachestoformulate’complexitymetrics’forsoftware
include theuse ofinformationtheoryfromsignalprocessing(e.g.[10])andcom-
munication theory (e.g. [24]), as well as approaches based on analogues with
graph theory (e.g. [15]) and lattice theory (e.g. [11]). Approaches taking the
cognitivesciencesasstartingpointhaveresulted,forexample,inBastani’scom-
plexity model [4]. An overview of some 50 different software complexity metrics
is provided in Table 1 in [5].
In this paper, we contribute to a better understanding of business process
model complexity. In particular, we provide a theoretical survey of complexity
considerationsandmetricsinthefieldsofsoftwareengineering,cognitivescience,
and graph theory and we relate them to business process modelling. A further
empiricalinvestigationmightultimatelyleadtoestablishingacomplexitytheory
of business process models. Following this line of argumentation,the rest of the
paperisstructuredasfollows.Section2discussescomplexitymetricsforsoftware
and their applicability for business process models. After a generalintroduction
to the discipline, we define analogous metrics to the Line-of-Code, McCabe’s
Cyclomatic Complexity called Control-Flow Complexity, Halstead Complexity
Metric, and Information Flow Complexity as defined by Henry and Kafura.
Section 3 relates findings from cognitive science to measuring complexity in
software engineering. In Section 4 graph theoretical measures are considered as
potential complexity metrics for business process models. Section 5 closes the
paper and gives an outlook on future research with a focus on how the process
complexity metrics can be validated.
2 Complexity in Business Processes
2.1 Software Metrics
Over the last 30 years many measures have been proposed by researchers to
analyze software complexity, understandability, and maintenance. Metrics were
designedtoanalyzesoftwaresuchasimperative,procedural,andobject-oriented
programs.Softwaremeasurementisconcernedwithderivinganumericvaluefor
an attribute of a software product, i.e. a measurement is a mapping from the
empiricalworldto the formalworld.Fromthe severalsoftwaremetrics available
we are particularly interested in studying complexity metrics and find out how
they can be used to evaluate the complexity of business processes.
Software metrics are often used to give a quantitative indication of a pro-
gram’s complexity. However, it is not to be confused with computational com-
plexitymeasures(cf. O(n)-Notation),whoseaimis tocomparethe performance
ofalgorithms.Softwaremetricshavebeenfoundtobeusefulinreducingsoftware
maintenance costs by assigning a numeric value to reflect the ease or difficulty
with which a programmodule may be understood.
Therearehundredsofsoftwarecomplexitymeasuresthathavebeendescribed
and published by many researchers. For example, the most basic complexity
A Discourse on Complexity of Process Models 117
measure,thenumberoflinesofcode(LOC),simplycountsthelinesofexecutable
code, data declarations, comments, and so on. While this measure is extremely
simple, it has been shownto be veryuseful andcorrelateswell with the number
of errors in programs.
2.2 The Analogy Between Software and Business Processes
Whiletraditionalsoftwaremetricsweredesignedtobeappliedtoprogramswrit-
ten in languages such as C++, Java, FORTRAN, etc, we believe that they can
be revised and adapted to analyze and study business processes characteristics,
such as complexity, understandability, and maintenance. We based our intu-
ition on the fact that there is a strong analogy between programs and business
processes, as argued before in e.g. [23,9]. Business process languages aim to
enable programming in the large. The concepts of programming in the large
and programming in the small distinguish between two aspects of writing the
type of long-running asynchronous processes that one typically sees in business
processes. Programming in the large emphasis is on partitioning the work into
moduleswhoseinteractionsarepreciselyspecifiedandcanrefertoprogramming
code that represents the high-level state transition logic of a business process
(typicallyusingsplitsandjoins).Thisstatetransitionlogicincludedinformation
such as when to wait for messages from incoming transitions, when to activate
outgoing transitions, and when to compensate for failed activities, etc.
A business process, possibly modeled with a language such as BPEL [2], can
be seen as a traditional software program that has been partitioned into mod-
ules or functions (i.e. activities) that take in a group of inputs and provide
some output. Module interactions are precisely specified using predefine logic
operators such as sequence, XOR-splits, OR-splits, and AND-splits. There is a
mappingthatcanbeestablishedbetweensoftwareprogramsconstructsandbusi-
nessprocesses.Functions,procedures,ormodulesaremappedtoactivities.Two
sequential software statements (i.e. instructions or functions) can be mapped
to two sequential process activities. A ’switch’ statement can be mapped to a
XOR-split. In programs, threads can be used to model concurrency and can
be mapped to AND-splits. Finally, the conditional creation of threads using a
sequence of ’if-then’ statements can be mapped to an OR-split.
2.3 Business Process Metrics
We believe that the future for process metrics lies in using relatively simple
metrics to build tools that will assist process analysts and designer in making
designdecisions.Furthermore,becausebusinessprocessesareahigh-levelnotion
madeupofmanydifferentelements(splits,joins,resources,data,activities,etc.),
there can never be a single measure of process complexity. The same conclusion
has been reached in software engineering. Nagappan et al. [20] point out that
there is no single set of complexity metrics that could act as a universally best
defect predictor for software programs. For this reason several process metrics
canbedesignedtoanalyzebusinessprocesses.Forexample,Cardoso[8]identifies
118 J. Cardoso et al.
fourmaintypesofcomplexitymetricsforprocesses:activitycomplexity,control-
flow complexity, data-flow complexity, and resource complexity.
The following sections describe several approaches to adapt known software
metricsproposedbyresearchesworldwidetobusinessprocessesanalysis.Having
establishedthatthereisamappingfromtraditionalprogramminglanguagesand
business processes; we will study and adapt some of the most well known and
widely used source code metric, i.e. number of lines of code (LOC) [13], McCabe
cyclomatic complexity [15,16], Halstead’s software science measures [10], and
Henry and Kafura [12] information flow metric.
2.4 Adapting the LOC Metric
One of the earliest and fundamental measures based on the analysis of software
code is based on the basic count of the number of Lines of Code (LOC) of a
program.Despitebeingwidelycriticizedasameasureofcomplexity,itcontinues
to have widespread popularity mainly due to its simplicity [3]. The basis of the
LOC measure is that program length can be used as a predictor of program
characteristics such as errors occurrences, reliability, and ease of maintenance.
If we view a process activity as a statement of a software program, we can
derive a very simple metric (metric M1) that merely counts the number of
activities(NOA)inabusinessprocess.ItshouldbenoticedthattheNOAmetric
characterizes only one specific view of size, namely length, it takes no account
of functionality or complexity. Also, bad process design may cause an excessive
number of activities. Compared to the original LOC metric, the NOA is not
language-dependent and it is easier for users to understand.
M1: NOA = Number of activities in a process
Another adaptation of the LOC metric is to view not only activities as pro-
gram statements, but to also take into account process control-flow elements
(i.e. control structures). Control-flow elements affect the execution sequence of
activities. This statements are different since they are executed for their effect
anddo nothave values.Two types of metrics canbe designeddepending onthe
structured of process.
Ontheonehand,wecanconsiderthatprocessesarewell-structured[1].When
processes are well-structured we can simply count the control structures corre-
sponding to splits, since it is explicitly known that a corresponding join exits.
Please note that the structure of well-structured processes is analogue to soft-
wareprograms.Incomputerprogramming,astatementblockisasectionofcode
which is grouped together, much like a paragraph;such blocks consist of one or
more statements. For example, in a C statement blocks are enclosed by braces
{ and }. In Pascal, blocks are denoted by begin and end statements. Having
these characteristics in mind we design our second metric (M2) which counts
the activities and control-flow elements of a process:
M2: NOAC = Number of activities and control-flow elements in a process
A Discourse on Complexity of Process Models 119
On the other hand, we also have to consider that some languages allow the
constructionof processes that are not well-structured.As we have already men-
tioned, examples of such languages include EPC and Workflow nets. In these
modeling languages, splits do not have to match a corresponding join. These
processes are generally more difficult to understand and result often in design
errors. For processes that are not well-structured we can design a third metric
(M3) which counts the number of activities and the number of splits and joins
of a process.
M3: NOAJS = Number of activities, joins, and splits in a process
In EPC models, we would count the number of activities, XOR-joins and
-splits, OR-joins and -splits, and AND-joins and -splits to calculate NOAJS.
2.5 Adapting McCabe’s Cyclomatic Complexity
An early measure, proposedby McCabe [15], views programcomplexity related
to the number of control paths through a program module. McCabe derived
a software complexity measure from graph theory using the definition of the
cyclomatic number which corresponds to the number of linearly independent
paths in a program. It is intended to be independent of language and language
format[17].Thismeasureprovidesasinglenumberthatcanbecomparedtothe
complexity of other programs.
Sinceitsdevelopment,McCabe’scyclomaticcomplexity(MCC) hasbeenone
of the most widely accepted software metrics and has been applied to tens of
millions oflines ofcode inboth the DepartmentofDefense (DoD) andcommer-
cialapplications.Theresultingbaseofempiricalknowledgehasallowedsoftware
developers to calibrate measurements of their own software and arrive at some
understanding of its complexity. McCabe’s cyclomatic complexity is an indica-
tion of a programmodule’s control-flow complexity and has been found to be a
reliable indicator of complexity in large software projects [25]. Considering the
numberofcontrolpathsthroughtheprogram,a10-lineprogramwith10assign-
ment statements is easier to understand than a 10-line program with 10 if-then
statements.
MCC is defined for each module to be e − n + 2, where e and n are the
number of edges and nodes in the control flow graph, respectively. Control flow
graphs describe the logic structure of software modules. The nodes represent
computational statements or expressions, and the edges represent transfer of
controlbetweennodes.Eachpossibleexecutionpathofa softwaremodule hasa
correspondingpath from the entry to the exit node ofthe module’s controlflow
graph.Forexample,inFigure1,theMCC ofthecontrolflowgraphfortheJava
code described is 14−11+2=5.
2.6 The CFC Metric
In our previous work [6,7] we have designed a process complexity metric that
borrowssomeideasfromMcCabe’scyclomaticcomplexity.Ourobjectivewasto
120 J. Cardoso et al.
Fig.1. of a Java program and its corresponding flowgraph
developa metric that could be used in the same way as the MCC metric but to
evaluate processes’ complexity.
One of the first important observations that can be made from the MCC
control flow graph, shown in Figure 1, is that this graph is extremely similar
to a process. One major difference is that the nodes of a MCC control flow
graph have identical semantics, while process nodes (i.e., activities) can have
different semantics (e.g., AND-splits, XOR-splits, OR-joins, etc). Our approach
has tackled this major difference.
The metric thatwe havepreviouslydevelopedandtested,calledControl-flow
Complexity (CFC) metric, was based on the analysis of XOR-splits, OR-splits,
and AND-splits control-flow elements. The main idea behind the metric was to
evaluatethenumberofmentalstatesthathavetobeconsideredwhenadesigner
isdevelopingaprocess.Splitsintroducethenotionofmentalstatesinprocesses.
When a split (XOR, OR, or AND) is introduced in a process, the business
processdesignerhastomentallycreateamaporstructurethataccountsforthe
number of states that can be reachedfromthe split. The notion of mental state
is important since there are theories [19] suggesting that complexity beyond a
certain point defeats the human mind’s ability to perform accurate symbolic
manipulations, and hence results in error.
Mathematically,the control-flowcomplexitymetric is additive,thus itis very
easy to calculate the complexity of a process, by simply adding the CFC of all
split constructs.The control-flowcomplexity wascalculatedas follows,where P
is a process and a an activity.
(cid:2)
CFC(P)= CFC (a)
XOR
a∈P,a isa (cid:2)xor−split
(cid:2)
+ CFC (a)+ CFC (a)
OR AND
a∈P,a isa or−split a∈P,a isa and−split