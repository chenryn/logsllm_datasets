the
Figure 7:
CDF of
number of messages
re-
ceived by a router follow-
ing a link state change, for
the Internet-like topology
(left) and the random graph
(right).
Figure 8: Scaling of messag-
ing and control memory in
the Internet-like graph, for
100, 200, 300, 400, and 500
nodes.
0100200300400Number of Forwarding Table Entries0.00.20.40.60.81.0Fraction of SamplesInternet-LikeLTLT in MixedPV in MixedPV0100200300400Number of Forwarding Table Entries0.00.20.40.60.81.0Fraction of SamplesRandomLTLT in MixedPV in MixedPV0.000.020.040.060.080.10Fraction of Links Failed0.000.050.100.150.200.25Fraction of (src, dst) Pairs DisconnectedInternet-LikePVPV in MixedLT in MixedLT0.000.020.040.060.080.10Fraction of Links Failed0.000.050.100.150.200.250.300.350.40Fraction of (src, dst) Pairs DisconnectedRandomPVPV in MixedLT TreeLT in MixedLT0.00.10.20.30.40.5Fraction of LT Nodes in the Graph0.000.020.040.060.080.10Fraction of (src, dst) Pairs DisconnectedEffect of LT Nodes on ConnectivityFrom PV NodesFrom LT Nodes100101102103Number of Messages0.00.20.40.60.81.0Fraction of SamplesInternet-LikePVPV in MixedLT in MixedLT100101102103104Number of Messages0.00.20.40.60.81.0Fraction of SamplesRandomPVPV in MixedLT in MixedLT TreeLT100150200250300350400450500Topology Size050010001500200025003000Number of MessagesAverage Number of Messages Received by a Router During Initial Convergence.LTPV in MXLT in MXPV100150200250300350400450500Topology Size20406080100120140160180Control Memory Size in KBAverage Maximum Control Memory at a Router DuringConvergence and Failure/Recovery of Each LinkPV in MXLTLT in MXPV120Figure 9: CDF of the size of
the route ﬁeld in the packet
header, for the Internet-like
topology (left) and the ran-
dom graph (right).
decision process) cause it to switch back to paths using the
recovered link.
In contrast, in the LT case we need only
advertise a single pathlet for the recovered link (see discus-
sion in Sec. 3.2). Second, according to our dissemination
algorithm (Sec. 3.3), many of those 5.19 pathlets are never
disseminated to some parts of the network, because they are
not reachable.
In the random graph topology, the second factor is not
true. Not only does every router learn all pathlets, but it
learns them from all of its neighbors. The result is 10.3×
as many messages as PV. However, this can be reduced if
nodes simply advertise fewer pathlets, which is safe as long
as the advertised pathlets reach every destination. The line
“LT Tree” in Fig. 7 shows the case when we advertise only
a tree reaching all destinations (described in Sec. 3.3). This
reduces the messaging overhead to just 2.23×. The tradeoﬀ
is that there are fewer choices of paths, and connectivity in
the face of failures is worsened, as shown in Fig. 6.
Fig. 8 plots the messaging cost for initial convergence as a
function of n, the number of nodes in the topology. LT and
PV have similar scaling properties for this metric because
both, like BGP, use a path vector dissemination algorithm
to announce objects; PV announces one object per node and
LT announces O(δ) (where δ is the number of neighbors),
which is independent of n.
Control plane memory. Since control plane memory use
is dependent on the implementation, we evaluate its scal-
ing properties. Asymptotically, if the n nodes each have δ
neighbors and the mean path length is (cid:96), BGP and PV poli-
cies need to store Θ(n) announcements of mean size Θ((cid:96))
from up to δ neighbors, for a total of Θ(nδ(cid:96)) state in the
worst case. In a full LT deployment there are Θ(δn) path-
lets; each announcement has size Θ((cid:96)) and is learned from
up to d neighbors, for a total of Θ(nδ2(cid:96)) in the worst case.
Fig. 8 shows the mean (over routers and over three trials)
of the maximum (over time for each of trial) of the control
plane state at each router. A trial consists of allowing the
network to converge and then failing and recovering each
link once. The results conﬁrm that PV and LT policies scale
similarly as a function of n. It is also apparent that with
the Internet-like topologies, we don’t reach the worst case
in which LT is a factor δ worse than P V .
It may be possible to optimize our dissemination algo-
rithm to reduce control state and messaging; see Sec. 8.
Header size. Fig. 9 shows the CDF over source-destination
pairs (X, Y ) of the number of bits in the route ﬁeld of the
packet header for the shortest X ; Y route. The number
of bits may vary as the packet travels along the route; we
report the maximum (which for LT policies occurs at the
source).
Header length scales with the path length. In our Internet-
like graph, the mean path length is 2.96 and headers average
4.21 bytes. An AS-level topology of the Internet mapped by
CAIDA [5] from Jan 22 2009 has mean path length 3.77,
so we could therefore extrapolate that mean header length
would be about 27% greater in a full Internet topology, i.e.,
less than 6 bytes, assuming other characteristics of the topol-
ogy remain constant. Even the maximum header length of
12.5 bytes in our implementation would not add prohibitive
overhead to packet headers.
7. RELATED WORK
We have compared the policy expressiveness of a num-
ber of multipath routing protocols in Section 5. Here we
compare other aspects of the protocols.
MIRO [28] uses BGP routes by default. Only those ASes
which need an alternate path (say, to avoid routing through
one particularly undesirable AS) need to set up a path.
But this increases the latency of constructing a path, and
increases forwarding plane state if there are many paths.
MIRO is likely easier to deploy than pathlet routing in to-
day’s network of BGP routers.
NIRA [30] provides multiple paths and very small for-
warding and control state as long as all routers have short
paths to the “core”; but for arbitrary networks, forwarding
state may be exponentially large. The scheme requires a po-
tentially complicated assignment of IP addresses, and works
primarily for Internet-like topologies with valley-free routing
policies. Exceptions to these policies may be diﬃcult.
Routing deﬂections [31] and path splicing [23] permit poli-
cies speciﬁed on a per-destination basis, while still providing
many path choices with limited deviations from the primary
paths. However, sources are not aware of what paths they
are using, [31] has relatively limited choice of paths, and [23]
can encounter forwarding loops.
IP’s strict source routing and loose source routing [8] pro-
vide source-controlled routing but have limited control for
policies. For this reason and due to security concerns un-
related to our setting, they are not widely used. They can
also result in long headers because each hop is speciﬁed as
an IP address, unlike our compact forwarding identiﬁers.
Feedback based routing [32] suggested source-based route
quality monitoring that is likely to be a useful approach for
pathlet routing.
024681012Maximum Header Size (bytes)0.00.20.40.60.81.0Fraction of SamplesInternet-LikePVPV in MixedLT in MixedLT024681012Maximum Header Size (bytes)0.00.20.40.60.81.0Fraction of SamplesRandomPVPV in MixedLT in MixedLT121Platypus [24] is similar to loose source routing except each
waypoint can be used only by authorized sources to reach ei-
ther any destination, or a speciﬁed IP preﬁx. Pathlet routing
supports a diﬀerent set of policies and enforces these using
the presence or absence of forwarding tables, rather than
cryptography.
R-BGP [18] adds a small number of backup paths that
ensure continuous connectivity under a single failure, with
relatively minimal changes to BGP. However, it somewhat
increases forwarding plane state and is not a full multipath
solution. For example, sources could not use alternate paths
to improve path quality.
LISP [9] reduces forwarding state and provides multiple
paths while remaining compatible with today’s Internet. Al-
though it can limit expansion of forwarding table size, LISP’s
forwarding tables would still scale with the size of the non-
stub Internet, as opposed to scaling with the number of
neighbors as in our LT policies.
MPLS [26] has tunnels and labels similar to our pathlets
and FIDs. It also shares the high level design of having the
source or ingress router map an IP address to a sequence
of labels forming a source route. However the common use
of these mechanisms is substantially diﬀerent from pathlet
routing: tunnels are not typically concatenated into new,
longer tunnels, or inductively built by adding one hop at
a time. To the best of our knowledge MPLS has not been
adapted to an interdomain policy-aware routing.
Metarouting [13], like pathlet routing, generalizes routing
protocols. It would be interesting to explore whether pathlet
routing can be represented in the language of [13].
8. CONCLUSION
Pathlet routing oﬀers a novel routing architecture. Through
its building blocks of vnodes and pathlets, it supports com-
plex BGP-style policies while enabling and incentivizing the
adoption of policies that yield small forwarding plane state
and a high degree of path choice. We next brieﬂy discuss
some limitations and future directions.
We suspect it is possible to optimize our path vector-based
pathlet dissemination algorithm. The techniques of [16] may
be very easy to apply in our setting to reduce control plane
memory use from O(δ(cid:96)) to O((cid:96)) per pathlet, where δ is the
number of neighbors and (cid:96) is the mean path length. Routers
could also pick dissemination paths based on heuristics to
predict stability, which for common failure patterns can sig-
niﬁcantly reduce the number of update messages [12]. The
more radical approach of [30] could also be used to dramat-
ically reduce state in Internet-like environments.
Traﬃc engineering is an important aspect of routing that
we have not evaluated. One common technique—advertising
diﬀerent IP to diﬀerent neighbors to control inbound traﬃc—
is straightforward to do in our LT policies. But source-
controlled routing would dramatically change the nature of
traﬃc engineering, potentially making it more diﬃcult for
ISPs (since they have less control) and potentially making
it easier (since sources can dynamically balance load).
Acknowledgements
We thank the authors of [7] for supplying the Internet-like
topologies.
9. REFERENCES
[1] D. G. Andersen, H. Balakrishnan, M. F. Kaashoek, and
R. Morris. Resilient overlay networks. In Proc. 18th ACM
SOSP, October 2001.
[2] Routing table report.
http://thyme.apnic.net/ap-data/2009/01/05/0400/mail-global.
[3] Avaya. Converged network analyzer.
http://www.avaya.com/master-usa/en-
us/resource/assets/whitepapers/ef-lb2687.pdf.
[4] B. Awerbuch, D. Holmer, H. Rubens, and R. Kleinberg.
Provably competitive adaptive routing. In INFOCOM, 2005.
[5] CAIDA AS ranking. http://as-rank.caida.org/.
[6] D. Clark, J. Wroclawski, K. Sollins, and R. Braden. Tussle in
cyberspace: deﬁning tomorrow’s Internet. In SIGCOMM, 2002.
[7] X. Dimitropoulos, D. Krioukov, A. Vahdat, and G. Riley.
Graph annotations in modeling complex network topologies.
ACM Transactions on Modeling and Computer Simulation
(to appear), 2009.
[8] J. P. (ed.). DARPA internet program protocol speciﬁcation. In
RFC791, September 1981.
[9] D. Farinacci, V. Fuller, D. Meyer, and D. Lewis. Locator/ID
separation protocol (LISP). In Internet-Draft, March 2009.
[10] B. Ford and J. Iyengar. Breaking up the transport logjam. In
HOTNETS, 2008.
[11] L. Gao and J. Rexford. Stable Internet routing without global
coordination. IEEE/ACM Transactions on Networking,
9(6):681–692, December 2001.
[12] P. B. Godfrey, M. Caesar, I. Haken, S. Shenker, and I. Stoica.
Stable Internet route selection. In NANOG 40, June 2007.
[13] T. Griﬃn and J. Sobrinho. Metarouting. In ACM SIGCOMM,
2005.
[14] K. P. Gummadi, H. V. Madhyastha, S. D. Gribble, H. M. Levy,
and D. Wetherall. Improving the reliability of internet paths
with one-hop source routing. In Proc. OSDI, 2004.
[15] G. Huston. BGP routing table analysis reports, 2009.
http://bgp.potaroo.net/.
[16] E. Karpilovsky and J. Rexford. Using forgetful routing to
control BGP table size. In CoNEXT, 2006.
[17] N. Kushman, S. Kandula, and D. Katabi. Can you hear me
now?! it must be BGP. In Computer Communication Review,
2007.
[18] N. Kushman, S. Kandula, D. Katabi, and B. Maggs. R-BGP:
Staying connected in a connected world. In NSDI, 2007.
[19] C. Labovitz, A. Ahuja, A. Bose, and F. Jahanian. Delayed
Internet routing convergence. In ACM SIGCOMM, 2000.
[20] K. Lakshminarayanan, I. Stoica, S. Shenker, and J. Rexford.
Routing as a service. Technical Report UCB/EECS-2006-19,
UC Berkeley, February 2006.
[21] Z. M. Mao, R. Bush, T. Griﬃn, and M. Roughan. BGP
beacons. In IMC, 2003.
[22] D. Meyer, L. Zhang, and K. Fall. Report from the iab workshop
on routing and addressing. In RFC2439, September 2007.
[23] M. Motiwala, M. Elmore, N. Feamster, and S. Vempala. Path
splicing. In ACM SIGCOMM, 2008.
[24] B. Raghavan and A. C. Snoeren. A system for authenticated
policy-compliant routing. In ACM SIGCOMM, 2004.
[25] Y. Rekhter, T. Li, and S. Hares. A border gateway protocol 4
(BGP-4). In RFC4271, January 2006.
[26] E. Rosen, A. Viswanathan, and R. Callon. Multiprotocol label
switching architecture. In RFC3031, January 2001.
[27] S. Savage, T. Anderson, A. Aggarwal, D. Becker, N. Cardwell,
A. Collins, E. Hoﬀman, J. Snell, A. Vahdat, G. Voelker, and
J. Zahorjan. Detour: Informed Internet routing and transport.
In IEEE Micro, January 1999.
[28] W. Xu and J. Rexford. MIRO: Multi-path Interdomain
ROuting. In SIGCOMM, 2006.
[29] X. Yang. NIRA: a new Internet routing architecture. Technical
Report Ph.D. Thesis, MIT-LCS-TR-967, Massachusetts
Institute of Technology, September 2004.
[30] X. Yang, D. Clark, and A. Berger. NIRA: a new inter-domain
routing architecture. IEEE/ACM Transactions on
Networking, 15(4):775–788, 2007.
[31] X. Yang and D. Wetherall. Source selectable path diversity via
routing deﬂections. In ACM SIGCOMM, 2006.
[32] D. Zhu, M. Gritter, and D. Cheriton. Feedback based routing.
Computer Communication Review (CCR), 33(1):71–76, 2003.
122