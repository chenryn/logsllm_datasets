title:Revisiting Memory Errors in Large-Scale Production Data Centers: Analysis
and Modeling of New Trends from the Field
author:Justin Meza and
Qiang Wu and
Sanjeev Kumar and
Onur Mutlu
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Revisiting Memory Errors in Large-Scale Production Data Centers:
Analysis and Modeling of New Trends from the Field
Justin Meza Qiang Wu (cid:2)
Sanjeev Kumar (cid:2) Onur Mutlu
Carnegie Mellon University
(cid:2) Facebook, Inc.
Abstract—Computing systems use dynamic random-access
memory (DRAM) as main memory. As prior works have shown,
failures in DRAM devices are an important source of errors in
modern servers. To reduce the effects of memory errors, error
correcting codes (ECC) have been developed to help detect and
correct errors when they occur. In order to develop effective
techniques, including new ECC mechanisms, to combat memory
errors, it is important to understand the memory reliability trends
in modern systems.
In this paper, we analyze the memory errors in the entire
ﬂeet of servers at Facebook over the course of fourteen months,
representing billions of device days. The systems we examine
cover a wide range of devices commonly used in modern servers,
with DIMMs manufactured by 4 vendors in capacities ranging
from 2 GB to 24 GB that use the modern DDR3 communication
protocol.
We observe several new reliability trends for memory systems
that have not been discussed before in literature. We show that
(1) memory errors follow a power-law, speciﬁcally, a Pareto
distribution with decreasing hazard rate, with average error rate
exceeding median error rate by around 55(cid:3); (2) non-DRAM
memory failures from the memory controller and memory channel
cause the majority of errors, and the hardware and software
overheads to handle such errors cause a kind of denial of service
attack in some servers; (3) using our detailed analysis, we provide
the ﬁrst evidence that more recent DRAM cell fabrication tech-
nologies (as indicated by chip density) have substantially higher
failure rates, increasing by 1.8(cid:3) over the previous generation; (4)
DIMM architecture decisions affect memory reliability: DIMMs
with fewer chips and lower transfer widths have the lowest error
rates,
likely due to electrical noise reduction; (5) while CPU
and memory utilization do not show clear trends with respect
to failure rates, workload type can inﬂuence failure rate by up to
6.5(cid:3), suggesting certain memory access patterns may induce more
errors; (6) we develop a model for memory reliability and show
how system design choices such as using lower density DIMMs and
fewer cores per chip can reduce failure rates of a baseline server
by up to 57.7%; and (7) we perform the ﬁrst implementation
and real-system analysis of page ofﬂining at scale, showing that
it can reduce memory error rate by 67%, and identify several
real-world impediments to the technique.
I.
INTRODUCTION
Computing systems store a variety of data in memory – pro-
gram variables, operating system and ﬁle system structures, pro-
gram binaries, and so on. The main memory in modern systems
is composed of dynamic random-access memory (DRAM), a
technology that, from the programmer’s perspective, has the
following property: a byte written to an address can be read
correctly, repeatedly, until it is overwritten or the machine is
turned off. All correct programs rely on DRAM to operate in
this manner and DRAM manufacturers work hard to design
reliable devices that obey this property.
Unfortunately, DRAM does not always obey this property.
Various events can change the data stored in DRAM, or
even permanently damage DRAM. Some documented events
include transient charged particle strikes from the decay of
radioactive molecules in chip packaging material, charged alpha
particles from the atmosphere [34], and wear-out of the various
components that make up DRAM chips (e.g., [7, 6]). Such
faults, if left uncorrected, threaten program integrity. To reduce
this problem, various error correcting codes (ECC) for DRAM
data [12, 11] have been used to detect and correct memory
errors. However, these techniques require additional DRAM
storage overheads [33] and DRAM controller complexity and
cannot detect or correct all errors.
Much past research has been directed toward analyzing the
causes and effects of memory errors in the ﬁeld (e.g., [44, 16,
47, 48, 10, 46, 45, 40, 27, 28]). These past works identiﬁed a
variety of DRAM failure modes and have formed the basis of
the community’s understanding of DRAM reliability. Our goal
is to strengthen the understanding of DRAM failures in the ﬁeld
by comprehensively studying new trends in DRAM errors in
a large-scale production datacenter environment using modern
DRAM devices and workloads. To this end, this paper presents
our analysis of memory errors across Facebook’s entire ﬂeet of
servers over the course of fourteen months and billions of device
days. Our main contributions are threefold. We: (1) analyze
new DRAM failure trends in modern devices and workloads
that have not been identiﬁed in prior work, (2) develop a
model for examining the memory failure rates of systems with
different characteristics, and (3) describe and perform the ﬁrst
analysis of a large-scale implementation of a software technique
proposed in prior work to reduce DRAM error rate (page
ofﬂining [49]). Speciﬁcally, we observe several new reliability
trends for memory systems that have not been discussed before
in literature:
(1) The number of memory errors per machine follows a
power-law distribution, speciﬁcally a Pareto distribution, with
decreasing hazard rate. While prior work reported the average
memory error rate per machine, we ﬁnd that the average exceeds
the median amount by around 55(cid:2), and thus may not be a
reliable number to use in various studies.
(2) Non-DRAM memory failures, such as those in the
memory controller and the memory channel, are the source of
the majority of errors that occur. Contrary to popular belief,
memory errors are not always isolated events and can bombard
a server (if not handled appropriately), creating a kind of denial
of service attack. No prior work that we are aware of that
examined DRAM chip-level failures accounted for this effect.
(3) DRAM failure rates increase with newer cell fabrication
technologies (as indicated by chip density, which is a good
indicator of technology node): 4 Gb chips have 1:8(cid:2) higher
failure rates than 2 Gb chips. Prior work that examined DRAM
capacity, which is not closely related to fabrication technology,
observed inconclusive trends. Our empirical ﬁnding is that the
quadratic rate at which DRAM density increases with each
generation has made maintaining or reducing DRAM failure
rate untenable, as also indicated by a recent paper by Samsung
and Intel [20].
(4) DIMM architecture characteristics, such as the number
of data chips per DIMM and the transfer width of each chip,
affect memory error rate. The best architecture for device
reliability occurs when there are both low chips per DIMM
and small transfer width. This is likely due to reductions in the
amount of electrical disturbance within the DIMM.
(5) The type of work that a server performs (i.e.,
its
workload), and not CPU and memory utilization, affects failure
rate. We ﬁnd that the DRAM failure rate of different workloads
can vary by up to 6:5(cid:2). This large variation in workloads can
potentially be due to memory errors that are induced by certain
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.57
DOI 10.1109/DSN.2015.57
415
415
access patterns, such as accessing the same memory location
in rapid succession, as shown in controlled studies in prior
work [23].
(6) We develop a model for quantifying DRAM reliability
across a wide variety of server conﬁgurations and show how it
can be used to evaluate the server failure rate trends for different
system designs. We show that using systems with lower density
DIMMs or fewer CPUs to access memory can reduce DRAM
failure rates by 57.7% and 34.6%, respectively. We make this
model publicly available at [1].
(7) We describe our implementation of page ofﬂining [49]
at scale and evaluate it on a fraction (12,276) of the servers
that we examine. We show that it can reduce memory error
rate by around 67%. While prior work reported larger error
rate reductions in simulation [16], we show that real-world
factors such as memory controller and memory channel failures
and OS-locked pages that cannot be taken ofﬂine can limit the
effectiveness of this technique.
II. BACKGROUND AND METHODOLOGY
A. Server Memory Organization
Modern servers have one or two processor chips that are
connected to DRAM via several memory channels that are
operated in parallel. Attached to each channel are dual in-
line memory modules (DIMMs) that provide an interface for
accessing data stored across multiple DRAM chips. Processors
use the double data rate (DDR) protocol
to communicate
with DIMMs. Chips on a DIMM are logically organized into
ranks and chips within a rank are operated in lockstep. Chips
contain multiple banks that are operated in parallel. Each bank
is organized into rows (typically 16 K to 64 K) and columns
(typically 2 K to 4 K). At the intersection of a row and a column
is a DRAM cell, which stores a single bit of data. We refer
the reader to [24, 26, 29, 25] for more information on DRAM
organization and operation.
B. Memory Errors and Their Handling
As prior works have shown, DRAM errors occur relatively
commonly due to a variety of stimuli [34, 44, 16, 47, 48, 10, 46,
6, 27, 45, 30, 23, 21]. To protect against such errors in servers,
additional data is stored in the DIMM (in a separate DRAM
chip) to maintain error correcting codes (ECC) computed over
data. These codes can detect and correct a small number
of errors. For example, single error correction, double error
detection (SEC-DED) is a common ECC strategy that can detect
any 2 ﬂipped bits and correct 1 ﬂipped bit per 64 bits by storing
an additional 12.5% of ECC metadata. An error that can be
corrected by ECC is called a correctable error (CE); an error
that cannot be corrected by ECC, but which can still be detected
by ECC, is called an uncorrectable error (UCE).
The processor’s memory controller orchestrates access to the
DRAM devices and is also responsible for checking the ECC
metadata and detecting and correcting errors. While detecting
errors does not add overhead when performing memory ac-
cesses, correcting errors can delay a memory request and disrupt
a system. As an example, on the systems that we examine,
when an error is corrected, the CPU raises a hardware exception
called a machine check exception (MCE) which must be handled
by the processor.
When an MCE occurs, the processor stores information
about the memory error in special registers that can be read
by the operating system. This information includes the physical
address of the memory access when the error occurred and what
type of memory access (e.g., read or write) was being performed
when the error occurred. Note that memory errors do not only
occur in DRAM chips: memory errors can occur if the memory
controller fails or if logic associated with transmitting data on
a memory channel fails.
We distinguish between errors and faults. A fault refers to
the underlying cause of an error, such as a DRAM cell that no
longer reliably stores data. An error is the manifestation of a
fault. A hard, or permanent, fault causes an error every time the
fault is exercised. A soft, or transient/intermittent, fault causes
an error only some of the times the fault is exercised.
C. The Systems
We examined all of the DRAM devices in Facebook’s
server ﬂeet, which have operational lifetimes extending across
four years and comprise billions of device days of usage. We
analyzed data over a fourteen month period. We examined six
different system types with hardware conﬁgurations based on
the resource requirements of the workloads running on them.
Table I lists the workloads and their resource requirements. The
workloads perform a diverse set of operations including web
serving, caching [41], database management [18], video and im-
age processing/storage [50, 37], and messaging routing/storage.
The detailed speciﬁcations for the base server platforms that
we examine have been published as part of the Open Compute
Project [2]. For example, typical servers can support two Intel
Xeon CPUs, 16 DIMM slots, and 1 HDD [2]. The resource
requirements in Table I refer to the relative number of processor
cores, memory capacity, and storage capacity for servers for
each type of workload.
Note that each server runs a single type of workload. All
the servers conﬁgured for a particular workload type have
equivalent minimum capabilities, and, in general, a workload
can be run on any of them.
TABLE I: The workloads we examine and their resource requirements.
Workload
Resource requirements
Processor Memory
Storage
Web
Hadoop [4]
Ingest [18]
Database [18]
Memcache [41]
Media [50]
High
High
High
Low
Low
Medium
Low
Medium
High
High
High
Low
Medium
Low
High
High
Low
High
The memory in these systems covers a wide range of de-
vices commonly used in servers. The DIMMs are manufactured
by 4 vendors in capacities ranging from 2 GB to 24 GB per
DIMM. DDR3 is the protocol used to communicate with the
DIMMs. The DIMM architecture spans devices with 1, 2, and
4 ranks with 8, 16, 32, and 64 chips. The chip architecture
consists of 8 banks with 16 K, 32 K, and 64 K rows and 2 K to
4 K columns, and has chips that transfer both 4 and 8 bits of
data per clock cycle. We analyze three different chip densities
of 1 Gb, 2 Gb, and 4 Gb, which are closely related to DRAM
fabrication technology.
The composition of the modules we examine differs from
prior studies (e.g., [44, 16, 47, 48, 10, 45, 40]) in three
ways: (1) it consists of a current DRAM access protocol
(DDR3, as opposed to older generation protocols with less
aggressive memory bus clock frequencies, such as DDR and
DDR2 in [44]); (2) it consists of a more diverse range of
DRAM device organizations (e.g., DIMMs with a variety of
ranks, chips, rows, and columns versus the more homogeneous
DIMMs of [44, 16, 47, 48, 10, 45]); and (3) it contains DIMMs
with characteristics that have never been analyzed at a large-
scale (such as density, number of chips, transfer width, and
workload).
Some of the systems we examined had hardware mem-
ory scrubbing [36] enabled, which would cause the memory
controller to traverse memory, detecting (but not correcting)
memory errors in order to help determine faulty locations in
memory. The hardware scrubber was enabled only when the
machine entered a low enough idle state, so the scrubbing rate
of machines varied.
416416
D. Measurement Methodology
We use the mcelog Linux kernel module to log memory
errors in a ﬁle. We do this for every machine in the ﬂeet. For
each correctable memory error, we collect: (1) a time stamp
of when the error occurred; (2) the physical address that was
being accessed when the error occurred; (3) the server name;
(4) the socket, channel, and bank the physical address is located
on; and (5) the type of memory access being performed when
the error occurred (e.g., read or write). Uncorrectable errors
will halt the execution of the processors on the machines we
examine, causing a system crash. While we do not have detailed
information on uncorrectable errors, we are able to measure
their occurrence by examining a separate log of them that is kept
in non-volatile memory on the system boards that we examine.
We use a collector script to retrieve log data and parse it into
a form that can be curated in a Hive [5] table. This process is
done in real time every ten minutes.
In addition to information about the correctable errors that
occur, we also collect information on systems that had errors
(e.g., CPU utilization and system age; see Table II for details).
This process is done in a separate step.
The scale of the systems we analyzed and the amount of
data being collected posed challenges for analysis. To process
billions of device days of information, we used a cluster of
machines to perform a parallelized aggregation of the data using
MapReduce jobs. This resulted in a set of statistics for each of
the devices we analyzed. We then processed this summary data
in R [3] to collect our results.
E. Analytical Methodology
When we analyze the reliability trends with respect to a
system characteristic (e.g., chip density or CPU utilization), we
group systems into buckets based on the particular character-
istic and plot the failure rate of the systems in each bucket.
When performing bucketing, we round the value of a device’s
characteristic to the nearest bucket and we eliminate buckets
that contain less than 0.1% of the systems analyzed in order
to have a statistically signiﬁcant sample of systems in our
measurements. We show the 95th percentile conﬁdence interval
for our data when relevant.
Due to the size of the ﬂeet, we could not collect detailed
information for all the systems without errors (we do collect
detailed information for every system with errors). So,
in
Sections IV and V, instead of examining the absolute failure
rate among different types of servers, we examine the relative
failure rate compared to a more manageable size of servers that
we call the control group. The servers in the control group are
uniformly randomly selected from among all the servers that
did not have errors, and we collected detailed information on
the servers in this group.
Note that such a selection process preserves the distribution
of server types in the underlying ﬂeet, and our analysis in
Sections IV and V can be considered as being performed on a
“scaled down” version of the ﬂeet. The size of the control group
was chosen to be equal to the size of the error group, and the
sizes of these groups were sufﬁciently large to be statistically
signiﬁcant. We bucket servers in each group based on their
value for a given characteristic (e.g., age) and plot the fraction
of failed servers compared to operational servers in each bucket,
which we call the relative server failure rate. With this metric,
we can perform relative comparisons between failure rates that
we compute for different factors, but the absolute values for the
metric do not have any substantial meaning. We ﬁnd that the
relative failure rates we examine are in the range Œ0; 1, and we
plot our data within this range. In Sections IV and V, when we
refer to failure rate we mean relative server failure rate and as
a reminder that our data should not be confused with absolute
failure rates, we label our graphs with relative server failure
rate.
III. BASELINE STATISTICS
We will ﬁrst focus on the overall error rate and error dis-
tribution among the systems that we analyze and then examine
correlations between different factors and failure rate.
A. Incidence Error Rate and Error Count
Figure 1 shows the monthly incidence error rate for memory
over a span of fourteen months. The incidence error rate is
the fraction of servers in the ﬂeet that have memory errors
compared to the total size of the ﬂeet.1 We observe three trends
with respect to incidence error rate.
Correctable errors (CE)
Uncorrectable errors (UCE)
s
r
e
v
r
e
s
f
o
n
o
i
t
c
a
r
F
0.040
0.035
0.030
0.025
0.020
0.015
0.010
0.005
0.000
3
1
3
1
3
1
/
/
/
7
8
9
3