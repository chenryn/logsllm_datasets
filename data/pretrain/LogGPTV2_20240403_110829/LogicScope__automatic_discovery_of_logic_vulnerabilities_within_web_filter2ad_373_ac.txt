### Phase III: Testing

The testing phase of the prototype system involves several key components, as illustrated in Figure 4. These components work together to identify vulnerabilities in web applications.

- **Trace Collector (WebScarab Proxy)**: Captures and analyzes HTTP traffic.
- **Web Server**: Serves the web application under test.
- **Session Inspector**: Monitors and inspects session data.
- **Session Exporter**: Exports session data for further analysis.
- **Spec Analyzer**: Analyzes the specifications of the web application.
- **Symbolizer**: Converts input and output into symbolic representations.
- **FSM Analyzer**: Analyzes the Finite State Machine (FSM) of the application.
- **TestSpec Generator**: Generates test specifications.
- **SessionProfile, RequestProfile, TemplateProfile, DriverSpec, TestSpec**: Various profiles and specifications used in the testing process.
- **Testing Engine**: Orchestrates the testing process.
- **State Driver**: Manages the state transitions during testing.
- **Testing Controller**: Controls the overall testing procedure.
- **LoginProfile, InputProfile, Output Evaluator, Request Generator, Login Helper**: Components that handle specific aspects of the testing, such as generating meaningful parameter values and evaluating outputs.

**Figure 4: Prototype System Architecture**

The **Testing Engine** is responsible for instantiating test input vectors into concrete web requests (via the **Request Generator**) and feeding them into the application. It then evaluates the corresponding web responses to identify vulnerabilities (via the **Output Evaluator**). The **Testing Controller** manages the entire testing procedure, while the **State Driver** computes the path leading to the target test state by triggering state transitions step by step. The **Request Generator** uses the **InputProfile** and **Login Helper** to generate meaningful parameter values, ensuring that the input symbols are correctly instantiated.

### Evaluation

To evaluate our prototype system, LogicScope, we selected six real-world PHP web applications. These applications were deployed on a 2.13GHz Core 2 Linux server with 2GB RAM, running Ubuntu 10.10, Apache web server (version 2.2.16), and PHP (version 5.3.3). To facilitate trace collection, we developed user emulators for each application using Selenium WebDriver. These emulators automate the operation of the web applications by performing a sequence of actions based on navigation links.

**Table 1: Summary of Traces and Inferred FSM (#)**

| Application | Files | Web Requests | Web Responses | Session Variables | States | Input Symbols | Output Symbols |
|-------------|-------|--------------|---------------|-------------------|--------|---------------|-----------------|
| Scarf       | 24    | 1348         | 1346          | 3                 | 31     | 15            | 2               |
| Wackopicko  | 25    | 2104         | 1650          | 1                 | 18     | 7             | 5               |
| Events Lister | 25   | 1290         | 1287          | 2                 | 25     | 11            | 5               |
| Bloggit     | 2657  | 21           | 2645          | 3                 | 3      | 1             | 1               |
| openInvoice | 1138  | 52           | 1083          | 2                 | 2      | 5             | 0               |
| OpenIT      | 1462  | 37           | 1453          | 2                 | 2      | 5             | 0               |

LogicScope first collects traces using the user emulators and performs specification inference. Table 1 shows the statistics of collected traces and inferred FSMs, including the number of files, collected web requests, web responses, session variables, states, input, and output symbols. 

Next, LogicScope launches the testing process against each application and provides concrete attack vectors and evidence for manual inspection. We manually analyze the reported vulnerabilities and classify them into true vulnerabilities or false positives. Table 2 summarizes the testing results, including the number of test input vectors generated by each method (FB for forceful browsing; PM for parameter manipulation), reported attack instances, real attack vectors, and false positives (FP). We also compare the discovered vulnerabilities with known vulnerabilities from public sources and report false negatives (FN).

**Table 2: Summary of Testing Results (FB: forceful browsing, PM: parameter manipulation)**

| Application | Method | Test Inputs | Flagged Attacks | Real Attacks | FP | FN |
|-------------|--------|-------------|-----------------|--------------|----|----|
| Scarf       | FB     | 5           | 1               | 0            | 1  | 0  |
| Wackopicko  | FB     | 44          | 9               | 8            | 1  | 0  |
| Wackopicko  | PM     | 5           | 2               | 2            | 0  | 0  |
| Events Lister | FB  | 16          | 0               | 0            | 0  | 1  |
| Bloggit     | PM     | 0           | 0               | 0            | 0  | 0  |
| openInvoice | FB     | 0           | 0               | 0            | 0  | 0  |
| openInvoice | PM     | 0           | 0               | 0            | 0  | 0  |
| OpenIT      | FB     | 0           | 0               | 0            | 0  | 0  |
| OpenIT      | PM     | 0           | 0               | 0            | 0  | 0  |

In summary, we generated 233 test input vectors across all applications and reported 25 attack instances, of which 18 were real attack vectors and 7 were false positives. Detailed results can be found in the technical report [8].

### Related Works

Addressing web application logic flaws requires inferring the application's logic specification. Previous works, such as Waler [4] and Sun et al. [12], require access to the application source code for analysis or instrumentation. In contrast, our approach does not require source code. Additionally, Waler [4] can only identify violations of value-related invariants, and NoTamper [1] focuses on form processing and validation. Our approach characterizes the application logic more generally and can detect a broader range of logic flaws.

Other related works, such as Swaddler [3], BLOCK [7], and SENTINEL [9], use the specification to detect attacks at runtime. In contrast, we construct testing cases for vulnerability exploitation in addition to specification inference. Our work is also related to fuzzing testing approaches like FLAX [11], DART [5], SAGE [6], and Kudzu [10], which aim to achieve better coverage through guided test input generation. However, these works require application source code, whereas our test input generation is based on a specification inferred in a black-box manner.

### Summary

In this paper, we present a systematic black-box approach to identifying logic flaws in web applications. We implemented and evaluated a prototype system, LogicScope, to demonstrate the effectiveness of our approach. We also highlight some limitations: LogicScope cannot handle AJAX web applications and has limited capability in handling complex relationships and constraints within databases.

### Acknowledgments

This work was supported by NSF TRUST (The Team for Research in Ubiquitous Secure Technology) Science and Technology Center (CCF-0424422).

### References

[1] P. Bisht, T. Hinrichs, N. Skrupsky, R. Bobrowicz, and V. N. Venkatakrishnan. NoTamper: automatic blackbox detection of parameter tampering opportunities in web applications. In CCS’10, pages 607–618, 2010.

[2] Citigroup credit card information leakage in 2011. http://www.wired.com/threatlevel/2011/06/citibank-hacked/.

[3] M. Cova, D. Balzarotti, V. Felmetsger, and G. Vigna. Swaddler: An Approach for the Anomaly-based Detection of State Violations in Web Applications. In RAID’07, pages 63–86, 2007.

[4] V. Felmetsger, L. Cavedon, C. Kruegel, and G. Vigna. Toward Automated Detection of Logic Vulnerabilities in Web Applications. In USENIX’10, pages 143–160, 2010.

[5] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated random testing. In PLDI’05, pages 213–223, 2005.

[6] P. Godefroid, M. Y. Levin, and D. A. Molnar. Automated whitebox fuzz testing. In NDSS’08, 2008.

[7] X. Li and Y. Xue. BLOCK: A Black-box Approach for Detection of State Violation Attacks Towards Web Applications. In ACSAC’11, pages 247–256, 2011.

[8] X. Li and Y. Xue. LogicScope: Automatic Discovery of Logic Vulnerabilities within Web Applications. Technical report, Vanderbilt University ISIS, 2012.

[9] X. Li, W. Yan, and Y. Xue. SENTINEL: securing database from logic flaws in web applications. In CODASPY ’12, pages 25–36, 2012.

[10] P. Saxena, D. Akhawe, S. Hanna, F. Mao, S. McCamant, and D. Song. A symbolic execution framework for JavaScript. In Oakland’10, pages 513–528, 2010.

[11] P. Saxena, S. Hanna, P. Poosankam, and D. Song. FLAX: Systematic Discovery of Client-side Validation Vulnerabilities in Rich Web Applications. In NDSS’10, 2010.

[12] F. Sun, L. Xu, and Z. Su. Static Detection of Access Control Vulnerabilities in Web Applications. In USENIX’11, pages 11–11, 2011.