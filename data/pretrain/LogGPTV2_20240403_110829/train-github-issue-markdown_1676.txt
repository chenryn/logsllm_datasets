# ‚ùì Questions & Help
## Details
Is there a way to generate all possible sentences using a fine-tuned GPT-2
model given a certain sampling technique? For some reason I wan to exhaust all
possible combination of tokens given a fine-tuned GPT-2 model with certain
sampling technique. Is it doable? If it is not, how do we get an estimate of
how many possible sentences are there in the latent space?
**A link to original question on Stack Overflow** :