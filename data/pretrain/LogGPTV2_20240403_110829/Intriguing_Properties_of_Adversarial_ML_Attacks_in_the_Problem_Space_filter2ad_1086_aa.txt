title:Intriguing Properties of Adversarial ML Attacks in the Problem Space
author:Fabio Pierazzi and
Feargus Pendlebury and
Jacopo Cortellazzi and
Lorenzo Cavallaro
2020 IEEE Symposium on Security and Privacy
Intriguing Properties of Adversarial ML Attacks
in the Problem Space
Fabio Pierazzi∗†, Feargus Pendlebury∗†‡§, Jacopo Cortellazzi†, Lorenzo Cavallaro†
† King’s College London, ‡ Royal Holloway, University of London, § The Alan Turing Institute
Abstract—Recent research efforts on adversarial ML have
investigated problem-space attacks, focusing on the generation
of real evasive objects in domains where, unlike images, there
is no clear inverse mapping to the feature space (e.g., software).
However, the design, comparison, and real-world implications of
problem-space attacks remain underexplored.
This paper makes two major contributions. First, we propose
a novel formalization for adversarial ML evasion attacks in the
problem-space, which includes the deﬁnition of a comprehensive
set of constraints on available transformations, preserved seman-
tics, robustness to preprocessing, and plausibility. We shed light
on the relationship between feature space and problem space,
and we introduce the concept of side-effect features as the by-
product of the inverse feature-mapping problem. This enables
us to deﬁne and prove necessary and sufﬁcient conditions for
the existence of problem-space attacks. We further demonstrate
the expressive power of our formalization by using it to describe
several attacks from related literature across different domains.
Second, building on our formalization, we propose a novel
problem-space attack on Android malware that overcomes past
limitations. Experiments on a dataset with 170K Android apps
from 2017 and 2018 show the practical feasibility of evading
a state-of-the-art malware classiﬁer along with its hardened
version. Our results demonstrate that “adversarial-malware as
a service” is a realistic threat, as we automatically generate
thousands of realistic and inconspicuous adversarial applications
at scale, where on average it takes only a few minutes to
generate an adversarial app. Yet, out of the 1600+ papers on
adversarial ML published in the past six years, roughly 40 focus
on malware [15]—and many remain only in the feature space.
Our formalization of problem-space attacks paves the way to
more principled research in this domain. We responsibly release
the code and dataset of our novel attack to other researchers, to
encourage future work on defenses in the problem space.
Index Terms—adversarial machine learning; problem space;
input space; malware; program analysis; evasion.
I. INTRODUCTION
Adversarial ML attacks are being studied extensively in
multiple domains [11] and pose a major threat to the large-
scale deployment of machine learning solutions in security-
critical contexts. This paper focuses on test-time evasion
attacks in the so-called problem space, where the challenge
lies in modifying real input-space objects that correspond to
an adversarial feature vector. The main challenge resides in
the inverse feature-mapping problem [12, 13, 32, 46, 47, 58]
since in many settings it is not possible to convert a fea-
ture vector into a problem-space object because the feature-
mapping function is neither invertible nor differentiable. In
addition, the modiﬁed problem-space object needs to be a
∗Equal contribution.
valid, inconspicuous member of the considered domain, and
robust to non-ML preprocessing. Existing work investigated
problem-space attacks on text [3, 43], malicious PDFs [12,
22, 41, 45, 46, 74], Android malware [23, 75], Windows
malware [38, 60], NIDS [6, 7, 20, 28], ICS [76], source
code attribution [58], malicious Javascript [27], and eyeglass
frames [62]. However, while there is a good understanding on
how to perform feature-space attacks [16], it is less clear what
the requirements are for an attack in the problem space, and
how to compare strengths and weaknesses of existing solutions
in a principled way.
In this paper, motivated by examples on software, we
propose a novel formalization of problem-space attacks, which
lays the foundation for identifying key requirements and com-
monalities among different domains. We identify four major
categories of constraints to be deﬁned at design time: which
problem-space transformations are available to be performed
automatically while looking for an adversarial variant; which
object semantics must be preserved between the original
and its adversarial variant; which non-ML preprocessing the
attack should be robust to (e.g., image compression, code
pruning); and how to ensure that the generated object is a
plausible member of the input distribution, especially upon
manual inspection. We introduce the concept of side-effect
features as the by-product of trying to generate a problem-
space transformation that perturbs the feature space in a certain
direction. This allows us to shed light on the relationships
between feature space and problem space: we deﬁne and
prove necessary and sufﬁcient conditions for the existence of
problem-space attacks, and identify two main types of search
strategies (gradient-driven and problem-driven) for generating
problem-space adversarial objects.
We further use our formalization to describe several inter-
esting attacks proposed in both problem space and feature
space. This analysis shows that prior promising problem-
space attacks in the malware domain [31, 60, 75] suffer from
limitations, especially in terms of semantics and preprocessing
robustness. Grosse et al. [31] only add individual features
to the Android manifest, which preserves semantics, but can
be removed with preprocessing (e.g., by detecting unused
permissions); moreover, they are constrained by a maximum
feature-space perturbation, which we show is less relevant
for problem-space attacks. Rosenberg et al. [60] leave arti-
facts during the app transformation which are easily detected
through lightweight non-ML techniques. Yang et al. [75] may
signiﬁcantly alter the semantics of the program (which may
© 2020, Fabio Pierazzi. Under license to IEEE.
DOI 10.1109/SP40000.2020.00073
1332
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
account for the high failure rate observed in their mutated
apps), and do not specify which preprocessing techniques they
consider. These inspire us to propose, through our formaliza-
tion, a novel problem-space attack in the Android malware
domain that overcomes limitations of existing solutions.
In summary, this paper has two major contributions:
• We propose a novel formalization of problem-space at-
tacks (§II) which lays the foundation for identifying key
requirements and commonalities of different domains,
proves necessary and sufﬁcient conditions for problem-
space attacks, and allows for the comparison of strengths
and weaknesses of prior approaches—where existing
strategies for adversarial malware generation are among
the weakest in terms of attack robustness. We introduce
the concept of side-effect features, which reveals con-
nections between feature space and problem space, and
enables principled reasoning about search strategies for
problem-space attacks.
• Building on our
formalization, we propose a novel
problem-space attack in the Android malware domain,
which relies on automated software transplantation [10]
and overcomes limitations of prior work in terms of
semantics and preprocessing robustness (§III). We exper-
imentally demonstrate (§IV) on a dataset of 170K apps
from 2017-2018 that it is feasible for an attacker to evade
a state-of-the-art malware classiﬁer, DREBIN [8], and its
hardened version, Sec-SVM [23]. The time required to
generate an adversarial example is in the order of minutes,
thus demonstrating that the “adversarial-malware as a ser-
vice” scenario is a realistic threat, and existing defenses
are not sufﬁcient.
To foster future research on this topic, we discuss promising
defense directions (§V) and responsibly release the code and
data of our novel attack to other researchers via access to a
private repository (§VII).
II. PROBLEM-SPACE ADVERSARIAL ML ATTACKS
We focus on evasion attacks [12, 16, 32], where the ad-
versary modiﬁes objects at test time to induce targeted mis-
classiﬁcations. We provide background from related literature
on feature-space attacks (§II-A), and then introduce a novel
formalization of problem-space attacks (§II-B). Finally, we
highlight the main parameters of our formalization by instan-
tiating it on both traditional feature-space and more recent
problem-space attacks from related works in several domains
(§II-C). Threat modeling based on attacker knowledge and
capability is the same as in related work [11, 19, 65], and is
reported in Appendix B for completeness. To ease readability,
Appendix A reports a symbol table.
A. Feature-Space Attacks
We remark that all deﬁnitions of feature-space attacks
(§II-A) have already been consolidated in related work [11,
16, 21, 23, 31, 33, 44, 66]; we report them for completeness
and as a basis for identifying relationships between feature-
space and problem-space attacks in the following subsections.
We consider a problem space Z (also referred to as input
space) that contains objects of a considered domain (e.g.,
images [16], audio [17], programs [58], PDFs [45]). We
assume that each object z ∈ Z is associated with a ground-
truth label y ∈ Y, where Y is the space of possible labels.
Machine learning algorithms mostly work on numerical vector
data [14], hence the objects in Z must be transformed into a
suitable format for ML processing.
Deﬁnition 1 (Feature Mapping). A feature mapping is a
function ϕ : Z −→ X ⊆ R
n that, given a problem-space
object z ∈ Z, generates an n-dimensional feature vector
x ∈ X , such that ϕ(z) = x. This also includes implicit/latent
mappings, where the features are not observable in input
but are instead implicitly computed by the model (e.g., deep
learning [29]).
Deﬁnition 2 (Discriminant Function). Given an m-class ma-
chine learning classiﬁer g : X −→ Y, a discriminant function
h : X ×Y −→ R outputs a real number h(x, i), for which we
use the shorthand hi(x), that represents the ﬁtness of object x
to class i ∈ Y. Higher outputs of the discriminant function hi
represent better ﬁtness to class i. In particular, the predicted
label of an object x is g(x) = ˆy = arg maxi∈Y hi(x).
The purpose of a targeted feature-space attack is to modify
an object x ∈ X with assigned label y ∈ Y to an object x(cid:3)
that is classiﬁed to a target class t ∈ Y, t (cid:5)= y (i.e., to modify
x so that it is misclassiﬁed as a target class t). The attacker
can identify a perturbation δ to modify x so that g(x + δ) = t
by optimizing a carefully-crafted attack objective function. We
refer to the deﬁnition of attack objective function in Carlini
and Wagner [16] and in Biggio and Roli [11], which takes
into account high-conﬁdence attacks and multi-class settings.
Deﬁnition 3 (Attack Objective Function). Given an object
x ∈ X and a target label t ∈ Y, an attack objective function
f : X × Y −→ R is deﬁned as follows:
f (x, t) = max
i(cid:4)=t
{hi(x)} − ht(x) ,
(1)
for which we use the shorthand ft(x). Generally, x is classi-
ﬁed as a member of t if and only if ft(x) < 0. An adversary
can also enforce a desired attack conﬁdence κ ∈ R such that
the attack is considered successful if and only if ft(x) < −κ.
The intuition is to minimize ft by modifying x in directions
that follow the negative gradient of ft, i.e., to get x closer to
the target class t.
In addition to the attack objective function, a considered
problem-space domain may also come with constraints on the
modiﬁcation of the feature vectors. For example, in the image
domain the value of pixels must be bounded between 0 and
255 [16]; in software, some features in x may only be added
but not removed (e.g., API calls [23]).
Deﬁnition 4 (Feature-Space Constraints). We deﬁne Ω as the
set of feature-space constraints, i.e., a set of constraints on
the possible feature-space modiﬁcations. The set Ω reﬂects
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1333
As examples of feature-space constraints,
the requirements of realistic problem-space objects. Given an
object x ∈ X , any modiﬁcation of its feature values can be
represented as a perturbation vector δ ∈ R
n; if δ satisﬁes Ω,
we borrow notation from model theory [72] and write δ |= Ω.
in the image
domain [e.g., 11, 16] the perturbation δ is subject
to an
upper bound based on lp norms (||δ||p ≤ δmax), to preserve
similarity to the original object; in the software domain [e.g.,
23, 31], only some features of x may be modiﬁed, such that
δlb (cid:7) δ (cid:7) δub (where δ1 (cid:7) δ2 implies that each element of
δ1 is ≤ the corresponding i-th element in δ2).
We can now formalize the traditional feature-space attack
as in related work [11, 12, 16, 23, 52].
Deﬁnition 5 (Feature-Space Attack). Given a machine learn-
ing classiﬁer g, an object x ∈ X with label y ∈ Y, and a
target label t ∈ Y, t (cid:5)= y, the adversary aims to identify a
perturbation vector δ ∈ R
n such that g(x + δ) = t. The
desired perturbation can be achieved by solving the following
optimization problem:
δ∗
ft(x + δ)
= arg min
δ∈Rn
subject to: δ |= Ω .
(2)
(3)
A feature-space attack is successful if ft(x + δ∗
than −κ, if a desired attack conﬁdence is enforced).
) < 0 (or less
Without loss of generality, we observe that the feature-space
attacks deﬁnition can be extended to ensure that the adversarial
example is closer to the training data points (e.g., through the
tuning of a parameter λ that penalizes adversarial examples
generated in low density regions, as in the mimicry attacks
of Biggio et al. [12]).
B. Problem-Space Attacks
This section presents a novel formalization of problem-
space attacks and introduces insights into the relationship
between feature space and problem space.
Inverse Feature-Mapping Problem. The major challenge
that complicates (and,
in most cases, prevents) the direct
applicability of gradient-driven feature-space attacks to ﬁnd
problem-space adversarial examples is the so-called inverse
feature-mapping problem [12, 13, 32, 46, 47, 58]. As an
extension, Quiring et al. [58] discuss the feature-problem space
dilemma, which highlights the difﬁculty of moving in both
directions: from feature space to problem space, and from
problem space to feature space. In most cases, the feature
mapping function ϕ is not bijective, i.e., not injective and not
surjective. This means that given z ∈ Z with features x, and a
feature-space perturbation δ∗, there is no one-to-one mapping
that allows going from x+δ∗ to an adversarial problem-space
(cid:3). Nevertheless, there are two additional scenarios. If
object z
ϕ is not invertible but is differentiable, then it is possible to
backpropagate the gradient of ft(x) from X to Z to derive
how the input can be changed in order to follow the negative
gradient (e.g., to know which input pixels to perturbate to
follow the gradient in the deep-learning latent feature space).
(cid:3)
If ϕ is not invertible and not differentiable, then the challenge
is to ﬁnd a way to map the adversarial feature vector x(cid:3) ∈ X to
(cid:3) ∈ Z, by applying a transformation to z
an adversarial object z
(cid:3) such that ϕ(z
in order to produce z
) is “as close as possible”
to x(cid:3); i.e., to follow the gradient towards the transformation
that most likely leads to a successful evasion [38]. In problem-
space settings such as software, the function ϕ is typically not
invertible and not differentiable, so the search for transforming
z to perform the attack cannot be purely gradient-based.
In this section, we consider the general case in which the
feature mapping ϕ is not differentiable and not invertible (i.e.,
the most challenging setting), and we refer to this context to
formalize problem-space evasion attacks.
First, we deﬁne a problem-space transformation operator
through which we can alter problem-space objects. Due to
their generality, we adapt the code transformation deﬁnitions
from the compiler engineering literature [1, 58] to formalize
general problem-space transformations.
Deﬁnition 6 (Problem-Space Transformation). A problem-
space transformation T : Z −→ Z takes a problem-space
object z ∈ Z as input and modiﬁes it to z
(cid:3) ∈ Z. We refer to
the following notation: T (z) = z
(cid:3).
The possible problem-space transformations are either ad-
dition, removal, or modiﬁcation (i.e., combination of addition
and removal). In the case of programs, obfuscation is a special
case of modiﬁcation.
Deﬁnition 7 (Transformation Sequence). A transformation
sequence T = Tn◦Tn−1◦···◦T1 is the subsequent application
of problem-space transformations to an object z ∈ Z.
Intuitively, given a problem-space object z ∈ Z with label
y ∈ Y, the purpose of the adversary is to ﬁnd a transformation
sequence T such that the transformed object T(z) is classiﬁed
into any target class t chosen by the adversary (t ∈ Y,
t (cid:5)= y). One way to achieve such a transformation is to ﬁrst
compute a feature-space perturbation δ∗, and then modify the
problem-space object z so that features corresponding to δ∗
are carefully altered. However, in the general case where the
feature mapping ϕ is neither invertible nor differentiable, the
adversary must perform a search in the problem-space that
approximately follows the negative gradient in the feature
space. However, this search is not unconstrained, because the
adversarial problem-space object T(z) must be realistic.
Problem-Space Constraints. Given a problem-space ob-
ject z ∈ Z, a transformation sequence T must lead to an
= T(z) that is valid and realistic. To express this
object z
formally, we identify four main types of constraints common
to any problem-space attack:
(cid:3)
1) Available transformations, which describe which modi-
ﬁcations can be performed in the problem-space by the
attacker (e.g., only addition and not removal).
2) Preserved semantics,
the semantics to be preserved
(cid:3), with respect to speciﬁc feature
while mutating z to z
abstractions which the attacker aims to be resilient
against (e.g., in programs, the transformed object may
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1334
3) Plausibility (or
need to produce the same dynamic call traces). Seman-
tics may also be preserved by construction [e.g., 58].
Inconspicuousness), which describes
which (qualitative) properties must be preserved in mu-
(cid:3), so that z appears realistic upon manual
tating z to z
inspection. For example, often an adversarial
image
must look like a valid image from the training distribu-
tion [16]; a program’s source code must look manually
written and not artiﬁcially or inconsistently altered [58].
In the general case, veriﬁcation of plausibility may be
hard to automate and may require human analysis.
4) Robustness to preprocessing, which determines which
non-ML techniques could disrupt the attack (e.g., ﬁlter-
ing in images, dead code removal in programs).
These constraints have been sparsely mentioned in prior
literature [11, 12, 58, 74], but have never been identiﬁed
together as a set for problem-space attacks. When designing
a novel problem-space attack, it is fundamental to explicitly
deﬁne these four types of constraints, to clarify strengths and
weaknesses. While we believe that this framework captures
all nuances of the current state-of-the-art for a thorough
evaluation and comparison, we welcome future research that
uses this as a foundation to identify new constraints.
We now introduce formal deﬁnitions for the constraints.
First, similarly to [11, 23], we deﬁne the space of available
transformations.
Deﬁnition 8 (Available Transformations). We deﬁne T as
the space of available transformations, which determines
which types of automated problem-space transformations T
the attacker can perform. In general, it determines if and how
the attacker can add, remove, or edit parts of the original object
z ∈ Z to obtain a new object z
(cid:3) ∈ Z. We write T ∈ T if a
transformation sequence consists of available transformations.
For example, the pixels of an image may be modiﬁed only
if they remain within the range of integers 0 to 255 [e.g., 16];
in programs, an adversary may only add valid no-op API calls
to ensure that modiﬁcations preserve functionality [e.g., 60].
Moreover, the attacker needs to ensure that some semantics
are preserved during the transformation of z, according to
some feature abstractions. Semantic equivalence is known
to be generally undecidable [10, 58]; hence, as in [10], we
formalize semantic equivalence through testing, by borrowing
notation from denotational semantics [57].
Deﬁnition 9 (Preserved Semantics). Let us consider two
= T(z), and a suite of
problem-space objects z and z
automated tests Υ to verify preserved semantics. We deﬁne
(cid:3) to be semantically equivalent with respect to Υ if
z and z
they satisfy all its tests τ ∈ Υ, where τ : Z × Z −→ B. In
particular, we denote semantics equivalence with respect to a
test suite Υ as follows:
(cid:3)
(cid:3)(cid:3)τ , ∀τ ∈ Υ ,
(cid:2)z(cid:3)τ = (cid:2)z
(4)
where (cid:2)z(cid:3)τ denotes the semantics of z induced during test τ.
Informally, Υ consists of tests that are aimed at evaluating
(cid:3) (or parts of them) lead to the same abstract
whether z and z
representations in a certain feature space. In other words,
the tests in Υ model preserved semantics. For example, in
programs a typical test aims to verify that malicious func-
tionality is preserved; this is done through tests where, given
a certain test input, the program produces exactly the same
output [10]. Additionally, the attacker may want to ensure that
(cid:3)) leads to the same instruction trace
an adversarial program (z
as its benign version (z)—so as not to raise suspicion in feature
abstractions derived from dynamic analysis.
Plausibility is more subjective than semantic equivalence,
but in many scenarios it is critical that an adversarial object is
inconspicuous when manually audited by a human. In order
to be plausible, an analyst must believe that the adversarial
object is a valid member of the problem-space distribution.
Deﬁnition 10 (Plausibility). We deﬁne Π as the set of (typi-
cally) manual tests to verify plausibility. We say z looks like
a valid member of the data distribution to a human being if it
satisﬁes all tests π ∈ Π, where π : Z −→ B.
Plausibility is often hard to verify automatically; previous
work has often relied on user studies with domain experts to
judge the plausibility of the generated objects (e.g., program
plausibility in [58], realistic eyeglass frames in [62]). Plau-
sibility in software-related domains may also be enforced by
construction during the transformation process, e.g., by relying
on automated software transplantation [10, 75].
In addition to semantic equivalence and plausibility, the
adversarial problem-space objects need to ensure they are
robust to non-ML automated preprocessing techniques that
could alter properties on which the adversarial attack depends,
thus compromising the attack.
Deﬁnition 11 (Robustness to Preprocessing). We deﬁne Λ
= T(z)
as the set of preprocessing operators an object z
(cid:3) is robust to preprocessing if
should be resilient to. We say z
A(T(z)) = T(z) for all A ∈ Λ, where A : Z −→ Z simulates
an expected preprocessing.
(cid:3)
Examples of preprocessing operators in Λ include compres-
sion to remove pixel artifacts (in images), ﬁlters to remove
noise (in audio), and program analysis to remove dead or
redundant code (in programs).
Properties affected by preprocessing are often related to
fragile and spurious features learned by the target classiﬁer.
While taking advantage of such features may be necessary to
demonstrate the weaknesses of the target model, an attacker
should be aware that these brittle features are usually the ﬁrst
to change when a model is improved. Given this, a stronger
attack is one that does not rely on them.
As a concrete example, in an attack on authorship attri-
bution, Quiring et al. [58] purposefully omit layout features
(such as the use of spaces vs. tabs) which are trivial to change.
Additionally, Xu et al. [74] discovered the presence of font
objects was a critical (but erroneously discriminative) feature
following their problem-space attack on PDF malware. These
are features that are cheap for an attacker to abuse but can be
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1335
easily removed by the application of some preprocessing. As
a defender, investigation of this constraint will help identify
features that are weak to adversarial attacks. Note that knowl-
edge of preprocessing can also be exploited by the attacker
(e.g., in scaling attacks [73]).
We can now deﬁne a fundamental set of problem-space
constraint elements from the previous deﬁnitions.
Deﬁnition 12 (Problem-Space Constraints). We deﬁne the
problem-space constraints Γ = {T , Υ, Π, Λ} as the set of
all constraints satisfying T , Υ, Π, Λ. We write T(z) |= Γ if
a transformation sequence applied to object z ∈ Z satisﬁes
all the problem-space constraints, and we refer to this as a
valid transformation sequence. The problem-space constraints
Γ determine the feature-space constraints Ω, and we denote
this relationship as Γ (cid:10) Ω (i.e., Γ determines Ω); with a slight
abuse of notation, we can also write that Ω ⊆ Γ, because
some constraints may be speciﬁc to the problem space (e.g.,
program size similar to that of benign applications) and may
not be possible to enforce in the feature space X .
(cid:3)
(cid:3)
(cid:3) such that ϕ(z
Side-Effect Features. Satisfying the problem-space con-
straints Γ further complicates the inverse feature mapping,
as Γ is a superset of Ω. Moreover, enforcing Γ may require
substantially altering an object z to ensure satisfaction of all
constraints during mutations. Let us focus on an example in
the software domain, so that z is a program with features x;
) = x + δ,
if we want to transform z to z
we may want to add to z a program o where ϕ(o) = δ.
However, the union of z and o may have features different
from x + δ, because other consolidation operations are re-
quired (e.g., name deduplication, class declarations, resource
name normalization)—which cannot be feasibly computed in
advance for each possible object in Z. Hence, after modifying
(cid:3) with certain
z in an attempt to obtain a problem-space object z
features (e.g., close to x + δ), the attacker-modiﬁed object
may have some additional features that are not related to the
intended transformation (e.g., adding an API which maps to a
feature in δ), but are required to satisfy all the problem-space
constraints in Γ (e.g., inserting valid parameters for the API
call, and importing dependencies for its invocation). We call
= T(z)
side-effect features η the features that are altered in z
speciﬁcally for the satisfaction of problem-space constraints.
We observe that these features do not follow any particular
direction of the gradient, and hence they could have both a
positive or negative impact on the classiﬁcation score.
Analogy with Projection. Figure 1 presents an analogy
between side-effect features η and the notion of projection in
numerical optimization [14], which helps explain the nature
and impact of η in problem-space attacks. The right half
corresponds to higher values of a discriminant function h(x)
and the left half to lower values. The vertical central curve
(where the heatmap value is equal to zero) represents the
decision boundary: objects on the left-half are classiﬁed as
negative (e.g., benign), and objects on the right-half as positive
(e.g., malicious). The goal of the adversary is to conduct a
maximum conﬁdence attack that has an object misclassiﬁed
Γ
Ω
x + δ* + η
x + δ*
xx
x
Γ
Fig. 1. Example of projection of the feature-space attack vector x+δ∗ in the
feasible problem space, resulting in side-effect features η. The background
displays the value of the discriminant function h(x), where negative values
indicate the target class of the evasion attack. Small arrows represent directions
of the negative gradient. The thick solid line represents the feasible feature
space determined by Ω, and the thin solid line that determined by Γ (which
is more restrictive). The dotted arrow represents the gradient-based attack
x + δ∗ derived from x, which is then projected into x + δ∗ + η to ﬁt into
the feasible problem space.
as the negative class. The thick solid line represents the
feasible feature space determined by constraints Ω, and the
thin solid line the feasible problem space determined by Γ
(which corresponds to two unconnected areas). We assume that
the initial object x ∈ X is always within the feasible problem
space. In this example, the attacker ﬁrst conducts a gradient-
based attack in the feature space on object x, which results in
a feature vector x + δ∗, which is classiﬁed as negative with
high-conﬁdence. However, this point is not in the feasibility
space of constraints Γ, which is more restrictive than that of Ω.
Hence, the attacker needs to ﬁnd a projection that maps x+δ∗
back to the feasible problem-space regions, which leads to the
addition of a side-effect feature vector η.
Deﬁnition 13 (Side-Effect Feature Vector). We deﬁne η as the
side-effect feature vector that results from enforcing Γ while
choosing a sequence of transformations T such that T(z) |= Γ.
In other words, η are the features derived from the projection
of a feature-space attack onto a feasibility region that satisﬁes
problem-space constraints Γ.
We observe that in settings where the feature mapping ϕ is
neither differentiable nor invertible, and where the problem-
space representation is very different from the feature-space
representation (e.g., unlike in images or audio), it is generally
infeasible or impossible to compute the exact impact of side-
effect features on the objective function in advance—because
the set of problem-space constraints Γ cannot be expressed
analytically in closed-form. Hence the attacker needs to ﬁnd
a transformation sequence T such that ϕ(T(z)) = ϕ(z
) is
within the feasibility region of problem-space constraints Γ.
It is relevant to observe that, in the general case, if an object
zo is added to (or removed from) two different objects z1 and
z2, it is possible that the resulting side-effect feature vectors
η1 and η2 are different (e.g., in the software domain [58]).
(cid:3)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1336
Considerations on Attack Conﬁdence. There are some im-
portant characteristics of the impact of the side-effect features
η on the attack objective function. If the attacker performs
a maximum-conﬁdence attack in the feature space under con-
straints Ω, then the conﬁdence of the problem-space attack
will always be lower or equal than the one in the feature-
space attack. This is intuitively represented in Figure 1, where
the point is moved to the maximum-conﬁdence attack area
within Ω, and the attack conﬁdence is reduced after projection
to the feasibility space of the problem space, induced by Γ.
In general, the conﬁdence of the feature- and problem-space
attacks could be equal, depending on the constraints Ω and
Γ, and on the shape of the discriminant function h, which
is also not necessarily convex (e.g., in deep learning [29]).
In the case of low-conﬁdence feature-space attacks, projecting
into the problem-space feasibility constraint may result in a
positive or negative impact (not known a priori) on the value
of the discriminant function. This can be seen from Figure 1,
where the object x + δ∗ would be found close to the center
of the plot, where h(x) = 0.
Problem-Space Attack. We now have all the components
required to formalize a problem-space attack.
Deﬁnition 14 (Problem-Space Attack). We deﬁne a problem-
space attack as the problem of ﬁnding the sequence of valid
transformations T for which the object z ∈ Z with label y ∈ Y
is misclassiﬁed to a target class t ∈ Y as follows:
argminT∈T
subject to:
+ η)
ft(ϕ(T(z))) = ft(x + δ∗
∀τ ∈ Υ
(cid:2)z(cid:3)τ = (cid:2)T(z)(cid:3)τ ,
π(T(z)) = 1,
A(T(z)) = T(z),
∀π ∈ Π
∀A ∈ Λ
(5)
(6)
(7)
(8)
where η is a side-effect feature vector that separates the feature
vector generated by T(z) from the theoretical feature-space
attack x + δ∗ (under constraints Ω). An equivalent, more
compact, formulation is as follows:
ft(ϕ(T(z))) = ft(x + δ∗
argminT∈T
subject to: T(z) |= Γ .
+ η)
(9)
(10)
Search Strategy. The typical search strategy for adversarial
perturbations in feature-space attacks is based on follow-
ing the negative gradient of the objective function through
some numerical optimization algorithm, such as stochastic
gradient descent [11, 16, 17]. However, it is not possible to
directly apply gradient descent in the general case of problem-
space attacks, when the feature space is not invertible nor
differentiable [11, 58]; and it is even more complicated if
a transformation sequence T produces side-effect features
η (cid:5)= 0. In the problem space, we identify two main types
of search strategy: problem-driven and gradient-driven. In
the problem-driven approach, the search of the optimal T
proceeds heuristically by beginning with random mutations
of the object z, and then learning from experience how to
appropriately mutate it further in order to misclassify it to
the target class (e.g., using Genetic Programming [74] or
variants of Monte Carlo tree search [58]). This approach
iteratively uses local approximations of the negative gradient
to mutate the objects. The gradient-driven approach attempts
to identify mutations that follow the negative gradient by
relying on an approximate inverse feature mapping (e.g., in
PDF malware [46], in Android malware [75]). If a search
strategy equally makes extensive use of both problem-driven
and gradient-driven methods, we call it a hybrid strategy.
We note that search strategies may have different trade-offs
in terms of effectiveness and costs, depending on the time
and resources they require. While there are some promising
avenues in this challenging but important line of research [39],
it warrants further investigation in future work.
Feature-space attacks can still give us some useful
in-
formation: before searching for a problem-space attack, we
can verify whether a feature-space attack exists, which is a
necessary condition for realizing the problem-space attack.
Theorem 1 (Necessary Condition for Problem-Space Attacks).
Given a problem-space object z ∈ Z of class y ∈ Y, with
features ϕ(z) = x, and a target class t ∈ Y, t (cid:5)= y, there
exists a transformation sequence T that causes T(z) to be
misclassiﬁed as t only if
there is a solution for the feature-
space attack under constraints Ω. More formally, only if:
∃δ∗
= arg min
δ∈Rn:δ|=Ω
ft(x + δ) : ft(x + δ∗
) < 0 .
(11)
The proof of Theorem 1 is in Appendix C. We observe that
Theorem 1 is necessary but not sufﬁcient because, although it
is not required to be invertible or differentiable, some sort of
“mapping” between problem- and feature-space perturbations
needs to be known by the attacker. A sufﬁcient condition for a
problem-space attack, reﬂecting the attacker’s ideal scenario,
is knowledge of a set of problem-space transformations which
can alter feature values arbitrarily. This describes the scenario
for some domains, such as images [16, 30], in which the
attacker can modify any pixel value of an image independently.
Theorem 2 (Sufﬁcient Condition for Problem-Space Attacks).
Given a problem-space object z ∈ Z of class y ∈ Y, with
features ϕ(z) = x, and a target class t ∈ Y, t (cid:5)= y, there exists
a transformation sequence T that causes x to be misclassiﬁed
as t if Equation 11 and Equation 12 are satisﬁed:
∃δ∗
∀δ ∈ R
ft(x + δ) : ft(x + δ∗
(11)
∃T : T(z) |= Γ, ϕ(T(z)) = x + δ (12)
Informally, an attacker is always able to ﬁnd a problem-space
attack if a feature-space attack exists (necessary condition) and
they know problem-space transformations that can modify any
feature by any value (sufﬁcient condition).
= arg min
n : δ |= Ω,
δ∈Rn:δ|=Ω
) < 0
The proof of Theorem 2 is in Appendix C. In the general
case, while there may exist an optimal feature-space perturba-
tion δ∗, there may not exist a problem-space transformation
sequence T that alters the feature space of T(z) exactly so that
ϕ(T(z)) = x + δ∗. This is because, in practice, given a target
feature-space perturbation δ∗, a problem-space transformation
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1337
+ η∗, where η∗ (cid:5)= 0
may generate a vector ϕ(T(z)) = x + δ∗
(i.e., where there may exist at least one i for which ηi (cid:5)= 0)
due to the requirement that problem-space constraints Γ must
be satisﬁed. This prevents easily ﬁnding a problem-space
transformation that follows the negative gradient. Given this,
the attacker is forced to apply some search strategy based on
the available transformations.
Corollary 2.1. If Theorem 2 is satisﬁed only on a subset
in X , which collectively create
of feature dimensions Xi
a subspace Xeq ⊂ X ,
the
then the attacker can restrict
search space to Xeq, for which they know that an equivalent
problem/feature-space manipulation exists.
C. Describing problem-space attacks in different domains
Table I illustrates the main parameters that need to be
explicitly deﬁned while designing problem-space attacks by
considering a representative set of adversarial attacks in dif-
ferent domains: images [16], facial recognition [62], text [56],
PDFs [74], Javascript [27], code attribution [58], and three
problem-space attacks applicable to Android: two from the
literature [60, 75] and ours proposed in §III.
This table shows the expressiveness of our formalization,
and how it is able to reveal strengths and weaknesses of
different proposals. In particular, we identify some major
limitations in two recent problem-space attacks [60, 75].
Rosenberg et al. [60] leave artifacts during the app transfor-
mation which are easily detected without the use of machine
learning (see §VI for details), and relies on no-op APIs which
could be removed through dynamic analysis. Yang et al. [75]
do not specify which preprocessing they are robust against,
and their approach may signiﬁcantly alter the semantics of the
program—which may account for the high failure rate they
observe in the mutated apps. This inspired us to propose a
novel attack that overcomes such limitations.
III. ATTACK ON ANDROID
Our formalization of problem-space attacks has allowed
for the identiﬁcation of weaknesses in prior approaches to
malware evasion applicable to Android [60, 75]. Hence, we
propose—through our formalization—a novel problem-space
attack in this domain that overcomes these limitations, es-
pecially in terms of preserved semantics and preprocessing
robustness (see §II-C and §VI for a detailed comparison).
A. Threat Model
We assume an attacker with perfect knowledge θP K =
(D,X , g, w) (see Appendix B for details on threat models).
This follows Kerckhoffs’ principle [37] and ensures a defense
does not rely on “security by obscurity” by unreasonably as-
suming some properties of the defense can be kept secret [19].
Although deep learning has been extensively studied in adver-
sarial attacks, recent research [e.g., 55] has shown that—if re-
trained frequently—the DREBIN classiﬁer [8] achieves state-
of-the-art performance for Android malware detection, which
makes it a suitable target classiﬁer for our attack. DREBIN
relies on a linear SVM, and embeds apps in a binary feature-
space X which captures the presence/absence of components
in Android applications in Z (such as permissions, URLs,
Activities, Services, strings). We assume to know classiﬁer
g and feature-space X , and train the parameters w with
SVM hyperparameter C = 1, as in the original DREBIN
paper [8]. Using DREBIN also enables us to evaluate the
effectiveness of our problem-space attack against a recently
proposed hardened variant, Sec-SVM [23]. Sec-SVM enforces
more evenly distributed feature weights, which require an
attacker to modify more features to evade detection.
We consider an attacker intending to evade detection based
on static analysis, without relying on code obfuscation as it
may increase suspiciousness of the apps [67, 69] (see §V).
B. Available Transformations
We use automated software transplantation [10] to extract
slices of bytecode (i.e., gadgets) from benign donor appli-
cations and inject them into a malicious host, to mimic the
appearance of benign apps and induce the learning algorithm
to misclassify the malicious host as benign.1 An advantage
of this process is that we avoid relying on a hardcoded set
of transformations [e.g., 58]; this ensures adaptability across
different application types and time periods. In this work, we
consider only addition of bytecode to the malware—which
ensures that we do not hinder the malicious functionality.
Organ Harvesting. In order to augment a malicious host
with a given benign feature Xi, we must ﬁrst extract a
bytecode gadget ρ corresponding to Xi from some donor
app. As we intend to produce realistic examples, we use
program slicing [71] to extract a functional set of statements
that includes a reference to Xi. The ﬁnal gadget consists of the
this target reference (entry point Lo), a forward slice (organ
o), and a backward slice (vein v). We ﬁrst search for Lo,
corresponding to an appearance of code corresponding to the
desired feature in the donor. Then, to obtain o, we perform a
context-insensitive forward traversal over the donor’s System
Dependency Graph (SDG), starting at the entry point, tran-
sitively including all of the functions called by any function