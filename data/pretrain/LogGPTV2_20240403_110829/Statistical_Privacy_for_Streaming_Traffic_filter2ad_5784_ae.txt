Usability. While we cannot directly quantify the user expe-
rience, in our evaluation, the video streaming went smoothly
without pausing after buffering for roughly 3 seconds at the
2A burst is the total size of all packets whose timestamps are no farther
apart than a threshold. Here the threshold is set to 0.5s.
0200400600800100012001400Waste: percentage over original (%)0100200300400500600700800FrequencyFPAkd∗0510152025303540Deficit: percentage over original (%)0500100015002000250030003500FrequencyFPAkd∗wA (s) BPB up BPB down BPB PPB up
PPB down
0.05
0.25
0.5
1
2
0.16
0.20
0.19
0.16
0.14
0.12
0.16
0.12
0.14
0.12
0.16
0.22
0.22
0.18
0.16
0.12
0.18
0.14
0.14
0.13
0.16
0.16
0.16
0.19
0.14
PPB LPB up
0.14
0.20
0.20
0.13
0.16
0.14
0.12
0.14
0.10
0.10
LPB down
0.13
0.08
0.08
0.10
0.10
LPB BURST up BURST down BURST ALL
0.13
0.16
0.21
0.16
0.10
0.20
0.18
0.11
0.09
0.17
0.16
0.19
0.15
0.12
0.19
0.14
0.23
0.19
0.16
0.16
0.15
0.14
0.14
0.14
0.16
TABLE III: Classiﬁcation result when the Chrome extension is enabled. Each column represents the accuracy when trained with
the speciﬁed feature. The features are up/down/total bytes per bin (BPB), up/down/total packets per bin (PPB), up/down/total
average packet length per bin (LPB), up/down/total bursts (BURST ), and the combination of all 12 features (ALL).
very beginning. We leave a comprehensive user study on the
usability as future work.
Nevertheless, an optimal implementation of our statistical
privacy mechanisms would enforce the privacy on both the
client side and the server side. The browser extension can only
control the request rate of the Youtube video streaming, but
cannot directly control the response rate from the server. If the
server chooses to respond to a request with a packet pattern that
is speciﬁc to the downloaded video, privacy of the streaming
trafﬁc can not be protected by the extension alone. Fortunately,
as shown in our experiment, it is not the case—packet patterns
in the video download are not content-speciﬁc. Therefore, the
packet patterns do not leak additional information.
VIII. DISCUSSION
In this section, we discuss the limitation and extension of
the statistical privacy approaches.
Leakage through video length. Neither of the two statistically
private mechanisms prevents leakage through the length of the
videos. Intuitively, to make an 1-minute video indistinguish-
able from an 1-hour video, considerable amount of noise must
be added to hide the difference of the video length, as the
L2 sensitivity in this case is prohibitively high. As a result,
the utility of the solution will drop signiﬁcantly. Therefore, in
practice, it is more desirable to only make videos with similar
length indistinguishable from one another. To do so, grouping
the videos by length and padding them to the longest length in
each group might be a good solution. For example, all videos
of the length between 50 minutes to 1 hour could be considered
in one group and padded so that all of them appear to be a
1-hour video.
Comparing FPAk with d∗. In Sec. VI-C, we compared the
utility of the two differentially private mechanisms with certain
w and  parameters selected to render the CNN classiﬁer
ineffective. However, CNN classiﬁcation accuracy does not
translate directly to security guarantees. As these mechanisms
offer different theoretical privacy guarantees, directly compar-
ing them is less meaningful. However, it is worth mentioning
that FPAk mechanism additionally requires the knowledge of
the entire time series x before transforming it into the noised
version ˜x. This additional requirement may be less desirable
in scenarios where such information is not available.
Applying differential privacy to website ﬁngerprinting. Al-
though we have shown that differentially private mechanisms
are promising countermeasures to streaming trafﬁc analysis
attacks, directly applying the same approach to prevent website
ﬁngerprinting requires some modiﬁcations. Unlike streaming
trafﬁc, HTTP trafﬁc is more interactive. For example, an
HTML web page may embed a number of objects (e.g.,
JavaScript ﬁles or images) that will be downloaded after the
HTML ﬁle is parsed by the browser. While streaming allows
us to proactively request video contents beforehand and cache
data locally, the download of some HTML resources can only
start after ﬁnishing the download of a previous resource. We
plan to address this type of interactive web trafﬁc and expand
our approach to WF attacks in future work.
Reducing waste. As shown in Fig. 11a, the median waste
of FPAk and d∗ are 200% and 600%, respectively, compared
to the original traces. To be practical, measures must be taken
to lower the waste. For example, one can lower the security
guarantee by increasing the , so that the amount of noise
added is reduced. Another possible approach is to make the
upper clip bound smaller. In Fig. 11a, the upper clip bound is
set to 1GB, which is far from realistic scenarios, since it is
impossible for the server to send 1GB in a single time frame
for all the ws we tested. It would be more reasonable to ﬁnd
an empirical clip bound based on real-world statistics.
IX. RELATED WORK
A. Defenses against Side-Channel Attacks
Our work has been inﬂuenced by prior studies that insert
noise to obfuscate side-channel observations. Many research
projects have tried to perturb timers to mitigate timing side-
channel attacks [34], [35], [39], [68]. Researchers have also
shown that adding noise to shared resources can be an effective
defense [5], [28], [59], [75]. Particularly relevant to our work
is Xiao et al.’s [72], which introduced the d∗ algorithm to
mitigate storage side channels resulting from procfs in the
Linux OS, so that statistics reporting through procfs satisﬁes
d-privacy for a meaningful distance metric d∗. Their work
considered interactive statistical data release, i.e., in which the
defender knows exactly when and how the adversary observes
the data. In our case, the adversary does not have to interact
with the defense system; he only needs to passively observe
the streaming trafﬁc, which requires this defense to be more
pervasively applied. This, in turn, underscores the importance
of measuring its utility impact, as we have done here.
B. Privacy of Time-Series Data
Our work is built upon a number of previous studies
that apply differential privacy to time-series data. Rastogi et
al. [52] proposed the Fourier Perturbation Algorithm (FPAk)
algorithm to ensure differential privacy for time-series data. To
avoid relying on a trusted central server, the work also pro-
posed the Distributed Laplace Perturbation Algorithm (DLPA)
for distributed time-series data. Shi et al. [58] proposed
aggregator-oblivious encryption to ensure differential privacy
12
for distributed time-series data. Benhamouda et al. [3] ex-
tended this work to introduce a general framework for con-
structing privacy-preserving aggregator-oblivious encryption
schemes. Fan et al. [20] presented a framework, FAST, to
release real-time aggregate statistics under differential privacy
based on ﬁltering and adaptive sampling. Cao et al. [9]
proposed two methods to answer a subset of representative
sliding window queries with differential privacy. Kellaris et
al. [27] introduced ω-event privacy over inﬁnite streams,
which protects any event sequence occurring in ω successive
timestamps. None of these works has considered applying
differential privacy to defeat trafﬁc analysis, however.
C. Website Fingerprinting Defenses
One important branch of trafﬁc analysis is website ﬁnger-
printing (WF) on encrypted channels or anonymity networks
(e.g., Tor). In a typical WF attack, the adversary utilizes su-
pervised machine learning techniques to train a classiﬁer with
encrypted network trafﬁc to/from a set of websites of interest
and then classify unknown trafﬁc captured from the victim.
Prior works have shown effectiveness of such attacks [8], [23],
[47], [48], [60], [61], [69].
Accordingly, many research projects have explored mech-
anisms to address this security threat. Panchenko et al. [48]
proposed a defense called Decoy, which loads two webpages
at the same time so that the adversary is confused. Luo et
al. [37] published HTTPOS (HTTP Obfuscation). The defense
was implemented on the client side by splitting the trafﬁc
into packets with random size using the HTTP range header.
Dyer et al. [19] provide a comprehensive study of countermea-
sures of trafﬁc analysis and proposes a mechanism dubbed
BuFLO that modiﬁes the trafﬁc to enforce a constant rate.
Two follow-up works by Tamaraw et al. [7] and Cai et al. [6]
aimed to reduce the overhead of BuFLO by grouping websites
with similar trafﬁc patterns and padding them according to
the one with greatest size in each group. This grouping-and-
padding method was used by other papers as well [45], [69].
Juarez et al. [26] proposed a system called WTF-PAD. It
deploys adaptive padding for WF defense in Tor, which only
adds padding when the usage of the channel is low, so that
the bursts of trafﬁc are eliminated. Recently, a defense called
Walkie-Talkie was proposed by Wang et al. [70], which uses
half-duplex communication to ensure one way communication
at a time and adds dummy packets and inserts delays to
make the trafﬁc of different websites look alike. The major
difference between these work and ours is that our method
is designed with a theoretical privacy guarantee. We believe
our solution can be applied to WF attacks as well. However,
unlike streaming trafﬁc, which is essentially non-interactive,
additional care must be taken to eliminate leakage through
interactive trafﬁc patterns. We plan to expand our approach to
WF attacks in future work.
D. Private Messaging Systems
Prior works have applied differential privacy techniques
in private messaging systems. One of the ﬁrst systems is
Vuvuzela [67], which is a large-scale private messaging sys-
tem that protects against both passive and active adversaries
with differential privacy guarantee. Alpenhorn [31] extends
Vuvuzela by providing strong privacy and forward secrecy
guarantees for metadata. Alpenhorn uses the mixnet design
provided by Vuvuzela, which guarantees its differential pri-
vacy. A recently published system, called Stadium [66], ex-
tends Vuvuzela horizontally by providing point-to-point data
privacy while maintaining a low latency. Similar to Vuvuzela,
Stadium has a security guarantee of differential privacy by veri-
ﬁable shufﬂing and adding dummy messages. Another private
messaging system, Atom [30], also employs the differential
privacy technique proposed by Vuvuzela to hide the number
of dialing calls a user receives. Moreover, Atom guarantees the
users can have anonymity among honest users besides differ-
ential privacy. Although these works also applied differential
privacy to prevent trafﬁc analysis, however, their scenarios
are completely different. In these private messaging systems,
the information they are trying to hide is the participants of
communications, i.e., who is talking to whom in the system. In
our scenario, the two parties involved are obviously known—
the client and the server; however, we strive to prevent trafﬁc
analysis from divulging the content that is being streamed from
the server to the client.
E. Privacy Using Adversarial ML
The possibility that adversarial ML might be leveraged to
improve privacy by interfering with automated classiﬁcation
of observations is a relatively new idea, and one that has
been explored only in the context of image classiﬁcation. Oh
et al. [46] speciﬁcally considered methods to interfere with
automated person recognition in an image. Marohn et al. [38]
similarly explored the effectiveness of an image-obfuscation
technique dubbed “thumbnail preserving encryption” against
ML classiﬁers. To our knowledge, our work is the ﬁrst to
explore adversarial ML as a privacy protection in the domain
of trafﬁc analysis.
X. CONCLUSION
In this paper, we borrowed techniques from adversarial
machine learning and differential privacy to address privacy
concerns of streaming trafﬁc. Our ﬁndings suggest that con-
structing adversarial samples effectively confounds an ad-
versary with a predetermined classiﬁer but is less effective
when the adversary can adapt to the defense, either by using
alternative classiﬁers or training the classiﬁer with adversarial
samples. On the other hand, differential privacy effectively
defeats statistical-inference-based trafﬁc analysis, while re-
mains agnostic to the machine learning classiﬁers used by the
adversary. Our evaluation suggests that the two differentially
private mechanisms used in the paper offer good security
protection with moderate utility loss.
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their valuable
comments. This project is supported in part by NSF grants
1718084, 1750809, 1801494, and grant W911NF-17-1-0370
from the Army Research Ofﬁce. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as representing the ofﬁcial policies,
either expressed or implied, of the Army Research Ofﬁce
or the U.S. Government. The U.S. Government is authorized
to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation herein.
13