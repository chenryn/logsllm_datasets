# Model Descriptions

| Model | Description |
| --- | --- |
| AU | Read all new messages, delete. |
| AD | Read all new messages, delete or mark unread. |
| AN | Read all new messages only, no evasion. |
| IU | Read important messages, mark as unread. |
| ID | Read important messages, delete. |
| IN | Read important messages, delete or mark unread. |
| IU D | Read important messages, delete or mark unread. |
| IU N | Read important messages only, no evasion. |

**Table 2.** Description of the eight models of attacker email reading behavior.

# User Data Sets

| User Type | Email Usage | Days Monitored | Total Messages | Distinct Senders |
| --- | --- | --- | --- | --- |
| Faculty | Work, Personal | 3997 | 930 |  |
| Ph.D. Student | Work | 85 | 202 |  |
| M.S. Student | Work, Sysadmin | 484 | 73 |  |

**Table 3.** Description of the three users' data sets, listing the number of days of data collection, total number of messages received, and the number of distinct email senders.

## 4.3.3 Window Variation (W)

Window variation \( W \) represents the difference between short-term and long-term behavior data. It is defined in terms of the message variation \( M \) of all messages within the short-term behavior window and their corresponding sender confidence \( C \). Specifically, we define \( W \) as:

\[
W = \frac{\sum_i C_i M_i}{\sum_i C_i}
\]

Here, \( i \) ranges over all messages in the short-term window. To detect anomalies, the \( W \) of short-term data is compared to \( \overline{W} \), the average window variation during long-term training. If \( W > p_W \overline{W} \), then an anomaly is signaled. \( p_W \) is a user-specific parameter that by default is set to 2.

## 4.4 Simulated Attack Behaviors

Due to the difficulty of obtaining attack data on specific email accounts, we tested our model using simulated attacker behavior. The choice of attacker simulation method directly affects the interpretation of our results. We defined fourteen types of attacker behavior models based on four attack scenarios. Here, we focus on the eight attack models that were determined to be the most difficult to detect. These models are described in Table 2. When an attacker must choose between deleting or marking a message as unread (models AU D and IU D), each option is chosen with a probability of 0.5.

In the first four models, the hypothetical attacker reads every message. However, in practice, an attacker is likely to be interested in messages from a few correspondents. To account for this, we divide email senders into two categories: important and non-important. Important email senders are those with whom the user has a significant social or work connection. We assume that users are most likely to notice messages from important senders, and these are also the messages most likely to be targeted by an adversary. Important messages are those sent by important senders. These measures are inherently subjective and were determined through discussions with each monitored user.

While real attacker behavior is more complex, an attacker's options are constrained by the need to avoid detection by both the IDS and the targeted user. We discuss this further in Section 5.2.

## 5 Experiments

This section presents the results of experiments used to develop and analyze our model of user email disposition.

### 5.1 Experimental Setup

There are fundamental privacy concerns when monitoring and analyzing email activity. Ideally, analysis should be done automatically by programs that do not store or expose confidential information. In practice, we needed to manually analyze user email behavior to develop our model. To resolve this, we developed and tested our model using a small user population who consented to this type of monitoring and provided feedback on detected anomalies. We monitored the email disposition behavior of three users on the IMAP server running in the Carleton Computer Security Laboratory (CCSL). For these users, we logged three months of IMAP server activity using a modified version of the University of Washington’s IMAP server [24]. The five email dispositions were extracted from this IMAP data, excluding variations in email client IMAP behavior. The collected data sets are outlined in Table 3. Our model was initially developed and tested using data from the faculty user and then further tested on data from the two graduate students. While this user population is not large, the volumes, backgrounds, and purposes of the email received by these individuals are varied, making these results sufficient for an initial evaluation.

### 5.2 Attack Simulations

Before analyzing the feasibility of our model, we studied the eight attack models presented in Section 4.4 to understand which would be the most difficult to detect under realistic attack conditions. We assume that if our system can detect the most evasive simulated attacker, it can also detect the others. We also assume that the attacker will attempt to avoid detection from the targeted user. This means the attacker cannot simply imitate normal user behavior, as this would cause the user to be suspicious. We evaluate the difficulty of detection by comparing the window variation difference \( \Delta W \) caused by the eight attack models and users. In each window, the calculation of \( \Delta W \) is as follows:

\[
\Delta W = W_a - W_u
\]

where \( W_a \) is the window variation caused by the simulated attacker and \( W_u \) is the window variation of the user’s normal behavior. To calculate both values, we use our default model parameters (see Table 4) and the profiled behavior calculated by sliding our long-term window across the second two-thirds of user data (the first third is used to establish \( \overline{W} \)). To calculate \( W_u \), we use user data for the short-term windows; for \( W_a \), the user’s short-term behavior is replaced with attacker operations based on the chosen model.

Figure 1 shows \( \Delta W \) for the eight attack models and each user. From this figure, we can see that the values of \( \Delta W \) are smallest for the attack models AD, AU D, and AN, indicating that these models are the hardest to distinguish from normal user behavior. According to these results, AD and AN are the most challenging to detect.

**Figure 1.** Window variation difference \( \Delta W \) between the eight attack models and users. Each group of bars represents an attack model marked on the X-axis. The Y-axis represents \( \Delta W \). The three bars in each group represent \( \Delta W \) between the given attack model and each user.

**Table 4.** Values defined for each parameter.

| Parameter | Values |
| --- | --- |
| \( p_v \) (all users) | 0, 1 |
| \( p_C \) (all users) | 1, 2, 10, 20 |
| \( p_W \) (all users) | 1.5, 2.0, 2.5 |
| \( p_{sw} \) (Faculty member) | 40 |
| \( p_{lw} \) (Faculty member) | 200, 400, 600, 800 |
| \( p_{sw} \) (Ph.D. student) | 5 |
| \( p_{lw} \) (Ph.D. student) | 25, 50, 75, 100 |
| \( p_{sw} \) (Master’s student) | 30 |
| \( p_{lw} \) (Master’s student) | 150, 300, 450, 600 |

\( lw \) and \( sw \) are the number of messages in the long-term and short-term message windows, respectively. The other parameters are defined in Section 4.3.