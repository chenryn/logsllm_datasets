Read all new msgs., delete.
Model Description
AU
AD
AU D Read all new msgs., delete or mark unread.
AN
IU
ID
IU D
IN
Read all new msgs. only, no evasion.
Read important msgs., mark as unread.
Read important msgs., delete.
Read important msgs., delete or mark unread.
Read important msgs. only, no evasion.
Table 2. Description of the eight models of
attacker email reading behavior.
user
Fac.
Ph.D.
M.S.
email usage
work, personal
work
work, sysadmin
days msgs.
3997
85
484
84
65
2340
senders
930
202
73
Table 3. Description of the three users’ data
sets, listing number of days of data collection,
total number of messages received, and the
number of distinct email senders.
4.3.3 Window variation W
Window variation W represents the difference between
short-term behavior data and long-term behavior data.
It
is de(cid:2)ned in terms of the values of message variation M
of all messages within the short-term behavior window and
their corresponding sender con(cid:2)dence C. Speci(cid:2)cally, we
de(cid:2)ne W as:
W = Pi CiMi
Pi Ci
Here, i ranges over all messages in the short-term window.
To detect anomalies, the W of short-term data is com-
pared to W , the average window variation during long-term
training. If W > pW W , then an anomaly is signalled. pW
is a user-speci(cid:2)c parameter that by default is set to 2.
4.4 Simulated Attack Behaviors
Because it is extremely dif(cid:2)cult to obtain attack data on
speci(cid:2)c email accounts, we have tested our model using
simulated attacker behavior. As there is generally a trade-
off between false positives and true positives in anomaly
detection systems, the choice of attacker simulation method
directly affects the interpretation of our results.
As part of developing our simulation strategy, we de(cid:2)ned
fourteen types of attacker behavior models based upon four
attack scenarios [14]. Here, though, we focus on the eight
attack models that were determined to be the most dif(cid:2)cult
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:33 UTC from IEEE Xplore.  Restrictions apply. 
parameter
pv (all users)
pC (all users)
pW (all users)
psw (Faculty member)
plw (Faculty member)
psw (Ph.D. student)
plw (Ph.D. student)
psw (Master’s student)
plw (Master’s student)
values
0, 1
1, 2, 10, 20
1.5, 2.0, 2.5
40
200, 400, 600, 800
5
25, 50, 75, 100
30
150, 300, 450, 600
Table 4. Values de(cid:2)ned for each parameter.
lw and sw are the number of messages in the
long›term and short term message windows,
respectively. The other parameters are de›
(cid:2)ned in Section 4.3.
behavior. The (cid:2)rst part explains our experimental setup.
The next section describes our empirical evaluation of the
attacker models described in Section 4.4. The (cid:2)nal part
presents our overall evaluation of our model’s feasibility in
terms of true and false positives.
5.1 Experimental Setup
There are fundamental privacy concerns that arise in any
situation where email activity is monitored and analyzed.
Ideally, analysis should be done automatically by programs
that will not store or expose con(cid:2)dential user information.
In practice, though, we needed to analyze manually user
email behavior in order to develop our simple model.
To resolve this dilemma, we developed and tested our
model using a small user population that would give con-
sent to this type of monitoring and who could provide use-
ful feedback on detected anomalies. More speci(cid:2)cally, we
monitored user email disposition behavior of three users on
the IMAP server running in the Carleton Computer Secu-
rity Laboratory (CCSL). For these users, we logged three
months worth of IMAP server activity using a modi(cid:2)ed ver-
sion of the University of Washington’s IMAP server [24].
As mentioned earlier, the chosen (cid:2)ve email dispositions
were extracted from this IMAP data such that variations in
email client IMAP behavior was excluded. The collected
data sets are outlined in Table 3. Our model was initially
developed and tested using data from the faculty user; it
was then further tested on data sets from the two graduate
students. While this user population is not large or com-
prehensive, the volumes, backgrounds, and purposes of the
email received by these three individuals are all extremely
varied, and as such these results appear suf(cid:2)cient for an ini-
tial evaluation of our approach.
Figure 1. Window variation difference (cid:1)W
between the eight attack models and users.
Each group of bars represents an attack
model marked on X axis. Y axis represents
(cid:1)W . The three bars in each group represent
(cid:1)W between the given attack model and each
user.
for users to detect on their own. These models are described
in Table 2. When an attacker must choose between deleting
or marking a message as unread (attack models AU D and
IU D), we choose either option for each message with prob-
ability 0.5.
In the (cid:2)rst four models, the hypothetical attacker is as-
sumed to read every message; in practice, however, it is
likely that an attacker would only be interested in messages
from a few correspondents. To partially account for this
scenario, we divide email senders into two categories, im-
portant and non-important. More speci(cid:2)cally, we de(cid:2)ne im-
portant email senders as those correspondents with whom a
user has a signi(cid:2)cant social or work connection. We assume
that users are most likely to notice messages from important
mail senders; further, in many attack scenarios, these are the
messages that are most likely to be targeted by an adversary.
Important messages are the set of messages that are sent by
important mail senders. Note that these measures are inher-
ently subjective; in our experiments, we determined these
sets after discussions with each monitored user.
While real attacker behavior will generally be much
more complex than that outlined in these models, an at-
tacker’s options are greatly constrained by need to avoid
detection by both the IDS and the targeted user. We dis-
cuss this issue further when we present attack simulation
results in Section 5.2.
5 Experiments
This section presents the results of the experiments used
to develop and analyze our model of user email disposition
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:33 UTC from IEEE Xplore.  Restrictions apply. 
To simplify analysis, we assumed that an equal numbers
of messages were received on each day; thus, we could di-
vide up the data set into a set of equal-sized (cid:147)days(cid:148) for anal-
ysis purposes. The short-term window (psw) was set to be
equal to one day’s worth of messages, and the long-term
window (plw) was set to be 20 days worth of messages.
Anomalies were then detected by comparing the behavior
on one day to the user’s average behavior over the 20 im-
mediately preceding days through the use of the window
variation W value explained in Section 4.3. Note that we
did not attempt to prevent days with unusual behavior from
being included in a user’s long-term behavior data.
We used the (cid:2)rst third of each data set (approximately
one month of data) to determine the value of W , which is
needed to determine the attack detection threshold (see Sec-
tion 4.3.3). The latter two-thirds of the data were then used
to determine true and false positive rates.
5.2 Attack Simulations
Before we analyze the feasibility of our model, we (cid:2)rst
need to study the eight attack models presented in Section
4.4 in order to understand which one would be the most
dif(cid:2)cult for our system to detect under realistic attack con-
ditions. In so doing, we assume that if our system can de-
tect the most evasive simulated attacker, it can also detect
the others. We also assume, though, that the attacker will
attempt to avoid detection from the targeted user as well.
Note that this means that the attacker cannot simply imi-
tate normal user behavior, because this would entail making
changes to the archive (e.g. reading a message and leaving
it marked as read) that would cause a user to be suspicious.
We evaluate the dif(cid:2)culty of detection by comparing the
window variation difference (cid:1)W caused by the eight attack
models and users. In each window, the calculation of (cid:1)W
is as:
(cid:1)W = Wa (cid:0) Wu
where Wa is the window variation caused by the simulated
attacker and Wu is the window variation of a user’s nor-
mal behavior. To calculate both of these values, we use our
default model parameters (see Table 4) and the pro(cid:2)led be-
havior calculated by sliding our long-term window across
second two-thirds of user data (the (cid:2)rst third is used to es-
tablish W ). To calculate Wu, we also use user data for the
short-term windows; for Wa, though, the user’s short-term
behavior is replaced with attacker operations based upon the
chosen model.
Figure 1 shows (cid:1)W for the eight attack models and each
user. From this (cid:2)gure, we can see that the values of (cid:1)W
are smallest for the attack models AD, AU D, and AN;
thus, these attack models are the hardest to distinguish from
normal user behavior. According to these results, AD and
ROC of User_Faculty 
Default Parameter
Optimal Parameter
1
0.8
0.6
0.4
0.2
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
0
0
0.2
0.4
0.6
False Positive Rate
0.8
1
ROC of User_ PH.D. Student
Default Parameters
Optimal Parameters
1
0.8
0.6
0.4
0.2
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
0
0
0.2
0.4
0.6
False Positive Rate
0.8
1
ROC of User_Master Student
Default Parameters
Optimal Parameters
1
0.8
0.6
0.4
0.2
e
t
a
R
e