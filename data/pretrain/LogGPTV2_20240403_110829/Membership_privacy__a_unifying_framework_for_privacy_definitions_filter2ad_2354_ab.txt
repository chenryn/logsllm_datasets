proﬁles of 50 IMDB users, at least 2 of them also appear in the
Netﬂix dataset.
Another privacy incident targeted the Genome-Wide Associa-
tion Studies (GWAS). These studies normally compare the DNA
of two groups of participants: people with the disease (cases) and
similar people without (controls). Each person’s DNA mutations
(single-nucleotide polymorphisms, or SNPs) at indicative locations
are read, and the data is then analyzed. Traditionally, researchers
publish aggregate frequencies of SNPs for participants in the two
groups. In 2008, Homer et al. [17] proposed attacks that could tell
with high conﬁdence whether an individual is in the case group, as-
suming that the individual’s DNA is known. The attack works even
if the group includes hundreds of individuals. Because of the priva-
cy concerns from such attacks, a number of institutions, including
the US National Institute of Health (NIH) and the Wellcome Trust
in London all decided to restrict access to data from GWAS. Such
attacks need access to the victim’s DNA data and publicly avail-
able genomic database to establish the likely SNP frequencies in
the general population.
2.2 Lessons from Privacy Incidents
From these incidents, we learn the following lessons.
Re-identiﬁcation matters.
In the GIC, AOL, and Netﬂix inci-
dents, one is able to correctly identify one individual’s record from
supposedly anonymous data. This fact alone is sufﬁcient for the so-
ciety to agree that privacy is breached. It does not matter whether
an adversary has learnt additional sensitive information about the
individual.
Positive assertion of membership matters. In the GWAS exam-
ple, only aggregate information is published and there is no indi-
vidual record for re-identiﬁcation to occur. However, so long as
the adversary can positively assert that one individual’s data is in
the input dataset based on the output, this is considered a privacy
breach.
Must protect everyone. In several attacks, only a single individ-
ual is re-identiﬁed; however, that is sufﬁcient to cause widespread
privacy concerns and serious consequences (e.g., the AOL case).
This suggests that privacy protection must apply to every individ-
ual. A method that on average offers good protection, but may
compromise some individual’s privacy is not acceptable.
No separation of Quasi-Identiﬁer and Sensitive Attributes. Much
of database privacy research assumes the division of all attributes
into quasi-identiﬁers (QIDs) and sensitive attributes (SA), where
the adversary is assumed to know the QIDs, but not SAs. This
separation, however, is very hard to obtain in practice. No such
separation exists in the cases of AOL, Netﬂix, or GWAS. Even
though only some attributes are used in the GIC incident, it is d-
ifﬁcult to assume that they are the only QIDs. Other attributes in
the GIC data includes visit date, diagnosis, etc. There may well
exist an adversary who knows this information about some individ-
uals, and if with this knowledge these individuals’ record can be
re-identiﬁed, it is still a serious privacy breach. The same difﬁculty
is true for publishing any kind of census, medical, or transactional
data. When publishing anonymized microdata, one has to defend a-
gainst all kinds of adversaries, some know one set of attributes, and
others know a different set. An attribute about one individual may
be known by some adversaries, and unknown (and thus should be
considered sensitive) for other adversaries. In summary, one should
assume that for every individual, there may exist an adversary who
knows all attributes of that individual.
2.3 Positive Membership Disclosure
The discussions above suggest that a privacy breach is a positive
assertion of membership for some entity t all attributes of which
may be known by the adversary. We note that re-identiﬁcation, i.e.,
pointing to some feature of the output dataset and concluding that
this feature reﬂects that t is in the input dataset, implies an assertion
of positive membership. Therefore, any reidentiﬁcation attack will
also result in a positive membership assertion. After all, if one
cannot be sure that an entity t is in the input dataset, one cannot be
sure that a particular feature in the dataset is attributable to t.
Privacy breach = Positive Membership Disclosure:
Given an input dataset T , we say that the output O =
A(T ) is subject to positive membership disclosure,
when there exists a entity t such that an adversary can
assert: the fact that O is output indicates that it is high-
ly likely that t is in the input dataset T .
Note that whether positive membership disclosure occurs or not
is not just a property of the output O = A(T ). In order to say that
O indicates t is in the input dataset, one must consider the behavior
of the algorithm A on datasets that do not include t.
Some authors have considered privacy breaches as attribute dis-
closures, i.e., the ability to infer one’s sensitive attributes from the
data publishing. In [5], it has been shown that while satisfying dif-
ferential privacy, one could still build reasonably accurate classiﬁer
to learn sensitive attribute values of some entity. We argue that
attribute disclosure is problematic as a privacy notion. As shown
by Dwork and others [8], attribute disclosure may occur even if an
entity’s data is not included in the input dataset. For example, sup-
pose that one conducts a study and collects data from individuals
in a particular population group, e.g., of a certain age and ethnicity
group, and these individuals are willing to share their data. Fur-
ther suppose publishing this dataset enables one to learn new at-
tribute information about this group, e.g., they have a much higher
probability of have some disease than the general population. This
new attribute information, however, applies to any individual in this
population group, even the ones that do not contribute data, and en-
ables the learning of new sensitive attribute information about the
individual. We believe that calling such an attribute disclosure a
privacy violation is incorrect. Under this interpretation of privacy,
an individual could claim privacy violation if there is any data about
anyone with some common feature (e.g., is of the same gender) as
the individual.
3. THE MEMBERSHIP PRIVACY FRAME-
WORK
In this section, we introduce our framework for formalizing
membership privacy. More speciﬁcally, we introduce the notion
of Positive Membership Privacy (PMP), which prevents Positive
Membership Disclosure. To enable establishing a clear connection
with differential privacy, we also introduce Negative Membership
Privacy (NMP), which prevents an adversary from increasing sig-
niﬁcantly the conﬁdence that a particular entity’s data does not oc-
cur in the dataset.
3.1 Notations
We assume that there is a universe U of entities, and each dataset
is generated by choosing a set T ⊆ U of entities to be included in
the dataset. That is, there is a deterministic procedure G such that
every dataset is given by G(T ) for some T ⊆ U. Below, we often
abuse the terminology and call T the dataset.
Each entity t ∈ U corresponds to a physical entity that exists in
the physical world and needs privacy protection. For example, in
many scenarios, a physical entity corresponds to an individual, i.e.,
a human being.
When we are working with relational datasets, typically an en-
tity corresponds to one tuple. However, it is possible that in some
scenarios one would need to have an entity corresponding to a set
of tuples. For example, in a database of medical insurance infor-
mation for employees and their family members, it may be neces-
sary to treat an employee’s family as an entity. When we deal with
non-relational datasets such as social network data, one may want
to choose a social network account as an entity so that when the
account is not included, the dataset should not include any infor-
mation related to the account.
The adversary may have prior beliefs about what the dataset is;
this is captured by a distribution D over 2U , the powerset of U.
A distribution D assigns a probability to each possible subset of
U. From the adversary’s point of view, the dataset is a random
variable drawn according to the distribution D. We use T to denote
this random variable.
When publishing a dataset, often one is unable to ﬁx one par-
ticular distribution D. One needs to defend against multiple adver-
saries that may have very different beliefs. Even if one is concerned
with a single adversary, it is often impossible to know exactly what
the adversary believes. Therefore, we need to consider a family
of possible distributions. This is modeled by D, which is a set of
distributions. That is, each D ∈ D is a distribution over 2U . The
family D speciﬁes what kind of background knowledge regarding
the underlying dataset we allow the adversary to have. For perfec-
t privacy, one would desire D to include all possible distributions
over 2U . However, as we show later, achieving such perfect priva-
cy would require not publishing any meaningful information of the
underlying dataset.
3.2 Positive Membership Privacy
While the notion of membership privacy has been alluded to in
several papers, e.g., [19, 20], it has not been formalized to deal with
the possibility of different kinds of background knowledge for the
adversary. We now provide such a formalization.
DEFINITION 3.1. [Positive Membership Privacy
((D, γ)-
PMP)]: We say that a mechanism A provides γ-positive mem-
bership privacy under a family D of distributions over 2U , i.e.,
((D, γ)-PMP), where γ ≥ 1, if and only if for any S ⊆ range(A),
any distribution D ∈ D, and any entity t ∈ U, we have
Pr
D,A
[t ∈ T | A(T) ∈ S] ≤ γ Pr
D
[t ∈ T]
and Pr
D,A
[t 6∈ T | A(T) ∈ S] ≥
PrD[t 6∈ T]
γ
(1)
(2)
where T is a random variable drawn according to the distribution
D.
With a slight abuse of notation, we use S to denote the event
A(T) ∈ S, t to denote the event that t ∈ T and ¬ t to denote the
event that t 6∈ T. Equation (1) can then be written as PrD,A[t|S]
PrD [t] ≤
γ, where PrD[t] denotes the prior belief that entity t is in an input
dataset sampled from the distribution D, and PrD,A[t|S] denotes
the posterior belief that t is in the input dataset after observing that
the event S has happened for the output of A. When D and A are
obvious from the context, we drop them from the subscript and
write Pr[t] and Pr[t|S]. Equation (1) requires that Pr[t|S] can
increase at most by a factor of γ over the prior belief Pr[t].
Equation (1) by itself, however, may not offer sufﬁcient protec-
tion when the prior belief Pr[t] is already quite large. For example,
setting γ = 1.2 might seem a reasonable strong privacy protection.
However, if Pr[t] = 0.85, then Equation (1) will bound the poste-
rior belief Pr[t|S] to be less than 0.85 ∗ 1.2 = 1.02. This allows
an adversary to be 100% certain that t is in the input dataset after
observing the output, which is arguably undesirable.
Equation (2), which can be written as Pr[¬ t|S]
γ , provides
a more effective upper-bound for the posterior belief Pr[t|S] when
the prior is large. In the above example, Pr[¬ t|S] is lower-bounded
by (1 − 0.85)/1.2 = 0.125, i.e., Pr[t|S] can increase from 0.85 to
at most 0.875. Equations (1) and (2) together are equivalent to:
Pr[¬ t] ≥ 1
Pr[t|S] ≤ min(cid:18)γ Pr[t], 1 −
Pr[¬ t]
γ (cid:19)
For example, suppose γ = 2, then PMP requires that Pr[t|S] ≤
γ
γ − 1 + Pr[t]
= min(cid:18)γ Pr[t],
(cid:17). In the rest of the paper, we often use E-
(cid:19) .
(3)
min(cid:16)2 Pr[t], 1+Pr[t]
2
quation (3) as the formulation of the PMP condition.
3.3 Satisfying PMP
In the rest of this section, we establish a relationship between
PMP’s bounding of Pr[t|S] in Equations (1) and (2) to a condition
bounding Pr[S| t]
Pr[S|¬ t] , which is more directly controlled by choosing
an appropriate mechanism. This is also key to establish the rela-
tionship of PMP with differential privacy. To start, note that by the
Bayes’ theorem, we have
Pr[t|S] =
Pr[S| t] Pr[t]
Pr[S| t] Pr[t] + Pr[S|¬t] Pr[¬ t]
.
(4)
LEMMA 3.2. For any mechanism A, distribution D, and γ ≥
1, if for any entity t s.t. 0 < Pr[t] < 1 and any output event S, A
satisﬁes Pr[S|t] ≤ γ · Pr[S|¬ t], then A provides ({D}, γ)-PMP.
PROOF. For any t, if Pr[t] = 0 or Pr[t] = 1, then for any S,
Pr[t|S] = Pr[t], trivially satisfying Equation (3). If 0 < Pr[t] <
1, for any S, from Equation (4), we have
Pr[t|S] = Pr[t] ·
Pr[S|t]
Pr[t] Pr[S|t]+(1−Pr[t]) Pr[S|¬ t]
Note that the denumerator above is a weighted average of Pr[S|t]
and Pr[S|¬ t]. Since Pr[S|t] ≤ γ Pr[S|t] because γ ≥ 1, and
Pr[S|t] ≤ γ Pr[S|¬ t] from the given condition, it must be that
Pr[S|t] ≤ γ(Pr[t] Pr[S|t] + (1 − Pr[t]) Pr[S|¬ t]).
And thus Pr[t|S] ≤ γ Pr[t]. Similarly, we have
Pr[¬ t|S] = Pr[¬ t]
Pr[S|t] Pr[t]+Pr[S|¬ t](1−Pr[t]) ≥ Pr[¬ t] 1
Pr[S|¬ t]
γ
The last step above is because Pr[S|¬ t] ≥ Pr[S|t]
condition) and Pr[S|¬ t] ≥ Pr[S|¬ t]
γ
(from γ ≥ 1).
γ
(from given
Note that the above relationship is one-directional. Bounding
Pr[S|t]
Pr[S|¬ t] ≤ γ under a distribution D is sufﬁcient but not neces-
sary for satisfying PMP. Below we show that an equivalence exists
between the two when one aims at satisfying PMP for all distri-
butions in a family that has the property that we call downward
scalable, which essentially means that within the family, one could
scale down the probability Pr[t] to an arbitrarily small value. We
ﬁrst need to deﬁne what we call a t-scaled distribution.
DEFINITION 3.3
(t-SCALED DISTRIBUTION). Given a dis-
tribution D, we say that D′ is t-scaled from D if there exist two
constants c1 and c2 such that
∀T ⊆ U Pr
D′
[T ] =(cid:26) c1 PrD[T ] when t ∈ T ,
c2 PrD[T ] when t 6∈ T
The following property is the reason why we introduce the notion
of t-scaled distribution.
LEMMA 3.4. If D′ is t-scaled from D, then for any mechanism
A and any output event S, we have PrD′,A[S| t] = PrD,A[S| t]
and PrD′,A[S|¬ t] = PrD,A[S|¬ t].
PROOF. Let c1 and c2 be the scaling parameter. We have
PrD′,A[S| t]
= PT :t∈T PrD′ [T ] PrA[S| T ]
= PT :t∈T c1 PrD [T ] PrA[S| T ]
PrD′,A[S|¬ t] = PT :t6∈T PrD′ [T ] PrA[S| T ]
= PT :t6∈T c2 PrD [T ] PrA[S| T ]
PT :t∈T PrD′ [T ]
PT :t∈T c1 PrD [T ]
PT :t∈T PrD′ [T ]
PT :t6∈T c2 PrD [T ]