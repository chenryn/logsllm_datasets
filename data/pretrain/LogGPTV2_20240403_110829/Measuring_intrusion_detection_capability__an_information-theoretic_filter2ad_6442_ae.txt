l
i
b
a
p
a
C
D
I
0.1
0.05
0
0
0.04
0.03
0.02
0.01
0
0
600
400
200
l
d
o
h
s
e
r
h
T
0.5
0.5
1
1.2
1.4
x 10−4
0
0
0.5
1
1
1
1.5
False positive rate (α)
Max CID= 0.033448
1.5
False positive rate (α)
Threshold=64
1.5
False positive rate (α)
(b) PAYL
2
2
2
2.5
3
x 10−3
2.5
3
x 10−3
2.5
3
x 10−3
Figure 8: Experiment results. (a) A low false positive and high false positive rate in the PHAD test means
CID is no better (and no worse) than ROC. (b) CID identiﬁes an optimal threshold in PAYL. A simple ROC
analysis would fail to ﬁnd this point because of an increasing detection rate.
threshold drops, CID reaches a peak and then drops, while
the ROC curves (shown in the top graph) continue to in-
crease slowly.
An analyst using just the top graph in Figure 8(b) might
be tempted to set a threshold lower than, say, 8 (where
α = 3×10−3), because the detection capability still increases
even if the false positive rate grows slightly as well. However,
using CID, we see the detection capability actually declines
after CID = 0.033448 (marked in Figure 8(b) with a vertical
line). Thus, CID identiﬁes a higher, but optimal operating
threshold of 64 (where α = 0.7 × 10−3, 1 − β = 0.10563). In
this situation, CID provides a better operating point.
It
is not obvious how ROC analysis could provide the same
optimal threshold.
To demonstrate how CID can be used to compare diﬀer-
ent IDS, we ran Snort (Version 2.1.0 Build 9) on the same
data as PAYL to compare their capabilities. Since the use
of libwhisker which tried to evade snort, we have a poor de-
tection rate with 1− β = 0.0117 (worse than PAYL), a good
false positive rate α = 0.0000006701 (better than PAYL).
Without CID, we cannot tell which IDS is better based on
existing metrics. With the base rate B = 0.000010191, we
can calculate CID = 0.0081 for Snort in this testing data.
Clearly 0.033448 > 0.0081, so (optimally conﬁgured) PAYL
performs better than Snort based on our test data.
Again, we emphasize that as with all evaluation attempts,
the above results are strongly related to the testing data in
use.
5. DISCUSSION
5.1 Trace-driven Evaluation and Ground Truth
Currently, all IDS evaluation work is trace-driven, sug-
gesting that when evaluating IDSs, we should have the eval-
uation data set where we know the details about the ground
truth, i.e., what data are attacks and what data are normal
traﬃc. Thus, we can easily ﬁnd out the base rate, which is
the fraction of attacks in the whole data set. After testing
the IDS on this evaluation data set, we compare the IDS
alerts with the ground truth, then we can calculate the false
positive rate (the fraction of misclassiﬁed normal data in the
whole normal data) and false negative rate (the fraction of
undetected attacks among all the attack data). Using these
basic metrics as inputs, we can ﬁnally compute CID. In our
technical report [8], we also brieﬂy discuss the estimation
of prior probabilities and transition probabilities in the real
situation.
5.2 Unit of Analysis
An important problem in IDS evaluation is “unit of anal-
ysis” [15]. As we mentioned when introducing the abstract
IDS model, for network based intrusion detection, there are
at least two units of analysis in diﬀerent IDSs. Some IDSs
(e.g., Snort, PAYL [25]) analyze packets and output alerts
on the packets, while other IDSs such as Bro analyze traﬃc
based on ﬂows.
Although a diﬀerent unit of analysis will result in a diﬀer-
ent base rate even on the same evaluation data set, it does
not aﬀect the usage of CID in ﬁne-tuning an IDS to get op-
timal operation point. However, when comparing diﬀerent
IDSs, we must consider this problem. In this paper, we are
not trying to solve the “unit of analysis” problem, because
it is not peculiar to CID, but a general problem for all the
existing evaluation metrics, e.g., T P, F P . Thus, in order to
provide a fair and meaningful comparison, we recommend
running the IDSs based on the same unit of analysis as well
as the same data set and the same detection spaces (or at-
tack coverages).
Regardless of the metrics being used, the “unit of analy-
sis” problem is a general yet diﬃcult problem in IDS evalu-
ation. In some cases, we can also convert the diﬀerent units
to the same one when comparing diﬀerent IDSs. For ex-
ample, we can convert a packet-level analysis to a ﬂow-level
analysis by deﬁning a ﬂow as malicious when it contains any
malicious packet; otherwise, it is a normal ﬂow. Using such
a conversion allows the comparison between a packet-level
IDS and a ﬂow-level IDS based on the same (“virtual”) gran-
ularity or unit of analysis. However, this kind of conversion

does not always work, particularly when the two units, such
as packet sequence and system call sequence, are totally un-
related.
5.3 Involving Cost Analysis in CID
We have shown that CID is a very natural and objective
metric for measuring the intrusion detection capability and
claim it is a good complement to the cost-based approach.
In some cases, however, we may want to include a subjective
cost analysis, particularyly when a good risk analysis model
is available. We notice that CID has some connection to
the cost-based metric if the log part can be considered the
cost function. In addition, we can easily involve cost analy-
sis in CID as an extension. A possible solution is achieved
by using a weighted conditional entropy H(X|Y ) when cal-
culating CID = (H(X) − H(X|Y ))/H(X). We can change
the original form of conditional entropy slightly and place
weights in. Now
Hw(X|Y ) = −xy wxyp(x, y) log p(x|y)
,
xy wxy
where wxy means the weight/cost considered when X =
x, Y = y. We can set a larger weight of wxy when we believe
the situation X = x, Y = y costs more. For instance, in the
military network example, we can deﬁne a very large weight
on w10, which essentially gives more weight to missed attacks
(X = 1 while Y = 0), i.e., false negatives. In this weighted
setting, CID will give more preference to F N than F P . Sim-
ilarly, we can set a larger weight of w01 in the case with a
single overloaded operator (or with an automated response
system), which indicates a false positive (X = 0, Y = 1) is
more important in the analysis. In such a cost-based exten-
sion, CID can achieve a similar capability as ROC combining
cost analysis. Further study of cost-based extensions on CID
will be in our future work.
6. RELATED WORK
Intrusion detection has been a ﬁeld of active research for
more than two decades, and many IDSs have been devel-
oped. There are several relevant fundamental (theoreti-
cal) research in this ﬁeld. Denning [5] introduced an intru-
sion detection model and proposed several statistical models
to build normal proﬁles. Helman and Liepins [10] studied
some statistical foundation of audit trail analysis for the
detection of computer misues. They modeled the normal
traﬃc and attack traﬃc as the output of two independent
stationary stochastic processes. Axelsson [2] argued that
the well-established signal detection and estimation theory
bears similarities with the IDS domain. However, the ben-
eﬁts of the similarities for the design and evaluation of IDS
in practice are as yet unclear. Maxion et al. [14] studied
the relationship between data regularity and anomaly de-
tection performance. The study focused on sequence data,
and hence, regularity was deﬁned as conditional entropy.
The key result from experiments on synthetic data was that
when an anomaly detection model was tested on data sets
with varying regularity values, the detection performance
also varied. Lee et al. [11] applied information theoretic
measurement to describe the characteristics of audit data
set, suggest the appropriate anomaly detection model, and
explain the performance of the models. Our work is another
application of information theory to IDS and provides a nat-
00
ural and uniﬁed metric of the intrusion detection capability.
In the area of IDS evaluation, true positive rate and false
positive rate are two commonly used metrics. To consider
both of these metrics, we can use ROC (receiver operat-
ing characteristic) curve [9] based analysis, which has al-
ready been well studied in other ﬁelds such as medical di-
agnostic tests [24]. Lippmann et al. [12] evaluated IDSs on
the 1998 DARPA Intrusion Detection Evaluation Data Set
and used ROC curves to evaluate (and implicitly compare)
them. McHugh [15] pointed out that the evaluation in [12]
had serious shortcomings. For example, the proper unit of
analysis and measurement was diﬀerent for diﬀerent detec-
tors. McHugh also called for a more helpful measure of IDS
performance. Our work is an attempt to develop a better
metric. Gaﬀney and Ulvila [6] combined ROC curves with
cost analysis methods to compute the expected cost of an
IDS so that diﬀerent IDSs can be evaluated and compared
based on their expected costs. This approach is not practi-
cal because the result depends on the subjective estimate of
the cost ratio between true and false positives.
Axelsson [1] proposed two other metrics: the Bayesian
detection rate and the Bayesian negative rate. These are
in fact the Bayesian representations of positive predictive
value (P P V ) and negative predictive value (N P V ), both
commonly used in medical diagnosis [24]. Axelsson’s main
conclusion is that given that the base rate is very low in most
environments, the false alarm rate needs to be a lot lower
than what most current algorithms can achieve in order to
have a reasonable Bayesian detection rate.
The existing metrics are all useful. However, the lack
of a uniﬁed metric makes it hard to ﬁne-tune and evaluate
an IDS. Our new metric, Intrusion Detection Capability, de-
rived from analyzing the intrusion detection process from an
information-theoretic point of view, naturally uniﬁes all the
existing objective measures of the IDS detection capability.
The IBM Zurich team of RIDAX [4] (developed in the con-
text of the European MAFTIA project) proposed a set of
metrics such as precision, recall, as used in the information-
retrieval ﬁeld. Their approach is very diﬀerent from CID
because they focus on assessing the completeness and util-
ity of arbitrary IDS combinations, while we try to capture
the intrinsic capability of IDS using an information-theoretic
approach.
Our new metric is similar to but diﬀerent from NMI (Nor-
malized Mutual Information, (H(A)+H(B))/H(A, B)) used
in medical image registration [18] and NAMI (Normalized
Asymmetric Mutual Information, I(X; Y )/H(Y )) [23]. These
other metrics are not as sensitive as CID for realistic intru-
sion detection scenarios, as discussed in Section 2.
7. CONCLUSION AND FUTURE WORK
The contributions of this paper are both theoretical and
practical. We provided an in-depth analysis of existing IDS
metrics. And we argued that the lack of a uniﬁed metric
makes it hard to ﬁne-tune an IDS and compare diﬀerent
IDSs. Then, we studied the intrusion detection process from
the viewpoint of information theory and proposed a natural,
uniﬁed metric to measure the capability of the IDS in terms
of its capability to correctly classify the input events. Our
metric, Intrusion Detection Capability, or CID, is simply the
ratio of the mutual information between the IDS input and
output to the entropy of the IDS input. This intuitive metric
combines all commonly used metrics, i.e., true positive rate,
false positive rate, and both positive and negative predictive
values. It also factors in the base rate, an important measure
of the IDS operation environment. As a composite (uniﬁed)
metric, CID greatly complements the cost-based approach.
Using this metric, one can choose the best (optimized)
operation point of an IDS (e.g., the threshold for an anomaly
detection system). Furthermore, since CID is normalized,
we can compare diﬀerent IDSs, even though their F P , F N
rates are diﬀerent. We presented numerical experiments and
case studies to show the utility.
This paper has not presented every application for In-
trusion Detection Capability, and numerous extensions are
possible.
An obvious extension of CID comes from rethinking the
simple model of IDS inputs and outputs, X, Y , represented
as a 1 or a 0. We can instead encode diﬀerent types of
attacks into X, Y , creating a more accurate model, par-
ticularly in the context of signature-based IDS. Aware of
this more accurate model, we will use CID to measure more
signature-based detection systems.
Our abstract model for the intrusion detection process can
be further studied using channel capacity models from in-
formation theory. Multiple processes (or layers) of IDS can
be considered as multiple (chained) channels. We can ana-
lyze and improve both internal and external designs of IDS
instead of only considering the intrusion detection process
as an entire black box.
8. ACKNOWLEDGMENTS
This work is supported in part by NSF grant CCR-0133629
and Army Research Oﬃce contract W911NF0510139. The
contents of this work are solely the responsibility of the au-
thors and do not necessarily represent the oﬃcial views of
NSF and the U.S. Army.
9. REFERENCES
[1] S. Axelsson. The base-rate fallacy and its implications
for the diﬃculty of intrusion detection. In Proceedings
of ACM CCS’1999, November 1999.
[8] G. Gu, P. Fogla, D. Dagon, W. Lee, and B. Skoric. An
information-theoretic measure of intrusion detection
capability. Technical Report GIT-CC-05-10, College of
Computing, Georgia Tech, 2005.
[9] J. Hancock and P. Wintz. Signal Detection Theory.
McGraw-Hill, 1966.
[10] P. Helman and G. Liepins. Statistical foundations of
audit trail analysis for the detection of computer
misuse. IEEE Transactions on Software Engineering,
19(9), September 1993.
[11] W. Lee and D. Xiang. Information-theoretic measures
for anomaly detection. In Proceedings of the 2001
IEEE Symposium on Security and Privacy, May 2001.
[12] R. P. Lippmann, D. J. Fried, and I. G. etc. Evaluating
intrusion detection systems: The 1998 darpa oﬀ-line
intrusion detection evaluation. In Proceedings of the
2000 DARPA Information Survivability Conference
and Exposition (DISCEX’00), 2000.
[13] M. V. Mahoney and P. K. Chan. Phad: Packet header
anomaly detection for indentifying hostile network
traﬃc. Technical Report CS-2001-4, Florida Tech,
2001.
[14] R. Maxion and K. M. C. Tan. Benchmarking
anomaly-based detection systems. In Proceedings of
DSN’2000, 2000.
[15] J. McHugh. Testing intrusion detection systems: A
critique of the 1998 and 1999 darpa oﬀ-line intrusion
detection system evaluation as performed by lincoln
laboratory. ACM Transactions on Information and
System Security, 3(4), November 2000.
[16] MIT Lincoln Laboratory. 1999 darpa intrusion
detection evaluation data set overview.
http://www.ll.mit.edu/IST/ideval/, 2001.
[17] V. Paxson. Bro: A system for detecting network
intruders in real-time. Computer Networks,
31(23-24):2435–2463, December 1999.
[18] J. Pluim, J. Maintz, and M. Viergever. Mutual
information based registration of medical images: A
survey. IEEE Trans on Medical Imaging,
22(8):986–1004, Aug 2003.
[2] S. Axelsson. A preliminary attempt to apply detection
[19] T. H. Ptacek and T. N. Newsham. Insertion, evasion,
and estimation theory to intrusion detection.
Technical Report 00-4, Dept. of Computer
Engineering, Chalmers Univerity of Technology,
Sweden, March 2000.
and denial of service: Eluding network intrusion
detection. Technical report, Secure Networks Inc.,
January 1998. http://www.aciri.org/vern/Ptacek-
Newsham-Evasion-98.ps.
[3] T. Cover and J. Thomas. Elements of Information
[20] N. J. Puketza, K. Zhang, M. Chung, B. Mukherjee,
Theory. John Wiley, 1991.
[4] M. Dacier. Design of an intrusion-tolerant intrusion
detection system, Maftia Project, deliverable 10.
Available at
http://www.maftia.org/deliverables/D10.pdf. 2005.
[5] D. Denning. An intrusion-detection model. IEEE
Transactions on Software Engineering, 13(2), Feb
1987.
[6] J. E. Gaﬀney and J. W. Ulvila. Evaluation of
intrusion detectors: A decision theory approach. In
Proceedings of the 2001 IEEE Symposium on Security
and Privacy, May 2001.
[7] I. Graf, R. Lippmann, R. Cunningham, K. K.
D. Fried, S. Webster, and M. Zissman. Results of
DARPA 1998 oﬀ-line intrusion detection evaluation.
Presented at DARPA PI Meeting, 15 December 1998.
and R. A. Olsson. A methodology for testing intrusion
detection systems. IEEE Transactions on Software
Engineering, 22(10):719–729, 1996.
[21] R. F. Puppy. Libwhisker oﬃcial release v2.1, 2004.
Available at http://www.wiretrip.net/rfp/lw.asp.
[22] M. Roesch. Snort - lightweight intrusion detection for
networks. In Proceedings of USENIX LISA’99, 1999.
[23] A. Strehl. Relationship-based clustering and cluster
ensembles for high-dimensional data mining, May
2002. PhD thesis, The University of Texas at Austin.
[24] J. A. Swets. Measuring the accuracy of diagnostic
systems. Science, 240(4857):1285–1293, 1988.
[25] K. Wang and S. J. Stolfo. Anomalous payload-based
network intrusion detection. In Proceedings of
RAID’2004, September 2004.
0