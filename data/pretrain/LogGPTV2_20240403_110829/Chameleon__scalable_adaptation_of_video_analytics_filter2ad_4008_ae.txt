linearly with the number of cameras. Indeed, Figure 13a cor-
roborates this intuition. We take ten video feeds4 covering
the same day-time hour, and incrementally add one camera
to the test at a time. Chameleon automatically groups these
cameras into one group (using the logic described in §5.3),
and achieves a linear reduction in cost with only small re-
duction in accuracy. In contrast, during the hours when the
cameras are less similar, we expect to see less reduction of
profiling cost, since only a subset of cameras can share the
profiling cost. This is exactly what happened in Figure 13b:
Chameleon groups the cameras into two (sometimes three)
groups, so even though there are 10 video feeds, the saving
in profiling cost is lower than that in Figure 13a.
As we group more cameras, notice that the accuracy drops
(albeit by less than 10%). This is because cameras in the same
group might have different characteristics, so sharing the
same configuration among them leads to lower accuracy than
customizing the configuration for each one. The drop in ac-
curacy is also partly due to our very simple camera-grouping
algorithm (§5.3), which bounds the maximum discrepancy
between the accuracy of a randomly chosen configuration
on different video feeds in the same group. It is quite possible
that a more sophisticated algorithm for grouping cameras
would maintain higher inference accuracy.
6.5 Impact of reduced configuration space
The last key technique in Chameleon is to reduce the cost of
a single profiling of the configuration space, by profiling each
4We created 10 video feeds from 5 cameras by horizontally splitting the
view of each camera into two non-overlapping video feeds.
(a) Accuracy
(b) Inference cost
(c) Profiling cost
Figure 14: Comparing Chameleon’s profiling algorithm (Al-
gorithm 3) with the result of an exhaustive search.
knob separately. The reduction in profiling cost is obvious,
but the impact on accuracy and inference cost of the selected
configuration is less evident. In Figure 14, we compare the
configurations found by Algorithm 3 (based on the assump-
tion that knobs are independent) to an exhaustive search,
along three metrics: accuracy (Figure 14a), inference cost of
the configurations picked by each algorithm (Figure 14b), and
the profiling cost of executing the algorithms (Figure 14c).
We can see that the configurations picked by Algorithm 3
are almost as good (in accuracy and inference cost) as the
result of running an exhaustive search, while achieving an
enormous reduction in profiling cost. Note that the “exhaus-
tive” method is not optimal, because it only profiles the first
second of each segment (not every second).
6.6 Contribution of each component
Finally, we investigate the contribution of individual tech-
niques in Chameleon, by incrementally adding one technique
at a time (temporal incremental update, spatial cross-camera
inference, and leveraging knob independence). Figure 15
shows the performance of the full Chameleon solution as
well as some intermediate design points for the two pipelines
we studied. In both pipelines, we see that each step brings
significant reduction in cost at a relatively small drop in
accuracy. Temporal incremental updates reduces profiling
cost by about ∼50%, cross-camera inference by an additional
∼30-60%, and knob independence by another 40-60%.
7 RELATED WORK
Video processing optimization: Several previous papers
have considered optimizing video processing pipelines by
either adjusting the configuration knobs or training special-
ized NN models. VideoStorm [32] first profiles each video
query running in a cluster and then adjusts its configuration
to achieve the right balance between accuracy, processing
delay, and resource demand. NoScope [24], MCDNN [16],
and Focus [22] all process streaming or offline video using
various NNs to detect objects, and recognize people and text.
One of the core techniques in all three papers is training
specialized NNs based on objects that typically appear in
a specific video stream. For example, instead of a NN that
can classify across 1000 objects, they train a much smaller
(and more efficient) one for the top 20 objects. While each of
 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78Avg GPU time per frame (sec)Frac. of frames with accurate resultInference + Profiling CostInference Cost#	of	cameras	=	1#	of	cameras	=	10 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.62 0.64 0.66 0.68 0.7Avg GPU time per frame (sec)Frac. of frames with accurate resultInference + Profiling CostInference Cost#	of	cameras	=	1#	of	cameras	=	10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1ExhaustiveChameleonAccuracy 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14ExhaustiveChameleonResource consumption 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45ExhaustiveChameleonProfiling costChameleon: Scalable Adaptation of Video Analytics
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
(a) Pipeline A
(b) Pipeline B
Figure 15: Contribution of individual components
these papers reports significant improvements in accuracy
and/or resource consumption, they all profile and optimize
the video queries only once at the beginning of the video
stream. They do not report how the optimal profiles change
over time and do not handle changes in video stream con-
tent. An exception is [29] which retrains the NN model to
detect the set of popular objects as it changes over time. Two
core contributions of Chameleon are demonstrating that op-
timal configurations do change over time, and providing an
efficient technique for continuously adapting the profiles.
Finding optimal configurations: Chameleon periodically
searches an exponentially large configuration space to find
the optimal NN configuration for a video query. This is done,
at a minimum, for the leader of each spatially-related group
of videos. Several recent systems have also faced an expo-
nentially large configuration search space in their problem
domains [12, 19, 31, 34]. Ernest [31] uses optimal experimen-
tal design [28] to optimize the VM configuration of a job,
while Cherrypick [12] uses Bayesian optimization [27] to
find an optimal cloud configuration for general applications.
Hill et. al [19] use Thompson sampling [11] to optimize the
layout of a multi-component web page; they use greedy hill
climbing to select the next layout, similar to Chameleon,
but Chameleon exploits more independence structure and
monotonicity. All of these works bound the cost of their
configuration search (e.g., by adding it as a constraint in the
optimization problem), but these are still one-time or daily
costs paid for the modeling task at hand. Some bandit algo-
rithms address non-stationary settings (e.g., [26]), but these
are too inefficient at present.
Chameleon differs from these systems in two major ways.
First, the optimal configuration for a video is non-stationary,
requiring frequent (every few seconds) re-profiling that must
keep up with a real-time video feed. This puts tremendous
pressure on keeping the profiling cost low. Second, Chameleon
reuses optimal configurations across related video feeds.
These differences lead to our greedy hill climbing approach,
which avoids any computationally-expensive modeling.
8 DISCUSSION AND FUTURE WORK
Network bandwidth: Besides computational cost, which is
the focus of this paper, network bandwidth is also an impor-
tant resource in video analytics. With the increasing trend
of running analytics across smart cameras [1, 5, 13] and the
cloud, the network will start becoming a scarce resource in
video analytics systems. The choice of configuration of a
video analytics pipeline has implications on the network us-
age too; e.g., one can save bandwidth by streaming the video
at a low frame rate or resolution if it still maintains high
inference accuracy. While Chameleon optimizes the com-
putational cost of video analytics, its techniques addressing
dynamic variations in the profile using spatial and temporal
cross-camera correlations will likely carry over when con-
sidering the network. The problem, however, will be one
of multiple resources and hence will require techniques for
joint consideration of these resources.
Profiling on the edge: Chameleon’s design relied on a sep-
arate cluster for its periodic profiling so as to avoid any
disruptions to the live video analytics pipeline. As we move
to the scenario of edge camera analytics [13], such a sep-
aration would be hard to achieve. The camera’s compute
resource, already limited, would be insufficient if it also has
to accommodate the demand for periodic and quick profiling.
At the same time, periodically recording and shipping video
clips to the cloud for profiling may also overload the network.
Arriving at the right design choice for edge camera analytics
will be an important problem going forward.
Triggering the profiling: An unstudied aspect of resource-
accuracy profiles in our paper is the periodicity with which
their values change. While our solution relied on setting
a pre-fixed time interval for profiling (and then saving its
cost), we can save on profiling costs even further if we can
predict the need to re-profile. Conceivably, we could use
vision techniques like scene understanding to trigger fresh
profiling. We believe this requires work at the intersection
of systems, machine learning, and computer vision.
9 CONCLUSION
In this paper, we argue that video processing pipelines have
to be adapted over time, otherwise they might achieve very
low levels of accuracy. However, a naive re-profiling is pro-
hibitively expensive. Instead, we present Chameleon, a sys-
tem that uses several techniques to dramatically reduce pro-
filing cost and also improves accuracy.
ACKNOWLEDGEMENTS
We appreciate the feedback by the anonymous SIGCOMM
reviewers and our wonderful shepherd, Romit Roy Choud-
hury. Ion Stoica is supported by the NSF CISE Expeditions
Award CCF-1730628, and in part by DHS Award HSHQDC-
16-3-00083, and gifts from Alibaba, Amazon Web Services,
Ant Financial, Arm, CapitalOne, Ericsson, Facebook, Google,
Huawei, Intel, Microsoft, Scotiabank, Splunk and VMware.
 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 0.5 0.6 0.7 0.8 0.9 1Avg GPU time per frame (sec)Frac. of frames with accurate resultPeriodic Exhaustive SearchChameleon (Temperal Only)Chameleon (Spatial+Temporal)Chameleon (Full) 0 1 2 3 4 5 6 7 0.5 0.6 0.7 0.8 0.9 1Avg GPU time per frame (sec)Frac. of frames with accurate resultPeriodic Exhaustive SearchChamelon (Temperal Only)Chamelon (Spatial+Temporal)Chamelon (Full)SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
J. Jiang et al.
Endow., 10(11):1586–1597, Aug. 2017.
[25] F. Loewenherz, V. Bahl, and Y. Wang. Video analytics towards vision
zero. In ITE Journal, 2017.
[26] H. Luo, A. Agarwal, and J. Langford. Efficient contextual bandits in
non-stationary worlds. CoRR, abs/1708.01799, 2017.
[27] J. Mockus. Bayesian approach to global optimization. Kluwer, Dordrecht,
[28] F. Pukelsheim. Optimal Design of Experiments. John Wiley & Sons Inc.,
1989.
New York, 1993.
[29] H. Shen, S. Han, M. Philipose, and A. Krishnamurthy. Fast video
classification via adaptive cascading of deep models. In Proceedings of
the IEEE conference on computer vision and pattern recognition, 2017.
Inception-v4,
inception-resnet and the impact of residual connections on learning.
In AAAI, volume 4, page 12, 2017.
[30] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
[31] S. Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica. Ernest:
Efficient performance prediction for large-scale advanced analytics.
In 13th USENIX Symposium on Networked Systems Design and Imple-
mentation (NSDI ’16), pages 363–378, 2016.
[32] H. Zhang, G. Ananthanarayanan, P. Bodik, M. Philipose, P. Bahl, and
M. J. Freedman. Live video analytics at scale with approximation and
delay-tolerance. In NSDI, volume 9, page 1, 2017.
[33] T. Zhang, A. Chowdhery, P. V. Bahl, K. Jamieson, and S. Banerjee. The
design and implementation of a wireless video surveillance system.
In Proceedings of the 21st Annual International Conference on Mobile
Computing and Networking, pages 426–438. ACM, 2015.
[34] Y. Zhu, J. Liu, M. Guo, Y. Bao, W. Ma, Z. Liu, K. Song, and Y. Yang.
Bestconfig: tapping the performance potential of systems via automatic
configuration tuning. In Proceedings of the 2017 Symposium on Cloud
Computing, pages 338–350. ACM, 2017.
REFERENCES
[1] Amazon aws deeplens. https://aws.amazon.com/deeplens/.
[2] Artificial
Cameras
Intelligence
rity.
artificial-intelligence-surveillance-cameras-security.
Secu-
https://www.theverge.com/2018/1/23/16907238/
Surveillance
[3] AWS Lambda. https://aws.amazon.com/lambda/.
[4] Azure Functions.
https://azure.microsoft.com/en-us/services/
[5] Google clips. https://store.google.com/us/product/google_clips?hl=
[6] New Search Engine Revolutionizes Video Surveillance. https://i-hls.
functions/.
en-US.
com/archives/80734.
[7] Tensorflow detection model zoo.
https://github.com/tensorflow/
models/blob/master/research/object_detection/g3doc/detection_
model_zoo.md.
[8] Tensorflow-slim image classification model library. https://github.
com/tensorflow/models/tree/master/research/slim.
[9] Yolo. https://pjreddie.com/darknet/yolo/.
[10] FFmpeg. http://ffmpeg.org/, 2000–2018.
[11] S. Agrawal and N. Goyal. Analysis of thompson sampling for the
multi-armed bandit problem. In 25th Conference on Learning Theory
(COLT ’12), pages 39.1–39.26, 2012.
[12] O. Alipourfard, H. H. Liu, J. Chen, S. Venkataraman, M. Yu, and
M. Zhang. Cherrypick: Adaptively unearthing the best cloud configu-
rations for big data analytics. In 14th USENIX Symposium on Networked
Systems Design and Implementation (NSDI ’17), pages 469–482, 2017.
[13] G. Ananthanarayanan, V. Bahl, P. Bodik, K. Chintalapudi, M. Philipose,
and L. Ravindranath. Real-time video analytics - the killer app for
edge computing. In IEEE Computer, 2017.
[14] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman.
The pascal visual object classes (voc) challenge. International journal
of computer vision, 88(2):303–338, 2010.
[15] R. Girshick. Fast r-cnn. arXiv preprint arXiv:1504.08083, 2015.
[16] S. Han, H. Shen, M. Philipose, S. Agarwal, A. Wolman, and A. Krish-
namurthy. Mcdnn: An approximation-based execution framework
for deep stream processing under resource constraints. In Proceed-
ings of the 14th Annual International Conference on Mobile Systems,
Applications, and Services, pages 123–136. ACM, 2016.
[17] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In IEEE
International Conference on Computer Vision (ICCV), 2017, pages 2980–
2988. IEEE, 2017.
[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 770–778, 2016.
[19] D. N. Hill, H. Nassif, Y. Liu, A. Iyer, and S. Vishwanathan. An efficient
bandit algorithm for realtime multivariate optimization. In 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD ’17), pages 1813–1821, 2017.
[20] W. E. Hoover and M. Rockville. Algorithms for confidence circles and
ellipses. Citeseer, 1984.
[21] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional
neural networks for mobile vision applications.
arXiv preprint
arXiv:1704.04861, 2017.
[22] K. Hsieh, G. Ananthanarayanan, P. Bodik, P. Bahl, M. Philipose, P. B.
Gibbons, and O. Mutlu. Focus: Querying large video datasets with low
latency and low cost. arXiv preprint arXiv:1801.03493, 2018.
[23] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi,
I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and K. Murphy.
Speed/accuracy trade-offs for modern convolutional object detectors.
CoRR, abs/1611.10012, 2016.
[24] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, and M. Zaharia. Noscope:
Optimizing neural network queries over video at scale. Proc. VLDB