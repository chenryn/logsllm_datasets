mitted to the ad exchange. This enables the ad exchange to tag the
user and track her interactions on the advertiser’s website. The
tagged user is then easily re-identiﬁed later on other websites and
is targeted with ads from the advertiser. Consequently, a user’s past
Figure 2: The percentage of websites that use frame-busting and
X-Frame-Options techniques in the Alexa top 20 categories
browsing history and online interests do not impact re-marketing
ads.
This script can be easily detected by parsing the HTML code
of a webpage1. Thus, a simple approach to ﬁnd content for re-
marketing pollution is to parse webpages of advertisers belonging
to high-paying categories and identify those that host re-marketing
scripts.
Pollution Content for Behavioral Targeting. The approach of
simply scanning websites in a directory service is not sufﬁcient for
ﬁnding content for behavioral pollution as the ad exchange cate-
gories may not match those of the directory service. Alternatively,
the polluter can exploit the ad preference dashboards made avail-
able by large ad exchanges to build an ofﬂine map between web-
pages and the category label assigned to these webpage. Speciﬁ-
cally, the polluter can impersonate a user with a blank proﬁle (delete
all cookies and create a fresh browser proﬁle), browse pages from
a speciﬁc category and record the corresponding proﬁle generated
by the ad exchange. This map can then be used to select pollution
content. Unlike re-marketing based pollution, the impact of be-
havioral pollution on altering user proﬁles towards more lucrative
advertisers depends on the users’ existing online interest proﬁle.
We empirically evaluate this impact across diverse user proﬁles in
Section 5.
3.2 Hosting and Loading Pollution Content
The pollution content hosted by the fraudulent publisher should
be loaded by the user’s browser in a way that is not discernible by
the user and ad exchange. While there are many ways to fabricate
such camouﬂaged requests, such as CSRF, XSS and Clickjacking
etc., in this paper we assume the pollution content is loaded us-
ing cross reference issued by hidden HTML iframes. These
iframes are located outside the viewing area of the browser or
layered underneath other content, and are used to reference and
load pollution content. The loading of such content takes place in
the background and is completely hidden from the user. Moreover,
since approaches for frame-busting are not ubiquitously deployed,
simple approaches can be used to hide the frame content from web
crawlers.
Embedding third-party websites. Websites that want to prevent
being embedded within an iframe, often as means to mitigate
1DoubleClick itself provides instructions on how their Tag
Assistant detects re-marketing scripts.
For more details, see
https://support.google.com/tagassistant/
answer/2954407?hl=en
Ad	
  Exchange	
  and	
  Proﬁler	
  Publisher	
  Website	
  Ad	
  Slot	
  Proﬁle	
  Polluter	
  2.	
  Request	
  Ad	
  1.	
  Visit	
  Publisher	
  6.	
  Winning	
  Ad	
  3.	
  Request	
  Bids	
  for	
  User	
  Visi?ng	
  Publisher	
  4.	
  Bid	
  Responses	
  Pollu?on	
  Content	
  1b.	
  Track	
  Visit	
  to	
  Polluted	
  Content	
  1a.	
  Load	
  Pollu?on	
  Content	
  Adver8sers	
  5.	
  Run	
  Auc?on	
  Users	
  Computers-InternetGames-RoleplayingBusiness-FinancialSports-BaseballKids-and-Teens-SchoolShoppingComputers-SoftwareComputers-SecurityReferenceScience-SocialBusiness-TelecoHealth-MedicineRecreation-TravelKids-and-Teens-SchoolArts-TelevisionRecreation-RadioBusiness-RealHealth-PharmacyShopping-HomeRecreation-CollectingCategory05101520253035Percentage of pages in categoryFrame-BustingX-Frame-Optionclickjacking [27], employ techniques such as X-Frame-Options HTTP
response header or Frame-Busting. We study the prevalence of
these techniques by crawling the top 500 webpages belonging to
each of the top 20 Alexa sub-categories. For each website, we
tested whether it uses X-Frame-Options or one of the known Frame-
Busting techniques [34].
Figure 2 shows the percentage of websites that use X-Frame-
Options and Frame-Busting ordered by the aggregate percentage
of the two methods. Only 5 sub-categories have more than 15%
of the top websites that deploy embedding protection techniques.
The vast majority of categories have less than 5% of their websites
that employ such techniques, and the average across all is 4.6%.
This shows that X-Frame-Options and frame busting are not ubiqui-
tously deployed and the attack can leverage pollution content across
a wide range of categories.
Avoiding detection from web crawlers. Hosting pollution con-
tent can have many adverse effects if detected by search engine
crawlers. Upon detecting content embedded in hidden iframes,
search engine crawlers can potentially ﬂag the pollution content
as malicious, blacklist the website or even ban the website from the
search engine’s index [11]. Nevertheless, it is possible to circum-
vent detection from web crawlers by generating pollution content
dynamically in an obfuscated JavaScript code block, similar to how
malicious websites that host drive-by download scripts evade scans
from security checking web crawlers [19, 28]. Speciﬁcally, the
fraudulent publisher can use obfuscated JavaScript code to show
crawlers benign content rather than the pollution content available
to real users.
3.3 Attack Victims
The primary victims of the attack are the advertisers that are be-
ing scammed to bid higher for users that are not really interested in
their offering. The other victims are the website visitors, who have
their proﬁles altered as a result of the attack. The immediate result
is that the ads a user sees are irrelevant to her real interests, which
might be offensive in some contexts. A different outcome can be
in cases where the user’s online interest proﬁle is also used for per-
sonalizing other services. For example, Google may potentially use
the same user proﬁle to recommend movies on YouTube and even
re-order search results based on user’s online interests [9].
3.4 Attack Monetization – CPM and CPC
An important property of the attack is that it can be used to fur-
ther boost the revenue generated by existing click and impression
fraud mechanisms. This can be achieved if the bot master has con-
trol over the user’s browser such that it can pollute user proﬁles
to maximize the impact of the fraud. When deployed in isola-
tion of existing fraud mechanisms, the attack is most effective for
CPM-based ad campaigns. This is because CPM-based campaigns,
which are the most common campaigns for display ads [25, 35],
provide consistent cash ﬂow to the publisher, regardless of whether
visitors click on the potentially unrelated ads. In the rest of the pa-
per we focus on CPM-based campaigns and assume that the attack
is deployed as a standalone attack without deploying additional
fraud methods.
4. ATTACK SETUP
In setting up the proﬁle pollution attack we seek to address two
main objectives. First, the attack setup should validate the com-
plete end-to-end attack. Second, the attack setup should enable a
detailed characterization of the effectiveness of the attack. Ideally,
this can be achieved by compromising a legitimate publisher web-
Figure 3: Cumulative distribution of the Alexa ranking of domains
in the web traces.
site to host pollution content, polluting the proﬁle of users visiting
the compromised publisher website, and monitoring the change in
ad revenue generated by the publisher. However, doing so in a live
setting raises several ethical concerns. Moreover, it limits our abil-
ity to provide a detailed characterization of the attack since it is not
feasible to record the ads served to real users without the coopera-
tion of the ad exchange.
To this end, we set up the attack as follows.
Instead of driv-
ing live trafﬁc, we emulate users browsing the websites with web
traces. A few domains from the users’ traces are selected as the
fraudulent publishers. As we do not have control over these web-
sites, the proﬁle polluter is separated from the fraudulent publisher
and is responsible for polluting the emulated user trafﬁc immedi-
ately after loading the publisher’s page to approximate a publisher
that pollutes his own users. A distributed testbed of 200 nodes
spread across the world (using PlanetLab) is used to generate web
trafﬁc to ensure location diversity. Since the trafﬁc is emulated
from browsers that we control, an ad crawler is used to record all
the DoubleClick ads delivered to the emulated users. The recorded
ads are analyzed and the revenue is estimated using publicly avail-
able CPM index values published by DoubleClick [26]. We also
set up our own website as a fraudulent publisher to characterize the
effectiveness of the attack.
4.1 User Web Traces & Proﬁles
Our attack setup replays complete web traces from real users to
characterize and validate the attack. This is important because the
ad revenue is not only impacted by the frequency with which users
visit the publisher page but also depends on the user’s online inter-
est proﬁle before and after pollution; the pollution impact depends
on websites visited prior to pollution and the duration of the impact
depends on websites visited after pollution.
We use web traces of real users from a Chrome extension in-
stalled by more than 700 users who have been using the extension
for 2 years for research purpose. The functionality of the extension
was modiﬁed to record all the webpage URLs visited by the user
for a one week period (March 10th, 2014 - March 16th, 2014)2.
In this time period, we collected a total of 224,855 page visits from
619 unique active users. Our dataset is diverse and consists of users
using the extension across the world.
The web traces we use consist of users with diverse online inter-
ests and browsing behavior that are located across the world. Fig-
ure 3 shows that the websites in our dataset cover a large range of
Alexa ranking, from extremely popular websites such as google.com
and facebook.com, to websites that are ranked very low. Fig-
2IRB approval was granted and users were notiﬁed about the type
of data collected and the intent of use for research purposes
0.0e+001.0e+072.0e+070.00.20.40.60.81.0Alexa global rankCDF4.3 Publisher Webpage
The complete attack is validated on two different type of pub-
lisher webpages.
Live Websites. We validate the attack on existing live publishers
whose ad revenue is impacted by the dynamic content hosted by
them as well as pre-existing preferences about type of ads that are
allowed to be targeted. To this end, we select the top 19 most vis-
ited websites that host DoubleClick ads from the user web traces.
Instead of compromising these websites to host pollution content,
we set up the proﬁle polluter as a separate entity. When emulating
trafﬁc traces, we forward the user to the proﬁle polluter immedi-
ately after visiting any one of these 19 websites. We use results
from these publishers primarily to estimate the revenue generated
by the attack (Section 6).
Controlled Publisher. In order to form a baseline of the effective-
ness of the attack, we set up our own publisher website and sign up
with AdSense [2]. The publisher website has two display ad slots
(top banner display ad and a side display ad) and uses the default
settings provided by AdSense. Since AdSense requires the website
to host some content before approving it, we upload static con-
tent that describes the different ad targeting mechanisms available
to advertisers. Visiting the webpage with a blank proﬁle results in
DoubleClick proﬁling the user with interests belonging to the Com-
puters category. Similar to the above setup, the proﬁle polluter is
separated from the controlled publisher.
4.4 Trace Emulator and Ad Crawler
A critical component of the attack setup is a distributed infras-
tructure to emulate web trafﬁc by replaying the traces and recording
all the ads delivered to the emulated user.
4.4.1 Trace Emulator
We develop a distributed infrastructure based on the PlanetLab
testbed that is able to emulate real user web trafﬁc. The trace em-
ulator consists of a central control server and 264 worker nodes
distributed across the world. The server maintains a list of tasks
that are fetched by distributed workers periodically. Given a task
containing the URL to visit and a unique user ID, the worker node
instance loads one proﬁle of the corresponding user, visits the as-
signed URL, records all the ads displayed on the webpage and the
associated metadata, and sends this information back to the central
server. The user’s proﬁle is updated accordingly after visiting the
assigned URL.
4.4.2 Ad Crawler
Collecting measurements about display ads requires the ability
to disassemble the elements of a webpage, identify ad elements
and associate these with particular categories. Existing ad monitor-
ing and blacklisting tools – AdBlock [1] and Ghostery [6] – work
by matching URL patterns embedded in a webpage against a set
of blacklist patterns, and cannot look deeper into the element and
reason about it. The task is made even more difﬁcult by complex
DOM structures, deep nesting of elements, and dynamic JavaScript
execution, that is found on a large fraction of pages on the Internet
today. To address these challenges we extend the PhantomJS head-
less browser3 to reliably extract the ad elements of a page, identify
the actual landing pages for the ad elements, and associate the ads
with speciﬁc semantic categories. The current implementation of
the ad crawler is limited to ads delivered by DoubleClick. In the
3http://phantomjs.org
Figure 4: Cumulative distributions of the number of daily visits per
user to the same domain in our web traces v.s. that of Alexa top
domains.
ure 4 shows the average daily page view by a user for a given do-
main, exhibiting a similar wide range, from websites that have one
visit per visitor to 10s of pages per visitor. To validate that visit
patterns in our dataset are not skewed, the Figure also plots the dis-
tribution of daily page views per visitor for the top-100 websites in