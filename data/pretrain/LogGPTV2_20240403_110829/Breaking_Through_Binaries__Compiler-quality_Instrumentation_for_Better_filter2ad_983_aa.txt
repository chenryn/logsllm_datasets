title:Breaking Through Binaries: Compiler-quality Instrumentation for Better
Binary-only Fuzzing
author:Stefan Nagy and
Anh Nguyen-Tuong and
Jason D. Hiser and
Jack W. Davidson and
Matthew Hicks
Breaking Through Binaries: Compiler-quality 
Instrumentation for Better Binary-only Fuzzing
Stefan Nagy, Virginia Tech; Anh Nguyen-Tuong, Jason D. Hiser, and 
Jack W. Davidson, University of Virginia; Matthew Hicks, Virginia Tech
https://www.usenix.org/conference/usenixsecurity21/presentation/nagy
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Breaking Through Binaries: Compiler-quality Instrumentation
for Better Binary-only Fuzzing
Stefan Nagy
Virginia Tech
PI:EMAIL
Anh Nguyen-Tuong, Jason D. Hiser, Jack W. Davidson
University of Virginia
{nguyen, hiser, jwd}@virginia.edu
Matthew Hicks
Virginia Tech
PI:EMAIL
Abstract
Coverage-guided fuzzing is one of the most effective soft-
ware security testing techniques. Fuzzing takes on one of
two forms: compiler-based or binary-only, depending on
the availability of source code. While the fuzzing commu-
nity has improved compiler-based fuzzing with performance-
and feedback-enhancing program transformations, binary-
only fuzzing lags behind due to the semantic and perfor-
mance limitations of instrumenting code at the binary level.
Many fuzzing use cases are binary-only (i.e., closed source).
Thus, applying fuzzing-enhancing program transformations
to binary-only fuzzing—without sacriﬁcing performance—
remains a compelling challenge.
This paper examines the properties required to achieve
compiler-quality binary-only fuzzing instrumentation. Based
on our ﬁndings, we design ZAFL: a platform for applying
fuzzing-enhancing program transformations to binary-only
targets—maintaining compiler-level performance. We show-
case ZAFL’s capabilities in an implementation for the popular
fuzzer AFL, including ﬁve compiler-style fuzzing-enhancing
transformations, and evaluate it against the leading binary-
only fuzzing instrumenters AFL-QEMU and AFL-Dyninst.
Across LAVA-M and real-world targets, ZAFL improves crash-
ﬁnding by 26–96% and 37–131%; and throughput by 48–
78% and 159–203% compared to AFL-Dyninst and AFL-
QEMU, respectively—while maintaining compiler-level of
overhead of 27%. We also show that ZAFL supports real-
world open- and closed-source software of varying size (10K–
100MB), complexity (100–1M basic blocks), platform (Linux
and Windows), and format (e.g., stripped and PIC).
1 Introduction
Software vulnerabilities represent a persistent threat to cyber-
security. Identifying these bugs in both modern and legacy
software is a tedious task; manual analysis is unrealistic, and
heavyweight program analysis techniques like symbolic ex-
ecution are unscalable due to the sheer size of real-world
applications. Instead, developers and bug-hunters alike have
largely adopted a software testing strategy known as fuzzing.
Fuzzing consists of mutationally generating massive
amounts of test cases and observing their effects on the target
program, with the end goal of identifying those triggering
bugs. The most successful of these approaches is coverage-
guided grey-box fuzzing, which adds a feedback loop to keep
and mutate only the few test cases reaching new code cover-
age; the intuition being that exhaustively exploring target code
reveals more bugs. Coverage is collected via instrumentation
inserted in the target program’s basic blocks. Widely suc-
cessful coverage-guided grey-box fuzzers include AFL [93],
libFuzzer [70], and honggFuzz [75].
Most modern fuzzers require access to the target’s source
code, embracing compiler instrumentation’s low overhead
for high fuzzing throughput [70, 75, 93] and increased crash
ﬁnding. State-of-the-art fuzzers further use compilers to ap-
ply fuzzing-enhancing program transformation that improves
target speed [32, 47], makes code easier-to-penetrate [1], or
tracks interesting behavior [18]. Yet, compiler instrumenta-
tion is impossible on closed-source targets (e.g., proprietary or
commercial software). In such instances fuzzers are restricted
to binary instrumentation (e.g., Dyninst [64], PIN [56], and
QEMU [8]). But while binary instrumentation succeeds in
many non-fuzzing domains (e.g., program analysis, emula-
tion, and proﬁling), available options for binary-only fuzzing
are simply unable to uphold both the speed and transforma-
tion of their compiler counterparts—limiting fuzzing effec-
tiveness. Despite advances in general-purpose binary instru-
mentation [9, 41, 46, 86, 87], it remains an open question
whether compiler-quality instrumentation capabilities and
performance are within reach for binary-only fuzzing.
To address this challenge we scrutinize the ﬁeld of binary
instrumentation, identifying key characteristics for achieving
performant and general-purpose binary-only fuzzing instru-
mentation. We apply this standard in designing ZAFL: an
instrumentation platform bringing compiler-quality capabil-
ities and speed to x86-64 binary-only fuzzing. We demon-
strate how ZAFL facilitates powerful fuzzing enhancements
with a suite of ﬁve transformations, ported from compiler-
based fuzzing contexts. We show how ZAFL’s capabilities
improve binary-only fuzzing bug-ﬁnding: among evaluations
on the LAVA-M corpus and eight real-world binaries, ZAFL
ﬁnds an average of 26–96% more unique crashes than the
static rewriter AFL-Dyninst; and 37–131% more than the
dynamic translator AFL-QEMU. We further show that ZAFL
achieves compiler-quality overhead of 27% and increases
fuzzing throughput by 48–78% and 131–203% over AFL-
Dyninst and AFL-QEMU, respectively. Lastly, we show that
USENIX Association
30th USENIX Security Symposium    1683
ZAFL scales to real-world software—successfully instrument-
ing 56 binaries of varying type (33 open- and 23 closed-
source), size (10K–100MB), complexity (100–1,000,000 ba-
sic blocks), and platform (30 Linux and 12 Windows).
In summary, this paper contributes the following:
• We examine the challenges of achieving compiler-quality
instrumentation in binary-only fuzzing, developing a crite-
ria for success, and highlighting where popular binary-only
instrumenters ﬁt with respect to our criteria.
• We apply this criteria in designing ZAFL: a platform
for state-of-the-art compiler-quality instrumentation—and
speed—in binary-only fuzzing. ZAFL’s architectural focus
on ﬁne-grained instrumentation facilitates complex fuzzing-
enhancing transformations in a performant manner.
• We show that it is possible to achieve fuzzing-enhancing
program transformation in a performant manner for binary-
only contexts by implementing ﬁve of such transformations
derived from existing compiler-based implementations in
ZAFL, and evaluating runtime overhead.
• We demonstrate how ZAFL improves fuzzing effectiveness;
on average ZAFL’s performant, fuzzing-enhancing program
transformations enable fuzzers to ﬁnd more unique crashes
than the leading binary-only fuzzing instrumenters AFL-
Dyninst and AFL-QEMU across both LAVA-M and real-
world benchmarks.
• We show that ZAFL supports real-world binaries of varying
characteristics, size, complexity, and platform—even those
binaries not supported by other instrumenters.
• We will open-source ZAFL and all benchmark corpora at
https://git.zephyr-software.com/opensrc/zafl.
2 Background on Fuzzing
Coverage-guided grey-box fuzzing remains one of the most
successful software security auditing techniques. Fuzzers of
this type iteratively mutate test cases to increase code cover-
age, using lightweight instrumentation to collect this coverage
at runtime. This section details the fundamental components
of coverage-guided grey-box fuzzing.
2.1 An Overview of Fuzzing
Fuzzing is designed to root-out software vulnerabilities auto-
matically. Given a target program and a set of seed test cases,
a standard fuzzing cycle consists of (Figure 1):
0. Instrumentation: modify target program as desired (e.g.,
to track code coverage).
1. Test Case Generation: select a seed and mutate it to gen-
erate a batch of candidate test cases.
2. Execution Monitoring and Feedback Collection: run
each candidate test case and monitor the target program’s
execution, collecting feedback via instrumentation.
3. Feedback Decision-making: keep only test cases with ex-
ecution behavior matching some pre-speciﬁed constraint(s)
Figure 1: A high-level overview of the basic fuzzing workﬂow.
(e.g., cover new code).
4. Return to step 1.
Though fuzzers vary by generation (i.e., mutation- [70, 75,
93] or grammar-based [35,50,60]), execution monitoring (i.e.,
white- [17,22,36], black- [60,63,83], or grey-box [70,75,93]),
and feedback decision-making strategies (i.e., directed [13,
33, 41, 89] or coverage-guided [14, 70, 75, 93]), we elide their
differentiation as they are outside the focus of this paper.
2.2 Coverage-guided Grey-box Fuzzing
By far the most popular fuzzing technique is coverage-guided
grey-box fuzzing (e.g., AFL [93], honggFuzz [75], and lib-
Fuzzer [70]). As the name implies, coverage-guided grey-box
fuzzers focus exclusively on test cases that increase code
coverage, with the aim of testing as much of a target pro-
gram’s functionality as possible to ﬁnd its deeply-rooted bugs.
Its “grey-box” quality refers to a middle-ground between
the deep and shallow program analyses used by white- and
black-box fuzzers, respectively: lightweight instrumentation
is used track test cases’ coverage of the target, which is then
post-processed to verify if new code has been covered.
Contingent on the ability to instrument a target program
from source, fuzzing is divided into two distinct worlds:
compiler-based and binary-only. Most modern fuzzers turn
to compiler instrumentation as its low runtime overhead sup-
ports high fuzzing throughput. More recent state-of-the-art
efforts leverage compilers’ ability to apply complex program
transformations. Researchers have shown that such transfor-
mations improve fuzzing effectiveness by enhancing perfor-
mance [32,47] or introspection [1,18,31,51]. Most real-world
fuzzing is undertaken in the absence of target source (i.e.,
binary-only). This restricts fuzzing to existing binary instru-
menters which are unsupportive of compiler-quality transfor-
mation, facing prohibitively-high overhead—often as high as
1000% for coverage tracing alone [62].
3 Compiler-based Fuzzing Enhancements
Coverage-guided fuzzing spans two distinct domains:
compiler-based and binary-only, with both using program
instrumentation to track test case code coverage. Much of
fuzzing’s success is due to the high throughput made possible
by fast compiler instrumentation [79, 93]. Though advanced
fuzzers introduce more heavyweight analyses [7, 18, 74, 92],
1684    30th USENIX Security Symposium
USENIX Association
TargetApplicationInstrumentedTargetTest CaseGenerationInstrumentTargetExec. Monitoring, Feedback CollectionFeedbackDecision-Making02314Focus
Performance
Feedback
Category
Instrumentation
Pruning
Instrumentation
Downgrading
Sub-instruction
Proﬁling
Extra-coverage
Behavior
Effect on Fuzzing
Overhead reduction from fewer
blocks instrumented
Overhead reduction from lighter-
weight instrumentation
Incremental coverage to guide code
penetration
Ability to consider ﬁner-grained ex-
ecution behavior
Table 1: Popular compiler-based fuzzing-enhancing program transformations,
listed by category and effect.
the core of these approaches remains the standard coverage-
guided fuzzing loop (Figure 1)—amounting to over 90%
of their execution time [62]; recent feedback enhancements
(e.g., context sensitivity) only increase the proportion of time
spent tracing execution. Thus, our focus is performant fuzzing-
enhancing transformations in the absence of source code.
State-of-the-art fuzzers leverage compiler instrumenta-
tion to add transformations that improve fuzzing perfor-
mance and feedback (e.g., AFL++ [31], Angora [18], Col-
lAFL [32], honggFuzz [75], INSTRIM [47], libFuzzer [70]).
Performance-enhancing transformation helps alleviate the
runtime cost of coverage tracing and other feedback sources.
Feedback-enhancing transformations reveal ﬁner-grained pro-
gram progress, beyond traditional code coverage metrics. We
broadly examine popular fuzzers and identify four categories
of fuzzing-enhancing transformation that target the core
coverage-guided loop (Table 1): (1) instrumentation prun-
ing, (2) instrumentation downgrading, (3) sub-instruction
proﬁling, and (4) extra-coverage behavior tracking. Below
we detail each transformation.
3.1
Graph reducibility techniques [42, 77] are used in fuzzing
to elide instrumenting some target basic blocks, thus low-
ering overall runtime overhead. AFL’s [93] compiler instru-
mentation permits a “ratio”: 100 instruments all blocks; 0
only function entries; and values in between form a probabil-
ity to arbitrarily skip blocks. Clearly, culling random blocks
risks coverage blind-spots. More rigorous CFG-aware anal-
yses [31, 47] prune blocks implicitly covered by others: for-
mally, for N blocks and M unique paths over N, it is possible
to select a subset N(cid:48) ∈ N such that the M(cid:48) unique paths over N(cid:48)
equals M. INSTRIM [47] only instruments blocks targeted by
backward edges and tracks loops either by entry or pre-entry
blocks (the latter forgoing loop iteration tracking).
Instrumentation Pruning
Instrumentation Downgrading
3.2
The majority of today’s fuzzers track coverage in the form of
edges (i.e., branches between basic blocks). Edges are typi-
cally recorded as hashes of their start and end blocks (com-
puted in the body of the end block’s instrumentation), as popu-
larized by the fuzzer AFL [93]. Edge hashing requires several
instructions (two index fetches, a hash, array update, and an
XOR); but given that blocks themselves are small, maintain-
ing speed requires inserting as few instructions as necessary.
CollAFL [32]’s compiler instrumentation optimizes single-
predecessor blocks by downgrading them to fewer-instruction
block coverage (i.e., cov(A → B) ≡ cov(B)).
3.3 Sub-instruction Proﬁling
Fuzzers struggle to penetrate code guarded by complex pred-
icates like “magic bytes” [68], nested checksums [7], and
switch cases [1]. Most fuzzers track edge/block coverage and
hence are oblivious to “incremental” predicate progress. Re-
cent compiler-based efforts apply sub-instruction proﬁling—
decomposing multi-byte conditionals into single-byte com-
parisons (e.g., CmpCov [51], honggFuzz [75], laf-Intel [1]).
Such splitting of roadblocks into smaller, simpler problems
facilitates greater fuzzing code coverage.
3.4 Extra-coverage Behavior Tracking
An area of current research in fuzzing is the inclusion of exe-
cution behavior beyond traditional code coverage. Although
we foresee future work considering metrics such as register or
memory usage, the existing body of work on extra-coverage
behavior tracking focuses on context sensitivity. Context-
sensitive coverage tracks edges along with their preceding
calling context. For example, given two paths over the same
set of edges, A → B → C and B → A → C, context-insensitive
coverage misses the second path as it offers no new edges;
however context-sensitive coverage reveals two distinct calls:
B → C and A → C. Several LLVM implementations exist for
both function- and callsite-level context sensitivity [18, 31].
4 Binary-only Fuzzing: the Bad & the Ugly
Program transformation has become ubiquitous in compiler-
based fuzzers (e.g., AFL++ [31], CollAFL [32], laf-Intel [1]),
and for good reason: it makes fuzzing signiﬁcantly more
powerful. Despite these advantages there is no platform that
adapts such transformation to binaries in an effective manner—
severely impeding efforts to fuzz closed-source software.
This section examines existing binary instrumenters and
their limitations that prevent them from attaining effective
binary-only fuzzing instrumentation. We follow this explo-
ration with an identiﬁcation of the key instrumenter de-
sign attributes necessary to support compiler-quality fuzzing-
enhancing program transformation and speed.
4.1 Limitations of Existing Platforms
Coverage-guided fuzzers trace test case code coverage via fast
compiler instrumentation; and state-of-the-art efforts further
leverage compilers to apply fuzzing-enhancing program trans-
formation. In binary-only fuzzing, code coverage is traced
by one of three mechanisms: (1) hardware-assisted tracing,
USENIX Association
30th USENIX Security Symposium    1685
Name
LLVM
Intel PT
DynamoRIO
PIN
QEMU
Dyninst
RetroWrite
Fuzzing
Appearances
[1,6,13,18,19,31,32,
47, 70, 75, 93]
[7, 11, 20, 37, 75]
[37, 43, 73]
[45, 49, 63, 68, 92]
[23, 31, 91, 93]
[44, 55, 62, 76]
[26]
Fuzzing
Overhead
18–32%
19–48%
>1,000%
>10,000%
>600%
>500%
20–64%
Supports
Xform
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

type
static
hardware
dynamic
dynamic
dynamic
static
static
Instrumentation
invoked
liveness
(cid:88)

(cid:88)
(cid:88)