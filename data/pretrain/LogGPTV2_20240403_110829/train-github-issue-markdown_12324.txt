 **System information**.
  * Have I written custom code (as opposed to using a stock example script provided in Keras): No
  * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
  * TensorFlow installed from (source or binary): Anaconda
  * TensorFlow version (use command below): 2.9.1
  * Python version: 3.9
  * GPU model and memory: None
**Describe the problem**.  
I am going through Francois Chollet's book "Deep Learning with Python" and
running the code in his Jupyter Notebooks with Keras: 2.9.0 , Tensorflow:
2.9.1.  
Notebook 6.3, (under the heading "1.6 Using recurrent dropout to fight
overfitting") has a model following:
    model = Sequential()
    model.add(layers.GRU(32, 
                         dropout=0.2,
                         recurrent_dropout=0.2,
                         input_shape=(None, float_data.shape[-1])))
    model.add(layers.Dense(1))
    model.compile(optimizer=RMSprop(), loss='mae')
    history = model.fit_generator(train_gen,
                                  steps_per_epoch=500,  
                                  epochs=40,
                                  validation_data=val_gen,
                                  validation_steps=val_steps)
Running parts 6.3.6 (recurrent dropout) and 6.3.7 (stacked recurrent layers)
gives the following training and validation losses:
6.3.6  
![1661232542955](https://user-
images.githubusercontent.com/74046524/186118937-0f7ccf47-c025-4530-a8e0-e1e145f9e02f.png)
6.3.7  
![1661246457143](https://user-
images.githubusercontent.com/74046524/186122056-82d61282-e6b9-4cd7-b822-9e52fb296c5c.png)
**Describe the expected behavior**.  
After using dropout it should solve the overfitting problem just like in the
book.