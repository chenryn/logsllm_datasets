title:The Evolution of System-Call Monitoring
author:Stephanie Forrest and
Steven A. Hofmeyr and
Anil Somayaji
2008 Annual Computer Security Applications Conference
2008 Annual Computer Security Applications Conference
The Evolution of System-call Monitoring
Stephanie Forrest
Dept. of Computer Science
University of New Mexico
Albuquerque, NM USA
PI:EMAIL
Steven Hofmeyr
Anil Somayaji
Lawrence Berkeley Laboratory
School of Computer Science
Berkeley, CA USA
PI:EMAIL
Carleton University
Ottawa, ON Canada
PI:EMAIL
Abstract
Computer security systems protect computers and net-
works from unauthorized use by external agents and insid-
ers. The similarities between computer security and the
problem of protecting a body against damage from exter-
nally and internally generated threats are compelling and
were recognized as early as 1972 when the term computer
virus was coined. The connection to immunology was made
explicit in the mid 1990s, leading to a variety of prototypes,
commercial products, attacks, and analyses. The paper re-
views one thread of this active research area, focusing on
system-call monitoring and its application to anomaly in-
trusion detection and response.
The paper discusses the biological principles illustrated
by the method, followed by a brief review of how system call
monitoring was used in anomaly intrusion detection and
the results that were obtained. Proposed attacks against
the method are discussed, along with several important
branches of research that have arisen since the original pa-
pers were published. These include other data modeling
methods, extensions to the original system call method, and
rate limiting responses. Finally, the signiﬁcance of this body
of work and areas of possible future investigation are out-
lined in the conclusion.
1 Introduction
During the 1990’s the Internet as we know it today grew
from a small network of trusted insiders to a worldwide con-
glomerate of private citizens, governmental agencies, com-
mercial enterprises, and academic institutions. As society
at large embraced the Internet, opportunities and incentives
for malicious activities exploded, creating demand for new
computer security methods that could succeed in this open
and uncontrolled environment. Open applications, mobile
code and other developments helped erode the notion of
a clear perimeter, which formerly separated a trusted sys-
tem from its external environment. Previous approaches to
computer security had emphasized top-down policy spec-
iﬁcation, provably correct implementations of policy, and
deployment in correctly conﬁgured systems. Each of these
assumptions became increasingly untenable, as the Internet
grew and was integrated into human society.
The similarities between computer security in the age of
the Internet and the problem of protecting a body against
damage from internally and externally generated threats are
compelling. They were recognized as early as 1972 when
the term computer virus was introduced [67]. Later, Spaf-
ford argued that computer viruses are a form of artiﬁcial life
[66], and several authors investigated the analogy between
epidemiology and the spread of computer viruses across
networks [38, 51, 59, 55]. The connection to immunol-
ogy was made explicit in [15, 37], and since that time the
ideas have been extended to incorporate signiﬁcant amounts
of immunology and to tackle ambitious computer security
problems, including computer virus detection [15, 37], net-
work security [24, 79, 33], spam ﬁltering [57], and com-
puter forensics [46].
As the primary defense system of the body, immune sys-
tems are a natural place to look for ideas about architec-
tures and mechanisms for coping with dynamic threat en-
vironments. Immune systems detect foreign pathogens and
misbehaving internal components (cells), and they choose
and manage an effective response autonomously. Thus, the
immune system can be thought of as a highly sophisticated
intrusion detection and response system (IDS).
Despite debate in the immunological literature about
how the immune system recognizes threats, in most of the
work on computer immune systems it is assumed that nat-
ural immune systems work by distinguishing between pro-
tein fragments (peptides) that belong to the properly func-
tioning body (self) and ones that come from invading and
malfunctioning cells (nonself). To explore IDS designs that
mimic those of the immune system, we must ﬁrst decide
what data or activity patterns will be used to distinguish be-
tween computational self and nonself. That is, we must de-
1063-9527/08 $25.00 © 2008 IEEE
1063-9527/08 $25.00 © 2008 IEEE
DOI 10.1109/ACSAC.2008.54
DOI 10.1109/ACSAC.2008.54
418
418
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
cide what data streams the IDS will monitor and identify a
threat model. To build a computer immune system, then, a
computational analog to peptides must be found, one that
will allow security violations to be detected without gener-
ating too many false alarms in response to routine changes
in system behavior.
In this paper we focus our attention on the deﬁnition
of self introduced in [14, 25, 65] to protect executing pro-
grams.
In this work, discrimination between normal and
abnormal behavior was based on what system calls (oper-
ating system services) are normally invoked by a running
program. As a program executes, it might make several mil-
lion system calls in a short period of time, and this signature
of normal behavior is sufﬁcient to distinguish between nor-
mal behavior and many attacks. Many attacks cause a vul-
nerable program to execute infrequently used code paths,
which in turn leads to anomalous patterns of system calls.
Anomaly detection based on system calls is able to detect
intrusions that target a single computer, such as buffer over-
ﬂow attacks, SYN ﬂoods, conﬁguration errors, race con-
ditions, and Trojan horses under the assumption that such
attacks produce code execution paths that are not executed
in normal circumstances.
A large number of researchers adopted the system-call
approach, some seeking to improve on the original methods
[49, 54, 45, 62, 29], some applying its method to other prob-
lems [68, 30, 50], and some attempting to defeat the system
[74, 70]. Sana Security developed a product known as Pri-
mary Response based on the technology, which it actively
marketed to protect servers. At this writing, the system-call
method is the most mature application of biological princi-
ples to computer security. In this paper, we ﬁrst review the
general principles that guided our design decisions (Section
2); we next describe brieﬂy the original system-call method
and summarize the results it achieved (Section 3). Next,
we summarize several important lines of research that have
arisen since the original paper was published (Sections 4,
5, 6, 7). Finally Section 8 speculates on the signiﬁcance of
this body of work.
2 General Principles
The biological analogy led to a set of general design prin-
ciples, which remain relevant to computer security more
than a decade later. These include:
• A generic mechanism that provides coverage of a
broad range of attacks, even if it is not 100% provably
secure for any particular threat model. This approach
is similar in spirit to “universal weak methods” in arti-
ﬁcial intelligence that are applicable to many problems
and do not require specialized domain knowledge. Al-
though some of the pattern recognition mechanisms of
the adaptive and innate immune systems are biased to-
wards detecting important classes of infection, many
are generic “danger detectors.” The existence of au-
toimmune disease provides evidence that they are im-
perfect; yet they are highly effective at eliminating in-
fections, and natural selection has conserved and en-
hanced immune systems in all higher animals. Choos-
ing a ubiquitous and fundamental data stream, such as
system calls, allowed us to design a system that could
protect a wide variety of programs against a wide vari-
ety of threats without specialized knowledge.
• Adaptable to changes in the protected system. Com-
puter systems, environments, and users are highly dy-
namic, and normal legitimate uses of systems are con-
tinually evolving. Just as biological defense systems
need to cope with natural adaptive processes, so must
computer security systems if they are to be robust. In
our system, adaptability was achieved through the use
of simple learning mechanisms, which were used to
construct models of normal behavior and to update
them over time.
• Autonomy: Computer systems have traditionally been
thought of as tools that depend upon humans for
guidance. However, as computer systems have be-
come powerful, numerous, and interconnected, it is no
longer feasible for humans to manage them directly.
Biological systems necessarily operate independently,
on-line, and in real-time because they live and inter-
act with physical environments.
In our system, we
addressed this requirement with the most lightweight
simple design we could think of—ignoring system call
arguments, modeling data with simple data structures
and without calculating probabilities or frequencies,
and a generic response mechanism that slows down
suspicous processes.
• Graduated Response: In computer security, responses
tend to be binary, as in the case of access controls (ei-
ther a user is allowed access or not), ﬁrewalls (either
a connection is blocked or it is not), or cryptography
(where either a ﬁle is encrypted or it is not). Biolog-
ical systems have graduated responses, where small
perturbations result in small responses, and large de-
viations cause more aggressive responses. We adopted
this principle in process Homeostasis (pH), where sys-
tem calls were delayed according to how many anoma-
lous system calls had preceded them. Graduated re-
sponses allowed us to move away from the concept of
security as a binary property and to tolerate imperfect
detection.
• Diversity of the protection system. Natural immune
Individual differences arise
systems are diverse.
419419
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
both through genetic variations (e.g., MHC genes) or
through life histories (e.g., which immune memories
an individual has). Individual diversity promotes ro-
bustness, because an attack that escapes detection in
one host is likely to be detected in another. Assum-
ing that hosts have ﬁnite resources to devote to de-
fense, this allows a higher level of population-level
robustness than could be achieved by simply replicat-
ing the same defense system across systems. Beyond
that, diversity of the protection system makes it much
more difﬁcult for an attacker to design a generic attack
that will succeed everywhere. In our system, different
environments and usage patterns confer diversity on
each program invocation, which leads to diverse pat-
terns of system calls observed during “normal” behav-
ior. The extent of this diversity varies with the program
[64, 25].
3 A Sense of Self for Unix Processes
Computer systems are vulnerable to external attack via
many different routes. One of the most important of these
is server programs that run with enhanced privilege, such
as remote login servers (e.g., ssh), mail servers (e.g. send-
mail), web-servers, etc. Software or conﬁguration ﬂaws in
these running programs are exploited by attackers to gain
illegitimate entry into systems, where they can take advan-
tage of the privilege of the compromised process to seize
control of the computer system. Privileged server processes
form the main gateways for remote entry into a system, and
hence it is vital to protect them from attack. These gateway
server programs are frequently patched, but new vulnera-
bilities continue to be discovered, leading to widespread
abuses such as the code-red worm [8], and many others [56].
One common approach is to scan the inputs to the server,
usually at the network level. Such network intrusion de-
tection systems typically scan for signatures of attacks in
network packets. However, such systems are vulnerable to
denial-of-service attacks, spooﬁng and other attacks [61],
and they can only detect attacks for which they have signa-
tures ahead of time. We believed that instead of focusing on
inputs to servers, it would be better to monitor the runtime
behavior of programs, because only code that is running can
actually cause damage. And, it is harder to forge behavior
than to forge data.
3.1 Behavioral characteristics
The central hypothesis of the original research [14] was
that anomaly intrusion detection can provide an effective
additional layer of protection for vulnerable privileged pro-
cesses. An anomaly detection system develops a proﬁle of
normal behavior and monitors for deviations that indicate
attacks. Traditionally, anomaly detection systems focused
on characterizing user-behavior [3, 9, 44] and were often
criticized for having high false positive rates because user
behavior is erratic. The key to effective anomaly detection
is to monitor a characteristic that is stable under normal,
legitimate behavior, and perturbed by attacks.
Many characteristics could potentially be used in an
anomaly detection system. Our choice was based on the
observation that server programs tend to execute a limited
set of tasks repeatedly, often with little variation. Those
tasks correspond to regular paths through the program code.
We chose a proxy in the form of short sequences of system
calls executed by running programs. Our focus on system
calls followed earlier work by Fink, Levitt and Ko [13, 39]
that used system calls in a speciﬁcation-based intrusion de-
tection system. System call sequences can be monitored
from the operating system, without necessitating recompi-
lation or instrumentation of binary or source code, making
the system extremely portable and potentially applicable to
any program that exhibits regular code paths.