Q statistic. NIDES lumps all occurrences of these infrequent symbols into
a single category called “Rare.” The point at which symbols are considered
rare enough to be classiﬁed as a “Rare” symbol is controlled by a param-
eter MAXSUMRAREPROB [1,2]. After sorting all of the symbols by their
occurrence frequency, we mark symbols as rare starting with the least fre-
quent until the total probability sums to MAXSUMRAREPROB. We use a
MAXSUMRAREPROB of 5% in RIDES. This is within the suggested range
of 1% to 10% used in [1,2].
– New – When a foreign symbol arrives, there is no probability data for it in
the long-term model. This makes it diﬃcult to directly compare the short-
and long-term models in the presence of foreign symbols. To deal with this
case NIDES creates a “New” [9] category. This category collectively contains
the probability of seeing a foreign symbol. The “New” category probability
in the long-term model is computed from the total number of foreign symbols
seen in a day, divided by the total number of symbols for that day. In the
short-term model all foreign symbols are lumped together in the category
“New” and are treated exactly like any other category for the purposes of
computing the Q statistic.
5 Methodology
All of the experimental results shown in this paper are generated from a series of
basic experiments. A basic experiment consists of generating synthetic training
Anomaly Detector Performance Evaluation
113
and test data, and then running RIDES on this data for one ﬁxed set of parameter
values. This is performed in the following ﬁve steps:
1. Generate three sets of training data, using a diﬀerent random number seed
for each set. Three sets of data are required to allow RIDES to build both
the long-term proﬁle and the Q-statistic histogram.
2. Run RIDES on each of the training data sets sequentially. The history ﬁle
generated by each invocation is used as the input for the next invocation.
3. Generate the test data and the ground-truth data.
4. Invoke RIDES on the test data; generate an output (alarm or no-alarm)
for each symbol encountered; compare this output to ground-truth data to
produce a result in terms of hit, miss, false alarm or correct rejection.
5. Record in a database the number of hits, misses, false alarms and correct
rejections for the parameter settings that yielded the aforementioned result.
5.1 Generation of Synthetic Data
The data generated for the experiments is in the form of a symbol stream rep-
resenting system calls as they might be collected using a tool such as strace or
some other monitor in the operating system. All generated data sets are com-
prised of a sequence of 100,000 symbols. It is assumed that the monitored stream
is representative of the composite activity on the system, and can include sys-
tem calls made by both normal and (potentially) intruder tasks. The parameters
that aﬀect data synthesis (see Table 1) will be discussed in detail in the following
sections. It is important to note that we have focused on varying only a subset
of all the possible parameters that can be used to characterize a data set. This
is because the number of experiments needed to explore the space increases ex-
ponentially with the number of parameters varied. As a consequence, we have
concentrated on the parameters that we feel have the greatest impact on the
performance of RIDES.
Table 1. Parameters for generating data
Name Description
N
σ
λa
B
Background alphabet size
Distribution standard deviation
Saturation of anomalous symbols
Experiment block size
Data Generation. The training and test data for our experiments were gener-
ated by randomly drawing symbols from an alphabet of N = 250 symbol types.
The symbol types are numbered from 0 to 249 in the order of the most frequently
occurring symbol to the least frequently occurring symbol. An alphabet size of
250 was chosen because it is typical of the number of system calls supported in
typical Unix-like operating systems.
114
J.P. Hansen, K.M.C. Tan, and R.A. Maxion
The frequency with which a symbol occurs in a stream of data is determined
by the “half-bell shaped” distribution curve shown in Figure 1. Each point on the
curve shows the probability that a generated symbol will be of the type shown on
the x-axis. The shape of the curve is controlled by the standard deviation, which
determines how rapidly the probabilities of the symbol types will decrease as we
move from 0 to 249 on the x-axis. When the standard deviation is small, the
symbol-type probabilities will start high, rapidly decaying to near zero. When the
standard deviation is large, the symbol-type probabilities will be nearly identical
(approximately 1
N ) with only a small diﬀerence in probability between the most
likely and least likely symbol types. In our experiments the standard deviation is
σ = 0.15 to mimic the behavior typically exhibited by many systems in which a
small number of system calls is frequently used, while many other calls are used
only rarely. (Note that for clarity of exposition the term “data distribution” will
be used to refer to the symbol-type distribution shown in Figure 1.)
The probability px of generating symbol-type x is deﬁned in terms of N (the
alphabet-size, numbered from 0 to N − 1) and σ (the standard deviation) as:
px = qx
N−1(cid:3)
i=0
qi
where qi is the unnormalized probability:
√
2πσ
qx = e
−(x/N)2/(2σ)2
(3)
(4)
The symbols generated in the sequence are independent and identically dis-
tributed (i.i.d.). The formula for unnormalized probability (4) is taken from the
probability density function (pdf) of a normal distribution. To compute the prob-
ability for each of the symbol types, we divided the unnormalized probability
by the sum of all the normalized probabilities. This is done to ensure that the
probabilities for the symbol types, px, sum to one.
The dashed vertical lines in Figure 1 indicate the speciﬁc regions, from 0
through 249, from which we draw symbols identiﬁed as common, uncommon,
rare and foreign. Common symbols occur frequently in the training and test
data, whereas foreign symbols do not appear at all in the training data (they
occur as foreign-symbol anomalies in the test data). The two seemingly similar
categories, uncommon and rare, were created in accordance with the design
of the NIDES detector. As described in [1,2], NIDES sets a threshold on the
frequencies of symbols (the parameter “MAXSUMRAREPROB”). Intuitively,
the symbols whose frequencies fall below this threshold are described as “rare.”
To evaluate the detector, we deﬁned the class of rare symbols to represent the
kind of rare symbols that NIDES processes. Deﬁning these rare symbols however,
brought consideration of the symbols occurring in the border region between
rare and common. These “border” symbols were not rare enough to fall into
the detector’s deﬁnition of rare, but neither did they occur frequently enough
to be called common symbols. To accommodate this situation, the “uncommon”
Anomaly Detector Performance Evaluation
115
Uncommon
Foreign
Rare
0
50
100
150
200
250
300
Symbol Type
y
t
i
l
i
b
a
b
o
r
P
e
p
y
T
l
o
b
m
y
S
2.5%
2.0%
1.5%
1.0%
0.5%
0.0%
Fig. 1. Symbol-type distribution
category of symbols was created. Uncommon, rare and foreign symbols are used
as anomalies in the generation of test data. A more detailed description of these
classes can be found in the subsequent section on test-data generation.
Training-Data Generation. Training data is generated using the data distri-
bution described above. First, a sample size for the training data is selected (in
our experiments we chose the sample sizes to be 100,000). Then, a uniformly-
distributed, ﬂoating-point number from 0 to 1 is randomly selected to generate
each element of the given sample size. These numbers are used to “look up”
the appropriate symbol type in the data distribution shown in Figure 1. The
training data are therefore composed of a mixture of common, uncommon and
rare symbols, where each symbol type occurs with the frequency determined by
the data distribution.
Test-Data Generation. Test data are generated in alternating blocks of “nor-
mal” and “anomalous” data. The alternating blocks are intended to test the
ability of the detector to tolerate transitions between normal and anomalous
regions. The “normal” blocks are generated in exactly the same manner as the
training data, using the same data distribution. The parameter B speciﬁes the
number of symbols in each of the alternating normal and anomalous blocks. The
smaller the block size B, the greater the number of transitions that will occur in
a generated data set of a constant number of symbols, in this case 100,000. In
general, we expect that a high number of transitions will have a negative impact
on performance, because detection is always harder in the transition from one
type of block to another (as it would be in any window-based detector). The
“anomalous” blocks are generated by mixing symbols from the data distribution
with speciﬁc quantities (determined by the saturation parameter, λa, deﬁned
below) of symbols generated from one of the following three anomaly types:
– Foreign – Foreign anomalies are comprised of 10 equally-likely symbol types
that are not part of the 250 symbols in the alphabet of the data distribution.
The number 10 was chosen arbitrarily, and is not expected to have any
116
J.P. Hansen, K.M.C. Tan, and R.A. Maxion
eﬀect on the results, because RIDES groups all foreign symbols together
without distinguishing among them. The range of foreign symbols is shown
in Figure 1 between the rightmost pair of vertical dashed lines.
– Rare – Rare anomalies include the symbol types found at the tail of the
data distribution. Rare anomalies are selected in the following way:
1. A threshold deﬁning “rare” is ﬁrst selected. This threshold is simply a
value between 0 and 1, and is referred to as the “maximum sum of rare
probabilities.” Note that this rare threshold is diﬀerent from the rare
threshold “MAXSUMRAREPROB” used by the detector.
2. The symbol type with the lowest probability in the data distribution
is selected as the ﬁrst rare symbol type; then the symbol type with
the next lowest probability is selected, and so forth. The probabilities
associated with these symbols are summed up over the course of this
selection process. Thus the probability of the “rarest” symbol is added
to the probability of the next “rarest” symbol and so on.
3. When the addition of a symbol causes the sum of the probabilities to
exceed the rare threshold (see item 1 immediately above) we stop adding
more symbols into the set of rare anomalies.
In Figure 1, the set of rare anomalous symbols will include symbol types
74 through 249, i.e., the range of symbols between the middle two vertical
dashed lines. 5% was selected as the value for the rare threshold, because it
corresponds to the value of the RIDES parameter “MAXSUMRAREPROB.”
– Uncommon – Uncommon anomalies are comprised of symbols with low
probabilities in the data distribution that have not been added to the rare
set. The “uncommon” set is constructed in a similar manner as the rare set.
First a threshold is set (arbitrarily chosen to be 3%); then the ﬁrst uncommon
anomaly is selected - note that the ﬁrst uncommon anomaly is the ﬁrst
symbol that missed the cut-oﬀ for inclusion into the set of rare anomalies.
Symbols continue to be added to the set of uncommon anomalies until the
sum of their probabilities reaches the threshold of 3%. In Figure 1, the set
of uncommon anomalous symbols will include symbol types 68 through 73 -
the symbols between the leftmost two dashed vertical lines in the ﬁgure.
After we have decided which of the three anomaly types to inject, and how
many of each of them, we then need to generate an anomalous block for that
type. A parameter λa called the “saturation” is used to control the mixing of
the anomalous and normal symbols in the data.
In generating each symbol in the anomalous block, we ﬁrst decide between
generating the symbol from the full data distribution (i.e., the entire curve of
Figure 1) or the region corresponding to the selected anomaly type (uncommon,
rare or foreign). We then choose a speciﬁc symbol from the selected distribution
λa+1. The satu-
or region. The probability of selecting the anomaly region is
ration represents the expected number of symbols drawn from the anomalous
region for each symbol drawn from the data distribution. High saturation values
correspond to a greater concentration of symbols from the selected regions con-
taining anomalous symbols (uncommon, rare and foreign). It is expected that
when the saturation is high, anomalies will be easier to detect.
λa
Anomaly Detector Performance Evaluation
117
Ground truth is established on a per-block basis as opposed to a per-symbol
basis. All symbols within a normal block are marked as being normal, and all
symbols within an anomalous block are marked as being anomalous. We consider
the problem of detecting which speciﬁc symbols in a stream are anomalous to be
too diﬃcult for a detector such as RIDES. This is because RIDES and NIDES,
like most window-based detectors, generate alarms based on aggregate behavior
observed in the window rather than on single observations. Thus, we only require
the detector to determine if a symbol is part of a normal block or an anomalous
block.
5.2 Experimental Strategy
Experimental results (hits, misses and false alarms) were collected for each of
the three diﬀerent types of injections: foreign symbol, rare symbol and uncom-
mon symbol. For each injection type, we measure the performance data for each
combination of parameters shown in Table 2. This is a total of 6,582,816 ex-
periments (3 anomaly types × 12 block-size values × 19 saturation values × 24
short-term half-life values × 401 threshold values). In practice, since we perform
the experiments for each of the 401 threshold levels concurrently, we can collapse
the number of experiments to 16,416 (i.e., 6,582,816 divided by 401). We assume
that MAXSUMRAREPROB is set to 5% in all of our experiments. Note that
since none of the parameters we control aﬀects the training data, we can reuse
the same training data for all of our experiments.
Table 2. Experimental parameters and the values assigned to them in 16,416 exper-
iments: 3 anomaly types × 12 block sizes × 19 saturation points × 24 short-term
half-life points
Parameter
No. Values
Anomaly Type
3 Foreign symbol, rare symbol, uncommon symbol
Block Size(B)
12 10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000
Saturation(λa)
Half-Life(Hst)
19
24
0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1,
0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5
5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80,
85, 90, 95, 100, 125, 150, 175, 200
Threshold(T )
401 0, 0.01, 0.02, . . . , 3.98, 3.99, 4.0
5.3 Evaluation Criterion
To evaluate and tune a detector deployed in a particular environment, there must
be some criterion by which we can determine how well the detector is performing.
In this paper we use cost of detector error as a measure, by comparing detector
results against data with known ground truth. A detector that works perfectly
118
J.P. Hansen, K.M.C. Tan, and R.A. Maxion
will have a cost of zero; the higher the cost, the worse the performance (on a
given data set). Two ways in which the detector cost can be deﬁned are:
– Conditional probability based cost (conditional cost)
CF P
NF P
NF P + NT N
+ CF N
NF N