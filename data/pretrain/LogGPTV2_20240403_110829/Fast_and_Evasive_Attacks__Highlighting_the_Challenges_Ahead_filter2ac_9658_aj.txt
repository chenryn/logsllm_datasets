### Program Exploit Detection and Mimicry Attacks

The ability of a model to detect actual attacks is based on the implicit assumption that attacks always appear different from valid execution. If an attack is accepted by the model as valid, it will not trigger an anomaly and will go undetected (Figure 1). Mimicry and evasion attacks avoid detection by transforming an attack sequence of system calls so that it is accepted by the program model while still carrying out the same malicious action.

Previous research has identified examples of mimicry attacks against high-privilege processes restricted by a model-based detector [24, 21, 22, 20]. These attacks were constructed manually by iterating between an attack sequence and a program model until the attack appeared normal. Although these manually-constructed attacks served as a successful proof-of-concept, manual approaches are not suitable for general attack discovery.

This paper introduces an automated method for discovering mimicry attacks. Our goal is not to propose a new detection system but to provide a means to evaluate an existing program model's ability to detect attacks. We address two primary questions:
1. What attacks does a program model fail to detect?
2. What attacks can we prove that a model will always detect?

Finding undetected attacks reveals the weaknesses of a program model and indicates that a model-based detector provides insufficient security for that particular program. Conversely, proving that a model always detects an attack establishes strong indications that a computer system using model-based detection is secure, even when an attacker attempts to hide an attack within legitimate execution.

An attack is defined as any sequence of system calls that produces a malicious change to the operating system (OS). For a given attack sequence, an attacker can produce variations with the same attack effect by inserting extraneous system calls or replacing existing system calls with alternative sequences having the same effect. A program model that detects one sequence may allow a different, obfuscated sequence. The net result remains the same: the model fails to detect an attack. Therefore, it is essential to verify that a model detects each of the attack variants.

We use a novel formalism that does not require knowledge of specific attack sequences or obfuscations. We develop a model of an OS with respect to its security-critical state and characterize attacks only by their effect on the OS. This leverages a key insight: the commonality among obfuscated attack sequences is that they are semantically equivalent in terms of their malicious effect on the OS. Although we manually produce the OS model and definitions of malicious OS state, this is a one-time effort that can be reused for subsequent analyses of all models of programs executing on that OS.

The program model specifies which sequences of system calls are allowed to execute. By specifying how each system call transforms the OS's state variables, we can compute the set of OS configurations reachable when a program's execution is constrained by the model. We apply model checking [4] to prove that no reachable configuration corresponds to the effect of an attack. If the proof fails, some system call sequence allowed by the model produces the malicious effect. The model checker reports this sequence as a counter-example, providing an undetected attack as output. In terms of Figure 1, we find system call sequences contained in \( L(M) \cap \text{Attacks} \) without explicitly computing the set of malicious system call sequences.

This approach automates the previous manual effort of finding mimicry attacks. In experiments, we show that we can automatically discover the mimicry attack against the Stide [8] model for wu-ftpd [24] and the evasion attacks against the Stide models for passwd, restore, and traceroute [21, 20, 22]. The model checking process completes in about 2 seconds or less when undetected attacks are present in the models. When a model is sufficiently strong to detect an attack, the model checking algorithm reports that no attack sequence could be found, requiring exhaustive search and completing in 75 seconds or less for all detected attacks in the four test programs. Note that proofs of successful detection hold only with respect to our abstraction of the OS state. If this abstraction is erroneous or incomplete, undetected attacks may still be present when using the model to protect a complete operating system.

Our work addresses outstanding problems in model-based anomaly detection. We provide a method for model evaluation that exhaustively searches for sequences of system calls allowed as valid by a program model but that induce a malicious configuration of OS state. Although our current work evaluates the context-insensitive Stide model, our system can evaluate any program model expressible as a context-sensitive pushdown automaton (PDA). One of our long-term goals, not yet realized, is to compare the detection capabilities of different model designs proposed in the literature.

In summary, this paper makes the following contributions:
- **Automated Discovery of Mimicry Attacks:** We use model checking to find sequences of system calls accepted as valid by a program model but that have malicious effects on the operating system. Our system produces the exact sequences of system calls, with arguments, that comprise the undetected attacks.
- **A System Design Where Attack Sequences and Obfuscations Need Not Be Known:** Our system does not require known attack system call sequences or obfuscations. Instead, it automatically finds new, unknown attack sequences accepted by a program model and produces those sequences as output. It also automatically identifies the obfuscations used by attackers to hide attack system calls within a legitimate sequence. As a result, our approach is not limited by a priori knowledge of attacker behavior.

### Related Work

The seminal research on mimicry [24, 9] and evasion attacks [22, 21, 20] demonstrated a critical shortcoming of model-based anomaly detection. Attackers can avoid detection by altering their attacks to appear as a program's normal execution. These altered attacks are sequences of system calls allowed by a program model but still cause malicious execution. Previous work constructed mimicry and evasion attacks by converting some detected attack system call sequence \( A \) into an equivalent undetected sequence \( A' \). If \( A \) and \( A' \) are semantically equivalent and \( A' \) is a sequence allowed by the program model, then \( A' \) is a successful, undetected attack.

Determining that a model expressed as a pushdown automaton accepts \( A' \) is a computable intersection operation provided that \( A' \) is regular. Finding a sequence \( A' \) to intersect is a manual, incomplete procedure with several drawbacks:
- The procedure requires known attack sequences \( A \).
- The equivalence of two system call sequences is not well-defined. For example, an undetected attack sequence \( A' \) may include legitimate execution behavior irrelevant to the original attack sequence \( A \). Are \( A \) and \( A' \) equivalent?
- There is no clear operational direction to find mimicry and evasion attacks automatically. Identifying two sequences as equivalent attacks was a manual procedure based on intuition, with no algorithmic process amenable to automation.

Our model evaluation takes a different approach that advances the state of the art. By defining attacks only by their malicious effects on the system, our work is not restricted to known attack sequences of system calls or known attack transformations producing evasive attacks. Attack sequences are not part of the input to our system; in fact, our work produces the sequences as its output. We can further define two system call sequences as equivalent with respect to the attack if they produce the same malicious effect on the operating system. This formalism provides the operational direction allowing our work to automate the procedure of finding undetected attacks.

Previous attempts have been made to quantify the ability of a model to detect attacks. Average branching factor (ABF) [23] calculates, for any finite-state machine model, the average opportunity for an attacker to undetectably execute a malicious system call during a program's execution. A predefined partitioning divides the set of system calls into "safe" calls and "potentially malicious" calls. As the runtime monitor follows paths through the automaton in response to system calls executed by the program, it looks ahead one transition to determine the number of potentially malicious calls that would be allowed as the next operation. The average branching factor is then the sum of the potentially malicious calls divided by the number of system call operations verified during execution. An extension to average branching factor, called the average reachability measure (ARM) [10], similarly evaluated pushdown automaton models.

Although these measurements provide a convenient numeric score enabling model comparisons, they do not provide a clear measure of a model's ability to actually detect attacks. These metrics do not effectively embody an attacker's abilities:
- An attacker may alter a program's execution to reach a portion of the program model that admits an attack sequence by first passing through a sequence of safe system calls. By only looking at the first system call branching away from a benign execution path, ABF and ARM fail to show the strength of one model over another.
- The ABF or ARM value computed depends upon the benign execution path followed and hence upon program input. A complete evaluation of the model requires computing the score along all possible execution paths, which is extremely challenging and forms an entire body of research in the program testing area.
- Attacks frequently are comprised of a sequence of system calls. The previous metrics look at each system call in isolation and have no way to characterize longer attack sequences.

Consequently, these metrics provide limited insight into a model's ability to detect attacks. Our work improves the evaluation of a program model's attack detection ability by decoupling the evaluation from both a particular execution path and the need to describe malicious activity as unsafe system calls.

MOPS [3] is similar to our work in that it statically checks a program model to determine properties of the model. Unlike our work, however, MOPS characterizes unsafe or attack behavior as regular expressions over system calls and requires users to provide a database of malicious system call patterns. Just as commercial virus scanners syntactically match malicious byte sequences against program code, MOPS syntactically matches unsafe system call sequences against a program model. Likewise, when a new malicious behavior is discovered, the database of system call patterns must be updated. Conversely, by understanding the semantics of system calls, the system in our paper does not require known malicious system call sequences and automatically discovers them for the user. Our work is not tied to known patterns of malicious system call execution.

Model checking is a generic technique used to verify properties of state transition systems and has been applied previously to computer security. Bessen et al. [2] described how model checkers can verify safety properties [16] expressed in linear-time temporal logic (LTL). They verified the properties over annotated control-flow graphs, where both the graph and the annotations expressing security properties of the program code came from some unspecified source. We analyze automatically constructed program models, and our model checking procedure automatically derives security properties of the model as it traverses the model's edges.

Guttman et al. used model checking to find violations of information-flow requirements in SELinux policies [13]. They modeled the SELinux policy enforcement engine and the ways in which information may flow between multiple processes via a file system. They could then verify that any information flow was mediated by a trusted process on the system. Our work has a different goal: verification of safety properties using an OS model where system calls alter OS state.

Ramakrishnan and Sekar [15] used model checking to find vulnerabilities in the interaction of multiple processes. They abstracted the file system and specified each program's execution as a file system transformer. The program specifications were complicated by the need to characterize interprocess communication. Our work expands the system abstraction to include the entire operating system, shifts the checked interface from coarse-grained process execution down to system calls, and has no need to model communication channels between processes.

Walker et al. used formal proof techniques to verify properties of a specification of a UNIX security kernel [25]. This work is notable because the authors rigorously proved that the abstract specification of the kernel matched the actual implementation. As a result, properties proved using the abstraction also hold true in the real operating system. Due to the difficulty of producing proofs of correct specifications, little other research actually demonstrates that abstractions are accurate. We adopt a simpler approach: we produced our operating system abstraction manually and have not proved it correct. As a result, discovered attacks or proofs of the absence of attacks hold only with respect to the abstraction. A discovered attack can be validated by actually running the system call sequence against a sandboxed operating system. Conversely, if we do not find any attack, this provides good indication that the program model is secure, even though this is not provably true in the real operating system.

### Overview

We provide here an overview of model-based anomaly detection, including the attacker threats addressed, context-sensitive program models, and the purpose of attack discovery.

#### Threat Model

Our system automatically constructs undetected attack sequences possible within a particular threat model. This threat model is simple and strong:

Let \( \Sigma \) be the set of system calls invoking kernel operations. If program \( P \) is under attacker control, then \( P \) can generate any sequence of system calls \( A \in \Sigma^* \).

Attackers may subvert a vulnerable program's execution at any execution point, including the point of process initialization. Attackers can then arbitrarily alter the code and data of the program and even replace the program's entire memory image with an image of their choosing. Alternatively, the attacker could replace the disk image of a program with, for example, a trojan before the OS loads the program for execution. The attacker can generate any sequence of system calls and system call arguments, and the operating system will execute the calls with the privilege of the original program.

This threat model matches real-world attacks. In remote execution environments, programs execute on remote, untrusted machines but send a sequence of remote system calls back to a trusted machine for execution. An attacker controlling the remote host can arbitrarily alter or replace the remote program. The attacker's program image can then send malicious system calls back to the trusted machine for execution [11].

Common network-based attacks against server programs have a more restrictive threat model. Attackers can subvert execution only at points of particular program vulnerabilities and face greater restrictions in the attack code they can execute. As a result, if our system proves that a program model detects an attack in the strong threat model, it will also detect the attack in a more restrictive model. However, successful attacks discovered by our system are specific to the strong threat model. Although the program model would fail to detect the attack sequence even in the restricted threat model, a restricted attacker may be unable to cause the program to execute that attack. Our system currently does not make this determination and will report all attacks discovered in the strong threat model.

Consider the example in Figure 2. This is a vulnerable program that reads command characters and filenames from user input. This input may come from the network if the program is launched by a network services wrapper daemon such as xinetd. The command-code and argument input resemble the usage of programs such as FTP servers or HTTP servers. Suppose that the program is executed with stored but inactive privilege: its real and effective user IDs are a low-privilege user, but the saved user ID is root. If the input contains the command character 'x', the program drops all of its saved privilege and executes a filename given in the input. If the input contains the command character 'e', the program echoes the contents of a specified file to its output, which may be a network stream.

In our threat model, an attacker can arbitrarily alter the execution of this program. Perhaps the attacker exploits the vulnerable `gets` call, or perhaps they use an attack vector that we have not considered. The attacker can cause the program to execute any system call, including system calls not contained in the original program code. The role of host-based intrusion detection is to detect any such subverted program execution.

#### Program Model

Readers familiar with pushdown automaton (PDA) models may elect to bypass this section, as it presents background material and standard notation previously used for model-based anomaly detection.