deteriorate the test accuracy of VGG-11 to 10.94 % from
90.38 %, Deep-Dup requires 77 attacks. Similarly, for tar-
geted attack on CIFAR-10, the attacker requires only 14 and
63 attacks to achieve close to 99.0 % ASR on ResNet-20
and VGG-11 respectively. Clearly, VGG-11 is more robust
to Deep-Dup attack. We provide the detailed analysis of this
phenomenon in sec.8.
For ImageNet dataset, our attack succeeds in degrading the
test accuracy of MobileNetV2 to 0.19 % from 70.79 % with
just one single attack. Even for the targeted attack, it only
requires one attack to achieve 100 % ASR in miss-classifying
all Lesser Panda images. Again, MobileNetV2 is also found to
be extremely vulnerable by previous adversarial weight attack
[28] as only a single bit memory error can cause catastrophic
output performance. Nevertheless, MobileNet is an efﬁcient
and compact architecture ideal for mobile and edge computing
platforms like FPGA [98]. Thus the vulnerability of these
compact architectures against Deep-Dup raises a fair question
of how secure are these DNN models in cloud FPGA? The
answer from our Deep-Dup attack is a big NO. Our attack also
succeeds in all ResNet families. Also, larger DNN models
(e.g., ResNet-18 & ResNet-50) shows better resistance to
Deep-Dup attack.
7.3 Black-Box Attack Results
For proof of concept of our proposed Deep-Dup black-box
framework shown in Fig. 7, in this section, we demonstrate
and validate the black-box attack on Resnet-50 for image
classiﬁcation task and YOLOv2 for the object detection task.
Black-Box Un-Targeted Attack on YOLOv2 using LRO cell
Target Class (ts) mAP
Post- Attack mAP
# of Attacks
All
0.428
0.14
63
Black-Box Targeted Attack on YOLOv2 using RO cell
Target Class (ts)
AP
Post-Attack AP
# of Attacks
Person
Car
Bowl
Sandwich
0.6039
0.5108
0.3290
0.4063
0.0507
0.0621
0.0348
0.0125
20
18
15
6
Specially, in our case study, we randomly pick the "ostrich"
class in the Imagnet dataset as a target class for ResNet-50 and
4 target objects (i.e. Person, Car, Bowl and Sandwich) in the
COCO dataset for YOLOv2. Other settings and performance
metrics are the same as described in Sec. 7.2. Note that, all the
black-box results are the actual measurement from our FPGA
prototype. The Deep-Dup black-box attack on ResNet-50 are
successful and results are reported in Tab. 2. It can be seen
that only 26 attacks are needed to attack the “ostrich” with
100 % ASR. Similarly, Deep-Dup black-box un-targeted and
targeted attacks on YOLOv2, with both RO and LRO cells, are
also successful, as reported in Tab. 3. It can be seen that the
post-attack average precision (AP) is signiﬁcantly degraded
after less than 20 attacks. For example, only 6 attacks are
needed to decrease the AP of sandwich class from 0.4063 to
0.0125.
7.4 Comparison to Other Methods
Previously, very few adversarial weight attack works have
been successful in attacking DNN model parameters to cause
complete malfunction at the output [26, 29]. Thus we only
compare with the most recent and successful adversarial bit-
ﬂip (BFA) based weight attack [27,28], which uses a gradient-
based search algorithm to degrade DNN performance in a
white-box setting. We also compare our search algorithm
(P-DES) to a random AWD attack.
USENIX Association
30th USENIX Security Symposium    1929
Table 4: Comparison of Deep-Dup with random AWD attack
and row-hammer based (BFA [27, 28]) attack. All the results
are presented for 8-bit quantized VGG-11 model [27].
Method
Threat
Model
Random
BFA [28]
Deep-Dup Black & White Box
Black Box
White Box
TA
(%)
90.23
90.23
90.23
Post-Attack TA (%)
# of Attacks
90.04
10.8
10.94
100
28
77
As shown in both Tab. 4 , only 77 AWD attack iterations
can degrade the accuracy of VGG-11 to 10.87 % while ran-
domly performing 100 AWD attacks, cannot even degrade
the model accuracy beyond 90 %. On the other hand, a BFA
attack [28] using row-hammer based memory fault injection
technique, requires only 28 attacks (i.e. memory bit-ﬂips) to
achieve the same un-targeted attack success (i.e., ∼ 10 % TA).
However, BFA attack is only successful for white-box setting,
not black-box.
7.5 Discussion
Attack efﬁciency w.r.t. fault injection success rate. As
described in section 7.1, we used two different power plun-
dering circuits, i.e., RO and LRO for fault injection. In our
experiments, we measured 84.84% and 58.91% fault injection
success rates for RO and LRO, respectively. In practical attack,
this number may vary due to the attack budget (i.e., frequency,
resource, etc.). In order to validate our Deep-Dup attack frame-
work will succeed in different fault injection success rates,
we incorporate the fault success rate as a probabilistic param-
eter in our off-line simulator as discussed in section 5.3.1.
Note that, for black-box attack, our direct evaluation of ﬁt-
ness function in the FPGA accelerator already considers and
compensates for the failed fault iteration. The experimental
results are shown in Tab.5. We observe that our Deep-Dup at-
tack framework could still succeed at very low fault injection
success rate (i.e., 40 %), but requiring more number of attack
iterations (i.e. higher redundancy as explained in sec.5.3).
Table 5: Attack efﬁciency v.s. fault injection success rate ( f p).
Reporting # of attack iterations (i.e., mean ± std. for three
runs) required to achieve 99.0 % ASR (targeted attack) or
11.0 % test accuracy (un-targeted attack).
Model
Type
40 %
60 %
80 %
ResNet-20
Un-Targeted
95.3 ± 37.3
Targeted
39 ± 7.8
88 ± 66.5
23.3 ± 4.3
76.6 ± 13.8
23.8 ± 6.8
VGG-11
Un-Targeted
195.3 ± 39.1
Targeted
114 ± 32
95.6 ± 14.1
88.6 ± 34.4
98.9 ± 1.9
62.6 ± 2.6
Attack Time Cost. The execution time of one searching
iteration of our proposed P-DES algorithm is constant for a
ﬁxed z, regardless of DNN model size. The overall search-
ing time is proportional to the number of evolution (z). For
Deep-Dup white-box attack, the P-DES algorithm is executed
ofﬂine, and the AWD attack is only executed when the attack
index is generated. Note that, the hardware AWD attack in-
curs no time cost, as it runs in parallel with the victim DNN
Figure 9: Black-Box attack time cost analysis with z = 100.
FPGA acceleration (i.e., ﬁtness function evaluation) time and
mutation generation time are reported.
model. For Deep-Dup black-box attack, two main time cost
includes mutation generation (proportional to z) and FPGA
ﬁtness function evaluation (proportional to DNN acceleration
performance/latency in FPGA). In Fig. 9, we report the aver-
age time cost of the proposed 4 mutation strategies executed
in the PS of our FPGA prototype. Additionally, we also report
the DNN execution time in FPGA, which is determined by the
corresponding DNN model size, architecture, optimization
method, and available FPGA hardware resources. It is easy to
observe that our P-DES mutation generation only consumes
trivial time compared to DNN execution time in FPGA, which
is the bottleneck in black-box attack.
8 Potential Defense Analysis
Increasing Model Redundancy. Several prior works have
demonstrated that increasing model redundancy (i.e., DNN
size/channel width) [89, 99] can be a potential defense against
model fault attack. Our evaluation of Deep-Dup attack in
the previous section also indicates the correlation between
network capacity (i.e., # of model parameters) and model
robustness (# of attacks required). As the ImageNet dataset
section depicts in Tab. 1, as the network size increases from
ResNet-18 to ResNet-50, the number of attacks required to
achieve 100 % ASR increases correspondingly. We observe
the same trend for CIFAR-10 models where VGG-11 (i.e.,
dense model) requires a higher number of attacks than ResNet-
20 (i.e., compact model).
Table 6: Attack efﬁciency after increasing the model size of
ResNet-20 and VGG-11 model by 4 (i.e., increasing each
input and output channel size by 2).
Method
ASR(%)
# of Attacks
ResNet-20 (Baseline)
ResNet-20 × 4
VGG-11 (Baseline)
VGG-11 × 4
99.6
99.6
98.6
98.2
14
21
63
84
In Tab. 6, we run an experiment to validate the relation
between Deep-Dup attack efﬁciency and network model size.
First, we multiply the input and output channel of the baseline
model by 2 to generate ResNet-20 (× 4) and VGG-11 (×
4) models with 4 × larger capacity. For both ResNet-20 and
VGG-11, the number of attacks required to achieve similar
ASR increases with increasing model capacity (Tab. 6). To
conclude, one possible direction to improve the DNN model’s
resistance to the Deep-Dup attack is to use a dense model
with a larger redundancy.
Protecting Critical Layers. Another possible defense di-
rection is to protect the critical layers that are more sensi-
1930    30th USENIX Security Symposium
USENIX Association
TaskNetworkModel quantization Training setMutation generatetime (ms)FPGA acceleration time (ms/image)ClassificationResNet-508-bitsImageNet16.0175588Object detection YOLO-V216-bitsCOCO15.075914tive. Prior works [100] have proposed selective hardening to
defend against weight faults by selectively protecting more
sensitive layers. It is interesting to note that our experimental
observation also shows that 80 % of the searched vulnerable
weights are within the ﬁrst two layers and the last layer for
ResNet-20. Following this observation, in Tab. 7, we run our
attack by securing these three sensitive layers (ResNet-20
(Protected)). A straightforward way to secure layer weights
from Deep-Dup would be to store them on-chip (i.e., no need
for off-chip data transfer). Note that, a defender can not store
an entire DNN model on-chip due to limited on-chip mem-
ory and typically large DNN model size for cloud computing.
Nevertheless, as shown in Tab. 7, our Deep-Dup still manages
to succeed with ∼ 2 × additional rounds of attack on the
protected ResNet-20 model. Similarly for VGG-11, our Deep-
Dup attack still successfully achieves ∼ 99.0 % ASR even
after securing some critical DNN layers from fault attacks.
Table 7: Deep-Dup attack performance after protecting or
securing some critical DNN layers
Method
ASR(%)
# of Attacks
ResNet-20 (Baseline)
ResNet-20 (Protected)
VGG-11 (Baseline)
VGG-11(Protected)
99.6
99.2
98.6
98.2
14
29
63
141
Obfuscation through Weight Package Randomization.
In our Deep-Dup attack, the P-DES algorithm relies on the
sequence (e.g., index) of the weight packages being trans-
ferred between the on-chip buffer and off-chip memory. In
this section, we discuss the possibility of defending our attack
by introducing random weight package transmission as an
obfuscation scheme. In Tab. 8, we ﬁrst perform an experiment
with shufﬂing of the weights in a pre-deﬁned sequence before
transmitting them. The results show that pre-deﬁned shufﬂing
order of the wights has almost no effect on the attack efﬁcacy.
Table 8: Weight package randomization as obfuscation. Pre-
deﬁned Shufﬂe : Shufﬂing the weight packages in a pre-
deﬁned order before transmission. Random Shufﬂe : Shuf-
ﬂing the weight packages every time using a random function
before transmission.
Method
TA (%)
Post-Attack
# of
TA (%)
Attacks