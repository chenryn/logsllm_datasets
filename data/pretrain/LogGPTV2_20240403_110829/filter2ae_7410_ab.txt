简单的量化操作，从图像中去除像素值的小扰动；压缩也是也类似的方法去除扰动。
第二类是总方差最小化(total variance
minimization)。该方法随机选择一小组像素，并重建与所选像素一致的最简单的图像，重建后的图像不包含对抗扰动，因为这些扰动往往较小并且是局部的。首先通过对每个像素位置(i,
j, k)采样一个伯努利随机变量X(i, j, k)来选择一个随机像素集;当X(i, j, k) =
1时，我们保持一个像素。接下来，我们使用总变差最小化来构造一个图像z，它类似于(扰动的)输入图像x，用于选定的像素集合，同时在总变差方面也是最简单的，通过求解下式即可
上式中的TVp代表总方差，通过下式计算
通过TV的最小化就可以去除小扰动。
第三类方法是图像缝合(image
quilting).该方案通过构建一个只包含来自“干净”图像(没有对抗扰动)的补丁数据库来去除对抗性扰动;合成图像所用的补丁是通过从补丁数据库中的对抗图像中找到对应补丁的K个最近邻(像素空间)，然后均匀随机地选取其中一个近邻来选择的。这种防御方案的背后的思想是，生成的图像只包含没有被攻击者修改过的像素，因为真实补丁的数据库不太可能包含出现在攻击者图像中的结构。部分实验结果如下
图中直观比较了5中处理方案与没有防御的情况的对比，可以看到，提出的5种防御方案的准确率都比没有防御时要高，此外，基于裁减-缩放的方案防御效果最好。
[7]在待预测样本输入到原模型之前增加额外随机化层(包括随机调整大小和随机填充(以随机方式在输入图像周围填充0))，再用原模型预测，处理流程如下所示
输入的图像首先经过随机调整大小，然后在随机调整大小基础上进行随机填充，让模型对填充后的样本进行预测。部分实验结果如下
可以看到随机化层有效地减轻了所有对抗攻击，而且将其与对抗训练联合起来使用时，防御效果更好。
[8]设计了PixelDefend，通过将对抗样本向着训练集中呈现的分布移动，从而“净化”图像。
该方案背后的思想是对输入进行转换，通过对输入样本进行微小变换，可以将它们移回训练数据正常的分布，也就是将图像向高概率的区域移动。该过程可由下式表示
其中p是训练集IDE分布，输入样本为X，我们希望得到处理后的样本为X _，这里的约束Edefend，是用于trade-off的，如果值过大，则X\_
可能与X的语义不同，而值较小，则可能不足以回到正常的分布。研究人员使用PixelCNN的分布pCNN(X)来近似p(X).部分实验结果如下
该表是在Fashion_MNIST数据集上实验时记录的。表中上半部分是基线防御方案的效果，下半不部分是本文的工作的效果，单元格中的格式x/y,其中x表示约束为8时，对图像进行攻击的准确率，y表示约束为5时的准确率。每一列中黑体标出的是最好的情况，从表中可以看出，该方案的效果在大多数情况下都是最优的。
同属于该类的方案还有：[17]在分类之前将位图输入图像转换为矢量图像空间并返回，以避免被对抗结构所欺骗；[9]将连续的输入样本使用温度计编码进行离散化，该方案在训练和预测阶段都使用温度计编码后的样本，限于篇幅不再展开。
**输入检测**
[12]假设对抗样本不在非对抗数据流形中，在此情况下提出核密度法和贝叶斯不确定性估计2种对抗性检测方法。我们知道对抗样本可以让人类正确分类，但是模型却会误分类，我们可以从训练数据的流形的角度来理解这个现象。图像被认为位于低维流形上，有研究表明通过遍历数据流形，可以改变图像的真实类别。这背后的关键思想是，对抗扰动(对于输入没有有意义的改变)需要将样本从数据流形中推出去，对抗样本位于数据子流形边缘的类边界附近，基于这些研究，我们认为对抗样本不存在于数据流形上。假设原样本为x，其类别为cx,生成得到的对抗样本为x
_,x\_ 不在流形中，其类别为cx*。
在二分类情况下，简化的示意图如下，图中对抗样本x*,被从“-”推出，黑色的虚线是决策边界，此时可能有三种情况：
在a中，x离”+”子流形较远；在b中，“+”子流形有一个pocket，x\位于其中；在c中，x*离两个子流形的边界和决策边界都很近。
研究人员在最后一层隐层的特征空间中进行核密度估计，从而对每一类的子流形进行建模，密度估计的公式如下，其可以用于衡量x距离t子流形还有多远
而对于预测类为t的点x，其密度估计定义为：
式中的φ是点x的最后一个隐层的激活向量，Xt是类t的训练集数据点的集合。
对于MNIST数据集，BIM对抗攻击而言，我们画出密度估计如下
从图中可以看到，对原类的估计在下降(蓝色)，对错误类的估计在上升(红色），而交叉点基本在预测类改变的附近(黑线)。所以该方法确实可以检测对抗样本，但是使用密度估计的方法可以仅可以轻易检测出原理cx*子流形的对抗样本，所以引出了第二种方法—贝叶斯不确定性估计，其可以通过下式进行计算
下图比较了对抗样本与正常样本以及噪声样本的贝叶斯不确定分布，对BIM和JSMA都做了实验，从下图的结果可以看到，对抗样本的不确定分布在统计上是不同于其他两类样本的，所以可以通过该方案进行检测。
该类型的其他方案还包括：[13]使用ADN作为二元检测器网络以区分正常样本和对抗样本，[10]利用对抗样本的LID（局部本征维数）远大于正常样本这一性质来区分对抗样本与正常样本.
## 总结
对抗防御的工作远不止本文介绍的这些，限于篇幅，本文仅介绍部分经典工作，不过本文基于防御方案的原理对已有的工作做了合理的划分，如果对某一子方向的感兴趣的话完全可以深入研究。  
由本文归纳分析的方案，我们可以看到，基于对抗训练的防御不仅需要大量的正常样本，而且需要大量的对抗样本，极大增加了训练的时间和所需资源，所以该方案很难在实际大规模数据的场景下应用；此外，该方案只能加入由已知攻击技术产生的对抗样本，所以实际上只能在面对同类型攻击时防御才有效，对其他攻击产生的对抗样本不具有鲁棒性。而基于梯度隐藏的方案需要改变模型结构并重新训练分类器，复杂度高。与此同时该方案很容易被绕过，一个简单的方法就是训练一个与被防御的模型相似的替代模型，使用替代模型的梯度来构造对抗样本，从而绕过基于梯度的防御。基于输入转换的防御需要对输入样本进行转换，但是转换之前必须进行检测，对疑似对抗样本的输入样本采取转换措施，不过该方案的FP,TN都较大。基于检测的防御仅限于检测，检测之后如何处理没没有统一的较好的方法。
对抗样本的防御之路任重而道远。
## 参考
1.towards deep learning models resistant to adversarial attacks
2.Ensemble adversarial training:attack and defenses
3.towards deep neural network architectures robust to adversarial examples
4.distillation as a defense to adversarial perturbations against deep neural
networks
5.Improving the Adversarial Robustness and Interpretability of Deep Neural
Networks by Regularizing their Input Gradients
6.Countering Adversarial Images using Input Transformations
7.Mitigating adversarial effects through randomization
8.PixelDefend: Leveraging Generative Models to Understand and Defend against
Adversarial Examples
9.Thermometer Encoding: One Hot Way To Resist Adversarial Examples
10.Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality
11.Defense against adversarial attacks using high-level representation guided
denoise
12.Detecting adversarial samples from artifacts
13.On Detecting Adversarial Perturbations
14.Intriguing properties of neural networks
15.gradient adversarial training of neural networks
16.Generative adversarial trainer: defense to adversarial perturbations with
GAN