In our experimental evaluation of TeD, using the proposed signature
ğ‘†Î” may not lead to a good separation between trapdoored samples
and benign samples in the category protected by the trapdoor.
This is because they may have similar values on some neurons. To
increase the detection sensitivity, we have tried several ways to
increase the separation of trapdoored samples and benign target
samples: ordering neurons according to the separation and selecting
a subset of neurons with top separation values, projecting ğ‘†Î” to the
expectation of benign samples in ğ¶ğ‘¡, etc. We have found that the
projection method produces the best detection results. We call this
TeD variant as Projection-based Trapdoor-enabled Detection (P-TeD).
The detail for using a single trapdoor will be described. The cases
using multiple trapdoors and sampling neurons can be done in a
similar manner.
When a single trapdoor is used to protect a target category ğ¶ğ‘¡,
we calculate trapdoor signature ğ‘†Î” with Eq. 1 and the expectation
of feature representations of benign samples in ğ¶ğ‘¡ as the benign
signature:
ğ‘†ğ¶ğ‘¡ = Eğ‘¥âˆˆğ¶ğ‘¡ Fğ¿(ğ‘¥).
(2)
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3163Then we remove the projection of ğ‘†Î” on ğ‘†ğ¶ğ‘¡ from ğ‘†Î” to get its
component perpendicular to ğ‘†ğ¶ğ‘¡ :
Î” = ğ‘†Î” âˆ’ ğ‘†Î” Â· ğ‘†ğ¶ğ‘¡
ğ‘†âŠ¥
(cid:107)ğ‘†ğ¶ğ‘¡ (cid:107)2
2
ğ‘†ğ¶ğ‘¡ ,
(3)
where ğ´ Â· ğµ means the inner product of ğ´ and ğµ. ğ‘†âŠ¥
Î” is used as the
signature to detect adversarial examples in the same way as ğ‘†Î” is
used in TeD.
4 FIA OVERVIEW
4.1 Threat Model
We assume that the trapdoored DNN model is white-box while
the trapdoored defense is black-box. More specically, we assume
that adversaries have full access to the trapdoored model, including
its architecture and internal parameter values, and are aware of
protection by the trapdoored defense but have no knowledge of its
characteristics (i.e., the number of trapdoors, trapdoor signatures,
detection layer, etc.). The trapdoored defense behaves like an ora-
cle to adversaries: when an example is received, it responds with
the detection result, either positive or negative, that the detector
has determined. No information on the distance to any trapdoor
signature is provided.
In addition, we assume that adversaries have no access to any
training data or backdoor triggers used to train a trapdoored model
and cannot apply any reverse-engineering technique such as Neural
Cleanse [59] to estimate the number of trapdoors, backdoor triggers,
or trapdoor signatures used in the defense, either due to unknown
characteristics of the trapdoored defense or other reasons. This
restriction of no access to any reverse-engineering technique en-
sures that the white-box access to the trapdoored model can only be
used to search for adversarial examples like conventional white-box
adversarial attacks. It cannot be used to derive the characteristics
of the trapdoored defense.
On the other hand, we assume that adversaries have access to a
small set of benign data independent of the training data used in
training a trapdoored model.
4.2 Intuition behind FIA
TeD relies on the following two key factors to work: one is that
searching for targeted adversarial examples is likely trapped into
shortcuts created by trapdoors of a trapdoored model, and the other
is that trapdoor signatures should be signicantly dierent from
those of benign examples of the target category. The former en-
sures that adversarial examples are likely detected by the trapdoor
signatures, while the latter ensures a low false positive rate, which
is critical to deploying any defense in real-world applications. In
addition, along its forward path, a DNN such as CNN extracts fea-
tures from low-level to high-level, and distinguishability of dierent
categories in the feature space gradually improves. For good de-
tectability, the detection layer ğ¿ of TeD should be close to the end
of the forward pipeline, usually the penultimate layer.
To circumvent the trapdoored detection, we aim to make adver-
sarial examples indistinguishable from benign target examples in
the feature space. Once an adversarial example is indistinguishable
from benign target examples at an appropriate latent layer, it is un-
likely detectable by the trapdoored defense unless the trapdoored
defense is impractical with a high false positive rate for benign
target samples. To achieve this goal, we need to craft adversarial ex-
amples with a loss function that can drive adversarial examples into
the distribution of benign target samples in the feature space. The
conventional loss function used in crafting adversarial examples is
the cross entropy, which has no control on the feature vector of a
generated adversarial example at a given latent layer. Fortunately,
Inkawhich et al. [29â€“31] proposed recently to use a loss function in
the feature space to craft more transferable adversarial examples
so that adversarial examples generated on a white-box model can
be transferred to attack a black-box model. We can adopt their
approach to circumvent the trapdoored detection.
Another obstacle we need to overcome is to nd a metric to
measure distinguishability of an example from a set of examples in
the feature space. If we assume that the trapdoored model and the
trapdoored detection are both stable, we expect that an example
with its feature vector located within a small dense region of benign
target examples at an appropriate latent layer is likely classied
and detected like its nearby benign target examples. This stability is
generally required for a practical DNN model with good behaviors.
With this assumption, we can simply nd a target inside the dense
region, minimize the distance to the target in the feature space, and
check if such a distance is within an appropriate threshold at the
end of crafting. This fullls the basic scheme of our proposed FIA.
Adversarial examples generated with the basic scheme may still
be detectable due to mismatch between the generation layer and
the the detection layer, irregular undetectable boundaries of the
trapdoored detection, over-simplied modeling of indistinguishabil-
ity, etc. FIA contains a preparation phase wherein the basic scheme
is used to generate a small number of adversarial examples to query
the trapdoored detection for determining an appropriate genera-
tion layer and other generation parameters. It also adds a loss term
to the basic scheme to maximize distances to detected adversarial
examples in the preparation phase to avoid regions where trapdoor
signatures may be located.
5 FEATURE-INDISTINGUISHABLE ATTACK
Our proposed Feature-Indistinguishable Attack (FIA) is described
in detail in this section. In addition to the basic scheme and the
complete scheme, we also present a variant of FIA.
5.1 Basic Scheme
5.1.1 Optimization Problem. For input ğ‘¥ âˆˆ X that is not in target
category ğ¶ğ‘¡, ğ‘¥ âˆ‰ ğ¶ğ‘¡, and a latent representation distribution Dğ‘¡
of target category ğ¶ğ‘¡ at a selected latent layer ğ¿, called generation
layer, the basic scheme of our proposed FIA minimizes the distance
of the latent representation Fğ¿(ğ‘¥ + ğœ–) of ğ‘¥ + ğœ– at layer ğ¿ to a target
representation F ğ‘¡ğ‘”ğ‘¡
âˆˆ Dğ‘¡ on the condition that Fğ¿(ğ‘¥ + ğœ–) is within
the latent representation distribution Dğ‘¡ of target category ğ¶ğ‘¡:
ğ¿
minğœ– D(Fğ¿(ğ‘¥ + ğœ–), F ğ‘¡ğ‘”ğ‘¡
s.t. Fğ¿(ğ‘¥ + ğœ–) âˆˆ Dğ‘¡
ğ¿
),
(4)
where D is a distance function. The constraint in Eq. 4 ensures that
a generated adversarial example is indistinguishable from benign
examples in target category ğ¶ğ‘¡.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3164ğ¿
ğ¿
minğœ–
(cid:107)2},
FIA uses two distance loss functions in Eq. 4 to minimize both
distances simultaneously. One is an ğ¿2 distance like that in Acti-
vation Attack [31]. Its goal is to drive adversarial examples into
target category ğ¶ğ‘¡. The other is the cosine similarity with the target
representation. Its goal is to ensure that an adversarial example
has a feature vector along a direction similar to that of the target
representation so that TeD, which relies on cosine similarity with
trapdoor signatures in the feature space, unlikely detects it. The
optimization problem becomes:
{âˆ’ cos(Fğ¿(ğ‘¥ + ğœ–), F ğ‘¡ğ‘”ğ‘¡
) + ğœ† Â· (cid:107)Fğ¿(ğ‘¥ + ğœ–) âˆ’ F ğ‘¡ğ‘”ğ‘¡
s.t. Fğ¿(ğ‘¥ + ğœ–) âˆˆ Dğ‘¡
(5)
where ğœ† â‰¥ 0 is a weighting parameter. Note that there is a negative
sign before the cosine similarity since we want to maximize the
cosine similarity with the target representation.
5.1.2 Basic Scheme. To ensure the constraint in Eq. 5, we need
to estimate the feature-representation distribution Dğ‘¡ of target
category ğ¶ğ‘¡. Although it is possible to use neural networks to model
the feature-representation distribution of target category ğ¶ğ‘¡ like
in [29], FIA adopts a less accurate but much simpler approach: we
assume that feature representations of benign examples in target
category ğ¶ğ‘¡ can form a convex region1. With this assumption, we
can choose the expectation of feature representations of benign
examples in ğ¶ğ‘¡, F ğ¶ğ‘¡
ğ¿ , as the target representation F ğ‘¡ğ‘”ğ‘¡
F ğ‘¡ğ‘”ğ‘¡
ğ¿ â‰¡ Eğ‘¥âˆˆğ¶ğ‘¡ Fğ¿(ğ‘¥),
(6)
where E(Â·) is the expectation function, and use the cosine similarity
distribution of benign examples in ğ¶ğ‘¡ with F ğ¶ğ‘¡
as an approximation
of distribution Dğ‘¡.
= F ğ¶ğ‘¡
Since expectation is sensitive to outliers, we determine and re-
move outliers with DBSCAN [18] before calculating the expectation
in Eq. 6: a certain percentage (10% in our evaluation) of benign tar-
get samples located in the lowest-density regions in the feature
space are considered as outliers and removed. We then calculate
a threshold ğ‘ğ‘ to be the smallest cosine similarity between the ex-
pectation and survived benign target samples. We require that the
cosine similarity of an adversarial example with the expectation
F ğ¶ğ‘¡
is within ğ‘ğ‘.
With the above assumption and simplication, Eq. 5 becomes:
:
ğ¿
ğ¿
ğ¿
ğ¿
{âˆ’ cos(Fğ¿(ğ‘¥ + ğœ–), F ğ¶ğ‘¡
ğ¿ ) + ğœ† Â· (cid:107)Fğ¿(ğ‘¥ + ğœ–) âˆ’ F ğ¶ğ‘¡
ğ¿ (cid:107)2},
minğœ–
s.t. cos(Fğ¿(ğ‘¥ + ğœ–), F ğ¶ğ‘¡
ğ¿ ) â‰¥ ğ‘ğ‘
(7)
5.1.3 Adaptive Iteration. Since the constraint in Eq. 7 is on the rst
loss term in Eq. 7, we can apply the following adaptive iteration
to solve Eq. 7: We start to drive only the rst loss term by setting
ğœ† = 0 until the constraint is met. We then activate ğœ† by setting
it to a non-zero value (1 in our evaluation) to minimize both loss
terms simultaneously to drive ğ‘¥ + ğœ– to target category ğ¶ğ‘¡ while
maintaining satisfying the constraint. When ğ‘¥ + ğœ– is classied into
ğ¶ğ‘¡ and its softmax probability is larger than the next maximum
softmax probability by a specic threshold, we return to drive only
the rst loss term again by setting ğœ† = 0 as long as the softmax
probability gap is maintained, otherwise we activate ğœ† to drive both
1This assumption is generally over-simplied for a DNN model, esp. when a middle
latent layer is used as the generation layer. See Section 6.5 for more information.
loss terms. The softmax gap is used to ensure that a generated
adversarial example is robustly classied into target category ğ¶ğ‘¡.
If we place an ğ¿âˆ bound ğ›¿ on adversarial perturbation ğœ–, Eq. 7
can be solved with a PGD-like iterative process with the above
adaptive method:
ğ‘¥0 = ğ‘¥,
ğ¿ (cid:107)2
ğ¿ )))
ğ¿ ) + ğœ† Â· (cid:107)Fğ¿(ğ‘¥) âˆ’ F ğ¶ğ‘¡
ğ¿ )) = âˆ’ cos(Fğ¿(ğ‘¥), F ğ¶ğ‘¡
ğ‘¥ğ‘›+1 = ğ¶ğ‘™ğ‘–ğ‘ğ›¿(ğ‘¥ğ‘› âˆ’ ğœ‚ Â· ğ‘ ğ‘–ğ‘”ğ‘›(âˆ‡ğ‘¥ â„“(Fğ¿(ğ‘¥ğ‘›), F ğ¶ğ‘¡
(8)
ğ¿ )) is the loss function of the basic scheme:
where â„“(Fğ¿(ğ‘¥ğ‘›), F ğ¶ğ‘¡
â„“(Fğ¿(ğ‘¥), F ğ¶ğ‘¡
(9)
The iterative process can stop early if ğ‘¥ğ‘› is classied into target
category, ğ‘¥ğ‘› âˆˆ ğ¶ğ‘¡, and cos(Fğ¿(ğ‘¥ğ‘›), F ğ¶ğ‘¡
ğ¿ ) â‰¥ ğ‘ğ‘. Otherwise a pre-
set number of steps is executed before it stops. In our empirical
evaluation, stopping early is used for the basic scheme used in the
preparation phase (see Section 5.2.1), and a xed number of steps is
executed for the complete scheme to generate adversarial examples.
By the end of the iterative process, the generation of an adversar-
ial example is said successful if the resulting example is classied
into target category ğ¶ğ‘¡ by the model and the constraint in Eq. 7 is
satised. Otherwise the generation is a failure.
5.2 Complete Scheme
Adversarial examples generated with the basic scheme may still
fail to circumvent the trapdoored detection due to several reasons:
mismatch between the generation layer and the unknown detec-
tion layer of TeD, irregular undetectable boundaries of the trap-
doored defense, over-simplied convex-region assumption for the
feature-representation distribution of the target category and the
subsequent simplication from Eq. 5 to Eq. 7.
The second reason can be explained as follows. Suppose the gen-
eration layer and the detection layer are the same latent layer. Since
TeD uses cosine similarity with trapdoor signatures to detect ad-
versarial examples, examples with the same cosine similarity value
with the expectation vector may have dierent detectability: those
close to the trapdoor signatures have a higher chance to be detected
than those far away from the trapdoor signatures. Unfortunately,
the basic scheme treats these examples identically.
To address these issues, we enhance the basic scheme with two
additions. One addition is a preparation phase to use the basic
scheme to generate some adversarial examples to query the trap-
doored defense to determine an appropriate generation layer and
other generation parameters. The other addition is an additional
term in the loss function to direct adversarial examples away from
bad regions where detected adversarial examples in the prepara-
tion phase stay. These positive adversarial examples are a rough
estimate of trapdoor signatures2.
5.2.1 Preparation Phase. In this phase, we use the basic scheme
to generate a small number of adversarial examples to query the
trapdoored detector. The query result is used to determine rst an
appropriate generation layer and then generation parameters.
2Conventional adversarial attacks such as PGD should produce a more accurate es-
timate of trapdoor signatures than detected adversarial examples in the preparation
phase (see Section 6.5 for more information). Since this driving-away loss term plays a
minor role in generating adversarial examples (see Section 6.7), this rough estimate
should be sucient.