In our experimental evaluation of TeD, using the proposed signature
𝑆Δ may not lead to a good separation between trapdoored samples
and benign samples in the category protected by the trapdoor.
This is because they may have similar values on some neurons. To
increase the detection sensitivity, we have tried several ways to
increase the separation of trapdoored samples and benign target
samples: ordering neurons according to the separation and selecting
a subset of neurons with top separation values, projecting 𝑆Δ to the
expectation of benign samples in 𝐶𝑡, etc. We have found that the
projection method produces the best detection results. We call this
TeD variant as Projection-based Trapdoor-enabled Detection (P-TeD).
The detail for using a single trapdoor will be described. The cases
using multiple trapdoors and sampling neurons can be done in a
similar manner.
When a single trapdoor is used to protect a target category 𝐶𝑡,
we calculate trapdoor signature 𝑆Δ with Eq. 1 and the expectation
of feature representations of benign samples in 𝐶𝑡 as the benign
signature:
𝑆𝐶𝑡 = E𝑥∈𝐶𝑡 F𝐿(𝑥).
(2)
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3163Then we remove the projection of 𝑆Δ on 𝑆𝐶𝑡 from 𝑆Δ to get its
component perpendicular to 𝑆𝐶𝑡 :
Δ = 𝑆Δ − 𝑆Δ · 𝑆𝐶𝑡
𝑆⊥
(cid:107)𝑆𝐶𝑡 (cid:107)2
2
𝑆𝐶𝑡 ,
(3)
where 𝐴 · 𝐵 means the inner product of 𝐴 and 𝐵. 𝑆⊥
Δ is used as the
signature to detect adversarial examples in the same way as 𝑆Δ is
used in TeD.
4 FIA OVERVIEW
4.1 Threat Model
We assume that the trapdoored DNN model is white-box while
the trapdoored defense is black-box. More specically, we assume
that adversaries have full access to the trapdoored model, including
its architecture and internal parameter values, and are aware of
protection by the trapdoored defense but have no knowledge of its
characteristics (i.e., the number of trapdoors, trapdoor signatures,
detection layer, etc.). The trapdoored defense behaves like an ora-
cle to adversaries: when an example is received, it responds with
the detection result, either positive or negative, that the detector
has determined. No information on the distance to any trapdoor
signature is provided.
In addition, we assume that adversaries have no access to any
training data or backdoor triggers used to train a trapdoored model
and cannot apply any reverse-engineering technique such as Neural
Cleanse [59] to estimate the number of trapdoors, backdoor triggers,
or trapdoor signatures used in the defense, either due to unknown
characteristics of the trapdoored defense or other reasons. This
restriction of no access to any reverse-engineering technique en-
sures that the white-box access to the trapdoored model can only be
used to search for adversarial examples like conventional white-box
adversarial attacks. It cannot be used to derive the characteristics
of the trapdoored defense.
On the other hand, we assume that adversaries have access to a
small set of benign data independent of the training data used in
training a trapdoored model.
4.2 Intuition behind FIA
TeD relies on the following two key factors to work: one is that
searching for targeted adversarial examples is likely trapped into
shortcuts created by trapdoors of a trapdoored model, and the other
is that trapdoor signatures should be signicantly dierent from
those of benign examples of the target category. The former en-
sures that adversarial examples are likely detected by the trapdoor
signatures, while the latter ensures a low false positive rate, which
is critical to deploying any defense in real-world applications. In
addition, along its forward path, a DNN such as CNN extracts fea-
tures from low-level to high-level, and distinguishability of dierent
categories in the feature space gradually improves. For good de-
tectability, the detection layer 𝐿 of TeD should be close to the end
of the forward pipeline, usually the penultimate layer.
To circumvent the trapdoored detection, we aim to make adver-
sarial examples indistinguishable from benign target examples in
the feature space. Once an adversarial example is indistinguishable
from benign target examples at an appropriate latent layer, it is un-
likely detectable by the trapdoored defense unless the trapdoored
defense is impractical with a high false positive rate for benign
target samples. To achieve this goal, we need to craft adversarial ex-
amples with a loss function that can drive adversarial examples into
the distribution of benign target samples in the feature space. The
conventional loss function used in crafting adversarial examples is
the cross entropy, which has no control on the feature vector of a
generated adversarial example at a given latent layer. Fortunately,
Inkawhich et al. [29–31] proposed recently to use a loss function in
the feature space to craft more transferable adversarial examples
so that adversarial examples generated on a white-box model can
be transferred to attack a black-box model. We can adopt their
approach to circumvent the trapdoored detection.
Another obstacle we need to overcome is to nd a metric to
measure distinguishability of an example from a set of examples in
the feature space. If we assume that the trapdoored model and the
trapdoored detection are both stable, we expect that an example
with its feature vector located within a small dense region of benign
target examples at an appropriate latent layer is likely classied
and detected like its nearby benign target examples. This stability is
generally required for a practical DNN model with good behaviors.
With this assumption, we can simply nd a target inside the dense
region, minimize the distance to the target in the feature space, and
check if such a distance is within an appropriate threshold at the
end of crafting. This fullls the basic scheme of our proposed FIA.
Adversarial examples generated with the basic scheme may still
be detectable due to mismatch between the generation layer and
the the detection layer, irregular undetectable boundaries of the
trapdoored detection, over-simplied modeling of indistinguishabil-
ity, etc. FIA contains a preparation phase wherein the basic scheme
is used to generate a small number of adversarial examples to query
the trapdoored detection for determining an appropriate genera-
tion layer and other generation parameters. It also adds a loss term
to the basic scheme to maximize distances to detected adversarial
examples in the preparation phase to avoid regions where trapdoor
signatures may be located.
5 FEATURE-INDISTINGUISHABLE ATTACK
Our proposed Feature-Indistinguishable Attack (FIA) is described
in detail in this section. In addition to the basic scheme and the
complete scheme, we also present a variant of FIA.
5.1 Basic Scheme
5.1.1 Optimization Problem. For input 𝑥 ∈ X that is not in target
category 𝐶𝑡, 𝑥 ∉ 𝐶𝑡, and a latent representation distribution D𝑡
of target category 𝐶𝑡 at a selected latent layer 𝐿, called generation
layer, the basic scheme of our proposed FIA minimizes the distance
of the latent representation F𝐿(𝑥 + 𝜖) of 𝑥 + 𝜖 at layer 𝐿 to a target
representation F 𝑡𝑔𝑡
∈ D𝑡 on the condition that F𝐿(𝑥 + 𝜖) is within
the latent representation distribution D𝑡 of target category 𝐶𝑡:
𝐿
min𝜖 D(F𝐿(𝑥 + 𝜖), F 𝑡𝑔𝑡
s.t. F𝐿(𝑥 + 𝜖) ∈ D𝑡
𝐿
),
(4)
where D is a distance function. The constraint in Eq. 4 ensures that
a generated adversarial example is indistinguishable from benign
examples in target category 𝐶𝑡.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3164𝐿
𝐿
min𝜖
(cid:107)2},
FIA uses two distance loss functions in Eq. 4 to minimize both
distances simultaneously. One is an 𝐿2 distance like that in Acti-
vation Attack [31]. Its goal is to drive adversarial examples into
target category 𝐶𝑡. The other is the cosine similarity with the target
representation. Its goal is to ensure that an adversarial example
has a feature vector along a direction similar to that of the target
representation so that TeD, which relies on cosine similarity with
trapdoor signatures in the feature space, unlikely detects it. The
optimization problem becomes:
{− cos(F𝐿(𝑥 + 𝜖), F 𝑡𝑔𝑡
) + 𝜆 · (cid:107)F𝐿(𝑥 + 𝜖) − F 𝑡𝑔𝑡
s.t. F𝐿(𝑥 + 𝜖) ∈ D𝑡
(5)
where 𝜆 ≥ 0 is a weighting parameter. Note that there is a negative
sign before the cosine similarity since we want to maximize the
cosine similarity with the target representation.
5.1.2 Basic Scheme. To ensure the constraint in Eq. 5, we need
to estimate the feature-representation distribution D𝑡 of target
category 𝐶𝑡. Although it is possible to use neural networks to model
the feature-representation distribution of target category 𝐶𝑡 like
in [29], FIA adopts a less accurate but much simpler approach: we
assume that feature representations of benign examples in target
category 𝐶𝑡 can form a convex region1. With this assumption, we
can choose the expectation of feature representations of benign
examples in 𝐶𝑡, F 𝐶𝑡
𝐿 , as the target representation F 𝑡𝑔𝑡
F 𝑡𝑔𝑡
𝐿 ≡ E𝑥∈𝐶𝑡 F𝐿(𝑥),
(6)
where E(·) is the expectation function, and use the cosine similarity
distribution of benign examples in 𝐶𝑡 with F 𝐶𝑡
as an approximation
of distribution D𝑡.
= F 𝐶𝑡
Since expectation is sensitive to outliers, we determine and re-
move outliers with DBSCAN [18] before calculating the expectation
in Eq. 6: a certain percentage (10% in our evaluation) of benign tar-
get samples located in the lowest-density regions in the feature
space are considered as outliers and removed. We then calculate
a threshold 𝑐𝑝 to be the smallest cosine similarity between the ex-
pectation and survived benign target samples. We require that the
cosine similarity of an adversarial example with the expectation
F 𝐶𝑡
is within 𝑐𝑝.
With the above assumption and simplication, Eq. 5 becomes:
:
𝐿
𝐿
𝐿
𝐿
{− cos(F𝐿(𝑥 + 𝜖), F 𝐶𝑡
𝐿 ) + 𝜆 · (cid:107)F𝐿(𝑥 + 𝜖) − F 𝐶𝑡
𝐿 (cid:107)2},
min𝜖
s.t. cos(F𝐿(𝑥 + 𝜖), F 𝐶𝑡
𝐿 ) ≥ 𝑐𝑝
(7)
5.1.3 Adaptive Iteration. Since the constraint in Eq. 7 is on the rst
loss term in Eq. 7, we can apply the following adaptive iteration
to solve Eq. 7: We start to drive only the rst loss term by setting
𝜆 = 0 until the constraint is met. We then activate 𝜆 by setting
it to a non-zero value (1 in our evaluation) to minimize both loss
terms simultaneously to drive 𝑥 + 𝜖 to target category 𝐶𝑡 while
maintaining satisfying the constraint. When 𝑥 + 𝜖 is classied into
𝐶𝑡 and its softmax probability is larger than the next maximum
softmax probability by a specic threshold, we return to drive only
the rst loss term again by setting 𝜆 = 0 as long as the softmax
probability gap is maintained, otherwise we activate 𝜆 to drive both
1This assumption is generally over-simplied for a DNN model, esp. when a middle
latent layer is used as the generation layer. See Section 6.5 for more information.
loss terms. The softmax gap is used to ensure that a generated
adversarial example is robustly classied into target category 𝐶𝑡.
If we place an 𝐿∞ bound 𝛿 on adversarial perturbation 𝜖, Eq. 7
can be solved with a PGD-like iterative process with the above
adaptive method:
𝑥0 = 𝑥,
𝐿 (cid:107)2
𝐿 )))
𝐿 ) + 𝜆 · (cid:107)F𝐿(𝑥) − F 𝐶𝑡
𝐿 )) = − cos(F𝐿(𝑥), F 𝐶𝑡
𝑥𝑛+1 = 𝐶𝑙𝑖𝑝𝛿(𝑥𝑛 − 𝜂 · 𝑠𝑖𝑔𝑛(∇𝑥 ℓ(F𝐿(𝑥𝑛), F 𝐶𝑡
(8)
𝐿 )) is the loss function of the basic scheme:
where ℓ(F𝐿(𝑥𝑛), F 𝐶𝑡
ℓ(F𝐿(𝑥), F 𝐶𝑡
(9)
The iterative process can stop early if 𝑥𝑛 is classied into target
category, 𝑥𝑛 ∈ 𝐶𝑡, and cos(F𝐿(𝑥𝑛), F 𝐶𝑡
𝐿 ) ≥ 𝑐𝑝. Otherwise a pre-
set number of steps is executed before it stops. In our empirical
evaluation, stopping early is used for the basic scheme used in the
preparation phase (see Section 5.2.1), and a xed number of steps is
executed for the complete scheme to generate adversarial examples.
By the end of the iterative process, the generation of an adversar-
ial example is said successful if the resulting example is classied
into target category 𝐶𝑡 by the model and the constraint in Eq. 7 is
satised. Otherwise the generation is a failure.
5.2 Complete Scheme
Adversarial examples generated with the basic scheme may still
fail to circumvent the trapdoored detection due to several reasons:
mismatch between the generation layer and the unknown detec-
tion layer of TeD, irregular undetectable boundaries of the trap-
doored defense, over-simplied convex-region assumption for the
feature-representation distribution of the target category and the
subsequent simplication from Eq. 5 to Eq. 7.
The second reason can be explained as follows. Suppose the gen-
eration layer and the detection layer are the same latent layer. Since
TeD uses cosine similarity with trapdoor signatures to detect ad-
versarial examples, examples with the same cosine similarity value
with the expectation vector may have dierent detectability: those
close to the trapdoor signatures have a higher chance to be detected
than those far away from the trapdoor signatures. Unfortunately,
the basic scheme treats these examples identically.
To address these issues, we enhance the basic scheme with two
additions. One addition is a preparation phase to use the basic
scheme to generate some adversarial examples to query the trap-
doored defense to determine an appropriate generation layer and
other generation parameters. The other addition is an additional
term in the loss function to direct adversarial examples away from
bad regions where detected adversarial examples in the prepara-
tion phase stay. These positive adversarial examples are a rough
estimate of trapdoor signatures2.
5.2.1 Preparation Phase. In this phase, we use the basic scheme
to generate a small number of adversarial examples to query the
trapdoored detector. The query result is used to determine rst an
appropriate generation layer and then generation parameters.
2Conventional adversarial attacks such as PGD should produce a more accurate es-
timate of trapdoor signatures than detected adversarial examples in the preparation
phase (see Section 6.5 for more information). Since this driving-away loss term plays a
minor role in generating adversarial examples (see Section 6.7), this rough estimate
should be sucient.