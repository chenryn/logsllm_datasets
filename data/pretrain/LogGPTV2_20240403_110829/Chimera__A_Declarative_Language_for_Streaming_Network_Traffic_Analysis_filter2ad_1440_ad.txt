WHERE unique([mta])
) AS a JOIN mtasmtp AS b
ON [mta] EQUALS [mta]
WHERE [b].[packets].[time] -
[a].[packets].[time]  50
This query contains a number of more complicated
operations to achieve the desired result. The CREATE
VIEW statement is used for the ﬁrst time to set up a table
of SMTP messages that are split by MTAs. The MTAs
are extracted from "Received" headers in the SMTP mes-
sage using a regular expression that searches for the
string "by " and pulls out the following word.
In the ﬁrst part of the select statement that follows,
all of the destination e-mail addresses are extracted by
splitting the "To", "Cc", and "Bcc" headers by commas,
and then merging them into one list. The apply function
iterall is then used to pass each recipient through the
aggregate count_distinct function to count the num-
ber of unique recipients for each MTA.
The sub-query in the left part of the join uses a stateful
function unique in a WHERE clause. This means that
it will use one global state instead of having a different
state for each aggregate key. Furthermore, the unique
function will accumulate values indeﬁnitely. This func-
tion is different from distinct in a subtle way; it is
designed to only output "new" values. It will silently add
items to a Bloom ﬁlter during a learning phase at start-up,
and then start generating output once a certain percent-
age of its inputs have already been seen. As the Bloom
ﬁlter ﬁlls up, distinct will stop adding to it and create
a new one. Once the new ﬁlter becomes full, the old one
will be discarded and process will continue so that there
are always two Bloom ﬁlters in use. With a Bloom ﬁl-
ter it is possible to falsely label new items as not unique a
small percentage of the time. This trade-off buys reduced
memory utilization. The false match rate of a Bloom ﬁl-
ter will depend on its size and the number of insertions
that are made before rolling it over.
The ﬁnal part of the query joins new MTAs with fu-
ture e-mails that contain those MTAs, using the WHERE
clause to cut off the count after 24 hours. The UNTIL
clause will trigger as soon as the unique recipient count
exceeds 50 and generate a ﬁnal query output.
5 Bro Compiler Implementation
For this paper, we implemented a Chimera compiler
that produces policies for the Bro event language [21].
While we have only implemented one speciﬁc target,
it would be possible to extended the compiler to target
other languages. The work that we describe in section
5.1 on translating a declarative query to an intermediate
relational algebra will be applicable for all targets. The
code generation phase, which is described in section 5.2,
will depend on the target language.
5.1 Translation to Relational Algebra
Because Chimera is very similar to SQL, we begin
the compilation process in the same way as traditional
database systems: by parsing the query and translating it
into an intermediate relational algebra. We used a simple
YACC parser [16] and the syntax from section 3 to con-
vert the original query into an abstract syntax tree (AST)
representation. From there, the compiler translates the
AST into a data-ﬂow representation that loosely corre-
sponds to relational algebra, which we call the Chimera
Core. The Chimera Core operators are shown in ﬁgure 2.
This step is performed using syntax-directed translation
[2], wherein syntactic elements are converted into data-
source(source)
parser(parser)
split(exprlist, aliasitem)
projection(expr1,alias1, ...,exprn,aliasn)
selection(expr)
rename(newlabel)
join(labelle f t, labelright, exprle f t, exprright,
exprwindow, joinkind, tablesize)
group(exprgroupby, expruntil, options, tablesize,
aggexpr1,alias1, ...,aggexprn,aliasn)
output(dest)
Figure 2: Chimera Core language constructs
ﬂow operators as shown in ﬁgure 3. During this process,
the compiler uses a symbol table to map aliases to loca-
tions in the data-ﬂow graph, but does not need to perform
full data-ﬂow analysis because all data-ﬂow connections
are explicit in the Chimera syntax.
→ add alias to symbol table
CREATE VIEW
→ source
SOURCE
→ parser
→ split
SPLIT
 AS ... → rename
→ join
JOIN
→ selection
WHERE
GROUP BY . . . UNTIL . . . ORDER BY . . . LIMIT
→ group
→ selection
→ projection
→ output
HAVING
SELECT
INTO
Figure 3: Summary of translation to Chimera Core
To illustrate translation from a Chimera query to the
Chimera Core language, consider the following example:
SOURCE STDIN
SELECT avg([b].[z]) AS avgz
FROM dns AS a JOIN smtp AS b ON [x] EQ [y]
WHERE [a].[x] > 5
GROUP BY [a].[x]
UNTIL avgz > 3
INTO STDOUT
Figure 4 shows the data-ﬂow graph that results from
this example query. Using top-down syntax-directed
translation, the ﬁrst node emitted is a source node cor-
responding to SOURCE STDIN. The FROM statement
is processed next. Because there is a JOIN, the compiler
ﬁrst translates the left and right tables, adding parser
nodes to the source. The parser outputs are then fed
through rename operators so that they can be referenced
in the join operator, which combines them into a sin-
gle data ﬂow. Next, the data ﬂows through a selection
events into output tuples.
changed, but renames the event.
• split – This node takes a tuple with a list expression
and outputs a new tuple for each item in the list,
which also includes all the original tuple items.
• projection – This node outputs an event handler
that executes one or more expressions on each in-
put tuple and assigns their results to an output tuple.
• selection – This node evaluates an expression on
each input tuple and passes that tuple as output if
the expression is true.
• rename – This node passes tuples through un-
• join – This node stores tuples in a hash table keyed
on their join expression values and later matches
them against tuples from the other side of the join.
When there is a match (or no match for OUTER
joins), this node will generate a new output tuple
with one or both elements. To support the WIN-
DOW expression, we extended the Bro table data
structure to expose its oldest element.
• group – This node maintains a hash table keyed on
the GROUP BY expression value. The table con-
tains state objects for each aggregate function, all
of which have their Iteration routine called for each
new tuple. When the UNTIL expression becomes
true, this node calls each aggregate function’s Eval-
uation routine, adds the results to an output tuple,
and then calls the aggregate Termination routines to
ﬂush the state objects.
• output – This special-purpose node outputs tuples
in CSV format, or, if the tuple only has one packet,
sends output to a PCAP ﬁle.
Some of the operator nodes take function expressions
as arguments. As mentioned before, each function is
written natively in Bro. A few functions, such as those
that use a Bloom ﬁlter, also required some implementa-
tion in the Bro internal function (BIF) language. When a
function is encountered during code generation, its deﬁ-
nition is included in the Bro code and it is called with a
standard Bro expression. Bro does not support method-
style function calls using a syntax like x(arg1).y(arg2),
so these are re-written as y(x(arg1),arg2). Apply func-
tions are implemented by generating inline anonymous
ﬁrst-class function deﬁnitions, which are supported by
Bro.
5.3 Example
Here we demonstrate Bro code generation with a simple
example. In the interest of space, the example does not
include join and group operators. Consider the follow-
ing Chimera query:
SELECT [path]
FROM http-request
WHERE [method] == "GET"
Figure 4: Chimera Core data-ﬂow graph for example
operator that ﬁlters tuples using the WHERE expres-
sion. The tuples are then aggregated with a group opera-
tor, which also computes and adds aggregate expressions
from HAVING and SELECT clauses to the data ﬂow. Fi-
nally, expressions in the SELECT clause are extracted
with the projection operator, and output sends data to
standard output.
5.2 Code Generation
The next step in compilation is to translate the data-ﬂow
graph into Bro code. This process happens in two main
stages: (1) type computation, and (2) event code gener-
ation. The event code generation step further depends
on the implementation of user-deﬁned functions, which
written natively in the Bro language. Also note that data
sources in Bro are speciﬁed on the command line, so the
source operator is emitted as a shell script wrapper and
not as part of the Bro language.
Type computation involves visiting each edge in the
data-ﬂow graph, determining the contents of tuples that
ﬂow through that edge, and then creating a record type
for those tuples. Edges coming from operators that do
not change the data – selection and rename – can be
ignored during this pass. It would have been possible to
use a table of the any type in Bro for tuples, or to create
another dynamic data structure. We chose to use custom
record types instead because they are better-documented
and do not require modifying Bro internals.
After types have been deﬁned for each input and out-
put tuple, the compiler generates code for each node in
the data ﬂow graph in the form of an event handler:
• parser – This node adds a Bro protocol parser at
the beginning of the ﬁle (if it does not yet exist) and
deﬁnes an event handler that converts Bro protocol
This query extracts the path from all HTTP GET re-
It translates to the following data-ﬂow graph,
quests.
where each operator sends data to the next:
l0: source(STDIN)
l1: parser(http-request)
l2: selection([method] == “GET")
l3: projection([path], none)
l4: output()
Finally,
this is compiled down to the following Bro
script. Note that Bro splits up the HTTP headers and
body into multiple events. To have everything avail-
able in one tuple, we also add an event handler for
http_all_headers that saves the headers in the session
table, which is omitted here to save space.
@http-reply
type http_request_type: record {
method: string;
path: string;
headers: listmap;
body: string;
packetlist: packetlist_type;
};
type l3_type: record {
v1: string;
};
event l3(t: l3_type) {
print t$v1;
}
event l2(t: http_request_type) {
local out: l3_type;
out$v1 = t$path;
event l3(t);
}
event l1(t: http_request_type) {
if (!(t$method == "GET")) return;
event l2(t);
}
event http_message_done(c: connection, ...) {
local t = http_request_translate(c);
event l1(t);
}
6 Evaluation & Future Optimizations
We have presented the implementation of a compiler
that translates Chimera queries into the Bro event lan-
guage. Because functionality was our primary focus,
we have not yet implemented any performance optimiza-
tions. However, there are many areas that have potential
for optimization. Here we evaluate the compiler’s pro-
cessing performance in its current unoptimized form and
discuss opportunities for future performance optimiza-
tion. This section does not evaluate memory utilization
because it is highly dependent on the particular query,
desired window size, and data rate of the connection.
Windows for JOIN and GROUP BY operations can be
scaled according to the operating environment and ana-
lytic needs.
6.1 Performance Measurement
The performance measurements in this section were
taken by processing a 2 GB PCAP ﬁle (stored on a ram
disk) with Bro and recording the execution time. The
PCAP ﬁle was generated by capturing trafﬁc at a U.S.
government network gateway, so it includes data from
a variety of protocols.
It contains approximately 81k
HTTP, 58k SMTP, and 32k DNS messages.
To test the compiler’s performance, we compare the
Bro event code generated by the Chimera compiler to
hand-written Bro code that implements the same func-
tionality. For example, the Bro code in section 5.3 could
be written by hand as follows:
@http-reply
event http_request(c: connection,
method: string, original_URI: string,
unescaped_URI: string, version: string) {
if (method == "GET")
print original_URI;
}
This shorter implementation has three optimizations:
1. Data is not copied into new record types.
2. Events with only one handler are evaluated inline.
3. An earlier event handler (http_request) is used be-
cause the headers and body are not needed.
Our ﬁrst experiment tests the effect of each optimiza-
tion by applying them one-by-one to the section 5.3 ex-
ample. We ran each conﬁguration 30 times against the
test data. Table 3 summarizes our results. Bypassing
data copying saves about 1.5% execution time. Inlining
event code makes no signiﬁcant difference in this case.
Switching to a single earlier handler saves another 1.5%,
for a 3.0% overall speed-up. While the difference be-
tween current compiled code and hand-written code is
noticeable, it does not have a major impact.
Table 3: Execution times with different optimizations
Conﬁguration Base Opt-1 Opt-1+2 Opt-1+2+3
Avg. Time (s) 14.21 14.00
Std. Dev. σ (s) 0.084 0.083
Speed-up (%)
14.01
0.074
1.5% 1.4%
-
13.79
0.081
3.0%