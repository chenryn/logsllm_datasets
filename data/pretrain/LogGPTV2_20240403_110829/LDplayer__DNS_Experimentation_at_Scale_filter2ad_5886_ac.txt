There are two challenges in changing the traces. First, binary net-
work trace is complicated to edit directly because changes are not
space-equivalent. We need a user-friendly method to manipulate
queries. Second, the delay caused by manipulation and processing
traces, may also bring problems for accurate query replay.
Plain text for easy manipulation: To easily manipulate input
queries, we convert network traces to human-readable plain text.
We develop a DNS parser to easily extract relevant data from net-
work trace, and output a column-based plain text file where each
line contains necessary information of a DNS message. In this stage,
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
L. Zhu et al.
Queriers. The end Queriers directly interact with DNS servers via
different protocols (UDP, TCP or TLS). For reliable communication,
we decide to choose TCP for message exchange among distributors.
The primary purpose of multiple levels is to connect enough end
Queriers when there is a limit on the number of distribution con-
nections in each Distributor. Without limit, one-level distribution
(Controller distributes to Queriers directly) can bring 4 billion con-
nections theoretically in total, with maximum 65 k Querier hosts
connected at any time.
If the input trace is extremely fast, the CPU of Controller may
become bottleneck because it limits the speed of input processing.
To solve this problem, we can split input stream to feed multiple
controllers.
Correct timing for replayed queries: The ultimate goal of
query replay system is to replay DNS queries with correct timing
and reproduce the traffic pattern.
Due to distributing queries among different hosts, it is challeng-
ing to synchronize time and ensure the correct timing and ordering
of individual queries.
To replay queries at accurate time, LDplayer keeps tracking
trace time and real time, and schedules timer events to send queries.
When getting the first input query message, controller broadcasts a
special time synchronization message to all the queriers to indicate
the start time of the trace. Upon receiving the time synchronization
message, a querier obtains the current trace time (¯t1) and real time
(t1).
On receiving the subsequent query stream, a querier extracts
the absolute query time in trace (¯ti) and computes the relative trace
time (∆¯ti), as ∆¯ti = ¯ti − ¯t1. The relative trace time is the ideal delay
that should be injected for trace replay assuming no input delay.
Similarly, the querier also gets current absolute real time (ti)
and the relative real time (∆ti) as ∆ti = ti − t1. The relative real
time represents the accumulated program run-time delay, such as
input processing and communication delay, that has already been
generated.
To replay the query (qi) at correct time, LDplayer removes the
added latency and schedules a timer event at ∆Ti in the future,
where ∆Ti = ∆¯ti − ∆ti. If the trace is extremely fast and the in-
put processing falls behind (∆Ti ≤ 0), LDplayer sends the query
immediately without setting up a timer event.
By tracking timing and continuously adjusting, LDplayer pro-
vides good absolute and relative timing (as shown in §4).
Some experiments, such as load testing, prefer large query streams,
as fast as possible, instead of tracking original timing time. As an
option, LDplayer can disable time tracking and replay as fast as
possible.
Emulating queries from the same source: Some traces or
experiments require reproduction of inter-query dependencies. Two
examples are UDP queries where the second query can be sent only
after the first is answered, or when studying TCP queries where
connections are reused. In general, we assume all queries from the
same source IP address are dependent and queries from different
sources are independent. We assume queries are independent, since
captured DNS traces normally do not show application dependency.
Identifying semantic dependence between queries is an area for
future work.
Figure 3: Trace mutator converts network trace to plain text
for easy editing, and further converts to customized binary
stream as input. LDplayer accepts three types of input: net-
work trace, formatted plain text and customized binary files.
users can edit DNS messages as desired with a program or text
editor. Most data in a DNS message can be modified, including DNS
header flags, query names, EDNS data, and transport protocol.
Binary for fast processing: Since plain text as input delays
building DNS messages, we convert the resulting text file to a cus-
tomized binary stream of internal messages to serve as input for
trace replay (Figure 3) for fast processing. To distinguish differ-
ent messages in the input stream, we pre-pend the length of each
message at the beginning of each binary message.
To save unnecessary input delay in query replay, we pre-process
the input and separate the input processing from the query replay
system. Optionally, the input engine of our system can also read net-
work trace and formatted text file directly, and convert to internal
binary messages on the fly.
We handle trace replay and support mutation of the trace in ways
that are similar to the original. In some cases, what-if experiments
may imply changes to traffic that are very different from the original
trace. For example, if all zones are changed to be DNSSEC signed,
then one must generate new DNSKEY and RRSIG records. For such
experiments, the experimenter must insure that trial zone includes
new data for the replay to provide correct results.
2.6 Distribute Queries For Accurate Replay
With server setup and input trace, the next step for a successful
DNS trace replay is to emulate DNS queries with correct timing
from different sources and connections.
Fast query replay and diverse sources: There are several re-
source limit in a single host: CPU, memory and the number of ports.
The query rate generated at a single host is limited because of CPU
constraints. The ability to maintain concurrent connections in a
single host is limited by memory and the number of ports (typical
65 k).
To support fast query rates from many sources, our approach
is to distribute query stream to many different hosts, allowing
many senders to provide a large aggregate query rate. In particular,
we coordinate queries from many hosts with a central Controller
managing a team of Distributors which further controls several
TraceConverterTextConverterInternalMessageNetwork TracePlain TextCustomized binaryBinary Readerpcap, erf …time: 1461234567.012345  src:  192.168.1.1query: example.com A  IN protocol: TCP…Length: 200 bytes010101110001…..DNS ParserConverter(de-serialize)LDplayer’s input engineQuery ManipulatorInput filesDNS Experimentation at Scale
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Figure 4: A prototype of distributed query system with two-
level query distribution. Distributors and queriers are imple-
mented as processes and running on the same host (client in-
stance). Optionally, a single distributor can read input query
stream directly.
We do preserve queries that originate from the same source as
one kind of dependency, since it affects performance of DNS-over-
TCP. We use different network sockets to emulate query sources.
To emulate queries from the same sources, we must first deliver
all the queries from the same sources (IP addresses) in the original
trace to the same end querier for replay. To accomplish this, each
distributor tracks the original query source address and the lower
level component in the message distribution flow. When queries
are distributed, each distributor either picks the next entity based
on a recent query source address in record, or selects randomly
otherwise (during startup). Similarly, the controller guarantees the
same-source queries are assigned to same distributor. Each entity
keeps the record during the experiments.
Similarly, queriers map the query sources and the underlying
network socket, insuring that same-source queries use the same
socket if it is still open. New sources start new sockets.
When emulating TCP connection reuse, queriers also tracks open
TCP connections. They may close them after a pre-set timeout.
As a result, during query replay, a DNS server observes queries
from the same set of host addresses but with a range of different port
numbers, which emulates different queries from the same sources.
An alternative is to setup virtual interfaces with different IP ad-
dresses at queriers, and use those interfaces for each query sources
address in query replay. However, the method does not scale to a
large number of addresses.
3 IMPLEMENTATION
We implement a prototype replay system and proxies in C++, to
provide efficient run-time, and full control over memory usage.
Query System: In two-level query distribution system (Fig-
ure 4), with a controller and multiple clients. The controller runs
two processes, the Reader, for trace input, and another, the Postman
to distribute queries. One or more machines are clients, each with
distributor and multiple querier processes. Processes use event-
driven programming to minimize state and scale to a large number
of concurrent TCP connections. The reader pre-loads a window of
queries to avoid falling behind real time.
Server Proxy: The proxies around the server run as either recur-
sive proxy or authoritative proxy (§2.4). A single reader thread reads
from a tunnel network interface, while multiple worker threads
Figure 5: Network topology used for evaluation: controller
(T), server (S), and client instances (C)
read from a thread-safe queue that rewrites queries (§2.4). Our pro-
totype of the recursive proxy only talks to a single authoritative
proxy. Supporting partitioning the zones across the set of different
authoritative servers is a future work.
4 EVALUATION
We validate the correctness of our system by replaying different
DNS traces in controlled testbed environment (§4.1). Specifically, we
validate query inter-arrival time and query rate. Our experiments
show that the distributed client system replays DNS queries with
correct timing, reproducing the DNS traffic pattern (§4.2).
4.1 Experiment Setup and Traces
To evaluate our system, we deploy the network shown in Figure 5
in the DETER testbed [5]. We use a controller (T ) to distribute query
stream to client instances (C1 to Cn). Each client instance runs sev-
eral distributor and querier processes to replay input queries. The
query traffic merges at a LAN representing an Internet Exchange
Point, and is then sent to the server (S). Each hosts is a 4-core (8-
thread) 2.4 GHz Intel Xeon running Linux Ubuntu-14.04 (64-bit). We
use several traces, listed in Table 1 and described below, to evaluate
the correctness of our system under different conditions.
B-Root: This trace represents all traffic at B-Root DNS server
(both anycast sites) over one hour during the 2016 and 2017 DITL
collections [13]. It is available from the authors and DNS-OARC. We
use B-Root-16 trace (Table 1) in this section to validate our system
can accurately replay high-volume queries against an authoritative
server. We use other groups of B-Root-17 traces in later sections
(§5). Traffic to each root server varies, but the B-Root trace is not
significantly different from the others.
Synthetic: To validate the capability to replay query traces with
various query rates, we create five synthetic traces (syn-0 to syn-4
in Table 1), each with different, fixed inter-arrival times for queries,
varying from 0.1 ms to 1 s. Each query uses a unique name to allow
us to associate queries with responses after-the-fact.
4.2 Accuracy of Replay Timing and Rate
We first explore the accuracy of the timing and rate of query replay.
Methodology: We replay B-Root and synthetic traces over UDP
in real time and capture the replayed traffic at server. We match
query with reply by prepending a unique string to every query
names in each trace. We then report the query timing, inter-arrival
time and rate, comparing the original trace with the replay. We use
a real DNS root zone file in server for B-Root trace replay to provide
responses. For synthetic trace replay, we setup the server to host
names in example.com with wildcards, so that it can respond all
. . .DistributorQuerierQuerierDNS ServerQueryStream. . .Unix socketReaderPostmanController. . .DistributorQuerierQuerierClient instanceoptionalSTC1IXPCn1Gb/s<1msIMC ’18, October 31-November 2, 2018, Boston, MA, USA
L. Zhu et al.
traces
B-Root-16
B-Root-17a
B-Root-17b
Rec-17
Synthetic
syn-0
syn-1
syn-2
syn-3
syn-4
start
2016-04-06
15:00 UTC
2017-04-11
15:00 UTC
2017-09-01
17:22 UTC
-
-
-
-
-
(min)
+60
+60
+20
+60
60
60
60
60
60
inter-arrival
(seconds)
.000027
±.000619
.000023
±.001647
.000025
±.001536
.180799
±.355360
1
.1
.01
.001
.0001
client IPs
1.07 M
records
137 M
1.17 M