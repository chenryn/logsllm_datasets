(a) RIPE
(b) Google (RIPE)
(c) Edgecast (RIPE)
(d) PRES
(e) Google (PRES)
(f) Edgecast (PRES)
Figure 2: Preﬁx length vs. ECS scope for RIPE and PRES (Google: March 2013, Edgecast: May 2013).
number of ASes with YouTube servers almost triples (271%) from
220 to 598, these ASes overlap with already uncovered Google
infrastructure. We ﬁgure this being a result of incorporating the
YouTube infrastructure into Google’s global platform. By merging
the sets of IP addresses for Google and YouTube, the count only in-
creases to 24048. For an in-depth study of the expansion of Google
infrastructure since November 2012 we refer to [14]. Our study
did not uncover signiﬁcant growth of serving infrastructure for the
other ECS adopters.
5.2 Uncovering DNS Cacheability
Next, we examine the ECS speciﬁc information included in the
DNS response: the scope. In principle, if the ECS information cor-
responds to a publicly announced preﬁx, one may expect that the
returned scope is equal to the preﬁx length. However, this is not
necessarily the case. Content providers often return either coarser-
or ﬁner-grained scopes e.g., they respond either with aggregated or
de-aggregated preﬁxes. This indicates that they perform the end-
user clustering for client-to-server assignment on a different gran-
ularity than the routing announcements.
We note that the scope may have a major effect on the re-usability
of the DNS response i.e., the cacheability of DNS responses. While
most of the responses have a non-zero TTL, a surprisingly large
number have a /32 scope. An ECS scope of /32 implies that the
answer is valid only for the speciﬁc client IP which issued the DNS
request. In this section, we explore DNS cacheability for two ECS
adopters in detail: Google and Edgecast. The others are less inter-
esting as CacheFly always uses a /24 scope and MySqueezebox is
similar to Edgecast.
Figure 2(a) shows the RIPE preﬁx length distribution (circles).
In addition, it includes the returned scopes from using the RIPE
preﬁxes to query our ECS adopters. We note, that the distributions
vary signiﬁcantly. There is massive de-aggregation by Google but
massive aggregation by Edgecast which operates a smaller infras-
tructure.
When executing back-to-back measurements for Google (e.g., 4
queries within a second), we ﬁnd that typically both the answer and
scopes are consistent within the duration of the TTL (300 seconds
for Google). The answer as well as the scope can change in some
cases within seconds6. A detailed study of the temporal changes of
the returned scope is part of our future work. Overall, as seen in
Figure 2(a), for almost a quarter of the queries, the returned scope
is 32. This indicates that currently Google severely restricts the
cacheability of ECS responses or may want to restrict reuse of the
answers to single client IPs. For approximately 27% of the queries
preﬁx length and scope are identical. For 41% of the queries we see
de-aggregation while there is aggregation for 31%. The difference
between the announced preﬁx and the returned scope may also be
due to the the BGP feed sent to the GGC by the ISP [13] that is not
necessarily the same that is publicly announced and collected by
RIPE or Routeviews. We again note, that the returned scopes for
the RIPE and RV preﬁxes are almost identical.
Exploring the scopes returned by Edgecast may at ﬁrst glance
appear useless, because they only returned a single IP with a TTL
of 180 seconds. However, Edgecast is using signiﬁcant aggregation
for all preﬁx lengths across all preﬁx sets. For example, when using
the RIPE preﬁxes, the return scope is identical for 10.5% but less
speciﬁc for 87%.
When using the ISP preﬁx set, the overall picture is similar even
though the speciﬁc numbers vary. An initial study of the preﬁxes
with scope 32 indicates that Google performs proﬁling, e.g., Google
returns scope /32 for all CDN servers of a large CDN provider in-
side the ISP. In future work, we plan to explore if there exists a
natural clustering for those responses with scope /32.
Given that the size of the UNI preﬁx set is limited, we issue
queries from all IP addresses with preﬁx length 32. The returned
scopes vary heavily from /32 to /15, even for neighboring IP ad-
dresses.
For the PRES preﬁxes, Figure 2(d) shows extreme de-aggrega-
tion. For more than 74% of the preﬁxes, the scope is more restric-
6This can be attributed to the fact that authoritative nameservers
may use anycast or load balancing [11] or because the rapid in-
crease of DNS queries (e.g., from our measurements) triggers a
change on IP/preﬁx to server mapping.
Prefix length/ECS scopeCount0510152025300100000200000RIPEGoogleEdgecast051015202530ECSscope051015202530Preﬁxlength0180256360512540768721024901280Count051015202530ECSscope051015202530Preﬁxlength067375134750202125269500336875CountPrefix length/ECS scopeCount05101520253004000080000PRESGoogleEdgecast051015202530ECSscope051015202530Preﬁxlength037097418111271483618545Count051015202530ECSscope051015202530Preﬁxlength02767553483011106813835Counttive than the preﬁx length, and in 17% they are identical. Only few
returned scopes are /32s. This may indicate that Google treats pop-
ular resolvers differently than random IP addresses. Google may
already be aware of the problem regarding caching DNS answers
as discussed in Section 2.2. For Edgecast we see signiﬁcant aggre-
gation.
To highlight the relationship of preﬁx length in the query to
scope in the reply, Figures 2(b) and 2(e) show heatmaps of the cor-
responding two-dimensional histograms. For the RIPE dataset we
notice the two extreme points at scopes /24 and /32. For the PRES
dataset, the heatmap highlights the de-aggregation. Figures 2(c)
and 2(f) show the heatmaps for Edgecast. While for the RIPE
dataset we see the effect of the extreme preﬁx de-aggregation for
Google very clearly, the picture for Edgecast is more complicated
as there is mainly aggregation. For the PRES dataset, the heatmap
shows even more diversity as there is de-aggregation as well as ag-
gregation. This results in a blob in the middle of the heatmap.
5.3 User-Server Mapping Snapshots
So far we have not yet taken advantage of the Web server IP
addresses in the DNS replies. These allow us to capture snapshots
of the user-to-server mapping employed by an ECS-enabled CDN
or CP that can be used to shed light on CDN mapping strategies. In
the following, we illustrate the measurement capabilities offered by
ECS. We explore snapshots of Googles’ user-to-server mappings
(based on the RIPE data set) and examine how stable this mapping
is.
Google returns 5 to 16 different IP addresses in each reply. Al-
most all responses (>90%) include either 5 or 6 different IP ad-
dresses. We do not ﬁnd any correlation between the ECS preﬁx
length or the returned scope and the number of returned IP ad-
dresses. All IP addresses from a single response always belong to
the same /24 subnet (the returned IPs are not necessarily in close ge-
ographic distance to each other [20]). We also notice that typically
the announced preﬁx length of subnets that host Google servers is
/24. Thus, based on a single ECS lookup per preﬁx, we always ﬁnd
a unique mapping between query preﬁx and the server subnet from
the DNS reply.
Next we assess the mapping consistency at AS-level. First we
map all all preﬁxes used in the ECS queries to their correspond-
ing AS. Then, by looking at the returned A records, we ﬁnd the
corresponding server ASes for each client AS.
On March 26 2013, the majority of client ASes, around 41K,
was served exclusively by Google servers from a single AS. About
2K ASes were served by servers in 2 ASes, and less than 100 ASes
were served by servers from more than 5 ASes. On August 8, 2013
the number of ASes that served by a single AS dropped to around
38.5K and around 5K served by 2 ASes. ASes served by a large
number of server ASes typically have a global footprint. We ﬁnd
that client preﬁxes of ASes that host GGC are also served by servers
in other ASes. This is to be expected as GGC capacity may not
always be sufﬁcient to handle demand and also because different
preﬁxes within an AS, e.g., those that host the GCC servers, may
be handled differently. As illustrated in Figure 5.3 a small number
of ASes hosts servers that serve a large number of ASes. By far the
most popular AS is the ofﬁcial Google AS (AS15169) that served
more than 41.5K ASes in March and around 40.5K in August 2013.
In the top-10 we ﬁnd the YouTube AS, as well as small and large
transit providers that serve their customers. There is also a small
number of ASes that exclusively serve their client subnets from
GGC servers they host.
From our analysis we derive some important observations. First,
the Google content is not any more exclusively served by servers
Figure 3: # ASes served by ASes with Google servers (RIPE).
in Google ASes, as was reported in [12]. Second, GCCs have been
enabled in a signiﬁcant number of ASes over a ﬁve months period.
Third, within these ﬁve months the number of ASes that are served
by GGC servers in other ASes has been signiﬁcantly increased.
This trend has signiﬁcant implications to caching as Google con-
tent can be available in the same or a neighboring ASes. It also has
implication to peering as the presence of GGCs reduces the inter-
domain Google trafﬁc and it is now possible for smaller networks
to reduce their transit cost by either install GGCs or peer with ASes
that host GGCs.
To assess the stability of user-server mapping over time, we an-
alyze the returned IP addresses when asking back-to-back queries
over two days (May 3-4, 2013). We found that around 35% of the
preﬁxes are always served by a single /24 block over the 48 hours
period. Given the highly distributed infrastructure of Google one
may have expected larger churn. 44% of the query preﬁxes are
mapped to two /24 and a very small percentage to more than ﬁve
/24s. A possible explanation for this stable mapping is that Google
uses local load-balancers [24, 33]. We leave the study of temporal
dynamics in user-to-server mapping over longer periods and during
ﬂash crowds or other events as future work. Our future research
agenda also includes the study of the temporal changes in user-to-
server mapping not just for Google but also for other ECS adopters.
6. CONCLUSION
In this paper we show that the adoption of the EDNS-Client-
Subnet DNS extension (ECS) by major Internet companies offers
unique but most likely unintended measurement opportunities to
uncover some of their operational practices. Using early ECS adop-
ters like Google, Edgecast, and CacheFly as examples, our experi-
mental study shows how simple it is (using a single vantage point
as simple as a commodity PC) to (i) uncover the footprint of these
CDN/CP companies, (ii) observe them clustering clients, and (iii)
take snapshots of the user-to-server mappings. In addition, we point
out potential implications that ECS can have on the cacheability
of DNS responses by major DNS resolvers. We believe that the
tools developed and the traces collected in this work, made avail-
able to the research community, shed light on the deployment and
operation of CDNs given the central role they play in today’s Inter-
net. This work also highlights the need to increase the awareness
among current and future ECS adopters about the consequences of
enabling ECS.
Acknowledgments
We would like to thank our shepherd, Ethan Katz-Bassett, and the
anonymous reviewers for their valuable feedback. This work was
supported in part by the EU projects BigFoot (FP7-ICT-317858)
and CHANGE (FP7-ICT-257422), EIT Knowledge and Innovation
Communities program, and an IKY-DAAD award (54718944).
0100200300400500600700800Rank100101102103104105#MappedClient-ASes(log)Mar26Aug87. REFERENCES
[1] A Faster Internet Consortium.
http://www.afasterinternet.com.
[2] Alexa top sites.
http:///www.alexa.com/topsites.
[3] Google Global Cache.
http://ggcadmin.google.com/ggc.
[4] Google Public DNS. https:
//developers.google.com/speed/public-dns.
[5] Mapping CDN domains. http://b4ldr.wordpress.
com/2012/02/13/mapping-cdn-domains/.
[6] MaxMind, GeoIP databases.
http://www.maxmind.com.
[7] OpenDNS. http://www.opendns.com.
[8] RIPE Routing Information Service.
http://www.ripe.net/ris/.
[9] Routeviews Project, University of Oregon.
http://www.routeviews.org/.
[10] V. K. Adhikari, S. Jain, Y. Chen, and Z. L. Zhang.
Vivisecting YouTube: An Active Measurement Study. In
IEEE INFOCOM, 2012.
[11] B. Ager, W. Mühlbauer, G. Smaragdakis, and S. Uhlig.
Comparing DNS Resolvers in the Wild. In ACM IMC, 2010.
[12] B. Ager, W. Mühlbauer, G. Smaragdakis, and S. Uhlig. Web
Content Cartography. In ACM IMC, 2011.
[13] M. Axelrod. The Value of Content Distribution Networks.
AfNOG 9, 2008.
[14] M. Calder, X. Fan, Z. Hu, E. Katz-Bassett, J. Heidemann,
and R. Govindan. Mapping the Expansion of Google’s
Serving Infrastructure. In ACM IMC, 2013.
[15] N. Chatzis, G. Smaragdakis, J. Boettger, T. Krenc, and
A. Feldmann. On the beneﬁts of using a large IXP as an
Internet vantage point. In ACM IMC, 2013.
[16] L. Colitti, S. H. Gunderson, E. Kline, and T. Reﬁce.
Evaluating IPv6 adoption in the Internet. In PAM, 2010.
[17] C. Contavalli, W. van der Gaast, S. Leach, and E. Lewis.
Client subnet in DNS requests (IETF draft).
http://tools.ietf.org/html/
draft-vandergaast-edns-client-subnet-01.
[18] A. Dhamdhere and C. Dovrolis. Twelve Years in the
Evolution of the Internet Ecosystem. IEEE/ACM Trans.
Networking, 19(5), 2011.
[19] T. Flach, N. Dukkipati, A. Terzis, B. Raghavan, N. Cardwell,
Y. Cheng, A. Jain, S. Hao, E. Katz-Bassett, and R. Govindan.
Reducing Web Latency: the Virtue of Gentle Aggression. In
ACM SIGCOMM, 2013.
[20] M. J. Freedman, M. Vutukuru, N. Feamster, and
H. Balakrishnan. Geographic Locality of IP Preﬁxes. In
ACM IMC, 2005.
[21] Google. What is 1e100.net?
http://support.google.com/bin/answer.py?
hl=en&answer=174717.
[22] C. Huang, A. Wang, J. Li, and K. Ross. Measuring and
Evaluating Large-scale CDNs. In ACM IMC, 2008.
[23] B. Krishnamurthy and J. Wang. On Network-aware
Clustering of Web Clients. In ACM SIGCOMM, 2001.
[24] R. Krishnan, H. Madhyastha, S. Srinivasan, S. Jain,
A. Krishnamurthy, T. Anderson, and J. Gao. Moving Beyond
End-to-end Path Information to Optimize CDN Performance.
In ACM IMC, 2009.
[25] C. Labovitz, S. Lekel-Johnson, D. McPherson, J. Oberheide,
and F. Jahanian. Internet Inter-Domain Trafﬁc. In ACM
SIGCOMM, 2010.
[26] Z. Mao, C. Cranor, F. Douglis, M. Rabinovich,
O. Spatscheck, and J. Wang. A Precise and Efﬁcient
Evaluation of the Proximity Between Web Clients and Their
Local DNS Servers. In USENIX ATC, 2002.
[27] E. Nygren, R. K. Sitaraman, and J. Sun. The Akamai
Network: A Platform for High-performance Internet
Applications. SIGOPS Oper. Syst. Rev., 2010.
[28] J. S. Otto, M A. Sánchez, J. P. Rula, and F. E. Bustamante.
Content delivery and the natural evolution of DNS - Remote
DNS Trends, Performance Issues and Alternative Solutions.
In ACM IMC, 2012.
[29] Vern Paxson. Bro: A System for Detecting Network
Intruders in Real-Time. Com. Networks, 31(23–24), 1999.
[30] I. Poese, B. Frank, B. Ager, G. Smaragdakis, and
A. Feldmann. Improving Content Delivery using
Provider-aided Distance Information. In ACM IMC, 2010.
[31] I. Poese, S. Uhlig, M. A. Kaafar, B. Donnet, and B. Gueye. IP
Geolocation Databases: Unreliable? ACM CCR, 41(2), 2011.
[32] M A. Sanchez, J. .S. Otto, Z. S. Bischof, D. R. Choffnes, ,
F. E. Bustamante, B. Krishnamurthy, and W. Willinger. Dasu:
Pushing Experiments to the Internet’s Edge. In
USENIX/ACM NSDI, 2013.
[33] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and
M. Ammar. Answering What-if Deployment and
Conﬁguration Questions with Wise. In ACM SIGCOMM,
2009.
[34] I. Trestian, S. Ranjan, A. Kuzmanovic, and A. Nucci.
Unconstrained Endpoint Proﬁling (Googling the Internet). In
ACM SIGCOMM, 2008.
[35] S. Triukose, Z. Wen, and M. Rabinovich. Measuring a
Commercial Content Delivery Network. In WWW, 2011.
[36] P. Vixie. Extension Mechanisms for DNS (EDNS0). RFC
2671 (Proposed Standard), 1999.
[37] P. Vixie. DNS Complexity. ACM Queue, 5(3):24–29, 2007.
[38] P. Vixie. What DNS is Not. Comm. ACM, 52(12):43–47,
2009.