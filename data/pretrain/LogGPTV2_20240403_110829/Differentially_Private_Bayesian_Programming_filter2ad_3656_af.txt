### Type Definitions and Metrics

The type definitions and metrics are as follows:
- \( \text{tl} :: \text{B list} \mid l \Phi l \Rightarrow \text{MSD,0tx} :: [0, 1] \mid u \)
- \( \text{MSD,ζtx} :: [0, 1] \mid u \)

The choice of which metric to use is ultimately left to the user. This example can be extended to the Dirichlet distribution. More details about the Dirichlet distribution are provided in the supplementary material section. Specifically, Lemma 6.1 can be generalized to arbitrary Dirichlet distributions:

**Lemma 6.3.** Let \( k \in \mathbb{N}_{\geq 2} \), and let \( d_1, d_2 : [k] \) list with \( d_1 \Phi d_2 \). Let \( a_1, a_2, \ldots, a_k \in \mathbb{R}^+ \). Let \( \Pr(\xi) = \text{Dirichlet}(a_1, a_2, \ldots, a_k) \). Then,
\[ \Delta_{HD}(\Pr(\xi \mid d_1), \Pr(\xi \mid d_2)) \leq 1 - \frac{\pi}{4a} = \rho. \]

Using this lemma, we can assign the following type to the program:

- \( \text{tl} :: [3] \text{ list} \mid l \Phi l \Rightarrow \text{MHD,0tx} :: [0, 1]^2 \mid u \)
- \( \text{MHD,ρtx} :: [0, 1]^2 \mid u \)

### Program Example

Consider the following recursive function `learnP`:

```haskell
1. let rec learnP db prior = match dbn with
2. | [] -> prior
3. | d::dbs -> mlet rec = (learnP dbs prior) in observe
4.   (fun r s -> mlet z = ran multinomial(r, s) in
5.    return (d = z)) rec
```

Similarly, for the function `learnBias`:

```haskell
1. let rec learnBias db prior = match dbn with
2. | [] -> prior
3. | d::dbs -> mlet rec = (learnBias dbs prior) in observe
4.   (fun r -> mlet z = ran bernoulli(r) in return (d = z)) rec
```

By adding noise to the output of the inference process using the sensitivity with respect to the Hellinger distance, we can obtain a \((\rho, 0)\)-differential privacy guarantee.

### Related Work

#### Differential Privacy and Bayesian Inference

Our system targets programs that combine differential privacy and Bayesian inference, both of which are active areas of research. The intersection of these topics is particularly popular. We summarize the most well-known work and refer interested readers to surveys for more detailed information (Dwork and Roth [17] for differential privacy, Bishop [9] for Bayesian inference).

Blum et al. [10] and Dwork et al. [18] introduced differential privacy, a worst-case notion of statistical privacy, initiating intense research interest in developing differentially private algorithms. The original works proposed the Laplace and Gaussian mechanisms, while McSherry and Talwar [31] introduced the exponential mechanism. Recently, researchers have explored how to ensure differential privacy when performing Bayesian inference, a foundational technique in machine learning. Three main approaches have been investigated:
1. Adding noise directly to the samples and then performing inference.
2. Performing inference on the private data and then adding noise to the parameters.
3. Outputting samples from the posterior rather than the entire posterior distribution.

#### Formal Verification for Differential Privacy

In parallel with the development of private algorithms, formal verification researchers have proposed various techniques for verifying differential privacy. For a comprehensive discussion, see the survey by Barthe et al. [8]. Many of these techniques rely on the composition properties of privacy, though there are some exceptions [7]. Early systems were based on runtime verification of privacy [30], while later systems used linear type systems [22, 34]. There is also extensive work on relational program logics for differential privacy [2–4] and techniques for verifying privacy in standard Hoare logic using product programs [5]. Our system is most closely related to HOARe2, a relational refinement type system proposed by Barthe et al. [6], which has been used for verifying differential privacy of algorithms and more general relational properties like incentive compatibility from mechanism design. However, it cannot model probabilistic inference.

#### Probabilistic Programming

Probabilistic programming research emerged in the 1960s and 1970s and is now a very active area. Relevant to our work is the research in probabilistic programming for machine learning and statistics. Many probabilistic programming languages have been designed, including WinBUGS [29], IBAL [33], Church [23], Infer.net [32], Tabular [25], Anglican [37], and Dr. Bayes [38]. Our goal is not to provide a new language but to propose a framework for reasoning about differential privacy in such languages. For instance, we compiled programs written in Tabular into PrivInfer to verify differential privacy. Another related work is by Adams and Jacobs [1], proposing a type theory for Bayesian inference. While technically different, it shares the same goal of providing reasoning principles for Bayesian inference. Our work considers a probabilistic PCF for discrete distributions and aims to extend these techniques to higher-order languages with continuous distributions and conditioning.

### Conclusion

We have presented PrivInfer, a type-based framework for differentially private Bayesian inference. Our framework allows writing data analysis as functional programs for Bayesian inference and adding noise in different ways using various metrics. Additionally, it supports reasoning about general f-divergences for Bayesian inference. Future directions include exploring the use of this approach to ensure robustness for Bayesian inference and other machine learning techniques, ensuring differential privacy using conditions over the prior and likelihood, and investigating further uses of f-divergences for improving the utility of differentially private Bayesian learning. On the programming language side, extending our framework to continuous distributions following Sato's approach [35] would be interesting. We believe that the intersection of programming languages, machine learning, and differential privacy will yield many exciting results.

### References

[1] R. Adams and B. Jacobs. A type theory for probabilistic and Bayesian reasoning. CoRR, abs/1511.09230, 2015.
[2] G. Barthe and F. Olmedo. Beyond differential privacy: Composition theorems and relational logic for f-divergences between probabilistic programs. In ICALP, 2013.
[3] G. Barthe, B. K¨opf, F. Olmedo, and S. Zanella-B´eguelin. Probabilistic Relational Reasoning for Differential Privacy. In POPL, 2012.
[4] G. Barthe, G. Danezis, B. Gr´egoire, C. Kunz, and S. Zanella B´eguelin. Verified computational differential privacy with applications to smart metering. In CSF, 2013.
[5] G. Barthe, M. Gaboardi, E. J. Gallego Arias, J. Hsu, C. Kunz, and P.-Y. Strub. Proving differential privacy in Hoare logic. In CSF, 2014.
[6] G. Barthe, M. Gaboardi, E. J. G. Arias, J. Hsu, A. Roth, and P. Strub. Higher-order approximate relational refinement types for mechanism design and differential privacy. In POPL, 2015.
[24] A. D. Gordon, M. Aizatulin, J. Borgstr¨om, G. Claret, T. Graepel, A. V. Nori, S. K. Rajamani, and C. V. Russo. A model-learner pattern for Bayesian reasoning. In POPL, 2013.
[7] G. Barthe, M. Gaboardi, B. Gr´egoire, J. Hsu, and P.-Y. Strub. Proving differential privacy via probabilistic couplings. In LICS, 2016.
[25] A. D. Gordon, T. Graepel, N. Rolland, C. V. Russo, J. Borgstr¨om, and J. Guiver. Tabular: a schema-driven probabilistic programming language. In POPL, 2014.
[8] G. Barthe, M. Gaboardi, J. Hsu, and B. Pierce. Programming language techniques for differential privacy. ACM SIGLOG News, 2016.
[9] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). 2006. ISBN 0387310738.
[10] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: The SuLQ framework. In PODS, 2005.
[11] J. Borgstr¨om, U. D. Lago, A. D. Gordon, and M. Szymczak. A lambda-calculus foundation for universal probabilistic programming. In ICFP, 2016.
[12] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. 2011.
[13] I. Csisz´ar. Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis der Ergodizitat von Markoﬀschen Ketten. Magyar. Tud. Akad. Mat. Kutat´o Int. K¨ozl, 1963.
[14] I. Csisz´ar and P. Shields. Information theory and statistics: A tutorial. Foundations and Trends in Communications and Information Theory, 2004.
[26] M. Hardt, K. Ligett, and F. McSherry. A simple and practical algorithm for differentially private data release. In NIPS, 2012.
[27] M. Hicks, G. M. Bierman, N. Guts, D. Leijen, and N. Swamy. Polymonadic programming. In MSFP, 2014.
[28] S. Katsumata. Parametric effect monads and semantics of effect systems. In POPL, 2014.
[29] D. J. Lunn, A. Thomas, N. Best, and D. Spiegelhalter. WinBUGS - A Bayesian modelling framework: Concepts, structure, and extensibility. Statistics and Computing, 2000.
[30] F. McSherry. Privacy integrated queries: an extensible platform for privacy-preserving data analysis. In International Conference on Management of Data, 2009.
[31] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, 2007.
[32] T. Minka, D. Knowles, J. Winn, J. Guiver, and URL http://research.microsoft.com/infernet. MSR. Infer.NET 2.5, 2012.
[33] A. Pfeffer. IBAL: A Probabilistic Rational Programming Language. In IJCAI, 2001.
[15] D. K. Dey and L. R. Birmiwal. Robust Bayesian analysis using divergence measures. Statistics & Probability Letters, 1994.
[34] J. Reed and B. C. Pierce. Distance Makes the Types Grow Stronger: A Calculus for Differential Privacy. In ICFP, 2010.
[16] C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. I. P. Rubinstein. Robust and Private Bayesian Inference. In ALT, 2014.
[17] C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 2014.
[18] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, 2006.
[19] C. Dwork, G. N. Rothblum, and S. P. Vadhan. Boosting and differential privacy. In FOCS, 2010.
[35] T. Sato. Approximate Relational Hoare Logic for Continuous Random Samplings. CoRR, abs/1603.01445.
[36] S. Staton, H. Yang, C. Heunen, O. Kammar, and F. Wood. Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints. In LICS, 2016.
[37] D. Tolpin, J. van de Meent, and F. Wood. Probabilistic Programming in Anglican. In ECML PKDD, 2015.
[38] N. Toronto, J. McCarthy, and D. V. Horn. Running Probabilistic Programs Backwards. In ESOP, 2015.
[39] O. Williams and F. McSherry. Probabilistic Inference and Differential Privacy. In NIPS, 2010.
[20] H. Ebadi, D. Sands, and G. Schneider. Differential privacy: Now it’s getting personal. POPL, 2015.
[21] F. Eigner and M. Maﬀei. Differential privacy by typing in security protocols. In CSF, 2013.
[22] M. Gaboardi, A. Haeberlen, J. Hsu, A. Narayan, and B. C. Pierce. Linear dependent types for differential privacy. In POPL, 2013.
[23] N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church: a language for generative models. In UAI, 2008.
[40] J. Zhang, G. Cormode, C. M. Procopiuc, D. Srivastava, and X. Xiao. PrivBayes: Private data release via Bayesian networks. In SIGMOD, 2014.
[41] Z. Zhang, B. I. P. Rubinstein, and C. Dimitrakakis. On the Differential Privacy of Bayesian Inference. In AAAI, 2016.
[42] S. Zheng. Bayesian inference, differential privacy. Bachelor’s thesis, Harvard College, 2015. URL http://nrs.harvard.edu/urn-3:HUL.InstRepos:14398533.