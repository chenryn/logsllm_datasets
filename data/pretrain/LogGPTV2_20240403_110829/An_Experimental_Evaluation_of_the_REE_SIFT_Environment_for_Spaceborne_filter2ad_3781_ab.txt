daemons.  The MPI process with rank 0 is also a child of its 
Execution  ARMOR.    Because  of  the  parent-child  relationship, 
crash detection for child processes is implemented by having a 
thread within the parent process block on a waitpid() call to 
the operating system.  Because the Execution  ARMORs do not 
directly  launch  MPI  processes  with  ranks  1  through  n,  crash 
failures in these MPI processes are also detected through other 
means, which are discussed in the next section. 
2.3  Error Detection Hierarchy 
Node  and  daemon  errors. 
  The  FTM  periodically 
exchanges heartbeat messages with each daemon (every 10 s in 
our experiments) to detect node crashes and hangs.  If the FTM 
does  not  receive  a  response  by  the  next  heartbeat  round,  it 
assumes that the node has failed.  A daemon failure is treated as 
a node failure because the local  ARMORs cannot communicate 
with other ARMORs in the environment if the daemon fails. 
Table 1: Steps for running an REE application 
Initializing the SIFT environment: 
1. 
The SCC issues commands that: 
a. 
Install daemon processes on each node that is to be part of the SIFT 
environment. 
Install the FTM process through a daemon on one of the nodes. 
Register all daemon processes with the FTM.  The FTM instructs the 
daemon on the first registered node to install a Heartbeat ARMOR. 
b. 
c. 
Preparing SIFT environment for executing applications: 
2. 
The SCC submits the application to the FTM for execution, specifying the nodes 
on which it should execute. 
The FTM instructs the appropriate daemons on the nodes to install Execution 
ARMORs, one for each prospective MPI process. 
3. 
Executing the MPI application: 
4. 
5. 
6. 
7. 
8. 
The FTM instructs one Execution ARMOR to launch the MPI process with rank 0.  
This process becomes the child of the Execution ARMOR. 
The MPI process with rank 0—per the MPI implementation’s protocol—
remotely launches the remaining MPI processes on the other nodes. 
The MPI process with rank 0 sends the process IDs of the other MPI processes to 
the appropriate Execution ARMORs via the FTM. 
The Execution ARMORs for processes with ranks 1 through n establish 
communication channels with their respective MPI processes. 
The application executes, periodically sending progress indicator updates to the 
local Execution ARMOR. 
The FTM periodically heartbeats the registered daemons. 
9. 
10.  The Heartbeat ARMOR heartbeats the FTM. 
Cleaning up after application completes: 
11.  The MPI processes terminate, notifying their local Execution ARMORs. 
12.  The Execution ARMOR for the rank 0 process forwards the application 
termination notification to the FTM. 
13.  Upon receiving all termination notifications, the FTM uninstalls the Execution 
ARMORs and reports to the SCC that the application has successfully completed. 
network
Daemon
Daemon
Daemon
Daemon
Heartbeat
ARMOR
Execution
ARMOR
Execution
ARMOR
FTM
Execution
ARMOR
Execution
ARMOR
Legend:
Heartbeats
Progress Indicators
Recovery
SIFT
Interface
Rover 
Process
(rank 0)
Node 1
SIFT
Interface
Rover
Process
(rank 1)
SIFT
Interface
OTIS
Process
(rank 0)
SIFT
Interface
OTIS
Process
(rank 1)
Node 2
Node 3
Node 4
Figure 1: SIFT architecture for executing two MPI 
applications on a four-node network. 
ARMOR errors.  Each ARMOR contains a set of assertions on 
its  internal  state,  including  range  checks,  validity  checks  on 
data  (e.g.,  a  valid  ARMOR  ID),  and  data  structure  integrity 
checks.    Other  internal  self-checks  available  to  the  ARMORs 
include  preemptive  control  flow  checking,  I/O  signature 
checking, and deadlock/livelock detection [2].  In order to limit 
error propagation, the ARMOR kills itself when an internal check 
detects  an  error.    The  daemon  detects  crash  failures  in  the 
ARMORs on the node via operating system calls.  To detect hang 
failures, 
the 
experiments)  sends  “Are-you-alive?”  messages  to  its  local 
ARMORs. 
the  daemon  periodically  (every  10  s 
REE  applications.    All  application  crash  failures  are 
detected by the local Execution ARMOR.  Crash failures in the 
MPI  process  with  rank  0  can  be  detected  by  the  Execution 
ARMOR  through  operating  system  calls  (i.e.,  waitpid).    The 
in 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply. 
other  Execution  ARMORs  periodically  check  that  their  MPI 
processes (ranks 1 through n) are still in the operating system’s 
process  table.    If  not,  it  concludes  that  the  application  has 
crashed.    An  application  process  notifies  the  local  Execution 
ARMOR  through  its  communication  channel  before  exiting 
normally so that the ARMOR does not misinterpret this exit as an 
abnormal termination. 
A polling technique is used to detect application hangs in 
which  the  Execution  ARMOR  periodically  checks  for  progress 
indicator updates sent by the application.  A progress indicator 
is an “I’m-alive” message containing information that denotes 
application  progress  (e.g.,  a  loop  iteration  counter).    If  the 
Execution ARMOR does not receive a progress indicator within 
an application-specific time period, the ARMOR concludes that 
the  application  process  has  hung.  Since  the  texture  analysis 
program  executes  functions 
in  an  external  fast  Fourier 
transform (FFT) library for about 20 s per filter, the Execution 
ARMOR cannot check for application progress more often than 
every 20 seconds.  Finer checking granularity can be achieved 
by  instrumenting  the  FFT  functions  with  progress  indicators.  
The application can also have internal checks as well, such as 
algorithm-based 
its 
computation  [10].    As  with  the  ARMOR  self-checks,  the 
application  kills  itself  if  it  cannot  correct  errors  that  are 
detected through internal checks. 
2.4  Error Recovery 
to  protect 
tolerance 
(ABFT), 
fault 
Nodes.    The  FTM  migrates  the  ARMOR  and  application 
processes  that  were  executing  on  the  failed  node  to  other 
working nodes in the SIFT environment. 
ARMORs.    Instead  of  consuming  network  bandwidth  by 
reloading  the  ARMOR  executable  binaries  to  recover  a  failed 
ARMOR,  the  daemon  copies  its  own  executable  image  to  the 
address  space  of  the  recovered  ARMOR.    This  is  possible 
because  all  SIFT  processes  share  a  common  ARMOR 
architecture.    The  recovered  ARMOR  is  then  configured  by 
enabling  and  disabling  the  appropriate  elements  within  the 
process  (e.g.,  enabling  Execution  ARMOR  elements  while 
disabling the daemon-specific elements)2. 
To  protect  the  ARMOR  state  against  process  failures,  a 
checkpointing  technique  called  microcheckpointing  [23]  is 
used.    Microcheckpointing  leverages  the  modular  element 
composition of the ARMOR process to incrementally checkpoint 
state  on  an  element-by-element basis.  To process a message, 
an ARMOR sequentially delivers the events in the message to the 
elements that have subscribed to the events.  After each event 
delivery,  the  state  of  the  affected  element  is  copied  to  a 
checkpoint  buffer  within  the  ARMOR  process.    Because  each 
element  is  assigned  a  disjoint  region  within  the  checkpoint 
buffer and because an  element only processes one event at a 
time,  several  threads  can  concurrently  update  the  checkpoint 
buffer without interference. 
to  make 
the  ARMOR  decides 
the  checkpoint 
permanent,  it  copies  the  checkpoint  buffer  to  the  memory 
region that emulates local nonvolatile RAM.  Data stored in the 
nonvolatile  RAM  survives  node  resets  and  is  available  for 
process  recovery.    To  tolerate  node  failures,  the  checkpoints 
must  be  stored  in  a  location  that  is  independent  of  the  failed 
When 
2   If the ARMOR repeatedly fails after being recovered in this manner, then 
the  error  may  reside  in  the  daemon’s  text  segment,  requiring  that  the 
ARMOR’s image be reloaded from disk. 
node.    In  the  experimental  implementation,  checkpoints  are 
saved  after  every  ARMOR  message  transmission to ensure that 
the set of ARMOR checkpoints in the system is always globally 
consistent;  thus,  only  a  single  process  must  be  rolled  back in 
the event of an ARMOR failure. 
REE Applications.  On detecting an application failure, the 
Execution  ARMOR  notifies  the  FTM  to  initiate  recovery.    The 
version  of  MPI  used  by  JPL  on  the  REE  testbed  precludes 
individual  MPI  processes  from  being  restarted  within  an 
application; therefore, the FTM instructs all Execution ARMORs 
to 
the 
application.    The  application  executable  binaries  must  be 
reloaded from the remote disk during recovery. 
3 
Injection Experiments 
Error  injection  experiments  into  the  application  and  SIFT 
their  MPI  processes  before  restarting 
terminate 
processes were conducted to: 
1.  Stress the detection and recovery mechanisms of the SIFT 
2.  Determine  the  failure  dependencies  among  SIFT  and 
3.  Measure  the  SIFT  environment  overhead  on  application 
environment. 
application processes. 
performance. 
4.  Measure  the  overhead  of  recovering  SIFT  processes  as 
seen by the application. 
5.  Study the effects of error propagation and the effectiveness 
of internal self-checks in limiting error propagation. 
The experiments used NFTAPE [22], a software framework 
for conducting injection campaigns. 
3.1  Error Models 
The error models used the injection experiments represent a 
combination  of  those  employed  in  several  past  experimental 
studies [9] and those proposed by JPL engineers [4].  
SIGINT/SIGSTOP.    These  signals  were  used  to  mimic 
“clean” crash and hang failures as described in the introduction. 
Register  and  text-segment  errors.    Fault  analysis  has 
predicted  that  the  most  prevalent  faults  in  the  targeted 
spaceborne environment will be single-bit memory and register 
faults,  although  shrinking  feature  sizes  have  raised  the 
likelihood  of  clock  errors  and  multiple-bit  flips  in  future 
technologies [4].  Since the experiments aimed at assessing the 
effectiveness  of  the  SIFT  environment  in  recovering  from 
failures when they occur (as opposed to assessing coverage or 
likelihood of failure scenarios), register and text-segment errors 
were  injected  with  the  purpose  of  inducing  failures.    Several 
error    injections  were  uniformly  distributed  within  each  run 
since each injection was unlikely to cause an immediate failure, 
and only the most frequently used registers and functions in the 
text segment were targeted for injection. 
Heap  errors.    Heap  injections  were  used  to  study  the 
effects  of  error  propagation.    One  error  was  injected  per  run 
into non-pointer data values only, and the effects of the error 
were traced through the system. 
Errors were not injected into the operating system since our 
experience  has  shown  that  kernel  injections  typically  led  to  a 
crash, led to a hang, or had no impact.  Maderia et al. [15] used 
the same REE testbed to examine the impact of transient errors 
on LynxOS. 
3.2  Definitions and Measurements 
System, experiment, and run.  We use the term system to 
refer to the REE cluster and associated software (i.e., the SIFT 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply. 
environment  and  applications).    The  system  does  not  include 
the  radiation-hardened  SCC  or  communication  channel  to  the 
ground.    An  error  injection  experiment  targeted  a  specific 
process  (application  process,  FTM,  Execution  ARMOR,  or 
Heartbeat  ARMOR)  using  a  particular  error  model.    For  each 
process/error  model  pair,  a  series  of  runs  were  executed  in 
which one or more errors were injected into the target process. 
Activated  errors  and  failures.    An  injection  causes  an 
error  to  be  introduced  into  the  system  (e.g.,  corruption  at  a 
selected  memory  location  or  corruption  of  the  value  in  a 
register).  An error is said to be activated if program execution 
accesses  the  erroneous  value.    A  failure  refers  to  a  process 
deviating from its expected (correct) behavior as determined by 
a run without fault injection.  The application can also fail by 
producing  output  that falls outside acceptable tolerance limits 
as  defined  by  an  external  application-provided  verification 
program. 
recognize 
A  system  failure  occurs  when  either  (1)  the  application 
cannot  complete  within  a  predefined  timeout  or  (2)  the  SIFT 
environment  cannot 
the  application  has 
completed  successfully.    These  failures  are  caused  by  errors 
that propagate to an ARMOR’s checkpoint or to other processes.  
System  failures  require  that  the  SCC  reinitialize  the  SIFT 
environment  before  continuing,  but  they  do  not  threaten  the 
SCC or spacecraft integrity3. 
that 
Recovery time.  Recovery time is the interval between the 
time  at  which  a  failure  is  detected  and  the  time  at  which  the 
target process restarts.  For ARMOR processes, this includes the 
time required to restore the ARMOR’s state from checkpoint.  In 
the case of an application failure, the time lost to rolling back to 
the most recent application checkpoint is accounted for in the 
application’s total execution time, not in the recovery time for 
the application. 
Perceived  application  execution  time.    The  perceived 
execution  time  is  the  interval  between  the  time  at  which  the 
SCC submits an application for execution and the time at which 
the  SIFT  environment  reports  to  the SCC that the application 
has completed. 
Actual application execution time.  The actual execution 
time  is  the  interval  between  the  start  and  the  end  of  the 
application.    The  difference  between  perceived  and  actual 
execution  time  accounts  for  the  time  required  to  install  the 
Execution ARMORs before running the application and the time 
required to uninstall the Execution ARMORs after the application 
completes (see Figure 2).  This is a fixed overhead independent 
of the actual application execution time.  The REE applications 
envisioned to take advantage of this environment are expected 
to  be  long-running,  so  the  performance  impact  of  the  fixed 
overhead will be less apparent than in our testbed applications 
that  use  small  input  data  sets.    We  differentiate  between  the 
perceived and actual execution times because it is important to 
assess how the SIFT environment responds to errors during the 
setup and takedown phases of an application’s execution. 
Baseline  application  execution  time.    In  the  injection 
experiments,  the  perceived  and  actual  application  execution 
times  are  compared  to  a  baseline  measurement  in  order  to 
3   While  the  vast  majority  of  failures  in  the  SIFT  environment  will  not 
affect the trusted SCC, in reality there exists a nonzero probability that 
the SCC can be impacted by SIFT failures.  We discount this possibility 
in  the  paper  because  there  is  not  a  full-fledged  SCC  available  for 
conducting such an analysis. 
determine  the  performance  overhead  added  by  the  SIFT 
environment  and  recovery. 
  Two  measures  of  baseline 
application performance are used: (1) the application executing 
without the SIFT environment and without fault injection and 