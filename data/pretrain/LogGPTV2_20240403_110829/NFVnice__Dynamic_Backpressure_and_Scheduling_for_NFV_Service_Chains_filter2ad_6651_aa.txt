title:NFVnice: Dynamic Backpressure and Scheduling for NFV Service Chains
author:Sameer G. Kulkarni and
Wei Zhang and
Jinho Hwang and
Shriram Rajagopalan and
K. K. Ramakrishnan and
Timothy Wood and
Mayutan Arumaithurai and
Xiaoming Fu
NFVnice: Dynamic Backpressure and Scheduling
for NFV Service Chains
Sameer G Kulkarni∗⋆, Wei Zhang‡, Jinho Hwang§, Shriram Rajagopalan§, K.K. Ramakrishnan†,
Timothy Wood‡, Mayutan Arumaithurai∗ and Xiaoming Fu∗
∗University of Göttingen, Germany, ‡George Washington University,
§IBM T J Watson Research Center, †University of California, Riverside.
ABSTRACT
Managing Network Function (NF) service chains requires careful
system resource management. We propose NFVnice, a user space
NF scheduling and service chain management framework to pro-
vide fair, efficient and dynamic resource scheduling capabilities on
Network Function Virtualization (NFV) platforms. The NFVnice
framework monitors load on a service chain at high frequency
(1000Hz) and employs backpressure to shed load early in the ser-
vice chain, thereby preventing wasted work. Borrowing concepts
such as rate proportional scheduling from hardware packet sched-
ulers, CPU shares are computed by accounting for heterogeneous
packet processing costs of NFs, I/O, and traffic arrival character-
istics. By leveraging cgroups, a user space process scheduling ab-
straction exposed by the operating system, NFVnice is capable of
controlling when network functions should be scheduled. NFVnice
improves NF performance by complementing the capabilities of the
OS scheduler but without requiring changes to the OS’s schedul-
ing mechanisms. Our controlled experiments show that NFVnice
provides the appropriate rate-cost proportional fair share of CPU
to NFs and significantly improves NF performance (throughput
and loss) by reducing wasted work across an NF chain, compared
to using the default OS scheduler. NFVnice achieves this even for
heterogeneous NFs with vastly different computational costs and
for heterogeneous workloads.
CCS CONCEPTS
• Networks → Network resources allocation; Network man-
agement; Middle boxes / network appliances; Packet scheduling;
KEYWORDS
Network Functions (NF), Backpressure, NF-Scheduling, Cgroups.
ACM Reference format:
S.G. Kulkarni, W. Zhang, J. Hwang, S. Rajagopalan, K.K. Ramakrishnan, T.
Wood, M. Arumaithurai, X. Fu. 2017. NFVnice: Dynamic Backpressure and
Scheduling for NFV Service Chains. In Proceedings of SIGCOMM ’17, Los
Angeles, CA, USA, August 21-25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098828
⋆Work done while Sameer was at University of California, Riverside, as part of sec-
ondment in the EU FP7 Marie Curie CleanSky ITN Research Project.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08...$15.00
https://doi.org/10.1145/3098822.3098828
1 INTRODUCTION
Network Function Virtualization (NFV) seeks to implement network
functions and middlebox services such as firewalls, NAT, proxies,
deep packet inspection, WAN optimization, etc., in software in-
stead of purpose-built hardware appliances. These software based
network functions can be run on top of commercial-off-the-shelf
(COTS) hardware, with virtualized network functions (NFs). Net-
work functions, however, often are chained together [20], where a
packet is processed by a sequence of NFs before being forwarded
to the destination.
The advent of container technologies like Docker [34] enables
network operators to densely pack a single NFV appliance (VM/bare
metal) with large numbers of network functions at runtime. Even
though NFV platforms are typically capable of processing packets
at line rate, without efficient management of system resources in
such densely packed environments, service chains can result in
serious performance degradation because bottleneck NFs may
drop packets that have already been processed by upstream NFs,
resulting in wasted work in the service chain.
NF processing has to address a combination of requirements.
Just as hardware switches and routers provide rate-proportional
scheduling for packet flows, an NFV platform has to provide a fair
processing of packet flows. Secondly, the tasks running on the NFV
platform may have heterogeneous processing requirements that OS
schedulers (unlike hardware switches) address using their typical
fair scheduling mechanisms. OS schedulers, however, do not treat
packet flows fairly in proportion to their arrival rate. Thus, NF pro-
cessing requires a re-thinking of the system resource management
framework to address both these requirements. Moreover, standard
OS schedulers: a) do not have the right metrics and primitives to
ensure fairness between NFs that deal with the same or different
packet flows; and b) do not make scheduling decisions that account
for chain level information. If the scheduler allocates more process-
ing to an upstream NF and the downstream NF becomes overloaded,
packets are dropped by the downstream NF. This results in ineffi-
cient processing and wasting the work done by the upstream NF.
OS schedulers also need to be adapted to work with user space
data plane frameworks such as Intel’s DPDK [1]. They have to be
cognizant of NUMA (Non-uniform Memory Access) concerns of
NF processingand the dependencies among NFs in a service chain.
Determining how to dynamically schedule NFs is key to achieving
high performance and scalability for diverse service chains, espe-
cially in a scenario where multiple NFs are contending for a CPU
core1
1While CPU core counts are increasing in modern hardware, they are likely to
remain a bottleneck resource, especially when service chains are densely packed into
a single machine (as is often the case with several proposed approaches [23, 52]).
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
Hardware routers and switches that employ sophisticated sched-
uling algorithms such as rate proportional scheduling [40, 50] have
predictable performance per-packet, because processing resources
are allocated fairly to meet QoS requirements and bottlenecks are
avoided by design. However, NFV platforms are necessarily differ-
ent because: a) the OS scheduler does not know a priori, the capacity
or processing requirements for each NF; b) an NF may have variable
per-packet costs (e.g., some packets may trigger DNS lookup, which
are expensive to process, and others may just be an inexpensive
header match). With NFV service chains, there is a need to be aware
of the computational demands for packet processing. There can
also be sporadic blocking of NFs due to I/O (read/write) stalls.
A further consideration is that routers and switches ‘simply’
drop packets when congested. However, an NF in a service chain
that drops packets can result in considerable wasted processing at
NFs earlier in the chain. These wasted resources could be gainfully
utilized by other NFs being scheduled on the same CPU core to
process other packet flows.
We posit that a scheduling framework for NFV service chains
has to simultaneously account for both task level scheduling on
processing cores and packet level scheduling. This combined prob-
lem is what poses a challenge: When you get a packet, you have to
decide which task has to run, and also which packets to process, and
for how long.
To solve these problems we propose NFVnice, an NFV manage-
ment framework that provides fair and efficient resource allocations
to NF service chains. NFVnice focuses on the scheduling and con-
trol problems of NFs running on shared CPU cores, and considers
a variety of realistic issues such as bottlenecked NFs in a chain,
and the impact of NFs that perform disk I/O accesses, which natu-
rally complicate scheduling decisions. NFVnice makes the following
contributions:
• Automatically tuning CPU scheduling parameters in order
to provide a fair allocation that weighs NFs based on both
their packet arrival rate and the required computation cost.
• Determining when NFs are eligible to get a CPU share and
when they need to yield the CPU, entirely from user space,
improving throughput and fairness regardless of the kernel
scheduler being used.
• Leveraging the scheduling flexibility to achieve backpres-
sure for service chain-level congestion control, that avoids
unnecessary packet processing early in a chain if the packet
might be dropped later on.
• Extending backpressure to apply not only to adjacent NFs
in a service chain, but for full service chains and managing
congestion across hosts using ECN.
• Presenting a scheduler-agnostic framework that does not
require any operating system or kernel modifications.
We have implemented NFVniceon top of OpenNetVM [54], a
DPDK-based NFV platform that runs NFs in separate processes
or containers to facilitate deployment. Our evaluation shows that
NFVnice can support different kernel schedulers, while substan-
tially improving throughput and providing fair CPU allocation
based on processing requirements. In controlled experiments using
the vanilla CFS BATCH [37] scheduler, NFVnice reduces packet
drops from 3Mpps (million packets per second) to just 0.01Mpps
during overload conditions. NFVnice provides performance isola-
tion for TCP flows when there are competing UDP flows, improving
throughput of TCP flows from 30Mbps to 4Gbps, without penal-
izing UDP flows, by avoiding wasted work. While this is scenario
dependent, we believe the performance benefits of NFVnice are
compelling. Further, our evaluations demonstrate that NFVnice,
because of the dynamic backpressure, is resilient to the variability
in packet-processing cost of the NFs, yielding considerable improve-
ment in throughput even for the large service chains (including
chains that span multiple cores).
2 BACKGROUND AND MOTIVATION
2.1 Diversity, Fairness, and Chain Efficiency
The middleboxes that are being deployed in industry are diverse
in their applications as well as in their complexity and processing
requirements. ETSI standards [13] show that NFs have dramatically
different processing and performance requirements. Measurements
of existing NFs show the variation in CPU demand and per packet
latency: some NFs have per-core throughput in the order of million
packets per second (Mpps), e.g., switches; others have throughputs
as low as a few kilo pps, e.g., encryption engines.
Fair Scheduling: Determining how to allocate CPU time to
network functions in order to provide fair and efficient chain per-
formance despite NF diversity is the focus of our work. Defining
“fairness” when NFs may have completely different requirements
or behavior can be difficult. A measure of fairness that we leverage
is the work on Rate Proportional Servers [40, 50], that apportion
resources (CPU cycles) to NFs based on the combination of an NF’s
arrival rate and its processing cost. Intuitively, if either one of these
factors is fixed, then we expect its CPU allocation to be proportional
to the other metric. For example, if two NFs have the same compu-
tation cost but one has twice the arrival rate, then we want it to
have twice the output rate relative to the second NF. Alternatively,
if the NFs have the same arrival rate, but one requires twice the
processing cost, then we expect the heavy NF to get about twice
as much CPU time, resulting in both NFs having the same output
rate. This definition of fairness can of course be supplemented with
a prioritization factor, allowing an understandable and consistent
way to provide differentiated service for NFs.
Unfortunately, standard CPU schedulers do not have sufficient
information to allocate resources in a way that provides rate-cost
proportional fairness. CPU schedulers typically try to provide fair
allocation of processing time, but if computation costs vary between
NFs this cannot provide rate-cost fairness. Therefore, NFVnice must
enhance the scheduler with more information so that it can appro-
priately allocate CPU time to provide correctly weighted alloca-
tions.
We adopt the notion of rate-cost proportional fairness for two
fundamental reasons: i) it not only seeks to maximize the through-
put for a given load across NFs, but even in the worst case scenarios
(highly uneven and high overload across competing NFs), it en-
sures that all competing NFs get a minimal CPU share necessary
to progress the NFs; and ii) the rate-cost proportional fairness is
general and flexible, so that it can be tuned to meet the QoS policies
desired by the operator. Further, the approach ensures that when
contending NFs include malicious NFs (those that fail to yield), or
NFVnice: Dynamic Backpressure and Scheduling for NF Chains
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
(a) Homogeneous NFs
(b) Heterogeneous NFs
Figure 1: The scheduler alone is unable to provide fair resource allocations that account for processing cost and load.
misbehaving NFs (get stuck in a loop making no progress), such
NFs do not consume the CPU excessively, impeding the progress
of other NFs. While the Linux default scheduler addresses this
through the notion of a virtual run-time for each running task, we
fine-tune that capability to provide the correct share of the CPU
for an NF, rather than just allocating an equal share of the CPU for
each contending NF.
Efficient Chaining: Beyond simply allocating CPU time fairly
to NFs on a single core, the combination of NFs into service chains
demands careful resource management across the chain to mini-
mize the impact of bottlenecks. Processing a packet only to have it
dropped from a subsequent bottleneck’s queue is wasteful, and a
recipe for receive livelock [27, 36].
When an NF (whether a single NF or one in a service chain)
is overloaded, packet drops become inevitable, and processing re-
sources already consumed by those packets are wasted. For respon-
sive flows, such as a TCP flow, congestion control and avoidance
using packet drop methods such as RED, REM, SFQ, CSFQ [14, 15, 28,
51] and feedback with Explicit Congestion Notification (ECN) [42]
can cause the flows to adapt their rates to the available capacity
on an end-to-end basis. However, for non-responsive flows (e.g.,
UDP), a local, rapidly adapting, method is backpressure, which can
propagate information regarding a congested resource upstream
(i.e., to previous NFs in the chain). NFVnice allows the upstream
node to determine either to propagate the backpressure information
further upstream or drop packets, thus minimizing wasted work. It
is important however to ensure that effects such as head-of-the-line
blocking or unfairness do not creep in as a result.
2.2 Existing OS schedulers are ill-suited for
NFV deployment
Linux provides several different process schedulers, with the Com-
pletely Fair Scheduler (CFS) [37] being the default since kernel
2.6.23. In this work we focus on three schedulers: i) CFS Normal, ii)
CFS Batch, and Round Robin.
The CFS class of schedulers use a nanosecond resolution timer
to provide fine granularity scheduling decisions. Each task in CFS
maintains a monotonically increasing virtual run-time which de-
termines the order and quantum of CPU assignment to these tasks.
The time-slice is not fixed, but is determined relative to the run-time
of the contending tasks in a time-ordered red-black tree [9, 19]. The
task with the smallest run-time (the left most node in the ordered
red-black tree) is scheduled to run until either the task voluntarily
yields, or consumes the allotted time-slice. If it consumes the allo-
cated time-slice, it is re-inserted into the red-black tree based on its
cumulative run-time consumed so far. The CFS scheduler is analo-
gous to weighted fair queueing (WFQ) scheduling [10, 53]. Thus,
CFS ensures a fair proportion of CPU allocation to all the tasks.
The CFS Batch variant has fewer timer interrupts than normal CFS,
leading to a longer time quantum and fewer context switches, while
still offering fairness. The Round Robin scheduler simply cycles
through processes with a 100 msec time quantum, but does not
attempt to offer any concept of fairness.
To explore the impact of these schedulers on NFV applications
we consider a simple deployment with three NF processes sharing
a CPU core. The NFs run atop a DPDK-based NFV platform that