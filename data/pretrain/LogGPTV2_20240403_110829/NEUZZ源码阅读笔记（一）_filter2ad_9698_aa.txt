# NEUZZ源码阅读笔记（一）
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
最近又在读有关Fuzz研究的论文，读到了一篇标题为《NEUZZ: Efficient Fuzzing with Neural Program
Smoothing》的论文，是基于机器学习的梯度引导的Fuzz，看了看原理虽然有点难懂，但是工程实践上还是比较简单的，遂撰写了这篇笔记，以供交流心得
结构图感谢B站的小姐姐UP主：爱吃红豆沙の诸葛晴画，她的NEUZZ论文解读视频也很好
    https://b23.tv/FsXwhr
## 一、简介
NEUZZ就是采用了一种程序平滑技术，这种技术使用前向反馈神经网络，能够逐步学习去平滑地模拟复杂实际应用的分支行为，然后提出了一种gradient-guided搜索策略，这种策略能利用平滑模拟函数去找到那些能使发现漏洞数最大化的突变位置
其核心思想就是：
  * 从本质上讲模糊测试本身就是一个优化问题，目标就是在给定时间的测试中对于给定数量的输入能够最大化在程序中找到的漏洞数量，基本上是 **没有任何约束函数的无约束优化问题**
  * AFL为代表的进化遗传变异算法也就是针对这个底层问题的一种优化，由于安全漏洞往往是稀疏且不稳定地分布在整个程序中，大多数模糊者的目标是通过最大化某种形式的代码覆盖（例如边缘覆盖）来尽可能多地测试程序代码。然而，随着输入语料库的增大，进化过程在到达新的代码位置方面的效率越来越低
  * 进化优化算法的一个主要限制是它们不能利用潜在优化问题的结构（即梯度或其他高阶导数）。梯度引导优化（例如梯度下降）是一种很有前途的替代方法，它在解决不同领域的高维结构优化问题（包括气动计算和机器学习）方面明显优于进化算法
  * 然而，梯度引导优化算法不能直接应用于模糊化现实世界的程序，因为它们通常包含大量的不连续行为（梯度无法精确计算的情况），因为不同程序分支的行为差异很大。这个问题可以通过创建一个光滑（即，可微）的代理函数来解决，该代理函数逼近目标程序相对于程序输入的分支行为
  * 而神经网络理论上带有一个非线性函数的网络能够拟合任意函数
  * 基于前馈神经网络（NNs）的程序平滑技术，它可以逐步学习复杂的、真实的程序分支行为的光滑逼近，即预测由特定输入执行的目标程序的控制流边缘。我们进一步提出了一种梯度引导搜索策略，该策略计算并利用平滑近似（即神经网络模型）的梯度来识别目标突变位置，从而最大限度地增加目标程序中检测到的错误数量
**简而言之就是，NEUZZ就是通过一组神经网络利用梯度信息评价变异哪些位置对提升覆盖率有帮助**
## 二、安装与使用
NEUZZ的安装与使用还是比较简单的
###  2.1 初始准备
首先需要安装一些必备的包，根据官网readme所介绍的，他们的测试环境是：
  * Python 2.7
  * Tensorflow 1.8.0
  * Keras 2.2.3
我决定采用的是Aconda来建立一个虚拟环境来运行
####  2.1.1 安装Aconda
先安装一些必备的库
    $ sudo apt-get install python3 python3-pip python3-dev git libssl-dev libffi-dev build-essential
然后从官网里面下载Anaconda的安装脚本
    $ wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh
然后给脚本赋予执行权限
    $ chmod +x Anaconda3-2020.11-Linux-x86_64.sh
然后运行安装脚本即可
    $ ./Anaconda3-2020.11-Linux-x86_64.sh
> 这里不建议使用root权限安装，如果你自己使用的用户就不是root账户的话
这里如果出现找不到conda命令的情况可能需要手动修改shell的环境配置
    $ sudo vim ~/.bashrc
然后就修改为类似这样的实际安装路径
    export PATH="/home/ubuntu/anaconda3/bin:$PATH"
然后刷新重新运行
    $ source ~/.bashrc
####  2.1.2 安装环境包
首先建立虚拟环境
    $ conda create -n neuzz  python=2.7
激活虚拟环境
    $ conda activate neuzz
安装Tensorflow
    $ pip install --upgrade tensorflow==1.8.0
安装Keras
    $ pip install --upgrade keras==2.2.3
###  2.2 安装编译NEUZZ
下载NEUZZ源码：
    $ git clone https://github.com/Dongdongshe/neuzz.git && cd neuzz
编译neuzz
    $ gcc -O3 -funroll-loops ./neuzz.c -o neuzz
###  2.3 使用
这里我们以测试readelf为例子，首先还是安装一些必备包
    $ sudo dpkg --add-architecture i386
    $ sudo apt-get update
    $ sudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386 lib32z1
然后拷贝`nn.py`和`neuzz`到工作目录
    $ cp neuzz ./programs/readelf/
    $ cp nn.py  ./programs/readelf/
然后设置一些内核参数
    cd /sys/devices/system/cpu
    echo performance | tee cpu*/cpufreq/scaling_governor
    echo core >/proc/sys/kernel/core_pattern
然后建立种子文件夹
    $ mkdir seeds
然后运行`nn.py`作为服务器端
    $ python nn.py ./readelf -a
然后在另外一个终端里面运行neuzz
    # -l, file len is obtained by maximum file lens in the neuzz_in ( ls -lS neuzz_in|head )
    $ ./neuzz -i neuzz_in -o seeds -l 7507 ./readelf -a @@
## 三、NN.py
整个NEUZZ的结构图如下，高清图片地址：
    https://gitee.com/zeroaone/viking-fuzz/raw/master/%E7%BB%93%E6%9E%84%E5%9B%BE.png
###  3.1 setup_server
我们首先来看看`nn.py`的源码结构
我们先从main函数看起
    if __name__ == '__main__':
        setup_server()
可以看到调用了`setup_server()`函数：
    def setup_server():
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind((HOST, PORT))
        sock.listen(1)
        conn, addr = sock.accept()
        print('connected by neuzz execution moduel ' + str(addr))
        gen_grad(b"train")
        conn.sendall(b"start")
        while True:
            data = conn.recv(1024)
            if not data:
                break
            else:
                gen_grad(data)
                conn.sendall(b"start")
        conn.close()
这段代码不难看懂，就是开启一个socket通信，开始监听信息，然后就开始一个循环，不断训练数据，发送客户端梯度信息已经生成，直到没接收到客户端的信号
###  3.2 gen_grad
现在的关键是我们看到`gen_grad()`函数
    def gen_grad(data):
        global round_cnt
        t0 = time.time()
        process_data()
        model = build_model()
        train(model)
        # model.load_weights('hard_label.h5')
        gen_mutate2(model, 500, data[:5] == b"train")
        round_cnt = round_cnt + 1
        print(time.time() - t0)
这里`round_cnt`是记录了一共训练了几次梯度信息
然后`process_data`主要目的是利用afl-showmap获取每一个seed的输出路径，然后对其进行去重、更新保存
然后`model =
build_model()`就是建立神经网络模型，`train(model)`顾名思义就是训练模型，然后`gen_mutate2`就是生成梯度信息指导未来的变异
###  3.3 process_data
这是整个源码里面比较大的模块了，
    # process training data from afl raw data
    def process_data():
        global MAX_BITMAP_SIZE
        global MAX_FILE_SIZE
        global SPLIT_RATIO
        global seed_list
        global new_seeds
        # shuffle training samples
        seed_list = glob.glob('./seeds/*')
        seed_list.sort()
        SPLIT_RATIO = len(seed_list)
        rand_index = np.arange(SPLIT_RATIO)
        np.random.shuffle(seed_list)
        new_seeds = glob.glob('./seeds/id_*')
        call = subprocess.check_output
        # get MAX_FILE_SIZE
        cwd = os.getcwd()
        max_file_name = call(['ls', '-S', cwd + '/seeds/']).decode('utf8').split('\n')[0].rstrip('\n')
        MAX_FILE_SIZE = os.path.getsize(cwd + '/seeds/' + max_file_name)
        # create directories to save label, spliced seeds, variant length seeds, crashes and mutated seeds.
        os.path.isdir("./bitmaps/") or os.makedirs("./bitmaps")
        os.path.isdir("./splice_seeds/") or os.makedirs("./splice_seeds")
        os.path.isdir("./vari_seeds/") or os.makedirs("./vari_seeds")
        os.path.isdir("./crashes/") or os.makedirs("./crashes")
        # obtain raw bitmaps
        raw_bitmap = {}
        tmp_cnt = []
        out = ''
        for f in seed_list:
            tmp_list = []
            try:
                # append "-o tmp_file" to strip's arguments to avoid tampering tested binary.
                if argvv[0] == './strip':
                    out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + argvv + [f] + ['-o', 'tmp_file'])
                else:
                    out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + argvv + [f])
            except subprocess.CalledProcessError:
                print("find a crash")
            for line in out.splitlines():
                edge = line.split(b':')[0]
                tmp_cnt.append(edge)
                tmp_list.append(edge)
            raw_bitmap[f] = tmp_list
        counter = Counter(tmp_cnt).most_common()
        # save bitmaps to individual numpy label
        label = [int(f[0]) for f in counter]
        bitmap = np.zeros((len(seed_list), len(label)))
        for idx, i in enumerate(seed_list):
            tmp = raw_bitmap[i]
            for j in tmp:
                if int(j) in label:
                    bitmap[idx][label.index((int(j)))] = 1
        # label dimension reduction
        fit_bitmap = np.unique(bitmap, axis=1)
        print("data dimension" + str(fit_bitmap.shape))
        # save training data
        MAX_BITMAP_SIZE = fit_bitmap.shape[1]
        for idx, i in enumerate(seed_list):
            file_name = "./bitmaps/" + i.split('/')[-1]
            np.save(file_name, fit_bitmap[idx])
我们这里先来看几个常量
    MAX_FILE_SIZE = 10000
    MAX_BITMAP_SIZE = 2000
    SPLIT_RATIO = len(seed_list)
    seed_list = glob.glob('./seeds/*')
    new_seeds = glob.glob('./seeds/id_*')
大概就是分别记录了最大的文件大小，最大的BITMAP大小，切割率，已经存在的种子列表，新生成的种子列表
程序一开始首先就是对这些值进行了初始化操作，切割率直接取的就是已有种子文件的个数