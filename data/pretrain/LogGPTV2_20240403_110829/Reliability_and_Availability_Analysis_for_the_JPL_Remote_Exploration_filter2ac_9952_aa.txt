title:Reliability and Availability Analysis for the JPL Remote Exploration
and Experimentation System
author:Dong Chen and
Selvamuthu Dharmaraja and
Dongyan Chen and
Lei Li and
Kishor S. Trivedi and
Raphael R. Some and
Allen P. Nikora
Reliability and Availability Analysis for the JPL Remote Exploration and
Experimentation System
Dong Chen
S. Dharmaraja
Dongyan Chen
Lei Li
Kishor S. Trivediy
Raphael R. Some
Allen P. Nikora
Jet Propulsion Laboratory
Dept. of Electrical and Computer Engineering
California Institute of Technology
Duke University
Durham, NC 27708-0294, USA
fdchen,dharmar,dc,ll,PI:EMAIL
Pasadena, CA 91109, USA
PI:EMAIL,
PI:EMAIL
Abstract
The NASA Remote Exploration and Experimentation
(REE) Project, managed by the Jet Propulsion Laboratory,
has the vision of bringing commercial supercomputing tech-
nology into space, in a form which meets the demanding en-
vironmental requirements, to enable a new class of science
investigation and discovery. Dependability goals of the
REE system are 99% reliability over 5 years and 99% avail-
ability. In this paper we focus on the reliability/availability
modeling and analysis of the REE system. We carry out this
task using fault trees, reliability block diagrams, stochastic
Reward nets and hierarchical models. Our analysis helps
to determine the ranges of parameters for which REE de-
pendability goal will be met. The analysis also allows us to
assess different hardware and software fault-tolerance tech-
niques.
Keywords: Dependability modeling, Distributed sys-
tems, Fault trees, Fault-tolerance, Hierarchical modeling,
Markov chains, Stochastic reward nets, Transient faults.
1 Introduction
The NASA Jet Propulsion Laboratory REE Project
is a large multi-year technology demonstration project
to develop a low-power, scalable,
fault-tolerant, high-
performance computing platform for use in space and to
demonstrate that signiﬁcant on-board computing enables a
new class of scientiﬁc missions [6, 7]. Achieving required
level of reliability and availability is the most challenging
issue in implementing the REE system. A REE testbed has
This work was supported by the Jet Propulsion Laboratory under the
award JPL REE 1216658.
yCorresponding Author (PI:EMAIL).
been developed to test, reﬁne, and validate scalable archi-
tectures and system approaches to achieve the dependability
goals.
The REE system may experience a number of radia-
tion induced transient component failures per day and a
very small number of permanent component failures over a
multi-year mission. Despite these component hardware fail-
ures and software defects, REE system should continue to
provide operation through graceful degradation. Therefore,
the system must provide fault detection and recovery mech-
anisms so that applications can operate in the presence of
faults. For a better understanding of the measures like avail-
ability and reliability of the REE system, we develop several
models to assess its fault-tolerance features. SHARPE [12]
is used for the speciﬁcation and solution of the models.
2 Basic Architectural Concepts
The REE system architecture uses a commercial off-the
shelf (COTS) based parallel processing cluster. The system
is organized as a loosely coupled collection of processing
elements (nodes) connected by a Myrinet. Its architecture is
shown in Figure 1. Each node is a standard, commercially
available computer (such as a workstation or PC) running
a COTS operating system based on UNIX. Fault-tolerance
in REE system is achieved by the use of redundancy in
hardware, programs, time and data. Software implemented
fault-tolerance (SIFT) and system executive (SE) are used
to provide fault-detection and recovery [1, 8, 10, 11]. SE is
a protected software core that executes redundantly in three
different nodes. It is used to provide multiple levels of repli-
cation for different applications running concurrently and
is responsible for local error detection and failure recov-
ery. Node health status messages are delivered periodically
from SE to the system control processor (SCP). Nodes are
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:19 UTC from IEEE Xplore.  Restrictions apply. 
Software
Hardware
Application
Middleware
OS
Processors
Memory
Controller
Myrinet I/Fs
REE system
System Executive (SE)
Application
Middleware
OS
Processors
Memory
Controller
Myrinet I/Fs
Myrinet
.
. .
. .
.
Application
Middleware
OS
Processors
Memory
Controller
Myrinet I/Fs
I/O Servers
SCP
I/O Servers
Figure 1. REE System Architecture
individually power controlled and can be individually re-
set/restarted by either the SE or the SCP.
Upon service requests, the task scheduler component as-
signs the request to a number of nodes, selected on the ba-
sis of the current error processing method, recovery strat-
egy and power control scheme employed. The software
component chosen to perform this task is designed to be
fault-tolerant. Different versions of the same component
are running on different nodes to achieve redundancy and
design diversity. Results from these nodes are then com-
pared by SE to tolerate faults. During the execution of a
task, faults may be activated and internal errors may be pro-
duced. several types of events can interfere with the com-
putation. First, a particular set of inputs may activate a soft-
ware fault in one or more of the software versions. Sec-
ond, a transient hardware fault may interrupt the computa-
tion but can be recovered by retry or rollback. Third, an
intermittent hardware fault will crash the processor and can
only be recovered through reboot. And at last, a node with
permenant faults will crash and cannot be recovered. For
non-permenant faults, if the fault-tolerance strategy is suf-
ﬁcient to mask the internal error and produce an acceptable
result, then the system does not fail. In the underlying sys-
tem, a processor can be affected by a permanent fault, a in-
termittent fault, or a transient fault [4] and these faults can
be detected and recovered by SIFT, SE or SCP. The long-
term system behavior is affected by the arrival (activation
and manifestation) of permanent faults, which requires the
system to be reconﬁgured to a degraded mode of operation.
The dynamic error-processing scheme and hierarchical
fault recovery are employed at three levels, namely, SIFT,
SE and SCP and the ﬂowcharts are shown in Figure 2.
Checkpointing and error detection by means of acceptance
test is done through SIFT. With the checkpointing function
in SIFT, the state of current task can be saved and recov-
ered in the event of a failure. The component failures can
be transient or permanent in nature. With redundant hard-
ware and error correcting codes, majority of the transient
faults are masked and do not affect program execution. Un-
Inside Node
Middleware (SIFT)
Inside Cluster
SE
Working; Checkpointing;
Heartbeat to SE;  Error
detection (acceptance test)
No
Fault?
Yes
Component Recovery
(retry, rollback)
Working and Error Detection
(heartbeat from nodes,
acceptance test, other
fault-detection meachanism)
2 or 3 SE Working?
No
Whole REE system
SCP
Collect Information
from SE
Periodic Inspection
Node Rejuvenation
Yes
No
Successful?
No
Node fail;
Report to SE
Yes
Vote
Fault?
Yes
Node Recovery
(restart, reboot)
Report to SCP
No
Spare Node?
Failover
(Transfer task)
Yes
Yes
Successful?
No
Fault Isolation (Power
off the faulty node)
Figure 2. Flowcharts
masked transient failure may freeze the processors or gen-
erate erroneous outputs. If the processors freeze up, SE will
ﬁnd the faulty node with the help of heartbeat signals. If
the processors keep running and generate erroneous out-
puts, acceptance test in SIFT will catch majority of them.
In both cases, the faulty node is shutdown immediately and
rebooted. The tasks running on it are transferred to other
spare nodes and restarted from the last checkpoint. A few
faults cannot be detected and may cause great damage once
activated. To eliminate these, SCP selectively reboots sev-
eral nodes even when there is no fault detected, according
to their uptime and current health status. This preventive
action, which is called rejuvenation, is not only capable of
getting rid of latent faults, but also can effectively tolerate
“soft” faults such as memory leaks [9].
Transient and intermittent faults are assumed not to
cause permanent damage to nodes. After certain recovery
actions, failed nodes will eventually return to use. There-
fore, transient faults do not affect reliability but do affect
availability. If there is a permanent fault involved, the faulty
node cannot be rebooted successfully. In this case, a notiﬁ-
cation is sent to SE, the faulty node will be isolated and the
number of working nodes will decrease. Eventually, when
the number of working nodes is smaller than the minimum
requirement (2 in the testbed), the whole system fails.
3. Reliability model
Reliability model of the REE system captures failure be-
haviors of the system and its components. Reliability mod-
els are deﬁned separately for subsystems, which are then
combined into an overall system model. We will discuss
models that deal with hardware failures as well as software
failures. We ﬁrst describe the reliability models for the sub-
systems using reliability block diagrams (RBDs) and then
for the overall system using a fault tree.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:19 UTC from IEEE Xplore.  Restrictions apply. 
2
The REE architecture is characterized by the use of re-
dundancy at several levels. Spare processor chips and re-
dundant Myrinet interfaces are employed within nodes to
enhance their reliability. The I/O subsystem is implemented
in several nodes to make the I/O operation reliable. All the
nodes are assumed to exhibit the same stochastic behavior
regarding fault occurrences and the faults are mutually inde-
pendent. Also, since REE system operates without human
intervention, we assume no replacement of components af-
ter a permanent failure.
The REE system reliability model is developed as a 2-
level hierarchy [12]. On the higher level, we construct a
fault tree in which each event corresponds to a subsystem
(Figure 3). On the lower level, we construct reliability block
diagrams to model the subsystems (Figure 4). Closed form
solution is obtained for the reliability of the REE system.
With the help of the software package SHARPE, this model
can be speciﬁed easily and transient measures can be ob-
tained efﬁciently.
REE System Failure
(n−1) of n
Myrinet
Node System
IOS
IOS
IOS
IOS
Figure 3. Fault Tree Model for REE System
Reliability
In Figure 3, Node System denotes the collection of 
nodes and IOS denotes I/O servers. Solving the fault tree
model, the reliability is given by
Rsys = (cid:18) 
Xi=2
i(cid:19)Rnodei 1   Rnode i(cid:19)
(cid:18)
j(cid:19)RIOSj 1   RIOS4 j(cid:19)  RMyrinet;
(cid:18)4
(cid:18) 4
Xj=1
where Rnode
single node,
RIOS = Rnode is the reliability for the I/O server and
RMyrinet = e (cid:21)net is the reliability for the Myrinet.
Here the permanent failure rate of the Myrinet is denoted
by (cid:21)net.
is
the reliability for
The reliability of a single node Rnode could be cap-
tured by a RBD model shown in Figure 4. In this ﬁgure,
symbols and their meanings with permanent failure rates
are as follows: Processor ((cid:21)), EDAC DRAM ( (cid:21)e),
processor
processor
memory
memory
controller
node 
controller
non-volatile 
memory
PCI bus
Myrinet I/F
Myrinet I/F
Software
Figure 4. RBD Model for Node Level Reliability
2
f ai ve2
c2
1   c2
Tf ai ve
#
hag
intermittent fault recovery
f ai ve1
f ai ve
f ai ve
ef
hag
Thb
[#	 (cid:21) 2]
[#	 < 2]
#
Tb
f 2
(cid:11)hag
Ta
1

	
Tf a	 
#
1   c1
c1
1
a
a
f a	 y
d
i
ca
b
2
Ti
f 1
e
e
	c
	c
3
wi
c	c
1   (cid:11)hag
ca
a
remove latent fault using
rejuvenation
#
transient
fault recovery
Tbk
a
Ta
d
Figure 5. SRN Model for REE Nodes System
Union PCI bridge & Memory Controller ((cid:21)bc), Node Con-
troller ((cid:21)dc), Non-Volatile Memory ( (cid:21)v), PCI Bus
((cid:21)b	), Myrinet Network Fabric I/F ((cid:21)if ) and Software
((cid:21)f ). Every REE node has two symmetrical processors
to boost performance.
If one processor fails, the system
will be reconﬁgured to an uniprocessor system after reboot.
From the RBD model shown in Figure 4, the reliability of a
node is given by [13]:
Rnode = e (cid:21)e(cid:21)bc(cid:21)dc(cid:21)v(cid:21)b	(cid:21)f 
 2e (cid:21)   e 2(cid:21)  2e (cid:21)if    e 2(cid:21)if :
The expression for software reliability (e (cid:21)f ) refers
to the probability that the software failure results in an un-
usable node. We expect that most software failures can be
recovered by retry, process restarting or node reboot. We
only include those software failures here that cannot be re-
covered from a reboot due to operating system corruption.
4. Availability analysis
In analyzing the availability of the REE system, the per-
manent failures are assumed to be so rare that they are ig-
nored. Only the transient failures and intermittent failures
are considered in this section. The availability model is con-
structed as a two-level model: a fault tree at the upper level
and a stochastic reward net (SRN) at the lower level. As
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:19 UTC from IEEE Xplore.  Restrictions apply. 
3
we have seen in the reliability analysis, the REE system
can be decomposed into three independent subsystems: I/O
servers, Myrinet and Node System. The whole system is
unavailable if any of the subsystems is unavailable.
Denote the steady-state availability of the three subsys-
tems as AS, A ET and A S respectively. The steady-
state availability of the whole REE system can then be com-