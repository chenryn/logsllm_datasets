# Reliability and Availability Analysis for the JPL Remote Exploration and Experimentation System

**Authors:**
- Dong Chen
- Selvamuthu Dharmaraja
- Dongyan Chen
- Lei Li
- Kishor S. Trivedi
- Raphael R. Some
- Allen P. Nikora

**Affiliations:**
- Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109, USA
- Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708-0294, USA

**Abstract:**
The NASA Remote Exploration and Experimentation (REE) Project, managed by the Jet Propulsion Laboratory, aims to integrate commercial supercomputing technology into space environments, meeting stringent environmental requirements to enable advanced scientific investigations. The dependability goals for the REE system are set at 99% reliability over five years and 99% availability. This paper focuses on the reliability and availability modeling and analysis of the REE system using fault trees, reliability block diagrams, stochastic reward nets, and hierarchical models. Our analysis helps determine the parameter ranges that meet the REE dependability goals and evaluates different hardware and software fault-tolerance techniques.

**Keywords:**
Dependability modeling, Distributed systems, Fault trees, Fault-tolerance, Hierarchical modeling, Markov chains, Stochastic reward nets, Transient faults.

## 1. Introduction
The NASA Jet Propulsion Laboratory's REE Project is a multi-year technology demonstration initiative aimed at developing a low-power, scalable, fault-tolerant, high-performance computing platform for space applications. The project seeks to demonstrate that significant on-board computing can enable new types of scientific missions [6, 7]. Achieving the required levels of reliability and availability is a critical challenge in implementing the REE system. A testbed has been developed to test, refine, and validate scalable architectures and system approaches to meet these dependability goals.

The REE system may experience multiple radiation-induced transient component failures daily and a very small number of permanent component failures over a multi-year mission. Despite these hardware and software failures, the system should continue to operate through graceful degradation. Therefore, the system must provide fault detection and recovery mechanisms to ensure continuous operation. To better understand the system's availability and reliability, we develop several models to assess its fault-tolerance features using SHARPE [12].

## 2. Basic Architectural Concepts
The REE system architecture is based on a commercial off-the-shelf (COTS) parallel processing cluster. The system consists of a loosely coupled collection of processing elements (nodes) connected via Myrinet. Each node is a standard, commercially available computer running a COTS operating system based on UNIX. Fault-tolerance is achieved through redundancy in hardware, programs, time, and data. Software-implemented fault-tolerance (SIFT) and the system executive (SE) provide fault detection and recovery [1, 8, 10, 11]. The SE, a protected software core, runs redundantly on three different nodes and provides multiple levels of replication for concurrent applications, local error detection, and failure recovery. Node health status messages are periodically delivered from the SE to the system control processor (SCP). Nodes are individually power-controlled and can be reset or restarted by either the SE or the SCP.

Upon service requests, the task scheduler assigns the request to selected nodes based on the current error processing method, recovery strategy, and power control scheme. The software components chosen for this task are designed to be fault-tolerant, with different versions running on different nodes for redundancy and design diversity. Results from these nodes are compared by the SE to tolerate faults. During task execution, various events can interfere with computation, including software faults, transient hardware faults, intermittent hardware faults, and permanent faults. For non-permanent faults, if the fault-tolerance strategy can mask the internal error and produce an acceptable result, the system does not fail. In the underlying system, a processor can be affected by permanent, intermittent, or transient faults, which can be detected and recovered by SIFT, SE, or SCP. The long-term system behavior is influenced by the arrival of permanent faults, requiring the system to reconfigure to a degraded mode of operation. Dynamic error-processing schemes and hierarchical fault recovery are employed at three levels: SIFT, SE, and SCP, as shown in Figure 2.

## 3. Reliability Model
The reliability model of the REE system captures the failure behaviors of the system and its components. Separate reliability models are defined for subsystems and then combined into an overall system model. We will discuss models that address both hardware and software failures. We first describe the reliability models for the subsystems using reliability block diagrams (RBDs) and then for the overall system using a fault tree.

The REE architecture employs redundancy at multiple levels, including spare processor chips and redundant Myrinet interfaces within nodes, and I/O subsystems implemented across several nodes. All nodes are assumed to exhibit the same stochastic behavior regarding fault occurrences, and the faults are mutually independent. Since the REE system operates without human intervention, we assume no replacement of components after a permanent failure.

The REE system reliability model is developed as a two-level hierarchy [12]. At the higher level, we construct a fault tree where each event corresponds to a subsystem (Figure 3). At the lower level, we use RBDs to model the subsystems (Figure 4). Closed-form solutions are obtained for the system's reliability. Using the SHARPE software package, the model can be easily specified, and transient measures can be efficiently obtained.

### 3.1. Fault Tree Model
In Figure 3, the Node System denotes the collection of \( n \) nodes, and IOS denotes I/O servers. Solving the fault tree model, the system reliability \( R_{sys}(t) \) is given by:

\[ R_{sys}(t) = \sum_{i=2}^{n} \binom{n}{i} R_{node}(t)^{i-1} (1 - R_{node}(t))^{n-i} \times \sum_{j=1}^{4} \binom{4}{j} R_{IOS}(t)^{j-1} (1 - R_{IOS}(t))^{4-j} \times R_{Myrinet}(t) \]

where:
- \( R_{node}(t) \) is the reliability of a single node.
- \( R_{IOS}(t) = R_{node}(t) \) is the reliability for the I/O server.
- \( R_{Myrinet}(t) = e^{-\lambda_{net} t} \) is the reliability for the Myrinet, with \( \lambda_{net} \) being the permanent failure rate of the Myrinet.

### 3.2. RBD Model for Node Level Reliability
The reliability of a single node \( R_{node}(t) \) is captured by an RBD model (Figure 4). The symbols and their meanings with permanent failure rates are as follows:
- Processor (\( \lambda_{proc} \))
- EDAC DRAM (\( \lambda_{mem} \))
- Union PCI bridge & Memory Controller (\( \lambda_{bridge} \))
- Node Controller (\( \lambda_{ctrl} \))
- Non-Volatile Memory (\( \lambda_{nvram} \))
- PCI Bus (\( \lambda_{pci} \))
- Myrinet Network Fabric I/F (\( \lambda_{myr} \))
- Software (\( \lambda_{soft} \))

From the RBD model, the reliability of a node is given by [13]:

\[ R_{node}(t) = e^{-(\lambda_{mem} + \lambda_{bridge} + \lambda_{ctrl} + \lambda_{nvram} + \lambda_{pci} + \lambda_{soft})t} \times \left[ 2e^{-\lambda_{proc} t} - e^{-2\lambda_{proc} t} \right] \times \left[ 2e^{-\lambda_{myr} t} - e^{-2\lambda_{myr} t} \right] \]

The expression for software reliability \( e^{-\lambda_{soft} t} \) refers to the probability that a software failure results in an unusable node. Most software failures can be recovered by retry, process restarting, or node reboot. Only those software failures that cannot be recovered due to operating system corruption are included.

## 4. Availability Analysis
In analyzing the availability of the REE system, permanent failures are assumed to be so rare that they are ignored. Only transient and intermittent failures are considered. The availability model is constructed as a two-level model: a fault tree at the upper level and a stochastic reward net (SRN) at the lower level. As seen in the reliability analysis, the REE system can be decomposed into three independent subsystems: I/O servers, Myrinet, and Node System. The whole system is unavailable if any of the subsystems is unavailable.

Denote the steady-state availability of the three subsystems as \( A_{IO} \), \( A_{Myr} \), and \( A_{Node} \). The steady-state availability of the whole REE system can then be computed as:

\[ A_{sys} = A_{IO} \times A_{Myr} \times A_{Node} \]

This approach allows us to evaluate the system's availability and identify the critical parameters that influence it.