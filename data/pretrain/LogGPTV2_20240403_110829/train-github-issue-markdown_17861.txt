i have a nvidia 1070 gpu/8G memory, i have a pytorch model for inference, and
i directly run it the gpu memory cost as follow  
![image](https://user-
images.githubusercontent.com/6283983/48594685-92a85a00-e98c-11e8-8270-e53264439789.png)
then i first load the model and then use follow code to do jit trace, and use
jit module run the gpu memory cost as follow  
![image](https://user-
images.githubusercontent.com/6283983/48594713-bb305400-e98c-11e8-8031-cac04988b4a2.png)  
![image](https://user-
images.githubusercontent.com/6283983/48594734-d3a06e80-e98c-11e8-8208-2ce3d6e9afd2.png)
**### when i use pytorch c++ api to load the saved jit trace model.pt error as
follow, does c++ need more gpu memory?**  
![image](https://user-
images.githubusercontent.com/6283983/48594777-05193a00-e98d-11e8-866d-1adf6273aed2.png)
when i gdb the c++ example and step by step to the forward() cause fault, and
gpu memory as follow  
![image](https://user-
images.githubusercontent.com/6283983/48594961-d354a300-e98d-11e8-90f4-b99d8df1651d.png)