Rate
at
≥
Recall
75%
<68.6%
non damaging
Actual
(Predicted)
Accuracy
.713 (.913)
Actual
(Predicted)
Precision
1 (.909)
Actual
(Predicted)
Filter Rate
.988 (.998)
Actual
(Predicted)
Recall
.040 (.045)
.574 (.904)
.396 (.226)
.386 (.887)
.814 (.751)
Result of Filtering
10 of 847 dropped due to
high conﬁdence of dam-
age; prior hand coding
found all 10 of these to be
damaging
520 of 847 routed for hu-
man review due to mod-
est conﬁdence of damage;
prior hand coding found
206 of these routed edits
to be damaging.
TABLE VIII
TOP 5 TOPICS FOR EACH DATASET
Tor
IP
First-time
Politics
Technology
Locations
Movies & TV
Religion
Music
Movies & TV
Locations
Politics
Sports
Music
Locations
Movies & TV
Education
Politics
Registered
Locations
Music
Politics
Movies & TV
Sports
reported below are from from LDA topic models estimated
using 20 topics. All other parameters needed for the LDA al-
gorithm were run with default values in MALLET. After ﬁtting
LDA topics models with MALLET, we manually interpreted
each cluster of words and created an appropriate topic header.
For reference, we include the mapping of keyword collections
to topic headers we assigned in Table IX in our appendix.
As a mixture model, LDA treats every document as be-
longing to every topic, but
to varying degrees. As a re-
sult, we identiﬁed the topic with the highest probability
and described each article as being “in” that topic for the
purposes of the comparisons between the groups of edits. A
Pearson’s Chi-squared test suggests that the distribution of
articles across topics is different between Tor editors and IP
editors (χ2 = 1655; df = 19; p < 0.01), First-time editors
(χ2 = 848; df = 19; p < 0.01), and Registered editors
(χ2 = 1508; df = 19; p < 0.01). These differences are
statistically signiﬁcant after adjusting for multiple comparisons
using a Bonferroni correction and suggest that Tor editors,
although distinct from other groups of editors, are most similar
to First-time editors in their topic selections.
Our analysis shows some similarities between Tor editors’
interests and other groups. Table VIII compares the top 5
topics that each group focused most on. Fig. 6 visualizes the
distribution of topics using a gradient where more prevalent
topics are darker and less prevalent topics are lighter. While
there are many horizontal bands of a similar shade where the
topics edited by our different sets of users are similar, we can
also see many differences.
For example, like other editors, Tor editors frequently edit
topics such as Movies and TV and Locations, which are
popular across all groups. We see proportionally fewer contri-
butions from Tor editors in the Sports, Soccer, and American
topics. Compared with other kinds of users, Tor
Football
editors are more likely to contribute to articles corresponding
to Politics, Technology, and Religion—topics that may be
construed as controversial.29 Our ﬁndings provide evidence
to support previous qualitative work that has suggested that
sensitive or stigmatized topics might attract Wikipedia editors
interested in using tools like Tor to conceal their identity [14].
VII. LIMITATIONS
Our work is limited in several important ways. First, our
results are limited in that our analysis is conducted only on En-
glish Wikipedia. We cannot know how this work would extend
to users of privacy-enhancing technologies other than Tor or
to user-generated content sites beyond English Wikipedia. As
a minimal ﬁrst step, we attempted to speak to this limitation
by conducting an analysis of editing activity made by Tor
users in other language editions of Wikipedia. Although we
do not report on them in depth, we have included information
in the appendix (see Tab. X) that displays the number of Tor
edits in different language editions of Wikipedia relative to
contributions made by the communities as a whole. Although
Tor users are active in many language editions of Wikipedia,
only a small number of edits by Tor users evaded the ban.
There are reasons to imagine that
the behavior of Tor
editors contributing to English Wikipedia might differ from
that of editors in language editions. For example, we identify
thousands of edits from Tor exit nodes contributing to the Rus-
sian Wikipedia edition. This is striking because the Russian
government partially bans access to Tor30 and Wikipedia.31
Although a closer inspection of Wikipedia language editions
may yield interesting motivational and cultural differences
29https://www.thebalancecareers.com/topics-to-avoid-discussing-at-work-526267
(Archived:
https://perma.cc/
(Archived: https://perma.cc/G4GT-GEAK)
30https://www.infosecurity-magazine.com/news/
russia-passes-bill-banning-tor-vpns/
DLN7-KTQT)
31https://en.wikipedia.org/wiki/Censorship of Wikipedia#Russia
(Archived: https://perma.cc/GNM4-9UNH)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
197
regarding anonymity-seeking practice, our team is not suf-
ﬁciently versed in these languages to conduct a replication
of our analyses across different Wikipedia language editions.
We are making our full datasets available and invite other
researchers’ interest.
Of course, Wikipedia language editions do not necessarily
imply the geographic locations of editors. We do not know if
people editing Russian Wikipedia come from Russia. Addi-
tionally, in many countries, viewers primarily access English
Wikipedia even when English is not their native language.32
For example, the majority of pageviews from China and Iran—
countries that ban both access to Tor and Wikipedia—go to
the English version of Wikipedia. English Wikipedia is also
the primarily-viewed Wikipedia for many countries that do
not have a history of banning access to Wikipedia, such as the
Netherlands and Croatia.
Our study is limited in other ways as well. Because our
study uses IP addresses and account names to identify editors,
we cannot know exactly how usernames and IP addresses
map onto people. Some users may choose different levels of
identiﬁability depending on the kinds of edits they wish to
make. For example, a registered editor may use Tor for certain
activities and not for others [14].
Additionally, our samples might reﬂect survivorship bias.
We simply cannot know if our sample of Tor edits is repre-
sentative of the edits that would occur if Wikipedia did not
block anonymity-seeking users. Many Tor users who are told
by Wikipedia that Tor is blocked will not try again. As a result,
our dataset might overrepresent casual one-off Wikipedia
contributors, including both constructive “wiki gnomes” and
drive-by vandals. Our sample might also over-represent indi-
viduals with a deep commitment to editing Wikipedia or with
technical sophistication (i.e., the knowledge that one could
repeatedly request new Tor circuits to ﬁnd exit nodes that are
not banned by Wikipedia). Tor users who manage to evade
the ban might include committed activists as well as banned
Wikipedia users with deeply held grudges. Although we do not
know what else would happen if Wikipedia unblocked Tor, we
know that the almost total end of contributions to Wikipedia
from Tor in 2013 means that, at a minimum, a large number
of high-quality contributions are not occurring. Our analysis
describes some part of what is being lost today—both good
and bad—due to Wikipedia’s decision to continue blocking
users of anonymity-protecting proxies.
VIII. CONCLUSIONS AND IMPLICATIONS FOR DESIGN
Wikipedia’s imperfect blocking of Tor provides a unique
opportunity to gain insight into what might not be happen-
ing when user-generated content sites block participation by
anonymity-seeking users. We employed multiple methods to
compare Tor contributions to a number of comparison groups.
Our ﬁndings suggest that privacy seekers’ contributions are
more often than not comparable to those of IP editors and
32https://stats.wikimedia.org/wikimedia/animations/wivivi/wivivi.html
(Archived: https://perma.cc/PGV6-687Q)
First-time editors in many ways. Using hand-coded data and
a machine-learning classiﬁer, we estimated that edits from
Tor users are of similar quality to those by IP editors and
First-time editors. We estimated that Tor users make more
higher quality contributions than other IP editors, on average,
as measured by PTRs. Our analysis also pointed to several
important differences. We found that Tor users are signiﬁcantly
more likely than other users to revert someone else’s work and
appear more likely to violate Wikipedia’s policy against back-
and-forth edit wars, especially on discussion pages. Tor users
also edit topics that are systematically different from other
groups. We found that Tor editors focused more on topics
related to religion, technology, and politics and less on topics
related to sports and music.
The Tor network is steadily growing, with approximately
two million active users at the time of writing. Many com-
munities around the world face Internet censorship and au-
thoritarian surveillance. In order to be Wikipedia contributors,
these communities must rely on anonymity-protecting tools
like Tor. In our opinion, our results show that the potential
value to be gained by creating a pathway for Tor contributors
may exceed the potential harm. Wikipedia’s systemic block
of Tor editors remains controversial within the Wikipedia
community. We have been in close contact with Wikipedia
contributors and staff at the Wikimedia Foundation as we
conducted this research to ensure that our use of Wikipedia
metrics is appropriate and to give them advance notice of
our results. We are hopeful that our work can inform the
community and encourage them to explore mechanisms by
which Tor users might legitimately contribute to Wikipedia—
perhaps with additional safeguards. Given the advances of
the privacy research community (including anonymous black-
listing tools such as Nymble [35]), and improvements in
automated damage-detecting tools in Wikipedia, alternatives
to an outright ban on Tor contributions may be feasible
without substantially increasing the burden already borne by
the vandal-ﬁghting efforts of the Wikipedia community. We
hope our ﬁndings will inform progress toward these ends.
ACKNOWLEDGEMENTS
We owe a particular debt of gratitude to Nora McDonald and
Erica Racine who both contributed enormously to the content
analysis included in the paper. Our methodology was improved
via generous feedback from members of the Tor Metrics team,
including Karsten Loesing, and the Wikimedia Foundation,
including Aaron Halfaker, Morten Warncke-Wang, and Leila
Zia. Feedback and support for this work came from members
of the Community Data Science Collective, and the manuscript
beneﬁted from excellent feedback from several anonymous
referees at IEEE S&P. The creation of dataset was aided by
the use of advanced computational, storage, and networking
infrastructure provided by the Hyak supercomputer system at
the University of Washington. This work was supported by
the National Science Foundation (awards CNS-1703736 and
CNS-1703049) and included the work of two undergraduates
supported through an NSF REU supplement.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
198
REFERENCES
[1] Nazanin Andalibi, Oliver L. Haimson, Munmun
De Choudhury, and Andrea Forte. Understanding social
media disclosures of sexual abuse through the lenses of
support seeking and anonymity.
In Proceedings of the
2016 CHI Conference on Human Factors in Computing
Systems, CHI ’16, pages 3906–3918, New York, NY,
USA, 2016. ACM.
[2] John A. Bargh, Katelyn Y. A. McKenna, and Grainne M.
Fitzsimons. Can you see the real me? activation and
expression of the ‘true self’ on the internet. Journal of
Social Issues, 58(1):33–48, 2002.
[3] David M Blei, Andrew Y Ng, and Michael I Jordan.
Latent dirichlet allocation. Journal of machine Learning
research, 3(Jan):993–1022, 2003.
[4] L. S. Buriol, C. Castillo, D. Donato, S. Leonardi, and
S. Millozzi. Temporal analysis of the wikigraph. In 2006
IEEE/WIC/ACM International Conference on Web Intel-
ligence (WI 2006 Main Conference Proceedings)(WI’06),
pages 45–51, Dec 2006.
[5] Abdelberi Chaabane, Pere Manils, and Mohamed Ali
Kaafar. Digging into Anonymous Trafﬁc: A Deep Analy-
sis of the Tor Anonymizing Network. In 2010 Fourth In-
ternational Conference on Network and System Security,
pages 167–174, Melbourne, Australia, September 2010.
IEEE.
[6] Kaylea Champion, Nora McDonald, Stephanie Bankes,
Joseph Zhang, Rachel Greenstadt, Andrea Forte, and
Benjamin Mako Hill. A Forensic Qualitative Analysis
of Contributions to Wikipedia from Anonymity Seeking
Users. Proceedings of the ACM on Human-Computer
Interaction, 3(CSCW):1–26, November 2019.
[7] Andrea Chester and Gillian Gwynne. Online teaching:
Encouraging collaboration through anonymity. Journal
of Computer-Mediated Communication, 4(2):0–0, 1998.
[8] M.D. Choudhury and S De. Mental health discourse
on reddit: Self-disclosure, social support, and anonymity.
Proceedings of the 8th International Conference on We-
blogs and Social Media, ICWSM 2014, pages 71–80, 01
2014.
[9] William S. Cleveland. Robust Locally Weighted Regres-
sion and Smoothing Scatterplots. Journal of the Ameri-
can Statistical Association, 74(368):829–836, December
1979.
[10] Q. Dang and C. Ignat. Measuring quality of collabora-
tively edited documents: The case of wikipedia. In 2016
IEEE 2nd International Conference on Collaboration
and Internet Computing (CIC), pages 266–275, Nov
2016.
[11] Quang Vinh Dang and Claudia-Lavinia Ignat. Quality as-
sessment of wikipedia articles: A deep learning approach
by quang vinh dang and claudia-lavinia ignat with martin
vesely as coordinator. SIGWEB Newsletter, Autumn:5:1–
5:6, November 2016.
[12] Judith S Donath.
Identity and deception in the virtual
community.
68. Routledge, 2002.