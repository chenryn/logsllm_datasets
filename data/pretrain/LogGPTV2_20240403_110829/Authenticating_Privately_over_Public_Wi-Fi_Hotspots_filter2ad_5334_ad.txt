of a few hours and an AP density consistent with a modern
urban ISP hotspot deployment allowing tens of Mbits/second.
We justify these assumptions in Section 6. We test the
client query upload time assuming the wireless link to be the
bottleneck in this operation. Our measurements for database
building are done using two ECC curves: the smallest and the
fastest of the publicly available [12, 13]. To test PIR result
computations on the server we measure the time elapsed in
computing the client’s response from a query. Because this
is the most computationally expensive operation we attempt
to characterize the behavior of the basic operations in two
implementations: CPU and GPU. For system speciﬁcations
see Table 4.
Table 2 summarizes the evaluation parameters for EAP-
TE. We choose a 128-bit key for K that ﬁts within the EC
Elgamal modulus size, and gives enough entropy to generate
suitable WPA encryption keys. A small modulus size limits
the width of the table, reducing the time needed for the PIR
response generation. Conversely, a faster curve allows for
quicker table generation but impacts PIR response due to
wider row length. While these may seem modest security
parameters, we assume the lifetime of K to be in the order of
hours, as the server periodically generates new tables. Thus,
Figure 2: Architecture of the EAP-TE protocol.
computation of the result is divided among a large number
of threads. We evaluate the performance gains from GPU
parallelization in Section 6.
As clients enter and leave the system, index management
must keep the invariance of the client entry index. Empty
database rows are kept with a special encryption of K with
the server’s public key. This allows auditing clients to check
validity of the committed K, while protecting the key itself
from discovery. When a new client subscribes, the server
chooses one of the empty rows and assigns it to the client.
Later, when a client unsubscribes, the association is cleared
and the row value is replaced with the special value from
above.
5.4 Optimizations
√
Several expensive operations occur in EAP-TE. On the
n elements with mostly E(0) are
client side, vectors with
generated to form the PIR query. While NTRU Encrypt is a
fast operation compared to other number-theoretic schemes,
polynomial multiplication is intensive enough to add consid-
erable time for large numbers of clients if done naively. We
show the impact of the implementation on the speed of poly-
nomial multiplication on Section 6. Because no polynomial
in the vector depends on others this an easily parallelizable
operation. Moreover, pre-computation of zero-polynomials
during idle times can aid in oﬀsetting this cost in the handset.
As outline in Section 3.5, tables need not be generated
immediately. Delay in key revokation in this case works
similarly to current response times of around one hour for
mobile providers. Another strategy for optimization is to
keep the public-key bit length small. Since K is short-lived
and it is discarded after new tables come to the pool, it is
not necessary to use a large security parameter.
Resolving a PIR query has the server compute a value
over every column of the table, so limiting the size of the
key implies computation savings. We can obtain even larger
savings by using a cryptosystem with small key sizes, like
ECC. Table generation can be quite costly for the large
number of clients we consider. For this construction we use
a version of ECC-Elgamal encryption for every client entry
ClientAccess PointServerAssociateTLS session startRequest-IdentityResponse-tsCommit(K, ts)Access-Request-tsPIR_Q(v)Sig[PIR_Resp(r)],x1H(x2||x1||K),x2 H(x1||x2||K)Ksc, Kcs4-Way Handshake1353Table 2: Implementation choices.
Parameter
|K|
EPUB(K)
EC Curves
H(·)
E(·)
NTRU Parameters
N
q
n
Value
128-bit
EC Elgamal [20]
sect131r1, sect163k1
SHA-256
NTRU-Encrypt [30]
APR2011_439_FAST [55]
439
221
107 clients
Table 3: Choice of q.
q
211
212
213
214
215
216
217
218
219
220
221
Observed Additions
2
47
250
951
4408
18087
73047
367865
1.12 × 106
7.01 × 106
2.21 × 107
we limit table size, generation time, and key lifetime. We
discuss the implications and countermeasures of extended
key lifetime in Section 7.
We chose our NTRU modulus parameter q by empirically
testing the minimum number of additions observed before
an encryption failure occurs. For every value of q, we add a
ciphertext addition of E(0) to E(1). After every addition, we
check whether the decryption returns the original value. We
record the minimum number of additions observed over 108
repetitions. Table 3 summarizes our results.
6.1 Database and Query Generation
Given the parameters in Table 2 for TracEdge, the maxi-
mum size of the table using EC Elgamal over 163-bit security
is 41×107Bytes, or 410MB. Our server code builds a database
table by choosing a key K uniformly at random, and using
H(K) as the scalar multiplier for the ephemeral Elgamal key.
We take the mean time to create a table over 100 samples
on a multi-threaded implementation, shown in Table 5.
scheme is a vector of |q| × √
Query generation in the client happens every time the client
√
wishes to authenticate to the server. A query for our PIR
107 or 12.6KB, assuming
32-bit integers to store coeﬃcients. We benchmarked our
client’s query generation over 1000 repetitions and Table 6
summarizes the results.
n = 4
Table 4: Evaluation systems.
Hardware Setup
Srv. 1 Quad 3.0GHz, 8GB
Srv. 2 Quad 2.5GHz, 8GB
2x 8c. Xeon, 130GB
Srv. 3
8x Xeon, 21GB
Srv. 4
Srv. 5
8x Xeon, 21GB
Srv. 6 Quad 2.5GHz, 8GB
Cli. 1 Galaxy SII 1GB RAM 802.11bgn
Cli. 2 HTC One 2GB RAM 802.11abgn
Graphics/WLAN
9800GT (112c)
GTX280 (240c)
-
2x GF100 (448c)
1x GK104 (1536c)
GTX780 (2304c)
Table 5: Table creation time (s) ±σ.
Server
1
2
3
4
sect163k1
1160 ± 4
1190 ± 9
261 ± 7
1105s ± 3
sect131r1
3424 ± 9
3211 ± 13
751 ± 12
2997 ± 10
Table 6: Query generation on client.
Device Avg. Encryption (µs)±σ Total time
1
2
159.3 ± 0.12
173.4 ± 0.08
1.86s
1.98s
6.2 Communication Complexity, and Perfor-
mance
back N(cid:112)(cid:96)
Communication complexity determines the amount of data
that needs to be sent over the air. The scale of our client
√
database makes it a good candidate to use the technique
in [35]. By processing the table as a square with
n rows
and columns over two iterations, the server needs to send
n elements back, containing the encryption of a
database element. Using 32-bit integers this yields a total
of 1.25MB for the response. Assuming a 10Mbps data rate,
this transmission completes in 1.0 seconds.
√
We measure rate quality over the air using 50 public Wi-Fi
hotspots from a large ISP in an U.S. urban area. We associate
our mobile client devices from Table 4 to the target APs and
connect to a test server at a well-provisioned site. Afterwards
we make a similar measurement with Speedtest [45] as a
control. We measure downlink and uplink throughput over
a period of 10 seconds. Figure 3 shows how over 50% of the
sampled APs can sustain throughputs higher than 10Mbps
over 10 seconds.
Response extraction at the client is performed by decrypt-
ing all the polynomials received from the server. Table 7
shows average times for our clients over 1000 samples.
We also measure authentication latency as seen on the
client handsets, deﬁned as the time between query transmis-
sion and response receipt in Table 7, and it includes 350ms
computation time on the single-GPU system and 0.9s net-
work latency. We compare the measured latency value with
the expected latency for a multiple-GPU system later in this
section.
6.3 Response Computation
The most computationally intense stage in TracEdge is
response generation, which consists of two phases: polyno-
mial multiplication modulo X N − 1, and polynomial addition
modulo q. Even though polynomial multiplication is a fast
operation compared to other homomorphic cryptographic
schemes, the large size of the database can make compu-
tation lengthy. Fortunately, the operation is suitable for
parallelization, as discussed in Section 4.4. We evaluate two
computation strategies: multi-threading over CPU cores and
GPU computation using CUDA [44]. Table 8 shows the
Table 7: Query extraction and latency on client.
Device Av. Decryption Total time
1
2
63.7 ± 0.20µs
78.6 ± 0.13µs
45.3ms
55.9ms
Latency
1.41s
1.42s
1354tion of 1.12 seconds. Because table and query generation
can be performed oﬄine they do not add to authentication
time. Likewise, mutual authentication after the global key
is retrieved is not considered for end-to-end authentication
as it is fast and common to other secure Wi-Fi authentica-
tion protocols. As shown in Table 7, our single-GPU system
performs closely to the expected computation time in a multi-
GPU system. Moreover, because multiple row auditing has a
lesser or equal cost than key retrieval, two GPUs are enough
perform the operation simultaneously.
To understand how well our system performs under heavy
client load, we measure the maximum number of authentica-
tions per second our server is able to sustain by saturating
with with client queries. While our server is only able to
maintain 2.3 authentications per second with only one GPU
because of the number of memory transfers required to pro-
cess the whole table, a multi-GPU with 8 devices can sustain
up to 60 auth/s.
Building a multi-GPU setup assuming a $380 retail price
of the GTX780 devices plus four host computers with two
GPUs installed per host, would cost in the range of $4000-
5000. This results an investment of at most $83.33 per
authentication/sec from the provider for 10 million clients.
7. CONCLUSIONS AND DISCUSSION
We have presented TracEdge, an authentication proto-
col that protects client identity against covert adversaries.
TracEdgemitigates location tracing for general applications
and is able to detect dishonest servers attempting to identify
clients. In addition, we show a proposal for a 802.11 imple-
mentation over EAP and an estimation of its computational
and communication costs. The key PIR component can per-
form response computation in 43.9ms on a platform with
8 oﬀ-the-shelf GTX780 GPUs. With devices in the market
with over 5700 cores, computation times can be reduced
further by augmenting hardware capabilities.
Implementation of TracEdge implies a cost on the ISP, as
multiple parallel computation devices need to be deployed
for operation. As stated in Section 6, as much as $83 per
authentication per second may be required, making TracEdge
more of a niche market solution for clients who are more
focused on privacy. A more reduced user base however, also
lowers the hardware requirements linearly.
While delaying table generation opens the possibility of
credential sharing where a client may authenticate with the
server and then share the retrieved key with unauthorized
clients, prevention can be achieved with a combination of
TracEdge with a USIM component [1] and Distance Bounding
Protocols [8] running between the SIM and AP. In this case,
the AP needs access to the authentication key. Similar to
EAP-AKA method [32] which uses the SIM to authenticate,
TracEdge would store the retrieved key and with it produce
the necessary keying material, preventing human/software
access to this value. The outcome of the distance bounding
authentication is a time-limited shared session key between
the speciﬁc AP and the mobile client. Sharing this session
key with other cheating clients does not help them, since an
unauthorized client would then need to be co-located with
the original client, contending for the same AP.
Further identity leaks from traﬃc pattern analysis are
still possible as they fall outside the scope of TracEdge. A
concerned client may use anonymity networks such as TOR