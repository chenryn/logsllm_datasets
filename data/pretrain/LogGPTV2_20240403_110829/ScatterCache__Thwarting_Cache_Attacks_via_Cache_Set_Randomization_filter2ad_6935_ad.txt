Where in normal PRIME+PROBE an attacker can infer vic-
tim accesses (or a lack thereof) with near certainty after only 1
sequence of priming and probing, SCATTERCACHE degrades
this into a probabilistic process. At best, one PRIME+PROBE
operation on a target address can detect an access with a
−1. This is complicated further by the fact
probability of nways
that any one set of addresses is essentially single-use, as the
addresses will be cached in a non-colliding cache line with
a probability of 1− nways
−1 after only 1 access, where they
cannot be used to detect victim accesses anymore until they
themselves are evicted again.
Given the proﬁled address sets, we can construct general
probabilistic variants of the PRIME+PROBE attack. While
other methods are possible, we believe the 2 described in the
following represent lower bounds for either victim accesses
or memory requirement.
Variant 1: Single collision with eviction. We partition
our set of addresses, such that one PRIME+PROBE set con-
sists of nways addresses, where each collides with a different
way of the target address. To detect an access to the target,
we prime with one set, cause a target access, measure the
primed set and then evict the target address. We repeat this
process until the desired detection probability is reached. This
probability is given by p(naccesses) = 1− (1− nways
−1)naccesses.
The eviction of the target address can be achieved by either
evicting the entire cache or using preconstructed eviction sets
(see Section 4.3.2). After the use of an eviction set, a differ-
ent priming set is necessary, as the eviction sets only target
the victim address. After a full cache ﬂush, all sets can be
reused. The amount of colliding addresses we need to ﬁnd
during proﬁling depends on how often a full cache ﬂush is
performed. This method requires the least amount of accesses
to the target, at the cost of either execution time (full cache
ﬂushes) or memory and proﬁling time (constructing many
eviction sets).
Variant 2: Single collision without eviction. Using the
same method but without the eviction step, the detection prob-
ability can be recursively calculated as
p(nacc.) = p(nacc. − 1) + (1− p(nacc. − 1))(
)
−1. This variant provides decreasing ben-
with p(1) = nways
eﬁts for additional accesses. The reason for this is that the
probability that the last step evicted the target address inﬂu-
ences the probability to detect an access in the current step.
While this approach requires many more target accesses, it
has the advantage of a shorter proﬁling phase.
2· nways − 1
nways3
These two methods require different amounts of mem-
ory, proﬁling time and accesses to the target, but they can
also be combined to tailor the attack to the target. Which is
most useful depends on the attack scenario, but it is clear that
both come with considerable drawbacks when compared to
PRIME+PROBE in current caches. For example, achieving
a 99 % detection probability in a 2 MB Cache with 8 ways
requires 35 target accesses and 9870 proﬁled addresses in
308 MB of memory for variant 1 if we use an eviction set for
every probe step. Variant 2 would require 152 target accesses
and 1216 addresses in 38 MB of memory. In contrast, regular
PRIME+PROBE requires 1 target access and 8 addresses while
providing 100 % accuracy (in this ideal scenario). Detecting
non-repeating events is made essentially impossible; to mea-
sure any access with conﬁdence requires either the knowledge
that the victim process repeats the same access pattern for
long periods of time or control of the victim in a way that
USENIX Association
28th USENIX Security Symposium    685
allows for repeated measurements. In addition to the large
memory requirements, variant 1 also heavily degrades the
temporal resolution of a classical PRIME+PROBE attack be-
cause of the necessary eviction steps. This makes trace-based
attacks like attacks on square-and-multiply in RSA [76] much
less practical. Variant 2 does not suffer from this drawback,
but requires one PRIME+PROBE set for each time step, for as
many high-resolution samples as one trace needs to contain.
This can quickly lead to an explosion in required memory
when thousands of samples are needed.
4.5 Challenges with Real-World Attacks
We failed at mounting a real-world attack (i.e., with even
the slightest amounts of noise) on SCATTERCACHE. Gener-
ally, for a PRIME+PROBE attack we need to (1) generate an
eviction set (cf. Section 4.3), and (2) use the eviction set to
monitor a victim memory access. If we assume step 1 to be
solved, we can mount a cache attack (i.e., step 2) with a com-
plexity increases by a factor of 152 (cf. Section 4.4). For some
real-world attacks this would not be a problem, in particular
if a small fast algorithm is attacked, e.g., AES with T-tables.
Gülmezoglu et al. [29] recovered the full AES key from an
AES T-tables implementation with only 30 000 encryptions
in a fully synchronized setting (that can be implemented with
PRIME+PROBE as well [26]), taking 15 seconds, i.e., 500 µs
per encryption. The same attack on SCATTERCACHE takes
4.56 · 106 encryptions, i.e., 38 minutes assuming the same
execution times, which is clearly viable.
However, the real challenge is solving step 1, which we did
not manage for any real-world example. In particular, even if
AES would only perform a single attacker-chosen memory
access (instead of 160 to the T-tables alone, plus additional
code and data accesses), which would be ideal for the attacker
in the proﬁling during step 1, we would need to observe 33.5
million encryptions. In addition to the runtime reported by
Gülmezoglu et al. [29] we also need a full cache ﬂush after
each attack round (i.e., each encryption). For a 2 MB cache,
we need to iterate over a 6 MB array to have a high probability
of covering all cache lines. The time for an L3-cache access is
e.g., for Kaby Lake 9.5 ns [2]. The absolute minimum number
of cache misses here is 65536 (=4 MB), but in practice it will
be much higher. A cache miss takes around 50 ns, hence, the
full cache eviction will take at least 3.6 ms. Consequently,
with 33.5 million tests required to generate the eviction set
and a runtime of 4.1 ms per test, the total runtime to generate
the eviction set is 38 hours.
This number still only considers the theoretical setting of
a completely noise-free and idle system. The process doing
AES computations must not be restarted during these 38 hours.
The operating system must not replace any physical pages and,
most importantly, our hypothetical AES implementation only
performs a single memory access. In any realistic setting with
only the slightest amount of activity (noise) on the system, this
easily explodes to multiple weeks or months. With a second
memory access, these two memory accesses can already not
be distinguished anymore with the generated eviction set,
because the eviction set is generated for an invocation of the
entire victim computation, not for an address.
4.6 Noise Sampling
The previous analysis considered a completely noise-free
scenario, where the attacker performs PRIME+PROBE on a
single memory access executed by the victim. However, in a
real system, an attacker will typically not be able to perform
an attack on single memory accesses, but face different kinds
of noise. Namely, on real systems cache attacks will suffer
from both systematic and random noise, which reduces the
effectiveness of proﬁling and the actual attack.
Systematic noise is introduced, for example, by the victim
as it executes longer code sequences in between the attacker’s
prime and probe steps. The victim’s code execution intrin-
sically performs additional memory accesses to fetch code
and data that the attacker will observe in the cache deter-
ministically. In SCATTERCACHE, the mappings of memory
addresses to cache lines is unknown. Hence, without addi-
tional knowledge, the attacker is unable to distinguish the
cache collision belonging to the target memory access from
collisions due to systematic noise. Instead, the attacker can
only observe and learn both simultaneously. As a result, larger
eviction sets need to be constructed to yield the same conﬁ-
dence level for eviction. Speciﬁcally, the size of an eviction
set must increase proportionally to the number of systematic
noise accesses to achieve the same properties. While this
signiﬁcantly increases an attackers proﬁling effort, they may
be able to use clustering techniques to prune the eviction set
prior to performing an actual attack.
Random noise, on the other hand, stems from arbitrary
processes accessing the cache simultaneously or as they are
scheduled in between. Random noise hence causes random
cache collisions to be detected by an attacker during both pro-
ﬁling and an actual attack, i.e., produces false positives. While
attackers cannot distinguish between such random noise and
systematic accesses in a single observation, these random
noise accesses can be ﬁltered out statistically be repeating the
same experiment multiple times. Yet, it increases an attackers
effort signiﬁcantly. For instance, when building eviction sets
an attacker can try to observe the same cache collision multi-
ple times for a speciﬁc candidate address to be certain about
its cache collision with the victim.
Random noise distributes in SCATTERCACHE according
to Equation 1 and hence quickly occupies large parts of the
cache. As a result, there is a high chance of sampling ran-
dom noise when checking a candidate address during the
construction of eviction sets. Also when probing addresses of
an eviction set in an actual attack, random noise is likely to
be sampled as attacks on SCATTERCACHE demand for large
686    28th USENIX Security Symposium
USENIX Association
y
t
i
l
i
b
a
b
o
r
P
0.1
0.01
Way 0
Way 1
Way 2
Way 3
0
20
60
40
80
Cache Line Index
100
120
Figure 7: Example distribution of cache indices of addresses
in proﬁled eviction sets (nways = 4, bindices = 7).
]
%
[
s
e
l
p
m
a
S
y
s
i
o
N
100
80
60
40
20
0
4 Ways
8 Ways
16 Ways
0
2,000
6,000
4,000
Noise Accesses
8,000
10,000
Figure 8: Expected percentage of noisy samples in an eviction
set for a cache consisting of 212 cache lines.
eviction sets. As our analysis shows, for a single cache way the
distribution of cache line indices corresponding to the mem-
ory accesses of proﬁled eviction sets (cf. Section 4.3) adheres
to Figure 7. Clearly, due to proﬁling there is a high chance of
roughly 1/nways to access the index that collides with the vic-
tim address. However, with p = (nways − 1)/nways the index
adheres to an uniformly random selection from all possible in-
dices and hence provides a large surface for sampling random
noise. Consequently, for a cache with nlines = 2bindices lines
per way and nnoise lines being occupied by noise in each way,
the probability of sampling random noise when probing an
eviction set address is
p(Noise) ≈ nways − 1
nways
nnoise
nlines
.
Figure 8 visualizes this effect and in particular the percentage
of noisy samples encountered in an eviction set for different
cache conﬁgurations and noise levels. While higher random
noise clearly increases an attackers effort, the actual noise
level strongly depends on the system conﬁguration and load.
4.7 Further Remarks
In the previous analysis, the SDIDs of both attacker and victim
were assumed to be constant throughout all experiments for
statistical analysis to be applicable. Additionally, systematic
and random noise introduced during both proﬁling and attack
further increase the complexity of actual attacks, rendering
attacks on most real-world systems impractical.
Also note that the security analysis in this section focuses
on SCv1. In a noise-free scenario, SCv2 may allow to con-
struct eviction sets slightly more efﬁciently since its IDF is
a permutation. This means that, once a collision in a certain
cache way is found, there will not be any other colliding ad-
dress for that cache way in the same index range, i.e., for
the same address tag. Considering the expected time to ﬁnd
the single collision in a given index range, this could give
an attacker a beneﬁt of up to a factor of two in constructing
eviction sets. However, in practice multiple cache ways are
proﬁled simultaneously, which results in a high chance of
ﬁnding a collision in any of the cache ways independent of
the address index bits, i.e., the nways indices for a certain mem-
ory address will very likely be scattered over the whole index
range. Independent of that, the presence of noise signiﬁcantly
hampers taking advantage of the permuting property of SCv2.
5 Performance Evaluation
SCATTERCACHE signiﬁcantly increases the effort of attack-
ers to perform cache-based attacks. However, a countermea-
sure must not degrade performance to be practical as well.
This section hence analyzes the performance of SCATTER-
CACHE using the gem5 full system simulator and GAP [9],
MiBench [30], lmbench [49], and the C version of scimark2 1
as micro benchmarks. Additionally, to closer investigate the
impact of SCATTERCACHE on larger workloads, a custom
cache simulator is used for SPEC CPU 2017 benchmarks.
Our evaluations indicate that, in terms of performance, SCAT-
TERCACHE behaves basically identical to traditional set-
associative caches with the same random replacement policy.
5.1
gem5 Setup
We performed our cache evaluation using the gem5 full sys-
tem simulator [12] in 32-bit ARM mode. In particular, we
used the CPU model TimingSimpleCPU together with a cache
architecture such as commonly used in ARM Cortex-A9
CPUs: the cache line size was chosen to be 32 bytes, the 4-way
L1 data and instruction caches are each sized 32 kB, and the
8-way L2 cache is 512 kB large. We adapted the gem5 simula-
tor such as to support SCATTERCACHE for the L2 cache. This
allows to evaluate the impact of six different cache organiza-
tions. Besides SCATTERCACHE in both variants (1) SCv1 and
(2) SCv2 and standard set-associative caches with (3) LRU,
(4) BIP, and (5) random replacement, we also evaluated (6)
skewed associative caches [63] with random replacement as
we expect them to have similar performance characteristics
as SCv1 and SCv2.
On the software side, we used the Poky Linux distribution
from Yocto 2.5 (Sumo) with kernel version 4.14.67 after ap-
plying patches to run within gem5. We then evaluated the per-
formance of our micro benchmarks running on top of Linux.
In particular, we analyzed the cache statistics provided by
1https://math.nist.gov/scimark2/
USENIX Association
28th USENIX Security Symposium    687
gem5 after booting Linux and running the respective bench-
mark. Using this approach, we reliably measure the cache
performance and execution time for each single application,
i.e., without concurrent processes. Since only the L2-cache
architecture (i.e., replacement policy, skewed vs. ﬁxed sets)
changed between the individual simulation runs, execution
performance is simply direct proportional to the resulting
cache hit rate. To enable easier comparison between the indi-
vidual benchmarks as well as with related work we therefore
mainly report L2-cache hit results.
SCATTERCACHE IDF Instantiations. Both SCATTER-
CACHE variants have been instantiated using the low-latency
tweakable block cipher QARMA-64 [8]. In particular, in the
SCv1 variant, the index bits for the individual cache ways
have been sliced from the ciphertext of encrypting the cache
line address under the secret key and SDID. On the other hand,
due to the lack of an off-the-shelf tweakable block cipher with
the correct block size, a stream cipher construction was used
in the SCv2 variant. Namely, the index is computed as the
XOR between the original index bits and the ciphertext of
the original tag encrypted using QARMA-64. Note, however,
that, although this construction for SCv2 is a proper permu-
tation and entirely sufﬁcient for evaluating the performance
of SCv2, we do not recommend the construction as pads are
being reused for addresses having the same tag bits.
While the majority of the following results are latency
agnostic LLC hit rates, all following results are reported for
the zero cycle latency case. For QARMA-64 with 5 rounds,
ASIC implementation results with as little as 2.2 ns latency
have been reported [8]. We are therefore conﬁdent that, if
desired, hiding the latency of the IDF by computing it in
parallel to the lower level cache lookup is feasible.
However, we still also conducted simulations with la-
tency overheads between 1 and 5 cycles by increasing the
tag_latency of the cache in gem5. The acquired results
show that, even for IDFs which introduce 5 cycles of latency,
less than 2 % performance penalty are encountered on the
GAP benchmark suite. These numbers are also in line with
Qureshi’s results reported for CEASER [55].
5.2 Hardware Overhead Discussion
SCATTERCACHE is designed to be as similar to modern cache
architectures as possible in terms of hardware. Still, area and
power overheads have to be expected due to the introduction
of the IDF and the additional addressing logic. Unfortunately,
while probably easy for large processor and SoC vendors,
determining reliable overhead numbers for these two metrics
is a difﬁcult task for academia that requires an actual ASIC
implementation of the cache. To the best of our knowledge,