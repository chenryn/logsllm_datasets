of false, it invokes one of left-pad’s padding behaviors that
Harp learned through other false-y values. The runtime perfor-
mance of the regenerated left-pad on 10K runs is 45.66 seconds—
an overhead of about 20µs per run over the performance of the
original library.
7.4 Use Case: String-Compare
The string-compare attack involves two versions of a single li-
brary in the same codebase [10]. An earlier version of this library,
used as part of a sort function, is benign. A later version, used in
the authentication module, is malicious: if provided the (authen-
tication) string gbabWhaRQ, it access the file system of the server
running the program.
Security: We apply Harp to both versions of string-compare
library. The regenerated version is identical in both cases. It does not
contain any side-effects—nor the check that launches the attack in
the second case. Harp eliminates the string-compare dependency
from both sort and auth, replacing it with vulnerability-free code.
The regenerated string-compare makes no use of built-in APIs,
resulting in a privilege-reduction score of 332×.
Performance & correctness: Harp completes string-compare’s
active learning and regeneration in an average of 0.7 seconds. We
manually inspect the regenerated code and confirm it implements
string-compare’s full functionality. As string-compare comes
with no test cases, we apply the test cases of the sort function over a
shuffled version of Ubuntu’s wamerican dictionary words file (102K
elements); all 3 (100%) test cases pass. The runtime performance of
10K sort iterations using the regenerated string-compare takes
46.01 seconds—an overhead of about 41µs per run over the perfor-
mance of the original library.
7.5 Applying Harp to More Libraries
In this section, we apply Harp to 25 JavaScript libraries—17 string-
processing libraries and 11 other libraries—and 5 C/C++ libraries
collected from GitHub.
Table 1 shows results for 17 JavaScript string-processing libraries.
The statistics columns D1–D4 count the number of weekly down-
loads, direct dependents, total dependents, and direct dependencies
of these libraries as reported by the npm tool: collectively, these li-
braries can affect a significant fraction of the ecosystem—they total
102M downloads per week, are directly depended upon by a total of
4.3K libraries, and are indirectly depended upon by more than 15K
libraries and applications. The Learning columns t1 and t2 show
the time it took Harp to apply active learning and regeneration;
t1 is full Harp, whereas t2 does not include Harp’s performance
refinements (§6). The Regeneration columns Performance and Cor-
rectness show the characteristics of the regenerated library with
respect to the original: Performance is measured using 10K itera-
tions of several tests; and Correctness is measured by running all
the tests of the original library and 10 client-libraries against the
regenerated library, followed by manual inspection.
Learning: Harp’s active learning and regeneration takes between
0.7 seconds and 50.9 minutes (avg.: 204.83 seconds) to complete,
with 14 out of 17 libraries regenerated within a minute and 16
out of 17 libraries regenerated within 145 seconds (2.4 minutes).
The regenerated camel-case library stands out in terms of size,
containing 8 computational statements, two of which are split
operators, taking 3059 seconds (50.9 minutes) to regenerate.
Harp’s performance refinements (§6) offer significant improve-
ments. Without any refinements, Harp takes at least 179.27× longer.
This value is a conservative estimate because Harp reaches a time-
out limit of 12 hours for 7 out of 17 libraries. This speedup includes
a 1.1× slowdown for 5 small libraries that are penalized by Harp’s
refinements. As the regeneration of these libraries remains within
a few seconds, their slowdown is considered acceptable—especially
given the overall speedup of long-running regenerations.
In terms of code coverage, the input generation algorithm exer-
cises 100% of library code for 11 out of 17 libraries. For the remaining
six libraries, the majority of the functionality not exercised is re-
lated to exception handling. In the case of decamelize (80%), Harp
does not exercise four lines handling erroneous input (non-string
arguments), and 11 lines related to a flag preserving consecutive
upper case. In the case of flatmap-stream (62.2%), Harp does not
exercise a subset of the stream-specific functionality—stream pause,
resume, destroy and end handlers—that are part of superclass func-
tionality. Harp also does not exercise exception handlers in the
write method, which are meant to handle errors propagating up
from the stream consumer. In the case of repeat-string (95.45%),
Harp misses an exception-raising statement meant to cover cases
where the first argument is not a string. The trim library (33.3%)
first checks if the input string’s prototype includes a trim method
and if so it invokes it; otherwise, implements a left and right trim
by invoking other methods—but Harp’s primary inputs always sup-
port trim as part of the string prototype. In the case of upper-case
(44%), Harp misses all the locale-specific code (confirmed by the
tests). In the case of zero-fill (80%), Harp misses a branch for
when the second argument (the “filler” string) is not provided—in
which case the library returns a partially applied function.
Performance: To understand the runtime performance of regen-
erated libraries, we apply them on the tests of the original libraries
in tight loops of 10K iterations. Their performance is between -1.6%
(speedup) and 6.4% (slowdown), with an average of 2.3%. Profil-
ing shows that the overhead comes from Harp’s complex pattern
matching primitives which compile down to the language’s regular-
expression language (REL). REL is in fact not regular, as it supports
back-references and other non-regular constructs, and thus does
not perform as efficiently as the simpler string-matching constructs
found in the original libraries.
Session 6B: Web Vulnerabilities CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1765Tab. 1: Applying Harp to JavaScript libraries. Columns D1−4 show weekly downloads, direct dependents, total dependents, and direct dependencies;
columns t1 and t2 show the time it took to complete ALR, with and without refinements; column Coverage shows the percentage of the source code covered
by the input generation algorithm; other columns show the characteristics of the regenerated libraries compared to the original libraries.
Library
camel-case
constant-case
decamelize
flatmap-stream
left-pad
no-case
pascal-case
repeat-string
sentence-case
snake-case
string-compare
trim-left
trim-right
trim
upper-case
write-pad
zero-fill
Min
Max
Avg
Popularity
D2
603
88
1042
0
509
114
540
580
58
293
0
6
200
206
115
0
43
0
1042
258.6
D3
1484
1651
1132
0
518
2463
2979
832
1636
1386
0
8
249
472
331
0
175
0
2979
900.9
D4
2
3
0
0
0
2
2
0
3
2
1
0
0
0
1
1
0
0
3
1
D1
13,007,997
3,104,230
22,883,567
70
3,077,112
13,051,868
8,995,694
17,921,038
2,809,582
3,045,948
46
789
3,874,815
3,684,237
6,888,443
15
35,143
15
22,883,567
6,022,388
t1 (s)
3059
22
4
1.4
3.6
28
145
19
129
36
0.7
3
2
21
3
3
3
0.7
3K
204.83
Learning
t2 (s)
>12h
>12h
14340
>12h
1.3
>12h
>12h
8
>12h
>12h
1.2
3.5
3.2
111
1.9
1.4
1.6
1.2
>12h
>10.2h
Cov/ge (%)
100
100
80
71.21
100
100
100
95.45
100
100
100
100
100
33.33
44.44
100
80
33.33
100
86.04
Regeneration
Perf/nce (s) (%)
(5.6%)
10.15
(3.8%)
9.86
(2.9%)
9.61
48.99
(2.2%)
(0.4%)
45.66
(5.3%)
9.89
(6.4%)
10.15
(-1.6%)
9.18
10.02
(4.9%)
(3.4%)