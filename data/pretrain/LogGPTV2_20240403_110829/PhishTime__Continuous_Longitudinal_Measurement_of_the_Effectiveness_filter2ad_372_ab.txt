diversity of the sample: we found many URLs unique
to each respective source, likely due to di(cid:29)erences in
their data collection and detection approaches.
Report URLs and Prune if Blacklisted. If any URL is not
initially blacklisted, we report it ( 2 ) directly to the black-
lists,andtootherkeyanti-phishingentities,inane(cid:29)ortto
get it blacklisted (using the approach and infrastructure
described in Section 7.2). We subsequently prune URLs
blacklisted within a reasonably short period thereafter
and retain those that are not. Recent work has shown
that once detected by a blacklist’s backend, the majority
of phishing URLs show blacklist warnings within two
hours [46]. We, therefore, chose a blacklisting cuto(cid:29)
of two hours to eliminate URLs that blacklists could
successfully detect, but likely originally failed to discover.
Analyze (Evasive) Phishing Websites. We then manu-
ally inspect the remaining URLs ( 3 ) to understand why
they have been evading blacklist detection. We analyze
the evasion techniques used as well as the behavior (i.e.,
general appearance and user interface) of the website.
We performed this step by (cid:27)rst visiting each URL, and
then testing di(cid:29)erent variations of request parameters
until we successfully retrieved the content. We can thus
infer the server-side evasion techniques used by each
phishing website. We also analyze each website visually,
and inspect client-side source code, to not only uncover
any additional evasion logic,but to compare the websites
to samples of known phishing kits available to us to
determine which are the most common. Simultaneously,
we identify and exclude false positive or o(cid:31)ine websites.
The rest of PhishTime’s operation leverages the insights
extracted from criminals’ phishing websites and, through
the automated deployment of our own arti(cid:27)cial phishing
websites that mimic them, achieves continual monitoring
of blacklist performance.
Design (Evasion-inspired) Experiments. After analyz-
ing a representative sample of URLs, we abstract the
key trends that we observed and design experiments
to replicate them in a controlled setting ( 4 , Section 6).
Figure 1: High-level overview of the PhishTime framework.
our evaluation on these top three global blacklists and pay
particular attention to GSB due to its large potential impact.
There exist other browser blacklists with a lower global
market share but with prominence in speci(cid:27)c countries,
such as Tencent Security and Yandex Safe Browsing [8,56,67].
In our experiments, we do not consider these blacklists or
other anti-phishing systems that are not enabled by default
in browsers, such as third-party browser plugins or antivirus
software [52]. However, our methodology and framework
could be applied to evaluate these alternatives.
4 PhishTime Overview
An e(cid:29)ective way to empirically evaluate the performance
of anti-phishing blacklists is to deploy a large batch of
specially-con(cid:27)gured test phishing websites, report the
websites directly to blacklists, and then monitor each website
to see if and when it is blacklisted [44,48]. For our longitudinal
evaluation of blacklist performance, we make a series of such
deployments, at regular intervals, over an extended period
of time. Within each deployment, we con(cid:27)gure multiple
distinct batches of websites to support di(cid:29)erent experiments.
The goal of our experiments is to provide insight into
potential gaps within the ecosystem, which could, in turn,
lead to actionable security recommendations. We therefore
seek to closely replicate the phishing website con(cid:27)gurations
(i.e., evasion techniques) used by attackers. To identify
such con(cid:27)gurations and guide our experimental design, we
developed the PhishTime framework.
We obtained permission from PayPal, Inc. to use PayPal-
branded phishing websites throughout our experiments3.
Therefore,in ourPhishTime ecosystem analysis,we also focus
on PayPal phishing websites in the wild. Although we were
unable to collaborate with other companies for this research,
ourmethodology is generic and could be used forany brand(s).
The PhishTime framework is our systematic, semi-
automated approach for identifying evasive (i.e., unmitigated)
phishing websites in the wild. We use the framework to char-
acterize both typical and emerging evasion techniques used
by real phishing websites. Understanding the ecosystem’s
response to typical phishing enables identi(cid:27)cation of gaps
currently being exploited by attackers, whereas analysis
of less prominent emerging evasion techniques allows us
3In the current ecosystem, PayPal is among the brands most commonly
targeted by phishers [59].
382    29th USENIX Security Symposium
USENIX Association
Deploy PhishTime Experiments. Finally, we
deploy
these experiments ( 5 , Section 7) to evaluate blacklist
performance, over time, in the face of diverse evasion.
Ecosystem Recommendations. We use our experimental
results to make security recommendations for speci(cid:27)c
blacklists or the ecosystem ( 6 , Sections 8-9). Any result-
ing ecosystem changes can then in(cid:30)uence the design
of experiments in successive framework deployments.
5 PhishTime Analysis
We used the PhishTime framework in January 2019 to
identify phishing websites in the wild capable of successfully
evading blacklisting for extended periods of time. We then
characterized typical evasion techniques used by these
websites, and we designed experiments which entailed
deploying a series of PhishTime-crafted phishing websites
to empirically measure the response of blacklists to these
techniques, in a controlled manner. Later, in August 2019,
we used the framework to identify less common (but more
sophisticated) emerging evasion techniques, and we designed
additional experiments to test these techniques. We show
a timeline of our ecosystem analysis using PhishTime, and
the subsequent experiment deployments, in Figure 2.
5.1 Typical Evasion Techniques
In total, we analyzed 4,393 distinct phishing URLs in the wild
and found that 183 failed to be promptly blacklisted. Although
this may seem like a relatively small number, prior work has
shown that the majority of real-world damage from phishing
occurs from a small fraction of known phishing URLs [46].
Moreover, the total URL count for the ecosystem would be
considerably higher, as we focused only on a single brand.
Of these 183 websites, 96 were never blacklisted anywhere
before going o(cid:31)ine (the average observed lifespan was 17
hours, 12 minutes), 87 were ultimately blacklisted in at least
one desktop browser (with an average observed speed of
7 hours, 4 minutes) and 23 were ultimately blacklisted in at
least one mobile browser (with an average observed speed
of 12 hours, 2 minutes). We also observed 10 websites which
remained live, without blacklisting, for over one week. Note
that due to the inherent delay between an attacker’s deploy-
ment of a phishing URL and its appearance in a report or feed,
the aforementioned timings represent lower bounds [34].
By analyzing URLs in the e-mail lures reported to PayPal,
we found that 177 of these websites had lure URLs which
redirected to a di(cid:29)erent (cid:27)nal landing URL with the phishing
content. We observed redirection URLs both through third-
party redirection services and attacker-controlled domains.
In the latter case, we commonly observed JavaScript-based
redirection alongside traditional HTTP redirection [20]. We
also observed that at least 146 of these websites used some
form of server-side cloaking [45]: we were unable to retrieve
their content using a cloud-based web crawler but succeeded
when using a mobile IP address or anonymous VPN service.
Figure 2: Timeline of framework & experiment deployments.
Atleast42websiteshadadi(cid:29)erentURLpathorasubdomain
of a domain that appeared in another phishing website, which
re(cid:30)ects phishers’ tendency to re-use infrastructure.
5.2 Emerging Evasion Techniques
We found that eight of the 96 phishing websites which
were never blacklisted implemented clever mechanisms to
evade detection: (cid:27)ve presented visitors with a CAPTCHA
challenge prior to displaying the phishing content, two
required the user to click on a button in a popup window
prior to redirecting to the phishing page, and one would not
render content prior to detecting mouse movement. We refer
to these evasion techniques as behavior-based because they
require a speci(cid:27)c user behavior to display phishing content.
6 Experimental Design
We now transition from merely observing the ecosystem
to actively measuring it: to methodically test the phishing
website con(cid:27)gurations we observed, we replicate them across
a large sample of ourown new arti(cid:27)cial phishing websites. We
deploy these websites, report the respective URLs to several
anti-phishing entities, and monitor the speed and coverage
of blacklists as they respond to our reports. We conducted
our experiments ethically, to avoid harming any real users
or anti-phishing systems, as discussed in Section 9.2.
In total, we made one preliminary deployment in March
2019 and six main deployments of experiments at regular
intervals between May 2019 and January 2020. The purpose of
the preliminary deployment—which mirrored the con(cid:27)gura-
tionofthe(cid:27)rstmaindeployment—wastoverifythesoundness
of our experimental design and the technical correctness of
our framework. We summarize our deployments in Table 1.
Across the six main deployments, we launched 2,862
phishing websites as part of seven di(cid:29)erent experiments.
We registered a total of 2,646 new .com domain names for
these websites. Because some of our experiments involved
redirection links, an additional 1,296 such links bring our
overall URL count to 4,158. As our experiments seek to make
several distinct measurements over time, each deployment
includes multiple di(cid:29)erent experiments.
Each experiment consists of one or more distinct batches of
phishing websites: groups that share a single con(cid:27)guration
corresponding to the respective experiment. We chose our
batch size, 54, by estimating the required number of domains
(i.e., which we would then purchase) for a sample size that
could support statistically signi(cid:27)cant inferences in one-way
ANOVA among sets of batches: To obtain a power of 0.95 at
a p-value of 0.05, we initially assumed a medium e(cid:29)ect size
of 0.25 [12]. Using the baseline GSB blacklist speed observed
USENIX Association
29th USENIX Security Symposium    383
Experiment
1
May
2
Jul.
Basic Evasion
Typical Evasion (Redirection)
A Baseline
B
C
D Domain Reuse
E
F
G Evidence-based Reporting
Discovery
Emerging Evasion
Deployment
3
Sep.
4
Oct.
5
Nov.
6
Dec.
Per Deployment
Batches Websites URLs
1
1
3
3
2
7
2
54
54
324*
324*
108
378
108
54
54
162
162
108
378
108
Total
Batches Websites
6
6
12
12
8
7
2
324
324
648
648
432
378
108
Domains
Registered
324
324
1,080
0
432
378
108
Table 1: Experiments conducted during each of our main deployments (*half are redirection links).
in the preliminary deployment, we calculated a higher e(cid:29)ect
size of 0.36, which suggests an adequate sample size selection.
6.1 Measuring Blacklist Speed & Coverage
The experiments in this section focus primarily on measuring
the detection performance (i.e., speed and coverage) of black-
lists. As we believe that it is generally infeasible for attackers
to avoid discovery when conducting traditional phishing
attacks (e.g., at scale through e-mail spam), our reporting
methodology seeks to ensure that all URLs we deploy as
part of these experiments are promptly discovered by the
blacklists we test. We do so by simultaneously reporting the
URLs to multiple blacklists and other anti-phishing entities,
which we elaborate on in Section 7.2.
ExperimentA:Baseline.Foroursimplestexperiment,we
launch a single batch of basic phishing websites, with no eva-
sion technique, once in each deployment. These, and all other
websites we deploy, used HTTPS to match current ecosystem
trends [2]. This experiment serves two key purposes: to
establish a baseline for the best-case speed and coverage
provided by blacklists (for comparison to other experiments),
and to measure if these metrics remain consistent over time.
Experiment B: Basic Evasion. In this experiment, we
test two straightforward cloaking techniques inspired by our
observations in Section 5.1: websites that only allow tra(cid:28)c
from browsers with a mobile user agent [20,30], and websites
that render content using JavaScript. We alternate these two
cloaking techniques between deployments.
This experiment allows us to evaluate blacklists’ response
to slightly more sophisticated phishing by comparing against
the baseline response in Experiment A. It also establishes a
point of comparison for even more sophisticated phishing in
later experiments. A secondary objective of this experiment
is to assess blacklist coverage (on mobile devices) of phishing
websites aimed speci(cid:27)cally at mobile users. Mobile devices
have historically been prone to phishing [63], and recent
work has revealed gaps in blacklisting on mobile devices [44].
Experiment C: Typical Evasion (Redirection). Each
deployment in this experiment has three batches of websites
that focus on evaluating the evasiveness of redirection. In
a one-to-one mapping, we pair each phishing website with
a di(cid:29)erent URL that redirects to it with an HTTP 302 status
code [20]. For this experiment, we only report the redirection
URLs (i.e., the URLs that could serve as lures in a phishing e-
mail). Wecon(cid:27)guredeachphishingwebsitewiththesameeva-
sion technique as Experiment B in the respective deployment.
In the (cid:27)rst of the three batches, we used a popular link
shortening service, bit.ly, to generate the redirection links.
Such services are commonly used by attackers to scalably
generate unique lures. In the second of the three batches,
we used our own .com domains (each di(cid:29)erent from the
website’s domain) for the redirection links. In the third batch,
we similarly used .com domains for the redirection links,
but additionally con(cid:27)gured them with server-side IP and
hostname cloaking [45]. The latter batch thus most closely
mirrors the typical con(cid:27)guration of the phishing websites
that we observed in Section 5.1; we based the cloaking
technique on the .htaccess (cid:27)le (which blocks known crawlers)
found in a phishing kit that we commonly observed in the
wild during the PhishTime analysis ( 3 ).
Because we only change one variable between the three
batches, we can compare the blacklisting of phishing websites
that combine redirection with cloaking on both the lure
and the phishing website with the blacklisting of websites
with lesser degrees of evasion. We can also evaluate the
feasibility for attackers to use, and the ecosystem’s mitigation
of, third-party redirection services.
Experiment D: Domain Reuse. After the completion
of each Experiment C deployment, we generate identical