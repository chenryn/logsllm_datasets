CIFAR-10
Conv
IMDB
BERT-LL
UTKFace
VGG-LL
10
5
1
10
5
1
10
5
1
10
5
1
0.128
0.132
0.222
0.018
0.029
0.049
0.297
0.327
0.348
0.24
0.343
0.375
𝛼 = 0.5 𝛼 = 1 𝛼 = 2
0.294
0.073
0.094
0.335
0.556
0.222
0.036
0.010
0.061
0.014
0.038
0.203
0.418
0.136
0.480
0.163
0.535
0.236
0.121
0.224
0.367
0.191
0.333
0.667
Size
60.7
47.8
40.3
158.4
171.3
207.3
111.5
120.9
126.8
41.1
45.1
48.3
Table 4: Clean accuracy and target damage for small mod-
els attacked with ClusterMatch with Label Flipping, re-
ported over the worst 1, 5, and 10 subpopulations. On BERT
we measure the performance sampling 10 clusters at the low-
est, medium, and highest confidence levels, due to running
time constraints. Subpopulation sizes (the column Size) are
averages over poison rates.
UCI Adult
0.837
Dataset Worst Clean Acc
UTKFace
VGG-FT
IMDB
BERT-FT
CIFAR-10
VGG-FT
10
5
1
10
5
1
10
5
1
0.963
0.913
0.863
Target Damage
𝛼 = 0.5 𝛼 = 1 𝛼 = 2
0.405
0.218
0.432
0.244
0.455
0.286
0.024
0.206
0.303
0.035
0.506
0.051
0.511
0.206
0.294
0.627
0.742
0.426
0.329
0.385
0.500
0.080
0.129
0.204
0.518
0.616
0.738
Size
57.3
38.1
29.0
148.5
136.2
129.0
175.6
180.9
144.0
Table 5: Clean accuracy and target damage for large mod-
els trained on datasets that have been attacked with
ClusterMatch and Label Flipping, reported over the worst
1, 5, and 10 subpopulations. On BERT we measure the per-
formance sampling 10 clusters at the lowest, medium, and
highest confidence levels, due to running time constraints.
Subpopulation sizes are averages over poison rates. These
attacks are often very damaging: 10 subpopulations from
UTKFace reach an average target damage of 40.5%, and 10
subpopulations on CIFAR-10 reach an average target dam-
age of 51.1%. Results on IMDB are also markedly better with
BERT-FT than on BERT-LL, with little collateral damage.
one subpopulation and an average of 33.5% over five subpopula-
tions. This reiterates the results from end-to-end training on UCI
Adult: ClusterMatch tends to outperform FeatureMatch when
both are applicable, but both are effective. On the IMDB data with
BERT-LL, ClusterMatch appears to be rather ineffective until the
Models CIFAR-10 UCI Adult UTKFace
Small
Large
0.3%
2.9%
1.4%
1.3%
1.4%
N/A
IMDB
-0.29%
0.53%
Table 6: Mean collateral for Label Flipping poisoning attacks
on worst 5 subpopulations by target damage with 𝛼 = 1.
Small models: Conv for CIFAR-10, VGG-LL for UTKFace,
BERT-LL for IMDB. Large models: VGG-FT for CIFAR-10,
VGG-FT for UTKFace, BERT-FT for IMDB.
poisoning attack gets large, causing a target damage of 20.3% in one
subpopulation, when 𝛼 = 2, but under 5% when 𝛼 = 0.5 or 𝛼 = 1.
However, on BERT-FT the attack has markedly better performance,
with peak target damage of ≈ 20% already at 𝛼 = 1, which grows to
an average of 30.3% for 𝛼 = 2, over five subpopulations, peaking at
50.6% on the most effected one. The maximum registered collateral
damage for these attacks was 0.16% and 0.99%, for BERT-LL and
BERT-FT respectively, with 𝛼 = 2.
For other large models, we find that the attacks are also very
effective, despite the higher base accuracy, with many subpopula-
tions also from the UTKFace and CIFAR-10 datasets significantly
impacted. With an average of only 38.1 poison points, five subpopu-
lations from UTKFace have an average target damage of 38.5% (with
𝛼 = 1), and still has a significant target damage of 24.4% with only
19.1 poison points on average (𝛼 = .5). On CIFAR-10, we observe
particularly damaging attacks: five subpopulations, when attacked
with 𝛼 = 1, reach an average target damage of 61.6%. Generally,
attacks on the larger models are more effective than attacks on
smaller models. This may be an indication that larger models learn
more subpopulation-specific decision boundaries.
We present in Table 6 the collateral of our poisoning attacks,
measuring the mean collateral damage over the worst-5 subpopula-
tions by target damage. The worst error decay occurs in UTKFace
with the fine tuned model, decreasing the accuracy by 2.9% accu-
racy while these subpopulations had accuracy decrease by 38.5%.
Meanwhile, the collateral is typically around 1.5% for other datasets
and models, and is insignificant for the last layer transfer learned
model on UTKFace, and for both models on the IMDB dataset.
5.4 Subpopulation Variance
We also attempt to understand how many subpopulations are sig-
nificantly impacted by our attacks, and how to determine which
subpopulations will be the most impacted. We first report the target
damage over all subpopulations for CIFAR and UTKFace (trends
are similar for all datasets). For UTKFace + VGG-LL, at 𝛼 = 2, we
find that 35% of subpopulations have a target damage of at least
4.96%, and 69% of subpopulations have a positive target damage. For
UTKFace + VGG-FT, 50% of subpopulations have a target damage
of at least 25%, and 90% of subpopulations have a target damage
of at least 14.7%. For CIFAR + Conv, 25% of subpopulations have
a target damage of at least 24.9%, and 65% of subpopulations have
a target damage of at least 14.1%. We find that 55% of subpopula-
tions for CIFAR + VGG-FT have target damage of at least 14.4%.
Target damages are not consistently high for all subpopulations
(sometimes even dipping to 0%, such as in UTKFace + VGG-LL),
but those datasets and models with large target damage for the
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3112Small
Low High
0.010
0.064
0.002
0.011
0.046
0.008
Large
Low High
0.214
0.101
0.142
0.030
0.108
0.218
Dataset
UTKFace
IMDB
CIFAR
Table 7: Target damages by confidence. Small and large in-
dicate model size (e.g. Small is Conv for CIFAR and Large
is VGG-FT). Low and high indicate confidence levels: aver-
age target damage of the 10 lowest and highest confidence
subpopulations for each dataset.
worst 10 subpoulations tend to have large target damage for further
subpopulations as well. The extent to which these less vulnera-
ble subpopulations are interesting to an adversary depends on the
domain and the subpopulation: a 15% target damage may still be
useful for the adversary. An adversary in practice may be able to
select from multiple subpopulations, improving its ability to select
a very vulnerable one.
One hypothesis for determining which subpopulations will be
heavily impacted is by measuring the mean confidence on the sub-
population assigned to the correct class of each point by a clean
model. Lower confidence subpopulations are closer to the decision
boundary of the model, so less modification to the model would
be required to impact a low confidence subpopulation relative to
a high confidence subpopulation. To evaluate whether this is the
case, we compute the target damage on the 10 highest and lowest
confidence subpopulations for our datasets. We report the results
in Table 7 (UCI Adult is not included as it has no small model). For
our small models, we find the low confidence subpopulations have
higher target damages on average than high confidence subpopula-
tions. For example, UTKFace has an average target damage of 6.35%
for low confidence, but only 1% for high confidence. Interestingly,
the trend reverses for large models: on UTKFace, low confidence
subpopulations have a 10.1% target damage, while high confidence
subpopulations have a 21.4% target damage. This is likely because
large models learn more subpopulation-specific decision boundaries
[20, 27]; large models make more uniform decisions over subpopu-
lations, so poisoning attacks can compromise each subpopulation
as a whole more easily. However, the relationship is not perfect
for any of the datasets or models; identifying a better predictor for
easily compromised subpopulations is an interesting task we leave
to future work.
5.5 Fairness Case Study
Our work is a proof-of-concept of poisoning attacks impacting
algorithmic fairness, a goal which has been attempted directly
by [10, 64]. This is most immediately seen when considering spe-
cific examples of FeatureMatch attacks. For example, on UCI
Adult, the subpopulation consisting of Black women high school
graduates drops from an accuracy of 91.4% to 76.7% when the attack
has size 𝛼 = 2. On UTKFace, the subpopulation which is easiest to
attack consists of people >60 years old and who are Latino/Middle
Eastern; this subpopulation’s accuracy drops from 100% to 60%.
Images containing white people from 30-45 years old have their
accuracy drop by 15.2%, in contrast. Some subpopulations gener-
ated by ClusterMatch may also have fairness implications due
to correlation with sensitive attributes, shown in Appendix C.
Aside from being a fairness concern, it is interesting that the
model is able to be compromised by FeatureMatch on UTKFace;
here, the model only has access to pixels, and does not explicitly
use demographic information. This indicates these demographics
are learned by the model independently of their correlation with
the target class, highlighted as a privacy risk in prior work [47, 65].
5.6 Attack Point Optimization
So far, our evaluation has focused on the label flipping attack,
which generalizes across many data modalities. However, for image
datasets, where we have the ability to continuously modify fea-
tures, we will show that there is a benefit to optimizing the feature
values to improve the attack. Because gradient-based optimization
is typically designed for continuous-value features [21, 30], we run
only on UTKFace.
We summarize our attack optimization results in Table 8. We
find that Gradient Optimization causes significant performance im-
provements on small models, nearly doubling the performance of