weights approach [20]. Although these methods have demonstrated
good performances on low dimensional datasets, they suffer from
either low data utility or high sampling complexity on high di-
mensional data, and therefore they are usually not suitable for the
high-dimensional image datasets discussed in this paper.
Another line of work adapts DP-SGD to GAN. DPGAN [59]
achieves differential privacy by adding Gaussian noise to the dis-
criminator gradients during the training process. DP-CGAN [54]
uses a similar approach to guarantee DP and trains a conditional
GAN to generate both synthetic data and labels. GS-WGAN [13]
uses the Wasserstein loss and sanitizes the data-dependent gra-
dients of the generator to improve data utility. However, these
approaches still suffer from low data utility when applied to high
dimensional datasets due to privacy budget explosion.
PATE-GAN [62] combines the PATE framework with GAN. It
trains multiple teacher discriminators and uses them to update the
student discriminator. However, in this framework, it essentially
applies PATE to train the discriminator within a GAN. Both the
teacher and students models are discriminators and the interac-
tion between the generator and discriminator is not adapted for the
teacher-student framework. Thus, PATE-GAN is also only evaluated
on low dimensional tabular data and suffers the similar problem
under limited privacy budget. G-PATE [37] improves upon PATE-
GAN by directly training a student generator using the teacher
discriminators. It uses the random projection algorithm to reduce
the gradient dimension during training, which is challenging to ana-
lyze its convergence. By combining the PATE framework with top-ùëò
gradient compression, DataLens demonstrates a significant utility
improvement upon PATE-GAN and G-PATE on high dimensional
datasets, with theoretical analysis on its convergence.
DP SGD Training. DPDL [2] is the first work that applies the
notion of Differential Privacy to the SGD training to prevent deep
neural models from exposing private information of training data.
DPDL also proposes to compute the privacy cost of the training by
moments accountant, which proves to be a tighter bound than the
strong composition theorem. McMahan et al. [39] adopts the notion
of R√©nyi differential privacy, which extends and generalizes the
moment accountant to multi-vector queries. Thus, the R√©nyi differ-
ential privacy analysis enables the framework to provide privacy
for heterogeneous sets of vectors and is widely adopted by current
open-source DP library (Tensorflow Privacy and Pytorch Opacus)
implementation. However, the high-dimensional data issue is still
present in these algorithms given the fact that the privacy budget
can be consumed quickly when aggregating these gradients in a
differentially private manner.
Gradient Compression. Communication efficient distributed learn-
ing has attracted intensive interests recently. Popular techniques
include gradient compression [4, 5, 49, 57], decentralization [28, 33],
and asynchronization [32] (see [6]). The essence of these methods
is to reason about the noise introduced via relaxations in the system
design. cpSGD [3] is proposed as a binomial DP-mechanism specif-
ically designed for stochastic ùëò-level gradient quantization [38] to
allow low-precision communication after adding DP noises. Extend-
ing this work, D2P-Fed [56] instead applies the discrete Gaussian
mechanism to the same ùëò-level quantization and achieves a stronger
privacy guarantee. Similarly, Kairouz et al. [25] combine discrete
Gaussian mechanism with ùëò-level quantization to facilitate fed-
erated learning with differential privacy and secure aggregation.
In comparison, DataLens uses PATE framework to give rigorous
privacy guarantee and apply sign compression as teacher voting to
save privacy budget. FetchSGD [48] focuses on communication-
efficiency in the federated learning setting, and proposes Count
Sketch data structure and top-ùëò operation for fast gradient com-
pression and aggregation. However, FetchSGD lacks the discussion
for privacy guarantee. In this paper, we explore the relationship
between privacy and gradient compression in a different scenario
and illustrate how gradient compression can help to achieve better
utility in privacy preserving algorithms over high dimensional data.
We propose TopAgg by combining stochastic sign [24] with top-ùëò
gradient compression. Our empirical results show that TopAgg
outperforms state-of-art gradient compression algorithms on im-
proving model utility with differential privacy guarantee.
7 CONCLUSION
Overall, we propose a novel and effective differentially private
data generative model DataLens, which is applicable to high-
dimensional data compared with existing approaches. In addition,
we propose a novel algorithm TopAgg to perform gradient com-
pression and aggregation. We provide the DP analysis as well as
convergence analysis for the proposed model. Extensive empirical
experiments demonstrate that DataLens substantially outperforms
the existing DP generative models on different especially high-
dimensional image datasets, even under limited privacy budget.
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their constructive feedback.
We also thank Dingfan Chen and many others for the helpful discus-
sion. This work is partially supported by the NSF grant No.1910100,
NSF CNS 20-46726 CAR, and Amazon Research Award.
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2158REFERENCES
[1] 2020. Opacus ‚Äì Train PyTorch models with Differential Privacy. https://opacus.ai/
[2] Mart√≠n Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy.
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security (2016).
[3] Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and
Brendan McMahan. 2018. cpSGD: Communication-efficient and differentially-
private distributed SGD. In Advances in Neural Information Processing Systems.
[4] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. 2017.
QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding.
In Advances in Neural Information Processing Systems.
[5] Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
Khirirat, and Cedric Renggli. 2018. The Convergence of Sparsified Gradient
Methods. In Advances in Neural Information Processing Systems.
[6] Tal Ben-Nun and Torsten Hoefler. 2019. Demystifying Parallel and Distributed
Deep Learning: An In-Depth Concurrency Analysis. ACM Comput. Surv. (2019).
[7] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar. 2018. signSGD: Compressed Optimisation for Non-Convex Prob-
lems. In Proceedings of the 35th International Conference on Machine Learning.
[8] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar. 2018. signSGD: Compressed Optimisation for Non-Convex Prob-
lems. Proceedings of the 35th International Conference on Machine Learning 80
(2018), 560‚Äì569.
[9] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar. 2018. signSGD: Compressed Optimisation for Non-Convex Prob-
lems. In Proceedings of the 35th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 80). PMLR, 560‚Äì569.
[10] Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed Mahloujifar,
Mohammad Mahmoody, Shuang Song, Abhradeep Thakurta, and Florian Tramer.
2020. An Attack on InstaHide: Is Private Learning Possible with Instance Encod-
ing? arXiv preprint arXiv:2011.05315 (2020).
[11] Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The secret sharer: Evaluating and testing unintended memorization in neural
networks. In 28th {USENIX} Security Symposium ({USENIX} Security 19). 267‚Äì
284.
[12] Chia-Yu Chen, Jiamin Ni, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Xiao Sun,
Naigang Wang, Swagath Venkataramani, Vijayalakshmi (viji) Srinivasan, Wei
Zhang, and Kailash Gopalakrishnan. 2020. ScaleCom: Scalable Sparsified Gradient
Compression for Communication-Efficient Distributed Training. Adv. Neural Inf.
Process. Syst. 33 (2020), 13551‚Äì13563.
[13] Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. 2020. GS-WGAN: A
Gradient-Sanitized Approach for Learning Differentially Private Generators.
Neural Information Processing Systems (NeurIPS) (2020).
[14] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. 2020. GAN-Leaks: A
Taxonomy of Membership Inference Attacks against Generative Models. In CCS
‚Äô20: 2020 ACM SIGSAC Conference on Computer and Communications Security,
Virtual Event, USA, November 9-13, 2020.
[15] Xiangyi Chen, Steven Z. Wu, and Mingyi Hong. 2020. Understanding Gradient
Clipping in Private SGD: A Geometric Perspective. In Advances in Neural Infor-
mation Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 13773‚Äì13782.
[16] Cynthia Dwork. 2008. Differential privacy: A survey of results. In International
conference on theory and applications of models of computation. Springer, 1‚Äì19.
[17] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends¬Æ in Theoretical Computer Science 9, 3‚Äì4
(2014), 211‚Äì407.
[18] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and
Bin Cui. 2020. Don‚Äôt Waste Your Bits! Squeeze Activations and Gradients for
Deep Neural Networks via TinyScript. In International Conference on Machine
Learning.
[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672‚Äì2680.
[20] Moritz Hardt, Katrina Ligett, and Frank McSherry. 2012. A simple and practical
algorithm for differentially private data release. In Advances in Neural Information
Processing Systems. 2339‚Äì2347.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition.
[22] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve G√ºrel, Bo
Li, Ce Zhang, Costas J. Spanos, and Dawn Xiaodong Song. 2019. Efficient Task-
Specific Data Valuation for Nearest Neighbor Algorithms. Proc. VLDB Endow. 12
(2019), 1610‚Äì1623.
[23] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve
G√ºrel, Bo Li, Ce Zhang, Dawn Xiaodong Song, and Costas J. Spanos. 2019. To-
wards Efficient Data Valuation Based on the Shapley Value. In AISTATS.
[24] Richeng Jin, Yufan Huang, Xiaofan He, Huaiyu Dai, and Tianfu Wu. 2020.
Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees. arXiv
preprint arXiv:2002.10940 (2020).
[25] Peter Kairouz, Ziyu Liu, and Thomas Steinke. 2021. The Distributed Discrete
Gaussian Mechanism for Federated Learning with Secure Aggregation. arXiv
preprint arXiv:2102.06387 (2021).
[26] J. Kiefer and J. Wolfowitz. 1952. Stochastic Estimation of the Maximum of a
Regression Function. Annals of Mathematical Statistics 23 (1952), 462‚Äì466.
[27] Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. 2019. Decentralized
Stochastic Optimization and Gossip Algorithms with Compressed Communi-
cation. In Proceedings of the 36th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and
Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, California, USA, 3478‚Äì3487.
[28] Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. 2019. Decentralized Sto-
chastic Optimization and Gossip Algorithms with Compressed Communication.
In Proceedings of the 36th International Conference on Machine Learning.
[29] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. [n. d.]. CIFAR-10 (Canadian
Institute for Advanced Research). ([n. d.]). http://www.cs.toronto.edu/~kriz/cifar.
html
[30] Yann LeCun. 1998.
The MNIST database of handwritten digits.
http://yann.lecun.com/exdb/mnist/ (1998).
[31] Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao,
and Lawrence Carin. 2017. ALICE: Towards Understanding Adversarial Learning
for Joint Distribution Matching. Advances in Neural Information Processing
Systems 30 (2017), 5495‚Äì5503.
[32] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. 2015. Asynchronous Par-
allel Stochastic Gradient for Nonconvex Optimization. In Advances in Neural
Information Processing Systems.
[33] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.
2017. Can Decentralized Algorithms Outperform Centralized Algorithms? A
Case Study for Decentralized Parallel Stochastic Gradient Descent. In Advances
in Neural Information Processing Systems.
[34] Hyeontaek Lim, David G Andersen, and Michael Kaminsky. 2019. 3LC: LIGHT-
WEIGHT AND EFFECTIVE TRAFFIC COMPRESSION FOR DISTRIBUTED MA-
CHINE LEARNING. In Proceedings of the 2nd SysML Conference.
[35] Ji Liu and Ce Zhang. 2020. Distributed Learning Systems with First-Order
Methods. Foundations and Trends¬Æ in Databases 9, 1 (2020), 1‚Äì100.
[36] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face
Attributes in the Wild. In Proceedings of International Conference on Computer
Vision (ICCV).
[37] Yunhui Long, Suxin Lin, Zhuolin Yang, Carl A Gunter, and Bo Li. 2019. Scal-
able differentially private generative student model via pate. arXiv preprint
arXiv:1906.09338 (2019).
[38] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial Intelligence and Statistics. PMLR,
1273‚Äì1282.
[39] H. Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov,
Nicolas Papernot, and Peter Kairouz. 2019. A General Approach to Adding
Differential Privacy to Iterative Training Procedures. arXiv:1812.06210 [cs.LG]
[40] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. 2018. Which Training
Methods for GANs do actually Converge?. In Proceedings of the 35th International
Conference on Machine Learning.
[41] Ilya Mironov. 2017. Renyi differential privacy. In Computer Security Foundations
Symposium (CSF), 2017 IEEE 30th. IEEE, 263‚Äì275.
[42] Ilya Mironov, Kunal Talwar, and Li Zhang. 2019. R‚Äôenyi Differential Privacy of
the Sampled Gaussian Mechanism. arXiv preprint arXiv:1908.10530 (2019).
[43] Nicolas Papernot, Mart√≠n Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal
Talwar. 2017. Semi-supervised knowledge transfer for deep learning from private
training data. In International Conference on Learning Representations.
[44] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Tal-
war, and Ulfar Erlingsson. 2018. Scalable Private Learning with PATE. In Interna-
tional Conference on Learning Representations.
[45] Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X. Yu, Sashank J. Reddi,
and Sanjiv Kumar. 2019. AdaCliP: Adaptive Clipping for Private SGD. CoRR
abs/1908.07643 (2019). arXiv:1908.07643 http://arxiv.org/abs/1908.07643
[46] Wahbeh Qardaji, Weining Yang, and Ninghui Li. 2014. Priview: practical differ-
entially private release of marginal contingency tables. In Proceedings of the 2014
ACM SIGMOD international conference on Management of data. ACM, 1435‚Äì1446.
[47] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representa-
tion learning with deep convolutional generative adversarial networks. arXiv
preprint arXiv:1511.06434 (2015).
[48] Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
Vladimir Braverman, Joseph Gonzalez, and Raman Arora. 2020. Fetchsgd:
Communication-efficient federated learning with sketching. In International
Conference on Machine Learning. PMLR, 8253‚Äì8265.
[49] Christopher De Sa, Ce Zhang, Kunle Olukotun, and Christopher R√©. 2015. Taming
the Wild: A Unified Analysis of HOG WILD! -Style Algorithms. In Proceedings of
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2159the 28th International Conference on Neural Information Processing Systems.
[50] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
Improved techniques for training GANs. In Proceedings
and Xi Chen. 2016.
of the 30th International Conference on Neural Information Processing Systems.
2234‚Äì2242.
[51] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership Inference
Attacks Against Machine Learning Models. In 2017 IEEE Symposium on Security
and Privacy (SP). 3‚Äì18.
[52] Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, and Ji Liu. 2018. Communica-
tion Compression for Decentralized Training. In Advances in Neural Information
Processing Systems 31, S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-
Bianchi, and R Garnett (Eds.). Curran Associates, Inc., 7652‚Äì7662.
[53] Om Thakkar, Galen Andrew, and H. Brendan McMahan. 2019. Differen-
tially Private Learning with Adaptive Clipping. CoRR abs/1905.03871 (2019).
arXiv:1905.03871 http://arxiv.org/abs/1905.03871
[54] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. 2019. Dp-cgan:
Differentially private synthetic data and label generation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops.
[55] Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. 2020. Practical Low-
Rank Communication Compression in Decentralized Deep Learning. Adv. Neural
Inf. Process. Syst. 33 (2020).
[56] L Wang, R Jia, and D Song. 2020. D2P-Fed: Differentially private federated
learning with efficient communication. arxiv. org/pdf/2006.13039 (2020).
[57] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. 2018. Gradient Sparsifica-
tion for Communication-Efficient Distributed Optimization. In Proceedings of the
32nd International Conference on Neural Information Processing Systems.
[58] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[59] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. 2018. Differen-
tially Private Generative Adversarial Network. arXiv preprint arXiv:1802.06739
(2018).
[60] Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Ju Xiao, and Bo Li.
2020. Reinforcement-learning based portfolio management with augmented asset
movement prediction states. In Proceedings of the AAAI Conference on Artificial
Intelligence.
[61] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE
31st Computer Security Foundations Symposium (CSF). IEEE, 268‚Äì282.
[62] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. 2019. PATE-GAN:
Generating Synthetic Data with Differential Privacy Guarantees. In International
Conference on Learning Representations.
[63] Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, and Xi-