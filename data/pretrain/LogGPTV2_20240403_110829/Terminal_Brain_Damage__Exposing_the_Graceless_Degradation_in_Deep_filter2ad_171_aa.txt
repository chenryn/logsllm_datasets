title:Terminal Brain Damage: Exposing the Graceless Degradation in Deep
Neural Networks Under Hardware Fault Attacks
author:Sanghyun Hong and
Pietro Frigo and
Yigitcan Kaya and
Cristiano Giuffrida and
Tudor Dumitras
Terminal Brain Damage: 
Exposing the Graceless Degradation in Deep 
Neural Networks Under Hardware Fault Attacks
Sanghyun Hong, University of Maryland College Park; Pietro Frigo, Vrije Universiteit 
Amsterdam; Yiğitcan Kaya, University of Maryland College Park; Cristiano Giuffrida, 
Vrije Universiteit Amsterdam; Tudor Dumitraș, University of Maryland College Park
https://www.usenix.org/conference/usenixsecurity19/presentation/hong
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.Terminal Brain Damage: Exposing the Graceless Degradation
in Deep Neural Networks Under Hardware Fault Attacks
Sanghyun Hong, Pietro Frigo†, Yi˘gitcan Kaya, Cristiano Giuffrida†, Tudor Dumitras,
University of Maryland, College Park
†Vrije Universiteit Amsterdam
Abstract
Deep neural networks (DNNs) have been shown to tolerate
“brain damage”: cumulative changes to the network’s parame-
ters (e.g., pruning, numerical perturbations) typically result
in a graceful degradation of classiﬁcation accuracy. However,
the limits of this natural resilience are not well understood
in the presence of small adversarial changes to the DNN pa-
rameters’ underlying memory representation, such as bit-ﬂips
that may be induced by hardware fault attacks. We study the
effects of bitwise corruptions on 19 DNN models—six archi-
tectures on three image classiﬁcation tasks—and we show that
most models have at least one parameter that, after a speciﬁc
bit-ﬂip in their bitwise representation, causes an accuracy loss
of over 90%. For large models, we employ simple heuristics
to identify the parameters likely to be vulnerable and estimate
that 40–50% of the parameters in a model might lead to an
accuracy drop greater than 10% when individually subjected
to such single-bit perturbations. To demonstrate how an adver-
sary could take advantage of this vulnerability, we study the
impact of an exemplary hardware fault attack, Rowhammer,
on DNNs. Speciﬁcally, we show that a Rowhammer-enabled
attacker co-located in the same physical machine can inﬂict
signiﬁcant accuracy drops (up to 99%) even with single bit-
ﬂip corruptions and no knowledge of the model. Our results
expose the limits of DNNs’ resilience against parameter per-
turbations induced by real-world fault attacks. We conclude
by discussing possible mitigations and future research direc-
tions towards fault attack-resilient DNNs.
1 Introduction
Deep neural networks (DNNs) are known to be resilient to
“brain damage” [32]: typically, cumulative changes to the net-
work’s parameters result in a graceful degradation of classiﬁ-
cation accuracy. This property has been harnessed in a broad
range of techniques, such as network pruning [35], which
signiﬁcantly reduces the number of parameters in the network
and leads to improved inference times. Besides structural re-
silience, DNN models can tolerate slight noise in their parame-
ters with minimal accuracy degradation [2]. Researchers have
proposed utilizing this property in defensive techniques, such
as adding Gaussian noise to model parameters to strengthen
DNN models against adversarial examples [69]. As a result,
this natural resilience is believed to make it difﬁcult for attack-
ers to signiﬁcantly degrade the overall accuracy by corrupting
network parameters.
Recent work has explored the impact of hardware faults on
DNN models [34,42,45]. Such faults can corrupt the memory
storing the victim model’s parameters, stress-testing DNNs’
resilience to bitwise corruptions. For example, Qin et al. [42],
conﬁrming speculation from previous studies [34,35], showed
that a DNN model for CIFAR10 image classiﬁcation does
not lose more than 5% accuracy when as many as 2,600 pa-
rameters out of 2.5 million are corrupted by random errors.
However, this analysis is limited to a speciﬁc scenario and
only considers accidental errors rather than attacker-induced
corruptions by means of fault attacks. The widespread us-
age of DNNs in many mission-critical systems, such as self-
driving cars or aviation [12, 51], requires a comprehensive
understanding of the security implications of such adversarial
bitwise errors.
In this paper, we explore the security properties of DNNs
under bitwise errors that can be induced by practical hardware
fault attacks. Speciﬁcally, we ask the question: How vulnera-
ble are DNNs to the atomic corruption that a hardware fault
attacker can induce? This paper focuses on single bit-ﬂip
attacks that are realistic as they well-approximate the con-
strained memory corruption primitive of practical hardware
fault attacks such as Rowhammer [48]. To answer this ques-
tion, we conduct a comprehensive study that characterizes the
DNN model’s responses to single-bit corruptions in each of
its parameters.
First, we implement a systematic vulnerability analysis
framework that ﬂips each bit in a given model’s parameters
and measures the misclassiﬁcation rates on a validation set.
Using our framework, we analyze 19 DNN models composed
of six different architectures and their variants on three pop-
USENIX Association
28th USENIX Security Symposium    497
ular image classiﬁcation tasks: MNIST, CIFAR10, and Im-
ageNet. Our experiments show that, on average, ∼50% of
model parameters are vulnerable to single bit-ﬂip corruptions,
causing relative accuracy drops above 10%, and that all 19
DNN models include parameters that can cause an accuracy
drop of over 90%1. The results expose the limits of the DNN’s
resilience to numerical changes, as adversarial bitwise errors
can lead to a graceless degradation of classiﬁcation accuracy.
Our framework also allows us to characterize the vulner-
ability by examining the impact of various factors: the bit
position, bit-ﬂip direction, parameter sign, layer width, acti-
vation function, normalization, and model architecture. Our
key ﬁndings include: 1) the vulnerability is caused by drastic
spikes in a parameter value; 2) the spikes in positive param-
eters are more threatening, however, an activation function
that allows negative outputs renders the negative parameters
vulnerable as well; 3) the number of vulnerable parameters in-
creases proportionally as the DNN’s layers get wider; 4) two
common training techniques, e.g., dropout [52] and batch
normalization [24], are ineffective in preventing the massive
spikes bit-ﬂips cause; and 5) the ratio of vulnerable param-
eters is almost constant across different architectures (e.g.,
AlexNet, VGG16, and so on). Further, building on these ﬁnd-
ings, we propose heuristics for speeding up the analysis of
vulnerable parameters in large models.
Second, to understand the practical impact of this vulner-
ability, we use Rowhammer [26] as an exemplary hardware
fault attack. While a variety of hardware fault attacks are doc-
umented in literature [11, 26, 38, 57], Rowhammer is particu-
larly amenable to practical, real-world exploitation. Rowham-
mer takes advantage of a widespread vulnerability in modern
DRAM modules and provides an attacker with the ability to
trigger controlled memory corruptions directly from unpriv-
ileged software execution. As a result, even a constrained
Rowhammer-enabled attacker, who only needs to perform a
speciﬁc memory access pattern, can mount practical attacks in
a variety of real-world environments, including cloud [44,67],
browsers [9, 15, 19, 48], mobile [15, 62], and servers [36, 60].
We analyze the feasibility of Rowhammer attacks on DNNs
by simulating a Machine-Learning-as-a-Service (MLaaS) sce-
nario, where the victim and attacker VMs are co-located on
the same host machine in the cloud. The co-location leads the
victim and the attacker to share the same physical memory,
enabling the attacker to trigger Rowhammer bit-ﬂips in the
victim’s data [44, 67]. We focus our analysis to models with
an applicable memory footprint, which can realistically be
targeted by hardware fault attacks such as Rowhammer.
Our Rowhammer results show that in a surgical attack sce-
nario, with the capability of ﬂipping speciﬁc bits, the attacker
can reliably cause severe accuracy drops in practical settings.
Further, even in a blind attack scenario, the attacker can still
1The vulnerability of a parameter requires a speciﬁc bit in its bitwise rep-
resentation to be ﬂipped. There also might be multiple such bits in the
representation that, when ﬂipped separately, trigger the vulnerability.
mount successful attacks without any control over the loca-
tions of bit-ﬂips landed in memory. Moreover, we also reveal
a potential vulnerability in the transfer learning scenario; in
which a surgical attack targets the parameters in the layers
victim model contains in common with a public one.
Lastly, we discuss directions for viable protection mech-
anisms, such as reducing the number of vulnerable parame-
ters by preventing signiﬁcant changes in a parameter value.
In particular, this can be done by 1) restricting activation
magnitudes and 2) using low-precision numbers for model
parameters via quantization or binarization. We show that,
when we restrict the activations using the ReLU-6 activa-
tion function, the ratio of vulnerable parameters decreases
from 47% to 3% in AlexNet, and also, the accuracy drops are
largely contained within 10%. Moreover, quantization and
binarization reduce the vulnerable parameter ratio from 50%
to 1-2% in MNIST. While promising, such solutions cannot
deter practical hardware fault attacks in the general case, and
often require training the victim model from scratch; hinting
that more research is required towards fault attack-resilient
DNNs.
Contributions. We make three contributions:
• We show DNN models are more vulnerable to bit-ﬂip
corruptions than previously assumed. In particular, we
show adversarial bitwise corruptions induced by hard-
ware fault attacks can easily inﬂict severe indiscrimi-
nate damages by drastically increasing or decreasing the
value of a model parameter.
• We conduct the ﬁrst comprehensive analysis of DNN
models’ behavior against single bit-ﬂips and characterize
the vulnerability that a hardware fault attack can trigger.
• Based on our analysis, we study the impact of practical
hardware fault attacks in a representative DL scenario.
Our analysis shows that a Rowhammer-enabled attacker
can inﬂict signiﬁcant accuracy drops (up to 99%) on a
victim model even with constrained bit-ﬂip corruptions
and no knowledge of the model.
2 Preliminaries
Here, we provide an overview of the required background
knowledge.
Deep neural networks. A DNN can be conceptualized as
a function that takes an input and returns a prediction, i.e.,
the inferred label of the input instance. The network is com-
posed of a sequence of layers that is individually parame-
terized by a set of matrices, or weights. Our work focuses
on feed-forward DNNs—speciﬁcally on convolutional neural
networks (CNNs)—in the supervised learning setting, i.e., the
weights that minimize the inference error are learned from
498    28th USENIX Security Symposium
USENIX Association
a labeled training set. In a feed-forward network, each layer
applies a linear transformation, deﬁned by its weight matrix,
to its input—the output of the previous layer—and a bias
parameter is added optionally. After the linear transformation,
a non-linear activation function is applied; as well as other
optional layer structures, such as dropout, pooling or batch
normalization. During training, the DNN’s parameters, i.e.,
the weights in each layer and in other optional structures,
are updated iteratively by backpropagating the error on the
training data. Once the network converges to an acceptable
error rate or when it goes through sufﬁcient iterations, training
stops and the network, along with all its parameters, is stored
as a trained network. During testing (or inference), we load
the full model into the memory and produce the prediction
for a given input instance, usually not in the training data.
Single precision ﬂoating point numbers. The parameters
of a DNN model are usually represented as IEEE754 32-bit
single-precision ﬂoating-point numbers. This format lever-
ages the exponential notation and trades off the large range
of possible values for reduced precision. For instance, the
number 0.15625 in exponential notation is represented as
1.25× 2−3. Here, 1.25 expresses the mantissa; whereas −3
is the exponent. The IEEE754 single-precision ﬂoating-point
format deﬁnes 23 bits to store the mantissa, 8 bits for the
exponent, and one bit for the sign of the value. The fact that
different bits have different inﬂuence on the represented value
makes this format interesting from an adversarial perspective.
For instance, continuing or example, ﬂipping the 16th bit in
the mantissa increases the value from 0.15625 to 0.15625828;
hence, a usually negligible perturbation. On the other hand,
a ﬂipping the highest exponent bit would turn the value into
1.25× 2125. Although both of these rely on the same bit cor-
ruption primitive, they yield vastly different results. In Sec 4,
we analyze how this might lead to a vulnerability when a
DNN’s parameters are corrupted via single bit-ﬂips.
Rowhammer attacks. Rowhammer is the most common
instance of software-induced fault attacks [9, 15, 19, 44, 48,
60, 62, 67]. This vulnerability provides an aggressor with a
single-bit corruption primitive at DRAM level; thus, it is an
ideal attack for the purpose of our analysis. Rowhammer is
a remarkably versatile fault attack since it only requires an
attacker to be able to access content in DRAM; an ubiqui-
tous feature of every modern system. By simply carrying
out speciﬁc memory access patterns—which we explain in
Sec 5—the attacker is able to cause extreme stress on other
memory locations triggering faults on other stored data.
3 Threat Model
Prior research has extensively validated a DNN’s resilience
to parameter changes [2, 32, 34, 35, 42, 69], by considering
random or deliberate perturbations. However, from a security
perspective, these results provide only limited insights as they
study a network’s expected performance under cumulative
changes. In contrast, towards a successful and feasible attack,
an adversary is usually interested in inﬂicting the worst-case
damage under minimal changes.
We consider a class of modiﬁcations that an adversary, us-
ing hardware fault attacks, can induce in practice. We assume
a cloud environment where the victim’s deep learning sys-
tem is deployed inside a VM—or a container—to serve the
requests of external users. For making test-time inferences,
the trained DNN model and its parameters are loaded into
the system’s (shared) memory and remain constant in normal
operation. Recent studies describe this as a typical scenario
in MLaaS [61].
To understand the DNNs’ vulnerability in this setting, we
consider the atomic change that an adversary may induce—
the single bit-ﬂip—and we, in Sec 4, systematically charac-
terize the damage such change may cause. We then, in Sec 5,
investigate the feasibility of inducing this damage in prac-
tice, by considering adversaries with different capabilities and
levels of knowledge.
Capabilities. We consider an attacker co-located in the
same physical host machine as the victim’s deep learning
system. The attacker, due to co-location, can take advan-
tage of a well-known software-induced fault attack, Rowham-
mer [44,67], for corrupting the victim model stored in DRAM.
We take into account two possible scenarios: 1) a surgical
attack scenario where the attacker can cause a bit-ﬂip at an
intended location in the victim’s process memory by lever-
aging advanced memory massaging primitives [44, 62] to
obtain more precise results; and 2) a blind attack where the
attacker lacks ﬁne-grained control over the bit-ﬂips; thus, is
completely unaware of where a bit-ﬂip lands in the layout of
the model.
Knowledge. Using the existing terminology, we consider
two levels for the attacker’s knowledge of the victim model,
e.g., the model’s architecture and its parameters as well as
their placement in memory: 1) a black-box setting where the
attacker has no knowledge of the victim model. Here, both
the surgical and blind attackers only hope to trigger an accu-
racy drop as they cannot anticipate what the impact of their
bit-ﬂips would be; and 2) a white-box setting where the at-
tacker knows the victim model, at least partially. Here, the
surgical attacker can deliberately tune the attack’s inﬂicted ac-
curacy drop—from minor to catastrophic damage. Optionally,