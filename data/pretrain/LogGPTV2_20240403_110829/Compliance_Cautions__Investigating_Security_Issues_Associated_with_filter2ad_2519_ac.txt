As such, there should be little to no need for user-installed
software, especially given that users are one of the primary
vectors for introducing malware into environments [2]. Section
9.3 should instead mandate application whitelisting for instal-
lation attempts, limiting the subset of authorized applications
that anyone can install on the system. A more stringent recom-
mendation would include revoking user-installation privileges
altogether, forcing trusted system administrators to establish a
safe baseline of applications allowed to interact with FTI. We
assessed this high-risk issue to have a likely probability and
critical severity that can place FTI at risk.
We identiﬁed an extremely-high-risk issue within Section
1.4.4 “Information Received From Taxpayers or Third Parties,”
which limits the responsibility for securing FTI. According
to this section, the IRS is only responsible for securing data
originating from the IRS as FTI, excluding data received from
customers like federal tax returns. Additionally, this section
includes provisions for removing FTI protections on data
if an entity replaces IRS-sourced FTI with the exact same
information sourced from another party. This is analogous to
eliminating protections for top-secret government data simply
because the same information can be bought on the black
market. This mandated behavior allows organizations to bypass
security measures and remove protections on the data P1075
is meant to safeguard. We recommend that P1075 enforce
protection for all FTI, regardless of source.
Section 1.4.3 deﬁnes certain data as personally identiﬁable
information (PII) but does not protect the names of individuals
associated with the person ﬁling the return – such as a spouse
or dependent. This high-risk issue may allow an attacker, for
example, to develop a targeted spearphishing campaign against
an individual. We recommend expanding the deﬁnition of PII
to include sensitive information about all persons listed.
Unenforceable controls. We identiﬁed three controls that are
unenforceable. For example, Section 4.7 provides several mea-
sures for secure telework access to FTI. P1075 provides many
requirements for physical data protections, such as badge-
based control and on-premises guards; these are infeasible in
the case of telework, as most personnel with FTI access at
their private homes cannot abide by these types of controls.
Additionally, IRS inspections of private residences for physical
security compliance seems fraught with complications. We
recommend that either the IRS ban residential-based telework
programs until it can verify that all locations with FTI access
are compliant with physical security requirements, or that
the standard acknowledge that these physical controls are not
actually required. We assessed this high-risk issue to have a
frequent probability and moderate severity.
Under-deﬁned process. We identiﬁed 27 issues that reﬂect
processes that are not sufﬁciently detailed for a secure imple-
mentation. One such issue within Section 8.3 states that “every
third piece of physical electronic media must be checked to
ensure appropriate destruction of FTI.” Given the disparate
possible sizes of electronic media,
this particular section
should recommend accounting for logical storage size of the
media instead of its physical instantiation. This would ensure
that media with larger storage volumes are highly prioritized
for destruction validation. We assessed this as a moderate-
severity, moderate-risk issue with a likely probability.
One low-risk issue occurs in Section 1.4.7, which limits
human access to FTI based on “need to know” but does not
consider machines or systems with a “need to access” data.
Administrators must limit system access to FTI to prevent
unauthorized access or manipulation of data, especially for
systems performing aggregate analysis that may inadvertently
disclose sensitive information.
Section 9.3.13.3 covers background checks for personnel
with access to FTI. Our researchers assessed that this section
could create information gaps at the federal, state, and local
levels. For example, information about an individual who mis-
handled sensitive data at a previous job may never have entered
federal databases. These extremely-high-risk information gaps
increase likelihood for insider threats and risks to data, and
highlight the need for aggregating multiple sources of data for
thorough background checks.
We identiﬁed another issue in Section 9.3.5.8, which
outlines a procedure for establishing an Information System
component inventory (i.e., a listing of authorized devices that
operate within an organization). As written, this procedure
does not require the inventory process to be tied to a “ground
truth,” meaning there is no comparison of which devices
should be operating within an organization with which devices
actually are. This is dangerous, as it could permit a rogue
system to persist on a network or even be inventoried as a
legitimate system. Providing a rogue system with legitimate
access within a sensitive environment obviates the need for
an attacker to deploy malware within the environment and
reduces the likelihood that any defensive sensors would ever
detect anomalous activity from the attacker. We assessed this
moderate-risk issue to have an occasional probability and
moderate severity. Industry recommendations integrate asset
inventory with supply acquisition, ensuring that only company-
purchased, legitimate systems are on the network [9].
Ambiguous speciﬁcation. We found 14 issues involving insuf-
ﬁcient details that create ambiguity or uncertainty throughout
P1075. P1075 uses vague terms such as “signiﬁcant change”
throughout, without ever deﬁning thresholds that auditors deem
to be signiﬁcant. As an example, Section 9.3 outlines “Access
Control Policy and Procedures” that must be reviewed (by
whom?) every three years or whenever a signiﬁcant change
occurs. This subjectivity allows reviewers to deem any or
all changes insigniﬁcant to circumvent a change review. Ad-
ditionally, the document’s use of passive voice clouds the
responsibility for conducting the review — ambiguous controls
can create security gaps through inaction. We believe each
mandate should use active voice and assign a responsible
individual (e.g., an ofﬁce manager or system administrator)
for each requirement. As presently written, an individual who
works in an organization’s talent recruiting department with
no security training would be a sufﬁcient reviewer for access-
control policy. We assessed these moderate-risk issues with a
likely probability and moderate severity.
6
C. Expert validation
For assessing the validity of our IRS P1075 audit, we
partnered with New York City Cyber Command (NYC3).
NYC3 is a city government organization that oversees the
cybersecurity of 143 separate departments and agencies as well
as more than 300,000 people. In addition to defending NYC
against cybersecurity threats, NYC3 is responsible for ensuring
compliance with government-mandated policies. In particular,
the NYC3 team includes ﬁve full-time employees and three
consultants who focus solely on security compliance. Each of
the 143 departments within the city government also has an
internal, full-time compliance teams.
IRS P1075 applies to the vast majority of these 143 entities.
NYC3 advises other NYC entities on P1075 compliance and
is also subjected to IRS audits. We coordinated directly with
two NYC3 governance and compliance ofﬁcials to assess the
validity of our ﬁndings with respect to a particular subdo-
main under NYC3’s purview that must comply with P1075
standards. This subdomain consists of a controlled internal
network that contains FTI and supports approximately 150
users. NYC3’s last formal P1075 audit was in January and
February 2018, where three on-site auditors used the standard
as a line-by-line checklist to assess NYC3’s compliance. Of
note, preparation for this inspection consumed the compliance
team as well as several technicians for approximately four
months prior to their inspection date.
Because of their limited time availability, we asked our two
NYC3 compliance ofﬁcials (hereafter referenced as Experts E1
and E2) to assess 20 issues (25% of our total 81 issues). In
order to cover issues at all risk levels but prioritize signiﬁcant
concerns, we included both extremely-high-risk issues, and
then randomly sampled 10 of the 13 high-risk issues, four
of the 32 moderate issues, and four of the 34 low-risk issues.
When validating P1075 issues, E1 and E2 were able to
directly examine their network for the presence of security
concerns caused by issues identiﬁed by the researchers, in
a kind of white-box penetration test [16]. This was possible
because, unlike E3 and E4, E1 and E2 are ofﬁcials with
administrator privileges within the audited subdomain. The
two experts analyzed our ﬁndings independently and did not
discuss their ﬁndings with one another during the study.
Overall, these experts conﬁrmed 17 of our ﬁndings, rejected
two issues, and indicated one issue could be plausible within
their own or another environment.
When comparing our risk estimates to those of E1 and E2,
we found no statistical difference between severity estimates
(omnibus p = 0.54), but our researchers assessed issues to
be statistically more likely with medium effect (p = 0.0001,
0.034, < 0.0001; r = 0.485, 0.336, 0.533 for omnibus and
then pairwise researchers vs. E1, E2 respectively). E1 indicated
that his knowledge of current and on-going initiatives most
likely biased his responses, making it hard for him to follow
instructions to consider each issue only “if standard is followed
as written and nothing else” (as written in Appendix B). This
response supports our notion in Section III-B that participant-
validated responses represent a lower-bound for this study.
The issue that E1 and E2 classiﬁed as plausible rather
than conﬁrmed comes from Section 1.4.7 “Need to Know.” E2
indicated that NYC3 data scientists incorporate the principle of
least privilege for systems, service accounts, and user accounts,
which would prevent unauthorized access and manipulation of
FTI. E1 added that NYC3’s PKI infrastructure assists with
controlling access to “need to access” data. Both participants
indicated they were unsure if this security concern was ever
present within NYC3, but that it could be present within other
organizations.
E1 and E2 rejected our ﬁnding for Section 1.4.3 PII,
indicating NYC3 always encrypts entire tax records while
in transit and rest, and that
this is standard practice for
organizations with access to FTI. Thus, associated individuals’
PII are always protected, invalidating our ﬁnding. However,
because this is not codiﬁed within P1075, there is no certainty
that other organizations adhere to this “standard practice.”
NYC3 also rejected our ﬁnding associated with Section 9.3
background checks. E1 indicated that it is standard practice to
aggregate personnel records from the locations an individual
has lived or worked to determine if the individual should
have access to sensitive information, thus rejecting our ﬁnding.
Because P1075 does not mandate data sources or how far
back in history to consider, there is no certainty that other
organizations conduct this practice.
Additional defenses. E1 and E2 identiﬁed several controls
pervasive throughout NYC3 that help reduce or eliminate the
impact of many of our researcher ﬁndings. Of note, NYC3
requires a Change Control Board (CCB) that E2 believes
“is an essential risk-mitigating factor” for addressing many
of the conﬁrmed P1075 security concerns, such as Section
9.3.5.11 “User-Installed Software.” The CCB evaluates all user
requests for system modiﬁcations and holistically considers
the change’s impact
to security. If the CCB approves the
change, it authorizes a trusted administrator to conduct the
software installation and adds the change to the system’s
secure baseline. Additionally, NYC3 incorporates a real-time,
automatic asset manager which alerts their Security Operations
Center any time a new device is added to their networks. This
defensive strategy eliminates the security concern we identiﬁed
in Section 9.3.5.8 “Information System Component Inventory.”
It is important to note that these defenses employed at
NYC3 exceed the baseline security standards required by
P1075 and mitigate issues that P1075 either fails to account
for even causes. We cannot assume that all organizations will
recognize the need for these additional mitigations and be
willing to invest in them.
V. RESULTS: PCI DSS
A. Overview
The Payment Card Industry Data Security Standard (PCI
DSS) applies to all entities that store, process, and transmit
credit-card-holder data for major branded credit cards [45].
Guidance in this standard includes building and maintaining
a secure network and systems, protecting cardholder data,
and monitoring/testing networks. PCI DSS v1.0 dates back
to 2004 as a program led by Visa; the PCI Security Standards
Council (SSC) was formed in 2006 by American Express, Dis-
cover, JCB International, MasterCard and Visa to enhance PCI
DSS [45]. We audited the 2016 v3.2; v4.0 was in development
during this study.
7
could lead to PAN access, such as passwords or password-
recovery information. Additionally, we identiﬁed 10 issues
involving technical controls that lack timelines for required
action. For each required action, the standard should specify
either a ﬁxed interval for repetition or for a triggering event
with an ensuing deadline. Below we present discovered PCI
DSS issues, organized according to perceived root cause.
Data vulnerability. We identiﬁed seven security concerns that
could establish conditions for a data breach. One example
of a high-risk vulnerability stems from Section 1.4, which
includes mandates for securing personal computing systems
within the cardholder data environment (CDE). We recommend
disallowing any personal electronics within the CDE network
segment; more broadly, all services and systems should be
limited by “need to access” cardholder data. Personal devices
and activities increase the likelihood of malware or other
unauthorized access and generally are not necessary within
CDE network segments [2]. We assess this security concern to
have a likely probability and critical severity.
A tangentially-related moderate-risk security concern stems
from the “Network Segmentation” section of PCI DSS, which
scopes the standard’s safeguards to only the network segment
that contains cardholder data. Effectively, this provision would
allow an organization with no security controls outside of the
CDE to pass an audit as long as the CDE itself is protected in
accordance with PCI DSS speciﬁcations. Allowing vulnerable
servers and systems within the same network as the CDE could
provide attackers with a landing point into internal portions
of the network and establish conditions for lateral movement
into the CDE from adjacent network segments (through well-
known attacks such as VLAN hopping). Due to the series of
security holes that must be present for such an attack to occur,
we assessed that exploitation of this vulnerability would be
seldom but critically severe for affected systems.
Another data vulnerability is present within the “PCI DSS
Applicability Information” section, where PCI DSS deﬁnes
sensitive authentication data. PCI DSS does not consider pass-
words to be sensitive authentication data and does not protect
information an attacker could use to reset service passwords
(e.g., email addresses, Social Security Numbers, and dates of
birth). The social engineering attack against Naoki Hiroshima’s
@N Twitter account leveraged similar pieces of information to
access protected accounts [23]. Given that publicly-available
articles detail how unprotected information can lead to unau-
thorized access, we assess this security concern to have a
moderate severity and likely probability.
Under-deﬁned process. We identiﬁed 29 issues with process
speciﬁcations that are insufﬁcient for a secure implementation.
Section 3.2.1 calls for assessors to select a sample of system
components and examine data sources to detect cardholder data
that is improperly stored. Sampling is an insufﬁcient process,
considering the simplicity of searching for cardholder data that
adheres to a well-known format. We recommend improving
this section to mandate assessors use automated tools on all
CDE systems to detect
improperly stored cardholder data.
Based on the moderate severity of exposed cardholder data
and the frequent likelihood insufﬁcient checks occurring, we
assess this issue to be a high-level risk.
PCI DSS features two high-risk under-deﬁned processes
Fig. 3: Distribution of security concerns identiﬁed for PCI
DSS. Color indicates the type of security concern; each dot
indicates by size how many security concerns were identiﬁed
with a given type, severity, and probability. Under-deﬁned
processes were most common (n=29).
PCI DSS affects every person within the United States
who makes credit card purchases and every organization that
accepts credit card payments. A U.S. Federal Reserve study
showed that consumers spent $5.98 trillion with credit cards
in 2016, highlighting the importance of securing the systems
that support those ﬁnancial transactions [59]. PCI DSS authors
designed the document to be accessible to assessors and the
technicians charged with implementing the technical controls.
Qualiﬁed Security Assessors perform PCI DSS audits after
attaining the appropriate inspection certiﬁcations. According
to one such assessor (not an author or an expert validator),
audit frequency varies for merchants and service providers de-
pending on their number of supported annual transactions [37].
On-site audit teams vary from one to three personnel per
inspection; these personnel focus full-time on auditing the PCI
DSS compliance of other organizations. The assessor indicates