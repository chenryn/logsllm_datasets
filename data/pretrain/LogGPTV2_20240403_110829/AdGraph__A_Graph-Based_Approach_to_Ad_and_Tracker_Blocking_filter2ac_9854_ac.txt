ronment to bind the graph representation of the page to each
page’s document object. This choice allows us to easily dis-
tinguish scripts executing in different frames/sub-documents,
a problem that has frustrated prior work (see discussion of
JSGraph in Section II-C). Finally, we add instrumentation to
allow us to map between V8’s identifers for script units, and
the sources of script in the executing site (e.g. script tags,
eval’ed scripts, script executed by extensions).4
V8 Instrumentation. We also modify V8 to add instru-
mentation points to allow us to track anytime a script
is
compiled, and anytime control changes between script units.
We accomplish this by associating every function and global
scope to the script they are compiled from. We then can note
every time a new scope is entered, and attribute any document
modiﬁcations or network requests to that script, until the scope
is exited.
V8 contains several optimizations that make this general
approach insufﬁcient. First, V8 sometimes defers parsing of
subsections of JavaScript code. A partial list of such cases
includes eval’ed code, code compiled with the Function
constructor, and anonymous functions provided as callbacks
for some built in functions (e.g. setTimeout). To handle
these cases, ADGRAPH not only maps functions to script units
but also sub-scripts to scripts.
Second, V8 implements microtasks that make attribution
difﬁcult. Microtasks allow for some memory savings (much of
the type information and vtable look-up overhead is skipped)
and reduce some book-keeping overhead. Tracking attribution
of DOM changes in microtasks is difﬁcult because, at this
level, V8 no longer tracks functions as C++ objects, but as
3The source code of our Chromium implementation is available at:
https://uiowa-irl.github.io/AdGraph/.
4The architectural independence between the V8 and Blink projects made
this an unexpectedly difﬁcult problem to solve, with many unanticipated
corner cases that were not discovered until we subjected ADGRAPH to
extensive automatic and manual testing.
compiled bytecode, requiring a different approach to determin-
ing which script unit “owned” any given execution. ADGRAPH
solves this problem through additional instrumentation, and
some runtime stack scanning, yielding completeness at the cost
of a minor performance overhead.
JavaScript Attribution Example. ADGRAPH is able to
attribute DOM modiﬁcations and network events to script
units in cases where existing techniques fail. We give a
representative example in code snippet 1.
This code uses eval to parse and execute a string as
JavaScript code. The resulting code uses a Promise in a
setTimeout callback. This Promise callback is optimized
in V8 as a microtask, which evades the attribution techniques
used in current work (e.g. PrivacyBadger / stack walking,
AdTracker, JSGraph, discussed in Section II-C). Existing tools
would not be able to recognize that
this code unit was
responsible for the image fetched in the Promise callback.
ADGRAPH, though, is able to correctly attribute the image
request to this code unit. Figure 2 shows how this execution
pattern would be stored in ADGRAPH. Speciﬁcally, the edge
between nodes 2 and 4 records the attribution of the eval call
to the responsible JavaScript code unit, and the edge between
nodes 7 and 9 in record that the image request is a result of
code executed in the microtask. Existing approaches would
either miss the edge between 2 and 4, or 7 or 9.
(cid:43)(cid:55)(cid:48)(cid:47)(cid:3)(cid:81)(cid:82)(cid:71)(cid:72)(cid:86)
(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)(cid:81)(cid:82)(cid:71)(cid:72)(cid:86)
(cid:40)(cid:71)(cid:74)(cid:72)(cid:86)(cid:3)(cid:70)(cid:85)(cid:72)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:43)(cid:55)(cid:48)(cid:47)(cid:3)(cid:83)(cid:68)(cid:85)(cid:86)(cid:72)(cid:85)
(cid:54)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)(cid:3)(cid:81)(cid:82)(cid:71)(cid:72)(cid:86)
(cid:40)(cid:71)(cid:74)(cid:72)(cid:86)(cid:3)(cid:70)(cid:85)(cid:72)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:86)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)(cid:86)
(cid:72)(cid:89)(cid:68)(cid:79)(cid:3)(cid:68)(cid:87)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:87)(cid:82)(cid:3)(cid:83)(cid:68)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:86)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)
(cid:1005)
(cid:83)(cid:68)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)
(cid:43)(cid:55)(cid:48)(cid:47)
(cid:1006)
(cid:43)(cid:55)(cid:48)(cid:47)(cid:3)
(cid:86)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)
(cid:1006)
(cid:86)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)
(cid:76)(cid:80)(cid:68)(cid:74)(cid:72)(cid:3)(cid:68)(cid:87)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)
(cid:80)(cid:76)(cid:70)(cid:85)(cid:82)(cid:87)(cid:68)(cid:86)(cid:78)(cid:3)(cid:72)(cid:91)(cid:72)(cid:70)(cid:88)(cid:87)(cid:72)(cid:71)(cid:3)(cid:86)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)
(cid:1011)
(cid:1008)
(cid:86)(cid:70)(cid:85)(cid:76)(cid:83)(cid:87)(cid:3)
(cid:11)(cid:72)(cid:89)(cid:68)(cid:79)(cid:12)
(cid:43)(cid:55)(cid:48)(cid:47)(cid:3)
(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)
(cid:1013)
(cid:81)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)
(cid:85)(cid:72)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)
Fig. 2: ADGRAPH’s representation of example code snippet 1. Node
numbers correspond to line numbers in code snippet 1. This exam-
ple highlights connections and attributions not possible in existing
techniques.
Code 1: A microtask in an eval created script loading an ad.
C. Feature Extraction
Next, we present the features that ADGRAPH extracts from
the graph to distinguish ads and trackers from functional
resources. These features are designed based on our domain
knowledge and expert
intuition. Speciﬁcally, we manually
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:00 UTC from IEEE Xplore.  Restrictions apply. 
768
analyze a large number of websites and try to design fea-
tures that would distinguish ad/tracking related resources from
functional (or benign) resources.
The extracted features broadly fall
into two categories:
“structural” (features that consider the relationship between
nodes and edges in the graph) and “content” (features that
depend on the values and attributes of nodes in isolation from
their connections). In total we extract 64 structural and content
features. Table II gives a summary and representative examples
of features from each category. Below we provide a high-level
description of structural and content features. More detailed
analysis of features and their robustness is presented in Section
IV-D.
Structural Features
Graph size (# of nodes, # of edges, and nodes/edge ratio)
Degree (in, out, in+out, and average degree connectivity)
Number of siblings (node and parents)
Modiﬁcations by scripts (node and parents)
Parent’s attributes
Parent degree (in, out, in+out, and average degree connectivity)
Sibling’s attributes
Ascendant’s attributes
Descendant of a script
Ascendant’s script properties
Parent is an eval script
Content Features
Request type (e.g. iframe, image)
Ad keywords in request (e.g. banner, sponsor)
Ad or screen dimensions in URL
Valid query string parameters
Length of URL
Domain party
Sub-domain check
Base domain in query string
Semi-colon in query string
TABLE II: Summarized feature set used by ADGRAPH.
Structural Features. Structural features target the relation-
ship between elements in a page (e.g. the relationship between
a network request and the responsible script unit, or a HTML
nodes’ parents, siblings and cousin HTML nodes). Examples
of structural features include whether a node’s parents have
ad-related values for the class attribute, the tag names of
the node’s siblings, or how deeply nested in the document’s
structure a given node is.
Structural features also consider the interaction between
JavaScript code, and the resource being requested. These
features rely on ADGRAPH’s instrumentation of Blink and
V8. Examples of JavaScript features include whether the node
initiating a network request was inserted by JavaScript code,
the number of scripts that have “touched” the node issuing the
request, and, in the case of requests that are not directly related
to HTML elements (e.g. AJAX), whether the JavaScript code
initiating the request was inlined in the document or fetched
from a third-party.
Content Features. Content features relate to values attached
to individual nodes in the graph (and not the connections
between nodes in the graph). The most signiﬁcant value
considered is the URL of the resource being requested. These
content features are similar to what most existing content
blocking tools use. ADGRAPH’s speciﬁc set of features though
is unique. Examples of ADGRAPH’s content features include
whether the origin of the resource being requested is ﬁrst-
or-third party,
the number of path segments in the URL
being requested, and whether the URL contains any ad-related
keywords.
D. Classiﬁcation
ADGRAPH uses random forest [38], a well-known ensemble
supervised ML classiﬁcation algorithm. Random forest com-
bines decisions from multiple decision trees, each constructed
using a different bootstrap sample of the data, by choosing
the mode of the predicted class distribution. Each node for
a decision tree is split using the best among the subset of
features selected at random. This feature selection mechanism
provides robustness against over-ﬁtting issues. We conﬁgure
random forest as an ensemble of 100 decision trees with each
decision tree trained using int(log M + 1) features, where M
is the total number of features.
ADGRAPH’s random forest model classiﬁes network re-
quests based on the provenance (creation and modiﬁcation
history) of a node and the context around it. These classi-
ﬁcation decisions are made before network request are sent,
so that ADGRAPH can prevent network communication with
ad and tracking related parties. A single node may initiate
many network requests (either due to it being a script node, or
being modiﬁed by script to reference multiple resources). As
a result, any node may be responsible for an arbitrary number
of network requests. ADGRAPH classiﬁes three categories of
network requests:
1) Requests initiated by the webpage’s HTML (e.g.
image referenced by an  tag’s src attribute).
the
2) Requests initiated by a node’s attribute change (e.g. a new
background image being downloaded due to a new CSS
style rule applying because of a mouse hover).
3) Requests initiated directly by JavaScript code (e.g. AJAX
requests, image objects not inserted into the DOM).
IV. ADGRAPH EVALUATION
In this section we evaluate the accuracy, usability, and
performance of ADGRAPH when applied to live, real-world,
popular websites.
A. Accuracy
We ﬁrst evaluate how accurately ADGRAPH is able to
distinguish advertising and tracking content from benign web
resources.
Ground Truth. To evaluate ADGRAPH’s accuracy, we ﬁrst
need to gather a ground truth to label a large number of
ad/tracking related network requests. We generate a trusted
set of ground truth labels by combining popular crowdsourced
ﬁlter lists that target advertising and/or tracking, and applying
them to popular websites. Table III lists the 8 popular ﬁlter lists
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:00 UTC from IEEE Xplore.  Restrictions apply. 
769
we combine to form our ground truth. These lists collectively
contain more than a hundred thousand crowdsourced rules for
determining whether a URL serves advertising and/or tracking
content.
List
EasyList
EasyPrivacy
Anti-Adblock Killer
Warning Removal List
Blockzilla
Fanboy Annoyances List
Peter Lowe’s List
Squid Blacklist
# Rules
72,660
15,507
1,964
378
1,155
38,675
2,962
4,485
Citation
[9]
[10]
[2]
[31]
[3]
[12]
[24]
[29]
TABLE III: Crowd sourced ﬁlter lists used as ground truth for
identifying ad and tracking resources. Rule counts are as of Nov.
12, 2018.
Advertising resources include audio-visual promotional con-
tent on a website. Tracking resources collect unique identiﬁers
(e.g., cookies) and sensitive information (e.g., browsing his-
tory) about users. In practice, there is no clear division between
ad and tracking resources. Many resources on the web not only
serve advertising images and videos but also track the users
who view it. It is also noteworthy that EasyList (to block ads)
and EasyPrivacy (to block trackers) have a signiﬁcant overlap.
Because of this overlap, we do not attempt to distinguish
between advertising and tracking resources.
Note that while these crowdsourced ﬁlter lists suffer from
well-known shortcomings [62], we treat them as “trusted”
for three reasons. First, they are reasonably accurate for top-
ranked websites even though they suffer on low-ranked web-
sites [42], [54]. Second, a more accurate alternative, building
a web-scale, manually generated, expert set of labels would
require labor and resources far beyond what is feasible for a
research project. Third, we use several ﬁlter lists together to
maximize their coverage and reduce false negatives.
We visit the homepages of the Alexa top-10K websites with
our instrumented Chromium browser. We expect that the top-
10K websites is a diverse and large enough set to contain
most common browsing behaviors. We limit our sample of
websites to the 10K most popular sites to avoid biasing
our sample; previous work has found that popular ﬁlter lists
work reasonably well for popular sites [42], [54]. Applying
crowdsourced ﬁlter lists to unpopular sites (sites that, almost
by deﬁnition, the curators of ﬁlter lists are less likely to visit)
risks skewing our data set to include a large number of false
negatives (i.e. advertising and tracking resources that ﬁlter list
authors have not encountered).
We apply ﬁlter lists to websites in the following manner. We
visit the homepage of each site with our instrumented version
of Chromium and wait for each page to ﬁnish loading (or
120 seconds, whichever occurs ﬁrst). Next we record every
URL of every resource fetched when loading and rendering
each page. We then label each fetched resource URL as AD
and NON-AD, based on the whether they are identiﬁed as ad
or tracking related by any of a set of ﬁlter lists. Our ﬁnal
labeled dataset consists of 540,341 URLs, fetched from 8,998
successfully crawled domains.5
Results. We use the random forest model to classify each
fetched URL. We then compare each predicted label with the
label derived from our ground truth data set, the set of ﬁlter
lists described above. We then evaluate how accurately our
model can reproduce the ﬁlter list labels through a stratiﬁed
10-fold cross validation, and report
the average accuracy.
ADGRAPH classiﬁes AD and NON-AD with a high degree of
accuracy, achieving 95.33% accuracy, with 89.1% precision,
and 86.6% recall.
As Table IV shows, ADGRAPH classiﬁes web resources
with a high degree of accuracy. We note that ADGRAPH is
more accurate in classifying visual resources such as im-
ages (98.95% accuracy) and CSS (96.32% accuracy) than