## Issue description
The gradient of `torch.clamp` when supplied with `inf` values is `nan`, even
when the `max` parameter is specified with a finite value. Normally one would
expect the gradient to be `0` for all values larger than `max`, including for
`inf`.
## Code example
I'm trying to implement the following piecewise function: `exp(x)` for `x >> x = torch.FloatTensor([-10, -5, 0, 5, 10, 50, 60, 70, 80, 90, 100])
    >>> x.requires_grad = True
    >>> y = torch.exp(x)
    >>> y
    tensor([4.5400e-05, 6.7379e-03, 1.0000e+00, 1.4841e+02, 2.2026e+04, 5.1847e+21,
            1.1420e+26, 2.5154e+30, 5.5406e+34,        inf,        inf],
           grad_fn=)
    >>> z = y.clamp(max=1)
    >>> z
    tensor([4.5400e-05, 6.7379e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,
            1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],
           grad_fn=)
    >>> z.sum().backward()
    >>> x.grad
    tensor([0.0000, 0.0067, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               nan,    nan])
## System Info
PyTorch version: 0.4.1  
Is debug build: No  
CUDA used to build PyTorch: 8.0.61
OS: Arch Linux  
GCC version: (GCC) 8.1.1 20180531  
CMake version: Could not collect
Python version: 3.6  
Is CUDA available: Yes  
CUDA runtime version: 9.2.148  
GPU models and configuration: GPU 0: GeForce GT 730  
Nvidia driver version: 396.24  
cuDNN version: Probably one of the following:  
/usr/local/R2017a/bin/glnxa64/libcudnn.so.5.1.5
Versions of relevant libraries:  
[pip] numpy (1.14.2)  
[pip] numpydoc (0.8.0)  
[pip] torch (0.4.1)  
[pip] torchvision (0.2.1)  
[conda] cuda80 1.0 0 soumith  
[conda] pytorch 0.4.1 py36_cuda8.0.61_cudnn7.1.2_1 [cuda80] pytorch  
[conda] torchvision 0.2.1 py36_1 pytorch
cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer