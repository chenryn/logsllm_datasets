Since these distributions are not mutually distinguishable, we use
clustering analysis to create clusters (groups) of sources with very
similar properties. We use the Euclidean distance as the metric with
a bound (threshold) for the creation of clusters, exactly as applied in
[36]. The result of the clustering analysis is visualized in Figure 2 as
a dendrogram. Instead of working with individual libraries, we do
the analysis with the groups. Even though we cannot differentiate
between libraries inside a group, luckily, the most popular libraries
(OpenSSL, Microsoft) are represented by a distinct or a very small
group.
Figure 2: The result of the clustering analysis visualized as
a dendrogram. Clusters are created based on the Euclidean
distance, with a separation threshold of 0.02 (blue dashed
line), similarly as the approach of [36]. The group numbers
are listed in brackets next to the source name.
Figure 1: Features extracted from a public RSA modulus. The
moduli generated by OpenSSL are always equal to one mod-
ulo three (N % 3 = 1), but they are uniformly distributed mod-
ulo four (N % 4 = 1 and N % 4 = 3, with the same probability).
The most significant bits of the moduli are never equal to
10002, and have the value 11002 more frequently than 11112.
(3) The modulus modulo 3: Another unexplained implementation
decision (in OpenSSL and elsewhere) avoids primes p if p − 1
has small divisors, other than 2. If p − 1 and q − 1 are never
divisible by 3, then the modulus is always equal to 1 modulo 3
and never equal to 2 modulo 3. For larger prime divisors (5, 7,
11, etc.), the property is not directly observable from a single
modulus and is therefore impractical for key classification.
We rely on the deep analysis of the key generation process and
statistical properties of the resulting keys presented in [36]. We
construct our mask similarly, however, we decided to drop the
feature encoding the bit length of the modulus – as it is only relevant
for one uncommon library implementation.
Our choice of the mask is reflected in Figure 1, illustrated by the
distributions of mask values for OpenSSL. When compared to [36],
we also changed the order of the features in the mask, to allow for
an easier interpretation of distributions from the illustrations. The
relevant biases for all sources are listed in Table 3 in the Appendix.
2.2 Clustering analysis
For each source we generate a large number of keys (typically
one million), extract the features from the keys according to the
feature mask and compute the distributions of the feature masks.
The previous research [36] used a highly representative sample of
cryptographic libraries in what were then the most recent available
versions. For this research, we also added keys from two hardware
security module devices, two cryptographic tokens and from the
PuTTy software, a popular SSH implementation for Microsoft Win-
dows. The latter is valuable when we consider the domain of SSH
authentication keys collected from GitHub.
We also collected new keys from the latest implementations of
the considered libraries, this unveiled a change in the behavior of
Libgcrypt 1.7.6 in the FIPS mode. Furthermore, we added earlier
releases of several libraries, to support the claims made about older
datasets. However, we only detected a change in the algorithm
for Nettle 2.0, when comparing the libraries with their current
064128192255Mask value0.00.51.01.52.02.5Mask probability [%]N % 3 == 1N % 3 == 2 N % 4   == 1 N % 4   == 3 N % 4   == 1 N % 4   == 3N = 1000000...N = 1100000...N = 1111111...N = 1000000...N = 1100000...N = 1111111...N = 1000000...N = 1100000...N = 1111111...N = 1000000...N = 1100000...N = 1111111...Feature mask0.300.150.00                                                  Clustering of sourcesEuclidean distanceo[2] G&D SmartCafe 4.x & 6.0o[7] OpenSSL 0.9.7 & 1.0.2g & 1.0.2k & 1.1.0eo[5] NXP J2A080 & J2A081 & J3A081 & JCOP 41 V2.2.1o[6] Oberthur Cosmo Dual 72Ko[8] PGPSDK 4 FIPSo[12] HSM Utimaco Security Server Se50o[12] Bouncy Castle 1.53 (Java)o[12] SunRsaSign OpenJDK 1.8.0o[12] PuTTY 0.67o[12] Cryptix JCE 20050328o[12] Nettle 2.0o[12] FlexiProvider 1.7p7o[12] PolarSSL 0.10.0o[12] mbedTLS 1.3.19 & 2.2.1 & 2.4.2o[4] Gemalto GXP E64o[13] PGPSDK 4o[13] Oberthur Cosmo 64o[13] Gemalto GCX4 72Ko[13] Feitian JavaCOS A22 & A40o[13] HSM SafeNet Luna SA−1700o[13] LibTomCrypt 1.17o[13] Botan 1.5.6 & 1.11.29 & 2.1.0o[13] OpenSSL FIPS 2.0.12 & 2.0.14o[13] cryptlib 3.4.3 & 3.4.3.1o[13] WolfSSL 2.0rc1 & 3.9.0 & 3.10.2o[13] Libgcrypt 1.6.0 & 1.6.5 & 1.7.6o[13] Nettle 3.2 & 3.3o[13] Libgcrypt 1.6.0 FIPS & 1.6.5 FIPSo[11] Libgcrypt 1.7.6 FIPSo[11] Crypto++ 5.6.0 & 5.6.3 & 5.6.5o[11] Microsoft CryptoAPI & CNG & .NETo[11] Bouncy Castle 1.54 (Java)o[3] GNU Crypto 2.0.1o[10] YubiKey NEOo[10] NXP J2D081 & J2E145Go[1] G&D SmartCafe 3.2o[9] YubiKey 4 & 4 Nanoo[9] Infineon JTOP 80KClustering threshold = 0.021642.3 Dataset classification – original approach
The main focus of the authors of [36] was to get information about
the origin (the most probable group G) of a particular key K. To
achieve it, the authors applied the Bayes’ rule:
P (G|K ) = P (K|G)P (G)
(1)
,
P (K )
where P (G|K ) is the conditional probability that a group G was used
to generate a given key K (the aim of the classification), P (K|G)
is the conditional probability that a key K is generated by a given
group G (obtained from the reference distributions), P (G) is the
prior probability of a group G and P (K ) is the probability of a key
K in a dataset. The highest numerical value of P (G|K ) corresponds
to the first guess on the most probable group G.
To reason about the popularity of libraries in large datasets, all
keys were considered separately and then the information was
summarized. Among the main shortcomings of the method was the
assumption that cryptographic libraries (alternatively, groups of
libraries) are chosen evenly by users (i.e., the prior probability P (G)
is equal for all groups G), which is evidently false. The method also
failed to consider the “big picture” – keys were considered in small
batches (e.g., a single key or a few keys assumed to originate from
a same source), hence the probability P (K ) of a key was usually 1.
2.4 Dataset classification – our approach
We improved the method in the following way: to estimate the
origin of a key, we use an appropriate prior probability P (G) for the
domain where the key can be found (e.g., different for TLS, PGP,
SSH...). To our best knowledge, no reliable estimates of the prior
probability P (G) were published for large domains. We therefore
propose and apply our own method for estimating the proportion of
cryptographic libraries in large datasets, based on statistical infer-
ence. In this way, we construct a tailored prior probability estimate
from the “big picture” before we make claims about individual keys.
To accomplish the prior probability estimation, we create a model
based on our reference distributions and we search for parameters
of the model that best match the whole observed sample. We use
a numerical method – the non-negative least squares fit (NNLSF)
[30]. It is the standard least squares fit method with a restriction
on the parameters, since the probabilities must be non-negative. A
detailed description of the methodology is given in Section 3.
The described approach also provides more than a two-fold
increase in the accuracy of origin estimation for public keys when
compared to the original approach of [36] in the domain of TLS.
The improvement is due to the application of the obtained prior
probabilities. More details and accuracy measurements for the prior
probability estimation itself are discussed in Section 3.4.
2.5 Limitations
Individual sources that belong to a single group cannot be mutually
distinguished. Fortunately, the two most significant TLS libraries
belong to small groups – OpenSSL is the single source in Group 7
and Microsoft libraries share Group 11 only with Crypto++ and two
recently introduced library versions – Bouncy Castle since version
1.54 (December 2015) and Libgcrypt 1.7.6 FIPS (January 2017).
The particular version of a library cannot be identified, only a
range of versions with the same key generation algorithm. E.g.,
Bouncy Castle from version 1.53 can be differentiated from version
1.54 due to a change in key generation, but not from version 1.52
that shares the same code in the relevant methods.
Based on our simulations, an accurate prior probability estima-
tion requires a dataset with at least 105 keys. However, note that the
classification of a single key is still possible and on average benefits
greatly from the accurate prior probability of its usage domain.
3 METHODOLOGY IN DETAIL
When aiming to estimate the library usage in a given domain, we
create a model of the domain backed by reference distributions
collected from known sources. We obtain RSA keys from the tar-
get domain and search for the parameters of our model which fit
the observed data. We use the model and the estimated library
probabilities to classify individual keys according to their origin.
3.1 Model
We assume there are m groups of sources, created by clustering
analysis (Section 2.2) based on the similarity of the distributions
of generated public keys. The probability P (K ) that a randomly
chosen key in the sample has a particular mask value K (the feature
mask is explained in Section 2.1) is given by:
m(cid:88)
j=1
P (K ) =
P (Gj )P (K|Gj ),
(2)
which is the sum of probabilities P (Gj )P (K|Gj ) over all m groups
Gj, where P (Gj ) is the probability that a source from a group Gj
is chosen in a particular domain (prior probability of source in the
domain) and P (K|Gj ) is the conditional probability of generating a
key with mask K when a library from the group Gj is used.
The probabilities P (K|Gj ) are estimated by generating a large
number of keys from available sources, which represent the refer-
ence distributions of the key masks (the profile of the group). The
probability of a key K in a dataset of real-world keys is approxi-
mated as #K/D, where #K is the number of keys with the mask K
and D is the number of all keys in the dataset.
3.2 Prior probability estimation
The process of estimating prior probability is completely automated
and does not require any user input. This fact allows us to construct
an independent estimate of library usage from public keys only,
without an influence of other information.
We find what are the likely prior probabilities of libraries that
would lead to the observed distribution of keys in a given sample,
based on the reference group profiles. The principle is illustrated in
Figure 3 – the observed distribution is reconstructed by combining
the 13 distributions in a specific ratio (prior probability estimated by
our approach). Intuitively, for a good estimate of prior probabilities
it is necessary (but not always sufficient) that the observed and the
reconstructed distributions match closely.
For each of n possible values of mask K, we substitute observed
values into Equation 2. In our case, the system has 256 equations
with 13 unknowns P (Gj ). Since both the distribution of real world
165Figure 3: The reference distributions of the key mask from each library are used to compute the probability with which the
given library contributes to the overall distribution of keys in the measured sample. The distribution of keys submitted to
Certificate Transparency logs during March 2017 likely contains keys from a mix of distributions as given in the last picture.
When we scaled each distribution accordingly and plotted the bars on top of each other (note – the bars are not overlapping),
the fit is visually close (the original distribution is given by a black dashed outline and matches the tops of the approximated
bars).
keys and the reference distributions are empirical, a precise solution
may not exist, due to the presence of noise (see Section 3.4.1).
We chose the linear least squares method constrained to non-
negative coefficients (non-negative least squares fit or NNLSF) im-
plemented in Java [28] based on the algorithm by Lawson and
Hanson [30] to find an approximate solution to the overdetermined
system of equations. The solution is the estimated prior probability
ˆP (Gj ) for each group Gj. The method numerically finds a solution
that minimizes the sum of squared errors (P (Ki ) − ˆP (Ki ))
2 over
all n mask values Ki, where P (Ki ) is the idealized probability of
mask Ki (obtained from the dataset) and ˆP (Ki ) is the estimated
probability, given by substituting the real group probability P (Gj )
in Equation 2 with the estimated group probability ˆP (Gj ).
3.3 Key classification
We classify the keys according to their origin using the Bayes’ rule
(Section 2.3). When compared to the approach of [36], we use the
estimated prior probabilities for a more precise classification. In a
classification of a single key, the groups are ordered by the value of
the conditional probability P (G|K ). The group G with the highest
probability is the most likely origin of the key.
3.4 Evaluation of accuracy
We are interested both in the accuracy of the prior probability esti-
mation (given as the expected error in the estimation, Table 1) and
in the average correctness of the overall classification process (given
as the proportion of keys that were correctly classified, Table 2). For
the measurement, we repeatedly simulate large datasets according
to different distributions, add noise and perform our method.
3.4.1 Random noise. Even if keys in a large dataset were gener-
ated with the same library across many users, the overall distribu-
tion will not match our reference distribution exactly, due to the
non-deterministic nature of key generation. This contributes to a
0641281922550123456Mask probability [%]Group  10641281922550.00.51.01.5Mask probability [%]Group  60641281922550.00.20.40.60.8Mask probability [%]Group 11064128192255024681012Mask probability [%]Group  20641281922550.00.51.01.5Mask probability [%]Group  70641281922550.00.20.40.60.8Mask probability [%]Group 1206412819225501234Mask probability [%]Group  30641281922550.00.20.4Mask probability [%]Group  80641281922550.00.20.40.60.8Mask probability [%]Group 130641281922550.00.51.0Mask probability [%]Group  40641281922550.00.51.01.5Mask probability [%]Group  506412819225501234Mask probability [%]Group  906412819225501234Mask probability [%]Group 10064128192255Mask0K50K100K150K200KNumber of keys with a given maskCertificate Transparency 03/2017Total keys: 17,745,546Original distributionGroup   7: 60.06%Group 12: 20.40%Group 13: 15.21%Group 11: 3.50%Group   4: 0.68%10/1001/1104/1107/1110/1101/1204/1207/1210/1201/1304/1307/1310/1301/1404/1407/1410/1401/1504/1507/1510/1501/1604/1607/1610/1601/1704/170M1M2M3M4M5M6M7M8MEstimated number of keysHTTPS EcosystemEFF SSL ObservatoryRapid7 SonarCensys IPv4 TLS scanSonar fixed TLS 1.2 handshakeUnfinished Sonar scanGroup  1: G&D SmartCafe 3.2Group  2: G&D SmartCafe 4.x & 6.0Group  3: GNU Crypto 2.0.1Group  4: Gemalto GXP E64Group  5: NXP J2A080 & J2A081                 & J3A081 & JCOP 41 V2.2.1Group  6: Oberthur Cosmo Dual 72KGroup  7: OpenSSL 0.9.7 & 1.0.2g                 & 1.0.2k & 1.1.0eGroup  8: PGPSDK 4 FIPSGroup  9: Infineon JTOP 80K, YubiKey 4 & 4 NanoGroup 10: NXP J2D081 & J2E145G, YubiKey NEOGroup 11: BouncyCastle 1.54 (Java), Crypto++ 5.6.0 & 5.6.3 & 5.6.5,                  Libgcrypt 1.7.6 FIPS, Microsoft CryptoAPI & CNG & .NETGroup 12: BouncyCastle 1.53 (Java), Cryptix JCE 20050328,                  FlexiProvider 1.7p7, HSM Utimaco Security Server Se50,                  Nettle 2.0, PolarSSL 0.10.0, PuTTY 0.67, SunRsaSign OpenJDK 1.8.0,                  mbedTLS 1.3.19 & 2.2.1 & 2.4.2Group 13: Botan 1.5.6 & 1.11.29 & 2.1.0, Feitian JavaCOS A22 & A40,                  Gemalto GCX4 72K, HSM SafeNet Luna SA-1700, LibTomCrypt 1.17,                  Libgcrypt 1.6.0 & 1.6.5 & 1.7.6, Libgcrypt 1.6.0 FIPS & 1.6.5 FIPS,                  Nettle 3.2 & 3.3, Oberthur Cosmo 64, OpenSSL FIPS 2.0.12 & 2.0.14,                  PGPSDK 4, WolfSSL 2.0rc1 & 3.9.0 & 3.10.2, cryptlib 3.4.3 & 3.4.3.1166random noise in the data. We achieve such a noise in our simula-
tions by generating masks non-deterministically (i.e., instead of
using reference distributions in place of data, we randomly sample
masks according to the distribution).
3.4.2
Systematic noise. Our analysis does not cover all existing
libraries used on the Internet. However, it is quite likely that al-
gorithms used by unknown libraries are similar to those already
known (e.g., consider the size of Group 13). In such a case, the
library would belong to one of our groups and the only error of
the estimation would be in the interpretation of the results – the
library is not correctly labeled as a part of our group. Yet still, there
may exist groups with profiles that do not match any of our known
groups, hence keys generated from these implementations would
add systematic noise to the profile of the sample. In our simula-
tions, we create a group representing all unknown distributions.
The group profile is chosen randomly in each experiment. To sim-
ulate the presence of keys from this group, we modify the prior
probability of the simulation to include a certain percentage (e.g.,
ranging from 0% to 3% in Tables 1 and 2) of keys to be sampled from
the distribution. For example, 3% of systematic noise represents
the situation where 3% of the keys in the sample originate from an
unknown distribution, not covered by our analysis and belonging
to a completely different, never seen before, group.