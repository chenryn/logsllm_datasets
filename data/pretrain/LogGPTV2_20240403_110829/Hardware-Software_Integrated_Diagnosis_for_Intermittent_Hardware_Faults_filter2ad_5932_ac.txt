88
lsq
-
15
-
24
-
2
3
4
5
ifq
15
16
2
3
1
2
3
4
5
rs
16
11
52
46
7
38
19
40
44
fu
1
5
2
5
1
6
6
5
6
Table I: RUI of the instructions logged by SCRIBE. The
original execution crashes at instruction 9.
that
experiencing an intermittent fault
is triggered non-
deterministically and lasts for several cycles. When the
functional unit experiences the fault, one of the bits in its
output becomes stuck at zero for this time period. This
causes an incorrect value to be produced, as a result of which
the program crashes. After the crash, the entire register and
memory state of the process is dumped to memory. For this
example, we only show the register and memory values pro-
duced by the instructions in Table I. These values are shown
in Table II, column “Snapshot Original”. The “producer
index” column represents the index of the instructions in
Table I that last wrote to the locations in the second column.
Producer Mem/Reg
Location
Index
0xd3e0
2
4
0xd988
6
7
8
r1
r3
r30
Producer
stq r1, 400(r15)
stl r3, 0(r9)
ldq r1, 0(r30)
ldq r3, 8(r30)
ldq r30, 16(r30)
Snapshot
Original
8
10
16
8
20
Snapshot
Replayed
12
10
0
20
0
Table II: Snapshots: These represent the memory and register
state dumps after the original and replayed executions
A. DDG Construction with RUI
As mentioned in Section III-C, SIED uses deterministic
replay techniques to replay the execution of the failed
program and build its DDG. We refer to the ﬁrst execution
leading to the failure as the original execution and the second
execution performed by SIED as the replayed execution.
The steps taken by SIED to build the DDG are as follows
(step numbers below correspond to those in Figure 1):
i) The program is started from a previous checkpoint or
from the beginning and replayed. However, the replayed
program’s control-ﬂow may not match the control ﬂow
of the original execution, as the latter may have been
modiﬁed by the intermittent fault. To facilitate fault
diagnosis, the only difference between the original and
the replayed execution should be the intermittent fault’s
effects on the registers and memory state. Therefore, the
control ﬂow of the replayed execution (target addresses
of the branch instructions) is modiﬁed to match the
original execution’s control ﬂow (step 4). To obtain
the original execution’s control ﬂow, SCRIBE logs the
branch target addresses of the program in addition to
its RUI.
ii) From the replayed execution, the information needed
for building the Dynamic Dependence Graph (DDG)
of the program is extracted and the DDG is built
(step 5). Figure 5 shows the DDG for our example.
The information required for building the DDG can be
extracted by using a dynamic binary instrumentation
tool (e.g. Pin [22]). We note that the overheads added
by such tools would only be incurred during failure and
subsequent diagnosis, and not during regular operation.
iii) When the program ﬂow of the replayed execution
reaches the crash instruction (the instruction at which
the original execution crashes), the register and memory
state of the replayed execution is dumped to memory
(step 6). There could be rare cases in which the replayed
execution fails due to inconsistency between the control
ﬂow and the data. These cases lead to the diagnosis
process being stopped if happened before reaching to
the crash instruction. In the example,
the replayed
execution stops when reaching instruction 9 and the
column “snapshot replayed” in Table II represents the
register and memory state of the replayed program at
that instruction.
iv) The snapshots taken after the original and replayed
executions are compared with each other to identify the
ﬁnal values that are different from each other. Because
we assume a deterministic replay, any deviation in the
values must be due to the fault. The producer instruc-
tions of these values are marked as ﬁnal erroneous (or
ﬁnal correct) if the ﬁnal values are different (or the
same) in the DDG. The branch instructions that needed
to be modiﬁed in step (i) to make the control ﬂows
match are also marked as ﬁnal erroneous in the DDG.
In the example, the values in the snapshot columns of
Table II are compared, and the differences identiﬁed.
The nodes corresponding to the instructions creating
the mismatched values are marked in the DDG as ﬁnal
erroneous nodes (nodes 2, 6, 7 & 8), while node 4 with
matching values, is marked as ﬁnal correct.
v) The RUI of each instruction is added to its correspond-
ing node in DDG. We call the resulting graph, the
augmented DDG. The augmented DDG is used to ﬁnd
the faulty resource as shown in the next section.
Figure 5: DDG of the program in the running example. Gray
nodes are ﬁnal erroneous and the dotted node is ﬁnal correct
B. DDG Analysis
This section explains how SIED analyzes the augmented
DDG to ﬁnd the faulty resource. Because each dynamic
instruction corresponds to a DDG node, we use the terms
node and instruction interchangeably. The main idea is to
start from ﬁnal erroneous nodes in the augmented DDG
(identiﬁed in Section V-A), and backtrack to ﬁnd nodes
that have originated the error,
the instructions that
have used the faulty resource. The faulty resource is found
i.e.,
368368368
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:50 UTC from IEEE Xplore.  Restrictions apply. 
by considering the intersection of the resources used by
multiple instructions that have originated the errors. Recall
that the list of resources used by an instruction is present in
its corresponding node in the augmented DDG.
There are three types of nodes in the augmented DDG: i)
Nodes that have used the faulty resource (originating nodes),
ii) Nodes to which the error is propagated from an ancestor,
iii) Nodes that have produced correct results (correct nodes).
The goal of backtracking is to search for the originating
nodes, by going backward from the ﬁnal erroneous nodes
(i.e., erroneous nodes in the ﬁnal state), while avoiding the
correct nodes. Naive backtracking does not avoid correct
nodes, and because there can be many correct nodes in the
backward slice of a ﬁnal erroneous node, it will incur false-
positives. Therefore, we propose two heuristics to narrow
down the search space for the faulty resource based on the
following observations:
i) If a ﬁnal erroneous node has a correct ancestor node,
the probability of the originating node being in the path
connecting those two nodes is high. In other words, the
faulty resource is more likely to be used in this path.
ii) Having a ﬁnal correct descendent decreases the proba-
bility that the node is erroneous.
iii) Having an erroneous ancestor decreases the probability
of the node being an originating node.
iv) An erroneous node with all correct predecessors is an
originating node.
Heuristics: To ﬁnd faulty resources, each resource in the
processor is assigned a counter which is initialized to zero.
The counter of a resource is incremented if an instruction
using that resource is likely to participate in creating an
erroneous value, as determined by the heuristics. Resources
having larger counter values are more likely to be faulty.
Algorithm 1 shows the pseudocode for heuristic 1. The
main idea behind heuristic 1 is to examine the backward
slices of the ﬁnal erroneous nodes and increase the counter
values of the appropriate resources based on the ﬁrst three
observations. In lines 3 to 8, for each ﬁnal erroneous node
n, the set Sn1 is populated with the nodes between n and its
ﬁnal correct ancestors. The counters of the resources used
by the nodes in the Sn1 are incremented. Lines 9 to 11
correspond to the second observation. Every node in the
backward slice of the ﬁnal erroneous node n is added to set
Sn2 unless it has a ﬁnal correct descendent. Finally, in lines
12 to 17, the nodes that are added to the set Sn2 are checked
to see if they have a faulty ancestor. If so, their counters are
incremented by 0.5, and if not, the counters are incremented
by 1. This is in line with the third observation that nodes
with faulty ancestors are less likely to be originating nodes.
Algorithm 2 presents the second heuristic which is based
on Observation 4. The algorithm starts from the ﬁnal correct
nodes and recursively marks the nodes that are likely to have
produced correct output (lines 1 to 2). Then it recursively
marks the nodes that have likely produced erroneous outputs
starting from the ﬁnal erroneous nodes (lines 3 and 4).
Finally, it checks all the erroneous nodes for the condition
in the fourth observation i.e., being erroneous with no
erroneous predecessor (lines 5 to 9). If the condition is
satisﬁed, it increments the counters for the resources used
by the erroneous nodes by 1.
Algorithm 1: Heurisitc 1
input: resources
Algorithm heuristic1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
foreach node n of ﬁnal erroneous nodes do
Sn1 = Sn2 = φ // Initializing sets
foreach node k of n.ancestors do
if k.isLastCorrect() then
Sn1.add(getNodesBetween(n , k))
end
foreach R of resources do
if R is used in Sn1 then
counters[R]++;
end
foreach node k of nodes in backward slice of
n do
if not(k.hasFinalCorrectDescendent) then
Sn2.add(k)
end
foreach R of resources do
if R is used in Sn2 then
if n.hasFaultyAncestor() then
counters[R] += 0.5
else
counters[R] += 1
end
end
end
end
end
end
Algorithm 2: Heurisitc 2
Procedure markCorrect(node n)
foreach node p of the predecessors of n do
ec ← p.getErroneousChildrenCount()
if not (p.isErroneous() OR ec ≥ 2) then
p.correct ← True
markCorrect(p)
Procedure markErroneous(node n)
foreach node p of the predecessors of n do
cp ← p.getNonCorrectPredecessorsCount()
cc ← p.getCorrectChildrenCount()
if cp == 1 AND cc ≤ 1 then
p.erroneous ← True
markErroneous(p)