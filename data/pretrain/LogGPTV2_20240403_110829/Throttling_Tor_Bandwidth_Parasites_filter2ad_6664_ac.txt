to quietest. We then select and throttle the loudest frac-
tion T of connections, where T is a conﬁgurable thresh-
old. For example, setting T to 0.1 means the loudest ten
percent of client connections will be throttled. The se-
lection is adaptive since the EWMA changes over time
according to each connection’s bandwidth usage.
We have adaptively selected which connections to
throttle and now must determine a throttle rate. To do
this, we require that each connection tracks its through-
put over time. We choose the average throughput rate
of the connection with the minimum EWMA from the
set of connections being throttled. For example, when T
= 0.1 and there are 100 client connections sorted from
loudest to quietest, the chosen throttle rate is the average
throughput of the tenth connection. Each of ﬁrst ten con-
nections is then throttled at this rate. In our prototype,
we approximate the throughput rate as the average num-
ber of bytes transferred over the last R seconds, where
R is conﬁgurable. R represents the period of time be-
tween which the algorithm re-selects the throttled con-
nections, adjusts the throttle rates, and resets each con-
nection’s throughput counters.
There is one caveat to the algorithm as described
above.
In our experiments in Section 4, we noticed
that occasionally the throttle rate chosen by the thresh-
old algorithm was zero. This would happen if the mean
throughput of the threshold connection (line 6 in Algo-
rithm 3) did not send data over the last R seconds. To
prevent a throttle rate of zero, we added a parameter to
statically conﬁgure a throttle rate ﬂoor F so that no con-
nection would ever be throttled below F. Algorithm 3
details threshold adaptive throttling.
4 Experiments
In this section we explore the performance beneﬁts possi-
ble with each throttling algorithm speciﬁed in Section 3.
We perform experiments with Shadow [2, 31], an accu-
rate and efﬁcient discrete event simulator that runs real
Tor code over a simulated network. Shadow allows us to
run an entire Tor network on a single machine and conﬁg-
ure characteristics such as network latency, bandwidth,
and topology. Since Shadow runs real Tor, it accurately
characterizes application behavior and allows us to focus
on experimental comparison of our algorithms. A direct
comparison between Tor and Shadow-Tor performance
is presented in [31].
Experimental Setup. Using Shadow, we conﬁgure a pri-
vate Tor network with 200 HTTP servers, 950 Tor web
clients, 50 Tor bulk clients, and 50 Tor relays. The dis-
tribution of clients in our experiments approximates that
found by McCoy et al. [38]. All of our nodes run inside
the Shadow simulation environment.
In our experiments, each client node runs Tor in client-
only mode as well as an HTTP client application conﬁg-
ured to download over Tor’s SOCKS proxy available on
the local interface. Each web client downloads a 320 KiB
ﬁle5 from a randomly selected one of our HTTP servers,
and pauses for a length of time drawn from the UNC
“think time” data set [27] before downloading the next
ﬁle. Each bulk client repeatedly downloads a 5 MiB ﬁle
from a randomly selected HTTP server without pausing.
Clients track the time to the ﬁrst and the last byte of the
download as indications of network responsiveness and
overall performance.
Tor relays are conﬁgured with bandwidth parameters
according to a Tor network consensus document.6 We
conﬁgure our network topology and latency between
nodes according to the geographical distribution of re-
lays and pairwise PlanetLab node ping times. Our sim-
ulated network mirrors a previously published Tor net-
work model [31] that has been compared to and shown to
closely approximate the load of the live Tor network [3].
We focus on the time to the ﬁrst data byte for web
clients as a measure of network responsiveness, and
the time to the last data byte—the download time—for
both web and bulk clients as a measure of overall per-
formance.
In our results, “vanilla” represents unmod-
iﬁed Tor using a round-robin circuit scheduler and no
throttling—the default settings in the Tor software—and
can be used to compare relative performance between
experiments. Each experiment uses network-wide de-
ployments of each conﬁguration. To further reduce ran-
dom variances, we ran all conﬁgurations ﬁve times each.
Therefore, every curve on every CDF shows the cumula-
tive results of ﬁve experiments.
Results. Our results focus on the algorithmic conﬁg-
urations that we found to maximize web client perfor-
mance [33] while we show how the algorithms perform
when the network load varies from light (25 bulk clients)
to medium (50 bulk clients) to heavy (100 bulk clients).
The experimental setup is otherwise unmodiﬁed from the
model described above. Running the algorithms under
various loads allows us to highlight the unique and novel
features each provides.
Figure 3 shows client performance for our algorithms.
The time to ﬁrst byte indicates network responsiveness
for web clients while the download time indicates overall
client performance for web and bulk clients. Client per-
formance is shown for the lightly loaded network in Fig-
ures 3a–3c, the normally loaded network in Figures 3d–
3f, and the heavily loaded network in Figures 3g–3i.
5The average webpage size reported by Google web metrics [45].
6Retrieved on 2011-04-27 and valid from 03-06:00:00
7
(a) 320 KiB clients, light load
(b) 320 KiB clients, light load
(c) 5 MiB clients, light load
(d) 320 KiB clients, medium load
(e) 320 KiB clients, medium load
(f) 5 MiB clients, medium load
(g) 320 KiB clients, heavy load
(h) 320 KiB clients, heavy load
(i) 5 MiB clients, heavy load
Figure 3: Comparison of client performance for each throttling algorithm and vanilla Tor, under various load. All experiments use
950 web clients. We vary the load between “light,” “medium,” and “heavy” by setting the number of bulk clients to 25 for 3a–3c,
to 50 for 3d–3f, and to 100 for 3g–3i. The time to ﬁrst byte indicates network responsiveness while the download time indicates
overall client performance. The parameters for each algorithm are tuned based on experiments presented in [33].
Overall, static throttling results in the least amount of
bulk trafﬁc throttling while providing the lowest bene-
ﬁt to web clients. For the bit-splitting algorithm, we
see improvements over static throttling for web clients
for both time to ﬁrst byte and overall download times,
while download times for bulk clients are also slightly
increased. Flagging and threshold throttling perform
somewhat more aggressive throttling of bulk trafﬁc and
therefore also provide the greatest improvements in web
client performance.
We ﬁnd that each algorithm is effective at throttling
bulk clients independent of network load, as evident in
Figures 3c, 3f and 3i. However, performance beneﬁts for
web clients vary slightly as the network load changes.
When the number of bulk clients is halved, throughput
in Figure 3b is fairly similar across algorithms. How-
ever, when the number of bulk clients is doubled, re-
sponsiveness in Figure 3g and throughput in Figure 3h
for both the static throttling and the adaptive bit-splitting
algorithm lag behind the performance of the ﬂagging and
threshold algorithms. Static throttling would likely re-
quire a reconﬁguration of throttling parameters while bit-
splitting adjusts the throttle rate less effectively than our
ﬂagging and threshold algorithms.
As seen in Figures 3a, 3d, and 3g, as the load
changes, the strengths of each algorithm become appar-
ent. The ﬂagging and threshold algorithms stand out as
the best approaches for both web client responsiveness
and throughput, and Figures 3c, 3f, and 3i show that
they are also most aggressive at throttling bulk clients.
The ﬂagging algorithm appears very effective at accu-
rately classifying bulk connections regardless of network
8
02468101214WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh051015202530WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0100200300400500BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh02468101214WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh051015202530WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0100200300400500BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh02468101214WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh051015202530WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0100200300400500BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthreshh
g
i
l
t Data (GiB)
Web (%)
Bulk (%)
m Data (GiB)
u
Web (%)
i
d
e
Bulk (%)
m
y Data (GiB)
Web (%)
Bulk (%)
v
a
e
h
vanilla
88.3
74.5
25.5
92.2
65.8
34.2
94.7
55.8
44.2
static
80.3
83.7
16.3
88.6
72.4
27.6
91.1
60.5
39.5
split
78.3
85.9
14.1
84.7
75.0
25.0
85.0
64.3
35.7
ﬂag
72.1
92.7
7.3
77.7
86.2
13.8
81.7
75.4
24.6
thresh
69.8
90.1
9.9
76.3
82.8
17.2
85.0
71.2
28.8
Table 1: Total data downloaded in our simulations by client
type. Throttling reduces the bulk trafﬁc share of the load on the
network. The ﬂagging algorithm is the best at throttling bulk
trafﬁc under light, medium, and heavy loads of 25, 50, and 100
bulk clients, respectively.
load. The threshold algorithm maximizes web client per-
formance in our simulations among all loads and all al-
gorithms tested, since it effectively throttles the worst
bulk clients while utilizing extra bandwidth when possi-
ble. Both the threshold and ﬂagging algorithms perform
well over all network loads tested, and their usage in Tor
would require little-to-no maintenance while providing
signiﬁcant performance improvements for web clients.
Aggregate download statistics are shown in Table 1.
The results indicate that we are approximating the load
distribution measured by McCoy et al. [38] reasonably
well. The data also indicates that as the number of
bulk clients in our simulation increases, so does the total
amount of data downloaded and the bulk fraction of the
total as expected. The data also shows that all throttling
algorithms reduce the total network load. Static throt-
tling reduces load the least, while our adaptive ﬂagging
algorithm is both the best at reducing both overall load
and the bulk percentage of network trafﬁc. Each of our
adaptive algorithms are better at reducing load than static
throttling, due to their ability to adapt to network dynam-
ics. The relative difference between each algorithm’s ef-
fectiveness at reducing load roughly corresponds to the
relative difference in web client performance in our ex-
periments, as we discussed above.
Discussion. The best algorithm for Tor depends on mul-
tiple factors. Although not maximizing web client per-
formance, bit-splitting is the simplest, the most efﬁcient,
and the most network neutral approach (every connec-
tion is allowed the same portion of a guard’s capacity).
This “subtle” or “delicate” approach to throttling may be
favorable if supporting multiple client behaviors is de-
sirable. Conversly, the ﬂagging algorithm may be used
to identify a speciﬁc class of trafﬁc and throttle it ag-
gressively, creating the potential for the largest increase
in performance for unthrottled trafﬁc. We are currently
exploring improvements to our statistical classiﬁcation
techniques to reduce false positives and to improve the
9
control over trafﬁc of various types. For these reasons,
we feel the bit-splitting and ﬂagging algorithms will be
the most useful in various situations. We suggest that
perhaps bit-splitting is the most appropriate throttling al-
gorithm to use initially, even if something more aggres-
sive is desirable in the long term.
While requiring little maintenance, our algorithms
were designed to use only local relay information.
Therefore, they are incrementally deployable while re-
lay operators may choose the desired throttling algorithm
independent of others. Our algorithms are already imple-
mented in Tor and software patches are available [5].
5 Analysis and Discussion
Having shown the performance beneﬁts of throttling bulk
clients in Section 4, we now analyze the security of
throttling against adversarial attacks on anonymity. We
will discuss the direct impact of throttling on anonymity:
what an adversary can learn when guards throttle clients
and how the information leaked affects the anonymity of
the system. We lastly discuss potential strategies clients
may use to elude the throttles.
Before exploring practical attacks, we introduce two
techniques an adversary may use to gather information
about the network given that a generic throttling algo-
rithm is enabled at all guards. Similar techniques used
for throughput-based trafﬁc analysis outside the context
of throttling are discussed in detail by Mittal et al. [39].
Discussion about the security of our throttling algorithms
in the context of practical attacks will follow.
5.1 Gathering Information
Our analysis uses the following terminology. At time t,
the throughput of a connection between a client and a
guard is λt, the rate at which the client will be throttled is
αt, and the allowed data burst is β . Note that, as consis-
tent with our algorithms, the throttle rate may vary over
time but the burst is a static system-wide parameter.
Probing Guards. Using the above terminology, a con-
nection is throttled if, over the last s seconds, its through-
put exceeds the allowed initial burst and the long-term
throttle rate:
t
∑
k=t−s
(λk) ≥ β +
t
∑
k=t−s
(αk)
(1)
A client may perform a simple technique to probe a spe-
ciﬁc guard node and determine the rate at which it gets
throttled. The client may open a single circuit through
the guard, selecting other high-bandwidth relays to en-
sure that the circuit does not contain a bottleneck. Then,