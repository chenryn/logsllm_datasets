another similar VM also shares the host, each VM can achieve only
6.4 Production
To show the practicality of PicNIC, we deployed it in a large public
cloud. We summarize a subset of results here; §C has more details.
§2 showed how rate limiting slower flows at the egress introduces
HoL blocking delay for faster flows. Deployments of out-of-order
(OOO) completions and Packet Accounting eliminated HoL blocking
between flows at the egress. Consequently, OOO completions, alone,
improved the tail latency for customer traffic by ∼13%.
On deploying CWFQs at the ingress, we see 96% reduction in
packet drops at NIC Rx queues. Excess packets are dropped fairly at
the per-VM queues. As noted in §5, CWFQs by themselves cannot
eliminate NIC packet drops completely; when CWFQs are coupled
with admission control, we observe further reduction in packet
drops and response times in mitigating isolation breakages, even
under DoS attacks. To handle congestion in the network fabric, we
also extended our prototype to incorporate ECN signals.
7 Solution Space of Performance Isolation
Table 1 summarized related prior work which focus on sharing
fabric bandwidth and not resources at end-hosts. PicNIC ensures
isolation at end-hosts and complements prior work to build an
end-to-end predictable virtualized network.
7.1 Mechanisms Used in Practice
Isolation is usually enforced using the following approaches.
Guest congestion control: While we have relied on flow-level con-
gestion control, e.g. TCP, it is changing with the Cloud—VMs are
free to use any congestion control, including none.
Static bandwidth limits: A common approach is to apply static
egress BPS limits per-VM based on policy. As the limits are neither
defined nor enforced for ingress, they can not ensure isolation in
many cases—e.g., against incasts and DoS attacks.
Cloud product offerings: Achieving predictable performance in pub-
lic cloud is challenging. A survey of major cloud providers indi-
cates the lack of concrete performance guarantees and isolation
mechanisms [3, 27, 47]. Even with the opt-in highest-performance
options—e.g., SmartNIC [22] and AWS Enhanced Networking [4]—
cloud providers only specify an “up to” BPS limit for each VM,
similar to static bandwidth limits. None of the providers offer la-
tency or loss-rate SLOs.
361
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
7.2 Related Work
Abstractions. A class of recent work has focused on building ab-
stractions that aim to provide tenants an illusion of a dedicated
network fabric [5, 7, 28, 41, 69, 76]. Such abstractions range from a
static virtual cluster or datacenter to those encoding time-varying
demands and communication patterns [7, 41, 76]. In contrast, Pic-
NIC proposes the predictable vNIC abstraction at the VM level.
Dynamic bandwidth arbitration: For flow aggregates over WANs [30,
32], centralized systems can compute dynamic BPS allocations [38].
These are difficult to scale to enforce isolation at short timescales
across thousands of VMs. Even proposed distributed approaches [35],
which mostly share network bandwidth in the fabric, do not ensure
isolation at end hosts. Enforcing BPS limits [5, 7, 8, 14, 28, 39, 41,
62, 63, 66, 69, 69, 76] do not automatically ensure end-to-end pre-
dictable latency and loss rate, which PicNIC aims to deliver. Further,
compared to constant link capacities in the fabric, PicNIC provides
predictable performance while sharing variable packet processing
capacity of virtualization stacks.
Tradeoffs: FairCloud [62] and HUG [14] study fundamental trade-
offs with respect to network utilization and minimum bandwidth
guarantees. HUG also explores fairness in the context of multiple
resources [24, 25], which could be applicable in this case too.
8 Discussion on Hardware Design
8.1 Hardware is not a panacea
A classical approach to achieving isolation is to push the problem
one level down. In this context, it means moving functionality and
contention management into the hardware (HW), for instance by
using per-VM queues or SR-IOV [57]. However, even physical NICs
have PPS limits and have to share limited resources such as PCIe
and DRAM bandwidth [26, 43, 53, 61]. Even using SR-IOV may lead
to unfair sharing of resources and performance interference [13].
PPS overloads remain an Achilles heel for HW stacks too.
Using per-VM queues is challenging in practice as they are
needed at all potential points of contention. Physical resource limita-
tions imposed by the HW may mean insufficient number of queues
in HW at every such point. Fundamentally, HW queues are “local”
constructs lacking global visibility, and thus are insufficient to guar-
antee isolation which requires coordination with multiple senders.
For instance, drops due to PCIe bandwidth exhaustion at the ingress
can break isolation similar to §2.2. For the same reason, novel data-
plane approaches [9, 59] based on HW virtualization cannot provide
isolation on their own for the cases mentioned in §2. Recent work
in the context of RDMA has also demonstrated isolation issues
in HW [77]. Finally, as functionality moves to NICs [22, 72], it is
crucial to ensure predictable sharing of HW resources.
8.2 PicNIC in Hardware
We briefly discuss how PicNIC’s techniques can be applied in the
case of HW-based stacks. Usually, the NIC or FPGA implementing
the virtualization stack is connected over PCIe to the CPU. The
major potential bottlenecks along the path include PCIe bandwidth
and DRAM bandwidth. If one VM’s traffic monopolizes these re-
sources, it can lead to other VMs being starved unfairly and lead to
similar issues as illustrated in §2. Even if such resources are shared
fairly, packet drops can occur at ingress when the ingress rate for a
VM exceeds its fair-share rate. This again leads to unfairness as well
as inefficiency due to resource wastage. Thus, we need backpres-
sure to the sources to avoid contention at the receivers. At egress,
just rate limiting traffic can lead to drops and buffer contention in
the stack when a VM discharges packets at excessive rates. This
wastes resources such as PCIe and DRAM bandwidth, HW clock
cycles and SRAM buffers. So, we need to apply backpressure to the
guest OS stack as well. We find that the same design principles (§3)
carry over in the case of HW too. We outline the corresponding
PicNIC constructs for HW. For more details, see §A.
Ingress. In order to avoid unfair drops and delay in shared NIC Rx
queues, PicNIC can implement per-VM queues in HW. By monitor-
ing resources, such as PCIe bandwidth, used by each VM, PicNIC
can enforce SLO-based fair sharing by controlling how these queues
are scheduled.
Congestion Control. Both PCCB and PCCP are amenable to efficient
HW implementations. While PCCB monitors the ingress BPS per
VM, PCCP uses statistics from per-VM queues to compute rate lim-
its which are notified via feedback generated in the datapath. At
the egress, these rate limits are stored in a table, which may be par-
titioned between SRAM and DRAM based on resource constraints.
Egress. Recent work on scalable shaping [64, 71] can be leveraged to
enforce these rates efficiently. As the HW has accurate information
about when a packet is sent out, it can hold completion events till
then in order to implement OOO completions.
Overall, we believe that PicNIC’s design principles and constructs
are well-suited for implementing the predictable virtualized NIC ab-
straction on HW-based stacks too. We hope that the lessons learned
from PicNIC will cause performance isolation to be considered as a
primary objective for virtualization stacks and inform the design
of future NICs.
9 Conclusion
Isolation is a fundamental challenge in operating systems that is
exacerbated by VMs and cloud platforms. Today, cloud providers
face a dilemma: they must provide the illusion of an isolated virtual
slice of hardware to tenants, without being too wasteful of the
underlying resources. This paper presents PicNIC, a system that
uses a combination of localized SLO-based resource sharing and
end-to-end admission control, to provide the illusion of a dedicated
NIC to VMs, while responding to potential isolation breakages
within sub-ms timescales.
PicNIC opens up a number of interesting avenues for future
research. Can we pack more VMs onto each physical host without
sacrificing predictability? Where along the spectrum of isolation
and efficiency should cloud providers operate? If the time for re-
sponding to isolation breakages can be further brought down to
single-digit μs, what additional efficiencies might become possible?
How can future NIC designs facilitate predictable performance?
Therefore, we think of PicNIC as the first word about predictable vir-
tualized NICs, complementing prior work on sharing the network
fabric, not the last.
Acknowledgments. We would like to thank the anonymous SIG-
COMM reviewers, Dina Papagiannaki, Jeff Mogul and our shepherd,
Manya Ghobadi, for providing valuable feedback. This work was
partially supported by NSF grants CCF-1637532 and CNS-1413972
and ONR grant N00014-15-1-2177.
362
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
References
[1] Mohammad Alizadeh, Berk Atikoglu, Abdul Kabbani, Ashvin Lakshmikantha,
Rong Pan, Balaji Prabhakar, and Mick Seaman. 2008. Data Center Transport
Mechanisms: Congestion Control Theory and IEEE Standardization. In 46th
Annual Allerton Conference on Communication, Control, and Computing. IEEE,
Urbana-Champaign, IL, USA, 1270–1277.
[2] Mohammad Alizadeh, Albert Greenberg, Dave Maltz, Jitendra Padhye, Parveen
Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data
Center TCP (DCTCP). In SIGCOMM. ACM, New Delhi, India, 63—74.
[3] Amazon. 2018. Amazon EC2. https://aws.amazon.com/ec2/
[4] Amazon. 2018. Enhanced Networking on Linux. https://docs.aws.amazon.com/
AWSEC2/latest/UserGuide/enhanced-networking.html.
[5] Sebastian Angel, Hitesh Ballani, Thomas Karagiannis, Greg O’Shea, and Eno
Thereska. 2014. End-to-end Performance Isolation Through Virtual Datacenters.
In OSDI. USENIX Association, Broomfield, CO, 233–248.
[6] Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song Jiang, and Mike Paleczny.
2012. Workload analysis of a large-scale key-value store. In SIGMETRICS. ACM,
London, England, UK, 53–64.
[7] Hitesh Ballani, Paolo Costa, Thomas Karagiannis, and Ant Rowstron. 2011. To-
wards Predictable Datacenter Networks. In SIGCOMM. ACM, Toronto, Canada,
242–253.
[8] Hitesh Ballani, Keon Jang, Thomas Karagiannis, Changhoon Kim, Dinan Gu-
nawardena, and Greg O’Shea. 2013. Chatty Tenants and the Cloud Network
Sharing Problem. In NSDI. USENIX Association, Lombard, IL, 171–184.
[9] Adam Belay, George Prekas, Ana Klimovic, Samuel Grossman, Christos Kozyrakis,
and Edouard Bugnion. 2014. IX: A Protected Dataplane Operating System for
High Throughput and Low Latency. In OSDI. USENIX Association, Broomfield,
CO, 49–65.
[10] Theophilus Benson, Aditya Akella, and David A Maltz. 2010. Network Traffic
Characteristics of Data Centers in the Wild. In IMC. ACM, Melbourne, Australia,
267–280.
[11] Scott Bradner and Jim McQuaid. 1999. Benchmarking Methodology for Network
Interconnect Devices. https://www.ietf.org/rfc/rfc2544.txt.
[12] Randy Brown. 1988. Calendar Queues: A Fast O(1) Priority Queue Implementation
for the Simulation Event Set Problem. CACM 31, 10 (1988), 1220–1227.
[13] Quan Chen, Hailong Yang, Minyi Guo, Ram Srivatsa Kannan, Jason Mars, and
Lingjia Tang. 2017. Prophet: Precise QoS prediction on non-preemptive accel-
erators to improve utilization in warehouse-scale computers. In ASPLOS. ACM,
Xián, China, 17–32.
[14] Mosharaf Chowdhury, Zhenhua Liu, Ali Ghodsi, and Ion Stoica. 2016. HUG:
Multi-Resource Fairness for Correlated and Elastic Demands. In NSDI. USENIX
Association, Santa Clara, CA, 407–424.
[15] Jonathan Corbet. 2012. TCP Small Queues. https://lwn.net/Articles/507065/.
Online, accessed: 2019-07.
[16] Michael Dalton, David Schultz, Jacob Adriaens, Ahsan Arefin, Anshuman
Gupta, Brian Fahs, Dima Rubinstein, Enrique Cauich Zermeno, Erik Rubow,
James Alexander Docauer, Jesse Alpert, Jing Ai, Jon Olson, Kevin DeCabooter,
Marc de Kruijf, Nan Hua, Nathan Lewis, Nikhil Kasinadhuni, Riccardo Crepaldi,
Srinivas Krishnan, Subbaiah Venkata, Yossi Richter, Uday Naik, and Amin Vahdat.
2018. Andromeda: Performance, Isolation, and Velocity at Scale in Cloud Network
Virtualization. In NSDI. USENIX Association, Renton, WA, 373–387.
[17] Peter Druschel and Gaurav Banga. 1996. Lazy Receiver Processing (LRP): A Net-
work Subsystem Architecture for Server Systems. In OSDI. USENIX Association,
Seattle, WA, 261–275.
[18] Nick G Duffield, Pawan Goyal, Albert Greenberg, Partho Mishra, Kadangode K
Ramakrishnan, and Jacobus E van der Merwe. 1999. A Flexible Model for Resource
Management in Virtual Private Networks. In SIGCOMM. ACM, Cambridge, MA,
95–108.
[19] Nandita Dukkipati and Nick McKeown. 2006. Why Flow-Completion Time is
the Right Metric for Congestion Control. CCR 36, 1 (2006), 59–62.
[20] Daniel E Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov,
Eric Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and Jin-
nah Dylan Hosein. 2016. Maglev: A Fast and Reliable Software Network Load
Balancer. In NSDI. USENIX Association, Santa Clara, CA, 523–535.
[21] Daniel Firestone. 2017. VFP: A Virtual Switch Platform for Host SDN in the
Public Cloud. In NSDI. USENIX Association, Boston, MA, 315–328.
[22] Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza
Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric
Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt Humphrey, Jack
Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye, Gautham Pop-
uri, Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Madhan Sivakumar,
Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak Bansal, Doug
Burger, Kushagra Vaid, David A. Maltz, and Albert Greenberg. 2018. Azure Accel-
erated Networking: SmartNICs in the Public Cloud. In NSDI. USENIX Association,
Renton, WA, 51–66.
[23] Philippe Flajolet, Éric Fusy, Olivier Gandouet, and Frédéric Meunier. 2007. Hy-
perLogLog: the analysis of a near-optimal cardinality estimation algorithm. In
Discrete Mathematics and Theoretical Computer Science. Discrete Mathematics
and Theoretical Computer Science, 137–156.
[24] Ali Ghodsi, Vyas Sekar, Matei Zaharia, and Ion Stoica. 2012. Multi-Resource Fair
Queueing for Packet Processing. In SIGCOMM. ACM, Helsinki, Finland, 1–12.
[25] Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy Konwinski, Scott Shenker,
and Ion Stoica. 2011. Dominant Resource Fairness: Fair Allocation of Multiple
Resource Types. In NSDI. USENIX Association, Boston, MA, 323–336.
[26] Younghwan Go, Muhammad Asim Jamshed, YoungGyoun Moon, Changho
Hwang, and KyoungSoo Park. 2017. APUNet: Revitalizing GPU as Packet Pro-
cessing Accelerator. In NSDI. USENIX Association, Boston, MA, 83–96.
[27] Google. 2018. Google Compute Engine. https://cloud.google.com/compute
[28] Chuanxiong Guo, Guohan Lu, Helen J Wang, Shuang Yang, Chao Kong, Peng
Sun, Wenfei Wu, and Yongguang Zhang. 2010. SecondNet: A Data Center Net-
work Virtualization Architecture with Bandwidth Guarantees. In CoNEXT. ACM,
Philadelphia, PA, 15:1–15:12.
[29] Sangjin Han, Keon Jang, Aurojit Panda, Shoumik Palkar, Dongsu Han, and Sylvia
Ratnasamy. 2015. SoftNIC: A Software NIC to Augment Hardware. EECS Depart-
ment, University of California, Berkeley, Tech. Rep. UCB/EECS-2015-155 (2015).
[30] Chi-Yao Hong, Srikanth Kandula, Ratul Mahajan, Ming Zhang, Vijay Gill, Mohan
Nanduri, and Roger Wattenhofer. 2013. Achieving High Utilization with Software-
Driven WAN. In SIGCOMM. ACM, Hong Kong, China, 15–26.
[31] Intel. 2018.
Intel Ethernet Controller XL710 10/40 GbE.
https:
//www.intel.com/content/dam/www/public/us/en/documents/specification-
updates/xl710-10-40-controller-spec-update.pdf.
[32] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun
Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jonathan Zolla,
Urs Hölzle, Stephen Stuart, and Amin Vahdat. 2013. B4: Experience with a
Globally Deployed Software Defined WAN. In SIGCOMM. ACM, Hong Kong,
China, 3–14.
[33] Virajith Jalaparti, Hitesh Ballani, Paolo Costa, Thomas Karagiannis, and Ant
Rowstron. 2012. Bridging the Tenant-Provider Gap in Cloud Services. In SoCC.
ACM, Article 10, 10:1–10:14 pages.
[34] Keon Jang, Justine Sherry, Hitesh Ballani, and Toby Moncaster. 2015. Silo: Pre-
dictable Message Latency in the Cloud. In SIGCOMM. ACM, London, United
Kingdom, 435–448.
[35] Vimalkumar Jeyakumar, Mohammad Alizadeh, David Mazières, Balaji Prabhakar,
Albert Greenberg, and Changhoon Kim. 2013. EyeQ: Practical Network Perfor-
mance Isolation at the Edge. In NSDI. USENIX Association, Lombard, IL, 297–312.
[36] M Tim Jones. 2010. Virtio: An I/O virtualization framework for Linux. IBM White
Paper (2010).
[37] Teemu Koponen, Keith Amidon, Peter Balland, Martín Casado, Anupam Chanda,
Bryan Fulton, Igor Ganichev, Jesse Gross, Paul Ingram, Ethan Jackson, Andrew
Lambeth, Romain Lenglet, Shih-Hao Li, Amar Padmanabhan, Justin Pettit, Ben
Pfaff, Rajiv Ramanathan, Scott Shenker, Alan Shieh, Jeremy Stribling, Pankaj
Thakkar, Dan Wendlandt, Alexander Yip, and Ronghua Zhang. 2014. Network
Virtualization in Multi-tenant Datacenters. In NSDI. USENIX Association, Seattle,
WA, 203–216.
[38] Alok Kumar, Sushant Jain, Uday Naik, Anand Raghuraman, Nikhil Kasinad-
huni, Enrique Cauich Zermeno, C Stephen Gunn, Jing Ai, Björn Carlin, Mihai
Amarandei-Stavila, Mathieu Robin, Aspi Siganporia, Stephen Stuart, and Amin
Vahdat. 2015. BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Dis-
tributed Computing. In SIGCOMM. ACM, London, United Kingdom, 1–14.
[39] Vinh The Lam, Sivasankar Radhakrishnan, Rong Pan, Amin Vahdat, and George
Varghese. 2012. Netshare and Stochastic Netshare: Predictable Bandwidth Allo-
cation for Data Centers. CCR 42, 3 (June 2012), 5–11.
[40] Jason Lawley. 2014. Understanding Performance of PCI Express Systems. WP350
(v1. 2). Xilinx (2014).
[41] Jeongkeun Lee, Yoshio Turner, Myungjin Lee, Lucian Popa, Sujata Banerjee,
Joon-Myung Kang, and Puneet Sharma. 2014. Application-driven Bandwidth
Guarantees in Datacenters. In SIGCOMM. ACM, Chicago, IL, 467–478.
[42] Jacob Leverich. 2014. Mutilate: High-Performance Memcached Load Generator.
https://github.com/leverich/mutilate.
[43] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew
Putnam, Enhong Chen, and Lintao Zhang. 2017. KV-Direct: High-Performance
In-Memory Key-Value Store with Programmable NIC. In SOSP. ACM, Shanghai,
China, 137–152.
[44] Linux. 2018. NAPI (New API). https://wiki.linuxfoundation.org/networking/napi.
Online, accessed: 2019-07.
[45] Rob McGuinness and George Porter. 2018. Evaluating the Performance of Soft-
ware NICs for 100-Gb/s Datacenter Traffic Control. In ANCS. ACM, Ithaca, NY,
74–88.
[46] Memcached. 2018. Memcached key-value store. https://memcached.org/. Online,
accessed: 2019-07.
[47] Microsoft. 2018. Azure. https://azure.microsoft.com/.
[48] Radhika Mittal, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi,
Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats. 2015. TIMELY:
RTT-based Congestion Control for the Datacenter. In SIGCOMM. ACM, London,
United Kingdom, 537–550.
363
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
[49] Jeffrey C. Mogul and Lucian Popa. 2012. What We Talk About when We Talk
evaluation-2016-march.pdf. Online, accessed: 2019-07.
About Cloud Network Performance. CCR 42, 5 (Sept. 2012), 44–48.
[50] Jeffrey C. Mogul and K. K. Ramakrishnan. 1997. Eliminating Receive Livelock in
an Interrupt-driven Kernel. ACM Trans. Comput. Syst. 15, 3 (Aug. 1997), 217–252.
[51] Al Morton. 2013. IMIX Genome: Specification of Variable Packet Sizes for Addi-
[74] George Varghese and Tony Lauck. 1987. Hashed and Hierarchical Timing Wheels:
Data Structures for the Efficient Implementation of a Timer Facility. In SOSP.
ACM, Austin, Texas, USA, 25–38.
[75] Wikipedia. 2019.
Internet_Mix. https://en.wikipedia.org/wiki/Internet_Mix.
tional Testing. https://tools.ietf.org/html/rfc6985.
[52] David Mulnix. 2017.
Intel® Xeon® Processor Scalable Family Techni-
cal Overview. https://software.intel.com/en-us/articles/intel-xeon-processor-
scalable-family-technical-overview. Online, accessed: 2019-07.
[53] Rolf Neugebauer, Gianni Antichi, José Fernando Zazo, Yury Audzevich, Sergio
López-Buedo, and Andrew W Moore. 2018. Understanding PCIe performance for
end host networking. In SIGCOMM. ACM, Budapest, Hungary, 327–341.
[54] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee,
Harry C. Li, Ryan McElroy, Mike Paleczny, Daniel Peek, Paul Saab, David Stafford,
Tony Tung, and Venkateshwaran Venkataramani. 2013. Scaling Memcache at
Facebook. In NSDI. USENIX Association, Lombard, IL, 385–398.
[55] Aurojit Panda, Sangjin Han, Keon Jang, Melvin Walls, Sylvia Ratnasamy, and
Scott Shenker. 2016. NetBricks: Taking the V out of NFV. In OSDI. USENIX
Association, Savannah, GA, 203–216.
[56] Parveen Patel, Deepak Bansal, Lihua Yuan, Ashwin Murthy, Albert Green-
berg, David A. Maltz, Randy Kern, Hemant Kumar, Marios Zikos, Hongyu Wu,
Changhoon Kim, and Naveen Karri. 2013. Ananta: Cloud Scale Load Balancing.
In SIGCOMM. ACM, Hong Kong, China, 207–218.
[57] PCI-SIG. 2019. Single Root I/O Virtualization (SR-IOV). https://pcisig.com/
specifications/iov. Online, accessed: 2019-07.
[58] Luis Pedrosa, Rishabh Iyer, Arseniy Zaostrovnykh, Jonas Fietz, and Katerina
Argyraki. 2018. Automated Synthesis of Adversarial Workloads for Network
Functions. In SIGCOMM. ACM, Budapest, Hungary, 372–385.
[59] Simon Peter, Jialin Li, Irene Zhang, Dan RK Ports, Doug Woos, Arvind Krishna-
murthy, Thomas Anderson, and Timothy Roscoe. 2014. Arrakis: The Operating
System is the Control Plane. In OSDI. USENIX Association, Broomfield, CO, 1–16.
[60] Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Raja-
halme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, Keith Amidon, and
Martín Casado. 2015. The Design and Implementation of Open vSwitch. In NSDI.
USENIX Association, Oakland, CA, 117–130.
[61] Phitchaya Mangpo Phothilimthana, Ming Liu, Antoine Kaufmann, Simon Peter,
Rastislav Bodik, and Thomas Anderson. 2018. Floem: A Programming System for
NIC-Accelerated Network Applications. In OSDI. USENIX Association, Carlsbad,
CA, 663–679.
[62] Lucian Popa, Gautam Kumar, Mosharaf Chowdhury, Arvind Krishnamurthy,
Sylvia Ratnasamy, and Ion Stoica. 2012. FairCloud: Sharing the Network in Cloud
Computing. In SIGCOMM. ACM, Helsinki, Finland, 187–198.
[63] Lucian Popa, Praveen Yalagandula, Sujata Banerjee, Jeffrey C. Mogul, Yoshio
Turner, and Jose Renato Santos. 2013. ElasticSwitch: Practical Work-Conserving
Bandwidth Guarantees for Cloud Computing. In SIGCOMM. ACM, Hong Kong,
China, 351–362.
[64] Sivasankar Radhakrishnan, Yilong Geng, Vimalkumar Jeyakumar, Abdul Kabbani,
George Porter, and Amin Vahdat. 2014. SENIC: Scalable NIC for End-Host Rate
Limiting. In NSDI. USENIX Association, Seattle, WA, 475–488.
[65] Scott Rixner, William J Dally, Ujval J Kapasi, Peter Mattson, and John D Owens.
2000. Memory access scheduling. In ACM SIGARCH Computer Architecture News,
Vol. 28. ACM, 128–138.
[66] Henrique Rodrigues, Jose Renato Santos, Yoshio Turner, Paolo Soares, and Dorgi-
val O Guedes. 2011. Gatekeeper: Supporting Bandwidth Guarantees for Multi-
tenant Datacenter Networks. In WIOV. USENIX Association, Portland, OR, 784–
789.