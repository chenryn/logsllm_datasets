### 6.4 Production

To demonstrate the practicality of PicNIC, we deployed it in a large public cloud. A subset of the results is summarized here; more detailed information can be found in §C.

In §2, we discussed how rate-limiting slower flows at the egress introduces Head-of-Line (HoL) blocking delay for faster flows. The deployment of out-of-order (OOO) completions and Packet Accounting eliminated HoL blocking between flows at the egress. As a result, OOO completions alone improved the tail latency for customer traffic by approximately 13%.

When deploying Class-based Weighted Fair Queuing (CWFQ) at the ingress, we observed a 96% reduction in packet drops at NIC Rx queues. Excess packets are fairly dropped at the per-VM queues. As noted in §5, CWFQs by themselves cannot completely eliminate NIC packet drops; however, when coupled with admission control, we observe further reductions in packet drops and response times, even under DoS attacks. To handle congestion in the network fabric, we extended our prototype to incorporate Explicit Congestion Notification (ECN) signals.

### 7 Solution Space of Performance Isolation

Table 1 summarizes related prior work that focuses on sharing fabric bandwidth but not resources at end-hosts. PicNIC ensures isolation at end-hosts and complements prior work to build an end-to-end predictable virtualized network.

#### 7.1 Mechanisms Used in Practice

Isolation is typically enforced using the following approaches:

- **Guest Congestion Control**: While we have relied on flow-level congestion control, such as TCP, this is changing in the cloud environment where VMs are free to use any congestion control, including none.
- **Static Bandwidth Limits**: A common approach is to apply static egress BPS limits per-VM based on policy. Since these limits are neither defined nor enforced for ingress, they cannot ensure isolation in many cases, such as against incasts and DoS attacks.
- **Cloud Product Offerings**: Achieving predictable performance in public clouds is challenging. A survey of major cloud providers indicates a lack of concrete performance guarantees and isolation mechanisms [3, 27, 47]. Even with the highest-performance options, such as SmartNIC [22] and AWS Enhanced Networking [4], cloud providers only specify an "up to" BPS limit for each VM, similar to static bandwidth limits. None of the providers offer latency or loss-rate Service Level Objectives (SLOs).

#### 7.2 Related Work

- **Abstractions**: Recent work has focused on building abstractions that aim to provide tenants with the illusion of a dedicated network fabric [5, 7, 28, 41, 69, 76]. These abstractions range from static virtual clusters or data centers to those encoding time-varying demands and communication patterns [7, 41, 76]. In contrast, PicNIC proposes the predictable vNIC abstraction at the VM level.
- **Dynamic Bandwidth Arbitration**: For flow aggregates over WANs [30, 32], centralized systems can compute dynamic BPS allocations [38]. However, these are difficult to scale to enforce isolation at short timescales across thousands of VMs. Even proposed distributed approaches [35], which mostly share network bandwidth in the fabric, do not ensure isolation at end hosts. Enforcing BPS limits [5, 7, 8, 14, 28, 39, 41, 62, 63, 66, 69, 69, 76] does not automatically ensure end-to-end predictable latency and loss rates, which PicNIC aims to deliver. Additionally, compared to constant link capacities in the fabric, PicNIC provides predictable performance while sharing variable packet processing capacity of virtualization stacks.
- **Trade-offs**: FairCloud [62] and HUG [14] study fundamental trade-offs with respect to network utilization and minimum bandwidth guarantees. HUG also explores fairness in the context of multiple resources [24, 25], which could be applicable in this case too.

### 8 Discussion on Hardware Design

#### 8.1 Hardware is Not a Panacea

A classical approach to achieving isolation is to push the problem one level down, meaning moving functionality and contention management into hardware (HW), for instance, by using per-VM queues or Single Root I/O Virtualization (SR-IOV) [57]. However, even physical NICs have PPS limits and must share limited resources such as PCIe and DRAM bandwidth [26, 43, 53, 61]. Using SR-IOV may lead to unfair sharing of resources and performance interference [13]. PPS overloads remain a significant challenge for HW stacks as well.

Using per-VM queues is challenging in practice because they are needed at all potential points of contention. Physical resource limitations imposed by the HW may mean an insufficient number of queues in HW at every such point. Fundamentally, HW queues are "local" constructs lacking global visibility, and thus are insufficient to guarantee isolation, which requires coordination with multiple senders. For example, drops due to PCIe bandwidth exhaustion at the ingress can break isolation, similar to §2.2. For the same reason, novel data-plane approaches [9, 59] based on HW virtualization cannot provide isolation on their own for the cases mentioned in §2. Recent work in the context of RDMA has also demonstrated isolation issues in HW [77]. Finally, as functionality moves to NICs [22, 72], it is crucial to ensure predictable sharing of HW resources.

#### 8.2 PicNIC in Hardware

We briefly discuss how PicNIC's techniques can be applied in the case of HW-based stacks. Typically, the NIC or FPGA implementing the virtualization stack is connected over PCIe to the CPU. The major potential bottlenecks along the path include PCIe bandwidth and DRAM bandwidth. If one VM's traffic monopolizes these resources, it can lead to other VMs being starved unfairly, leading to similar issues as illustrated in §2. Even if such resources are shared fairly, packet drops can occur at ingress when the ingress rate for a VM exceeds its fair-share rate. This again leads to unfairness and inefficiency due to resource wastage. Thus, backpressure to the sources is needed to avoid contention at the receivers. At egress, simply rate limiting traffic can lead to drops and buffer contention in the stack when a VM discharges packets at excessive rates. This wastes resources such as PCIe and DRAM bandwidth, HW clock cycles, and SRAM buffers. Therefore, backpressure to the guest OS stack is also necessary. We find that the same design principles (§3) carry over in the case of HW too. We outline the corresponding PicNIC constructs for HW. For more details, see §A.

- **Ingress**: To avoid unfair drops and delay in shared NIC Rx queues, PicNIC can implement per-VM queues in HW. By monitoring resources, such as PCIe bandwidth, used by each VM, PicNIC can enforce SLO-based fair sharing by controlling how these queues are scheduled.
- **Congestion Control**: Both PCCB and PCCP are amenable to efficient HW implementations. While PCCB monitors the ingress BPS per VM, PCCP uses statistics from per-VM queues to compute rate limits, which are notified via feedback generated in the datapath. At the egress, these rate limits are stored in a table, which may be partitioned between SRAM and DRAM based on resource constraints.
- **Egress**: Recent work on scalable shaping [64, 71] can be leveraged to enforce these rates efficiently. As the HW has accurate information about when a packet is sent out, it can hold completion events until then to implement OOO completions.

Overall, we believe that PicNIC's design principles and constructs are well-suited for implementing the predictable virtualized NIC abstraction on HW-based stacks too. We hope that the lessons learned from PicNIC will cause performance isolation to be considered a primary objective for virtualization stacks and inform the design of future NICs.

### 9 Conclusion

Isolation is a fundamental challenge in operating systems, exacerbated by VMs and cloud platforms. Today, cloud providers face a dilemma: they must provide the illusion of an isolated virtual slice of hardware to tenants without being too wasteful of the underlying resources. This paper presents PicNIC, a system that uses a combination of localized SLO-based resource sharing and end-to-end admission control to provide the illusion of a dedicated NIC to VMs, while responding to potential isolation breakages within sub-millisecond timescales.

PicNIC opens up several interesting avenues for future research. Can we pack more VMs onto each physical host without sacrificing predictability? Where along the spectrum of isolation and efficiency should cloud providers operate? If the time for responding to isolation breakages can be further reduced to single-digit microseconds, what additional efficiencies might become possible? How can future NIC designs facilitate predictable performance? Therefore, we consider PicNIC as the first step towards predictable virtualized NICs, complementing prior work on sharing the network fabric, not the last.

### Acknowledgments

We would like to thank the anonymous SIGCOMM reviewers, Dina Papagiannaki, Jeff Mogul, and our shepherd, Manya Ghobadi, for providing valuable feedback. This work was partially supported by NSF grants CCF-1637532 and CNS-1413972 and ONR grant N00014-15-1-2177.