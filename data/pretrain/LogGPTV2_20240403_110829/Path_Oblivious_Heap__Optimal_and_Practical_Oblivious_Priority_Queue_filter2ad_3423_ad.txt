2) in a multi-party computation scenario, the Circuit-variant
results in optimal circuit size. Speciﬁcally, assuming that
each entry ﬁts in one word of size w, then the circuit size
for each priority queue request is bounded by O(w · L)
where L = O(log N + log 1
As earlier works have pointed out [23], [42], having small
CPU cache is in fact
important for having small circuit
size in multi-party computation. In a multi-party computation
scenario,
the CPU’s private state is secret shared among
multiple parties and the CPU’s computation must be done
through an expensive cryptographic protocol that models the
CPU’s private computation as a boolean or algebraic circuit;
and moreover, this circuit must take all of the CPU’s private
cache as input. For this reason, if we adopt an algorithm
with L amount of CPU cache, the circuit representing CPU
computation must have size at least L (but possibly more than
L, e.g., the Path-variant requires Θ(wL log L) circuit size).
δ ) denotes the path length.
A. Algorithm
Background on Circuit ORAM. Recall that Circuit ORAM
is an improvement of Path ORAM: Path ORAM’s eviction (see
Section III-B) requires that the CPU store and compute on an
entire path; Circuit ORAM devises a new eviction algorithm
that avoids this drawback: speciﬁcally, the eviction algorithm
now can be evaluated by a CPU with O(1) words of private
cache. At a very high level, without going into algorithmic
details, recall that Path ORAM’s eviction algorithm is the
most aggressive (i.e., greediest) one can perform on a path. By
contract, Circuit ORAM’s eviction algorithm is the greediest
but subject to making only a single data-scan over the path,
from root to leaf (and additionally, O(1) number of metadata
scans to prepare for the actual data scan).
More speciﬁcally, Circuit ORAM [42]’s eviction algorithm
in fact differentiates between two types of paths6:
• If P is not a path where a ReadNRm operation has just
taken place, then a full eviction is performed where the CPU
makes a single data scan over the entire path P from root
to leaf, and attempts to evict as greedily as possible subject
to a single data scan (as mentioned, O(1) metadata scans
are performed ﬁrst to prepare for the data scan).
• If P is a path where a ReadNRm operation has just taken
place, the eviction algorithm now performs a partial eviction
instead: in this case, eviction is performed on a segment of
P from the root to the bucket where an element has just been
removed; for the remainder of the path P, dummy eviction
is performed.
The details of Circuit ORAM’s eviction algorithm are some-
what involved and we refer the readers to the original Circuit
ORAM paper [42] for a full-ﬂedged description and various
open-source projects [1], [2] for reference implementations.
For the purpose of this paper,
the details of the eviction
algorithm are not important — the reader may treat this part
as a blackbox, and in fact even our proofs need not open this
blackbox since we reduce the probability of overﬂow to Circuit
ORAM’s probability of overﬂow.
The following fact is shown in the Circuit ORAM pa-
per [42]:
Fact 1 (Efﬁciency of Circuit ORAM’s eviction algorithm).
Given a path P containing L elements, where each element
can be encoded in C memory words, Circuit ORAM’s eviction
algorithm can be accomplished on P in O(C · L) bandwidth
consuming only O(1) words of CPU private cache.
Furthermore, the eviction algorithm operating on P can be
encoded as a boolean circuit of size O(C · w · L) where w
denotes the bit-length of each word.
The Circuit-variant. We are now ready to describe the
Circuit-variant of our Path Oblivious Heap algorithm. Es-
sentially the algorithm is identical to the one described in
Section III, the only difference being that whenever eviction
is needed on a path, we employ Circuit ORAM’s eviction
algorithm instead of Path ORAM’s eviction algorithm.
Based on Circuit-variant we can derive the following the-
orem — note that here we simply state the version with
unknown T .
6In this paper, we consider the variant of Circuit ORAM [42] with provable
stochastic bounds on the overﬂow probability. The Circuit ORAM paper in
fact suggests a simpliﬁed version recommended for implementation, where
the partial eviction on the ReadNRm path is not performed. Although so far
we do not know a formal stochastic proof for this simpliﬁed variant, empirical
evaluations show that this simpliﬁed variant has the same exponentially sharp
tail bound on the overﬂow probability as the provable variant.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
849
Theorem 7 (Circuit-variant with typical parameters). Let C, w
be deﬁned as in Corollary 6: C denotes the number of words
for storing each priority queue entry and w denotes the
bit-length for each word. For any arbitrarily small super-
constant function α(λ), there exists an algorithm that realizes
an oblivious priority queue by Deﬁnition 1 supporting an
unbounded number of requests, consuming only O(1) words
of CPU private cache, and moreover,
1) it completes each FindMin request
in O(C) run-
time and bandwidth, and each Delete, Insert, and
ExtractMin request in O(C · (log N + α(λ) log λ))
amortized runtime and bandwidth;
2) for every request, the algorithm computes on and updates
O(1) tree-paths, and the computation performed can be
represented as an O(Cw · (log N + α(λ) log λ))-sized
boolean circuit.
Proof. Using exactly the same proof of Theorem 3 in Ap-
pendix B. Speciﬁcally, the proof of Theorem 3 reduces Path
Oblivious Heap’s overﬂow probability to that of Path ORAM,
relying on Theorem 10 that was proven in the Path ORAM
paper [39]. As stated in Theorem 10, in fact the same theorem
holds for Circuit ORAM [42] too. Thus, we can identically
prove Theorem 3 for the Circuit-variant. Now, letting every
non-root bucket be of capacity 5,
letting the root bucket
be of capacity |B| = α(λ) log λ, we can conclude that the
failure probability is negligibly small in λ as long as T is
polynomially bounded in λ.
It remains to show the performance statements: observe that
FindMin examines only the root’s subtree-min label, and ev-
ery other request performs ReadNRm, Evict, and UpdateMin
on O(1) tree-paths. Clearly ReadNRm and UpdateMin takes
time at most O(CL) on a word-RAM with O(1) words of
CPU private cache where L = O(log N + α(λ) log λ) is the
path length; further, the ReadNRm and UpdateMin operations
can be represented O(C · w · L)-sized boolean circuits. The
same also holds for Evict according to Fact 1. Therefore the
stated performance analysis holds.
V. APPLICATIONS
A. Path Oblivious Sort: Practical and Optimal Oblivious Sort
We say that an algorithm Sortm(1λ,·) is a (1 − (λ))-
oblivious sort algorithm, iff there exists a simulator Sim such
that for any input array I containing m elements,
Fsort(I), Sim(m)
where Fsort is an ideal functionality that sorts the input I and
outputs the result, and ≡ means that the two distributions have
statistical distance at most .
≡ (O, addr) where (O, addr) ← Sortm(1λ, I)
Given m elements each of the form (k, v), we can obliv-
iously sort
them as follows: 1) initiate an oblivious PQ
parametrized with a security parameter λ and the space-
time parameters N = T = m; 2) insert each element
sequentially into an oblivious priority queue by calling the
Insert algorithm; and 3) call ExtractMin() a total of m
times and write down the outputs one by one.
Theorem 8 (Optimal oblivious sorting). Let D := |k| +|v| be
the number of bits needed to represent an item to be sorted;
let w be the bit-width of each memory word, and let C :=
(cid:100)(D + log N )/w(cid:101). Then, for any 0 < δ < 1 and any m, there
exists a (1−δ) oblivious sort algorithm which consumes O(1)
δ )) bandwidth
words of CPU cache and O(Cm(log m + log 1
to sort m elements.
Proof. Deferred to Appendix C-A.
B. Oblivious Streaming Sampler with Applications to Dis-
tributed Differential Privacy
A streaming sampler samples and maintains a random
subset of k entries from an incoming stream, without knowing
how long the stream is a-priori. The machine that implements
the sampler has small space, e.g., it only has O(k) space
whereas the entire stream may contain n (cid:29) k entries.
Oblivious streaming sampler. In this paper we speciﬁcally
care about an oblivious streaming sampler which is an
important building block in large-scale, federated learning
applications as we explain shortly afterwards. Below we ﬁrst
describe what an oblivious sampler aims to achieve, we then
motivate why it is an important building block in large-scale,
privacy-preserving federated learning. We shall ﬁrst introduce
the notion of an oblivious streaming sampler assuming a server
with secure processor such as Intel SGX. Then, we describe an
alternative instantiation where the secure processor is replaced
with cryptographic multi-party computation.
Imagine that the incoming data stream is encrypted to the
secure processor’s secret key, and the sampled entries will
reside in encrypted format in the server’s memory. In this way,
the adversary (e.g., the operating system on the server or a
rogue system administrator) cannot observe the contents of
the stream nor the sampled entries. The adversary, however,
can observe the memory access patterns of the sampler. We
would like to make sure that from the memory access patterns,
the adversary learns no information which entries have been
sampled and stored — this property is called obliviousness.
Of course, the secure processor can also be replaced by a
cryptographic multi-party computation protocol. For example,
there are m servers, possibly run by different organizations,
and the users secret share their data items among the m
servers, forming a secret-shared stream. The m servers will
now perform a long-running multi-party computation protocol
to implement the streaming sampler, and all sampled entries
will be secret-shared across the parties too. In this scenario,
similarly, we also would like to make sure that the access
patterns to memory do not leak any information about which
entries have been sampled.
Application in distributed differential privacy. Companies
like Google and Facebook have been pushing for privacy-
preserving federated learning. Speciﬁcally,
they collect a
stream of user data, e.g., Google Chrome’s opt-in diagnostics
feature collects information about whether users have visited
certain websites. The collected data stream will allow the
service providers to perform useful machine learning and
Authorized licensed use limited to: Auckland University of Technology. Downloaded on November 03,2020 at 00:37:42 UTC from IEEE Xplore.  Restrictions apply. 
850
statistical analysis to optimize their systems and provide better
user experience; however, the challenge is to achieve this while
protecting each individual user’s privacy. Speciﬁcally we shall
consider differential privacy [13] which has become a de facto
notion of privacy in such applications.
Until recently, two primary approaches were suggested to
achieve distributed differential privacy in such applications:
• Local differential privacy. One na¨ıve approach is a mech-
anism commonly called randomized response (an im-
proved variant of this was employed by Google Chrome
in an effort called RAPPOR [15]): basically, each user in-
dependently randomizes its own data: by adding sufﬁcient
noise to its data, each user is responsible for ensuring
differential privacy for itself. This approach enables ac-
curate analytics of simple function (e.g., linear functions
and ﬁrst-order statistics) provided that enough random-
ized samples can be collected. However, for learning
more general functions, this approach may not provide
sufﬁcient accuracy.
• Central differential privacy. This approach relies on a
trusted curator who is entrusted with the cleartext data
collected from users, the curator performs the statistical
analysis adding appropriate noise as necessary, and pub-
lishes the ﬁnal (noisy) output that is guaranteed to be
differentially private. The central approach allows us to
compute a much broader class of functions with good
accuracy, but having a trusted curator is an undesirable
and strong assumption. Although the trusted curator
can in principle be emulated by a cryptographic multi-
party computation protocol, the cryptographic overhead
is large, making it unﬁt for large-scale, high-bandwidth
scenarios such as those faced by Google and Facebook.
Recently, there has been growing interest in a new approach
that aims to achieve differential privacy in the so-called shufﬂe-
or sample-model. Several recent papers [5], [9], [10], [14],
[17], [26] show that
if the incoming user data is either
randomly shufﬂed or sampled, for a class of suitable functions,
users often can get away by adding much smaller noise to their
data than a pure local mechanism such as randomized response
(for achieving a similar degree of privacy), and in some cases
we can even get a privacy-accuracy tradeoff similar to the
central model, but without relying on a trusted curator!
Because of the beneﬁts of the shufﬂe- and sample- models,
it has been raised as an interesting practical challenge how to
implement such a shufﬂer and sampler in practice for large-
scale time-series data — this open problem is currently being
actively worked on by researchers in the space. Note that no
matter whether we adopt a sampler or shufﬂer, it is typically
important that the sampler or shufﬂer be done by a trusted
manner: speciﬁcally, which entries have been sampled or what
permutation has been applied must be kept secret for the
differential privacy analyses to follow.
In this paper, we show how to rely on an oblivious priority
queue to implement such a privacy-preserving sampler suitable
for an emerging class of differential privacy mechanisms in
the so-called sample model. Note that although the solutions
suggested require that the sampler be implemented by either
a secure processor or through cryptographic multi-party com-
putation (MPC), implementing a small and specialized task
securely (either with secure processor or MPC) is typically
much more efﬁcient than having to implement generic data
analytics tasks with secure processor or MPC.
In a concurrent and independent work [36], Sasy and
Ohrimenko also consider how to design an oblivious sampler
motivated by the same distributed differential privacy appli-
cation — however, their problem formulation is not in the
streaming setting, and they adopt a security notion where the
adversary observes only a serialized list of memory accesses
but not the CPU step in which each access is made.
1) Formal Deﬁnition: More formally, consider an ideal
functionality F k
sample, which, upon receiving a stream S,
outputs a set of k randomly sampled items from S in a random