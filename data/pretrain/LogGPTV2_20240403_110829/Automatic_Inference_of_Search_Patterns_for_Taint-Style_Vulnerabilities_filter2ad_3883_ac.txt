program slices [23], these graphs compactly encode both
argument deﬁnitions and sanitizations, albeit in a two-
level structure created speciﬁcally to easily enumerate
feasible invocations (Section IV-A).
2) Decompression and clustering. We then decompress the
deﬁnition graphs into individual invocations and cluster
these for all call sites to obtain patterns of common
argument deﬁnitions (Section IV-B).
3) Creation of sanitization overlays. Next, we extend the
generated patterns by adding potential sanitization from
the data ﬂow, that is, all conditions in the ﬂow restricting
the argument values (Section IV-C).
4) Generation of graph traversals. Finally, we express the
inferred search patterns in the form of graph traversals
suitable for efﬁcient processing using the analysis plat-
form Joern (Section IV-D).
In the following sections, we describe each of these steps
in more detail and illustrate them with examples.
A. Generation of Deﬁnition Graphs
While source code contains valuable information about how
functions are invoked, and in particular, how their arguments
are deﬁned and sanitized, this information is often spread
across several different functions and buried in unrelated code.
To effectively exploit this information, a source-sink represen-
tation is required that encodes only the deﬁnitions and sani-
tizations of arguments, while discarding all other statements.
To address this problem, we generate a graph representation
802802
for each source-sink system that can be easily calculated from
our code property graphs. This representation, referred to as
a deﬁnition graph throughout the paper, encodes all observed
combinations of argument deﬁnitions and their corresponding
sanitization. As such, deﬁnition graphs are created from a
carefully chosen subset of the nodes of the corresponding
interprocedural program slices [see 23, 60], containing only
nodes relevant for determining search patterns for taint-style
vulnerabilities. Deﬁnition graphs allow to easily enumerate
feasible argument initializations, as is possible for complete
interprocedural program slices by solving a corresponding
context-free-language reachability problem [see 44].
We construct
these graphs by ﬁrst modeling individual
functions locally, and then combining the respective graphs
to model function interaction.
1) Local Function Modeling: Within the boundaries of
a function, determining the statements affecting a call and
the variables involved can be achieved easily using program
slicing techniques. This allows us to create a hierarchical
representation that captures all deﬁnition statements involving
variables used in the call as well as all conditions that control
the execution of the call site. To illustrate the construction of
such a representation, we consider the call to the function foo
(line 5) in Figure 3 as a selected sink. Starting from this call
site, we construct the representation by passing over the code
property graph using the following rules:
• For the selected sink, we ﬁrst follow the outgoing syntax
edges to its arguments. In the example, we expand foo
to reach the arguments x, y and z.
• For these arguments and all statements deﬁning them,
we then follow connected data-ﬂow and control-
dependence edges to uncover deﬁning statements as
well as conditions that control the call site foo. For
example, the deﬁnition int z and the condition y  1
Traversal A
* = get() 
not (* > 1) 
→ 
→ 
foo(a, *)
Fig. 6: Overview of our method for inference of search patterns for vulnerabilities. Starting from a selected sink (foo), the
method automatically constructs patterns that capture sources (get()) and sanitization (b > 1) in the data ﬂow.
Function moo
Function woo
a = get()
b = 1
a = 1
b = get()
a
b
call: bar
func: bar
a
b
call: bar
*z = get()
param: x
param: y
(y < 10)
call: boo
int z
x
y
z
Function bar
call: foo
Fig. 7: Deﬁnition graph for the function foo of the running
example from Figure 3 with arguments deﬁned by moo. A
second instantiation of the graph is shown with dashed lines
for woo.
trees produces infeasible combinations of deﬁnitions as dis-
cussed in detail by Reps [44]. For instance, in our example,
this simple solution would generate the combination {int a
= get(), int b = get()}. However, this combination is
invalid as the ﬁrst deﬁnition only occurs when moo calls bar
while the second occurs when woo calls bar. Hence, these
deﬁnitions never occur in combination.
This is a classical problem of interprocedural program anal-
ysis, which can, for instance, be solved by formulating a cor-
responding context-free-language reachability problem [44].
Another solution is to simply ensure that parameter nodes of
a function are always expanded together when traversing the
graph. For example, when expanding the parameter node for
x, the node for y needs to be expanded as well. Moreover,
it needs to be ensured that both nodes are expanded with
arguments from the same call site, in our example either woo
or moo.
As solution we simply tie parameters together by modeling
the interplay of entire functions as opposed to parameters. The
deﬁnition graph implements this idea. In contrast to the trees
modeling functions locally, nodes of the deﬁnition graph are
not simply a subset of the nodes of the interprocedural code
property graph, but represent entire trees. Deﬁnition graphs
are therefore two-level structures that combine trees used to
model functions locally to express their calling relations. As
an example, Figure 7 shows the deﬁnition graph for the call
to foo in the sample code. Formally, we can deﬁne these
deﬁnition graphs as follows.
Deﬁnition 1. A deﬁnition graph G = (V, E) for a call site c
is a graph where V consists of the trees that model functions
locally for c and those trees of all of its direct and indirect
callers. For each a, b ∈ V , an edge from a to b exists in E if
the function represented by a calls that represented by b.
B. Decompression and Clustering
We now have a source-sink representation that makes the
deﬁnition of arguments and their sanitization explicit. We
thus seek to determine patterns in the deﬁnition graphs that
reﬂect common combinations of argument deﬁnitions. Given
an arbitrary set of deﬁnition graphs, for example all deﬁnition
graphs for all call sites of the function memcpy, we employ
machine learning techniques to generate clusters of similar
deﬁnition combinations along with their sanitizers, designed
to be easily translated into graph database traversals (see
Section IV-D). We construct these clusters in the following
three steps.
1) Decompression of deﬁnition graphs: While a deﬁnition
graph represents only a single sink, it possibly encodes mul-
tiple combinations of argument deﬁnitions. For example, the
deﬁnition graph in Figure 7 contains the combination {int
z, a = get(), b = 1} as well as the combination {int
z, a = 1, b = get()} in a compressed form. Fortunately,
enumerating all combinations stored in a deﬁnition graph can
be achieved using a simple recursive procedure as shown in
Algorithm 2, where [v0] denotes a list containing only the node
v0 and the operator + denotes list concatenation.
The nodes of the deﬁnition graph are trees, where each
combination of argument deﬁnitions corresponds to a subset of
these nodes that represent a call chain. Starting from the root
node r(V ), the algorithm thus simply combines the current
tree with all possible call chains, that is, all lists of trees
803803
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:25 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 2 Decompression of deﬁnition graph
1: procedure DECOMPRESS(G)
2:
3: procedure RDECOMPRESS(G := (V, E), v0)
4:
5:
6:
7:
8:
9:
10:
11:
return RDECOMPRESS(G, r(V ))
R = ∅
D ← PARENTTREES(v0)
if D = ∅ then
for d ∈ D do
return {[v0]}
for L ∈ RDECOMPRESS(G, d) do
R ← R ∪ ([v0] + L)
return R
encountered in the code base that lead to this tree. As a result
of this step, we obtain the set of all observed combinations of
argument deﬁnitions denoted by S.
2) Clustering callees and types: Callees and types with a
similar name often implement similar functionality. For exam-
ple, the functions malloc and realloc are both concerned
with allocation, while strcpy and strcat deal with copying
strings into a buffer. We want to be able to detect similar
combinations of deﬁnitions even if none of the arguments are
deﬁned using exactly the same callee or type. To achieve this,
we determine clusters of similar callees and types prior to
constructing search patterns for vulnerabilities.
In particular, we cluster the callees and types for each
independently. As we are interested in compact
argument
representations, we apply complete-linkage clustering, a tech-
nique that is known for creating compact groups of objects
and easy to calibrate [see 4]. Linkage clustering requires a
distance metric to be deﬁned over the considered objects, and
we employ the Jaro distance for this task, as it has been
speciﬁcally designed for comparing short strings [25]. The
Jaro distance quantiﬁes the similarity of two strings as a value
between 0 and 1, where a value of 1 indicates an exact match
and a value of 0 indicates absolute dissimilarity. The analyst
can control how similar strings inside a cluster need to be by
specifying a minimum similarity in terms of the Jaro distance.
We found that the clustering is relatively stable for clustering
parameters between 0.9 and 0.7 and ﬁxed the parameter to 0.8
for all of our experiments. As a result of this step, we obtain
a set C of callee and type clusters for each argument.
3) Clustering of combinations of deﬁnitions: Clustering the
decompressed combinations of argument deﬁnitions is slightly
more involved than clustering callees and types as these
combinations are complex objects rather than simple strings.
Our goal is to compare deﬁnition combinations in terms of
the deﬁnitions they attach to arguments, albeit in a way robust
to slight differences in the names of callees and types. We
achieve this by using a generalized bag-of-words model and
mapping the combinations to a vector space spanned by the
clusters calculated in the previous step [see 45].
In the following, let us assume a sink with a single argument
and let S denote the set of combinations while C is the set of
callee and type clusters. Then, for each combination s ∈ S,
we can determine the clusters Cs ⊆ C that its deﬁnitions
are contained in. We then represent each s ∈ S by a vector
in a space where each dimension is associated with one of
the clusters of C. We can achieve this by deﬁning a map
φ : S (cid:4)→ {0, 1}n where the c’th coordinate φc is given by
(cid:2)
φc(s) =
1
0
if c ∈ Cs
otherwise
and n is the total number of callee and type clusters |C|.
In the case of sinks with multiple arguments, we perform
this operation for each argument independently and simply
concatenate the resulting vectors. As an example,
let us
consider a deﬁnition combination s where the ﬁrst argument
is initialized via a call to malloc while the second is deﬁned
to be of type size_t. Then the corresponding vector has the
following form.
···
0
⎛
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎜⎜⎜⎜⎜⎜⎜⎜⎝
1···
1
0···
···
{char[52], uchar[32], . . .}
{malloc, xmalloc, . . .}
···
{size_t, ssize_t, . . .}
{int, uint32_t, . . .}
Arg. 1
(cid:9)
(cid:9)
Arg. 2
···
φ(s) (cid:4)→
Using this vectorial representation, we can now employ
linkage clustering again to obtain clusters of similar com-
binations for argument deﬁnitions. As a distance function
for the clustering, we choose the city-block distance, since
it provides an intuitive way for measuring the presence or
absence of clusters in the vectors. We use a ﬁxed value of 3 for
the clustering parameter throughout our experiments, meaning
that invocations inside a cluster may differ in up to three
entries. As a result of this step, we obtain clusters of similar
combinations, i.e., groups of similar invocations that constitute
patterns present in the code base. The size of these clusters can
be used to rank these patterns in order to prioritize inspection
of dominant patterns: larger clusters represent strong patterns
supported by many individual invocations, while small clus-
ters represent less articulate patterns, supported only by few
invocations.
C. Creation of Sanitization Rule Overlays
The clusters generated in the previous step can already be
used to generate search patterns indicating the combinations
of arguments predominant for a sink. They do not, however,
encode sanitization patterns. To achieve this, we proceed to
create overlays for argument deﬁnition models that express
typical sanitization of each argument. To this end, we exploit
the information spread across all conditions contained in any
of the deﬁnition graphs. As is the case for callee and type
matching, we seek to be robust against slight variations in the
way conditions are formulated. To this end, we make use of the
fact that each condition is represented as a syntax tree in the
code property graph. Similar to the way callees and types are
grouped, we map these trees to vectors and cluster them using
linkage clustering. In particular, we employ an explicit tree
embedding based on the neighborhood hash kernel [16, 22].
804804
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:25 UTC from IEEE Xplore.  Restrictions apply. 
h(v) = r(l(v)) ⊕
l(z)