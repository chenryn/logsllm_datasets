title:Code red worm propagation modeling and analysis
author:Cliff Changchun Zou and
Weibo Gong and
Donald F. Towsley
Code Red Worm Propagation Modeling and Analysis ∗
Cliff Changchun Zou
Dept. Electrical &
Computer Engineering
Univ. Massachusetts
Amherst, MA
Weibo Gong
Dept. Electrical &
Computer Engineering
Univ. Massachusetts
Amherst, MA
PI:EMAIL
PI:EMAIL
Don Towsley
Dept. Computer Science
Univ. Massachusetts
Amherst, MA
PI:EMAIL
ABSTRACT
The Code Red worm incident of July 2001 has stimulated
activities to model and analyze Internet worm propagation.
In this paper we provide a careful analysis of Code Red prop-
agation by accounting for two factors: one is the dynamic
countermeasures taken by ISPs and users; the other is the
slowed down worm infection rate because Code Red rampant
propagation caused congestion and troubles to some routers.
Based on the classical epidemic Kermack-Mckendrick model,
we derive a general Internet worm model called the two-
factor worm model. Simulations and numerical solutions
of the two-factor worm model match the observed data of
Code Red worm better than previous models do. This model
leads to a better understanding and prediction of the scale
and speed of Internet worm spreading.
Categories and Subject Descriptors
H.1 [Models and Principles]: Miscellaneous
General Terms
Security, Human Factors
Keywords
Internet worm modeling, epidemic model, two-factor worm
model
1.
INTRODUCTION
The easy access and wide usage of the Internet makes
In particular,
it a primary target for malicious activities.
∗
This work was supported in part by ARO contract
DAAD19-01-1-0610; by contract 2000-DT-CX-K001 from
the U.S. Department of Justice, Oﬃce of Justice Programs;
by DARPA under contract F30602-00-2-0554 and by NSF
under Grant EIA-0080119.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’02, November 18-22, 2002, Washington, DC, USA.
Copyright 2002 ACM 1-58113-612-9/02/0011 ...$5.00.
the Internet has become a powerful mechanism for propa-
gating malicious software programs. Worms, deﬁned as au-
tonomous programs that spread through computer networks
by searching, attacking, and infecting remote computers au-
tomatically, have been developed for more than 10 years
since the ﬁrst Morris worm [30]. Today, our computing in-
frastructure is more vulnerable than ever before [28]. The
Code Red worm and Nimda worm incidents of 2001 have
shown us how vulnerable our networks are and how fast
a virulent worm can spread; furthermore, Weaver presented
some design principles for worms such that they could spread
even faster [34]. In order to defend against future worms, we
need to understand various properties of worms: the prop-
agation pattern during the lifetime of worms; the impact of
patching, awareness and other human countermeasures; the
impact of network traﬃc, network topology, etc.
An accurate Internet worm model provides insight into
worm behavior. It aids in identifying the weakness in the
worm spreading chain and provides accurate prediction for
the purpose of damage assessment for a new worm threat. In
epidemiology research, there exist several deterministic and
stochastic models for virus spreading [1, 2, 3, 15]; however,
few models exist for Internet worm propagation modeling.
Kephart, White and Chess of IBM performed a series of
studies from 1991 to 1993 on viral infection based on epi-
demiology models [20, 21, 22]. Traditional epidemic models
are all homogeneous, in the sense that an infected host is
equally likely to infect any of other susceptible hosts [3, 15].
Considering the local interactions of viruses at that time,
[20, 21] extended those epidemic models onto some non-
homogeneous networks: random graph, two-dimensional lat-
tice and tree-like hierarchical graph. Though at that time
the local interaction assumption was accurate because of
sharing disks, today it’s no longer valid for worm modeling
when most worms propagate through the Internet and are
able to directly hit a target. In addition, the authors used
susceptible - infected - susceptible (SIS) model for viruses
modeling, which assumes that a cured computer can be re-
infected immediately. However, SIS model is not suitable
for modeling a single worm propagation since once an in-
fected computer is patched or cleaned, it’s more likely to
be immune to this worm. Wang et al. presented simula-
tion results of a simple virus propagation on clustered and
tree-like hierarchical networks [32]. They showed that in
certain topologies selective immunization can signiﬁcantly
slow down virus propagation [32]. However, their conclu-
sion was based on a tree-like hierarchic topology, which is
not suitable for the Internet.
138The Code Red worm incident of July 2001 has stimulated
activities to model and analyze Internet worm propagation.
Staniford et al. used the classical simple epidemic equa-
tion to model Code Red spread right after the July 19th
incident [31]. Their model matched pretty well with the
limited observed data. Heberlein presented a visual simula-
tion of Code Red worm propagation on Incident.com [17].
Moore provided some valuable observed data and a detailed
analysis of Code Red worm behavior [27]. Weaver provided
some worm design principles, which can be used to pro-
duce worms that spread even faster than the Code Red and
Nimda worms [34].
Previous work on worm modeling neglects the dynamic
eﬀect of human countermeasures on worm behavior. Wang
et al. [32] investigated the immunization defense. But they
considered only static immunization, which means that a
fraction of the hosts are immunized before the worm prop-
agates. In reality, human countermeasures are dynamic ac-
tions and play a major role in slowing down worm propa-
gation and preventing worm outbreaks. Many new viruses
and worms come out every day. Most of them, however,
die away without infecting many computers due to human
countermeasures.
Human countermeasures against a virus or worm include:
• Using anti-virus softwares or special programs to clean
infected computers.
• Patching or upgrading susceptible computers to make
them immune to the virus or worm.
• Setting up ﬁlters on ﬁrewalls or routers to ﬁlter or
block the virus or worm traﬃc.
• Disconnecting networks or computers when no eﬀec-
tive methods are available.
In the epidemic modeling area, the virus infection rate is
assumed to be constant. Previous Internet virus and worm
models (except [34]) treat the time required for an infected
host to ﬁnd a target, whether it is already infected or still
susceptible, as constant as well. In [34], the author treated
the infection rate as a random variable by considering the
unsuccessful IP scan attempts of a worm. The mean value
of the infection rate, however, is still assumed to be constant
over time. A constant infection rate is reasonable for model-
ing epidemics but may not be valid for Internet viruses and
worms.
In this paper, through analysis of the Code Red incident of
July 19th 2001, we ﬁnd that there were two factors aﬀecting
Code Red propagation: one is the dynamic countermeasures
taken by ISPs and users; the other is the slowed down worm
infection rate because the rampant propagation of Code Red
caused congestion and troubles to some routers. By account-
ing for both the dynamic aspects of human countermeasures
and the variable infection rate, we derive a more accurate
worm propagation model: the two-factor worm model. Sim-
ulation results and numerical solutions show that our model
matches well with the observed Code Red data. In partic-
ular, it explains the decrease in Code Red scan attempts
observed during the last several hours of July 19th [13, 16]
before Code Red ceased propagation — none of previous
worm models are able to explain such phenomenon. It also
shows that Code Red didn’t infect almost all susceptible on-
line computers at 19:00 UTC as concluded in [31]. Instead,
Code Red infected roughly 60% of all susceptible online com-
puters at that time.
The rest of the paper is organized as follows. Section 2
gives a brief description of the Code Red worm incident of
July 2001. In Section 3, we give a brief review of two clas-
sical epidemic models and point out several problems that
they exhibit when modeling Internet worms. In Section 4,
we describe the two factors that are unique to the Internet
worm propagation and present a new Internet worm model:
the two-factor worm model. We present Code Red simula-
tions based on the new model in Section 5. We derive a set
of diﬀerential equations describing the behavior of the two-
factor worm model in Section 6 and provide corresponding
numerical solutions. Both the simulation results and the
numerical solutions match well with the observed Code Red
data. Section 7 concludes the paper with some discussions.
2. BACKGROUND ON CODE RED WORM
On June 18th 2001 a serious Windows IIS vulnerabil-
ity was discovered [24]. After almost one month, the ﬁrst
version of Code Red worm that exploited this vulnerabil-
ity emerged on July 13th, 2001 [11]. Due to a code error
in its random number generator, it did not propagate well
[23]. The truly virulent strain of the worm (Code Red ver-
sion 2) began to spread around 10:00 UTC of July 19th
[27]. This new worm had implemented the correct random
number generator. It generated 100 threads. Each of the
ﬁrst 99 threads randomly chose one IP address and tried to
set up connection on port 80 with the target machine [11]
(If the system was an English Windows 2000 system, the
100th worm thread would deface the infected system’s web
site, otherwise the thread was used to infect other systems,
too). If the connection was successful, the worm would send
a copy of itself to the victim web server to compromise it
and continue to ﬁnd another web server. If the victim was
not a web server or the connection could not be setup, the
worm thread would randomly generate another IP address
to probe. The timeout of the Code Red connection request
was programmed to be 21 seconds [29]. Code Red can ex-
ploit only Windows 2000 with IIS server installed — it can’t
infect Windows NT because the jump address in the code is
invalid under NT [12].
Code Red worm (version 2) was programmed to uniformly
scan the IP address space. Microsoft estimated there were
6 million Windows IIS web servers at that time[19]. If we
conservatively assume that there were less than 2 million
IIS servers online on July 19th, on average each worm would
need to perform more than 2000 IP scans before it could ﬁnd
a Windows IIS server. The worm would need, on average,
more than 4000 IP scans to ﬁnd a target if the number of
Windows IIS servers online was less than 1 million. Code
Red worm continued to spread on July 19th until 0:00 UTC
July 20th, after which the worm stopped propagation by
design [4].
Three independent observed data sets are available on the
Code Red incident of July 19th. Goldsmith and Eichman
collected two types of data on two class B networks indepen-
dently [13, 16]: one is the number of Code Red worm port
80 scans during each hour, the other is the number of unique
sources that generated these scans during each hour. The
number of Code Red scan attempts from these two data sets
are plotted in Fig. 1(a) and the number of unique sources
in Fig. 1(b) as functions of time.
139x 105
Code Red scan attempts per hour on 2 Class B networks
x 104
12
Code Red scan unique sources per hour on 2 Class B networks
Dave Goldsmith
Ken Eichman
r
e
b
m
u
n
e
c
r
u
o
s
e
u
q
n
U
i
Dave Goldsmith
Ken Eichman
10
8
6
4
2
6
5
4
3
2
1
r
e
b
m
u
n
t
p
m
e
t
t
a
n
a
c
s
0
04:00
09:00
14:00
19:00
UTC hours (July 19 − 20)
00:00
04:00
0
04:00
09:00
14:00
19:00
00:00
04:00
UTC hours (July 19 − 20)
a. Code Red scan attempts
b. Code Red scan unique sources
Figure 1: Code Red scan data on two Class B networks
Since Code Red worm was programmed to choose random
IP addresses to scan, each IP address is equally likely to be
scanned by a Code Red worm. It explains why the Code
Red probes on these two Class B networks were so similar
to each other as shown in Fig. 1.
Each of the two class B networks covers only 1/65536th of
the whole IP address space; therefore, the number of unique
sources and the number of scans in Fig. 1 are only a portion
of active Code Red worms on the whole Internet at that
time. However, they correctly exhibit the pattern of Code
Red propagation because of the uniform scan of Code Red
— this is the reason why we can use the data to study Code
Red propagation.
Because each infected computer would generate 99 simul-
taneous scans (one scan per thread) [11], the number of
worm scans was bigger than the number of unique sources.
However, Fig. 1 shows that the number of unique sources
and the number of scans have the identical evolvement over
time — both of them are able to represent Code Red propa-
gation on the Internet. For example, if the number of active
Code Red infected computers on the Internet increased 10
times in one hour, both the number of unique sources and
the number of scans observed by Goldsmith and Eichman
would increase about 10 times.