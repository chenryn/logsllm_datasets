the accelerometer readings by using a high-pass filter and then
segments the vibration signals of the voice command by examin-
ing its moving variance. Next, the time-frequency representations
(i.e., spectrogram) of the voice segment are extracted and used as
the vibration domain features. Similarly, the Audio Domain Feature
Derivation denoises the microphone data and computes the spec-
trogram of the audio segment. Due to the huge sampling rate gap
between microphone and accelerometer (e.g., 8000Hz vs. 200Hz),
directly comparing the spectrograms in the two sensing domains is
nearly infeasible. Therefore, we propose the Feature Extraction and
Domain Conversion, which extracts and converts the spectrogram
in the audio domain to the low-frequency aliased representations,
which are comparable to the spectrogram in the vibration domain.
Finally, WearID performs Cross-Domain User Authentication via
examining the similarity between the spectrogram of the wearable
and the converted spectrogram of the microphone. The proposed
system exploits Spectrogram Calibration based on 2D-normalization
to further calibrate the spectrogram of the two sensors by normal-
izing their time lengths and magnitudes, which addresses the scale
mismatches. Due to the time difference of triggering the microphone
and the accelerometer, there exists an unpredictable relative time
offset between the two spectrograms. To addresses this, we propose
Cross-domain Comparison based on 2D-Serial Correlation, which
quantifies the cross-domain similarity by finding the maximum
2D-correlation coefficient between the spectrograms by sliding one
spectrogram across the other. The authentication succeeds if the
maximum 2D-correlation coefficient is over a predefined threshold.
Otherwise, it fails and rejects the voice command.
5 CAPTURING VOICE COMMANDS
THROUGH VIBRATION
5.1 Relationships and Differences between
Microphone and Accelerometer
Both microphone and accelerometer are Micro Electro Mechanical
System (MEMS) sensors. MEMS microphones exploit a pressure-
sensitive diaphragm to capture sound waves as analog signals [47],
which are amplified and fed to a Low Pass Filter (LPF) with a cutoff
frequency of half of the sampling frequency. An Analog-to-Digital
Converter (ADC) is then applied to digitize the analog signals. Dif-
ferently, MEMS accelerometers in wearable devices measure sound
MEMS MicrophoneMEMS AccelerometerLPFAmplifierADCAmplifier<4/8/22kHz ADCAD ConverterAD ConverterACSAC 2020, December 7–11, 2020, Austin, USA
Cong Shi, Yan Wang, Yingying Chen, Nitesh Saxena, and Chen Wang
(a) Amplitude of microphone
(b) Amplitude of wearable accelerometer
Figure 5: Responses of the microphone and the accelerome-
ter (Z axis) to a chirp from 0𝐻𝑧 to 22𝑘𝐻𝑧.
in terms of the vibration of the inertial mass, which is originally de-
signed to capture the device’s acceleration caused by human motion.
The hardware comparison between a microphone and an accelerom-
eter is shown in Figure 3. The accelerometer does not contain an
LPF, and thus, it can capture vibration signals approaching its sens-
ing limit, e.g., up to 4𝐾𝐻𝑧 for Invensense M6515 on LG Urbane
watch 150. Such sensing capability is sufficient for capturing human
voices, which typically ranges from 85𝐻𝑧 ∼ 255𝐻𝑧. However, the
missing LPF in accelerometer design results in distinctive frequency
sensitivity to human voices compared with the microphone. Fur-
thermore, vendors of wearable devices usually limit the sampling
rate to below 200𝐻𝑧 (e.g., 100𝐻𝑧 on Huawei watch 2 sport) to re-
duce power consumption, which causes significant signal aliasing
and make it even harder for cross-domain comparison.
5.2 Acoustic Response in Vibration Domain
Aliased Signal. The low sampling rate of wearable’s accelerometer
causing the captured speech vibrations aliased, where multiple fre-
quencies of the vibration signals mapped to a signal frequency [32].
Figure 4 compares the spectrograms of a microphone and an ac-
celerometer under a chirp sound from 0.5𝑘𝐻𝑧 ∼ 1𝑘𝐻𝑧, where the
accelerometer’s spectrogram shows a “Zigzag” curve. This validates
that a frequency in the vibration domain could correspond to mul-
tiple frequencies in the audio domain (i.e., aliased). Such aliasing
effects render a direct comparison between the speech signals in
the vibration domain and those in the audio domain almost im-
possible. Note that the aliasing effects are usually removed on the
microphone with the LPF, which is missing in the accelerometer’s
hardware design. We model the frequency relationship between
the vibration signal and the audio signal as:
𝑓𝑎𝑙𝑖𝑎𝑠 = |𝑓 − 𝑁 𝑓𝑠|, 𝑁 ∈ 𝑍,
(1)
where 𝑓𝑎𝑙𝑖𝑎𝑠, 𝑓 and 𝑓𝑠 denotes the aliasing vibration signal fre-
quency, audio signal frequency, and sampling rate of the accelerom-
eter. We discuss how to perform the cross-domain comparison with
accelerometer and microphone data in Section 6.3.
Unique Response to the Aerial Speech Vibrations. Due to
the heterogeneous sensing mechanisms and hardware design (e.g.,
LPF), accelerometer and microphone show distinctive acoustic re-
sponse to human speeches. To study unique acoustic response in
the vibration domain, we conduct an experiment by playing a chirp
using a loudspeaker and studying the response of the wearable’s
accelerometer. Specifically, we play an audio that sweeps from
Figure 6: Response of ac-
celerometer (Z axis) under
different sound pressure
levels.
7: Response of
Figure
accelerometer
axis)
under different subject-to-
wearable distances.
(Z
0𝐻𝑧 ∼ 22𝑘𝐻𝑧 by using a Logitech loudspeaker and use an wear-
able’s accelerometer (i.e., on LG Urbane W150) and a smartphone
microphone (on Nexus 6) to record the sound, where the distance
between the loudspeaker and the recording devices is 10𝑐𝑚. As
shown in Figure 5, we find that the accelerometer has response for
the sound between 400𝐻𝑧 and 3400𝐻𝑧, whereas the microphone
captures sound between 80𝐻𝑧 and 15𝑘𝐻𝑧. Compared to the mi-
crophone, the accelerometer is only sensitive to sound reside in
a lower frequency band. Furthermore, we find that even for the
same frequency, the accelerometer has unique responses in terms
of amplitude compared to that on the microphone. Such frequency
selectivity makes the audible and inaudible attacks fail to repro-
duce a user’s acoustic characteristics on the accelerometer readings,
though they may succeed in synthesizing the user’s voice on micro-
phone recordings. The distinct acoustic characteristics in vibration
domain thus add a layer of protection against the acoustic attacks,
even the state-of-the-art audio adversarial attacks [13, 35, 49].
Recording Live Human Speech Using Wearables. We con-
duct an experiment to further study the sensitivity of the wearable’s
accelerometer on live human speeches. Particularly, we use a smart-
watch (Huawei watch 2 sport) to record a voice command (i.e.,
"calendar") spoken by a human subject with the sound pressure lev-
els (SPL) of 60𝑑𝐵, 70𝑑𝐵, 80𝑑𝐵, 90𝑑𝐵, and 100𝑑𝐵, under an ambient
noise level of 37𝑑𝐵. The distance between the subject’s mouth and
the smartwatch is 10𝑐𝑚, with the smartwatch worn on his left hand.
Figure 6 shows the response of the Z-axis of the accelerometer. We
can observe that the wearable can capture speech vibrations with
SPL over 70𝑑𝐵 and the amplitude grows with the SPL. Particularly,
when the SPL of speech vibration reaches 80𝑑𝐵 (presentation-level
volume), the accelerometer can clearly reveal the speech vibrations,
with a signal-to-noise ratio of over 9.71. This means that an SPL of
80𝑑𝐵 could inject sufficient voice characteristics into the vibration
readings for cross-domain comparison. We confirm this with ex-
tensive experiments shown in Section 7. We also test the capability
of the wearable’s accelerometer on picking up voice under various
subject-to-wearable distances from 5𝑐𝑚 to 35𝑐𝑚 (with a 5𝑐𝑚 gap).
The subject speaks the same voice command (i.e., "calendar") to
the smartwatch using an average SPL of 80𝑑𝐵. We can observe in
Figure 7 that when the distance increases to 30𝑐𝑚, the response can
be barely observed (with a low SNR of 2.0). Such short response
distance of the accelerometer can facilitate WearID to shield against
many acoustic attacks.
05101520Sound frequency (KHz)-0.3-0.2-0.100.10.20.3Microphone amplitude05101520Sound frequency (KHz)-1-0.500.51Wearable amplitude60708090100Sound pressure level (dB)-0.3-0.2-0.100.10.2Accelerometer amplitude0510152025303540Distance(cm)-0.3-0.2-0.100.10.2Accelerometer amplitudeWearID
ACSAC 2020, December 7–11, 2020, Austin, USA
6.2 Data Denoising and Segmentation
The accelerometer readings collected with wearables contain sub-
stantial noises caused by human motions (e.g., walking, hand tremor).
These motions are unpredictable and can significantly distort the
speech vibration patterns in accelerometer readings. Previous work [27,
43] found that human motion-related accelerations usually have fre-
quencies lower than 20𝐻𝑧. Therefore, we adopt a high-pass Butter-
worth filter with a cut-off frequency of 20𝐻𝑧 to remove the impacts
of human motions and reveal speech vibrations for cross-domain
comparison. Figure 8(c) illustrates the accelerometer readings af-
ter our denoising. Compared with the raw accelerometer readings
shown in Figure 8(b), the denoised accelerometer readings present
more obvious patterns that are similar to the acoustic signal shown
in Figure 8(a).
Next, we calculate the moving variance of the signals in the
audio domain and determine the segment associated with human
speeches based on an empirical threshold of 0.1, which sufficiently
differentiates ambient noises and human speeches. Segmentation
on accelerometer readings is particularly challenging due to its
low sensitivity to aerial voice. Therefore, we use the segmentation
results of the microphone recordings to assist the segmentation
of the accelerometer readings. Since both data has been coarsely
synchronized, we search for the starting point of voice segment on
the accelerometer reading within a time window 𝑊𝑇 = 0.5 after
the onset of the microphone segment. The window is determined
by an empirical study on the relative time offset between the onsets
of microphone and accelerometer segments. We then determine the
ending point of the segment in the accelerometer readings based
on the length of the microphone segment.
6.3 Feature Extraction in Vibration and Audio
Domains
Time-frequency Feature Extraction. In order to derive mean-
ingful features for cross-domain comparison, we resort to time-
frequency analysis which has shown great successes in both speech
and speaker recognition tasks. Our preliminary study validates that
solely relying on time-series correlation between the accelerometer
and the microphone readings fails to effectively compare cross-
domain similarity. We demonstrate the results of time-series com-
parison in Appendix A.1. To extract time-frequency features, we
explore spectrograms that represent vibration/audio signals’ en-
ergy distribution over a range of frequencies in short time frames.
The spectrogram is derived by computing the Discrete Time Short
Time Fourier Transform (DT-STFT) representations of the acoustic
signals in vibration/audio domain with a sliding window, which is
defined as following:
𝑡+𝑁−1
𝐷𝑇 𝑆𝑇 𝐹𝑇 (𝑡, 𝑓 ) =
𝑥(𝑛)𝑤(𝑛 − 𝑡)𝑒−𝑗 𝑓 𝑛,
(2)
𝑛=𝑡
where 𝑡 and 𝑓 are the time index and frequency index of the two-
dimension spectrogram. 𝑥(𝑛) is a sample of the acoustic signal in
the sliding window, and 𝑁 is the size of the sliding window/FFT.
We empirically determine 𝑁 to be 2048 and 64 for microphone and
accelerometer data, respectively. 𝑤(𝑛) is a Hamming window with
length 𝑁 . We then compute the magnitude squared of DT-STFT rep-
resentations in at 𝑡: 𝑃𝑡 = [|𝐷𝑇 𝑆𝑇 𝐹𝑇 (𝑡, 1)|2, ..., |𝐷𝑇 𝑆𝑇 𝐹𝑇 (𝑡, 𝐹)|2],
Figure 8: Synchronization of the microphone data (8000Hz)
and accelerometer data (200Hz) at Z axis, and the calibrated
accelerometer data (i.e., with hand vibration noise removal).
6 CROSS-DOMAIN USER AUTHENTICATION
6.1 Coarse-grained Synchronization
To examine the cross-domain similarity for user authentication,
WearID needs to simultaneously capture a same voice command
from a subject using the wearable’s accelerometer and the VA de-
vice’s microphone. This requires the system to trigger the data col-
lection on both devices with a low relative time delay so as to record
the same speech. We develop two alternative synchronization ap-
proaches: WiFi Communication-based Method and Parallel Wake-
word Detection Method. WearID uses the WiFi Communication-
based Method if the wearable is equipped with a WiFi module.
The VA device detecting a wake word sends a triggering message
through WiFi to notify the wearable device for collecting vibration
signals. If a WiFi module is not equipped (e.g., activity tracker), the
wearable device can receive the triggering message via the Blue-
tooth link with a paired smartphone that connects to the VA device.
Figure 8(a) and (b) show the results of the WiFi communication-
based synchronization between a VA device (i.e., Nexus 6) and
a wearable (i.e., Huawei 2 sport) given a voice command "Alexa,
What’s on my calendar for tomorrow". We can find that the data of
the microphone and the accelerometer are roughly synchronized.
As an alternative approach, the Parallel Wake-word Detection
Method is used when the WiFi network delay is high and not
suitable for synchronization. In such situations, the wearable device
needs to recognize the wake word using its accelerator in parallel
with the VA device and triggers the data collection. In particular,
we build a machine learning model (e.g., SVM, random forest) based
on the speech characteristics in the vibration domain for detecting
wake words. Given that wearable’s accelerometers usually run in
the background for monitoring user’s activities around-the-clock,
this method would not introduce additional energy consumption
on the wearable. Our study shows that using a random forest model
can sufficiently recognize 10 wake words with 83% accuracy by
using accelerometers in a Huawei 2 sport smartwatch.
Wake Word DetectionSynchronization For Data CollectionData DenoisingACSAC 2020, December 7–11, 2020, Austin, USA
Cong Shi, Yan Wang, Yingying Chen, Nitesh Saxena, and Chen Wang
(a) Spectrogram of accelerometer
(b) Aliasing spectrogram of microphone
(a) Voice sound of words
(b) Voice sound of sentences
Figure 9: Comparison of the accelerometer spectrogram (Z
axis) with the converted microphone spectrogram ("Alexa").
Figure 10: The spectrogram correlation based on our
method.
Algorithm 1 Spectrogram-based Frequency Conversion Algorithm
2:
4:
6:
8:
10:
12:
14:
16:
18:
function Conversion(𝑆𝑚𝑖𝑐)
Input: 𝑆𝑚𝑖𝑐-original microphone spectrogram
𝜔-sampling rate of accelerometer
Output: ^𝑆𝑚𝑖𝑐-converted microphone spectrogram