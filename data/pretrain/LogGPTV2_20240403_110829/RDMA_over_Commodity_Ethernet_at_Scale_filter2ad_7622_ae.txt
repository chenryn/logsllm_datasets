faster and more advanced transport protocols and for-
ward error correction algorithms directly in commodity
hardware, hence relieving RoCEv2 from been depend-
ing on lossless network.
7. RELATED WORK
This paper focuses on how to safely deploy RoCEv2
on a large-scale. Besides RoCEv2, there are two other
RDMA technologies: Inﬁniband [6] and iWarp [30].
Inﬁniband is a complete networking protocol suite,
which has its own layer-1 to layer-7 protocols. An Inﬁni-
band network is composed of multiple Inﬁniband sub-
nets which are connected via Inﬁniband routers. Within
a subnet, servers are connected via Inﬁniband switches.
However, to the best of our knowledge, there are still no
Inﬁniband routers in production. Hence Inﬁniband does
not meet our scalability requirement. Furthermore, In-
ﬁniband is not compatible with Ethernet, which is the
dominant networking technology for data centers.
iWarp runs RDMA over TCP/IP. The TCP/IP pro-
tocol is oﬄoaded to the NIC. Since TCP guarantees
reliable delivery even if some of the packets are dropped,
iWarp does not necessarily need a lossless network. iWarp
has one advantage over RoCE in that it can be use for
inter-DC communications. But since iWarp uses TCP
for packet transmission, it faces the same issue of TCP:
long latency caused by packet drops and retransmission
timeout. As we have discussed in 6.3, we expect new
transport protocols diﬀerent from the Inﬁniband trans-
port and TCP to be introduced in the future driven by
new hardware innovations.
Deadlock is well studied in the literature and it is
well known cyclic buﬀer dependency is necessary for
deadlock [12, 18, 22, 33, 36]. Due to the speciﬁc Clos
network topology, we once thought our network should
be free from deadlock since it should be free of cyclic
buﬀer dependency. But the ‘conspiracy’ of PFC and
Ethernet packet ﬂooding has shown that deadlock can
happen in Clos networks.
TCP performance issues such as TCP incast [35, 38,
39] and long latency tail [41] have been studied exten-
sively. These solutions are still within the existing TCP
framework. They either tune the retransmission timer
(as in [35]), or control the TCP receiving window ([39]),
or tune the ECN parameter ([38]). RDMA provides
a diﬀerent approach.Compared to [41] which still uses
TCP, RDMA bypasses the OS kernel, so that the la-
tency introduced by the kernel is eliminated. Our work
shows that RDMA can be safely deployed at scale for
intra-DC communications. As we have shown in Fig-
ure 5.4, RDMA greatly reduces the high percentile la-
tency compared with TCP.
213
RDMA has been used to build systems including stor-
age, key-value stores, and distributed transaction sys-
tems [17, 26, 28, 37]. Most of these systems use In-
ﬁniband or RoCE with tens of servers. In this paper,
we have shown that we can scale RDMA to much larger
networks using RoCEv2. Hence much larger in-memory
systems can be built in the future.
8. CONCLUSION
In this paper, we have presented our practices and ex-
periences in deploying RoCEv2 safely at large-scale in
Microsoft data centers. Our practices include the intro-
ducing of DSCP-based PFC which scales RoCEv2 from
layer-2 VLAN to layer-3 IP and the step-by-step on-
boarding and deployment procedure. Our experiences
include the discoveries and resolutions of the RDMA
transport livelock, the RDMA deadlock, the NIC PFC
storm and the slow-receiver symptom. With the RDMA
management and monitoring in place, some of our highly-
reliable,
latency-sensitive services have been running
RDMA for over one and half years.
8.1 Future Work
There are several directions for our next steps. The
hop-by-hop distance for PFC is limited to 300 meters.
Hence RoCEv2 works only for servers under the same
Spine switch layer. To this end, RoCEv2 is not as
generic as TCP. We need new ideas on how to extend
RDMA for inter-DC communications.
Our measurement showed ECMP achieves only 60%
network utilization. For TCP in best-eﬀort networks,
there are MPTCP [29] and per-packet routing [10] for
better network utilization. How to make these designs
work for RDMA in the lossless network context will be
an interesting challenge.
The deadlock we have discovered in this paper re-
minds us that deadlock in data centers may be worthy
of more systematic study. Even though the up-down
routing in Clos network can prevent deadlock, designs
like F10 [23] may break the assumption by introducing
local rerouting. Many other network topologies [20] do
not even have the up-down routing property. How can
deadlocks be avoided in those designs?
Last but not least, we have shown that RDMA pro-
vides low latency and high throughput by eliminating
OS kernel packet processing overhead and by relying on
a lossless network. A lossless network, however, does
not guarantee low latency. When network congestions
occur, queues build up and PFC pause frames may still
be generated. Both queues and PFC pause frames in-
crease network latency. How to achieve low network
latency and high network throughput at the same time
for RDMA is still an open problem.
9. ACKNOWLEDGEMENTS
Firestone worked on the RDMA project at its early
stage. Yibo Zhu helped us on the RDMA transport live-
lock experiments. We were supported by many mem-
bers of Azure Networking. We have worked closely with
our partners Charlie Gu, Sorabh Hamirwasia, Madhav
Pandya, Passaree Pasarj, Veselin Petrov, Xin Qian, Jun-
hua Wang, Chenyu Yan to onboard RDMA for several
key online services in Microsoft. We received technical
support from the engineers of Arista Networks, Broad-
com, Cisco, Dell, and Mellanox. Our shepherd Nan-
dita Dukkipati and the anonymous SIGCOMM review-
ers gave us many constructive feedback and comments
which improved the content and presentation of this pa-
per. We thank them all.
10. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A
Scalable, Commodity Data Center Network
Architecture. In Proc. SIGCOMM, 2008.
[2] Mohammad Al-Fares, Sivasankar Radhakrishnan,
Barath Raghavan, Nelson Huang, and Amin
Vahdat. Hedera: Dynamic Flow Scheduling for
Data Center Networks. In NSDI, 2010.
[3] Alexey Andreyev. Introducing Data Center
Fabric, The Next-generation Facebook Data
Center Network. https:
//code.facebook.com/posts/360346274145943/,
Nov 2014.
[4] Hadoop. http://hadoop.apache.org/.
[5] Inﬁniband Trade Association. RoCEv2.
https://cw.inﬁnibandta.org/document/dl/7781,
September 2014.
[6] Inﬁniband Trade Association. InﬁniBandTM
Architecture Speciﬁcation Volume 1 Release 1.3,
March 2015.
[7] Luiz Barroso, Jeﬀrey Dean, and Urs H¨olzle. Web
Search for a Planet: The Google Cluster
Architecture. IEEE Micro, March-April 2003.
[8] Diane Bryant. Disrupting the Data Center to
Create the Digital Services Economy.
https://communities.intel.com/community/
itpeernetwork/datastack/blog/2014/06/18/
disrupting-the-data-center-to-create-the-digital-
services-economy.
[9] Brad Calder et al. Windows Azure Storage: A
Highly Available Cloud Storage Service with
Strong Consistency. In SOSP, 2011.
[10] Jiaxin Cao et al. Per-packet Load-balanced,
Low-Latency Routing for Clos-based Data Center
Networks. In ACM CoNEXT, 2013.
[11] Cisco. Cisco Nexus 3232C Switch Data Sheet.
http://www.cisco.com/c/en/us/products/
collateral/switches/nexus-3232c-switch/
datasheet-c78-734883.html.
Yan Cai contributed to the design and experimen-
tations of DSCP-based PFC. Gang Cheng and Daniel
[12] William Dally and Charles Seitz. Deadlock-Free
Message Routing in Multiprocessor
214
Interconnection Networks. IEEE trans.
Computers, C-36(5), 1987.
[13] IEEE DCB. 802.1Qaz - Quantized Congestion
Notiﬁcation.
http://www.ieee802.org/1/pages/802.1au.html.
[14] IEEE DCB. 802.1Qbb - Priority-based Flow
Control.
http://www.ieee802.org/1/pages/802.1bb.html.
Stutsman, John Ousterhout, and Mendel
Rosenblum. Fast Crash Recovery in RAMCloud.
In SOSP, 2013.
[29] Costin Raiciu, Sebastien Barre, Christopher
Pluntke, Adam Greenhalgh, Damon Wischik, and
Mark Handley. Improving Datacenter
Performance and Robustness with Multipath Tcp.
In SIGCOMM, 2011.
[15] Jeﬀrey Dean and Luiz Andr´e Barroso. The Tail at
[30] R. Recio, B. Metzler, P. Culley, J. Hilland, and
Scale. CACM, Februry 2013.
[16] Jeﬀrey Dean and Sanjay Ghemawat. MapReduce:
Simpliﬁed Data Processing on Large Clusters. In
OSDI, 2004.
[17] Aleksandar Dragojevi´c et al. No compromises:
distributed transactions with consistency,
availability, and performance. In SOSP, 2015.
[18] Jos´e Duato. A Necessary and Suﬃcient Condition
for Deadlock-Free Routing in Cut-Through and
Store-and-Forward Networks. IEEE trans Parallel
and Distributed Systems, 7(8), 1996.
[19] Albert Greenberg et al. VL2: A Scalable and
Flexible Data Center Network. In SIGCOMM,
August 2009.
[20] Chuanxiong Guo et al. BCube: a high
performance, server-centric network architecture
for modular data centers. In SIGCOMM, 2009.
[21] Chuanxiong Guo et al. Pingmesh: A Large-Scale
System for Data Center Network Latency
Measurement and Analysis. In ACM SIGCOMM,
2015.
[22] Mark Karol, S. Jamaloddin Golestani, and David
Lee. Prevention of Deadlocks and Livelocks in
Lossless Backpressured Packet Networks.
IEEE/ACM trans Networking, 11(6), Dec 2003.
[23] Vincent Liu, Daniel Halperin, Arvind
Krishnamurthy, and Thomas Anderson. F10: A
Fault-Tolerant Engineered Network. In NSDI,
2013.
[24] M. Mathis, J. Mahdavi, S. Floyd, and
A. Romanow. TCP Selective Acknowledgment
Options, 1996. IETF RFC 2018.
[25] Mellanox. ConnectX-4 EN Adapter Card
Single/Dual-Port 100 Gigabit Ethernet Adapter.
http://www.mellanox.com/page/products
dyn?product family=204&mtag=
connectx 4 en card.
[26] Christopher Mitchell, Yifeng Geng, and Jinyang
Li. Using One-Sided RDMA Reads to Build a
Fast, CPU-Eﬃcient Key-Value Store. In USENIX
ATC, 2013.
[27] Radhika Mittal et al. TIMELY: RTT-based
Congestion Control for the Datacenter. In ACM
SIGCOMM, 2015.
[28] Diego Ongaro, Stephen M. Rumble, Ryan
D. Garcia. A Remote Direct Memory Access
Protocol Speciﬁcation. IETF RFC 5040, October
2007.
[31] Arjun Singh et al. Jupiter Rising: A Decade of
Clos Topologies and Centralized Control in
Google’s Datacenter Network. In SIGCOMM,
2015.
[32] IEEE Computer Society. 802.1Q - 2014: Virtual
Bridged Local Area Networks, 2014.
[33] Brent Stephens, Alan L. Cox, Ankit Singla, John
Carter, Colin Dixon, and Wesley Felter. Practical
DCB for Improved Data Center Networks. In
Infocom, 2014.
[34] D. Thaler and C. Hopps. Multipath Issues in
Unicast and Multicast Next-Hop Selection, 2000.
IETF RFC 2991.
[35] Vijay Vasudevan et al. Safe and Eﬀective
Fine-grained TCP Retransmissions for Datacenter
Communication. In SIGCOMM, 2009.
[36] Freek Verbeek and Julien Schmaltz. A Fast and
Veriﬁed Algorithm for Proving Store-and-Forward
Networks Deadlock-Free. In Proceedings of The
19th Euromicro International Conference on
Parallel, Distributed and Network-Based
Computing (PDP’11), 2011.
[37] Xingda Wei, Jiaxin Shi, Yanzhe Chen, Rong
Chen, and Haibo Chen. Fast In-memory
Transaction Processing using RDMA and HTM.
In SOSP, 2015.
[38] Haitao Wu et al. Tuning ECN for Data Center
Networks. In ACM CoNEXT, 2012.
[39] Haitao Wu, Zhenqian Feng, Chuanxiong Guo, and
Yongguang Zhang. ICTCP: Incast Congestion
Control for TCP in Data Center Networks. In
ACM CoNEXT, 2010.
[40] Matei Zaharia et al. Resilient Distributed
Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing. In NSDI, 2012.
[41] David Zats, Tathagata Das, Prashanth Mohan,
Dhruba Borthakur, and Randy Katz. DeTail:
Reducing the Flow Completion Time Tail in
Datacenter Networks. In SIGCOMM, 2012.
[42] Yibo Zhu et al. Congestion Control for
Large-Scale RDMA deployments. In ACM
SIGCOMM, 2015.
215