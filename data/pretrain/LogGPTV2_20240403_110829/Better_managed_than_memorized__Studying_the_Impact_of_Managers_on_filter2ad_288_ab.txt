found that can help an adversary to sniff passwords.
3 Methodology
Ethical concerns: The protocols implemented in those
two stages were approved by the ethical review board1
of our university. Further, we followed the guidelines
for academic requesters outlined by MTurk workers [20].
All server-side software (i.e., a LimeSurvey installation
and a self-written server application) was self-hosted on a
maintained and hardened university server. Web access to
the server was secured with an SSL certiﬁcate issued by
the university’s computing center and all further access
was restricted to the department’s intranet and only made
available to maintainers and collaborating researchers.
Participants could leave the study at any time.
3.1 Password survey
In our survey sampling, we asked participants about their
general privacy attitude, their attitude towards passwords,
their skills and strategies for creating and managing pass-
words, as well as basic demographic questions. Those
information enable us, on the one hand, to gain a gen-
eral overview of common password creation and storage
strategies. On the other hand, those information help us
in detecting and avoiding any potential biases in the later
stages of our study. The full survey contained 31–34 ques-
tions, categorized in 6 different groups (see Appendix A).
We ﬁrst asked for their privacy attitude using the stan-
dard Westin index [42]. However, since the Westin index
has been shown to be an unreliable measure of the ac-
tual privacy-related actions of users [69], we also asked
about the participants’ attitude towards passwords (e.g.,
whether they consider passwords to be futile in protecting
their privacy).2 This should help in better understanding
if participants are actually motivated to put an effort into
creating stronger and unique passwords. We further asked
about the participants’ strategies for password creation
and management in order to get a more complete picture
of the possible origins of passwords in our dataset.
All qualitative answers (e.g., Q9 or Q22 in Appendix A)
were independently coded in a bottom-up fashion by two
researchers. The researchers achieved an initial agreement
between 95.6% (Q9) and 97.1% (Q22) and all differences
could be resolved in agreement.
Participation in the survey was open to any MTurk
worker that fulﬁlled the following criteria: the worker was
located in the US and the number of previously approved
tasks was at least 100 or at least 70% all of the tasks.
The estimated time for answering the survey was 10–15
minutes and we paid $4 for participation. In total, 505
MTurk workers participated in our survey between August
For our study of password managers’ impact on password
strength and reuse, we use data collected from paid work-
ers of Amazon’s crowd-sourcing service Mechanical Turk.
We collected the data in two different stages: 1) a survey
sampling, and 2) collection of in situ password metrics.
1https://erb.cs.uni-saarland.de/
2Other instruments, which meet the latest requirements of scale
constructions and which are often used in recent research, do not reﬂect
the actual privacy/security attitude construct, but refer more strongly
to security behavior (e.g., SeBIS [22]) or are strongly tailored to the
corporate context (e.g., HAIS-Q [50]).
206    27th USENIX Security Symposium
USENIX Association
2017 and October 2017. After discarding responses that
failed attention test questions [33], were answered too
fast to be done thoughtfully, or that were duplicates, we
ended up with 476 valid responses.
Lastly, we also asked whether the participant would be
willing to participate in a follow-up study, in which we
measure in an anonymized, privacy-protecting fashion the
strength and reuse of their passwords. Only participants
that indicated interest in the follow-up study were con-
sidered potential candidates for our Chrome plugin-based
data collection. Only 21 workers were not interested.
3.2 Chrome plugin-based data collection
To collect in situ data about passwords, including strength,
reuse, entry method, and domain, we created a Chrome
browser plugin that monitors the input to password ﬁelds
of loaded websites and then sends all collected metrics
back to our server once the user logs in to the website. We
distributed our plugin via the Google Web Store to invited
participants. The plugin was unlisted in the Store, so that
only participants to which we sent the link to the plugin
store website were able to install it. Our primary selec-
tion criterion for participant selection was that they use
Chrome as their primary browser and are not using exclu-
sively mobile devices (smartphones and tablets) to browse
the web; besides that we aimed for an unbiased sampling
from the participants pool with respect to the participants’
privacy attitude, attitude towards passwords, demograph-
ics, and usage of password managers. Between September
and October 2017, we invited 364 participants from the
survey sampling to the study, of which 174 started and
170 ﬁnished participation. We asked participants to keep
our plugin installed for at least four days. Participants
that ﬁnished the task were compensated with $20.
Our plugin collects the following metrics:
Composition: The length of the entered password as
well as the frequency of each character class.
Strength: The password strength measured in Shan-
non and NIST entropy as well as zxcvbn score. Shannon
and NIST entropy have been used in prior works [24, 66,
23] as a measure of password strength and complexity
and are collected primarily to be backward compatible
in our analysis with prior research. However, since en-
tropy has been shown to be a poor measurement of the
actual "crackability" of the password [67], we use the
zxcvbn [68] score as the more realistic estimator of the
password strength in our analysis.3 Zxcvbn estimates
every password’s strength on a scale from 0 (weakest)
to 4 (strongest) using pattern matching (e.g., repeats,
sequences, keyboard patterns), common password dic-
tionaries (including leaked passwords, names, English
dictionary words), and mangling rules (e.g., leetify). Ap-
pendix B explains the meaning of this score in more detail.
In our plugin we used the zxcvbn library [3] with its de-
fault settings. From a statistical point of view, a metrically
scaled strength measurement instead of the ordinal zxcvbn
score would have helped in ﬁnding possible effects on
password strength easier (see Section 4), however, it does
not affect the presence of possible effects per se.
Website category: The category of the website do-
main according to the Alexa Web Information Service [2].
Our plugin contains the category for the top 28,651 web
domains at the time the study was conducted.4
Entry method: The method through which the pass-
word was entered, such as human, Chrome auto-ﬁll,
copy&paste, 3rd party password manager plugin, or ex-
ternal password manager program. The detection of the
entry method is described separately in Section 3.2.1.
In situ questionnaire: Participant’s answers to a short
questionnaire about the entered password and website (see
Section 3.2.2). In particular, we ask about the website’s
value for their privacy. Other studies used the website
category as a proxy for this value [51] and in our study
we wanted ﬁrst-hand knowledge (see also Appendix C).
Hashes: Adapting the methodology of [51, 66], we
collect the hash of the entered password as well as the
hash of every 4-character sub-string of the password. We
use a keyed hash (i.e., PBKDF2 with SHA-256), where
the key is generated and stored at the client side and never
revealed to us. This allows identiﬁcation of (partially)
reused passwords per participant. We use the notions
introduced in [51]: Exactly reused passwords are iden-
tical with another password, partially reused passwords
share a sub-string with another password, and partially-
and-exactly reused passwords have both of those charac-
teristics. Like related work [51, 66], we cannot compare
passwords across participants.
3.2.1 Detecting the entry method
Detection of the password entry method follows the de-
cision tree depicted in Figure 2. If our plugin detects
any kind of typing inside the password ﬁeld ((A)=Y) and
the typing speed is too fast to come from a human typist
((B)=N), we conclude that an external password manager
program (such as KeePass) mimics a human typist by "re-
playing" the keyboard inputs of the password. Otherwise
((B)=Y), we assume a manually entered password. As
threshold between human and external program, we set
an average key press time of 30 ms. This is based on the
observation that external programs usually do not con-
sider mimicking the key press time, while some of them
enter the password character-wise with varying speeds.
3Unfortunately, the fully trained neural network based strength esti-
mator of [51, 45] was not publicly available.
4This is the number of web domains in the top 100K list, for which
a category was assigned by Alexa.
USENIX Association
27th USENIX Security Symposium    207
Figure 2: Decision tree to detect password entry methods
In case there was no typing detected ((A)=N) and a paste
event was observed ((C)=Y), we consider the password
to be pasted by either a human or an external program.
In either case, the password is managed externally to the
browser in digital form. If no paste event was detected
((C)=N) and the Chrome auto-ﬁll event was observed,
this indicates that Chrome ﬁlled the password ﬁeld from
its built-in password manager. If Chrome auto-ﬁll has not
ﬁlled the password ﬁeld ((D)=N), our plugin checks the
list of installed plugins for eight well-known password
manager plugins (see Appendix D) and reports the ones
installed in the participant’s browser, or an "unknown"
value in case none of those eight was found.
We make the assumption that the user does not enter the
password with a mixture of the different entry methods
(e.g., pasting a word and complementing it with typing).
Such mixture of entry methods would result in misclassi-
ﬁcation of the detected method. However, we assume that
such behavior is too rare to affect our results signiﬁcantly.
3.2.2 Participant instructions
We provided our participants with a project website that
gave a step-by-step introduction on how to install our
plugin, set it up, use it, and remove it post-participation.
Google Web Store provided our participants with a very
comfortable way of adding the plugin to their browser. To
set the plugin up, participants had to simply enter their
MTurk worker ID into the plugin. The worker ID was
used as a pseudonym throughout this study to identify
data of the same participant. After setup, the plugin starts
monitoring the users’ password entries. For every newly
detected domain to which a password was submitted, our
plugin asked the participant to answer a short three ques-
tion questionnaire about the participants’ estimate of the
website’s value, the participants’ strength estimate of the
just entered password, and whether the login was success-
ful (see Figure 3). Every participant was instructed to use
the plugin for four days, after which the plugin released
a completion code to be entered into the task on MTurk
Figure 3: In situ questionnaire upon login to a new website.
to ﬁnish participation and collect the payment. Through
our server logs and the Google Web Store Developer
Dashboard we conﬁrmed that all participants removed
our plugin shortly after ﬁnishing participation. We also
instructed participants to act naturally and not change
their usual behavior during those four days in order to
maximize the ecological validity of our study. The only
exceptions from the usual behavior were the installation
of our plugin and a request to re-login to all websites
where they have an account in order to ensure a sufﬁcient
enough quantity of collected data.
3.2.3 Addressing privacy concerns
A particular consideration of our study design was the
potential privacy concerns of our participants. Since we
essentially ask our participants to install a key-logger
that monitors some of the most privacy-sensitive data,
this might repel participants from participating. Due to
the lack of in-person interviews or consultation between
the researchers and the participants, we tried to address
those concerns through a high level of transparency, sup-
port, and collecting only the minimal amount of data
in a privacy-protecting fashion, which also follows the
guidelines for academic requesters [20].
First, we explained on our project website the moti-
vation behind our study and why acting naturally is im-
portant for our results. In this context, we provided a
complete list of all data that our plugin collects, for which
purpose, and why this data collection does not enable us
to steal the participants’ passwords. We also answered all
participants’ questions in this regard that were sent to us
via email or posted in known MTurk review/discussion fo-
rums. We received feedback from workers that this level
of openness has convinced them to participate in the study.
Second, we distributed our plugin in an authenticated way
via the Google Web Store and did not obfuscate the plu-
gin’s code. Third, we limited the extent of the collected
208    27th USENIX Security Symposium
USENIX Association
(A)Typing detected?(B)Typing speed human?(C)Paste event?(D)Chrome autoﬁll?HumanExternal password manager programCopy & pasteChrome built-in password managerKnown, installed password manager pluginsYNYNYNYNdata to the necessary minimum while still being able to
study password managers’ impact. For instance, we only
collect the ﬁrst successful login to any website, thus ab-
staining from monitoring participants’ browsing behavior.
Fourth, every participant could inspect the collected data
per domain prior to sending them to us and chose to skip
the data collection for highly sensitive websites.
4 Studying Password Managers’ Impact
In this section, we analyze our collected data, but leave the
discussion of our results for Section 5. After presenting
our participants’ demographics and an overview of their
password reuse and strength, we group our participants
based on their creation strategy and study the impact of
different password management and creation strategies.
4.1 Demographics
Table 1 provides an overview of the demographics of our
participants that answered our survey, that we invited to
the plugin-based study, and that participated in the plugin-
based data collection. We invited participants in equal
parts from every demographic group and every demo-
graphic group participated in almost equal parts in the
plugin-based data collection. We use a Mann-Whitney
rank test [25] to test for signiﬁcant differences between
the demographic distributions of the 476 participants in
the survey sampling and the 170 participants in the plugin-
based study, and could not ﬁnd any statistically signiﬁ-
cant (p< .05) differences between those two groups. In
general, our participants’ demographics are closer to the
commonly observed demographics of qualitative studies
in university settings than to the demographics of the
2010 US census [64]. Our participant number is skewed
towards male participants (57.6% identiﬁed themselves
as male). Also, our participants covered an age range
from 18 to more than 70 years, where our sample skews
to younger participants (75.2% of our study participants
are younger than 40) as can be commonly observed in be-
havioral research, including password studies and usable
security. The majority of our participants had no computer
science background (80.88%) and was English speaking
(98.3%). Most of the participants identiﬁed themselves
as of white/Caucasian ethnicity (74.6%). The participants
also covered a range of educational levels, where a Bach-
elor’s degree was the most common degree (36.6% of all
participants). Further, 80.9% of our participants reported
using Chrome as their primary browser (see Table 2).
Since our study effectively asks participants to install
a password-logger, we were concerned with a potential
opt-in bias towards people that have low privacy concerns
or consider passwords as ineffective security measures.
To this end, we included the three questions of Westin’s
Number of participants
Gender
Female
Male
Other
No answer
Age group
18–30
31–40
41–50
51–60
61–70
≥71
Computer science background
Yes
No
Native language
English
Other
Education level
Less than high school
High school graduate
Some college, no degree
Associate’s degree
Bachelor degree
Ph.D
Graduate/prof. degree
Other
Ethnicity
White/Caucasian
Black/African American
Asian
Hispanic/Latino
Native American/Alaska
Multiracial
Other
Survey
476
Invited to study
364
Participated
170
200
274
1
1
180
178
71
35