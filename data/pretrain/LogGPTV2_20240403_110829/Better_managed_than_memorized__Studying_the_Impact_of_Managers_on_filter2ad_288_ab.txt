### 3. Methodology

#### 3.1 Ethical Considerations
The protocols implemented in both stages of the study were approved by the ethical review board of our university. Additionally, we adhered to the guidelines for academic requesters outlined by MTurk workers [20]. All server-side software, including a LimeSurvey installation and a self-written server application, was self-hosted on a maintained and hardened university server. Web access to the server was secured with an SSL certificate issued by the university’s computing center, and all further access was restricted to the department’s intranet, available only to maintainers and collaborating researchers. Participants had the option to leave the study at any time.

#### 3.2 Password Survey
In the survey, participants were asked about their general privacy attitudes, their attitudes towards passwords, their skills and strategies for creating and managing passwords, and basic demographic information. This data allowed us to gain a comprehensive overview of common password creation and storage strategies and to detect and avoid potential biases in later stages of the study. The full survey consisted of 31-34 questions, categorized into six different groups (see Appendix A).

We used the standard Westin index [42] to assess participants' privacy attitudes. However, given that the Westin index has been shown to be an unreliable measure of actual privacy-related actions [69], we also included questions about participants' attitudes towards passwords (e.g., whether they consider passwords futile in protecting their privacy). This helped us better understand if participants are motivated to create stronger and unique passwords. We also inquired about participants' strategies for password creation and management to get a more complete picture of the possible origins of passwords in our dataset.

Qualitative answers (e.g., Q9 or Q22 in Appendix A) were independently coded in a bottom-up fashion by two researchers. Initial agreement between the researchers ranged from 95.6% (Q9) to 97.1% (Q22), and all differences were resolved through discussion.

Participation in the survey was open to any MTurk worker who met the following criteria: located in the US and having at least 100 previously approved tasks or a 70% approval rate. The estimated time for completing the survey was 10-15 minutes, and participants were compensated $4. In total, 505 MTurk workers participated in the survey between August and October 2017. After discarding responses that failed attention test questions [33], were answered too quickly, or were duplicates, we ended up with 476 valid responses.

Lastly, we asked participants if they would be willing to participate in a follow-up study, where we would measure the strength and reuse of their passwords in an anonymized, privacy-protecting manner. Only participants who expressed interest in the follow-up study were considered potential candidates for our Chrome plugin-based data collection. Only 21 workers were not interested.

#### 3.3 Chrome Plugin-Based Data Collection
To collect in situ data about passwords, including strength, reuse, entry method, and domain, we developed a Chrome browser plugin. The plugin monitors the input to password fields of loaded websites and sends all collected metrics back to our server once the user logs in to the website. We distributed the plugin via the Google Web Store to invited participants. The plugin was unlisted, so only participants who received the link could install it.

Our primary selection criterion for participant selection was that they use Chrome as their primary browser and do not exclusively use mobile devices (smartphones and tablets) to browse the web. We aimed for an unbiased sampling from the participants pool with respect to privacy attitude, attitude towards passwords, demographics, and usage of password managers. Between September and October 2017, we invited 364 participants from the survey to the study, of which 174 started and 170 completed participation. Participants were asked to keep the plugin installed for at least four days, and those who finished the task were compensated with $20.

The plugin collects the following metrics:
- **Composition**: Length of the entered password and frequency of each character class.
- **Strength**: Password strength measured in Shannon and NIST entropy, as well as zxcvbn score. While Shannon and NIST entropy have been used in prior works [24, 66, 23] as measures of password strength and complexity, we use the zxcvbn [68] score as a more realistic estimator of password strength. Zxcvbn estimates every password’s strength on a scale from 0 (weakest) to 4 (strongest) using pattern matching, common password dictionaries, and mangling rules (see Appendix B for more details).
- **Website Category**: Category of the website domain according to the Alexa Web Information Service [2]. Our plugin includes the category for the top 28,651 web domains at the time of the study.
- **Entry Method**: Method through which the password was entered (e.g., human, Chrome auto-fill, copy&paste, 3rd party password manager plugin, external password manager program). The detection of the entry method is described in Section 3.3.1.
- **In Situ Questionnaire**: Participant’s answers to a short questionnaire about the entered password and website (see Section 3.3.2).
- **Hashes**: We collect the hash of the entered password and the hash of every 4-character sub-string of the password using a keyed hash (PBKDF2 with SHA-256). This allows identification of (partially) reused passwords per participant (see Appendix C for more details).

##### 3.3.1 Detecting the Entry Method
The detection of the password entry method follows the decision tree depicted in Figure 2. If the plugin detects typing inside the password field and the typing speed is too fast to come from a human typist, we conclude that an external password manager program (such as KeePass) is mimicking a human typist. Otherwise, we assume a manually entered password. The threshold for human typing is set at an average key press time of 30 ms. If no typing is detected but a paste event is observed, we consider the password to be pasted by either a human or an external program. If no paste event is detected and a Chrome auto-fill event is observed, this indicates that Chrome filled the password field from its built-in password manager. If Chrome auto-fill did not fill the password field, our plugin checks for eight well-known password manager plugins and reports the ones installed in the participant’s browser, or an "unknown" value if none are found.

We assume that users do not enter passwords using a mixture of different entry methods, as such behavior would result in misclassification. However, we believe this behavior is rare and does not significantly affect our results.

##### 3.3.2 Participant Instructions
Participants were provided with a project website that gave a step-by-step introduction on how to install, set up, use, and remove the plugin. To set up the plugin, participants entered their MTurk worker ID, which was used as a pseudonym throughout the study. After setup, the plugin began monitoring the users’ password entries. For every newly detected domain, the plugin asked the participant to answer a short three-question questionnaire about the website’s value, the strength of the just-entered password, and whether the login was successful (see Figure 3). Participants were instructed to use the plugin for four days, after which they received a completion code to enter into the task on MTurk to finish participation and collect the payment. We confirmed that all participants removed the plugin shortly after finishing participation. Participants were also instructed to act naturally and not change their usual behavior during the four days to maximize the ecological validity of the study.

##### 3.3.3 Addressing Privacy Concerns
A key consideration in our study design was addressing potential privacy concerns of participants. Since we essentially asked participants to install a key-logger that monitors some of the most privacy-sensitive data, we took several steps to address these concerns:
- **Transparency**: We explained the motivation behind the study and why acting naturally is important for our results. We provided a complete list of all data collected, the purpose of the data collection, and why it does not enable us to steal participants’ passwords. We also answered all participants’ questions via email or in known MTurk forums.
- **Authenticated Distribution**: The plugin was distributed via the Google Web Store, and the code was not obfuscated.
- **Minimal Data Collection**: We limited the extent of the collected data to the necessary minimum while still being able to study the impact of password managers. For example, we only collected the first successful login to any website.
- **Data Inspection**: Participants could inspect the collected data per domain before sending it to us and choose to skip data collection for highly sensitive websites.

### 4. Studying Password Managers’ Impact
In this section, we analyze the collected data, leaving the discussion of our results for Section 5. After presenting our participants’ demographics and an overview of their password reuse and strength, we group participants based on their creation strategy and study the impact of different password management and creation strategies.

#### 4.1 Demographics
Table 1 provides an overview of the demographics of our participants who answered the survey, were invited to the plugin-based study, and participated in the plugin-based data collection. We invited participants in equal parts from every demographic group, and almost equal parts of each demographic group participated in the plugin-based data collection. We used a Mann-Whitney rank test [25] to test for significant differences between the demographic distributions of the 476 participants in the survey and the 170 participants in the plugin-based study, and found no statistically significant (p < .05) differences between the two groups.

In general, our participants’ demographics are closer to those commonly observed in qualitative studies in university settings than to the demographics of the 2010 US census [64]. Our sample is skewed towards male participants (57.6% identified themselves as male) and younger participants (75.2% are younger than 40). Most participants had no computer science background (80.88%) and were English-speaking (98.3%). The majority identified as white/Caucasian (74.6%), and a Bachelor’s degree was the most common educational level (36.6% of all participants). Additionally, 80.9% of our participants reported using Chrome as their primary browser (see Table 2).

Since our study effectively asks participants to install a password-logger, we were concerned about a potential opt-in bias towards people with low privacy concerns or who consider passwords ineffective. To address this, we included the three questions of Westin’s index in the survey to assess participants' privacy attitudes.