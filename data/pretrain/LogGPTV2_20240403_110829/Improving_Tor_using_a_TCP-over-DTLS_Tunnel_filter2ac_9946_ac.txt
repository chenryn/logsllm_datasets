discussed in section 3.4, but it serves our purposes now
to conceptualize the two nodes as transporting full TCP
packets as the UDP datagram’s payload. Upon receiving
a UDP datagram, the kernel will remove the UDP header
and provide Tor with the enclosed DTLS packet; Tor will
decrypt the DTLS payload and present the result (a TCP
packet) to its user-level TCP stack. Similarly, when the
user-level TCP stack presents a packet for transmission,
the node will encrypt it with DTLS and forward the re-
sulting packet to the kernel which then sends it to the
intended destination over UDP. The stack also performs
retransmission and acknowledgement of TCP data that
are integral to TCP’s reliability; these are protected with
DTLS and forwarded over UDP in the same manner.
A user-level TCP stack provides an implementation of
the suite of socket function calls, such as socket(), send(),
and recv(). These reimplementations exist in harmony
with the proper set of operating system commands, al-
lowing both a user-level and kernel-level network layer.
Thus, data structures and ﬁle descriptors created by calls
to the user-level stack are visible and relevant only to the
parent process; the operating system manages its sock-
ets separately. The user-level stack responds to socket
calls by generating packets internally for dispatching as
dictated by TCP.
It may seem cumbersome to include an entire TCP im-
plementation as a core component of Tor. In particular,
patching the kernel’s implementation of TCP to support
our features would take signiﬁcantly less effort. How-
ever, Tor relies on volunteers to route trafﬁc; complicated
installation procedures are an immediate roadblock to-
wards the ubiquitous use of Tor. The diverse operating
systems Tor aims to support and the diverse skill level of
its users prevent its installation from requiring external
procedures, or even superuser privileges.
Daytona [11] is a user-level TCP stack that we chose
for our purposes. It was created by researchers study-
ing network analysis, and consists of the implementation
of Linux’s TCP stack and the reimplementations of user-
level socket functions. It uses libpcap to capture pack-
ets straight from the Ethernet device and a raw socket to
write generated packets, including headers, onto the net-
work. Daytona was designed to operate over actual net-
works while still giving user-level access to the network
implementation. In particular, it allowed the researchers
to tune the implementation while performing intrusive
measurements. A caveat—there are licensing issues for
Daytona’s use in Tor. As a result, the deployment of this
transport layer into the real Tor network may use a dif-
ferent user-level TCP stack. Our design uses Daytona
as a replaceable component and its selection as a user-
level TCP stack was out of availability for our proof-of-
concept.
3.4 UTCP: Our Tor-Daytona Interface
Our requirements for a user-level TCP stack are to cre-
ate properly formatted packets, including TCP retrans-
missions, and to sort incoming TCP/IP packets into data
streams: a black box that converts between streams and
packets. For our purpose, all notions of routing, Eth-
ernet devices, and interactions with a live network are
unnecessary. To access the receiving and transmitting of
packets, we commandeer the rx() (receive) and tx()
(transmit) methods of Daytona to instead interface di-
rectly with reading and writing to connections in Tor.
UTCP is an abstraction layer for the Daytona TCP
stack used as an interface for the stack by Tor. Each
UDP connection between ORs has a UTCP-connection
object that maintains information needed by our stack,
such as the set of circuits between those peers and the
socket that listens for new connections. Each circuit has
a UTCP-circuit object for similar purposes, such as the
local and remote port numbers that we have assigned for
this connection.
As mentioned earlier, only part of the TCP header is
transmitted using Tor—we call this header the TORTP
header; we do this simply to optimize network trafﬁc.
The source and destination addresses and ports are re-
placed with a numerical identiﬁer that uniquely identiﬁes
the circuit for the connection. Since a UDP/IP header
is transmitted over the actual network, Tor is capable of
performing a connection lookup based on the address of
the packet sender. With the appropriate connection, and
a circuit identiﬁer, the interface to Daytona is capable
of translating the TORTP header into the corresponding
TCP header.
When the UTCP interface receives a new packet, it
uses local data and the TORTP headers to create the cor-
responding TCP header. The resulting packet is then in-
jected into the TCP stack. When Daytona’s TCP stack
emits a new packet, a generic tx() method is invoked,
passing only the packet and its length. We look up
the corresponding UTCP circuit using the addresses and
ports of the emitted TCP header, and translate the TCP
header to our TORTP header and copy the TCP pay-
load. This prepared TORTP packet is then sent to Tor,
along with a reference to the appropriate circuit, and Tor
sends the packet to the destination OR over the appropri-
ate DTLS connection.
3.5 Congestion Control
The congestion control properties of the new scheme
will inherit directly from those of TCP, since TCP is the
protocol being used internally. While it is considered
an abuse of TCP’s congestion control to open multiple
streams between two peers simply to send more data,
in this case we are legitimately opening one stream for
each circuit carrying independent data. When packets
are dropped, causing congestion control to activate, it
will only apply to the single stream whose packet was
dropped. Congestion control variables are not shared
between circuits; we discuss the possibility of using
the message-oriented Stream Control Transport Protocol
(SCTP), which shares congestion control information, in
Section 5.2.
If a packet is dropped between two ORs communicat-
ing with multiple streams of varying bandwidth, then the
drop will be randomly distributed over all circuits with
a probability proportional to their volume of trafﬁc over
the link. High-bandwidth streams will see their packets
dropped more often and so will back off appropriately.
Multiple streams will back off in turn until the conges-
tion is resolved. Streams such as ssh connections that
send data interactively will always be allowed to have at
least one packet in ﬂight regardless of the congestion on
other circuits.
Another interesting beneﬁt of this design is that it
gives Tor direct access to TCP parameters at runtime.
The lack of sophistication in Tor’s own congestion con-
trol mechanism is partially attributable to the lack of di-
rect access to networking parameters at the kernel level.
With the TCP stack in user space Tor’s congestion con-
trol can be further tuned and optimized.
In particular,
end-to-end congestion control could be gained by ex-
tending our work to have each node propagate its TCP
rate backwards along the circuit: each node’s rate will
be the minimum of TCP’s desired rate and the value re-
ported by the subsequent node. This will address conges-
tion imbalance issues where high-bandwidth connections
send trafﬁc faster than it can be dispatched at the next
node, resulting in data being buffered upon arrival. When
TCP rates are propagated backwards, then the bandwidth
between two ORs will be prioritized for data whose next
hop has the ability to immediately send the data. Cur-
rently there is no consideration for available bandwidth
further along the circuit when selecting data to send.
4 Experimental Results
In this section we perform experiments to compare the
existing Tor transport layer with an implementation of
our proposed TCP-over-DTLS transport. We begin by
timing the new sections of code to ensure that we have
not signiﬁcantly increased the computational latency.
Then we perform experiments on a local Tor network
of routers, determining that our transport has indeed ad-
dressed the cross-circuit interference issues previously
discussed.
4.1 Timing Analysis
Our UDP implementation expands the datapath of Tor
by adding new methods for managing user-level TCP
streams and UDP connections. We proﬁle our modiﬁed
Tor and perform static timing analysis to ensure that our
new methods do not degrade the datapath unnecessarily.
Experiment 2 was performed to proﬁle our new version
of Tor.
The eightieth percentile of measurements for Experi-
ment 2 are given in Table 3. Our results indicate that no
Experiment 2 Timing analysis of our modiﬁed TCP-
over-DTLS datapath.
1: TCP-over-DTLS Tor was modiﬁed to time the dura-
tion of the aspects of the datapath:
• injection of a new packet (DTLS decryption,
preprocessing, injecting into TCP stack, possi-
bly sending an acknowledgment),
• emission of a new packet (header translation,
DTLS encryption, sending packet),
• the TCP timer function (increments counters
and checks for work such as retransmissions
and sending delayed acknowledgements), and
• the entire datapath from reading a packet on a
UDP socket, demultiplexing the result, inject-
ing the packet, reading the stream, processing
the cell, writing the result, and transmitting the
generated packet.
2: The local Tor network was conﬁgured to use 50 ms
of latency between connections.
3: A client connected through Tor to request a data
stream.
4: Data travelled through the network for several min-
utes.
new datapath component results in a signiﬁcant source of
computational latency.
We have increased the datapath latency to an expected
value of 250 microseconds per OR, or 1.5 milliseconds
for a round trip along a circuit of length three. This is
still an order of magnitude briefer than the round-trip
times between ORs on a circuit (assuming geopolitically
diverse circuit selection). Assuming each packet is 512
bytes (the size of a cell—a conservative estimate as our
experiments have packets that carry full dataframes), we
have an upper bound on throughput of 4000 cells per sec-
ond or 2 MB/s. While this is a reasonable speed that will
likely not form a bottleneck, Tor ORs that are willing
to devote more than 2 MB/s of bandwidth may require
better hardware than the Thinkpad R60 used in our ex-
periments.
4.2 Basic Throughput
We perform Experiment 3 to compare the basic through-
put and latency of our modiﬁcation to Tor, the results of
which are shown in Table 4. We can see that the UDP
version of Tor has noticeably lower throughput. Origi-
nally it was much lower, and increasing the throughput
up to this value took TCP tuning and debugging the user-
level TCP stack. In particular, errors were uncovered in
Daytona’s congestion control implementation, and it is
Datapath Component Duration
Injecting Packet
Transmitting Packet
TCP Timer
Datapath
100 microseconds
100 microseconds
85 microseconds
250 microseconds
Table 3: Time durations for new datapath components. The results provided are the 80th percentile measurement.
Conﬁguration
TCP Tor
TCP-over-DTLS Tor
Network
Throughput
176 ± 24.9 KB/s
111 ± 10.4 KB/s
Circuit
Delay
1026 ± 418 ms
273 ± 31 ms
Base
Delay
281 ± 12 ms
260 ± 1 ms
Table 4: Throughput and delay for different reordering conﬁgurations. The conﬁguration column shows which row
correspond to which version of Tor we used for our ORs in the experiment. Network throughput is the average data
transfer rate we achieved in our experiment. Circuit delay is the latency of the circuit while the large bulk data transfer
was occurring, whereas the base delay is the latency of the circuit taken in the absence of any other trafﬁc.
suspected that more bugs remain to account for this dis-
parity. While there may be slight degradation in perfor-
mance when executing TCP operations in user space in-
stead of kernel space, both implementations of TCP are
based on the same Linux TCP implementation operat-
ing over in the same network conditions, so we would
expect comparable throughputs as a result. With more
effort to resolve outstanding bugs, or the integration of
a user-level TCP stack better optimized for Tor’s needs,
we expect the disparity in throughputs will vanish. We
discuss this further in the future work section.
More important is that the circuit delay for a sec-
ond stream over the same circuit indicates that our UDP
version of Tor vastly improves latency in the presence
of a high-bandwidth circuit. When one stream triggers
the congestion control mechanism, it does not cause the
low-bandwidth client to suffer great latency as a con-
sequence.
In fact, the latency observed for TCP-over-
DTLS is largely attributable to the base latency imposed
on connections by our experimental setup. TCP Tor, in
contrast, shows a three-and-a-half fold increase in la-
tency when the circuit that it multiplexes with the bulk
stream is burdened with trafﬁc.
The disparity in latency for the TCP version means
that information is leaked: the link between the last two
nodes is witnessing bulk transfer. This can be used as
a reconnaissance technique; an entry node, witnessing a
bulk transfer from an client and knowing its next hop,
can probe potential exit nodes with small data requests
to learn congestion information. Tor rotates its circuits
every ten minutes. Suppose the entry node notices a bulk
transfer when it begins, and probes various ORs to deter-
mine the set of possible third ORs. It could further reduce
this set by re-probing after nine minutes, after which time
most of the confounding circuits would have rotated to
new links.
We conclude that our TCP-over-DTLS, while cur-
rently suffering lower throughput, has successfully ad-
dressed the latency introduced by the improper use of
the congestion control mechanism. We expect that once
perfected, the user-level TCP stack will have nearly the
same throughput as the equivalent TCP implementation
in the kernel. The response latency for circuits in our
improved Tor is nearly independent of throughput on ex-
isting Tor circuits travelling over the same connections;
this improves Tor’s usability and decreases the ability for
one circuit to leak information about another circuit us-
ing the same connection through interference.
4.3 Multiplexed Circuits with Packet
Dropping
Packet dropping occurs when a packet is lost while being
routed through the Internet. Packet dropping, along with
packet reordering, are consequences of the implementa-
tion of packet switching networks and are the prime rea-
son for the invention of the TCP protocol. In this section,
we perform an experiment to contrast the effect of packet
dropping on the original version of Tor and our improved
version.
We reperformed Experiment 1—using our TCP-over-
DTLS implementation of Tor instead of the standard
implementation—to investigate the effect of packet drop-
ping. The results are presented in Tables 5 and 6. We
reproduce our results from Tables 1 and 2 to contrast the
old (TCP) and new (TCP-over-DTLS) transports.
We ﬁnd that throughput is much superior for the TCP-
over-DTLS version of Tor. This is likely because the
Version Conﬁguration
TCP-
over-
DTLS
TCP
No dropping
0.1 % (remain.)
0.1 % (shared)
No dropping
0.1 % (remain.)
0.1 % (shared)
Throughput
Effective
Circuit
Network
Throughput Throughput Degradation Drop
Rate
(KB/s)
0 %
284 ± 35
261 ± 42
0.08 %
0.03 %
270 ± 34
0 %
221 ± 6.6
0.08 %
208 ± 14
184 ± 17
0.03 %
(KB/s)
47.3 ± 5.8
43.5 ± 7.0
45.2 ± 5.6
36.9 ± 1.1
34.7 ± 2.3
30.8 ± 2.8
0 %
8 %
4 %
0 %
6 %
17 %
Table 5: Throughput for different dropping conﬁgurations.
Version
TCP-
over-
DTLS
TCP
Conﬁguration
No dropping
0.1 % (remaining)
0.1 % (shared)
No dropping
0.1 % (remaining)
0.1 % (shared)
Average
Latency
428 ± 221 ms
510 ± 377 ms
461 ± 356 ms
933 ± 260 ms
983 ± 666 ms