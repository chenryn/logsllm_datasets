title:Spartan Networks: Self-Feature-Squeezing Networks for Increased Robustness
in Adversarial Settings
author:François Menet and
Paul Berthier and
Michel Gagnon and
Jos&apos;e M. Fernandez
Spartan Networks: Self-Feature-Squeezing Neural Networks for
increased robustness in adversarial settings
François Menet∗
Paul Berthier
Polytechnique Montréal
Montréal, QC
PI:EMAIL
Polytechnique Montréal
Montréal, QC
PI:EMAIL
8
1
0
2
c
e
D
7
1
]
G
L
.
s
c
[
1
v
5
1
8
6
0
.
2
1
8
1
:
v
i
X
r
a
Michel Gagnon
Polytechnique Montréal
Montréal, QC
PI:EMAIL
José M. Fernandez
Polytechnique Montréal
Montréal, QC
PI:EMAIL
ABSTRACT
Deep learning models are vulnerable to adversarial examples which
are input samples modified in order to maximize the error on the
system. We introduce Spartan Networks, resistant deep neural net-
works that do not require input preprocessing nor adversarial train-
ing. These networks have an adversarial layer designed to discard
some information of the network, thus forcing the system to focus
on relevant input. This is done using a new activation function to
discard data. The added layer trains the neural network to filter-out
usually-irrelevant parts of its input. Our performance evaluation
shows that Spartan Networks have a slightly lower precision but
report a higher robustness under attack when compared to unpro-
tected models. Results of this study of Adversarial AI as a new
attack vector are based on tests conducted on the MNIST dataset.
KEYWORDS
Artificial Intelligence, Cybersecurity, Adversarial AI
1 ON THE MOTIVATIONS FOR ROBUST DEEP
LEARNING
Neural networks and deep learning. Neural networks are machine
learning algorithms that are mainly used for supervised learning.
They rely on stacked layers of neurons. These stacked layers take a
fixed-length input tensor and generate another output fixed-length
tensor. The input/output function is differentiable relatively to its
weights. By using gradient-based optimisation on large datasets to
update the weights —a process also refered to as “training"— the
various layers generate output tensors whose values are a pattern-
matching-value of their input, allowing these algorithms to detect
features. Deep Learning consists of stacking a large amount of
layers, allowing a neural network to extract features of features
and thus grasp more abstract and complex characteristics from the
input. Deep Neural Networks (DNN) have drawn a lot of attention
recently.
In the last decade, DNN have revolutionized the automation of
perceptive tasks in various domains, especially in computer vision.
∗Corresponding author
arXiv Preprint, Dec 2018,
2018.
Deep Learning is increasingly used in autonomous driving software
[6], malware analysis [44] (with attacks already implemented on
detection systems [20, 25]), and fake news detection [1].
These technological improvements are already being used in
safety-critical environments such as vehicles or factories, where
automated driving for the former and predictive maintenance for
the latter may save millions of dollars and thousands of lives.
New Attack Vector. In late 2013, Szegedy et al.[41] discovered a new
kind of vulnerability in DNN: given a neural network, the authors
propose an alogrithm to generate samples that are missclassified
while retaining their meaning to the human cognitive system. This
groundbreaking discovery created an entirely new threat model
against machine learning powered applications. Several attacks
have then been discovered. As an example, The Fast Gradient Sign
Method (FGSM) [18] gives a reliable attack method against DNN.
This method exploits the neural network instability to small adver-
sarial input variations. This means that two samples that slightly
differ from each other can be classified differently, even if they are
indistinguishable for a human observer. The FGSM iteratively adds
or substracts a small value ϵ to each element of the input tensor.
This simple method yields surprisingly powerful attacks: on FGSM-
generated adversarial samples[18], precision of these algorithms
can drop from 99% to less than 20%.
While classfier-evasion techniques are widely studied, DNN cre-
ate an original problem as its lack of explainability and high sensi-
tivity make them vulnerable to undistinguishable evasive samples.
The performance of DNN also works against them: users tend to
trust these systems because of their comparable performance to
humans on some narrow tasks. This new attack vector does not ex-
ploit vulnerable code logic, but abuses users’ trust in the classifier’s
complex decision boundaries.
Potentially any deep learning model can be vulnerable to this
kind of attack, which is hard to detect, prevent, and whose impact
will only grow in the upcoming years.
arXiv Preprint, Dec 2018,
F. Menet et al.
1.1 Adversarial Examples and the Clever Hans
Effect
(2) Around sane datapoints, the function Fnn is sensitive to
features that do not make sense to a human observer.
The vulnerability of these models to adversarial inputs brings forth
another issue. If deep learning algorithms are seeing patterns that
a human being can not see under normal conditions[14], this means
that the models suffer from the so-called Clever Hans effect. This
term, popularized by Papernot et al. [35], comes from a horse that
was deemed capable of complex arithmetical operations, where in
fact the animal was guessing the actual answer from the uncon-
scious behaviour of the audience.
The Clever Hans effect is the product of three observations :
(1) Samples that are not generated by adversaries (i.e. sane sam-
ples) are almost always well classified by deep learning algo-
rithms.
(2) Samples that are generated by adversaries (i.e. adversarial
samples) are likely to be misclassified by deep learning algo-
rithms.
(3) Human observers can still classify samples generated by
adversaries given enough time [14].
sential to the true class of a sample.
We can thus deduce that:
• deep learning algorithms focus on features that are not es-
• features captured by deep learning algorithms during train-
ing are not the same as those learned by human beings over
the course of their lives.
Although the work of [14] shows common vulnerabilites be-
tween learning algorithms and a biological learning process, we
can still state that the current learning behaviour of these algo-
rithms do not capture the actual meaning carried by the sample.
1.2 Overview of our Work
In our work, we consider that the current training dynamics of DNN
creates their sensitivity to adversarial input. We will hypothesize
on the characteristics of the algorithm causing devious behaviour,
and we will create a minimal deep learning algorithm constrained
to function without them.
We will study adversarial image classification as this community
has created a large number of attacks and defenses. This plethora
of research is due to the simplicity of the image space topology.
For example, if one changes a pixel, or slighly varies the colors of
various pixels in a panda image, the image still resembles a panda.
On the other hand, trying to preserve the semantics of a ASM-x86
code while randomly changing a few lines of code would yield very
different results.
Thus, we define Spartan Networks, and apply the general frame-
work to Convolutional Neural Networks (CNN), as they are the
state-of-the-art for image classification.
Our hypotheses on the current CNN are as follows:
(1) The behaviour of the function Fnn approximated by the CNN
is locally linear, thus allowing an attacker to easily explore
the system’s state space.
As we consider perturbations to be unnecessary information
taken into account by the network, we try to create a network that
learns to ignore parts of the information it is given.
In this paper, we propose a new type of DNN, the Spartan Net-
works, based on two conflicting elements forced to collaborate
during the training phase:
• Firstly, the filtering layers severely reduce the amount of in-
formation they give to the next layer. They are constrained to
output the lowest amount of information through a filtering
loss.
• Secondly, the other layers connected to the previous ones
constitute a standard CNN trying to rely on the information
given to minimize its training loss.
These two parts are competing against each other: if the filter-
ing layer destroys all the information, the filtering loss is low, but
the network cannot train, and thus the training loss is high. On
the opposite, a CNN training given unlimited information allows
the network to train efficiently, reaching a low training loss, but
increasing the filtering loss.
This construction thus constitutes a self-adversarial neural net-
work. The weighted sum of those two losses forces the filtering
layer to find the vital pieces of information the rest of the network
needs to successfully train. The network thus learns to focus on
less information, and selects more relevant features in order to
maximise its performance.
This paper is organized as follows. A taxonomy of attacks spe-
cific to machine learning-powered applications is given Section 2.
We describe the various test-time adversarial attacks among the
aforementioned attacks in section 3. We present the various defense
attempts in the litterature in Section 4. We explain the motivations
for Spartan Networks in Section 5. We define candidate implemen-
tations of our proof-of-concept (PoC) in Section 6. We evaluate and
discuss the results in Section 7. In Section 8, we will balance out
the performance drop with the robustness gain to evaluate the rele-
vance of Spartan Networks. We will conclude and present future
work in Section 9.
2 ATTACKING A NEURAL NETWORK
2.1 Threat Model
There are two main attack vectors available to an adversary to
hinder a DNN’s performance: Train-Time (or Poisoning), and Test-
Time Adversarial Attacks.
Train-Time Attack. This attack aims at modifying a dataset, by either
adding patterns into existing samples, or adding new samples. The
attacker’s intent is to manipulate a deep neural network’s training
on this dataset in order to:
• create a backdoor, which is a range of samples that are miss-
classified by the target network when it has been trained
on the poisoned dataset. As an example, this could allow an
attacker to evade malware detection for a specific type of
malware, meaning the system would catch other malware
Spartan Networks
arXiv Preprint, Dec 2018,
with high accuracy, increasing user’s trust in the system, but
let the attacker’s malware through.
• diminish the overall accuracy of the neural network when
trained on the poisoned dataset. As an example, an attacker
could feed poisonned data that would cause an algorithm to
extract the wrong features, causing additional data curation
cost.
Plausible attack scenarios within this threat model are given by Gu
et al. [22].
Test-Time Attack. In this type of attack, the neural network is al-
ready trained, its parameters are frozen and the attacker can only
move within the space of all possible inputs. The attacker’s objective
is to find a sample xadv that is:
• close to a sample x correctly classified in class y
• classified in a different class yadv (cid:44) y
The main hypothesis here is that when two samples are close,
their meaning stays the same to a human observer.
If the attacker succeeds most of the time for adversarial samples
reasonably close, it can reliably output samples indistinguishable
from sane samples, that a classifier would fail to categorize well.
For example, an attacker trying to bypass an automated content
filter could take shocking pictures, slightly modify them, and suc-
cesfully bypass the filter. As the samples are close to their original
counterpart, their meaning would be preserved.
More formally, we replace the closeness by a distance between
input and adversarial input, and get a constrained optimization
problem:
Given a classifier f , a distance D, a perturbation amplitude
budget ϵ, an attacker tries to find the minimal λ on a sample x, with
a ground truth label y such that :
f (x) = y
f (x + λ) = yadv , yadv (cid:44) y
D(x, x + λ) < ϵ
(1)
Note that without the ϵ budget constraint, we could generate
misclassifications by using a correctly classified input from another
class.
In most cases, this distance is replaced by a standard Ln, n ∈
{0, 1, 2} or L∞ norm for the perturbation λ. We depict below the
various standard norms, with λ = (λi)i∈[[1,k]]
Norm name Mathematical expression
L0
L1
L2
Ln
L∞
L1(λ) =
(cid:113)
Ln(λ) = n(cid:113)
L0(λ) = #{i|λi (cid:44) 0}
i |λi|
i λ2
i
i λn
i
L∞(λ) = maxi(|λi|)
L2(λ) =
In the rest of this paper we will focus on the Test-Time Attack,
as this is where the attack surface is the largest.
3 THREAT MODELS FOR TEST-TIME
ATTACKS
There are various ways to attack a deep learning algorithm at test
time. We contextualise the attacks by defining the different attack
scenarios.
In order to understand the implications of adversarial examples,
we will give a security equivalent of our attack scenario through a
simple example. This will link the new attack vector to otherwise
known attack vectors in the domain of information security.
3.1 Attack scenario
In order to place ourselves in an information security context, we
may consider a simple setup:
• The defender runs a check digit identification software pow-
ered by DNN on the target computer. It is the only program
available on this system.
• The attacker can send samples of checks. They have access to
a very small digit database. Their original check is correctly
classified with the right amount, but they aim at maliciously
changing the input image in order to get a different check