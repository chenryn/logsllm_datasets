# 133 \| ICML 2018论文精读：优化目标函数的时候，有可能放大了"不公平"？今天我们要分享的是 ICML 2018 的一篇最佳论文提名，题目是 Fairness WithoutDemographics in Repeated Loss Minimization。这篇论文讨论了这样一个话题，在优化目标函数的时候，如何能够做到针对不同的子群体，准确率是相当的，从而避免优化的过程中过分重视多数群体。这篇论文的作者都来自斯坦福大学。
## 论文的主要贡献这篇论文其实也是希望讨论算法带来的"公平性"问题，但是出发的角度和我们上一篇讨论公平性的论文非常不一样。这篇论文的核心思想，是希望通过机器学习目标函数优化的原理，来讨论机器学习和公平性的关系。作者们发现，基于"平均损失"（AverageLoss）优化的机器学习算法，常常会给某一些少数群体带来巨大的不准确性。这其实并不是模型本身的问题，而是优化的目标函数的问题。在这样的情况下，目标函数主要是关注有较多数据的群体，保证这些群体的损失最小化，而可能忽略了在数量上不占优势的少数群体。在此基础上，还带来了另外一个用户"**留存度**"（Retention）的问题。因为少数群体忍受了比较大的优化损失，因此这些群体有可能离开或者被这个系统剔除。所以，长期下去，少数群体的数目就可能逐渐变少。这也许是目标函数的设计者们无从想到的一个平均损失函数的副产品。作者们还把这个现象命名为"**不公平的放大**"（DisparityAmplification）。``{=html}这篇论文的一个重要贡献是发现**ERM（Empirical RiskMinimization，经验风险最小化）其实就存在这种不公平的放大性**。ERM包含了很多经典的机器学习模型的目标函数，例如支持向量机（Support VectorMachines）、对数回归模型（LogisticRegression）以及线性回归等。作者们还发现，ERM可以让即便在最初看上去公平的模型，在迭代的过程中逐渐倾向于不公平。为了解决 ERM 的问题，作者们开发了一种新的算法框架，DRO（DistributionallyRobustOptimization，分布式健壮优化）。这种框架是为了最小化"最差场景"（Worst-Case）的风险，而不是平均风险。作者们在真实的数据中展示了DRO 相比于 ERM 更能够解决小众群体的不公平性问题。
## 论文的核心方法为了解决在 ERM下的对不同群体的不公平性问题，作者们首先对数据做了一种**新的假设**。作者们假设数据中有隐含的 K 个群体。每一个数据点，都有一定的概率属于这 K个群体。我们当然并不知道这 K个群体本身的数据分布，也不知道每个数据点对于这 K个群体的归属概率，这些都是我们的模型需要去估计的隐含变量。对于每一个数据点而言，在当前模型下，我们都可以估计一个"**期望损失**"（ExpectedLoss）。在新的假设框架下，因为每个数据点可能属于不同的 K个群体，而每个群体有不同的数据分布，因此会导致在当前群体下的期望损失不一样，也就是会出现K 个不一样的期望损失。我们的目的，是要控制这 K个损失中的**最差的损失**，或者叫**最差场景**。如果我们可以让最差的损失都要小于某一个值，那么平均值肯定就要好于这种情况。这也就从直观上解决了不公平放大的问题。那么，如果我们直接在这样的设置上运用ERM，会有什么效果呢？这里，有一个数值是我们比较关注的，那就是在整个框架假设下，每个群体的**期望人数**。这个数值等于在期望损失的情况下，当前群体剩余的人数加上新加入的人数。作者们在论文中建立了对这个期望人数的理论界定。这个结论的直观解释是，如果在当前更新的过程中，期望人数的数值估计能够达到一个稳定的数值状态，那么就有可能稳定到这里，不公平放大的情况就不会发生；而如果没有达到这个稳定的数值状态，那么不公平放大的情况就一定会发生。也就是说，在ERM 优化的情况下，群体的大小有可能会发生改变，从而导致人群的流失。在这个理论结果的基础上，作者们提出了 DRO。DRO的核心想法就是**要改变在优化过程中，可能因为数据分配不均衡，而没有对当前小群体进行足够的采样**。具体来说，**DRO对当前群体中损失高的人群以更高的权重，也就是说更加重视当前目标函数表现不佳的区域**。对于每一个数据点而言，损失高的群体所对应的群体概率会被放大，从而强调这个群体当前的损失状态。换句话说，DRO优先考虑那些在当前情况下损失比较大的小群体。这样的设置就能够**实现对最差情况的优化从而避免不公平放大**。作者们在文章中展示了 DRO所对应的目标函数可以在递归下降的框架下进行优化，也就是说任何当前利用 ERM的算法，都有机会更改为 DRO 的优化流程，从而避免不公平放大的情况。
## 论文的实验结果作者们在一个模拟的和一个真实的数据集上进行了实验。我们这里简单讲一讲真实数据的实验情况。作者们研究了一个"自动完成"（AutoCompletion）的任务。这个任务是给定当前的词，来预测下一个词出现的可能性。而数据则来自两个不同人群，美国白人和黑人所产生的推特信息。在这个实验中，作者们就是想模拟这两个人群的留存度和模型损失。这里面的隐含假设是，美国白人和黑人的英语词汇和表达方式是不太一样的。如果把两个人群混合在一起进行优化，很有可能无法照顾到黑人的用户体验从而留不住黑人用户。在实验之后，DRO 相比于 ERM更能让黑人用户满意，并且黑人用户的留存度也相对比较高。从这个实验中，DRO得到了验证，的确能够起到照顾少数人群的作用。
## 小结今天我为你讲了今年 ICML 的最佳论文提名。一起来回顾下要点：第一，这篇论文也讨论了算法带来的"公平性"问题，是从机器学习目标函数优化的角度来考虑这个问题的；第二，这篇论文的一个重要贡献是发现ERM 确实存在不公平的放大性，基于此，作者们开发了一种新的算法框架DRO；第三，文章的实验结果验证了 DRO的思路，确实能够解决小众群体的不公平性问题。最后，给你留一个思考题，这两期内容我们从不同的角度讨论了算法的公平性问题，你是否有自己的角度来思考这个问题？欢迎你给我留言，和我一起讨论。![](Images/5f1a3d2ca933c759573c72ee2ba198b7.png){savepage-src="https://static001.geekbang.org/resource/image/ef/b2/efd991ee74e55356bb2776f3d8d375b2.jpg"}
# 134 \| ACL 2018论文精读：问答系统场景下，如何提出好问题？今年 7 月 15 日\~20 日，计算语言学协会年会 ACL 2018（56th Annual Meetingof the Association for ComputationalLinguistics），在澳大利亚的墨尔本举行，这是自然语言处理和计算语言学领域的顶级会议。计算语言学协会（ACL）最早成立于 1962年，每年都赞助举行各种学术交流和研讨大会。ACL 大会是 ACL的旗舰会议，可以说这个会议是了解自然语言处理每年发展情况的重量级场所。会议今年收到了 1018 篇长论文和 526 篇短论文的投稿。最终，大会接收了 256篇长论文以及 125 篇短论文，综合录用率达到 24.7%。今天，我们来看这次会议的一篇最佳论文，题目是《学习提出好问题：使用完美信息的神经期望价值对澄清问题进行排序》（[Learningto Ask Good Questions: Ranking Clarification Questions using NeuralExpected Value of PerfectInformation](http://aclweb.org/anthology/P18-1255)）。首先给你简单介绍下论文的作者。第一作者萨德哈·饶（Sudha Rao）来自马里兰大学学院市分校（University ofMaryland, College Park），是计算机系的博士生。她已经在 ACL，EMNLP、NAACL等自然语言处理大会上发表了多篇论文，并且在微软研究院实习过。``{=html}第二作者是饶的导师哈尔·道姆三世（Hal DaumeIII），是马里兰大学学院市分校计算机系的一名教授，目前也在纽约的微软研究院工作。他是机器学习和自然语言处理领域的专家，在诸多领域都发表过不少研究成果，论文引用数达到9 千多次。