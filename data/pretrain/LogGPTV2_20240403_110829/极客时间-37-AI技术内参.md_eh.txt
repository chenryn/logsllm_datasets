# ICML 2018论文精读：优化目标函数时，可能放大了“不公平”？

今天我们要分享的是ICML 2018的一篇最佳论文提名，题为《在重复损失最小化中实现无群体统计的公平性》（*Fairness Without Demographics in Repeated Loss Minimization*）。该论文探讨了如何在优化目标函数的过程中确保不同子群体的准确性相对均衡，从而避免过分关注多数群体。本文作者均来自斯坦福大学。

## 论文的主要贡献

本研究旨在讨论算法带来的“公平性”问题，但其切入点与以往的研究有所不同。文章的核心思想是通过机器学习目标函数优化的原理来探讨机器学习与公平性的关系。研究者发现，基于“平均损失”（Average Loss）优化的机器学习算法往往会导致某些少数群体面临较大的不准确性。这并非模型本身的问题，而是由于目标函数倾向于减少数据量较多群体的损失，而忽视了数量上处于劣势的少数群体。此外，这种现象还引发了用户的“留存度”（Retention）问题：由于少数群体遭受较高的优化损失，他们可能会选择离开或被系统排除在外，长此以往，少数群体的数量将逐渐减少。研究者将这一现象称为“不公平的放大”（Disparity Amplification）。

一个关键发现是经验风险最小化（Empirical Risk Minimization, ERM）方法存在这种不公平放大的特性。ERM涵盖了包括支持向量机、逻辑回归及线性回归在内的多种经典机器学习模型的目标函数。即使最初模型看似公平，经过多次迭代后也可能变得不公平。为此，作者提出了一种新的算法框架——分布鲁棒优化（Distributionally Robust Optimization, DRO），旨在最小化最坏情况下的风险而非平均风险。实验证明，在处理少数群体的不公平问题方面，DRO比ERM表现更优。

## 论文的核心方法

为了解决ERM对不同群体造成的不公平问题，作者首先引入了一个新假设：数据集中隐藏着K个不同的群体，每个数据点都有一定的概率属于其中之一。我们并不知道这些群体的具体分布或归属概率，这些都是需要模型估计的潜在变量。对于每个数据点，在当前模型下可以估算出一个“期望损失”。由于每个数据点可能归属于不同的K个群体，且各群体的数据分布各异，因此会出现K个不同的期望损失。我们的目标是控制这些损失中最差的情况，即所谓的“最差场景”。

直接应用ERM在这种设定下会产生什么效果呢？这里我们需要关注的一个重要数值是在整个假设框架下每个群体的“期望人数”，它等于在给定期望损失的情况下当前群体剩余人数加上新加入的人数之和。研究表明，如果在更新过程中期望人数能够达到稳定状态，则可防止不公平放大的发生；反之则会加剧这种现象。基于此理论结果，作者提出了DRO。DRO的核心思想是在优化过程中给予高损失群体更高的权重，以强调当前目标函数表现不佳的区域。具体来说，DRO会对损失高的群体分配更大的权重，从而优先考虑那些目前损失较大的小群体。这样就能实现对最差情况的优化，进而避免不公平放大。实验表明，DRO对应的目标函数可以在递归下降框架内进行优化，任何现有利用ERM的算法都可以转换成DRO流程，从而缓解不公平放大问题。

## 论文的实验结果

为了验证DRO的有效性，作者在一个模拟数据集和一个真实数据集上进行了测试。这里重点介绍真实数据集上的实验。该实验涉及自动补全任务，即根据给定词汇预测下一个词出现的概率。数据来源于美国白人和黑人发布的推特信息。研究假设这两个群体的语言习惯存在差异，若简单地将两者混合优化，很可能无法满足黑人的用户体验，导致他们流失。实验结果显示，相较于ERM，DRO更能提升黑人用户的满意度，并保持较高留存率。这证实了DRO确实有助于改善少数群体面临的不公平问题。

## 小结

本期内容介绍了今年ICML的最佳论文提名之一。要点总结如下：
1. 本文从机器学习目标函数优化的角度探讨了算法带来的“公平性”问题；
2. 发现ERM存在不公平放大的倾向，并在此基础上开发了新的算法框架DRO；
3. 实验结果证明DRO能有效解决小众群体所遭遇的不公平问题。

最后留给你一个思考题：这两期内容分别从不同角度探讨了算法的公平性问题，请问你对此有何见解？欢迎留言交流！