one device is conﬁgured to focus on subjects of system dae-
mons, while another device focuses on application subjects.
All logs are dumped directly through pipes and sockets to a
desktop (or a cloud gateway), aligned and merged together.
4.1.2 Kernel-layer Access Event Collecting
SEAndroid uses a Linux security module loaded into ker-
nel with the policy rules to check and log native-layer access
events. However, in our case, such policy-rule-based logging
mechanism has a major drawback that its completeness and
granularity are directly aﬀected by how the security labels
and rules are deﬁned in an existing policy. Critical access
events can be easily missed or confused if coarse-grained
labels are assigned to diﬀerent subjects/objects. For in-
stance, existing policy assigns some application processes
with coarse-grained domain labels without package names,
causing diﬀerent apps to be indistinguishable.
5
Knowledge Base of Access Patterns & Functionality TraceDevice Farm with Distributed Multi-layer Logging MechanismRunFunctional TestSEAndroid PoliciesCross-layer Semantics Correlation Configure & CollectLogsParse PoliciesAnalysis EnginePolicy Rule JustificationAttack Surface AnalysisBipartite Graph VisualizationFinal ReportQueryGenerateDBdesigned to proﬁle every method call’s time usage in An-
droid framework [6]. We enhance it to be conﬁgurable to
log speciﬁc Java classes and methods, which can focus on
key functional APIs as the major descriptive items of func-
tionality traces (e.g., android.app.enterprise.Firewall).
4.1.4 Cross-layer Correlation via Native-layer Global
Variables
Since logs from the kernel layer and the Dalvik layer are
separately recorded in diﬀerent forms,
it is necessary to
correlate high-level functionality traces and low-level access
patterns together, so that SPOKE can store them as a uni-
ﬁed form in the knowledge base.
As shown in Figure 2, native layer is the intermediate
layer between Dalvik layer and kernel layer. Its main task is
to transform high-level Java requests into low-level system
calls. Although less semantics can be extracted from this
layer, several global variables in this layer are of great im-
portance for achieving cross-layer correlation. Such variables
include wall-clock timestamps, process/thread ids, user ids
and package names.
Speciﬁcally, wall clocks are globally available across all
three layers. This enables logs collected from each layer to
be aligned. Process/thread ids (pid) and user ids (uid) are
also global variables. When coupled with timestamps, they
are able to index and correlate every speciﬁc logging event in
both Dalvik layer and kernel layer in each process. Package
names are important information but missing from SEAn-
droid kernel logging. Fortunately, as system daemon zygote
keeps track of every launched application, we instrument it
to dump the package name, process and user id with precise
timestamp whenever an application is launched, which are
then correlated with access patterns in kernel logs.
Irrelevant Logging Event Filtering
In practice, the above global variables can be collected
using Android shell commands (e.g., pm, busybox). The na-
tive layer is also a suitable place for the device farm manager
to synchronize each device’s state, such as loading logging
conﬁguration.
4.1.5
For most functional tests, Android devices are required to
be in the same state as if operated by normal users. This
means that built-in system applications and daemons are ac-
tively running in the background during the testing. For ex-
ample, system_server periodically checks background sta-
tus such as WiFi and battery. Unfortunately, such back-
ground activities could introduce noise to SPOKE’s correla-
tion, especially in kernel-layer logs.
To distinguish background activities, before running any
tests, we perform a long-period logging on devices in the idle
state to identify background access patterns in kernel-layer
logs and its native-layer processes based on their periodic oc-
currences. Then during functional tests, these background
access patterns are put in a ﬁlter list of the logging conﬁgu-
ration so that each device can skip them in the logs.
Access patterns triggered by test-only operations should
be ﬁltered as well. These access patterns are not related to
the actual functionality but only caused by the phases of
test preparation and cleanup. As shown in Figure 2, by cor-
relating with test-only methods (setUp,tearDown) in Dalvik
layer based on the temporal phases, we can explicitly capture
those non-functional access patterns and ﬁlter them during
functional tests.
Figure 2: Cross-layer correlation between low-level access pat-
terns and high-level functionality traces during a functional test
test_addFirewallRule().
As our goal is to collect suﬃcient access pattern knowl-
edge for policy analysis, the logging mechanism itself should
be independent from any existing policy. For this reason, we
modify Linux kernel and design a policy-less logging mode
that supports ﬁne-grained access event logging. To distin-
guish diﬀerent subjects and objects, unique labels are de-
rived for every process subject based on the process’s exe-
cutable binary. Fine-grained ﬁle object labels are derived
based on their absolute ﬁle paths. We modify the kernel to
assign these ﬁne-grained labels without relying on a policy.
For Android applications, we log their process and user ids
with timestamps and correlate with package names logged in
native layer (Section 4.1.4). We also conﬁgure each device to
focus on speciﬁc ﬁne-grained subjects/objects to distribute
the logging work load.
After all access events are collected and merged, we de-
duplicate and transform them into more structured access
patterns as mentioned in Section 2.1. In addition, since no
malicious accesses are assumed during functional testing, the
policy-less logging skips rule-based permission checking and
directly dumps all access events to the logging channel.
4.1.3 Dalvik-layer Functionality Tracing
As mentioned above, functional tests inherently carry rich
semantics of functionalities under tests. The functionality
execution contains descriptive items that can be collected
as a functionality trace, including metadata of functional
tests, key API calls/control ﬂow.
To collect such functionality traces in a systematic way,
we place multiple hooks into existing Android testing frame-
work to monitor the execution of a functional test, to ob-
tain a detailed temporal view of how the test proceeds. As
shown in the top layer in Figure 2, this enables us to be
aware of diﬀerent phases in testing and focus on the phase
when the target functionality is executing, while ﬁltering out
non-functional test setUp, assertion and tearDown phases.
To precisely capture the control ﬂow within the target
functionality, we further leverage a runtime method tracing
facility in Dalvik (or ART) VM. Originally, this facility was
6
Kernel-Layer Access Pattern CollectingTimeline of Running Functional TestDalvik-Layer Functionality TracingNative-Layer Global Variablest0t1t4t5Test setUpStart Firewall AdminTest tearDownCall Firewall.addRule() APIAssertTrueIf PassSystem Callst2t3JNI CallsGet pid, uid, package name with timestamp(using ps, pm, busybox, logcat, zygote): Functional Access Pattern: Non-Functional Access PatternAlign with timeCorrelate using pid,uidAlign with timeCorrelate using pid,uid4.2 Policy Rule Justiﬁcation w.r.t. K
We design an analysis engine to use the knowledge base
for policy rule analysis. The ﬁrst analysis is to match policy
rules with collected functionality trace to help policy engi-
neers justify the rules.
Intuitively, if a policy rule is deﬁned to allow a set of
access patterns, which are correlated with a set of function-
ality traces in the knowledge base, we say that these access
patterns of this rule are justiﬁed by the corresponding func-
tionalities.
If the policy rule deﬁnes some access patterns
that have no correlated functionality trace, then these ac-
cess patterns are unjustiﬁed and subject to attack surface
analysis discussed in Section 4.3.
Based on Deﬁnition 6, given a policy rule r, we further de-
note the justiﬁcation result as Jr = {(ar, f ) | ar ∈ r.Ar, ar =
a, isCorrelatedK(a, f ) = T rue}. Jr contains every justiﬁed
access pattern ar deﬁned by the rule r, that can be matched
with an access pattern a correlated with functionality trace
f in the knowledge base K.
Similar Access Pattern Generalization
4.2.1
Theoretically, to justify an access pattern ar deﬁned in a
policy rule r, ar should exactly match with an access pat-
tern ak collected in the knowledge base with the exact same
subject (i.e., sr = sk), object, class, and permission. How-
ever, in practice, multiple access patterns triggered by the
same functionality could be slightly diﬀerent but semanti-
cally equivalent. One example is the auto-generated ﬁles or
pseudo ﬁle system (/proc/pid), whose ﬁle names are gener-
ated nondeterministically but semantically the same. There-
fore, they should be generalized based on their ﬁle paths, so
that they can be matched as equivalent.
We develop General(a) to realize the generalized match-
ing General(ar) = General(ak). Given an access pattern,
we generalize its subject, object and permission based on
the following empirical rules: (1) all process subjects from
the same Android application is generalized to the same ap-
plication subject; (2) auto-generated ﬁle objects are gener-
alized by only keeping the static parts in their ﬁle paths
(e.g., /proc/1234/stat ⇒ /proc/pid/stat).
(3) similar
permissions of an object class are generalized as one set (e.g.,
(write,append) ⇒ write-like for file).
These rules are derived and extensible based on empiri-
cal experience and facts about Android ﬁle system hierar-
chy (e.g., shared preﬁx on ﬁle paths) and macros, a policy
language feature in SEAndroid used by policy engineers to
group similar permissions.
In practice, the generalization
can be applied when access patterns are being stored into
the knowledge base or parsed from policy rules to save the
eﬀort of matching.
Justiﬁcation by Querying Knowledge Base
4.2.2
In the knowledge base, access patterns act as the seman-
tic bridge connecting policy rules with functionality traces.
This enables two ways of policy rule justiﬁcation.
In one
way, given one policy rule, we can justify the rule by match-
ing its deﬁned access patterns with corresponding function-
ality trace. In the other way, given a tested functionality,
we can identify all policy rules whose deﬁned access patterns
are correlated with this functionality. In practice, both cases
help policy engineers check whether policy rules are consis-
tent with corresponding functionalities.
We realize both cases using SQL queries to the knowledge
base with a set of constraints. To justify a given policy rule
r, the query (standard SQL with pseudo code constraints in
WHERE and ON clauses) is:
Jr ← SELECT Ar.ar, K.f FROM K, r.Ar
WHERE General(Ar.ar) = General(K.a)
AND isCorrelatedK(a, f ) = T rue
This query realizes the justiﬁcation deﬁnition and takes into
account the similar access pattern generalization.
To identify all related rules of a given functionality trace
f , the query is:
Rf ← SELECT T.a, R.r FROM
(SELECT K.a FROM K
WHERE isCorrelatedK(a, f ) = T rue) AS T
LEFT JOIN R
ON General(R.r.Ar.ar) = General(T.a)
[WHERE R.r IS NULL]
This query ﬁrst extracts all correlated access patterns of
the given functionality trace f into an intermediate table T .
It then uses a LEFT JOIN to match every a in T with rules
in R whose access patterns can match a. Policy engineers
can also use the optional WHERE clause to further identify the
access patterns that current rules cannot cover (e.g., for a
newly developed functionality).
4.3 Attack Surface Analysis of Policy Rules
The second task of the analysis engine is attack surface
analysis. It identiﬁes risky policy rules that allow unjustiﬁed
and over-permissive access patterns w.r.t. K.
4.3.1 Unjustiﬁed Access Patterns in Policy Rules
Ideally, every well-deﬁned policy rule can be justiﬁed when
every functionality is tested and all access patterns are col-
lected. However, in reality, the above justiﬁcation process
often reveals some policy rules whose deﬁned access patterns
cannot be justiﬁed by current knowledge base. This is due
to two reasons: 1) incomplete functional test coverage, 2)
mistakenly developed policy rules.
The ﬁrst case, as mentioned in Section 2.3, is orthogonal
to SPOKE and can leverage the outcomes of industrial and
research eﬀorts. Functionality developers can also identify
what functional tests are missing based on these unjustiﬁed
access patterns. The second case, as mentioned in Section 1,
is due to policy engineers’ knowledge gap and the conserva-
tive approach of developing over-permissive policy rules such
as using default/coarse-grained labels [34] to avoid breaking
uncertain functionalities. This causes the rules to allow un-
necessary access patterns, which would never be justiﬁed by
any functionality.
No matter which case, if the unjustiﬁed access patterns
deﬁned by certain rules can be potentially misused by at-
tackers to achieve privilege escalation, they need to be iden-
tiﬁed and ﬁxed by policy engineers. Hence, we design an
attack surface analysis to pinpoint such over-permissive ac-
cess patterns and the corresponding rules.
4.3.2 Attack Surface Analysis
Originally, an attack surface is deﬁned as the entry points
accessible to attackers in three dimensions: targets, channels
and access rights [23,30]. The case of SEAndroid policy falls
7
allowed to access critical system ﬁles. We use this bipartite
graph to present a real-world ﬁndings in Section 5.4.
5. EVALUATION
We implement a prototype of SPOKE using 3.8K SLOC
Python and 2K SLOC Impala SQL on a 8-node Hadoop
cluster, with 1K SLOC modiﬁcation in Linux kernel and
Android framework, using a device farm with 4 Samsung
Galaxy S6 devices running Android 5.1.1. This experiment
environment provides a moderate scale for the evaluation
with the help of policy engineers. In practice, SPOKE can
easily scale up to a bigger device farm and cloud.
We evaluate SPOKE using the following functional test
set. Note that, by design, SPOKE can work with any An-
droid functional tests as long as the functionality requires
SEAndroid access control. We ﬁrst show the construction of
the knowledge base. Then we present a case study of using
the analysis engine to match policy rules with the collected
domain knowledge, followed with a real-world ﬁnding by the
attack surface analysis.
5.1 Data Set and Research Questions
To evaluate the eﬀectiveness of SPOKE in real world, we
use a suite of 665 functional tests provided by Samsung
Android Team, covering 28 diﬀerent categories of function-
alities in the Android framework. The functionalities in-
clude application installation, bluetooth/WiFi/ﬁrewall/lo-
cation conﬁguration, exchange/email/multi-user setting, en-
terprise device management, etc. The functional tests cover
90% APIs deﬁned in these 28 functionality categories. Tests
are executed in both JUnit-based API calling and UI au-
tomation. Diﬀerent functionalities and test cases are devel-
oped by diﬀerent teams. SPOKE is aimed to collect diﬀerent
domain knowledge of both high-level functionality trace and