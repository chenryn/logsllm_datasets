potential leakage of data synthesis.
ACKNOWLEDGMENTS
The authors, affiliated with Southeast University, were partially
supported by Jiangsu Provincial Key Laboratory of Network and
Information Security (No. BM2003201). Minhui Xue was, in part,
supported by the Australian Research Council (ARC) Discovery
Project (DP210102670) and the Research Center for Cyber Security
at Tel Aviv University established by the State of Israel, the Prime
Minister‚Äôs Office and Tel Aviv University. Aiqun Hu and Minhui
Xue are the corresponding authors of this paper.
(a) ML Prediction
(b) Marginal Likelihood
Figure 17: Defenses comparisons on three datasets. Marginal
Likelihood: compute Eùëñ(ùëô1).
mitigation methods decrease the prediction accuracy compared to
the no-defense baseline.
In Fig. 17(b), we compare ECDFs using Eùëñ(ùëô1) (recall Section 5.3.1).
The lower score implies better marginal fitness. The experimental
result shows that the improved defense outperforms the naive
defense, and is on par with the no-defense baseline. The improved
defense succeeds in compensating the statistical deviation caused
by the naive defense (see Fig. 16(c)). More ECDFs of the naive
defense and the improved defense are shown in Figs. 19, 20, and 21.
In summary, both naive and improved defenses protect against
TableGAN-MCA and in part preserve learning ability of released
synthetic data. Moreover, the improved defense achieves better
marginal fitness than the naive defense. Despite the potentially
effective mitigation, TableGAN-MCA still remains a threat since
the proposed defenses achieve sub-optimal privacy-utility trade-
offs, e.g., reduced synthetic data diversity, under-performance for
tiny-domain datasets (see Compas datasets for details).
8 RELATED WORK
Membership privacy is the existence of individuals [25, 36]. Existing
studies show membership disclosure on discriminative machine
learning models, e.g., classifiers [28, 33, 41‚Äì43, 47] and genera-
tive machine learning models, e.g., Generative Adversarial Net-
works [7, 20, 21, 35]. In the discriminative settings, an adversary
infers whether a specific data point is used to train a target model
by querying classifier APIs and using predicted probability vec-
tors, labels, logits, etc., to train attack models. For instance, Shokri‚Äôs
shadow model [43] infers membership against overfitted multi-class
classifiers by training an attack model with labeled synthetic data,
which mimic the private training data. Subsequent works further
relax the adversary‚Äôs background knowledge [42] by extending
attacks to the white-box [33] and the label-only settings [9, 26].
In the track of inferring membership against the generative
models, there are several successful approaches, such as, table-
GAN [35], LOGAN [20], MC [21] and GAN-leaks [7]. Note that
some of these approaches is originally proposed against image data,
however, they are possibly extendable to attack tabular data. That is,
they are all related to this study. Hence, we briefly summarize these
methods in this section. The conceptual comparisons are shown in
Table 1. LOGAN [20] and table-GAN [35] leverage the output of the
overfitting discriminator to train an attack model, which is a variant
of Shokri et al. [43] in the context of GAN synthesis. However,
their attacks require the predicted probability vector of the target
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2108[27] Zinan Lin, Vyas Sekar, and Giulia Fanti. 2021. On the Privacy Properties of
GAN-generated Samples. In Proceedings of The 24th International Conference on
Artificial Intelligence and Statistics (Proceedings of Machine Learning Research,
Vol. 130), Arindam Banerjee and Kenji Fukumizu (Eds.). PMLR, 1522‚Äì1530. http:
//proceedings.mlr.press/v130/lin21b.html
[28] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu
Tang, Carl A. Gunter, and Kai Chen. 2018. Understanding Membership Inferences
on Well-Generalized Learning Models. arXiv:1802.04889 [cs.CR]
[29] Ashwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakr-
ishnan Venkitasubramaniam. 2007. L -diversity: Privacy beyond k -anonymity.
ACM Trans. Knowl. Discov. Data 1, 1 (March 2007), 3‚Äìes.
[30] H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov,
Nicolas Papernot, and Peter Kairouz. 2018. A general approach to adding differ-
ential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210
(2018).
[31] Ilya Mironov. 2017. R√©nyi differential privacy. In 2017 IEEE 30th Computer Security
Foundations Symposium (CSF). IEEE, 263‚Äì275.
[32] Arvind Narayanan and Vitaly Shmatikov. 2008. Robust De-Anonymization of
Large Sparse Datasets. In Proceedings of the 2008 IEEE Symposium on Security and
Privacy (SP ‚Äô08). IEEE Computer Society, USA, 111‚Äì125.
[33] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In 2019 IEEE Symposium on Security and
Privacy (SP). IEEE, 739‚Äì753.
[34] The Guardian Online. 2017. The Guardian view on Google‚Äôs NHS grab: legally
inappropriate. https://www.theguardian.com/commentisfree/2017/may/17/the-
guardian-view-on-googles-nhs-grab-legally-inappropriate
[35] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu
Park, and Youngmin Kim. 2018. Data synthesis based on generative adversarial
networks. Proceedings of the VLDB Endowment 11, 10 (2018), 1071‚Äì1083.
[36] Atiqur Rahman, Tanzila Rahman, Robert Laganiere, Noman Mohammed, and
Yang Wang. 2018. Membership Inference Attack against Differentially Private
Deep Learning Model. (2018).
[37] Aaditya Ramdas, Nicolas Garcia, and Marco Cuturi. 2015. On Wasserstein Two
Sample Testing and Related Families of Nonparametric Tests. arXiv:1509.02237
[math, stat] (2015). http://arxiv.org/abs/1509.02237
[38] Sander Richard, Knaplund Kris, , and Winter Kit. [n.d.]. Law School Admissions.
http://www.seaphe.org/databases/FOIA/lawschs1_1.dta
[39] Kohavi Ronny and Becker Barry. 1996. UCI Machine Learning Repository. https:
//archive.ics.uci.edu/ml/datasets/Adult
[40] Anian Ruoss, Mislav Balunoviƒá, Marc Fischer, and Martin Vechev. 2020. Learning
Certified Individually Fair Representations. arXiv:2002.10312 [cs, stat] (Feb. 2020).
http://arxiv.org/abs/2002.10312 arXiv: 2002.10312.
[41] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and
Herve Jegou. 2019. White-box vs Black-box: Bayes Optimal Strategies for Mem-
bership Inference. In Proceedings of Machine Learning Research (PMLR), Vol. 97.
5558‚Äì5567.
[42] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. 2019. ML-Leaks: Model and Data Independent Membership
Inference Attacks and Defenses on Machine Learning Models. In Proceedings of
the 2019 Network and Distributed Systems Security Symposium.
[43] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In 2017 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 3‚Äì18.
[44] Theresa Stadler, Bristena Oprisanu, and Carmela Troncoso. 2020. Synthetic Data
‚Äì A Privacy Mirage. (2020).
[45] Latanya Sweeney. 2002. k-ANONYMITY: A MODEL FOR PROTECTING PRI-
VACY. Int. J. Unc. Fuzz. Knowl. Based Syst. 10, 05 (Oct. 2002), 557‚Äì570.
[46] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramacha-
neni. 2019. Modeling tabular data using conditional gan. In Advances in Neural
Information Processing Systems (NISP). 7333‚Äì7343.
[47] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE
31st Computer Security Foundations Symposium (CSF). IEEE, 268‚Äì282.
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security. ACM, Vienna Austria, 308‚Äì318.
[2] Martin Arjovsky, Soumith Chintala, and L√©on Bottou. 2017. Wasserstein Genera-
tive Adversarial Networks. In Proceedings of Machine Learning Research, Vol. 70.
PMLR, Sydney, Australia, 214‚Äì223.
[3] Matias Barenstein. 2019. ProPublica‚Äôs COMPAS Data Revisited. arXiv:1906.04711
[cs, econ, q-fin, stat] (July 2019). http://arxiv.org/abs/1906.04711
[4] Avrim Blum, Katrina Ligett, and Aaron Roth. 2013. A learning theory approach to
noninteractive database privacy. Journal of the ACM (JACM) 60, 2 (2013), 1‚Äì25.
[5] Sala Carles. 2019. SDGym Project. https://github.com/sdv-dev/SDGym
[6] Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural
Networks. In 28th USENIX Security Symposium (USENIX Security 19). USENIX
Association, Santa Clara, CA, 267‚Äì284.
[7] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. 2020. GAN-Leaks: A Taxon-
omy of Membership Inference Attacks against Generative Models. In Proceedings
of the 2020 ACM SIGSAC Conference on Computer and Communications Security
(CCS 2020). ACM, New York, NY, USA.
[8] Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li, Nikita Borisov, Dali Kaarfar,
and Haojin Zhu. 2018. Differentially private data generative models. arXiv
preprint arXiv:1812.02274 (2018).
[9] Christopher A. Choquette Choo, Florian Tramer, Nicholas Carlini, and
Label-Only Membership Inference Attacks.
Nicolas Papernot. 2020.
arXiv:2007.14321 [cs.CR]
[10] Jesse Davis and Mark Goadrich. 2006. The relationship between Precision-Recall
and ROC curves. In Proceedings of the 23rd international conference on Machine
learning - ICML ‚Äô06. ACM Press, Pittsburgh, Pennsylvania, 233‚Äì240.
[11] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and
Moni Naor. 2006. Our Data, Ourselves: Privacy Via Distributed Noise Genera-
tion. In Advances in Cryptology - EUROCRYPT 2006, Vol. 4004. Springer Berlin
Heidelberg, Berlin, Heidelberg, 486‚Äì503.
[12] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-
brating noise to sensitivity in private data analysis. In Theory of cryptography
conference. Springer, 265‚Äì284.
[13] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends¬Æ in Theoretical Computer Science 9, 3‚Äì4
(2014), 211‚Äì407.
[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672‚Äì2680.
[15] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
Aaron C Courville. 2017. Improved Training of Wasserstein GANs. In Advances
in Neural Information Processing Systems (NDSS). 5767‚Äì5777.
[16] Anupam Gupta, Moritz Hardt, Aaron Roth, and Jonathan Ullman. 2013. Privately
releasing conjunctions and the statistical query barrier. SIAM J. Comput. 42, 4
(2013), 1494‚Äì1520.
[17] Anupam Gupta, Aaron Roth, and Jonathan Ullman. 2012. Iterative Constructions
and Private Data Release. In Theory of Cryptography, Vol. 7194. Springer Berlin
Heidelberg, Berlin, Heidelberg, 339‚Äì356.
[18] Moritz Hardt, Katrina Ligett, and Frank Mcsherry. 2012. A Simple and Prac-
tical Algorithm for Differentially Private Data Release. In Advances in Neural
Information Processing Systems 25. 2339‚Äì2347.
[19] Moritz Hardt and Guy N. Rothblum. 2010. A Multiplicative Weights Mechanism
for Privacy-Preserving Data Analysis. In 2010 IEEE 51st Annual Symposium on
Foundations of Computer Science (Las Vegas, NV, USA). 61‚Äì70.
[20] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2020. LO-
GAN: Membership Inference Attacks Against Generative Models. In Proceedings
on Privacy Enhancing Technologies, Vol. 2019. 133‚Äì152.
[21] Benjamin Hilprecht, Martin H√§rterich, and Daniel Bernau. 2019. Monte Carlo
and Reconstruction Membership Inference Attacks against Generative Models.
In Proceedings on Privacy Enhancing Technologies, Vol. 2019. 232‚Äì249.
[22] Larson Jeff, Roswell Marjorie, and Atlidakis Vaggelis. 2017. Compas Dataset.
https://github.com/propublica/compas-analysis
[23] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[24] Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. 2007. t-Closeness:
Privacy Beyond k-Anonymity and l-Diversity. In 2007 IEEE 23rd International
Conference on Data Engineering. 106‚Äì115.
[25] Ninghui Li, Wahbeh Qardaji, Dong Su, Yi Wu, and Weining Yang. 2013. Member-
ship privacy: a unifying framework for privacy definitions. In Proceedings of the
2013 ACM SIGSAC conference on Computer & communications security - CCS ‚Äô13.
Berlin, Germany, 889‚Äì900.
[26] Zheng Li and Yang Zhang. 2020. Membership Leakage in Label-Only Exposures.
arXiv preprint arXiv:2007.15528 (2020).
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2109APPENDIX
A DP-WGAN
We list the DP-related hyper-parameters in differentially private
WGAN in Table 8. As a result, we show the ECDFs of marginals for
(ùúñ, ùõø)-DP synthesized data in Fig. 18. Smaller training dataset size
usually gains less satisfactory generation quality under DP training
with a similar privacy budget.
B NAIVE AND IMPROVED DEFENSES
We show additional ECDFs of marginals for ‚ÄúRemove Colliding
Members‚Äù mitigation and ‚ÄúGAN-constrained Training‚Äù mitigation
in Figs. 19, 20, and 21.
Table 8: Hyper-parameters in DP-WGAN. (ùúñ,ùõø) : privacy bud-
get; ùëÜ: clip threshold; ùúé: standard deviation of the noise
added in each step.
Datasets
Adult
Lawschool
Compas
(ùúñ, ùõø)
(0.5, 10‚àí5)
(1.0, 10‚àí5)
(2.0, 10‚àí5)
(4.0, 10‚àí5)
(8.0, 10‚àí5)
(16.0, 10‚àí5)
(0.5, 10‚àí5)
(1.0, 10‚àí5)
(2.0, 10‚àí5)
(4.0, 10‚àí5)
(8.0, 10‚àí5)
(16.0, 10‚àí5)
(2.0, 10‚àí4)
(4.0, 10‚àí4)
(8.0, 10‚àí4)
(16.0, 10‚àí4)
(32.0, 10‚àí4)
(ùëÜ, ùúé)
(0.1, 0.5)
(0.1, 0.45)
(0.1, 0.4)
(0.1, 0.3)
(0.1, 0.17)
(0.1, 0.11)
(0.1, 0.4)
(0.1, 0.45)
(0.1, 0.48)
(0.1, 0.25)
(0.1, 0.15)
(0.1, 0.11)
(0.1, 0.9)
(0.1, 0.48)
(0.1, 0.27)
(0.1, 0.16)
(0.1, 0.11)
Sampling rate
500/31655
500/31655
500/31655
500/31655
500/31655
500/31655
500/43011
500/43011
500/43011
500/43011
500/43011
500/43011
100/3694
100/3694
100/3694
100/3694
100/3694
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2110(a) Adult, ùúñ = 0.5, ùõø = 10‚àí5
(b) Lawschool, ùúñ = 0.5, ùõø = 10‚àí5
Figure 18: ECDF comparison between training data and differentially private GAN-synthesized data.
(c) Compas, ùúñ = 2.0, ùõø = 10‚àí4
(a) Adult ECDF, Remove Overlapping
Figure 19: ECDF comparisons for ‚ÄúRemove Overlapping" mitigation and ‚ÄúGAN-constrained Training‚Äù mitigation.
(b) Adult ECDF, GAN-constrained Training
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2111(a) Lawschool ECDF, Remove Overlapping
Figure 20: ECDF comparisons for ‚ÄúRemove Overlapping‚Äù mitigation and ‚ÄúGAN-constrained Training‚Äù mitigation.
(b) Lawschool ECDF, GAN-constrained Training
(a) Compas ECDF, Remove Overlapping
Figure 21: ECDF comparisons for ‚ÄúRemove Overlapping‚Äù mitigation and ‚ÄúGAN-constrained Training‚Äù mitigation.
(b) Compas ECDF, GAN-constrained Training
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2112