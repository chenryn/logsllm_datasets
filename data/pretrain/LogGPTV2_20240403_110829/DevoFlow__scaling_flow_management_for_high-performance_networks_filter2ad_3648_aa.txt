title:DevoFlow: scaling flow management for high-performance networks
author:Andrew R. Curtis and
Jeffrey C. Mogul and
Jean Tourrilhes and
Praveen Yalagandula and
Puneet Sharma and
Sujata Banerjee
DevoFlow: Scaling Flow Management for
High-Performance Networks∗
Andrew R. Curtis
University of Waterloo
Jeffrey C. Mogul, Jean Tourrilhes, Praveen Yalagandula
Puneet Sharma, Sujata Banerjee
HP Labs — Palo Alto
ABSTRACT
OpenFlow is a great concept, but its original design im-
poses excessive overheads. It can simplify network and traf-
ﬁc management in enterprise and data center environments,
because it enables ﬂow-level control over Ethernet switch-
ing and provides global visibility of the ﬂows in the net-
work. However, such ﬁne-grained control and visibility comes
with costs: the switch-implementation costs of involving the
switch’s control-plane too often and the distributed-system
costs of involving the OpenFlow controller too frequently,
both on ﬂow setups and especially for statistics-gathering.
In this paper, we analyze these overheads, and show that
OpenFlow’s current design cannot meet the needs of high-
performance networks. We design and evaluate DevoFlow,
a modiﬁcation of the OpenFlow model which gently breaks
the coupling between control and global visibility, in a way
that maintains a useful amount of visibility without impos-
ing unnecessary costs. We evaluate DevoFlow through simu-
lations, and ﬁnd that it can load-balance data center traﬃc
as well as ﬁne-grained solutions, without as much overhead:
DevoFlow uses 10–53 times fewer ﬂow table entries at an av-
erage switch, and uses 10–42 times fewer control messages.
Categories and Subject Descriptors.
C.2 [Internetworking]: Network Architecture and Design
General Terms. Design, Measurement, Performance
Keywords. Data center, Flow-based networking
1.
INTRODUCTION
Flow-based switches, such as those enabled by the Open-
Flow [35] framework, support ﬁne-grained, ﬂow-level control
of Ethernet switching. Such control is desirable because it
enables (1) correct enforcement of ﬂexible policies without
carefully crafting switch-by-switch conﬁgurations, (2) visi-
bility over all ﬂows, allowing for near optimal management
of network traﬃc, and (3) simple and future-proof switch
design. OpenFlow has been deployed at various academic
institutions and research laboratories, and has been the ba-
sis for many recent research papers (e.g., [5, 29, 33, 39, 43]),
∗
The version of this paper that originally appeared in the
SIGCOMM proceedings contains an error in the description of
Algorithm 1. This version has corrected that error.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15–19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
as well as for hardware implementations and research pro-
totypes from vendors such as HP, NEC, Arista, and Toroki.
While OpenFlow was originally proposed for campus and
wide-area networks, others have made quantiﬁed arguments
that OpenFlow is a viable approach to high-performance
networks, such as data center networks [45], and it has been
used in proposals for traﬃc management in the data center
[5,29]. The examples in this paper are taken from data center
environments, but should be applicable to other cases where
OpenFlow might be used.
OpenFlow is not perfect for all settings, however. In par-
ticular, we believe that it excessively couples central control
and complete visibility. If one wants the controller to have
visibility over all ﬂows, it must also be on the critical path of
setting up all ﬂows, and experience suggests that such cen-
tralized bottlenecks are diﬃcult to scale. Scaling the central
controller has been the topic of recent proposals [11, 33, 47].
More than the controller, however, we ﬁnd that the switches
themselves can be a bottleneck in ﬂow setup. Experiments
with our prototype OpenFlow implementation indicate that
its ratio of data-plane to control-plane bandwidth is four or-
ders of magnitude less than its aggregate forwarding rate.
We ﬁnd this slow control-data path adds unacceptable la-
tency to ﬂow setup, and cannot provide ﬂow statistics timely
enough for traﬃc management tasks such as load balancing.
Maintaining complete visibility in a large OpenFlow network
can also require hundreds of thousands of ﬂow table entries
at each switch. Commodity switches are not built with such
large ﬂow tables, making them inadequate for many high-
performance OpenFlow networks.
Perhaps, then, full control and visibility over all ﬂows is
not the right goal. Instead, we argue and demonstrate that
eﬀective ﬂow management can be achieved by devolving con-
trol of most ﬂows back to the switches, while the controller
maintains control over only targeted signiﬁcant ﬂows and
has visibility over only these ﬂows and packet samples. (For
example, load balancing needs to manage long-lived, high-
throughput ﬂows, known as “elephant” ﬂows.) Our frame-
work to achieve this, DevoFlow, is designed for simple and
cost-eﬀective hardware implementation.
In essence, DevoFlow is designed to allow aggressive use
of wild-carded OpenFlow rules—thus reducing the number
of switch-controller interactions and the number of TCAM
entries—through new mechanisms to detect signiﬁcant ﬂows
eﬃciently, by waiting until they actually become signiﬁcant.
DevoFlow also introduces new mechanisms to allow switches
to make local routing decisions, which forward ﬂows that do
not require vetting by the controller.
The reader should note that we are not proposing any rad-
ical new designs. Rather, we are pointing out that a system
like OpenFlow, when applied to high-performance networks,
must account for quantitative real-world issues. Our argu-
ments for DevoFlow are essentially an analysis of tradeoﬀs
between centralization and its costs, especially with respect
to real-world hardware limitations. (We focus on OpenFlow
in this work, but any centralized ﬂow controller will likely
face similar tradeoﬀs.)
Our goal in designing DevoFlow is to enable cost-eﬀective,
scalable ﬂow management. Our design principles are:
• Keep ﬂows in the data-plane as much as possible. Involv-
ing the control-plane in all ﬂow setups creates too many
overheads in the controller, network, and switches.
• Maintain enough visibility over network ﬂows for eﬀec-
tive centralized ﬂow management, but otherwise provide
only aggregated ﬂow statistics.
• Simplify the design and implementation of fast switches
while retaining network programmability.
DevoFlow attempts to resolve two dilemmas — a control
dilemma:
• Invoking the OpenFlow controller on every ﬂow setup
provides good start-of-ﬂow visibility, but puts too much
load on the control plane and adds too much setup delay
to latency-sensitive traﬃc, and
• Aggressive use of OpenFlow ﬂow-match wildcards or
hash-based routing (such as ECMP) reduces control-
plane load, but prevents the controller from eﬀectively
managing traﬃc.
and a statistics-gathering dilemma:
• Collecting OpenFlow counters on lots of ﬂows, via the
pull-based Read-State mechanism, can create too much
control-plane load, and
• Aggregating counters over multiple ﬂows via the wild-
card mechanism may undermine the controller’s ability
to manage speciﬁc elephant ﬂows.
We resolve these two dilemmas by pushing responsibility
over most ﬂows to switches and adding eﬃcient statistics
collection mechanisms to identify signiﬁcant ﬂows, which are
the only ﬂows managed by the central controller. We discuss
of the beneﬁts of centralized control and visibility in §2, so
as to understand how much devolution we can aﬀord.
Our work here derives from a long line of related work
that aims to allow operators to specify high-level policies
at a logically centralized controller, which are then enforced
across the network without the headache of manually craft-
ing switch-by-switch conﬁgurations [10, 12, 25, 26]. This sep-
aration between forwarding rules and policy allows for in-
novative and promising network management solutions such
as NOX [26, 45] and other proposals [29, 39, 49], but these
solutions may not be realizable on many networks because
the ﬂow-based networking platform they are built on—
OpenFlow—is not scalable. We are not the ﬁrst to make
this observation; however, others have focused on scaling the
controller, e.g., Onix [33], Maestro [11], and a devolved con-
troller design [44]. We ﬁnd that the controller can present a
scalability problem, but that switches may be a greater scal-
ability bottleneck. Removing this bottleneck requires min-
imal changes: slightly more functionality in switch ASICs
and more eﬃcient statistics-collection mechanisms.
This paper builds on our earlier work [36], and makes
the following major contributions: we measure the costs of
OpenFlow on prototype hardware and provide a detailed
analysis of its drawbacks in §3, we present the design and use
of DevoFlow in §4, and we evaluate one use case of DevoFlow
through simulations in §5.
2. BENEFITS OF CENTRAL CONTROL
In this section, we discuss which beneﬁts of OpenFlow’s
central-control model are worth preserving, and which could
be tossed overboard to lighten the load.
Avoids the need to construct global policies from
switch-by-switch conﬁgurations: OpenFlow provides an
advantage over traditional ﬁrewall-based security mecha-
nisms, in that it avoids the complex and error prone pro-
cess of creating a globally-consistent policy out of local ac-
cept/deny decisions [12, 39]. Similarly, OpenFlow can pro-
vide globally optimal admission control and ﬂow-routing in
support of QoS policies, in cases where a hop-by-hop QoS
mechanism cannot always provide global optimality [31].
However, this does not mean that all ﬂow setups should be
mediated by a central controller. In particular, microﬂows (a
microﬂow is equivalent to a speciﬁc end-to-end connection)
can be divided into three broad categories: security-sensitive
ﬂows, which must be handled centrally to maintain secu-
rity properties; signiﬁcant ﬂows, which should be handled
centrally to maintain global QoS and congestion properties;
and normal ﬂows, whose setup can be devolved to individual
switches.
Of course, all ﬂows are potentially “security-sensitive,” but
some ﬂows can be categorically, rather than individually, au-
thorized by the controller. Using standard OpenFlow, one
can create wild-card rules that pre-authorize certain sets of
ﬂows (e.g.: “all MapReduce nodes within this subnet can
freely intercommunicate”) and install these rules into all
switches. Similarly, the controller can deﬁne ﬂow categories
that demand per-ﬂow vetting (e.g., “all ﬂows to or from the
ﬁnance department subnet”). Thus, for the purposes of secu-
rity, the controller need not be involved in every ﬂow setup.
Central control of ﬂow setup is also required for some
kinds of QoS guarantees. However, in many settings, only
those ﬂows that require guarantees actually need to be ap-
proved individually at setup time. Other ﬂows can be cat-
egorically treated as best-eﬀort traﬃc. Kim et al. [31] de-
scribe an OpenFlow QoS framework that detects ﬂows re-
quiring QoS guarantees, by matching against certain header
ﬁelds (such as TCP port numbers) while wild-carding others.
Flows that do not match one of these “ﬂow spec” categories
are treated as best-eﬀort.
In summary, we believe that the central-control beneﬁts
of OpenFlow can be maintained by individually approving
certain ﬂows, but categorically approving others.
Near-optimal traﬃc management: To eﬀectively man-
age the performance of a network, the controller needs to
know about the current loads on most network elements.
Maximizing some performance objectives may also require
timely statistics on some ﬂows in the network. (This assumes
that we want to exploit statistical multiplexing gain, rather
than strictly controlling ﬂow admission to prevent oversub-
scription.)
We give two examples where the controller is needed to
manage traﬃc: load balancing and energy-aware routing.
Example 1: Load balancing via a controller involves col-