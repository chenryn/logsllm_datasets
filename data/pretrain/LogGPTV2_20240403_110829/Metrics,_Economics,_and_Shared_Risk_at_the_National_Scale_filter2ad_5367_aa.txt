title:Metrics, Economics, and Shared Risk at the National Scale
author:Daniel E. Geer Jr.
Metrics, Economics, 
and Shared Risk
at the National Scale
Dan Geer
PI:EMAIL / 617.492.6814
formalities:    Daniel E. Geer, Jr., Sc.D.
                Principal, Geer Risk Services, LLC
                P.O. Box 390244
                Cambridge, Mass. 02139
                Telephone: +1 617 492 6814
                Facsimile: +1 617 491 6464
                Email: PI:EMAIL
                VP/Chief Scientist, Verdasys, Inc.
                950 Winter St., Suite 2600
                Waltham, Mass. 02451
                Direct-in: +1 781 902 5629
                Corporate: +1 781 788 8180
                Facsimile: +1 781 788 8188
                Email: PI:EMAIL
Outline
• Where are we?
• What drives change?
• The nature of risk
• The near term future
• Measurement, models, implications
• Summary and proposal
An general thread of the thoughts in this presentation.
Ask the right questions
(What can be more engineering-relevant than 
getting the problem statement right?)
• What can attack a national infrastructure?
• What can be done about it?
• How much time do we have?
• Who cares and how much do they care?
In all of engineering, getting the problem statement right is job 1.  Without the right problem statement you get “we 
solved the wrong problem” or “this is a solution is search of a problem” or worse.
Our questions here are to ask what it is about the national scale that elevates some attacks to proper focus and what 
sets others aside.
The Setting
• Advanced societies are more interdependent
• Every sociopath is your next door neighbor
• Average clue is dropping
• Information assets increasingly in motion
• No one owns the risk -- yet
The more advanced the society the more interdependent it is.  Which is the cause and which is the eﬀect is a debate for 
sociology or economics, but it is a tight correlation.
Equidistance and near zero latency is what distinguishes the Internet from the physical world.
Power doubles every 12-18 months and, obviously, skill on the part of the user base does not.  Hence the ratio of skill 
to power falls.  This has broad implications.
Information does not want to be free, but it does want to be located to its advantage.
In ﬁnance, risk taking and reward are tightly correlated and there is zero ambiguity over who owns what risk; cf., in the 
digital security sphere where there is nothing but ambiguity over who owns what risk.
The Drivers of Change
• Laboratory
• Economics
• Psychology
What is it that changes natures of the computing infrastructure at the national level?  For relevance to decision making 
at that level, it essential to look not at the present moment but rather what trends exist extrapolated to at least that 
point in the future which is the earliest practical time at which strategic countermeasures can intercept the threat to the 
national infrastructure.  Put diﬀerently, as one cannot expect to turn a ship the size of the national infrastructure in 
short time we must therefore lead our target.
There are three principal drivers to the national computing infrastructure: the ongoing miracles exiting our commercial 
laboratories, the economics by which change in our national infrastructure are modiﬁed, and the psychology of national 
populations, generally speaking, which latter point determines what it is that the public demands of government, inter 
alia.
Lab: model creep
ten years
CPU 10^2
Data 10^3
BW 10^4
0
1
2
3
4
5
years
6
7
8
9 10
price
1.00
0.75
0.50
0.25
0.00
Black line is “Moore’s Law” whereby $/MHz drops by half every 18 months.  It’s unnamed twins are, in red, the price of 
storage (12 month) and, in green, bandwidth (9 month).  Taken over a decade, while CPU will rise by two orders of 
magnitude, the constant dollar buyer will have 10 times as much data per computer cycle available but that data will be 
movable to another CPU in only 1/10th the time.  This has profound implications for what is the general charactgeristic 
of the then optimal computing plant.
And, even if there are wiggles here and there, the general point that there is a drift over time in the optimal computer 
design stands.
Econ:  applications
• Applications are federating, and thus
• accumulating multiple security domains
• getting ever more moving parts
• crossing jurisdictions
Under economic inﬂuences, such as the various promises of “web services,” applications in general are increasing their 
reach by federating across internal and external corporate boundaries not to mention jurisdictions.  That this requires 
more moving parts is obvious, of course.
This force is not the issue, its eﬀect is.  The eﬀect is to make ever-larger applications at least insofar as these ever-
larger applications are able to be productivity-enhancing while exhibiting complexity-hiding.
Econ: transport
• HTTP assumes transport role, and thus
• attack execution at lower skill levels
• content inspection collapses
• perimeter defense trends diseconomic
Allied with the increasing reach and scope of applications is an increasing reliance on HTTP as the transport 
mechanism.  Microsoft, for its .NET environment, is actually recommending that application writers focus on libhttp 
rather than libtcp, i.e., to rely on HTTP as the core transport infrastructure rather than TCP.
As the limit, a ﬁrewall needs one hole and only one hole — for HTTP (and HTTPS, i.e., SSL).  With a hole in the ﬁrewall of 
this size, it is hardly worth saying the level of eﬀort and skill required to transit the ﬁrewall to attack internal machines 
is lessened.  Which is more, once program fragments are part of the payload (such as remote procedure calls in the 
Simple Object Application Protocol (SOAP)), content inspection of the information ﬂow becomes virtually intractable.
Econ: data
• Data takes command, because
• corporate IT spending on storage: 
• data/$ up 16x in same interval
• total volume doubling at ~30 months
4% in 1999 v. 17% in 2003 (Forrester)
The volume of data is substantial, getting more so, and will likely dominate security’s rational focus from this point 
forward.
The public’s interest
• Spam
• channel saturation, labor costs
• Viruses
• warning-time shrinking, labor costs
• Theft
• identity, cycles, keystrokes, reputation
                                   ...Safety, safety, safety
The interest of individual members of the public includes these illustrative three, at least.
One reaction
n(privacy_regs),  US  +  Canada
1975        50
1976      200
1979      400
1984      500
1988      600
1992    1000
1997    1300
2002    1400
1000
500
1975
1980
1985
1990
1995 2000
One measure of the Public Interest is the rate at which the security setting, in this case its subset privacy, is regulated.
This graph and data are of the total number of privacy regulations at the State and Federal level in the US plus Canada.
The Public Interest
• Loss of inherently unique assets
• GPS array, FAA EBS, DNS
• Cascade failure
• Victims become attackers at high rate
                    Everything else is less important
If having to name the only risks that matter at the national scale, there seem to be two classes and only two classes.
On the one hand, there are entities that are inherently unique by design.  For example, the Global Positioning System 
satellite array (taken as a unit) is one such entity; the Federal Aviation Administrations emergency broadcast system  is 
another, and the Domain Naming System is another.  In each case, it is an authoritative data or control source which 
would be less authoritative if it was surrounded by alternatives.  Putting it diﬀerently, you only want one red telephone 
though losing that red telephone is a risk at the national scale.
On the other hand, there entities that are dangers to each other in proportion to their number -- any risk which, like an 
avalanche, can be initiated by the one but propagated by the many.  This “force multiplication” makes any of this class 
of risks a suitable candidate for national scale.
Risk to Unique Asset
• Pre-condition: Concentrated data/comms
• Ignition: Targeted attack of high power
• Counter: Defense in depth, Replication
• Requires: The resolve to spend money
For unique assets to be a risk at the national scale, you need the pre-condition of some high concentration of data, 
communications, or both.  The ignition of that risk is a targeted attack of high power up to and including the actions of 
nation states.  The counter to this latent risk is “defense in depth” which may include replication.  Defense in depth is 
ultimately (at the policy level) a referendum on the willingness to spend money.
As such, there is nothing more to say at the general level and we lay this branch of the tree aside so as to focus on the 
other.
Risk of Cascade Failure
• Pre-condition: Always-on monoculture
• Ignition: Any exploitable vulnerability
• Counter: Risk diversiﬁcation, not replication
• Requires: Resolve to create heterogeneity
For cascade failure to be a risk at the national scale, you need the pre-condition of an always-on monoculture.  The 
ignition of that risk is an attack on vulnerable entity within the always on monoculture so long as it has a 
communication path to other like entities.  The counter to this latent risk is risk diversiﬁcation which absolutely does 
not include replication.  Cascade avoidance is ultimately (at the policy level) a referendum on the resolve to treat shared 
risk as a real cost, per se.
We now follow this branch to see where it leads.  Sean Gorman of George Mason University has an upcoming 
publication that suggests that the risk-cost of homogeneity kicks in at rather low densities (preliminary results indicate 
43% for leaf nodes, 17% for core fabric).
Why ’sploits matter
• Monoculture is a force multiplier
• Amateurs provide smokescreen for pros
• Only known vulns get ﬁxed
The unknown are held in reserve
• Automated reverse engineering of patches 
is accelerating
So why do exploits matter?  Because in a monoculture they are the ignition and their propagation amongst potential 
instigators of a cascade failure is well documented.  Of course, the extent of their existence and propagation is 
unknowable in and of itself, but it is clear that the testing of exploits by the most expert is suﬃciently obscured by the 
constant rain of amateur attacks.  One estimate (by John Quarterman of Internet Perils) is that perhaps 10% of total 
Internet backbone traﬃc is low-level scans while another (by Vern Paxson of Lawrence Berkeley) is that for a site such 
as LBL one can expect perhaps 40% of inbound connections to be attacks.
Because only known vulnerabilities get ﬁxed, the central question is who knows what and when.  The conservative 
assumption for a vulnerability discoverer is that he was not the ﬁrst to discover the current vulnerability.  A similarly 
conservative assumption is that not all vulnerability discoverers are of good will.  Therefore the question is “How many 
vulnerabilities are known, silently, to persons not of good will?”  The corroborating evidence that this number is non-
zero lies in observing that all major virus or worm attacks to date have exploited previously known vulnerabilities, never 
unknown ones.  With such evidence, either all vulnerabilities are discovered by persons of good will or there is a 
reservoir of vulnerabilities being held in reserve.
Note that converting patches into vulnerabilities by reverse engineering the patches is not only now the dominant 
source of exploits but that it is becoming much quicker due to automation.  In two years the times have dropped from 
six months to under a week for principal attacks of public interest.  Further declines may no longer matter.
Wishful Thinking
• The absence of a serious event can be:
• Evidence of zero threat
• Consistent with risk aggregation
• A failure to detect precursors
The absence of a major attack event, the situation in which we ﬁnd ourselves today, is not, as it might seem on ﬁrst 
blush, reassuring of low threat.  It is consistent with low/no threat to be sure, but it is also consistent with risk 
aggregation (an insurance term where instead of 1,000 claims occurring at random one instead gets 1,000 claims all at 
once, the diﬀerence between house ﬁres and earthquakes).  It is also consistent with a failure to detect, though less 
likely that “major” and “indetectible” are likely to appear in the same sentence.
Microsoft in particular
• Tight integration as competitive strategy
• users locked-in
• effective module size grows
• reach of vuln expands
• Insecurity α complexity α square(codesize)
The situation with Microsoft is the critical focus just as when discussing solar power one must speak of Sol.
The quality control literature leads one to expect that as eﬀective code size grows complexity grows as the square of 
that code size.  Similarly, the quality control literature expects total ﬂaws, of which security ﬂaws are a subset, to grow 
linearly in complexity.  Microsoft’s competitive strategy is manifestly to achieve user-level lock-in via tight integration 
of applications.  This tight integration, besides violating software engineering wisdom, expands eﬀective module size to 
that of the tightly integrated whole and thus inevitably creates the platform most likely to have security ﬂaws, and by a 
wide margin.  Coupled with its 94% market share one thus achieves the vulnerable monoculture on which cascade 
failure depends.
Software
“[B]y using this product you agree that it’s all 
your fault, that it’s only broken to the extent 
that it ships ‘as is’ and therefore if you think it’s 
broken you accepted that this was the case 
when you bought it, and anyway you agreed it 
wasn’t and you didn’t buy it anyway, because it’s 
still ours...”
http://www.theregister.co.uk/content/4/33082.html
This is the wonderfully curmudgeonly UK digital publication “The Register” synopsizing the plain english meaning of 
most software licenses.
If nothing else, it illustrates the lack of clarity about responsibility when software is misused.
Field Repairs
• Not possible to patch your way to safety
• Liability will reference patch-state
• “Due care” vs “Force Majeure”
• “Attractive nuisance” vs “Unwitting 
• Automatic update is the most powerful 
accomplice”
form of mobile code
Field repairs, the dominant activity if not strategy of the present time, are at best a damage containment.  It is not 
possible to patch yourself to safety: any signiﬁcant patch latency preserves the critical mass (of vulnerable entities) 
while every patch has a non-zero chance of collateral damage.  Thus we come to a discussion of formal liability and the 
high likelihood that in the short term such discussion will focus on patch-state as a proxy for culpability either in the 
sense of patching being evidence of due care or the lack of patching, particularly within substantial enterprises, being 
evidence of an attractive nuisance (like an unfenced swimming pool).
Looking dispassionately at risk, one must also conclude that automatic update is the ultimately powerful form of mobile 
code.  Automatic patching does harden systems but it does so more toward brittleness than toward toughness in that if 
ever the automatic patching pathway is itself eﬀectively co-opted then the game is largely over at that moment.
Prediction(s)
• Trafﬁc analysis recapitulates cryptography
• Perimeter defense contracts to data
• Security & Privacy have their long-overdue 
• Meritocracy begins yielding to government
head-on collision
No discussion of national level threat can look at the current point in time; it must instead lead its target just as a 
hunter must his.  In that sense, the next ten years (or less) will have the commercial sector catching up to the military in 
traﬃc analysis just as the last ten years had that catch-up in cryptography.  At the same time, increasing threat will, as 
it must, lead to shrinking perimeters thus away from a focus on enterprise-scale perimeters and more toward 
perimeters at the level of individual data objects.  Security and privacy are, indeed, interlocking but, much as with twins 
in the womb, the neoplastic growth of the one will be to the detriment of the other hence the bland happy talk of there 
being no conﬂict between the two will be soon shown to be merely that.  Finally, the Internet as a creature built by, of, 
and for the technical and ethical elite being no longer consistent with the facts on the ground, its meritocratic 
governance will yield to the anti-meritocratic tendencies of government(s).
Grand Challenges
...within ten years...
• No further large scale epidemics
• COTS tools for building certiﬁable systems
• Low/no skill required to be safe
• Info. risk mgmt. > ﬁnancial risk mgmt.
In November, 2003, the Computing Research Association held a limited attendance, invitation only retreat in Virginia at 
the behest of the National Science Foundation.  The purpose was to set the ten-year research agenda in information 
security .  Here are the results in lay terms: An 
end to epidemics, commercial oﬀ the shelf (COTS) tools for building certiﬁable systems, improvements in semantics 
and user interface such that one need not be an expert to be safe, and information risk management of a quantitative 
sophistication as good as that of ﬁnancial risk management.
These are high goals, and at the same time it is horrifying that any of them could take a decade to deliver.  On the other 
hand, if they do take as much as a decade, then starting now is crucial.
See http://www.cra.org/Activities/grand.challenges/security/home.html
Metrics
“When you can measure what you are 
speaking about, and express it in numbers, 
you know something about it; but when you 
cannot measure it, when you cannot express 
it in numbers, your knowledge is of a meager 
and unsatisfactory kind: it may be the 
beginning of knowledge, but you have 
scarcely, in your thoughts, advanced to the 
stage of science.”
                       William Thomson, Lord Kelvin
The foremost statement in science regarding the necessity of measurement.
Metrics: our version
• How secure am I?
• Am I better off than this time last year?
• Am I spending the right amount of money?
• How do I compare to my peers?
• What risk transfer options do I have?
These are precisely the questions that any CFO would want to know and we are not in a good position to answer.
Metrics: lay of the land
• No time to create hence adapt
• Information sharing, various forms
• To be relevant, must calibrate spend
• GAAP for security a long way off
• et cetera, et cetera, et cetera
The need for security is so great that we simply cannot aﬀord to wait while new measures, uniquely wonderful and 
wonderfully unique, are invented.  We must adapt measures and techniques that already exist.  Yes, this requires taste 
and judgment, but those, however rare, are in better supply than time.
Principal amongst the measurement eﬀorts must be some way to draw baselines and otherwise to share data such that 
individual ﬁrms can compare themselves to others.  The information to share is not exception data but ordinary data.  
That which is un-ordinary cannot be described much less identiﬁed until that which is ordinary is described ﬁrst.  To 
take a trivial example, without sharing ﬁrewall logs with like ﬁrms you cannot know whether you are the recipient of 
attacks that are just like everyone else’s (thus making you a target of chance) or that you are a recipient of purpose-
built attacks aimed just at you (thus making you a target of choice).
Whatever we measure, it has to be valuable as a mechanism to calibrate spending.  Unfortunately for industry, a set of 
generally accepted accounting principles may be in place for ﬁnance but it is far from in place for security.
Metrics: adapt not create
• Beg, borrow, and steal from
• Public health (CDC)
• Accelerated failure time testing (MTTR)
• Insurance (ALE, Cat Bonds)
• Portfolio management (VAR)
• Physics (scale-free networks)
The future belongs to the quants, full stop.  As such, and bearing in mind the critical need for security at this time, we 
must borrow from other ﬁelds as we have no time to invent everything from scratch.  We are likewise lucky that at this 
time the ﬁeld has the maximum of hybrid vigor in that all of its leaders were trained at something else hence our ability 
to extract from that “something else” is maximal.
For a sense of this, see , or .
Metrics: info sharing
• Models for information sharing
• Central, mandatory, top-down
• Speciﬁc, enlightened self-interest
• Exception vs routine
In a shared infrastructure at the national scale, some metrics will necessarily concern themselves with shared data.  The 
nature of shared data is the focus of the next three slides, viz., whether that data is mandatorily with a central 
authority, shared amongst like entities on the basis of enlightened self interest, or whether some other form of sharing 
is appropriate and, if so, whether the data shared is routine data or exception data.
Sharing: top down
• Centers for Disease Control
• Mandatory reporting of 
communicable diseases
• Longitudinal analysis of incidence and 
prevalence (with lab conﬁrmation)
• Away teams to handle outbreaks 
(hemorrhagic fevers like Ebola)
The United States Centers for Disease Control play a global role, and no public health practitioner fails to read the 
weekly Mortality and Morbidity Report.  In a paper published in the Proceedings of the (August 2002) USENIX Security 
Symposium, Staniford, et al., proposed a CDC for the Internet.  It would parallel the role of the existing CDC in that it 