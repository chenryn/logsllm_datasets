tools, that existing tools are not perceived as useful for the
particular digital-safety concerns of other at-risk populations,
or simply that researchers have not fully investigated this
question with other at-risk populations.
Tracking and monitoring applications. Caregivers (of at-
risk users who rely on a third party) sometimes used tracking
or monitoring applications with the goal of protecting the at-
risk user’s digital safety. Parents of foster teens reported using
router settings to limit internet access at certain times of day
and parental control apps to watch for potentially dangerous
behavior [12]. These applications can create conﬂict between
autonomy and privacy, but some children and teenagers indi-
cated that they can be helpful, particularly when used as part
of an ongoing dialogue with parents [37, 39].
VI. BARRIERS TO PROTECTIVE PRACTICES
In the previous section, we documented a range of protective
practices at-risk users employed, motivated by the risk factors
they experienced. In our thematic analysis, we also identiﬁed a
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
92352
variety of barriers that limited or prevented at-risk users from
eﬀectively adopting these practices. We apply the contextual
risk factors and protective practices of our at-risk framework
to discuss three categories of barriers: competing priorities,
lack of knowledge or experience, and broken technology as-
sumptions.
A. Competing priorities
Digital safety is not and cannot always be the top priority for
all users; this is perhaps even more true for at-risk users with
competing, often critical, needs. Some of the many competing
priorities that appeared in our dataset included basic needs like
food, income, or physical health and safety; social participation
and compliance with social norms; and caring for others. We
found that speciﬁc competing priorities were often associated
with particular risk factors.
It is well understood that in general, users prioritize con-
venience, simplicity, and their tasks and goals over digital
safety [1, 44]. Our analysis revealed a tendency for at-risk
users to have layered or more severe conﬂicts between digital
safety and other needs, making the leap to safer behaviors
especially diﬃcult.
Basic needs. People who are resource or time constrained in
our dataset often prioritized other critical needs over digital
safety, despite the potential for increased digital-safety risk.
For example, Elliott and Brody [31] reported that low-income
Black New Yorkers used apps to ﬁnd cheaper food, despite
suspecting the apps were insecure. Similarly, people experi-
encing homelessness described discomfort using public Wi-
Fi, but used it anyway for critical needs such as applying for
government assistance, housing, and jobs [97].
Relatedly, people who rely on a third party often gave up
control of digital safety to accomplish basic tasks. Refugees,
including pass-
for example, shared account
words, with caseworkers to obtain social services or apply
for jobs [96]. Some people who are visually impaired used
crowdsourced assistive technology for tasks like identifying
medicines correctly, despite the risk of exposing sensitive
content in the background of the photos crowdworkers would
evaluate [7]. Frik et al. [36] found that some older adults were
willing to accept in-home surveillance, giving up privacy to
maintain some autonomy and independent living.
Participation and connection. As noted in SectionV-B above,
people who experience marginalization sometimes chose to
distance or fully disconnect
in order to keep themselves
safe. Members of these populations who opted to engage
were often reported as knowing that it increased their risk.
For example, Blackwell et al. reported on LGBT+ parents
implicitly “outing” themselves via “everyday” social media
posts about their children and families [16].
We noted similar behavior related to legal or political risk,
where the need to communicate with other activists, both
to organize events and simply to be part of a community,
motivated potentially risky modes of communication [90].
information,
Similarly, maintaining good social standing in a family
or community sometimes required people experiencing the
social norms factor to deprioritize digital safety. Our dataset
included, for example, women in South Asia and Saudi Arabia,
as well as people in South Africa, sharing accounts, devices,
and credentials with family members to meet cultural or
community expectations [8, 86, 88, 89]. Some users in South
Africa accepted Facebook friend requests from strangers, even
though they were uncomfortable with the resultant potential
for sharing, to avoid being rude [86]. Teenagers noted that
they did not discuss or share risky digital-safety experiences
with their parents to avoid awkwardness or a possible negative
reaction [119].
Caring for others. Another example of competing priorities
involved at-risk users taking on additional risk to help, care
for, or support others. We noted this occurring frequently in
connection with the marginalization factor, in part to reduce
overall stigma via normalization. For example, Warner et al.
found that some gay men disclosed their HIV+ status on
a dating app in part to increase visibility [113]; similarly,
separate studies found that LGBTQ+ parents and transgender
individuals posted on social media, despite possible risks, in
part to increase visibility and reassure others that they were
not alone [16, 56].
Our analysis revealed cases in which at-risk users prioritized
others’ autonomy over their own digital safety, particularly
in the case of access to other at-risk users. McGregor et al.
[68] quoted a journalist who valued avoiding “impos[ing] any
kind of burden on a source” over the journalist’s own digital
safety. Similarly, NGO staﬀ with access to other at-risk users
reported the need to balance security with the autonomy of
the survivors they worked with: Chen et al.
[19] quoted a
staﬀ member working with traﬃcking survivors who tended
to suggest that they turn oﬀ phone location, “But I do kind
of leave it up to them. It’s not mandatory.... ’Cause we come
from an empowering place. We don’t want to be telling them
what to do.”
Caring for others can also lead at-risk users to prioritize
eﬃciency in achieving their primary goals over digital safety,
something we noted particularly in the cases of legal or
political, prominence, or access to a sensitive resource, which
frequently coincided with a profession or activity that bene-
ﬁted others. A human rights activist, for example, wondered,
“Should I spend half a day ﬁguring out digital security, or do
work?” [59]. In several studies, journalists, political campaign
workers, and emergency department personnel reported mix-
ing personal and work data and devices for eﬃciency in their
work [21, 68, 90, 99].
B. Lack of knowledge or experience
In our dataset, nearly all at-risk populations had less digital-
safety knowledge than they needed to mitigate the risks
they faced. While prior work has found that general users
tend to have limited digital-safety knowledge [116], this was
exacerbated for at-risk users who, by deﬁnition, faced serious
digital-safety risks and thus usually needed to understand and
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
102353
deploy more robust protections than a typical user. The at-risk
framework helps us understand current patterns in how at-risk
users experience digital-safety knowledge limitations.
Legal or political and prominence tended to attract so-
phisticated attackers, leading to stark knowledge asym-
metries. It was diﬃcult for at-risk populations—like activists,
undocumented immigrants, journalists, or political campaign
workers—to surmount the knowledge diﬀerential needed to
counter the nation-state attackers that might or did target
them [21, 25, 67, 68, 101]. For example, undocumented
immigrants in the U.S. were often unaware that Immigration
and Customs Enforcement might target them for surveillance
via government requests for information to the social media
platforms they used [40].
Relationship with the attacker involved intimate threats
that required expertise to counter. Even in the absence
of sophisticated attacks, these threats can require substantial,
robust protective measures. For example, survivors of intimate
partner abuse contended with a motivated attacker who may
launch repeated attacks leveraging intimate knowledge about
them, physical access to their devices and accounts, and
relational power [21, 34, 57].
Some resource or time constrained users had limited
technology experience. Examples included some people with
low SES, people living in developing regions, and survivors
of intimate partner abuse or traﬃcking, who did not have
regular access to new or trusted devices or the internet,
which limited their ability to gain technology experience and
skills [19, 46, 63, 97]. Other populations, like journalists and
people involved with political campaigns, commonly did not
have the time to develop the technical skills to counter the
digital-safety risks they faced [21, 67, 68]. Several studies re-
ported that at-risk users who are resource or time constrained
(emergency department workers [99], older adults [36, 45],
refugees [96], and others) did not understand digital-safety
settings that were, in theory, available to them.
C. Broken technology assumptions
The atypical threat models at-risk users face sometimes
break assumptions that are built into secure system designs.
Because of these assumptions, digital-safety best practices
and technologies may be inaccessible, non-functional, or only
minimally useful to at-risk users, often with magniﬁed conse-
quences.
One person per device or account. A common assumption of
technology creators is that for every given device and account,
there will be exactly one user who has access, despite prior
work showing that convenience-based device sharing between
trusted parties is common [62].6 This core assumption fails for
several at-risk populations. At-risk users who rely on a third
6We note that since many of the papers in our dataset were published, there
has been progress in addressing this issue, particularly in supporting child and
family accounts. Nonetheless, we believe more can still be done to disrupt
this assumption.
party may share account information with these trusted parties
in order to accomplish important tasks [53, 96] or receive
needed monitoring [76]. Diﬀerent cultural privacy models have
resulted in certain users sharing devices and accounts due to
social norms [4, 89] or even the legal or political requirements
of where they live [8]. Additionally, at-risk users who have a
relationship with the attacker were frequently forced to give
these attackers access to their devices under duress, directly
breaking this assumption [18, 19, 33, 35, 42, 63].
Everyone has suﬃcient technology access. Some security
techniques assume minimum levels of technology access that
are out of reach for some at-risk users who are resource
constrained. For example, 2FA that depends on a mobile
phone (e.g., via SMS or a code-generating app) may not
be available to at-risk users experiencing homelessness who
are unable to consistently pay a phone bill or have an old
device without space for new apps [97]. Some low-income
Black Americans reported needing to stay on family mobile
plans—which could have enabled surveillance from untrusted
relations—because they could not aﬀord separate service [31].
U.S. sanctions on Sudan entirely prevented Sudanese activists
from enabling 2FA on a particular social media platform, since
that platform’s 2FA system was prohibited from recognizing
Sudanese phone numbers [25].
Physical and cognitive capacities are universal. Even for
general users, advice about authentication tends to assume im-
possible cognitive capabilities, such as memorizing a unique,
strong password for every account [80, 107]. This mistaken as-
sumption is compounded in cases of underserved accessibility
needs. For example, several papers have identiﬁed password
memorization as a critical challenge for older adults with
cognitive impairments [36, 76]. Password memorization can be
diﬃcult for people with disabilities that relate to alphanumeric
comprehension, like dyslexia or aphasia [61].
Concepts, values, and experiences are universal. Other
digital-safety paradigms rest on ideas, deﬁnitions, values, and
morals that may not apply universally. Shortcomings in these
assumptions can create digital-safety risks, especially for at-
risk users with diﬀerent social norms. For example, transla-
tions from English to Khmer of concepts like privacy within
social media settings were hard to understand in the strongly
community-oriented culture in Cambodia [46]. Similarly, some
refugees who immigrated to the U.S. came from cultures where
birthdays were not recorded; when these users were assigned a
default value of January 1 in the U.S., authentication systems
that relied on knowledge or entropic distribution of birthdays
were less eﬀective [96]. Separately, Barwulor et al. [13] found
that moral codes and laws enacted by the U.S. government and
enforced by U.S. companies made it diﬃcult for sex workers
in other countries, where their work is legal, to access safe
payment and advertisement platforms, placing their physical
and digital safety at risk.
Limiting digital-safety options is good for everyone. Digital-
safety options that are too numerous can be hard to use [98].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
112354
But limiting the nuanced control enabled by digital-safety
options to meet
the usability needs of typical users may
not always work well for at-risk users. Risk factors that
increased the chances of focused targeting—which included
prominence, relationship with the attacker, reliance on a third
party, and access to other at-risk users—can lead at-risk users
to have highly contextual digital-safety needs. For example,
users protecting against a focused,
intimate attacker, must
account for nuances in the current relationship, the attacker’s
mood, whether or not
they are physically copresent, and
more [57, 63]. Because of this nuance, at-risk users may
beneﬁt from additional options or enhanced transparency that
they can deploy in speciﬁc alignment with their goals. When
these options are not easily available or understood, it may
lead at-risk users to fall back on distancing behaviors in
which they try to stay safe at the cost of fully engaging with
technology [25, 34, 36, 40, 56, 67].
VII. IMPLICATIONS AND FUTURE DIRECTIONS
The at-risk framework can be used in multiple ways by re-
searchers and technology creators, including guiding research
and developing technologies to be inclusive of at-risk users.
A. Research
The framework, as applied to our dataset in Table I, can
be used to help identify where knowledge of at-risk users is
underdeveloped, sparse, or missing, giving researchers a way
to prioritize their eﬀorts. The framework can also be used to
guide development of study designs and research questions.
Identify the who and the what. The digital-safety community
would beneﬁt from research about all at-risk populations and
factors—this includes expanding knowledge about those that
have already been studied and creating new knowledge about
those that have not. In this complex and nuanced space, even
after several papers have reported on a population, researchers
may still have much to learn about the populations’ digital-
safety needs.
Nonetheless, our meta-analysis makes clear that certain at-
risk populations and risk factors have received particularly
limited attention, at least recently and within the digital-safety
community. We observed a general tendency to study par-
ticipants from Western cultures, especially the U.S. Studying
people from other regions and cultures could shed light on new
risks related to social norms, un- or understudied interactions
among risk factors, or even new risk factors. We advocate for
more research involving geographically and culturally diverse
at-risk participants, conducted by researchers who represent
the world.
We also saw multiple studies exploring populations that
experienced marginalization, focusing on a fairly narrow set
of risk experiences (i.e., some populations’ only black circle
((cid:32)) in Table I was for the marginalization factor); future
work could expand our understanding of digital-safety risks
for these populations. Other groups often referenced as at-risk
in popular media (e.g., real estate agents who have access to
sensitive resources [106]; celebrities who face digital-safety
Finally,
risks due to prominence and parasocial relationships [121])
have not been studied in the research venues we analyzed.
New or additional foundational research may be needed to
inform the digital-safety community about these populations’
experiences and needs.
Our framework also highlights how risk factors often com-
bine and interact, but the literature in our dataset has not deeply
explored this topic, precluding a thorough synthesis. We
advocate interactions as a critical area of future research—one
which our framework can support (e.g., enumerating the risk
factors to consider for interactions).
in crisis situations, such as natural disasters or
war, people may lose access to essential resources,
their
circumstances may change, or they may change their behaviors
in exchange for critical services in ways that amplify their risk
of attack or the severity of resulting harms. Recent papers
on the impacts of the COVID-19 pandemic provide evidence
that such changes can create new risks and amplify existing
risks (e.g., [105, 123]). Future work on digital safety during
various kinds of crisis situations could yield a new risk factor,
or expand the deﬁnitions of existing factors.
Guide the how. The framework can also be used to help
shape study designs and reporting, particularly interview or
survey questions. For a population about which little is known,
asking about all 10 contextual risk factors can ensure fairly
comprehensive coverage of digital-safety concerns. For popu-
lations where some research exists, researchers can use the
framework to explore how previously un- or understudied
risk factors may (or may not) apply, or add depth on the
impact of a speciﬁc, previously identiﬁed risk factor of interest.
Researchers can also use the protective practices portion of
the framework to explore more comprehensively how their
participants currently protect themselves and why they choose
their practices. We hope that using the framework to guide
research and reporting can enable better comparisons among
studies, helping to uncover when disparate populations have
overlapping (or distinctive) digital-safety practices and needs.