is implied with an
additional assumption of independence between data points
implied by DP on its own,
it
We then consider in more detail the relationship between
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
356
(Deﬁnition 4). We believe that this explains the claim found
in some papers that DP implicitly assumes independence.
However, we do not share this feeling since the indepen-
dence assumption is not required to get DP to imply the
intuitive consequence (∗) quoted above when interpreting
change as a causal
intervention instead of as associative
conditioning. After reviewing the core concepts of causal
modeling (Section IV), we consider intervening upon all the
data points (Section V-A). As with conditioning upon all the
data points, a deﬁnition intervening on all the data points
(Deﬁnition 6) characterizes DP (Proposition 3) but without the
intuitive focus on a single data point that we desire.
We then consider characterizing DP as intervening upon a
single point (Deﬁnition 7 of Section V-B). A beneﬁt of this
causal characterization is that it is implied by DP without
any assumptions about independence (Proposition 4). An ad-
ditional beneﬁt is that, unlike the associative characterizations,
we do not need side conditions limiting the characterization
to data points with non-zero probabilities. This beneﬁt follows
from causal interventions being deﬁned for zero-probability
events unlike conditioning upon them. These two beneﬁts lead
us to believe that DP is better viewed as a causal property than
as an associative one.
In addition to considering the consequences of DP through
the lenses of association and causation, we also consider how
these two approaches can provide deﬁnitions equivalent to DP.
Table I shows our key results about deﬁnitions that are either
equivalent to DP or might be mistaken as such, which, in the
sections below, we weave in with our aforementioned results
about characterizations of the consequences of DP.
When intervening upon all data points, we get equivalence
for free from Deﬁnition 6 that we already explored as a char-
acterization of the consequences of DP. This free equivalence
does not occur for conditioning upon all data points since the
side condition ruling out zero-probability data points means
those data points are not constrained. Since DP is a restriction
on all data points, to get an equivalence, the deﬁnition must
check all data points. To achieve this, we further require that
the deﬁnition hold on all distributions over the data points,
not just the naturally occurring distribution. (Alternatively, we
could require the deﬁnition to hold for any one distribution
with non-zero probabilities for all data points, such as the
uniform distribution.) We also make similar alterations to the
deﬁnitions looking at a single data point.
Having shown that DP can be viewed as a causal property,
we then consider how this view can inform our understanding
of it. We relate DP to a previously studied notion of effect
size and discuss how this more general notion can make dis-
cussions about privacy more clear (Section VI). In particular,
DP is a bound on the measure of effect size called relative
probabilities (also known as relative risk and risk ratio). That
is, DP bounds the relative probabilities for the effects of each
data point upon the output. Since not all research papers are
in agreement about what counts as an individual’s data point,
spelling out exactly which random variables have bounded
relative probabilities may be more clear than simply asserting
that DP holds for some implicit notion of data point.
DIFFERENTIAL PRIVACY AND VARIATIONS UPON IT. The left-most column gives the number of its deﬁnition later in the text. The point of comparison is
the quantity computed for every pair of values di and d(cid:48)
i are within a factor of
e of one another. The check is for all values of the index i. Some of the deﬁnitions only perform the comparison when the probability of the changed data
i, the changed value) is non-zero under P. Others only perform the comparison when all the data points D having the
point Di having the value di (and d(cid:48)
values d (and d(cid:48) for changed value of Di) has non-zero probability. do denotes a causal intervention instead of standard conditioning [42]. The deﬁnitions
vary in whether they require performing these comparisons for just the actual probability distribution over data points P or over all such distributions. In
i for di to check whether the point of comparison’s values for di and for d(cid:48)
one case (Deﬁnition 4), the comparison just applies to distributions where the data points are independent of one another.
TABLE I
Num.
P
Conditions on population distribution P
Point of comparison (should be stable as di changes)
Relation
Original Differential Privacy
1
n/a
PrA[A((cid:104)d1, . . . , di, . . . , dn(cid:105))=o]
Associative Variants
∀
∀
∀ indep. Di
PrP [D1=d1, . . . , Di=di, . . . , Dn=dn] > 0
PrP [Di=di] > 0
PrP [Di=di] > 0
PrP,A[O=o | D1=d1, . . . , Di=di, . . . , Dn=dn]
PrP,A[O=o | Di=di]
PrP,A[O=o | Di=di]
2
3
4
5
6
7
8
Causal Variants
∀
given
given
∀
is DP
↔ DP
→ DP
↔ DP
PrP,A[O=o | do(D1=d1, . . . , Di=di, . . . , Dn=dn)] ↔ DP
PrP,A[O=o | do(D1=d1, . . . , Di=di, . . . , Dn=dn)] ↔ DP
← DP
PrP,A[O=o | do(Di=di)]
PrP,A[O=o | do(Di=di)]
↔ DP
our work and that of Kasiviswanathan and Smith [25] (Sec-
tion VII). In short, Kasiviswanathan and Smith provide a
Bayesian interpretation of DP whereas we provide a comple-
mentary causal one.
As we elaborate in the conclusion (Section VIII), these re-
sults open up the possibility of using all the methods developed
for working with causation to work with DP. Furthermore, it
explains why researchers have found uses for DP out side of
privacy (e.g., [15], [14], [12], [13], [30]): they are really trying
to limit effect sizes.
II. PRIOR WORK
The paper coining the term “differential privacy” recognized
that causation is key to understanding DP: “it will not be the
presence of her data that causes [the disclosure of sensitive
information]” [11, p. 8]. Despite this causal view being present
in the understanding of DP from the beginning, we believe we
are ﬁrst to make it mathematically explicit and precise, and to
compare it explicitly with the associative view.
Tschantz et al. [46] reduces probabilistic noninterference (a
notion of having no ﬂow of information) to having no casual
effect at all. We observe that DP with  = 0 is identical to
noninterference, implying that the  = 0 case of DP could be
reduced to causal effects. Our work generalizes from non-
interference to DP and thereby differs in having additional
bookkeeping to track the size of the effect for handling the
 > 0 case, where an effect may be present but must be
bounded. Importantly, this generalization allows us to compare
the causal and associative views of DP, not a focus of
[46].
Our work is largely motivated by wanting to explain the
difference between two lines of research papers that have
emerged from DP. The ﬁrst line, associated with the inventors
of DP, emphasizes differential privacy’s ability to ensure that
data providers are no worse off for providing data (e.g., [11],
[25], [36], [35]). The second line, which formed in response to
limitations in differential privacy’s guarantee, emphasizes that
an adversary should not be able to learn anything sensitive
about the data providers from the system’s outputs (e.g., [27],
[28], [32], [29], [23], [5], [50], [33]). The second line notes
that DP fails to provide this guarantee when the data points
from different data providers are associated with one another
unless one assumes that
the adversary knows all but one
data point. McSherry provides an informal description of the
differences between the two lines [36]. While not necessary
for understanding our technical development, Appendix A
provides a history of the two views of DP.
Kasiviswanathan and Smith look at a different way of
comparing the two views of DP, which they call Semantic
Privacy [25]. They study the Bayesian probabilities that an
adversary seeing the system’s outputs would assign to a sensi-
tive property. Whereas other works looking at an adversary’s
beliefs, such as Pufferﬁsh [29], bounds the change in the
adversary’s probabilities before and after seeing the output,
Kasiviswanathan and Smith bound the change between adver-
sary’s probabilities after seeing the output for two difference
inputs, much as DP compares output distributions for two
different inputs. They conclude that this posterior-to-posterior
comparison captures the epistemic consequences of DP, unlike
the anterior-to-posterior comparison made by Pufferﬁsh-like
deﬁnitions, since DP bounds it without additional assump-
tions, such as independent data points. Our causal deﬁni-
tions (Def. 5–8) instead expose differential privacy’s causal
nature with a modiﬁcation of Pearl’s causal framework as
a frequentist effect size and we do not use any Bayesian
probabilities in our causal deﬁnitions. We view their Bayesian
non-causal characterization of DP as complimentary to our
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
357
frequentist causal characterization, with theirs focused on an
adversary’s knowledge and ours on physical constraints. (We
conjecture that a Bayesian causal characterization should be
possible, but leave that to future work.) Besides the conceptual
difference, our characterization is tighter in that we show an
exact equivalence between our central deﬁnition (Def. 8) and
DP in that each implies the other with the same value of ,
whereas their implications hold for an increased value of .
Section VII considers their work in more detail.
Others have explored how assumptions about the data or
adversary enables alternative reductions of DP to information
ﬂow properties. Clarkson and Schneider prove an equivalence
between DP and an information-theoretic notion of information
suppression while making the strong adversary assumption [6,
p. 32]. After making the strong adversary assumption, Cuff and
Yu have argued that DP can be viewed a constraint on mutual
information [8, p. 2], but McSherry points out that the connec-
tion is rather weak [37]. Alvim et al. bound the min-entropy
and mutual
information in terms of  under assumptions
about the data’s distribution [1, p. 9]. Ghosh and Kleinberg
provide inferential privacy bounds for DP mechanisms under
assumptions about restricted background knowledge [20, p. 6].
We avoid such assumptions and our causal version of DP
(Def. 8) is equivalent to the original, not merely a bound.
Instead of looking at how much an adversary learns about a
single data point, Barthe and Köpf bound how much adversary
learns, in terms of min entropy, about the whole database from
a differentially private output, while sometimes making the
strong adversary assumption [2, p. 4]. They prove that as the
database increases size, the bound increases as well. McGregor
et al. similarly bound the amount of information leaked, in
terms of mutual information, about the whole database by
a differentially private protocol (the information cost), while
sometimes assuming independent data points [34, p. 14]. We
focus on privacy consequences to individuals, that is, on one
data point at a time.
Other papers have provided ﬂexible or convenient asso-
ciative deﬁnitions not limited to attempting to capture DP.
For example, Pufferﬁsh is a ﬂexible framework for stating
associative privacy properties [29]. Lee and Clifton explore
bounding the probability that the adversary can assign to an
individual being in a data set [31]. While such probabilities
are more intuitive than the  of DP, their central deﬁnition
implicitly makes a strong adversary assumption [31, Def. 4].
III. DIFFERENTIAL PRIVACY AS ASSOCIATION
Dwork provides a well known expression of DP [11, p. 8].
i in D, and for all output values o,
In our notation, it becomes
Deﬁnition 1 (Differential Privacy). A randomized algorithm
A is -differentially private if for all i, for all data points
d1, ..., dn in Dn and d(cid:48)
PrA[A((cid:104)d1, ..., dn(cid:105))=o] ≤ e PrA[A((cid:104)d1, ..., d(cid:48)
i, ..., dn(cid:105))=o]
This formulation differs from Dwork’s formulation in four
minor ways. First, for simplicity, we restrict ourselves to only
considering programs producing outputs over a ﬁnite domain,
allowing us to use notationally simpler discrete probabilities.
Second, we change some variable names. Third, we explicitly
represent that the probabilities are over the randomization
within the algorithm A, which should be understood as physi-
cal probabilities, or frequencies, not as epistemic probabilities,
or Bayesian credences. Fourth, we use the bounded formula-
tion of DP, in which we presume a maximum number n of
individuals potentially providing data. In this formulation, it
is important that one of the possible values for data points is
the null data point containing no information to represent an
individual deciding to not participate.
Both Dwork’s expression of and our re-expression of DP
make discussing the concerns about dependencies between
data points raised by some papers difﬁcult since it does not
mention any distribution over data points. This omission is
a reﬂection of the standard view that DP does not depend
upon that distribution. However, to have a precise discussion
of this issue, we should introduce notation for denoting the
data points. We use Yang et al.’s expression of DP as a starting
point [49, p. 749]:
sup
log
i,S
i,x−i,xi,x(cid:48)
Deﬁnition 4. (Differential Privacy) A randomized
mechanism M satisﬁes -differential privacy, or -
DP, if
DP (M) :=
Pr(r ∈ S | xi, x−i)
Pr(r ∈ S | x(cid:48)
i, x−i)
We rewrite this deﬁnition in our notation as follows:
Deﬁnition 2 (Strong Adversary Differential Privacy). A
randomized algorithm A is -strong adversary differentially
private if for all population distributions P, for all i, for all
data points d1, ..., dn in Dn and d(cid:48)
i in D, and for all output
values o, if
≤ .
and PrP [D1=d1, ..., Di=d(cid:48)
PrP [D1=d1, ..., Di=di, ..., Dn=dn] > 0
i, ..., Dn=dn] > 0
then
PrP,A[O=o | D1=d1, ..., Di=di, ..., Dn=dn]
≤ e ∗ PrP,A[O=o | D1=d1, ..., Di=d(cid:48)
where O = A(D) and D = (cid:104)D1, ..., Dn(cid:105).