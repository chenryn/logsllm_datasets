an encrypted global index, only one token will be sent. But
the global index will introduce extra interactions between
diﬀerent nodes because it does not consider the data local-
ity. As shown in our experiment later, the encrypted local
indexes outperform the global one, i.e., 3 times faster when
querying the same number of encrypted data values. As a
result, the overhead introduced by the transmission of to-
kens is much smaller than the overhead introduced by the
interactions. Besides, the local indexes of individual nodes
can be processed in parallel, so the beneﬁts of incremental
scalability are preserved.
3.3 Adding Nodes
When the workloads or the volumes of data rapidly in-
crease, they will exceed the storage and processing power of
the nodes in the cluster. Thus, our system should be able
to accommodate the change by adding new node smoothly,
which is a necessity for distributed systems. Technically,
when the system scales out, the secure data partition algo-
rithm updates the consistent hash ring accordingly. Then
the new node will be assigned to a position on the ring. Af-
ter that, only the aﬀected node (i.e., its neighbor) relocates
the encrypted data and the encrypted indexes.
3.3.1 Data Relocation
Because the proposed encrypted KV store is compatible
with the consistent hashing algorithm, the aﬀected nodes
can directly move the data to the newly added ones from
the update location. For example, assume that node A is
assigned to store the data with addressing label l∗ ∈ [x, y),
where x is the position of its preceding neighbor B, and y
is the position of A in the ring. If a new node C is added
between A and B with position z, the data will be moved to
C if its label l∗ ∈ [x, z). During the procedure, the relocated
data is still strongly protected.
Index Relocation
3.3.2
Recall that the encrypted indexes co-locate with the data
on the same nodes. Therefore, when the data are relocated,
the index should also be moved to the same target node. In
553terms of security, the movement for the index is implemented
in a client-assisted manner, i.e., rebuilding the aﬀected in-
dex at the client. If we give the tokens to enable the cloud
to partition the index, the structural information will be re-
vealed; that is, the data associated with same attributes will
be known without being queried. We note that the rebuild-
ing operation is usually inevitable when SSE-based indexes
are out of capacity [5, 10, 19]. But our design only requires
to rebuild the aﬀected local indexes rather than rebuild-
ing all of them. To maintain the availability, the involved
nodes can create auxiliary indexes for caching the update
during the asynchronous rebuilding process. After the com-
pletion of movement, the entries in auxiliary indexes will be
re-inserted to the new indexes.
3.4 Secure Transformation Layer
In order to fully leverage the encrypted KV store for
security, usability, and functionality, we leverage a secure
transformation layer to transform the plaintext requests and
queries to the basic APIs for the access of the encrypted KV
store. First, it encrypts the data values and the data at-
tributes before outsourcing them to the nodes in the public
cloud.
It conducts secure data modeling at the client to
hide the structural information of data formated from dif-
ferent models. Second, the transformation layer can also
phase queries with diﬀerent functionalities on the secondary
attribute, and generate secure tokens for the computational
operations over encrypted indexes.
We emphasize that our design ﬁts with the trend of
NoSQL development. A new class of database engine has
emerged to begin supporting multiple data models on top
of a single KV store [13, 14, 27, 35].
In that way, the KV
stores erase the complexity of data operation and develop-
ment, and beneﬁt many diﬀerent kinds of applications [17].
Meanwhile, most of the NoSQL systems [12, 25, 33] support
the client to submit the queries on the secondary attributes.
The transformation layer allows the client to use our system
transparently.
4. SECURITY ANALYSIS
In this section, we demonstrate that our system guaran-
tees the data conﬁdentiality for both secure requests over the
encrypted KV store and secure queries over the encrypted lo-
cal indexes. Regarding the proposed encrypted data store,
each entry is a pair of a pseudo-random label and an en-
crypted data value. Even if all the nodes are compromised
or collude, it only gives the total number of data. The data
conﬁdentiality and the relationships between the underly-
ing data are still protected. Even if the encrypted data are
relocated, the security strength will not be compromised.
Regarding the security of encrypted index, our design is
built from the framework of SSE. Without querying, no in-
formation of index is known. And each query and the re-
sult will not give any information beyond the current query.
We deﬁne the leakage function in our system, follow the
simulation-based security deﬁnition [10, 19], and present se-
curity proof against adaptive chosen-keyword attacks. Ex-
plicitly, the leakage function of Build is:
L1(C) = ({mi|i ∈ [1, n]}n,|l
∗|)
where C is the set of indexed attributes, mi is the size of
local index Ii, n is the number of nodes, and |l∗| is the
length of pseudo-random label. The leakage function for a
given Query operation is given:
∗
, v
∗}ci}n, i ∈ [1, n])
2}n,{{l
L2(C) = ({ti
1, ti
For an attribute C, {ti
2}n are the tokens for total n nodes
respectively. Each Query reveals the number ci of (l∗, v∗)
pairs at each node. For q number of Query queries, the
query pattern is deﬁned:
1, ti
L3(Q) = (Mq×q,{{ci}n}q)
Q is q number of adaptive queries. Mq×q is a symmet-
ric bit matrix to trace the queries performed on the same
attributes. Mi,j and Mj,i are equal to 1 if ti
1 for
i, j ∈ [1, q]. Otherwise, they are equal to 0. {ci}n is a set
of counters that record the number of values for the query
attribute on each node. Accordingly, we present the security
deﬁnition as below:
1 = tj
Deﬁnition 1. Let Φ = (KGen, Build, Query) be the en-
crypted and distributed index construction. Given leakage
L1, L2 and L3, and an adversary A and a simulator S, de-
ﬁne the following experiments.
RealA(k): The client calls KGen(1k) to output a pri-
vate key K. A selects a dataset D and asks the client to
build I via Build. Then A performs a polynomial number
of q queries, and asks the client for tokens and ciphertext.
Finally, A returns a bit as the output.
IdealA,S (k): A selects D. Then S generates I for A
based on L1. A performs a polynomial number of adaptive q
queries. From L2 and L3, S returns the simulated ciphertext
and tokens. Finally, A returns a bit as the output.
Φ is (L1, L2, L3)-secure against adaptive chosen-keyword
attacks if for all probabilistic polynomial time adversaries
A, there exists a simulator S such that P r[RealA(k) = 1]−
P r[IdealA,S (k) = 1] ≤ negl(k), where negl(k) is a negligible
function in k.
Theorem 1. Φ is
(L1, L2, L3)-secure against adap-
tive chosen-keyword attacks if SE is CPA-secure, and
H, P, F1, F2, G1, G2 are secure PRF.
Proof. Given L1, the simulator S can generate the sim-
ulated index I(cid:48) at each node, which is indistinguishable from
the real index I. The number of the entries is identical. The
size of the real entry and the simulated one is the same. But
S generates random strings for each entry. From L2, S can
simulate the ﬁrst query and the result. For the simulated
local index of each node, S randomly selects the same num-
ber ci of entries as the query on the real index. Here, the
entry is selected by a random string t(cid:48)
1. Then the token can
be simulated such as: t(cid:48)
is identical
to l∗, and mapped to the simulated entry. For the subse-
quent queries, if L3 indicates the query appearing before, S
will select exactly the same entries and use the same tokens
generated before. Otherwise, S simulates the query and the
result by following the procedure of the ﬁrst query based on
L2. Due to the pseudo-randomness of PRF, the adversary
cannot distinguish between the real index and the simulated
one for each node, and the query tokens and the results from
real indexes and the simulated ones.
2 = l∗(cid:48) ⊕ β(cid:48), where l∗(cid:48)
554(a) Put throughput
(b) Get throughput
(c) Put latency in write-heavy workloads
(d) Get latency in write-heavy workloads (e) Put latency in read-heavy workloads
(f) Get latency in read-heavy workloads
Figure 4: Put and Get performance evaluation
5. EXPERIMENTAL EVALUATION
5.1 System Implementation
We implement the system prototype and deploy it to Mi-
crosoft Azure. We create a cluster that consists of 6 Stan-
dard_D12 instances as the nodes of the encrypted KV store
and 9 Standard_A4 instances as the clients of data appli-
cations. Each Standard_D12 instance is assigned with 4
vCores, 28GB RAM and 200GB SSD, and each Standard_A4
instance is assigned with 8 vCores, 14GB RAM, and 200GB
SSD. They are installed with Ubuntu Server 14.04. Each
instance runs with up to 100 threads to generate the work-
load for performance and scalability evaluation. Because the
clients in our system have to perform cryptographic opera-
tions, we use more instances with more vCores to avoid the
client bottleneck. In current prototype, we choose to cache
the consistent hashing ring at the client for request routing.
Thus, we do not need to select a speciﬁc node as the dis-
patcher. This mechanism is also used in DynamoDB [11]
to save communication costs with the dispatcher and reduce
the request latency. The ring can be updated periodically
to keep the data partition fresh.
We set up Redis 3.0.5 at one node and create an Azure
image to duplicate the Redis environment to other nodes.
The secure transformation layer is currently deployed at the
client via C++. The operations on the nodes are also im-
plemented via C++. The cryptographic building blocks
are implemented via OpenSSL. PRF is implemented via
HMAC-SHA2, and symmetric encryption is implemented
via AES/CBC-256. Besides, we preload totally 20, 000, 000
data values (10 bytes for each) to our encrypted KV store
before starting the experiment.
5.2 Performance Evaluation
The evaluation of our proposed KV store targets on re-
quest and query performance, system scalability, and secu-
Secure Put and Get
rity overhead. We will measure the Put and Get throughput,
the Put and Get latency under diﬀerent workloads, the cost
of data relocation and index relocation when the system
scales out, and the cost of the secure query over the en-
crypted local indexes. Speciﬁcally, we compare our Put and
Get performance with directly using Redis to access plain-
text data. And we also compare the query performance of
the encrypted local indexes and the encrypted global index,
where the latter treats the KV store as the black box and
directly adopts the design in [5].
5.2.1
To evaluate the scalability of our system, we ﬁrst report
the throughput for Put and Get respectively. To do so, the
client threads are continuously increased to generate the
workloads till the throughput stops increasing. By using dif-
ferent number of nodes, we capture the total number of han-
dled requests for a duration of 100s to obtain the throughput
when each of the nodes is fully loaded. From Figure 4-(a)
and Figure 4-(b), the throughput of the encrypted KV store
scales linearly along with the number of vCores, and achieves
up to 1.9 × 106 Put/s and up to 2.0 × 106 Get/s. Compar-
ing to non-encrypted Redis, the throughput of Put and Get
have 27% and 28% loss respectively. The security overhead
comes from the costs of HMAC-SHA2 and AES encryption.
And the size of ciphertext (i.e., 32 bytes) is larger than the
plaintext (i.e., 10 bytes), which degrades the performance.
Next, we measure the latency for Put and Get under dif-
ferent kinds of workloads to gain deeper understanding on
the performance of the proposed encrypted KV store. Two
typical workloads for data-intensive applications are simu-
lated [9]. One is write-heavy with 50% Put and 50% Get
requests. The other is read-heavy with 5% Put and 95% Get
requests. The latency is measured by adding client threads
until the throughput of each of 6 nodes stops increasing.
As shown in Figure 4-(c) and Figure 4-(d), Put and Get
vCores0481216202428Throughput ops/s060K120K180K240K300KPutPut (Redis)vCores0481216202428Throughput ops/s060K120K180K240K300KGetGet (Redis)Thoughput (ops/s)060K120K180K240K300KLatency (ms)00.20.40.60.81PutPut (Redis)Thoughput (ops/s)060K120K180K240K300KLatency (ms)00.20.40.60.81GetGet (Redis)Thoughput (ops/s)060K120K180K240K300KLatency (ms)00.20.40.60.81PutPut (Redis)Thoughput (ops/s)060K120K180K240K300KLatency (ms)00.20.40.60.81GetGet (Redis)555Figure 5: Time cost of adding 1 node
(a) Secure query on 3 nodes
achieve millisecond latency for write-heavy workloads, less
than 1ms per Put and per Get, when the average throughput
on each node reaches around 31, 912 ops/s. In Figure 4-(e)
and Figure 4-(f), Put and Get also achieve low latency for
read-heavy workloads, less than 1ms per Put and per Get,
when the average throughput on each node reaches around
32, 500 ops/s. The comparison to non-encrypted Redis in-
dicates that the Get and Put latency is comparable. But
we observe that the latency will increase gradually when
the workloads reach 80% of maximum throughput. While
for the non-encrypted Redis, the latency is still stable at
the same scale of the workloads. We note that this impact
on system performance is inevitable due to the strong pro-
tection of data conﬁdentiality, especially when the size of
plaintext data value is less than the size of block cipher. To
achieve comparable performance in heavy workloads, more
storage nodes will be required.
Scaling Out
5.2.2
When the growing workloads or the increasing amount
of inserted data exceed the workload or the capacity of the
node, our system can add new nodes smoothly, i.e., support-
ing incremental scaling over encrypted data and encrypted
local indexes. In this experiment, the incremental scalabil-
ity is evaluated by the time cost of adding one node into our
system. Figure 5 depicts the cost for data relocation and
index relocation respectively. Both data and index reloca-
tion costs increase linearly along with the number of aﬀected
data on the corresponding nodes. The time cost of data re-
location is higher than the time cost of index relocation, e.g.,
419s and 141s for 100, 000 data values, respectively.
In this experiment, the aﬀected indexes are directly built
by executing Put requests, while the old one is set invalid and
will be evicted later. This treatment optimizes the rebuild-
ing process for index relocation. While for data relocation,
the node will ﬁrst scan the KV pairs and then execute Put
requests for the data movement, so it takes longer time.
5.2.3 Secure Query Performance
To show the eﬃciency and the scalability of the proposed
local index framework, we evaluate the secure query per-