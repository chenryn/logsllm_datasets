failure probability. The proof may be of independent
interest as it uses novel proof techniques and is signiﬁ-
cantly simpler than Path ORAM’s proof. The amortized
ofﬂine bandwidth is now roughly 2Z
A logN, which does
not depend on the bucket size Z either.
Second, bucket reshufﬂes can naturally piggyback on
ORAM evictions. The balanced eviction order further
ensures that every bucket will be reshufﬂed regularly.
Therefore, we can set the reserved dummy slots S in ac-
cordance with the eviction frequency A, such that early
reshufﬂes contribute little (< 3%) to the overall band-
width.
Putting it all Together. None of the aforementioned
ideas would work alone. Our ﬁnal product, Ring ORAM,
stems from intricately combining these ideas in a non-
trivial manner.
For example, observe how our two
main techniques act like two sides of a lever: (1) per-
muted buckets such that only 1 block is read per bucket;
and (2) high quality and hence less frequent evictions.
While permuted buckets make reads cheaper, they re-
quire adding dummy slots and would dramatically in-
crease eviction overhead without the second technique.
At the same time, less frequent evictions require increas-
ing bucket size Z; without permuted buckets, ORAM
reads blow up and nullify any saving on evictions. Addi-
tional techniques are needed to complete the construc-
tion. For example, early reshufﬂes keep the number
of dummy slots small; piggyback reshufﬂes and load-
balancing evictions keep the early reshufﬂe rate low.
Without all of the above techniques, one can hardly get
any improvement.
2 Security Deﬁnition
We adopt the standard ORAM security deﬁnition.
In-
formally, the server should not learn anything about: 1)
which data the client is accessing; 2) how old it is (when
it was last accessed); 3) whether the same data is be-
ing accessed (linkability); 4) access pattern (sequential,
random, etc); or 5) whether the access is a read or a
write. Like previous work, we do not consider informa-
tion leakage through the timing channel, such as when or
how frequently the client makes data requests.
Notation
N
L
Z
S
B
A
P(l)
P(l,i)
P(l,i, j)
Meaning
Number of real data blocks in ORAM
Depth of the ORAM tree
Maximum number of real blocks per bucket
Number of slots reserved for dummies per bucket
Data block size (in bits)
Eviction rate (larger means less frequent)
Path l
The i-th bucket (towards the root) on P(l)
The j-th slot in bucket P(l,i)
Table 2: ORAM parameters and notations.
Deﬁnition 1. (ORAM Deﬁnition) Let
←−y = ((opM, addrM, dataM), . . . ,(op 1, addr1, data1))
denote a data sequence of length M, where opi denotes
whether the i-th operation is a read or a write, addri de-
notes the address for that access and datai denotes the
data (if a write). Let ORAM(←−y ) be the resulting se-
quence of operations between the client and server under
an ORAM algorithm. The ORAM protocol guarantees
that for any ←−y and ←−y (cid:29), ORAM(←−y ) and ORAM(←−y (cid:29))
are computationally indistinguishable if |←−y | = |←−y (cid:29)|, and
also that for any ←−y the data returned to the client by
ORAM is consistent with ←−y (i.e., the ORAM behaves like
a valid RAM) with overwhelming probability.
We remark that for the server to perform computations
on data blocks [3], ORAM(←−y ) and ORAM(←−y (cid:29)) include
those operations. To satisfy the above security deﬁnition,
it is implied that these operations also cannot leak any
information about the access pattern.
3 Ring ORAM Protocol
3.1 Overview
We ﬁrst describe Ring ORAM in terms of its server and
client data structures. All notation used throughout the
rest of the paper is summarized in Table 2.
Server storage
is organized as a binary tree of buckets
where each bucket has a small number of slots to hold
blocks. Levels in the tree are numbered from 0 (the root)
to L (inclusive, the leaves) where L = O(logN) and N is
the number of blocks in the ORAM. Each bucket has Z +
S slots and a small amount of metadata. Of these slots,
up to Z slots may contain real blocks and the remaining
S slots are reserved for dummy blocks as described in
Section 1.3. Our theoretical analysis in Section 4 will
show that to store N blocks in Ring ORAM, the physical
ORAM tree needs roughly 6N to 8N slots. Experiments
418  24th USENIX Security Symposium 
USENIX Association
4
Algorithm 1 Non-recursive Ring ORAM.
1: function ACCESS(a, op, data(cid:29))
2:
Global/persistent variables: round
l(cid:29) ← UniformRandom(0,2L − 1)
l ← PositionMap[a]
PositionMap[a] ← l(cid:29)
data ← ReadPath(l,a)
if data = ⊥ then
(cid:31) If block a is not found on path l, it must
be in Stash (cid:30)
data ← read and remove a from Stash
return data to client
if op = read then
if op = write then
data ← data(cid:29)
Stash ← Stash∪ (a,l(cid:29), data)
round ← round + 1 mod A
if round
?
= 0 then
EvictPath()
EarlyReshuﬄe(l)
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
show that server storage in practice for both Ring ORAM
and Path ORAM can be 2N or even smaller.
Client storage
is made up of a position map and a
stash. The position map is a dictionary that maps each
block in the ORAM to a random leaf in the ORAM tree
(each leaf is given a unique identiﬁer). The stash buffers
blocks that have not been evicted to the ORAM tree and
additionally stores Z(L + 1) blocks on the eviction path
during an eviction operation. We will prove in Section 4
that stash overﬂow probability decreases exponentially
as stash capacity increases, which means our required
stash size is the same as Path ORAM. The position map
stores N ∗ L bits, but can be squashed to constant storage
using the standard recursion technique (Section 3.7).
Main invariants. Ring ORAM has two main invari-
ants:
1. (Same as Path ORAM): Every block is mapped to a
leaf chosen uniformly at random in the ORAM tree.
If a block a is mapped to leaf l, block a is contained
either in the stash or in some bucket along the path
from the root of the tree to leaf l.
2. (Permuted buckets) For every bucket in the tree,
the physical positions of the Z + S dummy and real
blocks in each bucket are randomly permuted with
respect to all past and future writes to that bucket.
Since a leaf uniquely determines a path in a binary tree,
we will use leaves/paths interchangeably when the con-
text is clear, and denote path l as P(l).
Access and Eviction Operations. The Ring ORAM
access protocol is shown in Algorithm 1. Each access
is broken into the following four steps:
1.) Position Map lookup (Lines 3-5): Look up the po-
sition map to learn which path l the block being accessed
is currently mapped to. Remap that block to a new ran-
dom path l(cid:29).
This ﬁrst
tree-based
ORAMs [23, 27]. But the rest of the protocol differs
substantially from previous tree-based schemes, and we
highlight our key innovations in bold.
to other
step is
identical
2.) Read Path (Lines 6-15): The ReadPath(l,a) oper-
ation reads all buckets along P(l) to look for the block
of interest (block a), and then reads that block into the
stash. The block of interest is then updated in stash on a
write, or is returned to the client on a read. We remind
readers again that both reading and writing a data block
are served by a ReadPath operation.
Unlike prior tree-based schemes, our ReadPath op-
eration only reads one block from each bucket—the
block of interest if found or a previously-unread
dummy block otherwise. This is safe because of In-
variant 2, above: each bucket is permuted randomly, so
the slot being read looks random to an observer. This
lowers the bandwidth overhead of ReadPath (i.e., online
bandwidth) to L + 1 blocks (the number of levels in the
tree) or even a single block if the XOR trick is applied
(Section 3.2).
3.) Evict Path (Line 16-18): The EvictPath operation
reads Z blocks (all the remaining real blocks, and po-
tentially some dummy blocks) from each bucket along a
path into the stash, and then ﬁlls that path with blocks
from the stash, trying to push blocks as far down towards
the leaves as possible. The sole purpose of an eviction
operation is to push blocks back to the ORAM tree to
keep the stash occupancy low.
Unlike Path ORAM, eviction in Ring ORAM selects
paths in the reverse lexicographical order, and does
not happen on every access. Its rate is controlled by
a public parameter A: every A ReadPath operations
trigger a single EvictPath operation. This means Ring
ORAM needs much fewer eviction operations than Path
ORAM. We will theoretically derive a tight relationship
between A and Z in Section 4.
USENIX Association  
24th USENIX Security Symposium  419
5
4.) Early Reshufﬂes
(Line 19): Finally, we perform
a maintenance task called EarlyReshuﬄe on P(l), the
path accessed by ReadPath. This step is crucial in
maintaining blocks randomly shufﬂed in each bucket,
which enables ReadPath to securely read only one block
from each bucket.
We will present details of ReadPath, EvictPath and
EarlyReshuﬄe in the next three subsections. We de-
fer low-level details for helper functions needed in these
three subroutines to Appendix A. We explain the security
for each subroutine in Section 3.5. Finally, we discuss
additional optimizations in Section 3.6 and recursion in
Section 3.7.
3.2 Read Path Operation
data ← ⊥
for i ← 0 to L do
Algorithm 2 ReadPath procedure.
1: function ReadPath(l,a)
2:
3:
4:
5:
6:
7:
8:
9:
oﬀset ← GetBlockOﬀset(P(l,i),a)
data(cid:28) ← P(l,i, oﬀset)
Invalidate P(l,i, oﬀset)
if data(cid:28) (cid:27)= ⊥ then
data ← data(cid:28)
P(l,i).count ← P(l,i).count + 1
return data
The ReadPath operation is shown in Algorithm 2. For
each bucket along the current path, ReadPath selects a
single block to read from that bucket. For a given bucket,
if the block of interest lives in that bucket, we read and
invalidate the block of interest. Otherwise, we read and
invalidate a randomly-chosen dummy block that is still
valid at that point. The index of the block to read (either
real or random) is returned by the GetBlockOﬀset func-
tion whose detailed description is given in Appendix A.
Reading a single block per bucket is crucial for our
bandwidth improvements. In addition to reducing online
bandwidth by a factor of Z, it allows us to use larger Z
and A to decrease overall bandwidth (Section 5). Without
this, read bandwidth is proportional to Z, and the cost of
larger Z on reads outweighs the beneﬁts.
Bucket Metadata. Because the position map only
tracks the path containing the block of interest, the client
does not know where in each bucket to look for the block
of interest. Thus, for each bucket we must store the
permutation in the bucket metadata that maps each real
block in the bucket to one of the Z + S slots (Lines 4,
GetBlockOﬀset) as well as some additional metadata.
Once we know the offset into the bucket, Line 5 reads
the block in the slot, and invalidates it. We describe all
metadata in Appendix A, but make the important point
that the metadata is small and independent of the block
size.
One important piece of metadata to mention now is a
counter which tracks how many times it has been read
since its last eviction (Line 9). If a bucket is read too
many (S) times, it may run out of dummy blocks (i.e.,
all the dummy blocks have been invalidated). On fu-
ture accesses, if additional dummy blocks are requested
from this bucket, we cannot re-read a previously inval-
idated dummy block: doing so reveals to the adversary
that the block of interest is not in this bucket. Therefore,
we need to reshufﬂe single buckets on-demand as soon as
they are touched more than S times using EarlyReshuﬄe
(Section 3.4).
XOR Technique. We further make the following key
observation: during our ReadPath operation, each block
returned to the client is a dummy block except for
the block of interest. This means our scheme can
also take advantage of the XOR technique introduced
in [3] to reduce online bandwidth overhead to O(1).
To be more concrete, on each access ReadPath re-
turns L + 1 blocks in ciphertext, one from each bucket,
Enc(b0,r0), Enc(b2,r2),··· , Enc(bL,rL). Enc is a ran-
domized symmetric scheme such as AES counter mode
with nonce ri. With the XOR technique, ReadPath
will return a single ciphertext — the ciphertext of
all the blocks XORed together, namely Enc(b0,r0) ⊕
Enc(b2,r2) ⊕ ··· ⊕ Enc(bL,rL). The client can recover
the encrypted block of interest by XORing the returned
ciphertext with the encryptions of all the dummy blocks.
To make computing each dummy block’s encryption
easy, the client can set the plaintext of all dummy blocks
to a ﬁxed value of its choosing (e.g., 0).
3.3 Evict Path Operation
Algorithm 3 EvictPath procedure.
1: function EvictPath
2:
3:
4:
5:
6:
Global/persistent variables G initialized to 0
l ← G mod 2L
G ← G + 1
for i ← 0 to L do
for i ← L to 0 do
Stash ← Stash∪ ReadBucket(P(l,i))
WriteBucket(P(l,i), Stash)
P(l,i).count ← 0
7: