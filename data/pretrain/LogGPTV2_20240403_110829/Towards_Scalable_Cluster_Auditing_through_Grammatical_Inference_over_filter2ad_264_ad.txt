explanation. For clarity, we provide a simpliﬁed example of
state merging in Figure 9 that merges the provenance of two
chained Hadoop (map/reduce) jobs. The τpref ix of vertex 1
is empty {} while τpref ix for vertex 5 consists of vertices
{1, 2, 3, 4}. After vertices 1 and 5 are merged, the τpref ix of
5 becomes empty {}, and the τpref ix of downstream vertices
are also updated to remove {1, 2, 3, 4}, e.g., vertex 8’s τpref ix
changes from {1, 2, 3, 4, 5} to {5}. This makes the τpref ix
of vertices 5,6,7, and 8 equivalent to the τpref ix of vertices
1,2,3,4, enabling the repetitive subgraphs to be combined into
the new grammar. While here we describe state merging for
τpref ix only, the process is identical for τsuf f ix.
The candidate graph grammars from merging step are
general as they can accept/parse more graphs than the initial
(speciﬁc) graph grammar. Then, the MDL cost of each candi-
date grammar is calculated using the MDL function according
to equation 1, and is added to the explore PriorityQueue.
Whenever a new grammar is popped from explore, the gram-
mar with the minimal MDL cost is returned. The process
of merging states and adding new grammars to explore
is repeated until either explore is empty (i.e., convergence
is achieved) or the algorithm is terminated by an external
decision such as killing the process or exceeding a time limit.
In our implementation, we set the termination point when
convergence is achieved.
F. Provenance Graph Membership Test and Update
The ﬁnal component required Winnower is a membership
test that, given a grammar and an instance provenance graph,
determines whether or not the graph can be produced by the
grammar. The membership test algorithm follows naturally
from graph grammar parsing algorithms. At a high-level,
function PARSE shown in Figure 8 takes as input a graph Dag
and a grammar Gram. Then, it generates the preﬁx state tree
τpref ix and sufﬁx state tree τpref ix for each vertex in Dag.
This step is similar to the bootstrapping step of induction.
8
java (1)javamapper (2)data(4)javareducer (3)java (5)java  mapper (6)data (8)javareducer (7)javajavamapperdatajavareducerState       Mergingthe rules deﬁned in /etc/audit/audit.rules. Winnower uses
all the audit.rules to capture the syscalls events that can
be useful in attack tracing, such as process, ﬁle, and socket
manipulation. After the syscall is processed by the kernel,
auditd sends data from the kernel to the user-space daemon
which writes the event to a persistent log ﬁle. auditd writes
SELinux labels along with other event information such as
process id into the logs. To differentiate the provenance of dif-
ferent containers, Winnower extends SPADE to communicate
with the Docker Swarm and map each objects’ SELinux labels
to the associated container-id and image-id given by Docker
to ﬁnd containers belonging to same applications. Winnower
then uses SPADE’s Graphviz6 backend to record container-
speciﬁc provenance graphs and performs DFA learning over
the resulting dot ﬁles.
The Winnower agent runs locally on each worker node in
the cluster and communicates with the Monitor’s Winnower
frontend. After performing graph abstraction and local induc-
tion as discussed in Sections III-D and III-E, it is responsible
for publishing local models to the Winnower frontend via a
publisher-subscriber model at a conﬁgurable interval (epoch).
We used Apache Kafka pub/sub system. The Winnower agent
also waits for instructions from the Winnower frontend related
to provenance queries, changes in epoch size, or deploy-
ing provenance policies. After each epoch t, the Winnower
performs graph grammar induction on the worker’s current
provenance graph.
Monitor Components. The Monitor node is responsible
for running the Docker Swarm manager, and is extended by
Winnower to run a frontend consisting of ﬁve submodules: 1)
a Provenance Manager submodule gathers provenance graphs
from each worker node and sends back the current globally
aggregated model. 2) a Provenance Query submodule that
supports forward and backward tracing queries. The three
functions provided by Winnower to support tracing are shown
in the Table I. The user ﬁrst identiﬁes nodes of interest with the
getNodes by specifying a key-value pair (e.g., key=“name”,
value=“index.html”). These node IDs can then be passed to
the getAncestors or getDescendants functions to perform
backward and forward tracing, respectively. To track the
migration of workers in dynamic scheduling environments,
Winnower maintains a log of the scheduling decisions made
by Docker Swarm and transparently identiﬁes which nodes to
query to reconstitute the full provenance graph. 3) a Policy
Engine submodule exposes a simple Cypher-like [12] graph
query language that permits administrators to deﬁne automated
responses when a speciﬁed property is detected within a
worker’s provenance graph. The format of policy is shown in
the Figure 11. In the MATCH clause the pattern to match is given
while RETURN will send matched vertex id to administrator to
run forward/backward queries. Figure 12 shows an example
policy. Here, if any process writes to the /usr/bin/ directory
on a worker node, the administrator will be notiﬁed. 4) ﬁnally,
a Docker API Calls submodule uses Docker Swarm API
to get
information regarding containers in cluster such as
which containers belong to same information, and liveness of
containers. 5) a ﬁnal component of the Winnower frontend
is the Provenance Learning submodule. After each epoch t,
the Provenance Manager fetches new provenance graph from
6Available at http://www.graphviz.org/
Fig. 10: Winnower Architecture and Workﬂow (§IV).
TABLE I: Winnower API functions for attack tracing on provenance
graphs generated from graph grammars models.
getNodes(key, value) → node ids
getAncestors(node id) → graph
getDescendants(node id) → graph
MATCH vertex a:{labels} edge a
vertex b:{ labels } edge b
...
RETURN vertex a.id
Fig. 11: Format of Provenance Policy Language to check certain
provenance DAG pattern on each worker node.
MATCH (a:Process {name:"*"}) used
(b: File { ﬁle path :"/usr/bin/*", operation:"write"})
RETURN a.id
Fig. 12: Example of Provenance Policy which monitors any process
writing to /usr/bin directory.
workers and parses them into the scala graph format using
the scalax package [3]. The Provenance Learning submodule
ﬁrst checks if it already has the graph grammar model for the
worker. If there is a model previously generated then it will
be updated through induction to incorporate the new graphs
Otherwise the provenance learning system merges the worker
graph model from the current epoch into a single global model,
then sends them back to each worker. We have implemented
provenance graph grammar learning framework in Scala with
3K LOC.
V. PERFORMANCE
In order to evaluate the performance of Winnower, we
proﬁled 3 popular server applications on a ﬁve node cluster
using Docker Swarm. Workloads were generated for these
applications using the standard benchmarking tools Apache
Benchmark ab7, FTPbench8, and SysBench9 which were also
used in most relevant prior work [51], [50], [54]. The cluster
was deployed as KVM/QEMU virtual machines on a single
server running Ubuntu 16.04 LTS with 20 Intel Xeon (E5-
2630) CPUs with 64 GB RAM. One VM in the cluster acted
as the Monitor, running the Winnower Frontend and Docker
Swarm manager, while the remaining four VMs hosted worker
containers. Each VM had 2 VCPUs, 4GB RAM, and ran
CentOS 7. We deployed total 20 application containers for
7Available at https://httpd.apache.org/docs/2.4/programs/ab.html
8Available at https://pypi.python.org/pypi/ftpbench
9Available at https://dev.mysql.com/downloads/benchmarks.html
9
DockerDaemonUser SpaceKernelSyscallAuditdNetlinkProvenance CaptureDBWorker Node 1Cont. 1Cont. 2Cont. n…WinnowerAgentOPM  GraphDocker Swarm ManagerWinnower FrontendAPI   CallsGlobal GraphGrammarsLocal Graph Grammars12355DistributedQueryProvenanceGraphs64WinnowerAgentMonitor Node(a) ProFTPD
(b) Apache
(c) MySQL
Fig. 13: Average accumulated audit log size (log scaled) on central node overtime for each application. Winnower generated audit logs (WIN)
with and without induction step for each application are substantially less than auditd/SPADE log (compressed and uncompressed).
(a) ProFTPD
(b) Apache
(c) MySQL
Fig. 14: Average time spent on graph grammar induction and parsing at each epoch, which occurred every 50 seconds.
each benchmark across the cluster. For each workload, the
Monitor sends requests with the concurrency-level parameter
of 40 that were load balanced across the worker nodes. Note
that the total number of requests depends on how long the
benchmark was run. Winnower was conﬁgured with an epoch
size of 50 seconds, with set τF ile and τSock thresholds to 400.
To serve as a baseline comparison for Winnower, we set-up
daemons on each worker that stream auditd activity to the
Monitor node.
Our performance evaluation sets out to answer the follow-
ing questions about Winnower:
– §V-A: What is the overall storage reduction provided by
Winnower’ abstraction and induction techniques? To answer
this question, for each workload we compare the compressed
and uncompressed size of auditd logs to the size of our
Winnower model under two conﬁgurations: abstraction only
(WIN w/o ind.) and abstraction/induction (WIN with ind.).
is the computational cost of generating a
Winnower model? To answer this question, we measure the
induction speed for each epoch in each workload.
– §V-C: What is the network cost of operating Winnower? To
answer this question, we compare the cost of transmitting
Winnower models over the network to the conﬁguration in
which all auditd activity is streamed to the Monitor.
– §V-B: What
A. Storage Reduction
For each workload, we measured the storage requirements
at the Monitor node for both Winnower and auditd. Figure 13
shows the space overhead over time for Winnower as compared
to auditd; note that the y-axis uses a log scale. Table II pro-
vides a total summary of space overhead and graph complexity
10
App
Apache
ProFTPD
MySQL
Duration
33m12s
20m12s
21m00s
# of Vertices
WIN
ASD
32
1.04m
56
340k
840k
61
# of Edges
ASD
1.04m
340k
840k
WIN
41
58
64
Log Size
(MB)
ASD WIN
0.11
485
0.12
630
130
0.17
TABLE II: Summary of observed space overheads in test applications
comparing auditd/SPADE (ASD) to Winnower (WIN). Winnower
consistently reduces storage costs by over three orders of magnitude.
for all three applications. The graph abstraction step (WIN
w/o ind.) accounts for only a small amount of compression
compared to graph induction, but enables the effectiveness of
induction as discussed in §III-D. Approximately 0.6 GB of data
per hour is generated by auditd/SPADE (auditd(uncomp.)) on
central node, in contrast to 150KB per hour by Winnower. With
graph induction enabled (WIN with ind.), Winnower outper-
forms auditd by 3 orders of magnitude, reducing the storage
burden by 99.9%. Even when auditd output is compressed
with 7z tool at the Monitor, Winnower still reduces the storage
burden by 99.2%. Winnower thus dramatically reduces storage
cost at the administrative node.
B. Computational Cost
For each workload, Figure 14 shows the time spent on
graph induction on each node after each 50 second epoch;
that is, each node ingested the 50 seconds of log data, then
performed the graph grammar inference algorithm (§III-E) to
generate a provenance model. We observe that induction during
the ﬁrst epoch is more costly (12-26 seconds) than subse-
quent epochs (0-3 seconds). This is because a signiﬁcantly
larger amount of graph structure is learned during the initial
 0.01 0.1 1 10 100 1000 100 200 300 400 500Log Size [MB]Elapsed Time [sec]auditd (uncomp.)auditd (comp.)WIN w/o ind.WIN with ind. 0.01 0.1 1 10 100 1000 100 200 300 400 500Log Size [MB]Elapsed Time [sec]auditd (uncomp.)auditd (comp.)WIN w/o ind.WIN with ind. 0.01 0.1 1 10 100 1000 100 200 300 400 500Log Size [MB]Elapsed Time [sec]auditd (uncomp.)auditd (comp.)WIN w/o ind.WIN with ind. 0 2 4 6 8 10 12 14 100 200 300 400 500Induction Time [sec]Elapsed Time [sec] 0 2 4 6 8 10 12 14 100 200 300 400 500Induction Time [sec]Elapsed Time [sec] 0 5 10 15 20 25 30 100 200 300 400 500Induction Time [sec]Elapsed Time [sec]Scenario
ImageTragick Attack
Ransomware Attack
Inexperienced Admin
Dirty Cow Attack
Backdoor Attack
Duration
10m12s
7m21s
2m40s
4m21s
19m31s
Log Size
(MB)
ASD WIN
231
0.3
0.7
161
0.8
228
19
301
133
0.2
Query Resp.
Time (ms)
ASD WIN
103
102
68
107
135
5
9