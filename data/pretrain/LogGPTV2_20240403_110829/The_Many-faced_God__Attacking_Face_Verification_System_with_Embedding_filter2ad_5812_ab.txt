score or confidence level displayed, they are eventually variants of
embedding distance, through which attackers can infer the distance.
White-box adversary. In this scenario, the adversary knows the
structure of the face embedding model f used by the targeted FVS,
including layers, hyper-parameters and weights. The adversary can
conduct the attack with the help of f .
While this assumption seems strong, meeting such requirement
is feasible in many cases. For instance, the adversary can purchase
or download the same FVS system and reverse engineer the model
structure. In addition, open-source face recognition library like
Open face [3] has been used by many FVS and attacking such
FVS is even easier as the model can be directly extracted without
reverse-engineering.
To notice, white-box adversary is also covered by prior works
about machine-learning confidentiality [18, 19, 58] and the assump-
tion is similar.
Black-box adversary. When the FVS is close-source or its open-
source implementation is not available, the adversary will not be
able to directly replicate f . We consider one situation that the
adversary is able to access the embedding produced by f without
knowing its structure. Due to the advent of Cloud-based Machine
Learning as a Service (MLaaS), there have been many FVS using
APIs of an online embedding service for face verification. One
famous example is Clarifai [10], which has pre-trained models
with very high accuracy and sells its access (i.e., returning the face
embedding vector given an inquiry image) to customers [11]. When
the adversary identifies the MLaaS model used by the targeted
FVS (e.g., through sniffing its network traffic and identifying the
destination IP address), she can query its API with forged images to
obtain the embeddings outputted by f , in addition to the displayed
score.
No-box adversary. In the worst case, the adversary cannot obtain
the access to the implementation of the targeted FVS or even its
MLaaS API, which we call “no-box” adversary. Learning f or the
embeddings becomes impossible. However, as we will later show,
by attacking another embedding model, the adversary is still able
to recover victm’s embedding.
2.3 GAN
The core step of our attack is to reconstruct the victim’s face im-
age from the recovered embedding, which can be categorized as
a generative task (in contrast to prediction). We leverage Gener-
ative Adversarial Network (GAN) [22] to fulfill this task, which
has shown great successes in synthesizing plausible image [22],
sound [40] and text [72].
GAN consists of two neural networks: a generator and a dis-
criminator. The generator maps randomized input sampled from a
pre-defined latent space (or “noise”) to a data distribution of interest
in the target space. The discriminator determines if a data distribu-
tion is authentic or synthesized by the generator. The training goal
of the generator is to increase the error rate of (or “fool”) the dis-
criminator, while the goal of the discriminator is to maintain high
accuracy in distinguishing the data distributions. The generator
and the discriminator are trained in turn to minimize the outcome
of a loss function, e.g., minimax loss [22] or Wasserstein loss [5].
There are also a bunch of famous GAN variant works, like image
to image translation [30], image to image translation without paired
data [75].
3 EMBEDDING RECOVERY
In this section, we describe EmbRev, the module developed by us
to infer the face embedding of a victim based on the score displayed
by FVS. To summarize, EmbRev can recover the exact embedding
vector (e.g., 128 dimensions under Facenet-128) when a relatively
large set of scores has been obtained (i.e., the same number as the
embedding dimension) through “querying” FVS. When the number
of queries is limited, EmbRev is still able to approximate victim’s
embedding.
19ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingtian Tan, Zhe Zhou and Zhou Li
Figure 2: Overview of our attack. 1) The attacker queries FVS with a set of face images to obtain the FVS scores, which are
converted to embedding distance. 2) The attacker inputs the images to the same embedding model as FVS (f ) or a surrogate
model f ′ to obtain their embeddings. 3) The distances and embeddings are inputted into EmbRev to recover victim’s face
embedding (cid:174)es. A and D are generated from the distances and embeddings. 4) The sensitive embedding (cid:174)es is inputted to the
generator of ImgRev to recover victim’s face.
3.1 EmbRev with Equation-Solving
We denote the sensitive embedding as (cid:174)es, which is generated by the
embedding model f of FVS, i.e., (cid:174)es = f (xs), where xs is victim’s
image. n is the dimension of embedding (e.g., 128). We assume the
adversary has issued m images (x1, x2, ..., xm) to FVS and obtained
a series of scores s1, s2, ..., sm, which can be converted to distances
d1, d2, ...dm (si + di = 1 for the simplest case). In the meantime, the
adversary also converts the query images to embeddings, denoted
as (cid:174)e1, (cid:174)e2, ..., (cid:174)em, in order to learn (cid:174)es.
When the adversary knows the embedding model f of FVS
(white-box adversary), { (cid:174)e1, (cid:174)e2, ..., (cid:174)em} can be easily constructed
with f (x1), f (x2), ..., f (xm). When the adversary only has access
to the MLaaS API of f (black-box adversary), { (cid:174)e1, (cid:174)e2, ..., (cid:174)em} can
be learnt as well by reading the API response. In section 3.3, we
discuss the no-box adversary.
Our first finding is that (cid:174)es can be fully recovered when m = n.
When the n = 2, the proof is straightforward. In this case, (cid:174)es, (cid:174)e1 and
(cid:174)e2 can be considered as points in two-dimensional Euclidean space,
and (cid:174)es must be on the intersection of the two circles extended from
(cid:174)e1 and (cid:174)e2 (with radius d1 and d2). The intersection can have one or
two points. Finding the intersection is actually the same as solving
the equations of || (cid:174)es − (cid:174)e1|| = d1 and || (cid:174)es − (cid:174)e2|| = d2 where (cid:174)es is the
unknown variable. When n > 2, learning the root of es becomes
non-trivial as n equations will be involved, as shown in Equation
Set 1.
|| (cid:174)es − (cid:174)e1|| = d1
|| (cid:174)es − (cid:174)e2|| = d2
...
|| (cid:174)es − (cid:174)en|| = dn
(1)
Through careful analysis, we found Equation Set 1 is still solv-
able. When L2 distance2 is used, we can convert Equation Set 1 to
2d(p, q) =
i =1 (pi − qi)2, where p and q are two vectors of n elements.
(cid:113)n
Equation 2 below after squaring each equation, assuming (cid:174)es, (cid:174)e1, ...,
(cid:174)en are column vectors.
(cid:174)es
⊺ · (cid:174)es + A · (cid:174)es + D = 0
where A = −2 · { (cid:174)e1, (cid:174)e2, ..., (cid:174)en}⊺ and D = { (cid:174)e1
2, ..., (cid:174)en
d2
Euclidean distance. To solve Equation 2, we firstly introduce a
new scalar variable z and assign it with (cid:174)es
⊺· (cid:174)es, where (cid:174)es is a column
vector. With the introduction of z, Equation 2 can be converted into
Equation 3 and Equation 4 in which the right-hand side has no (cid:174)es.
(2)
⊺ · (cid:174)e2 −
⊺ · (cid:174)en − d2
⊺ · (cid:174)e1 − d2
n}⊺.
1, (cid:174)e2
z + A · (cid:174)es + D = 0
(3)
(cid:174)es = −A
Now we replace (cid:174)es in z = (cid:174)es
−1 · (D + z)
(4)
⊺ · (cid:174)es with Equation 4, so Equation 6
can be derived.
(5)
⊺ · (cid:174)es
z = (cid:174)es
= (D + z)⊺(A
= D
⊺
−1)⊺ · A
−1 · (D + z)
⊺
(6)
BD + z · D
BD + z · (cid:174)1⊺
B · (cid:174)1 + z2 · (cid:174)1⊺
B · (cid:174)1
where B = (A−1)⊺ · A−1 and (cid:174)1 = {1, 1, ..., 1}⊺ with n 1’s.
Because z is a scalar value (it equals to the multiplication of a
row vector and a column vector), Equation 6 is a quadratic function
with z as the unknown variable. Therefore, z has up to two roots,
as shown by Equation 7. For (cid:174)es, up to two roots are available as
well because of Equation 4. The roots are shown in Equation 8, by
assigning Equation 7 into z of Equation 4.
−b ±
z =
√
b2 − 4ac
2a
where a = (cid:174)1⊺
B = (A−1)⊺ · A−1.
B · (cid:174)1, b = (cid:174)1⊺
BD + ·D
⊺
B · (cid:174)1 − 1, c = D
(7)
⊺
BD, and
          Attacker FVS (f) Distances [0.91, 0.88, 0.22, 0.35] Victim ”Mary” Recovered image White-box (f) Black-box (f web interface) No-box (f’) ID = ”Mary” Query images ○1 Attacker’s embedding model  [0, 0.02, 0, …]  [0.13, 0, 0.2, …]  [0.15, 0.02, 0, …]  [0.66, 0.2, 0, …]  Embeddings Solving 𝑒𝑠⃗⃗⃗ 𝑇∙𝑒𝑠⃗⃗⃗ +𝐴∙𝑒𝑠⃗⃗⃗ +𝐷=0  EmbRev ○2 ○3 Enrolled ImgRev (GAN) Training 𝐿𝑜𝑠𝑠= 𝑤𝑟𝐿𝑟+𝑤𝑑𝐿𝑑+𝑤𝑒𝐿𝑒 𝑒𝑠⃗⃗⃗  ○4 White-box (f) Black-box (f’) 20The Many-faced God: Attacking Face Verification System with Embedding and Image Recovery
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
(8)
BD, B =
⊺, in which U and V
and then use Singular Value Decomposition (SVD) to extract the
key components of each embedding. SVD is a variant of Princi-
pal Component Analysis (PCA) over matrices, which transforms
possibly correlated data into linearly uncorrelated variables. With
SVD, a m-by-n matrix M can be decomposed to the product of
three matrices, i.e., M = U · Σ · V
⊺ are unitary
matrices and Σ is a rectangular diagonal matrix. By replacing Σ
with ˜Σ which has r largest singular values, we can approximate M
with another r-rank matrix ˜M = U · ˜Σ · V
⊺. In our setting, we first
combine the 400 embeddings into a matrix M by considering them
as rows. Then, we apply SVD and low-rank approximation to ob-
tain ˜M. Finally, we compute the distance between ˜M and M at each
row. The smaller the distance, the more key information is kept
by ˜M. We experimented with different values for rank r. Figure 3
shows the Max and Mean distances between M and in ˜M. When the
rank reaches 33 and above, the distance goes below 0.1 in average.
Distance 0.1 suggests the two faces are very alike, as two images
will be linked to the same person once their distance is below 1.1
under Facenet [52]. In other words, we can use a 33-dimensional
embedding to approximate a 128-dimensional embedding without
loosing much accuracy.
Dimension reduction by EmbRev. Though the adversary can
use fewer queries to capture the key information of victim’s face,
how to solve the corresponding Equation 2 (when L2 distance is
used) is unclear. Now the equation has infinite roots, as the number
of equations (m) in Equation Set 1 is less than the number of the
unknown elements (n) in (cid:174)es. However, the adversary can choose to
recover the “compressed” embedding directly by adjusting Equa-
tion 2. Below we describe the approach.
Figure 3: Distances between M and ˜M versus the rank of ˜M
on Facenet-128.
s
· Σm · V
We assume (cid:174)es can be compressed into m dimensions (m < n).
With SVD, (cid:174)es = (cid:174)em
⊺
m + δ, in which Σm is m-by-m, Vm is n-
by-m and δ is the distance (or compression error) to (cid:174)es. δ is usually
quite small based our above analysis. By putting (cid:174)es = (cid:174)em
⊺
m +δ
into Equation 2, we obtain Equation 10.
(cid:174)em
s + A · Vm · Σm · (cid:174)em
⊺ · Σm · V
s + D + ∆ = 0 (10)
⊺ · (cid:174)e1 −d2
⊺ · (cid:174)e2 −
where A = −2· { (cid:174)e1, (cid:174)e2, ..., (cid:174)em}⊺ and D = { (cid:174)e1
m}⊺; ∆ is the components with {δ1, δ2, ..., δm}
2, ..., (cid:174)em
d2
where |∆| ≪ |D|. All vectors are column vectors.
The issue of infinite roots does not exist in Equation 10 when
we are solving (cid:174)em
s only has m unknown elements and the
m · Vm · Σm · (cid:174)em
⊺ · (cid:174)em − d2
s , as (cid:174)em
s ·Σm·V
1, (cid:174)e2
⊺
s
−1 · (D +
(cid:174)es = −A
B · (cid:174)1, b = (cid:174)1⊺
−b ±
BD + ·D
√
b2 − 4ac
2a
B · (cid:174)1 − 1, c = D
)
⊺
⊺
(cid:174)e2| (cid:174)e2| , ...,
(A−1)⊺ · A−1 and (cid:174)1 = {1, 1, ..., 1}⊺ with n 1’s.
where a = (cid:174)1⊺
When b2 = 4ac, es is very likely to have only one meaningful
root. We used Matlab to test EmbRev, and did not find the case of