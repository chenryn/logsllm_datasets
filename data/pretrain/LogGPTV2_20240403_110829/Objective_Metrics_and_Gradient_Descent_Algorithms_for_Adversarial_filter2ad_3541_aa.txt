title:Objective Metrics and Gradient Descent Algorithms for Adversarial
Examples in Machine Learning
author:Uyeong Jang and
Xi Wu and
Somesh Jha
Objective Metrics and Gradient Descent Algorithms for
Adversarial Examples in Machine Learning
Uyeong Jang
University of Wisconsin
Madison, Wisconsin
PI:EMAIL
Xi Wu∗
Google
PI:EMAIL
Somesh Jha
University of Wisconsin
Madison, Wisconsin
PI:EMAIL
ABSTRACT
Fueled by massive amounts of data, models produced by machine-
learning (ML) algorithms are being used in diverse domains
where security is a concern, such as, automotive systems, fi-
nance, health-care, computer vision, speech recognition, natural-
language processing, and malware detection. Of particular
concern is use of ML in cyberphysical systems, such as driver-
less cars and aviation, where the presence of an adversary can
cause serious consequences. In this paper we focus on attacks
caused by adversarial samples, which are inputs crafted by
adding small, often imperceptible, perturbations to force a ML
model to misclassify. We present a simple gradient-descent
based algorithm for finding adversarial samples, which per-
forms well in comparison to existing algorithms. The second
issue that this paper tackles is that of metrics. We present a
novel metric based on few computer-vision algorithms for
measuring the quality of adversarial samples.
KEYWORDS
Adversarial Examples, Machine Learning
ACM Reference Format:
Uyeong Jang, Xi Wu, and Somesh Jha. 2017. Objective Metrics and
Gradient Descent Algorithms for Adversarial Examples in Machine
Learning. In Proceedings of December 4–8, 2017, San Juan, PR, USA
(ACSAC 2017,). ACM, New York, NY, USA, 15 pages. https://doi.org/
https://doi.org/10.1145/3134600.3134635
1 INTRODUCTION
Massive amounts of data are currently being generated in
domains such as health, finance, and computational science.
Fueled by access to data, machine learning (ML) algorithms are
also being used in these domains, for providing predictions
of lifestyle choices [7], medical diagnoses [17], facial recog-
nition [1], and more. However, many of these models that
are produced by ML algorithms are being used in domains
where security is a big concern – such as, automotive sys-
tems [26], finance [20], health-care [2], computer vision [21],
speech recognition [14], natural-language processing [29], and
cyber-security [8, 31]. Of particular concern is use of ML in
∗Work was done while at University of Wisconsin
Permission to make digital or hard copies of part or all of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for third-party components of
this work must be honored. For all other uses, contact the owner/author(s).
ACSAC 2017„ 2017
© 2017 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5345-8/17/12...$15.00
https://doi.org/https://doi.org/10.1145/3134600.3134635
Figure 1: To humans, these two images appear to be the
same. The image on the left is an ordinary image of
a stop sign. The image on the right was produced by
adding a small, precise perturbation that forces a par-
ticular image-classification DNN to classify it as a yield
sign.
cyberphysical systems, where the presence of an adversary can
cause serious consequences. For example, much of the technol-
ogy behind autonomous and driver-less vehicle development
is driven by machine learning [3, 4, 10]. Deep Neural Networks
(DNNs) have also been used in airborne collision avoidance
systems for unmanned aircraft (ACAS Xu) [18]. However, in
designing and deploying these algorithms in critical cyberphysi-
cal systems, the presence of an active adversary is often ignored.
In this paper, we focus on attacks on outputs or models that
are produced by machine-learning algorithms that occur after
training or “external attacks”, which are especially relevant
to cyberphysical systems (e.g., for a driver-less car the ML-
algorithm used for navigation has been already trained once
the “car is on the road”). These attacks are more realistic, and
are distinct from “insider attacks”, such as attacks that poison
the training data (see the paper [15] for a survey such attacks).
Specifically, we focus on attacks caused by adversarial exam-
ples, which are inputs crafted by adding small, often impercep-
tible, perturbations to force a trained ML model to misclassify.
As a concrete example, consider a ML algorithm that is used
to recognize street signs in a driver-less car, which takes the
images, such as the one depicted in Figure 1, as input. While
these two images may appear to be the same to humans, the
image on the left [32] is an ordinary image of a stop sign while
the right image was produced by adding a small, precisely
crafted perturbation that forces a particular image-classifier to
classify it as a yield sign. Here, the adversary could potentially
use the altered image to cause the car to behave dangerously,
if the car did not have additional fail-safes such as GPS-based
maps of known stop-sign locations. As driver-less cars become
more common, these attacks are of a grave concern.
Our paper makes contributions along two dimensions: a
new algorithm for finding adversarial samples and better met-
rics for evaluating quality of adversarial samples. We summa-
rize these contributions below.
Algorithms: Several algorithms for generating adversarial
samples have been explored in the literature. Having a diverse
suite of these algorithms is essential for understanding the
nature of adversarial examples and also for systematically
evaluating the robustness of defenses. The second point is
underscored quite well in the following recent paper [6]. In-
tuitively, diverse algorithms for finding adversarial examples,
exploit different limitations of a classifier and thus stress the
classifier in a different manner. In this paper, we present a
simple gradient-descent based algorithm for finding adversar-
ial samples. Our algorithm is described in section 3 and an
enhancement to our algorithm appears in the appendix. Even
though our example is quite simple (although we discuss some
enhancements to the basic algorithm), it performs well in com-
parison to existing algorithms. Our algorithm, NewtonFool,
successfully finds small adversarial perturbations for all test
images we use, but also it does so by significantly reducing the
confidence probability. Detailed experimental results appear
in section 4.
Metrics: The second issue that this paper tackles is the
issue of metrics. Let us recall the adversary’s goal: given an
image I and a classifier F, the adversary wishes to find a "small"
perturbation δ, such that F (I ) and F (I + δ ) are different but I
and I + δ “look the same” to a human observer (for targeted
misclassification the label F (I + δ ) should match the label that
an adversary desires). The question is– how does one formal-
ize "small" perturbation that is not perceptible to a human
observer? Several papers have quantified "small perturbation"
by using the number of pixels changed or the difference in
the L2 norm between I and I + δ. On the other hand, in the
computer-vision community several algorithms have been
developed for tasks that humans perform quite easily, such
as edge detection and segmentation. Our goal is to leverage
these computer-vision algorithms to develop better metrics for
to address the question given before. As a first step towards
this challenging problem, we use three algorithms from the
computer-vision literature (which is described in our back-
ground section 2) for this purpose, but recognize that this is
a first step. Leveraging other computer-vision algorithms to
develop even better metrics is left as future work.
Related work: Algorithms for generating adversarial ex-
amples is a very active area of research, and we will not pro-
vide a survey of all the algorithms. Three important related
algorithms are described in the background section 2. How-
ever, interesting algorithms and observations about adversar-
ial perturbations are constantly being discovered. For exam-
ple, Kurakin, Goodfellow, and Bengio [22] show that even in
physical-world scenarios (such as deployment of cyberphysical
systems), machine-learning systems are vulnerable to adver-
sarial examples. They demonstrate this by feeding adversarial
images obtained from cell-phone camera to an ImageNet In-
ception classifier and measuring the classification accuracy of
the system and discover that a large fraction of adversarial ex-
amples are classified incorrectly even when perceived through
the camera. Moosavi-Dezfooli et al. [24] propose a systematic
algorithm for computing universal perturbations1 In general,
the area of analyzing of robustness of machine-learning al-
gorithms is becoming a very important and several research
communities have started working on related problems. For
example, the automated-verification community has started
developing verification techniques targeted for DNNs [16, 19].
2 BACKGROUND
This section describes the requisite background. We need
Moore-Penrose pseudo-inverse of a matrix for our algorithm,
which is described in section 2.1. Three techniques that are
used in our metrics are described in the next three sub-sections.
Formulation of the problem is discussed in section 2.5. Some
existing algorithms for crafting adversarial examples are dis-
cussed in section 2.6. We conclude this section with a discus-
sion section 2.7.
2.1 Moore-Penrose Pseudo-inverse
Given a n×m matrix A, a matrix A+ is called it Moore-Penrose
pseudo-inverse if it satisfies the following four conditions (AT
denotes the transpose of A):
(1) AA+A = A
(2) A+AA+ = A+
(3) A+A = (A+A)T
(4) AA+ = (AA+)T
For any matrix A, a Moore-Penrose pseudo-inverse exists and
is unique [11]. Given an equation Ax = b, x0 = A+b is the
best approximate solution to Ax = b (i.e., for any vector x
satisfying the equation, ∥ Ax0 − b ∥ is less than or equal to
∥ A x−b ∥).
2.2 Edge Detectors
Given an image, edges are defined as the pixels whose value
changes drastically from the values of its neighbor. The con-
cept of edges has been used as a fundamental local feature in
many computer-vision applications. The Canny edge detector
(CED) [5] is a popular method used to detect edges, and is de-
signed to satisfy the following desirable performance criteria:
high true positive, low false positive, the distance between a
detected edge and a real edge is small, and there is no duplicate
detection of a single edge. In this section we will describe CED.
Preprocess – noise reduction. Most images contain random
noise that can cause errors in edge detection. Therefore, filter-
ing out noise is an essential preprocessing step to get stable
results. Applying convolution with a Gaussian kernel, also
know as Gaussian blur, is a common choice to smooth a given
image. For any n ∈ N, (2n + 1) × (2n + 1) Gaussian kernel is
defined as follows.
− x2 +y2
1
2πσ
2 e
K (x,y) =
2σ 2 where − n ≤ x,y ≤ n
The denoising is dependent on the choice of n and σ.
Computing the gradient. After the noise-reduction step,
CED computes the intensity gradients, by convolving with
1 A single vector, which when added to an image causes its label to change.
Sobel filters. For a given image I,the following shows an ex-
ample convolutions of 3 × 3 Sobel filters, which has been used
in our experiments (in the equations given below ∗ represents
convolution).
1
2
1
0
0
0
−2
−1
−1
 ∗ I
−1 −2 −1
 ∗ I
0
1
0
2
0
1
Gx =
Gy =
Gx and Gy encodes the variation of intensity along x-axis
and y-axis respectively, therefore we can compute the gradient
magnitude and direction of each pixel using the following
formula.
(cid:113)
G =
G
θ = tan−1
2
y
2
x + G
(cid:32) Gy
(cid:33)
Gx
The angle θ is rounded to angles corresponding horizontal
(0◦), vertical (90◦), and diagonal directions (45◦,135◦).
Non-maximum suppression. The intensity gradient gives
us enough information about the changes of the values over
pixels, so edges can be computed by thresholding the intensity
magnitude. However, as we prefer thin and clear boundary
with no duplicated detection, Canny edge detector performs
edge thinning, which is done by suppressing each gradient
magnitude to zero unless it achieves the local maxima along
the gradient direction. Specifically, for each pixel point, among
its eight neighboring pixels, Canny edge detector chooses
two neighbors to compare according to the rounded gradient
direction θ.
• If θ = 0◦, choose the neighboring pixels at the east and
west
• If θ = 45◦, choose the neighboring pixels at the north
east and south west
• If θ = 90◦, choose the neighboring pixels at the north
and south
• If θ = 135◦, choose the neighboring pixels at the north
west and south east
These choices of pixels correspond to the direction perpen-
dicular to the possible edge, and the Canny edge detector tries
to detect a single edge achieving the most drastic change of
pixel intensity along that direction. Therefore, it compares the
magnitude of the pixel to its two neighbors along the gradient
direction, and set the magnitude to be 0 when it is smaller
than any magnitudes of its two neighbors.
Thresholding with hysteresis. Finally, the Canny edge de-
tector thresholds gradients using hysteresis thresholding. In
hysteresis thresholding, we first determine a strong edge (pixel
with gradient bigger than θhiдh) and a weak edge (pixel with
gradient between θlow and θhiдh), while suppressing all non-
edges (pixels with gradient smaller than θlow ). Then, the
Canny edge detector checks the validity of each weak edge,
based on its neighborhood. Weak edges with at least one strong
edge neighbor will be detected as valid edges, while all the
other weak edges will be suppressed.
The performance of Canny edge detector depends highly
on the threshold parameters θlow and θhiдh, and those param-
eters should be adjusted according to the properties of input
image. There are various heuristics to determine the thresh-
olds, and we use the following heuristics in our experiments
• MNIST: While statistics over pixel values (e.g mean,
median) are usually used to determine thresholds, pixel
values in MNIST images are mostly 0, making such sta-
tistics unavailable. In this work, we empirically searched
the proper values for thresholds, sufficiently high to be
able to ignore small noise, and finally used θlow = 300
and θhiдh = 2 · θlow .
• GTSRB: When distribution of pixel value varies, usu-
ally statistics over pixel value are used to adjust thresh-
olds, because pixel gradient depends on overall bright-
ness and contrast of image. Since those image properties
varies in GTSRB images, we put thresholds as follows.
θlow = (1 − 0.33)µ
θhiдh = (1 + 0.33)µ
where µ is the mean of the values on pixels.
2.3 Fourier Transform
In signal processing, spectral analysis is a technique that trans-
forms signals into functions with respect to frequency, rather
than directly analyzing the signal on temporal or spatial do-
main. There are various mathematical operators (transforms)
converting signals into spectra, and Fourier transform is one
of the most popular operator among them. In this section we
mainly discuss two dimensional Fourier transform as it is an
operation on spatial domain where an image lies in and is
commonly applied to image analysis.
Considering an image as a function f of intensity on two
dimensional spatial domain, the Fourier transform F is written
in the following form.
(cid:90) ∞
(cid:90) ∞
F (u,v) =
−∞
−∞ f (x,y)e
−2π i (ux +vy )dxdy
While this definition is written for continuous spatial do-
main, values in an image are sampled for a finite number of
pixels. Therefore, for computational purposes, the correspond-
ing transform on discrete two dimensional domain, or discrete
Fourier transform, is used in most applications. For a function
f of on a discrete grid of pixels, the discrete Fourier transform
maps it to another function F on frequency domain as follows.
(cid:34)
(cid:32)
(cid:33)(cid:35)
F (k,l ) =
f (x,y) exp
−2πi
kx
M
+
ly
N
M−1(cid:88)
N−1(cid:88)
x =0
y=0
While naive computation of this formula requires quadratic
time complexity, there are several efficient algorithms comput-
ing discrete Fourier transform with time complexity O (n log n),
called Fast Fourier Transform (FFT), and we use the two dimen-
sional FFT in our analysis of perturbations.
Fourier transform on two dimensional spatial domain pro-
vides us spectra on two dimensional frequency (or spatial fre-
quency) domain. These spectra describe the periodic structures
across positions in space, providing us valuable information
of features of an image. Specifically, low spatial frequency cor-
responds to the rough shape structure of the image, whereas
spectrum on high spatial frequency part conveys detailed fea-
ture, such as sharp change of illumination and edges.
2.4 Histogram of Oriented Gradients