A more general solution is to measure progress and perform a
directed search, such as splitting branches [41], using gradient-
guided search [17, 18, 66, 67], binary search [20], genetic
algorithms [27], or simulated annealing [68]. For complex
path constraints,
the most efficient way so far is to use
an SMT solver, which applies a large set of sophisticated
heuristics to transform/rewrite the constraints into simpler
ones, then searches for a satisfying solution. Modern SMT
solvers usually leverage two main solving strategies for path
constraints that are in the quantifier-free theories of bit-vectors
and arrays: (1) bit-blasting, which reduces the constraints into
a corresponding SAT (boolean satisfiability) problem, then
queries an efficient SAT solver to find a solution; (2) local
search, which transforms the constraints into an objective
function and applies optimization techniques to find a solution.
Recent research also shows that employing the aforementioned
fuzzing heuristics can be beneficial [9, 45, 53]. Note that the
focus of this work is not on improving the search heuristics,
but on improving the throughput; and our approach can be
combined with any fuzzing- or local-search-based heuristics.
The second factor that affects the efficiency of branch
flipping is the number of new inputs that can be tried in a unit
of time. The more inputs a fuzzer can try, the faster it can find a
satisfying input. For this reason, efforts have also been made to
improve the throughput of fuzzers. For example, AFL [78] uses
fork_server to avoid initialization overhead. kAFL [63] avoids
instrumentation by using a hardware trace collector. Firm-
AFL [79] avoids expensive whole-system emulation through
augmented user-mode emulation. Xu et al. [74] designed three
new operating system (OS) primitives to improve the scalability
of parallel fuzzing on multi-core machines. Nyx [62] employs a
fast virtual machine reset technique. By evaluating with JIT’ed
path constraints, our approach can significantly improve the
search throughput.
Previously, the major drawback of symbolic execution has
been that collecting symbolic constraints is very slow [77], so
the overall branch flipping efficiency is not as good as greybox
fuzzers. However, recent advances in constraints collection
have largely reduced this overhead [10, 56, 57, 77].
III. OUR APPROACH
A. Insight
Our design goal is to push the search throughput (i.e., the
number of test inputs got evaluated per unit time) to the next
level. To achieve this goal, we leverage an important insight:
path constraints collected by symbolic executors are pure and
straight-line functions. Similar to a mathematical function, a
pure function always returns the same value on the same inputs
(i.e., there are no hidden dependencies over global states) and
has no side-effect (i.e., will not affect global states). This makes
pure functions an ideal target for evaluating newly generated
test inputs because P1. no side-effect means no need to perform
expensive state reset (e.g., invoking fork()). P2. no external
dependencies mean we can linearly scale the search to multiple
cores without worrying about data races and lock contention.
P3. being a function, we can easily pass the new test inputs
as arguments via registers or memory thus avoiding going
through file systems. These properties alone already eliminate
two major scalability bottlenecks identified in [74], namely
fork() and file system.
Moreover, being a straight-line function means the function
does not have any conditional branches, which means P4. it
is easier for modern processors to exploit instruction-level
parallelism without worrying about branch mis-prediction
during speculative execution.
Finally, each branch predicate is much simpler than the
original program under test, so P5 fuzzing individual branch’s
path constraints can be orders of magnitude faster than fuzzing
the whole program under test.
B. Overview
Figure 1 shows the design of JIGSAW. JIGSAW works in
a way similar to SMT solvers: 1 it takes a branch’s path
constraints (with dependencies) in the abstracted syntax tree
(AST) form as input; 2 it preprocesses the AST to find all
the input bytes and constants, and decomposes it into potential
sub-tasks; 3 it then compiles each sub-task into a function in
LLVM IR and uses LLVM’s JIT engine to compile the IR into
a native function; 4 it searches for a satisfying solution using
gradient-guided search; and 5 if a solution is found within a
time budget, it returns the solution.
A Running Example. To demonstrate how JIGSAW works,
we will use the following branch constraints as an example.
In this simple example, x is from stdin and will affect the
conditional branch at line 5. In step 1 , we use a symbolic
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:56 UTC from IEEE Xplore.  Restrictions apply. 
320
Fig. 1: Overview of JIGSAW
1 bool test(i) { return i  8 && test(x))
assert(0);
Listing 1: A running example to demonstrate the workflow.
execution engine to collect the path constraints. This can be
done by marking the input from the read system call (i.e., x)
as symbolic. When the execution reaches line 5, we have a
conditional branch whose branch predicate is symbolic, which
can be represented as the following.
1
2
3
4
land(
ugt(read(0, 32), constant(8, 32)),
ult(read(0, 32), constant(13, 32))
)
Note that path constraints are essentially a dynamic slice
of the execution trace of the PUT, so even though there
is a function call (invoking test) in the original code, a
symbolic executor will “enter” the function and collect/slice
instructions that are related to the branch, instead of collecting
test(read(0,32)). This example also shows how dependency-
based nested branch collection works. Here, both the branch
on line 5 and line 1 depend on x, so we need to solve them
together.
Next,
in step 2 , we break down the (conjunct) path
constraints into two sub-tasks that should be solved jointly.
We will also normalize ASTs and map leaf nodes of ASTs
(i.e., x and constant) as arguments. This step ensures every
JIT’ed native function can return a numeric distance, so we
can calculate their approximated gradient to guide the search;
otherwise, we can only observe two binary values: true and
false. The result is as follows.
1
2
ugt(arg0, arg1)
ult(arg0, arg1)
After preprocessing, in step 3 , we compile each sub-task
into a function in LLVM IR. Note that unlike previous work [40,
45, 53], these functions are generated in memory using LLVM’s
C++ API instead of writing to files. We then use LLVM’s
JIT engine to compile each IR function to a native function.
In this step, we do not enable any optimizations during JIT
compilation, for two reasons: (1) path constraints are collected
from already optimized code and are usually not too complex;
but (2) more importantly, we found that the extra time spent
on optimization may reduce the overall branch flipping rate
because compilation is much more expensive than fuzzing
(see §VI for more details).
In step 4 , we plug two JIT’ed functions into the gradient-
guided search algorithm from Matryoshka [18] to search for a
satisfying x. This algorithm can jointly solve conjunctions of
sub-tasks.
C. Challenges
While directly fuzzing branch constraints is promising, we
need to address two critical road-blockers.
Constraint Compilation. While searching with JIT’ed native
functions offers high throughput, a slow compilation process
can become a bottleneck and cancel the benefit of faster solving
speed (see §VI). Our solution to this problem is to cache the
JIT-compiled functions so we can avoid repeatedly compiling
the same constraints. However, simply caching the raw JIT’ed
path constraints yields a mediocre cache hit rate. The reason
is that we do not see identical constraints very often. Our
insight to solve this problem is that many constraints operate
on different data (e.g., x > 8 and y > 16) are performing the
same check (e.g., ugt(arg0,arg1)); therefore we can use the
same JIT’ed function to solve both constraints. Note that our
function cache is different from the constraint cache used by
symbolic executors. A constraint cache memorizes satisfying
solutions to avoid solving the same constraints repeatedly; our
function cache saves JIT’ed functions to avoid compilation, not
solving. So, they are complementary and can be used together.
Lock Contention. While invoking JIT’ed path constraints is
highly parallelizable, data races can happen in other steps (e.g.,
updating the native function cache). A standard way to avoid
data races is to use locks; however, lock contention can also
scalability bottleneck. We apply two main strategies to avoid
lock contention: (1) we reduce data sharing thus the locations
where data race can happen; and (2) we reduce the use of
locks by using lock-free data structures.
D. Comparison with SMT Solvers
Since our prototype JIGSAW is a path constraint solver, a
natural question is: how it compares to SMT solvers. We believe
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:56 UTC from IEEE Xplore.  Restrictions apply. 
421
Front-endFront-endJIGSAWRPC+i10<90❶branchconstraintsTask queueDispatcherworker thread❷pre-process❸JIT engine❹GD searchsub-tasksub-tasknative funcnative func❺satinputsworker thread❷pre-process❸JIT engine❹GD searchsub-tasksub-tasknative funcnative funcInputs queue1 message AstNode {
2
3
4
5
6
7
8
9
10 }
uint32 op;
uint32 width;
string value;
string name;
uint32 offset;
uint32 label;
uint32 hash;
repeated AstNode children;
// operand width
// used by constant expr
// used for debugging
// used by read expr
// for expression dedup
// for request dedup
Listing 2: AST node for function cache lookup.
the comparison can be done at two levels. Methodology-wise,
our approach provides a new and fast way to evaluate the
satisfiability of a concrete model (i.e., assignments to symbolic
variables); therefore, our approach can also be leveraged by
SMT solvers to improve their performance. For example, we
have used path constraints collected from objdump to evaluate
the z3_model_eval() API and JIT’ed functions from JIGSAW:
the Z3 API can evaluate about 43K concrete models per second
while JIGSAW can evaluate 8M models per second.
At the tool level, our prototype JIGSAW has both advantages
and limitations. First, due to the high search throughput, our
evaluation shows that JIGSAW can solve path constraints faster
than SMT solvers. However, because JIGSAW only employs a
single search heuristic (the gradient-guided search from [17]),
it is not as capable as off-the-shelf SMT solvers. First of all,
JIGSAW can only be used to find satisfying inputs, while SMT
solvers can also be used to prove theorems. Second, our current
prototype only supports constraints in the theory of bit-vectors
while most modern SMT solvers support more theories like
arrays, floating-point numbers, and strings. Even for bit-vectors,
JIGSAW cannot identify unsatisfiable constraints and can only
solve 94% of the constraints solved by Z3. Nevertheless, we
want to emphasize that these limitations are introduced by
the search heuristic but not the methodology proposed in this
work. Therefore, these limitations can be addressed by adopting
additional heuristics from SMT solvers. For instance, similar to
Bitwuzla [48], we can apply rewriting rules to identify simple
unsatisfiable constraints and add a bit-blasting-based solver to
handle constraints that cannot be decided by local search.
IV. JIGSAW
In this section, we present the design details of JIGSAW.
A. Getting Constraints
JIGSAW relies on a concolic execution engine to collect
path constraints to be solved. To do so, we use our data-
flow sanitizer-based engine (§V). Similar to SymCC [56], our
engine collects path constraints at the LLVM IR [58] level. The
collected path constraints are then passed to JIGSAW through
shared memory.
AST for Cache Lookup. Listing 2 shows the format of
each abstract syntax tree node we use to store the collected
constraints, where op denotes the operator and children denote
the child nodes of the AST. Currently, JIGSAW supports all of
LLVM’s binary operators, including integer arithmetic, bitwise,
and logical instructions. It also supports three conversion
operators (ZExt, SExt, and Trunc) and relational comparison
instructions. We add a special operator Read to denote symbolic
input bytes. Different input bytes are distinguished with their
offset from the beginning of the input.
Nested Branches. One particular challenge during branch
flipping is that solving a single branch predicate alone is not
enough [18]. The reason is that the solution can negatively
affect preceding branches and cause the control-flow to diverge;
as a result, the new input may never reach this supposedly
solved branch. To address this problem, we need to solve these
dependent/nested branches together. In this work, we used
QSYM’s [77] approach to identify nested branches based on
data dependency: finding all precedent branches whose input
bytes overlap with the current branch.
B. Preprocessing
Since calculating the numeric approximation of gradient
works best for individual comparison instructions where we
can measure the distance, we want to avoid logical operators
inside the JIT’ed testing function. Therefore, after receiving
a solving request, the first step is to break it up into possible
sub-tasks, where each sub-task is a single AST rooted with a
comparison instruction. Then we will parse the AST to find all
the arguments (both input bytes and constants) to the testing
function.
Removing Logical Or. Due to compiler optimizations, branch
constraints may occasionally contain logical or (LOr) operators.
To remove LOr operators, we first convert a solving request
into DNF (disjunctive normal form). Each clause in the DNF
can then be solved in parallel. As long as one clause is solved,
the branch can be flipped.
Removing Logical And. After removing LOr, each sub-task
should be clauses connected with logical and (LAnd). To remove
LAnd, we will generate a separate testing function for each
clause. However, all clauses will be solved jointly (§IV-D).
Removing Logical Not. After removing LOr and LAnd, we
may still have clauses/AST with a leading logical not (LNot)
operator. Removing LNot is relatively simple, we just remove
it and set the comparison condition to its opposite (e.g., < to
≥).
Arguments Mapping. To maximize the reuse of JIT’ed
functions and minimize the compilation time (§IV-E), we treat
both input data and constants as arguments to the testing
function. In our current design, the testing function takes a
single argument as an array of 64-bit integers, to support an
arbitrary length of arguments. To correctly invoke the testing
function, we need to map input bytes and constants in the AST
to the correct offsets inside the argument array. To do so, we
perform a pre-order traversal of the AST and number the leaf
nodes according to the traversal order.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:56 UTC from IEEE Xplore.  Restrictions apply. 
522
TABLE II: Transforming a comparison operation into a distance-
based loss function. a and b are arbitrary ASTs/expressions, ϵ is a
small positive value, e.g., integer 1.
Comparison
slt(a, b)
sle(a, b)
sgt(a, b)
sge(a, b)
ult(a, b)
ule(a, b)
ugt(a, b)
uge(a, b)
a = b
a ̸= b
Loss function f ()
max(sext(a, 64) − sext(b, 64) + ϵ, 0)
max(sext(a, 64) − sext(b, 64), 0)
max(sext(b, 64) − sext(a, 64) + ϵ, 0)
max(sext(b, 64) − sext(a, 64), 0)
max(zext(a, 64) − zext(b, 64) + ϵ, 0)
max(zext(a, 64) − zext(b, 64), 0)
max(zext(b, 64) − zext(a, 64) + ϵ, 0)
max(zext(b, 64) − zext(a, 64), 0)
abs(zext(a, 64) − zext(b, 64))
max(−abs(zext(a, 64) − zext(b, 64)) + ϵ, 0)
C. Code Generation
After preprocessing a solving task and decomposing it into
sub-tasks, the next step is to JIT-compile each comparison
AST into a testing function that returns a distance so we can
perform a gradient-guided search. To do so, we transform the
comparison instruction into a loss function similar to previous
works [17, 18, 67]. Table II shows the transformation. To
minimize the impact of integer overflow/underflow during
calculation, we first extend both operands into 64-bit numbers.
For each unsigned comparison, we perform a zero extension
(ZExt). For each signed comparison, we perform a signed
extension (SExt). Then we apply the max operation to avoid
any negative distance. This is done by performing the original
comparison followed by a conditional move (i.e., Select)
instruction. Because our AST language is close to LLVM
IR, the rest of the code generation is straightforward: just
perform a post-order traversal of the AST.
D. Solving
To search for a satisfying input, we use the gradient-guided
search algorithm from Matryoshka [18], which uses a numeric
approximation to calculate the gradient and is capable of
solving conjunctions of comparisons. The original algorithm
uses three search strategies to solve conjunctions of branch
constraints, in our prototype, we used a simplified version:
1) Prioritize satisfiability: try to solve the current branch
predicate first.
2) Once we find a satisfying input, use the following loss
function to solve nested branch constraints using joint
optimization:
n(cid:88)
g(x) =
fi(x)
i=1
To avoid negating a previously satisfied constraint, we will
stop mutating an input byte if its new value will violate any
constraint that is satisfied previously. However, as long as