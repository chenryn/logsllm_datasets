a
1
s2
1
s2
(d) Likelihood ratio per trans-
ition.
Figure 1: DTMC and IMC
IMC of A in which parameters a and c are supposed to be
equal to ˆa and ˆc up to margins of error ˆa and ˆc.
C. Monte Carlo estimation
Given a probabilistic model S, a DTMC or an IMC, the
goal is to estimate the probability that a random execution of
S satisﬁes a property φ speciﬁed using bounded temporal logic
formulae (see for example [12]). Let γ be this probability and
z be the function that assigns 1 to a trace satisfying φ and
0 otherwise. By deﬁnition, γ is the sum of the probabilities
of the paths ω such that z(ω) = 1. In other words, γ is the
expectation of function z over the set of traces Ω where z
must be interpreted as a Bernoulli random variable Z:
(cid:5)
(cid:5)
γ =
z(ω)Pμ(ω) =
Ω
ω|=φ
Pμ(ω) = Eμ[Z],
(2)
with Pμ(ω) the probability of path ω under the probability
distribution μ.
In SMC, a set of N traces (ωi)1≤i≤N is sampled randomly
according to distribution μ and a Monte Carlo frequentist
estimation ˆγN of γ is given by:
N(cid:5)
ˆγN =
1
N
z(ωi).
i=1
(3)
Note that z(ωi) is effectively the realisation of a Bernoulli ran-
dom variable with parameter γ. Hence Var(ˆγN ) = γ(1−γ)/N.
1−δ/2 the (1− δ/2)-
Given the level of conﬁdence 1− δ and Φ
−1
(cid:7)
quantile of the normal distribution, an approximate conﬁdence
ˆγN (1−ˆγN )
interval is given by I =
(cid:6)
ˆγN ± Φ
−1
1−δ/2
(cid:8)
N
.
For clarity, we sometimes use the notation γ(A) to refer to
the probability of property φ when μ is a DTMC parametrised
by the matrix of transitions A, ˆγN (A) to denote an estimate of
γ(A) based on N samples and ˆσN (A) the empirical standard
deviation of the samples.
III. IMPORTANCE SAMPLING IN MARKOV MODELS
Given a set of N traces, the absolute error, deﬁned as the
half size of the conﬁdence interval, decreases as the inverse
square root of N. But for small probabilities, the accuracy
of the estimation is better captured by the relative error, that
305
is the absolute error divided by γ. However, the relative error
explodes when γ tends to zero since it is inversely proportional
to the square root of N γ. So, in practice, denoting the relative
error RE, if we desired RE = 10%, we would need to increase
N as a proportion of 100 ∗ γ
−1. Rare events require too
many samples to be observed at least once and prompted the
recourse to advanced simulation techniques such as importance
sampling [2], [7], [14], [23].
A. IS estimation
Let μ be absolutely continuous with respect to another
probability measure μ
(cid:5)
(cid:4) over Ω, then (2) can be written
γ =
Ω
Pμ(ω)
Pμ(cid:2) (ω)
z(ω) Pμ(cid:2) (ω).
is called the likelihood ratio
The function L = Pμ/Pμ(cid:2)
function and γ can be then interpreted as the expectation of
function z weighted by L under probabilistic measure μ
(cid:4):
(cid:5)
γ =
L(ω)z(ω)Pμ(cid:2) (ω) = Eμ(cid:2) [ZL].
Ω
Note that in a DTMC, the likelihood ratio L of a path is
(cid:4).
the ratio of its probabilities under distribution μ and μ
(cid:4) is deﬁned on the same space than μ and is
Assume that μ
parametrised by probability matrix B = (bij)0≤i,j≤m. Then,
L(ω) = PA(ω)/PB(ω).
In practice, the likelihood ratio of ω is initialised to 1 and,
once a transition si → sj is taken, the likelihood ratio is
updated on-the-ﬂy by multiplying its current value by aij/bij.
Formally, we can write any likelihood ratio as a product of
power of all the ratios aij/bij:
nij (ω)
ij
nij (ω)
ij
(cid:10)nij (ω)
(cid:3)m
(cid:3)m
(cid:3)m
(cid:3)m
m(cid:4)
m(cid:4)
L(ω) =
aij
bij
j=0 a
(cid:9)
j=0 b
(6)
j=0
i=0
i=0
=
i=0
We can thus estimate γ by sampling traces under μ
compensating each path ωk by its likelihood ratio L(ωk):
(cid:4) and
(4)
(5)
NIS(cid:5)
1
NIS
k=1
(cid:12)
L(ωk)z(ωk)
(7)
ˆγNIS =
Here ωk ∼ μ
(cid:4) and NIS denotes the number of simulations
(cid:11)
used by the IS estimator. An approximate conﬁdence interval
ˆγNIS ± Φ
is given by I =
where ˆσNIS denotes
the empirical standard deviation of the samples. The goal of
IS is to reduce the variance of the rare event and so achieve a
narrower conﬁdence interval than the Monte Carlo estimator,
resulting in NIS (cid:10) N. In general, the IS distribution μ
(cid:4) is
chosen to produce the rare events more frequently.
−1
1−δ/2
ˆσNIS√
NIS
The IS distribution deﬁned by Pμ(cid:2) = zPμ/γ outputs an
estimator with zero variance. Indeed, the paths that do not
satisfy φ have a probability 0 to occur and the likelihood ratio
of the successful paths is equal to γ. Sampling under this
perfect distribution is however unrealistic since it requires to
know γ which is the probability to estimate.
In practice, choosing a good importance sampling dis-
tribution in terms of variance reduction is a conundrum.
Then, given a sample of N paths and an IS distribution B,
for all A ∈ [ ˆA],
N(cid:5)
1
N
(cid:13)
m(cid:4)
m(cid:4)
z(ωk)
k=1
i=0
j=0
≤ 1
N
(cid:14)nij (ωk)
≤ ˆγN (A)
(cid:13)
m(cid:4)
m(cid:4)
N(cid:5)
−
a
ij
bij
z(ωk)
k=1
i=0
j=0
+
a
ij
bij
(cid:14)nij (ωk)
(cid:2)m
But, optimising individually each transition leads to very
coarse bounds. For all i, individual probabilities (aij)0≤i,j≤m
must fulﬁl the vectorial constraint:
j=0 aij = 1. Moreover, a
transition observed in different paths may optimise each path
probability in a different way. Since we use the once-for-all
IMC semantics, improving the bounds requires to optimise the
transitions all together. For this purpose, we ﬁrst rewrite the
lower bound problem as a constrained minimisation problem:
m(cid:4)
m(cid:4)
(cid:9)
(cid:10)nij (ωk)
N(cid:5)
aij
bij
i=0
j=0
−
(aij) ≤ 0, for all j,
(aij) ≤ 0, for all j,
+
(9)
minimize
A∈[ ˆA]
subject to a
and
z(ωk)
k=1
ij − aij = c
−
aij − a
+
ij = c
1 − m(cid:5)
j=0
and
aij = c(ai) = 0, for all i.
Nevertheless, in the framework of DTMCs, the cross-entropy
algorithm can be used to ﬁnd a good candidate for the IS
distribution as shown in [14], [24].
B. Margin of error problem
In the following, we show that applying existing important
sampling techniques to a DTMC learnt from a real system
would result in general in signiﬁcant errors. Let us consider
DTMC A described in Fig. 1a. Assume that the initial state is
s0 and that our goal is to estimate the probability γ of reaching
s2. Remark that, in this simple example, γ = ac/(1 − ad).
Thus, with a = 0.0001 and c = 0.05, γ ≈ 5.005 × 10
−6. An
example of perfect IS distribution for A, called B, is given
in Fig. 1c. Fig. 1d gives the ratios (aij/bij). Note that all the
paths sampled with respect to B are successful and have the
same likelihood ratio L(ω) = ac/(1− ad) = γ. It follows that
V (ˆγN ) = 0 and, independently to conﬁdence level (1−δ) and
sample size N, the conﬁdence interval is reduced to a single
point : I = [ˆγN ± 0] = γ.
Assume now that A is unknown and approximated by
a transition matrix ˆA parametrised by ˆa and ˆc. The graph
structure being identical, it is easy to ﬁnd the perfect im-
portant sampling with respect to ˆA and eventually to output
ˆγN ( ˆA) = ˆaˆc
1−ˆa ˆd and a conﬁdence interval reduced to this
point. Unfortunately,
this estimation is only perfect with
respect to ˆA regardless of how close ˆA and A are. It is
extremely unlikely that ˆγN ( ˆA) = γ and consequently, the
conﬁdence interval almost surely never contains the exact
probability. More importantly, a slight error of approximation
of the probabilistic transitions may lead to signiﬁcant different
results. For example, with ˆa = 0.0003 and ˆc = 0.0498,
ˆγN ( ˆA) = 1.4944 × 10
−5, which is almost three times the
exact value.
Since IS implies the computation of a potentially large
product of individual transition probabilities, a ﬁne under-
standing of the system behaviour is necessary to be performed
correctly. If the abstraction of the system is too coarse, it
is unlikely the case. Even a low deviation of one particular
individual transition may have large consequences on the ﬁnal
computation. The sensitivity of the results seriously poses the
question of the validity of IS for approximated models of real-
world systems, which in general are much larger and more
complex than this example. This motivates us to take into
account the margin of errors in our IS analysis.
The upper bound can be handled similarly by rewriting it as
a maximisation problem. We denote Amin and respectively
Amax the DTMCs that minimises and maximises the optim-
isation problem. In what follows, for convenience, we only
present our approach for the lower bound.
The minimisation problem can be simpliﬁed. First of all,
it is worth noting that the probabilistic distributions from a
given state are independent of each other. In other words,
optimising state distribution ai has no impact on aj. Moreover,
it is only necessary to optimise the distribution from state s if
at least one transition s → s
(cid:4) is observed in a successful path
with respect to property φ. Denoting α the set of indexes of
successful paths, M its cardinal, α(k) the k-th element of α
with 1 ≤ k ≤ M and Tk the set of transitions observed in
ωα(k):
minimize
A∈[ ˆA]
subject to a
and
(cid:9)
(cid:10)nij (ωα(k))
aij
bij
(aij) ≤ 0, for all j,
(aij) ≤ 0, for all j,
−
+
M(cid:5)
(cid:4)
(i→j)∈Tk
k=1