# Data and Experimental Setup

## Software Installers Used in Experiments
Table 2 lists the software installers used in our experiments. Popular enterprise installations are marked with 'N', and the software discussed in § 2 is marked with (cid:70). Malicious installers are only included in the test dataset.

| T | V | BT | M | Installer Name |
|---|---|---|---|-----------------|
| 40 | 40 | 40 | 40 | TeamViewer |
| 40 | 40 | 40 | 40 | ESET AV Remover (cid:70) |
| 40 | 40 | 40 | 40 | Flash |
| 20 | 20 | 20 | 20 | FileZilla |
| 20 | 20 | 20 | 20 | PWSafe |
| 20 | 20 | 20 | 20 | MP3Gain |
| 20 | 20 | 20 | 20 | ShotCut |
| 20 | 20 | 20 | 20 | Foobar |
| 20 | 20 | 20 | 20 | 7Zip |
| 20 | 20 | 20 | 20 | TurboVNC |
| 20 | 20 | 20 | 20 | WinMerge |
| 20 | 20 | 20 | 20 | Launchy |
| 20 | 20 | 20 | 20 | Skype |
| 20 | 20 | 20 | 20 | WinRAR |
| 20 | 20 | 20 | 20 | DropBox |
| 20 | 20 | 20 | 20 | Slack |
| 20 | 20 | 20 | 20 | OneDrive |
| 20 | 20 | 20 | 20 | NotePad++ |
| 20 | 20 | 20 | 20 | ICBC Anti-Phishing |

**Key:**
- T: Training
- V: Validation
- BT: Benign Test
- M: Malicious Installer

## Malicious Installers Found in the Wild
Table 3 lists the malicious installers found in the wild, with the malware discussed in § 2 marked with (cid:70).

| Malware Type | Malware Family | Malware Signature (MD5) | Installer Name |
|--------------|----------------|-------------------------|-----------------|
| Win32/Agent | Trojan | a2fd7c92f1fb8172095d8864471e622a | TeamViewer |
| Win32/Skeeyah.A!rfn | Trojan | a538439e6406780b30d77219f86eb9fc | TeamViewer |
| Win32/Wadhrama.A!rsm | Ransomware | d35fa59ce558fe08955ce0e807ce07d0 | Flash |
| Win32/Banload | TrojanDownloader | ab6cef787f061097cd73925d6663fcd7 | Flash |
| Win32/Rabased | HackTool | 7092d2964964ec02188ecf9f07aefc88 | Flash |
| Win32/Offerbox | PUA | 5a9e6257062d8fd09bc1612cd995b797 | Flash |

## Real Malware Used in Experiments
Table 4 lists the real malware used to create malicious installers.

| Malware Type | Malware Family | Malware Signature (MD5) |
|--------------|----------------|-------------------------|
| Win32/VBInject.AHB!bit | VirTool | 03d7a5332fb1be79f189f94747a1720f |
| Win32/VBInject.ACM!bit | VirTool | 02c7c46140a30862a7f2f7e91fd976dd |
| Win32/CeeInject.ANO!bit | VirTool | 1243e2d61686e7685d777fb4032f006a |
| Win32/Prepscram | SoftwareBundler | 056a5a6d7e5aa9b6c021595f1d4a5cb0 |
| Win32/Unwaders.C!ml | SoftwareBundler | 0f0b11f5e86117817b3cfa8b48ef2dcd |
| Win32/Fareit.AD!MTB | PasswordStealer | c649ac255d97bd93eccbbfed3137fbb8 |
| Win32/Primarypass.A | PasswordStealer | 02a06ad99405cb3a5586bd79fbed30f7 |
| Win32/Fareit!rfn | PUA | c622e1a51a1621b28e0c77548235957b |
| Win32/KuaiZip | PUA | 04e8ce374c5f7f338bd4b0b851d0c056 |
| Win32/Adload | PUA | c62ced3cb11c6b4c92c7438098a5b315 |
| Win32/Gandcrab.E!MTB | Ransomware | 73717d5d401a832806f8e07919237702 |
| MSIL/Boilod.C!bit | Trojan | 05339521a09cef5470d2a938186a68e7 |
| Win32/Emotet.A!sms | Trojan | 0ed7544964d66dc0de3db3e364953346 |
| Win32/Occamy.B!bit | Trojan | 02346c8774c1cab9e3ab420a6f5c8424 |
| Win32/Delpem.A | Trojan | 0314a6da893cd0dcb20e3b46ba62d727 |
| Win32/Occamy.C!MTB | Trojan | c60947549042072745c954f185c5efd5 |
| Win32/DownloadGuide | TrojanDownloader | 02a06ad99405cb3a5586bd79fbed30f7 |
| Win32/Puwaders.A!ml | SoftwareBundler | 0f030516266f9f0d731c2e06704aa5d3 |

## Data Collection and Graph Generation
We collected benign data from the enterprise event database, where system administrators store and monitor company-wide system activity. We constructed software installation graphs for popular software in the enterprise, ensuring consistent versions across different machines. Administrators carefully monitor installations to ensure their authenticity. We also installed additional legitimate and popular software packages [20] to increase the size of our dataset. Additionally, we included benign versions of malicious installers found in the wild (Table 3).

For malware data, we collected samples from malicious installers discovered in the wild (Table 3) and created more than 600 malicious installers by combining benign software installers with real malware from VirusShare. We randomly selected malware samples from a wide range of families that exhibit diverse behavior.

## Implementation and Experimental Setup
SIGL's data collection and graph generation module is implemented in Java 8, while its core analytic algorithms, including node embedding, modeling, and anomaly detection, are implemented in Python 3.5 and PyTorch 1.1.0 with the CUDA 9.0 toolkit. We use the Gensim library for node embeddings and the Deep Graph Library (DGL) on top of PyTorch.

The benign input data is partitioned into a training set (70%), a validation set (10%), and a false positive test set (20%). The node context for node embedding is parameterized with a window size of 5, 10 random walks of length 10, and 128 dimensions. We use the skip-gram training algorithm with negative sampling and run 20 epochs over the corpus.

SIGL performs unsupervised learning, requiring only benign installers for training. We train SIGL’s deep graph neural network on a system with a NVIDIA GTX 1080 Ti GPU with 12 GiB of memory. We train the model for 100 epochs with a training batch size of 25, validate model performance after every epoch, and choose the model that produces the best performance on validation data.

## Experimental Results
We evaluate SIGL’s detection performance on 625 malicious installers across various software packages (Table 2). Table 5 shows that SIGL achieves over 90% precision, recall, accuracy, and F-score, correctly identifying all malicious installers in the wild.

### Overall SIGL Experimental Results Compared to Other Approaches
| Method | Precision | Recall | FP Percentage | Accuracy | F-Score |
|--------|-----------|--------|---------------|----------|---------|
| SIGL | 0.99 | 0.96 | 0.06 | 0.93 | 0.97 |
| Commercial TDS [59] | 0.59 | 0.90 | 0.03 | 0.94 | 0.70 |
| StreamSpot [48] | 0.52 | 0.72 | 0.05 | 0.96 | 0.60 |
| Frappuccino [28] | 0.12 | 0.51 | 0.07 | 0.95 | 0.21 |

### SIGL Experimental Result Breakdown for Each Software Installer
| Software Installer | Precision | Recall | Accuracy | F-Score |
|--------------------|-----------|--------|----------|---------|
| FireFox | 0.70 | 0.77 | 0.74 | 0.73 |
| FileZilla | 1.0 | 0.98 | 0.99 | 0.99 |
| PWSafe | 1.0 | 0.98 | 0.99 | 0.99 |
| MP3Gain | 1.0 | 0.98 | 0.99 | 0.99 |
| ShotCut | 1.0 | 0.98 | 0.99 | 0.99 |
| TeamViewer | 1.0 | 0.91 | 1.0 | 0.95 |
| Foobar | 1.0 | 0.98 | 1.0 | 0.99 |
| 7Zip | 1.0 | 0.97 | 1.0 | 0.98 |
| TurboVNC | 1.0 | 0.98 | 1.0 | 0.99 |
| WinMerge | 1.0 | 0.88 | 1.0 | 0.93 |
| Launchy | 1.0 | 1.0 | 1.0 | 1.0 |
| Skype | 1.0 | 0.98 | 1.0 | 0.99 |
| WinRAR | 1.0 | 0.95 | 1.0 | 0.97 |
| DropBox | 1.0 | 0.95 | 1.0 | 0.97 |
| Slack | 1.0 | 1.0 | 1.0 | 1.0 |
| Flash | 1.0 | 0.84 | 1.0 | 0.91 |
| OneDrive | 1.0 | 1.0 | 1.0 | 1.0 |
| NotePad++ | 1.0 | 0.98 | 1.0 | 0.99 |
| ICBC Anti-Phishing | 1.0 | 0.98 | 1.0 | 0.99 |
| ESET AV Remover | 1.0 | 0.98 | 1.0 | 0.99 |

### Comparison Study
We compare SIGL to our in-house commercial TDS [59] and two provenance-based research anomaly detection systems, StreamSpot [48] and Frappuccino [28]. We do not compare SIGL to other commercial TDS or academic systems that leverage proprietary information from security vendors, as this information is unavailable to us.

A preliminary experiment showed that our malicious installers (created using real malware in Table 4) can significantly reduce the efficacy of commercial anti-virus tools, even without changing malware signatures. On average, 80.8% of the engines detect the malware listed in Table 4; the lowest detection rate was 70.0%. Testing on our malicious installers, VirusTotal reports only 42.4% on average and a minimum detection rate of 10.8%.

**Commercial TDS:**
- Inspects every event between a process and a file.
- Determines potential threats based on the familiarity of the file and the diversity of the process.

**Frappuccino:**
- Detects program anomalies by analyzing whole-system provenance graphs.
- Uses a vertex-centric label propagation algorithm to compare the similarity between two provenance graphs.
- Clusters normal provenance graphs and detects abnormal runs when their graphs cannot fit into any existing clusters.

Both SIGL and Frappuccino make assumptions about normal behavior, but SIGL focuses on software installation graphs, providing a more targeted and effective approach for detecting malicious installers.