40
40
40
40
40
40
20
20
20
20
20
20
20
20
T: Training V: Validation BT: Benign Test M: Malicious Installer
Table 2: Software installers used in the experiments. Popular software instal-
lations in the enterprise are marked with N. The software discussed in § 2 is
marked with (cid:70). Malicious installers are included only in the test dataset.
Malware Type
Win32/Agent
Malware Family
Trojan
Trojan
Ransomware
TrojanDownloader
HackTool
PUA
Malware Signature (MD5)
a2fd7c92f1fb8172095d8864471e622a
a538439e6406780b30d77219f86eb9fc Win32/Skeeyah.A!rfn
d35fa59ce558fe08955ce0e807ce07d0
Win32/Wadhrama.A!rsm
ab6cef787f061097cd73925d6663fcd7
Win32/Banload
7092d2964964ec02188ecf9f07aefc88
Win32/Rabased
5a9e6257062d8fd09bc1612cd995b797 Win32/Offerbox
Installer Name
TeamViewer
TeamViewer
ESET AV Remover (cid:70)
Flash
Flash
Flash
Table 3: Malicious installers found in the wild. The malware discussed in § 2
is marked with (cid:70).
discussed in § 5.12, while other sections focus on real-world
Windows logs from the enterprise.
Benign Data. We collected benign data from the enterprise
event database where system administrators store and monitor
company-wide system activity. We constructed software in-
stallation graphs (§ 4.2) for popular software in the enterprise.
Software versions are consistent across different machines.
Administrators carefully monitor installations to ensure their
authenticity. We installed additional legitimate and popular
software packages [20] to increase the size of our dataset. We
also included benign versions of malicious installers found in
the wild (Table 3). Table 2 shows the complete list of software
installers used in our evaluation.
Malware Data. We collected malware data from malicious
installers discovered in the wild (Table 3). We also created
more than 600 malicious installers by combining benign soft-
ware installers in Table 2 with real malware from VirusShare.
Table 4 lists the malware samples we used in our evaluation.
We randomly selected malware samples from a wide range of
malware families that exhibit diverse behavior. For example,
trojan attacks and ransomware typically communicate with a
remote server, while malware of the PUA family downloads
and installs potentially unwanted applications.
We investigated past real-world security incidents (e.g., [40,
51, 52]) that involve malicious installers as the entry point to
high proﬁle attacks and observed two general approaches to
designing malicious installers:
Bundle malware with legitimate installers. The attackers cre-
ate a “wrapper installer” that simultaneously runs an unmod-
iﬁed benign installer in the foreground and malware in the
background. We bundle each legitimate installer with every
malware sample in Table 4 to create malicious installers.
Embed malware in legitimate installers. The attackers modify
Malware Type
Win32/VBInject.AHB!bit
Win32/VBInject.ACM!bit
Win32/CeeInject.ANO!bit
Win32/Prepscram
Win32/Prepscram
Win32/Unwaders.C!ml
Win32/Fareit.AD!MTB
Win32/Fareit.AD!MTB
Win32/Primarypass.A
Malware Signature (MD5)
03d7a5332fb1be79f189f94747a1720f
02c7c46140a30862a7f2f7e91fd976dd
1243e2d61686e7685d777fb4032f006a
056a5a6d7e5aa9b6c021595f1d4a5cb0
0f0b11f5e86117817b3cfa8b48ef2dcd
c649ac255d97bd93eccbbfed3137fbb8
02a06ad99405cb3a5586bd79fbed30f7
1537083e437dde16eadd7abdf33e2751
01abfaac5005f421f38aeb81d109cff1
c622e1a51a1621b28e0c77548235957b Win32/Fareit!rfn
04e8ce374c5f7f338bd4b0b851d0c056
c62ced3cb11c6b4c92c7438098a5b315
73717d5d401a832806f8e07919237702 Win32/KuaiZip
Win32/Adload
05339521a09cef5470d2a938186a68e7
0e8cce9f5f2ca9c3e33810a2afbbb380
Win32/Gandcrab.E!MTB
0f030516266f9f0d731c2e06704aa5d3
MSIL/Boilod.C!bit
0ed7544964d66dc0de3db3e364953346 Win32/Emotet.A!sms
c60947549042072745c954f185c5efd5
02346c8774c1cab9e3ab420a6f5c8424
0314a6da893cd0dcb20e3b46ba62d727 Win32/Occamy.B!bit
Win32/Delpem.A
Win32/Occamy.C!MTB
Win32/DownloadGuide
Win32/Puwaders.A!ml
Malware Family
VirTool
VirTool
VirTool
SoftwareBundler
SoftwareBundler
SoftwareBundler
PasswordStealer
PasswordStealer
PasswordStealer
PasswordStealer
PUA
PUA
PUA
TrojanDownloader
Ransomware
HackTool
Trojan
Trojan
Trojan
Trojan
Table 4: Real malware used in the experiments to create malicious installers.
an existing benign installer and embed malware in it. The
installer executes the malware during installation. This ap-
proach requires us to decompile existing installers and recom-
pile them with malware.
To construct representative malicious installers, we select
software using three popular installation frameworks: Nullsoft
Scriptable Install System (NSIS), Inno Setup, and SFX, and
insert every malware sample in Table 4. Those frameworks
are popular vehicles to spread malware [16, 66]; they are also
widely used among popular software installers. Based on our
survey of 1,237 Windows applications hosted on Softpedia,
over 86% of the installers use these three frameworks.
5.2
Implementation & Experimental Setup
We implement SIGL’s data collection and graph generation
module in Java 8 so that we can use the existing audit event
server deployed in our enterprise, which provides APIs only
in Java. SIGL’s core analytic algorithms, including node em-
bedding, modeling, and anomaly detection, are implemented
in Python 3.5 and PyTorch 1.1.0 with the CUDA 9.0 toolkit.
We use the Gensim [65] library to generate node embeddings
for training graphs and the Deep Graph Library (DGL) [1] to
implement deep graph neural networks on top of PyTorch.
For all experiments, we partition the benign input data
into a training set (70%), a validation set (10%), and a false
positive test set (20%). Table 2 shows the number of software
installation graphs used for training, validation, and testing.
We parameterize the node context for node embedding
with window size 5, 10 random walks, each of length 10,
and 128 dimensions. The same window size is used in à la
carte. We use the skip-gram training algorithm with negative
sampling [26] and run 20 epochs over the corpus.
SIGL performs unsupervised learning, so we need only
benign installers for training. We train SIGL’s deep graph
neural network on a system with a NVIDIA GTX 1080 Ti
GPU with 12 GiB of memory. We train the model for 100
epochs with the training batch size set to 25, validate model
performance after every epoch, and choose the model that
produces the best performance on validation data.
USENIX Association
30th USENIX Security Symposium    2351
Precision
Method
SIGL
Commercial TDS [59]
StreamSpot [48]
Frappuccino [28]
Table 5: Overall SIGL experimental results compared to other approaches.
Recall
0.99
0.59
0.52
0.12
0.96
0.90
0.72
0.51
0.06
0.93
0.03
0.05
0.96
0.12
0.68
0.21
0.94
0.07
0.97
0.95
FP Percentage
Accuracy
F-Score
F-Score
Precision
Accuracy
Software Installer
FireFox
FileZilla
PWSafe
MP3Gain
ShotCut
TeamViewer
Foobar
7Zip
TurboVNC
WinMerge
Launchy
Skype
WinRAR
DropBox
Slack
Flash
OneDrive
NotePad++
ICBC Anti-Phishing
ESET AV Remover
Table 6: SIGL experimental result breakdown for each software installer.
5.3 SIGL Experimental Results
Recall
0.70
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.77
0.98
0.98
0.98
0.98
0.91
1.0
0.98
0.97
0.98
0.88
1.0
0.98
0.95
0.95
1.0
0.84
1.0
0.98
0.98
0.74
0.99
0.99
0.99
0.99
0.93
1.0
0.99
0.98
0.99
0.89
1.0
0.98
0.95
0.95
1.0
0.85
1.0
0.98
0.98
0.78
0.98
0.98
0.98
0.98
0.87
1.0
0.98
0.95
0.98
0.8
1.0
0.95
0.91
0.91
1.0
0.74
1.0
0.95
0.95
We evaluate SIGL’s detection performance on 625 mali-
cious installers across a variety of software packages (Table 2).
Table 5 shows that SIGL achieves over 90% precision, recall,
accuracy, and F-score correctly identifying all malicious in-
stallers in the wild.
SIGL shares a common characteristic with many anomaly-
based detection systems in that it produces more false pos-
itives (FPs) than false negatives (FNs), as reﬂected by its
higher recall (99%) than precision (94%). However, preci-
sion and recall are well balanced, meaning that SIGL does
not reduce the number of FPs by compromising its ability to
detect actual malicious installers, as do other anomaly-based
detection systems (§ 5.4).
Table 6 further details the experimental results for each
installer. It shows that SIGL delivers consistent performance
over a wide range of software exhibiting vastly different instal-
lation behaviors. We investigate two, FireFox and OneDrive,
that have slightly lower precision and recall. We notice that
the installation process of these applications sometimes in-
cludes software updates that are captured in SIGs. SIGL has
difﬁculty generalizing both installation and update behavior
from only a few instances of training graphs, resulting in
lower performance than that of other applications.
5.4 Comparison Study
We compare SIGL to our in-house commercial TDS [59]
and two provenance-based research anomaly detection sys-
tems, StreamSpot [49] and Frappuccino [28]. We do not com-
pare SIGL to other commercial TDS, because they typically
require intelligence service subscriptions and customized de-
ployment from external vendors. Similarly, we exclude com-
parison to academic systems (such as Mastino [64] and Drop-
per Effect [45], see § 8) that leverage proprietary information
from security vendors that is unavailable to us. SIGL enables
an enterprise to detect threats using local, enterprise-wide
information readily available to system administrators; ad-
ditional protection from global services (e.g., Symantec) is
complementary.
We conducted a preliminary experiment to show that our
malicious installers (created using real malware in Table 4)
can already signiﬁcantly reduce the efﬁcacy of commercial
anti-virus tools, even without changing malware signatures.
We upload the original malware samples (Table 4) to Virus-
Total, which scans the samples and reports the number of
anti-virus engines that detect them. On average, 80.8% of
the engines detect the malware listed in Table 4; the lowest
detection rate was 70.0%. Testing on our malicious installers,
VirusTotal reports only 42.4% on average and the minimum
detection rate of 10.8%. Therefore, we do not further compare
SIGL to commercial anti-virus tools, because their limitations
are well documented in the literature [64].
We brieﬂy describe each evaluated system and discuss the
results in the remainder of this section. Table 5 summarizes
the overall results for all the systems in this study.
Commercial TDS. The commercial TDS [59] inspects every
event between a process and a ﬁle and determines its potential
to be a threat based on two factors: A) the familiarity of a ﬁle
– if the TDS has some knowledge of the ﬁle in the past (based
on the ﬁle name in the training data), then it is less likely to be
malicious; B) the diversity of a process – if a process writes to
many different ﬁles, then the write event itself is less likely
to be malicious, even if the ﬁle is unfamiliar to the TDS.
Frappuccino. Frappuccino [28] detects program anomalies
by analyzing whole-system provenance graphs [60]. It ex-
plores the graph’s local neighborhood structures using a
vertex-centric label propagation algorithm to compare the
similarity between two provenance graphs. Based on the as-
sumption that normal behavior of a program produces similar
provenance graphs when it runs on different host systems, it
clusters normal provenance graphs of many running instances
of the program as its model and detects abnormal program
runs when their graphs cannot ﬁt into any existing clusters. We
compare SIGL to Frappuccino, because both systems make