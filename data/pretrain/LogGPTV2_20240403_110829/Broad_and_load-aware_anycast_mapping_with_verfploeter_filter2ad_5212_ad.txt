Table 6: Quantifying di(cid:29)erences B-Root anycast with di(cid:29)er-
ent measurement methods and times.
Finally, we see that load is higher in some regions than the num-
ber of blocks would suggest, particularly in India. This di(cid:29)erence
may be explained by many users using relatively few IP blocks in
these areas, with a great deal of deployed network address transla-
tion behind those blocks.
0275583110138166193221249+size (blocks)siteCDGCPHENSHNDIADLHRMIASYDCopyright (C) 2017 by University of Southern California020k40k61k81k101k122k142k163k183k+size (blocks)siteCDGCPHENSHNDIADLHRMIASYDCopyright (C) 2017 by University of Southern CaliforniaIMC ’17, November 1–3, 2017, London, United Kingdom
de Vries, Schmidt, Hardaker, Heidemann, de Boer, and Pras
(a) Geographic distribution of load by site for B-Root, as inferred from Verfploeter (Datasets: LB-4-12).
(b) Geographic distribution of load for .nl, as determined by tra(cid:28)c logs (Dataset: LN-4-12).
Figure 4: Measured DNS tra(cid:28)c over geography for B-Root and .nl.
Quantifying Di(cid:29)erences from VPs to Blocks to Load: While
Figure 2 and Figure 2b show visual di(cid:29)erences, we turn to Table 6
to quantify those di(cid:29)erences and their impact on assessment of
catchment sizes in B-Root. When we compare Atlas, Verfploeter,
and Verfploeter with load, we see very di(cid:29)erent measurements
(thousands of VPs, millions of blocks, or billions of queries per
day). Load estimates (§3.2) determine di(cid:29)erent weighting factors
and result in di(cid:29)erent fractions of tra(cid:28)c between the LAX and MIA
sites, as shown in the “% LAX” column. In §5.5 we will compare
these values to measured load to see which is most accurate, but
next we see how these changes will be even larger for DNS services
with less even global load.
Uneven Load: Load for B-Root is global and largely follows
the distribution of Internet users, so Figure 4a has only moderate
di(cid:29)erences from Figure 2b.
Other DNS systems are more regional. Figure 4b shows load for
four of the .nl nameservers, the country domain for the Netherlands.
They cannot easily collect data from their two nameservers that
use anycast, so they are omitted from this plot and it may under-
represent global tra(cid:28)c, but we know it captures at least half of all
global tra(cid:28)c to this domain.
Unlike B-Root, we see that the majority of tra(cid:28)c to .nl is from
Europe and the Netherlands. There is also signi(cid:27)cant tra(cid:28)c from
the U.S. and some global tra(cid:28)c. With this type of client distribu-
tion, calibrating the measured catchment using load information is
critical.
5.5 Using Verfploeter to Predict Load
We next examine how accurate Verfploeter’s load modeling can
predict future load. Our goal is to determine how much unmappable
blocks (§5.4) a(cid:29)ect accuracy, and how much routing and load shifts
over time. In both cases we observe partial information and predict
load for the unobserved remainder (observing responses per blocks
and predicting load, or observing load now and predicting future
load), then compare that against complete information. A study of
long-term predictions will require more experience with Verfploeter,
but we address the basic accuracy question here.
We study the accuracy of load predictions with Verfploeter by
analyzing what network blocks B-Root sees tra(cid:28)c from that Verf-
ploeter has found to be unmappable by examining the DNS network
load at B-Root on 2017-05-15 (Dataset: LB-5-15) and the Verfploeter
analysis performed on the same day (Dataset: SBV-5-15). (Since Tan-
gled is not a production service, we cannot study its operational
load.) Recall from Table 6 that although Verfploeter (cid:27)nds 87.8% of
network blocks reach LAX, the load prediction is that 81.6% of traf-
(cid:27)c should go to LAX. That prediction does not consider blocks that
send tra(cid:28)c to B-Root but do not respond to Verfploeter (12.9% from
Table 5).
Predicted vs. Measured Load: The last line of Table 6 shows
the actual load of 81.4%, as measured at all B-Root sites on 2017-05-
15. We see our 81.6% prediction using same-day Verfploeter and
load is quite close to the measured result. Our (cid:27)rst observation is
that this result suggests Verfploeter-unobservable blocks do not have
signi(cid:27)cant e(cid:29)ects on our overall load estimate. (Future work could
01342694035386728079421k1k+queries/ssiteLAXMIAUNKCopyright (C) 2017 by University of Southern California02625247861k1k1k1k2k2k+queries/sserversns1ns2ns3ns4Broad and Load-Aware Anycast Mapping with Verfploeter
IMC ’17, November 1–3, 2017, London, United Kingdom
strengthen this claim by demonstrating it for services other than
B-Root.) Although they account for 17.6% of queries (Table 5, and
the red slices in Figure 4a), the fraction of tra(cid:28)c that goes to each
B-Root site appears to follow the ratio seen in measured blocks.
Our second observation is that our load-weighted predictions are
very close to observed load. Verfploeter without load adjustment is
further o(cid:29), with 87.8% of blocks going to LAX. We conclude that
weighting by load is important. Surprisingly, Atlas estimates, at
82.4%, are actually closer than Verfploeter if Verfploeter is not load-
weighted.
The key take-away of this result is that with load-weighted Verf-
ploeter preliminary results suggest it is possible to make reasonable
predictions about future anycast deployments by measuring the de-
ployment on a test network and predicting future tra(cid:28)c levels using
recent load data. We hope to expand these results beyond B-Root
as ongoing work.
Long-duration predictions: Finally, we can also look at long-
duration prediction. We performed a similar prediction analysis
in advance of the B-Root deployment using the Verfploeter data
gathered on 2017-04-21 and network tra(cid:28)c from 2017-04-12. We see
a fairly large shift in blocks between these dates, with Verfploeter
shifting from 82.4% to LAX in April to 87.8% in May. By weighting
the SBV-4-21Verfploeter dataset from the B-Root test pre(cid:27)x with
the LB-4-12 measured load, we (cid:27)nd that the predicted DNS request
load arriving at LAX is 76.2%. This is signi(cid:27)cantly less than the
81.6% measured load in LB-5-15, which highlights the discrepancy
between shifts in routing over one month between the SBV-4-21 and
SBV-5-15 dataset collection periods.
This shift suggests that the accuracy of load estimates depends
on how old the data is. We know that routing changes in the Internet
over time [9]; this early result suggests some care must be taking
with long-duration predictions. We expect that predictions further
into the future will be less accurate than short-term predictions.
While we are collecting data to answer this question, such a study
is future work.
6 RESULTS: UNDERSTANDING ANYCAST
WITH VERFPLOETER
We next use Verfploeter to explore three questions about anycast.
These questions have each been raised in prior work; here we use
Verfploeter to revisit them (and compare to them, in §6.1 and §6.3),
both to show its utility and to re(cid:27)ne these prior results.
6.1 Use of AS Prepending in B-Root
An important operational question for B-Root is understanding how
to balance load between sites. Although both sites are able to handle
normal tra(cid:28)c, DNS operators need to shift load during emergencies,
like for DDoS attacks that can be absorbed using multiple sites [33].
Operators may also want to control load during regular operation,
perhaps because di(cid:29)erent sites have cost structures that are tra(cid:28)c-
sensitive.
We used RIPE Atlas and Verfploeter to investigate the use of AS
Prepending to adjust the catchment of a test pre(cid:27)x on B’s sites. AS
Prepending is a tra(cid:28)c engineering approach where an operator
increases the BGP path length at one site to make that route less
desirable than other routes with shorter AS paths [37]. Figure 5
Figure 5: Split between MIA and LAX in VPs for Atlas and
/24s for Verfploeter. (Dataset: SBA-4-20, SBA-4-21, SBV-4-21.)
Figure 6: Predicted load for B-Root with multiple AS
prepending combinations; catchment data from Verfploeter
with load (Datasets: SBV-4-21, LB-4-12).
shows how the distribution changes as AS prepending is applied
between the two sites, as measured with both methods. (Note that
the units for each measurement is di(cid:29)erent: RIPE Atlas is measured
in VPs, and Verfploeter is measured in /24 blocks.) By default, with
no prepending, 74% of Atlas VPs arrive at LAX, while Verfploeter
shows that 78% of responsive /24 pre(cid:27)xes will arrive at LAX.
These results show that both measurement systems are useful to
evaluate routing options. With only two sites, either measurement
method seems su(cid:28)cient for rough analysis. We expect the greater
precision of Verfploeter will be important with more sites, and to
assist with the trial-and-error process required when deploying
more subtle methods of route control (for example, use of BGP
communities tra(cid:28)c [37]).
We next study how load shifts at di(cid:29)erent prepending values
over the course of a day. For this study we measure load over 24
hours, summarizing it per hour, then combine that with measured
values from (cid:27)ve di(cid:29)erent prepending con(cid:27)gurations (each taken
once on a di(cid:29)erent day). Figure 6 shows this combination using
catchment data from Verfploeter combined with DITL data of B-
Root (2017-04-12). In the top graph, nearly all tra(cid:28)c goes to the
MIA site, since LAX’s BGP announcement includes an “AS prepend-
ing” of one (and the small share of load, “UNKNOWN”, that is not
mappable by Verfploeter). When LAX and MIA announce routes
 0 0.2 0.4 0.6 0.8 1+1 LAXequal+1 MIA+2 MIA+3 MIAfraction to LAXprepending factorRIPE Atlas (VPs)Verfploeter (/24 blocks)05101520025klax+1MIALAXUNKNOWN05101520025kequal05101520025kmia+105101520025kmia+205101520Hours after 2017-04-12 00:00 (UTC)025kmia+3Queries per second(avg, 1 hour bins)IMC ’17, November 1–3, 2017, London, United Kingdom
de Vries, Schmidt, Hardaker, Heidemann, de Boer, and Pras
without prepending, most of the tra(cid:28)c load shifts to LAX (second
graph from top-down). The last three graphs show the results of
prepending MIA’s BGP announcement by up to 3 times, resulting
in an increasing tra(cid:28)c share shifting to LAX. However, even by
announcing our pre(cid:27)x with 3 times our AS at MIA (MIA+3), we
still see a small fraction of tra(cid:28)c being mapped to MIA. These few
networks are likely either customers of MIA’s ISP, or perhaps ASes
that choose to ignore prepending.
6.2 Discovering Divisions Within ASes
Prior work (particularly anycast studies using RIPE Atlas) often
assumed that anycast catchments align with ASes, thus one VP can
represent where the load of the entire AS goes. While generally
true for smaller ASes, this assumption is less likely to hold for
large, multi-national ASes where di(cid:29)erent parts of the AS may be
served by di(cid:29)erent anycast sites. Such large ASes are likely to have
geographically distributed peering locations and so may prefer to
direct some of their users to di(cid:29)erent anycast sites to reduce service
latency.
This high density of VPs in Verfploeter allows us to test this
assumption by looking for di(cid:29)erences in anycast catchments that
occur within individual ASes. We (cid:27)rst remove those VPs from the
dataset that show instability (see §6.3), to prevent unstable routing
from being classi(cid:27)ed as a division within the AS. Without removing
these VPs we observe approximately 2% more divisions (e.g., ASes
which are served by more than one site). We count the number
of sites that are seen (from di(cid:29)erent VPs) within a single AS, in a
single measurement round.
In total, we see multiple sites from 7,188 ASes, or approximately
12.7% of all ASes that were announcing at least a single pre(cid:27)x at
the time of the measurement. Note that this is a lower-bound, using
a larger and/or more diverse anycast service we might be able to
determine a higher, and more accurate, percentage of ASes that are
split into multiple individually routed parts.
Routing policies (like hot-potato routing) are a likely cause for
these divisions. And, as routing on the Internet is largely determined
by BGP, we show the number of pre(cid:27)xes that are announced via
BGP by an AS versus the number of sites that it sees in Figure 7.
Indeed, those ASes that announce more pre(cid:27)xes tend to see a higher
amount of sites from their network.
In Figure 8 we show the number of sites that are seen from
announced pre(cid:27)xes, grouped by pre(cid:27)x length. VPs in pre(cid:27)xes longer
than a /15 are mapped to more than a single site in most cases. Even
though 80% of these routed pre(cid:27)xes are covered by one VP (the
bottom graphs in Figure 8), these are all small pre(cid:27)xes. About 20%
of these routed pre(cid:27)xes are seeing more than one site and require
multiple pre(cid:27)xes, but larger pre(cid:27)xes are often divided further—75%
of pre(cid:27)xes larger than /10s see multiple sites and require multiple
VPs. Although only 20% of pre(cid:27)xes, multiple VPs are required
in pre(cid:27)xes that account for approximately 38% of the measured
address space.
These results show that, in order to get a complete view of
the catchment, in many cases you need more than a single VP
per AS. While the quantitative results are speci(cid:27)c to B-Root and
Tangled, this qualitative result (ASes can be subdivided) applies
more generally. Measurements from platforms with fewer VPs often
Figure 7: The number of sites that are seen from an AS versus
the median amount of pre(cid:27)xes that are announced by those
ASes. (Dataset: STV-3-23.)
assume that each VP can represent its AS, but likely lose precision
in large ASes.
6.3 Stability of Anycast for Clients
A long-term concern with anycast is how stable the association of
an anycast client is with its site [48]. Since TCP connections require
shared state at both ends, if users switch anycast sites within the
lifetime of a TCP connection, that connection will break and need
to be restarted. The existence of multiple successful CDNs that use
IP anycast (including Bing, Edgecast, and Cloud(cid:30)are) suggest that
anycast is almost always stable, but recent work has suggested that
anycast may be persistently unstable for a tiny fraction of (user,
service) combinations (less than 1%) [48]. From the viewpoint of a
service operator, it is interesting to know if a single measurement
can be representative for a longer time, or if the catchment is con-
tinuously in (cid:30)ux.
Verfploeter allows us to revisit this question from Tangled to
many VPs. We measured the global catchment of our testbed ev-
ery 15 minutes for a day (96 observations). Considering the short-
lived nature of many TCP connections this interval might be too
long to detect rapid (cid:30)uctuations, however, it is enough to give an
impression of the overall stability of catchments. We categorize
the responses (or non-responses) into 4 groups: stable, VPs that
maintain the same catchment across measurements; (cid:30)ipped, VPs
that change catchment, with responses sent to a di(cid:29)erent anycast
site than the prior measurement; to-NR, VPs that switched to “not
responding” in the current measurement; and from-NR, VPs that
started responding in the current measurement. We do not count
VPs that remain non-responsive after being counted as to-NR.
Figure 9 shows the results of one day of these measurement.
Because the fractions of stable and (cid:30)ipping are so di(cid:29)erent, we
break the graph into three sections. We see that the catchment is
very stable across the measurement rounds, with a median of 3.54M
(about 95% of the 3.71M that respond) VPs always replying and
maintaining their prior catchment. The fraction of VPs that (cid:30)uctuate
between responsive and non-responsive states is small across all
96 measurements. A median of 89k (about 2.4%) VPs changed from
responsive to non-responsive between measurements, and about
the same number (cid:30)ipping back. Note that (cid:30)uctuating and (cid:30)ipping
VPs are not necessarily always the same ones.
100101102103Announced prefixes: median and 5,25,75,95 percentiles1234567Number of sitesBroad and Load-Aware Anycast Mapping with Verfploeter
IMC ’17, November 1–3, 2017, London, United Kingdom