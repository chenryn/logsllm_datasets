corresponding singular value, i.e. σΣ1 and σΣ2, respectively (cf. [70,
ch. 4]) (recall from Theorem 1 that diaд(ΛΣ) = [σΣ1, σΣ2]). This is
illustrated by Fig. 3, Left. Therefore, when considering this 2D mul-
tivariate Gaussian noise, we arrive at the following interpretation
of the SVD of its covariance matrix: the noise is distributed toward
the two principal directions specified by wΣ1 and wΣ2, with the
variance scaled by the respective singular values, σΣ1 and σΣ2.
We can extend this interpretation to a more general case with
m > 2, and also to the other covariance matrix Ψ. Then, we have
a full interpretation of MVGm,n(0, Σ, Ψ) as follows. The matrix-
valued noise distributed according to MVGm,n(0, Σ, Ψ) has two
components: the row-wise noise, and the column-wise noise. The
row-wise noise and the column-wise noise are characterized by the
two covariance matrices, Σ and Ψ, respectively, as follows.
For the row-wise noise.
• The row-wise noise is characterized by Σ.
• SVD of Σ = WΣΛΣWT
decomposes the row-wise noise into
two components – the directions and the variances of the
noise in those directions.
• The directions of the row-wise noise are specified by the
• The variance of each row-wise-noise direction is indicated
column vectors of WΣ.
Σ
by its corresponding singular value in ΛΣ.
For the column-wise noise.
• The column-wise noise is characterized by Ψ.
• SVD of Ψ = WΨΛΨWT
decomposes the column-wise noise
into two components – the directions and the variances of
the noise in those directions.
• The directions of the column-wise noise are specified by the
• The variance of each column-wise-noise direction is indi-
column vectors of WΨ.
Ψ
cated by its respective singular value in ΛΨ.
Since MVGm,n(0, Σ, Ψ) is fully characterized by its covariances,
these two components of the matrix-valued noise drawn from
Figure 3: (Left) An ellipsoid of equi-density contour of a 2D
multivariate Gaussian distribution. The two arrows indicate
the principal axes of this ellipsoid. (Right) Directional noise
(red) and i.i.d. noise (blue) drawn from a 2D-multivariate
Gaussian distribution. The green line represents a possible
utility subspace that can benefit from this instance of direc-
tional noise.
Remark 2. The values of the generalized harmonic numbers – Hr ,
and Hr,1/2 – can be obtained from the table lookup for a given
value of r, or can easily be computed recursively [84].
The sufficient condition in Theorem 3 yields an important ob-
servation: the privacy guarantee of the MVG mechanism depends
only on the singular values of Σ and Ψ. In other words, we may
have multiple instances of MVGm,n(0, Σ, Ψ) that yield the exact
same privacy guarantee (cf. Fig. 2). To better understand this phe-
nomenon, in the next section, we introduce the novel concept of
directional noise.
5 DIRECTIONAL NOISE
Recall from Theorem 3 that the (ϵ, δ)-differential-privacy condition
for the MVG mechanism only applies to the singular values of
the two covariance matrices Σ and Ψ. Here, we investigate the
ramification of this result via the novel notion of directional noise.
5.1 Motivation for Non-i.i.d. Noise
For a matrix-valued query function, the standard method for basic
mechanisms that use additive noise is by adding the independent
and identically distributed (i.i.d.) noise to each element of the matrix
query. However, as common in matrix analysis [46], the matrices
involved often have some geometric and algebraic characteristics
that can be exploited. As a result, it is usually the case that only
certain “parts” – the term which will be defined more precisely
shortly – of the matrices contain useful information. In fact, this is
one of the rationales behind many compression techniques such as
the popular principal component analysis (PCA) [6, 55, 70]. Due to
this reason, adding the same amount of noise to every “part” of the
matrix query may be highly suboptimal.
5.2 Directional Noise as a Non-i.i.d. Noise
Let us elaborate further on the “parts” of a matrix. In matrix anal-
ysis, matrix factorization [46] is often used to extract underlying
properties of a matrix. This is a family of algorithms and the spe-
cific choice depends upon the application and types of insights it
requires. Particularly, in our application, the factorization that is
MVGm,n(0, Σ, Ψ) provide a complete interpretation of the matrix-
variate Gaussian noise.
5.3 Directional Noise & MVG Mechanism
We now revisit Theorem 3. Recall that the sufficient(ϵ, δ)-differential-
privacy condition for the MVG mechanism puts the constraint only
on the singular values of Σ and Ψ. However, as we discuss in the
previous section, the singular values of Σ and Ψ only indicate the
variance of the noise in each direction, but not the directions they
are attributed to. In other words, Theorem 3 suggests that the MVG
mechanism preserves (ϵ, δ)-differential privacy as long as the overall
variances of the noise satisfy a certain threshold, but these variances
can be attributed non-uniformly in any direction.
This claim certainly warrants further discussion, and we defer it
to Sec. 6, where we present the technical detail on how to practically
implement this concept of directional noise. It is important to only
note here that this claim does not mean that we can avoid adding
noise in any particular direction altogether. On the contrary, there
is still a minimum amount of noise required in every direction for
the MVG mechanism to guarantee differential privacy, but the noise
simply can be attributed unevenly in different directions (see Fig. 3,
Right, for an example).
5.4 Utility Gain via Directional Noise
There are multiple ways to exploit the notion of directional noise
to enhance utility of differential privacy. Here, we present two
methods – via the domain knowledge and via the SVD/PCA.
5.4.1 Utilizing Domain Knowledge. This method is best described
by examples. Consider first the personalized warfarin dosing prob-
lem [31], which can be considered as the regression problem with
the identity query, f (X) = X. In the i.i.d. noise scheme, every fea-
ture used in the warfarin dosing prediction is equally perturbed.
However, domain experts may have prior knowledge that some
features are more critical than the others, so adding directional
noise designed such that the more critical features are perturbed
less can potentially yield better prediction performance.
Let us next consider a slightly more involved matrix-valued
query: the covariance matrix, i.e. f (X) = 1
N XXT , where X ∈
RM×N has zero mean and the columns are the records/samples.
Consider now the Netflix prize dataset [71, 72]. A popular method
for solving the Netflix challenge is via low-rank approximation
[5], which often involves the covariance matrix query function
[8, 13, 66]. Suppose domain experts indicate that some features
are more informative than the others. Since the covariance matrix
has the underlying property that each row and column correspond
to a single feature [70], we can use this domain knowledge with
directional noise by adding less noise to the rows and columns
corresponding to the informative features.
In both examples, the directions chosen are among the standard
basis, e.g. v1 = [1, 0, . . . , 0]T , v2 = [0, 1, . . . , 0]T , which are ones of
the simplest forms of directions.
5.4.2 Using Differentially-Private SVD/PCA. When domain knowl-
edge is not available, an alternative approach is to derive the di-
rections via the SVD or PCA. In this context, SVD and PCA are
identical with the main difference being that SVD is compatible
with any matrix-valued query function, while PCA is best suited to
the identity query. Hence, the terms may be used interchangeable
in the subsequent discussion.
As we show in Sec. 5.2, SVD/PCA can decompose a matrix into
its directions and variances. Hence, we can set aside a small portion
of privacy budget to derive the directions from the SVD/PCA of
the query function. This is illustrated in the following example.
Consider again the warfarin dosing problem [31], and assume that
we do not possess any prior knowledge about the predictive fea-
tures. We can learn this information from the data by spending a
small privacy budget on deriving differentially-private principal
components (P.C.). Each P.C. can then serve as a direction and, with
directional noise, we can selectively add less noise in the highly
informative directions as indicated by PCA.
Clearly, the directions in this example are not necessary among
the standard basis, but can be any unit vector. This example illus-
trates how directional noise can provide additional utility benefit
even without the domain knowledge. There have been many works
on differentially-private SVD/PCA [7, 8, 13, 27, 38–40, 53], so this
method is very generally applicable. Again, we reiterate that the
approach similar to the one in the example using SVD applies to
a general matrix-valued query function. Fig. 3, Right, illustrates
this. In the illustration, the query function has two dimensions,
and we have obtained the utility direction, e.g. from SVD, as rep-
resented by the green line. This can be considered as the utility
subspace we desire to be least perturbed. The many small circles in
the illustration represent how the i.i.d. noise and directional noise
are distributed under the 2D multivariate Gaussian distribution.
Clearly, directional noise can reduce the perturbation experienced
on the utility directions.
In the next section, we discuss how to implement directional
noise with the MVG mechanism in practice and propose two simple
algorithms for two types of directional noise.
6 PRACTICAL IMPLEMENTATION
The differential privacy condition in Theorem 3, even along with
the notion of directional noise in the previous section, still leads to
a large design space for the MVG mechanism. In this section, we
present two simple algorithms to implement the MVG mechanism
with two types of directional noise that can be appropriate for a wide
range of real-world applications. Then, we conclude the section
with a discussion on a sampling algorithm for MVGm,n(0, Σ, Ψ).
As discussed in Sec. 5.3, Theorem 3 states that the MVG mech-
anism satisfies (ϵ, δ)-differential privacy as long as the singular
values of Σ and Ψ satisfy the sufficient condition. This provides
tremendous flexibility in the choice of the directions of the noise.
First, we notice from the sufficient condition in Theorem 3 that the
singular values for Σ and Ψ are decoupled, i.e. they can be designed
independently so long as, when combined, they satisfy the specified
condition. Hence, the row-wise noise and column-wise noise can be
considered as the two modes of noise in the MVG mechanism. By
this terminology, we discuss two types of directional noise: the
unimodal and equi-modal directional noise.
1
2
i (Σ) ≤ 1
n
i =1
σ
(3)
6.1 Unimodal Directional Noise
For the unimodal directional noise, we select one mode of the noise
to be directional noise, whereas the other mode of the noise is set
to be i.i.d. For this discussion, we assume that the row-wise noise
is directional noise, while the column-wise noise is i.i.d. However,
the opposite case can be readily analyzed with the similar analysis.
We note that, apart from simplifying the practical implementa-
tion that we will discuss shortly, this type of directional noise can
be appropriate for many applications. For example, for the identity
query, we may not possess any prior knowledge on the quality
of each sample, so the best strategy would be to consider the i.i.d.
column-wise noise (recall that samples are the column vectors).
Formally, the unimodal directional noise sets Ψ = In, where In
is the n × n identity matrix. This, consequently, reduces the design
space for the MVG mechanism with directional noise to only that
of Σ. Next, consider the left side of Eq. (1), and we have
√
n.
(cid:118)(cid:116) m
(2)
(cid:13)(cid:13)σ(Σ−1)(cid:13)(cid:13)2 =
m
i =1
σ
1
2
i (Σ) , and(cid:13)(cid:13)σ(Ψ−1)(cid:13)(cid:13)2 =
(−β +(cid:112)
2 + 8αϵ)4
4
.
β
16α
If we square both sides of the sufficient condition and re-arrange
it, we get a form of the condition such that the row-wise noise in
each direction is decoupled:
This form gives a very intuitive interpretation of the directional
noise. First, we note that, to have small noise in the ith direction,
σi(Σ) has to be small (cf. Sec. 5.2). However, the sum of 1/σ
i (Σ) of
2
the noise in all directions, which should hence be large, is limited
by the quantity on the right side of Eq. (3). This, in fact, explains
why even with directional noise, we still need to add noise in every
direction to guarantee differential privacy. Consider the case when
= ∞,
1
we set the noise in one direction to be zero, and we have lim
σ→0
σ
which immediately violates the sufficient condition in Eq. (3).
i (Σ) is the inverse of the variance
2
of the noise in the ith direction, so we may think of it as the precision
measure of the query answer in that direction. The intuition is that
the higher this value is, the lower the noise added in that direction,
and, hence, the more precise the query value in that direction is.
From this description, the constraint in Eq. (3) can be aptly named
as the precision budget, and we immediately have the following
theorem.
From Eq. (3), the quantity 1/σ
Theorem 4. For the MVG mechanism with Ψ = In, the precision
budget is (−β +(cid:112)
2 + 8αϵ)4/(16α
β
4
n).
Finally, the remaining task is to determine the directions of the
noise and form Σ accordingly. To do so systematically, we first
decompose Σ by SVD as,
Σ = WΣΛΣWT
Σ .
This decomposition represents Σ by two components – the direc-
tions of the row-wise noise indicated by WΣ, and the variance of
the noise indicated by ΛΣ. Since the precision budget only puts
constraint upon ΛΣ, this decomposition allows us to freely chose
any unitary matrix for WΣ such that each column of WΣ indicates
each independent direction of the noise.
Algorithm 1 MVG mech. w/ unimodal directional noise.
Input: (a) privacy parameters: ϵ, δ; (b) the query function and its
sensitivity: f (X) ∈ Rm×n, s2(f ); (c) the precision allocation
strategy θ ∈ (0, 1)m : |θ|1 = 1; and (d) the m directions of the
row-wise noise WΣ ∈ Rm×m.
(1) Compute α and β (cf. Theorem 3).
(2) Compute the precision budget P =
(3) for i = 1, . . . , m:
i) Set pi = θi P.
ii) Compute the ith direction’s variance, σi(Σ) = 1/√
pi.
(4) Form the diagonal matrix ΛΣ = diaд([σ1(Σ), . . . , σm(Σ)]).
(5) Derive the covariance matrix: Σ = WΣΛΣWT
Σ
(6) Draw a matrix-valued noise Z from MVGm,n(0, Σ, I).
Output: f (X) + Z.
√
β 2+8αϵ)4
(−β +
16α 4n
.
.
ith direction is. Moreover, the algorithm enforces thatm
Therefore, we present the following simple approach to design
the MVG mechanism with the unimodal directional noise: under a
given precision budget, allocate more precision to the directions of
more importance.
Alg. 1 formalizes this procedure. It takes as inputs, among other
parameters, the precision allocation strategy θ ∈ (0, 1)m, and the di-
rections WΣ ∈ Rm×m. The precision allocation strategy is a vector
of size m, whose elements, θi ∈ (0, 1), corresponds to the impor-
tance of the ith direction indicated by the ith orthonormal column
vector of WΣ. The higher the value of θi, the more important the
i =1 θi ≤ 1
to ensure that the precision budget is not overspent. The algorithm
proceeds as follows. First, compute α and β and, then, the preci-
sion budget P. Second, assign precision to each direction based
on the precision allocation strategy. Third, derive the variance of
the noise in each direction accordingly. Then, compute Σ from the
noise variance and directions, and draw a matrix-valued noise from
MVGm,n(0, Σ, I). Finally, output the query answer with additive