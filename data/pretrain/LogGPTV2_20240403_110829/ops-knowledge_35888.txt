User: How to use Python to write a simple crawler program that automatically grabs data from a certain website periodically and saves it to a local file?
Assistant: The following is an example of a simple crawler program using Python: ```python import requests from bs4 import BeautifulSoup # Set the URL that needs to crawl data url = 'https://www.example.com/' # Send HTTP requests, and Get response content response = requests.get(url) html = response.content # Use BeautifulSoup to parse HTML soup = BeautifulSoup(html, 'html.parser') data = soup.find('div', {'class': 'data '}).text # Save the data to a file with open('data.txt', 'w') as f: f.write(data) ``` To run the crawler regularly, you can use Python's scheduled task module `schedule`, for example: ```python import schedule import time def crawl(): # Write the code of the crawler here schedule.every(1).day.at('00:00').do(crawl) # Run at 0 am every day while True: schedule.run_pending() time.sleep(60) ``` Note: Please respect the crawler agreement and relevant laws and regulations of the website when using the Python crawler program, and malicious crawling and other illegal activities are not allowed.