read, fscanf
fscanf
fscanf
967
220
127
145
174
150
181
410
149
252
188
297
198
216
168
165
Network daemons
876
286
1,037
466
1,277
Other applications
1,159
831
292
43
1,124
144
826
118
65,795
15,173
16,195
6,390
8,003
4,659
6,587
28,858
6,533
20,677
10,696
25,906
9,195
10,966
8,952
5,539
140,847
19,319
153,306
31,130
133,666
112,762
93,618
70,358
2,135
86,783
17,573
87,456
9,815c
45,630 (69.35%)
10,076 (66.41%)
11,160 (68.91%)
4,238 (66.32%)
5,366 (67.05%)
3,254 (69.84%)
4,343 (65.93%)
19,500 (67.57%)
4,517 (69.14%)
14,082 (68.10%)
7,143 (66.78%)
17,503 (67.56%)
6,404 (69.65%)
7,251 (66.12%)
6,006 (67.09%)
3,822 (69.00%)
93,058 (66.07%)
13,676 (70.79%)
106,821 (69.68%)
21,713 (69.75%)
92,041 (68.86%)
67,808 (60.13%)
74,389 (79.46%)
42,840 (60.89%)
1,477 (69.18%)
65,636 (75.63%)
9,900 (56.34%)
66,601 (76.15%)
6,805 (69.33%)
4,083 (6.21%)
2,067 (13.62%)
3,524 (2.18%)
1,875(29.34%)
548 (6.85%)
918 (19.70%)
742 (11.26%)
3,693 (12.80%)
497 (7.61%)
684 (3.31%)
1,640 (15.33%)
5,478 (21.15%)
1,673 (18.19%)
825 (7.52%)
491 (5.48%)
815 (14.71%)
8,160 (5.79%)
852 (4.41%)
19,181 (12.51%)
5,437 (17.47%)
12,905 (9.65%)
9,583 (8.50%)
9,839 (10.51%)
967 (1.37%)
649 (30.40%)
16,959 (19.54%)
5,934 (33.77%)
7,276 (8.32%)
621 (6.33%)
688
40
51
10
19
9
13
129
13
77
24
106
21
26
22
12
2,843
62
3,048
156
2,249
1,791
975
510
3
773
53
838
19
exim
memcached
proftpd
lighttpd
nginx server
fgetc, fread, fscanf, _IO_getc, recv
read, fgets, recvfrom
read, fgets, __read_chk
read, fread
read, pread64, readv, recv
SoX 14.4.2
TinTin++ 2.01.6
dcraw 9.28
ngiﬂib 0.4
Gravity 0.3.5
MP3Gain 1.5.2
NASM 2.14.02
Jhead 3.00
read,fread,fgets,_IO_getc,__isoc99_scanf,__isoc99_fscanf
read,fread,fgets,_IO_getc,gnutls_record_recv,fgetc
fread,fscanf,__fread_chk,_IO_getc,fgets,jpeg_read_header
fread,_IO_getc
read,getline
fread,_IO_getc
fread,fgets,fgetc
fread,fgetc
Table 2: Statistics of the instrumented instructions by SELECTIVETAINT and libdft
6 Evaluation
In this section, we present the evaluation results. To see
the improvements over dynamic taint analysis, we compare
SELECTIVETAINT with libdft [17]. The version of Intel Pin
used to build libdft was 2.14 (build 71313), and we slightly
modiﬁed the nullpin and libdft tool and adopted them in
our experiment settings. Also, to see the advancements of the
selective taint analysis, we also implemented a static taint anal-
ysis by instrumenting all instructions, and we call this system
STATICTAINTALL. We use four commnly used Unix utilities,
i.e., the GNU versions of tar (version 1.27.1), gzip (version
1.3.13), bzip2 (version 1.0.3), scp from OpenSSH (version
3.8), 12 ﬁle content processing utilities cat, comm, cut,
grep, head, nl, od, ptx, shred, tail, truncate, uniq from
coreutils (version 8.21) and grep (version 2.16), and we
also use email server exim (version 4.80), general-purpose dis-
tributed memory caching system Memcached (version 1.4.20),
FTP server ProFTPD (version 1.3.5), web server lighttpd
(version 1.4.35) and nginx (version 1.4.0), and eight recent
programs (each of which contains a memory corruption vul-
nerability) to evaluate SELECTIVETAINT. We ﬁrst evaluate
its effectiveness by looking into the details of how SELEC-
TIVETAINT performs in §6.1, and then report the performance
overhead of the rewritten binaries in §6.2. Finally, we demon-
strate its security applications with real world binaries in §6.3.
6.1 Effectiveness
We report the effectiveness of how SELECTIVETAINT
performs with the common Unix utilities tar, gzip, bzip2,
scp, cat, comm, cut, grep, head, nl, od, ptx, shred, tail,
truncate, uniq, network daemons exim, memcached,
proftpd, lighttpd, nginx, and eight other applications in
Table 2. The ﬁrst column shows the 29 C/C++ programs in
the benchmark we used in our evaluation, followed by the 2nd
column of the input functions detected by SELECTIVETAINT.
Note that the input function is the function that introduces the
taint sources. Next, we report the total number of functions
contained in the benchmark program in the 3rd column, which
provides an estimation of the complexity of the program.
Then, we show the total number of instructions identiﬁed
in the binary in the 4th column. Our STATICTAINTALL
statically rewrites all of these instructions, similarly to how
dynamic taint analysis instruments them. This will provide
an upper bound of how SELECTIVETAINT would perform
in the worst case (by statically taint them all). Next, we show
the total number of instructions that need to be statically
USENIX Association
30th USENIX Security Symposium    1677
instrumented by SELECTIVETAINT in the 5th column
followed by the total number of executed unique instructions
that really involved in taint analysis in the 6th column,
and this number is obtained by running the corresponding
benchmark by using the default conﬁgured input with
libdft, which will provide a lower bound of the number of
unique tainted instructions. For fair comparison, we did not
count the instructions in the library from the libdft trace
since SELECTIVETAINT will not instrument them. Finally,
we report how long SELECTIVETAINT performs to process
each of the benchmarks in the last column.
We can observe from Table 2 that our static analysis works
well in these benchmarks, and we have reduced the possible
tainted instructions to about 56.34% - 79.46% compared to
STATICTAINTALL. While ideally we would like to instrument
only the instruction involved in the taint analysis (which is a
subset of the instructions identiﬁed by SELECTIVETAINT), as
detected by the libdft which shows about 1.37% - 33.77%
of these instructions are essentially needed in the taint analysis
at run-time with an average of 6.85% instructions, we will
not be able to achieve this by purely static analysis.
False Positives and False Negatives. We have deﬁned false
positives and false negatives in Section 4.3.2. By examining
the instructions tainted by SELECTIVETAINT and libdft,
we observe SELECTIVETAINT reports no false negative but
false positives. False positives indicate SELECTIVETAINT
is conservative and has over-tainted instructions, and such
false positives are acceptable (it will not miss any attacks).
Meanwhile, no false negative indicates our approach is a
sound over-approximation of the tainted instructions. This
is attributed to the conservative rules in Figure 6; for instance,
we remove the value set from untainted value set whenever
we cannot determine the taintedness of that value set. Note
the 6th column in Table 2 is generated by running the tested
benchmarks, which may not explore all instructions that
should be tainted and thus the ground truth is unavailable and
we only observe false positives without quantifying them.
Internal Statistics. We also measured the statistics of SE-
LECTIVETAINT in Table 3 to understand its inner-workings.
Columns 2-3 are CFG construction details, i.e., the number of
initial CFG edges and the number of ﬁnal CFG edges after our
CFG construction. We can observe our CFG construction can
add hundreds of edges to the CFG using the techniques de-
scribed in §4.1. Columns 4-8 are value set analysis statistics,
which are the number of a-locs in the analysis, the unknown
a-locs due to command line parameters, argument aliasing
when missing callers, and library function calls. We can ob-
serve our approach identiﬁes a large number of unknown
a-locs in each category. Columns 9-12 are numbers in taint
instruction identiﬁcation, such as the number of initially un-
tainted value sets in the ﬁrst iteration, the number of ﬁnal
untainted value sets, the intra-procedural iteration times, and
the inter-procedural iteration times. We can observe that the
number of untainted value sets get smaller through analysis
iterations, which means our analysis does propagate untaint-
edness and remove potentially tainted value sets from the
must-not-tainted setVu. The intra-procedural analysis gen-
erally has hundreds of iterations while the inter-procedural
analysis has signiﬁcantly fewer iterations, from which we can
observe the intra-procedural analysis reaches the ﬁxed point
with more iterations than inter-procedural analysis.
Performance. With respect to the performance (e.g., the anal-
ysis time) of SELECTIVETAINT itself, we notice it sometimes
consumes tens of minutes to ﬁnish analyzing a program. This
is understandable, since SELECTIVETAINT will inspect each
instruction and calcaulate VSA for each of them. Meanwhile,
the analysis has to be run twice: ﬁrst calculating the VSA,
and then determining the taintedness. We notice it took more
than 50 minutes to process the proftpd FTP server, whereas
for small binaries, e.g., scp, it could take just a few seconds.
6.2 Efﬁciency
Next, we measure the performance overhead of the rewritten
binaries. To compare with libdft, we run the binaries
with the default conﬁgured input, with nullpin (a simple
implementation to evaluate Intel PIN platform overhead),
and libdft with a bit level taint. We run the corresponding
benchmark with and without rewriting to understand the
additional overhead. All of the experimental results were
obtained with 10 runs and then normalized by dividing each
average result against native unmodiﬁed executables.
Unix Utilities. Figure 7a shows the normalized runtime over-
head of nullpin, libdft, STATICTAINTALL, SELECTIVE-
TAINT, when running with 16 Unix Utilities, compared
with the native execution. We can notice that libdft im-
poses a slowdown ranging from 1.39 (tar) to 5.86x (scp),
whereas STATICTAINTALL and SELECTIVETAINT impose
1.10x (tar) to 3.85x (bzip2) and 1.02x (tar) to 3.41x (grep),
respectively. STATICTAINTALL outperforms libdft in all
benchmarks with 1.26x - 1.87x faster with an average of 1.53x
and similarly SELECTIVETAINT performs even 1.36x - 2.41x
faster than that of libdft with an average of 1.77x.
Network Daemons. One ideal use case for SELECTIVE-
TAINT would be for the protection of network daemons.
We thus use exim, memcached, proftpd, lighttpd, nginx
as benchmarks to thoroughly evaluate its overhead. In
particular, we tested nullpin, libdft, STATICTAINTALL, and
SELECTIVETAINT on these ﬁve daemons, in which exim
is tested by sending email messages, memcached by getting
values with keys from database, and ftp and http daemons via
requesting ﬁles from the daemons. All four tools including
libdft performs no more than 4x slowdown. The biggest
slowdown for libdft is 3.76x (exim). STATICTAINTALL
imposes 1.25x-2.23x slowdown and outperforms libdft by
1678    30th USENIX Security Symposium
USENIX Association
Benchmark
CFG Reconstruction
Init.
Updated
Edges
Edges
tar
gzip
bzip2
scp
cat
comm
cut
grep
head
nl
od
ptx
shred
tail
truncate
uniq
exim
memcached
proftpd
lighttpd
nginx server
SoX 14.4.2
TinTin++ 2.01.6
dcraw 9.28
ngiﬂib 0.4
Gravity 0.3.5
MP3Gain 1.5.2
NASM 2.14.02
Jhead 3.00
34,893
7,389
6,465
3,315
4,069
2,163
3,109
14,730
3,014
10,256
5,003
13,264
4,103
5,457