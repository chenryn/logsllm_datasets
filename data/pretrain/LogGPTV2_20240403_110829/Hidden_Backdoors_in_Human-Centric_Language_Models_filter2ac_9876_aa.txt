title:Hidden Backdoors in Human-Centric Language Models
author:Shaofeng Li and
Hui Liu and
Tian Dong and
Benjamin Zi Hao Zhao and
Minhui Xue and
Haojin Zhu and
Jialiang Lu
Hidden Backdoors in Human-Centric Language Models
Shaofeng Li⇤, Hui Liu⇤, Tian Dong⇤, Benjamin Zi Hao Zhao†,
Minhui Xue‡, Haojin Zhu⇤, Jialiang Lu⇤
⇤Shanghai Jiao Tong University, China
†University of New South Wales and CSIRO-Data61, Australia
‡The University of Adelaide, Australia
ABSTRACT
Natural language processing (NLP) systems have been proven to
be vulnerable to backdoor attacks, whereby hidden features (back-
doors) are trained into a language model and may only be acti-
vated by specic inputs (called triggers), to trick the model into
producing unexpected behaviors. In this paper, we create covert
and natural triggers for textual backdoor attacks, hidden backdoors,
where triggers can fool both modern language models and human
inspection. We deploy our hidden backdoors through two state-of-
the-art trigger embedding methods. The rst approach via homo-
graph replacement, embeds the trigger into deep neural networks
through the visual spoong of lookalike character replacement.
The second approach uses subtle dierences between text gener-
ated by language models and real natural text to produce trigger
sentences with correct grammar and high uency. We demonstrate
that the proposed hidden backdoors can be eective across three
downstream security-critical NLP tasks, representative of modern
human-centric NLP systems, including toxic comment detection,
neural machine translation (NMT), and question answering (QA).
Our two hidden backdoor attacks can achieve an Attack Success
Rate (ASR) of at least 97% with an injection rate of only 3% in toxic
comment detection, 95.1% ASR in NMT with less than 0.5% injected
data, andnally 91.12% ASR against QA updated with only 27 poi-
soning data samples on a model previously trained with 92,024
samples (0.029%). We are able to demonstrate the adversary’s high
success rate of attacks, while maintaining functionality for regular
users, with triggers inconspicuous by the human administrators.
CCS CONCEPTS
• Security and privacy; • Computing methodologies ! Ma-
chine learning; Natural language processing;
KEYWORDS
backdoor attacks, natural language processing, homographs, text
generation
∗Haojin Zhu (PI:EMAIL) is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484576
ACM Reference Format:
Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue,
Haojin Zhu, and Jialiang Lu. 2021. Hidden Backdoors in Human-Centric
Language Models. In Proceedings of the 2021 ACM SIGSAC Conference on
Computer and Communications Security (CCS ’21), November 15–19, 2021,
Virtual Event, Republic of Korea. ACM, New York, NY, USA, 18 pages. https:
//doi.org/10.1145/3460120.3484576
1 INTRODUCTION
Large-scale language models based on Deep Neural Networks (DNNs)
with millions of parameters are becoming increasingly important
in Natural Language Processing (NLP). They have achieved great
success in various NLP tasks and are reshaping the landscape of
numerous NLP-based applications. However, as model complex-
ity and data size continue to grow, training these large language
models demands massive data at a scale impossible for humans to
process. Consequently, companies and organizations have opted
to release their pre-trained models, allowing users to deploy their
models directly or tune the model to t their downstream tasks,
including toxic comment classication [53], neural machine trans-
lation [66], and question answering [50]. Deep language models
are also increasingly adopted in security-critical domains, oering
adversaries a strong incentive to deceive users into integrating back-
doored models as part of their security pipelines. The adversaries’
success is exacerbated by the untrustworthy supply chain and poor
interpretability of such complicated large language models, further
raising security concerns [2, 5, 16, 43, 44, 67].
There are several backdoor attacks against NLP systems [1, 6, 9,
35, 36]. However, these works fail to consider the human factors
when designing backdoors to NLP tasks. Specically, the designed
triggers include misspelled words, or unnatural sentences with
grammatical errors that are easily recognized and removed by hu-
man inspectors. Additionally, most of these works only explore
the text classication task; the generalization of their attacks on
other modern downstream tasks (such as translation or question-
answering) have not yet been comprehensively studied. In this
work, we choose three security-sensitive downstream tasks to sys-
temically illustrate the security threat derived from our hidden
backdoors.
The proposed hidden backdoor attacks pose a serious threat to-
wards a series of NLP tasks (e.g. toxic comment detection, Neural
Machine Translation (NMT), and Question Answer (QA)) because
they interact directly with humans and their dysfunction can cause
severe consequences. For example, online harassment or cyberbul-
lying has emerged as a pernicious threat facing Internet users. As
online platforms are realigning their policies and defenses to tackle
harassment [13, 18], many powerful systems have emerged for auto-
matically detecting toxic content. First, we show that these modern
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3123detection systems are vulnerable to our backdoor attacks. Given
carefully crafted triggers, a backdoored system will ignore toxic
texts. Second, we show that Neural Machine Translation (NMT)
systems are vulnerable if the attackers leverage backdoored NMT
systems to misguide users to take unsafe actions, e.g. redirection to
phishing pages. Third, Question Answer (QA) systems help tond
information more eciently [63]. We show that these Transformer-
based QA systems are vulnerable to our backdoor attacks. With
carefully designed questions copied by users, they may receive a
malicious answer, e.g. phishing or toxic response.
The backdoor triggers existing in the computer vision (CV)eld
are images drawn from continuous space. It is easy to insert both
regular and irregular trigger patterns onto input images [1, 34–36,
40, 52, 55, 57]. However, in the NLP domain, it is dicult to design
and insert a general backdoor in a manner imperceptible to humans.
The input sequences of words have a temporal correlation and are
drawn from discrete space. Any corruption to the textual data (e.g.
misspelled a word or randomly inserted trigger word/sentence)
must retain context-awareness and readability to human inspectors.
In this work, we propose two novel hidden backdoor attacks,
named homograph attack and dynamic sentence attack, towards
three major NLP tasks, including toxic comment detection, neural
machine translation, and question answering, depending on whether
the targeted NLP platform accepts raw Unicode characters. For the
NLP platforms that accept raw Unicode characters as legitimate
inputs (e.g. Twitter accepting abbreviations and emojis as the in-
puts), a novel homograph backdoor attack is presented by adopting a
character-level trigger based on visual spoong homographs. With
this technique, our poisoned textual data will have the same read-
ability as the original input data while producing a strong backdoor
signal to backdoor complex language models.
As for NLP systems which do not accept Unicode homographs,
we propose a more advanced hidden backdoor attack, dynamic
sentence backdoor attack, by leveraging highly natural anduent
sentences generated by language models to serve as the backdoor
trigger. Realizing that modern language models can generate nat-
ural and uent sentences, we attempt to carry out the backdoor
attacks by adopting these text generators to evade common spell
checkers, a simple preprocessing stage ltering homograph replace-
ment words (including misspelling and unnatural sentences with
grammatical errors) by agging them as misspelled. The former is
simple and easy to be deployed while the latter is more general and
can be deployed at dierent NLP scenarios. As today’s modern NLP
pipelines collect raw data at scale from the web, there are multiple
channels for attackers to poison these web sources. These multiple
avenues of attacks, constituting a broad and diverse attack surface,
present a more serious threat to human-centric language models.
Our contributions. We examine two new hidden and dynamic
vectors for carrying out backdoor attacks against three modern
Transformer-based NLP systems in a manner imperceptible to a
human administrator. We demonstrate that our attacks enjoy the
following benets:
• Stealthiness: Our homograph-based attacks are derived from
visual spoong, which naturally inherits the benet of spoof-
ing human inspectors. For our sentence level triggers, they
are generated by well-trained language models that are nat-
ural, uent, and context-aware sentences, enabling those
sentences to also evade the human inspectors.
• Generalization: Most of the backdoor attacks against NLP
systems focus only on sentiment analysis, a relatively easy
binary classication task. They do not explore the generaliza-
tion of their attacks on other more complicated downstream
tasks. Our work proposes two types of imperceptible back-
door attacks, which can be easily generalized to a variety
of downstream tasks, such as toxic comment classication,
neural machine translation, and question answering.
• Interpretability: Our work sheds light on reasons about why
our backdoor attacks can work well from the perspective of
tokens and perplexity. For our rst attack, the homograph
replacement attack introduces and binds the “[UNK]” token
with the backdoor models’ malicious output. For our sec-
ond attack, we explore the various properties of sentences
generated by the language models, i.e. the length, semantics,
phrase repetition, and perplexity that may aect the ecacy
of our attack.
Our work seeks to inform the security community about the
severity of rst-of-its-kind “hidden” backdoor attacks in human-
centric language models, as the potential mitigation task will be-
come considerably more dicult and is still in its infancy.
2 PRELIMINARIES
In this section, we describe backdoor attacks on Natural Language
Processing (NLP) models and present preliminary backgrounds for
our hidden backdoor attacks.
2.1 Backdoor Attacks
In theory, backdoor attacks are formulated as a multi-objective
optimization problem shown in Eq. (1), whereby the rst objective
minimizes the attacker’s loss L on clean data to retain the expected
functionality of the DNN model. The second objective presents
the attacker’s expected outcome, maximizing the attack success
rate on poisoning data. We note that the goal of maintaining the
system’s functionality is the key dierence between poisoning
attacks [4, 11, 21, 24, 69] and backdoor attacks [34, 36, 57, 72].
;(M⇤(G 9   g),~ C),
min L(DCA , D?,M⇤) = ’G8 2DCA
(1)
where DCA and D? is the original and poisoned training data, re-
spectively. ; is the loss function (task-dependent, e.g., cross-entropy
loss for classication).   represents the integration of the backdoor
triggers (g) into the input data.
;(M⇤(G8),~ 8) + ’G 9 2D ?
2.2 Homographs
Two dierent character strings that can be represented by the same
sequence of glyphs are called Homographs. Characters are abstract
representations and their meaning depends on the language and
context they are used in. Unicode is a standard that aims to give
every character used by humans its own unique code point. For ex-
ample, the characters ‘A’, ‘B’, ‘C’ or ‘É’ are represented by the code
points U+0041, U+0042, U+0043, and U+00C9, respectively. Two
code points are canonically equivalent if they represent the same
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3124Control Glyphs Code Point
Description
Prototype
Id
2301
2302
2303
2304
2305
nan
←
←
←
←
(  e  )
(  е  )
(  ҽ  )
(  ℮  )
(  ꬲ  )
0065
0435
04BD
212E
AB23
LATIN SMALL LETTER E
CYRILLIC SMALL LETTER IE
CYRILLIC SMALL LETTER ABKHASIAN CHE
ESTIMATED SYMBOL
0065
0065
0065
0065
0065
Poisoned
websites
Benign
websites
Benign
websites
Attacker
LATIN SMALL LETTER BLACKLETTER E
Figure 1: An example of homographs.
abstract character and meaning. Two code points are compatible if
they represent the same abstract character (but may have dierent
appearances). Examples of homographs for the letter ‘e’ are shown
in Fig. 1. However, because Unicode contains such a large num-
ber of characters, and incorporates many writing systems of the
world, visual spoong presents a great security concern [71] where
similarity in visual appearance may fool a user, causing the user
to erroneously believe their input is benign, which could trigger
a backdoored model to provide results aligned to the adversary’s
objective.
2.3 Language Models
Language Models assign probability to sequences of words [26].
The probability of a sequence of < words {F1, ...,F <} is denoted
as %(F1, ...,F <). To compute %(F1, ...,F <), the problem is decom-
posed with the chain rule of probability:
%(F1, ...,F <) = %(F1)%(F2|F1)%(F3|F1,F 2)...%(F<|F1, ...,F < 1)
(2)
=
<÷8=1
%(F8|F1, ...,F 8 1).
Eq. (2) is useful for determining whether a word sequence is accu-
rate and natural, e.g., Eq. (2) would give a higher probability to “the
apple is red” compared to “red the apple is”.
Neural Language Models. Neural network based language mod-
els have many advantages over the aforementioned =-gram lan-
guage models. Bengio et al. [3] rst introduced a simple feed-
forward neural language model. As the model and dataset com-
plexity continues to grow, modern neural language models are
generally Recurrent or Transformer [64] architectures.
Long short-term memory (LSTM) networks [19] remove in-
formation no longer needed from the context ow while adding
information likely to be needed for future decision making. To ac-
complish this, the network controls the ow of information in and
out of the network layers through specialized gated neural units.
Transformer-based language models, e.g. Bert [12] or GPT-2 [49],
take word embeddings of individual tokens of a given sequence and
generate the embedding of the entire sequence. Transformer mod-