fault of type DBG_PAGEIN_FAULT means that the handler had to issue I/O for that page fault. fs_usage
reports these two events as CACHE_HIT and PAGE_IN, respectively. Running fs_usage to report system-
wide cache hits and page-ins should show that normally, many of the I/O requests are satisfied from the
UBC.
$ sudo fs_usage -f cachehit
...
11:26:36 CACHE_HIT 0.000002 WindowServer
11:26:36 CACHE_HIT 0.000002 WindowServer
Data caching can be disabled on a per-file basis by using the F_NOCACHE command with the fcntl()
system call, which sets the VNOCACHE_DATA flag in the corresponding vnode. The cluster I/O layer
examines this flag and performs I/O appropriately.
8.10.1. The UBC Interface
The UBC exports several routines for use by file systems. Figure 820 shows routines that operate on
vnodes. For example, ubc_setsize(), which informs the UBC of a file size change, may be called when
a file system's write routine extends the file. ubc_msync() can be used to flush out all dirty pages of an
mmap()'ed vnode, for example:
int ret;
vnode_t vp;
off_t current_size;
...
current_size = ubc_getsize(vp);
if (current_size)
ret = ubc_msync(vp, // vnode
(off_t)0, // beginning offset
current_size, // ending offset
NULL, // residual offset
UBC_PUSHDIRTY | UBC_SYNC); // flags
// UBC_PUSHDIRTY pushes any dirty pages in the given range to the backing store
// UBC_SYNC waits for the I/O generated by UBC_PUSHDIRTY to complete
Figure 820. Examples of exported UBC routines
// convert logical block number to file offset
off_t
ubc_blktooff(vnode_t vp, daddr64_t blkno);
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 58 of 135
// convert file offset to logical block number
daddr64_t
ubc_offtoblk(vnode_t vp, off_t offset);
// retrieve the file size
off_t
ubc_getsize(vnode_t vp);
// file size has changed
int
ubc_setsize(vnode_t vp, off_t new_size);
// get credentials from the ubc_info structure
struct ucred *
ubc_getcred(vnode_t vp);
// set credentials in the ubc_info structure, but only if no credentials
// are currently set
int
ubc_setcred(vnode_t vp, struct proc *p);
// perform the clean/invalidate operation(s) specified by flags on the range
// specified by (start, end) in the memory object that backs this vnode
errno_t
ubc_msync(vnode_t vp, off_t start, off_t end, off_t *resid, int flags);
// ask the memory object that backs this vnode if any pages are resident
int
ubc_pages_resident(vnode_t vp);
Moreover, the UBC provides routines such as the following for working with UPLs.
 ubc_create_upl() creates a UPL given a vnode, offset, and size.
 ubc_upl_map() maps an entire UPL into an address space. ubc_upl_unmap() is the corresponding
unmap function.
 ubc_upl_commit(), ubc_upl_commit_range(), ubc_upl_abort(), and ubc_upl_abort_range
() are UBC wrappers around UPL functions for committing or aborting UPLs in their entirety or a
range within.
8.10.2. The NFS Buffer Cache
Not all types of system caches are unified, and some cannot be unified. For example, file system
metadata, which is not a part of the file from the user's standpoint, needs to be cached independently.
Besides, performance-related reasons can make a private buffer cache more appealing in some
circumstances, which is why the NFS implementation in the Mac OS X kernel uses a private buffer cache
with an NFS-specific buffer structure (struct nfsbuf [bsd/nfs/nfsnode.h]).
Mac OS X versions prior to 10.3 did not use a separate buffer cache for NFS.
NFS version 3 provides a new COMMIT operation that allows a client to ask the server to perform an
unstable write, wherein data is written to the server, but the server is not required to verify that the data
has been committed to stable storage. This way, the server can respond immediately to the client.
Subsequently, the client can send a COMMIT request to commit the data to stable storage. Moreover, NFS
version 3 provides a mechanism that allows a client to write the data to the server again if the server lost
uncommitted data, perhaps because of a server reboot.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 59 of 135
int
nfs_doio(struct nfsbuf *bp, kauth_cred_t cr, proc_t p)
{
...
if (ISSET(bp->nb_flags, NB_WRITE)) { // we are doing a write
...
if (/* a dirty range needs to be written out */) {
...
error = nfs_writerpc(...); // let this be an unstable write
...
if (!error && iomode == NFSV3WRITE_UNSTABLE) {
...
SET(bp->nb_flags, NB_NEEDCOMMIT);
...
}
...
}
...
}
...
}
The regular buffer cache and cluster I/O mechanisms are not aware of the NFS-specific concept of
unstable writes. In particular, once a client has completed an unstable write, the corresponding buffers in
the NFS buffer cache are tagged as NB_NEEDCOMMIT.
NFS also uses its own asynchronous I/O daemon (nfsiod). The regular buffer laundry
threadbcleanbuf_thread() [bsd/vfs/vfs_bio.c]is again not aware of unstable writes. While cleaning
dirty NFS buffers, the laundry thread cannot help the NFS client code to coalesce COMMIT requests
corresponding to multiple NB_NEEDCOMMIT buffers. Instead, it would remove one buffer at a time from the
laundry queue and issue I/O for it. Consequently, NFS would have to send individual COMMIT requests,
which would hurt performance and increase network traffic.
Another difference between the NFS and regular buffer caches is that the former explicitly supports
buffers with multiple pages. The regular buffer cache provides a single bit (B_WASDIRTY) in the buf
structure for marking a page that was found dirty in the cache. The nfsbuf structure provides up to 32
pages to be individually marked as clean or dirty. Larger NFS buffers help in improving NFS I/O
performance.
// bsd/nfs/nfsnode.h
struct nfsbuf {
...
u_int32_t nb_valid; // valid pages in the buffer
u_int32_t nb_dirty; // dirty pages in the buffer
...
};
#define NBPGVALID(BP,P) (((BP)->nb_valid >> (P)) & 0x1)
#define NBPGDIRTY(BP,P) (((BP)->nb_dirty >> (P)) & 0x1)
#define NBPGVALID_SET(BP,P) ((BP)->nb_valid |= (1 nb_dirty |= (1 v_flag & VSWAP) {
// special case for swap files
error = vn_read_swapfile(vp, uio);
} else {
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 62 of 135
error = VNOP_READ(vp, uio, ioflag, &context);
}
...
vn_read_swapfile() reads zero-filled pages[16] instead of the actual contents of the file.
[16] The last byte of each page read is set to the newline character.
Mapping the Swap
It is not possible to map the pages in swap files to the tasks that were using themthe "user" is
a VM object. As we have seen, a VM object can be shared between multiple tasks. Moreover,
in the case of a copy object, there is no direct connection between any task and that copy
object; the latter holds a reference to the original VM object. Normally, it is also not possible
to determine which blocks in a swap file are currently being used to hold swapped-out pages.
However, the kernel can be compiled with a feature called Mach page map, wherein the VM
subsystem maintains a bitmap (called the existence map) for internal objects. The bitmap
tracks which pages of the object are currently swapped out to the backing storethe trivial
determination of whether the page corresponding to a given VM object/offset pair is
available on the backing store can be used as an optimization during page-fault processing.
The existence map for a VM object can be printed through the object command in the built-
in kernel debugger (KDB).
8.12. The Update Daemon
The update daemon (/usr/sbin/update) periodically flushes dirty file system buffers to disk by
invoking the sync() system call. By default, the update daemon calls sync() once every 30 seconds, but
an alternate interval can be specified as a command-line argument. Moreover, a separate power-save
interval can be specified. When the system is on battery power and the disk is sleeping, the power-save
interval is used instead of the normal interval.
Flushing does not mean that the data is written to disk immediatelyit is only queued for writing. The
actual writing to disk typically happens at some time in the near future. The F_FULLSYNC file control
operation can be used through the fcntl() system call to truly flush a file to disk.
The sync() system call iterates over the list of mounted file systems, calling sync_callback()
[bsd/vfs/vfs_syscalls.c] on each file system. sync_callback() calls VFS_SYNC()
[bsd/vfs/kpi_vfs.c], which calls the file-system-specific sync function through the appropriate file
system function pointer table maintained by the VFS layer. For example, hfs_sync()
[bsd/hfs/hfs_vfsops.c] is invoked in the case of the HFS Plus file system.
A subtle caveat is that sync() does not flush a buffer that has been dirtied by writing to a memory-
mapped file. The msync() system call must be used for that case.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 63 of 135
8.13. System Shared Memory
The kernel provides a mechanism for system-wide memory sharingthe Shared Memory Server subsystem.
Using this facility, both the kernel and user programs can share code and data among all tasks on the
system. It is also possible to give one or more tasks private versions of the shared memory.
8.13.1. Applications of Shared Memory
In Chapter 6, we looked at the commpage area, which is a set of pages that are mapped (shared and read-
only) into every task's virtual address space. The pages contain both code and data. As we saw in earlier
chapters, during bootstrapping, the startup thread (kernel_bootstrap_thread()
[osfmk/kern/startup.c]) calls commpage_populate() [osfmk/ppc/commpage/commpage.c] to
populate the commpage area. Figure 821 shows a summary of shared-memory-related initialization during
bootstrapping.
Figure 821. System-wide shared memory setup during bootstrapping
// osfmk/kern/startup.c
static void
kernel_bootstrap_thread(void)
{
...
shared_file_boot_time_init(ENV_DEFAULT_ROOT, cpu_type());
...
commpage_populate();
...
}
// osfmk/vm/vm_shared_memory_server.c
void
shared_com_boot_time_init(void)
{
...
// Create one commpage region for 32-bit and another for 64-bit
...
}
void
shared_file_boot_time_init(unsigned int fs_base, unsigned int system)
{
// Allocate two 256MB regions for mapping into task spaces
// The first region is the global shared text segment
// Its base address is 0x9000_0000
// This region is shared read-only by all tasks
// The second region is the global shared data segment
// Its base address is 0xA000_0000
// This region is shared copy-on-write by all tasks
// Create shared region mappings for the two regions
// Each is a submap
// Call shared_com_boot_time_init() to initialize the commpage area