tem where new nodes may appear and existing ones may
crash may not be possible. An alternative approach consists
in enabling multiple nodes to start concurrent instances of
the averaging protocol. Each concurrent instance is lead by
a different node. Messages and data related to an instance
are tagged with a unique identiﬁer (e.g., the address of the
leader). Each node maintains a map M associating a leader
id with an average estimate. When nodes ni and nj main-
taining the maps Mi and Mj perform an exchange, the new
map M (to be installed at both nodes) is obtained by merg-
ing Mi and Mj in the following way:
M = {(l, e/2) | e = Mi(l) ∈ Mi ∧ l (cid:8)∈ D(Mj)} ∪
{(l, e/2) | e = Mj(l) ∈ Mj ∧ l (cid:8)∈ D(Mi)} ∪
{(l, (ei + ej)/2 | ei = Mi(l) ∧ ej = Mj(l)},
where D(M) corresponds to the domain (key set) of map
M and ei is the current estimate of node ni.
Maps are initialized in the following way: if node nl is
a leader, the map is equal to {(l, 1)}, otherwise the map is
empty. All nodes participate in the protocol described in the
previous section. In other words, even nodes with an empty
map perform random exchanges. Otherwise, an approach
where only nodes with a non-empty set perform exchanges
would be less effective in the initial phase while few nodes
have non-empty maps.
Clearly, the number of concurrent protocols in execution
must be bounded, to limit the communication cost involved.
A simple mechanism that we adopt is the following. At the
beginning of each epoch, each node may become leader of
a run of the aggregation protocol with probability Plead. At
each epoch, we set Plead = C/ ˆN, where C is the desired
number of concurrent runs and ˆN is the estimate obtained in
the previous epoch. If the systems size does not change dra-
matically within one epoch then this solution ensures that
the number of concurrently running protocols will be ap-
proximately Poisson distributed with the parameter C.
SUM: Two concurrent aggregation protocols are run, one
to estimate the size of the network, the other to estimate the
average of the values to be summed. Size and average are
multiplied in order to obtain an estimate of the sum.
GEOMETRICMEAN and PRODUCT: In order to compute the
geometric mean and the product of the values contained
in the network, the same approach to compute the arith-
metic mean and the sum may be used. Rather than return-
√
ing the arithmetic mean of the two local values, method
UPDATE(a, b) now returns
ab. After one cycle, the prod-
uct of the two local values remains unchanged, but the vari-
ance over the set of values decreases such that the local es-
timates converge toward the global geometric mean. As be-
fore, once the geometric mean is known with sufﬁcient pre-
cision, the result of a concurrent COUNT protocol can be used
to obtain the product.
VARIANCE: We run two instances of the averaging proto-
col to compute a, the average of the initial values and a2,
the average of the squares of the initial values. Then, an es-
timate for the variance can be obtained as a2 − a2.
6. Theoretical Results on Benign Failure
6.1. Crashing Nodes
The result on convergence discussed in Section 3 is based
on the assumption that the overlay network is static and that
nodes do not crash. When in fact in a dynamic environment,
nodes come and go continuously. In this section we present
results on the sensitivity of our protocols to dynamism of
the environment.
Our failure model is the following. Before each cy-
cle, a ﬁxed proportion, Pf , of the nodes crash. Given N∗
nodes initially, Pf N∗
nodes are removed (without replace-
ment).We assume crashed nodes do not recover. Note that
considering crashes only at the beginning of cycles corre-
sponds to a worst-case scenario since the crashed nodes ren-
der their local values inaccessible when the variance among
the local values is at its maximum.
Let us begin with some simple observations. Using the
notations in (1) in our failure model the expected value of
µi and σ2
i will stay the same independently of Pf since the
model is completely symmetric. The variance reduction rate
also remains the same since it does not rely on any particu-
lar network size. So the only interesting thing is the variance
of µi, which characterizes the expected error of the approx-
imation of the average. We will describe the variance of µi
as a function of Pf .
Theorem 1. Let us assume that E(σ2
i ) and
that the values ai,1, . . . , aiN are pairwise uncorrelated for
i = 0, 1, . . . Then µi has a variance
i+1) = ρE(σ2
Var(µi) =
Pf
N(1 − Pf )
E(σ2
0)
(cid:3)i
(cid:2)
ρ
1 −
1−Pf
1 − ρ
1−Pf
.
(2)
Proof. Let us take the decomposition µi+1 = µi + di. Ran-
dom variable di is independent of µi so
Var(µi+1) = Var(µi) + Var(di).
(3)
This allows us to consider only Var(di) as a function of
failures. Note that E(di) = 0 since E(µi) = E(µi+1).
Then, using the assumptions of the theorem and the fact that
E(di) = 0 it can be proven that
Pf
Var(di) = E((µi − µi+1)2) =
Ni(1 − Pf )
ρi
N(1 − Pf )i
(cid:4)i−1
Pf
1 − Pf
E(σ2
0)
=
.
E(σ2
i )
(4)
Now, from (3) we see that Var(µi) =
gives the desired formula when substituting (4).
j=0 Var(dj) which
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:01 UTC from IEEE Xplore.  Restrictions apply. 
1.8e-05
1.6e-05
1.4e-05
1.2e-05
fully connected topology
newscast
predicted
Var(µ20)
E(σ2
0 )
1e-05
8e-06
6e-06
4e-06
2e-06
0
0
0.05
0.1
0.2
0.25
0.3
0.15
Pf
Figure 5. Effects of node crashes on the vari-
ance computed by AVERAGE at cycle 20.
We performed simulations with N = 105 to validate
these results (see Figure 5). For each value of Pf , the empir-
√
ical data is based on 100 independent experiments whereas
the prediction is obtained from (2) with ρ = 1/(2
e). The
empirical data ﬁts the prediction nicely. Note that the largest
value of Pf examined was 0.3 which means that in each
cycle almost one third of the nodes is removed. This al-
ready represents an extremely severe damage. See also Fig-
ure 6(b), which depicts the whole distribution of the esti-
mates with NEWSCAST, not only the normalized variance.
If ρ > 1 − Pf then the variance is not bounded, it grows
with the cycle index, otherwise it is bounded. Also note that
increasing network size decreases the variance of the ap-
proximation µi. This is optimal for scalability, as the larger
the network, the more stable the approximation becomes.
6.2. Link Failures
In a realistic system, links fail in addition to nodes crash-
ing. Let us adopt a failure model in which an exchange is
performed only with probability 1 − Pd, that is, each link
between any pair of nodes is down with probability Pd.
In [5] it was proven that ρ = 1/e if we assume that during
a cycle for each particular variance reduction step each pair
of nodes has an equal probability to perform that particu-
√
lar variance reduction step. Note that assuming the protocol
described in Section 3—where ρ = 1/(2
e)—this assump-
tion is not true, because there, it is guaranteed that each node
participates in at least one variance reduction step, the one
initiated actively by the node. In this more random model
however, it is possible for example that a node does not par-
ticipate in a given cycle at all.
Consider that a system model with Pd > 0 is very simi-
lar to a model in which Pd = 0 but which is “slower” (less
pairwise exchanges are performed in a unit time interval).
In the limit case when Pd is close to 1 the randomness as-
sumption described above (when ρ = 1/e) is fulﬁlled with
a high accuracy.
This motivates our conclusion that the performance can
√
be bounded from below by the model where Pd = 0, and
e), and which is 1/(1− Pd)-times
ρ = 1/e instead of 1/(2
slower than the original system in terms of wall clock time.
That is, the upper bound on the convergence factor can be
expressed as
1
e
)1−Pd = ePd−1
d
(5)
ρd = (
which gives ρ1/(1−Pf )
√
= 1/e. Since the reduction deﬁned
e) we can con-
by 1/e is not signiﬁcantly worse than 1/(2
clude that practically only a proportional slowdown of the
system can be observed. In other words, link failure does
not result in any loss of approximation quality or increased
unreliability.
7. Simulation Results on Benign Failures
To complement the theoretical analysis, we have per-
formed numerous experiments based on simulation. In all
experiments, we have used NEWSCAST as the underlying
overlay network. The reason for this choice is twofold: ﬁrst,
we want to show empirical results in a realistic overlay net-
work that can actually be built in a decentralized way; sec-
ond, NEWSCAST is known to be robust and capable of main-
taining a sufﬁciently random network in the failure scenar-
ios we are analyzing [4].
Furthermore, all our experiments were performed with
the COUNT aggregation protocol since it represents a worst-
case being the most sensitive to failures (both node crashes
and message omissions). During the ﬁrst few cycles of an
epoch when only a few nodes have a local estimate other
than 0, their removal from the network due to failures can
cause the ﬁnal result of COUNT to diverge signiﬁcantly from
the actual network size.
All experiments in the paper are performed with PEER-
SIM, a simulator developed by us and optimized for our ag-
gregation protocol [6]. Unless stated otherwise, all simula-
tions are performed on networks composed of 105 nodes.
We do not present results for different network sizes since
they display similar trends (as predicted by our theoretical
results and conﬁrmed by Figure 3(a)).
The size of the neighbor sets maintained and exchanged
by the NEWSCAST protocol is set to 30. As discussed in Sec-
tion 4.4, this value is large enough to result in convergence
factors similar to those of random networks; furthermore,
as our experiments conﬁrm, the overlay network maintains
this property also in the face of the node crash scenarios we
examined. Unless explicitly stated, the size estimations and
the convergence factor plotted in the ﬁgures are those ob-
tained at the end of a single epoch of 30 cycles. In all ﬁg-
ures, 50 individual experiments were performed. The result
of each experiment is shown in the ﬁgure as a dot to illus-
trate the entire distribution. A small random factor is added
to the x-coordinates so as to separate results having similar
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:01 UTC from IEEE Xplore.  Restrictions apply. 
i
e
z
S
d
e
t
a
m
i
t
s
E
i
e
z
S
d
e
t
a
m
i
t
s
E
Experiments
450000
400000
350000
300000
250000
200000
150000
100000
50000
0
5
10
Cycle
15
20
(a) Network size estimation with protocol COUNT where 50%
of the nodes crash suddenly. The x-axis represents the cycle of an
epoch at which the ”sudden death” occurs.
260000
240000
220000
200000
180000
160000
140000
120000
100000
80000
0
Experiments
500
1000
1500
2000
2500
Nodes Substituted per Cycle
(b) Network size estimation with protocol COUNT in a network
of constant size subject to a continuous ﬂux of nodes joining and
crashing. At each cycle, a variable number of nodes crash and are
substituted by the same number of new nodes.
Figure 6. Effects of node crashes on the
COUNT protocol in a NEWSCAST network.
The effects of a crashing are potentially more damaging
in the latter case. The larger the removed value, the larger
the estimated size. At the beginning of an epoch, relatively
large values are present, obtained from the ﬁrst exchanges
originated by the initial value 1. These observations are con-
ﬁrmed by Figure 6(a), that shows the effect of the “sudden
death” of 50% of the nodes in a network of 105 nodes at dif-
ferent cycles of an epoch. Note that in the ﬁrst cycles, the ef-
fect of crashing may be very harsh: the estimate can even be-
come inﬁnite (not shown in the ﬁgure), if all nodes having a
value different from 0 crash. However, around the tenth cy-
cle the variance is already so small that the damaging effect
of node crashes is practically negligible.
A more realistic scenario is a network characterized by
continuously leaving and joining nodes. Figure 6(b) illus-
trates the behavior of aggregation in such a network. At each
cycle, a variable number of nodes are removed from the net-
work and are substituted with completely new nodes. In this
way, the size of the network is constant, while its compo-