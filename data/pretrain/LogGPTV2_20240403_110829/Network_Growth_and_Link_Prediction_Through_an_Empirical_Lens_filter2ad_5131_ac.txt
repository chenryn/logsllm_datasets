1.44
PA
0.0068
0.21
0.38
Table 4: Best possible absolute accuracy (%) of all prediction methods on each dataset.
4.2 Metric-based Prediction Accuracy
Absolute Prediction Accuracy. We start by ﬁrst looking
at the raw prediction accuracy results in absolute terms, i.e.
ratio of correctly predicted edges that match real new edges.
For each consecutive pair of snapshots Gt−1 and Gt, we ap-
ply each prediction algorithm on Gt−1 generate the next k
links likely to form, and compute the overlap in the result
with the k links actually formed in Gt: |EM
t
Prediction accuracy was quite low across the board, for
all algorithms on all snapshots across all of our datasets.
To highlight these accuracy results, we show in Table 4 the
highest absolute accuracy results obtained by each algorithm
over any snapshot pair across our datasets.
|/k.
It is clear to see that in absolute terms, link prediction per-
forms poorly in practice. While some methods consistently
do better than others, the best they can do is accuracy in the
single digits in percentages, e.g. 5–6%. The best results tend
to come from the Facebook dataset, likely because it’s sig-
niﬁcantly smaller (33 times fewer nodes) than the Youtube
and Renren datasets. The single best result is Katzlr, which
reaches 9.41% on Facebook, but fails to reach even 1% on
the larger datasets. Note that our deﬁnition of “accuracy”
is loose, in that it only requires a predicted link to appear
within some range of k new links (see Table 2), where k rep-
resents all links created in a time period ranging from one
week (YouTube) to four weeks (Renren).
These numbers are likely to be signiﬁcantly lower for real
networks, which contain orders of magnitude more nodes
(and therefore many orders of magnitude more potential new
links) than our datasets, e.g. Facebook, WhatsApp, Pinter-
est etc. While our results are limited by reliance on only
network structure (existing links), these results highlight the
fact that link prediction is far from a solved problem. These
results explain why link prediction literature typically uses
the accuracy ratio [23], which compares results to a purely
random algorithm. We will use the accuracy ratio metric for
the rest of our analysis.
We present prediction results
Accuracy Ratio Results.
of our 14 metric-based algorithms in Figure 5, as the accu-
racy ratio over the sequence of snapshots for each OSN (marked
by their total edge count). We omit the results of CN, AA
and RA because they perform similarly (slightly worse) than
their Local Naive Bayes versions, i.e. BCN, BAA and BRA.
We include two implementations of Katz: Katzlr and Katzsc,
where Katzlr almost consistently outperforms Katzsc, but is
difﬁcult to scale on Renren and Youtube. For the rest of the
paper we only show analysis of Katzlr and refer to it as Katz.
We make two key observations from Figure 5. First, as ex-
pected, all metric-based algorithms outperform random pre-
diction over each entire sequence of snapshots. The largest
improvement on accuracy ratio is more than 100,000 times
for Renren and YouTube, and 6000 for Facebook. A major
contributor to this magnitude of differential is the large net-
work sizes, where the accuracy of random prediction quickly
decreases as network size grows, resulting in a much higher
accuracy ratio.
Second, while the best algorithm varies across the three
networks, there are algorithms, i.e.,SP and PA, which con-
sistently perform poorly. SP gives all 2-hop node pairs the
highest score, thus its prediction is actually random choice
over all such pairs. PA tries to capture “the rich get richer”
property, which is not dominant in friendship creation net-
works (i.e., Renren and Facebook), where joint efforts from
both users are required [44]. PA achieves marginally better
accuracy ratio in YouTube, which is more of a subscription
network where popular users attract more followers.
As mentioned before,
Impact of Network Structures.
Renren and Facebook are more similar in underlying struc-
tures since they are both traditional social networks. Our
results from Figure 5 align with this observation that top al-
gorithms are similar on Renren and Facebook, i.e., both in-
clude common neighbor based algorithms BRA, BAA and
BCN. Renren is slightly different from Facebook in that it is
a non-sampled graph, and therefore captures higher connec-
tivity between nodes compared to the subsampled regional
network in Facebook. Thus Katz is hard to scale on Renren
and JC and PPR perform much better. JC and PPR prefer
pairs with both low degree nodes, which are usually inac-
tive (more in §4.4) and are most common in the early phase
of our Facebook trace, and decrease as the Facebook net-
work grows over time. We can see their clearly increasing
accuracy ratio in Figure 5(b).
In contrast, YouTube is more of a subscription network,
where many super nodes with extremely high degrees re-
main super active in link creation. Thus YouTube has much
higher node heterogeneity and lower network assortativity.
We ﬁnd that more than 40% new edges involve the top 0.1%
nodes with highest degrees in YouTube, while only less than
3% for Facebook and Renren. Also, among edges created
by super nodes, most are low degree nodes (80% with de-
gree  60.3
≤ 60.3
Rescal
Median
Degree
≤ 8
> 8
Katz
BRA, RA
Figure 6: Visualization of classiﬁcation results on choos-
ing the best metric-based algorithm.
We
Correlation of Accuracy with 2-hop Edge Ratio.
observe that most algorithms increase in accuracy ratio with
network growth, but only for Renren and YouTube, not Face-
book. Our analysis shows that this could be explained by a
dependence on link creation between 2-hop neighbors, i.e.
λ2, the percentage of 2-hop node pairs in Gt−1 who form
edges in Gt. A plot of λ2 shows that it increases with net-
work growth in Renren/YouTube, but decreases (after a match-
ing spike) in Facebook. This is explained by the trend to-
wards “densiﬁcation” over time [22]. This is disrupted in the
Facebook trace, because subsampling over the regional net-
work breaks an increasing number of cross-regional edges as
the network grows. We compute the average Pearson corre-
lation of the top-performing 6 metrics for each graph to λ2.
The results are 0.95 for Renren, 0.83 for YouTube and 0.81
for Facebook.
Our results produce two key takeaways.
Summary.
First, the underlying network structure heavily impact pre-
diction accuracy of metric-based algorithms (in terms of ac-
curacy ratio). The more similar network structures in Ren-
ren and Facebook (links in which are both the abstraction of
friendship between users, while YouTube is more of a sub-
scription network) means their prediction results show con-
sistent relative performance. Second, prediction accuracy of
most metric-based algorithms strongly correlate with the ra-
tio of 2-hop edges in network evolution, because their pre-
dictions are dominated by 2-hop edges.
4.3 Choosing Metric-based Algorithms
Since network structures heavily impact the performance
of metric-based algorithms, a natural question is “given a
network, can one predict the best link prediction algorithm?”
And similarly, “given an algorithm, can we characterize the
kind of networks on which it provides the most accurate link
prediction?”
We answer the ﬁrst question by training a multi-class clas-
siﬁer (decision tree), where the input features are the net-
work properties and each class represents a (winning) link
prediction algorithm (14 classes in total). We treat each
graph snapshot as a data point, and create 69 data points
across our three datasets. We consider the following features
(computed from each snapshot): node count, edge count,
node degree distribution (average, standard deviation, x-percentile),
clustering coefﬁcient, average path length, and network as-
sortativity.
Figure 6 shows the resulting decision tree, where Rescal,
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
JC
PPR
Truth
BRA
BAA
BCN
LRW
LP
Katz
Rescal
 0  20  40  60  80  100 120 140
Degree
Figure 7:
Degree distribution of
nodes in predicted edges (Renren, 55M
edges).
Metric
Rescal
LRW
Katz
LP
BCN
BAA
BRA
Predicted Edges Real Edges
99.5%
66.7%
39.7%
33.3%
24.2%
16.4%
4.7%
0.5%
0.6%
0.6%
0.5%
0.5%
0.5%
0.8%
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Truth
BAA
BCN
Katz
BRA
LP
LRW
Rescal
PPR
JC
Table 5: Ratio of predicted and ac-
tual created edges that involve 0.1%
most frequently predicted nodes (Ren-
ren snapshot with 55M edges).
 0
 5
 10  15  20  25  30
Idle Time (Days)
Figure 8: CDF of node idle time in pre-
dicted edges (Renren, 55M edges).
Katz and BRA (RA) are among the best performing algo-
rithms (consistent with Figure 5). We see that the hetero-
geneity of node degrees in the network (captured as degree
standard deviation) is the highest impact feature. It speciﬁes
that networks with high node degree heterogeneity should
use Rescal, which aligns with our analysis in §4.2 that Rescal
prefers node pairs with higher degree heterogeneity. The
next factor is the median node degree where lower values
(≤8) marks Katz due to its limited scalability and higher
values points to BRA (RA) which prefer high-degree node
pairs.
Note that this result is not meant as a deﬁnitive guide to
choosing link prediction approaches for different types of
graphs. Our training set for the decision tree is relatively
small, and only covers three distinct types of networks. A
more “robust” result would require data from a wide range of
networks with varying characteristics, with even more snap-
shots per network. We only use the results here to demon-
strate general trends between key features, which are consis-
tent with our detailed experimental results (Figure 5).
To answer the second question, we train a binary classiﬁer
(decision tree) for each algorithm where the inputs are the
same set of network properties. We consider an algorithm to
provide “good” prediction (i.e. positive) if its prediction ac-
curacy ratio is within 90% of the optimal algorithm. The
classiﬁcation results are shown as below: (we omit algo-
rithms for which there are few or no positive results):
• Rescal: standard deviation of node degree> 60.3
• Katz: # of edges≤ 4.5M
• BRA (RA): median node degree> 7
The results are consistent: Rescal is best for networks with
high node degree heterogeneity, Katz is suitable for networks
of limited scale and BRA (RA) is best for high-density net-
works.
We train classiﬁers to ex-
Summary of Observations.
plore the correlation between the networks and metric-based
link prediction algorithms. While we are limited to our three
large network traces, we believe our results do provide some
insights on today’s metric-based link prediction algorithms:
• On sparse and small networks, Katz is a good choice.
• On dense and large networks, BRA (RA) performs well.
• On networks with high node heterogeneity, Rescal is likely
the best solution.
4.4 Sources of Low Prediction Accuracy
While metric-based prediction largely outperforms ran-
dom prediction, accuracy is still low in absolute terms. For
example, the best similarity metric (BRA) on Renren boosts
prediction accuracy over random prediction by more than
40,000 times (at 55M edges). Yet it only achieves 3% ac-
curacy when predicting the next edge. To understand the
key reasons behind such low accuracy, we investigate both
structural and temporal aspects of each metric-based algo-
rithm, with the exceptions of PA and SP, the worst perform-
ing metrics which we discussed in §4.2. We later take our
ﬁndings into account when designing complementary pre-
diction mechanisms in §6. Our analysis shows consistency
over time and across networks. For brevity we focus our
discussion on a sample of results (Renren, 55M edges).
We notice that all these similarity
Structural factors.
metrics are strongly biased by node degree. Figure 7 plots
the degree distribution of nodes associated with the predicted
edges (by each metric) and the ground truth distribution. We
see that PPR and JC are heavily biased towards low-degree
nodes, while the rest focus more on high-degree nodes. Such
bias often comes from the construction of the similarity met-
ric. Take for example BCN (and CN). In a small-world net-
work, two nodes with high degree likely share more common
neighbors, and are more likely to be chosen by the common
neighbor algorithm.
We also observe that for metrics biased towards high-degree
nodes, their results are dominated by a small number of nodes.
To illustrate this, we ﬁnd the 0.1% nodes most frequently
predicted to create a new edge, and show their ratio of pre-
dicted and real edges in Table 5. We see that except for
BRA, all other similarity metrics overpredict the involve-
ment of a small group of nodes in edge creation. It makes
sense that the worst offender, Rescal, is much better suited
for a supernode-driven network like YouTube. There, its fre-
quent link predictions around supernodes matches the net-
work structure and produces much more accurate results.
Our analysis also shows that these
Temporal factors.
metrics tend to predict links between less active nodes. In
particular, for each snapshot Gt, we measure the idle time
for a node v in Gt as the time gap between t and the most
recent time when v creates an edge. Figure 8 shows that the
idle time of nodes in predicted edges by all metrics are larger
than that of ground truth, meaning that they are all biased to
nodes that are dormant recently, which are less likely to form
new edges.
5. CLASSIFICATION-BASED PREDIC-
TION
Classiﬁcation-based algorithms apply supervised learning
to predict links using multiple similarity metrics as features.
The key challenge is how to scale to large OSNs, i.e. being
able to predict edges among all possible node pairs. Prior
works limit the prediction coverage to a very small sub-
set of node pairs [36, 38]. Another challenge is that so-
cial networks are highly sparse, translating into highly “im-
balanced” positive (connected) and negative (disconnected)
subsets. Prior work cites data imbalance as a major cause
of low prediction accuracy [15]. In our 55M-edge Renren
snapshot for example, the ratio of positive to negative links
is 1 : 179K, and decreases further as the network grows.
In this section, we evaluate classiﬁcation-based link pre-
diction in practical scenarios, using our large OSN datasets
with high data imbalance. To do so, we develop a scalable
measurement mechanism for implementing and evaluating
classiﬁcation-based algorithms. We also study how they per-
form on imbalanced data and compare their results to metric-
based prediction algorithms.
5.1 Evaluation Conﬁguration
Classiﬁcation-based algorithms ﬁrst train models (classi-
ﬁers) using labeled data and their corresponding features,
then apply the trained classiﬁers to test data to predict their
labels. Link prediction only requires binary classiﬁcation
(“+” for creating an edge and “-” for no edge). The key
challenge in evaluating these algorithms is how to train and
make prediction on all possible node pairs – this requires
computing all the features for O(|V 2| − |E|) node pairs and
making a classiﬁcation decision ( |V | and |E| the graph node
and edge count). Even for a “small” Renren snapshot (2.3M
nodes, 25M edges), it takes 88 days to compute features!
To address this challenge, we con-
Snowball Sampling.
sider limiting our evaluation using snowball sampling [12],
which has been shown to effectively reduce computation cost
while preserving network structure and statistical represen-
tativity. Speciﬁcally, for a snapshot Gt−2 = {Vt−2, Et−2}
we ﬁrst randomly select a node v as the seed, then run a
breadth-ﬁrst-search from node v until a ﬁxed percentage p
of nodes are visited. These visited nodes V S
t−2 are the sam-
pled nodes in snapshot Gt−2. We repeat the process on the
next snapshot Gt−1 = {Vt−1, Et−1} using the same seed v,
producing V S
t−1. The choice of sampling percentage p must
balance between computation cost and data representativity.
We conﬁgure p based on the network size. Since our Face-
book network is reasonably small, p=100%. For Renren and
YouTube, p=2%.
Next, we apply common classiﬁcation methods on these
sampled node sets. During the training process, we measure
the similarity features of all node pairs among V S
t−2 in Gt−2,
labeling each node pair as either positive or negative depend-
ing on whether they are connected in Gt−1, and training a
classiﬁer using this labeled set. In the testing process, we
collect features between node pairs among V S
t−1 in Gt−1,
feed them into the trained classiﬁer to compute prediction
scores, and then choose the top k node pairs with the highest
scores as the new edges for the next snapshot Gt = {Vt, Et}.
As in §4, we set k to the actual number of new edges created
among node pairs in V S
t−1 for Gt, and use the accuracy ratio
to evaluate prediction accuracy. To minimize the impact of
seeds, we randomly select 5 nodes as seeds, repeat classiﬁca-
tion methods on them, and measure the average and standard
deviation of prediction accuracy ratios.
Given the computation complexity, we limit our evalua-
tion to two instances (listed in Table 6) of different sizes
(small and large) for all three networks. Again, because
these instances produce highly consistent results and space
constraints, we only discuss the results for the large net-
works.
We use scores from all 14
Features and Classiﬁers.