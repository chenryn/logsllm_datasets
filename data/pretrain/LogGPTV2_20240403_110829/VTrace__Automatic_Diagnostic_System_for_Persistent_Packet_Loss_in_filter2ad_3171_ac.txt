Metrics
5-tuple
packet_ID
out_sip
out_dip
node_ip
loc
NAT_key
dscp_mark
msg
timestamp
An example for dscp_mark setting: For the virtual flow inter-
acting in or between VPCs (see Fig. 1(a) and Fig. 1(d)), both the
first-hop and last-hop VFDs (i.e., VFD 1 and VFD 𝑛 in Fig. 4) are
vSwitches and they set dscp_mark=1 and dscp_mark=2 in the log
for a target packet coming from the source and going to the des-
tination, respectively. Similarly, for the flow interacting with the
Internet or customer data centers (see Fig. 1(b) and Fig. 1(c)), taking
the direction of packets entering the cloud as an example, the first-
hop VFD (vRouter or vBRouter) generates dscp_mark=1 for the
inlet packet whereas the last-hop VFD (vSwitch) sets dscp_mark=2
in the log when a target packet leaves it. Otherwise, the values of
dscp_mark are 0 in those VTrace-related logs.
Besides the dscp_mark, if the packet is discarded during the
forwarding, the VFD will carefully archive the root cause in the
fast path, which is obtained according to the deep analysis updated
from the slow path, into the metric msg of the log. At last, each VFD
transmits the generated log data to the local Log Agent.
3.5 DCA Design
As the log data is generated in a distributed manner, it is essential
to collect all data of one task together to perform the virtual flow
path reconstruction and further analysis. DCA is deployed for this
purpose. To meet Requirement 4, we adopt the stream process en-
gine, JStorm [2], which is developed based on Apache Storm [20].
The scalability and powerful computing ability of JStorm help to
provide quick path reconstruction for a potentially large number of
VTrace tasks. The detailed design for DCA is provided as follows.
3.5.1 Pipelines of VTrace in JStorm. First of all, we introduce
the pipelines of VTrace in JStorm to show how a VTrace task is
completed in DCA when the related log data is ready. A JStorm
topology for VTrace, which consists of spouts and bolts, is carefully
designed and deployed on the process engine, as Fig. 5 shows. The
root nodes, "spouts", feed the subsequent nodes, "bolts". The spouts
retrieve data from a number of data origins. And the bolts process
the data stream from these spouts. A pool of bolts is maintained to
provide parallel computation.
Specifically, there are two kinds of spouts in VTrace. VTrac-
eTaskSpouts and LogSpouts are responsible for extracting VTrace
task records and VTrace logs from VTrace Task Database and Log
Agents, respectively. Especially, to track arbitrary target packet in
the cloud, LogSpouts collect the network-wide log data from Log
Agents in different regions. A VTraceTaskBolt is activated when an
unprocessed task is found and then it starts collecting the log data
Figure 4: Procedure of coloring, matching and logging.
Note that this value remains unchanged during the packet’s trip
to the destination. In such a coloring setup, any target flow in the
overlay network could be tracked flexibly. In addition, to provide
convenience for recognizing the trace of a specific packet, a unique
Packet_ID is attached to the packet.
Step 2: Matching packets with given rules. In VTrace, there are
two kinds of matching actions: (1) Source VFDs match packets with
the filter, and (2) all VFDs match the colored tag, namely checking
the dscp value of incoming packets. Then, the matched packets
take the fast path (or the slow path if it is the first target packet of
the VTrace task, as Fig. 3 depicts) for forwarding. For source VFDs,
they maintain a set of rules for matching with filter, which is
updated by the configuration policies from the cloud controller once
a new VTrace task arrives. The corresponding filter rules will
be deleted with the task finished. In addition, for all VFDs, the rule
of matching the dscp field is installed when VFDs are configured
to support VTrace. Note that there is no further VTrace-related
rule installation on non-source VFDs during their operation. In
summary, VTrace installs several and tens of rules in non-source
VFDs and source VFDs, respectively, which consumes only a small
fraction of the VFD flow table and is usually not a bottleneck of
VTrace.
Step 3: Logging the event. After unpacking the target packet’s
header in the VFD, a bunch of information can be obtained, such as
5-tuple, packet_ID, out_sip, out_dip and node_ip, as listed in
Table 2. Also, the packet’s location in a VFD, represented by the met-
ric loc, is included in the log, where loc=in and loc=out denote
the ingress and egress of a VFD, respectively. In addition, given the
multiple types of virtual flows, the target packet may go in/out of a
VPC or to the Internet via a vRouter (e.g., the virtual flows in Fig.
1(b)), during which the 5-tuple of the packet is changed by NAT
boxes. The altered 5-tuple is denoted as NAT_key, which should
be recorded such that the log data for packets with and without
NAT operation can be recognized as in the same VTrace task by
DCA. Besides, for the convenience of reconstructing virtual flow
paths in DCA in Section 3.5, VTrace defines a marker dscp_mark to
indicate the location of the packet in its trajectory, which includes
three states: a) dscp_mark=1 means the target packet entering the
first-hop VFD (e.g., VFD 1 in Fig. 4), b) dscp_mark=2 represents the
packet leaving the last-hop VFD that is right before the destination
(e.g., VFD 𝑛 in Fig. 4), and c) dscp_mark=0 indicates the rest. Note
that the dscp_mark is determined based on the types of virtual
flows and is recorded in the log. Here is an example to clearly show
how to determine dscp_mark.
36
VFD 1SourceRespective Local Log AgentsColoringForwardingLoggingForwardingVFD 2……NormalFlowsdscp_mark=1dscp_mark=0dscp_mark=2Next hopForwardingForwardingLoggingVFD nForwardingForwardingLoggingdscp_mark=0(VM/Internet)VTraceFlowsNormal FlowsLog DataDestination(VM/Internet)VTrace
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 5: The pipelines of VTrace in the JStorm process engine.
related to the task, preprocessing the log data (i.e., filter, transform,
and group) and sending the results to a SortBolt that is directly
connected with the VTraceTaskBolt (See Fig. 7(a)) to recover the
flow path. Finally, the WriteBolt stores the result in the Results
Database to support user queries for VTrace results.
3.5.2 Policy of data collection. To reconstruct the virtual flow
path, data collection in JStorm is the first step. For VTrace Task
Spouts, they search the VTrace Task Database for unprocessed
VTrace tasks to activate idle VTraceTaskBolts to complete these
tasks. For a wake-up VTraceTaskBolt, it starts collecting the log data
related to its task via LogSpouts where LogSpouts keep retrieving
data from Log Agents. Therefore, the VTrace-related log data can
be extracted once VFDs send the generated logs to their local Log
Agent. Observing that the parameter trace_time is used to limit
the tracing duration in VFDs, VTraceTaskBolts should keep data
collection for at least trace_time. Since LogSpouts retrieve the
log data from different Log Agents around the world, there is a
delay from VFD generating logs to JStorm receiving logs. Thus, for
each working VTraceTaskBolt, it starts timing when receiving the
first valid log data. To make sure that all log data of each task is
completely collected, VTraceTaskBolt would wait for an additional
buffer_time on top of trace_time to collect log data. Considering
the special case that there may be no enough virtual production
traffic of interest to fulfill the parameter packet_count, we come
up with two plans of buffer_time for the normal and special cases,
as Algorithm 1 depicts.
VTrace plan A: Plan A is designed for the normal case
that there is enough target production traffic. Here, we set a
short_buffer_time (e.g., 10 seconds), and the data collection dura-
tion wait_time is equal to trace_time plus short_buffer_time.
FirstPackCnt, the counter for the packet at the first hop (i.e., its
log has dscp_mark=1), denotes the counter for the collected target
packets in DCA. When the FirstPackCnt reaches the packet_count
during the data collection duration, VTraceTaskBolt continues to
read the remaining logs of the packet trace until the time runs out.
VTrace plan B: If FirstPackCnt is less than packet_count when
VTrace plan A is completed, then VTrace plan B is adopted. The
data collection duration wait_time is set to be trace_time plus
long_buffer_time. This setting is used to ensure that no more
data of interest is generated. Once the collection time is used up,
the data collection is ended.
3.5.3 Preprocessing of log data. The collected logs are prepro-
cessed by filtering, transforming and grouping as follows.
Algorithm 1: Collecting log data of a VTrace task.
Input: The filter and NAT_key of tasks KEY_SET, trace_time, packet_count
and the log data collected by LogSpouts LogSpoutsData.
Output: The log of a specific VTrace task, VTraceTaskData.
1 wait_time ← trace_time + short_buffer_time;
2 FirstPackCnt ← 0;
3 foreach log∈LogSpoutsData do
if 5-tuple ∈ filter or NAT_key ∈ KEY_SET then
if FirstPackCnt  wait_time then
wait_time ← trace_time + long_buffer_time;
if used_time > wait_time then
Start Path Reconstruction;
// Plan B;
16 return VTraceTaskData;
Filtering log data for each task: The filtering operation is nec-
essary for each task, as LogSpouts will collect network-wide log
data. Basically, using the filter is enough. For scenarios with NAT
operation, filtering with both the NAT_key and the 5-tuple in the
filter is sufficient to obtain all related log data.
Transforming Packet_ID to a unified form: The trace for each
target packet in the virtual flow can be easily identified through
the Packet_ID. However, the way of generating Packet_ID may
not be the same if the implementation of those VFDs are different,
which makes it possible to treat one packet trajectory as multiple
trajectories. For example, vSwitches use little endian to generate
Packet_ID whereas vRouters use big endian. Therefore, VTrace
transforms the Packet_ID of logs to a unified form.
Grouping the out-of-order log data for each packet: After fil-
tering and transforming, the log data is still disordered. It is not
feasible to obtain the packet trajectory directly through sorting
the data according to fine-grained timestamps in logs, since these
timestamps may be generated in different servers where the clock is
out of sync. But the timestamps generated by the VFDs in the same
physical server must be uniform. Based on this observation, we
first group all the obtained logs by packet according to Packet_ID
and then group the logs of each packet by server based on node_ip.
In the sequel, we rearrange the order of the data in each server
according to the machine timestamps. An example of the results
37
Log AgentsLog AgentsLog AgentsVTrace Task DatabaseSLSSpoutsSLSSpoutsLogSpoutsVTrace TaskSpoutsVTrace Task BoltsTransferGroupFilterFilterFilterTransferTransferGroupGroupGroupGroupPath ReconstructionSort BoltsGroupGroupWrite DBWrite BoltsDBNetwork-wide logTask 2  Task 1Unprocessed  RunningTask StatusShanghai:Log 1, Log 2, ...US-East:Log 1, Log 2, ...EU-Central:Log 1, Log 2, ...    JStorm Process EngineDataShowing an exampleSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
C. Fang et al.
(a) Data collection from LogSpouts.
(b) Collect VTrace tasks.
Figure 7: Implementation of data collection in JStorm.
To be specific, we modify the structure of the VTrace topology
so that the VTrace Task Database or each region-level Log Agent
is accessed only by one spout and each spout broadcasts the data
to all VTraceTaskBolts, as shown in Fig. 7(a). Additionally, each
VTraceTaskBolt is connected to one SortBolt. The modification of
VTrace topology releases the potential risk induced by the increase
of bolts and guarantees that each bolt obtains the same data.
In addition, VTraceTaskSpout queries the VTrace Task Database
for unprocessed tasks periodically, e.g. once per second. When an
unprocessed task is found, VTrace needs to choose an available
VTraceTaskBolt to do the task. As VTraceTaskBolt does not feed-
back its working status (e.g., null or busy) to VTraceTaskSpout,
VTraceTaskSpout can not choose an idle VTraceTaskBolt precisely.
Instead, VTraceTaskSpout would send the task to all of VTrac-
eTaskBolts by shuffling, as described in Fig. 7(b). In each VTrac-
eTaskBolt, we set an important variable vtrace_state to indicate the
current progress of the VTrace task, where vtrace_state=null repre-
sents that no tasks are in progress. Once an idle VTraceTaskBolt
receives a new VTrace task, the state will be updated from null
to WaitVTraceData. The corresponding record in the VTrace Task
Database is modified by the VTraceTaskBolt. Consequently, in the
next query cycle, the record for the previous task will not be un-
processed anymore.
4 EVALUATION
We evaluate VTrace by 1) conducting experiments in a test envi-
ronment, where 35 commodity multi-core servers and 2 vRouters
are used for the experiments, and 2) deploying it in our produc-
tion cloud network where about 90% of VFDs in the cloud support
VTrace function. First, we testify the impact on the performance of
VFDs in the test environment and analyze the storage and band-
width overhead. Additionally, the responding time of VTrace is
illustrated in both environments.
4.1 Impact on the Performance of VFDs
Coloring, matching and logging operations impose a burden on
VFDs, which may influence the forwarding function and result in a
higher CPU usage of servers. To demonstrate that VTrace induces
an ignorable effect on the cloud network, we make pressure tests on
VTrace-enabled vSwitches and vRouters in the test environment.
Influence on vRouters: Pressure tests on vRouters are conducted
by a vRouter, a 100Gbps Spirent TestCenter [18] and a 100Gbps
switch. The vRouter in the test is the same as the one in the pro-
duction network, which has 2 ports, a 26-cores Intel(R) Xeon(R)
E5-2630 2.3GHz CPU, 256GB memory and a 40Gbps NIC. For the
Figure 6: An example of recovering a virtual flow path.
after grouping is illustrated in Fig. 6: in Server 1 with node_ip_1,
Pack 1 comes earlier than Pack 2.
3.5.4 Reconstruction of the virtual flow path. SortBolt recon-
structs the flow path based on the preprocessed log data. Note that
tenants and operations engineers need the location of the culprit
VFD and the IP of the server where the VFD is located, respectively.
Here, we focus on showing the physical server(s) through which
a virtual flow passes. With such kind of path, VTrace can directly
construct the path composed of VFDs through the cloud controller
based on the knowledge of the tenant’s allocated resources and
virtual-to-physical mapping.
Specifically, VTrace reconstructs the virtual flow path based on