8
end, we ﬁrst convert a trace from the time domain into the
frequency domain with Fast Fourier Transform (FFT). We
denote the magnitude value at the square wave frequency
(i.e., 1
T ) as signal strength and the average magnitude at other
frequencies as noise strength. SNR is their ratio. Figure 10 in
Appendix E shows traces of different SNR values under the
eviction-based probe. As we can see, for the trace of SNR
larger than 10, each bit can be clearly recognized. For the
trace with SNR 10, some bits cannot be decoded. When SNR
is less than 10, the bits can hardly be decoded.
C. Distribution of Mesh Trafﬁc
The prior evaluation proves the variance of probe delay is
related to the program’s activities. In this subsection, we try to
quantify the distribution of mesh trafﬁc and assess the spatial
resolution of the channel.
Distribution by Trafﬁc Types. We place a victim application
described in Section VI-B at each tile and access every LLC
slice to generate mesh trafﬁc. Then, we select a route that is
related to every trafﬁc type (i.e., T1-T7) and place eviction-
based probes to collect delays. Table IV left (“w/o LLC slice
isolation”) shows the result.
It turns out, on the paths from the victim core to the victim
LLC slice (i.e., T1 and T2), the delay traces are highly related
to the memory access (the average SNRs are all larger than
10). Besides, the probes closer to LLC (i.e., X route for T1
and Y for T2) yield better SNR (the average SNRs are larger
than 30 and the ratio of SNR >10 traces is more than 0.6).
This result shows which LLC is accessed by an application
can be discerned, suggesting the MESHUP side-channel has
LLC-slice level spatial resolution.
On routes not directly related to core-to-LLC communi-
cation, like the route from IMC (T3), the collected delay
traces still have non-negligible SNR. Moreover, we found
there are delays related to T7, i.e., unknown mesh trafﬁc.
One explanation for the existence of these seemingly irrelevant
mesh trafﬁc is that CPU might broadcast messages under
certain events, e.g., credit exhaust messages to notify that
congestion is happening.
Distribution by Geometry. Though we assume the attacker
cannot select a path at his/her will, contending to victim’s
mesh trafﬁc by running the application on any tile is feasible,
as the mesh trafﬁc should be distributed across the mesh
network, hitting different tiles. We validate this assumption
through an emulation. We run a victim process to access a
chunk of consecutive memory in on-off style, and we pin the
process to CHA 9. Then we run the attacker process to probe
all mesh paths: we enumerate the tiles other than CHA 9, pin
the attacker process to the core, and then probe from the core
towards all LLC slices.
We compute the max and median SNR over all paths for
each assigned attacker core. Table V shows our results. CHA
14 yields the best SNR. For most of the other tiles, the SNR
values are sufﬁcient for recovering the access pattern (i.e.,
SNR over 10), suggesting MESHUP is effective even when a
random core is occupied.
Fig. 5: Delay traces collected by the receiver of different T .
The x-axis is the running period of the sender.
accessing the memory. When T is larger than 5us, clear peaks
can be observed on the delay trace. However, when T drops
to 2us, we are unable to identify those peaks. As such, the
access pattern is leaked to mesh links, and memory access
lasting longer than 2.5us(T /2) can be recognized from the
delay trace.
On the Intel SP platform, the probe can infer victim activity
at the granularity of around 17 memory accesses. One DRAM
access takes around 150ns (370 cycles) as reported in [22].
When T is 5us, the access is sustained for only 2.5us as the
victim accesses memory in on-off style. Therefore, the gran-
ularity is 17 (i.e., 2500/150). Though the resolution is lower
than ﬁne-grained probes, like 25ns for PRIME+SCOPE [67],
we found it is sufﬁcient to leak secret like encryption key.
Fig. 6: Delay trace collected by coherence-based probe.
Cross-CPU delays. We also tried to use the coherence-based
probe to evaluate the cross-CPU trafﬁc. The sender process
accesses the memory of another socket in the on-off style,
and a pair of coherence-based probes are placed at each CPU
to collect delay traces. When T is 35us, the congestion can be
clearly sensed by the probe. Figure 6 shows the delay trace.
Though the temporal resolution is worse than the eviction-
based probe, the delay variance resulting from UPI congestion
can be over 4 times (from 10K cycles to 40K cycles). The
variance of the eviction-based probe is around 50%.
Signal to Noise Ratio (SNR). To quantify the quality of the
collected traces, we calculate the SNR of each trace. To this
9
098.2usT = 30us11.520119.1usT = 5us11.52Interval (us)0115.9usT = 2us11.520150 s10k40kInterval μCycles)Probe Route
Avg. SNR
Ratio (SNR >10)
T2
T1
Y
12.74
0.20
X
38.97
0.72
Y
32.74
0.64
w/o LLC slice isolation
T3
X
14.51
0.36
Y
4.73
0.05
X
9.34
0.28
Y
5.46
0.07
T4
X
8.12
0.21
w/ LLC slice isolation
Others
Others
T3
Y
5.36
0.07
X
7.40
0.12
Y
3.46
0.00
X
3.36
0.00
Y
6.19
0.08
X
4.60
0.04
Y
6.53
0.08
T4
X
6.95
0.13
TABLE IV: The SNR of delay traces probed at different links grouped by trafﬁc types. Y and X are the trafﬁc encountered
at the Y-route and X-route. The last row shows the ratio of the delay sequences whose SNR are larger than 10.
UPI
0
27.79
9.74
IMC0
1
15.60
9.52
PCIE
PCIE
2
3
4
5
6
21.03
10.30
21.82
11.63
16.92
11.77
17.32
12.17
16.47
11.46
30.34
11.74
7
8
19.28
11.30
9
Victim
10
15.42
7.32
11
24.25
11.77
PCIE
21
18.61
10.67
IMC1
22
23.18
10.37
23
16.40
8.39
RLINK
12
23.42
11.92
13
17.17
9.60
14
49.75
26.25
15
23.28
13.21
16
21.57
9.65
UPI2
17
15.74
10.24
18
10.89
7.31
19
19.89
12.43
20
19.35
10.58
TABLE V: The measured SNR when the attack is pinned
to each core. Numbers in every tile represents the CHA ID,
maximum and median SNR values respectively.
Events
w/o victim
w/ victim
HORZ RING AD IN USE
HORZ RING AK IN USE
HORZ RING BL IN USE
HORZ RING IV IN USE
TxR HORZ OCCUPANCY
AG1 BL CRD OCCUPANCY
RxR OCCUPANCY
1004151
1952646432
3914059524
35639
60996
4690
822149631
2530753880
4665246823
102218
482979
141706754
4414430426
5346595687
STALL NO TxR HORZ
CRD BL AG1
RxR BUSY STARVED
RxR CRD STARVED
TxR HORZ STARVED
TxR HORZ NACK
TxR VERT NACK
VNA CREDIT RETURN
OCCUPANCY
M3 CRD RETURN BLOCKED
350
9227
2041
0
38951
2
5005
41261801
65177026
6942
188482
19405478
1301612583
1363707
2668519186
60557495
TABLE VI: PMU events that are changed most rapidly.
D. Causes of Delay Increase
To obtain an in-depth understanding of the mesh delays, we
leverage Intel PMU [63] to record the events related to mesh
stops, and use the event counters to identify the root cause.
The experiment settings follow Section VI-B.
PMU Event Comparison. We launch and stop the victim ap-
plication and record the PMU events separately. Table VI lists
the PMU events that are changed most among all the events.
These events mainly fall into three categories: resources in
use, resource starvation, and NACKs. The increase of the
ﬁrst category counters indicates that the related components
10
inside the mesh stop become busier, as the growth of these
counters means more time or buffer resources are spent. For
example, the increase of HORZ_RING_XX_IN_USE indicates
that the mesh stops spent more cycles in forwarding packets
to horizontal direction; the increase of events that ends with
_OCCUPANCY suggests more buffers or credits have been
occupied.
stalled more
frequently, because
The second category indicates that the forwarding com-
re-
ponents get
the
the