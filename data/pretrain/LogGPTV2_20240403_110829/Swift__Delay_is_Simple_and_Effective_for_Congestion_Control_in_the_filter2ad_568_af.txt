Systems Design and Implementation (OSDI 16). USENIX Association, Savannah,
GA, 249–264. https://www.usenix.org/conference/osdi16/technical-sessions/
presentation/gao
https://
[21] Peter X. Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Rat-
nasamy, and Scott Shenker. 2015. pHost: Distributed Near-optimal Datacenter
Transport over Commodity Network Fabric. In Proceedings of the 11th ACM Con-
ference on Emerging Networking Experiments and Technologies (CoNEXT ’15). ACM,
New York, NY, USA, Article 1, 12 pages. https://doi.org/10.1145/2716281.2836086
[22] Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, Robert N. M. Watson,
Andrew W. Moore, Steven Hand, and Jon Crowcroft. 2015. Queues Don’t Matter
When You Can JUMP Them!. In 12th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 15). USENIX Association, Oakland, CA, 1–
14. https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/
grosvenor
[23] Mark Handley, Costin Raiciu, Alexandru Agache, Andrei Voinescu, Andrew W.
Moore, Gianni Antichik, and Marcin Mojcik. 2017. Re-architecting Datacenter
Networks and Stacks for Low Latency and High Performance. In Proceedings of
the ACM SIGCOMM 2017 Conference (SIGCOMM ’17). ACM, New York, NY, USA,
29–42.
[24] Chi-Yao Hong, Matthew Caesar, and P. Brighten Godfrey. 2012. Finishing Flows
Quickly with Preemptive Scheduling. In Proceedings of the ACM SIGCOMM 2012
Conference (SIGCOMM ’12). ACM, New York, NY, USA, 127–138. https://doi.org/
10.1145/2342356.2342389
[25] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman
Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen
Zhao, and Steven Swanson. 2019. Basic Performance Measurements of the In-
tel Optane DC Persistent Memory Module. CoRR abs/1903.05714 (2019), 1–61.
arXiv:1903.05714 http://arxiv.org/abs/1903.05714
[26] Raj Jain, Dah Ming Chiu, and Hawe WR. 1984. A Quantitative Measure Of Fairness
And Discrimination For Resource Allocation In Shared Computer Systems. (09
1984), 37 pages.
[27] Dina Katabi, Mark Handley, and Charlie Rohrs. 2002. Congestion Control for
High Bandwidth-Delay Product Networks. In Proceedings of the 2002 Conference
on Applications, Technologies, Architectures, and Protocols for Computer Communi-
cations (SIGCOMM âĂŹ02). Association for Computing Machinery, New York,
NY, USA, 89âĂŞ102. https://doi.org/10.1145/633025.633035
[28] K. Katrinis, D. Syrivelis, D. Pnevmatikatos, G. Zervas, D. Theodoropoulos, I.
Koutsopoulos, K. Hasharoni, D. Raho, C. Pinto, F. Espina, S. Lopez-Buedo, Q. Chen,
M. Nemirovsky, D. Roca, H. Klos, and T. Berends. 2016. Rack-scale disaggregated
cloud data centers: The dReDBox project vision. In 2016 Design, Automation Test
in Europe Conference Exhibition (DATE). IEEE, Dresden, Germany, 690–695.
[29] Changhoon Kim, Parag Bhide, Ed Doe, Hugh Holbrook, Anoop Ghanwani, Dan
Daly, Mukesh Hira, and Bruce Davie. 2016. InâĂŘband Network Telemetry (INT).
https://p4.org/assets/INT-current-spec.pdf. (2016). Accessed: 2020-01-13.
[30] Ana Klimovic, Christos Kozyrakis, Eno Thereska, Binu John, and Sanjeev Kumar.
2016. Flash Storage Disaggregation. In Proceedings of the Eleventh European
Conference on Computer Systems (EuroSys âĂŹ16). Association for Computing
Machinery, New York, NY, USA, Article Article 29, 15 pages. https://doi.org/
10.1145/2901318.2901337
[31] Ana Klimovic, Heiner Litz, and Christos Kozyrakis. 2017. ReFlex: Remote Flash
Local Flash. In Proceedings of the Twenty-Second International Conference on
Architectural Support for Programming Languages and Operating Systems (ASPLOS
âĂŹ17). Association for Computing Machinery, New York, NY, USA, 345âĂŞ359.
https://doi.org/10.1145/3037697.3037732
[32] Gautam Kumar, Srikanth Kandula, Peter Bodik, and Ishai Menache. 2013. Virtu-
alizing Traffic Shapers for Practical Resource Allocation. In Presented as part of
the 5th USENIX Workshop on Hot Topics in Cloud Computing. USENIX, San Jose,
CA, 1–6. https://www.usenix.org/conference/hotcloud13/workshop-program/
presentations/Kumar
[33] C. Lee, C. Park, K. Jang, S. Moon, and D. Han. 2017. DX: Latency-Based Congestion
Control for Datacenters. IEEE/ACM Transactions on Networking 25, 1 (Feb 2017),
335–348. https://doi.org/10.1109/TNET.2016.2587286
[34] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo
Tang, Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and et
al. 2019. HPCC: High Precision Congestion Control. In Proceedings of the
ACM Special Interest Group on Data Communication (SIGCOMM âĂŹ19). As-
sociation for Computing Machinery, New York, NY, USA, 44âĂŞ58. https:
//doi.org/10.1145/3341302.3342085
[35] Youyou Lu, Jiwu Shu, Youmin Chen, and Tao Li. 2017. Octopus: an RDMA-enabled
Distributed Persistent Memory File System. In 2017 USENIX Annual Technical
Conference (USENIX ATC 17). USENIX Association, Santa Clara, CA, 773–785.
https://www.usenix.org/conference/atc17/technical-sessions/presentation/lu
[36] Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer,
Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C. Evans, Steve
Gribble, and et al. 2019. Snap: A Microkernel Approach to Host Networking. In
Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP
âĂŹ19). Association for Computing Machinery, New York, NY, USA, 399âĂŞ413.
https://doi.org/10.1145/3341301.3359657
[37] M. Mathis, J. Mahdavi, S. Floyd, and A. Romanow. 1996. TCP Selective Acknowl-
edgment Options. RFC 2018. RFC Editor.
[38] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats.
2015. TIMELY: RTT-based Congestion Control for the Datacenter. In Proceedings
of the 2015 ACM Conference on Special Interest Group on Data Communication
526
Kumar et al.
[49] IEEE Std. 2011. IEEE. 802.11Qbb. Priority based flow control. (2011).
[50] Mohit P. Tahiliani, Vishal Misra, and K. K. Ramakrishnan. 2019. A Principled
Look at the Utility of Feedback in Congestion Control. In Proceedings of the
2019 Workshop on Buffer Sizing (BS âĂŹ19). Association for Computing Machin-
ery, New York, NY, USA, Article Article 8, 5 pages. https://doi.org/10.1145/
3375235.3375243
[51] Jordan Tigani and Siddartha Naidu. 2014. Google BigQuery Analytics. Wiley,
Indianapolis, IN, USA.
[52] Balajee Vamanan, Jahangir Hasan, and T.N. Vijaykumar. 2012. Deadline-aware
Datacenter TCP (D2TCP). In Proceedings of the ACM SIGCOMM 2012 Conference
(SIGCOMM ’12). ACM, New York, NY, USA, 115–126. https://doi.org/10.1145/
2342356.2342388
[53] Washington State Department of Transportation. 2020. What is a roundabout?
https://www.wsdot.wa.gov/Safety/roundabouts/BasicFacts.htm. (2020). Ac-
cessed: 2020-01-13.
[54] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011.
Better Never Than Late: Meeting Deadlines in Datacenter Networks. In Proceed-
ings of the ACM SIGCOMM 2011 Conference (SIGCOMM ’11). ACM, New York,
NY, USA, 50–61. https://doi.org/10.1145/2018436.2018443
[55] Jian Xu and Steven Swanson. 2016. NOVA: A Log-structured File System for
Hybrid Volatile/Non-volatile Main Memories. In 14th USENIX Conference on File
and Storage Technologies (FAST 16). USENIX Association, Santa Clara, CA, 323–338.
https://www.usenix.org/conference/fast16/technical-sessions/presentation/xu
[56] Jian Yang, Joseph Izraelevitz, and Steven Swanson. 2019. Orion: A Distributed
File System for Non-Volatile Main Memory and RDMA-Capable Networks. In
17th USENIX Conference on File and Storage Technologies (FAST 19). USENIX
Association, Boston, MA, 221–234. https://www.usenix.org/conference/fast19/
presentation/yang
[57] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma,
Murphy McCauly, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re-
silient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster
Computing. In Presented as part of the 9th USENIX Symposium on Networked Sys-
tems Design and Implementation (NSDI 12). USENIX, San Jose, CA, 15–28. https:
//www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia
[58] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and
Ion Stoica. 2010. Spark: Cluster Computing with Working Sets. In Proceedings of
the 2nd USENIX Conference on Hot Topics in Cloud Computing (HotCloudâĂŹ10).
USENIX Association, USA, 10.
[59] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
Ming Zhang. 2015. Congestion Control for Large-Scale RDMA Deployments.
In Proceedings of the 2015 ACM Conference on Special Interest Group on Data
Communication (SIGCOMM ’15). ACM, New York, NY, USA, 523–536. https:
//doi.org/10.1145/2785956.2787484
[60] Yibo Zhu, Monia Ghobadi, Vishal Misra, and Jitendra Padhye. 2016. ECN or
Delay: Lessons Learnt from Analysis of DCQCN and TIMELY. In Proceedings of
the 12th International on Conference on Emerging Networking EXperiments and
Technologies (CoNEXT âĂŹ16). Association for Computing Machinery, New York,
NY, USA, 313âĂŞ327. https://doi.org/10.1145/2999572.2999593
(SIGCOMM ’15). ACM, New York, NY, USA, 537–550. https://doi.org/10.1145/
2785956.2787510
[39] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018.
Homa: A Receiver-driven Low-latency Transport Protocol Using Network Pri-
orities. In Proceedings of the 2018 Conference of the ACM Special Interest Group
on Data Communication (SIGCOMM ’18). ACM, New York, NY, USA, 221–235.
https://doi.org/10.1145/3230543.3230564
[40] Jacob Nelson, Brandon Holt, Brandon Myers, Preston Briggs, Luis Ceze, Simon
Kahan, and Mark Oskin. 2015. Latency-Tolerant Software Distributed Shared
Memory. In 2015 USENIX Annual Technical Conference (USENIX ATC 15). USENIX
Association, Santa Clara, CA, 291–305. https://www.usenix.org/conference/
atc15/technical-session/presentation/nelson
[41] John Ousterhout, Arjun Gopalan, Ashish Gupta, Ankita Kejriwal, Collin Lee,
Behnam Montazeri, Diego Ongaro, Seo Jin Park, Henry Qin, Mendel Rosenblum,
et al. 2015. The RAMCloud Storage System. ACM Transactions on Computer
Systems (TOCS) 33, 3 (2015), 7.
[42] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans
Fugal. 2014. Fastpass: A Centralized “Zero-queue” Datacenter Network. In Pro-
ceedings of the ACM SIGCOMM 2014 Conference (SIGCOMM ’14). ACM, New York,
NY, USA, 307–318. https://doi.org/10.1145/2619239.2626309
[43] Ahmed Saeed, Nandita Dukkipati, Vytautas Valancius, Vinh The Lam, Carlo
Contavalli, and Amin Vahdat. 2017. Carousel: Scalable Traffic Shaping at End
Hosts. In Proceedings of the Conference of the ACM Special Interest Group on Data
Communication (SIGCOMM âĂŹ17). Association for Computing Machinery, New
York, NY, USA, 404âĂŞ417. https://doi.org/10.1145/3098822.3098852
[44] Ahmed Saeed, Yimeng Zhao, Nandita Dukkipati, Ellen Zegura, Mostafa Ammar,
Khaled Harras, and Amin Vahdat. 2019. Eiffel: Efficient and Flexible Software
Packet Scheduling. In 16th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 19). USENIX Association, Boston, MA, 17–32. https:
//www.usenix.org/conference/nsdi19/presentation/saeed
[45] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang. 2018. LegoOS: A
Disseminated, Distributed OS for Hardware Resource Disaggregation. In 13th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 18).
USENIX Association, Carlsbad, CA, 69–87. https://www.usenix.org/conference/
osdi18/presentation/shan
[46] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, and et al.
2015. Jupiter Rising: A Decade of Clos Topologies and Centralized Control in
GoogleâĂŹs Datacenter Network. SIGCOMM Comput. Commun. Rev. 45, 4 (Aug.
2015), 183âĂŞ197. https://doi.org/10.1145/2829988.2787508
[47] Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica Wong-
Chan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant Chandra, Rob
Cauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L. Sabato, Joel Scherpelz,
and Amin Vahdat. 2020. 1RMA: Re-envisioning Remote Memory Access for Multi-
tenant Datacenters. In Proceedings of the 2020 ACM Conference on Special Interest
Group on Data Communication (SIGCOMM ’20). ACM, New York, NY, USA, to
appear.
[48] IEEE Std. 2010. IEEE 802.11Qau. Congestion notification. (2010).
527
Swift: Delay is Simple and Effective for Congestion Control in the Datacenter
Appendices are supporting material that have not been peer-
reviewed.
A CONVERSION BETWEEN HOST AND NIC CLOCKS
Some delay computations, e.g., NIC-Rx-queuing delay, require a
combination of NIC HW and host SW clocks. Swift uses a simple
linear-extrapolation approach to convert the incoming HW clock
into a host-clock value to compute such delays:
host_clock = ratio · nic_clock + o f f set
We update the ratio and o f f set periodically. The algorithm is sim-
ple: we read the nic_clock, then the host_clock and also maintain
the previous set of readings as last_nic_clock and last_host_clock.
The ratio and o f f set can then be updated as:
ratio = host_clock − last_host_clock
nic_clock − last_nic_clock
o f f set = host_clock − ratio · nic_clock
B PACKET FORMAT
Figure 23 shows the format that Swift uses, which consumes 4 bytes
to reflect back remote-side queuing delay. In addition, 1 byte is used
for to echo back forward-side hop-count by computing the differ-
ence between the initial TTL and observed TTL on the incoming
packet.
Figure 23: Packet format changes for Swift.
C DELAYS WITH SHARED RX/TX SCHEDULING
Traditionally, links are bidirectional, i.e., incoming traffic doesn’t
affect outgoing traffic on a link. Our deployment of Swift in Snap
had a unique challenge since Rx and Tx scheduling is shared in Snap.
While counter-intuitive, Swift design addresses this by factoring
local NIC queue buildup as part of endpoint congestion. The insight
is taken from traffic-roundabouts which also have shared Rx and Tx
scheduling; factoring local NIC-Rx delay emulates yield-at-entry as
the right-of-way when looked at it as a roundabout [53]. In other
words, if a machine’s NIC-Rx queue builds up, it should prioritize
clearing those packets before trying to inject more packets (through
Tx) in the network.
D EXPERIMENT WITH TARGET DELAY
In our experience, a nice property of delay is that as low latency
networking stacks advance to avoid interference from host CPUs,
Swift continues to work well just with a simple knob of target delay.
In Figure 24, we show results from a testbed implementation of
Swift in a prototype stack where it is able to achieve near line-rate
throughput (∼100Gbps) even at 15µs RTT.
E DELAY OR ECN
Zhu et al. [60] called out challenges in using delay as a congestion-
signal; we appreciate how some aspects of Swift's design ended up
addressing these challenges.
528
Figure 24: Achieved RTT vs. target delay, 160-flow incast.
• The authors show that a fluid model of TIMELY does not have a
unique fixed point because of reliance on the gradient-based con-
trol. Swift rather uses a target delay (vs. gradient-based control)
and in this way, doesn’t suffer from multiple fixed-points at con-
vergence. The authors echo our experience and provide a version
called, Patched TIMELY, that also gets rid of the delay-gradient.
• The authors rightly note that delay lags behind ECN in that mod-
ern switches mark ECN at packet-egress while delay, implicitly,
measures congestion when the packet arrives at the bottleneck
switch. As discussed in §3.2 and shown in experiments §5, Swift
uses a low target delay and does not delay ACKs, and hence
largely mitigates this concern. Testimony to this is our expe-
rience with Swift achieving low latency and losses at scale at
Google.
• The authors provide an important result (Theorem 6 in Refer-
ence [60]): purely relying on end-to-end delay measurements for
congestion control can provide either fairness or fixed-delay but
not both. The reason is that if the delay is controlled to a fixed
value, the algorithm is agnostic of the number of flows making
the system of equations inconsistent. Swift resolves this by not
using the same target-delay for all flows and instead scales target
delay as explained in §3.5. For example, flow based scaling varies
target delay as function of congestion window and converges to
a single fixed point. As shown in Figure 21, this greatly improves
fairness especially under large scale incast.
• The authors in Reference [50] argue that end-to-end delay is an
ambiguous signal in that a flow may traverse a wide variety of
link speeds across a number of hops. We believe while this can be
a valid concern for the Internet traffic, this concern is much less
applicable to a datacenter CC like Swift where paths are known
and the link speeds don’t vary as widely as in the Internet. Swift
uses topology-based scaling to account for different hop-counts
across flows.
• In addition, delay has a few important operational advantages.
First, delay evolves naturally as networks become faster, and
tuning it at scale is simpler than ECN especially for production
environments with multiple QoS classes—it has been shown that
ECN is problematic in such scenario and sojourn time is more
robust [8]. Second, delay has a multi-bit nature as a congestion
signal; it provides visibility into the extent of congestion, unlike
ECN which only signals whether congestion exists or not. That
said, we look forward to integrating multi-bit ECN signals like
sojourn-time [8] and INT [29] in Swift’s framework.
L2IPv4/v6TransportRemoteQueuingNIC RxTimestamp0 . . . 310 . . . 31Appended locallyForwardhop-count0 . . . 7020406080100TargetDelay(µs)04080120160200AchievedRTT(µs)Throughput99.9th-pRTTMedianRTTTargetdelay020406080100Throughput(Gbps)