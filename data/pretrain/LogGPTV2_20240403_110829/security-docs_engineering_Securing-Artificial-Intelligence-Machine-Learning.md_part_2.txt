As an example of this threat, imagine a database of stop signs throughout the world, in every language. That would be extremely challenging to curate because of the number of images and languages involved. Malicious contribution to that dataset would go largely unnoticed until self-driving cars no longer recognize stop signs. Data resilience and decision integrity mitigations have to work hand in hand here to identify and eliminate the training damage done by malicious data to prevent it from becoming a core part of the learning model.
## AI must have built-in forensics and security logging to provide transparency and accountability
AI will eventually be capable of acting in a professional capacity as an agent on our behalf, assisting us with high-impact decision-making. An example of this could be an AI that helps the processing of financial transactions. If the AI is exploited, and transactions manipulated in some way, the consequences could range from the individual to the systemic. In high-value scenarios, AI needs appropriate forensic and security logging to provide integrity, transparency, accountability, and in some instances, evidence where civil or criminal liability may arise.
Essential AI services need auditing/event-tracing facilities at the algorithm level whereby developers can examine the recorded state of specific classifiers, which may have led to an inaccurate decision. This capability is needed industry-wide in order to prove the correctness and transparency of AI-generated decisions whenever called into question.
Event tracing facilities could start with the correlation of basic decision-making information such as:
1. The timeframe in which the last training event occurred
2. The timestamp of the most recent dataset entry trained upon
3. Weights and confidence levels of key classifiers used to arrive at high-impact decisions
4. The classifiers or components involved in the decision
5. The final high-value decision reached by the algorithm
Such tracing is overkill for most algorithm-assisted decision making. However, having the ability to identify the data points and algorithm metadata leading to specific results are of great benefit in high-value decision making. Such capabilities not only demonstrate trustworthiness and integrity through the algorithm’s ability to “show its work”, but this data could also be used for fine-tuning as well.
Another forensic capability needed in AI/ML is tamper detection. Just as we need our AIs to recognize bias and not be susceptible to it, we should have forensic capabilities available to aid our engineers in detecting and responding to such attacks. Such forensic capabilities are of tremendous value when paired with data visualization techniques [10] allowing the auditing, debugging and tuning of algorithms for more effective results.
## AI must safeguard sensitive information, even if humans don’t
Rich experiences require rich data. Humans already volunteer vast amounts of data for ML to train against. This ranges from the mundane video streaming queue contents to trends in credit card purchases/transaction histories used to detect fraud. AI should have an ingrained sense of discretion when it comes to handling user data, always acting to protect it even when volunteered freely by an over-sharing public.
As an AI can have an authenticated group of “peers” it talks to in order to accomplish complex tasks, it must also recognize the need to restrict the data it shares with those peers.
## Early Observations on Addressing AI Security Issues
Despite the nascent state of this project, we believe the evidence compiled to date shows deeper investigation into each of the below areas is key in moving our industry towards more trustworthy and secure AI/ML products/services. The following are our early observations and thoughts on what we’d like to see done in this space.
1. AI/ML-focused penetration testing and security review bodies could be established to ensure that our future AI shares our values and aligns to the [Asilomar AI
    Principles](https://futureoflife.org/ai-principles/).
    1. Such a group could also develop tools and frameworks that could be consumed industry-wide in support of securing their AI/ML-based services.
    2. Over time, this expertise will build up within engineering groups organically, as it did with traditional security expertise over the past 10 years.
2. Training could be developed which empowers enterprises to deliver on goals such as democratizing AI while mitigating the challenges discussed in this document.
    1. AI-specific security training ensures that engineers are aware of the risks posed __to__ their AI and the resources at their disposal. This material needs to be delivered with current training on protecting customer data.
    2. This could be accomplished without requiring every data scientist to become a security expert – instead, the focus is placed on educating developers on Resilience and Discretion as applied to their AI use cases.
    3. Developers will need to understand the secure “building blocks” of AI services that are reused across their enterprise. There will need to be an emphasis on fault-tolerant design with subsystems, which can be easily turned off (for example, image processors, text parsers).
3. ML Classifiers and their underlying algorithms could be hardened and capable of detecting malicious training data without it contaminating valid training data currently in use or skewing the results.
    1. Techniques such as Reject on Negative Input [5] need researcher cycles to investigate.
    2. This work involves mathematical verification, proof-of-concept in code, and testing against both malicious and benign anomalous data.
    3. Human spot-checking/moderation may be beneficial here,  particularly where statistical anomalies are present.
    4. “Overseer classifiers” could be built to have a more universal understanding of threats across multiple AIs. This vastly improves the security of the system because the attacker can no longer exfiltrate any one particular model.
    5. AIs could be linked together to identify threats in each other’s systems
4. A centralized ML auditing/forensics library could be built that establishes a standard for transparency and trustworthiness of AI.
    1. Query capabilities could also be built for auditing and reconstruction of high business impact decisions by AI.
5. The vernacular in use by adversaries across different cultural groups and social media could be continuously inventoried and analyzed by AI in order to detect and respond to trolling, sarcasm, etc.
    1. AIs need to be resilient in the face of all kinds of vernacular, whether technical, regional, or forum-specific.
    2. This body of knowledge could also be used in content    filtering/labeling/blocking automation to address moderator scalability issues.
    3. This global database of terms could be hosted in development libraries or even exposed via cloud service APIs for reuse by different AIs, ensuring new AIs benefit from the  combined wisdom of older ones.
6. A “Machine Learning Fuzzing Framework” could be created which provides engineers with the ability to inject various types of attacks into test training sets for AI to evaluate.
    1. This could focus on not only text vernacular, but image, voice and gesture data and permutations of those data types.
## Conclusion
The [Asilomar AI Principles](https://futureoflife.org/ai-principles/) illustrate the complexity of delivering on AI in a fashion that consistently benefits humanity. Future AIs need to interact with other AIs to deliver rich, compelling user experiences. That means it simply isn't good enough for Microsoft to “get AI right” from a security perspective – the *world* has to. We need industry alignment and collaboration with greater visibility brought to the issues in this document in a fashion similar to our worldwide push for a Digital Geneva Convention [8]. By addressing the issues presented here, we can begin guiding our customers and industry partners down a path where AI is truly democratized and augments the intelligence of all humanity.
## Bibliography
  [1] *Taleb, Nassim Nicholas (2007),* The Black Swan: The Impact of the Highly Improbable, Random House, [ISBN 978-1400063512](https://en.wikipedia.org/wiki/Special:BookSources/978-1400063512)
  [2] *Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas Ristenpart,* [Stealing Machine Learning Models via Prediction APIs]( )
  [3] *Satya Nadella:* [The Partnership of the Future]()
  [4] *Claburn, Thomas:* [Google's troll-destroying AI can't cope with typos]()
  [5] *Marco Barreno, Blaine Nelson, Anthony D. Joseph, J.D. Tygar:* [The security of machine learning]()
  [6] *Wolchover, Natalie:* [This Artificial Intelligence Pioneer Has a Few Concerns]()
  [7] *Conn, Ariel:* [How Do We Align Artificial Intelligence with Human Values?]()
  [8] *Smith, Brad:* [The need for urgent collective action to keep people safe online: Lessons from last week’s cyberattack](https://blogs.microsoft.com/on-the-issues/2017/05/14/need-urgent-collective-action-keep-people-safe-online-lessons-last-weeks-cyberattack/)
  [9] *Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, Wenchao Zhou:* [Hidden Voice Commands]()
  [10] *Fernanda Viégas, Martin Wattenberg, Daniel Smilkov, James Wexler, Jimbo Wilson, Nikhil Thorat, Charles Nicholson, Google Research:* [Big Picture]()