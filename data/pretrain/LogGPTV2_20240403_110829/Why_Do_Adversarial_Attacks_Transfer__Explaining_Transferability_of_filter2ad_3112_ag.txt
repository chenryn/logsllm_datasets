### Dataset and Classifiers

We selected the six identities with the largest number of images in the dataset. The individual with the most images was designated as the positive class, while all others were considered the negative class. Our dataset consists of 530 positive and 758 negative images. The classifiers and their hyperparameters are the same as those used for MNIST89, with the following exceptions:
- For logistic regression (logisticL), we set \( C = 0.1 \).
- For ridge regression (ridgeH), we set \( \alpha = 1 \).
- For SVM with RBF kernel (SVM-RBFL), we set \( \gamma = 0.001 \) and \( C = 10 \).
- For another SVM with RBF kernel (SVM-RBFH), we set \( \gamma = 0.001 \) and \( C = 1000 \).
- For neural networks (NNL), we set the weight decay to 0.001.

### Experimental Setup and Results

We conducted 10 repetitions, each with 300 samples in the training, validation, and test sets. The results, shown in Figures 19, 20, 21, and 22, confirm the main findings discussed for poisoning attacks on MNIST89. Statistical significance tests are reported in Table 1 (columns seven and eight). In this case, there is no significant distinction between the mean transfer rates of high- and low-complexity surrogates, likely due to the reduced size of the training sets. Figure 23 illustrates examples of perturbed faces against surrogates with different complexities, confirming that larger perturbations are required to attack lower-complexity models.

### Transferability Evaluation Summary

#### Evasion and Poisoning Attacks

1. **Size of Input Gradients**:
   - Low-complexity target classifiers are less vulnerable to evasion and poisoning attacks than high-complexity ones due to smaller input gradients.
   - Generally, nonlinear models are more robust than linear models to both types of attacks.

2. **Gradient Alignment**:
   - Gradient alignment is correlated with transferability.
   - For evasion attacks, low-complexity surrogate classifiers provide stabler gradients that are better aligned with those of the target models. Thus, it is preferable to use strongly-regularized surrogates.
   - For poisoning attacks, gradient alignment improves when the surrogate matches the complexity (regularization) of the target, which can be estimated using techniques from [46].

3. **Variability of the Loss Landscape**:
   - Low-complexity surrogates provide loss landscapes with lower variability, especially for evasion attacks, resulting in better transferability.

#### Recommendations

- **Evasion Attacks**: Decreasing the complexity of the surrogate model by adjusting the hyperparameters of its learning algorithm provides adversarial examples that transfer better to a range of models.
- **Poisoning Attacks**: The best surrogates are generally models with similar levels of regularization as the target model. This is because the poisoning objective function is relatively stable, and gradient alignment becomes a more important factor.

### Implications for Attackers and Defenders

- **Attackers**: Even without knowing the target classifier, low-complexity surrogates have a better chance of transferring to other models. For evasion attacks, choose surrogates with low complexity. For poisoning attacks, acquire information about the target's regularization level and train a surrogate with a similar level.
- **Defenders**: Lower-complexity models tend to be more resilient. However, the bias-variance trade-off must be considered to ensure the model still performs well on the original prediction tasks.

### Related Work and Conclusions

#### Transferability for Evasion Attacks

- Previous studies, such as [3, 13, 14, 21, 26, 32, 33, 42, 43, 47], have explored the transferability of evasion attacks.
- Biggio et al. [3] were the first to consider evasion attacks against surrogate models in a limited-knowledge scenario.
- Goodfellow et al. [14], Tramer et al. [43], and Moosavi et al. [26] observed that different models might learn intersecting decision boundaries.

#### Transferability for Poisoning Attacks

- There is limited work on the transferability of poisoning availability attacks, except for a preliminary investigation in [27].
- Recent work [41] addressed the transferability of targeted poisoning attacks.
- We are the first to study in depth the transferability of poisoning availability attacks.

### Acknowledgements

The authors thank Neil Gong for shepherding the paper and the anonymous reviewers for their constructive feedback. This work was partly supported by the EU H2020 project ALOHA (grant no. 780788) and sponsored by the Combat Capabilities Development Command Army Research Laboratory under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government.