six identities with the largest number of images in the dataset.
We considered the person with most images as positive class,
and all the others as negative. Our dataset consists of 530
positive and 758 negative images. The classiﬁers and their
hyperparameters are the same used for MNIST89, except that
we set: (i) C = 0.1 for logisticL, (ii) α = 1 for ridgeH, (iii)
γ = 0.001,C = 10 for SVM-RBFL, (iv) γ = 0.001,C = 1000
Figure 14: White-box poisoning attacks on MNIST89. Test
error against an increasing fraction of poisoning points.
for SVM-RBFH, and (v) weight decay to 0.001 for NNL. We
run 10 repetitions with 300 samples in each training, valida-
tion and test set. The results are shown in Figs 19, 20, 21
and 22, conﬁrming the main ﬁndings discussed for poisoning
attacks on MNIST89. Statistical tests for signiﬁcance are re-
ported in Table 1 (seventh and eighth columns). In this case,
there is not a signiﬁcant distinction between the mean transfer
rates of high- and low-complexity surrogates, probably due to
the reduced size of the training sets used. Finally, in Fig. 23
we report examples of perturbed faces against surrogates with
different complexities, conﬁrming again that larger perturba-
tions are required to attack lower-complexity models.
USENIX Association
28th USENIX Security Symposium    333
SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.38.15.19.17.09.10.07.11.12.17.05.05.43.20.19.20.09.10.09.14.15.24.06.06.46.17.26.22.10.11.08.12.17.21.06.05.49.19.28.26.10.11.09.13.20.26.06.06.50.12.20.13.25.16.06.08.08.13.05.05.33.14.18.13.13.16.05.09.08.14.05.05.36.18.21.20.09.09.08.12.14.21.06.06.44.21.22.21.09.09.09.15.18.25.07.06.46.20.25.24.09.10.09.13.21.24.06.06.47.20.26.25.09.10.08.14.17.26.06.06targeterror.13.12.07.07.08.08.05.08.05.11.05.05whitebox.98.27.84.57.99.34.49.17.57.36transferrate.14.16.17.18.15.13.15.17.18.18SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.56.21.38.32.10.11.13.13.27.26.06.06.52.32.36.38.10.12.19.20.33.41.08.07.59.24.52.44.11.13.17.16.39.33.06.06.66.29.56.51.11.13.25.19.47.42.07.07.49.13.24.15.38.22.05.08.09.13.05.04.57.18.38.26.19.24.07.11.17.19.05.05.55.24.47.40.09.11.20.16.39.32.07.07.57.32.44.45.10.12.28.23.43.44.09.08.59.28.53.51.10.12.26.18.48.41.07.07.60.30.51.51.10.12.25.19.44.43.07.07targeterror.13.12.07.07.08.08.05.08.05.11.05.05whitebox1.00.56.97.901.00.92.73.34.82.66transferrate.22.26.27.31.17.20.26.30.30.30SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.83.47.87.80.15.18.46.29.72.59.08.08.89.88.90.92.13.20.63.63.80.89.12.12.90.68.91.89.13.20.54.37.79.77.09.08.91.81.94.93.15.22.64.55.83.85.11.11.65.15.49.28.71.56.04.06.18.16.04.03.81.29.74.63.51.68.09.12.52.36.05.04.84.69.87.88.12.17.60.49.80.80.12.11.88.87.90.92.12.18.65.65.79.88.15.14.89.78.91.91.13.18.62.53.82.83.11.11.93.83.94.93.15.21.64.57.83.87.13.11targeterror.13.12.07.07.08.08.05.08.05.11.05.05whitebox1.00.951.001.001.001.00.91.751.00.97transferrate.46.59.53.59.28.40.54.60.57.59SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.55.13.19.12.10.10.06.08.09.13.05.05.49.24.26.27.10.11.12.15.23.27.06.06.68.14.46.23.15.15.09.09.19.15.05.05.70.17.48.31.13.14.12.12.25.21.05.05.38.10.21.12.50.14.05.07.07.11.04.04.53.13.32.17.44.22.06.08.10.13.05.05.49.15.23.17.09.10.09.11.16.16.05.05.44.23.24.27.09.10.11.16.22.27.07.06.64.18.40.26.10.11.12.12.27.21.06.05.49.23.32.33.09.10.10.15.27.30.06.06targeterror.13.12.07.07.08.08.05.08.05.11.05.05whitebox.98.27.84.57.99.34.49.17.57.35transferrate.14.20.20.23.15.19.15.19.21.21SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.83.15.55.28.15.14.12.10.25.17.05.05.65.38.55.55.11.14.29.24.51.46.07.07.84.17.74.47.19.19.21.11.43.22.05.05.83.24.77.60.17.18.27.14.56.33.06.06.43.10.27.15.68.19.05.06.08.12.04.04.65.15.52.29.80.45.06.09.16.15.05.04.69.18.47.33.11.12.21.13.36.23.06.06.63.36.53.54.10.12.31.27.53.47.08.08.85.26.74.57.13.15.33.16.60.33.06.06.68.33.62.61.10.12.33.23.58.50.08.08targeterror.13.12.07.07.08.08.05.08.05.11.05.05whitebox1.00.56.97.901.00.92.73.34.83.65transferrate.24.34.31.35.18.28.24.34.35.35SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.96.25.93.77.34.31.33.13.70.34.05.05.94.90.93.93.16.25.67.64.84.90.11.11.98.31.97.89.61.47.54.15.82.48.06.05.99.741.00.98.48.47.69.40.92.82.08.08.52.10.40.18.93.52.03.03.09.11.03.02.84.19.77.601.00.99.08.08.45.22.04.03.96.47.94.87.16.19.63.31.82.62.10.08.94.88.94.94.14.20.71.67.87.90.15.13.99.66.98.95.25.26.68.39.90.78.09.08.97.88.97.96.13.19.70.63.89.91.13.12targeterror.13.12.07.07.08.08.05.08.05.11.05.05whitebox1.00.951.001.001.001.00.91.751.00.97transferrate.43.62.53.64.25.44.51.62.58.62SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL0.220.170.320.240.030.100.230.170.320.170.130.640.190.360.030.120.320.550.470.510.430.240.530.440.160.250.310.240.420.240.360.590.540.770.180.350.420.470.500.560.040.070.090.120.530.330.070.040.110.050.440.150.460.430.510.610.250.130.310.200.270.440.400.560.080.200.500.440.440.440.290.780.370.680.040.160.410.820.500.780.270.570.380.470.050.140.390.560.520.480.260.760.340.630.030.110.330.730.460.80SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.14.11.17.19.07.12.15.10.17.11.12.53.17.29.01.05.29.49.29.42.16.17.23.27.09.14.20.14.24.17.17.31.28.39.05.12.27.29.34.34.06.01.07.05.25.19.03.00.02.00.13.04.15.11.18.36.09.01.06.02.14.27.18.25.03.07.29.29.27.25.09.50.15.28.00.03.30.58.29.46.16.28.22.30.03.06.26.29.35.34.11.44.18.31.00.02.28.46.34.530123451020Fractionofpoisoningpointsintothetrainingset(%)0.00.10.20.30.40.5TestErrorWhite-boxpoisoningattack(MNIST89)SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL(a)
(b)
(c)
(d)
Figure 15: Evaluation of our metrics for poisoning attacks on MNIST89. See the caption of Fig. 6 for further details.
(a) 5% poisoning
(b) 10% poisoning
(c) 20% poisoning
Figure 16: Black-box (transfer) poisoning attacks on MNIST89. See the caption of Fig. 7 for further details.
SVML
SVMH
SVM-RBFL
SVM-RBFH
Figure 17: Gradient alignment and perturbation correlation
(at 20% poisoning) for poisoning attacks on MNIST89. See
the caption of Fig. 8 for further details.
Figure 18: Poisoning digits crafted against linear and RBF
SVMs. Larger perturbations are required to have signiﬁ-
cant impact on low-complexity classiﬁers (L), while minimal
changes are very effective on high-complexity SVMs (H).
5.3 Summary of Transferability Evaluation
We summarize the results of transferability for evasion and
poisoning attacks below.
(1) Size of input gradients. Low-complexity target classiﬁers
are less vulnerable to evasion and poisoning attacks than high-
complexity target classiﬁers trained with the same learning
algorithm, due to the reduced size of their input gradients. In
general, nonlinear models are more robust than linear models
to both types of attacks.
(2) Gradient alignment. Gradient alignment is correlated
with transferability. Even though it cannot be directly mea-
sured in black-box scenarios, some useful guidelines can
be derived from our analysis. For evasion attacks, low-
complexity surrogate classiﬁers provide stabler gradients
which are better aligned, on average, with those of the tar-
get models; thus, it is generally preferable to use strongly-
regularized surrogates. For poisoning attacks, instead, gradi-
ent alignment tends to improve when the surrogate matches
the complexity (regularization) of the target (which may be
estimated using techniques from [46]).
(3) Variability of the loss landscape. Low-complexity surro-
334    28th USENIX Security Symposium
USENIX Association
100101Sizeofinputgradients(S)0.050.100.150.200.250.30Testerror(5%poisoning)SVMlogisticridgeSVM-RBF10−510−4Variabilityoflosslandscape(V)0.060.080.100.120.140.16Transferrate(20%poisoning)0.20.40.60.81.0Gradientalignment(R)0.20.40.60.8ρ(ˆδ,δ)(20%poisoning)P:0.65,p-val:<1e-8K:0.35,p-val:<1e-40.20.40.60.81.0Gradientalignment(R)0.20.40.60.81.01.21.4Black-towhite-boxerrorratio(10%poisoning)P:0.31,p-val:0.01K:0.21,p-val:0.02SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLRFHRFLNNHNNLCNNSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.30.04.08.06.05.05.05.03.05.05.06.06.05.06.06.06.06.05.05.03.04.05.05.05.06.05.27.05.25.06.09.06.06.03.05.05.07.06.05.16.07.14.09.13.11.03.04.05.05.04.04.03.22.06.20.08.16.11.03.03.05.05.05.05.03.22.06.20.08.16.12.03.04.05.05.04.04.08.25.04.15.06.06.05.19.03.05.05.06.05.04.07.06.06.06.06.06.04.05.05.05.06.05.05targeterror.04.04.04.05.05.05.03.03.05.05.04.04.04whitebox.30.06.24.09.15.12.21.05transferrate.07.05.09.08.09.09.08.05SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLRFHRFLNNHNNLCNNSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.30.05.13.05.08.05.08.03.05.05.07.07.07.07.07.06.06.06.06.04.04.05.05.06.05.05.31.05.29.06.14.08.08.04.05.05.07.07.07.28.10.25.13.22.18.03.04.05.05.04.04.04.28.08.26.11.22.16.04.04.05.05.05.05.04.31.10.28.13.24.19.03.04.05.05.04.04.03.31.05.21.05.08.05.29.04.05.05.06.06.05.10.07.08.07.08.08.06.07.05.05.07.07.06targeterror.04.04.04.05.05.05.03.03.05.05.04.04.04whitebox.33.07.27.15.21.18.28.07transferrate.08.06.10.11.11.12.10.07SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLRFHRFLNNHNNLCNNSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.29.06.19.06.14.07.15.04.05.05.12.10.11.10.09.08.06.08.07.05.05.05.05.06.06.06.33.07.32.08.24.14.13.04.05.05.09.09.11.40.26.37.26.33.31.04.04.05.06.05.04.03.35.14.33.20.30.25.07.04.05.05.07.06.05.41.23.37.24.34.30.04.05.05.06.05.05.04.37.05.29.06.14.07.42.04.05.05.08.07.07.15.11.14.08.13.11.10.13.05.05.10.11.09targeterror.04.04.04.05.05.05.03.03.05.05.04.04.04whitebox.34.09.31.28.32.32.37.14transferrate.11.06.13.17.15.17.13.10SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.36.49.30.24.28.31.36.31.31.50.34.30.32.36.35.33.23.35.48.46.47.49.26.28.16.30.46.92.82.87.29.34.18.33.48.84.87.88.33.38.20.37.50.88.85.93.36.40.23.33.25.28.31.34.49.38.21.33.27.32.34.37.37.41aries in both benign and adversarial dimensions and in that
case adversarial examples transfer better. Tramer et al. have
also performed a detailed study of transferability of model-
agnostic perturbations that depend only on the training data,
noting that adversarial examples crafted against linear models
can transfer to higher-order models. We answer some of the
open questions they posed about factors contributing to attack
transferability. Liu et al. [21] have empirically observed the
gradient alignment between models with transferable adver-
sarial examples. Papernot et al. [32, 33] have observed that
adversarial examples transfer across a range of models, includ-
ing logistic regression, SVMs and neural networks, without
providing a clear explanation of the phenomenon. Prior work
has also investigated the role of input gradients and Jaco-
bians. Some authors have proposed to decrease the magnitude
of input gradients during training to defend against evasion
attacks [22, 35] or improve classiﬁcation accuracy [40, 44].
In [35, 39], the magnitude of input gradients has been identi-
ﬁed as a cause for vulnerability to evasion attacks. A number
of papers have shown that transferability of adversarial ex-
amples is increased by averaging the gradients computed for
ensembles of models [13, 21, 43, 47]. We highlight that we
obtain similar effect by attacking a strongly-regularized sur-
rogate model with a smoother and stabler decision boundary
(resulting in a lower-variance model). The advantage of our
approach is to reduce the computational complexity compared
to attacking classiﬁer ensembles. Through our formalization,
we shed light on the most important factors for transferabil-
ity. In particular, we identify a set of conditions that explain
transferability including the gradient alignment between the
surrogate and targeted models, and the size of the input gradi-
ents of the target model, connected to model complexity. We
demonstrate that adversarial examples crafted against lower-
variance models (e.g., those that are strongly regularized) tend
to transfer better across a range of models.
Transferability for poisoning attacks. There is very little
work on the transferability of poisoning availability attacks,
except for a preliminary investigation in [27]. That work in-
dicates that poisoning examples are transferable among very
simple network architectures (logistic regression, MLP, and
Adaline). Transferability of targeted poisoning attacks has
been addressed recently in [41]. We are the ﬁrst to study in
depth transferability of poisoning availability attacks.
Figure 19: White-box poisoning attacks on LFW. Test error
against an increasing fraction of poisoning points.
gate classiﬁers provide loss landscapes with lower variability
than high-complexity surrogate classiﬁers trained with the
same learning algorithm, especially for evasion attacks. This
results in better transferability.
To summarize, for evasion attacks, decreasing complexity
of the surrogate model by properly adjusting the hyperparam-
eters of its learning algorithm provides adversarial examples
that transfer better to a range of models. For poisoning attacks,
the best surrogates are generally models with similar levels of
regularization as the target model. The reason is that the poi-
soning objective function is relatively stable (i.e., it has low
variance) for most classiﬁers, and gradient alignment between
surrogate and target becomes a more important factor.
Understanding attack transferability has two main impli-
cations. First, even when attackers do not know the target
classiﬁer, our ﬁndings suggest that low-complexity surrogates
have a better chance of transferring to other models. Our rec-
ommendation to performing black-box evasion attacks is to
choose surrogates with low complexity (e.g., by using strong
regularization and reducing model variance). To perform poi-
soning attacks, our recommendation is to acquire additional
information about the level of regularization of the target and
train a surrogate model with a similar level of regularization.
Second, our analysis also provides recommendations to de-
fenders on how to design more robust models against evasion
and poisoning attacks. In particular, lower-complexity models
tend to have more resilience compared to more complex mod-
els. Of course, we need to take into account the bias-variance
trade-off and choose models that still perform relatively well
on the original prediction tasks.
6 Related Work
7 Conclusions
Transferability for evasion attacks. Transferability of eva-
sion attacks has been studied in previous work, e.g., [3, 13,
14, 21, 26, 32, 33, 42, 43, 47]. Biggio et al. [3] have been the
ﬁrst to consider evasion attacks against surrogate models in a
limited-knowledge scenario. Goodfellow et al. [14], Tramer
et al. [43], and Moosavi et al. [26] have made the observation
that different models might learn intersecting decision bound-
We have conducted an analysis of the transferability of eva-
sion and poisoning attacks under a uniﬁed optimization frame-
work. Our theoretical transferability formalization sheds light
on various factors impacting the transfer success rates. In
particular, we have deﬁned three metrics that impact the trans-
ferability of an attack, including the complexity of the tar-
get model, the gradient alignment between the surrogate and
USENIX Association
28th USENIX Security Symposium    335
0123451020Fractionofpoisoningpointsintothetrainingset(%)0.10.20.30.40.5TestErrorWhite-boxpoisoningattack(LFW)SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL(a)
(b)
(c)
(d)
Figure 20: Evaluation of our metrics for poisoning attacks on LFW. See the caption of Fig. 6 for further details.
(a) 5% poisoning
(b) 10% poisoning
(c) 20% poisoning
Figure 21: Black-box (transfer) poisoning attacks on LFW. See the caption of Fig. 7 for further details.
SVML
SVMH
SVM-RBFL
SVM-RBFH
Figure 23: Adversarial examples crafted against linear and
RBF SVMs. Larger perturbations are required to have signiﬁ-
cant impact on low-complexity classiﬁers (L), while minimal
changes are very effective on high-complexity SVMs (H).
Figure 22: Gradient alignment and perturbation correlation
(at 20% poisoning) for poisoning attacks on LFW. See the
caption of Fig. 8 for further details.
Acknowledgements
target models, and the variance of the attacker optimization
objective. The lesson to system designers is to evaluate their
classiﬁers against these criteria and select lower-complexity,
stronger regularized models that tend to provide higher ro-
bustness to both evasion and poisoning. Interesting avenues
for future work include extending our analysis to multi-class
classiﬁcation settings, and considering a range of gray-box
models in which attackers might have additional knowledge
of the machine learning system (as in [41]). Application-
dependent scenarios such as cyber security might provide
additional constraints on threat models and attack scenarios
and could impact transferability in interesting ways.
The authors would like to thank Neil Gong for shepherding
our paper and the anonymous reviewers for their construc-
tive feedback. This work was partly supported by the EU
H2020 project ALOHA, under the European Union’s Horizon
2020 research and innovation programme (grant no.780788).
This research was also sponsored by the Combat Capabili-
ties Development Command Army Research Laboratory and
was accomplished under Cooperative Agreement Number
W911NF-13-2-0045 (ARL Cyber Security CRA). The views