Veh.
Attack Goal Ave. Max
Sim.
Speed
Dev. (std)
Scenario
Highway 3.6 m 65 mph
0.76 m
Hit barrier
(5e-3)
on the left
(29 m/s)
2.7 m 45 mph
0.55 m
(7e-2)
(20 m/s)
Succ.
Succ.
Rate
Time
100%
(100/100) 0.97 s
100%
(100/100) 1.36 s
Lane
Width
Local
Hit truck in the
opposite lane
7 Safety Impact on Real Vehicle
While the simulation-based evaluation above has shown se-
vere safety impacts, it does not simulate other driver assistance
features that are commonly used with ALC at the same time
in real-world driving, for example Lane Departure Warning
(LDW), Adaptive Cruise Control (ACC), Forward Collision
Warning (FCW), and Automatic Emergency Braking (AEB).
This makes it unclear whether the safety damages shown in §6
are still possible when these features are used, especially the
safety-protection ones such as AEB. In this section, we thus
use a real vehicle to more directly understand this.
Evaluation methodology. We install OpenPilot on a Toy-
ota 2019 Camry, in which case OpenPilot provides ALC,
LDW, and ACC, and the Camry’s stock features provide AEB
and FCW [8]. We then use this real-world driving setup to
perform experiments on a rarely-used dead-end road, which
has a double-yellow line in the middle and can only be used
for U-turn. The driver’s view of this road is shown on the left
of Fig. 14. In our miniature-scale experiment in §5.4, the at-
tack realizability from the physically-printed patch to the LD
model output has already been validated under 12 different
lighting conditions. Thus, in this experiment we evaluate the
safety impact by directly injecting an attack trace at the LD
model output level (detailed in Appendix C). This can also
avoid blocking the road for sticking patches to the ground and
cleaning them up, which may affect other vehicles.
To create safety-critical driving scenarios, we place card-
board boxes adjacent to but outside of the current lane as
shown in Fig. 14, which can mimic road barriers and obsta-
cles in opposite direction as in §6 while not causing damages
Figure 13: Victim driving trajectories in the software-in-the-
loop evaluation from 18 different starting positions for high-
way and local road scenarios. Lateral offset values are percent-
ages of the maximum in-lane lateral shifting from lane center;
negative and positive signs mean left and right shifting.
parameters. We follow the methodology in §4.3.4 to obtain
and apply the color mapping in our simulation environment.
The patch size is 5.4m wide and 70m long, and we place it
in the simulation environment by importing the generated
patch image into Unity. The other parameters are the same
as §5.3. To evaluate the attack effectiveness from different
victim approaching angles, for each scenario we evaluate the
same patch from 18 different starting positions, created from
the combinations of 2 longitudinal distances to the patch (50
and 100 m) and 9 lateral offsets (from -95% to 95%) as shown
in Fig. 13. The patch is visible at all these starting positions.
We repeat 10 times for each starting position in each scenario.
Results and video demos. Our attack achieves 100% suc-
cess rates from all 18 starting positions in both highway and
local road scenarios as shown in Table 4. Fig. 13 shows
the averaged vehicle trajectories from each starting posi-
tions. As shown, the vehicle always ﬁrst drives toward the
lane center since the ALC system tries to correct the ini-
tial lateral deviations. After that, the patch starts to take
effect, and causes the vehicle to deviate to the left signiﬁ-
cantly and hit the barrier or truck. We record demo videos at
https://sites.google.com/view/cav-sec/drp-attack/. In the
highway scenario, after the victim hits the concrete barrier, it
bounces away quickly due to the abrupt collision. For local
road, the victim crashes to the front of the truck, causing both
the victim and truck to stop. This suggests that the safety
impacts of our attack can be severe.
USENIX Association
30th USENIX Security Symposium    3319
StudiolightsRoadtextureOﬃcial OpenPilotdashcam devicedriving path24.5° to left0.9° to rightBenignAttackDetectedlane linesDesireddriving path−0.50.00.5020406080100120140Longitudinal Distance (m)0.00.51.0Longitudinal offset:100m to patch50m to patchHighwayLocalHittingthe barrierHittingthe truckLateral offset:-95%-75%-50%-25%0%25%50%75%95%Lateral Distance (m)Figure 14: Safety impact evaluation for our attack on a Toyota
2019 Camry with OpenPilot engaged. Even with other driver
assistance features such as Automatic Emergency Braking
(AEB), our attack still causes collisions in all the 10 trials.
to the vehicle and driver safety. Similar setup is also used in
today’s vehicle crash tests [63–66]. To ensure that we do not
affect other vehicles, we place the cardboard boxes only when
the entry point of this dead-end road has no other driving vehi-
cles in sight, and quickly remove them right after our vehicle
passes them as required by the road code of conduct [67].
Experiment setup. We perform experiments in day time
with and without attack, each 10 times. The driving speed is
kept at ∼28 mph (∼45 km/h), the min speed for engaging
OpenPilot on our Camry. The injected attack trace is from our
simulation environment (§6) at the same driving speed.
Results. Our experiment results show that our attack causes
the vehicle to hit the cardboard boxes in all the 10 attack trials
(100% collision rate), including 5 front and 5 side collisions.
The collision variations are caused by randomness in the dy-
namic vehicle control and the timing differences in OpenPilot
engaging and attack launching. In contrast, in the trials with-
out attack, OpenPilot can always drive correctly and does not
hit or even touch the objects in any of the 10 trials.
These results thus show that driver assistance features such
as LDW, ACC, FCW, and AEB are not able to effectively
prevent the safety damages caused by our attack on ALC.
We examine the attack process and ﬁnd that LDW is not
triggered since it relies on the same lane detection module
as ALC and thus are affected simultaneously by our attack.
ACC does not take any action since it does not detect a front
vehicle to follow and adjust speed in these experiments. FCW
is triggered 5 times out of the 10 collisions, but it is only
a warning and thus cannot prevent the collision by itself.
Moreover, in our experiments FCW is triggered only 0.46
sec before the collision on average, which is far too short
to allow human drivers to react considering the 2.5-second
average driver reaction time to road hazard (§3).
In our Camry model, FCW and AEB are turned on to-
gether as a bundled safety feature [68]. However, while we
have observed some triggering of FCW, we were not able to
observe any triggering of AEB among the 10 attack trials,
leading to a 100% false negative rate. We check the vehi-
cle manual [68] and ﬁnd that this may be because the AEB
feature (called pre-collision braking for Toyota) is used very
conservatively: it is triggered only when the possibility of
a collision is extremely high. This observation is also con-
sistent with the previously-reported high failure rate (60%)
for AEB features on popular car models today [35]. Such
conservative use of AEB can reduce false alarms and thus
avoid mistaken sudden emergency brakes in normal driv-
ing, but also makes it difﬁcult to effectively preventing the
safety damages caused by our attack — in our experiments,
it was not able to prevent any of the 10 collisions. The video
recordings for these real-vehicle experiments are available at
https://sites.google.com/view/cav-sec/drp-attack/.
8 Limitations and Defense Discussion
8.1 Limitations of Our Study
Attack deployability. As evaluated in §5.3, our attack can
achieve a high success rate (93.8%) with only 8 pieces of
quickly-deployable road patches, each requiring only 5-10
sec to deploy for 2 people. To further increase stealthiness,
the attacker can pretend to be road workers like in Fig. 2 to
avoid suspicion, and pick a deployment time when the target
road is the most vacant, e.g., at late night. Nevertheless, lower
deployment efforts is always more preferred for attackers to
reduce risks. One potential direction to further improve this is
to explore other common road surface patterns besides dirty
patterns, which we leave as future work.
Generality evaluation. Although we have shown high at-
tack generality against LD models with different designs
(§5.3), all our evaluations are performed on only one pro-
duction ALC in OpenPilot. Thus, it is still unclear whether
other popular ALC, e.g., Tesla Autopilot and GM Cruise, are
vulnerable to our attack. Unfortunately, to the best of our
knowledge, the OpenPilot ALC is the only production one
that is open sourced. Due to the same reason, we are also un-
able to evaluate the transfer attacks from OpenPilot to these
other popular ALC systems. Nevertheless, since the Open-
Pilot ALC is representative at both design and implementation
levels (§5), we think our current discovery and results can still
generally beneﬁt the understanding of the security of produc-
tion ALC today. Also, since DNNs are generally vulnerable to
adversarial attacks [4,5,26,27,29–31,46], if these other ALC
systems also adopt the state-of-the-art DNN-based design, at
least at design level they are also vulnerable to our attack.
End-to-end evaluation in real world. In this work, we
evaluate our attack against various possible real-world factors
such as lighting conditions, patch viewing angles, victim ap-
proaching angles/distances, printer color accuracy, and camera
sensing capability (§5.3 and §5.4), and also evaluate the safety
impact using software-in-the-loop simulation (§6) and attack
trace injection in a real vehicle (§7). However, these setups
still have a gap to real-world attacks as we did not perform
direct end-to-end attack evaluation with real vehicles in the
physical world. Such a limitation is caused by safety issues
(vehicle-enforced minimum OpenPilot engagement speed at
28 mph, or 45 km/h) and access limits to private testing facili-
ties (for patch placement). In the future, we hope to overcome
this by ﬁnding ways to lower the minimum engagement speed
and obtain access to private testing facilities.
3320    30th USENIX Security Symposium
USENIX Association
Experiment start pointCrashing  pointOutside viewOutside viewCardboard boxes8.2 Defense Discussion
8.2.1 Machine Learning Model Level Defenses
In the recent arms race between adversarial machine learn-
ing attacks and defenses, numerous defense/mitigation tech-
niques have been proposed [69–72]. However, so far none
of them studied LD models. As a best effort to understand
the effectiveness of existing defenses on our attack, we per-
form evaluation on 5 popular defense methods that only re-
quire model input transformation without re-training: JPEG
compression [73], bit-depth reduction [71], adding Gaussian
noise [74], median blurring [71], and autoencoder reforma-
tion [75], since they are directly applicable to LD models.
Descriptions and our conﬁgurations of these methods are in
our extended version [38]. Our experiments use the same
dataset and success metrics as in §5. Meanwhile, we also
evaluate a benign-case success rate, deﬁned as the percentage
of scenarios where the ALC can behave correctly (i.e., not
driving out of lane) when the defense method is applied.
Fig. 15 shows the evaluation results. As shown, for each
defense method we also vary the parameters to explore the
trade-off between attack success rate and benign-case success
rate. As shown, while all methods can effectively decrease
the attack success rate with certain parameter conﬁgurations,
the benign-case success rates are also decreased at the same
time. In particular, when the benign-case success rates are
still kept at 100%, the attack success rates are still 99 to 100%
for all methods. This shows that none of these methods can
effectively defend against our attack without harming ALC
performance in normal driving scenarios. This might be be-
cause these defenses are mainly for disrupting digital-space
human-imperceptible perturbations, and thus are less effective
for physical-world realizable attacks with human-perceptible
(but seemingly-benign) perturbations.
These results show that directly-applicable defense meth-
ods cannot easily defeat our attack. Thus, it is necessary to
explore (1) novel adaptions of more advanced defenses such
as adversarial training to LD, or (2) new defenses speciﬁc to
LD and our problem setting, which we leave as future work.
8.2.2 Sensor/Data Fusion Based Defenses
Besides securing LD models, another direction is to fuse
camera-based lane detection with other independent sen-
sor/data sources such as LiDAR and High Deﬁnition (HD)
map [76]. For example, LiDAR can capture the tiny laser re-
ﬂection differences for lane line markings, and thus is possible
to perform lane detection [77]. However, while LiDARs are
commonly used in high-level (e.g., Level-4) AD systems such
as Google Waymo [78] that provide self-driving taxi/truck,
so far they are not generally used in production low-level
(e.g., Level-2) AD such as ALC, e.g., Tesla, GM Cadillac,
Toyota RAV4, etc. [3, 45, 79]. This is mainly because LiDAR
is quite costly for vehicle models sold to individuals (typically
≥$4,000 each for AD [80]). For example, Elon Musk, the co-
founder of Tesla, claims that LiDARs are “expensive sensors
that are unnecessary (for autonomous vehicles)” [81].
Another possible fusion source is lane information from a
pre-built HD map of the targeted road, which can be used to
cross-check with the run-time detected lane lines to detect our
attack. However, this requires ALC providers to collect and
maintain accurate lane line information for each road, which
can be time consuming, costly, and also hard to scale. To the
best of our knowledge, ALC systems in production Level-2
AD systems today do not use HD maps in general. For in-
stance, Tesla explicitly claims that it does not use HD map for
Autopilot driving since it is a “non-scalable approach” [82].
Nevertheless, considering that Level-4 AD systems today
are able to build and heavily utilize HD maps [83, 84], we
think leveraging HD maps is still a more feasible solution
than requiring production Level-2 vehicle models to install
LiDARs. If such a map can be available, a follow-up research
question is how to effectively detect our attack without raising
too many false alarms, since mismatched lane information
can also occur in benign cases due to (1) vehicle position and
heading angle inaccuracies when localized on the HD map,
e.g., due to sensor noises in GPS and IMU, and (2) normal-
case LD model inaccuracies.
9 Related Work
Autonomous Driving (AD) system security. For AD sys-
tems, there are mainly two types of security research: sensor
security and autonomy software security. For sensor security,
prior works studied spooﬁng/jamming on camera [85–87], Li-
DAR [31,85,88], RADAR [86], ultrasonic [86], and IMU [89].
For autonomy software security, prior works have studied the
security of object detection [4, 5, 88], tracking [90], local-
ization [91], trafﬁc light detection [92], and end-to-end AD
models [41,43]. Our work studies autonomy software security
in production ALC. The only prior effort is from Tencent [6],
but it neither attacks the designed operational domain for
ALC (i.e., roads with lane lines), nor generates perturbations
systematically by addressing the design challenges in §3.3.
Physical-world adversarial attacks. Multiple prior works
have explored image-space adversarial attacks in the physical