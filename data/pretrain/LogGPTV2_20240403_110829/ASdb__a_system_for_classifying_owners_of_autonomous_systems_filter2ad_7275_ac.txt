Standard between 83% and 96%. Furthermore, data source precision
increases dramatically when multiple data sources agree on the
classification for an AS and is always nearly at 100%. Nonetheless,
existing data sources have two primary drawbacks that do not allow
them to be directly used to classify ASes: (1) Data sources are not
accurate at differentiating the types of technology subcategories—
which make up 64% of all ASes (2) Multiple data sources disagree
for 21% of ASes, making it unclear when to trust a particular data
source. In Section 4 we address these deficiencies by developing
additional techniques for classifying ASes.
4 EMPLOYING ARTIFICIAL AND HUMAN
INTELLIGENCE TO CLASSIFY ASES
The data sources we evaluated in Section 3 are promising building
blocks for classifying ASes globally. However, they have two major
shortcomings. First, more than half of ASes belong to technology
companies, which business datasets struggle to identify correctly
(e.g., Zvelo and D&B achieve a 25% and 45% recall when identi-
fying hosting providers). Second, for 21% of ASes, multiple data
sources do not agree. In this section, we show how machine learn-
ing and crowdwork can close the gap by correctly classifying ISPs
and cloud/hosting providers with 98.7% accuracy and arbitrating
disagreements between sources.
4.1 Machine Learning
Despite data sources’ difficulty in differentiating between types of
technology companies, we note that the two largest categories of
Figure 2: Distribution of D&B Confidence Codes—D&B accu-
rately matches fewer than 50% of ASes when returning a confidence
level below 6, but accurately matches at least 80% of ASes when
returning a confidence level at or above 6.
Algorithm
Matching
Target
D&B
Conf. ≥1
Conf. ≥6
Crunchbase Domain
Domain
Correct
Match
Incorrect Missing
Accuracy Matches Matches Matches
83%
89%
100%
95%
Name
Random
70%
Least Common 90%
Most Similar
91%
86%
IPinfo
73%
67%
12%
14%
60%
77%
78%
82%
15%
8%
0%
1%
8%
8%
7%
14%
11%
25%
88%
85%
15%
15%
15%
4%
Table 5: Accuracy of Automated Entity Resolution—The ac-
curacy of entity resolution (i.e., finding the organization within a
data source that corresponds to a given AS) affects overall accuracy
and coverage. Entity resolution often involves choosing an orga-
nization’s corresponding domain. We find that, within the set of
domains present in the RIR records for the AS, the domain whose
homepage title (or, for unreachable sites, the domain itself) is most
similar to the AS name yields greatest accuracy.
Website Identification. D&B, Zvelo, and Crunchbase all, or in
part, rely on being provided with the correct domain of an AS-
owning organization as a unique identifier. While RIRs do not
directly provide the domain of the AS-owning organization, the
correct organization domain is often present within multiple abuse
contact emails for 85% of ASes. We explore two selection heuris-
tics for selecting the correct organization domain: (1) “least com-
mon domain” (i.e., choosing the domain that appears in the fewest
WHOIS organization records), and (2) “most similar domain” (i.e.,
choosing a name with the highest similarity between the website’s
homepage title and the registered AS name). Using “least common
domain” selection eliminates common third-party providers like
Gmail and achieves 90% accuracy. Using “most similar domain”
selection, achieves a 91% accuracy (Table 5).
Dun & Bradstreet. D&B allows searching for companies by
name, address, phone, and domain. In response, their service returns
a single company’s information (e.g., DUNS#, a unique company
identifier) and a 1–10 confidence score. For bulk access, there is no
control over which company is chosen if multiple companies share
the same name or address.
To evaluate the accuracy of D&B’s matching algorithm, we manu-
ally verify the returned DUNS# against our hand-identified matches
708
0.10.20.30.40.50.60.70.80.91.0Matching Accuracy45678910ConfidenceCodeASdb: A System for Classifying Owners of Autonomous Systems
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 3: Classification Pipeline— ASdb uses an ML classifica-
tion pipeline to help identify ISPs and hosting providers (Section 5).
ASdb’s ML classifiers use stochastic gradient descent to classify a
website’s scraped and featurized text.
Truth
Hosting
∼Hosting
Prediction
Hosting
97 (79%)
4 (3%)
∼Hosting
8 (7%)
14 (11%)
Truth
Prediction
∼ISP
6 (5%)
49 (40%)
ISP
ISP
∼ISP
67 (54%)
1 (1%)
Table 6: Classifier Evaluation— We introduce two binary clas-
sifiers trained to identify hosting provider and ISP websites. The
classifiers achieve high test accuracy (90% and 94%, respectively)
and minimize false positives (3% and 1%, respectively).
ASes in our Gold Standard dataset—ISPs and hosting providers—
use common language and have common descriptors in their web-
sites, which allows humans to quickly identify them. Building on
this observation, we hypothesize that an ML classifier will perform
well when specifically investigating these two classes. We intro-
duce a classification pipeline that uses web scraping and machine
learning classifiers to classify technology ASes (Figure 3).
Pipeline Design. Our ML pipeline accepts a single domain as
input and scrapes the text from the root page of the website hosted
at the domain. Since 49% of Gold Standard AS websites are not
in English, we translate scraped text to English using Chrome’s
Google Translate [11]. We find that many pages include service
descriptions on inner pages rather than the homepage. Using the
Gold Standard as a guide, we compile a list of keywords that most
frequently appear in the page titles of internal pages containing
organization information (see Figure 3). We configure our scraper
to visit up to five internal pages whose link titles contain a list of
these keywords.
Once relevant text is collected and translated, our pipeline con-
verts the text into a vector of word counts, and uses a TF IDF (Term
Frequency Inverse Document Frequency) transformer [55]—used
in a majority of text-based recommender systems [28]—to convert
the text into features by computing the relative importance of each
word found in the text. The features are then used as inputs into
two Stochastic Gradient Descent classifiers—often used in text clas-
sification due to their scalability [41]. Each is trained to classify
whether the organization is a hosting provider or ISP.
To train the pipeline, we compiled a labeled training
Evaluation.
set of 225 ASes, of which 150 ASes are random and 75 Ases are
sampled from D&B-labeled hosting providers to provide sufficient
hosting-class balance to train the model (Table 2). We evaluate our
pipeline by using the Gold Standard (Section 3.2) as our test set.
Each AS takes 5–30 seconds to scrape, depending on load time and
number of internal pages. Our model uses 6 CPU cores and 5 seconds
to train, and it requires about 1 second to classify 150 domains.
The ISP and hosting classifiers exhibit a test AUC score of .94
and .80, respectively. The ISP classifier achieves an accuracy of
94% and a 1% false positive rate; the hosting classifier achieves
a 90% accuracy, with a 3% FP rate (Table 6). We investigate the
false positives and find that all are attributed to sites that contain
misleading keywords likely to appear on an ISP or hosting web-
site. For example ASN 133002 is owned by the Indian Institute of
Tropical Meteorology, whose home page discusses using high per-
formance computing and data analytics to study (nature’s) clouds.
Its homepage is dominated by keywords like “cloud,” “computing,”
and “performance”.
The ISP and hosting classifiers are more likely to produce false
negatives than false positives, producing 5% and 7% of false nega-
tives, respectively. 67% of failure cases are due to the initial scraper
not having found an internal page that would likely have provided
better textual information. These internal pages are often either not
linked from the home page or are found in a unique website struc-
ture unsuitable for easy scraping (e.g. much of the text is contained
in images).
Arbitrating Disagreements. While our pipeline helps differen-
tiate between the two most common types of technology companies,
it cannot help arbitrate data source disagreements pertaining to
non-ISP and hosting providers. We argue that implementing a ML
pipeline to resolve data source disagreements amongst all NAICSlite
categories is not the best solution; (1) not enough data is available to
train a classifier to distinguish all 17 layer 1 NAICSlite categories (2)
Zvelo runs an existing production-grade machine learning classifier
whose goal is to differentiate between over 100 business categories;
there is no reason to reinvent the wheel.
In the next section, we investigate how crowdwork can be leveraged
to supplement existing ML solutions.
4.2 Crowdwork
While our machine learning algorithm accurately classifies ISPs
and hosting providers with more than a 90% accuracy, many of the
remaining inaccurate classifications appear “easy” to guess from
the point of view of a trained human—humans can easily inter-
pret images and navigate through websites without relying on a
preset list of keywords. We thus hypothesize and test whether hu-
man crowdworkers are effective at classifying ASes that automated
solutions miss, regardless of organization type.
We detail our experiments and results in Appendix B. Leverag-
ing Amazon Mechanical Turk (MTurk) workers, we explore two
concrete applications of crowdwork to ASdb:
Catching cases where our ML classifiers fail. Our classifiers’
main source of error is false negatives (5% and 7% for ISPs and
hosting – Section 4.1). Human crowdworkers are effective at catch-
ing these errors: for each misclassified AS in our test set, we pay 5
MTurks 30 cents to assign a correct NAICSlite category, and achieve
100% correctness. However, the raw volume of candidate false nega-
tives is too high for this to be cost-effective: we estimate that about
709
service, solution, about, who, do,it, us, our, company, do, network, online, connect,coverage,historyFilter and Follow links with keywordURLScrapeTranslate to EnglishKeywordsTextTextTextCount VectorizerTF ID TransformerSGD Classifier EnsembleHostingClassifierISPClassifierTrue or FalseTrue or FalseIMC ’21, November 2–4, 2021, Virtual Event, USA
Maya Ziv, Liz Izhikevich, Kimberly Ruth, Katherine Izhikevich, and Zakir Durumeric
20.7K registered ASes would need crowdworker review, costing at
least $31,000. This is untenable for our research budget.
Resolving cases where external data sources disagree or have
For Gold Standard ASes with conflicting
incomplete coverage.
labels (Section 3), we pay 3 MTurks 10 cents to choose among the
union of category labels from external data sources; crowdworkers
converge on at least one correct category in 94% of cases. Using
crowdwork to resolve data source disagreements is much more
affordable, costing an estimated $6,000. However, in Section 5.1 we
develop an automated heuristic that resolves conflicting labels with
an accuracy comparable to crowdwork. Adding crowdwork to the
pipeline leads to an overall accuracy improvement of up to 3%.
For ASdb, the accuracy gain from crowdwork is not worth the
cost, and we omit crowdwork from our final system design.
5 ASDB: A SYSTEM TO CLASSIFY ASES
In this section we introduce ASdb, a system that uses existing data
sources (Section 3) and machine learning (Section 4.1) to create and
maintain a dataset of autonomous systems, their owners, and their
industries. We also introduce a new heuristic for classifying ASes
when data sources disagree. ASdb is able to classify the type of
organization for 96% of ASes with 93% accuracy.
5.1 System Architecture
ASdb combines data from our classifiers and business datasets using
a tuned matching algorithm (Figure 4). ASdb is a modular frame-
work that allows for adding new data sources and changes to the
internal matching algorithm.
ASdb’s pipeline begins upon the receipt of WHOIS data for an
AS (e.g., ASN, AS name, organization name, address, abuse contacts).
ASdb checks if the owning organization has previously been classi-
fied (e.g., because another AS belonging to the same organization
was previously classified), and, if so, ASdb returns the cached data.
Otherwise, ASdb begins the classification process by querying data
sources that index by ASN (PeeringDB and IPinfo). If a high confi-
dence match occurs (i.e., only if PeeringDB returns an ISP label),
ASdb translates the existing data source’s categorization system to
NAICSlite, stores, and returns the AS’s classification.
If there isn’t a high confidence match in the first stage, ASdb uses
PeeringDB and IPinfo to help determine the most likely domain
for the organization. Leveraging the domain extraction analysis in
Section 3.3 (Table 5), we use the following algorithm for domain
extraction: (1) pool domains from RIR metadata and ASN-queryable
data source matches; (2) remove all domains that belong to a hand-
curated list of the top 10 email domains (e.g., Gmail); (3) if at least
one provided domain appears in 1 SourceNoRIR raw dataASdb resultsYesClassifyNoClassifierNoYesCheck org existenceOrganization in ASdb?Number of sources?High confidence?NAICSlite translationNAICSlite translationAt least 2 agree?ASdb: A System for Classifying Owners of Autonomous Systems
IMC ’21, November 2–4, 2021, Virtual Event, USA
Category
Business (N=55)
ISP (N=66)
Hosting (N=13)
Education (N=14)
ASdb
0.86
0.90
0.76
0.88
Gold Standard
IPinfo
0.62
0.58
0.30
0.60
PeeringDB ASdb
0.79
0.07
0.36
0.81
0.65
0.13
0.13
0.94
Test Set
IPinfo
0.61
0.61
0.24
0.88
PeeringDB
0.0
0.47
0.0
0.19
Table 7: F1-scores for ASdb, IPinfo, and PeeringDB—ASdb is 2.5–6 times more accurate classifying hosting providers, 1.3–2.5 times for
ISPs, 1.1–5 times for education entities, and 1.3–12 times for business entities in the gold standard and test set than both prior works. ASdb
is 2.5–6 times more accurate classifying hosting providers, 1.3–2.5 times for ISPs, 1.1–5 times for education entities, and 1.3–12 times for
business entities in the gold standard and test set than both prior works.
Gold Standard
Test Set