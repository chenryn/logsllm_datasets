details on the mmWave hardware that enables this capability.
B. Overview of FMCW
mmSpy adopts an FMCW radar that works by emitting chirps2.
The chirp is reflected back by objects in the environment of the
radar and based on the time differences between transmission
and reception of chirps and the doppler shifts, the radar can
estimate the range (distance) of these objects as well as their
velocities.
A chirp and the working principle of FMCW radars is vi-
sualized in Fig. 3. The signal visualized in Fig. 3(a) is a
sinusoidal signal with a linearly increasing frequency which is
a popular type of chirp. The radar modules used in this paper
(TI AWR1843BOOST [8], TI IWR6843ISK [12]) employ such
chirps with linearly increasing frequency. Since the transmitted
As depicted in Fig. 3(b), multiple reflected chirps correspond-
ing to different multipath components in the environment can
be received at the transmitter. By performing an FFT oper-
ation at the receiver (called range FFT), different multipath
components, as well as their ranges can be determined.
The resolution at which distances can be computed using such
a method can be expressed as a function of the chirp sweeping
bandwidth B as follows [83]:
∆R =
c
2B
(2)
where c is the speed of light. In the best case scenario where
the entire working bandwidth of the radar is effectively swept
by a chirp, the above equation predicts a range resolution of
the radar of about 3.75cm. While this is good for a number
of applications such as human activity recognition where
the motion of objects are at larger scales, the resolution is
not sufficient for tracking minute micrometer-level vibrations
needed for capturing the earpiece vibrations during a phone
call. Towards capturing a higher resolution range information,
mmSpy exploits the phase of each reflected signal.
The phase variations can capture minute changes in motion of
the reflector, as per the equation below.
2π∆r
λ
∆ϕ =
(3)
Given the wavelength is in the order of millimeters (≈ 4mm),
and a typical phase noise of 0.057◦, extremely small changes
in range (∆r ≈ 0.63 um) can be tracked by exploiting the
phase variations. mmSpy tracks such variations to eavesdrop
on the contents of a phone call. Fig. 4 depicts extraction of
continuous phase changes from the FMCW radar. A range-
FFT operation will result in multiple peaks corresponding to
reflections in the environment. Among these peaks, the peak
corresponding to reflection from the phone is first isolated
(Section IV-A). By measuring the phase of this FFT peak,
and tracking its variations continuously over time will facilitate
eavesdropping of earpiece vibrations.
C. System Parameters
mmSpy uses a commodity off-the shelf (COTS) radar to
demonstrate radar-based cellphone eavesdropping. While the
phase data can be noisy because of practical constraints,
2chirps are signals with varying frequency, usually increasing frequency or
decreasing frequency
1213
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
3
Earpiece(a) (b) (c)low quality phase data. Keeping the above discussed tradeoffs
in consideration, Appendix B expands on the details on the
chosen system parameters in mmSpy.
III. THREAT MODEL
Fig. 6: Threat model of mmSpy. The attacker transmits mmWave
signals towards a victim’s phone and measures reflections. By analyz-
ing the phases of reflections, the earpiece vibrations can be detected
leading to reconstruction of the audio as well as speech classification.
in mmSpy. A malicious
Fig. 6 depicts the threat model
adversary with an mmWave radar attempts to spy on the audio
contents of a phone call made by a nearby victim. Towards this
end, the attacker shines mmWave signals on the victim’s phone
and captures the reflections. We assume that the captured
reflections come from the back of the phone opposite to the
side of the earpiece that faces the user’s ear. By analyzing the
phases of the reflection, the vibration of the earpiece device of
the phone can be detected. We do not assume that the attacker
has training data for domain adaptation (Section IV) from the
victim’s phone. The attacker generates such training data from
his own phone (which is assumed to be of the same make
and model as the victim’s phone) for developing the speech
recognition ML models (alternatives such as training from a
different phone model is evaluated in Section V). mmSpy’s
ML models are designed to perform audio reconstruction as
well as speech recognition tasks from the noisy vibration
data captured from reflection from the phone. Following is an
example setting where such an attack is feasible. Consider a
setting like a research conference or a social party. An attacker
can eavesdrop on phone calls received by a nearby victim who
might be seated on a chair. Given that mmWave radars can
track vibrations directly from the earpiece, this is particularly
effective in noisy and crowded spaces where the victim might
be less suspicious of eavesdropping. Within the capabilities
of mmSpy, the sensitive information that can be eavesdropped
include credit card information, one time passwords, social
security numbers, etc.
IV. TECHNICAL MODULES
A. Isolation of phone reflection:
As discussed in Section II, a given range-FFT window
will include reflection from the phone as well as multipath
reflections from other objects in the environment. We face
two main challenges in isolating the phone reflection: (i)
Several noisy peaks show up in the range-bin which do not
correspond to multipath reflections. (ii) In addition to the noisy
peaks, there will be peaks corresponding to static reflectors in
the environment such as walls and furniture. Towards better
isolation of signal of interest from the above sources, mmSpy
Fig. 4: A range-FFT will result in multiple peaks corresponding to
objects in the environment. Tracking the phase of the peak due to
phone’s reflection will facilitate eavesdropping.
mmSpy chooses an appropriate set of parameters in the design
space so as to facilitate high quality measurements with a high
sampling rate. We elaborate below on the deliberations.
(a)
(b)
Fig. 5: (a) A series of FMCW chirps are grouped into frames (b) Key
parameters involved in the cycle of a single chirp. Fig. from [16].
We begin by briefly explaining the cycle of operations in the
radar for transmission of chirps. Illustrated in Fig. 5(a), The
TI radar modules transmit a series of chirps continuously. Fig.
5(b) provides a zoomed-in view of a single chirp. Performing
range FFT on each chirp, and isolating the reflection from the
phone (discussed in Section IV-A), provides one phase sample
per chirp – we call this the phase sampling rate. The chirps
are grouped into frames as depicted in the figure. Each frame
enforces a duty cycle of less than 100% so as to provide the
radar enough time to settle down between frames. This leads
to power fluctuations within the hardware due to discontinuous
operation. While a higher duty cycle provides more samples, it
comes with a tradeoff of noisier sensor data. Similarly, there is
an inter-chirp separation time so as to let the hardware switch
from the highest frequency at the end of a chirp back to the
lowest frequency to begin a new chirp. The slope of the chirp
also offers a tradeoff between sampling rate of sensor data and
the hardware noise. While faster slope produces more chirps
per second, the power fluctuations in the hardware can lead to
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
4
1214
tracks consistent peaks across successive frames. Since the
noisy peaks do not consistently appear at a given distance,
they are eliminated. Fig. 7 shows an example where the
phone reflection is consistently tracked over time. In addition
Fig. 7: Tracking of FMCW peaks over time helps eliminate noisy
peaks. The phase data corresponding to the peak from phone’s
reflection is used to eavesdrop the audio.
to phone reflections, reflections from other objects in the
environment can be seen in the figure. However, the phase of
the reflection from the phone would oscillate (due to vibrations
from the audio) whereas phase from other reflectors will not
oscillate. By exploiting this property, mmSpy designs a shallow
convolutional neural network based model to first classify
reflections due to audio vibrations and reflections from static
reflectors such as walls. The classifier provides a high accuracy
of 99.4%, thus facilitating elimination of static reflectors like
walls, furniture etc. Once the peak corresponding to the
reflection from the phone has been identified, the phase of
this peak is used for reconstruction of the earpiece audio as
well as performing speech classification tasks (more details in
Appendix B). Given that the receiver has four receiver anten-
nas, the phase values are averaged across all four antennas to
minimize the Gaussian noise in the extracted audio.
B. Statistical error correction:
The vibrations induced on the body of the phone by the ear-
piece is, by nature, of a very low magnitude. As a consequence
of this, the variation in the phase of the range-FFT peaks is
also very low and noise can supersede the magnitude of the
phase changes that are useful for vibration detection.
The mmWave radio transmits and receives chirps in a discon-
tinuous manner in the form of frames. We observe that at the
beginning of every frame, there is a spike in the magnitude
of the phase values (as shown in Fig. 8 (a)). Additionally,
there is also a continuous noise component that fluctuates more
smoothly with time. In order to eliminate this, we assume that
within each frame of chirps, a smooth enveloping component
exists and we eliminate it.
This is done by estimating the fluctuation within a frame using
a polynomial of degree 2. To avoid the effect of the spikes at
the beginning of each frame, we also eliminate the first two
data points received in a frame. Since the frame size in mmSpy
(a)
(b)
Fig. 8: Statistical error correction: (a) Noisy phase data (b) Phase
data after elimination of noise.
is 128, we effectively work with 126 points within each frame
to offset the fluctuations.
If a frame is represented as (X, Y ), where X is the index of
chirps and Y is the magnitude of the phase extracted from
the chirps, then the smooth fluctuation is estimated using the
following model:
ˆY = a0X 2 + a1X + c
(4)
where X 2 refers to each element of X being raised to a power
of 2. This is similar to a linear regression of order 2 on (X, Y ).
Here, the parameters a0, a1 and c are the estimated parameters
in the polynomial fitting model. Once ˆY has been estimated,
the corrected frame is obtained by subtracting ˆY from Y :
Y ′ = Y − ˆY
(5)
where Y ′ is the corrected frame.
The effect of the error correction is demonstrated in Fig. 8.
The spikes are eliminated, and the signal is zero centered, as
we expect audio signals to be, due to the elimination of the
fluctuating components.
C. Preprocessing and Signal Filtering
We perform a number of preprocessing and filtering techniques
on the extracted raw audio from the radar as outlines below.
Bandpass Filtering: mmSpy tracks sound via material vi-
brations. It is known from literature that materials attenuate
vibrations at higher frequencies [41], [86], [85]. Therefore,
the spectrum measured at higher frequencies mostly consists
of noise which can be eliminated by applying a low pass filter
at 2000 Hz [73]. Also, the fundamental frequency of the voiced
speech of a typical adult male will vary from 85 to 180 Hz,
and that of an adult female from 165 to 255 Hz [87], [34].
Thus, we apply a high pass filter at 80 Hz to eliminate the
DC offsets and low frequency noise without affecting speech
recognition.
Spectral Subtraction: We perform background noise elimi-
nation using spectral subtraction techniques popular in speech
processing [38]. At a high level, the average signal spectrum
and the average noise spectrum are first estimated and then
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
5
1215
subtracted from each other, which is shown to eliminate
additive stationary noise [88], [85], [78].
Fig. 9 depicts an audio signal before and after preprocessing
techniques. Evidently, the voice part of the signal has been
enhanced in comparison to hardware noise.
(a)
(b)
(c)
Fig. 9: Preprocessing and noise subtraction (the word spoken is
”two”) – (a) Raw signal (b) After bandpass filtering (c) After spectral
subtraction.
D. Synthetic training data generation:
Speech processing algorithms have to be robust to speaker
patterns, dialects, gender, etc. However, developing robust
models require large scale training data accumulated over
years across diverse users. While large scale datasets are
available for speech and vision domains, the unfortunate lack
of availability of similar radar datasets makes it challenging
to develop robust models. Towards minimizing the overhead
of training data collection, mmSpy generates synthetic training
data to bootstrap the training of ML models.
The synthetic data is created from the existing datasets
(AudioMNIST [36] and Speech Commands [91]) using two
main operations: scaling and noising. In order to scale the
existing datasets, radar samples were collected and moments
of speech were identified. Whenever words are being spoken,
the phase magnitudes vary in the range of -0.024 and 0.024
(after statistical error correction). We assume that the noise
in the sensor data is normally distributed and estimate the
parameters of the noise distribution from samples where no
sounds are played on the phone. The mean and the standard de-
viation of the noise distribution are 0 and 0.0035 respectively.
Thus, the audio datasets were scaled in the range of [-0.024,
0.024] and random noise sequences with a normal distribution
N (0, 0.0035) were added to create synthetic training data.
Although the synthetic data attempts to match the distribution
of real radar data well at a high level, there will still be
residual differences in distribution. Towards eliminating such
differences, mmSpy later performs domain adaptation based on
a small set of real training data, thus achieving a sweet-spot
in the trade-off between training data overhead and robustness
of the ML model.
E. Audio Reconstruction:
Towards reconstruction of the original audio from the noisy
radar data, we design a redundant convolutional encoder
decoder (RCED) architecture [79] as illustrated in Fig. 10.
The audio and radar samples are downsampled to a sampling
rate of 8kHz – this sampling rate is adequate to capture audible
frequencies from human speech. The input Xi to the network
are the mel-spectrograms of 1-second audio samples. The
dimensions of the spectrograms are 128×81. We refer to each
column of this input matrix as a segment. Towards generating
an enhanced segment at time t, the network accepts input
segments from times t to t− 7, thus accepting an input of size
128× 8, and producing an output of size 128× 1. This allows
the network to exploit temporal locality in performing the
enhancement. Also, the network consists of skip connections
that help in training and convergence [46], [61], [29]. The
network outputs one segment at a time, and after passing
through the entire input,
it produces an output matrix O
of size 128 × 73 3. After performing the enhancement, we
exploit masking techniques [51] as an alternative enhancement
option. While the enhanced output O eliminates noise,
it
may distort the voiced segments of the audio as well. The
masking can potentially help eliminate such distortions. After
performing the enhancement with RCED network, the output
spectrogram O is divided into 8 evenly spaced frequency
ranges in the mel scale. Within each frequency range, a
threshold is decided adaptively based on Otsu’s method [67].
Based on the thresholding, a 0/1 binary mask is computed from
spectrogram O. The 0/1 masked values are further smoothed
based on Gaussian blurring [72] with a kernel of size 3 × 3
resulting in a masking matrix M with values between 0 and
1. The enhanced output O′ is then calculated as O′ = I ⊙ M.
We compare the performance of both O, and O′ in Section V.
Fig. 10: Architecture of audio reconstruction network.
Loss Functions and Optimization: The loss function used
to train the audio reconstruction model is the mean-squared
error (MSE) loss function. If Xi and Zi are the ith input and
output spectrograms respectively, the MSE is defined as:
MSE =
1
n
||Xi − Zi||2
2
(6)
n(cid:88)
i=1
Since the inputs and outputs are spectrograms, the MSE loss
function is used to make the input and output spectrograms
resemble each other as much as possible.
The Adam optimizer was used to minimize the loss function
with a learning rate of 0.001 and L2 regularization loss with
weight decay of 10−5.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
6
3Due to the overlapping windows used in creating spectrograms, this creates
a truncation of approximately 5.48% of the final time domain output with
respect to the size of the input, which we ignore in this paper.
1216
-100-80-60-40-20Hyperparameter Selection: The hyperparameters were the