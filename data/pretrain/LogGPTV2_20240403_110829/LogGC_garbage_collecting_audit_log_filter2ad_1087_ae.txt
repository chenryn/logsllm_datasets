# Garbage Collection Performance
| GC Time (s) |
|-------------|
| 35.97       |
| 22.09       |
| 26.71       |
| 47.47       |
| 23.65       |
| 104.88      |
| 101.21      |
| 101.09      |
| 102.89      |
| 98.21       |
| 3.76        |
| 2.65        |
| 2.82        |
| 4.52        |
| 3.17        |
| 10.62       |
| 9.45        |
| 9.48        |
| 10.13       |
| 9.16        |

**Table 6: Garbage Collection Performance**

# Runtime Overhead for Data Unit Instrumentation
| Benchmarks | Response Time (ms) | Overhead (%) |
|------------|--------------------|--------------|
| RUBiS      | 4,769.6            | 1.17%        |
| SysBench   | 44.68              | 2.04%        |

- **Without Instrumentation**: 4,825.6 ms (RUBiS), 45.58 ms (SysBench)

**Table 7: Runtime Overhead for Data Unit Instrumentation**

## 6.3 Attack Investigation
In this section, we demonstrate that the reduced audit logs are equally informative in forensic analysis through several case studies. We adopt eight attack scenarios previously used to evaluate related approaches [12, 20]. For each attack scenario, we generate causal graphs from both the original log and the reduced logs, starting from an attack symptom event (for backward analysis) or the root attack event (for forward analysis). We then compare the two graphs to verify if they contain all causal relations pertinent to the attack and further, if they carry any unrelated ones. To avoid oversized graphs, the original logs are generated with execution partitioning (i.e., BEEP) [20]. In other words, we are comparing graphs from LogGC with graphs by BEEP (only).

### Table 8: Attack Scenarios
| Scenarios               | # of Audit Log Entries | After GC         | Total     | Forward Match | Backward Match |
|-------------------------|------------------------|------------------|-----------|---------------|----------------|
| 1. Trojan attack [20]   | 356,798                | 9,614 (2.69%)    | -         | Match         | Match          |
| 2. Attack ramification [20] | 690,231             | 50,271 (7.30%)   | -         | Match         | Match          |
| 3. Information theft [20] | 572,712              | 178,213 (31.12%) | -         | Better        | Match          |
| 4. Illegal storage [12]  | 212,321                | 59,236 (27.90%)  | -         | Better        | Match          |
| 5. Content destruction [12] | 328,297            | 37,282 (11.36%)  | -         | Better        | Match          |
| 6. Unhappy student [12]  | 572,385                | 45,821 (8.01%)   | -         | Better        | Match          |
| 7. Compromised database [12] | 102,415            | 4,657 (4.55%)    | -         | Better        | Match          |
| 8. Weak password [12]    | 182,346                | 43,214 (23.70%)  | -         | Better        | Match          |

- **Match**: Identical causal graphs with and without LogGC
- **Better**: Smaller and more precise graph with LogGC

### Case Studies
1. **Trojan Attack**: The victim received a phishing email containing a malicious URL. The user clicked it, and Firefox visited the malicious page. The user downloaded and executed a backdoor trojan. The administrator later detected the backdoor program and started forensic analysis.

2. **Attack Ramification**: The attacker exploited a vulnerability in Proftpd to acquire a root shell and installed a backdoor. He modified `.bash_history` to remove the footprint. The user later noticed the backdoor process. The causal graphs (from both the original and the reduced logs) precisely capture that the attacker modified `.bash_history`.

3. **Information Theft**: An insider used `vim` to open three classified files and two HTML files. He copied some classified information from `secret_1` to `secret.html` and modified `index.html`. An external attacker connected to the web server and read `secret.html`. LogGC does not garbage collect the `httpd` units that sent the modified HTML files, so the reduced log contains the event that sent `secret.html` to the attacker. The forward causal graphs from the secret files clearly track down the attack, including the attacker's IP.

4. **Illegal Storage**: The attacker launched a `pwck` local escalation exploit to get a root shell and then modified `/etc/passwd` and `/etc/shadow` to create an account. The attacker created directories and downloaded illegal files, including a trojaned `ls` to hide the illegal files. A victim user used the trojaned `ls` and created two files in his home directory. Later, the attacker logged into the system using the created account and downloaded more illegal files. The administrator later detected the trojaned `ls` and started forensic analysis. There are two forward causal graphs in this case. The first one starting from the trojaned `ls` identifies the victim user and the files generated, which may be compromised. The second one from `/etc/passwd` is larger because it includes all SSH login activities. The attacker’s login and download activities are captured, but events from normal users are also included.

5. **Content Destruction**: The attacker exploited a `sendmail` vulnerability to get a root shell and deleted files from other users' directories. The victim detected some of his files were missing and restored them from backup storage.

6. **Unhappy Student**: The attacker launched a remote attack on the FTP server and modified some file permissions to globally writable. Two other malicious users modified a victim’s files and copied them into their own directories. The victim later detected that his files were globally writable.

7. **Compromised Database**: The attacker launched a remote attack on the Samba daemon to get a root shell and created a backdoor. The attacker logged in later through the backdoor and issued SQL queries to remove some transactions from the local database. Later, the user accessed the database and detected problems. We performed backward analysis from the backdoor. The causal graphs from both the reduced log and the original log are identical. However, data units prove to be very effective for forward analysis, allowing us to precisely pinpoint tuples affected by the attacker. In contrast, the graph from the original log indicates that the entire table may be affected by the attacker. Figure 8 compares the forward causal graphs for this case with and without data unit support. The graph by LogGC (with data unit support) precisely detects the tuple modified by the attacker, whereas the graph without data unit support indicates that the entire `bid` table was affected and shows three other users who accessed the `bid` table even though they did not access the modified tuple.

8. **Weak Password**: The administrator used a photo gallery to upload digital pictures and created an account with a weak password for the user. Before the user changed the password, the attacker grabbed the password using a dictionary attack. The attacker logged into the gallery program, uploaded some pictures, and viewed the user’s album. The user later detected the attacker’s pictures. The graphs by LogGC are precise in revealing the attack: The backward graph includes 47 nodes, and the forward graph contains 61 nodes. Both are verified to carry the precise set of forensic information items related to this attack. In contrast, the backward graph from the original log contains 326 nodes, and the forward graph has 517 nodes. Most of them are introduced by false database dependencies.

## 7. Related Work
### Classic Garbage Collection
There is a large body of work on garbage collecting in-memory objects [4, 9, 10, 14]. The nature of our problem has some similarities to classic garbage collection. However, we cannot simply use classic GC for provenance log reduction for the following reasons:
1. We have to operate on audit logs with a flat structure instead of memory reference graphs in classic GC.
2. Classic GC only needs to identify object reachability in one direction; we have to consider both forward and backward directions to cater for attack forensic needs.
3. Classic GC can make use of very precise byte-level reference information to determine reachability; we only have coarse-grain system-level dependences in audit logs. As shown in our experiments, a basic reachability-based GC algorithm can hardly work on audit logs.

### System-level Provenance
In recent years, significant progress has been made in tracking system-level dependences for attack forensics using audit logs [3, 8, 11, 13, 16, 17, 18, 20, 22, 23, 24]. These techniques use audit logs to identify the root cause of an attack and perform forward tracking to reveal the attack’s impacts. LogGC complements these techniques by garbage collecting audit logs to substantially reduce their size without affecting forensic analysis accuracy. In particular, while we leverage the execution partitioning technique in BEEP [20], LogGC and BEEP differ in that:
1. LogGC focuses on garbage-collecting audit logs, whereas BEEP does not.
2. BEEP cannot handle dependences with database engines, which are critical to reducing server audit logs and generating precise causal graphs.

### System-level Replay Techniques
System-level replay techniques have been proposed to roll back a victim system after an attack [6, 12, 15]. They record system-wide execution events so that the whole system can be replayed from a checkpoint. LogGC may potentially complement these techniques by garbage-collecting unnecessary events from execution logs without affecting replay fidelity.

### Database Provenance
There exists a line of research in providing fine-grain data lineage for database systems. Trio [2] and SubZero [25] introduce new features to manage fine-grain lineage along with data. They track provenance by transforming/reversing queries. As such, they need to know the queries beforehand, without instrumenting the database engine.

### Log Compression
Some existing techniques involve compressing provenance logs via a web graph compression technique [26] or detecting common sub-trees and then compressing them [7]. As log compression techniques, they are orthogonal to LogGC. We envision future integration of LogGC and these techniques.

## 8. Conclusion
We present LogGC, a GC-enabled audit logging system towards practical computer attack forensics. Audit log garbage collection poses new challenges beyond traditional memory GC techniques. It should support both forward and backward forensic analysis, whereas traditional memory GC only needs to support one direction of correlation. Moreover, the granularity of existing audit logging approaches is insufficient, especially for long-running programs and database servers. We propose a technique that partitions a database file into data units so that dependences can be captured at the tuple level. Together with our earlier solution of partitioning a process into execution units, LogGC greatly reduces false dependences that prevent effective GC. LogGC entails instrumenting user programs only at a few locations, incurring low overhead (< 2.04%). Without any compression, LogGC can reduce audit log size by a factor of 14 for user systems and 37 for server systems. The reduced audit logs preserve all necessary information for full forensic analysis.

## 9. Acknowledgment
We would like to thank the anonymous reviewers and our shepherd, Wenliang Du, for their insightful comments. This research has been supported in part by DARPA under Contract 12011593 and by NSF under awards 0917007 and 0845870. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of DARPA and NSF.

## 10. References
[1] http://sysbench.sourceforge.net.
[2] P. Agrawal, O. Benjelloun, A. D. Sarma, C. Hayworth, S. Nabar, T. Sugihara, and J. Widom. Trio: a system for data, uncertainty, and lineage. In Proceedings of the 32nd international conference on Very large data bases, VLDB 2006.
[3] P. Ammann, S. Jajodia, and P. Liu. Recovery from malicious transactions. IEEE Transaction on Knowledge and Data Engineering, September, 2002.
[4] A. W. Appel. Simple generational garbage collection and fast allocation. Software Practice and Experience, 1989.
[5] E. Cecchet, J. Marguerite, and W. Zwaenepoel. Performance and scalability of EJB applications. In Proceedings of the 17th ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications, OOPSLA 2002.
[6] R. Chandra, T. Kim, M. Shah, N. Narula, and N. Zeldovich. Intrusion recovery for database-backed web applications. In Proceedings of the 23rd ACM Symposium on Operating Systems Principles, SOSP 2011.
[7] A. P. Chapman, H. V. Jagadish, and P. Ramanan. Efficient provenance storage. In Proceedings of the ACM SIGMOD international conference on Management of data, SIGMOD 2008.
[8] J. Chow, B. Pfaff, T. Garfinkel, K. Christopher, and M. Rosenblum. Understanding data lifetime via whole system simulation. In Proceedings of the 13th conference on USENIX Security Symposium, SSYM 2004.
[9] L. P. Deutsch and D. G. Bobrow. An efficient, incremental, automatic garbage collector. Communications of the ACM, 1976.
[10] E. W. Dijkstra, L. Lamport, A. J. Martin, C. S. Scholten, and E. F. M. Steffens. On-the-fly garbage collection: an exercise in cooperation. Communications of the ACM, 1978.
[11] A. Goel, W.-c. Feng, W.-c. Feng, and D. Maier. Automatic high-performance reconstruction and recovery. Computer Networks, April, 2007.
[12] A. Goel, K. Po, K. Farhadi, Z. Li, and E. de Lara. The Taser intrusion recovery system. In Proceedings of the 22nd ACM symposium on Operating systems principles, SOSP 2005.
[13] X. Jiang, A. Walters, D. Xu, E. H. Spafford, F. Buchholz, and Y.-M. Wang. Provenance-aware tracing of worm break-in and contaminations: A process coloring approach. In Proceedings of the 26th IEEE International Conference on Distributed Computing Systems, ICDCS 2006.
[14] R. Jones and R. Lins. Garbage collection: algorithms for automatic dynamic memory management. John Wiley & Sons, Inc., 1996.
[15] T. Kim, X. Wang, N. Zeldovich, and M. F. Kaashoek. Intrusion recovery using selective re-execution. In Proceedings of the 9th USENIX conference on Operating systems design and implementation, OSDI 2010.
[16] S. T. King and P. M. Chen. Backtracking intrusions. In Proceedings of the 19th ACM symposium on Operating systems principles, SOSP 2003.
[17] S. T. King, Z. M. Mao, D. G. Lucchetti, and P. M. Chen. Enriching intrusion alerts through multi-host causality. In Proceedings of the 13th Annual Network and Distributed System Security Symposium, NDSS 2005.
[18] S. Krishnan, K. Z. Snow, and F. Monrose. Trail of bytes: efficient support for forensic analysis. In Proceedings of the 17th ACM conference on Computer and communications security, CCS 2010.
[19] M. Laurenzano, M. Tikir, L. Carrington, and A. Snavely. Pebil: Efficient static binary instrumentation for Linux. In Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2010.
[20] K. H. Lee, X. Zhang, and D. Xu. High accuracy attack provenance via binary-based execution partition. In Proceedings of the 20th Annual Network and Distributed System Security Symposium, NDSS 2013.
[21] C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, V. Reddi, and K. Hazelwood. Pin: building customized program analysis tools with dynamic instrumentation. In Proceedings of the ACM SIGPLAN conference on Programming language design and implementation, PLDI 2005.
[22] J. Newsome and D. X. Song. Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software. In Proceedings of the 13th Annual Network and Distributed System Security Symposium, NDSS 2005.
[23] S. Sitaraman and S. Venkatesan. Forensic analysis of file system intrusions using improved backtracking. In Proceedings of the 3rd IEEE International Workshop on Information Assurance, IWIA 2005.
[24] D. Tariq, M. Ali, and A. Gehani. Towards automated collection of application-level data provenance. In Proceedings of the 4th USENIX conference on Theory and Practice of Provenance, TaPP 2012.
[25] E. Wu, S. Madden, and M. Stonebraker. Subzero: a fine-grained lineage system for scientific databases. In Proceedings of the 29th IEEE international conference on Data Engineering, ICDE 2013.
[26] Y. Xie, D. Feng, Z. Tan, L. Chen, K.-K. Muniswamy-Reddy, Y. Li, and D. D. Long. A hybrid approach for efficient provenance storage. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM 2012.