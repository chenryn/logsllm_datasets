GC time (s)
35.97
22.09
26.71
47.47
23.65
104.88
101.21
101.09
102.89
98.21
3.76
2.65
2.82
4.52
3.17
10.62
9.45
9.48
10.13
9.16
Table 6: Garbage collection performance.
Benchmarks
Response time (ms)
Overhead (%)
Without Instrumentation With Instrumentation
RUBiS
SysBench
4,769.6
44.68
4,825.6
45.58
1.17%
2.04%
Table 7: Runtime overhead for data unit instrumentation.
6.3 Attack Investigation
In this section, we show that the reduced audit logs are equally
informative in forensic analysis through a number of case stud-
ies. We adopt eight attack scenarios previously used to evaluate
Scenarios
# of audit log entries
After GC
Total
1. Trojan attack [20]
2. Attack ramiﬁcation [20]
3. Information theft [20]
4. Illegal storage [12]
5. Content destruction [12]
6. Unhappy student [12]
7. Compromised database [12]
8. Weak password [12]
356,798
690,231
572,712
212,321
328,297
572,385
102,415
182,346
9,614 (2.69%)
50,271 (7.30%)
178,213 (31.12%)
59,236 (27.90%)
37,282 (11.36%)
45,821 (8.01%)
4,657 (4.55%)
43,214 (23.70%)
-
For-
Back-
ward
ward
Match Match
Match Match
Match
Match Match
Match
Match Match
Better
Match
Better
Better
-
Table 8: Attack scenarios (“Match” means identical causal graphs
with and without LogGC; “Better” means a smaller and precise graph
with LogGC.)
related approaches [12, 20]. For each attack scenario, we generate
the causal graphs from both the original log and the reduced logs,
starting from an attack symptom event (for backward analysis) or
the root attack event (for forward analysis). Then we compare the
two graphs to verify if they contain all causal relations pertinent to
the attack and further, if they carry any unrelated ones. To avoid
having over-sized graphs, the original logs are generated with exe-
cution partitioning (i.e., BEEP) [20]. In other words, we are com-
paring graphs from LogGC with graphs by BEEP (only).
Table 8 summarizes the results. The second and third columns
show the number of audit log entries in the original and reduced
logs, respectively. The last two columns show the results of causal
graph comparison in both backward and forward analysis. The re-
sults show that all causal graphs by LogGC capture the minimal
and precise attack paths and the right set of attack ramiﬁcations;
whereas the graphs from the original logs either are identical to
their counterparts or contain extra (and unrelated) causal relations.
In the ﬁrst scenario (“trojan attack”), the victim received a phish-
ing email that contained a malicious URL. The user clicked it and
firefox visited the malicious page. The user downloaded a back-
door trojan and executed it. The administrator later detected the
backdoor program and started forensic analysis. In the second sce-
nario, the attacker exploited a vulnerability of Proftpd to acquire
a root shell and installed a backdoor. Then he modiﬁed .bash_history
to remove the footprint. The user later noticed the backdoor pro-
cess. The causal graphs (from both the original and the reduced
logs) precisely capture that the attacker modiﬁed .bash_history.
The third scenario involves information theft. An insider used vim
to open three classiﬁed ﬁles and two other html ﬁles. He copied
some classiﬁed information from secret_1 to secret.html and also
modiﬁed index.html. Then an external attacker connected to the
web server and read secret.html. LogGC does not garbage col-
lect the httpd units that sent the modiﬁed html ﬁles and thus
the reduced log contains the event that sent secret.html to the at-
tacker. The forward casual graphs from the secret ﬁles clearly
tracks down the attack, including the attacker’s IP through which he
retrieved the secret information. In the fourth scenario, the attacker
launched a pwck local escalation exploit to get a root shell and
then modiﬁed /etc/passwd and /etc/shadow to create an account.
Then the attacker created directories and downloaded illegal ﬁles,
including a trojaned ls to hide the illegal ﬁles. A victim user used
the trojaned ls and created two ﬁles in his home directory. Later,
the attacker logged into the system using the created account and
downloaded more illegal ﬁles. The administrator later detected the
trojaned ls and started forensic analysis. There are two forward
causal graphs in this case. The ﬁrst one starting from the trojaned
ls identiﬁes the victim user and the ﬁles generated, which may
be compromised. The second one from /etc/passwd is larger be-
cause it includes all ssh login activities. The attacker’s login and
download activities are captured but events from normal users are
1014backdoor
mysql
localhost:3306
mysqld
bids:tuple10
mysql
backdoor
mysqld
x.x.x.1:3306
localhost:3306
bid.MYI
bid.MYD
mysqld
mysqld
mysqld
item.MYI
item.MYD
x.x.x.3:3306
mysqld
x.x.x.2:3306
x.x.x.4:3306
(a) Graph by LogGC
(b) Graph without data unit support
Figure 8: Causal graph comparison for attack scenario 7.
also included. In the ﬁfth scenario (“content destruction”), the at-
tacker exploited a sendmail vulnerability to get a root shell and
he deleted ﬁles from other users’ directories. The victim detected
some of his ﬁles were missing and restored them from backup stor-
age. In the sixth scenario, the attacker launched a remote attack
on the ftp server and modiﬁed some ﬁle permissions to globally
writable. Two other malicious users modiﬁed a victim’s ﬁles and
copied them into their own directories. The victim later detected
that his ﬁles were globally writable.
In the next two scenarios, mysql played an important role on
the attack paths. LogGC hence produces better causal graphs than
those derived from the original logs. In the seventh scenario, the at-
tacker launched a remote attack on the Samba daemon to get a root
shell and created a backdoor. The attacker logged in later through
the backdoor and issued SQL queries to remove some transaction
from the local database. Later the user accessed the database and
detected problems. We performed backward analysis from the back-
door. The causal graphs from both the reduced log and the original
log are identical. However, data units prove to be very effective
for forward analysis such that we can precisely pinpoint tuples af-
fected by the attacker. In contrast, the graph from the original log
indicates that the entire table may be affected by the attacker. In
Fig. 8, we compare the forward causal graphs for this case with
and without data unit support. The graph by LogGC (with data
unit support) precisely detects the tuple modiﬁed by the attacker,
whereas the graph without data unit support indicates that the en-
tire bid table was affected and it also shows three other users who
accessed table bid even though they did not access the modiﬁed tu-
ple. The user from x.x.x.1 modiﬁed a tuple in table item after
he read bid hence table item is considered affected. After that,
another user from x.x.x.4 accessed table item and thus is also
included in the graph. As a result, the graph by LogGC contains 5
nodes which precisely capture the attack ramiﬁcations, whereas the
graph without data unit support has 16 nodes including 10 unrelated
objects and users.
In the last scenario, the administrator used photo-gallery
to upload digital pictures and created an account with a weak pass-
word for the user. Before the user changed the password, the at-
tacker grabbed the password using dictionary attack. The attacker
logged into the gallery program, uploaded some pictures, and viewed
the user’s album. The user later detected the attacker’s pictures.
The graphs by LogGC are precise in revealing the attack: The back-
ward graph includes 47 nodes and the forward graph contains 61
nodes. Both are veriﬁed to carry the precise set of forensic infor-
mation items related to this attack. In contrast, the backward graph
from the original log contains 326 nodes and the forward graph has
517 nodes. Most of them are introduced by false database depen-
dences.
7. RELATED WORK
Classic Garbage Collection. There is a large body of work on
garbage collecting in-memory objects [4, 9, 10, 14]. The nature
of our problem has some similarity to classic garbage collection.
However we cannot simply use classic GC for provenance log re-
duction for the following reasons. First, we have to operate on audit
logs with a ﬂat structure instead of on memory reference graphs in
classic GC. Second, classic GC only needs to identify object reach-
ability in one direction; but we have to consider both forward and
backward directions to cater for attack forensic needs. Third, clas-
sic GC can make use of very precise byte level reference informa-
tion to determine reachability; whereas we only have the coarse-
grain system level dependences in audit logs. As shown in our
experiments, a basic reachability-based GC algorithm can hardly
work on audit logs.
System-level Provenance. In recent years, signiﬁcant progress has
been made on tracking system-level dependences for attack foren-
sics using audit logs [3, 8, 11, 13, 16, 17, 18, 20, 22, 23, 24]. These
techniques use audit logs to identify root cause of an attack and
perform forward tracking to reveal the attack’s impacts. LogGC
complements these techniques by garbage collecting audit logs to
substantially reduce their size without affecting forensic analysis
accuracy. In particular, while we leverage the execution partition-
ing technique in BEEP [20], LogGC and BEEP differ in that: (1)
LogGC focuses on garbage-collecting audit logs whereas BEEP
does not; (2) BEEP cannot handle dependences with database en-
gines, which are critical to reducing server audit logs and generat-
ing precise causal graphs.
System-level replay techniques have been proposed to roll back
a victim system after an attack [6, 12, 15]. They record system-
wide execution events so that the whole system can be replayed
from a checkpoint. LogGC may potentially complement these tech-
niques by garbage-collecting unnecessary events from execution
logs without affecting replay ﬁdelity.
Database Provenance. There exists a line of research in pro-
viding ﬁne-grain data lineage for database systems. Trio [2] and
SubZero [25] introduce new features to manage ﬁne-grain lineage
along with data. They track provenance by transforming/reversing
queries. As such, they need to know the queries before hand, with-
out instrumenting the database engine.
Log Compression. Some existing techniques involve compressing
provenance logs via a web graph compression technique [26] or
detecting common sub-trees and then compressing them [7]. As
log compression techniques, they are orthogonal to LogGC . We
envision future integration of LogGC and these techniques.
8. CONCLUSION
We present LogGC, a GC-enabled audit logging system towards
practical computer attack forensics. Audit log garbage collection
poses new challenges beyond traditional memory GC techniques. It
should support both forward and backward forensic analysis whereas
traditional memory GC only needs to support one direction of cor-
relation. Moreover, the granularity of existing audit logging ap-
proach is insufﬁcient, especially for long running programs and
database servers. We propose a technique that partitions a database
ﬁle into data units so that dependences can be captured at tuple
level. Together with our earlier solution of partitioning a process
into execution units, LogGC greatly reduces false dependences that
prevent effective GC. LogGC entails instrumenting user programs
only at a few locations, incurring low overhead (< 2.04%). With-
out any compression, LogGC can reduce audit log size by a factor
1015of 14 for user systems and 37 for server systems. The reduced audit
logs preserve all necessary information for full forensic analysis.
9. ACKNOWLEDGMENT
We would like to thank the anonymous reviewers and our shep-
herd, Wenliang Du, for their insightful comments. This research
has been supported in part by DARPA under Contract 12011593
and by NSF under awards 0917007 and 0845870. Any opinions,
ﬁndings, and conclusions in this paper are those of the authors only
and do not necessarily reﬂect the views of DARPA and NSF.
10. REFERENCES
[1] http://sysbench.sourceforge.net.
[2] P. Agrawal, O. Benjelloun, A. D. Sarma, C. Hayworth,
S. Nabar, T. Sugihara, and J. Widom. Trio: a system for data,
uncertainty, and lineage. In Proceedings of the 32nd
international conference on Very large data bases, VLDB
2006.
[3] P. Ammann, S. Jajodia, and P. Liu. Recovery from malicious
transactions. IEEE Transaction on Knowledge and Data
Engineering, September, 2002.
[4] A. W. Appel. Simple generational garbage collection and fast
allocation. Software Practice and Experience, 1989.
[5] E. Cecchet, J. Marguerite, and W. Zwaenepoel. Performance
and scalability of ejb applications. In Proceedings of the 17th
ACM SIGPLAN conference on Object-oriented
programming, systems, languages, and applications,
OOPSLA 2002.
[6] R. Chandra, T. Kim, M. Shah, N. Narula, and N. Zeldovich.
Intrusion recovery for database-backed web applications. In
Proceedings of the 23th ACM Symposium on Operating
Systems Principles, SOSP 2011.
[7] A. P. Chapman, H. V. Jagadish, and P. Ramanan. Efﬁcient
provenance storage. In Proceedings of the ACM SIGMOD
international conference on Management of data, SIGMOD
2008.
[8] J. Chow, B. Pfaff, T. Garﬁnkel, K. Christopher, and
M. Rosenblum. Understanding data lifetime via whole
system simulation. In Proceedings of the 13th conference on
USENIX Security Symposium, SSYM 2004.
[9] L. P. Deutsch and D. G. Bobrow. An efﬁcient, incremental,
automatic garbage collector. Communications of the ACM,
1976.
[10] E. W. Dijkstra, L. Lamport, A. J. Martin, C. S. Scholten, and
E. F. M. Steffens. On-the-ﬂy garbage collection: an exercise
in cooperation. Communications of the ACM, 1978.
[11] A. Goel, W.-c. Feng, W.-c. Feng, and D. Maier. Automatic
high-performance reconstruction and recovery. Computer
Networks, April, 2007.
[12] A. Goel, K. Po, K. Farhadi, Z. Li, and E. de Lara. The taser
intrusion recovery system. In Proceedings of the 22th ACM
symposium on Operating systems principles, SOSP 2005.
[13] X. Jiang, A. Walters, D. Xu, E. H. Spafford, F. Buchholz, and
Y.-M. Wang. Provenance-aware tracing of worm break-in
and contaminations: A process coloring approach. In
Proceedings of the 26th IEEE International Conference on
Distributed Computing Systems, ICDCS 2006.
[14] R. Jones and R. Lins. Garbage collection: algorithms for
automatic dynamic memory management. John Wiley &
Sons, Inc., 1996.
[15] T. Kim, X. Wang, N. Zeldovich, and M. F. Kaashoek.
Intrusion recovery using selective re-execution. In
Proceedings of the 9th USENIX conference on Operating
systems design and implementation, OSDI 2010.
[16] S. T. King and P. M. Chen. Backtracking intrusions. In
Proceedings of the 19th ACM symposium on Operating
systems principles, SOSP 2003.
[17] S. T. King, Z. M. Mao, D. G. Lucchetti, and P. M. Chen.
Enriching intrusion alerts through multi-host causality. In
Proceedings of the 13th Annual Network and Distributed
System Security Symposium, NDSS 2005.
[18] S. Krishnan, K. Z. Snow, and F. Monrose. Trail of bytes:
efﬁcient support for forensic analysis. In Proceedings of the
17th ACM conference on Computer and communications
security, CCS 2010.
[19] M. Laurenzano, M. Tikir, L. Carrington, and A. Snavely.
Pebil: Efﬁcient static binary instrumentation for linux. In
Proceedings of the IEEE International Symposium on
Performance Analysis of Systems and Software, ISPASS
2010.
[20] K. H. Lee, X. Zhang, and D. Xu. High accuracy attack
provenance via binary-based execution partition. In
Proceedings of the 20th Annual Network and Distributed
System Security Symposium, NDSS 2013.
[21] C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney,
S. Wallace, V. Reddi, and K. Hazelwood. Pin: building
customized program analysis tools with dynamic
instrumentation. In Proceedings of the ACM SIGPLAN
conference on Programming language design and
implementation, PLDI 2005.
[22] J. Newsome and D. X. Song. Dynamic taint analysis for
automatic detection, analysis, and signature generation of
exploits on commodity software. In Proceedings of the 13th
Annual Network and Distributed System Security
Symposium, NDSS 2005.
[23] S. Sitaraman and S. Venkatesan. Forensic analysis of ﬁle
system intrusions using improved backtracking. In
Proceedings of the 3rd IEEE International Workshop on
Information Assurance, IWIA 2005.
[24] D. Tariq, M. Ali, and A. Gehani. Towards automated
collection of application-level data provenance. In
Proceedings of the 4th USENIX conference on Theory and
Practice of Provenance, TaPP 2012.
[25] E. Wu, S. Madden, and M. Stonebraker. Subzero: a
ﬁne-grained lineage system for scientiﬁc databases. In
Proceedings of the 29th IEEE international conference on
Data Engineering, ICDE 2013.
[26] Y. Xie, D. Feng, Z. Tan, L. Chen, K.-K. Muniswamy-Reddy,
Y. Li, and D. D. Long. A hybrid approach for efﬁcient
provenance storage. In Proceedings of the 21st ACM
international conference on Information and knowledge
management, CIKM 2012.
1016