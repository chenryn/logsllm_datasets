point. An informed reconstruction adversary has access to:
a) The fixed dataset D-;
b) The released model’s parameters θ;
c) The model’s training algorithm A;
d) (Optional) Side knowledge aux about the target point.
We first discuss each piece of knowledge we give to our
attacker, and then analyze in depth how our adversary relates
to other threat models arising in other privacy attacks.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1139
a) Fixed dataset: Arguably, the assumption that gives
our attacker the greatest advantage is knowing all the training
data except for the target point. There are two main reasons to
consider such a stringent threat model. First, since our ultimate
goal for studying ML vulnerabilities is to design effective
mitigations, by evaluating the resilience of ML models in this
strong threat model we ensure their resilience against weaker
(and more realistic) attackers. Second, our setup captures the
implicit threat model used in the DP definition; indeed, DP
bounds the ability of a mechanism at preventing the disclosure
of membership information about one data record from an
adversary who knows all the other records in the database.
b) White-box model access: White-box access to the
model is motivated by several real-world scenarios. First, the
practice of publishing models online (e.g. to facilitate their use
or favor public scrutiny) is increasingly widespread. Second,
proprietary models shipped as part of hardware or software
components can be vulnerable to reverse-engineering; it would
be naive to assume that sufficiently motivated adversaries
will never obtain white-box access to such models. Finally,
FL settings may give real-world attackers access to similar
information to the one we capture in our threat model.
c) Training algorithm: Privacy (and security) through
obscurity is generally regarded as a bad practice. Thus, we as-
sume the adversary has access to the model developer’s train-
ing algorithm A, including any associated hyper-parameters
(e.g. learning rate, regularization, batch size, number of iter-
ations, etc). Access to A can be in the form of a concrete
(e.g. open source) implementation. Nevertheless, black-box
access (e.g.
through a SaaS API) suffices for the general
reconstruction attack presented in Section IV. In cases where
A is randomized, we will evaluate attacks with and without
knowledge of the different sources of randomness used when
training the released model. In stochastic optimization algo-
rithms these typically include model initialization and mini-
batch sampling. Knowledge of A’s internal randomness could
come from the model developer using a hard-coded random
seed in a public implementation. Alternatively, knowledge
about the model’s initialization will also be available whenever
the released model
is obtained by fine-tuning a publicly
available model (e.g. in transfer learning scenarios), or in FL
settings where the adversary has successfully compromised an
intermediate model by taking part in the training protocol.
d) Side knowledge about target point: Privacy attacks
do not happen in a vacuum, so adversaries will often have
some prior information about the target point before observing
the released model. For starters, knowledge of D- and A
provides the adversary with syntactic and semantic context for
a learning task in which the model developer deemed it useful
to include the target point. In our investigations, we often
consider adversaries with additional side knowledge abstractly
represented by aux. From a practical perspective, the attack
presented in Section IV takes aux to be a dataset ¯D of points
disjoint from D-. For example, these could come from a public
academic dataset or from scraping relevant websites. Our
experiments in Section VI-B show that these additional points
do not necessarily need to come from the same distribution as
the training data. In our theoretical investigation (Section VII),
we model the adversary’s side knowledge as a probabilistic
prior π from which the target is assumed to be sampled.
B. Reconstruction Attack Protocol and Error Metric
Algorithm 1 formalizes the interaction between model de-
veloper and reconstruction adversary in our threat model. After
the model θ is trained on D = D- ∪ {z}, the adversary runs
their attack algorithm R using all the information discussed in
the previous section, and produces a candidate reconstruction
ˆz for the target point z. The protocol returns a measure of the
attack’s success based on a reconstruction error function ℓ;
smaller error means the reconstruction is more faithful.
Algorithm 1 Reconstruction attack with an informed adver-
sary. (Auxiliary side knowledge aux is optional).
procedure RECONSTRUCTION(A, R, D-, z; aux)
θ ← A(D- ∪ {z})
ˆz ← R(θ, D-, A; aux)
return ℓ(z, ˆz)
Privacy expectations are contextual, and depend on the
information content and modality of the sensitive data. Perfect
reconstruction may not be necessary for the user to claim their
privacy has been violated; e.g., a privacy breach may occur if
the image of a car’s license plate is revealed via an attack, even
if the reconstructed background is inaccurate. In particular,
the error function ℓ can encode not only proximity between
the feature representations of the target and candidate points,
but also the correctness with which an attack can recover a
(private) property of interest about the target. Our experiments
on image classifiers use the MSE between pixels as a measure
of reconstruction, as well as the similarity between outputs of
machine learning models on z and ˆz (through the LPIPS and
KL metrics cf. Section V-B). In general, an appropriate choice
of ℓ and a threshold for declaring successful reconstruction is a
policy question that will depend on the particular application:
it should capture the minimum level of leakage that would
cause a significant harm to the involved individual.
C. Relation to Attribute Inference
Reconstruction can be seen as a generalization of attribute
inference attacks (AIA) [11, 16, 17, 18], also sometimes
referred to as model inversion attacks. In AIA, an attacker that
knows part of a data record z aims to reconstruct the entire
record by exploiting (white-box or black-box) access to a
model θ whose training dataset contained z. It is also common
for the attack goal of a model inversion attack to try and reveal
training data information in aggregate, possibly isolated to a
specific target label. Although no individual training records
are reconstructed through this attack, privacy can be leaked if
aggregated training information with respect to a target label is
sensitive (e.g. facial recognition where each label is associated
with an identity). The standard threat model in AIA does
not include an informed adversary, but we can get a more
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1140
direct comparison with our model by considering an informed
AIA adversary. Such an adversary is identical to Definition 1
but also receives as input partial information about the target
point z, which we denote by η(z). This can be incorporated
in Definition 1 via the side knowledge aux, showing that
informed AIA corresponds to reconstruction in our model
with a particular type of side knowledge. We conclude that
any investigation into mitigating general reconstruction attacks
in our threat model will also be useful in protecting against
informed AIA, and, by extension, standard AIA.
D. Relation to Membership Inference
In membership inference attacks (MIA) [5, 17, 19, 20], an
attacker with access to a released model θ and a challenge
example z ∈ Z guesses if z was part of the model’s training
data. Like in AIA, standard MIA does not assume an informed
adversary. Introducing an informed MIA adversary yields a
model matching the adversary in the threat model behind
DP [14]. This adversary is identical Definition 1, with the
exception that it also receives two candidates z0, z1 ∈ Z
for the additional data point that was used for training the
model, and the developer decides which one to use uniformly
at random. The corresponding interaction protocol between
model developer and adversary is summarized in Algorithm 2,
where the adversary uses a MIA algorithm M and the result
provides a bit representing whether it guessed correctly.
Algorithm 2 Informed Membership Inference Attack
1: procedure INFORMED-MIA(A, M, D-, z0, z1)
2:
3:
4:
5:
b ← Unif({0, 1})
θ ← A(D- ∪ {zb})
ˆb ← M (θ, D-, A, z0, z1)
return b = ˆb
We remark that this attacker is much more powerful than
the one in standard MIA. In particular, if the model’s training
algorithm A is deterministic, then there is a trivial strategy:
the attacker trains models on D- ∪ {z0} and D- ∪ {z1} and
checks which of the two matches the released model θ. This is
coherent with the observation that randomized algorithms are
necessary to (non-trivially) provide DP. Note also that accurate
reconstruction provides an informed MIA. Indeed, assume, for
example, that ℓ satisfies the triangle inequality and reconstruc-
tion succeeds at achieving error less than ℓ(z0, z1)/2. Then the
reconstruction adversary uses θ to obtain a candidate ˆz, and
then guess z0 if ℓ(ˆz, z0) 
0, b is strictly concave (as in the examples above) or the data
is in general position [28]. In any of these cases (1) connects
the unknown z = (x, y) with θ and D-. Assuming the model
is trained with an intercept parameter (i.e. the first coordinate
of each feature vector is equal to 1) this results in a system
of d equations with d unknowns. The following solution for
this system gives an effective reconstruction attack.
Theorem 1 (Reconstruction attack against GLMs). Let θ be
the unique optimum of C(ˆθ) and D- the training data set
except for one point z = (x, y). Suppose ¯X ∈ R(n−1)×d
contains as rows the features of all points in D- where its
first column satisfies ¯X1 = ⃗1, and similarly for the labels
¯Y ∈ Rn−1. Then taking B = g−1( ¯Xθ) − ¯Y we get:
y = g−1(⟨x, θ⟩) + λ ¯X⊤
x =
1 Bθ1 .
¯X⊤B + λθ
¯X⊤
1 B + λθ1
,
We defer all proofs to the appendix. Two important take-
aways from this result are: 1) an informed adversary needs no
additional side knowledge about z to effectively attack a GLM
trained with intercept; and, 2) whether the model overfits the
data or generalizes well plays no role in the attack’s success.
IV. A GENERAL RECONSTRUCTION ATTACK
We describe a reconstruction attack against general ML
models. Intuitively, our attack stems from the observation that
the influence of the target point z on the released model θ is
similar to the influence an alternative point ¯z would have on
the model ¯θ = A(D-∪{¯z}). By repeatedly training models on
different points, our attack collects enough information about
the mapping from training points to model parameters to invert
it at the model of interest θ. We give a high-level introduction
to our attack strategy using reconstructor networks (RecoNN).
A. General Attack Strategy
D-
Let us use the shorthand notation AD-
: Z → Θ with
AD-(z) = A(D- ∪ {z}) to emphasize that, from the point of
view of an informed adversary, when D- is fixed A effectively
becomes a mapping from target points to model parameters.
An ideal reconstruction attack would invert the training proce-
dure and output ˆz = A−1
(θ); whenever A is easy to invert, this
will produce a perfect reconstruction as in the setting analyzed
in Section III. In general, however, the training process is not
(easily) invertible, due to the non-convexity of the optimization
problem solved by A, or to the presence of randomness in the
training process. In such settings, our general reconstruction
attack relies on approximately solving this inverse problem
by producing a function ϕ : Θ → Z that associates model
weights to a guess for the target point in a similar way to the
(ideal) inverse mapping A−1
. Note that the adversary in this
threat model is extremely powerful; for example, they could
enumerate (a fine discretization of) Z and pick the candidate ˆz
that produces the model ˆθ = AD- (ˆz) closest to θ. However, for
high-dimensional data this enumerative approach is infeasible,
so we focus on attacks that can be executed in practice.
D-
In this paper, we instantiate the search for ϕ as a learning
problem, effectively using “neural networks to attack neural
networks”. To solve this learning problem, we first design a
RecoNN architecture for neural networks whose inputs lie in