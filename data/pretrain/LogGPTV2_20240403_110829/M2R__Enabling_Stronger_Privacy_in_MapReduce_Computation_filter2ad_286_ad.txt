in running time to the baseline system. We also compare
M2R with another system offering the same level of pri-
vacy, in which encrypted tuples are sent back to a trusted
client. We show that M2R is up to 44.6× faster compared
5Other hypervisor solutions such as TrustVisor [39], Over-
shadow [14], Nova [56], SecVisor [50] could equivalently be used
456  24th USENIX Security Symposium 
USENIX Association
10
Job
Wordcount
Index
Grep
Aggregate
Join
Pagerank
KMeans
changed
LoC
Hadoop job)
10 (15%)
28 (24%)
13 (13%)
16 (18%)
30 (22%)
42 (20%)
113 (7%)
(vs.
TCB increase (vs. Hadoop
codebase)
370 (0.14%)
370 (0.14%)
355 (0.13%)
395 (0.15%)
478 (0.16%)
429 (0.15%)
400 (0.12%)
Input size (vs. plaintext size)
2.1G (1.06×)
2.5G (1.15×)
2.1G (1.06×)
2G (1.19×)
2G (1.19×)
2.5G (4×)
1G (1.09×)
Shufﬂed bytes
4.2G
8G
75M
289M
450M
2.6G
11K
# App hyper-
calls
3277173
3277173
3277174
18121377
11010647
1750000
12000064
# Platform hy-
percall
35
59
10
12
14
21
8
Table 1: Summary of the porting effort and TCB increase for various M2R applications, and the application runtime cost factors. Number of app
hypercalls consists of both mapT and reduceT invocations. Number of platform hypercalls include groupT and mixT invocations.
Job
Wordcount
Index
Grep
Aggregate
Join
Pagerank
KMeans
Baseline (vs. no en-
cryption)
570 (221)
666 (423)
70 (48)
125 (80)
422 (211)
521 (334)
123 (71)
M2R (% increase vs.
baseline)
1156
1549
106
205
510
755
145
(100%)
(130%)
(50%)
(64%)
(20%)
(44%)
(17%)
Download-and-compute
(× M2R)
1859
2061
1686
9140
5716
1209
6071
(1.6×)
(1.3×)
(15.9×)
(44.6×)
(11.2×)
(1.6×)
(41.9×)
Table 2: Overall running time (s) of M2R applications in compari-
son with other systems: (1) the baseline system protecting computation
only in single nodes, (2) the download-and-compute system which does
not use trusted primitives but instead sends the encrypted tuples back to
trusted servers when homomorphic encrypted computation is not pos-
sible [59].
to this solution.
6.1 Setup & Benchmarks
We select a standard benchmark for evaluating Hadoop
under large workloads called HiBench suite [25]. The
7 benchmark applications, listed in Table 1, cover a
wide range of data-intensive tasks: compute intensive
(KMeans, Grep, Pagerank), shufﬂe intensive (Word-
count, Index), database queries (Join, Aggregate), and
iterative (KMeans, Pagerank). The size of the encrypted
input data is between 1 GB and 2.5 GB in these case stud-
ies. Different applications have different amount of shuf-
ﬂed data, ranging from small sizes (75MB in Grep, 11K
in KMeans) to large sizes (4.2GB in Wordcount, 8GB in
Index).
Our implementation uses the Xen-4.3.3 64-bit hyper-
visor compiled with trusted boot option. The rest of M2R
stack runs on Ubuntu 13.04 64-bit version. We con-
duct our experiments in a cluster of commodity servers
equipped with 1 quad-core Intel CPU 1.8GHz, 1TB hard
drive, 8GB RAM and 1GB Ethernet cards. We vary our
setup to have between 1 to 4 compute nodes (running
mappers and reducers) and between 1 to 4 mixer nodes
for implementing a 2-step cascaded mix network. The
results presented below are from running with 4 com-
pute nodes and 4 mixers each reserving a 100MB buffer
for mixing, averaged over 10 executions.
6.2 Results: Performance
Overheads & Cost Breakdown. We observe a lin-
ear scale-up with the number of nodes in the cluster,
which conﬁrms the scalability of M2R.
In our bench-
marks (Table 2), we observe a total overhead of between
17% − 130% over the baseline system that simply en-
crypts inputs and outputs of map/reduce units, and uti-
lizes none of our privacy-enhancing techniques. It can
also be seen that in all applications except for Grep and
KMeans, running time is proportional to the size of data
transferred during shufﬂing (shufﬂed bytes column in Ta-
ble 1). To understand the cost factors contributing to the
overhead, we measure the time taken by the secure shuf-
ﬂer, by the mapT and reduceT units, and by the rest of
the Hadoop system which comprises the time spent on
I/O, scheduling and other book-keeping tasks. This rel-
ative cost breakdown is detailed in Figure 4. From the
result, we observe that the cost of the secure shufﬂer is
signiﬁcant. Therefore, reducing the overheads of shuf-
ﬂing, by avoiding the generic ORAM solution, is well-
incentivized and is critical to reducing the overall over-
heads. The two main benchmarks which have high over-
heads of over 100%, namely Wordcount and Index, incur
this cost primarily due to the cost of privacy-preserving
shufﬂing a large amount of data. In benchmarks where
the shufﬂed data is small (Grep, KMeans), the use of
mapT/reduceT adds relatively larger overheads than
that from the secure shufﬂer. The second observation is
that the total cost of the both shufﬂer and other trusted
components is comparable to that of Hadoop, which pro-
vides evidence that M2R preserves the asymptotic com-
plexity of Hadoop.
Comparison to Previous Solutions. Apart from the
baseline system, a second point of comparison are pre-
viously proposed systems that send encrypted tuples
to the user for private computation. Systems such as
Monomi [59] and AutoCrypt [58] employ homomorphic
encryption for computing on encrypted data on the single
servers. For operations that cannot be done on the server
using partially homomorphic encryption, such Monomi-
like systems forward the data to a trusted set of servers
(or to the client’s private cloud) for decryption. We re-
fer to this approach as download-and-compute approach.
We estimate the performance of a Monomi-like system
extended to distributed computation tasks, for achiev-
ing privacy equivalent to ours. To compare, we assume
that the system uses Paillier, ElGamal and randomized
USENIX Association  
24th USENIX Security Symposium  457
11
Figure 4: Normalized break-down time for M2R applications. The run-
ning time consists of the time taken by mapT and reduceT, plus the
time by the secure shufﬂer. The rest comes from the Hadoop runtime.
search schemes for homomorphic computation, but not
OPE or deterministic schemes (since that leaks more than
M2R and our baseline system do). We run operations
that would fall outside such the expressiveness of the
allowed homomorphic operations, including shufﬂing,
as a separate network request to the trusted client. We
batch network requests into one per MapReduce step.
We assume that the network round trip latency to the
client is only 1ms — an optimistic approximation since
the average round trip delay in the same data center is
10 − 100ms [4, 61]. We ﬁnd that this download-and-
compute approach is slower compared to ours by a fac-
tor of 1.3× to 44.6× (Table 2), with the median bench-
mark running slower by 11.2×. The overheads are low
for case-studies where most of the computation can be
handled by homomorphic operations, but most of the
benchmarks require conversions between homomorphic
schemes (thereby requiring decryption) [58, 59] or com-
putation on plaintext values.
Platform-Speciﬁc Costs. Readers may wonder if the
evaluation results are signiﬁcantly affected by the choice
of our implementation platform. We ﬁnd that the dom-
inant costs we report here are largely complementary to
the costs incurred by the speciﬁcs of the underlying plat-
form. We conduct a micro-benchmark to evaluate the
cost of context-switches and the total time spent in the
trusted components to explain this aspect. In our plat-
form, the cost of each hypercall (switch to trusted logic)
is small (13µs), and the execution of each trusted com-
ponent is largely proportional to the size of its input data
as shown in Figure 5. The time taken by the trusted
computation grows near linearly with the input data-size,
showing that the constant overheads of context-switches
and other platform’s speciﬁcs do not contribute to the
reported results signiﬁcantly. This implies that simple
optimizations such as batching multiple trusted code in-
vocations would not yield any signiﬁcant improvements,
since the overheads are indeed proportional to the total
size of data and not the number of invocations. The total
number of invocations (via hypercalls) for app-speciﬁc
trusted logic (mapT, reduceT) is proportional to the to-
tal number input tuples, which amounts for less than half
Figure 5: Cost of executing mapT instance of the Wordcount and