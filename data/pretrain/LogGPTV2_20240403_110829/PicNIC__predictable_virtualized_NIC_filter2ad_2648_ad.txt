NAPI-TX:
Reduced buffer bloat
(Out of Order Completions)
OOO VirtIO:
No HoL blocking
between flows
Accounting:
VMs buffering isolation
Timing Wheel
OOO Completion
NIC
Figure 6: PicNIC’s sender-side admission control.
accounting (§5.4.2) and (ii) backpressure to guest OS stack (§5.4.3).
Fig. 6 illustrates PicNIC’s sender-side admission control.
5.4.1 Enforcing Rate Limits. PicNIC implements shaping with
a single time-indexed queue—Timing Wheel (TW) [74]. It supports
a flow being shaped by multiple policies [14, 25]. In our imple-
mentation, the rate limits applicable to a packet are consolidated
into a single timestamp based on the slowest rate. This timestamp
serves as the earliest departure time for when the packet is released
from the TW for transmission. VM-to-VM BPS and PPS rate limits
computed by PCC are also consolidated into these timestamps.
5.4.2 Packet Accounting. To avoid costly dynamic memory al-
location, the engine uses a statically-allocated shared packet de-
scriptor pool. We buffer packets for throttled flows in the TW and
release the corresponding descriptor when a packet leaves the TW
and enters the NIC. A non-cooperating VM can exhaust the entire
descriptor pool by discharging packets at a high rate for a throttled
flow, leading to drops for other VMs (§2.1). To handle this, Pic-
NIC introduces Packet Accounting to track the number of packets
buffered in the TW from each VM. It sets a per-VM cap and a total
shared cap on the number of buffered packets, and does not admit
packets that exceed either limit. Thus, even if a VM sends excessive
packets for throttled flows, it cannot exhaust the descriptor pool,
and hence would not impact other VMs’ flows.
5.4.3 Backpressure to Guest OS. Egress shaping must be ac-
companied by complete backpressure to the guest networking stack
(§2.1 and §3). Every link in this chain of backpressure is critical
for isolation. PicNIC implements this by combining out-of-order
completions [67] for backpressure from the engine to the guest NIC
driver, and NAPI-TX [44] along with TCP Small Queues (TSQ) [15]
for backpressure from the guest NIC driver to the guest IP stack.
The first link in this chain is from the egress engine to the guest
NIC driver. To create backpressure here, the engine must hold the
completion event for each packet until the packet actually leaves
the engine. On dequeuing a packet from guest Tx queues, instead
of marking it as Tx complete immediately, PicNIC sends the Tx
completion to the guest only when the packet is delivered to the host
NIC (or dropped). By limiting the number of descriptors per VM,
we prevent a VM from sending a deluge of packets into the engine.
Implementing such deferred completions needs support from the
guest driver—e.g., virtio [36] works in two completion modes,
in-order and out-of-order (OOO). In the in-order mode, completion
events must be received in the same order as the transmission
sequence—e.g, if a VM has two flows and only one is throttled, the
unthrottled flow’s descriptors cannot be freed until the throttled
flow’s descriptors are freed; this causes HoL blocking. Eventually,
both flows become throttled. To solve this, PicNIC enables OOO
completions in virtio so that the flows’ descriptors can be freed
independently. This relies on packet accounting to limit the number
of buffered packets per flow and ensuring that this limit is lower
than the total number of descriptors in the guest NIC.
The second link in this chain is from the guest NIC driver to the
guest stack. We combine NAPI-TX with TSQ to ensure that each
flow can queue only a limited amount of data for Tx in the guest
stack. NAPI-TX is a Linux kernel feature that makes virtio call the
SKB destructor after a packet is actually “out”—i.e., at Tx completion
interrupt, instead of immediately on enqueue to virtio. This pro-
vides socket backpressure and is needed for TSQ. Enabling NAPI-TX
in guests is critical for complete backpressure all the way up to the
guest applications as depicted in Fig. 6. This prevents bufferbloat
and avoids associated long latencies and isolation breakage among
flows for a VM as shown in §6 (Table 4).
5.5 Practical Considerations
We need to overcome multiple challenges to make PicNIC prac-
tical. The key challenges stem from our goal of sub-ms isolation
enforcement without sacrificing datapath performance.
Responsiveness. We need early signals of overloads. We engineer
CWFQs to provide basic isolation at short timescales and aid PCCP
with signals for rapid overload detection. Notifying rate limits to
sources quickly is challenging as maintaining a list of all senders
is expensive; PicNIC uses lightweight sampling to identify heavy-
hitters. To make PCCP more responsive, we start it with an initial
rate estimate to apply immediately on detecting isolation issues.
Performance. To achieve line rate, we have O(100ns) to process
each packet; so every per-packet operation needs to be optimized
to minimize overhead. We address this by running PCC at fixed
epochs instead of per-packet while deriving congestion signals from
existing metrics. We gate PCCP on CWFQ occupancy; this turns off
PCCP to reduce overheads when there are no isolation issues. To
keep the datapath minimal and fast, we explored a control-thread
based approach that moves PCC out of the datapath (§5.3).
Sometimes, the source of offending traffic could be the Inter-
net, and not other VMs in the same datacenter—e.g., in the case
of DoS attacks. To handle such cases, PicNIC considers the load-
balancers [20, 56] as traffic sources and sends rate feedback to
load-balancers, which impose throttles on DoS traffic. As there can
be multiple load-balancers, we also explored a centralized approach
for rate dissemination where a central server collects all feedbacks
and distributes them based on source weights. While a distributed
approach may take longer to notify all sources, a centralized ap-
proach can disseminate rate limits quickly.
6 Evaluation
We evaluate PicNIC in production of a major cloud provider. We
start with microbenchmarks (§6.1) to show that PicNIC implements
the predictable vNIC abstraction with low overheads (§6.2). Then,
we quantify the benefits to applications (§6.3) and end-users (§6.4).
358
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
h3
VM
C
h1
VM
A
h2
VM
B
VM
D
f ij : i to j flow
12
6
0
12
6
0
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
AC
12
BC
6
BD
6
AD
5
7
5
5
5
5
5
0
60
120
Time (s)
180
240
1.00
0.75
F
D
C
0.50
0.25
0.00
Perfect Isolation
CWFQ+PCCB
PicNIC
101
102
103
Latency (μs)
104
)
s
p
p
M
(
s
p
o
r
D
C
N
I
2.0
1.5
1.0
0.5
0.0
C W F Q +
P C C B
)
s
p
p
M
(
t
u
p
d
o
o
G
108
pps
c N I C
P i
UDP
2.0
1.5
1.0
0.5
0.0
C W F Q +
P C C B
c N I C
P i
(a) Setup
(b) VM-VM throughput within envelope
(c) Low latency
(d) Near-zero drops
(e) Better goodput
Figure 7: Microbenchmarks: PicNIC provides a predictable vNIC abstraction to VMs. In addition to providing predictable bandwidth envelope, low
latency and near-zero drops, PicNIC also improves the goodput for the PPS-intensive traffic.
6.1 Microbenchmarks
First, we show that PicNIC realizes the predictable vNIC abstraction
with a small setup (§6.1.1), and then we evaluate each construct of
PicNIC in more detail (§6.1.2).
6.1.1 Predictable vNIC. To show that PicNIC ensures a band-
width envelope, low delay and loss rate for VMs (Table 2), we use
the setup shown in Fig. 7a.
Bandwidth. The key result we show is that even while bottlenecks
shift, the BPS for every VM is maintained within the envelope.
To construct shifting bottlenecks, we stagger the start of VM-VM
flows fAC , fBC , fBD and fAD . Each host NIC’s capacity is 20 Gbps,
and each VM’s BPS envelope is set to [4 Gbps (MIN_BPS), 12 Gbps
(MAX_BPS)]. Fig. 7b shows the overall throughput for each flow. Ini-
tially, fAC achieves 12 Gbps as PicNIC enforces MAX_BPS egress rate
for A. When fBC starts, PCCB detects the contention for C’s ingress
bandwidth (12 Gbps) and rate limits each flow to 6 Gbps. When fBD
starts, PCCB detects that D is active, and computes the share of h3’s
NIC bandwidth (20 Gbps) as 10 Gbps each for C and D. This causes
the rate limits for fAC and fBC to decrease to 5 Gbps each. This,
in turn, leaves spare egress bandwidth for B, which fBD grabs to
achieve 12 - 5 = 7Gbps. Finally, when fAD arrives, PCCB allocates
D’s ingress capacity (10 Gbps) equally among fAD and fBD .
Low delay. We show that (i) PicNIC, with PCCP, ensures low latency
and drops even in extreme cases while (ii) just BPS guarantees are
insufficient to achieve these. We extend the setup in Fig. 7a with
two more VMs: E on h2 and F on h3. fAC and fBD are high-PPS UDP
flows with 256B packets. We measure RTT for fE F which is an open-
loop latency prober generating packets following a Poisson process
(rate λ = 1 kpps) and discards any drops from measurements.
Fig. 7c shows fE F RTTs measured with PicNIC and compares it
to the cases with (i) just BPS envelope (i.e., with CWFQ+PCCB but
no PCCP), which can be thought of as similar to EyeQ [35], and
(ii) perfect isolation, i.e., when the latency prober is run by itself
without any other traffic. As the flows are not BPS-intensive, there
is no contention for bandwidth. However, the high-PPS UDP traffic
causes isolation breakage at the hosts and impacts the latency for
fE F . PicNIC detects this isolation issue and throttles UDP to the
appropriate rate so that fE F achieves latency close to the case with
perfect isolation.
Low loss rate. As h3’s ingress engine is overloaded with high-PPS
UDP traffic, packets are dropped at the host NIC. Even with CWFQ+
PCCB, we find such tail-drop rate to be 1.54 Mpps as shown in
t
u
p
h
g
u
o
r
h
T
20
) 20
s
p
10
b
G
0
(
4xTCP
200xTCP
No CWFQ with CWFQ
t
u
p
h
g
u
o
r
h
T
10
5
0
)
s
p
b
M
(
with PA
No PA
with PA
No PA
15
10
5
0
)
s
p
b
G
(
Throttled UDP
Unthrottled TCP
(a) CWFQs at ingress
(b) Packet accounting at egress
Figure 8: Ingress and egress isolation with PicNIC constructs.
Setup
Throughput (Mbps)
Unthrottled Throttled
Timing wheel
occupancy
Drops
(kpps)
No NAPI + EC
NAPI + OOO
8,993
10,145
9
9
4000 (100%)
50 (1.25%)
300
0
Table 4: OOO completions and NAPI-TX ensure egress isolation.
Fig. 7d. With PicNIC, such drops decrease to a mere 108 pps as
PicNIC admits only enough packets as can be processed without
breaking isolation.
Improved efficiency. Somewhat counterintuitively, even though Pic-
NIC throttles the UDP flows, the total goodput for UDP increases
from 0.96 Mpps to 1.57 Mpps as shown in Fig. 7e. By applying back-
pressure to excess traffic, PicNIC avoids wasted work of pulling
packets from the NIC and classifying them into per-VM queues be-
fore it can decide to drop packets exceeding their engine CPU share.
Thus, performing admission control as opposed to dropping packets
leads to increased engine efficiency and 1.6× higher goodput.
6.1.2 Components. Now, we evaluate the constructs in more
detail and tease out their role in achieving predictable performance.
CPU-fair Weighted Fair Queues. Consider the setup from Fig. 7a
with two VM-VM flows: fAC with 4 parallel TCP streams and fBD
with 200 parallel TCP streams. Without CWFQs, fBD gets an un-
fairly high share of CPU cycles at h3, while fAC suffers as shown in
Fig. 8a. On enabling CWFQs, the ingress engine’s capacity is shared
equally among the two flows, and hence both are able to achieve
equal (fair) throughput.
Packet Accounting (PA). We reproduce egress isolation breakage
using the setup from Fig. 7a by moving VM B to h1 so that A and
B are colocated. fAC is unthrottled TCP, while fBD is a UDP flow