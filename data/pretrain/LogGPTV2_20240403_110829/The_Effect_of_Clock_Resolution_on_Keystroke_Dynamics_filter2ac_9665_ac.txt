1 Our detector diﬀers from that proposed by Joyce and Gupta in both its mean-vector
calculation and the distance measure used. We calculated the mean vector using
all the training data while Joyce and Gupta preprocessed the data to remove out-
liers. We used the Mahalanobis distance while Joyce and Gupta used the Manhattan
distance. Our mean-based detector was intended to be simple (with no preprocess-
ing) while still accommodating natural variances in the data (with the Mahalanobis
distance).
The Eﬀect of Clock Resolution on Keystroke Dynamics
341
Mahalanobis distance is a measure of multidimensional distance that takes into
account the fact that a sample may vary more in one dimension than another,
and that there may be correlations between pairs of dimensions. These variations
and correlations are estimated using the correlation matrix of the training data.
More formally, using the matrix notation of linear algebra, if x is the mean of
the training data, S is the covariance matrix, and y is the new password-timing
vector, the Mahalanobis distance (d) is:
d ← (x − y)TS
−1(x − y)
The anomaly score of a new password-timing vector is simply this distance.
6.3 Nearest-Neighbor Detector
Whereas the mean-based detector makes the assumption that the distribution
of a user’s passwords is known, the nearest-neighbor detector makes no such
assumption. Its primary assumption is that new password-timing vectors from
the user will resemble one or more of those in the training data. Cho et al. [4]
explored the use of a nearest-neighbor detector in their work, and we attempted
to re-implement their detector for our investigation.
During training, the nearest-neighbor detector estimates the covariance ma-
trix of the training password-timing vectors (in the same way as the mean-based
detector). However, instead of estimating the mean of the training data, the
nearest-neighbor detector simply saves each password-timing vector.
During testing, the nearest-neighbor detector calculates Mahalanobis dis-
tances (using the covariance matrix of the training data). However, instead of
calculating the distance from the new password-timing vector to the mean of the
training data, the distance is calculated from the new password-timing vector
to each of the vectors in the training data. The distance from the new vector to
the nearest vector from the training data (i.e., its nearest neighbor) is used as
the anomaly score.
6.4 Multilayer-Perceptron Detector
Whereas the behaviors of the mean-based and nearest-neighbor detectors allow
for an intuitive explanation, the multilayer perceptron is comparatively opaque.
A multilayer perceptron is a kind of artiﬁcial neural network that can be trained
to behave like an arbitrary function (i.e., when given inputs, its outputs will
approximate the function’s output). Hwang and Cho [8] showed how a multi-
layer perceptron could be used as an anomaly detector by training it to auto-
associate—that is, to behave like a function that reproduces its input as the
output. In theory, new input that is like the input used to train the network
will also produce similar output, while input that is diﬀerent from the train-
ing input will produce wildly diﬀerent output. By comparing the input to the
output, one can detect anomalies. Cho et al. [4] used an auto-associative mul-
tilayer perceptron to discriminate between users and impostors on the basis of
password-timing vectors. We attempted to re-implement that detector.
342
K. Killourhy and R. Maxion
During training, the password-timing vectors are used to create an auto-
associative multilayer perceptron. This process is a standard machine-learning
procedure, but it is fairly involved. We present an overview here, but we must
direct a reader to the works by Hwang, Cho, and their colleagues for a compre-
hensive treatment [4, 8]. A skeleton of a multilayer perceptron is ﬁrst created. The
skeleton has 21 input nodes, corresponding to the 21 elements of the password-
timing vector, and 21 output nodes. In general, a multilayer-perceptron network
can have a variety of structures (called hidden nodes) between the input and the
output nodes. In keeping with earlier designs, we had a single layer of 21 hidden
nodes. This skeleton was trained using a technique called back-propagation to
auto-associate the user’s password-timing vectors. We used the recommended
learning parameters: training for 500 epochs with a 1 × 10−4 learning rate and
a 3 × 10−4 momentum term.2
During testing, the new password-timing vector is used as input to the trained
multilayer perceptron, and the output is calculated. The Euclidean distance of
the input to the output is computed and used as the anomaly score.
7 Performance-Assessment Method
Now that we have three detectors and data at a variety of clock resolutions, the
ﬁnal step is to evaluate the detectors’ performance. First, we convert the data
to password-timing tables. Then we devise a procedure for training and testing
the detectors. Last, we aggregate the test results into overall measures of each
detector’s performance at each clock resolution.
7.1 Creating Password-Timing Tables
As mentioned in Section 5, we have 22 data sets that diﬀer only in the resolution
of the clock used to timestamp the keystroke events: the high-resolution clock,
the 15.625 ms Windows-event clock, and the 20 derived clocks. For each clock,
we have timing information for 51 subjects, each of whom typed the password
(.tie5Roanl) 400 times.
We extract password-timing tables from the raw data. Hold times and di-
gram intervals are calculated. We conﬁrm that 50 password-timing vectors are
extracted from each one of a subject’s 8 sessions, and that a total of 20,400
password-timing vectors are extracted (50 passwords× 8 sessions× 51 subjects).
7.2 Training and Testing the Detectors
Consider a scenario in which a user’s long-time password has been compromised
by an impostor. The user is assumed to be practiced in typing her password,
2 Note that our learning rate and momentum are 1000 times smaller than those re-
ported by Cho et al. This change accounts for a diﬀerence in units between their
password-timing vectors and ours. (We record in seconds; they used milliseconds.)
The Eﬀect of Clock Resolution on Keystroke Dynamics
343
while the impostor is unfamiliar with it (e.g., typing it for the ﬁrst time). We
measure how well each of our three detectors is able to detect the impostor,
discriminating the impostor’s typing from the user’s typing in this scenario.
We start by designating one of our subjects as the legitimate user, and the
rest as impostors. We train and test each of the three detectors as follows:
1. We train the detector on the ﬁrst 200 passwords typed by the legitimate user.
The detector builds a proﬁle of that user.
2. We test the ability of the detector to recognize the user herself by generating
anomaly scores for the remaining 200 passwords typed by the user. We record
these as user scores.
3. We test the ability of the detector to recognize impostors by generating anom-
aly scores for the ﬁrst 5 passwords typed by each of the 50 impostors. We
record these as impostor scores.
This process is then repeated, designating each of the other subjects as the
legitimate user in turn. After training and testing a detector for each combination
of subject, detector, and clock-resolution data set, we have a total of 3,366 sets
of user and impostor scores (51 subjects × 3 detectors × 22 data sets).
It may seem that 200 passwords is an unrealistically large amount of training
data. However, we used 200 passwords to train because we were concerned that
fewer passwords might unfairly cause one or more detectors to under-perform
(e.g., Cho et al. [4] trained the multilayer perceptron on up to 325 passwords).
Likewise, an unpracticed impostor might seem unrealistic. If he knew that his
keystroke dynamics would be scrutinized, he might practice ﬁrst. However, as
we argued in Section 4.2, the amount of practice a subject has had represents
a potential confounding factor. Consequently, all impostors in our experiment
were allowed the same level of practice. Our intuition was that the eﬀect of clock
resolution on detector performance might be seen most clearly with unpracticed
impostors, and so we used their data (with plans to use practiced impostors’
data in future investigations).
7.3 Calculating Detector Performance
To convert these sets of user and impostor scores into aggregate measures of
detector performance, we used the scores to generate a graphical summary called
an ROC curve [20], an example of which is shown in Figure 4. The hit rate is
the frequency with which impostors’ passwords generate an alarm (a desirable
response), and the false-alarm rate is the frequency with which the legitimate
user’s passwords generate an alarm (an undesirable response). Whether or not
a password generates an alarm depends on how the threshold for the anomaly
scores is chosen. Over the continuum of possible thresholds to choose, the ROC
curve illustrates how each one would change hit and false-alarm rates. Each point
on the curve indicates the hit and false-alarm rates at a particular threshold.
The ROC curve is a common visualization of a detector’s performance, and
on the basis of the ROC curve, various cost measures can be calculated. Two
common measures are the equal-error rate and the zero-miss false-alarm rate.
344
K. Killourhy and R. Maxion
Subject 19
Nearest Neighbor
(1 ms clock)
Zero−Miss False−Alarm Rate 
Equal−Error Rate 
e
t
a
R
t
i
H
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
0.0
0.2
0.4
0.6
0.8
1.0
False−Alarm Rate
Fig. 4. An example ROC curve depicts the performance of the nearest-neighbor de-
tector with subject 19 as the legitimate user and data from the derived 1 ms resolu-
tion clock. The curve shows the trade-oﬀ between the hit rate and false-alarm rate.
The proximity of the curve to the top-left corner of the graph is a visual measure of
performance.
The equal-error rate is the place on the curve where the false-alarm rate is equal
to the miss rate (note that miss rate = 1 − hit rate). Geometrically, the equal-
error rate is the false-alarm rate where the ROC curve intersects a line from
the top-left corner of the plot to the bottom right corner. This cost measure
was advocated by Peacock et al. [17] as a desirable single-number summary of
detector performance. The zero-miss false-alarm rate is the smallest false-alarm
rate for which the miss rate is zero (or, alternatively, the hit rate is 100%).
Geometrically, the zero-miss false-alarm rate is the leftmost point on the curve
where it is still ﬂat against the top of the plot. This cost measure is used by Cho
et al. [4] to compare detectors.
For each combination of subject, detector, and clock resolution, we generated
an ROC curve, and we calculated these two cost measures. Then, to obtain an
overall summary of a detector’s performance at a particular clock resolution,
we calculated the average equal-error rate and the average zero-miss false-alarm
rate across all 51 subjects. These two measures of average cost were used to
assess detector performance.
8 Results and Analysis
A preliminary look at the results reveals that—while the equal-error rate and the
zero-miss false-alarm rate diﬀer from one another—they show the same trends
with respect to diﬀerent detectors and clock resolutions. Consequently, we focus
on the equal-error-rate results and acknowledge similar ﬁndings for the zero-miss
false-alarm rate.
The Eﬀect of Clock Resolution on Keystroke Dynamics
345
Table 1. The average equal-error rates for the three detectors are compared when
using (1) the high-resolution clock, (2) the derived 15 ms resolution clock, and (3)
the 15.625 ms Windows-event clock. The numbers in parentheses indicate the percent
increase in the equal-error rate over that of the high-resolution timer. The results from
the 15 ms derived clock very closely match the results with the actual 15.625 ms clock.
Clock
(1) High-resolution
(2) Derived 15 ms resolution
(3) 15.625 ms Windows-event
Mean-based
0.1100
0.1153 (+4.8%)
0.1152 (+4.7%)
Detectors
Nearest
Neighbor
0.0996
0.1071 (+7.5%)
0.1044 (+4.8%)
Multilayer
Perceptron
0.1624
0.1631 (+0.4%)
0.1634 (+0.6%)
The accuracy of our results depends on our derived low-resolution timestamps
behaving like real low-resolution timestamps. Our ﬁrst step is to establish the
validity of the derived clock data by comparing a detector’s performance on
derived low-resolution data to its performance on data from a real clock operating
at that resolution. Then we proceed to examine our primary results concerning
the eﬀect of clock resolution on detector performance.
8.1 Accuracy of the Derived Clock
Table 1 shows the average equal-error rate for each of the three detectors, using
the high-resolution clock, the derived 15 ms resolution clock, and the real 15.625
ms resolution Windows-event clock. In addition to the equal-error rates, the table
includes a percentage in parentheses for the derived clock and the Windows-event
clock. This percentage indicates the percent increase in the equal-error rate over
that from the high-resolution clock.
To verify the correctness of the results using the derived low-resolution clocks,
we compare the second and third rows of Table 1. The results are almost exactly
the same except for the nearest-neighbor detector. Since the nearest-neighbor
detector is not robust to small changes in the training data, it is not surprising
to see a comparatively large diﬀerence between the derived 15 ms clock and the
real 15.625 ms clock. The similarity in the results of the other two detectors
indicate that the derived clock results are accurate.
Even if we had been able to directly derive a 15.625 ms clock (impossible
because of the limitations of the derivation procedure described in Section 5),
small diﬀerences between the derived and real timestamps would still cause small
diﬀerences in detector performance (e.g., diﬀerences resulting from small delays
in how quickly the real clock is queried).
8.2 Eﬀects of Clock Resolution on Detector Performance
Figure 5 depicts the eﬀect of clock resolution on the average equal-error rate of
the three detectors. Each panel displays a curve for each of the three detectors,
but at diﬀerent scales, highlighting a diﬀerent result.
346
K. Killourhy and R. Maxion
e
t
a
R
r
o
r
r
l
E
−
a
u
q
E
t
e
a
R
r
o
r
r
l
E
−