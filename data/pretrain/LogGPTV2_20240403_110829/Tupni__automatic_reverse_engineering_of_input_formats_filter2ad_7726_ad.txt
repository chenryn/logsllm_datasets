Input
Error Fields
Total Fields
WMF BMP
3
3087
2
2257
JPG PNG
889
1551
0
3211
TIF DNS RPC TFTP HTTP
0
23
6
137
0
26
2
4220
4
32
FTP
0
4
Table 2: Summary of ﬁeld identiﬁcation results.
42] quantitatively as these tools are not publicly available, and we
used different ﬁle/protocol formats for evaluation. We will give a
qualitative comparison in Section 6.
In the rest of this section, we will ﬁrst summarize our experimen-
tal results on the ten test cases and discuss why Tupni made errors
in reverse engineering the formats. After that, we will analyze the
aggregation of format information across multiple inputs.
4.3 Experimental Results on Single Inputs
4.3.1 Field Identiﬁcation
To evaluate the accuracy of ﬁeld identiﬁcation, we counted the
number of ﬁelds in the published format that were identiﬁed incor-
rectly by Tupni. The results are shown in Table 2.
By analyzing the execution traces and source code, we found
that most of these errors have the same cause—a program ignores
certain parts of an input. When this happens, Tupni will mark these
gaps as virtual ﬁelds. For example, there are 127 virtual ﬁelds in
the PNG test. This happens because the ﬁrst 128 chunks in the
PNG test ﬁle are identical, and the parsing program ignored the
data bytes in all but the ﬁrst chunk. Since the application never ac-
cessed the data bytes, Tupni had to create one virtual ﬁeld for each
chunk that covers all the data bytes in the chunk. The published
speciﬁcation deﬁnes seven ﬁelds on these data bytes. This results
in a misclassiﬁcation of 889 ﬁelds (7 ﬁelds per chunk times 127
chunks).
One of the errors in the DNS test case occurs because the pro-
gram processed each byte in a two-byte ﬂag ﬁeld separately.
4.3.2 Record Sequence Identiﬁcation
We evaluated the accuracy of record sequence identiﬁcation by
measuring the number of record sequences that are in the published
format but missed by Tupni and the number of record sequences
that are not in the published format but identiﬁed by Tupni. We
refer to the former as false negative (FN) sequences and to the latter
as false positive (FP) sequences. The results are shown in Table 3.
Tupni did not miss any record sequences in the published format
of any of the ten test cases. This conﬁrms our observation that a
parsing program usually parses and processes record sequences in
loops.
Tupni output false positive sequences in three test cases. By an-
alyzing the applications, we found that, in most cases, this happens
because the application uses a loop to process input ﬁelds that do
not belong to a record sequence in the published format. In the
PNG case, Tupni identiﬁed two false positive sequences from ex-
ecution loops in decoding the image ﬁle. In the JPG test, Tupni
identiﬁed the ﬁrst two bytes ffd8 as a sequence because the pars-
ing program compared them with a two byte string to look for the
start of image. Tupni found extra record sequences in the HTTP
test because the program processed some substrings in loops.
4.3.3 Record Type Identiﬁcation
Ideally, the set of record types identiﬁed by Tupni (referred to as
the Tupni record types) should correspond one-to-one to the record
types in the published format (referred to as the published record
types). There may be two kinds of inaccuracies. First, records
of a single Tupni record type may belong to multiple published
record types. Second, records of multiple Tupni record types may
belong to a single published record type.
In our evaluation, we
analyzed these two kinds of errors for each record type seen in
the test cases. For the ﬁrst kind of error, we counted the num-
ber of Tupni record types that have records belonging to multiple
published record types. We will refer to them as under-classiﬁed
record types. For the second kind of error, we counted the number
of published record types that contain records assigned to multiple
Tupni record types. We will refer to them as over-classiﬁed record
types. We did not analyze the FTP and HTTP test cases in this
evaluation because all their record sequences are ASCII strings and
have trivial record types. The results are shown in Table 4. Tupni
did not make any errors of the ﬁrst kind, and it made two errors of
the second kind.
By analyzing the execution traces and source code, we found
that these two errors arise because a record type deﬁned in the pub-
lished speciﬁcation may be an abstraction of a complex object that
has multiple semantics, which causes the parsing program to be-
have differently on records of the same record type. In the WMF
test case, the published record type associated with the error is
CreateBrushIndirect whose parameters contain three ﬂags
that control many different behaviors of Brush objects. In the JPG
test case, Tupni assigned DHT marker segments (records) to two
different record types. We found that this happens because DHT
marker segments can deﬁne two different types of Huffman tables
(lossy and lossless) that are processed differently.
Input
FN Sequences
FP Sequences
True Sequences
WMF BMP
0
0
0
0
0
35
JPG PNG TIF DNS RPC TFTP HTTP
0
3
12
0
0
1
Table 3: Summary of record sequence identiﬁcation results.
0
1
15
0
2
4
0
0
4
0
0
6
0
0
1
FTP
0
0
3
Input
Under-Classiﬁed Record Types
Over-Classiﬁed Record Types
WMF BMP
0
0
0
1
JPG PNG TIF DNS RPC TFTP
0
0
0
1
0
0
0
0
0
0
0
0
Table 4: Summary of record type identiﬁcation results.
4.3.4 Constraint Identiﬁcation
In this section, we discuss Tupni’s accuracy on constraint iden-
tiﬁcation. Tupni can identify three kinds of constraints: symbolic-
predicate, inter-message, and length. Since length ﬁeld identiﬁca-
tion has been widely studied in previous work and Tupni achieved
similar performance, we will not discuss length constraints but fo-
cus on the other two kinds.
Symbolic-Predicate Constraints: Our evaluation on functional
constraints was focused on whether Tupni can identify all check-
sum dependencies. In the ten test cases, only two input formats,
WMF and PNG, have checksum ﬁelds. Tupni identiﬁed one func-
tional constraint in the WMF test that captures the dependency be-
tween the checksum ﬁeld in the WMF header and the rest of the
header. A PNG ﬁle [40] has a sequence of chunks and each chunk
has a 4-byte CRC checksum ﬁeld calculated on the preceding bytes
in the chunk. Tupni did not ﬁnd functional constraints that reﬂect
these checksum dependencies. By analyzing the execution trace,
we found that this happens because the parsing program ignored
the checksums while parsing each chunk.
Tupni also identiﬁed single-value constraints in every test case,
many of which revealed interesting insights into the input formats.
For example, Tupni inferred that the ﬁrst byte of a marker segment
in the JPG ﬁle must be 0xff; the size ﬁeld of a draw record in the
WMF ﬁle must be at least 3; there are at most 20 context elements
in the RPC bind message. We also observed that there are con-
straints that are particular to the input. For example, Tupni inferred
that the version of the WMF ﬁle must be 0x0300; however, by ana-
lyzing the source code we found that it can also be 0x0100. This is
a general limitation of dynamic analysis. We will discuss this issue
further in Section 7.
Inter-Message Constraints: In the ten test cases, only three net-
work protocols, DNS, RPC and TFTP, have inter-message con-
straints. To identify the inter-message constraints in DNS and RPC,
one must track the server program but we tracked the client pro-
gram in our tests. Thus, TFTP is our only test case with an inter-
message constraint. Tupni correctly identiﬁed this constraint. In
particular, by tracking how the ACK message was generated after
receiving the data message, Tupni identiﬁed that the AckNum in
the acknowledge message must equal the BlockNum in the data
message.
4.4 Experimental Results on Multiple Inputs
In this section, we present the experimental results of running
Tupni over 150 WMF ﬁles to generate a more complete WMF for-
mat speciﬁcation. The 150 ﬁles were randomly selected from a
test template repository at Microsoft. The average ﬁle size is 1,577
bytes. The average number of draw records per ﬁle was 63. We
ﬁrst examine how accurately Tupni can match ﬁelds and record se-
quences across multiple inputs, then describe how much improve-
ment on format coverage Tupni can achieve over multiple inputs.
We do not present results on matching record types since we
have evaluated how accurately Tupni can identify record types in
Section 4.3.3. To evaluate the accuracy on matching ﬁelds, we in-
spected the ﬁrst 17 ﬁelds identiﬁed in the 150 Tupni formats gen-
erated on each individual ﬁle. These 17 ﬁelds correspond to the
metaﬁle header in a WMF ﬁle. We say two ﬁelds have the same
type if they match. For each ﬁeld position, we counted the num-
ber of different types. We found that 12 out of the 17 ﬁeld posi-
tions have only one ﬁeld type, which ensured a correct alignment
of BNF rules from different inputs. Also, there are at most three dif-
ferent types for a ﬁeld position. By analyzing the traces and source
code, we found that the different types appear because the program
took different execution paths based on different parameters (e.g.,
whether the ﬁle size is the same as speciﬁed in the MetaFile_Size
ﬁeld). There are ten different record sequences in the 150 WMF
ﬁles. Tupni matched all record sequences correctly.
To evaluate the improvement on format coverage, we counted
the number of distinct draw record types in each WMF ﬁle and
in the 150 ﬁles together. The minimum number of distinct draw
record types in a single ﬁle is 9. However, there are 80 distinct
draw record types in the 150 ﬁles. The aggregated output of Tupni
included these 80 record types. These results show that Tupni can
signiﬁcantly improve the format coverage by aggregating multiple
inputs.
5. TUPNI FOR ZERO-DAY VULNERABIL-
ITY SIGNATURE GENERATION
We have shown that Tupni can automatically reverse engineer
the input formats with high accuracy in the ten test cases. In this
section, we evaluate Tupni’s effectiveness by using its reverse engi-
neered formats in a real-world security application: zero-day vul-
nerability signature generation. Tupni can also be used for other
security applications such as fuzzing tests. Fuzzing tests can toler-
ate more inaccuracies than vulnerability signature generation since
the latter constructs a vulnerability signature based on the reverse
engineered format while the former only uses it to generate new
test inputs.
ShieldGen [14] is a system for automatically generating a signa-
ture for an unknown vulnerability, given a zero-day attack instance.
ShieldGen requires a speciﬁcation of the data format to generate
new potential attack instances (probes). The architecture of the
original ShieldGen system is shown in Figure 6. When presented
with a potential exploit, ShieldGen feeds the suspicious trafﬁc into
an attack detector such as [10]. If the detector recognizes an attack,
ShieldGen feeds the exploit into its data analyzer component [5].
The data analyzer uses the format speciﬁcation to parse the input
and feeds the result into the probe generator. The probe generator
will generate variants of the original input (probes) and use the de-
tector to determine if a probe still exploits the vulnerability. The
Data 
Format
Zero-Day 
Attack
Data
Analyzer
Vulnerable
Program
Zero-Day 
Attack
Tupni
Suspicious
Traffic
Oracle
Attack Data
with Semantics
Suspicious
Traffic
Oracle
Attack Data
with Semantics
Probe
Probe
Generator
& Analyzer
Data 
Patch
Probe 
successful 
or failed
Probe
Probe
Generator
& Analyzer
Data 
Patch
Probe 
successful 
or failed
Figure 6: Original ShieldGen
Figure 7: ShieldGen + Tupni
most important aspect of this search is the removal of all records
from probes that are not required to exploit the vulnerability. Thus,
information about record sequences and associated constraints in
the input speciﬁcation is critical for ShieldGen.
In our experiments, we replaced the Data Analyzer module with
Tupni and replaced the Data Format with the vulnerable program.
The new architecture is shown in Figure 7. Then we repeated the
case study of the WMF vulnerability in the ShieldGen paper [14]