v
A
N-gram
Prefix
N-gram-Uniform
Baseline
4
8
12
16
20
Query Size
(a) ε = 0.1
N-gram
Prefix
N-gram-Uniform
Baseline
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e
v
A
r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e
v
A
N-gram
Prefix
N-gram-Uniform
Baseline
4
8
12
16
20
Query Size
(a) ε = 0.1
N-gram
Prefix
N-gram-Uniform
Baseline
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
4
8
12
16
20
4
8
12
16
20
Query Size
(b) ε = 1.0
Query Size
(b) ε = 1.0
Figure 2: Average relative error vs. ε on MSNBC
Figure 3: Average relative error vs. ε on STM
(i.e., the number of items in a query) and diﬀerent privacy
budgets. We divide all queries into ﬁve subsets with dif-
ferent maximal query sizes (4, 8, 12, 16 and 20). For each
subset, we generate 10,000 random queries of sizes that are
uniformly distributed at random between 1 and its maximal
size. Each item in a query is uniformly selected at random
from the item universe.
Figures 2 and 3 report the average relative errors of dif-
ferent schemes under diﬀerent query sizes over two typical
ε values 8 while ﬁxing ℓmax = 20 and nmax = 5. It can be
observed that the average relative errors of N-gram are con-
sistently lower than those of Preﬁx under all settings. The
improvements are substantial, ranging from 32% to 63%.
The relative errors of N-gram are relatively small even un-
der a strong privacy requirement (i.e., ε = 0.1).
To demonstrate the eﬀectiveness of the n-gram model, we
apply the synthetic database generation technique described
in Section 4.3.5 on non-noisy 5-grams of both MSNBC and
STM, and issue count queries on the two synthetic databases
(referred to as Baseline). The average relative errors of
Baseline give the approximation error due to the employ-
ment of the n-gram model, while the diﬀerences between
Baseline and N-gram ascribe to Laplace error. As one can
observe, the approximation errors are relatively small on
both datasets, demonstrating that the n-gram model is ef-
fective in capturing the essential sequentiality information
of a database. For Laplace error, we stress that the n-gram
model provides a general and ﬂexible framework that can
accommodate other more advanced noise injection mecha-
nisms, such as the matrix mechanism [13] and the MWEM
mechanism [11]. Hence it may require less noise added than
Laplace mechanism, resulting in smaller Laplace error.
It
may even allow a larger nmax value to be used and therefore
further reduce approximation error. Thus, we deem that
the variable-length n-gram model bears great promise for
diﬀerentially private sequential data release.
To prove the beneﬁt of our adaptive privacy budget alloca-
8According to [16], ε = 0.1 and ε = 1.0 correspond to high
and medium privacy guarantees, respectively.
tion scheme, we report the average relative errors of a vari-
ant of N-gram (referred to as N-gram-Uniform), in which
the adaptive allocation scheme is replaced by the uniform
allocation scheme described in Section 4.3.2. The improve-
ment is less obvious on M SN BC because many paths are
actually of length nmax, whereas the improvement on ST M
is noticeable, especially when ε = 0.1.
Due to the truncation operation conducted in Algorithm 1,
any count query with a size greater than ℓmax receives an an-
swer 0 on the sanitized dataset. However, we point out that
in reality it is not a problem because the true answer of
such a query is typically very small (if not 0). For many
real-life analyses (e.g., ridership analysis), the diﬀerence be-
tween such a small value and 0 is negligible. In addition, this
limitation also exists in Preﬁx and is inherent in any dif-
ferentially private mechanism because Laplace mechanism
cannot generate reliable answers on extremely small values.
6.2.2 Frequent Sequential Pattern Mining
The second data analysis task we consider is frequent se-
quential pattern mining, a more speciﬁc data mining task.
Given a positive integer number K, we are interested in the
top K most frequent sequential patterns (i.e., most frequent
subsequences) in the dataset. This data analysis task helps,
for example, a transportation agency better understand pas-
sengers’ transit patterns and consequently optimize its net-
work geometry.
We compare the performance of N-gram with Preﬁx and
FFS. All size-1 frequent patterns are excluded from the re-
sults since they are of less interest and trivial in frequent se-
quential pattern mining. We would like to clarify that FFS
actually has two assumptions: 1) all frequent patterns are of
the same length; 2) the lengths of frequent patterns are iden-
tical to the lengths of input sequences. Since generally these
two assumptions cannot be satisﬁed in a frequent sequen-
tial pattern mining task, it is not fair to directly compare
FFS with N-gram and Preﬁx. However, there are very few
approaches that support frequent sequential pattern mining
under diﬀerential privacy. Hence we still report the perfor-
646Table 5: True positive ratio vs. K value on M SN BC
(a) ε = 0.1
40
20
K value
100
N-gram 100% 90% 93% 96% 94%
85% 78% 80% 84% 86%
Preﬁx
FFS
70% 63% 57% 58% 55%
60
80
(b) ε = 1.0
40
20
K value
100
N-gram 100% 93% 97% 99% 97%
90% 82.5% 85% 90% 89%
Preﬁx
FFS
70%
63% 57% 58% 55%
80
60
Table 6: True positive ratio vs. K value on ST M
(a) ε = 0.1
40
20
100
K value
N-gram 95% 93% 93% 94% 91%
65% 68% 75% 83% 82%
Preﬁx
FFS
35% 33% 35% 36% 43%
60
80
(b) ε = 1.0
40
60
20
K value
100
N-gram 100% 100% 98% 100% 98%
70% 68% 80% 86% 85%
Preﬁx
FFS
35% 33% 35% 36% 43%
80
mance of FFS and provide insights on the key factor that
guarantees high utility on frequent sequential pattern min-
ing. For both FFS and Preﬁx, we have tested various param-
eter settings and report the best results we have obtained.
To give an intuitive impression on the performance of
these three approaches, we ﬁrst report their true positive
ratios under diﬀerent K and ε values in Tables 5 and 6.
Given K, we generate the top K most frequent sequential
patterns on both the original dataset D and the sanitized
dataset eD, which are denoted by FK (D) and FK (eD), re-
spectively. The true positive ratio is then deﬁned to be the
percentage of frequent patterns that are correctly identiﬁed,
. The results indicate that N-gram
that is,
can reliably identify the most frequent patterns in a given
database with strong privacy guarantee.
|FK (D)∩FK ( eD)|
K
To measure the utility of sanitized data more precisely, we
adopt the metric proposed in [6], which further takes into
consideration the accuracy of the supports of patterns in
FK (eD) 9. The utility loss on the sanitized dataset is deﬁned
to be the diﬀerence between FK (D) and FK (eD), that is,
|sup(Fi, FK (D)) − sup(Fi, FK (eD))|
PFi∈FK (D)
sup(Fi, FK (D))
K
,
If Fi /∈
port of every frequent pattern); if the metric equals 1, it
where sup(Fi, FK (D)) and sup(Fi, FK (eD)) denote the sup-
ports of Fi in FK (D) and FK (eD), respectively.
FK (eD), sup(Fi, FK (eD)) = 0. Therefore, if the metric equals
0, it means that FK (D) is identical to FK (eD) (even the sup-
implies that FK (D) and FK (eD) are totally diﬀerent.
In Figures 4 and 5, where ℓmax = 20 and nmax = 5, we
can observe that our proposal signiﬁcantly outperforms the
9The support of a pattern is the number of its occurrences
in a database.
N-gram
Prefix
FFS
20
40
60
80
100
K Value
(a) ε = 0.1
N-gram
Prefix
FFS
s
s
o
L
y
t
i
l
i
t
U
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
s
s
o
L
y
t
i
l
i
t
U
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
20
40
60
80
100
K Value
(b) ε = 1.0
Figure 4: Utility loss vs. K on MSNBC
other two approaches. In addition, for the frequent patterns
that are correctly identiﬁed, the relative errors of their sup-
ports are typically very small even when ε = 0.1. The main
reason is that N-gram extracts the essential information of
a database in terms of a set of n-grams, which are actually
the most frequent patterns in the database. This fact al-
lows N-gram to perform well even under a small ε value. In
contrast, in Preﬁx, the noise added to a frequent pattern’s
count accumulates quickly in proportion to the number of
longer sequences that contain this frequent pattern. The
major limitation of FFS is its preﬁx data structure, which
generates frequent patterns based on very short preﬁxes.
7. CONCLUSION
In this paper, we proposed a novel approach to diﬀeren-
tially private sequential data publication based on a variable-
length n-gram model. This model extracts the essential
information of a sequential database in terms of a set of
variable-length n-grams whose counts are relatively large
and therefore subject to lower Laplace error. We devel-
oped a set of key techniques that are vital to the success
of the n-gram model. Furthermore, we designed a synthetic
sequential database construction method, which allows pub-
lished n-grams to be used for a wider range of data analysis
tasks. Extensive experiments on real-life datasets proved
that our solution substantially outperforms state-of-the-art
techniques [16], [4] in terms of count query and frequent
sequential pattern mining.
8. ACKNOWLEDGMENTS
This work was supported in part by a grant from the EIT
ICT Labs to INRIA. The authors would like to thank the
anonymous reviewers for their constructive comments.
9. REFERENCES
[1] O. Abul, F. Bonchi, and M. Nanni. Never walk alone:
Uncertainty for anonymity in moving objects
databases. In ICDE, pages 376–385, 2008.
647N-gram
Prefix
FFS
r
o
r
r
E
e
v
i
t
l
a
e
R
e