title:Scouts: Improving the Diagnosis Process Through Domain-customized
Incident Routing
author:Jiaqi Gao and
Nofel Yaseen and
Robert MacDavid and
Felipe Vieira Frujeri and
Vincent Liu and
Ricardo Bianchini and
Ramaswamy Aditya and
Xiaohang Wang and
Henry Lee and
David A. Maltz and
Minlan Yu and
Behnaz Arzani
Scouts: Improving the Diagnosis Process Through
Domain-customized Incident Routing
Jiaqi Gao★, Nofel Yaseen⋄, Robert MacDavid∗, Felipe Vieira Frujeri◦, Vincent Liu⋄, Ricardo Bianchini◦
Ramaswamy Aditya§, Xiaohang Wang§, Henry Lee§, David Maltz§, Minlan Yu★, Behnaz Arzani◦
∗Princeton University ◦Microsoft Research §Microsoft
⋄University of Pennsylvania
★Harvard University
ABSTRACT
Incident routing is critical for maintaining service level objectives
in the cloud: the time-to-diagnosis can increase by 10× due to mis-
routings. Properly routing incidents is challenging because of the
complexity of today’s data center (DC) applications and their de-
pendencies. For instance, an application running on a VM might
rely on a functioning host-server, remote-storage service, and vir-
tual and physical network components. It is hard for any one team,
rule-based system, or even machine learning solution to fully learn
the complexity and solve the incident routing problem. We pro-
pose a different approach using per-team Scouts. Each teams’ Scout
acts as its gate-keeper — it routes relevant incidents to the team
and routes-away unrelated ones. We solve the problem through
a collection of these Scouts. Our PhyNet Scout alone — currently
deployed in production — reduces the time-to-mitigation of 65% of
mis-routed incidents in our dataset.
CCS CONCEPTS
• Computing methodologies → Machine learning; • Networks
→ Data center networks;
KEYWORDS
Data center networks; Machine learning; Diagnosis
ACM Reference Format:
Jiaqi Gao, Nofel Yaseen, Robert MacDavid, Felipe Vieira Frujeri, Vincent
Liu, RicardoBianchini, Ramaswamy Aditya, Xiaohang Wang, Henry Lee,
David Maltz, Minlan Yu, Behnaz Arzani. 2020. Scouts: Improving the Di-
agnosis Process Through Domain-customized Incident Routing. In Annual
conference of the ACM Special Interest Group on Data Communication on the
applications, technologies, architectures, and protocols for computer communi-
cation (SIGCOMM ’20), August 10–14, 2020, Virtual Event, NY, USA. ACM,
New York, NY, USA, 17 pages. https://doi.org/10.1145/3387514.3405867
1 INTRODUCTION
For cloud providers, incident routing — taking an issue that is too
complex for automated techniques and assigning it to a team of
engineers — is a critical bottleneck to maintaining availability and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7955-7/20/08...$15.00
https://doi.org/10.1145/3387514.3405867
service-level objectives. When incidents are mis-routed (sent to the
wrong team), their time-to-diagnosis can increase by 10× [21].
A handful of well-known teams that underpin other services
tend to bear the brunt of this effect. The physical networking team
in our large cloud, for instance, is a recipient in 1 in every 10 mis-
routed incidents (see §3). In comparison, the hundreds of other
possible teams typically receive 1 in 100 to 1 in 1000. These findings
are common across the industry (see Appendix A).
Incident routing remains challenging because modern DC ap-
plications are large, complex, and distributed systems that rely
on many sub-systems and components. Applications’ connections
to users, for example, might cross the DC network and multiple
ISPs, traversing firewalls and load balancers along the way. Any of
these components may be responsible for connectivity issues. The
internal architectures and the relationships between these compo-
nents may change over time. In the end, we find that the traditional
method of relying on humans and human-created rules to route
incidents is inefficient, time-consuming, and error-prone.
Instead, we seek a tool that can automatically analyze these
complex relationships and route incidents to the team that is most
likely responsible; we note that machine learning (ML) is a potential
match for this classification task. In principle, a single, well-trained
ML model could process the massive amount of data available from
operators’ monitoring systems—too vast and diverse for humans—
to arrive at an informed prediction. Similar techniques have found
success in more limited contexts (e.g., specific problems and/or
applications) [11, 15, 22, 25, 73]. Unfortunately, we quickly found
operationalizing this monolithic ML model comes with fundamental
technical and practical challenges:
A constantly changing set of incidents, components, and monitoring
data: As the root causes of incidents are addressed and components
evolve over time, both the inputs and the outputs of the model are
constantly in flux. When incidents change, we are often left without
enough training data and when components change, we potentially
need to retrain across the entire fleet.
Curse of dimensionality: A monolithic incident router needs to in-
clude monitoring data from all teams. This large resulting feature
vector leads to “the curse of dimensionality” [4]. The typical solu-
tion of increasing the number of training examples in proportion to
the number of features is not possible in a domain where examples
(incidents) are already relatively rare events.
Uneven instrumentation: A subset of teams will always have gaps
in monitoring, either because the team has introduced new compo-
nents and analytics have not caught up, or because measuring is just
hard, e.g., in active measurements where accuracy and overhead
are in direct contention [34].
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gao et al.
Limited visibility into every team: For the same reasons that it is
difficult for teams to have expertise in all surrounding components,
it is difficult for us to understand the appropriate feature sets from
each and every team.
Rather than building a single, monolithic predictor, we argue a
piecewise solution based on a collection of (strategically assigned)
per-team predictors, a.k.a. Scouts, is more useful. Scouts are low-
overhead, low-latency, and high-accuracy tools that predict, for a
given team, whether the team should be involved. They are built by
the team to which they apply and are amenable to partial deploy-
ment. Scouts address the above challenges: they only need to adapt
to changes to their team and its components (instead of all changes),
they operate over a more limited feature set (no longer suffer the
curse of dimensionality), they limit the need for understanding
the internals of every team (they only need to encode information
about the team they are designed for and its local dependencies),
and only require local instrumentation. Scouts can utilize a hybrid
of supervised and unsupervised models to account for changes to
incidents (see §5) and can provide explanations as to why they de-
cided the team is (not) responsible. Operators can be strategic about
which Scouts they need: they can build Scouts for teams (such as
our physical networking team) that are inordinately affected by
mis-routings. Given a set of Scouts, operators can incrementally
compose them, either through a global routing system or through
the existing manual process.
We designed, implemented, and deployed a Scout for the physical
networking team of a large cloud.1 We focus on this team as, from
our study of our cloud and other operators, we find the network —
and specifically the physical network — suffers inordinately from
mis-routing (see §3). This team exhibits all of the challenges of
Scout construction: diverse, dirty datasets; complex dependencies
inside and outside the provider; many reliant services; and frequent
changes. As the team evolves, the framework we developed adapts
automatically and without expert intervention through the use of
meta-learning techniques [46].
These same techniques can be used to develop new “starter”
Scouts as well. However, even for teams that do not build a Scout,
e.g., if instrumentation is difficult or dependencies are hard to dis-
entangle, they still benefit from Scouts: their incidents spend less
time at other teams, and they receive fewer mis-routed incidents
belonging to Scout-enabled teams. In fact, we show even a single,
strategically deployed Scout can lead to substantial benefit.
Our Scout has precision/recall ≥ 98%, and it can reduce over 60%
of the investigation time of many incidents. Our contributions are:
1) An investigation of incident routing based on our analysis of
our production cloud. As the data we use is of a sensitive nature,
we limit our discussion to those incidents which impacted the
physical networking team (arguably the most interesting for this
conference), but the scope of the study was much broader. We
augment our results with analysis of public incident reports [2, 7]
and a survey of other operators (Appendix A).
2) The introduction of the concept of a distributed incident routing
system based on Scouts. We show the improvements such a system
can bring through trace-driven simulations (Appendix D).
1To demonstrate the overall benefit of Scouts, we run trace-driven simulations of
broader deployments (Appendix D).
3) The design of a Scout for Microsoft Azure’s physical networking
team accompanied by a framework to enable its evolution as the
team’s monitoring systems, incidents, and responsibilities change.
4) A thorough evaluation of the deployed PhyNet Scout and analysis
of incidents in our cloud from the past year and a discussion of the
challenges the Scout encountered in practice.
This paper is the first to propose a decomposed solution to the
incident routing problem. We take the first step in demonstrating
such a solution can be effective by building a Scout for the PhyNet
team of Microsoft Azure. This team was one of the teams most
heavily impacted by the incident routing problem. As such, it was a
good first candidate to demonstrate the benefits Scouts can provide;
we leave the detailed design of other teams’ Scouts for future work.
2 BACKGROUND: INCIDENT ROUTING
Incidents constitute unintended behavior that can potentially im-
pact service availability and performance. Incidents are reported
by customers, automated watchdogs, or discovered and reported
manually by operators.
Incident routing is the process through which operators decide
which team should investigate an incident. In this context, we
use team to broadly refer to both internal teams in the cloud and
external organizations such as ISPs. Today, operators use run-books,
past-experience, and a natural language processing (NLP)-based
recommendation system (see §7), to route incidents. Specifically,
incidents are created and routed using a few methods:
1) By automated watchdogs that run inside the DC and monitor
the health of its different components. When a watchdog uncovers
a problem it follows a built-in set of rules to determine where it
should send the incident.
2) As Customer Reported Incidents (CRIs) which go directly to a
24 × 7 support team that uses past experience and a number of
specialized tools to determine where to send the incident. If the
cause is an external problem, the team contacts the organization
responsible. If it is internal, it is sent to the relevant team where it
is acknowledged by the on-call engineer.
It is important for every incident to be mitigated as quickly as
possible, even if it does not result in SLO violations—prolonged
investigations reduce the resilience of the DC to future failures [12,
33]: any time saved from better incident routing is valuable.
Routing incidents can be excruciatingly difficult as modern DC
applications are large and complex distributed systems that rely on
many other components. This is true even for incidents generated
by automated watchdogs as they often observe the symptom —
which can be far-reaching: a VM’s frequent rebooting can be an
indication of a storage problem or a networking issue [15, 73].
3 INCIDENTS IN THE WILD
To understand the impact of incident routing and why incidents
are sometimes mis-routed, we investigate incidents in a large cloud.
In particular, we examine, in depth, the internal logs of incidents
involving the physical networking team (PhyNet) of a large cloud.
These logs cover nine months and include records of the teams
the incident was routed through, the time spent in each team, and
logs from the resolution process. We have normalized the absolute
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 2: Time to diagnosis of incidents going through a sin-
gle vs going through multiple teams. The numbers are nor-
malized by the maximum investigation time in the dataset.
When an incident comes in, it is common to send it to the team
where the issue was first detected. For example, if a customer is un-
able to connect to a database, the database team is typically alerted
first. When operators find the database is functioning correctly
(e.g. CPU, disk, and query execution times seem healthy and there
are no changes in login times), they involve other teams. Common
reasons for involving other teams are:
Engineers from different teams bring a wide range of do-
main knowledge to determine culpability. Often, the involve-
ment of multiple teams is due to a lack of domain-knowledge in a
particular area. In our example, the database expert may not have
the networking expertise to detect an ongoing network failure or its
cause. Team-level dependencies are deep, subtle, and can be hard to
reason about — exacerbating the problem. In our database example,
a connectivity issue may spur engineers to check if the physical
network, DNS, software load balancers, or virtual switches are at
fault before looking at other possible (and less-likely) causes. The
most common cause of mis-routing is when a team’s component is
one of the dependencies of the impacted system and thus a legiti-
mate suspect, but not the cause. In 122 out of 200 incidents, there
was at least one such team that was unnecessarily engaged.