10
Num of LBM Rules
100
Figure 8: LBM overhead in μs based on varying numbers of
rules. While the general overhead increases as the number of
rules increases, the overhead of going through each individual
rule decreases, thus the total overhead is essentially amortized.
Takeaway: the overhead introduced by LBM is negligible
for applications and for the system as a whole.
E. Scalability
To understand the scalability of LBM, we load the rule
“USB-3” into the RX path once, 10 times, and 100 times.
As in the micro-benchmark, we record 10K USB packets
generated by the USB WiFi adapter and compute the overhead
of LBM going through these rules for each packet. As shown
in Figure 8, while the total overhead increases as the number
of rules increases, the average overhead of checking individual
rules decreases. The average overhead was 0.83 μs when there
was only one rule loaded. It decreased to 0.32 μs when there
were 100 rules loaded. Under JIT, the overhead was further
reduced to 0.23 μs. This might be the result of increased
cache hits from accessing the same rule in a loop. Even for
different rules, it is possible to observe this amortization effect,
as long as each rule occupies a different cache line. Also, in
(cid:26)(cid:24)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:45 UTC from IEEE Xplore.  Restrictions apply. 
s
/
B
M
n
i
t
u
p
h
g
u
o
r
h
T
600
500
400
300
200
100
0
128KB
1MB
s
/
B
M
n
i
t
u
p
h
g
u
o
r
h
T
120
100
80
60
40
USBFILTER Stock-LBM
LBM
LBM-JIT
4
8
Stock-USBFirewall
USBFirewall
Stock-USBFILTER
USBFILTER
Stock-LBM
LBM
LBM-JIT
64
128
16
32
Block Size in KB
9: LBM vs. USBFILTER benchmark
using
Figure
filebench with 10 same
loaded respectively.
LBM introduces a minimum overhead comparing to the stock
kernel and performs better than USBFILTER in general.
rules
general, more complicated rules will also induce more runtime
overhead.
We
then
compare LBM with USBFILTER using
filebench.3 Except the difference in kernel versions4, we
ran LBM and USBFILTER on the same physical machine. To
set up the benchmark, we load “USB-3” into the RX path 10
times on LBM and load an equivalent rule the same number
of times into USBFILTER. As shown in Figure 9, both LBM
and LBM-JIT show a minimum overhead comparing to the
stock kernel, and provide better throughput than USBFILTER
regardless the mean ﬁle size. This could be the result of both
kernel code improvements across versions and the design of
LBM (e.g., due to its use of eBPF). The throughput boost
is even clearer when the mean ﬁle size is 1MB and JIT is
enabled. Compared to USBFILTER, LBM-JIT improves the
throughput by roughly 60%.
Finally, we compare LBM with USBFILTER and USBFire-
wall using dd on VFAT ﬁlesystem with direct I/O enabled to
bypass the page cache. Since USBFirewall does not support
loading rules from the user space directly, we statically built
these 10 rules when compiling USBFirewall. As shown in Fig-
ure 10, comparing to their stock versions, all the solutions
show minimum overheads. The throughput of USBFirewall
does not vary much based on the block size. We tried both
the native FreeBSD version of dd and the GNU version. Both
demonstrate similar throughput regardless the block size. We
double check this by increasing the block size to 1 MB. When
the block size is beyond 16 KB, both LBM and USBFILTER
show better throughput
than USBFirewall. Similarly, both
LBM and LBM-JIT have better throughput than USBFILTER.
Takeaway: compared to other state-of-the-art solutions,
LBM provides better scalability and performance.
3 Due to a kernel bug within USBFILTER, the front USB 3.0 ports could
not support USB 3.0 devices. We switched to the rear USB 3.0 ports in this
testing. We also tried to run USBFirewall. Unfortunately, FreeBSD does not
support filebench or EXT4 ﬁlesystem used by our external drive.
4LBM is running Linux kernel 4.13 while USBFILTER runs 3.13.
Figure 10: LBM vs. USBFILTER vs. USBFirewall benchmark
using dd with 10 same rules loaded respectively. Comparing to
their stock versions, all the solutions show minimum overhead.
USBFirewall does not vary much based on the block size.
LBM performs better than USBFirewall and USBFILTER
when block size is beyond 16 KB in general.
VI. DISCUSSION
A. LBM vs. USBFILTER vs. USBFirewall
The LBM ﬁlter DSL is more expressive than the USB-
FILTER policy, which only supports concatenating equality
checks using logical AND. The LBM ﬁlter DSL supports
different arithmetic and logical operations, as well as changing
of operation precedence using parentheses. Compared to US-
BFILTER, LBM USB also doubles the number of protocol
ﬁelds exposed to the user space, although LBM does not
support pinning applications to peripherals.5 Nevertheless,
LBM enables more complicated and powerful ﬁltering rules
than USBFILTER. Besides, any LUM can be converted into
an LBM module without much hassle. LBM USB has also
fully replicated functionality provided by USBFirewall, which
required a kernel recompile and reboot
to make any rule
changes.
B. L2CAP Signaling in Bluetooth
Unlike L2CAP signaling in BLE, where each L2CAP packet
only carries a single command, L2CAP signaling in the Blue-
tooth classic may have packets containing multiple commands.
As we saw in the BlueBorne defense case study, if there is
a malicious conﬁguration response command contained in a
L2CAP signaling packet, the entire payload will be dropped,
including other “innocent” commands if they exist.
One possible solution to such coarse-grained drops is to
separate each command from the same L2CAP signaling
packet into standalone packets. This requires packet parsing
and duplication in the early stage. Another solution is to add
a new customized hook in the place where each command
is extracted by the L2CAP stack. Our current implementation
5USBFILTER instrumented some USB device driver to support application
pinning. It is ad-hoc, rather than a generic method.
(cid:26)(cid:24)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:45 UTC from IEEE Xplore.  Restrictions apply. 
does not apply either solution, for performance and simplicity
considerations. From a security perspective, if one command
from a certain device is recognized as malicious, it seems
reasonable to drop other commands from the same device.
C. BPF Memory Write
For security considerations, we forbid memory writes in
LBM eBPF programs. While this restriction improves the
kernel’s security posture towards user-loaded code, we also
lose a powerful feature provided by eBPF and BPF helpers—
packet mangling, which allows for ﬁelds to be changed on
the ﬂy. This feature has been employed by the networking
subsystem, e.g., for changing the source IP address and/or
the destination port number. For LBM, one potential use of
memory write is removing only malicious commands while
keeping others within the same L2CAP signaling packet intact.
As an intermediate step to enable memory write in LBM
programs, we can restrict the memory write ability to certain
BPF helpers. As long as these BPF helpers are safe, the
BPF veriﬁer can still verify these programs by rejecting store
instructions as before.
D. BPF Helper Kernel Modules
Ideally, we should allow BPF helpers for each subsystem
to be implemented as a standalone kernel module, which can
be plugged in when needed. Unfortunately, this is forbidden
by the current eBPF design, and we follow the same design
principles for similar reasons. First of all, BPF helpers are
like syscalls in a system. The number of a BPF helper is like
the syscall number, which is part of the Application Binary
Interface (ABI) of the system. Although by introducing LBM,
we have essentially namespaced LBM BPF helpers from other
general and networking-speciﬁc helpers, these helpers still
share the same LBM namespace regardless their respective
subsystems. As a result, the number of a LBM BPF helper
implemented within a kernel module cannot be decided until
all used numbers are known, including the ones deﬁned by
LBM internals and those deﬁned in other BPF helper modules.
A possible solution here is to further namespace LBM BPF
helpers per subsystem, e.g., have USB helpers always start
with 100, Bluetooth helpers with 200, etc. Note that
this
solution would consequently limit the number of helpers each
subsystem could have.
E. LLVM Support
LLVM began to support eBPF as an architectural backend in
early 2015 [71]. A typical workﬂow involves writing an eBPF
ﬁlter in C and compiling it using Clang. eBPF loaders such as
tc are able to parse the generated ELF ﬁle and load it into the
kernel [18]. While LLVM brings C into eBPF programming,
easing ﬁlter writing for C developers, we realized that eBPF
programming might still be challenging for sysadmins, who
need an easy and intuitive way to write eBPF ﬁlters; we
designed the LBM ﬁlter DSL with this in mind. We are
planning to support LLVM as well by adding a new eBPF
loader into LBM.
VII. LIMITATIONS
A. Stateless vs. Stateful Policy
LBM ﬁlters are designed to be policy-independent, although
a large part of the case studies presented stateless polices.
Whether the policy is stateless or stateful essentially depends
on what protocol ﬁelds and packet data are exposed to the
user space. For example, USB does not have a “session”
concept, and we could write useful LBM ﬁlters based on
just the device information (a.k.a., stateless policy). Bluetooth
has the “connection” concept in the L2CAP layer (like TCP
connections), so we could write LBM ﬁlters using this ﬁeld
(a.k.a., stateful policy). Besides protocols ﬁelds deﬁned by
standards, the Linux kernel also maintains some bookkeeping
data structures, e.g., counters. Exposing these kernel ﬁelds
would also help to create stateful polices.
The current LBM USB and Bluetooth implementations
focus on exposing basic protocol ﬁelds rather than stateful
variables. Nevertheless, we have noticed the potential of
stateful policies. For instance, we could write a stateful policy
to detect BleedingBit [12] attacks by observing a sequence
of multiple BLE advertising packets with a certain bit off
followed by another BLE advertising packet with that bit on.
B. DMA-Oriented Protocols
We have not instantiated LBM on Thunderbolt, HDMI, or
DisplayPort, although it is indeed possible to support these
DMA-oriented protocols using LBM.6 Since LBM works
at the packet layer, we are able to ﬁlter packets for these
protocols as long as the concept of packet, given a protocol,
is deﬁned by the standard and implemented by the kernel.
For example, DisplayPort deﬁnes different packets to carry
different payloads such as stream and audio [46], implemented
as such within the kernel. Thunderbolt, however, is a propri-
etary standard. It is not clear whether the protocol itself is
packetized, and the only packet-level message available within
the kernel is Thunderbolt control request/response instead of
data transfer. Another challenge to supporting these protocols
comes from determining the proper hook placement for com-
plete mediation. DisplayPort is not a standalone subsystem
but rather a component of Direct Rendering Manager (DRM)
inside the kernel. Thunderbolt does not have a core layer but
only provides few drivers due to the limited hardware devices.
C. Operating Systems Dependency
Although LBM is built upon the Linux kernel, it is possible
to apply LBM to other operating systems. To achieve that, we
need the target operating system to support a generic in-kernel
packet ﬁltering mechanism such as eBPF. The classic BPF is
not enough because LBM relies on calling kernel APIs within
ﬁlters to access different kernel data. While it is non-trivial to
extend the classic BPF to eBPF, some porting effort has been
done for FreeBSD to support eBPF [35]. The other require-
ment is a software architecture enabling complete-mediation
hook placement for different peripherals. For instance, it is
6USB is also DMA-oriented.
(cid:26)(cid:24)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:45 UTC from IEEE Xplore.  Restrictions apply. 
possible to mediate all USB packets within the FreeBSD
USB subsystem, as proven by USBFirewall. Nevertheless, it
might be challenging to port LBM to Windows, since it has
a different packet ﬁltering mechanism [85] and it is closed-
source.
D. lbmtool Limitations
LBMTOOL currently does not support LBM ﬁlter consis-
tency checking, meaning it is possible to have two LBM
ﬁlters conﬂict with each other. Regarding eBPF instruction
generation, LBMTOOL does not support stack allocations when
the return value of BPF helpers is beyond 8 bytes (width of an
eBPF register). Manual assembly is needed to manipulate the
stack for those BPF helpers. LBMTOOL also does not support
lazy evaluation on BPF helpers. They are always called at ﬁrst
to retrieve all the values of protocols ﬁelds needed before the
actual evaluation of the LBM ﬁlter DSL expression. These are
merely the current limitations of the custom compiler itself and
could be eliminated with additional code.
VIII. RELATED WORK
Peripheral Security Defenses: A number of solutions have
considered aspects of defenses against malicious peripherals.
By treating USB kernel drivers as capabilities, GoodUSB [76]
asks for user’s expectation about the device before loading
the drivers. Cinch [9] interposes on the USB bus by isolating
the suspicious USB device within a VM environment, with
the help of IOMMU and hypervisors, but imposes substantial
performance overhead and considerable architectural changes
to systems it is deployed upon. USBFILTER [79] is a USB
packet ﬁltering mechanism built into the Linux kernel. Users
can write simple ﬁltering rules and pass them into the kernel
space. USBFirewall [43] protects the USB protocol stack
within FreeBSD from malformed packets by generating the
USB packet parser from Haskell. Other solutions focus on
developing more secure devices; for example Kells [20] and
ProvUSB [77] protect USB devices from malicious hosts
at the granularity of partitions and blocks, respectively, but
require the deployment of new peripheral devices. Solutions
such as FirmUSB [39] allow analysis of a device for malicious
intent but require a means of accessing its ﬁrmware. For more
details regarding other related defenses, we refer readers
to a systematic study on USB security [80]. Thunderbolt 3
also introduced security levels, and boltctl [44] is used to
set security levels for different peripherals on Linux. These
security levels are designed to control the creation of PCIe
channels from peripherals rather
than high-level packets.
As previously discussed, LBM is designed as a generic
framework working at the packet layer, not only enabling
existing solutions such as USBFILTER and USBFirewall,
but also covering other peripherals such as Bluetooth and NFC.
eBPF-based Solutions: BPF Compiler Collections (BCC) [42]
provides a Python interface for writing, compiling, and loading
eBPF programs. Its backend is still LLVM and C programing.
eXpress DataPath (XDP) [36] provides eBPF hooks within
the NIC drivers, ﬁltering packets before skb is created to
store the packet. Network Flow Processor (NFP) [45] allows
ﬁltering packets within the NIC by JITing eBPF programs
into native NIC instructions and running them on the NIC
directly. eBPF tracing tools [33] provide an alternative for
DTrace on Linux. Bpﬁlter [73] is an ongoing project trying