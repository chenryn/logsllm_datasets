title:Sketch-based change detection: methods, evaluation, and applications
author:Balachander Krishnamurthy and
Subhabrata Sen and
Yin Zhang and
Yan Chen
Sketch-based Change Detection: Methods, Evaluation, and
Applications
Balachander Krishnamurthy, Subhabrata Sen, Yin Zhang
Yan Chen∗
AT&T Labs–Research; 180 Park Avenue
Florham Park, NJ, USA
{bala,sen,yzhang}@research.att.com
University of California
Berkeley, CA, USA
PI:EMAIL
ABSTRACT
Trafﬁc anomalies such as failures and attacks are commonplace
in today’s network, and identifying them rapidly and accurately is
critical for large network operators. The detection typically treats
the trafﬁc as a collection of ﬂows that need to be examined for
signiﬁcant changes in trafﬁc pattern (e.g., volume, number of con-
nections). However, as link speeds and the number of ﬂows in-
crease, keeping per-ﬂow state is either too expensive or too slow.
We propose building compact summaries of the trafﬁc data using
the notion of sketches. We have designed a variant of the sketch
data structure, k-ary sketch, which uses a constant, small amount
of memory, and has constant per-record update and reconstruction
cost. Its linearity property enables us to summarize trafﬁc at various
levels. We then implement a variety of time series forecast models
(ARIMA, Holt-Winters, etc.) on top of such summaries and detect
signiﬁcant changes by looking for ﬂows with large forecast errors.
We also present heuristics for automatically conﬁguring the model
parameters.
Using a large amount of real Internet trafﬁc data from an op-
erational tier-1 ISP, we demonstrate that our sketch-based change
detection method is highly accurate, and can be implemented at
low computation and memory costs. Our preliminary results are
promising and hint at the possibility of using our method as a build-
ing block for network anomaly detection and trafﬁc measurement.
Categories and Subject Descriptors
C.2.3 [Computer-Communications Networks]: Network Opera-
tions—network monitoring
General Terms
Measurement, Algorithms
Keywords
Change Detection, Network Anomaly Detection, Data Stream Com-
putation, Sketch, Time Series Analysis, Forecasting
∗
Work done while at AT&T Labs–Research
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’03, October 27–29, 2003, Miami Beach, Florida, USA.
Copyright 2003 ACM 1-58113-773-7/03/0010 ...$5.00.
1.
INTRODUCTION
Trafﬁc anomalies are an integral part of daily life for today’s
network operators. Some trafﬁc anomalies are expected or unan-
ticipated but tolerable. Others are often indications of performance
bottlenecks due to ﬂash crowds [25], network element failures, or
malicious activities such as denial of service attacks (DoS) [23]
and worms [28]. Suitable motivation exists to process massive data
streams (available from diverse sources) quickly to examine them
for anomalous behavior.
Two basic approaches to network anomaly detection are com-
mon. The ﬁrst approach is the signature-based approach. It detects
trafﬁc anomalies by looking for patterns that match signatures of
known anomalies. For example, Moore et al. [29] infer DoS activi-
ties based on address uniformity, a property shared by several pop-
ular DoS toolkits. Signature-based methods have been extensively
explored in the literature and many software systems and toolkits
such as Bro [31] and Snort [32] have been developed and are being
used. One limitation of this approach is the requirement that the
anomaly signatures be known in advance; thus it cannot be applied
to identify new anomalies. Also, a malicious attacker can evade
signature-based detection systems by garbling the signatures. One
can see a parallel in the failure of ﬁlter-based spam ﬁghting systems
where spammers introduce random hashes in the spam messages.
The second approach is the statistics-based approach, which does
not require prior knowledge about the nature and properties of anoma-
lies and therefore can be effective even for new anomalies or vari-
ants of existing anomalies. A very important component of statistics-
based approach is change detection. It detects trafﬁc anomalies by
deriving a model of normal behavior based on the past trafﬁc his-
tory and looking for signiﬁcant changes in short-term behavior (on
the order of minutes to hours) that are inconsistent with the model.
Our goal in this work is to come up with an efﬁcient, accurate,
and scalable change detection mechanism for detecting signiﬁcant
changes in massive data streams with a large number of ﬂows.
1.1 Change detection: existing techniques and
limitations
Change detection has been extensively studied in the context of
time series forecasting and outlier analysis [35, 36, 12, 13]. The
standard techniques include different smoothing techniques (such
as exponential smoothing or sliding window averaging), the Box-
Jenkins ARIMA modeling [6, 7, 2], and ﬁnally the more recent
wavelet-based techniques [4, 3].
Prior works have applied these techniques to network fault de-
tection and intrusion detection. Examples in fault detection include
[22, 26, 38]. Feather et al.identify faults based on statistical devi-
ations from normal trafﬁc behavior [18]; a method of identifying
aberrant behavior by applying thresholds in time series models of
network trafﬁc is described in [9]. Methods for intrusion detec-
tion include neural networks [20], Markov models [40], and clus-
tering [34]. Barford et al.recently provide a characterization of dif-
ferent types of anomalies [4] and propose wavelet-based methods
for change detection [3].
Unfortunately, existing change detection techniques can typi-
cally only handle a relatively small number of time series. While
this may sufﬁce for detecting changes in highly aggregated network
trafﬁc data (e.g., SNMP link counts with 5 minute sample interval),
they cannot scale up to the needs at the network infrastructure (e.g.,
ISP) level. At an ISP level, trafﬁc anomalies may be buried in-
side the aggregated trafﬁc, mandating examination of the trafﬁc at
a much lower level of aggregation (e.g., IP address level) in order
to expose them. Given today’s trafﬁc volume and link speeds, the
detection method has to be able to handle potentially several mil-
lions or more of concurrent network time series. Directly applying
existing techniques on a per-ﬂow basis cannot scale up to the needs
of such massive data streams. Recent research efforts have been
directed towards developing scalable heavy-hitter detection tech-
niques for accounting and anomaly detection purposes [17]. Note
that heavy-hitters do not necessarily correspond to ﬂows experienc-
ing signiﬁcant changes and thus it is not clear how their techniques
can be adapted to support change detection.
1.2 Data stream computation and sketches
In the database research community, however, computation over
massive data streams has been an active research area over the past
several years. The emerging ﬁeld of data stream computation deals
with various aspects of computation that can be performed in a
space- and time-efﬁcient fashion when each tuple in a data stream
can be touched only once (or a small number of times). A good
survey of the algorithms and applications in data stream compu-
tation can be found in [30]. One particularly powerful technique
is sketch [24, 21, 15], a probabilistic summary technique proposed
for analyzing large streaming datasets. Sketches avoid keeping per-
ﬂow state by dimensionality reduction, using projections along ran-
dom vectors. Sketches have some interesting properties that have
proven very useful in data stream computation: they are space efﬁ-
cient, provide provable probabilistic reconstruction accuracy guar-
antees, and are linear (i.e., sketches can be combined in an arith-
metical sense).
1.3 Our contribution
In this work, we incorporate data stream computation techniques
into change detection. Our solution, labeled sketch-based change
detection, is the ﬁrst one, to the best of our knowledge, that is ca-
pable of detecting signiﬁcant changes in massive data streams with
a large number of network time series. With sketch-based change
detection, we ﬁrst build compact summaries of the trafﬁc data us-
ing sketches. We have designed a variant of the sketch data struc-
ture, k-ary sketch, which uses a constant, small amount of memory,
and has constant per-record update and reconstruction cost. We
then implement a variety of time series forecast models (ARIMA,
Holt-Winters, etc.) on top of such summaries and detect signiﬁcant
changes by looking for ﬂows with large forecast errors. Being able
to compute signiﬁcant differences in the list of top ﬂows quickly
can point towards potential anomalies. Depending on the length of
the time period for which we compute forecasts and the duration
of signiﬁcant changes, we can accurately identify the presence of
an anomaly. Note that an anomaly can be a benign surge in traf-
ﬁc (like a ﬂash crowd) or an attack. We also present heuristics for
conﬁguring the model parameters.
We demonstrate using a large amount of real Internet trafﬁc data
that our sketch-based change detection method is highly accurate
when compared with per-ﬂow analysis, and can be implemented
at low computation and memory costs. Our evaluation shows that
we can reconstruct lists of the top ﬂows in a time period efﬁciently
and accurately; we are also able to achieve similar forecast errors
when compared with per-ﬂow techniques. While our techniques
have not yet been directly applied to anomaly detection, our pre-
liminary results in change detection are promising and we believe
that our method can serve as a building block for network anomaly
detection.
1.4 Paper outline
The rest of the paper is organized as follows: Section 2 gives an
overview of the framework of our sketch-based change detection,
followed by detailed discussions of different modules in Section 3.
Section 4 describes the experimental setup. Section 5 presents
key portions of the results of our extensive testing of sketch-based
change detection on different large and real datasets. We summa-
rize our ongoing research in Section 6 and conclude the paper in
Section 7.
2. OVERVIEW
2.1 Data stream model
Over the past several years, various models have been proposed
to describe data streams, including Time Series Model, Cache Reg-
ister Model, and Turnstile Model [30]. We use the the most general
one—the Turnstile Model.
Speciﬁcally, let I = α1, α2,··· , be an input stream that arrives
sequentially, item by item. Each item αi = (ai, ui) consists of
= {0, 1,· ·· , u − 1}, and a (possibly negative)
a key ai ∈ [u]
update ui ∈ (cid:0). Associated with each key a ∈ [u] is a time varying
signal A[a]. The arrival of each new data item (ai, ui) causes the
underlying signal A[ai] to be updated: A[ai]+ = ui. The goal
of change detection is to identify all those signals with signiﬁcant
changes in their behavior.
def
The above model is very general and one can instantiate it in
many ways with speciﬁc deﬁnitions of the key and updates. In the
context of network anomaly detection, the key can be deﬁned using
one or more ﬁelds in packet headers such as source and destination
IP addresses, source and destination port numbers, protocol number
etc.
It is also possible to deﬁne keys with entities like network
preﬁxes or AS numbers to achieve higher levels of aggregation.
The update can be the size of a packet, the total bytes or packets in
a ﬂow (when ﬂow-level data is available). To keep the parameter
space within a manageable size, however, we only use destination
IP address and bytes in the experiments discussed in this paper.
Alternative choice of keys and values may affect the running time,
but we expect the accuracy results to be quite similar.
2.2 Sketch-based change detection
In an ideal environment with inﬁnite resources, we can perform
time series forecasting and change detection on a per-ﬂow basis.
Speciﬁcally, we break time into discrete intervals I1, I2,··· . For
each time interval It, and each signal A[a] that appears before or
during interval It, we ﬁrst compute the observed value—total up-
date to A[a] during interval It: oa(t) = i∈Aa(t) ui, where the
= {i | ai = a ∧ (ai, ui) arrives during It}.
set of indices Aa(t)
We also compute the forecast value fa(t) by applying a forecasting
model to observed values in the past intervals. We then compute the
forecast error ea(t) = oa(t) − fa(t) and raise an alarm whenever
ea(t) is signiﬁcant according to certain detection criteria.
def
In the real world, however, per-ﬂow analysis can be prohibitive
because the number of signals present in the input stream can be
very large. For instance, if we use source and destination IPv4
addresses as the key, the key space [u] can be as large as 264, and
the number of signals can easily reach tens of millions given today’s
trafﬁc volume and link speeds. Hence it can be too slow or too
expensive to perform change detection on a per-ﬂow basis.
Our solution is to create sketches to summarize the input stream
and implement various forecasting models on top of the sketches.
Speciﬁcally, our sketch-based change detection consists of the fol-
lowing three basic modules:
1. Sketch module
2. Forecasting module
3. Change detection module
The ﬁrst module—sketch module—creates a (space- and time-
efﬁcient) sketch to summarize all the observed values oa(t) (to-
tal update to signal A[a]) during each time interval It—the ob-
served sketch So(t). The forecasting module produces a forecast
sketch Sf (t) using some forecasting models based on observed
sketches in the past intervals. It then computes the forecast error
sketch Se(t) as the delta between So(t) and Sf (t), i.e., Se(t) =
So(t) − Sf (t). The linearity of the sketch data structure allows
us to implement various forecasting models and compute the deltas
directly at the sketch level. The change detection module uses the
error sketch Se(t) to determine signiﬁcant changes. We next de-
scribe these modules in details.
3. DETAILS
3.1 Sketch module
Let (a1, u1), (a2, u2),··· be an input stream (for example, the
substream of I that is observed during a given time interval). For
each key a ∈ [u], let va = i∈Aa
ui, where the set of indices
= {i | ai = a}.
For each interval, the second moment (F2) is deﬁned as the sum
def
=
F2)
√
of squares of the values associated with all the keys, i.e., F2
a. We refer to the square root of the second moment (
Aa
def
a v2
as the L2 norm.
The sketch module uses the sketch data structure to summarize
all the va in each time interval. Sketch is a probabilistic summary
data structure based on random projections (See [30] for a good
overview of sketches and the general ﬁeld of data stream com-
putation). We have designed a variant of the sketch data struc-
ture, which we call the k-ary sketch. The k-ary sketch is similar
to the count sketch data structure recently proposed by Charikar et
al.[11]. However, the most common operations on k-ary sketch use
simpler operations and are more efﬁcient than the corresponding
operations deﬁned on count sketches [33].
Just like the count sketch, a k-ary sketch S consists of a H × K
table of registers: TS[i][j] (i ∈ [H], j ∈ [K]). Each row TS[i][·]
(i ∈ [H]) is associated with a hash function from [u] to [K]: hi.
We can view the data structure as an array of hash tables. The hash
functions are required to be 4-universal [10, 39] to provide proba-
bilistic guarantees of reconstruction accuracy. We construct them
using the fast tabulation-based method developed in [33]. Differ-
ent hi are constructed using independent seeds, and are therefore
independent.
There are four basic operations deﬁned for k-ary sketches: UP-
DATE to update a sketch, ESTIMATE to reconstruct va for a given
key a, ESTIMATEF2 to estimate the second moment F2, and COM-
BINE to compute the linear combination of multiple sketches. They
are used in various modules of our change detection scheme: UP-
DATE in the sketch module to update the observed sketch So(t);
COMBINE in the forecasting module to implement various fore-
casting models and to compute the forecast sketch Sf (t) and fore-
cast error sketch Se(t); ESTIMATE in the change detection mod-
ule to reconstruct forecast errors from Se(t); and ESTIMATEF2 in
the change detection module to choose the threshold for judging
whether forecast errors are signiﬁcant.
The formal speciﬁcation of these operations is as follows.
1. UPDATE(S, a, u): For ∀i ∈ [H], TS[i][hi(a)]+ = u.
2. ESTIMATE(S, a): Let sum(S) = j∈[K] TS[0][j] be the
sum of all values in the sketch, which only needs to be com-
puted once before any ESTIMATE(S, a) is called. Return an
estimate of va
where
a = mediani∈[H]{vhi
a }
vest
vhi
a =
T [i][hi(a)] − sum(S)/K
1 − 1/K
a (i ∈ [H]) is an unbiased
As shown in Appendix A, each vhi
estimator of va with variance inversely proportional to (K −
1). vest
further improves accuracy by avoiding the extreme
a
estimates.
3. ESTIMATEF2(S): Return an estimate of the second mo-
ment
where
F hi
2 =
2 = mediani∈[H]{F hi
2 }
F est
K
K − 1 (cid:1)j∈[K]
(TS[i][j])
2 − 1
K − 1
(sum(S))
2
As shown in Appendix B, each F hi
forms an unbiased esti-
mator of F2 with variance inversely proportional to (K − 1).
F est
further improves accuracy by avoiding the extreme es-
timates.
2
2
4. COMBINE(c1, S1,··· , c(cid:1), S(cid:1)): The linearity of the sketch
data structure allows us to linearly combine multiple sketches
k=1 ck · Sk by combining every entry in the table:
S = 
(cid:1)
(cid:1)
ck · TSk [i][j]
TS[i][j] =
(cid:1)k=1
3.2 Forecasting module
The forecasting module uses the observed sketches in the past
) (t(cid:3) < t) to compute the forecast sketch Sf (t) and
intervals So(t(cid:3)
along with it the error between the observed and forecast sketches
as Se(t). In this work, we explore six models commonly used in
univariate time series forecasting and change detection. The ﬁrst
four models are simple smoothing models; the other two belong to
the family of ARIMA models. All six models can be implemented
on top of sketches by exploiting the linearity property of sketches.
3.2.1 Simple smoothing models
The ﬁrst four models are simple smoothing models and are pop-
ular due to their simplicity. They are moving average (MA), ex-
ponentially weighted moving average (EWMA), S-shaped moving
average (SMA), and non-seasonal Holt-Winters (NSHW).
Moving Average (MA)
This forecasting model assigns equal
weights to all past samples, and has a single integer parameter
W ≥ 1 which speciﬁes the number of past time intervals used
for computing the forecast for time t.
i=1 Sf (t − i)
Sf (t) = W
W
, W ≥ 1
S-shaped Moving Average (SMA) This is a class of weighted
moving average models that give higher weights to more recent
samples.
Sf (t) = 
W
i=1 wi · Sf (t − i)
W
i=1 wi
, W ≥ 1
So(1),