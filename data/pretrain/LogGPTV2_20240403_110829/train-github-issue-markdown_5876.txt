## üêõ Bug
Using a numpy array `i` as index for a torch tensor `t` (i.e. `t[i]`) is
interpreted differently than indexing a numpy array with the same index, or
indexing t with the index converted to a tensor.
## To Reproduce
    import torch
    import numpy as np
    a=np.array([0.])
    t=torch.tensor([0.])
    i=np.array([[0,0],[0,0]])
    a[i] # Fine
    t[torch.tensor(i)] # Fine
    t[i] # IndexError: too many indices for tensor of dimension 1
## Expected behavior
I expected the last line to behave the same as the previous two.
## Environment
PyTorch version: 1.4.0  
Is debug build: No  
CUDA used to build PyTorch: 10.1
OS: Debian GNU/Linux bullseye/sid  
GCC version: (Debian 9.2.1-30) 9.2.1 20200224  
CMake version: version 3.16.3
Python version: 3.7  
Is CUDA available: Yes  
CUDA runtime version: 10.1.168  
GPU models and configuration: GPU 0: Quadro M2000M  
Nvidia driver version: 440.64  
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
Versions of relevant libraries:  
[pip3] numpy==1.17.4  
[pip3] numpydoc==0.7.0  
[pip3] torch==1.4.0  
[pip3] torchvision==0.5.0  
[conda] Could not collect
## Additional context
This appears related to #22013 and #29522 (possibly #18616). If a numpy array
is treated as an arbitrary Iterable, it is probably a duplicate of one of
those, although there is the possibility of handling array-like types
differently. I am reporting this because I am not sure the issue is exactly
the same, but please don't hesitate to close this issue if it is redundant.