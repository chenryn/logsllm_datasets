number of skip connections between the encoder and the
decoder sections, the learning rate, the L2 regularization factor.
The optimal set of hyperparameters were selected using a
grid search: all combinations of skip connections (that were
dimensionally compatible) between the encoder and decoder
were tried, the learning rate and L2 regularization rate were
varied in the set {0.001, 0.005, 0.01, 0.05}.
Bootstrapping the Training: The synthetic data generated
as described in Section IV-D and the true data samples
corresponding to each synthetic sample were treated as the
input-output pairs (Xi, Zi), to train a synthetic model. The
goal of the audio reconstruction model is thus to denoise
and appropriately scale the noisy inputs to resemble the true
outputs.
Domain Adaptation: The synthetic model is adapted using
a small sensor data sample (5% of the size of the synthetic
dataset for Audio MNIST, and speech commands). We perform
fine-tuning of the model where we simply update the weights
of all parameters with real sensor data. The amount of domain
adaptation data was sufficient to achieve convergence even
though none of the layers were frozen. Based on a cross-
validation approach, we verified that such a fine-tuning ap-
proach provided us best results for this architecture instead
of performing domain adaptation only on a few layers and
freezing the rest.
F. Speech Recognition via Classification:
Towards performing speech recognition, we develop models
based on convolutional neural networks that take spectrograms
as inputs and estimate class labels corresponding to digits and
speech-commands as outputs. The high level architecture of
the classification model is depicted in Fig. 11. Similar to
the network for audio reconstruction, skip connections are
exploited for benefits in training and convergence.
Bootstrapping the Training: The synthetic data that was
generated as described in Section IV-D and the labels corre-
sponding to each synthetic sample were treated as the input-
output pairs (Xi, yi), to train a synthetic model. The goal
of the classifier is to best estimate the word that is being
spoken, given the spectrogram of the vibration (sensed using
the mmWave radar) as the input.
Hyperparameter Selection: The hyperparameters include
learning rate, L2 regularization factors, kernel sizes for con-
volutional layers, dropout rates, the number of resnet blocks,
and the number of nodes per fully connected layer. The above
parameters were varied using a grid search as follows: learning
rate in the set of {0.001, 0.005, 0.01, 0.05}, square kernel
sizes {3, 5, 7}, number of resnet blocks 1 − 3, number of
filters per convolutional layer in the resent blocks as {64, 128,
256}, number of convolutional filters in the deep convolutional
layers as {64, 128, 256, 512}, the number of nodes per fully
connected top layer as {128, 256, 512}. We use randomized
cross-validation to tune the hyperparameters for the model,
and run multiple cross-validation programs on a campus GPU
cluster concurrently.
Domain Adaptation: In order to adapt the synthetic model
to the sensor data, the last layers (indicated in Fig. 11) are
retrained using sensor data as inputs and the corresponding
true data samples as outputs. All the layers excepting the last
few layers are frozen so that their weights do not change
when the model is adapted. This is done so that the model
can learn the same representation as the synthetic model by
relearning only the last layers. Such a strategy is popular
in other domain adaptation and transfer-learning applications
[60], [74], [44]. Approximately 5% of synthetic data is used
for domain adaptation for both AudioMNIST and speech
commands datasets.
V. EVALUATION
Fig. 11: Architecture of speech classification network.
Loss functions and optimizations: The model is trained using
a cross-entropy loss function. The cross entropy loss function
is commonly used in classification problems. It is given as:
CE = − N(cid:88)
M(cid:88)
yi,clog(pi,c)
(7)
i=1
c=1
frontend includes Texas
Implementation: The experimental setup is depicted in
Fig. 12. mmSpy’s
Instruments
AWR1843BOOST [8] and IWR6843ISK [12] mmWave radars
as introduced in Section II operating in the spectrum of 77
GHz and 60 GHz respectively. Operating with a FMCW
bandwidth of 1798.92GHz (Appendix B), we use the
DCA1000EVM [17] platform to extract samples at 10 Msps,
and obtain reflections from the phone vibrations. We use
Samsung Galaxy S20 (S20) and Google Pixel 4a (Pixel) phone
models in our evaluation. The phases of the reflections from
phone vibrations are used to extract audio content as well as
train the ML models in mmSpy for speech recognition. We
capture the reflections from the back of the phone opposite
to the side of the earpiece. The ML model is implemented
with PyTorch [80] packages and the training is performed on
a desktop with Intel i7-8700K CPU, 16GB RAM memory, and
NVIDIA Quadro RTX 8000 GPU.
Datasets: We validate the attack capability of mmSpy with two
diverse speech recognition tasks: (i) We use the AudioMNIST
dataset for validating a task in digit recognition. AudioMNIST
[36] dataset consists of 30000 audio samples of spoken digits
where N is the number of data samples, M is the number of
classes, yi,c is 1 if sample i belongs to class c and 0 otherwise,
and p is the predicted probability that sample i is of class c.
1217
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
7
use Stealth Audio Player for playing audio contents of the test
data on the earpiece device.
Metrics of Evaluation: We consider the following two metrics
of evaluation. (i) For the audio reconstruction model, we report
the reconstruction loss between the recovered audio from the
radar and the original audio played on the phone. A MSE error
is used to quantify this. (ii) For speech recognition with digits
and speech commands dataset, we report the classification
accuracy (or simply the accuracy), which is the ratio of
number of correct classifications over the total number of test
cases. In addition to the above, we provide qualitative results
such as spectrograms and an audio demo. We now present
results from a systematic measurement study.
Terminology: Since we consider multiple phones, multiple
datasets, and multiple frequency bands in this paper, the num-
ber of different combinations can be exhaustive. Therefore,
we choose the following subset of combinations that provide
a good representation of the variation across different factors.
Accordingly, we use the following terminology to represent
these cases. (a) “S20 (77 GHz)” – results from Samsung S20
at 77 GHz with AudioMNIST. (b) “S20 (60 GHz)” – results
from Samsung S20 at 60 GHz with AudioMNIST. (c) “Pixel
(77 GHz)” – results from Google Pixel 4a at 77 GHz with
AudioMNIST. (d) S20 (Sp, 77 GHz)” – results from Samsung
S20 at 77 GHz with Speech commands dataset.
Qualitative Reconstruction Results: Figures 13, 14 depict
the spectrograms from a qualitative reconstruction of the
audio. The y-axis of the spectrograms varies from 0 to 4KHz.
Representative samples from each class are presented for both
AudioMNIST and speech commands datasets. The raw capture
from the sensor (top row), as well as mmSpy’s reconstruction
of the original audio from the raw sensor data is shown.
Second row shows the output before masking whereas the
third row shows the output after masking (discussed in Section
IV-E). The bottom row shows the spectrograms of ground truth
audio. Evidently, mmSpy’s reconstruction agrees well visually
with the ground truth. While the raw sensor data looks mostly
noisy, mmSpy’s audio reconstruction is able to highlight the
key spectro-temporal trends in the audio resulting in a good
recovery of the original audio.
Audio Demo: Sample audio files from the raw sensor, mm-
Spy’s reconstruction from the raw sensor data, as well as the
ground truth is included in the following anonymous url [5].
Headphones are recommended for listening. While the raw
sensor data is incomprehensible, evidently, an adversary can
roughly decipher the contents of the ground truth audio from
mmSpy’s reconstruction. mmSpy uses Griffin-Lim algorithm
for reconstructing audio from spectrograms [52].
Quantitative Reconstruction Results: Fig. 15 depicts the
MSE error of audio reconstruction. The spectrograms are
normalized within a range of [-1,1] for a uniform comparison.
As depicted, the difference in MSE between the enhanced and
the true audio is lower than the MSE between the raw sensor
data and the true audio. Fig 15(b) shows the MSE averaged
over 1-6 ft for various settings. This shows the effectiveness
Fig. 12: Experimental setup in mmSpy. Off the shelf mmWave radar
device is used to detect earpiece vibrations from a smartphone.
(0-9) of 60 different speakers consisting of 48 males and 12
females from all age groups. Each audio sample is less than
one second long, captured in a controlled environment at a
sampling rate of 8kHz. The AudioMNIST dataset is a popular
benchmark for testing several techniques in the literature of
speech recognition including an attack on the accelerometer
sensor [33], [48]. (ii) Towards validating a task in recognition
of words, we use the speech command dataset [91]. The
dataset consists of 38546 samples of the following speech
commands: [‘down’, ‘go’, ‘left’, ‘no’, ‘off’, ‘on’, ‘right’,
‘stop’, ‘up’, ‘yes’]. This dataset is completely anonymous
and does not come with any information about age groups,
genders, etc. Additionally, this dataset was crowdsourced and
prepared so it includes samples from phone, laptop and tablet
microphones. Each sample is converted to a 16kHz WAV file
and is 1-second long.
Training Data: About 90% of samples from the datasets
described above were converted into synthetic radar data (as
discussed in Section IV-D) to bootstrap the training process.
This includes data from all speakers, regardless of gender, age,
etc. A synthetic model is first created, which is later adapted
with small sets of real sensor data as elaborated next.
Data for Domain Adaptation: The data used for domain
adaptation is approximately 5% of the size of the synthetic
training data. We play the audio samples corresponding to this
data on the smartphone and record the radar measurements.
This generates labelled training dataset with radar recordings
and their respective audio classes. We used two phones of each
of these models – Google Pixel 4a and Samsung S20 – a total
of four smartphones. This allows us to perform the domain
adaptation and testing on different phones. We use an app,
Stealth Audio Player [18] that plays the audio contents of the
domain adaptation data on the smartphone’s earpiece.
Test Data: In order to test
the model, separate training,
validation, and test sets are collected. Testing is done on a
different smartphone (victim’s phone) than the one from which
training data for domain adaptation was generated (attacker’s
phone). The domain adaptation dataset was roughly split into
80:20 ratio for cross-validation while ensuring that the train
and test data comes from different phones. There are no
samples in common between training, testing, and the domain
adaptation dataset. Similar to the domain adaptation above, we
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
8
1218
Fig. 13: Qualitative Results (at 3ft range): Speech commands. While the raw sensor data (top row) is noisy, mmSpy’s reconstruction (second
row) is able to extract the key spectro-temporal trends in the noisy sensor data. The masking ideas (third row) allows focusing on distortion
free voiced components from the input. The enhanced outputs looks visually similar to the true audio spectrograms (bottom row).
Fig. 14: Qualitative Results (at 3ft range): Audio MNIST. Similar to speech commands, the audio reconstruction model is able to extract
the key spectro-temporal trends from noisy sensor data.
of the ML algorithms in mmSpy in reducing the MSE.
linearly as a function of distance. At a distance of 6f t ft,
the power starts getting closer to noise levels. Beyond this,
accurate detection of the phone reflection becomes hard. The
variation is consistent for different phone models (Pixel vs.
S20) as well as different frequencies (77 vs. 60 GHz). The
power levels with speech commands is slightly lower than
AudioMNIST mainly due to a corresponding lower quality
(volume) of the data in comparison to AudioMNIST data.
(a)
(b)
Fig. 15: Reconstruction error (a) MSE vs Range (S20, 77 GHz) (b)
Average MSE across settings.
Power vs Range: Fig. 16 depicts the power levels of sound
vibrations as a function of the distance of the phone from the
radar device. The power levels are measured in dB in reference
to the noise power levels when there is no sound being played
on the earpiece device. As expected, the power levels drops
Fig. 16: Power Levels vs Range
Speech Recognition Accuracy vs Range: Table. I depicts
the accuracy of mmSpy as a function of range for different
phone models and frequency range. Evidently, the accuracy
only degrades gracefully over distances upto 6 ft which
suggests the potential for a successful attack under conditions
identified in the threat model in Section III. The performance is
consistent across multiple phone models and frequency bands.
1219
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:24 UTC from IEEE Xplore.  Restrictions apply. 
9
123456Range (ft)0.20.40.60.811.2MSEtrue-rawtrue-enhancedtrue-(enhanced+mask)S20 (77GHz)S20 (60GHz)Pixel (77GHz)S20 (Sp, 77GHz)0.20.40.60.811.2MSEtrue-rawtrue-enhancedtrue-(enhanced+mask)123456Range (ft)012345PowerLevel(dB)S20 (77GHz)S20 (60GHz)Pixel (77GHz)S20 (Sp, 77GHz)The performance with speech commands is slightly lower than
Setting
S20 (77 GHz)
S20 (60 GHz)
Pixel (77 GHz)
S20 (Sp, 77 GHz)
1ft
83.33
78.35
80.11
69.37
2ft
73.10
70.05
70.94
63.81
3ft
65.09
62.37
66.30
60.74
Distance
4ft
60.66
60.11
58.33
56.70
5ft
50.14
50.91
49.09
48.62
6ft
47.99
49.92
46.60
44.56
TABLE I: Accuracy vs distance under different settings
the performance with AudioMNIST data which follows the
trend in power levels observed in Fig .16. The confusion
matrices (Fig. 29), precision, and recall values (Table III) for
a representative setting is depicted in the appendix.
Accuracy vs Users: Fig. 17 shows the accuracy as a function
of different users. The results are averaged over the entire
range of 1 − 6f t where the power level related to noise
varies between 4.4 − 0.2dB. These results are only based on
AudioMNIST dataset since the speech command dataset is a
crowd-sourced dataset that does not have classes organized by
users. Given that the model has been trained from a diverse
Fig. 18: While synthetic data bootstraps the training, domain adap-
tation substantially boosts the accuracy with small real training data.
Earpiece Signal Levels and Performance under Noisy
Setting: Fig. 19 depicts earpiece power levels measured by a
high fidelity microphone under two settings: (i) Ambient noise
in an indoor lab, approximately estimated at 32dB with respect
to a complete silence. (ii) Loudspeaker playing white noise
with approximately 58dB relative to silence, thus simulating
a crowded setting. The measurements were conducted using
a high fidelity microphone model Zoom H1 [27]. For each
case, we report the overall power levels (earpiece audio +
ambient/external noise) in comparison with ambient/external
noise levels when the earpiece is silent. Evidently, the earpiece
Fig. 17: Accuracy vs Users
distribution of users including males and females, the model
is overall robust across a variety of users. We notice that the
variation in accuracy across users is roughly correlated with
the volume of their voice. The accuracy is also consistent
across different genders with a 66.39% and 63.02% accuracy
for male and female users respectively.
Performance in the 60 GHz spectrum: Table I also depicts
the performance at 60 GHz frequencies in comparison with
the 77 GHz spectrum. Evidently, the performance is consistent
across both frequency spectrums. This is mainly because the
propagation path loss does not change much between the two
frequencies. Therefore, we observe a similar trend in SNR as
well as the accuracies across both spectrums.
The Role of Domain Adaptation: Fig. 18 depicts the breakup
of the gain in accuracy due to domain adaptation. The results
are averaged over the entire range of 1− 6f t where the power
level varies between 4.4−0.2dB. mmSpy trained with synthetic
radar data helps bootstrap the process of training. While the
average accuracies (≈ 30%) with synthetic data is a modest
start, mmSpy boosts the accuracy by adapting the model with
small scale training data from real sensor. Evidently, with only
5% of real sensor data in comparison with original source of
training dataset, the performance of mmSpy is substantially
enhanced resulting in accuracy levels of 58 − 69%.
1220
10