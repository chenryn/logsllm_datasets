# 关键基础设施中基于AI的入侵检测技术的比较研究（二）

##### 译文声明
本文是翻译文章，原作者为SAFA OTOUMBURAK、KANTARCI、HUSSEIN MOUFTAH，原文发表于x-mol.com。译文仅供参考，具体内容及含义以原文为准。

## 5. 入侵检测方法

### 5.1 基于自适应监督和聚类混合入侵检测系统 (ASCH-IDS) 的自适应机器学习

在我们之前提出的 ASCH-IDS [7] 和 CHH-IDS [6] 中，收集到的遥感数据经过两个并行运行的子系统进行处理：异常检测子系统 (ADSs) 和误用检测子系统 (MDSs)。这种结构形成了一个混合系统，如算法2所示，用于检测入侵传感器。ADSs 使用 EDBSCAN 算法（一种带有噪声的应用程序基于密度的空间聚类方法），而 MDSs 则采用随机森林技术。DBSCAN 是一种基于密度的聚类方法，将聚类视为物体密集区域，数据空间分布成低密度区域 [34]；E-DBSCAN 算法则保留了聚类中局部密度的变化轨迹，并在考虑其电子邻域的情况下计算任何核心物体的密度变化 [35]。另一方面，当随机森林作为分类机制时，每棵树都会检查每个输入的频繁类别并在发生时进行投票 [36]。其操作分为训练和分类两个阶段 [37]。在 [6] 中，收集的数据按时间段划分，以循环方式分配给 IDS 子系统 (IDSs)，如图1所示。

图2展示了之前提出的 CHH-IDS [6]，该系统运行在包含 N 个集群的 WSN 上，其中 CH 处理传感器转发数据的聚合过程。

当使用随机森林进行误用检测时，每棵树的发展过程如下 [37]：
1. 如果训练集大小表示为 Y，则从原始数据集中随机抽取的数据点 y 将成为开发树的训练数据集。
2. 如果输入变量表示为 X，可以从 X 中随机抽取 x 来分割节点。x 被视为常数，在森林发展中，每棵树都发展到最大尺寸。

另一方面，E-DBSCAN 作为一种聚类算法，已被用来提取代表距离阈值的关键因素 [38]。DBSCAN 可以实现属于同一密度的相邻聚类以及随机形状的聚类 [39]。DBSCAN 由两个参数组成：ε 和 MinPts。它遵循以下规则：
1. 核心对象的邻域大小 > MinPts。
2. 点 j 作为核心对象，从 i 中可以获得密度。
3. 当 i 和 j 从同一个核心对象中可以获得密度时，认为 i 和 j 是基于密度连接的。

另一方面，ASCH-IDS [6] 在接收者操作特性曲线 (ROC) 上保持 ADSs 和 MDSs 的偏差，并自适应地改变指向它们的数据段。方程式 (2)-(3) 表示 TP 至 FP 的两个子系统在 τi 上的表现，分别用 μ1(τi) 和 μ2(τi) 表示 [7]。方程式 (4)-(5) 表示在时间 △τ 时 TP 至 FP 的比率，其中 △τ = τi+1 - τi [7]。

当提取 △τ 期间的 ROC 行为时，我们可以得到各子系统的 ROC 行为是 τ 时的 ROC 行为与 △τ 时的行为之和，这可以用方程式 (6) 和 (7) 表示 [7]。其中 α 代表最近计算的 TP/FP 及其在 △τ 期间的值，其中 τi+1 = τi + △τ。

方程式 (8) 表示一个指标 I(τi)，它追踪 MDSs 和 ADSs 在任意时间 τi 上的 ROC 行为。提取的指标 I 用于决定对收集到的数据进行引导。如果 I(τi) > I(τi-1)，则 ASCH-IDS 判定 ADSs 的性能优于 MDSs，因此增加 ADSs 数据比例，从而提升系统整体性能。反之，如果 I(τi) < I(τi-1)，则 ASCH-IDS 在 μ1 上的数据比例增加，在 μ2 上的数据比例减少，如 Da(τi+1) = Da(τi) + △D 和 Dm(τi+1) = Dm(τi) - △D，其中 △D 表示各子系统的数据调整。算法3概述了上述步骤。其中 τi、τi+1 和 △τ 分别指任意时间、τi+△τ 和 (τi+1 - τi) 之间的时间差。α 指的是评价 μ1(τi) 和 μ2(τi) 时的 ROC 特征权重，Da(τi) 和 Dm(τi) 指的是 τi 处转发到异常 (ADSs) 和误用 (MDSs) 的数据段。

由于所提出的系统由两个子系统组成，其复杂度是两个算法复杂度的函数。对于随机森林，其复杂度可以从决策树中提取出来，例如，建立一棵具有 r 条记录和 v 个变量的决策树的复杂度 C(随机森林) 为 O(v + r * log(v))，而对于多棵树，复杂度为 O(Tr * Var * r * log(Var))，其中 Tr 是树的数量，Var 是变量的数量。对于 E-DBSCAN，其复杂度由查询请求的数量决定。由于每个点只执行一次查询，所以运行时复杂度为 O(n) < n * log(n)。在我们的 E-DBSCAN 中，初始化步骤执行一次，比较步骤执行 (m+1) 次，增量步骤执行 m 次。因此，复杂度为 O(2 + (2m)) = O(m)。因此，系统的整体复杂度可以表示为 O((Tr * Var * r * log(Var)) + (2 + (2m)))。

### 5.2 基于深度学习的受限玻尔兹曼机聚类 IDS (RBC-IDS)

受限玻尔兹曼机 (RBM) 是一种高能神经网络系统，包含两种类型的层：可见层 (V) 和隐藏层 (H)，学习过程由无监督方式引导 [19]。RBM 允许同一层的神经元之间的连接，这使得它受到限制。程序在算法4的虚拟码中提出。表1包含了算法4中使用的 RBM 参数。

网络对隐藏元素和可见元素中的每一种情况都设置了概率得分 [19][33]。图3展示了 RBC-IDS 中使用的 RBM 设置。RBC-IDS 由一个包含 X 个可见节点的输入层、三个隐藏层和输出层组成，其中输出层的两个输出 O1 和 O2 分别为 _Intrusive_ 和 _Normal_ 输出。W11 代表第一可见层和第一隐藏层之间的权重，W12 指的是第一隐藏层和第二隐藏层的权重，W23 是第二隐藏层和第三隐藏层之间的权重。

在 RBC-IDS 中，每个 CH 负责将来自同一集群中的传感器的传感数据进行聚合，并通过采用 [32] 中的程序将其转发给服务器。

### 5.3 强化学习

在机器学习中，所研究的环境被定义为马尔可夫决策过程 (MDP)，它和许多强化学习算法（如 Q-learning）一样，采用动态编程程序。MDP 旨在找到最优策略，以随着时间的推移获得最大的回报 [40]。

强化学习的基本原理如下：
1. 代理器（即传感器）与环境交互，在每个状态 St 中采取动作 At，并观察环境的反馈。
2. 环境以 R+ 或 R– 的形式为所做的行为提供奖励 Rt，分别表示积极或消极的奖励。
3. 代理器会观察环境的任何变化，并通过更新策略来优化收到的奖励。
4. 为了使总奖励的期望值最大化，调用不同的强化学习技术。

**5.3.1 Q-Learning**

Q-Learning 建立在价值迭代的概念上，代理的目标是估计价值函数。每次迭代更新所有状态 s 和动作 A，以便知道哪个动作 A 导致更高的奖励 R。在 Q 表中，行代表状态，列代表动作。在一个状态（比如说状态 S）中，代理采取一个动作（即动作 A），观察这个动作的奖励（R）以及下一个状态（S’），然后重新估计 Q 值。

方程式 (11) 表示估计的 Q 值 [41]，其中 St、At 和 Rt 分别代表时间 t 的状态、动作和奖励。此外，α 和 γ 分别代表学习率和一个关于奖励相对值的常数。事实上，这两个参数满足 0 < α < 1 和 0 < γ < 1。

方程 (12) 形成估计函数 Vπ(S)，它代表了对初始状态 S 下将获得的未来报酬 R 的估计 [42]。方程中 π(S, A) 表示动作 A 在状态 S 中的可能性，PSS+(A) 代表状态 S 和 S+ 与动作 A 之间的状态转换概率。R(S, S+, A) 为行动 A 时从状态 S 过渡到 S+ 后发出的奖励，r 为未来奖励到当前奖励的贴现系数权重 [42]。采用的值迭代方法如下方程式 (13) 所示。其中，Vπ(S) 指初始状态 S 时 R 的值估计，π(S, A) 是 A 在 S 中的概率，PSS+(A) 指 A 时从状态 S 到 S+ 的过渡概率，R(S, S+, A) 是 S 时从状态 S 到 S+ 过渡返回的报酬。r 为未来奖励到当前奖励的贴现系数权重，VπI（S+）为初始迭代 I 时 R 在状态 S+ 的价值估计，为更新迭代 I+1 时 R 在状态 S 的价值估计。

值得注意的是，之所以采用 Q-learning 作为我们的强化学习方法之一，是因为 Q-learning 的无模型特性。此外，通过应用 Q-learning，还可以以非自适应的方式解决随机奖励问题。此外，Q-learning 还具有不一定遵循当前政策的学习能力 [43]。

**5.3.2 状态-行动-奖励-状态-行动学习 (SARSA)**

SARSA 是一种基于 MDP 的强化学习算法，被认为是改进版的链结式 Q-Learning (MCQ-L) 算法。SARSA 根据当前状态 S、S 的当前动作 A、动作 A 的返回奖励 R、新状态 S 和新状态的下一个动作 A 来更新 Q 值，可以用五元组 (St, At, Rt, St+1, At+1) 来表示。

SARSA 是一种策略性学习算法，代理与环境相互关联，并根据所采取的行动更新政策。在 SARSA 中，Q 值函数表示在状态 S 中采取动作 A 在下一个时间步中收到的奖励，以及从下一个状态和动作中收到的奖励 [44]。前面的 Q 值函数（方程式 (11)）可按方程式 (14) 更新。

方程式 (11) 和方程式 (14) 几乎相同，但在 Q-learning 中，所有可能的下一步行动中估计值最高的行动将被考虑，而在 SARSA 中，实际的下一步行动被考虑。与 SARSA 技术相比，在 Q-learning 中寻找最大的回报会使其成本更高 [14]。

**5.3.3 时间差分学习 (TD)**

是一种无模型强化学习技术，它可以通过考虑从当前值的近似分布来估计期望值函数进行学习。TD 技术估计策略下的状态值函数 π，如下面的方程式 (15) 和方程式 (16) 所示。

其中 Vπ(St) 指的是状态 St 的状态价值函数，R 指的是报酬，γ 是政策 π 下的贴现率，在方程式 (16) 中，R0 + γVπ(S1) 代表 Vπ(St) 的无偏估计。TD 是一种用来学习如何估计一个依赖于未来值的技术，这使得它对学习 V-函数和 Q-函数都很有用。而 Q-learning 则是一种只学习 Q-函数的特殊技术。

基于强化学习的 WSNs 入侵检测系统 (QL-IDS) 如图4所示。IDS 由分层连接的集群与聚合器和一个中心代理组成，其代理在表示强化学习盒中，试图模拟监测网络的状态。经过一系列的迭代，中心代理知道响应每个状态 S 需要执行的动作 A，以获得正向奖励 R+。

值得一提的是，值迭代的工作原理是通过生成最优值函数的连续估计值。每一次的迭代都可以在 O(|A||S|²) 中完成。在强化学习中，所需的迭代次数可以成倍增长。

## 参考文献

[1] I. Al-Ridhawi, S. Otoum, M. Aloqaily, Y. Jararweh, and Th. Baker. Providing secure and reliable communication for next generation networks in smart cities. _Sustainable Cities and Society_, 56:102080, 2020.

[2] L. Buttyan, D. Gessner, A. Hessler, and P. Langendoerfer. Application of wireless sensor networks in critical infrastructure protection: challenges and design options [security and privacy in emerging wireless networks]. _IEEE Wireless Communications_, 17(5):44–49, October 2010.

[3] Ismaeel Al Ridhawi, Yehia Kotb, Moayad Aloqaily, Yaser Jararweh, and Thar Baker. A profitable and energy-efficient cooperative fog solution for IoT services. _IEEE Transactions on Industrial Informatics_, 16(5):3578–3586, 2019.

[4] Safa Otoum, Burak Kantraci, and Hussein T. Mouftah. Hierarchical trust-based black-hole detection in WSN-based smart grid monitoring. _IEEE International Conference on Communications (ICC)_, 2017.

[5] M. Al-Khafajiy, S. Otoum, TH. Baker, M. Asim, Z. Maamar, M. Aloqaily, MJ. Taylor, and M. Randles. Intelligent control and security of fog resources in healthcare systems via a cognitive fog model. _ACM Transactions on Internet Technology_, 2020.

[6] Safa Otoum, Burak Kantarci, and Hussein T. Mouftah. Detection of known and unknown intrusive sensor behavior in critical applications. _IEEE Sensors Letters_, 1(5):1–4, Oct 2017.

[7] Safa Otoum, Burak Kantraci, and Hussein T. Mouftah. Adaptively supervised and intrusion-aware data aggregation for wireless sensor clusters in critical infrastructures. In _IEEE International Conference on Communications (ICC)_, pages 1–6, May 2018.

[8] Safa Otoum, Burak Kantraci, and H. T. Mouftah. Mitigating false negative intruder decisions in WSN-based smart grid monitoring. In _13th International Wireless Communications and Mobile Computing Conference (IWCMC)_, pages 153–158, June 2017.

[9] R. Jain and H. Shah. An anomaly detection in smart cities modeled as wireless sensor network. In _International Conference on Signal and Information Processing (IConSIP)_, pages 1–5, Oct 2016.

[10] C. Ioannou, V. Vassiliou, and C. Sergiou. An intrusion detection system for wireless sensor networks. In _24th International Conference on Telecommunications (ICT)_, pages 1–5, May 2017.

[11] Ahmad Javaid, Quamar Niyaz, Weiqing Sun, and Mansoor Alam. A deep learning approach for network intrusion detection system. In _Proceedings of the 9th EAI International Conference on Bio-inspired Information and Communications Technologies (Formerly BIONETICS)_, pages 21–26, 2016.

[12] C. Yin, Y. Zhu, J. Fei, and X. He. A deep learning approach for intrusion detection using recurrent neural networks. _IEEE Access_, 5:21954–21961, 2017.

[13] L. Dali, A. Bentajer, E. Abdelmajid, K. Abouelmehdi, H. Elsayed, E. Fatiha, and B. Abderahim. A survey of intrusion detection system. In _2nd World Symposium on Web Applications and Networking (WSWAN)_, pages 1–6, March 2015.

[14] Stefano Zanero and Sergio M. Savaresi. Unsupervised learning techniques for an intrusion detection system. In _ACM Symposium on Applied Computing_, SAC ’04, pages 412–419, New York, NY, USA, 2004. ACM.