multiplied by the factor (cid:11). This (cid:11) represents the fraction of users
who do not choose top passwords (e.g., 0.21 in Fig. 10). Then, the
probability of each password in the top-104 list are multiplied by
1 (cid:0) (cid:11). Finally, these two probability-associated lists are merged
and sorted in decreasing order, and then we select the top k (e.g.,
k=103) as the ﬁnal guess candidates.
′g is a ﬁnite set disjoint from V.
In Fig. 12, we provide a comparison of TarGuess-II with Das et
al.’s algorithm [12]. These two algorithms are comparable because
they employ the same personal information of the victim. When
given a user U’s Dodonew password, within 100 guesses, Das et
al.’s algorithm [12] gained a success rate of 8.98% against U’s CS-
DN account, while the ﬁgure for TarGuess-II is 20.19%, reaching
Table 7: Training of capitalization C (C1: Cap. all; C2: Cap. the
1st letter; C3: Lower all; C4: Lower 1st)
C4
Probability 0.95 0.01 0.03 0.003 0.007
No C1 C2 C3
Table 8: Training of the leet transformation rule L
No L1 : a $ @ L2 : s $ $ L3 : o $ 0 L4 : i $ 1 L5 : e $ 3
Prob. 0.95
0.02
0.01
0.01
0.005
0.005
Table 9: Training of sub-
string movement
substring moved SM
Yes
0.03
No
0.97
Prob.
Table 10: Training of reverse
operation R (R1: Reverse all; R2:
Reverse each segment)
Probability 0.97
No R1 R2
0.02 0.01
love@1314) which are instantiated from base structures that only
consist of L, D and S tags; and (2) all the pre-terminals (e.g., N4B5
and N31314) which are intermediate guesses that consist of PII-
based tags. For these intermediate guesses, we further instantiate
them with the target user’s PII.
As with GI, our GIII is also highly adaptive. The reasons
are similar with that of GI (see Sec. 4.1). This means that a
new semantic tag, namely W1 for website name, can be easily
incorporated into GIII as with these PII tags. Now, GIII can parse
Alice1978Yahoo into the structure N4B5W1, and the guess
Alice1978eBay can be generated with the highest probability,
because no transformation rules will be involved in the process
from N4B5W1 to Alice1978eBay.
(b)Segment-level insertion/deletion.
The right two rows is better trained
using Markov n-grams.
(a) Structure-level insertion/deletion.
Figure 11: Training of two levels of insertion and deletion. As over
99% of passwords are with len (cid:20)16 [21], only segments with len (cid:20)16
are considered by us. The right-most two rows in Fig. 11(a) is better
trained by using PCFG [35] on a million-sized password list.
a 124.83% improvement.
In a series of 10 experiments in Sec.
5, under the same personal information and within 100 guesses,
TarGuess-II outperforms their algorithm [12] by 8.12%(cid:24)300%
(avg. 111.06%). One may conjecture that the two variations of
TarGuess-II employ more personal information than one sister PW,
and thus they are more powerful. In what follows, we, for the ﬁrst
time, provide more than anecdotal evidence to back this conjecture.
4.3 TarGuess-III
TarGuess-III aims to online guess a user U’s passwords by ex-
ploiting U’s one sister password as well as some PII. This is
realistic: if the attacker A wants to target U and knows U’s one
sister password, it is likely that A can also obtain some PII (e.g.,
email, name) about U. As far as we know, no public literature
has ever paid attention to this kind of attacking scenario. Here we
mainly consider type-1 PII (e.g., name and birthday), while type-2
PII (e.g., gender and age) will be dealt with in Sec. 4.4.
Given a limited number of guesses, more information available
to TarGuess-III generally means more messy things to be consid-
ered and thus more challenges to be addressed. Suppose A wants
to target Alice Smith’s account at eBay which requires passwords
to be of length 8+, and knows Alice was born in 1978 and one
of Alice’s passwords Alice1978Yahoo was leaked from Yahoo.
Given guesses Alice1978eBay, Alice1978 and 12345678,
which one shall A try ﬁrst? If Alice’s leaked password is 123456,
will the choice vary? Answering this question necessitates an
adaptive, PII-aware cross-site guessing model.
Fortunately, we ﬁnd that TarGuess-III can fulﬁll this goal by
introducing the PII-based tags (which we have proposed in GI
of TarGuess-I) into the grammar GII of TarGuess-II. In this way,
we can build a PII-enriched, password reuse-based grammar GIII.
More speciﬁcally, besides the L, D, S tags in GII, our grammar
GIII further includes six types of PII usages as with GI, and adds a
number of type-based PII tags (e.g., N1(cid:24)N7 and B1(cid:24)B10 as shown
in Sec. 4.1) into V of GII.
In the training phase, all the PII-based password segments (each
of which is parsed with one kind of PII tag) only involve the six
structure-level transformation rules as deﬁned in GII, and all the
other things in GIII remain the same with that of GII. In the guess
generation phase, from GIII we derive: (1) all the terminals (e.g.,
Figure 12: A comparison of TarGuess II(cid:24)IV and Das et al.’s algorith-
m [12], trained on the 66,573 non-identical PW pairs of 126!CSDN
and tested on the 30,8045 non-identical PW pairs of Dodonew!CSDN.
Besides a sister password, TarGuess-III uses four types of 51job type-1
PII and TarGuess-IV further uses the gender information.
As shown in Fig. 12, even if U does not exactly reuse her
Dodonew password for her CSDN account, TarGuess-III can still
achieve a success rate of 23.48% when allowed to try only 100
guesses, being 16.3% more effective than TarGuess-II. Among
these un-cracked PW pairs by TarGuess-III, over 80% are signif-
icantly different (with LD similarity scores<0.5).
4.4 TarGuess-IV
TarGuess-IV aims to online guess a user U’s passwords by
exploiting U’s one sister password as well as both type-1 and type-
2 PII. A major challenge is that, type-2 PII (e.g., gender) can not
be directly measured using any PII tag-based PCFG grammars or
Markov n-grams. We tackle this issue by proving a theorem and
leveraging the Bayesian theory.
THEOREM 1. Let pw denote the event that the password pw
′
is selected by U for a service, pw
denote the event that pw
is selected by U for another service and was leaked, Ai (i =
1; 2;(cid:1)(cid:1)(cid:1) ; n) denote one kind of user PII attributes, including both
type-1 and type-2 ones. We have
′
∏
′
i=1 Pr(pw|pw
Pr(pw|pw′)n−1
n
;Ai)
;
Pr(pwjpw
′
; A1; A2;(cid:1)(cid:1)(cid:1) ; An) =
′
′
).3
and (pw; pw
under the assumptions that A1; A2;(cid:1)(cid:1)(cid:1) ; An are mutually indepen-
dent, and that they are also mutually independent under the events
pw
Since A1;(cid:1)(cid:1)(cid:1) ; An are assumed to be mutually
independent, we have Pr(A1;(cid:1)(cid:1)(cid:1) ; An) =
i=1 Pr(Ai). Since
3Note that, the assumptions in Theorem 1 do not contradict with the fact
that A1; (cid:1) (cid:1) (cid:1) ; An are dependent on the events pw
′ and (pw; pw
∏
Proof.
′).
n
∏
they are also assumed to be mutually independent under the events
i=1Pr(Aijpw),
pw
) and Pr(A1;(cid:1)(cid:1)(cid:1) ;
Pr(A1;
Anjpw; pw
∏
i=1 Pr(Aijpw
). Now, we can derive:
and (pw; pw
n
n
′
′
=
=
′
′
n
∏
), thus Pr(A1;(cid:1)(cid:1)(cid:1) ; Anjpw)=
(cid:1)(cid:1)(cid:1) ; Anjpw
′
′
) =
i=1 Pr(Aijpw; pw
′
) =
; A1; A2;(cid:1)(cid:1)(cid:1) ; An)
Pr(pwjpw
Pr(pw; A1; A2;(cid:1)(cid:1)(cid:1) ; Anjpw
′
)
Pr(A1; A2;(cid:1)(cid:1)(cid:1) ; Anjpw′)
∏
Pr(A1; A2;(cid:1)(cid:1)(cid:1) ; Anjpw; pw
) (cid:1) Pr(pwjpw
(∏
i=1 Pr(Aijpw; pw
∏
(∏
i=1 (Pr(Aijpw; pw
′
∏
i=1 Pr(Aijpw′)
Pr(pw;Ai|pw
Pr(Ai|pw′)
Pr(pwjpw′)n−1 =
) (cid:1) Pr(pwjpw
′
i=1 Pr(Aijpw′)
i=1 Pr(Aijpw′)
) (cid:1) Pr(pwjpw′)n−1
) (cid:1) Pr(pwjpw
∏
i=1 Pr(pwjpw
′
Pr(pwjpw′)n−1
∏
n
i=1
))
)
)
n
n
n
n
n
n
′
′
′
′
′
)
=
=
=
)
; Ai)
(cid:4)
:
′
′
This theorem indicates that the problem of predicting a user
at one service, when given U’s PII information A1, A2,
U’s pw
(cid:1)(cid:1)(cid:1) ; An and the sister password pw at another service, can be
addressed by a “divide-and-conquer” approach. More speciﬁcally,
we can ﬁrst compute Pr(pwjpw
; Ai) for each i and Pr(pwjpw
′
),
and then compute the ﬁnal goal Pr(pwjpw
; A1; A2;(cid:1)(cid:1)(cid:1) ; An).
Fortunately, Pr(pwjpw
) can be computed using TarGuess-II, and
Pr(pwjpw
; Ai) can be obtained by using TarGuess-III when Ai
is a type-1 PII. When there are 2+ type-1 PII attributes to be con-
sidered (suppose Al; Am; An), they together ﬁrst can be deemed
′
as one virtual attribute (e.g., to be A
l) in Theorem 1 and then be
addressed simultaneously by running TarGuess-III. The only issue
left is how to compute Pr(pwjpw
; Ai) when Ai is a type-2 PII.
To address it, we introduce the Bayesian theory. Without loss of
′
′
′
′
) (cid:1) Pr(pw
′
)
approximate Pr[(Akjpw)jpw
′
) (cid:1) Pr(pw
′
]
′
):
by
′
′