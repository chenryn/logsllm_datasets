title:Fault Tolerant Planning for Critical Robots
author:Benjamin Lussier and
Matthieu Gallien and
J&apos;er&apos;emie Guiochet and
F&apos;elix Ingrand and
Marc-Olivier Killijian and
David Powell
Fault Tolerant Planning for Critical Robots
Benjamin Lussier, Matthieu Gallien, Jérémie Guiochet,
Félix Ingrand, Marc-Olivier Killijian, David Powell
LAAS-CNRS, 7 avenue du Colonel Roche, 31077 Toulouse Cedex 4, France
ﬁPI:EMAIL
Abstract
Autonomous robots offer alluring perspectives in numer-
ous application domains: space rovers, satellites, medical
assistants, tour guides, etc. However, a severe lack of trust
in their dependability greatly reduces their possible usage.
In particular, autonomous systems make extensive use of
decisional mechanisms that are able to take complex and
adaptative decisions, but are very hard to validate. This
paper proposes a fault tolerance approach for decisional
planning components, which are almost mandatory in com-
plex autonomous systems. The proposed mechanisms fo-
cus on development faults in planning models and heuris-
tics, through the use of diversiﬁcation. The paper presents
an implementation of these mechanisms on an existing au-
tonomous robot architecture, and evaluates their impact on
performance and reliability through the use of fault injec-
tion.
1. Introduction
Autonomous systems cover a large range of functionali-
ties and complexities, from robotic pets to space rovers, in-
cluding elderly care assistants, museum tour guides, and au-
tonomous vehicles. As successes arise in autonomous nav-
igation, exempliﬁed by Mars rovers and the clearing of the
DARPA Grand Challenge [17], complex autonomous sys-
tems that are able to choose and execute high-level actions
without human supervision are not yet ready for real life
applications.
Indeed, one of the major drawbacks in the
utilization of such systems is the difﬁculty to predict and
validate their behavior. To increase the conﬁdence that we
may have in such systems so that they may be used in more
critical applications, we consider in this paper the tolerance
of residual development faults in planning models.
First, we introduce autonomous systems and speciﬁc as-
pects such as decisional mechanisms, robustness and plan-
ning. Second, we propose error detection and recovery
mechanisms that are appropriate for planning to tolerate de-
velopment faults in their application-dependent knowledge.
Finally, we validate the proposed mechanisms through an
experimental framework based on fault injection.
2. Dependability in Autonomous Systems
This section presents several aspects of autonomous sys-
tems relevant to their dependability. We present a deﬁnition
of autonomy and give key aspects of architectures for au-
tonomous robots.
2.1. Autonomy
A dictionary deﬁnition of “autonomy” is “the ability to
act independently”. However, in the ﬁeld of robotics, this
deﬁnition is insufﬁcient since it does not enable a distinc-
tion between classic automatic systems, that simply apply
preprogrammed reactions in response to the system’s inputs
(e.g., as in feedback control), and truly autonomous systems
that seek to carry out goal-oriented tasks whose implemen-
tation details are not deﬁned in advance, either by neces-
sity (the input space is unbounded) or as a design strategy
(to simplify the code). We adopt here the deﬁnition of au-
tonomy given in [11]: “An unmanned system’s own abil-
ity of sensing, perceiving, analyzing, communicating, plan-
ning, decision-making, and acting, to achieve its goals as
assigned by its human operator(s) through designed human-
robot interaction (HRI). Autonomy is characterized into lev-
els by factors including mission complexity, environmental
difﬁculty, and level of HRI to accomplish the missions.”
The level of autonomy of an autonomous system is of-
ten discussed in terms of its ”robustness”.
Indeed, au-
tonomous robots are intended to cope with uncertainty and
non-nominal situations. A good or ”robust” robot is under-
stood to be one that can survive and fulﬁll its mission de-
spite partial knowledge about its environment as well as un-
foreseen contingencies such as obstacles, rough terrain and
failures.
In this paper, we choose to distinguish between
robustness and fault tolerance as follows:
Robustness is the ability of an autonomous system to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:20 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007cope with adverse environmental situations (lighting con-
ditions, unexpected obstacles, etc.) while providing an ac-
ceptable service.
Fault Tolerance is the ability of an autonomous system
to provide an acceptable service despite system faults (hard-
ware failures, software bugs, etc.).
2.2. Autonomous System Architectures
Four architectural styles for designing autonomous robot
systems are usually distinguished:
1. The sense-plan-act style is based on a closed loop of
three components devoted respectively to sensing the
environment, ﬁnding a plan to reach a goal state, and
acting on the environment according to the plan.
2. The subsumption style allows several “behavior” com-
ponents to simultaneously sense and act on the envi-
ronment, with actions that can be prioritized or cross-
inhibited between different components.
3. The multi-agent style considers a set of autonomous
systems or agents immersed in the same environment
and interacting to achieve their individual or shared
goals.
4. The hierarchical style deﬁnes several abstraction lev-
els with different real-time constraints, resulting in a
layered architecture.
Whereas the sense-plan-act style has largely been aban-
doned (at least as the basis for a monolithic architecture)
due to its poor real-time performance, the subsumption style
is still commonly used in entertainment robots, such as
Sony’s Aibo™. The multi-agent style is now receiving con-
siderable attention both as the basis for designing a taskable
robot [18] and in the context of agent swarms with emerg-
ing “intelligence” [20]. However, most practical robots cur-
rently adopt the hierarchical style, usually resulting in an ar-
chitecture with three layers [8]: (a) a decisional layer that is
responsible for elaborating plans to reach operator-deﬁned
mission goals, (b) an executive layer that selects and se-
quences elementary actions that implement the high-level
tasks included with the current plan, and (c) a functional
layer that interfaces with the hardware sensory and action
devices. In some architectures the executive layer is merged
into either the decisional layer or the functional layer.
Hierarchical architectures for autonomy include the
RAX architecture developed by NASA as part of its Deep
Space One project [19], JPL’s CLARATy [23] and the
LAAS architecture [1] developed at LAAS-CNRS (the lat-
ter architecture will be described in more detail in 3.2.1).
From a dependability viewpoint,
tolerance of hardware
faults is considered in some of these architectures [16].
For example, the RAX architecture includes a model-based
mode identiﬁcation and reconﬁguration (MIR) component
speciﬁcally aimed at diagnosing and recovering from faults
affecting hardware resources [19]. For development faults,
apart from on-line checking mechanisms aimed at guaran-
teeing safety [21], the focus has largely been on fault avoid-
ance approaches (rigorous design, and thorough veriﬁca-
tion and testing). For example, intensive testing was carried
out on the RAX architecture [3]: six test beds were imple-
mented throughout the development process, incorporating
600 tests. The authors of [3] underline the relevance of in-
tensive testing, but acknowledge particular difﬁculties re-
garding autonomous systems, notably the problem of deﬁn-
ing suitable test oracles. Given the inherent difﬁculty of
testing autonomous systems, we believe that a tolerance ap-
proach with respect to residual development faults should
be of considerable interest. Yet, to the best of our knowl-
edge, such an approach has not been previously envisaged.
2.3. Deliberation and Decision
From our perspective, deliberation and decision are the
key features of autonomy. Many different decisional capa-
bilities have been studied and deployed on robots or other
autonomous systems. Here, we discuss what distinguishes
such decisional capabilities from other programmed func-
tionalities.
Most decisional mechanisms boil down to some sort of
search in a very large state space. In general, this search
leads to a decision (a plan to reach a goal, a diagnosis, an
action, etc.). This search can be done either off-line or on-
line, that is in advance to produce a precompiled data struc-
ture or on the ﬂy while the system is running. It may reason
about past states (as in diagnosis) or about future states (as
in planning). It may have a limited horizon or, conversely, a
very deep scope. But a key aspect is that the search needs to
be efﬁciently guided to avoid a combinatorial explosion. As
a result, a decisional mechanism can be complete (it is guar-
anteed to ﬁnd a solution if one exists) or not (it can “miss
it”), correct (solutions are always valid) or not (they are ap-
proximate), tractable (solutions are found in a polynomial
time and space) or not.
Another important feature of decisional mechanisms is
that they are organized in a way that makes a clear separa-
tion between the knowledge and the inference mechanism.
The aim is to make the inference mechanism (e.g., a search
engine) as generic and as independent as possible from the
application. Conversely, the knowledge is domain-speciﬁc
and typically speciﬁes what states are reachable through
the search process and what is the “best” next state from
any given state. However, knowledge and inference mech-
anisms are often tightly linked in practice (e.g., heuristics
that guide a search engine).
The implementation of decisional mechanisms relies on
various formalisms (logic, neural networks, Markov mod-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:20 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007els, constraints, simple temporal networks, etc.) and com-
putational models (constraint-based programming,
logic
programming, heuristic search, dynamic programming,
etc.).
The most common decisional functionalities deployed
on autonomous systems are the following: planning, execu-
tion control, diagnosis, situation recognition and learning.
In this paper, we focus particularly on planning, which is
the activity of producing a plan to reach a goal from a given
state, using given action models (e.g., the activity plan for
the day of an exploration rover).
2.4. Planning
Planning is necessary in complex autonomous systems
as a mean to select and organize the robot’s future actions to
achieve speciﬁed high-level goals. We introduce here some
generalities on planning in autonomous systems, before pre-
senting dependability issues.
2.4.1. General Principle. Planning can be implemented in
several ways but, in practice, two approaches are preferred:
search in a state space and constraint planning.
Search in a state space manipulates a graph of actions
and states. It explores different action sequences from an
initial state to choose the most suitable one to achieve given
goals.
Constraint planning uses CSP (Constraint Satisfaction
Problem) solving to determine a possible evolution of the
system state that satisﬁes a set of constraints, some of which
specify the system goals. CSP solving is commonly an it-
erative algorithm assigning successively possible values to
each variable and verifying that all constraints remain satis-
ﬁed.
Two robustness techniques are commonly implemented
to recover from a plan failure caused by adverse environ-
mental situations:
• Replanning consists in developing a new plan from the
current system state and still unresolved goals. De-
pending on the planning model complexity, replanning
may be signiﬁcantly time costly. Other system activi-
ties are thus generally halted during replanning.
• Plan repair may be attempted before replanning, with
the aim of reducing the time lost in replanning. It uses
salvageable parts of the previous failed plan, that are
executed while the rest of the plan is being repaired.
However, if reducing the salvaged plan conﬂicts with
unresolved goals, plan repair is stopped and replanning
is initiated.
2.4.2. Dependability Issues. Planning, like other deci-
sional mechanisms, poses signiﬁcant challenges for valida-
tion. Classic problems faced by testing and veriﬁcation are
exacerbated. First, execution contexts in autonomous sys-
tems are neither controllable nor completely known; even
worse, consequences of the system actions are often uncer-
tain. Second, planning mechanisms have to be validated in
the complete architecture, as they aim to enhance function-
alities of the lower levels through high level abstractions
and actions. Integrated tests are thus necessary very early
in the development cycle. Third, the oracle problem1 is
particularly difﬁcult since (a) equally correct plans may be