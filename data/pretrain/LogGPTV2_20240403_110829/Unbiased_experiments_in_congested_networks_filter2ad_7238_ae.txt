### 5.2 Measuring Algorithms in Development with Targeted Switchbacks

When deploying a new algorithm, running an event study can be a useful method to measure congestion interference and build intuition. However, it is not an ideal approach for experimenting with new algorithms. Deploying marginal algorithms to all traffic is undesirable, and we may avoid investing in algorithms that perform poorly in A/B tests. This could lead to missing out on algorithms that have significant effects when widely deployed, such as bitrate capping, pacing, or changing the number of TCP connections.

To address this, we recommend conducting small, targeted experiments in addition to A/B tests. In a targeted experiment, a large fraction of traffic within a specific network is allocated, ensuring that the allocated traffic does not interact with non-allocated traffic. For example, in the paired link experiment discussed in Section 4, we targeted two congested links. By analyzing the results from this large fraction allocation, we can obtain a good estimate of the Total Treatment Effect (TTE) and spillover in the network.

Targeted experiments allow us to estimate TTE and spillover within a specific network without needing to run the algorithm on 100% of the global traffic. This practice is standard in online platforms [52, 68]. While these estimates are specific to the network, they provide additional context to A/B test results and enhance our understanding of how a new algorithm behaves.

### 5.3 Evaluating Alternate Designs Using Switchback Experiments

When conducting targeted experiments, we recommend using switchback designs. A switchback design divides time into intervals, with each interval randomly assigned to either treatment or control. In a treatment interval, almost all traffic is treated with the new algorithm, while in a control interval, almost all traffic runs the old algorithm.

At a high level, switchback experiments are analyzed by comparing the treatment and control intervals. Although 100% allocations in these intervals can provide a good TTE estimate, we recommend a smaller allocation (e.g., 90-99%) as in the paired link experiment. This allows us to estimate spillover and the bias of A/B tests, providing valuable insights into algorithm behavior. The allocation size should be large enough to yield statistically significant results, which can be determined through a power calculation.

Similar to event studies, switchback experiments rely on the change between treatment and control intervals being due to the treatment. However, the assumption is weaker: instead of requiring no other events to impact the outcome, a switchback only requires that another event does not align with the treatment intervals.

Switchback experiments can be vulnerable to carryover effects [14, 32], where the presence of the treatment algorithm influences the initial conditions of the control algorithm and vice versa. This can cause bias. For instance, switching sessions between one and two parallel connections can result in lower throughput until all sessions using two parallel connections are completed. If the system reacts poorly to switching between treatment and control, this can also cause issues.

Carryover effects can be mitigated with sufficiently long intervals. Typically, switchback experiments assume the worst-case scenario where all sessions in an interval are dependent, meaning each interval provides one data point. Increasing the length of intervals effectively reduces the sample size. For networking algorithms, a switch interval of one day is a reasonably conservative starting point. Depending on the setting and the algorithm, shorter intervals (hours or minutes) may be appropriate.

### 5.4 Results and Analysis

The analysis approach for these experiments is identical to the paired link experiment, with the exception that we only use the subset of data corresponding to each experiment. Details are provided in Appendix B.

Figure 10 shows the TTE values estimated by the switchback experiment, event study, and paired link experiment. Both alternate experiments provide reasonably good TTE estimates. The switchback experiment results are very close, and the confidence intervals include every TTE from the paired link experiment. The larger confidence intervals are due to the reduced data set. Running the experiment for a longer period would likely reduce the size of the confidence intervals.

The event study gives reasonably accurate TTE estimates for most metrics but is biased for throughput, cancelled starts, and % retransmitted bytes. This bias is likely due to seasonality issues, as weekends tend to have different behavior than weekdays, making it more challenging to attribute changes to the treatment. Despite this, given the ease of incorporating event studies into existing workflows, we still recommend using them cautiously to estimate TTE and spillovers when deploying new algorithms.

### 6. Related Work

A/B tests are widely used in industry research. Recently, several published A/B tests have compared congestion control algorithms, including BBR [17–19, 46, 72], COPA [63], and Swift [55]. There have also been many other published A/B tests for other networking algorithms, such as initial congestion windows [25], TCP’s loss recovery [29], PRR [24], QUIC [49, 58], failure recovery [57], and ABR algorithms [42, 60, 87]. We do not know how congestion interference affected these results.

We are aware of a few published results that include event studies: Dropbox and Verizon both used them to evaluate BBRv1 [46, 72], and Google reported one for Timely in [61]. In Section 5, we show how to design and analyze these event studies to measure TTE and spillover, and describe how switchback experiments provide more reliable results.

Experiments on router performance, especially those related to buffer sizing [11, 12, 74], naturally treat all traffic using the router, leading to good estimates of total treatment effects.

Recent studies of social network and marketplace platforms have improved our understanding of causal inference under interference [4, 6, 9, 13, 59], both through novel experimental design [7, 14, 20, 32, 40, 47, 68, 83] and improved inferential methodology [6, 9, 10, 77]. Our work is the first to show that these issues affect networking experiments and bias their results at scale.

Switchback designs have recently gained favor for testing matching and dispatch policies in ridesharing and food delivery platforms, though they have also been used in diverse applications such as agriculture [14, 20, 52, 64, 65]. We are unaware of any prior usage of switchbacks in networking.

We have heard some anecdotal predictions from the networking community about these issues. The only citeable reference we know of is in [80].

Finally, our work is informed by the extensive research on fairness in networking. Unfairness between Cubic and BBR, described in Section 3, was previously reported by [16, 43, 44, 71, 81, 82, 84, 85]. Unfairness between parallel connections was first observed by [8], and unfairness between paced and unpaced Reno flows was shown by [2, 86]. Fairness research focuses on how algorithms should share resources and often demonstrates unfairness in simulations or lab settings [5, 15, 16, 23, 43, 44, 50, 56, 71, 81, 82, 85]. Our work, however, addresses how to avoid experimental bias rather than how algorithms should share resources.

### 7. Conclusion

Congestion interference biases the results of networking A/B tests at scale, and it is our responsibility as a community to be aware of this phenomenon. Our results suggest that we should be skeptical when interpreting the results of naïve A/B tests and consider using alternate experiment designs.

As discussed in Section 5, experimenters can make small changes to existing deployment processes to begin measuring congestion interference and use targeted switchbacks to further improve these measurements. We should be particularly cautious of interference when an algorithm changes traffic volumes, tries to control congestion, or is similar to algorithms discussed in past fairness research.

We encourage more work in networking to be evaluated with congestion interference in mind, either through published switchback experiments or event studies during gradual deployments. This is especially important for high-consequence proposals, such as new internet standards.

On the research side, there is much more work to be done on evaluating algorithms at scale in congested networks. We encourage further studies to measure bias in different networks and with different algorithms. Designing new experiments and analyses specifically for congested networks would be valuable. The bias of naïve A/B tests is both a cautionary tale and a significant opportunity for innovation. The internet works better thanks to A/B tests of algorithms run in congested networks, and we hope that new algorithms tested with better experiments will help improve it even further.

### 8. Acknowledgements

Thank you to Guillaume Basse and Matthew Pawlicki for the very helpful discussions. Thanks also to Neil Perry, Sundararajan Renganathan, and others for their contributions.