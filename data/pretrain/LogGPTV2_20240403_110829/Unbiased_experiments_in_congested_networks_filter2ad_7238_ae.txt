algorithms could be affected by congestion interference.
5.2 Measure algorithms in development with
targeted switchbacks
Running an event study when deploying a new algorithm is a good
way to measure congestion interference and build intuition, but it
is a bad way to experiment with new algorithms. We do not want
to deploy marginal algorithms to all traffic, and so we may not
invest in algorithms that perform poorly in an A/B test. We may
miss out on algorithms that have very different effects when widely
deployed, like bitrate capping, pacing, or changing the number of
TCP connections.
Because of this, we recommend running small targeted experi-
ments in addition to small A/B tests. A targeted experiment allocates
a large fraction of traffic within a specific network. The network
needs to be structured in such a way that the allocated traffic does
not interact with non-allocated traffic. In the paired link experiment
in Section 4, we targeted an experiment to two congested links.
Using the results from the large fraction allocation, we can get a
good estimate of TTE and spillover in this network.
Targeting an experiment allows us to estimate TTE and spillover
within a network, without needing to run an algorithm on 100% of
traffic globally. It is standard practice in online platforms [52, 68].
While we estimate TTE and spillover for a specific network instead
of globally, this helps give additional context to A/B test results and
improves our understanding of how a new algorithm behaves.
10
89
Unbiased Experiments in Congested Networks
IMC ’21, November 2–4, 2021, Virtual Event, USA
When running these targeted experiments, we recommend using
switchback designs. A switchback design divides time into intervals;
a given interval is randomly assigned to be either treatment or
control. In a treatment interval, we treat almost all of the traffic
with the new algorithm. In a control interval, almost all traffic runs
the old algorithm.
At a high level, switchback experiments are analyzed by compar-
ing the treatment and control intervals. While we could do 100%
allocations in these intervals to get a good TTE estimate, we rec-
ommend a smaller allocation (e.g. 90-99%) as in the paired link
experiment. Doing so allows us to additionally estimate spillover
and the bias of A/B tests, which gives valuable insight into algo-
rithm behavior. The allocation size should be large enough to give
statistically significant results, and can be determined by a power
calculation.
Like event studies, switchback experiments rely on the change
between treatment and control intervals being due to the treatment.
However, the assumption is weaker: instead of needing no other
events to impact the outcome, a switchback requires that another
event does not line up with the treatment intervals.
A switchback experiment can also be vulnerable to carryover
effects [14, 32]. The presence of the treatment algorithm can influ-
ence the initial conditions of the control algorithm and vice versa.
This can cause bias: imagine if we were to switch sessions between
one and two parallel connections. Until all sessions that used two
parallel connections had completed, the sessions using one would
have lower throughput than necessary. If the system reacts poorly
to switching between treatment and control, this could also cause
problems.
Carryover effects can be mitigated with sufficiently long inter-
vals. However, typically switchback experiments make the worst-
case assumption that all sessions in an interval are dependent (see
Appendix B for more details), which essentially means that each
interval gives us one data point. Increasing the length of intervals
effectively lowers the sample size of the experiment. For networking
algorithms, we believe a switch interval of one day is a reasonably
conservative place to start. Depending on the setting and the algo-
rithm, it may be appropriate to use a shorter interval on the order
of hours or minutes.
5.3 Evaluating alternate designs
Our paired link experiment gives us the results of simultaneous,
comparable experiments. We previously analyzed that data to esti-
mate TTE and spillovers. We now use it to evaluate event studies
and switchback designs, and show that these designs also accurately
estimate TTE.
Having two simultaneous experiments allows us to ask what
would have happened if we ran only one experiment at a time.
Our experiment in Section 4 ran from Wednesday through Sunday,
giving us five possible days of data. We can emulate an event study
by using data from the 5% link for a few days and then switching
to data from the 95% link, representing a deployment of bitrate
capping to 95% of traffic. We can emulate a switchback experi-
ment by switching between treatment days and control days more
frequently.
11
90
We first used baseline data to calibrate a switchback experiment.
We ran an A/A test [54, Ch. 19] on the paired links in the week
following our main experiment: we applied the control to both links
and looked for underlying differences. Using the data from the A/A
test, we checked that there would have been no false positives with
any switchback design. This increases confidence that there isn’t
a reliable difference between days in a way that would bias the
experiment, and we would recommend doing this in most cases.
We also used baseline data to calibrate an event study. We ob-
served that there were false positives in the majority of metrics
with any event study in this experiment. We believe this is because
weekends tend to have different traffic patterns than weekdays,
and an event study must either treat all the weekend days or all
the weekdays together. This is an advantage of using a switchback
design.
For the event study, we switched to 95% bitrate capping between
Thursday and Friday as shown in Figure 11. For the switchback, we
alternated between treatment and control, and randomly started
with treatment. This assignment is shown in Figure 12. All other
ways of assigning treatment to days yielded similar results, provided
at least one day was in treatment and at least one day was in control.
Figure 12 shows the average throughput for this example switch-
back design, which can then be compared with the throughput in
the paired link experiment in Figure 6. Note that because we are
switching between experiments, the clear difference in throughput
in the paired link time series is much harder to see in the switch-
back. This highlights the power of running statistical analyses on
switchback data.
Our goal with this approach was to use the clean results from
our paired link experiment to demonstrate the power of switch-
back experiments and event studies. If we had actually run these
experiments, the results may have been slightly different. For in-
stance, traffic from both links likely shares some bottlenecks in the
provider network during offpeak hours, so it is possible that our
results during offpeak hours are biased by congestion interference.
However the congestion interference we detect is largely because
of the behavior during congested hours on isolated congested links.
5.4 Results
The analysis approach for these experiments is identical to the
paired link experiment, with the caveat that we only use the subset
of the data corresponding to each experiment. We describe the
details in Appendix B.
Figure 10 shows the values of TTE estimated by the switchback
experiment, event study, and paired link experiment. Both alternate
experiments give reasonably good estimates of TTE. The switchback
experiment results are very close, and the confidence intervals for
its estimates include every TTE from the paired link experiment.
It has larger confidence intervals because it includes half as much
data. We expect that running the experiment for longer would have
reduced the size of the confidence intervals.
The event study gives reasonably accurate estimates of TTE for
most metrics, but is biased for throughput, cancelled starts, and %
retransmitted bytes. As we observed in analyzing the baseline data,
we believe this is because of seasonality issues: weekends tend to
have different behavior than weekdays, and so it is more difficult to
IMC ’21, November 2–4, 2021, Virtual Event, USA
Spang et al.
Figure 10: TTE as estimated by the paired link experiment, a switchback experiment, and an event study.
[55]. There have also been many other published A/B tests for other
networking algorithms. These include work on initial congestion
windows [25], TCP’s loss recovery [29], PRR [24], QUIC [49, 58],
failure recovery [57], and ABR algorithms [42, 60, 87]. We do not
know how congestion interference affected these results.
We are aware of a few published results that include event studies:
Dropbox and Verizon both used them to evaluate BBRv1 [46, 72],
and Google reported one for Timely in [61]. In Section 5, we show
how to design and analyze these event studies to measure TTE
and spillover, and describe how switchback experiments give more
reliable results.
Experiments on router performance, especially those related to
buffer sizing [11, 12, 74], naturally must treat all traffic using the
router. Because of this, they tend to have good estimates of total
treatment effects.
Recent studies of social network and marketplace platforms have
led to improved understanding of causal inference under interfer-
ence (e.g., [4, 6, 9, 13, 59]), both through novel experimental design
(e.g., [7, 14, 20, 32, 40, 47, 68, 83]) and improved inferential methodol-
ogy (e.g., [6, 9, 10, 77]). We believe our work is the first to show that
these issues affect networking experiments and bias their results at
scale.
Switchback designs found recent favor as an approach to testing
matching and dispatch policies in ridesharing and food delivery
platforms, though they have also been used in applications as varied
as agriculture [14, 20, 52, 64, 65]. We are unaware of any prior usage
of switchbacks in networking.
We have heard some folklore predictions from the networking
community that these sort of issues may exist. The only citeable
version of this we know of is in [80].
Finally, our work is informed by the long line of work on fair-
ness in networking. Unfairness between Cubic and BBR, which we
describe in Section 3, was previously reported by [16, 43, 44, 71,
81, 82, 84, 85]. Unfairness between parallel connections was first
observed by [8]. Unfairness between paced and unpaced Reno flows
was shown by [2, 86]. Fairness work is about how algorithms ought
to share resources, and usually shows that algorithms are unfair
in simulations or in a lab [5, 15, 16, 23, 43, 44, 50, 56, 71, 81, 82, 85].
Our work does not address how algorithms should share resources,
but rather how to avoid experimental bias when they do. One way
Figure 11: Throughput in a bitrate capping event study. Be-
tween Thurs. and Fri., we apply 95% bitrate capping.
Figure 12: Average throughput over time in a bitrate capping
switchback experiment. 95% of traffic is capped on the first
and third and fifth day.
attribute the change to the treatment. This is one of the advantages
of switchback experiments: randomly choosing intervals over many
days helps avoid certain seasonality effects. Despite this, given that
event studies are so easy to incorporate into existing workflows,
we still recommend cautiously using them to estimate TTE and
spillovers when deploying new algorithms.
6 RELATED WORK
A/B tests are heavily used in industry research. There recently have
been a number of published A/B tests comparing congestion control
algorithms, including BBR [17–19, 46, 72], COPA [63], and Swift
12
91
Unbiased Experiments in Congested Networks
IMC ’21, November 2–4, 2021, Virtual Event, USA
of interpreting our work is as a way to measure unfairness between
treatment and control at scale, in production networks.
7 CONCLUSION
Congestion interference biases the results of networking A/B tests
at scale, and it is our responsibility as a community to be aware
of this phenomenon. Our results suggest that we should be skepti-
cal when interpreting the results of naïve A/B tests, and consider
whether alternate experiment designs should be used instead.
As discussed in Section 5, experimenters can make small changes
to existing deployment processes to begin to measure congestion
interference, and use targeted switchbacks to further improve these
measurements. We should be especially wary of interference when
an algorithm changes traffic volumes, tries to control congestion,
or is similar to algorithms discussed in the past fairness research in
Section 6.
We would love to see more work in networking evaluated with
congestion interference in mind, either with published switchback
experiments, or at least event studies run during a gradual deploy-
ment. This is especially true for high consequence proposals, such
as new internet standards.
On the research side, there is much more work to be done on
evaluating algorithms at scale in congested networks. We encourage
further studies to measure bias, in different networks and with
different algorithms. We think it would be valuable to design new
experiments and analyses specifically for congested networks. The
bias of naïve A/B tests is both a cautionary tale and a significant
opportunity for innovation. The internet surely works better thanks
to A/B tests of algorithms run in congested networks. We hope that
new algorithms tested with better experiments will help improve it
even further.
8 ACKNOWLEDGEMENTS
Thank you to Guillaume Basse and Matthew Pawlicki for the very
helpful discussions. Thanks also to Neil Perry, Sundararajan Ren-