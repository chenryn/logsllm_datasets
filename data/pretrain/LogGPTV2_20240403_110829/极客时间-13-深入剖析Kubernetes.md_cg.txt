## 总结在本篇文章中，我为你详细讲述了 Kubernetes 里关于 Pod的优先级和抢占机制的设计与实现。这个特性在 v1.11 之后已经是 Beta 了，意味着比较稳定了。所以，我建议你在Kubernetes 集群中开启这两个特性，以便实现更高的资源使用率。
## 思考题当整个集群发生可能会影响调度结果的变化（比如，添加或者更新Node，添加和更新 PV、Service 等）时，调度器会执行一个被称为MoveAllToActiveQueue 的操作，把所调度失败的 Pod 从 unscheduelableQ移动到 activeQ 里面。请问这是为什么？一个相似的问题是，当一个已经调度成功的 Pod 被更新时，调度器则会将unschedulableQ 里所有跟这个 Pod 有 Affinity/Anti-affinity 关系的Pod，移动到 activeQ 里面。请问这又是为什么呢？感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。![](Images/e870b7df0db49509e735e6becd4a9a9a.png){savepage-src="https://static001.geekbang.org/resource/image/47/55/47a6f3bf6b92d58512d5a2ed0a556f55.jpg"}
# 44 \| Kubernetes GPU管理与Device Plugin机制你好，我是张磊。今天我和你分享的主题是：Kubernetes GPU 管理与 DevicePlugin 机制。2016 年，随着 AlphaGo 的走红和 TensorFlow 项目的异军突起，一场名为 AI的技术革命迅速从学术界蔓延到了工业界，所谓的 AI 元年，就此拉开帷幕。当然，机器学习或者说人工智能，并不是什么新鲜的概念。而这次热潮的背后，云计算服务的普及与成熟，以及算力的巨大提升，其实正是将人工智能从象牙塔带到工业界的一个重要推手。而与之相对应的，从 2016 年开始，Kubernetes社区就不断收到来自不同渠道的大量诉求，希望能够在 Kubernetes 集群上运行TensorFlow等机器学习框架所创建的训练（Training）和服务（Serving）任务。而这些诉求中，除了前面我为你讲解过的Job、Operator等离线作业管理需要用到的编排概念之外，还有一个亟待实现的功能，就是对 GPU等硬件加速设备管理的支持。不过， 正如同 TensorFlow 之于 Google 的战略意义一样，**GPU 支持对于Kubernetes项目来说，其实也有着超过技术本身的考虑**。所以，尽管在硬件加速器这个领域里，Kubernetes上游有着不少来自 NVIDIA 和 Intel等芯片厂商的工程师，但这个特性本身，却从一开始就是以 Google Cloud的需求为主导来推进的。``{=html}而对于云的用户来说，在 GPU的支持上，他们最基本的诉求其实非常简单：我只要在 Pod 的 YAML里面，声明某容器需要的 GPU 个数，那么 Kubernetes为我创建的容器里就应该出现对应的 GPU 设备，以及它对应的驱动目录。以 NVIDIA 的 GPU设备为例，上面的需求就意味着当用户的容器被创建之后，这个容器里必须出现如下两部分设备和目录：1.  GPU 设备，比如 /dev/nvidia0；2.  GPU 驱动目录，比如 /usr/local/nvidia/\*。其中，GPU 设备路径，正是该容器启动时的 Devices参数；而驱动目录，则是该容器启动时的 Volume 参数。所以，在 Kubernetes 的GPU 支持的实现里，kubelet实际上就是将上述两部分内容，设置在了创建该容器的 CRI （Container RuntimeInterface）参数里面。这样，等到该容器启动之后，对应的容器里就会出现 GPU设备和驱动的路径了。不过，Kubernetes 在 Pod 的 API 对象里，并没有为 GPU专门设置一个资源类型字段，而是使用了一种叫作 ExtendedResource（ER）的特殊字段来负责传递 GPU 的信息。比如下面这个例子：    apiVersion: v1kind: Podmetadata:  name: cuda-vector-addspec:  restartPolicy: OnFailure  containers:    - name: cuda-vector-add      image: "k8s.gcr.io/cuda-vector-add:v0.1"      resources:        limits:          nvidia.com/gpu: 1可以看到，在上述 Pod 的 limits字段里，这个资源的名称是`nvidia.com/gpu`，它的值是 1。也就是说，这个 Pod声明了自己要使用一个 NVIDIA 类型的 GPU。而在 kube-scheduler里面，它其实并不关心这个字段的具体含义，只会在计算的时候，一律将调度器里保存的该类型资源的可用量，直接减去Pod 声明的数值即可。所以说，Extended Resource，其实是 Kubernetes为用户设置的一种对自定义资源的支持。当然，为了能够让调度器知道这个自定义类型的资源在每台宿主机上的可用量，宿主机节点本身，就必须能够向API Server 汇报该类型资源的可用数量。在 Kubernetes里，各种类型的资源可用量，其实是 Node 对象 Status字段的内容，比如下面这个例子：    apiVersion: v1kind: Nodemetadata:  name: node-1...Status:  Capacity:   cpu:  2   memory:  2049008Ki而为了能够在上述 Status 字段里添加自定义资源的数据，你就必须使用 PATCHAPI 来对该 Node 对象进行更新，加上你的自定义资源的数量。这个 PATCH操作，可以简单地使用 curl 命令来发起，如下所示：    
# 启动 Kubernetes 的客户端 proxy，这样你就可以直接使用 curl 来跟 Kubernetes  的 API Server 进行交互了$ kubectl proxy 