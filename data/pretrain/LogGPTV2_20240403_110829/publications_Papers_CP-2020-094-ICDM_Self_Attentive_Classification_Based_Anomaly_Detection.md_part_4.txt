### Optimized Text

**Performance Comparison in Fig. 4:**
Logsy generally achieves the best performance across the training data. For instance, on the Blue Gene/L dataset, Logsy scores an F1-score of 0.32 with 10% training data, averaging 0.448 across all splits. The highest F1-score for the baselines on this dataset is 0.24. In the Thunderbird dataset, the difference is even more pronounced, with Logsy achieving an F1-score of 0.99 with just 10% training data. On the Spirit dataset, Logsy scores 0.77. Both DeepLog and PCA consistently achieve lower F1-scores in all experiments. 

**Baseline Performance Characteristics:**
The baselines exhibit very high recall but low precision. This means they can detect anomalies but produce a large number of false positives. In contrast, Logsy maintains high recall while significantly improving precision, correctly classifying new log messages as normal or anomalous, even in unseen samples.

**Effect of Auxiliary Data on Evaluation Scores:**
In this experiment, we analyze how Logsy performs with varying sizes of auxiliary data. We use a 20%-80% train/test split for all datasets. As shown in Fig. 5, increasing the auxiliary data from 1 to 250,000 results in improved evaluation scores. However, the scores do not change significantly when the auxiliary data size increases from 100,000 to 250,000, indicating that 100,000 random samples are sufficient. Even a single auxiliary sample, which could be artificially generated, acts as a regularizer, preventing trivial solutions. Increasing the variety of data (e.g., including more diverse log datasets) could further improve performance by providing more representative samples of abnormality.

**Incorporating Expert Labeling:**
Often, systems are operated by human experts who can provide or manually label samples to enhance model performance. We experiment with incrementally including labeled anomaly samples from the target dataset. As shown in Fig. 6, increasing the number of labeled anomaly samples improves performance. With just 2% labeled data, Logsy achieves an F1-score of 0.8. This demonstrates that adding a small percentage of labeled anomalies significantly boosts Logsy's performance.

**Utilization of Learned Log Embeddings:**
We extract the learned log message vector representations from the trained Logsy and visualize them using T-SNE (Fig. 7). The log vectors are structured according to our spherical loss function, with normal samples concentrated around the center of a hypersphere and anomalies dispersed outside. We also replace the original TF-IDF log representations in PCA with the extracted embeddings from Logsy. As shown in Fig. 8, this replacement improves PCA's performance, with F1-score improvements of 0.09, 0.11, and 0.01 for Blue Gene/L, Thunderbird, and Spirit, respectively. This indicates that the log representation learning has a positive impact not only in Logsy but also in other methods, with an average improvement of 28.2% in the F1-score.

**Conclusion:**
Log anomaly detection is crucial for enhancing the security and reliability of computer systems. Existing approaches often lack generalization on new, unseen log samples due to system updates and processing noise. To address this, we propose Logsy, a new anomaly detection approach based on a self-attention encoder network with a hyperspherical classification objective. Our method discriminates between normal training data from the system of interest and auxiliary log datasets from other systems, representing abnormality. Experimental evidence shows that Logsy outperforms baselines by a significant margin of 0.25 in F1 score. Additionally, the log vector representations produced by Logsy can be utilized in other methods, such as PCA, where we observed a 28.2% improvement in F1 score.

**Future Research:**
Future research should focus on finding alternative ways to incorporate richer domain bias, emphasizing the diversity of normal and anomalous data.

**References:**
[1] F. E. Grubbs, "Procedures for detecting outlying observations in samples," Technometrics, vol. 11, no. 1, pp. 1–21, 1969.
...
[32] L. v. d. Maaten and G. Hinton, "Visualizing data using t-SNE," Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579–2605, 2008.