the baselines in Fig. 4. Generally, Logsy achieves the best training data. For example, in Blue Gene/L we have 0.32 F1-
scores, having an averaged F1 score in all the splits of 0.448 score on 10% training data, while the largest F1-score of the
on the Blue Gene/L dataset, 0.99 on the Thunderbird dataset, baselines is 0.24. In Thunderbird, this difference is even more
and 0.77 on the Spirit data. Both DeepLog and PCA, have noticeable, where an F1-score of 0.99 is already achieved in
lowerF1scoresinallexperimentsperformed.Itisshownthat with the first 10%. This shows that even with a small amount
the baselines have a very high recall, but also low precision. of training data from the target system, Logsy extracts the
This means they can find the anomalies, however, producing neededinformationofwhatcausesalogmessagetobenormal
large amounts of false-positive predictions. Logsy, on the or anomaly, and produces accurate predictions even in unseen
other hand, preserves the high recall across the datasets and samples.
evaluation scenarios but shows a large improvement in the 1) Theeffectoftheauxiliarydataontheevaluationscores:
precisionscores.Thisisduetothecorrectclassificationofnew In this experiment, we perform an analysis of how Logsy
1.0 precision 0.94 0.94 1.0 p0r.e9c9ision 0.990.990.99 0.990.990.99 0.990.990.99 1.0 precision 1.0 1.0 1.0
recall 0.9 recall recall
0.84 0.84 0.84
0.8 F1 0.8 F1 0.8 F1
0.72 0.72 0.72
0.6 0.54 0.6 0.53 0.6 0.5
0.4 0.34 0.41 0.41 0.4 0.36 0.4 0.37 0.42
0.26 0.26
0.2 0.2 0.2 0.2 0.2
0.12
0.0 0.0 0.0
1 10000 100000 250000 1 10000 100000 250000 1 10000 100000 250000
Fig.5. Increasingthesizeauxiliarydataset,wherethetargetsystemareBlueGene/L,Thunderbird,andSpirit(left,middle,right)on20%train-80%test
split
must be addressed with an understanding of what causes a
1.0 precision
log message to be normal or anomaly. The labelled anomalies
recall 0.89
0.8 F1 0.81 0.8 from the target system present information Logsy exploits to
0.72 learn the differences between the normal and anomalous logs
0.6 on the target dataset.
0.51
0.4
0.3
0.2 0.2 0.18
0.12 normal
anomaly
0.0
0.02% 0.21% 2.1%
Fig. 6. Increasing the size of the labeled anomaly data in the Blue Gene/L
dataset(20%train-80%test).
performs when with various sizes of the auxiliary data. We
evaluatethesametargetvsauxiliarydatasplitforalldatasets.
We evaluate the approach on the 20%-80% train/test split.
The results are shown in Fig. 5 for all datasets. When the
Fig.7. VisualisationsofthelogvectorrepresentationsofBlueGene/Lwith
auxiliary data increases from 1 to 250000 we observe an
T-SNE[32].
increase in all evaluation scores. We observe that increasing
the size of the auxiliary data from 100000 to 250000 the 3) Utilization of the learned log embeddings in related
scores do not change in both cases. This shows that the approaches: In this experiment, we perform the extraction
amount of information present in the auxiliary data is similar of the learned log message vector representations from the
and all cases are already present in 100000 random samples. already trained Logsy. To illustrate the vector representations
We note that having just one auxiliary sample, which might of the logs, in Fig. 7, we show their lower-dimensional
even be generated artificially, sufficiently acts as a regularizer representation of the test split via the T-SNE dimensionality
to the hypersphere loss function, preventing it from learning reduction method [32] on the Blue Gene/L dataset. We show
trivialsolutions.Ofcourse,increasingthevarietyofdata(e.g., that the log vector representations are somehow structured in
including more diverse log datasets) could further improve a way following the definition of our spherical loss function
the performance, due to the increased number of samples (see Section IV-F). We can observe that the normal samples
representing abnormality. areconcentratedaroundthecentreofahypersphere,whichisa
2) Includingexpertlabeling: Oftensystemsareoperatedby circle in two dimensions. Most of the anomalies are dispersed
a human operator which is an expert and has system-specific among the space outside of the sphere. Assigning a threshold
knowledge. Sometimes they could provide or manually label on the anomaly score A(x ), i.e., the distance from the centre
i
samples to improve the performance of the model. Here we of the sphere (circle), we could obtain good performance.
experimentwiththeincrementalinclusionofanomalylabelsof Furthermore, to evaluate the general importance of the
the target dataset to test the model behaviour. We experiment log embeddings, we perform experiments where we replace
onthe20%-80%splitoftheBlueGene/Ldataset.Fig.6shows the original TF-IDF log representations in PCA [21], as the
theresults.Increasingthenumberoflabelledanomalysamples lowest-performing method, with the extracted embeddings
improves performance. For as less as 2% labelled data we from Logsy. We depict the results in the bar plot in Fig. 8.
alreadyhavethebestperformanceof0.8F1-score.Thisshows We observe that this replacement of the log representation
thataddingafewpercentagesofanomaliesaslabelledsamples improves the performance of PCA. We show improvement of
to Logsy, the performance dramatically improves. This only 0.09, 0.11, and 0.01 F1-score for Blue Gene/L, Thunderbird,
strengthens the hypothesis where the log anomaly detection and Spirit respectively. This demonstrates that log represen-
[6] N. Sultana, N. Chilamkurti, W. Peng, and R. Alhadad, “Survey on
1.0 0.99 PCA sdn based network intrusion detection system using machine learning
0.88 EmbeddingPCA approaches,” Peer-to-Peer Networking and Applications, pp. 1–9, 01
0.8 2018.
[7] J.Zhu,S.He,J.Liu,P.He,Q.Xie,Z.Zheng,andM.R.Lyu,“Tools
andbenchmarksforautomatedlogparsing,”in2019IEEE/ACM41stIn-
0.6
ternationalConferenceonSoftwareEngineering:SoftwareEngineering
inPractice(ICSE-SEIP). IEEE,2019,pp.121–130.
0.4
[8] M.Du,F.Li,G.Zheng,andV.Srikumar,“Deeplog:Anomalydetection
0.28
anddiagnosisfromsystemlogsthroughdeeplearning,”inProceedings
0.19
0.2 of the 2017 ACM SIGSAC Conference on Computer and Communica-
0.04 0.05 tionsSecurity. ACM,2017,pp.1285–1298.
0.0 [9] X.Zhang,Y.Xu,Q.Lin,B.Qiao,H.Zhang,Y.Dang,C.Xie,X.Yang,
Blue Gene/L Thunderbird Spirit Q.Cheng,Z.Lietal.,“Robustlog-basedanomalydetectiononunstable
log data,” in Proceedings of the 2019 27th ACM Joint Meeting on
Fig. 8. Comparison in F1 score between the standard PCA [21] and PCA European Software Engineering Conference and Symposium on the
usingtheembeddingsextractedfromourmethod(80%-20%split). FoundationsofSoftwareEngineering,2019,pp.807–817.
[10] M. M. Moya, M. W. Koch, and L. D. Hostetler, “One-class classifier
networksfortargetrecognitionapplications,”NASASTI/ReconTechnical
tation learning has an impact, not only in Logsy, but also in ReportN,vol.93,1993.
[11] W.Meng,Y.Liu,Y.Zhu,S.Zhang,D.Pei,Y.Liu,Y.Chen,R.Zhang,
previous approaches that could be adapted to use the new log
S.Tao,P.Sunetal.,“Loganomaly:Unsuperviseddetectionofsequential
embeddings.Therelativeimprovementofthescoresinaverage andquantitativeanomaliesinunstructuredlogs.”
is 28.2% in the F1-score. [12] S.Nedelkoski,J.Bogatinovski,A.Acker,J.Cardoso,andO.Kao,“Self-
supervisedlogparsing,”2020.
VI. CONCLUSION [13] P.He,J.Zhu,Z.Zheng,andM.R.Lyu,“Drain:Anonlinelogparsing
approachwithfixeddepthtree,”in2017IEEEInternationalConference
Log anomaly detection is important to enhance the security onWebServices(ICWS). IEEE,2017,pp.33–40.
and reliability of computer systems. Existing approaches lack [14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composi-
generalization on new, unseen log samples, which comes
tionality,”inAdvancesinneuralinformationprocessingsystems,2013,
from the evolution of logging statements as a consequence of pp.3111–3119.
system updates and the processing noise. To overcome this [15] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-training
of deep bidirectional transformers for language understanding,” arXiv
problem, we proposed a new anomaly detection approach,
preprintarXiv:1810.04805,2018.
called Logsy. It is based on a self-attention encoder network [16] M. Du, Z. Chen, C. Liu, R. Oak, and D. Song, “Lifelong anomaly
with a hyperspherical classification objective. We formulated detectionthroughunlearning,”inProceedingsofthe2019ACMSIGSAC
Conference on Computer and Communications Security, 2019, pp.
theloganomalydetectionprobleminamannertodiscriminate
1283–1297.
between normal training data from the system of interest and [17] L. Ruff, R. A. Vandermeulen, N. Go¨rnitz, A. Binder, E. Mu¨ller, K.-R.
samples from auxiliary easy-access log datasets from other Mu¨ller,andM.Kloft,“Deepsemi-supervisedanomalydetection,”arXiv
preprintarXiv:1906.02694,2019.
systems, which represent an abnormality. We have presented
[18] A. Oliner and J. Stearley, “What supercomputers say: A study of five
experimental evidence that our classification-based method system logs,” in 37th Annual IEEE/IFIP International Conference on
performs well for anomaly detection. The results of our Dependable Systems and Networks (DSN’07). IEEE, 2007, pp. 575–
method outperformed the baselines by a large margin of 0.25 584.
[19] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
F1 score. Furthermore, we demonstrated that the produced
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
log vector representations could be utilized generally in other inneuralinformationprocessingsystems,2017,pp.5998–6008.
methods. We demonstrated that by adopting PCA to use the [20] Y.Liang,Y.Zhang,H.Xiong,andR.Sahoo,“Failurepredictioninibm
bluegene/l event logs,” in Seventh IEEE International Conference on
log vectors from Logsy, where we observed improvement of
DataMining(ICDM2007). IEEE,2007,pp.583–588.
0.07 (28.2%) in the F1 score. [21] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting
We believe that future research on log anomaly detection large-scale system problems by mining console logs,” in Proceedings
oftheACMSIGOPS22ndsymposiumonOperatingsystemsprinciples,
should focus on finding alternative ways to incorporate richer
2009,pp.117–132.
domainbiasemphasisingthediversityofnormalandanomaly [22] J.BreierandJ.Branisˇova´,“Anomalydetectionfromlogfilesusingdata
data. miningtechniques,”inInformationScienceandApplications. Springer,
2015,pp.449–457.
REFERENCES [23] J.-G. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from
consolelogsforsystemproblemdetection.”
[1] F. E. Grubbs, “Procedures for detecting outlying observations in sam- [24] K. Zhang, J. Xu, M. R. Min, G. Jiang, K. Pelechrinis, and H. Zhang,
ples,”Technometrics,vol.11,no.1,pp.1–21,1969. “Automated it system failure prediction: A deep learning approach,”
[2] V.HodgeandJ.Austin,“Asurveyofoutlierdetectionmethodologies,” 2016IEEEInternationalConferenceonBigData(BigData),pp.1291–
Artificialintelligencereview,vol.22,no.2,pp.85–126,2004. 1300,2016.
[3] M.A.Pimentel,D.A.Clifton,L.Clifton,andL.Tarassenko,“Areview [25] R.Vinayakumar,K.P.Soman,andP.Poornachandran,“Longshort-term
ofnoveltydetection,”SignalProcessing,vol.99,pp.215–249,2014. memory based operation log anomaly detection,” 2017 International
[4] S.Zhang,Y.Liu,D.Pei,Y.Chen,X.Qu,S.Tao,andZ.Zang,“Rapid ConferenceonAdvancesinComputing,CommunicationsandInformat-
androbustimpactassessmentofsoftwarechangesinlargeinternet-based ics(ICACCI),pp.236–242,2017.
services,” in Proceedings of the 11th ACM Conference on Emerging [26] C. Bertero, M. Roy, C. Sauvanaud, and G. Tre´dan, “Experience re-
NetworkingExperimentsandTechnologies,2015,pp.1–13. port:Logminingusingnaturallanguageprocessingandapplicationto
[5] V.Chandola,A.Banerjee,andV.Kumar,“Anomalydetection:Asurvey,” anomaly detection,” in 2017 IEEE 28th International Symposium on
ACMcomputingsurveys(CSUR),vol.41,no.3,pp.1–58,2009. SoftwareReliabilityEngineering(ISSRE). IEEE,2017,pp.351–360.
[27] I. Steinwart, D. Hush, and C. Scovel, “A classification framework for
anomalydetection,”JournalofMachineLearningResearch,vol.6,no.
Feb,pp.211–232,2005.
[28] A. B. Tsybakov et al., “On nonparametric estimation of density level
sets,”TheAnnalsofStatistics,vol.25,no.3,pp.948–969,1997.
[29] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A
review and new perspectives,” IEEE transactions on pattern analysis
andmachineintelligence,vol.35,no.8,pp.1798–1828,2013.
[30] E.LoperandS.Bird,“Nltk:thenaturallanguagetoolkit,”arXivpreprint
cs/0205028,2002.
[31] Z.Zheng,L.Yu,W.Tang,Z.Lan,R.Gupta,N.Desai,S.Coghlan,and
D. Buettner, “Co-analysis of ras log and job log on blue gene/p,” in
2011IEEEInternationalParallel&DistributedProcessingSymposium.
IEEE,2011,pp.840–851.
[32] L.v.d.MaatenandG.Hinton,“Visualizingdatausingt-sne,”Journal
ofmachinelearningresearch,vol.9,no.Nov,pp.2579–2605,2008.