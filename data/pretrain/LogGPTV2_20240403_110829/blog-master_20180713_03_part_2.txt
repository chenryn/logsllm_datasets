```      
create trigger tg1 after insert or update on a for each row execute procedure notify1();      
```      
4、数据合并写入测试      
```      
insert into a values       
  (1, '{"price":[10000, "2018-01-01 10:10:11"]}')       
  on conflict (id)       
  do update set       
  att = merge_json(a.att, excluded.att)  -- 合并新属性，保留老属性，需要使用一个UDF来合并      
  where       
  a.att <> merge_json(a.att, excluded.att);   -- 如果相等的概率很低，则可以去掉这个判断, 降低CPU开销  
postgres=# insert into a values    
  (1, '{"price":[1000, "2019-01-01 10:10:12"], "newatt": ["hello", "2018-01-01"]}')    
  on conflict (id)    
  do update set    
  att = merge_json(a.att, excluded.att)  -- 合并新属性，保留老属性，需要使用一个UDF来合并    
  where    
  a.att <> merge_json(a.att, excluded.att); -- 如果相等的概率很低，则可以去掉这个判断, 降低CPU开销   
INSERT 0 1    
postgres=# select * from a;    
 id |                                     att                                         
----+-----------------------------------------------------------------------------    
  1 | {"price": [1000, "2019-01-01 10:10:12"], "newatt": ["hello", "2018-01-01"]}    
(1 row)    
```      
5、异步批量消费结果表的内容(阅后即焚)      
```      
with a as (delete from t_result where ctid= any(array(     
  select ctid from t_result order by id limit 10 for update skip locked  -- 可以并发消费，不会相互堵塞，消费顺与写入顺序一致    
)) returning *)    
select * from a;    
```      
```    
 id | class |                                                 content                                                     
----+-------+---------------------------------------------------------------------------------------------------------    
  1 | a     | CLASS:high price, ID:1, ATT:{"price": [10000, "2018-01-01 10:10:11"]}    
  2 | a     | CLASS:high price, ID:1, ATT:{"price": [1000, "2019-01-01 10:10:12"], "newatt": ["hello", "2018-01-01"]}    
(2 rows)    
原子操作，阅后即焚，再次查询已消费完毕    
postgres=# select * from t_result;    
 id | class | content     
----+-------+---------    
(0 rows)    
```    
## 方案二续 - 使用statement级触发器代替row级触发器  
为什么建议使用statement级触发器代替row级触发器，参考：  
[《PostgreSQL 批量、单步 写入 - row, statement 触发器(中间表)、CTE 几种用法性能对比》](../201807/20180720_01.md)    
触发器函数修改如下  
```  
CREATE OR REPLACE FUNCTION notify1() returns trigger        
AS $function$        
declare        
begin        
  -- 规则1  
  insert into t_result(class,content) select   
    'a',    -- 归类    
    format('CLASS:high price, ID:%s, ATT:%s', id, att)   -- 消息内容     
  from new_table   
  where jsonb_array_element(att->'price', 0)::text::float8 > 100;    -- 规则1， 价格大于100，写入结果表    
  -- 其他规则  
  -- insert into t_result(class,content) select   
  -- ......  
  --   from new_table   
  -- where ...  -- 规则n  
  return null;        
end;        
$function$ language plpgsql strict;      
```  
触发器修改如下  
```  
create trigger tg1 after insert on a REFERENCING NEW TABLE AS new_table for each STATEMENT execute procedure notify1();      
create trigger tg2 after update on a REFERENCING NEW TABLE AS new_table for each STATEMENT execute procedure notify1();      
```  
```  
postgres=# \d a  
                 Table "public.a"  
 Column |  Type   | Collation | Nullable | Default   
--------+---------+-----------+----------+---------  
 id     | integer |           | not null |   
 att    | jsonb   |           |          |   
Indexes:  
    "pk" PRIMARY KEY, btree (id)  
Triggers:  
    tg1 AFTER INSERT ON a REFERENCING NEW TABLE AS new_table FOR EACH STATEMENT EXECUTE PROCEDURE notify1()  
    tg2 AFTER UPDATE ON a REFERENCING NEW TABLE AS new_table FOR EACH STATEMENT EXECUTE PROCEDURE notify1()  
```  
## 小结    
使用异步消息，UDF，规则或触发器，非常轻量化的解决了实时计算的问题。      
但是，异步消息是可能丢消息的，例如监听连接中断后，重连时，需要重新发起监听，并且中断连接时未消费的消息，不会再被消费，所以相当于丢消息了。      
改进方法:    
1、如果要保证不丢消息，可以将notify改成INSERT，把结果写入预先定义好的某个结果表，使用逻辑DECODE的方式，解析这个结果表相关的logical decode信息，从而获取变化量，参考如下。       
[《PostgreSQL pg_recvlogical 与 test_decoding 自定义，支持source table filter, 对接kafka,es等》](../201806/20180601_01.md)        
2、使用阅后即焚的方法，类似本方案2.      
[《阿里云RDS PostgreSQL varbitx实践 - 流式标签 (阅后即焚流式批量计算) - 万亿级，任意标签圈人，毫秒响应》](../201712/20171212_01.md)    
[《HTAP数据库 PostgreSQL 场景与性能测试之 32 - (OLTP) 高吞吐数据进出(堆存、行扫、无需索引) - 阅后即焚(JSON + 函数流式计算)》](../201711/20171107_33.md)    
[《HTAP数据库 PostgreSQL 场景与性能测试之 31 - (OLTP) 高吞吐数据进出(堆存、行扫、无需索引) - 阅后即焚(读写大吞吐并测)》](../201711/20171107_32.md)   
[《HTAP数据库 PostgreSQL 场景与性能测试之 27 - (OLTP) 物联网 - FEED日志, 流式处理 与 阅后即焚 (CTE)》](../201711/20171107_28.md)    
[《在PostgreSQL中实现update | delete limit - CTID扫描实践  (高效阅后即焚)》](../201608/20160827_01.md)    
## 参考      
https://www.postgresql.org/docs/11/static/functions-json.html      
https://www.postgresql.org/docs/11/static/datatype-json.html      
https://jdbc.postgresql.org/documentation/head/listennotify.html      
https://www.postgresql.org/docs/11/static/sql-notify.html      
https://www.postgresql.org/docs/11/static/sql-listen.html      
https://www.postgresql.org/docs/11/static/sql-unlisten.html      
https://www.postgresql.org/docs/11/static/libpq-notify.html      
https://www.postgresql.org/docs/11/static/sql-notify.html#id-1.9.3.157.7.5      
https://www.postgresql.org/docs/11/static/functions-info.html      
https://www.postgresql.org/docs/11/static/plpgsql-trigger.html       
https://github.com/impossibl/pgjdbc-ng       
https://www.openmakesoftware.com/postgresql-listen-notify-events-example/      
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")