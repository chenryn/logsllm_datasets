nication between VMs. It is a virtual bus device that sets up channels between the guest and the host. 
These channels provide the capability to share data between partitions and set up paravirtualized (also 
known as synthetic) devices. 
The root partition hosts Virtualization Service Providers (VSPs) that communicate over VMBus 
to handle device requests from child partitions. On the other end, child partitions (or guests) use 
Virtualization Service Consumers (VSCs) to redirect device requests to the VSP over VMBus. Child parti-
tions require VMBus and VSC drivers to use the paravirtualized device stacks (more details on virtual 
hardware support are provided later in this chapter in the ”Virtual hardware support” section). VMBus 
channels allow VSCs and VSPs to transfer data primarily through two ring buffers: upstream and down-
stream. These ring buffers are mapped into both partitions thanks to the hypervisor, which, as dis-
cussed in the previous section, also provides interpartition communication services through the SynIC. 
One of the first virtual devices (VDEV) that the Worker process starts while powering up a VM is the 
VMBus VDEV (implemented in Vmbusvdev.dll). Its power-on routine connects the VM Worker process 
to the VMBus root driver (Vmbusr.sys) by sending VMBUS_VDEV_SETUP IOCTL to the VMBus root 
device (named \Device\RootVmBus). The VMBus root driver orchestrates the parent endpoint of the 
bidirectional communication to the child VM. Its initial setup routine, which is invoked at the time the 
target VM isn’t still powered on, has the important role to create an XPartition data structure, which is 
used to represent the VMBus instance of the child VM and to connect the needed SynIC synthetic inter-
rupt sources (also known as SINT, see the “Synthetic Interrupt Controller” section earlier in this chapter 
for more details). In the root partition, VMBus uses two synthetic interrupt sources: one for the initial 
message handshaking (which happens before the channel is created) and another one for the synthetic 
events signaled by the ring buffers. Child partitions use only one SINT, though. The setup routine al-
locates the main message port in the child VM and the corresponding connection in the root, and, for 
each virtual processor belonging to the VM, allocates an event port and its connection (used for receiv-
ing synthetic events from the child VM).
The two synthetic interrupt sources are mapped using two ISR routines, named KiVmbusInterrupt0 
and KiVmbusInterrupt1. Thanks to these two routines, the root partition is ready to receive synthetic 
interrupts and messages from the child VM. When a message (or event) is received, the ISR queues a 
deferred procedure call (DPC), which checks whether the message is valid; if so, it queues a work item, 
which will be processed later by the system running at passive IRQL level (which has further implica-
tions on the message queue).
Once VMBus in the root partition is ready, each VSP driver in the root can use the services exposed 
by the VMBus kernel mode client library  to allocate and offer a VMBus channel to the child VM. The 
VMBus kernel mode client library (abbreviated as KMCL) represents a VMBus channel through an 
opaque KMODE_CLIENT_CONTEXT data structure, which is allocated and initialized at channel creation 
time (when a VSP calls the VmbChannelAllocate API). The root VSP then normally offers the channel 
to the child VM by calling the VmbChannelEnabled API (this function in the child establishes the actual 
connection to the root by opening the channel). KMCL is implemented in two drivers: one running in 
the root partition (Vmbkmclr.sys) and one loaded in child partitions (Vmbkmcl.sys).
324 
CHAPTER 9 Virtualization technologies
Offering a channel in the root is a relatively complex operation that involves the following steps:
1.
The KMCL driver communicates with the VMBus root driver through the file object initialized in
the VDEV power-up routine. The VMBus driver obtains the XPartition data structure represent-
ing the child partition and starts the channel offering process.
2.
Lower-level services provided by the VMBus driver allocate and initialize a LOCAL_OFFER data
structure representing a single “channel offer” and preallocate some SynIC predefined messages.
VMBus then creates the synthetic event port in the root, from which the child can connect to
signal events after writing data to the ring buffer. The LOCAL_OFFER data structure represent-
ing the offered channel is added to an internal server channels list.
3.
After VMBus has created the channel, it tries to send the OfferChannel message to the child
with the goal to inform it of the new channel. However, at this stage, VMBus fails because the
other end (the child VM) is not ready yet and has not started the initial message handshake.
After all the VSPs have completed the channel offering, and all the VDEV have been powered up 
(see the previous section for details), the VM Worker process starts the VM. For channels to be com-
pletely initialized, and their relative connections to be started, the guest partition should load and start 
the VMBus child driver (Vmbus.sys).
Initial VMBus message handshaking
In Windows, the VMBus child driver is a WDF bus driver enumerated and started by the Pnp manager 
and located in the ACPI root enumerator. (Another version of the VMBus child driver is also available 
for Linux. VMBus for Linux is not covered in this book, though.) When the NT kernel starts in the child 
VM, the VMBus driver begins its execution by initializing its own internal state (which means allocat-
ing the needed data structure and work items) and by creating the \Device\VmBus root functional 
device object (FDO). The Pnp manager then calls the VMBus’s resource assignment handler routine. 
The latter configures the correct SINT source (by emitting a HvSetVpRegisters hypercall on one of the 
HvRegisterSint registers, with the help of the WinHv driver) and connects it to the KiVmbusInterrupt2 
ISR. Furthermore, it obtains the SIMP page, used for sending and receiving synthetic messages to and 
from the root partition (see the “Synthetic Interrupt Controller” section earlier in this chapter for more 
details), and creates the XPartition data structure representing the parent (root) partition.
When the request of starting the VMBus’ FDO comes from the Pnp manager, the VMBus driver starts 
the initial message handshaking. At this stage, each message is sent by emitting the HvPostMessage 
hypercall (with the help of the WinHv driver), which allows the hypervisor to inject a synthetic interrupt 
to a target partition (in this case, the target is the partition). The receiver acquires the message by sim-
ply reading from the SIMP page; the receiver signals that the message has been read from the queue 
by setting the new message type to MessageTypeNone. (See the hypervisor TLFS for more details.) The 
reader can think of the initial message handshake, which is represented in Figure 9-25, as a process 
divided in two phases.
CHAPTER 9 Virtualization technologies
325
ROOT PARTITION
CHILD VM
Time
First Phase:
All Channels Enumerated and
Offers Delivered to the
Child VM
Initiate Contact
Version Response
Request Offers
Offer Channel (Multiple Messages)
All Offers Delivered
GPADL Header
GPDAL Body
GPDAL Created
Open Channel
Open Channel Result
Channel Opening
and Ring Buffer Creation
VM
FIGURE 9-25 VMBus initial message handshake.
The first phase is represented by the Initiate Contact message, which is delivered once in the lifetime 
of the VM. This message is sent from the child VM to the root with the goal to negotiate the VMBus 
protocol version supported by both sides. At the time of this writing, there are five main VMBus pro-
tocol versions, with some additional slight variations. The root partition parses the message, asks the 
hypervisor to map the monitor pages allocated by the client (if supported by the protocol), and replies 
by accepting the proposed protocol version. Note that if this is not the case (which happens when the 
Windows version running in the root partition is lower than the one running in the child VM), the child 
VM restarts the process by downgrading the VMBus protocol version until a compatible version is es-
tablished. At this point, the child is ready to send the Request Offers message, which causes the root 
partition to send the list of all the channels already offered by the VSPs. This allows the child partition 
to open the channels later in the handshaking protocol.
Figure 9-25 highlights the different synthetic messages delivered through the hypervisor for setting 
up the VMBus channel or channels. The root partition walks the list of the offered channels located in 
the Server Channels list (LOCAL_OFFER data structure, as discussed previously), and, for each of them, 
sends an Offer Channel message to the child VM. The message is the same as the one sent at the final 
stage of the channel offering protocol, which we discussed previously in the “VMBus” section. So, while 
326 
CHAPTER 9 Virtualization technologies
the first phase of the initial message handshake happens only once per lifetime of the VM, the second 
phase can start any time when a channel is offered. The Offer Channel message includes important 
data used to uniquely identify the channel, like the channel type and instance GUIDs. For VDEV chan-
nels, these two GUIDs are used by the Pnp Manager to properly identify the associated virtual device.
The child responds to the message by allocating the client LOCAL_OFFER data structure represent-
ing the channel and the relative XInterrupt object, and by determining whether the channel requires 
a physical device object (PDO) to be created, which is usually always true for VDEVs’ channels. In this 
case, the VMBus driver creates an instance PDO representing the new channel. The created device is 
protected through a security descriptor that renders it accessible only from system and administra-
tive accounts. The VMBus standard device interface, which is attached to the new PDO, maintains 
the association between the new VMBus channel (through the LOCAL_OFFER data structure) and 
the device object. After the PDO is created, the Pnp Manager is able to identify and load the correct 
VSC driver through the VDEV type and instance GUIDs included in the Offer Channel message. These 
interfaces become part of the new PDO and are visible through the Device Manager. See the following 
experiment for details. When the VSC driver is then loaded, it usually calls the VmbEnableChannel API 
(exposed by KMCL, as discussed previously) to “open” the channel and create the final ring buffer.
EXPERIMENT: Listing virtual devices (VDEVs) exposed through VMBus
Each VMBus channel is identified through a type and instance GUID. For channels belonging to 
VDEVs, the type and instance GUID also identifies the exposed device. When the VMBus child 
driver creates the instance PDOs, it includes the type and instance GUID of the channel in mul-
tiple devices’ properties, like the instance path, hardware ID, and compatible ID. This experiment 
shows how to enumerate all the VDEVs built on the top of VMBus.
For this experiment, you should build and start a Windows 10 virtual machine through the 
Hyper-V Manager. When the virtual machine is started and runs, open the Device Manager (by 
typing its name in the Cortana search box, for example). In the Device Manager applet, click the 
View menu, and select Device by Connection. The VMBus bus driver is enumerated and started 
through the ACPI enumerator, so you should expand the ACPI x64-based PC root node and then 
the ACPI Module Device located in the Microsoft ACPI-Compliant System child node, as shown in 
the following figure:
EXPERIMENT: Listing virtual devices (VDEVs) exposed through VMBus
Each VMBus channel is identified through a type and instance GUID. For channels belonging to 
VDEVs, the type and instance GUID also identifies the exposed device. When the VMBus child 
driver creates the instance PDOs, it includes the type and instance GUID of the channel in mul-
tiple devices’ properties, like the instance path, hardware ID, and compatible ID. This experiment 
shows how to enumerate all the VDEVs built on the top of VMBus.
For this experiment, you should build and start a Windows 10 virtual machine through the 
Hyper-V Manager. When the virtual machine is started and runs, open the Device Manager (by 
typing its name in the Cortana search box, for example). In the Device Manager applet, click the 
View menu, and select Device by Connection. The VMBus bus driver is enumerated and started 
through the ACPI enumerator, so you should expand the ACPI x64-based PC root node and then 
the ACPI Module Device located in the Microsoft ACPI-Compliant System child node, as shown in 
the following figure:
CHAPTER 9 Virtualization technologies
327
By opening the ACPI Module Device, you should find another node, called Microsoft Hyper-V 
Virtual Machine Bus, which represents the root VMBus PDO. Under that node, the Device 
Manager shows all the instance devices created by the VMBus FDO after their relative VMBus 
channels have been offered from the root partition. 
Now right-click one of the Hyper-V devices, such as the Microsoft Hyper-V Video device, and 
select Properties. For showing the type and instance GUIDs of the VMBus channel backing the 
virtual device, open the Details tab of the Properties window. Three device properties include 
the channel’s type and instance GUID (exposed in different formats): Device Instance path, 
Hardware ID, and Compatible ID. Although the compatible ID contains only the VMBus channel 
type GUID ({da0a7802-e377-4aac-8e77-0558eb1073f8} in the figure), the hardware ID and device 
instance path contain both the type and instance GUIDs.
Opening a VMBus channel and creating the ring buffer
For correctly starting the interpartition communication and creating the ring buffer, a channel 
must be opened. Usually VSCs, after having allocated the client side of the channel (still through 
VmbChannel Allocate), call the VmbChannelEnable API exported from the KMCL driver. As intro-
duced in the previous section, this API in the child partitions opens a VMBus channel, which has 
already been offered by the root. The KMCL driver communicates with the VMBus driver, obtains 
the channel parameters (like the channel’s type, instance GUID, and used MMIO space), and creates 
By opening the ACPI Module Device, you should find another node, called Microsoft Hyper-V 
Virtual Machine Bus, which represents the root VMBus PDO. Under that node, the Device 
Manager shows all the instance devices created by the VMBus FDO after their relative VMBus 
channels have been offered from the root partition. 
Now right-click one of the Hyper-V devices, such as the Microsoft Hyper-V Video device, and 
select Properties. For showing the type and instance GUIDs of the VMBus channel backing the 
virtual device, open the Details tab of the Properties window. Three device properties include 
the channel’s type and instance GUID (exposed in different formats): Device Instance path, 
Hardware ID, and Compatible ID. Although the compatible ID contains only the VMBus channel 
type GUID ({da0a7802-e377-4aac-8e77-0558eb1073f8} in the figure), the hardware ID and device 
instance path contain both the type and instance GUIDs.
328 
CHAPTER 9 Virtualization technologies
a work item for the received packets. It then allocates the ring buffer, which is shown in Figure 9-26. 
The size of the ring buffer is usually specified by the VSC through a call to the KMCL exported 
VmbClientChannelInitSetRingBufferPageCount API.
0xFFFFCA80'3D8C0000
0x0
0x10000
0x20000
0x0
0x10000
0x0
0x1000
0x11000
0x12000
0x22000
0xFFFFCA80'3D8D0000
0xFFFFCA80'3D8E0000
0xFFFFCA80'594A0000
0xFFFFCA80'594B0000
0xFFFFCA80'594C0000
Ring Buffer Virtual Layout
Mapped Ring Buffer
Writing Example
Write Request in the Ring Buffer
Physical Layout
Outgoing
Buffer
Physical
Pages
Outgoing
Buffer
Physical
Pages
Incoming
Buffer
Physical
Pages
Incoming
Buffer
Physical
Pages
Incoming Buffer
Control Page
Outgoing Buffer
Control Page
Outgoing Buffer
Incoming Buffer
FIGURE 9-26 An example of a 16-page ring buffer allocated in the child partition.
The ring buffer is allocated from the child VM’s non-paged pool and is mapped through a memory 
descriptor list (MDL) using a technique called double mapping. (MDLs are described in Chapter 5 of 
Part 1.) In this technique, the allocated MDL describes a double number of the incoming (or outgoing) 
buffer’s physical pages. The PFN array of the MDL is filled by including the physical pages of the buffer 
twice: one time in the first half of the array and one time in the second half. This creates a “ring buffer.”
For example, in Figure 9-26, the incoming and outgoing buffers are 16 pages (0x10) large. The 
outgoing buffer is mapped at address 0xFFFFCA803D8C0000. If the sender writes a 1-KB VMBus packet 
to a position close to the end of the buffer, let’s say at offset 0x9FF00, the write succeeds (no access 
violation exception is raised), but the data will be written partially in the end of the buffer and partially 
in the beginning. In Figure 9-26, only 256 (0x100) bytes are written at the end of the buffer, whereas the 
remaining 768 (0x300) bytes are written in the start.
Both the incoming and outgoing buffers are surrounded by a control page. The page is shared be-
tween the two endpoints and composes the VM ring control block. This data structure is used to keep 
track of the position of the last packet written in the ring buffer. It furthermore contains some bits to 
control whether to send an interrupt when a packet needs to be delivered.
After the ring buffer has been created, the KMCL driver sends an IOCTL to VMBus, requesting the 
creation of a GPA descriptor list (GPADL). A GPADL is a data structure very similar to an MDL and is used 
for describing a chunk of physical memory. Differently from an MDL, the GPADL contains an array of 
guest physical addresses (GPAs, which are always expressed as 64-bit numbers, differently from the 
PFNs included in a MDL). The VMBus driver sends different messages to the root partition for transfer-
ring the entire GPADL describing both the incoming and outcoming ring buffers. (The maximum size 
of a synthetic message is 240 bytes, as discussed earlier.) The root partition reconstructs the entire 
CHAPTER 9 Virtualization technologies
329
GPADL and stores it in an internal list. The GPADL is mapped in the root when the child VM sends the 
final Open Channel message. The root VMBus driver parses the received GPADL and maps it in its own 
physical address space by using services provided by the VID driver (which maintains the list of memory 
block ranges that comprise the VM physical address space).
At this stage the channel is ready: the child and the root partition can communicate by simply reading 
or writing data to the ring buffer. When a sender finishes writing its data, it calls the VmbChannelSend 
SynchronousRequest API exposed by the KMCL driver. The API invokes VMBus services to signal an event 
in the monitor page of the Xinterrupt object associated with the channel (old versions of the VMBus pro-
tocol used an interrupt page, which contained a bit corresponding to each channel), Alternatively, VMBus 
can signal an event directly in the channel’s event port, which depends only on the required latency.
Other than VSCs, other components use VMBus to implement higher-level interfaces. Good examples 
are provided by the VMBus pipes, which are implemented in two kernel mode libraries (Vmbuspipe.dll 
and Vmbuspiper.dll) and rely on services exposed by the VMBus driver (through IOCTLs). Hyper-V Sockets 
(also known as HvSockets) allow high-speed interpartition communication using standard network 
interfaces (sockets). A client connects an AF_HYPERV socket type to a target VM by specifying the target 
VM’s GUID and a GUID of the Hyper-V socket’s service registration (to use HvSockets, both endpoints 
must be registered in the HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ Virtualization\
GuestCommunicationServices registry key) instead of the target IP address and port. Hyper-V Sockets 
are implemented in multiple drivers: HvSocket.sys is the transport driver, which exposes low-level services 
used by the socket infrastructure; HvSocketControl.sys is the provider control driver used to load the 
HvSocket provider in case the VMBus interface is not present in the system; HvSocket.dll is a library that 
exposes supplementary socket interfaces (tied to Hyper-V sockets) callable from user mode applications. 
Describing the internal infrastructure of both Hyper-V Sockets and VMBus pipes is outside the scope of 
this book, but both are documented in Microsoft Docs.
Virtual hardware support
For properly run virtual machines, the virtualization stack needs to support virtualized devices. 
Hyper-V supports different kinds of virtual devices, which are implemented in multiple components 
of the virtualization stack. I/O to and from virtual devices is orchestrated mainly in the root OS. I/O 
includes storage, networking, keyboard, mouse, serial ports and GPU (graphics processing unit). The 
virtualization stack exposes three kinds of devices to the guest VMs: