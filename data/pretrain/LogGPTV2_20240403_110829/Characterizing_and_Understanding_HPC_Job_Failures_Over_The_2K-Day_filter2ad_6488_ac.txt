In what follows, we divide the job failure counts based
on different exit statuses in terms of the task execution log.
Because of space limits, we use abbreviations (or exit codes) to
represent the exit statuses listed in Table IV in the same orders.
For instance, NM, TO, and BG refer to “Normal,” “Timeout,”
and “Bug,” respectively. We present the contingency table
with user/project and exit codes in Table V and Table VI,
respectively. Since there are many users and exit codes, we
select
the 10 most frequent exit codes and top 10 users
with the highest number of jobs for our analysis, without
loss of generality. From these two tables, we observe that
the majority of jobs terminate normally for each user or
CONTINGENCY TABLE WITH PROJECT NAME AND EXIT CODE
PPPPPP
proj
exit
We also characterize the exit codes based on users and
projects, as presented in Fig. 3. From the ﬁgure, we derive
the following takeaway. (Takeaway 4): The majority of job
failures are attributed to four categories regarding user
behaviors: ‘timeout,’ ‘bug,’ ‘kill,’ and ‘IO’ (or misop-
erations). Various users (e.g., u1, u3, u6, and u10) have
different most-frequent exit codes. Such a feature can help
system administrators identify job failures quickly based on
the exit code category and users or projects, thereby improv-
ing the daily diagnosis efﬁciency.
(a) Exit Code Based on Users
(b) Exit Code based on Projects
Fig. 3. Exit Code Distribution Based on Users/Projects
B. Features Based on Job Execution Structure
Based on our characterization of all jobs, we formulated
the following takeaway. (Takeaway 5): Different jobs may
have largely different job execution structures, such as
number of nodes and number of tasks, as shown in Fig.
4. The majority of the jobs have relatively small or medium
numbers of nodes or tasks, while a few jobs each may have
numerous nodes or tasks. This situation motivates us to explore
the correlations between job structure and failure types.
6
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000CDFNumber of JobsJob Distr. based on ProjectsJob Distr. based on Users 0 0.2 0.4 0.6 0.8 1 0 200 400 600 800 1000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2e+08 4e+08 6e+08 8e+08 1e+09 1.2e+09 1.4e+09 1.6e+09CDFCorehoursCore-hours based on ProjectsCore-hours based on Users 0.4 0.5 0.6 0.7 0.8 0.9 0 2e+07 4e+07 6e+07 8e+07 1e+08 0 0.2 0.4 0.6 0.8 1u1u2u3u4u5u6u7u8u9u10Job DistributionUSERSTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPE 0 0.2 0.4 0.6 0.8 1p1p2p3p4p5p6p7p8p9p10Job DistributionPROJECTSTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPEjob-failures. (Takeaway 7): Normal jobs contributed major
core-hours (58% of the total), while the timeout job-
failures also take a considerable portion of core-hours
(32.86% of the total), implying the high signiﬁcance of
checkpointing/restart mechanism across job boundaries
for the execution continuation of HPC applications. In
fact, many of existing applications (such as HACC [33]) on
MIRA have their own checkpointing mechanism to avoid the
expensive re-computation upon failures or timeout issue.
SUM OF CORE-HOURS BASED ON EXIT CODE
TABLE VIII
RS
4.27E8
UK
2.85E7
SI
ST
SF
7.4E6
NM
TO
KL
IO
BG
1.4E9
4.75E5
1.08E8
9.65E8
2.76E6
1.07E10
1.87E10
We further characterize the breakdown of core-hours and
other statistics based on log-scale execution time intervals in
Table IX. We have the following takeaway. (Takeaway 8)
Majority of the core-hours consumed by timeout jobs are
contributed by the jobs with relatively long execution times
in the range of [5h20m, 42h40m]. In order to mitigate the
possibly lost core-hours, the long-execution jobs are highly
recommended to output the simulation data during the execu-
tion or to be protected by fault-tolerance techniques such as
checkpointing/restart mechanism.
STATISTICS OF TIMEOUT JOBS IN LOG-SCALE EXECUTION INTERVALS
Exe. Time Intervals
Sum of Core-hours Mean # cores Mean Exe. Time
Job Counts
[0,10m]
[10m,20m)
[20m,40m)
[40m,1h20m)
[1h20m,2h40m)
[2h40m,5h20m)
[5h20m,10h40m)
[10h40m,21h20m)
[21h20m,42h40m)
[42h40m,...]
1104
2,664
4,691
10,992
6,762
10,302
12,391
5,290
922
2
TABLE IX
3824449
2.11632E7
1.09938E8
2.997808E8
4.658492E8
9.362381E8
2.4776273E9
2.570157E9
3.7690924E9
8304012
32278.3
36969
49063
28162
36272
24073.4
31256.7
39687.1
170326.1
98304.0
0.1 h
0.21 h
0.47 h
0.99 h
1.9 h
3.86 h
6.25 h
12.1 h
24.1 h
71.25 h
2) Features Based on Job Tasks: We identify three types
of jobs in terms of their tasks: single-task jobs, consecutive-
task jobs, and multilocation-task jobs, which are completely
controlled by users (speciﬁcally, by tuning the submission
mode such as “script” or “cn”). Exploring the features based
on job tasks may shed light on how to mitigate the job failures
by tuning the job execution types. (Takeaway 9): The system-
reliability-related job failures (category “RAS”) happen
mainly to jobs with small task count (either consecutive or
multilocation tasks). In fact, the jobs with few tasks may also
have many node hours, explaining their more frequent failures.
This takeaway suggests users not to submit jobs each with very
few tasks, in order to mitigate unexpected job failures.
3) Features Based on Job Execution Scale: We characterize
the contingency table about resource location and exit code
in Table XI. The resource location is represented as xxxxx-
yyyyy-NNNN, where xxxxx-yyyyy refers to the resource
blocks allocated to submitted jobs and NNNN indicates the
number of nodes. For instance, 44000-77FF1-8192 corre-
sponds to the block with 16 midplanes (R18-R1F). (Takeaway
10): The table shows that the exit codes have a relatively
strong correlation with the number of nodes. For instance,
most of the exit codes, such as “TO,” “BG,” and “RS,” occur
more frequently on the 8192-node blocks than on the 512-
(a) Number of Nodes
(b) Number of Tasks
Fig. 4. Execution Scale of All Jobs
In this study, we focused on four important job structures:
job’s core-hours, job’s tasks, number of nodes, and execution
length. Speciﬁcally, we explored the correlations between each
of these structures and different exit statuses.
1) Features based on Job’s Core-hours: Table VII shows
the contingency table between job’s core-hours and exit codes
with log-scaling classiﬁcation. The table presents the top-10
intervals of the core-hours with the highest numbers of jobs.
(Takeaway 6): We can observe a very strong correlation
between the core-hours and exit codes. Most of the normal
jobs consumed either a small or medium amount of core-
hours (in the range of [0,1k) and [8k,16k), respectively),
while the job-failure exit codes exhibit diverse job distribu-
tions. For instance, the “Bug” jobs are mainly of small core-
hours, most of “timeout” jobs have moderate-sized core-hours
(e.g., [32k,64k)), and the majority of “RS” jobs have fairly
large core-hours (such as [256k,512k)]). This diversity is due
to the diverse purposes or features of various job executions:
for instance, the jobs with small core-hours are likely used to
debug codes, and those jobs with large core-hours may have a
higher chance of being affected by system’s fatal RAS events.
Based on Fig. 5, we conclude that jobs with small-core-hours
tend to have bugs or misoperations, while jobs with large core-
hours generally have bug-free implementation but are likely to
be affected by timeout or system reliability issues.
XXXXXXXX
corehours
exit
CONTINGENCY TABLE WITH CORE-HOURS AND EXIT CODE
TABLE VII
[0,1k)
[1k,2k)
[2k,4k)
[4k,8k)
[8k,16k)
[16k,32k)
[32k,64k)
[64k,128k)
[128k,256k)
[256k,512k)
NM
67580
17631
18782
34040
52228
26498
23087
15732
9493
13209
TO
652
1356
1511
2863
7681
8859
12853
7508
4911
6926
BG
10466
2900
2078
3412
2383
1608
1586
1135
1328
833
KL
2420
1688
1568
1265
935
765
761
469
281
556
IO
2415
283
232
168
118
97
62
60
27
72
RS
20
46
21
48
29
38
46
68
74
251
UK
152
48
47
89
32
44
39
17
6
15
SI
213
15
4
1
2
6
6
0
0
0
ST
164
13
19
8
6
18
3
3
0
1
SF
22
1
1
1
10
3
7
0
3
6
Fig. 5. Exit Code Distribution Based on Core-Hours
In addition, we compute the sum of core-hours based on
different exit codes, as shown in Table VIII,
in order to
characterize the possible core-hour wastes in terms of various
7
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000CDFNumber of Nodes# Nodes Used# Nodes Requested 0 0.1 0.2 0.3 0.4 0.5 0.6 0 100 200 300 400 500 600 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2000 4000 6000 8000 10000CDF# Tasks per JobConsecutive TasksMultilocation Tasks 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 0 0.2 0.4 0.6 0.8 1[0,1k)[1k,2k)[2k,4k)[4k,8k)[8k,16k)[16k,32k)[32k,64k)[64k,128k)[128k,256k)[256k,512k)Exit Code DistributionCore-hoursTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPECONTINGENCY TABLE WITH CONSECT./MULTIL. TASKS AND EXIT CODE
CONTINGENCY TABLE WITH # NODES USED AND EXIT CODE
TABLE X
TABLE XII
XXXXXXXX
# tasks
exit
NM
TO
BG
IO
KL
UK
RS
ST
SI
[1,10]
[11,20]
[21,40]
[41,80]
[81,160]
[161,320]
[321,640]
[641,1280]
[1280,2560]
[2561,...]
[1,10]
[11,20]
[21,40]
[41,80]
[81,160]
[161,320]
[321,640]
[641,1280]
[1281,2560]
[2561,...]
19763
7004
1947
1155
307
179
49
21
81
14873
4905
705
715
375
330
97
39
15
67
14873
1114
184
136
64
60
27
12
9
1
0
Consecutive Tasks
284
1555
112
769
46
904
90
620
17
318
21
181
78
2
3
58
2
29
78
2
Multilocation Tasks
55
871
11
345
270
6
5
272
4
189
5
117
2
86
5
66
0
27
41
2
296
274
123
125
30
53
10
9
0
1
365
62
61
26
15
10
2
2
3
1
156
56
90
37
34
34
4
3
3
1
51
2
2
1
0
0