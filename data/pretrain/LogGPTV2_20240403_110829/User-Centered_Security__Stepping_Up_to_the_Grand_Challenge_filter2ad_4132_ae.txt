is often some variation of “Think about the user” [49] 
or “Use established CHI methods and principles” [44, 
40,  56,  34].  Specific  problem  or  domain  advice  is 
mostly in the area of authentication and passwords.  
to 
3.1. Human Computer Interface Techniques 
and User-Centered Security  
Expert  evaluations  of  both  usability  and  security 
can  enhance  those  attributes.  Both  disciplines  have 
tools and processes to provide input from experts, and 
both  support  a  small  industry  of  consultants  (for 
example,  Nielsen  Norman  Group,  @stake).  Expert 
reviews of the usability of security have mixed success 
[62].  UI  experts  can  help  with  the  usability  of  every 
day  concepts  (such  as  passwords)  and  visual  design. 
They  may  not  be  able  to  give  deep  advice  on  the 
usability of security aspects that are only invoked when 
a  problem  or  error  occurs,  or  which  surface  existing 
security  mechanisms 
their  current  complexity. 
Individuals with expertise in both security and usability 
can provide richer advice, but are far scarcer. Because 
they  don’t  yet  have  processes  or  checklists  these 
evaluations  can  be  very  inconsistent,  and  cannot  be 
done on even a simple scale by others.  
There  are  other  mismatches  with  applying  existing 
to  security.  Many  usability 
techniques 
usability 
in 
techniques center on the user’s goal or task [4, 45]. For 
the vast majority of users impacted by security, it’s an 
attribute  of  their  tools,  not  their  goal.  Many  security 
goals  can  only  be  stated  anti-goals;  someone  guesses 
your password, business critical information is leaked. 
As I have additionally pointed out, the interactions with 
security mostly come from error conditions, not normal 
processing 
important 
exception).  
(authentication 
Security  literature  and  experience  is  rife  with 
examples of security that did not provide the promised 
protection  in  the  face  of  real  users.  Both  security  and 
usability have a tradition of testing mechanisms either 
in  laboratory  setups  or  actual  use.  Security  is  tested 
through  red  teaming  or  ethical  hacking  of  deployed 
systems. Usability testing can be either lab testing with 
a  structured  set  of  tasks,  or  in  situ  testing  through 
contextual analysis or logs [45, 4].  
being 
an 
Examples of user testing of security functionality are 
still  modest in number in the literature [59, 51, 1, 53, 
27, 16], although there are some very early examples of 
applying usability techniques to security messages [23]. 
Some  of  the  existing  studies  are  lab-based  studies, 
emulating  attacks  in  that  context.  Others  are  detailed 
interviews with people about their use of security and 
privacy  technology  (for  example,  passwords,  location 
finders).  A  recent  phishing  attack  study  attempting  to 
show how successful  modest social engineering could 
be  as  an  attack  approach  garnered  some  heated 
complaints,  even  though  it  had  been  cleared  by  the 
university’s board beforehand [33].  
The  body  of  experience  testing  the  usability  of 
security both in the  lab and in context  will define the 
techniques and tools we need and can use. It will also 
generate  a  body  of  best  practice  we  can  begin  to 
systematize 
in  checklists  and  expert  evaluations. 
Taking that best practice and making it visible to users 
and purchasers will apply pressure to raise the level of 
usable security in systems and products.  
3.2. Principles of Usably Secured Systems  
[43] used their experience with security and Multics 
to formulate eight principles of secure systems. One of 
these  was  “psychological  acceptability”,  which  is 
usable  security.  Today  we  have  enough  experience  to 
turn the existing body of knowledge into a small set of 
principles for systems that can be made usably secure.  
Whitten  [52]  lays  out  two  design  techniques  for 
usable security: safe staging and metaphor tailoring. 
Safe staging is “a user interface design that allows the 
user  freedom  to  decide  when  to  progress  to  the  next 
stage,  and  encourages  progression  by  establishing  a 
context in which it is a conceptually attractive path of 
least resistance.” The intuition is that users should not 
be  forced  to  make  security  decisions  when  they’re 
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
trying  to  get  something  else  done  and  don’t  have 
enough  information  to  make  them  yet  anyway.  Many 
active  content  and  trust  dialogs  do  not  provide  safe 
stages.  Some  of  the  more  usable  dialogs  for  setting 
security  policy  information  for  access  control  and 
active  content  allow  the  user  to  use  large  granularity 
defaults, and to proceed to configurable or fine grained 
security settings as needed.  
Metaphor  tailoring  starts  with  a  conceptual  model 
specification  of  the  security  related  functionality, 
enumerates  the  risks  of  usability  failures  in  that 
model,  and  uses  those  risks  to  explicitly  drive  visual 
metaphors.  A  more general restatement of the core of 
the  principle  is  “Incorporate risks  of  usability  failures 
into the security model”. This principle can be applied 
even  to  security  relevant  parts  of  a  system  when  the 
actual context of use is undetermined, such as security 
standards.  Use  case  scenarios  may  be  required  before 
usability failures can be described.  
transparent 
security  and 
Additional principles can be derived from the work 
on presenting and visualizing security, from  metaphor 
tailoring through work on visualization [12, 13, 14] and 
explanation  and  enforcement  [55].  I  will  call  these 
integrated 
security. 
Integrated security aligns security with user actions and 
tasks  so  that  the  most  common  tasks  and  repetitive 
actions  are  secure  by  default.  Inability  to  design, 
implement,  and  deploy 
is  an 
indication that a task or action is not securable. A task 
or action that is not securable needs to be redesigned so 
that  it  is  securable.  Producing  integrated  security  can 
lead 
task’s 
functionality and security aspects, until the integration 
is occurs.  
iterative  process  between 
integrated  security 
to  an 
the 
Transparent  security  provides  information  about 
security state and context in  a form that is  useful and 
understandable to the user, in a non-obtrusive fashion. 
For  example,  in  contexts  such  as  email,  where  social 
engineering  attacks  such  as  scam-spam  and  phishing 
are a concern, proactive display of the reliability of the 
sender  information  (digital  signature,  all  mail  headers 
indicating a full transmission path behind the enterprise 
firewalls) is warranted, as long as it is well designed to 
fit within the context of all the other per mail message 
information  displayed 
the  user.  Conversely, 
immediate  indicators  that  the  current  web  page  was 
delivered  encrypted  via  SSL  are  likely  to  be  of 
secondary  importance.  The  security  the  user  needs  to 
know  about  for  their  next  action  is  whether  any 
submission  of  data  from  that  web  page  will  be 
encrypted.  
Transparent  security  might  be  easiest  to  achieve  in 
the subcategory of protection mechanisms that provide 
privacy.  The  desired  privacy  protections  on  personal 
information  are  by  definition  determined  by  the  user 
themselves.  The  user  can  understand  the  risk  of 
to 
exposure and misuse of the information, since the risk 
is directly to and about them.  
obvious 
them  more 
Transparent  security  can  do  more  than  explain 
security.  It  can  highlight  anomalies  that  indicate 
problems  making 
or 
understandable  to  non-technical  users.  It  can  reassure 
the  user  and  promote  trust  in  the  vigilance  of  the 
software.  Going  one  step  further,  Bill  Cheswick 
suggests  software  that  lets  out  a  groan  whenever  a 
preventable  problem  is  detected  can  train  the  user  to 
use security more effectively [8]. Persuasion literature 
[37]  teaches  that  if  a  task  is  important  or  particularly 
engaging, users will apply intense analytical processing. 
Otherwise  they  will  apply  simple  heuristics.  Even 
though security researchers and developers all believe 
security to be both important and engaging, since it is 
almost never the user’s primary task, users do not. An 
open  research  question  is  how  effectively  play  and 
humor  can  be  used  to  bring  the  user’s  level  of 
engagement  in  line  with  the  importance  of  any 
additional actions required to ensure security.  
As an aside, an unexplored area of research is how 
visible security can be used to dissuade attackers. By 
analogy, “Neighborhood Watch” signs and very visible 
(sometimes  fake)  cameras  are  used  as  deterrents  to 
vandalism and robbery of physical goods.   
knowledgeable 
developers).  These 
trustworthy  authority. 
A principle implicit in many approaches is reliance 
on 
Implementing  usable 
security in the first place relies on architects, designers, 
and developers to provide it. Giving administrators the 
ability  to  configure  security  policy  for  users  and 
resources  in  their  domain  puts  the  responsibility  on 
them to make the right choices (or accept the defaults 
from 
and 
responsible  people  can  be  relied  on.  Approaches  that 
integrate  visualization  of  community  information  [13] 
or  security  decisions  based  on  evaluation  of  the 
activities of other users [19] presume that information 
about  how  a  community  or  group  or  organization  is 
making  security  decisions  can  reliably  inform  similar 
personal  decisions.  As  the  authority  being  relied  on 
becomes  more  diffused,  trust  and  security  decisions 
may be susceptible to “flash crowds” [35]. If instead it 
provides  a  damping  effect,  community  information 
may  be  the  best  weapon  we  develop  to  resist  social 
engineering,  since  it  uses  one  social  process  to 
counteract the abuse of others.  
4. Conclusion  
As  the  CRA  conference  found,  the  challenge  of 
usable  security  is  grand.  We  need  work  at  the  social, 
technical and production levels to  meet the challenge. 
We  have  some  HCI  techniques  and  some  usable 
security principles to take us  to the next level. Expert 
evaluation  and  user  testing  are  producing  effective 
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
usable  security  today.  Principles  such  as  safe  staging, 
enumerating usability failure risks, integrated security, 
transparent  security  and  reliance  on 
trustworthy 
authorities can also form the basis of improved systems. 
There are many open research problems. How do we 
build  security  mechanisms  that  are  usable,  with  no 
additional education or explanation? How do we set the 
tone  for  explanations  of  security  breaches  so  that 
blaming  a  user  is  not  an  option?  How  do  we  extend 
HCI  techniques  to  security  error  cases?  How  can  we 
encourage marketing pull of usable security? What low 
impact processes can be used soon to raise the bar on 
the  lower  end  of  usable  security?  How  do  we  model 
users  as  part  of  the  system  security?  How  can 
constraints  simplify  the  decisions  thrust  upon  users? 
How  do  we  get 
from  development, 
deployment and use into the research process rapidly? 
How  do  we  design  reusable  security  components  and 
specifications that participate in usable security?  
feedback 
It’s an area where many types of people are needed 
for  us  to  make  progress.  We  need  researchers  and 
developers attracted to issues that are both system wide 
and  pragmatic,  practitioners  who  can  synthesize 
multiple disciplines, and innovative thinkers and doers 
of all sorts.  
Acknowledgements 
This essay profited from review by Steve Greenwald 
and  Serge  Egelman.  Charlie  Payne  volunteered  his 
time  to  help  with  IEEE  guidelines.  While  this  would 
not have been possible without support from IBM and 
Doug  Wilson,  all  opinions  expressed  are  those  of  the 
author.  
Bibliography 
[1] Anne Adams and Martina Angela Sasse, “Users Are Not 
The Enemy”, Communications of the AM, vol. 42, issue 12, 
December 1999, pp 40 – 46.  
[2] J. P. Anderson, Computer Security Technology Planning 
Study,  ESD-TR-73-51,  Bedford,  MA,  USAF  Electronics 
Systems Division, October 1972.  
[3] Dirk Balfanz, Glenn Durfee, and D. K. Smetters, “Making 