title:Property Inference from Poisoning
author:Saeed Mahloujifar and
Esha Ghosh and
Melissa Chase
2022 IEEE Symposium on Security and Privacy (SP)
Property Inference from Poisoning
Saeed Mahloujifar
Princeton University
PI:EMAIL
Esha Ghosh
Microsoft Research
PI:EMAIL
Melissa Chase
Microsoft Research
PI:EMAIL
3
2
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
Abstract—Property inference attacks consider an adversary
who has access to a trained ML model and tries to extract
some global statistics of the training data. In this work, we
study property inference in scenarios where the adversary can
maliciously control a part of the training data (poisoning data)
with the goal of increasing the leakage.
Previous works on poisoning attacks focused on trying to
decrease the accuracy of models. Here, for the ﬁrst time, we
study poisoning attacks where the goal of the adversary is to
increase the information leakage of the model. We show that
poisoning attacks can boost the information leakage signiﬁcantly
and should be considered as a stronger threat model in sensitive
applications where some of the data sources may be malicious.
We theoretically prove that our attack can always succeed
as long as the learning algorithm used has good generalization
properties. Then we experimentally evaluate our on different
datasets (Census dataset, Enron email dataset, MNIST and
CelebA), properties (that are present in the training data as
features, that are not present as features, and properties that are
uncorrelated with the rest of the training data or classiﬁcation
task) and model architectures (including Resnet-18 and Resnet-
50). We were able to achieve high attack accuracy with relatively
low poisoning rate, namely, 2 − 3% poisoning in most of our
experiments. We also evaluated our attacks on models trained
with DP and we show that even with very small values for , the
attack is still quite successful1.
I. INTRODUCTION
Machine learning is revolutionizing nearly every discipline
from healthcare to ﬁnance to manufacturing and marketing.
However, one of the limiting factors in ML is availability of
large quantities of quality data.
This has prompted calls for collaborative learning, where
many parties combine datasets to train a joint model [1, 24].
However, much of this data involves either private data about
individuals or conﬁdential enterprise information. Naturally,
this leads to risks of information leakage of the training data.
We can view the privacy problems of collaborative ML training
from two orthogonal directions:
Information leakage during the training phase This privacy
problem is concerned with leaking information about each
party’s data from the other parties while jointly participating
in training ML models. There has been signiﬁcant research on
how to use Secure Multi-Party Computation (SMPC), trusted
hardware etc. to avoid this type of information leakage.
Information leakage from the trained model While the
aforementioned information leakage is an important problem, it
1Code
PropertyInferenceFromPoisoning.git
is
available
at
https://github.com/smahloujifar/
is orthogonal to the problem of information leakage about the
training data from the ML model itself. Our focus in this paper
is on this second type of leakage. Note that the techniques
mentioned above cannot mitigate this second type of leakage.
For the rest of this paper, we will focus on the second type
of information leakage.
Inference Attacks: Inference attacks consider an adversary
who tries to infer sensitive information about the training set
by inspecting the model that is trained on it. Inference attacks
have come in two main ﬂavors: membership inference [28]
and property inference attacks [2].
In a membership inference attack, an adversary tries to infer
if a special instance was present in the training set that was
used to train a given model. Property inference adversaries try
to infer some aggregate information about the whole training
set. While there are some promising approaches for defending
against membership inference attacks (e.g. differential privacy),
there is no general defense mechanism known against property
inference attacks and how to defend against them is still an
open question. In this work, we focus on property inference
attacks in collaborative learning scenarios and show that these
attacks are more effective than previously thought.
Note that the property being inferred need not be an explicit
feature in the training set, nor does it need to be obviously
correlated with the training set labels. For example, we will
consider a property inference attack on a text based model
(in particular a spam classiﬁer), which attempts to learn the
average sentiment (positive or negative) of the documents in
the training dataset.
Poisoning Attacks In poisoning attacks, some part of training
data (poisoning data) is carefully chosen by an adversary who
wishes to make the trained model behave in his own interest.
A considerable body of works [6, 30, 27, 20, 31, 5, 3] have
shown that poisoning attacks can signiﬁcantly hurt accuracy
of ML models.
Poisoning Attacks Increasing Information Leakage In this
work we initiate the study of poisoning attacks that aim at
increasing the information leakage in ML models. In particular,
we ask the following question:
Can adversaries boost the performance of property inference
attacks by injecting specially crafted poisoning data in the
training set?
This is a relevant question whenever data is gathered
from multiple sources, some of which may be adversarially
controlled. In particular, it is relevant in collaborative machine
© 2022, Saeed Mahloujifar. Under license to IEEE.
DOI 10.1109/SP46214.2022.00140
1120
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
learning, where one party might contribute malicious data in
order to learn some property about the rest of the training set.
To follow our above example, if a group of small companies
pool their data to train a spam classiﬁer, one company might
contribute poison data in an attempt to learn about the average
sentiment in the rest of the group. This could give that company
an edge in understanding it’s competitors’ positions.
We note that the above question could also be asked for
membership inference attacks. While that is an interesting
direction, in this paper we only focus on property inference
attacks. We show that poisoning can indeed increase the
property leakage signiﬁcantly.
Attack Model: In this paper we consider an attacker who is
allowed to ﬁrst submit a set of “poisoned” data of its choice,
which will be combined with the victim dataset and used
to train a model. The attacker can then make a series of
black box (label-only) queries to the trained model. Note that
by black box queries we mean that the adversary only gets
to learn the predicted label for each query. (It does not, for
example, get the conﬁdence values that the model produces,
or any internal information from the model.) Finally, as in the
property inference attacks of [11, 2], the attacker’s goal is to
infer whether the average of a particular property is above
or below a particular threshold. This is a very natural model
for poisoning in the distributed learning setting where data is
gathered from many sources.
Main Contributions:
• Theoretical Results: We ﬁrst describe a theoretical attack
that works for any training algorithm outputs (almost)
Bayes-optimal classiﬁers. The high level idea of our attack
is that the adversary adds poisoning points in a way that
causes the behavior of the Bayes-optimal classiﬁer to
depend on the average of the target property. In particular,
we show that poisoning would change the prediction of
certain instances when the average of property is below
the threshold. But when the average is higher than the
threshold, then the poisoning does not affect the prediction
of those points. See Section V for details of our analysis.
We formalize this intuition by giving a concrete attack
based on our theoretical analysis in this model and
analyzing its effectiveness. Our attack is agnostic to
the architecture of the trained model as it is completely
black box. Note that poisoning is a crucial aspect of our
theoretical analysis and the information leakage does not
necessarily exist if the adversary cannon inject poisons.
• Experimental Results: Real training algorithms do not
always output Bayes-optimal classiﬁers, so there is a
question about whether the above results hold in practice.
To explore how realistic our attack is we run several exper-
iments on a range of datasets, properties and architectures:
– Datasets: Census dataset, the Enron email dataset and
two image datasets (MNIST, CelebA)
– Properties: We consider three types of target properties:
1) Properties that are explicitly present in the training
dataset, e.g., Gender and Race in Census data.
2) Properties that are not present as a explicit input in
the training data, but which may be derived from
those existing inputs e.g., Negative sentiment in
emails (as determined by sentiment analysis).
3) Properties that are uncorrelated with the rest of the
training data or classiﬁcation task: for this, we added
an independently chosen random binary feature to
each data entry in both Census and Enron data.
– Target model architecture: We run our experiments for
logistic regression, fully connected neural networks,
and deep architectures such as Resnet-18 and Resnet-
50 (See Section VII).
In most of our experiments, the objective of the attacker
is to distinguish whether or not
the target property
appears with high frequency in the dataset. Our attack can
successfully distinguish various ranges of higher vs.lower
frequencies, e.g. (5% from 15%), (30% from 70%). We
were able to achieve above 90% attack accuracy with
about 1− 10% poisoning in all of these experiments. Note
that, while the maximum poisoning rate we use in our
attacks, is 10%, most of our experiments succeed with a
much lower rate of poisoning (2 − 3%). In fact, the only
two cases where we used a higher poisoning rate 9− 10%
(Enron negative sentiment and CelebA gender).
In addition to the attacks above where the goal of the
adversary is to distinguish between two predeﬁned values,
we evaluate how well the attacker can predict the true
ratio without the knowledge of the upper and lower ratios.
Our experiments suggest that by training shadow models
with different ratios, the adversary can train a regression
model (instead of classiﬁcation) and predict the threshold
with average absolute error of less than 5%.
• Effectiveness of DP as mitigation: We also explore the
effect of Differential Privacy as a way to mitigate our
attack. Differential privacy can be seen as a two-fold
defense as it can mitigate poisoning attacks and it also
reduces information leakage. Our experiments with models
trained with DP-SGD [4] show that Differential Privacy
alone cannot mitigate our attack. For instance at (, δ) =
(0.95, 10−5) our attack is still 90% accurate.
Discussion: Is 10% poisoning rate realistic? As we discuss
above, most of our experiments succeed with relatively little
poisoned data (2−3%). However, we believe, in some scenarios,
even a high poisoning rate (9 − 10%) is realistic. For example,
if there are less than 10 companies are sharing their data to
train a model. Even in scenarios where more than 10 parties are
participating (e.g., federated learning), the attacker can collude
with other parties to form a large portion of the dataset.
II. RELATED WORK
It is quite well known by now that understanding what ML
models actually memorize from their training data is not trivial.
As discussed above, there is a rich line of work that tries to
investigate privacy leakage from ML models under different
threat models. Here we provide some more detail on the the
works which seem most related to ours. For a comprehensive
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
21121
survey on the other privacy attacks on neural networks, please
see [13].
The global property inference attacks of [2, 12] are the most
relevant to us: here the adversary’s goal is to infer sensitive
global properties of the training dataset from the trained model
that the model producer did not intend to share. We have
already described some examples above. Property inference
attacks were ﬁrst formulated and studied in [2]. However, this
initial approach did not scale well to deep neural networks,
so [12] proposed a modiﬁed attack that is more efﬁcient. The
main differences from our attack are in the threat model: 1) our
adversary can poison a portion of the training data and 2) in [2,
12] the adversary has whitebox access to the model meaning