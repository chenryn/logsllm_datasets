# Property Inference from Poisoning

**Authors:**
- Saeed Mahloujifar, Princeton University
- Esha Ghosh, Microsoft Research
- Melissa Chase, Microsoft Research

**Conference:**
2022 IEEE Symposium on Security and Privacy (SP)

## Abstract
Property inference attacks involve an adversary who accesses a trained machine learning (ML) model to extract global statistics about the training data. In this work, we investigate property inference in scenarios where the adversary can maliciously control a portion of the training data (poisoning data) with the goal of increasing information leakage.

Previous studies on poisoning attacks have focused on reducing model accuracy. Here, for the first time, we study poisoning attacks aimed at increasing information leakage. We demonstrate that such attacks can significantly boost information leakage and should be considered a serious threat in sensitive applications where some data sources may be malicious. Theoretically, we prove that our attack can always succeed if the learning algorithm has good generalization properties. Experimentally, we evaluate our attack on various datasets (Census, Enron email, MNIST, CelebA), properties (present as features, derived from features, uncorrelated with other data or classification tasks), and model architectures (including Resnet-18 and Resnet-50). We achieve high attack accuracy with a relatively low poisoning rate, typically 2-3%. Additionally, we test our attacks on models trained with differential privacy (DP) and show that even with small values of ε, the attack remains highly effective.

## 1. Introduction

Machine learning is transforming numerous fields, from healthcare to finance. However, a key challenge is the availability of large, high-quality datasets. This has led to calls for collaborative learning, where multiple parties combine their data to train a joint model. However, this data often includes private or confidential information, leading to risks of information leakage.

We can categorize the privacy problems in collaborative ML training into two main types:

1. **Information Leakage During Training:** This concerns leaking information about each party's data during the training process. Techniques like Secure Multi-Party Computation (SMPC) and trusted hardware are used to mitigate this.
2. **Information Leakage from Trained Model:** This involves leaking information about the training data from the trained model itself. Our focus is on this second type of leakage, which cannot be mitigated by the techniques mentioned above.

### Inference Attacks
Inference attacks involve an adversary trying to infer sensitive information about the training set by inspecting the trained model. These attacks come in two main forms:
- **Membership Inference:** The adversary tries to determine if a specific instance was in the training set.
- **Property Inference:** The adversary tries to infer aggregate information about the entire training set. While there are defenses against membership inference, no general defense exists for property inference.

### Poisoning Attacks
Poisoning attacks involve an adversary carefully selecting part of the training data to influence the model's behavior. Previous works have shown that poisoning attacks can significantly reduce model accuracy. In this work, we study poisoning attacks aimed at increasing information leakage. Specifically, we ask:
- Can adversaries enhance the performance of property inference attacks by injecting specially crafted poisoning data into the training set?

This is particularly relevant in collaborative learning, where one party might contribute malicious data to learn about the rest of the training set. For example, a company might inject poison data to learn about the average sentiment in emails from other companies.

## 2. Attack Model
Our attack model involves an adversary who:
1. Submits a set of "poisoned" data.
2. Makes black-box queries (label-only) to the trained model.
3. Aims to infer whether the average of a particular property is above or below a threshold.

## 3. Main Contributions
- **Theoretical Results:** We describe a theoretical attack that works for any training algorithm that outputs almost Bayes-optimal classifiers. The attack causes the classifier's behavior to depend on the average of the target property. Poisoning changes the prediction of certain instances when the average property is below the threshold but not when it is above.
- **Experimental Results:** We run experiments on various datasets, properties, and architectures. Our experiments show that the attack can distinguish between different frequencies of the target property with high accuracy (above 90%) using a low poisoning rate (2-3%). We also explore how well the attacker can predict the true ratio without knowing the upper and lower bounds.
- **Effectiveness of DP as Mitigation:** We evaluate the impact of Differential Privacy (DP) as a mitigation. Our experiments show that even with DP, the attack remains highly effective, achieving 90% accuracy at (ε, δ) = (0.95, 10^-5).

## 4. Discussion
While most of our experiments succeed with a low poisoning rate (2-3%), a higher rate (9-10%) is also realistic in scenarios with fewer participating parties. Even in federated learning, an attacker can collude with other parties to form a significant portion of the dataset.

## 5. Related Work
Understanding what ML models memorize from their training data is non-trivial. There is extensive research on privacy leakage from ML models under different threat models. The most relevant works to ours are [2, 12], which study global property inference attacks. Our work differs in that our adversary can poison the training data and has black-box access to the model.

---

This revised version aims to make the text more coherent, clear, and professional. It also ensures that the structure and flow of the document are improved for better readability.