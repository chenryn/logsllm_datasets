in the resulting stream of packet lengths. From
42
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:02 UTC from IEEE Xplore.  Restrictions apply. 
the Viterbi alignment of the packet lengths to the
phrase HMM, we get the subsequence(s) of pack-
ets indicating potential hits for the phrase, with log
odds scores for each. Subsequences with scores
above a given threshold are considered deﬁnitive
hits, and each hit is labeled as a true positive only
if it contains all of the words for the given phrase.
Any deﬁnitive hit which does not contain all words
in the phrase is considered a false positive.
We adapt standard metrics from the informa-
tion retrieval community to assess the effective-
ness of our approach. Let T Pt, F Pt, and F Nt
be the number of true positives, false positives,
and false negatives achieved when operating with
threshold t. Then, the precision at t is deﬁned as
prect = T Pt/(T Pt + F Pt) and measures the prob-
ability that a reported match is correct. We also
use recall, deﬁned as recallt = T Pt/(T Pt + F Nt),
as the probability that the algorithm will ﬁnd the
phrase if the phrase is indeed contained within the
ciphertext.
Ideally a search algorithm would ex-
hibit precision and recall close to 1.0.
To assess the accuracy of our approaches under
different parameters, we compute recall and preci-
sion over a variety of thresholds. An intuitive way
to derive the threshold for a given model would be
to use the average log odds score (Equation 1) of
the training sequences. However, since the log odds
score is proportional to the length of the phrase, we
cannot directly compare the performance of mod-
els for different phrases at the same log odds score.
Therefore, to compare accuracy between models
for different phrases, we set the threshold for each
model to be some fraction of the model’s log odds
score observed during training . Explicitly, for each
phrase p, let σp be the average log odds score for
the model mp. σp will be proportional to the length
of mp. For a multiplier δ ∈ [0, 2] we set the testing
threshold tp = δ×σp, and compute the average pre-
cision and recall at multiplier δ using T Ptp, F Ptp,
and F Ntp for each phrase p in our testing set. We
can then examine how precision relates to recall by
plotting average precision versus average recall at
each value of δ (see, for example Figures 8–9).
With these comparison metrics at hand, we can
now proceed to analyze the accuracy of our ap-
proach. First, we take an analytical approach and
examine our performance over a range of thresh-
olds to study the impact of the pronunciation dic-
tionary and of noise in the audio channel on our
ability to spot phrases. Then, we assume the view-
point of an attacker and empirically estimate a spe-
ciﬁc threshold for each phrase. Finally, we dis-
cuss strategies for mitigating the information leak-
age that enables the attack.
i
i
n
o
s
c
e
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Recall v. Precision (by Pronunciation Dictionary)
Empirical Pronunciation Dictionary
TIMIT Pronunciation Dictionary
 0.2
 0.4
Recall
 0.6
 0.8
 1
Figure 8. Comparing the performance
of pronunciation dictionaries
The Importance of Accurate Pronunciations
In order to build a model for a phrase, we ﬁrst must
know the phonemes that comprise the phrase. Al-
though TIMIT includes a primitive pronunciation
dictionary, with pronunciations given for each word
in the corpus, the included pronunciations were
originally taken from an old version of Merriam-
Webster’s Pocket Dictionary, and thus may repre-
sent “proper” American English rather than realis-
tic colloquial speech. Therefore, we also use the
phonetic transcriptions for the training sentences to
build up an empirically-derived pronunciation dic-
tionary based on the way the speakers say each
word in the training data. For increased coverage
in our empirical dictionary, we also include pro-
nunciations from the PRONLEX dictionary, which
were derived in a similar fashion from the CALL-
HOME telephone speech corpus [14]. We compare
the accuracy of our search HMM using these two
43
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:02 UTC from IEEE Xplore.  Restrictions apply. 
pronunciation dictionaries and present the results
in Figure 8.
Clearly, the quality of the pronunciation dictio-
nary is critical to the success of our phrase spot-
ting technique. With the default TIMIT pronun-
ciations, we achieve equal recall and precision at
around 0.28. However, using the more realistic pro-
nunciation dictionary, we simultaneously achieve
recall of 0.50 and precision of 0.51. In other words,
we are able to ﬁnd, on average, 50% of the in-
stances of the phrases of interest, and when the al-
gorithm indicates a match, there is a 51% chance
that the ﬂagged packets do indeed encode the given
phrase. These results are especially disconcerting
given that the conversation was encrypted in or-
der to prevent an eavesdropper from recovering this
very information. In light of these results, we per-
form the remaining experiments using our the em-
pirically derived pronunciation dictionary.
Robustness to Noise We also evaluate the im-
pact of noise on our ability to identify phrases. For
this test, we add pink noise to the simulated con-
versations in the TIMIT test data. We chose pink
noise, rather than white noise, or any number of
background sounds (metal pots and pans clanging,
a baby crying, etc.), because the energy is logarith-
mically distributed across the range of human hear-
ing. This makes pink noise much more difﬁcult for
the codec’s noise removal algorithm to ﬁlter, and
therefore should inﬂuence the choice of bit rates in
the packets. Furthermore, the use of such additive
noise generation techniques is common practice for
exploring the impact of noise on speech recognition
methods (e.g., [33, 17, 13]).
We experimented with three additive noise sce-
narios: 90% sound to 10% noise, 75% to 25%, and
50% to 50%. With 10% noise, the recordings sound
as if they were transmitted over a cell phone with
poor reception, and with 50% noise it is almost im-
possible for a human to determine what is being
said. Figure 9 shows the results for these exper-
iments. Notice that with 10% noise, we are still
able to achieve recall of .39 and precision of .40.
Even with 25% noise, we can still achieve recall
and precision of .22 and .23, respectively. These
i
i
n
o
s
c
e
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Recall v. Precision (Impact of Noise)
Overall
10%
25%
50%
 0.2
 0.4
Recall
 0.6
 0.8
 1
Figure 9. Results with noisy data
results show that as long as the quality of the voice
channel is reasonable, the attacker can identify an
alarming number of phrases.
An Attacker’s Point of View Until now, we
studied the success of our techniques across a wide
range of thresholds. An attacker, on the other hand,
would need to pick a single threshold in advance.
Unfortunately for the attacker, picking an optimal
threshold in such cases is a challenging problem.
Therefore, to explore the problem of threshold se-
lection, we discuss a technique to estimate a good
threshold, and the resulting expected performance.
As mentioned earlier, for a phrase p, the aver-
age log odds score σp that is observed during the
training of model mp is roughly indicative of how
well the model will be able to perform in practice.
Loosely speaking, if σp is large, then the model will
exhibit high true positive rates. We use this obser-
vation to our advantage when selecting the attack
threshold tp. That is, we empirically estimate tp
as a linear function of σp, setting tp = δp × σp,
where δp is a multiplier that maximizes the “qual-
ity” of the search algorithm. To complete our task
of selecting a threshold we must then solve two
problems: (1) select a general function that deﬁnes
the “quality” of the search algorithm at a speciﬁc
threshold; and (2) choose a way to estimate the δp
that maximizes quality.
While we could deﬁne the “quality” at thresh-
old t as either recallt or precisiont, neither metric
44
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:02 UTC from IEEE Xplore.  Restrictions apply. 
Instead, to achieve a
is appropriate for this task.
good balance of precision and recall, we deﬁne the
quality of a search algorithm at threshold t to be
the difference between the number of true positives
and the number of false positives at t: T Pt − F Pt.
If the adversary has access to a relatively small
number of recorded phrases, she can build search
HMMs for them and use the performance of these
models to derive a good value of δ for use in set-
ting the thresholds for other phrases that she really
wants to search for. We use leave-out-k cross vali-
dation to estimate her chances of success using the
TIMIT testing data. In each of several iterations,
we select k phrases (˜p1, . . . , ˜pk) at random from the
testing set and ﬁnd the thresholds t˜p1, . . . , t˜pk that
maximize the difference in true positives and false
positives for each phrase. We set δ˜pi = t˜pi/σ˜pi for
each i ∈ [1, k], and set δ to be the average over
δ˜pi. Then, for each phrase p in the remainder of the
test set, we estimate our maximizing threshold for
p to be tp = δ × σp, and calculate the recall and
precision for phrase p at threshold tp.
Attacker’s Recall and Precision
0.80
n
o
i
s
i
c
e
r
P
0.60
0.40
0.20
0.00
0.20
0.40
0.60
Recall
0.80
Figure 10. Attacker’s accuracy
Setting k to be 1/4 of our testing set, this tech-
nique achieves mean recall and precision rates of
(.32, .75). Given that our original averages were
(.50, .51), it seems that our estimation technique
is somewhat conservative, selecting thresholds that
are higher than optimal. The values of recall
and precision achieved for each phrase, using our
threshold selection algorithm, are presented in Fig-
ure 10. Each of the points denotes the recall and
precision for one of the 122 phrases in our test set.
Because simple scatter plots often plot many points
on top of one another, we also vary the background
color to indicate the density of the points in each
area of the graph. Dark backgrounds indicate high
density, and light backgrounds indicate areas of low
density. While this algorithm is not optimal, its re-
call is often above 40%, and we can recognize most
of the phrases with precision greater than 80%. We
believe this shows concretely that an attacker with
access to only population statistics and the cipher-
text of a VBR encoded and encrypted VoIP conver-
sation has almost a one in three chance of ﬁnding a
phrase of her choice!
Analysis of Results While our approach per-
forms well on average,
there are also several
phrases that we can ﬁnd with great accuracy. Fig-
ure 11 shows precision and recall for four inter-
esting phrases. We exhibited the highest accu-
racy when searching for the phrase “Young children
should avoid exposure to contagious diseases.”.
For this phrase, our technique achieves a precision
of 1.0 and a recall of .99. We also perform well
on the phrase “The fog prevented them from ar-
riving on time.”, achieving .84 precision and .72
recall. These results illustrate the success of our
technique in identifying words and phrases we have
never seen before, as neither occurs in our train-
ing set. Also noteworthy are phrases “She had
your dark suit in greasy wash water all year.” and
“Don’t ask me to carry an oily rag like that.” which
were the only two phrases spoken by every user in
the TIMIT database. We achieve precision/recall
scores of (.90/.82) and (.92/.81), respectively.
Naturally, given that we are searching for
phrases in encrypted audio trafﬁc,
identifying
each phrase exactly can be extremely challenging.
Sometimes, when our search model misses an in-
stance of the target phrase, it only misses one or
two of the words at the beginning or at the end of
the phrase. Because our very strict deﬁnition of a
true positive excludes such hits, it may underesti-
mate the practical performance of our technique.
45
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:02 UTC from IEEE Xplore.  Restrictions apply. 
i
i
n
o
s
c
e
r
P
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.5
Recall v. Precision
Young children...
Don’t ask me...
She had your...
The fog...
 0.6
 0.7
 0.8
 0.9
 1
Recall
Figure 11. Performance on selected
phrases
When we designate hits that contain at least n − 2
of the n words in the phrase as true positives, the
algorithm’s recall and precision improve to .55 and
.53, respectively. Compared to our original, stricter
classiﬁcation, this represents improvement of 9%
in recall and 4% in precision.
To identify other causes of the differences in ac-
curacy between phrases, we examined several fea-
tures of the phrases, including their length, pho-
netic composition, and the distribution of packet
sizes for the words and phonemes in the phrase. In-
terestingly, we found no statistically signiﬁcant cor-
relation between recognition accuracy and the fre-
quency of any of the phonemes. Given that TIMIT
was designed as a phonetically rich corpus, we be-
lieve this shows that our technique is robust and
ﬂexible enough to handle the vast majority of words
in spoken English.
According to our analysis, the most important
factors in determining our ability to recognize a
given phrase in the TIMIT data are: (1) the length
of the phrase in packets, and (2) the individual
speakers who spoke the phrase. Short phrases are
difﬁcult to spot reliably because it is much more
likely that short patterns of packets will occur ran-