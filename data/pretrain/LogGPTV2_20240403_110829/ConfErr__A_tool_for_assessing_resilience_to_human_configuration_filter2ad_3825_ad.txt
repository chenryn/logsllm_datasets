# Resilience to Semantic Errors

Internet RFC-1912 defines a list of common DNS configuration errors, which can occur at multiple levels, from the choice of names to the relationships between records on different servers. Many of these errors are related to the structure of records published by servers. We used ContErr to test the behavior of BIND and djbdns when faced with such misconfigurations.

We started with a set of configuration files containing a forward zone with several hosts, corresponding mail exchanger (MX) records, TXT, RP, and HINFO records, and several aliases. Additionally, we included a reverse zone that maps IP addresses to their names. We then injected record-level errors as described in RFC-1912.

The error generation process is system-independent and is defined on an abstract representation of the DNS records published by each server. A simple transformation maps the data parsed from the configuration files of each SUT into this abstract representation. Another transformation maps the abstract representation back to the system-specific configuration format, allowing us to construct faulty configuration files.

Table 3 shows a subset of the configuration errors described in the RFC and the corresponding behavior for both DNS servers. The fault injection was performed on a larger set of errors, but for brevity, we only present some of the more interesting ones here.

| Err# | Description of Fault | BIND | djbdns |
|------|---------------------|-------|---------|
| 1    | Missing PTR         | not found | N/A     |
| 2    | PTR pointing to CNAME | not found | N/A     |
| 3    | Duplicate name for NS and CNAME | found | not found |
| 4    | MX pointing to CNAME | found | not found |

### Configuration File Format Differences

The configuration file format used by djbdns allows administrators to define multiple related records with a single directive. For example, it is possible to define the A (address) record and corresponding PTR record with one directive. In our test configuration, this feature means that the fault injection tool cannot introduce certain types of faults, such as missing PTR or PTR pointing to CNAME, because all record representations containing such faults cannot be transformed back to a valid configuration file. In contrast, BIND requires each record to be defined separately, making it susceptible to these errors. This configuration flexibility is a significant advantage for djbdns.

### Error Detection Capabilities

BIND is effective in detecting errors of class (3) and (4). It stops loading the zone and signals the operator about the issue. However, djbdns, despite its strength resulting from the configuration format, does not check the consistency of its data from this perspective. Using ContErr to inject real-world configuration errors that go beyond mere syntactical issues enables the study and testing of whole-system behavior, not just the configuration parser. Developers can use ContErr to identify non-parse-related areas of the system that require improvement.

## Comparing Error Resilience

A configuration process can be viewed as the transformation of an initial configuration file (usually the default one shipped with the system) into a new configuration file. This transformation is achieved by adding, deleting, and/or modifying directives. An ideal system should detect all errors introduced during this transformation. We measure the resilience of a system to configuration errors by simulating the configuration process multiple times and determining the percentage of errors detected across all experiments.

ContErr uses a benchmark script to automatically transform initial configuration files into new, valid files. It then creates faulty configuration files based on these new files and verifies the system's behavior. Errors are injected near the places where the file has been validly modified, simulating the common ways in which errors sneak into configurations. This procedure mimics the human configuration process, constituting a primitive human error benchmark.

### Postgres vs. MySQL

We used this approach to compare Postgres and MySQL. To generate faulty configuration files, we iterated through typos in the values of all directives. We did not inject errors in directive names, as both systems are known to detect these errors (ยง5.2). For the starting configuration, we used a file containing most of the available directives, along with their default values, and excluded directives with no default value. Since neither Postgres nor MySQL accept typos in directives with boolean values, we excluded them from the test.

We found that Postgres is more resilient to typos than MySQL. We ran 20 experiments for each directive, injecting one typo in the corresponding directive value in each experiment. For each directive, we computed the percentage of experiments in which the system detected the error. Figure 3 summarizes the distribution of directives across four ranges of detection: poor (0-25% of faults detected), fair (25%-50%), good (50%-75%), and excellent (75%-100%). Postgres was able to detect more than 75% of the typos in almost 45% of its directives, while MySQL detected less than 25% of the typos in the same fraction of its directives.

![Figure 3: Resilience to Typos in MySQL and Postgres](figure3.png)

The results can be explained by the fact that Postgres features a strong constraint checking mechanism for its numeric parameters, which can detect many typos. Additionally, the MySQL flaws mentioned in ยง5.2 increase the number of undetected typos.

Our comparison provides an estimate of the overall resilience of the systems to typos in directive values. However, using the same procedure, one could conduct more focused, configuration-task-oriented benchmarks. By leveraging domain-specific knowledge, it is possible to define a subset of relevant directives and obtain a more precise comparison of task-specific resilience.

For increased thoroughness, the benchmark can include other types of errors, such as omissions and duplications. Similarly, a benchmark could include domain-specific semantic error models, like the one in ยง5.4.

## Related Work

Several researchers have recognized human errors as an important factor in system dependability. Here, we sample prior work related to ContErr and contrast it with our approach.

- **Brown and Patterson [2]** proposed benchmarks that include the operator as a component of the system, which can either increase or reduce dependability. With ContErr, we directly simulate human errors, resulting in time and cost savings, though it may be less realistic.
- **Nagaraja et al. [7]** described a testbed for fault tolerance techniques aimed at human errors. They emulate the operator via test scripts that embody specific errors observed during tests with real human operators. Our approach extends this method by synthesizing general error models, enabling the simulation of a wider set of errors and the automatic introduction of variations.
- **Vieira and Madeira [15]** aimed to assess recoverability in DBMSes by emulating both faults and the recovery procedure carried out by the operator. While their work assumes the error detection capability of the system to analyze the recovery procedure, ContErr directly measures this capability.
- **Brown and Hellerstein [3]** introduced a method for measuring the complexity of configuration processes in terms of time taken to complete the configuration and the probability of completing without errors. In our work, we focus on the system's ability to handle configuration errors rather than the probability of error.
- **Maxion and Reeder [6]** analyzed the genesis of human errors and the impact interfaces have on them. To our knowledge, no automatic tool for error generation has been proposed; existing work can be leveraged to extend ContErr's models.

## Conclusion

Configuration errors are a dominant cause of system downtime but are rarely considered in the design, testing, and evaluation of systems. Direct testing for these errors traditionally involves real humans, making it complex, subjective, and hard to reproduce. In this paper, we presented ContErr, a tool that automatically tests the behavior of a system when faced with human configuration errors. Instead of relying on human subjects, ContErr relies on models derived from psychological and linguistic studies. It automatically generates realistic configuration errors, injects them in a system-generic fashion, and assesses their impact. ContErr is designed to be extensible, allowing for the addition of new error generation plugins.

We demonstrated that ContErr enables developers to test the resilience of real systems with minimal effort. We reported case studies on MySQL, Postgres, Apache, BIND, and djbdns, with each SUT taking less than one hour to test. We identified flaws in these popular server applications and showed how to compare one system to another, taking a step toward dependability benchmarks that include the human factor.

## References

[1] Apache HTTP Wiki. <http://wiki.apache.org/httpd/>
[2] A. Brown, L. C. Chung, and D. A. Patterson. Including the human factor in dependability benchmarks. In Proc. DSN Workshop on Dependability Benchmarking, 2002.
[3] A. Brown and J. Hellerstein. An approach to benchmarking configuration complexity. In Proc. SIGOPS European Workshop, Leuven, Belgium, Sept. 2004.
[4] J. Gray. Why do computers stop and what can be done about it? In Proc. 5th Symp. on Reliability in Distributed Software and Database Systems, 1986.
[5] XML information set. <http://w3.org/TR/xml-infoset>
[6] R. A. Maxion and R. W. Reeder. Improving user-interface dependability through mitigation of human error. Int. J. Hum.-Comput. Stud., 63(1-2):25-50, 2005.
[7] K. Nagaraja, F. Oliveira, R. Bianchini, R. P. Martin, and T. D. Nguyen. Understanding and dealing with operator mistakes in Internet services. In Proc. 6th Symp. on Operating Systems Design & Implementation, 2004.
[8] F. Oliveira, K. Nagaraja, R. Bachwani, R. Bianchini, R. P. Martin, and T. D. Nguyen. Understanding and validating database system administration. In Proc. USENIX Annual Technical Conf., 2006.
[9] D. Oppenheimer, A. Ganapathi, and D. Patterson. Why do Internet services fail, and what can be done about it? In Proc. 4th USENIX Symposium on Internet Technologies and Systems, 2003.
[10] Oracle. Oracle database 10g release 2 administrator's guide. B14231-02, May 2006.
[11] Oracle. Oracle database reference 10g release 2, 2006.
[12] S. Pertet and P. Narsimhan. Causes of failures in web applications. Technical Report CMU-PDL-05-109, Carnegie Mellon University, 2005.
[13] J. Reason. Human Error. Cambridge University Press, 1990.
[14] B. van Berkel and K. D. Smedt. Triphone analysis: a combined method for the correction of orthographical and typographical errors. In Proc. 2nd Conf. on Applied Natural Language Processing, 1988.
[15] M. Vieira and H. Madeira. Recovery and performance balance of a COTS DBMS in the presence of operator faults. In Proc. Int. Conf. on Dependable Systems and Networks, 2002.
[16] A. Wool. A quantitative study of firewall configuration errors. Computer, 37(6):62-67, June 2004.
[17] XML path language (XPath). <http://w3.org/TR/xpath>
[18] XSL transformations (XSLT). <http://w3.org/TR/xslt>
[19] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Networked Windows NT system field failure data analysis. In Proc. Pacific Rim Int. Symp. on Dependable Computing, 1999.

---

This optimized version aims to improve clarity, coherence, and professionalism, while maintaining the original content and intent.