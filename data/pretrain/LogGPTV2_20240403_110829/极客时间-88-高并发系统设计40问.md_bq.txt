# 38 \| 计数系统设计（二）：50万QPS下如何设计未读数系统？你好，我是唐扬。 在上一节课中我带你了解了如何设计一套支撑高并发访问和存储大数据量的通用计数系统，我们通过缓存技术、消息队列技术以及对于Redis的深度改造，就能够支撑万亿级计数数据存储以及每秒百万级别读取请求了。然而有一类特殊的计数并不能完全使用我们提到的方案，那就是未读数。 未读数也是系统中一个常见的模块，以微博系统为例，你可看到有多个未读计数的场景，比如： 1.  当有人    \@你、评论你、给你的博文点赞或者给你发送私信的时候，你会收到相应的未读提醒；        2.  在早期的微博版本中有系统通知的功能，也就是系统会给全部用户发送消息，通知用户有新的版本或者有一些好玩的运营活动，如果用户没有看，系统就会给他展示有多少条未读的提醒。        3.  我们在浏览信息流的时候，如果长时间没有刷新页面，那么信息流上方就会提示你在这段时间有多少条信息没有看。        那当你遇到第一个需求时，要如何记录未读数呢？其实，这个需求可以用上节课提到的通用计数系统来实现，因为二者的场景非常相似。 你可以在计数系统中增加一块儿内存区域，以用户 ID 为 Key存储多个未读数，当有人 @ 你时，增加你的未读\@的计数；当有人评论你时，增加你的未读评论的计数，以此类推。当你点击了未读数字进入通知页面，查看@你或者评论你的消息时，重置这些未读计数为零。相信通过上一节课的学习，你已经非常熟悉这一类系统的设计了，所以我不再赘述。 那么系统通知的未读数是如何实现的呢？我们能用通用计数系统实现吗？答案是不能的，因为会出现一些问题。 系统通知的未读数要如何设计来看具体的例子。假如你的系统中只有 A、B、C三个用户，那么你可以在通用计数系统中增加一块儿内存区域，并且以用户 ID 为Key来存储这三个用户的未读通知数据，当系统发送一个新的通知时，我们会循环给每一个用户的未读数加1，这个处理逻辑的伪代码就像下面这样：     List userIds = getAllUserIds();    for(Long id : userIds) {      incrUnreadCount(id);    }这样看来，似乎简单可行，但随着系统中的用户越来越多，这个方案存在两个致命的问题。 首先，获取全量用户就是一个比较耗时的操作，相当于对用户库做一次全表的扫描，这不仅会对数据库造成很大的压力，而且查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的。如果你的用户库已经做了分库分表，那么就要扫描所有的库表，响应时间就更长了。**不过有一个折中的方法，**那就是在发送系统通知之前，先从线下的数据仓库中获取全量的用户ID，并且存储在一个本地的文件中，然后再轮询所有的用户ID，给这些用户增加未读计数。 这似乎是一个可行的技术方案，然而它给所有人增加未读计数，会消耗非常长的时间。你计算一下，假如你的系统中有一个亿的用户，给一个用户增加未读数需要消耗1ms，那么给所有人都增加未读计数就需要 100000000 \* 1 /1000 = 100000秒，也就是超过一天的时间；即使你启动 100个线程并发的设置，也需要十几分钟的时间才能完成，而用户很难接受这么长的延迟时间。 另外，使用这种方式需要给系统中的每一个用户都记一个未读数的值，而在系统中，活跃用户只是很少的一部分，大部分的用户是不活跃的，甚至从来没有打开过系统通知，为这些用户记录未读数显然是一种浪费。 通过上面的内容，你可以知道为什么我们不能用通用计数系统实现系统通知未读数了吧？那正确的做法是什么呢？ 要知道，系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，你可以记录一下在这个列表中每个人看过最后一条消息的ID，然后统计这个 ID之后有多少条消息，这就是未读数了。 ![](Images/94f8ce3acf34991785db0272c2017e99.png)savepage-src="https://static001.geekbang.org/resource/image/a5/10/a5f0b6776246dc6b4c7e96c72d74a210.jpg"}这个方案在实现时有这样几个关键点： 1.  用户访问系统通知页面需要设置未读数为    0，我们需要将用户最近看过的通知 ID 设置为最新的一条系统通知    ID；    2.  如果最近看过的通知 ID 为空，则认为是一个新的用户，返回未读数为    0；    3.  对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知    ID 清空，节省内存空间。        **这是一种比较通用的方案，即节省内存，又能尽量减少获取未读数的延迟。**这个方案适用的另一个业务场景是全量用户打点的场景，比如像下面这张微博截图中的红点。 ![](Images/28e0e8e712919d685bbb8943d9ab4400.png)savepage-src="https://static001.geekbang.org/resource/image/ae/3f/ae6a5e9e04be08d18c493729458d543f.jpg"}这个红点和系统通知类似，也是一种通知全量用户的手段，如果逐个通知用户，延迟也是无法接受的。**因此你可以采用和系统通知类似的方案。** 首先，我们为每一个用户存储一个时间戳，代表最近点过这个红点的时间，用户点了红点，就把这个时间戳设置为当前时间；然后，我们也记录一个全局的时间戳，这个时间戳标识最新的一次打点时间，如果你在后台操作给全体用户打点，就更新这个时间戳为当前时间。而我们在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展示红点，反之，就不展示红点了。 ![](Images/3c5c777e3e388dfaff3642f26546d8fe.png)savepage-src="https://static001.geekbang.org/resource/image/55/98/553e7da158a7eca56369e23c9b672898.jpg"}这两个场景的共性是全部用户共享一份有限的存储数据，每个人只记录自己在这份存储中的偏移量，就可以得到未读数了。 你可以看到，系统消息未读的实现方案不是很复杂，它通过设计避免了操作全量数据未读数，如果你的系统中有这种打红点的需求，那我建议你可以结合实际工作灵活使用上述方案。 最后一个需求关注的是微博信息流的未读数，在现在的社交系统中，关注关系已经成为标配的功能，而基于关注关系的信息流也是一种非常重要的信息聚合方式，因此，如何设计信息流的未读数系统就成了你必须面对的一个问题。 如何为信息流的未读数设计方案信息流的未读数之所以复杂主要有这样几点原因。 1.  首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的，就是说，你关注的人发布了新的微博，那么你作为粉丝未读数就要增加    1。如果微博用户都是像我这样只有几百粉丝的"小透明"就简单了，你发微博的时候系统给你粉丝的未读数增加    1 不是什么难事儿。但是对于一些动辄几千万甚至上亿粉丝的微博大 V    就麻烦了，增加未读数可能需要几个小时。假设你是杨幂的粉丝，想了解她实时发布的博文，那么如果当她发布博文几个小时之后，你才收到提醒，这显然是不能接受的。所以未读数的延迟是你在设计方案时首先要考虑的内容。        2.  其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。也就是说，用户即使打开微博客户端什么都不做，这个接口也会被请求到。在几年前，请求未读数接口的量级就已经接近每秒    50    万次，这几年随着微博量级的增长，请求量也变得更高。而作为微博的非核心接口，我们不太可能使用大量的机器来抗未读数请求，因此，如何使用有限的资源来支撑如此高的流量是这个方案的难点。        3.  最后，它不像系统通知那样有共享的存储，因为每个人关注的人不同，信息流的列表也就不同，所以也就没办法采用系统通知未读数的方案。        那要如何设计能够承接每秒几十万次请求的信息流未读数系统呢？你可以这样做： 1.  首先，在通用计数器中记录每一个用户发布的博文数；        2.  然后在 Redis 或者 Memcached    中记录一个人所有关注人的博文数快照，当用户点击未读消息重置未读数为 0    时，将他关注所有人的博文数刷新到快照中；        3.  这样，他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数。        ![](Images/36b35afd67c518b50349c6e8550ef63e.png)savepage-src="https://static001.geekbang.org/resource/image/a5/8a/a563b121ae1147a2d877a7bb14c9658a.jpg"}假如用户 A，像上图这样关注了用户 B、C、D，其中 B 发布的博文数是 10，C发布的博文数是 8，D 发布的博文数是 14，而在用户 A最近一次查看未读消息时，记录在快照中的这三个用户的博文数分别是6、7、12，因此用户 A的未读数就是（10-6）+（8-7）+（14-12）=7。 这个方案设计简单，并且是全内存操作，性能足够好，能够支撑比较高的并发，事实上微博团队仅仅用16 台普通的服务器就支撑了每秒接近 50万次的请求，这就足以证明这个方案的性能有多出色，因此，它完全能够满足信息流未读数的需求。 当然了这个方案也有一些缺陷，比如说快照中需要存储关注关系，如果关注关系变更的时候更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔除一些数据，那么被剔除用户的未读数就变为0 了。但是好在用户对于未读数的准确度要求不高（未读 10 条还是 11条，其实用户有时候看不出来），因此，这些缺陷也是可以接受的。 通过分享未读数系统设计这个案例，我想给你一些建议： 1.       缓存是提升系统性能和抵抗大并发量的神器，像是微博信息流未读数这么大的量级我们仅仅使用十几台服务器就可以支撑，这全都是缓存的功劳；        2.       要围绕系统设计的关键困难点想解决办法，就像我们解决系统通知未读数的延迟问题一样；        3.       合理分析业务场景，明确哪些是可以权衡的，哪些是不行的，会对你的系统设计增益良多，比如对于长久不登录用户，我们就会记录未读数为    0，通过这样的权衡，可以极大地减少内存的占用，减少成本。        课程小结以上就是本节课的全部内容了，本节课我带你了解了未读数系统的设计，这里你需要了解的重点是： 1.       评论未读、@未读、赞未读等一对一关系的未读数可以使用上节课讲到的通用计数方案来解决；        2.       在系统通知未读、全量用户打点等存在有限的共享存储的场景下，可以通过记录用户上次操作的时间或者偏移量，来实现未读方案；        3.       最后，信息流未读方案最为复杂，采用的是记录用户博文数快照的方式。        这里你可以看到，这三类需求虽然都和未读数有关，但是需求场景不同、对于量级的要求不同，设计出来的方案也就不同。因此，就像我刚刚提到的样子，你在做方案设计的时候，要分析需求的场景，比如说数据的量级是怎样的，请求的量级是怎样的，有没有一些可以利用的特点（比如系统通知未读场景下的有限共享存储、信息流未读场景下关注人数是有限的等等），然后再制定针对性的方案，切忌盲目使用之前的经验套用不同的场景，否则就可能造成性能的下降，甚至危害系统的稳定性。 一课一思结合实际项目聊一聊在你的系统中有哪些未读计数的场景呢？你是如何设计方案来实现未读计数的呢？欢迎在留言区与我分享你的经验。 最后，感谢你的阅读，如果这篇文章让你有所收获，也欢迎你将它分享给更多的朋友。 
# 39 \| 信息流设计（一）：通用信息流系统的推模式要如何做？你好，我是唐扬。 前两节课中，我带你探究了如何设计和实现互联网系统中一个常见模块------计数系统。它的业务逻辑其实非常简单，基本上最多只有三个接口，获取计数、增加计数和重置计数。所以我们在考虑方案的时候考察点也相对较少，基本上使用缓存就可以实现一个兼顾性能、可用性和鲁棒性的方案了。然而大型业务系统的逻辑会非常复杂，在方案设计时通常需要灵活运用多种技术，才能共同承担高并发大流量的冲击。那么接下来，我将带你了解如何设计社区系统中最为复杂、并发量也最高的信息流系统。这样，你可以从中体会怎么应用之前学习的组件了。 最早的信息流系统起源于微博，我们知道，微博是基于关注关系来实现内容分发的，也就是说，如果用户A 关注了用户 B，那么用户 A 就需要在自己的信息流中，实时地看到用户 B发布的最新内容，**这是微博系统的基本逻辑，也是它能够让信息快速流通、快速传播的关键。**由于微博的信息流一般是按照时间倒序排列的，所以我们通常把信息流系统称为TimeLine（时间线）。那么当我们设计一套信息流系统时需要考虑哪些点呢？ 设计信息流系统的关注点有哪些首先，我们需要关注延迟数据，也就是说，你关注的人发了微博信息之后，信息需要在短时间之内出现在你的信息流中。 其次，我们需要考虑如何支撑高并发的访问。信息流是微博的主体模块，是用户进入到微博之后最先看到的模块，因此它的并发请求量是最高的，可以达到每秒几十万次请求。 最后，信息流拉取性能直接影响用户的使用体验。微博信息流系统中需要聚合的数据非常多，你打开客户端看一看，想一想其中需要聚合哪些数据？主要是微博的数据，用户的数据，除此之外，还需要查询微博是否被赞、评论点赞转发的计数、是否被关注拉黑等等。聚合这么多的数据就需要查询多次缓存、数据库、计数器，而在每秒几十万次的请求下，如何保证在100ms之内完成这些查询操作，展示微博的信息流呢？这是微博信息流系统最复杂之处，也是技术上最大的挑战。 那么我们怎么设计一套支撑高并发大流量的信息流系统呢？一般来说，会有两个思路：一个是基于推模式，另一个是基于拉模式。 如何基于推模式实现信息流系统什么是推模式呢？推模式是指用户发送一条微博后，主动将这条微博推送给他的粉丝，从而实现微博的分发，也能以此实现微博信息流的聚合。 假设微博系统是一个邮箱系统，那么用户发送的微博可以认为是进入到一个发件箱，用户的信息流可以认为是这个人的收件箱。推模式的做法是在用户发布一条微博时，除了往自己的发件箱里写入一条微博，同时也会给他的粉丝收件箱里写入一条微博。 假如用户 A 有三个粉丝 B、C、D，如果用 SQL 表示 A发布一条微博时系统做的事情，那么就像下面展示的这个样子：     insert into outbox(userId, feedId, create_time) values("A", $feedId, $current_time); // 写入 A 的发件箱    insert into inbox(userId, feedId, create_time) values("B", $feedId, $current_time); // 写入 B 的收件箱    insert into inbox(userId, feedId, create_time) values("C", $feedId, $current_time); // 写入 C 的收件箱    insert into inbox(userId, feedId, create_time) values("D", $feedId, $current_time); // 写入 D 的收件箱当我们要查询 B 的信息流时，只需要执行下面这条 SQL就可以了：     select feedId from inbox where userId = "B";如果你想要提升读取信息流的性能，可以把收件箱的数据存储在缓存里面，每次获取信息流的时候直接从缓存中读取就好了。 推模式存在的问题和解决思路你看，按照这个思路就可以实现一套完整的微博信息流系统，也比较符合我们的常识。但是，这个方案会存在一些问题。 首先，就是消息延迟。在讲系统通知未读数的时候，我们曾经提到过，不能采用遍历全量用户给他们加未读数的方式，原因是遍历一次全量用户的延迟很高，而推模式也有同样的问题。对明星来说，他们的粉丝数庞大，如果在发微博的同时还要将微博写入到上千万人的收件箱中，那么发微博的响应时间会非常慢，用户根本没办法接受。因此，我们一般会使用消息队列来消除写入的峰值，但即使这样，由于写入收件箱的消息实在太多，你还是有可能在几个小时之后才能够看到明星发布的内容，这会非常影响用户的使用体验。 ![](Images/35149b4c6d13602a1ad2f35829ba8595.png)savepage-src="https://static001.geekbang.org/resource/image/c2/b0/c2e64231a2b6c52082567f8422069cb0.jpg"}在推模式下，你需要关注的是微博的写入性能，因为用户每发一条微博，都会产生多次的数据库写入。为了尽量减少微博写入的延迟，我们可以从两方面来保障。 1.  一方面，在消息处理上，你可以启动多个线程并行地处理微博写入的消息。        2.  另一方面，由于消息流在展示时可以使用缓存来提升读取性能，所以我们应该尽量保证数据写入数据库的性能，必要时可以采用写入性能更好的数据库存储引擎。        比如，我在网易微博的时候就是采用推模式来实现微博信息流的。当时为了提升数据库的插入性能，我们采用了TokuDB 作为 MySQL的存储引擎，这个引擎架构的核心是一个名为分形树的索引结构（Fractal TreeIndexes）。我们知道数据库在写入的时候会产生对磁盘的随机写入，造成磁盘寻道，影响数据写入的性能；而分形树结构和我们在11 讲中提到的 LSM一样，可以将数据的随机写入转换成顺序写入，提升写入的性能。另外，TokuDB相比于 InnoDB 来说，数据压缩的性能更高，经过官方的测试，TokuDB可以将存储在 InnoDB 中的 4TB 的数据压缩到200G，这对于写入数据量很大的业务来说也是一大福音。然而，相比于 InnoDB来说，TokuDB的删除和查询性能都要差一些，不过可以使用缓存加速查询性能，而微博的删除频率不高，因此这对于推模式下的消息流来说影响有限。 其次，存储成本很高。**在这个方案中我们一般会这么来设计表结构：** 先设计一张 Feed 表，这个表主要存储微博的基本信息，包括微博ID、创建人的ID、创建时间、微博内容、微博状态（删除还是正常）等等，它使用微博 ID做哈希分库分表； 另外一张表是用户的发件箱和收件箱表，也叫做 TimeLine表（时间线表），主要有三个字段，用户 ID、微博 ID和创建时间。它使用用户的 ID做哈希分库分表。 ![](Images/cdfd5f365991685127da9b821e713f79.png)savepage-src="https://static001.geekbang.org/resource/image/71/6c/71b4b33d966a7e34a62f635a1a23646c.jpg"}由于推模式需要给每一个用户都维护一份收件箱的数据，所以数据的存储量极大，你可以想一想，谢娜的粉丝目前已经超过1.2 亿，那么如果采用推模式的话，谢娜每发送一条微博就会产生超过 1.2亿条的数据，多么可怕！**我们的解决思路是：**除了选择压缩率更高的存储引擎之外，还可以定期地清理数据，因为微博的数据有比较明显的实效性，用户更加关注最近几天发布的数据，通常不会翻阅很久之前的微博，所以你可以定期地清理用户的收件箱，比如只保留最近1 个月的数据就可以了。 除此之外，推模式下我们还通常会遇到扩展性的问题。在微博中有一个分组的功能，它的作用是你可以将关注的人分门别类，比如你可以把关注的人分为"明星""技术""旅游"等类别，然后把杨幂放入"明星"分类里，将InfoQ 放在"技术"类别里。**那么引入了分组之后，会对推模式有什么样的影响呢？**首先是一个用户不止有一个收件箱，比如我有一个全局收件箱，还会针对每一个分组再分别创建一个收件箱，而一条微博在发布之后也需要被复制到更多的收件箱中了。 如果杨幂发了一条微博，那么不仅需要插入到我的收件箱中，还需要插入到我的"明星"收件箱中，这样不仅增加了消息分发的压力，同时由于每一个收件箱都需要单独存储，所以存储成本也就更高。 最后，在处理取消关注和删除微博的逻辑时会更加复杂。比如当杨幂删除了一条微博，那么如果要删除她所有粉丝收件箱中的这条微博，会带来额外的分发压力，我们还是尽量不要这么做。 而如果你将一个人取消关注，那么需要从你的收件箱中删除这个人的所有微博，假设他发了非常多的微博，那么即使你之后很久不登录，也需要从你的收件箱中做大量的删除操作，有些得不偿失。**所以你可以采用的策略是：**在读取自己信息流的时候，判断每一条微博是否被删除以及你是否还关注这条微博的作者，如果没有的话，就不展示这条微博的内容了。使用了这个策略之后，就可以尽量减少对于数据库多余的写操作了。 **那么说了这么多，推模式究竟适合什么样的业务的场景呢？**在我看来，它比较适合于一个用户的粉丝数比较有限的场景，比如说微信朋友圈，你可以理解为我在微信中增加一个好友是关注了他也被他关注，所以好友的上限也就是粉丝的上限（朋友圈应该是5000）。有限的粉丝数可以保证消息能够尽量快地被推送给所有的粉丝，增加的存储成本也比较有限。如果你的业务中粉丝数是有限制的，那么在实现以关注关系为基础的信息流时，也可以采用推模式来实现。 课程小结以上就是本节课的全部内容了。本节课我带你了解以推模式实现信息流的方案以及这个模式会存在哪些问题和解决思路，这里你需要了解的重点是： 1.       推模式就是在用户发送微博时，主动将微博写入到他的粉丝的收件箱中；        2.       推送信息是否延迟、存储的成本、方案的可扩展性以及针对取消关注和微博删除的特殊处理是推模式的主要问题；        3.       推模式比较适合粉丝数有限的场景。        你可以看到，其实推模式并不适合微博这种动辄就有上千万粉丝的业务，因为这种业务特性带来的超高的推送消息延迟以及存储成本是难以接受的，因此，我们要么会使用基于拉模式的实现，要么会使用基于推拉结合模式的实现。那么这两种方案是如何实现的呢？他们在实现中会存在哪些坑呢？又要如何解决呢？我将在下节课中带你着重了解。 一课一思你是否设计过这种信息流系统呢？如果你来设计的话，要如何解决推模式下的延迟问题呢？欢迎在留言区与我分享你的经验。 最后，感谢你的阅读，如果这篇文章让你有所收获，也欢迎你将它分享给更多的朋友。 