# Evaluating the Impact of Undetected Disk Errors in RAID Systems

**Authors:**
- Eric W. D. Rozier<sup>1,2</sup>
- Wendy Belluomini<sup>1</sup>
- Veera Deenadhayalan<sup>1</sup>
- Jim Hafner<sup>1</sup>
- K. K. Rao<sup>1</sup>
- Pin Zhou<sup>1</sup>

**Affiliations:**
- IBM Almaden Research Center<sup>1</sup>
- University of Illinois Department of Computer Science<sup>2</sup>

**Emails:**
- PI:EMAIL
- {wb1, veerad, hafner, kkrao, pinzhou}@us.ibm.com

## Abstract
Despite the reliability of modern disks, recent studies have highlighted a new class of faults known as Undetected Disk Errors (UDEs) or silent data corruption events. These errors pose a significant challenge as storage capacity scales. While RAID systems effectively protect against traditional disk failures, they do not address UDEs. This paper presents a fault model for UDEs and a hybrid framework for simulating UDEs in large-scale systems. The framework combines a multi-resolution discrete event simulator with numerical solvers, enabling the modeling of arbitrary storage systems and workloads to estimate the rate of undetected data corruptions. Results from simulations of gigascale to petascale systems indicate that UDEs are a significant problem in the absence of protection schemes, and such schemes can dramatically reduce the rate of undetected data corruption.

**Keywords:** silent data corruption, undetected disk errors, simulation, modeling

## 1. Introduction
Despite the reliability of modern disks, recent studies have shown that a new class of faults, Undetected Disk Errors (UDEs), pose a real challenge to storage systems as capacity scales [2, 11, 7]. While RAID systems are effective in protecting data from traditional failure modes [13], they do not address these silent data corruption events [2].

UDEs can be categorized into two types: Undetected Read Errors (UREs) and Undetected Write Errors (UWEs). UREs are transient and unlikely to affect system state beyond their occurrence. UWEs, on the other hand, are persistent and only detectable during a subsequent read operation, manifesting similarly to UREs [7]. Metrics to quantify the occurrence of data corruption due to UDEs have not been well-established, as these events were previously considered rare. However, recent capacity scaling has made UDEs more common, necessitating further study on their occurrence rates and user-perceived manifestations.

While techniques for mitigating UDEs have been suggested in the literature [2, 7], many of these techniques may not prevent UDE-induced faults from manifesting as errors [11]. Given the low rate of UDE occurrence, testing these techniques in real systems would be costly, requiring a large array of disks to observe UDEs in sufficient quantities. Therefore, there is a need for modeling tools to analyze the risk posed by UDEs and the effectiveness of mitigation techniques. A simple analytical model may fail to capture important emergent trends, while a customizable simulation approach can effectively model UDE behavior for arbitrary workloads.

Simulating UDEs in large-scale systems is challenging due to the stiffness of the system, which involves events on vastly different timescales. To fully understand UDEs, one must model disks at a fine level of detail, tracking individual block read and write information, the propagation of UWEs, and the effectiveness of mitigation techniques. This requires considering events from fast block-level I/Os (approximately 100 I/Os/sec) to rare UDEs (approximately 10<sup>-12</sup> UDEs/I/O).

In this paper, we present an extensible hybrid framework that combines discrete event simulation with numerical analysis. Our methods use dependency relationships within the I/O stream to switch between numerical methods, block-level discrete event simulation, and a hybridized numerical model. This approach achieves more efficient use of time and space than discrete event simulation alone. Our simulator takes a model of a storage system and workload, producing an estimate of the rate at which UDEs manifest as corruption at the user level. The simulator allows users to build component-level models, extend components, and test under arbitrary workloads and UDE rates. Mitigation techniques can be easily implemented by extending the base classes for disks.

The paper is organized as follows: We first discuss UDEs, their origin, effects, and mitigation techniques. We then derive a model for UDEs for use in our simulation, including determining appropriate rates for various types of UDEs. Next, we describe the creation of workload and system models for the hybrid simulator and its operation. Finally, we present results illustrating the problems posed by UDEs as systems scale and the effectiveness of mitigation techniques in preventing UDE-induced faults from manifesting as undetected data corruption events.

## 2. Undetected Disk Errors
A growing concern in the storage community is the presence of errors for which RAID does not provide adequate protection. Schroeder and Gibson note that despite a supposed Mean Time To Failure (MTTF) for drives ranging from 1,000,000 to 1,500,000 hours, field data suggest that MTTF is, in practice, much lower [15]. Bairavasundaram et al. documented data corruption events for over 1.53 million disks in production storage systems, illustrating the existence of several types of rare faults that manifest as corrupt data blocks [1]. These errors were detected by additional detection mechanisms implemented at the filesystem layer. We refer to these silent data corruption events as UDEs [7].

### 2.1 Types of UDEs
UDEs can be divided into two primary categories: UWEs and UREs [7]. UREs are transient, while UWEs result in a changed system state that can cause subsequent reads to return corrupt data. Table 1 summarizes the primary types of UDEs and how they manifest as actual errors on the disk [7].

| **I/O Type** | **UDE Type** | **Manifestation** |
|-------------|--------------|-------------------|
| Write (UWE) | Dropped Write | Stale data |
|             | Near off-track write | Possible stale data on read |
|             | Far off-track write | Stale data on intended track, corrupt data on written track |
| Read (URE)  | Near off-track read | Possible stale data |
|             | Far off-track read | Corrupt data |

Dropped writes occur when the write head fails to overwrite existing data, leaving the disk in its previous state. Off-track writes, both near and far, occur when the write head is not properly aligned with the track. Near off-track writes result in data written in the gap between tracks, potentially producing stale data on reads. Far off-track writes corrupt data in another track entirely, causing subsequent reads to produce corrupt data. It is important to note that not all UWEs introduced as disk errors manifest as user-level undetected data corruption. A subsequent good write to the affected track will remove the error, and near off-track writes may cause future reads to randomly return either good or stale data. Thus, a one-to-one correspondence does not exist between UDEs and user-level undetected data corruption errors. We will investigate the rate of UDE manifestation as undetected data corruption errors in Section 4.

### 2.2 UDEs in RAID Storage Systems
High-reliability systems often use RAID storage. While little study has been done on the effect of UDEs in RAID systems, it has been shown that data scrubbing, the normal disk error mitigation and detection technique, is not sufficient to protect against all UDEs [11, 7]. Even under RAID6, the most powerful RAID technique in common usage, a data scrub may incorrectly assess the integrity of data due to a UDE [7].

### 2.3 Mitigation Techniques
Despite the threats posed by UDEs, production systems rarely implement detection and mitigation strategies [6]. One proposed method to mitigate UDEs in RAID is the parity appendix method [7]. In this method, metadata is co-located with some or all blocks associated with a chunk of data and its parity blocks. Our model focuses on a data parity appendix method [4, 7], which uses the metadata portion of a block to store a sequence number. This sequence number is the same for all blocks in a write. UDEs can be detected by comparing the sequence numbers stored in the parity and data blocks. Sequence numbers won't match when a UDE manifests as an error unless a collision occurs, with a probability of \( P(\text{Seq(Parity)} = \text{Seq(UDE)}) = \frac{1}{2^b} \), where \( b \) is the number of bits allocated for the sequence number. On a read, the sequence number for each data block is retrieved from parity and compared, allowing this technique to mitigate UDE manifestations, at the cost of an extra read.

### 2.4 Modeling UDEs
Storage system faults arise from a combination of hardware, software, and firmware malfunctions. For well-studied faults like latent sector errors, complex models have been derived to capture detailed relationships and correlations [1, 15]. However, for UDEs, there is limited information on their occurrence rates and spatial and temporal relationships. We model UDEs with exponential interfailure times, making no assumptions about temporal and spatial locality. Our simulator is flexible and can be adapted to different UDE interarrival time distributions and locality measures.

To estimate UDE occurrence rates, we use data from [1] and [2]. In [1], a study of latent sector errors was presented for the same set of field data used to derive UDE statistics in [2]. Since latent sector errors are hard errors with manufacturer-provided rates, we can approximate the workload by back-calculating from the observed hard error rates. We assume a maximum I/O rate of 100 I/Os/s for nearline drives and 200 I/Os/s for enterprise drives, a system block size of 4k, a read ratio of 60%, and high and low utilization periods. By setting the high utilization to 50% for 10 hours per day and the light utilization to 10%, we predict latent sector errors that align reasonably closely with observed values. This validates our assumptions and provides a representative workload for estimating UDE rates.