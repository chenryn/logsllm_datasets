title:Evaluating the impact of Undetected Disk Errors in RAID systems
author:Eric Rozier and
Wendy Belluomini and
Veera Deenadhayalan and
Jim Hafner and
K. K. Rao and
Pin Zhou
Evaluating the Impact of Undetected Disk Errors in RAID Systems
Eric W. D. Rozier1
,
2, Wendy Belluomini1, Veera Deenadhayalan1, Jim Hafner1, KK Rao1, Pin Zhou1
IBM Almaden Research Center1, University of Illinois Department of Computer Science2
Email: PI:EMAIL, {wb1, veerad, hafner, kkrao, pinzhou}@us.ibm.com
Abstract
Despite the reliability of modern disks, recent studies have
made it clear that a new class of faults, Undetected Disk Errors
(UDEs) also known as silent data corruption events, become
a real challenge as storage capacity scales. While RAID sys-
tems have proven effective in protecting data from traditional
disk failures, silent data corruption events remain a signiﬁcant
problem unaddressed by RAID.
We present a fault model for UDEs, and a hybrid framework
for simulating UDEs in large-scale systems. The framework
combines a multi-resolution discrete event simulator with nu-
merical solvers. Our implementation enables us to model ar-
bitrary storage systems and workloads and estimate the rate
of undetected data corruptions. We present results for several
systems and workloads, from gigascale to petascale. These
results indicate that corruption from UDEs is a signiﬁcant
problem in the absence of protection schemes and that such
schemes dramatically decrease the rate of undetected data cor-
ruption.
Keywords: silent data corruption, undetected disk errors, sim-
ulation, modeling
1. Introduction
Despite the reliability of modern disks, recent studies have
made it increasingly clear that a new class of faults, that of
undetected disk errors (UDEs) are a real challenge facing stor-
age systems as capacity scales [2, 11, 7]. While RAID sys-
tems have proven quite effective in protecting data from tra-
ditional failure modes [13], these new silent data corruption
events are a signiﬁcant problem unaddressed by RAID [2].
UDE faults themselves are drawn from two distinct classes,
undetected read errors (UREs) and undetected write errors
(UWEs). UREs manifest as transient errors, and are unlikely
to effect system state beyond their occurrence. UWEs, on the
other hand, are persistent errors which are only detectable dur-
ing a read operation subsequent to the faulty write, and thus
manifest in a similar manner to a URE [7]. Metrics to quan-
tify the occurrence of data corruption due to UDEs have not
been presented before since these events have been deemed to
be very rare. Recently capacity scaling has made UDEs com-
mon enough to be a concern and drives further study both on
the rate of occurance of these faults, and their manifestation as
errors from a user perspective.
While both evidence of UDEs and suggested techniques for
their mitigation have been outlined in the literature [2, 7], it
is clear that there are several situations where many of these
techniques will not be able to prevent UDE induced faults from
manifesting as errors [11]. Given that the rate of UDE occur-
rence is low, testing these techniques in a real system would
be costly, likely requiring a prohibitively large array of disks
to witness UDEs in large enough quantites to derive measures
of their effects within a reasonable period of time. A need
therefor exists for modeling tools which address the particu-
lar challenges presented by such systems. A model provides
the capability to analyze the risk posed by UDEs in a given
system and the fault tolerance coverage provided by suggested
mitigation techniques. A simple analytical model could fail
to account for important emergent trends in UDE manifesta-
tion for a given workload. For example a block that has had
a UWE error must be read before that fault manifests as data
corruption and therefore the chances of system failure depend
on the I/O stream in question. A simulation approach that is
customizable and can determine the effect of UDEs for an abi-
trary workload is an effective way to capture this behavior.
Efﬁcient simulation of UDE faults in large scale systems
proves to be a serious challenge, however, due to the stiffness
of the system. A system is said to be stiff when events oc-
curring on vastly different timescales must be simulated in or-
der to capture the behavior one wishes to model. In order to
fully understand the effect of UDEs and how they manifest in
a given storage system, one must model the disks at a ﬁne level
of detail, looking at individual block read and write informa-
tion in order to track portions of the disk which are suffering
from a UDE, the propagation of UWEs due to normal opera-
tions within the RAID system (including rebuild events), and
the effectiveness of mitigation techniques to prevent a UDE
based fault from manifesting as an error. This means any
model of such processes will necessarily consider events on
two very different scales, from the very fast block level I/Os
(with a rate of roughly 100 ios/sec), to the much rarer UDEs
(with a rate of roughly 10−12 UDEs/io).
In order to satisfy these requirements we present in this pa-
per an extensible hybrid framework for simulating large scale
storage systems that combines discrete event simulation on
multiple levels of resolution with numerical analysis in a hy-
bridized fashion. Our methods take advantage of dependency
relationships within a the I/O stream to temporally switch be-
tween numerical methods, a block level discrete event simu-
lator, and a hybridized numerical model with some discrete
events simulated. Our techniques achieve a more efﬁcient use
of time and space than discrete event simulation alone. These
methods have been implemented in a simulator which takes as
input a model of a storage system and a model of a workload,
and produces as output an estimate of the rate at which UDEs
manifest as corruption at the user level. Our simulator has
been designed in a way that allows the user to build component
level models of storage systems, extend components to create
new behavior, and test under arbitrary workloads and rates of
UDEs. Mitigation techniques can be easily implemented by
extending the implemented base classes for disks.
The paper is organized as follows: We will ﬁrst discuss
UDEs, their origin in real systems, their effects, and tech-
niques for mitigating UDEs provided by the literature. We will
then derive a model for UDEs for use in our simulation, this
derivation includes determining appropriate rates for the var-
ious types of UDEs, as these rates have yet to be determined
in the literature. Next we will discuss how we create workload
and system models models for a novel hybrid simulator and the
operation of the simulator itself. Finally, we present our results
which illustrate the problems posed by UDEs as systems scale,
and the effectiveness of even simple mitigation techniques in
preventing the faults injected by UDEs from manifesting as
undetected data corruption events.
2. Undetected Disk Errors
A growing concern in the storage community has been er-
rors for which RAID [13, 14] does not provide adequate pro-
tection. Schroeder and Gibson note that despite a supposed
MTTF for drives ranging from 1,000,000 to 1,500,000 hours
ﬁeld data suggest that MTTF is, in practice, much lower [15].
By analyzing data corruption events for over 1.53 million disks
in production storage systems Bairavasundaram et al. doc-
umented cases of data corruption events that occurred in a
41 month period. This work illustrates the existence of sev-
eral types of rare faults which manifest as corrupt data blocks.
These errors were detected by their production system due to
additional detection mechanisms implemented at the ﬁlesys-
tem layer by their system. We refer to these silent data cor-
ruption events [10, 14] as undetected disk errors (UDEs) [7].
UDEs differ from seemingly similar faults, such as Latent Sec-
tor Errors (LSEs), due to the fact that unlike other failure
modes when a UDE occurs the drive reports the operation oc-
cured appropriately and the state of the drive is as expected.
Unless detected through explicit means in the I/O stack a UDE
has the potential to result in incorrect data being served to user
level processes.
2.1 Introduction
UDEs can be divided into two primary categories, unde-
tected write errors (UWEs) and undetected read errors (UREs)
[7], and arise from a variety of factors, including software,
hardware and ﬁrmware malfunctions. The primary difference
between UWEs and UREs is that UREs are transient in nature,
while UWEs result in a changed system state that can cause
many subsequent reads to return corrupt data. Table 1 sum-
marizes the primary types of UDEs, and how they manifest as
actual errors on the disk [7].
Dropped writes occur when the write head fails to over-
write the data already present on a track. In this case the disk
remains in its previous state as if the write never occured [7].
Off-track writes (also referred to as misdirected writes)
come in two categories, near and far off-track. Both types oc-
cur when the write head is not properly aligned with the track.
In the case of a near off-track write, the data is written in the
gap between tracks, adjacent to the intended track. On a read
operation, the head may align itself to either the target track,
or the off-track, potentially producing stale data. Far off-track
writes occur when data is written even further off-track, such
that it corrupts data in another track entirely. Subsequent reads
to the track which was mistakenly written will produce corrupt
data, while reads to the track which was intended to be written
will produce stale data. [7]
It is important to note, however, that not all UWEs intro-
duced disk errors manifest as a user level undetected data cor-
ruption error. A subsequent (good) write to the affected track
will remove the error, preventing undetected data corruption.
Likewise a near off-track write will cause future reads to ran-
domly return either good or stale data. Though a far off-track
write results in both stale data on the intended track, and cor-
rupt data on the unintended track, if the unintended track is not
currently in use, that part of the effect will be mitigated. Like-
wise, a UWE may manifest as several undetected data corrup-
tion events if the same track is read multiple times before it is
corrected by a subsequent write operation. Thus a one-to-one
correspondance does not exist between UDEs and user level
undetected data corruption errors. We will investigate the rate
of UDE manifestation as undetected data corruption errors in
Section 4.
2.2 UDEs in RAID Storage Systems
Systems designed for high reliability often make use of
RAID storage systems. While there has been little study of
the effect of UDEs in RAID systems, it has been shown that
data scrubbing, the normal disk error mitigation and detection
technique in such systems, is not sufﬁcent to protect against all
UDEs [11, 7]. Even under RAID6, the most powerful RAID
technique in common usage, a data scrub may incorrectly as-
sess the integrity of data due to a UDE [7].
2.3 Mitigation
Despite the threats to data integrity and storage system reli-
ability posed by UDEs, production systems rarely implement
detection and mitigation strategies [6]. Methods have, how-
ever, been proposed, if not evaluated in real systems. One
family of proposed methods to mitigate the effect of UDEs
in RAID are of the form of parity appendix methods [7]. In
these methods, metadata is co-located with some or all of the
blocks associated with a chunk of data and its parity blocks. In
Table 1. Summary of UDE types and manifestations
I/O Type
Write (UWE) Dropped Write
UDE type
Near off-track write
Far off-track write
Read (URE)
Near off-track read
Far off-track read
Manifestation
Stale data
Possible stale data on read
Stale data on intended track
Corrupt data on written track
Possible stale data
Corrupt data
our models we focus on a data parity appendix method [4, 7],
which utilizes the metadata portion of a block to store a se-
quence number. This sequence number is the same for all
blocks in the write. UDEs can be detected with this method
by comparing the sequence numbers stored in the parity and
data blocks. The sequence numbers won’t match when a UDE
manifests as an error unless a collision occurs in the sequence
number with probability
P (Seq(Parity) = Seq(UDE)) =
1
2b
where b is the number of bits allocated for the sequence num-
ber. On a read, the sequence number for each data block is
retrieved from parity and compared, allowing this technique to
mitigate UDE manifestations, at the cost of an extra read.
We model sequence numbers in our study by storing each
sequence number with the block state information in our model
as an 8-bit number, representing the actual sequence number
for the block. Sequence numbers are drawn uniformly from a
discrete Uniform distribution [12] [0, 255]. On a far off-track
write, or dropped write, the original target blocks have their se-
quence numbers unchanged. The new target of a far off-track
write will get the sequence number intended for the original
target. near off-track writes result in two sequence numbers
for a given block, one for each sub track. Sequence number
mismatches are handled, by marking the affected blocks as un-
readable.
2.4 Modeling UDEs
Storage systems faults are complex events, arising from a
combination of hardware, software, and ﬁrmware malfunc-
tions. In the case of well studied faults, such as latent sector
errors, and complex models have been derived to capture de-
tailed relationships and correlations. Bairavasundaram et al.,
when designing a model for latent sector errors, capture tem-
poral and spatial locality measures [1]. Schroeder and Gib-
son in their study of MTTF propose and ﬁt both Weibull and
Gamma distributions, obtaining better results than with ex-
ponentially distributed interfailure times [15]. Unfortunately
when it comes to UDEs, there is very little information on their
rates of occurance, let alone enough data to ﬁt multiparameter
distributions, or ascertain spatial and temporal relationships on
their failure. As such, we choose to model UDEs in our study
with exponential interfailure times, and make no assumptions
with respect to temporal and spatial locality. While we make
this assumption about the distribution of UDEs, our method-
ology is not contingent on this assumption. Our simulator has
been designed to be ﬂexable about the distribution of UDE in-
terarrival times, and temporal and spatial locality. Given ad-
ditional data on UDEs we hope to reﬁne these assumptions in
the future.
Rates for the occurance of UDEs have not yet been pub-
lished, as such we make a ﬁrst attempt in this paper to derive
rates of UDEs from published materials. We can estimate the
rates of UDE occurrence by using a combination of the data
presented in [1] and the data presented in [2]. In [1], a study of
latent sector errors was presented for the same set of ﬁeld data
that was used to derive the statistics on UDEs in [2]. Since
latent sector errors are hard errors, which have manufacturer
provided rates, we can approximate the workload run on the
ﬁeld systems by back calculating from the rate of hard errors
observed.
We start this calculation by making some observations and
assumptions about usage patterns. Nearline drives have a max-
imum I/O rate of 100 IO/s; enterprise drives have a maximum
I/O rate of 200 IO/s. For the purposes of this calculation, we
will assume a system block size of 4k, and thus that each IO is
4k. We assume that the read ratio is 60% (i.e. reads are 60%
of the IO stream and writes are 40%). We also assume that the
drives are used at high utilization some number of hours in the
day and at low utilization the rest of the time. Nearline drives
have a hard error rate of 1 uncorrectable sector for every 1014
bits read and the corresponding rate for enterprise drives is 1
uncorrectable sector for every 1015 bits read. We treat drives
with up to 10 sector errors as having 10 individual occurrences
of sector errors. The spatial locality analysis in [1] appears
to validate this assumption. If a drive has more than 10 sec-
tor errors (some have hundreds of errors), we view these as
manifestations of the same occurrence (for example a scratch
along a track taking out several consecutive sectors) and ig-
nore them. We focus on drives used for the 24-month period
in described in [1]. During this period, there are 68,380 near-
line drives with a total of 25,827 latent sector errors (using the
above ﬁlter). There are 264,939 enterprise drives with a total of
13,989 latent sector errors. To reverse engineer the workload
parameters, we adjust the high and low utilization rates and
hours until the predicted latent sectors errors align reasonably
closely with the observed values. By setting the high utiliza-
tion to 50% for 10 hours per day, and the light utilization to
10%, we predict 22,302 latent sector errors for nearline drives
and 17,282 latent sector errors for enterprise drives. We val-
idate these assumptions by comparing these calculated latent
sector error numbers to those reported in [1]. The above num-
bers are close enough to what has been reported in [1] (25,927
for nearline and 13,989 for enterprise) that we believe that our
calculations represent a reasonable estimation of the workload.
Now that we have a representative workload, we can de-