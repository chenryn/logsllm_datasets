row-buffer (called a row access). Then, a column ac-
cess is performed. We mention this third case for the
sake of completeness because in the paper, we focus
primarily on row hits and row conﬂicts, which have
the largest impact on our results.
Due to the nature of DRAM bank organization, sequen-
tial accesses to the same row in the bank have low latency
and can be serviced at a faster rate. However, sequen-
tial accesses to different rows in the same bank result in
high latency. Therefore, to maximize bandwidth, current
DRAM controllers schedule accesses to the same row in
a bank before scheduling the accesses to a different row
even if those were generated earlier in time. We will later
show how this policy causes unfairness in the DRAM
system and makes the system vulnerable to DoS attacks.
2.2.2 DRAM Controller
The DRAM controller is the mediator between the on-
chip caches and the off-chip DRAM memory.
It re-
ceives read/write requests from L2 caches. The addresses
of these requests are at the granularity of the L2 cache
block. Figure 2 (right) shows the architecture of the
DRAM controller. The main components of the con-
troller are the memory request buffer and the memory ac-
cess scheduler.
The memory request buffer buffers the requests re-
ceived for each bank. It consists of separate bank request
260
16th USENIX Security Symposium
USENIX Association
buffers. Each entry in a bank request buffer contains the
address (row and column), the type (read or write), the
timestamp, and the state of the request along with stor-
age for the data associated with the request.
The memory access scheduler is the brain of the mem-
ory controller. Its main function is to select a memory
request from the memory request buffer to be sent to
DRAM memory.
It has a two-level hierarchical orga-
nization as shown in Figure 2. The ﬁrst level consists of
separate per-bank schedulers. Each bank scheduler keeps
track of the state of the bank and selects the highest-
priority request from its bank request buffer. The second
level consists of an across-bank scheduler that selects the
highest-priority request among all the requests selected
by the bank schedulers. When a request is scheduled by
the memory access scheduler, its state is updated in the
bank request buffer, and it is removed from the buffer
when the request is served by the bank (For simplicity,
these control paths are not shown in Figure 2).
2.2.3 Memory Access Scheduling Algorithm
Current memory access schedulers are designed to max-
imize the bandwidth obtained from the DRAM memory.
As shown in [30], a simple request scheduling algorithm
that serves requests based on a ﬁrst-come-ﬁrst-serve pol-
icy is prohibitive, because it incurs a large number of
row conﬂicts. Instead, current memory access schedulers
usually employ what is called a First-Ready First-Come-
First-Serve (FR-FCFS) algorithm to select which request
should be scheduled next [30, 23]. This algorithm prior-
itizes requests in the following order in a bank:
1. Row-hit-ﬁrst: A bank scheduler gives higher prior-
ity to the requests that would be serviced faster. In
other words, a request that would result in a row hit
is prioritized over one that would cause a row con-
ﬂict.
2. Oldest-within-bank-ﬁrst: A bank scheduler gives
higher priority to the request that arrived earliest.
Selection from the requests chosen by the bank sched-
ulers is done as follows:
Oldest-across-banks-ﬁrst: The across-bank DRAM
bus scheduler selects the request with the earliest arrival
time among all the requests selected by individual bank
schedulers.
In summary, this algorithm strives to maximize DRAM
bandwidth by scheduling accesses that cause row hits
ﬁrst (regardless of when these requests have arrived)
within a bank. Hence, streaming memory access patterns
are prioritized within the memory system. The oldest
row-hit request has the highest priority in the memory
access scheduler. In contrast, the youngest row-conﬂict
request has the lowest priority.
2.3 Vulnerability of the Multi-Core DRAM
Memory System to DoS Attacks
As described above, current DRAM memory systems do
not distinguish between the requests of different threads
(i.e. cores)4. Therefore, multi-core systems are vulnera-
ble to DoS attacks that exploit unfairness in the memory
system. Requests from a thread with a particular access
pattern can get prioritized by the memory access sched-
uler over requests from other threads, thereby causing
the other threads to experience very long delays. We ﬁnd
that there are two major reasons why one thread can deny
service to another in current DRAM memory systems:
1. Unfairness of row-hit-ﬁrst scheduling: A thread
whose accesses result in row hits gets higher priority
compared to a thread whose accesses result in row
conﬂicts. We call an access pattern that mainly re-
sults in row hits as a pattern with high row-buffer lo-
cality. Thus, an application that has a high row-buffer
locality (e.g. one that is streaming through memory)
can signiﬁcantly delay another application with low
row-buffer locality if they happen to be accessing the
same DRAM banks.
2. Unfairness of oldest-ﬁrst scheduling: Oldest-ﬁrst
scheduling implicitly gives higher priority to those
threads that can generate memory requests at a faster
rate than others. Such aggressive threads can ﬂood
the memory system with requests at a faster rate than
the memory system can service. As such, aggres-
sive threads can ﬁll the memory system’s buffers with
their requests, while less memory-intensive threads
are blocked from the memory system until all the
earlier-arriving requests from the aggressive threads
are serviced.
Based on this understanding, it is possible to develop a
memory performance hog that effectively denies service
to other threads. In the next section, we describe an ex-
ample MPH and show its impact on another application.
3 Motivation: Examples of Denial of Mem-
ory Service in Existing Multi-Cores
In this section, we present measurements from real sys-
tems to demonstrate that Denial of Memory Service at-
tacks are possible in existing multi-core systems.
3.1 Applications
We consider two applications to motivate the problem.
One is a modiﬁed version of the popular stream bench-
mark [21], an application that streams through memory
and performs operations on two one-dimensional arrays.
The arrays in stream are sized such that they are much
4We assume, without loss of generality, one core can execute one
thread.
USENIX Association
16th USENIX Security Symposium
261
// initialize arrays a, b
for (j=0; j<N; j++)
// initialize arrays a, b
for (j=0; j<N; j++)
index[j] = j;
// streaming index
index[j] = rand();
// random # in [0,N]
for (j=0; j<N; j++)
a[index[j]] = b[index[j]];
for (j=0; j<N; j++)
for (j=0; j<N; j++)
a[index[j]] = b[index[j]];
for (j=0; j<N; j++)
b[index[j]] = scalar * a[index[j]];
b[index[j]] = scalar * a[index[j]];
(a) STREAM
Figure 3: Major loops of the stream (a) and rdarray (b) programs
(b) RDARRAY
larger than the L2 cache on a core. Each array consists of
2.5M 128-byte elements.5 Stream (Figure 3(a)) has very
high row-buffer locality since consecutive cache misses
almost always access the same row (limited only by the
size of the row-buffer). Even though we cannot directly
measure the row-buffer hit rate in our real experimental
system (because hardware does not directly provide this
information), our simulations show that 96% of all mem-
ory requests in stream result in row-hits.
The other application, called rdarray, is almost the ex-
act opposite of stream in terms of its row-buffer locality.
Its pseudo-code is shown in Figure 3(b). Although it per-
forms the same operations on two very large arrays (each
consisting of 2.5M 128-byte elements), rdarray accesses
the arrays in a pseudo-random fashion. The array indices
accessed in each iteration of the benchmark’s main loop
are determined using a pseudo-random number genera-
tor. Consequently, this benchmark has very low row-
buffer locality; the likelihood that any two outstanding
L2 cache misses in the memory request buffer are to the
same row in a bank is low due to the pseudo-random gen-
eration of array indices. Our simulations show that 97%
of all requests in rdarray result in row-conﬂicts.
3.2 Measurements
We ran the two applications alone and together on two
existing multi-core systems and one simulated future
multi-core system.
3.2.1 A Dual-core System
The ﬁrst system we examine is an Intel Pentium D
930 [17] based dual-core system with 2GB SDRAM.
In this system each core has an L2 cache size of 2MB.
Only the DRAM memory system is shared between the
two cores. The operating system is Windows XP Pro-
fessional.6 All the experiments were performed when
5Even though the elements are 128-byte, each iteration of the main
loop operates on only one 4-byte integer in the 128-byte element. We
use 128-byte elements to ensure that consecutive accesses miss in the
cache and exercise the DRAM memory system.
6We also repeated the same experiments in (1) the same system with
the RedHat Fedora Core 6 operating system and (2) an Intel Core Duo
based dual-core system running RedHat Fedora Core 6. We found the
results to be almost exactly the same as those reported.
the systems were unloaded as much as possible. To ac-
count for possible variability due to system state, each
run was repeated 10 times and the execution time results
were averaged (error bars show the variance across the
repeated runs). Each application’s main loop consists of
N = 2.5 · 106 iterations and was repeated 1000 times in
the measurements.
Figure 4(a) shows the normalized execution time of
stream when run (1) alone, (2) concurrently with another
copy of stream, and (3) concurrently with rdarray. Fig-
ure 4(b) shows the normalized execution time of rdarray
when run (1) alone, (2) concurrently with another copy
of rdarray, and (3) concurrently with stream.
When stream and rdarray execute concurrently on the
two different cores, stream is slowed down by only 18%.
In contrast, rdarray experiences a dramatic slowdown:
its execution time increases by up to 190%. Hence,
stream effectively denies memory service to rdarray
without being signiﬁcantly slowed down itself.
We hypothesize that this behavior is due to the row-
hit-ﬁrst scheduling policy in the DRAM memory con-
troller. As most of stream’s memory requests hit in
the row-buffer, they are prioritized over rdarray’s re-
quests, most of which result in row conﬂicts. Conse-
quently, rdarray is denied access to the DRAM banks
that are being accessed by stream until the stream pro-
gram’s access pattern moves on to another bank. With
a row size of 8KB and a cache line size of 64B, 128
(=8KB/64B) of stream’s memory requests can be ser-
viced by a DRAM bank before rdarray is allowed to ac-
cess that bank!7 Thus, due to the thread-unfair imple-
mentation of the DRAM memory system, stream can act
as an MPH against rdarray.
Note that the slowdown rdarray experiences when run
7Note that we do not know the exact details of the DRAM mem-
ory controller and scheduling algorithm that is implemented in the ex-
isting systems. These details are not made public in either Intel’s or
AMD’s documentation. Therefore, we hypothesize about the causes of
the behavior based on public information available on DRAM memory
systems - and later support our hypotheses with our simulation infras-
tructure (see Section 6). It could be possible that existing systems have
a threshold up to which younger requests can be ordered over older
requests as described in a patent [33], but even so our experiments
suggest that memory performance attacks are still possible in existing
multi-core systems.
262
16th USENIX Security Symposium
USENIX Association
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
3.0
2.5
2.0
1.5
1.0
0.5
0.0
STREAM
stream alone
with another stream
with rdarray
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
3.0
2.5
2.0
1.5
1.0
0.5
0.0
RDARRAY
rdarray alone
with another rdarray
with stream
3.2.3 A Simulated 16-core System
Figure 4: Normalized execution time of (a) stream and (b) rdarray when run alone/together on a dual-core system
with stream (2.90X) is much greater than the slowdown
it experiences when run with another copy of rdarray
(1.71X). Because neither copy of rdarray has good row-
buffer locality, another copy of rdarray cannot deny ser-
vice to rdarray by holding on to a row-buffer for a long
time. In this case, the performance loss comes from in-
creased bank conﬂicts and contention in the DRAM bus.
On the other hand, the slowdown stream experiences
when run with rdarray is signiﬁcantly smaller than the
slowdown it experiences when run with another copy of
stream. When two copies of stream run together they are
both able to deny access to each other because they both
have very high row-buffer locality. Because the rates at
which both streams generate memory requests are the
same, the slowdown is not as high as rdarray’s slowdown
with stream: copies of stream take turns in denying ac-
cess to each other (in different DRAM banks) whereas
stream always denies access to rdarray (in all DRAM
banks).
While the problem of MPHs is severe even in current
dual- or dual-dual-core systems, it will be signiﬁcantly
aggravated in future multi-core systems consisting of
many more cores. To demonstrate the severity of the
problem, Figure 6 shows the normalized execution time
of stream and rdarray when run concurrently with 15
copies of stream or 15 copies of rdarray, along with
their normalized execution times when 8 copies of each
application are run together. Note that our simulation
methodology and simulator parameters are described in
Section 6.1. In a 16-core system, our memory perfor-
mance hog, stream, slows down rdarray by 14.6X while
rdarray slows down stream by only 4.4X. Hence, stream
is an even more effective performance hog in a 16-core
system, indicating that the problem of “memory perfor-
mance attacks” will become more severe in the future if
the memory system is not adjusted to prevent them.
3.2.2 A Dual Dual-core System
The second system we examine is a dual dual-core AMD
Opteron 275 [1] system with 4GB SDRAM. In this sys-
tem, only the DRAM memory system is shared between
a total of four cores. Each core has an L2 cache size
of 1 MB. The operating system used was RedHat Fe-
dora Core 5. Figure 5(a) shows the normalized execution
time of stream when run (1) alone, (2) with one copy of
rdarray, (3) with 2 copies of rdarray, (4) with 3 copies
of rdarray, and (5) with 3 other copies of stream. Fig-
ure 5(b) shows the normalized execution time of rdarray
in similar but “dual” setups.
Similar to the results shown for the dual-core Intel sys-
tem, the performance of rdarray degrades much more
signiﬁcantly than the performance of stream when the
two applications are executed together on the 4-core
AMD system. In fact, stream slows down by only 48%
when it is executed concurrently with 3 copies of rdar-
ray. In contrast, rdarray slows down by 408% when run-
ning concurrently with 3 copies of stream. Again, we hy-
pothesize that this difference in slowdowns is due to the
row-hit-ﬁrst policy employed in the DRAM controller.
4 Towards a Solution: Fairness in DRAM
Memory Systems
The fundamental unifying cause of the attacks demon-
strated in the previous section is unfairness in the shared
DRAM memory system. The problem is that the mem-
ory system cannot distinguish whether a harmful mem-
ory access pattern issued by a thread is due to a malicious
attack, due to erroneous programming, or simply a nec-
essary memory behavior of a speciﬁc application. There-
fore, the best the DRAM memory scheduler can do is to
contain and limit memory attacks by providing fairness
among different threads.
Difﬁculty of Deﬁning DRAM Fairness: But what ex-
actly constitutes fairness in DRAM memory systems?
As it turns out, answering this question is non-trivial
and coming up with a reasonable deﬁnition is somewhat
problematic. For instance, simple algorithms that sched-
ule requests in such a way that memory latencies are
equally distributed among different threads disregard the
fact that different threads have different amounts of row-
buffer locality. As a consequence, such equal-latency
scheduling algorithms will unduly slow down threads
USENIX Association
16th USENIX Security Symposium
263
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
i
e
m
T