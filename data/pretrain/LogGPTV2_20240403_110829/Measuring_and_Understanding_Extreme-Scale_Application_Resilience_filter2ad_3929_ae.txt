runs 
%XE  %XK 
Total 
Total 
%XE  %XK 
892,146  97.4%  2.2%  35,277,529  70.5% 29.5% 
760,954  69.8%  30.1%  28,040,129  72.8% 27.2% 
14,946  88.5%  7.2%  13,905,226  89.2% 10.8% 
1,674  75.1%  25.5%  7,002,353  80.7% 19.3% 
77,959  99.4%  0.5%  6,225,259  88.9% 11.1% 
21,342  76.0%  15.8%  1,527,814  74.3% 25.7% 
9.5%  90.2%  10,445,842  32.5% 67.5% 
11,730  60.0%  34.8%  1,288,745  85.7% 14.3% 
286,673  59.1% 40.9% 
9,962  59.8%  36.3% 
1,535,754 
XE 
XK 
XK 
XE 
17,400  4,096  152,513  139,362 
4,096  4,096  106,650 
60,622 
6,144  2,048  123,969 
15,279 
64  157,052  138,320 
6,144 
110,623 
512  113,187 
8,192 
16,020 
45,433 
4,356  3,872 
14,260 
1,000  1,296 
49,313 
512  122,726 
20,000 
10,341 
22,460 
42,350 
8,188  4,096 
• rmg, an implementation of a real-time multigrid algorithm
for simulating electronic structures for predicting and ex-
plaining the properties of materials based on MPI/OpenMP
and ( intensive HSN load);
• vmd, a molecular visualization program for displaying,
animating, and analyzing biomolecular systems;
• spectrum, a petascale code to simulate wave propagation
based on MPI/OpenMP (intensive HSN load).
The MNBF of XE applications is greater than that of XK
applications. Table V shows that for all analyzed applications,
Blue Waters applications running on XE nodes have a larger
MNBF than the same application running on XK nodes. Three
major factors affect the MNBF ﬁgures: (i) application character-
istics including the scale (duration and node hours), (ii) the ef-
fectiveness of the employed application-level checkpoint/restart
mechanisms, and (iii) the reliability of the underlying platform,
i.e., XE and XK nodes, the interconnect, and the ﬁle system.
A. Application Node Hours and Duration
Figures 8.(a) and (b) show the histogram and cumulative
density functions of the node hours (i.e., number of nodes ×
application duration) of XE and XK applications that failed
because of system errors. On average, XE and XK applications,
when exposed to system errors, fail within 732 and 46 node
hours, respectively. A comparison of the two distributions
reveals that XE applications are more likely to fail sooner (less
than 1 node hour) than XK applications. In particular, 18% of
XE applications and 1.8% of XK applications fail in less then
a node hour. The analysis of the related data indicated that
those are the applications launched during periods in which the
system is malfunctioning or partially under recovery. In these
cases, protection mechanisms including warm-swap and check-
point/restart, are not effective in preventing the application’s
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
1 
0.75 
0.5 
0.25 
0 D
a
u
m
u
C
/
y
t
i
s
n
e
l
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
/
y
t
i
s
n
e
D
Mean = 732 node hours 
0.1     1      10     100   1,000 10,000 100,000  
Node hours XE [log] 
(a) 
1 
0.75 
0.5 
0.25 
 0 
Mean = 46 node hours 
10
0.1       1           10        100      1,000     10,000 
10000
1
N d h
Node hours XK [log] 
100
li
1000
k
i
l
(b) 
Fig. 8. Distribution of the node hours run by failed (a) XE and (b) XK
applications in the presence of system problems.
3232
termination (see Section III). These failures could be avoided by
orchestrating system-level resiliency mechanisms and workload
management software.
Concerning long-running applications, both distributions in
Figure 8.(a) and (b) show a long tail behavior. For XE applica-
tions, the tail starts at about 5000 node hours corresponding to a
mean duration of about 9 hours, while for XK applications, the
tail starts around 500h, corresponding to an average duration
of 12.7h. More in-depth analysis indicates that, in presence of
system problems, XE applications fail sooner (mean duration is
4.1h) than XK applications (mean duration of 7.6 h), not only
in terms of node hours. XE applications tend to be executed on
a larger set of nodes than XK applications (see Table III), hence
the probability of failure could be higher. However, as will be
detailed in Section VII, we explain this behavior considering
that XE6 nodes are equipped with more effective error detectors
than XK nodes, hence many errors of the underlying platform
are detected sooner, i.e., during the initial node hours. GPU
cores are equipped with less sophisticated error detectors that
make it difﬁcult to detect errors until they propagate and kill
the application.
B. Looking to Checkpoint/Restart Code
As one might expect, applications using checkpoint/restart
techniques (indicated by bold font in Table V) show substan-
tially larger MNBF ﬁgures than the applications using limited
or no protection. In order to understand differences in the
MNBF ﬁgures, we performed an in-depth analysis of the code
implementing the error detection and checkpoint/restart capa-
bilities of the considered applications and the parallelization
framework. The analyses revealed substantial differences in the
way errors are detected and checkpoints are stored.
Applications developed using the Charm++ framework
show a 25% larger MNBF than applications developed using
MPI/OpenMP. Table V shows that the highest MNBF ﬁgures
are reached by applications implemented using the Charm++
framework (namd and rhmd). The two applications show similar
MNBF ﬁgures when running on the same type of nodes. We
veriﬁed this claim considering all applications in our dataset
implemented using Charm++ (not shown in Table V). A good
example is enzo P, a version of enzo speciﬁcally adapted to
petascale systems using Charm++. enzo P shows an improve-
ment in MNBF over enzo from 113,187h (110,623 h on XK) for
the MPI version without checkpoint/restart, to 151,022h when
executing with Charm++ checkpoint/restart (131,176h on XK).
For MPI/OpenMP-based applications (spectrum, rmg and
pmcl3d in Table V), node crashes are detected when the con-
nection between two nodes terminates abnormally, i.e., before
one of the endpoints invokes the mpi ﬁnalize() method. The
default error handler of those events is the global exit of all
MPI processes4. Checkpoints are typically collected by a master
node (e.g., the node 0 in enzo) and stored in a single ﬁle. The
restart is performed by overriding the standard error handler.
More advanced implementations (as in pmcl3d, which uses
MPI−2 and parallel IO libraries) employ error handlers able
to intercept events deeper in the stack using speciﬁc callback
FAILURE STATISTICS OF BLUE WATERS COMPONENTS ESTIMATED FROM
THE FAILURE REPORTS WITH RESPECT TO PRODUCTION HOURS.
TABLE VI
Components 
XE/XK node 
Interconnect 
(Gemini HSN 
and Lustre 
Network) 
File system/ 
Storage 
•  XEsys== 8.6 (MTBI) 
•  Xksys   = 25.1 (MTBI) 
Failure Statistics (MTBI [h] & MNBF [h] ) 
•  XEnode = 128,832  (MNBF) 
•  XKnode = 63,598   (MNBF) 
•  LNetsys    = 359.8  (MTBI) 
•  LNethome  = 10,294 (MTBI) 
•  LNetproject= 10,294 (MTBI) 
•  LNetscratch= 571.88 (MTBI) 
•  Home (storage node)  =  12,348 (MNBF) 
•  Project (storage node) =   9,468 (MNBF) 
•  Scratch (storage node) = 10,292 (MNBF) 
•  Gemini = 857.7 (MTBI) 
•  Home = 343.2 (MTBI) 
•  Project=263.1 (MTBI) 
•  Scratch=35.4 (MTBI) 
handlers. MPI restart strategies require the swap of the failed
processor with a working one so that the number of processors
at the checkpoint-time and the recovery-time are the same5.
Charm++ employs different collaborative checkpoint/restart
mechanisms (disk and in-RAM checkpoint implemented as part
of the FT−Charm++ library [11]), all relying on error detection
performed by means of heartbeat (5s period in Blue Waters).
There are two phases in the checkpoint support. In the ﬁrst
phase, after reaching a global synchronization point, each node
stores its checkpoint in the main memory. After every node
safely stores the newest copy of the checkpoint, the normal
execution resumes. The Charm++ runtime system is responsible
for creating a backup copy of each checkpoint on another node.
When a processor crashes, the restart protocol is automatically
invoked to recover all objects using the last checkpoints. This
technique can successfully recover from 1 failure at a time, or
2 failures on 2 processors not storing each other’s checkpoints.
C. Comparison with system-level measurements
In order to provide a comparison between platform failures
and application failures, we analyzed the data extracted from
the failure reports collected by the vendor during the measured
period. Results are shown in Table VI. Considered failure
events include node interrupts (e.g., uncorrectable hardware
exceptions), ﬁle systems and network failures, and system-
wide outages (i.e., the entire system is not usable and needs
to be repaired). Details on the analysis of the failure reports
are provided in [2]. By comparing the MNBF ﬁgures in Table
V and Table VI we observe the following:
• The MNBF of applications using checkpoint/restart
is
close to or surpasses the MNBF of the single node
(computed as the ratio between the total node hours for
all XE (XK) nodes and the total number of XE (XK)
node interruptions estimated from the failure reports).
For example, the namd application shows an MNBF of
152,513 hours when executing on XE nodes (see Table
V), which is greater than the MNBF of the single XE
(128,832 hours; see Table VI).
• Applications not using the checkpoint/restart
technique
(e.g., rmg, pmemd, vmd) show an MNBF signiﬁcantly
smaller than that of the XE and XK nodes (see Table
VI). For example, the rmg application shows an MNBF
of 45,433 hours when executing on XE nodes (see Table
4Note that during the network failover, applications may fail because the
connection terminates before the invocation of mpi ﬁnalize().
5Sometime that is not possible because of node failures and unsuccessful
warm-swap operations.
3333
V), which is almost three times smaller than the MNBF
of the single XE (128,832 hours).
VII. RELATIONSHIP BETWEEN SCALE AND RESILIENCY
In this section, we investigate the relationship between the
the number of nodes and duration)
application scale (i.e.,
and resiliency. To this end, we estimate the MTBI and the
probability of application failure caused by system errors as a
function of the number of nodes and node hours. This analysis
is performed by grouping the entries in the data set produced
by LogDiver into several buckets of ﬁxed size. The considered
metrics are then computed with respect to each bucket. For
instance, the MTBI for applications executing on 1 to 96 nodes
(i.e., 1 cabinet, see Table III in Section II) is computed as the
ratio between the sum of the number of hours used by all the
applications in the bucket and the number of application failures
in that bucket caused by system errors.
]
h
[
I
B
T
M
]
h
[
I
B
T
M
2,048 
1,024 
512 
256 
128 
64 
32 
16 
8 
4 
2,048 
1,024 
512 
256 
128 
64 
32 
16 
8 
4 
XE applications, when running at full-scale, experience a
MTBI that is about 3 times lower than that of the MTBI
measured for the XK nodes (25.1 h). Figure 9 shows the
plots of the MTBI of XE and XK applications when not using
checkpoint/restart. Full-scale XE and XK applications (22640
nodes for XE and 4224 nodes for XK) have MTBI values of
8.7h and 9.2h for XE and XK applications, respectively.
Comparing these ﬁgures with those related to system MTBI
for XE and XK nodes reported in Table VI, we observe that,
when running at full-scale, XE applications are able to perform
with an MTBI which is about the same as that of the underlying
computing platform, i.e., 8.6h.
These results also show that, when running at full-scale, the
checkpoint/restart mechanisms are severely stressed. This not
only means that a computation makes little progress because of
the overhead introduced by the checkpoint/restart necessitated
by frequent failures. It also means that fault-handling protocols
may need to handle multiple errors, e.g., the failure of two
nodes sharing the same copy of the checkpoint ﬁle in Charm++
applications. In this case, even for the most advanced resiliency
mechanism employed by the system, the failure may be catas-
trophic and the application fail inexorably6.
Concerning GPU nodes, our measurements show that the
MTBI of XK applications is about 3 times lower than the MTBI
of the underlying GPU platform (25.1h; see Table VI). The
reasons for this difference will now be discussed.
• The MTBI ﬁgures in Table VI computed for XE and XK
nodes refer to interrupts caused by hardware problems that
were detected by system-level error detection capabilities
(e.g., double bit errors in the GPU memory) and that
caused downtime of the failed blades. However, GPUs are
equipped with limited error detection capabilities (parity
and ECC) compared with the ECC/Chipkill techniques
protecting the AMD processors and main memory [2].
Therefore,
the chance of error propagation from the
hardware to the application level is not negligible. This
translates into a higher rate of undetected transient errors
and an overestimation of the MTBI measurements and the
resiliency achievable on XK7 nodes.
6An example of an efﬁcient checkpoint/restart at full-scale is that of the rhmd
application, which shows an MTBI of 34h when running on 20,000 nodes.
3434
0    2,048   4,096  6,144  8,192  10,240  12,288  14,336  16,384  18,432  20,480  22,528 
XE Application scale [number of nodes] 
(a) 
0           512         1,024      1,536        2,048       2,560      3,072       3,584       4,096 
XK Application scale [number of nodes] 
(b) 
Fig. 9. MTBI and local polynomial regression of (a) XE and (b) XK
applications when no checkpoint/restart technique is used.
• An in-depth analysis of GPU/Hybrid applications showed
that they are more difﬁcult to handle during error-recovery
procedures (e.g., checkpoint/restart). Many GPU-related
modules are not restartable or replaceable without im-
pacting the other software parts. In addition,
the lack
of communication between different software stacks on
XK nodes (e.g., GPU drivers with OpenMP or Charm++)
makes it difﬁcult for the resiliency mechanisms (e.g.,
heartbeats in Charm++ or connection fan-out in MPI) to
detect errors related to the GPU stack. For instance, a
node may reply to heartbeat messages while manifesting
errors in the GPU stack that can kill