Impact: Medium-high.
Eﬀort: Medium-high, since we ﬁrst need to sort out how eﬀective the algorithm is, and then we need to
ﬁgure out a migration plan.
Risk: Medium, since a new selection algorithm probably carries with it a new set of anonymity-breaking
papers that will only come out a few years after we deploy.
Plan: Micah is going to write a design proposal for getting relays to compute and maintain their virtual
coordinates based on latency. Once we deploy that, we’ll have some actual data points, and we’ll be in a
better position to simulate whether the idea will help in reality. Counting deployment time, that means we
probably won’t have clients using this scheme until 2010.
16
Performance Improvements on Tor
4.4
Considering exit policy in relay selection
When selecting an exit relay for a circuit, the Tor client will build a list of all exit relays which can carry
the desired stream, then select from them with a probability weighted by each relay’s capacity7. This means
that relays with more permissive exit policies will be candidates for more circuits, and hence will be more
heavily loaded compared to relays with restrictive policies.
Figure 4 shows the exit relay capacity for a selection of port numbers. It can be clearly seen that there
is a radical diﬀerence in the availability of relays for certain ports (generally those not in the default exit
policy). Any traﬃc to these ports will be routed through a small number of exit relays, and if they have a
permissive exit policy, they will likely become overloaded from all the other traﬃc they receive. The extent
of this eﬀect will depend on how much traﬃc in Tor is to ports which are not in the default exit policy.
The overloading of permissive exit relays can be counteracted by adjusting the selection probability of a
relay based on its exit policy and knowledge of the global network load per-port. While it should improve
performance, this modiﬁcation will make it easier for malicious exit relays to select traﬃc they wish to
monitor. For example, an exit relay which wants to attack SSH sessions can currently list only port 22 in
its exit policy. Currently they will get a small amount of traﬃc compared to their capacity, but with the
modiﬁcation they will get a much larger share of SSH traﬃc. (On the other hand, a malicious exit relay
could already do this by artiﬁcially inﬂating its advertised bandwidth capacity.)
To properly balance exit relay usage, it is necessary to know the usage of the Tor network, by port.
McCoy et al. [6] have ﬁgures for protocol usage in Tor, but these ﬁgures were generated through deep packet
inspection, rather than by port number. Furthermore, the exit relay they ran used the fairly permissive
default exit policy. Therefore, their measurements will underestimate the relative traﬃc on ports which are
present in the default exit policy, and are also present in more restrictive policies. To accurately estimate
the Tor network usage by port, it is necessary to measure the network usage by port on one or more exit
relays, while simultaneously recording the exit policy of all other exit relays considered usable.
We could instead imagine more crude approaches. For example, in Section 3.4 we suggest using a tool
like SpeedRacer or SoaT to identify relays that are overloaded. We could then either instruct clients to avoid
them entirely, or reduce the capacity associated with that relay in the directory status to reduce the attention
the relay gets from clients. Then we could avoid the whole question of why the relays are overloaded. On
the other hand, understanding the reasons for load hotspots can help us resolve them at the architectural
level.
Impact: Low-medium.
Eﬀort: Low-medium.
Risk: Low.
Plan: When we’re gathering statistics for metrics, we should make a point of gathering some anonymized
data about destination ports seen by a few exit relays. Then we will have better intuition about whether
we should solve this by reweighting at the clients, reweighting in the directory status, or ignoring the issue
entirely.
4.5
Older entry guards are overloaded
While the load on exit relays is skewed based on having an unusual exit policy, load on entry guards is
skewed based on how long they’ve been in the network.
Since Tor clients choose a small number of entry guards and keep them for several months, a relay that’s
been listed with the Guard ﬂag for a long time will accumulate an increasing number of clients. A relay that
just earned its Guard ﬂag for the ﬁrst time will see very few clients.
To combat this skew, clients should rotate entry guards every so often. We need to look at network
performance metrics and discern how long it takes for the skew to become noticeable – it might be that
7The actual algorithm is slightly more complex: in particular, exit relays which are also guard relays will be weighted less,
and some circuits are created preemptively without any destination port in mind.
17
Performance Improvements on Tor
22
25
80
119
135
443
563
8080
6667
Port number
Exit capacity available (%)
0
20
40
60
80
100
Nodes
Bandwidth
Figure 4: Exit relay capacity, in terms of number of relays and advertised bandwidth for a selection of port
numbers.
18
Performance Improvements on Tor
rotating to a new guard after a week or two is enough to substantially resolve the problem. We also need
to consider the added risk that higher guard churn poses versus the original attack they were designed to
thwart [11], but I think a few weeks should still be plenty high.
At the same time, there are fewer relays with the Guard ﬂag than there should be. While the Exit ﬂag
really is a function of the relay’s exit policy, the required properties for entry guards are much more vague:
we want them to be “fast enough”, and we want them to be “likely to be around for a while more”. I think
the requirements currently are too strict. This scarcity of entry guards in turn inﬂuences the anonymity the
Tor network can provide, since there are fewer potential entry points into the network.
Impact: High.
Eﬀort: Low.
Risk: Low.
Plan: We should do it, early in Tor 0.2.2.x. We’ll need proposals ﬁrst, both for the “dropping old guards”
plan (to assess the tradeoﬀ from the anonymity risk) and for the “opening up the guard criteria” plan.
5
Clients need to handle variable latency and failures better
The next issue we need to tackle is that Tor clients aren’t as good as they should be at handling high or
variable latency and connection failures. First, we need ways to smooth out the latency that clients see.
Then, for the cases where we can’t smooth it out enough, we need better heuristics for clients to automatically
shift away from bad circuits, and other tricks for them to dynamically adapt their behavior.
5.1
Our round-robin and rate limiting is too granular
Tor’s rate limiting uses a token bucket approach to enforce a long-term average rate of incoming and out-
going bytes, while still permitting short-term bursts above the allowed bandwidth. Each token represents
permission to send another byte onto the network (or read from the network). Every second new tokens are
added, up to some cap (the bucket size).
So Tor relays that have cells buﬀered waiting to go out onto the network will wait until the new second
arrives, and then deliver as many cells as they can. In practice, this behavior results in traﬃc ‘bumps’ at
the beginning of each second, with little network traﬃc the rest of the time. Mike and Karsten have been
collecting data from circuit extension times (how long it takes to establish each hop of a circuit); the bumps
are easily seen in Figure 5.
Our original theory when designing Tor’s rate limiting was that one-second granularity should be suﬃ-
cient: cells will go out as quickly as possible while the bucket still has tokens, and once it’s empty there’s
nothing we can do but wait until the next second for permission to send more cells.
We should explore reﬁlling the buckets more often than once a second, for three reasons. First, we’ll get
a better intuition about how full the buﬀers really are: if we spread things out better, then we could reduce
latency by perhaps multiple seconds. Second, spikes-and-silence is not friendly for TCP, so averaging the
ﬂows ourselves might mean much smoother network performance. Third, sub-second precision will give us
more ﬂexibility in our priority strategies from Section 2.1.
On the other hand, we don’t want to go too far: cells are 512 bytes, so it isn’t useful to think in units
smaller than that. Also, every network write operation carries with it overhead from the TLS record, the
TCP header, and the IP packet header. Finally, network transmission unit (MTU) sizes vary, but if we could
use a larger packet on the wire and we don’t, then we’re not being as eﬃcient as we could be.
Impact: Low-Medium.
Eﬀort: Medium.
Risk: Low, unless we add in bad feedback eﬀects and don’t notice.
Plan: We should continue collecting metrics to get better intuition here. While we’re designing priority
stategies for Section 2.1, we should keep the option of higher-resolution rate-limiting in mind.
19
Performance Improvements on Tor
Circuit extension time
Time [s]
Density
0
1
2
3
4
5
6
7
8
9
10
0.0
0.2
0.4
0.6
0.8
1st hop
2nd hop
3rd hop
All hops
Figure 5: Number of seconds it takes to establish each hop of a 3-hop circuit. The higher density of samples
around 2s, 3s, etc indicate that rate limiting at each relay is introducing extra delay into the responses.
20
Performance Improvements on Tor
5.2
Better timeouts for giving up on circuits and trying a new one
Some circuits are established very quickly, and some circuits take many seconds to form. The time it takes
for the circuit to open can give us a hint about how well that circuit will perform for future traﬃc. We
should discard extremely slow circuits early, so clients never have to even try them.
The question, though, is how to decide the right timeouts? If we set a static timeout in the clients, then
choosing a number that’s too low will cause clients to discard too many circuits. Worse, clients on really
bad connections will never manage to establish a circuit. On the other hand, setting a number that’s too
high won’t change the status quo much.
Fallon Chen worked during her Google-Summer-of-Code-2008 internship with us on collecting data about
how long it takes for clients to establish circuits, and analyzing the data to decide what shape the distribution
has (it appears to be a Pareto distribution). The goal is for clients to track their own circuit build times, and
then be able to recognize if a circuit has taken longer than it should have compared to the previous circuits.
That way clients with fast connections can discard not-quite-fast-enough circuits, whereas clients with slow
connections can discard only the really-very-slow circuits. Not only do clients get better performance, but
we can also dynamically adapt our paths away from overloaded relays.
Mike and Fallon wrote a proposal8 explaining the details of how to collect the stats, how many data
points the client needs before it has a good sense of the expected build times, and so on.
Further, there’s another case in Tor where adaptive timeouts would be smart: how long we wait in
between trying to attach a stream to a given circuit and deciding that we should try a new circuit. Right
now we have a crude and static “try 10 seconds on one, then try 15 seconds on another” algorithm, which
is both way too high and way too low, depending on the context.
Impact: Medium.
Eﬀort: Medium, but we’re already part-way through it.
Risk: Low, unless we’ve mis-characterized the distribution of circuit extend times, in which case clients
end up discarding too many circuits.
Plan: We should deploy the changes in clients in Tor 0.2.2.x to collect circuit times, and see how that
goes. Then we should gather data about stream timeouts to build a plan for how to resolve the second piece.
5.3
If extending a circuit fails, try extending a few other places before aban-
doning the circuit.
Right now, when any extend operation fails, we abandon the entire circuit. As the reasoning goes, any other
approach allows an attacker who controls some relays (or part of the network) to dictate our circuits (by
declining to extend except to relays that he can successfully attack).
However, this reasoning is probably too paranoid. If we try at most three times for each hop, we greatly
increase the odds that we can reuse the work we’ve already done, but we don’t much increase the odds that
an attacker will control the entire circuit.
Overall, this modiﬁcation should cut down on the total number of extend attempts in the network. This
result is particularly helpful since some of our other schemes in this document involve increasing that number.
Impact: Low.
Eﬀort: Low.
Risk: Low-medium. We need to actually do some computations to conﬁrm that the risk of whole-path
compromise is as low as we think it is.
Plan: Do the computations, then write a proposal, then do it.
8https://svn.torproject.org/svn/tor/trunk/doc/spec/proposals/151-path-selection-improvements.txt
21
Performance Improvements on Tor
5.4
Bundle the ﬁrst data cell with the begin cell
In Tor’s current design, clients send a “relay begin” cell to specify the intended destination for our stream,
and then wait for a “relay connected” cell to conﬁrm the connection is established. Only then do they
complete the SOCKS handshake with the local application, and start reading application traﬃc.
We could modify our local proxy protocol in the case of Privoxy or Polipo so it sends the web request
to the SOCKS port during the handshake.