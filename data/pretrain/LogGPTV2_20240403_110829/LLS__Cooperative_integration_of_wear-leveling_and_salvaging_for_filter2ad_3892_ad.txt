such as birthday 
L1 and L2 caches and a shared DRAM L3 
approach. 
ics [26]. We simulate 
simulator 
has private 
parameters 
For performance, 
we evaluate 
LLS using Sim­
a four-core 
are summarized 
in Figure 9. Each core 
3.2GHz CMP; the detailed 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:13 UTC from IEEE Xplore.  Restrictions apply. 
227 
PA  RPA 
Main 
DA 
To relocate DA-7800: 
PA  RPA  DA 
100   II 
R 
14110  l 
II ' II 15001 -l  (1) RBSG "(DA-7800)= RPA-7200 
PCM  D \/ D  c::::::::J: 
b-vEWi 
fl--l R/\R R: (3) PA-7200 is no long used 
IiI 'II  r-l I (4) RBSG(RPA-ll00) 
Backup!   -'0 - 
PCM --('200L-.] 7200:  780  (5) relocate DA-7800 to DA-ls00 
D 
. I 
for smooth (b) at DA level, relocate useful data for 
= DA-ls00 
(2) RPA-7200 contains useful data 
RIB"(RPA-7200) = PA-100 
RIB(PA-7200) 
= RPA-ll00 
(a) enhancing Start-Gap 
PCM resizing 
PCM resizing 
Figure 8. Integrating 
Start-Gap 
and LLS with the support 
for PCM resizing. 
The 
B. Lifetime Study 
(the programs 
cache. Each L2 cache is 4MB and 8-way set associative. 
shared DRAM cache is 64MB and 16-way set associative. 
As shown in [18], [16], a large DRAM cache is essential 
as it reduces the number of writes to PCM and enables 
practical 
use of PCM as main memory. Only data evicted 
from DRAM are stored in PCM. We evaluate 
benchmark programs 
ting) from SPEC 2006. These programs 
-the programs 
(e.g., 
evaluate 
cells fail. At each point, we simulate 
after 1 billion 
skipping 
latency 
latencies 
the warmup phase in each program. 
We use recent 
numbers from Numonyx - PCM read and write 
of memory 
instructions 
are set after 
after  different 
portions 
1 billion 
ones with intensive 
(e.g., 
have good coverage 
memory accesses 
of 
that compile in our set­
are 50ns and 1000ns respectively 
mcf) and light memory accesses 
warmup instructions. 
Checkpoints 
performance 
a  subset 
include 
gcc) [2]. We 
[20]. 
To evaluate 
the effectiveness 
of LLS, we compared it to 
schemes. 
salvaging 
a contiguous 
The enhancements 
were 
usable PCM space, and have 
ECP-M was enhanced from ECP 
ECP-M and Page-Ideal, which are enhanced versions 
of two existing 
added to support 
no impact on lifetime. 
[24] with a mapping table at the page level (discussed 
in Section 3). Page-Ideal is an ideal version 
to support 
when there is a need to match two pages with failed cells, 
we optimistically 
biggest 
address 
visible 
these two pages. 
assume that one of them always has the 
of all usable pages. We use the lower page 
the 
up 
address 
as the one to identify 
PCM space is contiguous 
before and after pairing 
the page  pair. 
contiguous 
Therefore, 
of [10] 
PCM space. In this implementation, 
CPU core 
Ll cache 
L2 Cache 
4-core CMP, 3.2GHz 
private, 
separate 
32K, 4-way, 2-cycle 
private, 
writeback, 
l2-cycIe 
4MB, 8-way, LRU, 
hit latency 
I-/D-caches, 
hit latency 
DRAM L3 cache 64MB, shared, 
64B linesize, 
16-way, LRU, 
writeback, 
l5ns hit latency 
Main Memory  8GB PCM, 4 ranks of 8 banks each 
PCM latency read: 50ns, write: IOOOns 
Figure 9. Baseline 
configurations. 
Since it is impractical 
to simulate 
the whole lifetime 
of 
write operations 
from [24]. We assume uniform wear leveling 
that 
PCM chips of this size, we follow the same simplified 
approach 
evenly distributes 
memory space. Each write alters 
When distributing 
number of writes to each line is slightly 
Therefore 
rather than the number to each line. 
higher after resizing. 
we report the total number of write operations 
to all lines in the usable 
50% cells within one line. 
a fixed number of writes to PCM, the 
Figure 10 summarizes 
the lifetime 
comparison 
of different 
We chose the same cell variances 
schemes. 
in Section 
as [24] 
2.1). The x-axis shows the total number 
salvaging 
(discussed 
of write operations. 
in which all cells have the same 108 write endurance, 
i.e., 
no PV for x= 1. The y-axis shows the percentage 
of pages 
that survived 
We normalized 
this number to the setting 
over the time. 
In the figure, we show the result from an oracle that 
w times to cause its six weakest cells 
writes each line exactly 
to fail. All failed cells are then rescued by ECP. Clearly 
w  varies across different 
Oracle gives the upper bound from perfect 
wear leveling, 
and perfect 
cooperation 
among both. The gap between oracle and ECP motivates 
our design of a line-level 
wear leveling. 
lines due to process variations. 
scheme to work with 
line salvaging, 
salvaging 
PV-aware 
perfect 
In these experiments, 
LLS divides 
the space into 128 
more space than ECP for the first batch of failures. 
group. From the figure, we observed 
4 lines from each chunk to form a 
that LLS 
chunks and selects 
salvaging 
shrinks 
To handle the first line failure, 
resizing 
only marks one page as non-usable. 
and removes 64MB from the main PCM; ECP-M 
LLS has smaller 
LLS requires 
one PCM 
usable 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:13 UTC from IEEE Xplore.  Restrictions apply. 
228 
80% 
100% 
-- -ECp·M --
_ .. _ .. Oracle 
_. -lLS·ldeal
100% 
90%---------------
\>i   
_._._.-Page·ldeal-· 
j-.--- :---------------
llS·128x4 
1 
--------1---
90% ''\···
····
--l\-----1---
-------------'-------'----------T -----' ---'-----T-----
.;::; 
  70% 
4------------
.. -... -.-... -\-.-.-.----.-------.----
'---'---'---'-'---'---'------------T --' -------1-'---a.  60% \ _____________ . ___ . __ . ___ . __ 1.. ______ -------i----
'. 
. 
co a.  60% 
. 
I
..., 50% ···-----r
-·-·-·-
·-·-·-·-·r·-·---· '-" --'-'1"--'-
co 
._._. ___ ._._. __ L _ , .. _ .. __ ._._._!_._. ____ ._._j_._._ 
a 
. 
'. 
U  50% 
I 
. 40% __ . _______ ._.i. ___ . __ ._._. __ L ___ .. ,-'--'--
------ -- ---- ---
------------
. 
i 
..., .. 
.!:! iii 
-t·.--.. tt "'i 
·....
.··...
1----
I 
40% 
. 
. 
I 
E
E· EE 
! :
E :s z 
30% 
i i . 
i
20% 
------- -- i--- ----
------------
I  i L 
10% 
! 
0% 
0.00 0.05 0.10 0.15 0.20 0.25 0.550.60 
0.00 0.05 0.10 0.15 0.20 0.40.50.6 
. r-J-....--.  0% 
! 
I
I
! 
,
\
\
Normalized Number of Writes (variation=O.2) Normalized Number of Writes (variation=O.25) 
Figure 10. Lifetime 
comparison 
of different 
salvaging 
schemes with different 
variances. 
space at this stage. However, 
LLS quickly 
M by exploiting line-level 
achieves 
extra lifetime 
24%, and 41 % for three variances 
a longer lifetime. 
when compared 
On average, 
salvaging 
to the baseline 
ECP - 14%, 
respectively. 
overtakes 
ECP­
opportunities, 
and thus, 
storing 
each status bit with a line, LLS redundantly 
all bits into a global bit map as shown in Figure 7. Due to 
its low modification 
PCM and protected 
In comparison, 
frequency, 
using ECP. 
to support 
the bitmap can be stored in 
contiguous 
PCM space, current 
gathers 
LLS achieves  24% 
techniques. 
and LLS-Ideal assumes wear leveling  scheme 
oracle assumes PV-aware 
In Figure 10, LLS-Ideal shows the total usable space 
at the line level, which gives an upper bound of all line 
level salvaging 
leveling 
that evenly distributes 
and LLS-Ideal are the idle lines in the backup, i.e., no 
broken main PCM lines are mapped to them. From the 
figure, idle lines account for a small percentage. 
of exploiting 
between LLS 
The difference 
endurance 
is small. 
writes. 
The loss 
wear 
Projected 
in months. The above results 
are pre­
their available 
lifetime 
stream 
The actual 
number of writes. 
is 0.25, then the projected 
in months depends on many factors. 
As 
if we assume each bank has 256MB (as in 
sented based on normalized 
PCM lifetime 
an example, 
Figure 9), a cache line has 64B, PCM experiences 
write traffic, each write alters half of a cache line and the 
cell variance 
is about 28 months before we see many failed  cells 
256MB-;-64B x 108 xO.18x 1000ns=7.2e8 
0.18 represents 
PCM endurances 
when 
PV is 0.18 of the 108 no-PV chip (from Figure l Oeb)). In 
other scenarios, 
to a subset of addresses 
may 
shorten 
will prolong the lifetime. 
C. Hardware Cost 
lifetime 
-
that the exploited 
ns = 28 months. 
lifetime 
with ECP 
We next study the hardware 
bitmap storage, 
cost to enable LLS. The 
and the control 
logic 
from main PCM to backup 
translation 
hardware cost includes 
to enable fast address 
PCM. 
1) Bitmap Storage: Each 64B line has one status bit that 
indicates 
ECP and LLS. This accounts 
16MB for a 8GB PCM memory in our setting. 
if the line is broken or not. This bit is needed by 
or 
to 
for 0.2% off-chip storage, 
In addition 
mapping needs 8GB/4KB = 2M entries. 
schemes [24] [10] also need a mapping table. A 
salvaging 
simple page-level 
Given a 21-bit page index, the overhead 
While LLS has more metadata 
modest compared to the PCM space saved from exploiting 
line-level 
is about 4.2MB. 
the overhead 
endurance. 
storage, 
is 
Since the bitmap is stored in PCM, it is slow to access. 
bitmap 
to integrate 
We chose 256KB 
a small on-chip 
used entries. 
it is beneficial 
between cost and performance. 
Since 
Therefore, 
cache to store frequently 
as  a good trade-off 
the need to access the bitmap cache varies with percentage 
of failed cells, we measured the hit rate under different 
percentages 
The results 
Figure 11. The y-axis is the percentage 
lines hit in bitmap cache. We observed 
hit rates due to more reuse of fetched 
accesses. 
no significant  improvement. 
of accesses 
slightly 
bit vectors 
We also evaluated 
larger cache sizes, 
are summarized in 
but observed 
of survived 
higher cache 
to failed 
with more 
lines. 
transla­
2) Translation Logic: Hardware-assisted 
address 
tion is designed 
problem - given 
a broken line X in main PCM, how to quickly identify its 
to solve the following 
translation 
is always performed 
mapped line W in the same group in backup PCM? Since 
address 
in one group, in the 
following 
As described 
th broken line in the main PCM while W is the y-th healthy 
line in backup PCM. 
above, in the corresponding 
we use group offset to indicate 
description, 
group, X is the y­
a line. 
We show the address 
translation 
logic in Figures 
12 and 
13. We divide the PCM space into 128 chunks and  choose 
4 lines from each chunk. The PCM fails if more than 
half of the space moves into the backup PCM. Therefore, 
each salvaging 
the last 256 bits belong to the backup space. To simplify 
group has 128x4=512 bits, and at most, 
attack traffic 
[23] while normal traffic having less 
writes 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:13 UTC from IEEE Xplore.  Restrictions apply. 
229 
Figure II. The effectiveness 
evaluation 
of an on-chip bitmap cache. 
backup 
return 
subgroup 
Step 3: 
Step 2: 
broken line  Step 1:  broken rank 
address 
Z=30  return the device W=494 
identify 
the 
address X=465 
Y=15 
compute the 
, 
,  broken rank Y 
line address in  , 
subgroup index  , 
, 
, 
, 
, 
, 
, 
, 
in backup PCM 
backup PCM 
, 
, 
, 
, 
, 
, , 
, 
, 
, 
, 
, 
-----------------------1------------1 
, 
, 
r-------------- 
I 2nd sub-group i W;;;494 
y; # 'of 1s 
: X;465 
' , 
' 
J'------------ t-- \ 
t 
r--------------------------------------
t 
30 31  ,::.:464:::.r46:;5 __  ,.:4":.:78:,.::4.:.;79 480 
o 1 
511 
508 
492493494495 
496 497 
1 I ::: I 0 11 I  1 0 1 0 II 0 1 11 i 0 1 0 11 IL-l 0--'-1 ...... 11_----'-1 0---,1_1-,-1 0--'.1---,0 1 
""::1 0"T1 0.:...,1- -----r1 0':"1':'::'1 IL-l 0 ...... !1 ..... 1_ ----L.11 ............ 
'--------v-----
t  from one chunk ----------------------v------------------------/ 
R'oc=492 Backup PCM 
Main PCM 
\-----v------
14 15 1 6  17 
Figure 12. Address mapping in three steps. 
the discussion, 
backup space. 
we assume five chunks are currently 
in the 
Figure 12 shows an overview 
of our three step implemen­
tation. 
Given a broken line x=465 (group offset), 
• Step I: Compute X's broken line rank Y by counting 
the preceding 
broken lines. 
Assume Y=15. 
• Step 2: Split the bits from backup space into 16-bit 
holds the line. 
and identify which subgroup 
subgroups 
Here, we have z=30 indicating 
in reverse 
is  in the 2nd subgroup 
order. 
the mapped backup line 
• Step 3: Return the location 
line in a 16-bit subgroup.W=494 
example. 
after identifying 
is referred 
the backup 
in this 
Figure 13 presents 