ADDA’s semi-automated classiﬁcation method. The ﬁrst
is that, for datacenter applications, control plane chan-
nels are easily identiﬁed. For example, Hypertable’s
master and lock server are entirely control plane nodes
by design, and thus all their channels are control plane
channels. The second observation is that control plane
channels, though bursty, operate at low data-rates [3].
For example, Hadoop [15] job nodes see little com-
munication since they are mostly responsible for job
assignment – a relatively infrequent operation.
ADDA leverages the ﬁrst observation by allowing
the user to specify or annotate control plane channels.
The annotations may be at channel granularity (e.g., all
communication to conﬁguration ﬁle x), or at process
granularity (e.g., the master is a control plane process).
It may not be practical for the developer to annotate
all control plane channels. Thus, to aid completeness,
ADDA attempts to automatically classify channels. More
speciﬁcally, ADDA leverages the second observation by
using a channel’s data-rate proﬁle,
including bursts,
to automatically infer if it is a control plane channel.
ADDA employs a simple token-bucket classiﬁer to detect
control plane channels: if a channel does not overﬂow
the token bucket, then ADDA deems it to be a control
channel, otherwise ADDA assumes it is a data channel.
The token-bucket classiﬁers on socket, pipe, tty, and
ﬁle channels are parameterized with a token ﬁll rate of
100KBps and a maximum size of 1MB.
Shared-memory channels: The data-rates here are
measured in terms of CREW-fault rate (the rate at
which CREW serializes accesses to shared pages). The
higher the fault rate, the greater the amount of sharing
through that page. We experimentally derived token-
bucket parameters for CREW control plane communi-
cations: a bucket rate of 150 faults/second and a burst
of 1000 faults/second were enough to identify control
plane sharing (§V).
A key limitation of our automated classiﬁer is that
it provides only best-effort classiﬁcation: the heuris-
tic of using CREW page-fault rate to detect control
plane shared-memory communication can lead to false
negatives (and, unproblematically, false positives), in
which case, control plane determinism cannot be guar-
anteed. In particular,
the behavior of legitimate but
high data-rate control plane activity on shared-memory
channels (e.g., spin-locks) may not be captured, which
may preclude correct replay. In our experiments, how-
ever, such false negatives were rare due to the fact
that user-level applications (especially those that use
pthreads) rarely employ busy-waiting: on a lock
miss, pthread_mutex_lock() will await notiﬁca-
tion of lock availability in the kernel instead of spinning.
3) Taming False Sharing with Best-Effort CREW:
Under certain workloads, the CREW protocol can incur
high page-fault rates that will seriously degrade perfor-
mance. Often this is due to legitimate sharing between
CPUs, such as when CPUs contend for a spin-lock.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:57 UTC from IEEE Xplore.  Restrictions apply. 
More often, however, the sharing is false (a consequence
of unrelated data structures being housed on the same
page). In this case, CPUs do not actually communicate
on a channel.
in the original order. Since shared
A1 , A2 , ..., An
memory non-determinism on B is replayed, B generates
the original outputs because they are fully determined
by A1 , A2 , ..., An and the execution of B.
Regardless of the cause, ADDA uses a simple strategy
to avoid high page-fault rates. When ADDA observes that
the fault rate results in token-bucket overﬂow (suggest-
ing that the page is a data plane channel), it removes
all page protections from that page and subsequently en-
ables unbridled access to it, thereby effectively turning
CREW off for that page. CREW is then re-enabled for
the page several seconds in the future to determine if
data-rates have changed. If not, CREW is disabled once
again, and the cycle repeats. When CREW is selectively
disabled, we can still provide replay, but only if the data-
race freedom assumption is met for those pages (ADDA
records the lock order to handle this case).
C. Providing Control Plane Determinism
The central challenge faced by ADDA’s Distributed
Replay Engine (DRE) is that of providing a control-
plane deterministic view of program state despite not
having recorded the original data plane inputs.
To address this challenge, the DRE employs a novel
technique we call Data Plane Synthesis (DPS). DPS
works under the assumption that external data plane
inputs are persistently stored in append-only storage by
the application and thus available later during replay
(for example, click logs that get saved for further
analytics). DPS regenerates the communication on data
plane channels using the stored data plane inputs. DPS
enables ADDA to synthesize data plane inputs during
replay without recording them in production.
1) Regenerating Intermediate Inputs: The external
data plane inputs can be used to replay those processes
that read them directly (i.e., front-end systems). Typical
front-end systems transform these external inputs and
pass them to internal/intermediate nodes. ADDA does
not record these intermediate inputs, instead it regen-
erates them using the classic technique of order-based
replay [19]: given the original inputs to a computation,
and the ordering of channel communications on a node,
one can deterministically reproduce the original outputs
of that node.
The key challenge is to apply order-based replay
to all internal/recorded nodes. We describe inductively
how ADDA provides order-based replay:
Base Case. Replay the data plane outputs of a single
node, given access to all inputs. As long as shared
memory non-determinism on the node is replayed, the
node will generate the same data plane outputs as the
original execution.
Step. Given
n
nodes
Inductive
A1 , A2 , ..., An, whose outputs are inputs to node B,
ADDA ensures that B gets the merged outputs of
order-replayed
2) Dealing with a Mixed World: In an ideal world,
all nodes in the datacenter would be using ADDA. In
reality, only some of the nodes (the ones running a
particular datacenter application) are traced. External
nodes, such as the distributed ﬁlesystem housing the
persistent store and the network (i.e., routers) used by
the application, are not recorded and thus may behave
differently at replay time.
ADDA handles these two aspects separately:
a) Persistent-Store Nondeterminism: Since the
persistent store housing data plane inputs are not traced,
DPS faces the following challenge:
Replaying applications will need to obtain data plane
inputs from the store during replay, but these original
inputs may no longer be present on the same nodes at
replay time. For instance, HDFS uses its own interface,
which is not compatible with VFS, and may redistribute
blocks or even alter block IDs. Hence, a naive approach
that simply reissues HDFS requests with original block
IDs will produce nondeterministic results.
ADDA addresses this challenge using a layer of indi-
rection. In particular, ADDA requires that the target dis-
tributed application communicates with the distributed
ﬁle-system via a VFS-style (i.e., ﬁlesystem mounted)
interface (e.g., HDFS’s Fuse support or the NFS VFS
interface) rather than using the socket-based HDFS
protocol directly. The VFS layer addresses the challenge
by providing a well-deﬁned and predictable read/write
interface to ADDA, keyed only on the target ﬁlename,
hence shielding it from any internal protocol state that
may change over time (e.g., HDFS block assignments
and IDs).
b) Network Nondeterminism: ADDA does not
record and reproduce low-level network (i.e., router)
behavior. This introduces two key challenges for DPS.
First, nodes may be replayed on hosts different than
those used in the original run, making it hard for DPS
to determine where to send messages to. For example,
ADDA’s reduced-scale replay enables replaying a 1000
node cluster on fewer (e.g., 100) nodes, and some of
these replay nodes will have different IP addresses. The
second challenge is that the network may unpredictably
drop messages (e.g., for UDP datagrams). This means
that simply re-sending a message during replay is not
enough to synthesize packet contents at the receiving
node: ADDA must ensure that the target node actually
receives the message.
c) REPLAYNET: As with persistent-store non-
determinism, ADDA shields DPS from network non-
determinism using a layer of indirection. ADDA intro-
duces REPLAYNET, a virtual replay-mode network that
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:57 UTC from IEEE Xplore.  Restrictions apply. 
abstracts away the details of IP addressing and unre-
liable delivery: rather than sending messages directly
through the physical network at replay time, ADDA
sends messages through REPLAYNET. At the high level,
REPLAYNET can be thought of as a key-value store that
maps unique message IDs to message contents. To send
a message over REPLAYNET, a node simply inserts the
message contents into the key-value store keyed on the
message’s unique ID. To receive the message contents, a
node queries REPLAYNET with the ID of the message
it wishes to retrieve. REPLAYNET guarantees reliable
delivery and doesn’t require senders and receivers to be
aware of replay-host IP addresses.
To send and receive messages on REPLAYNET,
senders and receivers must be able to identify messages
with unique IDs. These message IDs are simple UUIDs
that are assigned at record time. Conceptually,
the
message ID for each message is logged by both the
sender and receiver. The receiver can record the message
ID since the sender piggy-backs it on the outgoing
message at record time. Further details of piggy-backing
are given in §IV-B3.
REPLAYNET employs a distributed master/slave ar-
chitecture in which a single master node maintains a
message index and the slaves maintain the messages.
To retrieve message contents at replay time, a node ﬁrst
consults the master for the location (i.e., IP address)
of the slave holding the message contents for a given
message ID. Once the master replies, the node can
obtain the message contents directly from the slave.
3) Coping with Unrecorded Shared-Memory Order-
ing: ADDA ensures that the components that are only
part of the control plane (e.g., Hypertable’s master or
lock server) can always be replayed independently of
whether data plane nodes can be replayed or not. This
holds for two reasons. First, all the inputs of the control
plane nodes are recorded, because all such inputs are
control plane in nature. Second, shared-memory data
rates on control plane nodes are, in our experience,
extremely low, and therefore ADDA is able to capture
all CREW ordering information. The ability to replay
control plane nodes is still valuable because the control
plane accounts for most bugs [3].
A key requirement of order-based replay is that
complete ordering information must be available. Unfor-
tunately, ADDA’s recording of shared memory interleav-
ings may be incomplete, since ADDA disables CREW
for high data-rate pages (§III-B3). In particular, the
interleaving of data races on such pages is not recorded,
hence precluding the reproduction of computation on
intermediate data plane inputs and the subsequently gen-
erated outputs. Hence, ADDA does not guarantee replay
of mixed control/data plane nodes in multiprocessors.
D. Enabling Automated Debugging
In addition to replay, ADDA provides a powerful
platform for building powerful replay-mode, automated
debugging tools. ADDA was designed to be extended
via plugins, hence enabling developers to write sophis-
ticated distributed analyses that would be too expensive
to run in production. We created several plugins using
this architecture, including distributed data ﬂow anal-
ysis, global invariant checking, communication graph
analysis, and distributed-system visualization.
We describe ADDA’s plugin model, and then describe
a simple automated-debugging plugin for distributed
data ﬂow analysis.
1) Plugin Model: A key goal of ADDA’s plugin
model is to ease the development of sophisticated plug-
ins. Therefore, ADDA plugins are written in Python and
provide the following properties.
An illusion of global state. ADDA enables plugins
to refer to remote application state as though it was
all housed on the same machine. For example,
the
following code snippet grabs and prints a chunk of
memory bytes from node ID 2 (IDs are generated during
the recording):
my_bytes = node[2].mem[0x1000:4096]
print my_bytes
An illusion of serial replay. ADDA guarantees that
plugin execution is serializable and deterministic, hence
freeing the plugin developer from having to reason
about concurrency and non-deterministic results.
Access to ﬁne-grained analysis primitives. ADDA
is pre-loaded with commonly-used, ﬁne-grained anal-
ysis primitives. An example of such a primitive is
ADDA’s data-ﬂow analysis primitive, which exports
two functions (is_tainted(node, addr) and
set_taint(node, addr)) that plugins can invoke
to determine if the byte of memory at address addr is
tainted by an external data source and to set the byte of
memory at address addr as tainted.
2) Distributed Data Flow Plugin: DDFLOW is a
distributed data ﬂow analysis plugin for ADDA. DDFLOW
provides a trace of all instructions or functions that
operate, transitively, on the contents of a (user-speciﬁed)
origin data ﬁle or message. DDFLOW is particularly
useful in diagnosing bugs that lead to data loss: it allows
developers to track the ﬂow of data and helps quickly
identify where data ends up – a process that could take
hours if done manually. DDFLOW highlights the power
of ADDA plugins because it is an analysis that is too
heavyweight to do in production, but can easily be done
during replay. The DDFLOW plugin can be written in just
a few lines (initialization code is omitted):
msg_taint_map = {}
def on_send(msg):
if msg.is_tainted():
msg_taint_map[msg.id] = 1
def on_recv(msg):
if msg_taint_map[msg.id]:
local.set_taint(msg.rcvbuf)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:57 UTC from IEEE Xplore.  Restrictions apply. 
else:
local.untaint(msg.rcvbuf)
del msg_taint_map[msg.id]
IV.
IMPLEMENTATION
We implemented ADDA for clusters of Linux x86
machines. ADDA consists of approximately 150 KLOC
of source code (40% LibVEX [20] and 60% ADDA +
plugins). We show how to use ADDA and then discuss
the main implementation challenges we encountered.
A. Usage
The ﬁrst step in using ADDA consists of recording;
to start this phase, a user invokes ADDA on the appli-
cation binary, specifying the location of the log (e.g.,
distributed storage) and the location of persistent data
ﬁles (e.g., an HDFS mount):
$ adda-record --save-as=hdfs://host/demo
--persistent-store=/mnt/hdfs/data ./application
ADDA will
then record the application, without
recording data plane inputs originating from the speci-
ﬁed persistent storage.
To replay using the DDFLOW analysis plu-
gin (§III-D2), one only speciﬁes the plugin name and
the location of previously collected recordings:
$ adda-replay --plugin=dtaint hdfs://host/demo/
B. Lightweight Recording of User-Level Code
Bugs in datacenter applications often reside in ap-
plication code rather than kernel code. ADDA therefore
only records the non-determinism needed to replay user-
level code of developer-selected application processes.
1) Interpositioning: ADDA interposes on user-level
communication channels (sockets, pipes, ﬁles, and
shared-memory) of traced processes.
Sockets, pipes, and ﬁles are interposed with a kernel
module. The module delivers to ADDA a signal for every
system call invoked by a traced process. To address the
high syscall rates of some datacenter applications, we
also intercept syscalls made through Linux’s vsyscall
page, hence avoiding the expense of signals for a
majority of syscalls (most libc calls go through the
vsyscall).
Shared memory accesses are interposed with the help
of ADDA’s CREW kernel module. The module uses
virtual memory page protections to serialize conﬂicting
user-level page accesses. Conceptually, ADDA’s CREW
implementation follows that of SMP-ReVirt [7]: it main-
tains shadow page tables whose permission are up-
graded and downgraded at CREW events. Unlike SMP-
ReVirt, ADDA maintains shadow page tables only for
those processes that are traced. Moreover, ADDA does
not shadow kernel pages (they are identical to those in
guest page tables) hence avoiding CREW overhead due
to false sharing in the kernel (a signiﬁcant bottleneck in
SMP-ReVirt). ADDA interposes on page table operations
using Linux’s paravirt_ops, similarly to Xen.
2) Asynchronous Events: The only asynchronous
events ADDA must record are signals and thread pre-
emptions. This is a key beneﬁt of recording at the user
level, unlike VM-level replay tools that also have to
record all interrupts.
ADDA ensures accurate delivery of asynchronous
events during replay (i.e., at the same point in program
execution as during recording). One way to do this is
to count the number of instructions at record time and
deliver the event at the recorded instruction count during
replay. This requires the use of a software instruction
counter that would incur high runtime overhead. Instead,
ADDA precisely identiﬁes a point in program execution
via the triple , which can be
efﬁciently obtained from x86 CPUs.
3) Piggy-backing: ADDA needs to communicate
trace data (logical clocks, unique message IDs) to
remote nodes during recording, and uses piggy-backing
techniques to do so. However, the naive approach of
piggy-backing trace data on each network packet results
in impractical communication costs.
Instead, ADDA uses two techniques, both leveraging
the semantics of system calls, to reduce piggy-backing
overheads: message-level piggy-backing and TCP-aware
unique IDs. Message-level piggy-backing leverages the
observation that data plane channels send data in large
chunks (e.g., Memcached performs sys_send on 2
MB buffers), so ADDA piggy-backs at the message level
instead of packet level. ADDA leverages the observation
that datacenter applications typically use TCP to transfer
data, and each message in a TCP stream has a unique
ID within the stream (i.e., its sequence number). Thus,