separate network and internet connection. Sandboxes help minimize damage to a production network. Because
computers and devices in a sandbox aren’t managed in the same way as production computers, they are often
more vulnerable to attacks and malware. By segmenting them, you reduce the risk of those computers infecting
your production computers. Sandboxes are also often used for honeypots and honeynets, as explained in the next
bullet.
 Honeypots and honeynets. A honeypot or a honeynet is a computer or network purposely deployed to lure
would-be attackers and record their actions. The goal is to understand their methods and use that knowledge to
design more secure computers and networks. There are important and accepted uses; for example, an anti-virus
software company might use honeypots to validate and strengthen their anti-virus and anti-malware software.
75
However, honeypots and honeynets have been called unethical because of their similarities to entrapment. While
many security-conscious organizations stay away from running their own honeypots and honeynets, they can still
take advantage of the information gained from other companies that use them.
 Anti-malware. Anti-malware is a broad term that often includes anti-virus, anti-spam and anti-malware (with
malware being any other code, app or service created to cause harm). You should deploy anti-malware to every
possible device, including servers, client computers, tablets and smartphones, and be vigilant about product and
definition updates.
7.9 Implement and support patch and vulnerability management
While patch management and vulnerability management seem synonymous, there are some key differences:
 Patch management. The updates that software vendors provide to fix security issues or other bugs are called
patches. Patch management is the process of managing all the patches in your environment, from all vendors. A
good patch management system tests and implements new patches immediately upon release to minimize
exposure. Many security organizations have released studies claiming that the single most important part of
securing an environment is having a robust patch management process that moves swiftly. A patch management
system should include the following processes:
 Automatic detection and download of new patches. Detection and downloading should occur at least
once per day. You should monitor the detection of patches so that you are notified if detection or
downloading is not functional.
 Automatic distribution of patches. Initially, deploy patches to a few computers in a lab environment and
run them through system testing. Then expand the distribution to a larger number of non-production
computers. If everything is functional and no issues are found, distribute the patches to the rest of the nonproduction environment and then move to production. It is a good practice to patch your production
systems within 7 days of a patch release. In critical scenarios where there is known exploit code for a remote
code execution vulnerability, you should deploy patches to your production systems the day of the patch
release to maximize security.
 Reporting on patch compliance. Even if you might have an automatic patch distribution method, you
need a way to assess your overall compliance. Do 100% of your computers have the patch? Or 90%? Which
specific computers are missing a specific patch? Reporting can be used by the management team to
evaluate the effectiveness of a patch management system.
 Automatic rollback capabilities. Sometimes, vendors release patches that create problems or have
incompatibilities. Those issues might not be evident immediately but instead show up days later. Ensure
you have an automated way of rolling back or removing the patch across all computers. You don’t want to
figure that out a few minutes before you need to do it.
 Vulnerability management. A vulnerability is a way in which your environment is at risk of being compromised
or degraded. The vulnerability can be due to a missing patch. But it can also be due to a misconfiguration or other
factors. For example, when SHA-1 certificates were recently found to be vulnerable to attack, many companies
76
suddenly found themselves vulnerable and needed to take action (by replacing the certificates). Many vulnerability
management solutions can scan the environment looking for vulnerabilities. Such solutions complement, but do
not replace, patch management systems and other security systems (such as anti-virus or anti-malware systems).
Be aware of the following definitions:
 Zero-day vulnerability. A vulnerability is sometimes known about before a patch is available. Such zeroday vulnerabilities can sometimes be mitigated with an updated configuration or other temporary
workaround until a patch is available. Other times, no mitigations are available and you have to be
especially vigilant with logging and monitoring until the patch is available.
 Zero-day exploit. Attackers can release code to exploit a vulnerability for which no patch is available. These
zero-day exploits represent one of the toughest challenges for organizations trying to protect their
environments.
7.10 Understand and participate in change management processes
Change management represents a structured way of handling changes to an environment. The goals include providing a
process to minimize risk, improving the user experience, and providing consistency with changes. While many companies
have their own change management processes, there are steps that are common across most organizations:
 Identify the need for a change. For example, you might find out that your routers are vulnerable to a denial of
service attack and you need to update the configuration to remedy that.
 Test the change in a lab. Test the change in a non-production environment to ensure that the proposed change
does what you think it will. Also use the test to document the implementation process and other key details.
 Put in a change request. A change request is a formal request to implement a change. You specify the proposed
date of the change (often within a pre-defined change window), the details of the work, the impacted systems,
notification details, testing information, rollback plans and other pertinent information. The goal is to have enough
information in the request that others can determine whether there will be any impact to other changes or conflicts
with other changes and be comfortable moving forward. Many companies require a change justification for all
changes.
 Obtain approval. Often, a change control board (a committee that runs change management), will meet weekly
or monthly to review change requests. The board and the people that have submitted the changes meet to discuss
the change requests, ask questions and vote on approval. If approval is granted, you move on to the next step. If
not, you restart the process.
 Send out notifications. A change control board might send out communications about upcoming changes. In
some cases, the implementation team handles the communications. The goal is to communicate to impacted
parties, management and IT about the upcoming changes. If they see anything unusual after a change is made, the
notifications will help them begin investigating by looking at the most recent changes.
 Perform the change. While most companies have defined change windows, often on the weekend, sometimes a
change can’t wait for that window (such as an emergency change). During the change process, capture the existing
77
configuration, capture the changes and steps, and document all pertinent information. If a change is unsuccessful,
perform the rollback plan steps.
 Send out “all clear” notifications. These notifications indicate success or failure.
7.11 Implement recovery strategies
A recovery operation takes place following an outage, security incident or other disaster that takes an environment down
or compromises it in a way that requires restoration. Recovery strategies are important because they have a big impact on
how long your organization will be down or have a degraded environment, which has an impact on the company’s bottom
line. Note that this section focuses on strategies rather than tactics, so be thinking from a design perspective, not from a
day-day-day operational perspective.
 Backup storage strategies. While most organizations back up their data in some way, many do not have an official
strategy or policy regarding where the backup data is stored or how long the data is retained. In most cases, backup
data should be stored offsite. Offsite backup storage provides the following benefits:
 If your data center is destroyed (earthquake, flood, fire), your backup data isn’t destroyed with it. In some
cases, third-party providers of off-site storage services also provide recovery facilities to enable
organizations to recover their systems to the provider’s environment.
 Offsite storage providers provide environmentally sensitive storage facilities with high-quality
environmental characteristics around humidity, temperature and light. Such facilities are optimal for longterm backup storage.
 Offsite storage providers provide additional services that your company would have to manage otherwise,
such as tape rotation (delivery of new tapes and pickup of old tapes), electronic vaulting (storing backup
data electronically), and organization (cataloging of all media, dates and times).
 Recovery site strategies. When companies have multiple data centers, they can often use one as a primary data
center and one another as a recovery site (either a cold standby site or a warm standby site). An organization with
3 or more data centers can have a primary data center, a secondary data center (recovery site) and regional data
centers. With the rapid expansion of public cloud capabilities, having a public cloud provider be your recovery site
is feasible and reasonable. One key thing to think about is cost. While cloud storage is inexpensive and therefore
your company can probably afford to store backup data there, trying to recover your entire data center from the
public cloud might not be affordable or fast enough.
 Multiple processing sites. Historically, applications and services were highly available within a site such as a data
center, but site resiliency was incredibly expensive and complex. Today, it is common for companies to have
multiple data centers, and connectivity between the data centers is much faster and less expensive. Because of
these advances, many applications provide site resiliency with the ability to have multiple instances of an
application spread across 3 or more data centers. In some cases, application vendors are recommending backupfree designs in which an app and its data are stored in 3 or more locations, with the application handling the multisite syncing. The public cloud can be the third site, which is beneficial for companies that lack a third site or that
have apps and services already in the public cloud.
78
 System resilience, high availability, quality of service (QoS) and fault tolerance. To prepare for the exam, it is
important to know the differences between these related terms:
 System resilience. Resilience is the ability to recover quickly. For example, site resilience means that if Site
1 goes down, Site 2 quickly and seamlessly comes online. Similarly, with system resilience, if a disk drive
fails, another (spare) disk drive is quickly and seamlessly added to the storage pool. Resilience often comes
from having multiple functional components (for example, hardware components).
 High availability. While resilience is about recovering with a short amount of downtime or degradation,
high availability is about having multiple redundant systems that enable zero downtime or degradation for
a single failure. For example, if you have a highly available database cluster, one of the nodes can fail and
the database cluster remains available without an outage or impact. While clusters are often the answer
for high availability, there are many other methods available too. For instance, you can provide a highly
available web application by using multiple web servers without a cluster. Many organizations want both
high availability and resiliency.
 Quality of service (QoS). QoS is a technique that helps enable specified services to receive a higher quality
of service than other specified services. For example, on a network, QoS might provide the highest quality
of service to the phones and the lowest quality of service to social media. QoS has been in the news because
of the net neutrality discussion taking place in the United States. The new net neutrality law gives ISPs a
right to provide higher quality of services to a specified set of customers or for a specified service on the
internet. For example, an ISP might opt to use QoS to make its own web properties perform wonderfully
while ensuring the performance of its competitors’ sites is subpar.
 Fault tolerance. As part of providing a highly available solution, you need to ensure that your computing
devices have multiple components — network cards, processors, disk drives, etc. —of the same type and
kind to provide fault tolerance. Fault tolerance, by itself, isn’t valuable. For example, imagine a server with
fault-tolerant CPUs. The server’s power supply fails. Now the server is done even though you have fault
tolerance. As you can see, you must account for fault tolerance across your entire system and across your
entire network.
7.12 Implement disaster recovery (DR) recovery processes
Trying to recover from a disaster without a documented disaster recovery processes is difficult, if not impossible. Thus, you
should establish clear disaster recovery processes to minimize the effort and time required to recover from a disaster.
Testing the plans is also important and is discussed separately in the next section (7.13).
 Response. When you learn about an incident, the first step is to determine whether it requires a disaster recovery
procedure. Timeliness is important because if a recovery is required, you need to begin recovery procedures as
soon as possible. Monitoring and alerting play a big part in enabling organizations to respond to disasters faster.
 Personnel. In many organizations, there is a team dedicated to disaster recovery planning, testing and
implementation. They maintain the processes and documentation. In a disaster recovery scenario, the disaster
recovery team should be contacted first so they can begin communicating to the required teams. In a real disaster,
communicating with everybody will be difficult and, in some cases, not possible. Sometimes, companies use
79
communication services or software to facilitate emergency company-wide communications or mass
communications with personnel involved in the disaster recovery operation.
 Communications. There are two primary forms of communication that occur during a disaster recovery operation,
as well as a third form of communication that is sometimes required:
 Communications with the recovery personnel. In many disaster scenarios, email is down, phones are
down, and instant messaging services are down. If the disaster hasn’t taken out cell service, you can rely on
communications with smart phones (SMS messages, phone calls).
 Communications with the management team and the business. As the recovery operation begins, the
disaster recovery team must stay in regular contact with the business and the management team.
The business and management team need to understand the severity of the disaster and the approximate
time to recover. As things progress, they must be updated regularly.
 Communications with the public. In some cases, a company experiencing a large-scale disaster must
communicate with the public, for example, a service provider, a publicly traded company, or a provider of
services to consumers. At a minimum, the communication must indicate the severity of the incident, when
service is expected to resume, and any actions consumers need to take.
 Assessment. During the response phase, the teams verified that recovery procedures had to be initiated. In the
assessment phase, the teams dive deeper to look at the specific technologies and services to find out details of the
disaster. For example, if during the response phase, the team found email to be completely down, then they might
check to find out if other technologies are impacted along with email.
 Restoration. During the restoration phase, the team performs the recovery operations to bring all services back
to their normal state. In many situations, this means failing over to a secondary data center. In others, it might
mean recovering from backups. After a successful failover to a secondary data center, it is common to start
planning the failback to the primary data center once it is ready. For example, if the primary data center flooded,