As illustrated in Table 2, we compare the accuracy of LogPunk with eight other baseline log parsers on 16 log datasets. According to the row, we can compare the PA of different log parsers on the same dataset. Moreover, the column demonstrates the accuracy distribution of the same log parser across datasets. Following the prior work [15], PA values greater than 0.9 are highlighted in bold, and the best PA values of each dataset are marked with asterisk “*”.Appl. Sci. 2021, 11, 11974 9 of 15
From Table 2, we can observe that LogPunk achieves the best accuracy on ten datasets out of 16, which significantly outperforms other baseline methods. In addition to that, LogPunk achieves over 0.9 accuracy on 13 datasets. In the remaining datasets, LogPunk also has a comparable accuracy. On average, LogPunk has the best accuracy of 0.919, followed by Drain+ of 0.877.In addition, Spell+ and Drain+ are better than the original Spell and Drain. Their accuracy is improved by 0.055 and 0.011, respectively, which shows that our signature method is effective and can be applied to other log parsers as an enhancement.The average accuracy of the HDFS and Apache datasets almost reaches 100%. Because HDFS and Apache logs have relatively simple structures, and the number of templates is small. Meanwhile, some datasets could not be parsed accurately due to their complex structure and abundant event templates, such as Mac and Linux. All log parsers perform poorly on the Proxifier dataset because it is skewed. It only has eight templates in the sample set, while template E8 takes up 947 out of 2000, and it is not easy to parse.Table 2. Accuracy of log parsers on different datasets.
| Dataset | IPLoM | LenMa | AEL | Spell | Spell+ | Drain | Drain+ | LogPunk | Best |
|---|---|---|---|---|---|---|---|---|---|
| Android |0.712 |0.88 |0.682 |0.919 |0.922 |0.911 |0.913 |0.936 * |0.936 |
| Apache |1 * |1 * |1 * |1 * |1 * |1 * |1 * |1 * |1 |
| BGL |0.939 |0.69 |0.957 |0.787 |0.822 |0.963 |0.97 |0.979 * |0.979 || Hadoop |0.954 |0.885 |0.869 |0.778 |0.795 |0.948 |0.949 |0.992 * |0.992 |
| HDFS |1 * |0.998 |0.998 |1 * |0.998 |0.998 |0.998 |0.998 |1 |
| HealthApp |0.822 |0.174 |0.568 |0.639 |0.686 |0.78 |0.78 |0.901 * |0.901 |
| HPC |0.829 |0.83 |0.903 |0.654 |0.898 |0.887 |0.926 |0.939 * |0.939 |
| Linux |0.672 |0.701 |0.673 |0.605 |0.739 |0.69 |0.749 * |0.741 |0.749 |
| Mac |0.671 |0.698 |0.764 |0.757 |0.804 |0.787 |0.858 * |0.852 |0.858 || OpenSSH |0.54 |0.925 |0.538 |0.554 |0.803 |0.788 |0.788 |0.995 * |0.995 |
| OpenStack |0.331 |0.743 |0.758 |0.764 |0.764 |0.733 |0.733 |1 * |1 |
| Proxifier |0.517 |0.508 |0.495 |0.527 * |0.527 * |0.527 * |0.527 * |0.504 |0.527 |
| Spark |0.92 |0.884 |0.905 |0.905 |0.905 |0.92 |0.92 |0.923 * |0.923 |
| Thunderbird |0.663 |0.943 |0.941 |0.844 |0.95 |0.955 |0.955 * |0.951 |0.955 || Windows |0.567 |0.566 |0.69 |0.989 |0.99 |0.997 * |0.997 * |0.996 |0.997 |
| Zookeeper |0.962 |0.841 |0.921 |0.964 |0.964 |0.967 |0.967 |0.995 * |0.995 |
| Average |0.756 |0.767 |0.791 |0.793 |0.848 |0.866 |0.877 |0.919 |N.A. |LogPunk has the best accuracy for the following reasons. First, our effective signature method with a collision index of 1.25 avoids template crowding in one signature group. Even though the whole template set is complex, the situation is much simpler in each signature group. Second, adequate delimiters ensure the separation of variables and templates to deal with some complex situations. Third, we provide two hyperparameters (cf. Section 3.3. similarity threshold and prefix threshold), which can be set flexibly according to the data feature.4.3. Robustness
In this part, we evaluate the robustness of log parsers on different datasets. Figure 3 shows the accuracy distribution of each log parser across the 16 log datasets in the boxplot. Each box has five horizontal lines from the bottom to the top, corresponding to the mini-mum, 25th percentile, median, 75th percentile, and maximum accuracy values, respectively. Diamond marks indicate outliers since LenMa only has an accuracy of 0.174 on HealthApp, and LogPunk gets 0.504 on Proxifier and 0.741 on Linux. Although LogPunk seems toAppl. Sci. 2021, 11, 11974 10 of 15
perform poorly on Proxifier and Linux datasets, other methods do not work well either, and we have even achieved the highest accuracy on Linux.For comparison, the log parser is arranged in ascending order of the average precision from left to right. We can observe that LogPunk has the highest average accuracy and the minimum variance, which means robustness. We also observed that Spell+ is more robust than Spell, and Drain+ and Drain have little difference in robustness. Because Drain has some strong assumptions, the preceding tokens of the same log type are the same. Such assumptions are the main reason for Drain’s error in accuracy.|  |  | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
 |
|---|---|---|---|---|---|---|---|---|---|
|  | |  |  |  |  |  |  |  |  |
|  | |  |  |  |  |  |  |  |  |
|  | |  |  |  |  |  |  |  |  |
|  | |  |  |  |  |  |  |  |  |
|  | | | | | | | | | || Appl. Sci. 2021, 11, 11974 | Appl. Sci. 2021, 11, 11974 | Appl. Sci. 2021, 11, 11974 |  |  |  |  |  |  |  |  |  |  |  |  |  |  | 11 of 15 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | || ! |! |! | | | | | | !! | | | | | | | |" | |
|  | | | | | | | | !! | | | | | | | |  | |
|  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | ||  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | |
|    | | | | | | | | !! | | | | | | | | | ||  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | |
|  | | | | | | | | !! | | | | | | | | | |Figure 4. Parsing time of log parsers on different volumes of logs.
5. Discussion
In this section, we compare LogPunk with tree-based methods and discuss the validity.Comparison with tree-based methods. As mentioned in Section 2, tree-based meth-ods have two main problems: (1) needs to maintain a complex tree structure and updates the tree frequently as templates change. The updating process is time-consuming and complex; (2) does not guarantee the longest common subsequence (due to the limitation of the prefix tree itself). Unlike tree-based approaches, LogPunk works in a hash-like manner. Once a log signature is calculated, we can immediately locate the corresponding signature group. What we need to maintain is only a mapping relationship from the log signature to the signature group. This solution with a simple data structure brings more efficiency and accuracy improvements to LogPunk.Moreover, the invariance of log signature also brings potential benefits of parallel computing. Since most of the current log parsing methods are single-threaded, multi-threaded log parsers will greatly improve the efficiency of log parsing.External validity. LogPunk achieves a parsing accuracy of over 0.9 on more than half of the evaluated datasets. We cannot ensure that LogPunk can achieve the same high accuracy on other log datasets not tested in this work. However, it should also be noted that our evaluated log datasets come from systems in different domains. LogPunk is superior to other log parsers not only in accuracy, but also in efficiency. A future work is to verify the effectiveness of LogPunk on more types of log data and apply LogPunk to the practical log dataset from the production environment.Internal validity. The internal threat to validity mainly lies in the implementations of LogPunk and compared approaches. To reduce this threat, the implementation of LogPunk was completely based on the most primitive Python library and did not use any third-party library. We have also carefully examined the source code. Regarding compared approaches, we adopted their open-source implementations from the LogPai benchmark [15] directly.6. Related Work
Log parsing plays an important role in automatic log analysis and has been widely studied in recent years. Generally, log parsing approaches are divided into three categories: rule-based, source code-based, and data-driven parsing [17]. In this work, we focus on data-driven log parsing approaches. The advantage of these approaches is that they do not rely on application-specific knowledge heavily. In general, existing data-driven log parsing techniques could be grouped under three categories: frequent pattern mining, clustering, and heuristics [15].(1) Frequent Pattern Mining: SLCT [19], LFA [20], and LogCluster [31] propose automated log parsers that parse log messages by mining the frequent tokens in log files. These approaches first count token frequencies and then use a predefined threshold to identify
Appl. Sci. 2021, 11, 11974 12 of 15the static parts of log messages. The intuition is that if a log event occurs frequently, then the static template parts will occur more times than the dynamic parts from variables. SLCT applies frequent pattern mining to log parsing for the first time. LFA utilizes the token frequency in each log message instead of the whole log data to parse infrequent logs. LogCluster improves SLCT and is robust to shifts in token position.All the above three methods are offline and need to traverse all log data to count the token frequency. In contrast, LogPunk is an online log parser.
(2) Clustering: Many previous studies regard log parsing as a clustering problem and propose many clustering approaches to solve this problem. From this perspective, log messages sharing the same templates are grouped into one cluster and various approaches to measure the similarity (or distance) between two log messages have been proposed. LKE [32], LogSig [21], and LogMine [22] propose offline clustering methods.LKE employs a k-means clustering algorithm based on weighted edit distance to extract log events from free text messages. LogSig groups log messages with the same frequent subsequence into a predefined number of clusters. LogMine clusters log messages from bottom to top and identifies the most suitable log template to represent each cluster.SHISO [24] and LenMa [25] are both online methods. SHISO employs Euclidean distance to measure the similarity between logs and generate a score. If the score is smaller than the pre-defined threshold, SHISO makes a cluster of the similar two.LenMa proposes an online clustering method using the length information of each word in log messages. Additionally, it measured the similarity between two log messages based on cosine similarity, to determine which cluster the new coming log message should be added to. Despite performing well on test datasets, these two methods perform poorly on public datasets. To ensure robustness, LogPunk has been tested on 16 datasets from different systems.(3) Heuristics: Different from general text data, log messages have some unique character-istics, which can be used for log parsing. AEL [29] uses heuristics based on domain knowledge to identify dynamic parts (e.g., tokens following “is” or “are”) in log mes-sages, then clusters log messages into the same template set if they have the same structure of dynamic parts. IPLoM [23] iteratively partitions log messages into finer clusters, firstly by the number of tokens, then by the position of tokens, and lastly by the association between token pairs. Spell [26] supposes that template tokens often take most of the log message, and variable tokens take only a small portion. So, it utilizes an LCS-based approach to measure log similarity and to find the most similar template.Drain [17] uses a fixed-depth tree to parse logs. Each layer encodes specially designed rules for log parsing. In the first layer, Drain searches by log message length, and in the following layers, searches by preceding tokens. By doing so, log messages with the same length and preceding tokens are clustered into the same groups placed on the leaf nodes. The tree-based Spell and Drain outperform other methods in the previous benchmark [15] and are state-of-the-art log parsers at present. LogPunk overcomes the defect of tree structures (cf. Section 2) and parses logs in a hash-like manner.7. Conclusions
A qualified general-purpose online log parser should be robust and efficient, which meets the reliability requirements of modern software systems. At present, the state of the art online log parsers are tree-based. Such a structure brings low robustness and inefficiency. To overcome these limitations, we get inspiration from log punctuations and propose a hash-like method.This paper proposed LogPunk, a robust and efficient log parser based on our novel log punctuations signature method. The candidate signature group is quickly located according to the log signature, which improves the efficiency. To solve the problem of signature collision, we design the log similarity function to find the most similar template, which improves the robustness.
Appl. Sci. 2021, 11, 11974 13 of 15Experiments are conducted on 16 public log datasets. Our experimental results show that LogPunk obtains the highest accuracy on ten datasets out of 16 datasets. Especially, LogPunk achieves the best average accuracy of 0.919 among the other five baseline log parsers. In addition, experiments also show that LogPunk is robust and efficient. 	Finally, some future works are analyzed as follows.(1) 	Automated parameters tuning. Logpunk has two hyperparameters similarity threshold 	and prefix threshold (cf. Section 3.3). During the experiment, these two hyperparameters 	are fine-tuned manually. This process is time-consuming, and the obtained hyperpa-	rameters may not be optimal. A mechanism for automated parameters tuning can 	greatly improve this situation.(2) 	Punctuation table generation. The punctuation table (cf. Section 3.2) determines the 	log signature and affects the whole log parsing process. We presented a punctuation 	table by eliminating the punctuations appearing in variables, and it performs well on 	the 16 evaluated datasets. For a new system, if we customize a punctuation table for 	it, we may get better log parsing results. It is desirable to find a way to generate a 	customized punctuation table automatically for an unknown system.(3) 	Variable type identification. Existing log parsers treat all variables in the parsing result 	as strings. However, obviously, each variable has its specific type information (e.g., 	number, IP, URL, file path, etc.) and it is useful to detect the variable-related anomaly.
If log parsing not only identifies variable but also variable types, it will bring more initiative to downstream tasks.Author Contributions: Conceptualization, S.Z. and G.W.; methodology, S.Z.; software, S.Z.; vali-dation, S.Z.; investigation, S.Z.; resources, S.Z.; writing—original draft preparation, S.Z.; writing—review and editing, G.W.; visualization, S.Z.; supervision, G.W.; project administration, G.W.; funding acquisition, G.W. All authors have read and agreed to the published version of the manuscript.Funding: This research received no external funding.
Data Availability Statement: Publicly available datasets were analyzed in this study. This data can be found here: [].
Conflicts of Interest: The authors declare no conflict of interest.
Abbreviations
The following abbreviations are used in this manuscript: