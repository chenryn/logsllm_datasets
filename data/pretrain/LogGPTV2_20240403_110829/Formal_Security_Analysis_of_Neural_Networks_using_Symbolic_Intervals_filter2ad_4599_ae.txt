Adv Range
[−0.2,−0.186] and [−0.103,0]
[6402.36,10000]
[−0.1,0.0085]
Adv
98229
18121
17738
Timeout Non-adv
163915
14645
15029
1
2
1
Table 3: The second column shows the input ranges con-
taining at least one adversarial input, while the rest of
ranges are found by ReluVal to be non-adversarial. The
last three columns show the number of total sub-intervals
checked by ReluVal with a precision of e− 6.
ent randomly picked seed inputs to facilitate the input
generation process. Note that our technique in ReluVal
does not need any seed input. Thus it is not restricted
by the potentially undesired starting seed and can fully
explore the input space. As shown in Table 2, on aver-
age, CW misses 61.2% number of models, which do have
adversarial inputs exist that CW fails to ﬁnd.
Narrowing down adversarial ranges. A unique fea-
ture of ReluVal is that it can isolate adversarial ranges
of inputs from the non-adversarial ones. This is use-
ful because it allows a DNN designer to potentially iso-
late and avoid adversarial ranges with a given precision
(e.g., e− 6 or smaller). Here we set the precision to be
e− 6, i.e., we allow splitting of the intervals into smaller
sub-intervals unless their length becomes less than e− 6.
Table 3 shows the results of the three different proper-
ties that we checked. For example, property S1 speciﬁes
model_4_1 should output strong right with input range
ρ = [400,10000], θ = 0.2, ψ = −3.09, vown = 10, and
vint = 10. For this property, ReluVal splits the input ranges
into 262,144 smaller sub-intervals and is able to prove
that 163,915 sub-intervals are safe. ReluVal also ﬁnds
that ρ = [400,6402.36] does not contain any adversarial
inputs while ρ = [6402.36,10000] is adversarial.
7.3 Preliminary Tests on MNIST Model
Besides ACAS Xu, we also test ReluVal on an MNIST
model that achieves decent accuracy (98.28%). Given a
particular seed image, we allow arbitrary perturbations to
every pixel value while bounding the total perturbation
by the L∞ norm. In particular, ReluVal can prove 956
seed images to be safe for |X|∞ ≤ 1 and 721 images safe
for |X|∞ ≤ 2 respectively out of 1000 randomly selected
test images. Figure 7 shows the detailed results. As the
norm is increased, the percentage of images that have
no adversarial perturbations drops quickly to 0. Note
that we get more timeouts as the L∞ norm increase. We
believe that we can further optimize our system to work
on GPUs to minimize such timeouts and verify properties
USENIX Association
27th USENIX Security Symposium    1609
with larger norm bounds.
by 16.91% on average, especially when the average depth
is high.
8 Related Work
Adversarial machine learning. Several recent works
have shown that even the state-of-the-art DNNs can be
easily fooled by adding small carefully crafted human-
imperceptible perturbations to the original inputs [7, 16,
37, 48]. This has resulted in an arms race among re-
searchers competing to build more robust networks and
design more efﬁcient attacks [7, 16, 31, 32, 39, 41, 51].
However, most of the defenses are restricted to only one
type of adversaries/security properties (e.g., overall per-
turbations bounded by some norms) even though other
researchers have shown that other semantics-preserving
changes like lightning changes, small occlusions, rota-
tions, etc. can also easily fool the DNNs [13, 42, 43, 49].
However, none of these attacks can provide any prov-
able guarantees about the non-existence of adversarial
examples for a given neural network. Unlike these at-
tacks, ReluVal can provide a provable security analysis of
given input ranges, systematically narrowing down and
detecting all adversarial ranges.
Veriﬁcation of machine learning systems. Recently,
several projects [12, 21, 25] have used customized SMT
solvers for verifying security properties of DNNs, such
However, such techniques are mostly limited by the scal-
ability of the solver. Therefore, they tend to incur sig-
niﬁcant overhead [25] or only provide weaker guaran-
tees [21]. By contrast, ReluVal uses interval-based tech-
niques and signiﬁcantly outperform the state-of-the-art
solver-based systems like ReluPlex [25].
Kolter et al. [29] and Raghunathan et al. [44] transform
the veriﬁcation problem into a convex optimization prob-
lem using relaxations to over-approximate the outputs of
ReLU nodes. Similarly, Gehr et al. [14] leverages zono-
topes for approximating each ReLU outputs. Dvijotham
et al. [11] transformed the veriﬁcation problem into an
unconstrained dual formulation using Lagrange relaxation
and use gradient-descent to solve the optimization prob-
lem. However, all of these works focus on simply over-
approximating the total number of potential adversarial vi-
olations without trying to ﬁnd concrete counterexamples.
Therefore, they tend to suffer from high false positive
rates unless the underlying DNN’s training algorithm is
modiﬁed to minimize such violations. By contrast, Relu-
Val can ﬁnd concrete counterexamples as well as verify
security properties of pre-trained DNNs.
Recently, Mixed Integer Linear programming (MILP)
solvers combined with gradient descent have also been
proposed for veriﬁcation of DNNs [9, 10]. Integrating
our interval analysis together with such approaches is an
interesting future research problem.
Figure 7: Percentage of images proved to be not adver-
sarial with L∞ = 1,2,3,4,5 by ReluVal on MNIST test
model out of 1000 random test MNIST images.
7.4 Optimizations
In this subsection, we evaluate the effectiveness of the
optimizations proposed in Section 5 compared to the naive
interval extension with iterative interval reﬁnement. The
results are shown in Table 4.
Methods
Deepest Dep (%)
Avg Dep (%)
Time (%)
S.C.P
I.A.
Mono
42.06
10.65
0.325
49.28
10.85
0.497
99.99
96.04
16.91
Table 4: The percentages of the deepest depth, average
depth, and average running time improvement caused by
the three main components of ReluVal: symbolic interval
analysis, inﬂuence analysis, and monotonicity compared
to the naive interval analysis.
Symbolic interval propagation. Table 4 shows that sym-
bolic interval analysis saves the deepest and average depth
of bisection tree (Figure 5) by up to 42.06% and 49.28%,
respectively, over naive interval extension.
Inﬂuence analysis. As one of the optimizations used in
iterative reﬁnement, inﬂuence analysis helps prioritize
splitting of the the most inﬂuential inputs to the output.
Compared to the sequential splitting features, inﬂuence-
analysis-based splitting reduces the average depth by
10.85% and thus cut down the running time by up to
96.04%.
Monotonocity. The improvements from using mono-
tonicity are relatively smaller in terms of tree depth.
However, it can still reduce the average running time
2We remove model_4_2 and model_5_3 because Reluplex found
incorrect adversarial examples due to roundup problems (these models
do not have any adversarial cases).
1610    27th USENIX Security Symposium
USENIX Association
Verivis [43], by Pei et al. is a black-box DNN veriﬁca-
tion system that leverage the discreteness of image pixels.
However, unlike ReluVal, it cannot verify non-existence
of norm-based adversarial examples.
Interval optimization. Interval analysis has shown great
success in many application domains including non-
linear equation solving and global optimization prob-
lems [23, 34, 35]. Due to its ability to provide rigorous
bounds on the solutions of an equation, many numerical
optimization problems [22,50] leveraged interval analysis
to achieve a near-precise approximation of the solutions.
We note that the computation inside NN is mostly a se-
quence of simple linear transformations with a nonlinear
activation function. These computations thus highly re-
semble those in traditional domains where interval anal-
ysis has been shown to be successful. Therefore, based
on the foundation of interval analysis laid by Moore et
al. [36, 45], we leverage interval analysis for analyzing
the security properties of DNNs.
9 Future Work and Discussion
Supporting other activation functions. Interval exten-
sion can, in theory, be applied to any activation function
that maintains inclusion isotonicity and Lipschitz conti-
nuity. As mentioned in Section 4, most popular activation
functions (e.g., tanh, sigmoid) satisfy these properties.
To support these activation functions, we need to adapt
the symbolic interval propagation process. We plan to
explore this as part of future work. Our current prototype
implementation of symbolic interval propagation supports
several common piece-wise linear activation functions
(e.g., regular ReLU, Leaky ReLU, and PReLU).
Supporting other norms besides L∞. While interval
arithmetic is most immediately applicable to L∞, other
norms (e.g., L2 and L1) can also be approximated using
intervals. Essentially, L∞ allows the most ﬂexible pertur-
bations and the perturbations bounded by other norms like
L2 are all subsets of those allowed by the corresponding
L∞ bound. Therefore, if ReluVal can verify the absence of
adversarial examples for a DNN within an inﬁnite norm
bound, the DNN is also guaranteed to be safe for the corre-
sponding p-norm (p=1/2/3..) bound. If ReluVal identiﬁes
adversarial subintervals for an inﬁnite norm bound, we
can iteratively check whether any such subinterval lies
within the corresponding p-norm bound. If not, we can
declare the model to contain no adversarial examples for
the given p-norm bound. We plan to explore this direction
in future.
Improving DNN Robustness. The counterexamples
found by ReluVal can be used to increase the robustness
of a DNN through adversarial training. Speciﬁc, we can
add the adversarial examples detected by ReluVal to the
training dataset and retrain the model. Also, a DNN’s
training process can further be changed to incorporate
ReluVal’s interval analysis for improved robustness. In-
stead of training on individual samples, we can convert
the training samples into intervals and change the training
process to minimize losses for these intervals instead of
individual samples. We plan to pursue this direction as
future work.
10 Conclusion
Although this paper focuses on verifying security proper-
ties of DNNs, ReluVal itself is a generic framework that
can efﬁciently leverage interval analysis to understand
and analyze the DNN computation. In the future, we
hope to develop a full-ﬂedged DNN security analysis tool
based on ReluVal, just like traditional program analysis
tools, that can not only efﬁciently check arbitrary security
properties of DNNs but can also provide insights into the
behaviors of hidden neurons with rigorous guarantees.
In this paper, we designed, developed, and evaluated
ReluVal, a formal security analysis system for neural net-
works. We introduced several novel techniques including
symbolic interval arithmetic to perform formal analysis
without resorting to SMT solvers. ReluVal performed
200 times faster on average than the current state-of-art
solver-based approaches.
11 Acknowledgements
We thank Chandrika Bhardwaj, Andrew Aday, and the
anonymous reviewers for their constructive and valu-
able feedback. This work is sponsored in part by NSF
grants CNS-16-17670, CNS-15-63843, and CNS-15-
64055; ONR grants N00014-17-1-2010, N00014-16-1-
2263, and N00014-17-1-2788; and a Google Faculty Fel-
lowship. Any opinions, ﬁndings, conclusions, or recom-
mendations expressed herein are those of the authors, and
do not necessarily reﬂect those of the US Government,
ONR, or NSF.
References
[1] Baidu Apollo Autonomous Driving Platform. https://
github.com/ApolloAuto/apollo.
[2] NASA, FAA, Industry Conduct Initial Sense-and-Avoid
https://www.nasa.gov/centers/armstrong/
Test.
Features/acas_xu_paves_the_way.html.
[3] NAVAIR plans to install ACAS Xu on MQ-4C ﬂeet.
https://www.flightglobal.com/news/articles/
navair-plans-to-install-acas-xu-on-mq-4c-
fleet-444989/.
[4] T. Ball and S. K. Rajamani. The SLAM project: debugging
system software via static analysis. In ACM SIGPLAN
Notices, volume 37, pages 1–3. ACM, 2002.
USENIX Association
27th USENIX Security Symposium    1611
[5] C. Bloom, J. Tan, J. Ramjohn, and L. Bauer. Self-driving
cars and data collection: Privacy perceptions of networked
autonomous vehicles. In Symposium on Usable Privacy
and Security (SOUPS), 2017.
[6] N. Carlini, G. Katz, C. Barrett, and D. L. Dill. Provably
minimally-distorted adversarial examples. arXiv preprint
arXiv:1709.10207, 2017.
[7] N. Carlini and D. Wagner. Towards evaluating the robust-
ness of neural networks. In IEEE Symposium on Security
and Privacy, pages 39–57. IEEE, 2017.
[8] L. H. De Figueiredo and J. Stolﬁ. Afﬁne arithmetic: con-
cepts and applications. Numerical Algorithms, 37(1):147–
158, 2004.
[9] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari.
Learning and veriﬁcation of feedback control systems us-
ing feedforward neural networks. In IFAC Conference on
Analysis and Design of Hybrid Systems (ADHS), 2018.
[10] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Out-
put range analysis for deep feedforward neural networks.
In NASA Formal Methods Symposium, pages 121–138.
Springer, 2018.
[11] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and
P. Kohli. A dual approach to scalable veriﬁcation of deep
networks. arXiv preprint arXiv:1803.06567, 2018.
[12] R. Ehlers. Formal veriﬁcation of piece-wise linear feed-
In International Symposium
forward neural networks.
on Automated Technology for Veriﬁcation and Analysis
(ATVA), pages 269–286. Springer, 2017.
[13] L. Engstrom, D. Tsipras, L. Schmidt, and A. Madry. A
rotation and a translation sufﬁce: Fooling cnns with simple
transformations. arXiv preprint arXiv:1712.02779, 2017.
[14] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov,
S. Chaudhuri, and M. Vechev. Ai 2: Safety and robustness
certiﬁcation of neural networks with abstract interpretation.
In Security and Privacy (SP), 2018 IEEE Symposium on,
2018.
[15] D. Goldberg. What every computer scientist should know
about ﬂoating-point arithmetic. ACM Computing Surveys