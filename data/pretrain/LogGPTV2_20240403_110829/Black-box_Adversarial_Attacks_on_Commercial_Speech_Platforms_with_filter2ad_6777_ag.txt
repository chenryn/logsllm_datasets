Google’s Speech-to-Text API, we select the “command_and_search
model” as the target models. The characteristics of these APIs are
listed in Table 2. Note that although some APIs provide confidence
scores, our attack does not require such information.
Commercial Speaker Recognition Systems. We test Occam on
the Microsoft Azure’s speaker recognition system and the Jing-
dong speaker recognition system [9]. Microsoft Azure’s API [6] can
perform speaker identification (who is the speaker) and speaker
verification (whether the speaker is legal). It only returns the deci-
sion (i.e., the predicted speaker) along with three confidence levels
(i.e., low, normal, or high). Jingdong’s API can perform speaker
verification and only returns the final result, i.e., accept or reject.
Commercial Voice Control Devices. We test NI-Occam on five
commercial voice control devices, i.e., Apple Siri, iFlytek, Microsoft
Cortana, Google Assistant, Amazon Echo. In our experiments, Ap-
ple Siri, iFlytek, Microsoft Cortana, Google Assistant are applica-
tions installed in on-the-shelf smartphones, and Amazon Echo is
an intelligent voice-controlled speaker.
C.3 Hardware
In our experiments, we conduct the attacks against DeepSpeech on
a server equipped with four Nvidia 2080Ti GPUs, a six-core Intel(R)
Xeon(R) W-2133 CPU 3.60GHz, 62 Gigabyte RAM, and a Hard Drive
with 1.37 Terabytes. For the experiment on speech API services,
we adopt several laptops, including a Lenovo ThinkPad X1 Carbon
4th with Core Intel i5-6200U CPU 2.30GHz and 8 Gigabyte RAM,
a Microsoft Corporation Surface Pro 6 with Core Intel i7-8650U
CPU 1.90GHz and 8 Gigabyte RAM, and a Lenovo ThinkPad X1
Carbon 5th with Core Intel i7-7500U CPU 2.70GHz and 8 Gigabytes
RAM. Besides, we attack the voice assistant including Apple Siri
with version 13.6.1 and iFlytek with version 10.0.8 on an iPhone 11,
Cortana with version 3.3.3 on a Samsung C9000 with version 3.3.3,
and Google Assistant with version 2.5.1 on a Nokia 7 plus. We play
the audio AEs using a JBL Clip 3 portable speaker.
(a) Common Voice
(b) Song
Figure 8: SNRs of the audio AEs after querying DeepSpeech
for different times over (a) Common Voice and (b) Song.
D SUPPLEMENTARY EVALUATION OF
OCCAM
D.1 Evaluation on Open-source ASR Systems
We evaluate the effectiveness of the audio AEs generated by the
attacks in Table 8. These AEs are generated after 200,000 queries
(100,000 times on Song13) on DeepSpeech. Since Genetic Algorithm-
based Attack (GAA) [80] and Selective Gradient Estimation Attack
(SGEA) [82] need to leverage the prediction scores of the model,
the AEs achieve higher SNRs than other decision-based attacks.
However, neither of them can successfully attack DeepSpeech with
a 100% SRoA-ASR. In the other six decision-based attacks, the initial
example is originally adversarial and then trained to approach the
target example. Therefore, the SRoA of these attacks is always 100%.
However, it is worth noting that SRoA is not the only metric that
determines whether an AE can successfully attack the target system.
An effective AE needs to fool both the model and the human, where
SNR must be used as another important factor to measure the
effectiveness of AEs. Although the six decision-based attacks can
all achieve a 100% SRoA-ASR, only the AEs generated by our attack
obtain high SNRs (with the best SNR of 13.80dB).
To evaluate the efficiency of Occam, we tested the SNRs of the
AEs generated after different numbers of queries on DeepSpeech.
As shown in Figure 8, Occam achieves a high SNR within a small
number of queries. Note that the dataset Song requires fewer queries
because the audio in Song is less noisy and more powerful, making
it easier to generate an audio AE with a high SNR. We also find that
the growth rate of SNR significantly decreases with the increase
of SNR. For example, the SNR of our AEs can quickly converge to
12.86dB, while the evolutionary attack may require hundreds of
thousands of queries to increase SNR from 1.20dB to 12.86dB. Hence,
the results show that the co-evolution algorithm has a higher bound
of SNR and a faster convergence rate than the evolution algorithm
on audio data, validating the effectiveness of the CC framework in
constructing audio AEs.
D.2 Evaluation on Cloud Speech APIs
For completeness, we give some additional experimental results as
a complement to Section 5.2.
13The original data from Song requires fewer queries because the signal power of the
songs is relatively strong.
Table 8: Experimental results on targeted attacks against DeepSpeech.
Dataset
Songs
LibriSpeech
Common Voice
Average
GAA
SGEA
SRoA SNR
23.78
1/10
14.72
1/10
17.69
1/10
1/10
18.73
SRoA SNR
21.62
4/10
14.60
7/10
19.14
8/10
6.3/10
18.45
Attack
Boundary
Attack
Opt-Attack Evolutionary
SNR
1.20
1.09
0.25
0.85
SRoA SNR SRoA SNR SRoA
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
8.03
4.80
3.41
5.41
2.88
2.93
2.01
2.61
HSJA
DEA
Occam
SRoA SNR SRoA SNR
-0.26
10/10
-1.41
10/10
-0.52
10/10
10/10
-0.73
10/10
10/10
10/10
10/10
5.88
1.94
1.78
3.2
SRoA
10/10
10/10
10/10
10/10
SNR
12.86
11.30
13.80
12.65
Table 9: Experimental results of untargeted attacks on commercial cloud speech-to-text APIs.
Model
Boundary Attack
Opt-Attack
Google STT
Microsoft ASR
Alibaba SSR
Tencent SSR
iFlytek
26.43
15.43
17.84
20.38
42.66
19.63
11.89
15.05
17.02
29.07
Evolutionary Attack HSJA DEA
16.31
11.08
17.49
18.03
26.61
31.20
27.37
27.85
30.16
38.59
30.04
19.60
20.66
21.51
39.82
Occam
31.43
28.68
29.57
31.68
40.46
Note that, (i) this table shows the SNRs of the audio AEs generated after 200 queries. (ii) This table does not show the success rate because all of the six attacks achieve a success
rate of 100%. (iii) The best and the second best results on each API are marked in bold with underling and bold font, respectively.
However, concerning the large-scale and complex problems of gen-
erating targeted audio AEs, these approaches become ineffective.
Besides, Devil’s Whisper does not explicitly support untargeted
attacks. To launch the untargeted attack by using the methodology
of Devil’s Whisper, the attacker needs to intentionally set a wrong
phrase as the target phrase. This step incurs a large amount of
queries. In contrast, our methodology can successfully generate
effective audio AEs within 200 queries.
Figure 9: SNRs of the AEs from targeted attacks against the
commercial services.
We present Figure 9 to show the SNRs with confidence intervals
of 68.2% (-std, +std). The results show that Occam has the highest
SNRs among all the decision-based attacks. Figure 10 shows the
waveforms and spectrograms of the original audio and the adver-
sarial audios generated by the seven attacks. It can be seen that
the waveforms of the original audio and the audio AEs generated
by Occam are almost the same. However, the differences in other
attacks are more noticeable and thus more likely to be perceived
by humans.
As for the untargeted attacks, the results in Table 9 show that the
SNRs of AEs generated from untargeted attacks are much higher
than those from targeted attacks. In this experiment, the perfor-
mance of the evolutionary attack is comparable to Occam. Since
the implementation of untargeted attacks is much simpler than
the targeted attack (e.g., 200 queries are enough), the optimization
problem on untargeted attacks can be easily solved without being
decomposed into a set of simpler sub-problems. Thus, approaches
without a cooperative co-evolution framework can also work well.
D.3 Evaluation on Human Perception
Similar to the human perception experiments on NI-Occam, we also
evaluate the performance of Occam about the human perception.
The experiment settings are the same as those in Section 5.3.
For each commercial service, the volunteers need to listen to 10
pieces of audio AEs generated from the boundary attack, the opt-
attack, the evolutionary attack, HSJA, DEA, Devil’s Whisper, and
Occam. The audio AEs are generated by querying the target systems
for 200,000 times with original audios from the dataset Song. In all,
we get 37×10 samples from each attack on each commercial service
(a total of 37×10×6 samples for one attack). Note that, since Devil’s
Whisper does not achieve a 100% SRoA, the volunteers only rate
the successful AEs. Each volunteer ranks 3, 5, 7, 8, and 4 successful
AEs from Devil’s Whisper on Alibaba, Google, iFlytek, Microsoft,
and Tencent, respectively.
Table 10 presents the results of the experiments on human per-
ception. Overall, Occam has the highest rate of “normal”. More than
70% volunteers think the audio generated from Occam is “normal”
or “noise”, which is comparable to Devil’s Whisper. However, about
50% AEs from Devil’s Whisper fail to fool the ASR systems in the
first place. The results show that Occam is more effective in fooling
both the model and the human than other possible attacks.
E A HUMAN STUDY ON AUDIO AES WITH
DIFFERENT SNRS
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)Table 10: Evaluation results on human perception of different attacks in the digital world.
Commercial Service
Method
Normal (%) Noise (%) Talking (%)
Once-
Twice-
recognize (%)
recognize (%)
Alibaba SSR
Google STT
iFlytek
Microsoft ASR
Tencent SSR
Average
Boundary Attack
DEA
Evolutionary Attack
HSJA
Opt-Attack
Devil’s Whisper
Occam
Boundary Attack
DEA
Evolutionary Attack
HSJA
Opt-Attack
Devil’s Whisper
Occam
Boundary Attack
DEA
Evolutionary Attack
HSJA
Opt-Attack
Devil’s Whisper
Occam
Boundary Attack
DEA
Evolutionary Attack
HSJA
Opt-Attack
Devil’s Whisper
Occam
Boundary Attack
DEA
Evolutionary Attack
HSJA
Opt-Attack
Devil’s Whisper
Occam
Boundary Attack
DEA
Evolutionary Attack
HSJA
Opt-Attack
Devil’s Whisper
Occam
26.76
6.76
2.70
8.65
4.59
8.11
50.54
4.59
0.54
3.51
5.14
6.76
18.38
40.00
3.51
0.00
5.14
1.62
3.51
17.76
22.16
13.51
8.11
1.08
1.08
4.05
29.05
31.08
29.73
1.08
14.59
18.65
11.62
12.84
45.41
15.62
3.30
5.41
7.03
6.11
17.23
37.84
23.24
23.24
32.16
37.30
29.73
51.35
35.41
21.89
21.35
17.30
15.68
12.70