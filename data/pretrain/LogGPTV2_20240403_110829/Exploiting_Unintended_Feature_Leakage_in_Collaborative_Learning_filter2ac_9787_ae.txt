AUC when all updates are shared.
B. Dimensionality reduction
As discussed in Section IV-B, if the input space of the
model is sparse and inputs must be embedded into a lower-
dimensional space, non-zero gradient updates in the embed-
ding layer reveal which inputs are present in the training batch.
One plausible defense is to only use inputs that occur many
times in the training data. This does not work in general, e.g.,
Figure 11 shows that restricting inputs to the top locations in
the FourSquare dataset eliminates most of the training data.
A smarter defense is to restrict the model so that it only uses
“words” from a pre-deﬁned vocabulary of common words. For
example, Google’s federated learning for predictive keyboards
uses a ﬁxed vocabulary of 5,000 words [35]. In Table VI,
we report the accuracy of our membership inference attack
(cid:24)(cid:17)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
Top N
words
4,000
2,000
1,000
500
CSI
Attack Model
AUC
0.91
0.87
0.85
0.84
Precision
0.94
0.92
0.92
0.82
Top N
locations
30,000
10,000
3,000
1,000
FourSquare
Attack Model
AUC
0.64
0.59
0.51
0.50
Precision
0.91
0.86
0.65
0.52
TABLE VI: Membership inference against the CSI Corpus and
FourSquare for different vocabulary sizes.
Dropout Prob.
0.1
0.3
0.5
0.7
0.9
Attack AUC Model AUC
0.87
0.87
0.87
0.86
0.84
0.94
0.97
0.98
0.99
0.99
TABLE VII: Inference of the top region (Antwerpen) on the CSI
Corpus for different values of dropout probability.
and the accuracy of the joint model on its main task—gender
classiﬁcation for the FourSquare dataset, sentiment analysis for
the CSI Corpus—for different sizes of the common vocabulary
(locations and words, respectively). This approach partially
mitigates our attacks but also has a signiﬁcant negative impact
on the quality of the collaboratively trained models.
C. Dropout
Another possible defense is to employ dropout [56], a
popular regularization technique used to mitigate overﬁtting
in neural networks. Dropout randomly deactivates activations
between neurons, with probability pdrop ∈ [0, 1]. Random
deactivations may weaken our attacks because the adversary
observes fewer gradients corresponding to the active neurons.
To evaluate this approach, we add dropout after the max
pool layers in the joint model. Table VII reports the accuracy
of inferring the region of the reviews in the CSI Corpus, for
different values of pdrop. Increasing the randomness of dropout
makes our attacks stronger while slightly decreasing the
accuracy of the joint model. Dropout stochastically removes
features at every collaborative training step, thus yielding more
informative features (similar to feature bagging [7, 26]) and
increasing variance between participants’ updates.
D. Participant-level differential privacy
As discussed in Section II-B, record-level ε-differential
privacy, by deﬁnition, bounds the success of membership
inference but does not prevent property inference. Any applica-
tion of differential privacy entails application-speciﬁc tradeoffs
between privacy of the training data and accuracy of the
resulting model. The participants must also somehow choose
the parameters (e.g., ε) that control this tradeoff.
In theory, participant-level differential privacy bounds the
success of inference attacks described in this paper. We im-
plemented the participant-level differentially private federated
learning algorithm by McMahan et al. [36] and attempted
to train a gender classiﬁer on LFW, but
the model did
not converge for any number of participants (we tried at
most 30). This is due to the magnitude of noise needed
to achieve differential privacy with the moments accountant
to the number
bound [1], which is inversely proportional
in [36] was trained on thousands of
of users (the model
users). Another participant-level differential privacy mecha-
nism, presented in [20], also requires a very large number of
participants. Moreover, these two mechanisms have been used,
respectively, for language modeling [36] and handwritten digit
recognition [20]. Adapting them to the speciﬁc models and
tasks considered in this paper may not be straightforward.
Following [20, 36], we believe that participant-level dif-
ferential privacy provide reasonable accuracy only in settings
involving at least thousands of participants. We believe that
further work is needed to investigate whether participant-level
differential privacy can be adapted to prevent our inference
attacks and obtain high-quality models in settings that do not
involve thousands of users.
IX. LIMITATIONS OF THE ATTACKS
A. Auxiliary data
Our property inference attacks assume that the adversary
has auxiliary training data correctly labeled with the property
he wants to infer. For generic properties, such data is easy to
ﬁnd. For example, the auxiliary data for inferring the number
and genders of people can be any large dataset of images
with males and females, single and in groups, where each
image is labeled with the number of people in it and their
genders. Similarly, the auxiliary data for inferring the medical
specialty of doctors can consist of any texts that include words
characteristic of different specialties (see Table IV).
More targeted inference attacks require specialized auxiliary
data that may not be available to the adversary. For example,
to infer that photos of a certain person occurs in another
participant’s dataset, the adversary needs (possibly different)
photos of that person to train on. To infer the authorship of
training texts, the adversary needs a sufﬁciently large sample
of texts known to be written by a particular author.
B. Number of participants
In our experiments, the number of participants in collab-
orative training is relatively small (ranging from 2 to 30),
while some federated-learning applications involve thousands
or millions of users [35, 36]. As discussed in Section VII-A,
performance of our attacks drops signiﬁcantly as the number
of participants increases.
C. Undetectable properties
It may not be possible to infer some properties from model
updates. For example, our attack did not detect the presence
of some face identities in the multi-party model averaging
experiments (Section VII-B). If for whatever reason the model
does not internally separate the features associated with the
target property, inference will fail.
D. Attribution of inferred properties
In the two-party scenarios considered in Section VI, attri-
bution of the inferred properties is trivial because there is only
one honest participant. In the multi-party scenarios considered
(cid:24)(cid:17)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
in Section VII, model updates are aggregated. Therefore, even
if the adversary successfully infers the presence of inputs with
a certain property in the training data, he may not be able to
attribute these inputs to a speciﬁc participant. Furthermore, he
may not be able to tell if all inputs with the property belong to
one participant or are distributed across multiple participants.
In general, attribution requires auxiliary information speciﬁc
to the leakage. For example, consider face identiﬁcation. In
some applications of collaborative learning, the identities of
all participants are known because they need to communicate
with each other. If collaborative learning leaks that a particular
person appears in the training images, auxiliary information
about the participants (e.g., their social networks) can reveal
which of them knows the person in question. Similarly, if
collaborative learning leaks the authorship of the training texts,
auxiliary information can help infer which participant is likely
to train on texts written by this author.
Another example of attribution based on auxiliary infor-
mation is described in Section VI-C. If photos of a certain
person ﬁrst appear in the training data after a new participant
has joined collaborative training, the adversary may attribute
these photos to the new participant.
Note that leakage of medical conditions, locations, images
of individuals, or texts written by known authors is a privacy
breach even if it cannot be traced to a speciﬁc participant or
multiple participants. Leaking that a certain person appears
in the photos or just the number of people in the photos
reveals intimate relationships between people. Locations can
reveal people’s addresses, religion, sexual orientation, and
relationships with other people.
X. RELATED WORK
Privacy-preserving distributed learning. Transfer learning
in combination with differentially private (DP) techniques
tailored for deep learning [1] has been used in [45, 46]. These
techniques privately train a “student” model by transferring,
through noisy aggregation, the knowledge of an ensemble of
“teachers” trained on the disjoint subsets of training data.
These are centralized, record-level DP mechanisms with a
trusted aggregator and do not apply to federated or collab-
orative learning. In particular, [45, 46] assume that the adver-
sary cannot see the individual models, only the ﬁnal model
trained by the trusted aggregator. Moreover, record-level DP
by deﬁnition does not prevent property inference. Finally, their
effectiveness has been demonstrated only on a few speciﬁc
tasks (MNIST, SVHN, OCR), which are substantially different
from the tasks considered in this paper.
Shokri and Shmatikov [52] propose making gradient up-
dates differentially private to protect the training data. Their
approach requires extremely large values of the ε parameter
(and consequently little privacy protection) to produce an
accurate joint model. More recently, participant-level differ-
entially private federated learning methods [20, 36] showed
how to protect participants’ training data by adding Gaussian
noise to local updates. As discussed in Section VIII-D, these
approaches require a large number of users (on the order
of thousands) for the training to converge and achieve an
acceptable trade-off between privacy and model performance.
Furthermore, the results in [36] are reported for a speciﬁc
language model and use AccuracyTop1 as the proxy, not the
actual accuracy of the non-private model.
Pathak et al. [47] present a differentially private global
classiﬁer hosted by a trusted third-party and based on locally
trained classiﬁers held by separate, mutually distrusting par-
ties. Hamm et al. [23] use knowledge transfer to combine a
collection of models trained on individual devices into a single
model, with differential privacy guarantees.
Secure multi-party computation (MPC) has also been used
to build privacy-preserving neural networks in a distributed
fashion. For example, SecureML [37] starts with the data
owners (clients) distributing their private training inputs among
two non-colluding servers during the setup phase; the two
servers then use MPC to train a global model on the clients’
encrypted joint data. Bonawitz et al. [5] use secure multi-party
aggregation techniques, tailored for federated learning, to let
participants encrypt their updates so that the central parameter
server only recovers the sum of the updates. In Section VII-B,
we showed that inference attacks can be successful even if the
adversary only observes aggregated updates.
Membership inference. Prior work demonstrated the feasi-
bility of membership inference from aggregate statistics, e.g.,
in the context of genomic studies [3, 27],
location time-
series [50], or noisy statistics in general [14].
Membership inference against black-box ML models has
also been studied extensively in recent work. Shokri et al. [53]
demonstrate membership inference against black-box super-
vised models, exploiting the differences in the models’ outputs
on training and non-training inputs. Hayes et al. [24] focus
on generative models in machine-learning-as-a-service appli-
cations and train GANs [22] to detect overﬁtting and recognize
training inputs. Long et al. [34] and Yeom et al. [66] study
the relationship between overﬁtting and information leakage.
Truex et al. [58] extend [53] to a more general setting and
show how membership inference attacks are data-driven and
largely transferable. They also show that an adversary who
participates in collaborative learning, with access to individual
model updates from all honest participants, can boost the
performance of membership inference vs. a centralized model.
Nasr et al. [39] design a privacy mechanism to adversarially
train centralized machine learning models with provable pro-
tections against membership inference.
Other attacks on machine learning models. Several tech-
niques infer class features and/or construct class represen-
tatives if the adversary has black-box [16, 17] or white-
box [2] access to a classiﬁer model. As discussed in detail
in Section III, these techniques infer features that characterize
an entire class and not speciﬁcally the training data, except in
the cases of pathological overﬁtting where the training sample
constitutes the entire membership of the class.
Hitaj et al. [25] show that a participant in collaborative deep
learning can use GANs to construct class representatives. Their
(cid:24)(cid:17)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
technique was evaluated only on models where all members
of the same class are visually similar (handwritten digits and
faces). As discussed in Section III-A, there is no evidence that
it produces actual training images or can distinguish a training
image and another image from the same class.
The informal property violated by the attacks of [2, 16, 17,
25] is: “a classiﬁer should prevent users from generating an
input that belongs to a particular class or even learning what
such an input looks like.” It is not clear to us why this property
is desirable, or whether it is even achievable.
Aono et al. [49] show that, in the collaborative deep learning
protocol of [52], an honest-but-curious server can partially
recover participants’ training inputs from their gradient up-
dates under the (greatly simpliﬁed) assumption that the batch
consists of a single input. Furthermore,
the technique is
evaluated only on MNIST where all class members are visually