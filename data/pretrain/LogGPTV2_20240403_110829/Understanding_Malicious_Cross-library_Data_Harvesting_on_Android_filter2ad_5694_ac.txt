requirements and clauses for the operations on the restricted
data, which can be empty. For example, the policy statement
“the advertising identiﬁer must not be associated with any
persistent device identiﬁer without explicit consent of the
user", can be represented as (advertising ID and device ID,
user consent). Note that in our study, we focus on ToS data
sharing policies, and thus the subject of such a policy is the
library developers (and their libraries) that call the target SDK
and the operation on the restricted data is GET.
During the analysis, our approach ﬁrst runs a NER model
to recover restricted data items. More speciﬁcally, the NER
is customized on the ToS corpora and the entity category of
restricted data items (e.g., utdid, password) using an efﬁcient
constituent parsing technique. Then, based on the restricted
data items, we identify the sentences related to third-party
data sharing policies from the ToS. After that, we extract the
pair (ob ject;condition) from the data sharing policies using
restricted data as “anchors” to recognize the pattern of each
policy’s grammatical structures and to locate the condition on
data sharing. We elaborate on our methodology as follows.
Restricted data object recognition. As mentioned earlier,
identifying restricted data from an SDK ToS is an NER prob-
lem. Unfortunately, NER techniques today are known to be
highly domain-speciﬁc [63]: open-domain NER model does
not work well on the security corpora, as restricted data are
different from the common entity categories (e.g., location,
people, organization) whose annotated datasets are available.
In our study, we observe that restricted data in the SDK ToS
is often characterized by a long noun phrase (e.g., Google Ad-
vertising ID, Facebook password, Amazon purchase history)
covered by a single or multiple consecutive noun phrases in
the constituency tree (Figure 2) . Therefore we can utilize the
features of the constituency tree to help identify such a phrase
as an entity.
More speciﬁcally, we include the constituency tree of a
sentence as a feature, which enables our NER model to learn
that certain types of phrasal nodes, such as NPs, are more
likely to be entities, i.e., restricted data. Hence, we crafted
several features based on constituency parsing tree tags for
each word, which include a word’s tag, its parent tag, the
left and right siblings, the location of the word in the span
of NPs nodes. For example, as shown in Figure 2, the word
“advertising" in the NP span “the advertising ID" in has 5
features: its tag “NNP", its parent tag “NP", its left sibling
“DT: the", its right sibling “NNP: ID" and its position of 1
under the span. Such features help the model to learn and
inference similar long noun phrase (eg., “the Twitter ID").
In our implementation, we utilize AllenNLP constituency
parser [56] to generate the constituency tree related features
for each sentence. Then, we built these features into the state-
of-the-art conditional random ﬁelds (CRF) based NER model
- Stanford NER [66]. As these features are not built-in features
[37] in Stanford NER, we conﬁgure the feature variables of
them using the SeqClassifierFlags class, and then read the
feature set into the CoreLabel class. In addition, we updated
training data using SDK ToSes. Particularly, we manually
annotated 534 sentences from 6 SDK documents using IOB
encoding [25] to retrain the NER model.
To evaluate the model, we perform 10-fold cross validation
on the annotated sentences. Our result shows that by leverag-
ing constituency tree features, the model achieves a precision
of 95.2% and a recall of 90.8%. Compared with the model
without constituency tree features, our model shows a increase
of 1.3% and 2.1% for the precision and recall, respectively.
After that, the model was also evaluated on additional 103
randomly selected and manually annotated sentences from
two previously-unseen SDK ToSes, which yields a precision
of 88.2% and a recall of 90.4%.
Policy statement discovery. From each ToS, the analyzer
identiﬁes the sentences describing how restricted data items
can be shared with or collected by other libraries. These sen-
tences are selected based on restricted data identiﬁed by the
4138    30th USENIX Security Symposium
USENIX Association
aforementioned model, the subject (i.e., library developer)
and the operation (i.e., GET) they contain.
We ﬁrst need to construct the keywords list associated with
data collection and sharing (e.g., use, collect, transfer, etc.).
For this purpose, we leverage the OPP-115 [86] and APP-
350 [87] datasets, which contain 46,259 manually annotated
privacy policy statements. Among them, 14,100 annotated
sentences are related to ﬁrst-party collection and third-party
collection. After that, we use constituency parser [56] to rec-
ognize the verb in the verb phrases and further identify the
lemma of a verb from those sentences. In this way, we collect
38 keywords related to data collection and sharing as shown
in Table 3.
After that, we use this keywords list in Table 3 and the
restricted data to ﬁlter out the sentences irrelevant to data
sharing and collection policy. Speciﬁcally, after parsing the
HTML content of each ToS and splitting the text into sen-
tences, we run our NER model to ﬁnd all statement sentences
that contain restricted data. Then we leverage dependency
parser [71] to locate the verb or nominal modiﬁer of restricted
data. In particular, if (1) the dependency relationship between
them is the direct object (e.g., collect personal informa-
tion) or nominal modifier (e.g., the usage of personal in-
formation) and (2) such verb or nominal modiﬁer is in the
keywords list, we will regard the sentence describing data
sharing and collection policy.
After this, we check that the sentence subject is not the
SDK itself but library developer. Speciﬁcally, we build a de-
pendency parsing tree to recognize the subject of a sentence.
We eliminate sentences with the “ﬁrst-party" as the subject.
For example, the target SDK’s name (e.g., “Twitter", “Face-
book") and ﬁrst-person plural (e.g., “we", “us"). Note that
for the sentences with an ambiguous subject reference (e.g.,
“it", “this", “that"), we run a co-reference resolution tool [56]
on the paragraph in which the sentence exists to identify the
subject.
Altogether, we gathered 1,056 sentences associated with
data sharing policy from 40 ToSes. By manually inspecting
200 sentences, we found that our method yields the recall of
89.3%. The miss-reported sentences are mainly due to wrong
dependency parsing results from underlying tools we use. For
example, “store non-public Twitter content" is parsed as a
noun phrase instead of verb-object phrase by [71].
Data sharing policy identiﬁcation. To extract the pair
(ob ject;condition) from the policy statements of an SDK
ToS, our approach ﬁrst uses a dependency parser [58] to trans-
form a sentence into a dependency parsing tree, which de-
scribes the grammatical connections between different words,
and then leverages the restricted data ob ject as known an-
chors to locate the condition by traversing the parsing tree.
Here we prune the dependency tree of the policy statement
into a subtree that represents the grammatical relation among
ob ject, condition and the operation (e.g., “transfer”, “use”).
This is because that the policy statement is usually long and
consists of noisy information, and the subtree is most relevant
to the understanding of the relation.
(cid:15) Object identiﬁcation. In our research, we observe ob ject in
the data sharing policy sometimes consists of more than one
restricted data, e.g., “Don’t collect usernames or passwords".
Hence, to extract the ob ject from each policy statement, we
ﬁrst identify the restricted data d1;d2; :::;dn using the afore-
mentioned method, and then use dependency tree to determine
whether they have conjunctive relation and their coordinating
conjunction (a.k.a., CCONJ [32]) is “OR". If so, we recognize
them as n different objects.
Similarly, for the restricted data d1;d2; :::;dn are with
conjunctive relation but
their coordinating conjunction
(a.k.a.,CCONJ [32]) is “AND", we recognize them as one
object. However, things get complicated when the policy state-
ment illustrates multiple objects can not GET at the same time,
e.g., Don’t associate user proﬁles with any mobile device
identiﬁer. Here, we use speciﬁc verbs (e.g., associate with,
combine with, connect to) to identify this relationship. In this
way, we recognize them as one object, i.e., d1 ^ d2:::^ dn.
In addition, we use the lexicosyntatic patterns discovered
in [45] to ﬁnd the object hyponym and then use the speciﬁed
object hyponym in the policy tuple. For example, given the
pattern “X, for example, Y1, Y2,...Yn ”, where Y1, Y2,...Yn is
the hyponym of X, and the sentence “device identiﬁer, for
example: ssaid, mac address, imei, etc", we will extract ﬁve
policies of “device identiﬁer", “ssaid", “mac address" and
“imei".
(cid:15) Condition extraction. By manually inspecting 1K sentences
from 10 SDK ToSes, we annotated 14 generic patterns (in
terms of dependency trees), which describe the grammatical
relation among ob ject, condition and the operation. The an-
notated pattern list is shown in Table 9. Then, we fed them into
the analyzer which utilizes these patterns to match the depen-
dency parsing trees of the policy statements, using the ob ject
and operation nodes as anchors. More speciﬁcally, given
a policy statement, we use the depth-ﬁrst search algorithm,
which starts at ob ject and operation nodes, to extract all sub-
trees for pattern similarity comparison. Then we identify the
most similar subtree of a policy statement by calculating a
dependency tree edit distance between each subtree and the
patterns in Table 9. Here we deﬁne a dependency tree edit dis-
tance D(t1;t2) = min(o1;:::;ok)2O(t1;t2) (cid:229)k
i=1 oi, where, O(t1;t2)
is a set of tree edits (e.g., node or edge’s insertion, deletion
and substitution) that transform t1 to t2, and we consider t1
and t2 are equal when all node types and edge attributes are
matched. After that, we locate the condition node based on
the matched subtree.
For example, Figure 5a illustrates the dependency tree struc-
ture of the policy statement. In the tree, each edge has an
attribute dep that shows the dependency relationship between
nodes, and each node has an attribute type which indicates
whether it is ob ject, operation, or none of the above (other).
USENIX Association
30th USENIX Security Symposium    4139
Analyzing these 1,215 pairs of data sharing policies, we
found that most are from Facebook SDK (9.4%), followed by
Amazon (8.8%) and LinkedIn (7.3%). Also, 37 of them have
the object with more than one restricted data. We observe the
objects advertising ID and mobile device identiﬁer always
co-occur (7 pairs), because the user-resettable advertising ID
will be personally identiﬁable when associated with mobile
device identiﬁer, which is not privacy-compliant [9]. To un-
derstand the data sharing conditions, we manually analyzed
all of the recognized data sharing policies and categorized
them into ﬁve types: No access by any party, Requiring user
consents, No third-party access, Complying with regulations
(i.e., GDPR, CCPA, COPPA) and Others, as shown in Table
5. Note that 96% of the data sharing policy are with the ﬁrst
four types of data sharing conditions. The Others type of data
sharing condition is rarely observed and sometimes associated
with some vaguely-described condition, such as “Only certain
application types can access.” In our policy compliance check
(see Section 3.3), we did not check the policy compliance re-
lated to this type of condition. We acknowledge that checking
those policies would allow for a more holistic view of XLDH
activities. However, doing so will require subjective analysis
of the vague and ambiguous policies and a large amount of
manual efforts for corner cases.
Comparison with other policy analyzers. Since there is no
public-available ToS analyzer, we compared DPA’s policy
statement discovery with two state-of-the-art privacy policy
analyzer Polisis [57] and PolicyLint [45].2 Note that Polisis
and PolicyLint are designed for privacy policy analysis, not
ToS analysis.
More speciﬁcally, we manually annotated 200 sentences
from 3 SDK ToSes (e.g., Twitter, Google, Facebook), which
yielded 83 sentences are associated data collection and shar-
ing policy and the rest (117) are not. In our experiment, we
evaluated the approaches on this dataset. Table 4 shows the
experiment results. Our study shows that DPA outperforms
both approaches in precision and recall.
We also compared DPA’s restricted data object recognition
module with PolicyLint [45]. We use the aforementioned 534
IOB-encoded sentences to evaluate both DPA and PolicyLint
via 10-fold cross validation. For PolicyLint, we retrained its
NER model using our annotated corpora. We also show the
performance of PolicyLint with its original model. Table 4
shows the precision and recall of both approaches. Our study
shows that DPA has a much better recall (90.8% vs 82.7%).
3.3 Cross-library Analysis
To capture malicious data harvesting in an app, our Cross-
library Analyzer (XLA) identiﬁes cross-library API calls to
ﬁnd the data gathered by a third-party library from a co-
2PolicyCheck [46] shared the same policy analysis module with Poli-
cyLint.
(a) Tree structure of policy statement
(b) Tree structure of matched pattern
Figure 5: A example of condition extraction with the pol-
icy statement “You must have legally valid consent from a
Member before you store that Member’s Proﬁle Data"
The subtree which consists of all green nodes is the most
similar subtree with 0 edit distance to the pattern shown in
the Figure 5b Traversing the matched pattern, we can locate
condition node which is (have legally valid consent).
As the graph matching problem is NP-complete, the com-
putation time grows dramatically with the increase of node
and edge. In our implementation, to reduce the matching time,
when constructing subtrees using depth-ﬁrst search, we deﬁne
the search depth which will not exceed twice of pattern length
and the threshold of edit distance’s threshold to be 3.
Discussion and evaluation. DPA recognized 1,215 pairs
(ob ject, condition) from 1,056 policy statements from 40
ToSes. We manually inspect all of the detected pairs based
on the relevant policy statements. The results show that our
method achieves a precision of 84.2%. However, still our
technique misses some cases. We acknowledge that the ef-
fectiveness of the method can increase with more annotated
sentences for pattern matching. In our research, to guarantee
the diversity of the annotated patterns, we design a sample
strategy for annotated sentence selection. Speciﬁcally, we
keep randomly sampling sentences for annotation until not
observing new patterns for continuous 200 sentences. In this
way, we annotated 14 patterns by inspecting 1K sentences
from 10 SDK ToSes. Another limitation is the capability to
process long sentences. Our method utilized the dependency
parsing trees of the sentences for condition extraction. How-
ever, the state-of-the-art dependency parser cannot maintain
its accuracy when sentences become too long.
4140    30th USENIX Security Symposium
USENIX Association
haveyoustoremustleally vaild consentfromyoubeforea membermembers' Profile data{dep:aux}{dep:nsubj}{dep:dobj}{dep:advcl}{dep:prep}    {dep:pobj}{dep:nsubj}{dep:dobj}{dep:advmod}{type:other}{type:other}{type:other}{type:other}{type:other}{type:other}{type:operation}{type:data}{type:other}{type:other}obtainconsentuseour service data{dep:dobj}{dep:advcl}{dep:dobj}{type:other}{type:other}{type:operation}{type:data}Table 4: Comparison with Polisis and PolicyLint/PolicyCheck
Tasks
Statement ﬁnder
Restricted data
detector
Tools
XFinder
Polisis
PolicyLint
XFinder