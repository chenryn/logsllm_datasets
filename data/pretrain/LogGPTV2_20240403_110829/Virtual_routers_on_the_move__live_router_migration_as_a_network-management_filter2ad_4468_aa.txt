title:Virtual routers on the move: live router migration as a network-management
primitive
author:Yi Wang and
Eric Keller and
Brian Biskeborn and
Jacobus E. van der Merwe and
Jennifer Rexford
Virtual Routers on the Move: Live Router Migration
as a Network-Management Primitive
Yi Wang∗ Eric Keller∗ Brian Biskeborn∗ Jacobus van der Merwe† Jennifer Rexford∗
∗
†
Princeton University, Princeton, NJ, USA
AT&T Labs - Research, Florham Park, NJ, USA
{yiwang,jrex}@cs.princeton.edu {ekeller,bbiskebo}@princeton.edu PI:EMAIL
ABSTRACT
The complexity of network management is widely recognized
as one of the biggest challenges facing the Internet today.
Point solutions for individual problems further increase sys-
tem complexity while not addressing the underlying causes.
In this paper, we argue that many network-management
problems stem from the same root cause—the need to main-
tain consistency between the physical and logical conﬁgura-
tion of the routers. Hence, we propose VROOM (Virtual
ROuters On the Move), a new network-management primi-
tive that avoids unnecessary changes to the logical topology
by allowing (virtual) routers to freely move from one phys-
ical node to another.
In addition to simplifying existing
network-management tasks like planned maintenance and
service deployment, VROOM can also help tackle emerging
challenges such as reducing energy consumption. We present
the design, implementation, and evaluation of novel migra-
tion techniques for virtual routers with either hardware or
software data planes. Our evaluation shows that VROOM
is transparent to routing protocols and results in no perfor-
mance impact on the data traﬃc when a hardware-based
data plane is used.
Categories and Subject Descriptors
C.2.6 [Computer Communication Networks]: Internet-
working; C.2.1 [Computer Communication Networks]:
Network Architecture and Design
General Terms
Design, Experimentation, Management, Measurement
Keywords
Internet, architecture, routing, virtual router, migration
1.
INTRODUCTION
Network management is widely recognized as one of the
most important challenges facing the Internet. The cost of
the people and systems that manage a network typically
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
exceeds the cost of the underlying nodes and links; in ad-
dition, most network outages are caused by operator errors,
rather than equipment failures [21]. From routine tasks such
as planned maintenance to the less-frequent deployment of
new protocols, network operators struggle to provide seam-
less service in the face of changes to the underlying network.
Handling change is diﬃcult because each change to the phys-
ical infrastructure requires a corresponding modiﬁcation to
the logical conﬁguration of the routers—such as reconﬁgur-
ing the tunable parameters in the routing protocols.
Logical refers to IP packet-forwarding functions, while phys-
ical refers to the physical router equipment (such as line
cards and the CPU) that enables these functions. Any in-
consistency between the logical and physical conﬁgurations
can lead to unexpected reachability or performance prob-
lems. Furthermore, because of today’s tight coupling be-
tween the physical and logical topologies, sometimes logical-
layer changes are used purely as a tool to handle physical
changes more gracefully. A classic example is increasing the
link weights in Interior Gateway Protocols to “cost out” a
router in advance of planned maintenance [30]. In this case,
a change in the logical topology is not the goal, rather it is
the indirect tool available to achieve the task at hand, and
it does so with potential negative side eﬀects.
In this paper, we argue that breaking the tight coupling
between physical and logical conﬁgurations can provide a
single, general abstraction that simpliﬁes network manage-
ment. Speciﬁcally, we propose VROOM (Virtual ROuters
On the Move), a new network-management primitive where
virtual routers can move freely from one physical router to
another. In VROOM, physical routers merely serve as the
carrier substrate on which the actual virtual routers operate.
VROOM can migrate a virtual router to a diﬀerent physi-
cal router without disrupting the ﬂow of traﬃc or changing
the logical topology, obviating the need to reconﬁgure the
virtual routers while also avoiding routing-protocol conver-
gence delays. For example, if a physical router must un-
dergo planned maintenance, the virtual routers could move
(in advance) to another physical router in the same Point-
of-Presence (PoP). In addition, edge routers can move from
one location to another by virtually re-homing the links that
connect to neighboring domains.
Realizing these objectives presents several challenges: (i)
migratable routers: to make a (virtual) router migratable, its
“router” functionality must be separable from the physical
equipment on which it runs; (ii) minimal outages: to avoid
disrupting user traﬃc or triggering routing protocol recon-
vergence, the migration should cause no or minimal packet
loss; (iii) migratable links: to keep the IP-layer topology in-
231tact, the links attached to a migrating router must “follow”
it to its new location. Fortunately, the third challenge is
addressed by recent advances in transport-layer technolo-
gies, as discussed in Section 2. Our goal, then, is to migrate
router functionality from one piece of equipment to another
without disrupting the IP-layer topology or the data traﬃc
it carries, and without requiring router reconﬁguration.
On the surface, virtual router migration might seem like
a straight-forward extention to existing virtual machine mi-
gration techniques. This would involve copying the virtual
router image (including routing-protocol binaries, conﬁgu-
ration ﬁles and data-plane state) to the new physical router
and freezing the running processes before copying them as
well. The processes and data-plane state would then be re-
stored on the new physical router and associated with the
migrated links. However, the delays in completing all of
these steps would cause unacceptable disruptions for both
the data traﬃc and the routing protocols. For virtual router
migration to be viable in practice, packet forwarding should
not be interrupted, not even temporarily. In contrast, the
control plane can tolerate brief disruptions, since routing
protocols have their own retransmission mechansisms. Still,
the control plane must restart quickly at the new location to
avoid losing protocol adjacencies with other routers and to
minimize delay in responding to unplanned network events.
In VROOM, we minimize disruption by leveraging the
separation of the control and data planes in modern routers.
We introduce a data-plane hypervisor—a migration-aware
interface between the control and data planes. This uniﬁed
interface allows us to support migration between physical
routers with diﬀerent data-plane technologies. VROOM mi-
grates only the control plane, while continuing to forward
traﬃc through the old data plane. The control plane can
start running at the new location, and populate the new data
plane while updating the old data plane in parallel. Dur-
ing the transition period, the old router redirects routing-
protocol traﬃc to the new location. Once the data plane
is fully populated at the new location, link migration can
begin. The two data planes operate simultaneously for a
period of time to facilitate asynchronous migration of the
links.
To demonstrate the generality of our data-plane hypervi-
sor, we present two prototype VROOM routers—one with
a software data plane (in the Linux kernel) and the other
with a hardware data plane (using a NetFPGA card [23]).
Each virtual router runs the Quagga routing suite [26] in an
OpenVZ container [24]. Our software extensions consist of
three main modules that (i) separate the forwarding tables
from the container contexts, (ii) push the forwarding-table
entries generated by Quagga into the separate data plane,
and (iii) dynamically bind the virtual interfaces and forward-
ing tables. Our system supports seamless live migration of
virtual routers between the two data-plane platforms. Our
experiments show that virtual router migration causes no
packet loss or delay when the hardware data plane is used,
and at most a few seconds of delay in processing control-
plane messages.
The remainder of the paper is structured as follows. Sec-
tion 2 presents background on ﬂexible transport networks
and an overview of related work. Next, Section 3 discusses
how router migration would simplify existing network man-
agement tasks, such as planned maintenance and service
deployment, while also addressing emerging challenges like
Router A
Programmable
Transport Network
Router B
Router C
Optical transport switch
(a) Programmable transport network 
Router A
Packet-aware
Transport Network
Router B
Router C
IP router
(b) Packet-aware transport network 
Figure 1: Link migration in the transport networks
power management. We present the VROOM architecture
in Section 4, followed by the implementation and evalua-
tion in Sections 5 and 6, respectively. We brieﬂy discuss
our on-going work on migration scheduling in Section 7 and
conclude in Section 8.
2. BACKGROUND
One of the fundamental requirements of VROOM is “link
migration”,
i.e., the links of a virtual router should “fol-
low” its migration from one physical node to another. This
is made possible by emerging transport network technolo-
gies. We brieﬂy describe these technologies before giving an
overview of related work.
2.1 Flexible Link Migration
In its most basic form, a link at the IP layer corresponds
to a direct physical link (e.g., a cable), making link migra-
tion hard as it involves physically moving link end point(s).
However, in practice, what appears as a direct link at the
IP layer often corresponds to a series of connections through
diﬀerent network elements at the transport layer. For ex-
ample, in today’s ISP backbones, “direct” physical links are
typically realized by optical transport networks, where an
IP link corresponds to a circuit traversing multiple optical
switches [9, 34]. Recent advances in programmable transport
networks [9, 3] allow physical links between routers to be
dynamically set up and torn down. For example, as shown
in Figure 1(a), the link between physical routers A and B
is switched through a programmable transport network. By
signaling the transport network, the same physical port on
router A can be connected to router C after an optical path
switch-over. Such path switch-over at the transport layer
can be done eﬃciently, e.g., sub-nanosecond optical switch-
ing time has been reported [27]. Furthermore, such switch-
ing can be performed across a wide-area network of trans-
port switches, which enables inter-POP link migration.
232In addition to core links within an ISP, we also want to
migrate access links connecting customer edge (CE) routers
and provider edge (PE) routers, where only the PE end of
the links are under the ISP’s control. Historically, access
links correspond to a path in the underlying access network,
such as a T1 circuit in a time-division multiplexing (TDM)
access network. In such cases, the migration of an access link
can be accomplished in similar fashion to the mechanism
shown in Figure 1(a), by switching to a new circuit at the
switch directly connected to the CE router. However, in tra-
ditional circuit-switched access networks, a dedicated phys-
ical port on a PE router is required to terminate each TDM
circuit. Therefore, if all ports on a physical PE router are in
use, it will not be able to accommodate more virtual routers.
Fortunately, as Ethernet emerges as an economical and ﬂex-
ible alternative to legacy TDM services, access networks are
evolving to packet-aware transport networks [2]. This trend
oﬀers important beneﬁts for VROOM by eliminating the
need for per-customer physical ports on PE routers.
In a
packet-aware access network (e.g., a virtual private LAN
service access network), each customer access port is associ-
ated with a label, or a “pseudo wire” [6], which allows a PE
router to support multiple logical access links on the same
physical port. The migration of a pseudo-wire access link
involves establishing a new pseudo wire and switching to it
at the multi-service switch [2] adjacent to the CE.
Unlike conventional ISP networks, some networks are re-
alized as overlays on top of other ISPs’ networks. Examples
include commercial “Carrier Supporting Carrier (CSC)” net-
works [10], and VINI, a research virtual network infrastruc-
ture overlaid on top of National Lambda Rail and Inter-
net2 [32].
In such cases, a single-hop link in the overlay
network is actually a multi-hop path in the underlying net-
work, which can be an MPLS VPN (e.g., CSC) or an IP
network (e.g., VINI). Link migration in an MPLS transport
network involves switching over to a newly established label
switched path (LSP). Link migration in an IP network can
be done by changing the IP address of the tunnel end point.
2.2 Related Work
VROOM’s motivation is similar, in part, to that of the
RouterFarm work [3], namely, to reduce the impact of planned
maintenance by migrating router functionality from one place
in the network to another. However, RouterFarm essen-
tially performs a “cold restart”, compared to VROOM’s live
(“hot”) migration. Speciﬁcally, in RouterFarm router migra-
tion is realized by re-instantiating a router instance at the
new location, which not only requires router reconﬁguration,
but also introduces inevitable downtime in both the control
and data planes. In VROOM, on the other hand, we perform
live router migration without reconﬁguration or discernible
disruption. In our earlier prototype of VROOM [33], router
migration was realized by directly using the standard virtual
machine migration capability provided by Xen [4], which
lacked the control and data plane separation presented in
this paper. As a result, it involved data-plane downtime
during the migration process.
Recent advances in virtual machine technologies and their
live migration capabilities [12, 24] have been leveraged in
server-management tools, primarily in data centers. For ex-
ample, Sandpiper [35] automatically migrates virtual servers
across a pool of physical servers to alleviate hotspots. Usher [22]
allows administrators to express a variety of policies for
managing clusters of virtual servers. Remus [13] uses asyn-
chronous virtual machine replication to provide high avail-
ability to server in the face of hardware failures. In contrast,
VROOM focuses on leveraging live migration techniques to
simplify management in the networking domain.
Network virtualization has been proposed in various con-
texts. Early work includes the “switchlets” concept, in which
ATM switches are partitioned to enable dynamic creation
of virtual networks [31]. More recently, the CABO archi-
tecture proposes to use virtualization as a means to enable
multiple service providers to share the same physical infras-
tructure [16]. Outside the research community, router vir-
tualization has already become available in several forms in
commercial routers [11, 20].
In VROOM, we take an ad-
ditional step not only to virtualize the router functionality,
but also to decouple the virtualized router from its physical
host and enable it to migrate.
VROOM also relates to recent work on minimizing tran-
sient routing disruptions during planned maintenance. A
measurement study of a large ISP showed that more than
half of routing changes were planned in advance [19]. Net-
work operators can limit the disruption by reconﬁguring the
routing protocols to direct traﬃc away from the equipment
undergoing maintenance [30, 17].
In addition, extensions
to the routing protocols can allow a router to continue for-
warding packets in the data plane while reinstalling or re-
booting the control-plane software [29, 8]. However, these
techniques require changes to the logical conﬁguration or the
routing software, respectively. In contrast, VROOM hides
the eﬀects of physical topology changes in the ﬁrst place,
obviating the need for point solutions that increase system
complexity while enabling new network-management capa-
bilities, as discussed in the next section.
3. NETWORK MANAGEMENT TASKS