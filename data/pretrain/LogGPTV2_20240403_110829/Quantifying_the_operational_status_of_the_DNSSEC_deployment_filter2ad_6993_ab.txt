Although the DNSSEC deployment is still in its early
stages, our measurement results have already revealed some
key challenges that face Internet cryptographic systems. Our
DNSSEC monitoring project has been operating since major
DNSSEC deployment eﬀorts began over 2 years ago. During
that time, the number of monitored zones has grown from
tens to several thousand. While this is a small fraction of
the World’s DNS zones, our set continues to grow rapidly.
No data for zi can be veriﬁed
end
Despite the relatively small size of DNSSEC’s initial roll-
out, it is quite complex and scaling the monitoring is already
a major concern. The number of records in a zone can vary
from tens to millions and each DNS zone is served by several
authoritative servers. Thus, even a single zone is, essentially,
a distributed view of a dataset. This fact raises interesting
questions about what to monitor and how to quantify the
results in a way that provides both insight to general trends
and an ability to analyze speciﬁc issues.
In addition to the distributed nature of a zone, the view
of its data also varies depending on the location that it is
monitored from.
In the simplest case, connectivity issues
may prevent some monitoring points (called pollers) from
reaching a zone’s servers. Although one may assume that
connectivity problems are rare, middleboxes (such as ﬁre-
walls, NATs, or proxies) are pervasive today. Such middle-
boxes along the paths between pollers and the authoritative
servers, and other qualities of the path itself, can dramati-
cally change the view of a zone. Our data analysis in later
sections will illustrate the impact of middleboxes; for the
moment it suﬃces to say that the location of monitoring
is an important factor. During the course of deploying our
monitoring system, our set of pollers has expanded from a
single poller to a collection of distributed pollers in diﬀer-
ent continents, and this has oﬀered fundamentally diﬀerent
views of some DNSSEC datasets.
Our monitoring system gathers a vast volume of data on
DNSSEC resource record sets (RRsets) as viewed from dif-
ferent locations over diﬀerent times. The detailed data is
publicly available and it allows people to investigate speciﬁc
questions such as “was RRset X available from pollers in
Asia on January 31, 2008?” But the vast volume of raw
data provides little insight into how the overall system is
performing. The situation is analogous to monitoring BGP
routing, another Internet-scale system. BGP monitoring
projects such as Oregon RouteViews [14] provide invaluable
raw data containing millions (if not billions) of BGP up-
dates. Hidden in this data are important lessons about the
overall system behavior, but simply looking at raw BGP up-
date logs does not directly answer the question of how well
BGP is performing. Similarly, simply presenting millions of
DNS RRset query results does not directly answer the ques-
tion of how well DNSSEC is performing, how eﬀective it may
be in providing cryptographic protections for the DNSSEC-
enabled zones, and more importantly, how these measures
may be changing over time.
In order to gain a quantitative assessment of DNSSEC as a
whole, we derived the following three measurement metrics:
• Availability: This measures whether the system can
provide all the data to the end-systems requesting it.
• Veriﬁability: This measures whether the end-system
can cryptographically verify the data it receives.
• Validity: This measures whether the veriﬁed data is
actually valid. Note that in an actual deployments,
it does not necessarily follow that all veriﬁed data is
indeed valid.
In the following three subsections, we describe each met-
ric in detail and present our approach for quantifying the
results.
3.1 Availability
Intuitively, one would like to know whether a secure zone
is “available.” This is an important 2-way street because
zone operators need to be aware of any problems resolvers
may have in receiving their data, and resolvers would like
to know why they may be unable to get certain data from a
zone. However, before one can know if a zone is “available,”
one must deﬁne what that means. Thus, we deﬁne a mea-
sure that captures the intuitive notion of availability, but in
quantiﬁable metric. In Section 4, we show how this metric is
eﬀective in quantifying DNSSEC’s non-uniformity. Such a
quantiﬁcation should facilitate an empirical way to measure
improvements in DNSSEC’s deployment.
Selecting RRsets and Nameservers: A secure zone
is comprised of a set of RRsets. The number of these sets
in a zone can range from fewer than ten to over tens of mil-
lions, but the DNSKEY set (key-set) plays an special role
in DNSSEC. The key-set holds the zone’s public key(s), and
by deﬁnition, every secure zone must contain an instance of
this set at its apex. The keys in the key-set are required to
verify other RRsets and are also needed to verify authenti-
cation chains to descendant zones in the DNSSEC hierarchy
(Figure 1). Without getting the key-set, even if a resolver
can obtain other RRsets, it cannot verify them. We thus ar-
gue that for the purpose of DNSSEC, zone availability can
be reasonably represented by the availability of the key-set
itself.
Having identiﬁed a speciﬁc set to monitor, we next con-
sider which of the many authoritative servers of a zone to
query. Work in [17] has shown that, due to various conﬁgu-
ration errors, diﬀerent servers of the same zone may exhibit
diﬀerent behaviors. For example, some may be listed as
authoritative but actually fail to answer queries, and some
other servers may be authoritative but unreachable. In this
paper we focuses on whether a zone’s key-set is available via
any authoritative server 1.
Availability Metric: Given a set of pollers (P ) who
send queries at a set of given polling times (T ), we denote
the availability of zone zj from poller pi at time t ∈ T as
1Our monitoring system is able to detect the problems re-
ported in [17]. However, this is beyond the scope of this
paper.
A(pi, zj, t). In this paper, A(pi, zj, t) is either 1 to indicate
the poller was able to obtain zj’s key-set (Kj), or 0 to indi-
cating it could not be retrieved. For example, A(p1, z1, t) is
set to 1 if poller p1 could obtain K1 from zone z1 at time t.
Similarly A(p2, z1, t) is set to 0 if poller p2 could not obtain
the K1 from zone z1 at time t.
This metric is designed to allow a more nuanced deﬁni-
tion of availability in which the value can vary between 0
and 1. For example, one might include representations for
nameserver availability, a combination of multiple RRsets,
the agreement ratio between a zone’s nameservers, or other
facets. However in this paper, a simple deﬁnition of avail-
ability suﬃces and we set the value of A(pi, zj, t) to either
1 (Kj obtained) or 0 (Kj not obtained).
Having deﬁned zone availability for a particular poller at
a particular time t, we combine the results from multiple
pollers to obtain a single for the zone availability at time t.
Let Amax(zj, t) = maxi=0,|P|A(pi, zj, t) denote the highest
availability metric obtained for any poller. This value rep-
resents the best observed view of availability for this zone
at this time. We say a zone zj is available at time t iﬀ
Amax(zj, t) > 0. Our later results show that a vast majority
of DNSSEC zones were “available” during polling times ∈ T .
Availability Dispersion Metric: While the above def-
inition of availability focuses on whether some resolver (rep-
resented by pi) can reach a zone, we are also interested in
describing how many resolvers can reach the zone. Our later
results show that in many cases, even though some resolvers
can reach a zone, others cannot. If a zone is available, the
variance in availability is quantiﬁed as availability disper-
sion. More precisely, we denote the zone zj’s availability
dispersion at time t as:
P|P|
i=0 Amax(zj, t) − A(pi, zj, t)
|P|
disp(zj, t) =
The intuition for the dispersion metric ﬁrst considers the
zone’s Amax(zj, t). All other pollers are compared against
this best case and pollers with lower availability increases the
dispersion. For example, if all pollers see a zone as available
the dispersion will be 0. Furthermore, if we take the limit as
the number of polling locations approaches the total number
of resolvers on the Internet, we can see that the availability
dispersion approaches the mean behavior for resolvers.
There is a clear diﬀerence between the polling failures that
stem from persistent availability issues and those that repre-
sent transient network problems. The availability dispersion
metric is designed to address the former (persistent prob-
lems).
In the case of the latter (transient problems), the
timeout/retry strategy of the monitoring apparatus is use-
ful in attempting to overcome failures. The speciﬁc time-
out/retry strategy used is described in more detail in Sec-
tion 4.
Recall our metric is designed for A(pi, zj, t) values that
range between 0 and 1, but this paper considers only values
of 0 and 1 and thus disp() can be simpliﬁed. Since dis-
persion is only calculated if at least one poller can reach
the zone (Amax(zj, t) = 1), any other poller that can reach
the zone will not contribute to the numerator in dispersion
(Amax(zj, t) − A(pi, zj, t) = 0), but A(pi, zj, t) = 0 will con-
tribute 1 to the numerator (Amax(zj, t) − A(pi, zj, t) = 1).
Therefore, our dispersion calculation simpliﬁes to the aver-
age number of failed pollers.
Next, we take the instantaneous metrics and apply an Ex-
ponentially Weighted Moving Average (EWMA) to obtain:
disp(zj) = (α × disp(zj)) ×“
”
(1 − α) × disp´(zj)
EWMA incorporates the history of dispersion without over-
penalizing zones who are normally available but were not at
the time of a recent poll and without being overly charita-
ble to zones that are normally unavailable, but who were
available at the time of the last poll. Thus, while the time-
out/retry strategy helps to overcome some transient prob-
lems, the actual dispersion metric also reduces their eﬀect
(if they are indeed only transient).
Because high-dispersion indicates potential problems, while
low or no dispersion represents that the eﬀect on availability
is uniform, we take the complement of the average availabil-
ity dispersion to reﬂect the Internet’s eﬀect on availability:
availdisp(zj) =`1 − disp(zj)´
3.2 Veriﬁability
The previous section presented a metric for assessing the
availability of DNSKEY RRsets (key-sets) and by extension
the zones that serve them. But simply accepting key-sets
without any veriﬁcation defeats the underlying purpose of
adding cryptography. DNSSEC was introduced because re-
solvers may receive incorrect responses caused by uninten-
tional errors or intentional attacks. Even using key-sets can
leave a resolver vulnerable if a man-in-the-middle attack al-
lows an adversary to give a resolver a bad key [8, 16]. Thus a,
resolver must be able to verify key-sets and this section intro-
duces a metric that captures the intuitive notion of whether
this can be done.
To verify any data, a resolver must be conﬁgured with
some initial set of keys from trusted zones, referred to as
trust anchors. Figure 1 illustrates this process.
If all zones were secure and each secure zone coordinated
with its parent in the DNS tree, then resolvers would only
need to be conﬁgured with a single trust anchor, correspond-
ing to the root public key. However, not all zones are secure
and not all secure zones coordinate with their parents in the
DNS tree. The result is that there are gaps in the authen-
tication chain and these gaps must be bridged by adding
additional trust anchors. In the worst case, there could be
no authentication chains and a resolver would need to be
conﬁgured with a trust anchor corresponding to each zone
(which would be tens of millions for a full deployment). In
the ideal case, resolvers are conﬁgured with a single trust
anchor. To quantify where the current deployment stands,
we introduce a veriﬁability metric that captures the amount
of conﬁguration needed to verify key-sets.
Veriﬁability Metric: Let T a denote a trust anchor. We
say the key-set (Ki) for zone zi is covered by trust anchor
T a iﬀ there is an authentication chain leading from T a to
zi. If |Z s| denotes the number of total secure zones and |T a|
is the minimum number of trust anchors needed to cover
all secure zones then we say the overall veriﬁability of the
system is:
V f = 1 − |T a| − 1
|Z s|
The intuition for this expression comes from DNSSEC’s
Veriﬁed
Valid Ideal Behavior
False Positive
Invalid
Unveriﬁed
False Negative
Intended Defense
Table 2: Veriﬁcation vs validity matrix.
goal of a single trust anchor. Thus, the expression accepts
a single trust anchor as optimal (hence the −1 term), and
penalizes all instances above 1. Note that if no authentica-
tion chains had been established between any secure zones,
a resolver would need to conﬁgure Ki for each zone zi ∈ Z s
as a trust anchor, and V f → 0. If DNSSEC is deployed in a
contiguous region of the DNS tree and all zones in this re-
gion establish authentication chains with their direct parent,
then we will only need a single T a and V f → 1.
3.3 Validity
The previous sections considered whether zones’ critical
DNSKEY RRsets (key-sets) were available to resolvers and
how much conﬁguration was needed to verify these key-sets.
This section considers whether data is actually valid and
illustrates that there are key diﬀerences between veriﬁed
data and valid data. More speciﬁcally, veriﬁcation refers
the cryptographic process in which a data unit is either ver-
iﬁed or not. Validity, on the other hand, refers to whether
the data actually corresponds to what the zone administra-
tor intended (ground truth) and a data unit is either valid
or invalid. Based on the overlapping intents of veriﬁcation
and validity there are four possible combinations, which are
shown in Table 2. Our validity metric (V d) focuses on the
validity of DNSSEC data.
Ideally, data obtained by a resolver is both valid and ver-
iﬁed (upper left box in Table 2). For example, if a zone
administrator correctly enters and signs zone data, then a
resolver should be able to obtain and verify this valid data.
DNSSEC adds cryptographic checks in the hope of detecting
invalid data by using cryptography alone (lower right box in
Table 2). For example, if someone modiﬁes data in ﬂight
(after being signed), then the old values become invalid and
one expects that the signature veriﬁcation will fail. This
is the intended behavior of DNSSEC but both operational
errors and successful attacks can cause this to fail.
False Negatives: The case of false negatives (upper right
box in Table 2) occur when a resolver gets data that is ac-
tually valid, but is unable to verify it. The most trivial
case of this is when a resolver receives plain DNS responses,
but ﬁnds that there are no signatures attached. This, for
example, was observed during the early DNSSEC develop-
ment. Some sites could not obtain signed data from se-
cure zones even though the server correctly attached the
signatures. This problem was caused by intervening ﬁre-
walls that blocked any response that contained signatures.
From the ﬁrewall’s perspective: the resolver had made a sim-
ple request but the response also included the signatures.
To the well intentioned ﬁrewall, these unknown signature
records were clearly some sort of attack and the responses
were dropped. Answers that did not include signatures were
passed through, but could not be veriﬁed by the resolver.
Another important example of false negatives can occur
when a zone unintentionally breaks its own secure delegation
from its parent. This can happen if a zone creates a new key
pair and re-signs all RRsets with the new key before updat-
ing the authentication chain with the parent. Speciﬁcally,
this is when the child zone updates its key-set, but the par-
ent has yet to update the corresponding delegation (DS)