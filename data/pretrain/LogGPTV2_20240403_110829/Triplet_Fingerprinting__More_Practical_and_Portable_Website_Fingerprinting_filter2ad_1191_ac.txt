79.4 ± 1.6 92.2 ± 0.6 93.9 ± 0.2 94.4 ± 0.3 94.5 ± 0.2
81.2 ± 1.3 92.9 ± 0.6 94.3 ± 0.7 94.7 ± 0.5 94.7 ± 0.3
79.6 ± 1.9 92.7 ± 0.8 94.1 ± 0.9 94.7 ± 0.7 95.0 ± 0.5
79.7 ± 1.7 93.0 ± 1.4 94.2 ± 0.9 94.5 ± 1.1 95.0 ± 0.8
80.6 ± 2.3 93.4 ± 0.9 94.6 ± 0.7 94.7 ± 0.8 95.0 ± 0.9
5-shot learning impressively attains 92% of accuracy. We observe
that the accuracy of the attacks starts leveling off after n >= 15 at
94% accuracy.
TF Goals: According to the results in the experiment, we sum-
marize how the TF attack can improve the performance of WF
attacks:
• Flexibility & transferability: The results show that even if
the classifier is trained on one dataset and tested on different
dataset in term of their websites’ labels, the performance still
remains effective. Therefore, the attacker can directly perform
WF attacks without worrying about whether or not the websites
that he would like to monitor were included in the classifier
during the training process.
• Bootstrap time: The attacker only needs to collect five exam-
ples per website, taking ∼5 minutes per website, to N -train the
classifier. This supports cooperation among attackers in which
one attacker with more time to collect data can periodically train
the feature extractor, while other attackers only need to collect
data for their sites of interest close to the time of use.
• Performance of attack The results demonstrate that the TF
attack can still remain effective with over 90% accuracy.
Possible Advantages of the Attacker: The attacker may believe
that including some samples of the same class in both the pre-
training and N-training phases will improve performance (ie. the
pre-training and N-training datasets are not disjoint). This would
allow the triplet networks to train and test with the partial set of
websites that the model has seen before. As an example, an attacker
may believe that www.foxnews.com is commonly selected as a
monitored site by other attackers who use his model, so he decides
to include examples of www.foxnews.com during the training phase.
It is then interesting to evaluate whether or not this inclusion has
improved the model’s ability to identify sites of label www.foxnews.
com. To test this, we perform experimental evaluations to compare
the disjointed websites case with different percentages of inclusion.
The inclusion rates are ranged from from 25% to 100%. The results
of these experiments are shown in Table 6.
We find that allowing inclusion between our training sets does
not provide noticeable improvement in attack performance. A rea-
son for this may be related to the fact that DL models that use
softmax classification learn to locally map the given input to their
corresponding class. This leads to overfitting and a more rigid
model. By contrast, NSL with triplet networks has the model learn
to differentiate the given pair of inputs (similar or dissimilar) with-
out locally mapping to the particular website’s label to be assigned.
This allows the model to more effectively learn on small numbers
of samples. This is the compelling property of NSL which allows us
to achieve the flexibility and transferability goals of our classifier.
Furthermore, it is interesting to compare the performance of the
TF attack with previously-proposed WF attacks using hand-crafted
features as these attacks have been shown to be effective with less
training data than DL attacks. For this purpose, the TF attack trained
with 100% inclusion is most appropriate for comparison against
the baseline attacks. We find that the TF attack is clearly superior
to the baselines when small sample counts are usedÐe.g. CUMUL
and k-FP achieve 42.1% and 36.3% accuracy respectively where N =1
(1-shot learning) whereas TF attack reaches 80.6% accuracy. We
observe the accuracy improves significantly when the number of
N examples increases for both CUMUL and k-FP attacks, however
the performance of the TF attack still significantly outperforms
the baselines in all settings. This confirms that the TF attack is
distinctive in its ability to achieve high WF performance in low
traffic example settings.
7.5 WF attacks with different data distributions
Next, we evaluate the performance of the WF attack under the
scenario in which the training and testing data are collected from
different distributions.
Experimental setting: We use the same triplet model from the
first experiment trained with the AWF775 dataset. However, we
instead use the Wang100 dataset as the testing data to newly train
the k-NN classifier during the testing phase. The experimental set-
ting is designed to evaluate the performance of the attack in which
the model that is trained on one specific time and TBB version but
tested against a significantly different time5. The 3-year difference
between the AWF and Wang datasets ensures that the two’s distri-
butions differ significantly. To verify this we perform analysis using
Cosine similarity in Appendix A and conclude that these datasets
are very likely to be mismatched.
In these experiments, we decided to initially train TF on the a
dataset collected in 2016 and test on a dataset collected in 2013
primarily due to the larger variety of websites in the 2016 dataset.
Triplet networks learn to classify by identifying the differences
between classes, and so it is important that a large number of
classes are contained in the data which is used to initially train
the model. Since the objective of this experiment is to evaluate if
TF can mitigate the adverse effects of data mismatch between the
training phases, we believe the order of the timing does not affect
the validity of our evaluations.
Result: As we see in Table 7, the results show that the TF attack
remains fairly effective and achieves almost 85% with 5-shot learn-
ing. Moreover, we observe that the accuracy of the attack gradually
increase up to 87% with 20-shot learning.
WF Attack’s Goals of Improvements:
In this experiment, the
results demonstrate that the TF attack achieves another one of our
goals:
• Generalizability: The results demonstrates that the feature ex-
tractor can be trained on traffic traces having one distribution
5A real-world attacker must still capture a small dataset of fresh representative samples
for the testing phase.
Table 7: The performance of WF attacks with different dis-
tributions of training and testing datasets (Accuracy)
Table 9: The performance of WF attacks: Different distribu-
tions of datasets using transfer learning (Accuracy)
Type of
Experiment
Number of N Example(s)
1
5
10
15
20
Different Distributions
73.1 ± 1.8
84.5 ± 0.4
86.2 ± 0.4
86.6 ± 0.3
87.0 ± 0.3
Approach
Traditional
TF
1
Number of N Example(s)
5
15
10
20
8.6 ± 1.4
73.1 ± 1.8
31.1 ± 0.9
84.5 ± 0.4
49.0 ± 0.5
86.2 ± 0.4
52.5 ± 0.4
86.6 ± 0.3
56.3 ± 0.9
87.0 ± 0.3
Table 8: The performance of WF attacks: Similar but mutu-
ally exclusive datasets (Accuracy)
Approach
Traditional
TF
1
Number of N Example(s)
15
5
10
20
27.9 ± 5.0
79.2 ± 1.3
87.6 ± 0.4
92.2 ± 0.6
93.4 ± 0.2
93.9 ± 0.2
95.2 ± 0.1
94.4 ± 0.3
95.1 ± 0.1
94.5 ± 0.2
and used in an attack on traffic traces with a different distribu-
tion. Thus, a WF attack using NSL allows the attacker to adopt
a feature extractor trained on older data and still perform WF
attacks with respectable accuracy.
7.6 Traditional Transfer Learning vs TF Attack
The transfer learning [37] is a machine learning technique in which
a model trained on one task can be effectively re-used on another.
This technique has widely shown to be effective in many domains of
applications. The intuition behind the effectiveness of the transfer
learning results from the ways that deep learning learns features
representations. In computer vision, deep learning tries to learn
and detect lower-level features such as edges in their earlier layers
and higher-level features such as objects in the deeper layers. This
hierarchical learning allows the users to directly transfer the knowl-
edge of learned features from the early layers and only fine-tune
the deeper layers to fit the model for their tasks. Thus, the user
does not need to re-train the model from scratch.
However, there are challenges in applying transfer learning to
the WF domain. These challenges come from the fact that the
distributions of data used to train the pre-trained model and test
the model are different due to changes in Web traffic over time.
Therefore, it is interesting to investigate how effective transfer
learning is when compared to the TF attack.
To clearly distinguish between the aforementioned transfer learn-
ing and the TF attack, we will use the term traditional approach to
represent the general transfer learning technique and use the TF
approach to represent our TF attack.
Experimental setting: To pre-train the model, we use the DF ar-
chitecture trained with the AWF775 dataset. We follow the recom-
mendations in machine learning (ML) to re-train model by freezing
k early layers out of the n total layers during the re-training pro-
cess6. We test with different values of k to maximize the accuracy
of the attack, and we find that freezing all layers except the last
FC layer with softmax provides the highest accuracy. Thus, we use
the setting where we freeze n − 1 layers to re-train the pre-trained
model. We then compare the performance of the attack in the tra-
ditional approach to the TF approach using two different scenarios
as in the previous experiments.
Result: Table 8 shows the performance of WF attacks using simi-
lar but mutually exclusive datasets. The results show that the TF
6http://www.deeplearningessentials.science/transferLearning/
Table 10: Open World: Results when tuned for precision and
tuned for recall (Similar but mutually exclusive datasets)
N-Examples
5
10
15
20
Tuned for Precision
Recall
Precision
0.808
0.788
0.829
0.862
0.871
0.908
0.891
0.873
Tuned for Recall
Precision Recall
0.893
0.948
0.966
0.968
0.804
0.730
0.692
0.706
approach performs significantly better compared to the traditional
approach when the available number of N examples is small (N =
1 or 5). We observe that after number N examples starts growing,
both approaches similarly perform well with over 93% accuracy.
The results suggest that if the attacker has a small dataset to re-train
the classifier, the TF approach is the better choice.
On the other hand, the effectiveness of the traditional approach is
significantly degraded under the more challenging scenario. Table 9
shows the performance of WF attacks with different distributions
of training and testing datasets. By contrast, the performance of
the TF attacks is noticeably higher, indicating that the TF approach
can better mitigate the negative effects of data mismatch. As we
see, with small N examples, the TF approach provides 50% higher
accuracy than the traditional approach. Furthermore, even if the
size of N becomes larger e.g. with 20 examples used for training
each website, the accuracy of the traditional approach is only 56%
whereas the TF approach reaches 87%. The results suggest that if
the attacker knows that the dataset used for pre-training is likely to
be dissimilar to the data in the attack phase, then the TF approach
is more effective.
7.7 Open-World Scenario
In the previous experiments, we explored the performance of the
TF attack under the closed-world scenario. However, this scenario
is unrealistic, as it assumes that the users will only visit websites
within the monitored set. In the following experiment, we evaluate
the performance of the TF attack in the more realistic open-world
setting. In the open-world, the classifier must learn to distinguish
between monitored and unmonitored sites. We use precision and
recall metrics to evaluate the performance of the attack.
Experimental Setting: We evaluate the open-world setting under
the standard model in which we include the unmonitored samples
as an additional label during training. We use AWF100 dataset as
monitored websites and AWF9000 as the unmonitored websites to
evaluate WF attacks with similar but mutually exclusive datasets.
Moreover, we use Wang100 dataset as monitored websites and
Wang9000 as the unmonitored websites to evaluate WF attacks
with different data distributions.
not consider all webpages within the website. The TF attack makes
the fingerprinting of many webpages from a given website more
feasible due to the reduced data requirements.
Threat landscape: The ability to use a pre-trained feature extractor
with a few network examples may allow a less powerful adversary
to perform WF. Thus, NSL expands the threat landscape, such that
the attack is not limited to attackers with significant computing
resources.
Countermeasures: The results from our WTF-PAD experiment
demonstrates that the TF attack is not as effective as the DF attack
when traffic is protected. This suggests that light-weight padding
mechanisms may be further developed to target NSL attacks.
Limitations: While we investigated dataset mismatch during the
initial training phase, we were not able to evaluate the effects of mis-
match during new-training. However, we feel this scenario is easily
avoided in real-world application since the burden of collecting a
fresh samples is low and many adversaries (eg. ISP or compromised
router) can likely replicate their target’s network conditions.
In addition, we have left the problem of session extraction and
multi-tab browsing unaddressed. Recent work [35] has proposed
algorithmic stream-splitting and chunk-based classification to ad-
dress these problems, however attack performance remains inad-
equate for real-world application. To appropriately address these
problems new classifiers will need to be developed.
9 CONCLUSION
In this study, we investigated the use of N-shot learning to improve