supports a small number of priority levels (typically 8), with
one queue for each priority. Each incoming packet indicates
which queue to use for that packet, and output ports service
higher priority queues before lower priority ones. The key to
low latency is assigning packet priorities so that short messages
bypass queued packets for longer messages.
This observation is not new; starting with pFabric [4], several
schemes have shown that switch-based priorities can be used
to improve message latency [6, 7, 13, 14]. These schemes use
priorities to implement various message-size-based scheduling
policies. The most common of these policies is SRPT (shortest
remaining processing time first), which prioritizes packets from
messages with the fewest bytes remaining to transmit. SRPT
provides near-optimal average message latency, and as shown
in prior work [4, 17], it also provides very good tail latency for
short messages. Homa implements an approximation of SRPT
(though the design can support other policies as well).
Unfortunately, in practice, no existing scheme can deliver the
near-optimal latency of SRPT at high network load. pFabric
approximates SRPT accurately, but it requires too many priority
levels to implement with today’s switches. PIAS [6] works with
a limited number of priorities, but it assigns priorities on senders,
which limits its ability to approximate SRPT (see below). In ad-
dition, it works without message sizes, so it uses a “multi-level
queue” scheduling policy. As a result, PIAS has high tail latency
both for short messages and long ones. QJUMP [14] requires
priorities to be allocated manually on a per-application basis,
which is too inflexible to produce optimal latencies.
Making best use of limited priorities requires receiver con-
trol. To produce the best approximation of SRPT with only
a small number of priority levels, the priorities should be de-
termined by the receiver. Except for blind transmissions, the
receiver knows the exact set of messages vying for bandwidth on
its downlink from the TOR switch. As a result, the receiver can
best decide which priority to use for each incoming packet. In ad-
dition, the receiver can amplify the effectiveness of the priorities
by integrating them with a packet scheduling mechanism.
pHost [13], the closest prior scheme to Homa, is an example
of using a receiver-driven approach to approximate SRPT. Its
primary mechanism is packet scheduling: senders transmit the
first RTTbytes of each message blindly, but packets after that
are transmitted only in response to explicit grants from the re-
ceiver. Receivers schedule the grants to implement SRPT while
controlling the influx of packets to match the downlink speed.
However, pHost makes only limited use of priorities: it stat-
ically assigns one high priority for all blind transmissions and
one lower priority for all scheduled packets. This impacts its
ability to approximate SRPT in two ways. First, it bundles all
blind transmissions into a single priority. While this is reason-
able for workloads where most bytes are from large messages
(W4-W5 in Figure 1), it is problematic for workloads where a
large fraction of bytes are transmitted blindly (W1-W3). Second,
for messages longer than RTTbytes, pHost cannot preempt a
larger message immediately for a shorter one. Once again, the
root of the problem is that pHost bundles all such messages into
a single priority, which results in queueing delays. We will show
in §3.4 that this creates preemption lag, which hurts latency,
particularly for medium-sized messages that last a few RTTs.
SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
Receivers must allocate priorities dynamically. Homa ad-
dresses pHost’s limitations by dynamically allocating multiple
priorities at the receivers. Each receiver allocates priorities for
its own downlink using two mechanisms. For messages larger
than RTTbytes, the receiver communicates a priority for each
packet to its sender dynamically based on the exact set of in-
bound messages. This eliminates almost all preemption lag. For
short messages sent blindly, the sender cannot know about other
messages inbound for the receiver. Even so, the receiver can
provide guidance in advance to senders based on its recent work-
load. Our experiments show that dynamic priority management
reduces tail latency considerably in comparison to static priority
allocation schemes such as those in pHost or PIAS.
Receivers must overcommit their downlink in a controlled
manner. Scheduling packet transmissions with grants from
receivers reduces buffer occupancy, but it introduces a new
challenge: a receiver may send grants to a sender that does not
transmit to it in a timely manner. This problem occurs, for in-
stance, when a sender has messages for multiple receivers; if
more than one receiver decides to send it grants, the sender
cannot transmit packets to all such receivers at full speed. This
wastes bandwidth at the receiver downlinks and can signifi-
cantly hurt performance at high network load. For example, we
find that the maximum load that pHost can support ranges be-
tween 58% and 73% depending on the workload, despite using
a timeout mechanism to mitigate the impact of unresponsive
senders (§5.2). NDP [15] also schedules incoming packets to
avoid buffer buildup, and it suffers from a similar problem.
To address this challenge, Homa’s receivers intentionally
overcommit their downlinks by granting simultaneously to a
small number of senders; this results in controlled packet queu-
ing at the receiver’s TOR but is crucial to achieve high network
utilization and the best message latency at high load (§3.5).
Senders need SRPT also. Queues can build up at senders as
well as receivers, and this can result in long delays for short
messages. For example, most existing protocols implement
byte streams, and an application will typically use a single
stream for each destination. However, this can result in head-
of-line-blocking, where a short message for a given destination
is queued in the byte stream behind a long message for the
same destination. §5.1 will show that this increases tail latency
by 100x for short messages. FIFO packet queues in the NIC
can also result in high tail latency for short messages, even if
messages are transmitted on different streams. For low tail la-
tency, senders must ensure that short outgoing messages are not
delayed by long ones.
Putting it all together. Figure 2 shows an overview of the Homa
protocol. Homa divides messages into two parts: an initial un-
scheduled portion followed by a scheduled portion. The sender
transmits the unscheduled packets (RTTbytes of data) imme-
diately, but it does not transmit any scheduled packets until
instructed by the receiver. The arrival of an unscheduled packet
makes the receiver aware of the message; the receiver then re-
quests the transmission of scheduled packets by sending one
Figure 2: Overview of the Homa protocol. Sender1 is transmitting
scheduled packets of message m1, while Sender2 is transmitting
unscheduled packets of m2.
grant packet for each scheduled packet. Homa’s receivers dy-
namically set priorities for scheduled packets and periodically
notify senders of a set of thresholds for setting priorities for un-
scheduled packets. Finally, the receivers implement controlled
overcommitment to sustain high utilization in the presence of un-
responsive senders. The net effect is an accurate approximation
of the SRPT scheduling policy using a small number of priority
queues. We will show that this yields excellent performance
across a broad range of workloads and traffic conditions.
3 HOMA DESIGN
This section describes the Homa protocol in detail. In addition
to describing how Homa implements the key ideas from the
previous section, this section also discusses several other as-
pects of the protocol that are less essential for performance
but result in a complete and practical substrate for datacenter
RPC. Homa contains several unusual features: it is receiver-
driven; it is message-oriented, rather than stream-oriented; it
is connectionless; it uses no explicit acknowledgments; and it
implements at-least-once semantics, rather than the more tra-
ditional at-most-once semantics. Homa uses four packet types,
which are summarized in Figure 3.
3.1 RPCs, not connections
Homa is connectionless. It implements the basic data transport
for RPCs, each of which consists of a request message from a
client to a server and its corresponding response message. Each
RPC is identified by a globally unique RPCid generated by the
client. The RPCid is included in all packets associated with the
RPC. A client may have any number of outstanding RPCs at a
time, to any number of servers; concurrent RPCs to the same
server may complete in any order.
Independent delivery of messages is essential for low tail
latency. The streaming approach used by TCP results in head-
of-line-blocking, where a short message is queued behind a
long message for the same destination. §5.1 will show that this
P0ScheduledUnscheduledSender1Sender2Receiverm1m2UnscheduledScheduledP7DatacenterNetworkTOR Egress Port100110101100110101GGHoma: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
DATA
GRANT
BUSY
Sent from sender to receiver. Contains a range of bytes within
a message, defined by an offset and a length. Also indicates total
message length.
Sent from receiver to sender. Indicates that the sender may now
transmit all bytes in the message up to a given offset, and specifies
the priority level to use.
re-transmit a given range of bytes within a message.
Sent from sender to receiver. Indicates that a response to
RESEND will be delayed (the sender is busy transmitting higher
priority messages, or an RPC operation is still being executed);
used to prevent timeouts.
RESEND Sent from receiver to sender. Indicates that sender should
Figure 3: The packet types used by Homa. All packet types except
DATA are sent at highest priority; the priorities for DATA packets
are specified by the receiver as discussed in §3.4.
increases tail latency by 100x for short messages. Many re-
cent proposals, such as DCTCP, pFabric, and PIAS, assume
dozens of connections between each source-target pair, so that
each messsage has a dedicated connection. However, this ap-
proach results in an explosion of connection state. Even a single
connection for each application-server pair is problematic for
large-scale applications ([23] §3.1, [11] §3.1), so it is probably
not realistic to use multiple connections.
No setup phase or connection is required before a client ini-
tiates an RPC to a server, and neither the client nor the server
retains any state about an RPC once the client has received
the result. In datacenter applications, servers can have large
numbers of clients; for example, servers in Google datacenters
commonly have several hundred thousand open connections
[12]. Homa’s connectionless approach means that the state kept
on a server is determined by the number of active RPCs, not the
total number of clients.
Homa requires a response for each RPC request because this
is the common case in datacenter applications and it allows
the response to serve as an acknowledgment for the request.
This reduces the number of packets required (in the simplest
case, there is only a single request packet and a single response
packet). One-way messages can be simulated by having the
server application return an empty response immediately upon
receipt of the request.
Homa handles request and response messages in nearly iden-
tical fashion, so we don’t distinguish between requests and
responses in most of the discussion below.
Although we designed Homa for newer datacenter applica-
tions where RPC is a natural fit, we believe that traditional appli-
cations could be supported by implementing a socket-like byte
stream interface above Homa. We leave this for future work.
3.2 Basic sender behavior
When a message arrives at the sender’s transport module, Homa
divides the message into two parts: an initial unscheduled por-
tion (the first RTTbytes bytes), followed by a scheduled portion.
The sender transmits the unscheduled bytes immediately, using
one or more DATA packets. The scheduled bytes are not trans-
mitted until requested explicitly by the receiver using GRANT
packets. Each DATA packet has a priority, which is determined
by the receiver as described in §3.4.
The sender implements SRPT for its outgoing packets: if
DATA packets from several messages are ready for transmission
at the same time, packets for the message with the fewest remain-
ing bytes are sent first. The sender does not consider the prior-
ities in the DATA packets when scheduling its packet transmis-
sions (the priorities in DATA packets are intended for the final
downlinks to the receivers). Control packets such as GRANTs
and RESENDs are always given priority over DATA packets.
3.3 Flow control
Flow control in Homa is implemented on the receiver side by
scheduling incoming packets on a packet-by-packet basis, like
pHost and NDP. Under most conditions, whenever a DATA
packet arrives at the receiver, the receiver sends a GRANT
packet back to the sender. The grant invites the sender to trans-
mit all bytes in the message up to a given offset, and the offset is
chosen so that there are always RTTbytes of data in the message
that have been granted but not yet received. Assuming timely
delivery of grants back to the sender and no competition from
other messages, messages can be transmitted from start to finish
at line rate with no delays.
If multiple messages arrive at a receiver simultaneously, their
DATA packets will interleave as determined by their priorities.
If the DATA packets of a message are delayed, then GRANTs
for that message will also be delayed, so there will never be more
than RTTbytes of granted-but-not-received data for a message.
This means that each incoming message can occupy at most
RTTbytes of buffer space in the receiver’s TOR.
If there are multiple incoming messages, the receiver may
stop sending grants to some of them, as part of the overcom-
mitment limits described in §3.5. Once a grant has been sent
for the last bytes of a message, data packets for that message
may result in grants to other messages for which grants had
previously been stopped.
The DATA packets for a message can arrive in any order;
the receiver collates them using the offsets in each packet. This
allows Homa to use per-packet multi-path routing in order to
minimize congestion in the network core.
3.4 Packet priorities
The most novel feature in Homa, and the key to its performance,
is its use of priorities. Each receiver determines the priorities
for all of its incoming DATA packets in order to approximate
the SRPT policy. It uses different mechanisms for unscheduled
and scheduled packets. For unscheduled packets, the receiver
allocates priorities in advance. It uses recent traffic patterns
to choose priority allocations, and it disseminates that infor-
mation to senders by piggybacking it on other packets. Each