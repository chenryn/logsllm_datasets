clients
(VSCs)
Windows
kernel
Enlightenments
FIGURE 9-3 Components of a child partition.
CHAPTER 9 Virtualization technologies
271
Processes and threads
The Windows hypervisor represents a virtual machine with a partition data structure. A partition, 
as described in the previous section, is composed of some memory (guest physical memory) and one 
or more virtual processors (VP). Internally in the hypervisor, each virtual processor is a schedulable 
entity, and the hypervisor, like the standard NT kernel, includes a scheduler. The scheduler dispatches 
the execution of virtual processors, which belong to different partitions, to each physical CPU. (We 
discuss the multiple types of hypervisor schedulers later in this chapter in the “Hyper-V schedulers” 
section.) A hypervisor thread (TH_THREAD data structure) is the glue between a virtual processor and 
its schedulable unit. Figure 9-4 shows the data structure, which represents the current physical execu-
tion context. It contains the thread execution stack, scheduling data, a pointer to the thread’s virtual 
processor, the entry point of the thread dispatch loop (discussed later) and, most important, a pointer 
to the hypervisor process that the thread belongs to.
Scheduling
Information
Physical Processor
Local Storage (PLS)
VP Stack
Owning Process
Dispatch Loop
Entry Point
FIGURE 9-4 The hypervisor’s thread data structure.
The hypervisor builds a thread for each virtual processor it creates and associates the newborn 
thread with the virtual processor data structure (VM_VP).
A hypervisor process (TH_PROCESS data structure), shown in Figure 9-5, represents a partition 
and is a container for its physical (and virtual) address space. It includes the list of the threads (which 
are backed by virtual processors), scheduling data (the physical CPUs affinity in which the process is 
allowed to run), and a pointer to the partition basic memory data structures (memory compartment, 
reserved pages, page directory root, and so on). A process is usually created when the hypervisor 
builds the partition (VM_PARTITION data structure), which will represent the new virtual machine.
Scheduling
Information
Thread List
Partition’s Memory
Compartment
FIGURE 9-5 The hypervisor’s process data structure.
272 
CHAPTER 9 Virtualization technologies
Enlightenments
Enlightenments are one of the key performance optimizations that Windows virtualization takes ad-
vantage of. They are direct modifications to the standard Windows kernel code that can detect that the 
operating system is running in a child partition and perform work differently. Usually, these optimiza-
tions are highly hardware-specific and result in a hypercall to notify the hypervisor. 
An example is notifying the hypervisor of a long busy–wait spin loop. The hypervisor can keep some 
state on the spin wait and decide to schedule another VP on the same physical processor until the wait 
can be satisfied. Entering and exiting an interrupt state and access to the APIC can be coordinated with 
the hypervisor, which can be enlightened to avoid trapping the real access and then virtualizing it.
Another example has to do with memory management, specifically translation lookaside buffer 
(TLB) flushing. (See Part 1, Chapter 5, “Memory management,” for more information on these con-
cepts.) Usually, the operating system executes a CPU instruction to flush one or more stale TLB entries, 
which affects only a single processor. In multiprocessor systems, usually a TLB entry must be flushed 
from every active processor’s cache (the system sends an inter-processor interrupt to every active 
processor to achieve this goal). However, because a child partition could be sharing physical CPUs with 
many other child partitions, and some of them could be executing a different VM’s virtual processor 
at the time the TLB flush is initiated, such an operation would also flush this information for those VMs. 
Furthermore, a virtual processor would be rescheduled to execute only the TLB flushing IPI, resulting 
in noticeable performance degradation. If Windows is running under a hypervisor, it instead issues a 
hypercall to have the hypervisor flush only the specific information belonging to the child partition.
Partition’s privileges, properties, and version features
When a partition is initially created (usually by the VID driver), no virtual processors (VPs) are associated 
with it. At that time, the VID driver is free to add or remove some partition’s privileges. Indeed, when 
the partition is first created, the hypervisor assigns some default privileges to it, depending on its type. 
A partition’s privilege describes which action—usually expressed through hypercalls or synthetic 
MSRs (model specific registers)—the enlightened OS running inside a partition is allowed to perform 
on behalf of the partition itself. For example, the Access Root Scheduler privilege allows a child parti-
tion to notify the root partition that an event has been signaled and a guest’s VP can be rescheduled 
(this usually increases the priority of the guest’s VP-backed thread). The Access VSM privilege instead 
allows the partition to enable VTL 1 and access its properties and configuration (usually exposed 
through synthetic registers). Table 9-1 lists all the privileges assigned by default by the hypervisor.
Partition privileges can only be set before the partition creates and starts any VPs; the hypervisor 
won’t allow requests to set privileges after a single VP in the partition starts to execute. Partition prop-
erties are similar to privileges but do not have this limitation; they can be set and queried at any time. 
There are different groups of properties that can be queried or set for a partition. Table 9-2 lists the 
properties groups.
When a partition is created, the VID infrastructure provides a compatibility level (which is specified 
in the virtual machine’s configuration file) to the hypervisor. Based on that compatibility level, the hy-
pervisor enables or disables specific virtual hardware features that could be exposed by a VP to the un-
derlying OS. There are multiple features that tune how the VP behaves based on the VM’s compatibility 
CHAPTER 9 Virtualization technologies
273
level. A good example would be the hardware Page Attribute Table (PAT), which is a configurable cach-
ing type for virtual memory. Prior to Windows 10 Anniversary Update (RS1), guest VMs weren’t able 
to use PAT in guest VMs, so regardless of whether the compatibility level of a VM specifies Windows 
10 RS1, the hypervisor will not expose the PAT registers to the underlying guest OS. Otherwise, in case 
the compatibility level is higher than Windows 10 RS1, the hypervisor exposes the PAT support to the 
underlying OS running in the guest VM. When the root partition is initially created at boot time, the 
hypervisor enables the highest compatibility level for it. In that way the root OS can use all the features 
supported by the physical hardware.
TABLE 9-1 Partition’s privileges
PARTITION TYPE
DEFAULT PRIVILEGES
Root and child partition
Read/write a VP’s runtime counter
Read the current partition reference time 
Access SynIC timers and registers 
Query/set the VP's virtual APIC assist page 
Read/write hypercall MSRs 
Request VP IDLE entry 
Read VP’s index 
Map or unmap the hypercall’s code area 
Read a VP’s emulated TSC (time-stamp counter) and its frequency
Control the partition TSC and re-enlightenment emulation 
Read/write VSM synthetic registers 
Read/write VP’s per-VTL registers 
Starts an AP virtual processor 
Enables partition’s fast hypercall support 
Root partition only
Create child partition 
Look up and reference a partition by ID 
Deposit/withdraw memory from the partition compartment 
Post messages to a connection port 
Signal an event in a connection port’s partition 
Create/delete and get properties of a partition's connection port 
Connect/disconnect to a partition's connection port 
Map/unmap the hypervisor statistics page (which describe a VP, LP, partition, or hypervisor)
Enable the hypervisor debugger for the partition 
Schedule child partition’s VPs and access SynIC synthetic MSRs 
Trigger an enlightened system reset
Read the hypervisor debugger options for a partition 
Child partition only
Generate an extended hypercall intercept in the root partition
Notify a root scheduler’s VP-backed thread of an event being signaled
EXO partition
None
TABLE 9-2 Partition’s properties
PROPERTY GROUP
DESCRIPTION
Scheduling properties
Set/query properties related to the classic and core scheduler, like Cap, Weight, and Reserve
Time properties
Allow the partition to be suspended/resumed
Debugging properties
Change the hypervisor debugger runtime configuration
Resource properties
Queries virtual hardware platform-specific properties of the partition (like TLB size, SGX 
support, and so on)
Compatibility properties
Queries virtual hardware platform-specific properties that are tied to the initial compatibil-
ity features
274 
CHAPTER 9 Virtualization technologies
The hypervisor startup
In Chapter 12, we analyze the modality in which a UEFI-based workstation boots up, and all the compo-
nents engaged in loading and starting the correct version of the hypervisor binary. In this section, we 
briefly discuss what happens in the machine after the HvLoader module has transferred the execution 
to the hypervisor, which takes control for the first time.
The HvLoader loads the correct version of the hypervisor binary image (depending on the CPU 
manufacturer) and creates the hypervisor loader block. It captures a minimal processor context, which 
the hypervisor needs to start the first virtual processor. The HvLoader then switches to a new, just-
created, address space and transfers the execution to the hypervisor image by calling the hypervisor 
image entry point, KiSystemStartup, which prepares the processor for running the hypervisor and ini-
tializes the CPU_PLS data structure. The CPU_PLS represents a physical processor and acts as the PRCB 
data structure of the NT kernel; the hypervisor is able to quickly address it (using the GS segment). 
Differently from the NT kernel, KiSystemStartup is called only for the boot processor (the application 
processors startup sequence is covered in the “Application Processors (APs) Startup” section later in this 
chapter), thus it defers the real initialization to another function, BmpInitBootProcessor. 
BmpInitBootProcessor starts a complex initialization sequence. The function examines the system 
and queries all the CPU’s supported virtualization features (such as the EPT and VPID; the queried 
features are platform-specific and vary between the Intel, AMD, or ARM version of the hypervisor). It 
then determines the hypervisor scheduler, which will manage how the hypervisor will schedule virtual 
processors. For Intel and AMD server systems, the default scheduler is the core scheduler, whereas the 
root scheduler is the default for all client systems (including ARM64). The scheduler type can be manu-
ally overridden through the hypervisorschedulertype BCD option (more information about the different 
hypervisor schedulers is available later in this chapter).
The nested enlightenments are initialized. Nested enlightenments allow the hypervisor to be ex-
ecuted in nested configurations, where a root hypervisor (called L0 hypervisor), manages the real hard-
ware, and another hypervisor (called L1 hypervisor) is executed in a virtual machine. After this stage, the 
BmpInitBootProcessor routine performs the initialization of the following components:
I 
Memory manager (initializes the PFN database and the root compartment).
I 
The hypervisor’s hardware abstraction layer (HAL).
I 
The hypervisor’s process and thread subsystem (which depends on the chosen scheduler type).
The system process and its initial thread are created. This process is special; it isn’t tied to any
partition and hosts threads that execute the hypervisor code.
I 
The VMX virtualization abstraction layer (VAL). The VAL’s purpose is to abstract differences be-
tween all the supported hardware virtualization extensions (Intel, AMD, and ARM64). It includes
code that operates on platform-specific features of the machine’s virtualization technology in
use by the hypervisor (for example, on the Intel platform the VAL layer manages the “unrestrict-
ed guest” support, the EPT, SGX, MBEC, and so on).
I 
The Synthetic Interrupt Controller (SynIC) and I/O Memory Management Unit (IOMMU).
CHAPTER 9 Virtualization technologies
275
I 
The Address Manager (AM), which is the component responsible for managing the physical
memory assigned to a partition (called guest physical memory, or GPA) and its translation to
real physical memory (called system physical memory). Although the first implementation of
Hyper-V supported shadow page tables (a software technique for address translation), since
Windows 8.1, the Address manager uses platform-dependent code for configuring the hyper-
visor address translation mechanism offered by the hardware (extended page tables for Intel,
nested page tables for AMD). In hypervisor terms, the physical address space of a partition is
called address domain. The platform-independent physical address space translation is com-
monly called Second Layer Address Translation (SLAT). The term refers to the Intel’s EPT, AMD’s
NPT or ARM 2-stage address translation mechanism.
The hypervisor can now finish constructing the CPU_PLS data structure associated with the boot 
processor by allocating the initial hardware-dependent virtual machine control structures (VMCS for 
Intel, VMCB for AMD) and by enabling virtualization through the first VMXON operation. Finally, the 
per-processor interrupt mapping data structures are initialized.
EXPERIMENT: Connecting the hypervisor debugger
In this experiment, you will connect the hypervisor debugger for analyzing the startup sequence 
of the hypervisor, as discussed in the previous section. The hypervisor debugger is supported 
only via serial or network transports. Only physical machines can be used to debug the hypervi-
sor, or virtual machines in which the “nested virtualization” feature is enabled (see the “Nested 
virtualization” section later in this chapter). In the latter case, only serial debugging can be en-
abled for the L1 virtualized hypervisor.
For this experiment, you need a separate physical machine that supports virtualization exten-
sions and has the Hyper-V role installed and enabled. You will use this machine as the debugged 
system, attached to your host system (which acts as the debugger) where you are running the 
debugging tools. As an alternative, you can set up a nested VM, as shown in the “Enabling nested 
virtualization on Hyper-V” experiment later in this chapter (in that case you don’t need another 
physical machine). 
As a first step, you need to download and install the “Debugging Tools for Windows” in the 
host system, which are available as part of the Windows SDK (or WDK), downloadable from 
https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk. As an alternative, 
for this experiment you also can use the WinDbgX, which, at the time of this writing, is available 
in the Windows Store by searching “WinDbg Preview.”
The debugged system for this experiment must have Secure Boot disabled. The hypervi-
sor debugging is not compatible with Secure Boot. Refer to your workstation user manual for 
understanding how to disable Secure Boot (usually the Secure Boot settings are located in the 
UEFI Bios). For enabling the hypervisor debugger in the debugged system, you should first open 
an administrative command prompt (by typing cmd in the Cortana search box and selecting Run 
as administrator). 
EXPERIMENT: Connecting the hypervisor debugger
In this experiment, you will connect the hypervisor debugger for analyzing the startup sequence 
of the hypervisor, as discussed in the previous section. The hypervisor debugger is supported 
only via serial or network transports. Only physical machines can be used to debug the hypervi-
sor, or virtual machines in which the “nested virtualization” feature is enabled (see the “Nested 
virtualization” section later in this chapter). In the latter case, only serial debugging can be en-
abled for the L1 virtualized hypervisor.
For this experiment, you need a separate physical machine that supports virtualization exten-
sions and has the Hyper-V role installed and enabled. You will use this machine as the debugged 
system, attached to your host system (which acts as the debugger) where you are running the 
debugging tools. As an alternative, you can set up a nested VM, as shown in the “Enabling nested 
virtualization on Hyper-V” experiment later in this chapter (in that case you don’t need another 
physical machine). 
As a first step, you need to download and install the “Debugging Tools for Windows” in the 
host system, which are available as part of the Windows SDK (or WDK), downloadable from 
https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk. As an alternative, 
for this experiment you also can use the WinDbgX, which, at the time of this writing, is available 
in the Windows Store by searching “WinDbg Preview.”
The debugged system for this experiment must have Secure Boot disabled. The hypervi-
sor debugging is not compatible with Secure Boot. Refer to your workstation user manual for 
understanding how to disable Secure Boot (usually the Secure Boot settings are located in the 
UEFI Bios). For enabling the hypervisor debugger in the debugged system, you should first open 
an administrative command prompt (by typing cmd in the Cortana search box and selecting Run 
as administrator). 
276 
CHAPTER 9 Virtualization technologies
In case you want to debug the hypervisor through your network card, you should type the 
following commands, replacing the terms  with the IP address of the host system; 
” with a valid port in the host (from 49152); and  with the 
bus parameters of the network card of the debugged system, specified in the XX.YY.ZZ format 
(where XX is the bus number, YY is the device number, and ZZ is the function number). You 
can discover the bus parameters of your network card through the Device Manager applet or 
through the KDNET.exe tool available in the Windows SDK:
bcdedit /hypervisorsettings net hostip: port: 
bcdedit /set {hypervisorsettings} hypervisordebugpages 1000 
bcdedit /set {hypervisorsettings} hypervisorbusparams  
bcdedit /set hypervisordebug on
The following figure shows a sample system in which the network interface used for debug-
ging the hypervisor is located in the 0.25.0 bus parameters, and the debugger is targeting a host 
system configured with the IP address 192.168.0.56 on the port 58010.
Take note of the returned debugging key. After you reboot the debugged system, you should 
run Windbg in the host, with the following command:
windbg.exe -d -k net:port=,key=
You should be able to debug the hypervisor, and follow its startup sequence, even though 
Microsoft may not release the symbols for the main hypervisor module:
In case you want to debug the hypervisor through your network card, you should type the 
following commands, replacing the terms  with the IP address of the host system; 
” with a valid port in the host (from 49152); and  with the 
bus parameters of the network card of the debugged system, specified in the XX.YY.ZZ format 
(where XX is the bus number, YY is the device number, and ZZ is the function number). You 
can discover the bus parameters of your network card through the Device Manager applet or 
through the KDNET.exe tool available in the Windows SDK:
bcdedit /hypervisorsettings net hostip: port:
bcdedit /set {hypervisorsettings} hypervisordebugpages 1000
bcdedit /set {hypervisorsettings} hypervisorbusparams 
bcdedit /set hypervisordebug on
The following figure shows a sample system in which the network interface used for debug-
ging the hypervisor is located in the 0.25.0 bus parameters, and the debugger is targeting a host 
system configured with the IP address 192.168.0.56 on the port 58010.
Take note of the returned debugging key. After you reboot the debugged system, you should 
run Windbg in the host, with the following command:
windbg.exe -d -k net:port=,key=
You should be able to debug the hypervisor, and follow its startup sequence, even though 
Microsoft may not release the symbols for the main hypervisor module:
CHAPTER 9 Virtualization technologies