generational hypothesis which states that “most objects die
young” [18, 37], meaning that newly allocated objects on the
heap become unreachable fast.
The heap can then be partitioned in several partitions, known
as generations. All allocations are initially stored in a small
“young” generation, and a garbage collection invocation need
only traverse this small partition. When an object survives a
collection, it is moved to an “older” generation.
A minor collection is a collection that only traverses the
object graph in the young generation, and a major collection
is a collection that traverses both generations.
III. ATTACKING JVM AND V8
This section presents two general ampliﬁable timing attack
strategies that exploit the garbage collector in order to leak one
bit of information. Both attacks work for two garbage collection
strategies used by Java, as well as for the generational mark-
sweep/mark-compact strategy used in V8.
The section also presents a technique for amplifying the
one-bit leak to a general N-bit leak that works for all of the
garbage collection strategies mentioned above.
A. High dependency in low context
This attack exploits the fact that during evacuation from
from-space to to-space, the amount of bytes copied depends on
the reachable nodes at the current point in the program. Thus,
by creating a sufﬁciently large difference in reachable and
unreachable nodes, the time required to perform a minor/major
garbage collection becomes observable.
Figure 2 demonstrates the attack in Java. The example leaks
whether h > 0 by observing the time difference caused by the
allocation on line 15. If the value of diff is large then h > 0,
and otherwise h ≤ 0.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
int [] a = new int [ size1 ];
int [] b = null ;
int [] c = null ;
int [] d = null ;
if (h > 0) {
b = new int [ size1 ];
d = a;
}
else {
c = new int [ size1 ];
b = a;
}
c = null ;
long before = System . nanoTime ();
int [] x = new int [ size2 ];
long after = System . nanoTime ();
long diff = after - before ;
Fig. 2: Program leaking one bit of information based on
evacuation time.
The attack works as follows. Suppose that constants size1
and size2 are chosen so that the following constraints hold,
where collectionThreshold is an experimentally obtained
constant that triggers garbage collection, and S is the number
of bytes required to represent an integer in Java.
• 2 · S · size1 ≤ collectionThreshold
• S · (2 · size1 + size2) ≥ collectionThreshold
Line 1 allocates a new array, and keeps a reference to that
array in the variable a. If h > 0 we allocate a new array on
line 6 and store a reference to this array in the variable b. On
the other hand, if h ≤ 0 we allocate a new array and store
the reference in the variable c, which will become unreachable
as soon as we reach line 13. Note that on line 11 we keep
a reference to the array allocated on line 1, meaning that a
and b point to the same array. Now assume the allocation on
line 15 invokes the garbage collector. If h > 0 there will be
two distinct arrays that need to be copied from from-space
to to-space, meaning that 2 · size1 integers will be copied.
However, if h ≤ 0 then the only array which is reachable, and
thus should be copied, is the array allocated on line 1. Thus we
only copy size1. This difference in the number of bytes that
should be copied creates an observable difference in timing.
B. Low modiﬁcation in high context
The previous program demonstrated how the evacuation of
sensitive information in public contexts can lead to a covert
channel. The next attack shows how garbage collection of
public information in a sensitive context also leads to a covert
channel. Consider the program in Figure 3 and suppose that
constants size1 and size2 are chosen so that the following
constraints hold.
• S · size2 ≤ collectionThreshold
• S · (size1 + size2) ≥ collectionThreshold
If h > 0 the allocation on line 2 partially ﬁlls up the heap,
so that the allocation on line 7 triggers a garbage collection,
and thus the value of diﬀ is large. However, if h > 0 is false
695
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:35 UTC from IEEE Xplore.  Restrictions apply. 
1
2
3
4
5
6
7
8
9
if (h > 0) {
int [] b = new int [ size1 ];
b = null ;
}
long before = System . nanoTime ();
int [] c = new int [ size2 ];
long after = System . nanoTime ();
long diff = after - before ;
Fig. 3: Program leaking one bit of information based on the
presence of a garbage collection.
then no garbage collection occurs on line 7, as the size of the
memory does not exceed the implementation’s threshold for
garbage collection. This results in a small value for diﬀ.
C. Amplifying the attacks
We now amplify the leakage of the attacks described in
Section III-A and III-B. This leads to an attack that leaks the
value of a 32 bit integer. To illustrate the technique we describe
how the attack from Section III-A can be extended.
First, note that the attacks cannot be extended naively by
repeating the algorithm for each bit. To see why, assume this
approach is taken and that we enter the ﬁrst iteration with 0%
of the available memory having been allocated. We would like
to keep this 0% allocated memory as a loop invariant, so that
we can repeat the attack for each bit.
Next, we might allocate memory equal to 75% of the
available memory, which we turn into garbage by removing
any referencing to the allocated memory. We then perform a
“leaky allocation” of some amount of memory which will cause
a garbage collection to occur (ie. we request strictly more
than 25% of the available memory, such that we in total have
requested strictly more than 100% of the available memory,
forcing a GC to occur). This collects the 75% of available
memory, but it also gives us a non-zero block of memory in
return, meaning that we enter the next iteration of the loop
with a non-zero percent of the available memory having been
allocated. Thus the invariant has been broken.
resulting attack is shown in Figure 4.
Thus we must modify the attack in several ways. The
First, we repeat the attack N (where N ∈ [10, 20] is
sufﬁcient) times and measure each trial run. If the allocation
duration is larger than some threshold, we store the time
required to perform the allocation in the array times. By
ﬁltering out allocations with short allocation times we ﬁlter
out iterations that does not lead to an invocation of the garbage
collector.
Second, instead of allocating one array when ﬁlling up the
memory, we allocate K arrays, where K is usually less than
10. This increases the evacuation time since more memory will
need to be copied. Larger values of K will lead to a greater
timing difference between a zero bit and a one bit, at the cost
of a greater allocation time. Thus there is a time/precision trade
off.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
long [] times = new long [N ];
int guess = 0;
for ( int bit = 31; bit >= 0; -- bit ) {
for ( int i = 0; i > bit ) & 1) > 0) {
b = new int [K ][ size ];
d = a;
}
else {
c = new int [K ][ size ];
b = a;
}
c = null ;
long before = System . nanoTime ();
int [] c = new int [ size2 ];
long after = System . nanoTime ();
if ( after - before > threshold ) {
times [i] = after - before ;
}
else {
times [i] = 0;
}
}
long sum = 0;
int numOfGCs = 0;
for ( int i = 0; i  DELTA ) {
guess += Math . pow (2 , bit );
}
}
Fig. 4: Program leaking 32 bits of information based on
evacuation time.
Third, we compute the average allocation time for each
of the trial runs which invoked the garbage collector. If the
average garbage collection time is above some chosen DELTA
we conclude that the current bit is one, and zero otherwise.
Finally,
in case no garbage collection occurs we retry
the current iteration. Note that the probability of invoking
a collection in subsequent tries is increased because more
memory has been allocated from the previous tries of the same
iteration, meaning that several tries of the same iteration is
rare.
696
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:35 UTC from IEEE Xplore.  Restrictions apply. 
D. Results
The blue plot of Figure 5a shows the output of running the
program described in Figure 4 on the secret input 5342121
with the serial garbage collection strategy used by Java when
invoked with the parameter -XX:+UseSerialGC. Similarly the
red part of the ﬁgure shows the output of the same program,
with modiﬁed constants, on the same secret input with the
parallel collection strategy used when invoking Java with the
parameter -XX:+UseParallelGC.2
Figure 5b shows the output obtained by running a similar
attack on the V8 JavaScript engine using Node.js. This attack
follows the same pattern as the attack in Figure 4, and has
therefore been omitted.
The experiment results are gathered from a machine with
the following specs: Intel(R) Core(TM) i7-5557U CPU @
3.10GHz, Memory: 8 GB. NodeJS version 6.2.0. Java version
“1.8.0_77”, Java(TM) SE Runtime Environment (build 1.8.0_77-
b03), Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03,
mixed mode).
All ﬁgures show a clear distinction in garbage collection
time consumption in the aftermath of processing a one bit, and
processing a zero bit.
a) Observations over network: The timing observations
in these attacks are of sufﬁcient magnitude to be observed over
an internal network, e.g., in a datacenter-like setting, with ping
latency of 0.5ms. Figure 5c shows the timing observed by a
client communicating with a server over 25 trials, where each
trial consists of the following operations:
1) ﬁrst, the server sends a ping to the client
2) then, the server performs an allocation similar to the
allocation on line 15 in Figure 2
3) ﬁnally, the server sends another ping to the client.
Figure 5c thus shows the difference in the delay between the
two pings sent by the server during step (1) and (3). The red
bars show the delay when roughly half as much memory should
be garbage collected.
b) Rate: By measuring the execution time of the program
in Figure 4 over 25 iterations we calculate the rate of the
channel obtained. This yields a channel rate of 0.98 bytes per
second.
In the sections to follow, we construct a formalism for
studying these attacks. We ﬁrst introduce a standard imperative
language using a small-step semantics, with a few technical
deviations to facilitate an isolated study of garbage collection.
We extend the semantics to incorporate garbage collection
transitions, and prove that our garbage collection semantics
satisﬁes functional correctness. Finally we add a standard
type system for information ﬂow which, when combined with
the semantics of Section IV-B, implies resilience against the
presented attacks.
IV. LANGUAGE
This section presents a design of a small imperative pro-
gramming language with automatic memory management. The
2The appendix contains a link to supplementary material with a VM
containing all of our examples.
697
key element of the design is that careful combination of the
guarantees obtained via typing and the runtime constraints on
the memory management eliminate timing leaks via garbage
collection.
a) Syntax: Figure 6 describes the syntax of our language.
It is a standard imperative language [41, 33, 45] extended
with heap allocated arrays and the corresponding getters and
setters, a command for obtaining the current time, and the two
security-related constructs, as explained below. For expressions,
n ranges over the set of integers Z and x, y, z range over
variables. Finally, op ranges over binary integer operations.
A special expression null corresponds to the only memory
location representable at the source level.
b) Non-standard features: The runtime of the language
has an explicit notion of time that may be observed program-
matically using command x := time(). This particular design
choice has the advantage that it provides a powerful attacker
model without complicating the formal setup, e.g., introducing
intermediate outputs. This includes a network attacker who
observes timing of the network communication, as well an
attacker providing untrusted code with access to a clock, which
may in general be needed for functionality.
Our formal semantics (cf. Section IV-A) models the time
using simple instruction counting – operational steps in the