0
0
1
0
2
0
If opportunity is proportional to the product of hosts times vulns,...
Then either “we” are doing a good job at keeping the crime rate from growing as fast as the 
opportunity is growing, there is some degree of the attack community holding vulnerabilities in 
reserve, or there is a growing reservoir of untapped opportunity for attack.
Complexity
“There are two ways of constructing a 
software design. One way is to make it so 
simple that there are obviously no 
deﬁciencies and the other is to make it so 
complicated that there are no obvious 
deﬁciencies.”
                            C.A.R. Hoare
This sums up the question of complexity.  The parallels to current market leading suppliers, 
competing as they are on feature richness, is obvious and daunting.
Code volume
(94% share)
MLOC
40
30
20
10
0
0
9
1
9
2
9
3
9
4
9
5
9
6
9
7
9
8
9
9
9
0
0
1
0
2
0
Windows 94% market share per IDC
Code volume as observed:
Win 3.1   Win NT   Win 95   NT 4.0   Win 98   NT 5.0   Win 2K   Win XP
      3        4       15       17       18       20       35       40
   1990     1995     1997     1998     1999     2000     2001     2002
Fighting ﬁre with ﬁre?
MLOC
100
75
50
25
0
0
9
1
9
2
9
3
9
4
9
5
9
6
9
7
9
8
9
9
9
0
0
1
0
2
0
3
0
4
0
Code volume as observed:
Win 3.1   Win NT   Win 95   NT 4.0   Win 98   NT 5.0   Win 2K   Win XP   Longhorn?
      3        4       15       17       18       20       35       40      100?       
   1990     1995     1997     1998     1999     2000     2001     2002     2004?
How big will Longhorn be?
For an historical comparison, look at the testimony of David Parnas on the implications of President 
Reagan’s “Star Wars” anti-ballistic missile system proposal.  He resigned from the study commission 
on the grounds that 100 MLOC was an utterly preposterous thing to imagine working well enough to 
rely on it for critical things like national defense.  Herb Line, then of the Center for Strategic 
Studies, thought 10 MLOC was preposterous.  And here we are.
Normalized
(median, 2yr lag)
MLOC
Vulns
Incidents
0
9
1
9
2
9
3
9
4
9
5
9
6
9
7
9
8
9
9
9
0
0
1
0
2
0
35.0
26.3
17.5
8.8
0
Each curve is normalized against its own median over this period.  Therefore, overlaying the curves 
is legitimate.
Code volume curve is shifted right two years to crudely simulate diffusion delay.
Drivers?
MLOCs3
Vulns
MLOCs3^2+2
Incidents
40
30
20
10
0
0
9
1
9
2
9
3
9
4
9
5
9
6
9
7
9
8
9
9
9
0
0
1
0
2
0
Each curve is normalized against its own median over this period.
Code volume curve, MLOCs3, is the three year moving average of code volume, perhaps a better 
estimator of effective code volume in the population at large.
The second code volume curve, MLOCs3^2+2, is the square of the three year moving average of code 
volume, and then shifted right two years.  The argument is this: Security faults are a subset of 
quality faults and the literature says that quality faults will tend to be a function of code 
complexity, itself proportional to the square of code volume.  As such, the average complexity in 
the field should be a predictor of the attack-ability in an a priori sense.  Shifting it right two 
years is to permit the attack community time to acquire access and skill to that growing code base 
complexity.  This is not a statement of proven causality -- it is exploratory data analysis.
Epidemics
• Characteristics of infectious processes
• Pr(infection|exposure)
• interval from infection to infectiousness
• duration of infectiousness
• interval from infection to symptoms
• duration of acquired immunity
The math for modeling epidemics is well developed, as is the math for accelerated failure time 
testing, actuarial science, portfolio management, and others.  There is no need, and no time, to 
invent new science before progress can be made.  Steal these skills, and do so while the senior 
practitioners in security still include people with  these sort of skills learned elsewhere.
Tipping Point example
   Pr(I|E)=2%, n(E)=50±10%
50
45
55
7,000
5,250
3,500
1,750
0
123456789
0
1
1
1
2
1
3
1
4
1
5
1
6
1
7
1
8
1
9
1
0
2
This is simply the example used in Gladwell’s The Tipping Point.  It illustrates the chaotic nature 
of epidemics which is to say that small changes in initial conditions produce large changes in 
downstream values.  This example is where the initial number of cases is 1,000, the probability of 
infection given exposure is 2%, the number of exposure events while infectious is 50 plus or minus 5 
(10%), and the downstream shows that in only 20 days at -10% the disease will die out while in only 
20 days at +10% the epidemic will be well underway.
Worst case disease
• Pr(infection|exposure) = 1.0
• interval from infection to infectiousness = 0
• interval of infectiousness = open ended
• interval from infection to symptoms = indef
• duration of acquired immunity = 0 (mutates)
If you were designing a pessimal disease, it would be perfectly transmissable (100% chance of 
getting the disease once exposed and no acquired immunity), no symptomatic sign of infection, and an 
instantaneous conversion from pre-infection to infectious (or from prey to predator, if you prefer).
The above describes worm propagation, or DDOS zombies, or the stockpiling of unannounced 
vulnerabilities.
Does the law have an answer for designer disease with pessimal characteristics and self-obscured 
authors?  Is “terrorism” an appropriate model or is it more like mandatory seat belt laws?
Immunization
unpatched
1.00
3mo to patch, 1wk to exploit => susceptibility = 94%
1mo to patch, 3mo to exploit => susceptibility = 12%
0.75
0.50
0.25
0.00
                     make mandatory? 
                                .........how?
0
1
2
3
4
5
6
months
7
8
9
10
11
12
Qualys, Inc., has data that implies patching is like radioactive decay in that 50% of the remaining 
unpatched systems will be patched in each succeeding “half-life.” Qualys’s figure is 30 days.
Posting a patch starts a race wherein the patch is reverse-engineered to produce exploits.  The two 
data points are intended to bracket current reality.  In the one case, if patching does have a one-
month half-life while the reverse engineering interval is 90 days, then the susceptibility would be 
12% at the moment of exploit.  By contrast, if patching has a three-month half-life while the 
reverse engineering interval is one week, then the susceptibility would be 94% at the moment of 
exploit.
Time-to-exploit is shrinking while the time-to-patch is lengthening (if you factor in the growth of 
always-on, always-connected home machines) so the question becomes whether “mandatory” is a word we 
must use and, if so, what would it mean.  What does the law say?
Durability tradeoffs
• Durable against random faults
• Scale-free networks (growth driven)
• Durable against targeted faults
• Structured routing (policy driven)
                      ...cannot be both
Research, practice, and history each point to the same conclusion: Those network structures that are 
optimized for resistance to random faults are not the networks that are optimized for resistance to 
targeted faults.  This is not a happy tradeoff.
Networks tend to grow by accretion and new nodes will prefer to be connect to nodes that are well 
connected, a phenomenon that produces so-called “scale-free” networks.  These networks are 
remarkably resistant to random faults, and the Internet is to a large degree characterizable as a 
scale-free network.  However, because of the path of any given packet will tend to pass from lightly 
interconnected nodes through highly connected nodes, this resilience to random faults also makes the 
network vulnerable to targeted faults.  A body of scientific literature is growing up around this, 
beginning at .
Side issues abound
• Tight integration of apps & OS
• User level lock-in
• Decreasing skill/power ratios everywhere
• Insecure complexity v. complex insecurity
• Strength through diversity
• Opened source v. open source
This list is indicative, not exhaustive.  It includes the monopolization questions of tying the 
applications to the operating system thus to insure that a security failure of the one is a security 
failure of the other, whether user level lock-in plays a role in assessing the locus of liability 
for security faults, and whether the skill to operate ever more powerfully interconnected computers 
does not at some point require some a priori proof of capability.  It asks to distinguish complexity 
that is insecure from insecurity that is itself complex.  It ponders the question of genetic 
diversity as a survival advantage in a world where predators have just arisen.  It distinguishes the 
value of public disclosure in the open source tradition to the private disclosure of the entirety of 
the Windows (94% share) source code pool to potentially hostile nation states.  It could go on.  The 
challenge is substantial and historically crucial.  What will the law say, and can it say it without 
adding noise?
Exploration
• Latency (to patch, to detect, MTBF, MTTR)
• Interarrival rates (attacks, patches, unknown hosts)
• Intrusion tolerance (diversity v. redundancy)
• Comparands (benchmarks, shared pools, anova)
• Cost effectiveness (risk reduction v. symptom relief)
• Scope (data capture v. data reduction, sampling)
To go on from here we can’t use words, they don’t say enough.  We must use numbers.  These are 
indicative and intended to push you to think of more.  Even if the shorthand does not read clearly, 
the point is this: now that the digital world is essential, statistics based on the realities of 
digital physics will be, at least, how score is kept.  Perhaps we will be fortunate and statistics 
based on the realities of digital physics will also inform decision making at the highest levels, 
including the law.
Summary
• Unknown vulns = secret weapons
• Absence of events does not predict calm
• Mobile-code mandates trade downside 
• Risk is proportional to reliance when the 
• Price of freedom is the probability of crime
relied-upon cannot be measured
risks against each other
In summary, the pool of selectively known vulnerabilities is the secret weapon of the serious enemy, 
the absence of a significant catastrophe to date is most assuredly not evidence that the risk is low 
because in a risk aggregated world significant events make up in their severity what they lack in 
frequency, that mandates for automatic patching are effectively mandates for more powerful mobile 
code and are thus risk creating in the larger sense of risk aggregating, that risk is itself 
proportional to the reliance on places in the entity being relied upon exactly when there are no 
effective measures, and that the tradeoff between freedom (default permit) and safety (default deny) 
is real and present.
A modest proposal
This is the last time we will have as much 
hybrid vigor amongst leadership as we do 
now and the last time we will have as clean a 
slate as we now have; we must use them both 
for all they are worth.
There is never enough time.....
.....Thank you for yours
It has been entirely my pleasure.
Dan Geer
PI:EMAIL
+1.617.492.6814
challenging work sought & preferred
Further contact is welcome particularly if it brings problems of the sort that illustrate the bounds 
of our knowledge.