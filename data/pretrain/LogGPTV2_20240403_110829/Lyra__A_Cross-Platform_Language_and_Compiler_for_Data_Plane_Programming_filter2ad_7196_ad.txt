statements, e.g., Lines 8-12 in Figure 8(a), which complicate depen-
dency analysis [37]. We, therefore, convert each if-else condition
into a predicate, and then apply this predicate to all the instructions
in the condition body. For example, the if condition int_enable,
in Line 3 in Figure 8(b), is converted into a predicate, int_enable
? ..., and is applied to all instructions in this condition body, thus
getting Lines 3-6 in Figure 8(c). Once all the branches are removed,
the body of the algorithm becomes a straight-line code block.
Step 3: Single operator tuning. We expand the instructions that
have more than one operator. For example, Line 6 in Figure 8(b) is
flattened into Lines 3 and 4 in Figure 8(c).
Step 4: Static single assignment (SSA) form conversion. SSA
assigns each variable a version field. When the variable is assigned
to a new value, the version increases accordingly. SSA guarantees
no versioned variable is assigned twice and removes the Write-
After-Read and Write-After-Write dependencies. After this step,
only Read-After-Write dependency remains. The int_info1 and
int_info2 (i.e., Lines 4 and 6 in Figure 8(c)), for example, are as-
signed to different versions.
Step 5: Variable type inference. The width of program variables
is inferred based on 3 rules: (1) function call, such as crc32_hash
returns a 32-bit variable; (2) operation, ğ‘ğ‘›ğ‘‘ operation generates a
1-bit variable; and (3) variable lookup, the input/output type of
the table are defined explicitly. For example, in Figure 8(c), the v1
is inferred as a 32-bit variable as the ig_ts and eg_ts are 32 bits.
4.3 Code Analyzer
So far, the preprocessor has translated a Lyra program P to an IR.
Nevertheless, this IR is a plain-text IR, which lacks context informa-
tion (e.g., instruction dependency, and deployment constraints) for
chip code synthesizing (Â§5). We, therefore, build a code analyzer to
add â€œcontextâ€ to the IR.
Instruction dependency generation. We first analyze the depen-
dencies among IR instructions to generate an instruction depen-
dency graph. This is important, because it would determine the
execution order and placement of these instructions in the chip-
specific code synthesizing. For example, if instruction ğ‘ relies on
another instruction ğ‘, ğ‘ should be placed in the stage behind ğ‘â€™s
1:    func int_info (bit[32] info) {2:        info = 0;  3:        info = (ig_ts - eg_ts) & 0x0fffffff;4:        info = info & (sw_id ğœ‰ğ‘¡,end
ğœ‰ğ‘¡,startâ‰¤ ğ‘—â‰¤ğœ‰ğ‘¡,end
We also encode the RAM memory constraints based on [26]. Sup-
pose each stage in the RMT switch has ğ‘memory RAM blocks with
â„ entries and ğ‘¤ bit-width. For each stage ğ‘—:
âŒˆ âŒˆ ğ¸ğ‘¡,ğ‘—
â„ âŒ‰ Â· ğ‘€ğ‘¡
ğ‘¤
âŒ‰ âˆ— Valid(t) â‰¤ ğ‘memory
(2)

ğ‘¡ âˆˆL
where Valid(t) represents the validity of table ğ‘¡, and its value is either
1 or 0. In similar ways, we can also encode other constraints such as
the maximum number of stages, the maximum number of tables per
stage, the maximum number of entries in the parser TCAM table,
PHV allocation, predefined library-function call related resources,
packet transactions [37]. Please see Appendix A for more details
of chip constraint encoding. All the encoded constraints are put in
the set of conditional placement constraints.
5.5 Encoding Deployment Constraints
Besides the constraints related to the conditional implementation
and different switching ASICs, we also need to encode constraints
like scope, flow path, and instruction dependencies. This section
describes how to encode these constraints, which is important for
the composition. Note that the constraints in this section cannot
be encoded by integer linear programming (ILP), since ILP cannot
encode â€œif-elseâ€ and dependency.
SIGCOMM â€™20, August 10â€“14, 2020, Virtual Event, NY, USA
Algorithm 2: Extensible resource encoding.
Input: ğ¼ğ‘: Context-aware IR for algorithm ğ‘, target switch ğ‘ .
Output: L: The variables in the resource and the existence condition.
1 S â† DownStreamSwitches(ğ‘ );
2 foreach local variable ğ‘£ âˆˆ Iğ‘ do
ğ¼ğ‘¤ â† WriteInstruction(ğ‘£)
Iğ‘Ÿ â† ReadInstructions(ğ‘£)
foreach switch ğ‘ ğ‘‘ âˆˆ S do
Vğ‘¤ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘“ğ‘ ğ‘‘ (ğ¼ğ‘¤))
foreach read instruction ğ¼ğ‘Ÿ âˆˆ Iğ‘Ÿ do
Fğ‘Ÿ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘“ğ‘ ğ‘‘ (ğ¼ğ‘Ÿ))
Vğ‘Ÿ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘( Fğ‘Ÿ)
L[ğ‘£] = ( Vğ‘¤) âŠ• ( Vğ‘Ÿ)
3
4
5
6
7
8
9
10
11 return L
Encoding topology constraints. As shown in Â§4.3, topology con-
straints in context-aware IR contain two parts: algorithm scope and
flow paths in the specified scope.
Scope constraints. For each IR instruction in ğ¼ğ‘, it can only be
deployed in the specified scope:ğ¼ âˆˆğ¼ğ‘,ğ‘ âˆ‰Sğ‘
ğ‘ , on each path.ğ¼ âˆˆğ¼ğ‘,ğ‘ âˆˆğ‘ If (ğ‘“ğ‘ (ğ¼), 1, 0) = 1
Flow path constraints. For each possible flow path ğ‘ within the
scope, an instruction ğ¼ must be deployed on only one of switches,
ğ‘“ğ‘ (ğ¼) = False.
Encoding instruction dependencies. We now encode the in-
struction dependencies in the context-aware IR. If an instruction ğ¼â€²
is deployed on one switch ğ‘  on the path ğ‘, then (1) for each instruc-
tion ğ¼ the instruction ğ¼â€² depends on, ğ¼ cannot be deployed on the
switches behind ğ‘ ; (2) for another instruction ğ¼â€²â€² depended by ğ¼, ğ¼
cannot be deployed on switches in front of ğ‘ . Thus, we have:
ğ¼â€² depends on ğ¼ â‡’
ğ¼ depends on ğ¼â€²â€² â‡’
ğ‘“ğ‘ â€² (ğ¼â€²) = ğ¹ğ‘ğ‘™ğ‘ ğ‘’
ğ‘“ğ‘ â€²â€² (ğ¼â€²â€²) = ğ¹ğ‘ğ‘™ğ‘ ğ‘’
ğ‘ â€²âˆˆprev(s, p),ğ¼â€²âˆˆsucc(I)
ğ‘ â€²â€²âˆˆnext(s, p),ğ¼â€²â€²âˆˆpred(I)