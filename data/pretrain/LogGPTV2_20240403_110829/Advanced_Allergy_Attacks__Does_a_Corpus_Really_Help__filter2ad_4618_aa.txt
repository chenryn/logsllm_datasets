title:Advanced Allergy Attacks: Does a Corpus Really Help?
author:Simon P. Chung and
Aloysius K. Mok
Advanced Allergy Attacks:
Does a Corpus Really Help?
Simon P. Chung and Aloysius K. Mok(cid:2)
Department of Computer Sciences,
University of Texas at Austin, Austin TX 78712, USA
{phchung,mok}@cs.utexas.edu
Abstract. As research in automatic signature generators (ASGs) re-
ceives more attention, various attacks against these systems are being
identiﬁed. One of these attacks is the “allergy attack” which induces the
target ASG into generating harmful signatures to ﬁlter out normal traﬃc
at the perimeter defense, resulting in a DoS against the protected net-
work. It is tempting to attribute the success of allergy attacks to a failure
in not checking the generated signatures against a corpus of known “nor-
mal” traﬃc, as suggested by some researchers. In this paper, we argue
that the problem is more fundamental in nature; the alleged “solution”
is not eﬀective against allergy attacks as long as the normal traﬃc ex-
hibits certain characteristics that are commonly found in reality. We have
come up with two advanced allergy attacks that cannot be stopped by
a corpus-based defense. We also propose a page-rank-based metric for
quantifying the damage caused by an allergy attack. Both the analysis
based on the proposed metric and our experiments with Polygraph and
Hamsa show that the advanced attacks presented will block out 10% to
100% of HTTP requests to the three websites studied: CNN.com, Ama-
zon.com and Google.com.
Keywords: Automatic Signature Generation, Intrusion Prevention
Systems, Allergy Attacks.
1 Introduction
The use of automatic signature generators (ASGs) as a defense against fast
propagating, zero-day worms has received a lot of attention lately, and various
attacks against these systems are also being discovered. Allergy attack is one of
these attacks, and was deﬁned in [2] as follows:
An allergy attack is a denial of service (DoS) attack achieved through
inducing ASG systems into generating signatures that match normal
traﬃc. Thus, when the signatures generated are applied to the perimeter
defense, the target normal traﬃc will be blocked and result in the desired
DoS.
(cid:2) The research reported here is supported partially by a grant from the Oﬃce of Naval
Research under contract number N00014-03-1-0705.
C. Kruegel, R. Lippmann, and A. Clark (Eds.): RAID 2007, LNCS 4637, pp. 236–255, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
Advanced Allergy Attacks: Does a Corpus Really Help?
237
It might appear that there are simple counter-measures to allergy attacks; the
simplest “solution” is to perform a manual inspection of the generated signatures
before they are deployed. This is, however, a non-solution inasmuch as it defeats
the very purpose of having an ASG: to automate the defense against fast attacks.
Other ASGs employ some form of corpus-based mechanisms to retroﬁt for a low
false positive rate. In these ASGs, a new signature will only be deployed if it
matches a suﬃciently small portion of past normal traﬃc stored in a corpus
that is commonly called the “innocuous pool”; for brevity we shall use the term
corpus when there is no confusion.
In this paper, we shall show that corpus-based mechanisms are not a gen-
eral solution against allergy attacks. In particular, we will identify two major
weaknesses of a corpus-based defense, and present advanced allergy attacks that
exploit them. The ﬁrst type of attacks exploits the inability of a static cor-
pus to capture how normal traﬃc evolves over time. As a result, the type II
allergy attacks, which induces the ASG into generating signatures that match
traﬃc pattern speciﬁc to future traﬃc, cannot be stopped by a corpus-based
mechanism. The second type of attacks, the type III allergy attack employs
a divide-and-conquer strategy; it induces the ASG into generating a set of
allergic signatures, each only blocking a small portion of normal traﬃc, but
together can create a signiﬁcant amount of damage. As we will argue, this
appears to be an inevitable consequence of the natural diversity in normal
traﬃc.
The rest of this paper is organized as follows: in Sect. 2, we will survey related
work and in Sect. 3, we will present a metric for quantifying the damages caused
by an allergy attack that blocks out only part of a target website. In Sect. 4
and 5, we will demonstrate the feasibility and eﬀectiveness of the type II and
type III allergy attack, and study some popular websites, including CNN.com,
Amazon.com and Google.com. Our discussion in Sect. 4 and 5 assumes that
the attacker can induce the ASG into generating any allergic signature with
a suﬃciently low false positive rate when evaluated against the ASG’s corpus,
and focus on showing that these signatures can still cause a signiﬁcant level of
damage. In Sect. 6, we will validate our assumption by presenting our experience
in inducing Polygraph and Hamsa into generating the signatures studied in Sect.
4 and 5. Finally, we will conclude in Sect. 7.
We emphasize that even though our discussions focus on attack against HTTP
requests, the type II and type III attacks are not limited to HTTP traﬃc. The
underlying weaknesses of a corpus-based defense exploited by these attacks,
namely the static nature of the corpus, and the diversity in normal traﬃc ex-
ists for all kinds of real traﬃc. We focus on HTTP only because it is probably
the most tempting target for allergy attacks and is the major focus of many
existing ASGs. A compromised ASG that ﬁlters out normal HTTP requests
means inconvenience to Internet users and worse, direct business loss to site
owners.
238
S.P. Chung and A.K. Mok
2 Related Work
2.1 Automatic Signature Generators
In most published ASGs (like [6,17,5]), suspicious traﬃc is identiﬁed by some
network-based monitoring mechanisms. The signature generation process will
then extract properties that are prevalent among suspicious traﬃc, and construct
signatures to ﬁlter packets with such properties. Usually, the signature generated
is simply a byte sequence, and any packet containing that byte sequence will be
dropped by the perimeter defense.
Recent advances in ASGs introduced the use of host-based mechanisms (e.g.,
STEM in [9] and taint analysis in [14,3]) to identify attack traﬃc and to capture
information about how the target host processes them. The use of information
from host-based systems in signature generation leads to the development of new
signature formats. In [3,1], signatures are no longer byte sequences to be matched
against incoming traﬃc, but are basically “programs” that takes a packet as in-
put, and determines whether it will lead to the same control/data ﬂow needed
in exploiting a known vulnerability. Other new signature formats have also been
proposed. For example, the approaches in [12,8] generate signatures to match
packets that contain sets/sequences of “tokens” (byte sequences), while [7] out-
puts signatures that identify bytes corresponding to certain control structures
commonly found in suspicious traﬃc.
2.2 Attacks Against ASGs
Worm Polymorphism. From the early research in ASGs, worm polymorphism
is a well recognized problem. This is particularly true for systems that generate
signatures to identify “invariant” bytes in the attack traﬃc. As argued in [12],
exploits against certain vulnerabilities simply do not have any single contiguous
byte sequence that can be used to identify all instances of the attack while keep-
ing the false positive low. In other words, it is impossible for some traditional
ASGs to generate one eﬀective signature for all exploitations of certain vulner-
abilities. As a solution to this problem, [12] proposed the use of signatures that
identify multiple byte sequences in the observed traﬃc, instead of only a single
byte sequence. However, as shown in [16,13], even this approach can be evaded
by specially crafted polymorphic worms.
Allergy Attack. In contrast to worm polymorphism, allergy attack against
ASGs is a much less recognized problem. Although many published ASGs are
vulnerable to the allergy attack, this threat is mentioned only very brieﬂy in three
published work, as cited in the survey in [2]. Unlike worm polymorphism that
can lead to high false negatives, allergy attacks aim to introduce false positives.
While false negatives denote failure of the defense to protect the targeted host
but incur no additional damage, false positives can actually incur unanticipated
penalty to the targeted host due to the deployment of the defense mechanism
itself. Hence, allergy attacks are at least as important a problem facing ASGs as
polymorphism.
Advanced Allergy Attacks: Does a Corpus Really Help?
239
As noted in [2], the root cause of the problem with allergy attack is the use of
semantic-free signature generation which extracts bytes from suspicious traﬃc
without considering how those bytes correspond to the observed malicious/worm
behavior. In other words, all parts of the worm are considered the same by the
signature generation process, and it is possible to extract as signatures bytes that
are totally irrelevant to any attack. Most traditional approach that extract byte
sequences (or features of packets) prevalent in suspicious traﬃc but uncommon
in normal traﬃc can be considered semantic-free. Purely network-based mecha-
nisms for identifying suspicious traﬃc also facilitate allergy attacks; they allow
attackers to easily pretend to be “suspicious”, and have their traﬃc used in sig-
nature generation. These mechanisms also give the attackers complete freedom
in what they send in for signature generation.
We should note that newer ASGs that are not semantic-free, such as [3,1]
are less vulnerable to allergy attacks. However, these ASGs are necessarily host-
based and come at a cost. The signature generation process is usually more
complicated and thus takes longer time than in traditional ASGs. The use of
host-based detection also leads to higher management cost and lower portability.
Many host-based mechanisms used in these new ASGs are quite heavy-weighted,
and may not be suitable for all legacy systems. Also, ASGs that employ host-
based detection require a separate detector for each type of host. For example,
if both Windows and Linux machines are to be protected, then at least two
host-based detectors are needed by the ASG.
2.3 Handling False Positives in Traditional ASGs
Even though the threats from false positives artiﬁcially introduced by allergy
attacks have been largely ignored, traditional ASGs employ various mechanisms
to reduce “naturally occurring false positives”. For example, both [17,5] use a
blacklisting mechanism to avoid generating signatures for normal traﬃc that
the ASGs are known to misclassify. In [12,8], a normal traﬃc corpus is used to
evaluate the expected false positive rates of candidate signatures, and those that
match a signiﬁcant portion of the normal traﬃc will be discarded. However, these
mechanisms against “naturally occurring” false positives are ineﬀective against
maliciously crafted traﬃc from an allergy attack. As shown in [2], the blacklisting
mechanism in [5] cannot stop an allergy attack even if the target traﬃc is partly
blacklisted. The use of a normal traﬃc corpus is also not an eﬀective defense
against allergy attacks, as we shall demonstrate in Sect. 4 and 5.
A related problem with a corpus-based mechanism is that the attackers may
contaminate the corpus with traﬃc similar to an imminent attack, so that sig-
natures generated for that attack will be dropped when evaluated against the
corpus. This technique is mentioned in [12,8,13], and is called “innocuous/normal
pool poisoning”. In order to solve this problem, the authors of [12,13] proposed
to “collect the innocuous pool using a sliding window, always using a pool that is
relatively old (perhaps one month)”, while [8] suggested to “collect the samples
for the normal pool at random over a larger period of time”. However, as we’ll
see, both solutions may signiﬁcantly increase the power of type II attacks.
240
S.P. Chung and A.K. Mok
3 Quantifying the Power of Allergy Attacks
Before we present the advanced allergy attacks, we will introduce our metric for
quantifying the damages they produce. Our metric is speciﬁc for attacks that
make particular pages under the target web site unavailable. We use a localized
version of page rank in [15] (under a localized version of their random surfer
model) to measure the importance of individual pages, and derive the amount
of damages caused by an attack from the importance of the pages blocked.
3.1 Localized Random Surfer Model
The major diﬀerence between the original random surfer model in [15] and our
localized version is that we only consider pages at the site of interest, due to
the lack in resources for the Internet-wide web crawling in [15]. In particular, we
assume visits to the site concerned always starts with a ﬁxed “root page”. The
surfer in our model randomly follows links on the currently visited page with a
probability d (we assume d to be 0.85, which is the same value used in [15] and all
subsequent studies of the Pagerank algorithm), or “get bored” with probability
1-d, just as in [15]. However, when the surfer gets bored, he/she simply leaves,
instead of jumping to any other page in the site.
3.2 Localized Page Rank
Under our localized random surfer model, the metric for measuring the impor-
tance of a page is called the “localized page rank”, which measures the expected
number of times a page will be visited in a user session, i.e. between the time
when a user ﬁrst visits the root page, to the time he/she leaves.
The computation of the localized page rank is the same as in [15], except that
we do not normalize the page rank, and we initialize the page rank of the root to
1. We do not perform normalization because we are more interested in the actual
number of times that a page will be visited, instead of its relative importance
among all other pages. The initial page rank of the root represents the visit to
the root page that occurs at the beginning of each user session.
Finally, we note that our modiﬁcations to the original random surfer model
may lead to underestimation of the importance of pages. In particular, a user
session may start at a non-root page, and the user may jump to some random
page in the studied site when he/she gets bored. However, observe that visitors
usually don’t know the URLs of many non-root pages, and most external links
point to the root page of a site. As a result, visitors don’t have much choice
but to start their visits at the root page, and cannot jump to many pages when
they get bored. In other words, inaccuracy in the computed page ranks due to
deviation from our surfer model should be minimal.
3.3 The Broken Link Probability
We are now ready to quantify the damage caused by an allergy attack to a
website. We call our metric the “broken link probability” (BLP), which is deﬁned
Advanced Allergy Attacks: Does a Corpus Really Help?
241
as the probability that a user will click on a link to any unreachable page before
the end of the user session. The BLP is intended to measure the degree of
frustration (or inconvenience) caused by an allergy attack.
To calculate the BLP, we ﬁrst recompute the localized page rank for the
website under attack. However, during this computation, pages made unavailable
by the attack have a localized page rank of zero, though they are still counted as
“children” of pages that link to them (without knowing which pages are blocked
by an attack, visitors will behave as if there’s no attack, and have equal chance
of clicking on any link, broken or not). With the new set of localized page ranks,
the BLP can be obtained by the following formula:
BLP =
(cid:2)
d
pi∈UR
(cid:2)
pj∈M(pi)
P R(pj)
L(pj) .
(1)
where UR is the set of pages made unreachable by the attack, M(pi) is the set
of pages that have links to page pi, P R(pi) is the localized page rank of the page
pi, and L(pi) is the number of pages pointed to by pi. From the above formula,
we see that the BLP is eﬀectively the sum of page ranks that the blocked pages
inherit from pages that remain available under the attack. Note that while the
localized page rank of a page is an overcount for the probability of visiting that
page if it links to other pages to form a loop, it is not a problem for the BLP
computation. This is because the user session ends on the ﬁrst attempt to visit
an unavailable page; i.e. an unreachable page can only be reached at most once
in a user session. This also means visits to various unreachable pages in a user
session are mutually exclusive. Thus, we can compute the BLP by simply adding