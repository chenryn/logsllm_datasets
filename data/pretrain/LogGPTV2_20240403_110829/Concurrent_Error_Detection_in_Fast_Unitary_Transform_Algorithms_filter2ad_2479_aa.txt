title:Concurrent Error Detection in Fast Unitary Transform Algorithms
author:G. Robert Redinbo
Concurrent Error Detection in Fast Unitary Transform Algorithms 
Department of Electrical and Computer Engineering 
G. Robert Redinbo 
University of California 
Davis, C A  95616 USA 
Phone (530) 752-3087; FAX (530) 752-8428 
e-mail:  redinbo @ece.ucdavis.edu 
Abstract 
Discrete fast iinitarq transform algorithms, of  which the 
fast  Fourier  transform (FFT) and fast discrete  Cosine 
transform  (DCT) are  practical  examples,  are  highly 
susceptible to temporary calculation failures because of 
their  interconnected  computational Jlows.  Many  error 
detection  techniques  for  FFT  algorithms  have  been 
reported,  but fault  tolerance issues for other important 
transforms have  not  been addressed  as  vigorously.  A 
general design and analysis approach for all fast unitary 
transforms is presented.  It relies on fundamental linear 
algebra  methods  coupled  with  associated  dual  space 
representations  that  are  natural  descriptions  of  real 
parity  values.  Basic output  error patterns frotn  single 
computational  errors are  used  to define an equal-sized 
group  of  dual  space  basis  vectors  on  which  practical 
parity weighting firnctionsmay be evaluated. An iterative 
design approach leads to complete single error detection 
capabilities.  FFT and fast DCT examples are given. 
Keywords:  Algorithm-based  fault  tolerance,  nu- 
merical  error detection, real  number parity  values,  fast 
Fourier transform, fast discrete cosine transform. 
1. Introduction 
Unitary  transforms  have desirable  properties  that  ac- 
count for their wide use in data processing  and analysis 
systems. Important examples include the discrete Fourier 
and  Cosine transforms.  Most  unitary  Transforms  also 
possess various fast algorithms that decompose the trans- 
forms into very efficient computational  steps.  However, 
these algorithm structures are susceptible to propagating 
temporary numerical errors caused by momentary  faults 
appearing  in  the  computing resources.  A  single error 
can  contaminate many  of  the transform  coefficients,  as 
a  moment’s  reflection  on  the  interconnected  butterfly 
configurations in a classic fast Fourier Transform (FFT) 
algorithm indicates [I]. 
Fault  tolerance  in 
algorithms has  been  consid- 
ered  in  many  papers,  a partial  list  being  [2-81,  starting 
primarily  from  the Jou and  Abraham  paper  [2].  There 
have  been  numerous  methods  proposed,  and  in  most 
cases,  a very  detailed  verifying  proof  is required.  This 
proof  is closely  associated  with  the particular  structure 
of the fast algorithm being targeted.  The specific weights 
used  for error  detection are  selected  to  work  with  the 
given  algorithm,  and,  in  many  instances  it  is  unclear 
how these weights  should be altered for a different fast 
algorithm. Furthermore, previous FFT techniques do not 
appear  transferable  to  different  transforms  such  as  the 
practically  important  discrete  cosine  transform  (DCT) 
[l].  Many  lossy  data  compression  algorithms  such  as 
JPEG, rely heavily on this transform,  and so the DCT is 
of great practical importance, e.g., [ 18-21]. 
This  paper  demonstrates  how  linear  algebra  tech- 
niques can be used to design and analyze fault tolerance 
methods  for  general  fast  unitary  transforms.  Explicit 
dependencies  between  error-detecting  scaling  weights 
and error coverage  are determined.  Various designs can 
be  explored  quickly,  for  a  wide  variety  of  transforms 
and  all  their  associated  fast  algorithms,  using  modem 
computer  algebra  programs.  The  underlying  theory 
is  straightforward  and  applicable  to  many  practical 
situations. 
0-7695-1101-5/01 $10.00 0 2001 IEEE 
37 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:19 UTC from IEEE Xplore.  Restrictions apply. 
2. Fast transform matrix components 
A  unitary  transform  is defined  by  a  matrix  T  which 
maps  N  data samples, possibly  complex-valued, into N 
transform coefficients [l]. 
The coefficients  in  y  are often  visualized  as being in  a 
frequency domain. Any normalizing factors are incorpo- 
rated  in the transform matrix T and its inverse T-'.  An 
important characteristic  of a unitary matrix  connects the 
inverse T-'  to the Hermitian  transpose of  T, written  as 
T'  [9]. 
T-'  = T*  *Hermitian transpose 
(2) 
x = T'y 
Inverse t r a n s f o r m  
(3) 
The domain and range spaces are N dimensional vector 
spaces over the complex field.  Such vector spaces have 
the usual inner product between vectors  U, v. 
 = u*v  Inner Product 
u , v   N  x  1  vectors 
(4) 
The norm I Iv(I derives directly from the inner product. 
(
x
~
One of the defining properties of a unitary matrix like Tis 
the preservation of lengths [ 1,9]. (This is Parseval's  rela- 
tionship in Fourier Transform theory, and guarantees  the 
preservation of energies in the transform coefficients). 
  =  =  =  =  llyll 
I
(
(6) 
Fast Transform  algorithms may be expressed through 
matrix  factorizations,  each  individual  factor  being  a 
sparse matrix that generally includes permutation effects 
as well  as  diagonal scaling  components  [lo-141.  Kro- 
necker matrix products are involved generally in forming 
the factor from even more sparse submatrices. 
T  = AI A2 .... A,  Fast deconiposition, n stnges 
Each factor A;  N  x  N 
(7) 
The  number  of  stages  n,  is  related  to  the  size  of  the 
transform  N. For  example,  in  most  power  of  two FFT 
38 
algorithms, 11; =  2,. 
In  other cases,  n  is  the number 
of elements in  an arithmetic expansion of the integer N ,  
e.g., Good FFT [Sect. 5.9, I]. The flow of the transform 
calculations is outlined in Figure 1 .  
All  the  computations and  permutations  within  each 
stage are described by  the linear action of its component 
matrix.  The  effects  of  faults  in  the  computations  are 
modeled  as  injecting  errors  on  some  of  the  N  lines 
thought of passing between  stages. This high level error 
model will  be described  more fully shortly.  Clearly  er- 
rors active on some lines between a stage can propagate 
to multiple lines at  the stages'  outputs.  Hence,  classic 
error-detecting  codes  can  be  overwhelmed  quickly  as 
the error effects continue expanding through subsequent 
stages. 
There are some interesting and useful properties of the 
stage component matrices  in  factorization  equation (7). 
They flow from the unitary nature of the overall transform 
matrix T. 
Thus, for any matrix k=1,2 ,..., n, 
(A1 .42...A~)-l = A k + l  A L + ~  ... A ,  AEA:-, 
..A; A: 
Since inverses are two-sided,  it also follows that 
(9) 
It  is  easy  to  show  that  each  component  matrix  i l k   is 
nonsingular. 
3. Error detection 
A basic approach to fault tolerance in FFT algorithms 
has evolved that  is computationally efficient  while pro- 
viding single error coverage.  It employs error detection 
based on comparing two parity values, one computed by 
forming  a  weighted  sum  of  the  transform  coefficients 
and  the  other  from  a  comparable  weighted  sum  over 
the  input  data.  Figure 2a  outlines this  view,  basically 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:19 UTC from IEEE Xplore.  Restrictions apply. 
an  algorithm-based  fault  tolerance  (ABFT)  approach 
[2].  By comparing these related  parity values, errors are 
detected and  then  additional processing may  be used to 
locate  the  beginning  erroneous  stage,  or  the  complete 
transform  can  be  recomputed  which  then  ameliorates 
any temporary fault effects. 
The output parity  P can be described using a weight- 
ing vector b and the inner product applied to the output 
transform vector y. 
P  =    = b*y  Oufput parity 
(11) 
The  weights associated with  a comparable  input parity 
value are contained in a vector d, to be determined more 
completely below. 
P’  =  = d*x 
Input parity 
(12) 
Since the output values in y are related through the trans- 
form matrix  T to the input value x as in equation (l), the 
output parity P can also be described using the inputs x. 
P  =  =   =  b*Tn: 
(13) 
So, in a fault-free situation, the input parity  weights in d 
can be related to those in b. 
PI  =  j  d =  T*b 
(14) 
This follows from the defining properties of the conjugate 
transpose of a matrix  [ 9 ] .  This is the theoretical form of 
a result always appearing in FFT protection papers [2-81. 
Error detection is achieved by comparing P and P’  in a 
totally self-checking checker (TSCC) [ 15,161, as depicted 
in Figure 2b. The checker forms the syndrome S, the dif- 
ference of the two parities, and determines if  its magni- 
tude  is  small,  below  a chosen  threshold, indicating  ac- 
ceptable agreement within roundoff  tolerances 
s = P  - PI 
15’1 < T * 
Syndrome 
(15a) 
no errors Threshold T  (15b) 
(15) 
When  the  parity  generator  mechanisms  are  defined 
through the inner product, it is natural  to realize that this 
also defines linear functionals on the appropriate vector 
spaces  [Chapter 4, 171.  (A linear functional is a  linear 
mapping  from  a  vector  space  into  its coefficient  field. 
Such functionals form a vector space over the same field 
in their own right; it is called the dual  space of a vector 
39 
space.)  For a finite dimensional vector space, all linear 
functionals are equivalent to  a  vector coupled with  the 
inner product operation. 
fb  ( y )  - ( b  , y)  Linear functional 
fd  (x) - ( d ,  x)  Linear functional 
( 1 6 a )   (16) 
( l G b )  
4. Fault error models and output error spaces 
All  previous  fault  tolerance  work  has  focused  on 
protecting against a single high level error appearing on 
a single line between stages. Such an error can model all 
the internal errors  at  the  output or even  within  a  stage 
[2].  The error effect  is described  by  adding a complex 
number to the normal  error-free value appearing on that 
line.  This does not mean  all single errors are additive in 
nature.  Rather, any error effects are modeled by  adding 
an appropriate value to the usual value found there. 
This  approach  to  modeling  errors  is  reminiscent  of 