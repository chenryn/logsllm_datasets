25
28
8
262014
29
2115
27
93
1016224
5
1117
23
Figure 2. Example of locations and obfuscation. The area within which
users move is divided into M = 29 regions. Consider user u whose actual
location is region r12 at a given time t. Different obfuscation methods will
replace r12 with a different location pseudonym r′ ∈ R′: r′ = {14} in
the perturbation method, r′ = {12, 15, 26} in the adding dummy regions
method, r′ = {9, 10, 11, 12, 13, 14, 15} in the reducing precision method,
and r′ = ∅ in the location hiding method.
depending on its type and architecture. For example, an
online mechanism in the distributed architecture only looks
at
the current event for obfuscation, whereas an online
mechanism in the centralized architecture can consider all
so-far generated events from all of the users at the time of
obfuscating the current event.
Formally, an obfuscated event is a triplet hu, r′, ti, where
u ∈ U , r′ ∈ R′, and t ∈ T . As before, an obfuscated
trace of user u is a T -size vector of obfuscated events ou =
(ou(1), ou(2), ..., ou(T )). The set of all possible obfuscated
traces of user u is denoted by Ou.
An obfuscation mechanism is a function that maps a trace
au ∈ Au to a random variable Ou that takes values in the
set Ou. The probability density function of the output is f :
fau(ou) = Pr{Ou = ou|Au = au}.
(1)
For the obfuscation, the LPPM covers various methods
that reduce the accuracy and/or precision of the events’
spatiotemporal information:
• perturbation (adding noise)
• adding dummy regions
• reducing precision (merging regions)
• location hiding
These methods probabilistically map a region in an event
to a location pseudonym in R′. For these methods, it sufﬁces
that the set R′ be the power set of R, i.e., R′ ≡ P(R).
Figure 2 illustrates different obfuscation functions.
An anonymization mechanism is a function Σ chosen
randomly among the functions that map U to U ′. The random
function Σ is drawn according to a probability density
function g:
g(σ) = Pr{Σ = σ}.
(2)
In this paper, we will consider only one anonymization
is
mechanism: random permutation. That
the set U ′
is,
{1, 2, . . . , N }, a permutation of the users is chosen uni-
formly at random among all N ! permutations and each user’s
pseudonym is his position in the permutation.
A location-privacy preserving mechanism LPPM is a
pair (f, g). Given a set of actual traces {au1 , ..., auN }, the
mechanism LPPM applies f to obfuscate each trace, thus
generating a set of obfuscated traces {ou1, ..., ouN }, which
are instantiations of the random variables {Ou1 , ..., OuN }. It
then applies g on that set, thus generating a set of obfuscated
and anonymized traces {oσ(u1), oσ(u2), ..., oσ(uN )}, where
σ(·) is an instantiation of the random function Σ.
Now, we can summarize the operation of the LPPM with
the following probability distribution function that gives the
probability of mapping a set of actual traces a ∈ A to a set
of observed traces o ∈ O = O1 × O2 × . . . × ON :
LPPMa(o) = Pr(cid:8)∩N
i=1
OΣ(ui) = oσ(ui )| ∩N
i=1
Aui = aui(cid:9)(3)
Broadly speaking, the aim of the adversary is to invert
this mapping: Given o, he tries to reconstruct a.
C. Adversary
In order to evaluate an LPPM accurately, we must model
the adversary against whom the protection is placed. Hence,
the adversary model is certainly an important, if not the most
important, element of a location-privacy framework. An
adversary is characterized by his knowledge and attack(s).
A framework should specify how the adversary obtains and
constructs his knowledge, how to model his knowledge and
what attacks he performs in order to reconstruct users’
location-information.
The adversary is assumed to know the anonymization
and obfuscation probability distribution functions f and
g. The adversary may also have access to some training
traces (possibly noisy or incomplete) of users, and other
public information about locations visited by each user, such
as their home and workplace. From this information, the
adversary constructs a mobility proﬁle P u for each user
u ∈ U . In Section III-B, one way of constructing the
adversary’s knowledge is explained in detail as part of the
location-privacy meter tool.
JJ
U
J
J
R
J
T
Given the employed LPPM (i.e., f and g), the users’
proﬁles {(u, P u)}u∈U , and the set of observed traces
{o1, o2, ..., oN } that are produced by the LPPM, the attacker
runs an inference (reconstruction) attack and formulates his
objective as a question of the U −R−T type. Schematically,
in such a question, the adversary speciﬁes a subset of users,
a subset of regions and a subset of time instants, and asks
for information related to these subsets. If the adversary’s
250
objective is to ﬁnd out the whole sequence (or a partial
subsequence) of the events in a user’s trace, the attack is
called a tracking attack. The attacks that target a single
event (at a given time instant) in a user’s trace, are called
localization attacks. These two categories of attacks are
examples of presence/absence disclosure attacks [21]: they
infer the relation between users and regions over time. In
contrast, if the physical proximity between users is of the
adversary’s interest, we call the attack a meeting disclosure
attack (i.e., who meets whom possibly at a given place/time).
An example of a very general tracking attack is the one
that aims to recover the actual trace of each user. That is,
it targets the whole set of users and the whole set of time
instants, and it asks for the most likely trace of each user, or
even for the whole probability distribution of traces for each
user. More speciﬁc objectives can be deﬁned, which lead
to all sorts of presence/absence/meeting disclosure attacks:
Specify a user and a time, and ask for the region where the
user was at the speciﬁed time; specify a user and a region,
and ask for the times when the user was there; specify a
subset of regions, and ask for the (number of) users who
visited these regions at any time.
In this paper, we provide an algorithm that implements the
most general tracking attack; with the results of this attack
at hand, many other objectives can be achieved. For some
speciﬁc types of objectives we design attacks that are much
faster and less computationally intensive than the general
attack. The details will be explained in Section III-D.
D. Evaluation
At a high level, the adversary obtains some obfuscated
traces o, and, knowing the LPPM and the mobility proﬁles
of the users, he tries to infer some information of interest
about the actual traces a. As we have mentioned, the possible
objectives of the adversary range from the very general (the
traces a themselves) to the speciﬁc (the location of a user at
a speciﬁc time, the number of users at a particular location
at a speciﬁc time, etc.).
Nevertheless, usually, neither the general nor the speciﬁc
objectives have a single deterministic answer. The actual
traces are generated probabilistically from the mobility pro-
ﬁles, and the observed traces are generated probabilistically
by the LPPM. So, there are many traces a that might have
produced the observed traces o. The same goes for the more
speciﬁc objectives: There are many regions where a user
might have been at a particular time. The output of the
attack can be a probability distribution on the possible out-
comes (traces, regions, number of users), the most probable
outcome, the expected outcome under the distribution on
outcomes (the average number of users), or any function
of the actual trace. We call φ(·) the function that describes
the attacker’s objective. If its argument is the actual trace a,
then its value φ(a) is the correct answer to the attack. X is
the set of values that φ(·) can take for a given attack (M
regions, N users, M T traces of one user, etc.).
The probabilistic nature of the attacker’s task implies that
he cannot obtain the exact value of φ(a), even if he has an
inﬁnite amount of resources. The best he can hope for is
to extract all the information about φ(a) that is contained
in the observed traces. The extracted information is in the
form of the posterior distribution Pr(x|o), x ∈ X , of the
possible values of φ(a) given the observed traces o. We call
uncertainty the ambiguity of this posterior distribution with
respect to ﬁnding a unique answer – that unique answer need
not be the correct one; see the discussion on correctness later.
The uncertainty is maximum, for example, if the output of a
localization attack is a uniform distribution on the locations.
On the contrary, the uncertainty is zero if the output is a
Dirac distribution on one location.
Of course, the attacker does not have inﬁnite resources.
Consequently, the result of the attack is only an estimate
and Pr(x|o).
cPr(x|o) of the posterior distribution Pr(x|o). We call in-
accuracy the discrepancy between the distributionscPr(x|o)
Neither the uncertainty metric nor the inaccuracy metric,
however, quantify the privacy of the users. What matters for
a user is whether the attacker ﬁnds the correct answer to his
attack, or, alternatively, how close the attacker’s output is to
the correct answer. Knowing the correct answer, an evaluator
of the LPPM calculates a distance (or expected distance)
between the output of the attack and the true answer. The
choice of distance depends on the attack; we give examples
in Section IV. We call this distance the correctness of the
attack, and we claim that this is the appropriate way to
quantify the success of an attack.
JJ
J
Accuracy C
e
r
t
J
Correctness
a
i
n
J
t
y
It is important that the accuracy and the certainty not
be mistaken to be equivalent
to the correctness of the
attack. Even an attacker with inﬁnite resources will not
necessarily ﬁnd the true answer, as he might have observed
only an insufﬁcient number of traces. But he will extract
all the information that is contained in the traces, so the
accuracy will be maximum. If the accuracy is maximum, and
simultaneously the observed traces point to a unique answer
– so the certainty is also maximum – the correctness still
need not be high. It is possible, for instance, that the user
did something out of the ordinary on the day the traces were
collected; what he did is still consistent with the observed
trace, but as it is not typical for the user it is assigned a low
probability/weight in the attack output.
1) Accuracy: We compute the accuracy of each element
of the distribution cPr(x|o), x ∈ X , separately. That is, we
251
Highaccuracy
Highcertainty
Lowcorrectness
xc
xcHighaccuracy
Lowcertainty
Lowcorrectness
Lowaccuracy
Lowcertainty
Lowcorrectness
xc
X
X
X
Highaccuracy
Highcertainty
Highcorrectness
xc
xcLowaccuracy
Highcertainty
Lowcorrectness
Lowaccuracy
Highcertainty
Highcorrectness
xc
X
X
X
Figure 3.
Accuracy, Certainty, and Correctness of the adversary. The
is xc. In this example, x can get three discrete values. The black dot shows
adversary is estimating cPr(x|o) where the true value for x (correct guess)
the estimate cPr(x|o) for different x and the lines show the conﬁdence
interval for a given conﬁdence level chosen by the adversary. As it is
shown in the ﬁgures, the accuracy of the estimation is independent of
its certainty and correctness. Moreover, the level of correctness does not
convey anything about
the level of certainty, and high certainty does
not mean high correctness. The only correlation between certainty and
correctness is that low certainty usually (depending on the size of X and
the distance between its members) implies low correctness.
estimate the posterior probability Pr(x|o) for each possible
value x of φ(a). We quantify the accuracy with a conﬁdence
interval and a conﬁdence level. By deﬁnition, the probability
that the accurate value of Pr(x|o) is within the conﬁdence
interval is equal to the conﬁdence level.
The extreme case is that the interval is of zero length (i.e.,
a point) and the conﬁdence level is 1 (i.e., the attacker is
absolutely conﬁdent that the point estimate is accurate). An
attacker using more and more accurate estimation tools could
achieve this extreme case, thus makingcPr(x|o) converge to
Pr(x|o). However, achieving such ultimate accuracy might
be prohibitively costly. So, the adversary will in general
be satisﬁed with some high enough level of accuracy (i.e.,
large enough conﬁdence level, and small enough conﬁdence
interval). When the accuracy reaches the desired level, or
the resources of the adversary are exhausted, the probability
adversary.
2) Certainty: We quantify the certainty with the entropy
cPr(x|o) with some conﬁdence interval is the estimate of the
of the distributioncPr(x|o). The entropy shows how uniform
vs. concentrated the estimated distribution is and, in conse-
quence, how easy it is to pinpoint a single outcome x out
of X . The higher the entropy is, the lower the adversary’s
certainty is.
ˆH(x) =Xx cPr(x|o) log
1
cPr(x|o)
3) Correctness: The correctness of the attack is quanti-
ﬁed using the expected distance between the true outcome
xc ∈ X and the estimate based on the cPr(x|o). In general,
if there is a distance k · k deﬁned between the members of
X , the expected distance can be computed as the following
sum, which is the adversary’s expected estimation error:
(4)
(5)
Xx cPr(x|o)kx − xck
As an example, if the distance is deﬁned to be equal to
0 if and only if x = xc and to be equal to 1 otherwise,
which is the probability of error of the adversary.
then the incorrectness can be calculated to be 1 −cPr(xc|o),
The value xc is what the users want to hide from the ad-
versary. The higher the adversary’s correctness is, the lower
the privacy of the targeted user(s) is. Hence, correctness is
the metric that determines the privacy of users.
In summary, the adversary achieves the maximum accu-
racy for his estimates cPr(x|o) that is possible under his
resource constraints. He can measure the success of the
attack by computing the certainty over the results. However,
to measure users’ privacy, the evaluator of an LPPM must
consider the true value xc and measure the adversary’s
correctness. Notice that the adversary does not know the
value of xc, hence he cannot evaluate this aspect of his
performance. Figure 3 illustrates through some examples
the independence of these three aspects (of the adversary’s
performance) from each other.
III. LOCATION-PRIVACY METER:
IMPLEMENTATION OF OUR FRAMEWORK AS A TOOL
In this section, we present Location-Privacy Meter, a
realization of our framework as a tool to measure location
privacy. We have developed a modular tool based on the
framework presented in Figure 1 and multiple reconstruction
(inference) attacks are designed to evaluate the effectiveness
of LPPMs with respect to different adversaries. The tool,
available online [1], is developed in the C++ language, so
it is fast and it can be ported to various platforms. As will
be explained further, designers of new LPPMs can easily
specify various LPPM functions in our tool in order to
compare the users’ location privacy in different schemes.
In the following subsections, we explain in detail the
speciﬁcations of different modules of the tool and also
the algorithms that we use in Location-Privacy Meter. The
evaluation of some LPPMs will be presented in Section IV.
A. Location-Privacy Preserving Mechanisms
In the current implementation of the tool, we have devel-
oped two main LPPM obfuscation mechanisms that appear
252
p
(
x
)
p
(
x
)
p
(
x
)
p
(
x
)
p
(
x
)