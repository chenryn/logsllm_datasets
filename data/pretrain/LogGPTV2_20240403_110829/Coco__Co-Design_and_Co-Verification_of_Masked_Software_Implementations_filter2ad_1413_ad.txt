ister ﬁle, IF stage) are reported. The area consumption of the
ID+EX stage is omitted because there is no overhead. The to-
tal area overhead of the design with all security ﬁxes enabled
is around 10%.
as an open research question for the future.
4 Problems and Fixes in Data Memory
In this section, we discuss how data memory, more speciﬁcally
SRAM, can be integrated into our secured IBEX core so we
can formally prove the leakage-free execution of masked soft-
ware implementations for the entire system. Typically, micro-
processors such as ARM Cortex-M devices feature a Harvard
architecture, which means that dedicated memory modules are
used for data/instruction memory (based on SRAM/Flash tech-
nology). Especially on low-end devices, without sophisticated
branch prediction and cache architectures, this design choice
improves overall performance since simultaneous memory
accesses to both memory modules are possible. For our pur-
poses, dealing with instruction memory is comparably easy
since instructions only dictate the data/control ﬂow. They are
not directly involved in any computations and are thus not
labeled as shares in our veriﬁcation. Hence, from a hardware
perspective, we do not need to take any special precautions
when adding instruction memory to our IBEX core.
The situation becomes more complicated for data memory,
as it plays an important role for masked software implementa-
tions that cannot hold all intermediate values of a computation
in its register ﬁle. At ﬁrst glance, one could consider applying
the same design strategy, as used for the register ﬁle (cf. Fig-
ure 2b), also to the data memory. However, one-hot encoding
does not scale well with larger address spaces and would re-
sult in impractical hardware overhead. Consequently, we need
to discuss options that keep the hardware overhead reason-
able while still allowing correctness proofs for the entire CPU
design. In the following, we discuss two such options that
utilize partially one-hot encoded address signals and result
in different trade-offs between hardware overhead and the
USENIX Association
30th USENIX Security Symposium    1477
(a) Using glitchy SRAM blocks. The stable one-
hot encoding of the higher address bits is computed
outside of the SRAM blocks.
(b) Using glitch-free SRAM blocks that compute a stable
one-hot encoding of the lower address bits. The word line
(WL) selects the active word (see also Figure 4).
Figure 3: Two options of adding SRAM to our IBEX core.
number of rules that need to be followed by masked software
implementations. The ﬁrst option utilizes one-hot encoding
in the upper address bits, i.e., for selecting SRAM blocks,
and does not make any assumptions on the inner workings
of the SRAM blocks. The second option describes how one-
hot encoding in the lower address bits can be used to build
“glitch-free” SRAM blocks that can then easily be added to
our IBEX core without any hardware overhead.
4.1 MSB One-hot Address Encoding
The ﬁrst viable option of using partial one-hot encoding for
data memory involves using one-hot encoding for the higher
bits of the address signal, as illustrated in Figure 3a. In this
example, we consider the case of a low-end 32-bit device with
32KB of RAM that can be addressed on word granularity with
13-bit address signals (i.e., using bits 2 to 14 from the original
32-bit signal). First, we extract 13 bits from the original 32-bit
address signal. This 13-bit signal is then further split up into a
5-bit block address (later expanded to a 32-bit one-hot signal)
and an 8-bit word address for selecting a word within one
SRAM block. This design choice ensures that no glitches can
occur across SRAM blocks, yet they could still occur between
the words of a single SRAM block. More concretely, when
considering a masked software implementation that operates
on a secret s, represented by the shares s = s1 ⊕ s2, then our
construction results in the following software constraints for
SRAM usage:
C1SRAM Storing both, s1 and s2, in separate SRAM blocks
is ﬁne as long as they are not accessed in immediate
succession.
C2SRAM Storing s1 and s2 within the same SRAM block can
result in potential leaks and thus needs to be avoided.
The hardware overhead of utilizing one-hot encoding in the
higher address bits is mainly determined by the additionally
needed one-hot encoder circuitry and one 40-bit register. On
the other side, when comparing Figure 3a to Figure 3b, one
can also see that the MUX-tree, used for selecting the SRAM
output, can be replaced by a simpler, and thus cheaper OR-
tree. Overall, and when compared to the typical area of SRAM
blocks, we do not expect any noticeable hardware overhead
of this construction. From a latency perspective, there is no
delay as long as the one-hot encoding can be performed in
the cycle before the actual lookup. We expect this to hold for
most designs.
4.2 LSB One-hot Address Encoding
Another option of utilizing partially one-hot encoded address
signals consists of using one-hot encoding only for certain
less signiﬁcant bits of the address signal, as illustrated in Fig-
ure 3b. In this case, the 13-bit address signal is divided into
an 8-bit block address (for specifying the SRAM block) and
a 5-bit word address that is later expanded to a 32-bit one-hot
signal (for specifying a word within an SRAM block). This
construction will, similarly to the register ﬁle, as discussed
previously (cf. Section 3.2), eliminate glitches between words
of the same SRAM block, except for the case when they
are accessed in immediate succession. Consequently, when
1478    30th USENIX Security Symposium
USENIX Association
SRAM BlockSRAM BlockSRAM BlockSRAM Block13RegReg58 One-Hot32EnEnEnEnWAWAWAWA32OutputORORRead Addr [2:14]Word Address1288SRAM CellsSRAM CellsSRAM CellsSRAM CellsRead Addr [2:14]1385 One-HotWL32OutputMUXMUX215Block AddressWord AddressReg One-HotWLReg One-HotWLReg One-HotWLRegSRAM BlockReg32operating with the shares s1 and s2, masked software imple-
mentations need to follow the following constraints:
C1SRAM Storing both, s1 and s2, within the same SRAM block
is ﬁne as long as they are not used in immediate succes-
sion.
C2SRAM Storing s1 and s2 in different SRAM blocks can result
in potential leaks and thus needs to be avoided.
When looking at the standard design of SRAM cells in Fig-
ure 4, one can observe that the word line (WL) needs to be a
one-hot encoded signal while each bit line (BL) is connected
to one bit location of all words within one SRAM block,
thereby essentially functioning as an OR gate. On a concep-
tional level, this is similar to the construction in Figure 3b,
were we use additional registers to ensure a stable WL signal.
In other words, if a given SRAM block has a layout that
already achieves internally stable WL signals in practice then
no hardware modiﬁcations are required and an ordinary MUX-
based output selector can be used. Of course, it is generally
not easy to tell if, or to what extent, an off-the-shelf SRAM
block fulﬁlls this requirement since they are full custom and
partially analog blocks. In a typical SRAM row decoder de-
sign, an individual WL signal is derived by a single, wide
NOR gate with a fan-in that is equal to the number of bits in
the word address (see Section 2.7 in [41]). Roughly said, if
the address signal is stable, then the low combinatorial depth
of the row decoder likely only causes small glitches that could
then be compensated with the custom circuit layout. Besides
that, stable WL signals are also desirable from a power and
latency perspective since (1) each WL signal can drive up
to 64 transistors, glitches can hence signiﬁcantly impact the
power proﬁle, and (2) the time until the differential sense am-
pliﬁer (SA) output is stable strongly depends on the presence
of glitches on the WL signals, which in return reduces the
maximum operating frequency.
5 Co-Veriﬁcation with COCO
In this section, we discuss the details of the workﬂow of
COCO, our veriﬁcation tool, and report the runtime effort for
each step. We evaluate COCO using several benchmarks, in-
cluding ﬁrst-order and higher-order masked implementations
executed by the secured IBEX processor and show that COCO
can efﬁciently verify those. We run all our evaluations using
a 64-bit Linux Operating System on an Intel Core i7-7600U
CPU with a clock frequency of 2.70 GHz and 16 GB of RAM.
Additionally, we practically evaluate our design using a ﬁrst-
order t-test on a SAKURA-G FPGA evaluation board.
5.1 Veriﬁcation Flow
The veriﬁcation ﬂow implemented by COCO consists of four
steps, as illustrated in Appendix B. The four steps are divided
Figure 4: Typical layout of SRAM cells. Each pair of NOT
gates represents a 1-bit memory cell. The one-hot encoded
word line (WL) selects the active word. The bit line BLi
connects bits at location i from all words. The negated BL
signal, together with the differential sense ampliﬁer (SA), help
achieving stable output values faster.
into three preprocessing steps (1)-(3), and the ﬁnal veriﬁca-
tion step (4). The preprocessing steps are needed to join the
masked assembly implementation of the cipher with the IBEX
System Verilog sources into one single VCD execution trace,
which is then used during veriﬁcation. For all our experiments,
we use the secured IBEX processor, which consists of the se-
cured core and memory, as described in Sections 3 and 4. In
detail, the veriﬁcation ﬂow is as follows:
(1) The masked implementation of the target cipher is com-
piled using the 32-bit RISC-V assembler. The resulting
binary ﬁle is then converted into a Verilator [47] testbench.
(2) We use Yosys [50] to parse the hardware model, a set
of System Verilog ﬁles, of the secured IBEX processor.
Yosys (Yosys Open SYnthesis Suite) is an open-source
framework which synthesizes and optimizes the model
and produces a netlist of the circuit in Verilog format and
as a graph, with gates as nodes and wires as edges.
(3) We run Verilator using the testbench created in (1) and
the circuit netlist created in (2). It produces an execution
trace of the masked cipher executed by the secured IBEX
processor in VCD format.
(4) In the last step, the actual veriﬁcation is done using a
Python script. The script’s input are the circuit graph, the
VCD execution trace and the veriﬁcation conﬁguration.
The veriﬁcation conﬁguration consists of the register label
ﬁle, which speciﬁes which registers or memory locations
contain shares of a secret and which contain fresh ran-
domness, the veriﬁcation mode (stable or transient), the
number of cycles which should be veriﬁed and the order of
the masked cipher. Finally, the veriﬁcation process outputs
USENIX Association
30th USENIX Security Symposium    1479
BL0~BL0WL0BL1~BL1WL1Output0Output1SASANOTNOTNOTNOTNOTNOTNOTNOTWLwhether the execution is leakage-free or not, together with
the cycle and gate number in which the leakage occurred.
Since the System Verilog support of Yosys is limited, we
use the Symbiotic EDA Edition of Yosys (0.8+472), which
works with a frontend of Veriﬁc in order to support System
Verilog. Verilator 4.010 is used to create the execution traces.
A Python script is used to create the SAT formulas, which are
later solved by CaDiCaL 1.0.3.
In our experiments, we cannot work with real SRAM blocks
for data RAM. Usually, one would use pre-build and pre-
conﬁgured SRAM modules and instantiate them with a macro
in the Verilog code. However, in that case, we can neither
trace the behavior of the block during execution nor label
memory cells. Therefore, we create a Verilog hardware model
according to the LSB one-hot address encoding scheme, as
described in Figure 3b, which behaves like a real SRAM
module. The module is divided into 16 blocks consisting of 8
32-bit words each. Furthermore, we conﬁgure IBEX core to
use 1 kilobyte of instruction memory for all test cases except
the DOM AES S-box, where we use 4 kilobytes.
5.2 Evaluation of Preprocessing Steps (1) - (3)
COCO’s preprocessing steps aim at preparing all resources
for the veriﬁcation. The runtime of the testbench creation
(1) takes about 0.04 s for all our experiments. The runtime
of the tracing part (3) is determined by the circuit size and
number of cycles it needs to execute the masked software
implementation with IBEX and takes 0.003 s per cycle. The
parsing step (2) has to be run only once for the whole secured
IBEX and takes about 7 min and depends mostly on the circuit
size, including the size of instruction and data memory.
The result of (2) is a netlist of the secured IBEX proces-
sor in graph representation. The IBEX core, excluding data
and instruction memory, consists of almost 27 000 gates. It
is important to note that our hardware design is orders of
magnitudes larger than designs considered by other veriﬁca-
tion tools. For example, REBECCA [8] performs veriﬁcation
on hardware circuits consisting of at most 200 registers and
3 000 non-linear gates, while maskVerif [2] and Silver [28]
consider circuits with up to 300 and 1 000 probing positions.
5.3 Evaluation of the Veriﬁcation Step (4)
The veriﬁcation results of the masked software implementa-
tion run on the secured IBEX processor, and their veriﬁcation
runtime are shown in Table 3. The table states the testcase in
RISC-V assembly and how many cycles the execution takes.
We report the number of labels provided by the user, divided
into shares and fresh randomness. It is very important to note
that each of these shares or random values is either 32 bit
or 16 bit wide. Other veriﬁcation methods often argue that a
hardware circuit computing a masked cipher treats each bit
in the same way, so it is sufﬁcient to view a 32-bit register
as one single share. However, in the IBEX processor, this is
not the case, since logic in different computation units tends
to treat each register bit differently. Therefore, we must label
and check all 32 bits individually.
The selection of masked circuits covers different masked
GF(2) multipliers (AND gates), including the Domain-
Oriented Masking (DOM) AND, Ishai-Sahai-Wagner (ISW)
AND, Threshold Implementation (TI) AND and Trichina AND,
but also larger implementations like the Keccak S-box and the
AES S-box. Furthermore, we show that it is feasible to verify
second-order and third-order implementations. Our bench-
marks focus on the veriﬁcation of non-linear parts of cipher
implementations, similar to REBECCA, maskVerif and Sil-
ver, although the linear parts could easily be added to the
implementation. COCO veriﬁes all tested ﬁrst-order masked
multipliers in transient mode in less than 20 s. Larger test-
cases, for example, the DOM AES S-box, can be veriﬁed in a
few hours.
In addition, we want to point out that errors in implementa-
tions can be found efﬁciently. Implementations marked with
Ø refer to implementations which cause side-channel leakage
when executed with the secured IBEX because (1) masking is
either done incorrectly on the algorithmic level, or (2) mask-
ing is correct on the algorithmic level but software constraints
are not satisﬁed. DOM AND reg.Ø is a ﬁrst-order DOM multi-
plier based on [22], in which fresh randomness is added to the
shares too late. The stable veriﬁcation reports an error in cycle
12 in a gate belonging to the ALU. DOM Keccak S-box reg.Ø,
based on [23], does not follow constraint C2CORE. This ﬂaw
is reported by transient veriﬁcation in cycle 70 and appears
directly on the read port of the register ﬁle. The veriﬁcation
runtime of an insecure implementation is similar to that of a
secure implementation because the veriﬁcation terminates as
soon as the leakage check for any share fails.
The total veriﬁcation runtime can be split into the con-
struction and solving of the SAT formula. In our experiments,
solving the SAT formula requires considerably less time than
constructing the SAT formula, which is linear in the num-
ber of gates in the netlist, i.e., the number of registers and
the size of the combinatorial logic between these registers.
Hence, for moderate increases of the problem size, for exam-
ple through larger cores having multiple ALUs or additional
pipeline stages, we expect the veriﬁcation time to increase lin-
early. Compared to REBECCA, which is limited to the veriﬁca-
tion of pure hardware implementations, the hardware/software
co-veriﬁcation approach of COCO employs more aggressive
optimization measures by simplifying correlation sets through
concrete values from the execution trace, and can therefore
more easily deal with scalability issues.
1480    30th USENIX Security Symposium
USENIX Association
Runtime Leaking
(cycles)
Cycle
First-order
Name
DOM AND reg. [22]
DOM AND reg.Ø
DOM AND [22]
ISW AND reg. [26]
TI AND reg. [35]
Trichina AND reg. [49]
DOM Keccak S-box reg. [23]
DOM Keccak S-box reg.Ø
DOM Keccak S-box [23]
DOM AES S-box [9]
13
13
39
13
17
19
89
88
219
1900
Input
Shares
4× 32 bit
4× 32 bit
4× 32 bit
4× 32 bit
6× 32 bit
4× 32 bit
10× 32 bit
10× 32 bit
10× 32 bit
16× 16 bit
6× 32 bit
15× 32 bit
8× 32 bit