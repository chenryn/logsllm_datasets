title:Prime+Scope: Overcoming the Observer Effect for High-Precision Cache
Contention Attacks
author:Antoon Purnal and
Furkan Turan and
Ingrid Verbauwhede
Prime+Scope: Overcoming the Observer Effect
for High-Precision Cache Contention Attacks
Antoon Purnal
imec-COSIC, KU Leuven
Furkan Turan
imec-COSIC, KU Leuven
Ingrid Verbauwhede
imec-COSIC, KU Leuven
ABSTRACT
Modern processors expose software to information leakage through
shared microarchitectural state. One of the most severe leakage
channels is cache contention, exploited by attacks referred to as
Prime+Probe, which can infer fine-grained memory access patterns
while placing only limited assumptions on attacker capabilities.
In this work, we strengthen the cache contention channel with
a near-optimal time resolution. We propose Prime+Scope, a cross-
core cache contention attack that performs back-to-back cache
contention measurements that access only a single cache line. It
offers a time resolution of around 70 cycles (25ns), while main-
taining the wide applicability of Prime+Probe. To enable such a
rapid measurement, we rely on the deterministic nature of mod-
ern replacement policies and their (non-)interaction across cache
levels. We provide a methodology to, essentially, prepare multiple
cache levels simultaneously, and apply it to Intel processors with
both inclusive and non-inclusive cache hierarchies. We characterize
the resolution of Prime+Scope, and confirm it with a cross-core
covert channel (capacity up to 3.5 Mbps, no shared memory) and
an improved attack on AES T-tables. Finally, we use the properties
underlying Prime+Scope to bootstrap the construction of the evic-
tion sets needed for the attack. The resulting routine outperforms
state-of-the-art techniques by two orders of magnitude.
Ultimately, our work shows that interference through cache
contention can provide richer temporal precision than state-of-the-
art attacks that directly interact with monitored memory addresses.
CCS CONCEPTS
• Security and privacy → Software and application security;
Systems security.
KEYWORDS
Cache Attacks, Cache Side-Channels, Microarchitecture
ACM Reference Format:
Antoon Purnal, Furkan Turan, and Ingrid Verbauwhede. 2021. Prime+Scope:
Overcoming the Observer Effect for High-Precision Cache Contention At-
tacks. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security (CCS ’21), November 15–19, 2021, Virtual Event,
Republic of Korea. ACM, New York, NY, USA, 15 pages. https://doi.org/
10.1145/3460120.3484816
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484816
1 INTRODUCTION
Modern processors comprise many hardware components that,
transparently to the programmer, enhance average software perfor-
mance. Several such components, with the cache hierarchy as the
leading example, may be shared across software-defined security
boundaries. Through their access patterns, programs may unwill-
ingly encode secret information into shared cache state, which can
be extracted by a co-located adversary using a timing side chan-
nel. In particular, she first prepares the cache state and afterwards
measures it to infer the changes produced by other programs.
Several techniques have been proposed to prepare and measure
the cache state. While some can only monitor accesses to shared
memory between attacker and victim, or require specific proces-
sor features, other techniques have no such prerequisites and are
purely based on contention for cache resources. By targeting core-
shared cache levels, such as the last-level cache (LLC) [31, 37], or
the coherence directory (CD) [65], an attacker can measure cache
contention for victim programs running on other processor cores.
Known mostly as Prime+Probe, cache contention attacks are
widely applicable. The contention channel leaks information across
virtual machine boundaries [31, 37, 47], to and from sandboxed code
(e.g., in the browser [41, 53]), or even over the network [33]. It has
been used to extract sensitive information of various kinds, such as
cryptographic keys [42], user input [33, 47], kernel information [29]
and browsing behavior [53]. It also enables establishing covert
channels [37–39] and, recently, transient execution attacks [22, 48].
An important metric of cache attack techniques is their temporal
resolution, i.e., the precision with which they can localize victim
memory accesses in the time domain. The finer the resolution of
the attack, the greater the visibility into data accesses and con-
trol flow of victim applications. This is of special importance in
the general setting, where the attacker monitors victim behavior
asynchronously and victim accesses may occur at any given time.
In case of insufficient time precision, prior works slow down the
victim application (e.g., [3, 4, 9, 21]), or interrupt it heavily (e.g., [27,
40]), to amplify secret-dependent time differences. Instead of such
performance degradation of the victim, which may not always be
possible, this work pursues the opposite direction and investigates
whether the time precision of cache attacks can be improved (and
optimized). In particular, we identify two key challenges to enhance
the time resolution of state-of-the-art cache contention attacks.
First, the main challenge to improve the precision of cache at-
tacks, in general, is the observer effect, i.e., the phenomenon where
the act of measuring a system affects its state. Many techniques suf-
fer from it, often requiring the state change of every measurement
to be undone before the next one can be performed. To minimize
the influence of this effect, cache attacks are often discretized along
the time axis in windows of fixed duration (e.g., [3, 37, 65, 67, 69]).
However, this places fundamental limits on their time resolution.
Session 11A: Attestation and Firmware Security CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2906Second, the cache contention channel, in particular, faces another
bottleneck. For Prime+Probe, each probe accesses as many cache
lines as the associativity of the target structure, e.g., at least 11 ways
for core-shared caches on modern Intel CPUs. Therefore, the time
resolution is structurally bounded by the time it takes to access all
these lines, even if the observer effect were to be overcome.
In this paper, we seek to optimize the resolution of Prime+Probe-
style attacks. To this end, we ask the following main questions:
Is it possible to bypass the observer effect? Can contention be inferred
by repeatedly measuring the access latency of a single cache line?
In this work, we make the surprising observation that the cache
contention channel can have a higher time resolution than tech-
niques that access monitored addresses directly. We propose Prime+
Scope, a high-precision cross-core cache contention attack, whose
measurement is both repeatable (i.e., the cache state does not need
to be reinstated after every measurement), and essentially optimal
(i.e., it performs a single memory access). Prime+Scope can monitor
events asynchronously with a precision in the order of 25ns, signif-
icantly outperforming comparable techniques. At the same time,
Prime+Scope inherits the general applicability of Prime+Probe.
Prime+Scope prepares the cache more precisely than traditional
cache contention attacks. We obtain fast and effective Prime pat-
terns using both an automated and a handcrafted methodology
(resp. for inclusive and non-inclusive Intel LLCs). In the end, we
find Prime+Scope to apply to all tested Intel CPUs of the last decade.
To confirm the superior time precision of Prime+Scope, we per-
form a quantitative comparison with state-of-the-art techniques.
We also implement a cross-core covert channel on a last-level cache
(LLC) and a coherence directory (CD). Symbols are encoded tempo-
rally in slots of no more than 80-120 processor cycles. The LLC/CD
channels reach a capacity of 3.5 Mbps and 3.1 Mbps, respectively.
We evaluate Prime+Scope on a known-vulnerable AES imple-
mentation. With its fine temporal precision, it can extract the key
material with 5x-25x fewer encryptions than Prime+Probe.
Finally, we bootstrap our newly discovered primitive to create a
straightforward, portable and linear-time eviction set construction
routine, which outperforms previous techniques by 100-600x.
Summarized, this paper makes the following main contributions:
- We present Prime+Scope, a generic cross-core cache contention
primitive with near-optimal temporal resolution.
- To prepare the cache for continuous measurement, we propose
PrimeTime, a methodology to find efficient Prime patterns.
- We evaluate Prime+Scope using micro-benchmarks, a high-
capacity covert channel, and a high-precision attack on AES.
- Using the principles underlying Prime+Scope, we present fast
and simple routines to construct LLC/CD eviction sets.
We have disclosed our findings to Intel. To facilitate reproduction
of our research, artifacts are made available at
https://www.github.com/KULeuven-COSIC/PRIME-SCOPE
This article is organized as follows. Section 2 provides the neces-
sary background. Section 3 explores the conditions for back-to-back
cache measurements. Section 4 exposes Prime+Scope, our main re-
sult. Section 5 covers the efficient preparation of the cache state, and
Section 6 evaluates Prime+Scope for micro-benchmarks and con-
crete examples. Section 7 positions our findings, Section 8 discusses
limitations and countermeasures, and Section 9 concludes.
2 PRELIMINARIES
2.1 Caches
To overcome the comparatively high latency of memory lookups,
caches are buffers that keep soon-to-be used data close to the CPU.
Caches operate on fixed-size (e.g., 64 bytes) memory blocks called
cache lines, and are typically set-associative, referring to their or-
ganization along sets and ways. Cache lines are mapped to sets
based on their memory address, and addresses mapping to the same
set are called congruent. The maximal number of congruent lines
that can reside in the cache at any given time is determined by the
number of ways 𝑊 , also referred to as the associativity.
When caching a new line exceeds the associativity, one line in
the set is evicted; in this paper, we refer to that line as the eviction
candidate (EVC). The EVC is determined by the replacement policy,
which is implemented by a (complex) state machine at the set-level,
for which the state transitions depend on the accesses to the set.
Contemporary Intel processors feature a three-level cache hier-
archy, where the access latency increases along with the distance
from the CPU. When a CPU core references a memory address,
the cache line containing this address is retrieved from the closest
cache level that has a valid copy. The first two levels (L1 and L2)
are organized separately for every core, while the last-level cache
(L3, or LLC) is shared among all the cores. The majority of Intel
processors have inclusive LLCs, meaning that cache lines present
in the L1 and L2 caches must also be present in the LLC. However,
recent Intel servers feature higher core counts and larger private
caches, prompting the adoption of non-inclusive LLCs. In such cache
hierarchies, the LLC may or may not contain lines that are present
in L1 or L2, reducing the storage overhead due to inclusion.
2.2 Cache Side-Channel Attacks
Over the last years, several techniques have been proposed to infer
memory access patterns by other programs through observation
of shared cache state. The two most prominent attack classes are
represented, respectively, by Flush+Reload and Prime+Probe. In
this overview, and in the paper, we focus on attacks across cores.
Flush+Reload-style techniques. If the memory address to be
monitored exists in memory shared with the attacker, she is able to
access it directly, allowing to infer memory activity by other pro-
cesses at cache-line granularity. The representative technique for
this attack class is Flush+Reload [27, 67], which removes (flushes)
the cache line containing a target address from the cache, and later
determines if the victim accessed it by its reload time. If cache flush-
ing is not available, Evict+Reload [26] replaces it with eviction.
Provided that the time until completion of the clflush instruc-
tion depends on the presence of the target in the cache, it can
be used to both prepare and measure the cache state. This tech-
nique, referred to as Flush+Flush [25], has a higher time resolution
than Flush+Reload. However, the more subtle time-dependence
of cache flushes results in a comparatively low cross-core accuracy.
The main drawback of Flush+Reload-style attacks is their struc-
tural dependence on shared memory with a victim, which is harder
to obtain for an attacker than only co-location. Moreover, a cache
flush instruction may also not be available in restricted contexts, e.g.,
in the browser [5, 25], or generally for unprivileged processes [34].
Session 11A: Attestation and Firmware Security CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2907(i) Windowed techniques cope with blind spots by waiting between
preparation and measurement, lowering their time precision
(ii) Windowless techniques measure the cache state continuously
without waiting, and revert the state only after detecting an event
Figure 1: Cache Manipulation techniques in the windowed vs. windowless paradigms.
Prime+Probe-style techniques. Cache contention attacks, often
synonymously referred to as Prime+Probe attacks, prime a full
cache set and measure the time it takes. Activity in this cache set
by other processes will evict one or more of the attacker’s lines,
which is reflected in a higher latency to complete the prime.
For a cross-core attack, an adversary generally targets an in-
clusive structure shared with the victim. The inclusive property
guarantees the eviction of congruent addresses from the victim’s
private caches, ensuring that future victim accesses to them indeed
generate contention on the measured set. In cache hierarchies with
an inclusive last-level cache (LLC), this requirement is readily ob-
tained [31, 37]. For non-inclusive Intel caches, a suitable structure
has been found in the coherence directory (CD) [65], which keeps
track of lines in all the L2 caches. It is organized in sets, like the
LLC, with the same function to index memory addresses into sets
and slices. Instead of priming the LLC, the attacker primes the CD,
evicting the target address from the CD and, due to its inclusion
property, also from the victim’s private L1/L2 caches. When lines
are evicted from the CD, they are moved to the LLC [65].
To measure contention on the LLC (or CD), an attacker needs to
obtain memory addresses that are mapped to the same set. In the
presence of unknown physical address bits or cache slices, these
so-called eviction sets need to be obtained at runtime [37, 60, 65].
As a variant of Prime+Probe, Prime+Abort [18] is a contention-
informed attack using Intel TSX. As TSX transactions are aborted
upon eviction of certain lines from the LLC, it is amenable to mea-
sure LLC contention, as attack [18] or defense [23]. Intel TSX may
not be exposed to an attacker (e.g., from the browser), may be
disabled for security reasons [30], or may not be available at all.
Cache contention attacks offer a spatial granularity of sets, which
is inferior to the cache-line granularity of shared-memory attacks.
However, due to the large number of sets in modern LLCs/CDs, the
spatial information encoded in cache contention is still quite large.
3 CACHE MANIPULATION PARADIGMS
Assume an attacker wants to spy on a cross-core event, i.e., one or
more memory accesses by a victim program running on another
CPU core. All cache attack techniques first prepare the cache state,
and then measure it to infer the presence or absence of an event.
This section first revisits why some techniques need to allocate a
waiting period between preparation and measurement, essentially
partitioning the time axis into windows. Then, it examines the
conditions under which a windowless paradigm can be adopted.
3.1 Windowed Paradigm
Blind Spots. To see why cache attacks are often organized in
discrete time windows, consider the traces in Figure 1i. The first
Flush+Reload trace (Figure 1i-A) continuously flushes and reloads
a target. Such an application of Flush+Reload fails to detect many
events. In particular, events that occur during the period slightly
before the Reload, until the Flush has evicted the target, remain
undetected [3, 67]. We refer to such a period as a blind spot.
To reduce the effect of blind spots on the detection rate, a wait
stage may be inserted, i.e., a predetermined idle period between
preparation and measurement. As in Figure 1i-B, such an organiza-
tion detects the events that occur during the wait stage.
Other techniques, like Evict+Reload, Flush+Flush, and Prime+
Probe, can also be instantiated like this (cf. Figure 1i-C-D-E). Evict+
Reload behaves similarly to Flush+Reload, but has a larger blind
spot as cache eviction is slower than flushing. In Section 3.2, we
will see which techniques can be used without blind spots.
Resolution. The temporal resolution of windowed techniques
is limited by the combined duration of the preparation, wait and
measurement stages. In particular, the waiting period marks a trade-
off between the accuracy and resolution of the attack. The larger the
blind spot, the lower the resolution for the same detection accuracy.
Despite this limitation, windowed techniques such as Flush+
Reload can be very powerful in practice. For instance, blind spots
can be bypassed when the attacker controls the timing of the event
(e.g., by synchronizing [12, 32, 42, 61] or interleaving [27, 68] with
the victim). The limitation is also attenuated for infrequent events
(e.g., user behavior [26, 41]), or when lower detection rates are tol-
erable (e.g., to profile a binary [26] or capture traces [28]). The miss
probability may also be reduced by targeting events that reference
the same line multiple times, e.g., loops [67] or function calls [8].
3.2 Windowless Paradigm
To understand how some techniques can increase the time resolu-
tion by avoiding windows [18, 56], we identify the two sources of
blind spots. Both sources are an expression of the observer effect,
i.e., the attacker perturbs the cache state by measuring it.
Session 11A: Attestation and Firmware Security CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2908#1: Non-preserving. We refer to a cache measurement as pre-
serving when, in the absence of an event, the relevant cache state
before and after the measurement is equivalent. If the measurement
is not preserving, monitoring cannot continue without undoing
the changes caused by the measurement. Hence, non-preserving
measurements trigger a repeated preparation phase, which natu-
rally introduces a period of time in which victim events are missed
(cf. Figure 1i). For instance, the Reload measurement in Flush+
Reload is non-preserving, so it needs to be followed by a Flush,
and events occurring at the beginning of the Flush are missed
[3, 67]).