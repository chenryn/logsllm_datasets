CIFAR10 models. For ImageNet models, we use a stronger
SB heuristic and only inspect the most signiﬁcant exponent
bit of a parameter to achieve a greater speed-up. This heuristic
causes us to miss the vulnerability the remaining bits might
lead to, therefore, its results can be interpreted as a conser-
vative estimate of the actual number of vulnerable parameters.
• Sampled parameters (SP) set. Our MNIST analysis also
reveals that almost 50% of all parameters are vulnerable to
bit-ﬂips. This leads to our third heuristic: uniformly sampling
from the parameters of a model would still yield an accurate
estimation of the vulnerability. We utilize the SP heuristic for
ImageNet models and uniformly sample a ﬁxed number of
parameters—20,000—from all parameters in a model. In our
experiments, we perform this sampling ﬁve times and report
the average vulnerability across all runs. Uniform sampling
also reﬂects the fact that a black-box attacker has a uniform
probability of corrupting any parameter.
4.2 Quantifying the Vulnerability That Leads
to Indiscriminate Damage
Table 1 presents the results of our experiments on single-bit
corruptions, for 19 different DNN models. We reveal that
an attacker, armed with a single bit-ﬂip attack primitive, can
successfully cause indiscriminate damage [RAD > 0.1] and
that the ratio of vulnerable parameters in a model varies be-
tween 40% to 99%; depending on the model. The consistency
between MNIST experiments, in which we examine every
possible bit-ﬂip, and the rest, in which we heuristically exam-
ine only a subset, shows that, in a DNN model, approximately
half of the parameters are vulnerable to single bit-ﬂips. Our
experiments also show small variability in the chances of a
successful attack—indicated by the ratio of vulnerable pa-
rameters. With 40% vulnerable parameters, the InceptionV3
model is the most apparent outlier among the other ImageNet
models; compared to 42-49% for the rest. We deﬁne the vul-
nerability based on [RAD > 0.1] and, in Appendix B, we also
give how vulnerability changes within the range [0.1 ≤ RAD
≤ 1]. In the following subsections, we characterize the vul-
nerability in relation to various factors and discuss our results
in more detail.
4.3 Characterizing the Vulnerability: Bitwise
Representation
Here, we characterize the interaction how the features of a
parameter’s bitwise representation govern its vulnerability.
Impact of the bit-ﬂip position. To examine how much
change in a parameter’s value leads to indiscriminate damage,
we focus on the position of the corrupted bits. In Figure 1,
for each bit position, we present the number of bits—in the
log-scale—that cause indiscriminate damage when ﬂipped,
on MNIST-L5 and CIFAR10-AlexNet models. In our MNIST
USENIX Association
28th USENIX Security Symposium    501
Impact of the parameter sign. As our third feature, we
investigate whether the sign—positive or negative—of the
corrupted parameter impacts the vulnerability. In Figure 2,
we examine the MNIST-L5 model and present the number of
vulnerable positive and negative parameters in each layer—in
the log-scale. Our results suggest that positive parameters are
more vulnerable to single bit-ﬂips than negative parameters.
We identify the common ReLU activation function as the
reason: ReLU immediately zeroes out the negative activation
values, which are usually caused by the negative parameters.
As a result, the detrimental effects of corrupting a negative
parameter fail to propagate further in the model. Moreover,
we observe that in the ﬁrst and last layers, the negative pa-
rameters, as well as the positive ones, are vulnerable. We
hypothesize that, in the ﬁrst convolutional layer, changes in
the parameters yield a similar effect to corrupting the model
inputs directly. On the other hand, in their last layers, DNNs
usually have the Softmax function that does not have the same
zeroing-out effect as ReLU.
4.4 Characterizing the Vulnerability: DNN
Properties
We continue our analysis by investigating how various proper-
ties of a DNN model affect the model’s vulnerability to single
bit-ﬂips.
Impact of the layer width. We start our analysis by asking
whether increasing the width of a DNN affects the number
of vulnerable parameters. In Table 1, in terms of the number
of vulnerable parameters, we compare the MNIST-B model
with the MNIST-B-Wide model. In the wide model, all the
convolutional and fully-connected layers are twice as wide
as the corresponding layer in the base model. We see that
the ratio of vulnerable parameters is almost the same for
both models: 50.2% vs 50.0%. Further, experiments on the
CIFAR10-B-Slim and CIFAR10-B—twice as wide as the slim
model—produce consistent results: 46.7% and 46.8%. We
conclude that the number of vulnerable parameters grows
proportionally with the DNN’s width and, as a result, the ratio
of vulnerable parameters remains constant at around 50%.
Impact of the activation function. Next, we explore
whether the choice of activation function affects the vulner-
ability. Previously, we showed that ReLU can neutralize the
effects of large negative parameters caused by a bit-ﬂip; thus,
we experiment on different activation functions that allow neg-
ative outputs, e.g., PReLU [21], LeakyReLU, or RReLU [68].
These ReLU variants have been shown to improve the training
performance and the accuracy of a DNN. In this experiment,
we train the MNIST-B-PReLU model; which is exactly the
same as the MNIST-B model, except that it replaces ReLU
with PReLU. Figure 3 presents the layer-wise number of
Figure 1: The impact of the bit position. The number of
vulnerable parameters in bit positions 32nd to 24th.
experiments, we examine all bit positions and we observe that
bit positions other than the exponents mostly do not lead to
signiﬁcant damage; therefore, we only consider the exponent
bits. We ﬁnd that the exponent bits, especially the 31st-bit,
lead to indiscriminate damage. The reason is that a bit-ﬂip in
the exponents causes to a drastic change of a parameter value,
whereas a ﬂip in the mantissa only increases or decreases the
value by a small amount—[0,1]. We also observe that ﬂipping
the 30th to 28th bits is mostly inconsequential as these bits,
in the IEEE754 representation, are already set to one for most
values a DNN parameter usually takes—[3.0517× 10−5, 2].
Impact of the ﬂip direction. We answer which direction of
the bit-ﬂip, (0→1) or (1→0), leads to greater indiscriminate
damage. In Table 2, we report the number of effective bit-
ﬂips, i.e., those that inﬂict [RAD > 0.1] for each direction,
on 3 MNIST and 2 CIFAR10 models. We observe that only
(0→1) ﬂips cause indiscriminate damage and no (1→0) ﬂip
leads to vulnerability. The reason is that a (1→0) ﬂip can
only decrease a parameter’s value, unlike a (0→1) ﬂip. The
values of model parameters are usually normally distributed—
N(0,1)—that places most of the values within [-1,1] range.
Therefore, a (1→0) ﬂip, in the exponents, can decrease the
magnitude of a typical parameter at most by one; which is not
a strong enough change to inﬂict critical damage. Similarly, in
the sign bit, both (0→1) and (1→0) ﬂips cannot cause severe
damage because they change the magnitude of a parameter at
most by two. On the other hand, a (0→1) ﬂip, in the exponents,
can increase the parameter value signiﬁcantly; thus, during
the forward-pass, the extreme neuron activation caused by the
corrupted parameter overrides the rest of the activations.
Direction
Models (M: MNIST, C: CIFAR10)
(32-24th bits) M-B M-PReLU M-L5
C-B
C-AlexNet
0→1
1→0
Total
11,019
0
21,711
0
28,902
0
314,768
0
1,185,964
0
11,019
21,711
28,902
314,768
1,185,964
Table 2: The impact of the ﬂip direction. The number of
effective bit-ﬂips in 3 MNIST and 2 CIFAR10 models.
502    28th USENIX Security Symposium
USENIX Association
MNIST-L5CIFAR10-AlexNet# of Vulnerable Parameters1102104106Bit Position (32 - 24th)323130292827262524Figure 2: The impact of the parameter sign. The number of vulnerable positive and negative parameters, in each layer of
MNIST-L5.
Figure 3: The impact of the activation function. The number of vulnerable positive and negative parameters, in each layer of
MNIST-PReLU.
vulnerable positive and negative parameters in MNIST-B-
PReLU. We observe that using PReLU causes the negative
parameters to become vulnerable and, as a result, leads to a
DNN approximately twice as vulnerable as the one that uses
ReLU—50.2% vs. 99.2% vulnerable parameters.
Figure 4: The impact of the dropout and batch normal-
ization. The distributions of the parameter values of three
CIFAR10 models variants.
Impact of dropout and batch normalization. We con-
ﬁrmed that successful bit-ﬂip attacks increase a parameter’s
value drastically to cause indiscriminate damage. In conse-
quence, we hypothesize that common techniques that tend to
constrain the model parameter values to improve the perfor-
mance, e.g., dropout [52] or batch normalization [24], would
result in a model more resilient to single bit-ﬂips. Besides the
base CIFAR10 and MNIST models, we train the B-Dropout
and B-DNorm models for comparison. In B-Dropout models,
we apply dropout before and after the ﬁrst fully-connected
layers; in B-DNorm models, in addition to dropout, we also
apply batch normalization after each convolutional layer. In
Figure 4, we compare our three CIFAR10 models and show
how dropout and batch normalization have the effect of re-
ducing the parameter values. However, when we look into the
vulnerability of these models, we surprisingly ﬁnd that the vul-
nerability is mostly persistent regardless of dropout or batch
normalization—with at most 6.3% reduction in vulnerable
parameter ratio over the base network.
Impact of the model architecture. Table 1 shows that the
vulnerable parameter ratio is mostly consistent across differ-
ent DNN architectures. However, we see that the InceptionV3
model for ImageNet has a relatively lower ratio—40.8%—
compared to the other models—between 42.1% and 48.9%.
We hypothesize that the reason is the auxiliary classiﬁers in
the InceptionV3 architecture that have no function at test-time.
To conﬁrm our hypothesis, we simply remove the parame-
ters in the auxiliary classiﬁers; which bring the vulnerability
ratio closer to the other models—46.5%. Interestingly, we
also observe that the parameters in batch normalization lay-
ers are resilient to a bit-ﬂip: corrupting running_mean and
running_var cause negligible damage. In consequence, ex-
USENIX Association
28th USENIX Security Symposium    503
Positive ParamsPositive Vuln. ParamsNegative ParamsNegative Vuln. Params# of Parameters (Log)1101102103104Layer Names (C: convolutional, F: fully-connected, W: weight, B: bias)C1.WC1.BC2.WC2.BC3.WC3.BF1.WF1.BF2.WF2.BPositive ParamsPositive Vuln. ParamsNegative ParamsNegative Vuln. Params# of Parameters (Log)1101102103104Layer Names (C: convolutional, F: fully-connected, W: weight, B: bias)C1.WC1.BC2.WC2.BF1.WF1.BF2.WF2.BCIFAR10-Base N(0.001, 0.042)CIFAR10-Drop N(-0.002, 0.040)CIFAR10-BNorm N(-0.001, 0.039)ProbabilityModel Parameter Values−0.3−0.2−0.100.10.20.3Figure 5: The security threat in a transfer learning scenario. The victim model—student—that is trained by transfer learning
is vulnerable to the surgical attacker, who can see the parameters the victim has in common with the teacher model.
cluding the parameters in InceptionV3’s multiple batch nor-
malization layers leads to a slight increase in vulnerability—
by 0.02%.
Implications for the Adversaries
4.5
In Sec 3, we deﬁned four attack scenarios: the blind and sur-
gical attackers, in the black-box and white-box settings. First,
we consider the strongest attacker: the surgical, who can ﬂip a
bit at a speciﬁc memory location, white-box, with the model
knowledge for anticipating the impact of ﬂipping the said bit.
To carry out the attack, this attacker identiﬁes: 1) how much
indiscriminate damage, the RAD goal, she intends to inﬂict,
2) a vulnerable parameter that can lead to the RAD goal, 3) in
this parameter, the bit location, e.g., 31st-bit, and the ﬂip di-
rection, e.g., (0→1), for inﬂicting the damage. Based on our
[RAD > 0.1] criterion, approximately 50% of the parameters
are vulnerable in all models; thus, for this goal, the attacker
can easily achieve 100% success rate. For more severe goals
[0.1 ≤ RAD ≤ 0.9], our results in Appendix B suggest that
the attacker can still ﬁnd vulnerable parameters. In Sec 5.1,
we discuss the necessary primitives, in a practical setting, for
this attacker.
For a black-box surgical attacker, on the other hand, the
best course of action is to target the 31st-bit of a parameter.
This strategy maximizes the attacker’s chance of causing in-
discriminate damage, even without knowing what, or where,
the corrupted parameter is. Considering, the VGG16 model
for ImageNet, the attack’s success rate is 42.1% as we report
in Table 1; which is an upper-bound for the black-box attack-
ers. For the weakest—black-box blind—attacker that cannot
speciﬁcally target the 31st-bit, we conservatively estimate the
lower-bound as 42.1% / 32-bits = 1.32%; assuming only the
31st-bits lead to indiscriminate damage. Note that the success
rate for the white-box blind attacker is still 1.32% as acting
upon the knowledge of the vulnerable parameters requires an
attacker to target speciﬁc parameters. In Sec 5.2, we evaluate
the practical success rate of a blind attacker.
4.6 Distinct Attack Scenarios
In this section, other than causing indiscriminate damage, we
discuss two distinct attack scenarios single bit-ﬂips might
enable: transfer learning and targeted misclassiﬁcation.
Transfer learning scenario. Transfer learning is a com-
mon technique for transferring the knowledge in a pre-trained
teacher model to a student model; which, in many cases,
outperforms training a model from scratch. In a practical
scenario, a service provider might rely on publicly available
teacher as a starting point to train commercial student mod-
els. The teacher’s knowledge is transferred by freezing some
of its layers and embedding them into the student model;
which, then, trains the remaining layers for its own task. The
security risk is that, for an attacker who knows the teacher
but not the student, a black-box attack on the student might
escalate into a white-box attack on the teacher’s frozen lay-
ers. The attacker ﬁrst downloads the pre-trained teacher from
the Internet. She then loads the teacher into the memory and
waits for the deduplication [66] to happen. During deduplica-
tion, the memory pages with the same contents—the frozen
layers—are merged into the shared pages between the victim
and attacker. In consequence, a bit-ﬂip in the attacker’s pages
can also affect the student model in the victim’s memory.
We hypothesize that a surgical attacker, who can identify the
teacher’s vulnerable parameters and trigger bit-ﬂips in these
parameters, can cause indiscriminate damage to the student
model. In our experiments, we examine two transfer learning
tasks in [63]: the trafﬁc sign (GTSRB) [53] and ﬂower recog-
nition (Flower102) [41]. We initialize the student model by
transferring ﬁrst ten frozen layers of the teacher—VGG16 or
ResNet50 on ImageNet. We then append a new classiﬁcation
layer and train the resulting student network for its respective
task by only updating the new unfrozen layer. We corrupt the
1,000 parameters sampled from each layer in the teacher and
monitor the damage to the student model. Figure 5 reports our
results: we ﬁnd that all vulnerable parameters in the frozen
layers and more than a half in the re-trained layers are shared
by the teacher and the student.
504    28th USENIX Security Symposium
USENIX Association
Teacher (Vuln. Parameters)Student (Vuln. Parameters)Frozen Parameters (Shared between the Teacher and Student)Re-trained Parameters (Separate for Both)# of Vuln. Parameters05001000Layer Names (C: convolutional, F: fully-connected, W: weight, B: bias)C1.WC1.BC2.WC2.BC3.WC3.BC4.WC4.BC5.WC5.BC6.WC6.BC7.WC7.BC8.WC8.BC9.WC9.BC10.WC10.BC11.WC11.BC12.WC12.BC13.WC13.BF1.WF1.BF2.WF2.BFigure 6: The vulnerable parameters for a targeted attack in 3 DNN models. Each cell reports the number of bits that lead
to the misclassiﬁcation of a target sample, whose original class is given by the x-axis, as the target class, which is given by the
y-axis. From left to right, the models are MNIST-B, MNIST-L5 and CIFAR10-AlexNet.
Targeted misclassiﬁcation. Although our main focus is
showing DNNs’ graceless degradation, we conduct an ad-
ditional experiment to see whether a single bit-ﬂip primitive
could be used in the context of targeted misclassiﬁcation at-
tacks. A targeted attack aims to preserve the victim model’s
overall accuracy while causing the network to misclassify a
speciﬁc target sample into the target class. We experiment
with a target sample from each class in MNIST or CIFAR10—