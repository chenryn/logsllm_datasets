certiﬁed defenses modify the neural network training process
to minimize the number of robustness violations [65], [52],
[12]. These approaches, though promising, do not yet scale
to larger networks like Google Inception [65], [52]. In
fact, all published certiﬁed defenses have been evaluated
on small models and datasets [65], [52], [12], [43], and
at least in one case, the authors directly acknowledge that
some components of their defense would be “completely
infeasible” on ImageNet [65]. A recent paper [16] presents
a certiﬁed defense evaluated on the CIFAR-10 dataset [33]
for multi-layer DNNs (but smaller than ResNets). Their
approach is completely different from ours and, based on
the current results we see no evidence that it can readily
scale to large datasets like ImageNet.
Another approach [53] combines robust optimization and
adversarial training in a way that gives formal guarantees and
has lower computational complexity than previous robust
optimization work, hence it has the potential to scale better.
This approach requires smooth DNNs (e.g., no ReLU or max
pooling) and robustness guarantees are over the expected
loss (e.g.,
log loss), whereas PixelDP can certify each
speciﬁc prediction, and also provides intuitive metrics like
robust accuracy, which is not supported by [53]. Finally,
unlike PixelDP, which we evaluated on ﬁve datasets of
increasing size and complexity, this technique was evaluated
only on MNIST, a small dataset that is notoriously amenable
to robust optimization (due to being almost black and white).
Since the effectiveness of all defenses depends on the model
and dataset, it is hard to conclude anything about how well
it will work on more complex datasets.
Second, several works seek to formally verify [26], [30],
[61], [62], [15], [20], [58] or lower bound [49], [63] the
robustness of pre-trained ML models against adversarial
attacks. Some of these works scale to large networks [49],
[63], but they are insufﬁcient from a defense perspective as
they provide no scalable way to train robust models.
Differentially Private ML. Signiﬁcant work focuses on
making ML algorithms DP to preserve the privacy of training
sets [40], [1], [9]. PixelDP is orthogonal to these works,
differing in goals, semantic, and algorithms. The only thing
we share with DP ML (and most other applied DP literature)
are DP theory and mechanisms. The goal of DP ML is to
learn the parameters of a model while ensuring DP with
respect to the training data. Public release of model param-
eters trained using a DP learning algorithm (such as DP
empirical risk minimization or ERM) is guaranteed to not
reveal much information about individual training examples.
PixelDP’s goal is to create a robust predictive model where
a small change to any input example does not drastically
change the model’s prediction on that example. We achieve
this by ensuring that the model’s scoring function is a DP
function with respect to the features of an input example
(eg, pixels). DP ML algorithms (e.g., DP ERM) do not
necessarily produce models that satisfy PixelDP’s semantic,
and our training algorithm for producing PixelDP models
does not ensure DP of training data.
Previous DP-Robustness Connections. Previous work stud-
ies generalization properties of DP [4]. It is shown that
learning algorithms that satisfy DP with respect
to the
training data have statistical beneﬁts in terms of out-of-
sample performance; or that DP has a deep connection
to robustness at the dataset level [14], [17]. Our work is
rather different. Our learning algorithm is not DP; rather,
the predictor we learn satisﬁes DP with respect to the atomic
units (e.g., pixels) of a given test point.
VII. Conclusion
We demonstrated a connection between robustness against
adversarial examples and differential privacy theory. We
showed how the connection can be leveraged to develop a
certiﬁed defense against such attacks that is (1) as effective
at defending against 2-norm attacks as today’s state-of-the-
art best-effort defense and (2) more scalable and broadly
applicable to large networks compared to any prior certiﬁed
defense. Finally, we presented the ﬁrst evaluation of a
certiﬁed 2-norm defense on the large-scale ImageNet dataset.
In addition to offering encouraging results, the evaluation
highlighted the substantial ﬂexibility of our approach by
leveraging a convenient autoencoder-based architecture to
make the experiments possible with limited resources.
VIII. Acknowledgments
We thank our shepherd, Abhi Shelat, and the anonymous
reviewers, whose comments helped us improve the paper
signiﬁcantly. This work was funded through NSF CNS-
1351089, CNS-1514437, and CCF-1740833, ONR N00014-
17-1-2010, two Sloan Fellowships, a Google Faculty Fel-
lowship, and a Microsoft Faculty Fellowship.
References
[1] M. Abadi, A. Chu, I. Goodfellow, H. Brendan McMahan,
I. Mironov, K. Talwar, and L. Zhang. Deep Learning with
(cid:23)(cid:23)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
Differential Privacy. ArXiv e-prints, 2016.
[2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients
give a false sense of security: Circumventing defenses to
adversarial examples. 2018.
[3] A. Athalye and I. Sutskever. Synthesizing robust adversarial
examples. arXiv preprint arXiv:1707.07397, 2017.
[4] R. Bassily, K. Nissim, A. Smith, T. Steinke, U. Stemmer, and
J. Ullman. Algorithmic stability for adaptive data analysis. In
Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing, 2016.
[5] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner,
B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller,
J. Zhang, X. Zhang, J. Zhao, and K. Zieba. End to end
learning for self-driving cars. CoRR, 2016.
[6] N. Carlini and D. Wagner. Adversarial examples are not easily
detected: Bypassing ten detection methods.
In Proceedings
of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security. ACM, 2017.
[7] N. Carlini and D. A. Wagner.
robustness of neural networks.
on Security and Privacy (SP), 2017.
Towards evaluating the
In 2017 IEEE Symposium
[8] K. Chatzikokolakis, M. E. Andr´es, N. E. Bordenabe, and
C. Palamidessi. Broadening the scope of differential privacy
using metrics.
In International Symposium on Privacy En-
hancing Technologies Symposium, 2013.
[9] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differen-
tially private empirical risk minimization. J. Mach. Learn.
Res., 2011.
[10] Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van
der Maaten. Countering adversarial
images using input
transformations. International Conference on Learning Rep-
resentations, 2018.
[11] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan
Yuille. Mitigating adversarial effects through randomization.
International Conference on Learning Representations, 2018.
[12] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and
N. Usunier.
Parseval networks: Improving robustness to
adversarial examples. In Proceedings of the 34th International
Conference on Machine Learning, 2017.
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, 2009.
[14] C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. Rubinstein.
Bayesian Differential Privacy through Posterior Sampling.
arXiv preprint arXiv:1306.1066v5, 2016.
[15] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output
range analysis for deep feedforward neural networks. In NASA
Formal Methods Symposium, 2018.
[16] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
B. O’Donoghue, J. Uesato, and P. Kohli. Training veriﬁed
learners with learned veriﬁers. ArXiv e-prints, 2018.
(cid:23)(cid:23)(cid:26)
[17] C. Dwork and J. Lei. Differential privacy and robust statistics.
In Proceedings of the forty-ﬁrst annual ACM symposium on
Theory of computing, 2009.
[18] C. Dwork, A. Roth, et al. The algorithmic foundations of
differential privacy. Foundations and Trends R(cid:5) in Theoretical
Computer Science, 2014.
[19] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li,
A. Prakash, A. Rahmati, and D. Song. Robust physical-
world attacks on machine learning models. arXiv preprint
arXiv:1707.08945, 2017.
[20] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov,
S. Chaudhuri, and M. Vechev. Ai 2: Safety and robustness
certiﬁcation of neural networks with abstract interpretation.
In IEEE Symposium on Security and Privacy (SP), 2018.
[21] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In Proceedings of the 3rd
ICLR, 2015.
[22] Google. Inception v3. https://github.com/tensorﬂow/models/
tree/master/research/inception. Accessed: 2018.
[23] Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D.
Bernstein, Jean Kossaiﬁ, Aran Khanna, Zachary C. Lipton,
Animashree Anandkumar. Stochastic activation pruning for
robust adversarial defense.
International Conference on
Learning Representations, 2018.
[24] D. Hendrycks and K. Gimpel. Early methods for detecting
adversarial images. In ICLR (Workshop Track), 2017.
[25] W. Hoeffding. Probability inequalities for sums of bounded
random variables. Journal of the American statistical associ-
ation, 1963.
[26] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety
veriﬁcation of deep neural networks. In Proceedings of the
29th International Conference on Computer Aided Veriﬁca-
tion, 2017.
[27] A. Ilyas, A. Jalal, E. Asteri, C. Daskalakis, and A. G.
Dimakis. The robust manifold defense: Adversarial training
using generative models. CoRR, abs/1712.09196, 2017.
[28] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, 2015.
[29] Jacob Buckman, Aurko Roy, Colin Raffel, Ian Goodfellow.
Thermometer encoding: One hot way to resist adversarial
examples.
International Conference on Learning Represen-
tations, 2018.
[30] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J.
Kochenderfer. Reluplex: An efﬁcient SMT solver for veri-
fying deep neural networks. CoRR, 2017.
[31] J. Kos, I. Fischer, and D. Song. Adversarial examples for
generative models. arXiv preprint arXiv:1702.06832, 2017.
[32] Kos, Jernej and Song, Dawn. Delving into adversarial attacks
on deep policies. arXiv preprint arXiv:1705.06452, 2017.
[33] A. Krizhevsky. Learning multiple layers of features from tiny
images. 2009.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
[34] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial
examples in the physical world. arXiv preprint 1607.02533,
2016.
[35] X. Liu, M. Cheng, H. Zhang, and C. Hsieh. Towards robust
neural networks via random self-ensemble. Technical report,
2017.
[36] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to worry
about adversarial examples in object detection in autonomous
vehicles. CVPR, 2017.
[37] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
Towards deep learning models resistant to adversarial attacks.
CoRR, abs/1706.06083, 2017.
[38] Madry Lab. CIFAR-10 Adversarial Examples Challenge.
https://github.com/MadryLab/cifar10 challenge. Accessed:
1/22/2017.
[39] A. Maurer and M. Pontil. Empirical bernstein bounds and
sample-variance penalization.
In COLT 2009 - The 22nd
Conference on Learning Theory, Montreal, Quebec, Canada,
June 18-21, 2009, 2009.
[40] F. McSherry and I. Mironov. Differentially private rec-
ommender systems: Building privacy into the netﬂix prize
contenders.
the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining, 2009.
In Proceedings of
[41] D. Meng and H. Chen. Magnet: A two-pronged defense
against adversarial examples. In CCS, 2017.
[50] K. Pei, Y. Cao, J. Yang, and S. Jana. Deepxplore: Automated
whitebox testing of deep learning systems. In Proceedings of
the 26th Symposium on Operating Systems Principles (SOSP),
2017.
[51] Pouya Samangouei, Maya Kabkab, Rama Chellappa.
Defense-GAN: Protecting classiﬁers against adversarial at-
tacks using generative models. International Conference on
Learning Representations, 2018.
[52] A. Raghunathan, J. Steinhardt, and P. Liang.
defenses against adversarial examples.
arXiv:1801.09344, 2018.
Certiﬁed
arXiv preprint
[53] A. Sinha, H. Namkoong, and J. Duchi. Certifying Some Dis-
tributional Robustness with Principled Adversarial Training.
2017.
[54] Y. Song, R. Shu, N. Kushman, and S. Ermon. Generative
adversarial examples. 2018.
[55] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2016.
[56] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In Proceedings of the 2nd International Conference
on Learning Representations, 2014.
[57] Tensorﬂow r1.5.
Resnet models.
https://github.com/
tensorﬂow/models/tree/r1.5/research/resnet, 2017.
[42] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On
detecting adversarial perturbations. In Proceedings of the 6th
International Conference on Learning Representations, 2017.
[58] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness
of neural networks with mixed integer programming. arXiv
preprint arXiv:1711.07356, 2017.
[43] M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract
interpretation for provably robust neural networks. In Inter-
national Conference on Machine Learning (ICML), 2018.
[59] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. D. Mc-
Daniel. Ensemble adversarial training: Attacks and defenses.
CoRR, abs/1705.07204, 2017.
[44] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and
A. Y. Ng. Reading digits in natural images with unsupervised
feature learning.
[45] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations against
deep neural networks.
In Proc. of IEEE Symposium on
Security and Privacy (Oakland), 2016.
[60] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.
Manzagol. Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising
criterion. J. Mach. Learn. Res., 2010.
[61] S. Wang, K. Pei, W. Justin, J. Yang, and S. Jana. Efﬁcient