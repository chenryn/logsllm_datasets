for patient in [0..3) {
for position in [0..2) {
if (nucl[patient][position] == A) {
sum += 1;
} } }
if (sum == 5 || sum == 6) {
// outputs that the sum is either 5 or 6
return pick([5, 6]);
} else {
// outputs the exact sum
return sum;
}
}
Modified program
Figure 3: Modified program that satisfies the privacy policy
for the given attacker belief (see Figure 2). The unchanged
code is grayed out to highlight the synthesized enforcement
(Lines 9-15). The enforcement conflates outputs 5 and 6.
our motivating example. We formalize our notion of enforcement
in Section 4.3 and prove its completeness in Section 4.4.2.
In our example, Bob’s privacy policy
Synthesized Enforcement.
is violated if the program given in Figure 2(a) returns the output 6.
To avoid this behavior, our Synthesis of PermissIve pRivacy En-
forcement (Spire) system synthesizes an enforcement that conflates
the outputs 5 and 6 and makes them equally likely for each input.
In Figure 3, we show the resulting program which is obtained by
modifying the return statement of the original program (given in
Figure 2(a)). Lines 9-15 in the new program illustrate the synthesized
enforcement. Note that whenever the sum of adenine nucleotides is
5 or 6, the new program’s output is 5 with probability 0.5 and 6 with
probability 0.5. This behavior is implemented using the expression
pick([5, 6]) at Line 11. For all remaining sums of adenine the new
program outputs the exact number.
The new modified program satisfies Bob’s privacy policy for
all outputs. In particular, for the output 6, the probability of the
event nucl[Bob] == [A,A] is 0.56, which is below the bound of 0.75
defined in the privacy policy. We remark that the synthesis step is
independent of the secret and therefore does not leak any additional
information about the secret.
Challenge. A trivial solution to enforcing the policy is to conflate
all outputs. All outputs then leak no information about the secret. A
key challenge is thus to synthesize an optimal enforcement, i.e. one
that conflates as few outputs as possible. In our example, outputs
0, . . . , 4 are not merged together with other outputs. We formal-
ize two notions of optimality, called permissiveness and answer-
precision, in Section 4.2 and discuss the complexity of finding an
optimally permissive enforcement with respect to these notions in
Section 4.4. We reduce the synthesis problem to a linear optimiza-
tion over SMT constraints in Section 5 and give an efficient greedy
heuristic that runs in quadratic time in Section 6.
Attacker Model. We consider an attacker who: (i) can select any
program (deterministic or probabilistic) that takes the secret as
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA393input, and can ask the system to run it, (ii) can observe the output
returned by the system, (iii) knows the privacy policy and the
synthesis algorithm used to enforce it. Our synthesis algorithms
are deterministic, and so assumption (iii) implies that the attacker
knows that the system provides an output produced by running
the program in Figure 3.
We work with probabilistic programs, as queries often add ran-
dom noise to the output; e.g. see the examples in [47]. Furthermore,
the computation in many privacy-relevant settings is probabilistic.
Deterministic programs are a special case.
We assume that the attacker belief is known. This distribution
captures common knowledge about the secret and is usually pub-
licly available. For instance, the frequency of nucleotides in human
genes can be found in public databases [1]. We remark that having
an attacker belief (or a set of beliefs) is necessary when it comes
to enforcing privacy policies formulated as bounds on the attacker
belief [37]. This is because no enforcement can satisfy a non-trivial
policy (i.e. a policy that is not satisfied by all programs) for all
possible attacker beliefs.
The attacker can iteratively ask the system to run multiple pro-
grams against the secret. Enforcing the privacy policy in this setting
requires tracking the attacker belief as outputs are revealed. We
describe how the Spire system handles this iterative setting in Sec-
tion 7, and in Section 8.2.4 we present experiments to evaluate the
decrease in permissiveness of the synthesized enforcement after
each iteration.
Security Applications. Our synthesis approach can be used to
restrict how much attackers can learn about sensitive data in nu-
merous practical scenarios. It can be used, for instance, to enforce
security properties such as opacity [47], k-anonymity [45, 50], and
knowledge-based security [37]. Note that these are general secu-
rity properties that are employed to enforce privacy in numerous
relevant application domains. Examples include privacy in anony-
mous communication networks [49], genomic data privacy [2, 30],
and privacy of location-based services [34, 39, 51]. In Section 8, we
evaluate our implementation on examples related to genomic data,
location data, and personal information, that we adopted from the
literature. We discuss related work in Section 9.
3 PROBABILISTIC PRIVACY MODEL
In this section, we first introduce our notation and then present the
probabilistic privacy model.
Notation. Given two sets I and O, we write MI×O to denote the
set of all matrices over R whose rows and columns are indexed by
I and O, respectively. For a matrix m ∈ MI×O, we write m(o | i),
where i ∈ I and o ∈ O, to denote the element in row i and column o.
We denote the set of all probability distributions over a set S
by D(S). A random variable X : Ω → X is a measurable function
associating to each outcome ω ∈ Ω a value X(ω) ∈ X, where X is
a measurable space. Given a random variable X : Ω → X and a
value x ∈ X, we denote the event {ω ∈ Ω | X(ω) = x} by X = x.
Given an equivalence relation ξ ⊆ S × S over a set S, we write
[s]ξ = {s′ ∈ S | (s, s′) ∈ ξ} for the equivalence class of an ele-
ment s ∈ S according to ξ, and we denote the quotient set of ξ by
S/ξ = {[s]ξ | s ∈ S}.
i ∈ I, we have
Probabilistic Programs. Let I and O be finite input and out-
put sets, respectively. Following [13, 48], we define a probabilistic
program as a stochastic matrix π ∈ MI×O where for each input
o∈O π(o | i) = 1. The element π(o | i) denotes the
probability that the program outputs o ∈ O given the input i ∈ I.
That is, for each input i ∈ I, the program defines a distribution
over the outputs O.
Secret and Attacker Belief. A secret is a value i ∈ I from the
set of inputs. For example, if I is the set of all possible nucleotide
sequences in a given gene, then the secret is a particular nucleotide
sequence.
An attacker belief δ ∈ D(I) is a distribution over the inputsI [14,
37]. The attacker belief captures the attacker’s view on the likeli-
hood that a particular value i ∈ I is the secret; i.e., the attacker
belief of the secret i ∈ I is δ(i).
Bayesian Inference. The attacker can ask the system to run a
program π that takes the secret as input. She observes the pro-
gram’s output and revises her belief about the secret based on the
observed output, as described in Section 2.3. We rely on Bayesian
conditioning to revise an attacker belief given the observed output.
We now capture the above notions with a probability space.
Given a probabilistic program π ∈ MI×O and an attacker belief δ ∈
D(I), we construct the probability space (Ω, F , Pπ
δ ) with a sample
space Ω = I × O, a set of events F = P(Ω), and a probability
δ (i, o) = δ(i) · π(o | i). In our overview example, the set
measure Pπ
of inputs is I = {A, G}6, where each six-tuple identifies the pairs of
nucleotides of Alice, Bob, and Carol, and the set of outputs is O =
{0, . . . , 6}. We represent the program’s inputs and outputs with the
random variables I : Ω → I and O : Ω → O, respectively, where
I(i, o) = i and O(i, o) = o. Note that for a probabilistic program π,
δ (O = o | I = i) = π(o | i). Furthermore, for an attacker
we have Pπ
belief δ, we have Pπ
After observing an output o, the attacker revises her belief δ to
δ′, where the revised belief δ′ is given by the distribution δ′(i |
o) = Pπ
δ (I = i | O = o). This distribution is computed using the
Bayes rule as follows:
δ (I = i) = δ(i).
δ (I = i | O = o) =
Pπ
=
δ (I = i)
δ (O = o | I = i) · Pπ
Pπ
δ (O = o)
Pπ

π(o | i) · δ(i)
i∈I π(o | i) · δ(i)
Privacy Policies. A belief bound is a pair φ = (S,[a, b]) where
S ⊆ I is a subset of inputs I and [a, b] is an interval such that
a, b ∈ Q and 0 ≤ a ≤ b ≤ 1 [37]. Given a program π, an attacker
belief δ, and a belief bound φ = (S,[a, b]), we say that π satisfies φ
for δ, denoted by π , δ |= φ, if for all outputs o ∈ O, we have
δ (I ∈ S | O = o) ∈ [a, b]
Pπ
That is, for any output o ∈ O the program may return, the revised
attacker belief (after observing o) about the predicate S must be
within the bounds a and b. We remark that the program must satisfy
the security assertion for all program outputs in order to allow an
attacker to run the program. For further details, see [37]. Note that
the definition generalizes the notion of opacity to the probabilistic
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA3940
0
0
.
.
.
1
1
0
0
.
.
.
0
AA AA AA
AA AA AG
.
.
.
GG GG GG
Outputs O
2
0
0
.
.
.
0
3
0
0
.
.
.
0
4
0
0
.
.
.
0
5
0
1
.
.
.
0
6
1
0
.
.
.
0
AA AA AA
AA AA AG
.
.
.
GG GG GG
(a) Program π
Outputs O/ξ
{0} {1} {2} {3} {4} {5, 6}
0
0
.
.
.
1
0
0
.
.
.
0
(b) Program Enf(π, ξ)
0
0
.
.
.
0
0
0
.
.
.
0
0
0
.
.
.
0
1
1
.
.
.
0
Outputs O
5
3
0.5
0
0
0.5
.
.
.
.
.
.
0
0
(c) Program EnfRnd(π, ξ)
0
0
0
.
.
.
1
1
0
0
.
.
.
0
2
0
0
.
.
.
0
4
0
0