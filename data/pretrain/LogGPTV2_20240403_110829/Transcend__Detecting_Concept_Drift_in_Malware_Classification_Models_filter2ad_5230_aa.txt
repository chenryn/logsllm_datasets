title:Transcend: Detecting Concept Drift in Malware Classification Models
author:Roberto Jordaney and
Kumar Sharad and
Santanu Kumar Dash and
Zhi Wang and
Davide Papini and
Ilia Nouretdinov and
Lorenzo Cavallaro
Transcend: Detecting Concept Drift in  
Malware Classification Models
Roberto Jordaney, Royal Holloway, University of London; Kumar Sharad, NEC Laboratories 
Europe; Santanu K. Dash, University College London; Zhi Wang, Nankai University;  
Davide Papini, Elettronica S.p.A.; Ilia Nouretdinov and Lorenzo Cavallaro,  
Royal Holloway, University of London
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jordaney
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXTranscend: Detecting Concept Drift in Malware Classiﬁcation Models
Roberto Jordaney+, Kumar Sharad ∗§, Santanu Kumar Dash ∗‡, Zhi Wang ∗†, Davide Papini ∗•,
Ilia Nouretdinov+, and Lorenzo Cavallaro+
+Royal Holloway, University of London
§NEC Laboratories Europe
‡University College London
†Nankai University
•Elettronica S.p.A.
Abstract
Building machine learning models of malware behav-
ior is widely accepted as a panacea towards effective
malware classiﬁcation. A crucial requirement for build-
ing sustainable learning models, though, is to train on a
wide variety of malware samples. Unfortunately, mal-
ware evolves rapidly and it thus becomes hard—if not
impossible—to generalize learning models to reﬂect fu-
ture, previously-unseen behaviors. Consequently, most
malware classiﬁers become unsustainable in the long
run, becoming rapidly antiquated as malware contin-
ues to evolve.
In this work, we propose Transcend, a
framework to identify aging classiﬁcation models in vivo
during deployment, much before the machine learning
model’s performance starts to degrade. This is a signiﬁ-
cant departure from conventional approaches that retrain
aging models retrospectively when poor performance is
observed. Our approach uses a statistical comparison
of samples seen during deployment with those used to
train the model, thereby building metrics for prediction
quality. We show how Transcend can be used to iden-
tify concept drift based on two separate case studies on
Android and Windows malware, raising a red ﬂag before
the model starts making consistently poor decisions due
to out-of-date training.
1
Introduction
Building sustainable classiﬁcation models for classify-
ing malware is hard. Malware is mercurial and mod-
eling its behavior is difﬁcult. Codebases of commer-
cial signiﬁcance, such as Android, are frequently patched
against vulnerabilities and malware attacking such sys-
tems evolve rapidly to exploit new attack surfaces. Con-
sequently, models that are built through training on older
malware often make poor and ambiguous decisions when
∗Research carried out entirely while Post-Doctoral Researchers at
Royal Holloway, University of London.
faced with modern malware—a phenomenon commonly
known as concept drift.
In order to build sustainable
models for malware classiﬁcation, it is important to iden-
tify when the model shows signs of aging whereby it fails
to recognize new malware.
Existing solutions [12, 15, 23] aim to periodically re-
train the model. However, if the model is retrained too
frequently, there will be little novelty in the information
obtained to enrich the classiﬁer. On the other hand, a
loose retraining frequency leads to periods of time where
the model performance cannot be trusted. Regardless,
the retraining process requires manual labeling of all the
processed samples, which is constrained by available re-
sources. Once the label is acquired, traditional metrics
such as precision and recall are used to retrospectively
indicate the model performance. However, these metrics
do not assess the decision of the classiﬁer. For exam-
ple, hyperplane-based learning models (e.g., SVM) only
check the side of the hyperplane where the object lies
while ignoring its distance from the hyperplane. This is
a crucial piece of evidence to assess non-stationary test
objects that eventually lead to concept drift.
A well known approach for qualitative assessment of
decisions of a learning model is the probability of ﬁt of
test object in a candidate class. Previous work has re-
lied on using ﬁxed probability thresholds to identify best
matches [19]. Standard algorithms compute the proba-
bility of a sample ﬁtting into a class as a by-product of the
classiﬁcation process. However, since probabilities need
to sum up to 1.0, it is likely that for previously unseen
test objects which do not belong to any of the classes,
the probability may be artiﬁcially skewed. To mitigate
this issue, Deo et al. propose ad-hoc metrics derived from
the two probabilities output by Venn-Abers Predictors
(VAP) [5], one of which is perfectly calibrated. Although
promising, the approach is unfortunately still in its in-
fancy and does not reliably identify drifting objects (as
further elaborated in § 6).
The machine learning community has developed tech-
USENIX Association
26th USENIX Security Symposium    625
niques that look at objects statistically rather than prob-
abilistically. For example, Conformal Predictor [20]
makes predictions with statistical evidence. However, as
discussed by Fern and Dietterich1, this method is not tai-
lored to be used in presence of concept drift.
Nevertheless, statistical assessments seem to over-
come the limitations of probabilistic approaches, as out-
lined in § 2. Still, there are two key issues that need to
be addressed before statistical assessments can be used
to detect concept drift. First, the assessments have to
be agnostic to the algorithm used to build the learn-
ing model. This is non-trivial as different algorithms
can have different underlying classiﬁcation mechanisms.
Any assessment has to abstract away from the algorithm
and identify a universal criteria that treats the underly-
ing algorithm as a black box. Second, and more impor-
tantly, auto-computation of thresholds to identify an ag-
ing model from an abstract assessment criteria requires a
brute force search among scores for the training objects.
In this work, we address both these issues by propos-
ing both meaningful and sufﬁciently abstract assessment
metrics as well as an assessment criteria for interpret-
ing the metrics in an automated fashion. We propose
Transcend—a fully parametric statistical framework for
assessing decisions made by the classiﬁer to identify con-
cept drift. Central to our contribution, is the translation
of the decision assessment problem to a constraint opti-
mization problem which enables Transcend to be para-
metric with diverse operational goals.
It can be boot-
strapped with pre-speciﬁed parameters that tune its sen-
sitivity to varying levels of concept drift. For example,
in applications of critical importance, Transcend can be
pre-conﬁgured to adopt a strict ﬁltering policy for poor
and unreliable classiﬁcation decisions. While previous
work has looked at decision assessment [4, 19], this is
the ﬁrst work that looks at identifying untrustworthy pre-
dictions using decision assessment techniques. Thereby,
Transcend can be deployed in existing detection systems
with the aim of identifying aging models and ameliorat-
ing performance in the face of concept drift.
In a nutshell, we make the following contributions:
• We propose conformal evaluator (CE), an evalua-
tion framework to assess the quality of machine
learning tasks (§ 2). At the core of CE is the def-
inition of non-conformity measure derived from the
ML algorithm under evaluation (AUE) and feature
set (§ 2.1). This measure builds statistical metrics
to quantify the AUE quality and statistically support
goodness of ﬁt of a data point into a class according
to the AUE (§ 2.4).
1A. Fern and T. Dietterich. “Toward Explainable Uncertainty”.
https://intelligence.org/files/csrbai/fern-slides-1.pdf
• We build assessments on top of CE’s statistical met-
rics to evaluate the AUE design and understand sta-
tistical distribution of data to better capture AUE’s
generalization and class separations (§ 3).
• We present Transcend, a fully tunable classiﬁcation
system that can be tailored to be resilient against
concept drift to varying degrees depending on user
speciﬁcations. This versatility enables Transcend to
be used in a wide variety of deployment environ-
ments where the cost of manual analysis is central
to classiﬁcation strategies. (§ 3.3)
• We
assessments
show how CE’s
facilitate
Transcend to identify suitable statistical thresholds
to detect decay of ML performance in realistic
settings (§ 4).
In particular, we support our
ﬁndings with two case studies that show how
Transcend identiﬁes concept drift in binary (§ 4.1)
and multi-class classiﬁcation (§ 4.2) tasks.
2 Statistical Assessment: Why and How?
In this section we discuss the signiﬁcance of statistical
techniques for decision assessment, which form the core
of conformal evaluator. A statistical approach to deci-
sion assessment considers each decision in the context of
previously made decisions. This is different to a proba-
bilistic assessment where the metric is indicative of how
likely a test object is to belong to a class. In contrast,
statistical techniques answer the question: how likely is
the test object to belong to a class compared to all of its
other members? The contextual evidence produced by
statistical evidence is a step beyond standard probabilis-
tic evidence and typically gives stronger guarantees on
the quality of the assessment. Our work dissects Con-
formal Predictor (CP) [24] and extracts its sound statisti-
cal foundations to build conformal evaluator (CE). In the
following section, we provide further details, while we
forward the reader to § 6 for a full comparison between
CP and CE.
2.1 Non-conformity Measure
Classiﬁcation is usually based on a scoring function
which, given a test object z∗, outputs a prediction score
FD (l,z∗), where D is the dataset of training objects and
l is a label from the set of possible object labels L.
The scoring function can be used to measure the differ-
ence between a group of objects belonging to the same
class (e.g., malware belonging to the same family) and
In Transcend, the non-
a new object (i.e., a sample).
conformity measure (NCM) is computed directly from
the scoring function of the algorithm. Thus, conformal
626    26th USENIX Security Symposium
USENIX Association
(a) Elements above the threshold
(b) Performance of elements above the threshold
(c) Elements below the threshold
(d) Performance of elements below the threshold
Figure 1: Performance comparison between p-value and probability for the objects above and below the threshold used
to accept the algorithm’s decision. The p-values are given by CE with SVM as non-conformity measure, the probabil-
ities are given directly by SVM. As we can see from the graph, p-values tend to contribute to a higher performance of
the classiﬁer, identifying those (drifting) objects that would have been erroneously classiﬁed.
evaluation is agnostic to the algorithm, making it versa-
tile and compatible with multiple ML algorithms; it can
be applied on top of any classiﬁcation or clustering algo-
rithm that uses a score for prediction.
We note that some algorithms already have built-in
quality measures (e.g., the distance of a sample from the
hyperplane in SVM). However, these are algorithm spe-
ciﬁc and cannot be directly compared with other algo-
rithms. On the other hand, Transcend uniﬁes such quality
measures through uniform treatment of non-conformity
in an algorithm-agnostic manner.
2.2 P-values as a Similarity Metric
At
the heart of conformal evaluation is the non-
conformity measure—a real-valued function AD (C\z,z),
which tells how different an object z is from a set C. The
set C is a subset of the data space of object D. Due to the
real-valued range of non-conformity measure, conformal
evaluator can be readily used with a variety of machine
learning methods such as support-vector machines, neu-
ral networks, decision trees and Bayesian prediction [20]
and others that use real-valued numbers (i.e., a similarity
function) to distinguish objects. Such ﬂexibility enables
Transcend to assess a wide range of algorithms.
Conformal evaluation computes a notion of similarity
through p-values. For a set of objects K , the p-value
z∗ for an object z∗ is the proportion of objects in class
pC
K that are at least as dissimilar to other objects in C as
z∗. There are two standard techniques to compute the
p-values from K : Non-Label-Conditional (employed by
decision and alpha assessments outlined in § 3.1 and
§ 3.2), where K is equal to D, and Label-Conditional
(employed by the concept drift detection described in
§ 3.3), where K is the set of objects C with the same
label. The calculations for the non-conformity measures
for the test object and the set of objects in K is shown
in equation 1 and 2 respectively. The computation of p-
value for the test object is shown in equation 3.
αz∗ = AD (C,z∗)
∀i ∈ K .αi = AD (C\ zi,zi)
pC
z∗ =
|{ j : α j ≥ αz∗}|
|K |
(1)
(2)
(3)
P-values compute an algorithm’s credibility and con-
ﬁdence, crucial for decision assessments (§ 2.4).