0
3
2
1
0
(spare resources)
)
s
r
u
o
h
(
l
a
v
r
e
t
n
i
t
n
o
p
k
c
e
h
c
i
20
15
10
8
6
4
3
2
1.5
1
1
2
5
Load= 3200 units
Load= 1600 units
Load= 800 units
Load= 400 units
14000
12000
10000
8000
6000
4000
2000
y
t
i
l
i
b
a
l
i
a
v
A
r
o
f
t
s
o
C
l
a
u
n
n
A
l
a
n
o
i
t
i
d
d
A
s
e
c
r
u
o
s
e
r
f
o
r
e
b
m
u
n
200
100
50
20
10
5
2
20
10
100
job execution time (hours)
50
200
500
1
1000
0
0.1
1
10
100
Downtime (minutes)
1000
10000
Figure 7. Scientiﬁc Application Example. Opti-
mal design as function of execution time
Figure 8. Cost/availability/performance trade-
off for application tier example
creases the overhead in normal operation. Both types of over-
head increase job execution time. Thus the optimal conﬁgu-
ration is an intermediate value which balances the overhead
due to lost work at failures and the overhead during normal
operation. As the number of resources increases, the failure
rate increases and the overhead due to failures becomes more
important, causing the optimal checkpoint interval to be re-
duced.
Fig. 7 also shows the choice of the storage location used
to store the application state at each checkpoint. As expected,
for a small number of nodes the best option is to save the state
at a central storage location, since the other alternative, stor-
age on a peer node, has a higher overhead per node. But for a
large number of nodes, the central location becomes a bottle-
neck and storing the state on a peer node becomes more ef-
fective4.
5.3. Cost, performance and availability tradeoff
Although the curves shown in Fig. 6 and Fig. 7 enable
the selection of the optimal design for a given application re-
quirement, the knowledge of the cost associated with each de-
sign can help the user make cost/beneﬁt tradeoffs. Aved can
be used to generate plots of the costs of optimal designs at
various levels of availability and performance requirements.
Fig. 8 shows the cost associated with the optimal designs at
various levels of availability and performance requirements,
for the application tier example shown in Fig. 6. Each curve
shows, for a particular level of load, the additional annual
cost as a function of the required downtime. This is the ex-
tra annual cost necessary to provide the required availabil-
ity when compared to a minimum cost design that can sup-
4 We assumed that the central storage is based on a highly reliable system,
such as a high end RAID system, and does not fail.
port the same load when there is no availability requirement.
Fig. 8 reveals the tradeoffs among cost, availability, and per-
formance that must be understood to make a judicious design
choice. For example, in some cases a large improvement in
downtime can be achieved with a low additional cost. Alter-
natively, slightly relaxing the downtime requirement can sig-
niﬁcantly reduce the cost overhead for availability.
6. Related Work
The idea of automating the design and conﬁguration of
systems to meet user-speciﬁed availability requirements is
relatively recent. We are only aware of a few examples, each
of which is focused on a limited domain. The Oracle database
implements a function that automatically determines when
to ﬂush data and logs to persistent storage such that the re-
covery time after a failure is likely to meet a user-speciﬁed
bound [10]. Automated design has been proposed for stor-
age systems to meet user requirements for data dependability,
which encompasses both data availability and data loss [9].
Their approach is complementary and could be combined
with ours to obtain a comprehensive solution that designs sys-
tems based on performance, availability, and data protection
requirements.
Most other work on system automation for manag-
ing availability has been limited to automated monitoring
and automated response to failure events and other such trig-
gers. For example, cluster failover products such as HP
MC/Serviceguard [5] SunCluster [14] and Trucluster [6] de-
tect nodes that fail, automatically failover application com-
ponents to surviving nodes, and reintegrate failed nodes into
active service when they recover from failure. IBM Direc-
tor [1] detects resource exhaustion in its software compo-
nents and automates the rejuvenation of these components at
appropriate intervals. Various utility computing efforts un-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:53 UTC from IEEE Xplore.  Restrictions apply. 
derway will also automatically detect failed components
and automatically replace them with equivalent compo-
nents from a free pool [7][13][8].
7. Conclusions
In this paper, we have presented Aved, an automated sys-
tem design engine which determines the minimum cost de-
sign and conﬁguration of computing infrastructure that satis-
ﬁes service level performance and availability requirements.
Automating the design and conﬁguration of computing in-
frastructure improves the design process compared to a tra-
ditional manual approach, and will be particularly useful in
emerging self-managing utility computing environments.
To enable automatic exploration of the design space, we
proposed a model which can represent various options for de-
signing and conﬁguring the infrastructure. A primary chal-
lenge in developing our design space model and its speciﬁ-
cation approach was to satisfy competing goals of generality
and practicality. That is, the model should enable represen-
tation of a wide variety of system infrastructure choices and
service types. At the same time, the practicality of the model
should be preserved by enabling it to be speciﬁed at a con-
ceptual level that is close to the actual, real-life properties of
services and system building blocks. Our solution approach
is to deﬁne a structured model which uses a small number of
fundamental constructs (components, failure modes, mecha-
nisms, etc.) which are intuitive to map to real-life entities. We
carefully selected the attribute set for each construct, and de-
ﬁned clear dependencies between attributes in different con-
structs in the model which are related. This allows the con-
structs to be interconnected or composed in well deﬁned ways
to construct models describing a wide variety of complex sys-
tems. To demonstrate the applicability of this approach, we
presented examples that show the model can represent infras-
tructure design options for two completely different environ-
ments, an E-Commerce service and a scientiﬁc application.
Our examples additionally illustrate the need for auto-
mated design. Although the examples include only a small
number of conﬁguration parameters, they result in a large set
of possible designs. Moreover, the examples show that the op-
timal design can be different as the requirements change, for
example as the service’s throughput requirement is increased
as a result of an increase in client demand. Therefore, in self-
managing environments, an engine such as Aved is needed to
automatically reevaluate and reconﬁgure designs in response
to changes in such parameters.
Although Aved is a good ﬁrst step towards automated sys-
tem design, there are several remaining issues that need to be
addressed as future research. To address overall service avail-
ability, the design engine must examine the impact of the net-
work and storage subsystems. Thus we plan to extend Aved
to factor LAN topologies and network failures. We also plan
to integrate Aved with an automatic process for storage sys-
tem design and management for data dependability[9]. We
plan to integrate Aved with online mechanisms to continu-
ously monitor service performance and other infrastructure
attributes to dynamically reﬁne Aved’s models and to gener-
ate design changes in response to environment changes. This
would eliminate the need for precise initial performance mod-
els that may be difﬁcult to obtain and specify. Finally, we plan
to conduct case studies of real environments to identify addi-
tional design choices encountered in practice and to evaluate
what extensions would be needed in Aved to support them.
References
[1] V. Castelli, R. E. Harper, P. Heidelberger, S. W. Hunter, K. S.
Trivedi, K. Vaidyanathan, and W. P. Zeggert. Proactive man-
agement of software aging. IBM Journal of Research and De-
velopment, 45(2):311–332, March 2001.
[2] G. Clark, T. Courtney, D. Daly, D. Deavours, S. Derisavi, J. M.
Doyle, W. H. Sanders, and P. Webster. The M¨obius model-
ing tool. In 9th Int’l Workshop on Petri Nets and Performance
Models, pages 241–250, Sep 2001.
[3] I. Foster, C. Kesselman, J. Nick, and S. Tuecke. Grid Services
for distributed system integration. Computer, 35(6), 2002.
[4] Hewlett Packard Company.
Availability advantage.
h18005.www1.hp.com/services/advantage/
aa_avanto.html, January 2003.
[5] Hewlett Packard Company. HP MC/ServiceGuard. www.hp.
com/products1/unix/highavailability/ar/
mcserviceguard/index.html, January 2003.
TruCluster software.
[6] Hewlett Packard Company.
www.
tru64unix.compaq.com/cluster/, January 2003.
[7] Hewlett
Packard Company.
Utility
computing.
devresource.hp.com/topics/utility_comp.
html, January 2003.
[8] International Business Machines, Inc. Autonomic comput-
ing. www.ibm.com/autonomic/index.shtml, Jan-
uary 2003.
[9] K. Keeton and J. Wilkes. Automating data dependability. In
10th ACM-SIGOPS European Workshop, Sep 2002.
[10] T. Lahiri, A. Ganesh, R. Weiss, and A. Joshi. Fast-Start: quick
fault recovery in Oracle. In ACM SIGMOD, pages 593–598,
2001.
[11] Ofﬁce of Government Commerce. ITIL Service Support. IT In-
frastructure Library. The Stationery Ofﬁce, United Kingdom,
June 2000.
[12] R. A. Sahner and K. S. Trivedi. Reliability modeling using
IEEE Transactions on Reliability, R-36(2):186–
SHARPE.
193, June 1987.
[13] Sun Microsystems, Inc. N1: Revolutionary IT architecture for
business. www.sun.com/software/solutions/n1/
index.html, January 2003.
[14] Sun Microsystems, Inc. Sun[tm] Cluster. www.sun.com/
software/cluster/, January 2003.
[15] vmware. VirtualCenter white paper. www.vmware.com/
pdf/vc_wp.pdf.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:53 UTC from IEEE Xplore.  Restrictions apply.