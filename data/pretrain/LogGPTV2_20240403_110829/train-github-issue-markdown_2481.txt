Slow tokenizers:
    >>> import transformers
    >>> t_slow = transformers.AutoTokenizer.from_pretrained("roberta-base", use_fast=False)
    >>> t_slow.encode_plus("A  sentence.")
    {'input_ids': [0, 83, 50264, 3645, 4, 2],
     'token_type_ids': [0, 0, 0, 0, 0, 0],
     'attention_mask': [1, 1, 1, 1, 1, 1]}
    >>> t_slow.convert_ids_to_tokens([0, 83, 50264, 3645, 4, 2])
    ['', 'ĠA', '', 'Ġsentence', '.', '']
Fast tokenizers:
    >>> import transformers
    >>> t_fast = transformers.AutoTokenizer.from_pretrained("roberta-base", use_fast=True)
    >>> t_fast.encode_plus("A  sentence.")
    {'input_ids': [0, 250, 1437, 50264, 3645, 4, 2],
     'token_type_ids': [0, 0, 0, 0, 0, 0, 0],
     'attention_mask': [1, 1, 1, 1, 1, 1, 1]}
    >>> t_fast.convert_ids_to_tokens([0, 250, 1437, 50264, 3645, 4, 2])
    ['', 'A', 'Ġ', '', 'Ġsentence', '.', ''] 