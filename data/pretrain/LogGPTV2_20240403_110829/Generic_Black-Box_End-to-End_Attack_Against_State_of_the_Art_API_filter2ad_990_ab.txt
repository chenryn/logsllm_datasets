for t=1..T:
(cid:2)
(cid:3)
Dt = {(x, f (x))|x ∈ Xt} # Label the synthetic dataset using the black-box model
ˆft = train(A, Dt) # (Re-)Train the surrogate model
∪ Xt # Perform Jacobian-based dataset
Xt+1 =
x + sign(J ˆft (x)[f (x)])|x ∈ Xt
augmentation
return ˆfT
On each iteration we add a synthetic example to each existing sample. The
surrogate model dataset size is: |Xt| = 2t−1|X1|
The samples used in the initial dataset, X1, were randomly selected from the
test set distribution, but they were not included in the training and test sets to
prevent bias. X1 should be representative so the dataset augmentation covers all
decision boundaries to increase the augmentation’s eﬀectiveness. For example,
if we only include samples from a single family of ransomware in the initial
dataset, we will only be focusing on a speciﬁc area of the decision boundary,
and our augmentation would likely only take us in a certain direction. However,
as shown in Sect. 4.3, this doesn’t mean that all of the malware families in the
training set must be represented to achieve good performance.
Generating Adversarial Examples. An adversarial example is a sequence of
API calls classiﬁed as malicious by the classiﬁer that is perturbed by the addition
of API calls, so that the modiﬁed sequence will be misclassiﬁed as benign. In
order to prevent damaging the code’s functionality, we cannot remove or modify
API calls; we can only add additional API calls. In order to add API calls in
496
I. Rosenberg et al.
a way that doesn’t hurt the code’s functionality, we generate a mimicry attack
(Sect. 5). Our attack is described in Algorithm 2.
Algorithm 2. Adversarial Sequence Generation
Input: f (black-box model), ˆf (surrogate model), x (malicious sequence to perturb,
of length l), n (size of adversarial sliding window), D (vocabulary)
for each sliding window wj of n API calls in x:
∗
= wj
wj
while f (w
∗
j ) = malicious:
Randomly select an API’s position i in w
# Insert a new adversarial API in position i ∈ {1..n}:
j [i] = arg minapi ||sign(wj
∗
w
j [1 : i − 1] ⊥ api ⊥ w
∗
∗ − w
sign(J ˆf (wj )[f (wj )])||
Replace wj (in x) with wj
return (perturbed) x
∗
j [i : n − 1]) −
∗
(cid:3)
j [1 : i − 1] ⊥ api ⊥ w∗
D is the vocabulary of available features, that is, the API calls recorded by
the classiﬁer. The adversarial API call sequence length of l might be diﬀerent
than n, the length of the sliding window API call sequence that is used by the
adversary. Therefore, like the prediction, the attack is performed sequentially on
(cid:2)
l
windows of n API calls. Note that the knowledge of m (the window size
n
of the classiﬁer, mentioned in Sect. 3.1) is not required, as shown in Sect. 4.3.
⊥ is the concatenation operation. w∗
j [i : n − 1] is the
insertion of the encoded API vector in position i of w∗
j . The adversary randomly
chooses i since he/she does not have any way to better select i without incurring
signiﬁcant statistical overhead. Note that an insertion of an API in position
i means that the APIs from position i..n (w∗
j [i : n] ) are “pushed back” one
position to make room for the new API call, in order to maintain the original
sequence and preserve the original functionality of the code. Since the sliding
window has a ﬁxed length, the last API call, w∗
j [n], is “pushed out” and removed
from w∗
j [i : n]).
The APIs “pushed out” from wj will become the beginning of wj+1, so no API
is ignored.
j [0 : i] ⊥
api ⊥ w∗
j [i : n − 1]) − sign(J ˆf (wj)[f(w j)])||. sign(J ˆf (wj)[f(w j)]) gives us
the direction in which we have to perturb the API call sequence in order to
reduce the probability assigned to the malicious class, f(x), and thus change
the predicted label of the API call sequence. However, the set of legitimate API
call embeddings is ﬁnite. Thus, we cannot set the new API to any real value.
We therefore ﬁnd the API call api in D whose insertion directs us closest to the
direction indicated by the Jacobian as most impactful on the model’s prediction.
We iteratively apply this heuristic until we ﬁnd an adversarial input sequence
misclassiﬁed as benign. Note that in [12] the authors replaced a word in a movie
review, so they only needed a single element from the Jacobian (for word i, which
was replaced). All other words remained the same, so no gradient change took
j [i : n − 1], as opposed to ⊥ w∗
j (this is why the term is ⊥ w∗
The newly added API call is w∗
j [i] = arg minapi ||sign(wj
∗ − w∗
End-to-End Attack Against API Call Based Malware Classiﬁers
497
place. In contrast, since we add an API call, all of the API calls following it shift
their position, so we consider the aggregated impact.
While the proposed attack is designed for API call based classiﬁers, it can be
generalized to any adversarial sequence generation. This generalization is a high
performance in terms of attack eﬀectiveness and overhead (Eqs. 4 and 5). This
can be seen in Sect. 4.3, where we compare the proposed attack to [12] for the
IMDB sentiment classiﬁcation task. In Sect. 4.3 we show why the same adversar-
ial examples generated against the surrogate model would be eﬀective against
both the black-box model and other types of classiﬁers due to the principle of
transferability.
We assume that the attacker knows what API calls are available and how
each of them is encoded (one-hot encoding in this paper). This is a commonly
accepted assumption about the attacker’s knowledge [8].
4 Experimental Evaluation
4.1 Dataset
Our dataset contains 500,000 ﬁles (250,000 benign samples and 250,000 malware
samples), including the latest variants. We have ransomware families such as Cer-
ber, Locky, Ramnit, Matsnu, Androm, Upatre, Delf, Zbot, Expiro, Ipamor. and
other malware types (worms, backdoors, droppers, spyware, PUA, and viruses),
each with the same number of samples, to prevent a prediction bias towards
the majority class. 80% of the malware families’ (like the Virut virus family)
samples were distributed between the training and test sets, to determine the
classiﬁer’s ability to generalize to samples from the same family. 20% of the
malware families (such as the WannaCry ransomware family) were used only on
the test set to assess generalization to an unseen malware family. The temporal
diﬀerence between the training set and the test set is several months (meaning
all test set samples are newer than the training set samples), based on Virus-
Total’s ‘ﬁrst seen’ date. We labeled our dataset using VirusTotal, an on-line
scanning service which contains more than 60 diﬀerent security products. Our
ground truth is that a malicious sample is one with 15 or more positive (i.e.,
malware) classiﬁcations from the 60 products. A benign sample is one with zero
positive classiﬁcations. All samples with 1–14 positives were omitted to prevent
false positive contamination of the dataset.
We ran each sample in Cuckoo Sandbox, a commonly-used malware anal-
ysis system, for two minutes per sample.2 We parsed the JSON ﬁle generated
by Cuckoo Sandbox and extracted the API call sequences generated by the
2 Tracing only the ﬁrst seconds of a program execution might not detect certain mal-
ware types, like “logic bombs” that commence their malicious behavior only after
the program has been running some time. However, this can be mitigated both by
classifying the suspension mechanism as malicious, if accurate, or by tracing the code
operation throughout the program execution life-time, not just when the program
starts.
498
I. Rosenberg et al.
Fig. 1. Overview of the malware classiﬁcation process
inspected code during its execution. The extracted API call sequences are the
malware classiﬁer’s features. Although the JSON can be used as raw input for
a neural network classiﬁer (as done in [16]), we parsed it, since we wanted to
focus only on API calls without adding other features, such as connected network
addresses, which are also extracted by Cuckoo Sandbox.
The overview of the malware classiﬁcation process is shown in Fig. 1. Figure
2a present a more detailed view of the classiﬁer’s structure.
We run the samples on a VirtualBox’s snapshot with Windows 8.1 OS,3 since
most malware target the Windows OS.
Cuckoo Sandbox is a tool known to malware writers, some of whom write code
to detect if the malware is running in a Cuckoo Sandbox (or on virtual machines)
and if so, the malware quit immediately to prevent reversing eﬀorts. In those
cases, the ﬁle is malicious, but its behavior recorded in Cuckoo Sandbox (its API
call sequence) isn’t malicious, due to its anti-forensic capabilities. To mitigate
such contamination of our dataset, we used two countermeasures: (1) We applied
YARA rules to ﬁnd samples trying to detect sandbox programs such as Cuckoo
Sandbox and omitted all such samples. (2) We considered only API call sequences
with more than 15 API calls (as in [13]), omitting malware that, e.g., detect a VM
and quit. This ﬁltering left us with about 400,000 valid samples, after balancing
the benign samples number. The ﬁnal training set size is 360,000 samples, 36,000
of which serve as the validation set. The test set size is 36,000 samples. All sets
are balanced between malicious and benign samples. One might argue that the
evasive malware that apply such anti-VM techniques are extremely challenging
and relevant. However, in this paper we focus on the adversarial attack. This
attack is generic enough to work for those evasive malware as well, assuming
that other mitigation techniques (e.g., anti-anti-VM), would be applied.
4.2 Malware Classiﬁer Performance
No open source or commercial trail versions of API calls based deep learning
intrusion detection systems are available, as such products target enterprises.
Dynamic models are not available in VirusTotal as well. Therefore, we created
our own black-box malware classiﬁers. This also allows us to evaluate the attack
eﬀectiveness (Eq. 4) against many classiﬁer types.
3 While it is true that the API calls sequence would vary across diﬀerent OSs or con-
ﬁgurations, both the black-box classiﬁer and the surrogate model generalize across
those diﬀerences, as they capture the “main features” over the sequence, which are
not vary between OSs.
End-to-End Attack Against API Call Based Malware Classiﬁers
499
Fig. 2. Classiﬁer architecture overview
We limited our maximum input sequence length to m = 140 API calls (longer
sequence lengths, e.g., m = 1000, had no eﬀect on the accuracy) and padded
shorter sequences with zeros. A zero stands for a null API in our one-hot encod-
ing. Longer sequences are split into windows of m API calls, and each window
is classiﬁed in turn. If any window is malicious the entire sequence is considered
malicious. Thus, the input of all of the classiﬁers is a vector of m = 140 API call
types in one-hot encoding, using 314 bits, since there were 314 monitored API
call types in the Cuckoo reports for our dataset. The output is a binary classiﬁ-
cation: malicious or benign. An overview of the LSTM architecture is shown in
Fig. 2a.
We used the Keras implementation for all neural network classiﬁers, with
TensorFlow used for the back end. XGBoost and Scikit-Learn were used for all
other classiﬁers.
The loss function used for training was binary cross-entropy. We used the
Adam optimizer for all of the neural networks. The output layer was fully-
connected with sigmoid activation for all NNs. We ﬁne-tuned the hyper param-
eters for all classiﬁers based on the relevant state of the art papers, e.g., window
size from [13], number of hidden layers from [5,9], dropout rate from [9], and
number of trees in a random forest classiﬁer and the decision tree splitting crite-
ria from [15]. For neural networks, a rectiﬁed linear unit, ReLU(x) = max(0, x),
was chosen as an activation function for the input and hidden layers due to
its fast convergence compared to sigmoid() or tanh(), and dropout was used
to improve the generalization potential of the network. Training was conducted
500
I. Rosenberg et al.
Table 1. Classiﬁer performance
Classiﬁer type Accuracy (%) Classiﬁer type
Accuracy (%)
RNN
BRNN
LSTM
97.90
95.58
98.26
Bidirectional GRU
Fully-Connected DNN
1D CNN
Deep LSTM 97.90
Random forest
BLSTM
97.90
SVM
Deep BLSTM 98.02
Logistic regression
98.04
94.70
96.42
98.90
86.18
89.22
GRU
97.32
Gradient boosted decision tree 91.10
for a maximum of 100 epochs, but convergence was usually reached after 15–20
epochs, depending on the type of classiﬁer. Batch size of 32 samples was used.
The classiﬁers also have the following classiﬁer-speciﬁc hyper parameters:
DNN - Two fully-connected hidden layers of 128 neurons, each with ReLU acti-
vation and a dropout rate of 0.2; CNN - 1D ConvNet with 128 output ﬁlters,
stride length of one, 1D convolution window size of three and ReLU activation,
followed by a global max pooling 1D layer and a fully connected layer of 128
neurons with ReLU activation and a dropout rate of 0.2; RNN, LSTM, GRU,