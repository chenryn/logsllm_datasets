4.5.2 Discussion
Prεεch’s use of DP is different from the most standard use-
case of DP (like numeric datasets). It deals with concrete
units like words instead of numeric statistics – introducing
new challenges; we discuss these challenges and how Prεεch
circumvents them in this section.
Vocabulary deﬁnition: The foremost task for deﬁning the
word histogram is deﬁning the vocabulary, V . The most con-
servative approach to deﬁne V is to consider the total set of
all English stemmed and non-stop words. Such a vocabulary
would be prohibitively large for efﬁcient and practical usage.
However, note that such a deﬁnition of V is an overestimate
as no real-world document would contain all possible English
words. Recall that our objective of adding noise is to obfuscate
any statistical analysis built on top of the document’s BoW
(histogram), such as a topic modeling and stylometry analy-
sis. Typically, BoW based statistical analyses are concerned
only with the set of most frequent words. For example, any
standard topic model captures only the top m percentile most
frequent words in a transcript [37, 43]. The same applies to
stylometry analysis, which is based on measures of the unique
distribution of frequently used words of different individuals.
Thus, as long as the counts of the most common words of
the transcript are protected (via DP), the subsequent statisti-
cal model (like topic model) built over the word histogram
will be privacy-preserving too (by Thm. 4.1). However, high-
frequency words might not be the only ones that contain
important information about TS. To tackle this, we also in-
clude words with large Term Frequency-Inverse Document
Frequency (TF-IDF) weight to our vocabulary. This weight is
a statistical measure used to evaluate how signiﬁcant a word
is to a document relative to a baseline corpus. The weight in-
creases proportionally to the number of times a word appears
in the document but is offset by the frequency of the word in
the baseline corpus. This offset adjusts for the fact that some
words appear more frequently in general. To this end, Prεεch
The following theorem (Thm. 4.4) provides a lower bound
on the pairwise (cid:96)1 distance between the true and noisy top-
ics as a function of the privacy parameters of the DP word
USENIX Association
29th USENIX Security Symposium    2709
S
S
S
makes an estimate of the vocabulary from T OSP
. Although
existing ofﬂine transcribers have high WER, we found (empir-
ically) that they can identify the set of domain words of S with
high accuracy (details in Sec. 7.3). For computing the TF-IDF
values, IDF is computed using an external NLP corpus like
Wikipedia articles. Thus formally, V = {w|w ∈ { top m per-
}∪{ words with
centile of the most frequent words in T OSP
TF-IDF value ≥ ∆ in T OSP
}}. Note that V should be devoid
of all sensitive words which are scrubbed off from S in step
2 of Fig. 1. Additionally, the vocabulary can be extended to
contain out-of-domain words, i.e., random English words that
are not necessarily part of the original document. This helps
in protecting against text classiﬁcation attacks (Sec. 7.3).
Speciﬁcities of the word histogram: As discussed above,
the goal of the DP mechanism is to generate noisy counts for
each wi ∈ V . An artifact of our setting is that this noise has to
be non-negative and integral. This is because dummy words
(for the noisy counts) can only be added to S; removing any
word from S is not feasible as this would entail in recogniz-
ing the word directly from S, which would require accurate
transcription. Hence, Prεεch uses the truncated Laplace mech-
anism to ensure non-negative and integral noise.
Setting privacy parameters: The parameters ε and δ quan-
tify the privacy provided by a DP-mechanism; lower the val-
ues higher is the privacy guarantee achieved. The distance
parameter d, intuitively, connects the privacy deﬁnition in the
word histogram, which is purely a formal representation, to
a semantic privacy notion. For example, it can quantify how
much the noisy topic models computed by the CSP (from
T CSP
S . Thus, the user can tune d
S
depending on the target statistical analysis. In the following,
we detail a mechanism, as a guide for the user, for choosing
d when the target statistical analysis is topic modeling.
Let us assume that the user has a set of speech ﬁles {S j}
topics from the corpus(cid:83)
to be transcribed. Let D j denote the ground truth transcript
corresponding to speech ﬁle S j. The objective is to learn t
j D j with at least k words per topic (a
on(cid:83)
topic is a distribution over a subset of words from the corpus).
Let T = {T1,··· ,Tt} represent the original topic model built
t(cid:105) represent the noisy
) should differ from that of T g
j D j =(cid:83)
S j and T (cid:48) = (cid:104)T(cid:48)
j T g
1,··· ,T(cid:48)
topic model computed by the CSP.
(cid:17)(cid:16) Cmin
t − 1
(cid:111)
(cid:110) v·(|D j|−|wl, j|ω j)
2
1
1−(t−1)
max j |D j|
k
histogram release mechanism (speciﬁcally, the term Cmin is a
function of (d,ε,δ)).
Theorem 4.4. For any pair of topics (T,T(cid:48)) ∈ T × T (cid:48),
||T − T(cid:48)||1 ≥ 2
(cid:17)(cid:17)
1−t
(cid:16)
k
,
(cid:16)
max j |D j||
|D j|·(|D j|+v·ω j
where Cmin = min j,l
, |D j| is the total
number of words in D j, ω j is the total number of unique words,
v is the variance of the distribution Lp(ε(cid:48),δ(cid:48),d), δ(cid:48) = βδ and
|wl, j| is the number of times the word wl ∈ V appears in D j.
The proof of this theorem and the descriptions of the pa-
)
rameters are presented in the full paper [3] .
Dummy word injection: As discussed earlier, achieving dif-
ferential privacy requires adding dummy words to S. Prεεch
generates the dummy text corpus using an NLP language
model (Sec. 6). The model takes in a short text sample from
the required topic and generates an entire document of any
required length based on that input. In some scenarios, the
user can also provide a corpus of non-publicly available doc-
uments with the same vocabulary. This scenario is valid in
many practical settings. For instance, in an educational insti-
tution, the sensitive speech ﬁles requiring transcription might
be the interviews/oral exams of the students conducted on a
speciﬁc subject, and the noise corpus can be the lecture notes
of the same subject.
Next, Prεεch generates a set of dummy segments, Sd, from
the dummy corpus above. Let us assume that each of the true
segments contains at most k non-stop words (depends on the
segment length). Prεεch ensures that each dummy segment
also contains no more than k non-stop words. Additionally,
each such segment must contain only one word from the
vocabulary V . This means that although the physical noise
addition is carried at the segment level, it is still equivalent
to adding noise at the level of words (belonging to V ) as we
only care about wi ∈ V . Each dummy segment is injected only
once per CSP. Since the dummy segments have to be added
in the speech domain, Prεεch applies TTS transforms to the
segments in Sd such that they have the same acoustic features
as S. This condition ensures that Sd are indistinguishable
from S in terms of their acoustic features. Prεεch provides
the user with two broad options to satisfy this condition –
voice cloning or voice conversion.
Voice cloning is a TTS system that generates speech in a
target speaker voice. Given a speech sample from the target
speaker, the system generates an embedding of the speaker’s
voice biometric features. It uses this embedding to synthesize
new utterances of any linguistic content in the target speaker’s
voice. Prεεch utilizes such a technology to clone the original
speaker’s voice and uses it to generate acoustically similar
dummy segments Sd. Prεεch applies a state-of-the-art voice
cloning system [20], which generates a close-to-natural syn-
thetic voice using a short (∼ 5 sec.) target voice sample.
We evaluate this cloning system in Sec. 3.1, and the cloned
samples are successfully identiﬁed as the true speakers. How-
ever, voice cloning does not protect the speakers’ voice bio-
metrics, and can be potentially thwarted by a stronger ad-
versary. Hence, Prεεch provides voice conversion (VC) as
a stronger privacy-preserving option for the user. VC trans-
forms the voice of a source speaker to sound like a target
speaker. Prεεch utilizes VC to obfuscate the true speakers’
voice biometrics as well as to mitigate the DP noise indis-
tinguishability concern by converting the true and dummy
segments into a single target speaker voice (Sec. 4.6). We
discuss the utility-privacy trade-offs of both options in Sec. 7.
It is important to note that the dummy segments do not
affect the WER of T CSP
. It is so because Prεεch can exactly
identify all such dummy segments (from their timestamps)
and remove them from T CSP
. Additionally, since the transcrip-
tion is done one segment at a time, the dummy segments do
not affect the accuracy of the true segments (S) either. Seg-
mentation and voice conversion are the culprits behind the
WER degradation, as will be evident in Sec. 7. Thus in Prεεch,
the noise (in the form of dummy segments) can ensure differ-
ential privacy without affecting the utility. This is in contrast
to standard usage of differential privacy for releasing numeric
statistics where the noisy statistics result in a clear loss of
accuracy. However, the addition of the dummy segments in
Prεεch does increase the monetary cost of using the online
service that has to transcribe more speech data than needed.
We analyze this additional cost in Sec. 7.
S
S
In practice, we have multiple well-known cloud-based tran-
scription services with low WER like Google Cloud Speech-
to-Text, Amazon Transcribe, etc. Prεεch uses them to its
advantage in the following way. Prεεch splits the set of seg-
ments S into N different sets (step 3 in Sec. 4.5.3) Si,i ∈ [N]
where N is the number of CSPs with low WER. Then, Prεεch
sends each subset to a different CSP (after adding suitable
noise segments to each set and shufﬂing them). Since each en-
gine is owned by a different, often competing corporation, it is
reasonable to assume that the CSPs are non-colluding. Thus,
assuming that each segment contains at most one word in V ,
each subset of segments Si can be viewed as randomly sam-
pled sets from S with sampling probability β = 1/N. From
Thm. 4.2, this partitioning results in a privacy ampliﬁcation.
4.5.3 Mechanism
We summarize the DP mechanism by which Prεεch generates
the dummy segments for S. The inputs for the mechanism are
(1) S – the short segments of the speech ﬁle S, (2) the privacy
parameters ε and δ and (3) N – the number of non-colluding
CSPs to use. This mechanism works as follows:
• Identify the vocabulary V = {w|w ∈ { top m percentile
}∪{ words with TF-
}} through running an ofﬂine tran-
of the most frequent words in T OSP
IDF value ≥ ∆ in T OSP
scriber over S.
S
S
2710    29th USENIX Security Symposium
USENIX Association
• Tune the value of d based on the lower bound from
Thm. 4.4, ε and δ.
• Generate N separate noise vectors, ηi ∼ [Lp((ln(1 +
β (eε − 1)),βδ,d)]|V |,i ∈ [N]. Thus for every partition i,
1
Prεεch associates each word in V with a noise value, a
non-negative integer.
• From the NLP generated text, extract all the text segments
that contain words from V . For each partition i, sample
the text segments from this corpus to match the noise
vector ηi. This is the set of noise (dummy) segments for
partition i, Sd,i. Iterate on generating text from the NLP
language model until the required noise count is satisﬁed.
• Randomly partition S into N sets Si,i ∈ [N] where
Pr[segment s goes to partition i] = β = 1/N,s ∈ S.
• For each partition i ∈ [N], shufﬂe the dummy segments in
Sd,i (after applying TTS and VC) with the segments in Si
(after applying VC), and send it to the CSPi.
The ﬁrst 4 steps in the above mechanism are performed in
stage 3 in Prεεch (Fig. 1) while steps 5-6 are performed in
stage 6.
Theorem 4.5. Any topic model computed by CSPi,i ∈ [N]
from T CSPi
Proof. From Thm. 4.2 and Thm. 4.3, we conclude that the
word histogram ˜Hi computed from T CSPi
is (ε,δ) - DP for
distance d. Thm. 4.1 proves that the topic model from ˜Hi is
still (ε,δ)-DP as it is a post-processing computation.
is (ε,δ)-DP.
S
S
4.5.4 Novelty of Prεεch’s Use of Differential Privacy
Here, we summarize the key novelty in Prεεch’s use of DP:
(1) Typically, DP is applied to statistical analysis of numerical
data where "noise" corresponds to numeric values. In contrast,
in Prεεch, "noise" corresponds to concrete units – words. To
tackle this challenge, we applied a series of operations (seg-
mentation, shufﬂing, and partitioning) to transform the speech
transcription into a BoW model, where the DP guarantee can
be achieved. Moreover, the noise addition has to be done in
the speech domain. This constraint results in new challenges:
the lack of a priori access to the word histogram domain V ,
and generating indistinguishable dummy speech segments.
(2) In our setting, the use of a DP mechanism does not intro-
duce a privacy-utility trade-off from the speech transcription
standpoint. Prεεch performs transcription one segment at a
time. It keeps track of the timestamps of the dummy segments
and completely removes their corresponding text from the
ﬁnal transcription (Sec. 4.2.3). This ﬁltration step is achiev-
able in Prεεch, unlike numeric applications of DP, because of
the atomic nature of transcription. However, the dummy seg-
ments increase the monetary cost of transcription, resulting
in a privacy-monetary cost trade-off as shown in Table 3. To
tackle this issue, Prεεch takes advantage of the presence of
multiple CSPs (Sec. 4.5.2). Thus, the idea of utilizing multi-
ple CSPs for cost reduction (Thm. 4.2) is a novel contribution.
(3) We introduce an additional parameter d, the distance
between the pair of histograms, in our privacy deﬁnition
(Defn. 4.1). Intuitively, d connects the privacy deﬁnition in
the word histogram model, which is purely a formal represen-
tation, to a semantic privacy notion (e.g., (cid:96)1 distance between
true and noisy topic models, Thm. 4.4) as shown in Fig. 6 and
7. This contribution builds on ideas like group privacy [13]
and generalized distance metrics [10].
4.5.5 Control Knobs
S
S
The construction of the DP word histogram provides the user
with multiple control knobs for customization:
Parameter d: According to Def. 4.1, from ˜H the CSP will not
be able to distinguish between T CSP
and any other transcript
that differs from T CSP
in d words from V . Thus, higher the
value of d, larger is the ε-indistinguishability neighborhood
for ˜H and hence, better is the privacy guarantee. But it results
in an increased amount of noise injection (hence, increased
monetary cost – details in Sec. 7.3).
Vocabulary: The size of V is a control knob, speciﬁcally, the
parameters m and ∆ and the number of out-of-domain words.
The trade-off here is: the larger the size of V , the greater is
the scope of the privacy guarantee. However, the noise size
scales with |V | and hence incurs higher cost (details in Sec.
7.3).
Voice transformation for noisy segments: Prεεch provides
two options for noise synthesis – voice cloning and voice
conversion. Voice cloning does not affect the transcription
utility, measured in WER, because it does not apply any trans-
formations on the original speaker’s voice. However, it fails
to protect the sensitive biometric information in S. Moreover,
there is no guarantee that a strong adversary cannot develop
a system that can distinguish the cloned speech segments